



















































Generating Personalized Recipes from Historical User Preferences


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 5976–5982,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

5976

Generating Personalized Recipes from Historical User Preferences

Bodhisattwa Prasad Majumder∗, Shuyang Li∗, Jianmo Ni, Julian McAuley
Computer Science and Engineering
University of California, San Diego

{bmajumde, shl008, jin018, jmcauley}@ucsd.edu

Abstract

Existing approaches to recipe generation are
unable to create recipes for users with culinary
preferences but incomplete knowledge of in-
gredients in specific dishes. We propose a new
task of personalized recipe generation to help
these users: expanding a name and incom-
plete ingredient details into complete natural-
text instructions aligned with the user’s histor-
ical preferences. We attend on technique- and
recipe-level representations of a user’s previ-
ously consumed recipes, fusing these ‘user-
aware’ representations in an attention fusion
layer to control recipe text generation. Exper-
iments on a new dataset of 180K recipes and
700K interactions show our model’s ability
to generate plausible and personalized recipes
compared to non-personalized baselines.

1 Introduction

In the kitchen, we increasingly rely on instructions
from cooking websites: recipes. A cook with a
predilection for Asian cuisine may wish to pre-
pare chicken curry, but may not know all neces-
sary ingredients apart from a few basics. These
users with limited knowledge cannot rely on ex-
isting recipe generation approaches that focus on
creating coherent recipes given all ingredients and
a recipe name (Kiddon et al., 2016). Such mod-
els do not address issues of personal preference
(e.g. culinary tastes, garnish choices) and incom-
plete recipe details. We propose to approach both
problems via personalized generation of plausi-
ble, user-specific recipes using user preferences
extracted from previously consumed recipes.

Our work combines two important tasks from
natural language processing and recommender
systems: data-to-text generation (Gatt and Krah-
mer, 2018) and personalized recommendation

∗ denotes equal contribution

(Rashid et al., 2002). Our model takes as user in-
put the name of a specific dish, a few key ingre-
dients, and a calorie level. We pass these loose
input specifications to an encoder-decoder frame-
work and attend on user profiles—learned latent
representations of recipes previously consumed by
a user—to generate a recipe personalized to the
user’s tastes. We fuse these ‘user-aware’ represen-
tations with decoder output in an attention fusion
layer to jointly determine text generation. Quan-
titative (perplexity, user-ranking) and qualitative
analysis on user-aware model outputs confirm that
personalization indeed assists in generating plau-
sible recipes from incomplete ingredients.

While personalized text generation has seen
success in conveying user writing styles in the
product review (Ni et al., 2017; Ni and McAuley,
2018) and dialogue (Zhang et al., 2018) spaces, we
are the first to consider it for the problem of recipe
generation, where output quality is heavily depen-
dent on the content of the instructions—such as
ingredients and cooking techniques.

To summarize, our main contributions are as
follows:

1. We explore a new task of generating plausi-
ble and personalized recipes from incomplete
input specifications by leveraging historical
user preferences;1

2. We release a new dataset of 180K+ recipes
and 700K+ user reviews for this task;

3. We introduce new evaluation strategies for
generation quality in instructional texts, cen-
tering on quantitative measures of coher-
ence. We also show qualitatively and quan-
titatively that personalized models generate
high-quality and specific recipes that align
with historical user preferences.

1Our source code and appendix are at
https://github.com/majumderb/
recipe-personalization

https://github.com/majumderb/recipe-personalization
https://github.com/majumderb/recipe-personalization


5977

2 Related Work

Large-scale transformer-based language models
have shown surprising expressivity and fluency
in creative and conditional long-text generation
(Vaswani et al., 2017; Radford et al., 2019). Re-
cent works have proposed hierarchical methods
that condition on narrative frameworks to generate
internally consistent long texts (Fan et al., 2018;
Xu et al., 2018; Yao et al., 2018). Here, we gener-
ate procedurally structured recipes instead of free-
form narratives.

Recipe generation belongs to the field of data-
to-text natural language generation (Gatt and
Krahmer, 2018), which sees other applications
in automated journalism (Leppänen et al., 2017),
question-answering (Agrawal et al., 2017), and
abstractive summarization (Paulus et al., 2018),
among others. Kiddon et al. (2015); Bosselut et al.
(2018b) model recipes as a structured collection of
ingredient entities acted upon by cooking actions.
Kiddon et al. (2016) imposes a ‘checklist’ atten-
tion constraint emphasizing hitherto unused ingre-
dients during generation. Yang et al. (2017) at-
tend over explicit ingredient references in the prior
recipe step. Similar hierarchical approaches that
infer a full ingredient list to constrain generation
will not help personalize recipes, and would be in-
feasible in our setting due to the potentially un-
constrained number of ingredients (from a space
of 10K+) in a recipe. We instead learn historical
preferences to guide full recipe generation.

A recent line of work has explored user- and
item-dependent aspect-aware review generation
(Ni et al., 2017; Ni and McAuley, 2018). This
work is related to ours in that it combines con-
textual language generation with personalization.
Here, we attend over historical user preferences
from previously consumed recipes to generate
recipe content, rather than writing styles.

3 Approach

Our model’s input specification consists of: the
recipe name as a sequence of tokens, a partial list
of ingredients, and a caloric level (high, medium,
low). It outputs the recipe instructions as a token
sequence: Wr = {wr,0, . . . , wr,T } for a recipe r
of length T . To personalize output, we use histor-
ical recipe interactions of a user u ∈ U .
Encoder: Our encoder has three embedding lay-
ers: vocabulary embedding V , ingredient embed-
ding I, and caloric-level embedding C. Each token

in the (length Ln) recipe name is embedded via V;
the embedded token sequence is passed to a two-
layered bidirectional GRU (BiGRU) (Cho et al.,
2014), which outputs hidden states for names
{nenc,j ∈ R2dh}, with hidden size dh. Similarly
each of theLi input ingredients is embedded via I,
and the embedded ingredient sequence is passed
to another two-layered BiGRU to output ingredi-
ent hidden states as {ienc,j ∈ R2dh}. The caloric
level is embedded via C and passed through a pro-
jection layer with weights Wc to generate calorie
hidden representation cenc ∈ R2dh .
Ingredient Attention: We apply attention (Bah-
danau et al., 2015) over the encoded ingredients
to use encoder outputs at each decoding time step.
We define an attention-score function α with key
K and query Q:

α(K,Q) =
exp (tanh (Wα [K +Q] + bα))

Z
,

with trainable weights Wα, bias bα, and normal-
ization term Z. At decoding time t, we calculate
the ingredient context ait ∈ Rdh as:

ait =
Li∑
j=1

α (ienc,j ,ht)× ienc,j .

Decoder: The decoder is a two-layer GRU with
hidden state ht conditioned on previous hidden
state ht−1 and input token wr,t from the original
recipe text. We project the concatenated encoder
outputs as the initial decoder hidden state:

h0
Ä
∈ Rdh

ä
=Wh0 [nenc,Ln ; ienc,Li ; cenc] + bh0

ht,ot = GRU
Äî
wr,t;a

i
t

ó
,ht−1

ä
.

To bias generation toward user preferences, we
attend over a user’s previously reviewed recipes to
jointly determine the final output token distribu-
tion. We consider two different schemes to model
preferences from user histories: (1) recipe inter-
actions, and (2) techniques seen therein (defined
in Section 4). Rendle et al. (2009); Quadrana et al.
(2018); Ueda et al. (2011) explore similar schemes
for personalized recommendation.
Prior Recipe Attention: We obtain the set of
prior recipes for a user u: R+u , where each recipe
can be represented by an embedding from a recipe
embedding layer R or an average of the name to-
kens embedded by V . We attend over the k-most
recent prior recipes, Rk+u , to account for tempo-
ral drift of user preferences (Moore et al., 2013).



5978

Figure 1: Sample data flow through model architec-
ture. Emphasis on prior recipe attention scores (darker
is stronger). Ingredient attention omitted for clarity.

These embeddings are used in the ‘Prior Recipe’
and ‘Prior Name’ models, respectively.

Given a recipe representation r ∈ Rdr (where
dr is recipe- or vocabulary-embedding size de-
pending on the recipe representation) the prior
recipe attention context arut is calculated as

arut =
∑

r∈Rk+u

α (r,ht)× r.

Prior Technique Attention: We calculate prior
technique preference (used in the ‘Prior Tech‘
model) by normalizing co-occurrence between
users and techniques seen in R+u , to obtain a pref-
erence vector ρu. Each technique x is embedded
via a technique embedding layer X to x ∈ Rdx .
Prior technique attention is calculated as

axut =
∑

x seen in R+u

(α (x,ht) + ρu,x)× x,

where, inspired by copy mechanisms (See et al.,
2017; Gu et al., 2016), we add ρu,x for technique x
to emphasize the attention by the user’s prior tech-
nique preference.
Attention Fusion Layer: We fuse all contexts
calculated at time t, concatenating them with de-
coder GRU output and previous token embedding:

aft =ReLU
Ä
Wf
î
wr,t;ot;a

i
t; (a

ru
t or a

xu
t )
ó
+bf

ä
.

We then calculate the token probability:

P (Sr,t) = softmax
Ä
WP [a

f
t ] + bP

ä
,

and maximize the log-likelihood of the generated
sequence conditioned on input specifications and
user preferences. Figure 1 shows a case where the
Prior Name model attends strongly on previously
consumed savory recipes to suggest the usage of
an additional ingredient (‘cilantro’).

Split # Users # Recipes # Actions Sparsity3

Train 25,076 160,901 698,901 99.983%
Dev 7,023 6,621 7,023 –
Test 12,455 11,695 12,455 –

Table 1: Statistics of Food.com interactions

4 Recipe Dataset: Food.com

We collect a novel dataset of 230K+ recipe texts
and 1M+ user interactions (reviews) over 18 years
(2000-2018) from Food.com.2 Here, we restrict to
recipes with at least 3 steps, and at least 4 and no
more than 20 ingredients. We discard users with
fewer than 4 reviews, giving 180K+ recipes and
700K+ reviews, with splits as in Table 1.

Our model must learn to generate from a di-
verse recipe space: in our training data, the av-
erage recipe length is 117 tokens with a maxi-
mum of 256. There are 13K unique ingredients
across all recipes. Rare words dominate the vocab-
ulary: 95% of words appear <100 times, account-
ing for only 1.65% of all word usage. As such,
we perform Byte-Pair Encoding (BPE) tokeniza-
tion (Sennrich et al., 2016; Radford et al., 2018),
giving a training vocabulary of 15K tokens across
19M total mentions. User profiles are similarly di-
verse: 50% of users have consumed ≤6 recipes,
while 10% of users have consumed >45 recipes.

We order reviews by timestamp, keeping the
most recent review for each user as the test set, the
second most recent for validation, and the remain-
der for training (sequential leave-one-out evalua-
tion (Kang and McAuley, 2018)). We evaluate
only on recipes not in the training set.

We manually construct a list of 58 cooking
techniques from 384 cooking actions collected by
Bosselut et al. (2018b); the most common tech-
niques (bake, combine, pour, boil) account for
36.5% of technique mentions. We approximate
technique adherence via string match between the
recipe text and technique list.

5 Experiments and Results

For training and evaluation, we provide our model
with the first 3-5 ingredients listed in each recipe.
We decode recipe text via top-k sampling (Rad-
ford et al., 2019), finding k = 3 to produce sat-
isfactory results. We use a hidden size dh = 256

2https://www.kaggle.com/shuyangli94/
food-com-recipes-and-user-interactions

3Ratio of unobserved actions to all possible actions.

https://www.kaggle.com/shuyangli94/food-com-recipes-and-user-interactions
https://www.kaggle.com/shuyangli94/food-com-recipes-and-user-interactions


5979

Model BPE PPL BLEU-1 BLEU-4 ROUGE-L D-1 (%) D-2 (%) UMA MRR PP (%)

NN – 20.279 0.465 16.871 0.931 9.394 0.100 0.293 –
Enc-Dec 9.611 28.391 3.385 25.001 0.220 1.928 0.100 0.293 –

Prior Tech 9.572 28.864 3.312 24.920 0.233 2.158 0.128 0.319 62.821
Prior Recipe 9.551 27.858 3.215 24.822 0.231 2.062 0.302 0.412 66.026
Prior Name 9.516 28.046 3.211 24.794 0.233 2.080 0.505 0.628 61.165

Table 2: Metrics on generated recipes from test set. D-1/2 = Distinct-1/2, UMA = User Matching Accuracy, MRR
= Mean Reciprocal Rank, PP = Pairwise preference over baseline (evaluated for 310 recipe pairs per model).

Input Name: Pomberrytini; Ingredients: pomegranate-blueberry juice, cranberry juice, vodka ; Calorie: Low

Gold Place everything except the orange slices in a cocktail shaker. Shake until well mixed and well chilled.
Pour into martini glasses and float an orange slice in each glass.

Enc-Dec Combine all ingredients. Cover and refrigerate. Serve with whipped topping.

Prior Tech Combine all ingredients. Store in refrigerator. Serve over ice. Enjoy!

Prior Recipe Pour the ice into a cocktail shaker. Pour in the vodka and vodka. Add a little water and shake to mix.
Pour into the glass and garnish with a slice of orange slices. Enjoy!

Prior Name Combine all ingredients except for the ice in a blender or food processor. Process to make a smooth
paste and then add the remaining vodka and blend until smooth. Pour into a chilled glass and
garnish with a little lemon and fresh mint.

Table 3: Sample generated recipe. Emphasis on personalization and explicit ingredient mentions via highlights.

for both the encoder and decoder. Embedding di-
mensions for vocabulary, ingredient, recipe, tech-
niques, and caloric level are 300, 10, 50, 50, and
5 (respectively). For prior recipe attention, we
set k = 20, the 80th %-ile for the number of
user interactions. We use the Adam optimizer
(Kingma and Ba, 2015) with a learning rate of
10−3, annealed with a decay rate of 0.9 (Howard
and Ruder, 2018). We also use teacher-forcing
(Williams and Zipser, 1989) in all training epochs.

In this work, we investigate how leveraging
historical user preferences can improve genera-
tion quality over strong baselines in our setting.
We compare our personalized models against two
baselines. The first is a name-based Nearest-
Neighbor model (NN). We initially adapted the
Neural Checklist Model of Kiddon et al. (2016)
as a baseline; however, we ultimately use a sim-
ple Encoder-Decoder baseline with ingredient at-
tention (Enc-Dec), which provides comparable
performance and lower complexity. All person-
alized models outperform baseline in BPE per-
plexity (Table 2) with Prior Name performing the
best. While our models exhibit comparable perfor-
mance to baseline in BLEU-1/4 and ROUGE-L,
we generate more diverse (Distinct-1/2: percent-
age of distinct unigrams and bigrams) and accept-
able recipes. BLEU and ROUGE are not the most

appropriate metrics for generation quality. A ‘cor-
rect’ recipe can be written in many ways with the
same main entities (ingredients). As BLEU-1/4
capture structural information via n-gram match-
ing, they are not correlated with subjective recipe
quality. This mirrors observations from Baheti
et al. (2018); Fan et al. (2018).

We observe that personalized models make
more diverse recipes than baseline. They thus
perform better in BLEU-1 with more key enti-
ties (ingredient mentions) present, but worse in
BLEU-4, as these recipes are written in a person-
alized way and deviate from gold on the phrasal
level. Similarly, the ‘Prior Name’ model generates
more unigram-diverse recipes than other personal-
ized models and obtains a correspondingly lower
BLEU-1 score.

Qualitative Analysis: We present sample out-
puts for a cocktail recipe in Table 3, and addi-
tional recipes in the appendix. Generation qual-
ity progressively improves from generic baseline
output to a blended cocktail produced by our
best performing model. Models attending over
prior recipes explicitly reference ingredients. The
Prior Name model further suggests the addition
of lemon and mint, which are reasonably asso-
ciated with previously consumed recipes like co-
conut mousse and pork skewers.



5980

Personalization: To measure personalization, we
evaluate how closely the generated text corre-
sponds to a particular user profile. We compute
the likelihood of generated recipes using identical
input specifications but conditioned on ten differ-
ent user profiles—one ‘gold’ user who consumed
the original recipe, and nine randomly generated
user profiles. Following Fan et al. (2018), we ex-
pect the highest likelihood for the recipe condi-
tioned on the gold user. We measure user match-
ing accuracy (UMA)—the proportion where the
gold user is ranked highest—and Mean Reciprocal
Rank (MRR) (Radev et al., 2002) of the gold user.
All personalized models beat baselines in both
metrics, showing our models personalize gener-
ated recipes to the given user profiles. The Prior
Name model achieves the best UMA and MRR by
a large margin, revealing that prior recipe names
are strong signals for personalization. Moreover,
the addition of attention mechanisms to capture
these signals improves language modeling perfor-
mance over a strong non-personalized baseline.

Recipe Level Coherence: A plausible recipe
should possess a coherent step order, and we eval-
uate this via a metric for recipe-level coherence.
We use the neural scoring model from Bosselut
et al. (2018a) to measure recipe-level coherence
for each generated recipe. Each recipe step is en-
coded by BERT (Devlin et al., 2019). Our scor-
ing model is a GRU network that learns the over-
all recipe step ordering structure by minimizing
the cosine similarity of recipe step hidden repre-
sentations presented in the correct and reverse or-
ders. Once pretrained, our scorer calculates the
similarity of a generated recipe to the forward and
backwards ordering of its corresponding gold la-
bel, giving a score equal to the difference between
the former and latter. A higher score indicates
better step ordering (with a maximum score of
2). Table 4 shows that our personalized models
achieve average recipe-level coherence scores of
1.78-1.82, surpassing the baseline at 1.77.

Recipe Step Entailment: Local coherence is also
crucial to a user following a recipe: it is crucial
that subsequent steps are logically consistent with
prior ones. We model local coherence as an entail-
ment task: predicting the likelihood that a recipe
step follows the preceding. We sample several
consecutive (positive) and non-consecutive (neg-
ative) pairs of steps from each recipe. We train a
BERT (Devlin et al., 2019) model to predict the

Model Recipe LevelCoherence
Recipe Step
Entailment

Enc-Dec 1.77 0.72

Prior Tech 1.78 0.73
Prior Recipe 1.80 0.76
Prior Name 1.82 0.78

Table 4: Coherence metrics on generated recipes from
test set.

entailment score of a pair of steps separated by a
[SEP] token, using the final representation of the
[CLS] token. The step entailment score is com-
puted as the average of scores for each set of con-
secutive steps in each recipe, averaged over every
generated recipe for a model, as shown in Table 4.

Human Evaluation: We presented 310 pairs of
recipes for pairwise comparison (Fan et al., 2018)
(details in appendix) between baseline and each
personalized model, with results shown in Table 2.
On average, human evaluators preferred personal-
ized model outputs to baseline 63% of the time,
confirming that personalized attention improves
the semantic plausibility of generated recipes. We
also performed a small-scale human coherence
survey over 90 recipes, in which 60% of users
found recipes generated by personalized models
to be more coherent and preferable to those gen-
erated by baseline models.

6 Conclusion

In this paper, we propose a novel task: to gen-
erate personalized recipes from incomplete in-
put specifications and user histories. On a large
novel dataset of 180K recipes and 700K reviews,
we show that our personalized generative models
can generate plausible, personalized, and coher-
ent recipes preferred by human evaluators for con-
sumption. We also introduce a set of automatic co-
herence measures for instructional texts as well as
personalization metrics to support our claims. Our
future work includes generating structured repre-
sentations of recipes to handle ingredient proper-
ties, as well as accounting for references to collec-
tions of ingredients (e.g. “dry mix”).

Acknowledgements. This work is partly sup-
ported by NSF #1750063. We thank all reviewers
for their constructive suggestions, as well as Rei
M., Sujoy P., Alicia L., Eric H., Tim S., Kathy C.,
Allen C., and Micah I. for their feedback.



5981

References
Aishwarya Agrawal, Jiasen Lu, Stanislaw Antol, Mar-

garet Mitchell, C. Lawrence Zitnick, Devi Parikh,
and Dhruv Batra. 2017. VQA: visual question an-
swering. IJCV, 123(1):4–31.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In ICLR.

Ashutosh Baheti, Alan Ritter, Jiwei Li, and Bill Dolan.
2018. Generating more interesting responses in
neural conversation models with distributional con-
straints. In EMNLP.

Antoine Bosselut, Asli Çelikyilmaz, Xiaodong He,
Jianfeng Gao, Po-Sen Huang, and Yejin Choi.
2018a. Discourse-aware neural rewards for coher-
ent text generation. In NAACL-HLT.

Antoine Bosselut, Omer Levy, Ari Holtzman, Corin
Ennis, Dieter Fox, and Yejin Choi. 2018b. Simulat-
ing action dynamics with neural process networks.
In ICLR.

Kyunghyun Cho, Bart van Merrienboer, Çaglar
Gülçehre, Dzmitry Bahdanau, Fethi Bougares, Hol-
ger Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using RNN encoder-decoder
for statistical machine translation. In EMNLP.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: pre-training of
deep bidirectional transformers for language under-
standing. In NAACL-HLT 2019.

Angela Fan, Mike Lewis, and Yann Dauphin. 2018. Hi-
erarchical neural story generation. In ACL.

Albert Gatt and Emiel Krahmer. 2018. Survey of the
state of the art in natural language generation: Core
tasks, applications and evaluation. J. Artif. Intell.
Res., 61:65–170.

Jiatao Gu, Zhengdong Lu, Hang Li, and Victor O. K.
Li. 2016. Incorporating copying mechanism in
sequence-to-sequence learning. In ACL.

Jeremy Howard and Sebastian Ruder. 2018. Universal
language model fine-tuning for text classification. In
ACL.

Wang-Cheng Kang and Julian McAuley. 2018. Self-
attentive sequential recommendation. In ICDM.

Chloé Kiddon, Ganesa Thandavam Ponnuraj, Luke
Zettlemoyer, and Yejin Choi. 2015. Mise en place:
Unsupervised interpretation of instructional recipes.
In EMNLP.

Chloé Kiddon, Luke Zettlemoyer, and Yejin Choi.
2016. Globally coherent text generation with neu-
ral checklist models. In EMNLP.

Diederik P. Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. In ICLR.

Leo Leppänen, Myriam Munezero, Mark Granroth-
Wilding, and Hannu Toivonen. 2017. Data-driven
news generation for automated journalism. In INLG.

Joshua L. Moore, Shuo Chen, Douglas Turnbull, and
Thorsten Joachims. 2013. Taste over time: The tem-
poral dynamics of user preferences. In ISMIR.

Jianmo Ni, Zachary C. Lipton, Sharad Vikram, and Ju-
lian McAuley. 2017. Estimating reactions and rec-
ommending products with generative models of re-
views. In IJCNLP.

Jianmo Ni and Julian McAuley. 2018. Personalized re-
view generation by expanding phrases and attending
on aspect-aware representations. In ACL.

Romain Paulus, Caiming Xiong, and Richard Socher.
2018. A deep reinforced model for abstractive sum-
marization. In ICLR.

Massimo Quadrana, Paolo Cremonesi, and Dietmar
Jannach. 2018. Sequence-aware recommender sys-
tems. In UMAP.

Dragomir R. Radev, Hong Qi, Harris Wu, and Weiguo
Fan. 2002. Evaluating web-based question answer-
ing systems. In LREC.

Alec Radford, Karthik Narasimhan, Tim Salimans, and
Ilya Sutskever. 2018. Improving language under-
standing by generative pre-training.

Alec Radford, Jeff Wu, Rewon Child, David Luan,
Dario Amodei, and Ilya Sutskever. 2019. Language
models are unsupervised multitask learners.

Al Mamunur Rashid, Istvan Albert, Dan Cosley, Shy-
ong K. Lam, Sean M. McNee, Joseph A. Konstan,
and John Riedl. 2002. Getting to know you: learn-
ing new user preferences in recommender systems.
In IUI.

Steffen Rendle, Christoph Freudenthaler, Zeno Gant-
ner, and Lars Schmidt-Thieme. 2009. BPR:
bayesian personalized ranking from implicit feed-
back. In UAI.

Abigail See, Peter J. Liu, and Christopher D. Manning.
2017. Get to the point: Summarization with pointer-
generator networks. In ACL.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Neural machine translation of rare words with
subword units. In ACL.

Mayumi Ueda, Mari Takahata, and Shinsuke Naka-
jima. 2011. User’s food preference extraction for
personalized cooking recipe recommendation. In
SPIM.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In NIPS.

https://doi.org/10.1007/s11263-016-0966-6
https://doi.org/10.1007/s11263-016-0966-6
http://arxiv.org/abs/1409.0473
http://arxiv.org/abs/1409.0473
https://aclanthology.info/papers/D18-1431/d18-1431
https://aclanthology.info/papers/D18-1431/d18-1431
https://aclanthology.info/papers/D18-1431/d18-1431
https://aclanthology.info/papers/N18-1016/n18-1016
https://aclanthology.info/papers/N18-1016/n18-1016
https://openreview.net/forum?id=rJYFzMZC-
https://openreview.net/forum?id=rJYFzMZC-
http://aclweb.org/anthology/D/D14/D14-1179.pdf
http://aclweb.org/anthology/D/D14/D14-1179.pdf
http://aclweb.org/anthology/D/D14/D14-1179.pdf
https://aclweb.org/anthology/papers/N/N19/N19-1423/
https://aclweb.org/anthology/papers/N/N19/N19-1423/
https://aclweb.org/anthology/papers/N/N19/N19-1423/
https://aclanthology.info/papers/P18-1082/p18-1082
https://aclanthology.info/papers/P18-1082/p18-1082
https://doi.org/10.1613/jair.5477
https://doi.org/10.1613/jair.5477
https://doi.org/10.1613/jair.5477
http://aclweb.org/anthology/P/P16/P16-1154.pdf
http://aclweb.org/anthology/P/P16/P16-1154.pdf
https://doi.org/10.18653/v1/P18-1031
https://doi.org/10.18653/v1/P18-1031
https://doi.org/10.1109/ICDM.2018.00035
https://doi.org/10.1109/ICDM.2018.00035
http://aclweb.org/anthology/D/D15/D15-1114.pdf
http://aclweb.org/anthology/D/D15/D15-1114.pdf
http://aclweb.org/anthology/D/D16/D16-1032.pdf
http://aclweb.org/anthology/D/D16/D16-1032.pdf
http://arxiv.org/abs/1412.6980
http://arxiv.org/abs/1412.6980
https://aclanthology.info/papers/W17-3528/w17-3528
https://aclanthology.info/papers/W17-3528/w17-3528
http://www.ppgia.pucpr.br/ismir2013/wp-content/uploads/2013/09/220_Paper.pdf
http://www.ppgia.pucpr.br/ismir2013/wp-content/uploads/2013/09/220_Paper.pdf
https://aclanthology.info/papers/I17-1079/i17-1079
https://aclanthology.info/papers/I17-1079/i17-1079
https://aclanthology.info/papers/I17-1079/i17-1079
https://aclanthology.info/papers/P18-2112/p18-2112
https://aclanthology.info/papers/P18-2112/p18-2112
https://aclanthology.info/papers/P18-2112/p18-2112
https://openreview.net/forum?id=HkAClQgA-
https://openreview.net/forum?id=HkAClQgA-
https://doi.org/10.1145/3209219.3209270
https://doi.org/10.1145/3209219.3209270
http://www.lrec-conf.org/proceedings/lrec2002/pdf/301.pdf
http://www.lrec-conf.org/proceedings/lrec2002/pdf/301.pdf
https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf
https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf
https://openai.com/blog/better-language-models/
https://openai.com/blog/better-language-models/
https://doi.org/10.1145/502716.502737
https://doi.org/10.1145/502716.502737
https://dslpitt.org/uai/displayArticleDetails.jsp?mmnu=1&smnu=2&article_id=1630&proceeding_id=25
https://dslpitt.org/uai/displayArticleDetails.jsp?mmnu=1&smnu=2&article_id=1630&proceeding_id=25
https://dslpitt.org/uai/displayArticleDetails.jsp?mmnu=1&smnu=2&article_id=1630&proceeding_id=25
https://doi.org/10.18653/v1/P17-1099
https://doi.org/10.18653/v1/P17-1099
http://aclweb.org/anthology/P/P16/P16-1162.pdf
http://aclweb.org/anthology/P/P16/P16-1162.pdf
http://dl.acm.org/citation.cfm?id=2887675.2887686
http://dl.acm.org/citation.cfm?id=2887675.2887686
http://papers.nips.cc/paper/7181-attention-is-all-you-need
http://papers.nips.cc/paper/7181-attention-is-all-you-need


5982

Ronald J. Williams and David Zipser. 1989. A learn-
ing algorithm for continually running fully recurrent
neural networks. Neural Computation, 1(2):270–
280.

Jingjing Xu, Xuancheng Ren, Yi Zhang, Qi Zeng, Xi-
aoyan Cai, and Xu Sun. 2018. A skeleton-based
model for promoting coherence among sentences in
narrative story generation. In EMNLP.

Zichao Yang, Phil Blunsom, Chris Dyer, and Wang
Ling. 2017. Reference-aware language models. In
EMNLP.

Lili Yao, Nanyun Peng, Ralph M. Weischedel, Kevin
Knight, Dongyan Zhao, and Rui Yan. 2018. Plan-
and-write: Towards better automatic storytelling.
CoRR, abs/1811.05701.

Saizheng Zhang, Emily Dinan, Jack Urbanek, Arthur
Szlam, Douwe Kiela, and Jason Weston. 2018. Per-
sonalizing dialogue agents: I have a dog, do you
have pets too? In ACL.

https://doi.org/10.1162/neco.1989.1.2.270
https://doi.org/10.1162/neco.1989.1.2.270
https://doi.org/10.1162/neco.1989.1.2.270
https://aclanthology.info/papers/D18-1462/d18-1462
https://aclanthology.info/papers/D18-1462/d18-1462
https://aclanthology.info/papers/D18-1462/d18-1462
https://aclanthology.info/papers/D17-1197/d17-1197
http://arxiv.org/abs/1811.05701
http://arxiv.org/abs/1811.05701
https://aclanthology.info/papers/P18-1205/p18-1205
https://aclanthology.info/papers/P18-1205/p18-1205
https://aclanthology.info/papers/P18-1205/p18-1205

