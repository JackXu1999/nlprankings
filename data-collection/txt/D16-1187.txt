



















































Deceptive Review Spam Detection via Exploiting Task Relatedness and Unlabeled Data


Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1817–1826,
Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics

Deceptive Review Spam Detection
via Exploiting Task Relatedness and Unlabeled Data

Zhen Hai† Peilin Zhao‡ Peng Cheng§ Peng Yang∗ Xiao-Li Li † Guangxia Li¶
†Institute for Infocomm Research, A*STAR, Singapore, {haiz,xlli}@i2r.a-star.edu.sg

‡Ant Financial, Hangzhou, China, peilin.zpl@alipay.com
§SCSE, Nanyang Technological University, Singapore, pcheng1@ntu.edu.sg

∗Tencent AI Lab, Shenzhen, China, henryppyang@tencent.com
¶SCST, Xidian University, Xi’an, China, gxli@xidian.edu.cn

Abstract

Existing work on detecting deceptive reviews
primarily focuses on feature engineering and
applies off-the-shelf supervised classification
algorithms to the problem. Then, one real
challenge would be to manually recognize
plentiful ground truth spam review data for
model building, which is rather difficult and
often requires domain expertise in practice. In
this paper, we propose to exploit the related-
ness of multiple review spam detection tasks
and readily available unlabeled data to address
the scarcity of labeled opinion spam data.
We first develop a multi-task learning method
based on logistic regression (MTL-LR), which
can boost the learning for a task by sharing
the knowledge contained in the training sig-
nals of other related tasks. To leverage the
unlabeled data, we introduce a graph Lapla-
cian regularizer into each base model. We
then propose a novel semi-supervised multi-
task learning method via Laplacian regular-
ized logistic regression (SMTL-LLR) to fur-
ther improve the review spam detection per-
formance. We also develop a stochastic al-
ternating method to cope with the optimiza-
tion for SMTL-LLR. Experimental results on
real-world review data demonstrate the benefit
of SMTL-LLR over several well-established
baseline methods.

1 Introduction

Nowadays, more and more individuals and organi-
zations have become accustomed to consulting user-
generated reviews before making purchases or on-
line bookings. Considering great commercial ben-

efits, merchants, however, have tried to hire peo-
ple to write undeserving positive reviews to promote
their own products or services, and meanwhile to
post malicious negative reviews to defame those of
their competitors. The fictitious reviews and opin-
ions, which are deliberately created in order to pro-
mote or demote targeted entities, are known as de-
ceptive opinion spam (Jindal and Liu, 2008; Ott et
al., 2011).

By formulating deceptive opinion spam detection
as a classification problem, existing work primarily
focuses on extracting different types of features and
applies off-the-shelf supervised classification algo-
rithms to the problem (Jindal and Liu, 2008; Ott et
al., 2011; Feng et al., 2012; Chen and Chen, 2015).
Then, one weakness of previous work lies in the de-
mand of manually recognizing a large amount of
ground truth review spam data for model training.
Unlike other forms of spamming activities, such as
email or web spam, deceptive opinion spam, which
has been deliberately written to sound authentic, is
more difficult to be recognized by manual read. In
an experiment, three undergraduate students were
(randomly) invited to identify spam reviews from
nonspam ones in hotel domain. As shown in Table 1,
their average accuracy is merely 57.3% (Ott et al.,
2011). Then, given a limited set of labeled review
data for a domain, e.g., hotel, it is almost impossible
to build a robust classification model for detecting
deceptive spam reviews in reality.

In this work, we deal with the problem of de-
tecting a textual review as spam or not, i.e., non-
spam. We consider each deceptive review spam de-
tection problem within each domain, e.g., detecting

1817



Judge-1 Judge-2 Judge-3
Accuracy 61.9% 56.9% 53.1%
F-spam 48.7% 30.3% 43.6%
F-nonspam 69.7% 68.8% 59.9%

Table 1: Performance of human judges for review spam
detection in hotel domain (Ott et al., 2011), where F-
spam/F-nonspam means F-score for spam/nonspam label.

spam hotel/restuarnt reviews from hotel/restaurnat
domain, to be a different task. Previous studies
have empirically shown that learning multiple re-
lated tasks simultaneously can significantly improve
performance relative to learning each task indepen-
dently, especially when only a few labeled data per
task are available (Caruana, 1997; Bakker and Hes-
kes, 2003; Argyriou et al., 2006). Thus, given the
limited labeled review data for each domain, we for-
mulate the review spam detection tasks for multi-
ple domains, e.g., hotel, restaurant, and so on, as a
multi-task learning problem.

We develop a multi-task learning method via lo-
gistic regression (MTL-LR) to address the problem.
One key advantage of the method is that it allows
to boost the learning for one review spam detection
task by leveraging the knowledge contained in the
training signals of other related tasks. Then, there
is often a large quantity of review data freely avail-
able online. In order to leverage the unlabeled data,
we introduce a graph Laplacian regularizer into each
base logistic regression model. We extend MTL-
LR, and propose a novel semi-supervised multi-task
learning model via Laplacian regularized logistic re-
gression (SMTL-LLR) to further boost the review
spam detection performance under the multi-task
learning setting. Moreover, to cope with the opti-
mization problem for SMTL-LLR, we also develop
a stochastic alternating optimization method, which
is computationally efficient.

To the best of our knowledge, this is the first work
that generalizes opinion spam detection from in-
dependent single-task learning to symmetric multi-
task learning setting. By symmetric, we mean that
the setting seeks to improve the performance of all
learning tasks simultaneously. In this sense, it is dif-
ferent from transfer learning (Pan and Yang, 2010),
where the objective is to improve the performance
of a target task using information from source tasks.

Under this new setting, we can exploit the com-
monality shared by related review spam detection
tasks as well as readily available unlabeled data, and
then alleviate the scarcity of labeled spam review
data. Experimental results on real-world review data
demonstrate the superiority of SMTL-LLR over sev-
eral representative baseline methods.

The rest of this paper is organized as follows. Sec-
tion 2 presents related work. Section 3 introduces
the proposed methods and stochastic alternating op-
timization algorithm. Then, in Section 4, we present
the experimental results in detail, and conclude this
paper in Section 5.

2 Related Work

Previous work typically formulates deceptive opin-
ion spam detection as a classification problem, and
then presents different types of features to train su-
pervised classification algorithms for the problem.
Jindal and Liu (2008) first studied opinion spam de-
tection problem. They built the ground truth review
data set by treating the duplicate reviews in a given
corpus as spam reviews and the rest as nonspam re-
views. They presented review, product, and reviewer
related features, and then trained logistic regression
(LR) model on the features for finding fake review
spam. Ott et al. (2011) created the ground truth re-
view data via a crowd-sourcing service called Ama-
zon Mechanical Turk1. They presented three dif-
ferent types of features for opinion spam detection,
i.e., genre identification features, psycholinguistic
deception features, and standard n-gram text fea-
tures. They found that the supervised support vec-
tor machines (SVM) trained on the textual n-gram
features can achieve good performance. Feng et al.
(2012) presented syntactic stylometry features and
trained SVM model for deception detection, while
Chen and Chen (2015) built the SVM classifier on
a diversity of features, such as content and thread
features, for opinion spam detection in web forum.
In addition, Li et al. (2014) employed a feature-
based additive model to explore the general rule for
deceptive opinion spam detection. Generally, in or-
der to build robust supervised review spam detection
models, we have to manually recognize large-scale
ground truth spam data. But this could be very ex-

1https://www.mturk.com

1818



pensive, and often requires domain expertise.
Though a large amount of unlabeled review data

are freely available online, very limited work has
been done on developing semi-supervised methods
for review spam detection. Li et al. (2011) used
a two-view co-training method (Blum and Mitchell,
1998) for semi-supervised learning to identify fake
review spam. One limitation of the work is that it
needs additional reviewer information when build-
ing model. Given a corpus of textual reviews, the
reviewer related view may not be always available
in reality. Moreover, the co-training method is not
intrinsically geared to learning from the unlabeled
review data, instead, simply makes use of the un-
labeled reviews within a fully supervised learning
framework, negating the semi-supervised learning
benefit. For some particular scenarios, the available
training data could be only a partially labeled set of
positive examples, e.g., spam reviews, and a large
set of unlabeled reviews. Positive unlabeled learn-
ing (PU) (De Comite et al., 1999; Liu et al., 2002)
may be then used for deceptive review spam detec-
tion (Hernandez et al., 2013). However, this clearly
contrasts with our problem, where our training data
contains a complete labeled set of positive (spam)
and negative (nonspam) reviews besides the unla-
beled set of review data.

In addition, instead of detecting spam reviews di-
rectly, considerable efforts have been made to recog-
nize review spammers, i.e., online users who have
written spam reviews. Lim et al. (2010) studied
different types of spamming behavioral indicators,
and then used a regression method on the indicators
for finding review spammers. Wang et al. (2012)
investigated the relationships among reviewers, re-
views, and stores, and developed a social review
graph based method to identify online store spam-
mers. Mukherjee et al. (2013) developed an author
spamicity Bayesian model to exploit the observed
behavioral footprints for spammer detection. In re-
ality, a group of online users may work together to
create spam reviews. Mukherjee et al. (2012) de-
veloped a group spam ranking algorithm to detect
spammer groups.

Multi-task learning is a learning paradigm that
seeks to boost generalization performance by learn-
ing a task together with other tasks at the same time
while using a shared representation (Caruana, 1997).

Most majority of existing work on multi-task learn-
ing does not infer actual task relations from train-
ing data automatically, instead, they typically make
the assumptions that the relations are existent or are
given as prior knowledge (Thrun and O’Sullivan,
1996; Bakker and Heskes, 2003; Evgeniou and Pon-
til, 2004; Argyriou et al., 2006; Liu et al., 2009).
To better fit the multi-task learning model to real-
world data, Zhang and Yeung (2010) proposed a
convex regularization formulation named multi-task
relation learning (MTRL), which can learn real rela-
tionships between tasks under a multi-task learning
framework.

In this work, we focus on detecting online decep-
tive review spam. We formulate review spam detec-
tion for multiple domains (e.g., hotel and restaurant)
as a multi-task learning problem. Following the con-
vex framework of MTRL, we first develop a multi-
task learning method via logistic regression (MTL-
LR). We employ logistic regression as base classifi-
cation model, because: 1) It is a robust model that
does not have configuration parameters to tune; 2) It
can be straightforwardly extended, and be efficiently
trained using convex optimization techniques (Hoi
et al., 2006; Minka, 2003); and 3) It has been shown
effective for large-scale text classification and fake
review detection problems (Hoi et al., 2006; Jindal
and Liu, 2008). Then, to leverage the large volume
of unlabeled review data, we extend the base logis-
tic regression model, and incorporate a graph Lapla-
cian regularizer into it. We thus develop a new semi-
supervised multi-task learning paradigm via Lapla-
cian regularized logistic regression, which is able to
further boost the performance for review spam de-
tection.

3 Methodology

3.1 Multi-task Learning via Logistic
Regression

Given m review domains D1, . . . ,Dm, we ac-
cordingly have m review spam detection tasks
T1, . . . , Tm, which share a common feature space
with d dimensions. For the task Ti in the domain
Di, there is a small labeled set of li review examples
Li = {(xi1, yi1), . . . , (xili , y

i
li
)}, where xij ∈ Rd is

the vectorial representation of the review j in the la-
beled set Li, and yij ∈ {+1,−1} refers to the spam

1819



(+1) or nonspam (−1) label of the review.
When there is only one review spam detection

task, for example, Ti, we can use logistic regression
(LR) model to learn a supervised classifier based on
the labeled set Li. The objective function of LR for
single-task learning is

P iLR(wi)

=
1

li

li∑

j=1

ln(1 + exp(−yijw⊤i xij)) +
λ

2
∥wi∥2,

where wi ∈ Rd, λ > 0 refers to regularization pa-
rameter.

Once the model is learned from solving the opti-
mization problem, given a test review instance xj′
of the task Ti, we can employ the model to predict it
as spam, i.e., ŷj′ = 1, with probability

Prob(ŷj′ = 1) =
1

1 + exp(−w⊤i xij′)
.

Now we have m review spam detection tasks for
multiple domains, and we would learn m supervised
classification models simultaneously. To achieve
this, we introduce a covariance matrix Ω to represent
the correlations among the m review spam detection
tasks, where Ωij refers to the relation/covariance be-
tween a pair of tasks Ti and Tj . Since Ω is a task
covariance matrix, we require it to satisfy the con-
straint Ω ≽ 0, i.e., positive semidefinite. We also re-
strict Tr(Ω) = 1 without of loss of generality, since
for any covariance matrix Tr(Σ) ̸= 1, we can use

Σ
Tr(Σ) as Ω. If the covariance matrix is given as prior
knowledge, then we introduce a supervised multi-
task learning (MTL) framework via logistic regres-
sion as follows

PΩMTL(W)

=

m∑

i=1

1

li

li∑

j=1

ln(1 + exp(−yijwTi xij))

+
λ

2
Tr(WWT) +

β

2
Tr(WΩ−1WT ),

where W = (w1, . . . ,wm), and β > 0 is a regular-
ization parameter.

Under this multi-task learning setting, the first
term refers to the sum of all the average empirical

loss, the second term refers to the regularizer used to
avoid over-fitting, and the last term is introduced to
leverage the shared knowledge from multiple learn-
ing tasks according to their relationships.

In reality, the covariance matrix may be not pro-
vided a priori. We then present the following multi-
task learning model, which can learn the model pa-
rameters W and Ω automatically from training re-
view data

PMTL(W,Ω)

=

m∑

i=1

1

li

li∑

j=1

ln(1 + exp(−yijwTi xij))

+
λ

2
Tr(WWT) +

β

2
Tr(WΩ−1WT )

s.t. Ω ≽ 0, T r(Ω) = 1,

If we have only one review spam detection task, i.e.,
m = 1, then it is straightforward to verify that the
above multi-task learning formulation would be re-
duced to single-task objective function of logistic re-
gression.

3.2 Semi-supervised Multi-task Learning via
Laplacian Regularized Logistic Regression

Generally, for a given review domain Di, there
is a large set of unlabeled reviews Ui =
{xili+1, . . . ,x

i
ni} in addition to the labeled review

set Li. Then, for each review spam detection task
Ti, we constracut a weighted neighborhood graph
Gi = (Vi, Ei) based on both labeled and unlabeled
review sets Li and Ui. V refers to the set of data
points, each of which stands for a review example
xij (j : 1, . . . , ni) from either Li or Ui. E refers to
the set of weighted edges. Specifically, if a review
example/point xij is among the K nearest neighbors
of the review point xik, we put an edge linking the
two examples, and vice versa. We also assign an ad-
jacent weight score sijk to the edge, which represents
the similarity or closeness between the two reviews.
Once the neighborhood graph Gi has been built for
each task, a Laplacian regularizer can be then con-
strcted on the graph to extend the regular logistic re-
gression model.

Considering the similarity matrix Si that corre-
sponds to the graph Gi for the task Ti, it is expected
that a good model would also minimize the follow-

1820



ing objective

∑

jk

sijk(w
⊤
i x

i
j −w⊤i xik)2,

This objective implies that w⊤i x
i
j should be close to

w⊤i x
i
k if the similarity s

i
jk is large. The objective

can be simplified as

∑

jk

sijk(w
⊤
i x

i
j −w⊤i xik)2

= Tr(w⊤i Xi(Di − Si)X⊤i wi)
= Tr(w⊤i XiLiX

⊤
i wi),

where Di = diag(Dijj) is a diagonal matrix, D
i
jj =∑

k s
i
jk, and Li = Di−Si refers to the graph Lapla-

cian matrix.
Then, given both labeled review set Li and un-

labeled set Ui for the task Ti, we extend the ba-
sic logistic regression by incorporating the graph
Laplacian regularizer into its learning framework,
and develop a new semi-supervised Laplacian regu-
larized logistic regression (LLR) model. The objec-
tive function of LLR for semi-supervised single-task
learning is given below

P iLLR(wi)

=
1

li

li∑

j=1

ln(1 + exp(−yijw⊤i xij))

+
λ

2
∥wi∥2 +

γ

2
Tr(w⊤i XiLiX

⊤
i wi),

where λ > 0 and γ > 0 are regularization parame-
ters.

The semi-supervised formulation of LLR bal-
ances several desires. The first term is used to min-
imize the loss of the model on the labeled review
data, the second term is used to minimize the com-
plexity of the model, and the last term refers to the
Laplacian regularizer, which is introduced to make
the prediction of the model smooth on the whole re-
view data set.

Next, based on the objective function of the above
LLR model, we extend the supervised multi-task
learning framework, and propose a novel semi-
supervised multi-task learning paradigm via Lapla-
cian regularized logistic regression (SMTL-LLR) as

follows

PSMTL(W,Ω)

=

m∑

i=1

1

li

li∑

j=1

ln(1 + exp(−yijwTi xij))

+
λ

2
Tr(WW⊤) +

β

2
Tr(WΩ−1W⊤)

+
γ

2

m∑

i=1

1

ni
Tr(wTi XiLiX

T
i wi)

s.t., Ω ≽ 0, T r(Ω) = 1.

Under this new semi-supervised unified frame-
work, our proposed SMTL-LLR model can lever-
age the large amount of unlabeled review data in ad-
dition to the labeled ones to learn multiple review
spam detection models simultaneously, and then,
what is learned for one task can help other related
tasks be learned better. In contrast, previous single-
task learning based review spam detection models,
which are trained independently, and are typically
built on a limited set of labeled review data, cannot
benefit from this.

3.3 Stochastic Alternating Method

There are two parameters W and Ω in the objective
function of the proposed SMTL-LLR model. It is
not easy to optimize the objective function against
the two parameters at the same time. We then de-
velop a stochastic alternating method to cope with
the optimization problem for SMTL-LLR, i.e., alter-
natively updating one parameter by fixing the other.
In particular, we initialize W with the values ran-
domly chosen from [0, 1], and initialize Ω as a di-
agonal matrix, where Ωii = 1m . For each iteration,
the key update steps for the two parameters are de-
scribed as follows

• Step 1: Update W while Ω is fixed.

W← argmin
W

PSMTL(W,Ω)

• Step 2: Update Ω while W is fixed.

Ω← argmin
Ω

PSMTL(W,Ω)

1821



3.3.1 Updating W While Fixing Ω
For Step 1 of the alternating optimization method,

we introduce a stochastic gradient descent method
to efficiently update the parameter W, while Ω is
fixed. Formally, given a learning task Ti, we ran-
domly choose a subset or mini-batch of reviews
Aib = {(xij , yij)|j ∈ [li]} from the labeled set Li in
a particular iteration, where [li] denotes {1, . . . , li}
and |Aib| = r ≪ li. Based on the subset of labeled
reviews Aib, we can construct an unbiased estimate
of the objective function

PSMTL(W,Ω, {Aib}mi=1)

=
m∑

i=1

1

r

∑

j∈Aib

ln(1 + exp(−yijwTi xij))

+
λ

2
Tr(WWT) +

β

2
Tr(WΩ−1WT )

+
γ

2

m∑

i=1

1

ni
Tr(wTi XiLiX

T
i wi)

We can then obtain an unbiased stochastic gradi-
ent of the objective

∇WPSMTL(W,Ω, {Aib}mi=1)
= [g1b , . . . ,g

m
b ] + λW + βWΩ

−1

+[γ
1

n1
X1L1X

T
1 w1, . . . , γ

1

nm
XmLmX

T
mwm],

where

gib =
1

r

∑

j∈Aib

−yijxij
1 + exp(yijw

T
i x

i
j)

.

Next, the model parameter W can be updated via
stochastic gradient descent method

Wt+ 1
2

= Wt − ηt∇WPSMTL(W,Ω, {Aib}mi=1)

where ηt > 0 refers to learning rate in iteration t.
Note that, after each update step for the parame-

ter W, we perform a scaling process by forcing the
solution

∥Wt+1∥F ≤
√

2m ln(2)/λ,

and then have the following update rule

Wt+1 = min(1,

√
2m ln(2)/λ

∥Wt+ 1
2
∥F

)Wt+ 1
2
.

We provide a straightforward theoretical analysis,
which shows an upper bound of the norm of the op-
tima solution W∗, and explains why we perform the
above scaling step. Using the fact that

PSMTL(W∗) ≤ PSMTL(0),

we thus have

λ

2
∥W∗∥2F ≤ PSMTL(W∗)
≤ PSMTL(0) = m ln(2).

The fist inequality is guaranteed by

ln(1 + exp(−yijw⊤i xij)) > 0,

T r(WΩ−1W⊤) ≥ 0,

and

Tr(w⊤i XiLiX
⊤
i wi) ≥ 0.

3.3.2 Updating Ω While Fixing W
The second step of the stochastic alternating

method is equivalent to solving the following opti-
mization problem

min
Ω

Tr(WΩ−1W⊤)

s.t., Ω ≽ 0, T r(Ω) = 1.

This convex formulation enjoys the following
closed-form solution (Zhang and Yeung, 2010)

Ω =
(W⊤W)

1
2

Tr((W⊤W)
1
2 )

.

It is obviously observed that Ω models the correla-
tions between each pair of the tasks or the models.

Algorithm 1 summarizes the stochastic alternat-
ing optimization method for SMTL-LLR. Given la-
beled and unlabeled review data for multiple review
domains, we run the algorithm for P alternating
loops. Within each loop p, we update the model pa-
rameter W for T iterations via stochastic gradient
descent method, where B is number of mini-batches;
after that, we update the task covariance matrix Ω
once based on new W. The procedure is performed
iteratively until it is converged. Then, multiple op-
timized review spam detection models and task co-
variance matrix would be learned finally.

1822



Algorithm 1 Stochastic Alternating Method
Input:
Labeled and unlabeled review data for multiple tasks
Initial learning rate η0, hyper-parameter δ
Regularization parameters λ, β, γ
Initialization:
Initialize W with values randomly chosen from [0, 1]
Initialize Ω = diag(1/m, . . . , 1/m)
for p = 1, . . . , P do

W̃1 = W
for t = 1, . . . , T do

Learning rate ηt = η01+η0δt
Randomly shuffle reviews in the training set
for b = 1, . . . , B do

Compute∇WPSMTL(W,Ω, {Aib}mi=1)
Update W̃t+ 12 = W̃t

−ηt∇WPSMTL(W,Ω, {Aib}mi=1)
W̃t+1 = min(1,

√
2m ln(2)/λ

∥W̃
t+ 1

2
∥F

)W̃t+ 12

end for
end for
Update W = W̃T+1

Update Ω = (W
⊤W)

1
2

Tr((W⊤W)
1
2 )

end for
Output: W and Ω

In addition, we also rely on the stochastic alter-
nating method to optimize the proposed MTL-LR
method. Differently, we need to remove all the terms
related to unlabeled data, i.e., discarding the Lapla-
cian regularization term from the objective function
and gradient.

4 Experiments

In this section, we evaluate the proposed multi-
task learning methods MTL-LR and SMTL-LLR
for review spam detection, and demonstrate the im-
proved effectiveness of the methods over other well-
established baselines.

4.1 Data Sets

Due to big challenge in manually recognizing de-
ceptive reviews, there are limited benchmark opin-
ion spam data in this field. We used three ground
truth data sets from the review domains, doctor2,

2https://www.ratemds.com

hotel3, and restaurant4, respectively, to evaluate the
proposed methods, which were created by following
the similar rules used in (Ott et al., 2011). Then, for
each ground truth review data set, we randomly col-
lected a large number of unlabeled reviews (10,000),
which were written about the same entities or do-
main. Table 2 shows some data statistics, where the
last column computes the ratio of labeled reviews to
unlabeled ones.

Spam/Nonspam Unlabeled Ratio
Doctor 200/200 10,000 4.0%
Hotel 300/300 10,000 6.0%
Restaurant 200/200 10,000 4.0%

Table 2: Some statistics of review data sets.

4.2 Experimental Setup

We followed previous work (Mihalcea and Strappa-
rava, 2009; Ott et al., 2011), and leveraged text un-
igram and bigram term-frequency features to train
our models for review spam detection. This problem
setting is quite useful, for example, when user be-
havior data are sparse or even not available in prac-
tical applications.

Supervised classification models, such as logis-
tic regression (LR) and support vector machines
(SVM), have been used to identify fake review
spam (Jindal and Liu, 2008; Ott et al., 2011). We
compared our methods with the two models. Semi-
supervised positive-unlabeled (PU) learning was
employed for review spam detection, then we chose
one representative PU learning method (Liu et al.,
2002) to evaluate our models. We did not compare
our methods with the two-view co-training method,
which was used for fake review detection (Li et
al., 2011), because the reviewer view data are not
available in the ground truth review sets. Instead,
we selected a well-known semi-supervised trans-
ductive SVM (TSVM) (Joachims, 1999) to evaluate
our models. Different from the proposed methods,
we trained each of above baselines in a single do-
main, because they are single-task learning methods.
Moreover, we also compared our methods with one
well-established multi-task learning baseline MTRL
(Zhang and Yeung, 2010), which has not been used

3https://www.tripadvisor.com
4http://www.yelp.com

1823



for review spam detection problem.
It is important to specify appropriate values for

the parameters in the proposed methods. In our
setting, we used the learning rates ηt that asymp-
totically decrease with iteration numbers (Bottou,
2012). Following previous work (Ott et al., 2011;
Chen and Chen, 2015), we conducted five-fold
cross-validation experiments, and determined the
values of the regularization and hyper parameters via
a grid-search method.

4.3 Experimental Results

Table 3 reports the spam and nonspam review detec-
tion accuracy of our methods SMTL-LLR and MTL-
LR against all other baseline methods. In terms
of 5% significance level, the differences between
SMTL-LLR and the baseline methods are consid-
ered to be statistically significant.

Doctor Hotel Restaurant Average
SMTL-LLR 85.4% 88.7% 87.5% 87.2%
MTL-LR 83.1% 86.7% 85.7% 85.2%
MTRL 82.0% 85.4% 84.7% 84.0%
TSVM 80.6% 84.2% 83.8% 82.9%
LR 79.8% 83.5% 83.1% 82.1%
SVM 79.0% 83.5% 82.9% 81.8%
PU 68.5% 75.4% 74.0% 72.6%

Table 3: Spam and nonspam review detection results in
the doctor, hotel, and restaurant review domains.

Under symmetric multi-task learning setting, our
methods SMTL-LLR and MTL-LR outperform all
other baselines for identifying spam reviews from
nonspam ones. MTL-LR achieves the average ac-
curacy of 85.2% across the three domains, which is
3.1% and 3.4% better than LR and SVM trained in
the single task learning setting, and 1.2% higher than
MTRL. Training with a large quantity of unlabeled
review data in addition to labeled ones, SMTL-LLR
improves the performance of MTL-LR, and achieves
the best average accuracy of 87.2% across the do-
mains, which is 3.2% better than that of MTRL, and
is 4.3% better than TSVM, a semi-supervised sin-
gle task learning model. PU gives the worst perfor-
mance, because learning only with partially labeled
positive review data (spam) and unlabeled data may
not generalize as well as other methods.

4.4 Performance versus Unlabeled Data Size
Figure 1 plots SMTL-LLR accuracy versus unla-
beled data sizes from 0 to 10,000, where 0 corre-
sponds to using only labeled data to build the model,
i.e., MTL-LR. Note that we first randomly sampled
2,000 unlabeled reviews to build the first set, and
then created the second set by appending another
randomly selected set of 2,000 reviews to the pre-
vious one. We repeated the process until all the un-
labeled review data sets were created.

Figure 1: Accuracy versus Unlabeled Data Size.

We observed that learning from unlabeled reviews
does help to boost the performance of MTL-LR,
which was trained with labeled data alone. The
performance of SMTL-LLR improves when training
with more and more unlabeled review data. This is
because the useful patterns learned from unlabeled
data perhaps supports SMTL-LLR to generalize bet-
ter. But continuing to learn from much more unla-
beled reviews may even harm the performance. One
explanation is that appending more unlabeled data
may also incur noisy information to learning pro-
cess. Interestingly, the performance of SMTL-LLR
keeps increasing on the doctor domain, when train-
ing with more and more unlabeled reviews up to
10,000. From above observations, we conclude that
an elaborately selected set of high-quality unlabeled
review data may help SMTL-LLR to learn better.

4.5 Task Correlation
Based on the covariance matrix (Ω) learned from the
review spam detection tasks, we obtained the corre-
lation between each pair of tasks for doctor, hotel,
and restaurant domains, as shown in Table 4. The re-
view spam detection tasks are highly correlated with
each other for hotel and restaurant domains (0.772).

1824



This is reasonable due to the large amount of com-
monality shared between the two domains. We can
see that the tasks are also positively correlated be-
tween hotel and doctor, as well as between doctor
and restaurant domains.

Doctor Hotel Restaurant
Doctor 1.0 0.688 0.638
Hotel 0.688 1.0 0.772
Restaurant 0.638 0.772 1.0

Table 4: Task correlations.

4.6 Shared Text Features among Tasks

Table 5 lists top weighted shared text features among
the review spam detection tasks for doctor, hotel,
and restaurant domains. Generally, review spam-
mers demonstrate similar motivations when creat-
ing deceptive review spam, i.e., promoting their own
products/services or defaming those of their com-
petitors. Though different aspects or entities can
be commented on across different domains, we find
that many features or expressions are indeed shared
among the three review domains. As we know,
deceptive reviewers normally write up reviews for
making money, thus they prefer choosing exagger-
ated language in their lies, no matter which domains
they are working with. As shown in the first row
for spam category, they tend to exaggerate their sen-
timents using the words like “definitely”, “sure”,
“highly”, and so on.

In contrast, truthful reviewers contribute reviews
for sharing their true feelings or personal anecdotes.
They are willing to write up detailed factual expe-
riences, for example, about the doctors they visited
or delicious foods they enjoyed. Their reviews thus
tend to contain language patterns in past tense, such
as “went”, “did”, and “took” shown in the second
row.

5 Conclusions

We have coped with the problem of detecting de-
ceptive review spam. Given the limited labeled re-
view data for individual domains, we formulated
it as a multi-task learning problem. We first de-
veloped a multi-task learning method via logistic
regression (MTL-LR), which allows to boost the

Labels Features
Spam staff, friendly,comfortable, really,

right, experience, best, way, amazing,
check, away, staff friendly, definitely,
sure, highly recommend

Nonspam good, just, like, went, did, people,
excellent, took, wonderful, things,
day, fantastic, know, going, nice

Table 5: Top weighted shared text features for
spam/nonspam category across the three review domains.

learning for one task by sharing the knowledge con-
tained in the training signals of other related tasks.
To leverage the unlabeled data, we introduced a
graph Laplacian regularizer into each base model,
and proposed a semi-supervised multi-task learning
model via Laplacian regularized logistic regression
(SMTL-LLR). Moreover, to deal with the optimiza-
tion problem, we developed a stochastic alternating
method. Experimental results on real-world review
data demonstrated the superiority of SMTL-LLR
over several well-established baseline methods.

For future work, we plan to create much more
ground truth review data from other review domains
and different applications like forums or microblogs,
and further test our proposed models for deceptive
opinion spam detection. We also plan to incorporate
our model into a practical opinion mining system, in
this way, more reliable opinion and sentiment anal-
ysis results can be then expected.

References
Andreas Argyriou, Theodoros Evgeniou, and Massimil-

iano Pontil. 2006. Multi-task feature learning. In
Proceedings of the Twentieth Annual Conference on
Neural Information Processing Systems, pages 41–48,
Vancouver, British Columbia, Canada.

Bart Bakker and Tom Heskes. 2003. Task clustering and
gating for bayesian multitask learning. The Journal of
Machine Learning Research, 4:83–99.

Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In Proceed-
ings of the eleventh annual conference on Computa-
tional learning theory, pages 92–100.

Léon Bottou. 1997. Stochastic gradient descent tricks.
In Neural Networks: Tricks of the Trade, pages 421–
436.

Rich Caruana. 1997. Multitask learning. In Machine
Learning, pages 41–75.

1825



Yu-Ren Chen and Hsin-Hsi Chen. 2015. Opinion spam
detection in web forum: A real case study. In Proceed-
ings of the 24th International Conference on World
Wide Web, pages 173–183, Republic and Canton of
Geneva, Switzerland.

Francesco De Comite, Francois Denis, Remi Gilleron,
and Fabien Letouzey. 1999. Positive and Unla-
beled Examples Help Learning. In Proceedings of the
Tenth International Conference on Algorithmic Learn-
ing Theory, Lecture Notes in Artificial Intelligence,
pages 219–230, Tokyo, Japan. Springer Verlag.

Theodoros Evgeniou and Massimiliano Pontil. 2004.
Regularized multi–task learning. In Proceedings of
the Tenth ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, pages 109–
117, New York, NY, USA.

Song Feng, Ritwik Banerjee, and Yejin Choi. 2012. Syn-
tactic stylometry for deception detection. In Proceed-
ings of the 50th Annual Meeting of the Association for
Computational Linguistics: Short Papers - Volume 2,
pages 171–175.

D. Hernandez, R. Guzman, M. Montes-y-Gomez, and P.
Rosso 2013. Using PU-learning to detect deceptive
opinion spam. In Proceedings of the 4th Workshop
on Computational Approaches to Subjectivity, Senti-
ment and Social Media Analysis, pages 38–45, At-
lanta, Georgia, USA.

Steven C. H. Hoi, Rong Jin, and Michael R. Lyu. 2006.
Large-scale text categorization by batch mode active
learning. In Proceedings of the 15th International
Conference on World Wide Web, pages 633–642, New
York, NY, USA.

Nitin Jindal and Bing Liu. 2008. Opinion spam and anal-
ysis. In Proceedings of the International Conference
on Web Search and Data Mining, pages 219–230, Palo
Alto, California, USA.

Thorsten Joachims. 1999. Transductive inference for
text classification using support vector machines. In
Proceedings of the Sixteenth International Conference
on Machine Learning, pages 200–209, San Francisco,
CA, USA.

Fangtao Li, Minlie Huang, Yi Yang, and Xiaoyan Zhu.
2011. Learning to identify review spam. In IJCAI
Proceedings-International Joint Conference on Artifi-
cial Intelligence, page 2488.

Jiwei Li, Myle Ott, Claire Cardie, and Eduard H. Hovy.
2014. Towards a general rule for identifying decep-
tive opinion spam. In Proceedings of the 52nd Annual
Meeting of the Association for Computational Linguis-
tics, pages 1566–1576, Baltimore, MD, USA.

Ee-Peng Lim, Viet-An Nguyen, Nitin Jindal, Bing Liu,
and Hady Wirawan Lauw. 2010. Detecting product

review spammers using rating behaviors. In Proceed-
ings of the 19th ACM International Conference on In-
formation and Knowledge Management, pages 939–
948.

Bing Liu, Wee Sun Lee, Philip S Yu, and Xiaoli Li. 2002.
Partially supervised classification of text documents.
In Proceedings of the Nineteenth International Con-
ference on Machine Learning, pages 387–394.

Jun Liu, Shuiwang Ji, and Jieping Ye. 2009. Multi-task
feature learning via efficient l 2, 1-norm minimization.
In Proceedings of the twenty-fifth conference on uncer-
tainty in artificial intelligence, pages 339–348. AUAI
Press.

Rada Mihalcea and Carlo Strapparava. 2009. The lie
detector: Explorations in the automatic recognition
of deceptive language. In Proceedings of the ACL-
IJCNLP 2009 Conference Short Papers, pages 309–
312, Stroudsburg, PA, USA.

Thomas P. Minka. 2003. A comparison of numerical
optimizers for logistic regression. Technical report,
CMU Technical Report.

Arjun Mukherjee, Bing Liu, and Natalie Glance. 2012.
Spotting fake reviewer groups in consumer reviews. In
Proceedings of the 21st International Conference on
World Wide Web, pages 191–200.

Arjun Mukherjee, Abhinav Kumar, Bing Liu, Junhui
Wang, Meichun Hsu, Malu Castellanos, and Riddhi-
man Ghosh. 2013a. Spotting opinion spammers us-
ing behavioral footprints. In Proceedings of the 19th
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, pages 632–640.

Myle Ott, Yejin Choi, Claire Cardie, and Jeffrey T. Han-
cock. 2011. Finding deceptive opinion spam by any
stretch of the imagination. In Proceedings of the 49th
Annual Meeting of the Association for Computational
Linguistics: Human Language Technologies - Volume
1, pages 309–319, Portland, Oregon.

Sinno Jialin Pan and Qiang Yang. 2010. A survey on
transfer learning. IEEE Transactions on Knowledge
and Data Engineering, 22(10):1345–1359, October.

S. Thrun and J. O’Sullivan. 1996. Discovering struc-
ture in multiple learning tasks: The TC algorithm. In
L. Saitta, editor, Proceedings of the 13th International
Conference on Machine Learning, San Mateo, CA.
Morgen Kaufmann.

Guan Wang, Sihong Xie, Bing Liu, and Philip S. Yu.
2012. Identify online store review spammers via so-
cial review graph. ACM Trans. Intell. Syst. Technol.,
3(4):61:1–61:21, September.

Yu Zhang and Dit-Yan Yeung. 2010. A convex formula-
tion for learning task relationships in multi-task learn-
ing. In Proceedings of the Twenty-Sixth Conference on
Uncertainty in Artificial Intelligence, pages 733–442,
Catalina Island, CA, USA.

1826


