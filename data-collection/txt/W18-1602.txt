



















































Stylistic Variation in Social Media Part-of-Speech Tagging


Proceedings of the 2nd Workshop on Stylistic Variation, pages 11–19
New Orleans, Louisiana, June 5, 2018. c©2018 Association for Computational Linguistics

Stylistic Variation in Social Media Part-of-Speech Tagging

Murali Raghu Babu Balusu and Taha Merghani and Jacob Eisenstein
School of Interactive Computing
Georgia Institute of Technology

Atlanta, GA, USA
{b.murali, tmerghani3, jacobe}@gatech.edu

Abstract

Social media features substantial stylistic vari-
ation, raising new challenges for syntactic
analysis of online writing. However, this vari-
ation is often aligned with author attributes
such as age, gender, and geography, as well
as more readily-available social network meta-
data. In this paper, we report new evidence
on the link between language and social net-
works in the task of part-of-speech tagging.
We find that tagger error rates are correlated
with network structure, with high accuracy in
some parts of the network, and lower accu-
racy elsewhere. As a result, tagger accuracy
depends on training from a balanced sample
of the network, rather than training on texts
from a narrow subcommunity. We also de-
scribe our attempts to add robustness to stylis-
tic variation, by building a mixture-of-experts
model in which each expert is associated with
a region of the social network. While prior
work found that similar approaches yield per-
formance improvements in sentiment analysis
and entity linking, we were unable to obtain
performance improvements in part-of-speech
tagging, despite strong evidence for the link
between part-of-speech error rates and social
network structure.

1 Introduction

Social media feature greater diversity than the for-
mal genres that constitute classic datasets such as
the Penn Treebank (Marcus et al., 1993) and the
Brown Corpus (Francis and Kucera, 1982): there
are more authors, more kinds of authors, more var-
ied communicative settings, fewer rules, and more
stylistic variation (Baldwin et al., 2013; Eisen-
stein, 2013). Previous work has demonstrated
precipitous declines in the performance of state-
of-the-art systems for core tasks such as part-of-
speech tagging (Gimpel et al., 2011) and named-
entity recognition (Ritter et al., 2010) when these

systems are applied to social media text, and
stylistic diversity seems the likely culprit. How-
ever, we still lack quantitative evidence of the role
played by language variation in the performance
of NLP systems in social media, and existing so-
lutions to this problem are piecemeal at best. In
this paper, we attempt to address both issues: we
quantify the impact of one form of sociolinguistic
variation on part-of-speech tagging accuracy, and
we design a model that attempts to adapt to this
variation.

Our contribution focuses on the impact of lan-
guage variation that is aligned with one or more
social networks among authors on the microblog-
ging platform Twitter. We choose Twitter because
language styles in this platform are particularly di-
verse (Eisenstein et al., 2010), and because mod-
erately large labeled datasets are available (Gim-
pel et al., 2011; Owoputi et al., 2013). We choose
social networks for several reasons. First, they
can readily be obtained from both metadata and
behavioral traces on multiple social media plat-
forms (Huberman et al., 2008). Second, social net-
works are strongly correlated with “demographic”
author-level variables such as age (Rosenthal and
McKeown, 2011), gender (Eckert and McConnell-
Ginet, 2003), race (Green, 2002), and geogra-
phy (Trudgill, 1974), thanks to the phenomenon
of homophily, also known as assortative mix-
ing (McPherson et al., 2001; Al Zamal et al.,
2012). These demographic variables are in turn
closely linked to language variation in American
English (Wolfram and Schilling-Estes, 2005), and
have been shown to improve some document clas-
sification tasks (Hovy, 2015). Third, there is grow-
ing evidence of the strong relationship between
social network structures and language variation,
even beyond the extent to which the social network
acts as a proxy for demographic attributes (Milroy,
1991; Dodsworth, 2017).

11



To measure the impact of socially-linked lan-
guage variation, we focus on part-of-speech tag-
ging, a fundamental task for syntactic analysis.
First, we measure the extent to which tagger per-
formance is correlated with network structure,
finding that tagger performance on friends is sig-
nificantly more correlated than would be expected
by chance. We then design alternative training and
test splits that are aligned with network structure,
and find that test set performance decreases in this
scenario, which corresponds to domain adaptation
across social network communities. This speaks
to the importance of covering all relevant social
network communities in training data.

We then consider how to address the prob-
lem of language variation, by building social
awareness into a recurrent neural tagging model.
Our modeling approach is inspired by Yang and
Eisenstein (2017), who train a mixture-of-experts
for sentiment analysis, where the expert weights
are computed from social network node embed-
dings. But while prior work demonstrated im-
provements in sentiment analysis and information
extraction (Yang et al., 2016), this approach does
not yield any gains on part-of-speech tagging. We
conclude the paper by briefly considering possi-
ble reasons for this discrepancy, and propose ap-
proaches for future work in social adaptation of
syntactic analysis.1

2 Data

We use the corrected2 OCT27 dataset from Gim-
pel et al. (2011) and Owoputi et al. (2013) as our
training set, which contains part-of-speech anno-
tations for 1,827 tweets sampled from Oct 27-28,
2010. We use the train and dev splits of OCT27
as our training dataset and the test split of OCT27
dataset as our validation dataset. The DAILY547
dataset from Owoputi et al. (2013) which has 547
tweets is used for evaluation. Table 2 specifies the
number of tweets and tokens in each dataset. The
tagset for this dataset is explained in Owoputi et al.
(2013); it differs significantly from the Penn Tree-
bank and Universal Dependencies tagsets.

In September 2017, we extracted author IDs
for each of the tweets and constructed three au-
thor social networks based on the follow, mention,
and retweet relations between the authors in the

1Code for rerunning the experiments is available here:
https://github.com/bmurali1994/socialnets postagging

2Owoputi et al. corrected inconsistencies in the ground
labeling of that/this in 100 (about 0.4%) total labels.

Dataset #Msg. #Tok.

OCT27 1,827 26,594
DAILY547 547 7,707

Table 1: Annotated datasets: number of messages
and tokens

Network #Authors #Nodes #Edges

Follow 1,280 905,751 1,239,358
Mention 1,217 384,190 623,754
Retweet 1,154 182,390 314,381

Table 2: Statistics for each social network

dataset, which we refer to as follow, mention and
retweet networks in Table 2. Specifically, we use
the Twitter API to crawl the friends of the OCT27
and DAILY547 users (individuals that they follow)
and the most recent 3,200 tweets in their timelines.
The mention and retweet links are then extracted
from the tweet text and metadata. Table 2 specifies
the total number of authors (whose tweets exist
in our dataset) in each network, the total number
of nodes and the total number of relations among
these nodes. We treat all social networks as undi-
rected graphs, where two users are socially con-
nected if there exists at least one social relation
between them. Several authors of the tweets can
no longer be queried from Twitter, possibly be-
cause their accounts have been deleted. They are
not included in the network, but their tweets are
still used for training and evaluation.

3 Linguistic Homophily

The hypothesis of linguistic homophily is that
socially connected individuals tend to use lan-
guage similarly, as compared to randomly selected
pairs of individuals who are not socially con-
nected (Yang and Eisenstein, 2017). We now de-
scribe two pilot studies that test this hypothesis.

3.1 Assortativity
We test whether errors in POS tagging are assorta-
tive on the social networks defined in the previous
section: that is, if two individuals (i, j) are con-
nected in the network, then a model’s error on the
tweets of author i suggests that the errors on the
tweets of author j are more likely. To measure as-
sortativity, we compute the average difference in
the tagger’s per-token accuracy on tweets for au-

12



(a) Most-common-tag baseline tagger

(b) Pre-trained MEMM tagger from Owoputi et al. (2013).

Figure 1: Average of the squared difference in tagging accuracy on observed (red) and randomized net-
works (blue).

thors i and j, averaged over all connected pairs in
the network. This measures whether classification
errors are related on the network structure.

We compare the observed assortativity against
the assortativity in a network that has been ran-
domly rewired. Each rewiring epoch involves a
number of random rewiring operations equal to the
total number of edges in the network. The edges
are randomly selected, so a given edge may not be
rewired in each epoch; furthermore, the degree of
each node is preserved throughout. If the squared
difference in accuracy is lower for the observed
networks than for their rewired counterparts, this
would indicate that tagger accuracy is correlated
with network structure. Figure 2 explains the met-
ric and rewiring briefly through an example.

We compute the assortativity for three taggers:

• We first use a naı̈ve tagger, which predicts
the most common tag seen during training if
the word exists in the vocabulary, and oth-
erwise predicts the the most common tag
for an unseen word. Preprocessing of each
tweet involves lowercasing, normalizing all
@-mentions to 〈@MENTION〉, and nor-
malizing URLs and email addresses to a com-
mon token (e.g. http : //bit.ly/dP8rR8 ⇒
〈URL〉).

Figure 2: Toy example: differences in tagging ac-
curacy on original and randomly-rewired network.

• We train a lexical, feature-rich CRF model.
Lexical features in the CRF model include
the word, previous two words, next two
words, prefixes and suffixes of the previ-
ous two, current and next two words, and
flags for special characters like hyphen, at-
mention, hashtag, hyphen and digits in the
current word.

• Finally, we repeat these experiments with the
pretrained maximum entropy Markov model
(MEMM) tagger from (Owoputi et al., 2013),
trained on OCT27 tweets.

Figure 1 shows the results for the naı̈ve tagger
and the MEMM tagger; the results were similar

13



for the CRF were similar. Tagger accuracy is well
correlated with network structure in the mention
and retweet graphs, consistent with the hypothe-
sis of linguistic homophily. These findings sup-
port prior work suggesting that “behavioral” social
networks such as mentions and retweets are more
meaningful than “articulated” networks like the
follower graph (Huberman et al., 2008; Puniyani
et al., 2010).

3.2 Clustering

Next, we examine whether linguistic homophily
can lead to mismatches between the test and train-
ing data. We embed each author’s social network
position into a vector representation of dimension
Dv, using the LINE method for social network
node embedding (Tang et al., 2015). These em-
beddings are obtained solely from the social net-
work, and not from the text.

We obtain Dv = 50-dimensional node embed-
dings, and apply k-means clustering (Hartigan and
Wong, 1979) to obtain two sets of authors (train
and test). By design, the training and test sets will
be in different regions of the network, so train-
ing and test authors will be unlikely to be socially
connected. We then train the lexical CRF tagger
on the training set, and apply it to the test set.
The same setup is then applied to a randomly-
selected training/test split, in which the social net-
work structure is ignored. This comparison is il-
lustrated in Figure 3. We repeat this experiment
for 10 times for all three social networks: follow,
mention and retweet.

The theory of linguistic homophily implies that
the test set performance should be worse in the
case that the test set and training sets are drawn
from different parts of the network, since the lin-
guistic style in the training set will not match the
test data. In contrast, when the training and test
sets are drawn in a manner that is agnostic to net-
work structure, the training and test sets are ex-
pected to be more linguistically similar, and there-
fore, test set performance should be better. As
shown in Table 3, the results support the theory:
predictive accuracy is higher when the test and
training sets are not drawn from different parts of
the network.

4 Adapting to socially-linked variation

In this section, we describe a neural network
method that leverages social network informa-

Network Network clusters Random

Follow 82.01% 83.83%
Mention 81.40% 83.07%
Retweet 81.01% 83.52%

Table 3: Comparison of tagger accuracy using
network-based and random training/test splits

tion to improve part-of-speech tagging. We em-
ploy the Social Attention neural network architec-
ture, where the system prediction is the weighted
combination of the outputs of several basis mod-
els (Yang and Eisenstein, 2017). We encour-
age each basis model to focus on a local re-
gion of the social network, so that classification
on socially connected individuals employs simi-
lar model combinations. This allows sharing of
strength for some similar properties between these
network components.

In this architecture, each prediction is the
weighted combination of the outputs of several
basis models. Given a set of labeled instances
{xi,yi} and authors {ai}, the goal of personalized
probabilistic classification is to estimate a condi-
tional label distribution p(y | x, a). We condition
on the author a by modeling the conditional label
distribution as a mixture over the posterior distri-
butions of K basis taggers,

p(y | x, a) =
K∑

k=1

πa,k × pk(y | x) (1)

The basis taggers pk(y | x) can be arbitrary
conditional distributions. We use a hierarchical
recurrent neural network model, in addition to
a tag dictionary and Brown cluster surface fea-
tures (Brown et al., 1992), which we describe in
more detail in § 4.2. The component weighting
distribution πa,k is conditioned on the social net-
work G, and functions as an attentional mecha-
nism, described in § 4.1. The main idea is that for
a pair of authors ai and aj who are nearby in the
social network G, the prediction rules should be-
have similarly if the attentional distributions are
similar, i.e., πai,k ≈ πaj ,k. If we have labeled
training data for ai and wish to make predictions
on author aj , some of the personalization from ai
will be shared by aj . The overall classification ap-
proach can be viewed as a mixture of experts (Ja-
cobs et al., 1991), leveraging the social network

14



Figure 3: (left) Network-aligned train/test split and (right) random train/test split

as side information to choose the distribution over
experts for each author.

4.1 Social Attention Model

The goal of the social attention model is to assign
similar basis weights to authors who are nearby
in the social network G. We operationalize so-
cial proximity by embedding each author’s so-
cial network position into a vector representation,
again using the LINE method for node embed-
ding (Tang et al., 2015). The resulting embeddings
va are treated as fixed parameters in a probabilis-
tic model over edges in the social network. These
embeddings are learned solely from the social net-
work G, without leveraging any textual informa-
tion. The attentional weights are then computed
from the embeddings using a softmax layer,

πa,k =
exp(φk · va + bk)∑K
k′ exp(φk′ · va + bk′ )

. (2)

The parameters φk and bk are learned in the
model. We observed that almost 50% of the au-
thors in our dataset do not appear in any social
network. For all these authors, we use the same
embedding v′ to let the model learn the proportion
weight of the individual basis models in the en-
semble. This embedding v′ is also learned as a pa-
rameter in the model. We have also tried comput-
ing the attentional weights using a sigmoid func-
tion,

πa,k = σ(φk · va + bk), (3)

so that πa is not normalized, but the results were
quite similar.

4.2 Modeling Surface Features

We use surface-level features in addition to the
basis models to improve the performance of our
model closer to the state-of-the-art results. Specif-
ically, we use the tag dictionary features and the

Brown cluster features as described by Gimpel
et al. (2011).

Since Brown clusters are hierarchical in a bi-
nary tree, each word is associated with a tree path
represented as a bitstring with length ≤ 16; we
use prefixes of the bitstring as features (for all pre-
fix lengths ∈ {2, 4, 6, ..., 16}). Concatenating the
Brown cluster features of the previous and next to-
ken along with the current token helped improve
the performance of the baseline model.

We also used the tag dictionary features from
Gimpel et al. (2011), by adding features for a
word’s most frequent part-of-speech tags from
Penn Treebank and Universal Dependencies. This
also helped improve the performance of the base-
line model. We found these surface features to be
vital. Nonetheless, we were not able to match the
performance of the state-of-the-art systems.

4.3 POS tagging with Hierarchical LSTMs

We next describe the baseline model: pk(y | x).
The baseline model is a word-level bi-LSTM, with
a character-level bi-LSTM to compute the embed-
dings of the words (Ling et al., 2015). In addi-
tion to the embeddings from the character level bi-
LSTM, we also learn the word embeddings which
are initialized randomly and also use fixed pre-
trained GloVe Twitter (Pennington et al., 2014)
embeddings for the word-level bi-LSTM. The fi-
nal input to the word-level LSTM is the concate-
nation of the embedding from the character level,
learned word embedding and the fixed pretrained
word embedding. The final hidden state for each
word hi is obtained and concatenated with the sur-
face features for each word ski , and the result is
passed through a fully connected neural network,
giving a latent representation rki . The conditional
probability is then computed as,

pk(yi = t | xi) =
exp(βt · rki + ct)∑
t′ exp(βt′ · rki + ct′ )

. (4)

15



4.4 Loss Function and Training

We train the ensemble model by minimizing the
negative log likelihood of the tags for all the to-
kens in all the tweets in the training dataset.

Alternative objectives We have also tried train-
ing the model using a hinge loss, but the results
were similar and hence excluded in the paper.
We also explored a variational autoencoder (VAE)
framework (Kingma and Welling, 2014), in which
the node embeddings were modeled with a latent
vector z, which was used both to control the mix-
ture weights πk, and to reconstruct the node em-
beddings. Again, results were similar to those ob-
tained with the simpler negative log-likelihood ob-
jective.

Training problems One potential problem with
this framework is that after initialization, a small
number of basis models may claim most of the
mixture weights for all the users, while other basis
models are inactive. This can occur because some
basis models may be initialized with parameters
that are globally superior. As a result, the “dead”
basis models will receive near-zero gradient up-
dates, and therefore can never improve. Care-
ful initialization of the parameters φk and bk and
using L2-regularization parameters of the model
helped mitigate the issue to some extent. Using
the attentional weights computed using the sig-
moid function as described in Equation 3 does not
have this problem, but the final evaluation results
were quite similar to the model with attentional
weights computed using softmax as mentioned in
Equation 2.

5 Experiments

Our evaluation focuses on the DAILY547
dataset (Owoputi et al., 2013). We train our
system on the train and dev splits of the OCT27
dataset (Gimpel et al., 2011) and use the test
split of OCT27 as our validation dataset and
evaluate on the DAILY547 dataset. Accuracy of
the tokens is our evaluation metric for the model.
We compare our results to our baseline model
and the state of the art results on the Twitter
OCT27+Daily547 dataset.

5.1 Experimental Settings

We use 100-dimensional pretrained Twitter GloVe
embeddings (Pennington et al., 2014) which are

System Accuracy

Owoputi et. al. 92.80%
BiLSTM tagger 90.50%
Ensemble of BiLSTM taggers 90.11%
BiLSTM taggers with social attention 89.80%

Table 4: Accuracy of the models on the DAILY547
dataset. The best results are in bold.

Network Accuracy

Follow 89.42%
Mention 89.80%
Retweet 89.65%

Table 5: Accuracy of the social attention model,
across each of the three networks.

trained on about two billion tweets. We use one-
layer for both the character-level and the word-
level bi-LSTM model with hidden state sizes of 50
and 150 dimensions respectively. The dimensions
of character embeddings is set to be 30 and the
learned word embeddings is 50. We use tanh acti-
vation functions all throughout the model and use
Xavier initialization (Glorot and Bengio, 2010)
for the parameters. The model is trained with
ADAM optimizer (Kingma and Ba, 2014) on L2-
regularized negative log-likelihood. The regular-
ization strength was set to 0.01, and the dropout
was set to 0.35. The best hyper-parameters for the
number of basis classifiers isK = 3 for the follow
and mention networks, and K = 4 for the retweet
network.

5.2 Results and Discussion

Table 4 summarizes the main empirical findings,
where we report results from author embeddings
trained on the mention network for Social Atten-
tion. The results of different social networks with
Social Attention is shown in Table 5.

We also evaluate the performance of the trained
Social Attention model on the subset of authors
who can be located in the social network. The ac-
curacy on these authors is similar to the overall
performance on the full dataset. We also observe
the attention distributions of the authors in the so-
cial network on the basis models in the ensemble.
For every pair of authors ai and aj connected in
the social network we compute Σk|πai,k − πaj ,k|
and average it across all pairs in the network. This

16



Network Actual Network Random

Follow 0.90 1.10
Mention 0.38 1.06
Retweet 0.36 0.68

Table 6: Comparison of the mean absolute dif-
ference in attention distributions of connected au-
thors in actual social networks versus randomly
rewired networks.

is compared with against a randomly rewired net-
work. If this value is lower for the social net-
work, then this indicates that the connected au-
thors tend to have similar attentional distributions
as explained in § 4. The results are presented in
Table 6. These results clearly indicate that the au-
thors who are connected in the social network tend
to have similar attentional distributions.

While the analyses in § 3 indicated a strong de-
gree of linguistic homophily, we do not observe
any significant gain in performance. We think the
following factors played an important role:

Missing authors. There are a large number of
missing authors in each of the social network
(about 50% of the authors of the tweets in
the dataset). The results from combining all
the three social networks by just concatenat-
ing this embeddings did not help either in our
experiments.

Tweets per author. We have only one tweet for
every author in our dataset and this makes it
harder for the model to extract relations be-
tween authors and their tweets.

Dataset size. The dataset contains only 2374
tweets, which could be the reason our deep
learning model is still behind the feature-rich
Markov Model of Owoputi et al. (2013) by
about 2%.

Sparse social networks. The social networks
that we constructed using the twitter IDs
from the tweet metadata of the OCT27 and
DAILY547 datasets were very sparse, and
the node degree distributions (number of
edges per node) have high variance.

6 Related Work

Previous problems on incorporating social rela-
tions have focused on sentiment analysis and en-

tity linking, where the existence of social relations
between users is considered as a clue that the sen-
timent polarities in the messages from the users
should be similar or the entities that they refer to
in their messages are the same. Speriosu et al.
(2011) constructs a heterogeneous network with
tweets, users, and n-grams as nodes, and the senti-
ment label distributions associated with the nodes
are refined by performing label propagation over
social relations. Tan et al. (2011) and Hu et al.
(2013) leverage social relations for sentiment anal-
ysis by exploiting a factor graph model and the
graph Laplacian technique respectively, so that the
tweets belonging to social connected users share
similar label distributions. Yang et al. (2016) pro-
posed a neural based structured learning architec-
ture for tweet entity linking, leveraging the ten-
dency of socially linked individuals to share simi-
lar interests on named entities — the phenomenon
of entity homophily. Yang and Eisenstein (2017)
proposed a middle ground between group-level
demographic characteristics and personalization,
by exploiting social network structure. We extend
this work by applying it for the first time to syn-
tactic analysis.

7 Conclusion

This paper describes the hypothesis of linguistic
homophily specifically linked to stylistic variation
on social media data and tests the effectiveness
of social attention to overcome language varia-
tion, leveraging the tendency of socially proxi-
mate individuals to use language similarly for POS
tagging. While our preliminary analyses demon-
strate a strong correlation between tagging accu-
racy and network structure, we are unable to lever-
age these correlations for improvements in tagging
accuracy.

How should we reconcile these conflicting re-
sults? In the limit of infinite resources, we could
train separate taggers for separate treebanks, fea-
turing each language variety. But even if language
variation is strongly associated with the network
structure, the effectiveness of this approach would
still be limited by the inherent difficulty of tagging
each language variety. In other words, augment-
ing the tagger with social network metadata may
not help much, because some parts of the network
may simply be harder to tag than others. However,
this pessimistic conclusion must be offset by not-
ing the small size of existing annotated datasets for

17



social media writing, which are orders of magni-
tude smaller than comparable corpora of newstext.
While some online varieties maybe hard to tag
well, it is equally possible that the advantages of
more flexible modeling frameworks only become
visible when there is sufficient data to accurately
estimate them. We are particularly interested to
explore the utility of semi-supervised techniques
for training such models in future work.

References
Faiyaz Al Zamal, Wendy Liu, and Derek Ruths. 2012.

Homophily and latent attribute inference: Inferring
latent attributes of Twitter users from neighbors. In
Proceedings of the International Conference on Web
and Social Media (ICWSM). pages 387–390.

Timothy Baldwin, Paul Cook, Marco Lui, Andrew
MacKinlay, and Li Wang. 2013. How noisy social
media text, how diffrnt social media sources. In Pro-
ceedings of the 6th International Joint Conference
on Natural Language Processing (IJCNLP 2013).
pages 356–364.

Peter F Brown, Peter V Desouza, Robert L Mercer,
Vincent J Della Pietra, and Jenifer C Lai. 1992.
Class-based n-gram models of natural language.
Computational linguistics 18(4):467–479.

Robin Dodsworth. 2017. Migration and dialect con-
tact. Annual Review of Linguistics 3(1):331–346.

P. Eckert and S. McConnell-Ginet. 2003. Language
and Gender. Cambridge Textbooks in Linguistics.
Cambridge University Press.

Jacob Eisenstein. 2013. What to do about bad language
on the internet. In Proceedings of the North Amer-
ican Chapter of the Association for Computational
Linguistics (NAACL). pages 359–369.

Jacob Eisenstein, Brendan O’Connor, Noah A. Smith,
and Eric P. Xing. 2010. A latent variable model for
geographic lexical variation. In Proceedings of Em-
pirical Methods for Natural Language Processing
(EMNLP). pages 1277–1287.

W Francis and Henry Kucera. 1982. Frequency analy-
sis of English usage. Houghton Mifflin Company.

Kevin Gimpel, Nathan Schneider, Brendan O’Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A Smith. 2011. Part-of-speech tagging for
twitter: Annotation, features, and experiments. ACL
pages 42–47.

Xavier Glorot and Yoshua Bengio. 2010. Understand-
ing the difficulty of training deep feedforward neural
networks. In Yee Whye Teh and Mike Titterington,
editors, Proceedings of the Thirteenth International
Conference on Artificial Intelligence and Statistics.

PMLR, Chia Laguna Resort, Sardinia, Italy, vol-
ume 9 of Proceedings of Machine Learning Re-
search, pages 249–256.

L.J. Green. 2002. African American English: A Lin-
guistic Introduction. Cambridge University Press.

J. A. Hartigan and M. A. Wong. 1979. A k-means
clustering algorithm. JSTOR: Applied Statistics
28(1):100–108.

Dirk Hovy. 2015. Demographic factors improve clas-
sification performance. ACL pages 752–762.

Xia Hu, Lei Tang, Jiliang Tang, and Huan Liu. 2013.
Exploiting social relations for sentiment analysis in
microblogging. In Proceedings of the Sixth ACM
International Conference on Web Search and Data
Mining. ACM, New York, NY, USA, WSDM ’13,
pages 537–546.

Bernardo A. Huberman, Daniel M. Romero, and Fang
Wu. 2008. Social networks that matter: Twitter un-
der the microscope. CoRR abs/0812.1045.

Robert A. Jacobs, Michael I. Jordan, Steven J. Nowlan,
and Geoffrey E. Hinton. 1991. Adaptive mixtures of
local experts. Neural Comput. 3(1):79–87.

Diederik P. Kingma and Jimmy Ba. 2014. Adam:
A method for stochastic optimization. CoRR
abs/1412.6980.

Diederik P Kingma and Max Welling. 2014. Auto-
encoding variational bayes. In Proceedings of the
International Conference on Learning Representa-
tions (ICLR).

Wang Ling, Chris Dyer, Alan W Black, Isabel Tran-
coso, Ramon Fermandez, Silvio Amir, Luis Marujo,
and Tiago Luis. 2015. Finding function in form:
Compositional character models for open vocab-
ulary word representation. Proceedings of Em-
pirical Methods for Natural Language Processing
(EMNLP) pages 1520–1530.

Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics 19(2):313–330.

Miller McPherson, Lynn Smith-Lovin, and James M
Cook. 2001. Birds of a feather: Homophily in social
networks. Annual review of sociology 27(1):415–
444.

Lesley Milroy. 1991. Language and Social Networks.
Wiley-Blackwell, 2 edition.

Olutobi Owoputi, Brendan O’Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. Pro-
ceedings of North American Association for Compu-
tational Linguistics (NAACL) .

18



Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors
for word representation. In Proceedings of Em-
pirical Methods for Natural Language Processing
(EMNLP). pages 1532–1543.

Kriti Puniyani, Jacob Eisenstein, Shay Cohen, and
Eric P. Xing. 2010. Social links from latent topics
in microblogs. In Proceedings of NAACL Workshop
on Social Media. Los Angeles.

Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Un-
supervised modeling of twitter conversations. In
Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the
Association for Computational Linguistics. Associ-
ation for Computational Linguistics, Stroudsburg,
PA, USA, HLT ’10, pages 172–180.

Sara Rosenthal and Kathleen McKeown. 2011. Age
prediction in blogs: A study of style, content, and
online behavior in pre-and post-social media genera-
tions. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies-Volume 1. Association
for Computational Linguistics, pages 763–772.

Michael Speriosu, Nikita Sudan, Sid Upadhyay, and Ja-
son Baldridge. 2011. Twitter polarity classification
with label propagation over lexical links and the fol-
lower graph. In Proceedings of the First Workshop
on Unsupervised Learning in NLP. Association for
Computational Linguistics, Stroudsburg, PA, USA,
EMNLP ’11, pages 53–63.

Chenhao Tan, Lillian Lee, Jie Tang, Long Jiang, Ming
Zhou, and Ping Li. 2011. User-level sentiment anal-
ysis incorporating social networks. In Proceedings
of the 17th ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining. ACM,
New York, NY, USA, KDD ’11, pages 1397–1405.

Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun
Yan, and Qiaozhu Mei. 2015. Line: Large-scale in-
formation network embedding. In WWW. ACM.

Peter Trudgill. 1974. Linguistic change and diffusion:
description and explanation in sociolinguistic dialect
geography. Language in Society 3(2):215246.

Walt Wolfram and Natalie Schilling-Estes. 2005.
American English: dialects and variation. Wiley-
Blackwell, second edition.

Yi Yang, Ming-Wei Chang, and Jacob Eisenstein.
2016. Toward socially-infused information extrac-
tion: Embedding authors, mentions, and entities. In
Proceedings of Empirical Methods for Natural Lan-
guage Processing (EMNLP).

Yi Yang and Jacob Eisenstein. 2017. Overcoming lan-
guage variation in sentiment analysis with social at-
tention. Transactions of the Association for Compu-
tational Linguistics (TACL) 5.

19


