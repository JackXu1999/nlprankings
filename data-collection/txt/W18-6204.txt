
























































Language Independent Sentiment Analysis with Sentiment-Specific Word Embeddings


Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 14–23
Brussels, Belgium, October 31, 2018. c©2018 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17

14

Language Independent Sentiment Analysis with Sentiment-Specific Word
Embeddings

Carl Saroufim, Akram Almatarky, Mohamed Abdel Hady
Microsoft Corporation, Redmond WA

{casarouf, akrgab, mohabdel}@microsoft.com

Abstract

Data annotation is a critical step to train a
text model but it is tedious, expensive and
time-consuming. We present a language in-
dependent method to train a sentiment polar-
ity model with limited amount of manually-
labeled data. Word embeddings such as
Word2Vec are efficient at incorporating se-
mantic and syntactic properties of words,
yielding good results for document classifica-
tion. However, these embeddings might map
words with opposite polarities, to vectors close
to each other. We train Sentiment Specific
Word Embeddings (SSWE) on top of an unsu-
pervised Word2Vec model, using either Recur-
rent Neural Networks (RNN) or Convolutional
Neural Networks (CNN) on data auto-labeled
as “Positive” or “Negative”. For this task, we
rely on the universality of emojis and emoti-
cons to auto-label a large number of French
tweets using a small set of positive and neg-
ative emojis and emoticons. Finally, we ap-
ply a transfer learning approach to refine the
network weights with a small-size manually-
labeled training data set. Experiments are con-
ducted to evaluate the performance of this ap-
proach on French sentiment classification us-
ing benchmark data sets from SemEval 2016
competition. We were able to achieve a per-
formance improvement by using SSWE over
Word2Vec. We also used a graph-based ap-
proach for label propagation to auto-generate
a sentiment lexicon.

1 Introduction

Text sentiment analysis is defined as the compu-
tational study of documents, sentences or phrases
(aspect level), to detect opinions, sentiments, emo-
tions, etc. It is particularly useful for companies
to collect feedback about their products, analyze
the public opinion about their brand, for politi-
cal parties to monitor the population support, etc.
Document-level sentiment analysis corresponds to
assigning an overall sentiment polarity to a docu-

ment. It can be formulated as a two-class classifi-
cation problem: positive or negative, excluding the
neutral case of documents with no polarity (Zhang
et al., 2018).

For this task, both supervised and unsuper-
vised learning approaches have been used. Super-
vised learning methods typically use bag-of-words
(which ignores word orders and semantics and suf-
fers from high dimensionality and sparsity), or,
more recently, word embeddings, which requires
unsupervised training on a big corpus of data. It
provides a mapping of words to dense vectors
of fixed length, encoding semantic and syntactic
properties of those words. The document can then
be represented by the average embedding vector
of the words it contains. Language-dependent fea-
tures such as Part of Speech, grammatical analysis,
lexicons of opinions and emotions have also been
applied successfully (Zhang et al., 2018).

While the use of standard word embedding
techniques such as Word2Vec (Mikolov et al.,
2013) or C&W (Collobert et al., 2011) can en-
hance the performance of prediction models and
perform well on general classification task, sen-
timent is not properly encoded in those vectors.
According to Tang (2014), words such as “good”
and “bad” might be close in the embeddings
space, although they have opposite polarities, be-
cause of similar usage and grammatical rules. We
show in this paper that training Sentiment Specific
Word Embeddings (SSWE) by updating an initial
Word2Vec model trained on a big corpus of tweets
provides better word embeddings for the task of
sentiment classification. SSWE training is per-
formed by updating word embeddings as part of
a supervised deep learning framework, by train-
ing a model on sentiment-labeled data. Through
backpropagation, the weights of the Embedding
Layer will be adjusted and incorporate sentiment.
At the end of the training, the embedding matrix
can be extracted from the model and will be used



15

to featurize words and documents. As a result, this
process needs a big amount of data labeled with
“Positive” and “Negative”. While labeled data sets
and sentiment lexicons exist in English and can
be used to label a big number of documents to
build the SSWE (e.g. SentiWordNet or ANEW in
English as lexicons), they are scarce in other lan-
guages (Pak and Paroubek, 2011). Hence the need
to find a systematic way of labeling a big number
of documents without any language knowledge,
then let prediction models “learn” from them.

One way to do it, is to rely on a universal opin-
ion lexicon that would hold true in any language.
With the rise of social media, the widespread use
of emojis1 provide us with such a tool. In particu-
lar, Twitter is one of the biggest online social me-
dia where users post over 500 million “tweets” ev-
ery day, with a frequent use of emojis2. A tweet is
a short (up to 140 characters) user-generated text,
typically noisy and written in a very casual lan-
guage.

By accessing and querying a big number of
tweets of a specific language and which have spe-
cific emojis, we can auto-label them based on the
polarity of those emojis. The assumption is that
if emojis are found in a tweet, then it expresses
a sentiment (ie. it is not neutral) which has the
same polarity as the emoji. This method is called
distant-supervised learning and has been applied
successfully in several similar scenarios (Pak and
Paroubek, 2011; Tang et al., 2014; Narr et al.,
2012).

In this paper, we focus on French, assuming no
language knowledge. The SSWE we get through
the described methodology is then used to featur-
ize documents when training a prediction model
on a new French data set for sentiment analysis.
The major contributions of the work presented in
this paper are:

– We develop SSWE models by updating
Word2Vec embeddings trained on tweets,
through supervised learning with Recurrent
Neural Networks (RNN) and Convolutional
Neural Networks (CNN), on a big data set of
tweets auto-labeled through emojis.

– We show that SSWE perform better than
the underlying Word2Vec embeddings, even

1In this paper, we will use the word ”emojis” to refer to
both ”emojis” and ”emoticons”.

2Twitter statistics: http://www.
internetlivestats.com/twitter-statistics

on data sets different and much less noisy
than tweets. SSWE trained with auto-labeled
tweets from which the emojis were removed,
improve the sentiment prediction on data sets
that have no or little emojis.

– We show that transfer learning starting with
the deep learning model used to train the
SSWE performs better than training a new
traditional ML prediction model from scratch
and using SSWE as features.

In Section 2, we present a literature review. In
Section 3, we present the methodology used to
auto-label tweets, train a Word2Vec model and up-
date it into SSWE. Section 4 describes the experi-
ment we conducted and discusses results. Finally,
in section 5 we summarize our results and present
directions for future work.

2 Related Work

We present here a brief review of previous work
for sentiment classification. Most early research
follows Pang (2002) who uses bag of words and
one-hot-encoding to represent words in movie re-
views, using linguistic features and various classi-
fiers. Work by Pang (2008), and Owoputi (2013)
focused on the features for learning and their ef-
fectiveness, such as part-of-speech, syntax, nega-
tion handling and topic-oriented features. Mo-
hammad (2013), achieved the best results for the
sentiment classification task of the SemEval com-
petition by using sentiment lexicons and hand-
crafted rules. Gamon (2004) investigated the sen-
timent analysis task on noisy data collected from
customer feedback surveys, using lexical (lem-
matized n-gram) and linguistic features (part-of-
speech tagging, context free phrase structure pat-
terns, transitivity of a predicate, tense information
etc.). They also tried feature reduction and applied
SVM model for classification.

Another approach for features representation is
to use low-dimensional real-valued vectors (word
embeddings) to represent the words, such as
Word2Vec or C&W. Maas (2011) proposed the use
of probabilistic document models with a sentiment
predictor function for each word. Bespalov (2012)
relied on latent semantic analysis to initialize em-
beddings. Labutov (2013) relies on pre-trained
word embeddings which are re-embedded in a su-
pervised task using labeled data by performing un-
constrained optimization of a convex objective.

http://www.internetlivestats.com/twitter-statistics
http://www.internetlivestats.com/twitter-statistics


16

Tang (2014) proposed to update word embed-
dings specifically for sentiment classification, ar-
guing that words with opposite polarities might
end up being neighbors. They learn word embed-
dings from a massive number of tweets collected
by a distant-supervised way based on the existence
of positive and negative emojis. They propose
a sentiment-specific word embedding (SSWE)
model that is a modification of C&W to predict
not just the lexical form (n-gram) but also the sen-
timent of the word in that context. They were able
to produce comparable results to the top system
(rule-based) in the SemEval 2013 competition.

Ren (2016) extended the work of Tang (2014),
arguing that the topic information of a tweet af-
fects the sentiment of its words. They modified the
learning objective of the C&W model to also in-
corporate the sentiment information as well as the
topic distribution provided by LDA models. They
also tackle the problem of word polysemy (words
that have multiple meanings based on their con-
text) by creating context representation for each
word occurrence (environment vector) and cluster
these vectors into ten groups using k-means algo-
rithm. The words get their meaning by their dis-
tance to the centroids of the clusters. Finally, they
train a CNN for sentiment classification.

In terms of classification models, with the re-
gained interest in deep learning, recent work
shifted to the use of RNNs. These networks pro-
cess every element of a sequence in a way depend-
ing of all previous computations, keeping track
of previous information across the sequence. In
particular, LSTM (Long Short-Term Memory) can
learn long-term dependencies and is thus popular
for sentiment classification of sentences. Gugilla
(2016) uses both LSTM and CNN with Word2Vec
and linguistic embeddings to distinguish between
neutral and sentiment documents. Qian (2017)
uses LSTM with linguistic resources such as sen-
timent lexicon, negation and intensity words, to
help identify the sentiment effect in sentences. Xu
(2016) proposed the use of cached LSTM model to
further enhance the capabilities of LSTM to cap-
ture the global semantic features and the local se-
mantic features for long text sentiment classifica-
tion.

For Twitter sentiment classification using dis-
tant supervision, while some research used
lexicon-based approaches with positive and neg-
ative sentiment words (Taboada et al., 2011; Thel-

# Raw # Tweets % Positive
Tweets cleaned in cleaned
(in M) (in M)

April 18 4,14 2,62 51.29
May 18 3,89 2,44 49.20
June 18 2,83 1,76 50.94

Total 10,86 6,82 50.43

Table 1: Characteristics of the auto-labeled tweets used
in this paper.

wall et al., 2012), other studies leveraged emojis
for distant supervision (Pak and Paroubek, 2011;
Zhao et al., 2012). To ensure repeatability of
experiments in many languages without any lan-
guage knowledge, we will also take this direction
even though it means we are neglecting precious
language-dependent features that would have in-
creased the prediction power of our models.

3 SSWE Training

3.1 Auto-Labeled Tweets and Preprocessing

Having access to a big database of tweets, we
first queried three months of French tweets: April,
May and June 2018 (partial). Only tweets con-
taining one or more of a specified list of emojis
are considered, but all emojis in a tweet should
have the same polarity (confusing tweets are dis-
carded). This polarity will correspond to the label
of the tweet. For example, :) is a positive emoji,
while :( is negative. To avoid dealing with class
imbalance, we downsample the majority class to
get a 1:1 ratio of positive to negative auto-labeled
tweets.

We perform the following operations on the
auto-labeled tweets (Preprocessing 0). Using reg-
ular expression (Regex), we remove “RT” (cor-
responding to Retweets), ”@” and name men-
tions, tweet links and duplicate tweets. We also
separate emojis when there is no space between
them. Otherwise, multiple stacked emojis would
be seen as one word instead of a succession of
emojis by standard open-source tokenizers. Fi-
nally, we replace emojis involving punctuation by
“Emoji 01”, “Emoji 02”, etc. Otherwise, standard
tokenizers would separate punctuation marks and
break emojis. For example: “:P” would be tok-
enized into [“:”, “P”].

Figures 1 and 2 respectively show the positive
and negative emojis used for auto-labeling.



17

Figure 1: List of positive emojis used for auto-labeling.

Figure 2: List of negative emojis used for auto-labeling.

Table 1 summarizes statistics about the cleaned
data sets.

We explored two additional preprocessing
methods to study their impact on sentiment pre-
diction.

– Preprocessing A: Remove emojis after auto-
annotation to avoid biasing the prediction
models into classifying mostly based on emo-
jis.

– Preprocessing B: Replace every occurrence
of three or more successive characters into
two of them.

The reasoning behind Preprocessing B is as
follows: since tweets contain casual language,
users sometimes repeat the same character many
times to stress out an emotion. For example, the
word “aime” could appear as “aime”, “aiime”, “ai-
iime”, “aiiiiiime”, etc. As a result, the number of
features will increase when using ngrams, and the
dictionary size of word embeddings would also in-
crease. This would dilute the word embeddings
because the same word would be considered as
different ones which will carry a lower weight than
the case where all words with repetitions are as-
sumed to be one. Assuming that when the letter
is repeated 2 or more times, the number of repeti-
tions is random and expresses the same meaning,
we experiment with replacing any occurrence of
two or more successive characters with two occur-
rences.

3.2 Word2Vec Model Training

Twitter sentiment classification has traditionally
been conducted through machine learning mod-

els using labor intensive features. A less labor-
intensive feature engineering approach has been
to rely on word embeddings such as Word2Vec,
which incorporates syntactic context of words.
After preprocessing the autolabeled tweets, we
trained two Word2Vec models on them in an
unsupervised way. One where only standard
preprocessing is applied to autolabeled data
(Preprocessing 0), and the second on autolabeled
data where characters repetitions above three are
removed (Preprocessing B). Note that since the
autolabeled data is divided in three files, and to
avoid loading all the tweets in-memory at once,
Word2Vec is trained incrementally on the three
files.

3.3 Sentiment Specific Word Embeddings
Training

While Word2Vec typically performs well on gen-
eral classification tasks, it might not be effective
enough for sentiment classification because it ig-
nores word sentiments. Hence the need to update
the trained Word2Vec embeddings to incorporate
sentiment. We train a deep learning network (ei-
ther RNN or CNN) using first an Embedding layer,
initialized with the trained Word2Vec embeddings.
During training on the autolabeled tweets, network
weights, including word embeddings, are updated.
At the end of training, we extract the embedding
matrix: it has the same vocabulary as the original
Word2Vec, but weights now incorporate sentiment
information. We call this embedding matrix a
“Sentiment Specific Word Embeddings” (SSWE)
(Tang et al., 2014). The assumption to test is if in-
deed, SSWE used as features would perform better
than Word2Vec for the task of sentiment analysis.



18

Figure 3: Methodology used to train the SSWE.

We trained four different types of models:

– SSWE C trained using a CNN structure
(three parallel convolutional layers and
global max pooling, then dense layer with
ReLu, then dense layer with softmax).

– SSWE R, SSWE R avg, and SSWE R max
trained using an RNN structure (two bidirec-
tional LSTM from which respectively the last
output, average of outputs or max of outputs
goes through a dense layer with softmax).

After performing Preprocessing A, we can
train four additional models by removing emojis
from the autolabeled data when updating the em-
beddings of Word2Vec. Note that in this case,
emojis still have embeddings but they are not up-
dated during the training of SSWE in order to
avoid biasing the weights into relying mostly on
emojis.

After performing Preprocessing B, we can get
an additional eight models by using the apropriate
trained Word2Vec (without character repetitions).

Note that since the auto-labeled data is divided
in three files, and to avoid loading all the tweets in-
memory at once, SSWE is trained incrementally
on the three files. For each one, we used three
epochs and a batch size of 500.

Figure 3 summarizes the methodology used to
train the SSWE.

Figure 43 and Figure 5 summarize the architec-
tures of the deep learning models used.

3http://www.wildml.com/2015/11/
understanding-convolutional-neural-
networks-for-nlp/

Figure 4: CNN architecture.

Figure 5: RNN architecture.

3.4 Graph-Based Label Propagation

We used the method introduced by Hamilton
(2016) to create a dictionary of positive and nega-
tive words from Word2Vec embeddings in an un-
supervised way. The idea revolves around build-
ing word embeddings (the paper uses Vector Space
Model instead of Word2Vec), then creating a
graph of words connected to their k-nearest neigh-
bors in the embeddings space with edges weighted
by the cosine similarity. Using a short list of “seed
words” for positive and negative polarities, senti-
ment is propagated through the network through a
random walk method. This idea was implemented
by Hamilton et al. in a Python package called
SENTPROP.

We use emojis as seed words and Word2Vec
trained on tweets from which we only kept a spe-
cific short set of positive and negative emojis (so
the focus is on actual words that are neighbors to
the seed emojis instead of other emojis).

http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/
http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/
http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/


19

4 Experiments and Results

4.1 Setup

We conduct experiments to evaluate the validity
of using SSWE over Word2Vec for French senti-
ment classification. In addition to the auto-labeled
tweets, four manually labeled French data sets
were used. The first three data sets come from the
SemEval 2016 competition (International Work-
shop on Semantic Evaluation). It is an ongoing
series of evaluations of computational semantic
analysis systems, organized under the umbrella of
SIGLEX, the Special Interest Group on the Lex-
icon of the Association for Computational Lin-
guistics. The first data set (Train) corresponds to
French Restaurant Reviews. The two others cor-
respond to French Restaurant Reviews (Test1; i.e.
Same domain as the training set) and French Mu-
seum Reviews (Test2). The latter being out-of-
domain, it is a good gauge of how generalizable
a model trained on the training set is to different
French data sets. Those three data sets have been
labeled for Aspect Based Sentiment Analysis. It is
the task of mining and summarizing opinions from
text about specific entities and their aspects (Apid-
ianaki et al., 2016) they are annotated with rele-
vant entities, aspects and polarity values. In order
to use those three data sets in the context of docu-
ment level sentiment classification, we ungrouped
documents into sentences, then filtered them:

- We reject the sentences with mixed polarity
values.

- We keep the rest of the sentences and label
them as “Positive” if all polarity values are pos-
itive and as “Negative” if all polarity values are
negative.

Note that we were not able to download all
the reviews as some of them were not available
anymore4. The fourth data set is comprised of
manually labeled French tweets that are not pub-
licly available (Test3). These tweets do not con-
tain emojis, and thus the performance on this
data set will solely rely on the understanding that
our trained models will have of the French lan-
guage. The four French data sets (after processing)
and their characteristics are summarized in Table
2. We can see that they are all relatively well-
balanced, so no special treatment for class imbal-
ance will be performed.

4The data can be downloaded from http:
//alt.qcri.org/semeval2016/task5/index.
php?id=data-and-tools

Data Set Naming # Doc % Positive
SemEval 2016 Train 1338 47.80

Restaurant Train
SemEval 2016 Test1 515 47.60
Restaurant Test
SemEval 2016 Test2 529 54.25
Museum Test

Tweets Test3 3296 48.90
Manually Labeled

Table 2: Characteristics of the training and testing sets
used.

4.2 Experiments

The experiments conducted and their results are
summarized in Table 3.

In order to establish the importance of the
SSWE for sentiment analysis, we trained multi-
ple models using standard features (word ngrams,
character ngrams) or more advanced features
(Word2Vec, SSWE: average sentence embedding)
with standard classifiers (logistic regression (LR),
SVM, Random Forest) or deep learning frame-
works (CNN or RNN with transfer learning on
the data set Train), then evaluated them on Test1,
Test2, and Test3. More precisely, we used the fol-
lowing approaches as baselines, training a Logistic
Regression (LR), SVM and Random Forest with
word and character ngrams, Word2Vec, or both
(experiments 1-3).

The three baselines were compared to four
methods using SSWE trained with either RNN
(taking the last output of the recurrent sequence)
or CNN trained on the auto-labeled data and used
for feature extraction on Train. We compared the
performance with SSWE trained on auto-labeled
data cleaned from the emojis to prevent the model
from giving a high weight to emojis (experiments
4-7).

We compared those approaches with the use
of average or maximum of the output sequences
of the RNN (trained on auto-labeled data without
emoji, then used for feature engineering on Train)
(experiments 8-9).

We then add the character and ngrams features
(experiments 10-15).

Finally, we compared them with deep learning
and transfer learning. Keeping the same network
that was used to train the SSWE, we fix the SSWE
and let the last layer(s) be trainable on Train. For
the RNN, the dense layer and softmax are train-
able. For the CNN, training only the dense layer
and softmax as part of transfer learning yields poor

http://alt.qcri.org/semeval2016/task5/index.php?id=data-and-tools
http://alt.qcri.org/semeval2016/task5/index.php?id=data-and-tools
http://alt.qcri.org/semeval2016/task5/index.php?id=data-and-tools


20

results, so we train the dense layer and ReLu as
well as the last dense layer and softmax during
transfer learning. The reason why transfer learn-
ing is used instead of direct training on Train is the
lack of data: Train having only 1338 unique doc-
uments, this is not enough to tune a deep learning
model which has a much larger number of param-
eters. (experiments 16-21)

We also explored two additional methods that
do not require any knowledge of the language and
that will be detailed in subsequent sections:

1) using a dictionary auto-built by label propa-
gation using specific emoji seeds on the Word2Vec
model (experiments 22-24).

2) repeating the experiment on data for which
two or more repetitions of characters are replaced
with two characters.

All approaches with Logistic Regression, SVM
or Random Forest used the Scikit-Learn Python
package. Word2Vec models were built using
genism, while SSWE and deep learning frame-
works used Keras with Tensorflow backend.

4.3 Evaluation Methods
For models trained with Logistic Regression,
SVM and Random Forest, we decided to report
and compare only results obtained with Logistic
Regression for fair comparison. The reported re-
sults correspond to those achieved with a set of
parameters giving the best results on part of the
training set, through sweeping. The parameters
that were swept on in Logistic Regression are: size
of word ngrams, size of character ngrams, lower-
casing of words, maximum number of iterations
for the ‘lbfgs’ solver, inverse of regularization
strength for ‘l2’ penalty. For models trained with
Deep Learning, multiple combinations of epochs
and batch sizes were used, and we report here the
best results obtained.

While F-score and Accuracy (since there is no
class imbalance) can be fair ways of evaluating
the performance, we decided to rely mainly on the
AUC (Area under the Curve) since it is indepen-
dent of the classification threshold choice and is a
good indicator of the ability of the models to dis-
tinguish between Positive and Negative examples.

4.4 Results and Analysis
With Logistic Regression, using only ngram fea-
tures (1) give the worse results on Test2 and
Test3. This is probably because ngrams ob-
tained on a specific training set fail to general-

Features Test1 Test2 Test3
LOGISTIC REGRESSION

1 ngram 86.27 75.36 72.49
2 Word2Vec 86.03 79.00 86.75
3 ngram + Word2Vec 90.18 83.3 86.71
4 SSWE R last 86.11 79.25 84.28
5 SSWE R last no emoji 87.86 83.12 86.52
6 SSWE C 85.34 78.91 83.72
7 SSWE C no emoji 89.15 83.59 87.83
8 SSWE R avg no emoji 88.01 83.07 87.17
9 SSWE R max no emoji 86.29 82.49 85.40
10 ngram + SSWE R last 89.86 83.10 85.06
11 ngram + 90.79 84.72 85.82

SSWE R last no emoji
12 ngram + 89.05 83.62 84.24

SSWE C
13 ngram + 91.42 85.67 87.50

SSWE C no emoji
14 ngram + 90.84 84.53 86.90

SSWE R avg no emoji
15 ngram + 91.20 85.75 87.37

SSWE R max no emoji
TRANSFER LEARNING

16 SSWE R last 90.17 84.2 83.89
17 SSWE R last no emoji 92.78 91.02 90.59
18 SSWE C 81.31 76.87 79.91
19 SSWE C no emoji 89.97 87.65 89.27
20 SSWE R avg no emoji 91.86 90.71 90.03
21 SSWE R max no emoji 91.61 91.13 90.34

WITH DICTIONARY (Dict)
22 Dict 63.52 63.86 64.85
23 LR + 88.95 76.97 77.73

ngram + Dict
24 LR + ngram + Dict + 91.21 85.73 87.38

SSWE C no emoji

Table 3: Experiments Results with Logistic Regres-
sion, Transfer Learning and Dictionary Lookup in
terms of AUC (in %).

ize well to out-of-domain data sets (Test2 and
Test3). Using Word2Vec only (2) is slightly worse
on Test1 but generalizes better on out-of-domain
data sets. Using both Word2Vec and ngram (3)
considerably improves the results compared to (1)
and (2), showing the importance of Word2Vec
in adding context information in word features,
which ngram does not. This will be the main base-
line to compare to the use of SSWE.

We notice that SSWE trained on the auto-
labeled data where emojis were kept (4), (6),
yields results similar or slightly lower than using
Word2Vec only (2). The same comparison holds
when we add ngrams: (10) and (12) vs. (3). This
is probably because the labels of the auto-labeled
data are directly correlated to specific positive and
negative emojis since they were used for autola-
beling. As a result, training SSWE on the auto-
labeled data without removing emojis likely bi-
ased the embeddings into giving higher weights



21

to emojis. Since all the test sets have little to no
emojis, the use of these SSWE yields poor results.

SSWE trained on auto-labeled data cleaned
from the emojis (without ngrams) in (5), (7), (8)
and (9) provides similar or better performance
than Word2Vec only (2). Adding ngrams to these
SSWE in (11), (13), (14) and (15) gives the best
results with Logistic Regression compared to the
best baseline (3). While beating the performance
of (3) on Test3 seems hard (likely due to the fact
that Word2Vec was trained on data similar to Test3
since they are both tweets), ngrams and SSWE
trained as part of a CNN model on auto-labeled
data without emojis (13) gives on average the best
results on the three data sets when using Logistic
Regression, compared to (3): 1.34%, 2.37%, and
0.79% improvement on Test1, Test2, Test3 respec-
tively. These results already confirm that SSWE
improves the prediction performance in sentiment
analysis over Word2Vec, likely because it incor-
porates sentiment polarity.

These results were even more confirmed with
deep learning. SSWE trained on auto-labeled
data with emojis also perform less good than their
counterpart trained without emojis: (16) vs. (17)
and (18) vs. (19). The best results are obtained
on average across the three test sets when transfer
learning is applied with the RNN, using the last
output from the bidirectional LSTM. Compared to
the best baseline (3), we achieve: 2.6%, 7.72%,
and 3.88% improvement on Test1, Test2, Test3 re-
spectively.

We also used the scores of the auto-generated
dictionary with a 0.5 threshold to assign a polarity
to the words. We then classify the documents by
comparing the distribution of negative and positive
words. This gave worse results than ngram with
logistic regression ((1) vs. (22)). We also used the
word scores to create a dictionary with 10 labels:
if the word has a score between 0 and 0.1, it is
assigned to the sentiment bin 1 class; between 0.1
and 0.2, it is assigned to sentiment bin 2, etc. The
distribution among those ten classes is then used
as an additional feature in our experiments with
logistic regression. However, this feature does not
improve the results((1) vs. (23) and (13) vs. (24)).

Finally, we did not notice an improvement when
removing repetition of characters above three.
This might be explained by the fact that character
repetition can be important for emphasis: the same
word with more character repetitions might have

a higher polarity. For example, “heureuxxxxx”
might show more excitement and be “more pos-
itive” than “heureuxx”.

4.5 Discussion
Given those results, multiple conclusions can be
drawn:

– Word2Vec boosts performance of ngrams, es-
pecially on out-of-domain testing set.

– SSWE trained on data autolabeled with emo-
jis and where emojis were not removed, neg-
atively impacts the performance of the model
on data that have little or no emojis.

– SSWE trained on data autolabeled with emo-
jis and where emojis were removed, provides
an improvement over Word2Vec.

– Transfer Learning in deep learning by fix-
ing the network structure (including SSWE
trained without emojis) on the training set,
yields the best improvements over the base-
line with an increase of more than 7% AUC
on Test2 for example. This is the most no-
ticeable when using RNN and taking the last
output of the bidirectional LSTM sequence.

5 Conclusion and Future Work

In this paper, we propose to label a big num-
ber of tweets in any language (here French) us-
ing a small set of positive and negative emojis,
train a Word2Vec model on the tweets, then up-
date the embeddings through deep learning with
bidirectional LSTM on the autolabeled data. The
embeddings we get are then enriched with senti-
ment information and can be used as features for
new data sets in the same language. If those data
sets have little or no emoji, the embeddings en-
richment should be performed using autolabeled
data filtered from emojis in order to avoid bias-
ing the embeddings and relying mostly on emo-
jis. We show that these sentiment specific word
embeddings perform better than plain Word2Vec
using SemEval 2016 French data of restaurant re-
views (Train) to train a model, another SemEval
2016 testing set of restaurant reviews (Test1), an-
other of museum reviews (Test2) and a manually
labeled data set of French tweets (Test3). It pro-
vides an AUC improvement of 1.34%, 2.37%, and
0.79% on Test1, Test2, Test3 respectively with Lo-
gistic Regression. The improvement gets to 2.6%,



22

7.72%, and 3.88% with transfer learning using
RNN with the network used to train the SSWE.
This methodology can be applied to any language
since it does not rely on linguistic features.

This work has a few limitations, which could
be better addressed in future work. This includes
the use of sentiment lexicons, stemming, normal-
ization, and negation handling. Our work did not
explore the graph propagation technique with dif-
ferent embeddings models such as Vector Space
Models. In addition, distant-supervised learning
was applied on tweets to get labeled data and then
train the SSWE. Tweets being written in a very ca-
sual language, the results on SemEval data (which
is clean) might improve if the SSWE were trained
on a more general data set. An area for future work
would be to explore distant supervised learning on
movie or product reviews, then compare the re-
sults with the created SSWE on SemEval data with
those obtained using the Twitter SSWE. Finally,
our work excludes the case of neutral text. We can
specify two thresholds for the polarity prediction
and assign a document to the neutral class when
the model yields a predicted probability between
those two thresholds. Another method is to train a
subjectivity classifier followed by a polarity clas-
sifier for subjective documents.

References
Marianna Apidianaki, Xavier Tannier, and Cécile

Richart. 2016. Datasets for aspect-based sentiment
analysis in french. In Proceedings of the Tenth In-
ternational Conference on Language Resources and
Evaluation (LREC 2016), Paris, France. European
Language Resources Association (ELRA).

Dmitriy Bespalov, Yanjun Qi, Bing Bai, and Ali Shoko-
ufandeh. 2012. Sentiment classification with super-
vised sequence embedding. In Proceedings of the
2012 European Conference on Machine Learning
and Knowledge Discovery in Databases - Volume
Part I, ECML PKDD’12, pages 159–174, Berlin,
Heidelberg. Springer-Verlag.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel P. Kuksa.
2011. Natural language processing (almost) from
scratch. CoRR, abs/1103.0398.

Michael Gamon. 2004. Sentiment classification on
customer feedback data: Noisy data, large feature
vectors, and the role of linguistic analysis. In Pro-
ceedings of the 20th International Conference on
Computational Linguistics, COLING ’04, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.

Chinnappa Guggilla, Tristan Miller, and Iryna
Gurevych. 2016. Cnn- and lstm-based claim classi-
fication in online user comments. In COLING 2016,
the 26th International Conference on Computational
Linguistics: Technical Papers, pages 2740–2751.

William L. Hamilton, Kevin Clark, Jure Leskovec, and
Dan Jurafsky. 2016. Inducing domain-specific sen-
timent lexicons from unlabeled corpora. CoRR,
abs/1606.02820.

Igor Labutov and Hod Lipson. 2013. Re-embedding
words. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 489–493.

Andrew L. Maas, Raymond E. Daly, Peter T. Pham,
Dan Huang, Andrew Y. Ng, and Christopher Potts.
2011. Learning word vectors for sentiment analy-
sis. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies - Volume 1, HLT ’11,
pages 142–150, Stroudsburg, PA, USA. Association
for Computational Linguistics.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeffrey Dean. 2013. Distributed represen-
tations of words and phrases and their composition-
ality. CoRR, abs/1310.4546.

Saif M. Mohammad, Svetlana Kiritchenko, and Xiao-
dan Zhu. 2013. Nrc-canada: Building the state-
of-the-art in sentiment analysis of tweets. CoRR,
abs/1308.6242.

Sascha Narr, Michael Hulfenhaus, and Sahin Al-
bayrak. 2012. Language-independent twitter senti-
ment analysis. Knowledge discovery and machine
learning (KDML), LWA, pages 12–14.

Olutobi Owoputi, Brendan O’Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A.
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 380–390. Association for Computational Lin-
guistics.

Alexander Pak and Patrick Paroubek. 2011. Twitter
for sentiment analysis: When language resources
are not available. In Proceedings of the 2011 22Nd
International Workshop on Database and Expert
Systems Applications, DEXA ’11, pages 111–115,
Washington, DC, USA. IEEE Computer Society.

Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Found. Trends Inf. Retr., 2(1-
2):1–135.

Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? sentiment classification us-
ing machine learning techniques. In Proceedings of
EMNLP, pages 79–86.



23

Qiao Qian, Minlie Huang, Jinhao Lei, and Xiaoyan
Zhu. 2017. Linguistically regularized lstm for sen-
timent classification. In Proceedings of the Annual
Meeting of the Association for Computational Lin-
guistics (ACL 2017.

Yafeng Ren, Yue Zhang, Meishan Zhang, and
Donghong Ji. 2016. Improving twitter sentiment
classification using topic-enriched multi-prototype
word embeddings.

Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-
berly D. Voll, and Manfred Stede. 2011. Lexicon-
based methods for sentiment analysis. 37:267–307.

Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, Ting
Liu, and Bing Qin. 2014. Learning sentiment-
specific word embedding for twitter sentiment clas-
sification. In 52nd Annual Meeting of the Associa-
tion for Computational Linguistics, ACL 2014 - Pro-
ceedings of the Conference, volume 1, pages 1555–
1565.

Mike Thelwall, Kevan Buckley, and Georgios Pal-
toglou. 2012. Sentiment strength detection for the
social web. J. Am. Soc. Inf. Sci. Technol., 63(1):163–
173.

Jiacheng Xu, Danlu Chen, Xipeng Qiu, and Xuanjing
Huang. 2016. Cached long short-term memory neu-
ral networks for document-level sentiment classifi-
cation. In Proceedings of the 2016 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1660–1669. Association for Computa-
tional Linguistics.

Lei Zhang, Shuai Wang, and Bing Liu. 2018. Deep
learning for sentiment analysis : A survey. CoRR,
abs/1801.07883.

Jichang Zhao, Li Dong, Junjie Wu, and Ke Xu. 2012.
Moodlens: An emoticon-based sentiment analysis
system for chinese tweets. In Proceedings of the
18th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, KDD ’12,
pages 1528–1531, New York, NY, USA. ACM.


