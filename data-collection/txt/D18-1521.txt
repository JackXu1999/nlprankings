



















































Learning Gender-Neutral Word Embeddings


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4847–4853
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

4847

Learning Gender-Neutral Word Embeddings

Jieyu Zhao Yichao Zhou Zeyu Li Wei Wang Kai-Wei Chang
University of California, Los Angeles

{jyzhao, yz, zyli, weiwang, kwchang}@cs.ucla.edu

Abstract

Word embedding models have become a fun-
damental component in a wide range of
Natural Language Processing (NLP) appli-
cations. However, embeddings trained on
human-generated corpora have been demon-
strated to inherit strong gender stereotypes that
reflect social constructs. To address this con-
cern, in this paper, we propose a novel training
procedure for learning gender-neutral word
embeddings. Our approach aims to preserve
gender information in certain dimensions of
word vectors while compelling other dimen-
sions to be free of gender influence. Based on
the proposed method, we generate a Gender-
Neutral variant of GloVe (GN-GloVe). Quanti-
tative and qualitative experiments demonstrate
that GN-GloVe successfully isolates gender
information without sacrificing the functional-
ity of the embedding model.

1 Introduction

Word embedding models have been designed for
representing the meaning of words in a vector
space. These models have become a fundamen-
tal NLP technique and have been widely used
in various applications. However, prior stud-
ies show that such models learned from human-
generated corpora are often prone to exhibit so-
cial biases, such as gender stereotypes (Bolukbasi
et al., 2016; Caliskan et al., 2017). For example,
the word “programmer” is neutral to gender by its
definition, but an embedding model trained on a
news corpus associates “programmer” closer with
“male” than “female”.

Such a bias substantially affects downstream
applications. Zhao et al. (2018) show that a coref-
erence resolution system is sexist due to the word
embedding component used in the system. This
concerns the practitioners who use the embedding
model to build gender-sensitive applications such

as a resume filtering system or a job recommenda-
tion system as the automated system may discrimi-
nate candidates based on their gender, as reflected
by their name. Besides, biased embeddings may
implicitly affect downstream applications used in
our daily lives. For example, when searching for
“computer scientist” using a search engine, as this
phrase is closer to male names than female names
in the embedding space, a search algorithm us-
ing an embedding model in the backbone tends to
rank male scientists higher than females’, hinder-
ing women from being recognized and further ex-
acerbating the gender inequality in the community.

To alleviate gender stereotype in word embed-
dings, Bolukbasi et al. (2016) propose a post-
processing method that projects gender-neutral
words to a subspace which is perpendicular to
the gender dimension defined by a set of gender-
definition words.1 However, their approach has
two limitations. First, the method is essentially a
pipeline approach and requires the gender-neutral
words to be identified by a classifier before em-
ploying the projection. If the classifier makes
a mistake, the error will be propagated and af-
fect the performance of the model. Second, their
method completely removes gender information
from those words which are essential in some do-
mains such as medicine and social science (Back
et al., 2010; McFadden et al., 1992).

To overcome these limitations, we propose a
learning scheme, Gender-Neutral Global Vectors
(GN-GloVe) for training word embedding mod-
els with protected attributes (e.g., gender) based
on GloVe (Pennington et al., 2014).2 GN-GloVe
represents protected attributes in certain dimen-

1Gender-definition words are the words associated with
gender by definition (e,g., mother, waitress); the remainder
are gender-neutral words.

2The code and data are released at https://github.
com/uclanlp/gn_glove

https://github.com/uclanlp/gn_glove
https://github.com/uclanlp/gn_glove


4848

sions while neutralizing the others during training.
As the information of the protected attribute is re-
stricted in certain dimensions, it can be removed
from the embedding easily. By jointly identifying
gender-neutral words while learning word vectors,
GN-GloVe does not require a separate classifier
to identify gender-neutral words; therefore, the er-
ror propagation issue is eliminated. The proposed
approach is generic and can be incorporated with
other word embedding models and be applied in
reducing other societal stereotypes.

Our contributions are summarized as follows:
1) To our best knowledge, GN-GloVe is the first
method to learn word embeddings with protected
attributes; 2) By capturing protected attributes in
certain dimensions, our approach ameliorates the
interpretability of word representations; 3) Qual-
itative and quantitative experiments demonstrate
that GN-GloVe effectively isolates the protected
attributes and preserves the word proximity.

2 Related Work

Word Embeddings Word embeddings serve as
a fundamental building block for a broad range
of NLP applications (dos Santos and Gatti, 2014;
Bahdanau et al., 2014; Zeng et al., 2015) and
various approaches (Mikolov et al., 2013b; Pen-
nington et al., 2014; Levy et al., 2015) have
been proposed for training the word vectors. Im-
provements have been made by leveraging se-
mantic lexicons and morphology (Luong et al.,
2013; Faruqui et al., 2014), disambiguating mul-
tiple senses (Šuster et al., 2016; Arora et al., 2018;
Upadhyay et al., 2017), and modeling contextual-
ized information by deep neural networks (Peters
et al., 2018). However, none of these works at-
tempts to tackle the problem of stereotypes exhib-
ited in embeddings.

Stereotype Analysis Implicit stereotypes have
been observed in applications such as on-
line advertising systems (Sweeney, 2013), web
search (Kay et al., 2015), and online reviews (Wal-
lace and Paul, 2016). Besides, Zhao et al. (2017)
and Rudinger et al. (2018) show that coreference
resolution systems are gender biased. The sys-
tems can successfully predict the link between
“the president” with male pronoun but fail with
the female one. Rudinger et al. (2017) use point-
wise mutual information to test the SNLI (Bow-
man et al., 2015) corpus and demonstrate gender
stereotypes as well as varying degrees of racial, re-

ligious, and age-based stereotypes in the corpus. A
temporal analysis about word embeddings (Garg
et al., 2018) captures changes in gender and ethnic
stereotypes over time. Researchers attributed such
problem partly to the biases in the datasets (Zhao
et al., 2017; Yao and Huang, 2017) and word em-
beddings (Garg et al., 2017; Caliskan et al., 2017)
but did not provide constructive solutions.

3 Methodology

In this paper, we take GloVe (Pennington et al.,
2014) as the base embedding model and gender
as the protected attribute. It is worth noting that
our approach is general and can be applied to
other embedding models and attributes. Follow-
ing GloVe (Pennington et al., 2014), we construct
a word-to-word co-occurrence matrixX , denoting
the frequency of the j-th word appearing in the
context of the i-th word as Xi,j . w, w̃ ∈ Rd stand
for the embeddings of a center and a context word,
respectively, where d is the dimension.

In our embedding model, a word vector w con-
sists of two parts w = [w(a);w(g)]. w(a) ∈ Rd−k
and w(g) ∈ Rk stand for neutralized and gendered
components respectively, where k is the number
of dimensions reserved for gender information.3

Our proposed gender neutralizing scheme is to
reserve the gender feature, known as “protected
attribute” into w(g). Therefore, the information
encoded in w(a) is independent of gender influ-
ence. We use vg ∈ Rd−k to denote the direc-
tion of gender in the embedding space. We cat-
egorize all the vocabulary words into three sub-
sets: male-definition ΩM , female-definition ΩF ,
and gender-neutral ΩN , based on their definition
in WordNet (Miller and Fellbaum, 1998).

Gender Neutral Word Embedding Our mini-
mization objective is designed in accordance with
above insights. It contains three components:

J = JG + λdJD + λeJE , (1)

where λd and λe are hyper-parameters.
The first component JG is originated from

GloVe (Pennington et al., 2014), which captures
the word proximity:

JG=
V∑

i,j=1

f(Xi,j)
(
wTi w̃j + bi + b̃j − logXi,j

)2
.

3We set k = 1 in this paper.



4849

Here f(Xi,j) is a weighting function to reduce
the influence of extremely large co-occurrence fre-
quencies. b and b̃ are the respective linear biases
for w and w̃.

The other two terms are aimed to restrict gen-
der information in w(g), such that w(a) is neutral.
Given male- and female-definition seed words ΩM
and ΩF , we consider two distant metrics and form
two types of objective functions.

In JL1D , we directly minimizing the negative dis-
tances between words in the two groups:

JL1D = −

∥∥∥∥∥∥
∑

w∈ΩM

w(g) −
∑

w∈ΩF

w(g)

∥∥∥∥∥∥
1

.

In JL2D , we restrict the values of word vectors in
[β1, β2] and push w(g) into one of the extremes:

JL2D =
∑

w∈ΩM

∥∥∥β1e− w(g)∥∥∥2
2
+
∑

w∈ΩF

∥∥∥β2e− w(g)∥∥∥2
2
,

where e ∈ Rk is a vector of all ones. β1 and β2
can be arbitrary values, and we set them to be 1
and −1, respectively.

Finally, for words in ΩN , the last term encour-
ages their w(a) to be retained in the null space of
the gender direction vg:

JE =
∑

w∈ΩN

(
vTg w

(a)
)2
,

where vg is estimating on the fly by averaging the
differences between female words and their male
counterparts in a predefined set,

vg =
1

|Ω′|
∑

(wm,wf )∈Ω′
(w(a)m − w

(a)
f ),

where Ω′ is a set of predefined gender word pairs.
We use stochastic gradient descent to optimize

Eq. (1). To reduce the computational complexity
in training the wording embedding, we assume vg
is a fixed vector (i.e., we do not derive gradient
w.r.t vg in updating w(a),∀w ∈ Ω′) and estimate
vg only at the beginning of each epoch.

4 Experiments

In this section, we conduct the following qualita-
tive and quantitative studies: 1) We visualize the
embedding space and show that GN-GloVe sepa-
rates the protected gender attribute from other la-
tent aspects; 2) We measure the ability of GN-
GloVe to distinguish between gender-definition

words and gender-stereotype words on a newly
annotated dataset; 3) We evaluate GN-GloVe on
standard word embedding benchmark datasets and
show that it performs well in estimating word
proximity; 4) We demonstrate that GN-Glove re-
duces gender bias on a downstream application,
coreference resolution.

We compare GN-GloVe with two embedding
models, GloVe and Hard-GloVe. GloVe is a
widely-used model (Pennington et al., 2014),
and we apply the post-processing step introduced
in (Bolukbasi et al., 2016) to reduce gender bias in
GloVe and name it after Hard-GloVe. All the em-
beddings are trained on 2017 English Wikipedia
dump with the default hyper-parameters decribed
in (Pennington et al., 2014). When training GN-
GloVe, we constrain the value of each dimension
within [−1, 1] to avoid numerical difficulty. We set
λd and λe both to be 0.8. In our preliminary study
on development data, we observe that the model
is not sensitive to these parameters. Unless other
stated, we use JL1D in the GN-GloVe model.

Separate protected attribute First, we demon-
strate that GN-GloVe preserves the gender asso-
ciation (either definitional or stereotypical asso-
ciations) in w(g)4. To illustrate the distribution
of gender information of different words, we plot
Fig. 1a using w(g) for the x-axis and a random
value for the y-axis to spread out words in the
plot. As shown in the figure, the gender-definition
words, e.g. “waiter” and “waitress”, fall far away
from each other in w(g). In addition, words such
as “housekeeper” and “doctor” are inclined to dif-
ferent genders and their w(g) preserves such infor-
mation.

Next, we demonstrate that GN-GloVe reduces
gender stereotype using a list of profession titles
from (Bolukbasi et al., 2016). All these profes-
sion titles are neutral to gender by definition. In
Fig. 1b and Fig. 1c, we plot the cosine similar-
ity between each word vector w(a) and the gen-
der direction vg (i.e.,

wT vg
‖w‖‖vg‖ ). Result shows that

words, such as “doctor” and “nurse”, possess no
gender association by definition, but their GloVe
word vectors exhibit strong gender stereotype. In
contrast, the gender projects of GN-GloVe word
vectors w(a) are closer to zero. This demonstrates

4We follow the original GloVe implementation using the
summation of word vector and context vector to represent a
word. Therefore, the elements of the word vectors are con-
strained in [-2, 2]



4850

2.0 1.5 1.0 0.5 0.0 0.5 1.0 1.5 2.0
w(g)

actress

ballerina

businesswoman
housewife

maid

nun
waitress

waiter

landlord

salesman
policeman

monk

handyman

congressman

headmaster

socialite

homemakerhousekeeper

nursenanny

hooker
marshal

archbishop
captain

colonel Female
Male
Neutral

(a) w(g) dimension for all the profes-
sions

1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00
similarity

socialite

housekeeper

nursenanny

homemaker hooker
marshal

archbishop
captain
colonel

Neutral

(b) Gender-neutral profession words
projected to gender direction in GloVe

1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00
similarity

socialite

housekeeper

nursenanny

homemaker hooker
marshal

archbishop
captain
colonel

Neutral

(c) Gender-neutral profession words pro-
jected to gender direction in GN-GloVe

Figure 1: Cosine similarity between the gender direction and the embeddings of gender-neutral words. In
each figure, negative values represent a bias towards female, otherwise male.

the gender information has been substantially di-
minished from w(a) in the GN-GloVe embedding.

We further quantify the gender information ex-
hibited in the embedding models. For each model,
we project the word vectors of occupational words
into the gender sub-space defined by “he-she” and
compute their average size. A larger projection in-
dicates an embedding model is more biased. Re-
sults show that the average projection of GloVe
is 0.080, the projection of Hard-GloVe is 0.019,
and the projection of Gn-Glove is 0.052. Com-
paring with GloVe, GN-GloVe reduces the bias by
35%. Although Hard-GloVe contains less gender
information, we will show later GN-GloVe can tell
difference between gender-stereotype and gender-
definition words better.

Gender Relational Analogy To study the qual-
ity of the gender information present in each
model, we follow SemEval 2012 Task2 (Jurgens
et al., 2012) to create an analogy dataset, SemBias,
with the goal to identify the correct analogy of “he
- she” from four pairs of words. Each instance in
the dataset consists of four word pairs: a gender-
definition word pair (Definition; e.g., “waiter -
waitress”), a gender-stereotype word pair (Stereo-
typ; e.g., “doctor - nurse”) and two other pairs
of words that have similar meanings (None; e.g.,
“dog - cat”, “cup - lid”)5. We consider 20 gender-
stereotype word pairs and 22 gender-definition
word pairs and use their Cartesian product to
generate 440 instances. Among the 22 gender-
definition word pairs, there are 2 word pairs that

5The pair is sampled from the list of word pairs with
“SIMILAR: Coordinates” relation annotated in (Jurgens
et al., 2012). The original list has 38 pairs. After removing
gender-definition word pairs, 29 are left.

Dataset Embeddings Definition Stereotype None

SemBias
GloVe 80.2 10.9 8.9
Hard-Glove 84.1 6.4 9.5
GN-GloVe 97.7 1.4 0.9

SemBias
(subset)

GloVe 57.5 20 22.5
Hard-Glove 25 27.5 47.5
GN-GloVe 75 15 10

Table 1: Percentage of predictions for each cate-
gory on gender relational analogy task.

are not used as a seed word during the training.
To test the generalization ability of the model, we
generate a subset of data (SemBias (subset)) of 40
instances associated with these 2 pairs.

Table 1 lists the percentage of times that each
class of pair is on the top based on a word embed-
ding model (Mikolov et al., 2013c). GN-GloVe
achieves 97.7% accuracy in identifying gender-
definition word pairs as an analogy to “he - she”.
In contrast, GloVe and Hard-GloVe makes signif-
icantly more mistakes. On the subset, GN-GloVe
also achieves significantly better performance than
Hard-Glove and GloVe, indicating that it can gen-
eralize the gender pairs on the training set to iden-
tify other gender-definition word pairs.

Word Similarity and Analogy In addition, we
evaluate the word embeddings on the benchmark
tasks to ensure their quality. The word similarity
tasks measure how well a word embedding model
captures the similarity between words comparing
to human annotated rating scores. Embeddings are
tested on multiple datasets: WS353-ALL (Finkel-
stein et al., 2001), RG-65 (Rubenstein and Goode-
nough, 1965), MTurk-287 (Radinsky et al., 2011),
MTurk-771 (Halawi et al., 2012), RW (Luong
et al., 2013), and MEN-TR-3k (Bruni et al., 2012)



4851

Embeddings
Analogy Similarity

Google MSR WS353-ALL RG-65 MTurk-287 MTurk-771 RW MEN-TR-3k
GloVe 70.8 45.8 62.0 75.3 64.8 64.9 37.3 72.2
Hard-GloVe 70.8 45.8 61.2 74.8 64.4 64.8 37.3 72.2
GN-GloVe-L1 68.9 43.7 62.8 74.1 66.2 66.2 40.0 74.5
GN-GloVe-L2 68.8 43.6 62.5 76.4 66.8 65.6 39.3 74.4

Table 2: Results on the benchmark datasets. Performance is measured in accuracy and in Spearman rank
correlation for word analogy and word similarity tasks, respectively.

datasets. The analogy tasks are to answer the ques-
tion “A is to B as C is to ?” by finding a word
vector w that is closest to wA − wB + wC in the
embedding space. Google (Mikolov et al., 2013a)
and MSR (Mikolov et al., 2013c) datasets are uti-
lized for this evaluation. The results are shown
in Table 2, where the suffix “-L1” and “-L2” of
GN-GloVe stand for the GN-GloVe using JL1D and
JL2D , respectively. Compared with others, GN-
GloVe achieves a higher accuracy in the similarity
tasks and its analogy score slightly drops indicat-
ing that GN-GloVe is capable of preserving prox-
imity among words.

Coreference Resolution Finally, we investigate
how the gender bias in word embeddings affects a
downstream application, such as coreference res-
olution. Coreference resolution aims at clustering
the denotative noun phrases referring to the same
entity in the given text. We evaluate our models on
the Ontonotes 5.0 (Weischedel et al., 2012) bench-
mark dataset and the WinoBias dataset (Zhao
et al., 2018).6 In particular, the WinoBias dataset
is composed of pro-stereotype (PRO) and anti-
stereotype (ANTI) subsets. The PRO subset con-
sists of sentences where a gender pronoun refers to
a profession, which is dominated by the same gen-
der. Example sentences include “The CEO raised
the salary of the receptionist because he is gen-
erous.” In this sentence, the pronoun “he” refers
to “CEO” and this reference is consistent with so-
cietal stereotype. The ANTI subset contains the
same set of sentences, but the gender pronoun in
each sentence is replaced by the opposite gender.
For instance, the gender pronoun “he” is replaced
by “she” in the aforementioned example. Despite
the sentence is almost identical, the gender pro-
noun now refers to a profession that is less repre-
sented by the gender. Details about the dataset are
in (Zhao et al., 2018).

6Specifically, we conduct experiments on the Type 1 ver-
sion.

Embeddings OntoNotes-test PRO ANTI Avg Diff
GloVe 66.5 76.2 46.0 61.1 30.2
Hard-Glove 66.2 70.6 54.9 62.8 15.7
GN-GloVe 66.2 72.4 51.9 62.2 20.5
GN-GloVe(wa) 65.9 70.0 53.9 62.0 16.1

Table 3: F1 score (%) on the coreference system.

We train the end-to-end coreference resolution
model (Lee et al., 2017) with different word em-
beddings on OntoNote and report their perfor-
mance in Table 3. For the WinoBias dataset, we
also report the average (Avg) and absolute dif-
ference (Diff) of F1 scores on two subsets. A
smaller Diff value indicates less bias in a system.
Results show that GN-GloVe achieves compara-
ble performance as Glove and Hard-GloVe on the
OntoNotes dataset while distinctly reducing the
bias on the WinoBias dataset. When only the w(a)

potion of the embedding is used in representing
words, GN-GloVe(w(a)) further reduces the bias
in coreference resolution.

5 Conclusion and Discussion

In this paper, we introduced an algorithm for train-
ing gender-neutral word embedding. Our method
is general and can be applied in any language as
long as a list of gender definitional words is pro-
vided as seed words (e.g., gender pronouns). Fu-
ture directions include extending the proposed ap-
proach to model other properties of words such
as sentiment and generalizing our analysis beyond
binary gender.

Acknowledgement

This work was supported by National Science
Foundation Grant IIS-1760523. We would like to
thank James Zou, Adam Kalai, Tolga Bolukbasi,
and Venkatesh Saligrama for helpful discussion,
and anonymous reviewers for their feedback.



4852

References
Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma,

and Andrej Risteski. 2018. Linear algebraic struc-
ture of word senses, with applications to polysemy.
Transactions of the Association of Computational
Linguistics, 6:483–495.

Sudie E Back, Rebecca L Payne, Annie N Simpson,
and Kathleen T Brady. 2010. Gender and pre-
scription opioids: Findings from the national sur-
vey on drug use and health. Addictive behaviors,
35(11):1001–1007.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
learning to align and translate. arXiv preprint
arXiv:1409.0473.

Tolga Bolukbasi, Kai-Wei Chang, James Y Zou,
Venkatesh Saligrama, and Adam T Kalai. 2016.
Man is to computer programmer as woman is to
homemaker? debiasing word embeddings. In Ad-
vances in Neural Information Processing Systems,
pages 4349–4357.

Samuel R. Bowman, Gabor Angeli, Christopher Potts,
and Christopher D. Manning. 2015. A large an-
notated corpus for learning natural language infer-
ence. In Proceedings of the 2015 Conference on
Empirical Methods in Natural Language Processing
(EMNLP). Association for Computational Linguis-
tics.

Elia Bruni, Gemma Boleda, Marco Baroni, and Nam-
Khanh Tran. 2012. Distributional semantics in tech-
nicolor. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics: Long Papers-Volume 1, pages 136–145.

Aylin Caliskan, Joanna J Bryson, and Arvind
Narayanan. 2017. Semantics derived automatically
from language corpora contain human-like biases.
Science, 356(6334):183–186.

Manaal Faruqui, Jesse Dodge, Sujay K Jauhar, Chris
Dyer, Eduard Hovy, and Noah A Smith. 2014.
Retrofitting word vectors to semantic lexicons.
arXiv preprint arXiv:1411.4166.

Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2001. Placing search in context: The
concept revisited. In Proceedings of the 10th inter-
national conference on World Wide Web, pages 406–
414.

Nikhil Garg, Londa Schiebinger, Dan Jurafsky, and
James Zou. 2017. Word embeddings quantify 100
years of gender and ethnic stereotypes. arXiv
preprint arXiv:1711.08412.

Nikhil Garg, Londa Schiebinger, Dan Jurafsky, and
James Zou. 2018. Word embeddings quantify
100 years of gender and ethnic stereotypes. Pro-
ceedings of the National Academy of Sciences,
115(16):E3635–E3644.

Guy Halawi, Gideon Dror, Evgeniy Gabrilovich, and
Yehuda Koren. 2012. Large-scale learning of word
relatedness with constraints. In Proceedings of
the 18th ACM SIGKDD international conference on
Knowledge discovery and data mining, pages 1406–
1414. ACM.

David A Jurgens, Peter D Turney, Saif M Mohammad,
and Keith J Holyoak. 2012. Semeval-2012 task 2:
Measuring degrees of relational similarity. In Pro-
ceedings of the First Joint Conference on Lexical
and Computational Semantics-Volume 1: Proceed-
ings of the main conference and the shared task, and
Volume 2: Proceedings of the Sixth International
Workshop on Semantic Evaluation, pages 356–364.
Association for Computational Linguistics.

Matthew Kay, Cynthia Matuszek, and Sean A Munson.
2015. Unequal representation and gender stereo-
types in image search results for occupations. In
Human Factors in Computing Systems, pages 3819–
3828.

Kenton Lee, Luheng He, Mike Lewis, and Luke Zettle-
moyer. 2017. End-to-end neural coreference resolu-
tion. EMNLP.

Omer Levy, Yoav Goldberg, and Ido Dagan. 2015. Im-
proving distributional similarity with lessons learned
from word embeddings. Transactions of the Associ-
ation for Computational Linguistics, 3:211–225.

Thang Luong, Richard Socher, and Christopher Man-
ning. 2013. Better word representations with recur-
sive neural networks for morphology. In Proceed-
ings of the Seventeenth Conference on Computa-
tional Natural Language Learning, pages 104–113.

Anna C McFadden, George E Marsh, Barrie Jo Price,
and Yunhan Hwang. 1992. A study of race and gen-
der bias in the punishment of school children. Edu-
cation and treatment of children, pages 140–146.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013a. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in neural information processing
systems, pages 3111–3119.

Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013c. Linguistic regularities in continuous space
word representations. In Proceedings of the 2013
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 746–751.

George Miller and Christiane Fellbaum. 1998. Word-
net: An electronic lexical database.



4853

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 confer-
ence on empirical methods in natural language pro-
cessing (EMNLP), pages 1532–1543.

Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word rep-
resentations. arXiv preprint arXiv:1802.05365.

Kira Radinsky, Eugene Agichtein, Evgeniy
Gabrilovich, and Shaul Markovitch. 2011. A
word at a time: computing word relatedness using
temporal semantic analysis. In Proceedings of the
20th international conference on World wide web,
pages 337–346.

Herbert Rubenstein and John B Goodenough. 1965.
Contextual correlates of synonymy. Communica-
tions of the ACM, 8(10):627–633.

Rachel Rudinger, Chandler May, and Benjamin
Van Durme. 2017. Social bias in elicited natural lan-
guage inferences. In Proceedings of the First ACL
Workshop on Ethics in Natural Language Process-
ing, pages 74–79.

Rachel Rudinger, Jason Naradowsky, Brian Leonard,
and Benjamin Van Durme. 2018. Gender bias in
coreference resolution. In NAACL.

Cicero dos Santos and Maira Gatti. 2014. Deep con-
volutional neural networks for sentiment analysis
of short texts. In Proceedings of COLING 2014,
the 25th International Conference on Computational
Linguistics: Technical Papers, pages 69–78.

Simon Šuster, Ivan Titov, and Gertjan Van Noord.
2016. Bilingual learning of multi-sense embed-
dings with discrete autoencoders. arXiv preprint
arXiv:1603.09128.

Latanya Sweeney. 2013. Discrimination in online ad
delivery. Queue, 11(3):10.

Shyam Upadhyay, Kai-Wei Chang, Matt Taddy, Adam
Kalai, and James Zou. 2017. Beyond bilingual:
Multi-sense word embeddings using multilingual
context. In Proceedings of the 2nd Workshop on
Representation Learning for NLP, pages 101–110.

Byron C Wallace and Michael J Paul. 2016. jerk or
judgemental? patient perceptions of male versus fe-
male physicians in online reviews.

Ralph Weischedel, Sameer Pradhan, Lance Ramshaw,
Jeff Kaufman, Michelle Franchini, Mohammed El-
Bachouti, Nianwen Xue, Martha Palmer, Jena D
Hwang, Claire Bonial, et al. 2012. Ontonotes re-
lease 5.0.

Sirui Yao and Bert Huang. 2017. Beyond parity: Fair-
ness objectives for collaborative filtering. In Ad-
vances in Neural Information Processing Systems,
pages 2925–2934.

Daojian Zeng, Kang Liu, Yubo Chen, and Jun Zhao.
2015. Distant supervision for relation extraction via
piecewise convolutional neural networks. In Pro-
ceedings of the 2015 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1753–
1762.

Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente
Ordonez, and Kai-Wei Chang. 2017. Men also
like shopping: Reducing gender bias amplifica-
tion using corpus-level constraints. arXiv preprint
arXiv:1707.09457.

Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Or-
donez, and Kai-Wei Chang. 2018. Gender bias in
coreference resolution: Evaluation and debiasing
methods. NAACL.


