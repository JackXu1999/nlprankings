










































Ensemble Triangulation for Statistical Machine Translation


International Joint Conference on Natural Language Processing, pages 252–260,
Nagoya, Japan, 14-18 October 2013.

Ensemble Triangulation for Statistical Machine Translation∗

Majid Razmara and Anoop Sarkar
School of Computing Science

Simon Fraser University
Burnaby, BC, Canada

{razmara,anoop}@sfu.ca

Abstract

State-of-the-art statistical machine transla-
tion systems rely heavily on training data
and insufficient training data usually re-
sults in poor translation quality. One so-
lution to alleviate this problem is triangu-
lation. Triangulation uses a third language
as a pivot through which another source-
target translation system can be built. In
this paper, we dynamically create multi-
ple such triangulated systems and combine
them using a novel approach called ensem-
ble decoding. Experimental results of this
approach show significant improvements
in the BLEU score over the direct source-
target system. Our approach also outper-
forms a strong linear mixture baseline.

1 Introduction

The objective of current statistical machine trans-
lation (SMT) systems is to build cheap and rapid
corpus-based SMT systems without involving hu-
man translation expertise. Such SMT systems
rely heavily on their training data. State-of-the-
art SMT systems automatically extract transla-
tion rules (e.g. phrase pairs), learn segmentation
models, re-ordering models, etc. and find tuning
weights solely from data and hence they rely heav-
ily on high quality training data. There are many
language pairs for which there is no parallel data
or the available data is not sufficiently large to
build a reliable SMT system. For example, there is
no Chinese-Farsi parallel text, although there ex-
ists sufficient parallel data between these two lan-
guages and English. For SMT, an important re-
search direction is to improve the quality of trans-
lation when there is no, insufficient or poor-quality
parallel data between a pair of languages.

∗This research was partially supported by an NSERC,
Canada (RGPIN: 264905) grant and a Google Faculty Award
to the second author.

One approach that has been recently proposed
is triangulation. Triangulation is the process of
translating from a source language to a target lan-
guage via an intermediate language (aka pivot, or
bridge). This is very useful specifically for low-
resource languages as SMT systems built using
small parallel corpora perform poorly due to data
sparsity. In addition, ambiguities in translating
from one language into another may disappear if a
translation into some other language is available.

One obvious benefit of triangulation is to in-
crease the coverage of the model on the input
text. In other words, we can reduce the number of
out-of-vocabulary words (OOVs), which are a ma-
jor cause of poor quality translations, using other
paths to the target language. This can be espe-
cially helpful when the model is built using a small
amount of parallel data.

Figure 1 shows how triangulation can be useful
in reducing the number of OOVs when translating
from French to English through three pivot lan-
guages: Spanish (es), German (de) and Italian (it).
The solid lines show the number of OOVs for a
direct MT system with regard to a multi-language
parallel test set (Section 6.2 contains the details
about the data sets) and the dotted lines show the
OOVs in the triangulated (src � pvt � tgt) sys-
tems. The number of OOVs on triangulated paths
can never be less that the first edge (i.e. src � pvt)
and it is usually higher than the second edge (i.e.
pvt � tgt) as well. Thus, the choice of intermediate
language is very important in triangulation.

Figure 1 also shows how combining multiple
triangulated systems can reduce this number from
2600 (16%) OOVs to 1536 (9%) OOVs. Thus,
combining triangulated systems with the original
src � tgt system is a good idea. When combining
multiple systems, the upper bound on the number
of OOVs is the minimum among all OOVs in the
different triangulations. These OOV rates provide
useful hints, among other clues, as to which pivot

252



languages will be more useful. In Figure 1, we can
expect Italian (it) to help more than Spanish (es)
and both to help more than German (de) in trans-
lation from French (fr) to English (en), which we
confirmed in our experimental results (Table 1).

In addition to providing translations for other-
wise untranslatable phrases, triangulation can find
new translations for current phrases. The condi-
tional distributions used for the translation model
have been estimated on small amounts of data and
hence are not robust due to data sparseness. Using
triangulation, these distributions are smoothed and
become more reliable as a result.

For each pivot language for which there exists
parallel data with the source and the target lan-
guage, we can create a src � tgt system by bridg-
ing through the pivot language. If there are a
number of such pivot languages with correspond-
ing data, we can use mixture approaches to com-
bine them in order to build a stronger model. We
propose to apply the ensemble decoding approach
of (Razmara et al., 2012) in this triangulation sce-
nario. Ensemble decoding allows us to combine
hypotheses from different models dynamically at
the decoder. We experimented with 12 different
language pairs and 3 pivot languages for each of
them. Experimental results of this approach show
significant improvements in the BLEU and ME-
TEOR scores over the direct source-target system
in all the 12 language pairs. We also compare to a
strong linear mixture baseline.

2 Related Work

Use of pivot languages in machine translation
dates back to the early days of machine transla-
tion. Boitet (1988) discusses the choice of pivot
languages, natural or artificial (e.g. interlingua), in
machine translation. Schubert (1988) argues that
a proper choice for an intermediate language for
high-quality machine translation is a natural lan-
guage due to the inherent lack of expressiveness
in artificial languages. Previous work in applying
pivot languages in machine translation can be cat-
egorized into these divisions:

2.1 System Cascades

In this approach, a src � pvt translation system
translates the source input into the pivot language
and a second pvt � tgt system takes the output of
the previous system and translates it into the tar-
get language. Utiyama and Isahara (2007) use this

es

de

fr en

it

25
77

35
39

2554

2600

3201
4076

30
06

3339

4370

3262

direct (fr-en) 2600 (16%)
triangulated (fr-{es, de, it}-en) 2066 (12%)
direct + triangulated 1536 (9%)

Figure 1: Number of OOVs when translating di-
rectly from fr to en (solid lines), triangulating
through es, de or it individually (dotted lines),
and when combining multiple triangulation sys-
tems with the direct system. OOV numbers are
based on a multi-language parallel test set and the
models are built on small corpora (10k sentence
pairs), which are not multi-parallel.

approach to triangulate between Spanish, German
and French through English. However, instead of
using only the best translation, they took the n-best
translations and translated them into the target lan-
guage. MERT (Och, 2003) has been used to tune
the weights for the new feature set which consists
of src � pvt and pvt � tgt feature functions. The
highest scoring sentence from the target language
is used as the final translation. They showed that
using 15 hypotheses in the pvt side is generally su-
perior to using only one best hypothesis.

2.2 Corpus Synthesis

Given a pvt � tgt MT system, one can translate the
pivot side of a src-pvt parallel corpus into the tar-
get language and create a noisy src-tgt parallel cor-
pus. This can also be exploited in the other direc-
tion, meaning that a pvt � src MT system can be
used to translate the pivot side of a pvt-tgt bitext.
de Gispert and Marino (2006), for example, trans-
lated the Spanish side of an English-Spanish bitext
into Catalan using an available Spanish-Catalan
SMT system. Then, they built an English-Catalan
MT system by training on this new parallel corpus.

253



2.3 Phrase-Table Triangulation
In this approach, instead of translating the input
sentences from a source language to a pivot lan-
guage and from that to a target language, triangu-
lation is done on the phrase level by triangulating
two phrase-tables: src � pvt and pvt � tgt:

(f̄ , ē) ∈ TF�E ⇐⇒

∃ī : (f̄ , ī) ∈ TF�I ∧ (̄i, ē) ∈ TI�E

where f̄ , ī and ē are phrases in the source F , pivot
I and target E languages respectively and T is a
set representing a phrase table.

Utiyama and Isahara (2007) also experimented
with phrase-table triangulation. They compared
both triangulation approaches when using Span-
ish, French and German as the source and target
languages and English as the only pivot language.
They showed that phrase-table triangulation is su-
perior to the MT system cascades but both of them
did not outperform the direct src � tgt system.

The phrase-table triangulation approach with
multiple pivot languages has been also investi-
gated in several work (Cohn and Lapata, 2007;
Wu and Wang, 2007). These triangulated phrase-
tables are combined together using linear and log-
linear mixture models. They also successfully
combined the mixed phrase-table with a src-tgt
phrase-table to achieve a higher BLEU score.

Bertoldi et al. (2008) formulated phrase triangu-
lation in the decoder where they also consider the
phrase-segmentation model between src-pvt and
the reordering model between src-tgt.

Beside machine translation, the use of pivot lan-
guages has found applications in other NLP ar-
eas. Gollins and Sanderson (2001) used a similar
idea in cross-lingual information retrieval where
query terms were translated through multiple pivot
languages to the target language and the trans-
lations are combined to reduce the error. Pivot
languages have also been successfully used in in-
ducing translation lexicons (Mann and Yarowsky,
2001) as well as word alignments for resource-
poor languages (Kumar et al., 2007; Wang et al.,
2006). Callison-Burch et al. (2006) used pivot lan-
guages to extract paraphrases for unknown words.

3 Baselines

In this paper, we compare our approach with two
baselines. A simple baseline is the direct system

between the source and target languages which is
trained on the same amount of parallel data as the
triangulated ones. In addition, we implemented a
phrase-table triangulation method (Cohn and Lap-
ata, 2007; Wu and Wang, 2007; Utiyama and Isa-
hara, 2007). This approach presents a probabilistic
formulation for triangulation by marginalizing out
the pivot phrases, and factorizing using the chain
rule:

p(ē | f̄) =
∑

ī

p(ē, ī | f̄)

=
∑

ī

p(ē | ī, f̄) p(̄i | f̄)

≈
∑

ī

p(ē | ī) p(̄i | f̄)

where f̄ , ē and ī are phrases in the source, tar-
get and intermediate language respectively. In this
equation, a conditional independence assumption
has been made that source f̄ and target phrases
ē are independent given their corresponding pivot
phrase(s) ī. The equation requires that all phrases
in the src � pvt direction must also appear in pvt �
tgt. All missing phrases are simply dropped from
the final phrase-table.

Using this approach, a triangulated source-
target phrase-table is generated for each pivot lan-
guage. Then, linear and log-linear mixture meth-
ods are used to combine these phrase-tables into
a single phrase-table in order to be used in the
decoder. We implemented the linear mixture ap-
proach, since linear mixtures often outperform
log-linear ones (Cohn and Lapata, 2007). We then
compare the results of these baselines with our ap-
proach over multiple language pairs (Section 6.2).
In linear mixture models, each feature in the mix-
ture phrase-table is computed as a linear interpo-
lation of corresponding features in the component
phrase-tables using a weight vector ~λ.

p(ē | f̄) =
∑

i

λi pi(ē | f̄)

p(f̄ | ē) =
∑

i

λi pi(f̄ | ē)

∀ λi > 1
∑

i

λi = 1

Following Cohn and Lapata (2007), we com-
bined triangulated phrase-tables with uniform
weights into a single phrase table and then interpo-
lated it with the phrase-table of the direct system.

254



4 Ensemble Decoding

SMT log-linear models (Koehn, 2010) find the
most likely target language output e given the
source language input f using a vector of feature
functions φ:

p(e|f) ∝ exp
(
w · φ

)
Ensemble decoding combines several models

dynamically at the decoding time. The scores
are combined for each partial hypothesis using a
user-defined mixture operation � over component
models.

p(e|f) ∝ exp
(
w1 · φ1 �w2 · φ2 � . . .

)
Razmara et al. (2012) successfully applied en-

semble decoding to domain adaptation in SMT
and showed that it performed better than ap-
proaches that pre-compute linear mixtures of dif-
ferent models. Several mixture operations were
proposed, allowing the user to encode belief about
the relative strengths of the component models.
These mixture operations receive two or more
probabilities and return the mixture probability
p(ē | f̄) for each rule f̄ → ē used in the decoder.
Different options for these operations are:

• Weighted Sum (wsum) is defined as:

p(ē | f̄) ∝
M∑
m

λm exp
(
wm · φm

)
where m denotes the index of component
models, M is the total number of them and
λm is the weight for component m.
• Weighted Max (wmax) is defined as:

p(ē | f̄) ∝ max
m

(
λm exp

(
wm · φm

))
• Model Switching (Switch): Each cell in the

CKY chart is populated only by rules from
one of the models and the other models’ rules
are discarded. Each component model is con-
sidered an expert on different spans of the
source. A binary indicator function δ(f̄ ,m)
picks a component model for each span:

δ(f̄ ,m) =


1, m = argmax

n∈M
ψ(f̄ , n)

0, otherwise

The criteria for choosing a model for each
cell, ψ(f̄ , n), is based on max top score, i.e.

for each cell, the model that has the highest
weighted best-rule score wins:

ψ(f̄ , n) = λn max
e

(wn · φn(ē, f̄))

The probability of each phrase-pair (ē, f̄) is
then:

p(ē | f̄) =
M∑
m

δ(f̄ ,m) pm(ē | f̄)

5 Our Approach

5.1 Dynamic Triangulation
Given a src � pvt and a pvt � tgt system which
are independently trained and tuned on their cor-
responding parallel data, these two systems can be
triangulated dynamically in the decoder.

For each source phrase f̄ , the decoder consults
the src � pvt system to get its translations on the
pivot side ī with their scores. Consequently, each
of these pivot-side translation phrases is queried
from the pvt � tgt system to obtain their transla-
tions on the target side with their corresponding
scores. Finally a (f̄ , ē) pair is constructed from
each (f̄ , ī) and (̄i, ē) pair, whose score is com-
puted as:

pI (f̄ | ē) ∝

max
ī

exp
(
w1 . φ1(f̄ , ī)︸ ︷︷ ︸

F�I

+ w2 . φ2(̄i , ē)︸ ︷︷ ︸
I�E

)

This method requires the language model score
of the src � pvt system. However for simplic-
ity we do not use the pivot-side language mod-
els and hence the score of the src � pvt system
does not include the language model and word
penalty scores. In this formulation for a given
source and target phrase pair (f̄ , ē), if there are
multiple bridging pivot phrases ī, we only use the
one that yields the highest score. This is in contrast
with previous work where they take the sum over
all such pivot phrases (Cohn and Lapata, 2007;
Utiyama and Isahara, 2007). We use max as it out-
performs sum in our preliminary experiments.

It is noteworthy that in computing the score for
pI (f̄ | ē), the scores from src � pvt and pvt � tgt
are added uniformly. However, there is no reason
why this should be the case. Two different weights
can be assigned to these two scores to highlight the
importance of one against the other one.

255



A naive implementation of phrase-triangulation
in the decoder would require O(n2) steps for each
source sub-span, where n is the average number
of translation fan-out (i.e. possible translations)
for each phrase. However, since the phrase can-
didates from both src � pvt and pvt � tgt are al-
ready sorted, we use a lazy algorithm that reduces
the computational complexity to O(n).

5.2 Combining Triangulated Systems

If we can make use of multiple pivot languages,
a system can be created on-the-fly for each pivot
language by triangulation and these systems can
then be combined together in the decoder using
ensemble decoding discussed in Section 4. Fol-
lowing previous work, these triangulated phrase-
tables can also be combined with the direct system
to produce a yet stronger model. However, we do
not combine them in two steps. Instead, all trian-
gulated systems and the direct one are combined
together in a single step.

Ensemble decoding is aware of full model
scores when it compares, ranks and prunes hy-
potheses. This includes the language model, word,
phrase and glue rule penalty scores as well as stan-
dard phrase-table probabilities.

Since ensemble decoding combines the scores
of common hypotheses across multiple systems
rather than combining their feature values as in
mixture models, it can be used to triangulate het-
erogeneous systems such as phrase-based, hierar-
chical phrase-based, and syntax-based with com-
pletely different feature types. Considering that
ensemble decoding can be used in these diverse
scenarios, it offers an attractive alternative to cur-
rent phrase-table triangulation systems.

5.3 Tuning Component Weights

Component weights control the contribution of
each model in the ensemble. A tuning proce-
dure should assign higher weights to the models
that produce higher quality translations and lower
weights to weak models in order to control their
noise propagation in the ensemble. In the ensem-
ble decoder, since we do not have explicit gradient
information for the objective function, we use a di-
rect optimizer for tuning. We used Condor (Van-
den Berghen and Bersini, 2005) which is a pub-
licly available toolkit based on Powell’s algorithm.

The ensemble between three triangulated mod-
els and a direct one requires tuning in a 4-

dimensional space, one for each system. If, on
average, the tuner evaluates the decoder n times
in each direction in the optimization space, there
needs to be n4 ensemble decoder evaluations,
which is very time consuming. Instead, we re-
sorted to a simpler approach for tuning: each tri-
angulated model is separately tuned against the di-
rect model with a fixed weights (we used a weight
of 1). In other words, three ensemble models are
created, each on a single triangulated model plus
the direct one. These ensembles are separately
tuned and once completed, these weights comprise
the final tuned weights. Thus, the total number
of ensemble evaluations reduces from O(n4) to
O(3n).

In addition to this significant complexity reduc-
tion, this method enables parallelism in tuning,
since the three individual tuning branches can now
be run independently. The final tuned weights are
not necessarily a local optima and one can run fur-
ther optimization steps around this point to get to
even better solutions which should lead to higher
BLEU scores.

6 Experiments & Results

6.1 Experimental Setup

For our experiments, we used the Europarl cor-
pus (v7) (Koehn, 2005) for training sets and
ACL/WMT 20051 data for dev/test sets (2k sen-
tence pairs) following Cohn and Lapata (2007).
Our goal in this paper was to understand how
multiple languages can help in triangulation, the
improvement in coverage of the unseen data due
to triangulation, and the importance of choosing
the right languages as pivot languages. Thus, we
needed to run experiments on a large number of
language pairs, and for each language pair we
wanted to work with many pivot languages. To
this end, we created small sub-corpora from Eu-
roparl by sampling 10,000 sentence pairs and con-
ducted our experiments on them. As we will show,
using larger data than this would result in pro-
hibitively large triangulated phrase tables. Table 2
shows the number of words on both sides of used
language pairs in our corpora.

The ensemble decoder is built on top of an in-
house implementation of a Hiero-style MT sys-
tem (Chiang, 2005) called Kriya (Sankaran et
al., 2012). This Hiero decoder obtains BLEU

1http://www.statmt.org/wpt05/mt-shared-task/

256



src↓ tgt →

de
pi

vo
ts

en
es
fr
it

direct
mixture
wmax
wsum
switch

en es fr
– 15.94 13.62

14.47 – 13.43
14.39 13.45 –
14.14 14.90 11.67
21.94 20.70 17.37
21.86 22.30 18.28
22.49 21.32 18.22
22.22 21.42 17.98
22.59 21.80 17.70

src↓ tgt →

en

pi
vo

ts

de
es
fr
it

direct
mixture
wmax
wsum
switch

de es fr
– 20.47 17.38

12.95 – 20.78
14.09 23.25 –
13.00 23.18 19.02
17.57 28.81 24.58
17.91 28.89 24.30
17.77 29.17 25.39
17.68 29.33 24.70
17.77 29.32 24.98

src↓ tgt →

es

pi
vo

ts

de
en
fr
it

direct
mixture
wmax
wsum
switch

de en fr
– 18.84 23.28

14.50 – 18.55
12.48 22.81 –
13.69 23.14 23.44
16.30 28.11 29.83
17.75 28.99 29.47
17.34 29.23 30.54
16.79 28.79 30.12
16.53 29.16 29.68

src↓ tgt →

fr

pi
vo

ts

de
en
es
it

direct
mixture
wmax
wsum
switch

de en es
– 20.15 22.96

14.84 – 27.84
14.35 23.59 –
14.08 24.08 30.38
16.56 28.79 35.27
17.39 28.83 35.27
17.67 29.95 36.07
17.41 28.62 35.98
17.78 28.79 36.33

Table 1: Results of i) single-pivot triangulation; ii) baseline systems including direct systems and linear
mixture of triangulated phrase-tables; iii) ensemble triangulation results based on different mixture op-
erations. The mixture and ensemble methods are based on multi-pivot triangulation. These methods are
built on 10k sentence-pair corpora.

L1 - L2 L1 tokens (K) L2 tokens (K)

de - en 232 249
de - es 232 263
de - fr 231 259
de - it 245 253
en - es 250 264
en - fr 251 262
en - it 260 251
es - fr 262 261
es - it 274 252
fr - it 272 251

Table 2: Number of tokens in each language pair
in the training data.

scores equal to or better than the state-of-the-art in
phrase-based and hierarchical phrase-based trans-
lation over a wide variety of language pairs and
data sets. It uses the following standard fea-
tures: forward and backward relative-frequency
and lexical TM probabilities; LM; word, phrase
and glue-rules penalty. GIZA++ (Och and Ney,

2000) has been used for word alignment with
phrase length limit of 10. In both systems, feature
weights were optimized using MERT (Och, 2003).
We used the target sides of the Europarl corpus
(2M sentences) to build 5-gram language models
and smooth them using the Kneser-Ney method.
We used SRILM (Stolcke, 2002) as the language
model toolkit.

6.2 Results

Table 1 shows the BLEU scores when using two
languages from {fr, en, es, de} as source and tar-
get, and the other two languages plus it as inter-
mediate languages. The first group of numbers
are BLEU scores for triangulated systems through
the specified pivot language. For example, trans-
lating from de to es through en (i.e. de � en �
es) gets 15.94% BLEU score. The second group
shows the BLEU scores of the baseline systems in-
cluding the direct system between the source and
target languages and the linear mixture baseline of
the three triangulated systems. The BLEU scores
of ensemble decoding using different mixture op-

257



90

92

94

96

98

de
 - e

n 

de
 - e

s 

de
 - f

r 

en 
- d

e 

en 
- e

s 

en 
- fr

 

es 
- d

e 

es 
- e

n 

es 
- fr

 

fr -
 de

 

fr -
 en

 

fr -
 es

 

C
o

ve
ra

g
e

 (
%

) 

 direct   triangulated(3)  triangulated(3) + direct 

direct 478K 393K 403K 665K 1,084K 1,155K 479K 927K 1,319K 394K 743K 976K
tri + direct 83M 102M 132M 113M 103M 133M 129M 101M 152M 141M 109M 129M

Figure 2: Coverage for i) direct system; ii) combined triangulated system with three 3 languages; and
iii) the combination of the triangulated phrase-tables and the direct one. The table shows the number of
rules for each system and language pair after filtering based on the source side of the test set.

erations are illustrated at the bottom.
As the table shows, our approach outperforms

the direct systems in all the 12 language pairs
while the mixture model systems fail to improve
over the direct system baseline for some of the
language pairs. Our approach also outperforms
the mixture models in most cases. Overall, en-
semble decoding with wmax as mixture opera-
tion performs the best among the different systems
and baselines. Figure 3 shows the average of the
BLEU score of the direct system, mixture models
and wmax on all 12 systems. On average the wmax
method obtains 0.33 BLEU points higher than the
mixture models.

24.6 

24.27 

23.82 

22.5 23 23.5 24 24.5 

ensemble 
(wmax) 

mixture 

direct 

BLEU 

Figure 3: The average BLEU scores of the direct
system, mixture models and wmax ensemble trian-
gulation approach over all 12 language pairs.

We also computed the Meteor scores
(Denkowski and Lavie, 2011) for all systems

and the results are summarized in Figure 4. As the
figure illustrates, our ensemble decoding approach
with wmax outperforms the mixture models in 11
of 12 language pairs based on Meteor scores.

6.3 Phrase table coverage

Figure 2 shows the phrase-table coverage of the
test set for different language pairs. The cover-
age is defined as the percentage of unigrams in
the source side of the test set for which the cor-
responding phrase-table has translations for. The
first set of bars shows the coverage of the direct
systems and the second one shows that of the
combined triangulated systems for three pivot lan-
guages. Finally, the last set of bars indicate the
coverage when the direct phrase-table is combined
with the triangulated ones. In all language pairs,
the combined triangulated phrase-tables have a
higher coverage compared to the direct phrase-
tables. As expected, the coverage increases when
these two phrase-tables are aggregated. The ta-
ble below the figure shows the number of rules for
each system and language pair after filtering out
based on the source side of the test set. This illus-
trates why running experiments on larger sizes of
parallel data is prohibitive for hierarchical phrase-
based models.

258



-1 

-0.5 

0 

0.5 

1 

1.5 

2 

de
-e

n 

de
-e

s 

de
-fr

 

en
-d

e 

en
-e

s 

en
-fr

 

es
-d

e 

es
-e

n 
es

-fr
 

fr-
de

 

fr-
en

 
fr-

es
 

M
ET
EO

R	  
Di
ff	  

mixture	   wmax	  

Figure 4: Meteor score difference between mixture models and direct systems as well as the difference
between ensemble decoding approach with wmax and the direct system.

6.3.1 Choice of Pivot Language
Cohn and Lapata (2007) showed that the pivot lan-
guage should be close to the source or the tar-
get language in order to be effective. For exam-
ple, when translating between Romance languages
(Italian, Spanish, etc.), the pivot language should
also be a Romance language. In addition to those
findings, based on the results presented in Table 1,
here are some observations for these five European
languages:

• When translating from or to de, en is the best
pivot language;

• Generally de is not a suitable pivot language
for any translation pair;

• When translating from en to any other lan-
guage, fr is the best pivot;

• it is the best intermediate language when
translating from fr or es to other languages;
except when translating to de for which en is
the best pivot language (c.f. first finding);

7 Conclusion and Future Work

In the paper, we introduced a novel approach for
triangulation which does phrase-table triangula-
tion and model combination on-the-fly in the de-
coder. Ensemble decoder uses the full hypothesis
score for triangulation and combination and hence
is able to mix hypotheses from heterogeneous sys-
tems.

Another advantage of this method to the phrase-
table triangulation approach is that our method is

applicable even when there exists no parallel data
between source and target languages for tuning be-
cause we only use the src-tgt tuning set to opti-
mize hyper-parameters, though phrase-table trian-
gulation methods use it to learn MT log-linear fea-
ture weights for which having a tuning set is much
more essential. Empirical results also showed that
this method with wmax outperforms the baselines.

Future work includes imposing restrictions on
the generated triangulated rules in order to keep
only ones that have a strong support from the word
alignments. By exploiting such constraints, we
can experiment with larger sizes of parallel data.
Specifically, a more natural experimental setup for
triangulation which we would like to try is to use
a small direct system with big src � pvt and pvt
� tgt systems. This resembles the actual situation
for resource-poor language pairs. We will also ex-
periment with higher number of pivot languages.

Currently, most research in this area focuses on
triangulation on paths containing only one pivot
language. We can also analyze our method when
using more languages in the triangulation chain
and see whether there would any gain in doing
such.

Finally, in current methods all (f̄ , ī) phrase
pairs of the src � pvt systems, for which there does
not exist any (̄i, ē) pair in pvt � tgt are simply dis-
carded. However in most cases, such ī phrases can
be segmented into smaller phrases (or rules for Hi-
ero systems) to be triangulated via them. This seg-
mentation is a decoding problem which requires
an efficient algorithm to be practical.

259



References
N. Bertoldi, M. Barbaiani, M. Federico, and R. Cattoni.

2008. Phrase-based statistical machine translation
with pivot languages. Proceeding of IWSLT, pages
143–149.

C. Boitet. 1988. Pros and cons of the pivot and trans-
fer approaches in multilingual machine translation.
Maxwell et al.(1988), pages 93–106.

C. Callison-Burch, P. Koehn, and M. Osborne. 2006.
Improved statistical machine translation using para-
phrases. In Proceedings of the main conference
on Human Language Technology Conference of the
North American Chapter of the Association of Com-
putational Linguistics, pages 17–24. Association for
Computational Linguistics.

David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In ACL
’05: Proceedings of the 43rd Annual Meeting on As-
sociation for Computational Linguistics, pages 263–
270, Morristown, NJ, USA. ACL.

Trevor Cohn and Mirella Lapata. 2007. Machine
translation by triangulation: Making effective use of
multi-parallel corpora. In Proceedings of the 45th
Annual Meeting of the Association of Computational
Linguistics, pages 728–735, Prague, Czech Repub-
lic, June. Association for Computational Linguis-
tics.

A. de Gispert and J.B. Marino. 2006. Catalan-english
statistical machine translation without parallel cor-
pus: bridging through spanish. In Proc. of 5th In-
ternational Conference on Language Resources and
Evaluation (LREC), pages 65–68.

Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic Metric for Reliable Optimization
and Evaluation of Machine Translation Systems. In
Proceedings of the EMNLP 2011 Workshop on Sta-
tistical Machine Translation.

T. Gollins and M. Sanderson. 2001. Improving cross
language retrieval with triangulated translation. In
Proceedings of the 24th annual international ACM
SIGIR conference on Research and development in
information retrieval, pages 90–95. ACM.

Martin Kay. 1997. The proper place of men and ma-
chines in language translation. Machine Transla-
tion, 12(1/2):3–23.

P. Koehn. 2005. Europarl: A parallel corpus for statis-
tical machine translation. In MT summit, volume 5.

Philipp Koehn. 2010. Statistical Machine Translation.
Cambridge University Press, New York, NY, USA,
1st edition.

Shankar Kumar, Franz Josef Och, and Wolfgang
Macherey. 2007. Improving word alignment with
bridge languages. In EMNLP-CoNLL, pages 42–50.
ACL.

Gideon S. Mann and David Yarowsky. 2001. Mul-
tipath translation lexicon induction via bridge lan-
guages. In Proceedings of the second meeting of
the North American Chapter of the Association for
Computational Linguistics on Language technolo-
gies, NAACL ’01, pages 1–8, Stroudsburg, PA,
USA.

F. J. Och and H. Ney. 2000. Improved statistical align-
ment models. In Proceedings of the 38th Annual
Meeting of the ACL, pages 440–447, Hongkong,
China, October.

Franz Josef Och. 2003. Minimum error rate training
for statistical machine translation. In Proceedings of
the 41th Annual Meeting of the ACL, Sapporo, July.
ACL.

Majid Razmara, George Foster, Baskaran Sankaran,
and Anoop Sarkar. 2012. Mixing multiple transla-
tion models in statistical machine translation. In The
50th Annual Meeting of the Association for Compu-
tational Linguistics, Proceedings of the Conference,
July 8-14, 2012, Jeju Island, Korea - Volume 1: Long
Papers, pages 940–949. The Association for Com-
puter Linguistics.

Baskaran Sankaran, Majid Razmara, and Anoop
Sarkar. 2012. Kriya – an end-to-end hierarchical
phrase-based mt system. The Prague Bulletin of
Mathematical Linguistics, 97(97), April.

K. Schubert. 1988. Implicitness as a guiding principle
in machine translation. In Proceedings of the 12th
conference on Computational linguistics-Volume 2,
pages 599–601. Association for Computational Lin-
guistics.

Andreas Stolcke. 2002. SRILM – an extensible lan-
guage modeling toolkit. In Proceedings Interna-
tional Conference on Spoken Language Processing,
pages 257–286.

M. Utiyama and H. Isahara. 2007. A comparison of
pivot methods for phrase-based statistical machine
translation. In Proceedings of NAACL-HLT, vol-
ume 7, pages 484–491.

Frank Vanden Berghen and Hugues Bersini. 2005.
CONDOR, a new parallel, constrained extension of
powell’s UOBYQA algorithm: Experimental results
and comparison with the DFO algorithm. Journal of
Computational and Applied Mathematics, 181:157–
175, September.

H. Wang, H. Wu, and Z. Liu. 2006. Word alignment
for languages with scarce resources using bilingual
corpora of other language pairs. In Proceedings of
the COLING/ACL on Main conference poster ses-
sions, pages 874–881. Association for Computa-
tional Linguistics.

H. Wu and H. Wang. 2007. Pivot language ap-
proach for phrase-based statistical machine transla-
tion. Machine Translation, 21(3):165–181.

260


