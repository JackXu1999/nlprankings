



















































Recurrent Neural Networks as Weighted Language Recognizers


Proceedings of NAACL-HLT 2018, pages 2261–2271
New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics

Recurrent Neural Networks as Weighted Language Recognizers
Yining Chen

Dartmouth College
yining.chen.18@dartmouth.edu

Sorcha Gilroy
ILCC

University of Edinburgh
s.gilroy@sms.ed.ac.uk

Andreas Maletti
Institute of Computer Science

Universität Leipzig
andreas.maletti@uni-leipzig.de

Jonathan May
Information Sciences Institute

University of Southern California
jonmay@isi.edu

Kevin Knight
Information Sciences Institute

University of Southern California
knight@isi.edu

Abstract

We investigate the computational complexity
of various problems for simple recurrent neu-
ral networks (RNNs) as formal models for
recognizing weighted languages. We focus
on the single-layer, ReLU-activation, rational-
weight RNNs with softmax, which are com-
monly used in natural language processing ap-
plications. We show that most problems for
such RNNs are undecidable, including con-
sistency, equivalence, minimization, and the
determination of the highest-weighted string.
However, for consistent RNNs the last prob-
lem becomes decidable, although the solution
length can surpass all computable bounds. If
additionally the string is limited to polynomial
length, the problem becomes NP-complete. In
summary, this shows that approximations and
heuristic algorithms are necessary in practical
applications of those RNNs.

1 Introduction

Recurrent neural networks (RNNs) are an attrac-
tive apparatus for probabilistic language model-
ing (Mikolov and Zweig, 2012). Recent exper-
iments show that RNNs significantly outperform
other methods in assigning high probability to
held-out English text (Jozefowicz et al., 2016).

Roughly speaking, an RNN works as follows.
At each time step, it consumes one input token,
updates its hidden state vector, and predicts the
next token by generating a probability distribution
over all permissible tokens. The probability of an
input string is simply obtained as the product of
the predictions of the tokens constituting the string
followed by a terminating token. In this man-
ner, each RNN defines a weighted language; i.e.
a total function from strings to weights. Siegel-
mann and Sontag (1995) showed that single-layer
rational-weight RNNs with saturated linear acti-
vation can compute any computable function. To

this end, a specific architecture with 886 hidden
units can simulate any Turing machine in real-time
(i.e., each Turing machine step is simulated in a
single time step). However, their RNN encodes
the whole input in its internal state, performs the
actual computation of the Turing machine when
reading the terminating token, and then encodes
the output (provided an output is produced) in a
particular hidden unit. In this way, their RNN al-
lows “thinking” time (equivalent to the computa-
tion time of the Turing machine) after the input has
been encoded.

We consider a different variant of RNNs that is
commonly used in natural language processing ap-
plications. It uses ReLU activations, consumes an
input token at each time step, and produces soft-
max predictions for the next token. It thus imme-
diately halts after reading the last input token and
the weight assigned to the input is simply the prod-
uct of the input token predictions in each step.

Other formal models that are currently used to
implement probabilistic language models such as
finite-state automata and context-free grammars
are by now well-understood. A fair share of
their utility directly derives from their nice algo-
rithmic properties. For example, the weighted
languages computed by weighted finite-state au-
tomata are closed under intersection (pointwise
product) and union (pointwise sum), and the cor-
responding unweighted languages are closed un-
der intersection, union, difference, and comple-
mentation (Droste et al., 2013). Moreover, toolkits
like OpenFST (Allauzen et al., 2007) and Carmel1

implement efficient algorithms on automata like
minimization, intersection, finding the highest-
weighted path and the highest-weighted string.

RNN practitioners naturally face many of
these same problems. For example, an RNN-

1https://www.isi.edu/licensed-sw/carmel/

2261



based machine translation system should ex-
tract the highest-weighted output string (i.e., the
most likely translation) generated by an RNN,
(Sutskever et al., 2014; Bahdanau et al., 2014).
Currently this task is solved by approxima-
tion techniques like heuristic greedy and beam
searches. To facilitate the deployment of large
RNNs onto limited memory devices (like mobile
phones) minimization techniques would be bene-
ficial. Again currently only heuristic approaches
like knowledge distillation (Kim and Rush, 2016)
are available. Meanwhile, it is unclear whether we
can determine if the computed weighted language
is consistent; i.e., if it is a probability distribution
on the set of all strings. Without a determination
of the overall probability mass assigned to all fi-
nite strings, a fair comparison of language models
with regard to perplexity is simply impossible.

The goal of this paper is to study the above
problems for the mentioned ReLU-variant of
RNNs. More specifically, we ask and answer the
following questions:
• Consistency: Do RNNs compute consistent

weighted languages? Is the consistency of the
computed weighted language decidable?
• Highest-weighted string: Can we (efficiently)

determine the highest-weighted string in a
computed weighted language?
• Equivalence: Can we decide whether two

given RNNs compute the same weighted lan-
guage?
• Minimization: Can we minimize the number

of neurons for a given RNN?

2 Definitions and notations

Before we introduce our RNN model formally, we
recall some basic notions and notation. An alpha-
bet Σ is a finite set of symbols, and we write |Σ|
for the number of symbols in Σ. A string s over
the alphabet Σ is a finite sequence of zero or more
symbols drawn from Σ, and we write Σ∗ for the set
of all strings over Σ, of which � is the empty string.
The length of the string s ∈ Σ∗ is denoted |s|
and coincides with the number of symbols con-
stituting the string. As usual, we write AB for the
set of functions {f | f : B → A}. A weighted
language L is a total function L : Σ∗ → R
from strings to real-valued weights. For example,
L(an) = e−n for all n ≥ 0 is such a weighted
language.

We restrict the weights in our RNNs to the ratio-

nal numbers Q. In addition, we reserve the use of
a special symbol $ to mark the start and end of an
input string. To this end, we assume that $ /∈ Σ for
all considered alphabets, and we let Σ$ = Σ∪{$}.
Definition 1. A single-layer RNN R is a 7-tuple
〈Σ, N, h−1,W,W ′, E,E′〉, in which
• Σ is an input alphabet,
• N is a finite set of neurons,
• h−1 ∈ QN is an initial activation vector,
• W ∈ QN×N is a transition matrix,
• W ′ = (W ′a)a∈Σ$ is a Σ$-indexed family of

bias vectors W ′a ∈ QN ,
• E ∈ QΣ$×N is a prediction matrix, and
• E′ ∈ QΣ$ is a prediction bias vector.
Next, let us define how such an RNN works.

We first prepare our input encoding and the ef-
fect of our activation function. For an input
string s = s1s2 · · · sn ∈ Σ∗ with s1, . . . , sn ∈ Σ,
we encode this input as $s$ and thus assume that
s0 = $ and sn+1 = $. Our RNNs use ReLUs
(Rectified Linear Units), so for every v ∈ QN
we let σ〈v〉 (the ReLU activation) be the vector
σ〈v〉 ∈ QN such that

σ〈v〉(n) = max
(
0, v(n)

)
for every n ∈ N .

In other words, the ReLUs act like identities on
nonnegative inputs, but clip negative inputs to 0.
We use softmax-predictions, so for every vec-
tor p ∈ QΣ$ and a ∈ Σ$ we let

softmax〈p〉(a) = e
p(a)

∑
a′∈Σ$ e

p(a′)
.

RNNs act in discrete time steps reading a single
letter at each step. We now define the semantics of
our RNNs.

Definition 2. Let R = 〈Σ, N, h−1,W,W ′, E,E′〉
be an RNN, s an input string of length n and
0 ≤ t ≤ n a time step. We define
• the hidden state vector hs,t ∈ QN given by

hs,t = σ〈W · hs,t−1 +W ′st〉 ,

where hs,−1 = h−1 and we use standard ma-
trix product and point-wise vector addition,
• the next-token prediction vector Es,t ∈ QΣ$

Es,t = E · hs,t + E′

• the next-token distribution E′s,t ∈ RΣ$

E′s,t = softmax〈Es,t〉 .

2262



Finally, the RNN R computes the weighted lan-
guage R : Σ∗ → R, which is given for every in-
put s = s1 · · · sn as above by

R(s) =
n∏

t=0

E′s,t(st+1) .

In other words, each component hs,t(n) of the
hidden state vector is the ReLU activation applied
to a linear combination of all the components of
the previous hidden state vector hs,t−1 together
with a summand W ′st that depends on the t-th in-
put letter st. Thus, we often specify hs,t(n) as
linear combination instead of specifying the ma-
trix W and the vectors W ′a. The semantics is then
obtained by predicting the letters s1, . . . , sn of the
input s and the final terminator $ and multiplying
the probabilities of the individual predictions.

Let us illustrate these notions on an example.
We consider the RNN 〈Σ, N, h−1,W,W ′, E,E′〉
with γ ∈ Q and
• Σ = {a} and N = {1, 2},
• h−1 = (−1, 0)T and

W =

(
1 0
1 0

)
and W ′$ = W

′
a =

(
1
0

)

• E($, ·) = (M + 1, −(M + 1)) and
E(a, ·) = (1, −1) and
• E′($) = −M and E′(a) = 0.

In this case, we obtain the linear combinations

hs,t = σ

〈
hs,t−1(1) + 1
hs,t−1(1)

〉

computing the next hidden state components.
Given the initial activation, we thus obtain
hs,t = σ〈t, t − 1〉. Using this information, we
obtain

Es,t($) = (M + 1) · (t− σ〈t− 1〉)−M
Es,t(a) = t− σ〈t− 1〉 .

Consequently, we assign weight e
−M

1+e−M to input ε,

weight 1
1+e−M ·

e1

e1+e1
to a, and, more generally,

weight 1
1+e−M ·

1
2n to a

n.
Clearly the weight assigned by an RNN is al-

ways in the interval (0, 1), which enables a prob-
abilistic view. Similar to weighted finite-state au-
tomata or weighted context-free grammars, each
RNN is a compact, finite representation of a

weighted language. The softmax-operation en-
forces that the probability 0 is impossible as as-
signed weight, so each input string is principally
possible. In practical language modeling, smooth-
ing methods are used to change distributions such
that impossibility (probability 0) is removed. Our
RNNs avoid impossibility outright, so this can be
considered a feature instead of a disadvantage.

The hidden state hs,t of an RNN can be used as
scratch space for computation. For example, with
a single neuron n we can count symbols in s via:

hs,t(n) = σ〈hs,t−1(n) + 1〉 .

Here the letter-dependent summand W ′a is
universally 1. Similarly, for an alphabet
Σ = {a1, . . . , am} we can use the method of
Siegelmann and Sontag (1995) to encode the com-
plete input string s in base m+ 1 using:

hs,t(n) = σ〈(m+ 1)hs,t−1(n) + c(st)〉 ,

where c : Σ$ → {0, . . . ,m} is a bijection.
In principle, we can thus store the entire in-
put string (of unbounded length) in the hidden
state value hs,t(n), but our RNN model out-
puts weights at each step and terminates im-
mediately once the final delimiter $ is read.
It must assign a probability to a string in-
crementally using the chain rule decomposition
p(s1 · · · sn) = p(s1) · . . . · p(sn | s1 · · · sn−1).

Let us illustrate our notion of RNNs on some
additional examples. They all use the alpha-
bet Σ = {a} and are illustrated and formally
specified in Figure 1. The first column shows an
RNN R1 that assigns R1(an) = 2−(n+1). The
next-token prediction matrix ensures equal values
for a and $ at every time step. The second column
shows the RNN R2, which we already discussed.
In the beginning, it heavily biases the next sym-
bol prediction towards a, but counters it starting
at t = 1. The third RNN R3 uses another count-
ing mechanism with hs,t = σ〈t− 100, t− 101, t〉.
The first two components are ReLU-thresholded
to zero until t > 101, at which point they over-
whelm the bias towards a turning all future pre-
dictions to $.

3 Consistency

We first investigate the consistency problem for
an RNN R, which asks whether the recognized
weighted language R is indeed a probability dis-
tribution. Consequently, an RNN R is consistent

2263



R1(a
n) = 2−(n+1) R2(ε) ≈ 0 R3(a100) ≈ 1

R2(a
n) ≈ 2−n (n ≥ 1) R3(an) ≈ 0 (n 6= 100)

N {1} {1, 2} {1, 2, 3}

h−1
(
0
) (−1

0

) 


0
0
0




W
(
0
) (1 0

1 0

) 


0 0 1
0 0 1
0 0 1




W ′$ W
′
a

(
0
) (

0
) (1

0

) (
1
0

) 

−99
−100

1






−99
−100

1




E$ Ea
(
0
) (

0
) ( M + 1

−(M + 1)

) (
1
−1

) 

M
−M

0






−M
M
0




E′$ E
′
a 0 0 −M 0 −M 0

Figure 1: Sample RNNs over single-letter alphabets, and the weighted languages they recognize. M is some
positive rational number which depends on the desired error margin. If we want to express the second and
the third languages with error margin δ, M is chosen so that M > − ln δ1−δ in column 2, and chosen so that
(1 + e−M )100 < 11−δ in column 3.

if
∑

s∈Σ∗ R(s) = 1. We first show that there is an
inconsistent RNN, which together with our exam-
ples shows that consistency is a nontrivial property
of RNNs.2

We immediately use a slightly more complex
example, which we will later reuse.

Example 3. Let us consider an arbitrary RNN

R = 〈Σ, N, h−1,W,W ′, E,E′〉

with the single-letter alphabet Σ = {a}, the
neurons {1, 2, 3, n, n′} ⊆ N , initial activation
h−1(i) = 0 for all i ∈ {1, 2, 3, n, n′}, and the
following linear combinations:

hs,t(1) = σ〈hs,t−1(1) + hs,t−1(n)− hs,t−1(n′)〉
2 For comparison, all probabilistic finite-state automata

are consistent, provided no transitions exit final states. Not all
probabilistic context-free grammars are consistent; necessary
and sufficient conditions for consistency are given by Booth
and Thompson (1973). However, probabilistic context-free
grammars obtained by training on a finite corpus using pop-
ular methods (such as expectation-maximization) are guaran-
teed to be consistent (Nederhof and Satta, 2006).

hs,t(2) = σ〈hs,t−1(2) + 1〉
hs,t(3) = σ〈hs,t−1(3) + 3hs,t−1(1)〉

Es,t($) = hs,t(3)− hs,t(2)
Es,t(a) = hs,t(2)

Now we distinguish two cases:
Case 1: If hs,t(n) − hs,t(n′) = 0 for all t ∈ N,
then hs,t(1) = 0 and hs,t(2) = t + 1 and
hs,t(3) = 0. Hence we have Es,t($) = −(t + 1)
and Es,t(a) = t + 1. In this case the termination
probability

E′s,t($) =
e−(t+1)

e−(t+1) + et+1
=

1

1 + e2(t+1)

(i.e., the likelihood of predicting $) shrinks rapidly
towards 0, so the RNN assigns less than 15% of
the probability mass to the terminating sequences
(i.e., the finite strings), so the RNN is inconsistent
(see Lemma 15 in the appendix).

Case 2: Suppose that there exists a time

2264



point T ∈ N such that for all t ∈ N

hs,t(n)− hs,t(n′) =
{

1 if t = T
0 otherwise.

Then hs,t(1) = 0 for all t ≤ T and hs,t(1) = 1
otherwise. In addition, we have hs,t(2) = t + 1
and hs,t(3) = σ〈3(t− T − 1)〉. Hence we have

Es,t($) = σ〈3(t− T − 1)〉 − (t+ 1)

=

{
−(t+ 1) if t ≤ T
2t− 3T − 4 otherwise

Es,t(a) = t+ 1 ,

which shows that the probability

E′s,t($) =





1
1+e2(t+1)

if t ≤ T
et−3T−5

1+et−3T−5 otherwise

of predicting $ increases over time and eventually
(for t� 3T ) far outweighs the probability of pre-
dicting a. Consequently, in this case the RNN is
consistent (see Lemma 16 in the appendix).

We have seen in the previous example that con-
sistency is not trivial for RNNs, which takes us to
the consistency problem for RNNs:

Consistency: Given an RNN R, return “yes” if
R is consistent and “no” otherwise.

We recall the following theorem, which, com-
bined with our example, will prove that consis-
tency is unfortunately undecidable for RNNs.

Theorem 4 (Theorem 2 of Siegelmann and Son-
tag (1995)). Let M be an arbitrary deterministic
Turing machine. There exists an RNN

R = 〈Σ, N, h−1,W,W ′, E,E′〉

with saturated linear activation, input alpha-
bet Σ = {a}, and 1 designated neuron n ∈ N
such that for all s ∈ Σ∗ and 0 ≤ t ≤ |s|
• hs,t(n) = 0 if M does not halt on ε, and
• if M does halt on empty input after T steps,

then

hs,t(n) =

{
1 if t = T
0 otherwise.

In other words, such RNNs with saturated lin-
ear activation can semi-decide halting of an arbi-
trary Turing machine in the sense that a particu-
lar neuron achieves value 1 at some point during

the evolution if and only if the Turing machine
halts on empty input. An RNN with saturated
linear activation is an RNN following our defini-
tion with the only difference that instead of our
ReLU-activation σ the following saturated linear
activation σ′ : QN → QN is used. For every vec-
tor v ∈ QN and n ∈ N , let

σ′〈v〉(n) =





0 if v(n) < 0
v(n) if 0 ≤ v(n) ≤ 1
1 if v(n) > 1 .

Since σ′〈v〉 = σ〈v〉 − σ〈v−~1〉 for all v ∈ QN ,
and the right-hand side is a linear transformation,
we can easily simulate saturated linear activation
in our RNNs. To this end, each neuron n ∈ N of
the original RNN R = 〈Σ, N, h−1, U, U ′, E,E′〉
is replaced by two neurons n1 and n2 in the new
RNN R′ = 〈Σ, N ′, h′−1, V, V ′, F, F ′〉 such that
hs,t(n) = h

′
s,t(n1) − h′s,t(n2) for all s ∈ Σ∗ and

0 ≤ t ≤ |s|, where the evaluation of h′s,t is per-
formed in the RNNR′. More precisely, we use the
transition matrix V and bias function V ′, which is
given by

V (n1, n
′
1) = V (n2, n

′
1) = U(n, n

′)

V (n1, n
′
2) = V (n2, n

′
2) = −U(n, n′)

V ′a(n1) = U
′
a(n)

V ′a(n2) = U
′
a(n)− 1

h′−1(n1) = h−1(n)

h′−1(n2) = 0

for all n, n′ ∈ N and a ∈ Σ ∪ {$}, where
n1 and n2 are the two neurons corresponding to n
and n′1 and n

′
2 are the two neurons corresponding

to n′ (see Lemma 17 in the appendix).

Corollary 5. Let M be an arbitrary deterministic
Turing machine. There exists an RNN

R = 〈Σ, N, h−1,W,W ′, E,E′〉

with input alphabet Σ = {a} and 2 designated
neurons n1, n2 ∈ N such that for all s ∈ Σ∗ and
0 ≤ t ≤ |s|
• hs,t(n1) − hs,t(n2) = 0 if M does not halt

on ε, and
• if M does halt on empty input after T steps,

then

hs,t(n1)− hs,t(n2) =
{

1 if t = T
0 otherwise.

2265



We can now use this corollary together with the
RNNR of Example 3 to show that the consistency
problem is undecidable. To this end, we simulate
a given Turing machine M and identify the two
designated neurons of Corollary 5 as n and n′ in
Example 3. It follows that M halts if and only if
R is consistent. Hence we reduced the undecid-
able halting problem to the consistency problem,
which shows the undecidability of the consistency
problem.

Theorem 6. The consistency problem for RNNs is
undecidable.

As mentioned in Footnote 2, probabilistic
context-free grammars obtained after training on
a finite corpus using the most popular methods are
guaranteed to be consistent. At least for 2-layer
RNNs this does not hold.

Theorem 7. A two-layer RNN trained to a
local optimum using Back-propagation-through-
time (BPTT) on a finite corpus is not necessarily
consistent.

Proof. The first layer of the RNN R with a single
alphabet symbol a uses one neuron n′ and has the
following behavior:

h−1(n′) = 0

hs,t(n
′) = σ〈hs,t−1(n′) + 1〉

The second layer uses neuron n and takes hs,t(n′)
as input at time t:

hs,t(n) = σ〈hs,t(n′)− 2〉
Es,t(a) = hs,t(n) Es,t($) = 0

E′s,t(a) =

{
1
2 if t ≤ 1
e(t−1)

1+e(t−1)
otherwise.

Let the training data be {a}. Then the objective
we wish to maximize is simply R(a). The deriva-
tive of this objective with respect to each parame-
ter is 0, so applying gradient descent updates does
not change any of the parameters and we have con-
verged to an inconsistent RNN.

It remains an open question whether there is a
single-layer RNN that also exhibits this behavior.

4 Highest-weighted string

Given a function f : Σ∗ → R we are often inter-
ested in the highest-weighted string. This corre-
sponds to the most likely sentence in a language

Best-path Best-string
General RNN Undecidable

Consistent RNN NP-c 3

Det. PFSA/PCFG P 4

Nondet. PFSA/PCFG NP-c 5

Table 1: Comparison of the difficulty of identifying the
most probable derivation (Best-path) and the highest-
weighted string (Best-string) for various models.

model or the most likely translation for a decoder
RNN in machine translation.

For deterministic probabilistic finite-state au-
tomata or context-free grammars only one path or
derivation exists for any given string, so the identi-
fication of the highest-weighted string is the same
task as the identification of the most probable path
or derivation. However, for nondeterministic de-
vices, the highest-weighted string is often harder
to identify, since the weight of a string is the sum
of the probabilities of all possible paths or deriva-
tions for that string. A comparison of the difficulty
of identifying the most probable derivation and the
highest-weighted string for various models is sum-
marized in Table 1, in which we marked our results
in bold face.

We present various results concerning the diffi-
culty of identifying the highest-weighted string in
a weighted language computed by an RNN. We
also summarize some available algorithms. We
start with the formal presentation of the three stud-
ied problems.

1. Best string: Given an RNNR and c ∈ (0, 1),
does there exist s ∈ Σ∗ with R(s) > c?

2. Consistent best string: Given a consistent
RNN R and c ∈ (0, 1), does there exist
s ∈ Σ∗ with R(s) > c?

3. Consistent best string of polynomial
length: Given a consistent RNN R, poly-
nomial P with P(x) ≥ x for x ∈ N+, and
c ∈ (0, 1), does there exist s ∈ Σ∗ with
|s| ≤ P(|R|) and R(s) > c?

As usual the corresponding optimization problems
are not significantly simpler than these decision
problems. Unfortunately, the general problem is
also undecidable, which can easily be shown us-
ing our example.

3Restricted to solutions of polynomial length
4Dijkstra shortest path / (Knuth, 1977)
5(Casacuberta and de la Higuera, 2000) / (Simaan, 1996)

2266



Theorem 8. The best string problem for RNNs is
undecidable.

Proof. Let M be an arbitrary Turing machine and
again consider the RNN R of Example 3 with
the neurons n and n′ identified with the des-
ignated neurons of Corollary 5. We note that
R(ε) = 1

1+e2
< 0.12 in both cases. If M does

not halt, then R(an) ≤ 1
1+e2(n+1)

≤ 1
1+e2

< 0.12
for all n ∈ N. On the other hand, if M halts after
T steps, then

R(a3T−5)

=
( T∏

t=0

e2(t+1)

1 + e2(t+1)

)
·
( 3T−6∏

t=T+1

1

1 + et−3T−5

)
· 1

2

≥ 2
(−1, e−2)∞

·
( 3T−6∏

t=T+1

e3T+5−t

e3T+5−t+1

)
· 1

2

≥ 2
(−1, e−2)∞ · (−1, e−1)∞

≥ 0.25

using Lemma 14 in the appendix. Consequently, a
string with weight above 0.12 exists if and only if
M halts, so the best string problem is also unde-
cidable.

If we restrict the RNNs to be consistent, then
we can easily decide the best string problem by
simple enumeration.

Theorem 9. The consistent best string problem
for RNNs is decidable.

Proof. Let R be the RNN over alphabet Σ and
c ∈ (0, 1) be the bound. Since Σ∗ is count-
able, we can enumerate it via f : N → Σ∗. In
the algorithm we compute Sn =

∑n
i=0R(f(i))

for increasing values of n. If we encounter a
weight R(f(n)) > c, then we stop with answer
“yes.” Otherwise we continue until Sn > 1− c, at
which point we stop with answer “no.”

Since R is consistent, limi→∞ Si = 1, so this
algorithm is guaranteed to terminate and it obvi-
ously decides the problem.

Next, we investigate the length |wmaxR | of the
shortest string wmaxR of maximal weight in the
weighted language R generated by a consistent
RNN R in terms of its (binary storage) size |R|.
As already mentioned by Siegelmann and Sontag
(1995) and evidenced here, only small precision
rational numbers are needed in our constructions,
so we assume that |R| ≤ c · |N |2 for a (reasonably
small) constant c, where N is the set of neurons

of R. We show that no computable bound on the
length of the best string can exist, so its length can
surpass all reasonable bounds.

Theorem 10. Let f : N+ → N be the function with

f(n) = max
consistent RNN R

|R|≤n
|wmaxR |

for all n ∈ N+. There exists no computable func-
tion g : N→ N with g(n) ≥ f(n) for all n ∈ N.
Proof. In the previous section (before Theorem 6)
we presented an RNN RM that simulates an
arbitrary (single-track) Turing machine M with
n states. By Siegelmann and Sontag (1995) we
have |RM | ≤ c · (4n + 16). Moreover, we ob-
served that this RNN RM is consistent if and only
if the Turing machine M halts on empty input. In
the proof of Theorem 8 we have additionally seen
that the length |wmaxR | of its best string exceeds the
number TM of steps required to halt.

For every n ∈ N, let BB(n) be the n-th “Busy
Beaver” number (Radó, 1962), which is

BB(n) = max
normalized n-state Turing machine M with

2 tape symbols that halts on empty input

TM

It is well-known that BB : N+ → N cannot be
bounded by any computable function. However,

BB(n) ≤ max
normalized n-state Turing machine M with
and 2 tape symbols that halts on empty input

|wmaxRM |

≤ max
consistent RNN R
|R|≤c·(4n+16)

|wmaxR |

= f(4nc+ 16c) ,

so f clearly cannot be computable and no com-
putable function g can provide bounds for f .

Finally, we investigate the difficulty of the best
string problem for consistent RNN restricted to so-
lutions of polynomial length.

Theorem 11. Identifying the best string of polyno-
mial length in a consistent RNN is NP-complete.

Proof. To show NP-hardness, we reduce from the
3-SAT problem. Let x1, . . . , xm be m Boolean
variables and

F =
k∧

i=1

(
`i1 ∨ `i2 ∨ `i3

)
,

be a formula in conjunctive normal form, where
`ij ∈ {x1, . . . , xm,¬x1, . . . ,¬xm}. 3-SAT asks
whether there is a setting of xis that makes F true.

2267



We initialize h−1(n) = 0, ∀ n ∈ N =
{x1, . . . , xm, c1, . . . , ck, c′1, . . . , c′k, F, n1, n2, n3, ?}.
Let s ∈ {0, 1}∗ be the input string. Denote the
value of F when xj = sj for all j ∈ [m] as F (s).
Let t ∈ N with t ≤ |s|. Set hs,t(xm) = σ〈I(st)〉,
where I(0) = I($) = 0 and I(1) = 1. This
stores the current input symbol in neuron xm,
so hs,t(xm) = I(st). In addition, we let
hs,t(xj) = σ〈hs,t−1(xj+1)〉 for all j ∈ [m − 1].
Consequently, for all j ∈ [m]

hs,t(xj) =

{
I(st−(m−j)) if m− j ≤ t
0 otherwise.

Next, we evaluate the clauses. For each i ∈ [k],
we use two neurons ci and c′i such that

hs,t(ci) = σ〈fs,t(`i1) + fs,t(`i2) + fs,t(`i3)〉
hs,t(c

′
i) = σ〈fs,t(`i1) + fs,t(`i2) + fs,t(`i3)− 1〉,

where fs,t(xm) = I(st), fs,t(¬xm) = 1 − I(st),
and ∀j ∈ [m − 1], fs,t(xj) = hs,t−1(xj+1),
fs,t(¬xj) = 1 − hs,t−1(xj+1). Note that
hs,t(ci) − hs,t(c′i) contains the evaluation of the
clause `i1 ∨ `i2 ∨ `i3. Let

hs,t(F ) = σ
〈 k∑

i=1

(
hs,t−1(ci)−hs,t−1(c′i)

)
−k+1

〉
,

so hs,t(F ) = F (s) contains the evaluation of the
formula F using the values in neurons x1, . . . , xm.

We use three counters n1, n2, n3 to ensure that
the only relevant inputs are of length m+ 2:

hs,t(n1) = σ〈hs,t−1(n3)− (m+ 2)〉
hs,t(n2) = σ〈hs,t−1(n3)− (m+ 1)〉
hs,t(n3) = σ〈hs,t−1(n3) + 1〉 ,

which yields hs,t(n3) = t + 1,
hs,t(n2) = σ〈t − (m + 1)〉, and
hs,t(n1) = σ〈t− (m+ 2)〉.

Our goal neuron is ?, which we set to

hs,t(?) = σ〈hs,t−1(F )−hs,t−1(n1)+hs,t−1(n2)−1〉

so that

hs,t(?) =

{
hs,t−1(F ) if t = m+ 2
0 otherwise,

so hs,t(?) = 1 if and only if t = m + 2 and
F (s) = 1.

Let m′ = m+ 4. The output is set as follows:

Es,t(0) = Es,t(1) = m
′(1− 2hs,t(?)

)

Es,t($) = −m′
(
1− 2hs,t(?)

)
,

This yieldsEs,t(0) = Es,t(1) = −Es,t($) = −m′
if t = m+2 and F (s) = 1, andm′ otherwise. For
a ∈ {0, 1},

E′s,t(a)=





e−m
′

2e−m′+em′
if t=m+2 and F (s)=1

em
′

2em′+e−m′
otherwise

E′s,t($)=





em
′

2e−m′+em′
if t=m+2 and F (s)=1

e−m
′

2em′+e−m′
otherwise.

Finally, we set the threshold ξ = 3−m
′
. When

|s| 6= m + 2, sm+3 6= $, so the weight of s con-
tains the factor e

−m′

2e−m′+em′
= 1

2+e2m′
and thus is

upper-bounded by 1
2+e2m′

< ξ. Hence no input of
length different from m+ 2 achieves a weight that
exceeds ξ. A string s of length m+ 2 achieves the
weight ws given by

ws=





em
′

2e−m′+em′
·∏m+2i=1 e

m′

2em′+e−m′
if F (s)=1

e−m
′

2em′+e−m′
·∏m+2i=1 e

m′

2em′+e−m′
otherwise.

When F (s) = 0, ws < e
−m′

2em′+e−m′
< ξ, so

if F is unsatisfiable, no input string achieves a
weight above the threshold ξ. When F (s) = 1,

ws =
em
′

2e−m′+em′
·
(

em
′

2em′+e−m′

)m+2
> ξ. An in-

put string with weight above ξ exists if and only
if F is satisfiable. Obviously, the reduction can be
computed in polynomial time since all constants
can be computed in logarithmic space. The con-
structed RNN is consistent, since the output pre-
diction is constant after m+ 3 steps.

5 Equivalence

We prove that equivalence of two RNNs is
undecidable. For comparison, equivalence of
two deterministic WFSAs can be tested in time
O(|Σ|(|QA| + |QB|)3), where |QA|, |QB| are the
number of states of the two WFSAs and |Σ| is the
size of the alphabet (Cortes et al., 2007); equiva-
lence of nondeterministic WFSAs are undecidable
(Griffiths, 1968). The decidability of language
equivalence for deterministic probabilistic push-
downtown automata (PPDA) is still open (Forejt
et al., 2014), although equivalence for determin-
istic unweighted push-downtown automata (PDA)
is decidable (Sénizergues, 1997).

2268



The equivalence problem is formulated as fol-
lows:

Equivalence: Given two RNNsR andR′, return
“yes” if R(s) = R′(s) for all s ∈ Σ∗, and “no”
otherwise.

Theorem 12. The equivalence problem for RNNs
is undecidable.

Proof. We prove by contradiction. Suppose Tur-
ing machine M decides the equivalence problem.
Given any deterministic Turing Machine M ′, con-
struct the RNN R that simulates M ′ on input � as
described in Corollary 5. Let Es,t(a) = 0 and
Es,t($) = hs,t(n1)−hs,t(n2). If M ′ does not halt
on �, for all t ∈ N, E′s,t(a) = E′s,t($) = 1/2;
if M ′ halts after T steps, E′s,T (a) = 1/(e + 1),
Es,T ($) = e/(e + 1). Let R′ be the trivial RNN
that computes {an : P (an) = 2−(n+1), n ≥ 0}.
We run M on input 〈R,R′〉. If M returns “no”,
M ′ halts on x, else it does not halt. Therefore
the Halting Problem would be decidable if equiv-
alence is decidable. Therefore equivalence is un-
decidable.

6 Minimization

We look next at minimization of RNNs. For
comparison, state-minimization of a deterministic
PFSA is O(|E| log |Q|) where |E| is the number
of transitions and |Q| is the number of states (Aho
et al., 1974). Minimization of a non-deterministic
PFSA is PSPACE-complete (Jiang and Raviku-
mar, 1993).

We focus on minimizing the number of hidden
neurons (|N |) in RNNs:
Minimization: Given RNN R and non-negative
integer n, return “yes” if ∃ RNN R′ with number
of hidden units |N ′| ≤ n such that R(s) = R′(s)
for all s ∈ Σ∗, and “no” otherwise.

Theorem 13. RNN minimization is undecidable.

Proof. We reduce from the Halting Problem. Sup-
pose Turing Machine M decides the minimization
problem. For any Turing Machine M ′, construct
the same RNN R as in Theorem 12. We run M
on input 〈R, 0〉. Note that an RNN with no hid-
den unit can only output constant E′s,t for all t.
Therefore the number of hidden units in R can
be minimized to 0 if and only if it always outputs
E′s,t(a) = E

′
s,t($) = 1/2. If M returns “yes”, M

′

does not halt on �, else it halts.

7 Conclusion

We proved the following hardness results regard-
ing RNN as a recognizer of weighted languages:

1. Consistency:
(a) Inconsistent RNNs exist.
(b) Consistency of RNNs is undecidable.

2. Highest-weighted string:
(a) Finding the highest-weighted string for

an arbitrary RNN is undecidable.
(b) Finding the highest-weighted string for

a consistent RNN is decidable, but the
solution length can surpass all com-
putable bounds.

(c) Restricting to solutions of polynomial
length, finding the highest-weighted
string is NP-complete.

3. Testing equivalence of RNNs and minimizing
the number of neurons in an RNN are both
undecidable.

Although our undecidability results are upshots
of the Turing-completeness of RNN (Siegelmann
and Sontag, 1995), our NP-completeness result is
original, and surprising, since the analogous hard-
ness results in PFSA relies on the fact that there are
multiple derivations for a single string (Casacu-
berta and de la Higuera, 2000). The fact that these
results hold for the relatively simple RNNs we
used in this paper suggests that the case would
be the same for more complicated models used in
NLP, such as long short term memory networks
(LSTMs; Hochreiter and Schmidhuber 1997).

Our results show the non-existence of (effi-
cient) algorithms for interesting problems that re-
searchers using RNN in natural language process-
ing tasks may have hoped to find. On the other
hand, the non-existence of such efficient or ex-
act algorithms gives evidence for the necessity of
approximation, greedy or heuristic algorithms to
solve those problems in practice. In particular,
since finding the highest-weighted string in RNN
is the same as finding the most-likely translation
in a sequence-to-sequence RNN decoder, our NP-
completeness result provides some justification for
employing greedy and beam search algorithms in
practice.

Acknowledgments

This work was supported by DARPA (W911NF-
15-1-0543 and HR0011-15-C-0115). Andreas
Maletti was financially supported by DFG
Graduiertenkolleg 1763 (QuantLA).

2269



References
Alfred V. Aho, John E. Hopcroft, and Jeffrey D. Ull-

man. 1974. The design and analysis of computer
algorithms. Addison-Wesley.

Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-
jciech Skut, and Mehryar Mohri. 2007. OpenFst: A
General and Efficient Weighted Finite-State Trans-
ducer Library, Springer Berlin Heidelberg, Berlin,
Heidelberg, pages 11–23.

D. Bahdanau, K. Cho, and Y. Bengio. 2014. Neural
machine translation by jointly learning to align and
translate. In Proc. ICLR.

T. L. Booth and R. A. Thompson. 1973. Ap-
plying probability measures to abstract languages.
IEEE Transactions on Computers C-22(5):442–450.
https://doi.org/10.1109/t-c.1973.223746.

Francisco Casacuberta and Colin de la Higuera. 2000.
Computational complexity of problems on proba-
bilistic grammars and transducers. Grammatical In-
ference: Algorithms and Applications Lecture Notes
in Computer Science pages 15–24. https://doi.org/
10.1007/978-3-540-45257-7 2.

Corinna Cortes, Mehryar Mohri, and Ashish Rastogi.
2007. Lp distance and equivalence of probabilistic
automata. International Journal of Foundations of
Computer Science 18(04):761–779. https://doi.org/
10.1142/s0129054107004966.

Manfred Droste, Werner Kuich, and Heiko Vogler.
2013. Handbook of Weighted Automata. Springer
Berlin.

Vojtch Forejt, Petr Janar, Stefan Kiefer, and James
Worrell. 2014. Language equivalence of probabilis-
tic pushdown automata. Information and Computa-
tion 237:1–11. https://doi.org/10.1016/j.ic.2014.04.
003.

T. V. Griffiths. 1968. The unsolvability of the equiva-
lence problem for ∧-free nondeterministic general-
ized machines. Journal of the ACM 15(3):409–413.
https://doi.org/10.1145/321466.321473.

Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long
short-term memory. Neural Comput. 9(8):1735–
1780. https://doi.org/10.1162/neco.1997.9.8.1735.

Tao Jiang and B. Ravikumar. 1993. Minimal NFA
problems are hard. SIAM Journal on Computing
22(6):1117–1141. https://doi.org/10.1137/0222067.

Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam
Shazeer, and Yonghui Wu. 2016. Exploring the lim-
its of language modeling. https://arxiv.org/pdf/1602.
02410.pdf.

Yoon Kim and Alexander M. Rush. 2016. Sequence-
level knowledge distillation. In Proceedings of the
2016 Conference on Empirical Methods in Natu-
ral Language Processing. Association for Computa-
tional Linguistics, Austin, Texas, pages 1317–1327.
https://aclweb.org/anthology/D16-1139.

Donald E. Knuth. 1977. A generalization of Dijkstra’s
algorithm. Information Processing Letters 6(1):1–5.
https://doi.org/10.1016/0020-0190(77)90002-3.

T. Mikolov and G. Zweig. 2012. Context depen-
dent recurrent neural network language model. In
2012 IEEE Spoken Language Technology Workshop
(SLT). pages 234–239. https://doi.org/10.1109/SLT.
2012.6424228.

Mark-Jan Nederhof and Giorgio Satta. 2006. Estima-
tion of consistent probabilistic context-free gram-
mars. In Proceedings of the Human Language
Technology Conference of the NAACL, Main Con-
ference. Association for Computational Linguistics,
New York City, USA, pages 343–350. http://www.
aclweb.org/anthology/N/N06/N06-1044.

Tibor Radó. 1962. On non-computable functions. Bell
System Technical Journal 41:877–884.

Géraud Sénizergues. 1997. The equivalence problem
for deterministic pushdown automata is decidable.
In Proc. Automata, Languages and Programming:
24th International Colloquium, Springer Berlin Hei-
delberg, pages 671–681.

Hava T. Siegelmann and Eduardo D. Sontag. 1995. On
the computational power of neural nets. Journal
of Computer and System Sciences 50(1):132–150.
https://doi.org/10.1006/jcss.1995.1013.

Khalil Simaan. 1996. Computational complexity
of probabilistic disambiguation by means of tree-
grammars. In Proc. COLING. pages 1175–1180.
https://doi.org/10.3115/993268.993392.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Z. Ghahramani, M. Welling, C. Cortes,
N. D. Lawrence, and K. Q. Weinberger, editors, Ad-
vances in Neural Information Processing Systems
27. Curran Associates, Inc., pages 3104–3112.

Appendix

Lemma 14. For every k ∈ N+
∏

t∈N

ek(t+1)

ek(t+1) + 1
=

2

(−1; e−k)∞
,

where (−1; e−k)∞ is the infinite e−k-Pochhammer
symbol.

Proof.

∏

t∈N

ek(t+1)

ek(t+1) + 1
=
∏

t∈N+

( ekt
ekt + 1

· e
−kt

e−kt

)

=
∏

t∈N+

1

1 + e−kt
=

(( ∏

t∈N+

1

1 + e−kt

)−1
)−1

2270



=
( ∏

t∈N+
(1 + e−kt)

)−1
=
(1

2

∏

t∈N
(1 + e−kt)

)−1

=
2

(−1; e−k)∞
Lemma 15. Reconsider the RNN of Example 3
and suppose that hs,t(n) − hs,t(n′) = 0 for all
t ∈ N. Then

∑

s∈Σ∗
R(s) = 1− 2

(−1; e−2)∞
≈ 0.14

Proof.
∑

s∈Σ∗
R(s) =

∑

n∈N
R(an)

=
∑

n∈N

( e−(n+1)
en+1 + e−(n+1)

·
n−1∏

t=0

et+1

et+1 + e−(t+1)

)

= 1−
∏

t∈N

e2(t+1)

e2(t+1) + 1
= 1− 2

(−1; e−2)∞
≈ 0.14 ,

where the final equality utilizes Lemma 14.

Lemma 16. Reconsider the RNN of Example 3
and suppose that there exists a time point T ∈ N
such that for all t ∈ N

hs,t(n)− hs,t(n′) =
{

1 if t = T
0 otherwise.

Then ∑

s∈Σ∗
R(s) = 1

Proof.
∑

s∈Σ∗
R(s) =

∑

n∈N
R(an)

=
( T∑

n=0

R(an)
)

+
( ∞∑

n=T+1

R(an)
)

=

T∑

n=0

( e−(n+1)
en+1 + e−(n+1)

·
n−1∏

t=0

et+1

et+1 + e−(t+1)

)

+

∞∑

n=T+1

e2n−3T−4

en+1 + e2n−3T−4

·
( T∏

t=0

et+1

et+1 + e−(t+1)

)
·
( n−1∏

t=T+1

et+1

et+1 + e2t−3T−4

)

=

T∑

n=0

( 1
e2(n+1) + 1

·
n−1∏

t=0

e2(t+1)

e2(t+1) + 1

)

+
∞∑

n=T+1

en−3T−5

1 + en−3T−5
·
( T∏

t=0

e2(t+1)

e2(t+1) + 1

)

·
( n−1∏

t=T+1

1

1 + et−3T−5

)

= 1−
( T∏

t=0

e2(t+1)

e2(t+1) + 1

)
+
( T∏

t=0

e2(t+1)

e2(t+1) + 1

)

·
∞∑

n=T+1

en−3T−5

1 + en−3T−5
·
( n−1∏

t=T+1

1

1 + et−3T−5

)

= 1−
( T∏

t=0

e2(t+1)

e2(t+1) + 1

)

·
(

1− 1 +
∞∏

t=T+1

1

1 + et−3T−5

)

= 1−
( T∏

t=0

e2(t+1)

e2(t+1) + 1

)
·
( ∞∏

t=T+1

1

1 + et−3T−5

)

≥ 1−
( T∏

t=0

e2(t+1)

e2(t+1) + 1

)
·
(∏

t∈N

1

1 + et

)

= 1

Lemma 17.

Proof. We set hs,−1(n) = h−1(n) for
all n ∈ N and h′s,−1(n′) = h′−1(n′)
for all n′ ∈ N ′. Then trivially
h′s,−1(n1)−h′s,−1(n2) = h−1(n)−0 = hs,−1(n).
Moreover, h′s,t(n1) = σ〈V · h′s,t−1 + V ′s[t]〉(n1)

= σ〈
∑

n′∈N ′
V (n1, n

′) · h′s,t−1(n′)

+ V ′s[t](n1)〉
= σ〈

∑

n′∈N

(
V (n1, n

′
1) · h′s,t−1(n′1)

+ V (n1, n
′
2) · h′s,t−1(n′2)

)
+ V ′s[t](n1)〉

= σ〈
∑

n′∈N
U(n, n′) ·

(
h′s,t−1(n

′
1)− h′s,t−1(n′2)

)

+ U ′s[t](n)〉
= σ〈

∑

n′∈N
U(n, n′) · hs,t−1(n′) + U ′s[t](n)〉

Similarly, we can show that h′s,t(n2) =

σ〈
∑

n′∈N
U(n, n′) · hs,t−1(n′) + U ′s[t](n)− 1〉

Hence h′s,t(n1) − h′s,t(n2) = hs,t(n) as required.

2271


