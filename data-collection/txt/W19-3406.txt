



















































An Analysis of Emotion Communication Channels in Fan-Fiction: Towards Emotional Storytelling


Proceedings of the Second Storytelling Workshop, pages 56–64
Florence, Italy, August 1, 2019. c©2019 Association for Computational Linguistics

56

An Analysis of Emotion Communication Channels in Fan Fiction:
Towards Emotional Storytelling

Evgeny Kim and Roman Klinger
Institut für Maschinelle Sprachverarbeitung

University of Stuttgart
Pfaffenwaldring 5b, 70569 Stuttgart, Germany

{evgeny.kim,roman.klinger}@ims.uni-stuttgart.de

Abstract
Centrality of emotion for the stories told by
humans is underpinned by numerous studies
in literature and psychology. The research in
automatic storytelling has recently turned to-
wards emotional storytelling, in which char-
acters’ emotions play an important role in
the plot development (Theune et al., 2004;
y Pérez, 2007; Méndez et al., 2016). How-
ever, these studies mainly use emotion to gen-
erate propositional statements in the form “A
feels affection towards B” or “A confronts B”.
At the same time, emotional behavior does
not boil down to such propositional descrip-
tions, as humans display complex and highly
variable patterns in communicating their emo-
tions, both verbally and non-verbally. In this
paper, we analyze how emotions are expressed
non-verbally in a corpus of fan fiction short
stories. Our analysis shows that stories written
by humans convey character emotions along
various non-verbal channels. We find that
some non-verbal channels, such as facial ex-
pressions and voice characteristics of the char-
acters, are more strongly associated with joy,
while gestures and body postures are more
likely to occur with trust. Based on our analy-
sis, we argue that automatic storytelling sys-
tems should take variability of emotion into
account when generating descriptions of char-
acters’ emotions.

1 Introduction and Motivation

As humans, we make sense of our experiences
through stories (McKee, 2003). A key compo-
nent of any captivating story is a character (Kress,
2005) and a key component of every charac-
ter is emotion, as “without emotion a charac-
ter’s personal journey is pointless” (Ackerman and
Puglisi, 2012, p. 1). Numerous works pinpoint
the central role of emotions in storytelling (Hogan,
2015; Johnson-Laird and Oatley, 2016; Ingerman-
son and Economy, 2009), as well as story com-

prehension and evaluation (Komeda et al., 2005;
Van Horn, 1997; Mori et al., 2019).

Emotion analysis and automatic recognition in
text is mostly channel-agnostic, i.e., does not
consider along which non-verbal communication
channels (face, voice, etc.) emotions are ex-
pressed. However, we know that the same emo-
tions can be expressed non-verbally in a variety of
ways (Barrett, 2017, p. 11), for example, through
internal feelings of the character, as shown in Fig-
ure 1. We argue that automatic storytelling sys-
tems should take this information into account, as
versatility of the emotion description is a prerequi-
site for engaging and believable storytelling (Ack-
erman and Puglisi, 2012, p. 3).

There is a growing body of literature in the field
of natural language generation that uses emotions
as a key component for automatic plot construc-
tion (Theune et al., 2004; y Pérez, 2007; Méndez
et al., 2016) and characterization of virtual agents
(Imbert and de Antonio, 2005; Dias and Paiva,
2011). However, these and other studies put an
emphasis on emotion per se (“A feels affection
towards B”), or on the social behavior of charac-
ters “A confronts B”) making little or no reference
to how characters express emotions, both verbally
and non-verbally.

In this paper, we aim at closing this gap by ana-
lyzing how characters express their emotions us-
ing non-verbal communication signals. Specifi-

His body ached to do something, to ... revel in violence.

Character
Farrington

Emotion
Anger

Channel
Physical sensations

Figure 1: Example of the emotion expressed using
non-verbal communication channel. The annotation of
character and emotion are available in the dataset by
Kim and Klinger (2019). Channel annotation (in blue)
is an extension to the original dataset.



57

cally, we analyze how eight emotions (joy, sad-
ness, anger, fear, trust, disgust, surprise, and
anticipation) defined by Plutchik (2001) are ex-
pressed along the following channels introduced
by van Meel (1995): 1) physical appearance, 2) fa-
cial expressions, 3) gaze, looking behavior, 4) arm
and hand gesture, 5) movements of body as a
whole, 6) characteristics of voice, 7) spatial rela-
tions, and 8) physical make-up.

This paper is an extension to our previous study
(Kim and Klinger, 2019), in which we presented
a corpus of emotion relations between characters
in fan fiction short stories. We post-annotate the
corpus with non-verbal expressions of emotions
and analyze two scenarios of non-verbal emo-
tion expression: when the feeler of an emotion
is alone, and when a communication partner, who
also plays a role in the development of emotion, is
present. In our analysis, we look into the emotions
associated with each non-verbal behavior, map-
ping emotions to non-verbal expressions they fre-
quently occur with.

Our contributions are therefore the following:
1) we propose that natural language generation
systems describing emotions should take into ac-
count how emotions are expressed non-verbally,
2) we extend the annotations presented in Kim and
Klinger (2019) and quantitatively analyze the data,
3) we show that facial expressions, voice, eyes
and body movements are the top three channels
among which the emotion is expressed, 4) based
on the data, we show that some emotions are more
likely to be expressed via a certain channel, and
this channel is also influenced by the presence or
non-presence of a communication partner.

Our corpus is available at https://www.ims.
uni-stuttgart.de/data/emotion.

2 Related Work

Emotion analysis has received great attention
in natural language processing (Mohammad and
Bravo-Marquez, 2017; Mohammad et al., 2018;
Klinger et al., 2018; Felbo et al., 2017; Abdul-
Mageed and Ungar, 2017; Zhou and Wang, 2018;
Gui et al., 2017, i.a.). Most existing studies on
the topic cast the problem of emotion analysis
as a classification task, by classifying documents
(e.g., social media posts) into a set of predefined
emotion classes. Emotion classes used for the
classification are usually based on discrete cate-
gories of Ekman (1970) or Plutchik (2001) (cf.

Alm et al. (2005), Suttles and Ide (2013), Mo-
hammad (2012)). Fewer studies address emotion
recognition using a dimensional emotion repre-
sentation (cf. Buechel and Hahn (2017); Preoţiuc-
Pietro et al. (2016)). Such representation is based
on the valence-arousal emotion model (Russell,
1980), which can be helpful to account for sub-
jective emotional states that do not fit into discrete
categories.

Early attempts to computationally model emo-
tions in literary texts date back to the 1980s and
are presented in the works by Anderson and Mc-
Master (1982, 1986), who build a computational
model of affect in text tracking how emotions de-
velop in a literary narrative.

More recent studies in the field of digital hu-
manities approach emotion analysis from various
angles and for a wide range of goals. Some stud-
ies use emotions as feature input for genre clas-
sification (Samothrakis and Fasli, 2015; Henny-
Krahmer, 2018; Yu, 2008; Kim et al., 2017),
story clustering (Reagan et al., 2016), mapping
emotions to geographical locations in literature
(Heuser et al., 2016), and construction of so-
cial networks of characters (Nalisnick and Baird,
2013; Jhavar and Mirza, 2018). Other studies use
emotion analysis as a starting point for stylome-
try (Koolen, 2018), inferring psychological char-
acters’ traits (Egloff et al., 2018), and analysis
of the causes of emotions in literature (Kim and
Klinger, 2018, 2019).

To the best of our knowledge, there is no pre-
vious research that addresses the question of how
emotions are expressed non-verbally. The only
work that we are aware of is a literary study by
van Meel (1995), who proposes several non-verbal
communication channels for emotions and per-
forms a manual analysis on a set of several books.
He finds that voice is the most frequently used
category, followed by facial expressions, arm and
hand gestures and bodily postures. Van Meel ex-
plains the dominancy of voice by the predominant
role that speech plays in novels. However, van
Meel does not link the non-verbal channels to any
specific emotions. In this paper, we extend his
analysis by mapping the non-verbal channels to a
set of specific emotions felt by the characters.

3 Corpus Creation

We post-annotate our dataset of emotion rela-
tions between characters in fan fiction (Kim and

https://www.ims.uni-stuttgart.de/data/emotion
https://www.ims.uni-stuttgart.de/data/emotion


58

Emotion Fa
ce

B
od

y

A
pp

ea
r.

L
oo

k.

Vo
ic

e

G
es

tu
re

Sp
tr

el
.

Se
ns

at
io

ns

N
o

ch
an

ne
l

To
ta

l

anger 23 20 5 38 51 7 0 4 163 311
anticipation 4 14 0 17 4 2 7 6 267 321
disgust 3 6 1 3 0 0 0 1 149 163
fear 4 28 13 16 8 1 0 25 124 219
joy 76 26 7 12 52 19 5 33 268 498
sadness 3 5 4 4 2 0 3 7 81 109
surprise 10 5 3 13 1 0 0 2 118 152
trust 4 15 1 4 1 21 3 0 144 193

Total 127 119 34 107 119 50 18 78 1314 1966

Table 1: Counts of emotion and expression-channel pairs. No channel means that instance contains no reference
to how emotion is expressed non-verbally.

Klinger, 2019) with non-verbal communication
channels of emotion expressions. The dataset in-
cludes complete annotations of 19 fan fiction short
stories and of one short story by James Joyce.
The emotion relation is characterized by a triple
(Cexp, e, Ccause), in which the character Cexp
feels the emotion e. The character Ccause (to
which we refer as a “communication partner”) is
part of an event which triggers the emotion e. The
emotion categorization presented in the dataset
follows Plutchik’s (2001) classification, namely
joy, sadness, anger, fear, trust, disgust, surprise,
and anticipation.

Given an annotation of a character with an emo-
tion, we annotate non-verbal channels of emotion
expressions following the classification proposed
by van Meel (1995), who defines the following
eight categories: 1) physical appearance, 2) facial
expressions, 3) gaze, looking behavior, 4) arm and
hand gesture, 5) movements of body as a whole, 6)
characteristics of voice, 7) spatial relations (refer-
ences to personal space), and 8) physical make-up.
To clarify the category of physical make-up, we
redefine it under the name of physical sensations,
i.e., references to one’s internal physiological sig-
nals perceived by the feeler of the emotion.

The task is exemplified in Figure 1. Labels
for emotion (Anger) and character (Farrington)
are given. Physical sensation is an example of a
channel annotation we use to extend the original
dataset.

The annotations were done by three graduate
students in our computational linguistics depart-

a1–a2 a1–a3 a2–a3

Span 31 29 45
Sentence 49 45 59

Table 2: F1 scores in % for agreement between annota-
tors on a span level. a1, a2, and a3 are different annota-
tors. Span: label of channel and offsets are considered.
Sentence: only label of the channel in the sentence is
considered.

ment within a one-month period. The annotators
were asked to read each datapoint in the dataset
and decide if the emotion expressed by the feeler
(character) has an associated non-verbal channel
of expression. If so, the annotators were instructed
to mark the corresponding textual span and select
a channel label from the list of non-verbal commu-
nication channels given above.

The results of inter-annotator agreement (IAA)
are presented in Table 2. We measure agreement
along two dimensions: 1) span, where we mea-
sure how well two people agree on the label of
a non-verbal emotion expression, as well as on
the exact textual position of this expression, and
2) sentence, where we measure how well two peo-
ple agree on the label of non-verbal emotion ex-
pression in a given sentence (i.e., the exact posi-
tions of the channel are not taken into account).
The agreement is measured using the F1 measure,
where we assume that annotations by one person
are true, and annotations by another person are
predicted. As one can see, the agreement scores
for spans (i.e., channel label and exact textual po-



59

face body appear. look. voice gest. sptrel sens
channel

anger

anticipation

disgust

fear

joy

sadness

surprise

trust

em
ot

io
n

0.16 0.14 0.03 0.26 0.34 0.05 0 0.03

0.07 0.26 0 0.31 0.07 0.04 0.13 0.11

0.21 0.43 0.07 0.21 0 0 0 0.07

0.04 0.29 0.14 0.17 0.08 0.01 0 0.26

0.33 0.11 0.03 0.05 0.23 0.08 0.02 0.14

0.11 0.18 0.14 0.14 0.07 0 0.11 0.25

0.29 0.15 0.09 0.38 0.03 0 0 0.06

0.08 0.31 0.02 0.08 0.02 0.43 0.06 0
0.00

0.08

0.16

0.24

0.32

0.40

Figure 2: Distribution of non-verbal channels with all
emotions. Darker color indicates higher value. Values
in the cells are percentage ratios. Each cell is normal-
ized by the row sum of absolute frequencies.

sitions) are relatively low (lowest 29%, highest
45% F1 respectively). The IAA scores on a sen-
tence level are higher (lowest agreement is 45%,
highest 59% F1 respectively), as we only consider
the label of the non-verbal channel in a sentence
without looking into the exact textual positions of
the annotations.

4 Analysis

Table 1 summarizes the results of the annotation of
non-verbal channels of emotion expressions, Ta-
ble 3 gives examples of these expressions in the
dataset.

In total, there are 652 annotations of non-verbal
emotion expressions. By absolute counts, facial
expressions (Face, 127 occurrences), body move-
ments (Body, 119), voice (Voice, 199), and look-
ing behavior (Look., 107) have the highest number
of annotations. Spatial relations (Sptrel., 78) and
physical appearance (Appear., 34) have the lowest
number of annotations.

4.1 Emotion-Channel associations

We start our analysis by looking into the emotion-
channel associations. Namely, we analyze which
non-verbal channels are associated with which
emotions. To that end, we plot a heatmap of the
emotion–non-verbal-channel matrix. The value of
each cell in the heatmap is normalized by the row
sum (i.e., total counts of channel annotations) and
represents the likelihood of emotion-channel as-
sociation in the dataset, for each emotion, respec-
tively.

As Figure 2 shows, anger is more likely to be
expressed with voice, while joy is more likely to

Figure 3: Emotion-channel map. Each branch is an
emotion, whose branches are the top three non-verbal
channels associated with the emotion.

be expressed with face. In contrast to all other
emotions, sadness is more likely to be experienced
internally (sens. column in Figure 2) by the feeler,
as opposed to being communicated non-verbally.
Some channels and emotions show no association,
such as gestures (gest.) and disgust or spatial re-
lations (sptrel.) and anger. Given the relatively
small size of the dataset, we do not argue that
these associations are not possible in principle.
For example, fear and spatial relations have zero
association in our analysis, however, it is likely
that somebody expresses this emotion by moving
away subconsciously (increasing personal space)
from the source of danger. At the same time, fear
is most strongly associated with body movements
as a whole, which can be difficult to distinguish
from spatial relations. However, we believe that
these associations still reflect the general trend:
emotions that require immediate actions and serve
evolutionary survival function, such as anger, dis-
gust, and fear, are more likely to be expressed
with actions. For example, anger is an unpleas-
ant emotion that often occurs as a response to an
appraisal of a blocked goal (Harmon Jones and
Harmon-Jones, 2016), which can be resolved by
using voice characteristics (commanding or shout-
ing at someone who prevents you from achieving
your goal).

Overall, we observe that face, look., voice, and
body channels are predominantly used with all
emotions. We visualize the strongest associations
of emotions and non-verbal channels in Figure 3.
For each emotion, the figure shows the top three
(in a descending order) strongly associated non-
verbal channels. As one can see, the branches
are dominated by face, look., voice, and body
channels. The only exception is trust, for which
the predominant way to express emotions non-



60

Channel Emotion Examples

Facial expressions anger rolled his eyes
fear smiled nervously

Body movements anger stormed back out
trust slumped his shoulders

Physical appearance fear blushed crimson red

Looking behavior fear averted her eyes
anticipation pause to look back

Voice joy purred
fear voice getting smaller and smaller

Arm and hand gestures trust opened her arms

Spatial relations joy leaping into her arms
trust pulled him closer to his chest

Physical sensations joy tingling all over
fear hear in his throat

Table 3: Textual examples of non-verbal emotion expressions.

face body appear. look. voice gest. sptrel sens
channel

anger

anticipation

disgust

fear

joy

sadness

surprise

trust

em
ot

io
n

-0.14 -0.09 0.01 0.12 0.14 -0.02 0 -0.02

0.03 -0.12 0 0.12 0.03 0 -0.06 -0.01

0.21 0.03 -0.13 0.01 0 0 0 -0.13

0.02 -0.05 0.02 0.07 0.01 -0.01 0 -0.06

-0.03 0 0.03 -0.04 -0.05 -0.01 0 0.08

0.11 -0.15 0.14 0.14 0.07 0 -0.22 -0.08

0.29 -0.85 0.09 0.38 0.03 0 0 0.06

-0.42 0.31 0.02 0.08 0.02 -0.07 0.06 0 0.75

0.50

0.25

0.00

0.25

Figure 4: The difference between situations, in which a
character feels an emotion and the communication part-
ner is present, and situations in which the communica-
tion partner is not present (normalized by row sum).
Darker color/higher values indicates that the channel is
more likely to be used when there is a communication
partner.

verbally is through gestures, and sadness, which
is predominantly felt “internally” (sensations).

4.2 Presence of a communication partner

The original dataset contains information whether
an emotion of the character is evoked by another
character (communication partner). We use this
information to understand how the presence of a
communication partner affects the choice of a non-
verbal channel.

To that end, we plot a heatmap (Figure 4) from

the delta values between situations, in which the
communication partner is present, and situations
in which the communication partner is not present.
As it can be seen from Figure 4, trust is more
strongly associated with body movements when a
communication partner is present. Sadness, which
is more likely to associate with inner physical sen-
sations in the feeler, is expressed through the phys-
ical appearance and looking behavior when the
communication partner is present. Likewise, dis-
gust is more likely to be expressed with facial ex-
pressions, and anticipation is more likely to be ex-
pressed with looking behavior.

Again, we observe that body, voice, face, and
look. channels are the predominant non-verbal
communication channels for emotion expressions.

4.3 Timeline analysis

To understand if there are patterns in the frequency
of use of non-verbal channels in a narrative, we
perform an additional analysis.

For this analysis, we split each short story in
the dataset in ten equally sized chunks and get fre-
quencies of each non-verbal channel, which are
then plotted as time-series with confidence inter-
vals (Figure 5). The averaged values for each
channel are plotted as a dark line with circular
markers. The lighter area around the main line
represents confidence intervals of 95%, which are
calculated using bootstrap resampling.



61

Figure 5: Distribution of non-verbal emotion expressions in the narrative. Markers on the plot lines indicate the
text chunk. The plots are given for ten chunks. Light area around the solid line indicates confidence interval of
95%. y-axis shows percentage.

We observe the general tendency of all non-
verbal channels to vanish towards the end of the
story. The only exception is Facial expressions,
which after peaking in the middle of a story reverts
to the mean. Overall, we find no consistent pattern
in the use of non-verbal channels from beginning
to an end of a story.

5 Discussion and Conclusion

The results of the analysis presented in Section
4 show that emotions are expressed in a vari-
ety of ways through different non-verbal channels.
However, the preferred communication channel
depends on whether a communication partner is
present or not. Some channels are used predom-
inantly only when the feeler communicates her
emotion to another character, other channels can
be used in any situation.

Sadness stands out from other emotions in a
sense that it is predominantly not expressed using
any external channels of non-verbal communica-
tion. In other words, it is more common for the
characters in the annotated dataset to go through
sadness “alone” and feel it “in the body”, rather
than show it to the outer world. However, when
another character (communication partner) is the
reason of sadness experienced by the feeler, he or
she will most likely use eyes and overall behavior
to show this emotion.

In this paper, we showed that in human-written
stories, emotions are not only expressed as propo-
sitions in the form of “A feels affection towards
B” or “A confronts B”. As Table 3 shows, of-

ten there is no direct mention of the feelings A
holds towards B (“rolled his eyes”, “purred”). It is,
therefore, important, that this observation finds its
place in automatic storytelling systems. Some at-
tempts have been done in natural language gener-
ation towards controllable story generation (Peng
et al., 2018; Tambwekar et al., 2018). We pro-
pose that emotion expression should be one of the
controllable parameters in automatic storytellers.
As more and more language generation systems
have started using emotion as one of the cen-
tral components for plot development and charac-
terization of characters, there will be a need for
a more versatile and subtle description of emo-
tions, which is realized not only through proposi-
tional statements. In the end, no single instance of
same emotion is expressed in the same way (Bar-
rett, 2017), and emotion-aware storytelling sys-
tems should take this information into account
when generating emotional profiles of characters.

6 Future Work

This paper proposes one approach to non-verbal
emotion description that relies on a rigid ontol-
ogy of emotion classes. However, it might be
reasonable to make use of unsupervised clustering
of non-verbal descriptions to overcome the limita-
tions of using a relatively small number of coarse
emotion categories for the description of character
emotions. Once clustered, such descriptions could
be incorporated in the generated text (e.g., a plot
summary) and would elaborate all the simplistic
descriptions of character emotions.



62

Other research directions seems feasible too.
For example, the annotations, which we presented
in this paper, can be used for building and training
a model that automatically recognizes non-verbal
channels of emotion expressions. This might, in a
multi-task learning setting, improve emotion clas-
sification. The data we provide could also be used
as a starting point for terminology construction,
namely bootstrapping a lexicon of emotion com-
munications with different channels. Finally, our
work can serve as a foundation for the develop-
ment of an automatic storytelling system that takes
advantage of such resources.

Acknowledgements

This research has been conducted within the
CRETA project (http://www.creta.uni-stuttgart.
de/) which is funded by the German Ministry
for Education and Research (BMBF) and partially
funded by the German Research Council (DFG),
projects SEAT (Structured Multi-Domain Emo-
tion Analysis from Text, KL 2869/1-1).

References
Muhammad Abdul-Mageed and Lyle Ungar. 2017.

EmoNet: Fine-grained emotion detection with gated
recurrent neural networks. In Proceedings of the
55th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
718–728, Vancouver, Canada. Association for Com-
putational Linguistics.

Angela Ackerman and Becca Puglisi. 2012. The Emo-
tion Thesaurus: A Writer’s Guide to Character Ex-
pression. JADD Publishing.

Cecilia Ovesdotter Alm, Dan Roth, and Richard
Sproat. 2005. Emotions from text: Machine learn-
ing for text-based emotion prediction. In Proceed-
ings of the Conference on Human Language Tech-
nology and Empirical Methods in Natural Language
Processing, HLT ’05, pages 579–586, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.

Clifford W. Anderson and George E. McMaster. 1982.
Computer assisted modeling of affective tone in
written documents. Computers and the Humanities,
16(1):1–9.

Clifford W. Anderson and George E. McMaster. 1986.
Modeling emotional tone in stories using tension
levels and categorical states. Computers and the Hu-
manities, 20(1):3–9.

Lisa Feldman Barrett. 2017. How emotions are made:
The secret life of the brain. Houghton Mifflin Har-
court.

Sven Buechel and Udo Hahn. 2017. Emobank: Study-
ing the impact of annotation perspective and repre-
sentation format on dimensional emotion analysis.
In Proceedings of the 15th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics: Volume 2, Short Papers, pages 578–
585, Valencia, Spain. Association for Computational
Linguistics.

João Dias and Ana Paiva. 2011. Agents with emotional
intelligence for storytelling. In Affective Comput-
ing and Intelligent Interaction, pages 77–86, Berlin,
Heidelberg. Springer Berlin Heidelberg.

Mattia Egloff, Antonio Lieto, and Davide Picca. 2018.
An ontological model for inferring psychological
profiles and narrative roles of characters. In Digi-
tal Humanities 2018: Conference Abstracts, Mexico
City, Mexico.

Paul Ekman. 1970. Universal facial expressions of
emotion. California Mental Health Research Di-
gest, 8(4):151–158.

Bjarke Felbo, Alan Mislove, Anders Søgaard, Iyad
Rahwan, and Sune Lehmann. 2017. Using millions
of emoji occurrences to learn any-domain represen-
tations for detecting sentiment, emotion and sar-
casm. In Proceedings of the 2017 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1615–1625, Copenhagen, Denmark. As-
sociation for Computational Linguistics.

Lin Gui, Jiannan Hu, Yulan He, Ruifeng Xu, Qin Lu,
and Jiachen Du. 2017. A question answering ap-
proach for emotion cause extraction. In Proceed-
ings of the 2017 Conference on Empirical Methods
in Natural Language Processing, pages 1593–1602,
Copenhagen, Denmark. Association for Computa-
tional Linguistics.

Eddie Harmon Jones and Cindy Harmon-Jones. 2016.
Anger. In Michael Lewis, Jeannette M. Haviland-
Jones, and Lisa Feldman Barrett, editors, Handbook
of Emotions, chapter 44, pages 774–792. Guilford
Publications, New York.

Ulrike Edith Gerda Henny-Krahmer. 2018. Explo-
ration of sentiments and genre in spanish american
novels. In Digital Humanities 2018: Conference
Abstracts, Mexico, Mexico.

Ryan Heuser, Franco Moretti, and Erik Steiner. 2016.
The Emotions of London. Technical report, Stan-
ford University. Pamphlets of the Stanford Literary
Lab.

Patrick Colm Hogan. 2015. What Literature Teaches
Us about Emotion, pages 273–290. Oxford Univer-
sity Press, USA.

Ricardo Imbert and Angélica de Antonio. 2005. An
emotional architecture for virtual characters. In Vir-
tual Storytelling. Using Virtual Reality Technologies
for Storytelling, pages 63–72, Berlin, Heidelberg.
Springer Berlin Heidelberg.

http://www.creta.uni-stuttgart.de/
http://www.creta.uni-stuttgart.de/
https://doi.org/10.18653/v1/P17-1067
https://doi.org/10.18653/v1/P17-1067
https://doi.org/10.3115/1220575.1220648
https://doi.org/10.3115/1220575.1220648
https://www.aclweb.org/anthology/E17-2092
https://www.aclweb.org/anthology/E17-2092
https://www.aclweb.org/anthology/E17-2092
https://doi.org/10.18653/v1/D17-1169
https://doi.org/10.18653/v1/D17-1169
https://doi.org/10.18653/v1/D17-1169
https://doi.org/10.18653/v1/D17-1169
https://doi.org/10.18653/v1/D17-1167
https://doi.org/10.18653/v1/D17-1167


63

Randy Ingermanson and Peter Economy. 2009. Writ-
ing fiction for dummies. John Wiley & Sons.

Harshita Jhavar and Paramita Mirza. 2018. EMOFIEL:
Mapping emotions of relationships in a story. In
Companion Proceedings of the The Web Conference
2018, WWW ’18, pages 243–246, Republic and
Canton of Geneva, Switzerland. International World
Wide Web Conferences Steering Committee.

Philip N. Johnson-Laird and Keith Oatley. 2016. Emo-
tions in Music, Literature, and Film. In Michael
Lewis, Jeannette M. Haviland-Jones, and Lisa Feld-
man Barrett, editors, Handbook of Emotions, chap-
ter 4, pages 82–97. Guilford Publications, New
York.

Evgeny Kim and Roman Klinger. 2018. Who feels
what and why? Annotation of a literature corpus
with semantic roles of emotions. In Proceedings
of the 27th International Conference on Computa-
tional Linguistics, pages 1345–1359, Santa Fe, New
Mexico, USA. Association for Computational Lin-
guistics.

Evgeny Kim and Roman Klinger. 2019. Frowning
Frodo, Wincing Leia, and a Seriously Great Friend-
ship: Learning to Classify Emotional Relationships
of Fictional Characters. In Proceedings of the An-
nual Conference of the North American Chapter of
the Association for Computational Linguistics, Min-
neapolis, USA. Association for Computational Lin-
guistics. Accepted.

Evgeny Kim, Sebastian Padó, and Roman Klinger.
2017. Investigating the relationship between liter-
ary genres and emotional plot development. In Pro-
ceedings of the Joint SIGHUM Workshop on Com-
putational Linguistics for Cultural Heritage, Social
Sciences, Humanities and Literature, pages 17–26,
Vancouver, Canada. Association for Computational
Linguistics.

Roman Klinger, Orphée de Clercq, Saif M. Moham-
mad, and Alexandra Balahur. 2018. IEST: WASSA-
2018 implicit emotions shared task. In Proceedings
of the 9th Workshop on Computational Approaches
to Subjectivity, Sentiment and Social Media Anal-
ysis, Brussels, Belgium. Association for Computa-
tional Linguistics.

Hidetsugu Komeda, Yoshiaki Nihei, and Takashi
Kusumi. 2005. Roles of reader’s feelings in under-
standing narratives: forefeel, empathy, and a sense
of strangeness. Shinrigaku kenkyu: The Japanese
Journal of Psychology, 75(6):479–486.

Corina Koolen. 2018. Women’s books versus books by
women. In Digital Humanities 2018: Conference
Abstracts, Mexico City, Mexico.

Nancy Kress. 2005. Characters, Emotion & Viewpoint:
Techniques and Exercises for Crafting Dynamic
Characters and Effective Viewpoints. Writer’s Di-
gest Books.

Robert McKee. 2003. Storytelling that moves peo-
ple. A conversation with screenwriting coach Robert
Mckee. Harvard business review, 81(6):51—5, 136.

Jacques M. van Meel. 1995. Representing emotions
in literature and paintings: A comparative analysis.
Poetics, 23(1):159 – 176. Emotions and Cultural
Products.

Gonzalo Méndez, Pablo Gervás, and Carlos León.
2016. On the use of character affinities for story
plot generation. In Knowledge, Information and
Creativity Support Systems, pages 211–225, Cham.
Springer International Publishing.

Saif M. Mohammad. 2012. From once upon a time to
happily ever after: Tracking emotions in mail and
books. Decision Support Systems, 53(4):730–741.

Saif M. Mohammad and Felipe Bravo-Marquez. 2017.
WASSA-2017 shared task on emotion intensity. In
Proceedings of the 8th Workshop on Computational
Approaches to Subjectivity, Sentiment and Social
Media Analysis, pages 34–49, Copenhagen, Den-
mark. Association for Computational Linguistics.

Saif M. Mohammad, Felipe Bravo-Marquez, Mo-
hammad Salameh, and Svetlana Kiritchenko. 2018.
SemEval-2018 task 1: Affect in tweets. In Proceed-
ings of The 12th International Workshop on Seman-
tic Evaluation, pages 1–17, New Orleans, Louisiana.
Association for Computational Linguistics.

Yusuke Mori, Hiroaki Yamane, Yoshitaka Ushiku, and
Tatsuya Harada. 2019. How narratives move your
mind: A corpus of shared-character stories for con-
necting emotional flow and interestingness. Infor-
mation Processing & Management.

Eric T. Nalisnick and Henry S. Baird. 2013. Extract-
ing sentiment networks from shakespeare’s plays. In
2013 12th International Conference on Document
Analysis and Recognition, pages 758–762.

Nanyun Peng, Marjan Ghazvininejad, Jonathan May,
and Kevin Knight. 2018. Towards controllable story
generation. In Proceedings of the First Workshop on
Storytelling, pages 43–49, New Orleans, Louisiana.
Association for Computational Linguistics.

Rafael Pérez y Pérez. 2007. Employing emotions to
drive plot generation in a computer-based storyteller.
Cognitive Systems Research, 8(2):89–109.

Robert Plutchik. 2001. The nature of emotions. Amer-
ican Scientist, 89(4):344–350.

Daniel Preoţiuc-Pietro, H. Andrew Schwartz, Gregory
Park, Johannes Eichstaedt, Margaret Kern, Lyle Un-
gar, and Elisabeth Shulman. 2016. Modelling va-
lence and arousal in facebook posts. In Proceedings
of the 7th Workshop on Computational Approaches
to Subjectivity, Sentiment and Social Media Analy-
sis, pages 9–15, San Diego, California. Association
for Computational Linguistics.

https://doi.org/10.1145/3184558.3186989
https://doi.org/10.1145/3184558.3186989
https://www.aclweb.org/anthology/C18-1114
https://www.aclweb.org/anthology/C18-1114
https://www.aclweb.org/anthology/C18-1114
http://arxiv.org/abs/1903.12453
http://arxiv.org/abs/1903.12453
http://arxiv.org/abs/1903.12453
http://arxiv.org/abs/1903.12453
https://doi.org/10.18653/v1/W17-2203
https://doi.org/10.18653/v1/W17-2203
https://arxiv.org/abs/1809.01083
https://arxiv.org/abs/1809.01083
http://europepmc.org/abstract/MED/12800716
http://europepmc.org/abstract/MED/12800716
http://europepmc.org/abstract/MED/12800716
https://doi.org/10.1016/0304-422X(94)00013-V
https://doi.org/10.1016/0304-422X(94)00013-V
https://doi.org/10.18653/v1/W17-5205
https://doi.org/10.18653/v1/S18-1001
https://doi.org/https://doi.org/10.1016/j.ipm.2019.03.006
https://doi.org/https://doi.org/10.1016/j.ipm.2019.03.006
https://doi.org/https://doi.org/10.1016/j.ipm.2019.03.006
https://doi.org/10.1109/ICDAR.2013.155
https://doi.org/10.1109/ICDAR.2013.155
https://doi.org/10.18653/v1/W18-1505
https://doi.org/10.18653/v1/W18-1505
https://www.jstor.org/stable/27857503
https://doi.org/10.18653/v1/W16-0404
https://doi.org/10.18653/v1/W16-0404


64

Andrew J. Reagan, Lewis Mitchell, Dilan Kiley,
Christopher M. Danforth, and Peter Sheridan Dodds.
2016. The emotional arcs of stories are dominated
by six basic shapes. EPJ Data Science, 5(1):31.

James A. Russell. 1980. A circumplex model of af-
fect. Journal of Personality and Social Psychology,
39:1161–1178.

Spyridon Samothrakis and Maria Fasli. 2015. Emo-
tional sentence annotation helps predict fiction
genre. PloS one, 10(11):e0141922.

Jared Suttles and Nancy Ide. 2013. Distant supervi-
sion for emotion classification with discrete binary
values. In Computational Linguistics and Intelligent
Text Processing, pages 121–136, Berlin, Heidelberg.
Springer Berlin Heidelberg.

Pradyumna Tambwekar, Murtaza Dhuliawala, Ani-
mesh Mehta, Lara J. Martin, Brent Harrison, and
Mark O. Riedl. 2018. Controllable neural story gen-
eration via reinforcement learning. arXiv preprint
arXiv:1809.10736.

Mariët Theune, Sander Rensen, Rieks op den Akker,
Dirk Heylen, and Anton Nijholt. 2004. Emotional
characters for automatic plot creation. In Interna-
tional Conference on Technologies for Interactive
Digital Storytelling and Entertainment, pages 95–
100. Springer.

Leigh Van Horn. 1997. The characters within us:
Readers connect with characters to create meaning
and understanding. Journal of Adolescent & Adult
Literacy, 40(5):342–347.

Bei Yu. 2008. An evaluation of text classification
methods for literary study. Literary and Linguistic
Computing, 23(3):327–343.

Xianda Zhou and William Yang Wang. 2018. Mo-
jiTalk: Generating emotional responses at scale. In
Proceedings of the 56th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 1128–1137, Melbourne, Aus-
tralia. Association for Computational Linguistics.

https://doi.org/10.1140/epjds/s13688-016-0093-1
https://doi.org/10.1140/epjds/s13688-016-0093-1
https://www.aclweb.org/anthology/P18-1104
https://www.aclweb.org/anthology/P18-1104

