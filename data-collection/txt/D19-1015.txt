



















































DialogueGCN: A Graph Convolutional Neural Network for Emotion Recognition in Conversation


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 154–164,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

154

DialogueGCN: A Graph Convolutional Neural Network for
Emotion Recognition in Conversation

Deepanway Ghosal†, Navonil Majumder‡, Soujanya Poria†∗,
Niyati Chhaya∇ and Alexander Gelbukh‡

† Singapore University of Technology and Design, Singapore
‡Instituto Politécnico Nacional, CIC, Mexico

∇ Adobe Research, India

{1004721@mymail.,sporia@}sutd.edu.sg, navo@nlp.cic.ipn.mx,
nchhaya@adobe.com, gelbukh@gelbukh.com

Abstract

Emotion recognition in conversation (ERC)
has received much attention, lately, from re-
searchers due to its potential widespread ap-
plications in diverse areas, such as health-care,
education, and human resources. In this pa-
per, we present Dialogue Graph Convolutional
Network (DialogueGCN), a graph neural net-
work based approach to ERC. We leverage self
and inter-speaker dependency of the interlocu-
tors to model conversational context for emo-
tion recognition. Through the graph network,
DialogueGCN addresses context propagation
issues present in the current RNN-based meth-
ods. We empirically show that this method al-
leviates such issues, while outperforming the
current state of the art on a number of bench-
mark emotion classification datasets.

1 Introduction

Emotion recognition has remained an active re-
search topic for decades (K. D’Mello et al., 2006;
Busso et al., 2008; Strapparava and Mihalcea,
2010). However, the recent proliferation of open
conversational data on social media platforms,
such as Facebook, Twitter, Youtube, and Red-
dit, has warranted serious attention (Poria et al.,
2019b; Majumder et al., 2019; Huang et al., 2019)
from researchers towards emotion recognition in
conversation (ERC). ERC is also undeniably im-
portant in affective dialogue systems (as shown in
Fig. 1) where bots understand users’ emotions and
sentiment to generate emotionally coherent and
empathetic responses.

Recent works on ERC process the constituent
utterances of a dialogue in sequence, with a re-
current neural network (RNN). Such a scheme is
illustrated in Fig. 2 (Poria et al., 2019b), that re-
lies on propagating contextual and sequential in-
formation to the utterances. Hence, we feed the

∗ Corresponding author

My head is aching 
Frustrated

I’m sorry to hear that! 
Sad

Did you suffer a head injury 
earlier? 

Neutral

Yes 
Neutral

Please, 
immediately see a 

nearby doctor 
Excited 

No 
Neutral

Pleased to hear 
that 

Happy 

Did you consume 
alcohol recently? 

Neutral 

User Health Assistant

You might be 
concussed! 

Sad 

Figure 1: Illustration of an affective conversation where
the emotion depends on the context. Health assistant
understands affective state of the user in order to gen-
erate affective and empathetic responses.

conversation to a bidirectional gated recurrent unit
(GRU) (Chung et al., 2014). However, like most
of the current models, we also ignore intent mod-
elling, topic, and personality due to lack of la-
belling on those aspects in the benchmark datasets.
In theory, RNNs like long short-term memory
(LSTM) (Hochreiter and Schmidhuber, 1997) and
GRU should propagate long-term contextual infor-
mation. However, in practice it is not always the
case (Bradbury et al., 2017). This affects the ef-
ficacy of RNN-based models in various tasks, in-
cluding ERC.

To mitigate this issue, some variants of
the state-of-the-art method, DialogueRNN (Ma-
jumder et al., 2019), employ attention mechanism
that pools information from entirety or part of the
conversation per target utterance. However, this
pooling mechanism does not consider speaker in-
formation of the utterances and the relative po-
sition of other utterances from the target utter-
ance. Speaker information is necessary for mod-



155

St+1B

StA

Ut+1B

UtA

PBPA Topic

It+1B

ItA

Ut−1B

U< tA,B

U< t−1A,B

Person A Person B

t

t + 1

EtA

Et+1B

It−2A

It−1B

Figure 2: Interaction among different controlling vari-
ables during a dyadic conversation between persons A
and B. Grey and white circles represent hidden and ob-
served variables, respectively. P represents person-
ality, U represents utterance, S represents interlocu-
tor state, I represents interlocutor intent, E represents
emotion and Topic represents topic of the conversation.
This can easily be extended to multi-party conversa-
tions.

elling inter-speaker dependency, which enables
the model to understand how a speaker coerces
emotional change in other speakers. Similarly, by
extension, intra-speaker or self-dependency aids
the model with the understanding of emotional in-
ertia of individual speakers, where the speakers re-
sist the change of their own emotion against ex-
ternal influence. On the other hand, considera-
tion of relative position of target and context ut-
terances decides how past utterances influence fu-
ture utterances and vice versa. While past utter-
ances influencing future utterances is natural, the
converse may help the model fill in some relevant
missing information, that is part of the speaker’s
background knowledge but explicitly appears in
the conversation in the future. We leverage these
two factors by modelling conversation using a di-
rected graph. The nodes in the graph represent
individual utterances. The edges between a pair
of nodes/utterances represent the dependency be-
tween the speakers of those utterances, along with
their relative positions in the conversation. By
feeding this graph to a graph convolution network
(GCN) (Defferrard et al., 2016), consisting of two
consecutive convolution operations, we propagate

contextual information among distant utterances.
We surmise that these representations hold richer
context relevant to emotion than DialogueRNN.
This is empirically shown in Section 5.

The remainder of the paper is organized as fol-
lows — Section 2 briefly discusses the relevant
and related works on ERC; Section 3 elaborates
the method; Section 4 lays out the experiments;
Section 5 shows and interprets the experimental
results; and finally, Section 6 concludes the paper.

2 Related Work

Emotion recognition in conversation is a pop-
ular research area in natural language process-
ing (Kratzwald et al., 2018; Colneriĉ and Dem-
sar, 2018) because of its potential applications in
a wide area of systems, including opinion mining,
health-care, recommender systems, education, etc.

However, emotion recognition in conversation
has attracted attention from researchers only in
the past few years due to the increase in availabil-
ity of open-sourced conversational datasets (Chen
et al., 2018; Zhou et al., 2018; Poria et al., 2019a).
A number of models has also been proposed
for emotion recognition in multimodal data i.e.
datasets with textual, acoustic and visual informa-
tion. Some of the important works include (Po-
ria et al., 2017; Chen et al., 2017; Zadeh et al.,
2018a,b; Hazarika et al., 2018a,b), where mainly
deep learning-based techniques have been em-
ployed for emotion (and sentiment) recognition
in conversation, in only textual and multimodal
settings. The current state-of-the-art model in
emotion recognition in conversation is (Majumder
et al., 2019), where authors introduced a party
state and global state based recurrent model for
modelling the emotional dynamics.

Graph neural networks have also been very
popular recently and have been applied to semi-
supervised learning, entity classification, link pre-
diction, large scale knowledge base modelling,
and a number of other problems (Kipf and
Welling, 2016; Schlichtkrull et al., 2018; Bruna
et al., 2013). Early work on graph neural networks
include (Scarselli et al., 2008). Our graph model
is closely related to the graph relational modelling
work introduced in (Schlichtkrull et al., 2018).

3 Methodology

One of the most prominent strategies for emotion
recognition in conversations is contextual mod-



156

elling. We identify two major types of context in
ERC – sequential context and speaker-level con-
text. Following Poria et al. (2017), we model these
two types of context through the neighbouring ut-
terances, per target utterance.

Computational modeling of context should also
consider emotional dynamics of the interlocutors
in a conversation. Emotional dynamics is typi-
cally subjected to two major factors in both dyadic
and multiparty conversational systems — inter-
speaker dependency and self-dependency. Inter-
speaker dependency refers to the emotional in-
fluence that counterparts produce in a speaker.
This dependency is closely related to the fact that
speakers tend to mirror their counterparts to build
rapport during the course of a dialogue (Navar-
retta et al., 2016). However, it must be taken into
account, that not all participants are going to af-
fect the speaker in identical way. Each participant
generally affects each other participants in unique
ways. In contrast, self-dependency, or emotional
inertia, deals with the aspect of emotional influ-
ence that speakers have on themselves during con-
versations. Participants in a conversation are likely
to stick to their own emotional state due to their
emotional inertia, unless the counterparts invoke a
change. Thus, there is always a major interplay
between the inter-speaker dependency and self-
dependency with respect to the emotional dynam-
ics in the conversation.

We surmise that combining these two dis-
tinct yet related contextual information schemes
(sequential encoding and speaker level encod-
ing) would create enhanced context representation
leading to better understanding of emotional dy-
namics in conversational systems.

3.1 Problem Definition

Let there be M speakers/parties p1, p2, . . . , pM in
a conversation. The task is to predict the emo-
tion labels (happy, sad, neutral, angry, excited,
frustrated, disgust, and fear) of the constituent ut-
terances u1, u2, . . . , uN , where utterance ui is ut-
tered by speaker ps(ui), while s being the mapping
between utterance and index of its corresponding
speaker. We also represent ui ∈ RDm as the feature
representation of the utterance, obtained using the
feature extraction process described below.

3.2 Context Independent Utterance-Level
Feature Extraction

A convolutional neural network (Kim, 2014) is
used to extract textual features from the transcript
of the utterances. We use a single convolutional
layer followed by max-pooling and a fully con-
nected layer to obtain the feature representations
for the utterances. The input to this network is
the 300 dimensional pretrained 840B GloVe vec-
tors (Pennington et al., 2014). We use filters of
size 3, 4 and 5 with 50 feature maps in each. The
convoluted features are then max-pooled with a
window size of 2 followed by the ReLU activa-
tion (Nair and Hinton, 2010). These are then con-
catenated and fed to a 100 dimensional fully con-
nected layer, whose activations form the represen-
tation of the utterance. This network is trained at
utterance level with the emotion labels.

3.3 Model

We now present our Dialogue Graph Convolu-
tional Network (DialogueGCN1) framework for
emotion recognition in conversational setups. Di-
alogueGCN consists of three integral components
— Sequential Context Encoder, Speaker-Level
Context Encoder, and Emotion Classifier. An
overall architecture of the proposed framework is
illustrated in Fig. 3.

3.3.1 Sequential Context Encoder
Since, conversations are sequential by nature, con-
textual information flows along that sequence. We
feed the conversation to a bidirectional gated re-
current unit (GRU) to capture this contextual in-
formation: gi =

←ÐÐ→
GRUS(gi(+,−)1, ui), for i = 1,

2,. . . , N, where ui and gi are context-independent
and sequential context-aware utterance represen-
tations, respectively.

Since, the utterances are encoded irrespective of
its speaker, this initial encoding scheme is speaker
agnostic, as opposed to the state of the art, Dia-
logueRNN (Majumder et al., 2019).

3.3.2 Speaker-Level Context Encoder
We propose the Speaker-Level Context Encoder
module in the form of a graphical network to cap-
ture speaker dependent contextual information in a
conversation. Effectively modelling speaker level
context requires capturing the inter-dependency

1Implementation available at https://github.
com/SenticNet/conv-emotion

https://github.com/SenticNet/conv-emotion
https://github.com/SenticNet/conv-emotion


157

h1

h2

h3

h4

h5g1

g2

g3

g4

g5

GCN

1. Sequential 
Context Encoding

2. Speaker-Level 
Context Encoding 3. Classification

Concatenation

u1

u2

GRUS

GRUS

g1

g2

u3 GRUS g3

u4 GRUS g4

u5 GRUS g5
Features

gi
hi

Classify

Labels

Speaker 2 (p2)
Speaker 1 (p1)

Edge Types:
p1 → p1
p2 → p2
p1 → p2
p2 → p1

Towards past
Towards future

Node Types:

×{ } }{
Figure 3: Overview of DialogueGCN, congruent to the illustration in Table 1.

and self-dependency among participants. We de-
sign a directed graph from the sequentially en-
coded utterances to capture this interaction be-
tween the participants. Furthermore, we propose
a local neighbourhood based convolutional fea-
ture transformation process to create the enriched
speaker-level contextually encoded features. The
framework is detailed here.

First, we introduce the following notation: a
conversation having N utterances is represented
as a directed graph G = (V,E ,R,W), with ver-
tices/nodes vi ∈ V , labeled edges (relations) rij ∈ E
where r ∈ R is the relation type of the edge be-
tween vi and vj and αij is the weight of the la-
beled edge rij , with 0 ⩽ αij ⩽ 1, where αij ∈ W
and i, j ∈ [1,2, ...,N].

Graph Construction: The graph is constructed
from the utterances in the following way,

Vertices: Each utterance in the conversation is
represented as a vertex vi ∈ V in G. Each vertex vi
is initialized with the corresponding sequentially
encoded feature vector gi, for all i ∈ [1,2, ...,N].
We denote this vector as the vertex feature. Vertex
features are subject to change downstream, when
the neighbourhood based transformation process
is applied to encode speaker-level context.

Edges: Construction of the edges E depends on
the context to be modeled. For instance, if we
hypothesize that each utterance (vertex) is con-
textually dependent on all the other utterances in
a conversation (when encoding speaker level in-
formation), then a fully connected graph would
be constructed. That is each vertex is connected
to all the other vertices (including itself) with an

edge. However, this results in O(N2) number of
edges, which is computationally very expensive
for graphs with large number of vertices. A more
practical solution is to construct the edges by keep-
ing a past context window size of p and a future
context window size of f . In this scenario, each
utterance vertex vi has an edge with the immedi-
ate p utterances of the past: vi−1, vi−2, ..vi−p, f ut-
terances of the future: vi+1, vi+2, ..vi+f and itself:
vi. For all our experiments in this paper, we con-
sider a past context window size of 10 and future
context window size of 10.

As the graph is directed, two vertices can have
edges in both directions with different relations.

Edge Weights: The edge weights are set using
a similarity based attention module. The attention
function is computed in a way such that, for each
vertex, the incoming set of edges has a sum total
weight of 1. Considering a past context window
size of p and a future context window size of f ,
the weights are calculated as follows,

αij = softmax(gTi We[gi−p, . . . , gi+f ]),
for j = i − p, . . . , i + f.

(1)

This ensures that, vertex vi which has incom-
ing edges with vertices vi−p, . . . , vi+f (as speaker-
level context) receives a total weight contribution
of 1.

Relations: The relation r of an edge rij is set
depending upon two aspects:

Speaker dependency — The relation depends
on both the speakers of the constituting vertices:
ps(ui) (speaker of vi) and ps(uj) (speaker of vj).

Temporal dependency — The relation also de-



158

pends upon the relative position of occurrence of
ui and uj in the conversation: whether ui is uttered
before uj or after. If there are M distinct speakers
in a conversation, there can be a maximum of M
(speaker of ui) ∗M (speaker of uj) ∗2 (ui occurs
before uj or after) = 2M2 distinct relation types r
in the graph G.

Each speaker in a conversation is uniquely af-
fected by each other speaker, hence we hypoth-
esize that explicit declaration of such relational
edges in the graph would help in capturing the
inter-dependency and self-dependency among the
speakers in play, which in succession would facil-
itate speaker-level context encoding.

As an illustration, let two parties p1, p2 partici-
pate in a dyadic conversation having 5 utterances,
where u1, u3, u5 are uttered by p1 and u2, u4 are
uttered by p2. If we consider a fully connected
graph, the edges and relations will be constructed
as shown in Table 1.

Feature Transformation: We now describe
the methodology to transform the sequentially en-
coded features using the graph network. The ver-
tex feature vectors (gi) are initially speaker inde-
pendent and thereafter transformed into a speaker
dependent feature vector using a two-step graph
convolution process. Both of these transforma-
tions can be understood as special cases of a ba-
sic differentiable message passing method (Gilmer
et al., 2017).

In the first step, a new feature vector h(1)i is
computed for vertex vi by aggregating local neigh-
bourhood information (in this case neighbour ut-
terances specified by the past and future context
window size) using the relation specific transfor-
mation inspired from (Schlichtkrull et al., 2018):

h
(1)
i = σ(∑

r∈R
∑
j∈Nri

αij

ci,r
W (1)r gj + αiiW

(1)
0 gi),

for i = 1,2, . . . ,N,
(2)

where, αij and αii are the edge weights, N ri de-
notes the neighbouring indices of vertex i under
relation r ∈ R. ci,r is a problem specific nor-
malization constant which either can be set in ad-
vance, such that, ci,r = ∣N ri ∣, or can be automat-
ically learned in a gradient based learning setup.
σ is an activation function such as ReLU, W (1)r
and W (1)0 are learnable parameters of the transfor-
mation. In the second step, another local neigh-

bourhood based transformation is applied over the
output of the first step,

h
(2)
i = σ( ∑

j∈Nri

W (2)h
(1)
j +W

(2)
0 h

(1)
i ),

for i = 1,2, . . . ,N,
(3)

where, W (2) and W (2)0 are parameters of these
transformation and σ is the activation function.

This stack of transformations, Eqs. (2) and (3),
effectively accumulates normalized sum of the lo-
cal neighbourhood (features of the neighbours) i.e.
the neighbourhood speaker information for each
utterance in the graph. The self connection en-
sures self dependent feature transformation.

Emotion Classifier: The contextually encoded
feature vectors gi (from sequential encoder) and
h
(2)
i (from speaker-level encoder) are concate-

nated and a similarity-based attention mechanism
is applied to obtain the final utterance representa-
tion:

hi = [gi, h(2)i ], (4)
βi = softmax(hTi Wβ[h1, h2 . . . , hN ]), (5)
h̃i = βi[h1, h2, . . . , hN ]T . (6)

Finally, the utterance is classified using a fully-
connected network:

li = ReLU(Wlh̃i + bl), (7)
Pi = softmax(Wsmaxli + bsmax), (8)
ŷi = argmax

k
(Pi[k]). (9)

Relation ps(ui), ps(uj) i < j (i, j)

1 p1, p1 Yes (1,3), (1,5), (3,5)

2 p1, p1 No
(1,1), (3,1), (3,3)
(5,1), (5,3), (5,5)

3 p2, p2 Yes (2,4)
4 p2, p2 No (2,2), (4,2), (4, 4)
5 p1, p2 Yes (1,2), (1,4), (3,4)
6 p1, p2 No (3,2), (5,2), (5,4)
7 p2, p1 Yes (2,3), (2,5), (4,5)
8 p2, p1 No (2,1), (4,1), (4,3)

Table 1: ps(ui) and ps(uj) denotes the speaker of ut-
terances ui and uj . 2 distinct speakers in the conver-
sation implies 2 ∗M2 = 2 ∗ 22 = 8 distinct relation
types. The rightmost column denotes the indices of the
vertices of the constituting edge which has the relation
type indicated by the leftmost column.



159

Training Setup: We use categorical cross-
entropy along with L2-regularization as the mea-
sure of loss (L) during training:

L = − 1
∑Ns=1 c(s)

N

∑
i=1

c(i)
∑
j=1

logPi,j[yi,j] + λ ∥θ∥2 ,

(10)

where N is the number of samples/dialogues, c(i)
is the number of utterances in sample i, Pi,j is the
probability distribution of emotion labels for utter-
ance j of dialogue i, yi,j is the expected class label
of utterance j of dialogue i, λ is the L2-regularizer
weight, and θ is the set of all trainable parameters.

We used stochastic gradient descent based
Adam (Kingma and Ba, 2014) optimizer to train
our network. Hyperparameters were optimized us-
ing grid search.

4 Experimental Setting

4.1 Datasets Used
We evaluate our DialogueGCN model on three
benchmark datasets — IEMOCAP (Busso et al.,
2008), AVEC (Schuller et al., 2012), and
MELD (Poria et al., 2019a). All these three
datasets are multimodal datasets containing tex-
tual, visual and acoustic information for every ut-
terance of each conversation. However, in this
work we focus on conversational emotion recog-
nition only from the textual information. Multi-
modal emotion recognition is outside the scope of
this paper, and is left as future work.

IEMOCAP (Busso et al., 2008) dataset con-
tains videos of two-way conversations of ten
unique speakers, where only the first eight speak-
ers from session one to four belong to the train-
set. Each video contains a single dyadic dialogue,
segmented into utterances. The utterances are an-
notated with one of six emotion labels, which are
happy, sad, neutral, angry, excited, and frustrated.

AVEC (Schuller et al., 2012) dataset is a
modification of SEMAINE database (McKeown
et al., 2012) containing interactions between hu-
mans and artificially intelligent agents. Each
utterance of a dialogue is annotated with four
real valued affective attributes: valence ([−1,1]),
arousal ([−1,1]), expectancy ([−1,1]), and power
([0,∞)). The annotations are available every
0.2 seconds in the original database. However,
in order to adapt the annotations to our need of

utterance-level annotation, we averaged the at-
tributes over the span of an utterance.

MELD (Poria et al., 2019a) is a multimodal
emotion/sentiment classification dataset which has
been created by the extending the EmotionLines
dataset (Chen et al., 2018). Contrary to IEMO-
CAP and AVEC, MELD is a multiparty dialog
dataset. MELD contains textual, acoustic and vi-
sual information for more than 1400 dialogues
and 13000 utterances from the Friends TV series.
Each utterance in every dialog is annotated as one
of the seven emotion classes: anger, disgust, sad-
ness, joy, surprise, fear or neutral.

Dataset
# dialogues # utterances

train val test train val test
IEMOCAP 120 31 5810 1623
AVEC 63 32 4368 1430
MELD 1039 114 280 9989 1109 2610

Table 2: Training, validation and test data distribution
in the datasets. No predefined train/val split is provided
in IEMOCAP and AVEC, hence we use 10% of the
training dialogues as validation split.

4.2 Baselines and State of the Art
For a comprehensive evaluation of DialogueGCN,
we compare our model with the following baseline
methods:

CNN (Kim, 2014) This is the baseline convolu-
tional neural network based model which is identi-
cal to our utterance level feature extractor network
(Section 3.2). This model is context independent
as it doesn’t use information from contextual ut-
terances.

Memnet (Sukhbaatar et al., 2015) This is an
end-to-end memory network baseline (Hazarika
et al., 2018b). Every utterance is fed to the net-
work and the memories, which correspond to the
previous utterances, is continuously updated in a
multi-hop fashion. Finally the output from the
memory network is used for emotion classifica-
tion.

c-LSTM (Poria et al., 2017) Context-aware ut-
terance representations are generated by capturing
the contextual content from the surrounding ut-
terances using a Bi-directional LSTM (Hochreiter
and Schmidhuber, 1997) network. The context-
aware utterance representations are then used for
emotion classification. The contextual-LSTM



160

model is speaker independent as it doesn’t model
any speaker level dependency.

c-LSTM+Att (Poria et al., 2017) In this variant
of c-LSTM, an attention module is applied to the
output of c-LSTM at each timestamp by following
Eqs. (5) and (6). Generally this provides better
context to create a more informative final utterance
representation.

CMN (Hazarika et al., 2018b) CMN models
utterance context from dialogue history using two
distinct GRUs for two speakers. Finally, utterance
representation is obtained by feeding the current
utterance as query to two distinct memory net-
works for both speakers. However, this model can
only model conversations with two speakers.

ICON (Hazarika et al., 2018b) ICON which is
an extension of CMN, connects outputs of indi-
vidual speaker GRUs in CMN using another GRU
for explicit inter-speaker modeling. This GRU is
considered as a memory to track the overall con-
versational flow. Similar to CMN, ICON can not
be extended to apply on multiparty datasets e.g.,
MELD.

DialogueRNN (Majumder et al., 2019) This is
the state-of-the-art method for ERC. It is a recur-
rent network that uses two GRUs to track individ-
ual speaker states and global context during the
conversation. Further, another GRU is employed
to track emotional state through the conversation.
DialogueRNN claims to model inter-speaker rela-
tion and it can be applied on multiparty datasets.

5 Results and Discussions

5.1 Comparison with State of the Art and
Baseline

We compare the performance of our proposed Di-
alogueGCN framework with the state-of-the-art
DialogueRNN and baseline methods in Tables 3
and 4. We report all results with average of 5 runs.
Our DialogueGCN model outperforms the SOTA
and all the baseline models, on all the datasets,
while also being statistically significant under the
paired t-test (p <0.05).

IEMOCAP and AVEC: On the IEMOCAP
dataset, DialogueGCN achieves new state-of-the-
art average F1-score of 64.18% and accuracy of
65.25%, which is around 2% better than Dia-
logueRNN, and at least 5% better than all the other

baseline models. Similarly, on AVEC dataset, Di-
alogueGCN outperforms the state-of-the-art on all
the four emotion dimensions: valence, arousal, ex-
pectancy, and power.

To explain this gap in performance, it is im-
portant to understand the nature of these mod-
els. DialogueGCN and DialogueRNN both try
to model speaker-level context (albeit differently),
whereas, none of the other models encode speaker-
level context (they only encode sequential con-
text). This is a key limitation in the baseline mod-
els, as speaker-level context is indeed very impor-
tant in conversational emotion recognition.

As for the difference of performance between
DialogueRNN and DialogueGCN, we believe that
this is due to the different nature of speaker-level
context encoding. DialogueRNN employs a gated
recurrent unit (GRU) network to model individ-
ual speaker states. Both IEMOCAP and AVEC
dataset has many conversations with over 70 utter-
ances (the average conversation length is 50 utter-
ances in IEMOCAP and 72 in AVEC). As recur-
rent encoders have long-term information propa-
gation issues, speaker-level encoding can be prob-
lematic for long sequences like those found in
these two datasets. In contrast, DialogueGCN tries
to overcome this issue by using neighbourhood
based convolution to model speaker-level context.

MELD: The MELD dataset consists of multi-
party conversations and we found that emotion
recognition in MELD is considerably harder to
model than IEMOCAP and AVEC - which only
consists of dyadic conversations. Utterances in
MELD are much shorter and rarely contain emo-
tion specific expressions, which means emotion
modelling is highly context dependent. Moreover,
the average conversation length is 10 utterances,
with many conversations having more than 5 par-
ticipants, which means majority of the participants
only utter a small number of utterances per con-
versation. This makes inter-dependency and self-
dependency modeling difficult. Because of these
reasons, we found that the difference in results be-
tween the baseline models and DialogueGCN is
not as contrasting as it is in the case of IEMOCAP
and AVEC. Memnet, CMN, and ICON are not
suitable for this dataset as they exclusively work in
dyadic conversations. Our DialogueGCN model
achieves new state-of-the-art F1 score of 58.10%
outperforming DialogueRNN by more than 1%.



161

Methods
IEMOCAP

Happy Sad Neutral Angry Excited Frustrated Average(w)
Acc. F1 Acc. F1 Acc. F1 Acc. F1 Acc. F1 Acc. F1 Acc. F1

CNN 27.77 29.86 57.14 53.83 34.33 40.14 61.17 52.44 46.15 50.09 62.99 55.75 48.92 48.18
Memnet 25.72 33.53 55.53 61.77 58.12 52.84 59.32 55.39 51.50 58.30 67.20 59.00 55.72 55.10
bc-LSTM 29.17 34.43 57.14 60.87 54.17 51.81 57.06 56.73 51.17 57.95 67.19 58.92 55.21 54.95
bc-LSTM+Att 30.56 35.63 56.73 62.90 57.55 53.00 59.41 59.24 52.84 58.85 65.88 59.41 56.32 56.19
CMN 25.00 30.38 55.92 62.41 52.86 52.39 61.76 59.83 55.52 60.25 71.13 60.69 56.56 56.13
ICON 22.22 29.91 58.78 64.57 62.76 57.38 64.71 63.04 58.86 63.42 67.19 60.81 59.09 58.54
DialogueRNN 25.69 33.18 75.10 78.80 58.59 59.21 64.71 65.28 80.27 71.86 61.15 58.91 63.40 62.75
DialogueGCN 40.62 42.75 89.14 84.54 61.92 63.54 67.53 64.19 65.46 63.08 64.18 66.99 65.25 64.18

Table 3: Comparison with the baseline methods on IEMOCAP dataset; Acc. = Accuracy; bold font denotes the
best performances. Average(w) = Weighted average.

Speaker 1 

There's nothing I can 
do for you, ma'am. 

angry 

Speaker 2

I don't-


frustrated

Speaker 2

I don't have time for this 
today, and-


frustrated

(a)

Okay 
frustrated 

I mean we've gone through 
all of this.  I've been to five 
people already who—


frustrated

Yes, lots of really like -- 
sentimental value only, but
—


frustrated

(b)

Figure 4: Visualization of edge-weights in Eq. (1) — (a) Target utterance attends to other speaker’s utterance for
correct context; (b) Short utterance attends to appropriate contextual utterances to be classified correctly.

We surmise that this improvement is a result of the
speaker dependent relation modelling of the edges
in our graph network which inherently improves
the context understanding over DialogueRNN.

5.2 Effect of Context Window

We report results for DialogueGCN model in Ta-
bles 3 and 4 with a past and future context window
size of (10, 10) to construct the edges. We also car-
ried out experiments with decreasing context win-
dow sizes of (8, 8), (4, 4), (0, 0) and found that
performance steadily decreased with F1 scores of
62.48%, 59.41% and 55.80% on IEMOCAP. Di-
alogueGCN with context window size of (0, 0) is
equivalent to a model with only sequential encoder
(as it only has self edges), and performance is ex-
pectedly much worse. We couldn’t perform exten-
sive experiments with larger windows because of
computational constraints, but we expect perfor-
mance to improve with larger context sizes.

5.3 Ablation Study

We perform ablation study for different level of
context encoders, namely sequential encoder and
speaker-level encoder, in Table 5. We remove
them one at a time and found that the speaker-

level encoder is slightly more important in over-
all performance. This is due to speaker-level en-
coder mitigating long distance dependency issue
of sequential encoder and DialogueRNN. Remov-
ing both of them results in a very poor F1 score
of 36.7 %, which demonstrates the importance
of contextual modelling in conversational emotion
recognition.

Further, we study the effect of edge relation
modelling. As mentioned in Section 3.3.2, there
are total 2M2 distinct edge relations for a con-
versation with M distinct speakers. First we re-
moved only the temporal dependency (resulting
in M2 distinct edge relations), and then only the
speaker dependency (resulting in 2 distinct edge
relations) and then both (resulting in a single edge
relation all throughout the graph). The results
of these tests in Table 6 show that having these
different relational edges is indeed very impor-
tant for modelling emotional dynamics. These re-
sults support our hypothesis that each speaker in
a conversation is uniquely affected by the others,
and hence, modelling interlocutors-dependency is
rudimentary. Fig. 4a illustrates one such instance
where target utterance attends to other speaker’s
utterance for context. This phenomenon is com-



162

Methods
AVEC

MELD
Valence Arousal Expectancy Power

CNN 0.545 0.542 0.605 8.71 55.02
Memnet 0.202 0.211 0.216 8.97 -
bc-LSTM 0.194 0.212 0.201 8.90 56.44
bc-LSTM+Att 0.189 0.213 0.190 8.67 56.70
CMN 0.192 0.213 0.195 8.74 -
ICON 0.180 0.190 0.180 8.45 -
DialogueRNN 0.168 0.165 0.175 7.90 57.03
DialogueGCN 0.157 0.161 0.168 7.68 58.10

Table 4: Comparison with the baseline methods on AVEC and MELD dataset; MAE and F1 metrics are user for
AVEC and MELD, respectively.

Sequential
Encoder

Speaker-Level
Encoder

F1

3 3 64.18
3 7 55.30
7 3 56.71
7 7 36.75

Table 5: Ablation results w.r.t the contextual encoder
modules on IEMOCAP dataset.

Speaker
Dependency

Edges

Temporal
Dependency

Edges
F1

3 3 64.18
3 7 62.52
7 3 61.03
7 7 60.11

Table 6: Ablation results w.r.t the edge relations in
speaker-level encoder module on IEMOCAP dataset.

monly observable for DialogueGCN, as compared
to DialogueRNN.

5.4 Performance on Short Utterances

Emotion of short utterances, like “okay”, “yeah”,
depends on the context it appears in. For exam-
ple, without context “okay” is assumed ‘neutral’.
However, in Fig. 4b, DialogueGCN correctly clas-
sifies “okay” as ‘frustration’, which is apparent
from the context. We observed that, overall, Di-
alogueGCN correctly classifies short utterances,
where DialogueRNN fails.

5.5 Error Analysis
We analyzed our predicted emotion labels and
found that misclassifications are often among sim-
ilar emotion classes. In the confusion matrix, we
observed that our model misclassifies several sam-
ples of ‘frustrated’ as ‘angry’ and ‘neutral’. This
is due to subtle difference between frustration and
anger. Further, we also observed similar misclassi-
fication of ‘excited’ samples as ‘happy’ and ‘neu-
tral’. All the datasets that we use in our experi-
ment are multimodal. A few utterances e.g., ‘ok.
yes’ carrying non-neutral emotions were misclas-
sified as we do not utilize audio and visual modal-
ity in our experiments. In such utterances, we
found audio and visual (in this particular exam-
ple, high pitched audio and frowning expression)
modality providing key information to detect un-
derlying emotions (frustrated in the above utter-
ance) which DialogueGCN failed to understand by
just looking at the textual context.

6 Conclusion

In this work, we present Dialogue Graph Con-
volutional Network (DialogueGCN), that mod-
els inter and self-party dependency to improve
context understanding for utterance-level emo-
tion detection in conversations. On three bench-
mark ERC datasets, DialogueGCN outperforms
the strong baselines and existing state of the art,
by a significant margin. Future works will fo-
cus on incorporating multimodal information into
DialogueGCN, speaker-level emotion shift detec-
tion, and conceptual grounding of conversational
emotion reasoning. We also plan to use Dia-
logueGCN in dialogue systems to generate affec-
tive responses.



163

References
James Bradbury, Stephen Merity, Caiming Xiong, and

Richard Socher. 2017. Quasi-Recurrent Neural Net-
works. In International Conference on Learning
Representations (ICLR 2017).

Joan Bruna, Wojciech Zaremba, Arthur Szlam, and
Yann LeCun. 2013. Spectral networks and lo-
cally connected networks on graphs. arXiv preprint
arXiv:1312.6203.

Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe
Kazemzadeh, Emily Mower, Samuel Kim, Jean-
nette N Chang, Sungbok Lee, and Shrikanth S
Narayanan. 2008. IEMOCAP: Interactive emo-
tional dyadic motion capture database. Language
resources and evaluation, 42(4):335–359.

Minghai Chen, Sen Wang, Paul Pu Liang, Tadas Bal-
trušaitis, Amir Zadeh, and Louis-Philippe Morency.
2017. Multimodal sentiment analysis with word-
level fusion and reinforcement learning. In Proceed-
ings of the 19th ACM International Conference on
Multimodal Interaction, pages 163–171. ACM.

Sheng-Yeh Chen, Chao-Chun Hsu, Chuan-Chun Kuo,
Lun-Wei Ku, et al. 2018. Emotionlines: An emotion
corpus of multi-party conversations. arXiv preprint
arXiv:1802.08379.

Junyoung Chung, Çaglar Gülçehre, KyungHyun Cho,
and Yoshua Bengio. 2014. Empirical Evaluation
of Gated Recurrent Neural Networks on Sequence
Modeling. CoRR, abs/1412.3555.

Niko Colneriĉ and Janez Demsar. 2018. Emotion
recognition on twitter: comparative study and train-
ing a unison model. IEEE Transactions on Affective
Computing.

Michaël Defferrard, Xavier Bresson, and Pierre Van-
dergheynst. 2016. Convolutional Neural Networks
on Graphs with Fast Localized Spectral Filtering. In
D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon,
and R. Garnett, editors, Advances in Neural Infor-
mation Processing Systems 29, pages 3844–3852.
Curran Associates, Inc.

Justin Gilmer, Samuel S Schoenholz, Patrick F Riley,
Oriol Vinyals, and George E Dahl. 2017. Neural
message passing for quantum chemistry. In Pro-
ceedings of the 34th International Conference on
Machine Learning-Volume 70, pages 1263–1272.
JMLR. org.

Devamanyu Hazarika, Soujanya Poria, Rada Mihal-
cea, Erik Cambria, and Roger Zimmermann. 2018a.
Icon: Interactive conversational memory network
for multimodal emotion detection. In Proceedings
of the 2018 Conference on Empirical Methods in
Natural Language Processing, pages 2594–2604.

Devamanyu Hazarika, Soujanya Poria, Amir Zadeh,
Erik Cambria, Louis-Philippe Morency, and Roger

Zimmermann. 2018b. Conversational Memory Net-
work for Emotion Recognition in Dyadic Dialogue
Videos. In Proceedings of the 2018 Conference
of the North American Chapter of the Associa-
tion for Computational Linguistics: Human Lan-
guage Technologies, Volume 1 (Long Papers), pages
2122–2132, New Orleans, Louisiana. Association
for Computational Linguistics.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Chenyang Huang, Amine Trabelsi, and Osmar R
Zaı̈ane. 2019. Ana at semeval-2019 task 3: Con-
textual emotion detection in conversations through
hierarchical lstms and bert. arXiv preprint
arXiv:1904.00132.

Sidney K. D’Mello, Scotty Craig, Jeremiah Sullins, and
Arthur Graesser. 2006. Predicting affective states
expressed through an emote-aloud procedure from
autotutor’s mixed-initiative dialogue. I. J. Artificial
Intelligence in Education, 16:3–28.

Yoon Kim. 2014. Convolutional neural networks for
sentence classification. In EMNLP 2014, pages
1746–1751.

Diederik P. Kingma and Jimmy Ba. 2014. Adam:
A Method for Stochastic Optimization. CoRR,
abs/1412.6980.

Thomas N Kipf and Max Welling. 2016. Semi-
supervised classification with graph convolutional
networks. arXiv preprint arXiv:1609.02907.

Bernhard Kratzwald, Suzana Ilic, Mathias Kraus, Ste-
fan Feuerriegel, and Helmut Prendinger. 2018. De-
cision support with text-based emotion recogni-
tion: Deep learning for affective computing. arXiv
preprint arXiv:1803.06397.

Navonil Majumder, Soujanya Poria, Devamanyu Haz-
arika, Rada Mihalcea, Alexander Gelbukh, and Erik
Cambria. 2019. DialogueRNN: An Attentive RNN
for Emotion Detection in Conversations. In Pro-
ceedings of the AAAI Conference on Artificial Intel-
ligence, volume 33, pages 6818–6825.

G. McKeown, M. Valstar, R. Cowie, M. Pantic, and
M. Schroder. 2012. The SEMAINE Database: An-
notated Multimodal Records of Emotionally Col-
ored Conversations between a Person and a Limited
Agent. IEEE Transactions on Affective Computing,
3(1):5–17.

Vinod Nair and Geoffrey E Hinton. 2010. Rectified
linear units improve restricted boltzmann machines.
In Proceedings of the 27th international conference
on machine learning (ICML-10), pages 807–814.

Costanza Navarretta, K Choukri, T Declerck, S Goggi,
M Grobelnik, and B Maegaard. 2016. Mirroring fa-
cial expressions and emotions in dyadic conversa-
tions. In LREC.

http://arxiv.org/abs/1412.3555
http://arxiv.org/abs/1412.3555
http://arxiv.org/abs/1412.3555
http://papers.nips.cc/paper/6081-convolutional-neural-networks-on-graphs-with-fast-localized-spectral-filtering.pdf
http://papers.nips.cc/paper/6081-convolutional-neural-networks-on-graphs-with-fast-localized-spectral-filtering.pdf
http://www.aclweb.org/anthology/N18-1193
http://www.aclweb.org/anthology/N18-1193
http://www.aclweb.org/anthology/N18-1193
http://arxiv.org/abs/1412.6980
http://arxiv.org/abs/1412.6980
https://doi.org/10.1609/aaai.v33i01.33016818
https://doi.org/10.1609/aaai.v33i01.33016818
https://doi.org/10.1109/T-AFFC.2011.20
https://doi.org/10.1109/T-AFFC.2011.20
https://doi.org/10.1109/T-AFFC.2011.20
https://doi.org/10.1109/T-AFFC.2011.20


164

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. In Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1532–
1543.

Soujanya Poria, Erik Cambria, Devamanyu Hazarika,
Navonil Majumder, Amir Zadeh, and Louis-Philippe
Morency. 2017. Context-Dependent Sentiment
Analysis in User-Generated Videos. In Proceed-
ings of the 55th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 873–883, Vancouver, Canada. Associa-
tion for Computational Linguistics.

Soujanya Poria, Devamanyu Hazarika, Navonil Ma-
jumder, Gautam Naik, Erik Cambria, and Rada Mi-
halcea. 2019a. MELD: A multimodal multi-party
dataset for emotion recognition in conversations. In
Proceedings of the 57th Annual Meeting of the As-
sociation for Computational Linguistics, pages 527–
536, Florence, Italy. Association for Computational
Linguistics.

Soujanya Poria, Navonil Majumder, Rada Mihalcea,
and Eduard Hovy. 2019b. Emotion recognition in
conversation: Research challenges, datasets, and re-
cent advances. IEEE Access, 7:100943–100953.

Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus
Hagenbuchner, and Gabriele Monfardini. 2008. The
graph neural network model. IEEE Transactions on
Neural Networks, 20(1):61–80.

Michael Schlichtkrull, Thomas N Kipf, Peter Bloem,
Rianne Van Den Berg, Ivan Titov, and Max Welling.
2018. Modeling relational data with graph convolu-
tional networks. In European Semantic Web Confer-
ence, pages 593–607. Springer.

Björn Schuller, Michel Valster, Florian Eyben, Roddy
Cowie, and Maja Pantic. 2012. AVEC 2012: The
Continuous Audio/Visual Emotion Challenge. In
Proceedings of the 14th ACM International Confer-
ence on Multimodal Interaction, ICMI ’12, pages
449–456, New York, NY, USA. ACM.

Carlo Strapparava and Rada Mihalcea. 2010. Annotat-
ing and identifying emotions in text. In Intelligent
Information Access, pages 21–38. Springer.

Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston,
and Rob Fergus. 2015. End-to-end Memory Net-
works. In Proceedings of the 28th International
Conference on Neural Information Processing Sys-
tems - Volume 2, NIPS’15, pages 2440–2448, Cam-
bridge, MA, USA. MIT Press.

Amir Zadeh, Paul Pu Liang, Navonil Mazumder,
Soujanya Poria, Erik Cambria, and Louis-Philippe
Morency. 2018a. Memory Fusion Network for
Multi-view Sequential Learning. In AAAI Confer-
ence on Artificial Intelligence, pages 5634–5641.

Amir Zadeh, Paul Pu Liang, Soujanya Poria, Prateek
Vij, Erik Cambria, and Louis-Philippe Morency.
2018b. Multi-attention recurrent network for hu-
man communication comprehension. In Proceed-
ings of the AAAI Conference on Artificial Intelli-
gence, pages 5642–5649.

Hao Zhou, Minlie Huang, Tianyang Zhang, Xiaoyan
Zhu, and Bing Liu. 2018. Emotional chatting ma-
chine: Emotional conversation generation with in-
ternal and external memory. In Thirty-Second AAAI
Conference on Artificial Intelligence.

http://www.aclweb.org/anthology/D14-1162
http://www.aclweb.org/anthology/D14-1162
http://aclweb.org/anthology/P17-1081
http://aclweb.org/anthology/P17-1081
https://www.aclweb.org/anthology/P19-1050
https://www.aclweb.org/anthology/P19-1050
https://doi.org/10.1109/ACCESS.2019.2929050
https://doi.org/10.1109/ACCESS.2019.2929050
https://doi.org/10.1109/ACCESS.2019.2929050
https://doi.org/10.1145/2388676.2388776
https://doi.org/10.1145/2388676.2388776
http://dl.acm.org/citation.cfm?id=2969442.2969512
http://dl.acm.org/citation.cfm?id=2969442.2969512
https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17341/16122
https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17341/16122

