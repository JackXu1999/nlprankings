



















































Injecting Word Embeddings with Another Language's Resource : An Application of Bilingual Embeddings


Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 116–121,
Taipei, Taiwan, November 27 – December 1, 2017 c©2017 AFNLP

Injecting Word Embeddings with Another Language’s Resource : An
Application of Bilingual Embeddings

Prakhar Pandey Vikram Pudi Manish Shrivastava
International Institute of Information Technology

Hyderabad, Telangana, India
prakhar.pandey@research.iiit.ac.in

{vikram,manish.shrivastava}@.iiit.ac.in

Abstract

Word embeddings learned from text cor-
pus can be improved by injecting knowl-
edge from external resources, while at
the same time also specializing them for
similarity or relatedness. These knowl-
edge resources (like WordNet, Paraphrase
Database) may not exist for all languages.
In this work we introduce a method to in-
ject word embeddings of a language with
knowledge resource of another language
by leveraging bilingual embeddings. First
we improve word embeddings of Ger-
man, Italian, French and Spanish using re-
sources of English and test them on variety
of word similarity tasks. Then we demon-
strate the utility of our method by creating
improved embeddings for Urdu and Tel-
ugu languages using Hindi WordNet, beat-
ing the previously established baseline for
Urdu.

1 Introduction

Recently fast and scalable methods to generate
dense vector space models have become very pop-
ular following the works of (Collobert and We-
ston, 2008; Mikolov et al., 2013; Pennington et al.,
2014). These methods take large amounts of text
corpus to generate real valued vector representa-
tion for words (word embeddings) which carry
many semantic properties.

Mikolov et al. (2013b) extended this model
to two languages by introducing bilingual embed-
dings where word embeddings for two languages
are simultaneously represented in the same vec-
tor space. The model is trained such that word
embeddings capture not only semantic informa-
tion of monolingual words, but also semantic re-
lationships across different languages. A number

of different methods have since been proposed to
construct bilingual embeddings (Zou et al., 2013;
Vulic and Moens, 2015; Coulmance et al., 2016).

A disadvantage of learning word embeddings
only from text corpus is that valuable knowl-
edge contained in knowledge resources like Word-
Net (Miller, 1995) is not used. Numerous meth-
ods have been proposed to incorporate knowledge
from external resources into word embeddings for
their refinement (Xu et al., 2014; Bian et al., 2014;
Mrksic et al., 2016). (Faruqui et al., 2015) intro-
duced retrofitting as a light graph based technique
that improves learned word embeddings.

In this work we introduce a method to im-
prove word embeddings of one language (tar-
get language) using knowledge resources from
some other similar language (source language).
To accomplish this, we represent both languages
in the same vector space (bilingual embeddings)
and obtain translations of source language’s re-
sources. Then we use these translations to im-
prove the embeddings of the target language by
using retrofitting, leveraging the information con-
tained in bilingual space to adjust retrofitting pro-
cess and handle noise. We also show why a dic-
tionary based translation would be ineffective for
this problem and how to handle situations where
vocabulary of target embeddings is too big or too
small compared to size of resource.

(Kiela et al., 2015) demonstrated the advantage
of specializing word embeddings for either sim-
ilarity or relatedness, which we also incorporate.
Our method is also independent of the way bilin-
gual embeddings were obtained. An added advan-
tage of using bilingual embeddings is that they are
better than monolingual counterparts due to incor-
porating multilingual evidence (Faruqui and Dyer,
2014; Mrkšić et al., 2017).

116



2 Background

2.1 Bilingual Embeddings
Various methods have been proposed to generate
bilingual embeddings. One class of methods learn
mappings to transform words from one monolin-
gual model to another, using some form of dic-
tionary (Mikolov et al., 2013b; Faruqui and Dyer,
2014). Another class of methods jointly optimize
monolingual and cross-lingual objectives using
word aligned parallel corpus (Klementiev et al.,
2012; Zou et al., 2013) or sentence aligned par-
allel corpus (Chandar A P et al., 2014; Hermann
and Blunsom, 2014). Also there are other meth-
ods which use monolingual data and a smaller set
of sentence aligned parallel corpus (Coulmance
et al., 2016) and those which use non-parallel doc-
ument aligned data (Vulic and Moens, 2015).

We experiment with translation invariant bilin-
gual embeddings by (Gardner et al., 2015). We
also experiment with method proposed by (Artetxe
et al., 2016) where they learn a linear transform
between two monolingual embeddings with mono-
lingual invariance preserved. They use a small
bilingual dictionary to accomplish this task. These
methods are useful in our situation because they
preserve the quality of original monolingual em-
beddings and do not require parallel text (benefi-
cial in case of Indian languages).

2.2 Retrofitting
Retrofitting was introduced by (Faruqui et al.,
2015) as a light graph based procedure for enrich-
ing word embeddings with semantic lexicons. The
method operates post processing i.e it can be ap-
plied to word embeddings obtained from any stan-
dard technique such as Word2vec, Glove etc. The
method encourages improved vectors to be simi-
lar to the vectors of similar words as well as sim-
ilar to the original vectors. This similarity rela-
tion among words (such as synonymy, hypernymy,
paraphrase) is derived from a knowledge resource
such as PPDB, WordNet etc. Retrofitting works as
follows:

Let matrix Q contain the word embeddings to
be improved. Let V = {w1, w2...wn} be the vocab-
ulary which is equal to number of rows in Q and
d be the dimension of word vectors which is equal
to number of columns. Also let Ω be the ontol-
ogy that contains the intra word relations that must
be injected into the embeddings. The objective of
retrofitting is to find a matrix Q̂ such that the new

word vectors are close to their original vectors as
well as vectors of related words. The function to
be minimized to accomplish this objective is:

Φ(Q) =
n∑
i=1

[
αi‖qi − q̂i‖2 +

∑
(i,j)∈E

βij‖qi − qj‖2
]

The iterative update equation is:

qi =
∑
j:(i,j)∈E βijqj + αiq̂i∑
j:(i,j)∈E βij + αi

α and β are the parameters used to control the
process. We discuss in Section 3.2 how we set
them to adapt the process to bilingual settings.

2.3 Dictionary based approach

Before discussing our method, we would like to
point that using a dictionary for translating the lex-
ical resource and then retrofitting with this trans-
lated resource is not feasible. Firstly obtaining
good quality dictionaries is a difficult and costly
process1. Secondly it is not necessary that one
would obtain translations that are within the vo-
cabulary of the embeddings to be improved. To
demonstrate this, we obtain translations for em-
beddings of 3 languages2 and show the results in
Table 1. In all cases the number of translations that
are also present in the embedding’s vocabulary are
too small.

Language Vocab Matches
German 43,527 9,950
Italian 73,427 24,716
Spanish 41,779 16,547

Table 1: Using a dictionary based approach

3 Approach

Let S, T and R be the vocabularies of source, tar-
get and resource respectively. Size of R is always
fixed while size of S and T depends on embed-
dings. The relation between S, T and R is shown
in Figure 1. Sets S and R have one to one map-
ping which in not necessarily onto, while T and
S have many to one mapping. Consider the ideal
case where every word in R is also in S and every
word in S has the exact translation from T as its
nearest neighbour in the bilingual space. Then the

1eg. Google and Bing Translate APIs have become paid.
2using Yandex Translate API, it took around 3 days

117



figlio

ragazzo
boy

T S R

Figure 1: Relationships between Source, Target
and Resource Vocabularies.

simple approach for translation would be assign-
ing every si ∈ S its nearest neighbour ti ∈ T as
the translation.

First problem is that in practical settings these
conditions are almost never satisfied. Secondly the
sizes of S, T andR can be very different. Suppose
the size of S, T is large compared to R or the size
of T is large but size of S is comparatively smaller.
In both cases size of translated resource will be too
small to make impact. Thirdly words common to
both R and S will be lesser than the total words in
R. So the size of R accessible to T using S will
be even lesser. A mechanism is therefore required
to control the size of translated resource and fil-
ter incorrect translations. We accomplish this as
follows:

3.1 Translating knowledge resource
For translation we adapt a dual approach that al-
lows control over the size of the translated list. We
iterate through T (not S) looking for translations
in S. A translation is accepted or rejected based
on whether the cosine similarity between words is
above the threshold η. This method stems from
the fact that mapping between T and S is many to
one. So the Italian word ragazzo is translated to
boy, but we can also translate (and subsequently
retrofit) figlio to boy (Figure 1) in order to get a
larger translated resource list with some loss in
quality of list. Thus η gives us direct control over
the size of translated resource list. Then to trans-
late the list of related words, we translate normally
(i.e from S to T ). Algorithm 1 summarizes this
process.

3.2 Retrofitting to translated resource
We modify the retrofitting procedure to accommo-
date noisy translations as follows:

As discussed earlier, retrofitting process con-
trols the relative shift of vectors by two parameters
α and β, where α controls the movement towards

original vector and β controls movement towards
vectors to be fitted with. (Faruqui et al., 2015) set
α as 1 and β as 1γ where γ is the number of vectors
to be fitted with. Thus they give equal weights to
each vector.

Cosine similarity between a word and its trans-
lation is a good measure of the confidence in its
translation. We use it to set β such that differ-
ent vectors get different weights. A word for
which we have higher confidence in translation
(i.e higher cosine similarity) is given more weight
when retrofitting. Therefore wi being the weights,
α, β are set as :

α =
γ∑
i=1

wi, βi = wi

Further reduction in weights of noisy words can
be done by taking powers of cosine similarity. An
example in Table 2 shows weights of similar words
for Italian word professore derived by taking pow-
ers of cosine similarity (we refer to this power as
filter parameter).

Words Similarity Weights
educatore 0.955 0.796
harvard 0.853 0.452
insegnando 0.980 0.903
insegnata 0.990 0.951

Table 2: Taking power of weights reduces weights
of noisy words (here harvard). Here filter = 5.

Algorithm 1 Translating Knowledge Resource
Input : Source (S), Target (T ), Resource (R), η
Output : Translated Knowledge Resource
R∗

R∗ = []
for t in T do
ts ← NearestNeighbour(S)
if similarity(t, ts) > η then
lexicons← S[ts]
for l in lexicons do
lt ← NearestNeighbour(T )
weight← similarity(l, lt)
R∗[t].add(lt, weight)

end for
else
continue

end if
end for

118



Language Vocab TRL Tasks Original Half En-riched
Full En-
riched

German 43,527
18,802
37,408

MC30
RG65
WS353 (sim)
Simlex999

0.631
0.503
0.600
0.333

0.643
0.531
0.631
0.356

0.662
0.600
0.635
0.373

Italian 73,427
22,022
44,309

WS353 (sim)
Simlex999

0.595
0.247

0.640
0.283

0.652
0.313

Spanish 41,779
17,434
35,034

MC30
RG65

0.312
0.608

0.286
0.615

0.412
0.634

French 40,523
16,203
32,602

RG65 0.547 0.582 0.673

Table 3: Retrofitting Translation Invariant Bilingual Embeddings for German, Italian, Spanish and
French using English Paraphrase Database. (TRL stands for Translated Resource Length)

4 Datasets and Benchmarks

For English as source language, we use the Para-
phrase Database (Ganitkevitch et al., 2013) to spe-
cialize embeddings for similarity as it gives the
best results (compared to other sources like Word-
Net). To specialize embeddings for relatedness,
we use University of South Florida Free Asso-
ciation Norms (Nelson et al., 2004) as indicated
by (Kiela et al., 2015). For Hindi as source
language, we use Hindi WordNet (Bhattacharyya
et al., 2010). Whenever the size of resource is big
enough, we first inject word embeddings with half
of the dataset (random selection) followed by full
length dataset to demonstrate the sequential gain
in performance.
Multilingual WS353 and SimLex999 datasets are
by (Leviant and Reichart, 2015). We also use
German RG65 (Gurevych, 2005), French RG65
(Joubarne and Inkpen, 2011) and Spanish MC30,
RG65 (Hassan and Mihalcea, 2009; Camacho-
Collados et al., 2015). For Indian languages we
use datasets provided by (Akhtar et al., 2017).

5 Results

In this section we present the experimental results
of our method.3 Before discussing the results we
explain how different parameters are chosen. We
do 10 iterations of retrofitting process for all our
experiments because 10 iterations are enough for
convergence (Faruqui et al., 2015) and also using
the same value for all experiments avoids over-
fitting. The value of filter parameter is set as 2

3The implementation of our method is available at
https://github.com/prakhar987/InjectingBilingual

because we believe the embeddings that we use are
well trained and low in noise. This value can be
increased further if word embeddings being used
are very noisy but in most cases a value of 2 is
enough. η value, as explained in previous sections
is set such that the translated resource obtained is
of sufficient length. If more lexicons in translated
resource are required, relax η and vice-versa.

5.1 European Languages
Table 3 shows the result of retrofitting translation
invariant bilingual embeddings of four European
languages for similarity using English Paraphrase
Database. For each language we set η as 0.70 and
filter as 2. The embeddings are evaluated specif-
ically on datasets measuring similarity. All em-
beddings are 40 dimensional. To show that our
method is effective, the embeddings are first fitted
with only half of the database followed by fitting
with full length database. Table 3 also contains in-
formation about the size of vocabulary and trans-
lated resource. One can compare the size of trans-
lated resource that we get using our method to the
dictionary based approach.

Table 4 shows the results of specializing word
embeddings for relatedness using the USF Asso-
ciation Norms and evaluation on WS353 related-
ness task. We test only with German and Italian as
only these languages had datasets to test for relat-
edness.

Language Original Fitted
German 0.461 0.520
Italian 0.460 0.523

Table 4: Specializing for relatedness

119



We also experiment with embeddings of large
dimensions (300) and large vocabulary size
(200,000) for English and Italian bilingual embed-
dings obtained by method described by (Artetxe
et al., 2016). Table 5 shows the improvements at-
tained for similarity task for Italian with 64,434
words in the translated resource, η = 0.35 and
filter = 2 (notice η is much smaller since we
want translated resource size to be comparable to
size of vocabulary).

Task Original Fitted
WS353 0.648 0.680
SimLex999 0.371 0.405

Table 5: Improving large embeddings

5.2 Indian Languages

To demonstrate the utility of our method, we ex-
periment with Indian languages, taking Hindi as
the source language (which has Hindi WordNet).
For target language, we take one language (Urdu)
which is very similar to Hindi (belongs to same
family) and one language (Telugu) which is very
different from Hindi (descendants from same fam-
ily). The vocabulary size of Urdu and Telugu were
129,863 and 174,008 respectively. The results are
shown in Table 6. Here again we fit with half
length followed by full length of Hindi WordNet.
As expected, we get much higher gains for Urdu
compared to Telugu.4

Language Original HalfFitted
Full
Fitted

Telugu 0.427 0.436 0.440
Urdu 0.541 0.589 0.612

Table 6: Retrofitting Indian languages

6 Conclusion

In this work we introduced a method to improve
word embeddings of a language using resources
from another similar language. We accomplished
this by translating the resource using bilingual
embeddings and modifying retrofitting while han-
dling noise. Using our method, we also created
new benchmark on Urdu word similarity dataset.

4enriched embeddings and evaluation scripts can be
downloaded from https://goo.gl/tN6p3w

References
Syed Sarfaraz Akhtar, Arihant Gupta, Avijit Vajpayee,

Arjit Srivastava, and Manish Shrivastava. 2017.
Word similarity datasets for indian languages: An-
notation and baseline systems. In Proceedings of the
11th Linguistic Annotation Workshop, pages 91–94.
Association for Computational Linguistics.

Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2016.
Learning principled bilingual mappings of word em-
beddings while preserving monolingual invariance.
In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, EMNLP
2016, pages 2289–2294.

Pushpak Bhattacharyya, Prabhakar Pande, and Laxmi
Lupu. 2010. Hindi wordnet. Language Resources
and Evaluation (LRE).

Jiang Bian, Bin Gao, and Tie-Yan Liu. 2014.
Knowledge-powered deep learning for word embed-
ding. In Proceedings of the European Conference
on Machine Learning and Knowledge Discovery in
Databases - Volume 8724, pages 132–148.

Jos Camacho-Collados, Mohammad Taher Pilehvar,
and Roberto Navigli. 2015. A framework for the
construction of monolingual and cross-lingualword
similarity datasets. In Proceedings of 53rd Annual
Meeting of the Association for Computational Lin-
guistics and the 7th International Joint Conference
on Natural Language Processing of the Asian Fed-
eration of Natural Language Processing, volume 2,
pages 1–7.

Sarath Chandar A P, Stanislas Lauly, Hugo Larochelle,
Mitesh Khapra, Balaraman Ravindran, Vikas C
Raykar, and Amrita Saha. 2014. An autoencoder
approach to learning bilingual word representations.
In Proceedings of Neural Information Processing
Systems, pages 1853–1861.

Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of the 25th International Conference on
Machine Learning, pages 160–167.

Jocelyn Coulmance, Jean-Marc Marty, Guillaume
Wenzek, and Amine Benhalloum. 2016. Trans-
gram, fast cross-lingual word-embeddings. CoRR,
abs/1601.02502.

Manaal Faruqui, Jesse Dodge, Sujay K. Jauhar, Chris
Dyer, Eduard Hovy, and Noah A. Smith. 2015.
Retrofitting word vectors to semantic lexicons. In
Proceedings of NAACL.

Manaal Faruqui and Chris Dyer. 2014. Improving vec-
tor space word representations using multilingual
correlation. In Proceedings of EACL.

Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2013. PPDB: The paraphrase
database. In Proceedings of NAACL-HLT, pages
758–764.

120



Matt Gardner, Kejun Huang, Evangelos Papalex-
akis, Xiao Fu, Partha Talukdar, Christos Falout-
sos, Nicholas Sidiropoulos, and Tom Mitchell. 2015.
Translation invariant word embeddings. In Proceed-
ings of EMNLP.

Iryna Gurevych. 2005. Using the structure of a con-
ceptual network in computing semantic relatedness.
IJCNLP 2005, pages 767–778.

Samer Hassan and Rada Mihalcea. 2009. Cross-lingual
semantic relatedness using encyclopedic knowledge.
In Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing: Volume
3, pages 1192–1201.

Karl Moritz Hermann and Phil Blunsom. 2014. Multi-
lingual models for compositional distributed seman-
tics. CoRR, abs/1404.4641.

Colette Joubarne and Diana Inkpen. 2011. Compar-
ison of semantic similarity for different languages
using the google n-gram corpus and second- order
co-occurrence measures. In Proceedings of the 24th
Canadian Conference on Advances in Artificial In-
telligence, pages 216–221.

Douwe Kiela, Felix Hill, and Stephen Clark. 2015.
Specializing word embeddings for similarity or re-
latedness. In Proceedings of EMNLP.

Alexandre Klementiev, Ivan Titov, and Binod Bhat-
tarai. 2012. Inducing crosslingual distributed rep-
resentations of words. In Proceedings COLING,
pages 1459–1474.

Ira Leviant and Roi Reichart. 2015. Judgment lan-
guage matters: Multilingual vector space models for
judgment language aware lexical semantics. CoRR,
abs/1508.00106.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. CoRR, abs/1301.3781.

Tomas Mikolov, Quoc V. Le, and Ilya Sutskever. 2013b.
Exploiting similarities among languages for ma-
chine translation. CoRR, abs/1309.4168.

George A. Miller. 1995. Wordnet: A lexical database
for english. Communications of the ACM, pages 39–
41.

Nikola Mrksic, Diarmuid Ó Séaghdha, Blaise Thom-
son, Milica Gasic, Lina Maria Rojas-Barahona, Pei-
Hao Su, David Vandyke, Tsung-Hsien Wen, and
Steve J. Young. 2016. Counter-fitting word vectors
to linguistic constraints. In Proceedings of NAACL,
pages 142–148.

Nikola Mrkšić, Ivan Vulić, Diarmuid Ó Séaghdha, Ira
Leviant, Roi Reichart, Milica Gašić, Anna Korho-
nen, and Steve Young. 2017. Semantic special-
isation of distributional word vector spaces using
monolingual and cross-lingual constraints. Transac-
tions of the Association for Computational Linguis-
tics.

Douglas L. Nelson, Cathy L. McEvoy, and Thomas A.
Schreiber. 2004. The university of south florida free
association, rhyme, and word fragment norms. Be-
havior Research Methods, Instruments, & Comput-
ers, 36(3):402–407.

Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. Glove: Global vectors for word
representation. In EMNLP, volume 12, pages 1532–
1543.

Ivan Vulic and Marie-Francine Moens. 2015. Bilin-
gual word embeddings from non-parallel document-
aligned data applied to bilingual lexicon induction.
In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the
7th International Joint Conference on Natural Lan-
guage Processing of the Asian Federation of Natu-
ral Language Processing, Volume 2: Short Papers,
pages 719–725.

Chang Xu, Yalong Bai, Jiang Bian, Bin Gao, Gang
Wang, Xiaoguang Liu, and Tie-Yan Liu. 2014. Rc-
net: A general framework for incorporating knowl-
edge into word representations. In Proceedings of
the 23rd ACM International Conference on Confer-
ence on Information and Knowledge Management,
pages 1219–1228.

Will Y. Zou, Richard Socher, Daniel Cer, and Christo-
pher D. Manning. 2013. Bilingual word embeddings
for phrase-based machine translation. In Proceed-
ings of EMNLP, pages 1393–1398.

121


