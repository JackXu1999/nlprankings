837

Coling 2010: Poster Volume, pages 837–845,

Beijing, August 2010

Machine Translation with Lattices and Forests

Haitao Mi†‡ Liang Huang‡ Qun Liu†

†Key Lab. of Intelligent Information Processing

Institute of Computing Technology

‡Information Sciences Institute
Viterbi School of Engineering

Chinese Academy of Sciences
{htmi,liuqun}@ict.ac.cn

University of Southern California
{lhuang,haitaomi}@isi.edu

Abstract

source

1-best segmentation

Traditional 1-best
translation pipelines
suffer a major drawback: the errors of 1-
best outputs, inevitably introduced by each
module, will propagate and accumulate
along the pipeline. In order to alleviate
this problem, we use compact structures,
lattice and forest, in each module instead
of 1-best results. We integrate both lat-
tice and forest into a single tree-to-string
system, and explore the algorithms of lat-
tice parsing, lattice-forest-based rule ex-
traction and decoding. More importantly,
our model takes into account all the proba-
bilities of different steps, such as segmen-
tation, parsing, and translation. The main
advantage of our model is that we can
make global decision to search for the best
segmentation, parse-tree and translation in
one step. Medium-scale experiments show
an improvement of +0.9 BLEU points over
a state-of-the-art forest-based baseline.

1

Introduction

Statistical machine translation (SMT) has wit-
nessed promising progress in recent years. Typi-
cally, conventional SMT is characterized as a 1-
best pipeline system (Figure 1(a)), whose mod-
ules are independent of each other and only take
as input 1-best results from the previous module.
Though this assumption is convenient to reduce
the complexity of SMT systems. It also bring a
major drawback of error propagation. The errors
of 1-best outputs, introduced inevitably in each
phase, will propagate and accumulate along the
pipeline. Not recoverable in the ﬁnal decoding

1-best tree

target

source

segmentation lattice

parse forest

target

(a)

(b)

Figure 1: The pipeline of tree-based system: (a) 1-
best (b) lattice-forest.

step. These errors will severely hurt the translation
quality. For example, if the accuracy of each mod-
ule is 90%, the ﬁnal accuracy will drop to 73%
after three separate phases.

To alleviate this problem, an obvious solution
is to widen the pipeline with k-best lists rather
than 1-best results. For example Venugopal et
al. (2008) use k-best alignments and parses in the
training phase. However, with limited scope and
too many redundancies, it is inefﬁcient to search
separately on each of these similar lists (Huang,
2008).

Another efﬁcient method is to use compact data
structures instead of k-best lists. A lattice or forest,
compactly encoded exponentially many deriva-
tions, have proven to be a promising technique.
For example, Mi and Huang (2008), Mi et al.
(2008), Liu et al. (2009) and Zhang et al. (2009)
use forests in rule extraction and decoding phases
to extract more general rules and weaken the inﬂu-
ence of parsing errors; Dyer et al. (2008) use word
lattice in Chinese word segmentation and Arabic
morphological variation phases to weaken the in-
ﬂuence of segmentation errors; Huang (2008) and

838

(0, 2, NR)

0
0

c0:B`u

1
1

c1:sh´ı

2
2

(2, 3, CC)

c2:yˇu
(2, 3, P)

(3, 5, NR)

(5, 6, VV)

(6, 8, NN)

(8, 9, NN)

3
3

c3:Sh¯a

4
4

c4:l´ong

5
5

c5:jˇu

6
6

c6:x´ıng

7
7

c7:tˇao

8
8

c8:l`un

9
9

(5, 7, VV)

(7, 9, NN)

Figure 2: The lattice of the example:“ B`u sh´ı yˇu Sh¯a l´ong jˇu x´ıng tˇao l`un.” The solid lines show the 1-best
result, which is wrong.

Jiang et al. (2008b) stress the problems in re-
ranking phase. Both lattices and forests have be-
come popular in machine translation literature.

However, to the best of our knowledge, previous
work only focused on one module at a time. In this
paper, we investigate the combination of lattice
and forest (Section 2), as shown in Figure 1(b).
We explore the algorithms of lattice parsing (Sec-
tion 3.2), rule extraction (Section 4) and decod-
ing (Section 5). More importantly, in the decoding
step, our model can search among not only more
parse-trees but also more segmentations encoded
in the lattice-forests and can take into account all
the probabilities of segmentations and parse-trees.
In other words, our model postpones the disambi-
guition of segmentation and parsing into the ﬁnal
translation step, so that we can do global search
for the best segmentation, parse-tree and transla-
tion in one step. When we integrate a lattice into
a forest system, medium-scale experiments (Sec-
tion 6) show another improvement of +0.9 BLEU
points over a state-of-the-art forest-based system.

2 Compact Structures

A word lattice (Figure 2) is a compact representa-
tion of all the possible of segmentations and POS
tags, while a parse forest (Figure 5) is a compact
representation of all parse trees.

2.1 Word Lattice
For a given input sentence C = c0..cn−1, where
ci denotes a character at position i, and n is the
length of the sentence.

A word lattice (Figure 2), or lattice in short, is
a set of edges L, where each edge is in the form
of (i, j, X), which denotes a word of tag X, cov-
ering characters ci through cj−1. For example, in
Figure 2, (7, 9, NN) is a noun “tˇaol`un” of two char-
acters.

The lattice in Figure 2 shows result of the ex-
ample:“ B`u sh´ı yˇu Sh¯a l´ong jˇu x´ıng tˇao l`un ”.
One ambiguity comes from the POS tag of word
“yˇu” (preposition (P) or conjunction (CC)). The
other one is the segmentation ambiguity of the last
four characters, we can segment into either “jˇu
x´ıngtˇao l`un” (solid lines), which means lift, beg-
ging and argument separately for each word or
“jˇux´ıng tˇaol`un” (dashed lines), which means hold
a discussion.

lift

begging

argument

5

jˇu

6 x´ıng

tˇao

7

l`un

8

9

hold

a discussion

The solid lines above (and also in Figure 2)
show the 1-best result, which is obviously wrong.
If we feed it into the next modules in the SMT
pipeline, parsing and translation will be become
much more difﬁcult, since the segmentation is not
recoverable. So it is necessary to postpone er-
ror segmentation decisions to the ﬁnal translation
step.

2.2 Parse Forest

In parsing scenario, a parse forest (Figrure 5), or
forest for short, can be formalized as a hyper-
graph H, a pair (cid:2)V, E(cid:3), where node v ∈ V is in
the form of X i,j, which denotes the recognition of
nonterminal X spanning the substring ci:j−1 from
positions ci through cj−1. Each hyperedge e ∈ E
is a pair (cid:2)tails(e), head (e)(cid:3), where head (e) ∈ V
is the consequent node in an instantiated deduc-
tive step, and tails(e) ∈ (V )∗ is the list of an-
tecedent nodes.

For the following deduction:

NR0,2 CC2,3 NR3,5

NP0,5

(*)

839

its hyperedge e∗ is notated:

(cid:2)(NR0,2, CC2,3, NR3,5), NP0,5(cid:3).

where

head(e∗) = {NP0,5}, and

tails(e∗) = {NR0,2, CC2,3, NR3,5}.

We also denote IN (v) to be the set of incoming
hyperedges of node v, which represents the dif-
ferent ways of deriving v. For simplicity, we only
show a tree in Figure 5(a) over 1-best segmenta-
tion and POS tagging result in Figure 2. So the
IN (NP0,5) is {e∗}.
3 Lattice Parsing

In this section, we ﬁrst brieﬂy review the con-
ventional CYK parsing, and then extend to lattice
parsing. More importantly, we propose a more ef-
ﬁcient parsing paradigm in Section 3.3.

3.1 Conventional Parsing
The conventional CYK parsing algorithm in Fig-
ure 3(a) usually takes as input a single sequence of
words, so the CYK cells are organized over words.
This algorithm consists of two steps: initialization
and parsing. The ﬁrst step is to initialize the CYK
cells, whose span size is one, with POS tags pro-
duced by a POS tagger or deﬁned by the input
string1. For example, the top line in Figure 3(a)
is initialized with a series of POS tags in 1-best
segmentation. The second step is to search for the
best syntactic tree under a context-free grammar.
For example, the tree composed by the solid lines
in Figure 5(a) shows the parsing tree for the 1-best
segmentation and POS tagging results.

3.2 Lattice Parsing
The main differences of our lattice parsing in Fig-
ure 3(b) from conventional approach are listed in
following: First, the CYK cells are organized over
characters rather than words. Second, in the ini-
tialization step, we only initialize the cells with
all edges L in the lattice. Take the edge (7, 9,
NN) in Figure 2 for example, the corresponding
cell should be (7, 9), then we add a leaf node
v = NN7,9 with a word tˇaol`un. The ﬁnal initial-
ization is shown in Figure 3(b), which shows that

1For simplicity, we assume the input of a parser is a seg-

mentation and POS tagging result

0 B`u 1 sh´ı 2 yˇu 3Sh¯a 4l´ong 5 jˇu 6x´ıng 7tˇao 8 l`un 9

NR

CC

NR

VV

NN

NN

NP

IP

VPB

O(n3

w)

(a): Parsing over 1-best segmentation

0 B`u 1 sh´ı 2 yˇu 3Sh¯a 4l´ong 5 jˇu 6x´ıng 7tˇao 8 l`un 9

NR

NP

IP

CC,P

PP

NR

VP

VV
VV NN NN

NN

VPB

O(n3)

(b): Parsing over characters

0 B`u 1 sh´ı 2 yˇu 3Sh¯a 4l´ong 5 jˇu 6x´ıng 7tˇao 8 l`un 9

NR

CC,P

NR

VV
VV NN NN

NN

PP

VP

NP

IP

VPB

O(n3
r)

(c): Parsing over most-reﬁned segmentation

Figure 3: CKY parsing charts (a): Conventional
parsing over 1-best segmentation. (b): Lattice
parsing over characters of input sentence. (c): Lat-
tice parsing over most-reﬁned segmentation of lat-
tice. nw and nr denotes the number of tokens over
the 1-best segmentation and the most-reﬁned seg-
menation respectively, and nw ≤ nr ≤ n.

lattice parsing can initialize the cells, whose span
size is larger than one. Third, in the deduction step
of the parsing algorithm i, j, k are the indexes be-
tween characters rather than words.

We formalize our lattice parser as a deductive

proof system (Shieber et al., 1994) in Figure 4.

Following the deﬁnitions of the previous Sec-

840

tion, given a set of edges L of a lattice for an in-
put sentence C = c0..cn−1 and a PCFG grammar:
a 4-tuple (cid:2)N, Σ, P, S(cid:3), where N is a set of non-
terminals, Σ is a set of terminal symbols, P is a
set of inference rules, each of which is in the form
of X → α : p for X ∈ N, α ∈ (N ∪ Σ)∗ and p is
the probability, and S ∈ N is the start symbol. The
deductive proof system (Figure 4) consists of ax-
ioms, goals and inference rules. The axioms are
converted by edges in L. Take the (5, 7, NN) as-
sociated with a weight p1 for example, the corre-
sponding axiom is NN → tˇaol`un : p1. All axioms
converted from the lattice are shown in Figure 3(b)
exclude the italic non-terminals. Please note that
all the probabilities of the edges L in a lattice are
taken into account in the parsing step. The goals
are the recognition X0,n ∈ S of the whole sen-
tence. The inference rules are the deductions in
parsing. Take the deduction (*) for example, it will
prove a new item NP0,5 (italic N P in Figure 3(b))
and generate a new hyper-edge e∗ (in Figure 5(b)).
So the parsing algorithm starts with the axioms,
and then applies the inference rules to prove new
items until a goal item is proved. The ﬁnal whole
forest for the input lattice (Figure 2) is shown in
Figure 5(b). The extra hyper-edges of lattice-forest
are highlighted with dashed lines, which can in-
ference the input sentence correctly. For example:
“yˇu” is tagged into P rather than CC.

3.3 Faster Parsing with Most-reﬁned Lattice

However, our statistics show that the average num-
ber of characters n in a sentence is 1.6 times than
the number of words nw in its 1-best segmenta-
tion. As a result, the parsing time over the charac-
ters will grow more than 4 times than parsing over
the 1-best segmentation, since the time complexity
is O(n3). In order to alleviate this problem, we re-
duce the parsing time by using most-reﬁned seg-
mentation for a lattice, whose number of tokens
is nr and has the property nw ≤ nr ≤ n.
Given a lattice with its edges L over indexes
(0, .., n), a index i is a split point, if and only if
there exists some edge (i, j, X) ∈ L or (k, i, X) ∈
L. The most-reﬁned segmentation, or ms for
short, is the segmentation result by using all split
points in a lattice. For example, the corresponding
ms of the example is “B`ush´ı yˇu Sh¯al´ong jˇu x´ıng
tˇao l`un” since points 1 and 4 are not split points.

Item form:

Axioms:

Xi,j

Xi,j : p(i, j, X)

(i, j, X) ∈ L

Infer. rules:

Xi,k : p1 Yk,j : p2

Zi,j : pp1p2

Z → XY : p ∈ P

Goals:

X0,n

Figure 4: Lattice parsing as deductive proof sys-
tem. The i, j, k are the indexes between characters.

Figure 3(c) shows the CKY parsing cells over
most-reﬁned segmentation, the average number
of tokens nr is reduced by combining columns,
which are shown with red dashed boxes. As a re-
sult, the search space is reduced without losing any
derivations. Theoretically, the parsing over fs will
speed up in O((n/nr)3). And our experiments in
Section 6 show the efﬁciency of our new approach.
It turns out that the parsing algorithm developed
in lattice-parsing Section 3.2 can be used here
without any change. The non-terminals inducted
are also shown in Figure 3(c) in italic style.

4 Rule Extraction with Lattice & Forest

We now explore the extraction algorithm from
aligned source lattice-forest and target string2,
which is a tuple (cid:2)F, τ, a(cid:3) in Figure 5(b). Following
Mi and Huang (2008), we extract minimal rules
from a lattice-forest also in two steps:

(1) frontier set computation

(2) fragmentation

Following the algorithms developed by Mi and
Huang (2008) in Algorithm 1, all the nodes in
frontier set (fs) are highlighted with gray in Fig-
ure 5(b).

Our process of fragmentation (lines 1- 13) is
to visit each frontier node v and initial a queue
(open) of growing fragments with a pair of empty
fragment and node v (line 3). Each fragment is as-
sociated with a list of expansion sites (front) being

2For simplicity and consistency, we use character-based
lattice-forest for the running example. The “B`u” and “sh´ı”
are aligned to the same word “Bush”. In our experiment,
we use most-reﬁned segmentation to run lattice-parsing and
word alignment.

841

IP0,9

NP0,5
e∗

VPB5,9

(a)

.NR0,2

.CC2,3

.NR3,5

.VV5,6

.NN6,8

0

.B`u

1

.sh´ı

2

.yˇu

3

.Sh¯a

4

.l´ong

5

.jˇu

6

.x´ıng

7

.tˇao

8

.NN8,9
.l`un

9

IP0,9

NP0,5
e∗

VP2,9

(b)

PP2,5

VPB5,9

. NR0,2 . CC2,3

. P2,3

. NR3,5

.VV5,6

. VV5,7

.NN6,8 .NN8,9

. NN7,9

0

.B`u

1

.sh´ı

2

.yˇu

3

.Sh¯a

4

.l´ong

5

.jˇu

6

.x´ıng

7

.tˇao

8

.l`un

9

Bush

held

a

discussion

with

Sharon

Forest only (Minimal rules)

Lattice & forest (Extra minimal rules)

IP(NP(x1:NR x2:CC x3:NR) x4:VPB)
→ x1 x4 x2 x3

CC(yˇu) →with
NR(Sh¯al´ong) →Sharon
NR(B`ush´ı) →Bush

VPB(VV(jˇu) NN(x´ıngtˇao) NN(l`un))
→held a discussion

IP(x1:NR x2:VP) → x1 x2
VP(x1:PP x2:VPB) → x2 x1
PP(x1:P x2:NR) → x1 x2
VPB(x1:VV x2:NN) → x1 x2
NN(tˇaol`un) →a discussion

VV(jˇux´ıng) →held

P(yˇu) →with

(c)

Figure 5: (a): The parse forest over the 1-best segmentation and POS tagging result. (b): Word-aligned
tuple (cid:2)F, τ, a(cid:3): the lattice-forest F , the target string τ and the word alingment a. The solid hyperedges
form the forest in (a). The dashed hyperedges are the extra hyperedges introduced by the lattice-forest.
(c): The minimal rules extracted on forest-only (left column), and the extra minimal rules extracted on
lattice-forest (right column).

the subset of leaf nodes of the current fragment
that are not in the fs except for the initial node
v. Then we keep expanding fragments in open in

following way. If current fragment is complete,
whose expansion sites is empty, we extract rule
corresponding to the fragment and its target string

842

Code 1 Rule Extraction (Mi and Huang, 2008).

Input: lattice-forest F , target sentence τ , and
alignment a
Output: minimal rule set R

(cid:4) frontier set

1: fs ← FROSET(F, τ, a)
2: for each v ∈ fs do
open ← {(cid:2)∅, {v}(cid:3)}
while open (cid:9)= ∅ do
(cid:2)frag, front(cid:3) ← open.pop()
if front = ∅ then

(cid:4) initial queue

else

(cid:4) ﬁnished?

generate a rule r using frag
R.append(r)
(cid:4) incomplete: further expand
u ← front.pop() (cid:4) expand frontier
for each e ∈ IN (u) do

f ← front ∪ (tails(e) \ fs)
open.append((cid:2)frag ∪ {e}, f (cid:3))

3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:

(line 7) . Otherwise we pop one expansion node
u to grow and spin-off new fragments by IN (u),
adding new expansion sites (lines 11- 13), until all
active fragments are complete and open queue is
empty.

The extra minimal rules extracted on lattice-
forest are listed at the right bottom of Figure 5(c).
Compared with the forest-only approach, we can
extract smaller and more general rules.

After we get all the minimal rules, we com-
pose two or more minimal rules into composed
rules (Galley et al., 2006), which will be used in
our experiments.

For each rule r extracted, we also assign a frac-
tional count which is computed by using inside-
outside probabilities:

Q

,

c(r) =

β(TOP)

α(root(r)) · P (lhs(r)) ·

v∈yield(root(r)) β(v)
(1)
where root(r) is the root of the rule, lhs(r) is
the left-hand-side of rule, rhs(r) is the right-
hand-side of rule, P (lhs(r)) is the product of
all probabilities of hyperedges involved in lhs(r),
yield(root(r)) is the leave nodes, TOP is the root
node of the forest, α(v) and β(v) are outside and
inside probabilities, respectively.

Then we compute three conditional probabili-

ties for each rule:

P (r | lhs(r)) =

c(r)

(cid:2)r(cid:3):lhs(r(cid:3))=lhs(r) c(r(cid:3))

(2)

P (r | rhs(r)) =

P (r | root(r)) =

c(r)

(cid:2)r(cid:3):rhs(r(cid:3))=rhs(r) c(r(cid:3))
(cid:2)r(cid:3):root(r(cid:3))=root(r) c(r(cid:3))

c(r)

(3)

.

(4)

All these probabilities are used in decoding step
(Section 5). For more detail, we refer to the algo-
rithms of Mi and Huang (2008).

5 Decoding with Lattice & Forest

Given a source-side lattice-forest F , our decoder
searches for the best derivation d∗ among the set of
all possible derivation D, each of which converts
a tree in lattice-forest into a target string τ :

d∗ = argmax
d∈D,T∈F

P (d|T )λ0 · eλ1|d|
· LM (τ (d))λ2 · eλ3|τ (d)|,

(5)

where |d| is the penalty term on the number of
rules in a derivation, LM (τ (d)) is the language
model and eλ3|τ (d)| is the length penalty term on
target translation. The P (d|T ) decomposes into
the product of rule probabilities P (r), each of
which is decomposed further into

P (d|T ) =(cid:3)r∈d

P (r).

(6)

Each P (r) in Equation 6 is decomposed further

into the production of ﬁve probabilities:

P (r) = P (r|lhs(r))λ4
· P (r|rhs(r))λ5
· P (r|root(lhs(r))λ6
· Plex(lhs(r)|rhs(r))λ7
· Plex(rhs(r)|lhs(r))λ8,

(7)

where the last two are the lexical probabilities be-
tween the terminals of lhs(r) and rhs(r). All the
weights of those features are tuned by using Min-
imal Error Rate Training (Och, 2003).

Following Mi et al. (2008), we ﬁrst convert the
lattice-forest into lattice translation forest with the
conversion algorithm proposed by Mi et al. (2008),

843

and then the decoder ﬁnds the best derivation on
the lattice translation forest. For 1-best search, we
use the cube pruning technique (Chiang, 2007;
Huang and Chiang, 2007) which approximately
intersects the translation forest with the LM. For
k-best search after getting 1-best derivation, we
use the lazy Algorithm 3 of Huang and Chiang
(2005) to incrementally compute the second, third,
through the kth best alternatives.

For more detail, we refer to the algorithms of

Mi et al. (2008).

6 Experiments

6.1 Data Preparation
Our experiments are on Chinese-to-English trans-
lation. Our training corpus is FBIS corpus with
about 6.9M/8.9M words in Chinese/English re-
spectively.

We use SRI Language Modeling Toolkit (Stol-
cke, 2002) to train a 4-gram language model with
Kneser-Ney smoothing on the ﬁrst 1/3 of the Xin-
hua portion of Gigaword corpus.

We use the 2002 NIST MT Evaluation test set
as development set and the 2005 NIST MT Eval-
uation test set as test set. We evaluate the trans-
lation quality using the case-insensitive BLEU-4
metric (Papineni et al., 2002). We use the standard
MERT (Och, 2003) to tune the weights.

6.1.1 Baseline Forest-based System
We ﬁrst segment the Chinese sentences into the
1-best segmentations using a state-of-the-art sys-
tem (Jiang et al., 2008a), since it is not necessary
for a conventional parser to take as input the POS
tagging results. Then we parse the segmentation
results into forest by using the parser of Xiong et
al. (2005). Actually, the parser will assign multiple
POS tags to each word rather than one. As a result,
our baseline system has already postponed the
POS tagging disambiguition to the decoding step.
Forest is pruned by using a marginal probability-
based pruning algorithm similar to Huang (2008).
The pruning threshold are pf = 5 and pf = 10 at
rule extraction and decoding steps respectively.

We word-align the strings of 1-best segmenta-
tions and target strings with GIZA++ (Och and
Ney, 2000) and apply the reﬁnement method
“grow-diag-ﬁnal-and” (Koehn et al., 2003) to get
the ﬁnal alignments. Following Mi and Huang

(2008) and Mi et al. (2008), we also extract rules
from forest-string pairs and translate forest
to
string.

6.1.2 Lattice-forest System
We ﬁrst segment and POS tag the Chinese sen-
tences into word lattices using the same sys-
tem (Jiang et al., 2008a), and prune each lat-
tice into a reasonable size using the marginal
probability-based pruning algorithm.

Then, as current GIZA++ (Och and Ney, 2000)
can only handle alignment between string-string
pairs, and word-alingment with the pairs of Chi-
nese characters and target-string will obviously re-
sult in worse alignment quality. So a much better
way to utilize GIZA++ is to use the most-reﬁned
segmentation for each lattice instead of the char-
acter sequence. This approach can be viewed as a
compromise between character-string and lattice-
string word-alignment paradigms. In our exper-
iments, we construct the most-reﬁned segmen-
tations for lattices and word-align them against
the English sentences. We again apply the reﬁne-
ment method “grow-diag-ﬁnal-and” (Koehn et al.,
2003) to get the ﬁnal alignments.

In order to get the lattice-forests, we modi-
ﬁed Xiong et al. (2005)’s parser into a lattice
parser, which produces the pruned lattice forests
for both training, dev and test sentences. Finally,
we apply the rule extraction algorithm proposed in
this paper to obtain the rule set. Both lattices and
forests are pruned using a marginal probability-
based pruning algorithm similar to Huang (2008).
The pruning threshold of lattice is pl = 20 at both
the rule extraction and decoding steps, the thresh-
olds for the latice-forests are pf = 5 and pf = 10
at rule extraction and decoding steps respectively.

6.2 Results and Analysis

Table 1 shows results of two systems. Our lattice-
forest (LF) system achieves a BLEU score of
29.65, which is an absolute improvement of 0.9
points over the forest (F) baseline system, and the
improvement is statistically signiﬁcant at p < 0.01
using the sign-test of Collins et al. (2005).

The average number of tokens for the 1-best
and most-reﬁned segmentations are shown in sec-
ond column. The average number of characters
is 46.7, which is not shown in Table 1. Com-

844

Sys

F
LF

Avg # of

tokens
28.7
37.1

links
35.1
37.1

Rules

All

dev&tst

BLEU

29.6M 3.3M 28.75
23.5M 3.4M 29.65

Table 1: Results of forest (F) and lattice-forest
(LF) systems. Please note that lattice-forest system
only extracts 23.5M rules, which is only 79.4% of
the rules extracted by forest system. However, in
decoding step, lattice-forest system can use more
rules after ﬁltered on dev and test sets.

pared with the characters-based lattice parsing, our
most-reﬁned lattice parsing speeds up parsing by
(37.1/46.7)3 ≈ 2 times, since parsing complexity
is O(n3).
More interestingly, our lattice-forest model only
extracts 23.5M rules, which is 79.4% percent of
the rules extracted by the baseline system. The
main reason lies in the larger average number
of words for most-reﬁned segmentations over lat-
tices being 37.1 words vs 28.7 words over 1-best
segmentations. With much ﬁner granularity, more
word aligned links and restrictions are introduced
during the rule extraction step by GIZA++. How-
ever, more rules can be used in the decoding step
for the lattice-forest system, since the lattice-forest
is larger than the forest over 1-best segmentation.
We also investigate the question of how often
the non 1-best segmentations are picked in the ﬁ-
nal translation. The statistic on our dev set sug-
gests 33% of sentences choose non 1-best segmen-
tations. So our lattice-forest model can do global
search for the best segmentation and parse-tree to
direct the ﬁnal translation. More importantly, we
can use more translation rules in the translation
step.

7 Related Works

Compactly encoding exponentially many deriva-
tions, lattice and forest have been used in some
previous works on SMT. To alleviate the prob-
lem of parsing error in 1-best tree-to-string trans-
lation model, Mi et al. (2008) ﬁrst use forest to
direct translation. Then Mi and Huang (2008) use
forest in rule extraction step. Following the same
direction, Liu et al. (2009) use forest in tree-
to-tree model, and improve 1-best system by 3
BLEU points. Zhang et al. (2009) use forest in

tree-sequence-to-string model and also achieve a
promising improvement. Dyer et al. (2008) com-
bine multiple segmentations into word lattice and
then use lattice to direct a phrase-based transla-
tion decoder. Then Dyer (2009) employ a single
Maximum Entropy segmentation model to gen-
erate more diverse lattice, they test their model
on the hierarchical phrase-based system. Lattices
and forests can also be used in Minimal Error
Rate Training and Minimum Bayes Risk Decod-
ing phases (Macherey et al., 2008; Tromble et al.,
2008; DeNero et al., 2009; Kumar et al., 2009; Li
and Eisner, 2009). Different from the works listed
above, we mainly focus on how to combine lattice
and forest into a single tree-to-string system.

8 Conclusion and Future Work

In this paper, we have proposed a lattice-forest
based model to alleviate the problem of error prop-
agation in traditional single-best pipeline frame-
work. Unlike previous works, which only focus on
one module at a time, our model successfully in-
tegrates lattice into a state-of-the-art forest tree-to-
string system. We have explored the algorithms of
lattice parsing, rule extraction and decoding. Our
model postpones the disambiguition of segmenta-
tion and parsing into the ﬁnal translation step, so
that we can make a more global decision to search
for the best segmentation, parse-tree and transla-
tion in one step. The experimental results show
that our lattice-forest approach achieves an abso-
lute improvement of +0.9 points in term of BLEU
score over a state-of-the-art forest-based model.

For future work, we would like to pay more
attention to word alignment between lattice pairs
and forest pairs, which would be more principled
than our current method of word alignment be-
tween most-reﬁned segmentation and string.

Acknowledgement

We thank Steve DeNeefe and the three anony-
mous reviewers for comments. The work is sup-
ported by National Natural Science Foundation
of China, Contracts 90920004 and 60736014,
and 863 State Key Project No. 2006AA010108
(H. M and Q. L.), and in part by DARPA GALE
Contract No. HR0011-06-C-0022, and DARPA
under DOI-NBC Grant N10AP20031 (L. H and
H. M).

845

References

David Chiang. 2007. Hierarchical phrase-based trans-

lation. Comput. Linguist., 33(2):201–228.

Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proceedings of ACL, pages 531–540,
Ann Arbor, Michigan, June.

John DeNero, David Chiang, and Kevin Knight. 2009.
Fast consensus decoding over translation forests. In
Proceedings of ACL/IJCNLP.

Christopher Dyer, Smaranda Muresan, and Philip
Resnik. 2008. Generalizing word lattice translation.
In Proceedings of ACL-08: HLT, pages 1012–1020,
Columbus, Ohio, June.

C. Dyer. 2009. Using a maximum entropy model to
build segmentation lattices for mt. In Proceedings
of NAACL.

Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models.
In Pro-
ceedings of COLING-ACL, pages 961–968, Sydney,
Australia, July.

Liang Huang and David Chiang. 2005. Better k-best

parsing. In Proceedings of IWPT.

Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language mod-
els. In Proceedings of ACL, pages 144–151, June.

Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
ACL.

Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan L¨u.
2008a. A cascaded linear model for joint chinese
word segmentation and part-of-speech tagging.
In
Proceedings of ACL-08: HLT.

Wenbin Jiang, Haitao Mi, and Qun Liu. 2008b. Word
lattice reranking for chinese word segmentation and
part-of-speech tagging.
In Proceedings of Coling
2008.

Philipp Koehn, Franz Joseph Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of HLT-NAACL, pages 127–133, Edmon-
ton, Canada, May.

Shankar Kumar, Wolfgang Macherey, Chris Dyer, and
Franz Och. 2009. Efﬁcient minimum error rate
training and minimum bayes-risk decoding for trans-
lation hypergraphs and lattices.
In Proceedings of
the ACL/IJCNLP 2009.

Zhifei Li and Jason Eisner. 2009. First- and second-
order expectation semirings with applications to
minimum-risk training on translation forests.
In
Proceedings of EMNLP, pages 40–51, Singapore,
August. Association for Computational Linguistics.

Yang Liu, Yajuan L¨u, and Qun Liu. 2009. Improving
tree-to-tree translation with packed forests. In Pro-
ceedings of ACL/IJCNLP, August.

Wolfgang Macherey, Franz Och, Ignacio Thayer, and
Jakob Uszkoreit. 2008. Lattice-based minimum er-
ror rate training for statistical machine translation.
In Proceedings of EMNLP 2008.

Haitao Mi and Liang Huang. 2008. Forest-based trans-
In Proceedings of EMNLP

lation rule extraction.
2008, pages 206–214, Honolulu, Hawaii, October.

Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of ACL-08:HLT,
pages 192–199, Columbus, Ohio, June.

Franz J. Och and Hermann Ney. 2000. Improved sta-
tistical alignment models. In Proceedings of ACL,
pages 440–447.

Franz J. Och. 2003. Minimum error rate training in
In Proceedings of

statistical machine translation.
ACL, pages 160–167.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation.
In Proceedings of
ACL, pages 311–318, Philadephia, USA, July.

Stuart M. Shieber, Yves Schabes, and Fernando C. N.
Pereira. 1994. Principles and implementation of de-
ductive parsing.

Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Proceedings of ICSLP,
volume 30, pages 901–904.

Roy Tromble, Shankar Kumar, Franz Och, and Wolf-
gang Macherey. 2008. Lattice Minimum Bayes-
Risk decoding for statistical machine translation. In
Proceedings of EMNLP 2008.

Ashish Venugopal, Andreas Zollmann, Noah A. Smith,
and Stephan Vogel. 2008. Wider pipelines: N-best
alignments and parses in MT training. In Proceed-
ings of AMTA.

Deyi Xiong, Shuanglong Li, Qun Liu, and Shouxun
Lin. 2005. Parsing the Penn Chinese Treebank with
Semantic Knowledge.
In Proceedings of IJCNLP
2005, pages 70–81.

Hui Zhang, Min Zhang, Haizhou Li, Aiti Aw, and
Chew Lim Tan. 2009. Forest-based tree sequence
to string translation model.
In Proceedings of the
ACL/IJCNLP 2009.

