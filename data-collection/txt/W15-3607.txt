



















































Identification and Classification of Emotional Key Phrases from Psychological Texts


Proceedings of the ACL 2015 Workshop on Novel Computational Approaches to Keyphrase Extraction, pages 32–38,
Beijing, China, July 30, 2015. c©2015 Association for Computational Linguistics

Identification and Classification of Emotional Key Phrases from Psycho-
logical Texts

Apurba Paul Dipankar Das
JIS College of Engineering Jadavpur University

Kalyani,Nadia 188, Raja S.C. Mullick Road, Kolkata
West Bengal, India West Bengal, India

apurba.saitech@gmail.com ddas@cse.jdvu.ac.in

Abstract

Emotions, a complex state of feeling results in
physical and psychological changes that influ-
ence human behavior. Thus, in order to extract
the emotional key phrases from psychological
texts, here, we have presented a phrase level
emotion identification and classification sys-
tem. The system takes pre-defined emotional
statements of seven basic emotion classes
(anger, disgust, fear, guilt, joy, sadness and
shame) as input and extracts seven types of
emotional trigrams. The trigrams were
represented as Context Vectors. Between a
pair of Context Vectors, an Affinity Score was
calculated based on the law of gravitation with
respect to different distance metrics (e.g.,
Chebyshev, Euclidean and Hamming). The
words, Part-Of-Speech (POS) tags, TF-IDF
scores, variance along with Affinity Score and
ranked score of the vectors were employed as
important features in a supervised classifica-
tion framework after a rigorous analysis. The
comparative results carried out for four differ-
ent classifiers e.g., NaiveBayes, J48, Decision
Tree and BayesNet show satisfactory perfor-
mances.

1 Introduction

Human emotions are the most complex and unique
features to be described. If we ask someone regard-
ing emotion, he or she will reply simply that it is a
'feeling'. Then, the obvious question that comes
into our mind is about the definition of feeling. It is
observed that such terms are difficult to define and
even more difficult to understand complete-
ly. Ekman  (1980)  proposed six basic emotions
(anger, disgust, fear, guilt, joy and sadness) that
have  a  shared   meaning on  the  level  of  facial
expressions across  cultures (Scherer, 1997; Scher-

er and Wallbott, 1994). Psychological texts contain
huge number of emotional words because psychol-
ogy and emotions are inter-wined, though they are
different (Brahmachari et.al, 2013). A phrase that
contains more than one word can be a better way
of representing emotions than a single word. Thus,
the emotional phrase identification and their classi-
fication from text have great importance in Natural
Language Processing (NLP).

In the present work, we have extracted
seven different types of emotional statements (an-
ger, disgust, fear, guilt, joy, sadness and shame)
from the Psychological corpus. Each of the emo-
tional statements was tokenized; the tokens were
grouped in trigrams and considered as Context
Vectors. These Context Vectors are POS tagged
and corresponding TF and TF-IDF scores were
measured for considering them as important fea-
tures or not. In addition, the Affinity Scores were
calculated for each pair of Context Vectors based
on different distance metrics (Chebyshev, Eucli-
dean and Hamming). Such features lead to apply
different classification methods like NaiveBayes,
J48, Decision Tree and BayesNet and after that the
results are compared.

The route map for this paper is the Related
Work (Section 2), Data Preprocessing Framework
(Section 3) followed by Feature Analysis and Clas-
sification framework (Section 4) and result analy-
sis (Section 5) along with the improvement due to
ranking. Finally, we have concluded the discussion
(Section 6).

2 Related Work

Strapparava and Valitutti (2004) developed the
WORDNET-AFFECT, a lexical resource that as-
signs one or more affective labels such as emotion,
mood, trait, cognitive state, physical state, beha-
vior, attitude and sensation etc to a number of

32



WORDNET synsets. A detailed annotation scheme
that identifies key components and properties of
opinions and emotions in language has been de-
scribed in (Wiebe et al., 2005). The authors in
(Kobayashi et al., 2004) also developed an opinion
lexicon out of their annotated corpora. Takamura
et al. (2005) extracted semantic orientation of
words according to the spin model, where the se-
mantic orientation of words propagates in two
possible directions like electrons. Esuli and Sebas-
tiani’s (2006) approach to develop the SentiWord-
Net is an adaptation to synset classification based
on the training of ternary classifiers for deciding
positive and negative (P-N) polarity. Each of the
ternary classifiers is generated using the Semi-
supervised rules.

On the other hand, Mohammad, et al., (2010)
has performed an extensive analysis of the annota-
tions to better understand the distribution of emo-
tions evoked by terms of different parts of speech.
The authors in (Das and Bandyopadhyay, 2009,
2010) created the emotion lexicon and systems for
Bengali language. The development of SenticNet
(Cambria et al., 2010) was inspired later by (Poria
et al., 2013). The authors developed an enriched
SenticNet with affective information by assigning
emotion labels. Similarly, ConceptNet1 is a multi-
lingual knowledge base, representing words and
phrases that people use and the common-sense re-
lationships between them.

Balahur et al., (2012) had shown that the task of
emotion detection from texts such as the one in the
ISEAR corpus (where little or no lexical clues of
affect are present) can be best tackled using ap-
proaches based on commonsense knowledge. In
this sense, EmotiNet, apart from being a precise
resource for classifying emotions in such exam-
ples, has the advantage of being extendable with
external sources, thus increasing the recall of the
methods employing it. Patra et al., (2013) adopted
the Potts model for the probability modeling of the
lexical network that was constructed by connecting
each pair of words in which one of the two words
appears in the gloss of the other.

In contrast to the previous approaches, the
present task comprises of classifying the emotional
phrases by forming Context Vectors and the expe-
rimentation with simple features like POS, TF-IDF
and Affinity Score followed by the computation of

1 http://conceptnet5.media.mit.edu/

similarities based on different distance metrics help
in making decisions to correctly classify the emo-
tional phrases.

3 Data Preprocessing Framework

3.1 Corpus Preparation

The emotional statements were collected from the
ISEAR7 (International Survey on Emotion Antece-
dents and Reactions) database. Each of the emotion
classes contains the emotional statements given by
the respondents as answers based on some prede-
fined questions. Student respondents, both psy-
chologists and non-psychologists were asked to
report situations in which they had experienced all
of the 7 major emotions (anger, disgust, fear, guilt,
joy, sadness, shame). The final data set contains
reports of 3000 respondents from 37 countries. The
statements were split in sentences and tokenized
into words and the statistics were presented in Ta-
ble 1. It is found that only 1096 statements belong
to anger, disgust sadness and shame classes whe-
reas the fear, guilt and joy classes contain 1095,
1093 and 1094 different statements, respectively.
Since each statement may contain multiple sen-
tences, so after sentence tokenization, it is ob-
served that the anger and fear classes contain the
maximum number of sentences. Similarly, it is ob-
served that the anger class contains the maximum
number of tokenized words.

Emotions Total No.
of
Statements

Total No.
of
Sentences

Total No. of
Tokenized
Words

Anger 1096 1760 24301
Disgust 1096 1607 20871
Fear 1095 1760 22912
Guilt 1093 1718 22430
Joy 1094 1554 18851
Sadness 1096 1606 19480
Shame 1096 1609 20948
Total 7,666 11,614 1,49,793

Table 1: Corpus Statistics

The tokenized words were grouped to form
trigrams in order to grasp the roles of the previous
and next tokens with respect to the target token.
Thus, each of the trigrams was considered as a
Context Window (CW) to acquire the emotional
phrases. The updated version of the standard word
lists of the WordNet Affect (Strapparava, and Vali-

33



tutti, 2004) was collected and it is observed that the
total of 2,958 affect words is present.

It is considered that, in each of the Context
Windows, the first word appears as a non-affect
word, second word as an affect word, and third
word as a non-affect word (<NAW1>, <AW>,
<NAW2>). It is observed from the statistics of CW
as shown in Table 2 that the anger class contains
the maximum number of trigrams (20,785) and joy
class has the minimum number of trigrams
(15,743) whereas only the fear class contains the
maximum number of trigrams (1,573) that follow
the CW pattern. A few example patterns of the
CWs which follows the pattern (<NAW1>, <AW>,
<NAW2>) are “advices, about, problems” (Anger),
“already, frightened, us” (Fear), “always, joyous,
one” (Joy), “acted, cruelly, to” (Disgust), “adoles-
cent, guilt, growing” (guilt), “always, sad, for”
(sad) , “and, sorry, just” (Shame) etc.

It was observed that the stop words are
mostly present in <NAW1, AW, NAW2> pattern
where similar and dissimilar NAWs are appeared
before and after their corresponding CWs. In case
of fear, a total of 979 stop words were found in
NAW1 position and 935 stop words in NAW2 posi-
tion. It is observed that in case of fear, the occur-
rence of similar NAW before and after of CWs is
only 22 in contrast to the dissimilar occurrences of
1551. Table 3 explains the statistics of similar and
dissimilar NAWs along with their appearances as
stop words.

3.2 Context Vector Formation

In order to identify whether the Context Windows
(CWs) play any significant role in classifying emo-
tions or not, we have mapped the Context Win-
dows in a Vector space by representing them as
vectors. We have tried to find out the semantic re-
lation or similarity between a pair of vectors using
Affinity Score which in turn takes care of different
distances into consideration. Since a CW follows
the pattern (NAW1, AW, NAW2), the formation of
vector with respect to each of the Context Win-
dows of each emotion class was done based on the
following formula,

1 2
CW( )

#NAW #NAW#A
= , ,

W
Vectoriza

T T T
tion    

Where,
T= Total count of CW in an emotion class

#NAW1 = Total occurrence of a non-
affect word in NAW1 position

#NAW2 = Total occurrence of a non-
affect word in NAW2 position

#AW = Total occurrence of an affect word in
AW position.

It was found that in case of anger emotion,
a CW identified as (always, angry, about) corres-
ponds to a Vector, <0.29, 10.69, 1.47>

Emotions Total No of
Trigrams

Total no of Tri-
grams that follows
<NAW1,AW,NAW2>
pattern (CW)

Anger 20785 1356
Disgust 17661 1283
Fear 19392 1573
Guilt 18997 1298
Joy 15743 1179
Sadness 16270 1210
Shame 17731 1058
Table 2: Trigrams and Affect Words Statistics

Emotions Total no.
of NAW 1
appeared
as stop
words in
CW

Total no.
of NAW2
appeared
as stop
words in
CW

Presence
of
similar
NAW
before
and after
of CW

Presence
of
dissimilar
NAW
before
and after
of CW

Anger 825 871 26 1330
Disgust 696 763 11 1272
Fear 979 935 22 1551
Guilt 695 874 18 1280
Joy 734 674 11 1168
Sadness 733 753 22 1188
Shame 604 647 16 1042
NAW1= Non Affect Word1; AW=Affect Word; NAW2=Non
Affect Word2
Table 3: Statistics for similar and dissimilar NAW

patterns and stop words

3.3   Affinity Score Calculation

We assume that each of the Context Vectors in an
emotion class is represented in the vector space at
a specific distance from the others. Thus, there
must be some affinity or similarity exists between
each of the Context Vectors. An Affinity Score
was calculated for each  pair of Context Vectors
(pu,qv) where u = {1,2,3,.........n} and v =
{1,2,3,.......n} for n number of vectors with respect
to each of the  emotion classes. The final Score is

34



calculated using the following gravitational formu-
la as described in (Poria et al., 2013):

 
  

 p  q
,

, q  

*
p q

p

 
 

  
  

Score
2dist

The Score of any two context vectors p and q of an
emotion class is the dot product of the vectors di-
vided by the square of distance (dist) between p
and q. This score was inspired by Newton’s law of
gravitation. This score values reflect the affinity
between two context vectors p and q. Higher score
implies higher affinity between p and q.

However, apart from the score values, we
also calculated the median, standard deviation and
inter quartile range (iqr) and only those context
windows were considered if their iqr values are
greater than some cutoff value selected during ex-
periments.

3.4 Affinity Scores using Distance Metrics

In the vector space, it is needed to calculate how
close the context vectors are in the space in order
to conduct better classification into their respective
emotion classes. The Score values were calculated
for all the emotion classes with respect to different
metrics of distance (dist) viz. Chebyshev, Eucli-
dean and Hamming. The distance was calculated
for each context vector with respect to all the vec-
tors of the same emotion class. The distance for-
mula is given below:
a. Chebyshev distance (Cd) = max |xi - yi |

where xi and yi represents two vectors.
b. Euclidean distance (Ed) = ||x - y||2 for vectors x
and y.
c. Hamming distance (Hd) = (c01 + c10) / n where cij
is the number of occurrence in the boolean vectors
x and y and x[k] = i and y[k] = j for k < n. Ham-
ming distance denotes the proportion of disagree-
ing components in x and y.

4 Feature Selection and Analysis

It is observed that the feature selection always
plays an important role in building a good pattern
classifier. Thus, we have employed different clas-
sifiers viz. BayesNet, J48, NaiveBayesSimple and
DecisionTree associated in the WEKA tool. Based
on the previous analysis, the following features

were selected for developing the classification
framework.

1. Affinity Scores based on Cd, Ed and Hd
2. Context Window(CW)
3. POS Tagged Context Window (PTCW)
4. POS Tagged Window (PTW)
5. TF and TF-IDF of CW
6. Variance and Standard Deviation of CW
7. Ranking Score of CW

4.1 POS Tagged Context Windows and Win-
dows (PTCW and PTW)

The sentences were POS tagged using the Stanford
POS Tagger and the POS tagged Context Windows
were extracted and termed as PTCW. Similarly,
the POS tag sequence from each of the PTCWs
were extracted and named each as POS Tagged
Window (PTW). It is observed that “fear” emotion
class has the maximum number of CWs and unique
PTCWs whereas the “anger” class contains the
maximum number of unique PTWs. The Figure 1
as shown below represents the counts of CW,
unique PTCWs and PTWs. It was noticed that the
total number of CWs is 8967, total number of
unique PTCW is 7609 and of unique PTW is 3117.
Obviously, the number of PTCW was less than
CW and number of PTW was less than  PTCW,
because of the uniqueness of PTCW and PTW. In
Figure 2, the total counts of CW, PTCW and PTW
have been shown. Some sample patterns of PTWs
that occur with the maximum frequencies in three
emotion classes are “VBD/RB_JJ_IN” (anger),
“NN/VBD_VBN_NN” (disgust) and
“VBD_VBN/JJ_IN/NN” (fear).

Figure 1: Count of CW, PTCW and PTW for seven
emotion classes

35



Figure 2:Total Count of CW, PTCW and PTW

4.2 TF and TF-IDF Measure

The Term Frequencies (TFs) and the Inverse Doc-
ument Frequencies (IDFs) of the CWs for each of
the emotion classes were calculated. In order to
identify different ranges of the TF and TF-IDF
scores, the minimum and maximum values of the
TF and the variance of TF were calculated for each
of the emotion classes. It was observed that guilt
has the maximum scores for Max_TF and variance
whereas the emotions like anger and disgust have
the lowest scores for Max_TF as shown in Figure
3. Similarly, the minimum, maximum and variance
of the TF-IDF values were calculated for each
emotion class, separately. Again, it is found that
the guilt emotion has the highest Max_TF-IDF and
disgust emotion has the lowest Max_TF-IDF as
shown in Figure 4.

Not only for the Context Windows (CWs),
the TF and TF-IDF scores of the POS Tagged
Context Windows (PTCWs) and POS Tagged
Windows (PTWs) were also calculated with
respect to each emotion. It was observed that,
similar results were found. Variance, or second
moment about the mean, is a measure of the
variability (spread or dispersion) of data. A large
variance indicates that the data is spread out; a
small variance indicates it is clustered closely
around the mean.The variance for TF_IDF of guilt
is 0.0000456874. A few slight differences were
found in the results of PTWs while calculating
Max_TF , Min_TF and variance as shown in
Figure 3. It was observed that fear emotion has the
highest Max_TF and anger has the lowest Max_TF
whereas the variance of TF for guilt is
0.0002435522. Similarly, Figure 4 shows that fear
has the highest Max_TF_IDF and anger contains

the lowest Max_TF-IDF values and the variance of
TF-IDF of fear is 0.000922226.

Figure 3:Variance,Max_TF,Min_TF of CW, PTCW
and PTW

Figure 4: Variance,Max_TF-IDF, Min_TF-IDF of CW,
PTCW and PTW

4.3 Ranking Score of CW

It was found that some of the Context Windows
appear more than one time in the same emotion
class. Thus, they were removed and a ranking
score was calculated for each of the context win-
dows. Each of the words in a context window was
searched in the SentiWordnet lexicon and if found,
we considered either positive or negative or both
scores. The summation of the absolute scores of all
the words in a Context Window is returned. The
returned scores were sorted so that, in turn, each of
the context windows obtains a rank in its corres-
ponding emotion class.

All the ranks were calculated for each
emotion class, successively. This rank is useful in
finding the important emotional phrases from the
list of CWs. Some examples from the list of top 12
important context windows according to their rank
are “much anger when” (anger), “whom love after”
(happy), “felt sad about” (sadness) etc.

36



5 Result Analysis

The accuracies of the classifiers were obtained by
employing user defined test data and data for 10
fold cross validation. It is observed that when Euc-
lidean distance was considered, the BayesNet
Classifier gives 100% accuracy on the Test data
and gives 97.91% of accuracy on 10-fold cross
validation data. On the other hand, J48 classifier
achieves 77% accuracy on Test data and 83.54%
on 10-fold cross validation data whereas the Nai-
veBayesSimple classifier obtains 92.30% accuracy
on Test data and 27.07% accuracy on 10-fold cross
validation data. In the Naïve BayesSimple with 10-
fold cross validation, the average Recall, Precision
and F-measure values are 0.271, 0.272 and 0.264,
respectively. But, the DecisionTree classifier ob-
tains 98.30% and 98.10% accuracies on the Test
data as well as 10-fold cross validation data. The
comparative results are shown in Figure 5. Overall,
it is observed from Figure 5 that the BayesNet
classifier achieves the best results on the score data
which was prepared based on the Euclidean
distance. In contrast, the BayesNet achieved
99.30% accuracy on the Test data and 96.92% ac-
curacy on 10-fold cross validation data when the
Hamming distance was considered. Similarly, J48
and Naïve BayesSimple classifiers produce
93.05% and 85.41% accuracies on the Test data
and 87.95% and 39.50% accuracies on 10-fold
cross validation data, respectively.

From Figure 6, it is observed that the
DecisionTree  classifier produces the best accuracy
on the score data that was found using Hamming
distance. When the score values are found by using
Chebyshev distance, the BayesNet classifier
obtains  100% accuracy on Test data and 97.57%
accuracy on 10-fold cross validation data.
Similarly, J48 achieves 84.82% accuracy on the
Test data and 82.75% accuracy on 10-fold cross
validation data whereas NaiveBayes and
DecisionTable achieve  80% , 29.85% and 98.62%
,96.93% accuracies on the Test data and 10-fold
cross validatation data, respectively.

It has to be mentioned based on Figure 7
that the DecisionTree classifier performs better in
comparison with all other classifiers and achieves
the best result among the rest of the classifiers on
affinity score data prepared based on the Cheby-
shev distance only.

Figure 5: Classification Results on Test data and 10-
fold cross validation using Euclidean distance (Ed)

Figure 6: Classification Results on Test data and 10-
fold cross validation using Hamming distance (Hd)

Figure 7: Classification Results on Test data and 10-
fold cross validation using Chebyshev distance (Cd)

6 Conclusions and Future Works
In this paper, vector formation was done for each
of the Context Windows; TF and TF-IDF measures
were calculated. The calculated affinity score, de-
pending on the distance values was inspired from
Newton's law of gravitation. To classify these
CWs, BayesNet, J48, NaivebayesSimple and Deci-
sionTable classifiers.

In future, we would like to incorporate
more number of lexicons to identify and classify
emotional expressions. Moreover, we are planning
to include associative learning process to identify
some important rules for classification.

37



References

Balahur A , Hermida J.  2012.Extending the EmotiNet
Knowledge Base to Improve the Automatic Detection
of Implicitly Expressed Emotions from Text. In Irec-
conference 2012,pp-1207-1214

Das, D. and Bandyopadhyay, S. 2009. Word to Sentence
Level Emotion Tagging for Bengali Blogs. In ACL-
IJCNLP 2009 (Short Paper), pp.149-152

Das, D. and Bandyopadhyay, S. 2010. Developing Ben-
gali WordNet Affect for Analyzing Emotion.
ICCPOL-2010, pp. 35-40

Ekman, P.1993. Facial expression and emotion. Ameri-
can Psychologist, vol. 48(4) 384–392.

Erik Cambria, Robert Speer, Catherine Havasi, Amir
Hussain.2010. SenticNet: A Publicly Available Se-
mantic Resource for Opinion Mining

Kobayashi, N., K. Inui, Y. Matsumoto, K. Tateishi, and
T. Fukushima. 2004. Collecting evaluative expres-
sions for opinion extraction. IJCNLP.

Mohammad S and Turney P,2010. Emotions Evoked by
Common Words and Phrases: Using Mechanical
Turk to Create an Emotion Lexicon. In Proceedings
of the NAACL-HLT 2010 Workshop on Computa-
tional Approaches to Analysis and Generation of
Emotion in Text, June 2010, LA, California

Patra B, Takamura H, Das D, Okumura M, and Ban-
dyopadhyay S 2013.Construction of Emotional Lex-
icon Using Potts Model. In IJCNLP 2013 pp-674-679

Poria S, Gelbukh A, Hussain  A,  Howard N, Das D,
Bandyopadhyay S. 2013. Enhanced SenticNet with
Affective Labels for Concept-Based Opinion Mining,
IEEE Intelligent Systems, vol. 28, no. 2, pp. 31-38,

Scherer, K. R., & Wallbott, H.G. (1994). Evidence for
universality and cultural variation of differential
emotion response patterning. Journal of Personality
and Social Psychology, 66, 310-328.

Scherer, K. R. (1997). Profiles of emotion-antecedent
appraisal: testing theoretical predictions across cul-
tures. Cognition and Emotion, 11, 113-150.

Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani.2008. SENTIWORDNET 3.0: An Enhanced Lex-
ical Resource for Sentiment Analysis and Opinion
Mining

Strapparava, C. and Valitutti, A. 2004. Wordnet-affect:
an affective extension of wordnet. In 4th LREC, pp.
1083-1086

Takamura Hiroya, Takashi Inui, and Manabu Okumura.
2005. Extracting semantic orientations of words us-

ing spin model. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Lin-
guistics(ACL’05), pages 133–140.

Wiebe, J., Wilson, T. and Cardie, C. 2005. Annotating
expressions of opinions and emotions in language.
LRE, vol. 39(2-3), pp. 165-210.

http://wordnet.princeton.edu

http://www.cs.waikato.ac.nz/ml/weka/

http://emotion-
research.net/toolbox/toolboxdatabase.2006-10-
13.2581092615

http://www.affective-sciences.org/researchmaterial

38


