



















































Representation Learning for Text-level Discourse Parsing


Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 13–24,
Baltimore, Maryland, USA, June 23-25 2014. c©2014 Association for Computational Linguistics

Representation Learning for Text-level Discourse Parsing

Yangfeng Ji
School of Interactive Computing
Georgia Institute of Technology
jiyfeng@gatech.edu

Jacob Eisenstein
School of Interactive Computing
Georgia Institute of Technology

jacobe@gatech.edu

Abstract

Text-level discourse parsing is notoriously
difficult, as distinctions between discourse
relations require subtle semantic judg-
ments that are not easily captured using
standard features. In this paper, we present
a representation learning approach, in
which we transform surface features into
a latent space that facilitates RST dis-
course parsing. By combining the machin-
ery of large-margin transition-based struc-
tured prediction with representation learn-
ing, our method jointly learns to parse dis-
course while at the same time learning a
discourse-driven projection of surface fea-
tures. The resulting shift-reduce discourse
parser obtains substantial improvements
over the previous state-of-the-art in pre-
dicting relations and nuclearity on the RST
Treebank.

1 Introduction

Discourse structure describes the high-level or-
ganization of text or speech. It is central to
a number of high-impact applications, such as
text summarization (Louis et al., 2010), senti-
ment analysis (Voll and Taboada, 2007; Somasun-
daran et al., 2009), question answering (Ferrucci
et al., 2010), and automatic evaluation of student
writing (Miltsakaki and Kukich, 2004; Burstein
et al., 2013). Hierarchical discourse representa-
tions such as Rhetorical Structure Theory (RST)
are particularly useful because of the computa-
tional applicability of tree-shaped discourse struc-
tures (Taboada and Mann, 2006), as shown in Fig-
ure 1.

Unfortunately, the performance of discourse
parsing is still relatively weak: the state-of-the-art
F-measure for text-level relation detection in the
RST Treebank is only slightly above 55% (Joty

when profit was $107.8 
million on sales of $435.5 

million.

The projections are in the 
neighborhood of 50 cents 

a share to 75 cents,

compared with a restated 
$1.65 a share a year 

earlier,

CIRCUMSTANCE

COMPARISON

Figure 1: An example of RST discourse structure.

et al., 2013). While recent work has introduced
increasingly powerful features (Feng and Hirst,
2012) and inference techniques (Joty et al., 2013),
discourse relations remain hard to detect, due in
part to a long tail of “alternative lexicalizations”
that can be used to realize each relation (Prasad et
al., 2010). Surface and syntactic features are not
capable of capturing what are fundamentally se-
mantic distinctions, particularly in the face of rel-
atively small annotated training sets.

In this paper, we present a representation learn-
ing approach to discourse parsing. The core idea
of our work is to learn a transformation from a
bag-of-words surface representation into a latent
space in which discourse relations are easily iden-
tifiable. The latent representation for each dis-
course unit can be viewed as a discriminatively-
trained vector-space representation of its meaning.
Alternatively, our approach can be seen as a non-
linear learning algorithm for incremental struc-
ture prediction, which overcomes feature sparsity
through effective parameter tying. We consider
several alternative methods for transforming the
original features, corresponding to different ideas
of the meaning and role of the latent representa-
tion.

Our method is implemented as a shift-reduce
discourse parser (Marcu, 1999; Sagae, 2009).
Learning is performed as large-margin transition-
based structure prediction (Taskar et al., 2003),
while at the same time jointly learning to project
the surface representation into latent space. The

13



resulting system strongly outperforms the prior
state-of-the-art at labeled F-measure, obtaining
raw improvements of roughly 6% on relation la-
bels and 2.5% on nuclearity. In addition, we show
that the latent representation coheres well with the
characterization of discourse connectives in the
Penn Discourse Treebank (Prasad et al., 2008).

2 Model

The core idea of this paper is to project lexical fea-
tures into a latent space that facilitates discourse
parsing. In this way, we can capture the meaning
of each discourse unit, without suffering from the
very high dimensionality of a lexical representa-
tion. While such feature learning approaches have
proven to increase robustness for parsing, POS
tagging, and NER (Miller et al., 2004; Koo et al.,
2008; Turian et al., 2010), they would seem to
have an especially promising role for discourse,
where training data is relatively sparse and ambi-
guity is considerable. Prasad et al. (2010) show
that there is a long tail of alternative lexicalizations
for discourse relations in the Penn Discourse Tree-
bank, posing obvious challenges for approaches
based on directly matching lexical features ob-
served in the training data.

Based on this observation, our goal is to learn
a function that transforms lexical features into
a much lower-dimensional latent representation,
while simultaneously learning to predict discourse
structure based on this latent representation. In
this paper, we consider a simple transformation
function, linear projection. Thus, we name the ap-
proach DPLP: Discourse Parsing from Linear Pro-
jection. We apply transition-based (incremental)
structured prediction to obtain a discourse parse,
training a predictor to make the correct incremen-
tal moves to match the annotations of training data
in the RST Treebank. This supervision signal is
then used to learn both the weights and the projec-
tion matrix in a large-margin framework.

2.1 Shift-reduce discourse parsing

We construct RST Trees using shift-reduce pars-
ing, as first proposed by Marcu (1999). At each
point in the parsing process, we maintain a stack
and a queue; initially the stack is empty and the
first elementary discourse unit (EDU) in the docu-
ment is at the front of the queue.1 The parser can

1We do not address segmentation of text into elemen-
tary discourse units in this paper. Standard classification-

Notation Explanation
V Vocabulary for surface features
V Size of V
K Dimension of latent space
wm Classification weights for class m
C Total number of classes, which correspond to

possible shift-reduce operations
A Parameter of the representation function (also

the projection matrix in the linear representa-
tion function)

vi Word count vector of discourse unit i
v Vertical concatenation of word count vectors

for the three discourse units currently being
considered by the parser

λ Regularization for classification weights
τ Regularization for projection matrix
ξi Slack variable for sample i
ηi,m Dual variable for sample i and class m
αt Learning rate at iteration t

Table 1: Summary of mathematical notation

then choose either to shift the front of the queue
onto the top of the stack, or to reduce the top two
elements on the stack in a discourse relation. The
reduction operation must choose both the type of
relation and which element will be the nucleus.
So, overall there are multiple reduce operations
with specific relation types and nucleus positions.
Shift-reduce parsing can be learned as a classifi-
cation task, where the classifier uses features of
the elements in the stack and queue to decide what
move to take. Previous work has employed deci-
sion trees (Marcu, 1999) and the averaged percep-
tron (Collins and Roark, 2004; Sagae, 2009) for
this purpose. Instead, we employ a large-margin
classifier, because we can compute derivatives of
the margin-based objective function with respect
to both the classifier weights as well as the projec-
tion matrix.

2.2 Discourse parsing with projected features

More formally, we denote the surface feature vo-
cabulary V , and represent each EDU as the nu-
meric vector v ∈ NV , where V = #|V| and the n-
th element of v is the count of the n-th surface fea-
ture in this EDU (see Table 1 for a summary of no-
tation). During shift-reduce parsing, we consider
features of three EDUs:2 the top two elements on

based approaches can achieve a segmentation F-measure
of 94% (Hernault et al., 2010); a more complex rerank-
ing model does slightly better, at 95% F-Measure with
automatically-generated parse trees, and 96.6% with gold an-
notated trees (Xuan Bach et al., 2012). Human agreement
reaches 98% F-Measure.

2After applying a reduce operation, the stack will include
a span that contains multiple EDUs. We follow the strong

14



the stack (v1 and v2), and the front of the queue
(v3). The vertical concatenation of these vectors
is denoted v = [v1;v2;v3]. In general, we can
formulate the decision function for the multi-class
shift-reduce classifier as

m̂ = arg max
m∈{1,...,C}

w>mf(v;A) (1)

where wm is the weight for the m-th class
and f(v;A) is the representation function
parametrized by A. The score for class m (in
our case, the value of taking the m-th shift-
reduce operation) is computed by the inner prod-
uct w>mf(v;A). The specific shift-reduce opera-
tion is chosen by maximizing the decision value in
Equation 1.

The representation function f(v;A) can be de-
fined in any form; for example, it could be a non-
linear function defined by a neural network model
parametrized by A. We focus on the linear projec-
tion,

f(v;A) = Av, (2)

where A ∈ RK×3V is projects the surface repre-
sentation v of three EDUs into a latent space of
size K � V .

Note that by setting w̃>m = w>mA, the decision
scoring function can be rewritten as w̃>mv, which
is linear in the original surface features. Therefore,
the expressiveness of DPLP is identical to a linear
separator in the original feature space. However,
the learning problem is considerably different. If
there are C total classes (possible shift-reduce op-
erations), then a linear classifier must learn 3V C
parameters, while DPLP must learn (3V + C)K
parameters, which will be smaller under the as-
sumption that K < C � V . This can be seen
as a form of parameter tying on the linear weights
w̃m, which allows statistical strength to be shared
across training instances. We will consider special
cases of A that reduce the parameter space still
further.

2.3 Special forms of the projection matrix

We consider three different constructions for the
projection matrix A.

• General form: In the general case, we place
compositionality criterion of Marcu (1996) and consider only
the nuclear EDU of the span. Later work may explore the
composition of features between the nucleus and satellite.

no special constraint on the form of A.

f(v;A) = A

 v1v2
v3

 (3)
This form is shown in Figure 2(a).

• Concatenation form: In the concatenation
form, we choose a block structure for A, in
which a single projection matrix B is applied
to each EDU:

f(v;A) =

[
B 0 0
0 B 0
0 0 B

][
v1
v2
v3

]
(4)

In this form, we transform the representa-
tion of each EDU separately, but do not at-
tempt to represent interrelationships between
the EDUs in the latent space. The number
of parameters in A is 13KV . Then, the total
number of parameters, including the decision
weights {wm}, in this form is (V3 + C)K.
• Difference form. In the difference form, we

explicitly represent the differences between
adjacent EDUs, by constructing A as a block
difference matrix,

f(v;A) =

[
C −C 0
C 0 −C
0 0 0

][
v1
v2
v3

]
, (5)

The result of this projection is that the la-
tent representation has the form [C(v1 −
v2);C(v1−v3)], representing the difference
between the top two EDUs on the stack, and
between the top EDU on the stack and the
first EDU in the queue. This is intended
to capture semantic similarity, so that reduc-
tions between related EDUs will be preferred.
Similarly, the total number of parameters to
estimate in this form is (V + 2C)K3 .

3 Large-Margin Learning Framework

We apply a large margin structure prediction ap-
proach to train the model. There are two pa-
rameters that need to be learned: the classifica-
tion weights {wm}, and the projection matrix A.
As we will see, it is possible to learn {wm} us-
ing standard support vector machine (SVM) train-
ing (holding A fixed), and then make a simple
gradient-based update to A (holding {wm} fixed).
By interleaving these two operations, we arrive at
a saddle point of the objective function.

15



A

W

y

v1 from stack v 2 from stack v 3 from queue

(a) General form

A

W

y

v1 from stack v 2 from stack v 3 from queue

(b) Concatenation form

A

W

y

v1 from stack v 2 from stack v 3 from queue

(c) Difference form

Figure 2: Decision problem with different representation functions

Specifically, we formulate the following con-
strained optimization problem,

min
{w1:C ,ξ1:l,A}

λ

2

C∑
m=1

‖wm‖22 +
l∑
i=1

ξi +
τ

2
‖A‖2F

s.t. (wyi−wm)>f(vi;A) ≥ 1− δyi=m − ξi,
∀ i,m

(6)

where m ∈ {1, . . . , C} is the index of the
shift-reduce decision taken by the classifier (e.g.,
SHIFT, REDUCE-CONTRAST-RIGHT, etc), i ∈
{1, · · · , l} is the index of the training sample, and
wm is the vector of classification weights for class
m. The slack variables ξi permit the margin con-
straint to be violated in exchange for a penalty, and
the delta function δyi=m is unity if yi = m, and
zero otherwise.

As is standard in the multi-class linear
SVM (Crammer and Singer, 2001), we can solve
the problem defined in Equation 6 via Lagrangian
optimization:

L({w1:C , ξ1:l,A, η1:l,1:C}) =
λ

2

C∑
m=1

‖wm‖22 +
l∑
i=1

ξi +
τ

2
‖A‖2F

+
∑
i,m

ηi,m
{
(w>m −w>yi)f(vi;A) + 1− δyi=m − ξi

}
s.t. ηi,m ≥ 0 ∀i,m

(7)
Then, to optimize L, we need to find a saddle
point, which would be the minimum for the vari-
ables {w1:C , ξ1:l} and the projection matrix A,
and the maximum for the dual variables {η1:l,1:C}.

If A is fixed, then the optimization problem is
equivalent to a standard multi-class SVM, in the
transformed feature space f(vi;A). We can obtain
the weights {w1:C} and dual variables {η1:l,1:C}
from a standard dual-form SVM solver. We then
update A, recompute {w1:C} and {η1:l,1:C}, and
iterate until convergence. This iterative procedure
is similar to the latent variable structural SVM (Yu
and Joachims, 2009), although the specific details
of our learning algorithm are different.

3.1 Learning Projection Matrix A
We update A while holding fixed the weights and
dual variables. The derivative of L with respect to
A is

∂L
∂A

= τA +
∑
i,m

ηi,m(w
>
m −w>yi)

∂f(vi;A)

∂A

= τA +
∑
i,m

ηi,m(wm −wyi)vi>
(8)

Setting ∂L∂A = 0, we have the closed-form solution,

A =
1

τ

∑
i,m

ηi,m(wm −wyi)vi>

=
1

τ

∑
i,j

(wyi −
∑
m

ηi,mwm)vi
>,

(9)

because the dual variables for each instance must
sum to one,

∑
m ηi,m = 1.

Note that for a given i, the matrix (wyi −∑
m ηi,mwm)vi

> is of (at most) rank-1. There-
fore, the solution of A can be viewed as the lin-
ear combination of a sequence of rank-1 matrices,
where each rank-1 matrix is defined by distribu-
tional representation vi and the weight difference
between the weight of true label wyi and the “ex-
pected” weight

∑
m ηi,mwm.

One property of the dual variables is that
f(vi;A) is a support vector only if the dual vari-
able ηi,yi < 1. Since the dual variables for each
instance are guaranteed to sum to one, we have
wyi −

∑
m ηi,mwm = 0 if ηi,yi = 1. In other

words, the contribution from non support vectors
to the projection matrix A is 0. Then, we can fur-
ther simplify the updating equation as

A =
1
τ

∑
vi∈SV

(wyi −
∑
m

ηi,mwm)vi> (10)

This is computationally advantageous since many
instances are not support vectors, and it shows that
the discriminatively-trained projection matrix only
incorporates information from each instance to the
extent that the correct classification receives low
confidence.

16



Algorithm 1 Mini-batch learning algorithm
Input: Training set D, Regularization parame-
ters λ and τ , Number of iteration T , Initializa-
tion matrix A0, and Threshold ε
while t = 1, . . . , T do

Randomly choose a subset of training sam-
ples Dt from D
Train SVM with At−1 to obtain {w(t)m } and
{η(t)i,m}
Update At using Equation 11 with αt = 1t
if ‖At−At−1‖F‖A2−A1‖F < ε then

Return
end if

end while
Re-train SVM with D and the final A
Output: Projection matrix A, SVM classifier
with weights w

3.2 Gradient-based Learning for A

Solving the quadratic programming defined by the
dual form of the SVM is time-consuming, espe-
cially on a large-scale dataset. But if we focus on
learning the projection matrix A, we can speed up
learning by sampling only a small proportion of
the training data to compute an approximate op-
timum for {w1:C , η1:l,1:C}, before each update of
A. This idea is similar to the mini-batch learning,
which has been used in large-scale SVM problem
(Nelakanti et al., 2013) and deep learning models
(Le et al., 2011).

Specifically, in iteration t, the algorithm ran-
domly chooses a subset of training samples Dt to
train the model. We cannot make a closed-form
update to A based on this small sample, but we
can take an approximate gradient step,

At = (1− αtτ)At−1+
αt
{ ∑

vi∈SV(Dt)

(
w(t)yi −

∑
m

η
(t)
i,mw

(t)
m

)
vi
>
}
, (11)

where αt is a learning rate. In iteration t, we
choose αt = 1t . After convergence, we obtain the
weights w by applying the SVM over the entire
dataset, using the final A. The algorithm is sum-
marized in Algorithm 1 and more details about im-
plementation will be clarified in Section 4. While
minibatch learning requires more iterations, the
SVM training is much faster in each batch, and the
overall algorithm is several times faster than using
the entire training set for each update.

4 Implementation

The learning algorithm is applied in a shift-reduce
parser, where the training data consists of the
(unique) list of shift and reduce operations re-
quired to produce the gold RST parses. On test
data, we choose parsing operations in an online
fashion — at each step, the parsing algorithm
changes the status of the stack and the queue ac-
cording the selected transition, then creates the
next sample with the updated status.

4.1 Parameters and Initialization
There are three free parameters in our approach:
the latent dimension K, and regularization pa-
rameters λ and τ . We consider the values K ∈
{30, 60, 90, 150}, λ ∈ {1, 10, 50, 100} and τ ∈
{1.0, 0.1, 0.01, 0.001}, and search over this space
using a development set of thirty document ran-
domly selected from within the RST Treebank
training data. We initialize each element of A0
to a uniform random value in the range [0, 1]. For
mini-batch learning, we fixed the batch size to be
500 training samples (shift-reduce operations) in
each iteration.

4.2 Additional features
As described thus far, our model considers only
the projected representation of each EDU in its
parsing decisions. But prior work has shown that
other, structural features can provide useful in-
formation (Joty et al., 2013). We therefore aug-
ment our classifier with a set of simple feature
templates. These templates are applied to individ-
ual EDUs, as well as pairs of EDUs: (1) the two
EDUs on top of the stack, and (2) the EDU on top
of the stack and the EDU in front of the queue.
The features are shown in Table 2. In computing
these features, all tokens are downcased, and nu-
merical features are not binned. The dependency
structure and POS tags are obtained from MALT-
Parser (Nivre et al., 2007).

5 Experiments

We evaluate DPLP on the RST Discourse Tree-
bank (Carlson et al., 2001), comparing against
state-of-the-art results. We also investigate the in-
formation encoded by the projection matrix.

5.1 Experimental Setup
Dataset The RST Discourse Treebank (RST-
DT) consists of 385 documents, with 347 for train-

17



Feature Examples

Words at beginning and end of the EDU 〈BEGIN-WORD-STACK1 = but〉〈BEGIN-WORD-STACK1-QUEUE1 = but, the〉
POS tag at beginning and end of the EDU 〈BEGIN-TAG-STACK1 = CC〉〈BEGIN-TAG-STACK1-QUEUE1 = CC, DT〉
Head word set from each EDU. The set includes words
whose parent in the depenency graph is ROOT or is not
within the EDU (Sagae, 2009).

〈HEAD-WORDS-STACK2 = working〉

Length of EDU in tokens 〈LEN-STACK1-STACK2 = 〈7, 8〉〉
Distance between EDUs 〈DIST-STACK1-QUEUE1 = 2〉
Distance from the EDU to the beginning of the document 〈DIST-FROM-START-QUEUE1 = 3〉
Distance from the EDU to the end of the document 〈DIST-FROM-END-STACK1 = 1〉
Whether two EDUs are in the same sentence 〈SAME-SENT-STACK1-QUEUE1 = True〉

Table 2: Additional features for RST parsing

ing and 38 for testing in the standard split. As
we focus on relational discourse parsing, we fol-
low prior work (Feng and Hirst, 2012; Joty et al.,
2013), and use gold EDU segmentations. The
strongest automated RST segmentation methods
currently attain 95% accuracy (Xuan Bach et al.,
2012).

Preprocessing In the RST-DT, most nodes have
exactly two children, one nucleus and one satellite.
For non-binary relations, we use right-branching
to binarize the tree structure. For multi-nuclear
relations, we choose the left EDU as “head”
EDU. The vocabulary V includes all unigrams af-
ter down-casing. No other preprocessing is per-
formed. In total, there are 16250 unique unigrams
in V .

Fixed projection matrix baselines Instead of
learning from data, a simple way to obtain a pro-
jection matrix is to use matrix factorization. Re-
cent work has demonstrated the effectiveness of
non-negative matrix factorization (NMF) for mea-
suring distributional similarity (Dinu and Lapata,
2010; Van de Cruys and Apidianaki, 2011). We
can construct Bnmf in the concatenation form
of the projection matrix by applying NMF to the
EDU-feature matrix, M ≈WH. As a result, W
describes each EDU with aK-dimensional vector,
and H describes each word with a K-dimensional
vector. We can then construct Bnmf by taking
the pseudo-inverse of H, which then projects from
word-count vectors into the latent space.

Another way to construct B is to use neural
word embeddings (Collobert and Weston, 2008).
In this case, we can view the product Bv as a com-
position of the word embeddings, using the simple
additive composition model proposed by Mitchell

and Lapata (2010). We used the word embeddings
from Collobert and Weston (2008) with dimension
{25, 50, 100}. Grid search over heldout training
data was used to select the optimum latent dimen-
sion for both the NMF and word embedding base-
lines. Note that the size K of the resulting projec-
tion matrix is three times the size of the embed-
ding (or NMF representation) due to the concate-
nate construction.

We also consider the special case where A = I.

Competitive systems We compare our approach
with HILDA (Hernault et al., 2010) and TSP (Joty
et al., 2013). Joty et al. (2013) proposed two dif-
ferent approaches to combine sentence-level pars-
ing models: sliding windows (TSP SW) and 1
sentence-1 subtree (TSP 1-1). In the comparison,
we report the results of both approaches. All re-
sults are based on the same gold standard EDU
segmentation. We cannot compare with the re-
sults of Feng and Hirst (2012), because they do
not evaluate on the overall discourse structure, but
rather treat each relation as an individual classifi-
cation problem.

Metrics To evaluate the parsing performance,
we use the three standard ways to measure the per-
formance: unlabeled (i.e., hierarchical spans) and
labeled (i.e., nuclearity and relation) F-score, as
defined by Black et al. (1991). The application
of this approach to RST parsing is described by
Marcu (2000b).3 To compare with previous works
on RST-DT, we use the 18 coarse-grained relations
defined in (Carlson et al., 2001).

3We implemented the evaluation metrics by ourselves.
Together with the DPLP system, all codes are published on
https://github.com/jiyfeng/DPLP

18



Method Matrix Form +Features K Span Nuclearity Relation
Prior work
1. HILDA (Hernault et al., 2010) 83.0 68.4 54.8
2. TSP 1-1 (Joty et al., 2013) 82.47 68.43 55.73
3. TSP SW (Joty et al., 2013) 82.74 68.40 55.71
Our work
4. Basic features A = 0 Yes 79.43 67.98 52.96
5. Word embeddings Concatenation No 75 75.28 67.14 53.79
6. NMF Concatenation No 150 78.57 67.66 54.80
7. Bag-of-words A = I Yes 79.85 69.01 60.21
8. DPLP Concatenation No 60 80.91 69.39 58.96
9. DPLP Difference No 60 80.47 68.61 58.27
10. DPLP Concatenation Yes 60 82.08 71.13 61.63
11. DPLP General Yes 30 81.60 70.95 61.75
Human annotation 88.70 77.72 65.75

Table 3: Parsing results of different models on the RST-DT test set. The results of TSP and HILDA are
reprinted from prior work (Joty et al., 2013; Hernault et al., 2010).

5.2 Experimental Results

Table 3 presents RST parsing results for DPLP and
some alternative systems. All versions
of DPLP outperform the prior state-of-the-art
on nuclearity and relation detection. This includes
relatively simple systems whose features are
simply a projection of the word count vectors
for each EDU (lines 7 and 8). The addition of
the features from Table 2 improves performance
further, leading to absolute F-score improvement
of around 2.5% in nuclearity and 6% in relation
prediction (lines 9 and 10).

On span detection, DPLP performs slightly
worse than the prior state-of-the-art. These sys-
tems employ richer syntactic and contextual fea-
tures, which might be especially helpful for span
identification. As shown by line 4 of the re-
sults table, the basic features from Table 2 pro-
vide most of the predictive power for spans; how-
ever, these features are inadequate at the more
semantically-oriented tasks of nuclearity and re-
lation prediction, which benefit substantially from
the projected features. Since correctly identifying
spans is a precondition for nuclearity and relation
prediction, we might obtain still better results by
combining features from HILDA and TSP with the
representation learning approach described here.

Lines 5 and 6 show that discriminative learning
of the projection matrix is crucial, as fixed projec-
tions obtained from NMF or neural word embed-
dings perform substantially worse. Line 7 shows
that the original bag-of-words representation to-
gether with basic features could give us some ben-
efit on discourse parsing, but still not as good as
results from DPLP. From lines 8 and 9, we see

that the concatenation construction is superior to
the difference construction, but the comparison
between lines 10 and 11 is inconclusive on the
merits of the general form of A. This suggests
that using the projection matrix to model interre-
lationships between EDUs does not substantially
improve performance, and the simpler concatena-
tion construction may be preferred.

Figure 3 shows how performance changes for
different latent dimensions K. At each value of
K, we employ grid search over a development set
to identify the optimal regularizers λ and τ . For
the concatenation construction, performance is not
overly sensitive to K. For the general form of A,
performance decreases with large K. Recall from
Section 2.3 that this construction has nine times as
many parameters as the concatenation form; with
large values of K, it is likely to overfit.

5.3 Analysis of Projection Matrix

Why does projection of the surface features im-
prove discourse parsing? To answer this question,
we examine what information the projection ma-
trix is learning to encoded. We take the projec-
tion matrix from the concatenation construction
and K = 60 as an example for case study. Re-
calling the definition in equation 4, the projection
matrix A will be composed of three identical sub-
matrices B ∈ R20×V . The columns of the B ma-
trix can be viewed as 20-dimensional descriptors
of the words in the vocabulary.

For the purpose of visualization, we further re-
duce the dimension of latent representation from
K = 20 to 2 dimensions using t-SNE (van der
Maaten and Hinton, 2008). One further simpli-

19



30 60 90 150
K

76

77

78

79

80

81

82

83

84

F-
sc

o
re

Concatenation DPLP

General DPLP

TSP 1-1 (Joty, et al., 2013)

HILDA (Hernault, et al., 2010)

(a) Span

30 60 90 150
K

65

66

67

68

69

70

71

72

F-
sc

o
re

Concatenation DPLP

General DPLP

TSP 1-1 (Joty, et al., 2013)

HILDA (Hernault, et al., 2010)

(b) Nuclearity

30 60 90 150
K

50

52

54

56

58

60

62

F-
sc

o
re

Concatenation DPLP

General DPLP

TSP 1-1 (Joty, et al., 2013)

HILDA (Hernault, et al., 2010)

(c) Relation

Figure 3: The performance of our parser over different latent dimension K. Results for DPLP include
the additional features from Table 3

fication for visualization is we consider only the
top 1000 frequent unigrams in the RST-DT train-
ing set. For comparison, we also apply t-SNE to
the projection matrix Bnmf recovered from non-
negative matrix factorization.

Figure 4 highlights words that are related to dis-
course analysis. Among the top 1000 words, we
highlight the words from 5 major discourse con-
nective categories provided in Appendix B of the
PDTB annotation manual (Prasad et al., 2008):
CONJUNCTION, CONTRAST, PRECEDENCE, RE-
SULT, and SUCCESSION. In addition, we also
highlighted two verb categories from the top 1000
words: modal verbs and reporting verbs, with their
inflections (Krestel et al., 2008).

From the figure, it is clear DPLP has learned a
projection matrix that successfully groups several
major discourse-related word classes: particularly
modal and reporting verbs; it has also grouped
succession and precedence connectives with some
success. In contrast, while NMF does obtain com-
pact clusters of words, these clusters appear to be
completely unrelated to discourse function of the
words that they include. This demonstrates the
value of using discriminative training to obtain the
transformed representation of the discourse units.

6 Related Work

Early work on document-level discourse parsing
applied hand-crafted rules and heuristics to build
trees in the framework of Rhetorical Structure
Theory (Sumita et al., 1992; Corston-Oliver, 1998;
Marcu, 2000a). An early data-driven approach
was offered by Schilder (2002), who used distribu-
tional techniques to rate the topicality of each dis-
course unit, and then chose among underspecified
discourse structures by placing more topical sen-

tences near the root. Learning-based approaches
were first applied to identify within-sentence dis-
course relations (Soricut and Marcu, 2003), and
only later to cross-sentence relations at the docu-
ment level (Baldridge and Lascarides, 2005). Of
particular relevance to our inference technique are
incremental discourse parsing approaches, such
as shift-reduce (Sagae, 2009) and A* (Muller et
al., 2012). Prior learning-based work has largely
focused on lexical, syntactic, and structural fea-
tures, but the close relationship between discourse
structure and semantics (Forbes-Riley et al., 2006)
suggests that shallow feature sets may struggle
to capture the long tail of alternative lexicaliza-
tions that can be used to realize discourse rela-
tions (Prasad et al., 2010; Marcu and Echihabi,
2002). Only Subba and Di Eugenio (2009) incor-
porate rich compositional semantics into discourse
parsing, but due to the ambiguity of their seman-
tic parser, they must manually select the correct
semantic parse from a forest of possiblities.

Recent work has succeeded in pushing the state-
of-the-art in RST parsing by innovating on sev-
eral fronts. Feng and Hirst (2012) explore rich
linguistic linguistic features, including lexical se-
mantics and discourse production rules suggested
by Lin et al. (2009) in the context of the Penn Dis-
course Treebank (Prasad et al., 2008). Muller et
al. (2012) show that A* decoding can outperform
both greedy and graph-based decoding algorithms.
Joty et al. (2013) achieve the best prior results
on RST relation detection by (i) jointly perform-
ing relation detection and classification, (ii) per-
forming bottom-up rather than greedy decoding,
and (iii) distinguishing between intra-sentence and
inter-sentence relations. Our approach is largely
orthogonal to this prior work: we focus on trans-

20



although until

however
also

though

but

thus

later

can

could
would

should
and

when
after so

once
will

might
may

before
then

says
say

reportedsaid
saying

believe
think

must
asked

report

(a) Latent representation of words from projection learning
with K = 20.

but
would

when

also may
can

then
must

might
once

however

sothough

thus

although

should
later

until
will
before

after
could

and

says
said

say

asked
saying

think

believe

report

Conjunction
Contrast
Precedence
Result
Succession
Modal verb
Reporting verb

(b) Latent representation of words from non-negative matrix
factorization with K = 20.

Figure 4: t-SNE Visualization on latent representations of words.

forming the lexical representation of discourse
units into a latent space to facilitate learning. As
shown in Figure 4(a), this projection succeeds
at grouping words with similar discourse func-
tions. We might expect to obtain further improve-
ments by augmenting this representation learning
approach with rich syntactic features (particularly
for span identification), more accurate decoding,
and special treatment of intra-sentence relations;
this is a direction for future research.

Discriminative learning of latent features for
discourse processing can be viewed as a form
of representation learning (Bengio et al., 2013).
Also called Deep Learning, such approaches
have recently been applied in a number of NLP
tasks (Collobert et al., 2011; Socher et al., 2012).
Of particular relevance are applications to the de-
tection of semantic or discourse relations, such
as paraphrase, by comparing sentences in an in-
duced latent space (Socher et al., 2011; Guo and
Diab, 2012; Ji and Eisenstein, 2013). In this work,
we show how discourse structure annotations can
function as a supervision signal to discriminatively
learn a transformation from lexical features to a la-
tent space that is well-suited for discourse parsing.
Unlike much of the prior work on representation
learning, we induce a simple linear transforma-
tion. Extension of our approach by incorporating
a non-linear activation function is a natural topic
for future research.

7 Conclusion

We have presented a framework to perform dis-
course parsing while jointly learning to project to
a low-dimensional representation of the discourse

units. Using the vector-space representation of
EDUs, our shift-reduce parsing system substan-
tially outperforms existing systems on nuclearity
detection and discourse relation identification. By
adding some additional surface features, we ob-
tain further improvements. The low dimensional
representation also captures basic intuitions about
discourse connectives and verbs, as shown in Fig-
ure 4(a).

Deep learning approaches typically apply a
non-linear transformation such as the sigmoid
function (Bengio et al., 2013). We have con-
ducted a few unsuccessful experiments with the
“hard tanh” function proposed by Collobert and
Weston (2008), but a more complete exploration
of non-linear transformations must wait for future
work. Another direction would be more sophis-
ticated composition of the surface features within
each elementary discourse unit, such as the hierar-
chical convolutional neural network (Kalchbren-
ner and Blunsom, 2013) or the recursive tensor
network (Socher et al., 2013). It seems likely that
a better accounting for syntax could improve the
latent representations that our method induces.

Acknowledgments

We thank the reviewers for their helpful feedback,
particularly for the connection to multitask learn-
ing. We also want to thank Kenji Sagae and
Vanessa Wei Feng for the helpful discussion via
email communication. This research was sup-
ported by Google Faculty Research Awards to the
second author.

21



References
Jason Baldridge and Alex Lascarides. 2005. Proba-

bilistic head-driven parsing for discourse structure.
In Proceedings of the Ninth Conference on Compu-
tational Natural Language Learning, pages 96–103.

Yoshua Bengio, Aaron Courville, and Pascal Vincent.
2013. Representation Learning: A Review and New
Perspectives. IEEE Transactions on Pattern Analy-
sis and Machine Intelligence, 35(8):1798–1828.

Ezra Black, Steve Abney, Dan Flickinger, Claudia
Gdaniec, Ralph Grishman, Phil Harrison, Don Hin-
dle, Robert Ingria, Fred Jelinek, Judith Klavans,
Mark Liberman, Mitchell Marcus, Salim Roukos,
Beatrice Santorini, and Tomek Strzalkowski. 1991.
A Procedure for Quantitatively Comparing the Syn-
tactic Coverage of English Grammars. In Speech
and Natural Language: Proceedings of a Workshop
Held at Pacific Grove, California, February 19-22,
1991, pages 306–311.

Jill Burstein, Joel Tetreault, and Martin Chodorow.
2013. Holistic discourse coherence annotation
for noisy essay writing. Dialogue & Discourse,
4(2):34–52.

Lynn Carlson, Daniel Marcu, and Mary Ellen
Okurowski. 2001. Building a Discourse-tagged
Corpus in the Framework of Rhetorical Structure
Theory. In Proceedings of Second SIGdial Work-
shop on Discourse and Dialogue.

Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceed-
ings of ACL, page 111. Association for Computa-
tional Linguistics.

R. Collobert and J. Weston. 2008. A Unified Architec-
ture for Natural Language Processing: Deep Neural
Networks with Multitask Learning. In ICML.

R. Collobert, J. Weston, L. Bottou, M. Karlen,
K. Kavukcuoglu, and P. Kuksa. 2011. Natural Lan-
guage Processing (Almost) from Scratch. Journal of
Machine Learning Research, 12:2493–2537.

Simon Corston-Oliver. 1998. Beyond string matching
and cue phrases: Improving efficiency and coverage
in discourse analysis. In The AAAI Spring Sympo-
sium on Intelligent Text Summarization, pages 9–15.

Koby Crammer and Yoram Singer. 2001. On the Algo-
rithmic Implementation of Multiclass Kernel-based
Vector Machines. Journal of Machine Learning Re-
search, 2:265–292.

Georgiana Dinu and Mirella Lapata. 2010. Measur-
ing Distributional Similarity in Context. In EMNLP,
pages 1162–1172.

Vanessa Wei Feng and Graeme Hirst. 2012. Text-level
Discourse Parsing with Rich Linguistic Features. In
Proceedings of ACL.

David Ferrucci, Eric Brown, Jennifer Chu-Carroll,
James Fan, David Gondek, Aditya A Kalyanpur,
Adam Lally, J William Murdock, Eric Nyberg, John
Prager, et al. 2010. Building Watson: An overview
of the DeepQA project. AI magazine, 31(3):59–79.

Katherine Forbes-Riley, Bonnie Webber, and Aravind
Joshi. 2006. Computing discourse semantics: The
predicate-argument semantics of discourse connec-
tives in D-LTAG. Journal of Semantics, 23(1):55–
106.

Weiwei Guo and Mona Diab. 2012. Modeling Sen-
tences in the Latent Space. In Proceedings of ACL,
pages 864–872, Jeju Island, Korea, July. Association
for Computational Linguistics.

Hugo Hernault, Helmut Prendinger, David A. duVerle,
and Mitsuru Ishizuka. 2010. HILDA: A Discourse
Parser Using Support Vector Machine Classification.
Dialogue and Discourse, 1(3):1–33.

Yangfeng Ji and Jacob Eisenstein. 2013. Discrimina-
tive Improvements to Distributional Sentence Simi-
larity. In EMNLP, pages 891–896, Seattle, Washing-
ton, USA, October. Association for Computational
Linguistics.

Shafiq Joty, Giuseppe Carenini, Raymond Ng, and
Yashar Mehdad. 2013. Combining Intra- and
Multi-sentential Rhetorical Parsing for Document-
level Discourse Analysis. In Proceedings of ACL.

Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
convolutional neural networks for discourse compo-
sitionality. In Proceedings of the Workshop on Con-
tinuous Vector Space Models and their Composition-
ality, pages 119–126, Sofia, Bulgaria, August. Asso-
ciation for Computational Linguistics.

Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple Semi-supervised Dependency Pars-
ing. In Proceedings of ACL-HLT, pages 595–603,
Columbus, Ohio, June. Association for Computa-
tional Linguistics.

Ralf Krestel, Sabine Bergler, and René Witte. 2008.
Minding the Source: Automatic Tagging of Re-
ported Speech in Newspaper Articles. In LREC,
Marrakech, Morocco, May. European Language Re-
sources Association (ELRA).

Quoc V. Le, Jiquan Ngiam, Adam Coates, Abhik
Lahiri, Bobby Prochnow, and Andrew Y. Ng. 2011.
On Optimization Methods for Deep Learning. In
ICML.

Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng. 2009.
Recognizing Implicit Discourse Relations in the
Penn Discourse Treebank. In EMNLP.

Annie Louis, Aravind Joshi, and Ani Nenkova. 2010.
Discourse indicators for content selection in summa-
rization. In Proceedings of the 11th Annual Meeting
of the Special Interest Group on Discourse and Di-
alogue, pages 147–156. Association for Computa-
tional Linguistics.

22



Daniel Marcu and Abdessamad Echihabi. 2002. An
Unsupervised Approach to Recognizing Discourse
Relations. In Proceedings of ACL, pages 368–375,
Philadelphia, Pennsylvania, USA, July. Association
for Computational Linguistics.

Daniel Marcu. 1996. Building Up Rhetorical Structure
Trees. In Proceedings of AAAI.

Daniel Marcu. 1999. A Decision-Based Approach to
Rhetorical Parsing. In Proceedings of ACL, pages
365–372, College Park, Maryland, USA, June. As-
sociation for Computational Linguistics.

Daniel Marcu. 2000a. The Rhetorical Parsing of Un-
restricted Texts: A Surface-based Approach. Com-
putational Linguistics, 26:395–448.

Daniel Marcu. 2000b. The Theory and Practice of Dis-
course Parsing and Summarization. MIT Press.

Scott Miller, Jethran Guinness, and Alex Zamanian.
2004. Name Tagging with Word Clusters and Dis-
criminative Training. In Daniel Marcu Susan Du-
mais and Salim Roukos, editors, HLT-NAACL, pages
337–342, Boston, Massachusetts, USA, May 2 -
May 7. Association for Computational Linguistics.

Eleni Miltsakaki and Karen Kukich. 2004. Evaluation
of text coherence for electronic essay scoring sys-
tems. Natural Language Engineering, 10(1):25–55.

Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388–1429.

Philippe Muller, Stergos Afantenos, Pascal Denis, and
Nicholas Asher. 2012. Constrained Decoding for
Text-Level Discourse Parsing. In Coling, pages
1883–1900, Mumbai, India, December. The COL-
ING 2012 Organizing Committee.

Anil Kumar Nelakanti, Cedric Archambeau, Julien
Mairal, Francis Bach, and Guillaume Bouchard.
2013. Structured Penalties for Log-Linear Lan-
guage Models. In EMNLP, pages 233–243, Seattle,
Washington, USA, October. Association for Com-
putational Linguistics.

Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, Gülsen Eryigit, Sandra Kübler, Svetoslav
Marinov, and Erwin Marsi. 2007. MaltParser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13(2):95–135.

Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The penn discourse treebank 2.0. In
LREC.

Rashmi Prasad, Aravind Joshi, and Bonnie Webber.
2010. Realization of discourse relations by other
means: alternative lexicalizations. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics: Posters, pages 1023–1031. Asso-
ciation for Computational Linguistics.

Kenji Sagae. 2009. Analysis of Discourse Structure
with Syntactic Dependencies and Data-Driven Shift-
Reduce Parsing. In Proceedings of the 11th Interna-
tional Conference on Parsing Technologies (IWPT),
pages 81–84, Paris, France, October. Association for
Computational Linguistics.

Frank Schilder. 2002. Robust discourse parsing via
discourse markers, topicality and position. Natural
Language Engineering, 8(3):235–255.

Richard Socher, Eric H. Huang, Jeffrey Pennington,
Andrew Y. Ng, and Christopher D. Manning. 2011.
Dynamic Pooling and Unfolding Recursive Autoen-
coders for Paraphrase Detection. In NIPS.

Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic Composi-
tionality Through Recursive Matrix-Vector Spaces.
In EMNLP.

Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng,
and Christopher Potts. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP).

Swapna Somasundaran, Galileo Namata, Janyce
Wiebe, and Lise Getoor. 2009. Supervised and
unsupervised methods in employing discourse rela-
tions for improving opinion polarity classification.
In Proceedings of EMNLP.

Radu Soricut and Daniel Marcu. 2003. Sentence Level
Discourse Parsing using Syntactic and Lexical Infor-
mation. In NAACL.

Rajen Subba and Barbara Di Eugenio. 2009. An effec-
tive Discourse Parser that uses Rich Linguistic In-
formation. In NAACL-HLT, pages 566–574, Boul-
der, Colorado, June. Association for Computational
Linguistics.

K. Sumita, K. Ono, T. Chino, T. Ukita, and S. Amano.
1992. A discourse structure analyzer for Japanese
text. In Proceedings International Conference on
Fifth Generation Computer Systems, pages 1133–
1140.

Maite Taboada and William C Mann. 2006. Applica-
tions of rhetorical structure theory. Discourse stud-
ies, 8(4):567–588.

Benjamin Taskar, Carlos Guestrin, and Daphne Koller.
2003. Max-margin markov networks. In NIPS.

Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010. Word Representation: A Simple and General
Method for Semi-Supervised Learning. In Proceed-
ings of ACL, pages 384–394.

Tim Van de Cruys and Marianna Apidianaki. 2011.
Latent Semantic Word Sense Induction and Disam-
biguation. In Proceedings of ACL, pages 1476–
1485, Portland, Oregon, USA, June. Association for
Computational Linguistics.

23



Laurens van der Maaten and Geoffrey Hinton. 2008.
Visualizing Data using t-SNE. Journal of Machine
Learning Research, 9:2759–2605, November.

Kimberly Voll and Maite Taboada. 2007. Not all
words are created equal: Extracting semantic orien-
tation as a function of adjective relevance. In Pro-
ceedings of Australian Conference on Artificial In-
telligence.

Ngo Xuan Bach, Nguyen Le Minh, and Akira Shimazu.
2012. A Reranking Model for Discourse Segmenta-
tion using Subtree Features. In Proceedings of the
13th Annual Meeting of the Special Interest Group
on Discourse and Dialogue, pages 160–168.

Chun-Nam John Yu and Thorsten Joachims. 2009.
Learning structural SVMs with latent variables. In
Proceedings of the 26th Annual International Con-
ference on Machine Learning, pages 1169–1176.
ACM.

24


