



















































Controlling Linguistic Style Aspects in Neural Language Generation


Proceedings of the Workshop on Stylistic Variation, pages 94–104
Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics

Controlling Linguistic Style Aspects in Neural Language Generation

Jessica Ficler and Yoav Goldberg
Computer Science Department

Bar-Ilan University
Israel

{jessica.ficler, yoav.goldberg}@gmail.com

Abstract

Most work on neural natural language
generation (NNLG) focus on controlling
the content of the generated text. We ex-
periment with controlling several stylistic
aspects of the generated text, in addition
to its content. The method is based on
conditioned RNN language model, where
the desired content as well as the stylis-
tic parameters serve as conditioning con-
texts. We demonstrate the approach on
the movie reviews domain and show that
it is successful in generating coherent sen-
tences corresponding to the required lin-
guistic style and content.

1 Introduction

The same message (e.g. expressing a positive sen-
timent towards the plot of a movie) can be con-
veyed in different ways. It can be long or short,
written in a professional or colloquial style, writ-
ten in a personal or impersonal voice, and can
make use of many adjectives or only few.

Consider for example the following to sen-
tences:

(1) “A genuinely unique, full-on sensory experi-
ence that treads its own path between narrative
clarity and pure visual expression.”
(2) “OMG... This movie actually made me cry a
little bit because I laughed so hard at some parts
lol.”

They are both of medium length, but the first
appears to be written by a professional critic, and
uses impersonal voice and many adjectives; while
the second is written in a colloquial style, using a
personal voice and few adjectives.

In a text generation system, it is desirable to
have control over such stylistic aspects of the

text: style variations are used to express the so-
cial meanings of a message, and controlling the
style of a text is necessary for appropriately con-
veying a message in a way that is adequate to the
social context (Biber and Conrad, 2009; Nieder-
hoffer and Pennebaker, 2002). This work focuses
on generating text while allowing control of its
stylistic properties.

The recent introduction of recurrent neural lan-
guage models and recurrent sequence-to-sequence
architectures to NLP brought with it a surge of
work on natural language generation. Most of
these research efforts focus on controlling the con-
tent of the generated text (Lipton et al., 2015; Kid-
don et al., 2016; Lebret et al., 2016; Kiddon et al.,
2016; Tang et al., 2016; Radford et al., 2017),
while a few model more stylistic aspects of the
generated text such as the identity of the speaker in
a dialog setting (Li et al., 2016); the politeness of
the generated text or the text length in a machine-
translation setting (Sennrich et al., 2016; Kikuchi
et al., 2016); or the tense in generated movie re-
views (Hu et al., 2017). Each of these works tar-
gets a single, focused stylistic aspect of the text.
Can we achieve finer-grained control over the gen-
erated outcome, controlling several stylistic as-
pects simultaneously?

We explore a simple neural natural-language
generation (NNLG) framework that allows for
high-level control on the generated content (sim-
ilar to previous work) as well as control over mul-
tiple stylistic properties of the generated text. We
show that we can indeed achieve control over each
of the individual properties.

As most recent efforts, our model (section 3) is
based on a conditioned language model, in which
the generated text is conditioned on a context vec-
tor.1 In our case, the context vector encodes a set

1 See (Hoang et al., 2016) for other conditioning models.

94



of desired properties that we want to be present in
the generated text.2 At training time, we work in
a fully supervised setup, in which each sentence is
labeled with a set of linguistic properties we want
to condition on. These are encoded into the con-
text vector, and the model is trained to generate the
sentence based on them. At test time, we can set
the values of the individual properties to get the
desired response. As we show in section 6.3, the
model generalizes fairly well, allowing the gener-
ation of text with property combinations that were
not seen during training.

The main challenge we face is thus obtaining
the needed annotations for training time. In sec-
tion 4 we show how such annotations can be ob-
tained from meta-data or using specialized text-
based heuristics.

Recent work (Hu et al., 2017) tackles a sim-
ilar goal to ours. They propose a novel gener-
ative model combining variational auto-encoders
and holistic attribute discriminators, in order to
achieve individual control on different aspects of
the generated text. Their experiments condition on
two aspects of the text (sentiment and tense), and
train and evaluate on sentences of up to 16 words.
In contrast, we propose a much simpler model
and focus on its application in a realistic setting:
we use all naturally occurring sentence lengths,
and generate text according to two content-based
parameters (sentiment score and topic) and four
stylistic parameters (the length of the text, whether
it is descriptive, whether it is written in a personal
voice, and whether it is written in professional
style). Our model is based on a well-established
technology - conditioned language models that
are based on Long Short-Term Memory (LSTM),
which was proven as strong and effective sequence
model.

We perform an extensive evaluation, and verify
that the model indeed learns to associate the dif-
ferent parameters with the correct aspects of the
text, and is in many cases able to generate sen-
tences that correspond to the requested parame-
ter values. We also show that conditioning on the
given properties in a conditioned language model
indeed achieve better perplexity scores compared
to an unconditioned language model trained on the
entire dataset, and also compared to unconditioned
models that are trained on subsets of the data that

2Another view is that of an encoder-decoder model, in
which the encoder component encodes the set of desired
properties.

correspond to a particular conditioning set. Fi-
nally, we show that the model is able to gener-
alize, i.e., to generate sentences for combinations
that were not observed in training.

2 Task Description and Definition

Our goal is to generate natural language text that
conforms to a set of content-based and stylistic
properties. The generated text should convey the
information requested by the content properties,
while conforming to the style requirements posed
by the style properties.

For example in the movie reviews domain,
theme is a content parameter indicating the top-
ical aspect which the review refers to (i.e. the
plot, the acting, and so on); and descriptive
is a style parameter that indicates whether the re-
view text uses many adjectives. The sentence
“A wholly original, well-acted, romantic comedy
that’s elevated by the modest talents of a lesser
known cast.” corresponds to theme:acting
and descriptive:true, as it includes many
descriptions and refers to the acting, while the sen-
tence “In the end, there are some holes in the story,
but it’s an exciting and tender film.” corresponds to
theme:plot and descriptive:false.

More formally, we assume a set of k parameters
{p1, . . . , pk}, each parameter pi with a set of pos-
sible values {v1, . . . , vpi}. Then, given a specific
assignment to these values our goal is to generate a
text that is compatible with the parameters values.
Table 1 lists the full set of parameters and values
we consider in this work, all in the movie reviews
domain. In section 4 we discuss in detail the dif-
ferent parameters and how we obtain their values
for the texts in our reviews corpus.

To give a taste of the complete task, we provide
two examples of possible value assignments and
sentences corresponding to them:

Type Parameter Value (1) Value (2)
Content Theme Acting Other
Content Sentiment Positive Negative
Style Professional True False
Style Personal False True
Style Length 21-40 words 11-20 words
Style Descriptive False True

Sentences for value set 1:
• “This movie is excellent, the actors aren’t all over

the place ,but the movie has a lot of fun, exploring
the lesson in a way that they can hold their own
lives.”

95



Parameter Description Source Possible values Examples
St

yl
e

Professional
Whether the review is written in
the style of a professional critic
or not

meta-data
False “So glad to see this movie !!”

True “This is a breath of fresh air, it’s a welcome re-
turn to the franchise’s brand of satirical humor.”

Personal
Whether the review describes
subjective experience (written in
personal voice) or not

content
derived

False “Very similar to the book.”

True “I could see the movie again, “The Kid With Me”
is a very good film.”

Length Number of words content
derived

≤ 10 words / 11-20 words / 21-40 words / > 40 words

Descriptive Whether the review is in descrip-
tive style or not

content
derived

True “Such a hilarious and funny romantic comedy.”

False “A definite must see for fans of anime fans, pop
culture references and animation with a good
laugh too.”

C
on

te
nt

Sentiment The score that the reviewer gave
the movie

meta-data

Positive “In other words: “The Four” is so much to keep
you on the edge of your seat.”

Neutral “While the film doesn’t quite reach the level of
sugar fluctuations, it’s beautifully animated.”

Negative “At its core ,it’s a very low-budget movie that just
seems to be a bunch of fluff.”

Theme
Whether the sentence’s con-
tent is about the Plot, Acting,
Production, Effects or none of
these (Other)

content
derived

Plot “The characters were great and the storyline had
me laughing out loud at the beginning of the
movie.”

Acting “The only saving grace is that the rest of the cast
are all excellent and the pacing is absolutely flaw-
less.”

Production “If you’re a Yorkshire fan, you won’t be disap-
pointed, and the director’s magical.”

Effects “Only saving grace is the sound effects.”

Other “I’m afraid that the movie is aimed at kids and
adults weren’t sure what to say about it.”

Table 1: Parameters and possible values in the movie-reviews domain.

• “It’s a realistic and deeply committed perfor-
mance from the opening shot, the movie gives an
excellent showcase for the final act, and the visu-
als are bold and daring.”

Sentences for value set 2:
• “My biggest gripe is that the whole movie is

pretty absurd and I thought it was a little too pre-
dictable.”
• “The first half is pretty good and I was hoping for

a few funny moments but not funny at all.”

3 Conditioned Language Model

Like in previous neural language-generation work
(Lipton et al., 2015; Tang et al., 2016), our model
is also a conditioned language model. In a regu-
lar language model (LM), each token wt is condi-
tioned on the previous tokens, and the probability
of a sentence w1, ..., wn is given by:

P (w1, ..., wn) = Πnt=1P (wt|w1, . . . wt−1) (1)

In a conditioned language model, we add an addi-
tional conditioning context, c:

P (w1, ..., wn|c) = Πnt=1P (wt|w1, . . . wt−1, c)
(2)

Each token in the sentence is conditioned on the
previous ones, as well the additional context c.

A conditioned language model can be imple-
mented using an recurrent neural network lan-
guage model (RNN-LM, (Mikolov et al., 2010)),
where the context c is a vector that is concatenated
to the input vector at each time step.

Conditioned language models were shown to be
effective for natural language generation. We dif-
fer from previous work by the choice of condition-
ing contexts, and by conditioning on many param-
eters simultaneously.

In our case, the condition vector c encodes the
desired textual properties. Each parameter value
is associated with an embedding vector, and c is
a concatenation of these embedding vectors. The
vector c is fed into the RNN at each step, concate-

96



nated to the previous word in the sequence.

Technical Details We use an LSTM-based
language model (Hochreiter and Schmidhuber,
1997), and encode the vocabulary using Byte Pair
Encoding (BPE), which allows representation of
an open vocabulary through a fixed-size vocabu-
lary by splitting rare words into subword units,
providing a convenient way of dealing with rare
words. Further details regarding layer sizes, train-
ing regime, vocabulary size and so on are provided
in the supplementary material.

4 Data-set Collection and Annotation

For training the model, we need a dataset of re-
view texts, each annotated with a value assign-
ment to each of the style and the content parame-
ters. We obtain these values from two sources: (1)
We derive it from meta-data associated with the
review, when available. (2) We extract it from the
review text using a heuristic. We use three kinds
of heuristics: based on lists of content-words;
based on the existence of certain function words;
and based on the distribution on part-of-speech
tags. These annotations may contain noise, and
indeed some of our heuristics are not very tight.
We demonstrate that we can achieve good perfor-
mance despite the noise. Naturally, improving the
heuristics is likely to results in improved perfor-
mance.

Our reviews corpus is based on the Rotten-
Tomatoes website.3 We collected 1,002,625
movie reviews for 7,500 movies and split
them into sentences. Each sentence is then
annotated according to four style parameters
(professional, personal, descriptive
and length) and two content parameters
(sentiment and theme). The meanings of
these properties and how we obtain values for
them are described below.

4.1 Annotations Based on Meta-data
Professional indicates whether the review is
written in a professional (true) or a collo-
quial (false) style. We label sentences as
professional:true if it is written by either
(1) a reviewer that is a professional critic; (2) a re-
viewer that is marked as a “super-reviewer” on the
RottenTomatoes website (a title given to reviewers
who write high-quality reviews). Other sentences
are labeled as professional:false.

3http://www.rottentomatoes.com

Sentiment reflects the grade that was given by
the review writer. Possible values for grade are:
positive, neutral, negative or none. In
audience reviews the movies are rated by the re-
viewer on a scale of 0 to 5 stars. In critic re-
views, the score was taken from the original re-
view (which is external to the rotten-tomatoes
website). We normalized the critics scores to be
on 0-5 scale. We then consider reviews with grade
0-2 as negative, 3 as neutral and 4-5 as
positive. Cases where no score information
was available are labeled as none.4

4.2 Annotations Derived from Text

Length We count the number of tokens in the
sentence and associate each sentence to one of
four bins: ≤10, 11-20, 21-40, >40.
Personal whether the sentence is written in
a personal voice, indicating a subjective point
of view (“I thought it was a good movie.”,
“Just not my cup of tea.”) or not (“Overall,
it is definitely worth watching.”, “The movie
doesn’t bring anything new.”), We label sentences
that include the personal pronoun or possessive
(“I”, “my”) as personal:true and others as
personal:false.

Theme the aspect of the movie that the sentence
refers to. The possible values are plot, acting,
production and effects. We assign a cate-
gory to a sentence using word lists. We went over
the frequent words in the corpus, and looked for
words that we believe are indicative of the dif-
ferent aspects (i.e., for plot this includes words
such as sciprt, story, subplots. The complete word
lists are available in the supplementary material).
Each sentence was labeled with the category that
has the most words in the sentence. Sentences that
do not include any words from our lists are labeled
as other.

Descriptive whether the sentence is descriptive
(“A warm and sweet, funny movie.”) or not (“It’s
one of the worst movies of the year, but it’s not a
total waste of time.”), Our (somewhat simplistic)
heuristic is based on the premise that descriptive
texts make heavy use of adjectives. We labeled
a sentence as descriptive:true if at least

4Note that while the sentiment scores are assigned to a
complete review, we associate them here with individual sen-
tences. This is a deficiency in the heuristic, which may ex-
plain some of the failures observed in section 6.1.

97



(a
)P

ro
fe

ss
io

na
l False

91.9%

True
8.1%

(b
)P

er
so

na
l

False
80%

True
20%

(c
)D

es
cr

ip
tiv

e

False
99%

True
1%

(d
)S

en
tim

en
t

Negative
28.5%Neutral

26%

Positive
40.2%

None
5.3%

(e
)T

he
m

e

Plot
14.9%

Acting
7.6%

Other
74%

Effects
2.5%

Production
1%

(f
)L

en
gt

h

≤ 10
33%

11-20
32.2%

21-40
27.4%

>40
7.4%

Figure 1: Movie reviews data-set statistics.

35% of its part-of-speech sequence tags are adjec-
tives (JJ). All other sentences were considered as
non-descriptive.

4.3 Dataset Statistics
Our final data-set includes 2,773,435 sentences
where each sentence is labeled with the 6 param-
eters. We randomly divided the data-set to train-
ing (#2,769,138), development (#2,139) and test
(#2,158) sets. Figure 1 shows the distribution of
the different properties in the dataset.

5 Evaluating Language Model Quality

In our first set of experiments, we measure the
quality of the conditioned language model in terms
of test-set perplexity.

5.1 Conditioned vs. Unconditioned
Our model is a language model that is conditioned
on various parameters. As a sanity check, we ver-
ify that knowing the parameters indeed helps in
achieving better language modeling results. We
compare the dev-set and test-set perplexities of our
conditioned language model to an unconditioned
(regular) language model trained on the same data.
The results, summarized in the following table,
show that knowing the correct parameter values
indeed results in better perplexity.

dev test
Not-conditioned 25.8 24.4
Conditioned 24.8 23.3

Table 2: Conditioned and not-conditioned lan-
guage model perplexities on the development and
test sets.

5.2 Conditioned vs. Dedicated LMs

A second natural baseline to the conditioned LM
is to train a separate unconditioned LM on a
subset of the data. For example, if we are in-
terested in generating sentences with the prop-
erties personal:false, sentiment:pos,
professional:false, theme:other and
length:≤10, we will train a dedicated LM on
just the sentences that fit these characteristics.

We hypothesize that the conditioned LM trained
on all the data will be more effective than a ded-
icated LM, as it will be able to generalize across
properties-combinations, and share data between
the different settings. In this set of experiment, we
verify this hypothesis.

For a set of parameters and values
{p1, p2, · · · pn}, we train n sub-models where
each sub-model mi is trained on the subset of
sentences that match parameters {p1, p2, · · · pi}.
For example, given the set of parameters val-
ues as above, we train 5 sub-models: the first
on data with personal:false only, the
second on data with persoal:false and
sentiment:positive, etc. As we add
parameters, the size of the training set of the
sub-model decreases.

For each dedicated sub-model, we measure its
perplexity on the test-set sentences that match the
criteria, and compare it to a conditioned LM with
these criteria, and to an unconditioned language
model. We do this for 4 different parameter-sets.
Figure 2 presents the results.

The results indicate that when only few condi-
tioning parameters are needed, and if the coverage
of the parameter combination in the training set
is large enough, the dedicated LM approach in-

98



Personal
False

Sentiment
Positive

Professional
False

Theme
None

Length
≤10

17

18

19

20

21

22

23

24

25

26

27

2215K

886K

821K

612K

300K

Parameters (incremental)

Dedicated LM
General LM

Conditioned LM

Professional
False

Length
11-20

Sentiment
Positive

Personal
True

11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26

327K

81K

2,544K 822K

Parameters (incremental)

Dedicated LM
General LM

Conditioned LM

Professional
True

Descriptive
True

Theme
Plot

24
25
26
27
28
29
30
31
32
33
34
35

224K
223K

33K

Parameters (incremental)

Dedicated LM
General LM

Conditioned LM

Theme
Acting

Length
21-40

21

22

23

24

25

26

27

28

209K

79K

Parameters (incremental)

Dedicated LM
General LM

Conditioned LM

(a) (b) (c) (d)

Figure 2: Perplexities of conditioned, unconditioned and dedicated language models for various param-
eter combinations. The numbers on the dedicated-model line indicates the number of sentences that the
sub-model was trained on.

deed outperforms the conditioned LM. This is the
case in the first three sub-models in 2a, and the
first two sub-models in 2c. With few condition-
ing criteria, the dedicated LM approach is effec-
tive. However, it is not scalable. As we increase
the number of conditioning factors, the amount
of available training data to the dedicated model
drops, and so does the modeling quality. In con-
trast, the conditioned model manages to generalize
from sentences with different sets of properties,
and is effective also with large number of condi-
tioning factors. We thus conclude that for our use
case, in which we need to condition on many dif-
ferent aspects of the generated sentence, the condi-
tioned LM is far more suitable than the dedicated
LM.

5.3 Conditioned vs. Flipped Conditioning
The previous experiments show that a condi-
tioned model outperforms an unconditioned one.
Here, we focus on the effect of the individual
conditioning parameters. We compare the per-
plexity when using the correct conditioning val-
ues to the perplexity achieved when flipping the
parameter value to an incorrect one. We do
that for parameters that have opposing values:
personal, professional, sentiment and
descriptive. The following table summarizes
the results:

Correct Value 23.3
Replacing Descriptive with non-Descriptive 27.2
Replacing Personal 27.5
Replacing Professional 25
Replacing Sentiment Pos with Neg 24.3

Table 3: Test-set perplexities when supplying the
correct parameter values and when supplying the
opposite values.

There is a substantial drop in quality (increase
in perplexity) when flipping the parameter values.
The drop is smallest for sentiment, and largest for
descriptiveness and personal voice. We conclude
that the model distinguishes descriptive text and
personal voice better than it distinguishes senti-
ment and professional text.

6 Evaluating the Generated Sentences

In section 5.3 we verified the effectiveness of the
conditioned model by showing that flipping a con-
ditioning parameter value results in worse perplex-
ity. However, we still need to verify that the model
indeed associates each parameter with the correct
behavior. In this set of experiments, we use the
model to generate random sentences with differ-
ent conditioning properties, and measure how well
they match the requested behavior.

We generated 3,285 sentences according to
the following protocol: for each property-
combination attested in the development set, we
generated 1,000 random sentences conditioned on
these properties. We then sorted the generated sen-
tences according to their probability, and chose the
top k = (cf/mf ) ∗ 100 sentences, where cf is the
frequency of the property-combination in the dev
set and mf is the frequency of the most frequent
property-combination in the dev set.

This process resulted in 3,285 high-scoring but
diverse sentences, with properties that are dis-
tributed according to the properties distribution in
the development set.

6.1 Capturing Individual Properties

Length We measure the average, minimum and
maximum lengths, and deviation of the sentences
that were generated for a requested length value.
The following table summarizes the results:

99



Requested Length Avg Min Max Deviationm=2
<=10 7.6 1 21 0.2 %
11-20 20.6 5 25 2.6 %
21-40 34 7 49 0.6 %

Table 4: Average, minimum and maximum
lengths of the sentences generated according to the
correspond length value; as well as deviation per-
centage with margin (m) of 2.

The average length fits the required range for
each of the values and the percentage of sentences
that exceed the limits with margin 2 is between
0.2% to 2.6%.

Descriptive We measure the percentage of
sentences that are considered as descriptive
(containing >35% adjectives) when requesting
descriptive:true, and when requesting
descriptive:false. When requesting de-
scriptive text, 85.7% of the generated sentences fit
the descriptiveness criteria. When requesting non-
descriptive text, 96% of the generated sentences
are non-descriptive according to our criteria.

Personal Voice We measure the percentage of
sentences that are considered as personal voice
(containing the pronouns I or my) when re-
questing personal:true, and when request-
ing personal:false. 100% of the sentence
for which we requested personal voice were in-
deed in personal voice. When requesting non-
personal text, 99.85% of the sentences are indeed
non-personal.

Theme For each of the possible theme values,
we compute the proportion of the sentences that
were generated with the corresponding value. The
confusion matrix in the following table

shows that the vast majority of sentences are
generated according to the requested theme.

Requested value % Plot % Acting % Prod % Effects % Other
Plot 98.7 0.8 0 0.2 0.3
Acting 2.5 95.3 0 0.6 1.6
Production 0 0 97.4 2.6 0
Effects 0 5.9 0 91.7 2.4
Other 0.04 0.03 0 0.03 99.9

Table 5: Percentage of generated sentences from
each theme, when requesting a given theme value.

Professional The professional property of
the generated sentences could not be evaluated au-

tomatically, and we thus performed manual eval-
uation using Mechanical Turk. We randomly cre-
ated 1000 sentence-pairs where one is generated
with professional:true and the other with
professional:false (the rest of the prop-
erty values were chosen randomly). For example
in the following sentence-pair the first is gener-
ated with professional:true and the sec-
ond with professional:false:

(t) “This film has a certain sense of imagination
and a sobering look at the clandestine indictment.”
(f) “I know it’s a little bit too long, but it’s a great
movie to watch !!!!”

The annotators were asked to determine which of
the sentences was written by a professional critic.
Each of the pairs was annotated by 5 different an-
notators. When taking a majority vote among the
annotators, they were able to tell apart the profes-
sional from non-professional sentences generated
sentences in 72.1% of the cases.

When examining the cases where the annotators
failed to recognise the desired writing style, we
saw that in a few cases the sentence that was gener-
ated for professional:true was indeed not
professional enough (e.g. “Looking forward to the
trailer.”, and that in many cases, both sentences
could indeed be considered as either professional
or not, as in the following examples:

(t) “This is a cute movie with some funny moments,
and some of the jokes are funny and entertaining.”
(f) “Absolutely amazing story of bravery and ded-
ication.”

(t) “A good film for those who have no idea what’s
going on, but it’s a fun adventure.”
(f) “An insult to the audience’s intelligence.”

Sentiment To measure sentiment genera-
tion quality, we again perform manual an-
notations using Mechanical Turk. We ran-
domly created 300 pairs of generated sen-
tences for each of the following settings:
positive/negative, positive/neutral
and negative/neutral. The annotators were
asked to mark which of the reviewers liked the
movie more than the other. Each of the pairs
was annotated by 5 different annotators and we
choose by a majority vote. The annotators cor-
rectly identified 86.3% of the sentence in the Pos-
itive/Negative case, 63% of the sentences in the
Positive/Neutral case, and 69.7% of the sentences

100



in the negative/neutral case.
Below are some examples for cases where the

annotators failed to recognize the intended senti-
ment:

(Pos) “It’s a shame that this film is not as good as
the previous film, but it still delivers.”
(Neg) “The premise is great, the acting is not bad,
but the special effects are so bad.”

(Pos) “The story line is a bit predictable but it’s a
nice one, sweet and hilarious in its own right.”
(Neg) “It’s a welcome return to form an episode
of Snow White, and it turns in a great way.”

6.2 Examples of Generated Sentences

All of the examples throughout the paper were
generated by the conditioned LM. Additional ex-
amples are available in the supplementary mate-
rial.

6.3 Generalization Ability

Finally, we test the ability of the model to gen-
eralize: can it generate sentences for parameter
combinations it has not seen in training? To this
end, we removed from the training set the 75,421
sentences which were labeled as theme:plot
and personal:true, and re-trained a condi-
tioned LM. The trained model did see 336,567 ex-
amples of theme:plot and 477,738 examples
of personal:true, but has never seen exam-
ples where both conditions hold together. We then
asked the trained model to generate sentences with
these parameter values. 100% of the generated
sentences indeed contained personal pronouns,
and 82.4% of them fit the theme:plot criteria
(in comparison, a conditioned model trained on all
the training data managed to fit the theme:plot
criteria in 97.8% of the cases). Some generated
sentence examples are:

“Some parts weren’t as good as I thought it would
be and the acting and script were amazing.”

“I had a few laughs and the plot was great, but the
movie was very predictable.”

“I really liked the story and the performances were
likable and the chemistry between the two leads is
great.”

“I’ve never been a fan of the story, but this movie
is a great film that is a solid performance from Brie
Larson and Jacob Tremblay.

7 Related Work
In neural-network based models for language
generation, most work focus on content that need
to be conveyed in the generated text. Similar
to our modeling approach, (Lipton et al., 2015;
Tang et al., 2016) generates reviews conditioned
on parameters such as category, and numeric rat-
ing scores. Some work in neural generation for
dialog (Wen et al., 2015; Dušek and Jurcicek,
2016b,a) condition on a dialog act (“request”,
“inform”) and a set of key,value pairs of infor-
mation to be conveyed (“price=low, food=italian,
near=citycenter”). The conditioning context is en-
coded either similarly to our approach, or by en-
coding the desired information as a string and
using sequence-to-seqeunce modeling with atten-
tion. Mei et al. (2016) condition the content on
a set of key,value pairs using an encoder-decoder
architecture with a coarse-to-fine attention mech-
anism. Kiddon et al. (2016) attempt to generate
a recipe given a list of ingredients that should be
mentioned in the text, tracking the ingredients that
were already mentioned to avoid repetitions. Le-
bret et al. (2016) condition on structured informa-
tion in Wikipedia infoboxes for generating textual
biographies. 5 These work attempt to control the
content of the generated text, but not its style.

In other works, the conditioning context corre-
spond to a specific writer or a group of writers.
In generation of conversational dialog, Li et al.
(2016) condition the text on the speaker’s identity.
While the conditioning is meant for improving the
factual consistency of the utterances (i.e., keeping
track of age, gender, location), it can be consid-
ered as conditioning on stylistic factors (capturing
personal style and dialect). A recent work that ex-
plicitly controls the style of the generated text was
introduced by Sennrich et al. (2016) in the context
of Machine Translation. Their model translates
English to German with a feature that encodes
whether the generated text (in German) should ex-
press politeness. All these works, with the excep-
tion of Sennrich et al condition on parameters that
were extracted from meta-data or some database,
while Sennrich et al heuristically extracts the po-
liteness information from the training data. Our

5Recent work by Radford et al. (2017) trained an uncondi-
tioned LSTM language model on movie reviews, and found
in a post-hoc analysis a single hidden-layer dimension that
allows controling the sentiment of the generated reviews by
fixing its value. While intriguuing, it is not a reliable method
of deriving controllable generation models.

101



work is similar to the approach of Sennrich et al
but extends it by departing from machine transla-
tion, conditioning on numerous stylistic aspects of
the generated text, and incorporating both meta-
data and heuristically derived properties.

The work of Hu et al. (2017) features a VAE
based method coupled with a discriminator net-
work that tackles the same problem as ours: condi-
tioning on multiple aspects of the generated text.
The Variational component allows for easy sam-
pling of examples from the resulting model, and
the discriminator network directs the training pro-
cess to associate the desired behavior with the con-
ditioning parameters. Compared to our work, the
VAE component is indeed a more elegant solution
to generating a diverse set of sentences. How-
ever, the approach does not seem to be scalable:
Hu et al. (2017) restrict themselves to sentences
of up to length 16, and only two conditioning as-
pects (sentiment and tense). We demonstrate that
our conditioned LSTM-LM appraoch easily scales
to naturally-occuring sentence lengths, and allows
control of 6 individual aspects of the generated
text, without requiring a dedicated discriminator
network. The incorporation of a variational com-
ponent is an interesting avenue for future work.

In Pre-neural Text Generation The incorpo-
ration of stylistic aspects was discussed from
very early on (McDonald and Pustejovsky, 1985).
Some works tackling stylistic control of text pro-
duced in a rule-based generation system include
the works of Power et al. (2003); Reiter and
Williams (2010); Hovy (1987); Bateman and Paris
(1989) (see (Mairesse and Walker, 2011) for a
comprehensive review). Among these, the work
of Power et al. (2003), like ours, allows the user
to control various stylistic aspects of the gener-
ated text. This works by introducing soft and hard
constraints in a rule-based system. The work of
Mairesse and Walker (2011) introduce statistics
into the stylistic generation process, resulting in
a system that allows a user to specify 5 personality
traits that influence the generated language.

More recent statistical generation works tack-
ling style include Xu et al. (2012) who attempt
to paraphrase text into a different style. They
learn to paraphrase text in Shakespeare’s style to
modern English using MT techniques, relying on
the modern translations of William Shakespeare
plays. Abu Sheikha and Inkpen (2011) generate
texts with different formality levels by using lists

of formal and informal words.
Finally, our work relies on heuristically extract-
ing stylistic properties from text. Computational
modeling of stylistic properties has been the fo-
cus of several lines of study, i.e. (Pavlick and
Tetreault, 2016; Yang and Nenkova, 2014; Pavlick
and Nenkova, 2015). Such methods are natural
companions for our conditioned generation ap-
proach.

8 Conclusions
We proposed a framework for NNLG allowing for
relatively fine-grained control on different stylis-
tic aspects of the generated sentence, and demon-
strated its effectiveness with an initial case study
in the movie-reviews domain. A remaining chal-
lenge is providing finer-grained control on the
generated content (allowing the user to specify ei-
ther almost complete sentences or a set of struc-
tured facts) while still allowing the model to con-
trol the style of the generated sentence.

Acknowledgements The research was sup-
ported by the Israeli Science Foundation (grant
number 1555/15) and the German Research Foun-
dation via the German-Israeli Project Cooperation
(DIP, grant DA 1600/1-1).

References
Fadi Abu Sheikha and Diana Inkpen. 2011. Gener-

ation of formal and informal sentences. In Pro-
ceedings of the 13th European Workshop on Natu-
ral Language Generation. Association for Compu-
tational Linguistics, Nancy, France, pages 187–193.
http://www.aclweb.org/anthology/W11-2826.

John A Bateman and Cecile Paris. 1989. Phrasing a
text in terms the user can understand. In IJCAI.
pages 1511–1517.

Douglas Biber and Susan Conrad. 2009. Register,
genre, and style. Cambridge University Press.

Ondřej Dušek and Filip Jurcicek. 2016a. A context-
aware natural language generator for dialogue
systems. In Proceedings of the 17th Annual
Meeting of the Special Interest Group on Dis-
course and Dialogue. Association for Computa-
tional Linguistics, Los Angeles, pages 185–190.
http://www.aclweb.org/anthology/W16-3622.

Ondřej Dušek and Filip Jurcicek. 2016b. Sequence-
to-sequence generation for spoken dialogue via
deep syntax trees and strings. In Proceed-
ings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics (Volume

102



2: Short Papers). Association for Computa-
tional Linguistics, Berlin, Germany, pages 45–51.
http://anthology.aclweb.org/P16-2008.

Cong Duy Vu Hoang, Gholamreza Haffari, and Trevor
Cohn. 2016. Incorporating side information into re-
current neural network language models. In Pro-
ceedings of NAACL-HLT . pages 1250–1255.

Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long
short-term memory. MIT Press, volume 9, pages
1735–1780.

Eduard Hovy. 1987. Generating natural language un-
der pragmatic constraints. Journal of Pragmatics,
volume 11, pages 689–719.

Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan
Salakhutdinov, and Eric P Xing. 2017. Controllable
text generation. In Proc. of ICML.

Chloé Kiddon, Luke Zettlemoyer, and Yejin Choi.
2016. Globally coherent text generation with
neural checklist models. In Proceedings of the
2016 Conference on Empirical Methods in Natu-
ral Language Processing. Association for Compu-
tational Linguistics, Austin, Texas, pages 329–339.
https://aclweb.org/anthology/D16-1032.

Yuta Kikuchi, Graham Neubig, Ryohei Sasano, Hi-
roya Takamura, and Manabu Okumura. 2016.
Controlling output length in neural encoder-
decoders. In Proceedings of the 2016 Con-
ference on Empirical Methods in Natural Lan-
guage Processing. Association for Computational
Linguistics, Austin, Texas, pages 1328–1338.
https://aclweb.org/anthology/D16-1140.

Rémi Lebret, David Grangier, and Michael Auli. 2016.
Neural text generation from structured data with ap-
plication to the biography domain. In Proceed-
ings of the 2016 Conference on Empirical Meth-
ods in Natural Language Processing. Association
for Computational Linguistics, Austin, Texas, pages
1203–1213. https://aclweb.org/anthology/D16-
1128.

Jiwei Li, Michel Galley, Chris Brockett, Georgios Sp-
ithourakis, Jianfeng Gao, and Bill Dolan. 2016. A
persona-based neural conversation model. In Pro-
ceedings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics (Volume
1: Long Papers). Association for Computational
Linguistics, Berlin, Germany, pages 994–1003.
http://www.aclweb.org/anthology/P16-1094.

Zachary C Lipton, Sharad Vikram, and Julian
McAuley. 2015. Capturing meaning in product re-
views with character-level generative text models.
arXiv preprint arXiv:1511.03683.

François Mairesse and Marilyn A Walker. 2011. Con-
trolling user perceptions of linguistic style: Train-
able generation of personality traits. MIT Press, vol-
ume 37, pages 455–488.

David D McDonald and James D Pustejovsky. 1985. A
computational theory of prose style for natural lan-
guage generation. In Proceedings of the second con-
ference on European chapter of the Association for
Computational Linguistics. Association for Compu-
tational Linguistics, pages 187–193.

Hongyuan Mei, Mohit Bansal, and Matthew R. Wal-
ter. 2016. What to talk about and how? se-
lective generation using lstms with coarse-to-fine
alignment. In Proceedings of the 2016 Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies. Association for Computational
Linguistics, San Diego, California, pages 720–730.
http://www.aclweb.org/anthology/N16-1086.

Tomas Mikolov, Martin Karafiát, Lukas Burget, Jan
Cernockỳ, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In Inter-
speech. volume 2, page 3.

Kate G Niederhoffer and James W Pennebaker. 2002.
Linguistic style matching in social interaction. Jour-
nal of Language and Social Psychology, volume 21,
pages 337–360.

Ellie Pavlick and Ani Nenkova. 2015. Inducing lex-
ical style properties for paraphrase and genre dif-
ferentiation. In Proceedings of the 2015 Confer-
ence of the North American Chapter of the As-
sociation for Computational Linguistics: Human
Language Technologies. Association for Computa-
tional Linguistics, Denver, Colorado, pages 218–
224. http://www.aclweb.org/anthology/N15-1023.

Ellie Pavlick and Joel Tetreault. 2016. An empiri-
cal analysis of formality in online communication.
Transactions of the Association for Computational
Linguistics, volume 4, pages 61–74.

Richard Power, Donia Scott, and Nadjet Bouayad-
Agha. 2003. Generating texts with style. In Proc.
of CiCLING. Springer, pages 444–452.

Alec Radford, Rafal Jozefowicz, and Ilya Sutskever.
2017. Learning to generate reviews and discovering
sentiment. arXiv preprint arXiv:1704.01444.

Ehud Reiter and Sandra Williams. 2010. Generating
texts in different styles. In The Structure of Style,
Springer, pages 59–75.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Controlling politeness in neural machine
translation via side constraints. In Proceedings of
NAACL-HLT . pages 35–40.

Jian Tang, Yifan Yang, Sam Carton, Ming Zhang, and
Qiaozhu Mei. 2016. Context-aware natural lan-
guage generation with recurrent neural networks.
arXiv preprint arXiv:1611.09900.

Tsung-Hsien Wen, Milica Gasic, Nikola Mrkšić,
Pei-Hao Su, David Vandyke, and Steve Young.

103



2015. Semantically conditioned lstm-based nat-
ural language generation for spoken dialogue
systems. In Proceedings of the 2015 Con-
ference on Empirical Methods in Natural Lan-
guage Processing. Association for Computational
Linguistics, Lisbon, Portugal, pages 1711–1721.
http://aclweb.org/anthology/D15-1199.

Wei Xu, Alan Ritter, Bill Dolan, Ralph Grishman, and
Colin Cherry. 2012. Paraphrasing for style. In Pro-
ceedings of COLING 2012. The COLING 2012 Or-
ganizing Committee, Mumbai, India, pages 2899–
2914. http://www.aclweb.org/anthology/C12-1177.

Yinfei Yang and Ani Nenkova. 2014. Detecting
information-dense texts in multiple news domains.
In AAAI. pages 1650–1656.

104


