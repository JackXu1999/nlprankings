



















































Lexicalized Reordering for Left-to-Right Hierarchical Phrase-based Translation


Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 612–618,
Valencia, Spain, April 3-7, 2017. c©2017 Association for Computational Linguistics

Lexicalized Reordering for Left-to-Right Hierarchical Phrase-based
Translation

Maryam Siahbani∗
Department of Computer Information Systems

University of the Fraser Valley
Abbotsford BC, Canada

maryam.siahbani@ufv.ca

Anoop Sarkar
School of Computing Science

Simon Fraser University
Burnaby BC, Canada
anoop@cs.sfu.ca

Abstract

Phrase-based and hierarchical phrase-
based (Hiero) translation models differ
radically in the way reordering is modeled.
Lexicalized reordering models play an im-
portant role in phrase-based MT and such
models have been added to CKY-based de-
coders for Hiero. Watanabe et al. (2006)
propose a promising decoding algorithm
for Hiero (LR-Hiero) that visits input
spans in arbitrary order and produces the
translation in left to right (LR) order which
leads to far fewer language model calls
and leads to a considerable speedup in de-
coding. We introduce a novel shift-reduce
algorithm to LR-Hiero to decode with our
lexicalized reordering model (LRM) and
show that it improves translation quality
for Czech-English, Chinese-English and
German-English.

1 Introduction
Phrase-based machine translation handles reorder-
ing between source and target languages by visit-
ing phrases in the source in arbitrary order while
generating the target from left to right. A dis-
tortion penalty is used to penalize deviation from
the monotone translation (no reordering) (Koehn
et al., 2003; Och and Ney, 2004). Identical dis-
tortion penalties for different types of phrases ig-
nore the fact that certain phrases (with certain
words) were more likely to reorder than others.
State-of-the-art phrase based translation systems
address this issue by applying a lexicalized re-
ordering model (LRM) (Tillmann, 2004; Koehn et
al., 2007; Galley and Manning, 2008; Galley and
Manning, 2010) which uses word aligned data to
score phrase pair reordering. These models distin-
guish three orientations with respect to the previ-
ously translated phrase: monotone (M), swap (S),

∗This work was done while the first author was a Ph.D.
student at SFU.

and discontinuous (D), which are primarily de-
signed to handle local re-orderings of neighbour-
ing phrases.

Hierarchical phrase-based translation (Hiero)
(Chiang, 2007) uses hierarchical phrases for trans-
lations represented as lexicalized synchronous
context-free grammar (SCFG). Non-terminals in
the SCFG rules correspond to gaps in phrases
which are recursively filled by other rules
(phrases). The SCFG rules are extracted from
word and phrase alignments of a bitext. Hiero
uses CKY-style decoding which parses the source
sentence with time complexity O(n3) and syn-
chronously generates the target sentence (transla-
tion).

Watanabe et al. (2006) proposed a left-to-right
(LR) decoding algorithm for Hiero (LR-Hiero)
which follows the Earley (Earley, 1970) algorithm
to parse the source sentence and synchronously
generate the translation in a left-to-right manner.
This algorithm is combined with beam search
and has time complexity O(n2b) where n is the
length of source sentence and b is the size of
beam (Huang and Mi, 2010). LR-Hiero con-
strains the SCFG rules to be prefix-lexicalized
on the target side aka Greibach Normal Form
(GNF). Throughout this paper we abuse the no-
tation for simplicity and use the term GNF gram-
mars for such SCFGs. This leads to a single lan-
guage model (LM) history for each hypothesis and
speeds up decoding significantly, up to four times
faster (Siahbani et al., 2013).

The Hiero translation model handles reorder-
ing very differently from a phrase-based model,
through weighted translation rules (SCFGs) deter-
mined by non-terminal mappings. The rule X →
〈neX1 pas, do notX1〉 indicates the translation of
the phrase between ne and pas will be after the En-
glish phrase do not. However, reordering features
can also be added to the Hiero log-linear transla-
tion model. Siahbani et al. (2013) introduce a new
distortion feature to Hiero and LR-Hiero which

612



1)X→⟨他 补充 说 , X1/Headded that X1⟩
2)X→⟨联合 政府 X1/ the coalition government X 1⟩
3 )X→⟨目前 X1稳定 X 2/ is now i n stable X1 X 2⟩
4 )X→⟨状况 /condition⟩
5 )X→⟨ ./ .⟩

⟨ , ⟦[0,10]⟧ ,0⟩
⟨ He added that ,⟦[ 4,10]⟧ ,4.3⟩
⟨ He added that the coalition government ,⟦[6,10 ]⟧,7.7 ⟩
⟨ He added that the coalition government is now i n stable , ⟦[7,8] [9,10 ]⟧,11.2 ⟩
⟨ He added that the coalition government is now i n stable condition,⟦[9,10]⟧ ,13.4 ⟩
⟨ He added that the coalition government is now i n stable condition. ,⟦⟧ ,14.3⟩

rules hypotheses

<s>
<s>

<s>

<s>

<s>

<s>

</s>

⟨ht , hs , hc ⟩

Figure 1: The process of translating a Chinese (Fig. 2) sentence to English using LR-Hiero. Left side shows the rule used in
each step of creating the derivation. The hypotheses column shows 3-tuple partial hypotheses: the translation prefix, ht, the
ordered list of yet-to-be-covered spans, hs, and cost hc.

他 补充 说 , 联合 政府 目前

He added that the coalition government is now in stable condition

状况 稳定
 0      1              2         3    4               5                6                 7                 8                9      10

.

.

1)⟨他 补充 说 , /He added that ⟩
2)⟨联合 政府 / the coalition government ⟩
3 )⟨目前 稳定 / is now i n stable ⟩*__*

4 )⟨状况 /condition⟩
5 )⟨ ./ .⟩

Figure 2: A word-aligned Chinese-English sentence pair on
the top (from devset data used in experiments.) The source-
target phrase pairs created by removing the non-terminals
from the rules used in decoding (Fig. 1) are shown on the
bottom.

significantly improves translation quality in LR-
Hiero and improves Hiero results to a lesser extent.
Nguyen and Vogel (2013) integrate phrase-based
distortion and lexicalized reordering features with
CKY-based Hiero decoder which significantly im-
prove the translation quality. In their approach,
each partial hypothesis during decoding is mapped
into a sequence of phrase-pairs then the distor-
tion and reordering features are computed simi-
lar to phrase-based MT. They use a LRM trained
for phrase-based MT (Galley and Manning, 2010)
which applies some restrictions on the Hiero rules.
(Cao et al., 2014; Huck et al., 2013) propose dif-
ferent approaches to directly train LRM for Hiero
rules. However, these approaches are designed
for CKY-decoding and cannot be directly used
or adapted for LR-Hiero decoding which uses an
Earley-style parsing algorithm. The crucial differ-
ence is the nature of bottom-up versus left to right
decisions for lexicalized reordering and generat-
ing the translation in left-to-right manner. In this
paper, we introduce a novel shift-reduce algorithm
to learn a lexicalized reordering model (LRM) for
LR-Hiero. We show that augmenting LR-Hiero
with an LRM improves translation quality for
Czech-English, significantly improves results for
Chinese-English and German-English, while per-
forming three times fewer language model queries
on average, compared to CKY-Hiero.

2 Lexicalized Reordering for LR-Hiero
The main idea in phrase-based LRM is to divide
possible reorderings into three orientations that
can be easily determined during decoding and also
from word-aligned sentence pairs (parallel cor-
pus). Given a source sentence f, a sequence of
target language phrases e = (ē1, . . . , ēn) is gen-
erated by the decoder. A phrase alignment a=
(a1, . . . an) defines a source phrase f̄ai for each
target phrase ēi. For each phrase-pair 〈f̄ai , ei〉, the
orientations are described in terms of the previ-
ously translated source phrase f̄ai−1 :
Monotone (M): f̄ai immediately follows f̄ai−1 .
Swap (S): f̄ai−1 immediately follows f̄ai .
Discontinuous (D): f̄ai and f̄ai−1 are not adjacent
in the source sentence.
We only define the left-to-right case here; the
right-to-left case (f̄ai+1) is symmetrical. The prob-
ability of an orientation given a phrase pair 〈f̄ , ē〉
can be estimated using relative frequency:

P (o|f̄ , ē) = cnt(o, f̄ , ē)∑
o′∈{M,S,D} cnt(o′, f̄ , ē)

(1)

where, o ∈ {M,S,D} and cnt is computed on
word-aligned parallel data (count phrase-pairs and
their orientations). Given the sparsity of the ori-
entation types, we use smoothing. As the decoder
develops a new hypothesis by translating a source
phrase, f̄ai , it scores the orientation, oi wrt ai−1.
The log probability of the orientation is added
as a feature function to the log-linear translation
model.

LR-Hiero uses a subset of the Hiero SCFG rules
where the target rules are in Greibach Normal
Form (GNF): 〈γ, ē β〉 where γ is a string of non-
terminal and source words, ē is a target phrase and
β is a possibly empty sequence of non-terminals.
We abuse notation slightly and call this a GNF
SCFG grammar. In LR-Hiero each hypothesis
consists of a translation prefix, ht, an ordered se-
quence of untranslated spans on the source sen-

613



tence, hs and a numeric cost, hc. The initial hy-
pothesis consists of an empty translation (〈s〉), a
span of the whole source sentence and cost 0 (Fig-
ure 1). To develop a new hypothesis from a current
hypothesis, the LR-Hiero decoder applies a GNF
rule to the first untranslated span, hs[0], of old hy-
pothesis. The translation prefix of the new hypoth-
esis is generated by appending the target side of
the applied rule, ē, to the translation prefix of the
old hypothesis, ht. Corresponding to the applied
rule, the uncovered spans of the old hypothesis are
also updated and assigned to the new hypothesis
(Figure 1).

Target generation in LR-Hiero is analogous to
phrase-based MT. Given an input sentence f, the
output translation is a sequence of contiguous
target-language phrases e = (ē1, . . . , ēn) incre-
mentally concatenated during decoding. We can
define a phrase alignment a = (a1, . . . an) which
align each target phrase, ēi to a source phrase fai
corresponding to source side of a rule, ri used
at step i. But unlike target, source phrases can
be discontiguous. Figure 1 illustrates the process
of translating a Chinese-English sentence pair by
LR-Hiero. Corresponding to each rule a phrase
pair can be created (shown in Figure 2). The final
translation is the ordered sequence of target side
of these phrase pairs. Although the target genera-
tion is similar to phrase-based MT, the LR-Hiero
decoder parse the source sentence using the SCFG
rules and the order for translating source spans is
determined by the grammar. However the LR-
Hiero decoder uses an Earley-style parsing algo-
rithm and unlike CKY does not utilise translated
smaller spans to generate translations for bigger
spans bottom-up.

2.1 Training

We compute P (o|f̄ , ē), which is the probability of
an orientation given phrase pair of a rule, r.p =
〈f̄ , ē〉, on word-aligned data using relative fre-
quency. We assume that phrase ē spans the word
range s . . . t in the target sentence and the phrase
f̄ spans the range u . . . v in the source sentence.

For a given phrase pair 〈f̄ , ē〉, we set o = M if
there is a phrase pair,〈f̄ ′, ē′〉, where its target side,
ē′, appears just before the target side of the given
phrase, ē, or s = t′ + 1, and its source side, f̄ ′,
also appears just before f̄ , or u = v′+ 1. Orienta-
tion is S if there is a phrase pair, 〈f̄ ′, ē′〉, where ē′
appears just before ē, or s = t′+ 1, and f̄ ′ appears
just after f̄ , or v = u′−1. Otherwise orientation is

rules ri.f̄ Oi S

1) 〈0 1 2 3 4 X1/
under such circumstance X1〉

2) 〈5 X1/, X1〉
3) 〈6 X1 11/when X1〉
4) 〈7 8 X1/the right of life X1〉
5) 〈9 10/was deprived〉
6) 〈12 X1/, X1〉
7) 〈13 14 X1/it can only X1〉
8) 〈15 16X118/take violenceX1〉
9) 〈17/to〉

{−1}
{0, 1, 2, 3, 4}

{5}
{6, 11}
{7, 8}
{9, 10}
{12}
{13, 14}
{15, 16, 18}
{17}

M

M

M

D

M

M

M

M

D

[(-1)-(-1)]

[(-1)-4]

[(-1)-5]

[(-1)-11]

[(-1)-11]

[(-1)-11]

[(-1)-12]

[(-1)-14]

[(-1)-18]

[(-1)-18]

Figure 3: Computing correct orientation for each rule during
decoding in LR-Hiero for the example in Fig. 4. rules: the
rules used in the derivation. ri.f̄ : the position of rule’s lexical
terms in the source sentence; Oi: the identified orientation. S
is the recent translated source span (possibly discontinuous).
At each step Oi is identified by comparing ri.f̄ to S in the
previous step or last translated source phrase ri−1.f̄ .

X

X

X

X

X

在 这 种 情况下 , 当 生命 权 被 剥夺 时 , 只 能 采取暴力的手段undersuchcircumstance,whentherightoflifewasdeprived
,
it

only
can

of

violance
take

….

….

Figure 4: An example showing that the shift-reduce algo-
rithm can capture local reorderings like: the right of life and
was deprived.

D. We consider phrase pairs of any length to com-
pute orientation. Note that although phrase pairs
extracted from the rules that can be discontinu-
ous (on source), just continuous source phrases in
each sentence pair are used to compute orienta-
tion (previously translated phrases). Once orien-
tation counts for rules (phrase-pairs obtained form
rules) are collected from the bitext, the probability
model P (o|f̄ , ē) is estimated using recursive MAP
smoothing as discussed in (Cherry, 2013).

2.2 Decoding

Phrase-based LRM uses local information to de-
termine orientation for a new phrase pair, 〈f̄ai , ēi〉,
during decoding (Koehn et al., 2007; Tillmann,
2004). For left-to-right order, f̄ai is compared to
the previously translated phrase f̄ai−1 . Galley and
Manning (2008) introduce the hierarchical phrase

614



reordering model (HRM) which increases the con-
sistency of orientation assignments. In HRM, the
emphasis on the previously translated phrase is
removed and instead a compact representation of
the full translation history, as represent by a shift-
reduce stack, is used. Once a source span is trans-
lated, it is shifted onto the stack; if the two spans
on the top are adjacent, then a reduction merges
the two. During decoding, orientations are always
determined with respect to the top of this stack,
rather than the previously translated phrase.

Although we reduce rules to phrase pairs to
train the reordering model, LR-Hiero decoder
uses SCFG rules for translation and the order
of source phrases (spans) are determined by the
non-terminals in SCFG rules. Therefore we
cannot simply rely on the previously translated
phrase to compute the orientation and reorder-
ing scores. Since LR-Hiero uses lexicalized glue
rules (Watanabe et al., 2006), non-terminals can
be matched to very long spans on the source sen-
tence. It makes LRM in LR-Hiero comparable to
HRM in phrase-based MT. However, we cannot
rely on the full translation history like HRM, since
translation model is a SCFG grammar encoding
reordering information.

We employ a shift-reduce approach to find a
compact representation of the recent translated
source spans which is also represented by a stack,
S, for each hypothesis. However, S always con-
tains just one source span (which might be discon-
tiguous), unlike HRM which maintains all previ-
ously translated solid spans (In Figure 4, the dotted
lines shows the only span in the stack during LR-
Hiero decoding). As the decoder applies a rule, ri,
the corresponding source phrase ri.f̄ is compared
respect to the span in S to determine the orien-
tation. If they are adjacent or S covers the span
ri.f̄ , they are reduced. Otherwise stack is set to
the span of new rule, S = ri.f̄ . The orientation
of ri.f̄ is computed with respect to S but if they
are not adjacent (M or S), we still need to con-
sider the possible local reordering with respect to
the previous rule ri−1.f̄ . In Figure 3, rules #5,#4
are monotone, while both are covered by the cur-
rent span in S. Since the stack always contains
one span, this algorithm runs in O(1). Therefore,
only a limited number of comparisons is used to
update S and compute orientation. Unlike HRM
which needs to maintain a sequence of contiguous
spans in the stack and runs in linear time.

Figure 3 illustrates the application of shift-
reduce approach to compute orientation for initial
decoding steps of a Chinese-English sentence pair
shown in Figure 4. We show source words in the
rules with the corresponding index in the source
sentence. S and ri.f̄ for the initial hypothesis are
set to −1, corresponding to the start of sentence
symbol, making it easy to compute the correct ori-
entation for spans at the beginning of the input
(with index 0).

3 Experiments

We evaluate lexicalized reordering model for LR-
Hiero on three language pairs: German-English
(De-En), Czech-English (Cs-En) and Chinese-
English (Zh-En). Table 1 shows the corpus statis-
tics for all language.

We train a 5-gram LM on the Gigaword corpus
using KenLM (Heafield, 2011). The weights in the
log-linear model are tuned by minimizing BLEU
loss through MERT (Och, 2003) on the dev set for
each language pair and then report BLEU scores
on the test set. Pop limit for Hiero and LR-Hiero
is 500 and beam size for Moses is 1000. Other
extraction and decoder settings such as maximum
phrase length, etc. are identical across different
settings.

We use 3 baselines in our experiments:

• Hiero: we use our in-house implementation
of Hiero, Kriya, in Python (Sankaran et al.,
2012). Kriya can obtain statistically signif-
icantly equal BLEU scores when compared
with Moses (Koehn et al., 2007) for sev-
eral language pairs (Razmara et al., 2012;
Callison-Burch et al., 2012).

• phrase-based: Moses (Koehn et al., 2007)
with and without lexicalized reordering fea-
tures.

• LR-Hiero: LR-Hiero decoding with cube
pruning and queue diversity of 10 (Siahbani
and Sarkar, 2014b).

To make the results comparable we use the stan-
dard SMT features for log-linear model in transla-
tion systems. relative-frequency translation proba-
bilities p(f |e) and p(e|f), lexical translation prob-
abilities pl(f |e) and pl(e|f), a language model
probability, word count, phrase count and distor-
tion. In addition, two distortion features proposed

615



Corpus Train/Dev/Test
Cs-En Europarl.v7; CzEng.v0.9; News

commentary(nc) 2008,2009,2011
7.95M/3000/3003

De-En Europarl.v7; WMT2006 1.5M/2000/2000
Zh-En HK + GALE ph1; MTC 1,3,4 2.3M/1928/919

Table 1: Corpus statistics in number of sentences. Tuning and test sets for Chinese-English has 4 references.

Model Cs-En De-En Zh-En
Hiero 6279.3 7152.3 6524.7
LR-Hiero + LRM 2015.1 2908.3 2225.7

Table 2: Translation time in terms of average number of LM
queries.

Model Cs-En De-En Zh-En
Phrase-based 20.32 24.71 25.68

+ LRM 20.74 25.99 26.61
Hiero 20.77 25.72 27.65
LR-Hiero 20.52 24.96 25.73

+ NVLRM 20.49 24.98 25.9
+ LRM 20.86 25.44 26.57

Table 3: Translation accuracy in terms of BLEU for different
baselines and LR-Hiero with lexicalized reordering model.
The rows are grouped such that each group use the same
model.

by (Siahbani et al., 2013) are added to both Hi-
ero and LR-Hiero. The LRM proposed in this pa-
per uses a GNF grammar and LR decoding, there-
fore we apply it only to LR-Hiero. The GNF rules
are obtained from word and phrase aligned bi-
text using the rule extraction algorithm proposed
by (Siahbani and Sarkar, 2014a).

Table 3 compares the performance of different
translation systems in terms of translation quality
(BLEU). In all language pairs the proposed lexi-
calized reordering model improves the translation
quality of LR-Hiero. These observations are com-
parable to the effect of LRM in phrase-based trans-
lation system. In Cs-En, LRM gets the best results
and it significantly improves the the LR-Hiero re-
sults for De-En and Zh-En (p-value<0.05, evalu-
ated by MultEval (Clark et al., 2011)). To com-
pare our approach to Nguyen and Vogel (2013),
we adopt their algorithm to LR-Hiero and use
the same LRM trained for GNF rules (marked as
NVLRM in Table 3). Unsurprisingly this approach
could not improve the translation quality in LR-
Hiero. This approach computes the LRM for all
candidate translation of each span after obtain-

ing the full translations. In bottom-up decoders it
helps to prune the hypotheses effectively while in
LR-Hiero decoder as we apply a rule before know-
ing the translation of smaller spans the computa-
tion of LRM will be postponed and gets less effec-
tive in decoding.

Table 2 shows the performance in terms of de-
coding speed. We use the same wrapper for Hiero
and LR-Hiero to query the language model and re-
port the average on a sample set of 50 sentences
from test sets. We can see LR-Hiero+LRM still
works 3 times faster than Hiero in terms of num-
ber of LM calls which leads to a faster decoder
speed.

4 Conclusion

We have proposed a novel lexicalized reordering
model (LRM) for the left-to-right variant of Hi-
ero called LR-Hiero distinct from previous LRM
models. The previous LRM models proposed for
Hiero are just applicable to bottom-up decoders
like CKY. We proposed a model for the left-to-
right decoding algorithm of LR-Hiero. We showed
that our novel shift-reduce algorithm to decode
with the lexicalized reordering model significantly
improved the translation quality of LR-Hiero on
three different language pairs.

Acknowledgments

The authors wish to thank the anonymous re-
viewers for their helpful comments. The re-
search was also partially supported by the Nat-
ural Sciences and Engineering Research Council
of Canada (NSERC RGPIN 262313 and RGPAS
446348) to the second author.

References
Chris Callison-Burch, Philipp Koehn, Christof Monz,

Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical ma-
chine translation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, pages
10–51, Montréal, Canada, June. Association for
Computational Linguistics.

616



Hailong Cao, Dongdong Zhang, Mu Li, Ming Zhou,
and Tiejun Zhao. 2014. A lexicalized reorder-
ing model for hierarchical phrase-based translation.
In Proceedings of COLING 2014, the 25th Inter-
national Conference on Computational Linguistics:
Technical Papers, pages 1144–1153, Dublin, Ire-
land, August. Dublin City University and Associa-
tion for Computational Linguistics.

Colin Cherry. 2013. Improved reordering for phrase-
based translation using sparse features. In Proceed-
ings of the 2013 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, pages 22–
31, Atlanta, Georgia, June. Association for Compu-
tational Linguistics.

David Chiang. 2007. Hierarchical phrase-based trans-
lation. Comput. Linguist., 33(2):201–228, June.

Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis testing for
statistical machine translation: Controlling for op-
timizer instability. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
176–181, Portland, Oregon, USA, June. Association
for Computational Linguistics.

Jay Earley. 1970. An efficient context-free parsing al-
gorithm. Commun. ACM, 13(2):94–102, February.

Michel Galley and Christopher D. Manning. 2008.
A simple and effective hierarchical phrase reorder-
ing model. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP 2008, pages 848–856, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.

Michel Galley and Christopher D. Manning. 2010.
Accurate non-hierarchical phrase-based translation.
In Human Language Technologies: The 2010 An-
nual Conference of the North American Chapter of
the Association for Computational Linguistics, HLT
’10, pages 966–974, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.

Kenneth Heafield. 2011. Kenlm: Faster and smaller
language model queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, WMT
’11, pages 187–197, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.

Liang Huang and Haitao Mi. 2010. Efficient incre-
mental decoding for tree-to-string translation. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, EMNLP
’10, pages 273–283, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.

Matthias Huck, Joern Wuebker, Felix Rietig, and Her-
mann Ney. 2013. A phrase orientation model for hi-
erarchical machine translation. In ACL 2013 Eighth
Workshop on Statistical Machine Translation, pages
452–463, Sofia, Bulgaria, August.

Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics on Human Language Technology - Vol-
ume 1, NAACL ’03, pages 48–54, Stroudsburg, PA,
USA. Association for Computational Linguistics.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondřej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL ’07, pages 177–180, Stroudsburg, PA, USA.
Association for Computational Linguistics.

ThuyLinh Nguyen and Stephan Vogel. 2013. Integrat-
ing phrase-based reordering features into a chart-
based decoder for machine translation. In Proceed-
ings of the 51st Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 1587–1596, Sofia, Bulgaria, August.
Association for Computational Linguistics.

Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Comput. Linguist., 30(4):417–449, Decem-
ber.

Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics - Volume 1, ACL ’03, pages 160–
167, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.

Majid Razmara, Baskaran Sankaran, Ann Clifton, and
Anoop Sarkar. 2012. Kriya - the sfu system for
translation task at wmt-12. In Proceedings of the
Seventh Workshop on Statistical Machine Transla-
tion, pages 356–361, Montréal, Canada, June. Asso-
ciation for Computational Linguistics.

Baskaran Sankaran, Majid Razmara, and Anoop
Sarkar. 2012. Kriya - an end-to-end hierarchical
phrase-based MT system. Prague Bull. Math. Lin-
guistics, 97:83–98.

Maryam Siahbani and Anoop Sarkar. 2014a. Ex-
pressive hierarchical rule extraction for left-to-right
translation. In Proceedings of the Eleventh Confer-
ence of the Association for Machine Translation in
the Americas (AMTA), volume 1, pages 1–14.

Maryam Siahbani and Anoop Sarkar. 2014b. Two im-
provements to left-to-right decoding for hierarchical
phrase-based machine translation. In Proceedings of
the 2014 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 221–
226, Doha, Qatar, October. Association for Compu-
tational Linguistics.

617



Maryam Siahbani, Baskaran Sankaran, and Anoop
Sarkar. 2013. Efficient left-to-right hierarchical
phrase-based translation with improved reordering.
In Proceedings of the 2013 Conference on Em-
pirical Methods in Natural Language Processing,
pages 1089–1099, Seattle, Washington, USA, Oc-
tober. Association for Computational Linguistics.

Christoph Tillmann. 2004. A unigram orientation
model for statistical machine translation. In Pro-
ceedings of HLT-NAACL 2004: Short Papers, HLT-
NAACL-Short ’04, pages 101–104, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.

Taro Watanabe, Hajime Tsukada, and Hideki Isozaki.
2006. Left-to-right target generation for hierar-
chical phrase-based translation. In Proceedings of
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 777–784,
Sydney, Australia, July. Association for Computa-
tional Linguistics.

618


