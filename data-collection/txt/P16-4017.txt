



















































MDSWriter: Annotation Tool for Creating High-Quality Multi-Document Summarization Corpora


Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics—System Demonstrations, pages 97–102,
Berlin, Germany, August 7-12, 2016. c©2016 Association for Computational Linguistics

MDSWriter: Annotation tool for creating high-quality
multi-document summarization corpora

Christian M. Meyer,†‡ Darina Benikova,†‡ Margot Mieskes,†¶ and Iryna Gurevych†‡
† Research Training Group AIPHES

‡ Ubiquitous Knowledge Processing (UKP) Lab,
Technische Universität Darmstadt, Germany

¶ University of Applied Sciences, Darmstadt, Germany
http://www.aiphes.tu-darmstadt.de

Abstract

In this paper, we present MDSWriter, a
novel open-source annotation tool for cre-
ating multi-document summarization cor-
pora. A major innovation of our tool is
that we divide the complex summarization
task into multiple steps which enables us
to efficiently guide the annotators, to store
all their intermediate results, and to record
user–system interaction data. This allows
for evaluating the individual components
of a complex summarization system and
learning from the human writing process.
MDSWriter is highly flexible and can be
adapted to various other tasks.

1 Introduction

Motivation. The need for automatic summariza-
tion systems has been rapidly increasing since
the amount of textual information in the web and
at large data centers became intractable for hu-
man readers. While single-document summariza-
tion systems can merely compress the information
of one given text, multi-document summaries are
even more important, because they can reduce the
actual number of documents that require attention
by a human. In fact, they enable users to acquire
the most salient information about a topic with-
out having to deal with the redundancy typically
contained in a set of documents. Given that most
search engine users only access the documents
linked on the first result pages (cf. Jansen and
Pooch, 2001), multi-document summaries even
have the potential to radically influence our infor-
mation access strategies to such textual data that
remains unseen by most current search practices.

At the same time, automatic summarization is
one of the most challenging natural language pro-
cessing tasks. Successful approaches need to per-

form several subtasks in a complex setup, includ-
ing content selection, redundancy removal, and
coherent writing. Training and evaluating such
systems is extremely difficult and requires high-
quality reference corpora covering each subtask.

Currently available corpora are, however, still
severely limited in terms of domains, genres, and
languages covered. Most of them are addition-
ally focused on the results of only one subtask,
most often the final summaries, which prevents the
training and evaluation of intermediate steps (e.g.,
redundancy detection). A major corpus creation is-
sue is the lack of tool support for complex annota-
tion setups. Existing annotation tools do not meet
our demands, as they are limited to creating fi-
nal summaries without storing intermediate results
and user interactions or are not freely available or
support only single document summarization.

Contribution. In this paper, we present MDS-
Writer, a software for manually creating multi-
document summarization corpora. The core inno-
vation of our tool with regard to previous work is
that we allow dividing the complex summarization
task into multiple steps. This has two major advan-
tages: (i) By linking each step to detailed annota-
tion guidelines, we support the human annotators
in creating high-quality summarization corpora in
an efficient and reproducible way. (ii) We sepa-
rately store the results of each intermediate step.
This is necessary to properly evaluate the individ-
ual components of a complex automatic summa-
rization system, but was largely neglected previ-
ously. Storing the intermediate results enables us
to improve the evaluation setup beyond measuring
inter-annotator agreement for the content selection
and ROUGE for the final summaries.

Furthermore, we put a particular focus on
recording the interactions between the users and
the annotation tool. Our goal is to learn summa-

97



rization writing strategies from the recorded user–
system interactions and the intermediate results
of the individual steps. Thus, we envision next-
generation summarization systems that learn the
human summarization process rather than trying
to only replicate its result.

To the best of our knowledge, MDSWriter is
the first attempt to support the complex annotation
task of creating multi-document summaries with
flexible and reusable software providing access to
process data and intermediate results. We designed
an initial, multi-step workflow implemented in
MDSWriter. However, our tool is flexible to de-
viate from this initial setup allowing a wide range
of summary creation workflows, including single-
document summarization, and even other complex
annotation tasks. We make MDSWriter available
as open-source software, including our exemplary
annotation guidelines and a video tutorial.1

2 Related work

There is a vast number of general-purpose tools
for annotating corpora, for example, WebAnno
(Yimam et al., 2013), Anafora (Chen and Styler,
2013), CSNIPER (Eckart de Castilho et al., 2012),
and the UAM CorpusTool (O’Donnell, 2008).
However, neither of these tools is suitable for
tasks that require access to multiple documents at
the same time, as they are focused on annotating
linguistic phenomena within single documents or
search results with limited contexts.

Tools for cross-document annotation tasks are
so far limited to event and entity co-reference,
e.g., CROMER (Girardi et al., 2014). These tools
are, however, not directly applicable to the task of
multi-document summarization. In fact, all tools
discussed so far lack a definition of complex anno-
tation workflows spanning multiple steps, which
we consider necessary for obtaining intermediate
results and systematically guiding the annotators.

With regard to the user–system interactions, the
work on the Webis text reuse corpus (Potthast et
al., 2013) is similar to ours. They ask crowdsource
workers to retrieve sources for a given topic and
record their search and text reuse actions. How-
ever, they approach a plagiarism detection task and
therefore focus on writing essays rather than sum-
maries and they do not provide detailed guidelines
which is necessary to create high-quality corpora.

Summarization-specific software tools address
1https://github.com/UKPLab/mdswriter

the assessment of written summaries, computer-
assisted summarization, or the manual construc-
tion of summarization corpora. The Pyramid an-
notation tool (Nenkova and Passonneau, 2004) and
the tool2 used for the MultiLing shared tasks (Gi-
annakopoulos et al., 2015) are limited to compar-
ing and scoring summaries, but do not provide any
writing functionality. Orăsan et al.’s (2003) CAST
tool assists users with summarizing a document
based on the output of an automatic summariza-
tion algorithm. However, their tool is restricted to
single-document summarization.

The works by Ulrich et al. (2008) and Nakano
et al. (2010) are most closely related to ours, since
they discuss the creation of multi-document sum-
marization corpora. Unfortunately, their proposed
annotation tools are not available as open-source
software and thus cannot be reused. In addition to
that, they do not record user–system interactions,
which we consider important for next-generation
automatic summarization methods.

3 MDSWriter

MDSWriter is a web-based tool implemented in
Java/JSP and JavaScript. The user interface con-
sists of a dashboard providing access to all anno-
tation projects and their steps. Each step commu-
nicates with a server application that is responsible
for recording the user–system interactions and the
intermediate results. Below, we describe our pro-
posed setup with seven subsequent steps motivated
by our initial annotation guidelines.

Dashboard. Our tool supports multiple users
and topics (i.e., the document sets that are to be
summarized). After logging in, a user receives a
list of all topics assigned to her or him, the num-
ber of documents per topic, and the status of the
summarization process. That is, for each annota-
tion step, the tool shows either a green checkmark
(if completed), a red cross (if not started), or a yel-
low circle (if this step comes next) to indicate the
user’s progress. By clicking on the yellow circle
of a topic, the user can continue his or her work.
Figure 2 (a) shows an example with ten topics.

Step 1: Nugget identification. The first step
aims at the selection of salient information within
the multiple source documents, which is the most
important and most time-consuming annotation
step. Figure 1 shows the overall setup. Two thirds

2http://143.233.226.97:60091/MMSEvaluator/

98



Figure 1: Nugget identification step with explanations of the most important components

of the screen are reserved for displaying the source
documents. In the current setup, a user can choose
to view a single source document over the full
width or display two different source documents
next to each other. The latter is useful for compar-
ison and for ensuring consistent annotation.

Analogous to a marker pen, users can select
salient parts of the text with the mouse. When re-
leasing the mouse button, the selected text part
receives a different background color and the se-
lected text is included in the list of all selections
shown on the right-hand side. The users may use
three different colors to organize their selections.
Existing selections can be modified and deleted.

To systematize the selection process, we define
the term important information nugget in our an-
notation guidelines. Each nugget should consist of
at least one verb with at least one of its arguments.
It should be important, topic-related, and coherent,
but not cross sentence boundaries. Typically, each
selection in the source document corresponds to
one nugget. But nuggets might also be discontinu-
ous in order to support the exclusion of parentheti-
cal phrases and other information of minor impor-
tance. Our tool models these cases by defining two

distinct selections and merging them by means of
a dedicated merge button.

Two special cases are nuggets referring to a cer-
tain source (e.g., a book) and nuggets within di-
rect or indirect speech, which indicate a speaker’s
opinion. In figure 1, the 2005 biography is the
source for the music-related selection. If a user
would select only the subordinate clause, some
people would be the speaker. As information about
the source or speaker is highly important for both
automatic methods and human writers, we provide
a method to select this information within the text.
The selection list shows the source/speaker in gray
color at the beginning of a selection (default: [?]).

Having finished the nugget identification for all
source documents, a user can return to the dash-
board by clicking on “step complete”.

Step 2: Redundancy detection. Redundancy is
a key characteristic of multiple documents about
a given topic. Automatic summarization methods
aim at removing this redundancy. But, at the same
time, most methods rely on the redundancy sig-
nal when estimating the importance of a phrase or
sentence. Therefore, our annotation guidelines for

99



(a) (b)

(c) (d)

Figure 2: Screenshots of the dashboard (a) and the steps 3 (b), 5 (c), and 7 (d)

step 1 suggest to identify all important nuggets, in-
cluding redundant ones. This type of intermediate
result will allow us to create a better setup for eval-
uating content selection algorithms than compar-
ing their outcome to redundancy-free summaries.

As our ultimate goal is, however, to compose
an actual summary, we still need to remove the
redundancy, which motivates our second annota-
tion step. Each user receives a list of his or her ex-
tracted information nuggets and may now reorder
them using drag and drop. As a result, nuggets
with the same or a highly similar content will yield
a single group. To allow for an informed decision,
users may expand each nugget to view a context
box showing the title of the source document and
a ±10 words window around the nugget text.
Step 3: Best nugget selection. In the third step,
users select a representative nugget from each
group, which we call the best nugget. We guide
their decision by suggesting to prefer declarative
and objective statements and to minimize context
dependence (e.g., by avoiding deixis or anaphora).

To select the best nugget, users can click on one of
the nuggets within a group, which then turns red.
Users may change their decisions and open a con-
text box similar to step 2. Figure 2 (b) shows an
example with two groups and a context box.

Step 4: Co-reference resolution. Although the
users should avoid nuggets with co-references in
step 3, there is often no other choice. Therefore,
we aim at resolving the remaining co-references
as part of a fourth annotation step. Even though
human writers make vast use of co-references in a
final summary, they usually change them with re-
gard to the source documents. For example, it is
uncommon to use a personal pronoun in the very
first sentence of a summary, even if this cataphor
would be resolved in the following sentences.
Therefore, our approach is to first resolve all co-
references in the best nuggets during step 4 and
establish a meaningful discourse structure later
when composing the actual summary in step 7.

To achieve this, MDSWriter displays one best
nugget at a time and allows the user to navigate

100



through them. For each best nugget, we show its
direct context, but also provide the entire source
document in case the referring expression is not
included in the surrounding ten words.

Step 5: Sentence formulation. Since our notion
of information nuggets is on sub-sentence level,
we ask our users to formulate each best nugget as a
complete, grammatical sentence. This type of data
will be useful for evaluating sentence compression
algorithms, which start with an entire sentence ex-
tracted from one of the source documents and aim
at compressing it to the most salient information.
In our guidelines, we suggest that the changes to
the nugget text should be minimal and that both
the statement’s source (step 1) and the resolved
co-references (step 4) should be part of the refor-
mulated sentence. We use the same user interface
as in the previous step. That is, we display a single
best nugget in its context and ask for the reformu-
lated version. Figure 2 (c) shows a screenshot of a
reformulated discontinuous nugget.

Step 6: Summary organization. While impor-
tant nuggets often keep their original order in
single-document summaries, there is no obvious
predefined order for multi-document summaries.
Therefore, we provide a user interface for orga-
nizing the sentences (step 5) in a meaningful way
to formulate a coherent summary. A user receives
a list of her or his sentences and may change the
order using drag and drop. Additionally, it is pos-
sible to insert subheadings (e.g., “conclusion”).

We consider this step important as previous ap-
proaches, for example, by Nakano et al. (2010,
p. 3127) “did not instruct summarizers about how
to connect parts” and thus do not control for coher-
ence. By explicitly defining the order, we get in a
position to learn from the human summarization
process and improve the coherence of automati-
cally generated extracts.

The user interface for step 6 is similar to the
steps 2 and 3. It shows a sentence list and allows
opening a context box with the original nugget.

Step 7: Summary composition. Our final step
aims at formulating a coherent summary based on
the structure defined in step 6. MDSWriter pro-
vides a text area that is initialized with the refor-
mulated (step 5) and ordered (step 6) best nuggets,
which can be arbitrarily changed. In our setup,
we ask the users to make only minimal changes,
such as introducing anaphors, discourse connec-

tives, and conjunctions. This will yield summaries
that are very close to the source documents, which
is especially useful for evaluating extractive sum-
marization methods. However, MDSWriter is not
limited to this procedure and future uses may
strive for abstractive summaries that require sub-
stantial revisions.

While writing the summary, the users have ac-
cess to all source documents, to their original
nuggets (step 1) and to their selection of best
nuggets (step 3). By means of a word counter, the
users can easily produce summaries with a cer-
tain word limit. Figure 2 (d) shows the correspond-
ing user interface. Having finished their summary,
users complete the entire annotation process for
the current topic and return to the dashboard.

Server application. Each user action, ranging
from the selection of a new nugget (step 1) to mod-
ifications of the final summary (step 7), is auto-
matically sent to our server application. We use
a WebSocket connection to ensure efficient bidi-
rectional communication. The user–system inter-
actions and all intermediate results are stored in an
SQL database. Conversely, the server loads previ-
ously stored inputs, such that the users can inter-
rupt their work at any time without losing data.

The client–server communication is based on a
simple text-based protocol. Each message consists
of a four character operation code (e.g., 7DNE in-
dicating that step 7 is now complete) and an ar-
bitrary number of tab-separated parameters. The
message 1NGN 1 25 100 2 indicates, for exam-
ple, that the current user added a new nugget of
length 100 characters to document 1 at offset 25,
which will be displayed in color 2 (yellow).

4 Extensibility

The annotation workflow discussed so far is one
example of dividing the complex setup of multi-
document summarization into clear-cut steps. We
argue that this division is important to ensure con-
sistent and reliable annotations and to record inter-
mediate results and process data. Despite this ex-
emplary setup, MDSWriter provides an ideal basis
for many other summarization workflows, such as
creating structured or aspect-oriented summaries.
This can be achieved by rearranging already ex-
isting steps and/or adding new steps. To this end,
we designed the bidirectional and easy-to-extend
message protocol described in the previous section
as well as a brief developer guide on GitHub.

101



Of particular interest is that MDSWriter fea-
tures cross-document annotations, the recording of
user–system interactions and intermediate results,
which is also highly relevant beyond the summa-
rization scenario. Therefore, we consider MDS-
Writer as an ideal starting point for a wide-range
of other complex multi-step annotation tasks, in-
cluding but not limited to information extraction
(combined entity, event, and relation identifica-
tion), terminology mining (selection of candidates,
filtering, describing, and organizing them), and
cross-document discourse structure annotation.

5 Conclusion and future work

We introduced MDSWriter, a tool for construct-
ing multi-document summaries. Our software fills
an important gap as high-quality summarization
corpora are urgently needed to train and evalu-
ate automatic summarization systems. Previously
available tools are not well-suited for this task, as
they do not support cross-document annotations,
the modeling of complex tasks with a number of
distinct steps, and reusing the tools under free li-
censes. As a key property of our tool, we store
all intermediate annotation results and record the
user–system interaction data. We argued that this
enables next-generation summarization methods
by learning from human summarization strategies
and evaluating individual components of a system.

In future work, we plan to create and evaluate
an actual corpus for multi-document summariza-
tion using our tool. We also plan to provide mon-
itoring components in MDSWriter, such as com-
puting inter-annotator agreement in real-time.

Acknowledgements. This work has been sup-
ported by the DFG-funded research training group
“Adaptive Preparation of Information form Het-
erogeneous Sources” (AIPHES, GRK 1994/1) and
by the Lichtenberg-Professorship Program of the
Volkswagen Foundation under grant№ I/82806.

References
Wei-Te Chen and Will Styler. 2013. Anafora: A Web-

based General Purpose Annotation Tool. In Pro-
ceedings of the 2013 NAACL/HLT Demonstration
Session, pages 14–19, Atlanta, GA, USA.

Richard Eckart de Castilho, Sabine Bartsch, and Iryna
Gurevych. 2012. CSniper – Annotation-by-query
for Non-canonical Constructions in Large Corpora.
In Proceedings of the 50th Annual Meeting of the
ACL: System Demonstrations, pages 85–90, Jeju Is-
land, Korea.

George Giannakopoulos, Jeff Kubina, John Conroy,
Josef Steinberger, Benoit Favre, Mijail Kabadjov,
Udo Kruschwitz, and Massimo Poesio. 2015.
MultiLing 2015: Multilingual Summarization of
Single and Multi-Documents, On-line Fora, and
Call-center Conversations. In Proceedings of the
16th Annual Meeting of the SIGDIAL, pages 270–
274, Prague, Czech Republic.

Christian Girardi, Manuela Speranza, Rachele Sprug-
noli, and Sara Tonelli. 2014. CROMER: a Tool
for Cross-Document Event and Entity Coreference.
In Proceedings of the Ninth International Confer-
ence on Language Resources and Evaluation, pages
3204–3208, Reykjavik, Iceland.

Bernard J. Jansen and Udo Pooch. 2001. A review of
Web searching studies and a framework for future
research. Journal of the American Society for Infor-
mation Science and Technology, 52(3):235–246.

Masahiro Nakano, Hideyuki Shibuki, Rintaro
Miyazaki, Madoka Ishioroshi, Koichi Kaneko,
and Tatsunori Mori. 2010. Construction of Text
Summarization Corpus for the Credibility of Infor-
mation on the Web. In Proceedings of the Seventh
International Conference on Language Resources
and Evaluation, pages 3125–3131, Valletta, Malta.

Ani Nenkova and Rebecca Passonneau. 2004. Evaluat-
ing Content Selection in Summarization: The Pyra-
mid Method. In Proceedings of the Human Lan-
guage Technology Conference of the North Amer-
ican Chapter of the ACL, pages 145–152, Boston,
MA, USA.

Mick O’Donnell. 2008. Demonstration of the UAM
CorpusTool for Text and Image Annotation. In Pro-
ceedings of the 46th Annual Meeting of the ACL:
Demo Session, pages 13–16, Columbus, OH, USA.

Constantin Orăsan, Ruslan Mitkov, and Laura Hasler.
2003. CAST: A computer-aided summarisation tool.
In Proceedings of the 10th Conference of the Euro-
pean Chapter of the ACL, pages 135–138, Budapest,
Hungary.

Martin Potthast, Matthias Hagen, Michael Völske, and
Benno Stein. 2013. Crowdsourcing Interaction
Logs to Understand Text Reuse from the Web. In
Proceedings of the 51st Annual Meeting of the ACL,
pages 1212–1221, Sofia, Bulgaria.

Jan Ulrich, Gabriel Murray, and Giuseppe Carenini.
2008. A Publicly Available Annotated Corpus for
Supervised Email Summarization. In Enhanced
Messaging: Papers from the 2008 AAAI Workshop,
Technical Report WS-08-04, pages 77–82. Menlo
Park, CA: AAAI Press.

Seid Muhie Yimam, Iryna Gurevych, Richard
Eckart de Castilho, and Chris Biemann. 2013.
WebAnno: A Flexible, Web-based and Visually
Supported System for Distributed Annotations. In
Proceedings of the 51st Annual Meeting of the ACL:
System Demonstrations, pages 1–6, Sofia, Bulgaria.

102


