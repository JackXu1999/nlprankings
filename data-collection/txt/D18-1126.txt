



















































Effective Use of Context in Noisy Entity Linking


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1024–1029
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

1024

Effective Use of Context in Noisy Entity Linking

David Mueller
Department of Computer Science

Johns Hopkins University∗

dam@jhu.edu

Greg Durrett
Department of Computer Science
The University of Texas at Austin
gdurrett@cs.utexas.edu

Abstract

To disambiguate between closely related con-
cepts, entity linking systems need to effec-
tively distill cues from a mention’s textual con-
text. We investigate several techniques for us-
ing these cues in the task of noisy entity link-
ing on short texts. Our starting point is a state-
of-the-art attention-based model from prior
work; while this model’s attention typically
identifies context that is topically relevant, it
fails to identify some of the most indicative
context words, especially those exhibiting lex-
ical overlap with the true title. Augmenting
the model with convolutional networks over
characters still leaves it largely unable to pick
up on these cues compared to sparse features
that target them directly, indicating that au-
tomatically learning how to identify relevant
character-level context features is a hard prob-
lem. Armed with these sparse features, our
final system1 outperforms past work on the
WikilinksNED test set by 2.8% absolute.

1 Introduction

Effectively using an entity mention’s context to
disambiguate it is the crux of the entity linking
task: in isolation, the mention Richard Wright
could refer to three possible entities in Wikipedia’s
knowledge base corresponding to an artist, a mu-
sician, or an author. Previous work in this area
has distilled context information by exploiting tf-
idf features (Cucerzan, 2007; Milne and Witten,
2008; Ratinov et al., 2011), global link coher-
ence (Hoffart et al.; Sil and Florian, 2016), cues
from coreference (Cheng and Roth, 2013; Ha-
jishirzi et al., 2013; Durrett and Klein, 2014), con-
volutional neural networks (Sun et al.; Francis-
Landau et al., 2016), or more sophisticated neural
architectures (Gupta et al., 2017; Sil et al., 2018).

∗Work done while at UT Austin.
1Code available at

https://github.com/davidandym/wikilinks-ned

These approaches typically focus on aggregat-
ing information from a mix of sources, including
long-range information from the textual context or
other linked entities. While this approach is suit-
able for entity linking settings such as newswire
(Bentivogli, 2010) and Wikipedia (Ratinov et al.,
2011), we cannot always rely on this information
in other settings like Twitter (Guo et al., 2013;
Fang and Chang, 2014; Huang et al., 2014; Dredze
et al., 2016), Snapchat (Moon et al., 2018), other
web platforms (Eshel et al., 2017), or dialogue sys-
tems (Bowden et al., 2018). We need models that
can make effective use of limited context windows
in noisy settings.

In this work, we investigate this problem of ef-
fectively using context in the setting of the Wik-
ilinksNED dataset from Eshel et al. (2017). The
examples in this dataset, which consists of 3.2 mil-
lion entity disambiguation examples derived from
Wikilinks (Singh et al., 2012), have at most 20
words of context on either side and usually no
other mentions of the entity being disambiguated.
We build off a state-of-the-art attentive LSTM
model from prior work (Eshel et al., 2017) and
show that despite its good performance, it fails to
resolve some examples that human readers would
find trivial. For example, disambiguating the iden-
tity of the song Down in Figure 1 is easy if we can
recognize the nearby string Jay Sean in the con-
text, but the model sometimes fails to do this.

We explore the performance of a standard at-
tention mechanism as well as two modifications.
First, we inject character information into the
model through character-level CNNs; these give
the model a deeper ability to recognize character
correspondences between the context and entity ti-
tle. However, these convolutional filters struggle
to learn useful features in this noisy context and
ultimately do not help performance. By contrast,
sparse features explicitly targeting these overlaps



1025

in 2009 Jay Sean’s single reached the number 1 spot

Attention

Linear

Down

Attention Down (Jay Sean Song)

Down (Blink 
182 Song)

Down (Band)
Features (Sec 4.3)[l · et, l � et, (l � et)2] [r · et, r � et, (r � et)2]

st

et
l r

Figure 1: Neural entity linking model. Down has three
possible link targets: in the attentive variants of our
model, each target computes attention weights over
GRUs that consume the left and right context. These
representations are passed into a layer that compares
them to the entity’s embedding, yielding a final score
which is normalized over all possible link targets.

appear to be more successful. We investigate the
relation between our model’s attention and what
the sparse features learn. Our final model, using
these features, achieves an accuracy of 75.8% on
this dataset, substantially outperforming our base-
line model as well as results from prior work.

2 Basic Model

The WikilinksNED dataset consists of entity men-
tions in context scraped from the web, with
gold annotation derived from the fact that those
mentions originally appeared with hyperlinks to
Wikipedia. We denote the mention text (i.e., an-
chor text of the hyperlink) by m, and denote the
left and right context of the mention by cl and cr
respectively; these are at most 20 words. For this
dataset, we can assume that the possible linked ti-
tles for a mention have been seen in training, and
the main task is instead to disambiguate between
them and identify the gold title t∗. We therefore
follow prior work (Eshel et al., 2017) and take
as candidates all gold entities in the training set
whose mention wasm rather than relying on a sep-
arate candidate generation scheme.

Our model places a distribution over titles
P (t|m, cl, cr), where t takes values in the set of
candidate Wikipedia titles for that mention. This
model, depicted in Figure 1, roughly follows that
of Eshel et al. (2017), with some key differences,
as we discuss in the rest of this section.

Embedding contexts Given an example of the
form (m, cl, cr), our model first uses a GRU layer
(Cho et al., 2014) over each context to convert cl

and cr into continuous vector representations l and
r, respectively. Our word embeddings are trained
over Wikipedia as described in the following para-
graph.

Embedding entities We follow the method of
Eshel et al. (2017) for generating entity embed-
dings, using word2vecf (Levy and Goldberg,
2014) to jointly train word and entity embeddings
simultaneously using Wikipedia article text. Each
title t is associated in turn with each content word
w in the article, yielding a set of (w, t) pairs that
are consumed by the training procedure. This
yields a set of title embeddings et which we can
treat as distributed representations of entities.

Entity-context comparison We systematically
compare the representations for l, r, and et as fol-
lows:

[l · et, r · et, l − et, r − et, (l − et)2, (r − et)2]

where · denotes the conventional dot product and
the other comparisons are elementwise. These fea-
tures form the input to a final feedforward layer
which produces a real-valued score st for the
given title. Repeating this computation for each
title, our model’s distribution is P (t|m, cl, cr) =
softmaxt(st).

Training Because our model involves substan-
tial computation for each possible title, we want
to limit the set of titles considered during train-
ing. For each example we consider, we construct a
set T containing the gold title and 4 negative “dis-
tractor” titles from the candidate set. Unlike Es-
hel et al. (2017), we structure training as a multi-
class decision among these titles rather than a bi-
nary prediction problem over each title as gold or
not. We run our model over the candidates t ∈ T
to produce the distribution P (t|m, cl, cr) and train
to maximize the log probablity logP (t∗|m, cl, cr)
of the gold title.

Results The model set forth in this section is the
basis for the remaining models in this paper; we
call it the GRU model as that is the only context en-
coding mechanism it uses. As shown in Table 1,
this GRU model gets a score of 73.4 on the Wik-
ilinksNED development set. In the next section,
we explore techniques for using the context in a
more sophisticated way to improve further on this
result.



1026

Model Accuracy on Test (%)

Eshel et al. (2017) 73.0
Eshel system release 72.2
GRU+ATTN 74.5
GRU+ATTN+FEATS 75.8

Model Accuracy on Dev (%)

GRU 73.4
GRU+ATTN 74.4
GRU+ATTN+FEATS 74.9
GRU+ATTN+CNN 73.8

Table 1: Results on the WikilinksNED dev and test
sets. Our model including features achieves state-of-
the-art performance on the test set, compared to both
the reported numbers from Eshel et al. (2017) as well as
their released software. Incorporating character CNNs
surprisingly leads to lower performance compared to
these simple features.

3 Exploiting Context Cues

3.1 Attention
One way to improve over the basic GRU model is
to use attention over the context based on the ti-
tle under consideration. The attention we use is a
modified version of the dot product attention (Lu-
ong et al., 2015) used by Eshel et al. (2017), al-
lowing the model to weight the importance of the
outputs of the GRU at each time step. Each con-
text (left and right) has its own attention weights.
For a given side of context and candidate t, the at-
tention first computes a transformation of the en-
tity embedding et as follows: qt = tanh(Wet).
This allows the model to learn an attention query
qt distinct from the candidate embedding et. The
model then computes attention probabilities αi for
each GRU output oi, normalized over the entire
sequence of GRU outputs (of length n):

αi = softmaxi(qt · oi)

The resulting probability distribution is used to
take a weighted sum of GRU outputs to get a rep-
resentation a:

a =
n∑
i

αioi

We compute al and ar independently and symmet-
rically for the left and right context. These vectors
are then fed forward through the model as the fi-
nal continuous representation of the left or right
context, l or r respectively.

Results In Table 1, we see that our model with
attention (GRU+ATTN) outperforms our basic GRU
model by around 1% absolute. It also outperforms
the roughly similar model of Eshel et al. (2017) on
the test set: this gain is due to a combination of
factors including the improved training procedure
and some small modeling changes.2 However, our
attention scheme is not without its shortcomings,
as we now discuss.

3.2 Shortcomings of Attention
One common and frustrating error our model
makes is failing to correctly disambiguate men-
tions whose contexts share similar words or
character overlap with the gold entity’s actual
Wikipedia title. In these instances, the model fails
to attend correctly to words that we, as human
readers, would most likely see as disambiguating
terms. For instance, in this example’s left context:

...known also for the B.P. Koirala In-
stitute of Health Sciences, one of the
biggest government hospital. The in-
digenous people of Dharan are Limbu ...

the model fails to identify people as a critical term
for disambiguation. This failure is partially due to
the model’s sole reliance on distributed represen-
tations: the embedding for people and the title em-
bedding for Limbu People need to somehow con-
tain enough common information for the model
to associate these, identify people as an important
token, and use it to disambiguate between candi-
date titles such as Limbu People, Limbu Language,
and Limbu Alphabet. Moreover, with such noisy,
unstructured context, it is difficult for the model
to learn to rely on other grammatical or semantic
cues (such as are indicating that the title is prob-
ably a plural noun, which alphabet and language
are not).

3.3 Character CNNs
One way to address these issues in the model is
to exploit more fine-grained character-level infor-
mation. This circumvents the need to separately
learn a distributed correspondence between terms
with lexical overlap, and is especially useful when
these terms may be unknown words; for exam-
ple, a year mentioned within a context is often

2Note that in Eshel et al. (2017), the authors point out that
their dataset has a high percentage of errors (35% of the errors
made by their model are spurious), meaning that the skyline
on this task is likely not higher than 90%.



1027

unknown and therefore assigned an UNK embed-
ding, even if that year matches exactly with a year
in the gold candidate’s title.

One solution to this is to allow our model to
consult character-level information, which past
models have done successfully for named en-
tity recognition (Chiu and Nichols, 2015; Lample
et al., 2016; Ma and Hovy, 2016), text classifica-
tion (Zhang et al., 2015), and POS tagging (San-
tos and Zadrozny, 2014). We use convolutional
neural networks (CNNs) to distill character repre-
sentations of words into vectors that we concate-
nate with our word representations. We addition-
ally use character CNNs over entity titles and con-
catenate these representations with the title em-
beddings et, to allow the model to learn to char-
acterize similarities between contexts and entity
titles. Our CNNs use window sizes of 6 and 100
filters each; these values were selected through hy-
perparameter tuning on the development set.

Table 1 shows the impact of incorporating char-
acter CNNs (GRU+ATTN+CNN). Surprisingly,
these have a mild negative impact on perfor-
mance. One possible explanation of this is that it
causes the model to split its attention between se-
mantically important and lexically similar context
terms. Consider the following example:

really think Final Fight could be a lot of
a fun as a vigilante justice movie with a
high quotient of hand-to-hand fight se-
quences. Think The Warriors

The gold title is The Warrior (film) and the base
model correctly places 90% of its attention weight
on the word movie when calculating attention
for this title. However, the character-level CNN
model only places 60% of its attention weight on
it, distributing its attention values more evenly
across the rest of the words. Such cases are fre-
quent: the average highest weight given by at-
tention in GRU+ATTN+CNN is about 6% lower
than the average highest attention weight given by
GRU+ATTN. The CNNs seem to have generally
decreased the model’s confidence in what context
clues are key for disambiguation, leading to lower
performance. We will return to more analysis of
this in Section 4.

3.4 Lexical Feature Set

To determine whether character level overlap be-
tween the entity title and context is useful, we take

Entity Title: Limbu People

Known also for the B.P. Koirala Institute of Health Sciences, one of 
the biggest government hospital. The indigenous people of Dharan 

are Limbu

Left Context:

[…[Left pos:4 has Exact overlap=True], [Left pos:5 has Exact overlap=False],…]

Figure 2: An example of feature generation from
an example. Here, because the word people
occurs in the title and in the left context 4
words away from the mention, the indicator feature
[Pos=4,Match=ExactWord,Context=Left] fires in the
feature set.

a more direct approach to incorporating that infor-
mation into our model and build a set of sparse
features that directly target it.

Figure 2 shows an example of how our features
are computed. We fire features on each word in
the context that is either an exact match or a sub-
string of a word in the candidate title; people is
the relevant token here. We conjoin that match
information with whether the word is in the left
or right context along with the bucketed offset of
the word from the mention. This feature set is
then appended to the vector comparison features
to form the input to the model’s feedforward layer
(see Figure 1).

Table 1 shows the results of stacking these
features on top of our model with attention
(GRU+ATTN+FEATS). We see our highest devel-
opment set performance and correspondingly high
test performance from this model. This indicates
that character-level information is useful for dis-
ambiguation, but character CNNs as we incorpo-
rated them are not able to distill it as effectively
as sparse features can. Our model augmented with
these sparse features achieves state-of-the-art re-
sults on the test set.

4 Attention and “Obvious” Terms

Now that we have identified features which seem
useful for this entity linking problem, we can ask
how the tokens attended by our attention mecha-
nism compare to those singled out by the features.

Table 2 contains statistics regarding the
attention values of our GRU+ATTN and
GRU+ATTN+CNN model on a subset of ex-
amples that both models got wrong. We define
accuracy as the percentage of examples in which
the model gives the highest attention to a word



1028

Stilton Cheese

Roquefort is a very strong stinky cheese from sheep’s milk. Like Stilton…

Delta Robot

I’m aiming high, for a little robot on my desk, of an articulated - or a delta…

 - GRU+ATTN
 - GRU+ATTN+CNN

Feat=[Pos=6-10,Match=ExactWord,Context=Left]

Feat=[Pos=5,Match=ExactWord,Context=Left]

Figure 3: Examples of our models putting high atten-
tion weight into irrelevant context words, not acknowl-
edging the relevance of disambiguating terms that share
lexical overlap with the correct title. We display the
weight given to the top 4 attended words above each
word for two of our models.

that contains one of our lexical features, out
of all examples where such a feature exists
anywhere. The reported probability mass is the
total attention mass that the model puts into words
that associated with lexical features, averaged
over all examples where such features exist. We
see that the model frequently fails to exploit this
information, and moreover the addition of CNNs
does not strongly improve this.

Figure 3 shows examples of this behavior. In
the first example, rather than identifying cheese
as a salient term, both models instead focus more
heavily on milk and like. Similarly, in the second
example, the model fails to recognize the impor-
tance of robot in the context.

One possible reason that CNNs don’t help more
is that the sparse features only trigger on a sub-
set of examples. Because the CNNs process ev-
ery example, they may not see enough examples
of lexical overlap to pick up on it, and instead
try to augment what the word embedding model
is already doing with subword information, which
ends up being unstable for this task. Naturally,
words with these overlap characteristics are not al-
ways the most disambiguating term. However, in
light of noisy contexts, when the standard repre-
sentation of context fails to be sufficient for allow-
ing the model to disambiguate, we want the model
to be able to leverage this character level informa-
tion to help it make intuitive decisions, which the
CNN fails to do.

5 Conclusion

In this paper, we observed that in noisy entity link-
ing settings on short texts, neural models relying

Context Acc (%) Prob Mass (%)

GRU+ATTN L 0.41 0.32
GRU+ATTN R 0.36 0.30
GRU+ATTN+CNN L 0.46 0.32
GRU+ATTN+CNN R 0.36 0.28

Table 2: Our models’ attention “accuracy”: how often
each model’s maximally-attended word also triggered
a feature to fire. Prob Mass indicates the average sum
of attention scores over feature-triggering words. All
values are computed over a sample of 10,000 examples
that each model got wrong.

on attention do not always pick up on the cor-
rect context clues, even when those clues exhibit
very obvious surface overlap with the correct en-
tity title. These models can perform better when
augmented with sparse features explicitly target-
ing this kind of lexical overlap: our system us-
ing these features achieves state-of-the-art disam-
biguation accuracy on the WikilinksNED dataset.
By contrast, automatically learning learning fine-
grained character-level features with CNNs in this
context is hard. More exploration is needed to bet-
ter understand what inductive biases are necessary
for an entity linking system to make maximally ef-
fective use of the information available to it.

Acknowledgments

This work was partially supported by NSF Grant
IIS-1814522, a Bloomberg Data Science Grant,
and an equipment grant from NVIDIA. The au-
thors acknowledge the Texas Advanced Comput-
ing Center (TACC) at The University of Texas at
Austin for providing HPC resources used to con-
duct this research. Thanks as well to the anony-
mous reviewers for their helpful comments.

References
Luisa Bentivogli. 2010. Extending English ACE

2005 Corpus Annotation with Ground-truth Links to
Wikipedia. In Proceedings of COLING.

Kevin K. Bowden, Jiaqi Wu, Shereen Oraby, Amita
Misra, and Marilyn Walker. 2018. SlugNERDS: A
Named Entity Recognition Tool for Open Domain
Dialogue Systems. In Proceedings of LREC.

Xiao Cheng and Dan Roth. 2013. Relational Inference
for Wikification. In Proceedings of EMNLP.

Jason P. C. Chiu and Eric Nichols. 2015. Named Entity
Recognition with Bidirectional LSTM-CNNs. In
Proceedings of ACL.



1029

Kyunghyun Cho, Bart van Merriënboer, Çalar
Gülçehre, Dzmitry Bahdanau, Fethi Bougares, Hol-
ger Schwenk, and Yoshua Bengio. 2014. Learn-
ing Phrase Representations using RNN Encoder–
Decoder for Statistical Machine Translation. In Pro-
ceedings of EMNLP.

Silviu Cucerzan. 2007. Large-Scale Named Entity Dis-
ambiguation Based on Wikipedia Data. In Proceed-
ings of EMNLP-CoNLL.

Mark Dredze, Nicholas Andrews, and Jay DeYoung.
2016. Twitter at the Grammys: A Social Media Cor-
pus for Entity Linking and Disambiguation. In Pro-
ceedings of The Fourth International Workshop on
Natural Language Processing for Social Media.

Greg Durrett and Dan Klein. 2014. A Joint Model for
Entity Analysis: Coreference, Typing, and Linking.
In TACL.

Yotam Eshel, Noam Cohen, Kira Radinsky, Shaul
Markovitch, Ikuya Yamada, and Omer Levy. 2017.
Named Entity Disambiguation for Noisy Text. In
Proceedings of CoNLL.

Yuan Fang and Ming-Wei Chang. 2014. Entity Linking
on Microblogs with Spatial and Temporal Signals.
In TACL.

Matthew Francis-Landau, Greg Durrett, and Dan
Klein. 2016. Capturing Semantic Similarity for En-
tity Linking with Convolutional Neural Networks.
In Proceedings of NAACL.

Stephen Guo, Ming-Wei Chang, and Emre Kiciman.
2013. To Link or Not to Link? A Study on End-
to-End Tweet Entity Linking. In Proceedings of
NAACL.

Nitish Gupta, Sameer Singh, and Dan Roth. 2017. En-
tity Linking via Joint Encoding of Types, Descrip-
tions, and Context. In Proceedings of EMNLP.

Hannaneh Hajishirzi, Leila Zilles, Daniel S. Weld, and
Luke Zettlemoyer. 2013. Joint Coreference Reso-
lution and Named-Entity Linking with Multi-Pass
Sieves. In Proceedings of EMNLP.

Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bor-
dino, Hagen Fürstenau, Manfred Pinkal, Marc Span-
iol, Bilyana Taneva, Stefan Thater, and Gerhard
Weikum. Robust Disambiguation of Named Entities
in Text. In Proceedings of EMNLP.

Hongzhao Huang, Yunbo Cao, Xiaojiang Huang, Heng
Ji, and Chin-Yew Lin. 2014. Collective Tweet Wik-
ification based on Semi-supervised Graph Regular-
ization. In Proceedings of ACL.

Guillaume Lample, Miguel Ballesteros, Sandeep Sub-
ramanian, Kazuya Kawakami, and Chris Dyer. 2016.
Neural Architectures for Named Entity Recognition.
In Proceedings of NAACL.

Omer Levy and Yoav Goldberg. 2014. Dependency-
Based Word Embeddings. In Proceedings of ACL.

Thang Luong, Hieu Pham, and Christopher D. Man-
ning. 2015. Effective Approaches to Attention-
based Neural Machine Translation. In Proceedings
of EMNLP.

Xuezhe Ma and Eduard H. Hovy. 2016. End-to-end
Sequence Labeling via Bi-directional LSTM-CNNs-
CRF. In Proceedings of ACL.

David Milne and Ian H. Witten. 2008. Learning to Link
with Wikipedia. In Proceedings of CIKM.

Seungwhan Moon, Leonardo Neves, and Vitor Car-
valho. 2018. Multimodal Named Entity Disam-
biguation for Noisy Social Media Posts. In Proceed-
ings of the ACL.

Lev Ratinov, Dan Roth, Doug Downey, and Mike An-
derson. 2011. Local and Global Algorithms for
Disambiguation to Wikipedia. In Proceedings of
NAACL.

Cicero Dos Santos and Bianca Zadrozny. 2014. Learn-
ing Character-level Representations for Part-of-
Speech Tagging. In Proceedings of ICML.

Avirup Sil and Radu Florian. 2016. One for all: To-
wards language independent named entity linking.
In Proceedings of ACL.

Avirup Sil, Gourab Kundu, Radu Florian, and Wael
Hamza. 2018. Neural cross-lingual entity linking.
In Proceedings of AAAI.

Sameer Singh, Amarnag Subramanya, Fernando
Pereira, and Andrew McCallum. 2012. Wikilinks:
A Large-scale Cross-Document Coreference Corpus
Labeled via Links to Wikipedia. Technical report.

Yaming Sun, Lei Lin, Duyu Tang, Nan Yang, Zhen-
zhou Ji, and Xiaolong Wang. Modeling Mention,
Context and Entity with Neural Networks for Entity
Disambiguation. In Proceedings of IJCAI.

Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015.
Character-level Convolutional Networks for Text
Classification. In Proceedings of NIPS.


