



















































Shared common ground influences information density in microblog texts


Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1587–1596,
Denver, Colorado, May 31 – June 5, 2015. c©2015 Association for Computational Linguistics

Shared common ground influences information density in microblog texts

Gabriel Doyle
Dept. of Psychology
Stanford University

Stanford, CA, USA, 94305
gdoyle@stanford.edu

Michael C. Frank
Dept. of Psychology
Stanford University

Stanford, CA, USA, 94305
mcfrank@stanford.edu

Abstract

If speakers use language rationally, they
should structure their messages to achieve
approximately uniform information density
(UID), in order to optimize transmission via
a noisy channel. Previous work identified a
consistent increase in linguistic information
across sentences in text as a signature of the
UID hypothesis. This increase was derived
from a predicted increase in context, but the
context itself was not quantified. We use
microblog texts from Twitter, tied to a sin-
gle shared event (the baseball World Series),
to quantify both linguistic and non-linguistic
context. By tracking changes in contextual
information, we predict and identify grad-
ual and rapid changes in information content
in response to in-game events. These find-
ings lend further support to the UID hypoth-
esis and highlights the importance of non-
linguistic common ground for language pro-
duction and processing.

1 Introduction

There are many ways express a given message in
natural language, so how do speakers decide be-
tween potential structures? One prominent hypothe-
sis is that they aim for structures that best convey the
intendeed message in the context of the communi-
cation. On this view, the use of natural languages is
assumed to follow optimal information transmission
results from information theory (Shannon, 1948).
In particular, speakers should structure their mes-
sages to approximate uniform information density
across symbols (words and phonemes), which is

optimal for transmission of information through a
noisy channel.

At least three lines of evidence suggest that speak-
ers do make choices to increase the uniformity of
information density across their utterances. First,
speakers phonologically reduce more predictable
material (Aylett and Turk, 2004; Aylett and Turk,
2006; Bell et al., 2003). Second, they omit or reduce
optional lexical material in cases where the subse-
quent syntactic information is relatively more pre-
dictable (Levy and Jaeger, 2007; Frank and Jaeger,
2008; Jaeger, 2010). Third, and most relevant to our
current hypothesis, speakers appear to increase the
complexity of their utterances as a discourse devel-
ops (Genzel and Charniak, 2002; Genzel and Char-
niak, 2003; Qian and Jaeger, 2012). We expand on
this finding below.

Following the UID hypothesis, Genzel and Char-
niak 2002 proposed that H(Yi), the total entropy of
part i of a message (e.g., a word) is constant. They
compute this expression by considering Xi, the ran-
dom variable representing the precise word that will
appear at position i, conditioned on all the previ-
ously observed words. They then further factor this
expression into two terms:

H(Yi) = H(Xi|Ci, Li)
= H(Xi|Li)− I(Xi;Ci|Li) (1)

where the first term H(Xi|Li) is the dependence of
the current word on only the local linguistic context
(e.g. within the rest of the sentence Li) and the sec-
ond is the mutual information between the current
word and the broader linguistic context Ci, given
the rest of the current sentence. On their logic, with

1587



greater amounts of contextual information, the pre-
dictability of linguistic material based on context,
I(Xi|Ci, Li), must go up. Therefore, they predicted
that H(Xi|Li) should also increase, so as to main-
tain a constant total amount of information.

Genzel and Charniak then approximated
H(Xi|Li) using a number of methods and showed
that it did increase systematically in documents.
Later work showed that this increase was strongest
within paragraphs and was general across document
types (Genzel and Charniak, 2003) and languages
(Qian and Jaeger, 2012). This work, however,
did not attempt to measure shared context (and its
influence on message expectations) directly. This
challenge is the focus of our current work.

1.1 Contextual effects on complexity
In psycholinguistics, the notion of shared common
ground is a more precise replacement for the general
notion of “context” (Clark, 1996). Common ground
is defined as the knowledge that participants in a dis-
course have and that participants know other partici-
pants have, including the current conversational con-
text. A large literature supports the idea that speak-
ers consider referential context and other linguistic
common ground in selecting the appropriate expres-
sion to refer to a particular physical object (Brennan
and Clark, 1996; Metzing and Brennan, 2003; Dale
and Reiter, 1995; Sedivy et al., 1999). In principle,
Genzel and Charniak’s formulation can be consid-
ered as capturing the relationship between all of the
shared common ground—both linguistic and non-
linguistic—and the predictability of language, even
though in the previous work only linguistic informa-
tion was considered.

When there is both linguistic and non-linguistic
information passing through the noisy channel, the
relevant quantity is not the marginal entropy of only
the linguistic stream but the joint entropy of both
streams. Let Tj be the linguistic information in part
j of the discourse, and Ej be the non-linguistic in-
formation in part j. IfCj is the built-up context from
the preceding parts {1, · · · , j − 1} of the discourse,
then we can break down the joint entropy as:

H(Tj , Ej |Cj)
= H(Tj |Ej , Cj) +H(Ej |Cj)
= H(Tj |Cj)− I(Tj ;Ej |Cj) +H(Ej |Cj)

= H(Tj)− I(Tj ;Cj)
−I(Tj ;Ej |Cj) +H(Ej |Cj)

= H(Tj)− I(Tj ;Ej , Cj) +H(Ej |Cj) (2)
By the UID hypothesis, we expect the left-hand

side of this equation, the information content of each
part of the discourse, to be constant. The first term
of the right-hand side is the out-of-context entropy
of the linguistic information. The second term is the
mutual information of the linguistic information and
the union of the preceding context plus the current
non-linguistic information (the events occurring at
the time). The third term is the entropy of the non-
linguistic information, given the preceding context.

This breakdown suggests that rational participants
in a discourse will exhibit both slow and fast adapta-
tion to context in order to maintain overall constant
entropy. As context slowly builds, the mutual infor-
mation term grows (and the non-linguistic entropy
likely shrinks), resulting in the time-based increase
in H(Tj) that previous work has found. In addition,
an individual event can have high or low information
content given the context, without having a large ef-
fect on the mutual information term. To maintain
constant entropy, high-information events should
be accompanied by low-information linguistic re-
sponses, and vice versa. With an operationalization
of shared context, we should be able to observe these
two types of adaptation directly, not just via the in-
creasing trend shown in previous work (Genzel and
Charniak, 2002; Qian and Jaeger, 2012).

To test this prediction, we leverage Twitter, a pop-
ular microblogging service, to operationalize com-
mon ground. Because of its structure, Twitter is
an ideal platform for this investigation. One com-
mon method of using Twitter is to mark messages
with hashtags, which serve as ad-hoc categories, al-
lowing anyone interested in a topic to find the mes-
sages relevant to that topic. This strategy is espe-
cially used when users are commenting on an exter-
nal event (e.g. a sporting, media, or political event).
We focus here on the World Series of baseball, an
annual sporting event with large viewership and a
single broadcast stream; in this case, the hashtag is
#worldseries. Hashtagged messages are part of
a discourse with extremely limited prior linguistic
context, as no two tweeters will have seen the same
set of tweets. The total shared context with the au-

1588



dience that can be assumed by the writer of a tweet
is the non-linguistic content of the event being hash-
tagged.

We begin by describing our corpus and our
method of calculating linguistic content (by comput-
ing entropy within a simple n-gram model). We then
investigate gradual changes in word-by-word infor-
mation content as the event goes on (testing adap-
tation driven by contextual mutual information in
Equation 2, replicating Genzel and Charniak 2002)
and rapid changes in the total information content
of tweets in response to important in-game events
(testing adaptation driven by non-linguistic informa-
tion in Equation 2). We end by considering con-
trol analyses that provide evidence against alterna-
tive accounts of our results.

2 Corpus and Methods

2.1 #Worldseries Corpus
Our current analysis looked at tweets during the
2014 World Series, a series of seven baseball games
in late October 2014. We obtained these tweets
by searching publicly-available tweets through the
Twitter API, using an adaptation of SeeTweet
(Doyle, 2014) to compile tweets containing the
hashtag #WorldSeries. To synchronize tweets
with game events, we used the Major League Base-
ball Advance Media XML repository,1 which con-
tains pitch-by-pitch data including the ongoing state
of the game and timestamps at the start of each at-
bat. Using this timestamp information, we binned
tweets by at-bats so that they could be co-registered
with other in-game statistics. These bins extend
from the time of the first pitch in an at-bat to the be-
ginning of the next at-bat, and thus provide time for
reactions to the events of the at-bat.2 The mean at-
bat length was 2.76 minutes, and there were 512 to-
tal at-bats. We limited our analysis to tweets times-
tamped during one of these at-bats, resulting in a
total corpus of 109,207 tweets. Each game had its
first pitch at approximately 0008 UTC, and lasted
between three and four hours.

1http://gd2.mlb.com/components/game/mlb/
2We tested a series of potential offset times in case Twitter

and MLB used different clocks or at-bats were not long enough
to capture reactions. We did not adjust the times as there was no
significant increase in the correlation between Leverage Index
(Sect. 5.1) and tweet rate for these offsets.

Our tweet corpus was compiled from the “garden-
hose” Twitter search API, which returns a subset
of all relevant tweets. Our searches captured ap-
proximately 4% of all relevant tweets; Twitter re-
ported 420,329 relevant tweets during Game 1 of
the World Series3, and our dataset contained 17,538
tweets during the same time period. We address po-
tential confounds from this sampling process in Sec-
tion 5.2.

2.2 Entropy Computation

Estimating the linguistic information content of each
tweet is a key task in this work. Social media text
has been described as “bad language” (Eisenstein,
2013): It can be difficult to model due to its idiosyn-
cratic abbreviations, typographic errors, and other
non-standard forms. Relevant to our goal of assess-
ing information content, it can also be difficult to
create an appropriate training corpus for language
models, since the vocabulary and composition of
tweets of change rapidly (Eisenstein, 2013).

We attempted to minimize these difficulties in two
ways. First, we estimated language models with
domain-specific corpora. In particular, for tweets
from each game we used a training corpus consist-
ing of the tweets from all the other games. This
training set provided a vocabulary and structure that
was similar in topic and style to the test set. We re-
moved all punctuation and emoji except word-initial
@ and #, which refer to users and hashtags, re-
spectively. Usernames were replaced with [MEN-
TION] to reduce sparsity; hashtags were not altered,
as these often function as words or phrases within
the tweet’s syntax. Words with fewer than 5 occur-
rences in the training corpus were marked as out-
of-vocabulary items. We estimated trigram models
using a modification of NLTK (Bird, 2006)4 with
Witten-Bell smoothing, and estimated per-word and
total entropy for each tweet from these models.

Second, we included tweet length (in characters)
as an alternative metric of information content (see
Section 5.2). Unless information rate varies sys-

3http://Twitter.com/TwitterData/status/
524972545930301440

4Smoothing on n-gram models in NLTK can be inaccurate
(see http://github.com/nltk/nltk/issues/367),
so we used a modified version courtesy of B. C. Roy (personal
communication).

1589



●

●●
●

●
●

●

●●

●●

●

●
●

●

●●

●

●

●●

●

●

●

●

●

●

●

●

●

●
●●●

●
●●

●
●

●

●

●

●

●

●

●

●

●

●
●
●

●●●
●

●

●
●●

●

●

●

●

●

●

●

●

●

●

●

●

●●
●

5.0

5.5

6.0

6.5

7.0

0 1 2 3 4
Time (hrs)

P
er

−
w

or
d 

en
tr

op
y

Game #

● 1

2

3

4

5

6

7

Time vs. per−word entropy

Figure 1: Per-word entropy increases with time for the
first two hours of the games, then levels off and slightly
declines. Color reflects in-game time; line shows loess fit
with 95% confidence intervals.

tematically and substantially across tweets of dif-
ferent lengths—counter to existing results suggest-
ing uniform information density operates at multi-
ple structural levels (e.g., Qian and Jaeger 2009)—
longer tweets will generally carry more information.

3 Gradual Changes in Information Rate

Our first analytic goal was to examine changes in
the information content of tweets due to the long-
term build-up of context in a shared event. We pre-
dicted that we would see similar developments in
information structure as in more traditional conver-
sational settings, even though there was no formal
conversation or explicit linguistic history to develop
common ground. Specifically, we predicted that the
build-up of contextual information would cause the
context-independent per-word entropy to rise over
time, replicating the effect that has been observed
across languages and genres (Genzel and Charniak,
2003; Qian and Jaeger, 2012).

Figure 1 shows evidence for changes in per-word
entropy over the course of games. Per-word entropy
rises throughout in the first two hours of each game,
slowly levels off and finally declines slightly over
time. This pattern is consistent with the constant en-
tropy rate proposal of Genzel and Charniak 2002,
and more specifically with the context decay model
of Qian and Jaeger 2012.5

5A late decline in per-word entropy also appeared in Qian
and Jaeger 2012’s analysis of Swedish.

We used mixed-effects linear regression to quan-
tify this relationship, using the time of an at-bat to
predict both per-word and per-tweet entropy during
the at-bat. Specifically, we used the logarithm of
time as our fixed-effect predictor, per the context-
decay models of Qian and Jaeger 2012. We added
game-specific random intercepts and slopes of log-
time to capture cross-game variation. This model
showed significant positive effects of time on en-
tropy, using likelihood-ratio tests for both models
(per-word entropy: .348 ± .045; p < .001, χ2(3) =
104.6, per-tweet entropy: 10.31 ± 2.08; p =
.001, χ2(3) = 74.65).

We hypothesize that this finding—greater linguis-
tic entropy for later tweets—is due to the accrual
of common ground across users from shared non-
linguistic information. As they watch more of the
game, they share more referents and have stronger
expectations about what aspects of the game will
be discussed. This shared common ground licenses
more complex language and more sophisticated lin-
guistic references. Table 1 gives example tweets
at different time points; as a game progresses, ref-
erences can expand from generic references to the
teams or series, to specific individuals and events,
and eventually to sequences of events.

While this finding is consistent with previous
work on the effect of context, it expands the defini-
tion of context. In previous work, the context came
from explicit linguistic information built up through
paragraphs in a formally-structured, written docu-
ment. In the Twitter dataset, the context comes from
real-world events during the games, as there is no
canonical shared sequence of tweets that the tweet-
ers can refer back to (indeed, two random users of
the #Worldseries hashtag probably have relatively
little Twitter context in common). In sum, contex-
tual influences on entropy need not be explicitly lin-
guistic, so long as discourse participants have rea-
son to believe that the other participants share their
knowledge.

4 Fast Changes In Information Content

Intuitively, after an exciting, game-changing event,
tweets will be shorter and make more reference to
the shared knowledge that this event has just hap-
pened. Such events should also generate more re-

1590



Minute Tweet Per-word entropy

0 It’s finally here! #WorldSeries 4.74

0 #WorldSeries Play Ball 4.96

0
IDEA: @mayoredlee, #SanFrancisco can pledge to throw our @SFGiants
an #OrangeOctober parade regardless of #WorldSeries outcome! #SFGiants

8.20

12 The guy with the Marlins sweater is behind home plate again. #worldseries 4.26

12 The Giants 3-0! #WorldSeries 5.43

12
Something about Hunter Pence really, really bothers me. Don’t ask me
what, cause I havent figured it out, but I don’t like him. #WorldSeries

6.64

73
Three HORRIBLE at-bats (mixed in with Cain’s walk) prevent Royals from
breaking through in the third. #WorldSeries

9.39

130 As Hardy Boy #2, Joe Panik just pulled the mask off of Vargas and discov-
ered it’s Old Man Withers from down the street. #WorldSeries

8.12

178 #WorldSeries it’s funny the non body names have a great hits. Frm now n
on consider the Postseson as Cinderla run. No names needed, #MLB

10.04

Table 1: Example tweets, grouped by minutes since the first pitch.

●●

●

●
●

●

●

●

●
●

● ●

●
●

●

●

●● ●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●
●

●

●
●

● ● ●

●
●

●

●

●
●

●

●

●

●

●●
●

●

●

●

●

●

●

●

●

●●●
●

●
●

●

●

●

●

●

● ●
●

●

●

●
●

●

●

●

●

●

●

●

●
●●

●

●

●

●

●

●
●

●●

●

●
●

●

●

● ●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

● ●

●●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●●●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●
●

● ●

●
●

● ●
●

●

●

●●

●
●

●

●

●
●

●

●

●

●

●
●

●

●

●

●
●

●

●

●●

●

●

●

●

●
●

●

●

●

●
●

●

●

●

●

●

●

●●

●

● ●

●●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●●

●

●

●

●

●

●
●

● ●

●

●

●
●

●

●

●

●

●

●

●

●● ●

●

●

●

●

●

●

●

●

●

●

●

●● ●

●
● ●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●
●

●
●

●

●

●

●●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●
●

●

●

●
●

●

●
●

●

●

●

●●
●

●

●

●

● ●●

●

● ●

●

●

●●

●

●
●

●

●

●
●

●

●

●

●

● ●

●

●
●

●

●

●
●

●

●

●
●

●

●

●●

●

●

●

●

●

●

●
●

●

●

●

●

●
●

●

●

●

● ●

● ●

●

●

●

●

●

●●

●

●

●

●

●
● ●

●

●

●

●●

●

●

●

●

●

●

●●

●

●
●

●

●

● ●

● ●

●

●

●
●

●●

●● ●

●

50

70

90

110

130

1.0 1.5 2.0 2.5 3.0
Log_10 Tweet Rate

P
er

−
tw

ee
t e

nt
ro

py

1

2

3

Time
(hrs)

Log tweet rate vs. per−tweet entropy

Figure 2: Total tweet entropy plotted against log tweet
rate. Color reflects in-game time; line shows loess fit with
95% confidence intervals.

sponses, suggesting that the number of tweets per
unit time can serve as a proxy for the information
content of an event. This relationship is captured by
Equation 2, in which unexpected events have large
information content, so linguistic information con-
tent should be reduced correspondingly to maintain
constant entropy. Our next set of analyses test this
relationship.

The examples shown in Table 2 provide anecdotal
evidence for the hypothesized relationship between

in-game events and linguistic complexity, with ex-
amples of consecutive tweets from high-rate and
low-rate at-bats, along with their information con-
tent. The top triplet comes from one of the highest-
rate at-bats, in which Gregor Blanco committed a
crucial error in the last inning of the last game. The
bottom triplet comes from a low-rate at-bat, mid-
game, with one team well ahead of the other; in this
case, tweets all refer to different events as there is no
single salient shared event.

We quantified the predicted relationship by again
fitting a mixed-effect linear regression model, in this
case using the logarithm of per-minute tweet rate
as a predictor of tweet entropy. Given its signifi-
cance in the previous model, we included log(time)
as a control factor in this analysis, and added by-
game random intercepts and slopes for log(rate) and
log(time). The log of the tweet rate had a sig-
nificant negative effect on per-word and per-tweet
entropy by likelihood-ratio tests (per-word-entropy:
−.333 ± .073; p < .001, χ2(4) = 59.37, per-tweet-
entropy: −21.82± 2.43; p < .001, χ2(4) = 194.6).

Log(time) retained significance (p < .001) as
a predictor for both entropy measures even when
rate was accounted for, showing evidence for both

1591



Log
rate

Tweet Per-word entropy

2.49 Holy shitballs, @Royals! #WorldSeries #Game7 3.99

2.49 Just when you thought the #WorldSeries was over.... #E8 4.76

2.49 Fuck you, Blanco. #Giants #WorldSeries 5.54

1.66 Lets Go Giants!!! 5-0 #SFGiants #WorldSeries 3.26

1.66
The guy in Marlins gear behind home plate needs to escorted off property
for annoying everybody. #WorldSeries #WhoDoesThat

4.85

1.66
I suppose I appreciate Bochy’s “ASG” approach with Bumgarner. Of course,
who are any of us to question him in late October? #WorldSeries

7.42

Table 2: Example tweets, grouped by the per-minute tweet rate during each at-bat.

slow and fast adaptation occurring in the discourse.
The effects are both in the predicted directions: En-
tropy increases with time as more informative con-
text builds up, but decreases with tweet rate as
more exciting events encourage less information-
laden tweets.

5 Control Analyses

5.1 Non-Rate Metrics of Context

Since tweet rate is an organic reflection of the in-
terest accrued by in-game events, it is an impor-
tant metric for examining fast adaptation. Never-
theless, it could be confounded with other factors
influencing tweet production. For instance, there is
evidence that online interactions exhibit rational re-
sponses to information overload, the state where the
amount of incoming information exceeds a user’s
ability to process it (Miller, 1956; Schoberth et al.,
2003). Previous investigations into forum posting
behavior have shown that users adapt to overload by
posting shorter messages (Jones et al., 2001b; Jones
et al., 2001a; Whittaker et al., 2003; Schoberth et al.,
2003), and a similar result was found for the more
explicitly conversational setting of IRC chat chan-
nels (Jones et al., 2008).

To show that the changes in information con-
tent are not merely reactions to increased tweet
competition—that they have independent informa-
tional motivations—we need metrics of event im-
portance and predictability that are not dependent on
social media behavior. Luckily, baseball has a long
history of statistical analysis, and as a result, there

●

● ●
●

●●

● ●
●

● ●

●

●●

●●

●

●

●●
●

●

●

●
●

●

●

●
●

●

●

●

●

●

●

●
●●

●

●

●

●

●

●

●●

●

●

●

●

●

●
●●

●

●

●●

●
●

●

●

●
●

●
●

●

●●

●

●

●

●●

40

60

80

100

120

0.0 0.1 0.2
WPA

P
er

−
tw

ee
t e

nt
ro

py

Game #

● 1

2

3

4

5

6

7

Win Prob. Added vs. per−tweet entropy

Figure 3: Total entropy decreases for at-bats with greater
win probability changes. Loess curve fitting with 95%
confidence intervals.

are independently-derived metrics that fit this bill.
Two that are appropriate for this purpose are Lever-
age Index (LI)6 and Win Probability Added (WPA)
(Tango et al., 2007).

LI is an estimate of how critical an at-bat is to the
outcome of the game. It is based on the difference
in resultant win probability if the current batter gets
a hit or an out, normalized by the mean change in
win probability over all at-bats. 1 is the average LI,
and greater LI indicates greater importance. LI, as a
measure of the expected change in win probability,
is similar to non-linguistic entropy term in Equation
2.

WPA depends on the result of an at-bat, and es-

6http://www.hardballtimes.com/
crucial-situations/

1592



timates how much the win probability changed as a
result of what happened during the at-bat. WPA thus
provides an estimate of how much information about
the game outcome this at-bat has provided, condi-
tioned on the current game context. These mea-
sures are well-correlated (Kendall’s τ = .77), since
a high-LI at-bat’s value comes from its ability to af-
fect win probability.

As high LI or WPA values indicate an at-bat
whose result has a large effect on the game, these
metrics provide an estimate for non-linguistic infor-
mativity that is independent of medium-specific in-
fluences on tweet production. To assess their ef-
fects, we constructed four mixed-effects linear re-
gression models, using LI and WPA to predict per-
word and per-tweet entropy in all pairwise combi-
nations (we built separate models for LI and WPA
due to their high collinearity). Fixed- and by-game
random-effects of log(time) and log(rate) were in-
cluded as controls in all models; if there is an effect
of LI or WPA beyond the effect of rate, this effect
can be interpreted as evidence of speaker adaptation
to non-linguistic information content.

Both LI and WPA had significant negative ef-
fects on per-tweet entropy (LI: −1.52 ± .43; p =
.001, χ2(5) = 20.1, WPA: −2.27 ± .40; p <
.001, χ2(5) = 44.18), over and above the effect of
tweet rate. Per-word entropy did not show a signifi-
cant effect of LI or WPA when rate was included as
a control factor. Each was a significant factor on per-
word entropy (p = .008, p = .005) when rate was
not included as a control, though, suggesting that the
explanatory power of these independent metrics may
be subsumed in the more complex factor of tweet
rate.

5.2 Speaker Normalization

A second alternative hypothesis for the observed be-
havioral changes with tweet rate is that they arise
not from changes in the behavior of individuals but
rather from a change in demographics. It is plausible
that rising tweet rates come from an influx of new
tweeters using the hashtag, and that these new tweet-
ers simply produce shorter, less informative tweets
in general. For instance, spambots often include
trending hashtags in their spam tweets (Martinez-
Romo and Araujo, 2013). To account for this, we
treated the users whose tweets are in our corpus as

●

● ●

●
●●

●

●
●

●

●

●
●●

●●
●

●

●

●

●

●
●

●

● ●

●

●

●

●

●

●

●

●

●●

●

●

●●

●

●

●

●

●

●

●

●

●

●

●

●

●●

●

●

●

●

●●

●

●
●

●
●
●

●
●
●
●

●

●

●

●

−40

−20

0

20

0.0 0.1 0.2
WPA

C
ha

ra
ct

er
s 

ab
ov

e 
av

er
ag

e Game #
● 1

2

3

4

5

6

7

Win Prob. Added vs. normalized tweet length

Figure 4: Speaker-normalized tweet length also decreases
for at-bats with greater win probability changes. Loess
curve fitting with 95% confidence intervals.

a “computational focus group” (Lin et al., 2013; Lin
et al., 2014), and used the Twitter API to collected a
further 100 tweets from each user outside the time-
frame of the games. We used these tweets to esti-
mate an average tweet length for each user, and sub-
tracted this value from the length of their #world-
series tweets during the games.7 If this baselined
metric displays the same effects as shown above, we
have reason to believe that users are in fact changing
their individual behaviors in response to information
factors, rather than that a demographic shift is mim-
icking a behavioral shift.

For this analysis, we created a mixed-effects
model with WPA, log(rate) and log(time) as predic-
tors of tweet length. All three factors were signifi-
cant (WPA: −1.64 ± .36; p < .001, χ2(5) = 72.3;
log(rate): −6.15 ± .47; p < .001, χ2(5) = 303.6;
log(time): .82 ± .40; p = .001, χ2(5) = 20.6). We
then created a second model using the same factors
to predict the mean change in tweet length from the
baseline length. Again, all three factors were signif-
icant (WPA: −2.01 ± .29; p < .001, χ2(5) = 70.2;
log(rate): −5.10 ± .49; p < .001, χ2(5) = 252.6;
log(time): .61 ± .35; p = .016, χ2(5) = 14.0).
By ruling out demographic shifts (e.g., an influx
of terser tweeters), this analysis provides additional
support for the idea that tweeters indeed shift their
behavior in response to in-game information.

7Note that these analyses are conducted over tweet length,
rather than total entropy, as there was no obvious way of nor-
malizing entropy by speaker.

1593



6 Discussion

We investigated the hypothesis that speakers opti-
mize their language production so as to approximate
uniform information density, a signature of efficient
communication through a noisy channel (Shannon,
1948; Levy and Jaeger, 2007). Previous work had
observed indirect evidence for UID via increases in
linguistic complexity (which were hypothesized to
reflect increasing discourse/contextual knowledge),
but this work neither measured contextual informa-
tion directly nor included non-linguistic measures
of context (Genzel and Charniak, 2002; Genzel and
Charniak, 2003; Qian and Jaeger, 2012). Our cur-
rent work takes a first step towards addressing these
issues by using microblog texts around shared events
(baseball games) as a case study in which a known
context can be characterized more precisely. With
this approach, we find systematic differences in in-
formation rate and total information content as a
function of nonlinguistic factors.

We successfully replicated the effect found in pre-
vious work: a gradual increase in entropy rate over
the course of individual baseball games. But in
addition to this effect, we found a striking pattern
of short-timescale changes in total message entropy
(reflected in the changing lengths of messages).
When in-game events were exciting, unpredictable,
and outcome-relevant (hence, highly informative),
message length and total entropy went down. This
regularity suggests that Twitter users were regulat-
ing the information content of their messages rela-
tive to the total communicative content of the con-
text more broadly, a prediction that can be derived
directly from the UID model.

Our work highlights the importance of non-
linguistic context for the informational content of
language. This relationship is widely acknowledged
in theories of pragmatic communication (Grice,
1975; Sperber and Wilson, 1986; Clark, 1996; Frank
and Goodman, 2012), but has been largely ab-
sent in information-theoretic treatments of linguis-
tic complexity. The omission of this information
has largely been for pragmatic, rather than theoret-
ical, reasons: As Genzel and Charniak 2002 note,
it is typically very difficult to compute semantic—
let alone non-linguistic—information content. Our
work suggests that internet communications sur-

rounding shared media events may be a promising
source of grounded language use where context can
be quantified more effectively due to the existence
of substantial metadata.

A growing literature suggests that the information
content of language is the critical variable for un-
derstanding processing difficulty in language com-
prehension (Levy, 2008; Demberg and Keller, 2008;
Boston et al., 2008; Smith and Levy, 2013). Un-
der surprisal theory (Hale, 2001; Levy, 2008), the
overall predictability of individual elements of lan-
guage is assumed to be due to a predictive model
of its likelihood in the current context. Given this
model of processing difficulty, our work here makes
a strong prediction: that the information processing
difficulty of a word or sentence should track with its
total information content (including its relationship
to the non-linguistic context), rather than its linguis-
tic information content alone. Some preliminary ev-
idence supports this idea. In a study of the process-
ing complexity of negative utterances, Nordmeyer
and Frank 2014 found that the processing cost of
negation was predicted by the surprisal of encoun-
tering the negation in a particular pragmatic context.
But future work should test this hypothesis across a
wider variety of structures and contexts.

In sum, our work contributes to the growing
body of evidence in favor of the UID hypothesis.
The mechanisms underlying the tendency to regu-
late information content are still unknown, however.
While UID would follow from a strong form of au-
dience design, in which speakers explicitly consider
the processing difficulty of different content (Clark,
1996), the UID hypothesis could also emerge from
simpler production processes. Untangling these pos-
sibilities will not be trivial. Regardless of the reso-
lution of this issue, however, UID appears to be an
important descriptive tool in capturing how speakers
make production choices.

Acknowledgments

We gratefully acknowledge the support of ONR
Grant N00014-13-1-0287.

1594



References
Matthew Aylett and Alice Turk. 2004. The smooth sig-

nal redundancy hypothesis: A functional explanation
for relationships between redundancy, prosodic promi-
nence, and duration in spontaneous speech. Language
and Speech, 47(1):31–56.

Matthew Aylett and Alice Turk. 2006. Language redun-
dancy predicts syllabic duration and the spectral char-
acteristics of vocalic syllable nuclei. The Journal of
the Acoustical Society of America, 119(5):3048–3058.

Alan Bell, Daniel Jurafsky, Eric Fosler-Lussier, Cynthia
Girand, Michelle Gregory, and Daniel Gildea. 2003.
Effects of disfluencies, predictability, and utterance
position on word form variation in English conversa-
tion. The Journal of the Acoustical Society of America,
113(2):1001–1024.

Steven Bird. 2006. NLTK: the natural language toolkit.
In Proceedings of the COLING/ACL on Interactive
presentation sessions, pages 69–72. Association for
Computational Linguistics.

Marisa Boston, John Hale, Reinhold Kliegl, Umesh Patil,
and Shravan Vasishth. 2008. Parsing costs as pre-
dictors of reading difficulty: An evaluation using the
potsdam sentence corpus. Journal of Eye Movement
Research, 2(1):1–12.

Susan E Brennan and Herbert H Clark. 1996. Concep-
tual pacts and lexical choice in conversation. Journal
of Experimental Psychology: Learning, Memory, and
Cognition, 22(6):1482.

Herbert H Clark. 1996. Using language, volume 1996.
Cambridge University Press Cambridge.

Robert Dale and Ehud Reiter. 1995. Computational
interpretations of the Gricean maxims in the gener-
ation of referring expressions. Cognitive Science,
19(2):233–263.

Vera Demberg and Frank Keller. 2008. Data from eye-
tracking corpora as evidence for theories of syntactic
processing complexity. Cognition, 109(2):193–210.

Gabriel Doyle. 2014. Mapping dialectal variation by
querying social media. In Proceedings of the Euro-
pean Chapter of the Association for Computational
Linguistics.

Jacob Eisenstein. 2013. What to do about bad language
on the internet. In Proceedings of the 2013 Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics: Human Language
Technologies, pages 359–369.

Michael C Frank and Noah D Goodman. 2012. Predict-
ing pragmatic reasoning in language games. Science,
336(6084):998–998.

Austin Frank and T Florian Jaeger. 2008. Speaking ratio-
nally: Uniform information density as an optimal strat-
egy for language production. In Proceedings of the

30th Annual Meeting of the Cognitive Science Society,
pages 933–938. Cognitive Science Society Washing-
ton, DC.

Dmitriy Genzel and Eugene Charniak. 2002. Entropy
rate constancy in text. In Proceedings of the 40th
Annual Meeting of the Association for Computational
Linguistics, pages 199–206. Association for Computa-
tional Linguistics.

Dmitriy Genzel and Eugene Charniak. 2003. Variation
of entropy and parse trees of sentences as a function of
the sentence number. In Proceedings of the 2003 Con-
ference on Empirical Methods in Natural Language
Processing, pages 65–72. Association for Computa-
tional Linguistics.

H Paul Grice. 1975. Logic and conversation. Syntax and
Semantics, 3:41–58.

John Hale. 2001. A probabilistic earley parser as a psy-
cholinguistic model. In Proceedings of the 2nd Meet-
ing of the North American Chapter of the Association
for Computational Linguistics on Language Technolo-
gies, pages 1–8. Association for Computational Lin-
guistics.

T Florian Jaeger. 2010. Redundancy and reduction:
Speakers manage syntactic information density. Cog-
nitive Psychology, 61(1):23–62.

Quentin Jones, Gilad Ravid, and Sheizaf Rafaeli. 2001a.
Empirical evidence for information overload in mass
interaction. In CHI’01 Extended Abstracts on Human
Factors in Computing Systems, pages 177–178. ACM.

Quentin Jones, Gilad Ravid, and Sheizaf Rafaeli. 2001b.
Information overload and virtual public discourse
boundaries. In INTERACT’01: 13th International
Conference on Human-Computer Interaction, page 43.
IOS Press.

Quentin Jones, Mihai Moldovan, Daphne Raban, and
Brian Butler. 2008. Empirical evidence of informa-
tion overload constraining chat channel community in-
teractions. In Proceedings of the 2008 ACM Confer-
ence on Computer Supported Cooperative Work, pages
323–332. ACM.

Roger Levy and T Florian Jaeger. 2007. Speakers opti-
mize information density through syntactic reduction.
In Advances in Neural Information Processing Sys-
tems, pages 849–856.

Roger Levy. 2008. Expectation-based syntactic compre-
hension. Cognition, 106(3):1126–1177.

Yu-Ru Lin, Drew Margolin, Brian Keegan, and David
Lazer. 2013. Voices of victory: A computational fo-
cus group framework for tracking opinion shift in real
time. In Proceedings of the 22nd international confer-
ence on World Wide Web, pages 737–748. International
World Wide Web Conferences Steering Committee.

1595



Yu-Ru Lin, Brian Keegan, Drew Margolin, and David
Lazer. 2014. Rising tides or rising stars?: Dynam-
ics of shared attention on twitter during media events.
PLoS One, 9(5):e94093.

Juan Martinez-Romo and Lourdes Araujo. 2013. Detect-
ing malicious tweets in trending topics using a statis-
tical analysis of language. Expert Systems with Appli-
cations, 40(8):2992–3000.

Charles Metzing and Susan E Brennan. 2003. When
conceptual pacts are broken: Partner-specific effects
on the comprehension of referring expressions. Jour-
nal of Memory and Language, 49(2):201–213.

George A Miller. 1956. The magical number seven, plus
or minus two: some limits on our capacity for process-
ing information. Psychological Review, 63(2):81.

Ann E Nordmeyer and Michael C Frank. 2014. A prag-
matic account of the processing of negative sentences.
In Proceedings of the 36th Annual Meeting of the Cog-
nitive Science Society.

Ting Qian and T Florian Jaeger. 2009. Evidence for effi-
cient language production in Chinese. In Proceedings
of the 31st Annual Meeting of the Cognitive Science
Society.

Ting Qian and T Florian Jaeger. 2012. Cue effective-
ness in communicatively efficient discourse produc-
tion. Cognitive Science, 36(7):1312–1336.

Thomas Schoberth, Jennifer Preece, and Armin Heinzl.
2003. Online communities: A longitudinal analysis
of communication activities. In Proceedings of the
36th Annual Hawaii International Conference on Sys-
tem Sciences, pages 10–18. IEEE.

Julie C Sedivy, Michael K Tanenhaus, Craig G Cham-
bers, and Gregory N Carlson. 1999. Achieving in-
cremental semantic interpretation through contextual
representation. Cognition, 71(2):109–147.

Claude E Shannon. 1948. Bell system tech. j. 27 (1948)
379; ce shannon. Bell System Tech. J, 27:623.

Nathaniel J Smith and Roger Levy. 2013. The effect
of word predictability on reading time is logarithmic.
Cognition, 128(3):302–319.

Dan Sperber and Deirdre Wilson. 1986. Relevance:
Communication and Cognition. Harvard University
Press, Cambridge, MA.

Tom M Tango, Mitchel G Lichtman, and Andrew E Dol-
phin. 2007. The book: Playing the percentages in
baseball. Potomac Books, Inc.

Steve Whittaker, Loen Terveen, Will Hill, and Lynn
Cherny. 2003. The dynamics of mass interaction. In
From Usenet to CoWebs, pages 79–91. Springer.

1596


