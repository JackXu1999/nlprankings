








































Evaluating Low-Level Speech Features Against Human Perceptual Data

Caitlin Richter
Dept. of Linguistics

University of Pennsylvania
ricca@sas.upenn.edu

Naomi H. Feldman
Dept. of Linguistics and UMIACS

University of Maryland
nhf@umd.edu

Harini Salgado
Dept. of Computer Science

Pomona College
harini.salgado@gmail.com

Aren Jansen
HLTCOE

Johns Hopkins University
arenjansen1@gmail.com

Abstract

We introduce a method for measuring the corre-
spondence between low-level speech features
and human perception, using a cognitive model
of speech perception implemented directly on
speech recordings. We evaluate two speaker
normalization techniques using this method
and find that in both cases, speech features
that are normalized across speakers predict hu-
man data better than unnormalized speech fea-
tures, consistent with previous research. Re-
sults further reveal differences across normal-
ization methods in how well each predicts hu-
man data. This work provides a new frame-
work for evaluating low-level representations
of speech on their match to human perception,
and lays the groundwork for creating more eco-
logically valid models of speech perception.

Index Terms: speech perception, automatic speech
recognition, speaker normalization, cognitive models

1 Introduction

Understanding the features that listeners extract from
the speech signal is a critical part of understand-
ing phonetic learning and perception. Different fea-
ture spaces imply different statistical distributions
of speech sounds in listeners’ input (Figure 1), and
these statistical distributions of speech sounds influ-
ence speech perception in both adults and infants
(Clayards et al., 2008; Maye et al., 2002).

The performance of automatic speech recognition
(ASR) systems is also affected by the way in which
these systems represent the speech signal. For ex-
ample, changing the signal processing methods that

are used to extract features from the speech wave-
form (Hermansky, 1990; Hermansky and Morgan,
1994) and applying speaker normalization techniques
to these features (Wegmann et al., 1996; Povey and
Saon, 2006) can improve the performance of a recog-
nizer. Recently there has been considerable interest
in representation learning, in which new features that
are created through exposure to data from the lan-
guage to be recognized, or through exposure to other
languages, lead to better system performance (Grézl
et al., 2014; Heigold et al., 2013; Kamper et al., 2015;
Sercu et al., 2016; Thomas et al., 2012; Wang et al.,
2015). It is potentially useful to know how closely
the feature representations in ASR resemble those
of human listeners, particularly for low-resource set-
tings, where systems rely heavily on these features
to guide generalization across speakers and dialects.

In this paper we introduce a method for measuring
the correspondence between low-level speech fea-
ture representations and human speech perception.
This allows us to compare different feature spaces
in terms of how well the locations of sounds in the
space can predict listeners’ perception of acoustic
similarity. Our method has potential relevance both
in ASR, for understanding how well the feature rep-
resentations in ASR systems capture the similarity
structure that guides human perception, and in cogni-
tive science, for evaluating hypotheses regarding the
feature spaces that human listeners use.

We evaluate the ability of a feature space to capture
listeners’ same-different responses in AX discrimi-
nation tasks, in which listeners hear two sounds and
decide whether they are acoustically identical. Rather
than assuming that listeners’ ability to discriminate

425

Transactions of the Association for Computational Linguistics, vol. 5, pp. 425–440, 2017. Action Editor: Eric Fosler-Lussier.
Submission batch: 11/2016; Revision batch: 3/2017; Published 11/2017.

c©2017 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license.



F1

400 600 800 1000 1200 1400

F
2

500

1000

1500

2000

2500

3000

3500
Unnormalized Vowels

Normalized F1

-2 -1 0 1 2 3

N
o
rm

a
li
z
e
d
 F

2

-2

-1.5

-1

-0.5

0

0.5

1

1.5

2

2.5
Normalized Vowels

/æ/ (bat)
/a/ (bot)
/ɔ/ (bought)
/ɛ/ (bet)
/e/ (bait)
/ɝ/ (Bert)
/ɪ/ (bit)
/i/ (beet)
/o/ (boat)
/ʊ/ (book)
/ʌ/ (but)
/u/ (boot)

Figure 1: Acoustic characteristics of vowels produced in hVd contexts by men, women, and children from Hillenbrand et
al. (1995), plotted as raw formant frequencies (left) and z-scored formant frequencies (right). These feature spaces yield
different distributions of sounds in acoustic space. If listeners’ perception is biased toward peaks in these distributions,
the feature spaces make different predictions for listeners’ behavior in perceptual discrimination tasks.

sounds is directly related to those sounds’ distance in
a feature space, we instead adopt a cognitive model
of speech perception which predicts that listeners’
perception of sounds is biased toward peaks in the
distribution of sounds in their input. This leads listen-
ers to perceive some sounds as closer together in the
feature space than they actually are, and others as far-
ther apart (Figure 2). The model has previously been
shown to accurately predict listeners’ same-different
discrimination judgments when listening to pairs of
sounds (Feldman et al., 2009; Kronrod et al., 2016).

Our innovation in this work is to estimate the dis-
tribution of sounds in the input from a corpus, using
different feature spaces. Under the model, listeners
are expected to bias their perception toward peaks
in the distribution of sounds in the corpus. Those
peaks occur in different locations for different fea-
ture spaces. Thus, different feature representations
yield different predictions about listeners’ discrimina-
tion. Features that yield a better match with listeners’
discrimination are assumed to more closely reflect
the way in which listeners generalize their previous
experience when perceiving speech.

In addition to providing a way to evaluate feature
representations, adapting a cognitive model to oper-
ate directly over speech recordings lays the ground-
work for building more ecologically valid models of
speech perception, by enabling cognitive scientists to
make use of the same rich corpus data that is often
used by researchers in ASR. These speech corpora
will allow models to be trained and tested on data

that more faithfully simulate the speech environment
that listeners encounter, rather than on artificially
simplified data.

As an initial case study, we use the perceptual
model to evaluate features derived from two speaker
normalization algorithms, which aim to reduce vari-
ability in the speech signal stemming from physical
characteristics of the vocal tract. We compare these
normalized features to a baseline set of unnormalized
features. Speaker normalization has previously been
found to improve phonetic categorization (Cole et
al., 2010; Nearey, 1978), increase a phonetic catego-
rization model’s match with human behavior (Apfel-
baum and McMurray, 2015; McMurray and Jongman,
2011), and improve the performance of speech recog-
nizers (Povey and Saon, 2006; Wegmann et al., 1996).
However, the degree to which specific normalization
techniques from ASR match human perception is
not yet known. Experiments in this paper replicate
the general benefit of speaker normalization using
our perceptual model, while also providing new data
on the degree to which different normalization algo-
rithms from ASR capture information that is similar
to what humans use in perceptual tasks.

The paper is organized as follows. We begin by
characterizing the speech features tested in this paper.
The following section describes the method we use
for evaluating these features against human discrimi-
nation data. Experiments are presented testing how
well each representation predicts human data. We
compare our method to previous work, and conclude

426



Actual Stimulus

Perceived Stimulus

Figure 2: Illustration from Feldman et al. (2009) show-
ing listeners’ perceptual bias toward peaks in their prior
distribution over sounds in a feature space. This bias
leads listeners to perceive some sounds as closer together,
and other sounds as farther apart, than they actually are.
Reprinted from “The influence of categories on perception:
Explaining the perceptual magnet effect as optimal statis-
tical inference” by N. H. Feldman, T. L. Griffiths, & J. L.
Morgan, 2009, Psychological Review, 116, p. 757. Copy-
right 2009 by the American Psychological Association.
Reprinted with permission.

by discussing implications and future directions.

2 Speech features

Speech recognition systems typically consist of a
front-end, which transforms short time windows of
the speech signal into numeric feature vectors, and a
back-end, which uses the output of the front-end sys-
tem together with phonetic models, language models,
dictionaries, and so on to infer what was said. Our
work analyzes the front-end feature vectors directly,
examining the effects of speaker normalization algo-
rithms on these feature vectors.

Speech contains commingled effects of linguis-
tic, paralinguistic, and purely physical sources of
variation. It has often been proposed that human
listeners normalize the speech signal to generalize
phonetic content across speakers. Listeners could
normalize for vocal tract length, for example, either
by attending to cues that are relatively constant across
speakers (Miller, 1989; Monahan and Idsardi, 2010;
Peterson, 1961) or by relying on relative, rather than
absolute, encoding of cues that are affected by vocal
tract length (Barreda, 2012; Cole et al., 2010). Alter-
natives to normalization include theories of adapta-
tion in which listeners create representations that are

specific to individual speakers or groups of speakers
(Dahan et al., 2008; Kleinschmidt and Jaeger, 2015),
and theories which argue that no normalization or
adaptation mechanisms are necessary at all (Johnson,
1997; Johnson, 2006). The empirical literature on
speaker normalization and adaptation includes results
that support each of these approaches.

Our focus here is on replicating an advantage of
normalized features found in cognitive models of
categorization. McMurray and Jongman (2011) and
Apfelbaum and McMurray (2015) conducted a direct
comparison between normalized and unnormalized
encodings of cues to fricative categorization. They
found that using normalized features yields a bet-
ter match with human categorization performance.
We aim to replicate the advantage of normalized
over unnormalized features using our discrimination
model, in order to validate our new evaluation method
against an existing cognitive modeling approach.

We test two methods for speaker normalization:
vocal tract length normalization (VTLN), which is
widely used in ASR systems (Wegmann et al., 1996);
and z-scoring, which has been applied in linguis-
tic analyses as well as in ASR systems (Lobanov,
1971; Viikki and Laurila, 1998). Neither VTLN
nor z-scoring is a cognitive model of speaker nor-
malization, but both have the potential to capture
information similar to what listeners might obtain if
they generalize across utterances from speakers with
different vocal tract lengths.

We apply these normalization methods to vowels
that are represented by mel frequency cepstral coeffi-
cients (MFCCs), which have been widely employed
as an input representation in ASR systems (Davis and
Mermelstein, 1980). MFCCs are a 12-dimensional
vector that describe a timepoint of speech by captur-
ing information about the spectral envelope, reflect-
ing vocal tract shape. They are computed by taking
the discrete time Fourier transform of the speech sig-
nal, mapping it onto a mel frequency scale with log
power (to reflect how people process frequency and
loudness), and applying the discrete cosine transform
to result in a cepstrum (inverse spectrum) whose real
values are the MFCC representation. This represen-
tation of the signal is indirectly related to formant
frequencies, which are peaks in the spectral envelope
that are known to correlate with vowel identity and
are often used to characterize vowels in studies of hu-

427



man speech perception. However, MFCCs have the
advantage that they are computed deterministically
from the speech signal, and thus are not subject to
the error inherent in automatic formant tracking.

This section gives an overview of both normaliza-
tion methods and characterizes their effects; we defer
details of their technical implementation to Section 4.

2.1 Vocal tract length normalization

The first normalization method we test, vocal tract
length normalization (VTLN), is a technique devel-
oped for automatic speech recognition (Cohen et al.,
1995; Wegmann et al., 1996). The aim of VTLN is
to adjust the corpus so that it is as if all the speakers
had identical vocal tract lengths. VTLN compensates
for speaker differences in vocal tract length, which
affects the resonant frequencies of the vocal tract, by
applying a piecewise linear transformation to expand
or compress the frequency axis of the productions of
each speaker. The particular adjustment to apply for
each speaker can be chosen by attempting to max-
imise the similarity of their adjusted speech to other
speakers. We use /i/ tokens as the basis of this com-
parison across speakers. VTLN is widely successful
in ASR systems, where it substantially decreases the
word error rate (Giuliani et al., 2006).

2.2 Z-scoring

The second normalization algorithm, within-speaker
z-scoring (Lobanov, 1971), has been suggested for de-
scriptive sociolinguistic research (Adank et al., 2004),
as it highlights learned linguistic content while re-
moving speaker-body confounds, such as vocal tract
length, from vowel formants. Z-scoring is also used
in ASR (Viikki and Laurila, 1998), where it is often
referred to as cepstral mean and variance normaliza-
tion (CMVN). Z-scoring has been shown to reduce
word error rate in speech recognizers, to improve
generalization to noisy input, and to improve general-
ization across speakers (Haeb-Umbach, 1999; Molau
et al., 2003; Tsakalidis and Byrne, 2005).

2.3 Effects of normalization

The method that we use in Section 4 for testing hy-
pothesized representations of speech against human
perception relies on the idea that different feature rep-
resentations imply different distributions of sounds
in listeners’ input. To examine these distributions of

sounds, we computed MFCCs, MFCCs with VTLN,
and z-scored MFCCs from vowel recordings in the
Vowels subcorpus of the Nationwide Speech Project
(NSP) (Clopper and Pisoni, 2006). This corpus con-
tains ten different vowels pronounced in the context
hVd (hid, had, etc.) by 5 female and 5 male speakers
from each of 6 dialect regions of the United States.
Each of these 60 speakers repeats each of the 10
hVd words 5 times, for a total of 3000 hVd tokens
balanced across vowel, gender, and dialect. For our
analyses, these vowel tokens are represented by the
speech features of their midpoint.1 Because the tem-
poral midpoint of a word may not be the midpoint
of its vowel, we detected the vocalic portion of each
NSP token as determined by relatively high total en-
ergy (more than one standard deviation above the
mean) and low zero-crossing rate (below 30%). We
used the temporal midpoint of this span.

We characterize the effects of each normalization
method on the distribution of vowels in the NSP
corpus by comparing vowel distributions across gen-
ders and dialect regions (averaged over 15 pairwise
comparisons of 6 dialect regions). Male and female
speakers differ in their vocal tract lengths, and thus
normalization algorithms would be expected to in-
crease similarity between their vowel productions.
Dialects also differ in their pronunciations of differ-
ent vowels; although this variation is not related to
vocal tract length, it may nevertheless be impacted
by normalization algorithms that seek to neutralize
speaker differences. To compare these distributions,
we computed symmetrized Kullback-Leibler diver-
gence, a measure of difference between two probabil-
ity distributions, using a nearest-neighbor algorithm
(Wang et al., 2006). Lower K-L divergence indicates
greater similarity between two distributions.

K-L divergence between genders and between di-
alect pairs is highest in MFCCs with no speaker nor-
malization, reflecting the effects these factors have
on the original acoustic signal (Table 1). Both VTLN
and z-scoring reduce K-L divergence between gen-
ders, as predicted, so that male and female speakers
saying the same vowel appear more similar after ei-

1We believe that dynamic representations of sounds are criti-
cal in human perception. However, this simplification is unlikely
to affect model performance for perceiving the sounds in our
experiments (Section 4), which have constant formant values
across the duration of each vowel token.

428



Table 1: Effect of normalization on symmetrized K-L
divergence.

MFCC VTLN z-scored

Gender KLDiv 7.84 6.14 4.58
Dialect KLDiv 4.41 4.19 2.09

ther of these normalizations than they are in unnor-
malized MFCCs. Z-scoring using all 10 NSP vowels
also sharply reduces K-L divergence between dialect
pairs. In contrast, VTLN that matches across speak-
ers on the basis of /i/, which differs little across US
English dialects (Clopper and Pisoni, 2006), has min-
imal effects on dialect K-L divergence: cross-dialect
vowel pronunciation differences remain distinct af-
ter VTLN. Overall, K-L divergence is lowest when
z-scoring by speaker. VTLN removes somewhat less
of the gender and dialect variation.

In summary, MFCCs, MFCCs with VTLN, and
z-scored MFCCs each correspond to different distri-
butions of vowels in the input, with z-scoring being
the most effective at increasing the overlap between
the distributions of vowels spoken by different speak-
ers. The next section describes a cognitive model that
uses these input distributions to quantitatively pre-
dict listeners’ vowel discrimination in the laboratory,
allowing us to ask which of these feature representa-
tions best corresponds with human perception.

3 Model of speech perception

Our goal is to evaluate MFCCs, MFCCs with VTLN,
and z-scored MFCCs on their match to human per-
ception. We perform this evaluation by integrating
each type of speech feature into a cognitive model
of speech perception. The model we adopt has pre-
viously been shown to accurately predict listeners’
perception of both vowels and consonants (Feldman
et al., 2009; Kronrod et al., 2016), but has not yet
been implemented directly on speech recordings.

3.1 Model overview

The model formalizes speech perception as an infer-
ence problem. Listeners perceive sounds by inferring
the acoustic detail of a speaker’s target production
through a noisy speech signal. This differs from the
problem of inferring a phoneme label, which is typi-
cally the goal of ASR systems, but is consistent with

a large body of empirical evidence showing that hu-
man listeners recover acoustic detail from the speech
signal in addition to category information (Andruski
et al., 1994; Blumstein et al., 2005; Joanisse et al.,
2007; Pisoni and Tash, 1974; Toscano et al., 2010).

To correct for noise in the speech signal, listen-
ers bias their perception toward acoustic values that
have high probability under their prior distribution.
This creates a dependency between the distribution of
sounds in the input, which determines listeners’ prior
distribution, and listeners’ perception of sounds.

Formally, speakers and listeners share a prior dis-
tribution over possible acoustic values that can be
produced in a language, p(T ). The acoustic values
are assumed to lie in a continuous feature space, such
as MFCCs. Prototypical sounds in the language have
highest probability under this distribution, but the
distribution is non-zero over a wide range of acous-
tic values, corresponding to all the ways in which
speech sounds might be realized. Speakers sample a
target production T from this distribution. The tar-
get production carries meaningful information aside
from category identity, such as dialect information or
coarticulatory information about upcoming sounds,
making its acoustic value something that listeners
wish to recover. The stimulus S heard by listeners is
similar to the target production T , but is assumed to
be corrupted by a noise process defined by a Gaussian
likelihood function,

p(S|T ) = N (T,ΣS) (1)

Both T and S are in Rd, where d is the dimension-
ality of the feature space. In our experiments, this
feature space is defined by either MFCCs, MFCCs
with VTLN, or z-scored MFCCs.

Listeners hear S and reconstruct T by drawing a
sample from the posterior distribution,

p(T |S) ∝ p(S|T )p(T ) (2)

We refer to a listener’s sample from the posterior
distribution as a percept. The top row of Figure 2
corresponds to S, and the bottom row corresponds to
listeners’ reconstruction of T .

3.2 Implementation on speech corpora
Previous work using this model has inferred listeners’
prior distribution by measuring their perceptual cat-

429



egorization, and estimating the parameters of Gaus-
sian phonetic categories that listeners appear to be
using to make these categorization judgments. In
the experiments below, we instead use vowel pro-
ductions from the NSP corpus to estimate listeners’
prior distributions. We avoid making parametric as-
sumptions about the form of this prior distribution
(such as expecting sounds from the corpus to fall into
Gaussian categories) by using importance sampling,
as proposed by Shi et al. (2010).

We assume that vowels in the NSP corpus con-
stitute a set of samples {T (i)} drawn from listeners’
prior distribution p(T ). We apply likelihood weight-
ing, weighting each sound T (i) by the probability that
it generated the stimulus being perceived, p(S|T (i)).
We then sample a sound from the corpus according to
its weight. A sound from the corpus that is sampled
in this way behaves as though it were drawn from the
posterior distribution, p(T |S).

This way of approximating the model through im-
portance sampling allows us to sample a percept from
the model’s posterior distribution without knowing
the analytical form of the prior distribution. We re-
quire only samples drawn from the prior, and can
reweight these in order to obtain a sample from the
posterior. In addition, this implementation of the
model does not require sounds from the corpus to
have phoneme labels, as the weights p(S|T (i)) are
defined by the model’s Gaussian likelihood function,
corresponding to speech signal noise.

3.3 Modeling discrimination data

The model can be used to predict listeners’ discrimi-
nation behavior. In AX discrimination tasks, listeners
hear two stimuli and decide whether they are acous-
tically identical. We assume that for each stimulus,
listeners sample a percept from their posterior distri-
bution, p(T |S). They then compute the distance be-
tween their percepts of the two stimuli and compare it
to a threshold �. If the percepts are separated by a dis-
tance less than �, listeners respond same; otherwise
they respond different. Given these assumptions, the
proportion of same responses for two stimuli, S1 and
S2, follows a binomial distribution whose parameter
is the probability that the percepts for the two sounds
are within a distance � of each other,

p(same) = p(|T1−T2|<�|S1, S2) (3)

We approximate the probability from (3) as

p(same) ≈ 1
n

n∑

i=1

1|T (i)1 −T
(i)
2 |<�

(4)

where T (i)1 and T
(i)
2 are samples drawn through

importance sampling from posterior distributions
p(T1|S1) and p(T2|S2). We draw 100 pairs of tar-
get productions from the posterior for each experi-
mental trial and approximate listeners’ probability
of responding same by estimating the proportion of
these pairs that yielded a same response, using add-
one smoothing.2 We use this estimate to compute
model likelihoods – the probability of the human
responses, given the model predictions – based on
listeners’ actual same-different responses.

The noise covariance ΣS and the response thresh-
old � are free parameters of the model, which we
fit to maximize model likelihoods.3 We implement
the model several times with different speech rep-
resentations: MFCCs, z-scored MFCCs, or MFCCs
with VTLN. Comparing model likelihoods across the
three speech representations allows us to ask which
representations best predict listeners’ discrimination.

4 Experiments

Experiments implemented the perceptual model with
normalized and unnormalized representations, com-
paring model predictions to human discrimination
data. To the extent that different representations of
speech yield different distributions of sounds in a
corpus, they should make different predictions about
the biases that listeners will exhibit in a speech per-
ception experiment. Representations that yield more
accurate predictions can be assumed to correspond
to a similarity space more similar to the dimensions
that human listeners use in speech perception.

2The distance computation in (4) uses the Mahalanobis dis-
tance between T (i)1 and T

(i)
2 , where the covariance matrix is

the noise variance ΣS . If listeners’ sensitivity is related to their
perceptual noise, then ΣS is the appropriate scaling factor for
this distance. In support of this, Feldman et al. (2009) found in
their one-dimensional model that increasing ΣS by adding white
noise increases listeners’ decision threshold to the same degree.

3Although our use of Mahalanobis distance allows a tradeoff
between ΣS and � in the distance computation, ΣS also affects
the weighting of exemplars through the perceptual model’s like-
lihood function. Thus, these parameters are identifiable.

430



4.1 Human perceptual data

We use vowel discrimination data from an AX dis-
crimination task conducted by Feldman et al. (2009)
in quiet listening conditions (Figure 3). Twenty
participants heard stimuli consisting of 13 isolated
vowels ranging from /i/ (as in ‘beet’) to /e/ (as in
‘bait’), which were synthesized to simulate a male
speaker. To control the placement of stimuli along the
vowel continuum, first and second formant frequen-
cies were equally spaced between /i/ and /e/ values
along the mel frequency scale. Participants heard
all ordered pairs of stimuli and judged whether each
pair was acoustically identical, responding different
if they could hear any difference between the stimuli.
MFCCs, MFCCs with VTLN, and z-scored MFCCs
computed from these 13 stimuli serve as input to the
model, as the stimulus S. Model predictions are then
compared with listeners’ same-different responses to
each pair of stimuli.

4.2 Speech representations

The samples {T (i)} that serve as the model’s prior
distribution are vowels from the Vowels subcorpus
of the NSP. Vowel midpoints were represented in the
model either as MFCCs, MFCCs with VTLN, or z-
scored MFCCs, computed using a 25 ms window and
a 10 ms step size.4 Features were scaled to have zero
mean and a variance of one across all vowels.5

VTLN was implemented using a procedure from
Wegmann et al. (1996) adapted for an unsupervised
setting, which compensates for vocal tract length
variation by finding a frequency adjustment for each
speaker that maximizes the similarity of their tokens
to all the other speakers’. This VTLN implemen-
tation first applies and then selects among several
frequency rescaling factors, ranging from -20% to
+20% of the original vocal tract length in steps of 5%.

4The zeroth cepstral dimension was excluded from the rep-
resentation in experiments; it mainly describes the total amount
of energy at each time in the speech signal, which is useful in
detecting whether or not the speech is vocalic but contributes
little to distinguishing different vowel sounds. Additionally, al-
though ASR systems frequently append deltas and double deltas
to the cepstral coefficients, corresponding to the first and second
derivatives of the MFCCs, we did not use these as they add free
parameters to the model.

5This was different from the z-scoring method for speaker
normalization, which scaled the vowels from each individual
speaker to have zero mean and a variance of one.

Together with the original speech (scale factor of 0%)
there are therefore 9 possible variants. The rescaling
factor for each speaker is chosen using expectation
maximization (Dempster et al., 1977). The E-step
trains a Gaussian mixture model with 9 components
and spherical covariance from all /i/ vowel frames in
the NSP corpus. The M-step finds a rescaling factor
for each speaker that maximizes the likelihood un-
der the current Gaussian mixture model. After EM
converges, the maximum likelihood rescaling factor
for the stimuli is selected under the final NSP-based
mixture model. Normalizing the stimuli is straight-
forward, due to the reliance of our VTLN procedure
on only /i/ tokens. Vowel midpoints of corpus tokens
and stimuli were excerpted after normalization.

Z-scoring was applied to MFCCs at vowel mid-
points. The NSP is an ideally balanced case for the
z-scoring normalization, because each speaker says
the same vowels the same number of times. MFCCs
for the stimulus ‘speaker’ were only available for
the /i/-/e/ vowel continuum, so it was not possible
to apply z-scoring directly to the stimuli in the same
way as it was applied to each NSP speaker. However,
the stimuli were synthesized according to average
formant values for male speakers (Peterson and Bar-
ney, 1952), and we verified that normalizing MFCCs
for these stimuli according to the average z-scoring
factors of the 30 male NSP speakers projected the
stimuli into an appropriate region of the acoustic
space.

Finally, although speech recognition systems typ-
ically use 12 MFCC dimensions, we additionally
include experiments for all feature types that omit
subsets of the higher dimensions, as formant informa-
tion that is useful for perceiving vowels is primarily
reflected in the lower dimensions.

4.3 Fitting parameters
The NSP corpus was divided into equally sized sets
to fit parameters and test model likelihoods in 2-fold
cross-validation. Two methods were used for divid-
ing the corpus. In one case, the two halves contained
different tokens from the same speakers. In the other
case, the two halves contained tokens from different
speakers (balanced for gender and dialect region).
Each division of the corpus was created once, and
used across experiments with all speech feature types.

The fitting procedure consisted of selecting the

431



response threshold � and the noise variance ΣS ,
constrained to be diagonal, to best fit the percep-
tual data. This was done using Metropolis-Hastings
sampling (Hastings, 1970) with parallel tempering,
which flexibly interleaves broad exploration of the
search space with fine-tuning of the best parameter
values. The sampler used Gaussian proposal distri-
butions for each parameter, with proposals Σ∗S and
�∗ drawn based on current estimates ΣS and � as
Σ∗S ∼ N (ΣS , σ21I) and �∗ ∼ N (�, σ22). Given this
symmetric proposal distribution, the acceptance ratio
is

A =
p(k|Σ∗S , �∗)
p(k|ΣS , �)

(5)

where k is a vector of counts corresponding to same-
different responses from human participants in the
experiment. The likelihood p(k|ΣS , �) is a product
of the binomial likelihood functions for each speech
sound contrast, whose parameters are approximated
according to (4). We run several chains, indexed
1..C, at temperatures τ1..τC , by raising each likeli-
hood p(k|ΣS , �) to a power of 1τ . At each iteration
there is a proposal to swap the parameters between
neighboring chains. The acceptance ratio for swap-
ping parameters between chains c1 and c2 is

A =
p(k|Σ(c2)S , �(c2))

1
τc1 p(k|Σ(c1)S , �(c1))

1
τc2

p(k|Σ(c1)S , �(c1))
1
τc1 p(k|Σ(c2)S , �(c2))

1
τc2

(6)

where τc1 and τc2 are the temperatures associated
with chains c1 and c2, respectively. Proposals are
accepted with probability min(1, A). Model param-
eters were fit using this procedure, with 50,000 itera-
tions through eleven chains at different temperatures
ranging from 0.01 to 1,000. Our analyses use the
last sample in the lowest temperature chain, with a
temperature of 0.01.

In each run of the model, one half of the corpus
was used to fit model parameters, while the other half
was used to compute model likelihoods at test. The
roles of the two sets were then reversed, resulting
in 2-fold cross-validation. Each half of the corpus
served as a test set for 10 runs of the model, so points
and error bars in Figures 4 and 5 represent means and
standard error calculated across all 20 replications;
the relatively small error bars indicate that results

were consistent across replications.6

4.4 Results

Stimuli in the behavioral discrimination task were
synthesized to mimic a single speaker, and thus
VTLN and z-scoring are not expected to directly
impact relative distances between stimuli S in the
feature space. Instead, differences in model behavior
across the three feature types should arise primarily
because VTLN and z-scoring impact the distribu-
tion of vowels from the NSP corpus. This affects
the model’s prior distribution and thus its perceptual
biases, leading to different reconstructions of T .

The predicted discrimination patterns that result
from different speech representations are illustrated
qualitatively in Figure 3, together with human data
and a model benchmark which is described at the
end of this section. Patches of darker shading in the
confusion matrices of Figure 3 indicate higher pro-
portions of same responses, corresponding to reduced
discrimination between stimuli in these regions. The
models vary in how well they reflect humans’ percep-
tual biases. One-dimensional models are extremely
poor regardless of feature type. Unnormalized MFCC
models, as shown most clearly in 4 and 6 dimensions,
often differ from human listeners by excessively re-
ducing discrimination around the initial (/i/) end of
the stimulus continuum. Z-scoring models, particu-
larly in the low to mid range of dimensionality, reflect
human judgments better than MFCC models but still
tend to deviate from human performance in visible
areas of reduced discrimination centered around stim-
uli 6 and 9. Models using VTLN representations of
moderately low dimensionality show the most faith-
ful fit with human patterns of perceptual bias.

These differences in model performance across
feature types can be quantified by computing the
log likelihood of a model, given the human data.
This measure treats the human same and different
responses as ground truth, and assesses model per-
formance on the binary decision task against that
ground truth; higher log likelihoods indicate a closer
match of a model to human perceptual data. Log like-
lihoods shown in Figure 4 quantify the performance
of models across feature types; this complements the

6Numerical values fit for model parameters were also consis-
tent across the 20 replications for each speech feature type.

432



Figure 3: Confusion matrices for how often each pair
of stimuli is judged to be the same (black) vs. different
(white) by humans, by a benchmark exemplar model, and
by representative models testing the different speech fea-
tures on familiar speakers. Axis labels of 1-13 denote
stimulus numbers from an AX trial.

detailed views of model behavior in Figure 3. Recall
that our primary goal is to replicate previous findings
with categorization models showing that normalized
representations are more consistent with listeners’
perception than unnormalized representations. We
do replicate this result with our discrimination model:
in almost all cases, unnormalized MFCCs have the

lowest likelihoods among the three representations
tested (Figure 4).

We also find that MFCCs normalized by VTLN
outperform z-scored MFCCs. This occurs in spite
of the fact that z-scoring within speakers puts male
and female speech into the most directly comparable
space and in general neutralizes the most differences
between speakers as measured by K-L divergence
(Table 1). This indicates that more effective speaker
normalization algorithms (in the sense of minimizing
differences between speakers in a feature space) do
not always translate into better matches with human
perception. Furthermore, it provides evidence that
the better performance of the VTLN models is not
merely an artifact of high acoustic similarity among
vowels in the corpus.

As suggested by Figure 3, models using speech
representations of one dimension are always ex-
tremely poor, with log likelihoods of −1300 to
−1600; the first cepstral coefficient on its own does
not provide enough information for the perceptual
task. In some experiments, such as for z-scored fea-
tures, test likelihood decreases with higher numbers
of dimensions, likely due to overfitting of param-
eters to particular sounds from the corpus.7 The
lower MFCC dimensions, particularly the second di-
mension, appear to contain information relevant to
listeners’ discrimination of an /i/-/e/ continuum.

As a representative example of the best fitting pa-
rameters, one trained model using four dimensions
of VTLN features found parameters8 of ΣS = 0.1617,
0.0092, 2.2400, 1.8194 and � = 3.4388.9 Dimen-
sions where noise is low indicate those that the model
found informative for this perceptual task: low noise
causes even numerically small differences between
tokens to be attended to as differences in perceived
vowel quality, whereas high noise variance generates
a strong perceptual bias toward the center of vowel

7The corresponding training likelihoods for z-scored dimen-
sions remained stable or even increased at higher numbers of
dimensions; there was not failure to converge in the training pro-
cedure. Similarly, the low likelihood observed at six dimensions
for MFCCs was due to several runs that achieved high likelihood
on the training set and low likelihood on the test set.

8The covariance matrix ΣS was constrained to be diagonal;
here we list only the diagonal elements.

9Across the 10 training replications, the standard deviation
in these trained parameters was respectively 0.0823, 0.0006,
0.8684, 1.1518 for ΣS and 0.1645 for �.

433



2 4 6 8 10 12
−800

−700

−600

−500

−400

−300

−200

Feature dimensionality

Tested on familiar speakers

L
o
g
 l
ik

e
lih

o
o

d

 

 

MFCC
VTLN

z−scored
Benchmark

2 4 6 8 10 12
−800

−700

−600

−500

−400

−300

−200

Feature dimensionality

Tested on new speakers

L
o
g
 l
ik

e
lih

o
o

d

 

 

MFCC
VTLN

z−scored
Benchmark

Figure 4: Model fit to human data, when generalizing to familiar speakers (left) and new speakers (right).

space that collapses the space between vowel per-
cepts along that dimension. Dimensions with high
noise parameter values therefore indicate that the
model benefited from perceiving all samples from the
prior distribution as being similar along that dimen-
sion, with variability along those dimensions treated
as noise relative to listeners’ perceptual judgments
about this vowel contrast. Across all feature types,
successful models had low noise in dimension 2 and
low variance in this noise parameter across training
replications. This consistency, together with the large
difference in the model’s ability to match human per-
formance between feature dimensionalities of 1 and
2, indicates that the second cepstral coefficient is
important for capturing listeners’ discrimination of
sounds along the /i/-/e/ vowel continuum.

Our next analyses examine the speech tokens sam-
pled as percepts by the model when perceiving the
experimental stimuli. The model did not use the
NSP’s speaker and phoneme labels for selecting these
percepts, but we take advantage of these labels for
our analyses. Across all models, the 100 percepts
drawn from the posterior distribution for each stim-
ulus contained on average 30 different tokens, indi-
cating that a number of speech productions (from
different speakers) are treated as linguistically sim-
ilar to each other. As a measure of model quality
and interpretability, we examine two aspects of the
tokens sampled by the model during each percep-
tual judgment (Figure 5). The percentage of samples
that belong to the classes of vowels along the /i/-/e/
continuum (NSP ‘heed’, ‘hayed’, and ‘hid’ tokens;

henceforth referred to as high front vowels) gives
information on model quality, because all the stimuli
are perceived by US English speakers as falling along
this continuum. The proportion of times a model sam-
ples female tokens to recover the linguistic target for
this experiment’s male-speaker stimuli gives an indi-
cation of the model’s ability to generalize linguistic
content across genders.

Models using unnormalized MFCCs tend to make
the least use of female tokens, indicating that this
representation does not recognize very much simi-
larity between male and female speakers saying the
same vowel. Models with z-scored features are clos-
est to sampling 50% female tokens; this confirms that
the z-scored representation is highly effective at neu-
tralizing differences between speakers of different
genders (Table 1), although as noted above, it is not
the representation that gives the best match to human
discrimination performance (Figure 4).

While models with 2 through 6 dimensions consis-
tently treated the experimental stimuli as being most
similar to high front vowels in the corpus, models
with higher orders of cepstral coefficients did not
(Figure 5), reinforcing the importance of the lower
MFCC dimensions in capturing listeners’ perception
of these stimuli. We suspect that failure to perceive
stimuli as high front vowels in models with higher
numbers of dimensions emerged due to the artificial
synthesis of the experimental stimuli, which resulted
in these stimuli being most similar to low back vow-
els from the corpus in two of the higher MFCC di-
mensions. The model can perceive stimuli as low

434



2 4 6 8 10 12
20

30

40

50

60

70

80

90

100

Feature dimensionality

%
 h

ig
h
 f
ro

n
t 
e
x
e
m

p
la

rs

Vowel quality

 

 

MFCC

VTLN

z−scored

2 4 6 8 10 12
0

10

20

30

40

50

60

Feature dimensionality

%
 f
e

m
a
le

 e
x
e
m

p
la

rs

Gender distribution

 

 

MFCC

VTLN

z−scored

Figure 5: Secondary evaluations, showing how often the stimuli were correctly perceived as high front vowels (left) and
how often the model based perceptions on female utterances (right). Data are averaged across the familiar and new
speaker test cases, which were very similar on these measures.

back vowels in cases where it generalizes along those
dimensions. This underscores the difficulty of bring-
ing together ecologically valid speech corpora with
the type of controlled stimuli typically used in experi-
mental settings, and illuminates areas in which future
research may provide insight by addressing these
challenges. Nevertheless, even if we omit results
from models that were affected by this mismatch be-
tween corpus tokens and experimental stimuli, the
relative ordering of the three feature types was quali-
tatively consistent across a range of dimensionalities.

Finally, we can compare the log likelihoods from
Figure 4 to a benchmark showing ideal model per-
formance. Feldman et al. (2009) estimated listeners’
prior distributions for the /i/ and /e/ categories from
perceptual categorization data, rather than from pro-
duction data in speech recordings. This method of
estimating listeners’ prior distributions sidesteps the
problem of mapping between perception and produc-
tion in that it estimates prior distributions directly
from perceptual data. Using their estimate, the model
yields an average log likelihood of -233, somewhat
higher than those obtained here. Our models should
approach this value as the distribution of sounds in
the corpus approaches the prior distribution that lis-
teners use in perceptual tasks.

5 Relation to previous work

To our knowledge, this is the first time a cognitive
model has been used to evaluate ASR representations

against human behavioral data. Front-end feature rep-
resentations in ASR are typically evaluated on their
ability to improve performance in an ASR system
(Junqua et al., 1993; Welling et al., 1997). Measuring
performance in an ASR system provides a direct eval-
uation relative to a particular task, but provides only
indirect evidence about the correspondence of feature
representations with human perception. Our work
measures the extent to which these feature represen-
tations match human judgments, parallel to analyses
that have been conducted in other domains of lan-
guage to compare representations used in language
technology to human data (Chang et al., 2009; Gersh-
man and Tenenbaum, 2015). As a feature evaluation
metric, our approach also has the advantage that it ex-
amines front-end feature representations directly, and
is not subject to arbitrary decisions about back-end
systems in an ASR pipeline.

There have also been proposals that use direct mea-
sures of feature variability (Haeb-Umbach, 1999) or
minimal pair ABX discrimination tasks (Carlin et
al., 2011; Schatz et al., 2013; Schatz et al., 2014) to
evaluate feature representations, and thus do not de-
pend on particular back-end ASR systems. These
approaches differ from ours in that they evaluate
whether discrimination performance is correct, rather
than whether discrimination performance is similar
to that of human listeners. In addition, these ap-
proaches assume a direct mapping between distance
in the feature space and perceived similarity between

435



two stimuli, whereas ours takes into account human
listeners’ perceptual bias toward sounds that occur
frequently within the feature space.

Research in cognitive science has previously exam-
ined the dimensions that guide listeners’ perception,
but has relied on data from categorization tasks, in
which listeners decide which category a sound be-
longs to (Apfelbaum and McMurray, 2015; Idemaru
et al., 2012; McMurray and Jongman, 2011; Nittrouer
and Miller, 1997; Repp, 1982). Our approach instead
focuses on acoustic discrimination tasks. Discrimina-
tion provides several advantages over categorization.
It is a more fine-grained measure than categorization,
and can be reliably measured in both adults and in-
fants, even when listeners do not have well-formed
categories for a given set of sounds. In addition,
whereas building a categorization model requires la-
beled training data, building a discrimination model
does not, as explained in Section 3.

6 Discussion

In this paper a cognitive model of speech perception
was implemented directly on speech recordings and
used to evaluate the low-level feature representations
corresponding to two speaker normalization methods:
VTLN and z-scoring. Both normalization methods
improved the model’s fit to human perceptual data.
Between the two normalization methods, VTLN out-
performed z-scoring, despite being less effective at
collapsing gender and dialect differences.

The ability of VTLN and z-scoring to improve the
match of a feature space with human perception may
be a relevant consideration in supporting the use of
these normalization methods in ASR systems. Fur-
thermore, although we do not mean to imply that
VTLN or z-scoring are cognitive models of speaker
normalization, the better performance of VTLN over
z-scoring does suggest that VTLN facilitates general-
ization across speakers in a way that is more similar
to human perceivers. For example, human listeners
may collapse across genders while retaining differ-
ences across dialects. Knowing how to model the
way in which listeners generalize across speakers
could have practical implications for assisting lis-
teners with hearing impairments (Liu et al., 2008),
and our work complements previous experimental
and computational evidence related to this question

(Barreda, 2012; Halberstam and Raphael, 2004; Mc-
Murray and Jongman, 2011; Nearey, 1989).

Speaker normalization and speaker adaptation are
often treated as competing theories of human speech
perception (Dahan et al., 2008), whereas speaker nor-
malization algorithms are applied in conjunction with
speaker adaptation algorithms in the ASR literature
(Gales and Young, 2007). Our experiments focused
only on the effects of normalization, without con-
sidering whether there might be a role of adaptation
in explaining listeners’ discrimination behavior. It
is not obvious how we would apply speaker adap-
tation in our discrimination model, which does not
incorporate explicit category representations.

Despite the advantage observed for normalized
over unnormalized features, none of the representa-
tions tested here match human perception exactly. A
prior distribution estimated from perceptual catego-
rization still outperforms prior distributions measured
from a speech corpus. Our primary goal has been to
introduce a new method for assessing speech feature
representations against human data, and to validate
that method against previous findings that showed a
benefit for normalization.

Our method is compatible with a wide range of fea-
ture spaces. The importance sampling approximation
used here can be implemented on any speech rep-
resentation for which a likelihood function p(S|T )
can be computed between experimental stimuli and
sounds from a corpus, and for which a distance met-
ric between sounds in the corpus can be compared
to a threshold �. The likelihood function does not
need to be Gaussian, and the feature space does not
need to have fixed or even finite dimensionality. Even
when assuming a Gaussian likelihood function, not
all feature spaces have uncorrelated dimensions, and
the model may need to be extended to use a full
covariance matrix for ΣS . This extension is straight-
forward to derive mathematically, but would require
additional human perceptual data to constrain the
model’s free parameters.

The speech corpus used in these experiments con-
sisted of /hVd/ utterances that were balanced across
vowel, gender, and dialect. Conducting initial exper-
iments with a corpus of vowels in neutral contexts
allowed us to investigate algorithms for speaker nor-
malization while sidestepping issues of how listeners
generalize across phonological contexts. However,

436



differences between this controlled corpus and lis-
teners’ experience may account in part for the lower
performance of the corpus-based models compared
to the model whose prior distribution was estimated
from perceptual categorization data. Future work that
estimates listeners’ prior distributions from speech
recordings can do so using more realistic corpora of
conversational speech. The model’s prior distribution
can in principle be estimated from any speech corpus,
and does not require phoneme labels.

Using larger corpora of conversational speech
would be particularly beneficial because it could pro-
vide sufficient data to derive and evaluate features
from representation learning algorithms, which typi-
cally involve training models with many parameters.
Recent work in ASR has proposed ways of optimiz-
ing feature representations based on linguistic data
(Grézl et al., 2014; Heigold et al., 2013; Kamper et
al., 2015; Sercu et al., 2016; Thomas et al., 2012;
Wang et al., 2015). These proposals differ from each
other in their objective functions, and our method can
help determine which of these objective functions
yield representations that capture human listeners’
generalizations. Testing the outcome of these learn-
ing algorithms against human data in high resource
languages could help select representation learning
algorithms for low resource settings. Such investi-
gations would not necessarily be limited to speaker
normalization algorithms, but could apply to any rep-
resentation learning algorithm designed to optimize
perception for a particular language.

Perhaps the most exciting future application of this
work is in investigating the way in which human lis-
teners’ feature spaces become tuned to their native
language. Research in cognitive science has begun
asking how language-specific feature spaces might
be learned (Holt and Lotto, 2006; Idemaru and Holt,
2011; Idemaru and Holt, 2014; Liu and Holt, 2015;
Nittrouer, 1992). The method proposed here provides
a way of testing the predictions of these learning theo-
ries against human data, and of testing whether repre-
sentation learning algorithms from the ASR literature
have analogues in human perceptual development.
Unlabeled speech corpora are available in multiple
languages, and cross-linguistic perceptual differences
are typically measured through discrimination tasks;
our discrimination model can take advantage of these
data. Thus, using the method proposed here, predic-

tions of human representation learning theories can
be evaluated with respect to adults’ cross-linguistic
differences in sensitivity to perceptual dimensions.
If the cognitive model were extended to infant dis-
crimination tasks, a similar strategy could be used
to evaluate theories of human representation learn-
ing with respect to children’s discrimination abilities
throughout the learning process. In this way, imple-
menting a cognitive model directly on corpora of
natural speech can lead to a richer understanding of
the way in which listeners’ perception is shaped by
their environment.

Acknowledgments

We thank Josh Falk for help in piloting the model,
Phani Nidadavolu for help computing speech fea-
tures, and Eric Fosler-Lussier, Hynek Hermansky,
Bill Idsardi, Feipeng Li, Vijay Peddinti, Amy Wein-
berg, the UMD probabilistic modeling reading group,
and three anonymous reviewers for helpful comments
and discussion. Previous versions of this work were
presented at the 8th Northeast Computational Phonol-
ogy Workshop and the 38th Annual Conference of the
Cognitive Science Society. The work was supported
by NSF grants BCS-1320410 and DGE-1321851.

References
Patti Adank, Roel Smits, and Roeland van Hout. 2004.

A comparison of vowel normalization procedures for
language variation research. Journal of the Acoustical
Society of America, 116(5):3099–3107.

Jean E. Andruski, Sheila E. Blumstein, and Martha Burton.
1994. The effect of subphonemic differences on lexical
access. Cognition, 52:163–187.

Keith S. Apfelbaum and Bob McMurray. 2015. Relative
cue encoding in the context of sophisticated models
of categorization: Separating information from catego-
rization. Psychonomic Bulletin and Review, 22(4):916–
943.

Santiago Barreda. 2012. Vowel normalization and the
perception of speaker changes: An exploration of the
contextual tuning hypothesis. Journal of the Acoustical
Society of America, 132(5):3453–3464.

Sheila E. Blumstein, Emily B. Myers, and Jesse Rissman.
2005. The perception of voice onset time: An fMRI
investigation of phonetic category structure. Journal of
Cognitive Neuroscience, 17(9):1353–1366.

Michael A. Carlin, Samuel Thomas, Aren Jansen, and
Hynek Hermansky. 2011. Rapid evaluation of speech

437



representations for spoken term discovery. Proceedings
of Interspeech.

Jonathan Chang, Jordan Boyd-Graber, Sean Gerrish,
Chong Wang, and David M. Blei. 2009. Reading tea
leaves: How humans interpret topic models. Advances
in Neural Information Processing Systems 22.

Meghan Clayards, Michael K. Tanenhaus, Richard N.
Aslin, and Robert A. Jacobs. 2008. Perception of
speech reflects optimal use of probabilistic speech cues.
Cognition, 108(3):804–809.

Cynthia G. Clopper and David B. Pisoni. 2006. The
nationwide speech project: A new corpus of American
English dialects. Speech Communication, 48(6):633–
644.

Jordan Cohen, Terri Kamm, and Andreas G. Andreou.
1995. Vocal tract normalization in speech recognition:
Compensating for systematic speaker variability. Jour-
nal of the Acoustical Society of America, 97(5):3246–
3247.

Jennifer Cole, Gary Linebaugh, Cheyenne M. Munson,
and Bob McMurray. 2010. Unmasking the acoustic
effects of vowel-to-vowel coarticulation: A statistical
modeling approach. Journal of Phonetics, 38:167–184.

Delphine Dahan, Sarah J. Drucker, and Rebecca A. Scar-
borough. 2008. Talker adaptation in speech perception:
Adjusting the signal or the representations? Cognition,
108:710–718.

Steven B. Davis and Paul Mermelstein. 1980. Compar-
ison of parametric representations for monosyllabic
word recognition in continuously spoken sentences.
Proceedings of the IEEE, pages 357–366.

Arthur P. Dempster, Nan M. Laird, and Donald B. Rubin.
1977. Maximum likelihood from incomplete data via
the EM algorithm. Journal of the Royal Statistical
Society, B, 39:1–38.

Naomi H. Feldman, Thomas L. Griffiths, and James L.
Morgan. 2009. The influence of categories on per-
ception: Explaining the perceptual magnet effect as
optimal statistical inference. Psychological Review,
116(4):752–782.

Mark Gales and Steve Young. 2007. The application of
hidden Markov models in speech recognition. Founda-
tions and Trends in Signal Processing, 1(3):195–304.

Samuel J. Gershman and Joshua B. Tenenbaum. 2015.
Phrase similarity in humans and machines. Proceed-
ings of the 37th Annual Conference of the Cognitive
Science Society.

Diego Giuliani, Matteo Gerosa, and Fabio Brugnara.
2006. Improved automatic speech recognition through
speaker normalization. Computer Speech & Language,
20(1):107–123.

František Grézl, Ekaterina Egorova, and Martin Karafiát.
2014. Further investigation into multilingual training

and adaptation of stacked bottle-neck neural network
structure. IEEE Spoken Language Technology Work-
shop, pages 48–53.

Reinhold Haeb-Umbach. 1999. Investigations on inter-
speaker variability in the feature space. Proceedings
of the IEEE International Conference on Acoustics,
Speech, and Signal Processing, pages 397–400.

Benjamin Halberstam and Lawrence J. Raphael. 2004.
Vowel normalization: the role of fundamental fre-
quency and upper formants. Journal of Phonetics,
32:423–434.

W. Hastings. 1970. Monte Carlo sampling methods us-
ing Markov chains and their applications. Biometrika,
57(1):97–109.

Georg Heigold, Vincent Vanhoucke, Andrew Senior,
Phuongrang Nguyen, Marc’Aurelio Ranzato, Matthieu
Devin, and Jesse Dean. 2013. Multilingual acoustic
models using distributed deep neural networks. Pro-
ceedings of the IEEE International Conference on
Acoustics, Speech and Signal Processing, pages 8619–
8623.

Hynek Hermansky and Nelson Morgan. 1994. RASTA
processing of speech. IEEE Transactions on Speech
and Audio Processing, 2(4):578–589.

Hynek Hermansky. 1990. Perceptual linear predictive
(PLP) analysis of speech. Journal of the Acoustical
Society of America, 87(4):1738–1752.

James Hillenbrand, Laura A. Getty, Michael J. Clark, and
Kimberlee Wheeler. 1995. Acoustic characteristics of
American English vowels. Journal of the Acoustical
Society of America, 97(5):3099–3111.

Lori L. Holt and Andrew J. Lotto. 2006. Cue weighting
in auditory categorization: Implications for first and
second language acquisition. Journal of the Acoustical
Society of America, 119(5):3059–3071.

Kaori Idemaru and Lori L. Holt. 2011. Word recognition
reflects dimension-based statistical learning. Journal
of Experimental Psychology: Human Perception and
Performance, 37(6):1939–1956.

Kaori Idemaru and Lori L. Holt. 2014. Specificity of
dimension-based statistical learning in word recogni-
tion. Journal of Experimental Psychology: Human
Perception and Performance, 40(3):1009–1021.

Kaori Idemaru, Lori L. Holt, and Howard Seltman. 2012.
Individual differences in cue weights are stable across
time: The case of Japanese stop lengths. Journal of the
Acoustical Society of America, 132(6):3950–3964.

Marc F. Joanisse, Erin K. Robertson, and Randy Lynn
Newman. 2007. Mismatch negativity reflects sen-
sory and phonetic speech processing. NeuroReport,
18(9):901–905.

Keith Johnson, 1997. Speech perception without speaker
normalization: An exemplar model, pages 145–165.
Academic Press, New York.

438



Keith Johnson. 2006. Resonance in an exemplar-based
lexicon: The emergence of social identity and phonol-
ogy. Journal of Phonetics, 34(4):485–499.

Jean-Claude Junqua, Hisashi Wakita, and Hynek Herman-
sky. 1993. Evaluation and optimization of perceptually-
based ASR front-end. IEEE Transactions on Speech
and Audio Processing, 1(1):39–48.

Herman Kamper, Micha Elsner, Aren Jansen, and Sharon
Goldwater. 2015. Unsupervised neural network based
feature extraction using weak top-down constraints.
Proceedings of the IEEE International Conference on
Acoustics, Speech and Signal Processing.

Dave F. Kleinschmidt and T. Florian Jaeger. 2015. Robust
speech perception: Recognize the familiar, generalize
to the similar, and adapt to the novel. Psychological
Review, 122(2):148–203.

Yakov Kronrod, Emily Coppess, and Naomi H. Feldman.
2016. A unified account of categorical effects in pho-
netic perception. Psychonomic Bulletin and Review,
23(6):1681–1712.

Ran Liu and Lori L. Holt. 2015. Dimension-based
statistical learning of vowels. Journal of Experimen-
tal Psychology: Human Perception and Performance,
41(6):1783–1798.

Chuping Liu, John Galvin III, Qian-Jie Fu, and
Shrikanth S. Narayanan. 2008. Effect of spectral
normalization on different talker speech recognition
by cochlear implant users. Journal of the Acoustical
Society of America, 123(5):2836–2847.

B. M. Lobanov. 1971. Classification of Russian vowels
spoken by different speakers. Journal of the Acoustical
Society of America, 49(2):606–608.

Jessica Maye, Janet F. Werker, and LouAnn Gerken. 2002.
Infant sensitivity to distributional information can affect
phonetic discrimination. Cognition, 82:B101–B111.

Bob McMurray and Allard Jongman. 2011. What in-
formation is necessary for speech categorization? har-
nessing variability in the speech signal by integrating
cues computed relative to expectations. Psychological
Review, 118(2):219–246.

James D. Miller. 1989. Auditory-perceptual interpretation
of the vowel. Journal of the Acoustical Society of
America, 85(5):2114–2134.

Sirko Molau, Florian Hilger, and Hermann Ney. 2003.
Feature space normalization in adverse acoustic con-
ditions. Proceedings of the IEEE International Con-
ference on Acoustics, Speech, and Signal Processing,
pages 656–659.

Philip J. Monahan and William J. Idsardi. 2010. Audi-
tory sensitivity to formant ratios: Toward an account
of vowel normalisation. Language and Cognitive Pro-
cesses, 25(6):808–839.

Terrance Michael Nearey. 1978. Phonetic feature systems
for vowels, volume 77. Indiana University Linguistics
Club.

Terrance M. Nearey. 1989. Static, dynamic, and rela-
tional properties in vowel perception. Journal of the
Acoustical Society of America, 85(5):2088–2113.

Susan Nittrouer and Marnie E. Miller. 1997. Pre-
dicting developmental shifts in perceptual weighting
schemes. Journal of the Acoustical Society of America,
101(4):2253–2266.

Susan Nittrouer. 1992. Age-related differences in per-
ceptual effects of formant transitions within syllables
and across syllable boundaries. Journal of Phonetics,
20:351–382.

Gordon E. Peterson and Harold L. Barney. 1952. Control
methods used in a study of the vowels. Journal of the
Acoustical Society of America, 24(2):175–184.

Gordon E. Peterson. 1961. Parameters of vowel quality.
Journal of Speech and Hearing Research, 4(1):10–29.

David B. Pisoni and Jeffrey Tash. 1974. Reaction times
to comparisons within and across phonetic categories.
Perception and Psychophysics, 15(2):285–290.

Daniel Povey and George Saon. 2006. Feature and model
space speaker adaptation with full covariance Gaus-
sians. Proceedings of Interspeech.

Bruno H. Repp. 1982. Phonetic trading relations and con-
text effects: New experimental evidence for a speech
mode of perception. Psychological Bulletin, 92(1):81–
110.

Thomas Schatz, Vijayaditya Peddinti, Francis Bach, Aren
Jansen, Hynek Hermansky, and Emmanuel Dupoux.
2013. Evaluating speech features with the minimal-pair
ABX task: Analysis of the classical MFC/PLP pipeline.
Proceedings of Interspeech, pages 1781–1785.

Thomas Schatz, Vijayaditya Peddinti, Xuan-Nga Cao,
Francis Bach, Hynek Hermansky, and Emmanuel
Dupoux. 2014. Evaluating speech features with the
minimal-pair ABX task (II): Resistance to noise. Pro-
ceedings of Interspeech.

Tom Sercu, Christia Puhrsch, Brian Kingsbury, and Yann
LeCun. 2016. Very deep multilingual convolutional
neural networks for LVCSR. Proceedings of the IEEE
International Conference on Acoustics, Speech and Sig-
nal Processing.

Lei Shi, Thomas L. Griffiths, Naomi H. Feldman, and
Adam N. Sanborn. 2010. Exemplar models as a mecha-
nism for performing Bayesian inference. Psychonomic
Bulletin and Review, 17(4):443–464.

Samuel Thomas, Sriram Ganapathy, and Hynek Herman-
sky. 2012. Multilingual MLP features for low-resource
LVCSR systems. Proceedings of the IEEE Interna-
tional Conference on Acoustics, Speech and Signal
Processing, pages 4269–4272.

439



Joseph C. Toscano, Bob McMurray, Joel Dennhardt, and
Steven J. Luck. 2010. Continuous perception and
graded categorization: Electrophysiological evidence
for a linear relationship between the acoustic signal and
perceptual encoding of speech. Psychological Science,
21(10):1532–1540.

Stavros Tsakalidis and William Byrne. 2005. Acous-
tic training from heterogeneous data sources: Exper-
iments in Mandarin conversational telephone speech
transcription. Proceedings of the IEEE International
Conference on Acoustics, Speech, and Signal Process-
ing, pages 461–464.

Olli Viikki and Kari Laurila. 1998. Cepstral domain
segmental feature vector normalization for noise robust
speech recognition. Speech Communication, 25(1):133–
147.

Qing Wang, Sanjeev R Kulkarni, and Sergio Verdú. 2006.
A nearest-neighbor approach to estimating divergence
between continuous random vectors. IEEE Interna-
tional Symposium on Information Theory, pages 242–
246.

Weiran Wang, Raman Arora, Karen Livescu, and Jeff A.
Bilmes. 2015. Unsupervised learning of acoustic fea-
tures via deep canonical correlation analysis. Proceed-
ings of the IEEE International Conference on Acoustics,
Speech, and Signal Processing.

Steven Wegmann, Don McAllaster, Jeremy Orloff, and
Barbara Peskin. 1996. Speaker normalization on con-
versational telephone speech. Proceedings of the IEEE
International Conference on Acoustics, Speech, and
Signal Processing, pages 339–341.

L. Welling, N. Haberland, and H. Ney. 1997. Acous-
tic front-end optimization for large vocabulary speech
recognition. Proceedings of Eurospeech, pages 2099–
2102.

440


