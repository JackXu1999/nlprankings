










































A Self-learning Template Approach for Recognizing Named Entities from Web Text


International Joint Conference on Natural Language Processing, pages 1139–1143,
Nagoya, Japan, 14-18 October 2013.

A Self-learning Template Approach for Recognizing Named Entities from
Web Text

Qian Liu†‡, Bingyang Liu†‡, Dayong Wu‡, Yue Liu‡, Xueqi Cheng‡
†University of Chinese Academy of Sciences, Beijing, China

‡Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China
{liuqian, liubingyang}@software.ict.ac.cn

{wudayong, liuyue, cxq}@ict.ac.cn

Abstract

Recognizing Chinese Named entities from
the Web is challenging, due to the lack of
labeled data and differences between Chi-
nese and English. We propose a semi-
supervised approach which leverages seed
entities and the large unlabeled data to
learn templates. Some high-quality tem-
plates are generated iteratively to extrac-
t new named entities based on the model
of quality metrics. Experimental results
show that our approach significantly out-
performs the baseline method and it is ro-
bust against the changes of the Web.

1 Introduction

Compared to English Named Entity Recognition
(NER), Chinese NER is more difficult. For exam-
ple, although capitalization plays an very impor-
tant role in English NER, this language-specific
feature is useless in Chinese NER. Moreover, the
lack of space between words in Chinese often
makes the work based on word segmentation (Sun
et al., 2002) failure. Especially, there are a lot of
new words and ambiguous words in the Web tex-
t, which increase the error of word segmentation.
The loss of precision propagates to NER.

Currently, NER is mainly based on supervised
models. Such models perform well in single do-
main (Wang, 2009; Du et al., 2010). Unfortunate-
ly, the data of the Web is open-domain and always
changing. The performance of them degrades bad-
ly, since the distribution of the Web data is differ-
ent from that of the training data. For instance, the
average F1 score of the Stanford NER, which is
trained on the CoNLL03 shared task data set and
achieves state-of-the-art performance on that task,
drops from 90.8% to 45.8% on tweets (Liu et al.,
2011). Despite a high-quality training data set,
which has the same distribution as the Web data

and covers all kinds of domains, can improve the
performance of NER, as far as we know, there are
no such labeled data. Furthermore, named entities
change over the time, especially person names and
company names. (Tsuchiya et al., 2009) shows
that 20%∼30% of named entity types are replaced
with new ones every year in Mainichi Newspaper
articles. Such change is even more obvious in the
Web data and leads to the unreliable labeled data.
Constantly annotating new data is time-consuming
and expensive.

In this paper, we propose a semi-supervised ap-
proach that uses self-learning templates to solve
above problems. Instead of annotating a massive
amount of data, we leverage a small number of
named entities and the large unlabeled data to dis-
cover new named entities that can not be identified
using the training data. Experiments show that our
approach raises the F1 from 75.9% to 88.6% on
the Chinese Web data without retraining the exist-
ing model, and it is robust against the changes of
the data. Since no language-specific knowledge is
used, our approach can easily be extended to other
languages.

2 Related Work

There have been many approaches proposed to
solve the problem of the lack of annotated da-
ta. (Wu et al., 2009; Chiticariu et al., 2010) fo-
cus on domain adaptation, which aims to reuse
the knowledge among different domains. (Ling
and Weld, 2012; Rüd et al., 2011; Han and Zhao,
2010) leverage information from external knowl-
edge sources, such as Wikipedia, WordNet and
search engine, to compensate for the insufficien-
t training data. Some work builds crowdsourc-
ing services to label data by human. For exam-
ple, Amazon Mechanical Turk1 provides a platfor-
m to obtain data in various domains such as email

1https://www.mturk.com/mturk/welcome

1139



(Lawson et al., 2010), medicine (Yetisgen-Yildiz
et al., 2010) and Twitter (Finin et al., 2010).

The work based on context templates is more
closely related to our approach. (Etzioni et al.,
2005) extracts named entities via domain-specific
templates, which are learned from predefined tem-
plates. (Whitelaw et al., 2008) builds training
data utilizing templates generated by millions of
seeds. Although context templates are used to im-
prove the perform of NER in our approach, they
are learned automatically from the unlabeled data
and we only use several seed entities.

3 Approach based on self-learning
Templates

3.1 Overall Framework of Our Approach

The main idea of our approach is that learning the
high-quality templates in the bootstrapping pro-
cess.

The details are shown in Algorithm 1, where
the pair ⟨name, type⟩ represents an entity,
named name, in class type. # denotes a
placeholder for entity. The substring, like
ts−2ts−1name

(i)
s ts+1ts+2, denotes the ith NE in

the set of seed entities and its context of four to-
kens long. First, for each entity e in Eseed, we find
sentences containing e and create a temporary set
of templates. Second, for each candidate template,
we relocate all possible named entities Etemp in
Ccorpus. Third, computing the quality of templates
and adding the high-scoring ones into template set
TStemplate. Lastly, we compute the confidence of
candidate entities that are generated in above pro-
cess, and remove the entities whose confidence are
below the threshold from the set of entities. The
value of threshold will be discussed in Section 4.

3.2 Features of Template

Given a candidate template, we define three statis-
tical features to measure the quality of it.

effectiveness (f1): This feature reflects whether
a candidate template is prone to mistakes. We as-
sume that tokens outside of Eseed are not named
entities. It is a reasonable assumption in practice,
because the loss caused by assumption will be-
come lower and lower with the increase of Eseed.
The effectiveness is calculated as

f1(T, c) = p(e|Tc) · p(Tc) =
#(correct e|Tc)∑

i

#(e|T (i)c )
(1)

Algorithm 1 Framework of Templates Learning
Input: a set of NEs: Eseed = {< name, type >} ; unlabeled web pages:

Corpus
Output: Eseed; TStemplate
1: Initialize Ccorpus: ϕ
2: Initialize TScandidate: ϕ
3: Initialize TStemplate: ϕ
4: while Eseed keep growing (or below the predefined number of loops) do
5: for each entity(i) =< name(i), type(i) >∈ Eseed do
6: Add all sentences containing name(i) to Ccorpus
7: Create templates < t−2t−1#t+1t+2, type > when the sub-

string ts−2ts−1name(i)s ts+1ts+2 belongs to some sentence
in Ccorpus and add them to TScandidate

8: end for
9: for each candidate template

T (i) =< t
(i)
−2t

(i)
−1#t

(i)
+1t

(i)
+2, type >∈ TScandidate do

10: Extract all matching tokens < token, type > while the
substring t(i)−2t

(i)
−1tokent

(i)
+1t

(i)
+2 belongs to some sentence in

Ccorpus and add them to Etemp
11: end for
12: for each template T (i) ∈ TScandidate do
13: Calculate the score (T (i), score)

= evaluation(TScandidate|Ccorpus, Eseed, Etemp, )
14: if score> δ then
15: Add T (i) to TStemplate
16: end if
17: end for
18: Find new candidate NEs Ecandidate using TStemplate from the

remaining unlabeled data
19: Select high-quality NEs:

E′seed = filter(Ecandidate|TStemplate)
20: Update: Eseed = Eseed ∪ E′seed
21: end while

where #(e|T (i)c ) denotes the number of correct en-
tities extracted by the ith template in class c.

discrimination (f2): This feature is used for
measuring how close a candidate template is re-
lated to a class. The value is computed as

f2(T, c) = tf(T, c) ·
(

1 + log
#C

1 + #cj

)
(2)

where tf(T, c) denotes the normalized frequency
of template, that is divided by the maximum fre-
quency, #C is the number of classes in Eseed, and
#cj is the number of classes that template T ap-
pears.

diversity (f3): The more different and correct
NEs in a class extracted by template T , the more
likely other good NEs within the same class will
be extracted by it. This feature is computed as

f3(T, c) =
#{(correct e|Tc) ∧ (different e|Tc)}

#(correct e|Tc)
(3)

3.3 Quality Metrics Model of Templates

Since the proposed features are not independen-
t of each other, we propose an approach in which
the value of a feature is adapted according to the
values of other features. Although it is similar to
(Wei et al., 2010), we improve it by only updat-
ing a part of templates to reduce the computational
cost.

1140



Formally, given a set of candidate templates
TScandidate = {T (1), T (2), ..., T (n)} ⊂ Rm , let
fk : TScandidate → R denote the ranking function
on the kth feature, where fk ∈ F = {f1, f2, f3}.
Our goal is to combine all features to produce
ranking list that are better than any individual fea-
ture and then return the templates with high rank-
ing scores.

Following the traditional manifold ranking pro-
cess (Zhou et al., 2004) with one ranking
function. 1) Defining the similarity matrix
W on the template set TScandidate: Wij =
similarity(T (i), T (j)). 2) Symmetrically normal-
izing W by S = D−1/2WD−1/2 in which D
is the diagonal matrix with (i, i)-element equal
to the sum of the ith row of W . 3) Iterating
F (t + 1) = αSF (t) + (1 − α)F (0) until a glob-
al stable state, where α is trade-off parameter in
(0, 1), F (0) denotes the initial ranking results and
F (t) denotes the ranking results of the tth round.

For two ranking functions f1 and f2, the rank-
ing score of f1 will be changed after combining
the ranking score of f2. Considering the cost of
consistency both the ranking results in initial f1
and the feedback from f2, we define the cost func-
tion caused by refining f1 with f2 in the (t + 1)th

round iteration as

φ(f1|f2) =
1

2

 n∑
i,j=1

Wij

∥∥∥∥∥ 1√Dii f(t+1)1 (T (i))− 1√Djj f(t)2 (T (i))
∥∥∥∥∥
2

+µ

n∑
i=1

∥∥∥f(t+1)1 (T (i))− f(0)1 (T (i))∥∥∥2
)

(4)

where f (0)1 denotes the initial ranking scores of f1.
Let the best refined ranking score is f∗, we have

∂

∂f1
φ(f1|f2)

∣∣
f1=f

∗ = f
∗ − S · f(t)2 + u(f

∗
1 − f

(0)
1 ) = 0 (5)

f
∗

=
1

1 + µ
· S · f(t)2 +

µ

1 + µ
· f(0)1 (6)

Let α = 11+µ , then we have

f
∗

= α · S · f(t)2 + (1− α) · f
(0)
1 (7)

Therefore, we can iteratively compute the rank-
ing scores in the (t + 1)th round to find the best
f∗ shown as follows, which is proven to be con-
vergent (Wei et al., 2010).

f
(t+1)
1 = α · S · f

(t)
2 + (1− α) · f

(0)
1 (8)

In our case, due to the templates with low rank-
ing scores on f2 are helpless to refine the results

ranked on f1. We omit the templates in f2 that
fall below the threshold and feedback the rest tem-
plates, signaled by Top f2, to f1. Then, the nor-
malized matrix S can be simplified as a block ma-

trix
(

STop f2 0
0 0

)
. The equation above can be

rewritten as

f
(t+1)
1 (Top f2) = α·ST op f2 ·f

(t)
2 (Top f2)+(1−α)·f

(0)
1 (Top f2)

(9)

Moreover, if we only improve the identical tem-
plates, the similarity matrix WTop f2 and normal-
ized matrix STop f2 degrade to identity matrices,
the final iteration equation is

f
(t+1)
1 (Top f2) = α · f

(t)
2 (Top f2)+ (1−α) · f

(0)
1 (Top f2) (10)

3.4 Confidence of Candidate NEs
There may be still noisy in the set of candidate
NEs despite using the high-quality templates to
find new entities. Therefore, we also need to fil-
ter out the non-NEs for the further processing to
insure the high accuracy. This section correspond-
s to the line 19 of Algorithm 1. We utilize the
pointwise mutual information (PMI) (Etzioni et
al., 2005) to measure the closeness between ex-
tracted NEs and templates.

Given an extracted named entity e and a tem-
plate T , the PMI score is computed as

PMI(e, T ) =
Hits(e + T )

Hits(e)
(11)

where Hits(·) denotes the number of sentences
searched in the whole unlabeled corpus.

The confidence of e extracted by template Tc
belonging to class c can be expressed as

confidence(entity, c) =
1

#Tc

∑
i

PMI(entity, T
(i)
c ) (12)

where #Tc denotes the number of templates in
class c.

4 Experiments

4.1 Data Set
We conducted experiments on a real data set col-
lected from the Web, which is from August 1th

2012 to August 31th 20122. The details of the
data set are given in Table 1. Two fine-grained
categories of Person are considered: Singer and
Athlete. Note that the NER on fine-grained cate-
gories is more difficult than that on coarse-grained

2During the preprocessing step, HTML tags and ads are
eliminated from the data.

1141



categories such as Person, Location and Organi-
zation. We randomly selected 8,955 sentences and
manually labeled them as test data, which contain-
s 232 singers and 1,807 athletes. The labeled data
is further split into two parts: one for training the
baseline system and the other for testing. The test
data set includes 65 singers and 406 athletes. We
trained a linear CRF model (Lafferty et al., 2001)
as the baseline using the BILOU scheme (Ratinov
and Roth, 2009).

News Forum Microblog
Number 1,087,926 420,278 49,037,301

Table 1: The composition of the data set.

4.2 Experimental Analysis

Table 2 gives some examples of learned templates.
Using the learned templates and extracted named
entities, an additional recognizer can be built very
easily. It can discover some new named entities
that are left out by models trained on labeled da-
ta. We performed experiments to make compar-
ison between baseline system and AD NER sys-
tem, which integrates additional recognizer into
the CRF model. The results are shown in Table
3∼5, where p is the threshold of confidence score
of candidate entity.

Template Q score Num. of NEs
Singer
欢、#、庾(Huan, #, Geng) 0.9000 60
#演唱(sing) 0.8109 279
天后#，(diva) 0.6620 319
听着#的歌(listen to the song) 0.6120 9
演唱#的歌(sing a song) 0.5820 16
Athlete
冠军#，(champion) 0.9059 2340
名将#，(famous athlete) 0.8711 481
选手#，(player) 0.8382 2440
战胜#夺冠(win) 0.7724 123
选手#在比(in a competition) 0.6423 274

Table 2: Examples of templates and their quali-
ties (Q score) and the number of extracted named
entities.

Precision Recall F1
Baseline 93.1 41.5 57.4
AD NER(p = 0.001) 71.3 83.1 76.8
AD NER(p = 0.01) 75.4 88.4 81.4
AD NER(p = 0.1) 90.6 47.7 62.5

Table 3: Results on Singer.

Precision Recall F1
Baseline 97.8 65.9 78.8
AD NER(p = 0.001) 79.9 91.5 85.3
AD NER(p = 0.01) 92.8 88.3 90.5
AD NER(p = 0.1) 93.9 71.4 81.1

Table 4: Results on Athlete.

Precision Recall F1
Baseline 97.3 62.3 75.9
AD NER(p = 0.001) 76.2 90.0 82.5
AD NER(p = 0.01) 91.3 86.1 88.6
AD NER(p = 0.1) 93.2 67.8 78.5

Table 5: Overall experimental results.

From Table 3 and Table 4, we find the best per-
formance of baseline on Singer is 57.4%, 21.4%
lower than that on Athlete. This can be explained
as the scale of labeled data impacts the perfor-
mance of supervised method, because, in Table 1,
the instances of Singer is not as sufficient as Ath-
lete. However, the performance of our approach in
the two categories remains stable. This illustrates
that the large-scale unlabeled data is useful in the
case of a lack of training data.

As shown in Table 5, the best F1 of AD NER is
88.6%, which is 12.7% higher than F1 of baseline.
Although there is a somewhat loss of precision, we
obtain a large number of named entities. More-
over, the cost of our approach is lower than auto-
matic annotation, since we do not need to retrain
the supervised model. It is more effective when la-
beled data is complex and hard to construct while
unlabeled data is abundant and easy to access. In
the practice, our proposed approach can easily re-
main up-to-date and extend the well-trained super-
vised model without fine-tuning or any human in-
tervention.

We further find that the overall precision of
AD NER with threshold at 0.1 is only 1.9% high-
er than that with threshold at 0.01, but the loss of
recall is 18.3%. For this reason, the threshold can
be set to 0.01.

5 Conclusion

In this paper, we propose an approach to build an
additional named entity recognizer that can assist
the existing supervised models. The experimental
results on the real data set from the Web show that
our method improves the F1 score from 75.9% to
88.6%.

1142



References
Laura Chiticariu, Rajasekar Krishnamurthy, Yunyao

Li, Frederick Reiss, and Shivakumar Vaithyanathan.
2010. Domain adaptation of rule-based annotators
for named-entity recognition tasks. In Proceedings
of the 2010 Conference on Empirical Methods in
Natural Language Processing, pages 1002–1012.

Junwu Du, Zhimin Zhang, Jun Yan, Yan Cui, and
Zheng Chen. 2010. Using search session context for
named entity recognition in query. In Proceedings
of the 33rd international ACM SIGIR conference on
Research and development in information retrieval,
pages 765–766.

Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld, and Alexander Yates. 2005. Un-
supervised named-entity extraction from the we-
b: An experimental study. Artificial Intelligence,
165(1):91–134.

Tim Finin, Will Murnane, Anand Karandikar, Nicholas
Keller, Justin Martineau, and Mark Dredze. 2010.
Annotating named entities in twitter data with
crowdsourcing. In Proceedings of the NAACL HLT
2010 Workshop on Creating Speech and Language
Data with Amazon’s Mechanical Turk, pages 80–88.

Xianpei Han and Jun Zhao. 2010. Structural semantic
relatedness: a knowledge-based method to named
entity disambiguation. In Proceedings of the 48th
Annual Meeting of the Association for Computation-
al Linguistics, pages 50–59.

John Lafferty, Andrew McCallum, and Fernando CN
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. Proceedings of the Eighteenth Interna-
tional Conference on Machine Learning, pages 282–
289.

Nolan Lawson, Kevin Eustice, Mike Perkowitz, and
Meliha Yetisgen-Yildiz. 2010. Annotating large e-
mail datasets for named entity recognition with me-
chanical turk. In Proceedings of the NAACL HLT
2010 Workshop on Creating Speech and Language
Data with Amazon’s Mechanical Turk, pages 71–79.

Xiao Ling and Daniel S Weld. 2012. Fine-grained en-
tity recognition. In Proceedings of the 26th Confer-
ence on Artificial Intelligence.

Xiaohua Liu, Shaodian Zhang, Furu Wei, and Ming
Zhou. 2011. Recognizing named entities in tweet-
s. In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies, pages 359–367.

Lev Ratinov and Dan Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
Proceedings of the Thirteenth Conference on Com-
putational Natural Language Learning, pages 147–
155.

Stefan Rüd, Massimiliano Ciaramita, Jens Müller, and
Hinrich Schütze. 2011. Piggyback: Using search
engines for robust cross-domain named entity recog-
nition. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 965–975.

Jian Sun, Jianfeng Gao, Lei Zhang, Ming Zhou, and
Changning Huang. 2002. Chinese named entity i-
dentification using class-based language model. In
Proceedings of the 19th international conference on
Computational linguistics, pages 1–7.

Masatoshi Tsuchiya, Shoko Endo, and Seiichi Nak-
agawa. 2009. Analysis and robust extraction of
changing named entities. In Proceedings of the 2009
Named Entities Workshop: Shared Task on Translit-
eration, pages 161–167.

Yefeng Wang. 2009. Annotating and recognising
named entities in clinical notes. In Proceedings of
the ACL-IJCNLP 2009 Student Research Workshop,
pages 18–26.

Furu Wei, Wenjie Li, and Shixia Liu. 2010. irank: A
rank-learn-combine framework for unsupervised en-
semble ranking. Journal of the American Society for
Information Science and Technology, 61(6):1232–
1243.

Casey Whitelaw, Alex Kehlenbeck, Nemanja Petrovic,
and Lyle Ungar. 2008. Web-scale named entity
recognition. In Proceedings of the 17th ACM con-
ference on Information and knowledge management,
pages 123–132.

Dan Wu, Wee Sun Lee, Nan Ye, and Hai Leong Chieu.
2009. Domain adaptive bootstrapping for named en-
tity recognition. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1523–1532.

Meliha Yetisgen-Yildiz, Imre Solti, Fei Xia, and Scot-
t Russell Halgrim. 2010. Preliminary experience
with amazon’s mechanical turk for annotating med-
ical named entities. In Proceedings of the NAACL
HLT 2010 Workshop on Creating Speech and Lan-
guage Data with Amazon’s Mechanical Turk, pages
180–183.

Dengyong Zhou, Olivier Bousquet, Thomas Navin Lal,
Jason Weston, and Bernhard Schölkopf. 2004.
Learning with local and global consistency. Ad-
vances in neural information processing systems,
16:321–328.

1143


