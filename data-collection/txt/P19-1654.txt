



















































VIFIDEL: Evaluating the Visual Fidelity of Image Descriptions


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6539–6550
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

6539

VIFIDEL: Evaluating the Visual Fidelity of Image Descriptions

Pranava Madhyastha⇔, Josiah Wang⇔ and Lucia Specia
Department of Computing

Imperial College London, UK
{pranava, josiah.wang, l.specia}@imperial.ac.uk

Abstract

We address the task of evaluating image de-
scription generation systems. We propose
a novel image-aware metric for this task:
VIFIDEL. It estimates the faithfulness of a
generated caption with respect to the content
of the actual image, based on the semantic
similarity between labels of objects depicted
in images and words in the description. The
metric is also able to take into account the rel-
ative importance of objects mentioned in hu-
man reference descriptions during evaluation.
Even if these human reference descriptions are
not available, VIFIDEL can still reliably eval-
uate system descriptions. The metric achieves
high correlation with human judgments on two
well-known datasets and is competitive with
metrics that depend on and rely exclusively on
human references.

1 Introduction

A popular task at the intersection of computer
vision and natural language is image description
generation (IDG), i.e. the task of generating as
output a sentence describing the visual content of
a given input image. While a variety of methods
have been proposed for this task (Kulkarni et al.,
2011; Li et al., 2011; Vinyals et al., 2015), its eval-
uation is still an understudied problem. Evalua-
tion of IDG is currently performed in two ways:
(i) human judgment; (ii) automatic metrics. Hu-
man judgments evaluate either the overall quality
of descriptions or specific criteria in isolation (rel-
evance, fluency, etc.). Such methods, however, can
be subjective and expensive to scale.

Automatic metrics address the scalability is-
sue by comparing candidate descriptions against
human-authored reference descriptions. These
metrics conflate various criteria implicitly into a

⇔ Pranava Madhyastha and Josiah Wang contributed
equally as joint first authors to this work.

single evaluation assumption, i.e. a good descrip-
tion is one that is similar to one or more human-
authored descriptions, presuming that these gold
descriptions are fluent, correct and relevant to the
image. Existing automatic metrics are thus use-
ful for measuring the quality of descriptions as a
whole, but this makes it difficult for the specific
capabilities of IDG systems to be inspected.

We argue that a fine-grained metric measuring
specific criteria would be more useful in under-
standing how an IDG system is better than an-
other. We focus on one such criterion, visual fi-
delity1. This criterion aims to measure how faith-
ful a description is with respect to what is depicted
in the image (i.e. systems should be rewarded for
describing elements depicted in the image and pe-
nalised for describing things that are not depicted).
For that, we propose to take image content into
account when evaluating descriptions, in contrast
to previous work (§2) that rely solely on words
in the reference descriptions. Given that most
datasets are crowd-sourced, reference descriptions
may not always accurately describe the image (de-
scribing non-depicted objects, not mentioning rel-
evant objects, e.g. Figure 1). For reliable evalu-
ation, multiple references are needed. Image an-
notations (with objects, attributes, relations, etc.),
on the other hand, are arguably more general and
less ambiguous for evaluating visual fidelity: they
require a single annotation per image, and are not
affected by language or style preferences. To our
knowledge, no existing metric for IDG has images
factored explicitly into the evaluation process.

Our main contribution is therefore an auto-
matic evaluation metric for IDG that measures
the fidelity of image descriptions with respect to
the image, using information derived from im-
ages directly as ‘reference’ (Figure 2). We name

1Also referred to as relevance, faithfulness or correctness.



6540

(a) ‘What is this??’ (b) ‘so what else isshe supposed to do?’

Figure 1: Examples of inadequate references taken
from (a) PASCAL-50S and (b) MSCOCO.

it VIFIDEL (VIsual Fidelity for Image Descrip-
tion EvaLuation). VIFIDEL can be used (i) based
only on images as reference (no textual references)
(§3.2), or (ii) in conjunction with textual refer-
ences to take into account the relevant image con-
tent people describe (§3.3). In addition, VIFIDEL
performs matching of images and text in an em-
beddings space, thus drawing both modalities to-
gether semantically while avoiding the pitfalls of
mainstream metrics that rely on exact or approx-
imate string matching. This is done by building
on the Word Mover’s Distance (WMD) metric,
which measures the distance between two texts in
a word embeddings space. Another contribution is
the extension of WMD to allow for multiple refer-
ences to be used to model object importance, i.e.
an approach for consensus within WMD. We eval-
uate the performance of VIFIDEL against human
judgments on two popular IDG datasets (§4).

2 Background

Various IDG metrics have either been adapted
from other fields or proposed specifically for IDG.
Examples of the former include BLEU (Pap-
ineni et al., 2002) and Meteor (Denkowski and
Lavie, 2014) from machine translation evalua-
tion, and ROUGE (Lin, 2004) (more specifically
ROUGEL (Lin and Och, 2004)) from text sum-
marisation evaluation. Metrics designed specif-
ically for evaluating image descriptions include
CIDEr (Vedantam et al., 2015), SPICE (Anderson
et al., 2016) and BAST (Ellebracht et al., 2015).

One main weakness of most metrics (BLEU,
ROUGE, Meteor, CIDEr) is that they rely on exact
string matching to measure the surface-level, n-
gram overlap between candidate texts and human
references. This can result in data spasity prob-
lems, especially with limited references. Meteor
partially addresses this by matching synonyms
from dictionaries and paraphrase tables, but it is
constrained to the availability of such dictionaries,

dog  frisbee  ball  chair table  potted plant 

a dog  stands  near  toys  by  the  beach  .

0.001  0.65  0.10.9  0.55 + + + +
Distance = 2.201

VIFIDEL = exp(­2.201) = 0.11

Figure 2: An illustration of VIFIDEL as a visual fi-
delity metric for IDG. Bright yellow arrows indicate
that the word is semantically similar to the object ac-
cording to Word Mover’s Distance, while darker arrows
show that they are less related (larger distance).

making it hard to scale to different languages.
Metrics like SPICE and BAST address the is-

sue of exact string matching by measuring simi-
larity on a semantic level. However, these meth-
ods rely heavily on linguistic resources such as
parsers, semantic role labellers, tailored rules, etc.,
making evaluation difficult to scale or adapt to
different languages and domains. Word Mover’s
Distance (WMD) (Kusner et al., 2015) has also
been proposed as an IDG metric (Kilickaya et al.,
2017). WMD finds optimal alignments between
word embeddings in candidate and reference de-
scriptions instead of performing n-gram matching
to address data sparseness issues. VIFIDEL is in-
spired by WMD, but goes beyond using reference
texts by comparing candidate texts against image
content (§3.2).

Using images for IDG evaluation. As far as
we are aware, no previous work uses images for
evaluating IDG. The closest related work is that
by Wang and Gaizauskas (2015), who propose
an f -measure-based metric to evaluate the task
of selecting relevant object instances to be men-
tioned in a description. The metric computes the
overlap between selected object instances and ob-
jects mentioned in references, averaged over mul-
tiple references. The averaging process implic-
itly captures consensus over which objects should
be mentioned (§3.3), i.e. objects mentioned in
more references should be more important than
those mentioned in fewer. Their work, however,
requires manual correspondence annotations be-
tween bounding box instances and object men-
tions in descriptions. Our proposed method lever-
ages word embeddings to circumvent the need for
exact correspondence annotations.



6541

Complementarity of metrics. Kilickaya et al.
(2017) report that the n-gram based metrics, the
semantic graph based SPICE, and the embedding-
based WMD capture complementary information,
and that linearly combining Meteor, SPICE and
WMD gives a better correlation score against hu-
man judgments. Similarly, Liu et al. (2017) op-
timise image description generation to capture
both semantic faithfulness and syntactic fluency
by combining metrics with complementary prop-
erties. However, the combination weights need to
be engineered towards the task.

3 VIFIDEL

In this section we describe the VIFIDEL metric.
It is inspired by Word Mover’s Distance (WMD)
(§3.1), which measures the distance between two
documents in a word embedding space. VIFIDEL
however explicitly incorporates semantic informa-
tion derived from images (§3.2), which can be
used with or without reference descriptions (§3.3),
using word embeddings as a bridge for matching
content in images to content in textual descrip-
tions. In contrast to WMD which compares pairs
of documents, VIFIDEL also allows for multi-
ple references to be taken into account with a
consensus-based approach.

3.1 Word Mover’s Distance (WMD)

WMD (Kusner et al., 2015) makes use of word
vector relationships between word embeddings to
compute the distance between two text documents.
WMD captures the minimal distance required to
move words from the first document to words in
the second document.

Let X ∈ RN×K be a matrix, with K-
dimensional word embeddings for a vocabulary of
N words. Let xi ∈ RK be a K-dimensional word
embedding vector for word i. A document ∆ is
represented as an N -dimensional normalised bag-
of-words (BOW) vector, d∆ = (d∆1 , d

∆
2 , ..., d

∆
N ),

where d∆i is the normalised frequency of word
i occurring in document ∆. Stop words are re-
moved from documents; only content words are
retained. Kusner et al. (2015) state that stop words
are generally less relevant for capturing semantic
similarity between documents, especially for bag-
of-words representations.

As a measure of word-level dissimilarity, Kus-
ner et al. (2015) propose the word travel cost, that
is the cost of moving from word i to word j, using

the Euclidean distance between the embeddings
corresponding to words. More precisely, the cost
is defined as:

c(i, j) = ‖xi − xj‖p2 , (1)

where p is usually 1 or 2 (we set p=2). This allows
documents with many closely related words to
have smaller distances than documents with very
dissimilar words.

To measure distances between two documents
α and β, WMD defines a transport matrix T ∈
RN×N , where Tij contains information about the
proportion of word i in dα that needs to be trans-
ported to word j in dβ . Formally, WMD computes
T that optimises:

WMD(dα, dβ) = min
T≥0

N∑
i,j=1

Tij c(i, j) (2)

such that:
∑N

j=1 Tij=d
α
i and

∑N
i=1 Tij=d

β
j , ∀ i, j.

Here, the normalised bag-of-words distribution of
the documents dα and dβ contains a combined
vocabulary from dα and dβ resulting in a square
transport matrix T of dimensionality N×N .

Kusner et al. (2015) note that WMD is a spe-
cial case of Earth Mover’s Distance (Rubner et al.,
2000), popular in the computer vision community,
or Wasserstein’s Distance (Datta et al., 2008), pop-
ular in the optimal transport community.

3.2 Using objects as image information
In this paper, we explore image information in the
form of explicit object detections, both using gold
or predicted object instances for a given image.
Previous works (Yin and Ordonez, 2017; Mad-
hyastha et al., 2018; Wang et al., 2018) have found
explicit object detections to be informative for im-
age description generation. Thus, we base our in-
tuition on the hypothesis that a thorough and true
description of the image should consist of infor-
mation about objects and their interactions (fre-
quencies, etc.) in the environment. VIFIDEL has
the capacity to capture these.

While objects represent one important type of
semantic image information, VIFIDEL can po-
tentially incorporate other semantic image infor-
mation including attributes, actions, positions,
scenes, relations between objects, and more fine-
grained information such as colour. For this pa-
per, we consider only the frequency of depicted
object category instances as semantic information,
and regard further enrichment as future work.



6542

As mentioned, a motivation for using image in-
formation for evaluating image descriptions is that
reference descriptions can be subjective, ambigu-
ous and may or may not specify all (and only) the
important elements of the image. In fact, they of-
ten focus on a subset of the image content. Using
object labels can minimise these issues. In addi-
tion, as shown in Figure 1, references are not al-
ways actual descriptions and can be incorrect.

Another advantage of a metric based on object-
level information is the cost of collecting data: ei-
ther objects are predicted (no labelling involved)
or, if gold labels are to be used for more reliable
results, object annotations can be gathered in more
trustworthy ways using a single annotation per im-
age. With descriptions, it has been shown that
multiple descriptions per image are needed for re-
liable evaluation (Vedantam et al., 2015).

Figure 2 gives an intuitive illustration of our
metric. The top row shows a set of detected ob-
ject instances in the image. The bottom row shows
content words in a system description that are se-
mantically very similar to the detected objects –
dog, toys, etc. The description does not men-
tion all detected objects (e.g. it misses table) and
contains the word beach that is not in the image.
VIFIDEL aims to capture these discrepancies.

More specifically, VIFIDEL is defined as the
similarity between the semantic content in image
I and a description S. It is specified by the in-
verse of the minimum cumulative cost required to
move semantic labels (e.g. object categories) from
image I to words in the description S. This con-
verts WMD from a distance measure to a similar-
ity measure. Formally:

VIFIDEL(I, S) = exp(−WMD(dI , dS)) (3)

where dS is the normalised bag of words represen-
tation for description S (§3.1), and dI is a seman-
tic vector representation for image I , specifically
a normalised bag of object category labels (§4.1).
WMD is defined in Eq. 2.

In its basic form, VIFIDEL can provide infor-
mation about the compatibility between an image
and a description without using reference descrip-
tions. We show the performance of VIFIDEL in
the absence of reference descriptions in §4.

3.3 Modelling object importance with
reference descriptions

We now expand the basic version of VIFIDEL to
use (one or more) human references when avail-

able. Human references allow for capturing the
human-likeness aspect of descriptions, that is, they
capture what humans consider important to be de-
scribed for a picture (Berg et al., 2012).

VIFIDEL can use human references as addi-
tional guidance to determine the importance of ob-
ject content in a given image. We exploit the fact
that each reference may only describe a particular
subset of the image content, and we assume that
important objects are mentioned more frequently
across references than less important ones. Our
proposal is similar to CIDEr in that we capture
consensus information given a set of human ref-
erences. However, we significantly differ in that
(i) we use the references to explicitly model ob-
ject importance, instead of directly comparing the
candidates against the references; (ii) we perform
word matching in a semantic space using word
embeddings rather than surface forms.

VIFIDEL also differs from previous ap-
proaches using WMD for image description eval-
uation (Kilickaya et al., 2017), where the metric is
only computed using the description and the sin-
gle closest reference. One of the problems in such
an approach is the biased choice of reference: in
this case, the reference with the smallest WMD
distance from the system description. A better ref-
erence may be available, e.g. mentioning more
image content, which would lead lower system
scores in an unbiased evaluation. This has been
a common problem in metrics based on a single
reference (Fomicheva and Specia, 2016).

Another contribution in this paper is therefore
the incorporation of an object importance model
into the WMD framework using human refer-
ences. Our approach rewards candidate descrip-
tions that mention objects depicted in the image
(i.e. faithful to image content), and that the objects
are also mentioned frequently across all references
(i.e. they mention important objects). Intuitively,
the WMD cost function (Eq. 1) is replaced with
a weighted Euclidean distance. These weights
are derived from human descriptions. While the
original cost function captures the faithfulness
of candidate descriptions to depicted objects, the
weights extend the function such that the cost
is lower for words that are mentioned frequently
across references (either as an exact match or a
semantically similar match), and higher for those
mentioned less frequently. The weights are ap-
plied to both the object labels from images and



6543

words for candidate descriptions.
Formally, let RI = (RI1, R

I
2, . . . , R

I
M ) be a set

of M human references for image I . The per-
image penalty weight, ρIk, for a word k (an object
label in image I or a content word in a candidate
description SI ) is computed as:

ρIk =
1

M

M∑
r=1

(
1−maxt∈{RIr} cos(xk, xt)

2

)
(4)

where {RIr} is the set of content words in the rth
reference for image I , and xt the word embedding
for word t. The denominator 2 ensures that wIk is
always in the range [0, 1].

For each image I , we compute the penalty ρIk
for each k ∈ {t|dIt > 0} ∪ {t|d

SI
t > 0}, i.e. the

union of all labels for objects depicted in I and all
content words in the candidate description SI to be
evaluated. Thus, ρIk is the effective cosine distance
(∈ [0, 1]) between each word/object label k and its
most similar content word in each human refer-
ence, averaged over all references for the image.
The averaging process implicitly captures the con-
sensus over which objects should be mentioned.
ρIk will be small for words/object labels that can
are mentioned across most references (using the
exact word or a semantically similar word), and
large for those that are mentioned only by a few.

The proposed approach of integrating object
importance replaces the cost c(i, j) in Eq. 2 with a
weighted cost c′(i, j|RI) to move from word i to
word j given references RI :

c′(i, j|RI) = ‖ρIi xi − ρIjxj‖
p
2 (5)

The updated Eq. 2 is then used in Eq. 3 to com-
pute a VIFIDEL score weighted by object im-
portance. Figure 3 illustrates a concrete example
of VIFIDEL’s object importance model using hu-
man references, showing how the cost c′(i, j|RI)
is calculated.

4 Experiments

We experiment with VIFIDEL in two datasets:
the PASCAL-50S Consensus dataset (§4.2) and
human ratings on MSCOCO (§4.3). We compare
VIFIDEL against commonly used IDG metrics.

4.1 Visual annotation and detectors
We test the performance of the following met-
ric variants, where VIFIDEL gold and VIFIDEL

D500 and their union are reported in the main ex-
periments (§4.2 and §4.3), while the remaining are
used for an ablation study (§4.5):
• VIFIDEL gold: This variant uses gold stan-

dard, object-level annotations provided by
the respective datasets. For the PASCAL-
50S dataset, we use the annotations for 20
pre-defined object categories (person, car,
cow, etc.) provided by the PASCAL VOC
challenge (Everingham et al., 2015). For
MSCOCO, we use annotations for 80 object
categories provided by MSCOCO (Lin et al.,
2014). In both datasets, the reference de-
scriptions are sourced independent of the im-
age annotations; thus there is no direct cor-
respondence between the visual annotations
and the descriptions.

• VIFIDEL D80: This variant uses the out-
put of an object detector pre-trained on the
MSCOCO dataset, for 80 MSCOCO cate-
gories. We use the TensorFlow Object De-
tection API (Huang et al., 2017) for this pur-
pose2. We set 0.6 as the confidence threshold
for detected objects.

• VIFIDEL D500: This variant uses the out-
put of an object detector, pre-trained on the
Open Images dataset (Krasin et al., 2017)
with bounding box annotations for 545 ob-
ject categories. Again, we use the Tensor-
Flow Object Detection API3, and set the con-
fidence threshold to 0.4.

• VIFIDEL gold∪D500: We combine the out-
puts of the gold annotation and D500 de-
tector and use unique object labels from the
combination.

• VIFIDEL D80∪D500: We combine the out-
puts of the D80 and D500 detectors.

In this paper, we use only the output labels of
the detectors, and represent the content of an im-
age I as a vector of normalised frequencies over
object labels, dI . A discussion on how the per-
formance of the metric can vary according to the
quality of the objects available is given in S4.5.
The ideal setting would count on a comprehensive
list of objects given by humans.

2faster rcnn inception resnet v2 atrous
coco from https://github.com/tensorflow/
models/blob/master/research/object_
detection/g3doc/detection_model_zoo.md.

3 faster rcnn inception resnet v2 atrous
oid.

https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md
https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md
https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md


6544

cat

tv

book

book

book

ball

Reference	descriptionsTest	image

a	black	cat	standing	next	to	a	ball	and	some	encyclopedias.
Candidate	description

cat encyclopediasbook ball

ρ(cat)	=	(0	+	0	+	0	+	0.25	+	0.25)	/	(5*2)	=	0.05

ρ(encyclopedias)	=	(0.46	+	0.79	+	0.73	+	0.73	+	0.46)	/	(5*2)	=	0.32

a	small	cat	standing	on	some	books	on	a	desk.
a	gray	cat	standing	on	top	of	a	blue	notebook.
a	cat	stands	on	a	binder	on	a	desk	next	to	a	pc.
a	small	gray	kitten	standing	on	a	binder
a	small	kitten	is	standing	on	books	and	binders.

c'( cat , cat )	= 

c'( , encyclopedias )	=book

ρ(cat)	 ρ(cat)	xcat xcat* - *|| ||2

ρ(book)	 xbook xencyclopedias* - *|| ||2ρ(encyclopedias)

ρ(book)	=	(0.26	+	0.69	+	0.71	+	0.71	+	0.26)	/	(5*2)	=	0.26

1 0 0 1 1 1 1

1 1 3 1 0 0 0
dI

dSI

Figure 3: Illustration of how object importance based on human references is computed and integrated into
VIFIDEL. In this example, we compute the weights ρIk for encyclopedias, cat and books. The most similar
word to encyclopedias in each reference description include books, notebook and binder according to the cosine
distance between their word embeddings. These are averaged to obtain a consensus penalty score. The word cat
has a low penalty score because it has either an exact match or a very close semantic match (kitten) to all refer-
ences. The penalty scores are then used as weights to compute the cost c′(book, encyclopedias|I) between the
object label book from the image and the word encyclopedias in the candidate description.

4.2 Accuracy on PASCAL-50S

In this section, we focus on the PASCAL-50S
dataset and tackle the binary forced-choice task
of predicting: “which description is more simi-
lar to A: B or C?”, as proposed by Vedantam
et al. (2015). We focus on the variant compar-
ing two machine generated captions. The dataset
contains multiple crowdsourced image description
for each of 1,000 images from the UIUC PASCAL
dataset (Rashtchian et al., 2010).

Evaluation of system outputs as relative rank-
ings has long been established as the best practice
in many fields where language outputs are pro-
duced and no single correct output exists. The
WMT yearly evaluation campaigns for machine
translation (Bojar et al., 2016), for example, have
argued that relative ranking leads to more reliable
judgments than absolute scores. We therefore con-
sider our findings on this dataset as the most im-
portant.

For the binary forced-choice task, Vedantam
et al. (2015) collected 48 descriptions A per im-
age, and formed pairs of descriptions B and C from
machine generated descriptions and/or the remain-
ing two human descriptions. This corresponds to
the (MM) setting of the dataset: comparing two
machine generated descriptions. Arguably, this is
most interesting subtask from a practical point of
view, which is the setting in which evaluation is
generally performed.

The dataset thus consists of 1,000 (B,C) pairs.
Crowd-sourced gold standard annotations were

References
0 1 5 48

BLEU1 - 0.58 0.61 0.62
BLEU4 - 0.56 0.59 0.60
ROUGEL - 0.58 0.61 0.64
METEOR - 0.62 0.66 0.68
CIDEr - 0.60 0.66 0.68
SPICE - 0.65 0.69 0.69
WMDbest - 0.66 0.70 0.70
WMDworst - 0.66 0.66 0.66
LM 0.54 0.54 0.54 0.54

VIFIDEL gold 0.68 0.69 0.70 0.71
VIFIDEL D500 0.69 0.71 0.72 0.71

VIFIDEL +LM 0.69 0.70 0.71 0.71
VIFIDEL +CIDEr 0.69 0.71 0.72 0.72

Table 1: Accuracy of VIFIDEL on the PASCAL-50S
binary forced-choice task using 0, 1, 5 and 48 refer-
ences, comparing two machine generated descriptions.

provided for each 48×1000 (A,B,C) triplet, and
the ‘consensus’ binary label per (B,C) pair was ob-
tained by majority vote across 48 references. We
also provide results on the other splits in the Ap-
pendix A.

Results and discussion Table 1 presents the
accuracies for the binary forced-choice task for
VIFIDEL, compared to the most commonly used
IDG metrics: BLEU1, BLEU4, ROUGEL, ME-
TEOR, CIDEr-D, and SPICE4. We also report re-
sults with the standard WMD as the metric and

4We use the official COCO (https://github.
com/tylin/coco-caption) and SPICE (https://
github.com/peteanderson80/SPICE) scripts.

https://github.com/tylin/coco-caption
https://github.com/tylin/coco-caption
https://github.com/peteanderson80/SPICE
https://github.com/peteanderson80/SPICE


6545

0 1 5 10 15 20 25 30 35 40 45 48
Number of references

0.475

0.500

0.525

0.550

0.575

0.600

0.625

0.650

0.675

0.700

0.725

A
cc

u
ra

cy

VIFIDELD500
VIFIDELgold

SPICE

METEOR

CIDEr

ROUGEL
BLEU1
BLEU4

Figure 4: Accuracy of consensus predictions on
PASCAL-50S, across increasing number of references,
for the Machine-Machine (MM) subset.

an RNN language model trained on training cap-
tions from MSCOCO (Chen et al., 2015). Note
that standard WMD is already a strong metric, but
it relies on references, like all other metrics in
the table and different from VIFIDEL. VIFIDEL
only uses references to up-weight or down-weight
the matches between image objects and words in
the captions. Notable is the metric’s performance
based solely on image information: VIFIDEL can
distinguish between a correct and incorrect de-
scription – the difficult task of differentiating be-
tween two machine generated descriptions, agree-
ing with human judgments more often than any
other IDG metric that uses a reference. With one
reference description, VIFIDEL gave the high-
est accuracy, suggesting that image-side informa-
tion is indeed helpful for evaluating the quality of
image descriptions when comparing two machine
generated descriptions.

Even when we take more references into ac-
count, VIFIDEL is as good or better than
reference-based metric. With 5 references (a
feasible number from a practical perspective),
VIFIDEL achieves the highest accuracy with a
slight improvement in score over zero or one ref-
erence. Thus, the object weighting scheme can
be seen to help VIFIDEL focus on which objects
are more important. With 48 references (the max-
imum possible, only available in PASCAL-50S),
VIFIDEL is still more accurate than existing met-
rics, showing that visual fidelity is an important
factor for rating machine generated descriptions.

Figure 4 depicts accuracies over an increasing
number of reference descriptions, starting from
zero references (only defined for VIFIDEL). We
note that VIFIDEL is more stable and consis-

tently outperforms other metrics for all numbers
of references. VIFIDEL D500 has a very slight ad-
vantage over VIFIDEL gold, most likely because
the visual information is richer in the former.

Overall, we conclude that measuring visual fi-
delity is important especially for ranking two ma-
chine generated descriptions, arguably the most
important evaluation setting, and that VIFIDEL
can measure visual fidelity by explicitly using in-
formation derived from images, particularly when
few or no references are available.

4.3 Correlation with human judges

We measure correlation with human judgments
on the MSCOCO portion of the COMPOSITE
dataset (Aditya et al., 2018). The dataset con-
tains 2, 007 candidate images from the MSCOCO
dataset (Lin et al., 2014) with annotations from
AMT workers. These annotations consist of judg-
ments on a scale of 1 (low) to 5 (high) regard-
ing (i) relevance of a description to the image and
(ii) thoroughness of a description given an im-
age. Candidate descriptions were sampled from
human references and image description systems.
To make our settings closer to those of real system
evaluations, we consider only system generated
descriptions as candidates and evaluate the Spear-
man’s correlation between the automated metrics
and human judgments. As mentioned in §4.2, ab-
solute human judgments, especially for subjective
tasks such as this, can be very subjective and there-
fore less reliable, especially when a single judg-
ment is collected per description.

Results and discussion We summarise our re-
sults in Table 2. For these experiments, we report
gold ∪ D500 detectors for object information, as
this was the best overall performance (non-gold
and other variants can be seen in §4.5). Our key
finding is that VIFIDEL using no reference de-
scriptions obtains comparable (albeit still lower)
correlation to metrics like BLEU and ROUGE
with only one reference description. The gap be-
tween VIFIDEL and such metrics is the inability
of the former to capture fluency, since it relies es-
sentially on bag of word embeddings.

4.4 Combining VIFIDEL with other metrics

As mentioned, VIFIDEL is a fine-grained metric
that exclusively evaluates the visual fidelity of an
image description. As such, by design, an IDG
system may achieve high VIFIDEL scores sim-



6546

5 Refs 1 Ref 0 Ref

Metric Relevance Thoroughness Relevance Thoroughness Relevance Thoroughness

BLEU1 0.26 0.25 0.23 0.20 - -
BLEU4 0.26 0.24 0.22 0.20 - -
ROUGEL 0.28 0.26 0.24 0.22 - -
METEOR 0.30 0.27 0.25 0.22 - -
CIDEr 0.32 0.28 0.26 0.24 - -
SPICE 0.35 0.31 0.27 0.27 - -
WMDBest 0.30 0.26 0.28 0.26 - -
WMDWorst 0.29 0.26 0.28 0.25 - -
LM -0.10 -0.16 -0.10 -0.16 -0.10 -0.16

VIFIDEL no refs,gold∪D500 0.22 0.21 0.22 0.21 0.21 0.22
VIFIDEL gold∪D500 0.30 0.27 0.29 0.27 - -

VIFIDELno refs+LM 0.21 0.21 0.21 0.21 0.21 0.21
VIFIDEL+CIDEr 0.29 0.29 0.29 0.28 - -

Table 2: Spearman’s rank correlation coefficient between automatic metrics and human judgment scores over the
MSCOCO portion of COMPOSITE dataset.

ply by listing all objects depicted in the image.
We do not consider this an issue, since we are in
effect evaluating what the image description de-
scribes (visual fidelity), rather than how the im-
age is being described (fluency). The latter can
instead be evaluated separately with a metric de-
signed specifically for the purpose. Thus, a sys-
tem that simply lists all objects will result in a high
VIFIDEL score but a low fluency score. This is in
line with our vision of evaluating the specific capa-
bilities of IDG systems to clearly understand how
one system is better than another. While this is
not the aim of the paper, we also demonstrate how
VIFIDEL can also be combined with fluency-
based metrics to evaluate image descriptions as
a single conflated metric. More specifically, we
explore combining VIFIDEL with two different
fluency-based strategies: an RNN language model
and CIDEr (other metrics are possible, CIDEr pro-
vides a good compromise between performance
and efficiency). In both cases, we simply aver-
aged the scores of the two metrics. The results
are shown at the bottom of Tables 1 and 2. On av-
erage, the addition of the fluency-based metric is
complementary. VIFIDEL +CIDEr is better per-
forming than VIFIDEL +LM]. This is expected
as LM only provides perplexity scores given a de-
scription, while CIDEr explicitly measures qual-
ity against references. We note that better com-
binations can potentially be achieved by learning
weights for a weighted average and optimising the
training towards fluency.

4.5 Ablation studies

Effect of object detectors and frequency counts:
Here we study the effect of using different ob-

Without Frequency With Frequency

Object Detectors Relevance Thoroughness Relevance Thoroughness

gold 0.25 0.24 0.25 0.23
D80 0.24 0.23 0.24 0.22
D500 0.25 0.24 0.25 0.23
gold ∪ D500 0.29 0.27 0.28 0.26
D80 ∪ D500 0.28 0.26 0.27 0.26

Table 3: Ablation experiments on VIFIDEL on the
MSCOCO part of the COMPOSITE dataset with dif-
ferent object detectors and with vs. without frequency
counts for objects.

ject detectors. We also investigate the contribution
of frequency counts in dI , by binarising the fre-
quency counts to indicate only the presence and
absence of the objects. The hypothesis is that the
number of object instances may be useful for eval-
uating visual fidelity. The results are summarised
in Table 3. The combination of gold and D500
object detectors performed the best for the dataset.
The gold object information is only slightly bet-
ter than D80 prediction-based object information.
Interestingly, D500, which is more fine-grained,
performs as well as the 80-category gold object
information. Using a binarised dI seemed to
give comparable correlation, perhaps even with a
marginal edge over its frequency-based counter-
part. We postulate that this could be because fre-
quency counts are likely to be mentioned in the de-
scriptions via quantifiers and other morphological
and typological variants, which cannot be easily
mapped to the frequency of detected objects.

Effect of number of detected objects: We used
a pre-trained captioning system (Anderson et al.,
2018) to generate captions on a sample of images
from the MSCOCO validation set that have differ-
ent gold object annotations. We present two ex-



6547

VIFIDEL Objects Caption References

0.79 truck a small truck sitting on
top of a field

1. A orange tractor sitting on top of a
lush green field.
2. A snowplow truck with two
snowplows on it
3. Farm equipment truck in a field near
a road.
4. an image of a truck with the
scaffolding
5. The yellow earth mover sits in the
field in front of the pole.

0.70 person, car,
backpack,
umbrella,
handbag,
bottle,
wine-glass,
cup fork,
knife, spoon,
bowl,
broccoli,
chair,
dining-table

a table full of people at
the restaurant

1. A group of people sitting around a
wooden table with food.
2. The nine people smile as they sit at
a dinner table.
3. A table full of people that are eating
at a restaurant.
4. Co-workers often get together after
a long day at work.
5. A group of people that are sitting
around a table.

Table 4: Ablation study on the number of detected ob-
jects: one detected object and fifteen detected objects.

person 0.75

person, dining-table 0.83

person, dining-table,
umbrella, handbag, bottle

0.74

person, car, backpack,
umbrella, handbag, bottle,
dining-table, cup, fork, knife

0.73

person, car, backpack,
umbrella, handbag, bottle,
wine-glass, cup, fork, knife,
spoon, bowl, broccoli, chair,
dining-table

0.70

Figure 5: VIFIDEL score changes with different num-
bers of object annotations.

amples in Table 4. In the first example the image
contains only one object. Here VIFIDEL relies
both on the object and on the semantic similarity
between the caption and references. In the second
example there are fifteen objects, however some
are more important than others for describing the
images. VIFIDEL gives higher importance to ob-
jects that are mentioned in the references. In Fig-
ure 5 we further explore the third example from
our ablation studies by computing VIFIDEL for
different subsets of object annotations, ranging
from one object to fifteen object annotations. We
see that, as the number of objects increase from
one to two, the VIFIDEL score also increases,
given that these objects are also mentioned in the
system caption. However, with more objects the
scores go down, since these are not mentioned in
the caption, but the decrease is gradual, even with
15 objects. This happens because such additional
objects do not seem too relevant to humans, as
these are mostly not mentioned in the references.
Therefore, the scores from VIFIDEL change with
both number of annotations and the reference de-
scriptions, while metrics like METEOR (0.354)
and SPICE (0.320) would remain constant.

Effect of word representations: We also stud-
ied the effect of various pre-trained embeddings
and found that the pre-trained model of word2vec
300-dimensional CBOW embeddings (Mikolov
et al., 2013) is slightly better than GLoVe
(Pennington et al., 2014) and FastText embed-
dings (Joulin et al., 2017). This could be because
of the amount of data on which these were trained.
FastText embeddings had similar performance as
word2vec embeddings even when only trained on
the Wikipedia as corpus. For consistency, we used
the word2vec embeddings pre-trained on Google
News.

5 Conclusions

We have introduced a new metric for image de-
scription evaluation that goes beyond comparing
descriptions to human references and is explicitly
based on object-level image information. Our hy-
pothesis is that the use of image information pro-
vides a more reliable pathway for measuring the
fidelity of a description for a given image. Fur-
ther, the metric relies on off-the shelf object de-
tectors and word-embeddings and computes the
scores in a semantic space. Our analysis on
two of the most widely used datasets for metric
comparison shows that our metric correlates well
with human judgments, and is particularly well
suited when few or no reference description is
available. The metric performs comparatively for
gold and predicted annotations on objects and is
lightweight in terms of dependency on linguistic
resources. Our implementation of VIFIDEL can
be accessed from: https://github.com/
ImperialNLP/vifidel

Acknowledgements

The authors thank the anonymous reviewers and
area chairs for giving up their time to provide
useful feedback on an earlier draft of this paper.
This work was supported by the MultiMT project
(H2020 ERC Starting Grant No. 678017). This
work was also supported by the MMVC project,
via an Institutional Links grant, ID 352343575,
under the Newton-Katip Celebi Fund partnership.
The grant is funded by the UK Department of
Business, Energy and Industrial Strategy (BEIS)
and Scientific and Technological Research Coun-
cil of Turkey (TÜBİTAK) and delivered by the
British Council. For further information, please
visit http://www.newtonfund.ac.uk.

https://github.com/ImperialNLP/vifidel
https://github.com/ImperialNLP/vifidel
http://www.newtonfund.ac.uk


6548

References
Somak Aditya, Yezhou Yang, Chitta Baral, Yiannis

Aloimonos, and Cornelia Fermüller. 2018. Image
understanding using vision and reasoning through
scene description graph. Computer Vision and Im-
age Understanding, 173:33–45.

Peter Anderson, Basura Fernando, Mark Johnson, and
Stephen Gould. 2016. SPICE: Semantic proposi-
tional image caption evaluation. In Proceedings
of the European Conference on Computer Vision
(ECCV), volume 9909 of Lecture Notes in Computer
Science, pages 382–398, Amsterdam, The Nether-
lands. Springer International Publishing.

Peter Anderson, Xiaodong He, Chris Buehler, Damien
Teney, Mark Johnson, Stephen Gould, and Lei
Zhang. 2018. Bottom-up and top-down attention for
image captioning and visual question answering. In
Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition (CVPR), pages
6077–6086, Salt Lake City, UT, USA. IEEE.

Alexander C. Berg, Tamara L. Berg, Hal Daumé III,
Jesse Dodge, Amit Goyal, Xufeng Han, Alyssa
Mensch, Margaret Mitchell, Aneesh Sood, Karl
Stratos, and Kota Yamaguchi. 2012. Understanding
and predicting importance in images. In Proceed-
ings of the IEEE Conference on Computer Vision
& Pattern Recognition (CVPR), pages 3562–3569,
Providence, RI, USA. IEEE.

Ondřej Bojar, Rajen Chatterjee, Christian Federmann,
Yvette Graham, Barry Haddow, Matthias Huck,
Antonio Jimeno Yepes, Philipp Koehn, Varvara
Logacheva, Christof Monz, Matteo Negri, Aure-
lie Neveol, Mariana Neves, Martin Popel, Matt
Post, Raphael Rubino, Carolina Scarton, Lucia Spe-
cia, Marco Turchi, Karin Verspoor, and Marcos
Zampieri. 2016. Findings of the 2016 conference
on machine translation. In Proceedings of the First
Conference on Machine Translation, pages 131–
198, Berlin, Germany. Association for Computa-
tional Linguistics.

Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakr-
ishna Vedantam, Saurabh Gupta, Piotr Dollr, and
C. Lawrence Zitnick. 2015. Microsoft COCO cap-
tions: data collection and evaluation server. CoRR,
abs/1504.00325.

Ritendra Datta, Dhiraj Joshi, Jia Li, and James Z.
Wang. 2008. Image retrieval: Ideas, influences, and
trends of the new age. ACM Computing Surveys,
40(2):5:1–5:60.

Michael Denkowski and Alon Lavie. 2014. Meteor
universal: Language specific translation evaluation
for any target language. In Proceedings of the Ninth
Workshop on Statistical Machine Translation, pages
376–380, Baltimore, Maryland, USA. Association
for Computational Linguistics.

Lily D. Ellebracht, Arnau Ramisa, Pranava Swa-
roop Madhyastha, Jose Cordero-Rama, Francesc

Moreno-Noguer, and Ariadna Quattoni. 2015. Se-
mantic tuples for evaluation of image to sentence
generation. In Proceedings of the Fourth Workshop
on Vision and Language, pages 18–28, Lisbon, Por-
tugal. Association for Computational Linguistics.

M. Everingham, S. M. A. Eslami, L. Van Gool, C. K. I.
Williams, J. Winn, and A. Zisserman. 2015. The
PASCAL Visual Object Classes challenge: A retro-
spective. International Journal of Computer Vision,
111(1):98–136.

Marina Fomicheva and Lucia Specia. 2016. Reference
bias in monolingual machine translation evaluation.
In Proceedings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 2:
Short Papers), pages 77–82, Berlin, Germany. Asso-
ciation for Computational Linguistics.

Jonathan Huang, Vivek Rathod, Chen Sun, Menglong
Zhu, Anoop Korattikara, Alireza Fathi, Ian Fischer,
Zbigniew Wojna, Yang Song, Sergio Guadarrama,
and Kevin Murphy. 2017. Speed/accuracy trade-
offs for modern convolutional object detectors. In
Proceedings of the IEEE Conference on Computer
Vision & Pattern Recognition (CVPR), pages 3296–
3297, Honolulu, HI, USA. IEEE.

Armand Joulin, Edouard Grave, Piotr Bojanowski, and
Tomas Mikolov. 2017. Bag of tricks for efficient
text classification. In Proceedings of the 15th Con-
ference of the European Chapter of the Association
for Computational Linguistics: Volume 2, Short Pa-
pers, pages 427–431, Valencia, Spain. Association
for Computational Linguistics.

Mert Kilickaya, Aykut Erdem, Nazli Ikizler-Cinbis,
and Erkut Erdem. 2017. Re-evaluating automatic
metrics for image captioning. In Proceedings of the
15th Conference of the European Chapter of the As-
sociation for Computational Linguistics: Volume 1,
Long Papers, pages 199–209, Valencia, Spain. As-
sociation for Computational Linguistics.

Ivan Krasin, Tom Duerig, Neil Alldrin, Vittorio Fer-
rari, Sami Abu-El-Haija, Alina Kuznetsova, Has-
san Rom, Jasper Uijlings, Stefan Popov, Shahab
Kamali, Matteo Malloci, Jordi Pont-Tuset, An-
dreas Veit, Serge Belongie, Victor Gomes, Ab-
hinav Gupta, Chen Sun, Gal Chechik, David
Cai, Zheyun Feng, Dhyanesh Narayanan, and
Kevin Murphy. 2017. OpenImages: A pub-
lic dataset for large-scale multi-label and multi-
class image classification. Dataset available
from https://storage.googleapis.com/
openimages/web/index.html.

Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Siming
Li, Yejin Choi, Alexander C. Berg, and Tamara L.
Berg. 2011. Baby talk: Understanding and gener-
ating simple image descriptions. In Proceedings of
the IEEE Conference on Computer Vision & Pattern
Recognition (CVPR), pages 1601–1608, Colorado
Springs, CO, USA. IEEE.

https://doi.org/10.1016/j.cviu.2017.12.004
https://doi.org/10.1016/j.cviu.2017.12.004
https://doi.org/10.1016/j.cviu.2017.12.004
https://doi.org/10.1007/978-3-319-46454-1_24
https://doi.org/10.1007/978-3-319-46454-1_24
https://doi.org/10.1109/CVPR.2018.00636
https://doi.org/10.1109/CVPR.2018.00636
https://doi.org/10.1109/CVPR.2012.6248100
https://doi.org/10.1109/CVPR.2012.6248100
https://doi.org/10.18653/v1/W16-2301
https://doi.org/10.18653/v1/W16-2301
http://arxiv.org/abs/1504.00325
http://arxiv.org/abs/1504.00325
https://doi.org/10.1145/1348246.1348248
https://doi.org/10.1145/1348246.1348248
https://doi.org/10.3115/v1/W14-3348
https://doi.org/10.3115/v1/W14-3348
https://doi.org/10.3115/v1/W14-3348
https://doi.org/10.18653/v1/W15-2806
https://doi.org/10.18653/v1/W15-2806
https://doi.org/10.18653/v1/W15-2806
https://doi.org/10.1007/s11263-014-0733-5
https://doi.org/10.1007/s11263-014-0733-5
https://doi.org/10.1007/s11263-014-0733-5
https://doi.org/10.18653/v1/P16-2013
https://doi.org/10.18653/v1/P16-2013
https://doi.org/10.1109/CVPR.2017.351
https://doi.org/10.1109/CVPR.2017.351
https://www.aclweb.org/anthology/E17-2068
https://www.aclweb.org/anthology/E17-2068
http://www.aclweb.org/anthology/E17-1019
http://www.aclweb.org/anthology/E17-1019
https://storage.googleapis.com/openimages/web/index.html
https://storage.googleapis.com/openimages/web/index.html
https://storage.googleapis.com/openimages/web/index.html
https://storage.googleapis.com/openimages/web/index.html
https://storage.googleapis.com/openimages/web/index.html
https://doi.org/10.1109/CVPR.2011.5995466
https://doi.org/10.1109/CVPR.2011.5995466


6549

Matt Kusner, Yu Sun, Nicholas Kolkin, and Kilian
Weinberger. 2015. From word embeddings to doc-
ument distances. In Proceedings of the 32nd In-
ternational Conference on Machine Learning, vol-
ume 37 of Proceedings of Machine Learning Re-
search, pages 957–966, Lille, France. PMLR.

Siming Li, Girish Kulkarni, Tamara L. Berg, Alexan-
der C. Berg, and Yejin Choi. 2011. Composing sim-
ple image descriptions using web-scale n-grams. In
Proceedings of the Fifteenth Conference on Com-
putational Natural Language Learning, pages 220–
228, Portland, OR, USA. Association for Computa-
tional Linguistics.

Chin-Yew Lin. 2004. ROUGE: A package for auto-
matic evaluation of summaries. In ACL Workshop
on Text Summarization Branches Out. Association
for Computational Linguistics.

Chin-Yew Lin and Franz Josef Och. 2004. Auto-
matic evaluation of machine translation quality us-
ing longest common subsequence and skip-bigram
statistics. In Proceedings of the 42nd Annual Meet-
ing of the Association for Computational Linguistics
(ACL-04), pages 605–612, Barcelona, Spain. Asso-
ciation for Computational Linguistics.

Tsung-Yi Lin, Michael Maire, Serge Belongie, James
Hays, Pietro Perona, Deva Ramanan, Piotr Dollár,
and C. Lawrence Zitnick. 2014. Microsoft COCO:
Common objects in context. In Proceedings of the
European Conference on Computer Vision (ECCV),
pages 740–755, Zurich, Switzerland. Springer Inter-
national Publishing.

Siqi Liu, Zhenhai Zhu, Ning Ye, Sergio Guadarrama,
and Kevin Murphy. 2017. Improved image cap-
tioning via policy gradient optimization of SPIDEr.
In Proceedings of the IEEE International Confer-
ence on Computer Vision (ICCV), pages 873–881,
Venice, Italy. IEEE.

Pranava Madhyastha, Josiah Wang, and Lucia Spe-
cia. 2018. End-to-end image captioning exploits
multimodal distributional similarity. In Proceed-
ings of the 29th British Machine Vision Conference
(BMVC), page 306, Newcastle, UK. BMVA Press.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word repre-
sentations in vector space. In Proceedings of the
1st International Conference on Learning Represen-
tations, ICLR 2013, Workshop Track Proceedings,
Scottsdale, AZ, USA.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311–318, Philadelphia,
PA, USA. Association for Computational Linguis-
tics.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. GloVe: Global vectors for word
representation. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 1532–1543, Doha,
Qatar. Association for Computational Linguistics.

Cyrus Rashtchian, Peter Young, Micah Hodosh, and
Julia Hockenmaier. 2010. Collecting image annota-
tions using Amazon’s Mechanical Turk. In Proceed-
ings of the NAACL HLT 2010 Workshop on Creating
Speech and Language Data with Amazon’s Mechan-
ical Turk, pages 139–147, Los Angeles, CA, USA.
Association for Computational Linguistics.

Yossi Rubner, Carlo Tomasi, and Leonidas J. Guibas.
2000. The earth mover’s distance as a metric for
image retrieval. International Journal of Computer
Vision, 40(2):99–121.

Ramakrishna Vedantam, C. Lawrence Zitnick, and
Devi Parikh. 2015. CIDEr: Consensus-based image
description evaluation. In Proceedings of the IEEE
Conference on Computer Vision & Pattern Recogni-
tion (CVPR), pages 4566–4575, Boston, MA, USA.
IEEE.

Oriol Vinyals, Alexander Toshev, Samy Bengio, and
Dumitru Erhan. 2015. Show and tell: A neural im-
age caption generator. In Proceedings of the IEEE
Conference on Computer Vision & Pattern Recogni-
tion (CVPR), pages 3156–3164, Boston, MA, USA.
IEEE.

Josiah Wang and Robert Gaizauskas. 2015. Generat-
ing image descriptions with gold standard visual in-
puts: Motivation, evaluation and baselines. In Pro-
ceedings of the 15th European Workshop on Natu-
ral Language Generation (ENLG), pages 117–126,
Brighton, UK. Association for Computational Lin-
guistics.

Josiah Wang, Pranava Madhyastha, and Lucia Specia.
2018. Object counts! Bringing explicit detections
back into image captioning. In Proceedings of the
North American Chapter of the Association of Com-
putational Linguistics: Human Language Technolo-
gies (NAACL HLT), , Volume 1 (Long Papers), pages
2180–2193, New Orleans, LA, USA. Association
for Computational Linguistics.

Xuwang Yin and Vicente Ordonez. 2017. Obj2Text:
Generating visually descriptive language from ob-
ject layouts. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 177–187, Copenhagen, Denmark.
Association for Computational Linguistics.

A Accuracy on PASCAL-50S (all groups)

The original PASCAL-50S dataset consisted of
four groups: (i) HC (Human-Human correct:
both correct human descriptions); (ii) HI (Human-
Human incorrect: one correct human description

http://proceedings.mlr.press/v37/kusnerb15.html
http://proceedings.mlr.press/v37/kusnerb15.html
http://www.aclweb.org/anthology/W11-0326
http://www.aclweb.org/anthology/W11-0326
http://www.aclweb.org/anthology/W04-1013
http://www.aclweb.org/anthology/W04-1013
https://doi.org/10.3115/1218955.1219032
https://doi.org/10.3115/1218955.1219032
https://doi.org/10.3115/1218955.1219032
https://doi.org/10.3115/1218955.1219032
https://doi.org/10.1007/978-3-319-10602-1_48
https://doi.org/10.1007/978-3-319-10602-1_48
https://doi.org/10.1109/ICCV.2017.100
https://doi.org/10.1109/ICCV.2017.100
http://www.bmva.org/bmvc/2018/contents/papers/0925.pdf
http://www.bmva.org/bmvc/2018/contents/papers/0925.pdf
http://arxiv.org/abs/1301.3781
http://arxiv.org/abs/1301.3781
https://doi.org/10.3115/1073083.1073135
https://doi.org/10.3115/1073083.1073135
https://doi.org/10.3115/v1/D14-1162
https://doi.org/10.3115/v1/D14-1162
http://www.aclweb.org/anthology/W10-0721
http://www.aclweb.org/anthology/W10-0721
https://doi.org/10.1023/A:1026543900054
https://doi.org/10.1023/A:1026543900054
https://doi.org/10.1109/CVPR.2015.7299087
https://doi.org/10.1109/CVPR.2015.7299087
https://doi.org/10.1109/CVPR.2015.7298935
https://doi.org/10.1109/CVPR.2015.7298935
https://doi.org/10.18653/v1/W15-4722
https://doi.org/10.18653/v1/W15-4722
https://doi.org/10.18653/v1/W15-4722
https://doi.org/10.18653/v1/N18-1198
https://doi.org/10.18653/v1/N18-1198
https://doi.org/10.18653/v1/D17-1017
https://doi.org/10.18653/v1/D17-1017
https://doi.org/10.18653/v1/D17-1017


6550

and one human description from another random
image); (iii) HM (Human-Machine: one correct
human description and one machine generated
description); (iv) MM (Machine-Machine: both
machine generated descriptions). We summarise
the accuracy of VIFIDEL and various evaluation
metrics on the PASCAL-50S binary forced-choice
task in Tables 5 and 6.

Metric
No references 1 reference

HC HI HM MM all HC HI HM MM all

BLEU1 - - - - - 0.61 0.86 0.88 0.58 0.73
BLEU4 - - - - - 0.59 0.83 0.80 0.56 0.70
ROUGEL - - - - - 0.61 0.87 0.86 0.58 0.73
METEOR - - - - - 0.61 0.91 0.89 0.62 0.76
CIDEr - - - - - 0.62 0.91 0.85 0.60 0.75
SPICE - - - - - 0.58 0.87 0.77 0.65 0.72

VIFIDEL gold 0.57 0.91 0.53 0.68 0.67 0.62 0.96 0.65 0.69 0.73
VIFIDEL D500 0.57 0.93 0.64 0.69 0.71 0.59 0.95 0.74 0.71 0.75

Table 5: Accuracy of VIFIDEL and other IDG metrics
on the PASCAL-50S binary forced-choice task using
no references and one reference.

Metric
5 references 48 references

HC HI HM MM all HC HI HM MM all

BLEU1 0.64 0.95 0.91 0.61 0.78 0.63 0.98 0.94 0.62 0.79
BLEU4 0.62 0.93 0.85 0.59 0.75 0.64 0.97 0.92 0.60 0.78
ROUGEL 0.66 0.95 0.93 0.61 0.78 0.67 0.98 0.95 0.64 0.81
METEOR 0.66 0.98 0.94 0.66 0.81 0.65 0.99 0.96 0.68 0.82
CIDEr 0.67 0.98 0.90 0.66 0.80 0.69 0.99 0.92 0.68 0.82
SPICE 0.67 0.97 0.89 0.69 0.80 0.62 0.99 0.94 0.69 0.81

VIFIDEL gold 0.65 0.98 0.66 0.70 0.75 0.64 0.98 0.67 0.71 0.75
VIFIDEL D500 0.64 0.97 0.75 0.72 0.77 0.63 0.97 0.76 0.71 0.77

Table 6: Accuracy of VIFIDEL and other IDG metrics
on the PASCAL-50S binary forced-choice task using 5
and 48 references.


