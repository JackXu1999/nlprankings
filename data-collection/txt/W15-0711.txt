



















































Rule-based Coreference Resolution in German Historic Novels


Proceedings of NAACL-HLT Fourth Workshop on Computational Linguistics for Literature, pages 98–104,
Denver, Colorado, June 4, 2015. c©2015 Association for Computational Linguistics

Rule-based Coreference Resolution in German Historic Novels 

 

 

Markus Krug, Frank Puppe Fotis Jannidis, Luisa Macharowsky,  
Isabella Reger, Lukas Weimer 

Wuerzburg university,  

Institute for Computer Science 

Wuerzburg university,  

Institute for German Studies 

Am Hubland Am Hubland 

D-97074 Würzburg, Germany D-97074 Würzburg, Germany 

markus.krug|frank.puppe@uni-wuerzburg.de fotis.jannidis@uni-wuerzburg.de  
 

Abstract 

Coreference resolution (CR) is a key task in 

the automated analysis of characters in stories. 

Standard CR systems usually trained on 

newspaper texts have difficulties with literary 

texts, even with novels; a comparison with 

newspaper texts showed that average sentence 

length is greater in novels and the number of 

pronouns, as well as the percentage of direct 

speech is higher. We report promising evalua-

tion results for a rule-based system similar to 

[Lee et al. 2011], but tailored to the domain 

which recognizes coreference chains in novels 

much better than CR systems like CorZu. 

Rule-based systems performed best on the 

CoNLL 2011 challenge [Pradhan et al. 2011]. 

Recent work in machine learning showed sim-

ilar results as rule-based systems [Durett et al. 

2013]. The latter has the advantage that its ex-

planation component facilitates a fine grained 

error analysis for incremental refinement of 

the rules.  

1 Introduction 

The overall goal of our research is the identifica-

tion of characters in German novels from the 19
th
 

century and an analysis of their attributes. The 

main steps are named entity recognition (NER) of 

the persons, coreference resolution (CR), attribu-

tion of persons and character description with fo-

cus on sentiment analysis. While NER in novels is 

discussed in [Jannidis et al. 2015], we report on 

work in progress on rule-based coreference resolu-

tion in novels. Tests with existing rule-based or 

machine learning NLP tools on our novels had 

unsatisfying results. In contrast to newspaper texts 

novels not only exhibit different topics and word-

ing, but also show a heavy use of pronouns  (in our 

corpus 70% of all NEs) and relative few large clus-

ters with long coreference chains opposed to many 

small clusters (see baseline analysis in table 1). 

Another important difference is the number and 

lengths of passages containing direct speech [Iosif, 

Mishra 2014]. 

  

 

We decided on a rule-based approach because: 

 A key aspect in coreference resolution is 
feature and constraint detection. Features 

and constraints for ruling in or out candi-

dates for coreference with a high precision 

can be combined to achieve a high recall. 

If such features and constraints are repre-

sented by rules, the explanation component 

of rule-based systems is very valuable in 

understanding the errors and thus enabling 

rapid rule refinement. 

 We do not have a large corpus with anno-
tated German novels to learn from. As 

mentioned above, there are substantial dif-

ferences between e.g. newspapers and 

novels, so that machine learning approach-

es with domain adaptation (e.g. [Yang et 

al. 2012]) are difficult.  

 We intend to use rule-based CR to semi-
automatically create a large corpus of an-

notated novels for experimenting with ma-

chine learning CR approaches.  

 

We present a state-of-the-art rule-based system 

tailored for CR in novels. In comparison to CorZu 

(see section 4) which recognizes CR well in news-

papers, we achieve better results in novels (MUC 

98



F1: 85.5% vs. 65.9%, B
3
 F1: 56.0% vs. 33.6%). 

Our explanation component facilitates a fine 

grained error analysis for incremental rule refine-

ment. 
 

2 Related Work 

Coreference Resolution itself is an old, but un-

solved task on which a huge amount of effort was 

spent during the last 40 years. Large conferences 

like ConLL 2011 [Pradhan et al. 2011], CoNLL 

2012 [Pradhan et al. 2012] and SemEval 2010 

[Recasens et al. 2010]) have offered challenges for 

the topic not only with English text (ConLL 2011), 

but also for Chinese and Arabic (CoNLL 2012) 

and German, Dutch, Italian, Catalan and Spanish 

(SemEval 2010). Most approaches are based on 

machine learning algorithms. A large part of ma-

chine learning approaches use two phases: first 

classification of pairs of NEs (nouns, persons, etc.) 

followed by a clustering or ranking of the results 

(so called mention-pair model [Aone 1995, Soon et 

al. 2001]). Since the mention-pair model suffers 

from serious problems [Ng 2010], newer ap-

proaches try to match named entities directly to 

clusters of mentions (entity-mention model). A 

multitude of approaches was developed under the 

focus to model the affiliation of a mention to a 

specific entity. One goal of such an approach is to 

avoid problems of the mention-pair approach like 

(A = B, B = C, A ≠ C, e.g. A = "Mr. Clinton", B = 

"Clinton", C = "she"). Aside of mention-ranking 

approaches [Denis, Baldridge 2008], [Rahman 

2009] the system developed by Durett, Hall and 

Klein [Durett et al. 2013] shows that task-specific 

graphical models perform well following the enti-

ty-mention approach. Since rules can model hard 

and soft constraints directly, e.g. for pronoun reso-

lution, rule-based approaches like the multi pass 

sieve for CR [Lee et al. 2011] deliver promising 

results, e.g. the best result in the challenge of 

CoNLL 2011 for English documents. Although the 

problem of CR has been a topic for forty years 

[Hobbs 1976], even good results are between 60% 

and 70%: [Durett et al. 2013]  report a MUC-Score 

of 63.7% and a B³ Score of 67.8% for the CoNLL 

blind test set, with the system of Stanford perform-

ing comparably with 61.5% and 69.2% respective-

ly – much worse than e.g. for NER. In [Lee et al. 

2011] the Stanford system got better results with 

78.6% and 80.5% showing the great influence of 

the data for the final results. In section 4 we there-

fore perform two separate experiments on two 

different datasets to manifest the reliability of our 

approach for the domain of literary novels. 
 

3 Methods and data 

Coreference resolution is based on a NLP-pipeline 

including tokenization, sentence splitting, part of 

speech tagging, lemmatization, named entity 

recognition and dependency parsing. In our pipe-

line we use the TreeTagger of Stuttgart university 

[Schmitt 1995] for tokenization and POS-tagging, 

OpenNLP
1
 with the according German model for 

sentence splitting and the MATE-Toolkit [Bohnet 

2010] for dependency parsing. Due to our overall 

goal, the identification and attribution of characters 

in German novels, we restrict coreference resolu-

tion to the resolution of persons, excluding e.g. 

geographic locations. 

The data used for this development consists of 

roughly 80 segments, each sampled from a differ-

ent novel. The sampling process determined a ran-

dom (syntactic) sentence in the text and used all 

following 129 sentences, therefore forming a con-

nected chunk of 130 sentences. This sampling pro-

cess ignored the beginning of a chapter, which 

bears an even greater challenge for the human an-

notators and the algorithms, because now some 

segments can even start with uninformative men-

tions such as pronouns. With the long-term goal to 

get a detailed attribution of entities in the novels 

we developed our own annotation tool based on 

eclipse-rcp
2
. Since former studies showed that the 

coreference task does not exhibit many ambiguities 

for humans our data is only annotated by one anno-

tator. 

 Our corpus used in the first evaluation comprises 

48 different novels. Thus, the first test corpus  

contains 143 000  tokens with ca. 19 000 refer-

ences including proper names, personal pronouns 

etc., while for our second experiment we used 30 

additional fragments with about 11 600 NEs and 

104 000 tokens. In comparison to the German 

TIGER corpus [Brants et al. 2004] which consists 

of newspaper articles, we have on average longer 

sentences with 24.2 tokens compared to 16.3 to-

                                                           
1 https://opennlp.apache.org/ 
2 https://wiki.eclipse.org/index.php/Rich_Client_Platform 

99



kens. On average, one sentence contains three ref-

erences to the same character or other characters. 

Each character appeared 10 times on average with-

in the small novel fragments of 130 sentences, 

compared to ca. 4 times in the ACE2004-nwire 

corpus used in [Lee et al. 2011]. The majority of 

references are pronouns (~ 70%). In German, pro-

noun resolution is more ambiguous than in Eng-

lish, e.g. the German "sie" has three possible 

meanings: "she", "they", and “you” in direct 

speech. Only for unambiguous pronouns like "er" 

[he] and "ihm" [him] we can use static features like 

in [Lee et al. 2011]. For pronoun resolution, our 

rules use features of NEs like gender (male, fe-

male, neuter, unknown), number (singular, plural, 

unknown), person (for pronouns: first, second, 

third person) and whether the NE is the subject of 

the sentence. In general, a substantial part of a 

novel is direct speech, so we segment the novels in 

narrative and direct speech parts in a preprocessing 

step.  In order to detect the speaker of a given di-

rect speech annotation we use the following rules: 

 Explicit speaker detection. 

 Speaker propagation for longer dialogues. 

 Pseudo speaker propagation for situations 
where in longer dialogues two persons 

talking in turn can be recognized, but 

speaker detection failed. 

 

In order to be able to determine features like gen-

der from non-pronoun references we use various 

resources like lists of male and female first names 

from CorZu [Klenner 2011], the morphological 

analysis tool of Stuttgart University SMOR 

[Fitschen et al. 2004], the morphological tagger 

from the above mentioned Mate-toolkit [Bohnet 

2010] and a self-trained classifier, a maximum 

entropy model trained on the TIGER corpus. After 

applying these tools in a precision based manner 

(lists, own system, mate, SMOR), where the sub-

sequent system is only used if the previous system 

detects “unknown”, we apply a set of correction 

rules in order to guarantee consistency among the 

NEs. The heuristic rules try to infer the gender of a 

NE by using context clues, e.g. a subsequent refer-

ence within the same “subsentence” (that is a sen-

tence up to the next comma) of "his" or "her" and 

the propagation of a recognized gender of a NE 

along a local chain of unambiguous NEs (e.g. for 

old fashioned first names like "Günderode"). Other 

rules exist for determination of the number of an 

NE and dependency parsing is used for determin-

ing the subject of a sentence. An evaluation of the 

number attribute which we had annotated aside of 

the coreferences and NEs resulted in an accuracy 

of approximately 93% in the used test data. We 

split our documents into a small training set with 

just 5 documents, a first test set of 48 documents 

that we used to compare our performance with the 

system CorZu, and into another test set consisting 

of 30 completely unseen documents where we 

evaluated the robustness of our system. 

 

Our system has a similar rule organization as [Lee 

et al. 2011] with passes, i.e. rule sets, which build 

on the results of former passes. While [Lee et al. 

2011] uses 7 passes, we extend this by using 11 

passes: 
 

1. Pass: exact match: All identical non-pronouns 

are marked as coreferent. They are also consid-

ered as coreferent if their cases differ (“An-

nette” vs “ANNETTE”). 

2. Pass: Nameflexion: We designed a distance 

metric that allows us to detect derivations (or 

nicknames) from names and mark them as co-

referent. (“Lydia”, “Lyden”,”Lydchen”). 

3. Pass: Attributes: We use all modifiers (derived 

from the output of the parser) and match them 

against the strings of the NEs of the other clus-

ter. Coreference occurs if there is an “equals-

ignore-case”-match. (“Die alte Getrud” ,…”die 

Alte”) [“the elderly Gertrud”, “the elderly” ] 

4. Pass: precise constructs: Appositions, relative 

and reflexive pronouns are assigned to the pre-

ceding NE. In addition, these pronouns get the 

gender and number of the NE in order to sup-

port subsequent resolution of other pronouns. 

5-7. Pass: 5. strict head match, 6. relaxed head 

match and 7. title match: These 3 passes rec-

ognize coreferent  NEs, where an NE consists 

of several words. The first rule, named strict 

head match, removes all titles from the given 

mentions and then compares the remaining 

words. Two NEs are said to be coreferent if 

there is at least one word that appears in both 

mentions and they agree in number and gender 

(“Baron Landsfeld” , “Herr Landsfeld”) [“Bar-

on Landsfeld”, “Mister Landsfeld”]. The re-

laxed head match only requires that one word of 

one NE is contained in a word of the other NE. 

Since titles were removed in the previous two 

100



rules we added another rule specifically for ti-

tles and match those to the most recent NE 

which contains the given title.  

8. Semantic pass: For this semantic pass we use 

the synonyms in the German resource Ger-

maNet
3
, again, an agreement in gender is re-

quired. This matches for example “Gatte” and 

“Gemahl” [“spouse”, “consort”]. 

9. Pass: pronoun resolution: pronouns are re-

solved to the most recent, suitable precedent 

NE. To respect salience we sorted our previous 

NEs in a manner that preferred the last subject 

of earlier sentences over the closest NEs in 

those sentences. A suitable precedent is a one 

that doesn’t conflict with a given constraint. In 

the current implementation we respect the fol-

lowing constraints: 

 Compliance in gender/number and person. 

 Compliance in its context (are they both 
part of a direct speech or both part of a 

narrative segment). 

 The candidate and the actual pronoun do 
not harm a given constraint of binding 

theory. 

 

. 10. Pass:  Detection of the addressed person in 

direct speech: For each direct speech annota-

tion we try to find the addressed NE. We do this 

by using several handcrafted lexico-syntactic 

patterns (matching against expressions such as 

“Alexander, you are great”). Based on the re-

sults of speaker detection we use a propagation 

of the addressed persons in dialogues. 

  

11. Pass: pronouns in direct speech: We then 

resolve all instances of <I> to the speaker and 

all instances of <you> to the person the speaker 

talks to (if known). If the speaker of two subse-

quent direct speech annotations doesn’t change, 

but the addressed person differs, we assume 

that the speaker only uses a different naming for 

the person he is talking to and therefore set 

these NEs as coreferent. 

 

Fig. 1 shows the explanation, annotation and error 

analysis component of our rule-based tool. 

                                                           
3 http://www.sfs.uni-tuebingen.de/GermaNet/ 

 
 
Fig. 1: Explanation and annotation editor for rule-based coref-

erence resolution. All named entities (NE) are already marked. 

Identical numbers above the NE denote that they are corefer-

ent to the same entity. For explanation, the user clicks on one 

NE (here: "Du" [you]) and the system shows in a pop up menu 

the last five NE tokens with some key attributes: Geschlecht 

([gender]: male, female, neuter, unknown), number (singular,  

plural, unknown), Person (for pronouns: first, second third 

persons and for nouns the type of NE like real name, appella-

tive name or pseudo-person) and whether the NE is the subject 

of the sentence. The annotator can classify errors like the 

missing coreference of "DU" to "Geliebter" [lover] with cate-

gories (here: "angesprochener falsch" [wrong reference]) with 

a drop down menu in the right frame. If there is direct speech, 

it is highlighted with a background color and the speaker (here 

"sie" [she]) is marked with a red circle. 

4 Evaluation and error analysis 

We evaluated the coreference resolution algorithm 

in two experiments. The first one uses the test cor-

pus of 48 novel fragments with about 19.000 man-

ually annotated character references in total. The 

following common evaluation metrics are used 

(see [Luo 2005]):  

 

 The MUC-Score. It is based on the idea to 
count the minimum amount of links which 

need to be added to derive the true set of 

entities from the predicted set of entities or 

vice versa, divided by the amount of links 

in the spanning tree of the true partition. 

The MUC-Score itself is the harmonic 

mean out of both numbers that you get 

101



when you switch the true partition with the 

gold partition. 

 The B3-Score. The MUC Score cannot 
measure the influence of singleton clusters, 

that’s why an additional evaluation metric 

is needed. The B³-Score scales the overlap 

of predicted clusters and true clusters, 

based on how many markables were cor-

rectly assigned to a given cluster. 

 

The effect of the different evaluation measures on 

a newspaper corpus with rather short coreference 

chains and on a novel corpus with long chains is 

shown in the baseline analysis in table 1. While for 

newspapers the baseline with n clusters for n NEs 

is very good, for novels the baseline with just one 

cluster for all NEs performs well. This can be ex-

plained using the structure of the underlying enti-

ties. While in newspaper texts many different 

entities with only a few mentions appear, our do-

main shows relatively few entities that tend to 

show up frequently. 

 

 
Table 1: Baseline analysis for a typical newspaper and novel 

corpus with assigning all n NEs to either just one cluster or to 

n different clusters.  

 

We compared our system with the free coreference 

resolution software CorZu, using ParZu
4
 [Sennrich 

2009] as its parser from the university of Zurich, 

which was developed using a newspaper corpus. 

CorZu was given the same annotated named enti-

ties in the same novel fragments, so that the detect-

ed chains were comparable. Table 2 shows the 

results. Our system is about 20 percent points bet-

ter than CorZu for both evaluation scores MUC F1 

and B
3
 F1. 

 
Scores in 

% 

MUC 

precision 

MUC 

recall 

MUC  

F1 

B³ 

prec. 

B³ 

recall 

B³ 

F1 

our system 89.1 83.2 85.5 70.5 83.2 56.0 

CorZu 77.0 57.7 65.9 69.5 22.7 33.6 

Table 2: evaluation results of our system and CorZu on 48 

novel fragments with about 19 000 named entities.  

 

The effect of the passes (see section 3) in our sys-

tem evaluated on the novel corpus is given in table 

                                                           
4
 http://www.cl.uzh.ch/research/coreferenceresolution_en.html 

3. For reference, we added the results from Lee et 

al. [Lee et al. 2011], the results of the system of 

Stanford, evaluated on an ACE newspaper corpus. 

It shows that pronoun resolution is much more 

important in novels than in newspapers, while ex-

act string matches and head matches already result 

in rather high scores on the ACE newspaper cor-

pus. 

 
Scores in 
% 

our system evaluated 
with the novel corpus 

Stanfords Sieve evaluat-
ed with ACE newspaper 

corpus 

Passes MUC F1 B³ F1 MUC F1 B³ F1 

1 27.5 24.6 47.8 69.4 

1-4 37.7 28.1 59.9 73.3 

1-8 38.9 28.9 67.1 76.9 

1-9 83.3 52.6 78.6 80.5 

1-11 85.5 56.0   

Table 3: Evaluation and comparison of the effects of the 

different passes of the rule-based algorithm. 

 

 

We finally evaluated our system on our second test 

set, comprising 30 completely unseen fragments 

and achieved an F1-score of 86% MUC-F1 and a 

B³-F1 of 55.5%. It is almost identical to the result 

of the first test set. Rule-based systems with an 

explanation component allow a fine-grained error 

analysis. Table 4 shows an error analysis for 5 

randomly selected novel fragments from the 30 

novels, drawn from the second test set that we used 

for evaluation:  
 

 
 

 
Table 4: Number of named entities, clusters, evaluation met-

rics and error types for a sample of 5 novel fragments, drawn 

randomly from our second test set comprising 30 fragments. 

The category "Wrong g|n|p" refers to the sum of mistakes the 

algorithm made that were caused by a wrong assignment of 

gender, number or person. The category "Ds related", contains 

all errors related to direct speech, e.g. by assigning a wrong 

speaker or the wrong detection of the addressed person to a 

given direct speech annotation. 

 

Table 4 shows that even though we combined 4 

different morphological resources the recognition 

102



of wrong number, gender and person still makes up 

a fraction of about 14% of the total amount of er-

rors in the analyzed documents. Another part with 

14% of the mistakes is the category that describes 

all errors related to direct speech, e.g. wrong 

speaker detection, missed detection of “Sie” [you] 

in the role of “du” or wrong detection of an ad-

dressed person. We intend to find some additional 

constraints to further reduce the errors made in 

these categories. The next category with 35% error 

contribution, labeled as heuristics, sums up all the 

errors which happened due to a wrong assumption 

of salience, parser errors or errors that were in-

duced by former misclassified NEs. Still the big-

gest share of mistakes (37%) and probably also the 

ones that are most difficult to fix is the class of 

semantic errors. Most of these misclassifications 

can only be resolved with additional knowledge 

about the world or the entities in the novel itself 

(“a widow is a woman who lost her husband; “his 

profession is forester”; …). Apart from these, there 

are other mistakes related to an unmodeled context, 

such as thoughts, songs or letters that appear 

throughout the text. We plan to integrate the work 

of Brunner [Brunner 2015] to detect those instanc-

es and thereby improve the quality of our system. 
 

5 Conclusion 

CR for NE can be viewed as a task in which candi-

dates for coreference are filtered out by constraints 

until only one candidate remains. Rule-based 

knowledge representation is well suited for this 

task. The more constraints can be modeled, the 

better the performance of the system. Our error 

analysis shows that we can cut our current error 

rate by roughly 28% with more precise grammati-

cal constraints (14% for errors related to direct 

speech and 14% for gender, number and person 

related errors). However, we also plan the integra-

tion of semantic constraints and information, simi-

lar to Haghighi and Klein [Haghighi, Klein 2009]. 

A promising way is to collect information about 

the persons in the text, which is also the next step 

in our overall goal, the automated character analy-

sis: determining all attributes assigned in a novel to 

a character. 

 

 

 

References 
 

Aone, C. and Bennett, S. 1995. Evaluating Automated 

and Manual Acquisition of Anaphora Resolution 

Strategies. Proceedings of the 33rd annual meeting 

on Association for Computational Linguistics (ACL 

'95). Association for Computational Linguistics, 

122-129.  

Bohnet, B. 2010. Very High Accuracy and Fast De-

pendency Parsing is not a Contradiction. The 23rd 

Int. Conf. on Computational Linguistics (COLING 

2010), Beijing, China. 

Brants, S., Dipper, S., Eisenberg, P., Hansen, S., König, 

E., Lezius, W., Rohrer, C., Smith, G. and Uszkoreit, 

H. 2004. TIGER: Linguistic Interpretation of a 

German Corpus. Journal of Language and Computa-

tion 2 (4), 597-620. 

Brunner, A. 2015. Automatische Erkennung von Redew-

iedergabe: Ein Beitrag zur Quantitativen Narratolo-

gie (Narratologia). [Automatic Recognition of 

Recorded Speech: a Contribution to Quantitative 

Narratologogia] Walter De Gruyter Inc. 

Durrett G., Hall D., and Klein D. 2013. Decentralized 

Entity-Level Modeling for Coreference Resolution. 

In Proceedings of the 51st Annual Meeting of the 

Association for Computational Linguistics (Volume 

1), 114-124. 

Ferrucci, D. and Lally, A. 2004. UIMA: An Architectur-

al Approach to Unstructured Information Pro-

cessing in the Corporate Research Environment. 

Nat. Lang. Eng. 10, 327-348. 

http://dx.doi.org/10.1017/S1351324904003523 

Fitschen, A., Schmid, H. and Heid, U. 2004.  SMOR: A 

German computational morphology covering deri-

vation, composition, and inflection. Proceedings of 

the IVth International Conference on Language Re-

sources and Evaluation (LREC 2004), 1263-1266. 

Haghighi, A. and Klein, D. 2009. Simple Coreference 

Resolution with Rich Syntactic and Semantic Fea-

tures. In Proceedings of the 2009 Conference on 

Empirical Methods in Natural Language Processing. 

Association for Computational Linguistics, Singa-

pore, 1152-1161. 

Hinrichs, E., Kübler, S. and Naumann, K. 2005. A uni-

fied representation for morphological, syntactic, se-

mantic, and referential annotations. Proceedings of 

the ACL Workshop on Frontiers in Corpus Anota-

tion II: Pie in the Sky, pages 13–20, Ann Arbor, MI. 

Hobbs, J.1976. Pronoun Resolution. Technical report, 

Dept. of Computer Science, CUNY, Technical Re-

port TR761. 

Iosif, E. and Mishra, T. 2014. From Speaker Identifica-

tion to Affective Analysis: A Multi-Step System for 

Analyzing Children`s Stories. Proceedings of the 3rd 

Workshop on Computational Linguistics for Litera-

ture. Gothenburg, Sweden, 40-49. 

103



Jannidis, F., Krug, M., Reger, I. Toepfer, M. Weimer, L, 

Puppe, F. 2015. Automatische Erkennung von Figu-

ren in deutschsprachigen Romanen. [Automatic 

recognition of Characters in German novels]  Digital 

Humanities im deutschsprachigen Raum (Dhd 

2015), Graz, Austria, 2015. 

Klenner, M; Tuggener, D. 2011. An Incremental Entity-

mention Model for Coreference Resolution with Re-

strictive Antecedent Accessibility. Recent Advances 

in Natural Language Processing (RANLP 2011), 

Hissar, Bulgaria, 178-185. 

Lee, H., Peirsman, Y., Chang, A., Chambers, N., 

Surdeanu, M. and Jurafsky, D. 2011. Stanford's Mul-

ti-pass Sieve Coreference Resolution System at the 

CoNLL-2011 Shared Task. Proc. of the 15th Confer-

ence on Computational Natural Language Learning: 

Shared Task (CONLL Shared Task '11). Association 

for Computational Linguistics, 28-34. 

Luo, X. 2005. On Coreference Resolution Performance 

Metrics. Proc. of the conference on Human Lan-

guage Technology and Empirical Methods in Natu-

ral Language Processing (HLT '05). Association for 

Computational Linguistics, 25-32.  

Ng, V. 2010. Supervised Noun Phrase Coreference 

Research: The First Fifteen Years. Proceedings of 

the 48th Annual Meeting of the Association for 

Computational Linguistics (ACL '10). Association 

for Computational Linguistics, 1396-1411. 

Pascal, D. and Baldridge, J. 2008. Specialized models 

and ranking for coreference resolution. In Proceed-

ings of the Conference on Empirical Methods in 

Natural Language Processing (EMNLP '08). Associ-

ation for Computational Linguistics, Stroudsburg, 

PA, USA, 660-669. 

Pradhan, S., Moschitti, A., Xue, N., Uryupina, O. and 

Zhang, Y. 2012. CoNLL-2012 Shared Task: Model-

ing Multilingual Unrestricted Coreference in Onto-

Notes. In Proceedings of EMNLP and CoNLL-2012: 

Shared Task, 1-40. 

Pradhan, S., Ramshaw, L., Marcus, M., Palmer, M., 

Weischedel, R., and Xue, N. 2011. CoNLL-2011 

Shared Task: Modeling Unrestricted Coreference in 

OntoNotes. Proceedings of the Fifteenth Conference 

on Computational Natural Language Learning: 

Shared Task (CONLL Shared Task '11). Association 

for Computational Linguistics, 1-27. 

Rahman, A and Ng, V. 2009. Supervised Models for 

Coreference Resolution. Proceedings of the 2009 

Conference on Empirical Methods in Natural Lan-

guage Processing, 968-977. 

Recasens, M., Martí, T., Taulé,  M., Màrquez, L., and 

Sapena, E. 2009. Coreference Resolution in Multiple 

Languages. Proceedings of the Workshop on Se-

mantic Evaluations: Recent Achievements and Fu-

ture Directions (SEW-2009), 70–75. 

Schmid, H. 1995. Improvements in Part-of-Speech Tag-

ging with an Application to German. Proceedings of 

the ACL SIGDAT-Workshop. Dublin, Ireland. 

Sennrich, R., Schneider, G., Volk, M., Warin, M. 2009. 

A New Hybrid Dependency Parser for German. Pro-

ceedings of GSCL Conference, Potsdam. 

Soon, W, Ng, H. and Lim, D. 2001. A Machine Learn-

ing Approach to Coreference Resolution of Noun 

Phrases. Comput. Linguist. 27 (4), 521-544. 

Yang, J., Mao, Q., Xiang, Q., Tsang, I., Chai, K., Chieu, 

H. 2012. Domain Adaptation for Coreference Reso-

lution: An Adaptive Ensemble Approach. Proceed-

ings of the 2012 Joint Conference on Empirical 

Methods in NLP and CNLL, 744-753. 

 

104


