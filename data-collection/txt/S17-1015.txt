



















































A Mixture Model for Learning Multi-Sense Word Embeddings


Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 121–127,
Vancouver, Canada, August 3-4, 2017. c©2017 Association for Computational Linguistics

A Mixture Model for Learning Multi-Sense Word Embeddings

Dai Quoc Nguyen1, Dat Quoc Nguyen2, Ashutosh Modi1, Stefan Thater1, Manfred Pinkal1

1Department of Computational Linguistics, Saarland University, Germany
{daiquocn, ashutosh, stth, pinkal}@coli.uni-saarland.de

2Department of Computing, Macquarie University, Australia
dat.nguyen@students.mq.edu.au

Abstract

Word embeddings are now a standard
technique for inducing meaning represen-
tations for words. For getting good repre-
sentations, it is important to take into ac-
count different senses of a word. In this
paper, we propose a mixture model for
learning multi-sense word embeddings.
Our model generalizes the previous works
in that it allows to induce different weights
of different senses of a word. The experi-
mental results show that our model outper-
forms previous models on standard evalu-
ation tasks.

1 Introduction

Word embeddings have shown to be useful in var-
ious NLP tasks such as sentiment analysis, topic
models, script learning, machine translation, se-
quence labeling and parsing (Socher et al., 2013;
Sutskever et al., 2014; Modi and Titov, 2014;
Nguyen et al., 2015a,b; Modi, 2016; Ma and
Hovy, 2016; Nguyen et al., 2017; Modi et al.,
2017). A word embedding captures the syntac-
tic and semantic properties of a word by repre-
senting the word in a form of a real-valued vector
(Mikolov et al., 2013a,b; Pennington et al., 2014;
Levy and Goldberg, 2014).

However, usually word embedding models do
not take into account lexical ambiguity. For ex-
ample, the word bank is usually represented by
a single vector representation for all senses in-
cluding sloping land and financial institution. Re-
cently, approaches have been proposed to learn
multi-sense word embeddings, where each sense
of a word corresponds to a sense-specific em-
bedding. Reisinger and Mooney (2010), Huang
et al. (2012) and Wu and Giles (2015) proposed
methods to cluster the contexts of each word and

then using cluster centroids as vector representa-
tions for word senses. Neelakantan et al. (2014),
Tian et al. (2014), Li and Jurafsky (2015) and
Chen et al. (2015) extended Word2Vec models
(Mikolov et al., 2013a,b) to learn a vector repre-
sentation for each sense of a word. Chen et al.
(2014), Iacobacci et al. (2015) and Flekova and
Gurevych (2016) performed word sense induction
using external resources (e.g., WordNet, Babel-
Net) and then learned sense embeddings using the
Word2Vec models. Rothe and Schütze (2015) and
Pilehvar and Collier (2016) presented methods us-
ing pre-trained word embeddings to learn embed-
dings from WordNet synsets. Cheng et al. (2015),
Liu et al. (2015b), Liu et al. (2015a) and Zhang
and Zhong (2016) directly opt the Word2Vec Skip-
gram model (Mikolov et al., 2013b) for learning
the embeddings of words and topics on a topic-
assigned corpus.

One issue in these previous works is that they
assign the same weight to every sense of a word.
The central assumption of our work is that each
sense of a word given a context, should correspond
to a mixture of weights reflecting different asso-
ciation degrees of the word with multiple senses
in the context. The mixture weights will help to
model word meaning better.

In this paper, we propose a new model for learn-
ing Multi-Sense Word Embeddings (MSWE). Our
MSWE model learns vector representations of a
word based on a mixture of its sense represen-
tations. The key difference between MSWE and
other models is that we induce the weights of
senses while jointly learning the word and sense
embeddings. Specifically, we train a topic model
(Blei et al., 2003) to obtain the topic-to-word and
document-to-topic probability distributions which
are then used to infer the weights of topics. We
use these weights to define a compositional vec-
tor representation for each target word to predict

121



its context words. MSWE thus is different from
the topic-based models (Cheng et al., 2015; Liu
et al., 2015b,a; Zhang and Zhong, 2016), in which
we do not use the topic assignments when jointly
learning vector representations of words and top-
ics. Here we not only learn vectors based on the
most suitable topic of a word given its context, but
we also take into consideration all possible mean-
ings of the word.

The main contributions of our study are: (i) We
introduce a mixture model for learning word and
sense embeddings (MSWE) by inducing mixture
weights of word senses. (ii) We show that MSWE
performs better than the baseline Word2Vec Skip-
gram and other embedding models on the word
analogy task (Mikolov et al., 2013a) and the word
similarity task (Reisinger and Mooney, 2010).

2 The mixture model

In this section, we present the mixture model for
learning multi-sense word embeddings. Here we
treat topics as senses. The model learns a repre-
sentation for each word using a mixture of its top-
ical representations.

Given a number of topics and a corpus D of
documents d = {wd,1, wd,2, ..., wd,Md}, we apply
a topic model (Blei et al., 2003) to obtain the topic-
to-word Pr(w|t) and document-to-topic Pr(t|d)
probability distributions. We then infer a weight
for themth word wd,m with topic t in document d:

λd,m,t = Pr(wd,m|t)× Pr(t|d) (1)

We define two MSWE variants: MSWE-1 learns
vectors for words based on the most suitable topic
given document d while MSWE-2 marginalizes
over all senses of a word to take into account all
possible senses of the word:

MSWE-1: swd,m =
vwd,m + λd,m,t′ × vt′

1 + λd,m,t′

MSWE-2: swd,m =
vwd,m +

∑T
t=1 λd,m,t × vt

1 +
∑T

t=1 λd,m,t

where swd,m is the compositional vector represen-
tation of the mth word wd,m and the topics in doc-
ument d; vw is the target vector representation of a
word type w in vocabulary V ; vt is the vector rep-
resentation of topic t; T is the number of topics;
λd,m,t is defined as in Equation 1, and in MSWE-1
we define t′ = arg max

t
λd,m,t.

We learn representations by minimizing the fol-
lowing negative log-likelihood function:

L = − ∑
d∈D

Md∑
m=1

∑
−k≤j≤k

j 6=0

log Pr(ṽwd,m+j |swd,m) (2)

where the mth word wd,m in document d is a tar-
get word while the (m+j)th word wd,m+j in doc-
ument d is a context word of wd,m and k is the
context size. In addition, ṽw is the context vec-
tor representation of the word type w. The proba-
bility Pr(ṽwd,m+j |swd,m) is defined using the soft-
max function as follows:

Pr(ṽwd,m+j |swd,m) =
exp(ṽTwd,m+jswd,m)∑

c′∈V exp(ṽ
T
c′swd,m)

Since computing log Pr(ṽwd,m+j |swd,m) is ex-
pensive for each training instance, we approximate
log Pr(ṽwd,m+j |swd,m) in Equation 2 with the
following negative-sampling objective (Mikolov
et al., 2013b):

Od,m,m+j = log σ
(
ṽTwd,m+jswd,m

)
+

K∑
i=1

log σ
(
−ṽTciswd,m

)
(3)

where each word ci is sampled from a noise distri-
bution.1 In fact, MSWE can be viewed as a gener-
alization of the well-known Word2Vec Skip-gram
model with negative sampling (Mikolov et al.,
2013b) where all the mixture weights λd,m,t are
set to zero. The models are trained using Stochas-
tic Gradient Descent (SGD).

3 Experiments

We evaluate MSWE on two different tasks: word
similarity and word analogy. We also pro-
vide experimental results obtained by the baseline
Word2Vec Skip-gram model and other previous
works.

Note that not all previous results are mentioned
in this paper for comparison because the train-
ing corpora used in most previous research work
are much larger than ours (Baroni et al., 2014; Li
and Jurafsky, 2015; Schwartz et al., 2015; Levy
et al., 2015). Also there are differences in the
pre-processing steps that could affect the results.
We could also improve obtained results by using a

1We use an unigram distribution raised to the 3/4 power
(Mikolov et al., 2013b) as the noise distribution.

122



larger training corpus, but this is not central point
of our paper. The objective of our paper is that the
embeddings of topic and word can be combined
into a single mixture model, leading to good im-
provements as established empirically.

3.1 Experimental Setup

Following Huang et al. (2012) and Neelakantan
et al. (2014), we use the Wesbury Lab Wikipedia
corpus (Shaoul and Westbury, 2010) containing
over 2M articles with about 990M words for train-
ing. In the preprocessing step, texts are lower-
cased and tokenized, numbers are mapped to 0,
and punctuation marks are removed. We extract a
vocabulary of 200,000 most frequent word tokens
from the pre-processed corpus. Words not occur-
ring in the vocabulary are mapped to a special to-
ken UNK, in which we use the embedding of UNK
for unknown words in the benchmark datasets.

We firstly use a small subset extracted from the
WS353 dataset (Finkelstein et al., 2002) to tune
the hyper-parameters of the baseline Word2Vec
Skip-gram model for the word similarity task (see
Section 3.2 for the task definition). We then
directly use the tuned hyper-parameters for our
MSWE variants. Vector size is also a hyper-
parameter. While some approaches use a higher
number of dimensions to obtain better results, we
fix the vector size to be 300 as used by the baseline
for a fair comparison. The vanilla Latent Dirichlet
Allocation (LDA) topic model (Blei et al., 2003) is
not scalable to a very large corpus, so we explore
faster online topic models developed for large cor-
pora. We train the online LDA topic model (Hoff-
man et al., 2010) on the training corpus, and use
the output of this topic model to compute the mix-
ture weights as in Equation 1.2 We also use the
same WS353 subset to tune the numbers of top-
ics T ∈ {50, 100, 200, 300, 400}. We find that the
most suitable numbers are T = 50 and T = 200
then used for all our experiments. Here we learn
300-dimensional embeddings with the fixed con-
text size k = 5 (in Equation 2) and K = 10 (in
Equation 3) as used by the baseline. During train-
ing, we randomly initialize model parameters (i.e.
word and topic embeddings) and then learn them
by using SGD with the initial learning rate of 0.01.

2We use default parameters in gensim (Řehůřek and So-
jka, 2010) for the online LDA model.

Dataset Word pairs Reference
WS353 353 Finkelstein et al. (2002)
SIMLEX 999 Hill et al. (2015)
SCWS 2003 Huang et al. (2012)
RW 2034 Luong et al. (2013)
MEN 3000 Bruni et al. (2014)

Table 1: The benchmark datasets. WS353:
WordSimilarity-353. RW: Rare-Words. SIMLEX:
SimLex-999. SCWS: Stanford’s Contextual Word
Similarities. MEN: The MEN Test Collection.
Each dataset contains similarity scores of human
judgments for pairs of words.

3.2 Word Similarity
The word similarity task evaluates the quality of
word embedding models (Reisinger and Mooney,
2010). For a given dataset of word pairs, the eval-
uation is done by calculating correlation between
the similarity scores of corresponding word em-
bedding pairs with the human judgment scores.
Higher Spearman’s rank correlation (ρ) reflects
better word embedding model. We evaluate MSWE
on standard datasets (as given in Table 1) for the
word similarity evaluation task.

Following Reisinger and Mooney (2010),
Huang et al. (2012), Neelakantan et al. (2014), we
compute the similarity scores for a pair of words
(w,w′) with or without their respective contexts
(c, c′) as:

GlobalSim
(
w,w′

)
= cos (vw,vw′)

AvgSim
(
w,w′

)
=

1

T 2

T∑
t=1

T∑
t′=1

cos (vw,t,vw′,t′)

AvgSimC
(
w,w′

)
=

1

T 2

T∑
t=1

T∑
t′=1

(
δ (vw,t,vc)× δ (vw′,t′ ,vc′)

× cos (vw,t,vw′,t′)
)

where vw is the vector representation of the word
w, vw,t is the multiple representation of the word
w and the topic t, vc is the vector representation
of the context c of the word w. And cos (v,v′)
is the cosine similarity between two vectors v
and v′. For our experiments, we set vw,t =

vw ⊕ (Pr(w|t)× vt) and vc =
(

1
|c|
∑

w∈c vw
)
⊕

(
∑

t Pr (t|c)× vt), in which ⊕ is the concatena-
tion operation and Pr (t|c) is inferred from the
topic models by considering context c as a docu-
ment. GlobalSim only regards word embeddings,

123



Model RW SIMLEX SCWS WS353 MEN
Huang et al. (2012) – – 58.6 71.3 –
Luong et al. (2013) 34.36 – 48.48 64.58 –
Qiu et al. (2014) 32.13 – 53.40 65.19 –
Neelakantan et al. (2014) – – 65.5 69.2 –
Chen et al. (2014) – – 64.2 – –
Hill et al. (2015) – 41.4 – 65.5 69.9
Vilnis and McCallum (2015) – 32.23 – 65.49 71.31
Schnabel et al. (2015) – – – 64.0 70.7
Rastogi et al. (2015) 32.9 36.7 65.6 70.8 73.9
Flekova and Gurevych (2016) – – – – 74.26
Word2Vec Skip-gram 32.64 38.20 66.37 71.61 75.49
MSWE-150 34.85 38.77 66.83 72.40 76.23
MSWE-1200 35.27 38.70 66.80 72.05 76.05
MSWE-250 34.98 38.79 66.61 71.71 75.90
MSWE-2200 35.56? 39.19? 66.65 72.29 76.37?

Table 2: Spearman’s rank correlation (ρ×100) for
the word similarity task when using GlobalSim.
Subscripts 50 and 200 denote the online LDA
topic model trained with T = 50 and T = 200
topics, respectively. ? denotes that our best score
is significantly higher than the score of the base-
line (with p < 0.05, online toolkit from http:
//www.philippsinger.info/?p=347). Scores in bold
and underline are the best and second best scores.

while AvgSim considers multiple representations
to capture different meanings (i.e. topics) and us-
ages of a word. AvgSimC generalizes AvgSim
by taking into account the likelihood δ (vw,t,vc)
that word w takes topic t given context c. δ (v,v′)
is the inverse of the cosine distance from v to v′

(Huang et al., 2012; Neelakantan et al., 2014).

3.2.1 Results for word similarity
Table 2 compares the evaluation results of MSWE
with results reported in prior work on the stan-
dard word similarity task when using GlobalSim.
We use subscripts 50 and 200 to denote the topic
model trained with T = 50 and T = 200 topics,
respectively. Table 2 shows that our model out-
performs the baseline Word2Vec Skip-gram model
(in fifth row from bottom). Specifically, on the RW
dataset, MSWE obtains a significant improvement
of 2.92 in the Spearman’s rank correlation (which
is about 8.5% relative improvement).

Compared to the published results, MSWE ob-
tains the highest accuracy on the RW, SCWS,
WS353 and MEN datasets, and achieves the second
highest result on the SIMLEX dataset. These in-
dicate that MSWE learns better representations for
words taking into account different meanings.

3.2.2 Results for contextual word similarity
We evaluate our model MSWE by using AvgSim
and AvgSimC on the benchmark SCWS dataset

Model AvgSim AvgSimC
Huang et al. (2012) 62.8 65.7
Neelakantan et al. (2014) 67.3 69.3
Chen et al. (2014) 66.2 68.9
Chen et al. (2015) 65.7 66.4
Wu and Giles (2015) – 66.4
Jauhar et al. (2015) – 65.7
Cheng and Kartsaklis (2015) 62.5 –
Iacobacci et al. (2015) 62.4 –
Cheng et al. (2015) – 65.9
MSWE-150 66.6 66.7
MSWE-1200 66.7 66.6
MSWE-250 66.4 66.6
MSWE-2200 66.6 66.6

Table 3: Spearman’s rank correlation (ρ× 100) on
SCWS, using AvgSim and AvgSimC.

which considers effects of the contextual informa-
tion on the word similarity task. As shown in Ta-
ble 3, MSWE scores better than the closely related
model proposed by Cheng et al. (2015) and gener-
ally obtains good results for this context sensitive
dataset. Although we produce better scores than
Neelakantan et al. (2014) and Chen et al. (2014)
when using GlobalSim, we are outperformed by
them when using AvgSim and AvgSimC. Nee-
lakantan et al. (2014) clustered the embeddings
of the context words around each target word to
predict its sense and Chen et al. (2014) used pre-
trained word embeddings to initialize vector rep-
resentations of senses taken from WordNet, while
we use a fixed number of topics as senses for
words in MSWE.

3.3 Word Analogy

We evaluate the embedding models on the word
analogy task introduced by Mikolov et al. (2013a).
The task aims to answer questions in the form of
“a is to b as c is to ?”, denoted as “a : b → c :
?” (e.g., “Hanoi : Vietnam → Bern : ?”). There
are 8,869 semantic and 10,675 syntactic questions
grouped into 14 categories. Each question is an-
swered by finding the most suitable word closest
to “vb−va +vc” measured by the cosine similar-
ity. The answer is correct only if the found closest
word is exactly the same as the gold-standard (cor-
rect) one for the question.

We report accuracies in Table 4 and show that
MSWE achieves better results in comparison with
the baseline Word2Vec Skip-gram. In particular,
MSWE reaches the accuracies of around 69.7%

124



Model Accuracy (%)
Pennington et al. (2014) 70.3
Baroni et al. (2014) 68.0
Neelakantan et al. (2014) 64.0
Ghannay et al. (2016) 62.3
Word2Vec Skip-gram 68.6
MSWE-150 69.6
MSWE-1200 69.9
MSWE-250 69.7
MSWE-2200 69.5

Table 4: Accuracies for the word analogy task. All
our results are significantly higher than the result
of Word2Vec Skip-gram (with two-tail p < 0.001
using McNemar’s test). Pennington et al. (2014)
used a larger training corpus of 1.6B words.

which is higher than the accuracy of 68.6% ob-
tained by Word2Vec Skip-gram.

4 Conclusions

In this paper, we described a mixture model for
learning multi-sense embeddings. Our model in-
duces mixture weights to represent a word given
context based on a mixture of its sense representa-
tions. The results show that our model scores bet-
ter than Word2Vec, and produces highly competi-
tive results on the standard evaluation tasks. In fu-
ture work, we will explore better methods for tak-
ing into account the contextual information. We
also plan to explore different approaches to com-
pute the mixture weights in our model. For exam-
ple, if there is a large sense-annotated corpus avail-
able for training, the mixture weights could be de-
fined based on the frequency (sense-count) distri-
butions, instead of using the probability distribu-
tions produced by a topic model. Furthermore, it is
possible to consider the weights of senses as addi-
tional model parameters to be then learned during
training.

Acknowledgments

This research was funded by the German Research
Foundation (DFG) as part of SFB 1102 “Informa-
tion Density and Linguistic Encoding”. We would
like to thank anonymous reviewers for their help-
ful comments.

References
Marco Baroni, Georgiana Dinu, and Germán

Kruszewski. 2014. Don’t count, predict! a
systematic comparison of context-counting vs.
context-predicting semantic vectors. In Proceedings
of the 52nd Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long
Papers). pages 238–247.

David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research 3:993–1022.

Elia Bruni, Nam Khanh Tran, and Marco Baroni. 2014.
Multimodal distributional semantics. Journal of Ar-
tificial Intelligence Research 49:1–47.

Tao Chen, Ruifeng Xu, Yulan He, and Xuan Wang.
2015. Improving distributed representation of word
sense via wordnet gloss composition and context
clustering. In Proceedings of the 53rd Annual Meet-
ing of the Association for Computational Linguistics
and the 7th International Joint Conference on Natu-
ral Language Processing (Volume 2: Short Papers).
pages 15–20.

Xinxiong Chen, Zhiyuan Liu, and Maosong Sun. 2014.
A unified model for word sense representation and
disambiguation. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP). pages 1025–1035.

Jianpeng Cheng and Dimitri Kartsaklis. 2015. Syntax-
aware multi-sense word embeddings for deep com-
positional models of meaning. In Proceedings of the
2015 Conference on Empirical Methods in Natural
Language Processing. pages 1531–1542.

Jianpeng Cheng, Zhongyuan Wang, Ji-Rong Wen, Jun
Yan, and Zheng Chen. 2015. Contextual text under-
standing in distributional semantic space. In Pro-
ceedings of the 24th ACM International on Confer-
ence on Information and Knowledge Management.
pages 133–142.

Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2002. Placing search in context: The
concept revisited. ACM Transactions on Informa-
tion Systems 20:116–131.

Lucie Flekova and Iryna Gurevych. 2016. Supersense
embeddings: A unified model for supersense inter-
pretation, prediction, and utilization. In Proceed-
ings of the 54th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers). pages 2029–2041.

Sahar Ghannay, Benoit Favre, Yannick Estve, and
Nathalie Camelin. 2016. Word embedding evalua-
tion and combination. In Proceedings of the Tenth
International Conference on Language Resources
and Evaluation (LREC 2016).

125



Felix Hill, Roi Reichart, and Anna Korhonen. 2015.
Simlex-999: Evaluating semantic models with gen-
uine similarity estimation. Computational Linguis-
tics 41:665–695.

Matthew Hoffman, Francis R. Bach, and David M.
Blei. 2010. Online learning for latent dirichlet al-
location. In Advances in Neural Information Pro-
cessing Systems 23. pages 856–864.

Eric H. Huang, Richard Socher, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Improving word
representations via global context and multiple word
prototypes. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics: Long Papers - Volume 1. pages 873–882.

Ignacio Iacobacci, Mohammad Taher Pilehvar, and
Roberto Navigli. 2015. Sensembed: Learning sense
embeddings for word and relational similarity. In
Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the
7th International Joint Conference on Natural Lan-
guage Processing (Volume 1: Long Papers). pages
95–105.

Sujay Kumar Jauhar, Chris Dyer, and Eduard Hovy.
2015. Ontologically grounded multi-sense repre-
sentation learning for semantic vector space models.
In Proceedings of the 2015 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies.
pages 683–693.

Omer Levy and Yoav Goldberg. 2014. Neural word
embedding as implicit matrix factorization. In Ad-
vances in Neural Information Processing Systems
27. pages 2177–2185.

Omer Levy, Yoav Goldberg, and Ido Dagan. 2015. Im-
proving distributional similarity with lessons learned
from word embeddings. Transactions of the Associ-
ation for Computational Linguistics 3:211–225.

Jiwei Li and Dan Jurafsky. 2015. Do multi-sense em-
beddings improve natural language understanding?
In Proceedings of the 2015 Conference on Empiri-
cal Methods in Natural Language Processing. pages
1722–1732.

Pengfei Liu, Xipeng Qiu, and Xuanjing Huang. 2015a.
Learning context-sensitive word embeddings with
neural tensor skip-gram model. In Proceedings of
the 24th International Conference on Artificial In-
telligence. pages 1284–1290.

Yang Liu, Zhiyuan Liu, Tat-Seng Chua, and Maosong
Sun. 2015b. Topical word embeddings. In AAAI
Conference on Artificial Intelligence. pages 2418–
2424.

Thang Luong, Richard Socher, and Christopher Man-
ning. 2013. Better word representations with recur-
sive neural networks for morphology. In Proceed-
ings of the Seventeenth Conference on Computa-
tional Natural Language Learning. pages 104–113.

Xuezhe Ma and Eduard Hovy. 2016. End-to-end se-
quence labeling via bi-directional lstm-cnns-crf. In
Proceedings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers). pages 1064–1074.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space. CoRR abs/1301.3781.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.
Corrado, and Jeffrey Dean. 2013b. Distributed rep-
resentations of words and phrases and their com-
positionality. In Advances in Neural Information
Processing Systems 26: 27th Annual Conference on
Neural Information Processing Systems 2013. pages
3111–3119.

Ashutosh Modi. 2016. Event embeddings for seman-
tic script modeling. In Proceedings of the Confer-
ence on Computational Natural Language Learning.
pages 75–83.

Ashutosh Modi and Ivan Titov. 2014. Inducing neu-
ral models of script knowledge. In Proceedings of
the Eighteenth Conference on Computational Natu-
ral Language Learning. pages 49–57.

Ashutosh Modi, Ivan Titov, Vera Demberg, Asad Say-
eed, and Manfred Pinkal. 2017. Modelling seman-
tic expectation: Using script knowledge for refer-
ent prediction. Transactions of the Association for
Computational Linguistics 5:31–44.

Arvind Neelakantan, Jeevan Shankar, Alexandre Pas-
sos, and Andrew McCallum. 2014. Efficient non-
parametric estimation of multiple embeddings per
word in vector space. In Proceedings of the 2014
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP). pages 1059–1069.

Dat Quoc Nguyen, Richard Billingsley, Lan Du, and
Mark Johnson. 2015a. Improving Topic Models
with Latent Feature Word Representations. Trans-
actions of the Association for Computational Lin-
guistics 3:299–313.

Dat Quoc Nguyen, Mark Dras, and Mark Johnson.
2017. A Novel Neural Network Model for Joint
POS Tagging and Graph-based Dependency Pars-
ing. In Proceedings of the CoNLL 2017 Shared
Task: Multilingual Parsing from Raw Text to Uni-
versal Dependencies.

Dat Quoc Nguyen, Kairit Sirts, and Mark Johnson.
2015b. Improving Topic Coherence with Latent
Feature Word Representations in MAP Estimation
for Topic Modeling. In Proceedings of the Aus-
tralasian Language Technology Association Work-
shop 2015. pages 116–121.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors
for word representation. In Proceedings of the
2014 Conference on Empirical Methods in Natural
Language Processing (EMNLP 2014). pages 1532–
1543.

126



Mohammad Taher Pilehvar and Nigel Collier. 2016.
De-conflated semantic representations. In Proceed-
ings of the 2016 Conference on Empirical Methods
in Natural Language Processing. pages 1680–1690.

Siyu Qiu, Qing Cui, Jiang Bian, Bin Gao, and Tie-Yan
Liu. 2014. Co-learning of word representations and
morpheme representations. In Proceedings of COL-
ING 2014, the 25th International Conference on
Computational Linguistics: Technical Papers. pages
141–150.

Pushpendre Rastogi, Benjamin Van Durme, and Ra-
man Arora. 2015. Multiview LSA: Representation
Learning via Generalized CCA. In Proceedings of
the 2015 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies. pages 556–566.

Radim Řehůřek and Petr Sojka. 2010. Software Frame-
work for Topic Modelling with Large Corpora. In
Proceedings of the LREC 2010 Workshop on New
Challenges for NLP Frameworks. pages 45–50.

Joseph Reisinger and Raymond J. Mooney. 2010.
Multi-prototype vector-space models of word mean-
ing. In Human Language Technologies: The 2010
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics.
pages 109–117.

Sascha Rothe and Hinrich Schütze. 2015. Autoex-
tend: Extending word embeddings to embeddings
for synsets and lexemes. In Proceedings of the
53rd Annual Meeting of the Association for Compu-
tational Linguistics and the 7th International Joint
Conference on Natural Language Processing, Vol-
ume 1: Long Papers. pages 1793–1803.

Tobias Schnabel, Igor Labutov, David Mimno, and
Thorsten Joachims. 2015. Evaluation methods for
unsupervised word embeddings. In Proceedings of
the Conference on Empirical Methods in Natural
Language Processing. pages 298–307.

Roy Schwartz, Roi Reichart, and Ari Rappoport. 2015.
Symmetric pattern based word embeddings for im-
proved word similarity prediction. In Proceedings
of CoNLL 2015. pages 258–267.

Cyrus Shaoul and Chris Westbury. 2010. The westbury
lab wikipedia corpus. Edmonton, AB: University of
Alberta .

Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. 2013. Recursive deep models
for semantic compositionality over a sentiment tree-
bank. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Process-
ing. pages 1631–1642.

Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.
Sequence to sequence learning with neural net-
works. In Proceedings of the 27th International
Conference on Neural Information Processing Sys-
tems. pages 3104–3112.

Fei Tian, Hanjun Dai, Jiang Bian, Bin Gao, Rui Zhang,
Enhong Chen, and Tie-Yan Liu. 2014. A probabilis-
tic model for learning multi-prototype word embed-
dings. In Proceedings of COLING 2014, the 25th In-
ternational Conference on Computational Linguis-
tics: Technical Papers. pages 151–160.

Luke Vilnis and Andrew McCallum. 2015. Word rep-
resentations via gaussian embedding. International
Conference on Learning Representations (ICLR) .

Zhaohui Wu and C. Lee Giles. 2015. Sense-aware se-
mantic analysis: A multi-prototype word represen-
tation model using wikipedia. In Proceedings of the
Twenty-Ninth AAAI Conference on Artificial Intelli-
gence. pages 2188–2194.

Heng Zhang and Guoqiang Zhong. 2016. Improv-
ing short text classification by learning vector
representations of both words and hidden topics.
Knowledge-Based Systems 102:76–86.

127


