



















































Shallow Domain Adaptive Embeddings for Sentiment Analysis


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 5549–5558,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

5549

Shallow Domain Adaptive Embeddings for Sentiment Analysis

Prathusha K Sarma, Yingyu Liang and William A Sethares

University of Wisconsin-Madison
{kameswarasar,sethares}@wisc.edu,

yliang@cs.wisc.edu

Abstract

This paper proposes a way to improve the per-
formance of existing algorithms for text clas-
sification in domains with strong language se-
mantics. We propose a domain adaptation
layer learns weights to combine a generic and
a domain specific (DS) word embedding into
a domain adapted (DA) embedding. The DA
word embeddings are then used as inputs to a
generic encoder + classifier framework to per-
form a downstream task such as classification.
This adaptation layer is particularly suited to
datasets that are modest in size, and which are,
therefore, not ideal candidates for (re)training
a deep neural network architecture. Results
on binary and multi-class classification tasks
using popular encoder architectures, includ-
ing current state-of-the-art methods (with and
without the shallow adaptation layer) show the
effectiveness of the proposed approach.

1 Introduction

Domain Adaptation (DA) algorithms are becom-
ing increasingly relevant in addressing issues re-
lated to i) lack of availability of training data in
domains of interest and in ii) exploiting domain
idiosyncrasies to improve performance of out-of-
domain algorithms. While some state of the art
DA algorithms focus on improving on a down-
stream task, such as classification in bilingual or
crosslingual framework (Hangya et al., 2018) (Liu
et al., 2018), others (Zeng et al., 2018), (Shang
et al., 2018) focus on tackling DA at the word level
for various downstream applications such as clas-
sification, tagging etc.

Particularly, work by (An et al., 2018)
and (K Sarma et al., 2018) address the issue of
context-based domain adaptation within a single
language. The authors of both works argue that
word context causes significant changes, espe-
cially in document level sentiment associations.

The idea is illustrated by providing examples of
words such as ‘kill,’ that has a more positive sen-
timent in a document describing video games than
when used in a news article. Similarly, words used
in social media and medical domains tend to be
atypical and idiosyncratic as compared to generic
language use.

DA algorithms and related transfer learning al-
gorithms are often successfully used to perform
sentiment analysis, where given a document, the
sentiment label for the document is to be deter-
mined. Since adaptation can be viewed as a shift
in the position of words within the embeddings
spaces across the domains of interest, shifts in the
spatial position of the words can be measured to
verify that the algorithm is indeed capturing do-
main knowledge. Once it is established that a
given DA method is capturing domain semantics
at the word level, the DA word embeddings can
then be used as input to an encoder+classification
layer to perform sentiment analysis.

This paper applies a generic adaptation frame-
work to tasks that make use of domain seman-
tics. Contributions of this paper are fourfold. i)
First, we propose a generic domain adaptation
layer that can be interfaced with any neural net-
work block with the aim of improving perfor-
mance on a downstream sentiment classification
task. ii) Second, we measure the significance of
the shift in word position when represented in a
generic embedding space and when represented in
the adapted embedding space. The strategy is to
construct DA word embeddings as in (K Sarma
et al., 2018) and make use of a seed lexicon
as in (Hangya et al., 2018) and (Mikolov et al.,
2013a). However, rather than use the seed lexicon
to transform words from two different domains
into a common space, we use the seed lexicon to
verify that the DA word embeddings have indeed
captured significant domain semantics. iii) We test



5550

our hypothesis on a recently introduced (Friedland
et al., 2017)1 data set of tweets from Liberal and
Conservative Twitter users from the state of Wis-
consin. As illustrated in (An et al., 2018), polit-
ical discourse makes for an interesting setting to
study domain adaptation. (iv) Via thorough exper-
imentation we show that using a DA layer helps
improve the performance of standard architectures
on sentiment analysis tasks by 2 − 8% on various
binary and multi-class balanced and imbalanced
data sets. We also, show that our DA architec-
tures outperform sophisticated architectures such
as BERT, LR-Bi-LSTM, Self-attention by 1−2%.

The rest of this paper is organized as follows,
Section 2 discusses related work. Section 3 de-
scribes in detail, the proposed algorithm, Section 4
presents the experimental results and Section 5
concludes this work.

2 Related Work

This paper discusses domain adaptation tech-
niques and the applicability of existing algorithms
to diverse downstream tasks. Central themes of
this paper tie into word level domain semantics,
while being related to the overall objective of do-
main adaptation.

Recent work such as SEMAXIS (An et al.,
2018) investigates the use of word level domain
semantics for applications beyond sentiment anal-
ysis. The authors introduce the concept of a se-
mantic axis based on word antonym pairs to cap-
ture semantic differences across corpora. Simi-
larly, work by (Hamilton et al., 2016) captures do-
main semantics in the form of sentiment lexicons
via graph propagation. While both these lexical
based approaches are similar to the ideas of this
paper, a major difference is that like (K Sarma
et al., 2018), we do not make use of any prede-
fined domain specific lexicons to capture domain
semantics. Our idea is to use word occurrences
and contexts to provide a raw estimate of the do-
main semantics. Using generic embedding spaces
as baselines, adaptation is performed by project-
ing generic embeddings into a learned ‘adaptation’
space.

Typical downstream applications such as cross
lingual and/or multi-domain sentiment classifica-
tion, using algorithms proposed by (Hangya et al.,
2018), (Liu et al., 2018), make use of DNNs

1https://github.com/naacl18sublong/
Friedland

with RNN blocks such as BiLSTMs to learn both
generic and domain specific representations. Par-
ticularly, work focused on multi-domain sentiment
classification as in (Liu et al., 2016), (Nam and
Han, 2016), proposes building neural architectures
for each domain of interest, in addition to shared
representation layers across all domains. While
these techniques are effective, they are not ideal
in domains with limited data.

On the other hand work such as ELMo (Pe-
ters et al., 2018) and BERT (Devlin et al., 2018)
propose deeply connected layers to learn sentence
embeddings by exploiting bi-directional contexts.
While both methods have achieved tremendous
success in producing word (ELMo) and sentence
(BERT) level encodings that perform well in sev-
eral disparate NLP tasks such as question-answer
solving, paraphrasing, POS tagging, sentiment
analysis etc, these models are computationally ex-
pensive and require large amounts of training data.
Particularly when used in a transfer learning set-
ting, both algorithms assume that a large amount
of data is present in the source as well as the tar-
get domains. In contrast, our proposed adapta-
tion layer is particularly well suited in applications
with limited data in the target domain.

Our proposed algorithms depart from these ap-
proaches by capturing domain semantics through
shallow layers for use with generic encoder archi-
tectures. Since some of the most successful algo-
rithms in text classification (Kim, 2014) and sen-
tence embeddings (Conneau et al., 2017) make use
of CNN and BiLSTM building blocks, we suggest
a generic adaptation framework that can be inter-
faced with these standard neural network blocks to
improve performance on downstream tasks, par-
ticularly on small sized data sets.

3 Shallow Domain Adaptation

This section introduces a ‘shallow’ adaptation
layer2, to be interfaced with a neural network
based sentence encoding layer, followed by a clas-
sification layer. Figure 1 describes a generic
framework of the proposed model. Brief descrip-
tions of the three layers follow.
Adaptation Layer: Generic word embed-
dings from GloVe (Pennington et al., 2014) or
word2vec (Mikolov et al., 2013b) are combined
with an LSA-based (Deerwester et al., 1990) Do-

2Codes for shallow adaptation will be made available
upon acceptance of paper for publication.

https://github.com/naacl18sublong/Friedland
https://github.com/naacl18sublong/Friedland


5551

Sentence Encoder

Classification Layer

α α α α αβ β β ββ

w this,G w this,DS w is,DSw is,G wa,G wa,DS wgreat,DSwgreat,G wmovie,G wmovie,DS

w this,DA w is,DA wa,DA wgreat,DA wmovie,DA

Figure 1: This figure illustrates the three part neural
network model. The first part comprises of an adap-
tation layer, second, a generic sentence encoder and
the lastly a classification layer. Inputs to the shallow
adaptation layer are the generic and the domain spe-
cific (DS) word embeddings. Output of the adaptation
layer is the domain adapted (DA) word embedding.

main Specific (DS) word embedding using KCCA
as proposed in (K Sarma et al., 2018). Once the
KCCA projections (w̄i,G, w̄i,DS) for the generic
and DS word embeddings are obtained for a given
word i, weights α and β are learned such that
the domain adapted word embedding is w̄i,DA =
αw̄i,G + βw̄i,DS . Weights α and β are learned by
a single CNN layer. Since this layer learns only
the weights used to obtain the DA embeddings, we
call this a ‘shallow’ adaptation layer.

The output of this layer (i.e., the DA embed-
dings), is used to initialize a generic sentence en-
coder. Output from the sentence encoder is sent to
a classification layer and the classification error is
back propagated to update weights of the adapta-
tion layer. By keeping the sentence encoder fixed,
(i.e., the parameters of the encoder are not updated
during back propagation), this method can be gen-
eralized for use with any pre-trained sentence en-
coder. This is advantageous, particularly in data
sets that are too small to be trained end-to-end.
Another advantage is that adaptation is not per-
formed on fully connected layers that need to be
trained end-to-end. This considerably reduces the
number of parameters that need to be learned.

Figure 2 illustrates the CNN encoder architec-
ture and the Adaptation Layer. Input to the adap-
tation layer is a word embedding of dimension 2d,
where d is the dimension of the generic and DS
embeddings. For a given word, the generic and
DS embeddings are concatenated and interleaved.
A single layer CNN learns α and β via a 2 × 1
kernel. The domain adapted word wi,DA is then
passed as input to the CNN encoder as shown in
the figure. Note that we also use this framework
with a BiLSTM sentence encoder as in (Conneau

et al., 2017) and is illustrated in Figure 2. Output
of the BiLSTM units is max-pooled to obtain the
sentence encoding.
Sentence Encoder: CNN encoder as in (Kim,
2014) and BiLSTM encoder proposed in (Con-
neau et al., 2017) are used to encode sentences in
this work. Since both models have been exten-
sively discussed in literature we shall skip elab-
orating on the basic model architectures. We re-
produced the basic encoder models for the experi-
ments in Section 4 with exception of few hyper-
parameters, since the choice of data sets in this
work differs from that of (Kim, 2014) and (Con-
neau et al., 2017).
Classification Layer: This layer learns weights
for multi-class classification using a soft-max
function.

3.1 Evaluating KCCA for Domain
Adaptation

In their work (K Sarma et al., 2018), the authors
demonstrate the effectiveness of KCCA based em-
beddings via classification experiments. However,
in order to verify that the word level adaptation
performed by the KCCA step can capture relevant
domain semantics, we perform the following ex-
periment.

3.2 Experimental Setup
Start with a dataset where language use is polar-
ized. For example, the language used in the tweets
of liberals likely differs from the language used in
the tweets of conservatives as they express opin-
ions on key political issues such as government,
immigration, border control etc. These two vo-
cabularies (Liberal and Conservative) are encoded
in DS embeddings which are each paired with a
generic embedding, and then again mapped into a
common DA space. The words in this common
space that are most different are then identified,
and compared to a ground truth/gold standard list
of keywords which represent ideas or concepts on
which the two groups of users are known to have
the most polarized opinions. Comparing the gold
standard list with the list derived from the DA em-
bedding demonstrates the efficacy of the method.
This experiment adopts the list of politically con-
tentious words that appear in (Li et al., 2017).

From the tokenized tweets of the users, con-
struct a vocabulary of words VLib and VCon that
represent the Liberal and Conservative tweets re-
spectively. Domain specific (DS) word embed-



5552

th
is is a

gr
ea

t
m

ov
ie

1 0 Logistic 
Regression

Max-
pooling 

layer

C
on

vo
lu

tio
n 

la
ye

r

Input 
sentence 

embedding 
matrix

Adaptation 
Layer

�
<latexit sha1_base64="g+IRNevPc1g3Nk0ZK/Q3r3J8OUI=">AAAB7HicbZBNS8NAEIYn9avWr6pHL8EieCqJCHosevFYwdRCG8pmO2mXbjZhdyKU0t/gxYMiXv1B3vw3btsctPWFhYd3ZtiZN8qkMOR5305pbX1jc6u8XdnZ3ds/qB4etUyaa44BT2Wq2xEzKIXCgARJbGcaWRJJfIxGt7P64xNqI1L1QOMMw4QNlIgFZ2StoBshsV615tW9udxV8AuoQaFmr/rV7ac8T1ARl8yYju9lFE6YJsElTivd3GDG+IgNsGNRsQRNOJkvO3XPrNN341Tbp8idu78nJiwxZpxEtjNhNDTLtZn5X62TU3wdToTKckLFFx/FuXQpdWeXu32hkZMcW2BcC7ury4dMM042n4oNwV8+eRVaF3Xf8v1lrXFTxFGGEziFc/DhChpwB00IgIOAZ3iFN0c5L86787FoLTnFzDH8kfP5A8PYjqQ=</latexit><latexit sha1_base64="g+IRNevPc1g3Nk0ZK/Q3r3J8OUI=">AAAB7HicbZBNS8NAEIYn9avWr6pHL8EieCqJCHosevFYwdRCG8pmO2mXbjZhdyKU0t/gxYMiXv1B3vw3btsctPWFhYd3ZtiZN8qkMOR5305pbX1jc6u8XdnZ3ds/qB4etUyaa44BT2Wq2xEzKIXCgARJbGcaWRJJfIxGt7P64xNqI1L1QOMMw4QNlIgFZ2StoBshsV615tW9udxV8AuoQaFmr/rV7ac8T1ARl8yYju9lFE6YJsElTivd3GDG+IgNsGNRsQRNOJkvO3XPrNN341Tbp8idu78nJiwxZpxEtjNhNDTLtZn5X62TU3wdToTKckLFFx/FuXQpdWeXu32hkZMcW2BcC7ury4dMM042n4oNwV8+eRVaF3Xf8v1lrXFTxFGGEziFc/DhChpwB00IgIOAZ3iFN0c5L86787FoLTnFzDH8kfP5A8PYjqQ=</latexit><latexit sha1_base64="g+IRNevPc1g3Nk0ZK/Q3r3J8OUI=">AAAB7HicbZBNS8NAEIYn9avWr6pHL8EieCqJCHosevFYwdRCG8pmO2mXbjZhdyKU0t/gxYMiXv1B3vw3btsctPWFhYd3ZtiZN8qkMOR5305pbX1jc6u8XdnZ3ds/qB4etUyaa44BT2Wq2xEzKIXCgARJbGcaWRJJfIxGt7P64xNqI1L1QOMMw4QNlIgFZ2StoBshsV615tW9udxV8AuoQaFmr/rV7ac8T1ARl8yYju9lFE6YJsElTivd3GDG+IgNsGNRsQRNOJkvO3XPrNN341Tbp8idu78nJiwxZpxEtjNhNDTLtZn5X62TU3wdToTKckLFFx/FuXQpdWeXu32hkZMcW2BcC7ury4dMM042n4oNwV8+eRVaF3Xf8v1lrXFTxFGGEziFc/DhChpwB00IgIOAZ3iFN0c5L86787FoLTnFzDH8kfP5A8PYjqQ=</latexit><latexit sha1_base64="g+IRNevPc1g3Nk0ZK/Q3r3J8OUI=">AAAB7HicbZBNS8NAEIYn9avWr6pHL8EieCqJCHosevFYwdRCG8pmO2mXbjZhdyKU0t/gxYMiXv1B3vw3btsctPWFhYd3ZtiZN8qkMOR5305pbX1jc6u8XdnZ3ds/qB4etUyaa44BT2Wq2xEzKIXCgARJbGcaWRJJfIxGt7P64xNqI1L1QOMMw4QNlIgFZ2StoBshsV615tW9udxV8AuoQaFmr/rV7ac8T1ARl8yYju9lFE6YJsElTivd3GDG+IgNsGNRsQRNOJkvO3XPrNN341Tbp8idu78nJiwxZpxEtjNhNDTLtZn5X62TU3wdToTKckLFFx/FuXQpdWeXu32hkZMcW2BcC7ury4dMM042n4oNwV8+eRVaF3Xf8v1lrXFTxFGGEziFc/DhChpwB00IgIOAZ3iFN0c5L86787FoLTnFzDH8kfP5A8PYjqQ=</latexit>

↵
<latexit sha1_base64="6EtSdgpKG2qhZbjGZ4j1QLtQzns=">AAAB7XicbZBNSwMxEIZn61etX1WPXoJF8FR2RdBj0YvHCvYD2qXMpmkbm02WJCuUpf/BiwdFvPp/vPlvTNs9aOsLgYd3ZsjMGyWCG+v7315hbX1jc6u4XdrZ3ds/KB8eNY1KNWUNqoTS7QgNE1yyhuVWsHaiGcaRYK1ofDurt56YNlzJBztJWBjjUPIBp2id1eyiSEbYK1f8qj8XWYUghwrkqvfKX92+omnMpKUCjekEfmLDDLXlVLBpqZsaliAd45B1HEqMmQmz+bZTcuacPhko7Z60ZO7+nsgwNmYSR64zRjsyy7WZ+V+tk9rBdZhxmaSWSbr4aJAKYhWZnU76XDNqxcQBUs3droSOUCO1LqCSCyFYPnkVmhfVwPH9ZaV2k8dRhBM4hXMI4ApqcAd1aACFR3iGV3jzlPfivXsfi9aCl88cwx95nz+Lg48Y</latexit><latexit sha1_base64="6EtSdgpKG2qhZbjGZ4j1QLtQzns=">AAAB7XicbZBNSwMxEIZn61etX1WPXoJF8FR2RdBj0YvHCvYD2qXMpmkbm02WJCuUpf/BiwdFvPp/vPlvTNs9aOsLgYd3ZsjMGyWCG+v7315hbX1jc6u4XdrZ3ds/KB8eNY1KNWUNqoTS7QgNE1yyhuVWsHaiGcaRYK1ofDurt56YNlzJBztJWBjjUPIBp2id1eyiSEbYK1f8qj8XWYUghwrkqvfKX92+omnMpKUCjekEfmLDDLXlVLBpqZsaliAd45B1HEqMmQmz+bZTcuacPhko7Z60ZO7+nsgwNmYSR64zRjsyy7WZ+V+tk9rBdZhxmaSWSbr4aJAKYhWZnU76XDNqxcQBUs3droSOUCO1LqCSCyFYPnkVmhfVwPH9ZaV2k8dRhBM4hXMI4ApqcAd1aACFR3iGV3jzlPfivXsfi9aCl88cwx95nz+Lg48Y</latexit><latexit sha1_base64="6EtSdgpKG2qhZbjGZ4j1QLtQzns=">AAAB7XicbZBNSwMxEIZn61etX1WPXoJF8FR2RdBj0YvHCvYD2qXMpmkbm02WJCuUpf/BiwdFvPp/vPlvTNs9aOsLgYd3ZsjMGyWCG+v7315hbX1jc6u4XdrZ3ds/KB8eNY1KNWUNqoTS7QgNE1yyhuVWsHaiGcaRYK1ofDurt56YNlzJBztJWBjjUPIBp2id1eyiSEbYK1f8qj8XWYUghwrkqvfKX92+omnMpKUCjekEfmLDDLXlVLBpqZsaliAd45B1HEqMmQmz+bZTcuacPhko7Z60ZO7+nsgwNmYSR64zRjsyy7WZ+V+tk9rBdZhxmaSWSbr4aJAKYhWZnU76XDNqxcQBUs3droSOUCO1LqCSCyFYPnkVmhfVwPH9ZaV2k8dRhBM4hXMI4ApqcAd1aACFR3iGV3jzlPfivXsfi9aCl88cwx95nz+Lg48Y</latexit><latexit sha1_base64="6EtSdgpKG2qhZbjGZ4j1QLtQzns=">AAAB7XicbZBNSwMxEIZn61etX1WPXoJF8FR2RdBj0YvHCvYD2qXMpmkbm02WJCuUpf/BiwdFvPp/vPlvTNs9aOsLgYd3ZsjMGyWCG+v7315hbX1jc6u4XdrZ3ds/KB8eNY1KNWUNqoTS7QgNE1yyhuVWsHaiGcaRYK1ofDurt56YNlzJBztJWBjjUPIBp2id1eyiSEbYK1f8qj8XWYUghwrkqvfKX92+omnMpKUCjekEfmLDDLXlVLBpqZsaliAd45B1HEqMmQmz+bZTcuacPhko7Z60ZO7+nsgwNmYSR64zRjsyy7WZ+V+tk9rBdZhxmaSWSbr4aJAKYhWZnU76XDNqxcQBUs3droSOUCO1LqCSCyFYPnkVmhfVwPH9ZaV2k8dRhBM4hXMI4ApqcAd1aACFR3iGV3jzlPfivXsfi9aCl88cwx95nz+Lg48Y</latexit>

w̄ 2 R2d
<latexit sha1_base64="jY03p1o5sw9rDrdRZjzDG7ij0bA=">AAACDnicbZC7TsMwFIYdrqXcCowsFlUlpipBSDBWsDAWRC9SUyrbdVoLx4nsE6CK8gQsvAoLAwixMrPxNjhtB2j5JUuf/nOOfM5PYykMuO63s7C4tLyyWlgrrm9sbm2XdnabJko04w0WyUi3KTFcCsUbIEDydqw5CankLXp7ntdbd1wbEalrGMW8G5KBEoFgBKzVK1V8SnTqA38AGqT3WYZ9obAfEhhSml5lN+lRPyv2SmW36o6F58GbQhlNVe+Vvvx+xJKQK2CSGNPx3Bi6KdEgmORZ0U8Mjwm7JQPesahIyE03HZ+T4Yp1+jiItH0K8Nj9PZGS0JhRSG1nvqeZreXmf7VOAsFpNxUqToArNvkoSCSGCOfZ4L7QnIEcWSBMC7srZkOiCQObYB6CN3vyPDSPqp7ly+Ny7WwaRwHtowN0iDx0gmroAtVRAzH0iJ7RK3pznpwX5935mLQuONOZPfRHzucPs+Gcew==</latexit><latexit sha1_base64="jY03p1o5sw9rDrdRZjzDG7ij0bA=">AAACDnicbZC7TsMwFIYdrqXcCowsFlUlpipBSDBWsDAWRC9SUyrbdVoLx4nsE6CK8gQsvAoLAwixMrPxNjhtB2j5JUuf/nOOfM5PYykMuO63s7C4tLyyWlgrrm9sbm2XdnabJko04w0WyUi3KTFcCsUbIEDydqw5CankLXp7ntdbd1wbEalrGMW8G5KBEoFgBKzVK1V8SnTqA38AGqT3WYZ9obAfEhhSml5lN+lRPyv2SmW36o6F58GbQhlNVe+Vvvx+xJKQK2CSGNPx3Bi6KdEgmORZ0U8Mjwm7JQPesahIyE03HZ+T4Yp1+jiItH0K8Nj9PZGS0JhRSG1nvqeZreXmf7VOAsFpNxUqToArNvkoSCSGCOfZ4L7QnIEcWSBMC7srZkOiCQObYB6CN3vyPDSPqp7ly+Ny7WwaRwHtowN0iDx0gmroAtVRAzH0iJ7RK3pznpwX5935mLQuONOZPfRHzucPs+Gcew==</latexit><latexit sha1_base64="jY03p1o5sw9rDrdRZjzDG7ij0bA=">AAACDnicbZC7TsMwFIYdrqXcCowsFlUlpipBSDBWsDAWRC9SUyrbdVoLx4nsE6CK8gQsvAoLAwixMrPxNjhtB2j5JUuf/nOOfM5PYykMuO63s7C4tLyyWlgrrm9sbm2XdnabJko04w0WyUi3KTFcCsUbIEDydqw5CankLXp7ntdbd1wbEalrGMW8G5KBEoFgBKzVK1V8SnTqA38AGqT3WYZ9obAfEhhSml5lN+lRPyv2SmW36o6F58GbQhlNVe+Vvvx+xJKQK2CSGNPx3Bi6KdEgmORZ0U8Mjwm7JQPesahIyE03HZ+T4Yp1+jiItH0K8Nj9PZGS0JhRSG1nvqeZreXmf7VOAsFpNxUqToArNvkoSCSGCOfZ4L7QnIEcWSBMC7srZkOiCQObYB6CN3vyPDSPqp7ly+Ny7WwaRwHtowN0iDx0gmroAtVRAzH0iJ7RK3pznpwX5935mLQuONOZPfRHzucPs+Gcew==</latexit><latexit sha1_base64="jY03p1o5sw9rDrdRZjzDG7ij0bA=">AAACDnicbZC7TsMwFIYdrqXcCowsFlUlpipBSDBWsDAWRC9SUyrbdVoLx4nsE6CK8gQsvAoLAwixMrPxNjhtB2j5JUuf/nOOfM5PYykMuO63s7C4tLyyWlgrrm9sbm2XdnabJko04w0WyUi3KTFcCsUbIEDydqw5CankLXp7ntdbd1wbEalrGMW8G5KBEoFgBKzVK1V8SnTqA38AGqT3WYZ9obAfEhhSml5lN+lRPyv2SmW36o6F58GbQhlNVe+Vvvx+xJKQK2CSGNPx3Bi6KdEgmORZ0U8Mjwm7JQPesahIyE03HZ+T4Yp1+jiItH0K8Nj9PZGS0JhRSG1nvqeZreXmf7VOAsFpNxUqToArNvkoSCSGCOfZ4L7QnIEcWSBMC7srZkOiCQObYB6CN3vyPDSPqp7ly+Ny7WwaRwHtowN0iDx0gmroAtVRAzH0iJ7RK3pznpwX5935mLQuONOZPfRHzucPs+Gcew==</latexit>

w 2 Rd
<latexit sha1_base64="t1Tk9+lc9t2k38t/R5ojsRj9v44=">AAACB3icbZDLSsNAFIYn9VbrLepSkMEiuCqJCLosunFZxV6gqWUynbRDJ5Mwc6KWkJ0bX8WNC0Xc+grufBsnbRfa+sPAx3/OYc75/VhwDY7zbRUWFpeWV4qrpbX1jc0te3unoaNEUVankYhUyyeaCS5ZHTgI1ooVI6EvWNMfXuT15h1TmkfyBkYx64SkL3nAKQFjde19D9gD+EF6n2GPS+yFBAa+n15nt2kvK3XtslNxxsLz4E6hjKaqde0vrxfRJGQSqCBat10nhk5KFHAqWFbyEs1iQoekz9oGJQmZ7qTjOzJ8aJweDiJlngQ8dn9PpCTUehT6pjNfU8/WcvO/WjuB4KyTchknwCSdfBQkAkOE81BwjytGQYwMEKq42RXTAVGEgokuD8GdPXkeGscV1/DVSbl6Po2jiPbQATpCLjpFVXSJaqiOKHpEz+gVvVlP1ov1bn1MWgvWdGYX/ZH1+QMsJJl6</latexit><latexit sha1_base64="t1Tk9+lc9t2k38t/R5ojsRj9v44=">AAACB3icbZDLSsNAFIYn9VbrLepSkMEiuCqJCLosunFZxV6gqWUynbRDJ5Mwc6KWkJ0bX8WNC0Xc+grufBsnbRfa+sPAx3/OYc75/VhwDY7zbRUWFpeWV4qrpbX1jc0te3unoaNEUVankYhUyyeaCS5ZHTgI1ooVI6EvWNMfXuT15h1TmkfyBkYx64SkL3nAKQFjde19D9gD+EF6n2GPS+yFBAa+n15nt2kvK3XtslNxxsLz4E6hjKaqde0vrxfRJGQSqCBat10nhk5KFHAqWFbyEs1iQoekz9oGJQmZ7qTjOzJ8aJweDiJlngQ8dn9PpCTUehT6pjNfU8/WcvO/WjuB4KyTchknwCSdfBQkAkOE81BwjytGQYwMEKq42RXTAVGEgokuD8GdPXkeGscV1/DVSbl6Po2jiPbQATpCLjpFVXSJaqiOKHpEz+gVvVlP1ov1bn1MWgvWdGYX/ZH1+QMsJJl6</latexit><latexit sha1_base64="t1Tk9+lc9t2k38t/R5ojsRj9v44=">AAACB3icbZDLSsNAFIYn9VbrLepSkMEiuCqJCLosunFZxV6gqWUynbRDJ5Mwc6KWkJ0bX8WNC0Xc+grufBsnbRfa+sPAx3/OYc75/VhwDY7zbRUWFpeWV4qrpbX1jc0te3unoaNEUVankYhUyyeaCS5ZHTgI1ooVI6EvWNMfXuT15h1TmkfyBkYx64SkL3nAKQFjde19D9gD+EF6n2GPS+yFBAa+n15nt2kvK3XtslNxxsLz4E6hjKaqde0vrxfRJGQSqCBat10nhk5KFHAqWFbyEs1iQoekz9oGJQmZ7qTjOzJ8aJweDiJlngQ8dn9PpCTUehT6pjNfU8/WcvO/WjuB4KyTchknwCSdfBQkAkOE81BwjytGQYwMEKq42RXTAVGEgokuD8GdPXkeGscV1/DVSbl6Po2jiPbQATpCLjpFVXSJaqiOKHpEz+gVvVlP1ov1bn1MWgvWdGYX/ZH1+QMsJJl6</latexit><latexit sha1_base64="t1Tk9+lc9t2k38t/R5ojsRj9v44=">AAACB3icbZDLSsNAFIYn9VbrLepSkMEiuCqJCLosunFZxV6gqWUynbRDJ5Mwc6KWkJ0bX8WNC0Xc+grufBsnbRfa+sPAx3/OYc75/VhwDY7zbRUWFpeWV4qrpbX1jc0te3unoaNEUVankYhUyyeaCS5ZHTgI1ooVI6EvWNMfXuT15h1TmkfyBkYx64SkL3nAKQFjde19D9gD+EF6n2GPS+yFBAa+n15nt2kvK3XtslNxxsLz4E6hjKaqde0vrxfRJGQSqCBat10nhk5KFHAqWFbyEs1iQoekz9oGJQmZ7qTjOzJ8aJweDiJlngQ8dn9PpCTUehT6pjNfU8/WcvO/WjuB4KyTchknwCSdfBQkAkOE81BwjytGQYwMEKq42RXTAVGEgokuD8GdPXkeGscV1/DVSbl6Po2jiPbQATpCLjpFVXSJaqiOKHpEz+gVvVlP1ov1bn1MWgvWdGYX/ZH1+QMsJJl6</latexit>

�!
h1

<latexit sha1_base64="pY+H/G6fjI1/QDz+iXwsXVR/7UE=">AAACAHicbZDLSsNAFIYn9VbrLerChZvBIrgqiQi6LLpxWcFeoA1hMp20QyeZMHOilJCNr+LGhSJufQx3vo2TNgttPTDw8f/ncOb8QSK4Bsf5tiorq2vrG9XN2tb2zu6evX/Q0TJVlLWpFFL1AqKZ4DFrAwfBeoliJAoE6waTm8LvPjCluYzvYZowLyKjmIecEjCSbx8NpLEVH42BKCUfs7GfuXle8+2603BmhZfBLaGOymr59tdgKGkasRioIFr3XScBLyMKOBUsrw1SzRJCJ2TE+gZjEjHtZbMDcnxqlCEOpTIvBjxTf09kJNJ6GgWmMyIw1oteIf7n9VMIr7yMx0kKLKbzRWEqMEhcpIGHXDEKYmqAUMXNXzEdE0UomMyKENzFk5ehc95wDd9d1JvXZRxVdIxO0Bly0SVqolvUQm1EUY6e0St6s56sF+vd+pi3Vqxy5hD9KevzB2PoluQ=</latexit><latexit sha1_base64="pY+H/G6fjI1/QDz+iXwsXVR/7UE=">AAACAHicbZDLSsNAFIYn9VbrLerChZvBIrgqiQi6LLpxWcFeoA1hMp20QyeZMHOilJCNr+LGhSJufQx3vo2TNgttPTDw8f/ncOb8QSK4Bsf5tiorq2vrG9XN2tb2zu6evX/Q0TJVlLWpFFL1AqKZ4DFrAwfBeoliJAoE6waTm8LvPjCluYzvYZowLyKjmIecEjCSbx8NpLEVH42BKCUfs7GfuXle8+2603BmhZfBLaGOymr59tdgKGkasRioIFr3XScBLyMKOBUsrw1SzRJCJ2TE+gZjEjHtZbMDcnxqlCEOpTIvBjxTf09kJNJ6GgWmMyIw1oteIf7n9VMIr7yMx0kKLKbzRWEqMEhcpIGHXDEKYmqAUMXNXzEdE0UomMyKENzFk5ehc95wDd9d1JvXZRxVdIxO0Bly0SVqolvUQm1EUY6e0St6s56sF+vd+pi3Vqxy5hD9KevzB2PoluQ=</latexit><latexit sha1_base64="pY+H/G6fjI1/QDz+iXwsXVR/7UE=">AAACAHicbZDLSsNAFIYn9VbrLerChZvBIrgqiQi6LLpxWcFeoA1hMp20QyeZMHOilJCNr+LGhSJufQx3vo2TNgttPTDw8f/ncOb8QSK4Bsf5tiorq2vrG9XN2tb2zu6evX/Q0TJVlLWpFFL1AqKZ4DFrAwfBeoliJAoE6waTm8LvPjCluYzvYZowLyKjmIecEjCSbx8NpLEVH42BKCUfs7GfuXle8+2603BmhZfBLaGOymr59tdgKGkasRioIFr3XScBLyMKOBUsrw1SzRJCJ2TE+gZjEjHtZbMDcnxqlCEOpTIvBjxTf09kJNJ6GgWmMyIw1oteIf7n9VMIr7yMx0kKLKbzRWEqMEhcpIGHXDEKYmqAUMXNXzEdE0UomMyKENzFk5ehc95wDd9d1JvXZRxVdIxO0Bly0SVqolvUQm1EUY6e0St6s56sF+vd+pi3Vqxy5hD9KevzB2PoluQ=</latexit><latexit sha1_base64="pY+H/G6fjI1/QDz+iXwsXVR/7UE=">AAACAHicbZDLSsNAFIYn9VbrLerChZvBIrgqiQi6LLpxWcFeoA1hMp20QyeZMHOilJCNr+LGhSJufQx3vo2TNgttPTDw8f/ncOb8QSK4Bsf5tiorq2vrG9XN2tb2zu6evX/Q0TJVlLWpFFL1AqKZ4DFrAwfBeoliJAoE6waTm8LvPjCluYzvYZowLyKjmIecEjCSbx8NpLEVH42BKCUfs7GfuXle8+2603BmhZfBLaGOymr59tdgKGkasRioIFr3XScBLyMKOBUsrw1SzRJCJ2TE+gZjEjHtZbMDcnxqlCEOpTIvBjxTf09kJNJ6GgWmMyIw1oteIf7n9VMIr7yMx0kKLKbzRWEqMEhcpIGHXDEKYmqAUMXNXzEdE0UomMyKENzFk5ehc95wDd9d1JvXZRxVdIxO0Bly0SVqolvUQm1EUY6e0St6s56sF+vd+pi3Vqxy5hD9KevzB2PoluQ=</latexit>

�!
h2

<latexit sha1_base64="MkrdTSbOLM7lBlaJfnT54VloRTY=">AAACAHicbZDLSsNAFIYnXmu9RV24cBMsgquSFEGXRTcuK9gLtCFMppN26GQmzJwoJWTjq7hxoYhbH8Odb+OkzUJbDwx8/P85nDl/mHCmwXW/rZXVtfWNzcpWdXtnd2/fPjjsaJkqQttEcql6IdaUM0HbwIDTXqIojkNOu+HkpvC7D1RpJsU9TBPqx3gkWMQIBiMF9vFAGlux0RiwUvIxGwdZI8+rgV1z6+6snGXwSqihslqB/TUYSpLGVADhWOu+5ybgZ1gBI5zm1UGqaYLJBI9o36DAMdV+Njsgd86MMnQiqcwT4MzU3xMZjrWexqHpjDGM9aJXiP95/RSiKz9jIkmBCjJfFKXcAekUaThDpigBPjWAiWLmrw4ZY4UJmMyKELzFk5eh06h7hu8uas3rMo4KOkGn6Bx56BI10S1qoTYiKEfP6BW9WU/Wi/VufcxbV6xy5gj9KevzB2VvluU=</latexit><latexit sha1_base64="MkrdTSbOLM7lBlaJfnT54VloRTY=">AAACAHicbZDLSsNAFIYnXmu9RV24cBMsgquSFEGXRTcuK9gLtCFMppN26GQmzJwoJWTjq7hxoYhbH8Odb+OkzUJbDwx8/P85nDl/mHCmwXW/rZXVtfWNzcpWdXtnd2/fPjjsaJkqQttEcql6IdaUM0HbwIDTXqIojkNOu+HkpvC7D1RpJsU9TBPqx3gkWMQIBiMF9vFAGlux0RiwUvIxGwdZI8+rgV1z6+6snGXwSqihslqB/TUYSpLGVADhWOu+5ybgZ1gBI5zm1UGqaYLJBI9o36DAMdV+Njsgd86MMnQiqcwT4MzU3xMZjrWexqHpjDGM9aJXiP95/RSiKz9jIkmBCjJfFKXcAekUaThDpigBPjWAiWLmrw4ZY4UJmMyKELzFk5eh06h7hu8uas3rMo4KOkGn6Bx56BI10S1qoTYiKEfP6BW9WU/Wi/VufcxbV6xy5gj9KevzB2VvluU=</latexit><latexit sha1_base64="MkrdTSbOLM7lBlaJfnT54VloRTY=">AAACAHicbZDLSsNAFIYnXmu9RV24cBMsgquSFEGXRTcuK9gLtCFMppN26GQmzJwoJWTjq7hxoYhbH8Odb+OkzUJbDwx8/P85nDl/mHCmwXW/rZXVtfWNzcpWdXtnd2/fPjjsaJkqQttEcql6IdaUM0HbwIDTXqIojkNOu+HkpvC7D1RpJsU9TBPqx3gkWMQIBiMF9vFAGlux0RiwUvIxGwdZI8+rgV1z6+6snGXwSqihslqB/TUYSpLGVADhWOu+5ybgZ1gBI5zm1UGqaYLJBI9o36DAMdV+Njsgd86MMnQiqcwT4MzU3xMZjrWexqHpjDGM9aJXiP95/RSiKz9jIkmBCjJfFKXcAekUaThDpigBPjWAiWLmrw4ZY4UJmMyKELzFk5eh06h7hu8uas3rMo4KOkGn6Bx56BI10S1qoTYiKEfP6BW9WU/Wi/VufcxbV6xy5gj9KevzB2VvluU=</latexit><latexit sha1_base64="MkrdTSbOLM7lBlaJfnT54VloRTY=">AAACAHicbZDLSsNAFIYnXmu9RV24cBMsgquSFEGXRTcuK9gLtCFMppN26GQmzJwoJWTjq7hxoYhbH8Odb+OkzUJbDwx8/P85nDl/mHCmwXW/rZXVtfWNzcpWdXtnd2/fPjjsaJkqQttEcql6IdaUM0HbwIDTXqIojkNOu+HkpvC7D1RpJsU9TBPqx3gkWMQIBiMF9vFAGlux0RiwUvIxGwdZI8+rgV1z6+6snGXwSqihslqB/TUYSpLGVADhWOu+5ybgZ1gBI5zm1UGqaYLJBI9o36DAMdV+Njsgd86MMnQiqcwT4MzU3xMZjrWexqHpjDGM9aJXiP95/RSiKz9jIkmBCjJfFKXcAekUaThDpigBPjWAiWLmrw4ZY4UJmMyKELzFk5eh06h7hu8uas3rMo4KOkGn6Bx56BI10S1qoTYiKEfP6BW9WU/Wi/VufcxbV6xy5gj9KevzB2VvluU=</latexit>

�!
h3

<latexit sha1_base64="5z16+6TLW3Cj+bwmadZzQDMGNeE=">AAACAHicbZDNSsNAFIVv6l+tf1UXLtwEi+CqJCrosujGZQVbC20Ik+mkGTqZCTMTpYRsfBU3LhRx62O4822ctFlo64WBj3Pu5c49QcKo0o7zbVWWlldW16rrtY3Nre2d+u5eV4lUYtLBggnZC5AijHLS0VQz0kskQXHAyH0wvi78+wciFRX8Tk8S4sVoxGlIMdJG8usHA2FsSUeRRlKKxyzys7M8r/n1htN0pmUvgltCA8pq+/WvwVDgNCZcY4aU6rtOor0MSU0xI3ltkCqSIDxGI9I3yFFMlJdND8jtY6MM7VBI87i2p+rviQzFSk3iwHTGSEdq3ivE/7x+qsNLL6M8STXheLYoTJmthV2kYQ+pJFiziQGEJTV/tXGEJMLaZFaE4M6fvAjd06Zr+Pa80boq46jCIRzBCbhwAS24gTZ0AEMOz/AKb9aT9WK9Wx+z1opVzuzDn7I+fwBm9pbm</latexit><latexit sha1_base64="5z16+6TLW3Cj+bwmadZzQDMGNeE=">AAACAHicbZDNSsNAFIVv6l+tf1UXLtwEi+CqJCrosujGZQVbC20Ik+mkGTqZCTMTpYRsfBU3LhRx62O4822ctFlo64WBj3Pu5c49QcKo0o7zbVWWlldW16rrtY3Nre2d+u5eV4lUYtLBggnZC5AijHLS0VQz0kskQXHAyH0wvi78+wciFRX8Tk8S4sVoxGlIMdJG8usHA2FsSUeRRlKKxyzys7M8r/n1htN0pmUvgltCA8pq+/WvwVDgNCZcY4aU6rtOor0MSU0xI3ltkCqSIDxGI9I3yFFMlJdND8jtY6MM7VBI87i2p+rviQzFSk3iwHTGSEdq3ivE/7x+qsNLL6M8STXheLYoTJmthV2kYQ+pJFiziQGEJTV/tXGEJMLaZFaE4M6fvAjd06Zr+Pa80boq46jCIRzBCbhwAS24gTZ0AEMOz/AKb9aT9WK9Wx+z1opVzuzDn7I+fwBm9pbm</latexit><latexit sha1_base64="5z16+6TLW3Cj+bwmadZzQDMGNeE=">AAACAHicbZDNSsNAFIVv6l+tf1UXLtwEi+CqJCrosujGZQVbC20Ik+mkGTqZCTMTpYRsfBU3LhRx62O4822ctFlo64WBj3Pu5c49QcKo0o7zbVWWlldW16rrtY3Nre2d+u5eV4lUYtLBggnZC5AijHLS0VQz0kskQXHAyH0wvi78+wciFRX8Tk8S4sVoxGlIMdJG8usHA2FsSUeRRlKKxyzys7M8r/n1htN0pmUvgltCA8pq+/WvwVDgNCZcY4aU6rtOor0MSU0xI3ltkCqSIDxGI9I3yFFMlJdND8jtY6MM7VBI87i2p+rviQzFSk3iwHTGSEdq3ivE/7x+qsNLL6M8STXheLYoTJmthV2kYQ+pJFiziQGEJTV/tXGEJMLaZFaE4M6fvAjd06Zr+Pa80boq46jCIRzBCbhwAS24gTZ0AEMOz/AKb9aT9WK9Wx+z1opVzuzDn7I+fwBm9pbm</latexit><latexit sha1_base64="5z16+6TLW3Cj+bwmadZzQDMGNeE=">AAACAHicbZDNSsNAFIVv6l+tf1UXLtwEi+CqJCrosujGZQVbC20Ik+mkGTqZCTMTpYRsfBU3LhRx62O4822ctFlo64WBj3Pu5c49QcKo0o7zbVWWlldW16rrtY3Nre2d+u5eV4lUYtLBggnZC5AijHLS0VQz0kskQXHAyH0wvi78+wciFRX8Tk8S4sVoxGlIMdJG8usHA2FsSUeRRlKKxyzys7M8r/n1htN0pmUvgltCA8pq+/WvwVDgNCZcY4aU6rtOor0MSU0xI3ltkCqSIDxGI9I3yFFMlJdND8jtY6MM7VBI87i2p+rviQzFSk3iwHTGSEdq3ivE/7x+qsNLL6M8STXheLYoTJmthV2kYQ+pJFiziQGEJTV/tXGEJMLaZFaE4M6fvAjd06Zr+Pa80boq46jCIRzBCbhwAS24gTZ0AEMOz/AKb9aT9WK9Wx+z1opVzuzDn7I+fwBm9pbm</latexit>

�!
h4

<latexit sha1_base64="OeZ7k/EdDasVa3MfuTkUqjfBBAg=">AAACAHicbZDLSsNAFIZPvNZ6i7pw4WawCK5KIgVdFt24rGAv0IYwmU7aoZNMmJkoJWTjq7hxoYhbH8Odb+OkzUJbDwx8/P85nDl/kHCmtON8Wyura+sbm5Wt6vbO7t6+fXDYUSKVhLaJ4EL2AqwoZzFta6Y57SWS4ijgtBtMbgq/+0ClYiK+19OEehEexSxkBGsj+fbxQBhbstFYYynFYzb2s0aeV3275tSdWaFlcEuoQVkt3/4aDAVJIxprwrFSfddJtJdhqRnhNK8OUkUTTCZ4RPsGYxxR5WWzA3J0ZpQhCoU0L9Zopv6eyHCk1DQKTGeE9VgteoX4n9dPdXjlZSxOUk1jMl8UphxpgYo00JBJSjSfGsBEMvNXRMZYYqJNZkUI7uLJy9C5qLuG7xq15nUZRwVO4BTOwYVLaMIttKANBHJ4hld4s56sF+vd+pi3rljlzBH8KevzB2h9luc=</latexit><latexit sha1_base64="OeZ7k/EdDasVa3MfuTkUqjfBBAg=">AAACAHicbZDLSsNAFIZPvNZ6i7pw4WawCK5KIgVdFt24rGAv0IYwmU7aoZNMmJkoJWTjq7hxoYhbH8Odb+OkzUJbDwx8/P85nDl/kHCmtON8Wyura+sbm5Wt6vbO7t6+fXDYUSKVhLaJ4EL2AqwoZzFta6Y57SWS4ijgtBtMbgq/+0ClYiK+19OEehEexSxkBGsj+fbxQBhbstFYYynFYzb2s0aeV3275tSdWaFlcEuoQVkt3/4aDAVJIxprwrFSfddJtJdhqRnhNK8OUkUTTCZ4RPsGYxxR5WWzA3J0ZpQhCoU0L9Zopv6eyHCk1DQKTGeE9VgteoX4n9dPdXjlZSxOUk1jMl8UphxpgYo00JBJSjSfGsBEMvNXRMZYYqJNZkUI7uLJy9C5qLuG7xq15nUZRwVO4BTOwYVLaMIttKANBHJ4hld4s56sF+vd+pi3rljlzBH8KevzB2h9luc=</latexit><latexit sha1_base64="OeZ7k/EdDasVa3MfuTkUqjfBBAg=">AAACAHicbZDLSsNAFIZPvNZ6i7pw4WawCK5KIgVdFt24rGAv0IYwmU7aoZNMmJkoJWTjq7hxoYhbH8Odb+OkzUJbDwx8/P85nDl/kHCmtON8Wyura+sbm5Wt6vbO7t6+fXDYUSKVhLaJ4EL2AqwoZzFta6Y57SWS4ijgtBtMbgq/+0ClYiK+19OEehEexSxkBGsj+fbxQBhbstFYYynFYzb2s0aeV3275tSdWaFlcEuoQVkt3/4aDAVJIxprwrFSfddJtJdhqRnhNK8OUkUTTCZ4RPsGYxxR5WWzA3J0ZpQhCoU0L9Zopv6eyHCk1DQKTGeE9VgteoX4n9dPdXjlZSxOUk1jMl8UphxpgYo00JBJSjSfGsBEMvNXRMZYYqJNZkUI7uLJy9C5qLuG7xq15nUZRwVO4BTOwYVLaMIttKANBHJ4hld4s56sF+vd+pi3rljlzBH8KevzB2h9luc=</latexit><latexit sha1_base64="OeZ7k/EdDasVa3MfuTkUqjfBBAg=">AAACAHicbZDLSsNAFIZPvNZ6i7pw4WawCK5KIgVdFt24rGAv0IYwmU7aoZNMmJkoJWTjq7hxoYhbH8Odb+OkzUJbDwx8/P85nDl/kHCmtON8Wyura+sbm5Wt6vbO7t6+fXDYUSKVhLaJ4EL2AqwoZzFta6Y57SWS4ijgtBtMbgq/+0ClYiK+19OEehEexSxkBGsj+fbxQBhbstFYYynFYzb2s0aeV3275tSdWaFlcEuoQVkt3/4aDAVJIxprwrFSfddJtJdhqRnhNK8OUkUTTCZ4RPsGYxxR5WWzA3J0ZpQhCoU0L9Zopv6eyHCk1DQKTGeE9VgteoX4n9dPdXjlZSxOUk1jMl8UphxpgYo00JBJSjSfGsBEMvNXRMZYYqJNZkUI7uLJy9C5qLuG7xq15nUZRwVO4BTOwYVLaMIttKANBHJ4hld4s56sF+vd+pi3rljlzBH8KevzB2h9luc=</latexit>

�!
h5

<latexit sha1_base64="bvE1peDSPfMqx6Inz6fuUl/anQ0=">AAACAHicbZDNSsNAFIVv6l+tf1UXLtwEi+CqJKLosujGZQVbC20Ik+mkGTqZCTMTpYRsfBU3LhRx62O4822ctFlo64WBj3Pu5c49QcKo0o7zbVWWlldW16rrtY3Nre2d+u5eV4lUYtLBggnZC5AijHLS0VQz0kskQXHAyH0wvi78+wciFRX8Tk8S4sVoxGlIMdJG8usHA2FsSUeRRlKKxyzys/M8r/n1htN0pmUvgltCA8pq+/WvwVDgNCZcY4aU6rtOor0MSU0xI3ltkCqSIDxGI9I3yFFMlJdND8jtY6MM7VBI87i2p+rviQzFSk3iwHTGSEdq3ivE/7x+qsNLL6M8STXheLYoTJmthV2kYQ+pJFiziQGEJTV/tXGEJMLaZFaE4M6fvAjd06Zr+Pas0boq46jCIRzBCbhwAS24gTZ0AEMOz/AKb9aT9WK9Wx+z1opVzuzDn7I+fwBqBJbo</latexit><latexit sha1_base64="bvE1peDSPfMqx6Inz6fuUl/anQ0=">AAACAHicbZDNSsNAFIVv6l+tf1UXLtwEi+CqJKLosujGZQVbC20Ik+mkGTqZCTMTpYRsfBU3LhRx62O4822ctFlo64WBj3Pu5c49QcKo0o7zbVWWlldW16rrtY3Nre2d+u5eV4lUYtLBggnZC5AijHLS0VQz0kskQXHAyH0wvi78+wciFRX8Tk8S4sVoxGlIMdJG8usHA2FsSUeRRlKKxyzys/M8r/n1htN0pmUvgltCA8pq+/WvwVDgNCZcY4aU6rtOor0MSU0xI3ltkCqSIDxGI9I3yFFMlJdND8jtY6MM7VBI87i2p+rviQzFSk3iwHTGSEdq3ivE/7x+qsNLL6M8STXheLYoTJmthV2kYQ+pJFiziQGEJTV/tXGEJMLaZFaE4M6fvAjd06Zr+Pas0boq46jCIRzBCbhwAS24gTZ0AEMOz/AKb9aT9WK9Wx+z1opVzuzDn7I+fwBqBJbo</latexit><latexit sha1_base64="bvE1peDSPfMqx6Inz6fuUl/anQ0=">AAACAHicbZDNSsNAFIVv6l+tf1UXLtwEi+CqJKLosujGZQVbC20Ik+mkGTqZCTMTpYRsfBU3LhRx62O4822ctFlo64WBj3Pu5c49QcKo0o7zbVWWlldW16rrtY3Nre2d+u5eV4lUYtLBggnZC5AijHLS0VQz0kskQXHAyH0wvi78+wciFRX8Tk8S4sVoxGlIMdJG8usHA2FsSUeRRlKKxyzys/M8r/n1htN0pmUvgltCA8pq+/WvwVDgNCZcY4aU6rtOor0MSU0xI3ltkCqSIDxGI9I3yFFMlJdND8jtY6MM7VBI87i2p+rviQzFSk3iwHTGSEdq3ivE/7x+qsNLL6M8STXheLYoTJmthV2kYQ+pJFiziQGEJTV/tXGEJMLaZFaE4M6fvAjd06Zr+Pas0boq46jCIRzBCbhwAS24gTZ0AEMOz/AKb9aT9WK9Wx+z1opVzuzDn7I+fwBqBJbo</latexit><latexit sha1_base64="bvE1peDSPfMqx6Inz6fuUl/anQ0=">AAACAHicbZDNSsNAFIVv6l+tf1UXLtwEi+CqJKLosujGZQVbC20Ik+mkGTqZCTMTpYRsfBU3LhRx62O4822ctFlo64WBj3Pu5c49QcKo0o7zbVWWlldW16rrtY3Nre2d+u5eV4lUYtLBggnZC5AijHLS0VQz0kskQXHAyH0wvi78+wciFRX8Tk8S4sVoxGlIMdJG8usHA2FsSUeRRlKKxyzys/M8r/n1htN0pmUvgltCA8pq+/WvwVDgNCZcY4aU6rtOor0MSU0xI3ltkCqSIDxGI9I3yFFMlJdND8jtY6MM7VBI87i2p+rviQzFSk3iwHTGSEdq3ivE/7x+qsNLL6M8STXheLYoTJmthV2kYQ+pJFiziQGEJTV/tXGEJMLaZFaE4M6fvAjd06Zr+Pas0boq46jCIRzBCbhwAS24gTZ0AEMOz/AKb9aT9WK9Wx+z1opVzuzDn7I+fwBqBJbo</latexit>

 �
h5

<latexit sha1_base64="cQzho9GqXV8kQKoqAVdHtcJsOqs=">AAAB/3icbZDLSsNAFIZP6q3WW1Rw4yZYBFclEUWXRTcuK9gLtKFMppN26GQmzEyUErPwVdy4UMStr+HOt3HSZqGtBwY+/v8czpk/iBlV2nW/rdLS8srqWnm9srG5tb1j7+61lEgkJk0smJCdACnCKCdNTTUjnVgSFAWMtIPxde6374lUVPA7PYmJH6EhpyHFSBupbx/0hLEZCTWSUjyko356nmWVvl11a+60nEXwCqhCUY2+/dUbCJxEhGvMkFJdz421nyKpKWYkq/QSRWKEx2hIugY5iojy0+n9mXNslIETCmke185U/T2RokipSRSYzgjpkZr3cvE/r5vo8NJPKY8TTTieLQoT5mjh5GE4AyoJ1mxiAGFJza0OHiGJsDaR5SF4819ehNZpzTN8e1atXxVxlOEQjuAEPLiAOtxAA5qA4RGe4RXerCfrxXq3PmatJauY2Yc/ZX3+AIzIlms=</latexit><latexit sha1_base64="cQzho9GqXV8kQKoqAVdHtcJsOqs=">AAAB/3icbZDLSsNAFIZP6q3WW1Rw4yZYBFclEUWXRTcuK9gLtKFMppN26GQmzEyUErPwVdy4UMStr+HOt3HSZqGtBwY+/v8czpk/iBlV2nW/rdLS8srqWnm9srG5tb1j7+61lEgkJk0smJCdACnCKCdNTTUjnVgSFAWMtIPxde6374lUVPA7PYmJH6EhpyHFSBupbx/0hLEZCTWSUjyko356nmWVvl11a+60nEXwCqhCUY2+/dUbCJxEhGvMkFJdz421nyKpKWYkq/QSRWKEx2hIugY5iojy0+n9mXNslIETCmke185U/T2RokipSRSYzgjpkZr3cvE/r5vo8NJPKY8TTTieLQoT5mjh5GE4AyoJ1mxiAGFJza0OHiGJsDaR5SF4819ehNZpzTN8e1atXxVxlOEQjuAEPLiAOtxAA5qA4RGe4RXerCfrxXq3PmatJauY2Yc/ZX3+AIzIlms=</latexit><latexit sha1_base64="cQzho9GqXV8kQKoqAVdHtcJsOqs=">AAAB/3icbZDLSsNAFIZP6q3WW1Rw4yZYBFclEUWXRTcuK9gLtKFMppN26GQmzEyUErPwVdy4UMStr+HOt3HSZqGtBwY+/v8czpk/iBlV2nW/rdLS8srqWnm9srG5tb1j7+61lEgkJk0smJCdACnCKCdNTTUjnVgSFAWMtIPxde6374lUVPA7PYmJH6EhpyHFSBupbx/0hLEZCTWSUjyko356nmWVvl11a+60nEXwCqhCUY2+/dUbCJxEhGvMkFJdz421nyKpKWYkq/QSRWKEx2hIugY5iojy0+n9mXNslIETCmke185U/T2RokipSRSYzgjpkZr3cvE/r5vo8NJPKY8TTTieLQoT5mjh5GE4AyoJ1mxiAGFJza0OHiGJsDaR5SF4819ehNZpzTN8e1atXxVxlOEQjuAEPLiAOtxAA5qA4RGe4RXerCfrxXq3PmatJauY2Yc/ZX3+AIzIlms=</latexit><latexit sha1_base64="cQzho9GqXV8kQKoqAVdHtcJsOqs=">AAAB/3icbZDLSsNAFIZP6q3WW1Rw4yZYBFclEUWXRTcuK9gLtKFMppN26GQmzEyUErPwVdy4UMStr+HOt3HSZqGtBwY+/v8czpk/iBlV2nW/rdLS8srqWnm9srG5tb1j7+61lEgkJk0smJCdACnCKCdNTTUjnVgSFAWMtIPxde6374lUVPA7PYmJH6EhpyHFSBupbx/0hLEZCTWSUjyko356nmWVvl11a+60nEXwCqhCUY2+/dUbCJxEhGvMkFJdz421nyKpKWYkq/QSRWKEx2hIugY5iojy0+n9mXNslIETCmke185U/T2RokipSRSYzgjpkZr3cvE/r5vo8NJPKY8TTTieLQoT5mjh5GE4AyoJ1mxiAGFJza0OHiGJsDaR5SF4819ehNZpzTN8e1atXxVxlOEQjuAEPLiAOtxAA5qA4RGe4RXerCfrxXq3PmatJauY2Yc/ZX3+AIzIlms=</latexit>

 �
h4

<latexit sha1_base64="+FjQcwobEn1xH4s0LKTcEHEej0o=">AAAB/3icbZDLSsNAFIZP6q3WW1Rw42awCK5KIgVdFt24rGAv0IYymU7aoZNMmJkoJWbhq7hxoYhbX8Odb+OkzUJbDwx8/P85nDO/H3OmtON8W6WV1bX1jfJmZWt7Z3fP3j9oK5FIQltEcCG7PlaUs4i2NNOcdmNJcehz2vEn17nfuadSMRHd6WlMvRCPIhYwgrWRBvZRXxib00BjKcVDOh6k9SyrDOyqU3NmhZbBLaAKRTUH9ld/KEgS0kgTjpXquU6svRRLzQinWaWfKBpjMsEj2jMY4ZAqL53dn6FTowxRIKR5kUYz9fdEikOlpqFvOkOsx2rRy8X/vF6ig0svZVGcaBqR+aIg4UgLlIeBhkxSovnUACaSmVsRGWOJiTaR5SG4i19ehvZ5zTV8W682roo4ynAMJ3AGLlxAA26gCS0g8AjP8Apv1pP1Yr1bH/PWklXMHMKfsj5/AItBlmo=</latexit><latexit sha1_base64="+FjQcwobEn1xH4s0LKTcEHEej0o=">AAAB/3icbZDLSsNAFIZP6q3WW1Rw42awCK5KIgVdFt24rGAv0IYymU7aoZNMmJkoJWbhq7hxoYhbX8Odb+OkzUJbDwx8/P85nDO/H3OmtON8W6WV1bX1jfJmZWt7Z3fP3j9oK5FIQltEcCG7PlaUs4i2NNOcdmNJcehz2vEn17nfuadSMRHd6WlMvRCPIhYwgrWRBvZRXxib00BjKcVDOh6k9SyrDOyqU3NmhZbBLaAKRTUH9ld/KEgS0kgTjpXquU6svRRLzQinWaWfKBpjMsEj2jMY4ZAqL53dn6FTowxRIKR5kUYz9fdEikOlpqFvOkOsx2rRy8X/vF6ig0svZVGcaBqR+aIg4UgLlIeBhkxSovnUACaSmVsRGWOJiTaR5SG4i19ehvZ5zTV8W682roo4ynAMJ3AGLlxAA26gCS0g8AjP8Apv1pP1Yr1bH/PWklXMHMKfsj5/AItBlmo=</latexit><latexit sha1_base64="+FjQcwobEn1xH4s0LKTcEHEej0o=">AAAB/3icbZDLSsNAFIZP6q3WW1Rw42awCK5KIgVdFt24rGAv0IYymU7aoZNMmJkoJWbhq7hxoYhbX8Odb+OkzUJbDwx8/P85nDO/H3OmtON8W6WV1bX1jfJmZWt7Z3fP3j9oK5FIQltEcCG7PlaUs4i2NNOcdmNJcehz2vEn17nfuadSMRHd6WlMvRCPIhYwgrWRBvZRXxib00BjKcVDOh6k9SyrDOyqU3NmhZbBLaAKRTUH9ld/KEgS0kgTjpXquU6svRRLzQinWaWfKBpjMsEj2jMY4ZAqL53dn6FTowxRIKR5kUYz9fdEikOlpqFvOkOsx2rRy8X/vF6ig0svZVGcaBqR+aIg4UgLlIeBhkxSovnUACaSmVsRGWOJiTaR5SG4i19ehvZ5zTV8W682roo4ynAMJ3AGLlxAA26gCS0g8AjP8Apv1pP1Yr1bH/PWklXMHMKfsj5/AItBlmo=</latexit><latexit sha1_base64="+FjQcwobEn1xH4s0LKTcEHEej0o=">AAAB/3icbZDLSsNAFIZP6q3WW1Rw42awCK5KIgVdFt24rGAv0IYymU7aoZNMmJkoJWbhq7hxoYhbX8Odb+OkzUJbDwx8/P85nDO/H3OmtON8W6WV1bX1jfJmZWt7Z3fP3j9oK5FIQltEcCG7PlaUs4i2NNOcdmNJcehz2vEn17nfuadSMRHd6WlMvRCPIhYwgrWRBvZRXxib00BjKcVDOh6k9SyrDOyqU3NmhZbBLaAKRTUH9ld/KEgS0kgTjpXquU6svRRLzQinWaWfKBpjMsEj2jMY4ZAqL53dn6FTowxRIKR5kUYz9fdEikOlpqFvOkOsx2rRy8X/vF6ig0svZVGcaBqR+aIg4UgLlIeBhkxSovnUACaSmVsRGWOJiTaR5SG4i19ehvZ5zTV8W682roo4ynAMJ3AGLlxAA26gCS0g8AjP8Apv1pP1Yr1bH/PWklXMHMKfsj5/AItBlmo=</latexit>

 �
h3

<latexit sha1_base64="o3/ElNac4meLQMq7qw45jAYRnD0=">AAAB/3icbZDLSsNAFIZP6q3WW1Rw4yZYBFclUUGXRTcuK9gLtKFMppN26GQmzEyUErPwVdy4UMStr+HOt3HSZqGtBwY+/v8czpk/iBlV2nW/rdLS8srqWnm9srG5tb1j7+61lEgkJk0smJCdACnCKCdNTTUjnVgSFAWMtIPxde6374lUVPA7PYmJH6EhpyHFSBupbx/0hLEZCTWSUjyko356lmWVvl11a+60nEXwCqhCUY2+/dUbCJxEhGvMkFJdz421nyKpKWYkq/QSRWKEx2hIugY5iojy0+n9mXNslIETCmke185U/T2RokipSRSYzgjpkZr3cvE/r5vo8NJPKY8TTTieLQoT5mjh5GE4AyoJ1mxiAGFJza0OHiGJsDaR5SF4819ehNZpzTN8e16tXxVxlOEQjuAEPLiAOtxAA5qA4RGe4RXerCfrxXq3PmatJauY2Yc/ZX3+AIm6lmk=</latexit><latexit sha1_base64="o3/ElNac4meLQMq7qw45jAYRnD0=">AAAB/3icbZDLSsNAFIZP6q3WW1Rw4yZYBFclUUGXRTcuK9gLtKFMppN26GQmzEyUErPwVdy4UMStr+HOt3HSZqGtBwY+/v8czpk/iBlV2nW/rdLS8srqWnm9srG5tb1j7+61lEgkJk0smJCdACnCKCdNTTUjnVgSFAWMtIPxde6374lUVPA7PYmJH6EhpyHFSBupbx/0hLEZCTWSUjyko356lmWVvl11a+60nEXwCqhCUY2+/dUbCJxEhGvMkFJdz421nyKpKWYkq/QSRWKEx2hIugY5iojy0+n9mXNslIETCmke185U/T2RokipSRSYzgjpkZr3cvE/r5vo8NJPKY8TTTieLQoT5mjh5GE4AyoJ1mxiAGFJza0OHiGJsDaR5SF4819ehNZpzTN8e16tXxVxlOEQjuAEPLiAOtxAA5qA4RGe4RXerCfrxXq3PmatJauY2Yc/ZX3+AIm6lmk=</latexit><latexit sha1_base64="o3/ElNac4meLQMq7qw45jAYRnD0=">AAAB/3icbZDLSsNAFIZP6q3WW1Rw4yZYBFclUUGXRTcuK9gLtKFMppN26GQmzEyUErPwVdy4UMStr+HOt3HSZqGtBwY+/v8czpk/iBlV2nW/rdLS8srqWnm9srG5tb1j7+61lEgkJk0smJCdACnCKCdNTTUjnVgSFAWMtIPxde6374lUVPA7PYmJH6EhpyHFSBupbx/0hLEZCTWSUjyko356lmWVvl11a+60nEXwCqhCUY2+/dUbCJxEhGvMkFJdz421nyKpKWYkq/QSRWKEx2hIugY5iojy0+n9mXNslIETCmke185U/T2RokipSRSYzgjpkZr3cvE/r5vo8NJPKY8TTTieLQoT5mjh5GE4AyoJ1mxiAGFJza0OHiGJsDaR5SF4819ehNZpzTN8e16tXxVxlOEQjuAEPLiAOtxAA5qA4RGe4RXerCfrxXq3PmatJauY2Yc/ZX3+AIm6lmk=</latexit><latexit sha1_base64="o3/ElNac4meLQMq7qw45jAYRnD0=">AAAB/3icbZDLSsNAFIZP6q3WW1Rw4yZYBFclUUGXRTcuK9gLtKFMppN26GQmzEyUErPwVdy4UMStr+HOt3HSZqGtBwY+/v8czpk/iBlV2nW/rdLS8srqWnm9srG5tb1j7+61lEgkJk0smJCdACnCKCdNTTUjnVgSFAWMtIPxde6374lUVPA7PYmJH6EhpyHFSBupbx/0hLEZCTWSUjyko356lmWVvl11a+60nEXwCqhCUY2+/dUbCJxEhGvMkFJdz421nyKpKWYkq/QSRWKEx2hIugY5iojy0+n9mXNslIETCmke185U/T2RokipSRSYzgjpkZr3cvE/r5vo8NJPKY8TTTieLQoT5mjh5GE4AyoJ1mxiAGFJza0OHiGJsDaR5SF4819ehNZpzTN8e16tXxVxlOEQjuAEPLiAOtxAA5qA4RGe4RXerCfrxXq3PmatJauY2Yc/ZX3+AIm6lmk=</latexit>

 �
h2

<latexit sha1_base64="pdTIcKs31vP+HLFnQlRAFETbXbA=">AAAB/3icbZDLSsNAFIZP6q3WW1Rw4yZYBFclKYIui25cVrAXaEOZTCft0MlMmJkoJWbhq7hxoYhbX8Odb+OkzUJbDwx8/P85nDN/EDOqtOt+W6WV1bX1jfJmZWt7Z3fP3j9oK5FITFpYMCG7AVKEUU5ammpGurEkKAoY6QST69zv3BOpqOB3ehoTP0IjTkOKkTbSwD7qC2MzEmokpXhIx4O0nmWVgV11a+6snGXwCqhCUc2B/dUfCpxEhGvMkFI9z421nyKpKWYkq/QTRWKEJ2hEegY5iojy09n9mXNqlKETCmke185M/T2RokipaRSYzgjpsVr0cvE/r5fo8NJPKY8TTTieLwoT5mjh5GE4QyoJ1mxqAGFJza0OHiOJsDaR5SF4i19ehna95hm+Pa82roo4ynAMJ3AGHlxAA26gCS3A8AjP8Apv1pP1Yr1bH/PWklXMHMKfsj5/AIgzlmg=</latexit><latexit sha1_base64="pdTIcKs31vP+HLFnQlRAFETbXbA=">AAAB/3icbZDLSsNAFIZP6q3WW1Rw4yZYBFclKYIui25cVrAXaEOZTCft0MlMmJkoJWbhq7hxoYhbX8Odb+OkzUJbDwx8/P85nDN/EDOqtOt+W6WV1bX1jfJmZWt7Z3fP3j9oK5FITFpYMCG7AVKEUU5ammpGurEkKAoY6QST69zv3BOpqOB3ehoTP0IjTkOKkTbSwD7qC2MzEmokpXhIx4O0nmWVgV11a+6snGXwCqhCUc2B/dUfCpxEhGvMkFI9z421nyKpKWYkq/QTRWKEJ2hEegY5iojy09n9mXNqlKETCmke185M/T2RokipaRSYzgjpsVr0cvE/r5fo8NJPKY8TTTieLwoT5mjh5GE4QyoJ1mxqAGFJza0OHiOJsDaR5SF4i19ehna95hm+Pa82roo4ynAMJ3AGHlxAA26gCS3A8AjP8Apv1pP1Yr1bH/PWklXMHMKfsj5/AIgzlmg=</latexit><latexit sha1_base64="pdTIcKs31vP+HLFnQlRAFETbXbA=">AAAB/3icbZDLSsNAFIZP6q3WW1Rw4yZYBFclKYIui25cVrAXaEOZTCft0MlMmJkoJWbhq7hxoYhbX8Odb+OkzUJbDwx8/P85nDN/EDOqtOt+W6WV1bX1jfJmZWt7Z3fP3j9oK5FITFpYMCG7AVKEUU5ammpGurEkKAoY6QST69zv3BOpqOB3ehoTP0IjTkOKkTbSwD7qC2MzEmokpXhIx4O0nmWVgV11a+6snGXwCqhCUc2B/dUfCpxEhGvMkFI9z421nyKpKWYkq/QTRWKEJ2hEegY5iojy09n9mXNqlKETCmke185M/T2RokipaRSYzgjpsVr0cvE/r5fo8NJPKY8TTTieLwoT5mjh5GE4QyoJ1mxqAGFJza0OHiOJsDaR5SF4i19ehna95hm+Pa82roo4ynAMJ3AGHlxAA26gCS3A8AjP8Apv1pP1Yr1bH/PWklXMHMKfsj5/AIgzlmg=</latexit><latexit sha1_base64="pdTIcKs31vP+HLFnQlRAFETbXbA=">AAAB/3icbZDLSsNAFIZP6q3WW1Rw4yZYBFclKYIui25cVrAXaEOZTCft0MlMmJkoJWbhq7hxoYhbX8Odb+OkzUJbDwx8/P85nDN/EDOqtOt+W6WV1bX1jfJmZWt7Z3fP3j9oK5FITFpYMCG7AVKEUU5ammpGurEkKAoY6QST69zv3BOpqOB3ehoTP0IjTkOKkTbSwD7qC2MzEmokpXhIx4O0nmWVgV11a+6snGXwCqhCUc2B/dUfCpxEhGvMkFI9z421nyKpKWYkq/QTRWKEJ2hEegY5iojy09n9mXNqlKETCmke185M/T2RokipaRSYzgjpsVr0cvE/r5fo8NJPKY8TTTieLwoT5mjh5GE4QyoJ1mxqAGFJza0OHiOJsDaR5SF4i19ehna95hm+Pa82roo4ynAMJ3AGHlxAA26gCS3A8AjP8Apv1pP1Yr1bH/PWklXMHMKfsj5/AIgzlmg=</latexit>

 �
h1

<latexit sha1_base64="bFnzhFNjKIT4p+MUSf+XkfA8iD0=">AAAB/3icbZDNSsNAFIVv6l+tf1HBjZvBIrgqiQi6LLpxWcG2QhvCZDpph04yYWailJiFr+LGhSJufQ13vo2TNgttvTDwcc693DsnSDhT2nG+rcrS8srqWnW9trG5tb1j7+51lEgloW0iuJB3AVaUs5i2NdOc3iWS4ijgtBuMrwq/e0+lYiK+1ZOEehEexixkBGsj+fZBXxib01BjKcVDNvIzN89rvl13Gs600CK4JdShrJZvf/UHgqQRjTXhWKme6yTay7DUjHCa1/qpogkmYzykPYMxjqjysun9OTo2ygCFQpoXazRVf09kOFJqEgWmM8J6pOa9QvzP66U6vPAyFieppjGZLQpTjrRARRhowCQlmk8MYCKZuRWREZaYaBNZEYI7/+VF6Jw2XMM3Z/XmZRlHFQ7hCE7AhXNowjW0oA0EHuEZXuHNerJerHfrY9ZascqZffhT1ucPhqyWZw==</latexit><latexit sha1_base64="bFnzhFNjKIT4p+MUSf+XkfA8iD0=">AAAB/3icbZDNSsNAFIVv6l+tf1HBjZvBIrgqiQi6LLpxWcG2QhvCZDpph04yYWailJiFr+LGhSJufQ13vo2TNgttvTDwcc693DsnSDhT2nG+rcrS8srqWnW9trG5tb1j7+51lEgloW0iuJB3AVaUs5i2NdOc3iWS4ijgtBuMrwq/e0+lYiK+1ZOEehEexixkBGsj+fZBXxib01BjKcVDNvIzN89rvl13Gs600CK4JdShrJZvf/UHgqQRjTXhWKme6yTay7DUjHCa1/qpogkmYzykPYMxjqjysun9OTo2ygCFQpoXazRVf09kOFJqEgWmM8J6pOa9QvzP66U6vPAyFieppjGZLQpTjrRARRhowCQlmk8MYCKZuRWREZaYaBNZEYI7/+VF6Jw2XMM3Z/XmZRlHFQ7hCE7AhXNowjW0oA0EHuEZXuHNerJerHfrY9ZascqZffhT1ucPhqyWZw==</latexit><latexit sha1_base64="bFnzhFNjKIT4p+MUSf+XkfA8iD0=">AAAB/3icbZDNSsNAFIVv6l+tf1HBjZvBIrgqiQi6LLpxWcG2QhvCZDpph04yYWailJiFr+LGhSJufQ13vo2TNgttvTDwcc693DsnSDhT2nG+rcrS8srqWnW9trG5tb1j7+51lEgloW0iuJB3AVaUs5i2NdOc3iWS4ijgtBuMrwq/e0+lYiK+1ZOEehEexixkBGsj+fZBXxib01BjKcVDNvIzN89rvl13Gs600CK4JdShrJZvf/UHgqQRjTXhWKme6yTay7DUjHCa1/qpogkmYzykPYMxjqjysun9OTo2ygCFQpoXazRVf09kOFJqEgWmM8J6pOa9QvzP66U6vPAyFieppjGZLQpTjrRARRhowCQlmk8MYCKZuRWREZaYaBNZEYI7/+VF6Jw2XMM3Z/XmZRlHFQ7hCE7AhXNowjW0oA0EHuEZXuHNerJerHfrY9ZascqZffhT1ucPhqyWZw==</latexit><latexit sha1_base64="bFnzhFNjKIT4p+MUSf+XkfA8iD0=">AAAB/3icbZDNSsNAFIVv6l+tf1HBjZvBIrgqiQi6LLpxWcG2QhvCZDpph04yYWailJiFr+LGhSJufQ13vo2TNgttvTDwcc693DsnSDhT2nG+rcrS8srqWnW9trG5tb1j7+51lEgloW0iuJB3AVaUs5i2NdOc3iWS4ijgtBuMrwq/e0+lYiK+1ZOEehEexixkBGsj+fZBXxib01BjKcVDNvIzN89rvl13Gs600CK4JdShrJZvf/UHgqQRjTXhWKme6yTay7DUjHCa1/qpogkmYzykPYMxjqjysun9OTo2ygCFQpoXazRVf09kOFJqEgWmM8J6pOa9QvzP66U6vPAyFieppjGZLQpTjrRARRhowCQlmk8MYCKZuRWREZaYaBNZEYI7/+VF6Jw2XMM3Z/XmZRlHFQ7hCE7AhXNowjW0oA0EHuEZXuHNerJerHfrY9ZascqZffhT1ucPhqyWZw==</latexit>

X

X
X

X
X

X X X X X

�
<latexit sha1_base64="g+IRNevPc1g3Nk0ZK/Q3r3J8OUI=">AAAB7HicbZBNS8NAEIYn9avWr6pHL8EieCqJCHosevFYwdRCG8pmO2mXbjZhdyKU0t/gxYMiXv1B3vw3btsctPWFhYd3ZtiZN8qkMOR5305pbX1jc6u8XdnZ3ds/qB4etUyaa44BT2Wq2xEzKIXCgARJbGcaWRJJfIxGt7P64xNqI1L1QOMMw4QNlIgFZ2StoBshsV615tW9udxV8AuoQaFmr/rV7ac8T1ARl8yYju9lFE6YJsElTivd3GDG+IgNsGNRsQRNOJkvO3XPrNN341Tbp8idu78nJiwxZpxEtjNhNDTLtZn5X62TU3wdToTKckLFFx/FuXQpdWeXu32hkZMcW2BcC7ury4dMM042n4oNwV8+eRVaF3Xf8v1lrXFTxFGGEziFc/DhChpwB00IgIOAZ3iFN0c5L86787FoLTnFzDH8kfP5A8PYjqQ=</latexit><latexit sha1_base64="g+IRNevPc1g3Nk0ZK/Q3r3J8OUI=">AAAB7HicbZBNS8NAEIYn9avWr6pHL8EieCqJCHosevFYwdRCG8pmO2mXbjZhdyKU0t/gxYMiXv1B3vw3btsctPWFhYd3ZtiZN8qkMOR5305pbX1jc6u8XdnZ3ds/qB4etUyaa44BT2Wq2xEzKIXCgARJbGcaWRJJfIxGt7P64xNqI1L1QOMMw4QNlIgFZ2StoBshsV615tW9udxV8AuoQaFmr/rV7ac8T1ARl8yYju9lFE6YJsElTivd3GDG+IgNsGNRsQRNOJkvO3XPrNN341Tbp8idu78nJiwxZpxEtjNhNDTLtZn5X62TU3wdToTKckLFFx/FuXQpdWeXu32hkZMcW2BcC7ury4dMM042n4oNwV8+eRVaF3Xf8v1lrXFTxFGGEziFc/DhChpwB00IgIOAZ3iFN0c5L86787FoLTnFzDH8kfP5A8PYjqQ=</latexit><latexit sha1_base64="g+IRNevPc1g3Nk0ZK/Q3r3J8OUI=">AAAB7HicbZBNS8NAEIYn9avWr6pHL8EieCqJCHosevFYwdRCG8pmO2mXbjZhdyKU0t/gxYMiXv1B3vw3btsctPWFhYd3ZtiZN8qkMOR5305pbX1jc6u8XdnZ3ds/qB4etUyaa44BT2Wq2xEzKIXCgARJbGcaWRJJfIxGt7P64xNqI1L1QOMMw4QNlIgFZ2StoBshsV615tW9udxV8AuoQaFmr/rV7ac8T1ARl8yYju9lFE6YJsElTivd3GDG+IgNsGNRsQRNOJkvO3XPrNN341Tbp8idu78nJiwxZpxEtjNhNDTLtZn5X62TU3wdToTKckLFFx/FuXQpdWeXu32hkZMcW2BcC7ury4dMM042n4oNwV8+eRVaF3Xf8v1lrXFTxFGGEziFc/DhChpwB00IgIOAZ3iFN0c5L86787FoLTnFzDH8kfP5A8PYjqQ=</latexit><latexit sha1_base64="g+IRNevPc1g3Nk0ZK/Q3r3J8OUI=">AAAB7HicbZBNS8NAEIYn9avWr6pHL8EieCqJCHosevFYwdRCG8pmO2mXbjZhdyKU0t/gxYMiXv1B3vw3btsctPWFhYd3ZtiZN8qkMOR5305pbX1jc6u8XdnZ3ds/qB4etUyaa44BT2Wq2xEzKIXCgARJbGcaWRJJfIxGt7P64xNqI1L1QOMMw4QNlIgFZ2StoBshsV615tW9udxV8AuoQaFmr/rV7ac8T1ARl8yYju9lFE6YJsElTivd3GDG+IgNsGNRsQRNOJkvO3XPrNN341Tbp8idu78nJiwxZpxEtjNhNDTLtZn5X62TU3wdToTKckLFFx/FuXQpdWeXu32hkZMcW2BcC7ury4dMM042n4oNwV8+eRVaF3Xf8v1lrXFTxFGGEziFc/DhChpwB00IgIOAZ3iFN0c5L86787FoLTnFzDH8kfP5A8PYjqQ=</latexit>

↵<latexit sha1_base64="6EtSdgpKG2qhZbjGZ4j1QLtQzns=">AAAB7XicbZBNSwMxEIZn61etX1WPXoJF8FR2RdBj0YvHCvYD2qXMpmkbm02WJCuUpf/BiwdFvPp/vPlvTNs9aOsLgYd3ZsjMGyWCG+v7315hbX1jc6u4XdrZ3ds/KB8eNY1KNWUNqoTS7QgNE1yyhuVWsHaiGcaRYK1ofDurt56YNlzJBztJWBjjUPIBp2id1eyiSEbYK1f8qj8XWYUghwrkqvfKX92+omnMpKUCjekEfmLDDLXlVLBpqZsaliAd45B1HEqMmQmz+bZTcuacPhko7Z60ZO7+nsgwNmYSR64zRjsyy7WZ+V+tk9rBdZhxmaSWSbr4aJAKYhWZnU76XDNqxcQBUs3droSOUCO1LqCSCyFYPnkVmhfVwPH9ZaV2k8dRhBM4hXMI4ApqcAd1aACFR3iGV3jzlPfivXsfi9aCl88cwx95nz+Lg48Y</latexit><latexit sha1_base64="6EtSdgpKG2qhZbjGZ4j1QLtQzns=">AAAB7XicbZBNSwMxEIZn61etX1WPXoJF8FR2RdBj0YvHCvYD2qXMpmkbm02WJCuUpf/BiwdFvPp/vPlvTNs9aOsLgYd3ZsjMGyWCG+v7315hbX1jc6u4XdrZ3ds/KB8eNY1KNWUNqoTS7QgNE1yyhuVWsHaiGcaRYK1ofDurt56YNlzJBztJWBjjUPIBp2id1eyiSEbYK1f8qj8XWYUghwrkqvfKX92+omnMpKUCjekEfmLDDLXlVLBpqZsaliAd45B1HEqMmQmz+bZTcuacPhko7Z60ZO7+nsgwNmYSR64zRjsyy7WZ+V+tk9rBdZhxmaSWSbr4aJAKYhWZnU76XDNqxcQBUs3droSOUCO1LqCSCyFYPnkVmhfVwPH9ZaV2k8dRhBM4hXMI4ApqcAd1aACFR3iGV3jzlPfivXsfi9aCl88cwx95nz+Lg48Y</latexit><latexit sha1_base64="6EtSdgpKG2qhZbjGZ4j1QLtQzns=">AAAB7XicbZBNSwMxEIZn61etX1WPXoJF8FR2RdBj0YvHCvYD2qXMpmkbm02WJCuUpf/BiwdFvPp/vPlvTNs9aOsLgYd3ZsjMGyWCG+v7315hbX1jc6u4XdrZ3ds/KB8eNY1KNWUNqoTS7QgNE1yyhuVWsHaiGcaRYK1ofDurt56YNlzJBztJWBjjUPIBp2id1eyiSEbYK1f8qj8XWYUghwrkqvfKX92+omnMpKUCjekEfmLDDLXlVLBpqZsaliAd45B1HEqMmQmz+bZTcuacPhko7Z60ZO7+nsgwNmYSR64zRjsyy7WZ+V+tk9rBdZhxmaSWSbr4aJAKYhWZnU76XDNqxcQBUs3droSOUCO1LqCSCyFYPnkVmhfVwPH9ZaV2k8dRhBM4hXMI4ApqcAd1aACFR3iGV3jzlPfivXsfi9aCl88cwx95nz+Lg48Y</latexit><latexit sha1_base64="6EtSdgpKG2qhZbjGZ4j1QLtQzns=">AAAB7XicbZBNSwMxEIZn61etX1WPXoJF8FR2RdBj0YvHCvYD2qXMpmkbm02WJCuUpf/BiwdFvPp/vPlvTNs9aOsLgYd3ZsjMGyWCG+v7315hbX1jc6u4XdrZ3ds/KB8eNY1KNWUNqoTS7QgNE1yyhuVWsHaiGcaRYK1ofDurt56YNlzJBztJWBjjUPIBp2id1eyiSEbYK1f8qj8XWYUghwrkqvfKX92+omnMpKUCjekEfmLDDLXlVLBpqZsaliAd45B1HEqMmQmz+bZTcuacPhko7Z60ZO7+nsgwNmYSR64zRjsyy7WZ+V+tk9rBdZhxmaSWSbr4aJAKYhWZnU76XDNqxcQBUs3droSOUCO1LqCSCyFYPnkVmhfVwPH9ZaV2k8dRhBM4hXMI4ApqcAd1aACFR3iGV3jzlPfivXsfi9aCl88cwx95nz+Lg48Y</latexit>

w̄ 2 R2d
<latexit sha1_base64="jY03p1o5sw9rDrdRZjzDG7ij0bA=">AAACDnicbZC7TsMwFIYdrqXcCowsFlUlpipBSDBWsDAWRC9SUyrbdVoLx4nsE6CK8gQsvAoLAwixMrPxNjhtB2j5JUuf/nOOfM5PYykMuO63s7C4tLyyWlgrrm9sbm2XdnabJko04w0WyUi3KTFcCsUbIEDydqw5CankLXp7ntdbd1wbEalrGMW8G5KBEoFgBKzVK1V8SnTqA38AGqT3WYZ9obAfEhhSml5lN+lRPyv2SmW36o6F58GbQhlNVe+Vvvx+xJKQK2CSGNPx3Bi6KdEgmORZ0U8Mjwm7JQPesahIyE03HZ+T4Yp1+jiItH0K8Nj9PZGS0JhRSG1nvqeZreXmf7VOAsFpNxUqToArNvkoSCSGCOfZ4L7QnIEcWSBMC7srZkOiCQObYB6CN3vyPDSPqp7ly+Ny7WwaRwHtowN0iDx0gmroAtVRAzH0iJ7RK3pznpwX5935mLQuONOZPfRHzucPs+Gcew==</latexit><latexit sha1_base64="jY03p1o5sw9rDrdRZjzDG7ij0bA=">AAACDnicbZC7TsMwFIYdrqXcCowsFlUlpipBSDBWsDAWRC9SUyrbdVoLx4nsE6CK8gQsvAoLAwixMrPxNjhtB2j5JUuf/nOOfM5PYykMuO63s7C4tLyyWlgrrm9sbm2XdnabJko04w0WyUi3KTFcCsUbIEDydqw5CankLXp7ntdbd1wbEalrGMW8G5KBEoFgBKzVK1V8SnTqA38AGqT3WYZ9obAfEhhSml5lN+lRPyv2SmW36o6F58GbQhlNVe+Vvvx+xJKQK2CSGNPx3Bi6KdEgmORZ0U8Mjwm7JQPesahIyE03HZ+T4Yp1+jiItH0K8Nj9PZGS0JhRSG1nvqeZreXmf7VOAsFpNxUqToArNvkoSCSGCOfZ4L7QnIEcWSBMC7srZkOiCQObYB6CN3vyPDSPqp7ly+Ny7WwaRwHtowN0iDx0gmroAtVRAzH0iJ7RK3pznpwX5935mLQuONOZPfRHzucPs+Gcew==</latexit><latexit sha1_base64="jY03p1o5sw9rDrdRZjzDG7ij0bA=">AAACDnicbZC7TsMwFIYdrqXcCowsFlUlpipBSDBWsDAWRC9SUyrbdVoLx4nsE6CK8gQsvAoLAwixMrPxNjhtB2j5JUuf/nOOfM5PYykMuO63s7C4tLyyWlgrrm9sbm2XdnabJko04w0WyUi3KTFcCsUbIEDydqw5CankLXp7ntdbd1wbEalrGMW8G5KBEoFgBKzVK1V8SnTqA38AGqT3WYZ9obAfEhhSml5lN+lRPyv2SmW36o6F58GbQhlNVe+Vvvx+xJKQK2CSGNPx3Bi6KdEgmORZ0U8Mjwm7JQPesahIyE03HZ+T4Yp1+jiItH0K8Nj9PZGS0JhRSG1nvqeZreXmf7VOAsFpNxUqToArNvkoSCSGCOfZ4L7QnIEcWSBMC7srZkOiCQObYB6CN3vyPDSPqp7ly+Ny7WwaRwHtowN0iDx0gmroAtVRAzH0iJ7RK3pznpwX5935mLQuONOZPfRHzucPs+Gcew==</latexit><latexit sha1_base64="jY03p1o5sw9rDrdRZjzDG7ij0bA=">AAACDnicbZC7TsMwFIYdrqXcCowsFlUlpipBSDBWsDAWRC9SUyrbdVoLx4nsE6CK8gQsvAoLAwixMrPxNjhtB2j5JUuf/nOOfM5PYykMuO63s7C4tLyyWlgrrm9sbm2XdnabJko04w0WyUi3KTFcCsUbIEDydqw5CankLXp7ntdbd1wbEalrGMW8G5KBEoFgBKzVK1V8SnTqA38AGqT3WYZ9obAfEhhSml5lN+lRPyv2SmW36o6F58GbQhlNVe+Vvvx+xJKQK2CSGNPx3Bi6KdEgmORZ0U8Mjwm7JQPesahIyE03HZ+T4Yp1+jiItH0K8Nj9PZGS0JhRSG1nvqeZreXmf7VOAsFpNxUqToArNvkoSCSGCOfZ4L7QnIEcWSBMC7srZkOiCQObYB6CN3vyPDSPqp7ly+Ny7WwaRwHtowN0iDx0gmroAtVRAzH0iJ7RK3pznpwX5935mLQuONOZPfRHzucPs+Gcew==</latexit>

w 2 Rd
<latexit sha1_base64="t1Tk9+lc9t2k38t/R5ojsRj9v44=">AAACB3icbZDLSsNAFIYn9VbrLepSkMEiuCqJCLosunFZxV6gqWUynbRDJ5Mwc6KWkJ0bX8WNC0Xc+grufBsnbRfa+sPAx3/OYc75/VhwDY7zbRUWFpeWV4qrpbX1jc0te3unoaNEUVankYhUyyeaCS5ZHTgI1ooVI6EvWNMfXuT15h1TmkfyBkYx64SkL3nAKQFjde19D9gD+EF6n2GPS+yFBAa+n15nt2kvK3XtslNxxsLz4E6hjKaqde0vrxfRJGQSqCBat10nhk5KFHAqWFbyEs1iQoekz9oGJQmZ7qTjOzJ8aJweDiJlngQ8dn9PpCTUehT6pjNfU8/WcvO/WjuB4KyTchknwCSdfBQkAkOE81BwjytGQYwMEKq42RXTAVGEgokuD8GdPXkeGscV1/DVSbl6Po2jiPbQATpCLjpFVXSJaqiOKHpEz+gVvVlP1ov1bn1MWgvWdGYX/ZH1+QMsJJl6</latexit><latexit sha1_base64="t1Tk9+lc9t2k38t/R5ojsRj9v44=">AAACB3icbZDLSsNAFIYn9VbrLepSkMEiuCqJCLosunFZxV6gqWUynbRDJ5Mwc6KWkJ0bX8WNC0Xc+grufBsnbRfa+sPAx3/OYc75/VhwDY7zbRUWFpeWV4qrpbX1jc0te3unoaNEUVankYhUyyeaCS5ZHTgI1ooVI6EvWNMfXuT15h1TmkfyBkYx64SkL3nAKQFjde19D9gD+EF6n2GPS+yFBAa+n15nt2kvK3XtslNxxsLz4E6hjKaqde0vrxfRJGQSqCBat10nhk5KFHAqWFbyEs1iQoekz9oGJQmZ7qTjOzJ8aJweDiJlngQ8dn9PpCTUehT6pjNfU8/WcvO/WjuB4KyTchknwCSdfBQkAkOE81BwjytGQYwMEKq42RXTAVGEgokuD8GdPXkeGscV1/DVSbl6Po2jiPbQATpCLjpFVXSJaqiOKHpEz+gVvVlP1ov1bn1MWgvWdGYX/ZH1+QMsJJl6</latexit><latexit sha1_base64="t1Tk9+lc9t2k38t/R5ojsRj9v44=">AAACB3icbZDLSsNAFIYn9VbrLepSkMEiuCqJCLosunFZxV6gqWUynbRDJ5Mwc6KWkJ0bX8WNC0Xc+grufBsnbRfa+sPAx3/OYc75/VhwDY7zbRUWFpeWV4qrpbX1jc0te3unoaNEUVankYhUyyeaCS5ZHTgI1ooVI6EvWNMfXuT15h1TmkfyBkYx64SkL3nAKQFjde19D9gD+EF6n2GPS+yFBAa+n15nt2kvK3XtslNxxsLz4E6hjKaqde0vrxfRJGQSqCBat10nhk5KFHAqWFbyEs1iQoekz9oGJQmZ7qTjOzJ8aJweDiJlngQ8dn9PpCTUehT6pjNfU8/WcvO/WjuB4KyTchknwCSdfBQkAkOE81BwjytGQYwMEKq42RXTAVGEgokuD8GdPXkeGscV1/DVSbl6Po2jiPbQATpCLjpFVXSJaqiOKHpEz+gVvVlP1ov1bn1MWgvWdGYX/ZH1+QMsJJl6</latexit><latexit sha1_base64="t1Tk9+lc9t2k38t/R5ojsRj9v44=">AAACB3icbZDLSsNAFIYn9VbrLepSkMEiuCqJCLosunFZxV6gqWUynbRDJ5Mwc6KWkJ0bX8WNC0Xc+grufBsnbRfa+sPAx3/OYc75/VhwDY7zbRUWFpeWV4qrpbX1jc0te3unoaNEUVankYhUyyeaCS5ZHTgI1ooVI6EvWNMfXuT15h1TmkfyBkYx64SkL3nAKQFjde19D9gD+EF6n2GPS+yFBAa+n15nt2kvK3XtslNxxsLz4E6hjKaqde0vrxfRJGQSqCBat10nhk5KFHAqWFbyEs1iQoekz9oGJQmZ7qTjOzJ8aJweDiJlngQ8dn9PpCTUehT6pjNfU8/WcvO/WjuB4KyTchknwCSdfBQkAkOE81BwjytGQYwMEKq42RXTAVGEgokuD8GdPXkeGscV1/DVSbl6Po2jiPbQATpCLjpFVXSJaqiOKHpEz+gVvVlP1ov1bn1MWgvWdGYX/ZH1+QMsJJl6</latexit>

Adaptation Layer

this great movieais

BiLSTM Layers

Max-pooling

Sentence encoding

Soft-max Layer

Figure 2: Left: This figure illustrates the adaptation layer, that takes as input a 2d dimensional word embedding
containing the generic and DS KCCA projections and learns a d dimensional DA word embedding. This DA word
embedding is then input to the CNN encoder. The network can be optimized to learn only parameters α and β or
can be trained end to end to learn sentence embeddings and classifier weights as well. Right: This figure illustrates
the adaptation layer, out of which, the DA word embedding is then input to the BiLSTM+max-pooling encoder.
The entire network can be trained end-to-end to learn parameters α and β in addition to the sentence embedding
and classifier, or the network can be optimized to learn only the weights α and β and weights of the classifier.

dings are first constructed from VLib and VCon.
Then, KCCA projections are obtained for generic
GloVe embeddings and the DS word embeddings
for each user group, in order to construct DA word
embeddings for Liberal ŴDA(Lib) and Conserva-
tive ŴDA(Con) users.

Next consider the set of common words in the
two vocabularies, i.e Vcommon = VLib ∩ VCon.
Perform a second KCCA to bring DA words in
Vcommon from both Liberal VLib and Conservative
VCon vocabularies into a common subspace. To
measure the ‘shift’ ψ, calculate the l2 distance for
each word embedding Ŵi ∈ Vcommon, i.e

ψ = ||Ŵi,DA(Lib) − Ŵi,DA(Con)||2 ∀i ∈ Vcommon
(1)

Words with large values of ψ are considered to
have shifted the most between the domains of lib-
eral and conservative users. Words are then or-
dered based on the magnitude of ψ, and cross ref-
erenced with words in the gold standard list.

In order to demonstrate that the KCCA can cap-
ture domain semantics when learning DA word
embeddings, the following section presents an
analysis against a random baseline.

3.3 Study of a random baseline
A random baseline is one that picks k words from
Vcommon, randomly, and reports these k words as
having shifted the most with respect to word use

within tweets that correspond to Liberal and Con-
servative users. This random baseline follows a
hypergeometric distribution, which is a discrete
probability distribution that calculates the proba-
bility of obtaining k successes (of a sample with a
specific feature) from n draws, from a finite pop-
ulation of V , without replacement, where exactly
K samples contain the specific feature.

For example, suppose |Vcommon| = V , and say
the number of words in the gold standard list are
K. Then the probability of the random baseline
picking k < K words out of n words is

Pr(X = k) =

(
K
k

)(
V−K
n−k

)(
V
n

) . (2)
To calculate these probabilities for the KCCA-

DA word embeddings, we shall use the LibCon
data set of Section 3.1. A gold standard list of
136 key topics/concepts important to Liberals and
Conservatives is obtained from the study by (Li
et al., 2017), and additional details about the Lib-
Con data set and the gold standard list can be
found in Section 4.1.

From the LibCon data set |Vcommon| = V =
1573. Out of these 1573 words, there are 74 words
that are present in the gold standard list. Hence
K = 74. After obtaining the DA word embed-
dings for words in Vcommon and calculating the
shift ψ, we take the top 200 words that shifted



5553

action cost dream help media reform women
amendment court education honor need republican work
attack crisis fact hope order right world
budget deal force income pledge risk
burden debate freedom information police rule
business debt fund insurance poll school
candidate decision funding justice power spending
care defense future labor president state
class deficit generation leader problem truth
college democrat government leadership program value
congress development governor legislature protection violence
control divide health legislation race wealth

Table 1: This table presents 74 words representing key
political concepts common to Liberal and Conservative
users on Twitter. Words in bold are the ones that ‘shift’
the most (by l2 distance) in use between Liberal and
Conservative users on Twitter.

most. From these words, at least 20 are present
in the gold standard list.

The number of words-of-interest in a sample
of 200 words forms a hypergeometric distribution
with mean µ = nKV ≈ 9.4, and standard deviation
2.7984.

Direct calculation shows that the probability
of picking exactly 20 words from this subset is
Pr(X = 20) = 0.000346. Similarly, the prob-
ability that 20 or more would have been chosen by
chance is,

p =
200∑
k=20

Pr(X = k) (3)

which, by numerical calculation, is p = 0.000524.
Such a result is highly unlikely to occur by chance.
Accordingly, the KCCA domain adapted word
embeddings are indeed capturing something sig-
nificant about the language usage in the domain.

Table 1 presents words from the gold standard
list with words in bold face indicating words that
shift the most across Liberal and Conservative do-
mains.

4 Experimental Results

The discussion in Section 3.1 provides motivation
for adopting the KCCA projections as a tool to
calculate DA word embeddings. These DA word
embeddings can be combined with standard neu-
ral network models to obtain sentence embeddings
that can be used to improve performance on down-
stream tasks such as sentiment analysis. To test the
effectiveness of our proposed approach, we con-
duct a series of binary (LibCon, MR and SST) and
multi-class (Beauty, Book and Music) classifica-
tion tasks, using our adaptation layer on top of sev-
eral standard architectures. The standard architec-
tures used in this paper are architectures that are

common in the current State-Of-The-Art (SOTA)
for sentence encoders (Conneau et al., 2017) and
multi-domain sentiment analysis (Liu et al., 2018).
These architectures are a BiLSTM based architec-
ture followed by some sort of pooling and then a
soft-max classification layer.

Our main result is that using the proposed do-
main adaptation layer on top of existing architec-
tures helps improve the performance of the archi-
tecture by about 2 − 8%. Furthermore, we show
that basic architectures enhanced with our DA
layer outperform SOTA architectures built specif-
ically for domain adaptation and transfer learning
problems. We now give a description of the vari-
ous datasets that we use and the various baseline
algorithm used for our experimental comparisons.

4.1 Data Sets

Motivated by the fact that our DA layer is most
useful when the target dataset is of modest size,
we present our experimental results on three such
datasets. Furthermore, the text used in these
datasets often uses the same set of words to ex-
press clearly contrastive sentiments, thereby set-
ting up groups or domains of distinct word use
within a single data set.
LibCon: The LibCon (Liberal and Conservative)
data set is obtained from a study that aims towards
understanding, analyzing and modeling the chang-
ing landscape of political discourse of an entire
state over a duration of 10 years (Friedland et al.,
2017). A key aspect of this study is the use of
Twitter data to analyze the latent network struc-
ture among key players of Wisconsin’s political-
media ecology. To do this, data was drawn from
a collection of Twitter data housed at the UW-
Madison School of Journalism and Mass Commu-
nication. Data is drawn from a 10% sample of
Twitter messages worldwide from Twitter’s API
(Twitter 2012). Note that the period of data collec-
tion corresponds to the re-election efforts of Gov-
ernor Walker (of Wisconsin). Snowball sampling
is employed to identify important Twitter handles
active in four 28-day periods in the first half of
2012. A hand curated list of Twitter handles cor-
responding to key players in Wisconsin’s political
climate is identified. Combined with Twitter han-
dles of nationwide users that interact frequently
with users in this list, the final list of Twitter han-
dles of interest is formed.

Twitter handles are confirmed to their corre-



5554

sponding political associations by verifying their
party membership. The Twitter handle pool in-
cludes a balanced representation from Republican
and Democratic accounts. In depth description of
data collection methodologies and outcomes can
be found in (Friedland et al., 2017) and readers are
encouraged to consult this resource for additional
information. A subsample of 1000 tweets, each
from known Liberal (Democratic party) and Con-
servative (Republican party) users are considered
for experiments here.

Gold standard list of topics key to both Demo-
crat and Republican parties are obtained by ana-
lyzing primary debate data over a period of almost
20 years (1999-present) (Li et al., 2017). A combi-
nation of modeling techniques involving semantic
spaces and neural network architectures is used to
arrive at the final list of 136 top/key concepts such
as ‘Army’, ‘Border’, ‘Democracy’, ‘Justice’ etc.
Out of these 136 words, 74 occur in the LibCon
data set as seen in Table 1.

Movie Review: This is a benchmark
dataset (Pang and Lee, 2005) from which we
randomly sample 2500 positive and 2500 negative
reviews for experiments.

SST: The Stanford Sentiment Tree (SST) bank
is yet another standard benchmark data set with
reviews binarized to ‘positive’ or ‘negative’. Our
experiments use a 5000 sample training data set
and a pre-determined test set of 5000 points.

Beauty, Book, Music: These data sets each
consist of 6000 beauty product, book and music
reviews with a balanced distribution of three class
labels. This data set was introduced by (He et al.,
2018). Data is obtained by sampling from a larger
data set of product reviews obtained from Ama-
zon (McAuley et al., 2015). Labels are ‘positive’,
‘negative’ and ‘neutral’. An imbalanced setting
for each data set is also available, where roughly
80% of the data points are ‘positive’ and the re-
maining are ‘negative’ and ‘neutral’. Details about
number of data points for each data set can be
found in the supplement.

4.2 Baselines

For all test data sets, in addition to contrasting
the performance of the proposed adaptation layer
added to a CNN and BiLSTM encoder, we also
present comparisons against ‘Vanilla’ CNN and
BiLSTM encoders. For popular data sets like the
MR and SST data sets we present results against

some additional baselines, all of which are variants
of the basic vanilla RNN encoders. The baselines
that we used in this paper are,

BoW: In this standard baseline each sentence is
expressed as a weighted sum of its constituent
word embeddings. Weights used are raw word
counts. Both generic (GloVe) and DA word em-
beddings are used in this baseline. Note that the
DA embeddings used in this baseline are the same
as in (K Sarma et al., 2018), i.e α = β = 0.5

Vanilla CNN: A CNN based sentence classifica-
tion framework as introduced by (Kim, 2014) is
used as a ‘Vanilla’ CNN baseline.

Vanilla BiLSTM: A Basic BiLSTM encoder fol-
lowed by max pooling as proposed by (Conneau
et al., 2017) is used as the ‘Vanilla’ BiLSTM base-
line. Note that the vanilla baseline contains no ad-
ditional features like attention.

BERT: Bidirectional Encoder Representations
from Transformers (BERT) (Devlin et al., 2018) is
a generic encoder framework that learns sentence
embeddings by jointly conditioning on both left
and right context in all layers. The Bert encoder
is generic and requires a tuneable output layer to
be learned in order to perform a chosen task. In
our work, we use the pooled output from the Bert
encoder to obtain sentence representations for all
our data and then learn weights for a classifier to
perform classification on the given data set.

Self-attention: A model that generates inter-
pretable, structured sentence embeddings using
Self-attention mechanism (Lin et al., 2017).

LR-Bi-LSTM: A model that imposes linguistic
roles to Bi-LSTM architectures (Qian et al., 2017).

DAS: The Domain Adaptive Semi-supervised al-
gorithm (He et al., 2018) is a transfer learning
based solution that performs sentiment classifica-
tion. In their experiments, the authors train on re-
view data from different source domains such as
book, music and electronics and test on a target do-
main such as beauty. In contrast, our method does
not train on these three sources; rather, it uses only
the Beauty reviews to learn the DS word embed-
dings (and then combines these with the generic
pretrained embedding to learn the DA embedding
used for classification. On the balanced data sets,
metric reported is Accuracy, the same used by (He
et al., 2018). On the imbalanced data sets, in addi-
tion to accuracy we report micro f-score.



5555

4.3 Experimental methodology

The LibCon, MR and Beauty, Book and Mu-
sic data sets do not have dedicated train/dev/test
splits, so we created 80/10/10 splits. For the SST
dataset we make use of pre-defined test data set
of 2210 data points. When pre-tuned models are
available, such as for BERT, we further fine-tune
it using our train/dev splits. All the baselines use
the same splits of data.

Hyperparameters: All word embeddings-
GloVe, DS and KCCA projections used to obtain
the DA embeddings are of dimension 300. Both
the CNN and BiLSTM encoders learn sentence
embeddings of 300 dimensions on all data sets,
except on the LibCon data set where the CNN en-
coders have a filter size of 120. Dropout proba-
bility is set to 0.5. The rest of the hyperparame-
ters used are the same as in (He et al., 2018) and
are found in the supplement.When using BERT,
the ‘bert-base-uncased’ model is used to obtain
sentence embeddings. Pooled output from the
BERT encoder is used to represent the input sen-
tence. The classifier learned during tuning on the
train data sets is similar to the linear classifica-
tion layer introduced in the BERT classification
framework, with the exception of an additional
Tanh activation. The additional Tanh activation
function was used in order to obtain comparable
results for BERT and other baselines. The size
of test data is particularly challenging for a large
model like BERT that has several learnable param-
eters, even when used only for fine-tuning the pre-
trained model. The BERT classification frame-
work is reproduced for use in our experiments, re-
flecting the original tuning scripts to the best of
our ability. Additional information about hyper-
parameters can be found in the supplement.

4.4 Results

Table 2 presents results on the LibCon and the bal-
anced (B) and imbalanced (I) Beauty, Book and
Music data sets and Table 3 presents results on the
SST and MR data sets. The performance metric
reported in both tables is accuracy with with addi-
tional micro f-scores reported in Table 2.

From Table 2, it is observed that on the Lib-
Con data set, where we have a considerable differ-
ence in language use between the two groups of
users, the adapted BiLSTM and adapted CNN per-
form much better than the vanilla baselines. Fur-
thermore, our proposed adaptation layer improves

the performance of the Vanilla BiLSTM to surpass
the performance of fine-tuned BERT. Note that the
BERT encoder is pre-trained on all of Wikipedia,
a much larger training data set than used for train-
ing the Adapted BiLSTM encoder. The adapted
BiLSTM and CNN encoders also outperform DAS
on the multiclass Beauty, Book and Music data
sets. Performance of LSTMs for text surpass that
of CNNs as seen in current literature. This obser-
vation is consistent in our results as well. Since
the DAS baseline makes use of a CNN encoder,
it is expected that a very basic LSTM such as the
Vanilla BiLSTM would perform much better than
any CNN framework. Results Table 4 strengthens
our hypothesis that the adaptation layer is partic-
ularly well suited for small training and test data
regimes.

Note that on the MR data set, the performance
of fine-tuned BERT is lower than the BiLSTM
baselines. While the size of training data used
to optimize BERT, makes the encoder an excel-
lent generic sentence encoder, the small size of the
tuning data sets poses a considerable challenge.
This is because, when the BERT encoder is fine
tuned for a particular task, output from the en-
coder is treated as a fixed feature on top of which
it learns a task specific layer. Using pre-trained
BERT provides a high quality feature vector that
can be used for applications on generic data sets,
but this does not assure good performance when
tuned on a small data set where words may be used
idiosyncratically. On the other hand, on a highly
polarized data set like the LibCon data set, while
the distinct linguistic features enable the BERT en-
coder to capture good sentence embeddings, the
size of the data set still poses a challenge.

Varying Size of Training Data: To further illus-
trate the effectiveness of our method on data sets
with limited training data, we train the Vanilla and
adapted BiLSTM and CNN encoders with even
smaller samples of the training data. Our results
are consistent on varying size of data sets and per-
formance metrics for the same can be found in the
supplement. Table 4 compares accuracy on the
MR and SST data sets, of Vanilla and adapted en-
coders trained with 1000 and 2500 points. Repeat-
ing this experiment with BERT resulted in tremen-
dous over-fitting on the test set and so we do not
present results from BERT in Table 4.

Qualitative analysis of DA word embeddings:
Here we present a small analysis of the DA embed-



5556

Algorithm Acc on LibCon Acc on Beauty (B) Acc on Book (B) Acc on Music (B)
Beauty (I) Book (I) Music (I)

Acc F-score Acc F-score Acc F-score
BoW (GloVe) 64.0 56.18 69.1 66.5 74.8 74.8 66.08 66.08 75.8 75.8
BoW (DA embeddings, α = β = 0.5) 65.3 59.02 71.9 67.6 75.12 75.12 67.0 67.0 77.12 77.12
Vanilla CNN 61.36 61.7 66.5 61.5 74,5 74.6 76.8 78.1 77.5 77.5
Vanilla BiLSTM 68.5 75.6 77.6 80.0 84.3 84.3 83.4 83.4 84.2 84.2
Adapted CNN 63.24 62.8 71.0 63.5 80.1 75.0 78.0 76.6 78.1 78.1
Adapted BiLSTM 72.1 77.3 80.3 81.5 85.2 85.2 84.7 84.7 85.6 85.6
BERT 70.3 - - - - - - - - -
DAS N/A 56.0 67.6 58.6 54.8 - 61.06 - 55.1 -

Table 2: This table reports performance (accuracy score) of the baseline algorithms on the LibCon and Beauty,
Book and Music data sets in balanced and imbalanced setting. Micro F-score is reported on the imbalanced Beauty,
Book and Music data sets as well. Bold face indicates best performing algorithm.

Algorithm Accuracy on MR Accuracy on SST
BoW (Generic) 75.7* 48.9*
BoW (DA embeddings) 77.0* 49.2*
Vanilla CNN 72.5* 49.06*
Vanilla BiLSTM 81.8* 50.3*
LR-Bi-LSTM 82.1 50.6
Self-attention 81.7 48.9
Adapted CNN 80.8* 50.0*
Adapted BiLSTM 83.1* 51.2
BERT 74.4* 51.5

Table 3: This table reports performance (Accuracy) on
the MR and SST data sets. Results with * indicate that
performance metric is reported on the test dataset after
training on a subset of the original data set. Bold face
indicates best performing algorithm

Algorithm Accuracy on MR Accuracy on SST
Vanilla CNN (1000 pts) 65.1 48.8
Adapted CNN (1000 pts) 74.7 49.6
Vanilla BiLSTM(1000 pts) 76.9 50.08
Adapted BiLSTM (1000 pts) 78.1 50.3
Vanilla CNN (2500 pts) 66.5 49.0
Adapted CNN (2500 pts) 77.4 50.7
Vanilla BiLSTM(2500 pts) 78.8 50.2
Adapted BiLSTM (2500 pts) 80.1 51.0

Table 4: This table presents the accuracy obtained by
Vanilla and adapted baselines on smaller subsamples of
the training data for the MR and SST data sets.

dings obtained from the weights learned via the
shallow adaptation layer. The analysis presented
here is on the Book review data set consisting
of roughly 10000 unique word tokens. Weights
learned via the CNN encoder on this data set are
α = 0.7145 and β = 0.3994. To compare the
DA embeddings against the GloVe common crawl
embeddings, we first standardize both sets of em-
beddings to have an average norm of 1. Then we
compute shift via the l2 distance as described in
Section 3.1. Out of 100 most shifted words, Ad-
jectives (21%) and Adverbs (22%) such as ‘emo-
tional’, ‘profound’ and ‘emotionally’ and ‘obvi-
ously’ shift the most. These words are used ex-
tensively when writing reviews about books. On
the other hand words that shift the least are Nouns

(69%) such as ‘Higgins’, ‘Gardner’ and ‘Schaf-
fer’ that correspond to author and character names.
This observation is consistent with the domain
adaptation hypothesis. A data set of book reviews
will use Adjectives and Adverbs differently in-
domain as opposed to out-of-domain. On the other
hand, Nouns and Noun Phrases such as character
and author names remain fixed regardless.

5 Conclusions and Future Work

This paper shows that domain semantics captured
in adapted word embeddings can improve the
performance of (pre-trained) encoders in down-
stream tasks such as sentiment classification; par-
ticularly on data sets where obtaining data suffi-
cient enough for tuning pre-trained encoders or
for end-to-end training is difficult. Experiments
show the effectiveness of the method in binary
and multiclass classification tasks. The proposed
framework outperforms competing transfer learn-
ing based algorithms in overcoming limitations
posed by the size of training data, while learn-
ing fewer parameters than an end-to-end trained
network along with obtaining better results on
classification tasks. Recent work that focuses on
capturing domain semantics in word embeddings,
demonstrate the effectiveness of such approaches
mostly via performance on a downstream task
such as sentiment analysis/classification alone. In
Section 3.1 we provide a simple yet effective
demonstration of how techniques such as KCCA,
that are used to capture domain semantics, can
capture relevant semantics in the learned word
embeddings. As future work, we will interface
the adaptation layer for use recently introduced
large scale language models such as BERT. Due
to size and memory complexities of available im-
plementation of large scale language models such
as BERT, an easy integration with our proposed
adaptation framework does not currently exist.



5557

References
Jisun An, Haewoon Kwak, and Yong-Yeol Ahn. 2018.

Semaxis: A lightweight framework to characterize
domain-specific word semantics beyond sentiment.
In Proceedings of the 56th Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers), pages 2450–2461. Association for
Computational Linguistics.

Alexis Conneau, Douwe Kiela, Holger Schwenk, Loı̈c
Barrault, and Antoine Bordes. 2017. Supervised
learning of universal sentence representations from
natural language inference data. In Proceedings of
the 2017 Conference on Empirical Methods in Nat-
ural Language Processing, pages 670–680, Copen-
hagen, Denmark. Association for Computational
Linguistics.

Scott Deerwester, Susan T Dumais, George W Fur-
nas, Thomas K Landauer, and Richard Harshman.
1990. Indexing by latent semantic analysis. Jour-
nal of the American society for information science,
41(6):391–407.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. arXiv preprint arXiv:1810.04805.

Wells Friedland, Wagner Shah, and Abhishek. 2017.
The civic state under threat: How social, political
and media changes eroded wisconsin’s civic culture.

William L. Hamilton, Kevin Clark, Jure Leskovec, and
Dan Jurafsky. 2016. Inducing domain-specific senti-
ment lexicons from unlabeled corpora. In Proceed-
ings of the 2016 Conference on Empirical Methods
in Natural Language Processing, pages 595–605,
Austin, Texas. Association for Computational Lin-
guistics.

Viktor Hangya, Fabienne Braune, Alexander Fraser,
and Hinrich Schütze. 2018. Two methods for do-
main adaptation of bilingual tasks: Delightfully sim-
ple and broadly applicable. In Proceedings of the
56th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
810–820. Association for Computational Linguis-
tics.

Ruidan He, Wee Sun Lee, Hwee Tou Ng, and Daniel
Dahlmeier. 2018. Adaptive semi-supervised learn-
ing for cross-domain sentiment classification. In
Proceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing, pages
3467–3476. Association for Computational Linguis-
tics.

Prathusha K Sarma, Yingyu Liang, and William
Sethares. 2018. Domain adapted word embeddings
for improved sentiment classification. In Proceed-
ings of the 56th Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Pa-
pers), pages 37–42. Association for Computational
Linguistics.

Yoon Kim. 2014. Convolutional neural networks
for sentence classification. In Proceedings of the
2014 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 1746–1751,
Doha, Qatar. Association for Computational Lin-
guistics.

Ping Li, Benjamin Schloss, and D Jake Follmer.
2017. Speaking two languages in america: A se-
mantic space analysis of how presidential candi-
dates and their supporters represent abstract politi-
cal concepts differently. Behavior research meth-
ods, 49(5):1668–1685.

Zhouhan Lin, Minwei Feng, Cicero Nogueira dos San-
tos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua
Bengio. 2017. A structured self-attentive sentence
embedding. arXiv preprint arXiv:1703.03130.

Pengfei Liu, Xipeng Qiu, and Xuanjing Huang.
2016. Recurrent neural network for text classi-
fication with multi-task learning. arXiv preprint
arXiv:1605.05101.

Qi Liu, Yue Zhang, and Jiangming Liu. 2018. Learning
domain representation for multi-domain sentiment
classification. In Proceedings of the 2018 Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, Volume 1 (Long Papers), pages
541–550. Association for Computational Linguis-
tics.

Julian McAuley, Christopher Targett, Qinfeng Shi, and
Anton Van Den Hengel. 2015. Image-based recom-
mendations on styles and substitutes. In Proceed-
ings of the 38th International ACM SIGIR Confer-
ence on Research and Development in Information
Retrieval, pages 43–52. ACM.

Tomas Mikolov, Quoc V Le, and Ilya Sutskever. 2013a.
Exploiting similarities among languages for ma-
chine translation. arXiv preprint arXiv:1309.4168.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in neural information processing
systems, pages 3111–3119.

Hyeonseob Nam and Bohyung Han. 2016. Learning
multi-domain convolutional neural networks for vi-
sual tracking. In Proceedings of the IEEE Confer-
ence on Computer Vision and Pattern Recognition,
pages 4293–4302.

Bo Pang and Lillian Lee. 2005. Seeing stars: Exploit-
ing class relationships for sentiment categorization
with respect to rating scales. In Proceedings of the
43rd annual meeting on association for computa-
tional linguistics, pages 115–124. Association for
Computational Linguistics.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word

http://aclweb.org/anthology/P18-1228
http://aclweb.org/anthology/P18-1228
https://www.aclweb.org/anthology/D17-1070
https://www.aclweb.org/anthology/D17-1070
https://www.aclweb.org/anthology/D17-1070
https://aclweb.org/anthology/D16-1057
https://aclweb.org/anthology/D16-1057
http://aclweb.org/anthology/P18-1075
http://aclweb.org/anthology/P18-1075
http://aclweb.org/anthology/P18-1075
http://aclweb.org/anthology/D18-1383
http://aclweb.org/anthology/D18-1383
http://aclweb.org/anthology/P18-2007
http://aclweb.org/anthology/P18-2007
http://www.aclweb.org/anthology/D14-1181
http://www.aclweb.org/anthology/D14-1181
https://doi.org/10.18653/v1/N18-1050
https://doi.org/10.18653/v1/N18-1050
https://doi.org/10.18653/v1/N18-1050
http://www.aclweb.org/anthology/D14-1162


5558

representation. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 1532–1543, Doha,
Qatar. Association for Computational Linguistics.

Matthew Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word repre-
sentations. In Proceedings of the 2018 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume 1 (Long Papers), pages 2227–
2237. Association for Computational Linguistics.

Qiao Qian, Minlie Huang, Jinhao Lei, and Xiaoyan
Zhu. 2017. Linguistically regularized lstm for senti-
ment classification. In Proceedings of the 55th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1679–
1689. Association for Computational Linguistics.

Jingbo Shang, Liyuan Liu, Xiaotao Gu, Xiang Ren,
Teng Ren, and Jiawei Han. 2018. Learning named
entity tagger using domain-specific dictionary. In
Proceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing, pages
2054–2064. Association for Computational Linguis-
tics.

Jiali Zeng, Jinsong Su, Huating Wen, Yang Liu,
Jun Xie, Yongjing Yin, and Jianqiang Zhao. 2018.
Multi-domain neural machine translation with word-
level domain context discrimination. In Proceedings
of the 2018 Conference on Empirical Methods in
Natural Language Processing, pages 447–457. As-
sociation for Computational Linguistics.

http://www.aclweb.org/anthology/D14-1162
https://doi.org/10.18653/v1/N18-1202
https://doi.org/10.18653/v1/N18-1202
https://doi.org/10.18653/v1/P17-1154
https://doi.org/10.18653/v1/P17-1154
http://aclweb.org/anthology/D18-1230
http://aclweb.org/anthology/D18-1230
http://aclweb.org/anthology/D18-1041
http://aclweb.org/anthology/D18-1041

