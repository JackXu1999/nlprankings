



















































Transition-Based Chinese AMR Parsing


Proceedings of NAACL-HLT 2018, pages 247–252
New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics

Transition-based Chinese AMR Parsing

Chuan Wang1 Bin Li2 Nianwen Xue1
1Michtom School of Computer Science

Brandeis University
{cwang24;xuen}@brandeis.edu

2School of Chinese Language and Literature
Nanjing Normal University
libin.njnu@gmail.com

Abstract

This paper presents the first AMR parser built
on the Chinese AMR bank. By applying a
transition-based AMR parsing framework to
Chinese, we first investigate how well the tran-
sitions first designed for English AMR parsing
generalize to Chinese and provide a compar-
ative analysis between the transitions for En-
glish and Chinese. We then perform a detailed
error analysis to identify the major challenges
in Chinese AMR parsing that we hope will in-
form future research in this area.

1 Introduction

Abstract Meaning Representation (AMR) (Ba-
narescu et al., 2013) is a semantic representation
where the meaning of a sentence is encoded as a
rooted, directed and acyclic graph. AMR parsing
has received a significant amount of attention in
the NLP research community. Since the release of
the AMR bank a number of AMR parsers have
been developed in recent years (Flanigan et al.,
2014; Wang et al., 2015b; Artzi et al., 2015; Pust
et al., 2015; Peng et al., 2015; Zhou et al., 2016;
Goodman et al., 2016). The initial benefit of AMR
parsing has also been demonstrated in various
downstream applications such as Information Ex-
traction (Pan et al., 2015; Huang et al., 2016), Ma-
chine Comprehension (Sachan and Xing, 2016),
and Natural Language Generation (Flanigan et al.,
2016; Butler, 2016).

In this paper, we present the first AMR parser
built using the Chinese AMR Bank (Li et al.,
2016). We adopt the transition-based parsing
framework first proposed for English (Wang et al.,
2015b, 2016), where AMR parsing is modeled as
a dependency tree to AMR graph transformation
using a set of linguistically motivated actions. We
briefly describe the Chinese AMR Bank in Sec-
tion 2, present the transition-based Chinese AMR

parsing model in Section 3, report and analyze ex-
perimental results in Section 4, and conclude our
paper in Section 5.

2 The Chinese AMR Bank

In our experiment, we use a pre-release version of
the Chinese AMR Bank (Li et al., 2016)1 consist-
ing of 10,149 sentences extracted from the Chi-
nese Treebank (CTB) 8.0 (Xue et al., 2005) 2,
which mainly consists of Chinese texts of web
logs and discussion forums. The average sentence
length is 22.43 words.

Similar to English, the Chinese AMRs are
also represented as rooted, directed and acyclic
graphs that share the abstract concepts and re-
lations used in the English AMR Bank. The
sense-disambiguated predicates are drawn from
the frame files developed for the Chinese Prop-
bank(Xue and Palmer, 2009), just as the sense-
disambiguated predicates in the AMR Bank are
drawn from the Propbank (Palmer et al., 2005).
About 47% of the 10,149 sentences have re-
entrancies, meaning that they have a graph struc-
ture that cannot be represented with a tree repre-
sentation.

3 Transition-based AMR Parsing

In a transition-based AMR parsing framework an
input sentence is first parsed into a dependency
tree and then transformed into an AMR graph via
a series of transitions formulated as “actions”. The
full set of actions are summarized in Table 1, and
we refer the reader to (Wang et al., 2015b,a) for
details regarding the training procedure and de-
coding algorithm. Note that NEXT-EDGE-lr and
NEXT-NODE-lc are action to label the current node

1http://www.cs.brandeis.edu/˜clp/camr/
camr.html.

2Available at https://catalog.ldc.upenn.
edu/LDC2013T21.

247



or current edge, where the candidate label is de-
fined as a parameter to the action. The INFER-
lc (ifr) is devised to predict abstract concepts that
are not aligned to any specific word in a sentence.
The rest of the actions are responsible for trans-
forming the structure of the partial graph.

4 Experiments

In this section, we present experiments designed
to probe the behavior of our Chinese AMR parser,
and where appropriate, compare it to its English
counterpart. We also devise several ablation tests
to further investigate the errors produced by our
Chinese AMR parser to gain insight that can be
used to guide future research.

4.1 Experiment Settings

We use the 10,149 sentences from the Chinese
AMR Bank and split the data according to their
original CTB8.0 document IDs, where articles
5061-5558 are used as the training set, arti-
cles 5000-5030 are used as the development set
and articles 5031-5060 are used as the test set.
The train/development/test ratio in this dataset
is 7608/1264/1277. As the data are drawn from
the Chinese Treebank where words are manu-
ally segmented, we will simply use the gold seg-
mentation in our experiments. We then process
the whole Chinese dataset using the Stanford
CoreNLP (Manning et al., 2014) toolkit to get
the POS and Named Entity tags. To get the de-
pendency parse for the Chinese data, we use the
transition-based constituent parser in (Wang and
Xue, 2014) to first parse the Chinese sentences
into constituent trees, which are then transformed
into dependency trees using the converter in the
Stanford CoreNLP toolkit. Note that this Chinese
constituent parser also uses the Chinese Treebank
8.0 to train its model. To avoid training on the
parser on AMR test set, we train the constituent
parser using a 10-fold cross-validation with each
fold parsed using a model trained on the other 9
folds. In order to compare results between Chi-
nese and English, we also train an English AMR
parsing model on the LDC2015E86 dataset used
in SemEval 2016 Task 8 with the standard split
16833/1368/1371 and the English AMR parser,
CAMR, is utilized to train the English model. All
the AMR parsing results are evaluated by the

Smatch toolkit (Cai and Knight, 2013)3.

4.2 Action Distribution

Before we train the parser, we first perform a quan-
titative comparison of the actions that are invoked
in English and Chinese AMR parsing. We run
the oracle function separately on the training data
of both languages and record the distribution of
the actions invoked, as shown in Figure 1. Note
that without any modification of the action set de-
signed for English, the “pseudo-gold” graphs gen-
erated by the oracle function have reached F1-
score of 0.99 when evaluated against gold Chi-
nese AMR graphs, and this indicates that the ac-
tion set is readily generalizable to Chinese. The
histograms in Figure 1 shows the distribution of
action types for both English and Chinese. We
leave out the NEXT-EDGE-lr and NEXT-NODE-
lc actions in the histogram as they do not trigger
structural transformations like other actions, and
thus are not our point of interest.

In Figure 1 we can see that there is a large dif-
ference in action distribution between Chinese and
English. First of all, there are a lot fewer DELETE-
NODE actions applied in the dependency-to-AMR
transformation process for Chinese, which indi-
cates that in Chinese data there is a smaller per-
centage of “stop words” that do not encode seman-
tic information. Also, in the Chinese data, more
INFER-lc actions are invoked than in English, im-
plying that Chinese AMRs use more abstract con-
cepts that don’t align to any word token.

0.00!
0.02!
0.04!
0.06!
0.08!
0.10!
0.12!

Re
att

ac
h!

De
let

e-n
od

e!
Sw

ap
!

Re
en

tra
nc

e!

Re
pla

ce
-he

ad
!

Me
rge

!
Inf

er!

English actions!
Chinese actions!

Figure 1: Action distribution on English and Chinese

To further investigate the different linguistic
patterns associated with each action in the two lan-
guages, for each action type t, we randomly sam-
ple 100 sentences in which action t is invoked
for both English and Chinese. We then conduct

3http://alt.qcri.org/semeval2016/
task8/data/uploads/smatch-v2.0.2.tar.gz

248



Action Description
NEXT-EDGE-lr (ned) Assign the current edge with edge label lr and go to next edge.

SWAP-lr (sw) Swap the current edge, make the current dependent as the new head, and
assign edge label lr to the swapped edge.

REATTACHk-lr (reat) Reattach current dependent to node k and assign label lr to new edge.
REPLACE-HEAD (rph) Replace current head node with current dependent node.

REENTRANCEk-lr (reen) Add another head node k to current dependent and assign label lr to
edge between k and current dependent.

MERGE (mrg) Merge two nodes connected by the edge into one node.
NEXT-NODE-lc (nnd) Assign the current node with concept label lc and go to next node.
DELETE-NODE (dnd) Delete the current node and all edges associated with current node.

INFER-lc (ifr) Insert concept with label lc between current node and its parent.

Table 1: Action set in Chinese AMR Parsing, where k,lr,lc are parameters of the action.

a detailed analysis of the sampled data. We find
that MERGE is mostly responsible for combining
spans of words to form a named entity in En-
glish parsing. However, in Chinese AMR pars-
ing, in addition to forming named entity concepts,
MERGE also handles a large portion of split verb
constructions. A “split verb” is a linguistic phe-
nomenon in Chinese in which the characters in
a multi-character verb are split into two discon-
tinuous parts by other lexical items. For exam-
ple, in (1), the sentence has a split verb “帮 /help
· · · 忙/business” that are merged by the MERGE
action to form the AMR concept “帮忙-01”, as
shown in Figure 2.

In the cases of SWAP and REPLACE-HEAD, we
notice that the linguistic patterns associated with
the two actions are mostly consistent across the
two languages. For example, as we already men-
tioned, the SWAP action is used to handle the struc-
tural divergence between the dependency tree and
AMR graph of coordination constructions. This
holds for both English and Chinese. Similarly, the
REPLACE-HEAD action is designed to resolve the
structural divergence between the dependency tree
and AMR graph of propositional phrases. Based
on our analysis of sampled data, the REPLACE-
HEAD action resolves the same dependency-AMR
divergence in Chinese AMR parsing.

(1) 他1帮2 了3 我4很5 大6的7忙8。
He helped PAST me very big DE business
“He helped me a lot.”

Being able to identify the linguistic environ-
ment for each action helps us understand what
the parser actually does when actions are applied.
More importantly, making the relation between the

Figure 2: AMR for Example (1)

linguistic structure and the parser actions trans-
parent is crucial to our ability to devise effective
features for the parsing model which directly im-
pacts the performance of the parser. For exam-
ple, knowing that the MERGE action is responsi-
ble for producing concepts from split verb con-
structions helps us understand the need to design
character-level features in addition to features tar-
geting named entities.

4.3 Main results for Chinese AMR Parsing

Using the configuration in Section 4.1, we train
our Chinese AMR parser with 5 iterations and re-
port results on both the development and test set.

0.584!

0.542!

0.560!

0.608!

0.567!

0.587!

0.500!

0.520!

0.540!

0.560!

0.580!

0.600!

0.620!

Precision! Recall! F-score!

Dev!
Test!

Figure 3: Parsing performance on the development and
test set

Figure 3 presents the parsing performance on
the development and test set in terms of the

249



Smatch score. Compared with the state of the art
in English AMR parsing, which is in the high 60
percentage points (May, 2016), this initial pars-
ing performance here is very strong, considering
the model is trained on a smaller training set. The
Chinese AMR parsing model also does not ben-
efit from the more extensive feature engineering
that has been done for English AMR parsing. For
example, the English AMR parser, CAMR, uses se-
mantic roles and coreference features that are not
available to the Chinese AMR parser. The other
important factor is that most of the Chinese lin-
guistic analyzers (dependency parsers, named en-
tity taggers, etc.) have a lower accuracy than their
English counterparts, and when used as preproces-
sors for the AMR parser, could further disadvan-
tage the Chinese AMR parsing model.

4.4 Fine-grained Error Analysis
So far, all of our experiments are evaluated using
the Smatch score, where only precision, recall and
F-score are reported based on the overall perfor-
mance of the parser. To gain more insights, we fur-
ther break down the Smatch score and report the
performance for each component using the evalu-
ation tool from Damonte et al. (2017). The evalu-
ation tool examines different aspects of the AMR
parsing result through different ablation tests that
we summarize as follows. The detailed description
of the ablation test can be found in Damonte et al.
(2017).

• Unlabeled. Smatch score obtained by ignor-
ing the edge labels (relation).

• No WSD. Smatch score without the word
sense disambiguation.

• NP (Noun Phrase)-only. Only evaluating the
noun phrases.

• Reentrancy. Only evaluating reentrancy
edges.

• Concepts. Evaluating the node labels (con-
cept).

• Named Ent. Named entity evaluation.
• Negation. Evaluation on negation detection.
• SRL. Semantic Role Labeling, which only

evaluates triples in AMR that have relations
starting with :ARG.

Note that we simply ignore the wikification
evaluation as Chinese AMRs do not have wikifi-
cation annotation at the current stage.

0.00!
0.10!
0.20!
0.30!
0.40!
0.50!
0.60!
0.70!
0.80!
0.90!

Sm
atc

h!

Un
lab

ele
d!

No
 W

SD
!

Na
me

d E
nt.
!

Ne
ga

tio
n!

Re
en

tra
nc

ies
!

SR
L!

English AMR Parsing! Chinese AMR Parsing!

Figure 4: Fine-grained AMR parsing evaluation on dev

Figure 4 shows the performance breakdown on
the Chinese and English development sets, where
we can see that the overall performance gap be-
tween English and Chinese is around 0.11 Smatch
score and there is a similar gap for Unlabeled, No
WSD and SRL evaluations. However, the largest
performance comes from Named Ent., where the
F-score for Chinese is 0.55 which is 0.25 lower
than English. This indicates that named entity is
one of the bottlenecks in Chinese AMR parsing.
This indicates that improving named entity recog-
nition, either as a preprocessing step or as an inte-
gral part of the parsing model, is crucial to Chinese
AMR parsing.

5 Conclusion

We present the first Chinese AMR parser trained
on the Chinese AMR Bank. We show that a
transition-based AMR parsing framework first
proposed for English is general enough to handle
the linguistic phenomena in Chinese and has pro-
duced a strong baseline that future research can
build on. In addition, we perform a detailed com-
parative analysis of the transition distributions for
English and Chinese as well as errors in Chinese
AMR parsing that we hope will inform future Chi-
nese AMR parsing research.

Acknowledgements

We want to thank the anonymous reviewers for
their suggestions. We also want to thank Yuan
Wen, Lijun Bu and Li Song on their contributions
to the creation of the Chinese AMR corpus. The
second author would also like to acknowledge sup-
port by the National Science Foundation of China
(61772278, 61472191).

250



References
Yoav Artzi, Kenton Lee, and Luke Zettlemoyer. 2015.

Broad-coverage CCG semantic parsing with AMR.
In Proceedings of the 2015 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1699–1710, Lisbon, Portugal. Association for Com-
putational Linguistics.

Laura Banarescu, Claire Bonial, Shu Cai, Madalina
Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin
Knight, Philipp Koehn, Martha Palmer, and Nathan
Schneider. 2013. Abstract meaning representation
for sembanking. In Proceedings of the 7th Linguis-
tic Annotation Workshop and Interoperability with
Discourse, pages 178–186, Sofia, Bulgaria.

Alastair Butler. 2016. Deterministic natural lan-
guage generation from meaning representations for
machine translation. In Proceedings of the 2nd
Workshop on Semantics-Driven Machine Transla-
tion (SedMT 2016), pages 1–9. Association for
Computational Linguistics.

Shu Cai and Kevin Knight. 2013. Smatch: an evalua-
tion metric for semantic feature structures. In Pro-
ceedings of the 51st Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 2: Short
Papers), pages 748–752. Association for Computa-
tional Linguistics.

Marco Damonte, Shay B. Cohen, and Giorgio Satta.
2017. An incremental parser for abstract meaning
representation. In Proceedings of the 15th Confer-
ence of the European Chapter of the Association
for Computational Linguistics: Volume 1, Long Pa-
pers, pages 536–546, Valencia, Spain. Association
for Computational Linguistics.

Jeffrey Flanigan, Chris Dyer, A. Noah Smith, and
Jaime Carbonell. 2016. Generation from Abstract
Meaning Representation using tree transducers. In
Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 731–739. Association for Computational Lin-
guistics.

Jeffrey Flanigan, Sam Thomson, Jaime Carbonell,
Chris Dyer, and Noah A. Smith. 2014. A Discrim-
inative Graph-Based Parser for the Abstract Mean-
ing Representation. In Proceedings of the 52nd An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1426–
1436, Baltimore, Maryland. Association for Compu-
tational Linguistics.

James Goodman, Andreas Vlachos, and Jason Narad-
owsky. 2016. Noise reduction and targeted explo-
ration in imitation learning for abstract meaning rep-
resentation parsing. In Proceedings of the 54th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1–11.
Association for Computational Linguistics.

Lifu Huang, Taylor Cassidy, Xiaocheng Feng, Heng Ji,
R. Clare Voss, Jiawei Han, and Avirup Sil. 2016.
Liberal event extraction and event schema induction.
In Proceedings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 258–268. Association for Com-
putational Linguistics.

Bin Li, Yuan Wen, Q. U. Weiguang, Lijun Bu, and Ni-
anwen Xue. 2016. Annotating the little prince with
chinese amrs. In Linguistic Annotation Workshop
Held in Conjunction with ACL, pages 7–15.

Christopher Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven Bethard, and David McClosky.
2014. The stanford corenlp natural language pro-
cessing toolkit. In Proceedings of 52nd Annual
Meeting of the Association for Computational Lin-
guistics: System Demonstrations, pages 55–60, Bal-
timore, Maryland. Association for Computational
Linguistics.

Jonathan May. 2016. Semeval-2016 task 8: Mean-
ing representation parsing. Proceedings of SemEval,
pages 1063–1073.

Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71–105.

Xiaoman Pan, Taylor Cassidy, Ulf Hermjakob, Heng Ji,
and Kevin Knight. 2015. Unsupervised entity link-
ing with abstract meaning representation. In Pro-
ceedings of the 2015 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
1130–1139.

Xiaochang Peng, Linfeng Song, and Daniel Gildea.
2015. A synchronous hyperedge replacement gram-
mar based approach for AMR parsing. In Proceed-
ings of the Nineteenth Conference on Computational
Natural Language Learning.

Michael Pust, Ulf Hermjakob, Kevin Knight, Daniel
Marcu, and Jonathan May. 2015. Parsing English
into abstract meaning representation using syntax-
based machine translation. In Proceedings of the
2015 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1143–1154, Lis-
bon, Portugal. Association for Computational Lin-
guistics.

Mrinmaya Sachan and Eric Xing. 2016. Machine
comprehension using rich semantic representations.
In Proceedings of the 54th Annual Meeting of the
Association for Computational Linguistics (Volume
2: Short Papers), pages 486–492. Association for
Computational Linguistics.

Chuan Wang, Sameer Pradhan, Xiaoman Pan, Heng
Ji, and Nianwen Xue. 2016. CAMR at semeval-
2016 task 8: An extended transition-based AMR
parser. In Proceedings of the 10th International

251



Workshop on Semantic Evaluation (SemEval-2016),
pages 1173–1178, San Diego, California. Associa-
tion for Computational Linguistics.

Chuan Wang, Nianwen Xue, and Sameer Pradhan.
2015a. Boosting transition-based AMR parsing
with refined actions and auxiliary analyzers. In Pro-
ceedings of the 53rd Annual Meeting of the Associ-
ation for Computational Linguistics and the 7th In-
ternational Joint Conference on Natural Language
Processing (Short Papers), pages 857–862.

Chuan Wang, Nianwen Xue, and Sameer Pradhan.
2015b. A transition-based algorithm for AMR pars-
ing. In Proceedings of the 2015 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 366–375, Denver, Colorado. Asso-
ciation for Computational Linguistics.

Zhiguo Wang and Nianwen Xue. 2014. Joint pos tag-
ging and transition-based constituent parsing in chi-
nese with non-local features. In Proceedings of the
52nd Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers),
pages 733–742, Baltimore, Maryland. Association
for Computational Linguistics.

Nianwen Xue and Martha Palmer. 2009. Adding se-
mantic roles to the chinese treebank. Natural Lan-
guage Engineering, 15(1):143–172.

Nianwen Xue, Fei Xia, Fudong Chiou, and Martha
Palmer. 2005. The penn chinese treebank: Phrase
structure annotation of a large corpus. Natural Lan-
guage Engineering, 11(2):207–238.

Junsheng Zhou, Feiyu Xu, Hans Uszkoreit, Weiguang
QU, Ran Li, and Yanhui Gu. 2016. AMR Parsing
with an Incremental Joint Model. In Proceedings of
the 2016 Conference on Empirical Methods in Nat-
ural Language Processing, pages 680–689, Austin,
Texas. Association for Computational Linguistics.

252


