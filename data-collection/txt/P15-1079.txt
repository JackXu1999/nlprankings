



















































String-to-Tree Multi Bottom-up Tree Transducers


Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 815–824,

Beijing, China, July 26-31, 2015. c©2015 Association for Computational Linguistics

String-to-Tree Multi Bottom-up Tree Transducers

Nina Seemann and Fabienne Braune and Andreas Maletti
Institute for Natural Language Processing, University of Stuttgart

Pfaffenwaldring 5b, 70569 Stuttgart, Germany
{seemanna,braunefe,maletti}@ims.uni-stuttgart.de

Abstract

We achieve significant improvements in
several syntax-based machine translation
experiments using a string-to-tree vari-
ant of multi bottom-up tree transducers.
Our new parameterized rule extraction al-
gorithm extracts string-to-tree rules that
can be discontiguous and non-minimal
in contrast to existing algorithms for the
tree-to-tree setting. The obtained models
significantly outperform the string-to-tree
component of the Moses framework in a
large-scale empirical evaluation on several
known translation tasks. Our linguistic
analysis reveals the remarkable benefits of
discontiguous and non-minimal rules.

1 Introduction

We present an application of a variant of local
multi bottom-up tree transducers (`MBOTs) as
proposed in Maletti (2011) to statistical machine
translation. `MBOTs allow discontinuities on the
target language side since they have a sequence
of target tree fragments instead of a single tree
fragment in their rules. The original approach
makes use of syntactic information on both the
source and the target side (tree-to-tree) and a cor-
responding minimal rule extraction is presented
in (Maletti, 2011). Braune et al. (2013) imple-
mented it as well as a decoder inside the Moses
framework (Koehn et al., 2007) and demonstrated
that the resulting tree-to-tree `MBOT system sig-
nificantly improved over its tree-to-tree baseline
using minimal rules. We can see at least two draw-
backs in this approach. First, experiments investi-
gating the integration of syntactic information on
both sides generally report quality deterioration.
For example, Lavie et al. (2008), Liu et al. (2009),
and Chiang (2010) noted that translation quality
tends to decrease in tree-to-tree systems because

the rules become too restrictive. Second, minimal
rules (i.e., rules that cannot be obtained from other
extracted rules) typically consist of a few lexi-
cal items only and are thus not the most suitable
to translate idiomatic expressions and other fixed
phrases. To overcome these drawbacks, we abol-
ish the syntactic information for the source side
and develop a string-to-tree variant of `MBOTs.
In addition, we develop a new rule extraction algo-
rithm that can also extract non-minimal rules. In
general, the number of extractable rules explodes,
so our rule extraction places parameterized restric-
tions on the extracted rules in the same spirit as
in (Chiang, 2007). In this manner, we combine the
advantages of the hierarchical phrase-based ap-
proach on the source side and the tree-based ap-
proach with discontinuiety on the target side.

We evaluate our new system in 3 large-scale ex-
periments using translation tasks, in which we ex-
pect discontinuiety on the target. MBOTs are pow-
erful but asymmetric models since discontinuiety
is available only on the target. We chose to trans-
late from English to German, Arabic, and Chi-
nese. In all experiments our new system signifi-
cantly outperforms the string-to-tree syntax-based
component (Hoang et al., 2009) of Moses. The
(potentially) discontiguous rules of our model are
very useful in these setups, which we confirm in a
quantitative and qualitative analysis.

2 Related work

Modern statistical machine translation sys-
tems (Koehn, 2009) are based on different
translation models. Syntax-based systems have
become widely used because of their ability to
handle non-local reordering and other linguistic
phenomena better than phrase-based models (Och
and Ney, 2004). Synchronous tree substitution
grammars (STSGs) of Eisner (2003) use a single
source and target tree fragment per rule. In con-
trast, an `MBOT rule contains a single source tree

815



concludes X →
( VAFIN

ist
, NP ,

VP

PP geschlossen

)

X on X →
(

NP ,
PP

über NN

)

human rights →
( NN

Menschenrechte

)
the X →

( NP
die NN

)

Figure 1: Several valid rules for our MBOT.

fragment and a sequence of target tree fragments.
`MBOTs can also be understood as a restriction of
the non-contiguous STSSGs of Sun et al. (2009),
which allow a sequence of source tree fragments
and a sequence of target tree fragments. `MBOT
rules require exactly one source tree fragment.

While the mentioned syntax-based models use
tree fragments for source and target (tree-to-tree),
Galley et al. (2004) and Galley et al. (2006) use
syntactic annotations only on the target language
side (string-to-tree). Further research by DeNeefe
et al. (2007) revealed that adding non-minimal
rules improves translation quality in this setting.
Here we improve statistical machine translation
in this setting even further using non-minimal
`MBOT rules.

3 Theoretical Model

As our translation model, we use a string-to-tree
variant of the shallow local multi bottom-up tree
transducer of Braune et al. (2013). We will call
our variant MBOT for simplicity. Our MBOT is
a synchronous grammar (Chiang, 2006) similar to
a synchronous context-free grammar (SCFG), but
instead of a single source and target fragment per
rule, our rules are of the form s → (t1, . . . , tn)
with a single source string s and potentially sev-
eral target tree fragments t1, . . . , tn. Besides lex-
ical items the source string can contain (several
occurrences of) the placeholder X, which links to
non-lexical leaves in the target tree fragments. In
contrast to an SCFG each placeholder can have
several such links. However, each non-lexical leaf
in a target tree fragment has exactly one such link
to a placeholder X. An MBOT is simply a finite
collection of such rules. Several valid rules are
depicted in Figure 1.

The sentential forms of our MBOTs, which
occur during derivations, have exactly the same
shape as our rules and each rule is a sentential

Matching sentential forms (underlining for emphasis):

concludes X →
( VAFIN

ist
, NP ,

VP

PP geschlossen

)

X on X →
(

NP ,
PP

über NN

)

Combined sentential form:

concludes X on X →
( VAFIN

ist
, NP ,

VP

PP

über NN

geschlossen
)

Figure 2: Substitution of sentential forms.

form. We can combine sentential forms with the
help of substitution (Chiang, 2006). Roughly
speaking, in a sentential form ξ we can replace
a placeholder X that is linked (left-to-right) to
non-lexical leaves C1, . . . , Ck in the target tree
fragments by the source string of any sentential
form ζ, whose roots of the target tree fragments
(left-to-right) read C1, . . . , Ck. The target tree
fragments of ζ will replace the respective linked
leaves in the target tree fragments of the sentential
form ξ. In other words, substitution has to respect
the symbols in the linked target tree fragments and
all linked leaves are replaced at the same time. We
illustrate substitution in Figure 2, where we re-
place the placeholder X in the source string, which
is linked to the underlined leaves NP and PP in the
target tree fragments. The rule below (also in Fig-
ure 1) is also a sentential form and matches since
its (underlined) root labels of the target tree frag-
ments read “NP PP”. Thus, we can substitute the
latter sentential form into the former and obtain
the sentential form shown at the bottom of Fig-
ure 2. Ideally, the substitution process is repeated
until the complete source sentence is derived.

4 Rule Extraction

The rule extraction of Maletti (2011) extracts min-
imal tree-to-tree rules, which are rules containing
both source and target tree fragments, from sen-
tence pairs of a word-aligned and bi-parsed paral-
lel corpus. In particular, this requires parses for
both the source and the target language sentences
which adds a source for errors and specificity po-
tentially leading to lower translation performance
and lower coverage (Wellington et al., 2006). Chi-
ang (2010) showed that string-to-tree systems—

816



that1 concludes2 the3 debate4 on5 human6 rights7

TOP[1,7]

PROAV[1,1]

damit1

VAFIN[2,2]

ist2

NP[3,4]

ART[3,3]

die3

NN[4,4]

Aussprache4

VP[5,7]

PP[5,6]

APPR[5,5]

über5

NN[6,6]

Menschenrechte6

VVPP[7,7]

geschlossen7

Figure 3: Word-aligned sentence pair with target-
side parse.

which he calls fuzzy tree-to-tree-systems— gen-
erally yield higher translation quality compared to
corresponding tree-to-tree systems.

For efficiency reasons the rule extraction of
Maletti (2011) only extracts minimal rules, which
are the smallest tree fragments compatible with the
given word alignment and the parse trees. Simi-
larly, non-minimal rules are those that can be ob-
tained from minimal rules by substitution. In par-
ticular, each lexical item of a sentence pair oc-
curs in exactly one minimal rule extracted from
that sentence pair. However, minimal rules are
especially unsuitable for fixed phrases consisting
of rare words because minimal rules encourage
small fragments and thus word-by-word transla-
tion. Consequently, such fixed phrases will often
be assembled inconsistently by substitution from
small fragments. Non-minimal rules encourage a
consistent translation by covering larger parts of
the source sentence.

Here we want to develop an efficient rule ex-
traction procedure for our string-to-tree MBOTs
that avoids the mentioned drawbacks. Natu-
rally, we could substitute minimal rules into each
other to obtain non-minimal rules, but perform-
ing substitution for all combinations is clearly in-
tractable. Instead we essentially follow the ap-
proach of Koehn et al. (2003), Och and Ney
(2004), and Chiang (2007), which is based on con-
sistently aligned phrase pairs. Our training corpus
contains word-aligned sentence pairs 〈e,A, f〉,
which contain a source language sentence e, a
target language sentence f , and an alignment
A ⊆ [1, `e] × [1, `f ], where `e and `f are the
lengths of the sentences e and f , respectively, and
[i, i′] = {j ∈ Z | i ≤ j ≤ i′} is the span (closed
interval of integers) from i to i′ for all positive in-
tegers i ≤ i′. Rules are extracted for each pair
of the corpus, so in the following let 〈e,A, f〉 be

a word-aligned sentence pair. A source phrase
is simply a span [i, i′] ⊆ [1, `e] and correspond-
ingly, a target phrase is a span [j, j′] ⊆ [1, `f ].
A rule span is a pair 〈p, ϕ〉 consisting of a source
phrase p and a sequence ϕ = p1 · · · pn of (non-
overlapping) target phrases p1, . . . , pn. Spans
overlap if their intersection is non-empty. If n = 1
(i.e., there is exactly one target phrase in ϕ) then
〈p, ϕ〉 is also a phrase pair (Koehn et al., 2003).
We want to emphasize that formally phrases are
spans and not the substrings occuring at that span.

Next, we lift the notion of consistently aligned
phrase pairs to our rule spans. Simply put, for
a consistently aligned rule span 〈p, p1 · · · pn〉 we
require that it respects the alignment A in the
sense that the origin i of an alignment (i, j) ∈ A
is covered by p if and only if the destination j
is covered by p1, . . . , pn. Formally, the rule
span 〈p, p1 · · · pn〉 is consistently aligned if for
every (i, j) ∈ A we have i ∈ p if and
only if j ∈ ⋃nk=1 pk. For example, given the
word-aligned sentence pair in Figure 3, the rule
span 〈[2, 4], [2, 4] [7, 7]〉 is consistently aligned,
whereas the phrase pair 〈[2, 4], [2, 7]〉 is not.

Our MBOTs use rules consisting of a source
string and a sequence of target tree fragments.
The target trees are provided by a parser for the
target language. For each word-aligned sentence
pair 〈e,A, f〉 we thus have a parse tree t for f . An
example is provided in Figure 3. We omit a for-
mal definition of trees, but recall that each node η
of the parse tree t governs a (unique) target phrase.
In Figure 3 we have indicated those target phrases
(spans) as subscript to the non-lexical node labels.
A consistently aligned rule span 〈p, p1 · · · pn〉 of
〈e,A, f〉 is compatible with t if there exist nodes
η1, . . . , ηn of t such that ηk governs pk for all
1 ≤ k ≤ n. For example, given the word-aligned
sentence pair and parse tree t in Figure 3, the con-
sistently aligned rule span 〈[2, 4], [2, 4] [7, 7]〉 is
not compatible with t because there is no node in t
that governs [2, 4]. However, for the same data, the
rule span 〈[2, 4], [2, 2] [3, 4] [7, 7]〉 is consistently
aligned and compatible with t. The required nodes
of t are labeled VAFIN, NP, VVPP.

Now we are ready to start the rule extrac-
tion. For each consistently aligned rule span
〈p, p1 · · · pn〉 that is compatible with t and each se-
lection of nodes η1, . . . , ηn of t such that nk gov-
erns pk for each 1 ≤ k ≤ n, we can extract the
rule e(p)→ (flat(tη1), . . . ,flat(tηn)), where

817



Initial rules for

rule span 〈[3, 3], [3, 3]〉:

the →
( ART

die

)
rule span 〈[4, 4], [4, 4]〉:

debate →
( NN

Aussprache

)
rule span 〈[3, 4], [3, 4]〉:

the debate →
( NP

die Aussprache

)

rule span 〈[5, 7], [5, 6]〉:

on human rights →
( PP

über Menschenrechte

)
rule span 〈[3, 7], [3, 4] [5, 6]〉:

the debate on human rights →
( NP

die Aussprache
,

PP

über Menschenrechte

)

rule span 〈[2, 2], [2, 2] [7, 7]〉:

concludes →
( VAFIN

ist
,

VVPP

geschlossen

)
rule span 〈[2, 4], [2, 2] [3, 4] [7, 7]〉:

concludes the debate →
( VAFIN

ist
,

NP

die Aussprache
,

VVPP

geschlossen

)

rule span 〈[2, 7], [2, 7]〉:

concludes the debate on human rights →
( VAFIN

ist
,

NP

die Aussprache
,

VP

über Menschenrechte geschlossen

)

Figure 4: Some initial rules extracted from the word-aligned sentence pair and parse of Figure 3.

• e(p) is the substring of e at span p,1
• flat(u) removes all internal nodes from u (all

nodes except the root and the leaves), and
• tη is the subtree rooted in η for node η of t.

The rules obtained in this manner are called initial
rules for 〈e,A, f〉 and t. For example, for the rule
span 〈[2, 4], [2, 2] [3, 4] [7, 7]〉 we can extract only
one initial rule. More precisely, we have
• e([2, 4]) = concludes the debate
• tη1 = (VAFIN ist)
• tη2 =

(
NP (ART die) (NN Aussprache)

)
,

• and tη3 = (VVPP geschlossen).
The function flat leaves tη1 and tη3 unchanged,
but flat(tη2) = (NP die Aussprache). Thus, we
obtain the boxed rule of Figure 4.

Clearly, the initial rules are just the start be-
cause they are completely lexical in the sense that
they never contain the placeholder X in the source
string nor a non-lexical leaf in any output tree frag-
ment. We introduce non-lexical rules using the
same approach as for the hierarchical rules of Chi-
ang (2007). Roughly speaking, we obtain a new
rule r′′ by “excising” an initial rule r from another
rule r′ and replacing the removed part by
• the placeholder X in the source string,
• the root label of the removed tree fragment in

the target tree fragments, and
• linking the removed parts appropriately,

so that the flatted substitution of r into r′′ can

1If p = [i, i′], then e(p) = e[i, i′] is the substring of e
ranging from the i-th token to the i′-th token.

Extractable rule [top] and initial rule [bottom]:

the debate on human rights →
( NP

die Aussprache
,

PP

über Menschenrechte

)

on human rights → ( PP
über Menschenrechte )

Extractable rule obtained after excision:

the debate X →
( NP

die Aussprache
, PP

)

Figure 5: Excision of the middle initial rule from
the topmost initial rule. Substituting the middle
rule into the result yields the topmost rule.

yield r′. This “excision” process is illustrated in
Figure 5, where we remove the middle initial rule
from the topmost initial rule. The result is dis-
played at the bottom in Figure 5. Formally, the set
of extractable rules R for a given word-aligned
sentence pair 〈e,A, f〉 with parse tree t for f is
the smallest set subject to the following two con-
ditions:
• Each initial rule is in R and thus extractable.
• For every initial rule r and extractable rule
r′ ∈ R, any flat rule r′′, into which we can
substitute r to obtain ρ with flat(ρ) = r′, is
in R and thus extractable.2

For our running example depicted in Figure 3 we
display some extractable rules in Figure 6.

2A rule ρ = s→ (t1, . . . , tn) is flat if flat(ρ) = ρ, where
flat(ρ) = s→ (flat(t1), . . . , flat(tn)).

818



Source string “the debate”:

concludes X on human rights →
( VAFIN

ist
, NP ,

VP

über Menschenrechte geschlossen

)

Source string “on human rights”:

concludes the debate X →
( VAFIN

ist
,

NP

die Aussprache
,

VP

PP geschlossen

)
Source string “the debate on human rights”:

concludes X →
( VAFIN

ist
, NP ,

VP

PP geschlossen

)

Figure 6: Extractable rules obtained by excising various initial rules (see Figure 4) from the initial rule
displayed at the bottom of Figure 4.

Unfortunately, already Chiang (2007) points out
that the set of all extractable rules is generally
too large and keeping all extractable rules leads to
slow training, slow decoding, and spurious ambi-
guity. Our MBOT rules are restricted by the parse
tree for the target sentence, but the MBOT model
permits additional flexibility due to the presence
of multiple target tree fragments. Overall, we ex-
perience the same problems, and consequently, in
the experiments we use the following additional
constraints on rules s→ (t1, . . . , tn):
(a) We only consider source phrases p of length at

most 10 (i.e., i′ − i < 10 for p = [i, i′]).3
(b) The source string s contains at most 5 occur-

rences of lexical items or X (i.e. `s ≤ 5).
(c) The source string s cannot have consecu-

tive Xs (i.e., XX is not a substring of s).
(d) The source string contains at least one lexical

item that was aligned in 〈e,A, f〉.
(e) The left-most token of the source string s can-

not be X (i.e., s[1, 1] 6= X).
Our implementation can easily be modified to han-
dle other constraints. Figure 7 shows extractable
rules violating those additional constraints.

Table 1 gives an overview on how many rules
are extracted. Our string-to-tree variant extracts
12–17 times more rules than the minimal tree-to-
tree rule extraction. For our experiments (see Sec-
tion 6), we filter all rule tables on the given input.
The decoding times for the minimal `MBOT and
our MBOT share the same order of magnitude.

5 Model Features

For each source language sentence e, we want to
determine its most likely translation f̂ given by

f̂ = arg maxf p(f | e) = arg maxf p(e | f) · p(f)
3Note that this restricts the set of initial rules.

for some unknown probability distributions p. We
estimate p(e | f) ·p(f) by a log-linear combination
of features hi(·) with weights λi scored on senten-
tial forms e→ (t) of our extracted MBOTM such
that the leaves of t read (left-to-right) f .

We use the decoder provided by MBOT-Moses
of Braune et al. (2013) and its standard features,
which includes all the common features (Koehn,
2009) and a gap penalty 1001−c, where c is the
number of target tree fragments that contributed
to t. This feature discourages rules with many tar-
get tree fragments. As usual, all features are ob-
tained as the product of the corresponding rule fea-
tures for the rules used to derive e→ (t) by means
of substitution. The rule weights for the transla-
tion weights are obtained as relative frequencies
normalized over all rules with the same right- and
left-hand side. Good-Turing smoothing (Good,
1953) is applied to all rules that were extracted at
most 10 times. The lexical translation weights are
obtained as usual.

6 Experimental Results

We considered three reasonable baselines: (i) min-
imal `MBOT, (ii) non-contiguous STSSG (Sun et
al., 2009), or (iii) a string-to-tree Moses system.
We decided against the minimal `MBOT as a base-
line since tree-to-tree systems generally get lower
BLEU scores than string-to-tree systems. We nev-
ertheless present its BLEU scores (see Table 3).
Unfortunately, we could not compare to Sun et
al. (2009) because their decoder and rule extrac-
tion algorithms are not publicly available. Fur-
thermore, we have the impression that their system
does not scale well:
• Only around 240,000 training sentences were

used. Our training data contains between
1.8M and 5.7M sentence pairs.
• The development and test set were length-

819



violates (b):

that concludes X on human rights →
( PROAV

damit
,

VAFIN

ist
, NP ,

VP

über Menschenrechte geschlossen

)

violates (c):

concludes X X →
( VAFIN

ist
, NP ,

VP

PP geschlossen

) violates (d):
X →

(
NP

)
violates (e):

X on human rights →
(

NP ,
PP

über Menschenrechte

)

Figure 7: Showing extractable rules violating the restrictions.

System number of extracted rules
English-To-German English-To-Arabic English-To-Chinese

minimal tree-to-tree `MBOT 12,478,160 28,725,229 10,162,325
non-minimal string-to-tree MBOT 143,661,376 491,307,787 162,240,663
string-to-tree Moses 14,092,729 55,169,043 17,047,570

Table 1: Overview of numbers of extracted rules with respect to the different extraction algorithms.

ratio filtered to sentences up to 50 characters.
We do not modify those sets.
• Only rules with at most one gap were al-

lowed which would be equivalent to restrict
the number of target tree fragments to 2 in
our system.

Hence we decided to use a string-to-tree Moses
system as baseline (see Section 6.1).

6.1 Setup

As a baseline system for our experiments we use
the syntax-based component (Hoang et al., 2009)
of the Moses toolkit (Koehn et al., 2007). Our
system is the presented translation system based
on MBOTs. We use the MBOT-Moses decoder
(Braune et al., 2013) which – similar to the base-
line decoder – uses a CYK+ chart parsing algo-
rithm using a standard X-style parse tree which is
sped up by cube pruning (Chiang, 2007) with in-
tegrated language model scoring.

Our and the baseline system use linguistic syn-
tactic annotation (parses) only on the target side
(string-to-tree). During rule extraction we impose
the restrictions of Section 4. Additional glue-rules
that concatenate partial translations without per-
forming any reordering are used in all systems.

For all experiments (English-to-German,
English-to-Arabic, and English-to-Chinese), the
training data was length-ratio filtered. The word
alignments were generated by GIZA++ (Och
and Ney, 2003) with the grow-diag-final-and
heuristic (Koehn et al., 2005). The following
language-specific processing was performed. The
German text was true-cased and the functional

and morphological annotations were removed
from the parse. The Arabic text was tokenized
with MADA (Habash et al., 2009) and translit-
erated according to Buckwalter (2002). Finally,
the Chinese text was word-segmented using the
Stanford Word Segmenter (Chang et al., 2008).

In all experiments the feature weights λi of the
log-linear model were trained using minimum er-
ror rate training (Och, 2003). The remaining infor-
mation for the experiments is presented in Table 2.

6.2 Quantitative Analysis

The overall translation quality was measured with
4-gram BLEU (Papineni et al., 2002) on true-
cased data for German, on transliterated data for
Arabic, and on word-segmented data for Chinese.
Significance was computed with Gimpel’s imple-
mentation (Gimpel, 2011) of pairwise bootstrap
resampling with 1,000 samples. Table 3 lists the
evaluation results. In all three setups the MBOT
system significantly outperforms the baseline. For
German we obtain a BLEU score of 15.90 which
is a gain of 0.68 points. For Arabic we get an in-
crease of 0.78 points which results in 49.10 BLEU.
For Chinese we obtain a score of 18.35 BLEU
gaining 0.66 points.4 We also trained a vanilla
phrase-based system for each language pair on the
same data as described in Table 2.

To demonstrate the usefulness of the multiple

4NIST-08 also shows BLEU for word-segmented output
(http://www.itl.nist.gov/iad/mig/tests/
mt/2008/doc/mt08_official_results_v0.
html). Best constrained system: 17.69 BLEU; best
unconstrained system: 19.63 BLEU.

820



English to German English to Arabic English to Chinese
training data 7th EuroParl corpus (Koehn, 2005) MultiUN corpus (Eisele and Chen, 2010)

training data size ≈ 1.8M sentence pairs ≈ 5.7M sentence pairs ≈ 1.9M sentence pairs
target-side parser BitPar (Schmid, 2004) Berkeley parser (Petrov et al., 2006)

language model 5-gram SRILM (Stolcke, 2002)
add. LM data WMT 2013 Arabic in MultiUN Chinese in MultiUN
LM data size ≈ 57M sentences ≈ 9.7M sentences ≈ 9.5M sentences

tuning data WMT 2013 cut from MultiUN NIST 2002, 2003, 2005
tuning size 3,000 sentences 2,000 sentences 2,879 sentences

test data WMT 2013 (Bojar et al., 2013) cut from MultiUN NIST 2008 (NIST, 2010)
test size 3,000 sentences 1,000 sentences 1,859 sentences

Table 2: Summary of the performed experiments.

Language pair System BLEU

English-to-German

Moses Baseline 15.22
MBOT ∗15.90

minimal `MBOT 14.09
Phrase-based Moses 16.73

English-to-Arabic

Moses Baseline 48.32
MBOT ∗49.10

minimal `MBOT 32.88
Phrase-based Moses 50.27

English-to-Chinese

Moses Baseline 17.69
MBOT ∗18.35

minimal `MBOT 12.01
Phrase-based Moses 18.09

Table 3: Evaluation results. The starred results
are statistically significant improvements over the
baseline (at confidence p < 1%).

target tree fragments of MBOTs, we analyzed the
MBOT rules that were used when decoding the
test set. We distinguish several types of rules. A
rule is contiguous if it has only 1 target tree frag-
ment. All other rules are (potentially) discontigu-
ous. Moreover, lexical rules are rules whose leaves
are exclusively lexical items. All other rules (i.e.,
those that contain at least one non-lexical leaf)
are structural. Table 4 reports how many rules of
each type are used during decoding for both our
MBOT system and the minimal `MBOT. Below,
we focus on analyzing our MBOT system. Out
of the rules used for German, 27% were (poten-
tially) discontiguous and 5% were structural. For
Arabic, we observe 67% discontiguous rules and
26% structural rules. For translating into Chinese
30% discontiguous rules were used and the struc-
tural rules account to 18%. These numbers show
that the usage of discontiguous rules tunes to the

specific language pair. For instance, Arabic uti-
lizes them more compared to German and Chi-
nese. Furthermore, German uses a lot of lexical
rules which is probably due to the fact that it is a
morphologically rich language. On the other hand,
Arabic and Chinese make good use of structural
rules. In addition, Table 4 presents a finer-grained
analysis based on the number of target tree frag-
ments. Only rules with at most 8 target tree frag-
ments were used. While German and Arabic seem
to require some rules with 6 target tree fragments,
Chinese probably does not. We conclude that the
number of target tree fragments can be restricted
to a language-pair specific number during rule ex-
traction.

6.3 Qualitative Analysis

In this section, we inspect some English-to-
German translations generated by the Moses base-
line and our MBOT system in order to provide
some evidence for linguistic constructions that our
system handles better. We identified (a) the real-
ization of reflexive pronouns, relative pronouns,
and particle verbs, (b) the realization of verbal
material, and (c) local and long distance reorder-
ing to be better throughout than in the baseline
system. All examples are (parts of) translations
of sentences from the test data. Ungrammatical
constructions are enclosed in brackets and marked
with a star. We focus on instances that seem rele-
vant to the new ability to use non-minimal rules.

We start with an example showing the realiza-
tion of a reflexive pronoun.
Source: Bitcoin differs from other types of virtual currency.
Reference: Bitcoin unterscheidet sich von anderen Arten

virtueller Währungen.
Baseline: Bitcoin [unterscheidet]? von anderen Arten [der

virtuellen Währung]?.

821



Target tree fragments
Language pair System Type Lex Struct Total 2 3 4 5 ≥ 6

English-to-German

our cont. 27,351 635 27,986
MBOT discont. 9,336 1,110 10,446 5,565 3,441 1,076 312 52
minimal cont. 55,910 4,492 60,402
`MBOT discont. 2,167 7,386 9,553 6,458 2,589 471 34 1

English-to-Arabic

our cont. 1,839 651 2,490
MBOT discont. 3,670 1,324 4,994 3,008 1,269 528 153 36
minimal cont. 18,389 2,855 21,244
`MBOT discont. 1,138 1,920 3,058 2,525 455 67 8 3

English-to-Chinese

our cont. 17,135 1,585 18,720
MBOT discont. 4,822 3,341 8,163 6,411 1,448 247 55 2
minimal cont. 34,275 8,820 43,095
`MBOT discont. 516 4,292 4,808 3,816 900 82 6 4

Table 4: Number of rules per type used when decoding test (Lex = lexical rules; Struct = structural rules;
[dis]cont. = [dis]contiguous).

MBOT: Bitcoin unterscheidet sich von anderen Arten [der
virtuellen Währung]?.

Here the baseline drops the reflexive pronoun sich,
which is correctly realized by the MBOT system.
The rule used is displayed in Figure 8.

differs from other →
( VVFIN

unterscheidet

,
PRF

sich

,
APPR

von

,
ADJA

anderen

)

Figure 8: Rule realizing the reflexive pronoun.

Next, we show a translation in which our system
correctly generates a whole verbal segment.
Source: It turned out that not only . . .
Reference: Es stellte sich heraus, dass nicht nur . . .
Baseline: [Heraus,]? nicht nur . . .
MBOT: Es stellte sich heraus, dass nicht nur . . .

The baseline drops the verbal construction
whereas the large non-minimal rule of Figure 9 al-
lows our MBOT to avoid that drop. Again, the re-
quired reflexive pronoun sich is realized as well as
the necessary comma before the conjunction dass.

It turned out that →
( PPER

Es

,
VVFIN

stellte

,
PRF

sich

,
PTKZU

heraus

,
$,

,

,
KOUS

dass

)

Figure 9: MBOT rule for the verbal segment.

Another feature of MBOT is its power to per-
form long distance reordering with the help of sev-
eral discontiguous output fragments.
Source: . . . weapons factories now, which do not endure

competition on the international market and . . .

Reference: . . . Rüstungsfabriken, die der internationalen
Konkurrenz nicht standhalten und . . .

Baseline: . . . [Waffen in den Fabriken nun]?, die nicht einem
Wettbewerb auf dem internationalen Markt []? und . . .

MBOT: . . . [Waffen Fabriken nun]?, die Konkurrenz auf dem
internationalen Markt nicht ertragen und . . .

Figure 10 shows the rules which enable the
MBOT system to produce the correct reordering.

which do not X →
( PRELS

die

,
NP

NP

,
PTKNEG

nicht

,
VP

VP

)

endure X →
( NP

NP

,
VP

ertragen

)

competition X →
( NP

Konkurrenz PP

)

on the international market →
( PP

auf dem internationalen Markt

)

Figure 10: Long distance reordering.

7 Conclusion

We present an application of a string-to-tree vari-
ant of local multi bottom-up tree transducers,
which are tree-to-tree models, to statistical ma-
chine translation. Originally, only minimal rules
were extracted, but to overcome the typically
lower translation quality of tree-to-tree systems
and minimal rules, we abolish the syntactic an-
notation on the source side and develop a string-
to-tree variant. In addition, we present a new pa-

822



rameterized rule extraction that can extract non-
minimal rules, which are particularly helpful for
translating fixed phrases. It would be interesting
to know how much can be gained when using only
one contribution at a time. Hence, we will explore
the impact of string-to-tree and non-minimal rules
in isolation.

We demonstrate that our new system signifi-
cantly outperforms the standard Moses string-to-
tree system on three different large-scale transla-
tion tasks (English-to-German, English-to-Arabic,
and English-to-Chinese) with a gain between 0.53
and 0.87 BLEU points. An analysis of the rules
used to decode the test sets suggests that the usage
of discontiguous rules is tuned to each language
pair. Furthermore, it shows that only discontigu-
ous rules with at most 8 target tree fragments are
used. Thus, further research could investigate a
hard limit on the number of target tree fragments
during rule extraction. We also perform a manual
inspection of the obtained translations and con-
firm that our string-to-tree MBOT rules can ade-
quately handle discontiguous phrases, which oc-
cur frequently in German, Arabic, and Chinese.
Other languages that exhibit such phenomena in-
clude Czech, Dutch, Russian, and Polish. Thus,
we hope that our approach can also be applied suc-
cessfully to other language pairs.

To support further experimentation by the
community, we publicly release our de-
veloped software and complete tool-chain
(http://www.ims.uni-stuttgart.de/
forschung/ressourcen/werkzeuge/
mbotmoses.html).

Acknowledgement

The authors would like to express their gratitude
to the reviewers for their helpful comments and
Robin Kurtz for preparing the Arabic corpus.

All authors were financially supported by
the German Research Foundation (DFG) grant
MA 4959 / 1-1.

References
Ondřej Bojar, Christian Buck, Chris Callison-Burch,

Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut, and
Lucia Specia. 2013. Findings of the 2013 Work-
shop on Statistical Machine Translation. In Proc.
8th WMT, pages 1–44. Association for Computa-
tional Linguistics.

Fabienne Braune, Nina Seemann, Daniel Quernheim,
and Andreas Maletti. 2013. Shallow local multi
bottom-up tree transducers in statistical machine
translation. In Proc. 51st ACL, pages 811–821. As-
sociation for Computational Linguistics.

Timothy Buckwalter. 2002. Arabic translit-
eration. http://www.qamus.org/
transliteration.htm.

Pi-Chuan Chang, Michel Galley, and Christopher D.
Manning. 2008. Optimizing Chinese word segmen-
tation for machine translation performance. In Proc.
3rd WMT, pages 224–232. Association for Compu-
tational Linguistics.

David Chiang. 2006. An introduction to synchronous
grammars. In Proc. 44th ACL. Association for Com-
putational Linguistics. Part of a tutorial given with
Kevin Knight.

David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201–228.

David Chiang. 2010. Learning to translate with source
and target syntax. In Proc. 48th ACL, pages 1443–
1452. Association for Computational Linguistics.

Steve DeNeefe, Kevin Knight, Wei Wang, and Daniel
Marcu. 2007. What can syntax-based MT learn
from phrase-based MT? In Proc. 2007 EMNLP,
pages 755–763. Association for Computational Lin-
guistics.

Andreas Eisele and Yu Chen. 2010. MultiUN: A mul-
tilingual corpus from United Nation documents. In
Proc. 7th LREC, pages 2868–2872. European Lan-
guage Resources Association.

Jason Eisner. 2003. Learning non-isomorphic tree
mappings for machine translation. In Proc. 41st
ACL, pages 205–208. Association for Computa-
tional Linguistics.

Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What’s in a translation rule?
In Proc. 2004 NAACL, pages 273–280. Association
for Computational Linguistics.

Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proc.
44th ACL, pages 961–968. Association for Compu-
tational Linguistics.

Kevin Gimpel. 2011. Code for statistical significance
testing for MT evaluation metrics. http://www.
ark.cs.cmu.edu/MT/.

Irving J. Good. 1953. The population frequencies of
species and the estimation of population parameters.
Biometrika, 40(3–4):237–264.

823



Nizar Habash, Owen Rambow, and Ryan Roth. 2009.
MADA+TOKAN: A toolkit for Arabic tokenization,
diacritization, morphological disambiguation, POS
tagging, stemming and lemmatization. In Proc. 2nd
MEDAR, pages 102–109. Association for Computa-
tional Linguistics.

Hieu Hoang, Philipp Koehn, and Adam Lopez. 2009.
A unified framework for phrase-based, hierarchical,
and syntax-based statistical machine translation. In
Proc. 6th IWSLT, pages 152–159.

Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
2003 NAACL, pages 48–54. Association for Compu-
tational Linguistics.

Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh system description
for the 2005 IWSLT Speech Translation Evaluation.
In Proc. 2nd IWSLT, pages 68–75.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proc. 45th ACL, pages 177–180. Associa-
tion for Computational Linguistics.

Philipp Koehn. 2005. Europarl: A parallel corpus
for statistical machine translation. In Proc. 10th
MT Summit, pages 79–86. Association for Machine
Translation in the Americas.

Philipp Koehn. 2009. Statistical Machine Translation.
Cambridge University Press.

Alon Lavie, Alok Parlikar, and Vamshi Ambati. 2008.
Syntax-driven learning of sub-sentential translation
equivalents and translation rules from parsed parallel
corpora. In Proc. 2nd SSST, pages 87–95. Associa-
tion for Computational Linguistics.

Yang Liu, Yajuan Lü, and Qun Liu. 2009. Improving
tree-to-tree translation with packed forests. In Proc.
47th ACL, pages 558–566. Association for Compu-
tational Linguistics.

Andreas Maletti. 2011. How to train your multi
bottom-up tree transducer. In Proc. 49th ACL, pages
825–834. Association for Computational Linguis-
tics.

NIST. 2010. NIST 2002 [2003, 2005, 2008] open ma-
chine translation evaluation. Linguistic Data Con-
sortium. LDC2010T10 [T11, T14, T21].

Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1):19–51.

Franz J. Och and Hermann Ney. 2004. The alignment
template approach to statistical machine translation.
Computational Linguistics, 30(4):417–449.

Franz J. Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. 41st ACL,
pages 160–167. Association for Computational Lin-
guistics.

Kishore Papineni, Salim Roukos, Todd Ward, and
Wei jing Zhu. 2002. BLEU: a method for auto-
matic evaluation of machine translation. In Proc.
40th ACL, pages 311–318. Association for Compu-
tational Linguistics.

Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proc. 44th ACL, pages
433–440. Association for Computational Linguis-
tics.

Helmut Schmid. 2004. Efficient parsing of highly
ambiguous context-free grammars with bit vectors.
In Proc. 20th COLING, pages 162–168. Association
for Computational Linguistics.

Andreas Stolcke. 2002. SRILM — an extensible
language modeling toolkit. In Proc. 7th INTER-
SPEECH, pages 257–286.

Jun Sun, Min Zhang, and Chew Lim Tan. 2009. A non-
contiguous tree sequence alignment-based model for
statistical machine translation. In Proc. 47th ACL,
pages 914–922. Association for Computational Lin-
guistics.

Benjamin Wellington, Sonjia Waxmonsky, and I. Dan
Melamed. 2006. Empirical lower bounds on the
complexity of translational equivalence. In Proc.
44th ACL, pages 977–984. Association for Compu-
tational Linguistics.

824


