



















































Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics


Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 784–792
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1073

Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 784–792
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1073

Polish evaluation dataset
for compositional distributional semantics models

Alina Wróblewska Katarzyna Krasnowska-Kieraś
Institute of Computer Science, Polish Academy of Sciences

alina@ipipan.waw.pl kasia.krasnowska@gmail.com

Abstract

The paper presents a procedure of building
an evaluation dataset1. for the validation
of compositional distributional semantics
models estimated for languages other than
English. The procedure generally builds
on steps designed to assemble the SICK
corpus, which contains pairs of English
sentences annotated for semantic related-
ness and entailment, because we aim at
building a comparable dataset. However,
the implementation of particular building
steps significantly differs from the orig-
inal SICK design assumptions, which is
caused by both lack of necessary extra-
neous resources for an investigated lan-
guage and the need for language-specific
transformation rules. The designed proce-
dure is verified on Polish, a fusional lan-
guage with a relatively free word order,
and contributes to building a Polish evalu-
ation dataset. The resource consists of 10K
sentence pairs which are human-annotated
for semantic relatedness and entailment.
The dataset may be used for the evaluation
of compositional distributional semantics
models of Polish.

1 Introduction and related work

1.1 Distributional semantics

The basic idea of distributional semantics, i.e. de-
termining the meaning of a word based on its
co-occurrence with other words, is derived from
the empiricists – Harris (1954) and Firth (1957).
John R. Firth drew attention to the context-
dependent nature of meaning especially with his

1The dataset is obtainable at:
http://zil.ipipan.waw.pl/Scwad/CDSCorpus

famous maxim “You shall know a word by
the company it keeps” (Firth, 1957, p. 11).

Nowadays, distributional semantics models are
estimated with various methods, e.g. word em-
bedding techniques (Bengio et al., 2003, 2006;
Mikolov et al., 2013). To ascertain the purport
of a word, e.g. bath, you can use the context
of other words that surround it. If we assume that
the meaning of this word expressed by its lexical
context is associated with a distributional vector,
the distance between distributional vectors of two
semantically similar words, e.g bath and shower,
should be smaller than between vectors represent-
ing semantically distinct words, e.g. bath and tree.

1.2 Compositional distributional semantics
Based on empirical observations that distribu-
tional vectors encode certain aspects of word
meaning, it is expected that similar aspects
of the meaning of phrases and sentences can also
be represented with vectors obtained via composi-
tion of distributional word vectors. The idea of se-
mantic composition is not new. It is well known
as the principle of compositionality:2 “The mean-
ing of a compound expression is a function
of the meaning of its parts and of the way they are
syntactically combined.” (Janssen, 2012, p. 19).

Modelling the meaning of textual units larger
than words using compositional and distribu-
tional information is the main subject of compo-
sitional distributional semantics (Mitchell and La-
pata, 2010; Baroni and Zamparelli, 2010; Grefen-
stette and Sadrzadeh, 2011; Socher et al., 2012, to
name a few studies). The fundamental principles
of compositional distributional semantics, hence-
forth referred to as CDS, are mainly propagated
with papers written on the topic. Apart from the
papers, it was the SemEval-2014 Shared Task 1

2As the principle of compositionality is attributed to Got-
tlob Frege, it is often called Frege’s principle.

784

https://doi.org/10.18653/v1/P17-1073
https://doi.org/10.18653/v1/P17-1073


(Marelli et al., 2014) that essentially contributed
to the expansion of CDS and increased an interest
in this domain. The goal of the task was to evaluate
CDS models of English in terms of semantic relat-
edness and entailment on proper sentences from
the SICK corpus.

1.3 The SICK corpus

The SICK corpus (Bentivogli et al., 2014) con-
sists of 10K pairs of English sentences contain-
ing multiple lexical, syntactic, and semantic phe-
nomena. It builds on two external data sources
– the 8K ImageFlickr dataset (Rashtchian et al.,
2010) and SemEval-2012 Semantic Textual Simi-
larity dataset (Agirre et al., 2012). Each sentence
pair is human-annotated for relatedness in mean-
ing and entailment.

The relatedness score corresponds to the degree
of semantic relatedness between two sentences
and is calculated as the average of ten human rat-
ings collected for this sentence pair on the 5-point
Likert scale. This score indicates the extent to
which the meanings of two sentences are related.

The entailment relation between two sentences,
in turn, is labelled with entailment, contradic-
tion, or neutral. According to the SICK guidelines,
the label assigned by the majority of human anno-
tators is selected as the valid entailment label.

1.4 Motivation and organisation of the paper

Studying approaches to various natural language
processing (henceforth NLP) problems, we have
observed that the availability of language re-
sources (e.g. training or testing data) stimulates
the development of NLP tools and the estimation
of NLP models. English is undoubtedly the most
prominent in this regard and English resources are
the most numerous. Therefore, NLP methods are
mostly designed for English and tested on English
data, even if there is no guarantee that they are
universal. In order to verify whether an NLP al-
gorithm is adequate, it is not enough to evaluate
it solely for English. It is also valuable to have
high-quality resources for languages typologically
different to English. Hence, we aim at building
datasets for the evaluation of CDS models in lan-
guages other than English, which are often under-
resourced. We strongly believe that the availability
of test data will encourage development of CDS
models in these languages and allow to better test
the universality of CDS methods.

We start with a high-quality dataset for Pol-
ish, which is a completely different language than
English in at least two dimensions. First, it is
a rather under-resourced language in contrast to
the resource-rich English. Second, it is a fusional
language with a relatively free word order in con-
trast to the isolated English with a relatively fixed
word order. If some heuristics is tested on e.g. Pol-
ish, the evaluation results can be approximately
generalised to other Slavic languages. We hope the
Slavic NLP community will be interested in de-
signing and evaluating methods of semantic mod-
elling for Slavic languages.

The procedure of building an evaluation dataset
for validating compositional distributional seman-
tics models of Polish generally builds on steps de-
signed to assemble the SICK corpus (described in
Section 1.3) because we aim at building an eval-
uation dataset which is comparable to the SICK
corpus. However, the implementation of particular
building steps significantly differs from the orig-
inal SICK design assumptions, which is caused
by both lack of necessary extraneous resources
for Polish (see Section 2.1) and the need for
Polish-specific transformation rules (see Section
2.2). Furthermore, the rules of arranging sentences
into pairs (see Section 2.3) are defined anew tak-
ing into account the characteristic of data and bi-
directional entailment annotations, since an entail-
ment relation between two sentences must not be
symmetric. Even if our assumptions of annotating
sentence pairs coincide with the SICK principles
to a certain extent (see Section 3.1), the annotation
process differs from the SICK procedure, in par-
ticular by introducing an element of human verifi-
cation of correctness of automatically transformed
sentences (see Section 3.2) and some additional
post-corrections (see Section 3.3). Finally, a sum-
mary of the dataset is provided in Section 4.1 and
the dataset evaluation is given in Section 4.2.

2 Procedure of collecting data

2.1 Selection and description of images

The first step of building the SICK corpus con-
sisted in the random selection of English sentence
pairs from existing datasets (Rashtchian et al.,
2010; Agirre et al., 2012). Since we are not aware
of accessibility of analogous resources for Polish,
we have to select images first and then describe
the selected images.

Images are selected from the 8K ImageFlickr

785



dataset (Rashtchian et al., 2010). At first we
wanted to take only these images the descriptions
of which were selected for the SICK corpus. How-
ever, a cursory check shows that these images are
quite homogeneous, with a predominant number
of dogs depictions. Therefore, we independently
extract 1K images and split them into 46 thematic
groups (e.g. children, musical instruments, mo-
torbikes, football, dogs). The numbers of images
within individual thematic groups vary from 6 im-
ages in the volleyball and telephoning groups to
94 images in the various people group. The sec-
ond largest groups are children and dogs with 50
images each.

The chosen images are given to two authors
who independently of each other formulate their
descriptions based on a short instruction. The au-
thors are instructed to write one single sentence
(with a sentence predicate) describing the ac-
tion in a displayed image. They should not de-
scribe an imaginable context or an interpretation
of what may lie behind the scene in the picture. If
some details in the picture are not obvious, they
should not be described either. Furthermore, the
authors should avoid multiword expressions, such
as idioms, metaphors, and named entities, because
those are not compositional linguistic phenomena.
Finally, descriptions should contain Polish diacrit-
ics and proper punctuation.

2.2 Transformation of descriptions
The second step of building the SICK corpus
consisted in pre-processing extracted sentences,
i.e. normalisation and expansion (Bentivogli et al.,
2014, p. 3–4). Since the authors of Polish descrip-
tions are asked to follow the guidelines (presented
in Section 2.1), the normalisation step is not essen-
tial for our data. The expansion step, in turn, is im-
plemented and the sentences provided by the au-
thors are lexically and syntactically transformed
in order to obtain derivative sentences with sim-
ilar, contrastive, or neutral meanings. The follow-
ing transformations are implemented:

1. dropping conjunction concerns sentences
with coordinated predicates sharing a sub-
ject, e.g. Rowerzysta odpoczywa i obserwuje
morze. (Eng. ‘A cyclist is resting and watch-
ing the sea.’). The finite form of one of the co-
ordinated predicates is transformed into:

• an active adjectival participle, e.g.
Odpoczywający rowerzysta obserwuje

morze. (Eng. ‘A resting cyclist is watch-
ing the sea.’) or Obserwujący morze
rowerzysta odpoczywa. (Eng. ‘A cyclist,
who is watching the sea, is resting.’),
• a contemporary adverbial participle,

e.g. Rowerzysta, odpoczywając, obser-
wuje morze. (Eng. ‘A cyclist is watch-
ing the sea, while resting.’) or Row-
erzysta odpoczywa, obserwując morze.
(Eng. ‘A cyclist is resting, while watch-
ing the sea.’).

2. removing conjunct in adjuncts, i.e. the dele-
tion of one of coordinated elements of an ad-
junct, e.g. Mały, ale zwinny kot miauczy.
(Eng. ‘A small but agile cat miaows.’) can
be changed into either Mały kot miauczy.
(Eng. ‘A small cat miaows.’) or Zwinny kot
miauczy. (Eng. ‘An agile cat miaows.’).

3. passivisation, e.g. Człowiek ujeżdża byka.
(Eng. ‘A man is breaking a bull in.’) can
be transformed into Byk jest ujeżdżany przez
człowieka. (Eng. ‘A bull is being broken in by
a man.’).

4. removing adjuncts, e.g. Dwa białe króliki
siedzą na trawie. (Eng. ‘Two small rabbits
are sitting on the grass.’) can be changed
into Króliki siedzą. (Eng. ‘The rabbits are sit-
ting.’).

5. swapping relative clause for participles,
i.e. a relative clause swaps with a participle
(and vice versa), e.g. Kobieta przytula psa,
którego trzyma na smyczy. (Eng. ‘A woman
hugs a dog which she keeps on a leash.’).
The relative clause is interchanged for a par-
ticiple construction, e.g. Kobieta przytula
trzymanego na smyczy psa. (Eng. ‘A woman
hugs a dog kept on a leash.’).

6. negation, e.g. Mężczyźni w turbanach na
głowach siedzą na słoniach. (Eng. ‘Men in
turbans on their heads are sitting on ele-
phants.’) can be transformed into Nikt nie
siedzi na słoniach. (Eng. ‘Nobody is sitting
on elephants.’), Żadni mężczyźni w turbanach
na głowach nie siedzą na słoniach. (Eng. ‘No
men in turbans on their heads are sitting on
elephants.’), and Mężczyźni w turbanach na
głowach nie siedzą na słoniach. (Eng. ‘Men
in turbans on their heads are not sitting on
elephants.’).

786



7. constrained mixing of dependents from var-
ious sentences, e.g. Dwoje dzieci siedzi
na wielbłądach w pobliżu wysokich gór.
(Eng. ‘Two children are sitting on camels
near high mountains.’) can be changed into
Dwoje dzieci siedzi przy zastawionym stole
w pobliżu wysokich gór. (Eng. ‘Two children
are sitting at the table laid with food near high
mountains.’).

The first five transformations are designed to pro-
duce sentences with a similar meaning, the sixth
transformation outputs sentences with a contra-
dictory meaning, and the seventh transformation
should generate sentences with a neutral (or unre-
lated) meaning. All transformations are performed
on the dependency structures of input sentences
(Wróblewska, 2014).

Some of the transformations are very produc-
tive (e.g. mixing dependents). Other, in turn, are
sparsely represented in the output (e.g. dropping
conjunction). The number of transformed sen-
tences randomly selected to build the dataset is in
the second column of Table 1.

transformation selected
dropping conjunction 139 2.0%
removing conjunct in adjunct 485 6.9%
passivisation 893 12.8%
removing adjuncts 1013 14.5%
swapping rc↔ptcp 1291 18.4%
negation 1304 18.6%
mixing dependents 1878 26.8%

Table 1: Numbers of transformed sentences se-
lected for annotation.

2.3 Data ensemble

The final step of building the SICK corpus
consisted in arranging normalised and expanded
sentences into pairs. Since our data diverges
from SICK data, the process of arranging Pol-
ish sentences into pairs also differs from pair-
ing in the SICK corpus. The general idea be-
hind the pair-ensembling procedure was to intro-
duce sentence pairs with different levels of relat-
edness into the dataset. Apart from pairs connect-
ing two sentences originally written by humans
(as described in Section 2.1), there are also pairs
in which an original sentence is connected with

a transformed sentence. For each of the 1K im-
ages, the following 10 pairs are constructed (for
A being the set of all sentences originally written
by the first author, B being the set of all sentences
originally written by the second author, a ∈ A and
b ∈ B being the original descriptions of the pic-
ture):

1. (a,b)

2. (a,a1), where a1 ∈ t(a), and t(a) is the set
of all transformations of the sentence a

3. (b,b1), where b1 ∈ t(b)

4. (a,b2), where b2 ∈ t(b)

5. (b,a2), where a2 ∈ t(a)

6. (a,a3), where a3 ∈ t(a′),a′ ∈ A, T (a′) =
T (a),a′ 6= a, for T (a) being the thematic
group3 of a

7. (b,b3), where b3 ∈ t(b′),b′ ∈ B, T (b′) =
T (b),b′ 6= b

8. (a,a4), where a4 ∈ A, T (a4) 6= T (a)4

9. (b,b4), where b4 ∈ B, T (b4) 6= T (b)

10. (a,a5), where a5 ∈ t(a),a5 6= a1 for
50% images, (b,b5) (analogously) for other
50%.5

For each sentence pair (a,b) created according
to this procedure, its reverse (b,a) is also included
in our corpus. As a result, the working set consists
of 20K sentence pairs.

3 Corpus annotation

3.1 Annotation assumptions

The degree of semantic relatedness between two
sentences is calculated as the average of all human
ratings on the Likert scale with the range from 0 to
5. Since we do not want to excessively influence

3The thematic group of a sentence a corresponds to the
thematic group of an image being the source of a (as de-
scribed in Section 2.1).

4The pairs (a,a4) of the same authors’ descriptions of
two images from different thematic groups are expected to be
unrelated. The same applies to (b,b4).

5A repetition of point 2 with a restriction that a different
pair is created (pairs of very related sentences are expected).
We alternate between authors A and B to obtain equal author
proportions in the final ensemble of pairs.

787



the annotations, the guidelines given to annotators
are mainly example-based:6

• 5 (very related): Kot siedzi na płocie.
(Eng. ‘A cat is sitting on the fence.’) vs. Na
płocie jest duży kot. (Eng. ‘There is a large
cat on the fence.’),

• 1–4 (more or less related):
Kot siedzi na płocie. (Eng. ‘A cat is sitting
on the fence.’) vs. Kot nie siedzi na płocie.
(Eng. ‘A cat is not sitting on the fence.’);
Kot siedzi na płocie. (Eng. ‘A cat is sitting on
the fence.’) vs. Właściciel dał kotu chrupki.
(Eng. ‘The owner gave kibble to his cat.’);
Kot siedzi na płocie. (Eng. ‘A cat is sitting
on the fence.’) vs. Kot miauczy pod płotem.
(Eng. ‘A cat miaows by the fence.’).

• 0 (unrelated): Kot siedzi na płocie.
(Eng. ‘A cat is sitting on the fence.’) vs.
Zaczął padać deszcz. (Eng. ‘It started to
rain.’).

Apart from these examples, there is a note in
the annotation guidelines indicating that the de-
gree of semantic relatedness is not equivalent to
the degree of semantic similarity. Semantic sim-
ilarity is only a special case of semantic related-
ness, semantic relatedness is thus a more general
term than the other one.

Polish entailment labels correspond directly to
the SICK labels (i.e. entailment, contradiction,
neutral). The entailment label assigned by the ma-
jority of human judges is selected as the gold label.
The entailment labels are defined as follows:

• a wynika z b (b entails a) – if a situation
or an event described by sentence b occurs,
it is recognised that a situation or an event
described by a occurs as well, i.e. a and b
refer to the same event or the same situation,

• a jest zaprzeczeniem b (a is the negation
of b) – if a situation or an event described
by b occurs, it is recognised that a situation
or an event described by a may not occur at
the same time,

6We realise that the boundary between semantic percep-
tion of a sentence by various speakers is fuzzy (it depends
on speakers’ education, origin, age, etc.). It was thus our well-
thought-out decision to draw only general annotation frames
and to enable annotators to rely on their feel for language.

• a jest neutralne wobec b (a is neutral to b) –
the truth of a situation described by a cannot
be determined on the basis of b.

3.2 Annotation procedure
Similar to the SICK corpus, each Polish sentence
pair is human-annotated for semantic relatedness
and entailment by 3 human judges experienced in
Polish linguistics.7 Since for each annotated pair
(a,b), its reverse (b,a) is also subject to anno-
tation, the entailment relation is in practice deter-
mined ‘in both directions’ for 10K sentence pairs.
For the task of relatedness annotation, the order of
sentences within pairs seems to be irrelevant, we
can thus assume to obtain 6 relatedness scores for
10K unique pairs.

Since the transformation process is fully auto-
matic and to a certain extent based on imperfect
dependency parsing, we cannot ignore errors in
the transformed sentences. In order to avoid anno-
tating erroneous sentences, the annotation process
is divided into two stages:

1. a sentence pair is sent to a judge with
the leader role, who is expected to edit and
to correct the transformed sentence from this
pair before annotation, if necessary,

2. the verified and possibly enhanced sentence
pair is sent to the other two judges, who can
only annotate it.

The leader judges should correct incomprehen-
sible and ungrammatical sentences with a mini-
mal number of necessary changes. Unusual sen-
tences which could be accepted by Polish speakers
should not be modified. Moreover, the modified
sentence may not be identical with the other sen-
tence in the pair. The classification and statistics
of distinct corrections made by the leader judges
are provided in Table 2.

A strict classification of error types is quite hard
to provide because some sentences contain more
than one error. We thus order the error types from
the most serious errors (i.e. ‘sense’ errors) to the
redundant corrections (i.e. ‘other’ type). If a sen-
tence contains several errors, it is qualified for the
higher order error type.

In the case of sentences with ‘sense’ errors,
the need for correction is uncontroversial and

7Our annotators have relatively strong linguistic back-
ground. Five of them have PhD in linguistics, five are PhD
students, one is a graduate, and one is an undergraduate.

788



error type # of errors % of errors
sense 171 12.3
semantic 407 29.2
grammatical 243 17.4
word order 141 10.1
punctuation 366 26.2
other 68 4.9

Table 2: Classification and statistics of correc-
tions.

arises from an internal logical contradiction.8

The sentences with ‘semantic’ changes are syn-
tactically correct, but deemed unacceptable by the
leader annotators from the semantic or pragmatic
point of view.9 The ‘grammatical’ errors mostly
concern missing agreement.10 The majority of
‘word order’ corrections are unnecessary, but we
found some examples which can be classified as
actual word or phrase order errors.11 The correc-
tion of punctuation consists in adding or deleting
a comma.12 The sentences in the ‘other’ group, in
turn, could as well have been left unchanged be-
cause they are proper Polish sentences, but were
apparently considered odd by the leader annota-
tors.

8An example of ‘sense’ error: the sentence Chłopak w
zielonej bluzie i czapce zjeżdża na rolkach na leżąco. (Eng.
‘A boy in a green sweatshirt and a cap roller-skates down-
hill in a lying position.’) is corrected into Chłopak w zielonej
bluzie i czapce zjeżdża na rolkach. (Eng. ‘A boy in a green
sweatshirt and a cap roller-skates downhill.’).

9An example of ‘semantic’ correction: the sentence
Dziewczyna trzyma w pysku patyk. (Eng. ‘A girl holds a stick
in her muzzle.’) is corrected into Dziewczyna trzyma w us-
tach patyk. (Eng. ‘A girl holds a stick in her mouth.’).

10An example of ‘grammatical’ error: the sentence
Grupasg.nom uśmiechających się ludzi tańcząpl. (Eng.
*‘A group of smiling people are dancing.’) is corrected
into Grupasg.nom uśmiechających się ludzi tańczysg . (Eng.
‘A group of smiling people is dancing.’).

11An example of word order error: the sentence Samochód,
który jest uszkodzony, koloru białego stoi na lawecie dużego
auta. (lit. ‘A car that is damaged, of the white color stands
on the trailer of a large car.’, Eng. ‘A white car that is dam-
aged is standing on the trailer of a large car.’) is corrected
into Samochód koloru białego, który jest uszkodzony, stoi na
lawecie dużego auta.

12An example of punctuation correction: the wrong comma
in the sentence Nad brzegiem wody, stoją dwaj mężczyźni z
wędkami. (lit. ‘On the water’s edge, two men are standing
with rods.’; Eng. ‘Two men with rods are standing on the wa-
ter’s edge.’) should be deleted, i.e. Nad brzegiem wody stoją
dwaj mężczyźni z wędkami.

3.3 Impromptu post-corrections
During the annotation process it came out that sen-
tences accepted by some human annotators are un-
acceptable for other annotators. We thus decided
to garner annotators’ comments and suggestions
for improving sentences. After validation of these
suggestions by an experienced linguist, it turns out
that most of these proposals concern punctuation
errors (e.g. missing comma) and typos in 312 dis-
tinct sentences. These errors are fixed directly in
the corpus because they should not impact the an-
notations of sentence pairs. The other suggestions
concern more significant changes in 29 distinct
sentences (mostly minor grammatical or seman-
tic problems overlooked by the leader annotators).
The annotations of pairs with modified sentences
are resent to the annotators so that they can verify
and update them.

4 Corpus summary and evaluation

4.1 Corpus statistics
Tables 3 and 4 summarise the annotations
of the resulting 10K sentence pairs corpus. Table
3 aggregates the occurrences of 6 possible related-
ness scores, calculated as the mean of all 6 indi-
vidual annotations, rounded to an integer.

relatedness # of pairs
0 1978
1 1428
2 1082
3 2159
4 2387
5 966

Table 3: Final relatedness scores rounded to inte-
gers (total: 10K pairs).

Table 4 shows the number of the particular en-
tailment labels in the corpus. Since each sentence
pair is annotated for entailment in both directions,
the final entailment label is actually a pair of two
labels:

• entailment+neutral points to ‘one-way’ en-
tailment,

• contradiction+neutral points to ‘one-way’
contradiction,

• entailment+entailment, contradiction+con-
tradiction, and neutral+neutral point to
equivalence.

789



While the actual corpus labels are ordered
in the sense that there is a difference between
e.g. entailment+neutral and neutral+entailment
(the entailment occurs in different directions),
we treat all labels as unordered for the purpose
of this summary (e.g. entailment+neutral covers
neutral+entailment as well, representing the same
type of relation between two sentences).

entailment # of pairs
neutral+neutral 6483
entailment+neutral 1748
entailment+entailment 933
contradiction+contradiction 721
contradiction+neutral 115

Table 4: Final entailment labels (total: 10K pairs).

4.2 Inter-annotator agreement

The standard measure of inter-annotator agree-
ment in various natural language labelling tasks is
Cohen’s kappa (Cohen, 1960). However, this coef-
ficient is designed to measure agreement between
two annotators only. Since there are three annota-
tors of each pair of ordered sentences, we decided
to apply Fleiss’ kappa13 (Fleiss, 1971) designed
for measuring agreement between multiple raters
who give categorical ratings to a fixed number
of items. An additional advantage of this measure
is that different items can be rated by different hu-
man judges, which doesn’t impact measurement.
The normalised Fleiss’ measure of inter-annotator
agreement is:

κ =
P̄ − P̄e
1− P̄e

where the quantity P̄ − P̄e measures the degree
of agreement actually attained in excess of chance,
while “[t]he quantity 1 − P̄e measures the de-
gree of agreement attainable over and above what
would be predicted by chance” (Fleiss, 1971,
p. 379).

We recognise Fleiss’ kappa as particularly use-
ful for measuring inter-annotator agreement with
respect to entailment labelling in our evaluation
dataset. First, there are more than two raters. Sec-
ond, entailment labels are categorically. Measured

13As Fleiss’ kappa is actually the generalisation of Scott’s
π (Scott, 1955), it is sometimes referred to as Fleiss’ multi-π,
cf. Artstein and Poesio (2008).

with Fleiss’ kappa, there is an inter-annotator
agreement of κ = 0.734 for entailment labels in
Polish evaluation dataset, which is quite satisfac-
tory as for a semantic labelling task.

Relative to semantic relatedness, the distinc-
tion in meaning of two sentences made by human
judges is often very subtle. This is also reflected
in the inter-annotator agreement scores measured
with Fleiss’ kappa. Inter-annotator agreement
measured for six semantic relatedness groups
corresponding to points on the Likert scale is
quite low: κ = 0.337. If we measure inter-
annotator agreement for three classes correspond-
ing to the three relatedness groups from the an-
notation guidelines (see Section 3.1), i.e. <0>,
<1, 2, 3, 4>, and <5>, the Fleiss’ score is sig-
nificantly higher: κ = 0.543. Hence, we con-
clude that Fleiss’ kappa is not a reliable measure
of inter-annotator agreement in relation to related-
ness scores. Therefore, we decided to use Krippen-
dorff’s α instead.

Krippendorff’s α (Krippendorff, 1980, 2013) is
a coefficient appropriate for measuring the inter-
annotator agreement of a dataset which is anno-
tated with multiple judges and characterised by
different magnitudes of disagreement and miss-
ing values. Krippendorff proposes distance met-
rics suitable for various scales: binary, nominal,
interval, ordinal, and ratio. In ordinal measure-
ment14 the attributes can be rank-ordered, but dis-
tances between them do not have any meaning.
Measured with Krippendorff’s ordinal α, there is
an inter-annotator agreement of α = 0.780 for re-
latedness scores in the Polish evaluation dataset,
which is quite satisfactory as well. Hence, we con-
clude that our dataset is a reliable resource for
the purpose of evaluating compositional distribu-
tional semantics model of Polish.

5 Conclusions

The goal of this paper is to present the proce-
dure of building a Polish evaluation dataset for
the validation of compositional distributional se-
mantics models. As we aim at building an evalua-

14Nominal measurement is useless for measuring agree-
ment between relatedness scores (α = 0.340 is the identi-
cal value as Fleiss’ kappa, since all disagreements are con-
sidered equal). We also test interval measurement, in which
the distance between the attributes does have meaning and
an average of an interval variable is computed. The inter-
val score measured for relatedness annotations is quite high
α = 0.785, but we doubt whether the distance between relat-
edness scores is meaningful in this case.

790



tion dataset which is comparable to the SICK cor-
pus, the general assumptions of our procedure cor-
respond to the design principles of the SICK cor-
pus. However, the procedure of building the SICK
corpus cannot be adapted without modifications.
First, the Polish seed-sentences have to be written
based on the images which are selected from 8K
ImageFlickr dataset and split into thematic groups,
since usable datasets are not publicly available.
Second, since the process of transforming sen-
tences seems to be language-specific, the linguistic
transformation rules appropriate for Polish have to
be defined from scratch. Third, the process of ar-
ranging Polish sentences into pairs is defined anew
taking into account the data characteristic and bi-
directional entailment annotations. The discrepan-
cies relative to the SICK procedure also concern
the annotation process itself. Since an entailment
relation between two sentences must not be sym-
metric, each sentence pair is annotated for entail-
ment in both directions. Furthermore, we intro-
duce an element of human verification of correct-
ness of automatically transformed sentences and
some additional post-corrections.

The presented procedure of building a dataset
was tested on Polish. However, it is very likely
that the annotation framework will work for other
Slavic languages (e.g. Czech with an excellent de-
pendency parser).

The presented procedure results in building
the Polish test corpus of relatively high quality,
confirmed by the inter-annotator agreement coeffi-
cients of κ = 0.734 (measured with Fleiss’ kappa)
for entailment labels and of α = 0.780 (measured
with Krippendorff’s ordinal alpha) for relatedness
scores.

Acknowledgments

We would like to thank the reliable and tena-
cious annotators of our dataset: Alicja Dziedzic-
Rawska, Bożena Itoya, Magdalena Król, Anna La-
tusek, Justyna Małek, Małgorzata Michalik, Ag-
nieszka Norwa, Małgorzata Szajbel-Keck, Alicja
Walichnowska, Konrad Zieliński, and some other.
The research presented in this paper was supported
by SONATA 8 grant no 2014/15/D/HS2/03486
from the National Science Centre Poland.

References

Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. SemEval-2012 Task 6: A
Pilot on Semantic Textual Similarity. In Proceed-
ings of the First Joint Conference on Lexical and
Computational Semantics (*SEM). pages 385–393.

Ron Artstein and Massimo Poesio. 2008. Inter-Coder
Agreement for Computational Linguistics. Compu-
tational Linguistics 34:557–596.

Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing. pages
1183–1193.

Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A Neural Probabilistic Lan-
guage Model. Journal of Machine Learning Re-
search 3:1137–1155.

Yoshua Bengio, Holger Schwenk, Jean-Sébastien
Senécal, Fréderic Morin, and Jean-Luc Gauvain.
2006. Neural Probabilistic Language Models. In
D.E. Holmes and L.C. Jain, editors, Innovations
in Machine Learning. Theory and Applications,
Springer-Verlag, Berlin Heidelberg, volume 194 of
Studies in Fuzziness and Soft Computing, pages
137–186.

Luisa Bentivogli, Raffaella Bernardi, Marco Marelli,
Stefano Menini, Marco Baroni, and Roberto Zam-
parelli. 2014. SICK through the SemEval Glasses.
Lesson learned from the evaluation of composi-
tional distributional semantic models on full sen-
tences through semantic relatedness and textual en-
tailment. Journal of Language Resources and Eval-
uation 50:95–124.

Jacob Cohen. 1960. A coefficient of agreement for
nominal scales. Educational and Psychological
Measurement 20:37–46.

John Rupert Firth. 1957. A synopsis of linguistic the-
ory, 1930-1955. Studies in Linguistic Analysis. Spe-
cial volume of the Philological Society pages 1–32.

Joseph L. Fleiss. 1971. Measuring nominal scale
agreement among many raters. Psychological Bul-
letin 75:378–382.

Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimental Support for a Categorical Composi-
tional Distributional Model of Meaning. In Pro-
ceedings of the 2011 Conference on Empirical
Methods in Natural Language Processing (EMNLP
2011). pages 1394–1404.

Zellig Harris. 1954. Distributional structure. Word
10:146–162.

791



Theo M. V. Janssen. 2012. Compositionality: its his-
toric context. In Wolfram Hinzen, Edouard Mach-
ery, and Markus Werning, editors, The Oxford Hand-
book of Compositionality, Oxford University Press,
Studies in Fuzziness and Soft Computing, pages 19–
46.

Klaus Krippendorff. 1980. Content Analysis: An In-
troduction to Its Methodology. Sage Publications,
Beverly Hills.

Klaus Krippendorff. 2013. Content Analysis: An Intro-
duction to Its Methodology. Sage Publication, Thou-
sand Oaks, 3rd edition.

Marco Marelli, Luisa Bentivogli, Marco Baroni, Raf-
faella Bernardi, Stefano Menini, and Roberto Zam-
parelli. 2014. SemEval-2014 Task 1: Evaluation of
Compositional Distributional Semantic Models on
Full Sentences through Semantic Relatedness and
Textual Entailment. In Proceedings of the 8th In-
ternational Workshop on Semantic Evaluation (Se-
mEval 2014). pages 1–8.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeffrey Dean. 2013. Distributed Represen-
tations of Words and Phrases and their Composition-
ality. In Advances in Neural Information Processing
Systems 26. Proceedings of Neural Information Pro-
cessing Systems 2013. pages 3111–3119.

Jeff Mitchell and Mirella Lapata. 2010. Composition
in Distributional Models of Semantics. Cognitive
Science 34:1388–1429.

Cyrus Rashtchian, Peter Young, Micah Hodosh, and
Julia Hockenmaier. 2010. Collecting Image Anno-
tations Using Amazon’s Mechanical Turk. In Pro-
ceedings of the NAACL HLT 2010 Workshop on Cre-
ating Speech and Language Data with Amazon’s
Mechanical Turk. pages 139–147.

William A. Scott. 1955. Reliability of Content Anal-
ysis: The Case of Nominal Scale Coding. Public
Opinion Quarterly 19:321–325.

Richard Socher, Brody Huval, Christopher Manning,
and Andrew Ng. 2012. Semantic Compositionality
through Recursive Matrix-Vector Spaces. In Pro-
ceedings of the 2012 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning. pages
1201–1211.

Alina Wróblewska. 2014. Polish Dependency Parser
Trained on an Automatically Induced Dependency
Bank. Ph.D. dissertation, Institute of Computer Sci-
ence, Polish Academy of Sciences, Warsaw.

792


	Polish evaluation dataset for compositional distributional semantics models

