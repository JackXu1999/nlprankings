



















































Computing the Most Probable String with a Probabilistic Finite State Machine


Proceedings of the 11th International Conference on Finite State Methods and Natural Language Processing, pages 1–8,
St Andrews–Sctotland, July 15–17, 2013. c©2013 Association for Computational Linguistics

Computing the Most Probable String with a Probabilistic Finite State
Machine

Colin de la Higuera
Université de Nantes,

CNRS, LINA, UMR6241,
F-44000, France

cdlh@univ-nantes.fr

Jose Oncina
Dep. de Lenguajes y Sistemas Informáticos,

Universidad de Alicante,
Alicante, Spain
oncina@ua.es

Abstract

The problem of finding the consensus / most
probable string for a distribution generated by
a probabilistic finite automaton or a hidden
Markov model arises in a number of natural
language processing tasks: it has to be solved
in several transducer related tasks like opti-
mal decoding in speech, or finding the most
probable translation of an input sentence. We
provide an algorithm which solves these prob-
lems in time polynomial in the inverse of the
probability of the most probable string, which
in practise makes the computation tractable in
many cases. We also show that this exact com-
putation compares favourably with the tradi-
tional Viterbi computation.

1 Introduction

Probabilistic finite state machines are used to de-
fine distributions over sets of strings, to model lan-
guages, help with decoding or for translation tasks.
These machines come under various names, with
different characteristics: probabilistic (generating)
finite state automata, weighted machines, hidden
Markov models (HMMs) or finite state transducers...

An important and common problem in all the set-
tings is that of computing the most probable event
generated by the machine, possibly under a con-
straint over the input string or the length. The typ-
ical way of handling this question is by using the
Viterbi algorithm, which extracts the most probable
path/parse given the requirements.

If in certain cases finding the most probable parse
is what is seeked, in others this is computed under
the generally accepted belief that the computation

of the most probable string, also called the consen-
sus string, is untractable and that the Viterbi score
is an acceptable approximation. But the probability
of the string is obtained by summing over the dif-
ferent parses, so there is no strong reason that the
string with the most probable parse is also the most
probable one.

The problem of finding the most probable
string was addressed by a number of authors, in
computational linguistics, pattern recognition and
bio-informatics [Sima’an, 2002, Goodman, 1998,
Casacuberta and de la Higuera, 1999, 2000, Lyngsø
and Pedersen, 2002]: the problem was proved to be
NP-hard; the associated decision problem is NP-
complete in limited cases only, because the most
probable string can be exponentially long in the
number of states of the finite state machine (a con-
struction can be found in [de la Higuera and Oncina,
2013]). As a corollary, finding the most probable
translation (or decoding) of some input string, when
given a finite state transducer, is intractable: the
set of possible transductions, with their conditional
probabilities can be represented as a PFA.

Manning and Schütze [1999] argue that the
Viterbi algorithm does not allow to solve the de-
coding problem in cases where there is not a one-
to-one relationship between derivations and parses.
In automatic translation Koehn [2010] proposes to
compute the top n translations from word graphs,
which is possible when these are deterministic. But
when they are not, an alternative in statistical ma-
chine translation is to approximate these thanks to
the Viterbi algorithm [Casacuberta and Vidal, 2004].
In speech recognition, the optimal decoding problem
consists in finding the most probable sequence of ut-
terances. Again, if the model is non-deterministic,

1



this will usually be achieved by computing the most
probable path instead.

In the before mentioned results the weight of each
individual transition is between 0 and 1 and the score
can be interpreted as a probability. An interesting
variant, in the framework of multiplicity automata
or of accepting probabilistic finite automata (also
called Rabin automata), is the question, known as
the cut-point emptiness problem, of the existence of
a string whose weight is above a specific threshold;
this problem is known to be undecidable [Blondel
and Canterini, 2003].

In a recent analysis, de la Higuera and Oncina
[2013] solved an associated decision problem: is
there a string whose probability is above a given
threshold? The condition required is that we are
given an upper bound to the length of the most
probable string and a lower bound to its probability.
These encouraging results do not provide the means
to actually compute the consensus string.

In this paper we provide three main results. The
first (Section 3) relates the probability of a string
with its length; as a corollary, given any fraction
p, either all strings have probability less than p, or
there is a string whose probability is at least p and

is of length at most (n+1)
2

p where n is the number
of states of the corresponding PFA. The second re-
sult (Section 4) is an algorithm that can effectively
compute the consensus string in time polynomial in
the inverse of the probability of this string. Our third
result (Section 5) is experimental: we show that our
algorithm works well, and also that in highly am-
biguous settings, the traditional approach, in which
the Viterbi score is used to return the string with the
most probable parse, will return sub-optimal results.

2 Definitions and notations

2.1 Languages and distributions

Let [n] denote the set {1, . . . , n} for each n ∈ N.
An alphabet Σ is a finite non-empty set of sym-
bols called letters. A string w over Σ is a finite
sequence w = a1 . . . an of letters. Letters will be
indicated by a, b, c, . . ., and strings by u, v, . . . , z.
Let |w| denote the length of w. In this case we have
|w| = |a1 . . . an| = n. The empty string is denoted
by λ.

We denote by Σ⋆ the set of all strings and by

Σ≤n the set of those of length at most n. When de-
composing a string into sub-strings, we will write
w = w1 . . . wn where ∀i ∈ [n] wi ∈ Σ⋆.

A probabilistic language D is a probability distri-
bution over Σ⋆. The probability of a string x ∈ Σ⋆
under the distribution D is denoted as PrD(x) and
must verify

∑
x∈Σ⋆ PrD(x) = 1. If L is a language

(thus a set of strings, included in Σ⋆), and D a dis-
tribution over Σ⋆, PrD(L) =

∑
x∈L PrD(x).

If the distribution is modelled by some syntactic
machine M, the probability of x according to the
probability distribution defined by M is denoted by
PrM(x). The distribution modelled by a machine
M will be denoted by DM and simplified to D if
the context is not ambiguous.

2.2 Probabilistic finite automata

Probabilistic finite automata (PFA) are generative
devices for which there are a number of possible def-
initions [Paz, 1971, Vidal et al., 2005]. In the sequel
we will use λ-free PFA: these do not have empty (λ)
transitions: this restriction is without loss of gen-
erality, as algorithms [Mohri, 2002, de la Higuera,
2010] exist allowing to transform, in polynomial
time, more general PFA into PFA respecting the fol-
lowing definition:

Definition 1. A λ-free Probabilistic Finite Automa-
ton (PFA) is a tuple A = 〈Σ, Q, S, F, δ〉, where:

- Σ is the alphabet;

- Q ={q1,. . . , q|Q|} is a finite set of states;

- S : Q → R ∩ [0, 1] (initial probabilities);

- F : Q → R ∩ [0, 1] (final probabilities);

- δ : Q × Σ × Q → R ∩ [0, 1] is the complete
transition function; δ(q, a, q′) = 0 can be in-
terpreted as “no transition from q to q′ labelled
with a”.

S, δ and F are functions such that:

∑

q∈Q
S(q) = 1, (1)

and ∀q ∈ Q,

F (q) +
∑

a∈Σ, q′∈Q
δ(q, a, q′) = 1. (2)

2



1 : q0

q1 q2

q3: 1

a 0.9

b 0.1

a 0.7

a 0.3

a 0.7

a 0.3

Figure 1: Graphical representation of a PFA.

An example of a PFA is shown in Fig. 1.
Given x∈Σ⋆, ΠA(x) is the set of all paths ac-

cepting x: an accepting x-path is a sequence π =
qi0a1qi1a2 . . . anqin where x = a1 · · · an, ai ∈ Σ,
and ∀j ∈ [n] such that δ(qij−1 , aj , qij ) 6= 0.

The probability of the path π is defined as
PrM(π) = S(qi0) ·

∏
j∈[n] δ(qij−1 , aj , qij ) · F (qin)

and the probability of the string x is obtained by
summing over the probabilities of all the paths in
ΠA(x). An effective computation can be done by
means of the Forward (or Backward) algorithm [Vi-
dal et al., 2005].

We denote by |A| the size of A as one more than
the number of its states. Therefore, for A as repre-
sented in Figure 1, we have |A| = 5.

We do not recall here the definitions for hidden
Markov models. It is known that one can transform
an HMM into a PFA and vice-versa in polynomial
time [Vidal et al., 2005].

2.3 The problem

The goal is to find the most probable string in a prob-
abilistic language. This string is also named the con-
sensus string.

Name: Consensus string (CS)
Instance: A probabilistic machine M
Question: Find x ∈ Σ⋆ such that ∀y ∈ Σ⋆
PrM(x) ≥ PrM(y).

For example, for the PFA from Figure 1, the con-
sensus string is aaaaa. Note that the string having
the most probable single parse is b.

2.4 Associated decision problem

In [de la Higuera and Oncina, 2013] the following
decision problem is studied:

Name: Bounded most probable string (BMPS)
Instance: A λ-free PFA A, an integer p ≥ 0, an in-

teger b
Question: Is there in Σ≤b a string x such that
PrA(x) > p?

BMPS is known to be NP-hard [Casacuberta and
de la Higuera, 2000]. De la Higuera and Oncina
[2013] present a construction proving that the most
probable string can be of exponential length: this
makes the bound issue crucial in order to hope to
solve CS. The proposed algorithm takes p and b
as arguments and solves BMPS in time complexity

O( b|Σ|·|Q|
2

p ). It is assumed that all arithmetic opera-
tions are in constant time.

The construction relies on the following sim-
ple properties, which ensure that only a reasonable
amount of incomparable prefixes have to be scruti-
nized.

Property 1. ∀u ∈ Σ⋆, PrA(uΣ⋆) ≥ PrA(u).
Property 2. If X is a set of strings such that (1)
∀u ∈ X,PrA(uΣ⋆) > p and (2) no string in X is a
prefix of another different string in X, then |X| < 1p .

3 Probable strings are short

Is there a relation between the lengths of the strings
and their probabilities? In other words, can we show
that a string of probability at least p must be reason-
ably short? If so, the b parameter is not required: one
can compute the consensus string without having to
guess the bound b. Let us prove the following:

Proposition 1. Let A be a λ-free PFA with n states
and w a string.

Then |w| ≤ (n+1)2PrA(w) .
As a corollary,

Corollary 1. Let A be a λ-free PFA with n states. If
there is a string with probability at least p, its length

is at most b = (n+1)
2

p .

Proof. Let w be a string of length len and
PrA(w) = p. A path is a sequence π =
qi0a1qi1a2 . . . alenqilen , with ai ∈ Σ.

Let ΠjA(w) be the subset of ΠA(w) of all paths π
for which state qj is the most used state in π.

If, for some path π, there are several values j such
that qj is the most used state in π, we arbitrarily add
π to the ΠjA(w) which has the smallest index j.

Then, because of a typical combinatorial ar-
gument, there exists at least one j such that

3



PrA(Π
j
A(w)) ≥ pn . Note that in any path in Π

j
A(w)

state qj appears at least len+1n times. Consider any

of these paths π in ΠjA(w). Let k be the small-
est integer such that qik = qj (ie the first time we
visit state qj in path π is after having read the first
k characters of w). Then for each value k′ such that
qik′ = qj , we can shorten the path π by removing the
cycle between qik and qik′ and obtain in each case a
path for a new string, and the probability of this path
is at least that of π.

We have therefore at least len+1n − 1 such alter-
native paths for π.

We call Alt(π, j) the set of alternative paths for π
and qj . Hence |Alt(π, j)| ≥ len+1n − 1.

And therefore

PrA(Alt(π, j)) ≥
(
len+ 1

n
− 1

)
PrA(π).

We now want to sum this quantity over the dif-
ferent π in ΠjA(w). Note that there could be a diffi-
culty with the fact that two different paths may share
an identical alternative that would be counted twice.
The following lemma (proved later) tells us that this
is not a problem.

Lemma 1. Let π and π′ be two different paths
in ΠjA(w), and π

′′ be a path belonging both to
Alt(π, j) and to Alt(π′, j). Then PrA(π′′) ≥
PrA(π) + PrA(π′).

Therefore, we can sum and

∑

π∈ΠjA(w)

PrA(Alt(π, j)) ≥
(
len+ 1

n
− 1

)
p

n
.

The left hand side represents a mass of probabil-
ities distributed by A to other strings than w. Sum-
ming with the probability of w, we obtain:

(len+ 1
n

− 1
)
· p
n
+ p ≤ 1

(len+ 1− n) · p+ pn2 ≤ n2

(len+ 1− n) ≤ n
2(1− p)

p

len ≤ n
2(1− p)

p
+ n− 1

It follows that len ≤ (n+1)2p .

Proof of the lemma. π′′ = πj and π′′ = π′j′. Neces-
sarily we have j = j′.

Now qi1kwkqi1k+1wk+1 . . . wk+tqi1k+t and
qi2k

wkqi2k+1
wk+1 . . . wk+tqi2k+t

are the two frag-
ments of the paths that have been removed from
π and π′. These are necessarily different, but
might coincide in part. Let h be the first index for
which they are different, ie ∀z < h, qi1z = qi2z and
qi1h

6= qi2h .
We have:
P (qi1h−1

, wh, qi1h
)+P (qi2h−1

, wh, qi2h
) ≤ 1 and the

result follows.

We use Proposition 1 to associate with a given
string w an upper bound over the probability of any
string having w as a prefix:

Definition 2. The Potential Probability PP of a pre-
fix string w is

PP(w) = min(PrA(wΣ⋆),
|A|2
|w| )

PP(w) is also an upper bound on the probability
of any string having w as a prefix:

Property 3. ∀u ∈ Σ⋆ PrA(wu) ≤ PP(w)
Indeed, PrA(wu) ≤ PrA(wΣ⋆) and, because of

Proposition 1, PrA(wu) ≤ |A|
2

|wu| ≤
|A|2
|w| .

This means that we can decide, for a given prefix
w, and a probability to be improved, if w is viable,
ie if the best string having w as a prefix can be better
than the proposed probability. Furtermore, given a
PFA A and a prefix w, computing PP(w) is simple.

4 Solving the consensus string problem

Algorithm 1 is given a PFA A and returns the most
probable string. The bounds obtained in the previ-
ous section allow us to explore the viable prefixes,
check if the corresponding string improves our cur-
rent candidate, add an extra letter to the viable prefix
and add this new prefix to a priority queue.

The priority queue (Q) is a structure in which the
time complexity for insertion is O(log |Q|) and the
extraction (Pop) of the first element can take place
in constant time. In this case the order for the queue
will depend on the value PP(w) of the prefix w.

4



Data: a PFA A
Result: w, the most probable string

1 Current Prob = 0;
2 Q = [λ];
3 Continue = true;
4 while not(Empty(Q))andContinue do
5 w = Pop(Q);
6 if PP(w) > Current Prob then
7 p = PrA(w);
8 if p > Current Prob then
9 Current Prob = p;

10 Current Best = w;

11 foreach a ∈ Σ do
12 if PP(wa) > Current Prob then
13 Insert(wa,PP(wa),Q)

14 else
15 Continue = false;

16 return Current Best
Algorithm 1: Finding the Consensus String

Analysis: Let popt be the probability of the con-
sensus string. Let Viable be the set of all strings w
such that PP(w) ≥ popt.

• Fact 1. Let w be the first element of Q at some
iteration. If PP(w) < popt, every other ele-
ment of Q will also have smaller PP and the
algorithm will halt. It follows that until the con-
sensus string is found, the first element w is in
Viable.

• Fact 2. If w ∈Viable, PP(w) ≥ popt, therefore
|A|2
|w| ≥ popt so |w| ≤

|A|2
popt

.

• Fact 3. There are at most 1popt pairwise
incomparable prefixes in Viable. Indeed,
all elements of Viable have PP(w) =
min(PrA(wΣ⋆),

|A|2
|w| ) ≥ popt so also have

PrA(wΣ⋆) ≥ popt and by Property 2 we are
done.

• Fact 4. There are at most 1popt ·
|A|2
popt

different
prefixes in Viable, as a consequence of facts 2
and 3.

• Fact 5. At every iteration of the main loop at

most |Σ| new elements are added to the priority
queue.

• Fact 6. Therefore, since only the first elements
of the priority queue will cause (at most |Σ|)
insertions, and these (fact 1) are necesarily vi-
able, the total number of insertions is bounded
by |Σ| · |A|2popt ·

1
popt

.

The time complexity of the algorithm is propor-
tional to the number of insertions in the queue and is
computed as follows:

• |Q | is at most |Σ|·|A|2
p2opt

;

• Insertion of an element into Q is in
O
(
log

(
|Σ|·|A|2

p2opt

))
.

5 Experiments

From the theoretical analysis it appears that the new
algorithm will be able to compute the consensus
string. The goal of the experiments is therefore to
show how the algorithm scales up: there are domains
in natural language processing where the probabili-
ties are very small, and the alphabets very large. In
others this is not the case. How well does the algo-
rithm adapt to small probabilities? A second line of
experiments consists in measuring the quality of the
most probable parse for the consensus string. This
could obviously not be measured up to now (because
of the lack of an algorithm for the most probable
string). Finding out how far (both in value and in
rank) the string returned by the Viterbi algorithm is
from the consensus string is of interest.

5.1 Further experiments

A more extensive experimentation may consist in
building a collection of random PFA, in the line of
what has been done in HMM/PFA learning compe-
titions, for instance [Verwer et al., 2012]. A con-
nected graph is built, the arcs are transformed into
transitions by adding labels and weights. A normal-
isation phase takes place so as to end up with a dis-
tribution of probabilities.

The main drawback of this procedure is that, most
often, the consensus string will end up by being the
empty string (or a very short string) and any algo-
rithm will find it easily.

5



Actually, the same will happen when testing on
more realistic data: a language model built from n-
grams will often have as most probable string the
empty string.

An extensive experimentation should also com-
pare this algorithm with alternative techniques
which have been introduced:

A first extension of the Viterbi approximation is
called crunching [May and Knight, 2006]: instead
of just computing for a string the probability of the
best path, with a bit more effort, the value associated
to a string is the sum of the probabilities of the n best
paths.

Another approach is variational decoding [Li
et al., 2009]: in this method and alternatives like
Minimum Risk Decoding, a best approximation by
n-grams of the distribution is used, and the most
probable string is taken as the one which maximizes
the probability with respect to this approximation.
These techniques are shown to give better results
than the Viterbi decoding, but are not able to cope
with long distance dependencies.

Coarse-to-fine parsing [Charniak et al., 2006] is
a strategy that reduces the complexity of the search
involved in finding the best parse. It defines a se-
quence of increasingly more complex Probabilistic
Context-Free grammars (PCFG), and uses the parse
forest produced by one PCFG to prune the search of
the next more complex PCFG.

5.2 A tighter bound on the complexity

The complexity of the algorithm can be measured
by counting the number of insertions into the prior-
ity queue. This number has been upper-bounded by

|Σ| · |A|2
p2opt

. But it is of interest to get a better and

tighter bound. In order to have a difficult set of test
PFAs we built a family of models for which the con-
sensus string has a parametrized length and where an
exponentially large set of strings has the same length
and slightly lower probabilities than the consensus
string.

In order to achieve that, the states are organised
in levels, and at each level there is a fixed number
of states (multiplicity). One of the states of the first
level is chosen as initial state. All the states have
an ending probability of zero except for one unique
state of the last level; there is a transition from each

0.0 0.0 > 0.0

0.0 0.0 0.0

Figure 2: The topology of an automaton with 3 levels and
multiplicity 2

state of level n to all states of level n+ 1 but also to
all the states of level k ≤ n.

An example of this structure, with 3 levels and
multiplicity 2 is represented in Fig. 2. The shortest
string with non-null probability will be of length n−
1 where n is the number of levels.

A collection of random PFA was built with vocab-
ulary size varying from 2 to 6, number of levels from
3 to 5, and the multiplicity from 2 to 3. 16 different
PFA were generated for each vocabulary size, num-
ber of levels and multiplicity. Therefore, a total of
480 automata were built. From those 16 were dis-
carded because the low probability of the consensus
string made it impossible to deal with the size of the
priority queue. 464 automata were therefore consid-
ered for the experiments.

In the first set of experiments the goal was to test
how strong is the theoretical bound on the number
of insertions in the priority queue.

Fig. 3 plots the inverse of the probability of the
consensus string versus the number of insertions in
the priority queue. The bound from Section 4 is

|Q| ≤ |Σ| · |A|
2

p2opt

Our data indicates that

|Q| ≤ 1
p2opt

≤ |Σ| · |A|
2

p2opt

Furthermore, 2popt seems empirically to be a better
bound: a more detailed mathematical analysis could
lead to prove this.

6



1e+00

1e+02

1e+04

1e+06

1e+08

1e+10

1e+12

1e+14

1e+16

1e+00 1e+01 1e+02 1e+03 1e+04 1e+05 1e+06 1e+07 1e+08

in
se

rt
io

ns

1
popt

Checking priority queue bound

automata
2/popt
1/p2opt

Figure 3: Number of insertions made in the priority queue

0 %

20 %

40 %

60 %

80 %

100 %

1e-08 1e-07 1e-06 1e-05 1e-04 1e-03 1e-02 1e-01 1e+00

R
el

at
iv

e
er

ro
r

Consensus string probability

Consensus string vs. most probable path discrepancy

Most probable path

Figure 4: Most probable path accuracy

5.3 How good is the Viterbi approximation?

In the literature the string with the most probable
path is often used as an approximation to the consen-
sus string. Using the same automata, we attempted
to measure the distance between the probability of
the string returned by the Viterbi algorithm, which
computes the most probable parse, and the probabil-
ity of the consensus string.

For each automaton, we have measured the proba-
bility of the most probable string (popt) and the prob-
ability of the most probable path (pp).

In 63% of the 464 PFA the consensus string and
the string with the most probable path were differ-
ent, and in no case does the probability of the con-
sensus string coincide with the probability of the
most probable path.

Figure 4 shows the relative error when using the
probability of the most probable path instead of the
probability of the consensus string. In other words
we measure and plot popt−pppopt . It can be observed
that the relative error can be very large and increases
as the probability of the consensus string decreases.
Furthermore, we ran an experiment to compute the
rank of the string with most probable path, when or-
dered by the actual probability. Over the proposed
benchmark, the average rank is 9.2 and the maxi-
mum is 277.

6 Conclusion

The algorithm provided in this work allows to com-
pute the most probable string with its exact probabil-
ity. Experimentally it works well in settings where
the number of paths involved in the sums leading to
the computation of the probability is large: in arti-
ficial experiments, it allowed to show that the best
string for the Viterbi score could be outranked by
more that 10 alternative strings.

Further experiments in natural language process-
ing tasks are still required in order to understand in
which particular settings the algorithm can be of use.
In preliminary language modelling tasks two diffi-
culties arose: the most probable string is going to
be extremely short and of little interest. Further-
more, language models use very large alphabets, so
the most probable string of length 10 will typically
have a probability of the order of 10−30. In our ex-
periments the algorithm was capable of dealing with
figures of the order of 10−6. But the difference is
clearly impossible to deal with.

The proposed algorithm can be used in cases
where the number of possible translations and
paths may be very large, but where at least one
string (or translation) has a reasonable probability.
Other future research directions concern in lifting
the λ-freeness condition by taking into acount λ-
transitions on the fly: in many cases, the transforma-
tion is cumbersome. Extending this work to proba-
bilistic context-free grammars is also an issue.

Acknowledgement

Discussions with Khalil Sima’an proved important
in a previous part of this work, as well as later help
from Thomas Hanneforth. We also wish to thank the

7



anonymous reviewer who pointed out a certain num-
ber of alternative heuristics that should be compared
more extensively.

The first author acknowledges partial support by
the Région des Pays de la Loire. The second au-
thor thanks the Spanish CICyT for partial support
of this work through projects TIN2009-14205-C04-
C1, and the program CONSOLIDER INGENIO 2010
(CSD2007-00018).

References

V. D. Blondel and V. Canterini. Undecidable prob-
lems for probabilistic automata of fixed dimen-
sion. Theory of Computer Systems, 36(3):231–
245, 2003.

F. Casacuberta and C. de la Higuera. Optimal lin-
guistic decoding is a difficult computational prob-
lem. Pattern Recognition Letters, 20(8):813–821,
1999.

F. Casacuberta and C. de la Higuera. Computational
complexity of problems on probabilistic gram-
mars and transducers. In A. L. de Oliveira, editor,
Grammatical Inference: Algorithms and Applica-
tions, Proceedings of ICGI ’00, volume 1891 of
LNAI, pages 15–24. Springer-Verlag, 2000.

F. Casacuberta and E. Vidal. Machine translation
with inferred stochastic finite-state transducers.
Computational Linguistics, 30(2):205–225, 2004.

E. Charniak, M. Johnson, M. Elsner, J. L. Auster-
weil, D. Ellis, I. Haxton, C. Hill, R. Shrivaths,
J. Moore, M. Pozar, and T. Vu. Multilevel coarse-
to-fine PCFG parsing. In Proceedings of HLT-
NAACL 2006. The Association for Computer Lin-
guistics, 2006.

C. de la Higuera. Grammatical inference: learning
automata and grammars. Cambridge University
Press, 2010.

C. de la Higuera and J. Oncina. The most probable
string: an algorithmic study. Journal of Logic and
Computation, doi: 10.1093/logcom/exs049, 2013.

J. T. Goodman. Parsing Inside–Out. PhD thesis,
Harvard University, 1998.

P. Koehn. Statistical Machine Translation. Cam-
bridge University Press, 2010.

Z. Li, J. Eisner, and S. Khudanpur. Variational de-
coding for statistical machine translation. In Pro-
ceedings of ACL/IJCNLP 2009, pages 593–601.
The Association for Computer Linguistics, 2009.

R. B. Lyngsø and C. N. S. Pedersen. The consen-
sus string problem and the complexity of compar-
ing hidden Markov models. Journal of Computing
and System Science, 65(3):545–569, 2002.

C. Manning and H. Schütze. Foundations of Statis-
tical Natural Language Processing. MIT Press,
1999.

J. May and K. Knight. A better n-best list: Practical
determinization of weighted finite tree automata.
In Proceedings of HLT-NAACL 2006. The Asso-
ciation for Computer Linguistics, 2006.

M. Mohri. Generic e-removal and input e-
normalization algorithms for weighted transduc-
ers. International Journal on Foundations of
Computer Science, 13(1):129–143, 2002.

A. Paz. Introduction to probabilistic automata. Aca-
demic Press, New York, 1971.

K. Sima’an. Computational complexity of proba-
bilistic disambiguation: NP-completeness results
for language and speech processing. Grammars,
5(2):125–151, 2002.

S. Verwer, R. Eyraud, and C. de la Higuera. Results
of the pautomac probabilistic automaton learn-
ing competition. In Journal of Machine Learning
Research - Proceedings Track, volume 21, pages
243–248, 2012.

E. Vidal, F. Thollard, C. de la Higuera, F. Casacu-
berta, and R. C. Carrasco. Probabilistic finite state
automata – part I and II. Pattern Analysis and Ma-
chine Intelligence, 27(7):1013–1039, 2005.

8


