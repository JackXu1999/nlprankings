



















































Online Segment to Segment Neural Transduction


Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1307–1316,
Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics

Online Segment to Segment Neural Transduction

Lei Yu1, Jan Buys1 and Phil Blunsom1,2
1University of Oxford

2DeepMind
{lei.yu, jan.buys, phil.blunsom}@cs.ox.ac.uk

Abstract

We introduce an online neural sequence to se-
quence model that learns to alternate between
encoding and decoding segments of the input
as it is read. By independently tracking the en-
coding and decoding representations our algo-
rithm permits exact polynomial marginaliza-
tion of the latent segmentation during train-
ing, and during decoding beam search is em-
ployed to find the best alignment path to-
gether with the predicted output sequence.
Our model tackles the bottleneck of vanilla
encoder-decoders that have to read and mem-
orize the entire input sequence in their fixed-
length hidden states before producing any out-
put. It is different from previous attentive
models in that, instead of treating the at-
tention weights as output of a deterministic
function, our model assigns attention weights
to a sequential latent variable which can be
marginalized out and permits online gener-
ation. Experiments on abstractive sentence
summarization and morphological inflection
show significant performance gains over the
baseline encoder-decoders.

1 Introduction

The problem of mapping from one sequence to an-
other is an importance challenge of natural language
processing. Common applications include machine
translation and abstractive sentence summarisation.
Traditionally this type of problem has been tackled
by a combination of hand-crafted features, align-
ment models, segmentation heuristics, and language
models, all of which are tuned separately.

The recently introduced encoder-decoder
paradigm has proved very successful for machine
translation, where an input sequence is encoded
into a fixed-length vector and an output sequence
is then decoded from said vector (Kalchbrenner
and Blunsom, 2013; Sutskever et al., 2014; Cho
et al., 2014). This architecture is appealing, as it
makes it possible to tackle the problem of sequence-
to-sequence mapping by training a large neural
network in an end-to-end fashion. However it is
difficult for a fixed-length vector to memorize all
the necessary information of an input sequence,
especially for long sequences. Often a very large
encoding needs to be employed in order to capture
the longest sequences, which invariably wastes
capacity and computation for short sequences.
While the attention mechanism of Bahdanau et al.
(2015) goes some way to address this issue, it still
requires the full input to be seen before any output
can be produced.

In this paper we propose an architecture to tackle
the limitations of the vanilla encoder-decoder model,
a segment to segment neural transduction model
(SSNT) that learns to generate and align simul-
taneously. Our model is inspired by the HMM
word alignment model proposed for statistical ma-
chine translation (Vogel et al., 1996; Tillmann et
al., 1997); we impose a monotone restriction on the
alignments but incorporate recurrent dependencies
on the input which enable rich locally non-monotone
alignments to be captured. This is similar to the se-
quence transduction model of Graves (2012), but we
propose alignment distributions which are parame-
terised separately, making the model more flexible

1307



and allowing online inference.
Our model introduces a latent segmentation which

determines correspondences between tokens of the
input sequence and those of the output sequence.
The aligned hidden states of the encoder and de-
coder are used to predict the next output token and to
calculate the transition probability of the alignment.
We carefully design the input and output RNNs such
that they independently update their respective hid-
den states. This enables us to derive an exact dy-
namic programme to marginalize out the hidden
segmentation during training and an efficient beam
search to generate online the best alignment path to-
gether with the output sequence during decoding.
Unlike previous recurrent segmentation models that
only capture dependencies in the input (Graves et al.,
2006; Kong et al., 2016), our segmentation model
is able to capture unbounded dependencies in both
the input and output sequences while still permitting
polynomial inference.

While attentive models treat the attention weights
as output of a deterministic function, our model as-
signs attention weights to a sequential latent variable
which can be marginalized out. Our model is gen-
eral and could be incorporated into any RNN-based
encoder-decoder architecture, such as Neural Turing
Machines (Graves et al., 2014), memory networks
(Weston et al., 2015; Kumar et al., 2016) or stack-
based networks (Grefenstette et al., 2015), enabling
such models to process data online.

We conduct experiments on two different trans-
duction tasks, abstractive sentence summarisation
(sequence to sequence mapping at word level) and
morphological inflection generation (sequence to se-
quence mapping at character level). We evaluate
our proposed algorithms in both the online setting,
where the input is encoded with a unidirectional
LSTM, and where the whole input is available such
that it can be encoded with a bidirectional network.
The experimental results demonstrate the effective-
ness of SSNT — it consistently output performs
the baseline encoder-decoder approach while requir-
ing significantly smaller hidden layers, thus show-
ing that the segmentation model is able to learn to
break one large transduction task into a series of
smaller encodings and decodings. When bidirec-
tional encodings are used the segmentation model
outperforms an attention-based benchmark. Quali-

</s>

.

year

new

lunar

the

for

thursday

close

markets

financial

chinese
ch

ine
se

ma
rke

ts

clo
sed for pu

bli
c

ho
lid

ay .
</s
>

Figure 1: Example output of our recurrent segmenta-
tion model on the task of abstractive sentence sum-
marisation. The path highlighted is the alignment
found by the model during decoding.

tative analysis shows that the alignments found by
our model are highly intuitive and demonstrates that
the model learns to read ahead the required number
of tokens before producing output.

2 Model

Let xI1 be the input sequence of length I and y
J
1 the

output sequence of length J . Let yj denote the j-
th token of y. Our goal is to model the conditional
distribution

p(y|x) =
J∏

j=1

p(yj |yj−11 ,x). (1)

We introduce a hidden alignment sequence aJ1
where each aj = i corresponds to an input position
i ∈ {1, . . . , I} that we want to focus on when gener-
ating yj . Then p(y|x) is calculated by marginalizing
over all the hidden alignments,

1308



p(y|x) = ∑a p(y,a|x) (2)
≈ ∑a

∏J
j=1 p(aj |aj−1,y

j−1
1 ,x)︸ ︷︷ ︸

transition probability

·

p(yj |yj−11 , aj ,x).︸ ︷︷ ︸
word prediction

Figure 1 illustrates the model graphically. Each
path from the top left node to the right-most column
in the graph corresponds to an alignment. We con-
strain the alignments to be monotone, i.e. only for-
ward and downward transitions are permitted at each
point in the grid. This constraint enables the model
to learn to perform online generation. Additionally,
the model learns to align input and output segments,
which means that it can learn local reorderings by
memorizing phrases. Another possible constraint on
the alignments would be to ensure that the entire in-
put sequence is consumed before last output word is
emitted, i.e. all valid alignment paths have to end in
the bottom right corner of the grid. However, we do
not enforce this constraint in our setup.

The probability contributed by an alignment is ob-
tained by accumulating the probability of word pre-
dictions at each point on the path and the transition
probability between points. The transition probabil-
ities and the word output probabilities are modeled
by neural networks, which are described in detail in
the following sub-sections.

2.1 Probabilities of Output Word Predictions

The input sentence x is encoded with a Recur-
rent Neural Network (RNN), in particular an LSTM
(Hochreiter and Schmidhuber, 1997). The encoder
can either be a unidirectional or bidirectional LSTM.
If a unidirectional encoder is used the model is able
to read input and generate output symbols online.
The hidden state vectors are computed as

h→i = RNN(h
→
i−1, v

(e)(xi)), (3)

h←i = RNN(h
←
i+1, v

(e)(xi)), (4)

where v(e)(xi) denotes the vector representation of
the token x, and h→i and h

←
i are the forward and

backward hidden states, respectively. For a bidi-
rectional encoder, they are concatenated as hi =

[h→i ;h
←
i ]; and for unidirectional encoder hi = h

→
i .

The hidden state sj of the RNN for the output se-
quence y is computed as

sj = RNN(sj−1, v(d)(yj−1)), (5)

where v(d)(yj−1) is the encoded vector of the pre-
viously generated output word yj−1. That is, sj en-
codes yj−11 .

To calculate the probability of the next word, we
concatenate the aligned hidden state vectors sj and
haj and feed the result into a softmax layer,

p(yj = l|yj−11 , aj ,x)
= p(yj = l|haj , sj)
= softmax(Ww[haj ; sj ] + bw)l.

(6)

The word output distribution in Graves (2012) is pa-
rameterised in similar way.

Figure 2 illustrates the model structure. Note that
the hidden states of the input and output decoders are
kept independent to permit tractable inference, while
the output distributions are conditionally dependent
on both.

2.2 Transition Probabilities
As the alignments are constrained to be monotone,
we can treat the transition from timestep j to j+1 as
a sequence of shift and emit operations. Specif-
ically, at each input position, a decision of shift
or emit is made by the model; if the operation is
emit then the next output word is generated; other-
wise, the model will shift to the next input word.
While the multinomial distribution is an alternative
for parameterising alignments, the shift/emit param-
eterisation does not place an upper limit on the jump
size, as a multinomial distribution would, and biases
the model towards shorter jump sizes, which a multi-
nomial model would have to learn.

We describe two methods for modelling the align-
ment transition probability. The first approach is in-
dependent of the input or output words. To parame-
terise the alignment distribution in terms of shift and
emit operations we use a geometric distribution,

p(aj |aj−1) = (1− e)aj−aj−1e, (7)

where e is the emission probability. This transition
probability only has one parameter e, which can be

1309



x3

x2

x1

s1

h1

<s> y1 y2 y3

y1

Figure 2: The structure of our model. (x1, x2, x3)
and (y1, y2, y3) denote the input and output se-
quences, respectively. The points, e.g. (i, j), in
the grid represent an alignment between xi and yj .
For each column j, the concatenation of the hidden
states [hi, sj ] is used to predict yj .

estimated directly by maximum likelihood as

e =

∑
n Jn∑

n In +
∑

n Jn
, (8)

where In and Jn are the lengths of the input and out-
put sequences of training example n, respectively.

For the second method we model the transition
probability with a neural network,

p(a1 = i) =

i−1∏

d=1

(1− p(ed,1))p(ei,1),

p(aj = i|aj−1 = k) =
i−1∏

d=k

(1− p(ed,j))p(ei,j),

(9)

where p(ei,j) denotes the probability of emit for
the alignment aj = i. This probability is obtained by
feeding [hi; sj ] into a feed forward neural network,

p(ei,j) = σ(MLP(Wt[hi; sj ] + bt)). (10)

For simplicity, p(aj = i|aj−1 = k, sj ,hik) is abbre-
viated as p(aj = i|aj−1 = k).

3 Training and Decoding

Since there are an exponential number of possi-
ble alignments, it is computationally intractable to

explicitly calculate every p(y,a|x) and then sum
them to get the conditional probability p(y|x). We
instead approach the problem using a dynamic-
programming algorithm similar to the forward-
backward algorithm for HMMs (Rabiner, 1989).

3.1 Training
For an input x and output y, the forward variable
α(i, j) = p(aj = i,y

j
1|x). The value of α(i, j) is

computed by summing over the probabilities of ev-
ery path that could lead to this cell. Formally, α(i, j)
is defined as follows:

For i ∈ [1, I]:

α(i, 1) = p(a1 = i)p(y1|hi, s1). (11)

For j ∈ [2, J ], i ∈ [1, I]:
α(i, j) = p(yj |hi, sj)· (12)

i∑

k=1

α(k, j − 1)p(aj = i|aj−1 = k).

The backward variables, defined as β(i, j) =
p(yJj+1|aj = i,yj1,x), are computed as:

For i ∈ [1, I]:
β(i, J) = 1. (13)

For j ∈ [1, J − 1], i ∈ [1, I]:

β(i, j) =
I∑

k=i

p(aj+1 = k|aj = i)β(k, j + 1)·

p(yj+1|hk, sj+1). (14)
During training we estimate the parameters by

minimizing the negative log likelihood of the train-
ing set S:

L(θ) = −
∑

(x,y)∈S
log p(y|x;θ)

= −
∑

(x,y)∈S
log

I∑

i=1

α(i, J).

(15)

Let θj be the neural network parameters w.r.t. the
model output at position j. The gradient is computed
as:

∂ log p(y|x;θ)
∂θ

=
J∑

j=1

I∑

i=1

∂ log p(y|x;θ)
∂α(i, j)

∂α(i, j)

∂θj
.

(16)

1310



The derivative w.r.t. the forward weights is

∂ log p(y|x;θ)
∂α(i, j)

=
β(i, j)

p(y|x;θ) . (17)

The derivative of the forward weights w.r.t. the
model parameters at position j is

∂α(i, j)

∂θj
=
∂p(yj |hi, sj)

∂θj

α(i, j)

p(yj |hi, sj)

+ p(yj |hi, sj)
i∑

k=1

α(j − 1, k) ∂
∂θj

p(aj=i|aj−1=k).

(18)

For the geometric distribution transition probabil-
ity model ∂∂θj p(aj = i|aj−1 = k) = 0.

3.2 Decoding

Algorithm 1 DP search algorithm
Input: source sentence x
Output: best output sentence y∗
Initialization: Q ∈ RI×Jmax , bp ∈ NI×Jmax ,
W ∈ NI×Jmax , Iend, Jend.
for i ∈ [1, I] do

Q[i, 1]← maxy∈V p(a1 = i)p(y|hi, s1)
bp[i, 1]← 0
W [i, 1]← argmaxy∈V p(a1 = i)p(y|hi, s1)

end for
for j ∈ [2, Jmax] do

for i ∈ [1, I] do
Q[i, j]← maxy∈V,k∈[1,i]Q[k, j − 1]·

p(aj = i|aj−1 = k)p(y|hi, sj)
bp[i, j],W [i, j]← argmaxy∈V,k∈[1,i] ·
Q[k, j − 1]p(aj = i|aj−1 = k)p(y|hi, sj)

end for
Iend ← argmaxiQ[i, j]
if W [Iend, j] = EOS then

Jend ← j
break

end if
end for
return a sequence of words stored in W by fol-
lowing backpointers starting from (Iend, Jend).

For decoding, we aim to find the best output se-
quence y∗ for a given input sequence x:

y∗ = argmax
y

p(y|x) (19)

The search algorithm is based on dynamic program-
ming (Tillmann et al., 1997). The main idea is to
create a path probability matrix Q, and fill each cell
Q[i, j] by recursively taking the most probable path
that could lead to this cell. We present the greedy
search algorithm in Algorithm 1. We also imple-
mented a beam search that tracks the k best partial
sequences at position (i, j). The notation bp refers
to backpointers, W stores words to be predicted, V
denotes the output vocabulary, Jmax is the maximum
length of the output sequences that the model is al-
lowed to predict.

4 Experiments

We evaluate the effectiveness of our model on two
representative natural language processing tasks,
sentence compression and morphological inflection.
The primary aim of this evaluation is to assess
whether our proposed architecture is able to outper-
form the baseline encoder-decoder model by over-
coming its encoding bottleneck. We further bench-
mark our results against an attention model in order
to determine whether our alternative alignment strat-
egy is able to provide similar benefits while process-
ing the input online.

4.1 Abstractive Sentence Summarisation
Sentence summarisation is the task of generating
a condensed version of a sentence while preserv-
ing its meaning. In abstractive sentence summari-
sation, summaries are generated from the given vo-
cabulary without the constraint of copying words in
the input sentence. Rush et al. (2015) compiled a
data set for this task from the annotated Gigaword
data set (Graff et al., 2003; Napoles et al., 2012),
where sentence-summary pairs are obtained by pair-
ing the headline of each article with its first sentence.
Rush et al. (2015) use the splits of 3.8m/190k/381k
for training, validation and testing. In previous
work on this dataset, Rush et al. (2015) proposed
an attention-based model with feed-forward neural
networks, and Chopra et al. (2016) proposed an
attention-based recurrent encoder-decoder, similar
to one of our baselines.

Due to computational constraints we place the fol-
lowing restrictions on the training and validation set:

1. The maximum lengths for the input sentences

1311



Model ROUGE-1 ROUGE-2 ROUGE-L

Seq2seq 25.16 9.09 23.06
Attention 29.25 12.85 27.32

uniSSNT 26.96 10.54 24.59
biSSNT 27.05 10.62 24.64
uniSSNT+ 30.15 13.59 27.88
biSSNT+ 30.27 13.68 27.91

Table 1: ROUGE F1 scores on the sentence sum-
marisation test set. Seq2seq refers to the vanilla
encoder-decoder and attention denotes the attention-
based model. SSNT denotes our model with align-
ment transition probability modelled as geometric
distribution. SSNT+ refers to our model with tran-
sition probability modelled using neural networks.
The prefixes uni- and bi- denote using unidirectional
and bidirectional encoder LSTMs, respectively.

and summaries are 50 and 25, respectively.

2. For each sentence-summary pair, the product
of the input and output lengths should be no
greater than 500.

We use the filtered 172k pairs for validation and
sample 1m pairs for training. While this training set
is smaller than that used in previous work (and there-
fore our results cannot be compared directly against
reported results), it serves our purpose for evaluat-
ing our algorithm against sequence to sequence and
attention-based approaches under identical data con-
ditions. Following from previous work (Rush et al.,
2015; Chopra et al., 2016; Gülçehre et al., 2016),
we report results on a randomly sampled test set
of 2000 sentence-summary pairs. The quality of
the generated summaries are evaluated by three ver-
sions of ROUGE for different match lengths, namely
ROUGE-1 (unigrams), ROUGE-2 (bigrams), and
ROUGE-L (longest-common substring).

For training, we use Adam (Kingma and Ba,
2015) for optimization, with an initial learning rate
of 0.001. The mini-batch size is set to 32. The
number of hidden units H is set to 256 for both our
model and the baseline models, and dropout of 0.2 is
applied to the input of LSTMs. All hyperparameters
were optimised via grid search on the perplexity of
the validation set. We use greedy decoding to gener-
ate summaries.

Model Configuration Perplexity

Seq2seq

H = 128, L = 1 48.5
H = 256, L = 1 35.6
H = 256, L = 2 32.1
H = 256, L = 3 31.0

biSSNT+
H = 128, L = 1 26.7
H = 256, L = 1 22.6

Table 2: Perplexity on the validation set with 172k
sentence-summary pairs.

Table 1 displays the ROUGE-F1 scores of our
models on the test set, together with baseline mod-
els, including the attention-based model. Our
models achieve significantly better results than
the vanilla encoder-decoder and outperform the
attention-based model. The fact that SSNT+ per-
forms better is in line with our expectations, as the
neural network-parameterised alignment model is
more expressive than that modelled by geometric
distribution.

To make further comparison, we experimented
with different sizes of hidden units and adding more
layers to the baseline encoder-decoder. Table 2 lists
the configurations of different models and their cor-
responding perplexities on the validation set. We can
see that the vanilla encoder-decoder tends to get bet-
ter results by adding more hidden units and stacking
more layers. This is due to the limitation of com-
pressing information into a fixed-size vector. It has
to use larger vectors and deeper structure in order to
memorize more information. By contrast, our model
can do well with smaller networks. In fact, even with
1 layer and 128 hidden units, our model works much
better than the vanilla encoder-decoder with 3 layers
and 256 hidden units per layer.

4.2 Morphological Inflection
Morphological inflection generation is the task of
predicting the inflected form of a given lexical item
based on a morphological attribute. The transforma-
tion from a base form to an inflected form usually in-
cludes concatenating it with a prefix or a suffix and
substituting some characters. For example, the in-
flected form of a German stem abgang is abgängen
when the case is dative and the number is plural.

In our experiments, we use the same dataset as

1312



Model Avg. accuracy

Seq2Seq 79.08
Seq2Seq w/ Attention 95.64
Adapted-seq2seq (FTND16) 96.20

uniSSNT+ 87.85
biSSNT+ 95.32

Table 3: Average accuracy over all the morpho-
logical inflection datasets. The baseline results for
Seq2Seq variants are taken from (Faruqui et al.,
2016).

Faruqui et al. (2016). This dataset was originally
created by Durrett and DeNero (2013) from Wik-
tionary, containing inflections for German nouns
(de-N), German verbs (de-V), Spanish verbs (es-
V), Finnish noun and adjective (fi-NA), and Finnish
verbs (fi-V). It was further expanded by Nicolai et
al. (2015) by adding Dutch verbs (nl-V) and French
verbs (fr-V). The number of inflection types for each
language ranges from 8 to 57. The number of base
forms, i.e. the number of instances in each dataset,
ranges from 2000 to 11200. The predefined split is
200/200 for dev and test sets, and the rest of the data
for training.

Our model is trained separately for each type of
inflection, the same setting as the factored model
described in Faruqui et al. (2016). The model is
trained to predict the character sequence of the in-
flected form given that of the stem. The output is
evaluated by accuracies of string matching. For all
the experiments on this task we use 128 hidden units
for the LSTMs and apply dropout of 0.5 on the input
and output of the LSTMs. We use Adam (Kingma
and Ba, 2015) for optimisation with initial learning
rate of 0.001. During decoding, beam search is em-
ployed with beam size of 30.

Table 3 gives the average accuracy of the
uniSSNT+, biSSNT+, vanilla encoder-decoder, and
attention-based models. The model with the best
previous average result — denoted as adapted-
seq2seq (FTND16) (Faruqui et al., 2016) — is also
included for comparison. Our biSSNT+ model out-
performs the vanilla encoder-decoder by a large
margin and almost matches the state-of-the-art result
on this task. As mentioned earlier, a characteristic
of these datasets is that the stems and their corre-

Dataset DDN13 NCK15 FTND16 biSSNT+

de-N 88.31 88.60 88.12 87.50
de-V 94.76 97.50 97.72 92.11
es-V 99.61 99.80 99.81 99.52
fi-NA 92.14 93.00 95.44 95.48
fi-V 97.23 98.10 97.81 98.10
fr-V 98.80 99.20 98.82 98.65
nl-V 90.50 96.10 96.71 95.90

Avg. 94.47 96.04 96.20 95.32

Table 4: Comparison of the performance of our
model (biSSNT+) against the previous state-of-the-
art on each morphological inflection dataset.

sponding inflected forms mostly overlap. Compare
to the vanilla encoder-decoder, our model is better at
copying and finding correspondences between pre-
fix, stem and suffix segments.

Table 4 compares the results of biSSNT+ and pre-
vious models on each individual dataset. DDN13
and NCK15 denote the models of Durrett and DeN-
ero (2013) and Nicolai et al. (2015), respectively.
Both models tackle the task by feature engineering.
FTND16 (Faruqui et al., 2016) adapted the vanilla
encoder-decoder by feeding the i-th character of the
encoded string as an extra input into the i-th position
of the decoder. It can be considered as a special case
of our model by forcing a fixed diagonal alignment
between input and output sequences. Our model
achieves comparable results to these models on all
the datasets. Notably it outperforms other models on
the Finnish noun and adjective, and verbs datasets,
whose stems and inflected forms are the longest.

5 Alignment Quality

Figure 3 presents visualisations of segment align-
ments generated by our model for sample instances
from both tasks. We see that the model is able to
learn the correct correspondences between segments
of the input and output sequences. For instance, the
alignment follows a nearly diagonal path for the ex-
ample in Figure 3c, where the input and output se-
quences are identical. In Figure 3b, it learns to add
the prefix ‘ge’ at the start of the sequence and replace
‘en’ with ‘t’ after copying ‘zock’. We observe that
the model is robust on long phrasal mappings. As

1313



</s>
.

,
director

managing
new

a
appointed

has
,

daily
business
us-based

the
of

edition
asian

the
,

asia
journal

street
wall
the

w
al

l

st
re

et

jo
ur

na
l

as
ia

na
m

es

ne
w

m
an

ag
in

g

di
re

ct
or

<
/s
>

...

(a)

</s>
n
e
k
c
o
z

g e z o c k t </s>

(b)

</s>
i
t
n
y
y

m
s
u
n
n
e
l
a

a l e n n u s m y y n t i </s>

(c)

Figure 3: Example alignments found by BiSSNT+. Highlighted grid cells represent the correspondence
between the input and output tokens.

shown in Figure 3a, the mapping between ‘the wall
street journal asia, the asian edition of the us-based
business daily’ and ‘wall street journal asia’ demon-
strates that our model learns to ignore phrasal mod-
ifiers containing additional information. We also
find some examples of word reordering, e.g., the
phrase ‘industrial production in france’ is reordered
as ‘france industrial output’ in the model’s predicted
output.

6 Related Work

Our work is inspired by the seminal HMM align-
ment model (Vogel et al., 1996; Tillmann et al.,
1997) proposed for machine translation. In contrast
to that work, when predicting a target word we addi-
tionally condition on all previously generated words,
which is enabled by the recurrent neural models.
This means that the model also functions as a con-
ditional language model. It can therefore be applied
directly, while traditional models have to be com-
bined with a language model through a noisy chan-
nel in order to be effective. Additionally, instead of
EM training on the most likely alignments at each

iteration, our model is trained with direct gradient
descent, marginalizing over all the alignments.

Latent variables have been employed in neural
network-based models for sequence labelling tasks
in the past. Examples include connectionist tem-
poral classification (CTC) (Graves et al., 2006) for
speech recognition and the more recent segmental
recurrent neural networks (SRNNs) (Kong et al.,
2016), with applications on handwriting recogni-
tion and part-of-speech tagging. Weighted finite-
state transducers (WFSTs) have also been aug-
mented to encode input sequences with bidirectional
LSTMs (Rastogi et al., 2016), permitting exact in-
ference over all possible output strings. While these
models have been shown to achieve appealing per-
formance on different applications, they have com-
mon limitations in terms of modelling dependencies
between labels. It is not possible for CTCs to model
explicit dependencies. SRNNs and neural WFSTs
model fixed-length dependencies, making it is diffi-
cult to carry out effective inference as the dependen-
cies become longer.

Our model shares the property of the sequence

1314



transduction model of Graves (2012) in being able
to model unbounded dependencies between output
tokens via an output RNN. This property makes it
possible to apply our model to tasks like summarisa-
tion and machine translation that require the tokens
in the output sequence to be modelled highly depen-
dently. Graves (2012) models the joint distribution
over outputs and alignments by inserting null sym-
bols (representing shift operations) into the output
sequence. During training the model uses dynamic
programming to marginalize over permutations of
the null symbols, while beam search is employed
during decoding. In contrast our model defines a
separate latent alignment variable, which adds flex-
ibility to the way the alignment distribution can be
defined (as a geometric distribution or parameterised
by a neural network) and how the alignments can
be constrained, without redefining the dynamic pro-
gram. In addition to marginalizing during training,
our decoding algorithm also makes use of dynamic
programming, allowing us to use either no beam or
small beam sizes.

Our work is also related to the attention-
based models first introduced for machine transla-
tion (Bahdanau et al., 2015). Luong et al. (2015)
proposed two alternative attention mechanisms: a
global method that attends all words in the input sen-
tence, and a local one that points to parts of the input
words. Another variation on this theme are pointer
networks (Vinyals et al., 2015), where the outputs
are pointers to elements of the variable-length in-
put, predicted by the attention distribution. Jaitly et
al. (2016) propose an online sequence to sequence
model with attention that conditions on fixed-sized
blocks of the input sequence and emits output tokens
corresponding to each block. The model is trained
with alignment information to generate supervised
segmentations.

Although our model shares the same idea of joint
training and aligning with the attention-based mod-
els, our design has fundamental differences and ad-
vantages. While attention-based models treat the at-
tention weights as output of a deterministic func-
tion (soft-alignment), in our model the attention
weights correspond to a hidden variable, that can be
marginalized out using dynamic programming. Fur-
ther, our model’s inherent online nature permits it
the flexibility to use its capacity to chose how much

input to encode before decoding each segment.

7 Conclusion

We have proposed a novel segment to segment neu-
ral transduction model that tackles the limitations of
vanilla encoder-decoders that have to read and mem-
orize an entire input sequence in a fixed-length con-
text vector before producing any output. By intro-
ducing a latent segmentation that determines corre-
spondences between tokens of the input and output
sequences, our model learns to generate and align
jointly. During training, the hidden alignment is
marginalized out using dynamic programming, and
during decoding the best alignment path is gener-
ated alongside the predicted output sequence. By
employing a unidirectional LSTM as encoder, our
model is capable of doing online generation. Exper-
iments on two representative natural language pro-
cessing tasks, abstractive sentence summarisation
and morphological inflection generation, showed
that our model significantly outperforms encoder-
decoder baselines while requiring much smaller hid-
den layers. For future work we would like to incor-
porate attention-based models to our framework to
enable such models to process data online.

Acknowledgments

We thank Chris Dyer, Karl Moritz Hermann, Ed-
ward Grefenstette, Tomáš Kǒciský, Gabor Melis,
Yishu Miao and many others for their helpful com-
ments. The first author is funded by EPSRC.

References

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In Proceedings of
ICLR.

Kyunghyun Cho, Bart van Merrienboer, Çaglar Gülçehre,
Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk,
and Yoshua Bengio. 2014. Learning phrase represen-
tations using RNN encoder-decoder for statistical ma-
chine translation. In Proceedings of EMNLP.

Sumit Chopra, Michael Auli, and Alexander M. Rush.
2016. Abstractive sentence summarization with at-
tentive recurrent neural networks. In Proceedings of
NAACL.

1315



Greg Durrett and John DeNero. 2013. Supervised learn-
ing of complete morphological paradigms. In Pro-
ceedings of HLT-NAACL.

Manaal Faruqui, Yulia Tsvetkov, Graham Neubig, and
Chris Dyer. 2016. Morphological inflection genera-
tion using character sequence to sequence learning. In
Proceedings of NAACL.

David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda.
2003. English gigaword. Linguistic Data Consortium,
Philadelphia.

Alex Graves, Santiago Fernández, Faustino Gomez, and
Jürgen Schmidhuber. 2006. Connectionist temporal
classification: labelling unsegmented sequence data
with recurrent neural networks. In Proceedings of
ICML.

Alex Graves, Greg Wayne, and Ivo Danihelka. 2014.
Neural turing machines. CoRR, abs/1410.5401.

Alex Graves. 2012. Sequence transduction with recur-
rent neural networks. arXiv preprint arXiv:1211.3711.

Edward Grefenstette, Karl Moritz Hermann, Mustafa Su-
leyman, and Phil Blunsom. 2015. Learning to trans-
duce with unbounded memory. In Proceedings of
NIPS, pages 1819–1827.

Çaglar Gülçehre, Sungjin Ahn, Ramesh Nallapati,
Bowen Zhou, and Yoshua Bengio. 2016. Pointing the
unknown words. CoRR, abs/1603.08148.

Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long
short-term memory. Neural computation, 9(8):1735–
1780.

Navdeep Jaitly, David Sussillo, Quoc V. Le, Oriol
Vinyals, Ilya Sutskever, and Samy Bengio. 2016. A
neural transducer. In Proceedings of NIPS.

Nal Kalchbrenner and Phil Blunsom. 2013. Recur-
rent continuous translation models. In Proceedings of
EMNLP.

Diederik P. Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. In Proceedings of
ICIR.

Lingpeng Kong, Chris Dyer, and Noah A Smith. 2016.
Segmental recurrent neural networks. In Proceedings
of ICLR.

Ankit Kumar, Ozan Irsoy, Jonathan Su, James Bradbury,
Robert English, Brian Pierce, Peter Ondruska, Ishaan
Gulrajani, and Richard Socher. 2016. Ask me any-
thing: Dynamic memory networks for natural lan-
guage processing. In Proceedings of ICML.

Thang Luong, Hieu Pham, and Christopher D. Manning.
2015. Effective approaches to attention-based neural
machine translation. In Proceedings of EMNLP.

Courtney Napoles, Matthew Gormley, and Benjamin
Van Durme. 2012. Annotated gigaword. In Proceed-
ings of the Joint Workshop on Automatic Knowledge
Base Construction and Web-scale Knowledge Extrac-
tion.

Garrett Nicolai, Colin Cherry, and Grzegorz Kondrak.
2015. Inflection generation as discriminative string
transduction. In Proceedings of NAACL.

Lawrence R Rabiner. 1989. A tutorial on hidden markov
models and selected applications in speech recogni-
tion. Proceedings of the IEEE, 77(2):257–286.

Pushpendre Rastogi, Ryan Cotterell, and Jason Eisner.
2016. Weighting finite-state transductions with neural
context. In Proceedings of NAACL.

Alexander M. Rush, Sumit Chopra, and Jason Weston.
2015. A neural attention model for abstractive sen-
tence summarization. In Proceedings of EMNLP.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Se-
quence to sequence learning with neural networks. In
Proceedings of NIPS.

Christoph Tillmann, Stephan Vogel, Hermann Ney, and
Alex Zubiaga. 1997. A DP-based search using mono-
tone alignments in statistical translation. In Proceed-
ings of EACL.

Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly.
2015. Pointer networks. In Proceedings of NIPS.

Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In Proceedings of COLING.

Jason Weston, Sumit Chopra, and Antoine Bordes. 2015.
Memory networks. In Proceedings of ICLR.

1316


