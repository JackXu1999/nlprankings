



















































Probing the Linguistic Strengths and Limitations of Unsupervised Grammar Induction


Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 1395–1404,

Beijing, China, July 26-31, 2015. c©2015 Association for Computational Linguistics

Probing the Linguistic Strengths and Limitations
of Unsupervised Grammar Induction

Yonatan Bisk and Julia Hockenmaier
Department of Computer Science

The University of Illinois at Urbana-Champaign
201 N Goodwin Ave Urbana, IL 61801
{bisk1,juliahmr@illinois.edu}

Abstract

Work in grammar induction should help
shed light on the amount of syntactic struc-
ture that is discoverable from raw word
or tag sequences. But since most cur-
rent grammar induction algorithms pro-
duce unlabeled dependencies, it is diffi-
cult to analyze what types of constructions
these algorithms can or cannot capture,
and, therefore, to identify where additional
supervision may be necessary. This pa-
per provides an in-depth analysis of the
errors made by unsupervised CCG parsers
by evaluating them against the labeled de-
pendencies in CCGbank, hinting at new
research directions necessary for progress
in grammar induction.

1 Introduction

Grammar induction aims to develop algorithms
that can automatically discover the latent syntactic
structure of language from raw or part-of-speech
tagged text. While such algorithms would have
the greatest utility for low-resource languages for
which no treebank is available to train supervised
parsers, most work in this area has focused on
languages where existing treebanks can be used
to measure and compare the performance of the
resultant parsers. Despite significant progress in
the last decade (Klein and Manning, 2004; Head-
den III et al., 2009; Blunsom and Cohn, 2010;
Spitkovsky et al., 2013; Mareček and Straka,
2013), there has been little analysis performed on
the types of errors these induction systems make,
and our understanding of what kinds of construc-
tions these parsers can or cannot recover is still
rather limited. One likely reason for this lack of
analysis is the fact that most of the work in this do-
main has focused on parsers that return unlabeled
dependencies, which cannot easily be assigned a
linguistic interpretation.

This paper shows that approaches that are
based on categorial grammar (Steedman, 2000)
are amenable to more stringent evaluation metrics,
which enable detailed analyses of the construc-
tions they capture, while the commonly used
unlabeled directed attachment scores hide linguis-
tically important errors. Any categorial grammar
based system, whether deriving its grammar
from seed knowledge distinguishing nouns and
verbs (Bisk and Hockenmaier, 2013), from a
lexicon constructed from a simple questionnaire
for linguists (Boonkwan and Steedman, 2011), or
from sections of a treebank (Garrette et al., 2015),
will attach linguistically expressive categories
to individual words, and can therefore produce
labeled dependencies. We provide a simple proof
of concept for how these labeled dependencies
can be used to isolate problem areas in CCG
induction algorithms. We illustrate how they
make the linguistic assumptions and mistakes of
the model transparent, and are easily comparable
to a treebank where available. They also allow us
to identify linguistic phenomena that require addi-
tional supervision or training signal to master. Our
analysis will be based on extensions of our earlier
system (Bisk and Hockenmaier, 2013), since it
requires less supervision than the CCG-based
approaches of Boonkwan and Steedman (2011)
or Garrette et al. (2015). Our aim in presenting
this analysis is to initiate a broader conversation
and classification of the impact of various types of
supervision provided to these approaches. We will
see that most of the constructions that our system
cannot capture, even when they are included in
the model’s search space, involve precisely the
kinds of non-local dependencies that elude even
supervised dependency parsers (since they require
dependency graphs, instead of trees), and that
have motivated the use of categorial grammar-
based approaches for supervised parsing.

1395



First, we provide a brief introduction to CCG.
Next, we define a labeled evaluation metric that al-
lows us to compare the labeled dependencies pro-
duced by Bisk and Hockenmaier (2013)’s unsu-
pervised parser with those in CCGbank (Hock-
enmaier and Steedman, 2007). Third, we ex-
tend their induction algorithm to allow it to induce
more complex categories, and refine their proba-
bility model to handle punctuation and lexicaliza-
tion, which we show to be necessary when han-
dling the larger grammars induced by our vari-
ant of their algorithm. While we also perform a
traditional dependency evaluation for comparison
to the non-CCG based literature, we focus on our
CCG-based labeled evaluation metrics to perform
a comparative analysis of Bisk and Hockenmaier
(2013)’s parser and our extensions.

2 Combinatory Categorial Grammar

CCG categories CCG (Steedman, 2000) is a
lexicalized grammar formalism which associates
each word with a set of lexical categories that fully
specify its syntactic behavior. Lexical categories
indicate the expected number, type and relative lo-
cation of arguments a word should take, or what
constituents it may modify. Even without explicit
evaluation against a treebank, the CCG lexicon
that an unsupervised parser produces provides an
easily interpretable snapshot of the assumptions
the model has made about a language (Bisk and
Hockenmaier, 2013). The set of CCG categories is
defined recursively over a small set of atomic cat-
egories (e.g. S,N,NP,PP). Complex categories
take the form X\Y or X/Y and represent functions
which create a result of category X when com-
bined with an argument Y. The slash indicates
whether the argument precedes (\) or follows (/)
the functor (descriptions of CCG commonly use
the vertical slash | to range over both / and \).
Modifiers are categories of the form X|X, and may
take arguments of their own.

CCG rules CCG rules are defined schematically
as function application (>,<), unary (>B1, <B1)
and generalized composition (>Bn, <Bn), type-
raising (>T, <T) and conjunction:

X/Y Y ⇒> X
X/Y Y|Z ⇒>B1 X|Z
X/Y Y|Z1|...|Zn ⇒>Bn X|Z1|...|Zn
X ⇒>T T/(T\X)
Y X\Y ⇒< X
Y|Z X\Y ⇒<B1 X|Z
Y|Z1|...|Zn X\Y ⇒<Bn X|Z1|...|Zn

X ⇒<T T\(T/X)

CCG derivations In the following derivation,
forward application is used in line 1) as both the
verb and the preposition take their NP arguments.
In line 2), the prepositional phrase modifies the
verb via backwards composition. Finally, in line
3), the derivation completes by producing a sen-
tence (S) via backwards application:

I saw her from afar

N (S\N1)/N2 N (S\S1)/N2 N
> >

1) S\N1 S\S1
<B

2) S\N1
<

3) S

CCG dependencies CCG has two standard
evaluation metrics. Supertagging accuracy sim-
ply computes how often a model chooses the cor-
rect lexical category for a given word. The cor-
rect category is a prerequisite for recovering the
correct labeled dependency. By tracing through
which word fills which argument of which cate-
gory, a set of dependency arcs, labeled by lexical
category and slot, can be extracted:

lexical head of a lexical category ci is the corre-
sponding word wi. In general, the lexical head of
a derived category is determined by the (primary)
functor, so that the lexical head of a category X
or X|Z1|...|Zn that resulted from combining X|Y
and Y or Y|Z1|...|Zn is identical to the lexical head
of X. However, when a modifier X|X with lexical
head m is combined with an X|... whose lexical
head is w, the lexical head of the resultant X|...
is w, not m.2 Otherwise, from would become the
lexical head of the S\N saw her from afar, and the
sentence You know I saw her from afar would have
a dependency between know and from, rather than
between know and saw.

In general, word wj is a dependent of word wi
if the k-th argument of the lexical category ci of
word wi is instantiated with the lexical category
of word wj . In the above derivation:

i j ci k wi wj

1 0 (S\N1)/N2 1 saw I
1 2 (S\N1)/N2 2 saw her
1 3 (S\S1)/N2 1 from saw
4 3 (S\S1)/N2 2 from afar

I saw her from afar

(S\S)/N2

(S\S)/N1

(S\N)/N2
(S\N)/N1

The use of categories as dependency labels
makes CCG labels more fine-grained than a stan-
dard dependency grammar. For example, the sub-
ject role of intransitive, transitive and ditransitive
verbs are all SUB in dependency treebanks but
take at least three different labels in CCGbank.

i j wj Label

2 1 I SUB
0 2 saw ROOT
2 3 her OBJ
2 4 from VMOD
4 5 afar PMOD

I saw her from afar

PMOD
VMOD

OBJ
SUB

ROOT

An additional complexity in CCGbank are cer-
tain types of lexical categories (e.g. for relative
pronouns or control verbs) which mediate non-
local dependencies via a co-indexation mecha-
nism. Identifying such non-local dependencies,
e.g. to distinguish between subject and object con-
trol (I promise her to come vs. I persuade her
to come), is most likely beyond the scope of any
purely syntactic grammar induction system but
will begin to emerge in a semi-supervised system.

2That is, the argument X and result X of a modifier X|X
are not two distinct instances of the same category, but unify.

Spurious ambiguity and normal-form parsing
Composition and type-raising introduce an expo-
nential number of derivations that are semantically
equivalent, i.e. yield the same set of dependen-
cies. In supervised CCG parsers (Hockenmaier
and Steedman, 2002; Clark and Curran, 2007),
this spurious ambiguity is largely eliminated be-
cause the derivations in CCGbank are in a normal
form that uses composition and type-raising only
when necessary, although it can be further allevi-
ated via the use of a normal-form parsing algo-
rithm (Eisner, 1996; Hockenmaier and Bisk, 2010)
that minimizes the use of composition (and type-
raising). We will show below that this spurious
ambiguity is particularly deleterious for unsuper-
vised CCG parsers that do not impose any normal-
form constraints.

3 Unsupervised CCG parsing

We now review the unsupervised CCG parser of
Bisk and Hockenmaier (2012b; 2013), which is
trained over parse forests obtained from a CCG
lexicon that was induced from POS-tagged text.

Unsupervised CCG induction The induction
algorithm needs to identify the set of lexical
categories and to learn the mapping between
words and lexical categories, e.g.:

N:{he, girl, lunch,...} N/N:{good, the, eating, ...}
S\N:{sleeps, ate, eating,...} (S\N)/N:{sees, ate, ...}
S\S:{quickly, today...} S/S:{Today,...}

Bisk and Hockenmaier (2012b) define an algo-
rithm that automatically induces a CCG lexicon
from part-of-speech tagged text in an iterative pro-
cess. This process starts with a small amount of
seed knowledge that defines which atomic cate-
gories (S, N and conj) can be assigned to which
part-of-speech tags (nominal POS tags may have
the category N, while verbs may have the cate-
gory S). Based on the assumption that, under mild
restrictions, words can either subcategorize for or
modify the words they are adjacent to, this process
produces lexical categories of increasing complex-
ity. Immediate neighbors of words with categories
S or N may act as modifiers with categories S|S
or N|N. The second round of induction can also
introduce modifiers (X|X)|(X|X) of existing mod-
ifiers X|X. In the first iteration, words with cate-
gory S can take adjacent N arguments. In the sec-
ond round, modifiers and words with category S|N
that are adjacent to words with the category N or

In this example, I fills the first argument of saw.
This is represented by an edge from saw to I, la-
beled as a transitive verb ((S\N)/N). This proce-
dure is followed for every argument of every pred-
icate, leading to a labeled directed graph.

Evaluation metrics for supervised CCG parsers
(Clark et al., 2002) measure labeled f-score (LF1)
precision of these dependencies (requiring the
functor, argument, lexical category of the func-
tor and slot of the argument to all match). A
second, looser, evaluation is often also performed
which measures unlabeled, undirected depen-
dency scores (UF1).

Non-local dependencies and complex argu-
ments One advantage of CCG is its ability to
recover the non-local dependencies involved in
control, raising, or wh-extraction. Since these
constructions introduce additional dependencies,
CCG parsers return dependency graphs (DAGs),
not trees. To obtain these additional dependen-
cies, relative pronouns and control verbs require
lexical categories that take complex arguments of
the form S\NP or S/NP, and a mechanism for co-
indexation of the NP inside this argument with an-
other NP argument (e.g. (NP\NPi)/(S|NPi) for
relative pronouns). These co-indexed subjects can
be seen in Figure 1.

1396



I
N

promise
(S\N)/(S\N)

to
(S\N)/(S\N)

pay
(S\N)/N

you
N

John
N

,
,

who
(N\N)/(S\N)

ran
(S\N)/N

home
N

,
,

ate
(S\N)/N

dinner
N

(I, promise) (I, pay) (John, ran) (John, ate)

Table 6: Unlabeled predicate argument structures for two sentences, both of whom result in DAGs, not
trees, as the subject is shared by multiple verbs.

Additional Category p(cat | tag)
((N\N)/(S\N))/N .93 WP$
N/(S/N) .14 WP
N/(S\N) .08 WP
((N\N)/S)\((N\N)/N) .07 WDT
((S\S)\(S\S))\N .04 RBR
S/(S\N) .04 WP
S/(S/N) .02 WP

Table 8: Common categories that the algorithm
cannot induce, and their corpus probability (given
their most frequent tag in Sec. 02-21)

Model Supervision LF1 UF1

B1 POS tags 34.5 60.6
B3

P&L + Punc & Words 37.1 64.9
BC1 + Complex Args 34.9 63.6

Table 9: Overall performance of the final systems
discussed in this paper (Section 23)

dicate missing information which only becomes
available later in the discourse.

7 Final Overall Model Performance

Finally, we evaluate these models again on the
standard Section 23 against our simplified labelset
and on undirected unlabeled arcs.

8 CoNLL vs CCGbank dependencies

Finally, we examine whether the performance
on standard unlabeled dependencies correlates
with performance on CCGbank dependencies (Ta-
ble 10)2. This also allows us to compare our
systems directly to an unsupervised dependency
parser (Naseem et al., 2010), who report directed
attachment (unlabeled dependency) scores of a
dependency-based HDP model that incorporates
either “universal” knowledge (e.g. that adjectives
may modify nouns) or “English-specific” knowl-
edge (e.g. that adjectives tend to precede nouns)
in the form of soft constraints. Their universal
knowledge is akin to, but more explicit and de-

2BH13 use hyperparameter schemes and report 64.2@20.

CCGbank 02-21 WSJ2-21 DA
Model LF1 UF1 @10 @20 @1
Naseem (Universal) 71.9 50.4
Naseem (English) 73.8 66.1

B1 33.8 60.3 70.7 63.1 58.4
B3

P&L 38.3 66.2 71.3 65.9 62.3
BC1 34.4 62.0 70.5 65.4 61.9

Table 10: Performance on CCGbank and CoNLL-
style dependencies (Sections 02-21) for a compar-
ison with Naseem et al. (2010).

tailed than the information given to the induction
algorithm (see Bisk and Hockenmaier (2013) for a
discussion). They evaluate on their training data,
i.e. sentences of up to length 20 (without punctu-
ation marks) of Sections 02-21 of the Penn Tree-
bank3.

We see that performance increases on CCG-
bank translate to similar gains on the CoNLL de-
pendencies on long sentences. We should note
that we expect this discrepancy to grow as sys-
tems capture more fine-grained distinction. In this
vein, we computed directed attachment recall be-
tween CCGbank dependencies and Yamada and
Matusumoto’s head finding rules and found only
a 72.5% overlap. Many of the discrepancies ap-
pear to be related to verb chains and analysis of
the many DAG structures previously discussed. A
full analsyis of the distinctions is beyond the scope
of this paper but there is an interesting emperical
question for future work as to whether annotation
standards make learning even more burdensome.

9 Conclusions

In this paper, we have touched upon many linguis-
tic phenomena that are common in language and
we feel are currently out of scope for grammar in-
duction systems. We focused our analysis on En-
glish for simplicity but many of the same types
of problems exist in other languages and can be
easily identified as stemming from the same lack

3With Yamada and Matsumoto’s (2003) head rules

I
N

promise
(S\N)/(S\N)

to
(S\N)/(S\N)

pay
(S\N)/N

you
N

John
N

,
,

who
(N\N)/(S\N)

ran
(S\N)/N

home
N

,
,

ate
(S\N)/N

dinner
N

(I, promise) (I, pay) (John, ran) (John, ate)

Table 6: Unlabeled predicate argument structures for two sentences, both of whom result in DAGs, not
trees, as the subject is shared by multiple verbs.

Additional Category p(cat | tag)
((N\N)/(S\N))/N .93 WP$
N/(S/N) .14 WP
N/(S\N) .08 WP
((N\N)/S)\((N\N)/N) .07 WDT
((S\S)\(S\S))\N .04 RBR
S/(S\N) .04 WP
S/(S/N) .02 WP

Table 8: Common categories that the algorithm
cannot induce, and their corpus probability (given
their most frequent tag in Sec. 02-21)

Model Supervision LF1 UF1

B1 POS tags 34.5 60.6
B3

P&L + Punc & Words 37.1 64.9
BC1 + Complex Args 34.9 63.6

Table 9: Overall performance of the final systems
discussed in this paper (Section 23)

dicate missing information which only becomes
available later in the discourse.

7 Final Overall Model Performance

Finally, we evaluate these models again on the
standard Section 23 against our simplified labelset
and on undirected unlabeled arcs.

8 CoNLL vs CCGbank dependencies

Finally, we examine whether the performance
on standard unlabeled dependencies correlates
with performance on CCGbank dependencies (Ta-
ble 10)2. This also allows us to compare our
systems directly to an unsupervised dependency
parser (Naseem et al., 2010), who report directed
attachment (unlabeled dependency) scores of a
dependency-based HDP model that incorporates
either “universal” knowledge (e.g. that adjectives
may modify nouns) or “English-specific” knowl-
edge (e.g. that adjectives tend to precede nouns)
in the form of soft constraints. Their universal
knowledge is akin to, but more explicit and de-

2BH13 use hyperparameter schemes and report 64.2@20.

CCGbank 02-21 WSJ2-21 DA
Model LF1 UF1 @10 @20 @1
Naseem (Universal) 71.9 50.4
Naseem (English) 73.8 66.1

B1 33.8 60.3 70.7 63.1 58.4
B3

P&L 38.3 66.2 71.3 65.9 62.3
BC1 34.4 62.0 70.5 65.4 61.9

Table 10: Performance on CCGbank and CoNLL-
style dependencies (Sections 02-21) for a compar-
ison with Naseem et al. (2010).

tailed than the information given to the induction
algorithm (see Bisk and Hockenmaier (2013) for a
discussion). They evaluate on their training data,
i.e. sentences of up to length 20 (without punctu-
ation marks) of Sections 02-21 of the Penn Tree-
bank3.

We see that performance increases on CCG-
bank translate to similar gains on the CoNLL de-
pendencies on long sentences. We should note
that we expect this discrepancy to grow as sys-
tems capture more fine-grained distinction. In this
vein, we computed directed attachment recall be-
tween CCGbank dependencies and Yamada and
Matusumoto’s head finding rules and found only
a 72.5% overlap. Many of the discrepancies ap-
pear to be related to verb chains and analysis of
the many DAG structures previously discussed. A
full analsyis of the distinctions is beyond the scope
of this paper but there is an interesting emperical
question for future work as to whether annotation
standards make learning even more burdensome.

9 Conclusions

In this paper, we have touched upon many linguis-
tic phenomena that are common in language and
we feel are currently out of scope for grammar in-
duction systems. We focused our analysis on En-
glish for simplicity but many of the same types
of problems exist in other languages and can be
easily identified as stemming from the same lack

3With Yamada and Matsumoto’s (2003) head rules

Figure 1: Unlabeled predicate-argument dependency graphs for two sentences with co-indexed subjects.

Errors exposed by labeled evaluation We now
illustrate how the lexical categories and labeled
dependencies produced by CCG parsers expose
linguistic mistakes. First, we consider a wildly in-
correct analysis of the first example sentence, in
which the subject is treated as an adverb, and the
PP as an NP object of the verb:

I saw her from afar

S/S (S/N1)/N2 N N/N1 N
> >

S/N1 N
>

S
>

S

None of the labeled directed CCG dependencies
are correct. But under the more lenient unlabeled
directed evaluation of Garrette et al. (2015), and
the even more lenient unlabeled undirected metric
of Clark et al. (2002), two (or three) of the four
dependencies would be deemed correct:

Incorrect parse Correct parse

nsubj dobj

prep

S/S

(S/N)/N

(S/N)/N N/N (S\N)/N

(S\S)/N

(S\S)/N(S\N)/N

pobj

I saw her from afar I saw her from afar

I saw her from afar I saw her from afar

nsubj dobj

prep

S/S

(S/N)/N

(S/N)/N N/N (S\N)/N

(S\S)/N

(S\S)/N(S\N)/N

pobj

I saw her from afar I saw her from afar

I saw her from afar I saw her from afar

When we translate the CCG analysis to an unla-
beled dependency tree (and hence flip the direction
of modifier dependencies and add a root edge), a
similar picture emerges, and three out of five at-
tachments are deemed correct:

Incorrect parse Correct parse

nsubj dobj

prep

S/S

(S/N)/N

(S/N)/N N/N (S\N)/N
(S\S)/N

(S\S)/N(S\N)/N

pobj

I saw her from afar I saw her from afar

I saw her from afar I saw her from afar

nsubj dobj

prep

S/S

(S/N)/N

(S/N)/N N/N (S\N)/N
(S\S)/N

(S\S)/N(S\N)/N

pobj

I saw her from afar I saw her from afar

I saw her from afar I saw her from afar

We now turn to a subtle distinction that corre-
sponds to a systematic mistake made by all mod-
els we evaluate. The categories of noun-modifying
prepositions (at) and possessive markers (’) differ
only in the directionality of their slashes:

X/Y Y )> X
X/Y Y|Z )>B1 X|Z
X/Y Y|Z1|...|Zn )>Bn X|Z1|...|Zn
Y X\Y )< X
Y|Z X\Y )<B1 X|Z
Y|Z1|...|Zn X\Y )<Bn X|Z1|...|Zn

A full explanation of the calculus can be found
in (Steedman, 2000) including discussion of a
type-raising and a ternary rule for conjunction. We
assume no type-changing in this work.

2.1 Dependencies
By tracing through which word fills which argu-
ment of a category a set of dependency arcs, la-
beled by lexical category and slot, can be extracted
and are used for evaluation:

lexical head of a lexical category ci is the corre-
sponding word wi. In general, the lexical head of
a derived category is determined by the (primary)
functor, so that the lexical head of a category X
or X|Z1|...|Zn that resulted from combining X|Y
and Y or Y|Z1|...|Zn is identical to the lexical head
of X. However, when a modifier X|X with lexical
head m is combined with an X|... whose lexical
head is w, the lexical head of the resultant X|...
is w, not m.2 Otherwise, from would become the
lexical head of the S\N saw her from afar, and the
sentence You know I saw her from afar would have
a dependency between know and from, rather than
between know and saw.

In general, word wj is a dependent of word wi
if the k-th argument of the lexical category ci of
word wi is instantiated with the lexical category
of word wj . In the above derivation:

i j ci k wi wj

1 0 (S\N1)/N2 1 saw I
1 2 (S\N1)/N2 2 saw her
1 3 (S\S1)/N2 1 from saw
4 3 (S\S1)/N2 2 from afar

I saw her from afar

(S\S)/N2

(S\S)/N1

(S\N)/N2
(S\N)/N1

The use of categories as dependency labels
makes CCG labels more fine-grained than a stan-
dard dependency grammar. For example, the sub-
ject role of intransitive, transitive and ditransitive
verbs are all SUB in dependency treebanks but
take at least three different labels in CCGbank.

i j wj Label

2 1 I SUB
0 2 saw ROOT
2 3 her OBJ
2 4 from VMOD
4 5 afar PMOD

I saw her from afar

PMOD
VMOD

OBJ
SUB

ROOT

An additional complexity in CCGbank are cer-
tain types of lexical categories (e.g. for relative
pronouns or control verbs) which mediate non-
local dependencies via a co-indexation mecha-
nism. Identifying such non-local dependencies,
e.g. to distinguish between subject and object con-
trol (I promise her to come vs. I persuade her
to come), is most likely beyond the scope of any
purely syntactic grammar induction system but
will begin to emerge in a semi-supervised system.

2That is, the argument X and result X of a modifier X|X
are not two distinct instances of the same category, but unify.

Spurious ambiguity and normal-form parsing
Composition and type-raising introduce an expo-
nential number of derivations that are semantically
equivalent, i.e. yield the same set of dependen-
cies. In supervised CCG parsers (Hockenmaier
and Steedman, 2002; Clark and Curran, 2007),
this spurious ambiguity is largely eliminated be-
cause the derivations in CCGbank are in a normal
form that uses composition and type-raising only
when necessary, although it can be further allevi-
ated via the use of a normal-form parsing algo-
rithm (Eisner, 1996; Hockenmaier and Bisk, 2010)
that minimizes the use of composition (and type-
raising). We will show below that this spurious
ambiguity is particularly deleterious for unsuper-
vised CCG parsers that do not impose any normal-
form constraints.

3 Unsupervised CCG parsing

We now review the unsupervised CCG parser of
Bisk and Hockenmaier (2012b; 2013), which is
trained over parse forests obtained from a CCG
lexicon that was induced from POS-tagged text.

Unsupervised CCG induction The induction
algorithm needs to identify the set of lexical
categories and to learn the mapping between
words and lexical categories, e.g.:

N:{he, girl, lunch,...} N/N:{good, the, eating, ...}
S\N:{sleeps, ate, eating,...} (S\N)/N:{sees, ate, ...}
S\S:{quickly, today...} S/S:{Today,...}

Bisk and Hockenmaier (2012b) define an algo-
rithm that automatically induces a CCG lexicon
from part-of-speech tagged text in an iterative pro-
cess. This process starts with a small amount of
seed knowledge that defines which atomic cate-
gories (S, N and conj) can be assigned to which
part-of-speech tags (nominal POS tags may have
the category N, while verbs may have the cate-
gory S). Based on the assumption that, under mild
restrictions, words can either subcategorize for or
modify the words they are adjacent to, this process
produces lexical categories of increasing complex-
ity. Immediate neighbors of words with categories
S or N may act as modifiers with categories S|S
or N|N. The second round of induction can also
introduce modifiers (X|X)|(X|X) of existing mod-
ifiers X|X. In the first iteration, words with cate-
gory S can take adjacent N arguments. In the sec-
ond round, modifiers and words with category S|N
that are adjacent to words with the category N or

These dependencies are the complete predicate ar-
gument structure of the sentence and supervised
evaluation is performed by computing a parser’s
precision and recall on matching the head, depen-
dant, category and slot of each arc. A second
looser evaluation is often also performed which
simply checks that the undirected and unlabeled
arcs match. An example of this difference that’s
particularly relevant to the discussion in this paper
is the headedness of prepositional phrases versus
posessives.

Prepositional Phrase

The
N/N

woman
N

at
(N\N)/N

the
N/N

company
N

laughed
S\N

(N\N)/N2
(N\N)/N1

S\N1

Possessive

The
N/N

woman
N

’s
(N/N)\N

IT
N/N

company
N

grew
S\N

(N/N)\N1
(N/N)\N2 S\N1

The undirected edges for the inital noun phrase
are identical, but the heads differ. In CCG, we as-
sume that categories of the form X|X where X is
atomic are modifiers. In this way, the first sentence
turns the prepositional phrase (at the company)
into a modifier of the woman. In contrast, in the
posessive sentence woman ’s modifies the com-
pany. Because, the arcs are so similar, the undi-
rected unlabeled score for confusing these analy-
ses is 80% correct but the labeled score would be
20%. This example demonstrates how the head-
edness of the resultant syntactic analysis requires
semantic knowledge about people and companies,

as getting the wrong head leads to the company
laughing or other semantically nonsensical analy-
ses.

2.2 Using Labels to Diagnose Errors

Finally, we quickly provide an incorrect analysis
of the first example sentence as a simple exercise
in using labels to diagnose mistakes:

I saw her from afar

S/S (S/N1)/N2 N N/N1 N
> >

S/N1 N
>

S
>

S

In this example, the verb analysis is trying to an-
alyze the language as VOS instead of SVO. Once
familiar with reading CCG categories the model’s
output and mistake can be easily diagnosed. A
model producing this analysis is not learning the
correct word order of the language, nor the correct
role for prepositions by taking afar as a subject.
This type of mistake is obvious to a speaker of the
language even without a treebank for evaluation.
In this way we believe label prediction eases the
analysis burden when diagnosing a system’s out-
put.

3 A Simplified Labeled Evaluation

In languages with treebanks, labeled evaluation
can make this style of analysis even simpler.
Fortunately, approaches using CCG can produce
labeled output but unfortunately there are mis-
matches between the basic set of categories and
those used in treebanks. We will focus on the En-
glish CCGbank but these details apply with only
minor changes to German and Chinese as well.

3.1 Simplification

Because the lexical categories guide parsing, the
set used in supervised parsing is extremely large
and augmented with features. These features are
not strictly part of the CCG calculus but mark
properties of the underlying words, for example
indicating if a verb is declarative or infinitival or if
a noun phrase contains a number. These features
are written as brackets modifying the atomic sym-
bols: (S[dcl]\NP, N/N[num], ... ). Prior work on
supervised parsing with CCG found that many of
these features can be recovered with proper mod-
eling of latent state splitting (Fowler and Penn,
2010). In our proposed simplification we re-
move these languge specific features. Secondly,

X/Y Y )> X
X/Y Y|Z )>B1 X|Z
X/Y Y|Z1|...|Zn )>Bn X|Z1|...|Zn
Y X\Y )< X
Y|Z X\Y )<B1 X|Z
Y|Z1|...|Zn X\Y )<Bn X|Z1|...|Zn

A full explanation of the calculus can be found
in (Steedman, 2000) including discussion of a
type-raising and a ternary rule for conjunction. We
assume no type-changing in this work.

2.1 Dependencies
By tracing through which word fills which argu-
ment of a category a set of dependency arcs, la-
beled by lexical category and slot, can be extracted
and are used for evaluation:

lexical head of a lexical category ci is the corre-
sponding word wi. In general, the lexical head of
a derived category is determined by the (primary)
functor, so that the lexical head of a category X
or X|Z1|...|Zn that resulted from combining X|Y
and Y or Y|Z1|...|Zn is identical to the lexical head
of X. However, when a modifier X|X with lexical
head m is combined with an X|... whose lexical
head is w, the lexical head of the resultant X|...
is w, not m.2 Otherwise, from would become the
lexical head of the S\N saw her from afar, and the
sentence You know I saw her from afar would have
a dependency between know and from, rather than
between know and saw.

In general, word wj is a dependent of word wi
if the k-th argument of the lexical category ci of
word wi is instantiated with the lexical category
of word wj . In the above derivation:

i j ci k wi wj

1 0 (S\N1)/N2 1 saw I
1 2 (S\N1)/N2 2 saw her
1 3 (S\S1)/N2 1 from saw
4 3 (S\S1)/N2 2 from afar

I saw her from afar

(S\S)/N2

(S\S)/N1

(S\N)/N2
(S\N)/N1

The use of categories as dependency labels
makes CCG labels more fine-grained than a stan-
dard dependency grammar. For example, the sub-
ject role of intransitive, transitive and ditransitive
verbs are all SUB in dependency treebanks but
take at least three different labels in CCGbank.

i j wj Label

2 1 I SUB
0 2 saw ROOT
2 3 her OBJ
2 4 from VMOD
4 5 afar PMOD

I saw her from afar

PMOD
VMOD

OBJ
SUB

ROOT

An additional complexity in CCGbank are cer-
tain types of lexical categories (e.g. for relative
pronouns or control verbs) which mediate non-
local dependencies via a co-indexation mecha-
nism. Identifying such non-local dependencies,
e.g. to distinguish between subject and object con-
trol (I promise her to come vs. I persuade her
to come), is most likely beyond the scope of any
purely syntactic grammar induction system but
will begin to emerge in a semi-supervised system.

2That is, the argument X and result X of a modifier X|X
are not two distinct instances of the same category, but unify.

Spurious ambiguity and normal-form parsing
Composition and type-raising introduce an expo-
nential number of derivations that are semantically
equivalent, i.e. yield the same set of dependen-
cies. In supervised CCG parsers (Hockenmaier
and Steedman, 2002; Clark and Curran, 2007),
this spurious ambiguity is largely eliminated be-
cause the derivations in CCGbank are in a normal
form that uses composition and type-raising only
when necessary, although it can be further allevi-
ated via the use of a normal-form parsing algo-
rithm (Eisner, 1996; Hockenmaier and Bisk, 2010)
that minimizes the use of composition (and type-
raising). We will show below that this spurious
ambiguity is particularly deleterious for unsuper-
vised CCG parsers that do not impose any normal-
form constraints.

3 Unsupervised CCG parsing

We now review the unsupervised CCG parser of
Bisk and Hockenmaier (2012b; 2013), which is
trained over parse forests obtained from a CCG
lexicon that was induced from POS-tagged text.

Unsupervised CCG induction The induction
algorithm needs to identify the set of lexical
categories and to learn the mapping between
words and lexical categories, e.g.:

N:{he, girl, lunch,...} N/N:{good, the, eating, ...}
S\N:{sleeps, ate, eating,...} (S\N)/N:{sees, ate, ...}
S\S:{quickly, today...} S/S:{Today,...}

Bisk and Hockenmaier (2012b) define an algo-
rithm that automatically induces a CCG lexicon
from part-of-speech tagged text in an iterative pro-
cess. This process starts with a small amount of
seed knowledge that defines which atomic cate-
gories (S, N and conj) can be assigned to which
part-of-speech tags (nominal POS tags may have
the category N, while verbs may have the cate-
gory S). Based on the assumption that, under mild
restrictions, words can either subcategorize for or
modify the words they are adjacent to, this process
produces lexical categories of increasing complex-
ity. Immediate neighbors of words with categories
S or N may act as modifiers with categories S|S
or N|N. The second round of induction can also
introduce modifiers (X|X)|(X|X) of existing mod-
ifiers X|X. In the first iteration, words with cate-
gory S can take adjacent N arguments. In the sec-
ond round, modifiers and words with category S|N
that are adjacent to words with the category N or

These dependencies are the complete predicate ar-
gument structure of the sentence and supervised
evaluation is performed by computing a parser’s
precision and recall on matching the head, depen-
dant, category and slot of each arc. A second
looser evaluation is often also performed which
simply checks that the undirected and unlabeled
arcs match. An example of this difference that’s
particularly relevant to the discussion in this paper
is the headedness of prepositional phrases versus
posessives.

Prepositional Phrase

The
N/N

woman
N

at
(N\N)/N

the
N/N

company
N

laughed
S\N

(N\N)/N2
(N\N)/N1

S\N1

Possessive

The
N/N

woman
N

’s
(N/N)\N

IT
N/N

company
N

grew
S\N

(N/N)\N1
(N/N)\N2 S\N1

The undirected edges for the inital noun phrase
are identical, but the heads differ. In CCG, we as-
sume that categories of the form X|X where X is
atomic are modifiers. In this way, the first sentence
turns the prepositional phrase (at the company)
into a modifier of the woman. In contrast, in the
posessive sentence woman ’s modifies the com-
pany. Because, the arcs are so similar, the undi-
rected unlabeled score for confusing these analy-
ses is 80% correct but the labeled score would be
20%. This example demonstrates how the head-
edness of the resultant syntactic analysis requires
semantic knowledge about people and companies,

as getting the wrong head leads to the company
laughing or other semantically nonsensical analy-
ses.

2.2 Using Labels to Diagnose Errors

Finally, we quickly provide an incorrect analysis
of the first example sentence as a simple exercise
in using labels to diagnose mistakes:

I saw her from afar

S/S (S/N1)/N2 N N/N1 N
> >

S/N1 N
>

S
>

S

In this example, the verb analysis is trying to an-
alyze the language as VOS instead of SVO. Once
familiar with reading CCG categories the model’s
output and mistake can be easily diagnosed. A
model producing this analysis is not learning the
correct word order of the language, nor the correct
role for prepositions by taking afar as a subject.
This type of mistake is obvious to a speaker of the
language even without a treebank for evaluation.
In this way we believe label prediction eases the
analysis burden when diagnosing a system’s out-
put.

3 A Simplified Labeled Evaluation

In languages with treebanks, labeled evaluation
can make this style of analysis even simpler.
Fortunately, approaches using CCG can produce
labeled output but unfortunately there are mis-
matches between the basic set of categories and
those used in treebanks. We will focus on the En-
glish CCGbank but these details apply with only
minor changes to German and Chinese as well.

3.1 Simplification

Because the lexical categories guide parsing, the
set used in supervised parsing is extremely large
and augmented with features. These features are
not strictly part of the CCG calculus but mark
properties of the underlying words, for example
indicating if a verb is declarative or infinitival or if
a noun phrase contains a number. These features
are written as brackets modifying the atomic sym-
bols: (S[dcl]\NP, N/N[num], ... ). Prior work on
supervised parsing with CCG found that many of
these features can be recovered with proper mod-
eling of latent state splitting (Fowler and Penn,
2010). In our proposed simplification we re-
move these languge specific features. Secondly,

The unlabeled dependencies inside the noun
phrases are identical, but the heads differ. The
first sentence turns the prepositional phrase (at the
company) into a modifier of woman. In contrast,
in the possessive case, woman ’s modifies com-
pany. According to an unlabeled (directed) score,
confusing these analyses would be 80% correct,
whereas LF1 would only be 20%. But without a
semantic bias for companies growing and women
laughing, there is no signal for the learner.

3 Labeled Evaluation for CCG Induction

We have just seen that labeled evaluation can ex-
pose many linguistically important mistakes. In
order to enable a fair and informative comparison
of unsupervised CCG parsers against the lexical
categories and labeled dependencies in CCGbank,
we define a simplification of CCGbank’s lexical
categories that does not alter the number or direc-
tion of dependencies, but makes the categories and
dependency labels directly comparable to those
produced by an unsupervised parser. We also
do not alter the derivations themselves, although
these may contain type-changing rules (which al-
low e.g. participial verb phrases S[ng]\NP to be
used as NP modifiers NP\NP) that are beyond the
scope of our induction algorithm.

Although the CCG derivations and dependen-
cies that CCG-based parsers return should in prin-
ciple be amenable to a quantitative labeled evalu-
ation when a gold-standard CCG corpus is avail-
able, there may be minor systematic differences
between the sets of categories assumed by the in-
duced parser and those in the treebank. In par-
ticular, the lexical categories in the English CCG-
bank are augmented with morphosyntactic fea-
tures that indicate e.g. whether sentences are
declarative (S[dcl]), or verb phrases are infiniti-
val (S[to]\NP). Prior work on supervised parsing
with CCG found that many of these features can
be recovered with proper modeling of latent state
splitting (Fowler and Penn, 2010). Since we wish
to evaluate a system that does not aim to induce
such features, we remove them. We also remove
the distinction between noun phrases (NP) and
nouns (N), which is predicated on knowledge of

1397



Our simplification of CCGbank’s lexical categories
Congress has n’t lifted the ceiling

Original NP (S[dcl]\NP)/(S[pt]\NP) (S\NP)\(S\NP) (S[pt]\NP)/NP NP[nb]/N N
Simplified N (S\N)/(S\N) S\S (S\N)/N N/N N

Figure 2: We remove morphosyntactic features, simplify verb phrase modifiers, and change NP to N.

CCGbank w/out Feats Simplified

All 1640 458 444
Lexical 1286 393 384

Table 1: Category types in CCGbank 02-21

determiners and other structural elements of a lan-
guage. Finally, CCGbank distinguishes between
sentential modifiers (which have categories of the
form S|S, without features) and verb phrase mod-
ifiers ((S\NP)|(S\NP), again without features).
But since the NP argument slot of a VP mod-
ifier is never filled, we can maintain the same
number of gold standard dependencies by remov-
ing this distinction and changing all VP modifiers
to be of the form S|S. However, categories of
the form (S[·]\NPi)/(S[·]\NPi), which are used
e.g. for modals and auxiliaries, are changed to
(S\Ni)/(S\Ni), allowing us to maintain the de-
pendency on the subject. With these three simplifi-
cations we eliminate much of the detailed knowl-
edge required to construct the precise CCGbank-
style categories, and dramatically reduce the set of
categories without losing expressive power. One
distinction that we do not conflate, even though
it is currently beyond the scope of the induc-
tion algorithm, is the distinction between PP argu-
ments (requiring prepositions to have the category
PP/NP) and adjuncts (requiring prepositions to be
(NP\NP)/NP or ((S\NP)\(S\NP))/NP).

This simplification is consistent with the most
basic components of CCG and can therefore be
easily used for the evaluation and analysis of any
weakly or fully supervised CCG system, not just
that of Bisk and Hockenmaier (2012). An example
simplification is present in Figure 2, and the reduc-
tion in the set of categories can be seen in Table 1.
Similar simplifications should also be possible for
CCGbanks in other languages.

4 Our approach

There are two parts to our approach: 1) induc-
ing a CCG grammar from seed knowledge and 2)
learning a probability model over parses. The in-
duction algorithm (Bisk and Hockenmaier, 2012)

uses the seed knowledge that nouns can take the
CCG category N, that verbs can take the category
S and may take N arguments, and that any word
may modify a constituent it is adjacent to, to iter-
atively induce a CCG lexicon to parse the train-
ing data. In Bisk and Hockenmaier (2013), we
introduced a model that is based on Hierarchical
Dirichlet Processes (Teh et al., 2006). This HDP-
CCG model gave state-of-the-art performance on a
number languages, and qualitative analysis of the
resultant lexicons indicated that the system was
learning the word order and many of the correct
attachments of the tested languages. But this sys-
tem also had a number of shortcomings: the in-
duction algorithm was restricted to a small frag-
ment of CCG, the model emitted only POS tags
rather than words, and punctuation was ignored.
Here, we use our previous HDP-CCG system as a
baseline, and introduce three novel extensions that
attempt to address these concerns.

5 Experimental Setup

For our experiments we will follow the standard
practice in supervised parsing of using WSJ Sec-
tions 02-21 for training, Section 22 for develop-
ment and error analysis, and a final evaluation of
the best models on Section 23. Because the in-
duced lexicons are overly general, the memory
footprint grows rapidly as the complexity of the
grammar increases. For this reason, we only train
on sentences that contain up to 20 words (as well
as an arbitrary number of punctuation marks). All
analyses and evaluation are performed with sen-
tences of all lengths unless otherwise indicated.
Finally, Bisk and Hockenmaier (2013) followed
Liang et al. (2007) in setting the values of the hy-
perparameters α to powers (eg. the square) of the
number of observed outcomes in the distribution.
But when the output consists of words rather than
POS tags, the concentration parameter α=V 2 is
too large to allow the model to learn. For this rea-
son, experiments will be reported with all hyper-
parameters set to a constant of 2500.1

1We tested three values (1000, 2500, 5000) and found that
the basic model at 2500 performed closest to the previously

1398



Base + Lexicalization + Punctuation + Punc & Lex + Allow (X|X)|X
Only Atomic Arguments B1 34.2 35.2 36.3 36.9 36.8

(S, N) B3 34.4 35.1 33.8 38.9 38.8

Allow Complex Arguments B1 33.0 34.9 33.2 35.7 35.8
(S, N, S|N) B3 29.4 29.5 31.2 31.2 31.2

Table 2: The impact of our changes to Bisk and Hockenmaier’s (2013) model (henceforth: B1, top left)
on CCGbank dependencies (LF1, Section 22, all sentences). The best overall model (B3

P&L) uses B3,
punctuation and lexicalization. The best model with complex arguments (BC1 ) uses only B1.

6 Extending the HDP-CCG system

We now examine how extending the HDP-CCG
baseline model to capture lexicalization and punc-
tuation, and how increasing the complexity of the
induced grammars affect performance (Table 2).

6.1 Modeling Lexicalization
In keeping with most work in grammar induction
from part-of-speech tagged text, Bisk and Hocken-
maier’s (2013) HDP-CCG treats POS tags t rather
than words w as the terminals it generates based
on their lexical categories c. The advantage of this
approach is that tag-based emissions p(t|c) are a
lot less sparse than word-based emissions p(w|c).
It is therefore beneficial to first train a model that
emits tags rather than words (Carroll and Rooth,
1998), and then to use this simpler model to ini-
tialize a lexicalized model that generates words in-
stead of tags. To perform the switch we simply es-
timate counts for the parse forests using the unlex-
icalized model during the E-Step and then apply
those counts to the lexicalized model during the
M-Step. Inside-Outside then continues as before.
Many words, like prepositions, differ systemati-
cally in their preferred syntactic role from that of
their part-of-speech tags. This change benefits all
settings of the model (Column 2 of Table 2).

6.2 Modeling Punctuation
Spitkovsky et al. (2011) performed a detailed anal-
ysis of punctuation for dependency-based gram-
mar induction, and proposed a number of con-
straints that aimed to capture the different ways
in which dependencies might cross constituent
boundaries implied by punctuation marks.

A constituency-based formalism like CCG al-
lows us instead to define a very simple, but effec-
tive Dirichlet Process (DP) based Markov gram-

reported dependency evaluation comparison with the work of
Naseem et al. (2010). We fixed this hyperparameter setting
for experimental simplicity but a more rigorous grid search
might find better parameters for the complex models.

mar that emits punctuation marks at the maximal
projections of constituents. We note that CCG
derivations are binary branching, and that virtually
every instance of a binary rule in a normal-form
derivation combines a head X or X|Y with an ar-
gument Y or modifier X|X. Without reducing the
set of strings generated by the grammar, we can
therefore assume that punctuation marks can only
be attached to the argument Y or the adjunct X|X:

Y
, ,

X/Y

X

Y

X\X
, ,

X

X

X\X

To model this, for each maximal projection (i.e.
whenever we generate a non-head child) with cate-
gory C, we first decide whether punctuation marks
should be emitted (M = {true, false}) to the left
or right side (Dir) of C. Since there may be mul-
tiple adjacent punctuation marks (... .”), we treat
this as a Markov process in which the history vari-
able captures whether previous punctuation marks
have been generated or not. Finally, we generate
an actual punctuation mark wm:

p(M | Dir ,Hist ,C) ∼ DP (α, p(M | dir))
p(M | Dir) ∼ DP (α, p(M))
p(wm | Dir ,Hist ,C) ∼ DP (α, p(wm | dir , hist))
p(wm | Dir ,Hist) ∼ DP (α, p(wm))
We treat # and $ symbols as ordinary lexi-

cal items for which CCG categories will be in-
duced by the regular induction algorithm, but treat
all other punctuation marks, including quotes and
brackets. Commas and semicolons (,, ;) can
act both as punctuation marks generated by this
Markov grammar, and as conjunctions with lexical
category conj. This model leads to further perfor-
mance gains (Columns 3 and 4 of Table 2).

6.3 Increasing Grammatical Complexity

The existing grammar induction scheme is very
simplistic. It assumes that adjacent words either
modify one another or can be taken as arguments.
Left unconstrained this space of grammatical cat-

1399



Model Supertagging LF1 UF1

B1 59.2 34.5 60.6
BC1 59.9 34.9 63.6
BP&L3 62.3 37.1 64.9

Table 3: Test set performance of the final systems
discussed in this paper (Section 23)

egories introduced grows very rapidly, introduc-
ing a tremendous number of incorrect categories
(analyzed later in Table 9). For this reason Bisk
and Hockenmaier (2013) applied the HDP-CCG
model to a context-free fragment of CCG, limit-
ing the arity of lexical categories (number of ar-
guments they can take) to two and the arity of
composition (how many arguments can be passed
through composition) to one. We know the space
of grammatical constructions is larger than this, so
we will allow the model to induce categories with
three arguments and use generalized composition
(B3). Bisk and Hockenmaier (2013) allow lexical
categories to only take atomic arguments, but, as
explained above, non-local dependencies require
complex arguments of the form S|N. We therefore
allow lexical categories to take up to one complex
argument of the form S|N. Atomic lexical cate-
gories are not allowed to take complex arguments,
eliminating S|(S|N) and N|(S|N). Increasing the
search space (Rows 3 and 4 of Table 2) shows cor-
responding decreases in performance.

Finally, Bisk and Hockenmaier (2013) elim-
inated the possessive-preposition ambiguity ex-
plained above by disallowing categories of the
form (X\X)/X and (X/X)\X to be used simulta-
neously. Removing this restriction does not harm
performance (Column 5 of Table 2).

6.4 Summary and test set performance

Table 2 shows the performance of 20 different
model settings on Section 22 under the simpli-
fied labeled CCG-based dependency evaluation
proposed above, starting with Bisk and Hocken-
maier’s (2013) original model (henceforth: B1,
top left). We see that modeling punctuation and
lexicalization both increase performance. We
also show that allowing categories of the form
(X\X)/X and (X/X)\X on top of the lexicalized
models with punctuation does not lead to a notice-
able decrease in performance. We also see that an
increase in grammatical and lexical complexity is
only beneficial for the grammars that allow only
atomic arguments, and only if both lexicalization

CCGbank 02-21 WSJ2-21 DA
Model LF1 UF1 @10 @20 @∞
Naseem (Universal) 71.9 50.4
Naseem (English) 73.8 66.1

B1 33.8 60.3 70.7 63.1 58.4
BC1 34.4 62.0 70.5 65.4 61.9
BP&L3 38.3 66.2 71.3 65.9 62.3

Table 4: Performance on CCGbank and CoNLL-
style dependencies (Sections 02-21) for a compar-
ison with Naseem et al. (2010).

and punctuation are modeled. Allowing complex
arguments is generally not beneficial, and perfor-
mance drops further if the grammatical complex-
ity is increased to B3. Our further analysis will
focus on the three bolded models, B1, BC1 (the
best model with complex arguments) and BP&L3
(the best overall model), whose supertag accuracy,
labeled (LF1) and unlabeled undirected CCG de-
pendency recovery on Section 23 are shown in Ta-
ble 3. We see that BC1 and B

P&L
3 both outperform

B1 on all metrics, although the unlabeled met-
ric (UF1) perhaps misleadingly suggests that BC1
leads to a greater improvement than the supertag-
ging and LF1 metrics indicate.

6.5 CCGbank vs. dependency trees

Finally, to compare our models directly to a com-
parable unsupervised dependency parser (Naseem
et al., 2010), we evaluate them against the un-
labeled dependencies produced by Yamada and
Matsumoto’s (2003) head rules for Sections 02-
21 of the Penn Treebank (Table 4)2. Naseem et al.
(2010) only report performance on sentences of up
to length 20 (without punctuation marks). Their
approach incorporates prior linguistic knowledge
either in the form of “universal” constraints (e.g.
that adjectives may modify nouns) or “English-
specific” constraints (e.g. that adjectives tend to
modify and precede nouns). These universal con-
straints are akin to, but more explicit and detailed
than the information given to the induction algo-
rithm (see Bisk and Hockenmaier (2013) for a dis-
cussion). Comparing these numbers to labeled
and unlabeled CCG dependencies on the same cor-
pus (all sentences, hence, @∞), we see that per-
formance increases on CCGbank do not translate
to similar gains on these unlabeled dependencies.
While we have done our best to convert the predi-
cate argument structure of CCG into dependencies

2BH13 use hyperparameter schemes and report 64.2@20.

1400



Correct B1 BP&L3 BC1
Category LR Used instead (%) LR Used instead (%) LR Used instead (%)

N 82.6 N/N 7.5 74.5 N/N 8.3 77.4 N/N 9.8
N/N 78.5 (S\S)\(S\S) 9.8 71.9 (S\S)\(S\S) 8.7 80.6 N 7.7
S\N 17.3 S\S 43.5 22.1 S\S 27.6 18.3 S\S 39.5
S\S 38.1 N 24.3 34.9 N 16.0 39.4 N 22.7
S/S 37.8 N\N 20.8 41.1 N/N 16.3 57.2 (S\S)/S 13.8
(N\N)/N 64.3 (S\S)/N 20.8 60.5 (S\S)/N 13.8 53.1 (S\S)/N 23.8
(S\N)/N 25.6 S/N 27.0 26.0 (S/N)/N 23.5 29.4 S/N 22.3
(S\S)/N 51.0 (N\N)/N 23.1 48.0 (N\N)/N 18.2 62.6 N/N 10.1
(S\N)/S 60.7 S\N 12.1 55.7 S\N 12.4 57.9 S\N 11.0
(S\S)/S 38.0 (N\N)/N 35.2 50.8 S/S 14.4 61.5 N 7.5

Table 5: Detailed supertagging analysis: Recall scores of B1, BC1 , and B3
P&L on the most common

recoverable (simplified) lexical categories in Section 22 along with the most commonly produced error.

Category Example usage Used instead by BC1 (%)

(N/N)\N The woman ’s company ... (N\N)/N 89.9 N/N 3.7 N 2.9
(S/S)/N Before Monday, ... S/S 69.9 N/N 14.8 (N\N)/N 8.2
(N/N)/(N/N) The very tall man ... N/N 38.0 (S\S)\(S\S) 33.9 (S\S)/N 10.1
(N\N)/(S\N) John, who ran home, ... (S\S)/(S/N) 26.5 N\N 23.3 S/S 14.9
(S\N)/(S\N) I promise to pay ... S\N 32.6 (S\S)/(S/N) 21.5 (S\N)/(S/N) 12.4
((S\N)/N)/N I gave her a gift. (S\N)/N 34.6 (S/N)/N 34.6 N/N 7.7
((S\N)/(S\N))/N I persuaded her to pay ... (S\N)/N 24.8 (S/N)/N 22.0 N/N 11.0

Table 6: Categories that are in the search space of the induction algorithm, but do not occur in any Viterbi
parse, and what BC1 uses instead.

there are many constructions which have vastly
different analysis, making a proper conversion too
difficult for the scope of this paper.3

7 Error analysis

Supertagging error analysis We first consider
the lexical categories that are induced by the mod-
els. Table 5 shows the accuracy with which they
recover the most common gold lexical categories,
together with the category that they most often
produced instead. We see that the simplest model
(B1) performs best on N, and perhaps over gen-
erates (N\N)/N (noun-modifying prepositions),
while the overall best model (BP&L3 ) outperforms
both other models only on intransitive verbs.

The most interesting component of our analysis
is the long tail of constructions that must be cap-
tured in order to produce semantically appropriate
representations. We can inspect the confusion ma-
trix of the lexical categories that the model fails to
use to obtain insight into how its predictions dis-
agree with the ground truth, and why these con-
structions may require special attention. Table 6
shows the most common CCGbank categories that

3The overlap (F-score of unlabeled undirected attachment
scores) between CCGbank dependencies and those obtained
via Matsumoto’s head finding rules is only 81.9%.

were in the search space of some of the more com-
plex models (e.g. BC3 ), but were never used by any
of the parsers in a Viterbi parse. These include
possessives, relative pronouns, modals/auxiliaries,
control verbs and ditransitives. We show the cat-
egories that the BC1 model uses instead. The gold
categories shown correspond to the bold words in
Table 6. While the reason many of these cases
are difficult is intuitive (e.g. very modifying tall
instead of man), a more difficult type of error
than previously discussed is that of recovering
non-local dependencies. The recovery of non-
local dependencies is beyond the scope of both
standard dependency-based approaches and Bisk
and Hockenmaier (2013)’s original induction al-
gorithm. But the parser does not learn to use lexi-
cal categories with complex arguments correctly
even when the algorithm is extended, to induce
them. For example, BC1 prefers to treat auxiliaries
or equi verbs like promise as intransitives rather
than as an auxiliary that shares its subject with
pay. The surface string supports this decision, as
it can be parsed without having to capture the non-
local dependencies (top row) present in the correct
(bottom row) analysis:

I promise to pay you
N S\N (S\S)/S S/N N
N (S\N)/(S\N) (S\N)/(S\N) (S\N)/N N

1401



1st Argument 2nd Argument

B1 B
C
1 B

P&L
3 B1 B

C
1 B

P&L
3

N/N 68.4 69.7 71.6
S\N 12.2 24.9 14.6
S\S 17.0 16.2 18.7
S/S 24.0 27.1 33.8
(N\N)/N 49.7 54.4 51.2 41.0 46.2 42.4
(S\N)/N 26.6 32.9 34.4 30.6 33.2 33.8
(S\S)/N 21.6 19.2 24.7 24.0 24.9 29.3
(S\N)/S 23.9 50.3 32.5 25.2 59.1 35.0
(S\S)/S 6.1 22.7 14.1 9.5 34.6 19.5

Table 7: LF1 scores of B1, BC1 and B3
P&L on the

most common dependency types in Section 22.

We also see that this model uses seemingly non-
English verb categories of the form (S/N)/N, both
for ditransitives, and object control verbs, perhaps
because the possibly spurious /N argument could
be swallowed by other categories that take argu-
ments of the form S/N, like the (incorrect) treat-
ment of subject relative pronouns. One possible
lesson we can extract from this is that practical
approaches for building parsers for new languages
might need to focus on injecting semantic infor-
mation that is outside the scope of the learner.

Dependency error analysis Table 7 shows the
labeled recall of the most common dependencies.
We see that both new models typically outper-
form the baseline, although they yield different
improvements on different dependency types. BC1
is better at recovering the subjects of intransitive
verbs (S\N) and verbs that take sentential com-
plements ((S\N)/S), while B3 is better for simple
adjuncts (N/N, S/S, S\S) and transitive verbs.
Wh-words and the long tail To dig slightly
deeper into the set of missing constructions, we
tried to identify the most common categories that
are beyond the search space of the current induc-
tion algorithm. We first computed the set of cat-
egories used by each part of speech tag in CCG-
bank, and thresholded the lexicon at 95% token
coverage for each tag. Removing the categories
that contain PP and those that can be induced by
the algorithm in its most general setting, we are
left with the categories shown in Table 8. The tags
that are missing categories are predominantly wh-
words required for wh-questions, relative clauses
or free relative clauses. Some of these categories
violate the assumptions made by the induction al-
gorithm: question words return a sentence (S) but
are not themselves verbs. Free relative pronouns
return a noun, but take arguments. However, this is

Additional Category p(cat | tag)
((N\N)/(S\N))/N .93 WP$
N/(S/N) .14 WP
N/(S\N) .08 WP
((N\N)/S)\((N\N)/N) .07 WDT
((S\S)\(S\S))\N .04 RBR
S/(S\N) .04 WP
S/(S/N) .02 WP

Table 8: Common categories that the algorithm
cannot induce

Size, ambiguity, coverage and precision
of the induced lexicons

Arguments: Atomic Complex
# Lexical Arity: 2 3 2 3

# Lexical Categories 37 53 61 133
Avg. #Cats / Tag 26.4 29.5 42.3 56.3
Token-based Coverage 84.3 84.4 89.8 90.2
Type-based Coverage 20.3 21.6 27.0 32.4
Type-based Precision 81.1 60.4 65.6 36.1

Table 9: Size, ambiguity, coverage and precision
(evaluated on Section 22) of the induced lexicons.

a surprisingly small set of special function words
and therefore perhaps a strategic place for super-
vision. Questions in particular pose an interesting
learning question – how does one learn that these
constructions indicate missing information which
only becomes available later in the discourse?

Grammatical complexity and size of the search
space As lexical categories are a good proxy for
the set of constructions the grammar will enter-
tain, we can measure the size and ambiguity of the
search space as a function of the number of lexical
category types it induces as compared to the per-
centage that are actually valid categories for the
language. In Table 9, we compare the lexicons in-
duced by variants of the induction algorithm by
their token-based coverage (the percent of tokens
in Sections 22 for which the induced tag lexicon
contains the correct category), type-based cover-
age (the percent of category types that the induced
lexicon contains), as well as type-based precision
(the percent of induced category types that occur
in Section 22). This analysis is independent of the
learned models, as their probabilities are not taken
into account. We see that as the number of lex-
ical categories induced (subject to the constraints
of Bisk and Hockenmaier (2012)) increases, the
percent that are valid English categories decreases
rapidly (type-based precision falls from 81.1% to
36.1%). Despite this, and despite a high token

1402



coverage of up to 90%, we still miss almost 70%
of the required category types. This helps explain
why performance degrades so much for BC3 , the
arity three lexicon with complex arguments.

8 Dealing with Non-Local Dependencies

While the methodology used here is restricted to
CCG based algorithms, we believe the lessons to
be very general. The aforementioned construc-
tions involve optional arguments, non-local de-
pendencies, and multiple potential heads. Even
though CCG is theoretically expressive enough to
handle these constructions, they present the un-
supervised learner with additional ambiguity that
will pose difficulties independently of the under-
lying grammatical representation.

For example, although our approach learns that
subject NPs are taken as arguments by verbs, the
task of deciding which verb to attach the subject
to is frequently ambiguous. This most commonly
occurs in verb chains, and is compounded in the
presence of subject-modifying relative clauses (in
CCGbank, both constructions are in fact treated
as several verbs sharing a single subject). To
illustrate this, we ran the BC1 and B3

P&L systems on
the following three sentences:

1. The woman won an award
2. The woman has won an award
3. The woman being promoted has won an award

The single-verb sentence is correctly parsed by
both models, but they flounder as distractors are
added. Both treat has as an intransitive verb, won
as an adverb and an as a preposition:

The woman won an award
B3

P&L/BC1 : N/N N (S\N)/N N/N N
The woman has won an award

B3
P&L/BC1 : N/N N S\N S\S (S\S)/N N

To accommodate the presence of two additional
verbs, both models analyze being as a noun modi-
fier that takes promoted as an argument. BC1 (cor-
rectly) stipulates a non-local dependency involv-
ing promoted, but treats it (arguably incorrectly)
as a case of object extraction:

... being promoted has won an award
B3

P&L
(N\N)/S S S\N S\S (S\S)/N N

BC1 (N\N)/(S/N) S/N S\N S\S (S\S)/N N
Discovering these, and many of the other sys-

tematic errors describe here, may be less obvi-
ous when analyzing unlabeled dependency trees.
But we would expect similar difficulties for any
unsupervised approach when sentence complexity
grows without a specific bias for a given analysis.

9 Conclusions

In this paper, we have introduced labeled evalu-
ation metrics for unsupervised CCG parsers, and
have shown that these expose many common syn-
tactic phenomena that are currently out of scope
for any unsupervised grammar induction systems.
While we do not wish claim that CCGbank’s anal-
yses are free of arbitrary decisions, we hope to
have demonstrated that these labeled metrics en-
able linguistically informed error analyses, and
hence allow us to at least in part address the ques-
tion of where and why the performance of these
approaches might plateau. We focused our analy-
sis on English for simplicity, but many of the same
types of problems exist in other languages and can
be easily identified as stemming from the same
lack of supervision. For example, in Japanese
we would expect problems with post-positions, in
German with verb clusters, in Chinese with mea-
sure words, or in Arabic with morphology and
variable word order.

We believe that one way to overcome the is-
sues we have identified is to incorporate a seman-
tic signal. Lexical semantics, if sparsity can be
avoided, might suffice; otherwise learning with
grounding or an extrinsic task could be used to
bias the choice of predicates, their arity and in turn
the function words that connect them. Alterna-
tively, a simpler solution might be to follow the
lead of Boonkwan and Steedman (2011) or Gar-
rette et al. (2015) where gold categories are as-
signed by a linguist or treebank to tags and words.
It is possible that more limited syntactic supervi-
sion might be sufficient if focused on the semanti-
cally ambiguous cases we have isolated.

More generally, we hope to initiate a conver-
sation about grammar induction which includes a
discussion of how these non-trivial constructions
can be discovered, learned, and modeled. Relat-
edly, in future extensions to semi-supervised or
projection based approaches, these types of con-
structions are probably the most useful to get right
despite comprising the tail, as analyses without
them may not be semantically appropriate. In
summary, we hope to begin to pull back the veil
on the types of information that a truly unsuper-
vised system, if one should ever exist, would need
to learn, and we pose a challenge to the commu-
nity to find ways that a learner might discover this
knowledge without hand-engineering it.

1403



10 Acknowledgments

This material is based upon work supported by
the National Science Foundation under Grants No.
1053856, 1205627 and 1405883. Any opinions,
findings, and conclusions or recommendations ex-
pressed in this material are those of the author(s)
and do not necessarily reflect the views of the Na-
tional Science Foundation.

References
Yonatan Bisk and Julia Hockenmaier. 2012. Simple

Robust Grammar Induction with Combinatory Cat-
egorial Grammars. In Proceedings of the Twenty-
Sixth Conference on Artificial Intelligence (AAAI-
12), pages 1643–1649, Toronto, Canada, July.

Yonatan Bisk and Julia Hockenmaier. 2013. An HDP
Model for Inducing Combinatory Categorial Gram-
mars. Transactions of the Association for Computa-
tional Linguistics, 1:75–88.

Phil Blunsom and Trevor Cohn. 2010. Unsupervised
Induction of Tree Substitution Grammars for Depen-
dency Parsing. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1204–1213, Cambridge, USA,
October.

Prachya Boonkwan and Mark Steedman. 2011. Gram-
mar Induction from Text Using Small Syntactic Pro-
totypes. In Proceedings of 5th International Joint
Conference on Natural Language Processing, pages
438–446, Chiang Mai, Thailand, November.

Glenn Carroll and M Rooth. 1998. Valence induction
with a head-lexicalized PCFG. In Proceedings of
the 3rd Conference on Empirical Methods in Natural
Language Processing, page 36–45, Granada, Spain.

Stephen Clark, Julia Hockenmaier, and Mark Steed-
man. 2002. Building Deep Dependency Structures
using a Wide-Coverage CCG Parser. In Proceedings
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 327–334, Philadelphia,
USA, July.

Timothy A D Fowler and Gerald Penn. 2010. Accu-
rate Context-Free Parsing with Combinatory Cate-
gorial Grammar. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, pages 335–344, Uppsala, Sweden, July.

Dan Garrette, Chris Dyer, Jason Baldridge, and Noah A
Smith. 2015. Weakly-Supervised Grammar-
Informed Bayesian CCG Parser Learning. In Pro-
ceedings of the Twenty-Ninth AAAI Conference on
Artificial Intelligence (AAAI-15), Austin, USA.

William P Headden III, Mark Johnson, and David Mc-
Closky. 2009. Improving Unsupervised Depen-
dency Parsing with Richer Contexts and Smoothing.

In Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 101–109, Boulder, USA, June.

Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: A Corpus of CCG Derivations and Depen-
dency Structures Extracted from the Penn Treebank.
Computational Linguistics, 33:355–396, September.

Dan Klein and Christopher D Manning. 2004. Corpus-
Based Induction of Syntactic Structure: Models of
Dependency and Constituency. In Proceedings of
the 42nd Meeting of the Association for Compu-
tational Linguistics (ACL’04), Main Volume, pages
478–485, Barcelona, Spain, July.

Percy Liang, Slav Petrov, Michael I Jordan, and Dan
Klein. 2007. The Infinite PCFG Using Hierarchi-
cal Dirichlet Processes. In Proceedings of the 2007
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL), pages 688–
697, Prague, Czech Republic, June.

David Mareček and Milan Straka. 2013. Stop-
probability estimates computed on a large corpus
improve Unsupervised Dependency Parsing. In Pro-
ceedings of the 51st Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), pages 281–290, Sofia, Bulgaria, August.

Tahira Naseem, Harr Chen, Regina Barzilay, and Mark
Johnson. 2010. Using Universal Linguistic Knowl-
edge to Guide Grammar Induction. In Proceed-
ings of the 2010 Conference on Empirical Methods
in Natural Language Processing, pages 1234–1244,
Cambridge, USA, October.

Valentin I Spitkovsky, Hiyan Alshawi, and Daniel Ju-
rafsky. 2011. Punctuation: Making a Point in Un-
supervised Dependency Parsing. In Proceedings of
the Fifteenth Conference on Computational Natural
Language Learning, pages 19–28, Portland, USA,
June.

Valentin I Spitkovsky, Hiyan Alshawi, and Daniel Ju-
rafsky. 2013. Breaking Out of Local Optima with
Count Transforms and Model Recombination: A
Study in Grammar Induction. In Proceedings of
the 2013 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1983–1995, Seattle,
USA, October.

Mark Steedman. 2000. The Syntactic Process. The
MIT Press.

Yee-Whye Teh, Michael I Jordan, Matthew J Beal, and
David M Blei. 2006. Hierarchical Dirichlet Pro-
cesses. Journal of the American Statistical Associa-
tion, 101(476):1566–1581.

Hiroyasu Yamada and Yuji Matsumoto. 2003. Statisti-
cal Dependency Analysis With Support Vector Ma-
chines. In Proceedings of 8th International Work-
shop on Parsing Technologies (IWPT), pages 195–
206, Nancy, France.

1404


