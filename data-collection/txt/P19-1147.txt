



















































On the Robustness of Self-Attentive Models


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1520–1529
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

1520

On the Robustness of Self-Attentive Models
Yu-Lun Hsieh1,2, Minhao Cheng3, Da-Cheng Juan4, Wei Wei4,

Wen-Lian Hsu1,5, Cho-Jui Hsieh3,4
1SNHCC, TIGP, Academia Sinica, Taiwan

2National Chengchi University, Taiwan
3University of California, Los Angeles, USA

4Google Research, USA
5PAIR Labs, Ministry of Science and Technology, Taiwan

morphe@iis.sinica.edu.tw, mhcheng@cs.ucla.edu, x@dacheng.info, wewei@google.com,

hsu@iis.sinica.edu.tw, chohsieh@cs.ucla.edu

Abstract

This work examines the robustness of self-
attentive neural networks against adversarial
input perturbations. Specifically, we investi-
gate the attention and feature extraction mech-
anisms of state-of-the-art recurrent neural net-
works and self-attentive architectures for sen-
timent analysis, entailment and machine trans-
lation under adversarial attacks. We also pro-
pose a novel attack algorithm for generating
more natural adversarial examples that could
mislead neural models but not humans. Exper-
imental results show that, compared to recur-
rent neural models, self-attentive models are
more robust against adversarial perturbation.
In addition, we provide theoretical explana-
tions for their superior robustness to support
our claims.

1 Introduction

Self-attentive neural models have recently become
a prominent component that achieves state-of-the-
art performances on many natural language pro-
cessing (NLP) tasks such as text classification
and machine translation (MT). This type of mod-
els, including Transformer (Vaswani et al., 2017)
and “Bidirectional Encoder Representations from
Transformers,” shortened as BERT (Devlin et al.,
2019), rely on the attention mechanism (Luong
et al., 2015) to learn a context-dependent repre-
sentation; compared to recurrent neural networks
(RNN), these self-attention-based models have
faster encoding speed and the capacity of mod-
eling a wider context. Particularly, BERT is re-
cently proposed to extend the directionality of the
Transformer model, and “pre-trained” using mul-
tiple objectives to strengthen its encoding capabil-
ity. Then, this pre-trained model can be fine-tuned
for various downstream tasks. BERT achieves
state-of-the-art performance on several NLP tasks
including classification and sequence-to-sequence

problems, often outperforming task-specific fea-
ture engineering or model architecture; therefore,
BERT is poised to be a key component in almost
every neural model for NLP tasks.

Despite the superior performance, it remains
unclear whether the self-attentive structure de-
ployed by Transformer or BERT is robust to ad-
versarial attacks compared with other neural net-
works. Adversarial attack refers to applying a
small perturbation on the model input to craft an
adversarial example, ideally imperceptible by hu-
mans, and cause the model to make an incorrect
prediction (Goodfellow et al., 2015). Unlike com-
puter vision models, generating an effective, tex-
tual adversarial example that misleads a model
but can go unnoticed by humans is a challeng-
ing and thriving research problem (Alzantot et al.,
2018). Therefore, the goal of this paper is to an-
swer the following questions: “Are self-attentive
models more robust to adversarial examples com-
pared with recurrent models? If so, why?” “Do
attention scores expose vulnerability in these self-
attentive models?”

This work verifies the robustness of self-
attentive models through performing adversarial
attacks and analyzing their effects on the model
prediction. In addition, we investigate the feasibil-
ity of utilizing the context-dependent embeddings
in these models to maximize semantic similarity
between real and adversarial sentences. We con-
duct experiments on two mainstream self-attentive
models: (a) Transformer for neural machine trans-
lation, and (b) BERT for sentiment and entailment
classification. To the best of our knowledge, this
paper brings the following contributions.
• We propose novel algorithms to generate

more natural adversarial examples that both
preserve the semantics and mislead the clas-
sifiers.
• We conduct comprehensive experiments to



1521

examine the robustness of RNN, Trans-
former, and BERT. Our results show that both
self-attentive models, whether pre-trained or
not, are more robust than recurrent models.
• We provide theoretical explanations to sup-

port the statement that self-attentive struc-
tures are more robust to small adversarial per-
turbations.

2 Target Neural Models

This section describes the target neural architec-
tures, LSTM and self-attentive models, and how
to adapt these models for the downstream tasks:
sentiment analysis, entailment and translation.

2.1 LSTM

For classification tasks including sentiment anal-
ysis and entailment detection, we use a Bidi-
rectional LSTM with an attention (Hochreiter
and Schmidhuber, 1997; Bahdanau et al., 2014)
layer as the sentence encoder, and a fully con-
nected layer for classification problems. For ma-
chine translation, we employ a common seq2seq
model (Sutskever et al., 2014), in which both the
encoder and decoder are a 2-layer stacked Bi-
LSTM with 512 hidden units.

2.2 Self-Attentive Models

Self-attentive models are further distinguished
into BERT and Transformers. The classification
problems adopt the BERT model with an identical
setup to the original paper (Devlin et al., 2019), in
which BERT is used as an encoder that represents
a sentence as a vector. This vector is then used by
a fully connected neural network for classification.
Note that models are tuned separately for each
task. We also experiment with a smaller BERT
model without pre-training, denoted as BERTNOPT,
in order to isolate the impact of pre-training. Due
to the limited size of the training data, we only
incorporate three layers of self-attention in the
smaller model.

To the best of our knowledge, there is no prior
work that uses pre-trained BERT for machine
translation. Thus, the Transformer model is em-
ployed for neural machine translation task.

3 Attack Methods

In this section, we provide five methods to gener-
ate adversarial examples (or called “attacks”). The
goal of an attack is to find and replace one word

in the original input sentence, turning the output
label (or sequence) from the model to be incor-
rect. The first method is based on random word
replacement, which serves as the baseline. The
second (list-based) and third (greedy) methods are
adapted from prior arts. The fourth (constrained
greedy) and fifth (attention-based) are proposed by
us. We also describe the evaluation metrics.

3.1 Random Attack

This attack randomly replaces one word in the in-
put sentence with another word from the vocabu-
lary. We repeat this process by 105 times and cal-
culate the average as the final performance. This
baseline is denoted as RANDOM.

3.2 List-based Attack

The second method is recently proposed by Alzan-
tot et al. (2018), denoted as LIST. LIST employs a
list of semantically similar words (i.e., synonyms),
and manages to replace a word in the input sen-
tence with another from the list to construct adver-
sarial examples. In other words, the list is used
to replace a word with one of its synonyms; this
process is repeated for every word in the input
sentence until the target model makes an incorrect
prediction. That is, for every sentence, we start by
replacing the first word with its synonyms, each
forming a new adversarial example. If none of
these successfully misleads the model, we move
to the next word (and the first word remains un-
changed), and repeat this process until either the
attack succeeds or all words have been tried.

3.3 Greedy Select + Greedy Replace

The third method (denoted as GS-GR) greed-
ily searches for the weak spot of the input sen-
tence (Yang et al., 2018) by replacing each word,
one at a time, with a “padding” (a zero-valued vec-
tor) and examining the changes of output probabil-
ity. After determining the weak spot, GS-GR then
replaces that word with a randomly selected word
in the vocabulary to form an attack. This process
is repeated until the attack succeeds or all words
in the vocabulary are exhausted.

3.4 Greedy Select + Embedding Constraint

Although the GS-GR method potentially achieves
a high success rate, the adversarial examples
formed by GS-GR are usually unnatural; some-
times GS-GR completely changes the semantics of



1522

the original sentence by replacing the most impor-
tant word with its antonym, for example: chang-
ing “this is a good restaurant” into “this is a bad
restaurant.” This cannot be treated as a successful
attack, since humans will notice the change and
agree with the model’s output. This is because
GS-GR only considers the classification loss when
finding the replacement word, and largely ignore
the actual semantics of the input sentence.

To resolve this issue, we propose to add a con-
straint on sentence-level (not word-level) embed-
ding: the attack must find a word with the mini-
mum L1 distance between two embeddings (from
the sentences before and after the word change) as
the replacement. This distance constraint requires
a replacement word not to alter the sentence-level
semantics too much. This method is denoted as
GS-EC. In the experimental results, we show that
the GS-EC method achieves a similar success rate
as GS-GR in misleading the model, while being
able to generate more natural and semantically-
consistent adversarial sentences.

3.5 Attention-based Select
We conjecture that self-attentive models rely heav-
ily on attention scores, and changing the word
with the highest or lowest attention score could
substantially undermine the model’s prediction.
Therefore, this attack method exploits and also in-
vestigates the attention scores as a potential source
of vulnerability. This method first obtains the at-
tention scores and then identifies a target word that
has the highest or lowest score. Target word is
then replaced by a random word in the vocabulary,
and this process is repeated until the model is mis-
led by the generated adversarial example. These
methods are denoted as ASMIN-GR that replaces
the word with the lowest score, and ASMAX-GR
with the highest score.

Furthermore, the constraint on the embedding
distance can also be imposed here for finding
semantically similar adversarial examples; these
methods are referred as ASMIN-EC and ASMAX-
EC, respectively. As a pilot study, we examine
the attention scores on the first and last layers of
the BERT model for understanding the model’s
behavior under attacks.

3.6 Evaluation Criteria
We evaluate the robustness of the classification
models (for sentiment analysis and entailment) by
the following three criteria: (a) the success rate

of the attacks misleading the model, (b) readabil-
ity, and (c) human accuracy. Both readability and
human accuracy are evaluated qualitatively by hu-
man raters. Readability measures the relative nat-
uralness of the adversarial examples generated by
different attack methods. For example, if 100
raters determine that the adversary generated by
method A is more readable than method B, and
40 raters think otherwise, the relative readability
scores of methods A and B will be 1 and 0.4, re-
spectively. And human accuracy is the percent-
age that human judgment of these examples re-
mains identical to the ground-truth label. In or-
der to evaluate the models and at the same time
keep reasonable execution time, we randomly se-
lect 100 samples from the test set that all models
answer correctly to perform attacks. For the ex-
periments on machine translation task, we evalu-
ate the attack success rate and BLEU scores (Pa-
pineni et al., 2002) for 200 sentence pairs in the
WMT 17 Task (Bojar et al., 2017).

4 Experiment I: Sentiment Analysis

We first evaluate the robustness of LSTM, BERT,
and BERTNOPT on binary sentiment analysis using
the Yelp dataset (Zhang et al., 2015). Models un-
der attack have accuracies of 93.7%, 87.3% and
90.7% for fine-tuned BERT model, BERTNOPT and
LSTM, respectively, on the test set. Note that for
attention-based attacks (i.e., ASMIN-GR, ASMAX-
GR, ASMIN-EC, and ASMAX-EC), the average of
the first (i.e., the one that is closest to the model
input) attention layer from all 12 heads in BERT
and BERTNOPT are used for our attacks.1

4.1 Results
To illustrate how adversarial attacks work, Fig. 1
shows the results from ASMAX-EC and ASMIN-EC
methods that select a word to change based on the
attention scores of the original sentence. A com-
prehensive quantitative comparison can be found
in Table 1, from which we make the following ob-
servations:
• Greedy-based attacks consistently achieve

higher successful rate than other attacks. The
proposed GS-EC method can achieve almost
identical success rates with GS-GR while re-
stricting the search space based on the em-
bedding distances. We will further show that

1As an alternative, we tested using the last layer during
ASMAX-ECattack. However, experimental results exhibit a <
10% success rate.



1523

Figure 1: Illustrations of attention scores of (a) the orig-
inal input, (b) ASMIN-EC, and (c) ASMAX-EC attacks.
The attention-based methods select words based on the
maximum or minimum attention, which is annotated by
red boxes. Both of them reversed the predicted senti-
ment of the sentence from positive to negative.

Model
Attack Method LSTM BERT BERTNOPT
RANDOM 1.1% 0.8% 1%

LIST 27% 6% 15%

ASMIN-GR 16% 11% 32%
ASMAX-GR 62% 17% 35%
ASMIN-EC 16% 10% 32%
ASMAX-EC 62% 17% 35%
Best attention attack(A∗) 62% 17% 35%

GS-GR 79% 52% 53%
GS-EC 78% 50% 53%

Table 1: Success rates of attack methods across mod-
els for sentiment analysis. Bold numbers indicate the
highest attack rate in a column.

GS-EC leads to higher quality adversarial ex-
amples in Section 4.2.
• We found that using attention, especially

ASMAX methods, can easily break the LSTM
model. However, the same vulnerability
does not exist in BERT or BERTNOPT mod-
els. Since different types of attention-based
attacks are suitable for different models, we
summarize the best attention-based attack
performance as A∗ in the table, which takes
the maximum over four different types of
attention-based attacks.
• Self-attentive models (BERT and BERTNOPT)

consistently lead to lower attack successful
rates compared with the LSTM model, un-
der RANDOM, LIST, attention-based attacks
and greedy-based attacks.

We demonstrate the robustness of BERT model
under GS-EC attack in Fig 2. We can see that,
GS-EC caused a substantial shift in the LSTM’s
attention map while that of BERT remain stable.

I
truly

enjoyed
my
visit
here

0.061

0.093

0.528

0.130

0.090

0.055

Original, Pred: P
I

truly
enjoyed

cello
visit
here

0.058

0.064

0.309

0.059

0.267

0.125

Adversarial, Pred: N

(a) LSTM

I
truly

enjoyed
my
visit
here

0.066

0.228

0.126

0.067

0.118

0.109

Original, Pred: P
I

truly
jammed

my
visit
here

0.061

0.238

0.133

0.067

0.118

0.109

Adversarial, Pred: N

(b) BERT

Figure 2: Attention scores in (a) LSTM and (b) BERT
models under GS-EC attacks. Although GS-EC suc-
cessfully flips the predicted sentiment for both models
from positive to negative, the attention scores remain
stable for BERT model. The LSTM model, however,
suffers from a large shift in attention distribution.

Method Sentence

GS-GR Pizzeria Bianco was a such never a nice treat that
was [...]

GS-EC Pizzeria Bianco was a such ostensibly a nice treat
that was [...]

GS-GR The desserts here are absolutely great 0 ! [...]

GS-EC The desserts here are absolutely great soluble !
[...]

Table 2: Adversarial examples for the BERT sentiment
analysis model generated by GS-GR and GS-EC meth-
ods. Both attacks caused the prediction of the model
to change. Note here that GS-EC model selects a word
that preserves local coherency due to the similarity con-
straints. GS-GR model, on the contrary, finds a word
that is less coherent with the context.

4.2 Quality of Adversarial Examples

We conduct experiments to assess the naturalness
of adversarial examples. First, Table 2 compares
the quality of the results generated by GS-GR and
GS-EC attacks on a BERT model. Here we see
that constraints imposed by GS-EC make it supe-
rior than GS-GR in terms of retrieving words that
are coherent with the context.

Furthermore, we organize a large-scale human
evaluation on Amazon Mechanical Turk regard-
ing the qualities of adversarial examples generated
by different methods. Each sample is voted by 3
turkers. Recall that we define “Readability” and
“Human accuracy” in Section 3.6. Readability is
regarded as the relative naturalness of the adver-



1524

sarial examples, normalized to the maximum be-
tween the compared methods. The human accu-
racy metric is the percentage of human responses
that matches the true label. Table 3 is a compari-
son of LSTM and BERT models using the GS-EC
attack. It shows that the distance in embeddings
space of BERT can better reflect semantic similar-
ity and contribute to more natural adversarial ex-
amples. And, in Table 4, we compare using GS-
GR and GS-EC method on BERT model. Again,
we see that the GS-EC method, which restricts the
distance between sentence embeddings of original
and adversarial inputs, can produce superior ad-
versaries.

Model Readability Human Accuracy

LSTM 0.6 52.1%
BERT 1.0 68.8%

Table 3: Comparison of LSTM and BERT models un-
der human evaluations against GS-EC attack. Read-
ability is a relative quality score between models, and
Human Accuracy is the percentage that human raters
correctly identify the adversarial examples.

Method Readability Human Accuracy

GS-GR 0.55 64.6%
GS-EC 1.0 68.8%

Table 4: Comparison of GS-GR and GS-EC attacks on
BERT model for sentiment analysis. Readability is a
relative quality score between attack methods, and Hu-
man Accuracy is the percentage that human raters cor-
rectly identify the sentiment of adversarial examples.

5 Experiment II: Textual Entailment

We conduct evaluations on MultiNLI (Williams
et al., 2018) dataset for textual entailment with ap-
proaches similar to the ones in the last section.
MultiNLI is one of the many datasets that see
major improvements by BERT. The BERT model
is trained to achieve 83.5% accuracy and LSTM
76%. BERTNOPT is excluded from this experiment
since it cannot reach a satisfactory accuracy.

5.1 Results
Results from entailment models fall into the same
pattern as those from sentiment analysis, which is
listed in Table 5. Our findings are summarized as
follows:
• The entailment task is more difficult than

single-sentence classification, as evidenced

by the higher success rates of attacks among
all models and attacks.
• The greedy-based attacks consistently

achieve higher success rates.
• ASMAX methods continue to be superior than

ASMIN, although the difference here is not as
drastic as in the previous experiment.
• BERT model remains more robust compared

with LSTM.

Model
Attack Method LSTM BERT
RANDOM 17.8% 9.2%

LIST 63% 56%

ASMIN-GR 57% 53%
ASMAX-GR 78% 54%
ASMIN-EC 55% 52%
ASMAX-EC 78% 51%
Best attention attack(A∗) 78% 54%

GS-GR 95% 75%
GS-EC 95% 75%

Table 5: Success rate of different attack methods on
LSTM and BERT for the MultiNLI development set.

5.2 Quality of Adversarial Examples
Samples illustrated in Table 6 show that the GS-
EC method can find more coherent words for the
attack, as opposed to GS-GR. For instance, chang-
ing the word “great” to “vast” can cause the model
to misjudge the entailment relation in the second
example. Unfortunately due to budget constraints,
we did not conduct large scale human experiments
on this dataset.

6 Experiment III: Machine Translation

We implement LSTM and Transformer machine
translation models using OpenNMT-py2. Specif-
ically, for the LSTM model, we train it with
453 thousand pairs from the Europarl corpus of
German-English WMT 15 Task3, common crawl,
and news-commentary. The LSTM model is
a two-layer bidirectional LSTM with 512 hid-
den units together with a attention layer. We
use the default hyper-parameters, and reproduce
the performance reported by Ha et al. (2016).
For the Transformer, we use a public pre-trained
model with 6 self-attention layers provided by
OpenNMT-py that reproduces the performance re-
ported by Vaswani et al. (2017).

2https://github.com/OpenNMT/OpenNMT-py
3http://www.statmt.org/wmt15/translation-task.html

https://github.com/OpenNMT/OpenNMT-py
http://www.statmt.org/wmt15/translation-task.html


1525

Label Sentence 1 Sentence 2

Contradiction No, I don’t know. (Original)Yes , I know.
→Neutral (GS-GR) Yes, I 0.

(GS-EC) Yes, I renovated.

Neutral
→Contradiction

That’s it. The girl looked at him, then passed her
hand across her forehead.

(Original)The girl looked at him with great interest.

(GS-GR) The girl looked at him with ! interest.
(GS-EC) The girl looked at him with vast interest.

Entailment
→Neutral

(Original)Workers are also represented in civil
rights and retaliation claims. Some workers are represented in civil rights and re-taliation claims.

(GS-GR) Workers are also represented in civil
rights and ? claims.

(GS-EC) Workers are also represented in civil
rights and targets claims.

Table 6: Adversarial examples generated by GS and GS-EC attacks for BERT entailment classifier.

Unlike the classification tasks, in machine trans-
lation the attack goal is harder to define. We chose
to evaluate the robustness under two types of at-
tacks. In the first type of “targeted keyword at-
tack” discussed in (Cheng et al., 2018), we attempt
to generate an adversarial input sequence such that
a specific keyword appears in the output sequence
within the threshold ∆ of number of word changes
we allowed. Empirically, we set ∆ = 3 in these
experiments and adopt the most successful attack,
GS-EC, to this case. For the second type of untar-
geted attack, we consider perturbing the input to
degrade the BLUE score of output sequences with
respect to the ground-truths. For doing this, we
conduct a typo-based attack (Belinkov and Bisk,
2018). Specifically, we randomly select one word
in each sentence and change it to a typo predefined
in a common typo list. This can be viewed as an
extension of LIST attack to the translation task.

6.1 Results

For the targeted keyword attack, the success rates
on both models are reported in Table 7. First, we
notice that the success rate of the attacks are below
30%, presumably because translation is substan-
tially more complex compared with the aforemen-
tioned text classification tasks. Nevertheless, the
attacks on the Transformer model is significantly
less successful than the LSTM-based one.

For the typo-based attack, the BLUE scores be-
fore/after the attack are reported in Table 8. We
observe that the Transformer-based model always
achieves a higher BLEU score over LSTM-based
model, i.e., have a better translation performance
whether the sentences contain typos or not. We
conclude that Transformer-based model exhibits
a greater robustness over LSTM-based model in

(a) LSTM (b) Transformer

Figure 3: Compare attention scores of the original ver-
sus adversarial inputs for LSTM and Transformer mod-
els for machine translation.

the case of machine translation. This is consistent
with our findings in the previous experiments on
sentiment and entailment classification problems.

In addition, we present some successful adver-
sarial examples in Table 9, and see that the greedy
attack can indeed generate natural examples for
both models.

Attack Method LSTM Transformer

GS-EC 27.5% 10.5%

Table 7: Targeted attack success rate with GS-EC in
translation tasks.

Model Original Adversarial

LSTM 25.10 13.44
Transformer 34.90 26.02

Table 8: BLEU scores using typo-based attack on
LSTM and Transformer translation models.

7 Theoretical Analysis

All the above experiments conclude that a self-
attentive model exhibits higher robustness com-
pared to a recurrent one. This is somewhat
counter-intuitive—at the first glance one may as-
sume that the self-attention layer is not robust



1526

Original input There is a fundamental philosophical reason for the differences between Donald Trump’s and
Hillary Clinton’s [...]

L
ST

M
Adv input There is a fundamental philosophical r for the differences between Donald Trump’s and Hillary

Clinton’s [...]
Original output Es gibt einen grundlegenden philosophischen Grund für die Unterschiede zwischen Donald

Trump und Hillary Clinton s
Adv output Es gibt eine grundlegende philosophischer Art , wie Unterschied e zwischen Donald Trump

und Hillary Clinton s

T
F

Original input And in this vein , he passed the prize money of 2 5,000 euros on straight away
Adv input And as this vein , he passed the prize money of 2 5,000 euros on straight away
Original output Und in diesem Sinne hat er sofort das Preis geld von 2 5.000 Euro über wiesen
Adv output Und als diese Art , ging er sofort das Preis geld von 2 5.000 Euro weiter

Table 9: Adversarial examples for LSTM and Transformer (shortened as TF) models with the target keyword
“Art.” in the output.

since perturbation in one word can affect all the
attention scores. In this section, we provide some
explanation regarding this phenomenon by study-
ing how error propagates through the self-attention
architecture. We show that the perturbation of one
input embedding can in fact only have sparse af-
fect to the attention scores when the input embed-
ding are scattered enough in the space.

Sensitivity of Self-Attention Layers : First,
we consider the simple case of one self-attention
layer with a single head. Assume a sentence has
n input words and each word is represented by
a d-dimensional embedding vectors, denoted by
x1, . . . , xn ∈ Rd. We useWQ,WK ,W V ∈ Rd×k
to denote the query, key and value transformations.
The contribution of each element j to i is then
computed by

sij = x
T
i W

Q(WK)Txj ,

and then the i-th embedding at the next layer is
obtained by

zi =
∑
j

esij∑
k e

sik
(W V xj),

Sometimes zi is fed into another linear layer to ob-
tain the embeddings. Now, consider that a small
perturbation is added to a particular index j̄, such
that xj̄ is changed to xj̄ + ∆x while all the other
{xj | j 6= j̄} remain unchanged. We then study
how much this perturbation will affect {zi}i∈[n].
For a particular i ( 6= j), the sij is only changed by
one term since

s′ij =

{
sij if j 6= j̄
sij + x

T
i W

Q(WK)T∆x if j = j̄
(1)

where we use s′ij to denote the value after the
perturbation. Therefore, with the perturbed in-
put, each set of {sij}nj=1 will only have one term

being changed. Furthermore, the changed term
in equation 1 is the inner product between xi and
a fixed vectorWQ(WK)T∆x; although this could
be large for some particular xi in the similar direc-
tion ofWQ(WK)T∆x, if the embeddings {xi}ni=1
are scattered enough over the space, the inner
products cannot be large for all {xi}ni=1. There-
fore, the change to the next layer will be sparse.
For instance, we can prove the sparsity under some
distributional assumptions on {xi}:
Theorem 1. Assume ‖∆x‖ ≤ δ and {xi}ni=1 are
d-dimensional vectors uniformly distributed on the
unit sphere, then E[|s′

ij̄
− sij̄ |] ≤ Cδ√d with C =

‖WQ‖‖WK‖ and P (|s′
ij̄
− sij̄ | ≥ �) ≤ Cδ�√d .

Proof. The value E[s′
ij̄
− sij̄ ] = E[xTi z] where

z = WQ(WK)T∆x is a fixed vector, and it is easy
to derive ‖z‖ ≤ ‖WQ‖‖WK‖δ. To bound this
expectation, we first try to bound a1 = E[xTi e1]
where e1 = [1, 0, . . . , 0]. Due to the rotation
invariance we can obtain a1 = · · · = ad and∑

i a
2
i = 1, so |a1| = 1√d . This implies E[x

T
i z] ≤

Cδ√
d

. Using Markov inequality, we can then find the
probability results.

Therefore, as the norm of WQ,WK are not too
large (usually regularized by L2 during training)
and the dimension d is large enough, there will be
a significant amount of i such that sij is perturbed
negligibly.

In contrast, embeddings from RNN-based mod-
els are relatively more sensitive to perturbation of
one word, as shown below. Similar to the previ-
ous case, we assume a sequence x1, . . . , xn, and a
word xj̄ is perturbed by ∆x. For the vanilla RNN
model, the embeddings are sequentially computed
as zi = σ(Axi + Bzi−1). If xj̄ is perturbed, then
all the {zi}ni=j̄ will be altered. Therefore, the at-



1527

tacker can more easily influence all the embed-
dings.

As an illustration of the proposed theory, we
plot a comparison of the degree of embeddings
variation from two models after changing one
word in Fig. 4. We observe that, for self-attentive
models, the distribution of change on embeddings
is sparse after going through the first self-attention
layer (layer 1) and then gradually propagate to the
whole sequence when passing through more lay-
ers. In contrast, the embeddings from LSTM ex-
hibit a denser pattern. To further validate our anal-
ysis, we calculate the ratio of the L2 norms of em-
beddings variation. Specifically, let z and zadv de-
note the embeddings of the original sentence and
adversarial input, respectively. We represent rela-
tive embedding variation Re = ‖z − zadv‖/‖z‖.
For the GS-EC attack in the sentiment analysis
task, embeddings from the LSTM model has an
average Re of 0.83 whereas for the BERT model
it is 0.56 under the same attack by changing one
word. This supports our claim that the impact
of an adversarial example is more severe on the
LSTM model than BERT, which presumably plays
an important role in the robustness of self-attentive
models.

(a) LSTM (b) BERT

Figure 4: Comparison of L2 norm of embedding vari-
ations after changing one word (marked by red box) in
the input to (a) LSTM (b) BERT.

8 Related Work

Robustness of neural network models has been
a prominent research topic since Szegedy et al.
(2013) discovered that CNN-based image classifi-
cation models are vulnerable to adversarial exam-
ples. However, attempts to examine the robustness
of NLP models are relatively few and far between.
Previous work on attacking neural NLP models
include using Fast Gradient Sign Method (Good-
fellow et al., 2015) to perturb the embedding
of RNN-based classifiers (Papernot et al., 2016;

Liang et al., 2017), but they have difficulties map-
ping from continuous embedding space to discrete
input space. Ebrahimi et al. (2018) propose the
‘HotFilp’ method that replaces the word or char-
acter with the largest difference in the Jacobian
matrix. Li et al. (2016) employ reinforcement
learning to find the optimal words to delete in or-
der to fool the classifier. More recently, Yang
et al. (2018) propose a greedy method to con-
struct adversarial examples by solving a discrete
optimization problem. They show superior per-
formance than previous work in terms of attack
success rate, but the greedy edits usually degrade
the readability or significantly change the seman-
tics. Zhao et al. (2018) utilize generative adversar-
ial networks (GAN) to generate adversarial attacks
against black-box models for applications includ-
ing image classification, textual entailment, and
machine translation. Alzantot et al. (2018) pro-
pose to use a pre-compiled list of semantically
similar words to alleviate this issue, but leads to
lower successful rate as shown in our experiments.
We thus include the latest greedy and list-based
approaches in our comparisons.

In addition, the concept of adversarial attacks
has also been explored in more complex NLP
tasks. For example, Jia and Liang (2017) attempt
to craft adversarial input to a question answer-
ing system by inserting irrelevant sentences at the
end of a paragraph. Cheng et al. (2018) develop
an algorithm for attacking seq2seq models with
specific constraints on the content of the adver-
sarial examples. Belinkov and Bisk (2018) com-
pare typos and artificial noise as adversarial in-
put to machine translation models. Also, Iyyer
et al. (2018) propose a paraphrase generator model
learned from back-translation data to generate le-
gitimate paraphrases of a sentence as adversaries.
However, the semantic similarity is not guaran-
teed. In terms of comparisons between LSTM and
Transformers, Tang et al. (2018) show that multi-
headed attention is a critical factor in Transformer
when learning long distance linguistic relations.

This work is unique in a number of aspects.
First, we examine the robustness of uni- and bi-
directional self-attentive model as compared to re-
current neural networks. And, we devise novel at-
tack methods that take advantage of the embed-
ding distance to maximize semantic similarity be-
tween real and adversarial examples. Last but not
least, we provide detail observations of the inter-



1528

nal variations of different models under attack and
theoretical analysis regarding their levels of ro-
bustness.

9 Conclusions

We show that self-attentive models are more ro-
bust to adversarial attacks than recurrent networks
under small input perturbations on three NLP
tasks, i.e., sentiment analysis, entailment, and
translation. We provide theoretical explanations
regarding why the self-attention structure leads to
better robustness, in addition to illustrative ex-
amples that visualize the model’s internal varia-
tions. Future work includes developing a adver-
sarial training scheme as well as devising a more
robust architecture based on our findings.

Acknowledgements

We are grateful for the insightful comments from
anonymous reviewers. This work is supported
by the Ministry of Science and Technology of
Taiwan under grant numbers 107-2917-I-004-001,
108-2634-F-001-005. The author Yu-Lun Hsieh
wishes to acknowledge, with thanks, the Tai-
wan International Graduate Program (TIGP) of
Academia Sinica for financial support towards at-
tending this conference. We also acknowledge
the support from NSF via IIS1719097, Intel and
Google Cloud.

References
Moustafa Alzantot, Yash Sharma, Ahmed Elgohary,

Bo-Jhang Ho, Mani Srivastava, and Kai-Wei Chang.
2018. Generating natural language adversarial ex-
amples. In Proceedings of the 2018 Conference on
Empirical Methods in Natural Language Process-
ing, pages 2890–2896. Association for Computa-
tional Linguistics.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
learning to align and translate. arXiv preprint
arXiv:1409.0473.

Yonatan Belinkov and Yonatan Bisk. 2018. Synthetic
and natural noise both break neural machine transla-
tion. In International Conference on Learning Rep-
resentations.

Ondřej Bojar, Rajen Chatterjee, Christian Federmann,
Yvette Graham, Barry Haddow, Shujian Huang,
Matthias Huck, Philipp Koehn, Qun Liu, Varvara
Logacheva, Christof Monz, Matteo Negri, Matt
Post, Raphael Rubino, Lucia Specia, and Marco
Turchi. 2017. Findings of the 2017 conference

on machine translation (wmt17). In Proceedings
of the Second Conference on Machine Translation,
Volume 2: Shared Task Papers, pages 169–214,
Copenhagen, Denmark. Association for Computa-
tional Linguistics.

Minhao Cheng, Jinfeng Yi, Huan Zhang, Pin-Yu Chen,
and Cho-Jui Hsieh. 2018. Seq2sick: Evaluating the
robustness of sequence-to-sequence models with ad-
versarial examples. CoRR, abs/1803.01128.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers),
pages 4171–4186, Minneapolis, Minnesota. Associ-
ation for Computational Linguistics.

Javid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing
Dou. 2018. Hotflip: White-box adversarial exam-
ples for text classification. In Proceedings of the
56th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 2: Short Papers), vol-
ume 2, pages 31–36.

Ian Goodfellow, Jonathon Shlens, and Christian
Szegedy. 2015. Explaining and harnessing adversar-
ial examples. In International Conference on Learn-
ing Representations.

Thanh-Le Ha, Jan Niehues, and Alexander Waibel.
2016. Toward multilingual neural machine trans-
lation with universal encoder and decoder. arXiv
preprint arXiv:1611.04798.

Sepp Hochreiter and Jürgen Schmidhuber. 1997. Lstm
can solve hard long time lag problems. In Ad-
vances in neural information processing systems,
pages 473–479.

Mohit Iyyer, John Wieting, Kevin Gimpel, and Luke
Zettlemoyer. 2018. Adversarial example generation
with syntactically controlled paraphrase networks.
In Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Volume 1 (Long Papers), volume 1, pages 1875–
1885.

Robin Jia and Percy Liang. 2017. Adversarial exam-
ples for evaluating reading comprehension systems.
In Proceedings of the 2017 Conference on Empiri-
cal Methods in Natural Language Processing, pages
2021–2031.

Jiwei Li, Will Monroe, and Dan Jurafsky. 2016. Un-
derstanding neural networks through representation
erasure. arXiv preprint arXiv:1612.08220.

Bin Liang, Hongcheng Li, Miaoqiang Su, Pan Bian,
Xirong Li, and Wenchang Shi. 2017. Deep
text classification can be fooled. arXiv preprint
arXiv:1704.08006.

http://aclweb.org/anthology/D18-1316
http://aclweb.org/anthology/D18-1316
https://openreview.net/forum?id=BJ8vJebC-
https://openreview.net/forum?id=BJ8vJebC-
https://openreview.net/forum?id=BJ8vJebC-
http://www.aclweb.org/anthology/W17-4717
http://www.aclweb.org/anthology/W17-4717
http://arxiv.org/abs/1803.01128
http://arxiv.org/abs/1803.01128
http://arxiv.org/abs/1803.01128
https://www.aclweb.org/anthology/N19-1423
https://www.aclweb.org/anthology/N19-1423
https://www.aclweb.org/anthology/N19-1423


1529

Thang Luong, Hieu Pham, and Christopher D Man-
ning. 2015. Effective approaches to attention-based
neural machine translation. In Proceedings of the
2015 Conference on Empirical Methods in Natural
Language Processing, pages 1412–1421.

Nicolas Papernot, Patrick McDaniel, Ananthram
Swami, and Richard Harang. 2016. Crafting adver-
sarial input sequences for recurrent neural networks.
In Military Communications Conference, MILCOM
2016-2016 IEEE, pages 49–54. IEEE.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of
the 40th annual meeting on association for compu-
tational linguistics, pages 311–318. Association for
Computational Linguistics.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in neural information process-
ing systems, pages 3104–3112.

Christian Szegedy, Wojciech Zaremba, Ilya Sutskever,
Joan Bruna, Dumitru Erhan, Ian Goodfellow, and
Rob Fergus. 2013. Intriguing properties of neural
networks. arXiv preprint arXiv:1312.6199.

Gongbo Tang, Mathias Müller, Annette Rios, and Rico
Sennrich. 2018. Why self-attention? a targeted
evaluation of neural machine translation architec-
tures. In Conference on Empirical Methods in Nat-
ural Language Processing, pages 4263–4272.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems, pages 5998–6008.

Adina Williams, Nikita Nangia, and Samuel Bowman.
2018. A broad-coverage challenge corpus for sen-
tence understanding through inference. In Proceed-
ings of the 2018 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, Volume 1
(Long Papers), pages 1112–1122. Association for
Computational Linguistics.

Puyudi Yang, Jianbo Chen, Cho-Jui Hsieh, Jane-Ling
Wang, and Michael I Jordan. 2018. Greedy attack
and gumbel attack: Generating adversarial examples
for discrete data. arXiv preprint arXiv:1805.12316.

Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015.
Character-level convolutional networks for text clas-
sification. In C. Cortes, N. D. Lawrence, D. D. Lee,
M. Sugiyama, and R. Garnett, editors, Advances in
Neural Information Processing Systems 28, pages
649–657. Curran Associates, Inc.

Zhengli Zhao, Dheeru Dua, and Sameer Singh. 2018.
Generating natural adversarial examples. In Inter-
national Conference on Learning Representations.


