



















































Learning to Select, Track, and Generate for Data-to-Text


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2102–2113
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

2102

Learning to Select, Track, and Generate for Data-to-Text

Hayate Iso
∗†Yui Uehara‡ Tatsuya Ishigaki\‡Hiroshi Noji‡

Eiji Aramaki†‡ Ichiro Kobayashi[‡Yusuke Miyao]‡Naoaki Okazaki\‡Hiroya Takamura\‡
†Nara Institute of Science and Technology ‡Artificial Intelligence Research Center, AIST

\Tokyo Institute of Technology [Ochanomizu University ]The University of Tokyo
{iso.hayate.id3,aramaki}@is.naist.jp koba@is.ocha.ac.jp

{yui.uehara,ishigaki.t,hiroshi.noji,takamura.hiroya}@aist.go.jp
yusuke@is.s.u-tokyo.ac.jp okazaki@c.titech.ac.jp

Abstract

We propose a data-to-text generation model
with two modules, one for tracking and the
other for text generation. Our tracking mod-
ule selects and keeps track of salient infor-
mation and memorizes which record has been
mentioned. Our generation module generates
a summary conditioned on the state of track-
ing module. Our model is considered to simu-
late the human-like writing process that gradu-
ally selects the information by determining the
intermediate variables while writing the sum-
mary. In addition, we also explore the ef-
fectiveness of the writer information for gen-
eration. Experimental results show that our
model outperforms existing models in all eval-
uation metrics even without writer informa-
tion. Incorporating writer information fur-
ther improves the performance, contributing to
content planning and surface realization.

1 Introduction

Advances in sensor and data storage technolo-
gies have rapidly increased the amount of data
produced in various fields such as weather, fi-
nance, and sports. In order to address the infor-
mation overload caused by the massive data, data-
to-text generation technology, which expresses the
contents of data in natural language, becomes
more important (Barzilay and Lapata, 2005). Re-
cently, neural methods can generate high-quality
short summaries especially from small pieces of
data (Liu et al., 2018).

Despite this success, it remains challenging
to generate a high-quality long summary from
data (Wiseman et al., 2017). One reason for the
difficulty is because the input data is too large for
a naive model to find its salient part, i.e., to deter-
mine which part of the data should be mentioned.

∗Work was done during the internship at Artificial Intel-
ligence Research Center, AIST

In addition, the salient part moves as the sum-
mary explains the data. For example, when gen-
erating a summary of a basketball game (Table 1
(b)) from the box score (Table 1 (a)), the input
contains numerous data records about the game:
e.g., Jordan Clarkson scored 18 points. Existing
models often refer to the same data record mul-
tiple times (Puduppully et al., 2019). The mod-
els may mention an incorrect data record, e.g.,
Kawhi Leonard added 19 points: the summary
should mention LaMarcus Aldridge, who scored
19 points. Thus, we need a model that finds salient
parts, tracks transitions of salient parts, and ex-
presses information faithful to the input.

In this paper, we propose a novel data-to-
text generation model with two modules, one for
saliency tracking and another for text generation.
The tracking module keeps track of saliency in the
input data: when the module detects a saliency
transition, the tracking module selects a new data
record1 and updates the state of the tracking mod-
ule. The text generation module generates a doc-
ument conditioned on the current tracking state.
Our model is considered to imitate the human-like
writing process that gradually selects and tracks
the data while generating the summary. In ad-
dition, we note some writer-specific patterns and
characteristics: how data records are selected to be
mentioned; and how data records are expressed as
text, e.g., the order of data records and the word
usages. We also incorporate writer information
into our model.

The experimental results demonstrate that, even
without writer information, our model achieves
the best performance among the previous models
in all evaluation metrics: 94.38% precision of re-
lation generation, 42.40% F1 score of content se-
lection, 19.38% normalized Damerau-Levenshtein

1We use ‘data record’ and ‘relation’ interchangeably.



2103

Distance (DLD) of content ordering, and 16.15%
of BLEU score. We also confirm that adding
writer information further improves the perfor-
mance.

2 Related Work

2.1 Data-to-Text Generation

Data-to-text generation is a task for generating de-
scriptions from structured or non-structured data
including sports commentary (Tanaka-Ishii et al.,
1998; Chen and Mooney, 2008; Taniguchi et al.,
2019), weather forecast (Liang et al., 2009; Mei
et al., 2016), biographical text from infobox in
Wikipedia (Lebret et al., 2016; Sha et al., 2018;
Liu et al., 2018) and market comments from stock
prices (Murakami et al., 2017; Aoki et al., 2018).

Neural generation methods have become the
mainstream approach for data-to-text generation.
The encoder-decoder framework (Cho et al., 2014;
Sutskever et al., 2014) with the attention (Bah-
danau et al., 2015; Luong et al., 2015) and copy
mechanism (Gu et al., 2016; Gulcehre et al., 2016)
has successfully applied to data-to-text tasks.
However, neural generation methods sometimes
yield fluent but inadequate descriptions (Tu et al.,
2017). In data-to-text generation, descriptions in-
consistent to the input data are problematic.

Recently, Wiseman et al. (2017) introduced
the ROTOWIRE dataset, which contains multi-
sentence summaries of basketball games with box-
score (Table 1). This dataset requires the selection
of a salient subset of data records for generating
descriptions. They also proposed automatic evalu-
ation metrics for measuring the informativeness of
generated summaries.

Puduppully et al. (2019) proposed a two-stage
method that first predicts the sequence of data
records to be mentioned and then generates a
summary conditioned on the predicted sequences.
Their idea is similar to ours in that the both con-
sider a sequence of data records as content plan-
ning. However, our proposal differs from theirs
in that ours uses a recurrent neural network for
saliency tracking, and that our decoder dynami-
cally chooses a data record to be mentioned with-
out fixing a sequence of data records.

2.2 Memory modules

The memory network can be used to maintain
and update representations of the salient informa-
tion (Weston et al., 2015; Sukhbaatar et al., 2015;

Graves et al., 2016). This module is often used
in natural language understanding to keep track
of the entity state (Kobayashi et al., 2016; Hoang
et al., 2018; Bosselut et al., 2018).

Recently, entity tracking has been popular for
generating coherent text (Kiddon et al., 2016; Ji
et al., 2017; Yang et al., 2017; Clark et al., 2018).
Kiddon et al. (2016) proposed a neural checklist
model that updates predefined item states. Ji et al.
(2017) proposed an entity representation for the
language model. Updating entity tracking states
when the entity is introduced, their method selects
the salient entity state.

Our model extends this entity tracking module
for data-to-text generation tasks. The entity track-
ing module selects the salient entity and appropri-
ate attribute in each timestep, updates their states,
and generates coherent summaries from the se-
lected data record.

3 Data

Through careful examination, we found that in the
original dataset ROTOWIRE, some NBA games
have two documents, one of which is sometimes in
the training data and the other is in the test or val-
idation data. Such documents are similar to each
other, though not identical. To make this dataset
more reliable as an experimental dataset, we cre-
ated a new version.

We ran the script provided by Wiseman et al.
(2017), which is for crawling the ROTOWIRE
website for NBA game summaries. The script col-
lected approximately 78% of the documents in the
original dataset; the remaining documents disap-
peared. We also collected the box-scores associ-
ated with the collected documents. We observed
that some of the box-scores were modified com-
pared with the original ROTOWIRE dataset.

The collected dataset contains 3,752 instances
(i.e., pairs of a document and box-scores). How-
ever, the four shortest documents were not sum-
maries; they were, for example, an announcement
about the postponement of a match. We thus
deleted these 4 instances and were left with 3,748
instances. We followed the dataset split by Wise-
man et al. (2017) to split our dataset into train-
ing, development, and test data. We found 14 in-
stances that didn’t have corresponding instances in
the original data. We randomly classified 9, 2, and
3 of those 14 instances respectively into training,
development, and test data. Finally, the sizes of



2104

TEAM H/V WIN LOSS PTS REB AST FG PCT FG3 PCT . . .

KNICKS H 16 19 104 46 26 45 46 . . .
BUCKS V 18 16 105 42 20 47 32 . . .

PLAYER H/V PTS REB AST BLK STL MIN CITY . . .

CARMELO ANTHONY H 30 11 7 0 2 37 NEW YORK . . .
DERRICK ROSE H 15 3 4 0 1 33 NEW YORK . . .
COURTNEY LEE H 11 2 3 1 1 38 NEW YORK . . .
GIANNIS ANTETOKOUNMPO V 27 13 4 3 1 39 MILWAUKEE . . .
GREG MONROE V 18 9 4 1 3 31 MILWAUKEE . . .
JABARI PARKER V 15 4 3 0 1 37 MILWAUKEE . . .
MALCOLM BROGDON V 12 6 8 0 0 38 MILWAUKEE . . .
MIRZA TELETOVIC V 13 1 0 0 0 21 MILWAUKEE . . .
JOHN HENSON V 2 2 0 0 0 14 MILWAUKEE . . .
. . . . . . . . . . . . . . . . . . . . .

(a) Box score: Top contingency table shows number of wins and
losses and summary of each game. Bottom table shows statistics
of each player such as points scored (PLAYER’s PTS), and total
rebounds (PLAYER’s REB).

The Milwaukee Bucks defeated the New York Knicks,
105-104, at Madison Square Garden on Wednesday. The
Knicks (16-19) checked in to Wednesday’s contest looking
to snap a five-game losing streak and heading into the fourth
quarter, they looked like they were well on their way to that
goal. . . . Antetokounmpo led the Bucks with 27 points, 13
rebounds, four assists, a steal and three blocks, his second
consecutive double-double. Greg Monroe actually checked
in as the second-leading scorer and did so in his customary
bench role, posting 18 points, along with nine boards, four
assists, three steals and a block. Jabari Parker contributed
15 points, four rebounds, three assists and a steal. Malcolm
Brogdon went for 12 points, eight assists and six rebounds.
Mirza Teletovic was productive in a reserve role as well,
generating 13 points and a rebound. . . . Courtney Lee
checked in with 11 points, three assists, two rebounds, a
steal and a block. . . . The Bucks and Knicks face off once
again in the second game of the home-and-home series, with
the meeting taking place Friday night in Milwaukee.

(b) NBA basketball game summary: Each summary
consists of game victory or defeat of the game and
highlights of valuable players.

Table 1: Example of input and output data: task defines box score (1a) used for input and summary document of
game (1b) used as output. Extracted entities are shown in bold face. Extracted values are shown in green.

t 199 200 201 202 203 204 205 206 207 208 209

Yt Jabari Parker contributed 15 points , four rebounds , three assists

Zt 1 1 0 1 0 0 1 0 0 1 0

Et
JABARI JABARI - JABARI - - JABARI - - JABARI -PARKER PARKER PARKER PARKER PARKER

At FIRST NAME LAST NAME - PLAYER PTS - - PLAYER REB - - PLAYER AST -
Nt - - - 0 - - 1 - - 1 -

Table 2: Running example of our model’s generation process. At every time step t, model predicts each random
variable. Model firstly determines whether to refer to data records (Zt = 1) or not (Zt = 0). If random variable
Zt = 1, model selects entity Et, its attribute At and binary variables Nt if needed. For example, at t = 202, model
predicts random variable Z202 = 1 and then selects the entity JABARI PARKER and its attribute PLAYER PTS.
Given these values, model outputs token 15 from selected data record.

our training, development, test dataset are respec-
tively 2,714, 534, and 500. On average, each sum-
mary has 384 tokens and 644 data records. Each
match has only one summary in our dataset, as
far as we checked. We also collected the writer
of each document. Our dataset contains 32 differ-
ent writers. The most prolific writer in our dataset
wrote 607 documents. There are also writers who
wrote less than ten documents. On average, each
writer wrote 117 documents. We call our new
dataset ROTOWIRE-MODIFIED.2

4 Saliency-Aware Text Generation

At the core of our model is a neural language
model with a memory state hLM to generate a
summary y1:T = (y1, . . . , yT ) given a set of data
records x. Our model has another memory state
hENT, which is used to remember the data records

2For information about the dataset, please follow
this link: https://github.com/aistairc/
rotowire-modified

that have been referred to. hENT is also used to up-
date hLM, meaning that the referred data records
affect the text generation.

Our model decides whether to refer to x, which
data record r ∈ x to be mentioned, and how to ex-
press a number. The selected data record is used to
update hENT. Formally, we use the four variables:

1. Zt: binary variable that determines whether the
model refers to input x at time step t (Zt = 1).

2. Et: At each time step t, this variable indi-
cates the salient entity (e.g., HAWKS, LEBRON
JAMES).

3. At: At each time step t, this variable indicates
the salient attribute to be mentioned (e.g., PTS).

4. Nt: If attribute At of the salient entity Et is
a numeric attribute, this variable determines if
a value in the data records should be output in
Arabic numerals (e.g., 50) or in English words
(e.g., five).

To keep track of the salient entity, our model
predicts these random variables at each time step

https://github.com/aistairc/rotowire-modified
https://github.com/aistairc/rotowire-modified


2105

t through its summary generation process. Run-
ning example of our model is shown in Table 2
and full algorithm is described in Appendix A. In
the following subsections, we explain how to ini-
tialize the model, predict these random variables,
and generate a summary. Due to space limitations,
bias vectors are omitted.

Before explaining our method, we describe our
notation. Let E and A denote the sets of en-
tities and attributes, respectively. Each record
r ∈ x consists of entity e ∈ E , attribute a ∈ A,
and its value x[e, a], and is therefore represented
as r = (e, a,x[e, a]). For example, the box-
score in Table 1 has a record r such that e =
ANTHONY DAVIS, a = PTS, and x[e, a] = 20.

4.1 Initialization
Let r denote the embedding of data record r ∈ x.
Let ē denote the embedding of entity e. Note that
ē depends on the set of data records, i.e., it de-
pends on the game. We also use e for static em-
bedding of entity e, which, on the other hand, does
not depend on the game.

Given the embedding of entity e, attribute a,
and its value v, we use the concatenation layer
to combine the information from these vectors
to produce the embedding of each data record
(e, a, v), denoted as re,a,v as follows:

re,a,v = tanh
(
W R(e⊕ a⊕ v)

)
, (1)

where ⊕ indicates the concatenation of vectors,
and W R denotes a weight matrix.3

We obtain ē in the set of data records x, by sum-
ming all the data-record embeddings transformed
by a matrix:

ē = tanh

(∑
a∈A

W Aa re,a,x[e,a]

)
, (2)

where W Aa is a weight matrix for attribute a.
Since ē depends on the game as above, ē is sup-
posed to represent how entity e played in the game.

To initialize the hidden state of each module, we
use embeddings of<SOD> for hLM and averaged
embeddings of ē for hENT.

4.2 Saliency transition
Generally, the saliency of text changes during text
generation. In our work, we suppose that the

3We also concatenate the embedding vectors that repre-
sents whether the entity is in home or away team.

saliency is represented as the entity and its at-
tribute being talked about. We therefore propose
a model that refers to a data record at each time-
point, and transitions to another as text goes.

To determine whether to transition to another
data record or not at time t, the model calculates
the following probability:

p(Zt = 1 | hLMt−1,hENTt−1) = σ(W z(hLMt−1 ⊕ hENTt−1)),
(3)

where σ(·) is the sigmoid function. If p(Zt = 1 |
hLMt−1,h

ENT
t−1) is high, the model transitions to an-

other data record.
When the model decides to transition to another,

the model then determines which entity and at-
tribute to refer to, and generates the next word
(Section 4.3). On the other hand, if the model de-
cides not transition to another, the model generates
the next word without updating the tracking states
hENTt = h

ENT
t−1 (Section 4.4).

4.3 Selection and tracking
When the model refers to a new data record
(Zt = 1), it selects an entity and its attribute. It
also tracks the saliency by putting the informa-
tion about the selected entity and attribute into the
memory vector hENT. The model begins to select
the subject entity and update the memory states if
the subject entity will change.

Specifically, the model first calculates the prob-
ability of selecting an entity:

p(Et = e | hLMt−1,hENTt−1)

∝

{
exp

(
hENTs W

OLDhLMt−1
)

if e ∈ Et−1
exp

(
ēW NEWhLMt−1

)
otherwise

, (4)

where Et−1 is the set of entities that have already
been referred to by time step t, and s is defined as
s = max{s : s ≤ t− 1, e = es}, which indicates
the time step when this entity was last mentioned.

The model selects the most probable entity as
the next salient entity and updates the set of enti-
ties that appeared (Et = Et−1 ∪ {et}).

If the salient entity changes (et 6= et−1), the
model updates the hidden state of the tracking
model hENT with a recurrent neural network with
a gated recurrent unit (GRU; Chung et al., 2014):

hENT
′

t =


hENTt−1 if et = et−1
GRUE(ē,hENTt−1) else if et 6∈ Et−1
GRUE(W Ssh

ENT
s ,h

ENT
t−1) otherwise.

(5)



2106

Note that if the selected entity at time step t, et, is
identical to the previously selected entity et−1, the
hidden state of the tracking model is not updated.

If the selected entity et is new (et 6∈ Et−1), the
hidden state of the tracking model is updated with
the embedding ē of entity et as input. In contrast,
if entity et has already appeared in the past (et ∈
Et−1) but is not identical to the previous one (et 6=
et−1), we use hENTs (i.e., the memory state when
this entity last appeared) to fully exploit the local
history of this entity.

Given the updated hidden state of the tracking
model hENTt , we next select the attribute of the
salient entity by the following probability:

p(At = a | et,hLMt−1,hENT
′

t ) (6)

∝ exp
(
ret,a,x[et,a]W

ATTR(hLMt−1 ⊕ hENT
′

t )
)
.

After selecting at, i.e., the most probable attribute
of the salient entity, the tracking model updates the
memory state hENTt with the embedding of the data
record ret,at,x[et,at] introduced in Section 4.1:

hENTt = GRU
A(ret,at,x[et,at],h

ENT′
t ). (7)

4.4 Summary generation
Given two hidden states, one for language model
hLMt−1 and the other for tracking model h

ENT
t , the

model generates the next word yt. We also incor-
porate a copy mechanism that copies the value of
the salient data record x[et, at].

If the model refers to a new data record (Zt =
1), it directly copies the value of the data record
x[et, at]. However, the values of numerical at-
tributes can be expressed in at least two different
manners: Arabic numerals (e.g., 14) and English
words (e.g., fourteen). We decide which one to use
by the following probability:

p(Nt = 1 | hLMt−1,hENTt ) = σ(W N(hLMt−1 ⊕ hENTt )),
(8)

where W N is a weight matrix. The model then
updates the hidden states of the language model:

h′t = tanh
(
W H(hLMt−1 ⊕ hENTt )

)
, (9)

where W H is a weight matrix.
If the salient data record is the same as the pre-

vious one (Zt = 0), it predicts the next word yt via
a probability over words conditioned on the con-
text vector h′t:

p(Yt | h′t) = softmax(W Yh′t). (10)

Subsequently, the hidden state of language model
hLM is updated:

hLMt = LSTM(yt ⊕ h′t,hLMt−1), (11)

where yt is the embedding of the word generated
at time step t.4

4.5 Incorporating writer information
We also incorporate the information about the
writer of the summaries into our model. Specif-
ically, instead of using Equation (9), we concate-
nate the embedding w of a writer to hLMt−1 ⊕ hENTt
to construct context vector h′t:

h′t = tanh
(
W ′H(hLMt−1 ⊕ hENTt ⊕w)

)
, (12)

where W ′H is a new weight matrix. Since this new
context vector h′t is used for calculating the proba-
bility over words in Equation (10), the writer infor-
mation will directly affect word generation, which
is regarded as surface realization in terms of tra-
ditional text generation. Simultaneously, context
vector h′t enhanced with the writer information is
used to obtain hLMt , which is the hidden state of
the language model and is further used to select the
salient entity and attribute, as mentioned in Sec-
tions 4.2 and 4.3. Therefore, in our model, the
writer information affects both surface realization
and content planning.

4.6 Learning objective
We apply fully supervised training that maximizes
the following log-likelihood:

log p(Y1:T , Z1:T , E1:T , A1:T , N1:T | x)

=

T∑
t=1

log p(Zt = zt | hLMt−1,hENTt−1)

+
∑

t:Zt=1

log p(Et = et | hLMt−1,hENTt−1)

+
∑

t:Zt=1

log p(At = at | et,hLMt−1,hENT
′

t )

+
∑

t:Zt=1,atis num attr

log p(Nt = nt | hLMt−1,hENTt )

+
∑

t:Zt=0

log p(Yt = yt | h′t)

4In our initial experiment, we observed a word repetition
problem when the tracking model is not updated during gen-
erating each sentence. To avoid this problem, we also update
the tracking model with special trainable vectors vREFRESH
to refresh these states after our model generates a period:
hENTt = GRUA(vREFRESH,hENTt )



2107

Method
RG CS CO

BLEU
# P% P% R% F1% DLD%

GOLD 27.36 93.42 100. 100. 100. 100. 100.
TEMPLATES 54.63 100. 31.01 58.85 40.61 17.50 8.43

Wiseman et al. (2017) 22.93 60.14 24.24 31.20 27.29 14.70 14.73
Puduppully et al. (2019) 33.06 83.17 33.06 43.59 37.60 16.97 13.96

PROPOSED 39.05 94.43 35.77 52.05 42.40 19.38 16.15

Table 3: Experimental result. Each metric evaluates whether important information (CS) is described accurately
(RG) and in correct order (CO).

5 Experiments

5.1 Experimental settings

We used ROTOWIRE-MODIFIED as the dataset for
our experiments, which we explained in Section 3.
The training, development, and test data respec-
tively contained 2,714, 534, and 500 games.

Since we take a supervised training approach,
we need the annotations of the random variables
(i.e., Zt, Et, At, and Nt) in the training data, as
shown in Table 2. Instead of simple lexical match-
ing with r ∈ x, which is prone to errors in the
annotation, we use the information extraction sys-
tem provided by Wiseman et al. (2017). Although
this system is trained on noisy rule-based annota-
tions, we conjecture that it is more robust to errors
because it is trained to minimize the marginalized
loss function for ambiguous relations. All training
details are described in Appendix B.

5.2 Models to be compared

We compare our model5 against two baseline
models. One is the model used by Wiseman
et al. (2017), which generates a summary with an
attention-based encoder-decoder model. The other
baseline model is the one proposed by Puduppully
et al. (2019), which first predicts the sequence of
data records and then generates a summary condi-
tioned on the predicted sequences. Wiseman et al.
(2017)’s model refers to all data records every
timestep, while Puduppully et al. (2019)’s model
refers to a subset of all data records, which is pre-
dicted in the first stage. Unlike these models, our
model uses one memory vector hENTt that tracks
the history of the data records, during generation.
We retrained the baselines on our new dataset. We
also present the performance of the GOLD and

5Our code is available from https://github.com/
aistairc/sports-reporter

TEMPLATES summaries. The GOLD summary is
exactly identical with the reference summary and
each TEMPLATES summary is generated in the
same manner as Wiseman et al. (2017).

In the latter half of our experiments, we exam-
ine the effect of adding information about writers.
In addition to our model enhanced with writer in-
formation, we also add writer information to the
model by Puduppully et al. (2019). Their method
consists of two stages corresponding to content
planning and surface realization. Therefore, by
incorporating writer information to each of the
two stages, we can clearly see which part of the
model to which the writer information contributes
to. For Puduppully et al. (2019) model, we attach
the writer information in the following three ways:

1. concatenating writer embedding w with the in-
put vector for LSTM in the content planning
decoder (stage 1);

2. concatenating writer embedding w with the in-
put vector for LSTM in the text generator (stage
2);

3. using both 1 and 2 above.

For more details about each decoding stage, read-
ers can refer to Puduppully et al. (2019).

5.3 Evaluation metrics

As evaluation metrics, we use BLEU score (Pap-
ineni et al., 2002) and the extractive metrics pro-
posed by Wiseman et al. (2017), i.e., relation gen-
eration (RG), content selection (CS), and content
ordering (CO) as evaluation metrics. The extrac-
tive metrics measure how well the relations ex-
tracted from the generated summary match the
correct relations6:

6The model for extracting relation tuples was trained on
tuples made from the entity (e.g., team name, city name,
player name) and attribute value (e.g., “Lakers”, “92”) ex-

https://github.com/aistairc/sports-reporter
https://github.com/aistairc/sports-reporter


2108

- RG: the ratio of the correct relations out of all
the extracted relations, where correct relations
are relations found in the input data records x.
The average number of extracted relations is
also reported.

- CS: precision and recall of the relations ex-
tracted from the generated summary against
those from the reference summary.

- CO: edit distance measured with normalized
Damerau-Levenshtein Distance (DLD) between
the sequences of relations extracted from the
generated and reference summary.

6 Results and Discussions

We first focus on the quality of tracking model and
entity representation in Sections 6.1 to 6.4, where
we use the model without writer information. We
examine the effect of writer information in Sec-
tion 6.5.

6.1 Saliency tracking-based model
As shown in Table 3, our model outperforms all
baselines across all evaluation metrics.7 One of
the noticeable results is that our model achieves
slightly higher RG precision than the gold sum-
mary. Owing to the extractive evaluation nature,
the generated summary of the precision of the rela-
tion generation could beat the gold summary per-
formance. In fact, the template model achieves
100% precision of the relation generations.

The other is that only our model exceeds the
template model regarding F1 score of the con-
tent selection and obtains the highest performance
of content ordering. This imply that the tracking
model encourages to select salient input records in
the correct order.

6.2 Qualitative analysis of entity embedding
Our model has the entity embedding ē, which de-
pends on the box score for each game in addition
to static entity embedding e. Now we analyze the
difference of these two types of embeddings.

We present a two-dimensional visualizations of
both embeddings produced using PCA (Pearson,

tracted from the summaries, and the corresponding attributes
(e.g., “TEAM NAME”, “PTS”) found in the box- or line-score.
The precision and the recall of this extraction model are re-
spectively 93.4% and 75.0% in the test data.

7The scores of Puduppully et al. (2019)’s model signifi-
cantly dropped from what they reported, especially on BLEU
metric. We speculate this is mainly due to the reduced amount
of our training data (Section 3). That is, their model might be
more data-hungry than other models.

Solomon Hill

Joe Ingles
PJ Tucker

Kevin Garnett

Jimmy Butler Shabazz Muhammad

Carlos Boozer

Jonathan Gibson

Bobby Portis

Jabari Brown
James Jones

Giannis Antetokounmpo

Deron Williams

Patrick Beverley

Shavlik Randolph

Alan Williams
Jared Sullinger

Joel Freeland

Amir Johnson

Dahntay Jones

Amar'e Stoudemire

Mario Hezonja
Nene

Terrence RossLarry Drew II Tony Snell

Tyler Ulis

Cheick Diallo

Jamal Crawford

Devin HarrisGordon Hayward
Alex Stepheson

Hedo Turkoglu
C.J. Watson

Joffrey Lauvergne

Karl-Anthony Towns

Dennis Schroder
Jarell Eddie

Isaiah Whitehead

Jordan McRae

Joel Embiid

Otto Porter Jr.

Arron Afflalo

Jae Crowder

Jannero Pargo

Al Horford

Boris Diaw

Serge Ibaka

Tyreke Evans

Shaun Livingston
Andre Roberson

Dwight Howard

Alexis AjincaRoy Hibbert

Hollis Thompson

Gerald Green

Isaiah Canaan

Pascal Siakam

Michael Kidd-Gilchrist

Darrun Hilliard

Mario Chalmers

Kelly Oubre Jr.

John Henson

Shelvin Mack

Quincy Pondexter

Thomas Robinson

Brian Roberts

Kris Humphries

Jerome Jordan

Mike Miller

Alec Burks
Zach Randolph Ricky Rubio

Nikola Mirotic

Randy Foye

Danny Granger

Nikola Pekovic

Ed Davis

Francisco Garcia
Donald Sloan

Nick CollisonEvan Fournier

Nerlens Noel

Jordan Hill

Semaj Christon

Ersan Ilyasova

Jarell MartinBen Gordon

Damien Inglis

Marreese Speights

Georgios PapagiannisJon Leuer

Denzel Valentine

Clint Capela

Willie Green

Sean Kilpatrick

Chris Douglas-Roberts

DeMarcus Cousins

Shawn Marion

Trey Burke
Brook Lopez

James Harden

Kevon Looney
Malcolm BrogdonNorris Cole

Lance Thomas
Jeremy Lin

Klay Thompson

Reggie Jackson

Channing Frye
Joe Harris

Jerian GrantDion Waiters

Tiago Splitter

Anthony Morrow

Aaron Brooks

Andrew Wiggins

Jusuf Nurkic

Greg Monroe

Justise Winslow

Ty Lawson

Bryce Cotton

Dorell Wright

Tim Hardaway Jr.
Carmelo Anthony

Alex Len
DeMarre Carroll

DeMar DeRozan

Jason Smith

Kevin Love

Marco Belinelli

Bruno Caboclo

KJ McDaniels

Mike Muscala
Spencer Hawes

Marcin Gortat

Alex Kirk

Donatas Motiejunas

Jordan AdamsAndrew Harrison

Nick Calathes
Tomas Satoransky

Tyrus Thomas
Lance Stephenson

Raul NetoStanley Johnson

T.J. McConnell

Rasual Butler

Ricky LedoLuis Scola

Dario Saric

Brandon Rush
Alex AbrinesDarius Miller

Evan Turner

Doug McDermott

Chris Johnson

Steve Novak

Tarik Black

Blake Griffin

Goran Dragic

Derrick Rose

Matt Bonner

Omer Asik
Courtney Lee

Okaro White

Gerald Henderson Shawn Long

Jordan Clarkson

Mike Scott

Derrick Favors

Eric Gordon

David WestKyle O'Quinn

Dirk Nowitzki

Kyrie Irving

Kendrick Perkins

Victor Oladipo JaMychal Green

Chris Bosh

Buddy Hield

Bismack Biyombo

Cody Zeller

Russ SmithJimmer Fredette

Garrett Temple

Dante Cunningham

Ish Smith

Robert Sacre

Sam Dekker

Malcolm Delaney

Raymond Felton

Austin Daye

Wayne Ellington

Vince Carter

Kris Dunn

Andre Iguodala
Shawne Williams

Eric Bledsoe
Corey Brewer

Paul George

Alonzo Gee

Nemanja Nedovic
Andre Drummond

Reggie Evans

Dante Exum
Reggie Bullock

Bradley Beal

Kirk Hinrich
Steve Blake

Mike Dunleavy

CJ McCollumMarcus Smart

Jared Cunningham

Kevin Durant

Jorge Gutierrez
Taj Gibson

T.J. Warren
Ian Mahinmi

Drew Gooden

Nick Young

Anthony Brown

O.J. Mayo

Trevor Ariza

LeBron James

Ray McCallum

Rondae Hollis-Jefferson Skal Labissiere

Ryan Hollins

Andrea Bargnani

Allen Crabbe

Chris Kaman

Sheldon Mac

Robin Lopez

Kobe Bryant

Shane Larkin

Marcus ThorntonMatt Barnes

Chase Budinger

Troy Williams

Marvin Williams

Kemba Walker

Luc Mbah a Moute
Kenyon Martin

Cameron PayneAustin RiversJabari Parker Elliot Williams
Justin Hamilton

Justin Holiday

Ryan Anderson
Emmanuel Mudiay

Omri Casspi
Jahlil Okafor

Aaron Gordon

Ron Baker

Avery Bradley

Derrick Williams

Kyle Korver

Ronny Turiaf

Juancho HernangomezCory Jefferson

DeAndre Liggins

Greivis VasquezTobias Harris

Kyle Anderson

D.J. Augustin

Rodney McGruder

Bernard James

Andrew Goudelock

Tyler Hansbrough
Ian Clark

Festus Ezeli

Tyler Johnson

Tyler Zeller

Troy Daniels
Fred VanVleet

John Lucas III

JJ Redick

Brandon Bass

Damian Jones
Jeff Teague

Paul Millsap Lavoy Allen

Jared Dudley

Zaza Pachulia

Danny Green

Henry Walker

Chris Copeland

Miles Plumlee

Matthew Dellavedova

Cristiano FelicioIsaiah Thomas

Jerami Grant

Jarrod Uthoff

Sasha Vujacic

JR Smith

Malachi Richardson

Adreian Payne

CJ Miles

Meyers Leonard
Stephen Curry

Jose Calderon

John Wall

Elijah Millsap

Tony Allen

Brandon Davies
Harrison Barnes

JaKarr Sampson
Rajon Rondo

Dorian Finney-Smith

Brandon Jennings

Lou Williams
DeAndre Jordan

Danilo Gallinari

Kelly Olynyk
Kosta Koufos

Jeff Withey

Hassan Whiteside
Rodney StuckeyDomantas SabonisQuincy Acy

Jarrett Jack

Glen Davis

Quincy Miller

AJ Price
Khris Middleton

Gary Neal

James Young

JaVale McGee

Michael Beasley

Deyonta Davis

Russell Westbrook
Anthony Davis E'Twaun Moore

Jonas Valanciunas

Tony Wroten

Perry Jones III
Henry SimsSpencer Dinwiddie

Carl Landry

Markel Brown

Wesley Matthews

Dwyane Wade

Dewayne Dedmon

Mason Plumlee

Tim Frazier

Gorgui Dieng

Brandon Knight

Ramon Sessions

Darrell Arthur

Nicolas Batum

Iman Shumpert

Gary Harris

Yogi Ferrell

Tyler Ennis

Beno Udrih

Draymond Green

Danuel House Jr.

Al Jefferson

Cole Aldrich

Thaddeus Young

Frank Kaminsky

J.J. Barea

Nate Wolters

Marcelo Huertas

Kevin Seraphin

Manny Harris

Devyn Marble

Chris Andersen

Mitch McGary

Davis Bertans

Cory Joseph
Aron Baynes

Alexey Shved

Charlie Villanueva

Kyle Singler

Kristaps Porzingis
Damian Lillard

DeJuan BlairTyson Chandler

Joey Dorsey

Tony Parker

Johnny O'Bryant III

Willy Hernangomez

Kawhi Leonard

Jeff Ayres

Marcus Morris

Bojan Bogdanovic

Zoran Dragic
Jonathon Simmons

Salah Mejri

Nemanja Bjelica

Alan Anderson

Larry Nance Jr.

Chris McCulloughShabazz Napier

Michael Carter-Williams

Norman Powell

Timothe Luwawu-Cabarrot

LaMarcus Aldridge

Sebastian Telfair

Justin Anderson
Wesley Johnson

Furkan Aldemir

Montrezl Harrell

Brice Johnson
Tristan Thompson

Tim Duncan

Nikola Jokic

Zach LaVine

Mo Williams

Robert Covington

Terrence Jones
Jaylen Brown

Robbie Hummel
Joakim Noah

Steven Adams
Anthony Bennett

Phil Pressey

Joe Johnson

Arinze Onuaku

Coty Clarke

Chris Paul

Seth CurryMonta Ellis Dwight Buycks
Chasson Randle

Brandan Wright

Kentavious Caldwell-Pope
Maurice Harkless

Jeremy Evans

Nikola Vucevic

Ronnie Price Andre Miller JJ Hickson

Andrew Bogut

Andrew Nicholson

Julius Randle
Markieff Morris

Chandler Parsons

Timofey Mozgov

Jason Richardson

Anderson Varejao

James Ennis III

Manu Ginobili

Jodie Meeks

Ivica Zubac

Jrue Holiday

Richaun Holmes

Luis Montero

Jordan Crawford

Caris LeVert

Pierre Jackson

Nik Stauskas

Darren Collison

Wilson Chandler

Josh Huestis

Patrick Patterson

Damjan Rudez
Jakob Poeltl

Jeff Green

Pau Gasol

Rodney Hood Willie Cauley-Stein

Greg Stiemsma

Rudy Gobert

Jerryd Bayless

Kenneth Faried

Jason Thompson
Paul Zipser

Glenn Robinson III

Trevor Booker

Jonas Jerebko

Marc Gasol
Toney Douglas

Tyus Jones

Richard Jefferson

Archie Goodwin
Mirza Teletovic

Jordan Hamilton

Thabo Sefolosha

John Jenkins

Landry Fields

Brandon Ingram

Joe Young

Samuel Dalembert
Pero Antic

Lou Amundson

Jeremy Lamb
Dwight Powell

Luol Deng

Patty Mills

Anthony Tolliver

Thanasis Antetokounmpo

Rashad Vaughn

Mike Conley

Enes Kanter

Devin Booker

Elfrid Payton

Langston Galloway

D'Angelo Russell

Will Barton

Kostas Papanikolaou

Kevin Martin
Ryan Kelly

James Michael McAdoo

Kent Bazemore

Al-Farouq Aminu
Paul Pierce

Josh Smith

Josh McRoberts

James JohnsonBen McLemore Mindaugas Kuzminskas

David Lee

Rudy Gay

George Hill

Jordan Mickey

Jameer Nelson
Cleanthony Early

Myles Turner
Jason Terry

John Salmons

James Anderson

Kyle Lowry

Tayshaun Prince

Gigi DatomeLeandro Barbosa

Nate Robinson

Figure 1: Illustrations of static entity embeddings e.
Players with colored letters are listed in the ranking top
100 players for the 2016-17 NBA season at https:
//www.washingtonpost.com/graphics/
sports/nba-top-100-players-2016/.
Only LeBron James is in red and the other players in
top 100 are in blue. Top-ranked players have similar
representations of e.

1901). As shown in Figure 1, which is the vi-
sualization of static entity embedding e, the top-
ranked players are closely located.

We also present the visualizations of dynamic
entity embeddings ē in Figure 2. Although we
did not carry out feature engineering specific to
the NBA (e.g., whether a player scored double
digits or not)8 for representing the dynamic en-
tity embedding ē, the embeddings of the players
who performed well for each game have similar
representations. In addition, the change in embed-
dings of the same player was observed depending
on the box-scores for each game. For instance, Le-
Bron James recorded a double-double in a game
on April 22, 2016. For this game, his embedding
is located close to the embedding of Kevin Love,
who also scored a double-double. However, he
did not participate in the game on December 26,
2016. His embedding for this game became closer
to those of other players who also did not partici-
pate.

6.3 Duplicate ratios of extracted relations
As Puduppully et al. (2019) pointed out, a gen-
erated summary may mention the same relation
multiple times. Such duplicated relations are not
favorable in terms of the brevity of text.

Figure 3 shows the ratios of the generated sum-
maries with duplicate mentions of relations in the
development data. While the models by Wiseman
et al. (2017) and Puduppully et al. (2019) respec-

8In the NBA, a player who accumulates a double-digit
score in one of five categories (points, rebounds, assists,
steals, and blocked shots) in a game, is regarded as a good
player. If a player had a double in two of those five cate-
gories, it is referred to as double-double.

https://www.washingtonpost.com/graphics/sports/nba-top-100-players-2016/
https://www.washingtonpost.com/graphics/sports/nba-top-100-players-2016/
https://www.washingtonpost.com/graphics/sports/nba-top-100-players-2016/


2109

4 2 0 2 4
April 22, 2016

4

2

0

2

4 LeBron James

Kevin Love

Tristan Thompson
JR Smith

Kyrie Irving

Iman Shumpert

Richard Jefferson
Channing Frye

Matthew Dellavedova

Dahntay Jones

James Jones

Timofey Mozgov

Mo Williams

Tobias Harris
Marcus MorrisAndre Drummond
Kentavious Caldwell-Pope

Reggie Jackson

Anthony Tolliver

Steve Blake

Aron Baynes
Stanley Johnson

Joel Anthony

Spencer Dinwiddie

Darrun Hilliard

Jodie Meeks

4 2 0 2 4 6
December 26, 2016

Richard Jefferson

Kevin Love

Tristan Thompson
DeAndre Liggins

Kyrie Irving

Iman Shumpert
Kay Felder

Mike Dunleavy

Channing Frye
James Jones

Jordan McRae

LeBron JamesJR Smith

Jon Leuer
Marcus Morris

Andre DrummondKentavious Caldwell-Pope
Reggie Jackson

Tobias Harris

Ish Smith

Stanley JohnsonAron Baynes

Darrun HilliardHenry EllensonMichael Gbinije

Beno Udrih

Figure 2: Illustrations of dynamic entity embedding ē. Both left and right figures are for Cleveland Cavaliers vs.
Detroit Pistons, on different dates. LeBron James is in red letters. Entities with orange symbols appeared only
in the reference summary. Entities with blue symbols appeared only in the generated summary. Entities with
green symbols appeared in both the reference and the generated summary. The others are with red symbols. 2
represents player who scored in the double digits, and 3 represents player who recorded double-double. Players
with4 did not participate in the game. ◦ represents other players.

Wiseman+'17 Pudupully+'19 Proposed
0.00

0.25

0.50

0.75

1.00

64.0%

20.2%
15.8%

84.2%

12.7%

95.8%

1
2
> 2

Figure 3: Ratios of generated summaries with dupli-
cate mention of relations. Each label represents number
of duplicated relations within each document. While
Wiseman et al. (2017)’s model exhibited 36.0% dupli-
cation and Puduppully et al. (2019)’s model exhibited
15.8%, our model exhibited only 4.2%.

tively showed 36.0% and 15.8% as duplicate ra-
tios, our model exhibited 4.2%. This suggests that
our model dramatically suppressed generation of
redundant relations. We speculate that the track-
ing model successfully memorized which input
records have been selected in hENTs .

6.4 Qualitative analysis of output examples

Figure 5 shows the generated examples from val-
idation inputs with Puduppully et al. (2019)’s
model and our model. Whereas both generations
seem to be fluent, the summary of Puduppully
et al. (2019)’s model includes erroneous relations
colored in orange.

Specifically, the description about DERRICK
ROSE’s relations, “15 points, four assists, three
rounds and one steal in 33 minutes.”, is also used
for other entities (e.g., JOHN HENSON and WILLY
HERNAGOMEZ). This is because Puduppully et al.
(2019)’s model has no tracking module unlike our

model, which mitigates redundant references and
therefore rarely contains erroneous relations.

However, when complicated expressions such
as parallel structures are used our model also gen-
erates erroneous relations as illustrated by the un-
derlined sentences describing the two players who
scored the same points. For example, “11-point
efforts” is correct for COURTNEY LEE but not for
DERRICK ROSE. As a future study, it is necessary
to develop a method that can handle such compli-
cated relations.

6.5 Use of writer information

We first look at the results of an extension of
Puduppully et al. (2019)’s model with writer in-
formation w in Table 4. By adding w to con-
tent planning (stage 1), the method obtained im-
provements in CS (37.60 to 47.25), CO (16.97
to 22.16), and BLEU score (13.96 to 18.18). By
adding w to the component for surface realization
(stage 2), the method obtained an improvement in
BLEU score (13.96 to 17.81), while the effects on
the other metrics were not very significant. By
adding w to both stages, the method scored the
highest BLEU, while the other metrics were not
very different from those obtained by adding w
to stage 1. This result suggests that writer infor-
mation contributes to both content planning and
surface realization when it is properly used, and
improvements of content planning lead to much
better performance in surface realization.

Our model showed improvements in most met-
rics and showed the best performance by incor-



2110

Method
RG CS CO

BLEU
# P% P% R% F1% DLD%

Puduppully et al. (2019) 33.06 83.17 33.06 43.59 37.60 16.97 13.96
+ w in stage 1 28.43 84.75 45.00 49.73 47.25 22.16 18.18
+ w in stage 2 35.06 80.51 31.10 45.28 36.87 16.38 17.81
+ w in stage 1 & 2 28.00 82.27 44.37 48.71 46.44 22.41 18.90

PROPOSED 39.05 94.38 35.77 52.05 42.40 19.38 16.15
+ w 30.25 92.00 50.75 59.03 54.58 25.75 20.84

Table 4: Effects of writer information. w indicates that WRITER embeddings are used. Numbers in bold are the
largest among the variants of each method.

The Milwaukee Bucks defeated the New York Knicks, 105-
104, at Madison Square Garden on Wednesday evening. The
Bucks (18-16) have been one of the hottest teams in the league,
having won five of their last six games, and they have now won
six of their last eight games. The Knicks (16-19) have now
won six of their last six games, as they continue to battle for the
eighth and final playoff spot in the Eastern Conference. Giannis
Antetokounmpo led the way for Milwaukee, as he tallied 27
points, 13 rebounds, four assists, three blocked shots and one
steal, in 39 minutes . Jabari Parker added 15 points, four re-
bounds, three assists, one steal and one block, and 6-of-8 from
long range. John Henson added two points, two rebounds, one
assist, three steals and one block. John Henson was the only
other player to score in double digits for the Knicks, with 15
points, four assists, three rebounds and one steal, in 33 min-
utes. The Bucks were led by Derrick Rose, who tallied 15
points, four assists, three rebounds and one steal in 33 minutes.
Willy Hernangomez started in place of Porzingis and finished
with 15 points, four assists, three rebounds and one steal in 33
minutes. Willy Hernangomez started in place of Jose Calderon
( knee ) and responded with one rebound and one block. The
Knicks were led by their starting backcourt of Carmelo An-
thony and Carmelo Anthony, but combined for just 13 points
on 5-of-16 shooting. The Bucks next head to Philadelphia to
take on the Sixers on Friday night, while the Knicks remain
home to face the Los Angeles Clippers on Wednesday.

(a) Puduppully et al. (2019)

The Milwaukee Bucks defeated the New York Knicks, 105-104, at
Madison Square Garden on Saturday. The Bucks (18-16) checked in
to Saturday’s contest with a well, outscoring the Knicks (16-19) by
a margin of 39-19 in the first quarter. However, New York by just a
25-foot lead at the end of the first quarter, the Bucks were able to pull
away, as they outscored the Knicks by a 59-46 margin into the second.
45 points in the third quarter to seal the win for New York with the
rest of the starters to seal the win. The Knicks were led by Giannis
Antetokounmpo, who tallied a game-high 27 points, to go along with
13 rebounds, four assists, three blocks and a steal. The game was
a crucial night for the Bucks’ starting five, as the duo was the most
effective shooters, as they posted Milwaukee to go on a pair of low
low-wise (Carmelo Anthony) and Malcolm Brogdon. Anthony added
11 rebounds, seven assists and two steals to his team-high scoring total.
Jabari Parker was right behind him with 15 points, four rebounds,
three assists and a block. Greg Monroe was next with a bench-leading
18 points, along with nine rebounds, four assists and three steals.
Brogdon posted 12 points, eight assists, six rebounds and a steal.
Derrick Rose and Courtney Lee were next with a pair of {11 / 11}
-point efforts. Rose also supplied four assists and three rebounds, while
Lee complemented his scoring with three assists, a rebound and a steal.
John Henson and Mirza Teletovic were next with a pair of {two / two}
-point efforts. Teletovic also registered 13 points, and he added a re-
bound and an assist. Jason Terry supplied eight points, three rebounds
and a pair of steals. The Cavs remain in last place in the Eastern
Conference’s Atlantic Division. They now head home to face the
Toronto Raptors on Saturday night.

(b) Our model

Table 5: Example summaries generated with Puduppully et al. (2019)’s model (left) and our model (right). Names
in bold face are salient entities. Blue numbers are correct relations derived from input data records but are not
observed in reference summary. Orange numbers are incorrect relations. Green numbers are correct relations
mentioned in reference summary.

porating writer information w. As discussed in
Section 4.5, w is supposed to affect both content
planning and surface realization. Our experimen-
tal result is consistent with the discussion.

7 Conclusion

In this research, we proposed a new data-to-text
model that produces a summary text while track-
ing the salient information that imitates a human-
writing process. As a result, our model outper-
formed the existing models in all evaluation mea-
sures. We also explored the effects of incorpo-
rating writer information to data-to-text models.
With writer information, our model successfully

generated highest quality summaries that scored
20.84 points of BLEU score.

Acknowledgments

We would like to thank the anonymous reviewers
for their helpful suggestions. This paper is based
on results obtained from a project commissioned
by the New Energy and Industrial Technology De-
velopment Organization (NEDO), JST PRESTO
(Grant Number JPMJPR1655), and AIST-Tokyo
Tech Real World Big-Data Computation Open In-
novation Laboratory (RWBC-OIL).



2111

References
Tatsuya Aoki, Akira Miyazawa, Tatsuya Ishigaki, Kei-

ichi Goshima, Kasumi Aoki, Ichiro Kobayashi, Hi-
roya Takamura, and Yusuke Miyao. 2018. Gener-
ating Market Comments Referring to External Re-
sources. In Proceedings of the 11th International
Conference on Natural Language Generation, pages
135–139.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In Proceedings of the
Third International Conference on Learning Repre-
sentations.

Regina Barzilay and Mirella Lapata. 2005. Collective
content selection for concept-to-text generation. In
Proceedings of the conference on Human Language
Technology and Empirical Methods in Natural Lan-
guage Processing, pages 331–338.

Antoine Bosselut, Omer Levy, Ari Holtzman, Corin
Ennis, Dieter Fox, and Yejin Choi. 2018. Simulating
Action Dynamics with Neural Process Networks. In
Proceedings of the Sixth International Conference
on Learning Representations.

David L Chen and Raymond J Mooney. 2008. Learn-
ing to sportscast: a test of grounded language ac-
quisition. In Proceedings of the 25th international
conference on Machine learning, pages 128–135.

Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Hol-
ger Schwenk, and Yoshua Bengio. 2014. Learn-
ing Phrase Representations using RNN Encoder–
Decoder for Statistical Machine Translation. In Pro-
ceedings of the 2014 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1724–
1734.

Junyoung Chung, Caglar Gulcehre, KyungHyun Cho,
and Yoshua Bengio. 2014. Empirical evaluation of
gated recurrent neural networks on sequence model-
ing. arXiv preprint arXiv:1412.3555.

Elizabeth Clark, Yangfeng Ji, and Noah A Smith. 2018.
Neural Text Generation in Stories Using Entity Rep-
resentations as Context. In Proceedings of the 16th
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 2250–2260.

Xavier Glorot and Yoshua Bengio. 2010. Understand-
ing the difficulty of training deep feedforward neu-
ral networks. In Proceedings of the thirteenth in-
ternational conference on artificial intelligence and
statistics, pages 249–256.

Alex Graves, Greg Wayne, Malcolm Reynolds,
Tim Harley, Ivo Danihelka, Agnieszka Grabska-
Barwińska, Sergio Gómez Colmenarejo, Edward
Grefenstette, Tiago Ramalho, John Agapiou, et al.
2016. Hybrid computing using a neural net-
work with dynamic external memory. Nature,
538(7626):471.

Jiatao Gu, Zhengdong Lu, Hang Li, and Victor OK
Li. 2016. Incorporating Copying Mechanism in
Sequence-to-Sequence Learning. In Proceedings of
the 54th Annual Meeting of the Association for Com-
putational Linguistics, volume 1, pages 1631–1640.

Caglar Gulcehre, Sungjin Ahn, Ramesh Nallapati,
Bowen Zhou, and Yoshua Bengio. 2016. Pointing
the Unknown Words. In Proceedings of the 54th An-
nual Meeting of the Association for Computational
Linguistics, pages 140–149.

Luong Hoang, Sam Wiseman, and Alexander Rush.
2018. Entity Tracking Improves Cloze-style Read-
ing Comprehension. In Proceedings of the 2018
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1049–1055.

Yangfeng Ji, Chenhao Tan, Sebastian Martschat, Yejin
Choi, and Noah A Smith. 2017. Dynamic Entity
Representations in Neural Language Models. In
Proceedings of the 2017 Conference on Empirical
Methods in Natural Language Processing, pages
1830–1839.

Chloé Kiddon, Luke Zettlemoyer, and Yejin Choi.
2016. Globally coherent text generation with neural
checklist models. In Proceedings of the 2016 Con-
ference on Empirical Methods in Natural Language
Processing, pages 329–339.

Sosuke Kobayashi, Ran Tian, Naoaki Okazaki, and
Kentaro Inui. 2016. Dynamic entity representation
with max-pooling improves machine reading. In
Proceedings of the 15th Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 850–855.

Rémi Lebret, David Grangier, and Michael Auli. 2016.
Neural Text Generation from Structured Data with
Application to the Biography Domain. In Proceed-
ings of the 2016 Conference on Empirical Methods
in Natural Language Processing, pages 1203–1213.

Percy Liang, Michael I Jordan, and Dan Klein. 2009.
Learning semantic correspondences with less super-
vision. In Proceedings of the Joint Conference of
the 47th Annual Meeting of the ACL and the 4th In-
ternational Joint Conference on Natural Language
Processing of the AFNLP, pages 91–99.

Tianyu Liu, Kexiang Wang, Lei Sha, Baobao Chang,
and Zhifang Sui. 2018. Table-to-text Generation by
Structure-aware Seq2seq Learning. In Proceedings
of the Thirty-Second AAAI Conference on Artificial
Intelligence.

Thang Luong, Hieu Pham, and Christopher D Man-
ning. 2015. Effective Approaches to Attention-
based Neural Machine Translation. In Proceedings
of the 2015 Conference on Empirical Methods in
Natural Language Processing, pages 1412–1421.

http://aclweb.org/anthology/W18-6515
http://aclweb.org/anthology/W18-6515
http://aclweb.org/anthology/W18-6515
https://arxiv.org/pdf/1409.0473.pdf
https://arxiv.org/pdf/1409.0473.pdf
http://aclweb.org/anthology/H05-1042
http://aclweb.org/anthology/H05-1042
https://openreview.net/pdf?id=rJYFzMZC-
https://openreview.net/pdf?id=rJYFzMZC-
https://icml.cc/Conferences/2008/papers/304.pdf
https://icml.cc/Conferences/2008/papers/304.pdf
https://icml.cc/Conferences/2008/papers/304.pdf
https://www.aclweb.org/anthology/D14-1179
https://www.aclweb.org/anthology/D14-1179
https://www.aclweb.org/anthology/D14-1179
http://aclweb.org/anthology/N18-1204
http://aclweb.org/anthology/N18-1204
http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf
http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf
http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf
https://www.nature.com/articles/nature20101.pdf
https://www.nature.com/articles/nature20101.pdf
http://www.aclweb.org/anthology/P16-1154
http://www.aclweb.org/anthology/P16-1154
http://aclweb.org/anthology/P16-1014
http://aclweb.org/anthology/P16-1014
http://www.aclweb.org/anthology/D18-1130
http://www.aclweb.org/anthology/D18-1130
http://aclweb.org/anthology/D17-1195
http://aclweb.org/anthology/D17-1195
http://aclweb.org/anthology/D16-1032
http://aclweb.org/anthology/D16-1032
http://www.aclweb.org/anthology/N16-1099
http://www.aclweb.org/anthology/N16-1099
http://www.aclweb.org/anthology/D16-1128
http://www.aclweb.org/anthology/D16-1128
http://aclweb.org/anthology/P09-1011
http://aclweb.org/anthology/P09-1011
https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/16599/16019
https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/16599/16019
http://www.aclweb.org/anthology/D15-1166
http://www.aclweb.org/anthology/D15-1166


2112

Hongyuan Mei, Mohit Bansal, and Matthew R Walter.
2016. What to talk about and how? Selective Gener-
ation using LSTMs with Coarse-to-Fine Alignment.
In Proceedings of the 15th Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 720–730.

Soichiro Murakami, Akihiko Watanabe, Akira
Miyazawa, Keiichi Goshima, Toshihiko Yanase, Hi-
roya Takamura, and Yusuke Miyao. 2017. Learning
to generate market comments from stock prices.
In Proceedings of the 55th Annual Meeting of the
Association for Computational Linguistics, pages
1374–1384.

Graham Neubig, Chris Dyer, Yoav Goldberg, Austin
Matthews, Waleed Ammar, Antonios Anastasopou-
los, Miguel Ballesteros, David Chiang, Daniel
Clothiaux, Trevor Cohn, et al. 2017. Dynet: The
dynamic neural network toolkit. arXiv preprint
arXiv:1701.03980.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of the 40th annual meeting on association for com-
putational linguistics, pages 311–318.

Karl Pearson. 1901. On lines and planes of closest fit to
systems of points in space. The London, Edinburgh,
and Dublin Philosophical Magazine and Journal of
Science, 2(11):559–572.

Ratish Puduppully, Li Dong, and Mirella Lapata. 2019.
Data-to-Text Generation with Content Selection and
Planning. In Proceedings of the Thirty-Third AAAI
Conference on Artificial Intelligence.

Sashank J Reddi, Satyen Kale, and Sanjiv Kumar.
2018. On the convergence of adam and beyond. In
Proceedings of the Sixth International Conference
on Learning Representations.

Lei Sha, Lili Mou, Tianyu Liu, Pascal Poupart, Sujian
Li, Baobao Chang, and Zhifang Sui. 2018. Order-
planning neural text generation from structured data.
In Proceedings of the Thirty-Second AAAI Confer-
ence on Artificial Intelligence.

Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al.
2015. End-to-end memory networks. In Advances
in neural information processing systems, pages
2440–2448.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in neural information process-
ing systems, pages 3104–3112.

Kumiko Tanaka-Ishii, Kôiti Hasida, and Itsuki Noda.
1998. Reactive content selection in the generation
of real-time soccer commentary. In Proceedings of
the 36th Annual Meeting of the Association for Com-
putational Linguistics and 17th International Con-
ference on Computational Linguistics, pages 1282–
1288.

Yasufumi Taniguchi, Yukun Feng, Hiroya Takamura,
and Manabu Okumura. 2019. Generating Live
Soccer-Match Commentary from Play Data. In Pro-
ceedings of the Thirty-Third AAAI Conference on
Artificial Intelligence.

Zhaopeng Tu, Yang Liu, Zhengdong Lu, Xiaohua Liu,
and Hang Li. 2017. Context gates for neural ma-
chine translation. Transactions of the Association
for Computational Linguistics, 5:87–99.

Jason Weston, Sumit Chopra, and Antoine Bordes.
2015. Memory Networks. In Proceedings of the
Third International Conference on Learning Repre-
sentations.

Sam Wiseman, Stuart Shieber, and Alexander Rush.
2017. Challenges in Data-to-Document Generation.
In Proceedings of the 2017 Conference on Empiri-
cal Methods in Natural Language Processing, pages
2253–2263.

Zichao Yang, Phil Blunsom, Chris Dyer, and Wang
Ling. 2017. Reference-Aware Language Models.
In Proceedings of the 2017 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1850–1859.

A Algorithm

The generation process of our model is shown
in Algorithm 1. For a concise description, we
omit the condition for each probability notation.
<SOD> and <EOD> represent “start of the doc-
ument” and “end of the document”, respectively.

B Experimental settings

We set the dimensions of the embeddings to 128,
and those of the hidden state of RNN to 512 and all
of parameters are initialized with the Xavier ini-
tialization (Glorot and Bengio, 2010). We set the
maximum number of epochs to 30, and choose the
model with the highest BLEU score on the devel-
opment data. The initial learning rate is 2e-3 and
AMSGrad is also used for automatically adjusting
the learning rate (Reddi et al., 2018). Our imple-
mentation uses DyNet (Neubig et al., 2017).

http://www.aclweb.org/anthology/N16-1086
http://www.aclweb.org/anthology/N16-1086
http://aclweb.org/anthology/P17-1126
http://aclweb.org/anthology/P17-1126
http://www.aclweb.org/anthology/P02-1040
http://www.aclweb.org/anthology/P02-1040
https://www.aaai.org/Papers/AAAI/2019/AAAI-PuduppullyR.754.pdf
https://www.aaai.org/Papers/AAAI/2019/AAAI-PuduppullyR.754.pdf
https://openreview.net/pdf?id=ryQu7f-RZ
https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/16203/16095
https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/16203/16095
https://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf
http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf
http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf
http://aclweb.org/anthology/P98-2209
http://aclweb.org/anthology/P98-2209
https://www.aaai.org/Papers/AAAI/2019/AAAI-TaniguchiY.3507.pdf
https://www.aaai.org/Papers/AAAI/2019/AAAI-TaniguchiY.3507.pdf
http://www.aclweb.org/anthology/Q17-1007
http://www.aclweb.org/anthology/Q17-1007
https://arxiv.org/pdf/1410.3916.pdf
http://aclweb.org/anthology/D17-1239
https://www.aclweb.org/anthology/D17-1197


2113

Algorithm 1: Generation process
Input: Data records s,
Annotations Z1:T , E1:T , A1:T , N1:T

1 Initialize {re,a,v}r∈x, {ē}e∈E , hLM0 , hENT0
2 t← 0
3 et, yt ← NONE, < SOD >
4 while yt 6=< EOD > do
5 t← t+ 1
6 if p(Zt = 1) ≥ 0.5 then

/* Select the entity */
7 et ← arg max p(Et = e′t)
8 if et 6∈ Et−1 then

/* If et is a new entity */

9 hENT
′

t ← GRUE(ēt,hENTt−1)
10 Et ← Et−1 ∪ {et}
11 else if et 6= et−1 then

/* If et has been observed before, but is different from
the previous one. */

12 hENT’t ← GRUE(W ShENTs ,hENTt−1),
13 where s = max{s : s ≤ t− 1, e = es}
14 else
15 hENT’t ← hENTt−1

/* Select an attribute for the entity, et. */
16 at ← arg max p(At = a′t)
17 hENTt ← GRUA(ret,at,x[et,at],h

ENT′
t )

18 if at is a number attribute then
19 if p(Nt = 1) ≥ 0.5 then
20 yt ← numeral of x[et, at]
21 else
22 yt ← x[et, at]
23 end
24 else
25 yt ← x[et, at]
26 h′t ← tanh

(
WH(hLMt−1 ⊕ hENTt )

)
27 hLMt ← LSTM(yt ⊕ h′t,hLMt−1)
28 else
29 et, at,h

ENT
t ← et−1, at−1,hENTt−1

30 h′t ← tanh
(
WH(hLMt−1 ⊕ hENTt )

)
31 yt ← arg max p(Yt)
32 hLMt ← LSTM(yt ⊕ h′t,hLMt−1)
33 end
34 if yt is “.” then
35 hENTt ← GRUA(vREFRESH,hENTt )
36 end
37 return y1:t−1;


