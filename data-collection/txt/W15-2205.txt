



















































Semantic Parsing for Textual Entailment


Proceedings of the 14th International Conference on Parsing Technologies, pages 40–49,
Bilbao, Spain; July 22–24, 2015. c©2015 Association for Computational Linguistics

Semantic Parsing for Textual Entailment

Elisabeth Lien
Department of Informatics
University of Oslo, Norway
elien@ifi.uio.no

Milen Kouylekov
Department of Informatics
University of Oslo, Norway
milen@ifi.uio.no

Abstract

In this paper we gauge the utility of
general-purpose, open-domain semantic
parsing for textual entailment recognition
by combining graph-structured meaning
representations with semantic technolo-
gies and formal reasoning tools. Our ap-
proach achieves high precision, and in two
case studies we show that when reasoning
over n-best analyses from the parser the
performance of our system reaches state-
of-the-art for rule-based textual entailment
systems.

1 Background and Motivation

There is a growing interest in recent years
in general-purpose semantic parsing into graph-
based meaning representations, which provide
greater expressive power than tree-based struc-
tures. Recent efforts in this spirit include, for
example, Abstract Meaning Representation (Ba-
narescu et al., 2013), and Semantic Dependency
Parsing (SDP) (Oepen et al., 2014; Oepen et al.,
2015). Simultaneously, in the Semantic Web com-
munity, a range of generic semantic technolo-
gies for storing and processing graph-structured
data has been made available, but these have not
been much used for natural language process-
ing tasks. We propose a flexible, generic frame-
work for precision-oriented Textual Entailment
(TE) recognition that combines semantic parsing,
graph-based representations of sentence meaning,
and semantic technologies.

During the decade since the TE task was de-
fined, (logical) inference-based approaches have
made some important contributions to the field.
Systems such as Bos and Markert (2006) and Tatu
and Moldovan (2006) employ automated proof
search over logical representations of the input
sentences. Other systems, such as Bar-Haim et

al. (2007), apply transformational rules to linguis-
tic representations of the sentence pairs, and deter-
mine entailment through graph subsumption. Be-
cause inference-based systems are vulnerable to
incomplete knowledge in the rule set and errors
in the mapping from natural language sentences
to logical forms or linguistics representations, and
because the definition of the TE task encourages
a more relaxed, non-logical notion of entailment,
the majority of TE systems have used more robust
approaches, however. Our work supports a notion
of logical inference for TE by reasoning with for-
mal rules over graph-structured meaning represen-
tations, while achieving results that are compara-
ble with robust approaches.

We use a freely available, grammar-driven se-
mantic parser and a well-defined reduction of un-
derspecified logical-form meaning representations
into variable-free semantic graphs called Elemen-
tary Dependency Structures (EDS) (Oepen and
Lønning, 2006). We capitalize on a pre-existing
storage and search infrastructure for EDSs using
generic semantic technologies. For entailment
classification, we create inference rules that enrich
the EDS graphs, apply the rules with a generic rea-
soner, and use graph alignment as a decision tool.

To test our generic setup, we perform two
case studies where we replicate well-performing
TE systems, one from the Parser Evaluation us-
ing Textual Entailments (PETE) task (Yuret et
al., 2010), and one from SemEval 2014 Task 1
(Marelli et al., 2014). The best published results
for the PETE task, Lien (2014), were obtained
through heuristic rules that align meaning repre-
sentations based on structural similarity. Lien and
Kouylekov (2014) extend the same basic approach
for SemEval 2014 by including lexical relations
and negation handling. We recast the handwrit-
ten heuristic rules from these systems as formal
Semantic Web Rule Language (SWRL) rules, and
run them with a generic reasoning tool over EDS

40



meaning representations. The PETE contribution
of Lien (2014) experimented with using n-best
analyses from the parser to boost TE recall, and we
can easily include n-best reasoning in our setup.

In Sections 2 and 3, we outline our approach
and describe the semantic parsing setup and se-
mantic technologies we employ. Sections 4 and 5
detail our replication of the two TE shared tasks.
Finally, in Section 6, we sum up our effort and
point to directions for future work.

2 General-purpose Semantic Parsing

General-purpose, open-domain semantic parsing
systems that output logical-form meaning rep-
resentations are freely available today, but have
not yet been widely used in TE systems. For
our replication of the PETE and SemEval tasks,
we use the English Resource Grammar (ERG)
(Flickinger, 2000), a broad-coverage HPSG-based
parser. The ERG has been continuously devel-
oped since around 1993, and today will typi-
cally allow parsing of 90-95% of the sentences
in naturally occuring running texts of various do-
mains and genres at average parse times of a
couple of seconds per sentence. The ERG in-
cludes a Maximum Entropy parse ranking model
that is trained on some 50,000 mixed-domain sen-
tences; the parser applies exact inference, i.e.,
constructs a complete parse forest and facilitates
extraction of n-best lists of analyses in globally
optimal rank order. In our experiments, we use
the ERG in its 1212 release version, together
with its standard PET parser (Callmeier, 2002),
and off-the-shelf models and settings. The ERG
outputs underspecified meaning representations in
the Minimal Recursion Semantics (MRS) frame-
work (Copestake et al., 2005). The MRS logical-
form meaning representations can be converted
to EDSs, which are variable-free semantic de-
pendency graphs. Kouylekov and Oepen (2014)
recently showed that the Resource Description
Framework (RDF) is suitable for representing var-
ious types of semantic graphs, and demonstrated
how to embed EDS meaning representations in
RDF. We opt for EDS over MRS because its
variable-free form integrates more naturally with
RDF technologies, while still retaining the seman-
tic information essential to entailment recognition.

In the EDS example in Figure 1, each line de-
picts a graph node (each corresponding to one el-
ementary predication in the original MRS), with

node identifiers prefixed to the node labels (sep-
arated by the colon), and a set of outgoing
arcs (role-argument pairs) enclosed in parenthe-
ses. The semantic arguments to the relation rep-
resentend by the node are directed arcs to other
nodes in the EDS graph. For instance, the node
for would v modal is connected to the node
for and c through an arc labeled ARG1. The
node labeled and c in turn has outgoing arcs
to wake v up and fret v about. The two
pron nodes do not have outgoing arcs, they
are connected to the structure through incoming
arcs from the verb nodes. Finally, each of the
pronoun q nodes is connected to a pron node
through a BV (“bound variable”) arc. A graphical
visualization of the same graph is shown in Figure
3 (ignoring nodes and arcs shown in green there,
which are added by our entailment processor).

There are two notable examples of logic-based
TE systems that have used the ERG parser and
MRS meaning representations: Wotzlaw and
Coote (2013) present a TE system which com-
bines the results of deep and shallow linguis-
tic analyses into scope-resolved MRS representa-
tions. The MRS expressions are translated into
another, semantically equivalent first-order logic
format, which, enriched with background knowl-
edge, is used for the actual inference. The system
of Bergmair (2010) also uses MRS as an interme-
diate format in constructing meaning representa-
tions. Input sentences are parsed with the ERG,
and the resulting MRSs are translated into logi-
cal formulae that can be prosessed by an infer-
ence engine. In contrast to these prior applica-
tions of generic semantic parsing using the ERG
to the TE task, our work simplifies the scopally
underspecified logical forms of MRS into more
compact graph-structured representations of core
predicate–argument relations, and we define TE-
specialized notions of inference over these seman-
tic graphs.

3 Semantic Technologies and Textual
Entailment

Kouylekov and Oepen (2014) map different types
of meaning representations, including the EDSs
used in our work, to RDF graphs, stored in off-
the-shelf RDF triple stores, and searched using
SPARQL queries. In our work, we build a TE sys-
tem that utilizes their infrastructure as a basis for
reasoning over EDS graphs.

41



{e3
x5:pron

1:pronoun q(BV x5)
e3: would v modal(ARG1 e13)
e11: wake v up(ARG1 x5)
e13: and c(L-INDEX e11, R-INDEX e15, L-HNDL e11, R-HNDL e15)
e15: fret v about(ARG1 x5, ARG2 x16)
x16:pron

2:pronoun q(BV x16)
}

Figure 1: EDS for He would wake up [...] and fret about it. (PETE id 5019).

Textual Entailment was defined by Dagan et al.
(2006) as the task of recognizing whether, given
two text fragments, the meaning of one text entails
the meaning of the other text. The text fragments
are conventionally referred to as the text T and the
hypothesis H, respectively. The notion of “entail-
ment” used in TE is informal and based at least in
part on general human knowledge of language and
the world.

Our textual entailment system uses graph align-
ment over EDS structures as the basis for entail-
ment decisions. We extend the approach by en-
riching the graphs in a forward-chaining spirit us-
ing SWRL rules, and the Jena reasoner1. After the
reasoning step, the actual alignment is performed
with a SPARQL query that tries to match the hy-
pothesis graph to the text graph. Along with a clas-
sification decision, the system outputs a “proof”
by listing every SWRL rule that was used in the
reasoning. In a sense, we are following the clas-
sical reasoning approach of trying to infer the hy-
pothesis from the text.

3.1 SWRL

Our subsumption approach to entailment recogni-
tion requires some rewriting of the EDS graphs
produced by the ERG parser. For example, the
EDS graph in Figure 1 needs to be rewritten so
that dependencies are propagated into the coordi-
nate structure, which will facilitate the subsump-
tion of subgraphs. We use SWRL, a semantic web
standard for reasoning over ontologies2, to encode
rewriting rules for EDS graphs. The graph struc-
tures are enriched with a set of forward-chaining
SWRL rules, and, thus, our graph-rewriting ap-
proach can be seen as a form of forward-chaining

1https://jena.apache.org/
2http://www.w3.org/Submission/SWRL/

inference.
The system uses two sets of SWRL rules, one

for the text and one for the hypothesis graph. The
function of these rules is to further normalize and
to add information to both graphs in order to make
matching possible. We adapt the rule sets for dif-
ferent data sets to accomodate variation in entail-
ment phenomena. The rule sets contain five types
of rules:

• abstraction rules

• predicate simplification rules

• structural rules

• lexical relation rules

• polarity marking rules

Abstraction Rules We employ a number of ab-
straction rules to allow matching of indefinite and
personal pronouns in the H graph to NPs in the
T graph. To be able to match the indefinite pro-
noun somebody to the personal pronoun he in e.g.
He has a point he wants to make [...] ⇒ Some-
body wants to make a point (PETE id 1026), the
rules label both pronouns with the same abstrac-
tion label, i.e., they add an additional rdf:type
property to these nodes, which can be used in sub-
sequent testing for node equivalence.

Our rules also abstract over certain quantifiers.
In the data sets we have examined, the text and hy-
pothesis sentence of an entailment pair often have
quantifier variations that are clearly not relevant
for recognizing the entailment relationship (e.g.,
A woman is cleaning a shrimp ⇒ The woman
is cleaning a shrimp, SemEval id 3364). We
group these quantifiers into candidate equivalence
classes using rules of the form:

42



[(?a eds:predicate " a q") ->
(?a rdf:type eds:equiv quant)]

[(?a eds:predicate " the q") ->
(?a rdf:type eds:equiv quant)]

These rules state that if a node ?a is la-
beled with a certain quantifier predicate ( a q or
the q, in this specific example), then the node
?a is of type equiv quant. This fact is added
to the EDS graph, which allows matching of the
node with other nodes that have the same type.

Simplified Predicates ERG lexical predicate
symbols conjoin information about the lemma,
part-of-speech, and sense of the wordform. To in-
crease the robustness of the matching, we add a
simplified predicate symbol which contains only
the lemma and part-of-speech. This makes match-
ing possible in cases where the ERG has given
different predicate symbol interpretations of the
same word in text and hypothesis. For instance,
trade v in and trade v 1 are associated

with different usages of the verb trade, and for our
purposes can be simplified to trade v.

Structural Rules Certain rules enrich the graph
structure without adding new meaning content
to the graph. By adding arcs to certain con-
structions in the text graph, we make matching
possible for cases where the hypothesis graph
contains a substructure of the text construction.
For instance, to make matching possible for the
text He would wake up [...] and fret about it
and the hypothesis He would wake up (PETE id
5019), we need to draw additional arcs from the
node would v modal to its indirect arguments
wake v up and fret v about, i.e., the argu-

ments of the conjunction node and c. This is
done by applying the rules in Figure 2. The first
two rules label all modal verb nodes as having
type modal verb, and coordinating nodes as be-
ing of type coordination. The third rule states
that if a node is of type modal verb, and it has
an ARG1 arc to a node of type coordination,
then we add ARG1 arcs to each of the argument
nodes of the coordination. When applied to the
EDS in Figure 1, the rules yield the structure
shown in Figure 3, where the new arcs are marked
in green.

Additional rules for lexical relations and polar-
ity marking are described in Sections 3.3 and 3.4,
respectively.

Figure 3: Additional ARG1 arcs (in green) have
been added to directly connect the modal verb
(node e3) to its subarguments (nodes e11 and
e15).

Removing Graph Components To make
matching possible, we also need an additional set
of rules that remove certain predicates, nodes, and
arcs from the hypothesis before the subsumption
algorithm is applied. For instance, the sentences
A boy is playing and There is a boy playing have
the same meaning content, but receive different
analyses from the ERG, where the existential
assertion, including its tense and aspect, is reified
as a separate relation. Removing the subgraph
corresponding to there is makes matching pos-
sible. Removing graph components is not part
of the general SWRL specification, but it is an
extension to the rule language provided by the
Jena reasoner. To ensure that the bulk of our
entailment rules remain SWRL compatible, we
keep the removal rules separated.

3.2 Subsumption Algorithm

Our reasoning-based system (RBS) processes an
entailment pair using an algorithm that has the fol-
lowing steps:

• The text T and hypothesis H are analyzed
with the ERG parser.

• The EDSs for T and H are converted into
RDF triples.

• H is enriched using the SWRL rules and
converted into a SPARQL query queryh in
which the query statements are the conjunc-

43



[modal verb type: (?a eds:predicate ?p), regex(?p, "ˆ.+modal$")
-> (?a rdf:type eds:modal verb)]

[coordination type: (?a eds:predicate ?p), regex(?p, "ˆ.+ c ?.*$")
-> (?a rdf:type eds:coordination)]

[(?a rdf:type eds:modal verb), (?b rdf:type eds:coordination),
(?a eds:arg1 ?b), (?b eds:l-index ?c), (?b eds:r-index ?d)
-> (?a eds:arg1 ?c), (?a eds:arg1 ?d)]

Figure 2: SWRL rule for making explicit (inserting) arcs from a modal verb to its indirect arguments

_1:_a_q[BV x6]
x6:_man_n_1[]
e3:_walk_v_1[ARG1 x6]

select x1 x2 x3 where{
x1 eds:predicate "_a_q" .
x2 eds:predicate "_man_n_1" .
x3 eds:predicate "_walk_v_1" .
x1 eds:BV x2 .
x3 eds:ARG1 x2
}

Figure 4: Converting an EDS structure into a
SPARQL query

tion of all of the triples in the RDF represen-
tation of H.

• The RDF triples of T and the SWRL rules
for expanding T are given as the input to the
reasoner.

• If the queryh is matched into the inferred
model for T, the entailment relation is as-
signed to the pair.

The algorithm defines textual entailment as a
subsumption problem. T entails H if the (en-
riched) RDF graph that represents T contains the
entire graph of H.

Converting H into a SPARQL query allows us
to use the standard RDF technology to perform
the graph subsumption. In Figure 4, we see an
example of how the EDS for the sentence A man
walks is converted into a SPARQL query. Us-
ing SPARQL automatically computes (and makes
available) the correspondence between the predi-
cations of T and H.

Using the RDF reasoner allows us to under-
stand the reason H was subsumed by T as the Jena

reasoner outputs a verbose log on each inference
step taken to obtain a specific triple in the inferred
model. In the log output example below, the pred-
ication x5 was recognized to be an indefinite pro-
noun because it has the predicate person, and is
the target of a BV (bound variable) relation from a
predication with the predicate some q:

Added statement
[x5, indef pronoun, "true"]

Used rule
[Rule someone-body is indef pron
concluded
(x5 indef pronoun ’true’)

<-
Fact (x5 predicate ’person’)
Fact ( 1 predicate ’ some q’)
Fact ( 1 bv x5)]

3.3 Lexical Relations

In our reasoning-based system we have integrated
lexical entailment rules extracted from Word-
Net (Fellbaum, 1998) as proposed in Lien and
Kouylekov (2014). For each predication in T we
dynamically create SWRL rules that expand the
RDF graph of T by adding new predications for
words that are synonyms or hypernyms of the orig-
inal predication. For example, for the predication
assistant n we expand the T graph with the

predications worker n and person n. Figure
5 shows a simplified version of these rules.

The creation of these rules is done once be-
fore the start of the inference. The system queries
WordNet for rules that can be used until no rules
can be added. If the SWRL rules add predicates
after the reasoning step that can be expanded us-
ing rules deducted from WordNet then these rules
are added to the reasoner and the reasoning is re-
strarted. We used this strategy as we were not able
to encode the entire WordNet database as rules in

44



[to-sense-rule: (x eds:predicate "_assistant_n_1")
-> (x eds:wordnet "assistant_n_1") ]

[hypernym-rule: (x eds:wordnet "assistant_n_1")
-> (x eds:wordnet "worker_n_1")]

[hypernym-rule: (x eds:wordnet "worker_n_1")
-> (x eds:wordnet "person_n_1")]

[to-predicate-rule: (x eds:wordnet "person_n_1")
-> (x eds:predicate "_person_n_1")]

Figure 5: Automatically generated WordNet rules.

the Jena reasoner.

3.4 Contradiction
The SemEval 2014 task uses a three-way classifi-
cation of the entailment pairs. Systems were re-
quired to assign to each pair one of the three cate-
gories ENTAILMENT, CONTRADICTION, or NEU-
TRAL. To handle three-way classification, we have
developed a special rule-based contradiction mod-
ule. Although the SemEval data display various
contradiction phenomena, we focus on negation,
which is the most frequent contradiction indicator.

For classification of pairs where event negation
or instance negation in one of the sentences cre-
ates contradiction, we combine polarity marking
of nodes with graph matching. The nodes that are
in the immediate scope of the negation are marked
as negative, and all other nodes as positive. For in-
stance, in the most simple case of event negation,
the predicate neg negates some event node via an
ARG1 arc (e.g., not singing). The following rule
marks both the node of the neg predicate, and the
event node as negative:

[(?a eds:predicate "neg"),
(?a eds:arg1 ?b) ->
(?a eds:polarity "negative"),
(?b eds:polarity "negative")]

In the parallell case for simple instance negation
(e.g., no woman), the node of the no q predicate
and its “bound variable”, the instance node, are
both labeled as negative:

[(?q eds:predicate " no q"),
(?q eds:bv ?a) ->
(?q eds:polarity "negative"),
(?a eds:polarity "negative")]

Since both events and instances can be complex
linguistic constructions, our rule set contains rules
that handle negation of e.g., compounds, nom-
inalizations, coordination, and nesting of verbs.
Broadly speaking, these rules are similar in spirit
to the “MRS crawling” process defined by Packard
et al. (2014) for the task of negation scope resolu-
tion.

In the classification process, we run the sys-
tem twice on each entailment pair: in the first run
the polarity markings are ignored, and in the sec-
ond run they are considered. If the system finds
a subsumption of H in the T graph without po-
larity markings, but no subsumption with polarity
markings, then the pair is classified as CONTRA-
DICTION.

Polarity marking allows us to use the same
structures for both entailment and contradiction
testing. Our polarity marking approach is paral-
lell to how negation is represented in AMR3.

3.5 N-best Matching
The ERG parser can output a ranked list of candi-
date analyses for a sentence. We extended our sys-
tem with n-best matching to facilitate entailment
recognition when the top-ranked analysis does not
correspond to the perceived meaning of the sen-
tence, i.e., to reduce the impact of errors in parse
ranking. Such errors include prepositional phrase
attachments, noun compounds, coordinate struc-
tures, and other interpretation variants. For exam-
ple in the sentence:

who invented the light bulb?
3https://github.com/amrisi/

amr-guidelines/blob/master/amr.md#
negation

45



the parser creates two valid (in principle, if not
equally likely) analyses based on the semantic in-
terpretation of the word light as 1) an adjective; 2)
part of a noun–noun compound. If the same phrase
occurs in T and H, but their contexts are different,
the top-ranked analyses from the parser ranker for
T and H may contain different interpretations of
the phrase. Our default assumption is that such
misalignment is the cause of many unwarranted
mismatches between the T and H graphs.

For each entailment pair i (pairi) we iterate
over all analyses of T and H. If the n-th analy-
sis of T entails the k-th analysis of H we assign
the ENTAILMENT relation to the entailment pair.
This definition is valid as each analysis of T and H
corresponds to a valid interpretation.

To determine the number of analyses for T and
H we need to consider 4 we have employed an op-
timization strategy. We have gradually increased
the number of considered analyses of T and H, and
measured the system performance on the training
set. The best n-m combination, where n are the
analyses considered for T and m are the analyses
considered for H, is used on the test set.

4 First Case Study: PETE

In our first case study, we recast the Lien (2014)
heuristic for the PETE shared task data as SWRL
rules. The objective of the PETE task was to pro-
pose an alternative method for parser evaluation:
instead of comparing parser output to gold anno-
tated treebank data, parsers can be evaluated in-
directly by examining how well the parser output
supports the task of entailment recognition. The
data provided for the task was constructed so that
syntactic analysis of the sentence pairs would be
sufficient to determine whether the text entails the
hypothesis. The PETE development and test sets
contain 66 and 301 sentence pairs, respectively.
Characteristically, the hypothesis sentence of the
positive entailment pairs is shorter that the text
sentence, and is a substructure of the text, fre-
quently with some minor changes (e.g., active-to-
passive conversion, a noun phrase in the text is re-
placed by a underspecified pronoun in the hypoth-
esis). In the negative entailment pairs the hypothe-
sis usually contains elements from the text that are
structured differently and thus give the hypothesis
a different meaning from the text.

4The ERG can return all the possible grammatical analy-
ses up to a user-supplied maximum rank n.

The best scoring system in the shared task was
the Cambridge system (Rimell and Clark, 2010),
with an accuracy of 72.4%.

Table 1 presents our 1-best and 10-best results
on the PETE test data, and compares them to the
results reported by Lien (2014), and the shared
task winner Rimell and Clark (2010). Our RBS
system outperforms the system developed by Lien
(2014), establishing a new state-of-the-art. The
two systems have close results on both single anal-
ysis input and n-best. This demonstrates that our
system correctly implements the approach pro-
posed in Lien (2014).

The main advantage of our system is the high
precision. The PETE data focus on entailments
that can be recognized using structural analy-
sis alone (allowing for the substitution of noun
phrases with generalized pronouns), which fits
nicely with our strict graph subsumption algorithm
over meaning representations. When we exam-
ine the system’s output for the PETE development
data, we see that two-thirds of the true positives in
the ENTAILMENT category concern sentence pairs
where H is a substructure of T. In these cases,
enriching the RDF graphs with arcs connecting
predicates to their indirect arguments, and allow-
ing noun phrases to match generalized pronouns,
is sufficient for entailment recognition. In the re-
maining one-third of the true positives, there are
syntactic differences from T to H, but the ERG
abstracts from these differences and assigns the
same analysis to both (the relevant substring of)
T and H. For instance, the T noun phrase steamed,
whole-wheat grains and the H sentence Grains are
steamed (PETE id 3081.N) receive the same EDS
analysis, with grains as the passive ARG2 of the
verb steam. In another example below (PETE id
2004), the relative pronoun which is ignored at
the level of ERG semantics, which instead directly
identifies the stream as the ARG2 of the seeing
event:

[...] the stream which he had seen [...].
⇒ Someone had seen the stream.

In the cases where our system fails to recognize
the entailment relationship, it is often the case that
one of the sentences is assigned an incorrect analy-
sis from the ERG parser. An incorrect assignment
of an argument role, or an incorrect attachment of
a prepositional phrase prevents our strict subsump-
tion algorithm from classifying the relationship as
entailment.

46



RBS RBS n-best Lien Lien n-best Rimell & Clark
Accuracy 72.1 77.1 70.7 76.4 72.4
Precision 89.0 81.1 88.6 81.4 79.6
Recall 52.6 72.7 50.0 70.5 62.8
F-Measure 66.1 76.6 63.9 75.5 70.2

Table 1: Performance of our reasoning-based system on the PETE test data.

The influence of imperfect parse ranking on the
system performance can be alleviated by running
it on n-best parser outputs. Considering multi-
ple analyses of T and H from the ERG parser in-
creases the performance of our system by adding
a significant boost to the recall without damaging
the precision. Using 1-best analyses for T and H,
our system has a performance compatible with the
previously best performing system on the PETE
task.

5 Second Case Study: SemEval 2014
Task 1

RBS RBS n-best UIO-Lien Illinois-LH
77.4 80.4 77.1 84.6

Table 2: Comparison of accuracy of RBS on the
SemEval test data.

Precision Recall F-Measure
Contradiction 95.9 66.1 78.3
Entailment 95.6 52.4 67.7

Table 3: Precision, recall and F-measure of RBS
n-best on the SemEval test data.

In our second case study, we revisit our contri-
bution to the SemEval 2014 task 1. The focus of
this task was evaluation of compositional distribu-
tional semantic models through entailment deci-
sion (and semantic relatedness) on sentence pairs,
in order to remedy the lack of benchmarks for
such models. The 10,000 sentence pair data set re-
leased for the task (50% training, 50% test) reflects
this goal by targeting phenomena that composi-
tional distributional semantic models are meant
to account for, e.g., lexical variation phenomena
such as contextual synonymy, active-passive and
other syntactic alternation, negation, and opera-
tor scope. The data do not require encyclope-
dic knowledge about instances of concepts, only

generic semantic knowledge about general con-
cept categories. Unlike in the PETE data set, the
text and hypothesis sentences are usually similar
in length, and either paraphrase or contradict each
other, or are more or less unrelated in meaning.

In the entailment subtask, systems were re-
quired to assign one of the categories ENTAIL-
MENT, CONTRADICTION, or NEUTRAL to each
sentence pair. The best scoring system was the
Illinois-LH system (Lai and Hockenmaier, 2014),
with an accuracy of 84.6%.

Table 2 presents our 1-best and 10-best re-
sults on the SemEval test data, and compares
them to the results for the UIO-Lien system (Lien
and Kouylekov, 2014) and the shared task winner
Illinois-LH.

The results obtained on the SemEval data set are
encouraging. As with the PETE data set we have
improved over the results we achieved with the
UIO-Lien system. This demonstrates the adapt-
ability of our approach to new data sets. When
we participated in the SemEval task with the UIO-
Lien system, we did not submit a run using the n-
best analyses from the ERG parser, so we are not
able to make a comparison for n-best results. Our
current n-best RBS system obtains a high accuracy
which makes it the 6th ranked system on the Se-
mEval data. With this result it is the top ranked
unsupervised rule based system.

It is worth noticing that our system achieves
a similar result as another task participant, Best-
gen (2014), which employs a similarity-based al-
gorithm and latents semantic analysis to recognize
entailment. Our advantage versus such approaches
is that we are able to create a reasoning chain that
motivates the system decision instead of present-
ing a simple similarity number. Still in the future
development of our system we can investigate the
possibility of using probabilistic rules to guide our
reasoner.

Similar to the PETE dataset results our system
obtained a high precision (more than 95.0% pre-
cision on both ENTAILMENT and CONTRADIC-

47



TION), maintaining a decent recall as shown in Ta-
ble 3.

The SemEval data display more variation in en-
tailment phenomena than the PETE data, and re-
quire the use of external knowledge sources. We
use WordNet to generate lexical inference rules.
This allows us to capture the same types of “syn-
tactic” entailments as in the PETE data, aug-
mented with synonymy and hypernymy relations
between predicates in T and H, as examplified by
the following entailment pair (SemEval id 4176):

An eggplant is being sliced by a woman
⇒ A woman is cutting a vegetable

We did not focus on capturing entailment phe-
nomena that were aimed specifically at evaluation
of compositional distributional semantic models,
and that require contextual information or equat-
ing structurally diverse phrases. In many cases,
it would require formulating specific rules that
would do little to improve the coverage of our sys-
tem.

The system’s high precision on ENTAILMENT
shows that the graph subsumption of semantic
structures is a reliable indicator of the entailment
relation. To further improve recall, the system
must incorporate more sources of knowledge and
semantic variation.

6 Conclusions and Future Work

In this paper we have described an approach to TE
which leans heavily on generic semantic parsing
technologies, combining the off-the-shelf ERG
parser with formats and tools developed for the
Semantic Web and a custom-built notion of in-
ference over graph-structured meaning represen-
tations. We have replicated our two previous TE
shared task contributions, and using n-best anal-
yses reached state-of-the-art for rule-based TE
systems. These results demonstrate the utility
of general-purpose, off-the-shelf semantic parsing
systems for textual entailment, in particular when
reasoning over ranked n-best lists can be applied
to compensate for parse ranking limitations. Our
system architecture rests on a comparatively small
number of reasonably generic rules, i.e., there is
very little task-specific engineering and tuning in
our approach (as a large part of the work in done
in the parser). Our 95 percent precision results
demonstrate that subsumption of semantic repre-
sentations is a strong indication for textual entail-

ment. Our work contributes to moving the TE field
towards logical reasoning.

One of the main strength of the system is its ver-
satility. We reduce the amount of task-specific en-
gineering by using generic off-the-shelf tools.

Future Work Our approach is useful for
precision-critical applications like information re-
trieval and particularly Question Answering. In
future work we plan to combine it with a shallow
information retrieval approach and use its evalua-
tion power to pick the correct answer. The system
also provides a detailed account of the reasoning
behind each entailment decision. This strength can
be used in an answer presentation module which
motivates why the system has chosen a particular
answer.

References
Laura Banarescu, Claire Bonial, Shu Cai, Madalina

Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin
Knight, Philipp Koehn, Martha Palmer, and Nathan
Schneider. 2013. Abstract meaning representation
for sembanking. In Proceedings of the 7th Linguis-
tic Annotation Workshop and Interoperability with
Discourse, page 178 – 186, Sofia, Bulgaria, August.

Roy Bar-Haim, Ido Dagan, Iddo Greental, and Eyal
Shnarch. 2007. Semantic inference and the lexical-
syntactic level. In Proceedings of the National Con-
ference on Artificial Intelligence (AAAI), pages 871–
876.

Richard Bergmair. 2010. Monte Carlo Semantics: Ro-
bust Inference and Logical Pattern Processing with
Natural Language Text. Ph.D. thesis, University of
Cambridge.

Yves Bestgen. 2014. CECL: a new baseline and a
noncompositional approach for the sick benchmark.
In Proceedings of the 8th International Workshop
on Semantic Evaluation (SemEval 2014), pages 1–
8, Dublin, Ireland, August. Association for Compu-
tational Linguistics and Dublin City University.

Johan Bos and Katja Markert. 2006. When logical
inference helps determining textual entailment (and
when it doesn’t). In Bernardo Magnini and Ido Da-
gan, editors, The Second PASCAL Recognising Tex-
tual Entailment Challenge. Proceedings of the Chal-
lenges Workshop, pages 98–103, Venice, Italy.

Ulrich Callmeier. 2002. Preprocessing and encod-
ing techniques in PET. In Stephan Oepen, Daniel
Flickinger, J. Tsujii, and Hans Uszkoreit, editors,
Collaborative Language Engineering. A Case Study
in Efficient Grammar-based Processing, page 127 –
140. CSLI Publications, Stanford, CA.

48



Ann Copestake, Dan Flickinger, Carl Pollard, and
Ivan A. Sag. 2005. Minimal Recursion Semantics.
An introduction. Research on Language and Com-
putation, 3(4):281 – 332.

Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The PASCAL recognising textual entailment
challenge. In Joaquin Quiñonero-Candela, Ido Da-
gan, Bernardo Magnini, and Florence d’Alché Buc,
editors, Machine Learning Challenges. Evaluating
Predictive Uncertainty, Visual Object Classification,
and Recognising Tectual Entailment, volume 3944
of Lecture Notes in Computer Science, page 177 –
190. Springer Berlin Heidelberg.

Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Bradford Books.

Dan Flickinger. 2000. On building a more efficient
grammar by exploiting types. Natural Language
Engineering, 6 (1):15 – 28.

Milen Kouylekov and Stephan Oepen. 2014. RDF
Triple Stores and a Custom SPARQL Front-End for
Indexing and Searching (Very) Large Semantic Net-
works. In COLING 2014, 25th International Con-
ference on Computational Linguistics, Proceedings
of the Conference System Demonstrations, August
23-29, 2014, Dublin, Ireland, pages 90–94.

Alice Lai and Julia Hockenmaier. 2014. Illinois-LH: A
denotational and distributional approach to seman-
tics. In Proceedings of the 8th International Work-
shop on Semantic Evaluation (SemEval 2014), pages
329–334, Dublin, Ireland, August. Association for
Computational Linguistics and Dublin City Univer-
sity.

Elisabeth Lien and Milen Kouylekov. 2014. UIO-
Lien: Entailment recognition using minimal recur-
sion semantics. In Proceedings of the 8th Interna-
tional Workshop on Semantic Evaluation (SemEval
2014), pages 699–703, Dublin, Ireland, August. As-
sociation for Computational Linguistics and Dublin
City University.

Elisabeth Lien. 2014. Using minimal recursion seman-
tics for entailment recognition. In Proceedings of
the Student Research Workshop at the 14th Confer-
ence of the European Chapter of the Association for
Computational Linguistics, pages 76–84, Gothen-
burg, Sweden, April. Association for Computational
Linguistics.

Marco Marelli, Luisa Bentivogli, Marco Baroni, Raf-
faella Bernardi, Stefano Menini, and Roberto Zam-
parelli. 2014. Semeval-2014 task 1: Evaluation of
compositional distributional semantic models on full
sentences through semantic relatedness and textual
entailment. In Proceedings of the 8th International
Workshop on Semantic Evaluation (SemEval 2014),
pages 1–8, Dublin, Ireland, August. Association for
Computational Linguistics and Dublin City Univer-
sity.

Stephan Oepen and Jan Tore Lønning. 2006.
Discriminant-based MRS banking. In Proceed-
ings of the 5th International Conference on Lan-
guage Resources and Evaluation, page 1250 – 1255,
Genoa, Italy.

Stephan Oepen, Marco Kuhlmann, Yusuke Miyao,
Daniel Zeman, Dan Flickinger, Jan Hajič, Angelina
Ivanova, and Yi Zhang. 2014. SemEval 2014 Task
8. Broad-coverage semantic dependency parsing. In
Proceedings of the 8th International Workshop on
Semantic Evaluation, Dublin, Ireland.

Stephan Oepen, Marco Kuhlmann, Yusuke Miyao,
Daniel Zeman, Silvie Cinkova, Dan Flickinger, Jan
Hajic, and Zdenka Uresova. 2015. Semeval 2015
task 18: Broad-coverage semantic dependency pars-
ing. In Proceedings of the 9th International Work-
shop on Semantic Evaluation (SemEval 2015), pages
915–926, Denver, Colorado, June. Association for
Computational Linguistics.

Woodley Packard, Emily M. Bender, Jonathon Read,
Stephan Oepen, and Rebecca Dridan. 2014. Sim-
ple negation scope resolution through deep parsing:
A semantic solution to a semantic problem. In Pro-
ceedings of the 52nd Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), pages 69–78, Baltimore, Maryland, June.
Association for Computational Linguistics.

Laura Rimell and Stephen Clark. 2010. Cam-
bridge: Parser Evaluation using Textual Entailment
by Grammatical Relation Comparison. In Proceed-
ings of the 5th International Workshop on Semantic
Evaluation, ACL 2010.

Marta Tatu and Dan Moldovan. 2006. A logic-
based semantic approach to recognizing textual en-
tailment. In Proceedings of the COLING/ACL 2006
Main Conference Poster Sessions, pages 819–826,
Sydney, Australia, July. Association for Computa-
tional Linguistics.

Andreas Wotzlaw and Ravi Coote. 2013. A Logic-
based Approach for Recognizing Textual Entailment
Supported by Ontological Background Knowledge.
CoRR, abs/1310.4938.

Deniz Yuret, Aydin Han, and Zehra Turgut. 2010.
SemEval-2010 Task 12: Parser Evaluation using
Textual Entailments. In Proceedings of the 5th
International Workshop on Semantic Evaluation,
pages 51–56. Association for Computational Lin-
guistics.

49


