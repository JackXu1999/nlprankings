



















































Cross-Lingual Abstract Meaning Representation Parsing


Proceedings of NAACL-HLT 2018, pages 1146–1155
New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics

Cross-lingual Abstract Meaning Representation Parsing

Marco Damonte Shay B. Cohen
School of Informatics, University of Edinburgh
10 Crichton Street, Edinburgh EH8 9AB, UK

m.damonte@sms.ed.ac.uk
scohen@inf.ed.ac.uk

Abstract

Abstract Meaning Representation (AMR) re-
search has mostly focused on English. We
show that it is possible to use AMR annota-
tions for English as a semantic representation
for sentences written in other languages. We
exploit an AMR parser for English and paral-
lel corpora to learn AMR parsers for Italian,
Spanish, German and Chinese. Qualitative
analysis show that the new parsers overcome
structural differences between the languages.
We further propose a method to evaluate the
parsers that does not require gold standard data
in the target languages. This method highly
correlates with the gold standard evaluation,
obtaining a (Pearson) correlation of 0.95.

1 Introduction

Abstract Meaning Representation (AMR) parsing
is the process of converting natural language sen-
tences into their corresponding AMR representa-
tions (Banarescu et al., 2013). An AMR is a graph
with nodes representing the concepts of the sen-
tence and edges representing the semantic rela-
tions between them. Most available AMR datasets
large enough to train statistical models consist of
pairs of English sentences and AMR graphs.

The cross-lingual properties of AMR across
languages has been the subject of preliminary dis-
cussions. The AMR guidelines state that AMR
is not an interlingua (Banarescu et al., 2013) and
Bojar (2014) categorizes different kinds of diver-
gences in the annotation between English AMRs
and Czech AMRs. Xue et al. (2014) show that
structurally aligning English AMRs with Czech
and Chinese AMRs is not always possible but
that refined annotation guidelines suffice to re-
solve some of these cases. We extend this line of
research by exploring whether divergences among
languages can be overcome, i.e., we investigate

This is the sovereignty of each country

sovereignty

countrythis

each

Questa è la sovranità di ogni paese

:poss:domain

:mod

Figure 1: AMR alignments for a English sentence and
its Italian translation.

whether it is possible to maintain the AMR an-
notated for English as a semantic representation
for sentences written in other languages, as in Fig-
ure 1.

We implement AMR parsers for Italian, Span-
ish, German and Chinese using annotation pro-
jection, where existing annotations are projected
from a source language (English) to a target lan-
guage through a parallel corpus (e.g., Yarowsky
et al., 2001; Hwa et al., 2005; Padó and Lapata,
2009; Evang and Bos, 2016). By evaluating the
parsers and manually analyzing their output, we
show that the parsers are able to recover the AMR
structures even when there exist structural differ-
ences between the languages, i.e., although AMR
is not an interlingua it can act as one. This method
also provides a quick way to prototype multilin-
gual AMR parsers, assuming that Part-of-speech
(POS) taggers, Named Entity Recognition (NER)
taggers and dependency parsers are available for
the target languages. We also propose an alterna-
tive approach, where Machine Translation (MT)
is used to translate the input sentences into En-
glish so that an available English AMR parser can

1146



be employed. This method is an even quicker so-
lution which only requires translation models be-
tween the target languages and English.

Due to the lack of gold standard in the target
languages, we exploit the English data to evalu-
ate the parsers for the target languages. Hence-
forth, we will use the term target parser to indicate
a parser for a target language. We achieve this by
first learning the target parser from the gold stan-
dard English parser, and then inverting this pro-
cess to learn a new English parser from the tar-
get parser. We then evaluate the resulting English
parser against the gold standard. We call this “full-
cycle” evaluation.

Similarly to Evang and Bos (2016), we also di-
rectly evaluate the target parser on “silver” data,
obtained by parsing the English side of a parallel
corpus.

In order to assess the reliability of these evalua-
tion methods, we collected gold standard datasets
for Italian, Spanish, German and Chinese by ac-
quiring professional translations of the AMR gold
standard data to these languages. We hypothesize
that the full-cycle score can be used as a more reli-
able proxy than the silver score for evaluating the
target parser. We provide evidence to this claim by
comparing the three evaluation procedures (silver,
full-cycle, and gold) across languages and parsers.

Our main contributions are:

• We provide evidence that AMR annotations
can be successfully shared across languages.

• We propose two ways to rapidly implement
non-English AMR parsers.

• We propose a novel method to evaluate non-
English AMR parsers when gold annotations
in the target languages are missing. This
method highly correlates with gold standard
evaluation, obtaining a Pearson correlation
coefficient of 0.95.

• We release human translations of an AMR
dataset (LDC2015E86) to Italian, Spanish,
German and Chinese.

2 Cross-lingual AMR parsing

AMR is a semantic representation heavily bi-
ased towards English, where labels for nodes and
edges are either English words or Propbank frames
(Kingsbury and Palmer, 2002). The goal of AMR
is to abstract away from the syntactic realization

of the original sentences while maintaining its un-
derlying meaning. As a consequence, different
phrasings of one sentence are expected to provide
identical AMR representations. This canonicaliza-
tion does not always hold across languages: two
sentences that express the same meaning in two
different languages are not guaranteed to produce
identical AMR structures (Bojar, 2014; Xue et al.,
2014). However, Xue et al. (2014) show that in
many cases the unlabeled AMRs are in fact shared
across languages. We are encouraged by this find-
ing and argue that it should be possible to develop
algorithms that account for some of these differ-
ences when they arise. We therefore introduce a
new problem, which we call cross-lingual AMR
parsing: given a sentence in any language, the goal
is to recover the AMR graph that was originally
devised for its English translation. This task is
harder than traditional AMR parsing as it requires
to recover English labels as well as to deal with
structural differences between languages, usually
referred as translation divergence. We propose
two initial solutions to this problem: by annota-
tion projection and by machine translation.

2.1 Method 1: Annotation Projection

AMR is not grounded in the input sentence, there-
fore there is no need to change the AMR anno-
tation when projecting to another language. We
think of English labels for the graph nodes as ones
from an independent language, which incidentally
looks similar to English. However, in order to
train state-of-the-art AMR parsers, we also need
to project the alignments between AMR nodes and
words in the sentence (henceforth called AMR
alignments). We use word alignments, similarly
to other annotation projection work, to project the
AMR alignments to the target languages.

Our approach depends on an underlying as-
sumption that we make: if a source word is word-
aligned to a target word and it is AMR aligned
with an AMR node, then the target word is also
aligned to that AMR node. More formally, let
S = s1 . . . s|s| be the source language sentence
and T = t1 . . . t|t| be the target language sentence;
As(·) be the AMR alignment mapping word to-
kens in S to the set of AMR nodes that are trig-
gered by it; At(·) be the same function for T ; v
be a node in the AMR graph; and finally, W (·) be
an alignment that maps a word in S to a subset of
words in T . Then, the AMR projection assump-

1147



tion is:

∀i, j, v tj ∈W (si) ∧ v ∈ As(si)⇒ v ∈ At(tj)

In the example of Figure 1, Questa is word-
aligned with This and therefore AMR-aligned with
the node this, and the same logic applies to the
other aligned words. The words is, the and of
do not generate any AMR nodes, so we ignore
their word alignments. We apply this method to
project existing AMR annotations to other lan-
guages, which are then used to train the target
parsers.

2.2 Method 2: Machine Translation

We invoke an MT system to translate the input sen-
tence into English so that we can use an available
English parser to obtain its AMR graph. Naturally,
the quality of the output graph depends on the
quality of the translations. If the automatic trans-
lation is close to the reference translation, then the
predicted AMR graph will be close to the refer-
ence AMR graph. It is therefore evident that this
method is not informative in terms of the cross-
lingual properties of AMR. However, its simplic-
ity makes it a compelling engineering solution for
parsing other languages.

2.3 Evaluation

We now turn to the problem of evaluation. Let us
assume that we trained a parser for a target lan-
guage, for example using the annotation projec-
tion method discussed in Section 2.1. In line with
rapid development of new parsers, we assume that
the only gold AMR dataset available is the one re-
leased for English.

SILVER We can generate a silver test set by
running an automatic (English) AMR parser on the
English side of a parallel corpus and use the output
AMRs as references. However, the silver test set
is affected by mistakes made by the English AMR
parser, therefore it may not be reliable.

FULL-CYCLE In order to perform the evalua-
tion on a gold test set, we propose full-cycle eval-
uation: after learning the target parser from the
English parser, we invert this process to learn a
new English parser from the target parser, in the
same way that we learned the target parser from
the English parser. The resulting English parser
is then evaluated against the (English) AMR gold
standard. We hypothesize that the score of the new

English parser can be used as a proxy to the score
of the target parser.

GOLD To show whether the evaluation methods
proposed can be used reliably, we also generated
gold test AMR datasets for four target languages
(Italian, Spanish, German and Chinese). In order
to do so, we collected professional translations for
the English sentences in the AMR test set.1 We
were then able to create pairs of human-produced
sentences with human-produced AMR graphs.

A diagram summarizing the different evaluation
stages is shown in Figure 2. In the case of MT-
based systems, the full-cycle corresponds to first
translating from English to the target language and
then back to English (back-translation), and only
then parsing the sentences with the English AMR
parser. At the end of this process, a noisy version
of the original sentence will be returned and its
parsed graph will be a noisy version of the graph
parsed from the original sentence.

3 Experiments

We run experiments on four languages: Italian,
Spanish, German and Chinese. We use Europarl
(Koehn, 2005) as the parallel corpus for Italian,
Spanish and German, containing around 1.9M
sentences for each language pair. For Chinese,
we use the first 2M sentences from the United
Nations Parallel Corpus (Ziemski et al., 2016).
For each target language we extract two paral-
lel datasets of 20,000/2,000/2,000 (train/dev/test)
sentences for the two step of the annotation pro-
jection (English → target and target → English).
These are used to train the AMR parsers. The pro-
jection approach also requires training the word
alignments, for which we use all the remaining
sentences from the parallel corpora (Europarl for
Spanish/German/Italian and UN Parallel Corpus
for Chinese). These are also the sentences we use
to train the MT models. The gold AMR dataset
is LDC2015E86, containing 16,833 training sen-
tences, 1,368 development sentences, and 1,371
testing sentences.

Word alignments were generated using
fast align (Dyer et al., 2013), while AMR align-
ments were generated with JAMR (Flanigan et al.,
2014). AMREager (Damonte et al., 2017) was
chosen as the pre-existing English AMR parser.
AMREager is an open-source AMR parser that

1These datasets are currently available upon request from
the authors.

1148



Gold e Silver f Gold f

Parser e Parser f

Parser eFULL-CYCLE

Ref

Eval

SILVER

Ref
Eval

GOLD

Ref

Eval

Figure 2: Description of SILVER, FULL-CYCLE and GOLD evaluations. e stands for English and f stands for
the target (foreign) language. Dashed lines represent the process of transferring learning across languages (e.g.
with annotation projection). SILVER uses a parsed parallel corpus as reference (“Ref”), FULL-CYCLE uses the
English gold standard (Gold e) and GOLD uses the target language gold standard we collected (Silver f ).

needs only minor modifications for re-use with
other languages.2 It requires tokenization, POS
tagging, NER tagging and dependency parsing,
which for English, German and Chinese are
provided by CoreNLP (Manning et al., 2014). We
use Freeling (Carreras et al., 2004) for Spanish, as
CoreNLP does not provide dependency parsing
for this language. Italian is not supported in
CoreNLP: we use Tint (Aprosio and Moretti,
2016), a CoreNLP-compatible NLP pipeline for
Italian.

In order to experiment with the approach of Sec-
tion 2.2, we experimented with translations from
Google Translate.3 As Google Translate has ac-
cess to a much larger training corpus, we also
trained baseline MT models using Moses (Koehn
et al., 2007) and Nematus (Sennrich et al., 2017),
with the same training data we use for the projec-
tion method and default hyper-parameters.

Smatch (Cai and Knight, 2013) is used to eval-
uate AMR parsers. It looks for the best align-
ment between the predicted AMR and the refer-
ence AMR and it then computes precision, re-
call and F1 of their edges. The original English
parser achieves 65% Smatch score on the test split
of LDC2015E86. Full-cycle and gold evaluations
use the same dataset, while silver evaluation is
performed on the split of the parallel corpora we
reserved for testing. Results are shown in Ta-
ble 1. The Google Translate system outperforms
all other systems, but is not directly comparable
to them, as it has the unfair advantage of being

2The multilingual adaptation of AMREager is avail-
able at http://www.github.com/mdtux89/
amr-eager-multilingual. A demo is available at
http://cohort.inf.ed.ac.uk/amreager.html.

3https://translate.google.com/toolkit.

System Silver Gold Cycle

IT

Projection 45 43 45
Moses 51 52 51
Nematus 49 43 41
GT 52 58 59

ES

Projection 44 42 44
Moses 53 53 51
Nematus 51 43 42
GT 56 60 60

DE

Projection 45 39 43
Moses 50 49 49
Nematus 47 38 39
GT 54 57 59

ZH

Projection 45 35 32
Moses 57 42 48
Nematus 57 39 40
GT 64 50 55

Table 1: Silver, gold and full-cycle Smatch scores for
projection-based and MT-based systems.

trained on a much larger dataset. Due to noisy
JAMR alignments and silver training data involved
in the annotation projection approach, the MT-
based systems give in general better parsing re-
sults. The BLEU scores of all translation systems
are shown in Table 2.

There are several sources of noise in the anno-
tation projection method, which affect the parsing
results: 1) the parsers are trained on silver data
obtained by an automatic parser for English; 2)
the projection uses noisy word alignments; 3) the
AMR alignments on the source side are also noisy;
4) translation divergences exist between the lan-
guages, making it sometimes difficult to project
the annotation without loss of information.

1149



Model Moses Nematus GT
EN-IT 23.83 21.27 61.31
IT-EN 23.74 19.77 42.20
EN-ES 29.00 26.14 78.14
ES-EN 27.66 21.63 50.78
EN-DE 15.47 15.74 63.48
DE-EN 21.50 14.96 41.78
EN-ZH 9.19 8.67 26.75
ZH-EN 10.81 10.37 22.21

Table 2: BLEU scores for Moses, Nematus and Google
Translate (GT) on the (out-of-domain) LDC2015E86
test set

4 Qualitative Analysis

Figure 3 shows examples of output parses4 for
all languages, including the AMR alignments by-
product of the parsing process, that we use to dis-
cuss the mistakes made by the parsers.

In the Italian example, the only evident error
is that Infine (Lastly) should be ignored. In the
Spanish example, the word medida (measure) is
wrongly ignored: it should be used to generate a
child of the node impact-01. Some of the :ARG
roles are also not correct. In the German exam-
ple, meines (my) should reflect the fact that the
speaker is talking about his own country. Finally,
in the Chinese example, there are several mistakes
including yet another concept identification mis-
take: intend-01 is erroneously triggered.

Most mistakes involve concept identification. In
particular, relevant words are often erroneously ig-
nored by the parser. This is directly related to
the problem of noisy word alignments in annota-
tion projection: the parser learns what words are
likely to trigger a node (or a set of nodes) in the
AMR by looking at their AMR alignments (which
are induced by the word alignments). If an im-
portant word consistently remains unaligned, the
parser will erroneously learn to discard it. More
accurate alignments are therefore crucial in order
to achieve better parsing results. We computed the
percentage of words in the training data that are
learned to be non-content-bearing in each parser
and we found that the Chinese parser, which is our
least accurate parser, is the one that most suffer
from this, with 33% non-content-bearing words.
On the other hand, in the German parser, which
is the highest scoring, only 26% of the words are

4In this section, all parsed graphs were generated with the
projection-based system of Section 2.1.

non-content-bearing, which is the lowest percent-
age amongst all parsers.

4.1 Translational Divergence

In order to investigate the hypothesis that AMR
can be shared across these languages, we now
look at translational divergence and discuss how
it affects parsing, following the classification used
in previous work (Dorr et al., 2002; Dorr, 1994),
which identifies classes of divergences for several
languages. Sulem et al. (2015) also follow the
same categorization for French.

Figure 4 shows six sentences displaying these
divergences. The aim of this analysis is to as-
sess how the parsers deal with the different kind of
translational divergences, regardless of the overall
quality of the output.

Categorical. This divergence happens when two
languages use different POS tags to express the
same meaning. For example, the English sentence
I am jealous of you is translated into Spanish as
Tengo envidia de ti (I have jealousy of you). The
English adjective jealous is translated in the Span-
ish noun envidia. In Figure 4a we note that the cat-
egorical divergence does not create problems since
the parsers correctly recognized that envidia (jeal-
ousy/envy) should be used as the predicate, regard-
less of its POS.

Conflational. This divergence happens when
verbs expressed in a language with a single word
can be expressed with more words in another lan-
guage. Two subtypes are distinguished: manner
and light verb. Manner refers to a manner verb
that is mapped to a motion verb plus a manner-
bearing word. For example, We will answer is
translated in the Italian sentence Noi daremo una
riposta (We will give an answer), where to answer
is translated as daremo una risposta (will give an
answer). Figure 4b shows that the Italian parser
generates a sensible output for this sentence by
creating a single node labeled answer-01 for the
expression dare una riposta.

In a light verb conflational divergence, a verb is
mapped to a light verb plus an additional mean-
ing unit, such as when I fear is translated as Io ho
paura (I have fear) in Italian: to fear is mapped to
the light verb ho (have) plus the noun paura (fear).
Figure 4e shows that also this divergence is dealt
properly by the Italian parser: ho paura correctly
triggers the root fear-01.

1150



adopt-01

date-entity

commission

communicate-01

1998final another

Infine,
Lastly,

nel
in

1998,
1998,

la
the

Commissione
Commission

ha
have

adottato
adopted

un’altra
another

comunicazione
communication

Lastly, in 1998, the Commission adopted a further communication

:ARG0

:time :ARG1

:year:mod :mod:ARG0

assess-01

impact-01

type export-01

this cocoa

Una
One

evaluación
evaluation

de
of

el
the

impacto
impact

de
of

una
a

medida
measure

de
of

este
this

tipo
type

sobre
on

las
the

exportaciones
exports

de
of

cacao
cocoa

A study into the impact of such a measure on the export of cocoa

:ARG1

:ARG1 :ARG1

:mod :ARG1
:ARG0

do-02

country
state

-

many

have-org-role-91

member

Viele
Lots

Mitgliedsländer,
Member State,

inklusive
included

meines
my

eigenen
own

Landes,
country,

haben
have

dies
this

nicht
not

getan
done

Many Member States, including my own, did not do as required

:location :polarity
:ARG0

:ARG0-of

:quant

:ARG2

and

intend-01 societyrepatriate-01

refugee

:op2 :op2:op1

:ARG1

nán mı́n
refugee

dı́
of

qiǎn fǎn
repatriation

hé
and

zhòng xīn
re-

róng rú
assimilate into

shè huı̀
society

Repatriation and reintegration of Malian refugees

Figure 3: Parsed AMR graph and alignments (dashed lines) for an Italian sentence, a Spanish sentence, a German
sentences and a Chinese sentence.

1151



Structural. This divergence happens when verb
arguments result in different syntactic configura-
tions, for example, due to an additional PP attach-
ment. When translating He entered the house with
Lui è entrato nella casa (He entered in the house),
the Italian translation has an additional in prepo-
sition. Also this parsed graph, in Figure 4c, is
structurally correct. The missing node he is due
to pronoun-dropping, which is frequent in Italian.

Head swapping. This divergence occurs when
the direction of the dependency between two
words is inverted. For example, I like eating,
where like is head of eating, becomes Ich esse gern
(I eat likingly) in German, where the dependency
is inverted. Unlike all other examples, in this case,
the German parser does not cope well with this di-
vergence: it is unable to recognize like-01 as the
main concept in the sentence, as shown in Fig-
ure 4d.

Thematic. Finally, the parse of Figure 4f has
to deal with a thematic divergence, which hap-
pens when the semantic roles of a predicate are
inverted. In the sentence I like grapes, translated
to Spanish as Me gustan uvas, I is the subject in
English while Me is the object in Spanish. Even
though we note an erroneous reentrant edge be-
tween grape and I, the thematic divergence does
not create problems: the parser correctly recog-
nizes the :ARG0 relationship between like-01 and
I and the :ARG1 relationship between like-01 and
grape. In this case, the edge labels are important,
as this type of divergence is concerned with the
semantic roles.

5 Discussion

Can AMR be shared across these languages?
As mentioned in Section 2.2, the MT-based sys-
tems are not helpful in answering this question and
we instead focus on the projection-based parsers.
Qualitative analysis showed that the parsers are
able to overcome translational divergence and that
concept identification must be more accurate in
order to provide good parsing results. We there-
fore argue that the suboptimal performance of the
parsers in terms of Smatch scores is due to the
many sources of noise in the annotation projection
approach rather than instability of AMR across
languages. We provide strong evidence that cross-
lingual AMR parsing is indeed feasible and hope
that the release of the gold standard test sets will

motivate further work in this direction.

Are silver and full-cycle evaluations reliable?
We computed the Pearson correlation coefficients
for the Smatch scores of Table 1 to determine how
well silver and full-cycle correlate with gold eval-
uation. Full-cycle correlates better than silver: the
Pearson coefficient is 0.95 for full-cycle and 0.47
for silver. Figure 5 shows linear regression lines.
Unlike silver, full-cycle uses the same dataset as
gold evaluation and it does not contain parsing
mistakes, which makes it more reliable than silver.
Interestingly, if we ignore the scores obtained for
Chinese, the correlation between silver and gold
dramatically increases, perhaps indicating that Eu-
roparl is more suitable than the UN corpus for
this task: the Pearson coefficient becomes 0.97 for
full-cycle and 0.87 for silver. A good proxy for
gold evaluation should rank different systems sim-
ilarly. We hence computed the Kendall-tau score
(Kendall, 1945), a measure for similarity between
permutations, of the rankings extracted from Table
1. The results further confirm that full-cycle ap-
proximate gold better than silver does: the score is
0.40 for silver and 0.82 for full-cycle. Full cycle
introduces additional noise but it is not as expen-
sive as gold and is more reliable than silver.

6 Related Work

AMR parsing for languages other than English has
made only a few steps forward. In previous work
(Li et al., 2016; Xue et al., 2014; Bojar, 2014),
nodes of the target graph were labeled with ei-
ther English words or with words in the target lan-
guage. We instead use the AMR annotation used
for English for the target language as well, with-
out translating any word. To the best of our knowl-
edge, the only previous work that attempts to auto-
matically parse AMR graphs for non-English sen-
tences is by Vanderwende et al. (2015). Sentences
in several languages (French, German, Spanish
and Japanese) are parsed into a logical represen-
tation, which is then converted to AMR using a
small set of rules. A comparison with this work is
difficult, as the authors do not report results for the
parsers (due to the lack of an annotated corpus) or
release their code.

Besides AMR, other semantic parsing frame-
works for non-English languages have been inves-
tigated (Hoffman, 1992; Cinková et al., 2009; Ges-
mundo et al., 2009; Evang and Bos, 2016). Evang
and Bos (2016) is the most closely related to our

1152



envy

I

:domain

(a) ES: Tengo envidia de ti

(I am jealous of you)

answer-01

we

:ARG0

(b) IT: Noi daremo una risposta

(We will answer)

enter-01

home

:ARG1

(c) IT: Lui è entrato nella casa

(He entered the house)

eat-01

I

:ARG0

(d) DE: Ich esse gern

(I like eating)

fear-01

I I

:ARG0 :ARG0

(e) IT: Io ho paura

(I fear)

like-01

I grape

:ARG1:ARG0

:ARG0

(f) ES: Me gustan uvas

(I like grapes)

Figure 4: Parsing examples in several languages involving common translational divergence phenomena: (a) con-
tains a categorical divergence, (b) and (e) conflational divergences, (c) a structural divergence, (d) an head swapping
and (f) a thematic divergence.

40 50 60 70

40

50

60

70

Gold Smatch

Si
lv

er
Sm

at
ch

40 50 60 70

40

50

60

70

Gold Smatch

Fu
ll-

cy
cl

e
Sm

at
ch

Figure 5: Linear regression lines for silver and full-
cycle.

work as it uses a projection mechanism similar to
ours for CCG. A crucial difference is that, in order
to project CCG parse trees to the target languages,
they only make use of literal translation. Previ-
ous work has also focused on assessing the stabil-

ity across languages of semantic frameworks such
as AMR (Xue et al., 2014; Bojar, 2014), UCCA
(Sulem et al., 2015) and Propbank (Van der Plas
et al., 2010).

Cross-lingual techniques can cope with the lack
of labeled data on languages when this data is
available in at least one language, usually English.
The annotation projection method, which we fol-
low in this work, is one way to address this prob-
lem. It was introduced for POS tagging, base noun
phrase bracketing, NER tagging, and inflectional
morphological analysis (Yarowsky et al., 2001)
but it has also been used for dependency parsing
(Hwa et al., 2005), role labeling (Padó and Lap-
ata, 2009; Akbik et al., 2015) and semantic parsing
(Evang and Bos, 2016). Another common thread
of cross-lingual work is model transfer, where
parameters are shared across languages (Zeman
and Resnik, 2008; Cohen and Smith, 2009; Co-
hen et al., 2011; McDonald et al., 2011; Søgaard,
2011).

7 Conclusions

We introduced the problem of parsing AMR struc-
tures, annotated for English, from sentences writ-
ten in other languages as a way to test the cross-
lingual properties of AMR. We provided evidence
that AMR can be indeed shared across the lan-

1153



guages tested and that it is possible to overcome
translational divergences. We further proposed
a novel way to evaluate the target parsers that
does not require manual annotations of the tar-
get language. The full-cycle procedure is not lim-
ited to AMR parsing and could be used for other
cross-lingual problems in NLP. The results of the
projection-based AMR parsers indicate that there
is a vast room for improvements, especially in
terms of generating better alignments. We encour-
age further work in this direction by releasing pro-
fessional translations of the AMR test set into four
languages.

Acknowledgments

The authors would like to thank the three anony-
mous reviewers and Sameer Bansal, Gozde Gul
Sahin, Sorcha Gilroy, Ida Szubert, Esma Balkır,
Nikos Papasarantopoulos, Joana Ribeiro, Shashi
Narayan, Toms Bergmanis, Clara Vania, Yang
Liu and Adam Lopez for their helpful comments.
This research was supported by a grant from
Bloomberg and by the H2020 project SUMMA,
under grant agreement 688139.

References
Alan Akbik, Laura Chiticariu, Marina Danilevsky,

Yunyao Li, Shivakumar Vaithyanathan, and Huaiyu
Zhu. 2015. Generating high quality proposition
banks for multilingual semantic role labeling. In
Proceedings of ACL.

Alessio Palmero Aprosio and Giovanni Moretti. 2016.
Italy goes to stanford: a collection of corenlp mod-
ules for italian. arXiv preprint arXiv:1609.06204 .

Laura Banarescu, Claire Bonial, Shu Cai, Madalina
Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin
Knight, Philipp Koehn, Martha Palmer, and Nathan
Schneider. 2013. Abstract meaning representation
for sembanking. Linguistic Annotation Workshop .

Zdenka Urešová Jan Hajic Ondrej Bojar. 2014. Com-
paring czech and english amrs. In Workshop on Lex-
ical and Grammatical Resources for Language Pro-
cessing.

Shu Cai and Kevin Knight. 2013. Smatch: an evalua-
tion metric for semantic feature structures. Proceed-
ings of ACL .

Xavier Carreras, Isaac Chao, Lluı́s Padró, and Muntsa
Padró. 2004. Freeling: An open-source suite of lan-
guage analyzers. In Proceedings of LREC.

Silvie Cinková, Josef Toman, Jan Hajic, Kristỳna
Cermáková, Václav Klimeš, Lucie Mladová,

Jana Šindlerová, Kristỳna Tomšu, and Zdenek
Zabokrtskỳ. 2009. Tectogrammatical annota-
tion of the wall street. The Prague Bulletin of
Mathematical Linguistics .

Shay B. Cohen, Dipanjan Das, and Noah A. Smith.
2011. Unsupervised structure prediction with non-
parallel multilingual guidance. In Proceedings of
EMNLP.

Shay B Cohen and Noah A Smith. 2009. Shared logis-
tic normal distributions for soft parameter tying in
unsupervised grammar induction. In Proceedings of
NAACL-HLT .

Marco Damonte, Shay B Cohen, and Giorgio Satta.
2017. An incremental parser for abstract meaning
representation. In Proceedings of EACL.

Bonnie J Dorr. 1994. Machine translation divergences:
A formal description and proposed solution. Com-
putational Linguistics 20(4):597–633.

Bonnie J Dorr, Lisa Pearl, Rebecca Hwa, and Nizar
Habash. 2002. Improved word-level alignment: In-
jecting knowledge about mt divergences. Technical
report, DTIC Document.

Chris Dyer, Victor Chahuneau, and Noah A Smith.
2013. A simple, fast, and effective reparameteri-
zation of ibm model 2. In Proceedings of NAACL-
HLT .

Kilian Evang and Johan Bos. 2016. Cross-lingual
learning of an open-domain semantic parser. In Pro-
ceedings of COLING.

Jeffrey Flanigan, Sam Thomson, Jaime G Carbonell,
Chris Dyer, and Noah A Smith. 2014. A discrim-
inative graph-based parser for the abstract meaning
representation. Proceedings of ACL .

Andrea Gesmundo, James Henderson, Paola Merlo,
and Ivan Titov. 2009. A latent variable model of
synchronous syntactic-semantic parsing for multiple
languages. In Proceedings of CoNLL.

Beryl Hoffman. 1992. A ccg approach to free word
order languages. In Proceedings of ACL.

Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Natural language engineering 11(03):311–325.

Maurice G Kendall. 1945. The treatment of ties in
ranking problems. Biometrika 33(3):239–251.

Paul Kingsbury and Martha Palmer. 2002. From tree-
bank to propbank. Proceedings of LREC .

Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT summit. vol-
ume 5, pages 79–86.

1154



Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al. 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of ACL.

Bin Li, Yuan Wen, Lijun Bu, Weiguang Qu, and Ni-
anwen Xue. 2016. Annotating the little prince with
chinese amrs. Linguistic Annotation Workshop .

Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David Mc-
Closky. 2014. The Stanford CoreNLP natural lan-
guage processing toolkit. In Proceedings of ACL.

Ryan McDonald, Slav Petrov, and Keith Hall. 2011.
Multi-source transfer of delexicalized dependency
parsers. In Proceedings of EMNLP.

Sebastian Padó and Mirella Lapata. 2009. Cross-
lingual annotation projection for semantic
roles. Journal of Artificial Intelligence Research
36(1):307–340.

Rico Sennrich, Orhan Firat, Kyunghyun Cho, Alexan-
dra Birch, Barry Haddow, Julian Hitschler, Marcin
Junczys-Dowmunt, Samuel Läubli, Antonio Valerio
Miceli Barone, Jozef Mokry, and Maria Nadejde.
2017. Nematus: a toolkit for neural machine trans-
lation. In Proceedings of EACL.

Anders Søgaard. 2011. Data point selection for cross-
language adaptation of dependency parsers. In Pro-
ceedings of ACL-HLT .

Elior Sulem, Omri Abend, and Ari Rappoport. 2015.
Conceptual annotations preserve structure across
translations: A french-english case study. In
Workshop on Semantics-Driven Statistical Machine
Translation.

Lonneke Van der Plas, Tanja Samardžić, and Paola
Merlo. 2010. Cross-lingual validity of propbank in
the manual annotation of French. In Linguistic An-
notation Workshop.

Lucy Vanderwende, Arul Menezes, and Chris Quirk.
2015. An amr parser for english, french, german,
spanish and japanese and a new amr-annotated cor-
pus. In Proceedings of NAACL-HLT .

Nianwen Xue, Ondrej Bojar, Jan Hajic, Martha Palmer,
Zdenka Uresova, and Xiuhong Zhang. 2014. Not an
interlingua, but close: Comparison of english amrs
to chinese and czech. In Proceedings of LREC.

David Yarowsky, Grace Ngai, and Richard Wicen-
towski. 2001. Inducing multilingual text analysis
tools via robust projection across aligned corpora.
In Proceedings of NAACL-HLT .

Daniel Zeman and Philip Resnik. 2008. Cross-
language parser adaptation between related lan-
guages. In Proceedings of IJCNLP.

Michal Ziemski, Marcin Junczys-Dowmunt, and Bruno
Pouliquen. 2016. The united nations parallel corpus
v1. 0. In Proceedings of LREC.

1155


