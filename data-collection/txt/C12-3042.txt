



















































Nonparametric Model for Inupiaq Word Segmentation


Proceedings of COLING 2012: Demonstration Papers, pages 337–344,
COLING 2012, Mumbai, December 2012.

Nonparametric Model for Inupiaq Morphology Tokenization

Thuy Linh N guyen1 Stephan Vogel2
(1) Carnegie Mellon University.

(2) Qatar Foundation, Qatar.
thuylinh@cs.cmu.edu, svogel@qf.org.qa

Abstract

We present how to use English translation for unsupervised word segmentation of low
resource languages. The inference uses a dynamic programming algorithm for efficient blocked
Gibbs sampling. We apply the model to Inupiaq morphology analysis and get better results than
monolingual model as well as Morfessor output.

Keywords: Nonparametric model, Gibbs sampling, morphology tokenization.

337



1 Introduction
The tasks of morphological analysis or word segmentation have relied on word-formation rules or
on available pretokenized corpora such as a Chinese word list, the Arabic or Korean treebanks, or
the Czech dependancy treebank. However, these resources are expensive to obtain and for small or
endangered languages, these resources are even not available. In recent years, unsupervised methods
have been developed to infer the segmentation from unlabeled data such using minimal description
length (Creutz and Lagus, 2007), log-linear model (Poon et al., 2009), nonparametric Bayesian
model (Goldwater et al., 2009).

1. Aņun maqpiġaaliuġaa Aiviq . the man is writing the book for Aiviq .
Aņun(man) maqpiġaa(book) liuġ(writing) aa Aiviq(Aiviq) .

2. Aņun maqpiġaaliuņitkaa Aiviq . the man is not writing the book for Aiviq .
Aņun(man) maqpiġaa(book) liu(writing) ņit(not) kaa Aiviq(Aiviq) .

Table 1: Two examples of Inupiaq text, their English translations and their morphology tokenizations
and English alignments.

A different promising approach is using information from a second language to learn the morphology
analysis of the low resource language. Look at the example of Inupiaq1 and its English translation
in Table 1. Without knowing the language, let alone its morphology, we can conjecture that
Inupiaq morpheme equivalent to English’s “not” must be a substring of “maqpiġaaliuņitkaa” and be
overlaping with“ņitk”. This derivation is not possible without English translation.

This paper presents a nonparametric model and blocked Gibbs sampling inference to automatically
derive morphology analysis of a text through its English translation. 2(Snyder and Barzilay, 2008)
also applied a Bayesian model with Dirichlet Process priors to multilingual word segmentation
task. Their prior distribution of souce word and target word alignment is defined based on the
phonetic matching between two words. The bilingual word segmentation therefore benefits only
when the source language and the target language belongs to the same family but does not achieve
the same benefit when two languages are unrelated, such as using English translation to segment
Arabic or Hebrew. We model the base distribution of target-source word alignment depends on the
cooccurance of the two words in parallel corpora, independent of the language pair, the experiment
results show the benefit of using English translation for Inupiaq morphology analysis.

Our model inherits (Nguyen et al., 2010)’s model of joint distribution of tokenized source text
and its alignment to the target language. The alignment consists of at most one-to-one mappings
between the source and target words with null alignments possible on both sides. To get samples of
the posterior distribution, we use blocked Gibbs sampling algorithm and sample the whole source
sentence and its alignment at the same time. The inference uses the dynamic programming method
to avoid explicitly considering all possible segmentations of the source sentence and their alignments
with the target sentence. Our inference technique is an extension of the forward sampling-backward
filtering presented by (Mochihashi et al., 2009) for monolingual word segmentation. Dynamic
programming algorithm has been also employed to sample PCFG parse trees (Johnson et al., 2007)
and grammar-based word segmentation (Johnson and Goldwater, 2009).

1Inupiaq is a language spoken in Northern Alaska
2 We use the term bilingual word segmentation for the problem we are working on to differentiate with monolingual word

segmentation problem of learning to segment the text without translation reference.

338



In the next section we will discuss the model. Section 3 will describe the inference in detail.
Experiments and results will be presented in section 4.

2 Model

A source sentence—s is a sequence of |—s| characters


c1, c2 . . . c|—s|


. A segmentation of—s is a sentence
s of |s| words si:


s1, . . . , s|s|


. We use the notation with bar on top—s to denote a sequence of

characters to distinguish with a sequence of segmented words s. In sentence s, the set of aligned
words is sal the set of nonaligned words is snal, each word in sal aligns to a word in the set of aligned
target words tal, the set of nonaligned target words is tnal.

In the first example Inupiaq segmentation in Table 1, the segmented s= “Aņun maqpiġaa liuġ aa
Aiviq .” the set sal = {Aņun, maqpiġaa liuġ, Aviq}, snal = {aa}, the mapping of sal and tal is {
Aņun(man), maqpiġaa(book), liuġ(writing), Aiviq(Aiviq)}, the set of unaligned target is tnal = {the,
the, for }.
The generative process of a sentence s and its alignment a consists of two steps. In the first step,
s and the alignments of each word in s are generated, each source word is either null aligned or
aligned to a target word. In the second step, the model generates null aligned target words. More
specifically, the two steps generative process of (s,a) as follows:

1. Repeatedly generate word si and its alignment:

• Generate the word si with probability p
�
si
�

.
• Mark si as not aligned with probability p

�
null | si

�
or aligned with probability�

1− p�null | si
��

.
– If si is aligned, s ∈ sal, generate its aligned target word tai ∈ tal with probability

p


tai |si


.

2. Generate the set of non-aligned target words tnal given aligned target words tal with probability
p
�
tnal |tal

�
.

We model the source word distibution p (s), null aligned source word p (null | s) and null aligned
target words p

�
tnal |tal

�
similar to the same model described in (Nguyen et al., 2010). The main

difference is in how we define the base distribution of a target word given a source word p (t |s).

2.1 Source-Target Alignment Model
The probability p (t |s) that a target word t aligns to a source word s is drawn from a Pitman-yor
process t | s ∼ PY�d,α,p0 (t |s)

�
here d and α are the input parameters, and p0 (t |s) is the base

distribution.

In word alignment model, highly co-occuring word pairs are likely to be the translation of each other,
this is the intuition behind all the statistical word alignment model. However, the standard IBM
word alignment models are not applicable to define conditional distribution of a pair of sequence of
characters as the source side and a target word. We propose source-target alignment base distribution
that captures the cooccurence of a sequence of source characters c̄ =


c1, . . . c|c̄|


and a target word

t as follows.

Given the sentence pair (s, t), let count(c̄, t)(s,t) be the number of times the pair (c̄, t) cooccur. In
Table 1, the example 1 has count(aa, the)(s,t) = 2; count(maqpiga, book)(s,t) = 1.

339



α
�
l, k, t1, t

�
=





p

null | c ll−k+1


p


c ll−k+1
 l−k∑

u=1

∑
t ′∈t1

or t ′=null

α
�
l − k, u, t1 \ t ′, t ′

�
if t = null


1− p


null | c ll−k+1


p


c ll−k+1


p


t |c ll−k+1


if t is not null
l−k∑
u=1

∑
t ′∈t1\t

or t ′=null

α
�
l − k, u, t1 \ t ′, t ′

�

Figure 1: Forward function α
�
l, k, t1, t

�
.

Given the training parallel corpora T number of times (c̄, t) cooccur in T is count(c̄, t)T =∑
(s,t)∈T count(c̄, t)(s,t).

Note that if “maqpiga” is the translation of “book” by cooccuring in sentence pairs, any substring
of the word such as “m”, “ap” also coocur with “book”, but book is not their translation candidate.
We therefore remove these counts from corpora coocurence count. Let c̄′ be a substring of c̄, if
count(c̄′, t)T = count(c̄, t)T , count(c̄′, t)T > θ1 and length


c̄′

< θ2, we remove count(c̄′, t)T

from the count, count(c̄′, t) = 0. Here θ1 and θ2 are the input thresholds to avoid removal of low
frequency words or long words. We define the base distribution for any sequence of character c̄ and
a target word t as: p0 (t | c̄) = count(c̄,t)∑

t count(c̄,t)
.

3 Inference
We use blocked Gibbs sampling algorithm to generate samples from the posterior distribution of
the model. In our experiment, the variables are potential word boundaries and sentence alignments.
We first initialize the training data with an initial segmentation and alignment. Then the sampler
iteratively removes a sentence from the data and samples the segmentation and alignment of the
sentence given the rest of training set. The new sentence is then added to the corpus. This process
continues until the samples are mixed up and represent the posterior of interest.

We apply the dynamic programming Forward filtering - Backward sampling algorithm. The Forward
filtering step calculates the marginalized probabilities of variables and the Backward sampling step
uses these probabilities to sample the variables.

3.1 Forward Filtering
We define the forward function α

�
l, k, t1, t

�
to iteratively calculate the marginalized probabilities

in step 1 of generation process. That is the probability of the substring c l1 with the final k characters
being a word aligned to t and c l−k1 aligns to the subset target words t1.

Figure 1 shows how to calculate α
�
l, k, t1, t

�
, the mathematic derivation of the function is similar

to forward function of HMM model. We will give the detail in a technical report.

3.2 Backward Sampling
The forward filtering step calculates function α

�
l, k, t1, t

�
in the order of increasing l. At the end

of the source sentence, the forward filtering step calculates the forward variables α
�|—s|, k, t1, t

�
for any k ≤ |—s|, t1 ⊂ t and t ∈ t, t /∈ t1. That is the probability that the sentence—s has the last

340



word with length k and the word aligns to t, the rest of the sentence aligns to the set t1. The set of
corresponding aligned target words is t1 ∪ {t}.
So the marginalized probability in step 1 of the generation process is

p
�
—s, tal

�
=
∑

k

∑
t ′∈t1\t

or t ′=null

α
�|—s|, k, t1 \ t ′, t ′

�

We have the marginalized probability that any tnal ⊂ t is the set of nonaligned target word:
p
�
—s, tal, tnal

�
= p
�
—s, tal

�× p�tnal |tal
�

.

In backward sampling, we first sample the set of nonaligned target words tnal proportional to the
marginalized probability p

�
—s, tal, tnal

�
.

Once we got the set of aligned target words tnal for string s, we sample the length k of the last word
of the sentence according to its marginalized probability:

∑
t∈tal

or t=null

α
�|—s|, k, tal \ t, t

�
.

then sample the alignment t of the last word c|—s||—s|−k+1 proportional to α
�|—s|, k, tal \ t, t

�
. The process

continues backward until we reach the beginning of the sentence.

4 Empirical Results
The Inupiaq-English data is from an ellicit parallel corpora which consists of 2933 parallel sentences
of average 4.34 words per Inupiaq sentence, 6 words per English sentences. On average, each
Inupiaq word has 11.35 characters. Our task is to segment Inupiaq into morphemes with average
2.31 morphemes per word. The number of morphemes per word has high variance, Inupiaq
usually has subject and oblique as separate words and the rest of the sentence as one long word.
It is a rich morphology language with relatively free reordering. For example, both sentences
“Maqpiaaliuġniaġaa Aiviq Paniattaam” and “Aiviq maqpiaaliuġniaġaa Paniattaam” have the same
translation “Paniattaaq will write a book for Aiviq”.

4.1 Experiment Setup
We report the best result of experiments with different values hyperparameter p0 (from 0.5 . . . 0.9 and
0.95, 0.99) which model the source word null alignment and the parameter represent word length
distribution. All other hyperparameters are fixed before the experiment. The MCMC inference
algorithm starts with an initial segmentation as full word form and initial monotone alignment. The
inference samples 100 iterations through the whole training set. The inference adopts simulated
annealing to speed convergence of the sampler and approximates the samples around the MAP
solution. In the last 10 iterations, the inference uses a temperature parameter τ and starts cooling
down from 1, 9

10
to 1

10
. The last iteration represents the MAP solution and is the segmentation output.

We leave the analysis of inference convergence to the future work. We report the segmentation
accuracy on F-score of:3 BF: word boundries score; F: word token score: both boundaries of a
word must be correctly identified to be counted as correct; LF: instead of reporting the word-token
accuracy as F, LF represents word-type score.

3the evaluation script is on http://homepages.inf.ed.ac.uk/sgwater/resources.html

341



One word in rich morphology language is often equivalent to several English words. But English
has morphology in the language too, an English past tense verb or plural noun are equivalent to
two morphemes. We also experiment our bilingual model with the target side is tokenized English.
The tokenized English is the result pf manually convert English past tense verbs into “verb PAST”,
plural nouns into “lematize_noun PL”, all other words are lemmatized. For example, the sentence
The men sang is converted to The man PL sing PAST.

4.2 Empirical Results

F BF LF
HDP-Monolingual 35.02 62.62 30.51
Morfessor 35.53 63.98 29.72
Bilingual 35.83 65.68 29.80
Bilingual-tokenized Eng 39.15 66.71 30.15

Table 2: Inupiaq morphology analysis results: comparing the baseline scores (upper row) and our
scores (lower row).
Table 2 presents our segmentation results. The top rows are monolingual baselines, the bottom rows
are our bilingual results using either English or tokenized English as the target language. The best
score belongs to bilingual model using tokenized English for unsupervised segmentation.

For monolingual baselines, we use (Goldwater et al., 2009)’s monolingual nonparametric word
segmentation. The HDP-Monolingual result in Table 2 is the best score from nearly 100 experiments
of their model with different hyperparameters. Unlike (Snyder and Barzilay, 2008) report that
bilingual word segmentation do not benefit when two languages are different. Our bilingual model
still has better performance than the nonparametric monolingual word segmentation on F, BF and
average scores. The informative source-target base distribution contribute to this better result.

We also use Morfessor (Creutz and Lagus, 2007), an unsupervised morphology analysis as another
monolingual baseline. Morfessor is the state-of-the-art unsupervised segmentation for complex
morphology languages such as Finish, Czech. The bilingual model using tokenized English signifi-
cantly outperforms Morfessor result. Our experiment on small Inupiaq-English data is typical for
low resource language scenario. The morphology analysis of the parallel data then can be used as
supervised data for monolingual segmentation task without given English translation.

5 Conclusion
We have presented a bilingual nonparametric word segmentation model for low resource languages.
The inference uses a dynamic programming algorithm for an efficient blocked Gibbs sampling. The
experiment shows the benefit of using translation in word segmentation task.

References
Creutz, M. and Lagus, K. (2007). Unsupervised models for morpheme segmentation and morphol-
ogy learning. ACM Trans. Speech Lang. Process., 4(1):1–34.

Goldwater, S., Griffiths, T. L., and Johnson, M. (2009). A Bayesian Framework for Word
Segmentation: Exploring the Effects of Context. Cognition, 112(1):21–54.

Johnson, M. and Goldwater, S. (2009). Improving Nonparameteric Bayesian Inference: Ex-
periments on Unsupervised Word Segmentation with Adaptor Grammars. In Proceedings of

342



NAACL-HLT ’09, pages 317–325, Stroudsburg, PA, USA. Association for Computational Linguis-
tics.

Johnson, M., Griffiths, T., and Goldwater, S. (2007). Bayesian inference for PCFGs via Markov
chain Monte Carlo. In Proceedings of NAACL-HLT ’07, pages 139–146, Rochester, New York.
Association for Computational Linguistics.

Mochihashi, D., Yamada, T., and Ueda, N. (2009). Bayesian Unsupervised Word Segmentation
with Nested Pitman-Yor Language Modeling. In Proceedings of ACL:09, pages 100–108, Suntec,
Singapore. Association for Computational Linguistics.

Nguyen, T., Vogel, S., and Smith, N. A. (2010). Nonparametric Word Segmentation for Machine
Translation. In Proceedings of Coling-10, pages 815–823, Beijing, China.

Poon, H., Cherry, C., and Toutanova, K. (2009). Unsupervised Morphological Segmentation with
Log-Linear Models. In Proceedings NAACL’09, pages 209–217.

Snyder, B. and Barzilay, R. (2008). Unsupervised multilingual learning for morphological seg-
mentation. In Proceedings of ACL-08: HLT, pages 737–745, Columbus, Ohio. Association for
Computational Linguistics.

343




