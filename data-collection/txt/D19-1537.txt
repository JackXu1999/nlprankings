



















































Editing-Based SQL Query Generation for Cross-Domain Context-Dependent Questions


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 5338–5349,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

5338

Editing-Based SQL Query Generation for
Cross-Domain Context-Dependent Questions

Rui Zhang† Tao Yu† He Yang Er†
Sungrok Shim† Eric Xue† Xi Victoria Lin¶ Tianze Shi§

Caiming Xiong¶ Richard Socher¶ Dragomir Radev†
† Yale University ¶ Salesforce Research § Cornell University
{r.zhang, tao.yu, dragomir.radev}@yale.edu
{xilin, cxiong, rsocher}@salesforce.com

Abstract

We focus on the cross-domain context-
dependent text-to-SQL generation task. Based
on the observation that adjacent natural lan-
guage questions are often linguistically de-
pendent and their corresponding SQL queries
tend to overlap, we utilize the interaction his-
tory by editing the previous predicted query
to improve the generation quality. Our edit-
ing mechanism views SQL as sequences and
reuses generation results at the token level
in a simple manner. It is flexible to change
individual tokens and robust to error prop-
agation. Furthermore, to deal with com-
plex table structures in different domains,
we employ an utterance-table encoder and a
table-aware decoder to incorporate the context
of the user utterance and the table schema.
We evaluate our approach on the SParC
dataset and demonstrate the benefit of editing
compared with the state-of-the-art baselines
which generate SQL from scratch. Our code
is available at https://github.com/
ryanzhumich/sparc_atis_pytorch.

1 Introduction

Generating SQL queries from user utterances is an
important task to help end users acquire informa-
tion from databases. In a real-world application,
users often access information in a multi-turn in-
teraction with the system by asking a sequence of
related questions. As the interaction proceeds, the
user often makes reference to the relevant men-
tions in the history or omits previously conveyed
information assuming it is known to the system.

Therefore, in the context-dependent scenario,
the contextual history is crucial to understand the
follow-up questions from users, and the system
often needs to reproduce partial sequences gener-
ated in previous turns. Recently, Suhr et al. (2018)
proposes a context-dependent text-to-SQL model

including an interaction-level encoder and an at-
tention mechanism over previous utterances. To
reuse what has been generated, they propose to
copy complete segments from the previous query.
While their model is successful to reason about ex-
plicit and implicit references, it does not need to
explicitly address different database schemas be-
cause the ATIS contains only the flight-booking
domain. Furthermore, the model is confined to
copy whole segments which are extracted by a
rule-based procedure, limiting its capacity to uti-
lize the previous query when only one or a few
tokens are changed in the segment.

To exploit the correlation between sequentially
generated queries and generalize the system to
different domains, in this paper, we study an
editing-based approach for cross-domain context-
dependent text-to-SQL generation task. We pro-
pose query generation by editing the query in the
previous turn. To this end, we first encode the
previous query as a sequence of tokens, and the
decoder computes a switch to change it at the to-
ken level. This sequence editing mechanism mod-
els token-level changes and is thus robust to error
propagation. Furthermore, to capture the user ut-
terance and the complex database schemas in dif-
ferent domains, we use an utterance-table encoder
based on BERT to jointly encode the user utter-
ance and column headers with co-attention, and
adopt a table-aware decoder to perform SQL gen-
eration with attentions over both the user utterance
and column headers.

We evaluate our model on SParC (Yu et al.,
2019b), a new large-scale dataset for cross-domain
semantic parsing in context consisting of coherent
question sequences annotated with SQL queries
over 200 databases in 138 domains. Experiment
results show that by generating from the previ-
ous query, our model delivers an improvement of
7% question match accuracy and 11% interaction

https://github.com/ryanzhumich/sparc_atis_pytorch
https://github.com/ryanzhumich/sparc_atis_pytorch


5339

Context Cross-Domain Interaction (train / dev / test) Question Turn Database Table Q. Length Q. Vocab
Spider 7 X 11,840 (8,659 / 1,034 / 2,147) 11,840 1.0 200 1020 13.4 4,818
ATIS X 7 1,658 (1,148 / 380 / 130) 11,653 7.0 1 27 10.2 1,582
SParC X X 4,298 (3,034 / 422 / 842) 12,726 3.0 200 1020 8.1 3,794

Table 1: Dataset Statistics.

WHERE AGG GROUP ORDER HAVING SET JOIN Nested
Spider 55.2 51.7 24.0 21.5 6.7 5.8 42.9 15.7
ATIS 100 16.6 0.3 0 0 0 96.6 96.6
SParC 42.8 39.8 20.1 17.0 4.7 3.5 35.5 5.7

Table 2: % of SQL queries that contain a particular SQL component.

Database: student dormitory containing 5 tables.

Goal: Find the first and last names of the students who are living in the dorms that have a TV Lounge as an amenity.

Q1: How many dorms have a TV Lounge?

S1: SELECT COUNT(*) FROM dorm AS T1 JOIN has amenity AS T2 ON T1.dormid = T2.dormid JOIN dorm amenity AS T3
ON T2.amenid = T3.amenid WHERE T3.amenity name = ‘TV Lounge’

Q2: What is the total capacity of these dorms?

S2: SELECT SUM(T1.student capacity) FROM dorm AS T1 JOIN has amenity AS T2 ON T1.dormid = T2.dormid JOIN
dorm amenity AS T3 ON T2.amenid = T3.amenid WHERE T3.amenity name = ‘TV Lounge’

Q3: How many students are living there?

S3: SELECT COUNT(*) FROM student AS T1 JOIN lives in AS T2 ON T1.stuid = T2.stuid WHERE T2.dormid
IN (SELECT T3.dormid FROM has amenity AS T3 JOIN dorm amenity AS T4 ON T3.amenid = T4.amenid

WHERE T4.amenity name = ‘TV Lounge’)

Q4: Please show their first and last names.

S4: SELECT T1.fname, T1.lname FROM student AS T1 JOIN lives in AS T2 ON T1.stuid = T2.stuid WHERE T2.dormid
IN (SELECT T3.dormid FROM has amenity AS T3 JOIN dorm amenity AS T4 ON T3.amenid = T4.amenid
WHERE T4.amenity name = ‘TV Lounge’)

Table 3: SParC example.

match accuracy over the previous state-of-the-art.
Further analysis shows that our editing approach is
more robust to error propagation than copying seg-
ments, and the improvement becomes more signif-
icant if the basic text-to-SQL generation accuracy
(without editing) improves.

2 Cross-Domain Context-Depencent
Semantic Parsing

2.1 Datasets

We use SParC 1 (Yu et al., 2019b), a large-scale
cross-domain context-dependent semantic parsing
dataset with SQL labels, as our main evaluation
benchmark. A SParC example is shown in Table
3. We also report performance on ATIS (Hemphill
et al., 1990; Dahl et al., 1994a) for direct compar-
ison to Suhr et al. (2018). In addition, we evalu-
ate the cross-domain context-independent text-to-
SQL ability of our model on Spider2 (Yu et al.,

1https://yale-lily.github.io/sparc
2https://yale-lily.github.io/spider

2018c), which SParC is built on.
We summarize and compare the data statistics

in Table 1 and Table 2. While the ATIS dataset
has been extensively studied, it is limited to a
particular domain. By contrast, SParC is both
context-dependent and cross-domain. Each inter-
action in SParC is constructed using a question in
Spider as the interaction goal, where the annota-
tor asks inter-related questions to obtain informa-
tion that completes the goal. SParC contains in-
teractions over 200 databases and it follows the
same database split as Spider where each database
appears only in one of train, dev and test sets.
In summary, SParC introduces new challenges to
context-dependent text-to-SQL because it (1) con-
tains more complex context dependencies, (2) has
greater semantic coverage, and (3) adopts a cross-
domain task setting.

2.2 Task Formulation

Let X denote a natural language utterance and Y
denote the corresponding SQL query. Context-

https://yale-lily.github.io/sparc
https://yale-lily.github.io/spider


5340

What is the customer id of the most recent customer?

What is their name?

SELECT	customer_id	FROM	customers	ORDER	BY
date_became_customer	DESC	LIMIT	1

LSTM Query Decoder

Interaction Encoder

Utterance Encoder

Utterance Encoder

SELECT	customer_name	FROM	customers	ORDER	BY
date_became_customer	DESC	LIMIT	1

LSTM Query Decoder

Attention Over Previous Utterances, Column Headers, Previous Query

Bi LSTM Query Encoder

Table Encoder

Table: customers customer_id customer_name

Figure 1: Model architecture of editing the previous query with attentions to the user utterances, the table schema,
and the previously generated query.

independent semantic parsing considers individ-
ual (X,Y ) pairs and maps X to Y . In context-
dependent semantic parsing, we consider an inter-
action I consisting of n utterance-query pairs in a
sequence:

I = [(Xi, Yi)]
n
i=1

At each turn t, the goal is to generate Yt given the
current utterance Xt and the interaction history

[(X1, Y1), (X2, Y2), . . . , (Xt−1, Yt−1)]

Furthermore, in the cross-domain setting, each
interaction is grounded to a different database.
Therefore, the model is also given the schema of
the current database as an input. We consider re-
lational databases with multiple tables, and each
table contains multiple column headers:

T = [c1, c2, . . . , cl, . . . , cm]

where m is the number of column headers, and
each cl consists of multiple words including its ta-
ble name and column name (§ 3.1).

3 Methodology

We employ an encoder-decoder architecture with
attention mechanisms (Sutskever et al., 2014; Lu-
ong et al., 2015) as illustrated in Figure 1. The
framework consists of (1) an utterance-table en-
coder to explicitly encode the user utterance and
table schema at each turn, (2) A turn attention in-
corporating the recent history for decoding, (3) a
table-aware decoder taking into account the con-
text of the utterance, the table schema, and the pre-
viously generated query to make editing decisions.

3.1 Utterance-Table Encoder

An effective encoder captures the meaning of user
utterances, the structure of table schema, and the
relationship between the two. To this end, we
build an utterance-table encoder with co-attention
between the two as illustrated in Figure 2.

Figure 2b shows the utterance encoder. For the
user utterance at each turn, we first use a bi-LSTM
to encode utterance tokens. The bi-LSTM hid-
den state is fed into a dot-product attention layer
(Luong et al., 2015) over the column header em-
beddings. For each utterance token embedding,
we get an attention weighted average of the col-
umn header embeddings to obtain the most rele-
vant columns (Dong and Lapata, 2018). We then
concatenate the bi-LSTM hidden state and the col-
umn attention vector, and use a second layer bi-
LSTM to generate the utterance token embedding
hE .

Figure 2c shows the table encoder. For each
column header, we concatenate its table name and
its column name separated by a special dot token
(i.e., table name . column name). Each
column header is processed by a bi-LSTM layer.
To better capture the internal structure of the table
schemas (e.g., foreign key), we then employ a self-
attention (Vaswani et al., 2017) among all column
headers. We then use an attention layer to cap-
ture the relationship between the utterance and the
table schema. We concatenate the self-attention
vector and the utterance attention vector, and use
a second layer bi-LSTM to generate the column
header embedding hC .

Note that the two embeddings depend on each
other due to the co-attention, and thus the column
header representation changes across different ut-
terances in a single interaction.



5341

Table 1: dorm Table 2: has Table 3: amenity

id id namename dorm_id amenity_id

foreign	key foreign	key

Column Headers

Utterance: how many dorms have a TV louge

(a) An example of user utterance and column headers.

how

Bi LSTM

many dorms have a TV louge

Attention over Table Columns

Bi LSTM

Concatenation

(b) Utterance Encoder.

Self-Attention Among Table Columns

Attention over Utterance Tokens

Bi LSTM Bi LSTM Bi LSTM Bi LSTM Bi LSTM Bi LSTM

dorm   .   id dorm   .   name has   .   dorm   id has   .   amenity   id amenity   .   id amenity   .   name

Bi LSTM

Concatennation

(c) Table Encoder.

Figure 2: Utterance-Table Encoder for the example in (a).

Utterance-Table BERT Embedding. We con-
sider two options as the input to the first layer bi-
LSTM. The first choice is the pretrained word em-
bedding. Second, we also consider the contextu-
alized word embedding based on BERT (Devlin
et al., 2019). To be specific, we follow Hwang
et al. (2019) to concatenate the user utterance and
all the column headers in a single sequence sepa-
rated by the [SEP] token:

[CLS], Xi,[SEP], c1,[SEP], . . . , cm,[SEP]

This sequence is fed into the pretrained BERT
model whose hidden states at the last layer is used
as the input embedding.

3.2 Interaction Encoder with Turn Attention
To capture the information across different utter-
ances, we use an interaction-level encoder (Suhr
et al., 2018) on top of the utterance-level encoder.
At each turn, we use the hidden state at the last
time step from the utterance-level encoder as the
utterance encoding. This is the input to a uni-
directional LSTM interaction encoder:

hUi = h
E
i,|Xi|

hIi+1 = LSTM
I(hUi ,h

I
i )

The hidden state of this interaction encoder hI

encodes the history as the interaction proceeds.
Turn Attention When issuing the current utter-
ance, the user may omit or explicitly refer to the
previously mentioned information. To this end,
we adopt the turn attention mechanism to capture
correlation between the current utterance and the
utterance(s) at specific turn(s). At the current turn
t, we compute the turn attention by the dot-product
attention between the current utterance and pre-
vious utterances in the history, and then add the
weighted average of previous utterance embed-
dings to the current utterance embedding:

si = h
U
t Wturn-atth

U
i

αturn = softmax(s)

cturnt = h
U
t +

t−1∑
i=1

αturni × hUi

(1)

The cturnt summarizes the context information and
the current user query and will be used as the ini-
tial decoder state as described in the following.

3.3 Table-aware Decoder
We use an LSTM decoder with attention to gen-
erate SQL queries by incorporating the interaction



5342

history, the current user utterance, and the table
schema.

Denote the decoding step as k, we provide the
decoder input as a concatenation of the embedding
of SQL query token qk and a context vector ck:

hDk+1 = LSTM
D([qk; ck],h

D
k )

where hD is the hidden state of the decoder
LSTMD, and the hidden state hD0 is initialized
by cturnt . When the query token is a SQL key-
word, qk is a learned embedding; when it is a col-
umn header, we use the column header embedding
given by the table-utterance encoder as qk. The
context vector ck is described below.
Context Vector with the Table and User Utter-
ance. The context vector consists of attentions
to both the table and the user utterance. First, at
each step k, the decoder computes the attention
between the decoder hidden state and the column
header embedding.

sl = h
D
k Wcolumn-atth

C
l

αcolumn = softmax(s)

ccolumnk =
∑
l

αcolumnl × hCl
(2)

where l is the index of column headers and hCl is
its embedding. Second, it also computes the at-
tention between the decoder hidden state and the
utterance token embeddings:

si,j = h
D
k Wutterance-atth

E
i,j

αutterance = softmax(s)

ctokenk =
∑
i,j

αutterancei,j × hEi,j
(3)

where i is the turn index, j is the token index, and
hEi,j is the token embedding for the j-th token of
i-th utterance. The context vector ck is a concate-
nation of the two:

ck = [c
column
k ; c

token
k ]

Output Distribution. In the output layer, our de-
coder chooses to generate a SQL keyword (e.g.,
SELECT, WHERE, GROUP BY, ORDER BY) or
a column header. This is critical for the cross-
domain setting where the table schema changes
across different examples. To achieve this, we use
separate layers to score SQL keywords and col-
umn headers, and finally use the softmax operation

Figure 3: Number of operations at different turns.

to generate the output probability distribution:

ok = tanh([h
D
k ; ck]Wo)

mSQL = okWSQL + bSQL

mcolumn = okWcolumnh
C

P (yk) = softmax([m
SQL;mcolumn])

(4)

3.4 Query Editing Mechanism
In an interaction with the system, the user of-
ten asks a sequence of closely related questions
to complete the final query goal. Therefore, the
query generated for the current turn often overlaps
significantly with the previous ones.

To empirically verify the usefulness of leverag-
ing the previous query, we consider the process
of generating the current query by applying copy
and insert operations to the previous query3. Fig-
ure 3 shows the SQL query length and the num-
ber of copy and insert operations at different turns.
As the interaction proceeds, the user question be-
comes more complicated as it requires longer SQL
query to answer. However, more query tokens
overlap with the previous query, and thus the num-
ber of new tokens remains small at the third turn
and beyond.

Based on this observation, we extend our table-
ware decoder with a query editing mechanism. We
first encode the previous query using another bi-
LSTM, and its hidden states are the query token
embeddings hQi,j′ (i.e., the j

′-th token of the i-th
query). We then extend the context vector with
the attention to the previous query:

ck = [c
column
k ; c

token
k ; c

query
k ]

where cqueryk is produced by an attention to query
tokens hQi,j′ in the same form as Equation 3.

3We use a diffing algorithm from https://github.
com/paulgb/simplediff

https://github.com/paulgb/simplediff
https://github.com/paulgb/simplediff


5343

At each decoding step, we predict a switch pcopy
to decide if we need copy from the previous query
or insert a new token.

pcopy = σ(ckWcopy + bcopy)

pinsert = 1− pcopy
(5)

Then, we use a separate layer to score the query
tokens at turn t − 1, and the output distribution is
modified as the following to take into account the
editing probability:

Pprev SQL = softmax(okWprev SQLh
Q
t−1)

mSQL = okWSQL + bSQL

mcolumn = okWcolumnh
C

PSQL
⋃

column = softmax([m
SQL;mcolumn])

P (yk) = pcopy · Pprev SQL(yk ∈ prev SQL)

+pinsert · PSQL⋃ column(yk ∈ SQL⋃ column)
(6)

While the copy mechanism has been introduced
by Gu et al. (2016) and See et al. (2017), they fo-
cus on summarization or response generation ap-
plications by copying from the source sentences.
By contrast, our focus is on editing the previously
generated query while incorporating the context of
user utterances and table schemas.

4 Related Work
Semantic parsing is the task of mapping natu-
ral language sentences into formal representations.
It has been studied for decades including using
linguistically-motivated compositional representa-
tions, such as logical forms (Zelle and Mooney,
1996; Clarke et al., 2010) and lambda calculus
(Zettlemoyer and Collins, 2005; Artzi and Zettle-
moyer, 2011), and using executable programs,
such as SQL queries (Miller et al., 1996; Zhong
et al., 2017) and other general-purpose program-
ming languages (Yin and Neubig, 2017; Iyer et al.,
2018). Most of the early studies worked on a
few domains and small datasets such as GeoQuery
(Zelle and Mooney, 1996) and Overnight (Wang
et al., 2015).

Recently, large and cross-domain text-to-SQL
datasets such as WikiSQL (Zhong et al., 2017) and
Spider (Yu et al., 2018c) have received an increas-
ing amount of attention as many data-driven neural
approaches achieve promising results (Dong and
Lapata, 2016; Su and Yan, 2017; Iyer et al., 2017;
Xu et al., 2017; Finegan-Dollak et al., 2018; Yu

et al., 2018a; Huang et al., 2018; Dong and Lap-
ata, 2018; Sun et al., 2018; Gur et al., 2018; Guo
et al., 2018; Yavuz et al., 2018; Shi et al., 2018).
Most of them still focus on context-independent
semantic parsing by converting single-turn ques-
tions into executable queries.

Relatively less effort has been devoted to
context-dependent semantic parsing on datasets
including ATIS (Hemphill et al., 1990; Dahl et al.,
1994b), SpaceBook (Vlachos and Clark, 2014),
SCONE (Long et al., 2016; Guu et al., 2017; Fried
et al., 2018; Suhr and Artzi, 2018; Huang et al.,
2019), SequentialQA (Iyyer et al., 2017), SParC
(Yu et al., 2019b) and CoSQL (Yu et al., 2019a).
On ATIS, Miller et al. (1996) maps utterances to
semantic frames which are then mapped to SQL
queries; Zettlemoyer and Collins (2009) starts
with context-independent Combinatory Categorial
Grammar (CCG) parsing and then resolves refer-
ences to generate lambda-calculus logical forms
for sequences of sentences. The most relevant
to our work is Suhr et al. (2018), who gener-
ate ATIS SQL queries from interactions by in-
corporating history with an interaction-level en-
coder and copying segments of previously gener-
ated queries. Furthermore, SCONE contains three
domains using stack- or list-like elements and
most queries include a single binary predicate. Se-
quentialQA is created by decomposing some com-
plicated questions in WikiTableQuestions (Pasu-
pat and Liang, 2015). Since both SCONE and Se-
quentialQA are annotated with only denotations
but not query labels, they don’t include many
questions with rich semantic and contextual types.
For example, SequentialQA (Iyyer et al., 2017) re-
quires that the answer to follow-up questions must
be a subset of previous answers, and most of the
questions can be answered by simple SQL queries
with SELECT and WHERE clauses.

Concurrent with our work, Yu et al. (2019a) in-
troduced CoSQL, a large-scale cross-domain con-
versational text-to-SQL corpus collected under the
Wizard-of-Oz setting. Each dialogue in CoSQL
simulates a DB querying scenario with a crowd
worker as a user and a college computer science
student who is familiar with SQL as an expert.
Question-SQL pairs in CoSQL reflect greater di-
versity in user backgrounds compared to other cor-
pora and involve frequent changes in user intent
between pairs or ambiguous questions that require
user clarification. These features pose new chal-



5344

lenges for text-to-SQL systems.
Our work is also related to recently proposed

approaches to code generation by editing (Hayati
et al., 2018; Yin et al., 2019; Hashimoto et al.,
2018). While they follow the framework of gen-
erating code by editing the relevant examples re-
trieved from training data, we focus on a context-
dependent setting where we generate queries from
the previous query predicted by the system itself.

5 Experimental Results
5.1 Metrics
On both Spider and SParC, we use the exact set
match accuracy between the gold and the pre-
dicted queries 4. To avoid ordering issues, instead
of using simple string matching, Yu et al. (2018c)
decompose predicted queries into different SQL
clauses such as SELECT, WHERE, GROUP BY,
and ORDER BY and compute scores for each
clause using set matching separately. On SparC,
we report two metrics: question match accuracy
which is the score average over all questions and
interaction match accuracy which is average over
all interactions.

5.2 Baselines
SParC. We compare with the two baseline models
released by Yu et al. (2019b).

(1) Context-dependent Seq2Seq (CD-Seq2Seq):
This model is adapted from Suhr et al. (2018).
The original model was developed for ATIS and
does not take the database schema as input hence
cannot generalize well across domains. Yu et al.
(2019b) adapt it to perform context-dependent
SQL generation in multiple domains by adding a
bi-LSTM database schema encoder which takes
bag-of-words representations of column headers
as input. They also modify the decoder to select
between a SQL keyword or a column header.

(2) SyntaxSQL-con: This is adapted from
the original context-agnostic SyntaxSQLNet (Yu
et al., 2018b) by using bi-LSTMs to encode the
interaction history including the utterance and the
associated SQL query response. It also employs a
column attention mechanism to compute represen-
tations of the previous question and SQL query.

Spider. We compare with the results as re-
ported in Yu et al. (2018b). Furthermore, we also
include recent results from Lee (2019) who pro-
pose to use recursive decoding procedure, Bogin

4More details at https://github.com/taoyds/spider/
tree/master/evaluation_examples

Dev Set Test Set
SQLNet (Xu et al., 2017) 10.9 12.4
SyntaxSQLNet (Yu et al., 2018b) 18.9 19.7

+data augmentation (Yu et al., 2018b) 24.8 27.2
Lee (2019) 28.5 24.3
GNN (Bogin et al., 2019) 40.7 39.4
IRNet (Guo et al., 2019) 53.2 46.7
IRNet (BERT) (Guo et al., 2019) 61.9 54.7
Ours 36.4 32.9

+ utterance-table BERT Embedding 57.6 53.4

Table 4: Spider results on dev set and test set.

et al. (2019) introducing graph neural networks
for encoding schemas, and Guo et al. (2019) who
achieve state-of-the-art performance by using an
intermediate representation to bridge natural lan-
guage questions and SQL queries.

5.3 Implementation Details

Our model is implemented in PyTorch (Paszke
et al., 2017). We use pretrained 300-dimensional
GloVe (Pennington et al., 2014) word embedding.
All LSTM layers have 300 hidden size, and we
use 1 layer for encoder LSTMs, and 2 layers for
decoder LSTMs. We use the ADAM optimizer
(Kingma and Ba, 2015) to minimize the token-
level cross-entropy loss with a batch size of 16.
Model parameters are randomly initialized from
a uniform distribution U [−0.1, 0.1]. The main
model has an initial learning rate of 0.001 and it
will be multiplied by 0.8 if the validation loss in-
creases compared with the previous epoch. When
using BERT instead of GloVe, we use the pre-
trained small uncased BERT model with 768 hid-
den size5, and we fine tune it with a separate con-
stant learning rate of 0.00001. The training typi-
cally converges in 10 epochs.

5.4 Overall Results

Spider. Table 4 shows the results on Spider
dataset. Since each question is standalone, we
don’t use interaction-level decoder or query edit-
ing. Our method can achieve the performance of
36.4% on dev set and 32.9% on test set, serving as
a strong model for the context-independent cross-
domain text-to-SQL generation. This demon-
strates the effectiveness of our utterance-table en-
coder and table-aware decoder to handle the se-
mantics of user utterances and the complexity of
table schemas to generate complex SQL queries
in unseen domains.

Furthermore, adding the utterance-table BERT
5
https://github.com/google-research/bert

https://github.com/taoyds/spider/tree/master/evaluation_examples
https://github.com/taoyds/spider/tree/master/evaluation_examples
https://github.com/google-research/bert


5345

Question Match Interaction Match
Dev Test Dev Test

SyntaxSQL-con (Yu et al., 2019b) 18.5 20.2 4.3 5.2
CD-Seq2Seq (Yu et al., 2019b)∗ 21.9 23.2 8.1 7.5

+ segment copy (w/ predicted query) 21.7 20.3 9.5 8.1
+ segment copy (w/ gold query) 27.3 26.7 10.0 8.4

Ours 31.4 – 14.7 –
+ query attention and sequence editing (w/ predicted query) 33.0 – 16.4 –
+ query attention and sequence editing (w/ gold query) 40.6 – 17.3 –

Ours + utterance-table BERT Embedding 40.4 – 18.1 –
+ query attention (w/ predicted query) 42.7 – 21.6 –
+ query attention and sequence editing (w/ predicted query) 47.2 47.9 29.5 25.3
+ query attention and sequence editing (w/ gold query) 53.4 54.5 29.2 25.0

Table 5: SParC results. For our models, we only report test set results of our best model on the dev set. ∗We
improve the CD-Seq2Seq performance over Yu et al. (2019b) by separating and parsing the column names (e.g.,
stu fname→ student first name) and using the schema-specific output vocabulary during decoding.

Dev Set Test Set
Query Relaxed Strict Query Relaxed Strict

FULL (Suhr et al., 2018) 37.5±0.9 63.0±0.7 62.5±0.9 43.6±1.0 69.3±0.8 69.2±0.8
Ours 36.2 60.5 60.0 43.9 68.5 68.1

Table 6: ATIS results on dev set and test set.

embedding gives significant improvement, achiev-
ing 57.6% on dev set and 53.4% on test set, which
is comparable to the state-of-the-art results from
IRNet with BERT. We attribute our BERT model’s
high performance to (1) the empirically power-
ful text understanding ability of pretrained BERT
model and (2) the early interaction between utter-
ances and column headers when they are concate-
nated in a single sequence as the BERT input.

SParC. Table 5 shows the results on SParC
dataset. Similar to Spider, our model with-
out previous query as input already outper-
forms SyntaxSQL-con, achieving 31.4% ques-
tion matching accuracy and 14.7% interaction
matching accuracy. In addition, compared with
CD-Seq2Seq, our model enjoys the benefits of
the table-utterance encoder, turn attention, and
the joint consideration of utterances and table
schemas during the decoding stage. This boosts
the performance by 10% question accuracy and
6% interaction accuracy.

Furthermore, we also investigate the effect of
copying segment. We use the same segment copy
procedure as Suhr et al. (2018): first deterministi-
cally extract segments from the previous query and
encode each segment using an LSTM, then gener-
ate a segment by computing its output probability
based on its segment encoding. However, since
the segment extraction from Suhr et al. (2018)

is exclusively designed for the ATIS dataset, we
implement our own segment extraction proce-
dure by extracting SELECT, FROM, GROUP BY,
ORDER BY clauses as well as different conditions
in WHERE clauses. In this way, 3.9 segments can
be extracted per SQL on average. We found that
adding segment copying to CD-Seq2Seq gives a
slightly lower performance on question matching
and a small gain on interaction matching, while
using segments extracted from the gold query can
have much higher results. This demonstrates that
segment copy is vulnerable to error propagation.
In addition, it can only copy whole segments
hence has difficulty capturing the changes of only
one or a few tokens in the query.

To better understand how models perform as
the interaction proceeds, Figure 4 (Left) shows the
performance split by turns on the dev set. The
questions asked in later turns are more difficult
to answer given longer context history. While the
baselines have lower performance as the turn num-
ber increases, our model still maintains 38%-48%
accuracy for turn 2 and 3, and 20% at turn 4 or
beyond. Similarly, Figure 4 (Right) shows the
performance split by hardness levels with the fre-
quency of examples. This also demonstrates our
model is more competitive in answering hard and
extra hard questions.

ATIS. We also report our model performance



5346

Figure 4: Performance split by different turns (Left) and hardness levels (Right) on SParC dev set.

Figure 5: Effect of query editing at different turns on
SParC dev set.

on ATIS in Table 6. Our model achieves 36.2%
dev and 43.9% test string accuracy, comparable
to Suhr et al. (2018). On ATIS, we only apply
our editing mechanism and reuse their utterance
encoder instead of the BERT utterance-table en-
coder, because ATIS is single domain.

5.5 Effect of Query Editing

We further investigate the effect of our query edit-
ing mechanism. To this end, we apply editing from
both the gold query and the predicted query on our
model with or without the utterance-table BERT
embedding. We also perform an ablation study to
validate the contribution of query attention and se-
quence editing separately.

As shown in Table 5, editing the gold query con-
sistently improves both question match and inter-
action match accuracy. This shows the editing ap-
proach is indeed helpful to improve the generation
quality when the previous query is the oracle.

Using the predicted query is a more realistic set-
ting, and in this case, the model is affected by error
propagation due to the incorrect queries produced
by itself. For the model without the utterance-

table BERT embedding, using the predicted query
only gives around 1.5% improvement. As shown
in Figure 5, this is because the editing mechanism
is more helpful for turn 4 which is a small fraction
of all question examples. For the model with the
utterance-table BERT embedding, the query gen-
eration accuracy at each turn is significantly im-
proved, thus reducing the error propagation effect.
In this case, the editing approach delivers con-
sistent improvements of 7% increase on question
matching accuracy and 11% increase on interac-
tion matching accuracy. Figure 5 also shows that
query editing with BERT benefits all turns.

Finally, as an ablation study, Table 5 also re-
ports the result with only query attention (use pre-
dicted query) on the dev set. This improves over
our vanilla BERT model without query attention
and achieves 42.7% question and 21.6% interac-
tion matching accuracy. With query editing, our
best model further improves to 47.2% question
and 29.5% interaction matching accuracy. This
demonstrates the effectiveness of our query atten-
tion and query editing separately, both of which
are essential to make use of the previous query.

6 Conclusions
In this paper, we propose an editing-based
encoder-decoder model to address the problem of
context-dependent cross-domain text-to-SQL gen-
eration. While being simple, empirical results
demonstrate the benefits of our editing mecha-
nism. The approach is more robust to error propa-
gation than copying segments, and its performance
increases when the basic text-to-SQL generation
quality (without editing) is better.

Acknowledgements

We thank the anonymous reviewers for their
thoughtful detailed comments.



5347

References
Yoav Artzi and Luke Zettlemoyer. 2011. Bootstrap-

ping semantic parsers from conversations. In Pro-
ceedings of the conference on empirical methods in
natural language processing, pages 421–432. Asso-
ciation for Computational Linguistics.

Ben Bogin, Matt Gardner, and Jonathan Berant. 2019.
Representing schema structure with graph neural
networks for text-to-sql parsing. In ACL.

James Clarke, Dan Goldwasser, Ming-Wei Chang, and
Dan Roth. 2010. Driving semantic parsing from
the world’s response. In Proceedings of the four-
teenth conference on computational natural lan-
guage learning, pages 18–27. Association for Com-
putational Linguistics.

Deborah A. Dahl, Madeleine Bates, Michael Brown,
William Fisher, Kate Hunicke-Smith, David Pallett,
Christine Pao, Alexander Rudnicky, and Elizabeth
Shriberg. 1994a. Expanding the scope of the atis
task: The atis-3 corpus. In Proceedings of the Work-
shop on Human Language Technology, HLT ’94,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.

Deborah A Dahl, Madeleine Bates, Michael Brown,
William Fisher, Kate Hunicke-Smith, David Pallett,
Christine Pao, Alexander Rudnicky, and Elizabeth
Shriberg. 1994b. Expanding the scope of the atis
task: The atis-3 corpus. In Proceedings of the work-
shop on Human Language Technology, pages 43–48.
Association for Computational Linguistics.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. In NAACL.

Li Dong and Mirella Lapata. 2016. Language to logi-
cal form with neural attention. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics, ACL 2016, August 7-12, 2016,
Berlin, Germany, Volume 1: Long Papers.

Li Dong and Mirella Lapata. 2018. Coarse-to-fine de-
coding for neural semantic parsing. In Proceed-
ings of the 56th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers).

Catherine Finegan-Dollak, Jonathan K. Kummerfeld,
Li Zhang, Karthik Ramanathan, Sesh Sadasivam,
Rui Zhang, and Dragomir Radev. 2018. Improv-
ing text-to-sql evaluation methodology. In Proceed-
ings of the 56th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers).

Daniel Fried, Jacob Andreas, and Dan Klein. 2018.
Unified pragmatic models for generating and follow-
ing instructions. In Proceedings of the 2018 Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, Volume 1 (Long Papers).

Jiatao Gu, Zhengdong Lu, Hang Li, and Victor O.K.
Li. 2016. Incorporating copying mechanism in
sequence-to-sequence learning. In Proceedings of
the 54th Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers).

Daya Guo, Yibo Sun, Duyu Tang, Nan Duan, Jian Yin,
Hong Chi, James Cao, Peng Chen, and Ming Zhou.
2018. Question generation from sql queries im-
proves neural semantic parsing. In Proceedings of
the 2018 Conference on Empirical Methods in Nat-
ural Language Processing.

Jiaqi Guo, Zecheng Zhan, Yan Gao, Yan Xiao,
Jian-Guang Lou, Ting Liu, and Dongmei Zhang.
2019. Towards complex text-to-sql in cross-domain
database with intermediate representation. In ACL.

Izzeddin Gur, Semih Yavuz, Yu Su, and Xifeng Yan.
2018. Dialsql: Dialogue based structured query
generation. In Proceedings of the 56th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers).

Kelvin Guu, Panupong Pasupat, Evan Liu, and Percy
Liang. 2017. From language to programs: Bridg-
ing reinforcement learning and maximum marginal
likelihood. In Proceedings of the 55th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers).

Tatsunori B Hashimoto, Kelvin Guu, Yonatan Oren,
and Percy S Liang. 2018. A retrieve-and-edit frame-
work for predicting structured outputs. In Advances
in Neural Information Processing Systems, pages
10052–10062.

Shirley Anugrah Hayati, Raphael Olivier, Pravalika
Avvaru, Pengcheng Yin, Anthony Tomasic, and Gra-
ham Neubig. 2018. Retrieval-based neural code
generation. In EMNLP.

Charles T Hemphill, John J Godfrey, and George R
Doddington. 1990. The atis spoken language sys-
tems pilot corpus. In Speech and Natural Language:
Proceedings of a Workshop Held at Hidden Valley,
Pennsylvania, June 24-27, 1990.

Hsin-Yuan Huang, Eunsol Choi, and Wen-tau Yih.
2019. Flowqa: Grasping flow in history for con-
versational machine comprehension. In ICLR.

Po-Sen Huang, Chenglong Wang, Rishabh Singh,
Wen-tau Yih, and Xiaodong He. 2018. Natural
language to structured query generation via meta-
learning. In Proceedings of the 2018 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 2 (Short Papers).

Wonseok Hwang, Jinyeung Yim, Seunghyun Park, and
Minjoon Seo. 2019. A comprehensive exploration
on wikisql with table-aware word contextualization.
arXiv preprint arXiv:1902.01069.



5348

Srinivasan Iyer, Ioannis Konstas, Alvin Cheung,
Jayant Krishnamurthy, and Luke Zettlemoyer. 2017.
Learning a neural semantic parser from user feed-
back. In Proceedings of the 55th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers).

Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and
Luke Zettlemoyer. 2018. Mapping language to code
in programmatic context. In Proceedings of the
2018 Conference on Empirical Methods in Natural
Language Processing.

Mohit Iyyer, Wen-tau Yih, and Ming-Wei Chang. 2017.
Search-based neural structured learning for sequen-
tial question answering. In Proceedings of the 55th
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers).

Diederik P Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. In ICLR.

Dongjun Lee. 2019. Recursive and clause-wise decod-
ing for complex and cross-domain text-to-sql gener-
ation. In CoRR.

Reginald Long, Panupong Pasupat, and Percy Liang.
2016. Simpler context-dependent logical forms via
model projections. In Proceedings of the 54th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers).

Minh-Thang Luong, Hieu Pham, and Christopher D
Manning. 2015. Effective approaches to attention-
based neural machine translation. In EMNLP.

Scott Miller, David Stallard, Robert Bobrow, and
Richard Schwartz. 1996. A fully statistical approach
to natural language interfaces. In Proceedings of the
34th Annual Meeting of the Association for Compu-
tational Linguistics.

Panupong Pasupat and Percy Liang. 2015. Compo-
sitional semantic parsing on semi-structured tables.
In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the
7th International Joint Conference on Natural Lan-
guage Processing (Volume 1: Long Papers).

Adam Paszke, Sam Gross, Soumith Chintala, Gre-
gory Chanan, Edward Yang, Zachary DeVito, Zem-
ing Lin, Alban Desmaison, Luca Antiga, and Adam
Lerer. 2017. Automatic differentiation in pytorch.
In NIPS 2017 Workshop Autodiff.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 confer-
ence on empirical methods in natural language pro-
cessing (EMNLP), pages 1532–1543.

Abigail See, Peter J. Liu, and Christopher D. Manning.
2017. Get to the point: Summarization with pointer-
generator networks. In Proceedings of the 55th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers).

Tianze Shi, Kedar Tatwawadi, Kaushik Chakrabarti,
Yi Mao, Oleksandr Polozov, and Weizhu Chen.
2018. Incsql: Training incremental text-to-sql
parsers with non-deterministic oracles. arXiv
preprint arXiv:1809.05054.

Yu Su and Xifeng Yan. 2017. Cross-domain seman-
tic parsing via paraphrasing. In Proceedings of the
2017 Conference on Empirical Methods in Natural
Language Processing.

Alane Suhr and Yoav Artzi. 2018. Situated mapping
of sequential instructions to actions with single-step
reward observation. In Proceedings of the 56th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers).

Alane Suhr, Srinivasan Iyer, and Yoav Artzi. 2018.
Learning to map context-dependent sentences to ex-
ecutable formal queries. In Proceedings of the 2018
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, Volume 1 (Long Papers).

Yibo Sun, Duyu Tang, Nan Duan, Jianshu Ji, Guihong
Cao, Xiaocheng Feng, Bing Qin, Ting Liu, and Ming
Zhou. 2018. Semantic parsing with syntax- and
table-aware sql generation. In Proceedings of the
56th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers).

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in neural information process-
ing systems, pages 3104–3112.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in neural information pro-
cessing systems, pages 5998–6008.

Andreas Vlachos and Stephen Clark. 2014. A new cor-
pus and imitation learning framework for context-
dependent semantic parsing. Transactions of the As-
sociation for Computational Linguistics.

Yushi Wang, Jonathan Berant, and Percy Liang. 2015.
Building a semantic parser overnight. In Proceed-
ings of the 53rd Annual Meeting of the Association
for Computational Linguistics and the 7th Interna-
tional Joint Conference on Natural Language Pro-
cessing (Volume 1: Long Papers).

Xiaojun Xu, Chang Liu, and Dawn Song. 2017. Sqlnet:
Generating structured queries from natural language
without reinforcement learning. arXiv preprint
arXiv:1711.04436.

Semih Yavuz, Izzeddin Gur, Yu Su, and Xifeng Yan.
2018. What it takes to achieve 100% condition ac-
curacy on wikisql. In Proceedings of the 2018 Con-
ference on Empirical Methods in Natural Language
Processing.



5349

Pengcheng Yin and Graham Neubig. 2017. A syntactic
neural model for general-purpose code generation.
In Proceedings of the 55th Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers).

Pengcheng Yin, Graham Neubig, Miltiadis Allamanis,
Marc Brockschmidt, and Alexander L Gaunt. 2019.
Learning to represent edits. In ICLR.

Tao Yu, Zifan Li, Zilin Zhang, Rui Zhang, and
Dragomir Radev. 2018a. Typesql: Knowledge-
based type-aware neural text-to-sql generation. In
Proceedings of NAACL. Association for Computa-
tional Linguistics.

Tao Yu, Michihiro Yasunaga, Kai Yang, Rui Zhang,
Dongxu Wang, Zifan Li, and Dragomir Radev.
2018b. Syntaxsqlnet: Syntax tree networks for com-
plex and cross-domain text-to-sql task. In Proceed-
ings of the 2018 Conference on Empirical Methods
in Natural Language Processing.

Tao Yu, Rui Zhang, He Yang Er, Suyi Li, Eric Xue,
Bo Pang, Xi Victoria Lin, Yi Chern Tan, Tianze
Shi, Zihan Li, Youxuan Jiang, Michihiro Yasunaga,
Sungrok Shim, Tao Chen, Alexander Fabbri, Zifan
Li, Luyao Chen, Yuwen Zhang, Shreya Dixit, Vin-
cent Zhang, Caiming Xiong, Richard Socher, Wal-
ter Lasecki, and Dragomir Radev. 2019a. Cosql: A
conversational text-to-sql challenge towards cross-
domain natural language interfaces to databases. In
Proceedings of the 2019 Conference on Empiri-
cal Methods in Natural Language Processing and
9th International Joint Conference on Natural Lan-
guage Processing. Association for Computational
Linguistics.

Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga,
Dongxu Wang, Zifan Li, James Ma, Irene Li,
Qingning Yao, Shanelle Roman, Zilin Zhang,
and Dragomir Radev. 2018c. Spider: A large-
scale human-labeled dataset for complex and cross-
domain semantic parsing and text-to-sql task. In
Proceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing.

Tao Yu, Rui Zhang, Michihiro Yasunaga, Yi Chern
Tan, Xi Victoria Lin, Suyi Li, Heyang Er, Irene
Li, Bo Pang, Tao Chen, Emily Ji, Shreya Dixit,
David Proctor, Sungrok Shim, Jonathan Kraft, Vin-
cent Zhang, Caiming Xiong, Richard Socher, and
Dragomir Radev. 2019b. Sparc: Cross-domain se-
mantic parsing in context. In Proceedings of ACL.

John M. Zelle and Raymond J. Mooney. 1996. Learn-
ing to parse database queries using inductive logic
programming. In AAAI/IAAI, pages 1050–1055,
Portland, OR. AAAI Press/MIT Press.

Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammars. UAI.

Luke S Zettlemoyer and Michael Collins. 2009. Learn-
ing context-dependent mappings from sentences to
logical form. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP: Volume 2-Volume
2, pages 976–984. Association for Computational
Linguistics.

Victor Zhong, Caiming Xiong, and Richard Socher.
2017. Seq2SQL: Generating structured queries
from natural language using reinforcement learning.
CoRR, abs/1709.00103.


