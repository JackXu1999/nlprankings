



















































Equivalences between Ranked and Unranked Weighted Tree Automata via Binarization


Proceedings of the ACL Workshop on Statistical NLP and Weighted Automata, pages 1–10,
Berlin, Germany, August 12, 2016. c©2016 Association for Computational Linguistics

Equivalences between Ranked and Unranked Weighted Tree Automata
via Binarization

Toni Dietze
Faculty of Computer Science

Technische Universität Dresden
01062 Dresden, Germany

toni.dietze@tu-dresden.de

Abstract

Encoding unranked trees to binary trees,
henceforth called binarization, is an im-
portant method to deal with unranked
trees. For each of three binarizations
we show that weighted (ranked) tree au-
tomata together with the binarization are
equivalent to weighted unranked tree au-
tomata; even in the probabilistic case. This
allows to easily adapt training methods
for weighted (ranked) tree automata to
weighted unranked tree automata.

1 Introduction

When dealing with trees, tree grammars, and for-
mal languages of trees, a restriction to binary trees
is often beneficial, e.g. for improved generaliza-
tion when generating grammars from treebanks,
or for reduced parsing complexity. A binarization
maps any (unranked) tree to a binary tree such that
the original tree can be reconstructed from the re-
sult.

In this paper we investigate three different bi-
narizations inspired by Matsuzaki et al. (2005,
Fig. 6). For each of these binarizations we show
that a weighted unranked tree language is rec-
ognizable by a weighted unranked tree automa-
ton (wuta) iff the binarization of the language is
recognizable by a weighted tree automaton (wta).
This even holds with restriction to probabilistic
automata. To support this result we show that
for any R≥0-weighted finite state automaton with
the sum of weights of all words being 1, there
is an equivalent probabilistic finite state automa-
ton. This implies that the class of weighted string
languages recognizable by probabilistic finite state
automata is closed under reversal.

These results suggest that by adding binariza-
tion to training methods for wta we effectively

get training methods for wuta. Alterations to the
weights and the state behaviour while training then
carry over to wuta. This also gives an explanation
for why the performance of the training by Mat-
suzaki et al. (2005) is rather independent from the
used binarization.

Related Work. Fülöp and Vogler (2009) give an
overview over wta, and Droste and Vogler (2011)
introduced wuta. For the unweighted case bina-
rizations (also called encodings) were investigated
by, e.g., Comon et al. (2007, Section 8.3). Their
first-child-next-sibling encoding is similar to our
left-branching binarization. Their extension en-
coding is also used to define stepwise tree au-
tomata (Carme et al., 2004). A stepwise tree au-
tomaton is defined like a (ranked) tree automa-
ton. It accepts an unranked tree, if it accepts the
extension encoding of the tree while the automa-
ton is interpreted as a (ranked) tree automaton.
Högberg et al. (2009, Lemmas 4.2 and 6.2) ex-
tend this connection to the weighted case and show
that weighted stepwise tree automata and wuta are
equally powerful.

Our results for weighted and for probabilistic
finite state automata are a special case of renor-
malization of weighted or of probabilistic context-
free grammars (Abney et al., 1999; Chi, 1999;
Nederhof and Satta, 2003). We provide alternative
proofs for this special case.

2 Preliminaries

LetA andB be sets, andR ⊆ A×B a relation. By
R(a) we denote the set {b ∈ B | (a, b) ∈ R} for
every a ∈ A. The inverse relation of R is defined
by R−1 = {(b, a) | (a, b) ∈ R}. Occasionally we
identify a function f : A → B with the relation
{(a, f(a)) | a ∈ A}.

An alphabet is a finite, non-empty set. LetΣ be
an alphabet. The set of words over Σ is denoted

1



by Σ∗. The length of a word w ∈ Σ∗ is denoted
by |w|. The empty word, denoted by ε, is the word
of length 0. The set of unranked trees over Σ, de-
noted by UΣ , is the smallest set U such that for
every σ ∈ Σ, k ∈ N, and t1, . . . , tk ∈ U we
have σ(t1, . . . , tk) ∈ U . We abbreviate σ() ∈ UΣ
by σ. Let t = σ(t1, . . . , tk) ∈ UΣ . We call σ
the root symbol of t. The root rank of t is defined
by rk(t) = k. The set of positions of t is recur-
sively defined by pos(t) = ε ∪ ⋃i∈{1,...,k}{iρ |
ρ ∈ pos(ti)}. Let ρ ∈ pos(t). The subtree of t
at ρ is recursively defined by t|ρ = t if ρ = ε and
t|ρ = ti|ρ′ if ρ = iρ′. The symbol of t at ρ, de-
noted by t(ρ), is defined as the root symbol of t|ρ.
The rank of t at ρ is defined as rk(t|ρ).

Let S be a non-empty set (of sorts). An S-
sorted alphabet is a family Σ = (Σ(s) | s ∈ S)
of pairwise disjoint sets such that their union is
non-empty and finite. For families of sets we of-
ten use the same identifier for denoting the fam-
ily and the union of its members. Let Σ be an
(S×S∗)-sorted alphabet. The family of (S-sorted)
trees over Σ, denoted by TΣ = (T

(s)
Σ | s ∈ S),

is defined as the smallest family (T (s) | s ∈ S)
such that for every (s0, s1 . . . sk) ∈ (S × S∗), for
every σ ∈ Σ(s0,s1...sk), for every t1 ∈ T (s1), . . . ,
tk ∈ T (sk), we have σ(t1, . . . , tk) ∈ T (s0). If S
is a singleton, this is equivalent to the usual defi-
nition of ranked trees. Note that TΣ ⊆ UΣ , so all
notions for unranked trees are also valid for sorted
trees. Also note that, for sorted trees, the symbol
at a position determines the rank at this position;
therefore we will use rk also for symbols from Σ.

A commutative semiring is an algebraic struc-
ture < = (R,+, ·, 0, 1) such that (R,+, 0) and
(R, ·, 1) are commutative monoids, · is distribu-
tive over +, and 0 is annihilating w.r.t. ·. We
often write < instead of R. Let A be a set and
f : A → < be a mapping. The support of f is
defined by supp(f) = {a ∈ A | f(a) 6= 0}.

In the following we will define devices to asso-
ciate weights with sorted trees, words, or unranked
trees. We will use weights from an arbitrary com-
mutative semiring. Therefore, if we use addition,
multiplication, 0, and 1, then these are operations
from that semiring. We will call two such devices
equivalent, if their (yet to define) semantics J·K is
the same.

Definition 1 (wsta). Let S be a set of sorts and
< be a commutative semiring. An <-weighted
S-sorted tree automaton (<-S-wsta) is a tuple

(Q,Σ, I,∆) where
• Q is an S-sorted alphabet (of states),
• Σ is an (S×S∗)-sorted alphabet (of terminals),
• I : Q→ < is a mapping (root weights), and
• ∆ = (∆(σ) : Q(s0) × . . . × Q(sk) → < |

(s0, s1 . . . sk) ∈ S × S∗, σ ∈ Σ(s0,s1...sk)) is
a family of mappings (transition weights).

Let M = (Q,Σ, I,∆) be an <-S-wsta. The
size of M is defined by size(M) = supp(I) +∑

σ∈Σ supp(∆
(σ)). For some s ∈ S, we callM

s-rooted, if for every s′ ∈ S\{s} and q ∈ Q(s′) we
have that I(q) = 0. We define the relation runM
that contains (t, r), if t ∈ TΣ , r : pos(t) → Q,
and for every ρ ∈ pos(t) we have r(ρ) ∈ Q(s0),
if t(ρ) ∈ Σ(s0,s1...sk), i.e., the sorts of states and
terminals at the same position match. We say that
r is a run ofM on t.
Definition 2 (semantics of wsta). Let M =
(Q,Σ, I,∆) be an <-S-wsta and t ∈ TΣ . The
weight of t under M is defined by JMK(t) =∑

r∈runM(t)JMK(t, r) where
JMK(t, r) = I(r(ε))
·
∏

ρ∈pos(t)
∆(σ)(r(ρ), r(ρ1), . . . , r(ρk))

and σ = t(ρ) and k = rk(σ).

Note: If S is a singleton set, then S-wsta are
equivalent to weighted tree automata over ranked
alphabets (Fülöp and Vogler, 2009). The sorts just
add syntactic restrictions that will help us in the
following.

Definition 3 (wfsa). Let < be a commutative
semiring. An <-weighted finite state automaton
(<-wfsa) is a tuple (P,Σ, J,Π, F ) where
• P is an alphabet (of states),
• Σ is an alphabet (of terminals),
• J : P → < is a mapping (initial weights),
• Π : P × Σ × P → < is a mapping (transition

weights), and
• F : P → < is a mapping (final weights).
By wfsa(<, Σ) we denote the set of all <-
wfsa with terminal alphabet Σ. Let N =
(P,Σ, J,Π, F ) be an <-wfsa. The size of N is
defined by size(N ) = supp(J) + supp(Π) +
supp(F ). We define the relation runN that con-
tains (w, r), if w ∈ Σ∗ and r : {0, . . . , |w|} → P .
We say that r is a run of N on t.
Definition 4 (semantics of wfsa). Let N =
(P,Σ, J,Π, F ) be an <-wfsa and w =

2



w1 . . . wk ∈ Σ∗ for some k ∈ N. The
weight of w under N is defined by JN K(w) =∑

r∈runN (w)JN K(w, r) whereJN K(w, r) = J(r(0))
·
( ∏
i∈{1,...,|w|}

Π(r(i− 1), wi, r(i))
)

· F (r(|w|)).
Definition 5 (wuta). Let < be a commutative
semiring. An <-weighted unranked tree automa-
ton (<-wuta) is a tuple (Q,Σ, I,∆) where
• Q is an alphabet (of states),
• Σ is an alphabet (of terminals),
• I : Q→ < is a mapping (root weights), and
• ∆ : Q×Σ → wfsa(<, Q) is a mapping.
Let M = (Q,Σ, I,∆) be an <-wuta. The
size of M is defined by size(M) = supp(I) +∑

q∈Q,σ∈Σ size(∆(q, σ)). The number of states of
M is defined as |Q| plus the numbers of states
of all wfsa in the image of ∆. We define the re-
lation runM that contains (t, r), if t ∈ UΣ and
r : pos(t)→ Q. We say that r is a run ofM on t.

Definition 6 (semantics of wuta). Let M =
(Q,Σ, I,∆) be an <-wuta. Let t ∈ UΣ . The
weight of t under M is defined by JMK(t) =∑

r∈runM(t)JMK(t, r) whereJMK(t, r) = I(r(ε))
·
∏

ρ∈pos(t)
J∆(r(ρ), σ)K(r(ρ1) . . . r(ρk))

and σ = t(ρ) and k = rk(t|ρ).
By exploiting distributivity it is easy to find

the following equivalent formulation. Let PM
be the set of all states of all wfsa in the image
of ∆. We define the relation ex-runM that con-
tains (t, e), if t ∈ UΣ and e = (r, s) where r ∈
runM(t) and s : pos(t)→

⋃
n∈N PM

{0,...,n} such
that s(ρ) ∈ run∆(r(ρ),t(ρ))(r(ρ1) . . . r(ρ rk(t|ρ)))
for every ρ ∈ pos(t). We say that e is an ex-
tended run of M on t. We have JMK(t) =∑

e∈ex-runM(t)JMK(t, e) where
JMK(t, (r, s)) = I(r(ε)) · ∏

ρ∈pos(t)
Jρ(s(ρ)(0))

·
(rk(t|ρ)∏

i=1

Πρ
(
s(ρ)(i− 1), r(ρi), s(ρ)(i)))

· Fρ
(
s(ρ)(rk(t|ρ))

)

and (Pρ, Q, Jρ, Πρ, Fρ) = ∆(r(ρ), t(ρ)). A sim-
ilar result was stated by Droste and Vogler (2011,
Def. 6.7 and Obs. 6.8).

3 Equivalences via Binarizations

In this section we will present three different sur-
jective mappings h : TΓ → UΣ where Σ is an
alphabet and Γ is a sorted alphabet with the max-
imum rank of a symbol being 2. We call h a bi-
narization and we binarize a tree by using h back-
wards. Note that this allows several representa-
tions for a single unranked tree. Occasionally we
also say that t′ is a binarization of t if t′ ∈ TΓ and
t = h(t′).

We show that wsta together with any of the pre-
sented binarizations and wuta are equally pow-
erful; more formally for every wuta M there is
a wsta M′ and vice versa such that JMK(t) =∑

t′∈h−1(t)JM′K(t′) for every t ∈ TΣ .
3.1 Left-branching Binarization

Our first binarization is inspired by the LEFT
binarization of Matsuzaki et al. (2005, Fig. 6).
It is similar to first-child-next-sibling encoding
(Comon et al., 2007, Sec. 8.3.1). It transforms
an unranked branch into a sequence of branches
growing rightwards (cf. Figure 1).

Let Σ be an alphabet and let S = {T,H}
be a set of sorts. Intuitively, T will be the sort
for trees and H will be the sort for hedges (se-
quences of trees). Based on Σ and assuming
CONS, NULL /∈ Σ, we define the (S × S∗)-sorted
alphabet Γ by Γ (T,H) = Σ, Γ (H,TH) = {CONS},
Γ (H,ε) = {NULL}.

There is a unique homomorphism h from the S-
sorted term algebra over Γ into the S-sorted al-
gebra ((A(s) | s ∈ S), (θσ | σ ∈ Γ )) where
A(T) = UΣ , A(H) = (UΣ)∗,

∀σ ∈ Σ : θσ(t1 . . . tk) = σ(t1, . . . , tk),
θCONS(t0, t1 . . . tk) = t0t1 . . . tk,

θNULL() = ε.

Figure 1 (ignoring the states for now) illustrates
h. Note that h is a bijection, where h(ξ′)(ρ) =
ξ′(12ρ1−11 · · · 12ρn−11) for every ξ′ ∈ T(T)Γ and
ρ = ρ1 · · · ρn ∈ pos(h(ξ′)).

Now letM′ = (Q′, Γ, I ′, ∆′) be a T-rooted <-
S-wsta andM = (Q,Σ, I,∆) be an <-wuta such
that Q and the state sets of the wfsa defined by ∆
are pairwise disjoint. We say thatM andM′ are

3



σ

CONS

α

NULL

p5

q 1
CONS

γ

CONS

α

NULL

p8

q4

NULL

p7

p6

q 2

CONS

β

NULL

p9

q3

NULL

p4

p
3

p
2

p1

q0

h7−→
f

σ

α

q1

γ

α

q4

q2

β

q3

q0

p1
p2 p3

p4

p5
p6 p7

p8

p9

Figure 1: Trees with runs of related wsta and wuta.

JMK(t, e)
= I(r(ε)) ·

∏
ρ∈pos(t)

Jρ(s(ρ)(0))

·
(rk(t|ρ)∏

i=1

Πρ
(
s(ρ)(i− 1), r(ρi), s(ρ)(i))) · Fρ(s(ρ)(rk(t|ρ)))

= I ′(r(ε)) ·
∏

ρ∈pos(t)
∆′(t(ρ))(r(ρ), s(ρ)(0))

·
(rk(t|ρ)∏

i=1

∆′(CONS)(s(ρ)(i− 1), r(ρi), s(ρ)(i))
)

·∆′(NULL)(s(ρ)(rk(t|ρ)))

(by definition of related)

= I ′(r′(ε)) ·
∏

ρ∈pos(t),
ρ′=12ρ1−11···12ρ|ρ|−11,

k=rk(t|ρ)

∆′(t
′(ρ′))(r′(ρ′), r′(ρ′1))

·
( k∏
i=1

∆′(t
′(ρ′12i−1))(r′(ρ′12i−1), r′(ρ′12i−11), r′(ρ′12i))

)
·∆′(t′(ρ′12k))(r′(ρ′12k))

(by definition of h and f )

= I ′(r′(ε)) ·
∏

ρ∈pos(t′)
∆′(t

′(ρ))(r′(ρ), r′(ρ1), . . . , r′(ρ rk(t′|ρ)))

(by commutativity of · and definition of h)
= JM′K(t′, r′)

Figure 2: Showing that JMK(t, e) = JM′K(t′, r′) for proof of Theorem 7, where (t, e) = f(t′, r′),
(r, s) = e, and (Pρ, Q, Jρ, Πρ, Fρ) = ∆(r(ρ), t(ρ)) for every ρ ∈ pos(t).

4



related, if Q′(T) = Q, Q′(H) is the union of the
state sets of all wfsa defined by ∆, and for every
σ ∈ Σ, q0, q ∈ Q, and p, p′ ∈ P we have

I ′(q) =

{
I(q) if q ∈ Q,
0 otherwise,

∆′(σ)(q0, p) = J(p),

∆′(CONS)(p, q, p′) = Π(p, q, p′),

∆′(NULL)(p) = F (p),

where (P,Q, J,Π, F ) = ∆(q0, σ). Note thatM′
is T-rooted.

Theorem 7. If M and M′ are related, thenJM′K(t) = JMK(h(t)) for every t ∈ TΓ .
Proof. Assume that M and M′ are related. Fig-
ure 1 shows an example tree and its image under
h. Moreover it shows a run and its image under
the function f : runM′ → ex-runM that is de-
fined as follows: For every (t′, r′) ∈ runM′ and
ρ ∈ pos(h(t)) we let ρ′ = 12ρ1−11 · · · 12ρ|ρ|−11
and define f(t′, r′) = (h(t′), (r, s)) where r(ρ) =
r′(ρ′) and s(ρ)(i) = r′(ρ′12i) for every i ∈
{0, . . . , rk(t|ρ)}. Note that f is a bijection.

Let (t′, r′) ∈ runM′ and (t, e) = f(t′, r′). In
Figure 2 we show that JMK(t, e) = JM′K(t′, r′).
This immediately implies that JMK(t) =JM′K(t′). q.e.d.

It is easy to see that a wuta and a wsta have the
same size and number of states, if they are related.

3.2 Right-branching Binarization
Our second binarization is based on the RIGHT bi-
narization of Matsuzaki et al. (2005, Fig. 6).

In comparison to left-branching binarization we
make the following changes. We define Γ by
Γ (T,H) = Σ, Γ (H,HT) = {SNOC}, and Γ (H,ε) =
{NULL}. To define h we replace the definition of
θCONS by θSNOC(t1 . . . tk, tk+1) = t1 . . . tktk+1.
In the definition for related, we just replace CONS
by SNOC and reverse the wfsa, i.e. we interchange
J and F and swap the states in transitions. The-
orem 7 still holds with these changes; the proof
works analogously.

3.3 Mixed Binarization
We now have a look at a binarization where the
direction of growth may be flipped at arbitrary po-
sitions from rightwards to leftwards; cf. CENTER-
PARENT and CENTER-HEAD binarization of Mat-
suzaki et al. (2005, Fig. 6). For this purpose,

let S = {T,H,H}. Based on Σ and assuming
FLIP, CONS, NULL, SNOC, NULL /∈ Σ, we define the
(S × S∗)-sorted alphabet Γ by

Γ (T,H) = Σ, Γ (H,HT) = {FLIP},
Γ (H,ε) = {NULL}, Γ (H,TH) = {CONS},
Γ (H,ε) = {NULL}, Γ (H,HT) = {SNOC}.

There is a unique homomorphism h from the S-
sorted term algebra over Γ into the S-sorted al-
gebra ((A(s) | s ∈ S), (θσ | σ ∈ Γ )) where
A(T) = UΣ , A(H) = A(H) = (UΣ)∗, and

∀σ ∈ Σ : θσ(t1 . . . tk) = σ(t1, . . . , tk),
θCONS(t0, t1 . . . tk) = t0t1 . . . tk,
θFLIP(t1 . . . tk, tk+1) = t1 . . . tktk+1,
θSNOC(t1 . . . tk, tk+1) = t1 . . . tktk+1,
θNULL() = θNULL() = ε.

Unfortunately, this homomorphism is just surjec-
tive, but not bijective. That means, there may be
several possible binarizations of an unranked tree.
Given a wsta M′ we will construct a wuta M,
such that the weight of an unranked tree t underM
is the sum of weights of all binarizations h−1(t)
underM′.

Figure 4 shows an unranked node with the three
subtrees t1, t2, t3 and Figure 3 shows one possible
binarization with the binarized subtrees t′1, t′2, t′3.
Note that in Figure 3 the rightmost subtree t′3 is
attached to the node labeled FLIP, yet this node is
located in the middle of the tree. Therefore, if we
follow the path from the root to the leaf labeled
NULL, we find t′1, t′3, and t′2 in this order. For the
indicated run, we find the states in the order p0, p1,
p3, p2.

Conversely, in the unranked case we find the
subtrees in the order t1, t2, t3. This is indicated
by the arrow in Figure 3. Therefore, in a run of a
wfsa from the constructed wuta, we have to pass
along the information that we visited the state p1,
because we need it at the rightmost subtree t3. Ad-
ditionally, since each transition of the wfsa deals
with one subtree, but the NULL node has no sub-
trees, we have to guess a child and pass on this
guess in the state. The constructed run is depicted
in Figure 4.

Construction 8. For this construction, we use Σ,
Γ , and S as defined above.

Let M′ = (Q′, Γ, I ′, ∆′) be a T-rooted
<-S-wsta. We construct the <-wuta M =

5



σ

CONS

t′1

q1

FLIP

SNOC

NULL

p2

t′2

q2

p3

t′3

q3

p1

p0

q0

Figure 3: A binarization of Figure 4. The arrow
indicates the processing order of the constructed
wuta (cf. Construction 8). Note that the arrow
touches p1 twice.

(Q′(T), Σ, I,∆) where, for every q0 ∈ Q′(T) and
σ ∈ Σ, we set I(q0) = I ′(q0) and ∆(q0, σ) =
(P,Q′(T), J,Π, F ) where P = Q′(H) ∪ {p̄q,s |
p̄ ∈ Q′(H), q ∈ Q′(T), s ∈ Q′(H)}, and for ev-
ery p, r, s ∈ Q′(H), p̄, r̄ ∈ Q′(H), and q, q′ ∈ Q′(T)
we set

J(p) = ∆′(σ)(q0, p),
Π(p, q, r) = ∆′(CONS)(p, q, r),

Π(s, q, r̄q,s) = ∆′(NULL)(r̄),

Π(p̄q′,s, q, r̄q,s) = ∆′(SNOC)(r̄, p̄, q′),

F (p̄q,s) = ∆′(FLIP)(s, p̄, q),

F (p) = ∆′(NULL)(p).

Every other weight is set to 0.

Theorem 8. ForM andM′ from Construction 8,
we have JMK(t) = ∑t′∈h−1(t)JM′K(t′) for every
t ∈ TΣ .

Proof. Analogously to the proof of Theorem 7, we
define a function f : runM′ → ex-runM. Fig-
ures 3 and 4 visualize this mapping. Note in con-
trast to h that f is injective.

By construction of M, we have thatJM′K(t′, r′) = JMK(f(t′, r′)) for every t′ ∈ TΓ
and r′ ∈ runM′(t′). For every (t, e) not in the
image of f we have JMK(t, e) = 0.

h7−→
f

σ

t1

q1

t2

q2

t3

q3

q0

p0
p1 p2 q2,p1

p3 q3,p1

Figure 4: Unranked node with three subtrees.

All in all we have

JMK(t) = ∑
e∈ex-runM(t)

JMK(t, e)
=

∑
e∈ex-runM(t)∩im(f)

JMK(t, e)
=

∑
e∈ex-runM(t)∩im(f)

JM′K(f−1(t, e))
=

∑
t′∈h−1(t)

∑
r′∈runM′ (t′)

JM′K(t′, r′)
=

∑
t′∈h−1(t)

JM′K(t′). q.e.d.
In Construction 8, the number of states of a sin-

gle wfsa of M is in O(|Q′|3) and its size is in
O(|Q′|2 · size(M′)). Since there is a wfsa for
every state and terminal of M, the number of
states ofM is in O(|Q′|4 · |Σ|) and its size is in
O(|Q′|3 · |Σ| · size(M′)). Note that Π and F are
the same for every constructed wfsa, so it might be
beneficial to share them in an implementation.

For the other direction, i.e. when constructing
a wsta given a wuta such that Theorem 8 holds,
note that trees resulting from left-branching and
right-branching binarization may also result from
mixed binarization (modulo different node labels).
Therefore the results from the previous sections
can be applied.

4 The Probabilistic Case

For this section, we consider the semiring of non-
negative reals with addition and multiplication.
Note that every element different from 0 has an
inverse with respect to multiplication. We will use
this fact later on. Since our semiring is fixed, we
will not mention it anymore in this section.

If we only consider weights between 0 and
1, and some additional conditions are met, these
weights can be intuitively interpreted as probabil-
ities. With this idea in mind we start with investi-
gating wfsa since they form the core of wuta.

6



4.1 Probabilistic Automata

A wfsa N = (P,Σ, J,Π, F ) is called
• out-probabilistic, if for every p ∈ P we have
F (p) +

∑
σ∈Σ,p′∈P Π(p, σ, p

′) = 1,
• semi-probabilistic, if it is out-probabilistic and∑

p∈P J(p) = 1,
• convergent, if∑w∈Σ∗JN K(w) is finite,
• consistent, if this sum is 1,
• probabilistic, if it is semi-probabilistic and con-

sistent, and
• reduced, if for every state p ∈ P there is a word
w ∈ Σ∗, a run r ∈ runN (w), and an index
i ∈ {0, . . . , |w|} such that JN K(w, r) > 0 and
r(i) = p.

These notions are strongly influenced by Dupont
et al. (2005). If a semi-probabilistic wfsa is re-
duced, then it is consistent and, hence, probabilis-
tic (Dupont et al., 2005, cf. Def. 9 and Prop. 2).
Note that you can construct an equivalent reduced
wfsa from any wfsa by removing those states p
that violate the above condition. These states can
be effectively determined.

Related Work For the remainder of this section
we investigate a special case of renormalization
of weighted or of probabilistic context-free gram-
mars (Abney et al., 1999; Chi, 1999; Nederhof and
Satta, 2003). We restrict our investigations to wfsa
and give alternative proofs.

In the following, we give an alternative view on
wfsa by using matrices. Let A be a matrix. The
matrix entry in the i-th row and the j-th column
is denoted by (A)i,j . If A has just a single row or
column, we write (A)i for the entry in the i-th col-
umn or row, respectively. A matrix with just a sin-
gle entry is identified with this entry. A diagonal
matrix is a matrix that has non-zero entries only
on its diagonal. For some vector X , by diag(X)
we denote the diagonal matrix that has the entries
of X on its diagonal. The identity matrix, denoted
by Id, is diag(1 . . . 1); the dimensions of Id will
always be clear from the context.

Let (P,Σ, J,Π, F ) be a wfsa. We may assume
w.l.o.g. that P = {1, . . . , |P |}. We will interpret
J as a 1×|P |matrix, F as a |P |×1 matrix, and we
will write Π(σ) for the |P |× |P |matrix defined as
(Π(σ))p,p′ = Π(p, σ, p′) for every σ ∈ Σ, p, p′ ∈
P .

Observation 9. Let N = (P,Σ, J,Π, F ) be a
wfsa and w = w1 . . . wk ∈ Σ∗. The weight of

p p

π1

π2

π3

π4

π5

π6

 
π1 · x

π2 · x

π3

π4

x
−1 · π5

x−1 · π6

Figure 5: Changing weights π1, . . . , π6 at state p
of a wfsa with a positive real x.

w under N can be alternatively calculated by

JN K(w) = J · ( |w|∏
i=1

Π(wi)
)
· F .

The idea of the next lemma is to locally change
weights of a wfsa without changing its semantics.
For this purpose, the weights of “incoming” tran-
sitions (including initial weights) of some state p
are scaled by some factor x while the weights of
“outgoing” transitions (including final weights) of
p are scaled by x−1. Weights of transitions from
p to p itself do not change. Figure 5 visualizes
this idea. Lemma 10 applies this idea to all states
simultaneously.

Lemma 10. Let N = (P,Σ, J,Π, F ) be a wfsa
and let X ∈ (R>0)|P |. Construct the wfsa

N ′ = (P,Σ, J · diag(X), Π ′, diag(X)−1 · F )

withΠ ′(σ) = diag(X)−1·Π(σ)·diag(X) for every
σ ∈ Σ. The wfsa N and N ′ are equivalent.

Proof. Let w ∈ Σ∗. If we calculate JN ′K(w) as
presented in Observation 9, it is easy to see that for
every factor diag(X) there is the adjacent factor
diag(X)−1 and vice versa. q.e.d.

Construction 11. Let N = (P,Σ, J,Π, F ) be
a convergent and w.l.o.g. reduced wfsa, and let
A =

∑
σ∈Σ Π

(σ). Then Id−A is invertible and
an out-probabilistic wfsa N ′ equivalent to N can
be constructed by applying Lemma 10 to N with
X = (Id−A)−1 · F .
Theorem 11. For every convergent wfsa there is
an equivalent out-probabilistic wfsa.

Proof. Note that
∑

w∈Σ∗JN K(w) = ∑j∈N J ·Aj ·
F . Hence, for every p, p′ ∈ P and i, k ∈ N we

7



have

∞ >
∑
j∈N

J ·Aj · F (by convergence)

≥
∑
j≥i+k

J ·Aj · F =
∑
j∈N

J ·Ai ·Aj ·Ak · F

≥
∑
j∈N

(J ·Ai)p · (Aj)p,p′ · (Ak · F )p′

= (J ·Ai)p ·
(∑
j∈N

(Aj)p,p′
)
· (Ak · F )p′ .

Since N is reduced, there are i, k ∈ N such that
(J · Ai)p > 0 and (Ak · F )p′ > 0; therefore∑

j∈N(A
j)p,p′ < ∞ for every p, p′ ∈ P . Hence,∑

j∈NA
j is a converging Neumann series. This

implies that the inverse of Id−A exists and is
equal to this sum; this seems to be a well known
result in the field of functional analysis; e.g. cf.
Heuser (2006, Thm. 12.4).

Now we need a vector X to apply Lemma 10
such that the resulting wfsa is out-probabilistic,
i. e., diag(X)−1 · A · diag(X) · (1 . . . 1)T +
diag(X)−1 · F = (1 . . . 1)T. This equation
can easily be transformed into (Id−A) ·X = F .
Since Id−A is invertible, the equation is solved
by X = (Id−A)−1 · F .

It remains to be shown that every entry of X
is strictly positive. Recall that every entry of A
is non-negative and that for every p ∈ P there is
a j ∈ N such that (Aj · F )p > 0. This implies
that every entry of (Id−A)−1 = ∑j∈NAj is non-
negative and that every entry of X = (

∑
j∈NA

j) ·
F is strictly positive. q.e.d.

Theorem 12. For every consistent wfsa there is an
equivalent probabilistic wfsa.

Proof. Since consistency implies convergence we
can apply Construction 11 and obtain N ′ =
(P,Σ, J ′, Π ′, F ′). By Lemma 10 N ′ is consis-
tent. It remains to be shown that N ′ is also semi-
probabilistic. By Construction 11 N ′ is already
out-probabilistic, so we just have to show that∑

p∈P J
′(p) = 1.

For p ∈ P , let Np = (P,Σ, Jp, Π ′, F ′) where
Jp is the 1 × |P | matrix where (Jp)p = 1 and
every other entry is 0. Obviously Np is semi-
probabilistic. Let N ′p be the equivalent reduced
wfsa that is created from Np just by removing
states. It is easy to see that N ′p is also semi-
probabilistic. Since N ′p is semi-probabilistic and

reduced, it is also consistent, hence, by equiva-
lency, also Np is consistent. That means

1 =
∑
w∈Σ∗

JNpK(w) = ∑
w∈Σ∗

Jp ·
( |w|∏
i=1

Π ′(wi)
)
· F ′

=
∑
w∈Σ∗

(( |w|∏
i=1

Π ′(wi)
) · F ′)

p
.

Since p was chosen arbitrarily, we have 1 =∑
w∈Σ∗JN ′K(w) = J ′ · (1 . . . 1)T. q.e.d.

Corollary 13. The class of weighted languages
recognizable by probabilistic wfsa is closed under
reversal.

Proof. A wfsa can easily be reversed by transpos-
ing the transition matrices and interchanging ini-
tial and final weights. The corollary follows by
Theorem 12. q.e.d.

Related Work. Paz (1971, Chapter III, Sec-
tion A, Theorem 1.8) presents the same result for
his probabilistic automata, which are slightly dif-
ferent from our probabilistic wfsa. His construc-
tion requires an exponential number of states in
comparison to the given automaton. Our defi-
nition of probabilistic wfsa allows the presented
construction, which does not change the state set
at all. Paz’ construction can be easily adapted to
our case, yet it is unclear if our construction can
also be adapted to his case.

4.2 Probabilities and Tree Automata

The notions probabilistic, semi-probabilistic, con-
vergent, consistent, and reduced can be easily gen-
eralized to wsta and wuta. Note that in the tree
case semi-probabilistic and reduced do not gener-
ally imply consistent. We now investigate what
happens to these properties when constructing a
wuta given a wsta or vice versa as presented in
Section 3.

If the input is reduced, so is the output, except
in case of mixed binarization when going from
wsta to wuta; yet the output can easily be re-
duced. In case of left-branching binarization, if
the input is semi-probabilistic, the output is also
semi-probabilistic. Note that in any case, if the
input is semi-probabilistic and reduced, then the
wfsa in the wuta are consistent. Hence, in case of
right-branching binarization, if the input is semi-
probabilistic and reduced, the output is reduced

8



and by Theorem 12 we can find an equivalent au-
tomaton that is also semi-probabilistic. In case of
mixed binarization the direction from wuta to wsta
is subsumed by the previous cases. Starting with a
semi-probabilistic and reduced wsta, we can apply
Construction 8, reduce the wfsa in the wuta with-
out breaking consistency, and apply Theorem 12,
ending up with a semi-probabilistic and reduced
wuta.

Theorems 7 and 8 imply that if the input is con-
vergent or even consistent, so is the output. Hence
in the previous paragraph we may replace semi-
probabilistic by probabilistic and the statements
still hold.

5 Outlook: Implications on Training
Methods

For each of three binarizations we have shown that
wsta together with a binarization are equally pow-
erful to wuta.

Our results suggest training methods for wuta:
By binarizing a wuta, training the resulting wsta,
and undoing the binarization, it is possible to
use training algorithms designed for wsta also on
wuta. The training may even alter the state be-
haviour, e.g. by splitting or merging states; cf. e.g.
Matsuzaki et al. (2005) and Petrov et al. (2006).
Merging (for example two) states q1 and q2 to a
new state q means to replace every occurrence of
q1 and q2 by q; the state set, the initial weights and
the transition weights have to be adapted appro-
priately. The opposite direction is the splitting of
a state q into (for example two) new states q1 and
q2, which means to replace every occurrence of q
by q1 or q2 in every possible combination, e.g., if
there is a transition with two occurrences of q be-
fore splitting, then there are four transitions with
different occurrences of q1 and q2 after splitting.
Note that q, q1, and q2 need to have the same sort,
otherwise there would be incompatibilities with
the sorts of the (unchanged) terminals after split-
ting or merging.

These results formally explain why the perfor-
mance of the training by Matsuzaki et al. (2005)
is rather independent from the used binariza-
tion. Note that they used probabilistic context-free
grammars with latent annotations (pcfg-la) while
we used wsta, but it is easy to see that both for-
malisms are equally powerful. Additionally our
binarizations use different node labels and intro-
duced additional unary nodes as well as NULL and

NULL; but again this does not change the power of
the formalism. Note that while changing latent an-
notations for pcfg-la it is not necessary to deal with
sorts, because a latent annotation is always consid-
ered together with the terminal1 it is attached to.

References
Steven Abney, David McAllester, and Fernando

Pereira. 1999. Relating probabilistic grammars
and automata. In Proceedings of the 37th Annual
Meeting of the Association for Computational Lin-
guistics on Computational Linguistics, ACL ’99,
page 542–549, Stroudsburg, PA, USA. Association
for Computational Linguistics. DOI: 10.3115/
1034678.1034759.

Julien Carme, Joachim Niehren, and Marc Tommasi.
2004. Querying unranked trees with stepwise tree
automata. In Vincent van Oostrom, editor, Rewrit-
ing Techniques and Applications, volume 3091 of
Lecture Notes in Computer Science, page 105–118.
Springer Berlin Heidelberg. DOI: 10.1007/
978-3-540-25979-4_8.

Zhiyi Chi. 1999. Statistical properties of probabilistic
context-free grammars. Computational Linguistics,
25(1):131–160, March. URL: http://dl.acm.
org/citation.cfm?id=973215.973219.

Hubert Comon, Max Dauchet, Remi Gilleron, Christof
Löding, Florent Jacquemard, Denis Lugiez, Sophie
Tison, and Marc Tommasi. 2007. Tree automata
techniques and applications, October. URL: http:
//tata.gforge.inria.fr/.

Manfred Droste and Heiko Vogler. 2011. Weighted
logics for unranked tree automata. Theory of Com-
puting Systems, 48(1):23–47. DOI: 10.1007/
s00224-009-9224-4.

Pierre Dupont, François Denis, and Yann Esposito.
2005. Links between probabilistic automata and
hidden Markov models: probability distributions,
learning models and induction algorithms. Pattern
Recognition, 38(9):1349–1371. Grammatical In-
ference. DOI: 10.1016/j.patcog.2004.03.
020.

Zoltán Fülöp and Heiko Vogler, 2009. Hand-
book of Weighted Automata, chapter Weighted Tree
Automata and Tree Transducers, pages 313–403.
Springer Berlin Heidelberg, Berlin, Heidelberg.
DOI: 10.1007/978-3-642-01492-5_9.

Harro Heuser. 2006. Funktionalanalysis / The-
orie und Anwendung. Teubner, fourth edi-
tion. URL: https://www.springer.com/
9783835100268.
1We stick to the notion of terminal as it is used in this

paper. In the context of pcfg-la, nullary terminals are called
terminal symbols while the other terminals are called non-
terminal symbols.

9



Johanna Högberg, Andreas Maletti, and Heiko Vogler.
2009. Bisimulation minimisation of weighted au-
tomata on unranked trees. Fundamenta Informati-
cae, 92(1-2):103–130, January. DOI: 10.3233/
FI-2009-0068.

Takuya Matsuzaki, Yusuke Miyao, and Jun’ichi Tsu-
jii. 2005. Probabilistic CFG with latent annotations.
In Proceedings of the 43rd Annual Meeting on As-
sociation for Computational Linguistics, ACL ’05,
pages 75–82, Stroudsburg, PA, USA. Association
for Computational Linguistics. DOI: 10.3115/
1219840.1219850.

Mark-Jan Nederhof and Giorgio Satta. 2003. Proba-
bilistic parsing as intersection. In 8th International
Workshop on Parsing Technologies, page 137–148.

Azaria Paz. 1971. Introduction to Proba-
bilistic Automata. Academic Press. URL:
http://www.sciencedirect.com/
science/book/9780125476508.

Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
the 21st International Conference on Computational
Linguistics and the 44th annual meeting of the As-
sociation for Computational Linguistics, ACL-44,
pages 433–440, Stroudsburg, PA, USA. Association
for Computational Linguistics. DOI: 10.3115/
1220175.1220230.

10


