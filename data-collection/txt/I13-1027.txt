










































A Noisy Channel Approach to Error Correction in Spoken Referring Expressions


International Joint Conference on Natural Language Processing, pages 234–242,
Nagoya, Japan, 14-18 October 2013.

A Noisy Channel Approach to Error Correction in
Spoken Referring Expressions

Su Nam Kim, Ingrid Zukerman, Thomas Kleinbauer and Farshid Zavareh
Faculty of Information Technology, Monash University

Clayton, Victoria 3800, Australia

Abstract
We offer a noisy channel approach for rec-
ognizing and correcting erroneous words
in referring expressions. Our mechanism
handles three types of errors: it removes
noisy input, inserts missing prepositions,
and replaces mis-heard words (at present,
they are replaced by generic words). Our
mechanism was evaluated on a corpus of
295 spoken referring expressions, improv-
ing interpretation performance.

1 Introduction
One of the main stumbling blocks for Spoken Di-
alogue Systems (SDSs) is the lack of reliability
of Automatic Speech Recognizers (ASRs) (Pelle-
grini and Trancoso, 2010). Recent research proto-
types of ASRs yield Word Error Rates (WERs) be-
tween 15.6% (Pellegrini and Trancoso, 2010) and
18.7% (Sainath et al., 2011) for broadcast news.
However, the commercial ASR employed in this
research had a WER of 30% and a Sentence Er-
ror Rate (SER) (proportion of sentences for which
no correct textual transcription was produced) of
65.3% for descriptions of household objects.

In addition to mis-recognized entities or actions,
ASR errors often yield ungrammatical sentences
that cannot be processed by subsequent interpreta-
tion modules of an SDS, e.g., “the blue plate” be-
ing mis-heard as “to build played”, and hesitations
(e.g., “ah”s) being mis-heard as “and” or “on” —
all of which happened in our trials.

In this paper, we offer a general framework for
error detection and correction in spoken utterances
that is based on the noisy channel model, and
present a first-stage implementation of this frame-
work that performs simple corrections of referring
expressions. Our model is implemented as a pre-
processing step for the Scusi? spoken language in-
terpretation system (Zukerman et al., 2008; Zuk-
erman et al., 2009).

Table 1: Spoken, heard and labeled descriptions.

Spoken: the stool to the left of the table
Heard: the storm the left of the table
Labels: Object Prep Specifier Landmark
Spoken: the plate in the microwave
Heard: to play it in the microwave
Labels: Object Noise Prep Landmark

The idea of the noisy channel model is that
a message is sent through a channel that intro-
duces errors, and the receiver endeavours to re-
construct the original message by taking into ac-
count the characteristics of the noisy channel and
of the transmitted information (Ringger and Allen,
1996; Brill and Moore, 2000; Zwarts et al., 2010).
The system described in this paper handles three
types of errors: noise (which is removed), missing
prepositions (which are inserted), and mis-heard
words (which are replaced). Table 1 shows two
descriptions that illustrate these errors. The first
row for each description displays what was spo-
ken, the second row displays what was heard by
the ASR, and the third row shows the semantic la-
bels assigned to each segment in the description
by a shallow semantic parser (Section 3.2). Specif-
ically, in the first example, the preposition “to”
is missing, and the object “stool” is mis-heard as
“storm”; and in the second example “the plate” is
mis-heard as “to play”, and the noisy “it” has been
inserted by the ASR.

Ideally, we would like to replace mis-heard
words with phonetically similar words, e.g., use
“plate” instead of “play”. However, at present, as a
first step, we replace mis-heard words with generic
options, e.g., “thing” for an object or landmark.
Further, we insert the generic preposition “at” for
a missing preposition. Thus, we deviate from the
noisy channel approach in that we do not quite re-
construct the original message. Instead, we con-
struct a grammatically correct version of this mes-
sage that enables the generation of reasonable in-
terpretations (rather than no interpretation or non-

234



sensical ones). For example, the mis-heard de-
scription “to play it in the microwave” in Table 1 is
modified to “the thing in the microwave”. Clearly,
this is not what the speaker said, but hopefully, this
modified text, which describes an object, rather
than an action, enables the identification of the in-
tended object, e.g., a plate, or at least a small set of
candidates, in light of the rest of the description.

Our mechanism was evaluated on a corpus of
295 spoken referring expressions, significantly im-
proving the interpretation performance of the orig-
inal Scusi? system (Section 6.3).

The rest of this paper is organized as follows. In
the next section, we discuss related work. In Sec-
tion 3, we outline the design of our system. Our
probabilistic model is described in Section 4, fol-
lowed by the noisy channel error correction proce-
dure. In Section 6, we discuss our evaluation, and
then present concluding remarks.

2 Related Research

This research combines three main elements: cor-
rection of ASR output, noisy channel models and
shallow semantic parsing.

López-Cózar and Griol (2010) used lexical ap-
proaches to replace, insert or delete words in a tex-
tual ASR output, and syntactic approaches to mod-
ify tenses of verbs and grammatical numbers to
better match grammatical expectations. However,
these actions make ad hoc changes.

The noisy channel model has been employed
for various NLP tasks, such as ASR output cor-
rection (Ringger and Allen, 1996), spelling cor-
rection (Brill and Moore, 2000), and disfluency
correction (Johnson and Charniak, 2004; Zwarts
et al., 2010). Our approach differs from the tra-
ditional noisy channel approach in that it uses a
word-error classifier to model the noisy channel,
and semantic information to model the input char-
acteristics.

Shallow semantic parsers for SDSs have been
used in (Coppola et al., 2009; Geertzen, 2009).
Coppola et al. (2009) used FrameNet (Baker et
al., 1998) to detect and filter the frames for tar-
get words, and employed a Support Vector Ma-
chine (SVM) classifier to perform semantic label-
ing. Geertzen (2009) used a shallow parser to de-
tect semantic units only when a dependency parser
failed to produce a parse tree. In contrast, our shal-
low semantic parser is part of a noisy channel
model that post-processes the output of an ASR.

3 System Design
Our error correction procedure (Section 5) re-
ceives as input alternatives produced by an ASR,
and generates modified versions of these alterna-
tives. It employs the following modules: (1) a clas-
sifier that determines whether a word in a text pro-
duced by the ASR is correct; (2) a shallow seman-
tic parser (SSP) that assigns semantic labels to seg-
ments in the text; and (3) a noisy channel error cor-
rection mechanism that decides which alterations
should be made to the ASR output on the basis of
the information provided by the other two mod-
ules. The resultant texts are given as input to the
Scusi? spoken language interpretation system.

In this section, we describe the word error clas-
sifier and SSP together with our semantic labels,
and report on their performance. We also provide
a brief outline of the Scusi? system.

The performance of the classifier and SSP was
evaluated in terms of accuracy over the corpus
constructed to evaluate the Scusi? system (Klein-
bauer et al., 2013). This corpus comprises 400
spoken descriptions generated by 26 speakers. We
performed 13-fold cross-validation, where each
fold contains two speakers (Section 6.1).

3.1 Word error classifier
We investigated three classifiers to determine
whether a word in the ASR textual output
is correct: the Weka implementation of Deci-
sion Trees (Quinlan, 1993) and Naïve Bayes
classifiers (Domingos and Pazzani, 1997) (cs.
waikato.ac.nz/ml/weka/), and the Mallet im-
plementation of the linear chain Conditional Ran-
dom Fields (CRF) algorithm (Lafferty et al., 2001)
(mallet.cs.umass.edu).

The best performance was obtained by the De-
cision Tree, which yielded an average accuracy of
80.9% over the 13 folds. The most influential fea-
tures were rr(w, d) and Part-of-Speech (PoS) tag
of the current word w in levels 1 and 2 of the De-
cision Tree respectively, where rr is the repetition
ratio of the current wordw in the textual ASR out-
puts for description d:

rr(w,d)=
# of ASR outputs for d that contain w

# of alternative ASR outputs for d .

3.2 Shallow Semantic Parser (SSP)
We found the following semantic labels useful for
referring expressions:

• Object – a lexical item designating an object,
optionally preceded by a determiner and one

235



or more gerunds, adjectives or nouns, e.g., “the
blue ceramic drinking mug”.

• Preposition – a preposition or prepositional ex-
pression, e.g., “on” or “further away from”.

• Landmark – same pattern as Object, but a de-
scription may contain more than one landmark,
e.g., “the mug on the table in the corner”.

• Noise – sighs or hesitations that are often mis-
heard by the ASR as “and”, “on” or “in”.

• Specifier – a further specification that normally
precedes a Landmark, e.g., “the center of”,
“front of” or “the left of”. The preposition “of”
at the end of a Specifier that precedes a Land-
mark is always required.

• Additional – words that are often superfluous,
e.g., “the mug that is on the table”.

We employed the Mallet implementation of the
linear chain Conditional Random Fields (CRF) al-
gorithm (Lafferty et al., 2001) to learn sequences
of semantic labels (mallet.cs.umass.edu).

Accuracy over texts and segments was respec-
tively measured as follows:

# of texts with perfectly matched label sequences
total # of texts

# of segments with perfectly matched labels
total # of segments

.

The CRF was trained separately for textual tran-
scriptions of spoken descriptions and for ASR out-
puts. Two annotators labeled the 400 transcribed
texts, and 800 samples from the ASR output: 400
from the best output and 400 from the worst. The
first annotator segmented and labeled the descrip-
tions, and the second annotator verified the annota-
tions; disagreements were resolved by consensus.

We considered the features found useful in the
CoNLL2001 shared task (http://www.cnts.ua.
ac.be/conll2000/chunking/). The features that
yielded the best performance were current word,
current PoS and previous word, achieving an ac-
curacy of 92% over the 400 textual transcriptions,
and 76.13% over the 800 ASR outputs. Accuracy
over segments was higher, at 96.26% for texts, and
87.28% for ASR outputs. However, SSP’s perfor-
mance for the identification of Noise was rather
poor, with an average accuracy of 54.75%.

3.3 Scusi?
Scusi? is a system that implements an anytime,
probabilistic mechanism for the interpretation of

spoken utterances, focusing on a household con-
text. It has four processing stages, where each
stage produces multiple outputs for a given input,
early processing stages may be probabilistically
revisited, and only the most promising options at
each stage are explored further.

The system takes as input a speech signal,
and uses an ASR (Microsoft Speech SDK 6.1)
to produce candidate texts. Each text is as-
signed a probability given the speech wave.
The second stage applies Charniak’s prob-
abilistic parser (http://bllip.cs.brown.edu/
resources.shtml#software) to syntactically an-
alyze the texts in order of their probability, yield-
ing at most 50 different parse trees per text. The
third stage applies mapping rules to the parse
trees to generate concept graphs (Sowa, 1984)
that represent the semantics of the utterance.
The final stage instantiates the concept graphs
within the current context. For example, given
a parse tree for “the blue mug on the table”,
the third stage returns the uninstantiated concept
graph mug(COLOR: blue) – on – table. The final
stage then returns candidate instantiated concept
graphs, e.g., mug1–location_on–table2, mug2–
location_on–table1. The probability of each in-
stantiated concept graph depends on (1) how well
the objects and relations in this graph match the
corresponding objects and relations in the unin-
stantiated concept graph (e.g., whether mug1 is a
mug, and whether it is blue); and (2) how well the
relations in this graph match the relations in the
context (e.g., whether mug1 is indeed on table2).

4 Probability Estimation

We use a distance measure inspired by the Mini-
mum Message Length (MML) principle (Wallace,
2005) to estimate the goodness of a message and
its semantic model. This principle is normally
used for model selection, based on the following
formulation:
Pr(data&model) = Pr(data|model)× Pr(model) ,
which strikes a balance between model complex-
ity and data fit, i.e., the highest-probability model
that best explains the data is the best model over-
all. That is, the best model is not necessarily the
model that fits the data best, as such a model may
over-fit the data; the model itself must also have
a high prior probability. In our case, the data is a
text, either heard by the ASR or modified, and the
model is a sequence of semantic labels. At present,

236



our model is restricted to semantic labels for seg-
ments in referring expressions, but in the future we
will use this formalism to compare models repre-
senting different dialogue acts, e.g., commands.

Our use of the MML principle differs from its
normal usage in that we employ it to compare a
text and its semantic model with a modified ver-
sion of this text and its own semantic model (rather
than comparing two models that try to account for
the same text). Modifications attract a penalty that
depends on the probability that they are required
(the higher the probability, the lower the penalty).
This penalty is applied to prevent arbitrary modi-
fications where a system hears what it expects.

Below we describe the estimation of the proba-
bility of a text and its semantic model. The next
section describes the combination of the noisy
channel model with the word-error classifier, SSP,
and the modifications made to texts.

The joint probability of a Text and its Semantic
Model is estimated as follows:

Pr(Text&SemModel) =
Pr(Text|SemModel)× Pr(SemModel) ,

where
• Pr(SemModel) =

Pr(SSP)×
N+2∏
i=0

Pr(Li|L0, . . . , Li−1) ,

where Pr(SSP) reflects SSP’s confidence in the
sequence of semantic labels it produced for
Text, N is the number of segments in the se-
quence, Li is the label for segment i, L−1 and
L0 are the special labels Beginning, and LN+1
and LN+2 are the special labels End. To make
this calculation tractable, we employ trigrams,
i.e., Pr(Li|L0, . . . , Li−1) ∼= Pr(Li|Li−2, Li−1).

• Pr(Text|SemModel) =
N∏

i=1

Pr(texti|Li) ,

where texti is the sequence of words in segment i,
and Pr(texti|Li) is estimated as follows:

Pr(texti|Li) =
Mi∏
j=1

Pr(HWordji|Li) ,

where Mi is the number of words in texti, and
HWordji is the jth heard word in texti.

Owing to the relatively small size of our corpus,
Pr(HWordji|Li) is roughly estimated as follows:
Pr(HWordji|Li) =∑Tji

k=1 Pr(HWordji|XpctPoSkji)Pr(XpctPoSkji|Li),
where XpctPoSkji is a PoS expected at position j
in segmenti, and Tji is the number of PoS expected

at position j in segmenti. Pr(HWordji|XpctPoSkji)
is obtained from a corpus, and Pr(XpctPoSkji|Li)
is estimated from our textual transcriptions of spo-
ken descriptions, except for the PoS associated
with Noise, which are estimated from our spoken
corpus (there is no Noise in texts). We obtain a
rough estimate of Pr(XpctPoSkji|Li) by consider-
ing three positions in a segment: first, middle (in-
termediate positions) and last. For instance, the
possible PoS for the first position of an Object
or Landmark are determiner, adjective, gerund,
verb(past) or noun.

To illustrate this calculation, consider the sec-
ond description in Table 1, which is heard as “to
play it in the microwave”. The probability of the
Semantic Model for this description is
Pr(SemModel) = Pr(O|B,B) Pr(N |O,B)
Pr(P |N,O) Pr(L|P,N) Pr(E|L,P ) Pr(E|E,L) .

All the probabilities involving Noise are set to
an arbitrarily low �, which yields

Pr(O|B,B) Pr(E|L,P ) Pr(E|E,L) �3 .
The probability of the Text given the Semantic

Model is
Pr(Text|SemModel) = Pr(“to play”|O) Pr(“it”|N)

Pr(“in”|P ) Pr(“the microwave”|L) ,
which is quite high for “it”|N, “in”|P and “the
microwave”|L, but is reduced due to the mis-
match between the PoS of “to play” (TO VB)
and the PoS expected by an Object, which are:
DT/JJ/VBG/VBD/NN for the first position, and
NN for the last position (Section 5.1.3).

Our system modifies this heard description by
replacing “to play” with “the thing” and removing
the noisy “it”, which yields “the thing in the
microwave” (Section 5). The probability of the
Semantic Model for this modified sentence is
Pr(SemModel′) = Pr(O|B,B) Pr(P |O,B)

Pr(L|P,O) Pr(E|L,P ) Pr(E|E,L) ,
which is higher than that of the original Semantic
Model, as is the probability of the new Text given
the new Semantic Model:

Pr(Text′|SemModel′) = Pr(“the thing”|O)
Pr(“in”|P ) Pr(“the microwave”|L) .

However, this gain is offset by the penalties
incurred by the modifications. The estimation of
these penalties is described in the next section.

5 Noisy Channel Error Correction
Given a textual output produced by an ASR, we
apply Algorithm 1 to remove noise, insert prepo-

237



sitions and replace wrong words. The probability
of the resultant text and its semantic model is re-
calculated after each change as described in Sec-
tion 4, and is moderated by the probability of the
penalty for the change. Since a modification may
yield a text where SSP identifies Noise, the Noise
removal step is repeated after every change.

After each modification, the probability of the
original text and semantic model is compared with
the probability of the new text, its semantic model
and any incurred penalties. The winning text and
semantic model (without penalties) are then taken
as the originals for the next modification. Upon
completion of this process, all the incurred penal-
ties are re-incorporated into the final probability of
a modified text, in order to enable a fair compari-
son with other texts that were not altered.

The application of this process to all the texts
produced by an ASR for a particular utterance
may yield identical texts (e.g., when words with
unexpected PoS are converted to “thing”). These
texts are merged, and their probabilities are re-
calculated. The resultant texts are ranked in de-
scending order of probability and ascending order
of the number of replaced words (i.e., texts with
fewer replacements are ranked ahead of texts with
more replacements, irrespective of their probabil-
ity). The final probabilities are adjusted to reflect
the ranking of a text.

5.1 Estimating penalties from modifications
The modifications performed by our system at-
tract a penalty that depends on the probability
that the relevant portion of a heard utterance is
wrong. The higher this probability, the lower the
penalty, which is implemented as a multiplier of
Pr(Text&SemModel).

5.1.1 Removing noise
The penalty for removing a heard word j in
segmenti that is labeled as Noise by SSP is es-
timated on the basis of its probability of being
Wrong (obtained from the word-error classifier,
Section 3.1), as follows:

Pr(remove HWordji) = (1){
Pr(IsW(HWordji))Pr(Class) if label = W
(1−Pr(IsC(HWordji)))Pr(Class) if label = C

where Pr(Class) is the accuracy of the classifier
(on training data), Pr(IsW(HWordji)) is the prob-
ability assigned by the classifier to heard word j
in segmenti being Wrong, and Pr(IsC(HWordji))

Algorithm 1 Noisy channel ASR error correction
Require: Text
1: SemModel← Run SSP on Text
2: Calculate Pr(Text&SemModel) (Section 4)

{ REMOVE NOISE }
3: while there is Noise do
4: Text’← Remove Noise from Text
5: SemModel’← Run SSP on Text’
6: Calculate Pr(Text′&SemModel′)
7: Text&SemModel← arg max{Pr(Text&SemModel) ,
8: Pr(Text′&SemModel′)Pr(Removal)}
9: end while

{ INSERT PREPOSITIONS }
10: while a preposition is missing do
11: Text’← Insert missing preposition into Text
12: SemModel’← Run SSP on Text’
13: Text’← Remove Noise from Text’ (Steps 3-9)
14: Calculate Pr(Text′&SemModel′)
15: Text&SemModel← arg max{Pr(Text&SemModel) ,
16: Pr(Text′&SemModel′)Pr(Insertion)}
17: end while

{ REPLACE WRONG WORDS }
18: for i=1 to N do
19: Text’← Replace wrong words in segmenti
20: SemModel’← Run SSP on Text’
21: Text’← Remove Noise from Text’ (Steps 3-9)
22: Calculate Pr(Text′&SemModel′)
23: Text&SemModel← arg max{Pr(Text&SemModel) ,
24: Pr(Text′&SemModel′)Pr(Replacement)}
25: end for
26: Pr(Text&SemModel)← Pr(Text&SemModel)
27: Pr(Removal)Pr(Insertion)Pr(Replacement)

is the probability of this word being Correct (the
last two probabilities add up to 1).

The rationale for this formula is that if SSP
deems a heard word to be Noise, and the clas-
sifier labels it Wrong with high probability, then
its removal should cause only a small reduction
in Pr(Text&SemModel). Conversely, if a heard
word deemed to be Noise by SSP is labeled Cor-
rect by the classifier with high probability, then
its removal should cause a large reduction in
Pr(Text&SemModel). In both cases, the probabili-
ties assigned to the labels by the classifier are mod-
erated by the classifier’s accuracy.

To illustrate this process, let’s return to the ex-
ample “to play it in the microwave”, where “it”
is labeled Noise by SSP, and Wrong by the clas-
sifier with probability Pr(IsW(“it”)). A new text
Text’ is obtained as a result of the removal of “it”,
and the penalty Pr(IsW(“it”)) Pr(Class) is multi-
plied by the new Pr(Text′&SemModel′).

5.1.2 Inserting a preposition
If a preposition is not found in a position where
one is expected, e.g., between an Object and
Landmark or between an Object and a Specifier,
we insert a generic preposition “at”. The penalty

238



for the insertion of a preposition depends on the
probability that the ASR failed to hear an uttered
preposition, which is estimated as follows:
Pr(insert Pi) = Pr(Pi appears in Text and doesn’t

appear in the ASR output for Text) ,
where Pi is a preposition in position i in Text.

To determine the frequency of this event, we
employ an edit distance algorithm that aligns the
texts produced by the ASR with their correspond-
ing textual transcriptions. This was done for 800
alternatives produced by the ASR (400 best and
400 worst), yielding a probability of 0.02 of the
ASR dropping a preposition. The corresponding
penalty for inserting a preposition (0.02) is hope-
fully offset by the increase in Pr(SemModel′) as a
result of this insertion. For instance, the probabil-
ity of the Semantic Model for the heard descrip-
tion (without a preposition) in the first example in
Table 1 is

Pr(SemModel) = Pr(O|B,B) Pr(S|O,B)
Pr(L|S,O) Pr(E|L, S) Pr(E|E,L) ,

where Pr(S|O,B) and Pr(L|S,O) are low, as they
are ungrammatical. After adding the preposition,
Pr(SemModel′) = Pr(O|B,B) Pr(P |O,B)

Pr(S|P,O) Pr(L|S, P ) Pr(E|L, S) Pr(E|E,L) .
Although the new expression has an extra fac-

tor, the probabilities of the new factors are higher
than those of their original counterparts.

5.1.3 Replacing a word
The decision to replace a word is based on the
match between expected PoS and the PoS of a
heard word. If they match, no replacement is per-
formed. Otherwise, replacements are performed
by applying the following rules, which are based
on the PoS expected by the different types of seg-
ments at each position (first, middle, last).

• Objects and Landmarks – The expected
PoS for Objects and Landmarks are:
DT/JJ/VBG/VBD/NN for the first word,
JJ/VBG/VBD/NN for the middle words, and
NN for the last word. Thus, if there is a PoS
mismatch, we perform the following replace-
ments (if there is only one word in an Object or
Landmark, we replace it with “thing” (NN)):

–HWord1⇒“the” (DT)
–HWordmid⇒“unknown” (JJ) (multiple times)
–HWordlast⇒“thing” (NN)

To illustrate this process, consider the heard
Object “to:TO battle:NN played:VB”, which

is replaced with “the:DT battle:NN thing:NN”.
Even though “battle” is incorrect, it is not mod-
ified, as its PoS is expected. However, Scusi?
can cope with such unknown object attributes.

• Prepositions and Prepositional Phrases –
This segment is more restricted than Objects
and Landmarks, as it is largely composed of
closed class words. We therefore use edit dis-
tance to find the prepositional phrase in the cor-
pus of textual transcriptions that best matches
the words in a heard prepositional phrase. The
phrase from the corpus then replaces the heard
segment. If there is no best-matching preposi-
tional phrase, the generic “at” is used as a re-
placement. For example, “for the wave from”
is replaced with “further away from” (with
“from” being the next-best match), while “a
all” is replaced with “at”.

• Specifiers – This segment is similar to Ob-
jects and Landmarks plus a final “of” when
it precedes a Landmark (about 5% of the de-
scriptions had Specifiers without Landmarks).
In addition, the head noun, which is normally
the penultimate word in a Specifier, must be
a positional noun, such as “center”, “edge” or
“corner”. Thus, a word is replaced if a PoS mis-
match occurs or the penultimate word is not an
expected positional noun, as follows:

–HWord1⇒ “the” (DT)
–HWordmid⇒“unknown” (JJ) (multiple times)
–HWordlast−1⇒ “position” (NN)
–HWordlast⇒ “of” (IN, preposition)

For instance, given the Specifier “the:DT
ride:NN into:IN” followed by a Landmark,
“of:IN” is appended, and “into:IN” is replaced
with “position:NN”, yielding “the:DT ride:NN
position:NN of:IN”. Clearly, other replacement
options are possible, which will be investigated
in the future.

In principle, the penalty for replacing a word
should depend on both the probability that it is
wrong (as for noise removal) and on the similar-
ity between the wrong word and the proposed re-
placement. That is, the higher the probability that
a word is wrong, and the higher the similarity be-
tween the original word and the replacement, the
lower the penalty for the replacement. However,
at present, we replace words that do not match an

239



expected PoS only with generic options, e.g., “un-
known” for expected adjectives, “thing” for ex-
pected nouns in Objects and Landmarks, and “po-
sition” for expected positional nouns in Specifiers.
Thus, our penalty consists only of the first of the
above factors moderated by a generic similarity
factor δ(= 0.5), as follows:

Pr(replace HWordji) = (2){
δ Pr(IsW(HWordji))Pr(Class) if label = W
δ (1−Pr(IsC(HWordji)))Pr(Class) if label = C

In the future, the generic δ will be replaced by a
function of the similarity between an original word
and its candidate replacements.

6 Evaluation
In this section, we describe our corpus and eval-
uation metrics, and compare the results obtained
with Scusi? plus error correction with those ob-
tained by the original Scusi? system.

6.1 Corpus
Our model’s performance was evaluated using part
of the corpus constructed to evaluate the Scusi?
system (Kleinbauer et al., 2013). The original cor-
pus comprises 432 free-form descriptions spoken
by 26 trial subjects to refer to 12 designated ob-
jects in four scenarios (three objects per scenario,
where a scenario contains between 8 and 16 ob-
jects; participants repeated or rephrased some de-
scriptions). 32 descriptions could not be processed
by the ASR, and 105 contained constructs that
could not be represented by Scusi?. The remain-
ing 295 descriptions were used in our evaluation.

The descriptions, which varied in length and
complexity, had an average description length of
10 words. Sample descriptions are: “the green
plate next to the screwdriver at the top of the ta-
ble”, “the large pink ball in the middle of the
room”, “the plate on the corner of the table” and
“the computer under the table”.

6.2 Evaluation metrics
We use the evaluation metrics discussed in (Klein-
bauer et al., 2013), viz %NotFound@N , the per-
centage of descriptions that have no correct inter-
pretation within the top N ranks; Fractional Re-
call@N (FRecall@N ), which represents the fact
that the ranked order of equiprobable interpreta-
tions is arbitrary; and Normalized Discounted Cu-
mulative Gain@N (NDCG@N ), which discounts
interpretations with higher (worse) ranks (Järvelin

and Kekäläinen, 2002). The last two metrics are
defined as follows:

FRecall@N(d) =

∑N
j=1 fc(Ij)

|C(d)|
,

where d is a description, C(d) is the set of correct
interpretations for d, Ij is an interpretation gener-
ated by Scusi? at rank j, and fc is the fraction of
correct interpretations among those with the same
probability as Ij (this is a proxy for the probability
that Ij is correct):

fc(Ij) =
cj

hj − lj + 1
,

where lj is the lowest rank of all the interpreta-
tions with the same probability as Ij , hj the high-
est rank, and cj the number of correct interpreta-
tions between rank lj and hj inclusively.

DCG@N allows the definition of a relevance
measure for a result, and divides this measure by
a logarithmic penalty that reflects the rank of the
result. Using fc(Ij) as a measure of the relevance
of interpretation Ij , we obtain

DCG@N(d) = fc(I1) +
N∑

j=2

fc(Ij)
log2 j

.

This score is normalized to the [0, 1] range by
dividing it by the score of an ideal answer where
|C(d)| correct interpretations are ranked in the top
|C(d)| places, yielding

NDCG@N(d) =
DCG@N(d)

1 +
∑min{|C(d)|,N}

j=2
1

log2 j

6.3 Results

Table 2 compares the performance of the origi-
nal Scusi? system with that of Scusi? plus error
correction, and displays the performance obtained
for three types of modifications: N+P, P+R and
N+P+R, where N stands for noise removal, P for
preposition insertion, and R for word replacement
(preposition insertion was folded into all the op-
tions, as it happens in only 2% of the cases). The
table shows the average of %NotFound, FRecall
and NDCG for the 295 descriptions in our corpus.
The best performance is boldfaced.

As seen in Table 2, Scusi? plus error cor-
rection with word replacement generally out-
performs the original Scusi? system (the Ob-
ject/Landmark replacement has the greatest im-
pact on performance among the three types of
word replacements). Scusi?+N+P+R yields the
best overall performance for FRecall@∞ and

240



Table 2: Performance comparison: original Scusi?
versus Scusi? + Noisy Channel Error Correction.

Average of Scusi? Noisy Channel Error Correction
N+P P+R N+P+R

%NotFound@∞ 22.37% 22.03% 14.24% 13.90%
%NotFound@20 28.14% 28.47% 23.39% 24.41%
%NotFound@10 31.86% 31.19% 24.75% 26.78%
%NotFound@3 37.97% 40.00% 32.88% 36.27%
%NotFound@1 44.75% 47.80% 40.00% 44.41%

FRecall@∞ 0.776 0.778 0.858 0.859
FRecall@20 0.709 0.699 0.753 0.741
FRecall@10 0.667 0.662 0.731 0.712
FRecall@3 0.598 0.567 0.636 0.600
FRecall@1 0.488 0.462 0.508 0.481
NDCG@∞ 0.641 0.626 0.688 0.666
NDCG@20 0.628 0.610 0.669 0.644
NDCG@10 0.617 0.601 0.663 0.636
NDCG@3 0.589 0.562 0.624 0.591
NDCG@1 0.516 0.490 0.538 0.511

%NotFound@∞ (statistically significantly better
than Scusi? with p<<0.01 for the Wilcoxon
signed rank test), while Scusi?+P+R yields the
best performance for the remaining measures (sta-
tistically significantly better than Scusi? for FRe-
call@∞,20,10,3, NDCG@∞,20,10,3 and all val-
ues of %NotFound; and statistically significantly
better than Scusi?+N+P+R for FRecall@3,1,
all values of NDCG and %NotFound@3,1,
p≤0.05). The fact that Scusi?+N+P+R outper-
forms Scusi?+P+R only for %NotFound@∞ and
FRecall@∞ indicates that while the combination
of noise removal with the other corrections enables
Scusi? to find additional correct interpretations,
these interpretations tend to appear in high (bad)
ranks. The performance of Scusi?+N+P is gener-
ally worse than that of the original Scusi? system
— a disappointing outcome that may be attributed
to the low accuracy of SSP in the identification of
Noise (54.75%, Section 3.2).

Further examination of our results reveals the
following types of errors: (1) ASR errors that ren-
dered a description unprocessable by other stages,
e.g., “the green plate next to the hammer” heard as
“degree in applied next to him are”, and “the pic-
ture above the table” heard as “the picture of the
that”; (2) ASR errors that were not corrected, as
the PoS was expected, e.g., “the center off/IN the
table”; (3) wrong expression replacements, e.g.,
“the plate:O | next to scholar of:P” corrected as
“the plate:O | next to:P”; and (4) out of vocabulary
terms, e.g., “motherboard” and “frame”.

An interesting pattern emerges when con-
sidering ASR errors. Both Scusi?+N+P+R and
Scusi?+P+R outperform the original version of

Table 3: Performance broken down by SER.

ASR output Average of Scusi? P+R N+P+R
all wrong %NotFound@1 61.66% 52.85% 54.40%
(193 desc.) %NotFound@10 44.56% 33.68% 35.75%
one correct %NotFound@1 12.75% 15.69% 25.50%
(102 desc.) %NotFound@10 7.84% 7.84% 9.80%

Scusi? for the 193 descriptions with no correct
textual interpretation (SER = 65.3%, Section 1),
while the original version of Scusi? performs at
least as well as the best option, Scusi?+P+R, for
the 102 descriptions where a correct textual inter-
pretation was found (Table 3). This indicates that
SSP is over-zealous in finding errors, and its per-
formance must be further investigated, or another
mode of operation considered (e.g., retaining both
the original and the modified ASR output).

7 Discussion and Future Work

We have offered a noisy channel approach for er-
ror correction in spoken utterances, with a first-
stage implementation that corrects errors by re-
moving noise, inserting prepositions, and replac-
ing wrong words with generic terms. Our ap-
proach yields significant improvements in inter-
pretation performance, and shows promise for
achieving further improvements with more sophis-
ticated interventions.

The structure of referring expressions is rather
rigid in terms of the order of the semantic seg-
ments. To test the general applicability of our
noisy channel model, we propose to consider other
types of dialogue acts, and take into account the
expectations from the dialogue, e.g., “to play a
CD” is modified when it is considered a mis-heard
description, but if it were a response to the ques-
tion “what would you like me to do?”, no changes
would be required. In addition, we will extend our
approach to propose specific, rather than generic,
word replacements, and to handle superfluous in-
formation (i.e., information that is meaningless to
the language interpretation module) or missing in-
formation (e.g., missing landmarks). Another av-
enue of research involves versions of Scusi? that
employ SSP as an alternative to or in combination
with a syntactic parser.

Acknowledgments
This research was supported in part by grants
DP110100500 and DP120100103 from the Aus-
tralian Research Council. The authors thank Ma-
sud Moshtaghi for his help with statistical issues.

241



References
C.F. Baker, C.J. Fillmore, and J.B. Lowe. 1998. The

Berkeley FrameNet project. In COLING’98 – Pro-
ceedings of the 17th International Conference on
Computational Linguistics, pages 86–90, Montreal,
Canada.

E. Brill and R.C. Moore. 2000. An improved er-
ror model for noisy channel spelling correction. In
ACL’2000 – Proceedings of the 38th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 286–293, Hong Kong.

B. Coppola, A. Moschitti, and G. Riccardi. 2009. Shal-
low semantic parsing for spoken language under-
standing. In NAACL-HLT 2009 – Proceedings of
Human Language Technologies: The Annual Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics, pages 85–88,
Boulder, Colorado.

P. Domingos and M. Pazzani. 1997. On the optimal-
ity of the simple Bayesian classifier under zero-one
loss. Machine Learning, 29:103–130.

J. Geertzen. 2009. Semantic interpretation of dutch
spoken dialogue. In IWCS-8 – Proceedings of the
8th International Conference on Computational Se-
mantics, pages 286–290, Tilburg, The Netherlands.

K. Järvelin and J. Kekäläinen. 2002. Cumulated gain-
based evaluation of IR techniques. ACM Trans-
actions on Information Systems (TOIS), 20(4):422–
446.

M. Johnson and E. Charniak. 2004. A TAG-based
noisy channel model of speech repairs. In ACL’04 –
Proceedings of the 42nd Annual Meeting of the As-
sociation for Computational Linguistics, pages 33–
39, Barcelona, Spain.

Th. Kleinbauer, I. Zukerman, and S.N. Kim. 2013.
Evaluation of the Scusi? spoken language interpre-
tation system – A case study. In Proceedings of the
6th International Joint Conference on Natural Lan-
guage Processing, Nagoya, Japan.

J.D. Lafferty, A. McCallum, and F.C.N. Pereira. 2001.
Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In
ICML’2001 – Proceedings of the 18th International
Conference on Machine Learning, pages 282–289,
Williamstown, Massachusetts.

R. López-Cózar and D. Griol. 2010. New technique
to enhance the performance of spoken dialogue sys-
tems based on dialogue states-dependent language
models and grammatical rules. In Proceedings of In-
terspeech 2010, pages 2998–3001, Makuhari, Japan.

T. Pellegrini and I. Trancoso. 2010. Improving ASR
error detection with non-decoder based features. In
Proceedings of Interspeech 2010, pages 1950–1953,
Makuhari, Japan.

J. R. Quinlan. 1993. C4.5: Programs for Machine
Learning. Morgan Kaufmann Publishers, San Ma-
teo, California.

E. Ringger and J.F. Allen. 1996. Error correction via a
postprocessor for continuous speech recognition. In
Proceedings of the IEEE International Conference
on Acoustics, Speech, and Signal Processing, pages
427–430, Atlanta, Georgia.

T.N. Sainath, B. Ramabhadran, M. Picheny, D. Na-
hamoo, and D. Kanevsky. 2011. Exemplar-
based sparse representation features: From TIMIT to
LVCSR. IEEE Transactions on Audio, Speech and
Language Processing, 19(8):2598–2613.

J.F. Sowa. 1984. Conceptual Structures: Information
Processing in Mind and Machine. Addison-Wesley,
Reading, MA.

C.S. Wallace. 2005. Statistical and Inductive Inference
by Minimum Message Length. Springer, Berlin,
Germany.

I. Zukerman, E. Makalic, M. Niemann, and S. George.
2008. A probabilistic approach to the interpreta-
tion of spoken utterances. In PRICAI 2008 – Pro-
ceedings of the 10th Pacific Rim International Con-
ference on Artificial Intelligence, pages 581–592,
Hanoi, Vietnam.

I. Zukerman, P. Ye, K.K. Gupta, and E. Makalic. 2009.
Towards the interpretation of utterance sequences in
a dialogue system. In Proceedings of the 10th SIG-
dial Conference on Discourse and Dialogue, pages
46–53, London, United Kingdom.

S. Zwarts, M. Johnson, and R. Dale. 2010. De-
tecting speech repairs incrementally using a noisy
channel approach. In COLING’2010 – Proceedings
of the 23rd International Conference on Computa-
tional Linguistics, pages 1371–1378, Beijing, China.

242


