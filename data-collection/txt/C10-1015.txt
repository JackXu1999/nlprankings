Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 125–133,

Beijing, August 2010

125

A Utility-Driven Approach to Question Ranking in Social QA

Razvan Bunescu
School of EECS
Ohio University

Yunfeng Huang
School of EECS
Ohio University

bunescu@ohio.edu

yh324906@ohio.edu

Abstract

We generalize the task of ﬁnding question
paraphrases in a question repository to a
novel formulation in which known ques-
tions are ranked based on their utility to
a new, reference question. We manually
annotate a dataset of 60 groups of ques-
tions with a partial order relation reﬂect-
ing the relative utility of questions inside
each group, and use it to evaluate mean-
ing and structure aware utility functions.
Experimental evaluation demonstrates the
importance of using structural informa-
tion in estimating the relative usefulness
of questions, holding the promise of in-
creased usability for social QA sites.

1

Introduction

Open domain Question Answering (QA) is one
of the most complex and challenging tasks in
natural language processing. While building on
ideas from Information Retrieval (IR), question
answering is generally seen as a more difﬁcult
task due to constraints on both the input represen-
tation (natural language questions vs. keyword-
based queries) and the form of the output (fo-
cused answers vs. entire documents). Recently,
community-driven QA sites such as Yahoo! An-
swers and WikiAnswers have established a new
approach to question answering in which the bur-
den of dealing with the inherent complexity of
open domain QA is shifted from the computer
system to volunteer contributors. The computer
is no longer required to perform a deep linguis-
tic analysis of questions and generate correspond-
ing answers, and instead acts as a mediator be-

tween users submitting questions and volunteers
providing the answers. In most implementations
of community-driven QA, the mediator system
has a well deﬁned strategy for enticing volun-
teers to post high quality answers on the website.
In general, the overall objective is to minimize
the response time and maximize the accuracy of
the answers, measures that are highly correlated
with user satisfaction. For any submitted ques-
tion, one useful strategy is to search the QA repos-
itory for similar questions that have already been
answered, and provide the corresponding ranked
list of answers, if such a question is found. The
success of this approach depends on the deﬁnition
and implementation of the question-to-question
similarity function. In the simplest solution, the
system searches for previously answered ques-
tions based on exact string matching with the
reference question. Alternatively, sites such as
WikiAnswers allow the users to mark questions
they think are rephrasings (“alternate wordings”,
or paraphrases) of existing questions. These ques-
tion clusters are then taken into account when per-
forming exact string matching, therefore increas-
ing the likelihood of ﬁnding previously answered
questions that are semantically equivalent to the
reference question. Like the original question an-
swering task, the solution to question rephrasing is
also based on volunteer contributions. In order to
lessen the amount of work required from the con-
tributors, an alternative solution is to build a sys-
tem that automatically ﬁnds rephrasings of ques-
tions, especially since question rephrasing seems
to be computationally less demanding than ques-
tion answering. The question rephrasing subtask
has spawned a diverse set of approaches. (Herm-

126

jakob et al., 2002) derive a set of phrasal patterns
for question reformulation by generalizing surface
patterns acquired automatically from a large cor-
pus of web documents. The focus of the work in
(Tomuro, 2003) is on deriving reformulation pat-
terns for the interrogative part of a question. In
(Jeon et al., 2005), word translation probabilities
are trained on pairs of semantically similar ques-
tions that are automatically extracted from an FAQ
archive, and then used in a language model that
retrieves question reformulations. (Jijkoun and de
Rijke, 2005) describe an FAQ question retrieval
system in which weighted combinations of simi-
larity functions corresponding to questions, exist-
ing answers, FAQ titles and pages are computed
using a vector space model. (Zhao et al., 2007)
exploit the Encarta logs to automatically extract
clusters containing question paraphrases and fur-
ther train a perceptron to recognize question para-
phrases inside each cluster based on a combina-
tion of lexical, syntactic and semantic similarity
features. More recently, (Bernhard and Gurevych,
2008) evaluated various string similarity measures
and vector space based similarity measures on the
task of retrieving question paraphrases from the
WikiAnswers repository.

According to previous work in this domain, a
question is considered a rephrasing of a reference
question Q0 if it uses an alternate wording to ex-
press an identical information need. For example,
Q0 and Q1 below may be considered rephrasings
of each other, and consequently they are expected
to have the same answer.

Q0 What should I feed my turtle?

Q1 What do I feed my pet turtle?

Community-driven QA sites are bound to face sit-
uations in which paraphrasings of a new ques-
tion cannot be found in the QA repository. We
believe that computing a ranked list of existing
questions that partially address the original infor-
mation need could be useful to the user, at least
until other users volunteer to give an exact an-
swer to the original, unanswered reference ques-
tion. For example, in the absence of any additional
information about the reference question Q0, the
expected answers to questions Q2 and Q3 above

may be seen as partially overlapping in informa-
tion content with the expected answer for the ref-
erence question. An answer to question Q4, on the
other hand, is less likely to beneﬁt the user, even
though it has a signiﬁcant lexical overlap with the
reference question.

Q2 What kind of ﬁsh should I feed my turtle?

Q3 What do you feed a turtle that is the size of a

quarter?

Q4 What kind of food should I feed a turtle dove?

In this paper, we propose a generalization of
the question paraphrasing problem to a question
ranking problem, in which questions are ranked
in a partial order based on the relative information
overlap between their expected answers and the
expected answer of the reference question. The
expectation in this approach is that the user who
submits a reference question will ﬁnd the answers
of the highly ranked question to be more useful
than the answers associated with the lower ranked
questions. For the reference question Q0 above,
the system is expected to produce a partial order
in which Q1 is ranked higher than Q2, Q3 and Q4,
whereas Q2 and Q3 are ranked higher than Q4. In
Section 2 we give further details on the question
ranking task and describe a dataset of questions
that have been manually annotated with partial or-
der information. Section 3 presents a set of initial
approaches to question ranking, followed by their
experimental evaluation in Section 4. The paper
ends with a discussion of future work, and con-
clusion.

2 A Partially Ordered Dataset for

Question Ranking

In order to enable the evaluation of question rank-
ing approaches, we created a dataset of 60 groups
of questions. Each group consists of a reference
question (e.g. Q0 above) that is associated with
a partially ordered set of questions (e.g. Q1 to
Q4 above). The 60 reference questions have been
selected to represent a diverse set of question cat-
egories from Yahoo! Answers. For each refer-
ence questions, its corresponding partially ordered
set is created from questions in Yahoo! Answers

127

REFERENCE QUESTION (Qr )

Q5 What’s a good summer camp to go to in FL?

Q6 What camps are good for a vacation during the summer in FL?
Q7 What summer camps in FL do you recommend?

PARAPHRASING QUESTIONS (P )

Q8 Does anyone know a good art summer camp to go to in FL?

USEFUL QUESTIONS (U )

Q9 Are there any good artsy camps for girls in FL?
Q10 What are some summer camps for like singing in Florida?

Q11 What is a good cooking summer camp in FL?
Q12 Do you know of any summer camps in Tampa, FL?
Q13 What is a good summer camp in Sarasota FL for a 12 year old?
Q14 Can you please help me ﬁnd a surﬁng summer camp for beginners in Treasure Coast, FL?
Q15 Are there any acting summer camps and/or workshops in the Orlando, FL area?
Q16 Does anyone know any volleyball camps in Miramar, FL?
Q17 Does anyone know about any cool science camps in Miami?
Q18 What’s a good summer camp you’ve ever been to?

Q19 What’s a good summer camp in Canada?
Q20 What’s the summer like in Florida?

NEUTRAL QUESTIONS (N )

Table 1: A question group.

and other online repositories that have a high co-
sine similarity with the reference question. Due to
the signiﬁcant lexical overlap between the ques-
tions, this is a rather difﬁcult dataset, especially
for ranking methods that rely exclusively on bag-
of-words measures. Inside each group, the ques-
tions are manually annotated with a partial order
relation, according to their utility with respect to
the reference question. We shall use the notation
hQi ≻ Qj|Qri to encode the fact that question Qi
is more useful than question Qj with respect to
the reference question Qr. Similarly, hQi = Qji
will be used to express the fact that questions Qi
and Qj are reformulations of each other (the refor-
mulation relation is independent of the reference
question). The partial ordering among the ques-
tions Q0 to Q4 above can therefore be expressed
concisely as follows: hQ0 = Q1i, hQ1 ≻ Q2|Q0i,
hQ1 ≻ Q3|Q0i, hQ2 ≻ Q4|Q0i, hQ3 ≻ Q4|Q0i.
Note that we do not explicitly annotate the rela-
tion hQ1 ≻ Q4|Q0i, since it can be inferred based
on the transitivity of the more useful than relation:
hQ1 ≻ Q2|Q0i ∧ hQ2 ≻ Q4|Q0i ⇒ hQ1 ≻
Q4|Q0i. Also note that no relation is speciﬁed

between Q2 and Q3, and similarly no relation can
be inferred between these two questions. This re-
ﬂects our belief that, in the absence of any addi-
tional information regarding the user or the “tur-
tle” referenced in Q0, we cannot compare ques-
tions Q2 and Q3 in terms of their usefulness with
respect to Q0.

Table 1 shows another reference question Q5
from our dataset, together with its annotated group
of questions Q6 to Q20. In order to make the anno-
tation process easier and reproducible, we divide
it into two levels of annotation. During the ﬁrst
annotation stage (L1), each question group is par-
titioned manually into 3 subgroups of questions:

• P is the set of paraphrasing questions.
• U is the set of useful questions.
• N is the set of neutral questions.

A question is deemed useful if its expected answer
may overlap in information content with the ex-
pected answer of the reference question. The ex-
pected answer of a neutral question, on the other

128

hand, should be irrelevant with respect to the ref-
erence question. Let Qr be the reference question,
Qp ∈ P a paraphrasing question, Qu ∈ U a useful
question, and Qn ∈ N a neutral question. Then
the following relations are assumed to hold among
these questions:

1. hQp ≻ Qu|Qri: a paraphrasing question is

more useful than a useful question.

2. hQu ≻ Qn|Qri: a useful question is more

useful than a neutral question.

We also assume that, by transitivity, the following
ternary relations also hold: hQp ≻ Qn|Qri, i.e. a
paraphrasing question is more useful than a neu-
tral question. Furthermore, if Qp1, Qp2 ∈ P are
two paraphrasing questions, this implies hQp1 =
Qp2|Qri.

For the vast majority of questions,

the ﬁrst
annotation stage is straightforward and non-
controversial. In the second annotation stage (L2),
we perform a ﬁner annotation of relations between
questions in the middle group U . Table 1 shows
two such relations (using indentation): hQ8 ≻
Q9|Q5i and hQ8 ≻ Q10|Q5i. Question Q8 would
have been a rephrasing of the reference question,
were it not for the noun “art” modifying the focus
noun phrase “summer camp”. Therefore, the in-
formation content of the answer to Q8 is strictly
subsumed in the information content associated
with the answer to Q5. Similarly, in Q9 the fo-
cus noun phrase is further specialized through the
prepositional phrase “for girls”. Therefore, (an
answer to) Q9 is less useful to Q5 than (an an-
swer to) Q8, i.e. hQ8 ≻ Q9|Q5i. Furthermore,
the focus “art summer camp” in Q8 conceptually
subsumes the focus “summer camps for singing”
in Q10, therefore hQ8 ≻ Q10|Q5i.

Table 2 below presents the following statistics
on the annotated dataset: the number of reference
questions (Qr), the total number of paraphrasings
(P), the total number of useful questions (U ), the
total number of neutral questions (N ), and the to-
tal number of more useful than ordered pairs en-
coded in the dataset, either explicitly or through
transitivity, in the two annotation levels L1 and
L2.

Qr
60

P
177

U
847

N
427

L1

L2

7,378

7,639

Table 2: Dataset statistics.

3 Question Ranking Methods

An ideal question ranking method would take an
arbitrary triplet of questions Qr, Qi and Qj as
input, and output an ordering between Qi and
Qj with respect to the reference question Qr,
i.e. one of hQi ≻ Qj|Qri, hQi = Qj|Qri, or
hQj ≻ Qi|Qri. One approach is to design a
usefulness function u(Qi, Qr) that measures how
useful question Qi is for the reference question
Qr, and deﬁne the more useful than (≻) relation
as follows:

hQi ≻ Qj|Qri ⇔ u(Qi, Qr) > u(Qj, Qr)
If we deﬁne I(Q) to be the information need as-
sociated with question Q, then u(Qi, Qr) could
be deﬁned as a measure of the relative overlap be-
tween I(Qi) and I(Qr). Unfortunately, the infor-
mation need is a concept that, in general, is de-
ﬁned only intensionally and therefore it is difﬁ-
cult to measure. For lack of an operational def-
inition of the information need, we will approxi-
mate u(Qi, Qr) directly as a measure of the simi-
larity between Qi and Qr. The similarity between
two questions can be seen as a special case of
text-to-text similarity, consequently one possibil-
ity is to use a general text-to-text similarity func-
tion such as cosine similarity in the vector space
model (Baeza-Yates and Ribeiro-Neto, 1999):

cos(Qi, Qr) =

i Qr

QT
kQikkQrk

Here, Qi and Qr denote the corresponding tf×idf
vectors. As a measure of question-to-question
similarity, cosine has two major drawbacks:

1. As an exclusively lexical measure, it is obliv-
ious to the meanings of words in each ques-
tion.

2. Questions are treated as bags-of-words,
and thus important structural information is
missed.

129

3.1 Meaning Aware Measures

The three questions below illustrate the ﬁrst prob-
lem associated with cosine similarity. Q22 and
Q23 have the same cosine similarity with Q21,
they are therefore indistinguishable in terms of
their usefulness to the reference question Q21,
even though we expect Q22 to be more useful than
Q23 (a place that sells hydrangea often sells other
types of plants too, possibly including cacti).

Q21 Where can I buy a hydrangea?

Q22 Where can I buy a cactus?

Q23 Where can I buy an iPad?

To alleviate the lexical chasm, we can redeﬁne
u(Qi, Qr) to be the similarity measure proposed
by (Mihalcea et al., 2006) as follows:

(Wu and Palmer, 1994) ﬁnds the least common
subsumer (LCS) of the two input concepts in the
WordNet hierarchy, and computes the ratio be-
tween its depth and the sum of the depths of the
two concepts:

wup(ci, cr) =

2 ∗ depth(lcs(ci, cr))
depth(ci) + depth(cr)

Resnik’s measure is based on the Information
Content (IC) of a concept c deﬁned as the negative
log probability − log P (c) of ﬁnding that concept
in a large corpus:

res(ci, cr) = IC(lcs(ci, cr))

Lin’s similarity measure can be seen as a normal-
ized version of Resnik’s information content:

lin(ci, cr) =

2 ∗ IC(lcs(ci, cr))
IC(ci) + IC(cr)

mcs(Qi, Qr) =

X

w∈{Qi}

(maxSim(w, Qr) ∗ idf (w))

X

idf (w)

w∈{Qi}

X

w∈{Qr}

(maxSim(w, Qi) ∗ idf (w))

X

idf (w)

w∈{Qr}

Since scaling factors are immaterial for ranking,
we have ignored the normalization constant con-
tained in the original measure. For each word
w ∈ Qi, maxSim(w, Qr) computes the maxi-
mum semantic similarity between w and any word
wr ∈ Qr. The similarity scores are then weighted
by the corresponding idf’s, and normalized. A
similar score is computed for each word w ∈ Qr.
The score computed by maxSim depends on the
actual function used to compute the word-to-word
semantic similarity.
In this paper, we evaluated
four of the knowledge-based measures explored
in (Mihalcea et al., 2006): wup (Wu and Palmer,
1994), res (Resnik, 1995), lin (Lin, 1998), and
jcn (Jiang and Conrath, 1997). Since all these
measures are deﬁned on pairs of WordNet con-
cepts, their analogues on word pairs (wi, wr) are
computed by selecting pairs of WordNet synsets
(ci, cr) such that wi belongs to concept ci, wr be-
longs to concept cr, and (ci, cr) maximizes the
similarity function. The measure introduced in

Jiang & Conrath’s measure is closely related to
lin and is computed as follows:

+

jcn(ci, cr) = [IC(ci) + IC(cr) − 2 ∗ IC(lcs(ci, cr))]−1

3.2 Structure Aware Measures

Cosine similarity, henceforth referred as cos,
treats questions as bags-of-words. The meta-
measure proposed in (Mihalcea et al., 2006),
henceforth called mcs, treats questions as bags-
of-concepts. Consequently, both cos and mcs may
miss important structural information. If we con-
sider the question Q24 below as reference, ques-
tion Q26 will be deemed more useful than Q25
when using cos or mcs because of the higher rel-
ative lexical and conceptual overlap with Q24.
However, this is contrary to the actual ordering
hQ25 ≻ Q26|Q24i, which reﬂects that fact that
Q25, which expects the same answer type as Q24,
should be deemed more useful than Q26, which
has a different answer type.

Q24 What are some good thriller movies?

Q25 What are some thriller movies with happy

ending?

Q26 What are some good songs from a thriller

movie?

130

The analysis above shows the importance of us-
ing the answer type when computing the simi-
larity between two questions. However, instead
of relying exclusively on a predeﬁned hierarchy
of answer types, we have decided to identify the
question focus of a question, deﬁned as the set of
maximal noun phrases in the question that corefer
with the expected answer. Focus nouns such as
movies and songs provide more discriminative in-
formation than general answer types such as prod-
ucts. We use answer types only for questions such
as Q27 or Q28 below that lack an explicit question
focus. In such cases, an artiﬁcial question focus
is created from the answer type (e.g. location for
Q27, or method for Q28) and added to the set of
question words.

Q27 Where can I buy a good coffee maker?

Q28 How do I make a pizza?

Let qsim be a general bag-of-words question sim-
ilarity measure (e.g. cos or mcs). Furthermore, let
wsim by a generic word meaning similarity mea-
sure (e.g. wup, res, lin or jcn). The equation be-
low describes a modiﬁcation of qsim that makes it
aware of the questions focus:

qsimf (Qi, Qr) = wsim(fi, fr) ∗

qsim(Qi−{fi}, Qr−{fr})

Here, Qi and Qr refer both to the questions and
their sets of words, while fi and fr stand for the
corresponding focus words. We deﬁne qsim to
return 1 if one of its arguments is an empty set,
i.e. qsim(∅, ) = qsim( ,∅) = 1. The new
similarity measure qsimf multiplies the seman-
tic similarity between the two focus words with
the bag-of-words similarity between the remain-
ing words in the two questions. Consequently, the
word “movie” in Q26 will not be compared with
the word “movies” in Q24, and therefore Q26 will
receive a lower utility score than Q25.

In addition to the question focus, the main verb
of a question can also provide key information
in estimating question-to-question similarity. We
deﬁne the main verb to be the content verb that
is highest in the dependency tree of the question,
e.g. buy for Q27, or make for Q28. If the question
does not contain a content verb, the main verb is

deﬁned to be the highest verb in the dependency
tree, as for example are in Q24 to Q26. The utility
of a question’s main verb in judging its similarity
to other questions can be seen more clearly in the
questions below, where Q29 is the reference:

Q29 How can I transfer music from iTunes to my

iPod?

Q30 How can I upload music to my iPod?

Q31 How can I play music in iTunes?

The fact that upload, as the main verb of Q30, is
more semantically related to transfer (upload is a
hyponym of transfer in WordNet) is essential in
deciding that hQ30 ≻ Q31|Q29i, i.e. Q30 is more
useful than Q31 to Q29.
Like the focus word, the main verb can be in-
corporated in the question similarity function as
follows:

qsimf v(Qi, Qr) = wsim(fi, fr) ∗ wsim(vi, vr) ∗

qsim(Qi−{fi, vi}, Qr−{fr, vr})

The new measure qsimf v takes into account
both the focus words and the main verbs when
estimating the semantic similarity between ques-
tions. When decomposing the questions into focus
words, main verbs and the remaining words, we
have chosen to multiply the corresponding sim-
ilarities instead of, for example, summing them.
Consequently, a close to zero score in each of
them would drive the entire similarity to zero.
This reﬂects the belief that question similarity is
sensitive to each component of a question.

4 Experimental Evaluation

We use the question ranking dataset described in
Section 2 to evaluate the two similarity measures
cos and mcs, as well as their structured versions
cosf , cosf v, mcsf , and mcsf v. We report one
set of results for each of the four word similarity
measures wup, res, lin or jcn. Each question simi-
larity measure is evaluated in terms of its accuracy
on the set of ordered pairs for each of the two an-
notation levels described in Section 2. Thus, for
the ﬁrst annotation level (L1) , we evaluate only
over the set of relations deﬁned across the three

131

Question
similarity
(qsim)

cos
cosf
cosf v
mcs
mcsf
mcsf v

Word similarity (wsim)

wup

res

lin

jcn

L1
69.1
69.9
69.9
62.6
64.2
65.8

L2
69.3
70.1
70.1
62.5
64.4
66.0

L1
69.1
72.5
72.5
65.0
68.5
68.8

L2
69.3
72.7
72.6
65.0
68.5
68.8

L1
69.1
71.0
71.0
65.6
68.8
69.7

L2
69.3
71.2
71.2
65.7
68.9
69.8

L1
69.1
69.6
69.6
66.8
67.2
67.7

L2
69.3
69.8
69.8
66.9
67.4
67.8

Table 3: Accuracy results, with and without meaning and structure information.

sets R, U , and N .
If hQi ≻ Qj|Qri is a rela-
tion speciﬁed in the annotation, we consider the
tuple hQi, Qj, Qri correctly classiﬁed if and only
if u(Qi, Qr) > u(Qj, Qr), where u is the ques-
tion similarity measure (Section 3). For the sec-
ond annotation level (L2), we also consider the re-
lations annotated between useful questions inside
the group U .

We used the NLTK 1 implementation of the four
similarity measures wup, res, lin or jcn. The idf
values for each word were computed from fre-
quency counts over the entire Wikipedia. For each
question, the focus is identiﬁed automatically by
an SVM tagger trained on a separate corpus of
2,000 questions manually annotated with focus in-
formation. The SVM tagger uses a combination
of lexico-syntactic features and a quadratic ker-
nel to achieve a 93.5% accuracy in a 10-fold cross
validation evaluation on the 2,000 questions. The
main verb of a question is identiﬁed deterministi-
cally using a breadth ﬁrst traversal of the depen-
dency tree.

The overall accuracy results presented in Ta-
ble 3 show that using the focus word improves the
performance across all 8 combinations of question
and word similarity measures. For cosine simi-
larity, the best performing system uses the focus
words and Resnik’s similarity function to obtain a
3.4% increase in accuracy. For the meaning aware
similarity mcs, the best performing system uses
the focus words, the main verb and Lin’s word
similarity to achieve a 4.1% increase in accu-
racy. The improvement due to accounting for fo-
cus words is consistent, whereas adding the main

1http://www.nltk.org

verb seems to improve the performance only for
mcs, although not by a large margin. The second
level of annotation brings 261 more relations in
the dataset, some of them more difﬁcult to anno-
tate when compared with the three groups in the
ﬁrst level. Nevertheless, the performance either
remains the same (somewhat expected due to the
relatively small number of additional relations), or
is marginally better. The random baseline – as-
signing a random similarity value to each pair of
questions – results in 50% accuracy. A somewhat
unexpected result is that mcs does not perform
better than cos on this dataset. After analysing
the result in more detail, we have noticed that mcs
seems to be less resilient than cos to variations in
the length of the questions. The Microsoft para-
phrase corpus was speciﬁcally designed such that
“the length of the shorter of the two sentences, in
words, is at least 66% that of the longer” (Dolan
and Brockett, 2005), whereas in our dataset the
two questions in a pair can have signiﬁcantly dif-
ferent lengths 2.

The questions in each of the 60 groups have a
high degree of lexical overlap, making the dataset
especially difﬁcult. In this context, we believe the
results are encouraging. We expect to obtain fur-
ther improvements in accuracy by allowing rela-
tions between all the words in a question to in-
ﬂuence the overall similarity measure. For exam-
ple, question Q19 has the same focus word as the
reference question Q5 (repeated below), yet the
difference between the focus word prepositional
modiﬁers makes it a neutral question.

2Our implementation of mcs did performed better than

cos on the Microsoft dataset.

132

Q5 What’s a good summer camp to go to in FL?

Q19 What’s a good summer camp in Canada?

Some of the questions in our dataset illustrate the
need to design a word similarity function specif-
ically tailored to reﬂect how words change the
relative usefulness of a question.
In the set of
questions below, in deciding that Q33 and Q34
are more useful than Q36 for the reference ques-
tion Q32, an ideal question ranker needs to know
that the “Mayﬂower Hotel” and the “Queensboro
Bridge” are in the proximity of “Midtown Man-
hattan”, and that proximity relations are relevant
when asking for directions. A coarse measure
of proximity can be obtained for the pair (“Man-
hattan”, “Queensboro Bridge”) by following the
meronymy links connecting the two entities in
WordNet. However, a different strategy needs to
be devised for entities such as “Mayﬂower Hotel”,
“JFK”, or “La Guardia” which are not covered in
WordNet.

Q32 What is the best way to get to Midtown Man-

hattan from JFK?

We also plan to extend the word similarity func-
tions to better reﬂect the types of relations that
are relevant when measuring question utility, such
as proximity relations between locations. Further-
more, we intend to take advantage of databases of
interrogative paraphrases and paraphrase patterns
that were created in previous research on question
reformulation.

6 Conclusion

We presented a novel question ranking task in
which previously known questions are ordered
based on their relative utility with respect to a new,
reference question. We created a dataset of 60
groups of questions 3 annotated with a partial or-
der relation reﬂecting the relative utility of ques-
tions inside each group, and used it to evaluate
the ranking performance of several meaning and
structure aware utility functions. Experimental re-
sults demonstrate the importance of using struc-
tural information in judging the relative usefulness
of questions. We believe that the new perspective
on ranking questions has the potential to signiﬁ-
cantly improve the usability of social QA sites.

Q33 What’s the best way from JFK to Mayﬂower

Hotel?

Acknowledgments

Q34 What’s the best way from JFK to Queens-

boro Bridge?

We would like to thank the anonymous reviewers
for their helpful suggestions.

Q35 How do I get from Manhattan to JFK airport

by train?

References

Q36 What is the best way to get to LaGuardia

from JFK?

Baeza-Yates, Ricardo and Berthier Ribeiro-Neto.
1999. Modern Information Retrieval. ACM Press,
New York.

Finally, to realize why question Q35 is useful one
needs to know that, once directions on how to get
by train from location X to location Y are known,
then normally it sufﬁces to reverse the list of stops
in order to obtain directions on how to get from Y
back to X.

5 Future Work

We plan to integrate the entire dependency struc-
ture of the question in the overall similarity mea-
sure, possibly by deﬁning kernels between ques-
tions in a maximum margin model for ranking.

Bernhard, Delphine and Iryna Gurevych. 2008. An-
swering learners’ questions by retrieving question
paraphrases from social Q&A sites. In EANL ’08:
Proceedings of the Third Workshop on Innovative
Use of NLP for Building Educational Applications,
pages 44–52, Morristown, NJ, USA. Association for
Computational Linguistics.

Dolan, William B. and Chris Brockett. 2005. Auto-
matically constructing a corpus of sentential para-
phrases. In Proceedings of the Third International
Workshop on Paraphrasing (IWP2005), pages 9–16.

3The dataset will be made publicly available.

133

Hermjakob, Ulf, Abdessamad Echihabi, and Daniel
Marcu. 2002. Natural language based reformula-
tion resource and web exploitation for question an-
swering. In Proceedings of TREC-2002.

Jeon, Jiwoon, W. Bruce Croft, and Joon Ho Lee. 2005.
Finding similar questions in large question and an-
swer archives. In Proceedings of the 14th ACM in-
ternational conference on Information and knowl-
edge management (CIKM’05), pages 84–90, New
York, NY, USA. ACM.

Jiang, J.J. and D.W. Conrath. 1997. Semantic similar-
ity based on corpus statistics and lexical taxonomy.
In Proceedings of the International Conference on
Research in Computational Linguistics, pages 19–
33.

Jijkoun, Valentin and Maarten de Rijke. 2005. Re-
trieving answers from frequently asked questions
pages on the Web. In Proceedings of the 14th ACM
international conference on Information and knowl-
edge management (CIKM’05), pages 76–83, New
York, NY, USA. ACM.

Lin, Dekang. 1998. An information-theoretic def-
inition of similarity.
In Proceedings of the Fif-
teenth International Conference on Machine Learn-
ing (ICML ’98), pages 296–304, San Francisco, CA,
USA. Morgan Kaufmann Publishers Inc.

Mihalcea, Rada, Courtney Corley, and Carlo Strappa-
rava. 2006. Corpus-based and knowledge-based
measures of text semantic similarity.
In Proceed-
ings of the 21st national conference on Artiﬁcial in-
telligence (AAAI’06), pages 775–780. AAAI Press.

Resnik, Philip. 1995. Using information content to
evaluate semantic similarity in a taxonomy. In IJ-
CAI’95: Proceedings of the 14th international joint
conference on Artiﬁcial intelligence, pages 448–
453, San Francisco, CA, USA. Morgan Kaufmann
Publishers Inc.

Tomuro, Noriko. 2003.

Interrogative reformulation
patterns and acquisition of question paraphrases. In
Proceedings of the Second International Workshop
on Paraphrasing, pages 33–40, Morristown, NJ,
USA. Association for Computational Linguistics.

Wu, Zhibiao and Martha Palmer. 1994. Verbs se-
mantics and lexical selection. In Proceedings of the
32nd annual meeting on Association for Computa-
tional Linguistics, pages 133–138, Morristown, NJ,
USA. Association for Computational Linguistics.

Zhao, Shiqi, Ming Zhou, and Ting Liu. 2007. Learn-
ing question paraphrases for QA from Encarta logs.
In Proceedings of the 20th international joint con-
ference on Artiﬁcal intelligence (IJCAI’07), pages
1795–1800, San Francisco, CA, USA. Morgan
Kaufmann Publishers Inc.

