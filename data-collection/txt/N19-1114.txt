




































Unsupervised Recurrent Neural Network Grammars


Proceedings of NAACL-HLT 2019, pages 1105–1117
Minneapolis, Minnesota, June 2 - June 7, 2019. c©2019 Association for Computational Linguistics

1105

Unsupervised Recurrent Neural Network Grammars

Yoon Kim† Alexander M. Rush† Lei Yu3
Adhiguna Kuncoro‡,3 Chris Dyer3 Gábor Melis3

†Harvard University ‡University of Oxford 3DeepMind
{yoonkim,srush}@seas.harvard.edu

{leiyu,akuncoro,cdyer,melisgl}@google.com

Abstract
Recurrent neural network grammars (RNNG)
are generative models of language which
jointly model syntax and surface structure by
incrementally generating a syntax tree and
sentence in a top-down, left-to-right order.
Supervised RNNGs achieve strong language
modeling and parsing performance, but re-
quire an annotated corpus of parse trees. In
this work, we experiment with unsupervised
learning of RNNGs. Since directly marginal-
izing over the space of latent trees is in-
tractable, we instead apply amortized varia-
tional inference. To maximize the evidence
lower bound, we develop an inference net-
work parameterized as a neural CRF con-
stituency parser. On language modeling, unsu-
pervised RNNGs perform as well their super-
vised counterparts on benchmarks in English
and Chinese. On constituency grammar in-
duction, they are competitive with recent neu-
ral language models that induce tree structures
from words through attention mechanisms.

1 Introduction
Recurrent neural network grammars (RNNGs)

(Dyer et al., 2016) model sentences by first gen-
erating a nested, hierarchical syntactic structure
which is used to construct a context representation
to be conditioned upon for upcoming words. Su-
pervised RNNGs have been shown to outperform
standard sequential language models, achieve ex-
cellent results on parsing (Dyer et al., 2016; Kun-
coro et al., 2017), better encode syntactic proper-
ties of language (Kuncoro et al., 2018), and cor-
relate with electrophysiological responses in the
human brain (Hale et al., 2018). However, these
all require annotated syntactic trees for training.
In this work, we explore unsupervised learning of
recurrent neural network grammars for language
modeling and grammar induction.

Work done while the first author was an intern at DeepMind.
Code available at https://github.com/harvardnlp/urnng

The standard setup for unsupervised structure
learning is to define a generative model pθ(x, z)
over observed data x (e.g. sentence) and unob-
served structure z (e.g. parse tree, part-of-speech
sequence), and maximize the log marginal like-
lihood log pθ(x) = log

∑
z pθ(x, z). Success-

ful approaches to unsupervised parsing have made
strong conditional independence assumptions (e.g.
context-freeness) and employed auxiliary objec-
tives (Klein and Manning, 2002) or priors (John-
son et al., 2007). These strategies imbue the learn-
ing process with inductive biases that guide the
model to discover meaningful structures while al-
lowing tractable algorithms for marginalization;
however, they come at the expense of language
modeling performance, particularly compared to
sequential neural models that make no indepen-
dence assumptions.

Like RNN language models, RNNGs make no
independence assumptions. Instead they encode
structural bias through operations that compose
linguistic constituents. The lack of independence
assumptions contributes to the strong language
modeling performance of RNNGs, but make unsu-
pervised learning challenging. First, marginaliza-
tion is intractable. Second, the biases imposed by
the RNNG are relatively weak compared to those
imposed by models like PCFGs. There is little
pressure for non-trivial tree structure to emerge
during unsupervised RNNG (URNNG) learning.

In this work, we explore a technique for han-
dling intractable marginalization while also inject-
ing inductive bias. Specifically we employ amor-
tized variational inference (Kingma and Welling,
2014; Rezende et al., 2014; Mnih and Gregor,
2014) with a structured inference network. Varia-
tional inference lets us tractably optimize a lower
bound on the log marginal likelihood, while em-
ploying a structured inference network encour-
ages non-trivial structure. In particular, a con-



1106

ditional random field (CRF) constituency parser
(Finkel et al., 2008; Durrett and Klein, 2015),
which makes significant independence assump-
tions, acts as a guide on the generative model
to learn meaningful trees through regularizing the
posterior (Ganchev et al., 2010).

We experiment with URNNGs on English and
Chinese and observe that they perform well as
language models compared to their supervised
counterparts and standard neural LMs. In terms
of grammar induction, they are competitive with
recently-proposed neural architectures that dis-
cover tree-like structures through gated attention
(Shen et al., 2018). Our results, along with other
recent work on joint language modeling/structure
learning with deep networks (Shen et al., 2018,
2019; Wiseman et al., 2018; Kawakami et al.,
2018), suggest that it is possible learn generative
models of language that model the underlying data
well (i.e. assign high likelihood to held-out data)
and at the same time induce meaningful linguistic
structure.

2 Unsupervised Recurrent Neural
Network Grammars

We use x = [x1, . . . , xT ] to denote a sentence of
length T , and z ∈ ZT to denote an unlabeled bi-
nary parse tree over a sequence of length T , repre-
sented as a a binary vector of length 2T − 1. Here
0 and 1 correspond to SHIFT and REDUCE actions,
explained below.1 Figure 1 presents an overview
of our approach.

2.1 Generative Model
An RNNG defines a joint probability distribu-
tion pθ(x, z) over sentences x and parse trees
z. We consider a simplified version of the orig-
inal RNNG (Dyer et al., 2016) by ignoring con-
stituent labels and only considering binary trees.
The RNNG utilizes an RNN to parameterize a
stack data structure (Dyer et al., 2015) of partially-
completed constituents to incrementally build the
parse tree while generating terminals. Using the
current stack representation, the model samples
an action (SHIFT or REDUCE): SHIFT generates
a terminal symbol, i.e. word, and shifts it onto
the stack,2 REDUCE pops the last two elements off
the stack, composes them, and shifts the composed

1The cardinality of ZT ⊂ {0, 1}2T−1 is given by the
(T − 1)-th Catalan number, |ZT | = (2T−2)!T !(T−1)! .

2A better name for SHIFT would be GENERATE (as in
Dyer et al. (2016)), but we use SHIFT to emphasize similarity
with the shift-reduce parsing.

Figure 1: Overview of our approach. The inference
network qφ(z |x) (left) is a CRF parser which pro-
duces a distribution over binary trees (shown in dotted
box). Bij are random variables for existence of a con-
stituent spanning i-th and j-th words, whose potentials
are the output from a bidirectional LSTM (the global
factor ensures that the distribution is only over valid bi-
nary trees). The generative model pθ(x, z) (right) is an
RNNG which consists of a stack LSTM (from which
actions/words are predicted) and a tree LSTM (to ob-
tain constituent representations upon REDUCE). Train-
ing involves sampling a binary tree from qφ(z |x), con-
verting it to a sequence of shift/reduce actions, and op-
timizing the log joint likelihood log pθ(x, z).
representation onto the stack.

Formally, let S = [(0,0)] be the initial stack.
Each item of the stack will be a pair, where the first
element is the hidden state of the stack LSTM, and
the second element is an input vector, described
below. We use top(S) to refer to the top pair in
the stack. The push and pop operations are de-
fined imperatively in the usual way. At each time
step, the next action zt (SHIFT or REDUCE) is sam-
pled from a Bernoulli distribution parameterized
in terms of the current stack representation. Let-
ting (hprev,gprev) = top(S), we have

zt ∼ Bernoulli(pt), pt = σ(w>hprev + b).
Subsequent generation depend on zt:

• If zt = 0 (SHIFT), the model first generates a
terminal symbol via sampling from a categori-
cal distribution whose parameters come from an
affine transformation and a softmax,

x ∼ softmax(Whprev + b).
Then the generated terminal is shifted onto the
stack using a stack LSTM,

hnext = LSTM(ex,hprev),

push(S, (hnext, ex)),

where ex is the word embedding for x.



1107

• If zt = 1 (REDUCE), we pop the last two ele-
ments off the stack,

(hr,gr) = pop(S), (hl,gl) = pop(S),

and obtain a new representation that combines
the left/right constituent representations using a
tree LSTM (Tai et al., 2015; Zhu et al., 2015),

gnew = TreeLSTM(gl,gr).

Note that we use gl and gr to obtain the new
representation instead of hl and hr.3 We then
update the stack using gnew,

(hprev,gprev) = top(S),

hnew = LSTM(gnew,hprev),

push(S, (hnew,gnew)).

The generation process continues until an end-of-
sentence symbol is generated. The parameters θ of
the generative model are w, b,W,b, and the pa-
rameters of the stack/tree LSTMs. For a sentence
x = [x1, . . . , xT ] of length T , the binary parse tree
is given by the binary vector z = [z1, . . . , z2T−1].4

The joint log likelihood decomposes as a sum of
terminal/action log likelihoods,

log pθ(x, z) =
T∑
t=1

log pθ(xt |x<t, z<n(t))︸ ︷︷ ︸
log pθ(x | z)

+

2T−1∑
j=1

log pθ(zj |x<m(j), z<j)︸ ︷︷ ︸
log pθ(z |x<z)

, (1)

where z<n(t) refers to all actions before generating
the t-th word, and similarly x<m(j) refers to all
words generated before taking the j-th action. For
brevity, from here on we will use log pθ(x | z) to
refer to the first term (terminal log likelihood) and
log pθ(z |x<z) to refer to the second term (action
log likelihood) in the above decomposition.5

3The update equations for the tree LSTM (and the stack
LSTM) also involve cell states in addition to the hidden
states. To reduce notational clutter we do not explicitly show
the cell states and instead subsume them into g. If one (or
both) of the inputs to the tree LSTM is a word embedding,
the associated cell state is taken to be zero. See Tai et al.
(2015) for the exact parameterization.

4As it stands, the support of z is {0, 1}2T−1, all binary
vectors of length 2T − 1. To restrict our distribution to ZT
(binary vectors which describe valid trees), we constrain zt to
be valid at each time step, which amounts to deterministically
choosing zt = 0 (SHIFT) if there are fewer than two elements
(not counting the initial zero tuple) on the stack.

5The action log likelihood is the sum of log conditional
priors, which is obviously different from the unconditional
log prior log pθ(z) = log

∑
x pθ(x, z).

In the supervised case where ground-truth z
is available, we can straightforwardly perform
gradient-based optimization to maximize the joint
log likelihood log pθ(x, z). In the unsupervised
case, the standard approach is to maximize the log
marginal likelihood,

log pθ(x) = log
∑

z′∈ZT

pθ(x, z
′).

However this summation is intractable be-
cause zt fully depends on all previous actions
[z1, . . . , zt−1]. Even if this summation were
tractable, it is not clear that meaningful latent
structures would emerge given the lack of explicit
independence assumptions in the RNNG (e.g. it is
clearly not context-free). We handle these issues
with amortized variational inference.

2.2 Amortized Variational Inference
Amortized variational inference (Kingma and
Welling, 2014) defines a trainable inference net-
work φ that parameterizes qφ(z |x), a variational
posterior distribution, in this case over parse trees
z given the sentence x. This distribution is used to
form an evidence lower bound (ELBO) on the log
marginal likelihood,

ELBO(θ, φ;x) = Eqφ(z |x)
[
log

pθ(x, z)

qφ(z |x)

]
.

We maximize the ELBO with respect to both
model parameters θ and inference network param-
eters φ. The ELBO is still intractable to calculate
exactly, but this formulation will allow us to ob-
tain unbiased gradient estimators based on Monte
Carlo sampling.

Observe that rearranging the ELBO gives the
following optimization problem,

max
θ,φ

log pθ(x)−KL[qφ(z |x) ‖ pθ(z |x)].

Thus, φ is trained to match the variational poste-
rior qφ(z |x) to the true posterior pθ(z |x), but θ is
also trained to match the true posterior to the vari-
ational posterior. Indeed, there is some evidence to
suggest that generative models trained with amor-
tized variational inference (i.e. variational autoen-
coders) learn posterior distributions that are close
to the variational family (Cremer et al., 2018).

We can use this to our advantage with an in-
ference network that injects inductive bias. We
propose to do this by using a context-free model
for the inference network, in particular, a neural
CRF parser (Durrett and Klein, 2015). This choice



1108

can seen as a form of posterior regularization that
limits posterior flexibility of the overly powerful
RNNG generative model.6,7

The parameterization of span scores is similar to
recent works (Wang and Chang, 2016; Stern et al.,
2017; Kitaev and Klein, 2018): we add position
embeddings to word embeddings and run a bidi-
rectional LSTM over the input representations to
obtain the forward [

−→
h 1, . . . ,

−→
h T ] and backward

[
←−
h 1, . . . ,

←−
h T ] hidden states. The score sij ∈ R

for a constituent spanning xi to xj is given by,

sij = MLP([
−→
h j+1 −

−→
h i;
←−
h i−1 −

←−
h j ]).

Letting B be the binary matrix representation of a
tree (Bij = 1 means there is a constituent span-
ning xi and xj), the CRF parser defines a distri-
bution over binary trees via the Gibbs distribution,

qφ(B |x) =
1

ZT (x)
exp

(∑
i≤j

Bijsij

)
,

where ZT (x) is the partition function,

ZT (x) =
∑

B′∈BT

exp
(∑
i≤j

B′ijsij

)
,

and φ denotes the parameters of the inference net-
work (i.e. the bidirectional LSTM and the MLP).
Calculating ZT (x) requires a summation over an
exponentially-sized set BT ⊂ {0, 1}T×T , the set
of all binary trees over a length T sequence. How-
ever we can perform the summation in O(T 3) us-
ing the inside algorithm (Baker, 1979), shown in

6While it has a similar goal, this formulation differs the
from posterior regularization as formulated by Ganchev et al.
(2010), which constrains the distributional family via linear
constraints on posterior expectations. In our case, the condi-
tional independence assumptions in the CRF lead to a curved
exponential family where the vector of natural parameters has
fewer dimensions than the vector of sufficient statistics of the
full exponential family. This curved exponential family is a
subset of the marginal polytope of the full exponential fam-
ily, but it is an intersection of both linear and nonlinear man-
ifolds, and therefore cannot be characterized through linear
constraints over posterior expectations.

7In preliminary experiments, we also attempted to learn
latent trees with a transition-based parser (which does not
make explicit independence assumptions) that looks at the en-
tire sentence. However we found that under this setup, the in-
ference network degenerated into a local minimum whereby
it always generated left-branching trees despite various opti-
mization strategies. Williams et al. (2018) observe a similar
phenomenon in the context of learning latent trees for classi-
fication tasks. However Li et al. (2019) find that it is possible
use a transition-based parser as the inference network for de-
pendency grammar induction, if the inference network is con-
strained via posterior regularization (Ganchev et al., 2010)
based on universal syntactic rules (Naseem et al., 2010).

Algorithm 1 Inside algorithm for calculating ZT (x)

1: procedure INSIDE(s) . scores sij for i ≤ j
2: for i := 1 to T do . length-1 spans
3: β[i, i] = sii
4: for ` := 1 to T − 1 do . span length
5: for i := 1 to T − ` do . span start
6: j = i+ ` . span end
7: β[i, j] =

∑j−1
k=i sij · β[i, k] · β[k + 1, j]

8: return β[1, T ] . return partition function ZT (x)

Algorithm 1. This computation is itself differen-
tiable and amenable to gradient-based optimiza-
tion. Finally, letting f : BT → ZT be the bijec-
tion between the binary tree matrix representation
and a sequence of SHIFT/REDUCE actions, the in-
ference network defines a distribution over ZT via
qφ(z |x) , qφ(f−1(z) |x).
2.3 Optimization
For optimization, we use the following variant of
the ELBO,

Eqφ(z |x)[log pθ(x, z)] +H[qφ(z |x)],

where H[qφ(z |x)] = Eqφ(z |x)[− log qφ(z |x)] is
the entropy of the variational posterior. A Monte
Carlo estimate for the gradient with respect to θ is

∇θ ELBO(θ, φ;x) ≈
1

K

K∑
k=1

∇θ log pθ(x, z(k)),

with samples z(1), . . . , z(K) from qφ(z |x). Sam-
pling uses the intermediate values calculated dur-
ing the inside algorithm to sample split points re-
cursively (Goodman, 1998; Finkel et al., 2006),
as shown in Algorithm 2. The gradient with re-
spect to φ involves two parts. The entropy term
H[qφ(z |x)] can be calculated exactly in O(T 3),
again using the intermediate values from the in-
side algorithm (see Algorithm 3).8 Since each
step of this dynamic program is differentiable, we
can obtain the gradient ∇φH[qφ(z |x)] using au-
tomatic differentation.9 An estimator for the gra-
dient with respect to Eqφ(z |x)[log pθ(x, z)] is ob-
tained via the score function gradient estimator
(Glynn, 1987; Williams, 1992),
∇φEqφ(z |x)[log pθ(x, z)]

= Eqφ(z |x)[log pθ(x, z)∇φ log qφ(z |x)]

≈ 1
K

K∑
k=1

log pθ(x, z
(k))∇φ log qφ(z(k) |x).

8We adapt the algorithm for calculating tree entropy in
PCFGs from Hwa (2000) to the CRF case.

9∇φH[qφ(z |x)] can also be computed using the inside-
outside algorithm and a second-order expectation semir-
ing (Li and Eisner, 2009), which has the same asymptotic
runtime complexity but generally better constants.



1109

Algorithm 2 Top-down sampling a tree from qφ(z |x)
1: procedure SAMPLE(β) . β from running INSIDE(s)
2: B = 0 . binary matrix representation of tree
3: Q = [(1, T )] . queue of constituents
4: while Q is not empty do
5: (i, j) = pop(Q)
6: τ =

∑j−1
k=i β[i, k] · β[k + 1, j]

7: for k := i to j − 1 do . get distribution over splits
8: wk = (β[i, k] · β[k + 1, j])/τ
9: k ∼ Cat([wi, . . . , wj−1]) . sample a split point

10: Bi,k = 1, Bk+1,j = 1 . update B
11: if k > i then . if left child has width > 1
12: push(Q, (i, k)) . add to queue
13: if k + 1 < j then . if right child has width > 1
14: push(Q, (k + 1, j)) . add to queue
15: z = f(B) . f : BT → ZT maps matrix represen-

tation of tree to sequence of actions.
16: return z

The above estimator is unbiased but typically suf-
fers from high variance. To reduce variance, we
use a control variate derived from an average of
the other samples’ joint likelihoods (Mnih and
Rezende, 2016), yielding the following estimator,

1

K

K∑
k=1

(log pθ(x, z
(k))− r(k))∇φ log qφ(z(k) |x),

where r(k) = 1K−1
∑

j 6=k log pθ(x, z
(j)). This

control variate worked better than alternatives
such as estimates of baselines from an auxiliary
network (Mnih and Gregor, 2014; Deng et al.,
2018) or a language model (Yin et al., 2018).

3 Experimental Setup
3.1 Data
For English we use the Penn Treebank (Marcus
et al., 1993, PTB) with splits and preprocessing
from Dyer et al. (2016) which retains punctu-
ation and replaces singleton words with Berke-
ley parser’s mapping rules, resulting in a vocab-
ulary of 23,815 word types.10 Notably this is
much larger than the standard PTB LM setup from
Mikolov et al. (2010) which uses 10K types.11

Also different from the LM setup, we model each
sentence separately instead of carrying informa-
tion across sentence boundaries, as the RNNG is
a generative model of sentences. Hence our per-
plexity numbers are not comparable to the PTB
LM results (Melis et al., 2018; Merity et al., 2018;
Yang et al., 2018).

Since the PTB is rather small, and since the
URNNG does not require annotation, we also test
our approach on a subset of the one billion word

10https://github.com/clab/rnng
11Both versions of the PTB data can be obtained from http:

//demo.clab.cs.cmu.edu/cdyer/ptb-lm.tar.gz.

Algorithm 3 Calculating the tree entropy H[qφ(z |x)]
1: procedure ENTROPY(β) . β from running INSIDE(s)
2: for i := 1 to T do . initialize entropy table
3: H[i, i] = 0
4: for l := 1 to T − 1 do . span length
5: for i := 1 to T − l do . span start
6: j = i+ l . span end
7: τ =

∑j−1
u=i β[i, u] · β[u+ 1, j]

8: for u := i to j − 1 do
9: wu = (β[i, u] · β[u+ 1, j])/τ

10: H[i, j] =
∑j−1
u=i(H[i, u] +H[u+ 1, j]

11: − logwu) · wu
12: return H[1, T ] . return tree entropy H[qφ(z |x)]

corpus (Chelba et al., 2013). We randomly sam-
ple 1M sentences for training and 2K sentences
for validation/test, and limit the vocabulary to 30K
word types. While still a subset of the full corpus
(which has 30M sentences), this dataset is two or-
ders of magnitude larger than PTB. Experiments
on Chinese utilize version 5.1 of the Chinese Penn
Treebank (CTB) (Xue et al., 2005), with the same
splits as in Chen and Manning (2014). Singleton
words are replaced with a single 〈UNK〉 token, re-
sulting in a vocabulary of 17,489 word types.

3.2 Training and Hyperparameters
The stack LSTM has two layers with input/hidden
size equal to 650 and dropout of 0.5. The tree
LSTM also has 650 units. The inference network
uses a one-layer bidirectional LSTM with 256 hid-
den units, and the MLP (to produce span scores
sij for i ≤ j) has a single hidden layer with a
ReLU nonlinearity followed by layer normaliza-
tion (Ba et al., 2016) and dropout of 0.5. We share
word embeddings between the generative model
and the inference network, and also tie weights
between the input/output word embeddings (Press
and Wolf, 2016).

Optimization of the model itself required stan-
dard techniques for avoiding posterior collapse in
VAEs.12 We warm-up the ELBO objective by
linearly annealing (per batch) the weight on the
conditional prior log pθ(z |x<z) and the entropy
H[qφ(z |x)] from 0 to 1 over the first two epochs
(see equation (1) for definition of log pθ(z |x<z)).
This is analogous to KL-annealing in VAEs with
continuous latent variables (Bowman et al., 2016;
Sønderby et al., 2016). We train for 18 epochs
(enough for convergence for all models) with a
batch size of 16 and K = 8 samples for the Monte
Carlo gradient estimators. The generative model is
optimized with SGD with learning rate equal to 1,

12Posterior collapse in our context means that qφ(z |x) al-
ways produced trivial (always left or right branching) trees.



1110

except for the affine layer that produces a distribu-
tion over the actions, which has learning rate 0.1.
Gradients of the generative model are clipped at
5. The inference network is optimized with Adam
(Kingma and Ba, 2015) with learning rate 0.0001,
β1 = 0.9, β2 = 0.999, and gradient clipping at
1. As Adam converges significantly faster than
SGD (even with a much lower learning rate), we
stop training the inference network after the first
two epochs. Initial model parameters are sampled
from U [−0.1, 0.1]. The learning rate starts decay-
ing by a factor of 2 each epoch after the first epoch
at which validation performance does not improve,
but this learning rate decay is not triggered for
the first eight epochs to ensure adequate training.
We use the same hyperparameters/training setup
for both PTB and CTB. For experiments on (the
subset of) the one billion word corpus, we use a
smaller dropout rate of 0.1. The baseline RNNLM
also uses the smaller dropout rate.

All models are trained with an end-of-sentence
token, but for perplexity calculation these tokens
are not counted to be comparable to prior work
(Dyer et al., 2016; Kuncoro et al., 2017; Buys and
Blunsom, 2018). To be more precise, the inference
network does not make use of the end-of-sentence
token to produce parse trees, but the generative
model is trained to generate the end-of-sentence
token after the final REDUCE operation.

3.3 Baselines
We compare the unsupervised RNNG (URNNG)
against several baselines: (1) RNNLM, a standard
RNN language model whose size is the same as
URNNG’s stack LSTM; (2) Parsing Reading Pre-
dict Network (PRPN) (Shen et al., 2018), a neu-
ral language model that uses gated attention lay-
ers to embed soft tree-like structures into a neu-
ral network (and among the current state-of-the-art
in grammar induction from words on the full cor-
pus); (3) RNNG with trivial trees (left branching,
right branching, random); (4) supervised RNNG
trained on unlabeled, binarized gold trees.13 Note
that the supervised RNNG also trains a discrim-
inative parser qφ(z |x) (alongside the generative
model pθ(x, z)) in order to sample parse forests
for perplexity evaluation (i.e. importance sam-
pling). This discriminative parser has the same ar-

13We use right branching binarization—Matsuzaki et al.
(2005) find that differences between various binarization
schemes have marginal impact. Our supervised RNNG
therefore differs the original RNNG, which trains on non-
binarized trees and does not ignore constituent labels.

PTB CTB
Model PPL F1 PPL F1

RNNLM 93.2 – 201.3 –
PRPN (default) 126.2 32.9 290.9 32.9
PRPN (tuned) 96.7 41.2 216.0 36.1
Left Branching Trees 100.9 10.3 223.6 12.4
Right Branching Trees 93.3 34.8 203.5 20.6
Random Trees 113.2 17.0 209.1 17.4
URNNG 90.6 40.7 195.7 29.1

RNNG 88.7 68.1 193.1 52.3
RNNG→ URNNG 85.9 67.7 181.1 51.9

Oracle Binary Trees – 82.5 – 88.6

Table 1: Language modeling perplexity (PPL) and
grammar induction F1 scores on English (PTB) and
Chinese (CTB) for the different models. Note that our
PTB setup from Dyer et al. (2016) differs consider-
ably from the usual language modeling setup (Mikolov
et al., 2010) since we model each sentence indepen-
dently and use a much larger vocabulary (see §3.1).
chitecture as URNNG’s inference network. For all
models, we perform early stopping based on vali-
dation perplexity.

4 Results and Discussion
4.1 Language Modeling
Table 1 shows perplexity for the different models
on PTB/CTB. As a language model URNNG out-
performs an RNNLM and is competitive with the
supervised RNNG.14 The left branching baseline
performs poorly, implying that the strong perfor-
mance of URNNG/RNNG is not simply due to
the additional depth afforded by the tree LSTM
composition function (a left branching tree, which
always performs REDUCE when possible, is the
“deepest” model). The right branching baseline
is essentially equivalent to an RNNLM and hence
performs similarly. We found PRPN with de-
fault hyperparameters (which obtains a perplex-
ity of 62.0 in the PTB setup from Mikolov et al.
(2010)) to not perform well, but tuning hyperpa-
rameters improves performance.15 The supervised
RNNG performs well as a language model, despite
being trained on the joint (rather than marginal)
likelihood objective.16 This indicates that explicit

14For RNNG and URNNG we estimate the log
marginal likelihood (and hence, perplexity) with
K = 1000 importance-weighted samples, log pθ(x) ≈
log
(

1
K

∑K
k=1

log p(x,z(k))

qφ(z
(k) |x)

)
. During evaluation only, we

also flatten qφ(z |x) by dividing span scores sij by a
temperature term 2.0 before feeding it to the CRF.

15Using the code from https://github.com/yikangshen/
PRPN, we tuned model size, initialization, dropout, learning
rate, and use of batch normalization.

16RNNG is trained to maximize log pθ(x, z) while
URNNG is trained to maximize (a lower bound on) the lan-
guage modeling objective log pθ(x).



1111

Figure 2: Perplexity of the different models grouped by
sentence length on PTB.
modeling of syntax helps generalization even with
richly-parameterized neural models. Encouraged
by these observations, we also experiment with
a hybrid approach where we train a supervised
RNNG first and continue fine-tuning the model
(including the inference network) on the URNNG
objective (RNNG→ URNNG in Table 1).17 This
approach results in nontrivial perplexity improve-
ments, and suggests that it is potentially possi-
ble to improve language models with supervision
on parsed data. In Figure 2 we show perplexity
by sentence length. We find that a standard lan-
guage model (RNNLM) is better at modeling short
sentences, but underperforms models that explic-
itly take into account structure (RNNG/URNNG)
when the sentence length is greater than 10. Ta-
ble 2 (top) compares our results against prior work
on this version of the PTB, and Table 2 (bot-
tom) shows the results on a 1M sentence sub-
set of the one billion word corpus, which is two
orders of magnitude larger than PTB. On this
larger dataset URNNG still improves upon the
RNNLM. We also trained an RNNG (and RNNG
→URNNG) on this dataset by parsing the training
set with the self-attentive parser from Kitaev and
Klein (2018).18 These models improve upon the
RNNLM but not the URNNG, potentially high-
lighting the limitations of using predicted trees for
supervising RNNGs.

4.2 Grammar Induction
Table 1 also shows the F1 scores for grammar
induction. Note that we induce latent trees di-
rectly from words on the full dataset.19 For
RNNG/URNNG we obtain the highest scoring

17We fine-tune for 10 epochs and use a smaller learning
rate of 0.1 for the generative model.

18To parse the training set we use the benepar en2
model from https://github.com/nikitakit/self-attentive-parser,
which obtains an F1 score of 95.17 on the PTB test set.

19Past work on grammar induction usually train/evaluate
on short sentences and also assume access to gold POS tags
(Klein and Manning, 2002; Smith and Eisner, 2004; Bod,
2006). However more recent works do train directly words
(Jin et al., 2018; Shen et al., 2018; Drozdov et al., 2019).

PTB PPL

KN 5-gram (Dyer et al., 2016) 169.3
RNNLM (Dyer et al., 2016) 113.4
Original RNNG (Dyer et al., 2016) 102.4
Stack-only RNNG (Kuncoro et al., 2017) 101.2
Gated-Attention RNNG (Kuncoro et al., 2017) 100.9
Generative Dep. Parser (Buys and Blunsom, 2015) 138.6
RNNLM (Buys and Blunsom, 2018) 100.7
Sup. Syntactic NLM (Buys and Blunsom, 2018) 107.6
Unsup. Syntactic NLM (Buys and Blunsom, 2018) 125.2
PRPN† (Shen et al., 2018) 96.7
This work:

RNNLM 93.2
URNNG 90.6
RNNG 88.7
RNNG→ URNNG 85.9

1M Sentences PPL

PRPN† (Shen et al., 2018) 77.7
RNNLM 77.4
URNNG 71.8
RNNG‡ 72.9
RNNG‡→ URNNG 72.0

Table 2: (Top) Comparison of this work as a language
model against prior works on sentence-level PTB with
preprocessing from Dyer et al. (2016). Note that pre-
vious versions of RNNG differ from ours in terms of
parameterization and model size. (Bottom) Results on
a subset (1M sentences) of the one billion word corpus.
PRPN† is the model from Shen et al. (2018), whose hy-
perparameters were tuned by us. RNNG‡ is trained on
predicted parse trees from Kitaev and Klein (2018).
tree from qφ(z |x) through the Viterbi inside (i.e.
CKY) algorithm. We calculate unlabeled F1 using
evalb, which ignores punctuation and discards
trivial spans (width-one and sentence spans).20

Since we compare F1 against the original, non-
binarized trees (per convention), F1 scores of
models using oracle binarized trees constitute the
upper bounds.

We confirm the replication study of Htut et al.
(2018) and find that PRPN is a strong model for
grammar induction. URNNG performs on par
with PRPN on English but PRPN does better on
Chinese; both outperform right branching base-
lines. Table 3 further analyzes the learned trees
and shows the F1 score of URNNG trees against

20Available at https://nlp.cs.nyu.edu/evalb/. We evaluate
with COLLINS.prm parameter file and LABELED option
equal to 0. We observe that the setup for grammar induction
varies widely across different papers: lexicalized vs. unlex-
icalized; use of punctuation vs. not; separation of train/test
sets; counting sentence-level spans for evaluation vs. ig-
noring them; use of additional data; length cutoff for train-
ing/evaluation; corpus-level F1 vs. sentence-level F1; and,
more. In our survey of twenty or so papers, almost no two
papers were identical in their setup. Such variation makes
it difficult to meaningfully compare models across papers.
Hence, we report grammar induction results mainly for the
models and baselines considered in the present work.



1112

Tree PTB CTB

Gold 40.7 29.1
Left 9.2 8.4
Right 68.3 51.2
Self 92.3 87.3
RNNG 55.4 47.1
PRPN 41.0 47.2

Label URNNG PRPN

SBAR 74.8% 28.9%
NP 39.5% 63.9%
VP 76.6% 27.3%
PP 55.8% 55.1%
ADJP 33.9% 42.5%
ADVP 50.4% 45.1%

Table 3: (Left) F1 scores of URNNG against other
trees. “Self” refers to another URNNG trained with
a different random seed. (Right) Recall of constituents
by label for URNNG and PRPN. Recall for a particular
label is the fraction of ground truth constituents of that
label that were identified by the model.

F1 +PP

PRPN-UP‡ 39.8 45.4
PRPN-LM‡ 42.8 42.4
ON-LSTM‡ (Shen et al., 2019) 49.4 −
DIORA‡ (Drozdov et al., 2019) 49.6 56.2
PRPN (tuned) 49.0 49.9
URNNG 52.4 52.4

Table 4: PTB F1 scores using the same evaluation
setup as Drozdov et al. (2019), which evaluates against
binarized trees, counts punctuation and trivial spans,
and uses sentence-level F1. +PP indicates a post-
processing heuristic which directly attaches trailing
punctuation to the root. This does not change URNNG
results since it learns to do so anyway. Results with ‡

are copied from Table 1 of Drozdov et al. (2019).
other trees (left), and the recall of URNNG/PRPN
trees against ground truth constituents (right). We
find that trees induced by URNNG and PRPN
are quite different; URNNG is more sensitive to
SBAR and VP, while PRPN is better at identifying
NP. While left as future work, this naturally sug-
gests a hybrid approach wherein the intersection
of constituents from URNNG and PRPN is used to
create a corpus of partially annotated trees, which
can be used to guide another model, e.g. via poste-
rior regularization (Ganchev et al., 2010) or semi-
supervision (Hwa, 1999). Finally, Table 4 com-
pares our results using the same evaluation setup
as in Drozdov et al. (2019), which differs consid-
erably from our setup.

4.3 Distributional Metrics
Table 5 shows some standard metrics related
to the learned generative model/inference net-
work. The “reconstruction” perplexity based on
Eqφ(z |x)[log pθ(x | z)] is much lower than regular
perplexity, and further, the Kullback-Leibler diver-
gence between the conditional prior and the varia-
tional posterior, given by

Eqφ(z |x)
[
log

qφ(z |x)
pθ(z |x<z)

]
,

PTB CTB
RNNG URNNG RNNG URNNG

PPL 88.7 90.6 193.1 195.7
Recon. PPL 74.6 73.4 183.4 151.9
KL 7.10 6.13 11.11 8.91
Prior Entropy 7.65 9.61 9.48 15.13
Post. Entropy 1.56 2.28 6.23 5.75
Unif. Entropy 26.07 26.07 30.17 30.17

Table 5: Metrics related to the generative
model/inference network for RNNG/URNNG.
For the supervised RNNG we take the “inference
network” to be the discriminative parser trained
alongside the generative model (see §3.3). Re-
con. PPL is the reconstruction perplexity based on
Eqφ(z |x)[log pθ(x | z)], and KL is the Kullback-Leibler
divergence. Prior entropy is the entropy of the con-
ditional prior pθ(z |x<z), and uniform entropy is the
entropy of the uniform distribution over binary trees.
is highly nonzero. (See equation (1) for definitions
of log pθ(x | z) and log pθ(z |x<z)). This indi-
cates that the latent space is being used in a mean-
ingful way and that there is no posterior collapse
(Bowman et al., 2016). As expected, the entropy
of the variational posterior is much lower than the
entropy of the conditional prior, but there is still
some uncertainty in the posterior.

4.4 Syntactic Evaluation
We perform a syntactic evaluation of the differ-
ent models based on the setup from Marvin and
Linzen (2018): the model is given two mini-
mally different sentences, one grammatical and
one ungrammatical, and must identify the gram-
matical sentence by assigning it higher probabil-
ity.21 Table 6 shows the accuracy results. Overall
the supervised RNNG significantly outperforms
the other models, indicating opportunities for fur-
ther work in unsupervised modeling. While the
URNNG does slightly outperform an RNNLM,
the distribution of errors made from both models
are similar, and thus it is not clear whether the out-
performance is simply due to better perplexity or
learning different structural biases.

4.5 Limitations
There are several limitations to our approach.
For one, the URNNG takes considerably more
time/memory to train than a standard language
model due to the O(T 3) dynamic program in
the inference network, multiple samples to obtain
low-variance gradient estimators, and dynamic
computation graphs that make efficient batching

21We modify the publicly available dataset from https://
github.com/BeckyMarvin/LM syneval to only keep sentence
pairs that did not have any unknown words with respect to our
vocabulary, resulting in 80K sentence pairs for evaluation.



1113

RNNLM PRPN RNNG URNNG

PPL 93.2 96.7 88.7 90.6

Overall 62.5% 61.9% 69.3% 64.6%
Subj. 63.5% 63.7% 89.4% 67.2%
Obj. Rel. 62.6% 61.0% 67.6% 65.7%
Refl. 60.7% 68.8% 57.3% 60.5%
NPI 58.7% 39.5% 46.8% 55.0%

Table 6: Syntactic evaluation based on the setup from
Marvin and Linzen (2018). Subj. is subject-verb agree-
ment in sentential complement, across prepositional
phrase/subjective relative clause, and VP coordination;
Obj. Rel. refers to subject-verb agreement in/across an
objective relative clause; Refl. refers to reflexive pro-
noun agreement with antecedent; NPI is negative po-
larity items.
nontrivial. The model is sensitive to hyperparam-
eters and required various optimization strategies
(e.g. separate optimizers for the inference network
and the generative model) to avoid posterior col-
lapse. Finally, the URNNG also seemed to rely
heavily on punctuation to identify constituents and
we were unable to improve upon a right-branching
baseline when training the URNNG on a version
of PTB where punctuation is removed.22

5 Related Work
There has been much work on incorporating tree
structures into deep models for syntax-aware lan-
guage modeling, both for unconditional (Emami
and Jelinek, 2005; Buys and Blunsom, 2015; Dyer
et al., 2016) and conditional (Yin and Neubig,
2017; Alvarez-Melis and Jaakkola, 2017; Rabi-
novich et al., 2017; Aharoni and Goldberg, 2017;
Eriguchi et al., 2017; Wang et al., 2018; Gu et al.,
2018) cases. These approaches generally rely on
annotated parse trees during training and maxi-
mizes the joint likelihood of sentence-tree pairs.
Prior work on combining language modeling and
unsupervised tree learning typically embed soft,
tree-like structures as hidden layers of a deep net-
work (Cho et al., 2014; Chung et al., 2017; Shen
et al., 2018, 2019). In contrast, Buys and Blun-
som (2018) make Markov assumptions and per-
form exact marginalization over latent dependency

22Many prior works that induce trees directly from words
often employ additional heuristics based on punctuation
(Seginer, 2007; Ponvert et al., 2011; Spitkovsky et al., 2013;
Parikh et al., 2014), as punctuation (e.g. comma) is usu-
ally a reliable signal for start/end of constituent spans. The
URNNG still has to learn to rely on punctuation, similar to
recent works such as depth-bounded PCFGs (Jin et al., 2018)
and DIORA (Drozdov et al., 2019). In contrast, PRPN (Shen
et al., 2018) and Ordered Neurons (Shen et al., 2019) induce
trees by directly training on corpus without punctuation. We
also reiterate that punctuation is used during training but ig-
nored during evaluation (except in Table 4).

trees. Our work is also related to the recent line
of work on learning latent trees as part of a deep
model through supervision on other tasks, typ-
ically via differentiable structured hidden layers
(Kim et al., 2017; Bradbury and Socher, 2017; Liu
and Lapata, 2018; Tran and Bisk, 2018; Peng et al.,
2018; Niculae et al., 2018; Liu et al., 2018), policy
gradient-based approaches (Yogatama et al., 2017;
Williams et al., 2018; Havrylov et al., 2019), or
differentiable relaxations (Choi et al., 2018; Mail-
lard and Clark, 2018).

The variational approximation uses amortized
inference (Kingma and Welling, 2014; Mnih and
Gregor, 2014; Rezende et al., 2014), in which an
inference network is used to obtain the variational
posterior for each observed x. Since our inference
network is structured (i.e., a CRF), it is also re-
lated to CRF autoencoders (Ammar et al., 2014)
and structured VAEs (Johnson et al., 2016; Krish-
nan et al., 2017), which have been used previously
for unsupervised (Cai et al., 2017; Drozdov et al.,
2019; Li et al., 2019) and semi-supervised (Yin
et al., 2018; Corro and Titov, 2019) parsing.

6 Conclusion
It is an open question as to whether explicit mod-
eling of syntax significantly helps neural mod-
els. Strubell et al. (2018) find that supervising
intermediate attention layers with syntactic heads
improves semantic role labeling, while Shi et al.
(2018) observe that for text classification, syntac-
tic trees only have marginal impact. Our work sug-
gests that at least for language modeling, incorpo-
rating syntax either via explicit supervision or as
latent variables does provide useful inductive bi-
ases and improves performance.

Finally, in modeling child language acquisition,
the complex interaction of the parser and the gram-
matical knowledge being acquired is the object
of much investigation (Trueswell and Gleitman,
2007); our work shows that apparently grammati-
cal constraints can emerge from the interaction of
a constrained parser and a more general grammar
learner, which is an intriguing but underexplored
hypothesis for explaining human linguistic biases.

Acknowledgments
We thank the members of the DeepMind language
team for helpful feedback. YK is supported by
a Google Fellowship. AR is supported by NSF
Career 1845664.



1114

References
Roee Aharoni and Yoav Goldberg. 2017. Towards

String-to-Tree Neural Machine Translation. In Pro-
ceedings of ACL.

David Alvarez-Melis and Tommi S. Jaakkola. 2017.
Tree-structured Decoding with Doubly-Recurrent
Neural Networks. In Proceedings of ICLR.

Waleed Ammar, Chris Dyer, and Noah A. Smith. 2014.
Conditional Random Field Autoencoders for Unsu-
pervised Structured Prediction. In Proceedings of
NIPS.

Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hin-
ton. 2016. Layer Normalization. In Proceedings of
NIPS.

James K. Baker. 1979. Trainable Grammars for Speech
Recognition. In Proceedings of the Spring Confer-
ence of the Acoustical Society of America.

Rens Bod. 2006. An All-Subtrees Approach to Unsu-
pervised Parsing. In Proceedings of ACL.

Samuel R. Bowman, Luke Vilnis, Oriol Vinyal, An-
drew M. Dai, Rafal Jozefowicz, and Samy Ben-
gio. 2016. Generating Sentences from a Continuous
Space. In Proceedings of CoNLL.

James Bradbury and Richard Socher. 2017. Towards
Neural Machine Translation with Latent Tree Atten-
tion. In Proceedings of the 2nd Workshop on Struc-
tured Prediction for Natural Language Processing.

Jan Buys and Phil Blunsom. 2015. Generative Incre-
mental Dependency Parsing with Neural Networks.
In Proceedings of ACL.

Jan Buys and Phil Blunsom. 2018. Neural Syntactic
Generative Models with Exact Marginalization. In
Proceedings of NAACL.

Jiong Cai, Yong Jiang, and Kewei Tu. 2017. CRF Au-
toencoder for Unsupervised Dependency Parsing. In
Proceedings of EMNLP.

Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,
Thorsten Brants, Phillipp Koehn, and Tony Robin-
son. 2013. One Billion Word Benchmark for Mea-
suring Progress in Statistical Language Modeling.
arXiv:1312.3005.

Danqi Chen and Christopher D. Manning. 2014. A Fast
and Accurate Dependency Parser using Neural Net-
works. In Proceedings of EMNLP.

Kyunghyun Cho, Bart van Merrienboer, Dzmitry Bah-
danau, and Yoshua Bengio. 2014. On the Properties
of Neural Machine Translation: Encoder-Decoder
Approaches. In Proceedings of Eighth Workshop on
Syntax, Semantics and Structure in Statistical Trans-
lation.

Jihun Choi, Kang Min Yoo, and Sang goo Lee. 2018.
Learning to Compose Task-Specific Tree Structures.
In Proceedings of AAAI.

Junyoung Chung, Sungjin Ahn, and Yoshua Bengio.
2017. Hierarchical Multiscale Recurrent Neural
Networks. In Proceedings of ICLR.

Caio Corro and Ivan Titov. 2019. Differentiable
Perturb-and-Parse: Semi-Supervised Parsing with a
Structured Variational Autoencoder. In Proceedings
of ICLR.

Chris Cremer, Xuechen Li, and David Duvenaud.
2018. Inference Suboptimality in Variational Au-
toencoders. In Proceedings of ICML.

Yuntian Deng, Yoon Kim, Justin Chiu, Demi Guo, and
Alexander M. Rush. 2018. Latent Alignment and
Variational Attention. In Proceedings of NIPS.

Andrew Drozdov, Patrick Verga, Mohit Yadev, Mohit
Iyyer, and Andrew McCallum. 2019. Unsupervised
Latent Tree Induction with Deep Inside-Outside Re-
cursive Auto-Encoders. In Proceedings of NAACL.

Greg Durrett and Dan Klein. 2015. Neural CRF Pars-
ing. In Proceedings of ACL.

Chris Dyer, Miguel Ballesteros, Wang Ling, Austin
Matthews, and Noah A. Smith. 2015. Transition-
Based Dependency Parsing with Stack Long Short-
Term Memory. In Proceedings of ACL.

Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros,
and Noah A. Smith. 2016. Recurrent Neural Net-
work Grammars. In Proceedings of NAACL.

Ahmad Emami and Frederick Jelinek. 2005. A Neu-
ral Syntactic Language Model. Machine Learning,
60:195–227.

Akiko Eriguchi, Yoshimasa Tsuruoka, and Kyunghyun
Cho. 2017. Learning to Parse and Translate Im-
proves Neural Machine Translation. In Proceedings
of ACL.

Jenny Rose Finkel, Alex Kleeman, and Christopher D.
Manning. 2008. Efficient, Feature-based, Condi-
tional Random Field Parsing. In Proceedings of
ACL.

Jenny Rose Finkel, Christopher D. Manning, and An-
drew Y. Ng. 2006. Solving the Problem of Cas-
cading Errors: Approximate Bayesian Inference for
Linguistic Annotation Pipelines. In Proceedings of
EMNLP.

Kuzman Ganchev, João Graça, Jennifer Gillenwater,
and Ben Taskar. 2010. Posterior Regularization for
Structured Latent Variable Models. Journal of Ma-
chine Learning Research, 11:2001–2049.

Peter Glynn. 1987. Likelihood Ratio Gradient Estima-
tion: An Overview. In Proceedings of Winter Simu-
lation Conference.

Joshua Goodman. 1998. Parsing Inside-Out. PhD the-
sis, Harvard University.



1115

Jetic Gu, Hassan S. Shavarani, and Anoop Sarkar.
2018. Top-down Tree Structured Decoding with
Syntactic Connections for Neural Machine Transla-
tion and Parsing. In Proceedings of EMNLP.

John Hale, Chris Dyer, Adhiguna Kuncoro, and
Jonathan R. Brennan. 2018. Finding Syntax in Hu-
man Encephalography with Beam Search. In Pro-
ceedings of ACL.

Serhii Havrylov, Germán Kruszewski, and Armand
Joulin. 2019. Cooperative Learning of Disjoint Syn-
tax and Semantics. In Proceedings of NAACL.

Phu Mon Htut, Kyunghyun Cho, and Samuel R. Bow-
man. 2018. Grammar Induction with Neural Lan-
guage Models: An Unusual Replication. In Pro-
ceedings of EMNLP.

Rebecca Hwa. 1999. Supervised Grammar Induction
Using Training Data with Limited Constituent Infor-
mation. In Proceedings of ACL.

Rebecca Hwa. 2000. Sample Selection for Statistical
Grammar Induction. In Proceedings of EMNLP.

Lifeng Jin, Finale Doshi-Velez, Timothy Miller,
William Schuler, and Lane Schwartz. 2018. Un-
supervised Grammar Induction with Depth-bounded
PCFG. In Proceedings of TACL.

Mark Johnson, Thomas L. Griffiths, and Sharon Gold-
water. 2007. Bayesian Inference for PCFGs via
Markov chain Monte Carlo. In Proceedings of
NAACL.

Matthew Johnson, David K. Duvenaud, Alex
Wiltschko, Ryan P. Adams, and Sandeep R.
Datta. 2016. Composing Graphical Models with
Neural Networks for Structured Representations
and Fast Inference. In Proceedings of NIPS.

Kazuya Kawakami, Chris Dyer, and Phil Blunsom.
2018. Unsupervised Word Discovery with Segmen-
tal Neural Language Models. arXiv:1811.09353.

Yoon Kim, Carl Denton, Luong Hoang, and Alexan-
der M. Rush. 2017. Structured Attention Networks.
In Proceedings of ICLR.

Diederik P. Kingma and Jimmy Ba. 2015. Adam: A
Method for Stochastic Optimization. In Proceed-
ings of ICLR.

Diederik P. Kingma and Max Welling. 2014. Auto-
Encoding Variational Bayes. In Proceedings of
ICLR.

Nikita Kitaev and Dan Klein. 2018. Constituency Pars-
ing with a Self-Attentive Encoder. In Proceedings of
ACL.

Dan Klein and Christopher Manning. 2002. A Genera-
tive Constituent-Context Model for Improved Gram-
mar Induction. In Proceedings of ACL.

Rahul G. Krishnan, Uri Shalit, and David Sontag. 2017.
Structured Inference Networks for Nonlinear State
Space Models. In Proceedings of AAAI.

Adhiguna Kuncoro, Miguel Ballesteros, Lingpeng
Kong, Chris Dyer, Graham Neubig, and Noah A.
Smith. 2017. What Do Recurrent Neural Network
Grammars Learn About Syntax? In Proceedings of
EACL.

Adhiguna Kuncoro, Chris Dyer, John Hale, Dani Yo-
gatama, Stephen Clark, and Phil Blunsom. 2018.
LSTMs Can Learn Syntax-Sensitive Dependencies
Well, But Modeling Structure Makes Them Better.
In Proceedings of ACL.

Bowen Li, Jianpeng Cheng, Yang Liu, and Frank
Keller. 2019. Dependency Grammar Induction with
a Neural Variational Transition-based Parser. In
Proceedings of AAAI.

Zhifei Li and Jason Eisner. 2009. First- and Second-
Order Expectation Semirings with Applications to
Minimum-Risk Training on Translation Forests. In
Proceedings of EMNLP.

Yang Liu, Matt Gardner, and Mirella Lapata. 2018.
Structured Alignment Networks for Matching Sen-
tences. In Proceedings of EMNLP.

Yang Liu and Mirella Lapata. 2018. Learning Struc-
tured Text Representations. In Proceedings of
TACL.

Jean Maillard and Stephen Clark. 2018. Latent Tree
Learning with Differentiable Parsers: Shift-Reduce
Parsing and Chart Parsing. In Proceedings of the
Workshop on the Relevance of Linguistic Structure
in Neural Architectures for NLP.

Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a Large Anno-
tated Corpus of English: The Penn Treebank. Com-
putational Linguistics, 19:313–330.

Rebecca Marvin and Tal Linzen. 2018. Targeted Syn-
tactic Evaluation of Language Models. In Proceed-
ings of EMNLP.

Takuya Matsuzaki, Yusuke Miyao, and Junichi Tsujii.
2005. Probabilistic CFG with Latent Annotations.
In Proceedings of ACL.

Gábor Melis, Chris Dyer, and Phil Blunsom. 2018. On
the State of the Art of Evaluation in Neural Lan-
guage Models. In Proceedings of ICLR.

Stephen Merity, Nitish Shirish Keskar, and Richard
Socher. 2018. Regularizing and Optimizing LSTM
Language Models. In Proceedings of ICLR.

Tomas Mikolov, Martin Karafiat, Lukas Burget, Jan
Cernocky, and Sanjeev Khudanpur. 2010. Recurrent
Neural Network Based Language Model. In Pro-
ceedings of INTERSPEECH.



1116

Andriy Mnih and Karol Gregor. 2014. Neural varia-
tional inference and learning in belief networks. In
Proceedings of ICML.

Andriy Mnih and Danilo J. Rezende. 2016. Variational
Inference for Monte Carlo Objectives. In Proceed-
ings of ICML.

Tahira Naseem, Harr Chen, Regina Barzilay, and Mark
Johnson. 2010. Using Universal Linguistic Knowl-
edge to Guide Grammar Induction. In Proceedings
of EMNLP.

Vlad Niculae, André F. T. Martins, and Claire Cardie.
2018. Towards Dynamic Computation Graphs via
Sparse Latent Structure. In Proceedings of EMNLP.

Ankur P. Parikh, Shay B. Cohen, and Eric P. Xing.
2014. Spectral Unsupervised Parsing with Additive
Tree Metrics. In Proceedings of ACL.

Hao Peng, Sam Thomson, and Noah A. Smith. 2018.
Backpropagating through Structured Argmax using
a SPIGOT. In Proceedings of ACL.

Elis Ponvert, Jason Baldridge, and Katrin Erk. 2011.
Simpled Unsupervised Grammar Induction from
Raw Text with Cascaded Finite State Methods. In
Proceedings of ACL.

Ofir Press and Lior Wolf. 2016. Using the Output Em-
bedding to Improve Language Models. In Proceed-
ings of EACL.

Maxim Rabinovich, Mitchell Stern, and Dan Klein.
2017. Abstract Syntax Networks for Code Gener-
ation and Semantic Parsing. In Proceedings of ACL.

Danilo J. Rezende, Shakir Mohamed, and Daan Wier-
stra. 2014. Stochastic Backpropagation and Ap-
proximate Inference in Deep Generative Models. In
Proceedings of ICML.

Yoav Seginer. 2007. Fast Unsupervised Incremental
Parsing. In Proceedings of ACL.

Yikang Shen, Zhouhan Lin, Chin-Wei Huang, and
Aaron Courville. 2018. Neural Language Modeling
by Jointly Learning Syntax and Lexicon. In Pro-
ceedings of ICLR.

Yikang Shen, Shawn Tan, Alessandro Sordoni, and
Aaron Courville. 2019. Ordered Neurons: Integrat-
ing Tree Structures into Recurrent Neural Networks.
In Proceedings of ICLR.

Haoyue Shi, Hao Zhou, Jiaze Chen, and Lei Li. 2018.
On Tree-based Neural Sentence Modeling. In Pro-
ceedings of EMNLP.

Noah A. Smith and Jason Eisner. 2004. Annealing
Techniques for Unsupervised Statistical Language
Learning. In Proceedings of ACL.

Casper Kaae Sønderby, Tapani Raiko, Lars Maaløe,
Søren Kaae Sønderby, and Ole Winther. 2016. Lad-
der Variational Autoencoders. In Proceedings of
NIPS.

Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Ju-
rafsky. 2013. Breaking Out of Local Optima with
Count Transforms and Model Recombination: A
Study in Grammar Induction. In Proceedings of
EMNLP.

Mitchell Stern, Jacob Andreas, and Dan Klein. 2017.
A Minimal Span-Based Neural Constituency Parser.
In Proceedings of ACL.

Emma Strubell, Patrick Verga, Daniel Andor,
David Weiss, and Andrew McCallum. 2018.
Linguistically-Informed Self-Attention for Seman-
tic Role Labeling. In Proceedings of EMNLP.

Kai Sheng Tai, Richard Socher, and Christopher D.
Manning. 2015. Improved Semantic Representa-
tions From Tree-Structured Long Short-Term Mem-
ory Networks. In Proceedings of ACL.

Ke Tran and Yonatan Bisk. 2018. Inducing Grammars
with and for Neural Machine Translation. In Pro-
ceedings of the 2nd Workshop on Neural Machine
Translation and Generation.

John C. Trueswell and Lila R. Gleitman. 2007. Learn-
ing to Parse and its Implications for Language Ac-
quisition. The Oxford Handbook of Psycholinguis-
tics.

Wenhui Wang and Baobao Chang. 2016. Graph-based
Dependency Parsing with Bidirectional LSTM. In
Proceedings of ACL.

Xinyi Wang, Hieu Pham, Pengcheng Yin, and Graham
Neubig. 2018. A Tree-based Decoder for Neural
Machine Translation. In Proceedings of EMNLP.

Adina Williams, Andrew Drozdov, and Samuel R.
Bowman. 2018. Do Latent Tree Learning Models
Identify Meaningful Structure in Sentences? In Pro-
ceedings of TACL.

Ronald J. Williams. 1992. Simple Statistical Gradient-
following Algorithms for Connectionist Reinforce-
ment Learning. Machine Learning, 8.

Sam Wiseman, Stuart M. Shieber, and Alexander M.
Rush. 2018. Learning Neural Templates for Text
Generation. In Proceedings of EMNLP.

Naiwen Xue, Fei Xia, Fu dong Chiou, and Marta
Palmer. 2005. The Penn Chinese Treebank: Phrase
Structure Annotation of a Large Corpus. Natural
Language Engineering, 11:207–238.

Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and
William W. Cohen. 2018. Breaking the Softmax
Bottleneck: A High-Rank RNN Language Model.
In Proceedings of ICLR.

Pengcheng Yin and Graham Neubig. 2017. A Syntac-
tic Neural Model for General-Purpose Code Gener-
ation. In Proceedings of ACL.



1117

Pengcheng Yin, Chunting Zhou, Junxian He, and Gra-
ham Neubig. 2018. StructVAE: Tree-structured La-
tent Variable Models for Semi-supervised Semantic
Parsing. In Proceedings of ACL.

Dani Yogatama, Phil Blunsom, Chris Dyer, Edward
Grefenstette, and Wang Ling. 2017. Learning to
Compose Words into Sentences with Reinforcement
Learning. In Proceedings of ICLR.

Xiaodan Zhu, Parinaz Sobhani, and Hongyu Guo.
2015. Long Short-Term Memory Over Tree Struc-
tures. In Proceedings of ICML.


