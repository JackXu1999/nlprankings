



















































UdS-Sant: English--German Hybrid Machine Translation System


Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 152–157,
Lisboa, Portugal, 17-18 September 2015. c©2015 Association for Computational Linguistics.

UdS-Sant: English–German Hybrid Machine Translation System

Santanu Pal1, Sudip Kumar Naskar2, Josef van Genabith1
1Universität des Saarlandes, Saarbrücken, Germany

2 Jadavpur University, Kolkata, India
{santanu.pal, josef.vangenabith}@uni-saarland.de

sudip.naskar@cse.jdvu.ac.in

Abstract

This paper describes the UdS-Sant
English–German Hybrid Machine Trans-
lation (MT) system submitted to the
Translation Task organized in the Work-
shop on Statistical Machine Translation
(WMT) 2015. Our proposed hybrid
system brings improvements over the
baseline system by incorporating ad-
ditional knowledge such as extracted
bilingual named entities and bilingual
phrase pairs induced from example-based
methods. The reported final submission
is the result of a hybrid system obtained
from confusion network based system
combination that combines the best per-
formance of each individual system in a
multi-engine pipeline.

1 Introduction

In this paper, we present Universität des Saarlan-
des (UdS) submission (named UdS-Sant) to WMT
2015 using a Hybrid MT framework. We partici-
pated in the generic translation shared task for the
English-German (EN-DE) language pair.

Corpus-based MT (CBMT) has delivered pro-
gressively improved quality translations since its
inception. There are two main approaches to
corpus-based MT – Example Based Machine
Translation (EBMT) (Carl and Way, 2003) and
Statistical Machine Translation (SMT) (Brown et
al., 1993; Koehn, 2010). Out of these two, in terms
of large-scale evaluations, SMT is the most suc-
cessful MT paradigm. However, each approach
has its own advantages and disadvantages along
with its own methods of applying and acquiring
translation knowledge from the bilingual parallel
training data. EBMT phrases tend to be more lin-
guistically motivated compared to SMT phrases
which essentially operate on n-grams. The knowl-
edge extraction as well as representation process,

in both EBMT and SMT, uses very different tech-
niques in order to extract resources. Even though,
SMT is the most popular MT paradigm, it some-
times fails to deliver sufficient quality in transla-
tion output for some languages, since each lan-
guage has its own difficulties.

Multiword Expressions (MWEs) and Named
Entities (NEs) offer challenges within a language.
MWEs are defined as idiosyncratic interpretations
that cross word boundaries (Sag et al., 2002).
Named entities on the other hand often consist of
more than one word, so that they can be consid-
ered as a specific type of MWEs such as noun
compounds (Jackendoff, 1997). Traditional ap-
proaches to word alignment such as IBM Mod-
els (Brown et al., 1993) are unable to tackle NEs
and MWEs properly due to their inability to han-
dle many-to-many alignments. In another well-
known word alignment approach, Hidden Markov
Model (HMM: (Vogel et al., 1996)), the alignment
probabilities depend on the alignment position of
the previous word. It does not explicitly consider
many-to-many alignment either.

We address this alignment problem indirectly.
The objective of the present work is threefold.
Firstly, we would like to determine how treat-
ment of MWEs as a single unit affects the over-
all MT quality (Pal et al., 2010; Pal et al., 2011).
Secondly, whether a prior automatic NE aligned
parallel corpus as well as example based parallel
phrases can bring about any further improvement
on top of that. And finally, whether system com-
bination can provide any additional advantage in
terms of translation quality and performance.

The remainder of the paper is organised as fol-
lows. Section 2 details the components of our sys-
tem, in particular named entity extraction, transla-
tion memory, and EBMT, followed by description
of 3 types of Hybrid systems and the system com-
bination module. In Section 3, we outline the com-
plete experimental setup for the shared task and

152



provide results and analysis on the performance on
the test set in Section 4. Section 5 concludes the
proposed research.

2 System Description

Our system is designed with three basic compo-
nents: (i) preprocessing, (ii) hybrid systems and
(iii) system combination.

2.1 Preprocessing
Data pre-processing plays a very crucial part in
any data-driven approach. We carried out prepro-
cessing in two steps:

• Cleaning and clustering sentences based on
sentence length.

• Effective preprocessing of data in the form of
explicit alignment of bilingual terminology
(viz. NEs and MWEs).

The preprocessing has been shown (cf. Section
2.1.2) to improve the output quality of the base-
line PB-SMT system (Pal et al., 2013; Tan and Pal,
2014).

2.1.1 Corpus cleaning
We utilized all the parallel training data provided
by the WMT 2015 shared task organizers for
English–German translation. The training data in-
clude Europarl, News Commentary and Common
Crawl. The provided corpus is noisy and con-
tains some non-German as well as non-English
words and sentences. Therefore, we applied a
Language Identifier (Shuyo, 2010) on both bilin-
gual English–German parallel data and monolin-
gual German corpora. We discarded those par-
allel sentences from the bilingual training data
which were detected as belonging to some differ-
ent language by the language identifier. The same
method was also applied to the monolingual data.

Successively, the corpus cleaning process was
carried out first by calculating the global mean ra-
tio of the number of characters in a source sen-
tence to that in a target sentence and then filter-
ing out sentence pairs that exceed or fall below
20% of the global ratio (Tan and Pal, 2014). We
sorted the entire parallel training corpus based on
their sentence length.Tokenisation and punctua-
tion normalisation were performed using Moses
scripts. In the final step of cleaning, we filtered
the parallel training data on maximum allowable
sentence length of 100 and sentence length ratio

of 1:2 (either direction). Approximately 36% sen-
tences were removed from the total training data
during the cleaning process.

2.1.2 Explicit Preprocessing of Terminologies

Two kinds of terminologies, viz. NEs and MWEs,
were considered in the present work. Intuitively,
MWEs should be both aligned in the parallel cor-
pus and translated as a whole. However, state-of-
the-art PB-SMT (or any other approaches to SMT)
does not generally treat MWEs as special tokens.
This is the motivation behind considering MWEs
for special treatment in this work. By converting
the MWEs into single tokens, we make sure that
PB-SMT also treats them as a whole.

NE Alignment (NEA): For NE alignment, we
first identify NEs on both sides of the parallel cor-
pus using Stanford NER1. Next, we try to align
the extracted source and target NEs. If both sides
contain only one NE then the alignment is triv-
ial, and we add such NE pairs to seed another
parallel NE corpus that contains examples having
only one token in both sides. Otherwise, we es-
tablish alignments between the source and target
NEs using minimum edit distance method. For
language pairs having different orthographies (e.g.
English–Hindi) NE alignments can be established
through transliteration (Pal et al., 2010). If both
the source and target sides contain n number of
NEs, and the alignments of n − 1 NEs can be es-
tablished through minimum edit distance method
or by means of already existing alignments, then
the nth alignment is trivial. The bilingual NE pairs
extracted thus serve as additional training material
and they improve the word alignment at the start
of the MT pipeline.

MWE Identification: Translation correspon-
dences between English MWEs and German
MWEs are mainly many-to-one correspondences.
Therefore, instead of extracting a bilingual MWE
list between source and target, we identify the
MWEs from the English training sentences and
prepare an English MWE list. Once the MWEs
are identified, they are converted into single tokens
by replacing the spaces with underscores (“ ”) so
that their alignments can be mapped to single to-
kens . Before decoding, MWEs in the source side
of the testset are also single tokenized by look-
ing up the extracted MWE list. In this experi-
ment, we have followed Point-wise Mutual Infor-

1http://nlp.stanford.edu/software/CRF-NER.shtml

153



mation (PMI), Log-likelihood Ratio (LLR), Phi-
coefficient and Co-occurrence measures for iden-
tification of MWEs on the English side. Finally,
a system combination model has been developed
which provides a normalized score for each of
the extracted MWEs. A predefined cut-off score
has been considered and the candidates having
scores above the threshold value are considered as
MWEs.

Example Based Phrase Extraction: We use
EBMT techniques to extract additional phrase
pairs from the training data to augment the SMT
(baseline) phrase pairs in our experiments. We ex-
tract EBMT phrase pairs based on the work de-
scribed in (Cicekli and Güvenir, 2001), a com-
piled approach of EBMT to automatically extract
translation templates from sentence-aligned bilin-
gual text. They observed the similarities and dif-
ferences between two example pairs. Two types of
translation templates, i.e. generalized and atomic
templates, are extracted by applying this approach.
A generalized translation template replaces sim-
ilar or differing sequences with variables while
an atomic translation template does not contain
any variable. The atomic translation templates are
used as additional phrase pairs for our Hybrid MT
system. This particular approach has a cubic run-
time complexity with respect to the number of sen-
tences in the parallel corpus. It takes a significant
amount of time to extract phrase pairs even from a
small corpus. Therefore we used heuristics to re-
duce the time complexity. We divided the entire
corpus into n clusters based on sentence length
such that similar length sentences belong to the
same cluster. We extract atomic translations from
each of these clusters. For this task, we applied
EBMT phrases as addition parallel training ex-
ample to explicitly enhanced the word alignment
model of the MT pipeline.

2.2 Hybrid System

The Hybrid approach is investigated by combining
multiple knowledge sources such as NEA, EBMT
Phrases and MWEs and followed different strate-
gies. As mentioned earlier, we implemented sev-
eral different systems, namely:

(1) Baseline PB-SMT,

(2) Baseline PB-SMT with NE alignment
(NEA),

(3) NEA with EBMT phrase extraction (NEA–
EBMT),

(4) NEA with EBMT phrase extraction and
single-tokenised MWE (NEA–EBMT–
MWE) and

(5) LM–NEA–EBMT–MWE hybrid system
(see Section 2.2.1).

The baseline SMT system is trained on the cleaned
English-German parallel corpus. The NEA sys-
tem makes use of NE aligned parallel data as addi-
tional parallel examples. Similarly, EBMT phrase
pairs as well as NE aligned data are also used as
additional training example in the NEA–EBMT
system. The NEA–EBMT–MWE system is very
similar to the above mentioned the NEA–EBMT
system, the only difference being that the identi-
fied source side English MWEs are converted into
single tokens for NEA–EBMT–MWE. In order to
achieve optimal performance from the component
modules, we finally generated a composite transla-
tion output using confusion network-based system
combination (cf. Section 2.3).

2.2.1 LM-NEA-EBMT-SMT hybrid system
In this system, we experiment with the above de-
scribed models with varying size of monolingual
data. We experimented with 4 folds of monolin-
gual data to train the language Models (LM):

• LM1: Only using the target side (i.e. Ger-
man) of the parallel training data (L) for lan-
guage modeling

• LM2: L + double size of L in terms of num-
ber of sentences, collected from the cleaned
monolingual corpus

• LM3: L + triple size of L from the cleaned
monolingual corpus

• LM4: L + all the cleaned monolingual data

Therefore, finally there were 16 different sys-
tems (4 systems, i.e., Baseline, NEA, NEA–
EBMT and NEA–EBMT–MWE, each with 4 LM
settings) output available for system combination.

2.2.2 Post-processing
As a final step, we try to generate translations of
out-of-vocabulary (OOV) words that remain un-
translated in the output. These OOV words may

154



include some NEs that are already there in the par-
allel NE list, however they might remain untrans-
lated during decoding. Our system post processed
the output by replacing each such OOV NE with
the corresponding target language NE after look-
ing up the extracted NE list from the parallel cor-
pus (cf. Section 2.1.2).

2.3 System Combination

System Combination is a technique, which com-
bines translation hypotheses (outputs) produced
by multiple MT systems. We applied a system
combination method on the outputs of the dif-
ferent MT system described earlier. We imple-
ment the Minimum Bayes Risk coupled with Con-
fusion Network (MBR-CN) framework described
in (Du et al., 2009). The MBR decoder (Kumar
and Byrne, 2004) selects the single best hypoth-
esis from amongst the multiple candidate transla-
tions by minimising BLEU (Papineni et al., 2002)
loss. This single best hypothesis serves as the
backbone (also referred to as skeleton) of the con-
fusion network and determines the general word
order of the confusion network. A confusion net-
work (Matusov et al., 2006) is built from the back-
bone while the remaining hypotheses are aligned
against the backbone using METEOR (Lavie and
Agarwal, 2007) and the TER metric (Snover et
al., 2006). The features used to score each arc
in the confusion network are word posterior prob-
ability, target language model (3-gram, 4-gram),
and length penalties. Minimum Error Rate Train-
ing (MERT) (Och, 2003) is applied to tune the CN
weights (Pal et al., 2014).

3 Experiment Setup

3.1 Baseline Settings

The effectiveness of the present work is demon-
strated by using the standard log-linear PB-SMT
model as our baseline system. For building the
baseline system, we used a maximum phrase
length of 7 and a 5-gram language model. The
other experimental settings were: SymGIZA++
aligner (Junczys-Dowmunt and Szał, 2012), which
is a modified version of GIZA++ word align-
ment models by updating the symmetrizing mod-
els between chosen iterations of the original
word alignment training algorithms and phrase-
extraction (Koehn et al., 2003). The reordering
model was trained on hier-mslr-bidirectional (i.e.
using both forward and backward models) and

conditioned on both source and target language.
The reordering model was built by calculating
the probabilities of the phrase pairs being asso-
ciated with the given orientation such as mono-
tone (m), swap (s) and discontinuous (d). The
5-gram target language model was trained using
KENLM (Heafield, 2011). Parameter tuning was
carried out using both k-best MIRA (Cherry and
Foster, 2012) and Minimum Error Rate Training
(MERT) (Och, 2003) on a held-out development
set. After the parameters were tuned, decoding
was carried out on the held out testset.

Note that all the systems described in Section 2
employ the same PB-SMT settings (apart from the
feature weights which are obtained via MERT) as
the Baseline system.

4 Results and Analysis

As described in Section 2.2.1, we developed 16
different systems. Instead of using all these 16 dif-
ferent systems, we apply only the 6 best perform-
ing systems for system combination. Performance
is measured on the devset. Table 1 reports the final
evaluation results obtained on the test dataset. The
best 6 systems are as follows:

• System 1: NEA–EBMT (selective high fre-
quency phrases) with baseline PB-SMT set-
tings and LM1.

• System 2: System 1 experimental settings +
single tokenised source MWEs (i.e. NEA–
EBMT–MWE, cf. Section 2.2).

• System 3: System 2 with MIRA-MERT cou-
pled tuning.

• System 4: System 3 with LM2.
• System 5: System 3 with LM3.
• System 6: System 3 with LM4.

System 6 provides the individual best system. Sys-
tem combination (System-7 in Table 1) of the 6
best performing individual systems brings consid-
erable improvements over each of the individual
component systems.

5 Conclusions and Future Work

A hybrid system (System 6) with NE alignment,
EBMT phrases, single-tokenized source MWEs,
and MIRA-MERT coupled tuning results in the
best performing system. However, confusion

155



Systems BLEU BLEU(Cased) TER
Baseline 16.7 16.2 89.6
System 1 18.1 17.5 88.2
System 2 18.1 17.6 87.8
System 3 19.0 18.4 85.3
System 4 20.0 19.5 84.1
System 5 20.3 19.7 83.8
System 6 20.7 20.2 83.5
System 7 22.6 22.1 82.3

Table 1: Results.

network-based system combination outperforms
all the individual MT systems. The fact that the
systems were tuned with BLEU scores may be one
of the reasons behind the poor TER scores pro-
duced by the systems. In future, we will carry
out in depth investigation of the impacts of MWEs
within the current experimental settings. We will
also analyze the usability and contribution of the
novel EBMT phrases in the SMT decoder.

Acknowledgments

The research leading to these results has received
funding from the EU FP7 Project EXPERT - the
People Programme (Marie Curie Actions) (Grant
No. 317471)

References
Peter F Brown, Vincent J Della Pietra, Stephen A Della

Pietra, and Robert L Mercer. 1993. The Mathe-
matics of Statistical Machine Translation: Parameter
Estimation. Computational linguistics, 19(2):263–
311.

Michael Carl and Andy Way. 2003. Recent advances
in example-based machine translation, volume 21.
Springer Science & Business Media.

Colin Cherry and George Foster. 2012. Batch Tun-
ing Strategies for Statistical Machine Translation. In
Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 427–436.

Ilyas Cicekli and H Altay Güvenir. 2001. Learning
Translation Templates From Bilingual Translation
Examples. Applied Intelligence, 15(1):57–76.

Jinhua Du, Yifan He, Sergio Penkale, and Andy Way.
2009. MATREX: The DCU MT System for WMT
2009. In Proceedings of the 4th Workshop on Statis-
tical Machine Translation, pages 95–99, March.

Kenneth Heafield. 2011. KenLM: Faster and Smaller
Language Model Queries. In Proceedings of the
Sixth Workshop on Statistical Machine Translation,
pages 187–197.

Ray Jackendoff. 1997. The architecture of the lan-
guage faculty. Number 28. MIT Press.

Marcin Junczys-Dowmunt and Arkadiusz Szał. 2012.
SyMGiza++: Symmetrized Word Alignment Mod-
els for Statistical Machine Translation. In Proceed-
ings of the 2011 International Conference on Secu-
rity and Intelligent Information Systems, pages 379–
390.

Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-based Translation. In Pro-
ceedings of the 2003 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics on Human Language Technology, pages
48–54.

Philipp Koehn. 2010. Statistical Machine Translation.
Cambridge University Press, New York, NY, USA,
1st edition.

Shankar Kumar and William Byrne. 2004. Minimum
Bayes Risk Decoding for Statistical Machine Trans-
lation. In Proceedings of the North American As-
sociation for Computational Linguistics (NAACL),
pages 169–176, March.

Alon Lavie and Abhaya Agarwal. 2007. METEOR:
An Automatic Metric for MT Evaluation with High
Levels of Correlation with Human Judgments. In
Proceedings of the Second Workshop on Statistical
Machine Translation, pages 228–231.

Evgeny Matusov, Nicola Ueffing, and Hermann Ney.
2006. Computing Consensus Translation from Mul-
tiple Machine Translation Systems Using Enhanced
Hypotheses Alignment. In Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics, pages 33–40, Trento, Italy, April.

Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proceedings
of the 41st Annual Meeting on Association for Com-
putational Linguistics - Volume 1, pages 160–167.

Santanu Pal, Sudip Kumar Naskar, Pavel Pecina, Sivaji
Bandyopadhyay, and Andy Way. 2010. Handling
Named Entities and Compound Verbs in Phrase-
Based Statistical Machine Translation. In In Pro-
ceedings of the of Multiword Expression Workshop
(MWE-2010). The 23rd International conference of
computational linguistics (Coling 2010).

Santanu Pal, Tanmoy Chakraborty, and Sivaji Bandy-
opadhyay. 2011. Handling Multiword Expres-
sions in Phrase-Based Statistical Machine Transla-
tion. Machine Translation Summit XIII, pages 215–
224.

156



Santanu Pal, Sudip Kumar Naskar, and Sivaji Bandy-
opadhyay. 2013. MWE Alignment in Phrase Based
Statistical Machine Translation. The XIV Machine
Translation Summit, pages 61–68.

Santanu Pal, Ankit Srivastava, Sandipan Dandapat,
Josef van Genabith, Qun Liu, and Andy Way. 2014.
USAAR-DCU Hybrid Machine Translation System
for ICON 2014. In Proceedings of the 11th Interna-
tional Conference on Natural Language Processing,
Goa, India.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A Method for Automatic
Evaluation of Machine Translation. In Proceedings
of the 40th Annual Meeting on Association for Com-
putational Linguistics, ACL ’02, pages 311–318.

Ivan A Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multiword
Expressions: A Pain in the Neck for NLP. In Com-
putational Linguistics and Intelligent Text Process-
ing, pages 1–15. Springer.

Nakatani Shuyo. 2010. Language Detection Library
for Java.

Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study
of Translation Edit Rate with Targeted Human An-
notation. In Proceedings of association for machine
translation in the Americas, pages 223–231.

Liling Tan and Santanu Pal. 2014. Manawi: Using
Multi-word Expressions and Named Entities to Im-
prove Machine Translation. In Proceedings of Ninth
Workshop on Statistical Machine Translation.

Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical
translation. In Proceedings of the 16th conference
on Computational linguistics-Volume 2, pages 836–
841. Association for Computational Linguistics.

157


