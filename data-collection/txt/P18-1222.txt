
























































hyperdoc2vec.pdf


Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 2384–2394
Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics

2384

hyperdoc2vec: Distributed Representations of Hypertext Documents

Jialong Han♠, Yan Song♠, Wayne Xin Zhao�, Shuming Shi♠, Haisong Zhang♠
♠Tencent AI Lab

�School of Information, Renmin University of China
{jialonghan,batmanfly}@gmail.com,{clksong,shumingshi,hansonzhang}@tencent.com

Abstract

Hypertext documents, such as web pages
and academic papers, are of great impor-
tance in delivering information in our daily
life. Although being effective on plain
documents, conventional text embedding
methods suffer from information loss if di-
rectly adapted to hyper-documents. In this
paper, we propose a general embedding
approach for hyper-documents, namely,
hyperdoc2vec, along with four crite-
ria characterizing necessary information
that hyper-document embedding models
should preserve. Systematic comparisons
are conducted between hyperdoc2vec
and several competitors on two tasks, i.e.,
paper classification and citation recom-
mendation, in the academic paper do-
main. Analyses and experiments both val-
idate the superiority of hyperdoc2vec
to other models w.r.t. the four criteria.

1 Introduction

The ubiquitous World Wide Web has boosted re-
search interests on hypertext documents, e.g., per-
sonal webpages (Lu and Getoor, 2003), Wikipedia
pages (Gabrilovich and Markovitch, 2007), as well
as academic papers (Sugiyama and Kan, 2010).
Unlike independent plain documents, a hypertext
document (hyper-doc for short) links to another
hyper-doc by a hyperlink or citation mark in its
textual content. Given this essential distinction,
hyperlinks or citations are worth specific model-
ing in many tasks such as link-based classifica-
tion (Lu and Getoor, 2003), web retrieval (Page
et al., 1999), entity linking (Cucerzan, 2007), and
citation recommendation (He et al., 2010).

To model hypertext documents, various ef-
forts (Cohn and Hofmann, 2000; Kataria et al.,

2010; Perozzi et al., 2014; Zwicklbauer et al.,
2016; Wang et al., 2016) have been made to de-
pict networks of hyper-docs as well as their con-
tent. Among potential techniques, distributed rep-
resentation (Mikolov et al., 2013; Le and Mikolov,
2014) tends to be promising since its validity and
effectiveness are proven for plain documents on
many natural language processing (NLP) tasks.

Conventional attempts on utilizing embedding
techniques in hyper-doc-related tasks generally
fall into two types. The first type (Berger et al.,
2017; Zwicklbauer et al., 2016) simply downcasts
hyper-docs to plain documents and feeds them into
word2vec (Mikolov et al., 2013) (w2v for short)
or doc2vec (Le and Mikolov, 2014) (d2v for
short). These approaches involve downgrading
hyperlinks and inevitably omit certain information
in hyper-docs. However, no previous work inves-
tigates the information loss, and how it affects the
performance of such downcasting-based adapta-
tions. The second type designs sophisticated em-
bedding models to fulfill certain tasks, e.g., cita-
tion recommendation (Huang et al., 2015b), pa-
per classification (Wang et al., 2016), and entity
linking (Yamada et al., 2016), etc. These models
are limited to specific tasks, and it is yet unknown
whether embeddings learned for those particular
tasks can generalize to others. Based on the above
facts, we are interested in two questions:

• What information should hyper-doc embed-
ding models preserve, and what nice property
should they possess?

• Is there a general approach to learning task-
independent embeddings of hyper-docs?

To answer the two questions, we formalize the
hyper-doc embedding task, and propose four cri-
teria, i.e., content awareness, context awareness,
newcomer friendliness, and context intent aware-



2385

ness, to assess different models. Then we discuss
simple downcasting-based adaptations of existing
approaches w.r.t. the above criteria, and demon-
strate that none of them satisfy all four. To this
end, we propose hyperdoc2vec (h-d2v for
short), a general embedding approach for hyper-
docs. Different from most existing approaches,
h-d2v learns two vectors for each hyper-doc to
characterize its roles of citing others and being
cited. Owning to this, h-d2v is able to directly
model hyperlinks or citations without downgrad-
ing them. To evaluate the learned embeddings, we
employ two tasks in the academic paper domain1,
i.e., paper classification and citation recommenda-
tion. Experimental results demonstrate the supe-
riority of h-d2v. Comparative studies and con-
trolled experiments also confirm that h-d2v ben-
efits from satisfying the above four criteria.

We summarize our contributions as follows:

• We propose four criteria to assess different
hyper-document embedding models.

• We propose hyperdoc2vec, a general em-
bedding approach for hyper-documents.

• We systematically conduct comparisons with
competing approaches, validating the superi-
ority of h-d2v in terms of the four criteria.

2 Related Work

Network representation learning is a related
topic to ours since a collection of hyper-docs re-
semble a network. To embed nodes in a network,
Perozzi et al. (2014) propose DeepWalk, where
nodes and random walks are treated as pseudo
words and texts, and fed to w2v for node vectors.
Tang et al. (2015b) explicitly embed second-order
proximity via the number of common neighbors of
nodes. Grover and Leskovec (2016) extend Deep-
Walk with second-order Markovian walks. To im-
prove classification tasks, Tu et al. (2016) explore
a semi-supervised setting that accesses partial la-
bels. Compared with these models, h-d2v learns
from both documents’ connections and contents
while they mainly focus on network structures.

Document embedding for classification is an-
other focused area to apply document embeddings.

1Although limited in tasks and domains, we expect that
our embedding approach can be potentially generalized to, or
serve as basis to more sophisticated methods for, similar tasks
in the entity domain, e.g., Wikipedia page classification and
entity linking. We leave them for future work.

Le and Mikolov (2014) employ learned d2v vec-
tors to build different text classifiers. Tang et al.
(2015a) apply the method in (Tang et al., 2015b)
on word co-occurrence graphs for word embed-
dings, and average them for document vectors. For
hyper-docs, Ganguly and Pudi (2017) and Wang
et al. (2016) target paper classification in unsuper-
vised and semi-supervised settings, respectively.
However, unlike h-d2v, they do not explicitly
model citation contexts. Yang et al. (2015)’s ap-
proach also addresses embedding hyper-docs, but
involves matrix factorization and does not scale.

Citation recommendation is a direct downstream
task to evaluate embeddings learned for a cer-
tain kind of hyper-docs, i.e., academic papers. In
this paper we concentrate on context-aware cita-
tion recommendation (He et al., 2010). Some pre-
vious studies adopt neural models for this task.
Huang et al. (2015b) propose Neural Probabilistic
Model (NPM) to tackle this problem with embed-
dings. Their model outperforms non-embedding
ones (Kataria et al., 2010; Tang and Zhang, 2009;
Huang et al., 2012). Ebesu and Fang (2017) also
exploit neural networks for citation recommenda-
tion, but require author information as additional
input. Compared with h-d2v, these models are
limited in a task-specific setting.

Embedding-based entity linking is another topic
that exploits embeddings to model certain hyper-
docs, i.e., Wikipedia (Huang et al., 2015a; Yamada
et al., 2016; Sun et al., 2015; Fang et al., 2016; He
et al., 2013; Zwicklbauer et al., 2016), for entity
linking (Shen et al., 2015). It resembles citation
recommendation in the sense that linked entities
highly depend on the contexts. Meanwhile, it re-
quires extra steps like candidate generation, and
can benefit from sophisticated techniques such as
collective linking (Cucerzan, 2007).

3 Preliminaries

We introduce notations and definitions, then for-
mally define the embedding problem. We also pro-
pose four criteria for hyper-doc embedding models
w.r.t their appropriateness and informativeness.

3.1 Notations and Definitions
Let w ∈ W be a word from a vocabulary W , and
d ∈ D be a document id (e.g., web page URLs and
paper DOIs) from an id collection D. After filter-
ing out non-textual content, a hyper-document H
is reorganized as a sequence of words and doc ids,



2386

(Koehn et al., 2007)

(Zhao and Gildea, 2010)

(Papineni et al., 2002)

Original

Source doc 

Context
words 

Target
doc 

…
We also evaluate our model
by computing the machine 
translation BLEU score (Papineni
et al., 2002) using the Moses 
system (Koehn et al., 2007)

…

…
…

(a) Hyper-documents.

Citation as word

BLEU

evaluate

(Papineni
et al., 2012)

“Word” Vectors

…

w2v

…We also evaluate our model
by computing the machine 
translation BLEU score (Papineni
et al., 2002) using the Moses 
system (Koehn et al., 2007)…

… …

(b) Citation as word.

Context as content

BLEU

evaluate

(Zhao and 
Gildea, 2010)

…

…

Word Vectors

Doc Vectors

(Papineni
et al., 2002)

d2v

(Koehn et al., 2007)

(Zhao and Gildea, 2010)

(Papineni et al., 2002)

…We also evaluate our model
by computing the machine 
translation BLEU score using the 
Moses system …

… machine 
translation 

BLEU score … … Moses system …

(c) Context as content.

Figure 1: An example of Zhao and Gildea (2010) citing Papineni et al. (2002) and existing approaches.

i.e., W∪D. For example, web pages could be sim-
plified as streams of words and URLs, and papers
are actually sequences of words and cited DOIs.

If a document id dt with some surrounding
words C appear in the hyper-doc of ds, i.e., Hds ,
we stipulate that a hyper-link 〈ds, C, dt〉 is formed.
Herein ds, dt ∈ D are ids of the source and tar-
get documents, respectively; C ⊆ W are context
words. Figure 1(a) exemplifies a hyperlink.

3.2 Problem Statement
Given a corpus of hyper-docs {Hd}d∈D with D
and W , we want to learn document and word em-
bedding matrices D ∈ Rk×|D| and W ∈ Rk×|W |
simultaneously. The i-th column di of D is a k-
dimensional embedding vector for the i-th hyper-
doc with id di. Similarly, wj , the j-th column
of W, is the vector for word wj . Once embed-
dings for hyper-docs and words are learned, they
can facilitate applications like hyper-doc classifi-
cation and citation recommendation.

3.3 Criteria for Embedding Models
A reasonable model should learn how contents and
hyperlinks in hyper-docs impact both D and W.
We propose the following criteria for models:

• Content aware. Content words of a hyper-
doc play the main role in describing it, so
the document representation should depend
on its own content. For example, the words
in Zhao and Gildea (2010) should affect and
contribute to its embedding.

• Context aware. Hyperlink contexts usu-
ally provide a summary for the target docu-
ment. Therefore, the target document’s vec-
tor should be impacted by words that others
use to summarize it, e.g., paper Papineni et al.
(2002) and the word “BLEU” in Figure 1(a).

• Newcomer friendly. In a hyper-document
network, it is inevitable that some documents

are not referred to by any hyperlink in other
hyper-docs. If such “newcomers” do not
get embedded properly, downstream tasks in-
volving them are infeasible or deteriorated.

• Context intent aware. Words around a hy-
perlink, e.g., “evaluate . . . by” in Figure 1(a),
normally indicate why the source hyper-doc
makes the reference, e.g., for general refer-
ence or to follow/oppose the target hyper-
doc’s opinion or practice. Vectors of those
context words should be influenced by both
documents to characterize such semantics or
intents between the two documents.

We note that the first three criteria are for hyper-
docs, while the last one is desired for word vectors.

4 Representing Hypertext Documents

In this section, we first give the background of two
prevailing techniques, word2vec and doc2vec.
Then we present two conversion approaches for
hyper-documents so that w2v and d2v can be ap-
plied. Finally, we address their weaknesses w.r.t.
the aforementioned four criteria, and propose our
hyperdoc2vec model. In the remainder of this
paper, when the context is clear, we mix the use of
terms hyper-doc/hyperlink with paper/citation.

4.1 word2vec and doc2vec
w2v (Mikolov et al., 2013) has proven effective
for many NLP tasks. It integrates two models, i.e.,
cbow and skip-gram, both of which learn two
types of word vectors, i.e., IN and OUT vectors.
cbow sums up IN vectors of context words and
make it predictive of the current word’s OUT vec-
tor. skip-gram uses the IN vector of the current
word to predict its context words’ OUT vectors.

As a straightforward extension to w2v, d2v
also has two variants: pv-dm and pv-dbow.
pv-dm works in a similar manner as cbow, ex-
cept that the IN vector of the current document



2387

Desired Property
Impacts Task? Addressed by Approach?

Classification Citation Rec-ommendation w2v d2v-nc d2v-cac h-d2v

Context aware � � � × � �
Content aware � � × � � �
Newcomer friendly � � × � � �
Context intent aware × � × × × �

Table 1: Analysis of tasks and approaches w.r.t. desired properties.

Model Output

DI DO WI WO

w2v � � � �
d2v (pv-dm) � × � �
d2v (pv-dbow) � × × �
h-d2v � � � �

Table 2: Output of models.

is regarded as a special context vector to average.
Analogously, pv-dbow uses IN document vec-
tor to predict its words’ OUT vectors, following
the same structure of skip-gram. Therefore in
pv-dbow, words’ IN vectors are omitted.

4.2 Adaptation of Existing Approaches
To represent hyper-docs, a straightforward strat-
egy is to convert them into plain documents in a
certain way and apply w2v and d2v. Two conver-
sions following this strategy are illustrated below.

Citation as word. This approach is adopted by
Berger et al. (2017).2 As Figure 1(b) shows, doc-
ument ids D are treated as a collection of spe-
cial words. Each citation is regarded as an oc-
currence of the target document’s special word.
After applying standard word embedding meth-
ods, e.g., w2v, we obtain embeddings for both
ordinary words and special “words”, i.e., docu-
ments. In doing so, this approach allows target
documents interacting with context words, thus
produces context-aware embeddings for them.

Context as content. It is often observed in aca-
demic papers when citing others’ work, an author
briefly summarizes the cited paper in its citation
context. Inspired by this, we propose a context-
as-content approach as in Figure 1(c). To start, we
remove all citations. Then all citation contexts of a
target document dt are copied into dt as additional
contents to make up for the lost information. Fi-
nally, d2v is applied to the augmented documents
to generate document embeddings. With this ap-
proach, the generated document embeddings are
both context- and content-aware.

4.3 hyperdoc2vec
Besides citation-as-word with w2v and context-
as-content with d2v (denoted by d2v-cac for
short), there is also an alternative using d2v on
documents with citations removed (d2v-nc for

2It is designed for document visualization purposes.

short). We made a comparison of these approaches
in Table 1 in terms of the four criteria stated in Sec-
tion 3.3. It is observed that none of them satisfy all
criteria, where the reasons are as follows.

First, w2v is not content aware. Following our
examples in the academic paper domain, consider
the paper (hyper-doc) Zhao and Gildea (2010)
in Figure 1(a), from w2v’s perspective in Fig-
ure 1(b), “. . . computing the machine translation
BLEU . . . ” and other text no longer have as-
sociation with Zhao and Gildea (2010), thus not
contributing to its embedding. In addition, for
papers being just published and having not ob-
tained citations yet, they will not appear as special
“words” in any text. This makes w2v newcomer-
unfriendly, i.e., unable to produce embeddings for
them. Second, being trained on a corpus without
citations, d2v-nc is obviously not context aware.
Finally, in both w2v and d2v-cac, context words
interact with the target documents without treat-
ing the source documents as backgrounds, which
forces IN vectors of words with context intents,
e.g., “evaluate” and “by” in Figure 1(a), to simply
remember the target documents, rather than cap-
ture the semantics of the citations.

The above limitations are caused by the conver-
sions of hyper-docs where certain information in
citations is lost. For a citation 〈ds, C, dt〉, citation-
as-word only keeps the co-occurrence information
between C and dt. Context-as-content, on the
other hand, mixes C with the original content of
dt. Both approaches implicitly downgrade cita-
tions 〈ds, C, dt〉 to 〈C, dt〉 for adaptation purposes.

To learn hyper-doc embeddings without such
limitations, we propose hyperdoc2vec. In this
model, two vectors of a hyper-doc d, i.e., IN and
OUT vectors, are adopted to represent the docu-
ment of its two roles. The IN vector dI character-
izes d being a source document. The OUT vector
dO encodes its role as a target document. We note
that learning those two types of vectors is advan-
tageous. It enables us to model citations and con-



2388

(Zhao and
Gildea, 2010)

DI

also evaluate

…

(Papineni et al., 2002)Classifier

Average

Document
Matrix

BLEU

WI WI WI

DO

(Zhao and Gildea, 2010) “… also evaluate … BLEU …” (Papineni et al., 2002)
A citation

Figure 2: The hyperdoc2vec model.

tents simultaneously without sacrificing informa-
tion on either side. Next, we describe the details
of h-d2v in modeling citations and contents.

To model citations, we adopt the architecture in
Figure 2. It is similar to pv-dm, except that docu-
ments rather than words are predicted at the output
layer. For a citation 〈ds, C, dt〉, to allow context
words C interacting with both vectors, we average
dIs of ds with word vectors of C, and make the re-
sulted vector predictive of dOt of dt. Formally, for
all citations C = {〈ds, C, dt〉}, we aim to optimize
the following average log probability objective:

max
DI ,DO,WI

1

|C|
∑

〈ds,C,dt〉∈C
logP (dt|ds, C) (1)

To model the probability P (dt|ds, C) where dt is
cited in ds with C, we average their IN vectors

x =
1

1 + |C|
(

dIs +
∑
w∈C

wI
)

(2)

and use x to compose a multi-class softmax clas-
sifier on all OUT document vectors

P (dt|ds, C) = exp(x
�dOt )∑

d∈D exp(x�d
O)

(3)

To model contents’ impact on document vec-
tors, we simply consider an additional objective
function that is identical to pv-dm, i.e., enumer-
ate words and contexts, and use the same input ar-
chitecture as Figure 2 to predict the OUT vector
of the current word. Such convenience owes to the
fact that using two vectors makes the model pa-
rameters compatible with those of pv-dm. Note
that combining the citation and content objectives
leads to a joint learning framework. To facilitate
easier and faster training, we adopt an alterna-
tive pre-training/fine-tuning or retrofitting frame-
work (Faruqui et al., 2015). We initialize with a
predefined number of pv-dm iterations, and then
optimize Eq. 1 based on the initialization.

Dataset Docs Citations Years

NIPS
Train 1,590 512 Up to 1998
Test 150 89 1999
Total 1,740 601 Up to 1999

ACL
Train 18,845 91,792 Up to 2012
Test 1,563 16,937 2013
Total 20,408 108,729 Up to 2013

DBLP
Train 593,378 2,565,625 Up to 2009
Test 55,736 308,678 From 2010
Total 649,114 2,874,303 All years

Table 3: The statistics of three datasets.

Finally, similar to w2v (Mikolov et al., 2013)
and d2v (Le and Mikolov, 2014), to make training
efficient, we adopt negative sampling:

log σ(x�dOt ) +
n∑

i=1

Edi∼PN (d) log σ(−x�dOi )
(4)

and use it to replace every logP (dt|ds, C). Fol-
lowing Huang et al. (2015b), we adopt a uniform
distribution on D as the distribution PN (d).

Unlike the other models in Table 1, h-d2v sat-
isfies all four criteria. We refer to the example in
Figure 2 to make the points clear. First, when op-
timizing Eq. 1 with the instance in Figure 2, the
update to dO of Papineni et al. (2002) depends
on wI of context words such as “BLEU”. Sec-
ond, we pre-train dI with contents, which makes
the document embeddings content aware. Third,
newcomers can depend on their contents for dI ,
and update their OUT vectors when they are sam-
pled3 in Eq. 4. Finally, the optimization of Eq. 1
enables mutual enhancement between vectors of
hyper-docs and context intent words, e.g., “evalu-
ate by”. Under the background of a machine trans-
lation paper Zhao and Gildea (2010), the above
two words help point the citation to the BLEU pa-
per (Papineni et al., 2002), thus updating its OUT
vector. The intent “adopting tools/algorithms” of
“evaluate by” is also better captured by iterating
over many document pairs with them in between.

5 Experiments

In this section, we first introduce datasets and ba-
sic settings used to learn embeddings. We then
discuss additional settings and present experimen-
tal results of the two tasks, i.e., document classifi-
cation and citation recommendation, respectively.

3Given a relatively large n.



2389

Model Original w/ DeepWalkMacro Micro Macro Micro

DeepWalk 61.67 69.89 61.67 69.89
w2v (I) 10.83 41.84 31.06 50.93
w2v (I+O) 9.36 41.26 25.92 49.56
d2v-nc 70.62 77.86 70.64 78.06
d2v-cac 71.83 78.09 71.57 78.59

h-d2v (I) 68.81 76.33 73.96 79.93
h-d2v (I+O) 72.89 78.99 73.24 79.55

Table 4: F1 scores on DBLP.

Model Content Aware/ Original w/ DeepWalkNewcomer Friendly Macro Micro Macro Micro

DeepWalk - 66.57 76.56 66.57 76.56

w2v (I) × / × 19.77 47.32 59.80 72.90
w2v (I+O) × / × 15.97 45.66 50.77 70.08
d2v-nc �/ � 61.54 73.73 69.37 78.22
d2v-cac �/ � 65.23 75.93 70.43 78.75
h-d2v (I) �/ � 58.59 69.79 66.99 75.63
h-d2v (I+O) �/ � 66.64 75.19 68.96 76.61

Table 5: F1 on DBLP when newcomers are discarded.

5.1 Datasets and Experimental Settings

We use three datasets from the academic paper do-
main, i.e., NIPS4, ACL anthology5 and DBLP6,
as shown in Table 3. They all contain full text of
papers, and are of small, medium, and large size,
respectively. We apply ParsCit7 (Councill et al.,
2008) to parse the citations and bibliography sec-
tions. Each identified citation string referring to a
paper in the same dataset, e.g., [1] or (Author et al.,
2018), is replaced by a global paper id. Consecu-
tive citations like [1, 2] are regarded as multiple
ground truths occupying one position. Following
He et al. (2010), we take 50 words before and after
a citation as the citation context.

Gensim (Řehůřek and Sojka, 2010) is used to
implement all w2v and d2v baselines as well as
h-d2v. We use cbow for w2v and pv-dbow for
d2v, unless otherwise noted. For all three base-
lines, we set the (half) context window length to
50. For w2v, d2v, and the pv-dm-based ini-
tialization of h-d2v, we run 5 epochs following
Gensim’s default setting. For h-d2v, its iteration
is set to 100 epochs with 1000 negative samples.
The dimension size k of all approaches is 100. All
other parameters in Gensim are kept as default.

5.2 Document Classification

In this task, we classify the research fields of pa-
pers given their vectors learned on DBLP. To ob-
tain labels, we use Cora8, a small dataset of Com-
puter Science papers and their field categories.
We keep the first levels of the original categories,

4https://cs.nyu.edu/ roweis/data.html
5http://clair.eecs.umich.edu/aan/index.php (2013 release)
6http://zhou142.myweb.cs.uwindsor.ca/academicpaper.html

This page has been unavailable recently. They provide a
larger CiteSeer dataset and a collection of DBLP paper
ids. To better interpret results from the Computer Science
perspective, we intersect them and obtain the DBLP dataset.

7https://github.com/knmnyn/ParsCit
8http://people.cs.umass.edu/˜mccallum/data.html

e.g., “Artificial Intelligence” of “Artificial Intelli-
gence - Natural Language Processing”, leading to
10 unique classes. We then intersect the dataset
with DBLP, and obtain 5,975 labeled papers.

For w2v and h-d2v outputing both IN and
OUT document vectors, we use IN vectors or con-
catenations of both vectors as features. For new-
comer papers without w2v vectors, we use zero
vectors instead. To enrich the features with net-
work structure information, we also try concate-
nating them with the output of DeepWalk (Perozzi
et al., 2014), a representative network embedding
model. The model is trained on the citation net-
work of DBLP with an existing implementation9

and default parameters. An SVM classifier with
RBF kernel is used. We perform 5-fold cross vali-
dation, and report Macro- and Micro-F1 scores.

5.2.1 Classification Performance
In Table 4, we demonstrate the classification re-
sults. We have the following observations.

First, adding DeepWalk information almost al-
ways leads to better classification performance,
except for Macro-F1 of the d2v-cac approach.

Second, owning to different context awareness,
d2v-cac consistently outperforms d2v-nc in
terms of all metrics and settings.

Third, w2v has the worst performance. The rea-
son may be that w2v is neither content aware nor
newcomer friendly. We will elaborate more on the
impacts of the two properties in Section 5.2.2.

Finally, no matter whether DeepWalk vectors
are used, h-d2v achieves the best F1 scores.
However, when OUT vectors are involved, h-d2v
with DeepWalk has slightly worse performance.
A possible explanation is that, when h-d2v IN
and DeepWalk vectors have enough information to
train the SVM classifiers, adding another 100 fea-
tures (OUT vectors) only increase the parameter

9https://github.com/phanein/deepwalk



2390

Model NIPS ACL Anthology DBLPRec MAP MRR nDCG Rec MAP MRR nDCG Rec MAP MRR nDCG

w2v (cbow, I4I) 5.06 1.29 1.29 2.07 12.28 5.35 5.35 6.96 3.01 1.00 1.00 1.44
w2v (cbow, I4O) 12.92 6.97 6.97 8.34 15.68 8.54 8.55 10.23 13.26 7.29 7.33 8.58
d2v-nc (pv-dbow, cosine) 14.04 3.39 3.39 5.82 21.09 9.65 9.67 12.29 7.66 3.25 3.25 4.23
d2v-cac (same as d2v-nc) 14.61 4.94 4.94 7.14 28.01 11.82 11.84 15.59 15.67 7.34 7.36 9.16
NPM (Huang et al., 2015b) 7.87 2.73 3.13 4.03 12.86 5.98 5.98 7.59 6.87 3.28 3.28 4.07

h-d2v (random init, I4O) 3.93 0.78 0.78 1.49 30.98 16.76 16.77 20.12 17.22 8.82 8.87 10.65
h-d2v (pv-dm retrofitting, I4O) 15.73 6.68 6.68 8.80 31.93 17.33 17.34 20.76 21.32 10.83 10.88 13.14

Table 6: Top-10 citation recommendation results (dimension size k = 100).

space of the classifiers and the training variance.
For w2v with or without DeepWalk, it is also the
case. This may be because information in w2v’s
IN and OUT vectors is fairly redundant.

5.2.2 Impacts of Content Awareness and
Newcomer Friendliness

Because content awareness and newcomer friend-
liness are highly correlated in Table 1, to isolate
and study their impacts, we decouple them as fol-
lows. In the 5,975 labeled papers, we keep 2,052
with at least one citation, and redo experiments in
Table 4. By carrying out such controlled exper-
iments, we expect to remove the impact of new-
comers, and compare all approaches only with re-
spect to different content awareness. In Table 5,
we provide the new scores obtained.

By comparing Tables 4 and 5, we observe that
w2v benefits from removing newcomers with zero
vectors, while all newcomer friendly approaches
get lower scores because of fewer training exam-
ples. Even though the change, w2v still cannot
outperform the other approaches, which reflects
the positive impact of content awareness on the
classification task. It is also interesting that Deep-
Walk becomes very competitive. This implies that
structure-based methods favor networks with bet-
ter connectivity. Finally, we note that Table 5 is
based on controlled experiments with intentionally
skewed data. The results are not intended for com-
parison among approaches in practical scenarios.

5.3 Citation Recommendation

When writing papers, it is desirable to recommend
proper citations for a given context. This could be
achieved by comparing the vectors of the context
and previous papers. We use all three datasets for
this task. Embeddings are trained on papers before
1998, 2012, and 2009, respectively. The remain-
ing papers in each dataset are used for testing.

We compare h-d2vwith all approaches in Sec-

tion 4.2, as well as NPM10 (Huang et al., 2015b)
mentioned in Section 2, the first embedding-based
approach for the citation recommendation task.
Note that the inference stage involves interactions
between word and document vectors and is non-
trivial. We describe our choices as below.

First, for w2v vectors, Nalisnick et al. (2016)
suggest that the IN-IN similarity favors word pairs
with similar functions (e.g., “red” and “blue”),
while the IN-OUT similarity characterizes word
co-occurrence or compatibility (e.g., “red” and
“bull”). For citation recommendation that relies on
the compatibility between context words and cited
papers, we hypothesize that the IN-for-OUT (or
I4O for short) approach will achieve better results.
Therefore, for w2v-based approaches, we average
IN vectors of context words, then score and and
rank OUT document vectors by dot product.

Second, for d2v-based approaches, we use the
learned model to infer a document vector d for
the context words, and use d to rank IN document
vectors by cosine similarity. Among multiple at-
tempts, we find this choice to be optimal.

Third, for h-d2v, we adopt the same scoring
and ranking configurations as for w2v.

Finally, for NPM, we adopt the same ranking
strategy as in Huang et al. (2015b). Following
them, we focus on top-10 results and report the
Recall, MAP, MRR, and nDCG scores.

5.3.1 Recommendation Performance
In Table 6, we report the citation recommendation
results. Our observations are as follows.

First, among all datasets, all methods perform
relatively well on the medium-sized ACL dataset.
This is because the smallest NIPS dataset provides

10Note that the authors used n = 1000 for negative sam-
pling, and did not report the number of training epoches. Af-
ter many trials, we find that setting the number of both the
negative samples and epoches at 100 to be relatively effective
and affordable w.r.t. training time.



2391

100 200 300 400 500
Dimension

5

10

15

20

25

30

R
ec

@
10

 (
%

)
w2v (I4O)
d2v-nc
d2v-cac
NPM
h-d2v

Figure 3: Varying k on DBLP. The scores of w2v
keeps increasing to 26.63 at k = 1000, and then
begins to drop. Although at the cost of a larger
model and longer training/inference time, it still
cannot outperform h-d2v of 30.37 at k = 400.

too few citation contexts to train a good model.
Moreover, DBLP requires a larger dimension size
k to store more information in the embedding vec-
tors. We increase k and report the Rec@10 scores
in Figure 3. We see that all approaches have bet-
ter performance when k increases to 200, though
d2v-based ones start to drop beyond this point.

Second, the I4I variant of w2v has the worst
performance among all approaches. This obser-
vation validates our hypothesis in Section 5.3.

Third, the d2v-cac approach outperforms its
variant d2v-nc in terms of all datasets and met-
rics. This indicates that context awareness matters
in the citation recommendation task.

Fourth, the performance of NPM is sandwiched
between those of w2v’s two variants. We have
tried our best to reproduce it. Our explanation is
that NPM is citation-as-word-based, and only de-
pends on citation contexts for training. Therefore,
it is only context aware but neither content aware
nor newcomer friendly, and behaves like w2v.

Finally, when retrofitting pv-dm, h-d2v gen-
erally has the best performance. When we substi-
tute pv-dm with random initialization, the perfor-
mance is deteriorated by varying degrees on differ-
ent datasets. This implies that content awareness
is also important, if not so important than context
awareness, on the citation recommendation task.

5.3.2 Impact of Newcomer Friendliness
Table 7 analyzes the impact of newcomer friendli-
ness. Opposite from what is done in Section 5.2.2,
we only evaluate on testing examples where at
least a ground-truth paper is a newcomer. Please
note that newcomer unfriendly approaches do not

Model NewcomerFriendly Rec MAP MRR nDCG

w2v (I4O) × 3.64 3.23 3.41 2.73
NPM × 1.37 1.13 1.15 0.92
d2v-nc � 6.48 3.52 3.54 3.96
d2v-cac � 8.16 5.13 5.24 5.21
h-d2v � 6.41 4.95 5.21 4.49

Table 7: DBLP results evaluated on 63,342 cita-
tion contexts with newcomer ground-truth.

Category Description

Weak Weakness of cited approach

CoCoGM Contrast/Comparison in Goals/Methods (neutral)
CoCo- Work stated to be superior to cited work
CoCoR0 Contrast/Comparison in Results (neutral)
CoCoXY Contrast between 2 cited methods

PBas Author uses cited work as basis or starting point
PUse Author uses tools/algorithms/data/definitions
PModi Author adapts or modifies tools/algorithms/data
PMot This citation is positive about approach used or

problem addressed (used to motivate work in cur-
rent paper)

PSim Author’s work and cited work are similar
PSup Author’s work and cited work are compati-

ble/provide support for each other
Neut Neutral description of cited work, or not enough

textual evidence for above categories, or unlisted
citation function

Table 8: Annotation scheme of citation functions
in Teufel et al. (2006).

necessarily get zero scores. The table shows that
newcomer friendly approaches are superior to un-
friendly ones. Note that, like Table 5, this table is
also based on controlled experiments and not in-
tended for comparing approaches.

5.3.3 Impact of Context Intent Awareness
In this section, we analyze the impact of context
intent awareness. We use Teufel et al. (2006)’s
2,824 citation contexts11 with annotated citation
functions, e.g., emphasizing weakness (Weak) or
using tools/algorithms (PBas) of the cited papers.
Table 8 from Teufel et al. (2006) describes the full
annotating scheme. Teufel et al. (2006) also use
manual features to evaluate citation function clas-
sification. To test all models on capturing con-
text intents, we average all context words’ IN vec-
tors (trained on DBLP) as features. Noticing that
pv-dbow does not output IN word vectors, and
OUT vectors do not provide reasonable results, we
use pv-dm here instead. We use SVM with RBF

11The number is 2,829 in the original paper. The inconsis-
tency may be due to different regular expressions we used.



2392

Query and Ground Truth Result Ranking of w2v Result Ranking of d2v-cac Result Ranking of h-d2v

. . . We also evaluate our model
by computing the machine trans-
lation BLEU score (Papineni
et al., 2002) using the Moses
system (Koehn et al., 2007). . .

(Papineni et al., 2002) BLEU: a
Method for Automatic Evalua-
tion of Machine Translation
(Koehn et al., 2007) Moses:
Open Source Toolkit for Sta-
tistical Machine Translation

1. HMM-Based Word Alignment in
Statistical Translation
2. Indirect-HMM-based Hypothe-
sis Alignment for Combining Outputs
from Machine Translation Systems
3. The Alignment Template Approach
to Statistical Machine Translation

. . .
9. Moses: Open Source Toolkit for
Statistical Machine Translation
57. BLEU: a Method for Automatic
Evaluation of Machine Translation

1. Discriminative Reranking for Ma-
chine Translation
2. Learning Phrase-Based Head Trans-
duction Models for Translation of Spo-
ken Utterances
3. Cognates Can Improve Statistical
Translation Models

. . .
6. BLEU: a Method for Automatic
Evaluation of Machine Translation
29. Moses: Open Source Toolkit for
Statistical Machine Translation

1. BLEU: a Method for Au-
tomatic Evaluation of Machine
Translation
2. Statistical Phrase-Based
Translation
3. Improved Statistical Align-
ment Models
4. HMM-Based Word Align-
ment in Statistical Translation
5. Moses: Open Source Toolkit
for Statistical Machine Trans-
lation

Table 9: Papers recommended by different approaches for a citation context in Zhao and Gildea (2010).

W
ea

k

Co
Co

GM

Co
Co

R0

Co
Co

-

Co
Co

XY
PB

as
PU

se

PM
od

i

PM
ot

PS
im

PS
up

Ne
ut

0

20

40

60

80

100

F
1
 (

%
)

EMNLP'06 (Macro: 57.00  Micro: 77.00)
w2v            (Macro: 44.86  Micro: 74.43)
d2v-cac      (Macro: 24.19  Micro: 70.64)
h-d2v          (Macro: 54.37  Micro: 75.39)

Figure 4: F1 of citation function classification.

kernels and default parameters. Following Teufel
et al. (2006), we use 10-fold cross validation.

Figure 4 depicts the F1 scores. Scores of Teufel
et al. (2006)’s approach are from the original pa-
per. We omit d2v-nc because it is very inferior to
d2v-cac. We have the following observations.

First, Teufel et al. (2006)’s feature-engineering-
based approach has the best performance. Note
that we cannot obtain their original cross valida-
tion split, so the comparison may not be fair and is
only for consideration in terms of numbers.

Second, among all embedding-based methods,
h-d2v has the best citation function classification
results, which is close to Teufel et al. (2006)’s.

Finally, the d2v-cac vectors are only good at
Neutral, the largest class. On the other classes and
global F1, they are outperformed by w2v vectors.

To study how citation function affects citation
recommendation, we combine the 2,824 labeled
citation contexts and another 1,075 labeled con-
texts the authors published later to train an SVM,
and apply it to the DBLP testing set to get cita-
tion functions. We evaluate citation recommenda-
tion performance of w2v (I4O), d2v-cac, and
h-d2v on a per-citation-function basis. In Fig-
ure 5, we break down Rec@10 scores on citation
functions. On the six largest classes (marked by
solid dots), h-d2v outperforms all competitors.

100

102

104

106

C
ou

nt

W
ea

k

Co
Co

GM

Co
Co

R0

Co
Co

-

Co
Co

XY
PB

as
PU

se

PM
od

i

PM
ot

PS
im

PS
up

Ne
ut

10

20

30

40

R
ec

@
10

 (
%

)

w2v
d2v-cac
h-d2v
Category Size

Figure 5: Rec@10 w.r.t. citation functions.

To better investigate the impact of context intent
awareness, Table 9 shows recommended papers of
the running example of this paper. Here, Zhao and
Gildea (2010) cited the BLEU metric (Papineni
et al., 2002) and Moses tools (Koehn et al., 2007)
of machine translation. However, the additional
words “machine translation” lead both w2v and
d2v-cac to recommend many machine transla-
tion papers. Only our h-d2v manages to recog-
nize the citation function “using tools/algorithms
(PBas)”, and concentrates on the citation intent to
return the right papers in top-5 results.

6 Conclusion

We focus on the hyper-doc embedding problem.
We propose that hyper-doc embedding algorithms
should be content aware, context aware, new-
comer friendly, and context intent aware. To meet
all four criteria, we propose a general approach,
hyperdoc2vec, which assigns two vectors to
each hyper-doc and models citations in a straight-
forward manner. In doing so, the learned embed-
dings satisfy all criteria, which no existing model
is able to. For evaluation, paper classification and
citation recommendation are conducted on three
academic paper datasets. Results confirm the ef-
fectiveness of our approach. Further analyses also
demonstrate that possessing the four properties
helps h-d2v outperform other models.



2393

References
Matthew Berger, Katherine McDonough, and Lee M.

Seversky. 2017. cite2vec: Citation-driven document
exploration via word embeddings. IEEE Trans. Vis.
Comput. Graph. 23(1):691–700.

David A. Cohn and Thomas Hofmann. 2000. The
missing link - A probabilistic model of document
content and hypertext connectivity. In Advances in
Neural Information Processing Systems 13, Papers
from Neural Information Processing Systems (NIPS)
2000. pages 430–436.

Isaac G. Councill, C. Lee Giles, and Min-Yen Kan.
2008. Parscit: an open-source CRF reference string
parsing package. In Proceedings of the Interna-
tional Conference on Language Resources and Eval-
uation, LREC 2008.

Silviu Cucerzan. 2007. Large-scale named entity dis-
ambiguation based on wikipedia data. In EMNLP-
CoNLL 2007, Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning. pages 708–716.

Travis Ebesu and Yi Fang. 2017. Neural citation net-
work for context-aware citation recommendation. In
Proceedings of the 40th International ACM SIGIR
Conference on Research and Development in Infor-
mation Retrieval. pages 1093–1096.

Wei Fang, Jianwen Zhang, Dilin Wang, Zheng Chen,
and Ming Li. 2016. Entity disambiguation by
knowledge and text jointly embedding. In Proceed-
ings of the 20th SIGNLL Conference on Computa-
tional Natural Language Learning, CoNLL 2016.
pages 260–269.

Manaal Faruqui, Jesse Dodge, Sujay Kumar Jauhar,
Chris Dyer, Eduard H. Hovy, and Noah A. Smith.
2015. Retrofitting word vectors to semantic lexi-
cons. In NAACL HLT 2015, The 2015 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies. pages 1606–1615.

Evgeniy Gabrilovich and Shaul Markovitch. 2007.
Computing semantic relatedness using wikipedia-
based explicit semantic analysis. In IJCAI 2007,
Proceedings of the 20th International Joint Confer-
ence on Artificial Intelligence. pages 1606–1611.

Soumyajit Ganguly and Vikram Pudi. 2017. Pa-
per2vec: Combining graph and text information for
scientific paper representation. In Advances in In-
formation Retrieval - 39th European Conference on
IR Research, ECIR 2017. pages 383–395.

Aditya Grover and Jure Leskovec. 2016. node2vec:
Scalable feature learning for networks. In Proceed-
ings of the 22nd ACM SIGKDD International Con-
ference on Knowledge Discovery and Data Mining.
pages 855–864.

Qi He, Jian Pei, Daniel Kifer, Prasenjit Mitra, and
C. Lee Giles. 2010. Context-aware citation recom-
mendation. In Proceedings of the 19th International
Conference on World Wide Web, WWW 2010. pages
421–430.

Zhengyan He, Shujie Liu, Mu Li, Ming Zhou, Longkai
Zhang, and Houfeng Wang. 2013. Learning entity
representation for entity disambiguation. In Pro-
ceedings of the 51st Annual Meeting of the Associa-
tion for Computational Linguistics, ACL 2013, Vol-
ume 2: Short Papers. pages 30–34.

Hongzhao Huang, Larry P. Heck, and Heng Ji.
2015a. Leveraging deep neural networks and
knowledge graphs for entity disambiguation. CoRR
abs/1504.07678.

Wenyi Huang, Saurabh Kataria, Cornelia Caragea,
Prasenjit Mitra, C. Lee Giles, and Lior Rokach.
2012. Recommending citations: translating papers
into references. In 21st ACM International Confer-
ence on Information and Knowledge Management,
CIKM’12. pages 1910–1914.

Wenyi Huang, Zhaohui Wu, Liang Chen, Prasenjit Mi-
tra, and C. Lee Giles. 2015b. A neural probabilistic
model for context based citation recommendation.
In Proceedings of the Twenty-Ninth AAAI Confer-
ence on Artificial Intelligence. pages 2404–2410.

Saurabh Kataria, Prasenjit Mitra, and Sumit Bhatia.
2010. Utilizing context in generative bayesian mod-
els for linked corpus. In Proceedings of the Twenty-
Fourth AAAI Conference on Artificial Intelligence,
AAAI 2010.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
ACL 2007, Proceedings of the 45th Annual Meeting
of the Association for Computational Linguistics.

Quoc V. Le and Tomas Mikolov. 2014. Distributed rep-
resentations of sentences and documents. In Pro-
ceedings of the 31th International Conference on
Machine Learning, ICML 2014. pages 1188–1196.

Qing Lu and Lise Getoor. 2003. Link-based classifi-
cation. In Machine Learning, Proceedings of the
Twentieth International Conference (ICML 2003).
pages 496–503.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.
Corrado, and Jeffrey Dean. 2013. Distributed rep-
resentations of words and phrases and their com-
positionality. In Advances in Neural Information
Processing Systems 26: 27th Annual Conference
on Neural Information Processing Systems 2013..
pages 3111–3119.



2394

Eric T. Nalisnick, Bhaskar Mitra, Nick Craswell, and
Rich Caruana. 2016. Improving document ranking
with dual word embeddings. In Proceedings of the
25th International Conference on World Wide Web,
WWW 2016, Companion Volume. pages 83–84.

Lawrence Page, Sergey Brin, Rajeev Motwani, and
Terry Winograd. 1999. The pagerank citation rank-
ing: Bringing order to the web. .

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics. pages 311–318.

Bryan Perozzi, Rami Al-Rfou, and Steven Skiena.
2014. Deepwalk: online learning of social repre-
sentations. In The 20th ACM SIGKDD International
Conference on Knowledge Discovery and Data Min-
ing, KDD ’14. pages 701–710.

Radim Řehůřek and Petr Sojka. 2010. Software Frame-
work for Topic Modelling with Large Corpora. In
Proceedings of the LREC 2010 Workshop on New
Challenges for NLP Frameworks. ELRA, Valletta,
Malta, pages 45–50. http://is.muni.cz/
publication/884893/en.

Wei Shen, Jianyong Wang, and Jiawei Han. 2015. En-
tity linking with a knowledge base: Issues, tech-
niques, and solutions. IEEE Trans. Knowl. Data
Eng. 27(2):443–460.

Kazunari Sugiyama and Min-Yen Kan. 2010. Schol-
arly paper recommendation via user’s recent re-
search interests. In Proceedings of the 2010 Joint In-
ternational Conference on Digital Libraries, JCDL
2010. pages 29–38.

Yaming Sun, Lei Lin, Duyu Tang, Nan Yang, Zhen-
zhou Ji, and Xiaolong Wang. 2015. Modeling men-
tion, context and entity with neural networks for en-
tity disambiguation. In Proceedings of the Twenty-
Fourth International Joint Conference on Artificial
Intelligence, IJCAI 2015. pages 1333–1339.

Jian Tang, Meng Qu, and Qiaozhu Mei. 2015a. PTE:
predictive text embedding through large-scale het-
erogeneous text networks. In Proceedings of
the 21th ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining. pages
1165–1174.

Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun
Yan, and Qiaozhu Mei. 2015b. LINE: large-scale
information network embedding. In Proceedings of
the 24th International Conference on World Wide
Web, WWW 2015. pages 1067–1077.

Jie Tang and Jing Zhang. 2009. A discriminative ap-
proach to topic-based citation recommendation. In
Advances in Knowledge Discovery and Data Min-
ing, 13th Pacific-Asia Conference, PAKDD 2009.
pages 572–579.

Simone Teufel, Advaith Siddharthan, and Dan Tidhar.
2006. Automatic classification of citation function.
In EMNLP 2007, Proceedings of the 2006 Confer-
ence on Empirical Methods in Natural Language
Processing. pages 103–110.

Cunchao Tu, Weicheng Zhang, Zhiyuan Liu, and
Maosong Sun. 2016. Max-margin deepwalk: Dis-
criminative learning of network representation. In
Proceedings of the Twenty-Fifth International Joint
Conference on Artificial Intelligence, IJCAI 2016.
pages 3889–3895.

Suhang Wang, Jiliang Tang, Charu C. Aggarwal, and
Huan Liu. 2016. Linked document embedding for
classification. In Proceedings of the 25th ACM In-
ternational Conference on Information and Knowl-
edge Management, CIKM 2016. pages 115–124.

Ikuya Yamada, Hiroyuki Shindo, Hideaki Takeda, and
Yoshiyasu Takefuji. 2016. Joint learning of the em-
bedding of words and entities for named entity dis-
ambiguation. In Proceedings of the 20th SIGNLL
Conference on Computational Natural Language
Learning, CoNLL 2016. pages 250–259.

Cheng Yang, Zhiyuan Liu, Deli Zhao, Maosong Sun,
and Edward Y. Chang. 2015. Network representa-
tion learning with rich text information. In Proceed-
ings of the Twenty-Fourth International Joint Con-
ference on Artificial Intelligence, IJCAI 2015. pages
2111–2117.

Shaojun Zhao and Daniel Gildea. 2010. A fast fertil-
ity hidden markov model for word alignment using
MCMC. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP 2010. pages 596–605.

Stefan Zwicklbauer, Christin Seifert, and Michael
Granitzer. 2016. Robust and collective entity dis-
ambiguation through semantic embeddings. In Pro-
ceedings of the 39th International ACM SIGIR con-
ference on Research and Development in Informa-
tion Retrieval, SIGIR 2016. pages 425–434.
















<<
  /ASCII85EncodePages false
  /AllowTransparency false
  /AutoPositionEPSFiles true
  /AutoRotatePages /None
  /Binding /Left
  /CalGrayProfile (Dot Gain 20%)
  /CalRGBProfile (sRGB IEC61966-2.1)
  /CalCMYKProfile (U.S. Web Coated \050SWOP\051 v2)
  /sRGBProfile (sRGB IEC61966-2.1)
  /CannotEmbedFontPolicy /Error
  /CompatibilityLevel 1.4
  /CompressObjects /Tags
  /CompressPages true
  /ConvertImagesToIndexed true
  /PassThroughJPEGImages true
  /CreateJobTicket false
  /DefaultRenderingIntent /Default
  /DetectBlends true
  /DetectCurves 0.0000
  /ColorConversionStrategy /CMYK
  /DoThumbnails false
  /EmbedAllFonts true
  /EmbedOpenType false
  /ParseICCProfilesInComments true
  /EmbedJobOptions true
  /DSCReportingLevel 0
  /EmitDSCWarnings false
  /EndPage -1
  /ImageMemory 1048576
  /LockDistillerParams false
  /MaxSubsetPct 100
  /Optimize true
  /OPM 1
  /ParseDSCComments true
  /ParseDSCCommentsForDocInfo true
  /PreserveCopyPage true
  /PreserveDICMYKValues true
  /PreserveEPSInfo true
  /PreserveFlatness true
  /PreserveHalftoneInfo false
  /PreserveOPIComments true
  /PreserveOverprintSettings true
  /StartPage 1
  /SubsetFonts true
  /TransferFunctionInfo /Apply
  /UCRandBGInfo /Preserve
  /UsePrologue false
  /ColorSettingsFile ()
  /AlwaysEmbed [ true
  ]
  /NeverEmbed [ true
  ]
  /AntiAliasColorImages false
  /CropColorImages true
  /ColorImageMinResolution 300
  /ColorImageMinResolutionPolicy /OK
  /DownsampleColorImages true
  /ColorImageDownsampleType /Bicubic
  /ColorImageResolution 300
  /ColorImageDepth -1
  /ColorImageMinDownsampleDepth 1
  /ColorImageDownsampleThreshold 1.50000
  /EncodeColorImages true
  /ColorImageFilter /DCTEncode
  /AutoFilterColorImages true
  /ColorImageAutoFilterStrategy /JPEG
  /ColorACSImageDict <<
    /QFactor 0.15
    /HSamples [1 1 1 1] /VSamples [1 1 1 1]
  >>
  /ColorImageDict <<
    /QFactor 0.15
    /HSamples [1 1 1 1] /VSamples [1 1 1 1]
  >>
  /JPEG2000ColorACSImageDict <<
    /TileWidth 256
    /TileHeight 256
    /Quality 30
  >>
  /JPEG2000ColorImageDict <<
    /TileWidth 256
    /TileHeight 256
    /Quality 30
  >>
  /AntiAliasGrayImages false
  /CropGrayImages true
  /GrayImageMinResolution 300
  /GrayImageMinResolutionPolicy /OK
  /DownsampleGrayImages true
  /GrayImageDownsampleType /Bicubic
  /GrayImageResolution 300
  /GrayImageDepth -1
  /GrayImageMinDownsampleDepth 2
  /GrayImageDownsampleThreshold 1.50000
  /EncodeGrayImages true
  /GrayImageFilter /DCTEncode
  /AutoFilterGrayImages true
  /GrayImageAutoFilterStrategy /JPEG
  /GrayACSImageDict <<
    /QFactor 0.15
    /HSamples [1 1 1 1] /VSamples [1 1 1 1]
  >>
  /GrayImageDict <<
    /QFactor 0.15
    /HSamples [1 1 1 1] /VSamples [1 1 1 1]
  >>
  /JPEG2000GrayACSImageDict <<
    /TileWidth 256
    /TileHeight 256
    /Quality 30
  >>
  /JPEG2000GrayImageDict <<
    /TileWidth 256
    /TileHeight 256
    /Quality 30
  >>
  /AntiAliasMonoImages false
  /CropMonoImages true
  /MonoImageMinResolution 1200
  /MonoImageMinResolutionPolicy /OK
  /DownsampleMonoImages true
  /MonoImageDownsampleType /Bicubic
  /MonoImageResolution 1200
  /MonoImageDepth -1
  /MonoImageDownsampleThreshold 1.50000
  /EncodeMonoImages true
  /MonoImageFilter /CCITTFaxEncode
  /MonoImageDict <<
    /K -1
  >>
  /AllowPSXObjects false
  /CheckCompliance [
    /None
  ]
  /PDFX1aCheck false
  /PDFX3Check false
  /PDFXCompliantPDFOnly false
  /PDFXNoTrimBoxError true
  /PDFXTrimBoxToMediaBoxOffset [
    0.00000
    0.00000
    0.00000
    0.00000
  ]
  /PDFXSetBleedBoxToMediaBox true
  /PDFXBleedBoxToTrimBoxOffset [
    0.00000
    0.00000
    0.00000
    0.00000
  ]
  /PDFXOutputIntentProfile ()
  /PDFXOutputConditionIdentifier ()
  /PDFXOutputCondition ()
  /PDFXRegistryName ()
  /PDFXTrapped /False

  /CreateJDFFile false
  /Description <<
    /ARA <FEFF06270633062A062E062F0645002006470630064700200627064406250639062F0627062F0627062A002006440625064606340627062100200648062B062706260642002000410064006F00620065002000500044004600200645062A064806270641064206290020064406440637062806270639062900200641064A00200627064406450637062706280639002006300627062A0020062F0631062C0627062A002006270644062C0648062F0629002006270644063906270644064A0629061B0020064A06450643064600200641062A062D00200648062B0627062606420020005000440046002006270644064506460634062306290020062806270633062A062E062F062706450020004100630072006F0062006100740020064800410064006F006200650020005200650061006400650072002006250635062F0627063100200035002E0030002006480627064406250635062F062706310627062A0020062706440623062D062F062B002E0635062F0627063100200035002E0030002006480627064406250635062F062706310627062A0020062706440623062D062F062B002E>
    /BGR <FEFF04180437043f043e043b043704320430043904420435002004420435043704380020043d0430044104420440043e0439043a0438002c00200437043000200434043000200441044a0437043404300432043004420435002000410064006f00620065002000500044004600200434043e043a0443043c0435043d04420438002c0020043c0430043a04410438043c0430043b043d043e0020043f044004380433043e04340435043d04380020043704300020043204380441043e043a043e043a0430044704350441044204320435043d0020043f04350447043004420020043704300020043f044004350434043f0435044704300442043d04300020043f043e04340433043e0442043e0432043a0430002e002000200421044a04370434043004340435043d043804420435002000500044004600200434043e043a0443043c0435043d044204380020043c043e0433043004420020043404300020044104350020043e0442043204300440044f0442002004410020004100630072006f00620061007400200438002000410064006f00620065002000520065006100640065007200200035002e00300020043800200441043b0435043404320430044904380020043204350440044104380438002e>
    /CHS <FEFF4f7f75288fd94e9b8bbe5b9a521b5efa7684002000410064006f006200650020005000440046002065876863900275284e8e9ad88d2891cf76845370524d53705237300260a853ef4ee54f7f75280020004100630072006f0062006100740020548c002000410064006f00620065002000520065006100640065007200200035002e003000204ee553ca66f49ad87248672c676562535f00521b5efa768400200050004400460020658768633002>
    /CHT <FEFF4f7f752890194e9b8a2d7f6e5efa7acb7684002000410064006f006200650020005000440046002065874ef69069752865bc9ad854c18cea76845370524d5370523786557406300260a853ef4ee54f7f75280020004100630072006f0062006100740020548c002000410064006f00620065002000520065006100640065007200200035002e003000204ee553ca66f49ad87248672c4f86958b555f5df25efa7acb76840020005000440046002065874ef63002>
    /CZE <FEFF005400610074006f0020006e006100730074006100760065006e00ed00200070006f0075017e0069006a007400650020006b0020007600790074007600e101590065006e00ed00200064006f006b0075006d0065006e0074016f002000410064006f006200650020005000440046002c0020006b00740065007200e90020007300650020006e0065006a006c00e90070006500200068006f006400ed002000700072006f0020006b00760061006c00690074006e00ed0020007400690073006b00200061002000700072006500700072006500730073002e002000200056007900740076006f01590065006e00e900200064006f006b0075006d0065006e007400790020005000440046002000620075006400650020006d006f017e006e00e90020006f007400650076015900ed007400200076002000700072006f006700720061006d0065006300680020004100630072006f00620061007400200061002000410064006f00620065002000520065006100640065007200200035002e0030002000610020006e006f0076011b006a016100ed00630068002e>
    /DAN <FEFF004200720075006700200069006e0064007300740069006c006c0069006e006700650072006e0065002000740069006c0020006100740020006f007000720065007400740065002000410064006f006200650020005000440046002d0064006f006b0075006d0065006e007400650072002c0020006400650072002000620065006400730074002000650067006e006500720020007300690067002000740069006c002000700072006500700072006500730073002d007500640073006b007200690076006e0069006e00670020006100660020006800f8006a0020006b00760061006c0069007400650074002e0020004400650020006f007000720065007400740065006400650020005000440046002d0064006f006b0075006d0065006e0074006500720020006b0061006e002000e50062006e00650073002000690020004100630072006f00620061007400200065006c006c006500720020004100630072006f006200610074002000520065006100640065007200200035002e00300020006f00670020006e0079006500720065002e>
    /DEU <FEFF00560065007200770065006e00640065006e0020005300690065002000640069006500730065002000450069006e007300740065006c006c0075006e00670065006e0020007a0075006d002000450072007300740065006c006c0065006e00200076006f006e002000410064006f006200650020005000440046002d0044006f006b0075006d0065006e00740065006e002c00200076006f006e002000640065006e0065006e002000530069006500200068006f006300680077006500720074006900670065002000500072006500700072006500730073002d0044007200750063006b0065002000650072007a0065007500670065006e0020006d00f60063006800740065006e002e002000450072007300740065006c006c007400650020005000440046002d0044006f006b0075006d0065006e007400650020006b00f6006e006e0065006e0020006d006900740020004100630072006f00620061007400200075006e0064002000410064006f00620065002000520065006100640065007200200035002e00300020006f0064006500720020006800f600680065007200200067006500f600660066006e00650074002000770065007200640065006e002e>
    /ESP <FEFF005500740069006c0069006300650020006500730074006100200063006f006e0066006900670075007200610063006900f3006e0020007000610072006100200063007200650061007200200064006f00630075006d0065006e0074006f00730020005000440046002000640065002000410064006f0062006500200061006400650063007500610064006f00730020007000610072006100200069006d0070007200650073006900f3006e0020007000720065002d0065006400690074006f007200690061006c00200064006500200061006c00740061002000630061006c0069006400610064002e002000530065002000700075006500640065006e00200061006200720069007200200064006f00630075006d0065006e0074006f00730020005000440046002000630072006500610064006f007300200063006f006e0020004100630072006f006200610074002c002000410064006f00620065002000520065006100640065007200200035002e003000200079002000760065007200730069006f006e0065007300200070006f00730074006500720069006f007200650073002e>
    /ETI <FEFF004b00610073007500740061006700650020006e0065006900640020007300e4007400740065006900640020006b00760061006c006900740065006500740073006500200074007200fc006b006900650065006c007300650020007000720069006e00740069006d0069007300650020006a0061006f006b007300200073006f00620069006c0069006b0065002000410064006f006200650020005000440046002d0064006f006b0075006d0065006e00740069006400650020006c006f006f006d006900730065006b0073002e00200020004c006f006f0064007500640020005000440046002d0064006f006b0075006d0065006e00740065002000730061006100740065002000610076006100640061002000700072006f006700720061006d006d006900640065006700610020004100630072006f0062006100740020006e0069006e0067002000410064006f00620065002000520065006100640065007200200035002e00300020006a00610020007500750065006d006100740065002000760065007200730069006f006f006e00690064006500670061002e000d000a>
    /FRA <FEFF005500740069006c006900730065007a00200063006500730020006f007000740069006f006e00730020006100660069006e00200064006500200063007200e900650072002000640065007300200064006f00630075006d0065006e00740073002000410064006f00620065002000500044004600200070006f0075007200200075006e00650020007100750061006c0069007400e90020006400270069006d007000720065007300730069006f006e00200070007200e9007000720065007300730065002e0020004c0065007300200064006f00630075006d0065006e00740073002000500044004600200063007200e900e90073002000700065007500760065006e0074002000ea0074007200650020006f007500760065007200740073002000640061006e00730020004100630072006f006200610074002c002000610069006e00730069002000710075002700410064006f00620065002000520065006100640065007200200035002e0030002000650074002000760065007200730069006f006e007300200075006c007400e90072006900650075007200650073002e>
    /GRE <FEFF03a703c103b703c303b903bc03bf03c003bf03b903ae03c303c403b5002003b103c503c403ad03c2002003c403b903c2002003c103c503b803bc03af03c303b503b903c2002003b303b903b1002003bd03b1002003b403b703bc03b903bf03c503c103b303ae03c303b503c403b5002003ad03b303b303c103b103c603b1002000410064006f006200650020005000440046002003c003bf03c5002003b503af03bd03b103b9002003ba03b103c42019002003b503be03bf03c703ae03bd002003ba03b103c403ac03bb03bb03b703bb03b1002003b303b903b1002003c003c103bf002d03b503ba03c403c503c003c903c403b903ba03ad03c2002003b503c103b303b103c303af03b503c2002003c503c803b703bb03ae03c2002003c003bf03b903cc03c403b703c403b103c2002e0020002003a403b10020005000440046002003ad03b303b303c103b103c603b1002003c003bf03c5002003ad03c703b503c403b5002003b403b703bc03b903bf03c503c103b303ae03c303b503b9002003bc03c003bf03c103bf03cd03bd002003bd03b1002003b103bd03bf03b903c703c403bf03cd03bd002003bc03b5002003c403bf0020004100630072006f006200610074002c002003c403bf002000410064006f00620065002000520065006100640065007200200035002e0030002003ba03b103b9002003bc03b503c403b103b303b503bd03ad03c303c403b503c103b503c2002003b503ba03b403cc03c303b503b903c2002e>
    /HEB <FEFF05D405E905EA05DE05E905D5002005D105D405D205D305E805D505EA002005D005DC05D4002005DB05D305D9002005DC05D905E605D505E8002005DE05E105DE05DB05D9002000410064006F006200650020005000440046002005D405DE05D505EA05D005DE05D905DD002005DC05D405D305E405E105EA002005E705D305DD002D05D305E405D505E1002005D005D905DB05D505EA05D905EA002E002005DE05E105DE05DB05D90020005000440046002005E905E005D505E605E805D5002005E005D905EA05E005D905DD002005DC05E405EA05D905D705D4002005D105D005DE05E605E205D505EA0020004100630072006F006200610074002005D5002D00410064006F00620065002000520065006100640065007200200035002E0030002005D505D205E805E105D005D505EA002005DE05EA05E705D305DE05D505EA002005D905D505EA05E8002E05D005DE05D905DD002005DC002D005000440046002F0058002D0033002C002005E205D905D905E005D5002005D105DE05D305E805D905DA002005DC05DE05E905EA05DE05E9002005E905DC0020004100630072006F006200610074002E002005DE05E105DE05DB05D90020005000440046002005E905E005D505E605E805D5002005E005D905EA05E005D905DD002005DC05E405EA05D905D705D4002005D105D005DE05E605E205D505EA0020004100630072006F006200610074002005D5002D00410064006F00620065002000520065006100640065007200200035002E0030002005D505D205E805E105D005D505EA002005DE05EA05E705D305DE05D505EA002005D905D505EA05E8002E>
    /HRV (Za stvaranje Adobe PDF dokumenata najpogodnijih za visokokvalitetni ispis prije tiskanja koristite ove postavke.  Stvoreni PDF dokumenti mogu se otvoriti Acrobat i Adobe Reader 5.0 i kasnijim verzijama.)
    /HUN <FEFF004b0069007600e1006c00f30020006d0069006e0151007300e9006701710020006e0079006f006d00640061006900200065006c0151006b00e90073007a00ed007401510020006e0079006f006d00740061007400e100730068006f007a0020006c006500670069006e006b00e1006200620020006d0065006700660065006c0065006c0151002000410064006f00620065002000500044004600200064006f006b0075006d0065006e00740075006d006f006b0061007400200065007a0065006b006b0065006c0020006100200062006500e1006c006c00ed007400e10073006f006b006b0061006c0020006b00e90073007a00ed0074006800650074002e0020002000410020006c00e90074007200650068006f007a006f00740074002000500044004600200064006f006b0075006d0065006e00740075006d006f006b00200061007a0020004100630072006f006200610074002000e9007300200061007a002000410064006f00620065002000520065006100640065007200200035002e0030002c0020007600610067007900200061007a002000610074007400f3006c0020006b00e9007301510062006200690020007600650072007a006900f3006b006b0061006c0020006e00790069007400680061007400f3006b0020006d00650067002e>
    /ITA <FEFF005500740069006c0069007a007a006100720065002000710075006500730074006500200069006d0070006f007300740061007a0069006f006e00690020007000650072002000630072006500610072006500200064006f00630075006d0065006e00740069002000410064006f00620065002000500044004600200070006900f900200061006400610074007400690020006100200075006e00610020007000720065007300740061006d0070006100200064006900200061006c007400610020007100750061006c0069007400e0002e0020004900200064006f00630075006d0065006e007400690020005000440046002000630072006500610074006900200070006f00730073006f006e006f0020006500730073006500720065002000610070006500720074006900200063006f006e0020004100630072006f00620061007400200065002000410064006f00620065002000520065006100640065007200200035002e003000200065002000760065007200730069006f006e006900200073007500630063006500730073006900760065002e>
    /JPN <FEFF9ad854c18cea306a30d730ea30d730ec30b951fa529b7528002000410064006f0062006500200050004400460020658766f8306e4f5c6210306b4f7f75283057307e305930023053306e8a2d5b9a30674f5c62103055308c305f0020005000440046002030d530a130a430eb306f3001004100630072006f0062006100740020304a30883073002000410064006f00620065002000520065006100640065007200200035002e003000204ee5964d3067958b304f30533068304c3067304d307e305930023053306e8a2d5b9a306b306f30d530a930f330c8306e57cb30818fbc307f304c5fc59808306730593002>
    /KOR <FEFFc7740020c124c815c7440020c0acc6a9d558c5ec0020ace0d488c9c80020c2dcd5d80020c778c1c4c5d00020ac00c7a50020c801d569d55c002000410064006f0062006500200050004400460020bb38c11cb97c0020c791c131d569b2c8b2e4002e0020c774b807ac8c0020c791c131b41c00200050004400460020bb38c11cb2940020004100630072006f0062006100740020bc0f002000410064006f00620065002000520065006100640065007200200035002e00300020c774c0c1c5d0c11c0020c5f40020c2180020c788c2b5b2c8b2e4002e>
    /LTH <FEFF004e006100750064006f006b0069007400650020016100690075006f007300200070006100720061006d006500740072007500730020006e006f0072011700640061006d00690020006b0075007200740069002000410064006f00620065002000500044004600200064006f006b0075006d0065006e007400750073002c0020006b00750072006900650020006c0061006200690061007500730069006100690020007000720069007400610069006b007900740069002000610075006b01610074006f00730020006b006f006b007900620117007300200070006100720065006e006700740069006e00690061006d00200073007000610075007300640069006e0069006d00750069002e0020002000530075006b0075007200740069002000500044004600200064006f006b0075006d0065006e007400610069002000670061006c006900200062016b007400690020006100740069006400610072006f006d00690020004100630072006f006200610074002000690072002000410064006f00620065002000520065006100640065007200200035002e0030002000610072002000760117006c00650073006e0117006d00690073002000760065007200730069006a006f006d00690073002e>
    /LVI <FEFF0049007a006d0061006e0074006f006a00690065007400200161006f00730020006900650073007400610074012b006a0075006d00750073002c0020006c0061006900200076006500690064006f00740075002000410064006f00620065002000500044004600200064006f006b0075006d0065006e007400750073002c0020006b006100730020006900720020012b00700061016100690020007000690065006d01130072006f00740069002000610075006700730074006100730020006b00760061006c0069007401010074006500730020007000690072006d007300690065007300700069006501610061006e006100730020006400720075006b00610069002e00200049007a0076006500690064006f006a006900650074002000500044004600200064006f006b0075006d0065006e007400750073002c0020006b006f002000760061007200200061007400760113007200740020006100720020004100630072006f00620061007400200075006e002000410064006f00620065002000520065006100640065007200200035002e0030002c0020006b0101002000610072012b00200074006f0020006a00610075006e0101006b0101006d002000760065007200730069006a0101006d002e>
    /NLD (Gebruik deze instellingen om Adobe PDF-documenten te maken die zijn geoptimaliseerd voor prepress-afdrukken van hoge kwaliteit. De gemaakte PDF-documenten kunnen worden geopend met Acrobat en Adobe Reader 5.0 en hoger.)
    /NOR <FEFF004200720075006b00200064006900730073006500200069006e006e007300740069006c006c0069006e00670065006e0065002000740069006c002000e50020006f0070007000720065007400740065002000410064006f006200650020005000440046002d0064006f006b0075006d0065006e00740065007200200073006f006d00200065007200200062006500730074002000650067006e0065007400200066006f00720020006600f80072007400720079006b006b0073007500740073006b00720069006600740020006100760020006800f800790020006b00760061006c0069007400650074002e0020005000440046002d0064006f006b0075006d0065006e00740065006e00650020006b0061006e002000e50070006e00650073002000690020004100630072006f00620061007400200065006c006c00650072002000410064006f00620065002000520065006100640065007200200035002e003000200065006c006c00650072002000730065006e006500720065002e>
    /POL <FEFF0055007300740061007700690065006e0069006100200064006f002000740077006f0072007a0065006e0069006100200064006f006b0075006d0065006e007400f300770020005000440046002000700072007a0065007a006e00610063007a006f006e00790063006800200064006f002000770079006400720075006b00f30077002000770020007700790073006f006b00690065006a0020006a0061006b006f015b00630069002e002000200044006f006b0075006d0065006e0074007900200050004400460020006d006f017c006e00610020006f007400770069006500720061010700200077002000700072006f006700720061006d006900650020004100630072006f00620061007400200069002000410064006f00620065002000520065006100640065007200200035002e0030002000690020006e006f00770073007a0079006d002e>
    /PTB <FEFF005500740069006c0069007a006500200065007300730061007300200063006f006e00660069006700750072006100e700f50065007300200064006500200066006f0072006d00610020006100200063007200690061007200200064006f00630075006d0065006e0074006f0073002000410064006f0062006500200050004400460020006d00610069007300200061006400650071007500610064006f00730020007000610072006100200070007200e9002d0069006d0070007200650073007300f50065007300200064006500200061006c007400610020007100750061006c00690064006100640065002e0020004f007300200064006f00630075006d0065006e0074006f00730020005000440046002000630072006900610064006f007300200070006f00640065006d0020007300650072002000610062006500720074006f007300200063006f006d0020006f0020004100630072006f006200610074002000650020006f002000410064006f00620065002000520065006100640065007200200035002e0030002000650020007600650072007300f50065007300200070006f00730074006500720069006f007200650073002e>
    /RUM <FEFF005500740069006c0069007a00610163006900200061006300650073007400650020007300650074010300720069002000700065006e007400720075002000610020006300720065006100200064006f00630075006d0065006e00740065002000410064006f006200650020005000440046002000610064006500630076006100740065002000700065006e0074007200750020007400690070010300720069007200650061002000700072006500700072006500730073002000640065002000630061006c006900740061007400650020007300750070006500720069006f006100720103002e002000200044006f00630075006d0065006e00740065006c00650020005000440046002000630072006500610074006500200070006f00740020006600690020006400650073006300680069007300650020006300750020004100630072006f006200610074002c002000410064006f00620065002000520065006100640065007200200035002e00300020015f00690020007600650072007300690075006e0069006c006500200075006c0074006500720069006f006100720065002e>
    /RUS <FEFF04180441043f043e043b044c04370443043904420435002004340430043d043d044b04350020043d0430044104420440043e0439043a043800200434043b044f00200441043e043704340430043d0438044f00200434043e043a0443043c0435043d0442043e0432002000410064006f006200650020005000440046002c0020043c0430043a04410438043c0430043b044c043d043e0020043f043e04340445043e0434044f04490438044500200434043b044f00200432044b0441043e043a043e043a0430044704350441044204320435043d043d043e0433043e00200434043e043f0435044704300442043d043e0433043e00200432044b0432043e04340430002e002000200421043e043704340430043d043d044b04350020005000440046002d0434043e043a0443043c0435043d0442044b0020043c043e0436043d043e0020043e0442043a0440044b043204300442044c002004410020043f043e043c043e0449044c044e0020004100630072006f00620061007400200438002000410064006f00620065002000520065006100640065007200200035002e00300020043800200431043e043b043504350020043f043e04370434043d043804450020043204350440044104380439002e>
    /SKY <FEFF0054006900650074006f0020006e006100730074006100760065006e0069006100200070006f0075017e0069007400650020006e00610020007600790074007600e100720061006e0069006500200064006f006b0075006d0065006e0074006f0076002000410064006f006200650020005000440046002c0020006b0074006f007200e90020007300610020006e0061006a006c0065007001610069006500200068006f0064006900610020006e00610020006b00760061006c00690074006e00fa00200074006c0061010d00200061002000700072006500700072006500730073002e00200056007900740076006f00720065006e00e900200064006f006b0075006d0065006e007400790020005000440046002000620075006400650020006d006f017e006e00e90020006f00740076006f00720069016500200076002000700072006f006700720061006d006f006300680020004100630072006f00620061007400200061002000410064006f00620065002000520065006100640065007200200035002e0030002000610020006e006f0076016100ed00630068002e>
    /SLV <FEFF005400650020006e006100730074006100760069007400760065002000750070006f0072006100620069007400650020007a00610020007500730074007600610072006a0061006e006a006500200064006f006b0075006d0065006e0074006f0076002000410064006f006200650020005000440046002c0020006b006900200073006f0020006e0061006a007000720069006d00650072006e0065006a016100690020007a00610020006b0061006b006f0076006f00730074006e006f0020007400690073006b0061006e006a00650020007300200070007200690070007200610076006f0020006e00610020007400690073006b002e00200020005500730074007600610072006a0065006e006500200064006f006b0075006d0065006e0074006500200050004400460020006a00650020006d006f0067006f010d00650020006f0064007000720065007400690020007a0020004100630072006f00620061007400200069006e002000410064006f00620065002000520065006100640065007200200035002e003000200069006e0020006e006f00760065006a01610069006d002e>
    /SUO <FEFF004b00e40079007400e40020006e00e40069007400e4002000610073006500740075006b007300690061002c0020006b0075006e0020006c0075006f00740020006c00e400680069006e006e00e4002000760061006100740069007600610061006e0020007000610069006e006100740075006b00730065006e002000760061006c006d0069007300740065006c00750074007900f6006800f6006e00200073006f00700069007600690061002000410064006f0062006500200050004400460020002d0064006f006b0075006d0065006e007400740065006a0061002e0020004c0075006f0064007500740020005000440046002d0064006f006b0075006d0065006e00740069007400200076006f0069006400610061006e0020006100760061007400610020004100630072006f0062006100740069006c006c00610020006a0061002000410064006f00620065002000520065006100640065007200200035002e0030003a006c006c00610020006a006100200075007500640065006d006d0069006c006c0061002e>
    /SVE <FEFF0041006e007600e4006e00640020006400650020006800e4007200200069006e0073007400e4006c006c006e0069006e006700610072006e00610020006f006d002000640075002000760069006c006c00200073006b006100700061002000410064006f006200650020005000440046002d0064006f006b0075006d0065006e007400200073006f006d002000e400720020006c00e4006d0070006c0069006700610020006600f60072002000700072006500700072006500730073002d007500740073006b00720069006600740020006d006500640020006800f600670020006b00760061006c0069007400650074002e002000200053006b006100700061006400650020005000440046002d0064006f006b0075006d0065006e00740020006b0061006e002000f600700070006e00610073002000690020004100630072006f0062006100740020006f00630068002000410064006f00620065002000520065006100640065007200200035002e00300020006f00630068002000730065006e006100720065002e>
    /TUR <FEFF005900fc006b00730065006b0020006b0061006c006900740065006c0069002000f6006e002000790061007a006401310072006d00610020006200610073006b013100730131006e006100200065006e0020006900790069002000750079006100620069006c006500630065006b002000410064006f006200650020005000440046002000620065006c00670065006c0065007200690020006f006c0075015f007400750072006d0061006b0020006900e70069006e00200062007500200061007900610072006c0061007201310020006b0075006c006c0061006e0131006e002e00200020004f006c0075015f0074007500720075006c0061006e0020005000440046002000620065006c00670065006c0065007200690020004100630072006f006200610074002000760065002000410064006f00620065002000520065006100640065007200200035002e003000200076006500200073006f006e0072006100730131006e00640061006b00690020007300fc007200fc006d006c00650072006c00650020006100e70131006c006100620069006c00690072002e>
    /UKR <FEFF04120438043a043e0440043804410442043e043204430439044204350020044604560020043f043004400430043c043504420440043800200434043b044f0020044104420432043e04400435043d043d044f00200434043e043a0443043c0435043d044204560432002000410064006f006200650020005000440046002c0020044f043a04560020043d04300439043a04400430044904350020043f045604340445043e0434044f0442044c00200434043b044f0020043204380441043e043a043e044f043a04560441043d043e0433043e0020043f0435044004350434043404400443043a043e0432043e0433043e0020043404400443043a0443002e00200020042104420432043e04400435043d045600200434043e043a0443043c0435043d0442043800200050004400460020043c043e0436043d04300020043204560434043a0440043804420438002004430020004100630072006f006200610074002004420430002000410064006f00620065002000520065006100640065007200200035002e0030002004300431043e0020043f04560437043d04560448043e04570020043204350440044104560457002e>
    /ENU (Use these settings to create Adobe PDF documents best suited for high-quality prepress printing.  Created PDF documents can be opened with Acrobat and Adobe Reader 5.0 and later.)
  >>
  /Namespace [
    (Adobe)
    (Common)
    (1.0)
  ]
  /OtherNamespaces [
    <<
      /AsReaderSpreads false
      /CropImagesToFrames true
      /ErrorControl /WarnAndContinue
      /FlattenerIgnoreSpreadOverrides false
      /IncludeGuidesGrids false
      /IncludeNonPrinting false
      /IncludeSlug false
      /Namespace [
        (Adobe)
        (InDesign)
        (4.0)
      ]
      /OmitPlacedBitmaps false
      /OmitPlacedEPS false
      /OmitPlacedPDF false
      /SimulateOverprint /Legacy
    >>
    <<
      /AddBleedMarks false
      /AddColorBars false
      /AddCropMarks false
      /AddPageInfo false
      /AddRegMarks false
      /ConvertColors /ConvertToCMYK
      /DestinationProfileName ()
      /DestinationProfileSelector /DocumentCMYK
      /Downsample16BitImages true
      /FlattenerPreset <<
        /PresetSelector /MediumResolution
      >>
      /FormElements false
      /GenerateStructure false
      /IncludeBookmarks false
      /IncludeHyperlinks false
      /IncludeInteractive false
      /IncludeLayers false
      /IncludeProfiles false
      /MultimediaHandling /UseObjectSettings
      /Namespace [
        (Adobe)
        (CreativeSuite)
        (2.0)
      ]
      /PDFXOutputIntentProfileSelector /DocumentCMYK
      /PreserveEditing true
      /UntaggedCMYKHandling /LeaveUntagged
      /UntaggedRGBHandling /UseDocumentProfile
      /UseDocumentBleed false
    >>
  ]
>> setdistillerparams
<<
  /HWResolution [2400 2400]
  /PageSize [612.000 792.000]
>> setpagedevice

