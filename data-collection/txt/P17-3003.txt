



















































Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics


Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics- Student Research Workshop, pages 11–16
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-3003

Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics- Student Research Workshop, pages 11–16
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-3003

Bilingual Word Embeddings with Bucketed CNN for Parallel Sentence
Extraction

Jeenu Grover
IIT Kharagpur
India - 721302

groverjeenu@gmail.com

Pabitra Mitra
IIT Kharagpur
India - 721302

pabitra@cse.iitkgp.ernet.in

Abstract

We propose a novel model which can be
used to align the sentences of two differ-
ent languages using neural architectures.
First, we train our model to get the bilin-
gual word embeddings and then, we cre-
ate a similarity matrix between the words
of the two sentences. Because of differ-
ent lengths of the sentences involved, we
get a matrix of varying dimension. We dy-
namically pool the similarity matrix into a
matrix of fixed dimension and use Convo-
lutional Neural Network (CNN) to classify
the sentences as aligned or not. To further
improve upon this technique, we bucket
the sentence pairs to be classified into dif-
ferent groups and train CNN’s separately.
Our approach not only solves sentence
alignment problem but our model can be
regarded as a generic bag-of-words sim-
ilarity measure for monolingual or bilin-
gual corpora.

1 Introduction

Parallel bilingual corpora are very crucial for
building natural language processing systems, in-
cluding machine translation, word disambigua-
tion, and cross-language information retrieval.
Machine translation tasks need a lot of parallel
data for training purposes (Brown et al., 1993).
Sentence alignment between two parallel mono-
lingual corpora is used for getting the parallel
data. But the present sentence alignment al-
gorithms rely mainly on surface based proper-
ties of the sentence pairs and lexicon-based ap-
proaches (Varga et al., 2007; Moore, 2002; Gale
and Church, 1993). A little work has been done
using the neural network models for sentence
alignment. We propose a novel approach based

on neural networks for sentence alignment which
performs exceedingly well as compared to stan-
dard alignment techniques which can not capture
the semantics being conveyed by the text. Our
model uses distributed word-embeddings which
have been behind the success of many NLP ap-
plications in recent years. We then leverage upon
the use of CNNs (Zou et al., 2013; Kusner et al.,
2015; Norouzi et al., 2013) for capturing the word-
overlapping and word-ordering features in similar-
ity matrix for classification.

2 Model

The different aspects of our model are presented
below.

2.1 Bilingual word embeddings

Bilingual Word embeddings (Bengio and Corrado,
2015; Luong et al., 2015) are a representation of
the words of two languages in the same seman-
tic space. The three ways of getting the bilin-
gual word representations as mentioned by Lu-
ong et al. are: bilingual mapping (Mikolov et al.,
2013a), monolingual adaptation (Zou et al., 2013)
and bilingual training (AP et al., 2014; Pham et al.,
2015).

The semantic relatedness of two words across
different languages have been compared by using
translation dictionaries for the purpose of sentence
alignment tasks. But, this method of telling if the
two words across languages are synonyms or not,
is very discrete, because a word would be called
a synonym only if it is present in list of top k
synonyms of the other word. Words having a lit-
tle similarity with each other would be totally ig-
nored. Our approach intends to mitigate this dif-
ference by using bilingual word embeddings. Our
method is not discrete, because even if the word
is not present in the list of top synonyms for the

11

https://doi.org/10.18653/v1/P17-3003
https://doi.org/10.18653/v1/P17-3003


other, we would still get their similarity (albeit
low) using the bilingual embeddings.

In our paper, we would be using bilingual train-
ing for getting word representations as proposed
by (Luong et al., 2015). Bilingual training does
not depend on independently trained word repre-
sentation on monolingual corpora of either lan-
guage. Instead, we learn monolingual and bilin-
gual representations jointly on a parallel corpora.

2.2 Similarity Matrix
Two semantically similar sentences in same or dif-
ferent languages would use same or similar words
albeit with a change of order introduced by var-
ious factors like language grammar, change of
narrative, change of tense or even paraphrasing.
We assume that if a sentence pair has high word-
overlap, then it might be conveying the same se-
mantics. Here, we are ignoring the cases in which
same set of words may convey different seman-
tics because parallel corpora contains a few such
instances. Handling such instances would lead to
minor changes in precision or recall due to fewer
instances and is beyond the scope of this paper.
Moreover, none of the existing approaches to sen-
tence alignment deal with such cases.

To capture word-overlap, some alignment tech-
niques use measures like TF-IDF similarity (Fung
and Cheung, 2004) or even the basic Jaccard sim-
ilarity along with translation dictionaries. In our
approach, we generate a similarity matrix for each
sentence pair, where entries in rows are corre-
sponding to the words of the sentence of one lan-
guage in their order of occurrence. Same way,
columns denote the entries for the words of sen-
tence of second language. The entry S(i, j) of S
denotes a similarity measure between the words
wi of s1 and wj

′
of s2. If we find a word in the

sentence which is not present in the corresponding
vocabulary, we simply omit it. The different sim-
ilarity measures that we used include cosine simi-
larity and euclidean distance between the embed-
dings.

2.3 Bucketing and Dynamic Poling
Last step gave us the similarity matrix for the sen-
tences s1 and s2, but the size of the matrix is
variable owing to different sentence lengths. We
would pool this similarity matrix dynamically to
convert it into a matrix of fixed dimension. But
we were a bit skeptical of this step as even sen-
tences with very short and very long lengths would

be pooled to the same dimension. To overcome it,
we bucketed the sentences into different sentence
length ranges. Bucketing is done on the basis of
mean length of the two sentences in the pair to be
classified.

Thus, we trained different classifiers for each
range of the buckets. The main limitation of this
method is that the effective training data reduces
for each of the classifiers. This may degrade the
performance of the model as compared to a single
classifier which has all the data available for train-
ing. If parallel annotated data is available in abun-
dance, then this method would work better than a
single classifier.

To convert matrix to a fixed size representation,
we pool it dynamically to a matrix of fixed dimen-
sion as mentioned in Socher et al. We divide each
dimension of 2D matrix into dim chunks of

⌊
len
dim

⌋

size, where dim is the bucket size and len is the
length of dimension. If the length len of any di-
mension is lesser than dim, we duplicate the ma-
trix entries along that dimension till len becomes
greater than or equal to dim. If there are l left-
over entries where l = len − dim ∗

⌊
len
dim

⌋
, we

distribute them to the last l chunks. We do it for
both the dimensions.

Now, for a particular chunk, we can pool the
entries in it using methods like average, minimum
and maximum pooling. When we take cosine sim-
ilarity between words as the matrix entries, we
take max-pooling and with euclidean distance, we
take min-pooling. This is because we do not want
to fade the effect of two words with high similarity
entries in the same chunk and the mentioned pool-
ing methods for each similarity measure, take care
of it.

2.4 Convolution Neural Network

CNN’s have been found to be performing well
where we need to capture the spatial properties
of the input in the neural network (Krizhevsky
et al., 2012; Kim, 2014). The intention behind us-
ing CNN’s on matrix rather than training a simple
neural classifier on input of flattened data is that
the similarity matrices not only contain the simi-
larity scores between words but they also capture
the word-order. Thus a matching phrase would
appear as a high intensity diagonal streak in the
similarity matrix (Refer figure 4). A single 1D
vector would loose such visual word-ordering fea-
tures of the similarity matrix. We also report the

12



performance of a multilayer perceptron classifier
as compared to CNN’s, justifying our choice.

3 Experiments

3.1 Data
We performed our experiment on the sentence
alignment dataset (Zweigenbaum et al., 2016)
provided by shared task of 10th BUCC work-
shop1. We performed experiments on the English-
German (en-de) dataset. The dataset consists of
399,337 sentences of monolingual English Cor-
pora and 413,869 sentences of monolingual Ger-
man corpora. The gold alignment has been pro-
vided with the data. The gold alignment between
the two languages consists of 9580 matching sen-
tence pairs.

3.2 Sampling
As described above, the sizes of monolingual cor-
pora are large as compared to actual aligned sen-
tences. The total possible sentence pairs are 165
billion which contains just 9580 positive sentence
pairs, creating a huge class imbalance. It would
be difficult to train or even store 165 billion sen-
tence pairs. Moreover, large presence of negative
data in the corpora would also make the classifiers
less sensitive towards positive data. To overcome
these difficulties, we sampled negative examples
from the data. Table 1 gives the sizes of datasets
after sampling.

3.3 Training of Word Embeddings
We used the English German word embeddings
as mentioned in the paper by Luong et al.2 The
dimension of all embeddings used in our exper-
iments is 128. The embeddings are trained on
parallel Europarl v7 corpus between German (de)
and English (en) (Koehn, 2005), which consists of
1.9M parallel sentences. The training settings as
described in Mikolov et al. and Luong et al. were
used. The hyper-parameter α is 1 and β is 4 in our
experiments as described in Luong et al. The vo-
cabulary size for en is 40,054 and for de is 95,195.

3.4 Similarity Matrix and Bucketing
We split the sentence pairs in data into training,
validation and testing set having a ratio of 6:1:3

1https://comparable.limsi.fr/bucc2017/
bucc2017-task.html

2The code and pre-trained embeddings can be down-
loaded from http://stanford.edu/˜lmthang/
bivec

B size Train Valid Test Par Ratio
[0,5] 0 0 0 4431 0
(5,8] 76 15 43 10281 7.392

(8,10] 1024 189 469 15681 65.302
(10,12] 3561 630 1829 22281 159.822
(12, 15] 8585 1630 4409 34431 249.339
(15,18] 4862 867 2602 49281 98.659
(18,20] 452 78 222 60681 7.449
(20,25] 4 2 7 94431 0.0424
Total 18654 3411 9581 34431 541.779

Table 1: Table showing bucket-ranges and the
number of sentence pairs in train, validation and
test set. Number of parameters (Par) and Ra-
tio of Training data to Parameter size (Ratio =
Train ÷ Par, in 10−3 units) are shown for each
bucket

respectively, giving data splits of size 18654, 3411
and 9581 respectively. The bucketed data ranges
for each data split are shown in Table1. There were
no sentence pairs in our dataset with mean length
of pair below 5 or above 25. More over, the splits
for range (5,8] and (20,25] do not contain enough
data for training, so we exclude them from the ex-
periments. Thus, out of total 8 buckets in Table 1,
we ran the experiments for only 5 buckets as well
as all non-bucketed data. We report our results us-
ing cosine similarity, as it performed better than
using euclidean distance as similarity measure.

3.5 Model Parameters
The neural network architecture is described be-
low:

• Convolution Layer 1 with Relu Activation:
We used a 3D convolution volume of area 3×
3 and depth 12 on input of size dim×dim×1
where dim is the bucket size. The strides of
1 unit were used in each direction. The zero
padding was done to keep output height and
width same as input. The convolution layer
was followed by a layer of Rectified Linear
Units (Relu) to introduce the non-linearity in
the model

• Max-Pooling Layer 1: We used max pooling
on the output of the previous layer to reduce
its size by half. This was done using strides
of 2 units for both height and width.

• Convolution Layer 2 with Relu : It uses a 3D
volume of area 3 × 3 and depth 16. Rest all
properties are same as Convolution Layer 1.

• Max-Pooling Layer 2: It again reduces the

13



size of input by half using strides of 2 units
in both directions. The output of this layer
is flattened from 3D to 1D to pass it to next
layer as input.

• Fully Connected Layer 1 (FC1) with Relu:
This layer maps the flattened input to a hid-
den layer with 200 units. Relu Activation was
used here on the output of hidden units.

• Dropout Layer: We used a layer with dropout
probability of 0.2 to prevent over-fitting in the
model.

• Full Connected layer 2: This layer maps
the output of previous layer with 200 hidden
units into a single output, which is further
passed to sigmoid activation unit. The value
of the sigmoid unit is the predicted output.

The model was trained using 20 epochs with
batch-size of 5. The loss function is the mean
squared error between actual and predicted output.
The Adam optimizer (Kingma and Ba, 2014) was
used for stochastic optimization for backpropaga-
tion. The learning rate parameter (eta) is 0.0005.
The motivation behind using Relu as activation
function is to overcome gradient decays, which
hinder the training of the neural networks. All the
hyper-parameters were tuned by random search in
hyper-parameter space and testing on the valida-
tion dataset. The total number of parameters to
be trained in the model are 150 × dim2 + 681,
where dim is the bucket size. This gives us num-
ber of parameters ranging from 15,681 for lowest
size bucket with dim = 10 and 60,681 for largest
bucket with dim = 20 .

4 Results and Analysis

To evaluate, we ran our algorithm on the en-de Test
set mentioned beforehand. Our algorithm assigns
a score to each sentence pair, denoting the proba-
bility of two sentences conveying same semantics.
If the score is greater than a certain threshold th,
we take it as a positive. Table 2 shows results for
th = 0.5. When we performed experiments for
all data (Total in Table 2), without bucketing, we
chose dim = 15 as highest number of data entries
fall in that bucket. We expected that bucketing
data would yield better results compared to non-
bucketing as each sentence pair would be pooled
to a matrix of the dimension comparable to its

T Data TP FP FN P R F1
(8,10] 128 2 3 .9846 .9771 .9808

(10,12] 472 6 34 .9874 .9328 .9593
(12, 15] 1207 63 13 .9504 .9893 .9695
(15,18] 904 4 22 .9956 .9762 .9858
(18,20] 67 0 6 1.0000 .9178 .9571
Total* 2825 18 49 .9937 .9830 .9883
Macro - - - .9836 .9586 .9710
Micro 2778 75 78 .9737 .9726 .9731

Baseline 2221 92 653 .9603 .7728 .8564

Table 2: Table showing Test dataset type
(T Data), True Positives (TP), False Positives
(FP), False Negatives (FN), Precision (P), Recall
(R), and F1-score (F1).*Includes all non-bucketed
data

actual length. But as seen in Table 2, the non-
bucketed approach performs very well and in some
cases, even better than a few buckets. This hap-
pens because the training data available for non-
bucketed approach (18654 pairs) is atleast double
of any of the buckets. Ratio column in Table 1
shows the ratio of Training data to number of pa-
rameters in the model, which is highest for non-
bucketed data with dim = 15. If we had more
training data in each bucket, then all the buckets
might have achieved better performance than non-
bucketed approach. Macro and Micro in Table 2
denote the Macro-average and Micro-average re-
spectively for all the 5 buckets taken for experi-
ment.

Figure 1: Precision-Recall Curve for all the buck-
ets as well as total data. The different dimensions
have been zoomed appropriately to show relevant
parts of the plot.

We also used a multilayer perceptron classifier
on non-bucketed data with flattened matrices as
input (Baseline in Table 2), but that performed
poorly with F1 score of 0.8564. Figure 2 and 3 de-
pict the similarity matrices for True Positives and
True Negatives. We can clearly observe some vi-

14



sual features of the similarity matrices, such as the
presence of high intensity streaks along diagonal,
which denote high similarity between the entries
in close vicinity in one sentence to the entries in
close vicinity in the other. This justifies our hy-
pothesis that, unlike multilayer perceptron, CNN
is able to capture the relations between word sim-
ilarity and their ordering, which is represented by
the matrices. Our method also performs much bet-
ter than other baseline methods such as a classifier
using sentence length features.

Figure 2: True Positive examples. Left image
shows a sentence pair with high overlap and right
image shows a sentence pair with low overlap.

Figure 3: True Negative examples. Both the im-
ages are visually different from True Positives.

As an abstraction, our model can be viewed as a
neural equivalent of bag-of-words similarity mea-
sures such as TF-IDF similarity or Jaccard sim-
ilarity as this approach covers the word-overlap
between two documents (sentences here). More-
over, rather just capturing the overlap, it also cap-
tures the order in which the words match in two
documents. So, it can be dubbed as neural-bag-
of-words like model which remembers matching
order.

5 Future Work

We have used the dynamically pooled similarity
matrices with CNN for the purpose of sentence
alignment. But as already mentioned, our ap-
proach specifies a general way of obtaining the
similarity between two texts whether they belong
to same or different languages. The sentences be-
longing to the same language can be handled in

the same way, but only monolingual word embed-
dings would suffice for that purpose.

A unique feature of our similarity measure is
that we get the similarity between two texts with-
out mapping them to their respective vectors in the
vector space. We can interpret it like a kernel func-
tion which gives dot product φ(x).φ(y) between
two entities without actually transforming x or y
to φ(x) or φ(y) respectively. Also, unlike TF-IDF,
where each vector is of the size of the vocabulary,
our similarity approach takes only dim2 space per
sentence pair, which is much lesser than the for-
mer.

Our current approach assumes that the two texts
are of comparable length, because that is generally
the case for aligned sentences and that’s why we
took the dynamically pooled matrices with both
dimensions of same size. But, if we want to use
our approach for information retrieval purposes,
then the size of documentD would be much larger
than size of the query q. In that case, we would
have to take rectangular dynamically pooled ma-
trices with appropriate dimensions. We would like
to study the efficacy of our approach in all such
scenarios.

Also, since our approach can tell the parts of
the document it is matching, unlike TF-IDF, we
can use it to assign different scores for matching
phrases in different parts of the document. For ex-
ample, to search for a query on a webpage which
has an article and comments from the readers (just
like a blog), our approach can be trained to give
more importance to the matches in the article as
compared to the reader comments, thus leading
to a better information retrieval approach. In the
future, we would also like to study how different
properties like time and space complexity of our
approach scale for large dataset. We would also
like to explore the applications of our approach for
tasks like cross-lingual as well as monolingual in-
formation retrieval, query expansion, cross-lingual
recommender systems etc.

6 Conclusion

The novelty of our approach lies in using neu-
ral word embeddings in bilingual semantic space
along with CNN to capture the sentence similar-
ity and we have achieved very good results over
the dataset by BUCC. Our model provides a new
equivalent of bag-of-words similarity measures
which is also aware of the matching order. The

15



architecture proposed by our algorithm is not just
for this specific task but can be used for a number
of other tasks like Information Retrieval in mono-
lingual as well as bilingual corpora, query expan-
sion for cross-lingual search etc. We would like to
study the different properties and explore the ap-
plications of our approach in future.

7 Acknowledgements

We would like to thank all the anonymous review-
ers for their valuable feedback.

References
Sarath Chandar AP, Stanislas Lauly, Hugo Larochelle,

Mitesh Khapra, Balaraman Ravindran, Vikas C
Raykar, and Amrita Saha. 2014. An autoencoder
approach to learning bilingual word representations.
In Advances in Neural Information Processing Sys-
tems. pages 1853–1861.

Yoshua Bengio and Greg Corrado. 2015. Bilbowa:
Fast bilingual distributed representations without
word alignments .

Peter F Brown, Vincent J Della Pietra, Stephen A Della
Pietra, and Robert L Mercer. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
mation. Computational linguistics 19(2):263–311.

Pascale Fung and Percy Cheung. 2004. Mining very-
non-parallel corpora: Parallel sentence and lexicon
extraction via bootstrapping and e. In EMNLP. Cite-
seer, pages 57–63.

William A Gale and Kenneth W Church. 1993. A
program for aligning sentences in bilingual corpora.
Computational linguistics 19(1):75–102.

Yoon Kim. 2014. Convolutional neural net-
works for sentence classification. arXiv preprint
arXiv:1408.5882 .

Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980 .

Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT summit. vol-
ume 5, pages 79–86.

Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hin-
ton. 2012. Imagenet classification with deep con-
volutional neural networks. In Advances in neural
information processing systems. pages 1097–1105.

Matt J Kusner, Yu Sun, Nicholas I Kolkin, Kilian Q
Weinberger, et al. 2015. From word embeddings
to document distances. In ICML. volume 15, pages
957–966.

Thang Luong, Hieu Pham, and Christopher D Man-
ning. 2015. Bilingual word representations with
monolingual quality in mind. In Proceedings of the
1st Workshop on Vector Space Modeling for Natural
Language Processing. pages 151–159.

Tomas Mikolov, Quoc V Le, and Ilya Sutskever. 2013a.
Exploiting similarities among languages for ma-
chine translation. arXiv preprint arXiv:1309.4168
.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in neural information processing
systems. pages 3111–3119.

Robert C Moore. 2002. Fast and accurate sentence
alignment of bilingual corpora. In Conference of the
Association for Machine Translation in the Ameri-
cas. Springer, pages 135–144.

Mohammad Norouzi, Tomas Mikolov, Samy Bengio,
Yoram Singer, Jonathon Shlens, Andrea Frome,
Greg S Corrado, and Jeffrey Dean. 2013. Zero-shot
learning by convex combination of semantic embed-
dings. arXiv preprint arXiv:1312.5650 .

Hieu Pham, Minh-Thang Luong, and Christopher D
Manning. 2015. Learning distributed representa-
tions for multilingual text sequences. In Proceed-
ings of NAACL-HLT . pages 88–94.

Richard Socher, Eric H Huang, Jeffrey Pennington,
Andrew Y Ng, and Christopher D Manning. 2011.
Dynamic pooling and unfolding recursive autoen-
coders for paraphrase detection. In NIPS. vol-
ume 24, pages 801–809.

Dániel Varga, Péter Halácsy, András Kornai, Vik-
tor Nagy, László Németh, and Viktor Trón. 2007.
Parallel corpora for medium density languages.
AMSTERDAM STUDIES IN THE THEORY AND
HISTORY OF LINGUISTIC SCIENCE SERIES 4
292:247.

Will Y Zou, Richard Socher, Daniel M Cer, and
Christopher D Manning. 2013. Bilingual word em-
beddings for phrase-based machine translation. In
EMNLP. pages 1393–1398.

Pierre Zweigenbaum, Serge Sharoff, and Reinhard
Rapp. 2016. Towards preparation of the second bucc
shared task: Detecting parallel sentences in compa-
rable corpora. In Ninth Workshop on Building and
Using Comparable Corpora. page 38.

16


	Bilingual Word Embeddings with Bucketed CNN for Parallel Sentence Extraction

