516

Coling 2010: Poster Volume, pages 516–524,

Beijing, August 2010

Effective Constituent Projection across Languages

Wenbin Jiang and Yajuan L¨u and Yang Liu and Qun Liu

Key Laboratory of Intelligent Information Processing

Institute of Computing Technology

Chinese Academy of Sciences

{jiangwenbin, lvyajuan, yliu, liuqun}@ict.ac.cn

Abstract

We describe an effective constituent pro-
jection strategy, where constituent pro-
jection is performed on the basis of de-
pendency projection. Especially, a novel
measurement is proposed to evaluate the
candidate projected constituents for a tar-
get language sentence, and a PCFG-style
parsing procedure is then used to search
for
the most probable projected con-
stituent tree. Experiments show that, the
parser trained on the projected treebank
can signiﬁcantly boost a state-of-the-art
supervised parser. When integrated into a
tree-based machine translation system, the
projected parser leads to translation per-
formance comparable with using a super-
vised parser trained on thousands of anno-
tated trees.

1 Introduction

In recent years, supervised constituent parsing has
been well studied and achieves the state-of-the-art
for many resource-rich languages (Collins, 1999;
Charniak, 2000; Petrov et al., 2006). Because
of the cost and difﬁculty in treebank construc-
tion, researchers have also investigated the utiliza-
tion of unannotated text, including the unsuper-
vised parsing which totally uses unannotated data
(Klein and Manning, 2002; Klein and Manning,
2004; Bod, 2006; Seginer, 2007), and the semi-
supervised parsing which uses both annotated and
unannotated data (Sarkar, 2001; Steedman et al.,
2003; McClosky et al., 2006).

Because of the higher complexity and lower
performance of unsupervised methods, as well as

the need of reliable priori knowledge in semi-
supervised methods, it seems promising to project
the syntax structures from a resource-rich lan-
guage to a resource-scarce one across a bilingual
corpus. Lots of researches have so far been de-
voted to dependency projection (Hwa et al., 2002;
Hwa et al., 2005; Ganchev et al., 2009; Smith
and Eisner, 2009). While for constituent projec-
tion there is few progress. This is due to the fact
that the constituent syntax describes the language
structure in a more detailed way, and the degree of
isomorphism between constituent structures ap-
pears much lower.

In this paper we propose for constituent pro-
jection a stepwise but totally automatic strategy,
which performs constituent projection on the ba-
sis of dependency projection, and then use a con-
straint EM optimization algorithm to optimized
the initially projected trees. Given a word-aligned
bilingual corpus with source sentences parsed, we
ﬁrst project the dependency structures of these
constituent trees to the target sentences using a
dynamic programming algorithm, then we gener-
ate a set of candidate constituents for each target
sentence and design a novel evaluation function
to calculate the probability of each candidate con-
stituent, ﬁnally, we develop a PCFG-style parsing
procedure to search for the most probable pro-
jected constituent tree in the evaluated candidate
constituent set. In addition, we design a constraint
EM optimization procedure to decrease the noise
in the initially projected constituent treebank.

Experimental results validate the effectiveness
of our approach. On the Chinese-English FBIS
corpus, we project the English parses produced
by the Charniak parser across to the Chinese sen-

517

tences. A berkeley parser trained on this pro-
jected treebank can effectively boost the super-
vised parsers trained on bunches of CTB trees.
Especially, the supervised parser trained on the
smaller CTB 1.0 beneﬁts a signiﬁcant F-measure
increment of more than 1 point from the projected
parser. When using the projected parser in a tree-
based translation model
(Liu et al., 2006), we
achieve translation performance comparable with
using a state-of-the-art supervised parser trained
on thousands of CTB trees. This surprising re-
sult gives us an inspiration that better translation
would be achieved by combining both projected
parsing and supervised parsing into a hybrid pars-
ing schema.

2 Stepwise Constituent Projection

We ﬁrst introduce the dynamic programming pro-
cedure for dependency projection, then describe
the PCFG-style algorithm for constituent projec-
tion which is conducted on projected dependent
structures, and ﬁnally show the constraint EM
procedure for constituent optimization.

2.1 Dependency Projection

For dependency projection we adopt a dynamic
programming algorithm, which searches the most
probable projected target dependency structure
according to the source dependency structure and
the word alignment.

In order to mitigate the effect of word alignment
errors, multiple GIZA++ (Och and Ney, 2000) re-
sults are combined into a compact representation
called alignment matrix. Given a source sentence
with m words, represented as E1:m, and a target
sentence with n words, represented as F1:n, their
word alignment matrix A is an m × n matrix,
where each element Ai,j denotes the probability
of the source word Ei aligned to the target word
Fj.

Using P (DF|DE, A) to denote the probability
of the projected target dependency structure DF
conditioned on the source dependency structure
DE and the alignment matrix A, the projection al-
gorithm aims to ﬁnd

˜DF = argmax

DF

P (DF|DE, A)

(1)

for l ∈ V[i, k] and r ∈ V[k + 1, j] do

buf ← ∅
for k ← i..j − 1 do

Algorithm 1 Dependency projection.
1: Input: F , and Pe for all word pairs in F
2: for hi, ji ⊆ h1,|F|i in topological order do
3:
4:
5:
6:
7:
8:
V[i, j] ← top K derivations of buf
9: Output: the best derivation of V[1,|F|]
10: function DERIV(p, c, Pe)
11:
12:
13:

insert DERIV(l, r, Pe) into buf
insert DERIV(r, l, Pe) into buf

d ← p ∪ c ∪ {p · root y c · root} ⊲ new derivation
⊲ evaluation function
d · evl ← EVAL(d, Pe)
return d

⊲ all partitions

P (DF|DE , A) can be factorized into each depen-
dency edge x y y in DF

P (DF|DE, A) = Yxyy∈DF

Pe(x y y|DE, A)

Pe can then be obtained by simple accumulation
across all possible situations of correspondence
Pe(x y y|DE, A)
= X1≤x′,y′≤|E|

Ax,x′ × Ay,y′ × δ(x′, y′|DE)

where δ(x′, y′|DE) is a 0-1 function that equals
1 only if the dependent relation x′ y y′ holds in
DE.

The search procedure needed by the argmax op-
eration in equation 1 can be effectively solved
by the Chu-Liu-Edmonds algorithm used in (Mc-
Donald et al., 2005). In this work, however, we
adopt a more general and simple dynamic pro-
gramming algorithm as shown in Algorithm 1,
in order to facilitate the possible expansions. In
practice, the cube-pruning strategy (Huang and
Chiang, 2005) is used to speed up the enumera-
tion of derivations (loops started by line 4 and 5).

2.2 Constituent Projection
The PCFG-style parsing procedure searches for
the most probable projected constituent tree in
a shrunken search space determined by the pro-
jected dependency structure and the target con-
stituent tree. The shrunken search space can be
built as following. First, we generates the candi-
date constituents of the source tree and the can-
didate spans of the target sentence, so as to enu-
merate the candidate constituents of the target sen-
tence. Then we compute the consistent degree for

518

each pair of candidate constituent and span, and
further estimate the probability of each candidate
constituent for the target sentence.

2.2.1 Candidate Constituents and Spans

For the candidate constituents of the source
tree, using only the original constituents imposes
a strong hypothesis of isomorphism on the con-
stituent projection between two languages, since
it requires that each couple of constituent and span
must be strictly matched. While for the candi-
date spans of the target sentences, using all sub-
sequences makes the search procedure suffer from
more perplexity. Therefore, we expand the candi-
date constituent set and restrict the candidate span
set:

• Candidate Constituent: Suppose a produc-
tion in the source constituent tree, denoted as
p → c1c2..ch..c|p|, and ch is the head child
of the parent p. Each constituent, p or c, is a
triple hlb, rb, nti, where nt denotes its non-
terminal, while lb and rb represent its left-
and right bounds of the sub-sequence that the
constituent covers. The candidate constituent
set of this production consists the head of
the production itself, and a set of incomplete
constituents,
{hl, r, p · nt∗i|c1 · lb ≤ l ≤ ch · lb∧

ch · rb ≤ r ≤ c|p| · rb∧
(l < ch · lb ∨ r > ch · rb)}
where the symbol ∗ indicates an incomplete
non-terminal. The candidate constituent set
of the entire source tree is the uniﬁcation of
the sets extracted from all productions of the
tree.

• Candidate Span: A candidate span of the tar-
get sentence is a tuple hlb, rbi, where lb and
rb indicate the same as in a constituent. We
deﬁne the candidate span set as the spans of
all regular dependent segments in the corre-
sponding projected dependency structure. A
regular dependency segment is a dependent
segment that every modiﬁer of the root is a
complete dependency structure. Suppose a
dependency structure rooted at word p, de-
noted as clL..cl2cl1 x p y cr1cr2..crR, it

has L (L ≥ 0) modiﬁers on its left and R
(R ≥ 0) modiﬁers on its right, each of them
is a smaller complete dependency structure.
Then the word p itself is a regular depen-
dency segment without any modiﬁer, and

{cli..cl1 x p y cr1..crj|0 ≤ i ≤ L∧
0 ≤ j ≤ R∧
(i > 0 ∨ j > 0)}
is a set of regular dependency structures with
at least one modiﬁer. The regular depen-
dency segments of the entire projected de-
pendency structure can simply be accumu-
lated across all dependency nodes.

2.2.2 Span-to-Constituent Correspondence

After determining the candidate constituent set
of the source tree, denoted as ΦE, and the can-
didate span set of the target sentence, denoted as
ΨF , we then calculate the consistent degree for
each pair of candidate constituent and candidate
span.

Given a candidate constituent φ ∈ ΦE and a
candidate span ψ ∈ ΨF , their consistent degree
C(ψ, φ|A) is the probability that they are aligned
to each other according to A.
We display the derivations from bottom to up.
First, we deﬁne the alignment probability from a
word i in the span ψ to the constituent φ as

P (i 7→ φ|A) = Pφ·lb≤j≤φ·rb Ai,j

Pj Ai,j

Then we deﬁne the alignment probability from the
span ψ to the constituent φ as

P (ψ 7→ φ|A) = Yψ·lb≤i≤ψ·rb

P (i 7→ φ|A)

Note that we use i to denote both a word and its in-
dex for simplicity without causing confusion. Fi-
nally, we deﬁne C(φ, ψ|A) as
C(ψ, φ|A) = P (ψ 7→ φ|A) × P (φ 7→ ψ|AT ) (2)
Where P (φ 7→ ψ|AT ) denotes the alignment
probability from the constituent φ to the span ψ, it
can be calculated in the same manner.

519

2.2.3 Constituent Projection Algorithm

The purpose of constituent projection is to ﬁnd
the most probable projected constituent tree for
the target sentence conditioned on the source con-
stituent tree and the word alignment

˜TF = argmax
TF ⊆ΦF

P (TF|TE, A)

(3)

Here, we use ΦF to denote the set of candidate
constituents of the target sentence

ΦF = ΨF ⊗ N T (ΦE)

= {φF|ψ(φF ) ∈ ΨF ∧ nt(φF ) ∈ N T (ΦE)}
where ψ(·) and nt(·) represent the span and the
non-terminal of a constituent respectively, and
N T (·) represents the set of non-terminals ex-
tracted from a constituent set. Note that TF is a
subset of ΦF if we treat a tree as a set of con-
stituents.

The probability of the projected tree TF can be
factorized into the probabilities of the projected
constituents that composes the tree

P (TF|TE, A) = YφF ∈TF

Pφ(φF|TE, A)

while the probability of the projected source con-
stituent can be deﬁned as a statistics of span-to-
constituent- and constituent-to-constituent consis-
tent degrees

Pφ(φF|TE, A) = PφE∈ΦE C(φF , φE|A)
PφE∈ΦE C(ψ(φF ), φE|A)

where C(φF , φE|A) in the numerator denotes the
consistent degree for each pair of constituents,
which can be calculated based on that of span and
constituent described in Formula 2

C(φF , φE) =(cid:26) 0

if φF · nt 6= φE · nt
else

C(ψ(φF ), φE )

Algorithm 2 shows the pseudocode for con-
stituent projection. A PCFG-style parsing pro-
cedure searches for the best projected constituent
tree in the constrained space determined by ΨF .
Note that the projected trees are binarized, and can
be easily recovered according to the asterisks at
the tails of non-terminals.

for k ← i..j − 1 do

Algorithm 2 Constituent projection.
1: Input: ΨF , ΦF , and Pφ for all spans in ΨF
2: for hi, ji ∈ Ψ in topological order do
3:
buf ← ∅
for p ∈ ΦF s.t. ψ(p) = hi, ji do
4:
5:
6:
7:
8:
V[i, j] ← top K derivations of buf
9: Output: the best derivation of V[1,|F|]
10: function DERIV(l, r, p, Pφ)
11:
12:
13:

d ← l ∪ r ∪ {p}
d · evl ← EVAL(d, Pφ)
return d

for l ∈ V[i, k] and r ∈ V[k + 1, j] do
insert DERIV(l, r, p, Pφ) into buf

⊲ new derivation
⊲ evaluation function

⊲ all partitions

2.3 EM Optimization

Since the constituent projection is conducted on
each sentence pair separately, the projected tree-
bank is apt to suffer from more noise caused by
free translation and word alignment error. It can
be expected that an EM iteration over the whole
projected treebank will lead to trees with higher
consistence.

We adopt the inside-outside algorithm to im-
prove the quality of the initially projected tree-
bank. Different from previous works, all expecta-
tion and maximization operations for a single tree
are performed in a constrained space determined
by the candidate span set of the projected target
dependency structure. That is to say, all the sum-
mation operations, both for calculating α/β values
and for re-estimating the rule probabilities, only
consider the spans in the candidate span set. This
means that the projected dependency structures
are supposed believable, and the noise is mainly
introduced in the following constituent projection
procedure.

Here we give an overall description of the tree-
First, an initial
bank optimization procedure.
PCFG grammar G0
F is estimated from the original
projected treebank. Then several iterations of α/β
calculation and rule probability re-estimation are
performed. For example in the i-the iteration, α/β
values are calculated based on the current gram-
mar Gi−1
F , afterwards the optimized grammar Gi
F
is obtained based on these α/β values. The itera-
tive procedure terminates when the likelihood of
whole treebank increases slowly. Finally, with the
optimized grammar, a constrained PCFG parsing
procedure is conducted on each of the initial pro-

520

jected trees, so as to obtain an optimized treebank.

3 Applications of Constituent Projection

The most direct contribution of constituent pro-
jection is pushing an initial step for the statis-
tical constituent parsing of resource-scarce lan-
guages.
It also has some meaningful applica-
tions even for the resource-rich languages. For
instances, the projected treebank, due to its large
scale and high coverage, can used to boost an tra-
ditional supervised-trained parser. And, the parser
trained on the projected treebank can adopted to
conduct tree-to-string machine translation, since
it give parsing results with larger isomorphism
with the target language than a supervised-trained
parser dose.

3.1 Boost an Traditional Parser
We ﬁrst establish a uniﬁed framework for the en-
hanced parser where a projected parser is adopted
to guide the parsing procedure of the baseline
parser.

For a given target sentence S, the enhanced
parser selected the best parse ˜T among the set
of candidates Ω(S) according to two evaluation
functions, given by the baseline parser B and the
projected guide parser G, respectively.
P (T|B) × P (T|G)λ

(4)

˜T = argmax
T∈Ω(S)

These two evaluation functions can be integrated
deeply into the decoding procedure (Carreras et
al., 2008; Zhang and Clark, 2008; Huang, 2008),
or can be integrated at a shallow level in a rerank-
ing manner
(Collins, 2000; Charniak and John-
son, 2005). For simplicity and generability, we
adopt the reranking strategy. In k-best reranking,
Ω(S) is simply a set of candidate parses, denoted
as {T1, T2, ..., Tk}, and we use the single parse of
the guide parser, TG, to re-evaluate these candi-
dates. Formula 4 can be redeﬁned as

˜T (TG) = argmax
T∈Ω(S)

w · f (T, TG)

(5)

Here, f (T, TG) and w represent a high dimen-
sional feature representation and a correspond-
ing weight vector, respectively. The ﬁrst feature
f1(T, TG) = logP (T|B) is the log probability

of the baseline parser, while the remaining fea-
tures are integer-valued guide features, and each
of them represents the guider parser’s predication
result for a particular conﬁguration in candidate
parse T , so as to utilize the projected parser’s
knowledge to guide the parsing procedure of the
traditional parser.

In our work a guide feature is composed of two
parts, the non-terminal of a certain constituent φ
in the candidate parse T ,1 and the non-terminal
at the corresponding span ψ(φ) in the projected
parse TG. Note that in the projected parse this
span does not necessarily correspond to a con-
stituent.
In such situations, we simply use the
non-terminal of the constituent that just be able
to cover this span, and attach a asterisk at the tail
of this non-terminal. Here is an example of the
guide features

f100(T, TG) = V P ∈ T ◦ P P∗ ∈ TG

It represents that a V P in the candidate parse cor-
responds to a segment of a P P in the projected
parse. The quantity of its weight w100 indicates
how probably a span can be predicated as V P if
the span corresponds to a partial P P in the pro-
jected parse.
the perceptron algorithm to train
We adopt
the reranker.
To reduce overﬁtting and pro-
duce a more stable weight vector, we also use
a reﬁnement strategy called averaged parameters
(Collins, 2002).

3.2 Using in Machine Translation
Researchers have achieved promising improve-
ments in tree-based machine translation (Liu et
al., 2006; Huang et al., 2006). Such models use
a parsed tree as input and converts it into a target
tree or string. Given a source language sentence,
ﬁrst we use a traditional source language parser
to parse the sentence to obtain the syntax tree T ,
and then use the translation decoder to search for
the best derivation ˜d, where a derivation d is a se-
quence of transformations that converts the source
tree into the target language string
P (d|T )

˜d = argmax

(6)

d∈D

1Using non-terminals as features brings no improvement
in the reranking experiments, so as to examine the impact of
the projected parser.

521

Here D is the candidate set of d, and it is deter-
mined by the source tree T and the transformation
rules.

Since the tree-based models are based on
the synchronous transformational grammars, they
suffer much from the isomerism between the
source syntax and the target sentence structure.
Considering that the parsed tree produced by a
projected parser may have larger isomorphism
with the target language, it would be a promis-
ing idea to adopt the projected parser to parse the
input sentence for the subsequent translation de-
coding procedure.

4 Experiments

In this section, we ﬁrst invalidate the effect of con-
stituent projection by evaluating a parser trained
on the projected treebank. Then we investigate
two applications of the projected parser: boosting
an traditional supervised-trained parser, and inte-
gration in a tree-based machine translation sys-
tem. Following the previous works, we depict the
parsing performance by F-score on sentences with
no more than 40 words, and evaluate the transla-
tion quality by the case-sensitive BLEU-4 metric
(Papineni et al., 2002) with 4 references.

4.1 Constituent Projection
We perform constituent projection from English
to Chinese on the FBIS corpus, which contains
239K sentence pairs with about 6.9M/8.9M words
in Chinese/English. The English sentences are
parsed by the Charniak Parser and the dependency
structures are extracted from these parses accord-
ing to the head-ﬁnding rules of
(Yamada and
Matsumoto, 2003). The word alignment matrixes
are obtained by combining the 10-best results of
GIZA++ according to (Liu et al., 2009).

We ﬁrst project the dependency structures from
English to Chinese according to section 2.1, and
then project the constituent structures according
to section 2.2. We deﬁne an assessment criteria
to evaluate the conﬁdence of the ﬁnal projected
constituent tree

c = npP (DF|DE, A) × P (TF|TE, A)

where n is the word count of a Chinese sentence
in our experiments. A series of projected Chi-

Thres c
0.5
0.4
0.3
0.2
0.1

#Resrv Cons-F1
12.6K
17.8K
27.2K
45.1K
87.0K

23.9
23.9
25.4
26.6
27.8

Span-F1

32.7
33.4
35.7
38.0
40.4

Table 1: Performances of the projected parsers
on the CTB test set. #Resrv denotes the amount
of reserved trees within threshold c. Cons-F1 is
the traditional F-measure, while Span-F1 is the F-
measure without consideration of non-terminals.

nese treebanks with different scales are obtained
by specifying different c as the ﬁltering threshold.
The state-of-the-art Berkeley Parser is adopted to
train on these treebanks because of its high per-
formance and independence of head word infor-
mation.

Table 1 shows the performances of these pro-
jected parsers on the standard CTB test set, which
is composed of sentences in chapters 271-300.
We ﬁnd that along with the decrease of the ﬁlter-
ing threshold c, more projected trees are reserved
and the performance of the projected parser con-
stantly increases. We also ﬁnd that the traditional
F-value, Cons-F1, is obviously lower than the one
without considering non-terminals, Span-F1. This
indicates that the constituent projection procedure
introduces more noise because of the higher com-
plexity of constituent correspondence. In all the
rest experiments, however, we simply use the pro-
jected treebank ﬁltered by threshold c = 0.1 and
do not try any smaller thresholds, since it already
takes more than one weak to train the Berkeley
Parser on the 87 thousands trees resulted by this
threshold.

The constrained EM optimization procedure
described in section 2.3 is used to alleviate the
noise in the projected treebank, which may be
caused by free translation, word alignment errors,
and projection on each single sentence pair. Fig-
ure 1 shows the log-likelihood on the projected
treebank after each EM iteration. It is obvious that
the log-likelihood increases very slowly after 10
iterations. We terminate the EM procedure after
40 iterations.

Finally we train the Berkeley Parser on the op-
timized projected treebank, and test its perfor-

522

d
o
o
h

i
l

e
k

i
l
-
g
o
L

-58
-59
-60
-61
-62
-63
-64
-65

 88
 86
 84
 82
 80
 78
 76
 74
 72
 70

)

%

(
 
e
r
o
c
s
-
F

 
l
a
v
e
s
r
a
P

CTB 5.0

CTB 1.0

baseline
boosted parser

 0

 5  10  15  20  25  30  35  40

 1000

 10000

EM iteration

Scale of treebank (log)

Figure 1: Log-likelihood of the 87K-projected
treebank after each EM interation.

Train Set
Original 87K
Optimized 87K

Cons-F1

27.8
22.8

Span-F1

40.4
40.2

Table 2: Performance of the parser trained on the
optimized projected treebank, compared with that
of the original projected parser.

Train Set Baseline Bst-Ini Bst-Opt
CTB 1.0
CTB 5.0

75.6
85.2

76.4
85.5

76.9
85.7

Table 3: Performance improvement brought by
the projected parser to the baseline parsers trained
on CTB 1.0 and CTB 5.0, respectively. Bst-
Ini/Bst-Opt: boosted by the parser trained on the
initial/optimized projected treebank.

mance on the standard CTB test set. Table 2
shows the performance of the parser trained on
the optimized projected treebank. Unexpectedly,
we ﬁnd that the constituent F1-value of the parser
trained on the optimized treebank drops sharply
from the baseline, although the span F1-value re-
mains nearly the same. We assume that the EM
procedure gives the original projected treebank
more consistency between each single tree while
the revised treebank deviates from the CTB anno-
tation standard, but it needs to be validated by the
following experiments.

4.2 Boost an Traditional Parser

The projected parser is used to help the reranking
of the k-best parses produced by another state-of-
the-art parser, which is called the baseline parser
In our experiments we choose
for convenience.
the revised Chinese parser
(Xiong et al., 2005)

Figure 2: Boosting performance of the projected
parser on a series of baseline parsers that are
trained on treebanks of different scales.

based on Collins model 2 (Collins, 1999) as the
baseline parser.2

The baseline parser is respectively trained on
CTB 1.0 and CTB 5.0. For both corpora we
follow the traditional corpus splitting: chapters
271-300 for testing, chapters 301-325 for devel-
opment, and else for training. Experimental re-
sults are shown in Table 3. We ﬁnd that both
projected parsers bring signiﬁcant improvement to
the baseline parsers. Especially the later, although
performs worse on CTB standard test set, gives a
larger improvement than the former. This to some
degree conﬁrms the previous assumption. How-
ever, more investigation must be conducted in the
future.

We also observe that for the baseline parser
trained on the much larger CTB 5.0, the boost-
ing performance of the projected parser is rela-
tively lower. To further investigate the regularity
that the boosting performance changes according
to the scale of training treebank of the baseline
parser, we train a series of baseline parsers with
different amounts of trees, then use the projected
parser trained on the optimized treebank to en-
hance these baseline parsers. Figure 2 shows the
experimental results. From the curves we can see
that the smaller the training corpus of the baseline
parser, the more signiﬁcant improvement can be
obtained. This is a good news for the resource-
scarce languages that have no large treebanks.

2The Berkeley Parser fails to give k-best parses for some
sentences when trained on small treebanks, and these sen-
tences have to be deleted in the k-best reranking experiments.

523

4.3 Using in Machine Translation

We investigate the effect of the projected parser
in the tree-based translation model on Chinese-to-
English translation. A series of contrast transla-
tion systems are built, each of which uses a super-
vised Chinese parser (Xiong et al., 2005) trained
on a particular amount of CTB trees.

We use the FBIS Chinese-English bitext as the
training corpus, the 2002 NIST MT Evaluation
test set as our development set, and the 2005 NIST
MT Evaluation test set as our test set. We ﬁrst ex-
tract the tree-to-string translation rules from the
training corpus by the algorithm of
(Liu et al.,
2006), and train a 4-gram language model on
the Xinhua portion of GIGAWORD corpus with
Kneser-Ney smoothing using the SRI Language
Modeling Toolkit
(Stolcke and Andreas, 2002).
Then we use the standard minimum error-rate
training (Och, 2003) to tune the feature weights

to maximize the system.s BLEU score.

Figure 3 shows the experimental results. We
ﬁnd that the translation system using the projected
parser achieves the performance comparable with
the one using the supervised parser trained on
CTB 1.0. Considering that the F-score of the pro-
jected parser is only 22.8%, which is far below of
the 75.6% F-score of the supervised parser trained
on CTB 1.0, we can give more conﬁdence to the
assumption that the projected parser is apt to de-
scribe the syntax structure of the counterpart lan-
guage. This surprising result also gives us an in-
spiration that better translation would be achieved
by combining projected parsing and supervised
parsing into hybrid parsing schema.

5 Conclusion

This paper describes an effective strategy for con-
stituent projection, where dependency projection
and constituent projection are consequently con-
ducted to obtain the initial projected treebank,
and an constraint EM procedure is then per-
formed to optimized the projected trees. The
projected parser,
trained on the projected tree-
bank, signiﬁcantly boosts an existed state-of-the-
art supervised-trained parser, especially trained on
a smaller treebank. When using the projected
parser in tree-based translation, we achieve the

CTB 5.0

0.270

0.260

0.250

0.240

0.230

e
r
o
c
s
 
U
E
L
B

use projected parser

CTB 1.0

0.220

 1000

use supervised parsers

Scale of treebank (log)

 10000

Figure 3: Performances of the translation systems,
which use the projected parser and a series of su-
pervised parsers trained CTB trees.

translation performance comparable with using a
supervised parser trained on thousands of human-
annotated trees.

As far as we know, this is the ﬁrst time that
the experimental results are systematically re-
ported about the constituent projection and its ap-
plications. However, many future works need
to do. For example, more energy needs to be
devoted to the treebank optimization, and hy-
brid parsing schema that integrates the strengths
of both supervised-trained parser and projected
parser would be valuable to be investigated for
better translation.

Acknowledgments

The authors were supported by 863 State Key
Project No. 2006AA010108, National Natural
Science Foundation of China Contract 60873167,
Microsoft Research Asia Natural Language Pro-
cessing Theme Program grant (2009-2010), and
National Natural Science Foundation of China
Contract 90920004. We are grateful to the anony-
mous reviewers for their thorough reviewing and
valuable suggestions.

References

Bod, Rens. 2006. An all-subtrees approach to unsu-
pervised parsing. In Proceedings of the COLING-
ACL.

Carreras, Xavier, Michael Collins, and Terry Koo.
2008. Tag, dynamic programming, and the percep-
tron for efﬁcient, feature-rich parsing. In Proceed-
ings of the CoNLL.

524

Charniak, Eugene and Mark Johnson. 2005. Coarse-
to-ﬁne-grained n-best parsing and discriminative
reranking. In Proceedings of the ACL.

McClosky, David, Eugene Charniak, and Mark John-
son. 2006. Reranking and self-training for parser
adaptation. In Proceedings of the ACL.

Charniak, Eugene.

2000. A maximum-entropy-

inspired parser. In Proceedings of the NAACL.

Collins, Michael. 1999. Head-driven statistical mod-

els for natural language parsing. In Ph.D. Thesis.

Collins, Michael. 2000. Discriminative reranking for
In Proceedings of the

natural language parsing.
ICML, pages 175–182.

Collins, Michael. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of the EMNLP, pages 1–8, Philadelphia, USA.

Ganchev, Kuzman, Jennifer Gillenwater, and Ben
Taskar. 2009. Dependency grammar induction via
bitext projection constraints. In Proceedings of the
47th ACL.

Huang, Liang and David Chiang. 2005. Better k-best
parsing. In Proceedings of the IWPT, pages 53–64.

Huang, Liang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of the AMTA.

Huang, Liang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
the ACL.

Hwa, Rebecca, Philip Resnik, Amy Weinberg, and
Okan Kolak. 2002. Evaluating translational corre-
spondence using annotation projection. In Proceed-
ings of the ACL.

Hwa, Rebecca, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak.
2005. Bootstrap-
ping parsers via syntactic projection across paral-
lel texts.
In Natural Language Engineering, vol-
ume 11, pages 311–325.

Klein, Dan and Christopher D. Manning. 2002. A
generative constituent-context model for improved
grammar induction. In Proceedings of the ACL.

Klein, Dan and Christopher D. Manning. 2004. Cor-
pusbased induction of syntactic structure: Models
of dependency and constituency. In Proceedings of
the ACL.

Liu, Yang, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proceedings of the ACL.

Liu, Yang, Tian Xia, Xinyan Xiao, and Qun Liu. 2009.
Weighted alignment matrices for statistical machine
translation. In Proceedings of the EMNLP.

McDonald, Ryan, Fernando Pereira, Kiril Ribarov, and
Jan Haji˘c. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings
of HLT-EMNLP.

Och, Franz J. and Hermann Ney. 2000.

Improved
statistical alignment models. In Proceedings of the
ACL.

Och, Franz Joseph. 2003. Minimum error rate train-
ing in statistical machine translation.
In Proceed-
ings of the 41th Annual Meeting of the Association
for Computational Linguistics, pages 160–167.

Papineni, Kishore, Salim Roukos, Todd Ward, and
Weijing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings
of the ACL.

Petrov, Slav, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and in-
terpretable tree annotation.
In Proceedings of the
ACL.

Sarkar, Anoop. 2001. Applying co-training methods

to statistical parsing. In Proceedings of NAACL.

Seginer, Yoav. 2007. Fast unsupervised incremental

parsing. In Proceedings of the ACL.

Smith, David and Jason Eisner. 2009. Parser adap-
tation and projection with quasi-synchronous gram-
mar features. In Proceedings of EMNLP.

Steedman, Mark, Miles Osborne, Anoop Sarkar,
Stephen Clark, Rebecca Hwa, Julia Hockenmaier,
Paul Ruhlen, Steven Baker, and Jeremiah Crim.
2003. Bootstrapping statistical parsers from small
datasets. In Proceedings of the EACL.

Stolcke and Andreas. 2002. Srilm - an extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Process-
ing, pages 311–318.

Xiong, Deyi, Shuanglong Li, Qun Liu, and Shouxun
Lin. 2005. Parsing the penn chinese treebank with
semantic knowledge.
In Proceedings of IJCNLP
2005, pages 70–81.

Yamada, H and Y Matsumoto. 2003. Statistical de-
pendency analysis using support vector machines.
In Proceedings of IWPT.

Zhang, Yue and Stephen Clark.

2008. A tale of
investigating and combining graph-
two parsers:
based and transition-based dependency parsing us-
ing beam-search. In Proceedings of EMNLP.

