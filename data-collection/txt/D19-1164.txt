



















































Enhancing Context Modeling with a Query-Guided Capsule Network for Document-level Translation


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 1527–1537,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

1527

Enhancing Context Modeling with a Query-Guided
Capsule Network for Document-level Translation

Zhengxin Yang123, Jinchao Zhang3, Fandong Meng3
Shuhao Gu12, Yang Feng12?, Jie Zhou3

1 University of Chinese Academy of Sciences
2 Key Laboratory of Intelligent Information Processing

Institute of Computing Technology, Chinese Academy of Sciences (ICT/CAS)
3 Pattern Recognition Center, WeChat AI, Tencent Inc, China

{yangzhengxin17z,gushuhao17g,fengyang}@ict.ac.cn
{dayerzhang,fandongmeng,withtomzhou}@tencent.com

Abstract

Context modeling is essential to gener-
ate coherent and consistent translation for
Document-level Neural Machine Translations.
The widely used method for document-level
translation usually compresses the context in-
formation into a representation via hierarchi-
cal attention networks. However, this method
neither considers the relationship between
context words nor distinguishes the roles of
context words. To address this problem,
we propose a query-guided capsule networks
to cluster context information into different
perspectives from which the target transla-
tion may concern. Experiment results show
that our method can significantly outperform
strong baselines on multiple data sets of dif-
ferent domains.

1 Introduction

The encoder-decoder based Neural machine trans-
lation (NMT) models (Sutskever et al., 2014; Bah-
danau et al., 2014; Wu et al., 2016; Vaswani et al.,
2017; Zhang et al., 2019) have made great pro-
gresses and drawn much attention in recent years.
In practical applications, NMT systems are often
fed with a document-level input which requires
reference resolution, the consistency of tenses and
noun expressions and so on. Many researchers
have proven that contextual information is essen-
tial to generate coherent and consistent translation
for document-level translation (Hardmeier, 2012;
Meyer and Webber, 2013; Sim Smith, 2017; Jean
et al., 2017; Maruf and Haffari, 2018; Miculicich
et al., 2018; Zhang et al., 2018a; Voita et al., 2018;
Wang et al., 2017; Tu et al., 2018; Maruf et al.,
2019). Despite the great success of the above mod-
els, they are designed for sentence-level transla-

Joint work with Pattern Recognition Center, WeChat AI,
Tencent Inc, China.

? Corresponding Author

tion tasks and exclude contextual information in
the model architecture.

On these grounds, the widely used hierarchi-
cal attention networks (HAN) was proposed to in-
tegrate contextual information in document-level
translation (Miculicich et al., 2018). In this
method, the context sentences are considered in
the form of hierarchical attentions retrieved by the
current generation. That is, it utilizes a word-
level attention to represent a sentence and then
a sentence-level attention to represent all the in-
volved context. In this way, the final attention
representation has to encode all the information
needed for coherent and consistent translation, in-
cluding reference information, tenses, expressions
and so on. To get the multi-perspective informa-
tion, it is necessary to distinguish the role of each
context word and model their relationship espe-
cially when one context word could take on mul-
tiple roles (Zhang et al., 2018b). However, this is
difficult to realize for the HAN model as its final
representation for the context is produced with an
isolated relevance with the query word which ig-
nores relations with other context words.

To address the problem, we introduce Capsule
Networks into document-level translation which
have proven good at modelling the parts-wholes
relations between low-level capsules and high-
level capsules (Hinton et al., 2011; Xiao et al.,
2018; Sabour et al., 2017; Hinton et al., 2018;
Gu and Feng, 2019). With capsule networks, the
words in a context source sentence is taken as low-
level capsules and the information of different per-
spectives is treated as high-level capsules. Then
in the dynamic routing process of capsule net-
works, all the low-level capsules trade off against
each other and consider over all the high-level cap-
sules and drop themselves at a proper proportion
to the high-level capsules. In this way, the relation
among low-level capsules and that between low-



1528

level capsules and high-level capsules are both
explored. In order to make sure high-level cap-
sules indeed cluster information needed by the tar-
get translation, we apply capsule networks to both
sides of the current sentence and add a regulariza-
tion layer using Pearson Correlation Coefficients
to force the high-level capsules on the two sides to
approach to each other. In addition, we still need
to ensure the final output of capsule networks is
relevant to the current sentence. Therefore we pro-
pose a Query-guided Capsule Network (QCN) to
have the current source sentence to take part in the
routing process so that high-level capsules can re-
tain information related to the current source sen-
tence.

To the best of our knowledge, this is the
first work which applies capsule networks to
document-level translation tasks and QCN is also
the first attempt to customize attention for cap-
sule networks in translation tasks. We conducted
experiments on three English-German translation
data sets in different domains and the results
demonstrate that our method can significantly im-
prove the performance of document-level transla-
tion compared with strong baselines.

2 Background

2.1 Sentence-level NMT and Transformer
Model

Sentence-level NMTs are generally based on
an encoder-decoder framework (Sutskever et al.,
2014; Bahdanau et al., 2014; Wu et al., 2016;
Vaswani et al., 2017; Zhang et al., 2019; Meng and
Zhang, 2019). In this framework, the encoder en-
codes the source sentence x = {x1, x2, · · · , xM}
into a sequence of continuous representations z =
{z1, z2, · · · , zM}. Given z, the decoder predicts
target words in order and returns the target trans-
lation y = {y1, y2, · · · , yN}. The objective func-
tion of NMT is to maximize the log-likelihood of
a set of source-target language sentence pairs as

L(Θ;x,y) = 1
|S|

∑
(x,y)∈S

logP(y|x; Θ) (1)

where S is the training set.
As we implement our approaches based on the

Transformer architecture (Vaswani et al., 2017),
which is a strong sentence-level NMT baseline, we
give a brief description of the Transformer model.
The encoder and decoder are composed of simi-
lar layers which consists of two general types of

sub-layers: multi-head attention mechanism and
point-wise fully connected feed-forward network
(FFN).

Encoder: The encoder of the Transformer is
composed of N identical layers. Each of these
layers includes a multi-head self-attention mecha-
nism that allows each position of the output of pre-
vious encoder layer to attend to all other positions,
as well as a position-wise FFN, which is stacked
on top of the multi-head self-attention, composed
of two linear transformations and a ReLU activa-
tion function.

Decoder: The architecture of the decoder is
similar to the encoder, however, it employs an ad-
ditional multi-head attention sub-layer over the en-
coder output between the multi-head self-attention
sub-layer and position-wise FFN sub-layer. The
multi-head self-attention sub-layer needs to mask
the input target tokens in the future.

2.2 Document-level NMT

The document-level translation task is to trans-
late each source sentence with consideration of the
previous context in the document. Formally, the
translation of a document D containing |D| = J
sentence pairs can be defined as given the source
document X in order and the translation system
generates each translation yj ∈ Y in order. The
document translation probability can be defined
as:

P(Y |X; Θ) =
J∏

j=1

P(yj |xj ,D<j ; Θ) (2)

where xj , yj denote the jth source and target sen-
tence respectively, and D<j denotes all of the pre-
vious sentence pairs in document. For each sen-
tence xj , each target word is generated according
to the source representation and the generated tar-
get hypothesis, therefore the Eq. (2) can be formu-
lated as:

P(Y |X; Θ) =
J∏

j=1

I j∏
i=1

P(y ji |,y
j
<i,x

j ,D<j ; Θ)

(3)
where y ji denotes the ith word of the jth transla-
tion yj with the length as I j and yj<i denotes the
generated target hypothesis.

The training objective of document-level NMTs
is to maximize the log-likelihood of translations in



1529

Figure 1: The overall architecture consists of three modules: the Query-guided Capsule Network in the upper left
of this figure, the Regularization Layer in the lower left of this figure and the Sub-layer-expanded Transformer in
the right of this figure.

document context as following:

L(Θ;X,Y ) = 1
|D|

∑
(X,Y )∈D

logP(Y |X; Θ)

(4)
where D is the training set of the DocNMT.

Compared with the sentence-level NMTs, the
critical part of document-level NMTs is to effec-
tively capture and utilize the related contextual
information when translating the to-be-translated
source sentence.

3 Our Approach

In this section, we introduce the proposed Query-
guided Capsule Network (QCN) for enhancing the
document-level NMT. First, we present the overall
architecture of the network, and then we describe
the QCN in detail.

3.1 Overall Architecture

We aim to enhance the document-level NMT per-
formance through effectively capturing and em-
ploying the contextual features in each histor-
ical sentence that related to the current sen-
tence. We integrating a novel Query-guided Cap-
sule Network into the sentence-level Transformer-
based NMT (Vaswani et al., 2017) to capture the
document-level contextual information for trans-
lating the current source sentence.

As shown in Figure 1, the overall architecture of
our translation model is composed of three mod-
ules:

• The Query-guided Capsule Network takes
the to-be-translated source sentence as the
query to guide the procedure of retrieving
related and helpful contextual features from
historical sentences with a novel dynamic
routing algorithm.

• Sub-layer-expanded Transformer contains
a new sub-layer that attending the contex-
tual features extracted from the QCN to ef-
fectively utilize them for translation.

• Regularization Layer contains two conven-
tional Capsule Networks to unify the source
sentence and the target sentence into an iden-
tical semantic space through computing an
extra PCCs loss item at the training stage.

3.2 Query-guided Capsule Network

The Capsule Network (CapsNet) (Sabour et al.,
2017) was proposed to build parts-wholes relation-
ships in the iterative routing procedure, which can
be used to capture features in historical sentences
from low level to high level. Capsules in the lower
layer vote for those in the higher layer by aggregat-
ing their transformations with iteratively updated



1530

Figure 2: Query-guided Capsule Network, the green
blocks in the middle of the figure are lower-level cap-
sules ui, right-side blocks indicate the higher-level
capsules vj and the query vector q is initially tiled to
length |v| and updated with corresponding higher-level
capsules. Here shows a Query-guided Capsule Net-
work with 3 high-level (feature) capsules. In each iter-
ation, the PPCs is computed according to the input vec-
tor ui and the query vector qj , then pijûj|i and cijûj|i
are added to obtain the higher-level capsules which will
be used to update corresponding query later.

coupling coefficients. However, there exists an ob-
vious drawback of directly applying the Capsule
Network into the document-level NMT for cap-
turing contextual features. The reason is that the
CapsNet can only extract internal features without
considering whether features are related to the to-
be-translated source sentence.

To address this issue, we proposed the Query-
guided Capsule Network (QCN), which employ
the representation of the to-be-translated source
sentence as the query to guide the feature ex-
traction procedure of the Capsule Network. In
this way, contextual features that generated from
the QCN are based on the internal semantic rela-
tions among each historical sentence and the ex-
ternal semantic relations between historical sen-
tences and the to-be-translated source sentence.
The QCN is based on an improved dynamic rout-
ing algorithm which will be detailed introduced in
the next section.

Improved Dynamic Routing of QCN

Given a query vector q and a set of input capsules
u = {u1,u2, · · · ,un}, the dynamic routing algo-
rithm iteratively calculates the correlation between
the query vector and each input capsule and up-
dates output capsules. Specifically, query vector
q is the representation of source to-be-translated
sentence and input capsules u are all word em-
beddings in previous sentences, which can be for-

Algorithm 1 Improved Dynamic Routing
Input: r , q and u = {u1,u2, · · · ,un}
Output: v = {v1,v2, · · · ,vm}

1: // Initialization
2: for each output capsule vj in higher-layer do
3: for each input capsule ui in lower-layer do
4: ûj|i ←W ijui
5: αij ← 0
6: qj ← q
7: pij ← tanh(PCCs(ui, qj))
8: . PCCs computes Eq 7
9: end for

10: end for
11:
12: // Iteration
13: itr ← 0
14: repeat
15: for each input capsule ui in lower-layer do
16: ci ← softmax(αi)
17: end for
18:
19: for each output capsule vj in higher-layer do
20: sj ←

∑n
i=1(cij + pij)ûj|i

21: vj ← squash(sj)
22: . squash computes Eq 8
23: end for
24:
25: for each output capsule vj in higher-layer do
26: for each input capsule ui in lower-layer do
27: αij ← αij + pijûj|ivj
28: end for
29: qj ←

qj+vj
2

30: for each input capsule ui in lower-layer do
31: pij ← tanh(PCCs(ui, qj))
32: end for
33: end for
34: itr ← itr + 1
35: until itr = r

mulated as:

q = g(
∑
x∈x

embedding(x)) (5)

u−ki = f([embedding(x
−k
i ); onehot(k)]) (6)

where f and g are both linear transformation
functions and k indicates the distance of histor-
ical sentence from the to-be-translated sentence.
Each word embedding in historical sentences is
concatenated with a distance-determined one-hot
vector to provide positional markers. Com-
pared to the dynamic routing method proposed by
(Sabour et al., 2017), our improved dynamic rout-
ing method of QCN can model information that
related to the query among input capsules.

Algorithm 1 shows details of the algorithm and
Figure 2 illustrates the interaction of the vari-
ous components in the QCN. Initially, the im-
proved dynamic routing algorithm of QCN re-
cieves a sequence of lower-level capsules u =
{u1,u2, · · · ,un} and a query vector q and



1531

then calculates a Pearson Correlation Coefficients
(PCCs) between q and each input capsule ui (line
7). The PCCs is a measure of the linear correla-
tion between two variables. When PCCs is close
to +1, it means they have very strong positive lin-
ear correlation, and close to -1 means total nega-
tive correlation. Given a pair of variables (A,B),
the formula for computing PPCs is

PCCs(A,B) =
Cov(A,B)

σAσB

=
(A− 1n

∑n
i=1 ai)

T (B − 1n
∑n

i=1 bi)

‖A‖ · ‖B‖
(7)

where A and B are both n-dimension vectors,
Cov is the covariance and σA, σB is the standard
deviation of A and B respectively.

The routing iteration process then computes
coupling coefficients, denoted as ci(line 16), with
regard to a input capsule ui and all the higher-
level capsules v. In the original dynamic rout-
ing algorithm (Sabour et al., 2017), coupling co-
efficients are only determined by the cumulative
“agreement”, which are the prior probabilities that
capsule ui should be coupled to capsule vj . How-
ever, in DocNMT situation, it is far from enough
to cluster the high-level information from the cap-
sules in lower-level that related to query vector ac-
cording to a naive “agreement”. To address this is-
sue, we reduce the “agreement”, when PCCs show
a negative linear correlation between the query
vector q and the input vector ui. Instead, we in-
crease the “agreement”, when PCCs are positive
(line 27). The query vector q is initially tiled
to length |v| and updated with the corresponding
higher-level capsules vj in each iteration (line 29).

Our routing iteration updates higher-level cap-
sules by adding

∑n
i=1 pijûj|i to sj . This step can

add more information related to the query vector
and cut off the unrelated features (line 20). It is
necessary to get different length of output cap-
sules shrunk into the 0 to 1 interval using “squash”
function (line 21) proposed by Sabour et al. (2017)
which is shown in Eq.(8).

squash(t) =
||t||2

1 + ||t||2
t

||t||
(8)

where t can be the initial input capsule ui or the
vector sj for predicting the output capsule.

3.3 Sub-layer-expanded Transformer
To effectively utilize contextual features extracted
from each historical sentence by QCN, we intro-

Figure 3: Regularization Layer. In the training stage,
this layer takes the inputs of the encoder (left part of
the figure) and decoder (right part of the figure) as the
input capsules of two capsule networks separately, and
computes the PCCs between the high-level capsules of
two networks.

duce an additional context-aware multi-head at-
tention sub-layer in each layer of the Transformer
encoder. The multi-head attention can attend to all
the positions of the contextual features with out-
puts of the previous sub-layer as a query. Then the
output of the context-aware multi-head attention
sub-layer is fed into the point-wise feed-forward
sub-layer in each layer of the encoder. The right
part in Figure 1 shows details of the Sub-layer-
expanded Transformer. Specifically, the equation
of multi-head attention computing procedure is as
following:

MultiHead(Q,K,V )=Concat(H1,· · ·, Hh)WO

Hi = Attention(QWi
Q,KWi

K , V Wi
V ) (9)

where WiQ, WiK and WiV are the parameter ma-
trices, Q, K and V indicates the query, key and
value representations. The computation of the at-
tention function is as following:

Attention(Q,K, V ) = softmax(
QKT√
dk

)V (10)

where dk indicates the dimension of queriesQ and
keys K.

3.4 Regularization Layer
To better modeling the document-level translation
task, we incorporate a regularization layer (Figure
3) into the whole architecture to restrict the source
sentence and target sentence to an identical seman-
tic space. This layer separately feeds the inputs



1532

of encoder and decoder into two capsule networks
Capsenc and Capsdec, and computes the PCCs be-
tween the outputs of two networks. We regard the
PCCs as an extra regularization term in the final
objective at the training stage. The loss function
of our model can be formulated as:

L(Θ;X,Y ) = 1
|D|

×
J∑

j=1

I j∑
i=1

{logP(y ji |,y
j
<i,x

j ,D<j ; Θ)

+ PCCs(Capsenc(x
j),Capsdec(y

j))}
(11)

where Θ are parameters of the model, D<j are
historical sentences of the to-be-translated source
sentence, xj is the to-be-translated sentence and
yj<i denotes the generated target hypothesis.

4 Experiments

4.1 Settings
Datasets and Evaluation Metrics
We carry out experiments on English-German
translation tasks in three different domains: talks,
news, and speeches. The corpora statistics are
shown in Table 1.

• TED. This corpus is a Machine Transla-
tion part of the IWSLT 2017 (Cettolo et al.,
2012) evaluation compaigns1, each TED talk
is considered to be a document. we take the
tst2016-207 as the test set, and other as our
development set.

• News. We take the sentence-aligned
document-delimited News Commentary v11
corpus2 as our training set. The WMT’16
news-test2015 and news-test2016 are used
for development and testing respectively.

• Europarl. The corpus are extracted from
the Europarl v7 (Koehn, 2005) according to
the method mentioned in Maruf and Haffari
(2018). The training, development and test
sets are obtained through randomly splitting
the corpus.

We download all of above extracted corpora3

from Maruf et al. (2019). The tokenization and
1https://wit3.fbk.eu/
2http://www.casmacat.eu/corpus/news-commentary.html
3https://github.com/sameenmaruf/selective-

attn/tree/master/data

truecase pre-processing are implemented on all
datasets using the scripts of the Moses Toolkit4

(Koehn et al., 2007). We also apply segmentation
into BPE subword units5 (Sennrich et al., 2016)
with 30K merge operations.

We use two metrics: BLEU (Papineni et al.,
2002) and Meteor (Lavie and Agarwal, 2007) to
evaluate the translation quality.

Models and Baselines
We use the Transformer architecture as our
context-agnostic baseline and adopt three context-
aware baselines (Zhang et al., 2018a; Miculicich
et al., 2018; Maruf and Haffari, 2018).

We performed the same configuration on our
models according to the settings of the Maruf and
Haffari (2018). Specifically, for the Transformer,
we set the hidden size and point-wise FFN size as
512 and 2048 respectively. We use 4 layers and 8
attention heads in both encoder and decoder. All
dropout rates are set to 0.1 for context-agnostic
model and 0.2 for context-aware model.

In the training phase, we use the default Adam
optimizer (Kingma and Ba, 2014) with a fixed
learning rate of 0.0001. The batch size is 1500 on
TED dataset and 900 on both News and Europarl
datasets.

4.2 Results and Analysis
Main Results
Table 2 shows that our model surpasses all
the context-agnostic(Vaswani et al., 2017) and
context-aware(Zhang et al., 2018a; Miculicich
et al., 2018; Maruf and Haffari, 2018) baselines
on TED and Europarl datasets. For TED dataset,
the performance of our model greatly exceeds that
of all other baselines, and is better than Miculicich
et al. (2018) with a gain of +0.59 BLEU and +0.61
Meteor. For Europarl dataset, our model got im-
provements with a gain of +0.07 on BLEU metric,
but the Meteor score is +0.64 higher than Maruf
et al. (2019) which utilize the whole document as
the contextual information, whereas we only using
3 previous sentences.

Results on the sequence-level Transformer and
our DocNMTs show that the captured contex-
tual features provide helpful semantic information
for enhancing the translation quality. The regu-
larization term that we proposed can effectively

4https://github.com/moses-
smt/mosesdecoder/tree/master/scripts

5https://pypi.org/project/subword-nmt/



1533

TED News Europarl
Sent No. Doc len avg Sent No. Doc len avg Sent No. Doc len avg

Training 206,126 121.39 236,287 38.93 1,666,904 14.14
Development 8,967 96.42 2,169 26.78 3,587 14.95
Test 2,271 98.74 2,999 19.35 5,134 14.26

Table 1: The statistics of the training/development/test corpora in number of sentence pairs and the average
document length (in sentences).

TED News Europarl
Model BLEU Meteor BLEU Meteor BLEU Meteor
Transformer-DocNMT (Zhang et al., 2018a) 24.00 44.69 23.08 42.40 29.32 46.72
HAN-DocNMT (Miculicich et al., 2018) 24.58 45.48 25.03 44.02 28.60 46.09
SAN-DocNMT (Maruf et al., 2019) 24.42 45.38 24.84 44.27 29.75 47.22
Transformer 23.28 44.17 21.67 41.11 28.72 46.22
Transformer + Regularization Term 24.55 45.57 22.09 41.77 29.42 47.59
Transformer + QCN 24.41 46.09 22.22 41.90 29.48 47.49
Transformer + QCN + Regularization Term 25.19 45.91 22.37 41.88 29.82 47.86

Table 2: BLEU and Meteor scores of models. There exists three context-aware baseline models and a context-
agnostic model. “Regularization Term” denotes integrating the regularization layer into the Transformer model
and “QCN” denotes incorporating Query-guided Capsule Network in the Transformer model.

Figure 4: Effect of Contextual Information Scope. It
is not that the more historical sentences we utilize, the
better translation performance is. Effect of scope on
BLEU and Meteor scores are inconsistent to some de-
gree.

further improve the model performance on TED
and Europarl datasets. For the restriction of the
GPU memory, we have to filter long sentences to
keep our model running. Although, it hurts the
model performance on the “News” dataset (con-
tains many long sentences), the QCN module and
regularization term still bring improvements.

Effect of Contextual Information Scope
To investigate the effect of contextual information
scope, we carry on the number of historical sen-
tences hyper-parameter experiments on the TED
talk dataset. We fix the hyper-parameters of the
QCN by setting both the number of higher-level

Figure 5: Effect of Feature Capsule Number. Appro-
priate capsule number is important for model perfor-
mance.

capsules and routing iteration to four, and inves-
tigating the impact of changes in the number of
historical sentences on BLEU and Meteor scores.
Figure 4 shows that using one historical sentence
in QCN can obtain the best Meteor score while the
highest BLEU score is presented when we utilize
two historical sentences. We found that the growth
of Meteor and BLEU scores are opposite. There-
fore, the choice of utilizing how many historical
sentences is a trade-off between both scores. We
choose three historical sentences as our final set-
ting. Through experimentation, we also found it
is not that the more historical sentences we utilize,
the better translation performance is.



1534

Figure 6: Agreement distribution of a sentence (26
words) as inputs of the QCN and 4 contextual features
as output. Blue color means higher agreement and the
red means lower. From top to bottom are four heat
maps in 0th to 3th iterations.

Effect of Feature Capsule Number
QCN is the crucial part of our overall architec-
ture and the positive and negative impact depends
on the configuration of the QCN. Therefore, We
also investigate the effect of the hyper-parameter
of QCN: the number of feature capsules.

We set the number of historical sentence as 3 ac-
cording to the previously experimental results, and
the number of routing iteration is set to 4. Figure
5 shows that Meteor score become highest when
the number of higher-level capsules is set as 2, but
BLEU score can obtain best score at 4. We finally
choose 4 as the final setting because both BLEU
and Meteor can obtain relatively good results.

Visualization of Agreement and PCCs
Coupling Coefficients (CCs) can indirectly reflect
the variation of the “agreement”, so we visual-
ize the coefficients in each routing iteration at the
stage of decoding as shown in Figure 6. In the first
iteration, all coupling coefficients are initialized
in a uniform distribution, all higher-level capsules
are voted by lower-level capsules equally. Then,
lower-level capsules are iteratively trained to send
more information to the proper higher-level cap-
sule. Figure 6 shows that most of input capsules
are tend to vote the 2ed feature capsule finally.

Different from the CCs, PCCs show the linear
correlation between query and inputs. Initially, the
query vector is tiled with the number of feature
capsules and is used to calculate the PCCs with
each input capsules, as the Figure 7 shows that the
color of every column is identical. As the iterative
routing begins, each query is updated according
to the higher-level capsules. The function of the
PPCs is to increase or decrease the coupling coef-
ficients value according to the positive or negative

Figure 7: PCCs distribution of a sentence has 17 words
as inputs of the QCN and 4 duplicated query. Blue
color denotes positive correlation and red means neg-
ative. From top to bottom are four heat maps in 0th to
3th iterations.

value. See Figure 7, we can find that PCCs varies
as iteration changes.

5 Related Work

Document-level Machine Translation

Document-level machine translation became a hot
research direction in the later stage of statistical
machine translation era. Hardmeier and Federico
(2010) represented the links between word pairs
in the context using a word dependency model for
SMT to improve the translation of anaphoric pro-
nouns. Hardmeier et al. (2012, 2013) first pro-
posed a new document-level SMT paradigm that
translates whole documents as units. However, in
this period, most of the work has not achieved too
many compelling results or has been only focused
on a part of difficulties.

With the coming of the era of Neural Ma-
chine Translation, many works began to focus on
Document-level NMT tasks. Xiong et al. (2019)
trained a reward teacher to refine the transla-
tion quality from a document perspective. Tiede-
mann and Scherrer (2017) simply concatenated
sentences in one document as models’ input or
output. Jean et al. (2017) used additional con-
text encoder to capture larger-context information.
Kuang et al. (2017); Tu et al. (2018) used a cache
to memorize most relevant words or features in
previous sentences or translations.

Recently, several studies integrated additional
modules into the Transformer-based NMTs for
modeling contextual information. (Voita et al.,
2018; Zhang et al., 2018a), Maruf and Haffari
(2018) proposed a document-level NMT using a



1535

memory-networks, and Wang et al. (2017) and Mi-
culicich et al. (2018) integrated hierarchical atten-
tion network in RNN-based NMT or Transformer
to model the document-level information. Maruf
et al. (2019) used the whole document as the con-
textual information and firslty divided document-
level translation tasks into two types: offline and
online.

Capsule Networks

Hinton et al. (2011) proposed the capsule concep-
tion to use vector for describing the pose of an
object. The dynamic routing algorithm was pro-
posed by Sabour et al. (2017) to build the part-
whole relationship through the iterative routing
procedure. Hinton et al. (2018) designed a new
routing style based on the EM algorithm. Some
researchers investigated to apply the capsule net-
work for various tasks. Wang et al. (2018) investi-
gated a novel capsule network with dynamic rout-
ing for linear time NMT. Yang et al. (2018) ex-
plored capsule networks for text classification with
strategies to stabilize the dynamic routing process.
Gu and Feng (2019) introduces capsule networks
into Transformer to model the relations between
different heads in multi-head attention. We specif-
ically investigated dynamic routing algorithms for
the document-level NMT.

6 Conclusion

We have proposed a novel Query-guided Cap-
sule Network with an improved dynamic rout-
ing algorithm for enhancing context modeling for
the document-level Neural Machine Translation
Model. Experiments on English-German in differ-
ent domains showed our model significantly out-
performs sentence-level NMTs and achieved state-
of-the-art performance on two of three datasets,
which proved the effectiveness of our approaches.

Acknowledgments

We thank the anonymous reviewers for their in-
sightful comments. This work was supported by
the National Natural Science Foundation of China
(NO.61662077, NO.61876174) and National Key
R&D Program of China (NO.2017YFE9132900).

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-

gio. 2014. Neural machine translation by jointly

learning to align and translate. arXiv preprint
arXiv:1409.0473.

Mauro Cettolo, Christian Girardi, and Marcello Fed-
erico. 2012. Wit3: Web inventory of transcribed and
translated talks. In Proceedings of the 16th Con-
ference of the European Association for Machine
Translation (EAMT), pages 261–268, Trento, Italy.

Shuhao Gu and Yang Feng. 2019. Improving multi-
head attention with capsule networks. In Proceed-
ings of the 8th CCF International Conference on
Natural Language Processing and Chinese Comput-
ing.

Christian Hardmeier. 2012. Discourse in statistical ma-
chine translation. a survey and a case study. Dis-
cours. Revue de linguistique, psycholinguistique et
informatique. A journal of linguistics, psycholin-
guistics and computational linguistics, (11).

Christian Hardmeier and Marcello Federico. 2010.
Modelling pronominal anaphora in statistical ma-
chine translation. In IWSLT (International Work-
shop on Spoken Language Translation); Paris,
France; December 2nd and 3rd, 2010., pages 283–
289.

Christian Hardmeier, Joakim Nivre, and Jörg Tiede-
mann. 2012. Document-wide decoding for phrase-
based statistical machine translation. In Proceed-
ings of the 2012 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1179–1190, Jeju Island, Korea. Association for
Computational Linguistics.

Christian Hardmeier, Sara Stymne, Jörg Tiedemann,
and Joakim Nivre. 2013. Docent: A document-level
decoder for phrase-based statistical machine trans-
lation. In Proceedings of the 51st Annual Meeting
of the Association for Computational Linguistics:
System Demonstrations, pages 193–198, Sofia, Bul-
garia. Association for Computational Linguistics.

Geoffrey E Hinton, Alex Krizhevsky, and Sida D
Wang. 2011. Transforming auto-encoders. In Inter-
national Conference on Artificial Neural Networks,
pages 44–51. Springer.

Geoffrey E Hinton, Sara Sabour, and Nicholas Frosst.
2018. Matrix capsules with EM routing. In Interna-
tional Conference on Learning Representations.

Sébastien Jean, Stanislas Lauly, Orhan Firat, and
Kyunghyun Cho. 2017. Does neural machine
translation benefit from larger context? CoRR,
abs/1704.05135.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT summit, vol-
ume 5, pages 79–86.

https://www.aclweb.org/anthology/D12-1108
https://www.aclweb.org/anthology/D12-1108
https://www.aclweb.org/anthology/P13-4033
https://www.aclweb.org/anthology/P13-4033
https://www.aclweb.org/anthology/P13-4033
https://openreview.net/forum?id=HJWLfGWRb


1536

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al. 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of the 45th annual meeting of the associ-
ation for computational linguistics companion vol-
ume proceedings of the demo and poster sessions,
pages 177–180.

Shaohui Kuang, Deyi Xiong, Weihua Luo, and
Guodong Zhou. 2017. Cache-based document-
level neural machine translation. arXiv preprint
arXiv:1711.11221.

Alon Lavie and Abhaya Agarwal. 2007. Meteor: An
automatic metric for mt evaluation with high levels
of correlation with human judgments. In Proceed-
ings of the Second Workshop on Statistical Machine
Translation, pages 228–231. Association for Com-
putational Linguistics.

Sameen Maruf and Gholamreza Haffari. 2018. Docu-
ment context neural machine translation with mem-
ory networks. In Proceedings of the 56th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), pages 1275–
1284, Melbourne, Australia. Association for Com-
putational Linguistics.

Sameen Maruf, André FT Martins, and Gholamreza
Haffari. 2019. Selective attention for context-aware
neural machine translation. In Proceedings of the
2019 Conference of the North American Chapter
of the Association for Computational Linguistics:
Human Language Technologies, Minneapolis, Min-
nesota. Association for Computational Linguistics.

Fandong Meng and Jinchao Zhang. 2019. Dtmt: A
novel deep transition architecture for neural machine
translation. In Proceedings of the AAAI Conference
on Artificial Intelligence, volume 33, pages 224–
231.

Thomas Meyer and Bonnie Webber. 2013. Implicita-
tion of discourse connectives in (machine) transla-
tion. In Proceedings of the Workshop on Discourse
in Machine Translation, pages 19–26.

Lesly Miculicich, Dhananjay Ram, Nikolaos Pappas,
and James Henderson. 2018. Document-level neural
machine translation with hierarchical attention net-
works. In Proceedings of the 2018 Conference on
Empirical Methods in Natural Language Process-
ing, pages 2947–2954, Brussels, Belgium. Associ-
ation for Computational Linguistics.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of
the 40th annual meeting on association for compu-
tational linguistics, pages 311–318. Association for
Computational Linguistics.

Sara Sabour, Nicholas Frosst, and Geoffrey E Hin-
ton. 2017. Dynamic routing between capsules. In
Advances in neural information processing systems,
pages 3856–3866.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Neural machine translation of rare words
with subword units. In Proceedings of the 54th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1715–
1725, Berlin, Germany. Association for Computa-
tional Linguistics.

Karin Sim Smith. 2017. On integrating discourse in
machine translation. In Proceedings of the Third
Workshop on Discourse in Machine Translation,
pages 110–121, Copenhagen, Denmark. Association
for Computational Linguistics.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Z. Ghahramani, M. Welling, C. Cortes,
N. D. Lawrence, and K. Q. Weinberger, editors, Ad-
vances in Neural Information Processing Systems
27, pages 3104–3112. Curran Associates, Inc.

Jörg Tiedemann and Yves Scherrer. 2017. Neural ma-
chine translation with extended context. In Proceed-
ings of the Third Workshop on Discourse in Machine
Translation, pages 82–92, Copenhagen, Denmark.
Association for Computational Linguistics.

Zhaopeng Tu, Yang Liu, Shuming Shi, and Tong
Zhang. 2018. Learning to remember translation his-
tory with a continuous cache. Transactions of the
Association for Computational Linguistics, 6:407–
420.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in neural information pro-
cessing systems, pages 5998–6008.

Elena Voita, Pavel Serdyukov, Rico Sennrich, and Ivan
Titov. 2018. Context-aware neural machine transla-
tion learns anaphora resolution. In ACL.

Longyue Wang, Zhaopeng Tu, Andy Way, and Qun
Liu. 2017. Exploiting cross-sentence context for
neural machine translation. In Proceedings of the
2017 Conference on Empirical Methods in Natu-
ral Language Processing, pages 2826–2831, Copen-
hagen, Denmark. Association for Computational
Linguistics.

Mingxuan Wang, Jun Xie, Zhixing Tan, Jinsong Su,
Deyi Xiong, and Chao Bian. 2018. Towards lin-
ear time neural machine translation with capsule net-
works. CoRR, abs/1811.00287.

Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V
Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus
Macherey, et al. 2016. Google’s neural ma-
chine translation system: Bridging the gap between

https://www.aclweb.org/anthology/P18-1118
https://www.aclweb.org/anthology/P18-1118
https://www.aclweb.org/anthology/P18-1118
https://www.aclweb.org/anthology/D18-1325
https://www.aclweb.org/anthology/D18-1325
https://www.aclweb.org/anthology/D18-1325
https://doi.org/10.18653/v1/P16-1162
https://doi.org/10.18653/v1/P16-1162
https://doi.org/10.18653/v1/W17-4814
https://doi.org/10.18653/v1/W17-4814
http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf
http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf
https://doi.org/10.18653/v1/W17-4811
https://doi.org/10.18653/v1/W17-4811
https://doi.org/10.1162/tacl_a_00029
https://doi.org/10.1162/tacl_a_00029
https://doi.org/10.18653/v1/D17-1301
https://doi.org/10.18653/v1/D17-1301
http://arxiv.org/abs/1811.00287
http://arxiv.org/abs/1811.00287
http://arxiv.org/abs/1811.00287


1537

human and machine translation. arXiv preprint
arXiv:1609.08144.

Liqiang Xiao, Honglun Zhang, Wenqing Chen,
Yongkun Wang, and Yaohui Jin. 2018. Mcapsnet:
Capsule network for text with multi-task learning.
In Proceedings of the 2018 Conference on Empiri-
cal Methods in Natural Language Processing, pages
4565–4574.

Hao Xiong, Zhongjun He, Hua Wu, and Haifeng Wang.
2019. Modeling coherence for discourse neural ma-
chine translation. CoRR, abs/1811.05683.

Min Yang, Wei Zhao, Jianbo Ye, Zeyang Lei, Zhou
Zhao, and Soufei Zhang. 2018. Investigating cap-
sule networks with dynamic routing for text classifi-
cation. In Proceedings of the 2018 Conference on
Empirical Methods in Natural Language Process-
ing, pages 3110–3119, Brussels, Belgium. Associ-
ation for Computational Linguistics.

Jiacheng Zhang, Huanbo Luan, Maosong Sun, Feifei
Zhai, Jingfang Xu, Min Zhang, and Yang Liu.
2018a. Improving the transformer translation model
with document-level context. In Proceedings of the
2018 Conference on Empirical Methods in Natu-
ral Language Processing, pages 533–542, Brussels,
Belgium. Association for Computational Linguis-
tics.

Wen Zhang, Yang Feng, Fandong Meng, Di You, and
Qun Liu. 2019. Bridging the gap between train-
ing and inference for neural machine translation. In
Proceedings of the 57th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 4334–
4343. Association for Computational Linguistics.

Wen Zhang, Jiawei Hu, Yang Feng, and Qun Liu.
2018b. Refining source representations with rela-
tion networks for neural machine translation. In
Proceedings of the 27th International Conference on
Computational Linguistics, pages 1292–1303.

https://doi.org/10.18653/v1/D18-1350
https://doi.org/10.18653/v1/D18-1350
https://doi.org/10.18653/v1/D18-1350
https://www.aclweb.org/anthology/D18-1049
https://www.aclweb.org/anthology/D18-1049

