



















































Combining Formal and Distributional Models of Temporal and Intensional Semantics


Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 28–32,
Baltimore, Maryland USA, June 26 2014. c©2014 Association for Computational Linguistics

Combining Formal and Distributional Models of Temporal and
Intensional Semantics

Mike Lewis
School of Informatics

University of Edinburgh
Edinburgh, EH8 9AB, UK
mike.lewis@ed.ac.uk

Mark Steedman
School of Informatics

University of Edinburgh
Edinburgh, EH8 9AB, UK

steedman@inf.ed.ac.uk

Abstract
We outline a vision for computational se-
mantics in which formal compositional
semantics is combined with a powerful,
structured lexical semantics derived from
distributional statistics. We consider how
existing work (Lewis and Steedman, 2013)
could be extended with a much richer
lexical semantics using recent techniques
for modelling processes (Scaria et al.,
2013)—for example, learning that visit-
ing events start with arriving and end with
leaving. We show how to closely inte-
grate this information with theories of for-
mal semantics, allowing complex compo-
sitional inferences such as is visiting→has
arrived in but will leave, which requires
interpreting both the function and content
words. This will allow machine reading
systems to understand not just what has
happened, but when.

1 Combined Distributional and Logical
Semantics

Distributional semantics aims to induce the mean-
ing of language from unlabelled text. Traditional
approaches to distributional semantics have repre-
sented semantics in vector spaces (Baroni et al.,
2013). Words are assigned vectors based on col-
locations in large corpora, and then these vectors
a composed into vectors representing longer utter-
ances. However, so far there is relatively limited
empirical evidence that composed vectors provide
useful representations for whole sentences, and it
is unclear how to represent logical operators (such
as universal quantifiers) in a vector space. While
future breakthroughs may overcome these limita-
tions, there are already well developed solutions in
the formal semantics literature using logical rep-
resentations. On the other hand, standard for-
mal semantic approaches such as Bos (2008) have

found that hand-built ontologies such as Word-
Net (Miller, 1995) provide an insufficient model
of lexical semantics, leading to low recall on appli-
cations. The complementary strengths and weak-
nesses of formal and distributional semantics mo-
tivate combining them into a single model.

In Lewis and Steedman (2013), we proposed
a solution to these problems which uses CCG
(Steedman, 2012) as a model of formal semantics,
making it straightforward to build wide-coverage
logical forms. Hand built representations are
added for a small number of function words such
as negatives and quantifiers—but the lexical se-
mantics is represented by first clustering predi-
cates (based on their usage in large corpora), and
then using the cluster-identifiers as symbols in the
logical form. For example, the induced CCG lexi-
con might contain entries such as the following1:
write ` (S\NP)/NP

: λyλxλe.rel43(x, y, e)
author `N/PPof

: λyλxλe.rel43(x, y, e)
Equivalent sentences like Shakespeare wrote

Macbeth and Shakespeare is the author of
Macbeth can then both be mapped to a
rel43(shakespeare,macbeth) logical form, us-
ing derivations such as:

Shakespeare wrote Macbeth

NP (S\NP)/NP NP
shakespeare λyλxλe.rel43(x, y, e) macbeth

>
S\NP

λxλe.rel43(x,macbeth, e)
<

S
λe.rel43(shakespeare,macbeth, e)

This approach interacts seamlessly with stan-
dard formal semantics—for example modelling
negation by mapping Francis Bacon didn’t write
Macbeth to ¬rel43(francis bacon,macbeth).
Their method has shown good performance on a
dataset of multi-sentence textual inference prob-
lems involving quantifiers, by using first-order the-
1The e variables are Davidsonian event variables.

28



orem proving. Ambiguity is handled by a proba-
bilistic model, based on the types of the nouns.

Beltagy et al. (2013) use an alternative approach
with similar goals, in which every word instance
expresses a unique semantic primitive, but is con-
nected to the meanings of other word instances us-
ing distributionally-derived probabilistic inference
rules. This approach risks requiring very large
number of inference rules, which may make infer-
ence inefficient. Our approach avoid this problem
by attempting to fully represent lexical semantics
in the lexicon.

2 Proposal

We propose how our previous model could be ex-
tended to make more sophisticated inferences. We
will demonstrate how many interesting problems
in semantics could be solved with a system based
on three components:

• A CCG syntactic parse for modelling com-
position. Using CCG allows us to handle in-
teresting forms of composition, such as co-
ordination, extraction, questions, right node
raising, etc. CCG also has both a developed
theory of operator semantics and a transpar-
ent interface to the underlying predicate ar-
gument structure.

• A small hand built lexicon for words
with complex semantics—such as negatives,
quantifiers, modals, and implicative verbs.

• A rich model of lexical semantics de-
rived from distributionally-induced entail-
ment graphs (Berant et al., 2011), extended
with subcategories of entailment relations in
a similar way to Scaria et al. (2013). We show
how such graphs can be converted into a CCG
lexicon.

2.1 Directional Inference
A major limitation of our previous model is
that it uses a flat clustering to model the
meaning of content words. This method en-
ables them to model synonymy relations be-
tween words, but not relations where the en-
tailment only holds in one direction—for ex-
ample, conquers→invades, but not vice-versa.
This problem can be addressed using the en-
tailment graph framework introduced by Berant
et al. (2011), which learns globally consistent
graphs over predicates in which directed edges

indicate entailment relations. Exactly the same
methods can be used to build entailment graphs
over the predicates derived from a CCG parse:

1 attackarg0,arg1

2
invadearg0,arg1
invasionposs,of

3
conquerarg0,arg1
annexarg0,arg1

4
bombarg0,arg1

The graph can then be converted to a CCG lexi-
con by making the semantics of a word be the con-
junction of all the relation identifiers it implies in
the graph. For example, the above graph is equiv-
alent to the following lexicon:
attack ` (S\NP)/NP

: λxλyλe.rel1(x, y, e)
bomb ` (S\NP)/NP

: λxλyλe.rel1(x, y, e)∧rel4(x, y, e)
invade ` (S\NP)/NP

: λxλyλe.rel1(x, y, e)∧rel2(x, y, e)
conquer` (S\NP)/NP

: λxλyλe.rel1(x, y, e) ∧
rel2(x, y, e) ∧ rel3(x, y, e)

This lexicon supports the correct infer-
ences, such as conquers→attacks and didn’t
invade→didn’t conquer.

2.2 Temporal Semantics

One case where combining formal and distribu-
tional semantics may be particularly helpful is in
giving a detailed model of temporal semantics. A
rich understanding of time would allow us to un-
derstand when events took place, or when states
were true. Most existing work ignores tense, and
would treat the expressions used to be president
and is president either as equivalent or completely
unrelated. Failing to model tense would lead to in-
correct inferences when answering questions such
as Who is the president of the USA?

Another motivation for considering a detailed
model of temporal semantics is that understanding
the time of events should improve the quality of
the distributional clustering. It has recently been
shown that such information is extremely useful
for learning equivalences between predicates, by
determining which sentences describe the same

29



events using date-stamped text and simple tense
heuristics (Zhang and Weld, 2013). Such meth-
ods escape common problems with traditional ap-
proaches to distributional similarity, such as con-
flating causes with effects, and may prove very
useful for building entailment graphs.

Temporal information is conveyed by both by
auxiliary verbs such as will or used to, and in
the semantics of content words. For example, the
statement John is visiting Baltimore licences en-
tailments such as John has arrived in Baltimore
and John will leave Baltimore, which can only be
understood through both knowledge of tense and
lexical semantic relations.

The requisite information about lexical seman-
tics could be represented by labelling edges in the
entailment graphs, along the lines of Scaria et al.
(2013). Instead of edges simply representing en-
tailment, they should represent different kinds of
lexical relations, such as precondition or conse-
quence. Building such graphs requires training
classifiers that predict fine-grained semantic rela-
tions between predicates, and defining transitivity
properties of the relations (e.g. a precondition of a
precondition is a precondition). For example, the
system might learn the following graph:

1 visitarg0,arg1

3
leavearg0,arg1
exitarg0,arg1
departarg0,from

2
arrivearg0,in
reacharg0,arg1

initiated by terminated by

By defining a simple mapping between edge la-
bels and logical forms, this graph can be converted
to CCG lexical entries such as:
visit ` (S\NP)/NP

: λyλxλe.rel1(x, y, e) ∧
∃e′[rel2(x, y, e′)∧ before(e, e′)]∧
∃e′′[rel3(x, y, e′′) ∧ after(e, e′′)]

arrive ` (S\NP)/PPin
: λyλxλe.rel2(x, y, e)

leave ` (S\NP)/NP
: λyλxλe.rel3(x, y, e)

These lexical entries could be complemented
with hand-built interpretations for a small set of
common auxiliary verbs:

has ` (S\NP)/(Sb\NP)
: λpλxλe.before(r, e) ∧ p(x, e)

will ` (S\NP)/(Sb\NP)
: λpλxλe.after(r, e) ∧ p(x, e)

is ` (S\NP)/(Sng\NP)
: λpλxλe.during(r, e) ∧ p(x, e)

used ` (S\NP)/(Sto\NP)
: λpλxλe.before(r, e) ∧ p(x, e) ∧
¬∃e′[during(r) ∧ p(x, e′)]

Here, r is the reference time (e.g. the time that
the news article was written). It is easy to verify
that such a lexicon supports inferences such as is
visiting→will leave, has visited→has arrived in,
or used to be president→is not president.

The model described here only discusses tense,
not aspect—so does not distinguish between John
arrived in Baltimore and John has arrived in Bal-
timore (the latter says that the consequences of his
arrival still hold—i.e. that he is still in Baltimore).
Going further, we could implement the much more
detailed proposal of Moens and Steedman (1988).
Building this model would require distinguishing
states from events—for example, the semantics of
arrive, visit and leave could all be expressed in
terms of the times that an is in state holds.

2.3 Intensional Semantics
Similar work could be done by subcatego-
rizing edges in the graph with other lexi-
cal relations. For example, we could ex-
tend the graph with goal relations between
words, such as between set out for and ar-
rive in, search and find, or invade and conquer:

1
set outarg0,for
headarg0,to

2
arrivearg0,in
reacharg0,arg1

goal

The corresponding lexicon contains entries such
as:
set out ` (S\NP)/PPfor

: λyλxλe.rel1(x, y, e) ∧
�∃e′[goal(e, e′) ∧ rel2(x, y, e′)]

The modal logic � operator is used to mark that
the goal event is a hypothetical proposition, that
is not asserted to be true in the real world—so
Columbus set out for India6→Columbus reached
India. The same mechanism allows us to handle
Montague (1973)’s example that John seeks a uni-
corn does not imply the existence of a unicorn.

Just as temporal information can be expressed
by auxiliary verbs, relations such as goals can

30



Columbus failed to reach India
<

Sdcl/(Sdcl\NP ) (Sdcl\NP )/(Sto\NP ) (Sto\NP )/(Sb\NP ) Sb\NP
λp.p(Columbus) λpλxλe. � ∃e′[p(x, e′) ∧ goal(e′, e)] ∧ ¬∃e′′[p(x, e′′)] λpλxλe.p(x, e) λxλe.rel2(x, India, e)

>
Sto\NP

λxλe.rel2(x, India, e)
>

Sdcl\NP
λxλe. � ∃e′[rel2(x, India, e′) ∧ goal(e′, e)] ∧ ¬∃e′′[rel2(x, India, e′′)]

>
Sdcl

λe. � ∃e′[rel2(Columbus, India, e′) ∧ goal(e′, e)] ∧ ¬∃e′′[rel2(Columbus, India, e′′)]

Figure 1: Output from our system for the sentence Columbus failed to reach India

be expressed using implicative verbs like try or
fail. As the semantics of implicative verbs is of-
ten complex (Karttunen, 1971), we propose hand-
coding their lexical entries:
try ` (S\NP)/(Sto\NP)

: λpλxλe.�∃e′[goal(e, e′)∧p(x, e′)]
fail ` (S\NP)/(Sto\NP)

: λpλxλe.�∃e′[goal(e, e′)∧p(x, e′)]∧
¬∃e′′[goal(e, e′′) ∧ p(x, e′′)]

The � operator is used to assert that the comple-
ment of try is a hypothetical proposition (so try to
reach 6→reach). Our semantics for fail is the same
as that for try, except that it asserts that the goal
event did not occur in the real world.

These lexical entries allow us to make complex
compositional inferences, for example Columbus
failed to reach India now entails Columbus set
out for India, Columbus tried to reach India and
Columbus didn’t arrive in India.

Again, we expect that the improved model of
formal semantics should increase the quality of
the entailment graphs, by allowing us to only clus-
ter predicates based on their real-world arguments
(ignoring hypothetical events).

3 Conclusion

We have argued that several promising recent
threads of research in semantics can be combined
into a single model. The model we have described
would enable wide-coverage mapping of open-
domain text onto rich logical forms that model
complex aspects of semantics such as negation,
quantification, modality and tense—whilst also
using a robust distributional model of lexical se-
mantics that captures the structure of events. Con-
sidering these interwined issues would allow com-
plex compositional inferences which are far be-
yond the current state of the art, and would give
a more powerful model for natural language un-
derstanding.

Acknowledgements

We thank Omri Abend, Michael Roth and the
anonymous reviewers for their helpful comments.
This work was funded by ERC Advanced Fellow-
ship 249520 GRAMPLUS and IP EC-FP7-270273
Xperience.

References
M. Baroni, R. Bernardi, and R. Zamparelli. 2013.

Frege in space: A program for compositional dis-
tributional semantics. Linguistic Issues in Language
Technologies.

Islam Beltagy, Cuong Chau, Gemma Boleda, Dan Gar-
rette, Katrin Erk, and Raymond Mooney. 2013.
Montague meets markov: Deep semantics with
probabilistic logical form. pages 11–21, June.

Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2011. Global learning of typed entailment rules.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies - Volume 1, HLT ’11, pages
610–619. Association for Computational Linguis-
tics.

Johan Bos. 2008. Wide-coverage semantic analy-
sis with boxer. In Johan Bos and Rodolfo Del-
monte, editors, Semantics in Text Processing. STEP
2008 Conference Proceedings, Research in Compu-
tational Semantics, pages 277–286. College Publi-
cations.

L. Karttunen. 1971. The Logic of English Predi-
cate Complement Constructions. Linguistics Club
Bloomington, Ind: IU Linguistics Club. Indiana
University Linguistics Club.

Mike Lewis and Mark Steedman. 2013. Combined
Distributional and Logical Semantics. Transactions
of the Association for Computational Linguistics,
1:179–192.

G.A. Miller. 1995. Wordnet: a lexical database for
english. Communications of the ACM, 38(11):39–
41.

31



Marc Moens and Mark Steedman. 1988. Temporal on-
tology and temporal reference. Computational lin-
guistics, 14(2):15–28.

Richard Montague. 1973. The proper treatment of
quantification in ordinary english. In Approaches to
natural language, pages 221–242. Springer.

Aju Thalappillil Scaria, Jonathan Berant, Mengqiu
Wang, Peter Clark, Justin Lewis, Brittany Harding,
and Christopher D. Manning. 2013. Learning bi-
ological processes with global constraints. In Pro-
ceedings of EMNLP.

Mark Steedman. 2012. Taking Scope: The Natural
Semantics of Quantifiers. MIT Press.

Congle Zhang and Daniel S Weld. 2013. Harvest-
ing parallel news streams to generate paraphrases of
event relations.

32


