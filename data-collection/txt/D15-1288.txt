



















































Compact, Efficient and Unlimited Capacity: Language Modeling with Compressed Suffix Trees


Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2409–2418,
Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.

Compact, Efficient and Unlimited Capacity: Language Modeling with
Compressed Suffix Trees

Ehsan Shareghi,[ Matthias Petri,\ Gholamreza Haffari[ and Trevor Cohn\
[ Faculty of Information Technology, Monash University

\ Computing and Information Systems, The University of Melbourne
first.last@{monash.edu,unimelb.edu.au}

Abstract

Efficient methods for storing and querying
language models are critical for scaling to
large corpora and high Markov orders. In
this paper we propose methods for mod-
eling extremely large corpora without im-
posing a Markov condition. At its core,
our approach uses a succinct index – a
compressed suffix tree – which provides
near optimal compression while support-
ing efficient search. We present algorithms
for on-the-fly computation of probabilities
under a Kneser-Ney language model. Our
technique is exact and although slower
than leading LM toolkits, it shows promis-
ing scaling properties, which we demon-
strate through∞-order modeling over the
full Wikipedia collection.

1 Introduction

Language models (LMs) are critical components
in many modern NLP systems, including machine
translation (Koehn, 2010) and automatic speech
recognition (Rabiner and Juang, 1993). The most
widely used LMs are mgram models (Chen and
Goodman, 1996), based on explicit storage of
mgrams and their counts, which have proved
highly accurate when trained on large datasets. To
be useful, LMs need to be not only accurate but
also fast and compact.

Depending on the order and the training corpus
size, a typical mgram LM may contain as many
as several hundred billions of mgrams (Brants
et al., 2007), raising challenges of efficient stor-
age and retrieval. As always, there is a trade-off
between accuracy, space, and time, with recent
papers considering small but approximate lossy
LMs (Chazelle et al., 2004; Talbot and Osborne,
2007; Guthrie and Hepple, 2010), or loss-less
LMs backed by tries (Stolcke et al., 2011), or re-
lated compressed structures (Germann et al., 2009;

Heafield, 2011; Pauls and Klein, 2011; Sorensen
and Allauzen, 2011; Watanabe et al., 2009). How-
ever, none of these approaches scale well to very
high-order m or very large corpora, due to their
high memory and time requirements. An impor-
tant exception is Kennington et al. (2012), who
also propose a language model based on a suffix
tree which scales well with m but poorly with the
corpus size (requiring memory of about 20× the
training corpus).

In contrast, we1 make use of recent advances in
compressed suffix trees (CSTs) (Sadakane, 2007)
to build compact indices with much more mod-
est memory requirements (≈ the size of the cor-
pus). We present methods for extracting frequency
and unique context count statistics for mgram
queries from CSTs, and two algorithms for com-
puting Kneser-Ney LM probabilities on the fly us-
ing these statistics. The first method uses two
CSTs (over the corpus and the reversed corpus),
which allow for efficient computation of the num-
ber of unique contexts to the left and right of an
mgram, but is inefficient in several ways, most
notably when computing the number of unique
contexts to both sides. Our second method ad-
dresses this problem using a single CST backed by
a wavelet tree based FM-index (Ferragina et al.,
2007), which results in better time complexity and
considerably faster runtime performance.

Our experiments show that our method is prac-
tical for large-scale language modelling, although
querying is substantially slower than a SRILM
benchmark. However our technique scales much
more gracefully with Markov order m, allowing
unbounded ‘non-Markov’ application, and enables
training on large corpora as we demonstrate on the
complete Wikipedia dump. Overall this paper il-
lustrates the vast potential succinct indexes have

1For the implementation see: https://github.com/eehsan/
lm-sdsl.

2409



for language modelling and other ‘big data’ prob-
lems in language processing.

2 Background

Suffix Arrays and Suffix Trees Let T be a
string of size n drawn from an alphabet Σ of size
σ. Let T [i..n − 1] be a suffix of T . The suffix
tree (Weiner, 1973) of T is the compact labeled
tree of n + 1 leaves where the root to leaf paths
correspond to all suffixes of T $, where $ is a ter-
minating symbol not in Σ. The path-label of each
node v corresponds to the concatenation of edge
labels from the root node to v. The node depth
of v corresponds to the number of ancestors in the
tree, whereas the string depth corresponds to the
length of the path-label. Searching for a pattern α
of size m in T translates to finding the locus node
v closest to the root such that α is a prefix of the
path-label of v in O(m) time. We refer to this ap-
proach as forward search. Figure 1a shows a suffix
tree over a sample text. A suffix tree requiresO(n)
space and can be constructed inO(n) time (Ukko-
nen, 1995). The children of each node in the suffix
tree are lexicographically ordered by their edge la-
bels. The i-th smallest suffix in T corresponds to
the path-label of the i-th leaf. The starting position
of the suffix can be associated its corresponding
leaf in the tree as shown in Figure 1a. All occur-
rences of α in T can be retrieved by visiting all
leaves in the subtree of the locus of α. For exam-
ple, pattern “the night” occurs at positions 12 and
19 in the sample text. We further refer the number
of children of a node v as its degree and the num-
ber of leaves in the subtree rooted at v as the size
of v.

The suffix array (Manber and Myers, 1993) of
T is an array SA[0 . . . n− 1] such that SA[i] corre-
sponds to the starting position of the i-th smallest
suffix in T or the i-th leaf in the suffix tree of T .
The suffix array requires n log n bits of space and
can also be constructed in O(n) time (Kärkkäinen
et al., 2006). Using only the suffix array and the
text, pattern search can be performed using bi-
nary search in O(m log n) time. For example, the
pattern “the night” is found by performing binary
search using SA and T to determine SA[18, 19], the
interval in SA corresponding the the suffixes in T
prefixed by the pattern. In practice, suffix arrays
use 4 − 8n bytes of space whereas the most ef-
ficient suffix tree implementations require at least
20n bytes of space (Kurtz, 1999) which are both

much larger than T and prohibit the use of these
structures for all but small data sets.

Compressed Suffix Structures Reducing the
space usage of suffix based index structure has
recently become an active area of research. The
space usage of a suffix array can be reduced sig-
nificantly by utilizing the compressibility of text
combined with succinct data structures. A suc-
cinct data structure provides the same function-
ality as an equivalent uncompressed data struc-
ture, but requires only space equivalent to the
information-theoretic lower bound of the underly-
ing data. For simplicity, we focus on the FM-Index
which emulates the functionality of a suffix array
over T using nHk(T ) + o(n log σ) bits of space
where Hk refers to the k-th order entropy of the
text (Ferragina et al., 2007). In practice, the FM-
Index of T uses roughly space equivalent to the
compressed representation of T using a standard
compressor such as bzip2. For a more compre-
hensive overview on succinct text indexes, see the
excellent survey of Ferragina et al. (2008).

The FM-Index relies on the duality between
the suffix array and the BWT (Burrows and
Wheeler, 1994), a permutation of the text such that
T bwt[i] = T [SA[i] − 1] (see Figure 1). Search-
ing for a pattern using the FM-Index is performed
in reverse order by performing RANK(T bwt, i, c)
operations O(m) times. Here, RANK(T bwt, i, c)
counts the number of times symbol c occurs in
T bwt[0 . . . i − 1]. This process is usually referred
to as backward search. Let SA[li, ri] be the in-
terval corresponding to the suffixes in T match-
ing α[i . . .m − 1]. By definition of the BWT,
T bwt[li, ri] corresponds to the symbols in T pre-
ceding α[i . . .m − 1] in T . Due to the lexico-
graphical ordering of all suffixes in SA, the interval
SA[li−1, ri−1] corresponding to all occurrences of
α[i − 1 . . .m − 1] can be determined by comput-
ing the rank of all occurrences of c = α[i − 1] in
T bwt[li, ri]. Thus, we compute RANK(T bwt, li, c),
the number of times symbol c occurs before li and
RANK(T bwt, ri + 1, c), the number of occurrences
of c in T bwt[0, ri]. To determine SA[li−1, ri−1],
we additionally store the starting positions Cs of
all suffixes for each symbol s in Σ at a negligi-
ble cost of σ log n bits. Thus, the new interval
is computed as li−1 = Cc+RANK(T bwt, li, c) and
ri−1 = Cc+RANK(T bwt, ri + 1, c).

The time and space complexity of the FM-
index thus depends on the cost of storing and pre-

2410



22

2

10

21

11

0

18

8

17

7

14

4

15

5

20

13

3

16

6

19

12

1

9

$

#

th
e
in

in
ke
ep

.
.
k
e
e
p
e
r

k
e
e
p
s

n
i
g
h
t

old..$

the

town..$

$
t
h
e

$
.
.
n
i
g
h
t ol

d
n
i
g
h
t
.
.
$

$#
ni
gh
t

$
.
.
k
e
e
p
e
r
n
i
g
h
t
t
h
e
#
t
o
w
n

$#n
igh

t

.
.
n
i
g
h
t
t
h
e
t
o
w
n

$#
ni
gh
t

.
.
t
h
e
t
o
w
n

$
#
n
i
g
h
t

t
o
w
n
#
t
h
e
n
i
g
h
t
.
.

$#

k
e
e
p
e
r
.
.

$
#
n
i
g
h
t

t
o
w
n
#
t
h
e
.
.
$

t
h
e
i
n
k
e
e
p

n
i
g
h
t

$
#

k
e
e
p
e
r
k
e
e
p
s
.
.
$

old..$

town..$

$#
ni
gh
t t

o
w
n
#
t
h
e
n
i
g
h
t
.
.
$

(a) Word-based Suffix Tree.

#nT$ppttnnrrttotssi##it
01100011110011111100001

#$pprri##i
0011110000

nTttnnttotsst
0111001101001

#$i##i
001001

pprr
0011

p r
#$##
1011

i

$ #

nnnoss
000100

Ttttttt
1000000

t T
nnnss
11100

o

s n
10

0 10 1

10

1 10 0

0 110

0 1

(b) Wavelet tree and RANK(T bwt, 17, ‘t’) = 5.
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22

$ # # # i i p p r r s s n n n o t t t t t t T

22 21 11 0 18 8 17 7 14 4 15 5 20 13 3 2 16 6 19 12 1 9 10

# n T $ p p t t n n r r t t o t s s i # # i t

T [SA[i]]
SA

T bwt

Figure 1: Data structures for the sample text T =“#the old night keeper keeps the keep in the town#
the night keeper keeps the keep in the night#$” with alphabet Σ={the, old, night, keeper, keeps, keep,
in, town, #} and code words $=0000, #=0001, i=in=001, p=keep=010, r=keeper=011, s=keeps=1000,
o=old=101, t=the=110, n=night=1001 and T=town=111.

processing T bwt to answer RANK efficiently. A
wavelet tree can be used to answer RANK over
T bwt in O(log σ) time. The wavelet tree re-
duces RANK over an alphabet Σ into multiple
RANK operations over a binary alphabet which
can be answered in O(1) time and o(n) bits
extra space by periodically storing absolute and
relative RANK counts (Munro, 1996). The al-
phabet is reduced by recursively splitting sym-
bols based on their code words into subgroups
to form a binary tree as shown in Figure 1b
for T bwt. To answer RANK(T bwt, i, c), the tree
is traversed based on the code word of c, per-
forming binary RANK at each level. For exam-
ple, RANK(T bwt, 17, ‘t’) translates to performing
RANK(WTroot, 17, 1) = 12 on the top level of
the wavelet tree, as t=the=110. We recurse
to the right subtree of the root node and com-
pute RANK(WT1, 12, 1) as there were 12 ones
in the root node and the next bit in the code-
word of ‘the’ is also one. This process contin-
ues until the correct leaf node is reached to answer
RANK(T bwt, 17, ‘t’) = 5 in O(log σ) time. The
space usage of a regular wavelet tree is n log σ +
o(n log σ) bits which roughly matches the size of
the text.2 If locations of matches are required, ad-

2However, if code-words for each symbol are chosen
based on their Huffman-codes the size of the wavelet tree

ditional space is needed to access SA[i] or the in-
verse suffix array SA−1[SA[i]] = i. In the sim-
plest scheme, both values are periodically sam-
pled using a given sample rate SAS (e.g. 32) such
that SA[i] mod SAS = 0. Then, for any SA[i]
or SA−1[i], at most O(SAS) RANK operations on
T bwt are required to access the value. Differ-
ent sample rates, bitvector implementations and
wavelet tree types result in a wide variety of time-
space tradeoffs which can be explored in prac-
tice (Gog et al., 2014).

In the same way the FM-index emulates the
functionality of the suffix array in little space,
compressed suffix trees (CST) provide the func-
tionality of suffix trees while requiring signifi-
cantly less space than their uncompressed coun-
terparts (Sadakane, 2007). A CST uses a com-
pressed suffix array (CSA) such as the FM-Index
but stores additional information to represent the
shape of the suffix tree as well as information
about path-labels. Again a variety of different stor-
age schemes exist, however for simplicity we fo-
cus on the CST of Ohlebusch et al. (2010) which
we use in our experiments. Here, the shape of the
tree is stored using a balanced-parenthesis (BP) se-
quence which for a tree of p nodes requires ≈ 2p
reduces to nH0(T )(1 + o(1)) bits which can be further be
reduced to to nHk(T ) + o(n log σ) bits by using entropy
compressed bitvectors.

2411



bits. Using little extra space and advanced bit-
operations, the BP-sequence can be used to per-
form operations such as string-depth(v), parent(v)
or accessing the i-th leaf can be answered in con-
stant time. To support more advanced operations
such as accessing path-labels, the underlying CSA
or a compressed version of the LCP array are re-
quired which can be more expensive.3 In practice,
a CST requires roughly 4 − 6n bits in addition to
the cost of storing the CSA. For a more extensive
overview of CSTs see Russo et al. (2011).

Kneser Ney Language Modelling Recall our
problem of efficient mgram language modeling
backed by a corpus encoded in a succinct index.
Although our method is generally applicable to
many LM variants, we focus on the Kneser-Ney
LM (Kneser and Ney, 1995), specifically the inter-
polated variant described in Chen and Goodman
(1996), which has been shown to outperform other
ngram LMs and has become the de-facto standard.

Interpolated Kneser-Ney describes the condi-
tional probability of a word wi conditioned on the
context of m− 1 preceding words, wi−1i−m+1, as

P (wi|wi−1i−m+1) =
max

[
c(wii−m+1)−Dm, 0

]
c(wi−1i−m+1)

+
DmN

1+(wi−1i−m−1 ·)
c(wi−1i−m+1)

P̄ (wi|wi−1i−m+2), (1)

where lower-order smoothed probabilities are de-
fined recursively (for 1 < k < m) as

P̄ (wi|wi−1i−k+1) =
max

[
N1+(·wii−k+1)−Dk, 0]
N1+(·wi−1i−k+1 ·)

+
DkN

1+(wi−1i−k+1 ·)
N1+(·wi−1i−k+1 ·) P̄ (wi|wi−1i−k+2) . (2)

In the above formula, Dk is the kgram-specific
discount parameter, and the occurrence count
N1+(α·) = |{w : c(αw) > 0}| is the number of
observed word types following the pattern α; the
occurrence counts N1+(·α) and N1+(·α·) are
defined accordingly. The recursion stops at uni-
gram level where the unigram probabilities are de-
fined as P̄ (wi) = N1+(·wi)/N1+(··).4

3See Supplementary Materials Table 1 for an overview of
the complexities of the functionality of the CST that is used
in our experiments.

4Modified Kneser-Ney, proposed by Chen and Good-
man (1996), typically outperforms interpolated Kneser-Ney
through its use of context-specific discount parameters. The

3 Using CSTs for KN Computation

The key requirements for computing probability
under a Kneser-Ney language model are two types
of counts: raw frequencies of mgrams and occur-
rence counts, quantifying how many different con-
texts the mgram has occurred in. Figure 2 (right)
illustrates the requisite counts for calculating the
probability of an example 4-gram. In electing to
store the corpus directly in a suffix tree, we need to
provide mechanisms for computing these counts
based on queries into the suffix tree.

The raw frequency counts are the simplest to
compute. First we identify the locus node v in
the suffix tree for the query mgram; the frequency
corresponds to the node’s size, an O(1) operation
which returns the number of leaves below v. To il-
lustrate, consider searching for c(the night) in Fig-
ure 1a, which matches a node with two leaves (la-
belled 19 and 12), and thus c = 2.

More problematic are the occurrence counts,
which come in several flavours: right contexts,
N1+(α·), left contexts, N1+(·α), and contexts
to both sides of the pattern, N1+(·α·). The first
of these can be handled easily, as

N1+(α·) = { degree(v), if α = label(v)1, otherwise
where v is the node matching α, and label(v) de-
notes the path-label of v.5 For example, keep
in has two child nodes in Figure 1a, and thus
there are two unique contexts in which it can oc-
cur, N1+(keep in ·) = 2, while the keep par-
tially matches an edge in the forward suffix tree
in Figure 1a as it can only be followed by in,
N1+(the keep ·) = 1. A similar line of reason-
ing applies to computing N1+(·α). Assuming
we also have a second suffix tree representing the
reversed corpus, we first identify the reversed pat-
tern (e.g., in keepR) and then use above method to
compute the occurrence count (denoted hereafter
N1P(t, v, α)6, where t is the CST.).

implementation of this with our data structures is straight-
forward in principle, but brings a few added complexities
in terms of dynamic computing other types of occurrence
counts, which we leave for future work.

5See the Supplementary Materials for the explicit algo-
rithm, but note there are some corner cases involving sen-
tinels # and $, which must be excluded when computing oc-
currence counts. Such tests have been omitted from the pre-
sentation for clarity.

6In the presented algorithms, we overload the pattern ar-
gument in function calls for readability, and use · to denote
the query context.

2412



root:%[0,n] root:%[0,n] root:%[0,n]

town,

the%town,

in%the%town,

keep%in%the%town,

the,

in%the,

the2

in%the2

keep%in%the2

c(keep%in%the%town) c(keep%in%the) N¹⁺(keep%in%the%•)

N¹⁺(•%in%the%town) N¹⁺(•%in%the%•) N¹⁺(in%the%•)

N¹⁺(•%the%town) N¹⁺(•%the%•) N¹⁺(the%•)

N¹⁺(•%town) N¹⁺(••)

Figure 2: Counts required for computing P (town|keep in the) (right) and the suffix tree nodes required
for computing each value (left). The two left-most columns correspond to vallR and vR and are updated
using forward-search in the reverse CST, while the righter-most column correspond to vF and is updated
using backward-search in the forward CST. See Algorithm 2 for details.

The final component of the Kneser-Ney LM
computation isN1+(·α·), the number of unique
contexts considering symbols on both sides of the
pattern. Unfortunately this does not map to a sim-
ple suffix tree operation, but instead requires enu-
meration, N1+(·α·) = ∑s∈F (α)N1+(·αs),
where F (α) is the set of symbols that can follow
α. Algorithm 1 shows how this is computed, with
lines 7 and 8 enumerating s ∈ F (α) using the
edge labels of the children of v. For each symbol,
line 9 searches for an extended pattern incorporat-
ing the new symbol s in the reverse CSA (part of
the reverse CST), by refining the existing match
vR using a single backward search operation af-
ter which we can compute N1+(·αs).7 Line 5
deals with the special case where the pattern does
not match a complete edge, in which case there
is only only unique right context and therefore
N1+(·α·) = N1+(·α).

N1P and N1PFRONTBACK can compute the
requisite occurence counts for mgram language
modelling, however at considerable cost in terms
of space and time. The need for twin reverse and
forward CSTs incurs a significant storage over-
head, as well as the search time to match the pat-
tern in both CSTs. We show in Section 5 how
we can avoid the need for the reversed suffix tree,
giving rise to lower memory requirements and
faster runtime. Beyond the need for twin suf-
fix trees, the highest time complexity calls are
string-depth, edge and backward-search. Calling
string-depth is constant time for internal nodes,
but O(SAS log σ) for leaf nodes; fortunately we

7Backward search in the reverse tree corresponds to
searching for the reversed pattern appended with one symbol.

Algorithm 1 Two-sided occ., N1+(·α·)
Precondition: vF in forward CST tF matches α
Precondition: vR in reverse CST tR matches α

1: function N1PFRONTBACK(tF, vF, tR, vR, α)
2: o← 0
3: d← string-depth(vF)
4: if d > |α| then
5: o← N1P(tR, vR,·α)
6: else
7: for uF ← children(vF) do
8: s← edge(uF, d+ 1)
9: uR← back-search(vR, s)

10: o← o+ N1P(tR, uR,·αs)
11: return o

can avoid this call for leaves, which by definition
extend to the end of the corpus and consequently
extend further than our pattern.8 The costly calls
to edge and backward-search however cannot be
avoided. This leads to an overall time complex-
ity of O(1) for N1P and O(F (α) × SAS × log σ)
for N1PFRONTBACK, where F (α) is the number
of following symbols and SAS is the suffix array
value sample rate described in Section 2.

4 Dual CST Algorithm

The methods above for computing the frequency
and occurrence counts provide the ingredients
necessary for computing mgram language model
probabilities. This leaves the algorithmic problem

8We assume search patterns do not extend beyond a single
sentence, and thus will always be shorter than the edge labels.

2413



Algorithm 2 KN probability P
(
wk|wk−1k−(m−1)

)
1: function PROBKNESERNEY(tF, tR,w,m)
2: vF ← root(tF) . match for suffix of wk−1k−(m−1)
3: vR ← root(tR) . match for suffix of wk−1k−(m−1)
4: vallR ← root(tR) . match for suffix of wkk−(m−1)
5: p← 1
6: for i← 1 to m do
7: vallR ← forw-search(vallR , wk−i+1)
8: if i > 1 then
9: vF ← back-search(vF, wk−i+1)

10: if i < m then
11: vR ← forw-search(vR, wk−i+1)
12: Di← lookup discount for igram
13: if i = m then
14: c← size(vallR )
15: d← size(vF)
16: else
17: c← N1P(tR, vallR ,·wkk−i+1)
18: d←

N1PFRONTBACK(tF, vF, tR, vR,·wk−1k−i+1 ·)
19: if i > 1 then
20: if vF is valid then
21: q← N1P(tF, vF, wk−1k−i+1 ·)
22: p← 1

d
(max(c−Di, 0) +Diqp)

23: else if i = 1 then
24: p← c/N1+(··)
25: return p

of efficiently ordering the search operations in for-
ward and reverse CST structures.

This paper considers an interpolated LM for-
mulation, in which probabilities from higher or-
der contexts are interpolated with lower order es-
timates. This iterative process is apparent in Fig-
ure 2 (right) which shows the quantities required
for probability scoring for an example mgram.
Equivalently, the iteration can be considered in re-
verse, starting from unigram estimates and suc-
cessively growing to large mgrams, in each stage
adding a single new symbol to left of the pattern.
This suits incremental search in a CST in which
search bounds are iteratively refined, which has a
substantially lower time complexity compared to
searching over the full index in each step.

Algorithm 2 presents an outline of the approach.
This uses a forward CST, tF, and a reverse CST,
tR, with three CST nodes (lines 2–4) tracking the
match progress for the full igram (vallR ) and the
(i − 1)gram context (vF, vR), i = 1 . . .m. The
need to maintain three concurrent searches arises
from the calls to size, N1+(·α), N1+(α·) and
N1+(·α·) (lines 14, 15; 17; 21; and 18, respec-
tively). These calls impose conditions on the di-
rection of the suffix tree, e.g., such that the edge
labels and node degree can be used to compute

Algorithm 3 Precompute KN discounts
1: function PRECOMPUTEDISCOUNTS(tR,m)
2: ck,f ← 0 ∀k ∈ [1,m], f ∈ [1, 2]
3: N1k,g ← 0 ∀k ∈ [1,m], g ∈ [1, 2]
4: N1+(··)← 0
5: for vR ← descendents(root(tR)) do . depth-first
6: dP ← string-depth(parent(vR))
7: d← string-depth(vR)
8: for k ← dP + 1 to min (d, dP +m) do
9: s← edge(vR, k)

10: if s is the end of sentence sentinel then
11: skip all children of vR
12: else
13: if k = 2 then
14: N1+(··)← N1+(··) + 1
15: f ← size(vR)
16: if 1 ≤ f ≤ 2 then
17: ck,f ← ck,f + 1
18: if k < d then
19: g← 1
20: else
21: g← degree(vR)
22: if 1 ≤ g ≤ 2 then
23: N1k,g ← N1k,g + 1
24: return c,N1, N1+(··)
the number of left or right contexts in which a
pattern appears. The matching process is illus-
trated in Figure 2 where the three search nodes are
shown on the left, considered bottom to top, and
their corresponding count operations are shown to
the right. The N1+(·α) calls require a match
in the reverse CST (left-most column, vallR ), while
the N1+(α·) require a match in the forward CST
(right-most column, vF, matching the (i− 1)gram
context). The N1+(·α·) computation reuses the
forward match while also requiring a match for the
(i−1)gram context in the reversed CST, as tracked
by the middle column (vR). Because of the mix
of forward and reverse CSTs, coupled with search
patterns that are revealed right-to-left, incremen-
tal search in each of the CSTs needs to be han-
dled differently (lines 7–11). In the forward CST,
we perform backward search to extend the search
pattern to the left, which can be computed very ef-
ficiently from the BWT in the CSA.9 Conversely
in the reverse CST, we must use forward search as
we are effectively extending the reversed pattern
to the right; this operation is considerably more
costly.

The discounts D on line 12 of Algorithm 2 and
N1+(··) (a special case of line 18) are precom-
puted directly from the CSTs thus avoiding several
costly computations at runtime. The precomputa-

9See Supplementary Materials Table 1 for the time com-
plexities of this and other CSA and CST methods.

2414



tion algorithm is provided in Algorithm 3 which
operates by traversing the nodes of the reverse
CST and at each stage computing the number of
mgrams that occur 1–2 times (used for computing
Dm in eq. 1), or with N1+(·α) ∈ [1 − 2] (used
for computing Dk in eq. 2), for various lengths
of mgrams. These quantities are used to compute
the discount parameters, which are then stored for
later use in inference.10 Note that the PRECOM-
PUTEDISCOUNTS algorithm can be slow, although
it is significantly faster if we remove the edge calls
and simply include in our counts all mgrams fin-
ishing a sentence or spanning more than one sen-
tence. This has a negligible (often beneficial) ef-
fect on perplexity.

5 Improved Single CST Approach

The above dual CST algorithm provides an el-
egant means of computing LM probabilities of
arbitrary order and with a limited space com-
plexity (O(n), or roughly n in practice). How-
ever the time complexity is problematic, stem-
ming from the expensive method for computing
N1PFRONTBACK and repeated searches over the
CST, particularly forward-search. Now we out-
line a method for speeding up the algorithm by
doing away with the reverse CST. Instead the crit-
ical counts, N1+(·α) and N1+(·α·) are com-
puted directly from a single forward CST. This
confers the benefit of using only backward search
and avoiding redundant searches for the same pat-
tern (cf. lines 9 and 11 in Algorithm 2).

The full algorithm for computing LM prob-
abilities is given in Algorithm 4, however for
space reasons we will not describe this in de-
tail. Instead we will focus on the method’s most
critical component, the algorithm for computing
N1+(·α·) from the forward CST, presented in
Algorithm 5. The key difference from Algorithm 1
is the loop from lines 6–9, which uses the interval-
symbols (Schnattinger et al., 2010) method. This
method assumes a wavelet tree representation of
the SA component of the CST, an efficient encod-
ing of the BWT as describes in section 2. The
interval-symbols method uses RANK operations
to efficiently identify for a given pattern the set of
preceding symbols P (α) and the ranges SA[ls, rs]
corresponding to the patterns sα for all s ∈ P (α)

10Discounts are computed up to a limit on mgram size,
here set to 10. The highest order values are used for comput-
ing the discount of mgrams above the limit at runtime.

Algorithm 4 KN probability P
(
wk|wk−1k−(m−1)

)
using a single CST
1: function PROBKNESERNEY1(tF,w,m)
2: vF ← root(tF) . match for context wk−1k−i
3: vallF ← root(tF) . match for wkk−i
4: p← 1
5: for i← 1 to m do
6: vallF ← back-search([lb(vallF ), rb(vallF )], wk−i+1)
7: if i > 1 then
8: vF ← back-search([lb(vF), rb(vF)], wk−i+1)
9: Di← discount parameter for igram

10: if i = m then
11: c← size(vallF )
12: d← size(vF)
13: else
14: c← N1PBACK1(tF, vallF ,·wk−1k−i+1)
15: d← N1PFRONTBACK1(tF, vF,·wk−1k−i+1 ·)
16: if i > 1 then
17: if vF is valid then
18: q← N1P(tF, vF, wk−1k−i+1 ·)
19: p← 1

d
(max(c−Di, 0) +Diqp)

20: else
21: p← c/N1+(··)
22: return p

Algorithm 5 N1+(·α·), using forward CST
Precondition: vF in forward CST tF matches α
1: function N1PFRONTBACK1(tF, vF, α)
2: o← 0
3: if string-depth(vF) > |α| then
4: o← N1PBACK1(tF, vF,·α)
5: else
6: for 〈l, r, s〉 ← int-syms(tF, [lb(vF), rb(vF)]) do
7: l′← Cs + l
8: r′← Cs + r
9: o← o+ N1P(tF, node(l′, r′), sα·)

10: return o

by visiting all leaves of the wavelet tree of sym-
bols occurring in T bwt[l, r] (corresponding to α)
in O(|P (α)| log σ) time (lines 6-8). These ranges
SA[l′, r′] can be used to find the corresponding suf-
fix tree node for each sα in O(1) time. To illus-
trate, consider the pattern α = “night” in Fig-
ure 1a. From T bwt we can see that this is pre-
ceeded by s =“old” (1st occurrence in T bwt) and
s =“the” (3rd and 4th); from which we can com-
pute the suffix tree nodes, namely [15, 15] and
[16 + (3 − 1), 16 + (4 − 1)] = [18, 19] for “old”
and “the” respectively.11

N1PBACK1 is computed in a similar way, us-
ing the interval-symbols method to compute the
number of unique preceeding symbols (see Sup-
plementary Materials, Algorithm 7). Overall the
time complexity of inference for both N1PBACK1

11Using the offsets into the SA for each symbol, Cold = 15
and Cthe = 16, while −1 adjusts for counting from 1.

2415



Language Size(MiB) Tokens(M) Word Types Sentences(K)

BG 36.11 8.53 114930 329
CS 53.48 12.25 174592 535
DE 171.80 44.07 399354 1785
EN 179.15 49.32 124233 1815
FI 145.32 32.85 721389 1737
FR 197.68 53.82 147058 1792
HU 52.53 12.02 318882 527
IT 186.67 48.08 178259 1703
PT 187.20 49.03 183633 1737

Wikipedia 8637 9057 196 87835

Table 1: Dataset statistics, showing total un-
compressed size; and tokens, types and sentence
counts for the training partition. For Wikipedia
the Word Types, and Tokens are computed based
on characters.

and N1PFRONTBACK1 is O(P (α) log σ) where
P (α) is the number of preceeding symbols of α, a
considerable improvement over N1PFRONTBACK
using the forward and reverse CSTs. Overall
this leads to considerably faster computation of
mgram probabilities compared to the two CST ap-
proach, and although still slower than highly opti-
mised LM toolkits like SRILM, it is fast enough to
support large scale experiments, and has consider-
ably better scaling performance with the Markov
order m (even allowing unlimited order), as we
will now demonstrate.

6 Experiments

We used Europarl dataset and the data was num-
berized after tokenizing, splitting, and excluding
XML markup. The first 10k sentences were used
as the test data, and the last 80% as the train-
ing data, giving rise to training corpora of be-
tween 8M and 50M tokens and uncompressed size
of up to 200 MiB (see Table 1 for detailed cor-
pus statistics). We also processed the full 52 GiB
uncompressed “20150205” English Wikipedia ar-
ticles dump to create a character level language
model consisting of 72M sentences. We excluded
10k random sentences from the collection as test
data. We use the SDSL library (Gog et al., 2014) to
implement all our structures and compare our in-
dexes to SRILM (Stolcke, 2002). We refer to our
dual-CST approach as D-CST, and the single-CST
as S-CST.

We evaluated the perplexity across different lan-
guages and using mgrams of varying order from
m = 2 to∞ (unbounded), as shown on Figure 3.
Our results matched the perplexity results from

100%

102%

105%

110%

120%

150%

200%

2 3 4 5 6 7 8 9 10 15 20 ∞
mgram size

Pe
rp

le
xi

ty
[R

el
at

iv
e

to
∞

] Language (2-gram pplx∞-gram pplx)
BG (2-gram 117.39 ∞-gram 73.01)
CS (2-gram 232.36 ∞-gram 161.15)
DE (2-gram 178.11 ∞-gram 108.27)
EN (2-gram 67.14 ∞-gram 59.92)
FI (2-gram 446.29 ∞-gram 314.22)
FR (2-gram 89.95 ∞-gram 47.91)
HU (2-gram 251.49 ∞-gram 182.18)
IT (2-gram 132.26 ∞-gram 77.80)
PT (2-gram 121.42 ∞-gram 68.58)

Figure 3: Perplexity results on several Europarl
languages for different mgram sizes, m =
2 . . . 10, 15, 20,∞.

SRILM (for smaller values of m in which SRILM
training was feasible, m ≤ 10). Note that perplex-
ity drops dramatically from m = 2 . . . 5 however
the gains thereafter are modest for most languages.
Despite this, several large mgram matches were
found ranging in size up to a 34-gram match. We
speculate that the perplexity plateau is due to the
simplistic Kneser-Ney discounting formula which
is not designed for higher order mgram LMs and
appear to discount large mgrams too aggressively.
We leave further exploration of richer discounting
techniques such as Modified Kneser-Ney (Chen
and Goodman, 1996) or the Sequence Memoizer
(Wood et al., 2011) to our future work.

Figure 4 compares space and time of our in-
dexes with SRILM on the German part of Eu-
roparl. The construction cost of our indexes in
terms of both space and time is comparable to
that of a 3/4-gram SRILM index. The space us-
age of D-CST index is comparable to a compact
3-gram SRILM index. Our S-CST index uses only
177 MiB RAM at query time, which is compara-
ble to the size of the collection (172 MiB). How-
ever, query processing is significantly slower for
both our structures. For 2-grams, D-CST is 3 times
slower than a 2-gram SRILM index as the expen-
sive N1+(·α·) is not computed. However, for
large mgrams, our indexes are much slower than
SRILM. For m > 2, the D-CST index is roughly
six times slower than S-CST. Our fastest index, is
10 times slower than the slowest SRILM 10-gram
index. However, our run-time is independent of
m. Thus, as m increases, our index will become
more competitive to SRILM while using a constant
amount of space.

2416



Construction Cost Query Cost

2

3

4
5

6
7

89
10

2

3

4
5

6
7 8

910

2−∞
2−∞

2

3− 5

2

3−∞

2

3

4
5

6
7

8
910

2

3

4
5

6
7

8
910

10

100

1 k

10 k

100 M 1 G 10 G 100 M 1 G 10 G
Space Usage [bytes]

Ti
m

e
[s

ec
]

D-CST S-CST srilm-compact srilm-default

Figure 4: Time versus space tradeoffs measured on
Europarl German (de) dataset, showing memory
and time requirements.

0

500

1000

1500

0

100

200

300

D
-C

S
T

S-C
S

T

2 3 4 5 6 8 10 ∞
mgram size

Ti
m

e
pe

rS
en

te
nc

e
[m

se
c]

N1PFRONTBACK
fw-search
back-search
N1PBACK
N1PFRONT

Figure 5: Runtime breakdown of a single pattern
averaged over all patterns for both methods over
the Wikipedia collection.

Next we analyze the performance of our in-
dex on the large Wikipedia dataset. The S-CST,
character level index for the data set requires
22 GiB RAM at query time whereas the D-CST re-
quires 43 GiB. Figure 5 shows the run-time per-
formance of both indexes for different mgrams,
broken down by the different components of the
computation. As discussed above, 2-gram per-
formance is much faster. For both indexes, most
time is spent computing N1PFRONTBACK (i.e.,
N1+(·α·)) for all m > 2. However, the wavelet
tree traversal used in S-CST roughly reduces the
running time by a factor of three. The complex-
ity of N1PFRONTBACK depends on the number of
contexts, which is likely small for larger mgrams,
but can be large for small mgrams, which sug-
gest partial precomputation could significantly in-
crease the query performance of our indexes. Ex-
ploring the myraid of different CST and CSA con-
figurations available could also lead to significant

improvements in runtime and space usage also re-
mains future work.

7 Conclusions

This paper has demonstrated the massive poten-
tial that succinct indexes have for language mod-
elling, by developing efficient algorithms for on-
the-fly computing of mgram counts and language
model probabilities. Although we only consid-
ered a Kneser-Ney LM, our approach is portable to
the many other LM smoothing method formulated
around similar count statistics. Our complexity
analysis and experimental results show favourable
scaling properties with corpus size and Markov or-
der, albeit running between 1-2 orders of magni-
tude slower than a leading count-based LM. Our
ongoing work seeks to close this gap: preliminary
experiments suggest that with careful tuning of the
succinct index parameters and caching expensive
computations, query time can be competitive with
state-of-the-art toolkits, while using less memory
and allowing the use of unlimited context.

Acknowledgments

Ehsan Shareghi and Gholamreza Haffari are grate-
ful to National ICT Australia (NICTA) for gen-
erous funding, as part of collaborative machine
learning research projects. Matthias Petri is the
recipient of an Australian Research Councils Dis-
covery Project scheme (project DP140103256).
Trevor Cohn is the recipient of an Australian Re-
search Council Future Fellowship (project number
FT130101105).

References

Thorsten Brants, Ashok C Popat, Peng Xu, Franz J
Och, and Jeffrey Dean. 2007. Large language
models in machine translation. In Proc. EMNLP-
CoNLL.

M. Burrows and D. Wheeler. 1994. A block sorting
lossless data compression algorithm. Technical Re-
port 124, DEC.

Bernard Chazelle, Joe Kilian, Ronitt Rubinfeld, and
Ayellet Tal. 2004. The bloomier filter: An efficient
data structure for static support lookup tables. In
Proc. SODA, pages 30–39.

Stanley F Chen and Joshua Goodman. 1996. An em-
pirical study of smoothing techniques for language
modeling. In Proc. ACL, pages 310–318.

2417



P. Ferragina, G. Manzini, V. Mäkinen, and G. Navarro.
2007. Compressed representations of sequences
and full-text indexes. ACM Trans. on Algorithms,
3(2):article 20.

Paolo Ferragina, Rodrigo González, Gonzalo Navarro,
and Rossano Venturini. 2008. Compressed text in-
dexes: From theory to practice. ACM J. of Exp. Al-
gorithmics, 13.

Ulrich Germann, Eric Joanis, and Samuel Larkin.
2009. Tightly packed tries: How to fit large models
into memory, and make them load fast, too. In Proc.
of the Workshop on Software Engineering, Testing,
and Quality Assurance for Natural Language Pro-
cessing, pages 31–39.

Simon Gog, Timo Beller, Alistair Moffat, and Matthias
Petri. 2014. From theory to practice: Plug and play
with succinct data structures. In Proc. SEA, pages
326–337.

David Guthrie and Mark Hepple. 2010. Storing the
web in memory: Space efficient language models
with constant time retrieval. In Proc. EMNLP, pages
262–272.

Kenneth Heafield. 2011. KenLM: Faster and smaller
language model queries. In Proc. WMT.

Juha Kärkkäinen, Peter Sanders, and Stefan Burkhardt.
2006. Linear work suffix array construction. J.
ACM, 53(6):918–936.

Casey Redd Kennington, Martin Kay, and Annemarie
Friedrich. 2012. Suffix trees as language models.
In Proc. LREC, pages 446–453.

Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In
Proc. ICASSP, volume 1, pages 181–184.

Philipp Koehn. 2010. Statistical Machine Translation.
Cambridge University Press, New York, NY, USA.

Stefan Kurtz. 1999. Reducing the space requirement
of suffix trees. Softw., Pract. Exper., 29(13):1149–
1171.

Udi Manber and Eugene W. Myers. 1993. Suffix
arrays: A new method for on-line string searches.
SIAM J. Comput., 22(5):935–948.

Ian Munro. 1996. Tables. In Proc. FSTTCS, pages
37–42.

Enno Ohlebusch, Johannes Fischer, and Simon Gog.
2010. CST++. In Proc. SPIRE, pages 322–333.

Adam Pauls and Dan Klein. 2011. Faster and smaller
n-gram language models. In Proc. ACL-HLT.

Lawrence Rabiner and Biing-Hwang Juang. 1993.
Fundamentals of speech recognition. Prentice-Hall.

L. Russo, G. Navarro, and A. Oliveira. 2011. Fully-
compressed suffix trees. ACM Trans. Algorithms,
7(4):article 53.

Kunihiko Sadakane. 2007. Compressed suffix trees
with full functionality. Theory Comput. Syst.,
41(4):589–607.

Thomas Schnattinger, Enno Ohlebusch, and Simon
Gog. 2010. Bidirectional search in a string with
wavelet trees. In Proc. CPM, pages 40–50.

Jeffrey Sorensen and Cyril Allauzen. 2011. Unary
data structures for language models. In Proc. IN-
TERSPEECH, pages 1425–1428.

Andreas Stolcke, Jing Zheng, Wen Wang, and Victor
Abrash. 2011. Srilm at sixteen: Update and out-
look. In Proc. ASRU, page 5.

Andreas Stolcke. 2002. SRILM–an extensible lan-
guage modeling toolkit. In Proc. INTERSPEECH.

David Talbot and Miles Osborne. 2007. Randomised
language modelling for statistical machine transla-
tion. In Proc. ACL.

Esko Ukkonen. 1995. On-line construction of suffix
trees. Algorithmica, 14(3):249–260.

Taro Watanabe, Hajime Tsukada, and Hideki Isozaki.
2009. A succinct n-gram language model. In Proc.
ACL Short Papers, pages 341–344.

Peter Weiner. 1973. Linear pattern matching algo-
rithms. In Proc. SWAT, pages 1–11.

Frank Wood, Jan Gasthaus, Cédric Archambeau,
Lancelot James, and Yee Whye Teh. 2011. The se-
quence memoizer. CACM, 54(2):91–98.

2418


