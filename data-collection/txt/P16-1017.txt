



















































Neural Greedy Constituent Parsing with Dynamic Oracles


Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 172–182,
Berlin, Germany, August 7-12, 2016. c©2016 Association for Computational Linguistics

Neural Greedy Constituent Parsing with Dynamic Oracles

Maximin Coavoux1,2 and Benoı̂t Crabbé1,2,3
1Univ. Paris Diderot, Sorbonne Paris Cité

2Alpage, Inria
3Institut Universitaire de France

maximin.coavoux@inria.fr
benoit.crabbe@linguist.univ-paris-diderot.fr

Abstract

Dynamic oracle training has shown sub-
stantial improvements for dependency
parsing in various settings, but has not
been explored for constituent parsing. The
present article introduces a dynamic ora-
cle for transition-based constituent pars-
ing. Experiments on the 9 languages
of the SPMRL dataset show that a neu-
ral greedy parser with morphological fea-
tures, trained with a dynamic oracle, leads
to accuracies comparable with the best
non-reranking and non-ensemble parsers.

1 Introduction

Constituent parsing often relies on search methods
such as dynamic programming or beam search, be-
cause the search space of all possible predictions
is prohibitively large. In this article, we present
a greedy parsing model. Our main contribution
is the design of a dynamic oracle for transition-
based constituent parsing. In NLP, dynamic or-
acles were first proposed to improve greedy de-
pendency parsing training without involving addi-
tional computational costs at test time (Goldberg
and Nivre, 2012; Goldberg and Nivre, 2013).

The training of a transition-based parser in-
volves an oracle, that is a function mapping a con-
figuration to the best transition. Transition-based
parsers usually rely on a static oracle, only well-
defined for gold configurations, which transforms
trees into sequences of gold actions. Training
against a static oracle restricts the exploration of
the search space to the gold sequence of actions.
At test time, due to error propagation, the parser
will be in a very different situation than at train-
ing time. It will have to infer good actions from
noisy configurations. To alleviate error propaga-
tion, a solution is to train the parser to predict the

best action given any configuration, by allowing
it to explore a greater part of the search space at
train time. Dynamic oracles are non-deterministic
oracles well-defined for any configuration. They
give the best possible transitions for any config-
uration. Although dynamic oracles are widely
used in dependency parsing and available for most
standard transition systems (Goldberg and Nivre,
2013; Goldberg et al., 2014; Gómez-Rodrı́guez et
al., 2014; Straka et al., 2015), no dynamic oracle
parsing model has yet been proposed for phrase
structure grammars.

The model we present aims at parsing mor-
phologically rich languages (MRL). Recent re-
search has shown that morphological features are
very important for MRL parsing (Björkelund et
al., 2013; Crabbé, 2015). However, traditional
linear models (such as the structured perceptron)
need to define rather complex feature templates
to capture interactions between features. Addi-
tional morphological features complicate this task
(Crabbé, 2015). Instead, we propose to rely on
a neural network weighting function which uses
a non-linear hidden layer to automatically capture
interactions between variables, and embeds mor-
phological features in a vector space, as is usual
for words and other symbols (Collobert and We-
ston, 2008; Chen and Manning, 2014).

The article is structured as follows. In Section
2, we present neural transition-based parsing. Sec-
tion 3 motivates learning with a dynamic oracle
and presents an algorithm to do so. Section 4 in-
troduces the dynamic oracle. Finally, we present
parsing experiments in Section 5 to evaluate our
proposal.

2 Transition-Based Constituent Parsing

Transition-based parsers for phrase structure
grammars generally derive from the work of Sagae

172



A[h]

E[e]D[d]X[h]C[c]B[b]

A[h]

E[e]A:[h]

D[d]A:[h]

A:[h]

X[h]C[c]

B[b]

Figure 1: Order-0 head markovization.

and Lavie (2005). In the present paper, we extend
Crabbé (2015)’s transition system.

Grammar form We extract the grammar from
a head-annotated preprocessed constituent tree-
bank (cf Section 5). The preprocessing involves
two steps. First, unary chains are merged, ex-
cept at the preterminal level, where at most one
unary production is allowed. Second, an order-0
head-markovization is performed (Figure 1). This
step introduces temporary symbols in the bina-
rized grammar, which are suffixed by “:”. The re-
sulting productions have one the following form:

X[h]→ A[a] B[b] X[h]→ A[a] b
X[h]→ h X[h]→ a B[b]

where X,A,B are delexicalised non-terminals, a,
b and h ∈ {a, b} are tokens, and X[h] is a lexical-
ized non-terminal. The purpose of lexicalization
is to allow the extraction of features involving the
heads of phrases together with their tags and mor-
phological attributes.

Transition System In the transition-based
framework, parsing relies on two data structures:
a buffer containing the sequence of tokens to
parse and a stack containing partial instantiated
trees. A configuration C = 〈j, S, b, γ〉 is a tuple
where j is the index of the next token in the buffer,
S is the current stack, b is a boolean, and γ is the
set of constituents constructed so far.1

Constituents are instantiated non-terminals, i.e.
tuples (X, i, j) such that X is a non-terminal and
(i, j) are two integers denoting its span. Although
the content of γ could be retrieved from the stack,
we make it explicit because it will be useful for the
design of the oracle in Section 4.

From an initial configuration C0 = 〈0, �,⊥, ∅〉,
the parser incrementally derives new configura-
tions by performing actions until a final configura-
tion is reached. S(HIFT) pops an element from the

1The introduction of γ is the main difference with Crabbé
(2015)’s transition system.

Stack: S|(C, l, i)|(B, i, k)|(A, k, j)
Action Constraints

RL(X) or RR(X), X∈ N A/∈ Ntmp and B /∈ Ntmp
RL(X:) or RR(X:), X:∈ Ntmp C/∈ Ntmp or j < n

RR(X) B/∈ Ntmp
RL(X) A/∈ Ntmp

Table 1: Constraints to ensure that binary trees can
be unbinarized. n is the sentence length.

Input w0w1 . . . wn−1

Axiom 〈0, �,⊥, ∅〉

S
〈j, S,⊥, γ〉

〈j + 1, S|(tj , j, j + 1),>, γ〉

RL(X)
〈j, S|(A, i, k)|(B, k, j),⊥, γ〉
〈j, S|(X, i, j),⊥, γ ∪ {(X, i, j)}〉

RU(X)
〈j, S|(tj−1, j − 1, j),>, γ〉

〈j, S|(X, j − 1, j),⊥, γ ∪ {(X, j − 1, j)}〉

GR
〈j, S,>, γ〉
〈j, S,⊥, γ〉

Figure 2: Transition system, the transition RR(X)
and the lexicalization of symbols are omitted.

buffer and pushes it on the stack. R(EDUCE)(X)
pops two elements from the stack, and pushes a
new non-terminal X on the stack with the two el-
ements as its children. There are two kinds of bi-
nary reductions, left (RL) or right (RR), depend-
ing on the position of the head. Finally, unary re-
ductions (RU(X)) pops only one element from the
stack and pushes a new non-terminal X. A deriva-
tion C0⇒τ = C0

a0⇒ . . . aτ−1⇒ Cτ is a sequence
of configurations linked by actions and leading to
a final configuration. Figure 2 presents the algo-
rithm as a deductive system. G(HOST)R(EDUCE)
actions and boolean b (> or ⊥) are used to ensure
that unary reductions (RU) can only take place
once after a SHIFT action.2

Constraints on the transitions make sure that
predicted trees can be unbinarized. Figure 3 shows
two examples of trees that could not have been
obtained by the binarization process. In the first
tree, a temporary symbol rewrites as two tempo-

2This transition system is similar to the extended system
of Zhu et al. (2013). The main difference is the strategy
used to deal with unary reductions. Our strategy ensures that
derivations for a sentence all have the same number of steps,
which can have an effect when using beam search. We use a
GHOST-REDUCE action, whereas they use a padding strategy
with an IDLE action.

173



A[h]

C:[c]A:[h]

A[h]

C[h]A:[a]

Figure 3: Examples of ill-formed binary trees

rary symbols. In the second one, the head of a
temporary symbol is not the head of its direct par-
ent. Table 1 shows a summary of the constraints
used to ensure that any predicted tree is a well-
formed binarized tree.3 In this table, N is the set
of non-terminals and Ntmp ⊂ N is the set of tem-
porary non-terminals.

Weighted Parsing The deductive system is in-
herently non-deterministic. Determinism is pro-
vided by a scoring function

s(C0⇒τ ) =
τ∑
i=1

fθ(Ci−1, ai)

where θ is a set of parameters. The score of a
derivation decomposes as a sum of scores of ac-
tions. In practice, we used a feed-forward neu-
ral network very similar to the scoring model of
Chen and Manning (2014). The input of the net-
work is a sequence of typed symbols. We consider
three main types (non-terminals, tags and termi-
nals) plus a language-dependent set of morpholog-
ical attribute types, for example, gender, number,
or case (Crabbé, 2015). The first layer h(0) is a
lookup layer which concatenates the embeddings
of each typed symbol extracted from a configura-
tion. The second layer h(1) is a non-linear layer
with a rectifier activation (ReLU). Finally, the last
layer h(2) is a softmax layer giving a distribution
over possible actions, given a configuration. The
score of an action is its log probability.

Assuming v1,v2 . . . ,vα are the embeddings of
the sequence of symbols extracted from a config-
uration, the forward pass is summed up by the fol-
lowing equations:

h(0) = [v1;v2; . . . ;vα]

h(1) = max{0,W(h) · h(0) + b(h)}
h(2) = Softmax(W(o) · h(1) + b(o))

fθ(C, a) = log(h(2)a )
3There are additional constraints which are not presented

here. For example, SHIFT assumes that the buffer is not
empty. A full description of constraints typically used in a
slightly different transition system can be found in Zhang and
Clark (2009)’s appendix section.

s2.ct[s2.wt] s1.ct[s1.wt]

s1.cl[s1.wl] s1.cr[s1.wr]

s0.ct[s0.wt]

s0.cl[s0.wl] s0.cr[s0.wr] q1 . . .q4

︸ ︷︷ ︸
stack

︸ ︷︷ ︸
queue

Figure 4: Schematic representation of local ele-
ments in a configuration.

Thus, θ includes the weights and biases for each
layer (W(h), W(o), b(h),b(o)), and the embed-
ding lookup table for each symbol type.

We perform greedy search to infer the best-
scoring derivation. Note that this is not an exact
inference. Most propositions in phrase structure
parsing rely on dynamic programming (Durrett
and Klein, 2015; Mi and Huang, 2015) or beam
search (Crabbé, 2015; Watanabe and Sumita,
2015; Zhu et al., 2013). However we found that
with a scoring function expressive enough and a
rich feature set, greedy decoding can be surpris-
ingly accurate (see Section 5).

Features Each terminal is a tuple containing the
word form, its part-of-speech tag and an arbi-
trary number of language-specific morphological
attributes, such as CASE, GENDER, NUMBER,
ASPECT and others (Seddah et al., 2013; Crabbé,
2015). The representation of a configuration de-
pends on symbols at the top of the two data struc-
tures, including the first tokens in the buffer, the
first lexicalised non-terminals in the stack and pos-
sibly their immediate descendants (Figure 4). The
full set of templates is specified in Table 6 of An-
nex A. The sequence of symbols that forms the in-
put of the network is the instanciation of each posi-
tion described in this table with a discrete symbol.

3 Training a Greedy Parser with an
Oracle

An important component for the training of a
parser is an oracle, that is a function mapping a
gold tree and a configuration to an action. The
oracle is used to generate local training examples
from trees, and feed them to the local classifier.

A static oracle (Goldberg and Nivre, 2012) is
an incomplete and deterministic oracle. It is only
well-defined for gold configurations (the configu-
rations derived by the gold action sequence) and
returns the unique gold action. Usually, parsers
use a static oracle to transform the set of bina-
rized trees into a set D = {C(i), a(i)}1≤i≤T of
training examples. Training consists in minimiz-

174



ing the negative log likelihood of these examples.
The limitation of this training method is that only
gold configurations are seen during training. At
test time, due to error propagation, the parser will
have to predict good actions from noisy configu-
rations, and will have much difficulty to recover
after mistakes.

To alleviate this problem, a line of work (Daumé
III et al., 2006; Ross et al., 2011) has cast the prob-
lem of structured prediction as a search problem
and developed training algorithms aiming at ex-
ploring a greater part of the search space. These
methods require an oracle well-defined for every
search state, that is, for every parsing configura-
tion.

A dynamic oracle is a complete and non-
deterministic oracle (Goldberg and Nivre, 2012).
It returns the non-empty set of the best transitions
given a configuration and a gold tree. In depen-
dency parsing, starting from Goldberg and Nivre
(2012), dynamic oracle algorithms and training
methods have been proposed for a variety of tran-
sition systems and led to substantial improvements
in accuracy (Goldberg and Nivre, 2013; Goldberg
et al., 2014; Gómez-Rodrı́guez et al., 2014; Straka
et al., 2015; Gómez-Rodrı́guez and Fernández-
González, 2015).

Online training An online trainer iterates sev-
eral times over each sentence in the treebank, and
updates its parameters until convergence. When a
static oracle is used, the training examples can be
pregenerated from the sentences. When we use a
dynamic oracle instead, we generate training ex-
amples on the fly, by following the prediction of
the parser (given the current parameters) instead
of the gold action, with probability p, where p is
a hyperparameter which controls the degree of ex-
ploration. The online training algorithm for a sin-
gle sentence s, with an oracle function o is shown
in Figure 5. It is a slightly modified version of
Goldberg and Nivre (2013)’s algorithm 3, an ap-
proach they called learning with exploration.

In particular, as our neural network uses a cross-
entropy loss, and not the perceptron loss used in
Goldberg and Nivre (2013), updates are performed
even when the prediction is correct. When p = 0,
the algorithm acts identically to a static oracle
trainer, as the parser always follows the gold tran-
sition. When the set of actions predicted by the
oracle has more than one element, the best scor-
ing element among them is chosen as the reference

function TRAINONESENTENCE(s,θ, p, o)
C ← INITIAL(s)
while C is not a final configuration do

A← o(C, s) . set of best actions
â← argmaxa fθ(C)a
if â ∈ A then

t← â . t: target
else

t← argmaxa∈A fθ(C)a
θ ← UPDATE(θ, C, t) . backprop
if RANDOM() < p then

C ← â(C) . Follow prediction
else

C ← t(C) . Follow best action
return θ

Figure 5: Online training for a single annotated
sentence s, using an oracle function o.

action to update the parameters of the neural net-
work.

4 A Dynamic Oracle for
Transition-Based Parsing

This section introduces a dynamic oracle algo-
rithm for the parsing model presented in the pre-
vious 2 sections, that is the function o used in the
algorithm in Figure 5.

The dynamic oracle must minimize a cost func-
tion L(c; t, T ) computing the cost of applying
transition t in configuration c, with respect to a
gold parse T . As is shown by Goldberg and Nivre
(2013), the oracle’s correctness depends on the
cost function. A correct dynamic oracle o will
have the following general formulation:

o(c, T ) = {t|L(c; t, T ) = min
t′
L(c; t′, T )} (1)

The correctness of the oracle is not necessary
to improve training. The oracle needs only to
be good enough (Daumé et al., 2009), which
is confirmed by empirical results (Straka et al.,
2015). Goldberg and Nivre (2013) identified
arc-decomposability, a powerful property of cer-
tain dependency parsing transition systems for
which we can easily derive correct efficient or-
acles. When this property holds, we can infer
whether a tree is reachable from the reachability
of individual arcs. This simplifies the calculation
of each transition cost. We rely on an analogue
property we call constituent decomposition. A

175



set of constituents is tree-consistent if it is a sub-
set of a set corresponding to a well-formed tree. A
phrase structure transition system is constituent-
decomposable iff for any configuration C and any
tree-consistent set of constituents γ, if every con-
stituent in γ is reachable from C, then the whole
set is reachable from C (constituent reachability
will be formally defined in Section 4.1).

The following subsections are structured as fol-
lows. First of all, we present a cost function (Sec-
tion 4.1). Then, we derive a correct dynamic ora-
cle algorithm for an ideal case where we assume
that there is no temporary symbols in the grammar
(Section 4.2). Finally, we present some heuris-
tics to define a dynamic oracle for the general case
(Section 4.3).

4.1 Cost Function
The cost function we use ignores the lexicalization
of the symbols. For the sake of simplicity, we mo-
mentarily leave apart the headedness of the binary
reductions (until the last paragraph of Section 4)
and assume a unique binary REDUCE action.

For the purpose of defining a cost func-
tion for transitions, we adopt a represen-
tation of trees as sets of constituents. For
example, (S (NP (D the) (N cat))
(VP (V sleeps))) corresponds to the set
{(S, 0, 3), (NP, 0, 2), (VP, 2, 3)}. As is shown in
Figure 2, every reduction action (unary or binary)
adds a new constituent to the set γ of already
predicted constituents, which was introduced in
Section 2. We define the cost of a predicted set
of constituents γ̂ with respect to a gold set γ∗ as
the number of constituents in γ∗ which are not
in γ̂ penalized by the number of predicted unary
constituents which are not in the gold set:

Lr(γ̂, γ∗) = |γ∗ − γ̂|
+ |{(X, i, i+ 1) ∈ γ̂|(X, i, i+ 1) /∈ γ∗}| (2)

The first term penalizes false negatives and the
second one penalizes unary false positives. The
number of binary constituents in γ∗ and γ̂ depends
only on the sentence length n, thus binary false
positives are implicitly taken into account by the
fist term.

The cost of a transition and that of a configura-
tion are based on constituent reachability. The
relation C ` C ′ holds iff C ′ can be deduced
from C by performing a transition. Let `∗ de-
note the reflexive transitive closure of `. A set of

constituents γ (possibly a singleton) is reachable
from a configuration C iff there is a configuration
C ′ = 〈j, S, b, γ′〉 such that C `∗ C ′ and γ ⊆ γ′,
which we write C ; γ.

Then, the cost of an action t for a configura-
tion C is the cost difference between the best tree
reachable from t(C) and the best tree reachable
from C:

Lr(t;C, γ∗) = min
γ:t(C);γ

L(γ, γ∗)− min
γ:C;γ

L(γ, γ∗)

This cost function is easily decomposable (as a
sum of costs of transitions) whereas F1 measure
is not.

By definition, for each configuration, there is at
least one transition with cost 0 with respect to the
gold parse. Otherwise, it would entail that there
is a tree reachable from C but unreachable from
t(C), for any t. Therefore, we reformulate equa-
tion 1:

o(C, γ∗) = {t|Lr(C; t, γ∗) = 0} (3)

In the transition system, the grammar is left im-
plicit: any reduction is allowed (even if the corre-
sponding grammar rule has never been seen in the
training corpus). However, due to the introduction
of temporary symbols during binarization, there
are constraints to ensure that any derivation cor-
responds to a well-formed unbinarized tree. These
constraints make it difficult to test the reachability
of constituents. For this reason, we instantiate two
transition systems. We call SR-TMP the transition
system in Figure 2 which enforces the constraints
in Table 1, and SR-BIN, the same transition system
without any of such constraints. SR-BIN assumes
an idealized case where the grammar contains no
temporary symbols, whereas SR-TMP is the actual
system we use in our experiments.

4.2 A Correct Oracle for SR-BIN Transition
System

SR-BIN transition system provides no guarantees
that predicted trees are unbinarisable. The only
condition for a binary reduction to be allowed is
that the stack contains at least two symbols. If so,
any non-terminal in the grammar could be used. In
such a case, we can define a simple necessary and
sufficient condition for constituent reachability.

Constituent reachability Let γ∗ be a tree-
consistent constituent set, and C = 〈j, S, b, γ〉 a

176



parsing configuration, such that:

S = (X1, i0, i1) . . . (Xp, ip−1, i)|(A, i, k)|(B, k, j)
A binary constituent (X,m, n) is reachable iff it
satisfies one of the three following properties :

1. (X,m, n) ∈ γ
2. j < m < n

3. m ∈ {i0, . . . ip−1, i, k}, n ≥ j
and (m,n) 6= (k, j)

The first two cases are trivial and correspond re-
spectively to a constituent already constructed and
to a constituent spanning words which are still in
the buffer.

In the third case, (X,m, n) can be constructed
by performing n − j times the transitions SHIFT
and GHOST-REDUCE (or REDUCE-UNARY), and
then a sequence of binary reductions ended by an
X reduction. Note that as the index j in the config-
uration is non-decreasing during a derivation, the
constituents whose span end is inferior to j are not
reachable if they are not already constructed. For
a unary constituent, the condition for reachability
is straightforward: a constituent (X, i − 1, i) is
reachable from configuration C = 〈j, S, b, γ〉 iff
(X, i− 1, i) ∈ γ or i > j or i = j ∧ b = >.
Constituent decomposability SR-BIN is con-
stituent decomposable. In this paragraph, we give
some intuition about why this holds. Reason-
ing by contradiction, let’s assume that every con-
stituent of a tree-consistent set γ∗ is reachable
from C = 〈j, S|(A, i, k)|(B, k, j), b, γ〉 and that
γ∗ is not reachable (contraposition). This entails
that at some point during a derivation, there is
no possible transition which maintains reachabil-
ity for all constituents of γ∗. Let’s assume C is
in such a case. If some constituent of γ∗ is reach-
able fromC, but not from SHIFT(C), its span must
have the form (m, j), where m ≤ i. If some con-
stituent of γ∗ is reachable from C, but not from
REDUCE(X)(C), for any label X , its span must
have the form (k, n), where n > j. If both condi-
tions hold, γ∗ contains incompatible constituents
(crossing brackets), which contradicts the assump-
tion that γ∗ is tree-consistent.

Computing the cost of a transition The condi-
tions on constituent reachability makes it easy to
compute the cost of a transition t for a given con-
figuration C = 〈j, S|(A, i, k)|(B, k, j), b, γ〉 and
a gold set γ∗:

1: function O(〈j, S|(A, i, k)|(B, k, j), b, γ〉, γ∗)
2: if b = > then . Last action was SHIFT
3: if (X, j − 1, j) ∈ γ∗ then
4: return {REDUCEUNARY(X)}
5: else
6: return {GHOSTREDUCE}
7: if ∃n > j, (X, k, n) ∈ γ∗ then
8: return {SHIFT}
9: if (X, i, j) ∈ γ∗ then

10: return {REDUCE(X)}
11: if ∃m < i, (X,m, j) ∈ γ∗ then
12: return {REDUCE(Y),∀Y }
13: return {a ∈ A|a is a possible action}

Figure 6: Oracle algorithm for SR-BIN.

• The cost of a SHIFT is the number of con-
stituents not in γ, reachable from C and
whose span ends in j.

• The cost of a binary reduction REDUCE(X) is
a sum of two terms. The first one is the num-
ber of constituents of γ∗ whose span has the
form (k, n) with n > j. These are no longer
compatible with (X, i, j) in a tree. The sec-
ond one is one if (Y, i, j) ∈ γ∗ and Y 6= X
and zero otherwise. It is the cost of misla-
belling a constituent with a gold span.

• The cost of a unary reduction or that of a
ghost reduction can be computed straightfor-
wardly by looking at the gold set of con-
stituents.

We present in Figure 6 an oracle algorithm derived
from these observations.

4.3 A Heuristic-based Dynamic Oracle for
SR-TMP transition system

The conditions for constituent reachability for SR-
BIN do not hold any longer for SR-TMP. In par-
ticular, constituent reachability depends crucially
on the distinction between temporary and non-
temporary symbols. The algorithm in Figure 6 is
not correct for this transition system. In Figure
7, we give an illustration of a prototypical case in
which the algorithm in Figure 6 will fail. The con-
stituent (C:, i, j) is in the gold set of constituents
and could be constructed with REDUCE(C:). The
third symbol on the stack being temporary symbol
D:, the reduction to a temporary symbol will jeop-
ardize the reachability of (C,m, j) because reduc-

177



tions are not possible when the two symbols at
the top of the stack are temporary symbols. The
best course of action is then a reduction to any
non-temporary symbol, so as to keep (C,m, j)
reachable. Note that in this case, the cost of RE-
DUCE(C:) cannot be smaller than that of a single
mislabelled constituent.

In fact, this example shows that the constraints
inherent to SR-TMP makes it non constituent-
decomposable. In the example in Figure 7, both
constituents in the set {(C,m, j), (C:, i, j)}, a
tree-consistent constituent set, is reachable. How-
ever, the whole set is not reachable, as RE-
DUCE(C:) would make (C,m, j) not reachable.

In dependency parsing, several exact dy-
namic oracles have been proposed for non arc-
decomposable transition systems (Goldberg et al.,
2014), including systems for non-projective pars-
ing (Gómez-Rodrı́guez et al., 2014). These ora-
cles rely on tabular methods to compute the cost
of transitions and have (high-degree) polynomial
worst case running time. Instead, to avoid re-
sorting to more computationally expensive exact
methods, we adapt the algorithm in Figure 6 to the
constraints involving temporary symbols using the
following heuristics:

• If the standard oracle predicts a reduction,
make sure to choose its label so that every
reachable constituent (X,m, j) ∈ γ∗ (m <
i) is still reachable after the transition. Practi-
cally, if such constituent exists and if the third
symbol on the stack is a temporary symbol,
then do not predict a temporary symbol.

• When reductions to both temporary symbols
and non-temporary symbols have cost zero,
only predict temporary symbols. This should
not harm training and improve precision for
the unbinarized tree, as any non temporary

Configuration stack Gold tree

D:m,i Ai,k Bk,j

Cm,j

Dm,i C:i,j

Ai,k Bk,j

Figure 7: Problematic case. Due to the temporary
symbol constraints enforced by SR-TMP, the algo-
rithm in Figure 6 will fail on this example.

Dev F1 (EVALB) Decoding toks/sec

static (this work) 88.6 greedy
dynamic (this work) 89.0 greedy

Test F1 (EVALB)

Hall et al. (2014) 89.2 CKY 12
Berkeley (Petrov et al., 2006) 90.1 CKY 169

Durrett and Klein (2015)† 91.1 CKY -
Zhu et al. (2013)† 91.3 beam=16 1,290

Crabbé (2015) 90.0 beam=8 2,150
Sagae and Lavie (2006) 85.1 greedy -

static (this work) 88.0 greedy 3,820�

dynamic (this work) 88.6 greedy 3,950�

Table 3: Results on the Penn Treebank (Mar-
cus et al., 1993). † use clusters or word vectors
learned on unannotated data. � different architec-
ture (2.3Ghz Intel), single processor.

symbol in the binarized tree corresponds to
a constituent in the n-ary tree.

Head choice In some cases, namely when re-
ducing two non-temporary symbols to a new con-
stituent (X, i, j), the oracle must determine the
head position in the reduction (REDUCE-RIGHT
or REDUCE-LEFT). We used the following heuris-
tic: if (X, i, j) is in the gold set, choose the same
head position, otherwise, predict both RR(X) and
RL(X) to keep the non-determinism.

5 Experiments

We conducted parsing experiments to evaluate
our proposal. We compare two experimental set-
tings. In the ‘static’ setting, the parser is trained
only on gold configurations; in the ‘dynamic’ set-
ting, we use the dynamic oracle and the training
method in Figure 5 to explore non-gold configura-
tions. We used both the SPMRL dataset (Seddah
et al., 2013) in the ‘predicted tag’ scenario, and
the Penn Treebank (Marcus et al., 1993), to com-
pare our proposal to existing systems. The tags
and morphological attributes were predicted using
Marmot (Mueller et al., 2013), by 10-fold jack-
knifing for the train and development sets. For
the SPMRL dataset, the head annotation was car-
ried out with the procedures described in Crabbé

Number of possible values ≤ 8 ≤ 32 > 32
Dimensions for embedding 4 8 16

Table 4: Size of morphological attributes embed-
dings.

178



Arabic Basque French German Hebrew Hungarian Korean Polish Swedish Avg

Decoding Development F1 (EVALBSPMRL)

Durrett and Klein (2015)† CKY 80.68 84.37 80.65 85.25 89.37 89.46 82.35 92.10 77.93 84.68
Crabbé (2015) beam=8 81.25 84.01 80.87 84.08 90.69 88.27 83.09 92.78 77.87 84.77

static (this work) greedy 80.25 84.29 79.87 83.99 89.78 88.44 84.98 92.38 76.63 84.51
dynamic (this work) greedy 80.94 85.17 80.31 84.61 90.20 88.70 85.46 92.57 77.87 85.09

Test F1 (EVALBSPMRL)

Björkelund et al. (2014)† 81.32∗ 88.24 82.53 81.66 89.80 91.72 83.81 90.50 85.50 86.12

Berkeley (Petrov et al., 2006) CKY 79.19 70.50 80.38 78.30 86.96 81.62 71.42 79.23 79.18 78.53
Berkeley-Tags CKY 78.66 74.74 79.76 78.28 85.42 85.22 78.56 86.75 80.64 80.89

Durrett and Klein (2015)† CKY 80.24 85.41 81.25 80.95 88.61 90.66 82.23 92.97 83.45 85.09
Crabbé (2015) beam=8 81.31 84.94 80.84 79.26 89.65 90.14 82.65 92.66 83.24 84.97

Fernández-González and Martins (2015) - 85.90 78.75 78.66 88.97 88.16 79.28 91.20 82.80 (84.22)

static (this work) greedy 79.77 85.91 79.62 79.20 88.64 90.54 84.53 92.69 81.45 84.71
dynamic (this work) greedy 80.71 86.24 79.91 80.15 88.69 90.51 85.10 92.96 81.74 85.11

dynamic (this work) beam=2 81.14 86.45 80.32 80.68 89.06 90.74 85.17 93.15 82.65 85.48
dynamic (this work) beam=4 81.59 86.45 80.48 80.69 89.18 90.73 85.31 93.13 82.77 85.59
dynamic (this work) beam=8 81.80 86.48 80.56 80.74 89.24 90.76 85.33 93.13 82.80 85.64

Table 2: Results on development and test corpora. Metrics are provided by evalb spmrl with
spmrl.prm parameters (http://www.spmrl.org/spmrl2013-sharedtask.html). † use
clusters or word vectors learned on unannotated data. ∗ Björkelund et al. (2013).

(2015), using the alignment between dependency
treebanks and constituent treebanks. For English,
we used Collins’ head annotation rules (Collins,
2003). Our system is entirely supervised and
uses no external data. Every embedding was
initialised randomly (uniformly) in the interval
[−0.01, 0.01]. Word embeddings have 32 dimen-
sions, tags and non-terminal embeddings have 16
dimensions. The dimensions of the morphological
attributes depend on the number of values they can
have (Table 4). The hidden layer has 512 units.4

For the ‘dynamic’ setting, we trained every
other k sentence with the dynamic oracle and
the other sentences with the static oracle. This
method, used by Straka et al. (2015), allows for
high values of p, without slowing or preventing
convergence. We used several hyperparameters
combinations (see Table 5 of Annex A). For each
language, we present the model with the combi-
nation which maximizes the developement set F-
score. We used Averaged Stochastic Gradient De-
scent (Polyak and Juditsky, 1992) to minimize the
negative log likelihood of the training examples.
We shuffled the sentences in the training set be-
fore each iteration.

Results Results for English are shown in Table
3. The use of the dynamic oracle improves F-score

4We did not tune these hyperparameters for each lan-
guage. Instead, we chose a set of hyperparameters which
achieved a tradeoff between training time and model accu-
racy. The effect of the morphological features and their di-
mensionality are left to future work.

by 0.4 on the development set and 0.6 on the test
set. The resulting parser, despite using greedy de-
coding and no additional data, is quite accurate.
For example, it compares well with Hall et al.
(2014)’s span based model and is much faster.

For the SPMRL dataset, we report results on the
development sets and test sets in Table 2. The
metrics take punctuation and unparsed sentences
into account (Seddah et al., 2013). We compare
our results with the SPMRL shared task baselines
(Seddah et al., 2013) and several other parsing
models. The model of Björkelund et al. (2014)
obtained the best results on this dataset. It is
based on a product grammar and a discriminative
reranker, together with morphological features and
word clusters learned on unannotated data. Durrett
and Klein (2015) use a neural CRF based on CKY
decoding algorithm, with word embeddings pre-
trained on unannotated data. Fernández-González
and Martins (2015) use a parsing-as-reduction ap-
proach, based on a dependency parser with a la-
bel set rich enough to reconstruct constituent trees
from dependency trees. Finally, Crabbé (2015)
uses a structured perceptron with rich features and
beam-search decoding. Both Crabbé (2015) and
Björkelund et al. (2014) use MARMOT-predicted
morphological tags (Mueller et al., 2013), as is
done in our experiments.

Our results show that, despite using a very sim-
ple greedy inference and being strictly supervised,
our base model (static oracle training) is compet-
itive with the best single parsers on this dataset.

179



We hypothesize that these surprising results come
both from the neural scoring model and the mor-
phological attribute embeddings (especially for
Basque, Hebrew, Polish and Swedish). We did not
test these hypotheses systematically and leave this
investigation for future work.

Furthermore, we observe that the dynamic ora-
cle improves training by up to 0.6 F-score (aver-
aged over all languages). The improvement de-
pends on the language. For example, Swedish,
Arabic, Basque and German are the languages
with the most important improvement. In terms of
absolute score, the parser also achieves very good
results on Korean and Basque, and even outper-
forms Björkelund et al. (2014)’s reranker on Ko-
rean.

Combined effect of beam and dynamic ora-
cle Although initially, dynamic oracle training
was designed to improve parsing without rely-
ing on more complex search methods (Goldberg
and Nivre, 2012), we tested the combined effects
of dynamic oracle training and beam search de-
coding. In Table 2, we provide results for beam
decoding with the already trained local models
in the ‘dynamic’ setting. The transition from
greedy search to a beam of size two brings an im-
provement comparable to that of the dynamic or-
acle. Further increase in beam size does not seem
to have any noticeable effect, except for Arabic.
These results show that effects of the dynamic or-
acle and beam decoding are complementary and
suggest that a good tradeoff between speed and
accuracy is already achieved in a greedy setting
or with a very small beam size

6 Conclusion

We have described a dynamic oracle for con-
stituent parsing. Experiments show that training
a parser against this oracle leads to an improve-
ment in accuracy over a static oracle. Together
with morphological features, we obtain a greedy
parser as accurate as state-of-the-art (non rerank-
ing) parsers for morphologically-rich languages.

Acknowledgments

We thank the anonymous reviewers, along with
Héctor Martı́nez Alonso and Olga Seminck for
valuable suggestions to improve prior versions of
this article.

References
Anders Björkelund, Özlem Çetinoğlu, Richárd Farkas,

Thomas Mueller, and Wolfgang Seeker. 2013.
(re)ranking meets morphosyntax: State-of-the-art
results from the SPMRL 2013 shared task. In Pro-
ceedings of the Fourth Workshop on Statistical Pars-
ing of Morphologically-Rich Languages, pages 135–
145, Seattle, Washington, USA, October. Associa-
tion for Computational Linguistics.

Anders Björkelund, Özlem Çetinoğlu, Agnieszka
Faleńska, Richárd Farkas, Thomas Mueller, Wolf-
gang Seeker, and Zsolt Szántó. 2014. Introduc-
ing the ims-wrocław-szeged-cis entry at the spmrl
2014 shared task: Reranking and morpho-syntax
meet unlabeled data. In Proceedings of the First
Joint Workshop on Statistical Parsing of Morpho-
logically Rich Languages and Syntactic Analysis of
Non-Canonical Languages, pages 97–102, Dublin,
Ireland, August. Dublin City University.

Léon Bottou. 2010. Large-Scale Machine Learn-
ing with Stochastic Gradient Descent. In Yves
Lechevallier and Gilbert Saporta, editors, Proceed-
ings of COMPSTAT’2010, pages 177–186. Physica-
Verlag HD.

Danqi Chen and Christopher D Manning. 2014. A fast
and accurate dependency parser using neural net-
works. In Empirical Methods in Natural Language
Processing (EMNLP).

Michael Collins. 2003. Head-driven statistical mod-
els for natural language parsing. Comput. Linguist.,
29(4):589–637, December.

Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of the 25th International Conference on
Machine Learning, ICML ’08, pages 160–167, New
York, NY, USA. ACM.

Benoit Crabbé. 2015. Multilingual discriminative lex-
icalized phrase structure parsing. In Proceedings of
the 2015 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1847–1856, Lisbon,
Portugal, September. Association for Computational
Linguistics.

Hal Daumé, John Langford, and Daniel Marcu. 2009.
Search-based structured prediction. Machine Learn-
ing, 75(3):297–325.

Hal Daumé III, John Langford, and Daniel Marcu.
2006. Searn in practice.

Greg Durrett and Dan Klein. 2015. Neural crf pars-
ing. In Proceedings of the Association for Computa-
tional Linguistics, Beijing, China, July. Association
for Computational Linguistics.

Daniel Fernández-González and André F. T. Martins.
2015. Parsing as reduction. In Proceedings of the

180



53rd Annual Meeting of the Association for Compu-
tational Linguistics and the 7th International Joint
Conference on Natural Language Processing (Vol-
ume 1: Long Papers), pages 1523–1533, Beijing,
China, July. Association for Computational Linguis-
tics.

Yoav Goldberg and Joakim Nivre. 2012. A dynamic
oracle for arc-eager dependency parsing. In Pro-
ceedings of COLING 2012, pages 959–976, Mum-
bai, India, December. The COLING 2012 Organiz-
ing Committee.

Yoav Goldberg and Joakim Nivre. 2013. Training
deterministic parsers with non-deterministic oracles.
Transactions of the Association for Computational
Linguistics, 1:403–414.

Yoav Goldberg, Francesco Sartorio, and Giorgio Satta.
2014. A tabular method for dynamic oracles in
transition-based parsing. TACL, 2:119–130.

Carlos Gómez-Rodrı́guez and Daniel Fernández-
González. 2015. An efficient dynamic oracle for
unrestricted non-projective parsing. In Proceedings
of the 53rd Annual Meeting of the Association for
Computational Linguistics and the 7th International
Joint Conference on Natural Language Processing
(Volume 2: Short Papers), pages 256–261, Beijing,
China, July. Association for Computational Linguis-
tics.

Carlos Gómez-Rodrı́guez, Francesco Sartorio, and
Giorgio Satta. 2014. A polynomial-time dy-
namic oracle for non-projective dependency pars-
ing. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Process-
ing (EMNLP), pages 917–927, Doha, Qatar, Octo-
ber. Association for Computational Linguistics.

David Hall, Greg Durrett, and Dan Klein. 2014. Less
grammar, more features. In Proceedings of the 52nd
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers), Bal-
timore, Maryland, June. Association for Computa-
tional Linguistics.

Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of english: The penn treebank. Computa-
tional Linguistics, 19(2):313–330.

Haitao Mi and Liang Huang. 2015. Shift-reduce con-
stituency parsing with dynamic programming and
pos tag lattice. In Proceedings of the 2015 Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, pages 1030–1035, Denver, Col-
orado, May–June. Association for Computational
Linguistics.

Thomas Mueller, Helmut Schmid, and Hinrich
Schütze. 2013. Efficient higher-order CRFs for
morphological tagging. In Proceedings of the 2013

Conference on Empirical Methods in Natural Lan-
guage Processing, pages 322–332, Seattle, Wash-
ington, USA, October. Association for Computa-
tional Linguistics.

Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 433–440,
Sydney, Australia, July. Association for Computa-
tional Linguistics.

B. T. Polyak and A. B. Juditsky. 1992. Acceleration
of stochastic approximation by averaging. SIAM J.
Control Optim., 30(4):838–855, July.

Stéphane Ross, Geoffrey J. Gordon, and Drew Bagnell.
2011. A reduction of imitation learning and struc-
tured prediction to no-regret online learning. In Ge-
offrey J. Gordon, David B. Dunson, and Miroslav
Dudı́k, editors, AISTATS, volume 15 of JMLR Pro-
ceedings, pages 627–635. JMLR.org.

Kenji Sagae and Alon Lavie. 2005. A classifier-based
parser with linear run-time complexity. In Proceed-
ings of the Ninth International Workshop on Parsing
Technology, pages 125–132. Association for Com-
putational Linguistics.

Kenji Sagae and Alon Lavie. 2006. A best-first prob-
abilistic shift-reduce parser. In Proceedings of the
COLING/ACL on Main conference poster sessions,
pages 691–698. Association for Computational Lin-
guistics.

Djamé Seddah, Reut Tsarfaty, Sandra Kübler, Marie
Candito, Jinho D. Choi, Richárd Farkas, Jen-
nifer Foster, Iakes Goenaga, Koldo Gojenola Gal-
letebeitia, Yoav Goldberg, Spence Green, Nizar
Habash, Marco Kuhlmann, Wolfgang Maier, Joakim
Nivre, Adam Przepiórkowski, Ryan Roth, Wolfgang
Seeker, Yannick Versley, Veronika Vincze, Marcin
Woliński, Alina Wróblewska, and Eric Villemonte
de la Clergerie. 2013. Overview of the SPMRL
2013 shared task: A cross-framework evaluation of
parsing morphologically rich languages. In Pro-
ceedings of the Fourth Workshop on Statistical Pars-
ing of Morphologically-Rich Languages, pages 146–
182, Seattle, Washington, USA, October. Associa-
tion for Computational Linguistics.

Milan Straka, Jan Hajič, Jana Straková, and Jan
Hajič jr. 2015. Parsing universal dependency tree-
banks using neural networks and search-based or-
acle. In Proceedings of Fourteenth International
Workshop on Treebanks and Linguistic Theories
(TLT 14), December.

Taro Watanabe and Eiichiro Sumita. 2015. Transition-
based neural constituent parsing. In Proceedings
of the 53rd Annual Meeting of the Association for
Computational Linguistics and the 7th International
Joint Conference on Natural Language Processing

181



(Volume 1: Long Papers), pages 1169–1179, Bei-
jing, China, July. Association for Computational
Linguistics.

Yue Zhang and Stephen Clark. 2009. Transition-
based parsing of the chinese treebank using a global
discriminative model. In Proceedings of the 11th
International Conference on Parsing Technologies,
IWPT ’09, pages 162–171, Stroudsburg, PA, USA.
Association for Computational Linguistics.

Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang,
and Jingbo Zhu. 2013. Fast and accurate shift-
reduce constituent parsing. In ACL (1), pages 434–
443. The Association for Computer Linguistics.

A Supplementary Material

‘static’ and ‘dynamic’ setting ‘dynamic’ setting

learning rate α iterations k p

{0.01, 0.02} {0, 10−6} [1, 24] {8, 16} {0.5, 0.9}

Table 5: Hyperparameters. α is the decrease con-
stant used for the learning rate (Bottou, 2010).

s0.ct s0.wt.tag s0.wt.form q1.tag
s0.cl s0.wl.tag s0.wl.form q2.tag
s0.cr s0.wr.tag s0.wr.form q3.tag
s1.ct s1.wt.tag s1.wt.form q4.tag
s1.cl s1.wl.tag s1.wl.form q1.form
s1.cr s1.wr.tag s1.wr.form q2.form
s2.ct s2.wt.tag s2.wt.form q3.form

q4.form
s0.wt.m∀m ∈M q0.m∀m ∈M
s1.wt.m∀m ∈M q1.m∀m ∈M

Table 6: These templates specify a list of ad-
dresses in a configuration. The input of the neural
network is the instanciation of each address by a
discrete typed symbol. Each vi (Section 2) is the
embedding of the ith instantiated symbol of this
list. M is the set of all available morphological
attributes for a given language. We use the follow-
ing notations (cf Figure 4): si is the ith item in the
stack, c denotes non-terminals, top, left and right,
indicate the position of an element in the subtree.
Finally, w and q are respectively stack and buffer
tokens.

182


