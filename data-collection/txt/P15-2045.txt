



















































Improving distant supervision using inference learning


Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 273–278,

Beijing, China, July 26-31, 2015. c©2015 Association for Computational Linguistics

Improving distant supervision using inference learning

Roland Roller1, Eneko Agirre2, Aitor Soroa2 and Mark Stevenson1
1 Department of Computer Science, University of Sheffield

roland.roller,mark.stevenson@sheffield.ac.uk
2 IXA NLP group, University of the Basque Country

e.agirre,a.soroa@ehu.eus

Abstract

Distant supervision is a widely applied ap-
proach to automatic training of relation
extraction systems and has the advantage
that it can generate large amounts of la-
belled data with minimal effort. How-
ever, this data may contain errors and
consequently systems trained using dis-
tant supervision tend not to perform as
well as those based on manually labelled
data. This work proposes a novel method
for detecting potential false negative train-
ing examples using a knowledge inference
method. Results show that our approach
improves the performance of relation ex-
traction systems trained using distantly su-
pervised data.

1 Introduction

Distantly supervised relation extraction relies on
automatically labelled data generated using infor-
mation from a knowledge base. A sentence is
annotated as a positive example if it contains a
pair of entities that are related in the knowledge
base. Negative training data is often generated us-
ing a closed world assumption: pairs of entities not
listed in the knowledge base are assumed to be un-
related and sentences containing them considered
to be negative training examples. However this as-
sumption is violated when the knowledge base is
incomplete which can lead to sentences containing
instances of relations being wrongly annotated as
negative examples.

We propose a method to improve the quality of
distantly supervised data by identifying possible
wrongly annotated negative instances. Our pro-
posed method includes a version of the Path Rank-
ing Algorithm (PRA) (Lao and Cohen, 2010; Lao
et al., 2011) which infers relation paths by com-
bining random walks though a knowledge base.

We use this knowledge inference to detect possi-
ble false negatives (or at least entity pairs closely
connected to a target relation) in automatically la-
belled training data and show that their removal
can improve relation extraction performance.

2 Related Work

Distant supervision is widely used to train relation
extraction systems with Freebase and Wikipedia
commonly being used as knowledge bases, e.g.
(Mintz et al., 2009; Riedel et al., 2010; Krause
et al., 2012; Zhang et al., 2013; Min et al., 2013;
Ritter et al., 2013). The main advantage is its
ability to automatically generate large amounts of
training data automatically. On the other hand,
this automatically labelled data is noisy and usu-
ally generates lower performance than approaches
trained using manually labelled data. A range of
filtering approaches have been applied to address
this problem including multi-class SVM (Nguyen
and Moschitti, 2011) and Multi-Instance learn-
ing methods (Riedel et al., 2010; Surdeanu et al.,
2012). These approaches take into account the fact
that entities might occur in different relations at
the same time and may not necessarily express the
target relation. Other approaches focus directly on
the noise in the data. For instance Takamatsu et al.
(2012) use a generative model to predict incorrect
data while Intxaurrondo et al. (2013) use a range
of heuristics including PMI to remove noise. Au-
genstein et al. (2014) apply techniques to detect
highly ambiguous entity pairs and discard them
from their labelled training set.

This work proposes a novel approach to the
problem by applying an inference learning method
to identify potential false negatives in distantly la-
belled data. Our method makes use of a modi-
fied version of PRA to learn relation paths from a
knowledge base and uses this information to iden-
tify false negatives.

273



3 Data and Methods

We chose to apply our approach to relation ex-
traction tasks from the biomedical domain since
this has proved to be an important problem within
these documents (Jensen et al., 2006; Hahn et al.,
2012; Cohen and Hunter, 2013; Roller and Steven-
son, 2014). In addition, the first application of dis-
tant supervision was to biomedical journal articles
(Craven and Kumlien, 1999). In addition, the most
widely used knowledge source in this domain, the
UMLS Metathesaurus (Bodenreider, 2004), is an
ideal resource to apply inference learning given its
rich structure.

We develop classifiers to identify relations
found in two subsets of UMLS: the National Drug
File-Reference Terminology (ND-FRT) and the
National Cancer Institute Thesaurus (NCI). A cor-
pus of approximately 1,000,000 publications is
used to create the distantly supervised training
data. The corpus contains abstracts published be-
tween 1990 and 2001 annotated with UMLS con-
cepts using MetaMap (Aronson and Lang, 2010).

3.1 Distantly labelled data

Distant supervision is carried out for a target
UMLS relation by identifying instance pairs and
using them to create a set of positive instance
pairs. Any pairs which also occur as an instance
pair of another UMLS relation are removed from
this set. A set of negative instance pairs is then
created by forming new combinations that do not
occur within the positive instance pairs. Sentences
containing a positive or negative instance pair are
then extracted to generate positive and negative
training examples for the relation. These candi-
date sentences are then stemmed (Porter, 1997)
and PoS tagged (Charniak and Johnson, 2005).

The sets of positive and negative training exam-
ples are then filtered to remove sentences that meet
any of the following criteria: contain the same
positive pair more than once; contain both a posi-
tive and negative pair; more than 5 words between
the two elements of the instance pair; contain very
common instance pairs.

3.2 PRA-Reduction

PRA (Lao and Cohen, 2010; Lao et al., 2011)
is an algorithm that infers new relation instances
from knowledge bases. By considering a knowl-
edge base as a graph, where nodes are connected
through typed relations, it performs random walks

over it and finds bounded-length relation paths that
connect graph nodes. These paths are used as
features in a logistic regression model, which is
meant to predict new relations in the graph. Al-
though initially conceived as an algorithm to dis-
cover new links in the knowledge base, PRA can
also be used to learn relevant relation paths for
any given relation. For instance, if x and y are
related via sibling relation, the model trained by
PRA would learn that the relation path parent(x,a)
∧ parent(a,y)1 is highly relevant, as siblings share
the same parents.

Knowledge graphs were extracted from the ND-
FRT and NCI vocabularies generating approxi-
mately 200, 000 related instance pairs for ND-
FRT and 400, 000 for NCI. PRA is then run on
both graphs in order to learn paths for each tar-
get relation. Table 1 shows examples of the paths
PRA generated for the relation biological-process-
involves-gene-product together with their weights.
We only make use of relation paths with positive
weights generated by PRA.

path weight
gene-encodes-gene-product(x,a) ∧ gene-
plays-role-in-process(a,y)

10.53

isa(x,a) ∧ biological-process-involves-gene-
product(a,y)

6.17

isa(x,a) ∧ biological-process-involves-gene-
product(a,y)

2.80

gene-encodes-gene-product(x,a) ∧ gene-
plays-role-in-process(a,b) ∧ isa(b,y)

-0.06

Table 1: Example PRA-induced paths and weights
for the NCI relation biological-process-involves-
gene-product.

The paths induced by PRA are used to iden-
tify potential false negatives in the negative train-
ing examples (Section 3.1). Each negative training
example is examined to check whether the entity
pair is related in UMLS by following any of the
relation paths extracted by PRA for the relevant
target relation. Examples containing related en-
tity pairs are assumed to be false negatives, since
the relation can be inferred from the knowledge
base, and removed from the set of negatives train-
ing examples. For instance, using the path in the
top row of Table 1, sentences containing the enti-
ties x and y would be removed if the path gene-
encodes-gene-product(x,a) ∧ gene-plays-role-in-
process(a,y) could be identified within UMLS.

1An underline (‘ ’) prefix represents the inverse of a rela-
tion while ∧ represents path composition.

274



3.3 Evaluation

Relation Extraction system: The MultiR system
(Hoffmann et al., 2010) with features described by
Surdeanu et al. (2011) was used for the experi-
ments.
Datasets: Three datasets were created to train
MultiR and evaluate performance. The first (Un-
filtered) uses the data obtained using distant su-
pervision (Section 3.1) without removing any ex-
amples identified by PRA. The overall ratio of
positive to negative sentences in this dataset was
1:5.1. However, this changes to 1:2.3 after remov-
ing examples identified by PRA. Consequently the
bias in the distantly supervised data was adjusted
to 1:2 to increase comparability across configura-
tions. Reducing bias was also found to increase re-
lation extraction performance, producing a strong
baseline. The PRA-reduced dataset is created by
applying PRA reduction (Section 3.2) to the Un-
filtered dataset to remove a portion of the nega-
tive training examples. Removing these examples
produces a dataset that is smaller than Unfiltered
and with a different bias. Changing the bias of
the training data can influence the classification re-
sults. Consequently the Random-reduced dataset
was created by removing randomly selected nega-
tive examples from Unfiltered to produce a dataset
with the same size and bias as PRA-reduced. The
Random-reduced dataset is used to show that ran-
domly removing negative instances leads to lower
results than removing those suggested by PRA.

Evaluation: Two approaches were used to eval-
uate performance.

The Held-out datasets consist of the Unfiltered,
PRA-reduced and Random-reduced data sets. The
set of entity pairs obtained from the knowledge
base is split into four parts and a process similar
to 4-fold cross validation applied. In each fold the
automatically labelled sentences obtained from the
pairs in 3 of the quarters are used as training data
and sentences obtained from the remaining quarter
used for testing.

The Manually labelled dataset contains 400
examples of the relation may-prevent and 400 of
may-treat which were manually labelled by two
annotators who were medical experts. Both rela-
tions are taken from the ND-FRT subset of UMLS.
Each annotator was asked to label every sentence
and then re-examine cases where there was dis-
agreement. This process lead to inter-annotator
agreement of 95.5% for may-treat and 97.3% for

may-prevent. The annotated data set is publicly
available2. Any sentences in the training data con-
taining an entity pair that occurs within the man-
ually labelled dataset are removed. Although this
dataset is smaller than the held-out dataset, its an-
notations are more reliable and it is therefore likely
to be a more accurate indicator of performance ac-
curacy. This dataset is more balanced than the
held-out data with a ratio of 1:1.3 for may-treat
and 1:1.8 for may-prevent.

Evaluation metric: Our experiments use en-
tity level evaluation since this is the most appropri-
ate approach to determine suitability for database
population. Precision and recall are computed
based on the proportion of entity pairs identified.
For the held-out data the set of correct entity pairs
are those which occur in sentences labeled as pos-
itive examples of the relation and which are also
listed as being related in UMLS. For the manually
labelled data it is simply the set of entity pairs that
occur in positive examples of the relation.

4 Results

4.1 Held-out data
Table 2 shows the results obtained using the held-
out data. Overall results, averaged across all re-
lations with maximum recall, are shown in the top
portion of the table and indicate that applying PRA
improves performance. Although the highest pre-
cision is obtained using the Unfiltered classifier,
the PRA-reduced classifier leads to the best recall
and F1. Performance of the Random-reduced clas-
sifier indicates that the improvement is not simply
due to a change in the bias in the data but that the
examples it contains lead to an improved model.

The lower part of Table 2 shows results for each
relation. The PRA-reduced classifier produces the
best results for the majority of relations and always
increases recall compared to Unfiltered.

It is perhaps surprising that removing false neg-
atives from the training data leads to an increase
in recall, rather than precision. False negatives
cause the classifier to generate an overly restrictive
model of the relation and to predict positive ex-
amples of a relation as negative. Removing them
leads to a less constrained model and higher recall.

There are two relations where there is also an in-
crease in precision (contraindicating-class-of and
mechanism-of-action-of ) and these are also the
ones for which the fewest training examples are

2https://sites.google.com/site/umlscorpus/home

275



Unfiltered Random-reduced PRA-reduced
Prec. Rec. F1 Prec. Rec. F1 Prec. Rec. F1

Overall 62.30 51.82 56.58 44.49 74.26 55.64 56.85 77.10 65.44
NCI relations

biological process involves gene product 89.61 43.18 57.86 65.67 78.79 71.38 70.63 84.85 76.97
disease has normal cell origin 60.20 83.86 69.95 43.2 95.21 58.85 42.80 91.88 57.91

gene product has associated anatomy 41.65 64.04 49.96 29.22 74.63 41.81 37.94 65.28 47.82
gene product has biochemical function 86.43 72.00 78.33 60.66 91.57 72.90 70.58 95.80 81.17

process involves gene 78.92 50.71 61.54 51.38 80.64 62.73 68.16 87.34 76.47
ND-FRT relations

contraindicating class of 40.00 20.83 26.14 28.48 72.50 39.58 41.30 82.50 54.33
may prevent 27.48 14.69 18.87 20.61 44.79 27.94 38.11 35.63 36.64

may treat 48.66 39.63 43.14 39.57 50.00 43.84 50.88 57.93 54.11
mechanism of action of 47.15 40.63 43.12 40.25 59.38 47.62 52.85 59.38 55.82

Table 2: Evaluation using held-out data

Unfiltered Random-reduced PRA-reduced
relation Prec. Rec. F1 Prec. Rec. F1 Prec. Rec. F1

may prevent 54.17 21.67 30.95 53.57 25.00 34.09 39.66 38.33 38.98
may treat 40.00 47.48 43.42 43.21 50.36 46.51 41.05 67.63 51.09

Table 3: Evaluation using manually labelled data

 0.4

 0.5

 0.6

 0.7

 0.8

 0.9

 1

 0  0.2  0.4  0.6  0.8  1

P
re

c
is

io
n

Recall

unfiltered

PRA-reduced

Random-reduced

Figure 1: Precision/Recall Curve for Held-out data

available. The classifier has access to such a lim-
ited amount of data for these relations that remov-
ing the false negatives identified by PRA allows it
to learn a more accurate model.

Figure 1 presents a precision/recall curve com-
puted using MultiR’s output probabilities. Results
for the PRA-reduced and the Random-reduced
classifiers show that reducing the amount of nega-
tive training data increases recall. However, using
PRA-reduced generally leads to higher precision,
indicating that PRA is able to identify suitable in-
stances for removal from the training set. The Un-
filtered classifier produces good results but preci-
sion and recall are lower than PRA-reduced.

4.2 Manually labelled

Table 3 shows results of evaluation on the more
reliable manually labelled data set. The best over-

all performance is once again obtained using the
PRA-reduced classifier. There is an increase in re-
call for both relations and a slight increase in pre-
cision for may treat. Performance of the Random-
reduced classifier also improves due to an increas-
ing recall but remains below PRA-reduced. Per-
formance of the Random-reduced classifier is also
better than Unfiltered, with the overall improve-
ment largely resulting from increased recall, but
below PRA-reduced. These results confirm that re-
moving examples identified by PRA improves the
quality of training data.

Further analysis indicated that the PRA-reduced
classifier produces the fewest false negatives in its
predictions on the manually annotated dataset. It
incorrectly labels 82 entity pairs (45 may-treat, 37
may-prevent) as negative while Unfiltered predicts
120 (73, 47) and Random-reduced 114 (69, 45).
This supports our initial hypothesis that remov-
ing potential false negatives from training data im-
proves classifier predictions.

5 Conclusions and Future Work

This paper proposes a novel approach to identify-
ing incorrectly labelled instances generated using
distant supervision. Our method applies an infer-
ence learning method to detect and discard pos-
sible false negatives from the training data. We
show that our method improves performance for
a range of relations in the biomedical domain by
making use of information from UMLS.

In future we would like to explore alternative

276



methods for selecting PRA relation paths to iden-
tify false negatives. Furthermore we would like
to examine the PRA-reduced data in more detail.
We would like to find which kind of entity pairs
are detected by our proposed method and whether
the reduced data can also be used to extend the
positive training data. We would also like to ap-
ply the approach to other domains and alternative
knowledge bases. Finally it would be interesting
to compare our approach to other state of the art re-
lation extraction systems for distant supervision or
biased-SVM approaches such as Liu et al. (2003).

Acknowledgements

The authors are grateful to the Engineering
and Physical Sciences Research Council for
supporting the work described in this paper
(EP/J008427/1).

References
A. Aronson and F. Lang. 2010. An overview of

MetaMap: historical perspective and recent ad-
vances. Journal of the American Medical Associ-
ation, 17(3):229–236.

Isabelle Augenstein, Diana Maynard, and Fabio
Ciravegna. 2014. Relation extraction from the web
using distant supervision. In Proceedings of the
19th International Conference on Knowledge Engi-
neering and Knowledge Management (EKAW 2014),
Linköping, Sweden, November.

Olivier Bodenreider. 2004. The unified medical lan-
guage system (umls): integrating biomedical termi-
nology. Nucleic acids research, 32(suppl 1):D267–
D270.

Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
ACL ’05, pages 173–180, Stroudsburg, PA, USA.
Association for Computational Linguistics.

K Bretonnel Cohen and Lawrence E Hunter. 2013.
Text mining for translational bioinformatics. PLoS
computational biology, 9(4):e1003044.

Mark Craven and Johan Kumlien. 1999. Constructing
biological knowledge bases by extracting informa-
tion from text sources. In In Proceedings of the Sev-
enth International Conference on Intelligent Systems
for Molecular Biology (ISMB), pages 77–86. AAAI
Press.

Udo Hahn, K Bretonnel Cohen, Yael Garten, and
Nigam H Shah. 2012. Mining the pharmacoge-
nomics literaturea survey of the state of the art.
Briefings in bioinformatics, 13(4):460–494.

Raphael Hoffmann, Congle Zhang, and Daniel S.
Weld. 2010. Learning 5000 relational extractors. In
Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics, ACL ’10,
pages 286–295, Stroudsburg, PA, USA. Association
for Computational Linguistics.

Ander Intxaurrondo, Mihai Surdeanu, Oier Lopez
de Lacalle, and Eneko Agirre. 2013. Remov-
ing noisy mentions for distant supervision. Proce-
samiento del Lenguaje Natural, 51:41–48.

Lars Juhl Jensen, Jasmin Saric, and Peer Bork. 2006.
Literature mining for the biologist: from informa-
tion retrieval to biological discovery. Nature reviews
genetics, 7(2):119–129.

Sebastian Krause, Hong Li, Hans Uszkoreit, and Feiyu
Xu. 2012. Large-scale learning of relation-
extraction rules with distant supervision from the
web. In Proceedings of the 11th International
Conference on The Semantic Web - Volume Part
I, ISWC’12, pages 263–278, Berlin, Heidelberg.
Springer-Verlag.

Ni Lao and William W. Cohen. 2010. Relational re-
trieval using a combination of path-constrained ran-
dom walks. Mach. Learn., 81(1):53–67, October.

Ni Lao, Tom Mitchell, and William W. Cohen. 2011.
Random walk inference and learning in a large scale
knowledge base. In Proceedings of the 2011 Con-
ference on Empirical Methods in Natural Language
Processing, pages 529–539, Edinburgh, Scotland,
UK., July. Association for Computational Linguis-
tics.

Bing Liu, Yang Dai, Xiaoli Li, Wee Sun Lee, and
Philip S. Yu. 2003. Building text classifiers using
positive and unlabeled examples. In Intl. Conf. on
Data Mining, pages 179–188.

Bonan Min, Ralph Grishman, Li Wan, Chang Wang,
and David Gondek. 2013. Distant supervision for
relation extraction with an incomplete knowledge
base. In Proceedings of the 2013 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 777–782, Atlanta, Georgia, June.
Association for Computational Linguistics.

Mike Mintz, Steven Bills, Rion Snow, and Dan Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Vol-
ume 2 - Volume 2, ACL ’09, pages 1003–1011,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.

Truc-Vien T. Nguyen and Alessandro Moschitti. 2011.
End-to-end relation extraction using distant super-
vision from external semantic repositories. In Pro-
ceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies: short papers - Volume 2, HLT

277



’11, pages 277–282, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.

M. F. Porter. 1997. Readings in information retrieval.
chapter An Algorithm for Suffix Stripping, pages
313–316. Morgan Kaufmann Publishers Inc., San
Francisco, CA, USA.

Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions with-
out labeled text. In Proceedings of the European
Conference on Machine Learning and Knowledge
Discovery in Databases (ECML PKDD ’10).

Alan Ritter, Luke Zettlemoyer, Oren Etzioni, et al.
2013. Modeling missing data in distant supervision
for information extraction. Transactions of the As-
sociation for Computational Linguistics, 1:367–378.

Roland Roller and Mark Stevenson. 2014. Self-
supervised relation extraction using umls. In Pro-
ceedings of the Conference and Labs of the Evalua-
tion Forum 2014, Sheffield, England.

Mihai Surdeanu, David McClosky, Mason Smith, An-
drey Gusev, and Christopher Manning. 2011. Cus-
tomizing an information extraction system to a new
domain. In Proceedings of the ACL 2011 Work-
shop on Relational Models of Semantics, pages 2–
10, Portland, Oregon, USA, June. Association for
Computational Linguistics.

Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,
and Christopher D. Manning. 2012. Multi-instance
multi-label learning for relation extraction. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, EMNLP-
CoNLL ’12, pages 455–465, Stroudsburg, PA, USA.
Association for Computational Linguistics.

Shingo Takamatsu, Issei Sato, and Hiroshi Nakagawa.
2012. Reducing wrong labels in distant supervi-
sion for relation extraction. In Proceedings of the
50th Annual Meeting of the Association for Compu-
tational Linguistics: Long Papers - Volume 1, ACL
’12, pages 721–729, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.

Xingxing Zhang, Jianwen Zhang, Junyu Zeng, Jun
Yan, Zheng Chen, and Zhifang Sui. 2013. Towards
accurate distant supervision for relational facts ex-
traction. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguis-
tics (Volume 2: Short Papers), pages 810–815, Sofia,
Bulgaria, August. Association for Computational
Linguistics.

278


