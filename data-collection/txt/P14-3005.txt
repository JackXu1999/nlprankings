



















































A Mapping-Based Approach for General Formal Human Computer Interaction Using Natural Language


Proceedings of the ACL 2014 Student Research Workshop, pages 34–40,
Baltimore, Maryland USA, June 22-27 2014. c©2014 Association for Computational Linguistics

A Mapping-Based Approach for General Formal
Human Computer Interaction Using Natural Language

Vincent Letard
LIMSI CNRS

letard@limsi.fr

Sophie Rosset
LIMSI CNRS

rosset@limsi.fr

Gabriel Illouz
LIMSI CNRS

illouz@limsi.fr

Abstract

We consider the problem of mapping nat-
ural language written utterances express-
ing operational instructions1 to formal lan-
guage expressions, applied to French and
the R programming language. Developing
a learning operational assistant requires
the means to train and evaluate it, that is,
a baseline system able to interact with the
user. After presenting the guidelines of
our work, we propose a model to repre-
sent the problem and discuss the fit of di-
rect mapping methods to our task. Finally,
we show that, while not resulting in excel-
lent scores, a simple approach seems to be
sufficient to provide a baseline for an in-
teractive learning system.

1 Introduction

Technical and theoretical advances allow achiev-
ing more and more powerful and efficient opera-
tions with the help of computers. However, this
does not necessarily make it easier to work with
the machine. Recent supervised learning work
(Allen et al., 2007; Volkova et al., 2013) exploited
the richness of human-computer interaction for
improving the efficiency of a human performed
task with the help of the computer.

Contrary to most of what was proposed so far,
our long term goal is to build an assistant system
learning from interaction to construct a correct for-
mal language (FL) command for a given natural
language (NL) utterance, see Table 1. However,
designing such a system requires data collection,
and early attempts highlighted the importance of
usability for the learning process: a system that is
hard to use (eg. having very poor performance)

1We call operational instruction the natural language ex-
pression of a command in any programming language.

would prevent from extracting useful learning ex-
amples from the interaction. We thus need to pro-
vide the system with a basis of abilities and knowl-
edge to allow both incremental design and to keep
the interest of the users, without which data turn
to be way more tedious to collect. We assume that
making the system usable requires the ability to
provide help to the user more often than it needs
help from him/her, that is an accuracy over 50%.

We hypothesize that a parametrized direct
mapping between the NL utterances and the FL
commands can reach that score. A knowledge set
K is built from parametrized versions of the asso-
ciations shown in Table 1. The NL utterance Ubest
from K that is the closest to the request-utterance
according to a similarity measure is chosen and its
associated command C(Ubest) is adapted to the
parameters of the request-utterance and returned.
For example, given the request-utterance Ureq:
”Load the file data.csv”, the system should rank
the utterances of K by similarity with Ureq. Con-
sidering the associations represented in Table 1,
the first utterance should be the best ranked, and
the system should return the command:
”var1 <- read.csv("data.csv")”.
Note that several commands can be proposed at
the same time to give the user alternate choices.

We use Jaccard, tf-idf, and BLEU similarity
measures, and consider different selection strate-
gies. We highlight that the examined similarity
measures show enough complementarity to permit
the use of combination methods, like vote or sta-
tistical classification, to improve a posteriori the
efficiency of the retrieval.

2 Related Work

2.1 Mapping Natural Language to Formal
Language

Related problems have been previously processed
using different learning methods. Branavan (2009,

34



NL utterances FL commands (in R)

1
Charge les données depuis ”res.csv”

var1=read.csv("res.csv")Load the data from ”res.csv”

2
Trace l’histogramme de la colonne 2 de tab

plot(hist(tab[[2]]))Draw a bar chart with column 2 of tab

3
Dessine la répartition de la colonne 3 de tab

plot(hist(tab[[3]]))Draw the distribution of column 3 of tab

4
Somme les colonnes 3 et 4 de tab

var2=c(sum(tab[3]),sum(tab[4]))Compute the sum of columns 3 and 4 of tab

5
Somme les colonnes 3 et 4 de tab

var3=sum(c(tab[[3]],tab[[4]]))Compute the sum of columns 3 and 4 of tab

Table 1: A sample of NL utterances to FL commands mapping
These examples specify the expected command to be returned for each utterance. The tokens in bold
font are linked with the commands parameters, cf. section 4. Note that the relation between utterances
and commands is a n to n. Several utterances can be associated to the same command and conversely.

2010) uses reinforcement learning to map En-
glish NL instructions to a sequence of FL com-
mands. The mapping takes high-level instructions
and their constitution into account. The scope
of usable commands is yet limited to graphical
interaction possibilities. As a result, the learn-
ing does not produce highly abstract schemes. In
the problematic of interactive continuous learning,
Artzi and Zettlemoyer (2011) build by learning a
semantic NL parser based on combinatory cate-
gorial grammars (CCG). Kushman and Barzilay
(2013) also use CCG in order to generate regu-
lar expressions corresponding to their NL descrip-
tions. This constructive approach by translation
allows to generalize over learning examples, while
the expressive power of regular expressions cor-
respond to the type-3 grammars of the Chomsky
hierarchy. This is not the case for the program-
ming languages since they are at least of type-2.
Yu and Siskind (2013) use hidden Markov mod-
els to learn a mapping between object tracks from
a video sequence and predicates extracted from
a NL description. The goal of their approach is
different from ours but the underlying problem of
finding a map between objects can be compared.
The matched objects constitute here a FL expres-
sion instead of a video sequence track.

2.2 Machine Translation

Machine translation usually refers to transforming
a NL sentence from a source language to another
sentence of the same significance in another natu-
ral language, called target language. This task is
achieved by building an intermediary representa-
tion of the sentence structure at a given level of

abstraction, and then encoding the obtained object
into the target language. While following a dif-
ferent goal, one of the tasks of the XLike project
(Marko Tadić et al., 2012) was to examine the
possibility of translating statements from NL (En-
glish) to FL (Cycl). Adapting such an approach
to operational formal target language can be inter-
esting to investigate, but we will not focus on that
track for our early goal.

2.3 Information Retrieval

The issue of information retrieval systems can be
compared with the operational assistant’s (OA),
when browsing its knowledge. Question an-
swering systems in particular (Hirschman and
Gaizauskas, 2001), turn out to be similar to OA
since both types of systems have to respond to a
NL utterance of the user by generating an accu-
rate reaction (which is respectively a NL utterance
containing the wanted information, or the execu-
tion of a piece of FL code). However, as in (Toney
et al., 2008), questions answering systems usually
rely on text mining to retrieve the right informa-
tion. Such a method demands large sets of anno-
tated textual data (either by hand or using an au-
tomatic annotator). Yet, tutorials, courses or man-
uals which could be used in order to look for re-
sponses for operational assistant systems are het-
erogeneous and include complex or implicit ref-
erences to operational knowledge. This makes
the annotation of such data difficult. Text min-
ing methods are thus not yet applicable to oper-
ational assistant systems but could be considered
once some annotated data is collected.

35



3 Problem Formulation

As we introduced in the first section, we represent
the knowledge K as a set of examples of a binary
relation R : NL → FL associating a NL utter-
ance to a FL command. If we consider the simple
case of a functional and injective relation, each
utterance is associated to exactly one command.
This is not realistic since it is possible to reformu-
late nearly any NL sentence. The case of a non in-
jective relation covers better the usual cases: each
command can be associated with one or more ut-
terances, this situation is illustrated by the second
and third examples of Table 1. Yet, the real-life
case should be a non injective nor functional rela-
tion. Not only multiple utterances can refer to a
same command, but one single utterance can also
stand for several distinct commands (see the fourth
and fifth examples2 in Table 1). We must consider
all these associations when matching a request-
utterance Ureq for command retrieval in K .

At this point, several strategies can be used to
determine what to return, with the help of the sim-
ilarity measure σ : NL × NL → R between two
NL utterances. Basically, we must determine if
a response should be given, and if so how many
commands to return. To do this, two potential
strategies can be considered for selecting the as-
sociated utterances in K .

The first choice focuses on the number of re-
sponses that are given for each request-utterance.
The n first commands according to the rankings of
their associated utterances in K are returned. The
rank r of a given utterance U is computed with:

r(U |Ureq) =
˛̨˘

U ′ ∈ K : σ(Ureq, U ′) > σ(Ureq, U)
¯˛̨

(1)

The second strategy choice can be done by de-
termining an absolute similarity threshold below
which the candidate utterances from K and their
associated sets of commands are considered too
different to match. The resulting set of commands
is given by:

Res = {C ∈ FL : (U,C) ∈ K, σ(Ureq, U) < t} (2)

with t the selected threshold. Once selected the
set of commands to be given as response, if there
are more than one, the choice of the one to execute
can be done interactively with the help of the user.

2The command 4 returns a vector of the sums of each col-
umn, while the command 5 returns the sum of the columns as
a single integer.

4 Approach

We are given a simple parsing result of both the ut-
terance and the command. The first step to address
is the acquisition of examples and the way to up-
date the knowledge. Then we examine the meth-
ods for retrieving a command from the knowledge
and a given request-utterance.

Correctly mapping utterances to commands re-
quires at least to take their respective parameters
into account (variable names, numeric values, and
quoted strings). We build generic representations
of utterances and commands by identifying the pa-
rameters in the knowledge example pair (see Ta-
ble 1), and use them to reconstruct the command
with the parameters of the request-utterance.

4.1 Retrieving the Commands

We applied three textual similarity measures to
our model in order to compare their strengths and
weaknesses on our task: the Jaccard similarity co-
efficient (Jaccard index), a tf-idf (Term frequency-
inverse document frequency) aggregation, and the
BLEU (Bilingual Evaluation Understudy) mea-
sure.

4.1.1 Jaccard index

The Jaccard index measures a similarity between
two sets valued in the same superset. For the
present case, we compare the set of words of the
input NL instruction and the one of the compared
candidate instruction, valued in the set of possible
tokens. The adapted formula for two sentences S1
and S2 results in:

J(s1, s2) =
|W (s1) ∩ W (s2)|
|W (s1) ∪ W (s2)| (3)

where W (S) stands for the set of words of the
sentence S. The Jaccard index is a baseline to
compare co-occurences of unigrams, and should
be efficient mainly with corpora containing few
ambiguous examples.

4.1.2 tf-idf

The tf-idf measure permits, given a word, to clas-
sify documents on its importance in each one, re-
garding its importance in the whole set. This mea-
sure should be helpful to avoid noise bias when it
comes from frequent terms in the corpus. Here,
the documents are the NL utterances from K , and
they are classified regarding the whole request-
utterance, or input sentence si. We then use the

36



following aggregation of the tf-idf values for each
word of si.

tfidfS(si, sc) =
1

|W (si)|
X

w∈W (si)
tfidf(w, sc, S) (4)

with S = {s|(s, com) ∈ K}, where si is the input
sentence, sc ∈ S is the compared sentence, and
where the tf-idf is given by:

tfidf(w, sc, S) = f(w, sc)idf(w, S) (5)

idf(w, S) = log

„ |S|
|{s ∈ S|w ∈ s}|

«
(6)

where at last f(w, s) is the frequency of the word
w in the sentence s. As we did for the Jaccard in-
dex, we performed the measures on both raw and
lemmatized words. On the other hand, getting rid
of the function words and closed class words is not
here mandatory since the tf-idf measure already
takes the global word frequency into account.

4.1.3 The BLEU measure

The bilingual evaluation understudy algorithm
(Papineni et al., 2002) focuses on n-grams co-
occurrences. This algorithm can be used to dis-
card examples where the words ordering is too far
from the candidate. It computes a modified pre-
cision based on the ratio of the co-occurring n-
grams within candidate and reference sentences,
on the total size of the candidate normalized by n.

PBLEU (si, S) =
X

grn∈si

maxsc∈S occ(grn, sc)
grams(si, n)

(7)

where grams(s, n) = |s| − (n− 1) is the number
of n-grams in the sentence s and occ(grn, s) =∑

grn′∈s [grn = grn
′] is the number of occur-

rences of the n-gram grn in s. BLEU also uses
a brevity penalty to prevent long sentences from
being too disadvantaged by the n-gram based pre-
cision formula. Yet, the scale of the length of the
instructions in our corpus is sufficiently reduced
not to require its use.

4.2 Optimizing the similarity measure

We applied several combinations of filters to the
utterances compared before evaluating their sim-
ilarity. We can change the set of words taken
into account, discarding or not the non open-class
words3. Identified non-lexical references such as

3Open-class words include nouns, verbs, adjectives, ad-
verbs and interjections.

variable names, quoted character strings and nu-
meric values can also be discarded or transformed
to standard substitutes. Finally, we can apply or
not a lemmatization4 on lexical tokens.By discard-
ing non open-class words, keeping non-lexical ref-
erences and applying the lemmatization, the sec-
ond utterance of Table 1 would then become:

draw bar chart column xxVALxx xxVARxx

5 Experimental Setup

5.1 Parsing

The NL utterances first pass through an arith-
metic expression finder to completely tag them be-
fore the NL analyzer. They are then parsed us-
ing WMATCH, a generic rule-based engine for
language analysis developed by Olivier Galibert
(2009). This system is modular and dispose of
rules sets for both French and English. As an ex-
ample, the simplified parsing result of the first ut-
terance of Table 1 looks like:
<_operation>
<_action> charge|_˜V </_action>
<_det> les </_det>
<_subs> données|_˜N </_subs>
<_prep> depuis </_prep>
<_unk> "res.csv" </_unk>

</_operation>

Words tagged as unknown are considered as po-
tential variable or function names. We also added
a preliminary rule to identify character strings and
count them among the possibly linked features of
the utterance. The commands are normalized by
inserting spaces between every non semantically
linked character pair and we identify numeric val-
ues, variable/function names and character strings
as features.

Only generative forms of the commands are
associated to utterances in the knowledge. This
form consists in a normalized command with unre-
solved references for every parameter linked with
the learning utterance. These references are re-
solved at the retrieving phase by matching with the
tokens of the request-utterance.

5.2 Corpus Constitution

Our initial corpus consists in 605 associations be-
tween 553 unique NL utterances in French and
240 unique R commands.

4Lemmatization is the process of transforming a word to
its canonical form, or lemma, ignoring the inflections. It can
be performed with a set of rules or with a dictionary. The
developed system uses a dictionary.

37



The low number of documents describing a
majority of R commands and their heterogeneity
make automatic example gathering not yet achiev-
able. These documentations are written for human
readers having global references on the task. Thus,
we added each example pair manually, making
sure that the element render all the example infor-
mation and that the format correspond to the cor-
pus specifications. Those specifications are meant
to be the least restrictive, that is: a NL utterance
must be written as to ask for the execution of the
associated R task. It therefore should be mostly
in the imperative form and reflect, for experienced
people, a usual way they would express the con-
cerned operation for non specialists.

5.3 Evaluation Metrics

The measures that can contribute to a relevant
evaluation of the system depend on its purpose.
Precision and recall values of information retrieval
systems are computed as follows:

P =
# correct responses
# responses given

(8)

R =
# correct responses

# responses in K
(9)

Note that the recall value is not as important as for
information retrieval: assuming that the situation
showed by the fourth and fifth associations of Ta-
ble 1 are not usual5, there should be few different
valid commands for a given request-utterance, and
most of them should be equivalent. Moreover, the
number of responses given is fixed (so is the num-
ber of responses in K), the recall thus gives the
same information as the precision, with a linear
coefficient variation.

These formulae can be applied to the ”command
level”, that is measuring the accuracy of the sys-
tem in terms of its good command ratio. However,
the user satisfaction can be better measured at the
”utterance level” since it represents the finest gran-
ularity for the user experience. We define the ut-
terance precision uP as:

uP =
# correct utterances
# responses given

(10)

where ”# correct utterances” stands for the num-
ber of request-utterances for which the system pro-
vided at least one good command.

5Increasing the tasks covering of the corpus will make
these collisions more frequent, but this hypothesis seems rea-
sonable for a first approach.

6 Results and Discussion

The system was tested on 10% of the corpus (61
associations). The set of known associations K
contains 85% of the corpus (514 associations), in-
stead of 90% in order to allow several distinct
drawings (40 were tested), and thus avoid too
much noise.

6.1 Comparing similarity measures

As shown in Table 2 the tf-idf measure outper-
forms the Jaccard and BLEU measures, whichever
filter combination is applied. The form of the ut-
terances in the corpus causes indeed the repetition
of a small set of words across the associations.
This can explain why the inverse document fre-
quency is that better.

non-lexical included not included
lemmatize yes no yes no

Jaccard 36.5 36.5 21.2 23.0
tf-idf 48.0 51.9 36.5 40.4
BLEU 30.8 32.7 26.9 30.8
chance 1.9

Table 2: Scores of precision by utterance (uP ),
providing 3 responses for each request-utterance.

The lemmatization and the inclusion of non
open-class words (not shown here) does not seem
to have a clear influence on uP , whereas including
the non-lexical tokens allows a real improvement.
This behaviour must result from the low length av-
erage (7.5 words) of the utterances in the corpus.

0.3

0.4

0.5

0.6

0.7

1 2 3 4 5 6 7 8 9
Number of responses

U
tte

ra
nc

e 
pr

ec
is

io
n 

(u
P

)

measure
tfidf
tfidf_inl

Figure 1: Utterance precision (uP ) for a fixed
number of responses by utterance. The tfidf inl
curve includes the non-lexical tokens.
Note that uP is obtained with Equation 10, which
explains the increase of the precision along the
number of responses.

38



Figure 1 shows the precision obtained with tfidf
while increasing the number of commands given
for each request-utterance. It comes out that it
is useful to propose at least 3 commands to the
user. It would not be interesting, though, to offer a
choice of more than 5 items, because the gain on
uP would be offset by the time penalty for retriev-
ing the good command among the proposals.

6.2 Allowing silence

We also tested the strategy of fixing an absolute
threshold to decide between response and silence.
Given a request-utterance and an associated order-
ing of K according to σ, the system will remain
silent if the similarity of the best example in K is
below the defined threshold.

Surprisingly, it turned out that for every mea-
sure, the 6 best similar responses at least were all
wrong. This result seems to be caused by the ex-
istence, in the test set of commands uncovered by
K , of some very short utterances that contain only
one or two lexical tokens.

6.3 Combinations

0.0

0.2

0.4

0.6

0.8

1 2 3 4 5 6 7 8 9
Number of responses

U
tte

ra
nc

e 
pr

ec
is

io
n 

(u
P

)

method
vote
tfidf_inl
vote_oracle
learning

Figure 2: Comparison of the combinations with
the tf-idf inl method. Oracle and actual vote are
done using tf-idf, Jaccard, and BLEU, with and
without non-lexical tokens. The training set for
learning is the result of a run on K .

Having tested several methods giving differ-
ent results, combining these methods can be very
interesting depending on their complementarity.
The oracle vote using the best response among
the 6 best methods shows an encouraging progres-
sion margin (cf. Figure 2). The actual vote it-
self outperforms the best method for giving up to
3 responses (reaching 50% for only 2 responses).
However, the curve position is less clear for more

responses, and tests must be performed on other
drawings of K to measure the noise influence.

The complementarity of the methods can also
be exploited by training a classification model to
identify when a method is better than the others.
We used the similarity values as features and the
measure that gave a good response as the refer-
ence class label (best similarity if multiple, and
”none” class if no good response). This setup was
tested with the support vector machines using lib-
svm (Chang and Lin, 2011) and results are shown
in Figure 2. As expected, machine learning per-
forms poorly on our tiny corpus. The accuracy
is under 20% and the system only learned when
to use the best method, and when to give no re-
sponse. Still, it manages to be competitive with
the best method and should be tested again with
more data and multiple drawings of K .

7 Conclusion and Future Work

The simple mapping methods based on similar-
ity ranking showed up to 60% of utterance pre-
cision6 remaining below a reasonable level of user
sollicitation, which validate our prior hypothesis.
A lot of approaches can enhance that score, such
as adding or developing more suitable similarity
measures (Achananuparp et al., 2008), combining
learning and vote or learning to rerank utterances.

However, while usable as a baseline, these
methods only allow poor generalization and really
need more corpus to perform well. As we pointed
out, the non-functionality of the mapping relation
also introduces ambiguities that cannot be solved
using the only knowledge of the system.

Thanks to this baseline method, we are now able
to collect more data by developing an interactive
agent that can be both an intelligent assistant and
a crowdsourcing platform. We are currently de-
veloping a web interface for this purpose. Finally,
situated human computer interaction will allow the
real-time resolving of ambiguities met in the re-
trieval with the help of the user or with the use of
contextual information from the dialogue.

Aknowledgements

The authors are grateful to every internal and ex-
ternal reviewer for their valuable advices. We also
would like to thank Google for the financial sup-
port for the authors participation to the conference.

6The corpus will soon be made available.

39



References

Palakorn Achananuparp, Xiaohua Hu, and Xiajiong
Shen. 2008. The Evaluation of Sentence Similar-
ity Measures. In Data Warehousing and Knowledge
Discovery, Springer.

James Allen, Nathanael Chambers, George Ferguson,
Lucian Galescu, Hyuckchul Jung, Mary Swift, and
William Tayson. 2007. PLOW: A Collaborative
Task Learning Agent. In Proceedings of the 22nd
National Conference on Artificial Intelligence.

Yoav Artzi, and Luke S. Zettlemoyer. 2011. Boot-
strapping semantic parsers from conversations. Pro-
ceedings of the conference on empirical methods in
natural language processing.

S.R.K. Branavan, Luke S. Zettlemoyer, and Regina
Barzilay. 2010. Reading Between the Lines: Learn-
ing to Map High-level Instructions to Commands. In
Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics.

S.R.K. Branavan, Harr Chen, Luke S. Zettlemoyer, and
Regina Barzilay. 2009. Reinforcement Learning for
Mapping Instructions to Actions. In Proceedings of
the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP.

Chih-Chung Chang, and Chih-Jen Lin. 2011. LIB-
SVM: A Library for Support Vector Machines. ACM
Transactions on Intelligent Systems and Technology

Olivier Galibert. 2009. Approches et méthodologies
pour la réponse automatique à des questions
adaptées à un cadre interactif en domaine ouvert.
Doctoral dissertation, Université Paris Sud XI.

Lynette Hirschman, and Robert Gaizauskas. 2001.
Natural language question answering: The view
from here. Natural Language Engineering 7. Cam-
bridge University Press.

Nate Kushman, and Regina Barzilay. 2013. Using Se-
mantic Unification to Generate Regular Expressions
from Natural Language. In Proceedings of the Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic
Evaluation of Machine Translation. In Proceedings
of the 40th Annual Meeting of the Association for
Computational Linguistics.

Marko Tadić, Božo Bekavac, Željko Agić, Matea
Srebačić, Daša Berović, and Danijela Merkler.
2012. Early machine translation based semantic an-
notation prototype XLike project www.xlike.org .

Dave Toney, Sophie Rosset, Aurélien Max, Olivier
Galibert, and éric Billinski. 2008. An Evaluation of
Spoken and Textual Interaction on the RITEL Inter-
active Question Answering System In Proceedings

of the Sixth International Conference on Language
Resources and Evaluation.

Svitlana Volkova, Pallavi Choudhury, Chris Quirk, Bill
Dolan, and Luke Zettlemoyer. 2013. Lightly Su-
pervised Learning of Procedural Dialog System In
Proceedings of the 51st Annual Meeting of the As-
sociation for Computational Linguistics.

Haonan Yu, and Jeffrey Mark Siskind. 2013.
Grounded Language Learning from Video De-
scribed with Sentences. In Proceedings of the 51st
Annual Meeting of the Association for Computa-
tional Linguistics.

40


