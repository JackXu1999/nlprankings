











































Unsupervised Random Walk Sentence Embeddings: A Strong but Simple Baseline


Proceedings of the 3rd Workshop on Representation Learning for NLP, pages 91–100
Melbourne, Australia, July 20, 2018. c©2018 Association for Computational Linguistics

91

Unsupervised Random Walk Sentence Embeddings:
A Strong but Simple Baseline

Kawin Ethayarajh
Department of Computer Science, University of Toronto

kawin@cs.toronto.edu

Abstract

Using a random walk model of text gen-
eration, Arora et al. (2017) proposed a
strong baseline for computing sentence
embeddings: take a weighted average of
word embeddings and modify with SVD.
This simple method even outperforms far
more complex approaches such as LSTMs
on textual similarity tasks. In this paper,
we first show that word vector length has a
confounding effect on the probability of a
sentence being generated in Arora et al.’s
model. We propose a random walk model
that is robust to this confound, where the
probability of word generation is inversely
related to the angular distance between the
word and sentence embeddings. Our ap-
proach beats Arora et al.’s by up to 44.4%
on textual similarity tasks and is competi-
tive with state-of-the-art methods. Unlike
Arora et al.’s method, ours requires no hy-
perparameter tuning, which means it can
be used when there is no labelled data.

1 Introduction

Distributed representations of words, better known
as word embeddings, have become fixtures of
current methods in natural language processing.
Word embeddings can be generated in a number
of ways (Bengio et al., 2003; Collobert and We-
ston, 2008; Pennington et al., 2014; Mikolov et al.,
2013) by capturing the semantics of a word using
the contexts it appears in. Recent work has tried
to extend that intuition to sequences of words, us-
ing methods ranging from a weighted average of
word embeddings to convolutional, recursive, and
recurrent neural networks (Le and Mikolov, 2014;
Kiros et al., 2015; Luong et al., 2013; Tai et al.,
2015). Still, Wieting et al. (2016b) found that

these sophisticated architectures are often outper-
formed, particularly in transfer learning settings,
by sentence embeddings generated as a simple av-
erage of tuned word embeddings.

Arora et al. (2017) provided a more power-
ful approach: compute the sentence embeddings
as weighted averages of word embeddings, then
subtract from each one the vector projection on
their first principal component. The weighting
scheme, smoothed inverse frequency (SIF), is de-
rived from a random walk model where words in
a sentence s are produced by the random walk of
a latent discourse vector cs. A word unrelated to
cs can be produced by chance or if it is part of
frequent discourse such as stopwords. This ap-
proach evens outperforms more complex models
such as LSTMs on textual similarity tasks. Arora
et al. argued that the simplicity and effectiveness
of their method make it a tough-to-beat baseline
for sentence embeddings. Though they call their
approach unsupervised, others have noted that it
is actually ‘weakly supervised’, since it requires
hyperparameter tuning (Cer et al., 2017).

In this paper, we first propose a class of
worst-case scenarios for Arora et al.’s (2017) ran-
dom walk model. Specifically, given some sen-
tence g that is dominated by words with zero sim-
ilarity, and some sentence h that is dominated by
identical words, we show that their approach can
return two discourse vectors cg and ch such that
p(g|cg) ⇡ p(h|ch), provided that the word vec-
tors for g have a sufficiently greater length than
those for h. In other words, word vector length
has a confounding effect on the probability of a
sentence being generated, and this effect can be
strong enough to yield completely unintuitive re-
sults. This problem is not endemic to these sce-
narios, though they are the most illustrative of it;
because of the underlying log-linear word produc-
tion model, Arora et al.’s model is fundamentally



92

sensitive to word vector length.
Our contributions in this paper are three-fold.

First, we propose a random walk model that is
robust to distortion by vector length, where the
probability of a word vector being generated by
a discourse vector is inversely related to the an-
gular distance between them. Second, we derive
a weighting scheme from this model and com-
pute a MAP estimate for the sentence embedding
as follows: normalize the word vectors, take a
weighted average of them, and then subtract from
each weighted average vector the projection on
their first m principal components. We call the
weighting scheme derived from our random walk
model unsupervised smoothed inverse frequency
(uSIF). It is similar to SIF (Arora et al., 2017) in
practice, but requires no hyperparameter tuning at
all – it is completely unsupervised, allowing it to
be used when there is no labelled data. Lastly,
we show that our approach outperforms Arora et
al.’s by up to 44.4% on textual similarity tasks, and
is even competitive with state-of-the-art methods.
Given the simplicity, effectiveness, and unsuper-
vised nature of our method, we suggest it be used
as a baseline for computing sentence embeddings.

2 Related Work

Word Embeddings Word embeddings are dis-
tributed representations of words, typically in a
low-dimensional continuous space. These word
vectors can capture semantic and lexical proper-
ties of words, even allowing some relationships to
be captured algebraically (e.g., vBerlin�vGermany +
vFrance ⇡ vParis) (Mikolov et al., 2013). Word em-
beddings are generally obtained in two ways: (a)
from internal representations of words in shal-
low neural networks (Bengio et al., 2003; Mikolov
et al., 2013; Collobert and Weston, 2008); (b) from
low rank approximations of co-occurrence matri-
ces (Pennington et al., 2014).

Word Sequence Embeddings Embeddings for
sequences of words (e.g., sentences) are created
by composing word embeddings. This can be
done simply, by doing coordinate-wise multipli-
cation (Mitchell and Lapata, 2008) or taking an
unweighted average (Mikolov et al., 2013) of the
word vectors. More sophisticated architectures
can also be used: for instance, recursive neural
networks (Socher et al., 2011, 2013), LSTMs (Tai
et al., 2015), and convolutional neural networks
(Kalchbrenner et al., 2014) can be defined and

trained on parse and dependency trees.
Other approaches are based on the presence of

a latent vector for the entire sequence. Paragraph
vectors (Le and Mikolov, 2014) are latent repre-
sentations that influence the distribution of words.
Skip-thought vectors (Kiros et al., 2015) are hid-
den representations of a neural network that en-
codes a sentence by trying to reconstruct its sur-
rounding sentences. Conneau et al. (2017) lever-
age transfer learning by using the hidden repre-
sentation of a sentence in an LSTM trained for
another task, such as textual entailment. The in-
spiration for Arora et al. (2017) is Wieting et al.
(2016b), who use word averaging after updating
word embeddings by tuning them on paraphrase
pairs. A later work by Wieting et al. (2017a) tried
trigram-averaging and LSTM-averaging in addi-
tion to word-averaging. In that approach, vectors
were tuned on the ParaNMT-50M dataset, created
by using neural machine translation to translate
51M Czech-English sentence pairs into English-
English pairs. This yielded state-of-the-art results
on textual similarity tasks, beating the previous
baseline by a wide margin.

3 Approach

3.1 The Log-Linear Random Walk Model
In Arora et al.’s original model (2016), words are
generated dynamically by the random walk of a
time-variant discourse vector ct 2 Rd, represent-
ing “what is being talked about”. Words are rep-
resented as vw 2 Rd. The probability of a word w
being generated at time t is given by a log-linear
production model (Mnih and Hinton, 2007):

p(w|ct) µ exp(hct ,vwi) (1)

Assuming that the discourse vector ct does not
change much over the course of the sentence,
Arora et al. replace the sequence of discourse vec-
tors {ct} across all time steps with a single dis-
course vector cs. The MAP estimate of cs is then
the unweighted average of word vectors (ignoring
any scalar multiplication).

Arora et al.’s improved random walk model
(2017) allows words to also be generated: (a) by
chance, with probability a · p(w), where a is some
scalar and p(w) is the frequency; (b) if the word
is correlated with the common discourse vector,
which represents frequent discourse such as stop-
words. We use c0 to denote the common discourse
vector, to be consistent with the literature. Among



93

other things, these changes help explain words that
appear frequently despite being poorly correlated
with the discourse vectors — words like the, for
example. The probability of a word w being gen-
erated by a discourse vector cs is then given as:

p(w|cs) = a · p(w)+(1�a) ·
exp(hecs,vwi)

Zecs
,

where ecs , b · c0 +(1�b ) · cs,c0 ? cs
Zecs , Â

w02V
exp(hecs,vw0 i)

where a,b are scalar hyperparameters, V is the
vocabulary, ecs is a linear combination of the dis-
course and common discourse vectors parameter-
ized by b , and Zecs is the partition function.

The sentence embedding for a sentence is de-
fined as the MAP estimate of the discourse vec-
tor cs that generated the sentence. To compute
this tractably, Arora et al. (2017) assume that word
vectors vw are roughly uniformly dispersed in the
latent space. This implies that the partition func-
tion Zecs is roughly the same for all ecs, allowing
it to be replaced with a constant Z. Assuming a
uniform prior over ecs, the maximum likelihood es-
timator for ecs on the unit sphere (ignoring normal-
ization) is then approximately proportional to:

1
|s| Âw2s

a
a+ p(w)

· vw,where a ,
1�a
a ·Z

Since Z cannot be evaluated, and a is not
known, a is a hyperparameter that needs tuning.
This weighting scheme is called smoothed inverse
frequency (SIF) and places a lower weight on more
frequent words. The first principal component of
all {ecs} in the corpus is used as the estimate for the
common discourse vector c0. The final discourse
vector cs is then produced by subtracting the pro-
jection of the weighted average on the common
component (common component removal):

cs , ecs�projc0 ecs

Arora et al. call their approach unsupervised, but
others (Cer et al., 2017) have correctly noted that
it is weakly supervised, since the hyperparameter
a needs to be tuned on a validation set.

3.2 The Confounding Effect of Vector Length
We now propose worst-case scenarios where word
vector length clearly distorts p(s|cs) due to the un-
derlying log-linear word production model. Note

that we discuss these scenarios because they are
illustrative, not because they circumscribe the uni-
verse of all scenarios in which word vector length
has a confounding effect.

Consider a sentence g comprising two rare
words x and y, where x and y have zero similar-
ity. Also consider some sentence h, where the only
word z appears twice. g might not occur natu-
rally, but its weighted average ecg would be simi-
lar to that of some longer sentence where x,y are
the only non-stopwords (i.e., those with non-neg-
ligible weight). For simplicity, further assume that
common component removal has negligible effect:

hvx,vyi= 0

cg = ecg =
1
2

✓
a

a+ p(x)
· vx +

a
a+ p(y)

· vy
◆

ch = ech =
a

a+ p(z)
· vz

(2)

Words x,y,z are so infrequent that the probability
of them being produced by chance or by the com-
mon discourse vector is negligible; the likelihood
of them being produced is therefore proportional
to the inner product of the discourse and word vec-
tors. Given that the words x,y 2 g have zero simi-
larity, and given that the only word z 2 h is identi-
cal to its discourse vector, we would expect:

p(h|ch)� p(g|cg) (3)

However, (3) does not always hold. Suppose that
the word embeddings lie in R2. Then any scalar k
can be used to create a valid set of assignments for
word embeddings vx,vy,vz that satisfy (2):

vx =


2k
0

�
,vy =


0
2k

�
,vz =


k
k

�
(4)

Assuming the words x,y,z have roughly the same
frequency, they should have the same SIF-weight.
Then the weighted averages, and by extension the
discourse vectors (2), are the same:

cg = ch =
a

a+ p(x)


k
k

�

)hcg,xi= hcg,yi= hch,zi=
a

a+ p(x)
·2k2

) p(g|cg) = p(h|ch)

Thus it is possible for g to be generated by dis-
course vector cg with roughly the same probability



94

as h by ch, contradicting (3). How is this possible,
given that the words in g have zero similarity with
each other while those in h are identical to each
other? The answer can be found in the word vector
lengths. Because ||vx||2 =

p
2||vz||2, and p(w|cs)

depends on the inner product of the word and dis-
course vectors (1), words with longer word vectors
are more likely to be produced. In fact, if vx and
vy were multiplied by some scalar greater than 1,
then p(h|ch) would be less than p(g|cg).

Generalizing Worst-Case Scenarios By ma-
nipulating the word vector length, we can also
come up with a more general class of assignments
that can contradict (3):

vx =


bk1s
bk2(1�s)

�
vy =


bk1(1�s)

bk2s

�
vz =


k1
k2

�

(5)

where s 2 [0,1],b 2 R,b � 2. For convenience,
we replace aa+p(x) with C below:

cg =C
1

2 bk1
1
2 bk2

�
,ch =C


k1
k2

�

For simplicity, we assume that the words x,y,z
across the two sentences are so infrequent that the
probability of them being generated by chance is
zero. Then the conditional probabilities of the sen-
tences being generated are:

p(g|cg) µ exp(hcg,vxi+ hcg,vyi)

= exp
✓

1
2

b 2C
�
k12 + k22

�◆

p(h|ch) µ exp(hch,vzi+ hch,vzi)
= exp

�
2C
�
k12 + k22

��

) b � 2) p(g|cg)� p(h|ch)

(6)

In this general formulation, not all scenarios are
worst-case. This describes a spectrum of scenar-
ios ranging from acceptable (e.g., vx = vy = vz
when b = 2,s = 0.5) to completely counter-in-
tuitive (see (4)). Though these assignments only
apply for word vectors in R2, they can easily be
extended to higher-dimensional spaces.

The confound of vector length persists for
longer, naturally occurring sentences. Ultimately,
the underlying log-linear word production model
(1) means that words with longer word vectors are
more likely to be generated. Because this con-
found is due to model design, rather than the MLE,
removing it requires redesigning the model. The
exact degree of the confound varies across sen-
tences, but in theory, it is unbounded.

3.3 An Angular Distance–Based Random
Walk Model

To address the confounding effect of word vector
length, we propose a random walk model where
the probability of observing a word w at time t is
inversely related to the angular distance between
the time-variant discourse vector ct 2 Rd and the
word vector vw 2 Rd:

p(w|ct) µ 1�
arccos(cos(vw,ct))

p
,

where cos(vw,ct),
vw · ct

kvwk2·kctk2
(7)

where arccos(cos(vw,ct)) is the angular distance.
For the intuition behind the use of this distance
metric, note that the angular distance between two
vectors is equal to the geodesic distance between
them on the unit sphere. Thus the angular distance
can also be interpreted as the length of the short-
est path between the L2 normalized word vector
and the L2 normalized discourse vector on the unit
sphere. Since the angular distance lies in [0,p],
we divide it by p to bound it in [0,1]. Our choice
of angular distance – as opposed to, say, the expo-
nentiated cosine similarity – is critical to avoiding
hyperparameter tuning.

Assuming that the discourse vector ct does not
change much over the course of the sentence, the
sequence of discourse vectors {ct} across all time
steps can be replaced with a single discourse vec-
tor cs for the sentence s. To model sentences more
realistically, we allow words to be generated in
two additional ways, as proposed in Arora et al.
(2017): (a) by chance, with probability a · p(w),
where a is some scalar and p(w) is the frequency;
(b) if the word is correlated with one of m common
discourse vectors {c0m}, which represent various
types of frequent discourse, such as stopwords.
The probability of a word w being generated by
discourse vector cs is then:

p(w|cs) = a · p(w)+(1�a) ·
d (ecs,vw)

Zecs
,

where ecs , (1�b )cs +b
m

Â
i=1

li c0i, cs ? c0i

d (ecs,vw), 1�
arccos(cos(vw,ct))

p
,

Zecs , Â
w02V

d (ecs,vw0)

(8)

where a,b ,{li} are scalar hyperparameters, V is
the vocabulary, ecs is a linear combination of the



95

discourse and common discourse vectors parame-
terized by b and {li}, and Zecs is the partition func-
tion. Instead of searching for the optimal hyperpa-
rameter values over some large space, as Arora et
al. (2017) did, we make some simple assumptions
to directly compute them.

We define the sentence embedding for some
sentence s to be the MAP estimate of the discourse
vector cs that generates s. Assuming a uniform
prior over possible cs, the MAP estimate is also
the MLE estimate for cs. The log-likelihood of a
sentence s is:

log p(s|cs) = Â
w2s

log p(w|cs)

To maximize log p(s|cs), we can approximate
log p(w|cs) using a first-degree Taylor polynomial:

fw(ecs), log p(w|ecs)

— fw(ecs) =


1�a
p ·Zecs · exp( fw(ecs))

� ∂
∂ ecs cos(vw, ecs)p
1� cos2 (vw, ecs)

,

∂
∂ ecs

cos =
vw

kvwk2·kecsk2
� cos(vw, ecs)

ecs
kecsk22

Where a , (1�a)/(aZecs), C is a constant, and v0w
is a vector orthogonal to vw with length kvwk�1:

fw(ecs)⇡ fw(v0w)+— fw(v0w)
| �ecs� v0w

�

=C+
a

p ·
�

p(w)+ 12 ·a
� · vw

�
ecs� v0w

�

=C+
1
p

 
a

p(w)+ 12 ·a
hecs,vwi

!

The MLE for ecs on the unit sphere (ignoring nor-
malization) is then approximately proportional to:

1
|s| Âw2s

a
p(w)+ 12 a

· vw (9)

The MLE of ecs is approximately a weighted av-
erage of word vectors, where more frequent words
are down-weighted. In fact, it very closely re-
sembles the SIF weighting scheme (Arora et al.,
2017)! However, there are two key differences.
For one, as we show later in this subsection, we
have derived this weighting scheme from a model
that is robust to the confounding effect of word
vector length. Secondly, in SIF, a is a hyperpa-
rameter that needs to be tuned on a validation set.
We now show that in our approach, we can calcu-
late a directly as a function of the vocabulary V
and the number of words in the sentence, |s|.

Normalization Before weighting the word vec-
tors, we normalize them along each dimension:
we construct a matrix [vw1 ...ww|s| ] and take the L2
norm of each row, which corresponds to a sin-
gle dimension in Rd. We then multiply this d-
dimensional vector element-wise with every vec-
tor in the sentence. This helps reduce the differ-
ence in variance across the dimensions.

Partition Function To calculate Zecs , we borrow
the key assumption from Arora et al. (2017) that
the word vectors vw are roughly uniformly dis-
persed in the latent space. Then the expected
geodesic distance between a latent discourse vec-
tor and a word vector on the unit sphere is p/2, so
Ew02V [d (ecs,vw0)] = 12 . Then:

Zecs = Â
w02V

d (ecs,vw0)

= |V| Ew02V [d (ecs,vw0)] =
1
2
|V|

(10)

Odds of Random Production a is the proba-
bility that a word w will be produced by chance
instead of by the discourse or common discourse
vectors. To estimate a , we first consider the prob-
ability that a random word w will be produced by
a discourse vector cs at least once over n steps of a
random walk:

p(w|c1s , ...,cns ) = 1�
n

’
t=1


1� d(c

t
s,vw)
Zcs

�

Ew⇠V [p(w|c1s , ...,cns )] = 1�
✓

1� 1|V|

◆n

The number of steps taken during the random walk
is itself a random variable, so we let n = Es2S|s|.
We assume that if the frequency is greater than this
expectation, then the word is always produced by
chance; less than this expectation, and it is always
produced by the discourse or common discourse
vectors. a is the proportion of the vocabulary with
p(w) above this threshold:

a = Âw2V
⇥
p(w)> Ew⇠V [p(w|c1s , ...,cns )]

⇤

|V|
(11)

Since we can directly calculate Zecs and a , we can
also directly calculate a = (1�a)/(aZecs).

Common Discourse Vectors We estimate the m
common discourse vectors as the first m singular
vectors from the singular value decomposition of



96

the weighted average vectors. {li} are the weights
on the common discourse vectors. In reality, these
are unique to the word for which p(w|cs) is being
evaluated. However, we let li be:

li =
s2i

Âmj s2j

where si is the singular value for c0i. li can be in-
terpreted as the proportion of variance explained
by {c01, ...,c0m} that is explained by c0i. If removing
the common discourse vectors is a form of denois-
ing (Arora et al., 2017), increasing m, in theory,
should improve results. Because the variance ex-
plained by a singular vector falls with every ad-
ditional vector that is included, the choice of m
is thus a trade-off between variance explained and
computational cost. When m = 1, this is equiva-
lent to the removal in Arora et al. (2017). We fix
m at 5, since we find empirically that singular vec-
tors beyond that do not explain much more vari-
ance. To get cs, we subtract from ecs the weighted
projection on each singular vector:

cs , ecs�
m

Â
i=1

li projc0i ecs

We call this piecewise common component re-
moval. Because our weighting scheme requires
no hyperparameter tuning, it is completely unsu-
pervised. For this reason, we call it unsupervised
smoothed inverse frequency (uSIF). The full algo-
rithm is given in Algorithm 1.

Note that while it is certainly possible to tune
the hyperparameters in our model to achieve op-
timal results, it is not necessary to do so, which
allows our method to be used when there is no la-
belled data. By contrast, in Arora et al.’s model
(2017), hyperparameter tuning is a necessity.

Confound of Vector Length To understand
why this model is not prone to the confound of
word vector length, we reconsider the class of as-
signments for vx,vy,vz in (5) and the resulting val-
ues for ecg and ech. Recall that in our example, sen-
tence g comprises words x,y and sentence h com-
prises two instances of the word z. Under our new
weighting scheme, C in (5) is replaced with C0 =

a
p(x)+ 12 a

. Note that we use p(x) in C0 because of the
simplifying assumption that p(x) = p(y) = p(z).
Assuming again that p(x) ⇡ 0 and that piecewise
common component removal has negligible effect,

Algorithm 1 uSIF Sentence Embedding
Input: vocabulary V , word vectors {vw : w 2 V},
frequencies {p(w) : w 2 V}, sentences S
Output: sentence embeddings {cs : s 2 S}

1: procedure EMBED
2: m 5
3: n Es2S|s|
4: for all s 2 S do

5: a  
Âw2V

h
p(w)>1�

⇣
1� 1|V |

⌘ni

|V |
6: Z |V|/2
7: a (1�a)/(a ·Z)
8: ecs 1|s| Âw2s

a
p(w)+ 12 a

vw
9: end for

10: A 
�
fcs1 . . .fcsn

�

11: for all i in 1...m do
12: c0i ith singular vector of A
13: si ith singular value of A
14: end for
15: for all i in 1...m do
16: li s

2
i

Âmj s2j
17: end for
18: for all s 2 S do
19: cs ecs�Âmi=1 li projc0i ecs
20: end for
21: end procedure

we can see how p(g|cg) and p(h|ch) change in our
random walk model:

p(g|cg) µ ’
w2{x,y}

✓
1�

arccos(cos(cg,vw))
p

◆

p(h|ch) µ
✓

1� arccos(cos(ch,vz))
p

◆2
= 1

Because p(g|cg) is ultimately based on the cosine
similarities between the discourse vector and word
vectors, it is a function of the parameter s 2 [0,1]
that controls the degree of similarity between vx
and vy. For example, for the worst-case assign-
ments (4), p(g|cg) µ 9/16. Conversely, when
vx = vy = vz, we get p(g|cg) = p(h|ch) µ 1. Re-
call that in Arora et al.’s model (2017), b � 2 was
sufficient to ensure the counter-intuitive result of
p(g|cg) � p(h|ch) (6), where b was a scalar that
controlled the word vector length. In contrast, in
our random walk model, the effect of b – and thus
the confound of vector length – is entirely absent;
only the similarity between the word vectors is in-
fluential.



97

4 Results and Discussion

4.1 Textual Similarity Tasks
We test our approach on the SemEval semantic
textual similarity (STS) tasks (2012-2015) (Agirre
et al., 2012, 2013, 2014, 2015), the SemEval 2014
Relatedness task (SICK’14) (Marelli et al., 2014),
and the STS Benchmark dataset (Cer et al., 2017).
In these tasks, the goal is to determine the seman-
tic similarity between a given pair of sentences;
the evaluation criterion is the Pearson correlation
coefficient between the predicted and actual sim-
ilarity scores. To predict the similarity score, we
simply encode each sentence and take the cosine
similarity of their vectors. The individual scores
for STS tasks are in Table 4 in the Appendix and
the average scores are in Table 1. The STS bench-
mark scores are in Table 2. We compare our re-
sults with those from several methods, which are
categorized by Cer et al. (2017) as ‘unsupervised’,
‘weakly supervised’, or ‘supervised’.

4.2 Experimental Settings
For a fair comparison with Arora et al. (2017),
we use the unigram probability distribution used
by them, based on the enwiki dataset (Wikipedia,
3B words). Our preprocessing of the sentences is
limited to tokenization. We try our method with
three types of word vectors: GloVe vectors (Pen-
nington et al., 2014), PARAGRAM-SL999 (PSL)
vectors (Wieting et al., 2015), tuned on the Sim-
Lex999 dataset, and ParaNMT-50 vectors (Wiet-
ing and Gimpel, 2017a), tuned on 51M English-
English sentence pairs translated from English-
Czech sentence pairs. The value of n in (11) is
Es2S|s| ⇡ 11 and was estimated using sentences
from all corpora. The value of a in (9) is then
1.2⇥ 10�3. Our results are denoted as X+UP,
where X 2 {‘GloVe’, ‘PSL’, ‘ParaNMT’}, U de-
notes uSIF-weighting, and P denotes piecewise
common component removal.

4.3 Results
Our model outperforms Arora et al.’s by up to
44.4% on individual tasks (see GloVe+UP vs.
GloVe+WR for the STS’12 MSRpar task in Ta-
ble 4) and by up to 15.5% on yearly averages (see
GloVe+UP vs. GloVe+WR for STS’12 in Table 1).
Our approach proves most useful in cases where
Arora et al. (2017) underperform others, such as
for STS’12, where our models – GloVe+UP and
PSL+UP – outperform their equivalents in Arora

Model STS’12 STS’13 STS’14 STS’15 SICK14
Wieting et al. (2016b) - unsupervised
PP 58.7 55.8 70.9 75.8 71.6
PP-XXL 61.5 58.9 73.1 77.0 72.7
tfidf-GloVe 58.7 52.1 63.8 60.6 69.4
skip-thought 30.8 24.8 31.4 31.0 49.8
Arora et al. (2017) - weakly supervised
GloVe+WR 56.2 56.6 68.5 71.7 72.2
PSL+WR 59.5 61.8 73.5 76.3 72.9
Wieting et al.(2017b) - weakly supervised
LSTM AVG 64.8 63.1 75.8 76.7 71.3
AVG 61.6 59.4 75.8 77.9 72.4
GRAN 62.5 63.4 75.9 77.7 72.9
Conneau et al. (2017) - unsupervised (transfer learning)
InferSent (AllSNLI) 58.6 51.5 67.8 68.3 -
InferSent (SNLI) 57.1 50.4 66.2 65.2 -
Wieting et al. (2017a) - unsupervised
ParaNMT Word Avg. 66.2 61.8 76.2 79.3 -
ParaNMT BiLSTM Avg. 67.4 60.3 76.4 79.7 -
ParaNMT Trigram-Word 67.8 62.7 77.4 80.3 -
Our Approach - unsupervised
GloVe+UP 64.9 63.6 74.4 76.1 73.0
PSL+UP 65.8 65.2 75.9 77.6 72.3
ParaNMT+UP 68.3 66.1 78.4 79.0 73.5

Table 1: Average results (Pearson’s r⇥ 100) on
textual similarity tasks. The highest score in each
column is in bold. “Glove+UP” is the application
of uSIF-weighting (U) and piecewise common
component removal (P) to GloVe word vectors;
“PSL+UP’ to PSL word vectors; “ParaNMT+UP”,
to ParaNMT word vectors.

et al.’s results by 15.5% and 10.6% respectively.
On average, our approach outperforms Arora et
al.’s by around 7.6%, but the improvement is
highly variable. This may be because the hy-
perparameter values we derived may be closer to
the optima for some corpora more than others or
because our other improvements – normalization
and piecewise common component removal – are
more effective for certain datasets.

Our best model, ParaNMT+UP, is also com-
petitive with the state-of-the-art model, ParaNMT
Trigram-Word, an average of trigram and word
embeddings tuned on the ParaNMT-dataset.
ParaNMT+UP outperforms ParaNMT Tri-
gram-Word on STS’12, STS’13, and STS’14; it
is narrowly outperformed on STS’15 and the STS
benchmark. ParaNMT Trigram-Word’s inclusion
of trigram embeddings gives it an edge over
our model for out-of-vocabulary words (Wieting
and Gimpel, 2017a). It should be noted that
ParaNMT+UP outperforms both ParaNMT Word
Avg. and ParaNMT BiLSTM Avg., implying
that our model composes words better than both
simple averaging and BiLSTMs. Similarly, our
model PSL+UP outperforms PP-XXL (Wieting
et al., 2016b), despite the latter using the same
word vectors and a learned projection instead.

Ablation Study On average, our weighting
scheme alone is responsible for a roughly 4.4%



98

Unsupervised
Doc2Vec DBOW (Le and Mikolov, 2014) 64.9
GloVe+UP 71.5
Charagram (Wieting et al., 2016a) 71.6
Paragram-Phrase (Wieting et al., 2016b) 73.2
PSL+UP 74.8
Sent2vec (Pagliardini et al., 2017) 75.5
InferSent (bi-LSTM trained on SNLI) (Conneau et al., 2017) 75.8
ParaNMT Word Avg. (Wieting and Gimpel, 2017a) 79.2
ParaNMT BiLSTM Avg. (Wieting and Gimpel, 2017a) 79.2
ParaNMT+UP 79.5
ParaNMT Trigram-Word Addition (Wieting and Gimpel, 2017a) 79.9

Weakly Supervised
GloVe+WR (Arora et al., 2017) 72.0
GRAN (Wieting and Gimpel, 2017b) 76.4

Supervised
Constituency Tree-LSTM (Tai et al., 2015) 71.9
CNN (HCTI) (Shao, 2017) 78.4

Table 2: Results (Pearson’s r⇥ 100) on the STS
Benchmark dataset. The highest score is in bold.
The scores of our approaches are underlined.

improvement over Arora et al. The piecewise
common component removal alone is responsible
for a roughly 5.1% improvement, and the normal-
ization alone is responsible for a roughly 6.7% im-
provement. This suggests that the benefits of our
individual contributions have much overlap. The
choice of tuned word vectors (e.g., ParaNMT over
GloVe) can also improve results by up to 11.2%.

4.4 Supervised Tasks
We also test our approach on three supervised
tasks: the SICK similarity task (SICK-R), the
SICK entailment task (SICK-E), and the Stan-
ford Sentiment Treebank (SST) binary classifica-
tion task (Socher et al., 2013). To a large ex-
tent, performance on these tasks depends on the
architecture that is trained with the sentence em-
beddings. We take the embeddings that perform
best on the textual similarity tasks, ParaNMT+UP,
and follow the setup in Wieting et al. (2016b). As
seen in Table 3, both SIF-weighting with com-
mon component removal (Arora et al., 2017) and
uSIF-weighting with piecewise common compo-
nent removal (ours) perform slightly better than
simple word averaging, but not as well as more so-
phisticated models. Past work has found that tun-
ing the word embeddings in addition to the param-
eters of the model yields much better performance
(Wieting et al., 2016b), as does increasing the size
of the hidden layer in the classifier (Arora et al.,
2017). The results here, however, suggest that re-
gardless of such changes, our approach would not
be any more effective than Arora et al.’s on these
tasks. Still, our approach retains the advantage of
being a completely unsupervised method that can
be used when there is no labelled data.

Model SST SICK-R SICK-E
ParaNMT-based (Wieting and Gimpel, 2017a)
ParaNMT Word Avg. (300d) 80.0 83.6 80.6
ParaNMT Trigram Avg. (300d) 73.6 79.3 78.0
ParaNMT LSTM Avg. (300d) 80.6 83.9 81.9
LSTM (600d) 80.0 85.2 82.6
LSTM (900d) 81.6 86.0 83.0
BiLSTM (600d) 79.1 85.4 84.3
BiLSTM (900d) 81.3 85.8 84.4
Trigram-Word (600d, concatenation) 79.7 84.6 82.0
Trigram-Word-LSTM (900d, concatenation) 82.0 85.4 83.8
BILSTM AVG (4096) 82.8 85.9 83.8
ParaNMT+WR† (Arora et al., 2017) 80.5 83.9 80.9
ParaNMT+UP† (ours) 80.7 83.8 81.1
Other Approaches
BiLSTM-Max (on AllNLI) (Conneau et al., 2017) 84.6 88.4 86.3
skip-thought (Kiros et al., 2015) 82.0 85.8 82.3
BYTE mLSTM (Radford et al., 2017) 91.8 79.2 -

Table 3: Results on the SST, SICK-R, and SICK-E
tasks. The best score for each task is bolded. †
indicates our implementation.

5 Future Work

There are several possibilities for future work. For
one, the values we derived for Zecs ,a,a and {li}
are not necessarily optimal. While they are based
on reasonable assumptions, there are likely sen-
tence-specific and task-specific values that yield
better results. Hyperparameter search is one way
of finding these values, but that would require su-
pervision. It may be possible, however, to theoret-
ically derive more optimal values.

6 Conclusion

We first showed that word vector length has a
confounding effect on the log-linear random walk
model of generating text (Arora et al., 2017), the
basis of a strong baseline method for sentence em-
beddings. We then proposed an angular distance–
based random walk model where the probability
of a sentence being generated is robust to distor-
tion from word vector length. From this model, we
derived a simple approach for creating sentence
embeddings: normalize the word vectors, com-
pute a weighted average, and then modify it using
SVD. Unlike in Arora et al., our approach does
not require hyperparameter tuning – it is com-
pletely unsupervised and can therefore be used
when there is no labelled data. Our approach out-
performs Arora et al.’s by up to 44.4% on tex-
tual similarity tasks and is even competitive with
state-of-the-art methods. Because our simple ap-
proach is tough-to-beat, robust, and unsupervised,
it is an ideal baseline for computing sentence em-
beddings.



99

Acknowledgments

We thank the Natural Sciences and Engineering
Research Council of Canada (NSERC) for finan-
cial support. We thank John Wieting for provid-
ing pre-trained ParaNMT word embeddings and
Graeme Hirst for his many insightful suggestions.

References
Eneko Agirre, Carmen Banea, Claire Cardie, Daniel M

Cer, Mona T Diab, Aitor Gonzalez-Agirre, Weiwei
Guo, Inigo Lopez-Gazpio, Montse Maritxalar, Rada
Mihalcea, et al. 2015. Semeval-2015 task 2: Seman-
tic textual similarity, English, Spanish and pilot on
interpretability. In Proceedings SemEval@ NAACL-
HLT, pages 252–263.

Eneko Agirre, Carmen Banea, Claire Cardie, Daniel M
Cer, Mona T Diab, Aitor Gonzalez-Agirre, Weiwei
Guo, Rada Mihalcea, German Rigau, and Janyce
Wiebe. 2014. Semeval-2014 task 10: Multilin-
gual semantic textual similarity. In Proceedings Se-
mEval@ COLING, pages 81–91.

Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. Sem 2013 shared
task: Semantic textual similarity, including a pilot
on typed-similarity. In SEM 2013: The Second Joint
Conference on Lexical and Computational Seman-
tics. Association for Computational Linguistics.

Eneko Agirre, Mona Diab, Daniel Cer, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A pi-
lot on semantic textual similarity. In Proceedings of
the First Joint Conference on Lexical and Computa-
tional Semantics-Volume 1: Proceedings of the main
conference and the shared task, and Volume 2: Pro-
ceedings of the Sixth International Workshop on Se-
mantic Evaluation, pages 385–393. Association for
Computational Linguistics.

Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma,
and Andrej Risteski. 2016. A latent variable model
approach to PMI-based word embeddings. Transac-
tions of the Association for Computational Linguis-
tics, 4:385–399.

Sanjeev Arora, Yingyu Liang, and Tengyu Ma. 2017.
A simple but tough-to-beat baseline for sentence em-
beddings. In International Conference on Learning
Representations.

Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3(Feb):1137–1155.

Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-
Gazpio, and Lucia Specia. 2017. Semeval-2017
task 1: Semantic textual similarity-multilingual and
cross-lingual focused evaluation. arXiv preprint
arXiv:1708.00055.

Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of the 25th International Conference on
Machine Learning, pages 160–167. ACM.

Alexis Conneau, Douwe Kiela, Holger Schwenk, Loic
Barrault, and Antoine Bordes. 2017. Supervised
learning of universal sentence representations from
natural language inference data. arXiv preprint
arXiv:1705.02364.

Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. 2014. A convolutional neural network for
modelling sentences. In Proceedings of the 52nd
Annual Meeting of the Association for Computa-
tional Linguistics, pages 655–665.

Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,
Richard Zemel, Raquel Urtasun, Antonio Torralba,
and Sanja Fidler. 2015. Skip-thought vectors. In Ad-
vances in Neural Information Processing Systems,
pages 3294–3302.

Quoc Le and Tomas Mikolov. 2014. Distributed repre-
sentations of sentences and documents. In Proceed-
ings of the 31st International Conference on Ma-
chine Learning (ICML-14), pages 1188–1196.

Thang Luong, Richard Socher, and Christopher D
Manning. 2013. Better word representations with
recursive neural networks for morphology. In
SIGNLL Conference on Computational Natural
Language Learning (CoNLL), pages 104–113.

Marco Marelli, Luisa Bentivogli, Marco Baroni, Raf-
faella Bernardi, Stefano Menini, and Roberto Zam-
parelli. 2014. Semeval-2014 task 1: Evaluation of
compositional distributional semantic models on full
sentences through semantic relatedness and textual
entailment. In Proceedings SemEval@ COLING,
pages 1–8.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems, pages 3111–3119.

Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
the 46th Annual Meeting of the Association for Com-
putational Linguistics, pages 236–244.

Andriy Mnih and Geoffrey Hinton. 2007. Three new
graphical models for statistical language modelling.
In Proceedings of the 24th International Conference
on Machine Learning, pages 641–648. ACM.

Matteo Pagliardini, Prakhar Gupta, and Martin Jaggi.
2017. Unsupervised learning of sentence embed-
dings using compositional n-gram features. arXiv
preprint arXiv:1703.02507.



100

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 1532–1543.

Alec Radford, Rafal Jozefowicz, and Ilya Sutskever.
2017. Learning to generate reviews and discovering
sentiment. arXiv preprint arXiv:1704.01444.

Yang Shao. 2017. Hcti at semeval-2017 task 1:
Use convolutional neural network to evaluate se-
mantic textual similarity. In Proceedings of the
11th International Workshop on Semantic Evalua-
tion (SemEval-2017), pages 130–133.

Richard Socher, Eric H Huang, Jeffrey Pennin, Christo-
pher D Manning, and Andrew Y Ng. 2011. Dy-
namic pooling and unfolding recursive autoencoders
for paraphrase detection. In Advances in Neural In-
formation Processing Systems, pages 801–809.

Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D Manning, Andrew Ng, and
Christopher Potts. 2013. Recursive deep models
for semantic compositionality over a sentiment tree-
bank. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1631–1642.

Kai Sheng Tai, Richard Socher, and Christopher D
Manning. 2015. Improved semantic representations
from tree-structured long short-term memory net-
works. Proceedings of the 53rd Annual Meeting of
the Association for Computational Linguistics.

John Wieting, Mohit Bansal, Kevin Gimpel, and Karen
Livescu. 2016a. Charagram: Embedding words and
sentences via character n-grams. arXiv preprint
arXiv:1607.02789.

John Wieting, Mohit Bansal, Kevin Gimpel, and Karen
Livescu. 2016b. Towards universal paraphrastic
sentence embeddings. In International Conference
on Learning Representations.

John Wieting, Mohit Bansal, Kevin Gimpel, Karen
Livescu, and Dan Roth. 2015. From paraphrase
database to compositional paraphrase model and
back. Transactions of the Association for Compu-
tational Linguistics, 3:345–358.

John Wieting and Kevin Gimpel. 2017a. Pushing
the limits of paraphrastic sentence embeddings with
millions of machine translations. arXiv preprint
arXiv:1711.05732.

John Wieting and Kevin Gimpel. 2017b. Revisiting re-
current networks for paraphrastic sentence embed-
dings. arXiv preprint arXiv:1705.00364.


