



















































Opinion Recommendation Using A Neural Model


Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1626–1637
Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics

Opinion Recommendation Using A Neural Model∗

Zhongqing Wang†,‡ and Yue Zhang‡
†Soochow University, Suzhou, China

‡Singapore University of Technology and Design, Singapore
wangzq.antony@gmail.com, yue zhang@sutd.edu.sg,

Abstract

We present opinion recommendation, a
novel task of jointly generating a review
with a rating score that a certain user
would give to a certain product which is
unreviewed by the user, given existing re-
views to the product by other users, and
the reviews that the user has given to oth-
er products. A characteristic of opinion
recommendation is the reliance of multi-
ple data sources for multi-task joint learn-
ing. We use a single neural network to
model users and products, generating cus-
tomised product representations using a
deep memory network, from which cus-
tomised ratings and reviews are construct-
ed jointly. Results show that our opinion
recommendation system gives ratings that
are closer to real user ratings on Yelp.com
data compared with Yelp’s own ratings.
our methods give better results compared
to several pipelines baselines.

1 Introduction

Offering a channel for customers to share opin-
ions and give scores to products and services, re-
view websites have become a highly influential in-
formation source that customers refer to for mak-
ing purchase decisions. Popular examples include
IMDB.com on the movie domain, Epinions.com
on the product domain, and Yelp.com on the ser-
vice domain. Figure 1 shows a screenshot of a
restaurant review page on Yelp.com, which offers
two main types of information. First, an overal-
l rating score is given under the restaurant name;
second, detailed user reviews are listed below the
rating.

∗This work has been done when the first author worked
at SUTD.

Figure 1: A restaurant review on Yelp.com.

Though offering useful overview and details
about a product or service, such information has
several limitations for a user who has not used the
product or service. First, the overall rating is gen-
eral and not necessarily agreeable to the taste of
an individual customer. Being a simple reflection
of all customer scores, it serves an average cus-
tomer well, but can be rather inaccurate for indi-
viduals. For example, the authors themselves of-
ten find highly rated movies being tedious. Sec-
ond, there can be hundreds of reviews for a prod-
uct or service, which makes it infeasible for ex-
haustive reading. It would be useful to have a brief
summary of all reviews, which ideally should be
customized to the reader.

To address the limitations above, we propose a
new task called opinion recommendation, which
is to generate a customized review score of the
product that the user is likely to give, as well as a
customized review that the user would have written
for the target product, if the user had reviewed the
product. The proposed opinion recommendation
task is closely related to several existing lines of
work in NLP. The first is sentiment analysis (Hu
and Liu, 2004; Pang and Lee, 2008) and opinion
summarization (Nishikawa et al., 2010; Wang and
Ling, 2016), which is to give a rating score or sum-
mary based on existing customer reviews. Our

1626



task is different in that we aim to generate user rat-
ing scores and review of a product unreviewed by
the user. The second is recommendation (Su and
Khoshgoftaar, 2009; Yang et al., 2014), which is to
give a ranking score to a certain product or service
based on the purchase history of the user and other
customers who have purchased the target product.
Our task is different in the source of input, which
is textual customer reviews and ratings rather than
numerical purchase history.

There are two types of inputs for our task,
namely existing reviews of the target product, and
the reviews of the user on other products, and t-
wo types of outputs, namely a customized rating
score and a customized review. The ideal solu-
tion should consider the interaction between all
given types of information, jointly predicting the
two types of outputs. This poses significant chal-
lenges to statistical models, which require man-
ually defined features to capture relevant pattern-
s from training data. Deep learning is a relative-
ly more feasible choice, offering viabilities of in-
formation fusion by fully connected hidden layer-
s (Collobert et al., 2011; Henderson et al., 2013;
Zhang and Weiss, 2016; Chen et al., 2016a). We
leverage this advantage in building our model.

In particular, we use a sub RNN to model the
semantic content of each review. A sub product
model is used to consolidate existing reviews for
the target product, and a user model is built by
consolidating the reviews of the given user into a
single vector form. To address potential sparsi-
ty of a user’s history reviews, neighbor users are
identified by collaborative filtering (Ding et al.,
2006), and a vector representation is learned by
using a neural neighborhood model. Finally, a
deep memory network is utilized to find the asso-
ciation between the user and target product, joint-
ly yielding the rating score and customised re-
view. Experiments on a Yelp dataset show that
the model outperforms several pipelined base-
lines. We make our source code publicly avail-
able under GPL at https://github.com/
wangzq870305/opinion_recommend.

2 Related Work

Sentiment Analysis. Our task is related to
document-level sentiment classification (Pang and
Lee, 2008) for various neural network model-
s have been used, including convolutional neu-
ral networks (Kim, 2014), recursive neural net-

work (Socher et al., 2013) and recurrent neural
network (Teng et al., 2016; Tai et al., 2015), Re-
view rating prediction aims to predict the numer-
ic rating of a given review. Pang and Lee (2005)
pioneered this task by regarding it as a classifica-
tion/regression problem. Most subsequent work
focuses on designing effective textural features of
reviews (Qu et al., 2010; Li et al., 2011; Wan,
2013).

User information has been widely investigated
in sentiment analysis. Gao et al. (2013) developed
user-specific features to capture user leniency, and
Li et al. (2014) incorporated textual topic and user-
word factors through topic modeling. For integrat-
ing user information into neural network models,
Tang et al. (2015) predicted the rating score giv-
en a review by using both lexical semantic infor-
mation and a user embedding model. Chen et al.
(2016b) proposed a neural network to incorporate
global user and product information for sentiment
classification via an attention mechanism.

Different from the above research, which focus-
es on predicting the opinion on existing reviews,
our task is to recommend the score that a user
would give to a new product without knowing his
review text. The difference originates from the ob-
jective. Previous research aims to predict opinions
on reviewed products, while our task is to recom-
mend opinion on new products, which the user has
not reviewed.

Opinion Summarization. Our work also over-
laps with to the area of opinion summarization,
which constructs natural language summaries for
multiple product reviews (Hu and Liu, 2004).
Most previous work extracts opinion words and
aspect terms. Typical approaches include asso-
ciation mining of frequent candidate aspects (Hu
and Liu, 2004; Qiu et al., 2011), sequence label-
ing based methods (Jakob and Gurevych, 2010;
Yang and Cardie, 2013), as well as topic mod-
eling techniques (Lin and He, 2009). Recently,
word embeddings and recurrent neural network-
s are also used to extract aspect terms (Irsoy and
Cardie, 2014; Liu et al., 2015). While all the meth-
ods above are extractive, Ganesan et al. (2010) p-
resented a graph-based summarization framework
to generate concise abstractive summaries of high-
ly redundant opinions, and Wang and Ling (2016)
used an attention-based neural network model to
absorb information from multiple text units and
generate summaries of movie reviews. We also

1627



perform abstractive summarization. However, dif-
ferent from the above research, which summarize
existing reviews, we generate customized reviews
for a unreviewed product.

Recommendation. has been solved on mainly
purchase history. There are two main approach-
es, which are content-based and collaborative-
filtering (CF) based (Adomavicius and Tuzhilin,
2005; Yang et al., 2014), respectively. Most exist-
ing social recommendation systems are CF-based,
and can be further grouped into model-based CF
and neighborhood-based CF (Kantor et al., 2011;
Su and Khoshgoftaar, 2009). Matrix Factoriza-
tion (MF) is one of the most popular models for
CF. In recent MF-based social recommendation
works, user-user social trust information is inte-
grated with user-item feedback history (e.g., rat-
ings, clicks, purchases) to improve the accuracy
of traditional recommendation systems, which on-
ly factorize user-item feedback data (Ding et al.,
2006; Koren, 2008; He et al., 2016).

There has been work integrating sentiment anal-
ysis and recommendation systems, which use rec-
ommendation strategies such as matrix factoriza-
tion to improve the performance of sentiment anal-
ysis (Leung et al., 2006; Singh et al., 2011). These
methods typically use ensemble learning (Singh
et al., 2011) or probabilistic graph models (Wu
and Ester, 2015). For example, Zhang et al.
(2014) proposed a factor graph model to recom-
mend opinion rating scores by using explicit prod-
uct features as hidden variables. Different from
the above research, we recommend user opinions.

Neural Network Models. Multi-task learn-
ing has been recognised as a strength of neu-
ral network models for natural language process-
ing (Collobert et al., 2011; Henderson et al., 2013;
Zhang and Weiss, 2016; Chen et al., 2016a), where
hidden feature layers are shared between different
tasks that have common basis. Our work can be
regarded as an instance of such multi-tasks learn-
ing via shared parameters, which has been widely
used in the research community recently.

Dynamic memory network models have been
applied for NLP tasks such as question answer-
ing (Sukhbaatar et al., 2015; Kumar et al., 2016),
language modeling (Tran et al., 2016) and ma-
chine translation (Wang et al., 2016). There are
typically used to find abstract semantic represen-
tations of texts towards certain tasks, which are
consistent with our main need, namely abstract-

LSTM
LSTM

LSTM

Attention

Customized Review

Attention

Hop n
Deep memory

…

User Model

LSTM

Attention

…

Neighborhood Model

Rating Score

Product Model

…

r1 r2 rm
r1 r2 rn r1 r2 rn

Figure 2: Overview of proposed model.

ing the representation of a product that is biased
towards the taste of a certain user. We use a vari-
ation of the memory network model for obtaining
user-specific review representation.

3 Model

Formally, the input to our model is a tuple
〈RT , RU , RN 〉, where RT = {rT1 , rT2 , ..., rTnt}
is the set of existing reviews of a target product,
RU = {rU1 , rU2 , ..., rUnu} is the set of user’s his-
tory reviews, and RN = {rN1 , rN2 , ..., rNnn} is
the set of the user’s neighborhood reviews. All the
reviews are sorted with temporal order. The output
is a pair 〈YS , YR〉, where YS is a real number be-
tween 0 and 5 representing the customized rating
score of the target product, and YR is a customised
review. A characteristic of our model is that YS
and YR are generated on a product that the user
has not reviewed.

For capturing both general and personalized in-
formation, we first build a product model, a user
model, and a neighborhood model, respectively,
and using a memory network model to integrate
these three types of information, constructing a
customized product model. Finally, we predict a
customized rating score and a review collectively
using neural stacking framework. The overall ar-
chitecture of the model is shown in Figure 2.

3.1 Review Model

A review is the foundation of our model, based
on which we derive representations of both a us-
er and a target product. In particular, a user pro-
file can be achieved by modeling all the reviews
RU of the user, and a target product profile can
be obtained by using all existing reviews RT of
the product. We use the average of word embed-
dings to model a review. Formally, given a review
r = {x1, x2, ..., xm}, where m is the length of

1628



the review, each word xk is represented with a K-
dimensional embedding ewk (Mikolov et al., 2013).
We use the

∑
k(e

w
k )/m for the representation of

the review ed(r).

3.2 User Model

A standard LSTM (Hochreiter and Schmidhu-
ber, 1997) is used to learn the hidden states
of an user’s reviews to build the user mod-
el. Denoting the recurrent function at step t as
LSTM(xt, ht−1), we obtain a sequence of hid-
den state vectors {hU1 , hU2 , ..., hUnu} recurrently
by feeding {ed(rU1), ed(rU2), ..., ed(rUnu )} as in-
puts, where hUi = LSTM(e

d(rUi), hUi−1). The
initial state and all standard LSTM parameters are
randomly initialized and tuned during training.

Not all reviews contribute equally to the repre-
sentation of a user. We use the attention mech-
anism (Bahdanau et al., 2014; Yang et al., 2016)
to extract the reviews that are relatively more
important, aggregating the representation of re-
views to form a vector. Taking the hidden s-
tate {hU1 , ...hU2 , ..., hUnu} of user model as input,
the attention model outputs, a continuous vector
vU ∈ Rd×1, which is computed as a weighted sum
of each hidden state hUi , namely

vU =
nu∑
i

αihUi (1)

where nu is the hidden variable size, αi ∈ [0, 1] is
the weight of hUi , and

∑
i αi = 1.

For each piece of hidden state hUi , the scoring
function is calculated by

ui = tanh(WUhUi + bU ) (2)

αi =
exp(ui)∑
j exp(uj)

(3)

where WU and bU are model parameters. The at-
tention vector vU is used to represent the user for
the User Model.

3.3 Neighborhood Model

We use neighborhood reviews to improve the us-
er model, since a user may not have sufficient re-
views to construct a reliable model. Here a neigh-
bor refers to a user that has similar tastes to the
target user (Koren, 2008; Desrosiers and Karypis,
2011). The same as the user model, we construct

the neighborhood model vN using the neighbor-
hood reviews RN = {rN1 , rN2 , ..., rNnn} with an
attention recurrent network.

A key issue in building the neighborhood model
is how to find neighbors of a certain user. In this
study, we use matrix factorization (Koren, 2008)
to detect neighbors, which is a standard approach
for recommendation (Ding et al., 2006; Li et al.,
2009; He et al., 2016). In particular, users’ rat-
ing scores of products are used to build a product-
users matrix M ∈ Rnt×nu with nt products and
nu users. We approximate it using three factors,
specifying soft membership of products and user-
s (Ding et al., 2006) by finding:

min
F,S,T

||M − FST T ||
s.t.S ≥ 0, F ≥ 0, T ≥ 0

(4)

where F ∈ Rnt×K represents the posterior prob-
ability of K topic clusters for each product; S ∈
RK×K encodes the distribution of each topic k;
and T ∈ RK×nu indicates the posterior probabili-
ty of K topic clusters for each user.

As a result of matrix factorization, we directly
obtain the probability of each user on each topic
from the person-topic matrix T . To infer T , the
optimization problem in Eq.4 can be solved using
the following updating rule:

Tjk ← Tjk (M
TFS)jk

(TT TMTFS)jk
(5)

With the user-topic matrix T , we measure the im-
plicit connection between two users using:

sim(i, j) =
k∑

k=1

TikTjk (6)

where sim(i, j) measure the implicit connection
degree between users i and j. If sim(i, j) is high-
er than a threshold η, we consider user j as the
neighbor of user i.

3.4 Product Model
Given the representations of existing reviews
{ed(rT1), ed(rT2), ..., ed(rTnt )} of the produc-
t, we use LSTM to model their temporal or-
ders, obtaining a sequence of hidden vectors
hT = {hT1 , hT2 , ..., hTnt} by recurrently feeding
{ed(rT1), ed(rT2), ..., ed(rTnt} as inputs. The hid-
den state vectors hT are used to represent the prod-
uct.

1629



Customized Product Model. The produc-
t model represents salient information of existing
reviews in their temporal order, yet do not reflect
the taste of a particular user. We build the cus-
tomised product model to integrate user informa-
tion and product information (as reflected by the
product model), resulting in a single vector that
represents a customised product. From this vector
we are able to synthesis both a customised review
and a customised rating score. In particular, we
use the user representation vU and the neighbour
representation vN to transform the target produc-
t representation hT = {hT1 , hT2 , ..., hTnt} into a
customised product representation vC , which is
tailored to the taste of the user.

A naive model of yielding vC could utilise the
attention mechanism over ht, deriving a weight-
ed sum according to user information. On the
other hand, dynamic memory networks have been
shown highly useful for deriving abstract semantic
information compared with simple attention, and
hence we follow Sukhbaatar et al. (2015) and X-
iong et al. (2016), building a variation of DMN
to iteratively find increasingly abstract representa-
tions of ht, by injecting vU and vN information.

The memory model consists of multiple dynam-
ic computational layers (hops), each of which con-
tains an attention layer and a linear layer. In the
first computational layer (hop 1), we take the hid-
den variables hTi (0 ≤ i ≤ nt) of product model
as input, adaptively selecting important evidences
through one attention layer using vU and vN . The
output of the attention layer gives a linear inter-
polation of hT , and the result is considered as in-
put to the next layer (hop 2). In the same way,
we stack multiple hops and run the steps multiple
times, so that more abstract representations of the
target product can be derived.

The attention model outputs a continuous vector
vC ∈ Rd×1, which is computed as a weighted sum
of hTi (0 ≤ i ≤ nt), namely

vC =
nt∑
i

βihTi (7)

where nt is the hidden variable size, βi ∈ [0, 1] is
the weight of hTi , and

∑
i βi = 1. For each piece

of hidden state hTi , we use a feed forward neural
network to compute its semantic relatedness with
the abstract representation vC . The scoring func-

tion is calculated as follows at hop t:

uti = tanh(WThTi +WCv
t−1
C

+WUvU +WNvN + b)
(8)

βti =
exp(uti)∑
j exp(u

t
j)

(9)

The vector vC is used to represent the customized
product model. At the first hop, we define V 0C =∑

i hTi/nt.
The product model hTi (0 ≤ i ≤ nt) rep-

resents salient information of existing reviews in
their temporal order, they do not reflect the taste
of a particular user. We use the customised prod-
uct model to integrate user information and prod-
uct information (as reflected by the product mod-
el), resulting in a single vector that represents a
customised product. From this vector we are able
to synthesis both a customised review and a cus-
tomised rating score.

3.5 Customized Review Generation

The goal of customized review generation is to
generate a review YR from the customized prod-
uct representation vC , composed by a sequence of
words yR1 , ..., yRnr . We use a standard LSTM de-
coder (Rush et al., 2015) to decompose the predic-
tion of YR into a sequence of word-level predic-
tions:

logP (YR|vC) =∑
j

P (yRj |yR1 , ..., yRj−1 , vC) (10)

where each word yRj is predicted conditional on
the previously generated yR1 , ..., yRj−1 and the
customized product vector vC . The probability is
estimated by using standard word softmax:

P (yRj |yR1 , ..., yRj−1 , vC) =
softmax(hRj )

(11)

where hRj is the hidden state variable at times-
tamp j, which is modeled as LSTM(uj−1, hRj).
Here a LSTM is used to generate a new state hRj
from the representation of the previous state hRj−1
and uj−1. uj−1 is the concatenation of previously
generated word yRj−1 and the input representation
of customized model vC .

1630



3.6 Customized Rating Prediction

A straightforward approach to predicting the rat-
ing score of a product is to take the average of ex-
isting review scores. However, the drawback is
that it cannot reflect the the variance in user tastes.
In order to integrate user preferences into the rat-
ing, we instead take a user-based weighted aver-
age of existing rating scores, so that the scores of
reviews that are closer to the user preference are
given higher weights. However, existing ratings
can be all different from a users personal rating, if
the existing reviews do not come from the user’s
neighbours. We thus use the customized product
vector vc as a bias of the weighted average of ex-
isting rating scores.

Formally, given the rating scores s1, s2, ..., sn
of existing reviews, and the the customized prod-
uct representation vC , we calculate:

YS =
n∑
i

βi · si + µ tanh(WSvC + bS) (12)

In the left term
∑n

i βi ·si, we use attention weights
βi in Eq.9 to measure the important of each rating
score si. The right term tanh(WSvC + bS) is a
review-based shift, weighted by µ.

Since the result of customized review genera-
tion can be helpful for rating score prediction, we
use neural stacking additionally feeding the last
hidden state hRn of review generation model as
input for YS prediction, resulting in

YS =
n∑
i

αi · si+

+ µ tanh(WS(vC ⊕ hRn) + bS)
(13)

where ⊕ denotes vector concatenation.
3.7 Training

For our task, there are two joint training objec-
tives, for review scoring and review summarisa-
tion, respectively. For review scoring, the loss
function is defined as:

L(Θ) =
N∑

i=1

(Y ∗Si − YSi)2 +
λ

2
||Θ||2 (14)

where Y ∗Si is the predicted rating score, YSi is the
rating score in the training data, Θ is the set of
model parameters and λ is a parameter for L2 reg-
ularization.

Amount
Business 15,584
Review 334,997
User 303,032

Table 1: Statistics of the dataset.

For customized review generation, loss is de-
fined by maximizing the log probability of E-
q.10 (Sutskever et al., 2014; Rush et al., 2015).
The two loss functions for score and review pre-
diction share the representation vectors under vC ,
hence forming multi-task learning.

Standard back propagation is performed to op-
timize parameters, where gradients also propagate
from the scoring objective to the review genera-
tion objective due to neural stacking (Eq.13). We
apply online training, where model parameters are
optimized by using AdaGrad (Duchi et al., 2011).
Word embeddings are trained using the Skip-gram
algorithm (Mikolov et al., 2013)1.

4 Experiments

4.1 Experimental Settings
Our data are collected from the yelp academic
dataset2, provided by Yelp.com, a popular restau-
rant review website. The data set contains three
types of objects: business, user, and review, where
business objects contain basic information about
local businesses (i.e. restaurants), review objects
contain review texts and star rating, and user ob-
jects contain aggregate information about a single
user across all of Yelp. Table 1 illustrates the gen-
eral statistics of the dataset.

For evaluating our model, we choose 4,755
user-product pairs from the dataset. The user-
product pairs are extracted by following criterions:
for each selected user-product pair, the user should
have written 10 reviews at least, and the product
should contain 100 reviews at least. In addition,
the gold-standard review that the user write for the
corresponding product should contain 10 helpful
hits at least. We did not try alternative data selec-
tion rules. We will give the detail in our draft.

For each pair, the existing reviews of the target
service (restaurant) are used for the product mod-
el. The rating score given by each user to the target
service is considered as the gold customized rating
score, and the review of the target service given by

1 https://code.google.com/p/word2vec/
2https://www.yelp.com/academic dataset

1631



each user is used as the gold-standard customized
review for the user. The remaining reviews of each
user are used for training the user model. We use
3,000 user-product pairs to train the model, 1,000
pairs as testing data, and remaining data for devel-
opment.

We use the ROUGE-1.5.5 (Lin, 2004) toolk-
it for evaluating the performance of customized
review generation, and report unigram overlap
(ROUGE-1) as a means of assessing informative-
ness. Mean Square Error (MSE) (Wan, 2013; Tang
et al., 2015) is used as the evaluation metric for
measuring the performance of customized rating
score prediction. MSE penalizes more severe er-
rors more heavily.

4.2 Hyper-parameters

There are several important hyper-parameters in
our models, and we tune their values using the
development dataset. We set the regularization
weight λ = 10−8 and the initial learning rate to
0.01. We set the size of word vectors to 128, the
size of hidden vectors in LSTM to 128. In order to
avoid over-fitting, dropout (Hinton et al., 2012) is
used for word embedding with a ratio of 0.2. The
neighbor similarity threshold η is set to 0.25.

4.3 Development Experiments

4.3.1 Ablation Test

Effects of various configurations of our model, are
shown on Table 2, where Joint is the full model of
this paper, -user ablates the user model, -neighbor
ablates the neighbor model, -rating is a single-task
model that generates a review without the rating
score, and -generation yields only the rating.

By comparing “Joint” and “-user,-neighbor”,
we can find that customized information have sig-
nificant influence on both the rating and review
generation results (p − value < 0.01 using t-
test). In addition, comparison between “-Joint”
and “-user”, and between “-user” and “-user, -
neighbor” shows that both the user information
and the neighbour user information of the user are
effective for improving the results. A users neigh-
bours can indeed alleviate scarcity of user reviews.

Finally, comparison between “Joint” and “-
generation”, and between “Joint” and “-rating”
shows that multi-task learning by parameter shar-
ing is highly useful.

Rating Generation
Joint 0.904 0.267
-user 1.254 0.220
-neighbor 1.162 0.245
-user,-neighbor 1.342 0.205
-rating - 0.254
-generation 1.042 -

Table 2: Feature ablation tests.

0.20

0.21

0.22

0.23

0.24

0.25

0.26

0.27

0.28

0 1 2 3 4 5 6 7 8 9 10

R
O

U
G

E
-1

Hop

Figure 3: Influence of hops.

4.3.2 Influence of Hops
We show the influence of hops of memory net-
work for customized review generation on Fig-
ure 3. When hop = 0, the model considers only
the general product reviews (−user,−neighbor).
When hop ≥ 1, customized product information
is leveraged. From the figure we can find that,
when hop = 3, the performance is the best. It
indicates that multiple hops can capture more ab-
stract evidences from external memory to improve
the performance. However, too many hops leads
to over-fitting, thereby harms the performance. As
a result, we choose 3 as the number of hops in our
final test.

4.3.3 Influence of µ
We show the influence of the bias weight parame-
ter µ for rating prediction in Figure 4. With µ be-
ing 0, the model uses the weighted sum of existing
reviews to score the product. When µ is very large,
the system tends to use only the customized prod-
uct representation vc to score the product, hence
ignoring existing review scores, which are a use-
ful source of information. Our results show that
when µ is 1, the performance is optimal, thus indi-
cating both existing review scores and review con-
tents are equally useful.

4.4 Final Results

We show the final results for opinion recommen-
dation, comparing our proposed model with the

1632



HOP Bais
0 1.342 0 1.102
1 1.102 1 0.904
2 1.046 2 1.067
3 0.904 3 1.136
4 0.987 4 1.206
5 1.102 5 1.227
6 1.045
7 1.126
8 1.172
9 1.152
10 1.167

0.90

0.95

1.00

1.05

1.10

1.15

1.20

1.25

0 1 2 3 4 5

M
S

E

μ

0.90

0.95

1.00

1.05

1.10

1.15

1.20

1.25

1.30

1.35

1.40

0 1 2 3 4 5 6 7 8 9 10

M
S

E

hop

Figure 4: Influence of bias score.

following state-of-the-art baseline systems:

• RS-Average-Yelp is the widely-adopted base-
line (e.g., by Yelp.com), using the averaged
review scores as the final score.

• RS-Linear estimates the rating score that a
user would give by sui = sall + su + si (Ric-
ci et al., 2011), where su and si are the the
training deviations of rating score of the user
u and the product i, respectively.

• RS-Item applies kNN to estimate the rating
score (Sarwar et al., 2001). We choose the
cosine similarity between vc to measure the
distance between product.

• RS-MF is a state-of-the-art recommendation
model, which uses matrix factorisation to
predict rating score (Ding et al., 2006; Li
et al., 2009; He et al., 2016).

• Sum-Opinosis uses a graph-based framework
to generate abstractive summarisation given
redundant opinions (Ganesan et al., 2010).

• Sum-LSTM-Att is a state-of-the-art neural ab-
stractive summariser, which uses an atten-
tional neural model to consolidate informa-
tion from multiple text sources, generat-
ing summaries using LSTM decoding (Rush
et al., 2015; Wang and Ling, 2016).

Being non-opinion recommendation methods,
all the baselines are single-task models, with-
out considering rating and summarisation predic-
tion jointly. The results are shown in Table 3.
Our model (“ Joint”) significantly outperforms
both “RS-Average-Yelp” and “RS-Linear” (p −
value < 0.01 using t-test). Note that, our pro-
posed rating recommendation for the user are sig-
nificantly closer individual real user rating com-
pared with Yelp’s rating.

Rating Generation
RS-Average-Yelp 1.280 -
RS-Linear 1.234 -
RS-Item 1.364 -
RS-MF 1.143 -
Sum-Opinosis - 0.183
Sum-LSTM-Att - 0.196
Joint 1.023 0.250

Table 3: Final results.

Our proposed model also significantly outper-
forms state-of-the-art recommendation systems
(RS-Item and RS-MF) (p− value < 0.01 using t-
test), indicating that textual information are a use-
ful addition to the rating scores themselves for rec-
ommending a product.

Finally, comparison between our proposed
model and state-of-the-art summarisation tech-
niques (Sum-Opinosis and Sum-LSTM-Att)
shows the advantage of leveraging user informa-
tion to enhance customised review generation,
and also the strength of joint learning.

4.5 Example Output

Table 4 shows example outputs of rating scores
and reviews. Ref. is the rating score and re-
view written by user her/himself, and Base is the
baseline model, that generates the rating score by
RS-MF, and review by Sum-LSTM-Att. From
these examples, we can find that, both rating s-
core and review which generated by the proposed
Joint model is closer to the real user. In particular,
in the first example, the baseline system correctly
identifies the main both price and quality informa-
tion, which the target user wrote in the review, yet
the baseline model did not yield comments about
the price based on reviews of other users. Associ-
ating reviews and ratings closely, the joint model
gives a rating score that is much closer to the re-
al user score compared to the score given by the
recommendation model MF. In addition, we can
also find some habits of certain users from their
customized reviews, for example, Mexican food,
cheap and clean restaurant.

5 Conclusion

We proposed a novel task called opinion recom-
mendation, which is to generate the review and
rating score that a certain user would give to an
unreviewed product or service. In particular, a

1633



Rating Review

Ref. 5.0
Amazing beer selection, enough
food choices, and a much smaller
bill than I was expecting ...

Base 4.0
Boulders is JAM, favorite neigh-
borhood bar, have amazing food
...

Joint 4.6 Bar is cheap, food is good enough...

Ref. 4.0
This is one of my favorite Mex-
ican fast food restaurants. It’s
clean and cool in the summer...

Base 5.0
The restaurant is great, This
Chipotle is a great location, Their
medium salsa good ...

Joint 4.2 Mexican food my favorite, placeis clean ...

Table 4: Example outputs.

deep memory network was utilized to find the as-
sociation between the user and the product, jointly
yielding the rating score and customised review.
Results show that our methods are better result-
s compared to several pipelines baselines using
state-of-the-art sentiment rating and summarisa-
tion systems. Review scores given by the opinion
recordation system are closer to real user review
scores compared to the review scores which Yelp
assigns to target products.

Acknowledgments

The corresponding author is Yue Zhang. We are
grateful for the help of Xuanyi Li for his initial
exploration. We thank our anonymous reviewers
for their constructive comments, which helped to
improve the paper. This work is supported by the
Temasek Lab grant IGDST1403012 at Singapore
University of Technology and Design, and sup-
ported by the National Natural Science Founda-
tion of China (No.61402314).

References
Gediminas Adomavicius and Alexander Tuzhilin.

2005. Toward the next generation of recommender
systems: A survey of the state-of-the-art and pos-
sible extensions. IEEE transactions on knowledge
and data engineering, 17(6):734–749.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by joint-
ly learning to align and translate. CoRR, ab-
s/1409.0473.

Hongshen Chen, Yue Zhang, and Qun Liu. 2016a.
Neural network for heterogeneous annotations. In
Proceedings of the 2016 Conference on Empirical

Methods in Natural Language Processing, EMNLP
2016, Austin, Texas, USA, November 1-4, 2016,
pages 731–741.

Huimin Chen, Maosong Sun, Cunchao Tu, Yankai Lin,
and Zhiyuan Liu. 2016b. Neural sentiment classifi-
cation with user and product attention. In Proceed-
ings of the 2016 Conference on Empirical Method-
s in Natural Language Processing, EMNLP 2016,
Austin, Texas, USA, November 1-4, 2016, pages
1650–1659.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel P. Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research,
12:2493–2537.

Christian Desrosiers and George Karypis. 2011. A
comprehensive survey of neighborhood-based rec-
ommendation methods. In Recommender Systems
Handbook, pages 107–144.

Chris Ding, Tao Li, Wei Peng, and Haesun Park. 2006.
Orthogonal nonnegative matrix t-factorizations for
clustering. In Proceedings of the 12th ACM SIGKD-
D international conference on Knowledge discovery
and data mining, pages 126–135. ACM.

John C. Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. Journal of Machine
Learning Research, 12:2121–2159.

Kavita Ganesan, ChengXiang Zhai, and Jiawei Han.
2010. Opinosis: a graph-based approach to abstrac-
tive summarization of highly redundant opinions. In
Proceedings of the 23rd international conference on
computational linguistics, pages 340–348. Associa-
tion for Computational Linguistics.

Wenliang Gao, Naoki Yoshinaga, Nobuhiro Kaji, and
Masaru Kitsuregawa. 2013. Modeling user leniency
and product popularity for sentiment classification.
In IJCNLP, pages 1107–1111.

Xiangnan He, Hanwang Zhang, Min-Yen Kan, and
Tat-Seng Chua. 2016. Fast matrix factorization for
online recommendation with implicit feedback. In
Proceedings of the 39th International ACM SIGIR
conference on Research and Development in Infor-
mation Retrieval, SIGIR 2016, Pisa, Italy, July 17-
21, 2016, pages 549–558.

James Henderson, Paola Merlo, Ivan Titov, and
Gabriele Musillo. 2013. Multilingual joint pars-
ing of syntactic and semantic dependencies with a
latent variable model. Computational Linguistics,
39(4):949–998.

Geoffrey E. Hinton, Nitish Srivastava, Alex
Krizhevsky, Ilya Sutskever, and Ruslan Salakhut-
dinov. 2012. Improving neural networks by
preventing co-adaptation of feature detectors.
CoRR, abs/1207.0580.

1634



Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural Computation,
9(8):1735–1780.

Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the Tenth
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, Seattle, Washing-
ton, USA, August 22-25, 2004, pages 168–177.

Ozan Irsoy and Claire Cardie. 2014. Opinion mining
with deep recurrent neural networks. In EMNLP,
pages 720–728.

Niklas Jakob and Iryna Gurevych. 2010. Extracting
opinion targets in a single and cross-domain setting
with conditional random fields. In Proceedings of
the 2010 Conference on Empirical Methods in Nat-
ural Language Processing, EMNLP 2010, 9-11 Oc-
tober 2010, MIT Stata Center, Massachusetts, USA,
A meeting of SIGDAT, a Special Interest Group of
the ACL, pages 1035–1045.

Paul B Kantor, Lior Rokach, Francesco Ricci, and
Bracha Shapira. 2011. Recommender systems hand-
book.

Yoon Kim. 2014. Convolutional neural networks for
sentence classification. In Proceedings of the 2014
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP 2014, October 25-29,
2014, Doha, Qatar, A meeting of SIGDAT, a Special
Interest Group of the ACL, pages 1746–1751.

Yehuda Koren. 2008. Factorization meets the neigh-
borhood: a multifaceted collaborative filtering mod-
el. In Proceedings of the 14th ACM SIGKDD in-
ternational conference on Knowledge discovery and
data mining, pages 426–434. ACM.

Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyy-
er, James Bradbury, Ishaan Gulrajani, Victor Zhong,
Romain Paulus, and Richard Socher. 2016. Ask me
anything: Dynamic memory networks for natural
language processing. In Proceedings of the 33nd In-
ternational Conference on Machine Learning, ICM-
L 2016, New York City, NY, USA, June 19-24, 2016,
pages 1378–1387.

Cane WK Leung, Stephen CF Chan, and Fu-lai Chung.
2006. Integrating collaborative filtering and sen-
timent analysis: A rating inference approach. In
Proceedings of the ECAI 2006 workshop on recom-
mender systems, pages 62–66.

Fangtao Li, Nathan Nan Liu, Hongwei Jin, Kai Zhao,
Qiang Yang, and Xiaoyan Zhu. 2011. Incorporat-
ing reviewer and product information for review rat-
ing prediction. In IJCAI 2011, Proceedings of the
22nd International Joint Conference on Artificial In-
telligence, Barcelona, Catalonia, Spain, July 16-22,
2011, pages 1820–1825.

Fangtao Li, Sheng Wang, Shenghua Liu, and Ming
Zhang. 2014. Suit: A supervised user-item based
topic model for sentiment analysis. In AAAI, pages
1636–1642.

Tao Li, Yi Zhang, and Vikas Sindhwani. 2009. A non-
negative matrix tri-factorization approach to senti-
ment classification with lexical prior knowledge. In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
of the AFNLP: Volume 1-Volume 1, pages 244–252.
Association for Computational Linguistics.

Chenghua Lin and Yulan He. 2009. Joint sentimen-
t/topic model for sentiment analysis. In Proceedings
of the 18th ACM Conference on Information and
Knowledge Management, CIKM 2009, Hong Kong,
China, November 2-6, 2009, pages 375–384.

Chin-Yew Lin. 2004. Rouge: A package for automat-
ic evaluation of summaries. In Text summarization
branches out: Proceedings of the ACL-04 workshop,
volume 8. Barcelona, Spain.

Pengfei Liu, Shafiq R. Joty, and Helen M. Meng. 2015.
Fine-grained opinion mining with recurrent neural
networks and word embeddings. In Proceedings of
the 2015 Conference on Empirical Methods in Nat-
ural Language Processing, EMNLP 2015, Lisbon,
Portugal, September 17-21, 2015, pages 1433–1443.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.
Corrado, and Jeffrey Dean. 2013. Distributed rep-
resentations of words and phrases and their com-
positionality. In Advances in Neural Information
Processing Systems 26: 27th Annual Conference on
Neural Information Processing Systems 2013. Pro-
ceedings of a meeting held December 5-8, 2013,
Lake Tahoe, Nevada, United States., pages 3111–
3119.

Hitoshi Nishikawa, Takaaki Hasegawa, Yoshihiro Mat-
suo, and Gen-ichiro Kikui. 2010. Optimizing in-
formativeness and readability for sentiment summa-
rization. In ACL 2010, Proceedings of the 48th An-
nual Meeting of the Association for Computation-
al Linguistics, July 11-16, 2010, Uppsala, Sweden,
Short Papers, pages 325–330.

Bo Pang and Lillian Lee. 2005. Seeing stars: Exploit-
ing class relationships for sentiment categorization
with respect to rating scales. In ACL 2005, 43rd An-
nual Meeting of the Association for Computation-
al Linguistics, Proceedings of the Conference, 25-
30 June 2005, University of Michigan, USA, pages
115–124.

Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and trends in infor-
mation retrieval, 2(1-2):1–135.

Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen.
2011. Opinion word expansion and target extraction
through double propagation. Computational Lin-
guistics, 37(1):9–27.

Lizhen Qu, Georgiana Ifrim, and Gerhard Weikum.
2010. The bag-of-opinions method for review rat-
ing prediction from sparse text patterns. In COLING

1635



2010, 23rd International Conference on Computa-
tional Linguistics, Proceedings of the Conference,
23-27 August 2010, Beijing, China, pages 913–921.

Francesco Ricci, Lior Rokach, Bracha Shapira, and
Paul B. Kantor, editors. 2011. Recommender Sys-
tems Handbook. Springer.

Alexander M. Rush, Sumit Chopra, and Jason Weston.
2015. A neural attention model for abstractive sen-
tence summarization. In Proceedings of the 2015
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP 2015, Lisbon, Portugal,
September 17-21, 2015, pages 379–389.

Badrul M. Sarwar, George Karypis, Joseph A. Konstan,
and John Riedl. 2001. Item-based collaborative fil-
tering recommendation algorithms. In Proceedings
of the Tenth International World Wide Web Confer-
ence, WWW 10, Hong Kong, China, May 1-5, 2001,
pages 285–295.

Vivek Kumar Singh, Mousumi Mukherjee, and Ghan-
shyam Kumar Mehta. 2011. Combining collab-
orative filtering and sentiment classification for
improved movie recommendations. In Multi-
disciplinary Trends in Artificial Intelligence - 5th In-
ternational Workshop, MIWAI 2011, Hyderabad, In-
dia, December 7-9, 2011. Proceedings, pages 38–
50.

Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng,
and Christopher Potts. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP 2013, 18-21 October 2013, Grand Hy-
att Seattle, Seattle, Washington, USA, A meeting of
SIGDAT, a Special Interest Group of the ACL.

Xiaoyuan Su and Taghi M Khoshgoftaar. 2009. A sur-
vey of collaborative filtering techniques. Advances
in artificial intelligence, 2009:4.

Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston,
and Rob Fergus. 2015. End-to-end memory net-
works. In Advances in Neural Information Process-
ing Systems 28: Annual Conference on Neural In-
formation Processing Systems 2015, December 7-
12, 2015, Montreal, Quebec, Canada, pages 2440–
2448.

Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.
Sequence to sequence learning with neural network-
s. In Advances in Neural Information Processing
Systems 27: Annual Conference on Neural Informa-
tion Processing Systems 2014, December 8-13 2014,
Montreal, Quebec, Canada, pages 3104–3112.

Kai Sheng Tai, Richard Socher, and Christopher D.
Manning. 2015. Improved semantic representation-
s from tree-structured long short-term memory net-
works. In Proceedings of the 53rd Annual Meet-
ing of the Association for Computational Linguistics

and the 7th International Joint Conference on Natu-
ral Language Processing of the Asian Federation of
Natural Language Processing, ACL 2015, July 26-
31, 2015, Beijing, China, Volume 1: Long Papers,
pages 1556–1566.

Duyu Tang, Bing Qin, Ting Liu, and Yuekui Yang.
2015. User modeling with neural network for review
rating prediction. In Proceedings of the Twenty-
Fourth International Joint Conference on Artificial
Intelligence, IJCAI 2015, Buenos Aires, Argentina,
July 25-31, 2015, pages 1340–1346.

Zhiyang Teng, Duy-Tin Vo, and Yue Zhang. 2016.
Context-sensitive lexicon features for neural senti-
ment analysis. In Proceedings of the 2016 Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP 2016, Austin, Texas, USA,
November 1-4, 2016, pages 1629–1638.

Ke M. Tran, Arianna Bisazza, and Christof Monz.
2016. Recurrent memory networks for language
modeling. In NAACL HLT 2016, The 2016 Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, San Diego California, USA,
June 12-17, 2016, pages 321–331.

Xiaojun Wan. 2013. Co-regression for cross-language
review rating prediction. In Proceedings of the 51st
Annual Meeting of the Association for Computation-
al Linguistics, ACL 2013, 4-9 August 2013, Sofia,
Bulgaria, Volume 2: Short Papers, pages 526–531.

Lu Wang and Wang Ling. 2016. Neural network-
based abstract generation for opinions and argu-
ments. In NAACL HLT 2016, The 2016 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, San Diego California, USA, June 12-
17, 2016, pages 47–57.

Mingxuan Wang, Zhengdong Lu, Hang Li, and Qun
Liu. 2016. Memory-enhanced decoder for neural
machine translation. In Proceedings of the 2016
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP 2016, Austin, Texas, US-
A, November 1-4, 2016, pages 278–286.

Yao Wu and Martin Ester. 2015. FLAME: A proba-
bilistic model combining aspect based opinion min-
ing and collaborative filtering. In Proceedings of
the Eighth ACM International Conference on Web
Search and Data Mining, WSDM 2015, Shanghai,
China, February 2-6, 2015, pages 199–208.

Caiming Xiong, Stephen Merity, and Richard Socher.
2016. Dynamic memory networks for visual and
textual question answering. In Proceedings of the
33nd International Conference on Machine Learn-
ing, ICML 2016, New York City, NY, USA, June 19-
24, 2016, pages 2397–2406.

Bishan Yang and Claire Cardie. 2013. Joint inference
for fine-grained opinion extraction. In Proceed-
ings of the 51st Annual Meeting of the Association

1636



for Computational Linguistics, ACL 2013, 4-9 Au-
gust 2013, Sofia, Bulgaria, Volume 1: Long Papers,
pages 1640–1649.

Xiwang Yang, Yang Guo, Yong Liu, and Harald Steck.
2014. A survey of collaborative filtering based so-
cial recommender systems. Computer Communica-
tions, 41:1–10.

Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He,
Alex Smola, and Eduard Hovy. 2016. Hierarchical
attention networks for document classification. In
NAACL 2016, 15th Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
San Diego, US.

Yongfeng Zhang, Guokun Lai, Min Zhang, Yi Zhang,
Yiqun Liu, and Shaoping Ma. 2014. Explicit factor
models for explainable recommendation based on
phrase-level sentiment analysis. In The 37th Inter-
national ACM SIGIR Conference on Research and
Development in Information Retrieval, SIGIR ’14,
Gold Coast , QLD, Australia - July 06 - 11, 2014,
pages 83–92.

Yuan Zhang and David Weiss. 2016. Stack-
propagation: Improved representation learning for
syntax. In Proceedings of the 54th Annual Meeting
of the Association for Computational Linguistics, A-
CL 2016, August 7-12, 2016, Berlin, Germany, Vol-
ume 1: Long Papers.

1637


