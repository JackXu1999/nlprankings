



















































Generation from Abstract Meaning Representation using Tree Transducers


Proceedings of NAACL-HLT 2016, pages 731–739,
San Diego, California, June 12-17, 2016. c©2016 Association for Computational Linguistics

Generation from Abstract Meaning Representation using Tree Transducers

Jeffrey Flanigan♠ Chris Dyer♠ Noah A. Smith♥ Jaime Carbonell♠
♠School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, USA
♥Computer Science & Engineering, University of Washington, Seattle, WA, USA

{jflanigan,cdyer,jgc}@cs.cmu.edu, nasmith@cs.washington.edu

Abstract

Language generation from purely semantic
representations is a challenging task. This pa-
per addresses generating English from the Ab-
stract Meaning Representation (AMR), con-
sisting of re-entrant graphs whose nodes are
concepts and edges are relations. The new
method is trained statistically from AMR-
annotated English and consists of two major
steps: (i) generating an appropriate spanning
tree for the AMR, and (ii) applying tree-to-
string transducers to generate English. The
method relies on discriminative learning and
an argument realization model to overcome
data sparsity. Initial tests on held-out data
show good promise despite the complexity of
the task. The system is available open-source
as part of JAMR at:
http://github.com/jflanigan/jamr

1 Introduction

We consider natural language generation from the
Abstract Meaning Representation (AMR; Banarescu
et al., 2013). AMR encodes the meaning of a sen-
tence as a rooted, directed, acyclic graph, where con-
cepts are nodes, and edges are relationships among
the concepts.

Because AMR models propositional meaning1

while abstracting away from surface syntactic re-
alizations, and is designed with human annotation
in mind, it suggests a separation of (i) engineering
the application-specific propositions that need to be

1In essence, AMR handles semantic roles, entity types,
within-sentence coreference, discourse connectives, modality,
negation, and some other phenomena.

communicated about from (ii) general-purpose re-
alization details, modeled by a generator shareable
across many applications. The latter is our focus
here.

Because any AMR graph has numerous valid re-
alizations, and leaves underspecified many impor-
tant details—including tense, number, definiteness,
whether a concept should be referred to nominally
or verbally, and more—transforming an AMR graph
into an English sentence is a nontrivial problem.

To our knowledge, our system is the first for gen-
erating English from AMR. The approach is a sta-
tistical natural language generation (NLG) system,
trained discriminatively using sentences in the AMR
bank (Banarescu et al., 2013). It first transforms
the graph into a tree, then decodes into a string us-
ing a weighted tree-to-string transducer and a lan-
guage model (Graehl and Knight, 2004). The de-
coder bears a strong similarity to state-of-the-art ma-
chine translation systems (Koehn et al., 2007; Dyer
et al., 2010), but with a rule extraction approach tai-
lored to the NLG problem.

2 Overview

Generation of English from AMR graphs is accom-
plished as follows: the input graph is converted to
a tree, which is input into the weighted intersection
of a tree-to-string transducer (§4) with a language
model. The output English sentence is the (ap-
proximately) highest-scoring sentence according to
a feature-rich discriminatively trained linear model.
After discussing notation (§3), we describe our ap-
proach in §4. The transducer’s rules are extracted
from the limited AMR corpus and learned general-

731



izations; they are of four types: basic rules (§5),
synthetic rules created using a specialized model
(§6), abstract rules (§7), and a small number of
handwritten rules (§8).
3 Notation and Definitions

AMR graphs are directed, weakly connected graphs
with node labels from the set of concepts LN and
edge labels from the set of relations LE .

AMR graphs are transformed to eliminate cycles
(details in §4); we refer to the resulting tree as a
transducer input representation (TI representa-
tion). For a node n with label C and outgoing edges
n

L1−→ n1, . . . , n Lm−−→ nm sorted lexicographically
by Li (each an element of LE), the TI representa-
tion of the tree rooted at n is denoted:2

(X C (L1 T1) . . . (Lm Tm)) (1)

where each Ti is the TI representation of the tree
rooted at ni. See Fig. 1 for an example. A LISP-like
textual formatting of the TI representation in Fig. 1
is:

(X want-01 (ARG0 (X boy)) (ARG1 (X ride-01
(ARG0 (X bicycle (mod (X red)))))))

To ease notation, we use the function sort [] to lex-
icographically sort edge labels in a TI representa-
tion. Using this function, an equivalent way of rep-
resenting the TI representation in Eq. 1, if the Li are
unsorted, is:

(X C sort [(L1 T1) . . . (Lm Tm)])

The TI representation is converted into a word se-
quence using a tree-to-string transducer. The tree
transducer formalism we use is one-state extended
linear, non-deleting tree-to-string (1-xRLNs) trans-
ducers (Huang et al., 2006; Graehl and Knight,
2004).3

Definition 1. (From Huang et al., 2006.) A 1-
xRLNs transducer is a tuple (N,Σ,W,R) whereN

2If there are duplicate child edge labels, then the conver-
sion process is ambiguous and any of the conversions can be
used. The ordering ambiguity will be handled later in the tree-
transducer rules.

3Multiple states would be useful for modeling dependencies
in the output, but we do not use them here.

want-01

boy

ride-01

bicycle

red

ARG0
ARG1

ARG1

mod

ARG0

want-01

X

ARG0 ARG1

X X

boy

bicycle mod

X

red

ride-01 ARG0

X

The boy wants to ride the red bicycle .

Figure 1: The generation pipeline. An AMR graph
(top), with a deleted re-entrancy (dashed), is con-
verted into a transducer input representation (TI rep-
resentation, middle), which is transduced to a string
using a tree-to-string transducer (bottom).

is the set of nonterminals (relation labels and X), Σ
is the input alphabet (concept labels), W is the out-
put alphabet (words), and R is the set of rules. A
rule inR is a tuple (t, s, φ) where:

1. t is the LHS tree, whose internal nodes are la-
beled by nonterminal symbols, and whose fron-
tier nodes are labeled terminals from Σ or vari-
ables from a set X = {X1, X2, . . .};

732



2. s ∈ (X ∪W )∗ is the RHS string;
3. φ is a mapping from X to nonterminals N .

A rule is a purely lexical rule if it has no variables.
As an example, the tree-to-string transducer rules

which produce the output sentence from the TI rep-
resentation in Fig. 1 are:

(X want-01 (ARG0 X1) (ARG1 X2))→
The X1 wants to X2 .

(X ride-01 (ARG1 X1))→ ride the X1
(X bicycle (mod X1))→ X1 bicycle

(X red)→ red
(X boy)→ boy (2)

Here, all Xi are mapped by a trivial φ to the nonter-
minal X .

The output string of the transducer is the target
projection of the derivation, defined as follows:
Definition 2. (From Huang et al., 2006.) A deriva-
tion d, its source and target projections, denoted
S(d) and E(d) respectively, are recursively defined
as follows:

1. If r = (t, s, φ) is a purely lexical rule, then
d = r is a derivation, where S(d) = t and
E(d) = s;

2. If r = (t, s, φ) is a rule, and di is a (sub)-
derivation with the root symbol of its source
projection maching the corresponding substi-
tion node in r, i.e., root(S(di)) = φ(xi), then
d = r(d1, . . . , dm) is also a derivation, where
S(d) = [xi 7→ S(di)]t and E(d) = [xi 7→
E(di)]s.

The notation [xi 7→ yi]t is shorthand for the result of
substituting yi for each xi in t, where xi ranges over
all variables in t.

The set of all derivations of a target string e with
a transducer T is denoted

D(e, T ) = {d | E(d) = e}
where d is a derivation in T .

We use a shorthand notation for the transducer
rules that will be useful when discussing rule extrac-
tion and synthetic rules. Let fi be a TI representa-
tion. The TI representation has the form

fi = (X C (L1 T1) . . . (Lm Tm))

where Li ∈ LE and T1, . . . , Tm are TI representa-
tions.4 Let A1, . . . An ∈ LE . We use

(fi, A1, . . . , An)→ r (3)

as shorthand for the rule:

(X C sort [(L1 T1) . . . (Lm Tm)
(A1 X1) . . . (An Xn)])→ r (4)

Note r must contain the variables X1 . . . Xn. In
(3) and (4), argument slots with relation labels Ai
have been added as children to the root node of the
TI representation fi.

For example, the shorthand for the transducer
rules in (2) is:

((X want-01),ARG0,ARG1)→
The X1 wants to X2 .

((X ride-01),ARG1)→ ride the X1
((X bicycle),mod)→ X1 bicycle

((X red))→ red (5)

4 Generation

To generate a sentence e from an input AMR graph
G, a spanning tree G′ of G is computed, then trans-
formed into a string using a tree-to-string transducer.

Spanning tree. The choice of the graph G’s span-
ning tree G′ could have a big effect on the output,
since the transducer’s output will always be a pro-
jective reordering of the tree’s leaves. Our spanning
tree results from a breadth-first-search traversal, vis-
iting child nodes in lexicographic order of the re-
lation label (inverse relations are visited last). The
edges traversed are included in the tree. This sim-
ple heuristic is a baseline which can potentially be
improved in future work.

Decoding. Let T = (N,Σ,W,R) be a tree-to-
string transducer. The output sentence is the highest
scoring transduction of G′:

e = E
(

arg max
d∈D(G′,T )

score(d;θ)

)
(6)

4If fi is just a single concept with no children, then m = 0
and fi = (X C).

733



Eq. 6 is solved approximately using the cdec de-
coder for machine translation (Dyer et al., 2010).
The score of the transduction is a linear function
(with coefficients θ) of a vector of features in-
cluding the output sequence’s language model log-
probability and features associated with the rules in
the derivation (denoted f ; Table 1):

score(d;θ) = θLM log(pLM(E(d))) +
∑
r∈d

θ>f(r)

The feature weights are trained on a development
dataset using MERT (Och, 2003).

In the next four sections, we describe the rules
extracted and generalized from the training corpus.

5 Inducing Basic Rules

The basic rules, denoted RB , are extracted from
the training AMR data using an algorithm sim-
ilar to extracting tree transucers from tree-string
aligned parallel corpora (Galley et al., 2004). In-
formally, the rules are extracted from a sentence
w = 〈w1, . . . , wn〉 with AMR graph G as follows:

1. The AMR graph and the sentence are aligned;
we use the JAMR aligner from Flanigan et
al. (2014), which aligns non-overlapping sub-
graphs of the graph to spans of words. The sub-
graphs that JAMR aligns are called fragments.
In JAMR’s aligner, all fragments are trees.

2. G is replaced by its spanning tree by deleting
relations that use a variable in the AMR anno-
tation.

3. In the spanning tree, for each node i, we
keep track of the word indices b(i) and e(i)
in the original sentence that trap all of i’s de-
scendants. (This is calculated using a simple
bottom-up propagation from the leaves to the
root.)

4. For each aligned fragment i, a rule is extracted
by taking the subsequence 〈wb(i) . . . we(i)〉 and
“punching out” the spans of the child nodes
(and their descendants) and replacing them
with argument slots.

See Fig. 2 for examples.
More formally, assume the nodes in G are num-

bered 1, . . . , N and the fragments are numbered

1, . . . , F . Let nodes : {1, . . . , F} → 2{1,...,N} and
root : {1, . . . , F} → {1, . . . , N} be functions that
return the nodes in a fragment and the root of a frag-
ment, respectively, and let children : {1, . . . , N} →
2{1,...,N} return the child nodes of a node. We con-
sider a node aligned if it belongs to an aligned frag-
ment. Let the span of an aligned node i be denoted
by endpoints ai and a′i; for unaligned nodes, ai =∞
and a′i = −∞ (depicted with superscripts in Fig. 2).
The node alignments are propagated by defining b(·)
and e(·) recursively, bottom up:

b(i) = min(aj , min
j∈children(i)

b(j))

e(i) = max(a′j , max
j∈children(i)

e(j))

Also define functions b̃ and ẽ, from fragment indices
to integers, as:

b̃(i) = b(root(i))

ẽ(i) = e(root(i))

For fragment i, let Ci = children(root(i)) −
nodes(i), which is the children of the fragment’s
root concept that are not included in the fragment.
Let fi be the TI representation for fragment i.5 If Ci
is empty, then the rule extracted for fragment i is:

ri : (fi)→ wb̃(i):ẽ(i) (7)

Otherwise, let m = |Ci|, and denote the edge labels
from root(i) to elements of Ci as A1(i) . . . Am(i).
For j ∈ {1, . . . ,m}, let kj select the elements ckj
of Ci in ascending order of b(kj). Then the rule ex-
tracted for fragment i is:

ri : (fi, Ak1(i), . . . Akm(i))→
wb̃(i):b̃(k1) X1 wẽ(k1):b̃(k2) X2 . . .

. . . wẽ(km−1):b̃(km)Xm wẽ(km):ẽ(i) (8)

A rule is only extracted if the fragment i is aligned
and the child spans do not overlap. Fig. 2 gives an
example of a tree annotated with alignments, b and
e, and the extracted rules.

5I.e., the nodes in fragment i, with the edges between them,
represented as a TI representation.

734



Name Description
Rule 1 for every rule
Basic 1 for basic rules, else 0
Synthetic 1 for synthetic rules, else 0
Abstract 1 for abstract rules, else 0
Handwritten 1 for handwritten rules, else 0
Rule given concept log(number of times rule extracted / number of times concept observed in training

data) (only for basic rules, 0 otherwise)
. . . without sense same as above, but with sense tags for concepts removed

Synthetic score model score for the synthetic rule (only for synthetic rules, 0 otherwise)
Word count number of words in the rule
Stop word count number of words not in a stop word list
Bad stop word number of words in a list of meaning-changing stop words, such as “all, can, could,

only, so, too, until, very”
Negation word number of words in “no, not, n’t”

Table 1: Rule features in the transducer. There is also an indicator feature for every handwritten rule.

6 Modeling Synthetic Rules

The synthetic rules, denoted RS(G), are created
to generalize the basic rules and overcome data
sparseness resulting from our relatively small train-
ing dataset. Our synthetic rule model considers an
AMR graph G and generates a set of rules for each
node in G. S synthetic rule’s LHS is a TI represen-
tation f with argument slots A1 . . . Am (this is the
same form as the LHS for basic rules). For each
node in G, one or more LHS are created (we will
discuss this further below), and for each LHS, a set
of k-best synthetic rules are produced. The simplest
case of a LHS is just a concept and argument slots
corresponding to each of its children.

For a given LHS, the synthetic rule model cre-
ates a RHS by concatenating together a string inW ∗

(called a concept realization and corresponding
to the concept fragment) with strings in W ∗XW ∗
(called an argument realization and corresponding
to the argument slots). See the top of Fig. 3 for a syn-
thetic rule with concept and argument realizations
highlighted.

Synthetic rules have the form:

r : (f,A1, . . . Am)→ (9)
lk1Xk1rk1 . . . lkcXkcrkc c

lkc+1Xkc+1rkc+1 . . . lkmXkmrkm

where:

• f is a TI representation.

• Each Ai ∈ LE .
• 〈k1, . . . , km〉 is a permutation of 〈1, . . . ,m〉
• c ∈ W ∗ is the realization of TI representa-

tion f .

• Each li, ri ∈ W ∗ and Xi ∈ X . Let Ri =
〈li, ri〉 denote the realization of argument i.
• c ∈ [0,m] is the position of c among the real-

izations of the arguments.

Let F be the space of all possible TI represen-
tations. Synthetic rules make use of three lookup
tables (which are partial functions) to provide can-
didate realizations for concepts and arguments: a ta-
ble for concept realizations lex : F → 2W ∗ , a table
for argument realizations when the argument is on
the left left lex : F × LE → 2W ∗ , and a table for
argument realizations when the argument is on the
right right lex : F × LE → 2W ∗ . These tables are
constructed during basic rule extraction, the details
of which are discussed below .

Synthetic rules are selected using a linear
model with features g and coefficients φ, which
scores each RHS for a given LHS. For LHS =
(f,A1, . . . Am), the RHS is specified completely by
c, c, R1, . . . , Rm and a permutation k1, . . . , km. For
each node in G, and for each TI representation f in
the domain of lex that matches the node, a LHS is
created, and a set of K synthetic rules is produced
for each c ∈ lex (f). The rules produced are the

735



0 The 1 ((boy) 2 wants 3 to 4 (ride 5 the 6 ((red) 7
bicycle))) 8
(a) Sentence annotated with indexes, and bracketed accord-
ing to b(i) and e(i) from the graph in (b).

want-01[2,3] 

[1,8]

boy[1,2] 
[1,2] ride-01

 [4,5] 
[4,8]

bicycle [7,8] 

[6,8]

red [6,7] 
[6,7]

ARG0 ARG1

ARG1

mod

(b) Tree annotated with ai, a′i (superscripts)
and b(i), e(i) (subscripts).

want-01

X

ARG0 ARG1

X1 X2

X

ride-01 ARG0

X1

X1 wants to X2

ride the X1

bicycle mod

X1

X

X1 bicycle

X

boy

boy
X

red

red

(c) Extracted rules.

Figure 2: Example rule extraction from an AMR-
annotated sentence.

to DEST

ARG0 ride-01 DEST

ARG0 was

ARG0

the ARG0

rides to the DEST

ride-01

X

ARG0 DEST

X1 X2

the X1 rides to the X2

Figure 3: Synthetic rule generation for the rule
shown at top. In the rule RHS, the realization for
ARG0 is blue, the realization for DEST is red, and
the realization for ride-01 is black. For a fixed per-
mutation of the concept and arguments, choosing the
argument realizations can be seen as a sequence la-
beling problem (bottom). The highlighted sequence
corresponds to the rule at top.

K-best solutions to:

arg max
c,k1...km,R1,...,Rm

( c∑
i=1

ψ>g(Rki , Aki , c, i, c)

+ψ>g(〈�, �〉, ∗, c, c+ 1, c)

+
m∑

i=c+1

ψ>g(Rki , Aki , c, i+ 1, c)
)

(10)

where the max is over c ∈ 0 . . .m, k1, . . . , km is
any permutation of 1, . . . ,m, and Ri ∈ left lex (Ai)
for i < c and Ri ∈ right lex (Ai) for i > c. ∗ is used
to denote the concept position. � is the empty string.

The best solution to Eq. 10 is found exactly by
brute force search over concept position c ∈ [0,m+
1] and the permutation k1, . . . , km. With fixed
concept position and permutation, each Ri for the
arg max is found independently. To obtain the ex-
act K-best solutions, we use dynamic programming
with a K-best semiring (Goodman, 1999) to keep
track of the K best sequences for each concept posi-
tion and permutation, and take the bestK sequences
over all values of c and k·.

The synthetic rule model’s parameters are esti-
mated using basic rules extracted from the training
data. Basic rules are put into the form of Eq. 9 by

736



Feature name Value
POS + Ai + “dist” |c− i|
POS + Ai + side 1.0
POS + Ai + side + “dist” |c− i|
POS + Ai + Ri + side 1.0
c + Ai + “dist” |c− i|
c + Ai + side 1.0
c + Ai + side + “dist” |c− i|
c + POS + Ai + side + “dist” |c− i|

Table 2: Synthetic rule model features. POS is the
most common part-of-speech tag sequence for c,
“dist” is the string “dist”, and side is “L” if i < c,
“R” otherwise. + denotes string concatenation.

segmenting the RHS into the form

l1X1r1 . . . c . . . lmXmrm (11)

by choosing c, li, ri ∈ W ∗ for i ∈ {1, . . . ,m}. An
example segmentation is the rule RHS in Fig. 3.

Segmenting the RHS of the basic rules into the
form of Eq. 11 is done as follows: c is the aligned
span for f . For the argument realizations, arguments
to the left of c pick up words to their right, and argu-
ments to the right pick up words to their left. Specif-
ically, for i < c (Ri to the left of c but not next to
c), li is empty and ri contains all words between ai
and ai+1. For i = c (Ri directly to the left of c), li is
empty and ri contains all words between ac and c.
For i > c+1, li contains all words between ai−1 and
ai, and for i = c + 1, li contains all words between
c and ai.

The tables for lex , left lex , and right lex are popu-
lated using the segmented basic rules. For each ba-
sic rule extracted from the training corpus and seg-
mented according to the previous paragraph, f → c
is added to lex , andAki → 〈li, ri〉 is added to left lex
for i ≤ c and right lex for i > c. The permutation ki
is known during extraction in Eq. 8.

The parameters ψ are trained using Ada-
Grad (Duchi et al., 2011) with the perceptron loss
function (Rosenblatt, 1957; Collins, 2002) for 10
iterations over the basic rules. The features g are
listed in Table 2.

7 Abstract Rules

Like the synthetic rules, the abstract rules RA(G)
generalize the basic rules. However, abstract rules

Split Sentences Tokens
Train 10,000 210,000
Dev. 1,400 29,000
Test 1,400 30,000
MT09 204 5,000

Table 3: Train/dev./test/MT09 split.

are much simpler generalizations which use part-
of-speech (POS) tags to generalize. Abstract rules
make use of a POS abstract rule table, which is
a table listing every combination of the POS of the
concept realization, the child arguments’ labels, and
rule RHS with the concept realization removed and
replaced with ∗. This table is populated from the
basic rules extracted from the training corpus. An
example entry in the table is:

(VBD,ARG0,DEST)→
X1 〈∗〉 to the X2

For the LHS (f,A1, . . . Am), an abstract rule is
created for each member of c ∈ lex (f) and the
most common POS tag p for c by looking up p,
A1, . . . Am in the POS abstract rule table, finding
the common RHS, and filling in the concept posi-
tion with c. The set of all such rules is returned.

8 Handwritten Rules

We have handwritten rules for dates, conjunctions,
multiple sentences, and the concept have-org-role-
91. We also create pass-through rules for concepts
by removing sense tags and quotes (for string liter-
als).

9 Experiments

We evaluate on the AMR Annotation Release ver-
sion 1.0 (LDC2014T12) dataset. We follow the rec-
ommended train/dev./test splits, except that we re-
move MT09 data (204 sentences) from the training
data and use it as another test set. Statistics for this
dataset and splits are given in Table 3. We use a 5-
gram language model trained with KenLM (Heafield
et al., 2013) on Gigaword (LDC2011T07), and use
100-best synthetic rules.

We evaluate with the Bleu scoring metric (Pap-
ineni et al., 2002) (Table 4). We report single ref-

737



Rules Test MT09
Full 22.1 21.2
Full − basic 22.1 20.9
Full − synthetic 9.1 7.8
Full − abstract 22.0 21.2
Full − handwritten 21.9 20.5

Table 4: Uncased Bleu scores with various types of
rules removed from the full system.

erence Bleu for the LCD2014T12 test set, and four-
reference Bleu for the MT09 set. We report ablation
experiments for different sources of rules. When
ablating handwritten rules, we do not ablate pass-
through rules.

The full system achieves 22.1 Bleu on the test
set, and 21.2 on MT09. Removing the synthetic
rules drops the results to 9.1 Bleu on test and 7.8 on
MT09. Removing the basic and abstract rules has
little impact on the results. This may be because the
synthetic rule model already contains much of the
information in the basic and abstract rules. Remov-
ing the handwritten rules has a slightly larger effect,
demonstrating the value of handwritten rules in this
statistical system.

10 Related Work

There is a large body of work for statistical and non-
statistical NLG from a variety of input representa-
tions. Statistical NLG systems have been built for
input representations such as HPSG (Nakanishi et
al., 2005), LFG (Cahill and Van Genabith, 2006;
Hogan et al., 2007), and CCG (White et al., 2007),
as well as surface and deep syntax (Belz et al.,
2011). The deep syntax representations in Bohnet
et al. (2010) and Belz et al. (2011) share similari-
ties with AMR: the representations are graphs with
re-entrancies, and have an concept inventory from
PropBank (Palmer et al., 2005).

The Nitrogen and Halogen systems (Langkilde
and Knight, 1998; Langkilde, 2000) used an input
representation that was a precursor to the modern
version of AMR, which was also called AMR, al-
though it was not the same representation as Ba-
narescu et al. (2013).

Techniques from statistical machine translation
have been applied to the problem of NLG (Wong

and Mooney, 2006), and many grammar-based ap-
proaches can be formulated as weighted tree-to-
string transducers. Jones et al. (2012) developed
technology for generation and translation with syn-
chronous hyperedge replacement (SHRG) gram-
mars applied to the GeoQuery corpus (Wong and
Mooney, 2006), which in principle could be applied
to AMR generation.

11 Conclusion

We have presented a two-stage method for natural
language generation from AMR, setting a baseline
for future work. We have also demonstrated the im-
portance of modeling argument realization for good
performance. Our feature-based, tree-transducer ap-
proach can be easily extended with rules and fea-
tures from other sources, allowing future improve-
ments.

Acknowledgments

The authors would like to thank Adam Lopez and
Nathan Schneider for valuable feedback, and Sam
Thomson and the attendees of the Fred Jelinek
Memorial Workshop in 2014 in Prague for help-
ful discussions. This work is supported by the
U.S. Army Research Office under grant number
W911NF-10-1-0533. Any opinion, findings, and
conclusions or recommendations expressed in this
material are those of the author(s) and do not nec-
essarily reflect the view of the U.S. Army Research
Office or the United States Government.

References
Laura Banarescu, Claire Bonial, Shu Cai, Madalina

Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin
Knight, Philipp Koehn, Martha Palmer, and Nathan
Schneider. 2013. Abstract meaning representation for
sembanking. In Proc. of the 7th Linguistic Annotation
Workshop and Interoperability with Discourse.

Anja Belz, Michael White, Dominic Espinosa, Eric Kow,
Deirdre Hogan, and Amanda Stent. 2011. The first
surface realisation shared task: Overview and evalua-
tion results. In Proc. of the 13th European Workshop
on Natural Language Generation.

Bernd Bohnet, Leo Wanner, Simon Mille, and Alicia
Burga. 2010. Broad coverage multilingual deep sen-
tence generation with a stochastic multi-level realizer.
In Proc. of COLING.

738



Aoife Cahill and Josef Van Genabith. 2006. Robust pcfg-
based generation using automatically acquired LFG
approximations. In Proc. of COLING-ACL.

Michael Collins. 2002. Discriminative training methods
for hidden Markov models: Theory and experiments
with perceptron algorithms. In Proc. of EMNLP.

John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning and
stochastic optimization. JMLR, 12.

Chris Dyer, Adam Lopez, Juri Ganitkevitch, Johnathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,
Vladimir Eidelman, and Philip Resnik. 2010. cdec: A
decoder, alignment, and learning framework for finite-
state and context-free translation models. In Proc. of
ACL.

Jeffrey Flanigan, Sam Thomson, Jaime Carbonell, Chris
Dyer, and Noah A. Smith. 2014. A discriminative
graph-based parser for the abstract meaning represen-
tation. In Proc. of ACL.

Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What’s in a translation rule? In Proc.
of HLT-NAACL.

Joshua Goodman. 1999. Semiring parsing. CL, 25(4).
Jonathan Graehl and Kevin Knight. 2004. Training tree

transducers. In Proc. of HLT-NAACL.
Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H. Clark,

and Philipp Koehn. 2013. Scalable modified Kneser-
Ney language model estimation. In Proc. of ACL.

Deirdre Hogan, Conor Cafferkey, Aoife Cahill, and Josef
Van Genabith. 2007. Exploiting multi-word units
in history-based probabilistic generation. In Proc. of
ACL.

Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proc. of AMTA.

Bevan Jones, Jacob Andreas, Daniel Bauer, Karl Moritz
Hermann, and Kevin Knight. 2012. Semantics-
based machine translation with hyperedge replacement
grammars. In Proc. of COLING.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, et al. 2007. Moses: Open source toolkit for
statistical machine translation. In Proc. of ACL.

Irene Langkilde and Kevin Knight. 1998. Generation
that exploits corpus-based statistical knowledge. In
Proc. of COLING-ACL.

Irene Langkilde. 2000. Forest-based statistical sentence
generation. In Proc. of NAACL 2000.

Hiroko Nakanishi, Yusuke Miyao, and Jun’ichi Tsujii.
2005. Probabilistic models for disambiguation of an
HPSG-based chart generator. In Proc. of IWPT.

Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. of ACL.

Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated corpus of
semantic roles. CL, 31(1).

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In Proc. of ACL.

Frank Rosenblatt. 1957. The perceptron—a perceiving
and recognizing automaton. Technical Report 85-460-
1, Cornell Aeronautical Laboratory.

Michael White, Rajakrishnan Rajkumar, and Scott Mar-
tin. 2007. Towards broad coverage surface realiza-
tion with CCG. In Proc. of the Workshop on Using
Corpora for NLG: Language Generation and Machine
Translation.

Yuk Wah Wong and Raymond J Mooney. 2006. Learn-
ing for semantic parsing with statistical machine trans-
lation. In Proc. of HLT-NAACL.

739


