



















































What's in a Domain? Learning Domain-Robust Text Representations using Adversarial Training


Proceedings of NAACL-HLT 2018, pages 474–479
New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics

What’s in a Domain?
Learning Domain-Robust Text Representations

using Adversarial Training

Yitong Li and Timothy Baldwin and Trevor Cohn
School of Computing and Information Systems

The University of Melbourne, Australia
yitongl4@student.unimelb.edu.au, {tbaldwin,tcohn}@unimelb.edu.au

Abstract
Most real world language problems require
learning from heterogenous corpora, raising
the problem of learning robust models which
generalise well to both similar (in domain) and
dissimilar (out of domain) instances to those
seen in training. This requires learning an
underlying task, while not learning irrelevant
signals and biases specific to individual do-
mains. We propose a novel method to optimise
both in- and out-of-domain accuracy based on
joint learning of a structured neural model with
domain-specific and domain-general compo-
nents, coupled with adversarial training for
domain. Evaluating on multi-domain lan-
guage identification and multi-domain senti-
ment analysis, we show substantial improve-
ments over standard domain adaptation tech-
niques, and domain-adversarial training.

1 Introduction

Heterogeneity is pervasive in NLP, arising from
corpora being constructed from different sources,
featuring different topics, register, writing style,
etc. An important, yet elusive, goal is to produce
NLP tools that are capable of handling all types of
texts, such that we can have, e.g., text classifiers
that work well on texts from newswire to wikis
to micro-blogs. A key roadblock is application
to new domains, unseen in training. Accordingly,
training needs to be robust to domain variation,
such that domain-general concepts are learned in
preference to domain-specific phenomena, which
will not transfer well to out-of-domain evaluation.
To illustrate, Bitvai and Cohn (2015) report learn-
ing formatting quirks of specific reviewers in a re-
view text regression task, which are unlikely to
prove useful on other texts.

This classic problem in NLP has been tack-
led under the guise of “domain adaptation”, also
known as unsupervised transfer learning, us-
ing feature-based methods to support knowledge

transfer over multiple domains (Blitzer et al.,
2007; Daumé III, 2007; Joshi et al., 2012;
Williams, 2013; Kim et al., 2016). More recently,
Ganin and Lempitsky (2015) proposed a method
to encourage domain-general text representations,
which transfer better to new domains.

Inspired by the above methods, in this paper
we propose a novel technique for multitask learn-
ing of domain-general representations.1 Specifi-
cally, we propose deep learning architectures for
multi-domain learning, featuring a shared rep-
resentation, and domain private representation.
Our approach generalises the feature augmenta-
tion method of Daumé III (2007) to convolu-
tional neural networks, as part of a larger deep
learning architecture. Additionally, we use ad-
versarial training such that the shared represen-
tation is explicitly discouraged from learning do-
main identifying information (Ganin and Lempit-
sky, 2015). We present two architectures which
differ in whether domain is conditioned on or gen-
erated, and in terms of parameter sharing in form-
ing private representations.

We primarily evaluate on the task of lan-
guage identification (“LangID”: Cavnar and Tren-
kle (1994)), using the corpora of Lui and Baldwin
(2012), which combine large training sets over a
diverse range of text domains. Domain adapta-
tion is an important problem for this task (Lui and
Baldwin, 2014; Jurgens et al., 2017), where text
resources are collected from numerous sources,
and exhibit a wide variety of language use. We
show that while domain adversarial training over-
all improves over baselines, gains are modest.
The same applies to twin shared/private architec-
tures, but when the two methods are combined, we
observe substantial improvements. Overall, our

1Code, data and evaluation scripts available at
https://github.com/lrank/Domain_Robust_
Text_Representation.git

474



methods outperform the state-of-the-art (Lui and
Baldwin, 2012) in terms of out-of-domain accu-
racy. As a secondary evaluation, we use the Multi-
Domain Sentiment Dataset (Blitzer et al., 2007),
where we once again observe a clear advantage
for our approaches, illustrating the potential of our
technique more broadly in NLP.

2 Multi-domain Learning

A primary consideration when formulating mod-
els of multi-domain data is how best to use the do-
main. Basic methods might learn several separate
models, or simply ignore the domain and learn a
single model. Neither method is ideal: the for-
mer fails to share statistics between the models to
capture the general concept, while the latter dis-
cards information that can aid classification, e.g.,
domain-specific vocabulary or class skew.

To address these issues, we propose two archi-
tectures as illustrated in Figure 1 (a and b), param-
eterised as a convolutional network (CNN) over
the input instance, chosen based on the success
of CNNs for text categorisation problems (Kim,
2014); note, however, that our method is general
and can be applied with other network types. Both
representations are based on the idea of twin rep-
resentations of each instance,2 denoted shared and
private representations, which are trained to cap-
ture domain-general versus domain-specific con-
cepts, respectively. This is achieved using various
loss functions, most notably an adversarial loss to
discourage learning of domain-specific concepts
in the shared representations. The two architec-
tures differ in whether the domain is provided as
an input (COND) or an output (GEN). Below, we
elaborate on the details of the two models.

2.1 Domain-Conditional Model (COND)

The first model, illustrated in Figure 1a, includes a
collection of domain-specific CNNs, and for each
training instance x, the domain-specific CNNpdi
is used to compute its private representation hp.
In this manner, the model conditions on the do-
main identifier. The COND model also computes
a shared representation, hs, directly from x, using
a shared CNNs, and the two representations are
concatenated together to form input to linear soft-
max classification function c for predicting class
label y. Thus far, the approach resembles Daumé

2This differs from standard architectures, e.g., ‘baseline’
in Figure 1c, which uses a single representation.

xi
CNNpdi(θ

p
di
)

CNNs(θs)

hpi

hsi

yi

Ds(θd)
di

θc

(a) Domain-conditional (COND) model.

xi
CNNp(θp)

CNNs(θs)

hpi

hsi

yi

Ds(θd)
di

Dp(θg)
θc

(b) Domain-generative (GEN) model.

xi
CNN(θ)

h yi

Ds(θd)
di

θc

(c) Baseline CNN model with adversarial loss.

Figure 1: Proposed model architectures, showing a sin-
gle training instance (xi, yi) with domain di, and base-
line model with domain adversarial loss. CNN denotes
a convolutional network, D indicates a discriminator (d
for domain adversarial and g for domain generation),
and red dashed and blue lines denote adversarial and
standard loss, resp.

III (2007), a method for multitask learning based
on feature augmentation in a linear model, which
works by replicating the input features to create
both general shared features, and domain-specific
features. Note that the approaches differ in that our
method uses deep learning to form the two repre-
sentations, in place of feature replication.

Adversarial Supervision A key challenge for
the COND model is that the ‘shared’ representation
can be contaminated by domain-specific concepts.
To address this, we borrow ideas from adversar-
ial learning (Goodfellow et al., 2014; Ganin et al.,
2016). The central idea is to learn a good general
representation (suitable for the shared component)
to maximize end task performance, yet obscure the
domain information, as modelled by a discrimi-
nator, Ds. This reduces the domain-specific in-
formation in the shared representation, however
note that important domain-specific components
can still be captured in the private representation.

Overall, this results in the training objective:

LCOND = min
θc,θs,{θp· }

max
θd
X (y|Hs,Hp,d; θc)

− λdX (d|Hs; θd)
(1)

475



where X denotes the cross-entropy classification
loss, Hs = {hsi (xi)}ni=1 are the shared represen-
tations for the training set of n instances, and like-
wise Hp = {hpi (xi, di)}ni=1 are the private rep-
resentations, which are both functions of θs and
{θp· }, respectively. Note the negative sign of the
adversarial loss (referred to as d), and the max-
imisation with respect to the discriminator param-
eters θd. This has the effect of learning a max-
imally accurate discriminator wrt θd, while mak-
ing it maximally inaccurate wrt representation Hs,
and is implemented using a gradient reversal step
during backpropagation (Ganin et al., 2016).

Minimum Entropy Inference As COND condi-
tions on the domain, this imposes the requirement
that the domain of the test data is known (and cov-
ered in training), which is incompatible with our
goal of unsupervised adaptation. To deal with this
situation, we consider each domain in the test set
as belonging to one of the training domains, and
then select the domain with the minimum entropy
classification distribution. This is based on an as-
sumption that a closely matching domain should
be able to make confident predictions.3

2.2 Domain-Generative Model (GEN)

The second model is based on generation of,
rather than conditioning on, the domain, which al-
lows the model to learn domain signals that trans-
fer across some, but not all, domains. Most com-
ponents are common with the COND model as de-
scribed in §2.1, including the use of private and
shared representations, their use in the classifica-
tion output, and the adversarial loss based on dis-
criminating the domain from the shared represen-
tation. There are two key differences: (1) the pri-
vate representation, hp, is computed using a single
CNNp, rather than several domain-specific CNNs,
which confers benefits of domain-generalisation,
a more compact model, and simpler test infer-
ence;4 and (2) the private representation is used
to positively predict the domain, which further en-
courages the split between domain general and
domain-specific aspects of the representation.

3The minimum entropy method is quite effective, trailing
oracle selection by only 0.8% accuracy.

4The domain need not be known for test examples, so the
model can be used directly.

GEN has the following training objective,

LGEN = min
θc,θs,θp,θg

max
θd
X (y|Hs,Hp; θc)

− λdX (d|Hs; θd) + λgX (d|Hp; θg)
(2)

where notation follows that used in §2.1, with the
exception of Hp = {hpi (xi)}ni=1 that is redefined,
with hpi (xi) a function of θ

p, and the addition of
the last term to capture the generation loss g. The
same gradient reversal method from §2.1 is used
during training for the adversarial component.

3 Experiments

3.1 Language Identification

To evaluate our approach, we first consider the lan-
guage identification task.

Data We follow the settings of Lui and Bald-
win (2012), involving 5 training sets from 5 differ-
ent domains with 97 languages in total: Debian,
JRC-Acquis, Wikipedia, ClueWeb and RCV2,
derived from Lui and Baldwin (2011).5 We evalu-
ate accuracy on seven holdout benchmarks: Eu-
roGov, TCL, Wikipedia26 (all from Baldwin
and Lui (2010)), EMEA (Tiedemann, 2009), Eu-
roPARL (Koehn, 2005), T-BE (Tromp and Pech-
enizkiy, 2011), and T-SC (Carter et al., 2013).

Documents are tokenized as a byte sequence
(consistent with Lui and Baldwin (2012)), and
truncated or padded to a length of 1k bytes.7

Hyper-parameters We perform a grid search
for the hyper-parameters, and selected the follow-
ing settings to optimise accuracy over heldout data
from each of the training domains. All byte to-
kens are mapped to byte embeddings, which are
random initialized with size 300. We use the filter
sizes of 2, 4, 8, 16 and 32, with 128 filters for each,
to capture n-gram features of different lengths. A
dropout rate of 0.5 was applied to all the repre-
sentation layers. We set the factors λd and λg
to 10−3. All the models are optimized using the
Adam Optimizer (Kingma and Ba, 2015) with a
learning rate of 10−4.

5As ClueWeb in Lui and Baldwin (2012) is not publicly
accessible, we used a slightly different set of languages but
comparable number of documents for training.

6Note that the two Wikipedia datasets have no overlap.
7We also tried different document length limits, such as

10k, but observed no substantial change in performance.

476



Models EuroGov TCL Wikipedia2 EMEA EuroPARL T-BE T-SC ALLout

baseline CNN 99.9 91.7 88.9 93.1 98.2 85.2 92.2 92.7
+d 99.9 92.4 88.4 90.2 98.2 87.7 93.1 92.8
+g 99.9 92.0 88.7 91.6 98.4 86.8 92.8 92.9

COND 99.9 91.3 88.2 92.0 98.7 91.5 94.5 93.7
+d 99.9 93.5 90.1 91.3 98.7 92.6 97.9 94.9

GEN 99.9 92.3 88.0 93.3 98.6 87.1 93.8 93.3
+d+ g 99.9 93.1 88.7 92.5 99.1 91.2 96.1 94.4

LANGID.PY 98.7 90.4 91.3 93.4 99.2 94.1 88.6 93.6
CLD2 99.0 85.0 85.3 90.7 98.5 85.0 93.4 91.0

Table 1: Accuracy [%] of the different models over the seven heldout datasets, and the macro-averaged accuracy
out-of-domain over the 7 test domains (“ALLout”). The best result for each dataset is indicated in bold. Key: +d
= domain adversarial, +g = domain generation component.

3.1.1 Results and Analysis

Baseline and comparisons For comparison, we
implement a CNN baseline8 which is trained us-
ing all the data without domain knowledge (i.e.
the simple union of the different training datasets).
We also employ adversarial learning (d) and gen-
eration (g) of domain to the baseline model to bet-
ter understand the utility of these methods. Note
that the baseline +d is a multi-domain variant of
Ganin and Lempitsky (2015), albeit trained with-
out any text in the testing domains. For our mod-
els, we report results of configurations both with
and without the d and g components. We also
report the results for two state-of-the-art off-the-
shelf LangID tools: (1) LANGID.PY9 (Lui and
Baldwin, 2012); and (2) Google’s CLD2.10

Out-of-domain Results Our primary concern
in terms of evaluating the ability of the differ-
ent models to generalise, is out-of-domain perfor-
mance. Table 1 provides a breakdown of out-of-
domain results over the 7 holdout domains. The
accuracy varies greatly between test domains, de-
pending on the mix of languages, length of test
documents, etc. Both our models, COND and
GEN, achieve competitive performance, and are
further improved by d and g.

For the baseline, applying either d or g results
in mild improvements over the baseline, which is
surprising as the two forms of supervision work in
opposite directions. Overall the small change in
performance means neither method appears to be
a viable technique for domain adaptation.

8The baseline here used a double capacity hidden repre-
sentation, in order to better match the increased expressivity
of the shared/private models.

9https://github.com/saffsd/langid.py
10https://github.com/CLD2Owners/cld2

Overall, the raw COND and GEN perform bet-
ter than the baseline. Specifically, for COND, we
observed performance gains on EuroPARL, T-BE
and T-SC. These three datasets are notable in con-
taining shorter documents, which benefit the most
from shared learning. However, as discussed ear-
lier, multi-domain data can introduce noise to the
shared representation, causing the performance to
drop over TCL, Wikipedia2 and EMEA. This
observation demonstrates the necessity of apply-
ing adversarial learning to COND. On the other
hand, it is a different story for GEN: vanilla GEN
achieves accuracy gains relative to the baseline
over 5 domains, but is slightly below COND for
4 domains, a result of parameter-sharing over the
private representation.

In terms of the adversarial learning, we see
that by adding an adversarial component (+d or
+d + g), COND and GEN realises substantial im-
provements out of domain, with the exception of
EMEA. As we motivated, the domain adversar-
ial part d can obscure the domain-specific infor-
mation in the shared representation, which helps
COND have better generalisation to other domains.
Additionally, applying g to GEN helps the private
representation to generalize better. These results
demonstrate that both d and g are necessary com-
ponents of multi-domain models. EMEA is note-
worthy in that its pattern of results is overall differ-
ent to the other domains, in that applying d hurts
performance. For this domain, the baseline CNN
performs very well, and GEN does much better
than COND. We believe the reason is that, as a
medical domain, EMEA is very much an outlier
and does not align to any single training domain.
Also, there is a lot of borrowing of terms such
as drug and disease names verbatim between lan-

477



Models Deb JRCA Wiki ClWb RCV2 ALLin

baseline CNN 96.6 99.8 97.8 90.7 97.9 96.6
baseline +d 96.5 99.8 97.8 90.7 97.0 96.4

COND 97.0 99.9 97.8 90.9 98.0 96.7
COND +d 97.0 99.9 98.4 90.8 98.1 96.8
GEN +d+ g 97.8 99.9 98.0 91.1 97.9 96.9

LANGID.PY 97.4 99.8 97.6 91.3 99.3 97.1
CLD2 92.2 99.8 92.3 92.3 89.8 93.3

Table 2: Accuracy [%] of different models over five in-
domain datasets using cross-validation evaluation and
macro-averaged accuracy (“ALLin”).

guages, further complicating the task.
Overall, our best models (COND +d and GEN

+d+ g) outperform both LANGID.PY and CLD2
in terms of average out-of-domain accuracy.

In-domain Results Table 2 reports the in-
domain performance over the 5 training domains,
using 5-fold cross validation, as well as the macro-
averaged accuracy. Our proposed methods (COND
+d and COND +d + g) consistently achieve bet-
ter performance than the baseline. Both COND
and GEN achieve competitive performance with
the state-of-the-art LANGID.PY in the in-domain
scenario. Although LANGID.PY performs slightly
better on average accuracy, our best model outper-
forms LANGID.PY for three of the five datasets.

3.2 Product Reviews

To evaluate the generalization of our methods to
other tasks, we experiment with the Multi-Domain
Sentiment Dataset (Blitzer et al., 2007).11 We
select the 20 domains with the most review in-
stances, and discard the remaining 5 domains.

For model parameterization, we adopt the same
basic hyper-parameter settings and training pro-
cess as for LangID in §3.1, but change the filter
sizes to 3, 4 and 5, use word-based tokenisation,
and truncate sentences to 256 tokens, for better
compatible with shorter documents.

We perform a out-of-domain evaluation over
four target domains, “book” (B), “dvd” (D), “elec-
tronics” (E) and “kitchen & housewares” (K), as
used in Blitzer et al. (2007). Our experimental
setup differs from theirs, in that they train on a
single domain and then evaluate on another, while
we train over 16 domains, then evaluate on the four

11From https://www.cs.jhu.edu/˜mdredze/
datasets/sentiment/, using the positive and negative
files from unprocessed, up to 2,000 instances per do-
main. For the four test domains we automatically aligned the
reviews in the processed and unprocessed, such that
we can compare results directly against prior work.

Models B D E K

baseline CNN 79.6 81.2 86.3 87.2
baseline +d 78.7 81.6 86.6 87.1

COND 79.2 81.8 85.8 87.2
COND +d 79.8 82.3 86.8 87.4
GEN +d+ g 80.2 82.4 87.3 87.8

SCL MI ♣ 76.0 78.5 77.9 85.9
DANN ♦ 72.3 78.4 84.3 85.4

IN DOMAIN ♣ 82.4 80.4 84.4 87.7

Table 3: Accuracy [%] of different models over 4 do-
mains (B, D, E and K) under out-of-domain evalua-
tions on Multi Domain Sentiment Dataset. Key: ♣

from Blitzer et al. (2007); ♦ from Ganin and Lempitsky
(2015).

test domains.
Table 3 presents the results. Overall, our pro-

posed methods consistently outperform the base-
lines, with the GEN +d + g approach a con-
sistent winner over all other techniques. Note
also the lacklustre performance when the base-
line is trained with the adversarial loss, mirror-
ing our findings for language identification in §3.1.
For comparison, we also report the best results
of SCL-MI and DANN, in both cases using an
oracle selection of source domain. Our method
consistently outperform these approaches, despite
having no test oracle, although note that we use
more diverse data sources for training.

4 Conclusions

We have proposed a novel deep learning method
for multi-domain learning, based on joint learn-
ing of domain-specific and domain-general com-
ponents, using either domain conditioning or do-
main generation. Based on our evaluation over
multi-domain language identification and multi-
domain sentiment analysis, we show our models to
substantially outperform a baseline deep learning
method, and set a new benchmark for state-of-the-
art cross-domain LangID. Our approach has po-
tential to benefit other NLP applications involving
multi-domain data.

Acknowledgments

We thank the anonymous reviewers for their help-
ful feedback and suggestions, and the National
Computational Infrastructure Australia for com-
putation resources. This work was supported by
the Australian Research Council (FT130101105).

478



References
Timothy Baldwin and Marco Lui. 2010. Language

identification: The long and the short of the mat-
ter. In Proceedings of Human Language Technolo-
gies: Conference of the North American Chapter of
the Association of Computational Linguistics. pages
229–237.

Zsolt Bitvai and Trevor Cohn. 2015. Non-linear text re-
gression with a deep convolutional neural network.
In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the
7th International Joint Conference on Natural Lan-
guage Processing Short Papers.

John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, bollywood, boom-boxes and
blenders: Domain adaptation for sentiment classi-
fication. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics.
pages 440–447.

Simon Carter, Wouter Weerkamp, and Manos
Tsagkias. 2013. Microblog language identification:
Overcoming the limitations of short, unedited and
idiomatic text. Language Resources and Evaluation
47(1):195–215.

William B Cavnar and John M Trenkle. 1994. N-
gram-based text categorization. In Proceedings of
the Third Symposium on Document Analysis and In-
formation Retrieval.

Hal Daumé III. 2007. Frustratingly easy domain adap-
tation. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguis-
tics. pages 256–263.

Yaroslav Ganin and Victor Lempitsky. 2015. Unsuper-
vised domain adaptation by backpropagation. In In-
ternational Conference on Machine Learning 2015.
pages 1180–1189.

Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan,
Pascal Germain, Hugo Larochelle, François Lavi-
olette, Mario Marchand, and Victor Lempitsky.
2016. Domain-adversarial training of neural net-
works. Journal of Machine Learning Research
17:59:1–59:35.

Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,
Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron C. Courville, and Yoshua Bengio. 2014. Gen-
erative adversarial nets. In Advances in Neural In-
formation Processing Systems 27. pages 2672–2680.

Mahesh Joshi, Mark Dredze, William W. Cohen, and
Carolyn Penstein Rosé. 2012. Multi-domain learn-
ing: When do domains matter? In Proceedings of
the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning. pages 1302–1312.

David Jurgens, Yulia Tsvetkov, and Dan Jurafsky.
2017. Incorporating dialectal variability for socially

equitable language identification. In Proceedings of
the 55th Annual Meeting of the Association for Com-
putational Linguistics. pages 51–57.

Yoon Kim. 2014. Convolutional neural networks for
sentence classification. In Proceedings of the 2014
Conference on Empirical Methods in Natural Lan-
guage Processing. pages 1746–1751.

Young-Bum Kim, Karl Stratos, and Ruhi Sarikaya.
2016. Frustratingly easy neural domain adaptation.
In Proceedings of COLING 2016, the 26th Inter-
national Conference on Computational Linguistics:
Technical Papers. pages 387–396.

Diederik P. Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. In Proceedings
of the International Conference on Learning Repre-
sentations.

Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT Summit 2005.
pages 79–86.

Marco Lui and Timothy Baldwin. 2011. Cross-domain
feature selection for language identification. In Fifth
International Joint Conference on Natural Lan-
guage Processing. pages 553–561.

Marco Lui and Timothy Baldwin. 2012. langid.py:
An off-the-shelf language identification tool. In
Proceedings of ACL 2012 System Demonstrations.
pages 25–30.

Marco Lui and Timothy Baldwin. 2014. Accurate lan-
guage identification of Twitter messages. In Pro-
ceedings of the 5th workshop on language analysis
for social media. pages 17–25.

Jörg Tiedemann. 2009. News from OPUS – a collec-
tion of multilingual parallel corpora with tools and
interfaces. In Recent Advances in Natural Language
Processing. volume 5, pages 237–248.

Erik Tromp and Mykola Pechenizkiy. 2011. Graph-
based n-gram language identification on short texts.
In Proceedings of the 20th Machine Learning Con-
ference of Belgium and The Netherlands. pages 27–
34.

Jason Williams. 2013. Multi-domain learning and gen-
eralization in dialog state tracking. In Proceedings
of the SIGDIAL 2013 Conference. pages 433–441.

479


