



















































Narrative Modeling with Memory Chains and Semantic Supervision


Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 278–284
Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics

278

Narrative Modeling with Memory Chains and Semantic Supervision

Fei Liu Trevor Cohn Timothy Baldwin
School of Computing and Information Systems

The University of Melbourne
Victoria, Australia

fliu3@student.unimelb.edu.au
t.cohn@unimelb.edu.au tb@ldwin.net

Abstract

Story comprehension requires a deep se-
mantic understanding of the narrative,
making it a challenging task. Inspired by
previous studies on ROC Story Cloze Test,
we propose a novel method, tracking var-
ious semantic aspects with external neural
memory chains while encouraging each
to focus on a particular semantic aspect.
Evaluated on the task of story ending pre-
diction, our model demonstrates superior
performance to a collection of competitive
baselines, setting a new state of the art. 1

1 Introduction

Story narrative comprehension has been a long-
standing challenge in artificial intelligence (Wino-
grad, 1972; Turner, 1994; Schubert and Hwang,
2000). The difficulties of this task arise from the
necessity for understanding not only narratives,
but also commonsense and normative social be-
haviour (Charniak, 1972). Of particular interest
in this paper is the work by Mostafazadeh et al.
(2016) on understanding commonsense stories in
the form of a Story Cloze Test: given a short story,
we must predict the most coherent sentential end-
ing from two options (e.g. see Figure 1).

Many attempts have been made to solve
this problem, based either on linear classifiers
with handcrafted features (Schwartz et al., 2017;
Chaturvedi et al., 2017), or representation learning
via deep learning models (Mihaylov and Frank,
2017; Bugert et al., 2017; Mostafazadeh et al.,
2017). Another widely used component of com-
petitive systems is language model-based features,
which require training on large corpora in the story
domain.

1Code available at http://github.com/liufly/
narrative-modeling.

Context: Sam loved his old belt. He matched it with
everything. Unfortunately he gained too much weight.
It became too small.
Coherent Ending: Sam went on a diet.
Incoherent Ending: Sam was happy.

Figure 1: Story Cloze Test example.

The current state-of-the-art approach of
Chaturvedi et al. (2017) is based on understanding
the context from three perspectives: (1) event
sequence, (2) sentiment trajectory, and (3) topic
consistency. Chaturvedi et al. (2017) adopt exter-
nal tools to recognise relevant aspect-triggering
words, and manually design features to incorpo-
rate them into the classifier. While identifying
triggers has been made easy by the use of various
linguistic resources, crafting such features is
time consuming and requires domain-specific
knowledge along with repeated experimentation.

Inspired by the argument for tracking the dy-
namics of events, sentiment and topic, we pro-
pose to address the issues identified above with
multiple external memory chains, each responsi-
ble for a single aspect. Building on Recurrent En-
tity Networks (EntNets), a superior framework
to LSTMs demonstrated by Henaff et al. (2017) for
reasoning-focused question answering and cloze-
style reading comprehension, we introduce a novel
multi-task learning objective, encouraging each
chain to focus on a particular aspect. While still
making use of external linguistic resources, we do
not extract or design features from them but rather
utilise such tools to generate labels. The generated
labels are then used to guide training so that each
chain focuses on tracking a particular aspect. At
test time, our model is free of feature engineering
such that, once trained, it can be easily deployed to
unseen data without preprocessing. Moreover, our

http://github.com/liufly/narrative-modeling
http://github.com/liufly/narrative-modeling


279

approach also differs in the lack of a ROC Stories
language model component, eliminating the need
for large, domain-specific training corpora.

Evaluated on the task of Story Cloze Test, our
model outperforms a collection of competitive
baselines, achieving state-of-the-art performance
under more modest data requirements.

2 Methodology

In the story cloze test, given a story of length
L, consisting of a sequence of context sentences
c = {c1, c2, . . . , cL}, we are interested in predict-
ing the coherent ending to the story out of two end-
ing options o1 and o2. Following previous studies
(Schwartz et al., 2017), we frame this problem as a
binary classification task. Assuming o1 and o2 are
the logical and inconsistent endings respectively
with their associated labels being y1 and y2, we
assign y1 = 1 and y2 = 0. At test time, given
a pair of possible endings, the system returns the
one with the higher score as the prediction. In this
section, we first describe the model architecture
and then detail how we identify aspect-triggering
words in text and incorporate them in training.

2.1 Proposed Model

First, to take neighbouring contexts into account,
we process context sentences and ending options
at the word level with a bi-directional gated recur-
rent unit (“Bi-GRU”: Chung et al. (2014)):

−→
h i =−−→

GRU(wi,
−→
h i−1) where wi is the embedding of

the i-th word in the story or ending option. The
backward hidden representation

←−
h i is computed

analogously but with a different set of GRU pa-
rameters. We simply take the sum hi =

−→
h i +

←−
h i

as the representation at time i. An ending option,
denoted o, is represented by taking the sum of the
final states of the same Bi-GRU over the ending
option word sequence.

This representation is then taken as input to
a Recurrent Entity Network (“EntNet”: Henaff
et al. (2017)), capable of tracking the state of
the world with external memory. Developed in
the context of machine reading comprehension,
EntNet maintains a number of “memory chains”
— one for each entity — and dynamically up-
dates the representations of them as it progresses
through a story. Here, we borrow the concept of
“entity” and use each chain to track a unique as-
pect. An illustration of EntNet is provided in
Figure 2.

hi

φ

key kj

σL

C

�
update
gate
gji

m̃ji

+
memory mji−1 m

j
i

Figure 2: Illustration of EntNet with a single
memory chain at time i. φ and σ represent Equa-
tions 1 and 2, while circled nodes L, C, � and
+ depict the location, content terms, Hadamard
product, and addition, resp.

Memory update candidate. At every time step
i, the internal memory update candidate

−→̃
mji for

the j-th memory chain is computed as:

−→̃
mji = φ(

−→
U−→mji−1 +

−→
V kj +

−→
Whi) (1)

where kj is the embedding for the j-th entity
(key),

−→
U ,
−→
V and

−→
W are trainable parameters, and

φ is the parametric ReLU (He et al., 2015).

Memory update. The update of each memory
chain is controlled by a gating mechanism:

−→g ji = σ(hi · k
j + hi ·

−→mji−1) (2)
−→
m̊ji =

−→mji−1 +
−→g ji �

−→̃
mji (3)

where � denotes Hadamard product (resulting
in the unnormalised memory representation

−→
m̊ji ),

and σ is the sigmoid function. The gate −→g ji con-
trols how much the memory chain should be up-
dated, a decision factoring in two elements: (1) the
“location” term, quantifying how related the cur-
rent input hi (the output of the Bi-GRU at time i)
is to the key kj of the entity being tracked; and (2)
the “content” term, measuring the similarity be-
tween the input and the current state −→mji−1 of the
entity tracked by the j-th memory chain.

Normalisation. We normalise each memory
representation:−→mji =

−→
m̊ji/‖

−→
m̊ji‖where ‖

−→
m̊ji‖ de-

notes the Euclidean norm of
−→
m̊ji . In doing so, we

allow the model to forget in the sense that, as −→mji
is constrained to be lying on the surface of the unit
sphere, adding new information carried by

−→̃
mji and

then performing normalisation inevitably causes
forgetting in that the cosine distance between the
original and updated memory decreases.



280

Bi-directionality. In contrast to EntNet, we
apply the above steps in both directions, scanning
a story both forward and backward, with arrows
denoting the processing direction.←−mji is computed
analogously to −→mji but with a different set of pa-
rameters, and mji =

−→mji +
←−mji . We further define

gji to be the average of the gate values of both di-
rections at time i for the j-th chain:

gji = (
−→g ji +

←−g ji )/2 (4)

which will be used for semantic supervision.

Final classifier. The final prediction ŷ to an end-
ing option given its context story is performed by
incorporating the last states of all memory chains
in the form of a weighted sum u:

pj = softmax((kj)>Watto) (5)

u =
∑
j

pjmjT (6)

where T denotes the total number of words in a
story and Watt is a trainable weight matrix. We
then transform u to get the final prediction:

ŷ = σ(Rφ(Hu + o)) (7)

where R and H are trainable weight matrices.

2.2 Semantically Motivated Memory Chains
In order to encourage each chain to focus on a
particular aspect, we supervise the activation of
each update gate (Equation 2) with binary labels.
Intuitively, for the sentiment chain, a sentiment-
bearing word (e.g. amazing) receives a label of 1,
allowing the model to open the gate and therefore
change the internal state relevant to this aspect,
while a neutral one (e.g. school) should not trig-
ger the activation with an assigned label of 0. To
achieve this, we use three memory chains to in-
dependently track each of: (1) event sequence, (2)
sentiment trajectory, and (3) topical consistency.
To supervise the memory update gates of these
chains, we design three sequences of binary labels:
lj = {lj1, l

j
2, . . . , l

j
T } for j ∈ [1, 3] representing

event, sentiment, and topic, and lji ∈ {0, 1}. The
label at time i for the j-th aspect is only assigned
a value of 1 if the word is a trigger for that particu-
lar aspect: lji = 1; otherwise l

j
i = 0. During train-

ing, we utilise such sequences of binary labels to
supervise the memory update gate activations of
each chain. Specifically, each chain is encouraged

Ricky fell while hiking in the woods

lEvent 0 1 0 1 0 0 1
lSentiment 0 1 0 0 0 0 0
lTopic 1 1 0 1 0 0 1

Table 1: An example of the linguistically moti-
vated memory chain supervision binary labels.

to open its gate only when it sees a trigger bearing
information semantically sensitive to that particu-
lar aspect. Note that at test time, we do not apply
such supervision. This effectively becomes a bi-
nary tagging task for each memory chain indepen-
dently and results in individual memory chains be-
ing sensitive to only a specific set of triggers bear-
ing one of the three types of signals.

While largely inspired by Chaturvedi et al.
(2017), our approach differs in how we extract
and use such features. Also note that, while still
making use of external linguistic resources to de-
tect trigger words, our approach lets the mem-
ory chains decide how such words should in-
fluence the final prediction, as opposed to the
handcrafted conditional probability features of
Chaturvedi et al. (2017).

To identify the trigger words, we use external
linguistic tools, and mark trigger words for each
aspect with a label of 1. An example is presented
in Table 1, noting that the same word can act as
trigger for multiple aspects.

Event sequence. We parse each sentence into
its FrameNet representation with SEMAFOR (Das
et al., 2010), and identify each frame target (word
or phrase tokens evoking a frame).

Sentiment trajectory. Following Chaturvedi
et al. (2017), we utilise a pre-compiled list of sen-
timent words (Liu et al., 2005). To take nega-
tion into account, we parse each sentence with
the Stanford Core NLP dependency parser
(Manning et al., 2014; Chen and Manning, 2014)
and include negation words as trigger words.

Topical consistency. We process each sentence
with the Stanford Core NLP POS tagger and
identify nouns and verbs, following Chaturvedi
et al. (2017).

2.3 Training Loss

In addition to the cross entropy loss of the final
prediction of right/wrong endings, we also take
into account the memory update gate supervision



281

of each chain by adding the second term. More for-
mally, the model is trained to minimise the loss:

L = XEntropy(y, ŷ) + α
∑
i,j

XEntropy(lji , g
j
i )

where ŷ and gji are defined in Equations 7 and 4 re-
spectively, y is the gold label for the current ending
option o, and lji is the semantic supervision binary
label at time i for the j-th memory chain. In our
experiments, we empirically set α to 0.5 based on
development data.

3 Experiments

3.1 Experimental Setup

Dataset. To test the effectiveness of our model,
we employ the Story Cloze Test dataset of
Mostafazadeh et al. (2016). The development and
test set each consist of 1,871 4-sentence stories,
each with a pair of ending options. Consistent with
previous studies, we split the development set into
a training and validation set (for early stopping),
resulting in 1,683 and 188 in each set, resp. Note
that while most current approaches make use of
the much larger training set, comprised of 100K 5-
sentence ROC stories (with coherent endings only,
also released as part of the dataset) to train a lan-
guage model, we make no use of this data.

Model configuration. We initialise our model
with word2vec embeddings (300-D, pre-trained
on 100B Google News articles, not updated during
training: Mikolov et al. (2013a,b)). In addition to
the three supervised chains, we also add a “free”
chain, unconstrained to any semantic aspect.

Training is carried out over 200 epochs with
the FTRL optimiser (McMahan et al., 2013) and a
batch size of 128 and learning rate of 0.1. We use
the following hyper-parameters for weight matri-
ces in both directions: R ∈ R300×1, H, U, V, W
are all matrices of size R300×300, and hidden size
of the Bi-GRU is 300. Dropout is applied to the
output of φ in the final classifier (Equation 7) with
a rate of 0.2. Moreover, we employ the technique
introduced by Gal and Ghahramani (2016) where
the same dropout mask is applied at every step to
the input wi to the Bi-GRU and the input hi to the
memory chains with rates of 0.5 and 0.2 respec-
tively. Lastly, to curb overfitting, we regularise the
last layer (Equation 7) with an L2 penalty on its
weights: λ‖R‖ where λ = 0.001.

Model Acc. SD

DSSM 58.5 —
TBMIHAYLOV 72.8 —
MSAP 75.2 —
HCM 77.6 —
EntNet † 77.6 ±0.5
Our model† 78.5 ±0.5

Table 2: Performance on the Story Cloze test set.
Bold = best performance, † = ave. accuracy over 5
runs, SD = standard deviation, “—” = not reported.

Evaluation. Following previous studies, we
evaluate the performance in terms of coherent end-
ing prediction accuracy: #correct#total . Here, we report
the average accuracy over 5 runs with different
random seeds. Echoing Melis et al. (2018), we also
observe some variance in model performance even
if training is carried out with the same random
seed, which is largely due to the non-deterministic
ordering of floating-point operations in our envi-
ronment (Tensorflow (Abadi et al., 2015) with
a single GPU). Therefore, to account for the ran-
domness, we further train our model 5 times for
each random seed and select the model with the
best validation performance.

We benchmark against a collection of strong
baselines, including the top-3 systems of the 2017
LSDSem workshop shared task (Mostafazadeh
et al., 2017): MSAP (Schwartz et al., 2017), HCM
(Chaturvedi et al., 2017)2, and TBMIHAYLOV
(Mihaylov and Frank, 2017). The first two pri-
marily rely on a simple logistic regression clas-
sifier, both taking linguistic and probability fea-
tures from a ROC Stories domain-specific neu-
ral language model. TBMIHAYLOV is LSTM-
based; we also include DSSM (Mostafazadeh et al.,
2016). We additionally implement a bi-directional
EntNet (Henaff et al., 2017) with the same
hyper-parameter settings as our model and no se-
mantic supervision.3

3.2 Results and Discussion

The experimental results are shown in Table 2.

State-of-the-art results. Our model outper-
forms a collection of strong baselines, setting a
new state of the art. Note that this is achieved with-

2We take the performance reported in a subsequent paper,
3.2% better than the original submission to the shared task.

3EntNet is also subject to the same model selection cri-
terion described above.



282

Event Key Sentiment Key Topic Key Free Key
Event Word Sentiment Word Topic Word

Figure 3: t-SNE scatter plot of aspect-triggering
words (output representations hi by Bi-GRU,
500 randomly sampled from each aspect) and the
learned keys. EntNet (left) vs. our model (right).
Best viewed in colour.

out any external linguistic resources at test time or
domain-specific language model-based probabil-
ity features, highlighting the effectiveness of the
proposed model.

EntNet vs. our model. We see clear improve-
ments of the proposed method over EntNet,
an absolute gain of 0.9% in accuracy. This vali-
dates the hypothesis that encouraging each mem-
ory chain to focus on a unique, well-defined aspect
is beneficial.

Discussion. To better understand the benefits of
the proposed method, we visualise the learned
word representations (output representation of the
Bi-GRU, hi) and keys between EntNet and our
model in Figure 3. With EntNet, while senti-
ment words form a loose cluster, words bearing
event and topic signal are placed in close prox-
imity and are largely indistinguishable. With our
model, on the other hand, the degree of separation
is much clearer. The intersection of a small por-
tion of the event and topic groups is largely due to
the fact that both aspects include verbs. Another
interesting perspective is the location of the au-
tomatically learned keys (displayed as diamonds):
while all the keys with EntNet end up overlap-
ping, indicating little difference among them, the
keys learned by our method demonstrate seman-
tic diversity, with each placed within its respective
cluster. Note that the free key is learned to com-
plement the event key, a difficult challenge given
the two disjoint clusters.

Ablation study. We further perform a ablation
study to analyse the utility of each component
in Table 3. The uni-directional variant, with its
performance down to 77.8, is inferior to its bi-

Model Acc. SD

Our model 78.5 ±0.5
—bi-directionality 77.8 ±0.7
—all semantic supervision 77.6 ±0.5

—event sequence 78.7 ±0.2
—sentiment trajectory 75.9 ±0.4
—topical consistency 77.3 ±0.4
—free chain 77.0 ±0.4

Table 3: Ablation study. Average accuracy over
5 runs on the Story Cloze test set (all subject to
the same model selection criterion as our model).
Bold: best performance. SD: standard deviation.

directional cousin. Removing all semantic super-
vision (resulting in 4 free memory chains) is
also damaging to the accuracy (dropping to 77.6).
Among the different types of semantic super-
vision, we see various degrees of performance
degradation, with removing sentiment trajectory
being the most detrimental, reflecting its value
to the task. Interestingly, we observe improve-
ment when removing event sequence supervision
from consideration. We suspect that this is mainly
due to the noise introduced by the rather inaccu-
rate FrameNet representation output of SEMAFOR
(F1 = 61.4% in frame identification as reported in
Das et al. (2010)). While it is possible that replac-
ing SEMAFOR with the SemLM approach (with
the extended frame definition to include explicit
discourse markers, e.g. but) in Peng and Roth
(2016) and Chaturvedi et al. (2017) may poten-
tially reduce the amount of noise, we leave this
exercise for future work.

4 Conclusion

In this paper, we have proposed a novel model
for tracking various semantic aspects with exter-
nal memory chains. While requiring less domain-
specific training data, our model achieves state-
of-the-art performance on the task of ROC Story
Cloze ending prediction, beating a collection of
strong baselines.

Acknowledgments

We thank the anonymous reviewers for their
valuable feedback, and gratefully acknowledge
the support of Australian Government Research
Training Program Scholarship. This work was
also supported in part by the Australian Research
Council.



283

References
Martı́n Abadi, Ashish Agarwal, Paul Barham, Eugene

Brevdo, Zhifeng Chen, Craig Citro, Greg S. Cor-
rado, Andy Davis, Jeffrey Dean, Matthieu Devin,
Sanjay Ghemawat, Ian Goodfellow, Andrew Harp,
Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal
Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh
Levenberg, Dandelion Mané, Rajat Monga, Sherry
Moore, Derek Murray, Chris Olah, Mike Schuster,
Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Ku-
nal Talwar, Paul Tucker, Vincent Vanhoucke, Vi-
jay Vasudevan, Fernanda Viégas, Oriol Vinyals, Pete
Warden, Martin Wattenberg, Martin Wicke, Yuan
Yu, and Xiaoqiang Zheng. 2015. TensorFlow:
Large-scale machine learning on heterogeneous sys-
tems. Software available from tensorflow.org.

Michael Bugert, Yevgeniy Puzikov, Andreas Rücklé,
Judith Eckle-Kohler, Teresa Martin, Eugenio
Martı́nez-Cámara, Daniil Sorokin, Maxime Peyrard,
and Iryna Gurevych. 2017. LSDSem 2017: Explor-
ing data generation methods for the story cloze test.
In Proceedings of the 2nd Workshop on Linking
Models of Lexical, Sentential and Discourse-level
Semantics (LSDSem 2017), pages 56–61, Valencia,
Spain.

Eugene Charniak. 1972. Toward a model of chil-
dren”s story comprehension. Technical report,
Massachusetts Institute of Technology, Cambridge,
USA.

Snigdha Chaturvedi, Haoruo Peng, and Dan Roth.
2017. Story comprehension for predicting what hap-
pens next. In Proceedings of the 2017 Conference
on Empirical Methods in Natural Language Pro-
cessing (EMNLP 2017), pages 1603–1614, Copen-
hagen, Denmark.

Danqi Chen and Christopher Manning. 2014. A fast
and accurate dependency parser using neural net-
works. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Processing
(EMNLP 2014), pages 740–750, Doha, Qatar.

Junyoung Chung, Caglar Gulcehre, KyungHyun Cho,
and Yoshua Bengio. 2014. Empirical evaluation of
gated recurrent neural networks on sequence model-
ing. In Proceedings of the NIPS 2014 Deep Learn-
ing and Representation Learning Workshop.

Dipanjan Das, Nathan Schneider, Desai Chen, and
Noah A. Smith. 2010. Probabilistic frame-semantic
parsing. In Human Language Technologies: The
2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics (NAACL-HLT 2010), pages 948–956, Los
Angeles, USA.

Yarin Gal and Zoubin Ghahramani. 2016. A theo-
retically grounded application of dropout in recur-
rent neural networks. In Proceedings of Advances
in Neural Information Processing Systems (NIPS
2016), pages 1027–1035, Barcelona, Spain.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2015. Delving deep into rectifiers: Surpass-
ing human-level performance on ImageNet classifi-
cation. In Proceedings of the 2015 IEEE Interna-
tional Conference on Computer Vision (ICCV 2015),
pages 1026–1034, Washington, DC, USA.

Mikael Henaff, Jason Weston, Arthur Szlam, Antoine
Bordes, and Yann LeCun. 2017. Tracking the world
state with recurrent entity networks. In Proceed-
ings of the 5th International Conference on Learn-
ing Representations (ICLR 2017), Toulon, France.

Bing Liu, Minqing Hu, and Junsheng Cheng. 2005.
Opinion observer: analyzing and comparing opin-
ions on the web. In Proceedings of the 14th Interna-
tional Conference on World Wide Web (WWW 2005),
pages 342–351, Chiba, Japan.

Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David Mc-
Closky. 2014. The Stanford CoreNLP natural lan-
guage processing toolkit. In Association for Compu-
tational Linguistics (ACL) System Demonstrations,
pages 55–60.

H. Brendan McMahan, Gary Holt, David Sculley,
Michael Young, Dietmar Ebner, Julian Grady,
Lan Nie, Todd Phillips, Eugene Davydov, Daniel
Golovin, et al. 2013. Ad click prediction: A view
from the trenches. In Proceedings of the 19th ACM
SIGKDD International Conference on Knowledge
Discovery and Data Mining (KDD 2013), pages
1222–1230, Chicago, USA.

Gábor Melis, Chris Dyer, and Phil Blunsom. 2018.
On the state of the art of evaluation in neural lan-
guage models. In Proceedings of the 6th Inter-
national Conference on Learning Representations
(ICLR 2018), Vancouver, Canada.

Todor Mihaylov and Anette Frank. 2017. Story cloze
ending selection baselines and data examination. In
Proceedings of the 2nd Workshop on Linking Models
of Lexical, Sentential and Discourse-level Semantics
(LSDSem 2017), pages 87–92, Valencia, Spain.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space. In Proceedings of the 1st
International Conference on Learning Representa-
tions (ICLR 2013), Scottsdale, USA.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In Proceedings of the 27th Annual Conference
on Neural Information Processing Systems (NIPS
2013), pages 3111–3119, Stateline, USA.

Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong
He, Devi Parikh, Dhruv Batra, Lucy Vanderwende,
Pushmeet Kohli, and James Allen. 2016. A cor-
pus and cloze evaluation for deeper understanding of
commonsense stories. In Proceedings of the 2016

https://www.tensorflow.org/
https://www.tensorflow.org/
https://www.tensorflow.org/


284

Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies (NAACL-HLT 2016), pages
839–849, San Diego, USA.

Nasrin Mostafazadeh, Michael Roth, Annie Louis,
Nathanael Chambers, and James Allen. 2017. LS-
DSem 2017 shared task: The story cloze test. In
Proceedings of the 2nd Workshop on Linking Models
of Lexical, Sentential and Discourse-level Semantics
(LSDSem 2017), pages 46–51, Valencia, Spain.

Haoruo Peng and Dan Roth. 2016. Two discourse
driven language models for semantics. In Proceed-
ings of the 54th Annual Meeting of the Association
for Computational Linguistics (ACL 2016), pages
290–300, Berlin, Germany.

Lenhart K. Schubert and Chung Hee Hwang. 2000.
Episodic logic meets little red riding hood: A com-
prehensive, natural representation for language un-
derstanding. In Lucja Iwanska and Stuart C.
Shapiro, editors, Natural Language Processing and
Knowledge Representation: Language for Knowl-
edge and Knowledge for Language, pages 111–174.
MIT/AAAI Press, Menlo Park, USA.

Roy Schwartz, Maarten Sap, Ioannis Konstas, Leila
Zilles, Yejin Choi, and Noah A. Smith. 2017. The
effect of different writing tasks on linguistic style:
A case study of the ROC story cloze task. In Pro-
ceedings of the 21st Conference on Computational
Natural Language Learning (CoNLL 2017), pages
15–25, Vancouver, Canada.

Scott R. Turner. 1994. The Creative Process: A
Computer Model of Storytelling and Creativity.
Lawrence Erlbaum, Hillsdale, USA.

Terry Winograd. 1972. Understanding natural lan-
guage. Cognitive Psychology, 3(1):1–191.


