



















































Steps Toward Automatic Understanding of the Function of Affective Language in Support Groups


Proceedings of The Fourth International Workshop on Natural Language Processing for Social Media, pages 26–33,
Austin, TX, November 1, 2016. c©2016 Association for Computational Linguistics

Steps Toward Automatic Understanding of the Function of Affective
Language in Support Groups

Amit Navindgi∗
Veritas Technologies
Mountain View, CA

navindgi@usc.edu

Caroline Brun, Cécile Boulard Masson
Xerox Research Centre Europe

Meylan, France
{caroline.brun,
cecile.boulard}
@xrce.xerox.com

Scott Nowson∗
Accenture Centre for Innovation

Dublin, Ireland
scott.nowson@accenture.com

Abstract

Understanding expression of emotions in sup-
port forums has great value and NLP methods
are key to automating this. Many approaches
use subjective categories which are more fine-
grained than a straightforward polarity-based
spectrum. However, the definition of such cat-
egories is non-trivial, and we argue for a need
to incorporate communicative elements even
beyond subjectivity. To support our position,
we report experiments on a sentiment-labelled
corpus of posts from a medical support forum.
We argue that a more fine-grained approach
to text analysis important, and also simultane-
ously recognising the social function behind
affective expressions enables a more accurate
and valuable level of understanding.

1 Introduction

There are a wealth of opinions on the internet. So-
cial media has lowered the accessibility bar to an
even larger audience who are now able to share their
voice. However, more than just opinions on external
matters, people are able to share their emotions and
feelings, talking openly about very personal mat-
ters. Online presence has been shown to increase
the chance of sharing personal information and emo-
tions compared to face-to-face interactions (Han-
cock et al., 2007).

Medical support forums are one platform on
which users generate emotion-rich content, ex-
change factual information about elements such as

∗Work carried out while all authors at Xerox Research Cen-
tre Europe.

treatments or hospitals, and provide emotional sup-
port to others (Bringay et al., 2014). This sharing
through open discussion is known to be considerably
beneficial (Pennebaker et al., 2001).

Understanding affective language in the health-
care domain is an effective application of natu-
ral language technologies. Sentiment mining on
platforms such as Twitter, for example, is a quick
method to gauge public opinion of government poli-
cies (Speriosu et al., 2011). However, the level of af-
fective expressions in a support forum setting is con-
siderably more complex than a traditional positive-
negative polarity spectrum.

More than just a more-fined grained labelling
scheme, we also need a deeper understanding on
the language being used. Much sentiment analysis
research has focused on classifying the overall sen-
timent of documents onto a positive-negative spec-
trum (Hu and Liu, 2004). Recently, research work
targeting finer grained analysis has emerged, such
as aspect-based sentiment analysis (Liu, 2012; Pon-
tiki et al., 2014), or semantic role labelling of emo-
tions (Mohammad et al., 2014). This relatively new
trend in social media analytics enables the detection
of not simply binary sentiment, but more nuanced
sentiments and mixed feelings. Such affective ex-
pressions often serve a social purpose (Rothman and
Magee, 2016).

With this in mind, we explore a dataset drawn
from a health-related support forum, labelled for a
variety of expressed sentiments. Here, we do not
necessarily seek state-of-the-art performance, but
use this task to argue for two key positions:

• that sub-document level analysis is required to

26



best understand affective expressions
• that to fully understand expressions of emo-

tion in support forums, a fine-grained annota-
tion scheme is required which takes into ac-
count the social function of such expressions.

This paper begins by reviewing work related to
our propositions above. In Section 3 we describe the
data which we have used, paying particular attention
to the annotation scheme. We then report on our ex-
periments, which were defined in order to support
the hypotheses above. Following this, in Section 5
we discuss the implication of this work.

2 Related Work

As reported earlier, polarity-based studies in the
healthcare domain have considerable value. One
work squarely in the public policy domain sought
to classify tweets related to the recent health care re-
form in the US into positive and negative (Speriosu
et al., 2011). Ali et al. (2013) experimented with
data from multiple forums for people with hearing-
loss. They use the subjectivity lexicon of Wilson et
al. (2005) and count-based syntactic features (e.g.
number of adjectives, adverbs, etc.). This approach
outperformed a baseline bag-of-words model, high-
lighting the importance of subjective lexica for text
analysis in health domain. Ofek et al. (2013) use
a dynamic sentiment lexicon to improve sentiment
analysis in an online community for cancer sur-
vivors.

Sokolova and Bobicev (2013) took the lexicon ap-
proach further: they defined a more fine-grained an-
notation scheme (see Section 3) and labelled data
from an IVF-related forum. Their category-specific
set of lexicons performed better, at 6-class classifi-
cation, than a generic subjectivity lexicon. In select-
ing their data, Sokolova and Bobicev (2013) – as Ali
et al. (2013) and others have done – tapped into the
domain of on-line support communities. Eastin and
LaRose (2005) showed that people who seek support
on-line – be it emotional or informational support –
typically find it.

Informational support is based on sharing knowl-
edge and experiences. Emotional support – framed
as empathic communication – has four motiva-
tions: understanding, emotions, similarities and con-
cerns (Pfeil and Zaphiris, 2007). In addition to

direct support, another dimension of such online
groups is self-disclosure (Prost, 2012). Barak and
Gluck-Ofri (2007) identify self-disclosure as spe-
cific to open support groups (e.g. “Emotional Sup-
port for Adolescents”) as opposed to, for exam-
ple, subject-specific discussion forums (e.g. “Veg-
etarianism and Naturalism” or “Harry Potter The
Book”). Self-disclosure serves three social functions
(Tichon and Shapiro, 2003): requesting implicit sup-
port by showing confusion and worries; providing
support by sharing details of a personal experience
and sharing information to further develop social re-
lationships.

3 Data

3.1 Data Source
The data used here1 is that of Bobicev and Sokolova
(2015), an extension of the data described in
Sokolova and Bobicev (2013). Data was collected
from discussion threads on a sub-forum of an In
Vitro Fertilization (IVF) medical forum2 used by
participants who belong to a specific age-group
(over 35s). The dataset (henceforth MedSenti) orig-
inally contained 1321 posts across 80 different top-
ics.

3.2 Annotation Details
There are two approaches to annotation of subjec-
tive aspects of communication: from the perspec-
tive of a reader’s perception (Strapparava and Mihal-
cea, 2007) or that of the author (Balahur and Stein-
berger, 2009). In labelling MedSenti Sokolova and
Bobicev (2013) opted for the reader-centric model
and hence asked the annotators to analyse a post’s
sentiment as if they were other discussion partici-
pants. This is an important differentiation for au-
tomated classification style tasks - models are built
to predict how people will understand the emotion
expressed, as opposed to the emotion or sentiment
an author feels they are conveying. The annotation
scheme was evolved over multiple rounds of data ex-
ploration, and ultimately three sentiment categories
were defined:

1. confusion, (henceforth CONF) which includes
aspects such as “worry, concern, doubt, im-

1Kindly provided to us by the authors.
2http://ivf.ca/forums

27



patience, uncertainty, sadness, angriness, em-
barrassment, hopelessness, dissatisfaction, and
dislike”

2. encouragement, (ENCO) includes “cheering,
support, hope, happiness, enthusiasm, excite-
ment, optimism”

3. gratitude, (GRAT) which represents thankful-
ness and appreciation

This set of labels captures important dimensions
identified in the sociology literature. CONF here, for
example, maps to expressions of confusion (Tichon
and Shapiro, 2003) and those of concern (Pfeil and
Zaphiris, 2007).
CONF is essentially a negative category while

ENCO is positive. GRAT would therefore be a sub-
set of positive expressions. In contrast, however, it
was clear that certain expressions which might be
considered negative on a word level – such as those
of compassion, sorrow, and pity – were used with
a positive, supportive intention. They were there-
fore included in the ENCO category, and were often
posted with other phrases which would in isolation
fall under this label.

In addition to the subjective categories, Sokolova
and Bobicev (2013) identified two types of objec-
tive posts: those with strictly factual information
(FACT), and those which combined factual infor-
mation and short emotional expression (typically of
the ENCO type) which were labelled as endorsement
(ENDO). Each of the 1321 individual posts was la-
belled with one of the above five classes by two an-
notators.3

3.3 Data and Label Preprocessing

We select document labels as per Bobicev and
Sokolova (2015): when two labels match, reduce
to a single label; when the labels disagree the post
is marked with a sixth label ambiguous (AMBI),
which was not used in any experiment here. Posts
with previous post quotation are annotated with
(“QOTEHERE”), and quoting posts which contained
no additional content were removed. This leaves
1137 posts in our MedSenti corpus, with the cate-
gory distribution as per Table 1.

3Fleiss kappa = 0.73 (Bobicev and Sokolova, 2015).

Class # Posts %age # Sents %age
CONF 115 10.1 1087 13.5%
ENCO 309 27.2 1456 18.0%
ENDO 161 14.2 1538 19.1%
GRAT 122 10.7 733 9.1%
FACT 430 37.8 3257 40.4%
TOTAL 1137 8071
Table 1: Class distribution of posts and sentences

4 Experiments

To support our positions for understanding affective
expressions in support forums, and highlight some
of the challenges with current approaches, we report
a series of experiments.

4.1 Broad methodology

We use a robust dependency syntactic parser (Ait-
Mokhtar et al., 2001) to extract a wide range of
textual features, from n-grams to more sophisti-
cated linguistic attributes. Our experiments are
framed as multi-class classification tasks using lib-
linear (Fan et al., 2008) and used 5-fold stratified
cross-validation. We do not use, here, a domain-
tuned lexicon. We re-implemented the Health Af-
fect Lexicon (Sokolova and Bobicev, 2013) and it
performed as well as previously reported. How-
ever, such lexicons do not generalise well, and label-
based tuning is very task specific. We use the current
set of categories to make more general points about
work in support-related domains.

4.2 Document Level analysis

Here, we consider each post as a single unit of text
with a single label.

4.2.1 5-class classification
We utilised combinations of different linguistic

feature sets, ranging from basic n-grams, through se-
mantic dependency features. Here, we list the best
performing combination: word uni-, bi-, and tri-
grams; binary markers for questions, conjunctions
and uppercase characters; and a broad-coverage po-
larity lexicon. Results can be seen in Table 2

Our best overall score (macro averaged F1 =
0.449) is significantly above the majority class base-
line (F = 0.110). This compares favorably with
the six-class performance of semantic features of the
original data analysis (F1 = 0.397, Sokolova and

28



P R F
CONF 0.363 0.357 0.360
ENCO 0.555 0.854 0.673
ENDO 0.147 0.062 0.087
GRAT 0.583 0.492 0.533
FACT 0.573 0.502 0.535
MacroAvg 0.444 0.453 0.449

Table 2: Precision, Recall and F1 for the best feature set on
5-class document-level classification

Bobicev, 2013). However, more important – and
not previously reported – is the per-category perfor-
mance which gives more insight into the data. Es-
sentially, we see that ENCO, GRAT and FACT per-
form relatively well while CONF and in particular
ENDO are considerably poor.

To further explore this result we analyzed the er-
ror matrix (Navindgi et al., 2016). Looking at ENDO
we see that incredibly only 6% has been correctly
classified, while 86% is classified as either FACT or
ENCO. This is theoretically understandable since the
ENDO category is defined as containing aspects of
both the other two categories directly. The reverse
mis-classification is considerably less common, as
is mis-classification as GRAT. CONF is also mis-
classified as FACT a majority, with 43%. One-vs-
All analysis allows further insight (Navindgi et al.,
2016). It is clear that this challenge is not a triv-
ial one - there are distinct patterns of errors when
classifying at the document level. In order to inves-
tigate this further, we move to sentence-level classi-
fication.

4.3 Sentence Level analysis

In sentence-level analysis, we tokenise each post
into its constituent sentences. The 1137 MedSenti
posts become 8071 sentences, MedSenti-sent. As
manual annotation at sentence level would be too
costly, we used automated methods to label the cor-
pus with the five categories of sentiment.

4.3.1 Naı̈ve Labelling
The most trivial approach to label sentences is for

each sentence to inherit the label of the post in which
it is present. Following this method, we obtain the
distribution as reported in Table 1

We run the 5-class classification scenario on
MedSenti-sent using the same conditions and the
previous best feature set; the results are shown in

Table 3. Overall, the performance is worse than the
post-level counterpart, with the exception of a small
improvement to ENDO. FACT is the best performing
individual category, though now with greater recall
than precision.

P R F
CONF 0.235 0.157 0.188
ENCO 0.343 0.360 0.351
ENDO 0.174 0.088 0.117
GRAT 0.264 0.225 0.243
FACT 0.443 0.598 0.509
MacroAvg 0.291 0.286 0.289

Table 3: Precision, Recall and F1 for Sentence-level classifica-
tion

We also explore the model performance with the
error matrix (Navindgi et al., 2016). Our main
observation is that the drop in performance of the
four subjective categories is largely due to mis-
classification of sentences as FACT. Sentences in
this category are the majority in MedSenti-sent.
However, the proportional differences with Med-
Senti do not seem to be not enough to explain the
significant changes.

A more likely explanation is simply that the errors
arise because – at the very least – there can be FACT-
like sentences in any post. At the time of creation,
annotators were asked to label “the most dominant
sentiment in the whole post” (Sokolova and Bobicev,
2013, p. 636). For example, post 141143 contains
the sentence:

Also, a nurse told me her cousin, 44,
got pregnant (ivf)- the cousin lives in the
USA.

The post itself is labelled ENCO. Strictly speaking,
this sentence reports a fact, although it is easy to see
how its purpose is to encourage others.

4.3.2 Subjectivity-informed labelling
One approach to re-labelling of data is to take

advantage of coarser levels of annotation: that of
subjectivity. Is it possible to at least distinguish
which sentences are objective, and could be labelled
as FACT? We have developed a subjectivity model4

built for the SemEval 2016 Aspect Based Senti-
ment Analysis track (Pontiki et al., 2016), which

4brun-perez-roux:2016:SemEval

29



was among the top performing models for polarity
detection. We ran this model on all sentences of
the corpus in order to assess their subjectivity. Any
sentence with a subjectivity likelihood of < 0.7 we
consider to be objective; we also removed any sub-
jective sentences which were previously FACT. This
MedSenti-sent-subj set consists of 4147 sentences.
We use the same experimental settings as previously,
with results presented in Table 4.

P R F
CONF 0.315 0.169 0.220
ENCO 0.390 0.457 0.421
ENDO 0.289 0.126 0.176
GRAT 0.284 0.294 0.289
FACT 0.543 0.745 0.628
MacroAvg 0.364 0.358 0.361

Table 4: Precision, Recall and F1 for Sentence-level classifica-
tion of subjectivity-adjusted corpus

Performance is marginally better with this ap-
proach (against a majority macro averaged baseline
of F1 = 0.107). Importantly, in analysing the er-
ror matrix5 the proportion of data mis-classified has
dropped considerably (from 51% to 37%). However,
a related consequence is that the error-rate between
the subjective categories has increased.

5 Discussion

Despite the disappointing results in our sentence
level experiments, we maintain that this level of
analysis, as a step toward aspect-based understand-
ing, is important to explore further. One reason for
poor performance with both the MedSenti-sent and
MedSenti-sent-subj is the approach to annotation at
the sentence level. Naturally manual annotation of
8K sentences is considerably expensive and time
consuming. However, there are clear examples in
the data set of distinct labels being required. Con-
sider the following example, (with manually anno-
tated, illustrative labels):

post id 226470 author1 “author2
said [...] <ENCO> Thanks,I think we
were afraid of rushing into such a big de-
cision but now I feel it is most important
not to have regrets. </ENCO> <FACT>

5Not presented here for space concerns.

The yale biopsy is a biopsy of the lin-
ing of my uterus and it is a new test con-
ducted by Yale University. Here is a link
you can read: URL This test is optional
and I have to pay for it on my own... no
coverage.</FACT>”

The first statement of this post is clearly intended
to encourage the person to whom the author was re-
sponding. The second set of sentences is convey-
ing deliberately factual information about their sit-
uation. In the MedSenti set this post is labelled as
ENDO- the combination of ENCO and FACT. How-
ever, the FACT component of the post is a response
to a question in an even earlier post than the quoted
message. It could be argued therefore that these sen-
timent do not relate in the way for which the ENDO
label was created. To consider post-level labels,
then, we would argue is too coarse grained.

To explore the possible confusion introduced by
the ENDO category, particularly after removing the
objective sentences in MedSenti-sent-subj, we con-
ducted experiments with this category. In this three-
class experiment (ENCO, CONF, and GRAT), perfor-
mance was again reasonable against baseline (F1 =
0.510 over F1 = 0.213), but the error rate was
still high, particularly for GRAT. Regardless of the
linguistic feature sets, the models do not appear to
be capturing the differences between the subjective
categories. This seems contradictory to the origi-
nal authors’ intention of building “a set of senti-
ments that [...] makes feasible the use of machine
learning methods for automate sentiment detection.”
(Sokolova and Bobicev, 2013, p. 636). This is in-
teresting because, from a human reader perspective
(see Section 3), the annotation scheme makes intu-
itive sense. That the expressions of “negative” emo-
tions such as sympathy be considered in the “posi-
tive” category of ENCO aligns with the social pur-
pose behind such expressions (Pfeil and Zaphiris,
2007). Without explicitly calling attention to it,
Sokolova and Bobicev (2013) encoded social pur-
pose into their annotation scheme. As with previous
effort in the space, the scheme they have defined is
very much tuned to the emotional support domain.

In an attempt to understand potential reasons for
errors, we created a visualisation of the annotation
scheme in terms of scheme category label, higher

30



level polarity, and sentiment target, which can be
seen in Figure 1. As per the definitions of the cat-

Figure 1: Visualisation of polarity to category mapping given
affect target – one of either self, fellow forum participant, or

external entity

egories, emotions expressed towards external enti-
ties, or oneself are clearly either positive-ENCO or
negative-CONF. However, the pattern is different in
interpersonal expression between forum contribu-
tors. In the medical support environment “negative”
expressions, as previously discussed serve a posi-
tive and supportive purpose. Also, the category of
GRAT– a positive expression – is always in this situ-
ation directed to another participant. This makes the
interpersonal expression loadings both overloaded
both in terms of classification and polarity. These re-
lationships, in many ways, make machine modelling
therein overly noisy.

Of course, it is fair to say that one direction of
work in such a social domain that we did not ex-
plore is context. The original authors report subse-
quently on incorporating context into their experi-
ments: both in terms of the position within a dis-
cussion of a post (Bobicev and Sokolova, 2015) and
the posting history of an author (Sokolova and Bo-
bicev, 2015). In this work we have eschewed con-
text, though acknowledge that it is significantly im-
portant: in the ENCO-FACT sample above, for exam-
ple, context may enable a better understanding that
the ENCO sentence is in response to another ENCO
statement, while the FACT is a response to a direct
question. In this sense, there is a clear motivation to
understand document-level relationships at the sen-
tence level.

Another direction which could be explored is an
alternative annotation scheme. Prost (2012) sug-
gests an annotation scheme used to identify the shar-
ing of both practice-based and emotional support
among participants of online forums for teachers.
This annotation scheme is a combination of schemes
developed for social support forums with those cre-
ated for practice-based forums. Identifed categories
and sub-categories are described in Table 5.

Category Subcategory

Self disclosure

professional experience
personal experience
emotional expression
support request

Knowledge sharing from personal experienceConcrete info or documents
Opinion/evaluation na
Giving advice na
Giving emotional support na
Requesting clarification na

Community building

reference to community
humour
broad appreciation
direct thanks

Personal attacks na
Table 5: Categories and subcategories from support annotation
scheme of Prost (2012)

Most of the categories are relevant for both types
of forums, support and practice-based. Prost anno-
tated texts at the sub-sentence level, with these 15
categories. In order to produce the volumes of data
that would be necessary for machine-learning based
approaches to understanding support forum, this is
impractical. There is clearly a balance to be struck
between utility and practicality. However, Prost’s
scheme illustrates that in sociological circles, it is
important to consider the social context of subjec-
tive expressions: there are two categories equivalent
to GRAT here, one which is more directed, and the
other which concerns a bigger picture expression of
the value of community.

6 Conclusion

In this work we have argued two positions. De-
spite seemingly poor results at sentence-level, we
are convinced that the examples we have provided
demonstrate that document-level analysis is insuffi-
cient to accurately capture expressions of sentiment
in emotional support forums. We have also shown

31



that there are important social dimensions to this
type of domain which should also be taken into ac-
count. It is clear that there is considerable value to
be gained from automated understanding of this in-
creasing body of data; we in the Social NLP commu-
nity need to consider some more refined approaches
in order to maximise both the value itself and its fi-
delity.

References

Salah Ait-Mokhtar, Jean-Pierre Chanod, and Claude
Roux. 2001. A multi-input dependency parser. In
Proceedings of the Seventh International Workshop on
Parsing Technologies.

Tanveer Ali, David Schramm, Marina Sokolova, and Di-
ana Inkpen. 2013. Can i hear you? sentiment analysis
on medical forums. In Proceedings of the sixth inter-
national joint conference on natural language process-
ing, Asian Federation of Natural Language Process-
ing, Nagoya, Japan, October 2013, pages 667–673.

Alexandra Balahur and Ralf Steinberger. 2009. Rethink-
ing sentiment analysis in the news: from theory to
practice and back. In Proceedings of the 1st Workshop
on Opinion Mining and Sentiment Analysis, 2009.

Azy Barak and Orit Gluck-Ofri. 2007. Degree and reci-
procity of self-disclosure in online forums. Cyberpsy-
chology & Behavior, 10(3):407–417.

Victoria Bobicev and Marina Sokolova. 2015. No sen-
timent is an island - sentiment classification on medi-
cal forums. In Nathalie Japkowicz and Stan Matwin,
editors, Discovery Science - 18th International Con-
ference, DS 2015, Banff, AB, Canada, October 4-6,
2015, Proceedings, volume 9356 of Lecture Notes in
Computer Science, pages 25–32. Springer.

Sandra Bringay, Eric Kergosien, Pierre Pompidor, and
Pascal Poncelet, 2014. Identifying the Targets of the
Emotions Expressed in Health Forums, pages 85–97.
Springer Berlin Heidelberg, Berlin, Heidelberg.

Matthew S. Eastin and Robert LaRose. 2005.
Alt.support: modeling social support online. Comput-
ers in Human Behaviour, 21(6):977–992.

Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A li-
brary for large linear classification. Journal of Ma-
chine Learning Research, 9:1871–1874.

Jeffrey T. Hancock, Catalina Toma, and Nicole Ellison.
2007. The truth about lying in online dating profiles.
In Proceedings of the SIGCHI Conference on Human
Factors in Computing Systems, CHI ’07, pages 449–
452, New York, NY, USA. ACM.

Minqing Hu and Bing Liu. 2004. Mining and summariz-
ing customer reviews. In KDD, pages 168–177.

Bing Liu. 2012. Sentiment Analysis and Opinion Min-
ing. Synthesis Lectures on Human Language Tech-
nologies. Morgan & Claypool Publishers.

Saif Mohammad, Xiaodan Zhu, and Joel Martin, 2014.
Proceedings of the 5th Workshop on Computational
Approaches to Subjectivity, Sentiment and Social Me-
dia Analysis, chapter Semantic Role Labeling of Emo-
tions in Tweets, pages 32–41. Association for Compu-
tational Linguistics.

Amit Navindgi, Caroline Brun, Cécile Boulard Masson,
and Scott Nowson. 2016. Toward automatic under-
standing of the function of affective language in sup-
port groups. Computation and Language, arXiv pre-
print, http://arxiv.org/abs/1610.01910.

Nir Ofek, Cornelia Caragea, Lior Rokach, and Greta E
Greer. 2013. Improving sentiment analysis in an on-
line cancer survivor community using dynamic senti-
ment lexicon. In Social Intelligence and Technology
(SOCIETY), 2013 International Conference on, pages
109 – 113.

James W. Pennebaker, Emmanuelle Zech, and Bernard
Rim, 2001. Disclosing and sharing emotion: Psycho-
logical, social, and health consequences, pages 517–
539. American Psychological Association.

Ulrike Pfeil and Panayiotis Zaphiris. 2007. Patterns of
empathy in online communication. In Proceedings of
the SIGCHI Conference on Human Factors in Comput-
ing Systems (CHI07), pages 919–928, San Jose, CA,
USA.

Maria Pontiki, Dimitrios Galanis, John Pavlopoulos, Har-
ris Papageorgiou, Ion Androutsopoulos, and Suresh
Manandhar. 2014. Semeval-2014 task 4: Aspect
based sentiment analysis. In International Workshop
on Semantic Evaluation (SemEval).

Maria Pontiki, Dimitris Galanis, Haris Papageorgiou,
Ion Androutsopoulos, Suresh Manandhar, Mohammad
AL-Smadi, Mahmoud Al-Ayyoub, Yanyan Zhao, Bing
Qin, Orphee De Clercq, Veronique Hoste, Marianna
Apidianaki, Xavier Tannier, Natalia Loukachevitch,
Evgeniy Kotelnikov, Núria Bel, Salud Marı́a Jiménez-
Zafra, and Gülşen Eryiğit. 2016. Semeval-2016 task
5: Aspect based sentiment analysis. In Proceedings of
the 10th International Workshop on Semantic Evalu-
ation (SemEval-2016), pages 19–30, San Diego, Cal-
ifornia, June. Association for Computational Linguis-
tics.

Magali Prost. 2012. changes entre professionnels de
l’ducation sur les forums de discussion: entre sou-
tien psychologique et acquisition de connaissances sur
la pratique. Ph.D. thesis, Telecom ParisTech, Paris,
France.

32



Naomi B. Rothman and Joe C. Magee. 2016. Affective
expressions in groups and inferences about members’
relational well-being: The effects of socially engag-
ing and disengaging emotions. Cognition & Emotion,
Special Issue on Emotions in Groups, 30(1):150–166.

Marina Sokolova and Victoria Bobicev. 2013. What sen-
timents can be found in medical forums? In Galia
Angelova, Kalina Bontcheva, and Ruslan Mitkov, ed-
itors, Recent Advances in Natural Language Process-
ing, RANLP 2013, 9-11 September, 2013, Hissar, Bul-
garia, pages 633–639. RANLP 2013 Organising Com-
mittee / ACL.

Marina Sokolova and Victoria Bobicev. 2015. Learning
relationship between authors’ activity and sentiments:
A case study of online medical forums. In Galia An-
gelova, Kalina Bontcheva, and Ruslan Mitkov, edi-
tors, Recent Advances in Natural Language Process-
ing, RANLP 2015, 7-9 September, 2015, Hissar, Bul-
garia, pages 604–610. RANLP 2015 Organising Com-
mittee / ACL.

Michael Speriosu, Nikita Sudan, Sid Upadhyay, and Ja-
son Baldridge. 2011. Twitter polarity classification
with label propagation over lexical links and the fol-
lower graph. In Proceedings of the First workshop on
Unsupervised Learning in NLP, EMNLP ’11, pages
53–63.

Carlo Strapparava and Rada Mihalcea. 2007. Semeval-
2007 task 14: Affective text. In Proceedings of the
2008 ACM symposium on Applied computing, 2008.

Jennifer G. Tichon and Margaret Shapiro. 2003. The
process of sharing social support in cyberspace. Cy-
berpsychology & Behavior, 6(2):161–170.

Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proc. of HLT-EMNLP-2005.

33


