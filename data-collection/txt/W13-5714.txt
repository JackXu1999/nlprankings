




































Towards Fully Lexicalized Dependency Parsing for Korean

Jungyeul Park
UMR 6074 IRISA

Université de Rennes 1
Lannion, France

jungyeul.park@
univ-rennes1.fr

Daisuke Kawahara Sadao Kurohashi
Graduate School of Informatics

Kyoto University
Kyoto, Japan
{dk,kuro}@

i.kyoto-u.ac.jp

Key-Sun Choi
Dept. of Computer Science

KAIST
Daejeon, Korea
kschoi@
kaist.edu

Abstract

We propose a Korean dependency parsing sys-
tem that can learn the relationships between Ko-
rean words from the Treebank corpus and a
large raw corpus. We first refine the training
dataset to better represent the relationship using
a different POS tagging granularity type. We
also introduce lexical information and propose
an almost fully lexicalized probabilistic model
with case frames automatically extracted from a
very large raw corpus. We evaluate and com-
pare systems with and without POS granularity
refinement and case frames. The proposed lexi-
calized method outperforms not only the base-
line systems but also a state-of-the-art super-
vised dependency parser.

1 Introduction

Korean dependency parsing has been studied more
in comparison with constituent parsing owing to its
relatively free word order in Korean (Chung, 2004;
Lee and Lee, 2008; Oh and Cha, 2010). A depen-
dency structure is less restricted by the word order be-
cause it does not require that one constituent is fol-
lowed by another. Statistical parsing trained from
an annotated dataset has been widespread. However,
while there are manually annotated several Korean
Treebank corpora such as the Sejong Treebank cor-
pus (SJTree), only a few works on statistical Korean
parsing have been conducted. For constituent pars-
ing, (Sarkar and Han, 2002) used a very early ver-
sion of the Korean Penn Treebank (KTB) to train
lexicalized Tree Adjoining Grammars (TAG). (Chung
et al., 2010) used context-free grammars and tree-
substitution grammars trained on data from the KTB.
Most recently, (Choi et al., 2012) proposed a method

to transform the word-based SJTree into an entity-
based Korean Treebank corpus to improve the pars-
ing accuracy. For dependency parsing, (Chung, 2004)
presented a model for dependency parsing using sur-
face contextual information. (Oh and Cha, 2010) de-
veloped a parsing model with cascaded chunking by
means of conditional random fields learning. (Choi
and Palmer, 2011) used the Korean dependency Tree-
bank converted automatically from the SJTree.

In this paper, we start with an unlexicalized Ko-
rean dependency parsing system as a baseline sys-
tem that can learn the relationship between Korean
words from the Treebank corpus. Then, we try to
improve the parsing accuracy using internal and ex-
ternal resources. For internal resources, we can re-
fine the training dataset for a better representation of
the relationship by means of POS tagging granular-
ity. For external resources, we introduce lexical infor-
mation and propose a lexicalized probabilistic model
with case frames. We automatically extract predicate-
argument structures from a large raw corpus outside
of the training dataset and collect them as case frames
to improve parsing performance.

2 Dependency grammars

Converting phrase-structure grammars from the Tree-
bank corpus into dependency grammars is not a trivial
task (Wang, 2003; Gelbukh et al., 2005; Candito et
al., 2010). We implement a word-to-word conversion
algorithm for the Sejong Treebank corpus. Firstly, we
assign an anchor for nonterminal nodes using bottom-
up breadth-first search. An anchor is the terminal node
where each nonterminal node can have as a lexical
head node. We use lexical head rules described in
(Park, 2006). It assigns only the lexical head for non-
terminal nodes at the moment and finds dependencies

120



프랑스/NNP���
+의/JKG	


‘France+GEN’	

	

1 	


세계/NNG+적/XSN���
+이/VCP+ㄴ/ETM	


‘world-class’	

	

2	


의상���
/NNG 	


‘fashion’	

	

3	


디자이너���
/NNG	


‘designer’ 	

	

4	


엠마누엘���
/NNP	


‘Emmanuel’	

	

5 	


웅가로/NNP���
+가/JKS	


‘Ungaro+NOM’	

	

6 	


실내���
/NNG 	


‘interior’	

	

7	


장식/NNG���
+용/XSN 	


‘decoration’	

	

8	


디자이너���
/NNG+로/JKB 	

‘designer+AS’	


	

10	


나서/VV+었/EP���
+다/EF+./SF	


‘work as’	

	

11	


NP:3	


직물���
/NNG	


‘textile’ 	

	

9	


NP:4	
 NP:5	
 NP-SBJ:6	
 NP:7	
 NP:8	
 NP:9	
 NP-AJT:10	
NP-MOD:1	
 VNP-MOD:2	
 VP:11	


NP:8	
NP:4	


NP:9	


S:11	


NP-SBJ:6	


NP:4	


NP:4	


NP-SBJ:6	


NP-AJT:10	


VP:11	


Figure 1: Example of the original SJTree (above) and its dependency representation (below) for the example sentence
‘The world-class French fashion designer Emanuel Ungaro worked as an interior textile designer.’: The address of terminal
nodes (underneath) and the anchor of nonterminal node (on its right) are assigned using lexical head rules. The head of the
terminal node 1 is the node 4, which is the anchor of the parent of the parent node (NP:4). The head of the terminal node 4
is the node 6 where the anchor of its ancestor node is changed from itself (NP-SBJ:6). The head of the terminal node 11 is
itself where the anchor of the root node and itself are same (S:11).

would be in the next step. Lexical head rules give pri-
orities to the rightmost child node, which inherits in
general the same phrase tag. On the other hand, in the
case of VP VP for the construction of the main predi-
cate and the auxiliary verb, the leftmost child node is
exceptionally assigned as an anchor.

Then, we can find dependency relations between
terminal nodes using the anchor information. The
head is the anchor of the parent of the parent node of
the current node (For example, terminal nodes 1, 2, 3,
5 and 7 in Figure 1). If the anchor of the parent of the
parent node is the current node and if the parent of the
parent node does not have the right sibling, the head is
itself (the anchor of the root node and itself are same)
(Terminal node 11), or the head is the anchor of its
ancestor node where the anchor is changed from itself
to other node (Terminal nodes 4, 6, 8 and 10). If the
anchor of the parent of the parent node is the current
node and if the parent of the parent node has another
right sibling, the head is the anchor of the right sibling.
The last condition is for the case of an auxiliary verb
construction where the leftmost child node is assigned
as an anchor. Assigning the lexical anchor and finding
dependencies at the separated step enables arguments
for the verb to be correctly dependent on the main

verb and the main verb to be dependent on the aux-
iliary verb in the ambiguous annotation scheme in the
SJTree.1 Figure 1 shows the original SJTree phrase
structure and its corresponding converted representa-
tion in dependency grammars.

3 Parsing Model

Our parsing model gives a probability to each possible
dependency tree T for a sentence S = e1, e2, ..., en,
where ei is a Korean word. The model finally selects
the dependency tree T ∗ that maximizes P (T |S) as
follows:

T ∗ = argmax
T

P (T |S). (1)

1(Oh and Cha, 2010; Choi and Palmer, 2011) also introduced
an conversion algorithm of dependency grammars for the SJTree.
(Choi and Palmer, 2011) proposed head percolation rules for the
SJTREE. However, we found some errors such as S related rules,
where it gives lower priority to S than VP. It would fail to assign
a head node correctly for S→ VP S. Moreover, they did not con-
sider auxiliary verb constructions annotated as VP in the SJTREE.
According to their head rules, arguments for the main verb are de-
pendent on the auxiliary verb instead of the main verb because of
the annotation of the corpus (in general, VP→ VP VP where the
former VP in RHS is for the main verb and the latter VP is for the
auxiliary verb). (Oh and Cha, 2010) corrected such ambiguities as
a post-processing step (personal communication, August 2012).

121



We use the CKY algorithm to decode the dependency
trees by employing bottom-up parsing and dynamic
programming. P (T |S) is defined as the product of
probabilities as follows:

P (T |S) =
∏

Epa∈T
P (Epa, dist|eh), (2)

where Epa represents a clause dominated by a pred-
icate or a genitive nominal phrase, eh represents the
head Korean word of Epa, and dist is the distance be-
tweenEpa and eh. Instead of specifying the actual dis-
tance, it is classified into three bins: 1, 2 – 5, and 6 –.
If the dependent Korean word appears right next to the
head, the distance is 1. If it appears between 2 and 5,
the distance is 2. If it appears past 6, the distance is
6. P (T |S) is calculated in the similar way as (Kawa-
hara and Kurohashi, 2006a). We describe the outline
of this model below. Each probability in equation (2)
is decomposed into two ways according to the type
of Epa. If Epa is a clause dominated by a predicate,
it is decomposed into a predicate-argument structure
(content part) PAm and a function part fm. eh is also
decomposed into a content part ch and a function part
fh.

P (Epa, dist|eh) = P (PAm, fm, dist|ch, fh)
= P (PAm|fm, dist, ch, fh)× P (fm, dist|ch, fh)

≈ P (PAm|fm, ch)× P (fm, dist|fh) (3)

The first term in the last equation represents a fully-
lexicalized generative probability of the predicate-
argument structure. This probability is calculated
based on automatically compiled case frames in the
same way as (Kawahara and Kurohashi, 2006a). The
second term of the last equation is a generative prob-
ability of function morphemes, in which fm and fh
are defined as the POS patterns of morphemes for the
predicate of Epa and the head Korean word eh, re-
spectively. This probability is estimated from training
data using maximum likelihood estimation. IfEpa is a
genitive nominal phrase, it consists of a Korean word
that is decomposed into cm and fm. Its probability is
defined similarly as follows:

P (Epa, dist|eh) = P (cm, fm, dist|ch, fh).
= P (cm|fm, dist, ch, fh)× P (fm, dist|ch, fh).

≈ P (cm|ch)× P (fm, dist|fh). (4)

The first term in the last equation represents a fully-
lexicalized generative probability of the genitive nom-
inal phrase. This probability is calculated from the
constructed database of N1 ui N2 (N2 of N1) struc-
tures. The second term is the same as the second
term in equation (3). In our experiments, we use an
unlexicalized parsing model as a baseline. This un-
lexicalized model regards the above lexicalized prob-
abilities as uniform and actually calculates the prod-
uct of generative probabilities of function morphemes,
P (fm, dist|fh).

4 POS Sequence Granularity

Given that Korean is an agglutinative language, a
combination of Korean words is very productive and
exponential. Actually, a larger dataset would not al-
leviate this issue. The number of POS patterns would
not converge even with a corpus of 10 million words
in the Sejong morphologically analyzed corpus. The
wide range of POS patterns in words is mainly due to
the fine-grained morphological analysis results, where
they show all possible segmentations divided into lex-
ical and functional morphemes. For example, most
Korean language resources to represent Korean mor-
phological analyses including the SJTree would an-
alyze the word kimkyosunim (‘Professor Kim+HON’)
as kim/NNP + kyosu/NNG + nim/XSN (‘Kim + pro-
fessor + Hon’). Instead of keeping the fine-grained
morphological analysis results, we simplify POS se-
quences as much as possible using the linguistically
motivated method. It would be helpful if we can refine
the dataset for a better representation of the relation-
ship. We introduce four level POS granularity: PUNC,
MERG, CONT and FUNC.

PUNC: Punctuation marks (denoted as SF for pe-
riods, question marks and exclamation points, SP
for commas and SE for ellipsis) and non-punctuation
marks (for example, a period in the number is equally
denoted as SF such as 3/SN + ./SF + 14/SN) are distin-
guished. Recurrent punctuation marks such as .../SE
+ .../SE in the word are also merged into a single sym-
bol.

MERG: Special characters such as mathematical
characters denoted as SW are merged into an entity
with adjacent morphemes. Other non-Korean charac-
ters such as SL (Roman letters), SH (Chinese char-

122



acters) and SN (cardinal numbers) are either merged
into an entity with adjacent morphemes or are consid-
ered as nouns when they appear alone. Secondly, all
suffixes are merged with adjacent morphemes. Func-
tional morpheme-related refining rules are described
as follows with the number of occurrences in the
SJTree.

The nominal prefix (XPN) and suffix (XSN) with
adjacent morphemes are merged into the POS of the
corresponding morphemes (17,955 cases). The noun
derivational suffix (ETN) with precedent morphemes
is merged into the noun (5,186 cases). The non-
autonomous lexical root (XR) is merged into the fol-
lowing POS (5,322 cases). The verb and adjective
derivational suffix (XSV and XSA) with precedent
morphemes are merged into the verb and adjective
(20,178 and 9,096 cases, respectively). The adjective
and the adverbial derivation gey/EC are merged into
the adverb (2,643 cases). Refinement rules are applied
recursively to all POS tags until there are no rules to
apply. For example, soljk/XR + ha/XSA + gey/EC
is applied both according to the XR rule and the ad-
verbial derivation rule to become soljikhagey/MAG
(‘frankly’).

CONT: All content morphemes in the word are
merged together. For example, the sequence of the
different type of nouns in a word is merged as a sin-
gle noun with the priority of proper noun (NNP) >
common noun (NNG) > dependent noun (NNB). For
example, the sequence of NNP and NNG such as
masan/NNP + yek/NNG (‘Masan station’), is merged
into an NNP. The sequence of the different type of
verbs in a word is also merged as a single verb. The
difference between MERG and CONT is the nature of
merged morphemes. MERG concerns about merging
functional morphemes and CONT about merging lexi-
cal morphemes.

FUNC: All functional morphemes are merged
together. For example, eoss/EP (PAST) + da/EF
(‘DECL’) is merged into a single verbal ending
eossda/EF.

5 Exploiting Lexical Information

This section aims at exploiting lexical information and
proposes a lexicalized probabilistic model with case
frames aggregated from predicate-argument structures
and the database of N of N structures to improve the

parsing system.

5.1 Constructing case frames

It is difficult to make wide-coverage predicate-
argument structures manually. Therefore, it is neces-
sary to compile them automatically from a large cor-
pus for our purpose. We introduce two methods us-
ing POS patterns and parsed corpora to extract case
frames automatically from a raw corpus. We then
apply clustering to the extracted predicate-argument
structures to produce case frames.

Firstly, we use POS patterns to select predicate-
argument structures after automatically assigning
POS tags to a raw corpus. The key criteria for deter-
mining the predicate-argument structures are the ap-
pearance of the final or conjunctive verbal endings
(denoted as EC and EF, respectively). Using func-
tional morphemes, we are able to detect the end of
predicate-argument structures in the sentence. In Fig-
ure 1, we can find two case markers agglutinated
to NPs for the predicate naseo+ss+da: -ga and -
ro for nominative and adverbial case markers (JKS
and JKB). Therefore, we can select the predicate-
argument structure composed of ungaro+ga (‘Un-
garo+NOM’) and designer+ro (‘designer+AS’) as ar-
guments for the verb naseo (‘work as’). Our algo-
rithm for selecting predicate-argument structures us-
ing POS patterns is described below. All arguments
with case markers except JKG (genitive) and JC (con-
nective postpositions) are extracted as a predicate-
argument structure. JX (auxiliary postpositions) are
not extracted because they can be interpreted either
nominative or accusative and it becomes ambiguous.

var pa
while wi in the sentence do

if wi ends with case markers then
pa += wi;

else if wi contains final or conjunctive verbal
endings && pa is not NULL then

print and initialize pa;
else if wi contains other verbal endings then

initialize pa;
else

do nothing;
end

end

Secondly, to use parsed corpora, we employ
the method proposed in (Kawahara and Kurohashi,

123



2006b) and re-implement it to extract case frames. A
large corpus is automatically parsed and case frames
are constructed from modifier-head examples in the
resulting parsed corpus. Then we extract dependency
lists depending on their head as follows. Dependency
lists consist of modifier1 ... modifiern head where
n >= 1. Then, we select dependency lists only if the
head is a predicate such as unggaroga 6 dijaineoro 10
naseoeossda 11.

Thereafter, predicate-argument structures are clus-
tered to merge similar ones, as described in (Kawa-
hara, 2005). We distinguish predicate-argument struc-
tures by the predicate and its closest case instance
to the predicate as described in Figure 22: In order
to merge predicate-argument structures we introduce
similarities between two structures calculated by the
production of the similarities between arguments and
the ratio of common instances.

Similartycase frames = simcf · ratioci (5)

We use the semantic hierarchy of nouns from Ko-
rean CoreNet (Choi, 2003) for the similarities be-
tween two instances of arguments. CoreNet is com-
posed of 2,937 concepts represented by kortermnum
(knum). A cipher of knum tells a hierarchy depth. For
instance, COUNTRY (knum: 11125) has ORGANIZA-
TION (1112) as a parent concept (hypernym). hanguk
(11125, ‘Korea) and namhan (11125, ‘South Korea)
share COUNTRY (11125) as a concept. Therefore,
similarity between two instances is obtained as fol-
lows. common is the shared length of knum for i1
and i23:

siminst =
lenknum(common ∗ 2)

lenknum(i1) + lenknum(i2)
(6)

Then, we calculate similarities between arguments
of the same case marker in two predicate-argument
structure as follows:

simarg =

∑
x=1

∑
y=1 siminst ·

√
|ex||ey|

∑
x=1

∑
y=1

√
|ex||ey|

(7)

where ex and ey are the number of the occurrences of
the instance example e of the same case maker. The
ratio of common instances is calculated as follows:

2{inbu3} (‘worker’) means that the instance inbu has 3 occur-
rences.

3common = 0 if either i1 or i2 is not included in CoreNet.

ratioci =

∑
i=1

√
|ex|∑

j=1

√
|ey|

(8)

where i is the number of the occurrences of the in-
stance examples of the same case marker and j is the
number of the occurrences of the instance examples
of the all case marker.

5.2 Constructing the database of N1 ui N2
structures

We also integrate lexical information on Korean noun
phrases of the form N1 ui N2, which roughly corre-
sponds to N2 of N1 in English. Even though Ko-
rean genitive marker ui does not have a broad usage
as much as no in Japanese as described in (Kuro-
hashi and Sakai, 1999), it sometime does not modify
the immediate constituent such as Kyungjiui meylon-
hyang binwuleul (‘Melon-flavored soap of Kyungji’)
where Kyungjiui modifies binwuleul instead of mey-
lonhyang. The N1 ui N2 structure is very useful to
recognize the meaning of natural language text can
improve head-modifier relationships between genitive
nouns.

6 Experiment and Results

6.1 Parsing results

We use the Sejong Treebank corpus (SJTree) in our
experiment.4 We use standard dataset split for train-
ing, development and testing. We report here final
evaluation results on the baseline unlexicalized pars-
ing and different POS granularities. We crawl news
articles published in 2007 from the website of Chosun
Ilbo5 (literally, ‘Korea Daily News’), which is one of
the major newspapers in Korea to integrate lexical in-
formation. We collect 212,401 pages and extract Ko-
rean sentences. We acquire a raw corpus with over
three million sentences. Then, we use the Espresso
POS Tagger and Dependency Parser for Korean to as-
sign POS and parse sentences to extract POS patterned
and parsed case frames.6 We extract the database of

4Differently from other Korean Treebank corpora, the SJTree
contains non-sentences such as noun phrases. We select only
complete sentences. We also remove erroneous sentences in the
SJTree using heuristics such as non-defined POS tags and not-
well-formed morpheme and POS tag pairs.

5http://www.chosun.com
6http://air.changwon.ac.kr/research/

software

124



CF1: {inbu3}:i {cha2,teuleok1}:ey {gabang5}:eul silneunda.
{worker}:NOM {car,truck}:LOC {bag}:ACC load

CF2: {teuleok2}:ey {jim3}:eul silneunda.
{truck}:LOC {baggage}:ACC load

Figure 2: Predicate-argument structures distinguished by the predicate and its closest case instance

UAS
Baseline system 71.735%

PUNC 73.714%
POS granularity MERG 76.993%

CONT 81.515%
FUNC 82.702%

Table 1: Evaluation results on unlexicalized parsing

UAS
Lexical CF-parsed + NoN 86.037%

information CF-pos + NoN 86.433%

Table 2: Evaluation results on lexicalized parsing with
FUNC

N1 ui N2 structures from the same corpus. During
building case frame structures, we ignore JX postpo-
sitions (mostly for topic markers) which can be inter-
preted as either NOM or ACC. Instead, we explicitly
specify this ambiguity in the input to let the parser
consider both cases to select the correct one. For
case frame structures extracted without the subject,
we intentionally insert the dummy subject to represent
the complete construction of the sentence without any
missing constituents.

6.2 Discussion

The basic parsing model is directly based on the POS
patterns of words. If some sentences have POS pat-
terns that are not seen in the training dataset, our base-
line system cannot handle them. By introducing POS
sequence granularity we can increase recall and even-
tually it makes the dataset more parsable with less un-
trained POS sequences. Integrating lexical informa-
tion is prominent. We can increase precision and it
can fix many predicate-argument dependency errors
in unlexicalized parsing. Results with case frames
extracted from the automatically parsed corpus are
slightly lower than results with POS patterned case
frames because the nature of the corpus. The automat-
ically parsed corpus contains inevitably much more
errors than the POS tagged corpus. Moreover, the sim-

pler method using POS patterns can guarantee less er-
rors contained case frames. Filtering out erroneously
parsed sentences and building case frame structures
only using reliable sentences would yield better re-
sults.

Only small numbers of research projects about sta-
tistical parsing have been conducted using the same
Treebank corpus. (Oh and Cha, 2010; Choi and
Palmer, 2011) used the early version of the Sejong
Treebank and obtained up to 86.01% F1 score and
85.47% UAS, respectively. (Choi et al., 2012) ob-
tained 78.74% F1 score for phrase structure parsing.
Our current results outperform previous work. We
also test MaltParser7 on the same dataset and we ob-
tain 85.41% for UAS. It still shows the better perfor-
mance of our proposed method. The advantage of our
proposed system is the capability of adding lexical-
ized information from external corpora.

7 Conclusion

In this paper, we improved Korean dependency pars-
ing accuracy using various factors, including POS
granularity changes and lexical information. We re-
fined the training dataset for a better representation
of the relationship between words. We also intro-
duced the use of lexical information. The accuracy
was improved and it shows promising factors. The
lexical knowledge extracted from a much bigger cor-
pus would be interesting to pursue when seeking fur-
ther improvement opportunities pertaining to the deep
processing of Korean sentences.

Acknowledgments

This work was supported by the Industrial Tech-
nology International Cooperation Program (FT-1102,
Creating Knowledge out of Interlinked Data) of
MKE/KIAT, and the IT R&D program of MSIP/KEIT
(10044494, WiseKB: Big data based self-evolving
knowledge base and reasoning platform).

7http://www.maltparser.org

125



References

Marie Candito, Benoı̂t Crabbé, and Pascal Denis. 2010.
Statistical French Dependency Parsing: Treebank Con-
version and First Results. In Nicoletta Calzolari (Con-
ference Chair), Khalid Choukri, Bente Maegaard,
Joseph Mariani, Jan Odijk, Stelios Piperidis, Mike Ros-
ner, and Daniel Tapias, editors, Proceedings of the Sev-
enth conference on International Language Resources
and Evaluation (LREC’10), Valletta, Malta, May. Euro-
pean Language Resources Association (ELRA).

Jinho D. Choi and Martha Palmer. 2011. Statistical
Dependency Parsing in Korean: From Corpus Genera-
tion To Automatic Parsing. In Proceedings of the Sec-
ond Workshop on Statistical Parsing of Morphologically
Rich Languages, pages 1–11, Dublin, Ireland, October.
Association for Computational Linguistics.

DongHyun Choi, Jungyeul Park, and Key-Sun Choi. 2012.
Korean Treebank Transformation for Parser Training. In
Proceedings of the ACL 2012 Joint Workshop on Statis-
tical Parsing and Semantic Processing of Morphologi-
cally Rich Languages, pages 78–88, Jeju, Republic of
Korea, July 12. Association for Computational Linguis-
tics.

Key-Sun Choi. 2003. CoreNet: Chinese-Japanese-Korean
WordNet with Shared Semantic Hierarchy. In Proceed-
ings of Natural Language Processing and Knowledge
Engineering, pages 767–770, 26-29 October, 2003.

Tagyoung Chung, Matt Post, and Daniel Gildea. 2010.
Factors Affecting the Accuracy of Korean Parsing. In
Proceedings of the NAACL HLT 2010 First Workshop on
Statistical Parsing of Morphologically-Rich Languages,
pages 49–57, Los Angeles, CA, USA, June. Association
for Computational Linguistics.

Hoojung Chung. 2004. Statistical Korean Dependency
Parsing Model based on the Surface Contextual Infor-
mation. Ph.D. thesis, Korea University.

Alexander Gelbukh, Sulema Torres, and Hiram Calvo.
2005. Transforming a Constituency Treebank into a De-
pendency Treebank. Procesamiento del Lenguaje Natu-
ral, 35:145–152.

Daisuke Kawahara and Sadao Kurohashi. 2006a. A Fully-
Lexicalized Probabilistic Model for Japanese Syntactic
and Case Structure Analysis. In Proceedings of the Hu-
man Language Technology Conference of the NAACL,
Main Conference, pages 176–183, New York City, USA,
June. Association for Computational Linguistics.

Daisuke Kawahara and Sadao Kurohashi. 2006b.
Case Frame Compilation from the Web using High-
Performance Computing. In Proceedings of the 5th

International Conference on Language Resources and
Evaluation (LREC2006), pages 1344–1347.

Daisuke Kawahara. 2005. Automatic Construction of
Japanese Case Frames for Natural Langage Under-
standing. Ph.D. thesis, Kyoto University, July.

Sadao Kurohashi and Yasuyuki Sakai. 1999. Semantic
Analysis of Japanese Noun Phrases - A New Approach
to Dictionary-Based Understanding. In Proceedings of
the 37th Annual Meeting of the Association for Com-
putational Linguistics, pages 481–488, College Park,
Maryland, USA, June. Association for Computational
Linguistics.

Yong-Hun Lee and Jong-Hyeok Lee. 2008. Korean Pars-
ing using Machine Learning Techniques. In Proceed-
ings of the Korea Computer Congress (KCC) 2008,
pages 285–288, Jeju, Korea.

Jin-Young Oh and Jeong-Won Cha. 2010. High Speed Ko-
rean Dependency Analysis Using Cascaded Chunking.
Simulation Journal, 19(1):103–111.

Jungyeul Park. 2006. Extraction automatique d’une gram-
maire d’arbres adjoints à partir d’un corpus arboré
pour le coréen. Ph.D. thesis, Université Paris 7 - De-
nis Diderot, mai.

Anoop Sarkar and Chung-Hye Han. 2002. Statistical Mor-
phological Tagging and Parsing of Korean with an LTAG
Grammar. In Proceedings of the 6th International Work-
shop on Tree Adjoining Grammars and Related For-
malisms (TAG+ 6), pages 48–56, Venice, Italy.

Wen Wang. 2003. Statistical Parsing and Language Mod-
eling based on Constraint Dependency Grammar. Ph.D.
thesis, Purdue University, December.

126


