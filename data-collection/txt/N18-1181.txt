



















































Comparing Theories of Speaker Choice Using a Model of Classifier Production in Mandarin Chinese


Proceedings of NAACL-HLT 2018, pages 1997–2005
New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics

Comparing Theories of Speaker Choice Using a Model of Classifier
Production in Mandarin Chinese

Meilin Zhan
Massachusetts Institute of Technology

77 Massachusetts Avenue
Cambridge, MA 02139
meilinz@mit.edu

Roger Levy
Massachusetts Institute of Technology

77 Massachusetts Avenue
Cambridge, MA 02139
rplevy@mit.edu

Abstract

Speakers often have more than one way to ex-
press the same meaning. What general princi-
ples govern speaker choice in the face of op-
tionality when near semantically invariant al-
ternation exists? Studies have shown that op-
tional reduction in language is sensitive to con-
textual predictability, such that the more pre-
dictable a linguistic unit is, the more likely it
is to get reduced. Yet it is unclear to what ex-
tent these cases of speaker choice are driven
by audience design versus toward facilitating
production.

Here we argue that for a different option-
ality phenomenon, namely classifier choice
in Mandarin Chinese, Uniform Information
Density and at least one plausible variant of
availability-based production make opposite
predictions regarding the relationship between
the predictability of the upcoming material and
speaker choices. In a corpus analysis of Man-
darin Chinese, we show that the distribution of
speaker choices supports the availability-based
production account, and not Uniform Informa-
tion Density.

1 Introduction

The expressivity of natural language often gives
speakers multiple ways to convey the same mean-
ing. Meanwhile, linguistic communication takes
place in the face of environmental and cognitive
constraints. For instance, language users have lim-
ited memory and cognitive resources, the environ-
ment is noisy, and so forth. What general princi-
ples govern speaker choice in the face of alterna-
tions that are (nearly) semantically invariant? To
the extent that we are able to provide a general
answer to this question it will advance our funda-
mental knowledge of human language production.

Studies have shown that alternations are very
often sensitive to contextual predictability. For

well-studied cases of optional REDUCTION in lan-
guage, the following trend is widespread: the more
predictable a linguistic unit is, the more likely it is
to get reduced. Predictable words are phonetically
reduced (Jurafsky et al., 2001; Bell et al., 2009;
Seyfarth, 2014) and have shorter lexical forms (Pi-
antadosi et al., 2011), and optional function words
are more likely to be omitted when the phrase
they introduce is predictable (Jaeger, 2010). Yet
it is unclear to what extent speakers’ choices when
faced with an alternation are made due to audi-
ence design or to facilitate production. For exam-
ple, the above pattern of predictability sensitivity
in optional reduction phenomena is predicted by
both the Uniform Information Density (UID) hy-
pothesis (Levy and Jaeger, 2007), a theory which
that the speaker aims to convey information at a
relatively constant rate and which can be moti-
vated via considerations of optimality from the
comprehender’s perspective (e.g., Smith and Levy,
2013), and by the speaker-centric availability-
based production hypothesis (Bock, 1987; Ferreira
and Dell, 2000), which hypothesizes that the dom-
inant factor in determining speaker choice is that
the speaker uses whatever material is readily avail-
able when it comes time to convey a particular part
of a planned message.

Here we argue that for a different optionality
phenomenon, namely classifier choice in Man-
darin Chinese, UID and availability-based produc-
tion make opposite predictions regarding the re-
lationship between the predictability of upcoming
material and speaker choice. In a corpus analysis
of Mandarin Chinese, we show that the distribu-
tion of speaker choices supports the availability-
based production account, and not UID.

1997



2 Uniform Information Density and
Availability-based Production

In Sections 2 and 3, we explain why the UID and
availability-based production accounts make the
same predictions in many cases, but can be poten-
tially disentangled using Chinese classifier choice.
Here we exemplify predictions of these two ac-
counts in the case of optional function word omis-
sion.

For optional function word omission such as
that-omission ((1) and (2)), predictability effects
have been argued to be consistent with both
the speaker-oriented account of AVAILABILITY-
BASED PRODUCTION (Bock, 1987; Ferreira and
Dell, 2000) and the potentially audience-oriented
account of UNIFORM INFORMATION DENSITY
(Levy and Jaeger, 2007). On both accounts, but for
different reasons, the less predictable the clause in-
troduced by the functional word, the more likely
the speaker will be to produce the function word
that.

(1) The student (that) you tutored graduated.

(2) The woman thought (that) we were crazy.

The UID hypothesis claims that within bound-
aries defined by grammar, when multiple options
are available to encode a message, speakers pre-
fer the variant that distributes information density
most uniformly, thus lowering the chance of in-
formation loss or miscommunication (Levy and
Jaeger, 2007; Jaeger, 2010). In (1), if the func-
tion word that is omitted, the first word of the
relative clause you serves two purposes: signal-
ing the onset of the relative clause, and convey-
ing part of the contents of the relative clause itself.
These both contribute to the information content
of the first relative clause-internal word. If one or
both is high-surprisal, then the first relative clause-
internal word might be a peak in information den-
sity, as illustrated in Figure 1 (top left). If instead
the function word that is produced, that signals the
onset of the relative clause, and you only commu-
nicates part of the content of the relative clause
itself. This could help eliminate any sharp peak in
information density, as illustrated in Figure 1 (bot-
tom left). Thus, if the speaker’s goal is to transfer
information as smoothly as possible, the less pre-
dictable the upcoming clause, the more inclined
the speaker would be to produce the function word
that.

On the availability-based production hypothe-
sis, speaker choice is governed by the relationship
by the relative time-courses of (i) when a part of
a message needs to be expressed within an utter-
ance, and (ii) when the linguistic material to en-
code that part of the message becomes available
for production. If material that specifically en-
codes a part of the message is available when it
comes time to convey that part of the message,
it will be used—that is the PRINCIPLE OF IMME-
DIATE MENTION of Ferreira and Dell (2000). If,
on the other hand, that material is not yet avail-
able, then other available material consistent with
the grammatical context produced thus far and that
does not cut off the speaker’s future path to con-
veying the desired content will be used. In (1),
assuming the function word that is always avail-
able when the speaker plans to produce a relative
clause, the speaker will produce that when the up-
coming relative clause or the first part of its con-
tents are not yet available. If phrase structures
and phrase contents take longer to become avail-
able when they are lower-predictability—an as-
sumption consistent with the literatures on picture
naming (Oldfield and Wingfield, 1965) and word
naming (Balota and Chumbley, 1985)—then the
less predictable the relative clause, the lower the
probability that its first word, w1, will be avail-
able when the time comes to begin the relative
clause, as illustrated in Figure 2 (left). Under
these circumstances, the speaker would choose to
produce other available material, namely function
word that. If, in contrast, the upcoming relative
clause is predictable, then w1 will be more likely
to be available, and the speaker would be more
likely to omit the function word that and imme-
diately proceed with w1.

While these two accounts differ at many levels,
they make the same prediction for function word
omission in syntactic reduction such as (1) and (2).
It is difficult to disentangle these accounts empir-
ically.1 Below we will show that for a different
optionality phenomenon, namely classifier choice
in Mandarin, these accounts may make different
predictions.

1Prior work (Jaeger, 2010) acknowledged this entangle-
ment of the predictions of these accounts, and attempted to
tease the accounts apart via joint modeling using logistic re-
gression. The present study builds on these efforts by ex-
ploring a case involving a starker disentanglement of the ac-
counts’ predictions.

1998



●

●

●

●

the student you tutored

time t

In
fo

rm
at

io
n 

de
ns

ity
 

 in
 b

its
/w

or
d

●

●

●

three CL.general table

time t

In
fo

rm
at

io
n 

de
ns

ity
 

 in
 b

its
/w

or
d

●

●

●

● ●

the student that you tutored

time t

In
fo

rm
at

io
n 

de
ns

ity
 

 in
 b

its
/w

or
d

●

●

●

three CL.flat table

time t

In
fo

rm
at

io
n 

de
ns

ity
 

 in
 b

its
/w

or
d

Figure 1: Schematic illustrations of Uniform Information Density in the context of relative clause (left) and classi-
fier choice (right). The grey lines indicate a hypothetical channel capacity.

3 Classifiers in Mandarin Chinese

Languages in the world can be broadly grouped
into classifier languages and non-classifier lan-
guages. In non-classifier languages, such as En-
glish and other Indo-European languages, a nu-
meral modifies a noun directly: e.g., three tables,
two projects. In Mandarin Chinese and other clas-
sifier languages, a numeral classifier is obligatory
when a noun is to be preceded with a numeral
(and often obligatory with demonstratives): e.g.,
san zhang zhuozi “three CL.flat table”, liang xiang
gongcheng “two CL.item project”. Although it has
been hypothesized that numeral classifiers play a
functional role analogous to that of the singular–
plural distinction in other languages (Greenberg,
1972), it is not clear whether there is a meaningful
correlation between the presence of numeral clas-
sifiers and plurality among the languages of the
world (Dryer and Haspelmath, 2013).

In Mandarin, classifiers, together with their as-
sociated numeral or demonstrative, precede the
head noun of a noun phrase. There are about 100
individual numeral classifiers (Ma, 2015). While
different nouns are compatible with different SPE-
CIFIC classifiers, there is a GENERAL classifier
ge(个) that can be used with most nouns. In some
cases, the alternating options between using a gen-
eral or a specific classifier with the same noun are
almost semantically invariant. Table 1 shows ex-
amples of classifier options in fragments of natu-
rally occuring texts.

Yet these options have different effects on the

information densities of the following nouns. A
specific classifier is more likely to reduce the in-
formation density of the upcoming noun than a
general classifier because a specific classifier con-
strains the space of possible upcoming nouns more
tightly (Klein et al., 2012). Consider the following
pair of classifier examples (3) and (4).

(3) 我
wo
买了
mai-le

三
san
张
zhang

桌子
zhuozi

I bought three CL.flat table (“I bought three tables”)

(4) 我
wo
买了
mai-le

三
san
个
ge
桌子
zhuozi

I bought three CL.general table (“I bought three ta-
bles”)

As shown in Figure 1 (top right), while a general
classifier has some information (e.g., signaling
there will be a noun), it has relatively low infor-
mation density—it is the most frequent and gen-
erally the highest-probability classifier in many
contexts. In comparison, as illustrated in Figure
1 (bottom right), a specific classifier has higher
information density—specific classifiers are less
frequent than the general classifier and typically
lower-predictability—but, crucially, it constrains
the hypothesis space for the identity of the upcom-
ing noun, since the noun’s referent must meet cer-
tain semantic requirement that the classifier is as-
sociated with. The UID hypothesis predicts that
speakers choose a specific classifier more often
when the predictability of the noun would other-

1999



the student that you tutored ...

the student you tutored ...

0.00

0.25

0.50

0.75

1.00

RC onset at time t

P
ro

ba
bi

lit
y 

of
 w

1 
of

 R
C

 is
 r

ea
dy

 a
t t

im
e 

t Relative Clause

Predictable

Unpredictable

three CL.general table

three CL.flat table

0.00

0.25

0.50

0.75

1.00

CL onset at time t

P
ro

ba
bi

lit
y 

of
 n

ou
n 

le
am

a 
&

 s
pe

ci
fic

 C
L 

is
 a

cc
es

si
bl

e

Noun

Predictable

Unpredictable

Figure 2: Schematic illustrations of availability-based production in the context of relative clause (left) and classi-
fier choice (right). X axis presents the progression of time. The dotted lines indicate onset times for relative clause
and classifier respectively.

wise be low.
Availability-based production, provided three

plausible assumptions, makes different predictions
than UID. The first assumption is that a speaker
must access a noun lemma in order to access its ap-
propriate specific classifier. The second assump-
tion is that unpredictable noun lemmas are harder
and/or slower to access (as described in Section 2,
this assumption is supported by findings from the
naming literature). The third assumption is that
the general classifier is always available, regard-
less of the identity of the upcoming noun, as it
is compatible with virtually every noun. Under
these assumptions, for unpredictable nouns, spe-
cific classifiers will less often be available to the
speaker when the time comes to initiate production
of classifier, as shown in Figure 2 (right). Since
noun lemmas need to be accessed before their as-
sociated specific classifiers, the less predictable
the noun, the less likely the noun lemma and hence
the associated specific classifier is to be available
by the classifier onset time t. The general classi-
fier, in contrast, is always accessible. Under these
assumptions, the availability-based production hy-
pothesis thus predicts that speakers choose a gen-
eral classifier more often when the following noun
is less predictable.

4 Data and Processing

To provide data for this study, we created a corpus
of naturally occurring classifier-noun pairs from
SogouCS, a collection of online news texts from

various channels of Sohu News (Sogou, 2008).
The deduplicated version of the corpus (see Sec-
tion 4.1 for deduplication details) has 11,548,866
sentences. To parse and annotate the data, we
built a pipeline to 1) clean and deduplicate the
data, 2) part-of-speech tag and syntactically parse
the clean text, and 3) extract and filter classifier-
noun pairs from the parsed text. We are aware
that a spoken corpus would be ideal to investigate
speaker choice, but nothing this big is available.
Instead we used SogouCS to approximate the lan-
guage use of native speakers.

4.1 Cleaning and deduplication

Since the data contain web pages, many snippets
are not meaningful content but automatically gen-
erated text such as legal notices. To use this cor-
pus as a reasonable approximation of language ex-
perience of speakers, we performed deduplication
on the data, following similar practice adopted by
other work dealing with web-based corpora (Buck
et al., 2014). After cleaning the text, we removed
repeated lines in the corpus.

4.2 Word segmentation, POS-tagging and
syntactic parsing

We used the Stanford CoreNLP toolkit for word
segmentation, part-of-speech tagging, and syn-
tactic parsing (Manning et al., 2014). We used
CoreNLP’s Shift-Reduce model for parsing (Zhu
et al., 2013). We also got dependency parsing re-
sults as part of the Stanford CoreNLP output.

2000



Noun 个 (ge, CL.general) 项 (xiang, CL.item) 张 (zhang, CL.flat)

公告 一口气发布 11个公告 连续发布三项公告 门口贴了一张公告

announ- a CL breath release 11 CL consecutively release three CL door paste a CL announcement

cement announcement announcement

“release 11 announcements at one “release three announcements in a” “there is an announcement on the door”

go” row”

账单 女儿拿着一个账单就过来了 在一张账单上解决所有收费问题

bill daughter carry a CL bill at once not co-occurring on a CL bill solve all charge problem

come

“daughter came with a bill at once” “solve all charge problems on a bill”

工程 跟圆明园有关的一个工程 抓好六项重点工程

project to Yuanmingyuan related de a CL grasp six CL key project not co-occurring

project

“a project related to Yuanmingyuan” “manage six key projects”

活动 昨天我参加了一个活动 广州市今天开展的一项活动

activity yesterday I attend a CL activity Guangzhou today hold de a CL not co-occurring

activity

“yesterday I attended an activity” “an activity held by Guangzhou today”

Table 1: Examples from development set of available classifier options that are semantically (near-)invariant

4.3 Extracting and filtering classifier-noun
pairs

From the parsed corpus, we extracted all obser-
vations where the head noun has a nummod re-
lation with a numeral and the numeral has a
mark:clf relation with a classifier. Figure 3 illus-
trates two such examples. We included classifiers
in the list of 105 individual classifiers identified
by Ma (2015) that are identified by the Stanford
CoreNLP toolkit. For the purpose of restricting
our data to cases of (nearly) semantically invariant
alternation, we excluded classifiers such as zhong
(“CL.kind”) that would introduce a clear truth-
conditional change in utterance meaning, com-
pared with the general classifier ge. We did fur-
ther filtering to get nouns that can be used with
both the general classifier and at least one specific
classifier. This left us 1,479,579 observations of
classifier-noun pairs.

To construct the development set, we randomly
sampled about 10% of the noun types (1,179)
and extracted all observations with of these noun
types. We manually checked and filtered applica-
ble classifiers for these noun types and we ended
up with 713 noun types for the development set.
For the test set, we also randomly sampled about
10% of the noun types (1,093) and extracted all
observations with these noun types. We did not
perform manual filtering of the test set. We reserve
the remaining 80% for future work.

三 张 桌子
san zhang zhuozi

three CL table

nummod

mark:clf

六 项 重点 工程
san xiang zhongdian gongcheng
six CL key project

nummod

mark:clf nmod

Figure 3: Classifier examples where the head noun has
a nummod relation with a numeral and the numeral has
a mark:clf relation with the classifier

5 Model estimation

We use SURPRISAL, the negative log probabil-
ity of the word in the context (Hale, 2001; Levy,
2008; Demberg and Keller, 2008; Frank and Bod,
2011; Smith and Levy, 2013), generated from a
language model to estimate noun predictability.
Since classifiers occur before their corresponding
nouns, to avoid circularity, we mapped all tar-
get classifiers to the same token, CL, in the seg-
mented text for language modeling, analogous to
the procedure used in (Levy and Jaeger, 2007) and
similar studies. We implemented 5gram modified
Kneser-Ney smoothed models with the SRI Lan-

2001



guage Modeling toolkit (Stolcke, 2002) and per-
formed ten-fold cross-validation to estimate noun
surprisal.

We used a mixed-effect logit model to inves-
tigate the relationship between noun predictabil-
ity and classifier choice. The dependent variable
was the binary outcome of whether a general or a
specific classifier was used. For each noun type,
we also identified its most frequently used spe-
cific classifier. We included two predictors in the
analysis: noun surprisal and noun log frequency.2

We included noun frequency as a control factor for
two reasons. First, noun frequency has shown ef-
fects on many aspects of speaker behavior. Sec-
ond, surprisal and frequency of a word are intrinsi-
cally correlated. Taken together, these two reasons
make noun frequency an important potential con-
found to be controlled for in investigating any po-
tential effect of noun surprisal on classifier choice.

We included noun and potential specific clas-
sifier as random factors, both with random inter-
cepts and random slopes for noun surprisal. This
random effect structure is maximal with regard
to testing effects of noun surprisal, which varies
within noun and within classifier (Barr et al.,
2013). We then applied the model to the test set.
The full formula in the style of R’s lme4 package
(Bates et al., 2014) is:

cl_choice˜noun_surprisal+log_noun_freq

+(1+noun_surprisal|noun)

+(1+noun_surprisal|potential_spec_cl)

We used Markov chain Monte Carlo (MCMC)
methods in the R package MCMCglmm (Hadfield
et al., 2010) for significance testing, an based our
p-values on the posterior distribution of regression
model parameters using an uninformative prior
and determining the largest possible symmetric
posterior confidence interval on one side of zero,
as is common for MCMC-based mixed model fit-
ting (Baayen et al., 2008).

6 Results

In both the development set and the test set, overall
we saw more observations with a specific classifier
than with a general classifier (55.4% vs. 44.6%
in the development set, 63.1% vs. 36.9% in the
test set). For the development set, we find that the
less predictable the noun, the less likely a specific

2We used base 2 here to be consistent with the base used
in noun surprisal.

classifier is to be used (β = −0.038, p < 0.001,
Figure 4). There was no effect of noun frequency
(β = 0.018, p = 0.51, Figure 5). For the test set,
the result of noun predictability replicates (β =
−0.059, p < 0.001, Figure 6).3 In the test set but
not in the development set, we also found an effect
of noun frequency (β = −0.11, p < 0.001, Fig-
ure 7): the more frequent the noun, the less likely
a specific classifier is to be used. Further analy-
sis suggests that this effect of noun frequency in
the test set is likely to be an artifact of incorrect
noun–classifier associations that would disappear
were we to filter the test set in the same way as we
filtered the development set.4 The consistent effect
of noun surprisal on classifier choice in both our
development and test sets supports the availability-
based production hypothesis, and is inconsistent
with the predictions of UID.

One potential concern regarding the above con-
clusion that noun predictability drives classifier
choice is that it might not fully take into account
effects of the frequencies of classifiers themselves
on availability. The availability-based production
hypothesis does not exclude the possibility that a
classifier’s accessibility is substantially dependent
on its frequency, and the general classifier is in-
deed the most frequently used classifier. However,
if specific classifier frequency were confounding
the apparent effect of noun surprisal that we see
in our analysis, there would have to be a correla-
tion in our dataset between specific classifier fre-
quency and noun surprisal. Our inclusion of a by-
specific-classifier random intercept largely rules
out the possibility that even a correlation that the
above-mentioned one could be driving our effect.
To be thorough, we tried a version of our regres-
sion analysis that also include a fixed effect for the
log frequency of potential specific classifier as a
control. We did not find any qualitative change to

3As can be seen in Figure 6, there is a bump at bin 27 in
the rate of using a specific classifier. We consider this likely
to be due to data sparsity: the number of observations is small
in the last two bins of noun surprisal (n = 27 and n = 3),
and there is no such bump in the development set.

4We found a marginal effect of noun frequency in our un-
filtered development set, where the more frequent the noun
was, the less likely it was used with a specific classifier.
We did further analysis with the dev set and found that
the“nouns” (some of them were misclassified as nouns from
the results of the automatic parsing) that were excluded tend
to have a higher frequency compared to the ones that were
included, and the excluded ones also had a lower rate of con-
curring with a specific classifier. This tendency suggests that
in the unfiltered test set, illegible nouns may contribute at
least partially to the noun frequency effect.

2002



●
●

●

●

●

●

● ●

●
●

● ● ●

● ●

121,826 obs., 60 classifier types, 713 noun types

0.0

0.2

0.4

0.6

0.8

1.0

1 3 5 7 9 11 13 15 17 19 21 23 25 27 29
Binned Noun Surprisal

R
at

e 
of

 U
si

ng
 a

 S
pe

ci
fic

 C
la

ss
ifi

er

0

5000

10000

15000

20000

1 3 5 7 9 11 13 15 17 19 21 23 25 27 29

C
ou

nt

Figure 4: Dev set: N-gram estimated noun surprisal
and the rate of using a specific classifier (as opposed
to the general classifier ge).

●
●

●

●

●

●

●

●

121,826 obs., 60 classifier types, 713 noun types

0.0

0.2

0.4

0.6

0.8

1.0

3 5 7 9 11 13 15 17
Binned Log Noun Frequency

R
at

e 
of

 U
si

ng
 a

 S
pe

ci
fic

 C
la

ss
ifi

er

0

50

100

150

200

3 5 7 9 11 13 15 17

C
ou

nt
 (

ty
pe

s)

Figure 5: Dev set: Noun frequency (log scale) and
the rate of using a specific classifier (as opposed to
the general classifier ge).

●

●
●

● ● ● ● ●

●
●

● ●

●

●

●

175,988 obs., 62 classifier types, 1093 noun types

0.0

0.2

0.4

0.6

0.8

1.0

1 3 5 7 9 11 13 15 17 19 21 23 25 27 29
Binned Noun Surprisal

R
at

e 
of

 U
si

ng
 a

 S
pe

ci
fic

 C
la

ss
ifi

er

0

10000

20000

30000

1 3 5 7 9 11 13 15 17 19 21 23 25 27 29

C
ou

nt

Figure 6: Test set: N-gram estimated noun surprisal
and the rate of using a specific classifier.

●
●

●
●

●

●
●

●

●

●

175,988 obs., 62 classifier types, 1093 noun types

0.0

0.2

0.4

0.6

0.8

1.0

3 5 7 9 11 13 15 17 19 21
Binned Log Noun Frequency

R
at

e 
of

 U
si

ng
 a

 S
pe

ci
fic

 C
la

ss
ifi

er

0

100

200

3 5 7 9 11 13 15 17 19 21

C
ou

nt
 (

ty
pe

s)

Figure 7: Test set: Noun frequency (log scale) and
the rate of using a specific classifier.

2003



the results: the effect of noun surprisal on specific
classifier choice remains the same. We also note
that in this new analysis, we do not find a signif-
icant effect of specific classifier log frequency on
classifier choice (p = 0.629 for the dev set and
p = 0.7 for the test set). This additional analysis
suggests that it is unlikely that the effect of spe-
cific classifier frequency to be driving the effect of
noun surprisal.

Overall, we did not find evidence for the UID
hypothesis at the level of alternating options with
different information density, in our case, a spe-
cific classifier versus a general classifier. We
demonstrate that within the scope of near seman-
tically invariant alternation, classifier choice is
modulated by noun predictability with the ten-
dency to facilitate speaker production. Our results
lend support to an availability-based production
model. We did not find consistent evidence for
the effect of noun frequency on classifier choice.
The effect of noun frequency remains unclear and
we will need to test it with a larger sample of noun
types.

7 Conclusion

Though it has proven difficult to disentangle UID
and availability-based production through optional
word omission phenomena, we have demonstrated
here that the two accounts can potentially be
distinguished through at least one word alterna-
tion phenomenon. The UID hypothesis predicts
that predictable nouns favor the general classi-
fier whereas availability-based production predicts
that predictable nouns favor a specific classifier.
Our empirical results favor the availability-based
production account.

To the best of our knowledge, this is the first
study that demonstrates contextual predictability
is correlated with classifier choice. This study
provides a starting point to understand the cog-
nitive mechanisms governing speaker choices as
manifested in various language optionalities. Ulti-
mately we plan to complement our corpus analysis
with real-time language production experiments
to more throughly test hypotheses about speaker
choice.

Acknowledgments

We gratefully acknowledge valuable feedback
from Naomi Feldman, members of MIT’s Com-
putational Psycholinguistics Laboratory, three

anonymous reviewers, technical advice for data
processing from Wenzhe Qiu, and support from
NSF grants BCS-1456081 and BCS-1551866 to
RPL, and an MIT Henry E. Singleton (1940) Fel-
lowship to MZ.

References
R Harald Baayen, Douglas J Davidson, and Douglas M

Bates. 2008. Mixed-effects modeling with crossed
random effects for subjects and items. Journal of
Memory and Language 59(4):390–412.

David A Balota and James I Chumbley. 1985. The lo-
cus of word-frequency effects in the pronunciation
task: Lexical access and/or production? Journal of
Memory and Language 24(1):89–106.

Dale J Barr, Roger Levy, Christoph Scheepers, and
Harry J Tily. 2013. Random effects structure for
confirmatory hypothesis testing: Keep it maximal.
Journal of Memory and Language 68(3):255–278.

Douglas Bates, Martin Maechler, Ben Bolker, Steven
Walker, et al. 2014. lme4: Linear mixed-effects
models using eigen and s4. R package version 1(7).

Alan Bell, Jason M Brenier, Michelle Gregory, Cyn-
thia Girand, and Dan Jurafsky. 2009. Predictability
effects on durations of content and function words
in conversational English. Journal of Memory and
Language 60(1):92–111.

Kathryn Bock. 1987. An effect of the accessibility of
word forms on sentence structures. Journal of Mem-
ory and Language 26(2):119–137.

Christian Buck, Kenneth Heafield, and Bas Van Ooyen.
2014. N-gram counts and language models from the
common crawl. In LREC. volume 2, page 4.

Vera Demberg and Frank Keller. 2008. Data from eye-
tracking corpora as evidence for theories of syntactic
processing complexity. Cognition 109(2):193–210.

Matthew S. Dryer and Martin Haspelmath, editors.
2013. WALS Online. Max Planck Institute for Evo-
lutionary Anthropology, Leipzig. http://wals.
info/.

Victor S Ferreira and Gary S Dell. 2000. Effect of am-
biguity and lexical availability on syntactic and lex-
ical production. Cognitive Psychology 40(4):296–
340.

Stefan L Frank and Rens Bod. 2011. Insensitivity of
the human sentence-processing system to hierarchi-
cal structure. Psychological Science 22(6):829–834.

Joseph H Greenberg. 1972. Numeral classifiers and
substantival number: Problems in the genesis of a
linguistic type. Working Papers on Language Uni-
versals 9.

2004



Jarrod D Hadfield et al. 2010. MCMC methods for
multi-response generalized linear mixed models: the
MCMCglmm R package. Journal of Statistical Soft-
ware 33(2):1–22.

John Hale. 2001. A probabilistic Earley parser as a
psycholinguistic model. In Proceedings of the sec-
ond meeting of the North American Chapter of the
Association for Computational Linguistics on Lan-
guage technologies. Association for Computational
Linguistics, pages 1–8. http://aclweb.org/
anthology/N/N01/N01-1021.pdf.

T. Florian Jaeger. 2010. Redundancy and reduc-
tion: Speakers manage syntactic information den-
sity. Cognitive Psychology 61(1):23–62.

Daniel Jurafsky, Alan Bell, Michelle Gregory, and
William D Raymond. 2001. Probabilistic relations
between words: Evidence from reduction in lex-
ical production. Typological studies in language
45:229–254.

Natalie M Klein, Greg N Carlson, Renjie Li, T Flo-
rian Jaeger, and Michael K Tanenhaus. 2012. Clas-
sifying and massifying incrementally in chinese lan-
guage comprehension. Count and mass across lan-
guages pages 261–282.

Roger Levy. 2008. Expectation-based syntactic com-
prehension. Cognition 106(3):1126–1177.

Roger P. Levy and T. Florian Jaeger. 2007. Speakers
optimize information density through syntactic re-
duction. In Advances in Neural Information Pro-
cessing Systems. pages 849–856.

Aimin Ma. 2015. Hanyu geti liangci de chansheng
yu fazhan[The Emergence and Development of Chi-
nese Individual Classifiers]. China Social Sciences
Press.

Christopher D Manning, Mihai Surdeanu, John Bauer,
Jenny Rose Finkel, Steven Bethard, and David Mc-
Closky. 2014. The Stanford CoreNLP natural lan-
guage processing toolkit. In ACL (System Demon-
strations). pages 55–60. http://www.aclweb.
org/anthology/P14-5010.

R.C. Oldfield and A. Wingfield. 1965. Response laten-
cies in naming objects. Quarterly Journal of Exper-
imental Psychology 17(4):273–281.

Steven T Piantadosi, Harry Tily, and Edward Gibson.
2011. Word lengths are optimized for efficient com-
munication. Proceedings of the National Academy
of Sciences 108(9):3526–3529.

Scott Seyfarth. 2014. Word informativity influences
acoustic duration: Effects of contextual predictabil-
ity on lexical representation. Cognition 133(1):140–
155.

Nathaniel J Smith and Roger Levy. 2013. The effect of
word predictability on reading time is logarithmic.
Cognition 128(3):302–319.

Sogou. 2008. Sogou lab data: Sohu news cor-
pus 2008 version. http://www.sogou.com/
labs/resource/cs.php. Accessed: 2017-05-
30.

Andreas Stolcke. 2002. SRILM-an extensible lan-
guage modeling toolkit. In Interspeech. volume
2002, page 2002.

Muhua Zhu, Yue Zhang, Wenliang Chen, Min
Zhang, and Jingbo Zhu. 2013. Fast and accu-
rate shift-reduce constituent parsing. In ACL (1).
pages 434–443. http://www.aclweb.org/
anthology/P13-1043.

2005


