



















































A Latent Concept Topic Model for Robust Topic Inference Using Word Embeddings


Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 380–386,
Berlin, Germany, August 7-12, 2016. c©2016 Association for Computational Linguistics

A Latent Concept Topic Model for Robust Topic Inference
Using Word Embeddings

Weihua Hu† and Jun’ichi Tsujii‡§
†Department of Computer Science, The University of Tokyo, Japan

‡Artificial Intelligence Research Center, AIST, Japan
§School of Computer Science, The University of Manchester, UK
{hu,j-tsujii}@ms.k.u-tokyo.ac.jp,@aist.go.jp

Abstract

Uncovering thematic structures of SNS
and blog posts is a crucial yet challeng-
ing task, because of the severe data spar-
sity induced by the short length of texts
and diverse use of vocabulary. This hin-
ders effective topic inference of traditional
LDA because it infers topics based on
document-level co-occurrence of words.
To robustly infer topics in such contexts,
we propose a latent concept topic model
(LCTM). Unlike LDA, LCTM reveals top-
ics via co-occurrence of latent concepts,
which we introduce as latent variables to
capture conceptual similarity of words.
More specifically, LCTM models each
topic as a distribution over the latent con-
cepts, where each latent concept is a local-
ized Gaussian distribution over the word
embedding space. Since the number of
unique concepts in a corpus is often much
smaller than the number of unique words,
LCTM is less susceptible to the data spar-
sity. Experiments on the 20Newsgroups
show the effectiveness of LCTM in deal-
ing with short texts as well as the capabil-
ity of the model in handling held-out doc-
uments with a high degree of OOV words.

1 Introduction

Probabilistic topic models such as Latent Dirich-
let allocation (LDA) (Blei et al., 2003), are widely
used to uncover hidden topics within a text corpus.
LDA models each document as a mixture of top-
ics where each topic is a distribution over words.
In essence, LDA reveals latent topics in a corpus
by implicitly capturing document-level word co-
occurrence patterns (Wang and McCallum, 2006).

In recent years, Social Networking Services and
blogs have become increasingly prevalent due to

the explosive growth of the Internet. Uncover-
ing the themantic structures of these posts is cru-
cial for tasks like market review, trend estimation
(Asur and Huberman, 2010) and so on. How-
ever, compared to more conventional documents,
such as news articles and academic papers, ana-
lyzing the thematic content of blog posts can be
challenging, because of their typically short length
and the use of diverse vocabulary by various au-
thors. These factors can substantially decrease the
chance of topically related words co-occurring in
the same post, which in turn hinders effective topic
inference in conventional topic models. Addition-
ally, sometimes small corpus size can further exac-
erbate topic inference, since word co-occurrence
statistics becomes more sparse as the number of
documents decreases.

Recently, word embedding models, such as
word2vec (Mikolov et al., 2013) and GloVe (Pen-
nington et al., 2014) have gained much attention
with their ability to form clusters of conceptually
similar words in the embedding space. Inspired
by this, we propose a latent concept topic model
(LCTM) that infers topics based on document-
level co-occurrence of references to the same con-
cept. More specifically, we introduce a new la-
tent variable, termed a latent concept to capture
conceptual similarity of words, and redefine each
topic as a distribution over the latent concepts.
Each latent concept is then modeled as a localized
Gaussian distribution over the embedding space.
This is illustrated in Figure 1, where we denote
the centers of the Gaussian distributions as con-
cept vectors. We see that each concept vector
captures a representative concept of surrounding
words, and the Gaussian distributions model the
small variation between the latent concepts and
the actual use of words. Since the number of
unique concepts that are referenced in a corpus
is often much smaller than the number of unique

380



Figure 1: Projected latent concepts on the word
embedding space. Concept vectors are annotated
with their representative concepts in parentheses.

words, we expect topically-related latent concepts
to co-occur many times, even in short texts with
diverse usage of words. This in turn promotes
topic inference in LCTM.

LCTM further has the advantage of using con-
tinuous word embedding. Traditional LDA as-
sumes a fixed vocabulary of word types. This
modeling assumption prevents LDA from han-
dling out of vocabulary (OOV) words in held-out
documents. On the other hands, since our topic
model operates on the continuous vector space, it
can naturally handle OOV words once their vector
representation is provided.

The main contributions of our paper are as fol-
lows: We propose LCTM that infers topics via
document-level co-occurrence patterns of latent
concepts, and derive a collapsed Gibbs sampler
for approximate inference. We show that LCTM
can accurately represent short texts by outperform-
ing conventional topic models in a clustering task.
By means of a classification task, we furthermore
demonstrate that LCTM achieves superior perfor-
mance to other state-of-the-art topic models in
handling documents with a high degree of OOV
words.

The remainder of the paper is organized as fol-
lows: related work is summarized in Section 2,
while LCTM and its inference algorithm are pre-
sented in Section 3. Experiments on the 20News-
groups are presented in Section 4, and a conclu-
sion is presented in Section 5.

2 Related Work

There have been a number of previous studies on
topic models that incorporate word embeddings.
The closest model to LCTM is Gaussian LDA

(Das et al., 2015), which models each topic as
a Gaussian distribution over the word embedding
space. However, the assumption that topics are
unimodal in the embedding space is not appropri-
ate, since topically related words such as ‘neural’
and ‘networks’ can occur distantly from each other
in the embedding space. Nguyen et al. (2015) pro-
posed topic models that incorporate information
of word vectors in modeling topic-word distribu-
tions. Similarly, Petterson et al. (Petterson et al.,
2010) exploits external word features to improve
the Dirichlet prior of the topic-word distributions.
However, both of the models cannot handle OOV
words, because they assume fixed word types.

Latent concepts in LCTM are closely related
to ‘constraints’ in interactive topic models (ITM)
(Hu et al., 2014). Both latent concepts and con-
straints are designed to group conceptually simi-
lar words using external knowledge in an attempt
to aid topic inference. The difference lies in their
modeling assumptions: latent concepts in LCTM
are modeled as Gaussian distributions over the
embedding space, while constraints in ITM are
sets of conceptually similar words that are interac-
tively identified by humans for each topic. Each
constraint for each topic is then modeled as a
multinomial distribution over the constrained set
of words that were identified as mutually related
by humans. In Section 4, we consider a variant of
ITM, whose constraints are instead inferred using
external word embeddings.

As regards short texts, a well-known topic
model is Biterm Topic Model (BTM) (Yan et
al., 2013). BTM directly models the genera-
tion of biterms (pairs of words) in the whole cor-
pus. However, the assumption that pairs of co-
occurring words should be assigned to the same
topic might be too strong (Chen et al., 2015).

3 Latent Concept Topic Model

3.1 Generative Model
The primary difference between LCTM and the
conventional topic models is that LCTM describes
the generative process of word vectors in docu-
ments, rather than words themselves.

Suppose α and β are parameters for the Dirich-
let priors and let vd,i denote the word embedding
for a word type wd,i. The generative model for
LCTM is as follows.

1. For each topic k

(a) Draw a topic concept distribution ϕk ∼
Dirichlet(β).

381



(a) LDA. (b) LCTM.

Figure 2: Graphical representation.

2. For each latent concept c

(a) Draw a concept vector µc ∼
N (µ, σ20I).

3. For each document d
(a) Draw a document topic distribution

θd ∼ Dirichlet(α).
(b) For the i-th word wd,i in document d

i. Draw its topic assignment zd,i ∼
Categorical(θd).

ii. Draw its latent concept assignment
cd,i ∼ Categorical(ϕzd,i).

iii. Draw a word vector vd,i ∼
N (µcd,i , σ2I).

The graphical models for LDA and LCTM are
shown in Figure 2. Compared to LDA, LCTM
adds another layer of latent variables to indicate
the conceptual similarity of words.

3.2 Posterior Inference
In our application, we observe documents consist-
ing of word vectors and wish to infer posterior dis-
tributions over all the hidden variables. Since there
is no analytical solution to the posterior, we derive
a collapsed Gibbs sampler to perform approximate
inference. During the inference, we sample a la-
tent concept assignment as well as a topic assign-
ment for each word in each document as follows:

p(zd,i = k | cd,i = c,z−d,i, c−d,i,v)

∝
(
n−d,id,k + αk

)
· n

−d,i
k,c + βc

n−d,ik,· +
∑

c′ βc′
, (1)

P (cd,i = c | zd,i = k,vd,i, z−d,i, c−d,i,v−d,i)
∝
(
n−d,ik,c + βc

)
· N (vd,i|µc, σ2cI), (2)

where nd,k is the number of words assigned to
topic k in document d, and nk,c is the number of
words assigned to both topic k and latent concept
c. When an index is replaced by ‘·’, the number is

obtained by summing over the index. The super-
script −d,i indicates that the current assignments
of zd,i and cd,i are ignored. N (·|µ,Σ) is a mul-
tivariate Gaussian density function with mean µ
and covariance matrix Σ. µc and σ2c in Eq. (2)
are parameters associated with the latent concept
c and are defined as follows:

µc =
1

σ2 + n−d,i·,c σ20

σ2µ + σ20 · ∑
(d′,i′)∈A−d,ic

vd′,i′

 ,
(3)

σ2c =

(
1 +

σ20

n−d,i·,c σ20 + σ2

)
σ2, (4)

where A−d,ic ≡ {(d′, i′) | cd′,i′ = c ∧ (d′, i′) ̸=
(d, i)} (Murphy, 2012). Eq. (1) is similar to the
collapsed Gibbs sampler of LDA (Griffiths and
Steyvers, 2004) except that the second term of
Eq. (1) is concerned with topic-concept distribu-
tions. Eq. (2) of sampling latent concepts has an
intuitive interpretation: the first term encourages
concept assignments that are consistent with the
current topic assignment, while the second term
encourages concept assignments that are consis-
tent with the observed word. The Gaussian vari-
ance parameter σ2 acts as a trade-off parameter
between the two terms via σ2c . In Section 4.2, we
study the effect of σ2 on document representation.

3.3 Prediction of Topic Proportions

After the posterior inference, the posterior means
of {θd}, {ϕk} are straightforward to calculate:

θd,k =
nd,k + αk

nd,· +
∑

k′ αk′
, ϕk,c =

nk,c + βc
nk,· +

∑
c′ βc′

. (5)

Also posterior means for {µc} are given by
Eq. (3). We can then use these values to predict
a topic proportion θdnew of an unseen document
dnew using collapsed Gibbs sampling as follows:

p(zdnew,i = k | vdnew,i,v−dnew,i,z−dnew,i,ϕ,µ)

∝
(
n−dnew,idnew,k + αk

)
·
∑

c

ϕk,c
N (vdnew,i|µc, σ2c )∑
c′ N (vdnew,i|µc′ , σ2c′)

.

(6)

The second term of Eq. (6) is a weighted average
of ϕk,c with respect to latent concepts. We see that
more weight is given to the concepts whose corre-
sponding vectors µc are closer to the word vec-
tor vdnew,i. This to be expected because statistics
of nearby concepts should give more information
about the word. We also see from Eq. (6) that the

382



topic assignment of a word is determined by its
embedding, instead of its word type. Therefore,
LCTM can naturally handle OOV words once their
embeddings are provided.

3.4 Reducing the Computational Complexity

From Eqs. (1) and (2), we see that the computa-
tional complexity of sampling per word is O(K +
SD), where K, S and D are numbers of topics, la-
tent concepts and embedding dimensions, respec-
tively. Since K ≪ S holds in usual settings, the
dominant computation involves the sampling of
latent concept, which costs O(SD) computation
per word.

However, since LCTM assumes that Gaussian
variance σ2 is relatively small, the chance of a
word being assigned to distant concepts is negli-
gible. Thus, we can reasonably assume that each
word is assigned to one of M ≪ S nearest con-
cepts. Hence, the computational complexity is
reduced to O(MD). Since concept vectors can
move slightly in the embedding space during the
inference, we periodically update the nearest con-
cepts for each word type.

To further reduce the computational complexity,
we can apply dimensional reduction algorithms
such as PCA and t-SNE (Van der Maaten and Hin-
ton, 2008) to word embeddings to make D smaller.
We leave this to future work.

4 Experiments

4.1 Datasets and Models Description

In this section, we study the empirical perfor-
mance of LCTM on short texts. We used the
20Newsgroups corpus, which consists of discus-
sion posts about various news subjects authored
by diverse readers. Each document in the corpus is
tagged with one of twenty newsgroups. Only posts
with less than 50 words are extracted for training
datasets. For external word embeddings, we used
50-dimensional GloVe1 that were pre-trained on
Wikipedia. The datasets are summarized in Ta-
ble 1. See appendix A for the detail of the dataset
preprocessing.

We compare the performance of the LCTM to
the following six baselines:

• LFLDA (Nguyen et al., 2015), an extension
of Latent Dirichlet Allocation that incorpo-
rates word embeddings information.

1Downloaded at
http://nlp.stanford.edu/projects/glove/

Dataset Doc size Vocab size Avg len
400short 400 4729 31.87
800short 800 7329 31.78
1561short 1561 10644 31.83
held-out 7235 37944 140.15

Table 1: Summary of datasets.

• LFDMM (Nguyen et al., 2015), an extension
of Dirichlet Multinomial Mixtures that incor-
porates word embeddings information.

• nI-cLDA, non-interactive constrained Latent
Dirichlet Allocatoin, a variant of ITM (Hu et
al., 2014), where constraints are inferred by
applying k-means to external word embed-
dings. Each resulting word cluster is then re-
garded as a constraint. See appendix B for
the detail of the model.

• GLDA (Das et al., 2015), Gaussian LDA.
• BTM (Yan et al., 2013), Biterm Topic Model.
• LDA (Blei et al., 2003).
In all the models, we set the number of topics

to be 20. For LCTM (resp. nI-ITM), we set the
number of latent concepts (resp. constraints) to
be 1000. See appendix C for the detail of hyper-
parameter settings.

4.2 Document Clustering
To demonstrate that LCTM results in a superior
representation of short documents compared to the
baselines, we evaluated the performance of each
model on a document clustering task. We used
a learned topic proportion as a feature for each
document and applied k-means to cluster the doc-
uments. We then compared the resulting clus-
ters to the actual newsgroup labels. Clustering
performance is measured by Adjusted Mutual In-
formation (AMI) (Manning et al., 2008). Higher
AMI indicates better clustering performance. Fig-
ure 3 illustrates the quality of clustering in terms
of Gaussian variance parameter σ2. We see that
setting σ2 = 0.5 consistently obtains good clus-
tering performance for all the datasets with vary-
ing sizes. We therefore set σ2 = 0.5 in the later
evaluation. Figure 4 compares AMI on four topic
models. We see that LCTM outperforms the topic
models without word embeddings. Also, we see
that LCTM performs comparable to LFLDA and
nl-cLDA, both of which incorporate information
of word embeddings to aid topic inference. How-
ever, as we will see in the next section, LCTM can

383



Figure 3: Relationship between σ2 and AMI.

Figure 4: Comparisons on clustering performance
of the topic models.

better handle OOV words in held-out documents
than LFLDA and nl-cLDA do.

4.3 Representation of Held-out Documents
with OOV words

To show that our model can better predict topic
proportions of documents containing OOV words
than other topic models, we conducted an exper-
iment on a classification task. In particular, we
infer topics from the training dataset and predicted
topic proportions of held-out documents using col-
lapsed Gibbs sampler. With the inferred topic
proportions on both training dataset and held-out
documents, we then trained a multi-class classi-
fier (multi-class logistic regression implemented
in sklearn2 python module) on the training dataset
and predicted newsgroup labels of the held-out
documents.

We compared classification accuracy using
LFLDA, nI-cLDA, LDA, GLDA, LCTM and a
variant of LCTM (LCTM-UNK) that ignores OOV
in the held-out documents. A higher classifica-
tion accuracy indicates a better representation of
unseen documents. Table 2 shows the propor-
tion of OOV words and classification accuracy

2See http://scikit-learn.org/stable/.

Training Set 400short 800short 1561short
OOV prop 0.348 0.253 0.181
Method Classification Accuracy
LCTM 0.302 0.367 0.416
LCTM-UNK 0.262 0.340 0.406
LFLDA 0.253 0.333 0.410
nI-cLDA 0.261 0.333 0.412
LDA 0.215 0.293 0.382
GLDA 0.0527 0.0529 0.0529
Chance Rate 0.0539 0.0539 0.0539

Table 2: Proportions of OOV words and classifi-
cation accuracy in the held-out documents.

of the held-out documents. We see that LCTM-
UNK outperforms other topic models in almost
every setting, demonstrating the superiority of
our method, even when OOV words are ignored.
However, the fact that LCTM outperforms LCTM-
UNK in all cases clearly illustrates that LCTM can
effectively make use of information about OOV to
further improve the representation of unseen docu-
ments. The results show that the level of improve-
ment of LCTM over LCTM-UNK increases as the
proportion of OOV becomes greater.

5 Conclusion

In this paper, we have proposed LCTM that is
well suited for application to short texts with di-
verse vocabulary. LCTM infers topics according
to document-level co-occurrence patterns of la-
tent concepts, and thus is robust to diverse vocab-
ulary usage and data sparsity in short texts. We
showed experimentally that LCTM can produce a
superior representation of short documents, com-
pared to conventional topic models. We addition-
ally demonstrated that LCTM can exploit OOV to
improve the representation of unseen documents.
Although our paper has focused on improving per-
formance of LDA by introducing the latent con-
cept for each word, the same idea can be readily
applied to other topic models that extend LDA.

Acknowledgments

We thank anonymous reviewers for their construc-
tive feedback. We also thank Hideki Mima for
helpful discussions and Paul Thompson for in-
sightful reviews on the paper. This paper is based
on results obtained from a project commissioned
by the New Energy and Industrial Technology De-
velopment Organization (NEDO).

384



References
Sitaram Asur and Bernardo A Huberman. 2010. Pre-

dicting the future with social media. In Web Intel-
ligence and Intelligent Agent Technology (WI-IAT),
2010 IEEE/WIC/ACM International Conference on,
volume 1, pages 492–499. IEEE.

David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent Dirichlet allocation. the Journal of
machine Learning research, 3:993–1022.

Weizheng Chen, Jinpeng Wang, Yan Zhang, Hongfei
Yan, and Xiaoming Li. 2015. User based aggre-
gation for biterm topic model. Proceedings of the
53rd Annual Meeting of the Association for Compu-
tational Linguistics, 2:489–494.

Rajarshi Das, Manzil Zaheer, and Chris Dyer. 2015.
Gaussian LDA for topic models with word embed-
dings. In Proceedings of the 53nd Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 795–804.

Thomas L Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences, 101(suppl 1):5228–5235.

Yuening Hu, Jordan L. Boyd-Graber, Brianna Satinoff,
and Alison Smith. 2014. Interactive topic modeling.
Machine Learning, 95(3):423–469.

Christopher D Manning, Prabhakar Raghavan, Hinrich
Schütze, et al. 2008. Introduction to information re-
trieval, volume 1. Cambridge university press Cam-
bridge.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.

Kevin P Murphy. 2012. Machine learning: a proba-
bilistic perspective. MIT press.

Dat Quoc Nguyen, Richard Billingsley, Lan Du, and
Mark Johnson. 2015. Improving topic models with
latent feature word representations. Transactions
of the Association for Computational Linguistics,
3:299–313.

Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. GloVe: Global vectors for word
representation. In EMNLP, volume 14, pages 1532–
1543.

James Petterson, Wray Buntine, Shravan M Narayana-
murthy, Tibério S Caetano, and Alex J Smola. 2010.
Word features for latent Dirichlet allocation. In Ad-
vances in Neural Information Processing Systems,
pages 1921–1929.

Laurens Van der Maaten and Geoffrey Hinton. 2008.
Visualizing data using t-SNE. Journal of Machine
Learning Research, 9(2579-2605):85.

Xuerui Wang and Andrew McCallum. 2006. Top-
ics over time: a non-Markov continuous-time model
of topical trends. In Proceedings of the 12th ACM
SIGKDD international conference on Knowledge
discovery and data mining, pages 424–433. ACM.

Xiaohui Yan, Jiafeng Guo, Yanyan Lan, and Xueqi
Cheng. 2013. A biterm topic model for short texts.
In Proceedings of the 22nd international conference
on World Wide Web, pages 1445–1456. International
World Wide Web Conferences Steering Committee.

A Dataset Preprocessing

We preprocessed the 20Newsgroups as follows:
We downloaded bag-of-words representation of
the corpus available online3. Stop words4 and
words that were not covered in the GloVe were
both removed. After the preprocessing, we ex-
tracted short texts containing less than 50 words
for training datasets. We created three training
datasets with varying numbers of documents, and
one held-out dataset. Each dataset was balanced
in terms of the proportion of documents belonging
to each newsgroup.

B Non-Interactive Contained LDA
(nI-cLDA)

We describe nI-cLDA, a variant of interactive
topic model (Hu et al., 2014). nl-cLDA is non-
interactive in the sense that constraints are inferred
from the word embeddings instead of being in-
teractively identified by humans. In particular,
we apply k-means to word embeddings to cluster
words. Each resulting cluster is then regarded as
a constraint. In general, constraints can be differ-
ent from topic to topic. Let rk,w be a constraint of
topic k which word w belongs to. The generative
process of nl-cLDA is as follows. It is essentially
the same as (Hu et al., 2014)

1. For each topic k

(a) Draw a topic constraint distribution
ϕk ∼ Dirichlet(β).

(b) For each constraint s of topic k
i. Draw a constraint word distribution

πk,s ∼ Dirichlet(γ).
2. For each document d

(a) Draw a document topic distribution
θd ∼ Dirichlet(α).

(b) For the i-th word wd,i in document d
i. Draw its topic assignment zd,i ∼

Categorical(θd).
3http://qwone.com/˜jason/20Newsgroups/
4Available at http://www.nltk.org/

385



ii. Draw its constraint ld,i ∼
Categorical(ϕzd,i).

iii. Draw a word wd,i ∼
Categorical(πzd,i,ld,i).

Let V be the set of vocabulary. We note
that πk,s is a multinomial distribution over Wk,s,
which is a subset of V , defined as Wk,s ≡ {w ∈
V | rk,w = s}. Wk,s represents a constrained set
of words that are conceptually related to each other
under topic k.

In our application, we observe documents and
constraints for each topic, and wish to infer poste-
rior distributions over all the hidden variables. We
apply collapsed Gibbs sampling for the approxi-
mate inference. For the detail of the inference, see
(Hu et al., 2014).

C Hyperparameter Settings

For all the topic models, we used symmetric
Dirichlet priors. The hyperparameters were set
as follows: for our model (LCTM and LCTM-
UNK), nI-cLDA and LDA, we set α = 0.1 and
β = 0.01. For nl-cLDA, we set the parameter of
Dirichlet prior for constraint-word distribution (γ
in appendix B) as 0.1. Also for our model, we
set, σ20 = 1.0 and µ to be the average of word
vectors. We randomly initialized the topic assign-
ments in all the models. Also, we initialized the la-
tent concept assignments using k-means clustering
on the word embeddings. The k-means clustering
was implemented using sklearn5 python module.
We set M (number of nearest concepts to sample
from) to be 300, and updated the nearest concepts
every 5 iterations. For LFLDA, LFDMM, BTM
and Gaussian LDA, we used the original imple-
mentations available online6 and retained the de-
fault hyperparameters.

We ran all the topic models for 1500 iterations
for training, and 500 iterations for predicting held-
out documents.

5See http://scikit-learn.org/stable/.
6LFTM: https://github.com/datquocnguyen/LFTM

BTM: https://github.com/xiaohuiyan/BTM
GLDA: https://github.com/rajarshd/Gaussian LDA

386


