



















































Incremental Prediction of Sentence-final Verbs: Humans versus Machines


Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning (CoNLL), pages 95–104,
Berlin, Germany, August 7-12, 2016. c©2016 Association for Computational Linguistics

Incremental Prediction of Sentence-final Verbs: Humans versus Machines

Alvin C. Grissom II1, Naho Orita2, and Jordan Boyd-Graber1

1University of Colorado Boulder, Department of Computer Science
2Tohoku University, Graduate School of Information Sciences

{Alvin.Grissom,Jordan.Boyd.Graber}@colorado.edu
naho@ecei.tohoku.ac.jp

Abstract
Verb prediction is important in human sen-
tence processing and, practically, in simul-
taneous machine translation. In verb-final
languages, speakers select the final verb
before it is uttered, and listeners predict
it before it is uttered. Simultaneous in-
terpreters must do the same to translate
in real-time. Motivated by the problem
of SOV-SVO simultaneous machine trans-
lation, we provide a study of incremental
verb prediction in verb-final languages. As
a basis of comparison, we examine incre-
mental verb prediction with human par-
ticipants in a multiple choice setting us-
ing crowdsourcing to gain insight into in-
cremental human performance in a con-
strained setting. We then examine a compu-
tational approach to incremental verb pre-
diction using discriminative classification
with shallow features. Both humans and
machines predict verbs more accurately as
more of a sentence becomes available, and
case markers—when available—help hu-
mans and sometimes machines predict final
verbs.

1 The Importance of Verb Prediction

Humans predict future linguistic input before it is
observed (Kutas et al., 2011). This predictability
has been formalized in information theory (Shan-
non, 1948)—the more predictable a word is, the
lower the entropy—and has explained various lin-
guistic phenomena, such as garden path ambigu-
ity (Den and Inoue, 1997; Hale, 2001). Such in-
stances of linguistic prediction are fundamental to
statistical NLP. Auto-complete from search engines
has made next-word prediction one of best known
NLP applications.

Long-distance word prediction, such as verb pre-
diction in SOV languages (Levy and Keller, 2013;
Momma et al., 2015; Chow et al., 2015), is im-
portant in simultaneous machine translation from
subject-object-verb (SOV) languages to subject-
verb-object (SVO) languages. In SVO languages
such as English, for example, the main verb phrase
usually comes after the first noun phrase—the main
subject—in a sentence, while in verb-final lan-
guages such as Japanese or German, it comes very
last. Human simultaneous translators must make
predictions about the unspoken final verb to in-
crementally translate the sentence. Minimizing
interpretation delay thus requires making constant
predictions and deciding when to trust those pre-
dictions and commit to translating in real-time.

Such prediction can also aid machines. Mat-
subara et al. (2000) use pattern-matching rules;
Grissom II et al. (2014) use a statistical n-gram
approach; and Oda et al. (2015) extend the idea of
using prediction by predicting entire syntactic con-
stituents for English-Japanese translation. These
systems require fast, accurate verb prediction to
further improve simultaneous translation systems.
We focus on verb prediction in verb-final languages
such as Japanese with this motivation in mind.

In Section 2, we present what is, to our knowl-
edge, the first study of humans’ ability to incremen-
tally predict the verbs in Japanese. We use these
human data as a yardstick to which to compare
computational incremental verb prediction. Incor-
porating some of the key insights from our human
study into a discriminative model—namely, the im-
portance of case markers— Section 3 presents a
better incremental verb classifier than existing verb
prediction schemes. Having established both hu-
man and computer performance on this challenging
and interesting task, Section 4 reviews our work’s
relationship to other studies in NLP and linguistics.

95



2 Human Verb Prediction

We first examine human verb selection in a con-
strained setting to better understand what perfor-
mance we should demand of computational ap-
proaches. While we know that humans make in-
cremental predictions across sentences, we do not
know how skilled they are in doing so. While it’s
possible that machines—with unbounded memory
and access to Internet-sized data—could do better
than humans, this study allows us to appropriately
gauge our expectations for computational systems.

We use crowdsourcing to measure how well
novice humans can predict the final verb phrase
of incomplete Japanese sentences in a multiple
choice setting. We use Japanese text of the Ky-
oto Free Translation Task corpus (Neubig, 2011,
KFT), a collection of Wikipedia articles in English
and Japanese, representing standard, grammatical
text and readily usable for future SOV-SVO machine
translation experiments.

2.1 Extracting Verbs and Sentences

This section describes the data sources, preparation,
and methodology for crowdsourced verb prediction.
Given an incomplete sentence, participants select a
sentence-final verb phrase containing a verb from
a list of four choices to complete the sentence, one
of which is the original completion.

We randomly select 200 sentences from the de-
velopment set of the KFT corpus (Neubig, 2011).
We use these data because the sentences are from
Wikipedia articles and thus represent widely-read,
grammatical sentences. These data are directly
comparable to our computational experiments and
readily usable for future SOV-SVO machine transla-
tion experiments.

We ask participants to predict a “verb chunk”
that would be natural for humans. More technically,
this is a sentence-final bunsetsu.1 We identify verb
bunsetsu with a dependency parser (Kurohashi and
Nagao, 1994). Of interest are bunsetsu at the end of
a sentence that contain a verb. We also use bunsetsu
for segmenting the incomplete sentences we show
to humans, only segmenting between bunsetsu to
ensure each segment is a meaningful unit.

1A bunsetsu is a commonly used linguistic unit in Japanese,
roughly equivalent to an English phrase: a collection of con-
tent words and zero or more functional words. Japanese verb
bunsetsu often encompass complex conjugation. For example,
a verb phrase 読みたくなかった (read-DESI-NEG-PAST),
meaning ‘didn’t want to read’, has multiple tokens capturing
tense, negation, etc. necessary for translation.

Answer Choice Selection We display the cor-
rect verb bunsetsu and three incorrect bunsetsu
completions as choices that occur in the data with
frequency close to the correct answer in the overall
corpus. We manually inspect the incorrect answers
to ensure that these choices are semantically distant,
i.e., excluding synonyms or troponyms.

Sentence Presentation We create two test sets
of truncated sentences from the KFT corpus: The
first, the full context set, includes all but the final
bunsetsu—i.e., the verb phrase—to guess. The
second set, the random length set, contains the
same sentences truncated at predetermined, random
bunsetsu boundaries. The average sentence length
is nine bunsetsu, with a maximum of fourteen and
minimum of three. We display sentences in the
original Japanese script.

Participants view the task as a game of guessing
the final verb. Each fragment has four concurrently
displayed completion options, as in the prompt (2)
and answers (3). Users receive no feedback from
the interface.

We use CrowdFlower2 to collect participants’
answers, at a total cost of approximately USD$300.
From an initial pool of fifty-six participants, we re-
move twenty via a Japanese fluency screening. We
verify the efficacy of this test with non-native but
highly proficient Japanese learners; none passed.
We collect five judgments per sentence from each
participant.

(2) 谷崎潤一郎は
Junichiro Tanizaki-TOP

数寄屋を
tea-ceremony house-OBJ

(3) (a) 好んだ
like-PAST

(b) 変えられた
change-PASS CAP-PAST

(c) 始まったとされている
begin-PAST-COMP-suppose-AUX.PRES

(d) 増やしていた
increase-AUX.PAST

2.2 Presenting Partial and Complete
Sentences

The first task, on the full context set, shows how
humans predict the sentence-final verb chunk with
all context available. The second task, on the

2http://www.crowdflower.com/

96



0

25

50

75

100

0 5 10
Sentence Length

A
c
c
u
ra

c
y
 %

Figure 1: Full context set: Accuracy is generally
high, but slightly decreases on longer, more com-
plicated sentences, averaging 81.1%.

random length set, shows how the amount of re-
vealed data affects the predictability of the final
verb chunk. We examine a correlation between
the length of the pre-verb sentence fragment and
participants’ accuracy (Figure 1).

Psycholinguistic experiments using lexical deci-
sion tasks suggest Japanese speakers start syntactic
processing by using case—the type and number of
case-marked arguments—before the verb’s avail-
ability (Yamashita, 2000). We also examine the
correlation between the number of case markers3

and accuracy. It is likely that the number of case
markers and the length of the sentence fragment are
confounded; so, we create a measure, the propor-
tion of case markers to the overall sentence infor-
mation (the number of case markers in the fragment
divided by the number of bunsetsu chunks). We
call this case density.

2.3 Results of Human Experiments

In the full context set, average accuracy over 200
sentences is 81.1%, significantly better than chance
(p < 2.2 · 10−16). Figure 1 shows the accu-
racy per sentence length as defined by the bun-
setsu unit. A one-way ANOVA reveals a signifi-
cant effect of the sentence length (F (1, 998) =
7.512, p < 0.00624), but not the case density
(F (1, 998) = 1.2, p = 0.274).

In the random length set, average accuracy over
200 sentences is 54.2%, significantly better than
chance (t(199) = 11.8205, p < 2.2 · 10−16). Fig-
ure 2 shows the accuracy per percentage of length

3In this study, we counted case markers that mark nom-
inative (-ga), accusative (-wo), ablative (-kara), and dative
(-ni).

0

25

50

75

100

0.25 0.50 0.75 1.00
% Sentence Revealed

%
 A

c
c
u
ra

c
y

Figure 2: Random length set: The accuracy of
human verb predictions reliably increases as more
of the sentence is revealed.

of the presented sentence fragment. A one-way
ANOVA reveals a significant effect of the sentence
length (F (1, 998) = 57.44, p < 7.94 · 10−14). We
also find a significant effect of the case density
(F (1, 998) = 5.884, p = 0.0155).

2.4 Discussion
Predictability increases with the percent of the sen-
tence available in all of our experiments. By the
end of the sentence, the verb chunks are highly pre-
dictable by humans in the multiple choice setting.
Participants choose the final verb more accurately
as they gain access to more case markers in the
random length set but not in the full context set.

Case density is a significant factor in predictive
accuracy on the random length set for humans,
suggesting that case is more helpful in predicting a
sentence-final verb when the preceding contextual
information is insufficient. The following exam-
ple illustrates how case helps in prediction. The
nominative and accusative markers greatly narrow
the choices, as shown in (4).4 Our results further
support the proposition case markers modulate pre-
dictability in SOV verb-final processing.
(4) 江戸幕府区-ががが

Edo shogunate-NOM
成立すると
establish-do-CONJ

寺院法度-ににに-より
temple-prohibition-etc.-ACC-for

—

4A recent psycholinguistics study on incremental Japanese
verb-final processing (Momma et al., 2015) argues that native
Japanese speakers plan verbs in advance, before the articula-
tion of object nouns, but not subject nouns. Since case markers
assign the roles of subject and object in Japanese, we expect
that a high ratio of case markers to words will increase pre-
dictability of verbs. In addition, Yamashita (1997) argues that
the variety of case markers increases predictability just before
the verb.

97



Verb

P
(v

er
b)

Reuters Kyoto

Figure 3: Distribution of the top 100 content verbs
in the Kyoto corpus and the Reuters Japanese news
corpus. Both are Zipfian, but the Reuters corpus is
even more skewed, even with the common special
cases excluded.

‘After Edo shogunate has established, due to
the temple prohibition etc. —’

In other cases, there exist choices, which, while
incorrect, could naturally complete the sentence.
These questions are frequently missed. For in-
stance, in one 90% revealed sentence, the partici-
pant has the choices: (i)収める (put-PRES), (ii)
厳しくなる (strict-become), (iii)収録されてい
る (record-do.PASS-AUX.PRES), and (iv)務める
(work-PRES). Choice (i) is the correct answer, but
choice (iii) is a reasonable choice for a Japanese
speaker. All participants missed this question, and
all chose the same wrong answer (iii). We leave a
cloze task where participants can freely fill in the
sentence-final to future work.

These results provide a basis of comparison for
automatic prediction. In the next section, we ex-
amine whether computational models can predict
final verbs and compare the models’ performance
to that of humans.

3 Machine Verb Prediction

Now that we have the results of the previous sec-
tion, we have baselines against which we can com-
pare computational verb prediction approaches. In
this section, we introduce incremental verb classifi-
cation with a linear classifier.5 For our investigation
of computational verb classification, we use two

5While we use logistic regression, using hinge loss
achieves similar accuracy.

very different languages that both have verb-final
syntax—Japanese, which is agglutinative, and Ger-
man, which is not—and show that discriminative
classifiers can predict final verbs with increasing
accuracy as more context of sentences is revealed.

A simple verb prediction scheme applied to Ger-
man (Grissom II et al., 2014) achieves poor accu-
racy. Their approach creates a Kneser-Ney n-gram
language model for the prior context associated
with each verb in the corpus; i.e., 50 n-gram mod-
els for 50 verbs. Given pre-verb n-gram context c
in a sentence St, and verb prediction v(t) ∈ V , the
verb selection is defined by the following equation:

v(t) ≡ arg max
v

∏
c∈St

p(c | v)p(v). (1)

It chooses the verb that maximizes the proba-
bility of the observed context, scaled by the prior
probability of the verb in the overall corpus. Un-
surprisingly, given the distribution of verbs in real
data (Figure 3), this n-gram-based approach has
low accuracy and tends to predict the most common
verb. For a translation system, this often degener-
ates into the less interesting problem of whether to
trust whether the final verb is indeed a common one.
While this improves translation delay, better predic-
tions will lead to more significant improvements.
We instead opt for a one-vs-all discriminative clas-
sification approach.6

3.1 Classification on Human Data
We first incrementally classify verbs on the same
200 sentences from Section 2. Since the answer
choices are often complex verb bunsetsu and since
many of these verb phrase answer choices do not
appear among the most common verbs, lemmatiz-
ing the verbs and performing one-vs-all classifica-
tion yields extremely low accuracy. Thus, we use
binary classification with a single linear classifier
to produce a probability for each candidate answer,
encoding the verb phrase itself into the feature vec-
tor.

3.1.1 Training a Morphological Model
The processing is as follows: We train on 463,716
verb-final sentences extracted from the training
data. We use both context features and final verb
features. Our context features, i.e., those preced-
ing the final verb, are represented as follows: the
context unigrams and bigrams take a value of 1

6One-vs-all classification builds a classifier for each class
versus the aggregate all other classes.

98



0

10

20

30

40

50

0.00 0.25 0.50 0.75 1.00
Revealed

A
cc

ur
ac

y

Figure 4: Verb classification results on crowd-
sourced sentences. Despite many out-of-
vocabulary items and significant noise, the aver-
age accuracy, shown in the non-monotonic line in
the plot, increases over the course of the sentence.
Larger, darker circles indicate more examples for a
given position. Accuracy was calculated by aggre-
gating the guesses at 5% intervals.

if they are present and 0 otherwise; case markers
observed in the sentence context are represented
as unigrams and bigrams in the order that they ap-
pear; and we reserve a distinct feature for the last
observed case marker in the sentence. Our verb
features consist of the final verb’s tokens given
by the morphological analyzer, which, in addition
to the verb stem itself, typically include tense and
aspect information. These are represented as uni-
grams and bigrams in the feature vector.

To allow the classifier to learn, we must encode
the interactions between the verb features and the
context features. Thus, we use the Cartesian prod-
uct of sentence and verb features to encode inter-
actions between them: for each training sentence
we generate both a positive and a negative exam-
ple. The example with the correct verb phrase is
labeled as a positive example (+1), and we uni-
formly select a random verb phrase from one of
the 500 most common verb phrases and label it
as negative (−1) example for the same sentence
context,7 yielding 927,432 training examples and
267,037,571 features.

For clarity, we describe this feature representa-
tion more formally. Given sentence St with a pre-

7We experimented with several numbers of weighted neg-
ative examples and found that one negative example with
of equal weight to the positive gave the best results of the
configurations we tried.

verb context consisting of unigrams, bigrams, and
case marker tokens, C = {c0, ..., cn}, and bunsetsu
verb phrase tokens A = {a0, ..., ak}, the feature
vector consists of C×A = {c0∧a0, c0∧a1, ..., cn∧
ak}, where ∧ concatenates the two context and an-
swer strings. During learning, the weights learned
for the concatenated tokens are thus based on the re-
lationship between a context token and a bunsetsu
token and mapped to {+1,−1}. More concretely,
individual morphemes of the Japanese verb phrase
are combined with the pre-verb unigrams, bigrams,
and uniquely identified case marker tokens. Ac-
curacy improves when the morphemes used in the
negative examples and positive examples are dis-
joint; so, we enforce this constraint when selecting
negative examples. For example, if the positive
example includes the past tense morpheme, た,
the negative example is altogether disallowed from
having this morpheme as a verb feature.

3.1.2 Choosing an Answer
At test time, we test progressively longer fragments
of each sentence, extracting the aforementioned
features online until the entire pre-verb context is
available. For every sentence fragment, the classi-
fier determines the probability of each of the four
possible verbs by adding their verb features to the
feature vector of the example. The answer choice
with the highest probability of +1 (or the lowest
probability of −1) is chosen as the answer. By
taking this approach, we can model complex verbs
and their context jointly. Intuitively, the probability
of a (+1) is the model’s prediction of how well the
bunsetsu verb phrase fits with the sentence context
(represented by the feature vector).

Some verbs are absent from the training data,
forcing the classifier to rely on morphemes to dis-
tinguish between them. The alternative—e.g., in a
typical one-vs-all classification approach—is that
the classifier could reason from nothing whatsoever
when a fully-inflected verb is absent from the train-
ing data. Given the complexity of bunsetsu, this
happens often even in large corpora for a language
such as Japanese.

3.1.3 Multiple Choice Results
Despite only choosing among four choices, this
task is in many ways more difficult than the 50-
label classification problem described in the next
section because of the added complexity inherent
modeling the effect of morphemes and missing
examples. These limitations notwithstanding, the

99



●

●

●

●

●

●

●

●

●

●

●

●

●0

20

40

60

80

5 10
Sentence Length

%
 A

cc
ur

ac
y

Figure 5: Classification accuracy as a function of
sentence length on the full context set. While there
is a clear correlation between sentence length and
accuracy, there are several outliers. Compare to
Figure 1.

accuracy does improve as more of the sentence is
revealed (Figure 4), indicating that the algorithm
learns to use these features to rank verbs, though
the performance significantly lags that of both the
human participants and our later experiments. Ad-
ditionally, on the full context set, sentence length
is negatively correlated with accuracy (Figure 5),
as in the much more convincing results of our hu-
man experiments (Figure 1), though the trend is not
entirely consistent, making it difficult to draw firm
conclusions. Case density is again positively corre-
lated with accuracy on both the random (Figure 6)
and full context sets.

An Illustrative Example To gain some insight
into how features can influence the classifier, we
here examine an example of the classifier’s behav-
ior on the multiple choice data.

(5) 少年時代-は
childhood days-TOP

熊本藩-の
Kumamoto domain-GEN

藩校-で
clan school-LOC

儒学-を
Confucianism-ACC

学び、
study:MED

後-に
subsequently-LOC

西本願寺-において
Nishihongan Temple-LOC

修行-に
discipline-ALL

(6) (a) 励ん-だ
strive-PAST

(b) 創刊-さ-れ-る
issue-do-PASS-NPST

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

● ●

●

●

●

●

●

●

0

25

50

75

100

0.0 0.1 0.2 0.3
Case Density

%
 A

cc
ur

ac
y

Figure 6: Classification accuracy as a function
of case density on the incremental sentences. The
accuracy is correlated with case density, but the
data are extremely noisy. Full-context accuracy has
a similar trend (not shown).

(c) 加え-られ-てい-る
add-PASS-CONT-NPST

(d) 勤め-る
serve-NPST

In Example (5), the classifier incorrectly chooses
“issue” as the verb until observing the accusative
case marker attached to “Confucianism”. At this
point, the classifier’s confidence in the correct an-
swer rises to 0.74—and correctly chooses “strive”.
This answer goes unchanged for the remainder of
the sentence, though “study” attaches to “Confu-
cianism”, not the final verb. The combined evi-
dence, however, is enough for the classifier to select
correctly, and indeed, most of the following tokens
only increase the classifier’s confidence. Adding
“subsequently” increases confidence to 0.84, an in-
tuitive increase given the likely tense information
contained in such a word. The somewhat redun-
dant case marker here only increases confidence
to 0.86. Adding the reference to the temple de-
creases confidence again to 0.79. But adding the
final case marker, which also forms a new bigram
with the previous word, results in a huge increase
in confidence, to 0.90.

3.2 Multiclass Verb Prediction
While the multiple choice experiment was more
open-ended (predicting random verbs), we now fo-
cus on a more constrained task: how well can we
predict the most frequent verbs. This is the cen-
tral conceit of Grissom II et al. (2014): if you can

100



do a good job of this, you can improve simultane-
ous translation. They show a slight improvement
in simultaneous translation by using n-gram lan-
guage model-based verb prediction. We show a
large improvement over their approach to verb pre-
diction using a discriminative multiclass logistic
classifier (Langford et al., 2007).

Data Preparation Our classes for multiclass
classification are the fifty most common verbs in
the KFT (Japanese, as in the human study) and
Wortschatz corpora (Biemann et al., 2007, Ger-
man).

We use data from the training and test sets of the
KFT Japanese corpus of Wikipedia articles and a
random split of the German Wortschatz web cor-
pus, from which we extract the verb-final sentences.
Grissom II et al. (2014) use an n-gram model to dis-
tinguish between the fifty most common German
verbs for SOV-SVO simultaneous machine transla-
tion, which we replicate as our baseline. Following
this study, we train a model on the fifty most com-
mon verbs in the training set.

In Japanese, due to the small size of the standard
test set, we split the data randomly, training on
60,926 verb-final sentences ending in the top fifty
verbs and testing on 1,932. Our total feature count
is 4,649,055. We use the MeCab (Kudo, 2005)
morphological analyzer for segmentation and verb
identification. We consider only verb-final sen-
tences. We skip semantically vacuous post-verbal
copulas when identifying final verbs.

Finding Verbs We identify verbs in the German
text with a part-of-speech tagger (Toutanova et al.,
2003) and select from the top fifty verbs. We con-
sider the sentence-ending set of verbs to be the final
verbs. We train on 76,209 verb-final sentences end-
ing in the top fifty verbs and test on 9,386. In
German, to approximate the case information that
we extract in Japanese, we test the inclusion of
equivalent unigram and bigram features for Ger-
man articles, the surface forms of which determine
the case of the next noun phrase.

In Japanese, we omit some special cases of light
verbs that combine with other verbs, as well as
ambiguous surface forms and copulas.8

8In Japanese, we omit some ambiguous cases and variants
of “is” and “do”: excluded are variants of suru (“to do”),
which combines with nouns to form new verbs, aru (“is”,
inanimate case), and iru (“is”, animate case). The tokens aru
and iru also combine with other verbs to change tense and
aspect, in which case they are not verbs, and can form the

Features All features are encoded as binary fea-
tures indicating their presence or absence. For
Japanese, we again include case unigrams, and
case bigrams, which encode as distinct features
the for case markers observed thus far.9 We also in-
clude a feature for the last observed case marker.
For both Japanese and German, we normalize the
verbs to the non-past, plain form, both providing
more training data for each verb and simplifying
the job of our classifier.

German case is conveyed primarily through ar-
ticles and pronouns, so we include special fea-
tures for articles. For example, for the sen-
tence “Es wurde ihnen von einem alten Freund
geholfen”, we add the features ART es ihnen and
ART ihnen einem to convey case information be-
yond individual words and bigrams.

Individual tokens are also used as binary features,
as well as token bigrams.

An Example for Every Word In a simultaneous
interpretation, a person or algorithm receives a con-
stant stream of words, and each new word provides
new information that can aid in prediction. Previ-
ous predictive approaches to simultaneous machine
interpretation have taken this approach, and we also
use it here: as each new word is observed, we make
a prediction. This is a generalization of random
presentation of prefixes in the human study.

3.3 Classification Results and Discussion

Better at the End A discriminative classifier
does better than an n-gram classifier, which has
a tendency to over-predict frequent verbs. By the
end of the sentence, accuracy reaches 39.9% for
German (Figure 7) and 29.9% Japanese (Figure 8),
greatly exceeding choosing the most frequent class
baseline of 3.7% (German) and 6.05% (Japanese).
The n-gram language model also outperforms this
baseline, but not by much. It also improves over
the course of the sentence, but the model cannot
reliably predict more than a handful of verbs in
either language.

copula de aru. Distinguishing between all of these cases is
beyond the scope of this study; so, they are excluded. We also
omit duplicates that are spelled differently (i.e., the same word
but spelled without Chinese (kanji) characters and slightly
different forms of the same root).

We also omit the light verb naru (“to become” or “to make
up”) for similar reasons to suru. The increasing trend shown
in the results does not change with their inclusion.

9For instance, given a sentence fragment X-に Y-を, rep-
resenting X-DAT Y-ACC, the case bigram would beに∧を.

101



10

20

30

40

0.25 0.50 0.75 1.00
Revealed

A
cc

ur
ac

y

1+2gms 1gms 1gms+Articles LM

Figure 7: German average prediction accuracy over
the course of sentences. Bigrams help slightly in
the second half of the sentence. Adding special fea-
tures for case-assigning articles to unigrams nearly
matches the performance of adding all bigrams in
the final 10%. All handily outperform the trigram
language model.

Richer Features Help (Mostly at the End) Bi-
gram features help both languages, but Japanese
more than German; beyond bigrams, however, tri-
grams and longer features overfit the training data
and hurt performance. The better performance for
Japanese bigrams is likely because word bound-
aries are not well-defined in Japanese, and individ-
ual morphemes can combine in ways that signifi-
cantly add information. German word boundaries
are more precise and words (particularly nouns)
can carry substantial information themselves.

Richer features matter more toward the end of
the sentence. In Japanese, adding bigrams con-
sistently outperforms unigrams alone, but in both
languages, adding special features for tokens with
case information helps almost as much as adding
the full set of bigrams. In Japanese, case mark-
ings always immediately follow the words marked,
and in German the articles precede the nouns to
which they assign case; thus, rather than relying
on isolated unigrams, using bigrams provides op-
portunities to encode case-marked words that more
narrowly select for verbs. In Japanese, the differ-
ences are more pronounced toward the very end of
the sentences (and less so in German).

Richer features help more at the end, but not
merely because the last words of the sentence rep-
resent the densest feature vectors. In Japanese, the

10

20

30

0.25 0.50 0.75 1.00
Revealed

A
cc

ur
ac

y

1+2gms 1gms 1gms+Case LM

Figure 8: Japanese average prediction accuracy
over the course of sentences. Adding bigrams con-
sistently outperforms unigrams alone in Japanese,
possibly due to the agglutinative nature of the lan-
guage. The accuracies diverge the most toward the
end of the sentences: Adding only explicit case
markers to unigrams nearly matches performance
of adding all bigrams toward the end. All outper-
form the trigram language model.

last word is usually a case-marked noun phrase or
adverb that matches the main predicate. The final
word is therefore immune to subclause interfer-
ence and must modify the final verb, boosting the
classifier performance in these final positions and
amplifying the predictive discrepancies between
the various feature sets. Accuracy spikes at the
end of Japanese sentences, where case informa-
tion helps nearly as much as adding the entire set
of bigrams, further supporting case information’s
importance. Deeper processing—e.g., separating
case-marked words in subclauses from those in the
main clause—would likely be more useful. Fea-
tures and feature-selection strategies that we tried
which did not help included the following: adding
only case marker unigrams (instead of bigrams);
filtering the features by using only case-marked
words; only allowing one word per case marker in
the feature vector (the most recent); using decay-
ing weights on features further in the past; adding
part-of-speech tag n-grams; and adding the word
nearest to the centroid of the observed context in
a word embedding space. While these features
may have potential, they did not lead to meaningful
increases in accuracy in our experiments.

102



4 Related Work

While to our knowledge our work is the first in-
depth study of incremental verb prediction, it is
not the first study of verb prediction in humans or
machines. This section reviews that related work.

Human Verb Prediction Prediction is easier
with more context and explicit case markings. Ter-
amura (1987) shows that next word prediction in
Japanese improves as more words are incremen-
tally revealed. While only looking at verb pre-
diction given the complete preceding context, Ya-
mashita (1997) finds that scrambling word order
in Japanese—a case rich language that allows such
scrambling—does not harm final verb prediction,
but that explicit case marking helps final verb pre-
diction. Our results show that this is true even
for incremental verb prediction. Levy and Keller
(2013) also find that dative markers aid German
verb prediction.

Neurolinguistic measurements by Friederici and
Frisch (2000) suggest processing verb-final clauses
in German use both semantic and syntactic infor-
mation, but that they are processed differently. In
Japanese, Koso et al. (2011) measure the effect
of case markings on predicting verbs with strong
case preferences. This is consistent with our use
of case-based features and suggests that further
gains are possible using richer syntactic representa-
tions. Chow et al. (2015) use N400 measurements
to investigate two competing hypotheses for the
initial prediction of an upcoming verb: whether
predictions are dependent on all words equally (the
Bag-of-words hypothesis), or alternatively, whether
prediction is selectively modulated by the final
verb’s arguments (the Bag-of-arguments hypoth-
esis). They argue for the latter.

The literature on incremental verb prediction is
sparse. A key finding of Matsubara et al. (2002)
is that Japanese-English simultaneous interpreters,
when given access to lecture slides, would refer to
them to predict the next phrase.

Prediction for Simultaneous Machine Transla-
tion The Verbmobil simultaneous translation sys-
tem (Kay et al., 1992) uses deleted interpolation (Je-
linek, 1990) to create a weighted n-gram mod-
els to predict dialogue acts—almost identical to
predicting the next word (Reithinger et al., 1996).
Konieczny and Döring (2003) predict verbs with
a recurrent neural network, but Matsubara et al.
(2000) was the first to use verb predictions as

part of a simultaneous interpretation system. They
use pattern matching-based predictions of English
verbs. In contrast, Grissom II et al. (2014) use a
statistical approach, using n-gram models to pre-
dict German verbs and particles (in Section 3 we
show that this model predicts verbs poorly). How-
ever, their simultaneous translation system is able
to learn when to trust these predictions. Oda et
al. (2015) extend the idea of using prediction by
predicting entire syntactic constituents for English-
Japanese simultaneous machine translation. Both
systems will likely benefit from our improved verb
prediction presented here.

5 Conclusion

Verb prediction is hard for both machines and hu-
mans but impossible for neither. Verbs become
more predictable in discriminative settings as more
of the sentence is revealed, and when all of the prior
context is available, the verbs are highly predictable
by humans when a limited number of choices is
available, though even then not perfectly so. While
we make no claims concerning upper or lower
bounds of predictability in different settings, our
dataset provides benchmarks for future verb pre-
diction research on publicly available corpora: cog-
nitive scientists can validate prediction, confusion,
and anticipation; engineers have a human bench-
mark for their systems; and linguists can conduct
future experiments on predictability. Shallow fea-
tures can be used to predict verbs more accurately
with more context. Improving verb prediction can
benefit simultaneous translations systems that have
already shown to benefit from verb predictions, as
well as enable new applications that involve pre-
dicting future linguistic input.

6 Acknowledgments

We would like to thank the anonymous reviewers
for their comments. We thank Yusuke Miyao for
his helpful support. We would also like to thank
James H. Martin, Martha Palmer, Hal Daumé III,
Mans Hulden, Mohit Iyyer, John Morgan, Shota
Momma, Graham Neubig, and Sho Hoshino for
their invaluable discussions and input. This work
was supported by NSF grant IIS-1320538. Boyd-
Graber is also partially supported by NSF grants
CCF-1409287 and NCSE-1422492. Any opinions,
findings, conclusions, or recommendations ex-
pressed here are those of the authors and do not
necessarily reflect the view of the sponsor.

103



References
Chris Biemann, Gerhard Heyer, Uwe Quasthoff, and

Matthias Richter. 2007. The Leipzig corpora
collection-monolingual corpora of standard size.
Proceedings of Corpus Linguistics.

Wing-Yee Chow, Cybelle Smith, Ellen Lau, and Colin
Phillips. 2015. A “bag-of-arguments” mechanism
for initial verb predictions. Language, Cognition
and Neuroscience, pages 1–20.

Yasuhara Den and Masakatsu Inoue. 1997. Disam-
biguation with verb-predictability: Evidence from
Japanese garden-path phenomena. In Proceedings
of the Cognitive Science Society, pages 179–184.
Lawrence Erlbaum.

Angela D Friederici and Stefan Frisch. 2000. Verb
argument structure processing: The role of verb-
specific and argument-specific information. Journal
of Memory and Language, 43(3):476–507.

Alvin C. Grissom II, He He, Jordan Boyd-Graber, John
Morgan, and Hal Daumé III. 2014. Don’t until the
final verb wait: Reinforcement learning for simulta-
neous machine translation. In Empirical Methods in
Natural Language Processing.

John Hale. 2001. A probabilistic earley parser as a
psycholinguistic model. In Conference of the North
American Chapter of the Association for Computa-
tional Linguistics.

Fred Jelinek. 1990. Self-organized language modeling
for speech recognition. Readings in speech recogni-
tion, pages 450–506.

Martin Kay, Peter Norvig, and Mark Gawron. 1992.
Verbmobil: A translation system for face-to-face di-
alog. University of Chicago Press.

Lars Konieczny and Philipp Döring. 2003. Antic-
ipation of clause-final heads: Evidence from eye-
tracking and srns. In Proceedings of iccs/ascs.

Ayumi Koso, Shiro Ojima, and Hiroko Hagiwara.
2011. An event-related potential investigation of
lexical pitch-accent processing in auditory Japanese.
Brain research, 1385:217–228.

Taku Kudo. 2005. Mecab: Yet another part-of-speech
and morphological analyzer. http://mecab. source-
forge. net/.

Sadao Kurohashi and Makoto Nagao. 1994. Kn parser:
Japanese dependency/case structure analyzer. In
Proceedings of the Workshop on Sharable Natural
Language Resources.

Marta Kutas, Katherine A DeLong, and Nathaniel J
Smith. 2011. A look around at what lies ahead:
prediction and predictability in language processing.
Predictions in the brain: Using our past to generate
a future, pages 190–207.

John Langford, Lihong Li, and Alex Strehl. 2007.
Vowpal wabbit online learning project.

Roger P Levy and Frank Keller. 2013. Expectation
and locality effects in german verb-final structures.
Journal of memory and language, 68(2):199–222.

Shigeki Matsubara, Keiichi Iwashima, Nobuo
Kawaguchi, Katsuhiko Toyama, and Yasuyoshi
Inagaki. 2000. Simultaneous Japanese-English in-
terpretation based on early predition of English verb.
In Symposium on Natural Language Processing.

Shigeki Matsubara, Akira Takagi, Nobuo Kawaguchi,
and Yasuyoshi Inagaki. 2002. Bilingual spoken
monologue corpus for simultaneous machine inter-
pretation research. In LREC.

Shota Momma, L Robert Slevc, and Colin Phillips.
2015. The timing of verb selection in japanese sen-
tence production. Journal of experimental psychol-
ogy. Learning, memory, and cognition.

Graham Neubig. 2011. The Kyoto free translation task.
Available online at http://www. phontron. com/kftt.

Yusuke Oda, Graham Neubig, Sakriani Sakti, Tomoki
Toda, and Satoshi Nakamura. 2015. Syntax-based
simultaneous translation through prediction of un-
seen syntactic constituents. Proceedings of the As-
sociation for Computational Linguistics, June.

Norbert Reithinger, Ralf Engel, Michael Kipp, and
Martin Klesen. 1996. Predicting dialogue acts for
a speech-to-speech translation system. volume 2,
pages 654–657. IEEE.

Claude Elwood Shannon. 1948. A mathematical the-
ory of communication. Bell Systems Technical Jour-
nal, 27:379–423, 623–656.

Hideo Teramura. 1987. Kikitori ni okeru yosoku
nouryoku to bunpouteki tisiki. Nihongogaku,
3;6:56–68.

Kristina Toutanova, Dan Klein, Christopher D Man-
ning, and Yoram Singer. 2003. Feature-rich part-
of-speech tagging with a cyclic dependency network.
In Conference of the North American Chapter of the
Association for Computational Linguistics, pages
173–180.

Hiroko Yamashita. 1997. The effects of word-order
and case marking information on the processing
of Japanese. Journal of Psycholinguistic Research,
26(2):163–188.

Hiroko Yamashita. 2000. Structural computation and
the role of morphological markings in the processing
of japanese. Language and speech, 43(4):429–455.

104


