



















































Learning Structured Perceptrons for Coreference Resolution with Latent Antecedents and Non-local Features


Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 47–57,
Baltimore, Maryland, USA, June 23-25 2014. c©2014 Association for Computational Linguistics

Learning Structured Perceptrons for Coreference Resolution
with Latent Antecedents and Non-local Features

Anders Björkelund and Jonas Kuhn
Institute for Natural Language Processing

University of Stuttgart
{anders,jonas}@ims.uni-stuttgart.de

Abstract

We investigate different ways of learning
structured perceptron models for coref-
erence resolution when using non-local
features and beam search. Our experi-
mental results indicate that standard tech-
niques such as early updates or Learning
as Search Optimization (LaSO) perform
worse than a greedy baseline that only uses
local features. By modifying LaSO to de-
lay updates until the end of each instance
we obtain significant improvements over
the baseline. Our model obtains the best
results to date on recent shared task data
for Arabic, Chinese, and English.

1 Introduction

This paper studies and extends previous work us-
ing the structured perceptron (Collins, 2002) for
complex NLP tasks. We show that for the task of
coreference resolution the straightforward combi-
nation of beam search and early update (Collins
and Roark, 2004) falls short of more limited fea-
ture sets that allow for exact search. This contrasts
with previous work on, e.g., syntactic parsing
(Collins and Roark, 2004; Huang, 2008; Zhang
and Clark, 2008) and linearization (Bohnet et
al., 2011), and even simpler structured prediction
problems, where early updates are not even nec-
essary, such as part-of-speech tagging (Collins,
2002) and named entity recognition (Ratinov and
Roth, 2009).

The main reason why early updates underper-
form in our setting is that the task is too difficult
and that the learning algorithm is not able to profit
from all training data. Put another way, early up-
dates happen too early, and the learning algorithm
rarely reaches the end of the instances as it halts,
updates, and moves on to the next instance.

An alternative would be to continue decod-
ing the same instance after the early updates,

which is equivalent to Learning as Search Opti-
mization (LaSO; Daumé III and Marcu (2005b)).
The learning task we are tackling is however
further complicated since the target structure is
under-determined by the gold standard annotation.
Coreferent mentions in a document are usually an-
notated as sets of mentions, where all mentions in
a set are coreferent. We adopt the recently pop-
ularized approach of inducing a latent structure
within these sets (Fernandes et al., 2012; Chang et
al., 2013; Durrett and Klein, 2013). This approach
provides a powerful boost to the performance of
coreference resolvers, but we find that it does not
combine well with the LaSO learning strategy. We
therefore propose a modification to LaSO, which
delays updates until after each instance. The com-
bination of this modification with non-local fea-
tures leads to further improvements in the cluster-
ing accuracy, as we show in evaluation results on
all languages from the CoNLL 2012 Shared Task –
Arabic, Chinese, and English. We obtain the best
results to date on these data sets.1

2 Background

Coreference resolution is the task of grouping re-
ferring expressions (or mentions) in a text into dis-
joint clusters such that all mentions in a cluster
refer to the same entity. An example is given in
Figure 1 below, where mentions from two clusters
are marked with brackets:

[Drug Emporium Inc.]a1 said [Gary Wilber]b1 was
named CEO of [this drugstore chain]a2 . [He]b2 suc-
ceeds his father, Philip T. Wilber, who founded [the
company]a3 and remains chairman. Robert E. Lyons
III, who headed the [company]a4 ’s Philadelphia re-
gion, was appointed president and chief operating offi-
cer, succeeding [Gary Wilber]b3 .

Figure 1: An excerpt of a document with the men-
tions from two clusters marked.

1Our system is available at http://www.ims.
uni-stuttgart.de/˜anders/coref.html

47



In recent years much work on coreference res-
olution has been devoted to increasing the ex-
pressivity of the classical mention-pair model, in
which each coreference classification decision is
limited to information about two mentions that
make up a pair. This shortcoming has been ad-
dressed by entity-mention models, which relate a
candidate mention to the full cluster of mentions
predicted to be coreferent so far (for more discus-
sion on the model types, see, e.g., (Ng, 2010)).

Nevertheless, the two best systems in the lat-
est CoNLL Shared Task on coreference resolu-
tion (Pradhan et al., 2012) were both variants of
the mention-pair model. While the second best
system (Björkelund and Farkas, 2012) followed
the widely used baseline of Soon et al. (2001), the
winning system (Fernandes et al., 2012) proposed
the use of a tree representation.

The tree-based model of Fernandes et al. (2012)
construes the representation of coreference clus-
ters as a rooted tree. Figure 2 displays an example
tree over the clusters from Figure 1. Every men-
tion corresponds to a node in the tree, and arcs be-
tween mentions indicate that they are coreferent.
The tree additionally has a dummy root node. Ev-
ery subtree under the root node corresponds to a
cluster of coreferent mentions.

Since coreference training data is typically not
annotated with trees, Fernandes et al. (2012) pro-
posed the use of latent trees that are induced dur-
ing the training phase of a coreference resolver.
The latent tree provides more meaningful an-
tecedents for training.2 For instance, the popular
pair-wise instance creation method suggested by
Soon et al. (2001) assumes non-branching trees,
where the antecedent of every mention is its lin-
ear predecessor (i.e., heb2 is the antecedent of
Gary Wilberb3). Comparing the two alternative
antecedents of Gary Wilberb3 , the tree in Fig-
ure 2 provides a more reliable basis for training a
coreference resolver, as the two mentions of Gary
Wilber are both proper names and have an exact
string match.

3 Representation and Learning

LetM = {m0,m1, ...,mn} denote the set of men-
tions in a document, including the artificial root
mention (denoted by m0). We assume that the

2We follow standard practice and overload the terms
anaphor and antecedent to be any type of mention, i.e., names
as well as pronouns. An antecedent is simply the mention to
the left of the anaphor.

Drug Emporium Inc.a1

the companya3this drugstore chaina2

Gary Wilberb1

Heb2 Gary Wilberb3

root

companya4

Figure 2: A tree representation of Figure 1.

mentions are ordered ascendingly with respect to
the linear order of the document, where the docu-
ment root precedes all other mentions.3 For each
mention mj , let Aj denote the set of potential an-
tecedents. That is, the set of all mentions that
precede mj according to the linear order includ-
ing the root node, or, Aj = {mi | i < j}. Fi-
nally, let A denote the set of all antecedent sets
{A0, A1, ..., An}.

In the tree model, each mention corresponds to
a node, and an antecedent-anaphor pair 〈ai,mi〉,
where ai ∈ Ai, corresponds to a directed edge (or
arc) pointing from antecedent to anaphor.

The score of an arc 〈ai,mi〉 is defined as
the scalar product between a weight vector w
and a feature vector Φ(〈ai,mi〉), where Φ is
a feature extraction function over an arc (thus
extracting features from the antecedent and the
anaphor). The score of a coreference tree y =
{〈a1,m1〉, 〈a2,m2〉, ..., 〈an,mn〉} is defined as
the sum of the scores of all the mention pairs:

score(〈ai,mi〉) = w · Φ(〈ai,mi〉) (1)
score(y) =

∑
〈ai,mi〉∈y

score(〈ai,mi〉)

The objective is to find the output ŷ that maxi-
mizes the scoring function:

ŷ = arg max
y∈Y(A)

score(y) (2)

where Y(A) denotes the set of possible trees given
the antecedent sets A. By treating the mentions as
nodes in a directed graph and assigning scores to
the arcs according to (1), Fernandes et al. (2012)
solved the search problem using the Chu-Liu-
Edmonds (CLE) algorithm (Chu and Liu, 1965;

3We impose a total order on mentions. In case of nested
mentions, the mention that begins first is assumed to precede
the embedded one. If two mentions begin at the same token,
the longer one is taken to precede the shorter one.

48



Edmonds, 1967), which is a maximum spanning
tree algorithm that finds the optimal tree over a
connected directed graph. CLE, however, has the
drawback that the scores of the arcs must remain
fixed and can not change depending on other arcs
and it is not clear how to include non-local features
in a CLE decoder.

3.1 Online learning
We find the weight vector w by online learning us-
ing a variant of the structured perceptron (Collins,
2002). Specifically, we use the passive-aggressive
(PA) algorithm (Crammer et al., 2006), since we
found that this performed slightly better in prelim-
inary experiments.4

The structured perceptron iterates over train-
ing instances 〈xi, yi〉, where xi are inputs and yi
are outputs. For each instance it uses the current
weight vector w to make a prediction ŷi given the
input xi. If the prediction is incorrect, the weight
vector is updated in favor of the correct structure.
Otherwise the weight vector is left untouched. In
our setting inputs xi correspond to documents and
outputs yi are trees over mentions in a document.
The training data is, however, not annotated with
trees, but only with clusters of mentions. That is,
the yi’s are not defined a priori.

3.2 Latent antecedents
In order to have a tree structure to update against,
we use the current weight vector and apply the
decoder to a constrained antecedent set and ob-
tain a latent tree over the mentions in a docu-
ment, where each mention is assigned a single cor-
rect antecedent (Fernandes et al., 2012). We con-
strain the antecedent sets such that only trees that
correspond to the correct clustering can be built.
Specifically, let Ãj denote the set of correct an-
tecedents for a mention mj , or

Ãj =

{
{m0} if mj has no correct antecedent
{ai | COREF(ai,mj), ai ∈ Aj} otherwise

that is, if mention mj is non-referential or the first
mention of its cluster, Ãj contains only the docu-
ment root. Otherwise it is the set of all mentions
to the left that belong to the same cluster as mj .
Analogously to A, let Ã denote the set of con-
strained antecedent sets. The latent tree ỹ needed

4We also implement the feature mapping function Φ as
a hash kernel (Bohnet, 2010) and apply averaging (Collins,
2002), though for brevity we omit this from the pseudocode.

for updates is then defined to be the optimal tree
over Y(Ã), subject to the current weight vector:

ỹ = arg max
y∈Y(Ã)

score(y)

The intuition behind the latent tree is that during
online learning, the weight vector will start favor-
ing latent trees that are easier to learn (such as the
one in Figure 2).

Algorithm 1 PA algorithm with latent trees
Input: Training data D, number of iterations T
Output: Weight vector w
1: w =

−→
0

2: for t ∈ 1..T do
3: for 〈Mi,Ai, Ãi〉 ∈ D do
4: ŷi = arg maxY(A) score(y) . Predict
5: if ¬CORRECT(ŷi) then
6: ỹi = arg maxY(Ã) score(y) . Latent tree
7: ∆ = Φ(ŷi)− Φ(ỹi)
8: τ = ∆·w+LOSS(ŷi)‖∆‖2 . PA weight
9: w = w + τ∆ . PA update

10: return w

Algorithm 1 shows pseudocode for the learn-
ing algorithm, which we will refer to as the base-
line learning algorithm. Instead of looping over
pairs 〈x, y〉 of documents and trees, it loops over
triples 〈M,A, Ã〉 that comprise the set of men-
tions M and the two sets of antecedent candidates
(line 3). Moreover, rather than checking that the
tree is identical to the latent tree, it only requires
the tree to correctly encode the gold clustering
(line 5). The update that occurs in lines 7-9 is the
passive-aggressive update. A loss function LOSS
that quantifies the error in the prediction is used
to compute a scalar τ that controls how much the
weights are moved in each update. If τ is set to 1,
the update reduces to the standard structured per-
ceptron update. The loss function can be an arbi-
trarily complex function that returns a numerical
value of how bad the prediction is. In the sim-
plest case, Hamming loss can be used, i.e., for
each incorrect arc add 1. We follow Fernandes
et al. (2012) and penalize erroneous root attach-
ments, i.e., mentions that erroneously get the root
node as their antecedent, with a loss of 1.5. For all
other arcs we use Hamming loss.

4 Incremental Search

We now show that the search problem in (2) can
equivalently be solved by the more intuitive best-
first decoder (Ng and Cardie, 2002), rather than
using the CLE decoder. The best-first decoder

49



works incrementally by making a left-to-right pass
over the mentions, selecting for each mention the
highest scoring antecedent.

The key aspect that makes the best-first decoder
equivalent to the CLE decoder is that all arcs point
from left to right, both in this paper and in the work
of Fernandes et al. (2012). We sketch a proof that
this decoder also returns the highest scoring tree.

First, note that this algorithm indeed returns a
tree. This can be shown by assuming the opposite,
in which case the tree has to have a cycle. Then
there must be a mention that has its antecedent to
the right. Though this is not possible since all arcs
point from left to right.

Second, this tree is the highest scoring tree.
Again, assume the contrary, i.e., that there is a
higher scoring tree in Y(A). This implies that for
some mention there is a higher scoring antecedent
than the one selected by the decoder. This contra-
dicts the fact that the best-first decoder selects the
highest scoring antecedent for each mention.5

5 Introducing Non-local Features

Since the best-first decoder makes a left-to-right
pass, it is possible to extract features on the partial
structure on the left. Such non-local features are
able to capture information beyond that of a men-
tion and its potential antecedent, e.g., the size of
a partially built cluster, or features extracted from
the antecedent of the antecedent.

When only local features are used, greedy
search (either with CLE or the best-first decoder)
suffices to find the highest scoring tree. That is,
greedy search provides an exact solution to equa-
tion 2. Non-local features, however, render the ex-
act search problem intractable. This is because
with non-local features, locally suboptimal (i.e.,
non-greedy) antecedents for some mentions may
lead to a higher total score over a whole document.

In order to keep some options around during
search, we extend the best-first decoder with beam
search. Beam search works incrementally by
keeping an agenda of state items. At each step,
all items on the agenda are expanded. The subset
of size k (the beam size) of the highest scoring ex-
pansions are retained and put back into the agenda
for the next step. The feature extraction function Φ

5In case there are multiple maximum spanning trees, the
best-first decoder will return one of them. This also holds for
the CLE algorithm. With proper definitions, the proof can be
constructed to show that both search algorithms return trees
belonging to the set of maximum spanning trees over a graph.

is also extended such that it also receives the cur-
rent state s as an argument: Φ(〈mi,mj〉, s). The
state encodes the previous decisions and enables Φ
to extract features from the partial tree on the left.

We now outline three different ways of learning
the weight vector w with non-local features.

5.1 Early updates

The beam search decoder can be plugged into the
training algorithm, replacing the calls to arg max.
Since state items leading to the best tree may
be pruned from the agenda before the decoder
reaches the end of the document, the introduc-
tion of non-local features may cause the decoder
to return a non-optimal tree. This is problem-
atic as it might cause updates although the correct
tree has a higher score than the predicted one. It
has previously been observed (Huang et al., 2012)
that substantial gains can be made by applying an
early update strategy (Collins and Roark, 2004):
if the correct item is pruned before reaching the
end of the document, then stop and update.

While beam search and early updates have been
successfully applied to other NLP applications,
our task differs in two important aspects: First,
coreference resolution is a much more difficult
task, which relies on more (world) knowledge than
what is available in the training data. In other
words, it is unlikely that we can devise a feature
set that is informative enough to allow the weight
vector to converge towards a solution that lets the
learning algorithm see the entire documents dur-
ing training, at least in the situation when no ex-
ternal knowledge sources are used.

Second, our gold structure is not known but
is induced latently, and may vary from iteration
to iteration. With non-local features this is trou-
blesome since the best latent tree of a complete
document may not necessarily coincide with the
best partial tree at some intermediate mentionmj ,
j < n, i.e., a mention before the last in a docu-
ment. We therefore also apply beam search to find
the latent tree to have a partial gold structure for
every mention in a document.

Algorithm 2 shows pseudocode for the beam
search and early update training procedure. The
algorithm maintains two parallel agendas, one for
gold items and one for predicted items. At ev-
ery mention, both agendas are expanded and thus
cover the same set of mentions. Then the predicted
agenda is checked to see if it contains any correct

50



Algorithm 2 Beam search and early update
Input: Data set D, epochs T , beam size k
Output: weight vector w
1: w =

−→
0

2: for t ∈ 1..T do
3: for 〈Mi,Ai, Ãi〉 ∈ D do
4: AgendaG = {}
5: AgendaP = {}
6: for j ∈ 1..n do
7: AgendaG = EXPAND(AgendaG , Ãj ,mj , k)
8: AgendaP = EXPAND(AgendaP , Aj ,mj , k)
9: if ¬CONTAINSCORRECT(AgendaP ) then

10: ỹ = EXTRACTBEST(AgendaG)
11: ŷ = EXTRACTBEST(AgendaP )
12: update . PA update
13: GOTO 3 . Skip and move to next instance
14: ŷ = EXTRACTBEST(AgendaP )
15: if ¬CORRECT(ŷ) then
16: ỹ = EXTRACTBEST(AgendaG)
17: update . PA update

item. If there is no correct item in the predicted
agenda, search is halted and an update is made
against the best item from the gold agenda. The
algorithm then moves on to the next document. If
the end of a document is reached, the top scoring
predicted item is checked for correctness. If it is
not, an update is made against the best gold item.

A drawback of early updates is that the remain-
der of the document is skipped when an early up-
date is applied, effectively discarding some train-
ing data.6 An alternative strategy that makes bet-
ter use of the training data is to apply the max-
violation procedure suggested by Huang et al.
(2012). However, since our gold trees change from
iteration to iteration, and even inside of a single
document, it is not entirely clear with respect to
what gold tree the maximum violation should be
computed. Initial experiments with max-violation
updates indicated that they did not improve much
over early updates, and also had a tendency to only
consider a smaller portion of the training data.

5.2 LaSO

To make full use of the training data we imple-
mented Learning as Search Optimization (LaSO;
Daumé III and Marcu, 2005b). It is very similar
to early updates, but differs in one crucial respect:
When an early update is made, search is continued
rather than aborted. Thus the learning algorithm
always reaches the end of a document, avoiding
the problem that early updates discard parts of the
training data.

6In fact, after 50 iterations about 70% of the mentions in
the training data are still being ignored due to early updates.

Correct items are computed the same way as
with early updates, where an agenda of gold items
is maintained in parallel. When search is resumed
after an intermediate LaSO update, the prediction
agenda is re-seeded with gold items (i.e., items
that are all correct). This is necessary since the
update influences what the partial gold structure
looks like, and the gold agenda therefore needs to
be recreated from the beginning of the document.
Specifically, after each intermediate LaSO update,
the gold agenda is expanded repeatedly from the
beginning of the document to the point where the
update was made, and is then copied over to seed
the prediction agenda. In terms of pseudocode,
this is accomplished by replacing lines 12 and 13
in Algorithm 2 with the following:
12: update . PA update
13: AgendaG = {}
14: for mi ∈ {m1, ...,mj} . Recreate gold agenda
15: AgendaG = EXPAND(AgendaG , Ãi,mi, k)
16: AgendaP = COPY(AgendaG)
17: GOTO 6 . Continue

5.3 Delayed LaSO updates

When we applied LaSO, we noticed that it per-
formed worse than the baseline learning algorithm
when only using local features. We believe that the
reason is that updates are made in the middle of
documents which means that lexical forms of an-
tecedents are “fresh in memory” of the weight vec-
tor. This results in fewer mistakes during training
and leads to fewer updates. While this feedback
makes it easier during training, such feedback is
not available during test time, and the LaSO learn-
ing setting therefore mimics the testing setting to
a lesser extent.

We also found that LaSO updates change the
shape of the latent tree and that the average dis-
tance between mentions connected by an arc in-
creased. This problem can also be attributed to
how lexical items are fresh in memory. Such trees
tend to deviate from the intuition that the latent
trees are easier to learn. They also render distance-
based features (which are standard practice and
generally rather useful) less powerful, as distance
in sentences or mentions becomes less of a reliable
indicator for coreference.

To cope with this problem, we devised the
delayed LaSO update, which differs from LaSO
only in the respect that it postpones the actual up-
dates until the end of a document. This is accom-
plished by summing the distance vectors ∆ at ev-
ery point where LaSO would make an update. At

51



Algorithm 3 Delayed LaSO update
Input: Data set D, iterations T , beam size k
Output: weight vector w
1: w =

−→
0

2: for t ∈ 1..T do
3: for 〈Mi,Ai, Ãi〉 ∈ D do
4: AgendaG = {}
5: AgendaP = {}
6: ∆acc =

−→
0

7: lossacc = 0
8: for j ∈ 1..n do
9: AgendaG = EXPAND(AgendaG , Ãj ,mj , k)

10: AgendaP = EXPAND(AgendaP , Aj ,mj , k)
11: if ¬CONTAINSCORRECT(AgendaP ) then
12: ỹ = EXTRACTBEST(AgendaG)
13: ŷ = EXTRACTBEST(AgendaP )
14: ∆acc = ∆acc + Φ(ŷ)− Φ(ỹ)
15: lossacc = lossacc + LOSS(ŷ)
16: AgendaP = AgendaG
17: ŷ = EXTRACTBEST(AgendaP )
18: if ¬CORRECT(ŷ) then
19: ỹ = EXTRACTBEST(AgendaG)
20: ∆acc = ∆acc + Φ(ŷ)− Φ(ỹ)
21: lossacc = lossacc + LOSS(ŷ)
22: if ∆acc 6= −→0 then
23: update w.r.t. ∆acc and lossacc

the end of a document, an update is made with re-
spect to the sum of all ∆’s. Similarly, a running
sum of the partial loss is maintained within a doc-
ument. Since the PA update only depends on the
distance vector ∆ and the loss, it can be applied
with respect to these sums at the end of the doc-
ument. When only local features are used, this
update is equivalent to the updates in the baseline
learning algorithm. This follows because greedy
search finds the optimal tree when only local fea-
tures are used. Similarly, using only local features,
the beam-based best-first decoder will also return
the optimal tree. Algorithm 3 shows the pseu-
docode for the delayed LaSO learning algorithm.

6 Features

In this section we briefly outline the type of fea-
tures we use. The feature sets are customized for
each language. As a baseline we use the features
from Björkelund and Farkas (2012), who ranked
second in the 2012 CoNLL shared task and is pub-
licly available. The exact definitions and feature
sets that we use are available as part of the down-
load package of our system.

6.1 Local features

Basic features that can be extracted on one or
both mentions in a pair include (among oth-
ers): Mention type, which is either root, pro-

noun, name, or common; Distance features, e.g.,
the distance in sentences or mentions; Rule-based
features, e.g., StringMatch or SubStringMatch;
Syntax-based features, e.g., category labels or
paths in the syntax tree; Lexical features, e.g., the
head word of a mention or the last word of a men-
tion.

In order to have a strong local baseline, we ap-
plied greedy forward/backward feature selection
on the training data using a large set of local fea-
ture templates. Specifically, the training set of
each language was split into two parts where 75%
was used for training, and 25% for testing. Feature
templates were incrementally added or removed
in order to optimize the mean of MUC, B3, and
CEAFe (i.e., the CoNLL average).

6.2 Non-local Features

We experimented with non-local features drawn
from previous work on entity-mention mod-
els (Luo et al., 2004; Rahman and Ng, 2009), how-
ever they did not improve performance in prelimi-
nary experiments. The one exception is the size of
a cluster (Culotta et al., 2007). Additional features
we use are
Shape encodes the linear “shape” of a cluster in
terms of mention type. For instance, the clusters
representing Gary Wilber and Drug Emporium
Inc. from the example in Figure 1, would be repre-
sented as RNPN and RNCCC, respectively. Where
R, N, P, and C denote the root node, names, pro-
nouns, and common noun phrases, respectively.
Local syntactic context is inspired by the Entity
Grid (Barzilay and Lapata, 2008), where the ba-
sic assumption is that references to an entity fol-
low particular syntactic patterns. For instance, an
entity may be introduced as an object in one sen-
tence, whereas in subsequent sentences it is re-
ferred to in subject position. Grammatical func-
tions are approximated by the path in the syntax
tree from a mention to its closest S node. The par-
tial paths of a mention and its linear predecessor,
given the cluster of the current antecedent, informs
the model about the local syntactic context.
Cluster start distance denotes the distance in
mentions from the beginning of the document
where the cluster of the antecedent in considera-
tion begins.

Additionally, the non-local model also has ac-
cess to the basic properties of other mentions in
the partial tree structure, such as head words. The

52



non-local features were selected with the same
greedy forward strategy as the local features, start-
ing from the optimized local feature sets.

7 Experimental Setup

We apply our model to the CoNLL 2012 Shared
Task data, which includes a training, develop-
ment, and test set split for three languages: Ara-
bic, Chinese and English. We follow the closed
track setting where systems may only be trained
on the provided training data, with the exception
of the English gender and number data compiled
by Bergsma and Lin (2006). We use automatically
extracted mentions using the same mention extrac-
tion procedure as Björkelund and Farkas (2012).

We evaluate our system using the CoNLL 2012
scorer, which computes several coreference met-
rics: MUC (Vilain et al., 1995), B3 (Bagga and
Baldwin, 1998), and CEAFe and CEAFm (Luo,
2005). We also report the CoNLL average (also
known as MELA; Denis and Baldridge (2009)),
i.e., the arithmetic mean of MUC, B3, and CEAFe.
It should be noted that for B3 and the CEAF met-
rics, multiple ways of handling twinless mentions7

have been proposed (Rahman and Ng, 2009; Stoy-
anov et al., 2009). We use the most recent ver-
sion of the CoNLL scorer (version 7), which im-
plements the original definitions of these metrics.8

Our system is evaluated on the version of the
data with automatic preprocessing information
(e.g., predicted parse trees). Unless otherwise
stated we use 25 iterations of perceptron training
and a beam size of 20. We did not attempt to tune
either of these parameters. We experiment with
two feature sets for each language: the optimized
local feature sets (denoted local), and the opti-
mized local feature sets extended with non-local
features (denoted non-local).

8 Results

Learning strategies. We begin by looking at the
different learning strategies. Since early updates
do not always make use of the complete docu-
ments during training, it can be expected that it
will require either a very wide beam or more iter-
ations to get up to par with the baseline learning
algorithm. Figure 3 shows the CoNLL average on

7i.e., mentions that appear in the prediction but not in
gold, or the other way around

8Available at http://conll.cemantix.org/
2012/software.html

54

56

58

60

62

64

0 10 20 30 40 50

C
o
N
L
L
 
a
v
g
.

Iterations

Baseline
Early (local), k=20
Early (local), k=100

Early (non-local), k=20
Early (non-local), k=100

Figure 3: Comparing early update training with
the baseline training algorithm.

the English development set as a function of num-
ber of training iterations with two different beam
sizes, 20 and 100, over the local and non-local fea-
ture sets. The figure shows that even after 50 itera-
tions, early update falls short of the baseline, even
when the early update system has access to more
informative non-local features.9

In Figure 4 we compare early update with LaSO
and delayed LaSO on the English development set.
The left half uses the local feature set, and the right
the extended non-local feature set. Recall that with
only local features, delayed LaSO is equivalent to
the baseline learning algorithm. As before, early
update is considerably worse than other learning
strategies. We also see that delayed LaSO out-
performs LaSO, both with and without non-local
features. Note that plain LaSO with non-local fea-
tures only barely outperforms the delayed LaSO
with only local features (i.e., the baseline), which
indicates that only delayed LaSO is able to fully
leverage non-local features. From these results we
conclude that we are better off when the learning
algorithm handles one document at a time, instead
of getting feedback within documents.

Local vs. Non-local feature sets. Table 1 dis-
plays the differences in F-measures and CoNLL
average between the local and non-local systems
when applied to the development sets for each lan-
guage. All metrics improve when more informa-
tive non-local features are added to the local fea-
ture set. Arabic and English show considerable
improvements, and the CoNLL average increases

9Although the Early systems still seem to show slight in-
creases after 50 iterations, it needs a considerable number of
iterations to catch up with the baseline – after 100 iterations
the best early system is still more than half a point behind the
baseline.

53



58

59

60

61

62

63

64

65

Local Non-local

C
o
N
L
L
 
a
v
g
.

Early
LaSO

Delayed LaSO

Figure 4: Comparison of learning algorithms eval-
uated on the English development set.

MUC B3 CEAFm CEAFe CoNLL
Arabic

local 47.33 42.51 49.71 46.49 45.44
non-local 49.31 43.52 50.96 47.18 46.67

Chinese
local 65.84 57.94 62.23 57.05 60.27
non-local 66.4 57.99 62.37 57.12 60.5

English
local 69.95 58.7 62.91 56.03 61.56
non-local 70.74 60.03 65.01 56.8 62.52

Table 1: Comparison of local and non-local fea-
ture sets on the development sets.

about one point. For Chinese the gains are gen-
erally not as pronounced, though the MUC metric
goes up by more than half a point.

Final results. In Table 2 we compare the re-
sults of the non-local system (This paper) to the
best results from the CoNLL 2012 Shared Task.10

Specifically, this includes Fernandes et al.’s (2012)
system for Arabic and English (denoted Fernan-
des), and Chen and Ng’s (2012) system for Chi-
nese (denoted C&N). For English we also com-
pare it to the Berkeley system (Durrett and Klein,
2013), which, to our knowledge, is the best pub-
licly available system for English coreference res-
olution (denoted D&K). As a general baseline, we
also include Björkelund and Farkas’ (2012) sys-
tem (denoted B&F), which was the second best
system in the shared task. For almost all met-
rics our system is significantly better than the best
competitor. For a few metrics the best competitor
outperforms our results for either precision or re-
call, but in terms of F-measures and the CoNLL
average our system is the best for all languages.

10Thanks to Sameer Pradhan for providing us with the out-
puts of the other systems for significance testing.

9 Related Work

On the machine learning side Collins and Roark’s
(2004) work on the early update constitutes our
starting point. The LaSO framework was intro-
duced by Daumé III and Marcu (2005b), but has,
to our knowledge, only been applied to the related
task of entity detection and tracking (Daumé III
and Marcu, 2005a). The theoretical motivation for
early updates was only recently explained rigor-
ously (Huang et al., 2012). The delayed LaSO
update that we propose decomposes the predic-
tion task of a complex structure into a number of
subproblems, each of which guarantee violation,
using Huang et al.’s (2012) terminology. We be-
lieve this is an interesting novelty, as it leverages
the complete structures for every training instance
during every iteration, and expect it to be applica-
ble also to other structured prediction tasks.

Our approach also resembles imitation learning
techniques such as SEARN (Daumé III et al., 2009)
and DAGGER (Ross et al., 2011), where the search
problem is reduced to a sequence of classification
steps that guide the search algorithm through the
search space. These frameworks, however, rely on
the notion of an expert policy which provides an
optimal decision at each point during search. In
our context that would require antecedents for ev-
ery mention to be given a priori, rather than using
latent antecedents as we do.

Perceptrons for coreference. The perceptron
has previously been used to train coreference re-
solvers either by casting the problem as a binary
classification problem that considers pairs of men-
tions in isolation (Bengtson and Roth, 2008; Stoy-
anov et al., 2009; Chang et al., 2012, inter alia) or
in the structured manner, where a clustering for an
entire document is predicted in one go (Fernandes
et al., 2012). However, none of these works use
non-local features. Stoyanov and Eisner (2012)
train an Easy-First coreference system with the
perceptron to learn a sequence of join operations
between arbitrary mentions in a document and ac-
cesses non-local features through previous merge
operations in later stages. Culotta et al. (2007) also
apply online learning in a first-order logic frame-
work that enables non-local features, though using
a greedy search algorithm.

Latent antecedents. The use of latent an-
tecedents goes back to the work of Yu and
Joachims (2009), although the idea of determining

54



MUC B3 CEAFm CEAFe CoNLL
Rec Prec F1 Rec Prec F1 Rec Prec F1 Rec Prec F1 avg.

Arabic

B&F 43.9 52.51 47.82 35.7 49.77 41.58 43.8 50.03 46.71 40.45 41.86 41.15 43.51
Fernandes 43.63 49.69 46.46 38.39 47.7 42.54 47.6 50.85 49.17 48.16 45.03 46.54 45.18
This paper 47.53 53.3 50.25 44.14 49.34 46.6 50.94 55.19 52.98 49.2 49.45 49.33 48.72

Chinese

B&F 58.72 58.49 58.61 49.17 53.2 51.11 56.68 51.86 54.14 55.36 41.8 47.63 52.45
C&N 59.92 64.69 62.21 51.76 60.26 55.69 59.58 60.45 60.02 58.84 51.61 54.99 57.63
This paper 62.57 69.39 65.8 53.87 61.64 57.49 58.75 64.76 61.61 54.65 59.33 56.89 60.06

English

B&F 65.23 70.1 67.58 49.51 60.69 54.47 56.93 59.51 58.19 51.34 49.14 59.21 57.42
Fernandes 65.83 75.91 70.51 51.55 65.19 57.58 57.48 65.93 61.42 50.82 57.28 53.86 60.65
D&K 66.58 74.94 70.51 53.2 64.56 58.33 59.19 66.23 62.51 52.9 58.06 55.36 61.4
This paper 67.46 74.3 70.72 54.96 62.71 58.58 60.33 66.92 63.45 52.27 59.4 55.61 61.63

Table 2: Comparison with other systems on the test sets. Bold numbers indicate significance at the
p < 0.05 level between the best and the second best systems (according to the CoNLL average) using
a Wilcoxon signed rank sum test. We refrain from significance tests on the CoNLL average, as it is an
average over other F-measures.

meaningful antecedents for mentions can be traced
back to Ng and Cardie (2002) who used a rule-
based approach. Latent antecedents have recently
gained popularity and were used by two systems in
the CoNLL 2012 Shared Task, including the win-
ning system (Fernandes et al., 2012; Chang et al.,
2012). Durrett and Klein (2013) present a corefer-
ence resolver with latent antecedents that predicts
clusterings over entire documents and fit a log-
linear model with a custom task-specific loss func-
tion using AdaGrad (Duchi et al., 2011). Chang
et al. (2013) use a max-margin approach to learn
a pairwise model and rely on stochastic gradient
descent to circumvent the costly operation of de-
coding the entire training set in order to compute
the gradients and the latent antecedents. None of
the aforementioned works use non-local features
in their models, however.

Entity-mention models. Entity-mention mod-
els that compare a single mention to a (partial)
cluster have been studied extensively and several
works have evaluated non-local entity-level fea-
tures (Luo et al., 2004; Yang et al., 2008; Rah-
man and Ng, 2009). Luo et al. (2004) also apply
beam search at test time, but use a static assign-
ment of antecedents and learns log-linear model
using batch learning. Moreover, these works al-
ter the basic feature definitions from their pair-
wise models when introducing entity-level fea-
tures. This contrasts with our work, as our
mention-pair model simply constitutes a special
case of the non-local system.

10 Conclusion

We presented experiments with a coreference re-
solver that leverages non-local features to improve
its performance. The application of non-local fea-
tures requires the use of an approximate search al-
gorithm to keep the problem tractable. We eval-
uated standard perceptron learning techniques for
this setting both using early updates and LaSO. We
found that the early update strategy is considerably
worse than a local baseline, as it is unable to ex-
ploit all training data. LaSO resolves this issue by
giving feedback within documents, but still under-
performs compared to the baseline as it distorts the
choice of latent antecedents.

We introduced a modification to LaSO, where
updates are delayed until each document is pro-
cessed. In the special case where only local fea-
tures are used, this method coincides with stan-
dard structured perceptron learning that uses exact
search. Moreover, it is also able to profit from non-
local features resulting in improved performance.
We evaluated our system on all three languages
from the CoNLL 2012 Shared Task and present
the best results to date on these data sets.

Acknowledgments

We are grateful to the anonymous reviewers as
well as Christian Scheible and Wolfgang Seeker
for comments on earlier versions of this paper.
This research has been funded by the DFG via
SFB 732, project D8.

55



References
Amit Bagga and Breck Baldwin. 1998. Algorithms for

scoring coreference chains. In In The First Interna-
tional Conference on Language Resources and Eval-
uation Workshop on Linguistics Coreference, pages
563–566.

Regina Barzilay and Mirella Lapata. 2008. Modeling
local coherence: An entity-based approach. Compu-
tational Linguistics, 34(1):1–34.

Eric Bengtson and Dan Roth. 2008. Understand-
ing the value of features for coreference resolution.
In Proceedings of the 2008 Conference on Empiri-
cal Methods in Natural Language Processing, pages
294–303, Honolulu, Hawaii, October. Association
for Computational Linguistics.

Shane Bergsma and Dekang Lin. 2006. Bootstrapping
path-based pronoun resolution. In Proceedings of
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 33–40,
Sydney, Australia, July. Association for Computa-
tional Linguistics.

Anders Björkelund and Richárd Farkas. 2012. Data-
driven multilingual coreference resolution using re-
solver stacking. In Joint Conference on EMNLP and
CoNLL - Shared Task, pages 49–55, Jeju Island, Ko-
rea, July. Association for Computational Linguistics.

Bernd Bohnet, Simon Mille, Benoı̂t Favre, and Leo
Wanner. 2011. <stumaba >: From deep represen-
tation to surface. In Proceedings of the Generation
Challenges Session at the 13th European Workshop
on Natural Language Generation, pages 232–235,
Nancy, France, September. Association for Compu-
tational Linguistics.

Bernd Bohnet. 2010. Top accuracy and fast depen-
dency parsing is not a contradiction. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics (Coling 2010), pages 89–97, Bei-
jing, China, August.

Kai-Wei Chang, Rajhans Samdani, Alla Rozovskaya,
Mark Sammons, and Dan Roth. 2012. Illinois-
coref: The ui system in the conll-2012 shared task.
In Joint Conference on EMNLP and CoNLL - Shared
Task, pages 113–117, Jeju Island, Korea, July. Asso-
ciation for Computational Linguistics.

Kai-Wei Chang, Rajhans Samdani, and Dan Roth.
2013. A constrained latent variable model for coref-
erence resolution. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing, pages 601–612, Seattle, Washington,
USA, October. Association for Computational Lin-
guistics.

Chen Chen and Vincent Ng. 2012. Combining the
best of two worlds: A hybrid approach to multilin-
gual coreference resolution. In Joint Conference on
EMNLP and CoNLL - Shared Task, pages 56–63,

Jeju Island, Korea, July. Association for Computa-
tional Linguistics.

Yoeng-jin Chu and Tseng-hong Liu. 1965. On the
shortest aborescence of a directed graph. Science
Sinica, 14:1396–1400.

Michael Collins and Brian Roark. 2004. Incremen-
tal parsing with the perceptron algorithm. In Pro-
ceedings of the 42nd Meeting of the Association for
Computational Linguistics (ACL’04), Main Volume,
pages 111–118, Barcelona, Spain, July.

Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings
of the 2002 Conference on Empirical Methods in
Natural Language Processing, pages 1–8. Associ-
ation for Computational Linguistics, July.

Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
passive–aggressive algorithms. Journal of Machine
Learning Reseach, 7:551–585, March.

Aron Culotta, Michael Wick, and Andrew McCallum.
2007. First-order probabilistic models for corefer-
ence resolution. In Human Language Technologies
2007: The Conference of the North American Chap-
ter of the Association for Computational Linguistics;
Proceedings of the Main Conference, pages 81–88,
Rochester, New York, April. Association for Com-
putational Linguistics.

Hal Daumé III and Daniel Marcu. 2005a. A large-
scale exploration of effective global features for a
joint entity detection and tracking model. In Pro-
ceedings of Human Language Technology Confer-
ence and Conference on Empirical Methods in Natu-
ral Language Processing, pages 97–104, Vancouver,
British Columbia, Canada, October. Association for
Computational Linguistics.

Hal Daumé III and Daniel Marcu. 2005b. Learning
as search optimization: approximate large margin
methods for structured prediction. In ICML, pages
169–176.

Hal Daumé III, John Langford, and Daniel Marcu.
2009. Search-based structured prediction. Machine
Learning, 75(3):297–325.

Pascal Denis and Jason Baldridge. 2009. Global Joint
Models for Coreference Resolution and Named En-
tity Classification. In Procesamiento del Lenguaje
Natural 42, pages 87–96, Barcelona: SEPLN.

John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. J. Mach. Learn. Res.,
12:2121–2159, July.

Greg Durrett and Dan Klein. 2013. Easy victories and
uphill battles in coreference resolution. In Proceed-
ings of the 2013 Conference on Empirical Methods
in Natural Language Processing, pages 1971–1982,

56



Seattle, Washington, USA, October. Association for
Computational Linguistics.

Jack Edmonds. 1967. Optimum branchings. Jour-
nal of Research of the National Bureau of Standards,
71(B):233–240.

Eraldo Fernandes, Cı́cero dos Santos, and Ruy Milidiú.
2012. Latent structure perceptron with feature in-
duction for unrestricted coreference resolution. In
Joint Conference on EMNLP and CoNLL - Shared
Task, pages 41–48, Jeju Island, Korea, July. Associ-
ation for Computational Linguistics.

Liang Huang, Suphan Fayong, and Yang Guo. 2012.
Structured perceptron with inexact search. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
142–151, Montréal, Canada, June. Association for
Computational Linguistics.

Liang Huang. 2008. Forest reranking: Discrimina-
tive parsing with non-local features. In Proceedings
of ACL-08: HLT, pages 586–594, Columbus, Ohio,
June. Association for Computational Linguistics.

Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, Nanda
Kambhatla, and Salim Roukos. 2004. A mention-
synchronous coreference resolution algorithm based
on the bell tree. In Proceedings of the 42nd Meet-
ing of the Association for Computational Linguis-
tics, pages 135–142, Barcelona, Spain, July.

Xiaoqiang Luo. 2005. On coreference resolution
performance metrics. In Proceedings of Human
Language Technology Conference and Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 25–32, Vancouver, British Columbia,
Canada, October. Association for Computational
Linguistics.

Vincent Ng and Claire Cardie. 2002. Improving ma-
chine learning approaches to coreference resolution.
In Proceedings of 40th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 104–
111, Philadelphia, Pennsylvania, USA, July. Asso-
ciation for Computational Linguistics.

Vincent Ng. 2010. Supervised noun phrase coref-
erence research: The first fifteen years. In Pro-
ceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 1396–
1411, Uppsala, Sweden, July. Association for Com-
putational Linguistics.

Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. Conll-
2012 shared task: Modeling multilingual unre-
stricted coreference in ontonotes. In Joint Confer-
ence on EMNLP and CoNLL - Shared Task, pages
1–40, Jeju Island, Korea, July. Association for Com-
putational Linguistics.

Altaf Rahman and Vincent Ng. 2009. Supervised mod-
els for coreference resolution. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing, pages 968–977, Singapore,
August. Association for Computational Linguistics.

Lev Ratinov and Dan Roth. 2009. Design chal-
lenges and misconceptions in named entity recog-
nition. In Proceedings of the Thirteenth Confer-
ence on Computational Natural Language Learning
(CoNLL-2009), pages 147–155, Boulder, Colorado,
June. Association for Computational Linguistics.

Stéphane Ross, Geoffrey J. Gordon, and J. Andrew
Bagnell. 2011. A reduction of imitation learning
and structured prediction to no-regret online learn-
ing. In AISTATS, pages 627–635.

Wee Meng Soon, Hwee Tou Ng, and Daniel
Chung Yong Lim. 2001. A machine learning ap-
proach to coreference resolution of noun phrases.
Computational Linguistics, 27(4):521–544.

Veselin Stoyanov and Jason Eisner. 2012. Easy-first
coreference resolution. In Proceedings of COLING
2012, pages 2519–2534, Mumbai, India, December.

Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and
Ellen Riloff. 2009. Conundrums in noun phrase
coreference resolution: Making sense of the state-
of-the-art. In Proceedings of the Joint Conference of
the 47th Annual Meeting of the ACL and the 4th In-
ternational Joint Conference on Natural Language
Processing of the AFNLP, pages 656–664, Suntec,
Singapore, August. Association for Computational
Linguistics.

Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model the-
oretic coreference scoring scheme. In Proceedings
MUC-6, pages 45–52, Columbia, Maryland.

Xiaofeng Yang, Jian Su, Jun Lang, Chew Lim Tan,
Ting Liu, and Sheng Li. 2008. An entity-
mention model for coreference resolution with in-
ductive logic programming. In Proceedings of ACL-
08: HLT, pages 843–851, Columbus, Ohio, June.
Association for Computational Linguistics.

Chun-Nam Yu and T. Joachims. 2009. Learning struc-
tural svms with latent variables. In International
Conference on Machine Learning (ICML).

Yue Zhang and Stephen Clark. 2008. A tale of two
parsers: Investigating and combining graph-based
and transition-based dependency parsing. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing, pages 562–
571, Honolulu, Hawaii, October. Association for
Computational Linguistics.

57


