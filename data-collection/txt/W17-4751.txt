



















































OSU Multimodal Machine Translation System Report


Proceedings of the Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 465–469
Copenhagen, Denmark, September 711, 2017. c©2017 Association for Computational Linguistics

OSU Multimodal Machine Translation System Report

Mingbo Ma, Dapeng Li, Kai Zhao† and Liang Huang
Department of EECS

Oregon State University
Corvallis, OR 97331, USA

{mam, lidap, zhaok, liang.huang}@oregonstate.edu

Abstract
This paper describes Oregon State Uni-
versity’s submissions to the shared
WMT’17 task “multimodal translation
task I”. In this task, all the sentence
pairs are image captions in different
languages. The key difference be-
tween this task and conventional ma-
chine translation is that we have cor-
responding images as additional infor-
mation for each sentence pair. In
this paper, we introduce a simple but
effective system which takes an im-
age shared between different languages,
feeding it into the both encoding and
decoding side. We report our sys-
tem’s performance for English-French
and English-German with Flickr30K
(in-domain) and MSCOCO (out-of-
domain) datasets. Our system achieves
the best performance in TER for
English-German for MSCOCO dataset.

1 Introduction
Natural language generation (NLG) is one
of the most important tasks in natural lan-
guage processing (NLP). It can be applied to
a lot of interesting applications such like ma-
chine translation, image captioning, question
answering. In recent years, Recurrent Neu-
ral Networks (RNNs) based approaches have
shown promising performance in generating
more fluent and meaningful sentences com-
pared with conventional models such as rule-
based model (Mirkovic et al., 2011), corpus-
based n-gram models (Wen et al., 2015) and
trainable generators (Stent et al., 2004).

† Current address: Google Inc., 111 8th Avenue,
New York, New York, USA.

More recently, attention-based encoder-
decoder models (Bahdanau et al., 2014) have
been proposed to provide the decoder more ac-
curate alignments to generate more relevant
words. The remarkable ability of attention
mechanisms quickly update the state-of-the-
art performance on variety of NLG tasks, such
as machine translation (Luong et al., 2015),
image captioning (Xu et al., 2015; Yang et al.,
2016), and text summarization (Rush et al.,
2015; Nallapati et al., 2016).
However, for multimodal translation (Elliott

et al., 2015), where we translate a caption from
one language into another given a correspond-
ing image, we need to design a new model since
the decoder needs to consider both language
and images at the same time.
This paper describes our participation in the

WMT 2017 multimodal task 1. Our model
feeds the image information to both the en-
coder and decoder, to ground their hidden rep-
resentation within the same context of image
during training. In this way, during testing
time, the decoder would generate more rele-
vant words given the context of both source
sentence and image.

2 Model Description

For the neural-based machine translation
model, the encoder needs to map sequence of
word embeddings from the source side into an-
other representation of the entire sequence us-
ing recurrent networks. Then, in the second
stage, decoder generates one word at a time
with considering global (sentence representa-
tion) and local information (weighted context)
from source side. For simplicity, our proposed
model is based on the attention-based encoder-
decoder framework in (Luong et al., 2015), ref-

465



ereed as “Global attention”.
On the other hand, for the early work

of neural-basic caption generation mod-
els (Vinyals et al., 2015), the convolutional
neural networks (CNN) generate the image
features which feed into the decoder directly
for generating the description.
The first stage of the above two tasks both

map the temporal and spatial information into
a fixed dimensional vector which makes it fea-
sible to utilize both information at the same
time.
Fig. 1 shows the basic idea of our proposed

model (OSU1). The red character I represents
the image feature that is generated from CNN.
In our case, we directly use the image features
that are provided by WMT, and these features
are generated by residual networks (He et al.,
2016).
The encoder (blue boxes) in Fig. 1 takes

the image feature as initialization for generat-
ing each hidden representation. This process
is very similar to neural-basic caption genera-
tion (Vinyals et al., 2015) which grounds each
word’s hidden representation to the context
given by the image. On the decoder side (green
boxes in Fig. 1), we not only let each decoded
word align to source words by global attention
but also feed the image feature as initialization
to the decoder.

x0 x1 x2 x3 x4

I
s0 s1h0 h1 h2 h3 h4

[y0;  ] [y1;s0]

…

I
Figure 1: The image information is feed to both en-
coder and decoder for initialization. I (in red) repre-
sents the image feature that are generated by CNN.

3 Experiments
3.1 Datasets
In our experiments, we use two datasets
Flickr30K (Elliott et al., 2016) and
MSCOCO (Lin et al., 2014) which are
provided by the WMT organization. For
both datasets, there are triples that contains
English as source sentence, its German and
French human translations and correspond-
ing image. The system is only trained on

Flickr30K datasets but are also tested on
MSCOCO besides Flickr30K. MSCOCO
datasets are considered out-of-domain (OOD)
testing while Flickr30K dataset are considered
in-domain testing. The datasets’ statics is
shown in Table 1

Datasets Train Dev Test OOD ?
Flickr30K 29, 000 1, 014 1, 000 No
MSCOCO - - 461 Yes

Table 1: Summary of datasets statistics.

3.2 Training details
For preprocessing, we convert all of the sen-
tences to lower case, normalize the punctua-
tion, and do the tokenization. For simplicity,
our vocabulary keeps all the words that show
in training set. For image representation, we
use ResNet (He et al., 2016) generated image
features which are provided by the WMT or-
ganization. In our experiments, we only use
average pooled features.
Our implementation is adapted from on

Pytorch-based OpenNMT (Klein et al., 2017).
We use two layered bi-LSTM (Sutskever et al.,
2014) on the source side as encoder. Our batch
size is 64, with SGD optimization and a learn-
ing rate at 1. For English to German, the
dropout rate is 0.6, and for English to French,
the dropout rate is 0.4. These two parame-
ters are selected by observing the performance
on development set. Our word embeddings are
randomly initialized with 500 dimensions. The
source side vocabulary is 10,214 and the tar-
get side vocabulary is 18,726 for German and
11,222 for French.

3.3 Beam search with length reward
During test time, beam search is widely used
to improve the output text quality by giving
the decoder more options to generate the next
possible word. However, different from tradi-
tional beam search in phrase-based MT where
all hypotheses know the number of steps to fin-
ish the generation, while in neural-based gen-
eration, there is no information about what is
the most ideal number of steps to finish the
decoding. The above issue also leads to an-
other problem that the beam search in neural-
based MT prefers shorter sequences due to

466



probability-based scores for evaluating differ-
ent candidates. In this paper, we use Optimal
Beam Search (Huang et al., 2017) (OBS) dur-
ing decoding time. OBS uses bounded length
reward mechanism which allows a modified
version of our beam search algorithm to re-
main optimal.
Figure 2 and Figure 3 show the BLEU score

and length ratio with different rewards for dif-
ferent beam size. We choose beam size equals
to 5 and reward equals to 0.1 during decoding.

3.4 Results
WMT organization provides three different
evaluating metrics: BLEU (Papineni et al.,
2002), METEOR (Lavie and Denkowski, 2009)
and TER (Snover et al., 2006).
Table 2 to Table 5 summarize the perfor-

mance with their corresponding rank among
all other systems. We only show a few top
performing systems in the tables to make a
comparison. OSU1 is our proposed model and
OSU2 is our baseline system without any im-
age information. For MSCOCO dataset, the
translation from English to German (Table 3),
which is the hardest tasks compared with oth-
ers since it is from English to German on OOD
dataset, we achieve best TER score across all
other systems.

System Rank TER METEOR BLEU
UvA-TiCC 1 47.5 53.5 33.3

NICT 2 48.1 53.9 31.9
LIUMCVC 3 & 4 48.2 53.8 33.2

CUNI 5 50.7 51 31.1
OSU2† 6 50.7 50.6 31
OSU1† 8 51.6 48.9 29.7

Table 2: Experiments on Flickr30K dataset for trans-
lation from English to German. 16 systems in total. †
represents our system.

System Rank TER METEOR BLEU
OSU1† 1 52.3 46.5 27.4

UvA-TiCC 2 52.4 48.1 28
LIUMCVC 3 52.5 48.9 28.7
OSU2† 8 55.9 45.7 26.1

Table 3: Experiments on MSCOCO dataset for trans-
lation from English to German. 15 systems in total. †
represents our system.

As describe in section 2, OSU1 is the model
with image information for both encoder and

System Rank TER METEOR BLEU
LIUMCVC 1 28.4 72.1 55.9

NICT 2 28.4 72 55.3
DCU 3 30 70.1 54.1
OSU2† 5 32.7 68.3 51.9
OSU1† 6 33.6 67.2 51

Table 4: Experiments on Flickr30K dataset for trans-
lation from English to French. 11 systems in total. †
represents our system.

System Rank TER METEOR BLEU
LIUMCVC 1 34.2 65.9 45.9

NICT 2 34.7 65.6 45.1
DCU 3 35.2 64.1 44.5
OSU2† 4 36.7 63.8 44.1
OSU1† 6 37.8 61.6 41.2

Table 5: Experiments on MSCOCO dataset for trans-
lation from English to French. 11 systems in total.

decoder, and OSU2 is only the neural machine
translation baseline without any image infor-
mation. From the above results table we found
that image information would hurt the perfor-
mance in some cases. In order to have more
detailed analysis, we show some test examples
for the translation from English to German on
MSCOCO dataset.
Fig 4 shows two examples that NMT base-

line model performances better than OSU1
model. In the first example, OSU1 generates
several unseen objects from given image, such
like knife. The image feature might not repre-
sent the image accurately. For the second ex-
ample, OSU1 model ignores the object “box”
in the image.
Fig 5 shows two examples that image feature

helps the OSU1 to generate better results. In
the first example, image feature successfully
detects the object “drink” while the baseline
completely neglects this. In the second exam-
ple, the image feature even help the model fig-
ure out the action of the cat is “sleeping”.

4 Conclusion

We describe our system submission to the
shared WMT’17 task “multimodal translation
task I”. The results for English-German and
English-French on Flickr30K and MSCOCO
datasets are reported in this paper. Our
proposed model is simple but effective and
we achieve the best performance in TER for

467



 39

 39.1

 39.2

 39.3

 39.4

 39.5

 39.6

 39.7

 39.8

 0  1  2  3  4  5  6  7  8  9  10  11

D
e

v
 s

e
t 

B
L

E
U

beam size

r=0

r=0.1

r=0.2

r=0.3

r=0.4

Figure 2: BLEU vs. beam size

 0.9

 0.92

 0.94

 0.96

 0.98

 1

 1.02

 0  1  2  3  4  5  6  7  8  9  10  11

L
e

n
g

th
 r

a
ti
o

 o
n

 d
e

v

beam size

r=0
r=0.1
r=0.2
r=0.3
r=0.4

Figure 3: length ratio vs. beam size

input a finger pointing at a hotdog with cheese , sauerkraut and ketchup .
OSU1 ein finger zeigt auf einen hot dog mit einem messer , wischmobs und napa .
OSU2 ein finger zeigt auf einen hotdog mit hammer und italien .

Reference ein finger zeigt auf einen hotdog mit käse , sauerkraut und ketchup .

input a man reaching down for something in a box
OSU1 ein mann greift nach unten , um etwas zu irgendeinem .
OSU2 ein mann greift nach etwas in einer kiste .

Reference ein mann bückt sich nach etwas in einer schachtel .

Figure 4: Two testing examples that image information confuses the NMT model.

input there are two foods and one drink set on the clear table .
OSU1 da sind zwei speisen und ein getränk am klaren tisch .
OSU2 zwei erwachsene und ein erwachsener befinden sich auf dem rechteckigen tisch .

Reference auf dem transparenten tisch stehen zwei speisen und ein getränk .

input a camera set up in front of a sleeping cat .
OSU1 eine kameracrew vor einer schlafenden katze .
OSU2 eine kamera vor einer blonden katze .

Reference eine kamera , die vor einer schlafenden katze aufgebaut ist

Figure 5: Two testing examples that image information helps the NMT model.

468



English-German for MSCOCO dataset.

5 Acknowledgment

This work is supported in part by NSF IIS-
1656051, DARPA FA8750-13-2-0041 (DEFT),
DARPA N66001-17-2-4030 (XAI), a Google
Faculty Research Award, and an HP Gift.

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua

Bengio. 2014. Neural machine translation by
jointly learning to align and translate. CoRR
.

D. Elliott, S. Frank, K. Sima’an, and L. Specia.
2016. Multi30k: Multilingual english-german
image descriptions. Proceedings of the 5th Work-
shop on Vision and Language pages 70–74.

Desmond Elliott, Stella Frank, and Eva Hasler.
2015. Multi-language image description with
neural sequence models. CoRR .

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and
Jian Sun. 2016. Deep residual learning for im-
age recognition. Conference on Computer Vi-
sion and Pattern Recognition CVPR .

Liang Huang, Kai Zhao, and Mingbo Ma. 2017.
Optimal beam search for neural text generation
(modulo beam size). In EMNLP 2017 .

G. Klein, Y. Kim, Y. Deng, J. Senellart, and A. M.
Rush. 2017. Opennmt: Open-source toolkit for
neural machine translation. ArXiv e-prints .

Alon Lavie and Michael J. Denkowski. 2009. The
meteor metric for automatic evaluation of ma-
chine translation. Machine Translation .

Tsung-Yi Lin, Michael Maire, Serge J. Belongie,
Lubomir D. Bourdev, Ross B. Girshick, James
Hays, Pietro Perona, Deva Ramanan, Piotr Dol-
lár, and C. Lawrence Zitnick. 2014. Microsoft
COCO: common objects in context .

Minh-Thang Luong, Hieu Pham, and Christo-
pher D. Manning. 2015. Effective approaches
to attention-based neural machine translation.
CoRR .

Danilo Mirkovic, Lawrence Cavedon, Matthew
Purver, Florin Ratiu, Tobias Scheideck, Fuliang
Weng, Qi Zhang, and Kui Xu. 2011. Dialogue
management using scripts and combined confi-
dence scores. US Patent pages 7,904,297.

Ramesh Nallapati, Bowen Zhou, and Mingbo Ma.
2016. Classify or select: Neural architectures for
extractive document summarization. CoRR .

Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. Bleu: A method for auto-
matic evaluation of machine translation. Pro-
ceedings of the 40th Annual Meeting on Associ-
ation for Computational Linguistics .

Alexander M. Rush, Sumit Chopra, and Jason We-
ston. 2015. A neural attention model for ab-
stractive sentence summarization .

Matthew Snover, Bonnie Dorr, Richard Schwartz,
Linnea Micciulla, and John Makhoul. 2006. A
study of translation edit rate with targeted hu-
man annotation. In Proceedings of Association
for Machine Translation in the Americas .

Amanda Stent, Rashmi Prasad, and Marilyn
Walker. 2004. Trainable sentence planning for
complex information presentation in spoken di-
alog systems. Proceedings of the 42Nd Annual
Meeting on Association for Computational Lin-
guistics .

Ilya Sutskever, Oriol Vinyals, and Quoc V. Le.
2014. Sequence to sequence learning with neu-
ral networks. Proceedings of the 27th Interna-
tional Conference on Neural Information Pro-
cessing Systems .

Oriol Vinyals, Alexander Toshev, Samy Bengio,
and Dumitru Erhan. 2015. Show and tell: A
neural image caption generator. IEEE Confer-
ence on Computer Vision and Pattern Recogni-
tion pages 3156–3164.

Tsung-Hsien Wen, Milica Gasic, Dongho Kim,
Nikola Mrksic, Pei-hao Su, David Vandyke, and
Steve J. Young. 2015. Stochastic language gen-
eration in dialogue using recurrent neural net-
works with convolutional sentence reranking.
CoRR .

Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun
Cho, Aaron Courville, Ruslan Salakhudinov,
Rich Zemel, and Yoshua Bengio. 2015. Show, at-
tend and tell: Neural image caption generation
with visual attention. Proceedings of the 32nd
International Conference on Machine Learning
(ICML-15) .

Zhilin Yang, Ye Yuan, Yuexin Wu, William W.
Cohen, and Ruslan Salakhutdinov. 2016. Re-
view networks for caption generation. Advances
in Neural Information Processing Systems .

469


