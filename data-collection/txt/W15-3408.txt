



















































Evaluating Features for Identifying Japanese-Chinese Bilingual Synonymous Technical Terms from Patent Families


Proceedings of the Eighth Workshop on Building and Using Comparable Corpora, pages 52–61,
Beijing, China, July 30, 2015. c©2015 Association for Computational Linguistics

Evaluating Features for
Identifying Japanese-Chinese Bilingual Synonymous Technical Terms

from Patent Families
Zi Long

Takehito Utsuro
Grad. Sch. Sys. & Inf. Eng.,
University of Tsukuba,
Tsukuba, 305-8573, Japan

Tomoharu Mitsuhashi
Japan Patent

Information Organization,
4-1-7, Tokyo, Koto-ku,
Tokyo, 135-0016, Japan

Mikio Yamamoto
Grad. Sch. Sys. & Inf. Eng.,
University of Tsukuba,
Tsukuba, 305-8573, Japan

Abstract
In the process of translating patent doc-
uments, a bilingual lexicon of technical
terms is inevitable knowledge source. It is
important to develop techniques of acquir-
ing technical term translation equivalent
pairs automatically from parallel patent
documents. We take an approach of uti-
lizing the phrase table of a state-of-the-
art phrase-based statistical machine trans-
lation model. First, we collect candi-
dates of synonymous translation equiva-
lent pairs from parallel patent sentences.
Then, we apply the Support Vector Ma-
chines (SVMs) to the task of identify-
ing bilingual synonymous technical terms.
This paper especially focuses on the issue
of examining the effectiveness of each fea-
ture and identifies the minimum number
of features that perform as comparatively
well as the optimal set of features. Finally,
we achieve the performance of over 90%
precision with the condition of more than
or equal to 25% recall.

1 Introduction

For both high quality machine and human transla-
tion, a large scale and high quality bilingual lex-
icon is the most important key resource. Since
manual compilation of bilingual lexicon requires
plenty of time and huge manual labor, in the re-
search area of knowledge acquisition from natural
language text, automatic bilingual lexicon compi-
lation have been studied. Techniques invented so
far include translation term pair acquisition based
on statistical co-occurrence measure from parallel
sentences (Matsumoto and Utsuro, 2000), compo-
sitional translation generation based on an exist-
ing bilingual lexicon for human use (Tonoike et
al., 2006), translation term pair acquisition by col-
lecting partially bilingual texts through the search

engine (Huang et al., 2005), and translation term
pair acquisition from comparable corpora (Fung
and Yee, 1998; Aker et al., 2013; Kontonatsios et
al., 2014; Rapp and Sharoff, 2014).
Among those efforts of acquiring bilingual lex-

icon from text, Morishita et al. (2008) studied to
acquire Japanese-English technical term transla-
tion lexicon from phrase tables, which are trained
by a phrase-based SMT model with parallel sen-
tences automatically extracted from parallel patent
documents. Furthermore, based on the achieve-
ment above, Liang et al. (2011a) studied the is-
sue of identifying Japanese-English synonymous
translation equivalent pairs in the task of acquiring
Japanese-English technical term translation equiv-
alent pairs. Based on the technique and the re-
sults of identifying Japanese-English synonymous
translation equivalent pairs in Liang et al. (2011a),
Long et al. (2014) next studied how to identify
Japanese-Chinese synonymous translation equiv-
alent pairs from Japanese-Chinese patent families.
In the task of identifying Japanese-Chinese

synonymous translation equivalent pairs from
Japanese-Chinese patent families (Figure 1) stud-
ied in Long et al. (2014), this paper modifies some
of the features studied in Long et al. (2014) and
further focuses on the issue of examining the ef-
fectiveness of each feature. This paper especially
identifies the minimum number of features that
perform as comparatively well as the optimal set
of features, where the most effective feature is dis-
covered to be the rate of intersection in translation
by the phrase table. Based on the evaluation re-
sults, we finally achieve the performance of over
90% precision with the condition of more than or
equal to 25% recall.

2 Japanese-Chinese Parallel Patent
Documents

Japanese-Chinese parallel patent documents are
collected from the Japanese patent documents

52



Figure 1: Framework of Identifying Japanese-Chinese Bilingual Synonymous Technical Terms from
Patent Families

published by the Japanese Patent Office (JPO) in
2004-2012 and the Chinese patent documents pub-
lished by State Intellectual Property Office of the
People’s Republic of China (SIPO) in 2005-2010.
From them, we extract 312,492 patent families,
and the method of Utiyama and Isahara (2007) is
applied1 to the text of those patent families, and
Japanese and Chinese sentences are aligned. In
this paper, we use 3.6M parallel patent sentences
with the highest scores of sentence alignment2.

1We used a Japanese-Chinese translation lexicon consist-
ing of about 170,000 Chinese head words.

2The maximum score of the method of Utiyama and Isa-
hara (2007) is set to be 1.0, while the lower bound of its score
is about 0.152 with the 3.6M parallel patent sentences.

3 Phrase Table of an SMT Model

As a toolkit of a phrase-based SMT model, we
use Moses (Koehn et al., 2007) and apply it to
the whole 3.6M parallel patent sentences. Be-
fore applying Moses, Japanese sentences are seg-
mented into a sequence of morphemes by the
Japanese morphological analyzer MeCab3 with
the morpheme lexicon IPAdic4. For Chinese sen-
tences, we examine two types of segmentation,

3http://mecab.sourceforge.net/
4http://sourceforge.jp/projects/

ipadic/

53



Figure 2: Developing a Reference Set of Bilingual Synonymous Technical Terms

i.e., segmentation by characters5 and segmentation
by morphemes6.
As the result of applying Moses, we have a

phrase table in the direction of Japanese to Chi-
nese translation, and another one in the opposite
direction of Chinese to Japanese translation. In the
direction of Japanese to Chinese translation, when
Chinese side of parallel sentences are segmented
by morphemes, we finally obtain 108M transla-
tion pairs with 75M unique Japanese phrases with
Japanese to Chinese phrase translation probabili-
ties P (pC | pJ) of translating a Japanese phrase
pJ into a Chinese phrase pC . When Chinese
sentences are segmented by characters, on the
other hand, we obtain 274M translation pairs with
197M unique Japanese phrases. For each Japanese
phrase, those multiple translation candidates in the
phrase table are ranked in descending order of

5A consecutive sequence of numbers as well as a consec-
utive sequence of alphabetical characters are segmented into
a token.

6Chinese sentences are segmented into a sequence of
morphemes by the Chinese morphological analyzer Stanford
Word Segment (Tseng et al., 2005) trained with Chinese Penn
Treebank.

Japanese to Chinese phrase translation probabili-
ties. In the similar way, in the phrase table in the
opposite direction of Chinese to Japanese transla-
tion, for each Chinese phrase, multiple Japanese
translation candidates are ranked in descending or-
der of Chinese to Japanese phrase translation prob-
abilities.

Those two phrase tables are then referred to
when identifying a bilingual technical term pair,
given a parallel sentence pair 〈SJ , SC〉 and a
Japanese technical term tJ , or a Chinese techni-
cal term tC . In the direction of Japanese to Chi-
nese, as shown in Figure 1 (a), given a parallel
sentence pair 〈SJ , SC〉 containing a Japanese tech-
nical term tJ , Chinese translation candidates col-
lected from the Japanese to Chinese phrase table
are matched against the Chinese sentence SC of
the parallel sentence pair. Among those found
in SC , t̂C with the largest translation probability
P (tC | tJ) is selected and the bilingual technical
term pair 〈tJ , t̂C〉 is identified. Similarly, in the
opposite direction of Chinese to Japanese, given a
parallel sentence pair 〈SJ , SC〉 containing a Chi-
nese technical term tC , the Chinese to Japanese

54



phrase table is referred to when identifying a bilin-
gual technical term pair.

4 Developing a Reference Set of Bilingual
Synonymous Technical Terms

When developing a reference set of bilingual syn-
onymous technical terms (detailed procedure to
be found in Long et al. (2014)), as illustrated in
Figure 2, starting from a seed bilingual term pair
sJC = 〈sJ , sC〉, we repeat the translation esti-
mation procedure of the previous section in both
Japanese-Chinese direction and Chinese-Japanese
direction six times in total, and generate the set
CBP (sJ) of candidates of bilingual synonymous
technical term pairs. Then, we manually di-
vide the set CBP (sJ) into SBP (sJC), those of
which are synonymous with sJC , and the remain-
ing NSBP (sJC). As in Table 1, we collect 114
seeds, where the number of bilingual technical
terms included in SBP (sJC) in total for all of the
114 seed bilingual technical term pairs is around
2,300 to 2,400, which amounts to around 21 per
seed on average7. As shown in Figure 1 (b), to
all of those bilingual term pairs, the procedure of
identifying the synonymous sets is applied.

5 Identifying Bilingual Synonymous
Technical Terms by Machine Learning

In this section, we apply the Support Vector Ma-
chines (SVMs) (Vapnik, 1998) to the task of iden-
tifying bilingual synonymous technical terms. In
this paper, we model the task of identifying bilin-
gual synonymous technical terms by the SVMs as
that of judging whether or not the input bilingual
term pair 〈tJ , tC〉 is synonymous with the seed
bilingual technical term pair sJC = 〈sJ , sC〉 .

5.1 The Procedure
First, let CBP be the union of the sets CBP (sJ)
of candidates of bilingual synonymous technical
term pairs for all of the 114 seed bilingual tech-
nical term pairs. In the training and testing of
the classifier for identifying bilingual synonymous
technical terms, we first divide the set of 114
seed bilingual technical term pairs into 10 sub-
sets. Here, for each i-th subset (i = 1, . . . , 10), we
construct the union CBPi of the sets CBP (sJ)

7We manually generate the reference set by discarding the
bilingual pairs which are judged as not synonymous with the
seed pair. The procedure of generating the whole reference
sets took about 30 hours, i.e., about 3 seconds for judging a
bilingual term pair on average.

of candidates of bilingual synonymous technical
term pairs, where CBP1, . . . , CBP10 are 10 dis-
joint subsets8 of CBP .
As a tool for learning SVMs, we use

TinySVM (http://chasen.org/˜taku/
software/TinySVM/). As the kernel func-
tion, we use the polynomial (1st order) kernel9.
In the testing of a SVMs classifier, we regard the
distance from the separating hyperplane to each
test instance as a confidence measure, and return
test instances satisfying confidence measures over
a certain lower bound only as positive samples
(i.e., synonymous with the seed). In the training
of SVMs, we use 8 subsets out of the whole 10
subsets CBP1, . . . , CBP10. Then, we tune the
lower bound of the confidence measure with one
of the remaining two subsets. With this subset, we
also tune the parameter of TinySVM for trade-off
between training error and margin. Finally, we
test the trained classifier against another one of the
remaining two subsets. We repeat this procedure
of training / tuning / testing 10 times, and average
the 10 results of test performance.

5.2 Features

Table 2 lists all the features used for training and
testing of SVMs for identifying bilingual syn-
onymous technical terms. Features are roughly
divided into two types: those of the first type
f1, . . . , f6 simply represent various characteris-
tics of the input bilingual technical term 〈tJ , tC〉,
while those of the second type f7, . . . , f17 repre-
sent relation of the input bilingual technical term
〈tJ , tC〉 and the seed bilingual technical term pair
sJC = 〈sJ , sC〉
Among the features of the first type are the fre-

quency (f1), ranks of terms with respect to the
conditional translation probabilities (f2 and f3),
length of terms (f4 and f5), and the number of
times repeating the procedure of generating trans-
lation with the phrase tables until generating input
terms tJ and tC from the Japanese seed term sJ
(f6).
Among the features of the second type are iden-

tity of monolingual terms (f7 and f8), edit distance
of monolingual terms (f9), character bigram sim-

8Here, we divide the set of 114 seed bilingual technical
term pairs into 10 subsets so that the numbers of positive (i.e.,
synonymous with the seed) / negative (i.e., not synonymous
with the seed) samples in each CBPi (i = 1, . . . , 10) are
comparative among the 10 subsets.

9We compare the performance of the 1st order and 2nd or-
der kernels, where we have almost comparative performance.

55



Table 1: Number of Bilingual Technical Terms: Candidates and Reference of Synonyms

(a) With the Phrase Table based on Chinese Sentences Segmented by Morphemes
# of bilingual technical terms
for the total 114 seeds average per seed

Candidates of Synonyms�

sJ

CBP (sJ)

included only
in the set (a) 12,640 24,621

110.9
216.0included in the intersection

of the sets (a) and (b) 11,981 105.1

Reference of Synonyms�

sJC

SBP (sJC)

included only
in the set (a) 228 2,473

2.0
21.7included in the intersection

of the sets (a) and (b) 2,245 19.7

(b) With the Phrase Table based on Chinese Sentences Segmented by Characters
# of bilingual technical terms
for the total 114 seeds average per seed

Candidates of Synonyms�

sJ

CBP (sJ)

included only
in the set (b) 6,358 17,478

55.8
153.3included in the intersection

of the sets (a) and (b) 11,120 97.5

Reference of Synonyms�

sJC

SBP (sJC)

included only
in the set (b) 287 2,318

2.5
20.3included in the intersection

of the sets (a) and (b) 2,031 17.8

Table 4: Pairs of Features having No Significant
Difference (5% Significance Level) with Maxi-
mum Precision Features and their Evaluation Re-
sults (%)

(a) Chinese sentences are segmented by morphemes
feature precision recall f-measure

f15 + f16 85.6 25.4 39.2
f9 + f16 86.8 24.9 38.7

f13 + f14 + f16 86.8 24.8 38.6

(b) Chinese sentences are segmented by characters
feature precision recall f-measure
f9 + f15 87.4 25.4 39.3

ilarity of monolingual terms (f10), rate of identi-
cal morphemes (in Japanese, f11) / characters (in
Chinese, f12), string subsumption and variants for
Japanese (f13), identical stem for Chinese (f14),
rate of intersection in translation by the phrase ta-
ble (f15), rate of intersection in translation by the
phrase table for the substrings not common be-
tween the seed and a term (f16), and translation
by the phrase tables (f17).

As we discuss in the next section, among all of
those features, f15 and f16, which utilize the rate
of intersection in translation by the phrase table,
are the most effective, where we add f16 in this
paper to those studied in Long et al. (2014).

5.3 Evaluating the Effectiveness of Features

Table 3 shows the evaluation results for a baseline
as well as for SVMs. As the baseline, we sim-
ply judge the input bilingual term pair 〈tJ , tC〉 as
synonymous with the seed bilingual technical term
pair sJC = 〈sJ , sC〉 when tJ and sJ are identical,
or, tC and sC are identical. When training / testing
a SVMs classifier, we tune the lower bound of the
confidence measure of the distance from the sepa-
rating hyperplane in two ways: i.e., for maximiz-
ing precision and for maximizing F-measure. As
shown in Table 3, when we use the set of features
which maximize precision, we achieve higher pre-
cisions of 89.0% and 90.4% for morpheme-based
segmentation and character-based segmentation,
respectively, compared with when we use all of
the proposed features (86.5% and 89.0%) with
the condition of more than or equal to 40% F-
measure10. The sets of features which maximize
precision are f1∼6 + f9∼16 for morpheme-based

10Out of 655 (for morpheme-based segmentation) / 605
(for character-based segmentation) pairs which are correctly
judged as synonymous with the seed pair by SVM , 197
(30.1%) / 161 (26.6%) are not judged as synonymous by the
baseline method, i.e., neither the Japanese term nor the Chi-
nese term is identical to that of the seed pair. On the other
hand, out of 986 (for morpheme-based segmentation) / 927
(for character-based segmentation) pairs which are correctly
judged as synonymous by the baseline method, 458 (46.5%) /
444 (47.9%) are judged as synonymous with the seed pair by
SVM, while the rests are not judged as synonymous by SVM.

56



Table 2: Features for Identifying Bilingual Synonymous Technical Terms by Machine Learning
class feature definition ( where X denotes J or C,and 〈sJ , sC〉 denotes the seed bilingual technical term pair )
features
for
bilingual
technical
terms
〈tJ , tC〉

f1: frequency log of the frequency of 〈tJ , tC〉 within the whole parallel patent
sentences

f2: rank of the Chinese term given tJ , log of the rank of tC with respect to the descending order
of the conditional translation probability P(tC | tJ )

f3: rank of the Japanese term given tC , log of the rank of tJ with respect to the descending order
of the conditional translation probability P(tJ | tC)

f4: number of Japanese charac-
ters

number of characters in tJ

f5: number of Chinese charac-
ters

number of characters in tC

f6: number of times generating
translation by applying the
phrase tables

the number of times repeating the procedure of generating transla-
tion by applying the phrase tables until generating tC or tJ from
sJ , as in sC → · · · → tJ → tC , or, sJ → · · · → tC → tJ

features
for the
relation
of
bilingual
technical
terms
〈tJ , tC〉
and the
seed
〈sJ , sC〉

f7: identity of Japanese terms returns 1 when tJ = sJ
f8: identity of Chinese terms returns 1 when tC = sC
f9: edit distance similarity of

monolingual terms
f9(tX , sX) = 1 − ED(tX ,sX)max(|tX |,|sX |) (where ED is the edit dis-
tance of tX and sX , and | t | denotes the number of characters of
t.)

f10: character bigram similarity
of monolingual terms

f10(tX , sX) =
|bigram(tX )∩bigram(sX)|

max(|tX |,|sX |)−1 (where bigram(t)
is the set of character bigrams of the term t.)

f11: rate of identical morphemes
(for Japanese terms)

f11(tJ , sJ) =
|const(tJ )∩const(sJ)|

max(|const(tJ)|,|const(sJ )|) (where const(t) is
the set of morphemes in the Japanese term t.)

f12: rate of identical characters
(for Chinese terms)

f11(tC , sC) =
|const(tC)∩const(sC)|

max(|const(tC)|,|const(sC)|) (where const(t) is
the set of Characters in the Chinese term t.)

f13: subsumption relation of
strings / variants relation of
surface forms (for Japanese
terms )

returns 1 when the difference of tJ and sJ is only in their suffixes,
or only whether or not having the prolonged sound “ー”, or only in
their hiragana parts.

f14: identical stem (for Chinese
terms)

returns 1 when the difference of tC and sC is only whether or not
having the word “的” which is not the prefix or suffix.

f15: rate of intersection in trans-
lation by the phrase table

f15(tX , sX) =
|trans(tX)∩trans(sX)|

max(|trans(tX)|,|trans(sX)|) ( where trans(t)
is the set of translation of term t from the phrase table.)

f16: rate of intersection in trans-
lation by the phrase table
(for the substrings not com-
mon between tX and sX)

Suppose that x1t , . . . , xmt and x1s, . . . , xns are the substrings which
are not common between tX and sX . Here, we find l (=
min(m,n)) pairs of one-to-one mappings between xit (i =
1, . . . ,m) and xjs (j = 1, . . . , n) which maximize the product of
the rates f15(xit, xjs) of intersection in translation by the phrase ta-
ble and return this product.

f17: translation by the phrase ta-
ble

returns 1 when sJ can be generated by translating tC with the
phrase table, or, sC can be generated by translating tJ with the
phrase table.

segmentation and f2,3 + f6∼9 + f11,12,15,16 for
character-based segmentation, respectively. How-
ever, their differences are not significant (5% sig-
nificance level). Next, we evaluate the effect of
each single feature as well as combinations of
small number of features, where, among those
results, Table 4 shows pairs of features each of
which achieves a precision with no significant dif-
ference (5% significance level) with the set of fea-
tures having the maximum precision. It is obvi-
ous that features f15 and f16, which utilize the
rate of intersection in translation by the phrase ta-
ble, are the most effective. Also, when we re-
move features f15 and f16 from all the features,
precisions are significantly damaged (5% signifi-
cance level) to 78.5% and 79.4% for morpheme-

based and character-based segmentations, respec-
tively. The reason why these features are the most
effective among other features is that they directly
measure the degree of being synonymous within
one language with respect to the rate of intersec-
tion of translations into the other language, while
other features just measure the character-based or
morpheme-based similarity within one language.
We further compare the performance of the pro-

posed features with those studied in Tsunakawa
and Tsujii (2008), where we modify the features
of Tsunakawa and Tsujii (2008) as shown in Ta-
ble 5, and then evaluate those modified features.
As we compare the performance of the proposed
features and the modified features of Tsunakawa
and Tsujii (2008) in Table 3, it is clear that the pro-

57



Table 3: Evaluation Results (%)
segmented by morphemes segmented by characters
precision recall f-measure precision recall f-measure

baseline (tJ and sJ are identical,
or, tC and sC are identical.) 71.4 40.0 51.3 74.0 40.1 52.0

SVM
(all features)

maximum
precision 86.5 26.5 40.5 89.0 26.1 40.4
maximum
f-measure 64.3 64.1 64.2 63.5 65.3 64.4

SVM
(features with

maximum
precision 89.0 23.9 37.7 90.4 25.5 40.4

maximum precision) (f1∼6 + f9∼16) (f2,3 + f6∼9 + f11,12,15,16)

SVM (features in
Tsunakawa and
Tsujii (2008))

maximum
precision 72.6 26.1 38.4 74.4 36.7 49.2
maximum
f-measure 71.0 54.7 61.5 72.7 53.7 61.8

Table 5: Features for Identifying Bilingual Synonymous Technical Terms by Tsunakawa and Tsu-
jii (2008)
class features definition

basical
features

h1J，h1C : agreement of the first
characters

returns 1 when the fisrt characters of tX and sX match.

h2J，h2C : edit distance of simi-
larity of monolingual
terms

the same as f9

h3J，h3C : character of bigram
similarity of monolin-
gual terms

the same as f10

h4J，h4C : agreement of word
substring

return the count that substrings of tX match sX . (Here, Tsunakawa
and Tsujii (2008) count not only the common substrings but also
substrings in known synonymous relation between tX and sX .
However, in our work, we have no lexicon available for synonymous
relation. So, we utilize only the count of common substrings.)

h5J，h5C : translation by the
phrase table

the same as f17．(Here, instead of the phrase table, Tsunakawa and
Tsujii (2008) utilize a bilingual lexicon and consider the existence
of bilingual lexical items as features. )

h6: identical stem for Chi-
nese terms

the same as f14 (Although Tsunakawa and Tsujii (2008) define this
feature as examining the acronym relation of English terms, we
modify this feature as examining the difference of the Chinese terms
as the Chinese word “的”.)

h7: subsumption relation
of strings / variants
relation of surface
forms for Japanese
terms

the same as f13 (Although Tsunanakwa and Tsujii (2008) examine
only the katakana variant, we additionally examine the difference
of suffixes and variants of hiragana parts.)

combina-
torial
feature

h1J ∧ h1C —√
h2J · h2C —√
h3J · h3C —

h5J ∧ h5C —
h6 · h2J —
h7 · h2C —

posed features outperform the modified features of
Tsunakawa and Tsujii (2008).

Next, Table 6 shows examples of improvement
by SVM compared with the baseline. As shown
in Table 6 (a), the relation between input bilin-
gual term pairs and seed bilingual term pairs is
correctly judged as “synonym”, while judgement
by the baseline is “not synonym” since neither the
Chinese terms nor the Japanese terms are iden-

tical. In our proposed features, f17 contributes
to the correct judgement, where it returns 1 be-
cause of the existence of the translation pairs 〈“
ガラス転移温度”,“ ” 〉 and 〈“ガラス転移
点”,“ ”〉 in the phrase table. In the case
of another example shown in Table 6 (b), on the
other hand, the proposed method correctly judges
as “not synonym” by SVM compared with the
baseline, where both the edit distance similarity

58



Table 6: Examples of Improvement in Identifying Bilingual Synonymous Technical Terms by SVM
Baseline: Judge the input bilingual term pair 〈tJ , tC〉 as synonymous with the seed bilingual

term pair 〈sJ , sC〉 when tJ and sJ are identical, or, tC and sC are identical.
SVM: Maximize precision by tuning the lower bound of the confidence measure of the distance

from the separating hyperplane (Chinese sentences are segmented by morphemes).
(a) Correct Judgement as “Synonym” only by SVM

(b) Correct Judgement as “Not Synonym” only by SVM

(f9) and the character bigram similarity (f10) be-
tween the Japanese terms “集電装置” and “コ
レクト” are 0 ( f9(〈tJ , tC〉, 〈sJ , sC〉) = 0 and
f10(〈tJ , tC〉, 〈sJ , sC〉)=0).
Finally, Table 7 shows examples of er-

roneous judgements by SVM. As shown in
Table 7 (a), since erroneous translation pairs
〈“断熱体”,“ ”〉 and 〈“インシュレー
ター”,“ ”〉 exist in the phrase table, both
f17 (both of the translations pairs 〈sJ , tC〉 and
〈tJ , sC〉 exist in the phrase table) and f17 (either
the translation pair 〈sJ , tC〉 or 〈tJ , sC〉 exist in
the phrase table) return 1, resulting in erroneous
judgement.
Another example is shown in Table 7 (b), where

the proposed method returns erroneous judgement
as “not synonym”. In this case, since the transla-
tion pair 〈“成膜チャンバー”,“ ”〉 only ex-
ists in the phrase table, f17 (either the transla-
tion pair 〈sJ , tC〉 or 〈tJ , sC〉 exist in the phrase
table) returns 1, while f17 (both of translations
pairs 〈sJ , tC〉 and 〈tJ , sC〉 exist in the phrase ta-
ble) returns 0. Furthermore, even though Chinese
words “ ” and “ ” are synonymous, their
character bigram similarity is computed as 0, since
they have opposite character orderings.

6 Related Work

Among related works on acquiring bilingual lexi-
con from text, Lu and Tsou (2009) and Yasuda and
Sumita (2013) studied to extract bilingual terms
from comparable patents, where, they first ex-
tract parallel sentences from comparable patents,
and then extract bilingual terms from parallel sen-
tences. Those studies differ from this paper in
that those studies did not address the issue of

acquiring bilingual synonymous technical terms.
Tsunakawa and Tsujii (2008) is mostly related to
our study, in that they also proposed to apply ma-
chine learning technique to the task of identifying
bilingual synonymous technical terms. However,
Tsunakawa and Tsujii (2008) studied the issue of
identifying bilingual synonymous technical terms
only within manually compiled bilingual techni-
cal term lexicon and thus are quite limited in its
applicability. Our approach, on the other hand, is
quite advantageous in that we start from parallel
patent documents which continue to be published
every year and then, that we can generate candi-
dates of bilingual synonymous technical terms au-
tomatically. Furthermore, as we show in the pre-
vious section, the features proposed in this paper
outperform that of Tsunakawa and Tsujii (2008).

7 Conclusion

In the task of acquiring Japanese-Chinese techni-
cal term translation equivalent pairs from paral-
lel patent documents, this paper studied the issue
of identifying synonymous translation equivalent
pairs. This paper especially focused on the issue
of examining the effectiveness of each feature and
identified the minimum number of features that
perform as comparatively well as the optimal set
of features. One of the most important future work
is definitely to improve recall. To do this, we plan
to apply the semi-automatic framework (Liang et
al., 2011b) which have been invented in the task of
identifying Japanese-English synonymous transla-
tion equivalent pairs and have been proven to be
effective in improving recall. Another important
future work is to train the SVM of identifying
bilingual synonymous technical pairs with a set

59



Table 7: Examples of Errors in Identifying Bilingual Synonymous Technical Terms By the Proposed
Method

(a) Incorrect Judgement as “Synonym” by SVM

(a) Incorrect Judgement as “Not Synonym” by SVM

of patent families, and then to evaluate the trained
SVM against parallel patent sentences and phrase
tables extracted from another set of patent fami-
lies.

References
A. Aker, M. Paramita, and R. Gaizauskas. 2013.
Extracting bilingual terminologies from comparable
corpora. In Proc. 51st ACL, pages 402–411.

P. Fung and L. Y. Yee. 1998. An IR approach for
translating new words from nonparallel, comparable
texts. In Proc. 17th COLING and 36th ACL, pages
414–420.

F. Huang, Y. Zhang, and S. Vogel. 2005. Mining
key phrase translations from Web corpora. In Proc.
HLT/EMNLP, pages 483–490.

P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Con-
stantin, and E. Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proc.
45th ACL, Companion Volume, pages 177–180.

G. Kontonatsios, I. Korkontzelos, J. Tsujii, and S. Ana-
niadou. 2014. Using random forest classifier to
compile bilingual dictionaries of technical terms
from comparable corpora. In Proc. 14th EACL,
pages 111–116.

B. Liang, T. Utsuro, and M. Yamamoto. 2011a. Iden-
tifying bilingual synonymous technical terms from
phrase tables and parallel patent sentences. Proce-
dia - Social and Behavioral Sciences, 27:50–60.

B. Liang, T. Utsuro, and M. Yamamoto. 2011b.
Semi-automatic identification of bilingual synony-
mous technical terms from phrase tables and parallel
patent sentences. In Proc. 25th PACLIC, pages 196–
205.

Z. Long, L. Dong, T. Utsuro, T. Mitsuhashi, andM. Ya-
mamoto. 2014. Identifying Japanese-Chinese bilin-
gual synonymous technical terms from patent fami-
lies. In Proc. 7th BUCC, pages 49–54.

B. Lu and B. K. Tsou. 2009. Towards bilingual term
extraction in comparable patents. In Proc. 23rd
PACLIC, pages 755–762.

Y.Matsumoto and T. Utsuro. 2000. Lexical knowledge
acquisition. In R. Dale, H. Moisl, and H. Somers,
editors, Handbook of Natural Language Processing,
chapter 24, pages 563–610. Marcel Dekker Inc.

Y. Morishita, T. Utsuro, and M. Yamamoto. 2008. In-
tegrating a phrase-based SMT model and a bilin-
gual lexicon for human in semi-automatic acquisi-
tion of technical term translation lexicon. In Proc.
8th AMTA, pages 153–162.

R. Rapp and S. Sharoff. 2014. Extracting multiword
translations from aligned comparable documents. In
Proc. 3rdWorkshop on Hybrid Approaches to Trans-
lation, pages 83–91.

60



M. Tonoike, M. Kida, T. Takagi, Y. Sasaki, T. Utsuro,
and S. Sato. 2006. A comparative study on compo-
sitional translation estimation using a domain/topic-
specific corpus collected from the web. In Proc. 2nd
Intl. Workshop on Web as Corpus, pages 11–18.

H. Tseng, P. Chang, G. Andrew, D. Jurafsky, and
C. Manning. 2005. A conditional random field
word segmenter for Sighan bakeoff 2005. In Proc.
4th SIGHAN Workshop on Chinese Language Pro-
cessing, pages 168–171.

T. Tsunakawa and J. Tsujii. 2008. Bilingual synonym
identification with spelling variations. In Proc. 3rd
IJCNLP, pages 457–464.

M. Utiyama and H. Isahara. 2007. A Japanese-English
patent parallel corpus. In Proc. MT Summit XI,
pages 475–482.

V. N. Vapnik. 1998. Statistical Learning Theory.
Wiley-Interscience.

K. Yasuda and E. Sumita. 2013. Building a bilin-
gual dictionary from a Japanese-Chinese patent cor-
pus. In Computational Linguistics and Intelligent
Text Processing, volume 7817 of LNCS, pages 276–
284. Springer.

61


