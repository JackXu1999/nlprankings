



















































What Matters for Neural Cross-Lingual Named Entity Recognition: An Empirical Analysis


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 6395–6401,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

6395

What Matters for Neural Cross-Lingual Named Entity Recognition:
An Empirical Analysis

Xiaolei Huang1∗, Jonathan May2, Nanyun Peng2
1Dept. of Information Science, University of Colorado Boulder

2University of Southern California Information Sciences Institute
xiaolei.huang@colorado.edu, {jonmay, npeng}@isi.edu ∗

Abstract

Building named entity recognition (NER)
models for languages that do not have much
training data is a challenging task. While
recent work has shown promising results on
cross-lingual transfer from high-resource lan-
guages to low-resource languages, it is unclear
what knowledge is transferred. In this paper,
we first propose a simple and efficient neu-
ral architecture for cross-lingual NER. Exper-
iments show that our model achieves competi-
tive performance with the state-of-the-art. We
further analyze how transfer learning works
for cross-lingual NER on two transferable fac-
tors: sequential order and multilingual em-
beddings, and investigate how model perfor-
mance varies across entity lengths. Finally,
we conduct a case-study on a non-Latin lan-
guage, Bengali, which suggests that leveraging
knowledge from Wikipedia will be a promis-
ing direction to further improve the model per-
formances. Our results can shed light on future
research for improving cross-lingual NER.

1 Introduction

Named Entity Recognition (NER) is an important
NLP task that identifies the boundary and type of
named entities (e.g., person, organization, loca-
tion) in texts. However, for some languages, it is
hard to obtain enough labeled data to build a fully
supervised learning model. Cross-lingual transfer
models, which train on high-resource languages
and transfer to target languages are a promis-
ing direction for languages with little annotated
data (Bharadwaj et al., 2016; Tsai et al., 2016;
Pan et al., 2017; Yang et al., 2017; Mayhew et al.,
2017; Wang et al., 2017; Cotterell and Duh, 2017;
Feng et al., 2018; Xie et al., 2018; Zhou et al.,
2019).

∗The work was done when the first author worked as an
intern at USC ISI.

Cross-lingual NER models learn transferable
knowledge mainly from four sources: transla-
tion (Mayhew et al., 2017; Feng et al., 2018), bilin-
gual embeddings (Ni et al., 2017; Xie et al., 2018),
phonetic alphabets (Bharadwaj et al., 2016) and
multi-source learning (Mayhew et al., 2017; Lin
et al., 2018; Zhou et al., 2019; Chen et al., 2019;
Rahimi et al., 2019). Translation can be applied
at either the sentence level via machine transla-
tion or at the word and phrase level via applica-
tion of bilingual dictionaries. Bilingual embed-
dings are a form of bilingual dictionaries; they
constitute a projection from one pre-trained lan-
guage representation into the same vector space
as the other such that words with the same mean-
ing have similar representations (Conneau et al.,
2018). Phonetic alphabets enable different lan-
guages to share the same pronunciation characters
so that the character-level knowledge is transfer-
able between languages that otherwise have dif-
ferent character sets, such as English and Ben-
gali (Hermjakob et al., 2018). Multi-source learn-
ing is effective when multiple or similar lan-
guage resources are available by learning share-
able knowledge. For example, training a model on
both English and Hindi can significantly improve
the model performance on Bengali than only using
English (Mayhew et al., 2017). However, there
is little prior work with detailed analysis of how
cross-lingual NER models transfer knowledge be-
tween languages on different levels.

In this paper, we focus on a single-source zero-
shot transfer setting where we transfer from En-
glish to target languages that have no annotated
data. In our settings, the resources are limited to
annotated source language data, bilingual dictio-
naries, and unlabeled corpora from both source
and target languages. We first propose a neural
cross-lingual NER model that combines the ideas
of translation, bilingual embedding, and phonetic



6396

alphabets. Next, we conduct qualitative analy-
ses to answer the following questions on how the
model transfers knowledge under the cross-lingual
settings: 1) does the source language syntax mat-
ter? 2) how do word and character embeddings af-
fect the model transfer? We analyze how F1 scores
differ across different entity lengths. Finally, we
conduct a case study on Bengali.

2 Model

NER models take a sequence of tokens as input
and predict a sequence of labels such as person
(PER) or location (LOC). In this paper, we adopt
the neural architecture NER model from Lample
et al. (2016); Ma and Hovy (2016). The model first
combines pre-trained word and character embed-
dings as token representations, then feeds the rep-
resentations to one layer of a Bidirectional Long
Term Short Memory (Bi-LSTM) (Hochreiter and
Schmidhuber, 1997), and finally predicts the out-
puts via a Conditional Random Field (CRF). We
show the model architecture on left of Figure 1.
However, languages express the same named en-
tity in different words and characters. To bridge
the barriers, we combine three strategies: bilin-
gual embedding, reverse translation, and translit-
eration.

Bilingual Embedding. Word embeddings are
usually trained for each language separately and
therefore in different vector spaces (Ruder et al.,
2019). To map word embeddings into the same
shared space, we use MUSE (Conneau et al.,
2018) to build our bilingual embeddings. The
bilingual embeddings enable the words with the
same meaning to have similar word representa-
tions in the shared space.

Reverse Translation. We use the bilingual
dictionaries provided by Rolston and Kirchhoff
(2016). Language variations exist among source
and target languages and training on monolingual
corpus might limit learning the variations. There-
fore, we translate the source language, English,
to the target language for the training corpus to
reconstruct source language sentences and learn
source and target languages jointly. However, one
English word might have multiple corresponding
translations. To select the best translation, we de-
fine an empirical score function F (w,wt,i) that
calculates the cosine similarity between the trans-
lated target word (wt,i) and the English source

word (w) based on its contextual words (wc,j).

F (w,wt,i) = α · cos(E(w), E(wt,i))

+ (1− α) ·
m∑
j=1

cos(E(wt,i), E(wc,j))

(dj + 1)2
(1)

where E(w) is the bilingual embedding vector of
the word, dj is the sequential distance between
the word and its contextual word j, and α is the
trade-off factor that balances impacts of transla-
tion pair and contextual words. In this study, we
set α to 0.5. We choose the translation pair with
the highest similarity score. Note that the reverse
only applies during the training step that translates
English into the target language.

Target word Source word Uroman Type
grünen green gruenen ORG

Europe iuropera LOC

Table 1: Examples of Uroman that maps different lan-
guages into the same character space. We show two
target languages, German (top) and Bengali (bottom).
The second column is a translation of the target lan-
guage. The transliterations show the phonetic similar-
ity between the source and target languages.

Character - Uromanization. Different lan-
guages may not share the same characters, but
some named entities in the multilingual corpora
share phonetic similarities (Huang et al., 2004).
To map multilingual corpora into the same char-
acter space and connect the phonetic similarity
between entities, we employ Uroman1 (Herm-
jakob et al., 2018), which transliterates any lan-
guage into something roughly pronounceable En-
glish spellings, while keeps the English words un-
changed. We show some examples of Uroman in
Table 1.

3 Experiments

In this study, we first evaluate our proposed cross-
lingual method on the CoNLL 2002 and 2003
NER datasets (Tjong Kim Sang, 2002; Tjong
Kim Sang and De Meulder, 2003). We then con-
duct ablation studies to examine how the model
learns transferable knowledge under the cross-
lingual settings. Finally, we conduct a case study
on Bengali, a low resource language.

1https://www.isi.edu/˜ulf/uroman.html

https://www.isi.edu/~ulf/uroman.html


6397

B-ORG I-ORG E-ORG O S-PERCRF

Hidden
States

Bi-LSTM

Tokens Representations

grün Uroman

….

….
….

….

…
.

gruen

……

g r nu

Character
Bi-LSTM

Bilingual
Embedding

Translate

……

Tokengreen

Figure 1: Architecture of our proposed model. Each word is translated from the source. Each token representation
contains two concatenated parts: a bilingual word embedding (light orange) and character representations (blue).
The character representation for each word is generated via the Bi-LSTM. Words are first transliterated and split
into individual characters before being fed into the character Bi-LSTM.

3.1 Data

The CoNLL datasets contain four different Euro-
pean languages, Spanish, Dutch, English, and Ger-
man. The data contains four types of named enti-
ties: person (PER), organization (ORG), location
(LOC) and MISC. We use Bengali language data
from LDC2015E13 (V2.1). To be consistent, we
only keep the PER, ORG and LOC tags and ignore
MISC tags from the Bengali corpus.

In this study, we choose the BIOSE tag schema
instead of standard BIO, where the B, I, O, S, E re-
fer to the beginning, inside, outside, single and end
of an entity, respectively. Previous work shows
that BIOSE can learn a more expressive model
than the BIO schema does (Ratinov and Roth,
2009). We lowercase the data and replace numbers
and URLs as “num” and “url” respectively. We
use the training data to train our model, the devel-
opment sets to select the best well-trained model,
and the test sets to evaluate performance, where
the training data is English and the development
and test sets are from the target language.

3.2 Experimental Settings

Bilingual Embedding. We use the 300-
dimensional bilingual word embeddings pre-
trained by MUSE (Conneau et al., 2018) and 300-
dimensional randomly initialized character em-
beddings. Specifically, we first collect mono-
lingual pre-trained word embeddings from fast-
Text (Bojanowski et al., 2017). We then align the
embeddings using the MUSE supervised model.
We normalize the inputs to MUSE and keep the
other hyperparameters as defaults. We merge the
aligned word embeddings and remove duplicates.
Updating the embedding during training time will

change the vector space of partial bilingual em-
beddings and therefore break the vector alignment.
Thus, we freeze the embedding weights during
the training step. For OOV (out-of-vocabulary)
words, we use a randomly initialized 300 dimen-
sion vector within [−0.1, 0.1].

Reverse Translation. We follow the general
translation step in Section 2. Specifically, we re-
place English words with target language words if
the translation pairs exist in the bilingual dictio-
naries (Rolston and Kirchhoff, 2016). If a trans-
lation pair does not exist for a word, we keep the
word unchanged in the training data. We use the
pre-trained bilingual embedding to help select the
best choice of polysemy.

Model parameters. We use 300-dimension
hidden states for both character and token level
Bi-LSTMs. To prevent overfitting, we apply
dropout (Srivastava et al., 2014) with a rate of 0.5
on outputs of the two Bi-LSTMs. We follow the
Conditional Random Field (CRF) setup of Peng
and Dredze (2015). We then randomly initialize
the weights of the layers within [−0.1, 0.1]. We
train the model for 200 epochs and optimize the
parameters by Stochastic Gradient Descent (SGD)
with momentum, gradient clipping, and learning
rate decay. We set the learning rate (lr) and the de-
cay rate (dr) as 0.01 and 0.05 respectively. We up-
date the learning rate by lr(1+(n+1)∗dr) after epoch
n. We clip the gradients to the range of [-5.0, 5.0].
We measure performance by F1 score.

3.3 Baselines
In this study, we compare our proposed method
with three close works under the cross-lingual set-
tings. We compare our method with the best-
reported performance from their works. We briefly



6398

summarize the the three baselines in this section.

• WikiNER (Tsai et al., 2016) first links named
entities in the multilingual Wikipedia corpora
and extracts page categories as “wikifier” fea-
tures, and use these features to achieve cross-
lingual transfer.

• CTNER (Mayhew et al., 2017) first translates
the corpora into English via bilingual dictio-
naries and multilingual Wikipedia entries on
both word and phrase levels, and directly per-
form NER on the translated target language.

• NCNER (Xie et al., 2018) proposes a neural
NER model with attention mechanism. The
work is closest to ours. However, our model
takes different approaches in obtaining multi-
lingual embedding, translation, and translit-
eration as Xie et al. (2018), while the neural
models share the similar architecture of Bi-
LSTMs-CRF, and use multi-lingual embed-
dings to achieve transfer.

3.4 Results

Models Spanish Dutch German
WikiNER* 60.55 61.60 48.10
CTNER* 51.82 53.94 50.96

CTNER*+ 65.95 66.50 59.11
NCNER 72.37 71.25 57.76

Our Model 64.48 73.44 62.26

Table 2: F1 score comparisons of cross-lingual models
on Spanish, Dutch and German. The “*” indicates the
model uses the Wikipedia resources. The “+” means
training model by multiple language resources.

As seen in Table 2, our proposed method out-
performs the previous works in nearly all cross-
lingual tasks. Moreover, the two neural model
methods exceed the performance of the other mod-
els. These results suggest the effectiveness of
neural cross-lingual transfer through multi-lingual
embedding space, and show competitive results of
our proposed methods.

3.5 Ablation Studies
We conduct ablation analyses to understand how
the model transfers learned knowledge from one
language to the other. We focus on two aspects:
syntax and embeddings.

Syntax analysis. Different languages might
not share the same syntax structures. The neu-
ral model learns sequential information from the

Models Spanish Dutch German
Full Model 64.48 73.44 62.26

Shuffle 49.44 40.61 27.25
Word-only 53.00 59.60 52.58
Char-only 18.88 12.15 16.46

Table 3: F1 scores of different ablation analyses, com-
pared to our full model.. “Shuffle” means the train-
ing data is shuffled. “Word-only” means our proposed
model is only fed bilingual word embeddings, and
“Char-only” means the model only receives character
embeddings as input.

source language and applies the learned knowl-
edge on the target language. The importance of
this sequential information is unknown. We shuf-
fle the sentences of training data while keeping
the internal order of named entities unchanged.
We show the results in Table 3. The results show
that after shuffling, the performances of our model
decrease and the decreases vary among different
languages. This suggests the importance of se-
quential information for those languages under the
cross-lingual settings.

Embeddings analysis. We train our proposed
models with both character and bilingual word
embeddings, however, how the model values dif-
ferent embeddings is unknown. We feed the model
with either bilingual word or character embed-
dings. Table 3 shows that bilingual word embed-
dings have better performance than character em-
beddings across the three languages. The results
suggest that the aligned bilingual embeddings are
more important than the character embeddings for
the three languages.

3.6 Entity length analysis

While we can observe how models perform over-
all on the named entities, we can not know how
models differ from short to long entities. Particu-
larly, probing if models hold strong biases towards
shorter entities can help interpret the process of
cross-lingual transfer learning. To make the com-
parison, we first categorize entities into three lev-
els: single token, two tokens and greater or equal
to three tokens. We then calculate the F1-score of
the three entity lengths in the test set and among
correctly predicted entities. Finally we summarize
the F1-score in Table 4.

With comparing to the overall performance
across the three languages in Table 2, we can ob-
serve that the single token shows relatively closer



6399

Language 1 2 ≥3
Spanish 68.93 68.21 47.57
Dutch 69.49 82.46 60.85

German 59.50 70.76 37.22

Table 4: F1 scores of different lengths of entities across
the three languages: Spanish, Dutch and German. The
number 1 refers to the entities with single token, the 2
means the entities with two tokens and the ≥ 3 indi-
cates the entities with not less than three tokens.

to the performance, the entities with two tokens
achieve the higher scores, while the entities with
more than 2 tokens decrease significantly rang-
ing from 12.59 to 25.04 absolute percentages of
F1 scores. The observation indicates that entities
longer than two tokens are more difficult to in-
fer. This might encourage us to balance the weight
of long entities in our current evaluation method
which ignores entity length when datasets have
high volumes of long entities.

4 Case Study: Bengali

Models F1 score
CTNER 30.47

CTNER+ 31.70
CTNER* 46.28

CTNER*+ 45.70
Our Model 34.29

Table 5: F1 score comparisons of translation-based
models on Bengali. The “*” indicates the model uses
Wikipedia resources. The “+” means a model is trained
with multiple language resources.

The previous cross-lingual settings were only
for European languages, which share similar al-
phabets. However, many languages use non-Latin
orthography. In this work, we present a case
study on Bengali, which does not use a Latin al-
phabet. We compare our proposed method with
the translation-based method, CTNER (Mayhew
et al., 2017). The results in Table 5 show that our
model outperforms the previous methods without
Wikipedia.

The results suggest multilingual Wikipedia is
critical for future performance improvements be-
yond simple transfer. This is to be expected; a do-
main discrepancy exists between the source and
target language data and therefore many named
entities are missing or mismatched: partial data
sources of the Bengali come from social media

and online forums (Cieri et al., 2016). By contrast,
the only data source of CoNLL data is from news
articles (Tjong Kim Sang, 2002; Tjong Kim Sang
and De Meulder, 2003). While transfer can help
provide universal context clue information across
languages there is no substitute for a resource of
actual names.

Language Type Token
Spanish 2.5 0.8
Dutch 2.5 0.9

German 2.3 1.5
Bengali 18.2 12.2

Table 6: OOV rate (percentage) in our bilingual word
embeddings across the four languages. Type indicates
unique words, and token refers to counting token num-
bers.

The colloquium words from social media may
cause the issue of out of vocabulary (OOV) and
further impact the transferring process. We sum-
marize the percentage of missing words in our pre-
trained bilingual word embeddings in Table 6. The
OOV rate of Bengali is significantly higher than
that of the other three languages. This suggests
that the high OOV rate and the discrepancy of data
sources may hurt the effectiveness of transfer.

5 Conclusion

We have presented a simple but efficient method
to adapt neural NER for the cross-lingual set-
tings. The proposed method benefits from multi-
ple transferable knowledge and shows competitive
performances with the state of the art using limited
resources. We examine multiple factors that im-
pact the transfer process and conduct an ablation
study to measure their influences. Our experiment
on Bengali shows that leveraging knowledge from
Wikipedia will be a promising direction for future
research.

6 Acknowledgments

This work is partially funded by DARPA
(HR0011-15-C-0115) and NIH R01 (LM012592).
The authors thank the anonymous reviewers for
their helpful suggestions. We thank Boliang
Zhang, Stephen Mayhew, Michael J. Paul and
Thamme Gowda for their useful feedback. Any
opinions, findings, conclusions, or recommenda-
tions expressed here are those of the authors and
do not necessarily reflect the view of the sponsor.



6400

References
Akash Bharadwaj, David Mortensen, Chris Dyer, and

Jaime Carbonell. 2016. Phonologically aware neu-
ral model for named entity recognition in low re-
source transfer settings. In Proceedings of the 2016
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1462–1472, Austin, Texas.
Association for Computational Linguistics.

Piotr Bojanowski, Edouard Grave, Armand Joulin, and
Tomas Mikolov. 2017. Enriching word vectors with
subword information. Transactions of the Associa-
tion for Computational Linguistics, 5:135–146.

Xilun Chen, Ahmed Hassan Awadallah, Hany Has-
san, Wei Wang, and Claire Cardie. 2019. Multi-
source cross-lingual model transfer: Learning what
to share. In Proceedings of the 57th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 3098–3112, Florence, Italy. Association
for Computational Linguistics.

Christopher Cieri, Mike Maxwell, Stephanie Strassel,
and Jennifer Tracey. 2016. Selection criteria for
low resource language programs. In Proceedings of
the Tenth International Conference on Language Re-
sources and Evaluation (LREC 2016), pages 4543–
4549, Portorož, Slovenia. European Language Re-
sources Association (ELRA).

Alexis Conneau, Guillaume Lample, Marc’Aurelio
Ranzato, Ludovic Denoyer, and Hervé Jégou. 2018.
Word translation without parallel data. In Interna-
tional Conference on Learning Representations.

Ryan Cotterell and Kevin Duh. 2017. Low-
resource named entity recognition with cross-
lingual, character-level neural conditional random
fields. In Proceedings of the Eighth International
Joint Conference on Natural Language Processing
(Volume 2: Short Papers), volume 2, pages 91–
96, Taipei, China. Asian Federation of Natural Lan-
guage Processing.

Xiaocheng Feng, Xiachong Feng, Bing Qin, Zhangyin
Feng, and Ting Liu. 2018. Improving low resource
named entity recognition using cross-lingual knowl-
edge transfer. In Proceedings of the Twenty-Seventh
International Joint Conference on Artificial Intel-
ligence, IJCAI-18, pages 4071–4077, Stockholm,
Sweden. International Joint Conferences on Artifi-
cial Intelligence Organization.

Ulf Hermjakob, Jonathan May, and Kevin Knight.
2018. Out-of-the-box universal romanization tool
uroman. Proceedings of ACL 2018, System Demon-
strations, pages 13–18.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Fei Huang, Stephan Vogel, and Alex Waibel. 2004.
Improving named entity translation combining pho-
netic and semantic similarities. In Proceedings

of the Human Language Technology Conference of
the North American Chapter of the Association for
Computational Linguistics: HLT-NAACL 2004.

Guillaume Lample, Miguel Ballesteros, Sandeep Sub-
ramanian, Kazuya Kawakami, and Chris Dyer. 2016.
Neural architectures for named entity recognition.
In Proceedings of NAACL-HLT, pages 260–270, San
Diego, California. Association for Computational
Linguistics.

Ying Lin, Shengqi Yang, Veselin Stoyanov, and Heng
Ji. 2018. A multi-lingual multi-task architecture
for low-resource sequence labeling. In Proceed-
ings of the 56th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 799–809, Melbourne, Australia. Asso-
ciation for Computational Linguistics.

Xuezhe Ma and Eduard Hovy. 2016. End-to-end se-
quence labeling via bi-directional lstm-cnns-crf. In
Proceedings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), volume 1, pages 1064–1074, Berlin,
Germany. Association for Computational Linguis-
tics.

Stephen Mayhew, Chen-Tse Tsai, and Dan Roth. 2017.
Cheap translation for cross-lingual named entity
recognition. In Proceedings of the 2017 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 2536–2545, Copenhagen, Denmark.
Association for Computational Linguistics.

Jian Ni, Georgiana Dinu, and Radu Florian. 2017.
Weakly supervised cross-lingual named entity
recognition via effective annotation and represen-
tation projection. In Proceedings of the 55th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), volume 1,
pages 1470–1480, Vancouver, Canada. Association
for Computational Linguistics.

Xiaoman Pan, Boliang Zhang, Jonathan May, Joel
Nothman, Kevin Knight, and Heng Ji. 2017. Cross-
lingual name tagging and linking for 282 languages.
In Proceedings of the 55th Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers), volume 1, pages 1946–1958, Van-
couver, Canada. Association for Computational Lin-
guistics.

Nanyun Peng and Mark Dredze. 2015. Named en-
tity recognition for chinese social media with jointly
trained embeddings. In Proceedings of the 2015
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 548–554, Lisbon, Portugal.
Association for Computational Linguistics.

Afshin Rahimi, Yuan Li, and Trevor Cohn. 2019. Mas-
sively multilingual transfer for ner. In Proceed-
ings of the 57th Annual Meeting of the Association
for Computational Linguistics, pages 151–164, Flo-
rence, Italy. Association for Computational Linguis-
tics.

https://www.aclweb.org/anthology/D16-1153
https://www.aclweb.org/anthology/D16-1153
https://www.aclweb.org/anthology/D16-1153
https://doi.org/10.1162/tacl_a_00051
https://doi.org/10.1162/tacl_a_00051
https://www.aclweb.org/anthology/P19-1299
https://www.aclweb.org/anthology/P19-1299
https://www.aclweb.org/anthology/P19-1299
https://www.aclweb.org/anthology/L16-1720
https://www.aclweb.org/anthology/L16-1720
https://openreview.net/forum?id=H196sainb
https://www.aclweb.org/anthology/I17-2016
https://www.aclweb.org/anthology/I17-2016
https://www.aclweb.org/anthology/I17-2016
https://www.aclweb.org/anthology/I17-2016
https://www.ijcai.org/proceedings/2018/0566.pdf
https://www.ijcai.org/proceedings/2018/0566.pdf
https://www.ijcai.org/proceedings/2018/0566.pdf
https://www.aclweb.org/anthology/P18-4003
https://www.aclweb.org/anthology/P18-4003
https://www.mitpressjournals.org/doi/pdfplus/10.1162/neco.1997.9.8.1735
https://www.aclweb.org/anthology/N04-1036
https://www.aclweb.org/anthology/N04-1036
https://www.aclweb.org/anthology/N16-1030
https://doi.org/10.18653/v1/P18-1074
https://doi.org/10.18653/v1/P18-1074
https://www.aclweb.org/anthology/P16-1101
https://www.aclweb.org/anthology/P16-1101
https://doi.org/10.18653/v1/D17-1269
https://doi.org/10.18653/v1/D17-1269
https://www.aclweb.org/anthology/P17-1135
https://www.aclweb.org/anthology/P17-1135
https://www.aclweb.org/anthology/P17-1135
https://www.aclweb.org/anthology/P17-1178
https://www.aclweb.org/anthology/P17-1178
https://www.aclweb.org/anthology/D15-1064
https://www.aclweb.org/anthology/D15-1064
https://www.aclweb.org/anthology/D15-1064
https://www.aclweb.org/anthology/P19-1015
https://www.aclweb.org/anthology/P19-1015


6401

Lev Ratinov and Dan Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
Proceedings of the Thirteenth Conference on Com-
putational Natural Language Learning, pages 147–
155, Boulder, Colorado. Association for Compu-
tational Linguistics, Association for Computational
Linguistics.

Leanne Rolston and Katrin Kirchhoff. 2016. Collec-
tion of bilingual data for lexicon transfer learning.
Technical report, Technical Report UW-EE-2016-
0001.

Sebastian Ruder, Ivan Vulić, and Anders Søgaard.
2019. A survey of cross-lingual word embedding
models. Journal of Artificial Intelligence Research,
65:569–631.

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: a simple way to prevent neural networks
from overfitting. The Journal of Machine Learning
Research, 15(1):1929–1958.

Erik F. Tjong Kim Sang. 2002. Introduction to
the conll-2002 shared task: Language-independent
named entity recognition. In Proceedings of the 6th
Conference on Natural Language Learning - Volume
20, COLING-02, pages 1–4, Stroudsburg, PA, USA.
Association for Computational Linguistics.

Erik F. Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the CoNLL-2003 shared task:
Language-independent named entity recognition. In
Proceedings of the Seventh Conference on Natu-
ral Language Learning at HLT-NAACL 2003, pages
142–147, Edmonton, Canada. Association for Com-
putational Linguistics.

Chen-Tse Tsai, Stephen Mayhew, and Dan Roth. 2016.
Cross-lingual named entity recognition via wikifi-
cation. In Proceedings of The 20th SIGNLL Con-
ference on Computational Natural Language Learn-
ing, pages 219–228, Berlin, Germany. Association
for Computational Linguistics.

Dingquan Wang, Nanyun Peng, and Kevin Duh. 2017.
A multi-task learning approach to adapting bilin-
gual word embeddings for cross-lingual named en-
tity recognition. In Proceedings of the Eighth In-
ternational Joint Conference on Natural Language
Processing (Volume 2: Short Papers), pages 383–
388, Taipei, Taiwan. Asian Federation of Natural
Language Processing.

Jiateng Xie, Zhilin Yang, Graham Neubig, Noah A.
Smith, and Jaime Carbonell. 2018. Neural cross-
lingual named entity recognition with minimal re-
sources. In Proceedings of the 2018 Conference on
Empirical Methods in Natural Language Process-
ing, pages 369–379, Brussels, Belgium. Association
for Computational Linguistics.

Zhilin Yang, Ruslan Salakhutdinov, and William W
Cohen. 2017. Transfer learning for sequence tag-

ging with hierarchical recurrent networks. In Inter-
national Conference on Learning Representations,
Palais des Congrès Neptune, Toulon, France.

Joey Tianyi Zhou, Hao Zhang, Di Jin, Hongyuan Zhu,
Meng Fang, Rick Siow Mong Goh, and Kenneth
Kwok. 2019. Dual adversarial neural transfer for
low-resource named entity recognition. In Proceed-
ings of the 57th Conference of the Association for
Computational Linguistics, pages 3461–3471, Flo-
rence, Italy. Association for Computational Linguis-
tics.

https://www.aclweb.org/anthology/W09-1119
https://www.aclweb.org/anthology/W09-1119
https://vannevar.ece.uw.edu/techsite/papers/documents/UWEETR-2016-0001.pdf
https://vannevar.ece.uw.edu/techsite/papers/documents/UWEETR-2016-0001.pdf
https://doi.org/10.1613/jair.1.11640
https://doi.org/10.1613/jair.1.11640
http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf
http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf
https://doi.org/10.3115/1118853.1118877
https://doi.org/10.3115/1118853.1118877
https://doi.org/10.3115/1118853.1118877
https://www.aclweb.org/anthology/W03-0419
https://www.aclweb.org/anthology/W03-0419
https://www.aclweb.org/anthology/K16-1022
https://www.aclweb.org/anthology/K16-1022
https://www.aclweb.org/anthology/I17-2065
https://www.aclweb.org/anthology/I17-2065
https://www.aclweb.org/anthology/I17-2065
https://doi.org/10.18653/v1/D18-1034
https://doi.org/10.18653/v1/D18-1034
https://doi.org/10.18653/v1/D18-1034
https://openreview.net/pdf?id=ByxpMd9lx
https://openreview.net/pdf?id=ByxpMd9lx
https://www.aclweb.org/anthology/P19-1336
https://www.aclweb.org/anthology/P19-1336

