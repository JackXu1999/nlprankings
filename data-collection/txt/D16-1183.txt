



















































Neural Shift-Reduce CCG Semantic Parsing


Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1775–1786,
Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics

Neural Shift-Reduce CCG Semantic Parsing

Dipendra K. Misra and Yoav Artzi
Department of Computer Science and Cornell Tech

Cornell University
New York, NY 10011

{dkm,yoav}@cs.cornell.edu

Abstract

We present a shift-reduce CCG semantic
parser. Our parser uses a neural network ar-
chitecture that balances model capacity and
computational cost. We train by transferring a
model from a computationally expensive log-
linear CKY parser. Our learner addresses two
challenges: selecting the best parse for learn-
ing when the CKY parser generates multiple
correct trees, and learning from partial deriva-
tions when the CKY parser fails to parse. We
evaluate on AMR parsing. Our parser per-
forms comparably to the CKY parser, while
doing significantly fewer operations. We also
present results for greedy semantic parsing
with a relatively small drop in performance.

1 Introduction

Shift-reduce parsing is a class of parsing methods
that guarantees a linear number of operations in sen-
tence length. This is a desired property for practical
applications that require processing large amounts of
text or real-time response. Recently, such techniques
were used to build state-of-the-art syntactic parsers,
and have demonstrated the effectiveness of deep
neural architectures for decision making in linear-
time dependency parsing (Chen and Manning, 2014;
Dyer et al., 2015; Andor et al., 2016; Kiperwasser
and Goldberg, 2016). In contrast, semantic parsing
often relies on algorithms with polynomial number
of operations, which results in slow parsing times
unsuitable for practical applications. In this paper,
we apply shift-reduce parsing to semantic parsing.
Specifically, we study transferring a learned Combi-
natory Categorial Grammar (CCG; Steedman, 1996,

2000) from a dynamic-programming CKY model to
a shift-reduce neural network architecture.

We focus on the feed-forward architecture of
Chen and Manning (2014), where each parsing step
is a multi-class classification problem. The state of
the parser is represented using simple feature em-
beddings that are passed through a multilayer per-
ceptron to select the next action. While simple, the
capacity of this model to capture interactions be-
tween primitive features, instead of relying on sparse
complex features, has led to new state-of-the-art per-
formance (Andor et al., 2016). However, applying
this architecture to semantic parsing presents learn-
ing and inference challenges.

In contrast to dependency parsing, semantic pars-
ing corpora include sentences labeled with the sys-
tem response or the target formal representation, and
omit derivation information. CCG induction from
such data relies on latent-variable techniques and re-
quires careful initialization (e.g., Zettlemoyer and
Collins, 2005, 2007). Such feature initialization
does not directly transfer to a neural network archi-
tecture with dense embeddings, and the use of hid-
den layers further complicates learning by adding
a large number of latent variables. We focus on
data that includes sentence-representation pairs, and
learn from a previously induced log-linear CKY
parser. This drastically simplifies learning, and can
be viewed as bootstrapping a fast parser from a slow
one. While this dramatically narrows down the num-
ber of parses per sentence, it does not eliminate am-
biguity. In our experiments, we often get multiple
correct parses, up to 49K in some cases. We also
observe that the CKY parser generates no parses for

1775



Some old networks remain inoperable

NP[x]/N[x] N[x]/N[x] N[pl] S\NP[pl]/(N[pl]/N[pl]) N[x]/N[x]
λf.A(λx.f(x) ∧ quant(x, λf.λx.f(x)∧ λn.network(n) λf.λx.f(λr.remain-01(r)∧ λf.λx.f(x) ∧ ARG3(x,
A(λs.some(s)))) mod(x,A(λo.old(o))) ARG1(r, x)) A(λp.possible(p) ∧ polarity(p,−)∧

domain(p,A(λo.operate-01(o)))))
> >

N[pl] S\NP[pl]
λn.network(n)∧ λx.λr.remain-01(r) ∧ ARG1(r, x) ∧ ARG3(r,A(λp.possible(p)

mod(n,A(λo.old(o))) ∧polarity(p,−) ∧ domain(p,A(λo.operate-01(o)))))
>

NP[pl]
A(λn.network(n) ∧mod(n,A(λo.old(o))) ∧ quant(n,A(λs.some(s))))

<
S

λr.remain-01(r) ∧ ARG1(r,A(λn.network(n) ∧mod(n,A(λo.old(o))) ∧ quant(n,A(λs.some(s)))))∧
ARG3(r,A(λp.possible(p) ∧ polarity(p,−) ∧ domain(p,A(λo.operate-01(o)))))

Figure 1: Example CCG tree with five lexical entries, three forward applications (>) and a backward application (<).

a significant number of training sentences. There-
fore, we propose an iterative algorithm that automat-
ically selects the best parses for training at each iter-
ation, and identifies partial derivations for best-effort
learning, if no parses are available.

CCG parsing largely relies on two types of ac-
tions: using a lexicon to map words to their cate-
gories, and combining categories to acquire the cat-
egories of larger phrases. In most semantic pars-
ing approaches, the number of operations is dom-
inated by the large number of categories available
for each word in the lexicon. For example, the lex-
icon in our experiments includes 1.7M entries, re-
sulting in an average of 146, and up to 2K, ap-
plicable actions. Additionally, both operations and
parser state have complex structures, for example
including both syntactic and semantic information.
Therefore, unlike in dependency parsing (Chen and
Manning, 2014), we can not treat action selection as
multi-class classification, and must design an archi-
tecture that can accommodate a varying number of
actions. We present a network architecture that con-
siders a variable number of actions, and emphasizes
low computational overhead per action, instead fo-
cusing computation on representing the parser state.

We evaluate on Abstract Meaning Representa-
tion (AMR; Banarescu et al., 2013) parsing. We
demonstrate that our modeling and learning contri-
butions are crucial to effectively commit to early de-
cisions during parsing. Somewhat surprisingly, our
shift-reduce parser provides equivalent performance
to the CKY parser used to generate the training data,
despite requiring significantly fewer operations, on
average two orders of magnitude less. Similar to
previous work, we use beam search, but also, for
the first time, report greedy CCG semantic parsing
results at a relatively modest 9% decrease in perfor-
mance, while the source CKY parser with a beam of
one demonstrates a 71% decrease. While we focus

on semantic parsing, our learning approach makes
no task-specific assumptions and has potential for
learning efficient models for structured prediction
from the output of more expensive ones.1

2 Task and Background

Our goal is to learn a function that, given a sentence
x, maps it to a formal representation of its meaning
z with a linear number of operations in the length of
x. We assume access to a training set ofN examples
D = {(x(i), z(i))}Ni=1, each containing a sentence
x(i) and a logical form z(i). Since D does not con-
tain complete derivations, we instead assume access
to a CKY parser learned from the same data. We
evaluate performance on a test set {(x(i), z(i))}Mi=1
of M sentences x(i) labeled with logical forms z(i).
While we describe our approach in general terms,
we apply our approach to AMR parsing and evalu-
ate on a common benchmark (Section 6).

To map sentences to logical forms, we use CCG,
a linguistically-motivated grammar formalism for
modeling a wide-range of syntactic and seman-
tic phenomena (Steedman, 1996, 2000). A CCG
is defined by a lexicon Λ and sets of unary Ru
and binary Rb rules. In CCG parse trees, each
node is a category. Figure 1 shows a CCG tree
for the sentence Some old networks remain inop-
erable. For example, S\NP[pl]/(N[pl]/N[pl]) :
λf.λx.f(λr.remain-01(r)∧ARG1(r, x)) is the cat-
egory of the verb remain. The syntactic type
S\NP[pl]/(N[pl]/N[pl]) indicates that two argu-
ments are expected: first an adjectiveN[pl]/N[pl] and
then a plural noun phrase NP[pl]. The final syntac-
tic type will be S. The forward slash / indicates
the argument is expected on the right, and the back-
ward slash \ indicates it is expected on the left. The
syntactic attribute pl is used to express the plural-

1The source code and pre-trained models are available at
http://www.cs.cornell.edu/~dkm/ccgparser.

1776



ity constraint of the verb. The simply-typed lambda
calculus logical form in the category represents se-
mantic meaning. The typing system includes atomic
types (e.g., entity e, truth value t) and functional
types (e.g., 〈e, t〉 is the type of a function from e to
t). In the example category above, the expression on
the right of the colon is a 〈〈〈e, t〉, 〈e, t〉〉, 〈e, 〈e, t〉〉〉-
typed function expecting first an adjectival modi-
fier and then an ARG1 modifier. The conjunction
∧ specifies the roles of remain-01. The lexicon Λ
maps words to CCG categories. For example, the
lexical entry remain ` S\NP[pl]/(N[pl]/N[pl]) :
λf.λx.f(λr.remain-01(r) ∧ARG1(r, x)) pairs the
example category with remain. The parse tree in the
figure includes four binary operations: three forward
applications (>) and a backward application (<).

3 Neural Shift Reduce Semantic Parsing

Given a sentence x = 〈x1, . . . , xm〉 with m tokens
xi and a CCG lexicon Λ, let GEN(x; Λ) be a function
that generates CCG parse trees. We design GEN as
a shift-reduce parser, and score decisions using em-
beddings of parser states and candidate actions.

3.1 Shift-Reduce Parsing for CCG
Shift-reduce parsers perform a single pass of the
sentence from left to right to construct a parse tree.
The parser configuration2 is defined with a stack and
a buffer. The stack contains partial parse trees, and
the buffer the remainder of the sentence to be pro-
cessed. Formally, a parser configuration c is a tu-
ple 〈σ, β〉, where the stack σ is a list of CCG trees
[sl · · · s1], and the buffer β is a list of tokens from x
to be processed [xi · · ·xm].3 For example, the top-
left of Figure 2 shows a parsing configuration with
two partial trees on the stack and two words on the
buffer (remain and inoperable).

Parsing starts with the configuration
〈[], [x1 · · ·xm]〉, where the stack is empty and
the buffer is initialized with x. In each parsing
step, the parser either consumes a word from
the buffer and pushes a new tree to the stack, or
applies a parsing rule to the trees at the top of the
stack. For simplicity, we apply CCG rules to trees,

2We use the terms parser configuration and parser state
interchangeably.

3The head of the stack σ is the right-most entry, and the
head of the buffer β is the left-most entry.

where a rule is applied to the root categories of the
argument trees to create a new tree with the argu-
ments as children. We treat lexical entries as trees
with a single node. There are three types of actions:4

SHIFT(l, 〈σ, xi| · · · |xj |β〉) = 〈σ|g, β〉
BINARY(b, 〈σ|s2|s1, β〉) = 〈σ|b(s2, s1), β〉
UNARY(u, 〈σ|s1, β〉) = 〈σ|u(s1), β〉 .

Where b ∈ Rb is a binary rule, u ∈ Ru is a unary
rule, and l is a lexical entry xi, . . . xj ` g for the to-
kens xi,. . . ,xj and CCG category g. SHIFT creates a
tree given a lexical entry for the words at the top of
the buffer, BINARY applies a binary rule to the two
trees at the head of the stack, and UNARY applies a
unary rule to the tree at head of the stack. A config-
uration is terminal when no action is applicable.

Given a sentence x, a derivation is a sequence of
action-configuration pairs 〈〈c1, a1〉, . . . , 〈ck, ak〉〉,
where action ai is applied to configuration ci to gen-
erate configuration ci+1. The result configuration
ck+1 is of the form 〈[s], []〉, where s represents a
complete parse tree, and the logical form z at the
root category represents the meaning of the com-
plete sentence. Following previous work with CKY
parsing (Zettlemoyer and Collins, 2005), we disal-
low consecutive unary actions. We denote the set of
actions allowed from configuration c as A(c).

3.2 Model

Our goal is to balance computation and model ca-
pacity. To recover a rich representation of the con-
figuration, we use a multilayer perceptron (MLP) to
create expressive interactions between a small num-
ber of simple features. However, since we con-
sider many possible actions in each step, comput-
ing activations for multiple hidden layers for each
action is prohibitively expensive. Instead, we opt
for a computationally-inexpensive action represen-
tation computed by concatenating feature embed-
dings. Figure 2 illustrates our architecture.

Given a configuration c, the probability of an ac-
tion a is:

p(a | c) = exp {φ(a, c)WbF(ξ(c))}∑
a′∈A(c) exp {φ(a′, c)WbF(ξ(c))}

,

4We follow the standard notation of L|x indicating a list
with all the entries from L and x as the right-most element.

1777



Stack Buffer

h2 = max{0,W2h1 + b2}

h1 = max{0,W1h0 + b1}

h3 = W3h2 + b3

Embedding Layer

Hidden 
Layers

Dimensionality 
Reduction Layer

Embedding Layer

Embedding Layer

Bilinear Softmax Layer

Configuration 
Embedding   

�(a1, c)

�(a2, c)

cConfiguration A(c)Actions

FMLP

⇠(c)

s2 s1 b2

Some old networks remain inoperable

NP[x]/N[x] N[x]/N[x] N[pl] S\NP[pl]/(N[pl]/N[pl]) N[x]/N[x]
�f.A(�x.f(x) ^ quant(x, �f.�x.f(x)^ �n.network(n) �f.�x.f(�r.remain-01(r)^ �f.�x.f(x) ^ARG3(x,A(�p.possible(p)^

A(�s.some(s)))) MOD(x,A(�o.old(o))) ARG1(r, x)) polarity(p,�) ^ domain(p,A(�o.operate-01(o)))))
> >

N[pl] S\NP[pl]
�n.network(n)^ �x.�r.remain-01(r) ^ARG1(r, x) ^ARG3(r,A(�p.possible(p)

MOD(n,A(�o.old(o))) ^polarity(p,�) ^ domain(p,A(�o.operate-01(o)))))
>

NP[pl]
A(�n.network(n) ^MOD(n,A(�o.old(o))) ^ quant(n,A(�s.some(s))))

<
S

�r.remain-01(r) ^ARG1(r,A(�n.network(n) ^MOD(n,A(�o.old(o))) ^ quant(n,A(�s.some(s)))))^
ARG3(r,A(�p.possible(p) ^ polarity(p,�) ^ domain(p,A(�o.operate-01(o)))))

1

Some old networks remain inoperable

NP[x]/N[x] N[x]/N[x] N[pl] S\NP[pl]/(N[pl]/N[pl]) N[x]/N[x]
�f.A(�x.f(x) ^ quant(x, �f.�x.f(x)^ �n.network(n) �f.�x.f(�r.remain-01(r)^ �f.�x.f(x) ^ARG3(x,A(�p.possible(p)^

A(�s.some(s)))) MOD(x,A(�o.old(o))) ARG1(r, x)) polarity(p,�) ^ domain(p,A(�o.operate-01(o)))))
> >

N[pl] S\NP[pl]
�n.network(n)^ �x.�r.remain-01(r) ^ARG1(r, x) ^ARG3(r,A(�p.possible(p)

MOD(n,A(�o.old(o))) ^polarity(p,�) ^ domain(p,A(�o.operate-01(o)))))
>

NP[pl]
A(�n.network(n) ^MOD(n,A(�o.old(o))) ^ quant(n,A(�s.some(s))))

<
S

�r.remain-01(r) ^ARG1(r,A(�n.network(n) ^MOD(n,A(�o.old(o))) ^ quant(n,A(�s.some(s)))))^
ARG3(r,A(�p.possible(p) ^ polarity(p,�) ^ domain(p,A(�o.operate-01(o)))))

1

Some old networks remain inoperable

NP[x]/N[x] N[x]/N[x] N[pl] S\NP[pl]/(N[pl]/N[pl]) N[x]/N[x]
�f.A(�x.f(x) ^ quant(x, �f.�x.f(x)^ �n.network(n) �f.�x.f(�r.remain-01(r)^ �f.�x.f(x) ^ARG3(x,A(�p.possible(p)^

A(�s.some(s)))) MOD(x,A(�o.old(o))) ARG1(r, x)) polarity(p,�) ^ domain(p,A(�o.operate-01(o)))))
> >

N[pl] S\NP[pl]
�n.network(n)^ �x.�r.remain-01(r) ^ARG1(r, x) ^ARG3(r,A(�p.possible(p)

MOD(n,A(�o.old(o))) ^polarity(p,�) ^ domain(p,A(�o.operate-01(o)))))
>

NP[pl]
A(�n.network(n) ^MOD(n,A(�o.old(o))) ^ quant(n,A(�s.some(s))))

<
S

�r.remain-01(r) ^ARG1(r,A(�n.network(n) ^MOD(n,A(�o.old(o))) ^ quant(n,A(�s.some(s)))))^
ARG3(r,A(�p.possible(p) ^ polarity(p,�) ^ domain(p,A(�o.operate-01(o)))))

1

b1

pl

xNP/N

N

Some old networks remain inoperable

NP[x]/N[x] N[x]/N[x] N[pl] S\NP[pl]/(N[pl]/N[pl]) N[x]/N[x]
�f.A(�x.f(x) ^ quant(x, �f.�x.f(x)^ �n.network(n) �f.�x.f(�r.remain-01(r)^ �f.�x.f(x) ^ARG3(x,A(�p.possible(p)^

A(�s.some(s)))) MOD(x,A(�o.old(o))) ARG1(r, x)) polarity(p,�) ^ domain(p,A(�o.operate-01(o)))))
> >

N[pl] S\NP[pl]
�n.network(n)^ �x.�r.remain-01(r) ^ARG1(r, x) ^ARG3(r,A(�p.possible(p)

MOD(n,A(�o.old(o))) ^polarity(p,�) ^ domain(p,A(�o.operate-01(o)))))
>

NP[pl]
A(�n.network(n) ^MOD(n,A(�o.old(o))) ^ quant(n,A(�s.some(s))))

<
S

�r.remain-01(r) ^ARG1(r,A(�n.network(n) ^MOD(n,A(�o.old(o))) ^ quant(n,A(�s.some(s)))))^
ARG3(r,A(�p.possible(p) ^ polarity(p,�) ^ domain(p,A(�o.operate-01(o)))))

1

Some old networks remain inoperable

NP[x]/N[x] N[x]/N[x] N[pl] S\NP[pl]/(N[pl]/N[pl]) N[x]/N[x]
�f.A(�x.f(x) ^ quant(x, �f.�x.f(x)^ �n.network(n) �f.�x.f(�r.remain-01(r)^ �f.�x.f(x) ^ARG3(x,A(�p.possible(p)^

A(�s.some(s)))) MOD(x,A(�o.old(o))) ARG1(r, x)) polarity(p,�) ^ domain(p,A(�o.operate-01(o)))))
> >

N[pl] S\NP[pl]
�n.network(n)^ �x.�r.remain-01(r) ^ARG1(r, x) ^ARG3(r,A(�p.possible(p)

MOD(n,A(�o.old(o))) ^polarity(p,�) ^ domain(p,A(�o.operate-01(o)))))
>

NP[pl]
A(�n.network(n) ^MOD(n,A(�o.old(o))) ^ quant(n,A(�s.some(s))))

<
S

�r.remain-01(r) ^ARG1(r,A(�n.network(n) ^MOD(n,A(�o.old(o))) ^ quant(n,A(�s.some(s)))))^
ARG3(r,A(�p.possible(p) ^ polarity(p,�) ^ domain(p,A(�o.operate-01(o)))))

1

a1 = Binary(forward-apply, c)

a2 = Unary(bare-plural, c)

Some networks remain inoperable

NP[x]/N[x] N[pl] S\NP[pl]/(N[pl]/N[pl]) N[x]/N[x]
�f.A1(�x.f(x) ^ REL(x, �n.network(n) �f.�x.f(�r.remain-01(r)^ �f.�x.f(x) ^ REL(x,A3(�p.possible(p) ^ REL(p,�)^

A2(�s.some(s)))) ARG1(r, x)) REL(p,A4(�o.operate-01(o)))))
> >

NP[pl] S\NP[pl]
A1(�n.network(n) ^ REL(n,A2(�s.some(s)))) �x.�r.remain-01(r) ^ARG1(r, x) ^ REL(r,A3(�p.possible(p) ^ REL(p,�)^

REL(p,A4(�o.operate-01(o)))))
<

S
�r.remain-01(r) ^ARG1(r,A1(�n.network(n) ^ REL(n,A2(�s.some(s)))))^
REL(r,A3(�p.possible(p) ^ REL(p,�) ^ REL(p,A4(�o.operate-01(o)))))

1

a|A(c)| = Shift

 
, c

!

P (ai) / exp(�(ai, c)Wbh3)
8i = 1 . . . |A(c)|

�(a|A(c)|, c)

Figure 2: Illustration of scoring the next action given the configuration c when parsing the sentence Some old networks
remain inoperable. Embeddings of the same feature type are colored the same. The configuration embedding ξ(c) is a
concatenation of syntax embeddings (green) and the logical form embedding (blue; computed by ψ) for the top entries
in the stack. We then pass ξ(c) through the MLP F . Given the actions A(c), we compute the embeddings φ(ai, c),
i = 1 . . . |A(c)|. The actions and MLP representation are combined with a bilinear softmax layer. The number of
concatenated vectors and stack elements used is for illustration. The details are described in Section 3.2.

where φ(a, c) is the action embedding, ξ(c) is the
configuration embedding, and F is an MLP. Wb
is a bilinear transformation matrix. Given a sen-
tence x and a sequence of action-configuration pairs
〈〈c1, a1〉, . . . , 〈ck, ak〉〉, the probability of a CCG
tree y is

p(y | x) =
∏

i=1...k

p(ai | ci) .

The probability of a logical form z is then
p(z | x) =

∑

y∈Y(z)
p(y | x) ,

where Y(z) is the set of CCG trees with the logical
form z at the root.

MLP Architecture F We use a MLP with two
hidden layers parameterized by {W1,W2,b1,b2}
with a ReLu non-linearity (Glorot et al., 2011).
Since the output of F influences the dimensionality
of Wb, we add a linear layer parameterized by W3
and b3 to reduce the dimensionality of the configu-
ration, thereby reducing the dimensionality of Wb.

Configuration Embedding ξ(c) Given a config-
uration c = 〈[sl · · · s1], [xi · · ·xm]〉, the input to F
is a concatenation of syntactic and semantic embed-
dings, as illustrated in Figure 2. We concatenate em-

beddings from the top three trees in the stack s1, s2,
s3.5 When a feature is not present, for example when
the stack or buffer are too small, we use a tunable
null embedding.

Given a tree on the stack sj , we define two syn-
tactic features: attribute set and stripped syntax.
The attribute feature is created by extracting all the
syntactic attributes of the root category of sj . The
stripped syntax feature is the syntax of the root cat-
egory without the syntactic attributes. For example,
in Figure 2, we embed the stripped category N and
attribute pl for s1, and NP/N and x for s2. The at-
tributes are separated from the syntax to reduce spar-
sity, and the interaction between them is computed
by F . The sparse features are converted to dense
embeddings using a lookup table and concatenated.
In addition, we also embed the logical form at the
root of sj . Figure 3 illustrates the recursive embed-
ding function ψ.6 Using a recursive function to em-
bed logical forms is computationally intensive. Due
to strong correlation between sentence length and
logical form complexity, this computation increases

5For simplicity, the figure shows only the top two trees.
6The algorithm is provided in the supplementary material.

1778



{Wr, �r}

{Wr, �r}

{Wr, �r}

JOHN

arg0

e

x

x

�x.arg0(x, JOHN)

arg0(x, JOHN)

(x, JOHN)

he, he, tii
{Wr, �r}

{Wr, �r}

JOHN

arg0

Figure 3: Illustration of embedding the logical form
λx.arg0(x, JOHN) with the recursive embedding func-
tion ψ. In each level in ψ, the children nodes are com-
bined with a single-layer neural network parameterized
by Wr, δr, and the tanh activation function. Com-
puted embeddings are in dark gray, and embeddings from
lookup tables are in light gray. Constants are embed-
ded by combining name and type embeddings, literals
are unrolled to binary recursive structures, and lambda
terms are combinations of variable type and body em-
beddings. For example, JOHN is embedded by com-
bining the embeddings of its name and type, the literal
arg0(x, JOHN) is recursively embedded by first embed-
ding the arguments (x, JOHN) and then combining the
predicate, and the lambda term is embedded to create the
embedding of the entire logical form.

the cost of configuration embedding by a factor lin-
ear in sentence length. In Section 6, we experiment
with including this option, balancing between poten-
tial expressivity and speed.

Action Embedding φ(a, c) Given an action a ∈
A(c), and the configuration c, we generate the action
representation by computing sparse features, con-
verting them to dense embeddings via table lookup,
and concatenating. If more than one feature of the
same type is triggered, we average their embed-
dings. When no features of a given type are trig-
gered, we use a tunable placeholder embedding in-
stead. The features include all the features used by
Artzi et al. (2015), including all conjunctive fea-
tures, as well as properties of the action and configu-
ration, such as the POS tags of tokens on the buffer.7

Discussion Our use of an MLP is inspired by Chen
and Manning (2014). However, their architecture is
designed to handle only a fixed number of actions,
while we observe varying number of actions. There-
fore, we adopt a probabilistic model similar to Dyer
et al. (2015) to effectively combine the benefits of

7See the supplementary material for feature details.

the two approaches.8 We factorize the exponent in
our objective into action φ(a, c) and configuration
F(ξ(c)) embeddings. While every parse step in-
volves a single configuration, the number of actions
is significantly higher. With the goal of minimizing
the amount of computation per action, we use simple
concatenation only for action embedding. However,
this requires retaining sparse conjunctive action fea-
tures since they are never combined through hidden
layers similar to configuration features.

3.3 Inference

To compute the set of parse trees GEN(x; Λ), we
perform beam search to recover the top-k parses.
The beam contains configurations. At each step, we
expand all configurations with all actions, and keep
only the top-k new configurations. To promote di-
versity in the beam, given two configurations with
the same signature, we keep only the highest scor-
ing one. The signature includes the previous config-
uration in the derivation, the state of the buffer, and
the root categories of all stack elements. Since all
features are computed from these elements, this op-
timization does not affect the max-scoring tree. Ad-
ditionally, since words are assigned structured cat-
egories, a key problem is unknown words or word
uses. Following Zettlemoyer and Collins (2007), we
use a two-pass parsing strategy, and allow skipping
words controlled by the term γ in the second pass.
The term γ is added to the exponent of the action
probability when words are skipped. See the sup-
plementary material for the exact form.

Complexity Analysis The shift-reduce parser pro-
cesses the sentence from left to right with a linear
number of operations in sentence length. We define
an operation as applying an action to a configuration.
Formally, the number of operations for a sentence of
lengthm is bounded byO(4mk(|λ|+ |Rb|+ |Ru|)),
where |λ| is the number of lexical entries per to-
ken, k is the beam size, Rb is the set of binary
rules, and Ru the set of unary rules. In compari-
son, the number of operations for the CKY parser,
where an operation is applying a rule to a single
cell or two adjacent cells in the chart, is bounded
by O(m|λ| + m3k2|Rb| + m2b|Ru|). For sentence

8We experimented with an LSTM parser similar to Dyer
et al. (2015). However, performance was not competitive. This
direction remains an important avenue for future work.

1779



length 25, the mean in our experiments, the shift-
reduce parser performs 100 time fewer operations.
See the supplementary material for the full analysis.

4 Learning

We assume access to a training set of N examples
D = {(x(i), z(i))}Ni=1, each containing a sentence
x(i) and a logical form z(i). The data does not in-
clude information about the lexical entries and CCG
parsing operations required to construct the correct
derivations. We bootstrap this information from a
learned parser. In our experiments we use a learned
dynamic-programming CKY parser. We transfer the
lexicon Λ directly from the input parser, and focus
on estimating the parameters θ, which include fea-
ture embeddings, hidden layer matrices, and bias
terms. The main challenge is learning from the noisy
supervision provided by the input parser. In our ex-
periments, the CKY parser fails to correctly parse
40% of the training data, and returns on average 147
max-scoring correct derivations for the rest. We pro-
pose an iterative algorithm that treats the choice be-
tween multiple parse trees as latent, and effectively
learns from partial analysis when no correct deriva-
tion is available.

The learning algorithm (Algorithm 1) starts by
processing the data using the CKY parser (lines 3 -
4). For each sentence x(i), we collect the max-
scoring CCG trees with z(i) at the root. The CKY
parser often contains many correct parses with iden-
tical scores, up to 49K parses per sentence. There-
fore, we randomly sample and keep up to 1K trees.
This process is done once, and the algorithm then
runs for T iterations. At each iteration, given the sets
of parses from the CKY parser Y , we select the max-
probability parse according to our current parame-
ters θ (line 10) and add all the shift-reduce decisions
from this parse to DA (line 12), the action data set
that we use to estimate the parameters. We approxi-
mate the arg max with beam search using an oracle
computed from the CKY parses.9 CONFGEN aggre-
gates the configuration-action pairs from the highest
scoring derivation. Parse selection depends on θ and
this choice will gradually converge as the parame-
ters improve. The action data set is used to compute
the `2-regularized negative log-likelihood objective

9Our oracle is non-deterministic and incomplete (Goldberg
and Nivre, 2013).

Algorithm 1 The learning algorithm.

Input: Training set D = {(x(i), z(i))}Ni=1, learning rate µ,
regularization parameter `2, and number of iterations T .

Definitions: GENMAXCKY(x, z) returns the set of max-
scoring CKY parses for x with z at the root. SCORE(y, θ)
scores a tree y according to the parameters θ (Section 3.2).
CONFGEN(x, y) is the sequence of action-configuration
pairs that generates y given x (Section 3.1). BP(∆J )
takes the objective J and back-propagates the error ∇J
through the computation graph for the sample used to com-
pute the objective. ADAGRAD(∆) applies a per-feature
learning rate to the gradient ∆ (Duchi et al., 2011).

Output: Model parameters θ.
1: » Get trees from CKY parser.
2: Y ← []
3: for i = 1 to N do
4: Y[i] = GENMAXCKY(x(i), z(i))
5: for t = 1 to T do
6: » Pick max-scoring trees and create action dataset.
7: DA = ∅
8: for i = 1 to N do
9: if Y[i] 6= ∅ then

10: A← CONFGEN(x(i),
11: arg maxy∈Y[i] SCORE(y, θ))
12: for 〈c, a〉 ∈ A do
13: DA ← DA ∪ {〈c, a〉}
14: » Back-propagate the loss through the network.
15: for 〈c, a〉 ∈ DA do
16: J def= − log p(a | c) + `2

2
θT θ

17: ∆← BP(∇J )
18: θ ← θ − µADAGRAD(∆)
19: return θ

J (line 16) and back-propagate the error to compute
the gradient (line 17). We use AdaGrad (Duchi et al.,
2011) to update the parameters θ (line 18).

4.1 Learning from Partial Derivations

The input parser often fails to generate correct
parses. In our experiments, this occurs for 40% of
the training data. In such cases, we can obtain a
forest of partial parse trees Yp. Each partial tree
y ∈ Yp corresponds to a span of tokens in the sen-
tence and is scored by the input parser. In practice,
the spans are often overlapping. Our goal is to gen-
erate high quality configuration-action pairs 〈c, a〉
from Yp. These pairs will be added to DA for train-
ing. While extracting actions a is straightforward,
generating configurations c requires reconstructing
the stack σ from an incomplete forest of partial trees
Yp. Figure 4 illustrates our proposed process. Let
CKYSCORE(y) be the CKY score of the partial tree
y. To reconstruct σ, we select non-overlapping par-

1780



x1 x2 x3 x16

x1:6 x11:14

Figure 4: Partial derivation selection for learning (Sec-
tion 4.1). The dotted triangles represent skipped spans
in the sentence, where no high quality partial trees were
found. Dark triangles represent the selected partial trees.
We identify two contiguous spans, 1-6 and 11-14, and
generate two synthetic sentences for training: the tokens
are treated as complete sentences and actions and stack
state are generated from the partial trees.

tial trees Y that correspond to the entire sentence
by solving arg maxY⊆Yp CKYSCORE(y) under two
constraints: (a) no two trees from Y correspond to
overlapping tokens, and (b) for each token in x, there
exists y ∈ Y that corresponds to it. We solve the
arg max using dynamic programming. The gener-
ated set Y approximates an intermediate state of a
shift-reduce derivation. However, Yp often does not
contain high quality partial derivation for all spans.
To skip low quality partial trees and spans that have
no trees, we generate empty trees ye for every span,
where CKYSCORE(ye) = 0, and add them to Yp.
If the set of selected partial trees Y includes empty
trees, we divide the sentence to separate examples
and ignore these parts. This results in partial and
approximate stack reconstruction. Finally, since YP
is noisy, we prune from it partial trees with a root
that does not match the syntactic type for this span
from an automatically generated CCGBank (Hock-
enmaier and Steedman, 2007) syntactic parse.

Our complete learning algorithm alternates be-
tween epochs of learning with complete parse trees
and learning with partial derivations. In epochs
where we use partial derivations, we use a modified
version of Algorithm 1, where lines 9-10 are updated
to use the above process.

5 Related work

Our approach is inspired by recent results in de-
pendency parsing, specifically by the architecture
of Chen and Manning (2014), which was further
developed by Weiss et al. (2015) and Andor et al.
(2016). Dyer et al. (2015) proposed to encode
the parser state using an LSTM recurrent architec-
ture, which has been shown generalize well between
languages (Ballesteros et al., 2015; Ammar et al.,
2016). Our network architecture combines ideas

from the two threads: we use feature embeddings
and a simple MLP to score actions, while our prob-
ability distribution is similar to the LSTM parser.

The majority of CCG approaches for semantic
parsing rely on CKY parsing with beam search (e.g.,
Zettlemoyer and Collins, 2005, 2007; Kwiatkowski
et al., 2010, 2011; Artzi and Zettlemoyer, 2011,
2013; Artzi et al., 2014; Matuszek et al., 2012;
Kushman and Barzilay, 2013). Semantic parsing
with other formalisms also often relied on CKY-
style algorithms (e.g., Liang et al., 2009; Kim and
Mooney, 2012). With a similar goal to ours, Berant
and Liang (2015) designed an agenda-based parser.
In contrast, we focus on a method with linear num-
ber of operations guarantee.

Following the work of Collins and Roark (2004)
on learning for syntactic parsers, Artzi et al. (2015)
proposed an early update procedure for inducing
CCG grammars with a CKY parser. Our partial
derivations learning method generalizes this method
to parsers with global features.

6 Experimental Setup
Task and Data We evaluate on AMR parsing with
CCG. AMR is a general-purpose meaning represen-
tation, which has been used in multiple tasks (Pan
et al., 2015; Liu et al., 2015; Sawai et al., 2015;
Garg et al., 2016), We use the newswire portion
of AMR Bank 1.0 release (LDC2014T12), which
displays some of the fundamental challenges in se-
mantic parsing, including long newswire sentences
with a broad array of syntactic and semantic phe-
nomena. We follow the standard train/dev/test split
of 6603/826/823 sentences. We evaluate with the
SMATCH metric (Cai and Knight, 2013). Our parser
is incorporated into the two-stage approach of Artzi
et al. (2015). The approach includes a bi-directional
and deterministic conversion between AMR and
lambda calculus. Distant references, for example
such as introduced by pronouns, are represented
using Skolem IDs, globally-scoped existentially-
quantified unique IDs. A derivation includes a CCG
tree, which maps the sentence to an underspecified
logical form, and a constant mapping, which maps
underspecified elements to their fully specified form.
The key to the approach is the underspecified logi-
cal forms, where distant references and most rela-
tions are not fully specified, but instead represented

1781



AMR Underspecified Logical Form Logical Form
(c/conclude-02

:ARG0 (l/lawyer)
:ARG1 (a/argument

:poss l)
:time (l2/late))

A1(λc.conclude-02(c) ∧
ARG0(c,A2(λl.lawyer(l))) ∧
ARG1(c,A3(λa.argument(a) ∧

poss(a,R(IDIDID)))) ∧
RELRELREL(c,A4(λl2.late(l2))))

A1(λc.conclude-02(c) ∧
ARG0(c,A2(λl.lawyer(l))) ∧
ARG1(c,A3(λa.argument(a) ∧

poss(a,R(2)))) ∧
time(c,A4(λl2.late(l2))))

Figure 5: AMR for the sentence the lawyer concluded his arguments late. In Artzi et al. (2015), The AMR (left) is
deterministically converted to the logical form (right). The underspecified logical form is the result of the first stage,
CCG parsing, and contains two placeholders (bolded): ID for a reference, and REL for a relation. To generate the
final logical form, the second stage resolves ID to the identifier of the lawyer (2), and REL to the relation time. We
focus on a model for the first stage and use an existing model for the second stage.

as placeholders. Figure 5 shows an example AMR,
its lambda calculus conversion, and its underspec-
ified logical form. (Artzi et al., 2015) use a CKY
parser to identify the best CCG tree, and a factor
graph for the second stage. We integrate our shift-
reduce parser into the two-stage setup by replacing
the CKY parser. We use the same CCG configura-
tion and integrate our parser into the join probabilis-
tic model. Formally, given a sentence x, the proba-
bility of an AMR logical form z is

p(z | x) =
∑

u

p(z | u, x)
∑

y∈Y(u)
p(y | x) ,

where u is an underspecified logical form, Y(u) is
the set of CCG trees with u at the root. We use our
shift-reduce parser to compute p(y | x) and use the
pre-trained model from Artzi et al. (2015) for p(z |
u, x). Following Artzi et al. (2015), we disallow
configurations that will not result in a valid AMR,
and design a heuristic post-processing technique to
recover a single logical form from terminal config-
urations that include multiple disconnected partial
trees on the stack. We use the recovery technique
when no complete parses are available.

Tools We evaluate with the SMATCH metric (Cai
and Knight, 2013). We use EasyCCG (Lewis and
Steedman, 2014) for CCGBank categories (Sec-
tion 4.1). We implement our system using Cornell
SPF (Artzi, 2016), and the deeplearning4j library.10

The setup of Artzi et al. (2015) also includes the Illi-
nois NER (Ratinov and Roth, 2009) and Stanford
CoreNLP POS Tagger (Manning et al., 2014).

Parameters and Initialization We minimize our
loss on a held-out 10% of the training data to tune
our parameters, and train the final model on the
full data. We set the number of epochs T = 3,
regularization coefficient `2 = 10−6, learning rate

10http://deeplearning4j.org/

Parser P R F
CKY (Artzi et al., 2015) 67.2 65.1 66.1
Greedy CKY 64.1 11.29 19.19
SR (complete model) 67.0 63.4 65.3

w/o semantic embedding 67.1 63.3 65.1
w/o partial derivation learning 66.0 62.2 64.0

Ensemble SR (syntax) 68.2 64.1 66.0
Ensemble SR (syntax, semantics) 68.1 63.9 65.9
SR with CKY model 52.5 49.36 50.88

Table 1: Development SMATCH results.

Parser P R F

JAMR11 67.8 59.2 63.2
CKY (Artzi et al., 2015) 66.8 65.7 66.3
Shift Reduce 68.1 64.2 66.1

Wang et al. (2015a)13 72.0 67.0 70.0

Table 2: Test SMATCH results.12

µ = 0.05, skipping term γ = 1.0. We set the di-
mensionality of feature embeddings based on the vo-
cabulary size of the feature type. The exact dimen-
sions are listed in the supplementary material. We
use 65 ReLU units for h1 and h2, and 50 units for
h3. We initialize θ with the initialization scheme of
Glorot and Bengio (2010), except the bias term for
ReLu layers, which we initialize to 0.1 to increase
the number of active units on initialization. During
test, we use the vector 0 as embedding for unseen
features. We use a beam of 512 for testing and 2 for
CONFGEN (Section 4).
Model Ensemble For our final results, we
marginalize the output over three models M using
p(z | x, θ,Λ) = 1|M |

∑
m∈M p(z | m,x, θ,Λ).

7 Results
Table 1 shows development results. We trained each
model three times and report the best performance.
We observed a variance of roughly 0.5 in these runs.
We experimented with different features for con-
figuration embedding and with removing learning
with partial derivations (Section 4.1). The com-

1782



plete model gives the best single-model performance
of 65.3 F1 SMATCH, and we observe the benefits
for semantic embeddings and learning from partial
derivations. Using partial derivations allowed us
to learn 370K more features, 22% of observed em-
beddings. We also evaluate ensemble performance.
We observe an overall improvement in performance.
However, with multiple models, the benefit of us-
ing semantic embeddings vanishes. This result is
encouraging since semantic embeddings can be ex-
pensive to compute if the logical form grows with
sentence length. We also provide results for run-
ning a shift-reduce log-linear parser p(a | c) ∝
exp{wTφCKY(a, c)} using the input CKY model.
We observe a significant drop in performance, which
demonstrates the overall benefit of our architecture.

Figure 6 shows the development performance of
our best performing ensemble model for different
beam sizes. The performance decays slowly with
decreasing beam size. Surprisingly, our greedy
parser achieves 59.77 SMATCH F1, while the CKY
parser with a beam of 1 achieves only 19.2 SMATCH
F1 (Table 1). This allows our parser to trade-off a
modest drop in accuracy for a significant improve-
ment in runtime.

Table 2 shows the test results using our best per-
forming model (ensemble with syntax features). We
compare our approach to the CKY parser of Artzi
et al. (2015) and JAMR (Flanigan et al., 2014).11,12

We also list the results of Wang et al. (2015b), who
demonstrated the benefit of auxiliary analyzers and
is the current state of the art.13 Our performance is
comparable to the CKY parser of (Artzi et al., 2015),
which we use to bootstrap our system. This demon-
strates the ability of our parser to match the perfor-
mance of a dynamic-programming parser, which ex-
ecutes significantly more operations per sentence.

Finally, Figure 7 shows our parser runtime rel-
ative to sentence length. In this analysis, we fo-
cus on runtime, and therefore use a single model.

11 JAMR results are taken from Artzi et al. (2015).
12 Pust et al. (2015), Flanigan et al. (2014), and Wang et al.

(2015b) report results on different sections of the corpus. These
results are not comparable to ours.

13Our goal is to study the effectiveness of our model trans-
fer approach and architecture. Therefore, we avoid using any
resources used in (Wang et al., 2015b) that are not used in the
CKY parser we compare to.

1 32 128 256 512 600

59

62

66

Beam size

S
M

A
T

C
H

F1

Figure 6: The effect of beam size on model performance.

5 10 15 20 25 30 35 40 45 50 55 60
0

20

40

60

Sentence length

W
al

lt
im

e
(s

ec
.)

Figure 7: Wall-time performance of shift reduce parser
with only syntax features (blue), with syntax and seman-
tic features (orange) and the CKY parser of Artzi et al.
(2015) (black).

We compare two versions of our system, including
and excluding semantic embeddings, and the CKY
parser of Artzi et al. (2015). We run both parsers
with 16 cores and 122GB memory. The shift-reduce
parser is three times faster on average, and up to ten
times faster on long sentences. Since our parser is
currently using CPUs, future work focused on GPU
porting is likely to see further improvements.

8 Conclusion
Our parser design emphasizes a balance between
model capacity and the ability to combine atomic
features against the computational cost of scor-
ing actions. We also design a learning algorithm
to transfer learned models and learn neural net-
work models from ambiguous and partial supervi-
sion. Our model shares many commonalities with
transition-based dependency parsers. This makes it
a good starting point to study the effectiveness of
other dependency parsing techniques for semantic
parsing, for example global normalization (Andor
et al., 2016) and bidirectional LSTM feature repre-
sentations (Kiperwasser and Goldberg, 2016).

Acknowledgments
This research was supported in part by gifts from
Google and Amazon. The authors thank Kenton Lee
for technical advice, and Adam Gibson and Alex
Black of Skymind for help with Deeplearning4j. We
also thank Tom Kwiatkowski, Arzoo Katiyar, Tianze
Shi, Vlad Niculae, the Cornell NLP Group, and the
reviewers for helpful advice.

1783



References

Ammar, W., Mulcaire, G., Ballesteros, M., Dyer, C.,
and Smith, N. A. (2016). Many languages, one
parser. Transactions of the Association for Com-
putational Linguistics.

Andor, D., Alberti, C., Weiss, D., Severyn, A.,
Presta, A., Ganchev, K., Petrov, S., and Collins,
M. (2016). Globally normalized transition-based
neural networks. CoRR.

Artzi, Y. (2016). Cornell SPF: Cornell semantic
parsing framework. ArXiv e-prints.

Artzi, Y., Das, D., and Petrov, S. (2014). Learn-
ing compact lexicons for CCG semantic parsing.
In Proceedings of the Conference on Empirical
Methods in Natural Language Processing.

Artzi, Y., Lee, K., and Zettlemoyer, L. (2015).
Broad-coverage CCG semantic parsing with
AMR. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing.

Artzi, Y. and Zettlemoyer, L. S. (2011). Bootstrap-
ping semantic parsers from conversations. In Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing.

Artzi, Y. and Zettlemoyer, L. S. (2013). Weakly su-
pervised learning of semantic parsers for mapping
instructions to actions. Transactions of the Asso-
ciation for Computational Linguistics, 1.

Ballesteros, M., Dyer, C., and Smith, N. A. (2015).
Improved transition-based parsing by modeling
characters instead of words with LSTMs.

Banarescu, L., Bonial, C., Cai, S., Georgescu, M.,
Griffitt, K., Hermjakob, U., Knight, K., Koehn,
P., Palmer, M., and Schneider, N. (2013). Abstract
meaning representation for sembanking. In Pro-
ceedings of the Linguistic Annotation Workshop.

Berant, J. and Liang, P. (2015). Imitation learning
of agenda-based semantic parsers. Transactions
of the Association for Computational Linguistics,
3.

Cai, S. and Knight, K. (2013). Smatch: an eval-
uation metric for semantic feature structures. In
Proceedings of the Conference of the Association
of Computational Linguistics.

Chen, D. and Manning, C. D. (2014). A fast and ac-
curate dependency parser using neural networks.
In Proceedings of the Conference on Empirical
Methods in Natural Language Processing.

Collins, M. and Roark, B. (2004). Incremental pars-
ing with the perceptron algorithm. In Proceedings
of the Annual Meeting on Association for Compu-
tational Linguistics.

Duchi, J., Hazan, E., and Singer, Y. (2011). Adap-
tive subgradient methods for online learning and
stochastic optimization. The Journal of Machine
Learning Research.

Dyer, C., Ballesteros, M., Ling, W., Matthews, A.,
and Smith, N. A. (2015). Transition-based depen-
dency parsing with stack long short-term memory.
In Proceedings of the Annual Meeting on Associ-
ation for Computational Linguistics.

Flanigan, J., Thomson, S., Carbonell, J., Dyer, C.,
and Smith, N. A. (2014). A discriminative graph-
based parser for the Abstract Meaning Represen-
tation. In Proceedings of the Conference of the
Association of Computational Linguistics.

Garg, S., Galstyan, A., Hermjakob, U., and Marcu,
D. (2016). Extracting biomolecular interactions
using semantic parsing of biomedical text. In Pro-
ceedings of the Conference on Artificial Intelli-
gence.

Glorot, X. and Bengio, Y. (2010). Understanding the
difficulty of training deep feedforward neural net-
works. In International Conference on Artificial
Intelligence and Statistics.

Glorot, X., Bordes, A., and Bengio, Y. (2011). Deep
sparse rectifier neural networks. In International
Conference on Artificial Intelligence and Statis-
tics.

Goldberg, Y. and Nivre, J. (2013). Training de-
terministic parsers with non-deterministic ora-
cles. Transactions of the Association for Com-
putational Linguistics, 1.

Hockenmaier, J. and Steedman, M. (2007). CCG-
Bank: A corpus of CCG derivations and depen-
dency structures extracted from the Penn Tree-
bank. Computational Linguistics.

Kim, J. and Mooney, R. J. (2012). Unsupervised
PCFG induction for grounded language learning

1784



with highly ambiguous supervision. In Proceed-
ings of the Conference on Empirical Methods in
Natural Language Processing.

Kiperwasser, E. and Goldberg, Y. (2016). Simple
and accurate dependency parsing using bidirec-
tional LSTM feature representations. Transac-
tions of the Association for Computational Lin-
guistics, 4.

Kushman, N. and Barzilay, R. (2013). Using se-
mantic unification to generate regular expressions
from natural language. In Proceedings of the
Human Language Technology Conference of the
North American Association for Computational
Linguistics.

Kwiatkowski, T., Zettlemoyer, L. S., Goldwater, S.,
and Steedman, M. (2010). Inducing probabilistic
CCG grammars from logical form with higher-
order unification. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language
Processing.

Kwiatkowski, T., Zettlemoyer, L. S., Goldwater, S.,
and Steedman, M. (2011). Lexical generalization
in CCG grammar induction for semantic parsing.
In Proceedings of the Conference on Empirical
Methods in Natural Language Processing.

Lewis, M. and Steedman, M. (2014). A* CCG pars-
ing with a supertag-factored model. In Proceed-
ings of the Conference on Empirical Methods in
Natural Language Processing.

Liang, P., Jordan, M., and Klein, D. (2009). Learn-
ing semantic correspondences with less supervi-
sion. In Proceedings of the Joint Conference of
the Association for Computational Linguistics the
International Joint Conference on Natural Lan-
guage Processing.

Liu, F., Flanigan, J., Thomson, S., Sadeh, N., and
Smith, N. A. (2015). Toward abstractive summa-
rization using semantic representations. In Pro-
ceedings of the North American Association for
Computational Linguistics.

Manning, C. D., Surdeanu, M., Bauer, J., Finkel, J.,
Bethard, S. J., and McClosky, D. (2014). The
Stanford CoreNLP natural language processing
toolkit. In Proceedings of the Annual Meeting of
the Association for Computational Linguistics.

Matuszek, C., FitzGerald, N., Zettlemoyer, L. S.,
Bo, L., and Fox, D. (2012). A joint model of lan-
guage and perception for grounded attribute learn-
ing. In Proceedings of the International Confer-
ence on Machine Learning.

Pan, X., Cassidy, T., Hermjakob, U., Ji, H., and
Knight, K. (2015). Unsupervised entity linking
with Abstract Meaning Representation. In Pro-
ceedings of the North American Association for
Computational Linguistics.

Pust, M., Hermjakob, U., Knight, K., Marcu, D.,
and May, J. (2015). Parsing english into abstract
meaning representation using syntax-based ma-
chine translation. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language
Processing.

Ratinov, L. and Roth, D. (2009). Design challenges
and misconceptions in named entity recognition.
In Proceedings of the Conference on Computa-
tional Natural Language Learning.

Sawai, Y., Shindo, H., and Matsumoto, Y. (2015).
Semantic structure analysis of noun phrases using
abstract meaning representation. In Proceedings
of the annual meeting on Association for Compu-
tational Linguistics.

Steedman, M. (1996). Surface Structure and Inter-
pretation. The MIT Press.

Steedman, M. (2000). The Syntactic Process. The
MIT Press.

Wang, C., Xue, N., and Pradhan, S. (2015a). Boost-
ing transition-based amr parsing with refined ac-
tions and auxiliary analyzers. In Proceedings of
the Annual Meeting of the Association for Com-
putational Linguistics.

Wang, C., Xue, N., Pradhan, S., and Pradhan, S.
(2015b). A transition-based algorithm for AMR
parsing. In Proceedings of the North American
Association for Computational Linguistics.

Weiss, D., Alberti, C., Collins, M., and Petrov, S.
(2015). Structured training for neural network
transition-based parsing. In Proceedings of the
annual meeting on Association for Computational
Linguistics.

Zettlemoyer, L. S. and Collins, M. (2005). Learning
to map sentences to logical form: Structured clas-

1785



sification with probabilistic categorial grammars.
In Proceedings of the Conference on Uncertainty
in Artificial Intelligence.

Zettlemoyer, L. S. and Collins, M. (2007). Online
learning of relaxed CCG grammars for parsing to
logical form. In Proceedings of the Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning.

1786


