



















































Transition-Based Deep Input Linearization


Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 643–654,
Valencia, Spain, April 3-7, 2017. c©2017 Association for Computational Linguistics

Transition-Based Deep Input Linearization

Ratish Puduppully †∗, Yue Zhang ‡ and Manish Shrivastava †
†Kohli Center on Intelligent Systems (KCIS),

International Institute of Information Technology, Hyderabad (IIIT Hyderabad)
‡Singapore University of Technology and Design

ratish.surendran@research.iiit.ac.in, yue zhang@sutd.edu.sg,
m.shrivastava@iiit.ac.in

Abstract

Traditional methods for deep NLG adopt
pipeline approaches comprising stages
such as constructing syntactic input, pre-
dicting function words, linearizing the
syntactic input and generating the sur-
face forms. Though easier to visual-
ize, pipeline approaches suffer from er-
ror propagation. In addition, informa-
tion available across modules cannot be
leveraged by all modules. We construct
a transition-based model to jointly per-
form linearization, function word predic-
tion and morphological generation, which
considerably improves upon the accuracy
compared to a pipelined baseline system.
On a standard deep input linearization
shared task, our system achieves the best
results reported so far.

1 Introduction

Natural language generation (NLG) (Reiter and
Dale, 1997; White, 2004) aims to synthesize nat-
ural language text given input syntactic, seman-
tic or logical representations. It has been shown
useful in various tasks in NLP, including machine
translation (Chang and Toutanova, 2007; Zhang
et al., 2014), abstractive summarization (Barzilay
and McKeown, 2005) and grammatical error cor-
rection (Lee and Seneff, 2006).

A line of traditional methods treat the problem
as a pipeline of several independent steps (Bohnet
et al., 2010; Wan et al., 2009; Bangalore et al.,
2000; H. Oh and I. Rudnicky, 2000; Langkilde
and Knight, 1998). For example, shown in Fig-
ure 1b, a pipeline based on the meaning text the-
ory (MTT) (Melčuk, 1988) splits NLG into three

∗Part of the work was done when the author was a vis-
iting student at Singapore University of Technology and De-
sign.

Deep
graph

Shallow
graph String Morph

(a)

Sem
graph

Synt
tree

String Morph

(b)

Deep
graph Morphological String with Syntactic tree

(c)

Figure 1: Linearization pipelines (a) NLG pipeline
with deep input graph, (b) pipeline based on the
meaning text theory, (c) this paper.

be

meanwhile

.

price

think

have increase

ADV PSBJ

VC

C-A1

VC

A1

A1
AM-TMP

Figure 2: Sample deep graph for the sentence:
meanwhile, prices are thought to have increased.
Note that words are replaced by their lemmas. The
function word to and comma are absent in graph.

independent steps 1. syntactic generation: gen-
erating an unordered and lemma-formed syntactic
tree from a semantic graph, introducing function
words; 2. syntactic linearization: linearizing the
unordered syntactic tree; 3. morphological gener-
ation: generating the inflection for each lemma in
the string.

In this paper we focus on deep graph as input.
Exemplified in Figure 2, the deep input type is
intended to be an abstract representation of the
meaning of a sentence. Unlike semantic input,
where the nodes are semantic representations of
input, deep input is more surface centric, with lem-

643



mas for each word being connected by semantic
labels (Banarescu et al., 2013; Melčuk, 2015). In
contrast to shallow syntactic trees, function words
in surface forms are not included in deep graphs
(Belz et al., 2011). Deep inputs can more com-
monly occur as input of NLG systems where enti-
ties and content words are available, and one has to
generate a grammatical sentence using them with
only provision for inflections of words and intro-
duction of function words. Such usecases include
summarization, dialog generation etc.

A pipeline of deep input linearization is shown
in Figure 1a. Generation involves predicting the
correct word order, deciding inflections and also
filling in function words at the appropriate posi-
tions. The worst-case complexity is n! for per-
muting n words, 2n for function word prediction
(assuming that a function word can be inserted af-
ter each content word), and 2n for inflection gen-
eration (assuming two morphological forms for
each lemma). On the dataset from the First Sur-
face Realisation Shared Task, Bohnet et al. (2011)
achieved the best reported results on linearizing
deep input representation, following the pipeline
of Figure 1b (with input as deep graph instead of
semantic graph). They construct a syntactic tree
from deep input graph followed by function word
prediction, linearization and morphological gener-
ation. A rich set of features are used at each stage
of the pipeline and for each adjacent pair of stages,
an SVM decoder is defined.

Pipelined systems suffer from the problem of er-
ror propagation. In addition, because the steps are
independent of each other, information available
in a later stage is not made use of in the earlier
stages. We introduce a transition-based (Nivre,
2008) method for joint deep input surface realisa-
tion integrating linearization, function word pre-
diction and morphological generation. The model
is shown in Fig 1c, as compared with the pipelined
baseline in Fig 1a. For a directly comparable base-
line, we construct a pipeline system of function
words prediction, linearization and morphologi-
cal generation similar to the pipeline of Bohnet et
al. (2011), but with the following difference. Our
baseline pipeline system makes function word pre-
diction for a deep input graph, whereas Bohnet et
al. (2011) have a preprocessing step to construct
a syntactic tree from the deep input graph, which
is given as input to the function word prediction
module. Our pipeline is directly comparable to the

joint system with regard to the use of information.
Standard evaluations show that: 1. Our joint

model for deep input surface realisation achieves
significantly better scores over its pipeline coun-
terpart. 2. We achieve the best results reported on
the task. Our system scores 1 BLEU point better
over Bohnet et al. (2011) without using any ex-
ternal resources. We make the source code avail-
able at https://github.com/SUTDNLP/
ZGen/releases/tag/v0.3.

2 Related Work

Related work can be broadly summarized into
three areas: abstract word ordering, applications
of meaning-text theory and joint modelling of
NLP tasks. In abstract word ordering (Wan et
al., 2009; Zhang, 2013; Zhang and Clark, 2015),
De Gispert et al. (2014) compose phrases over in-
dividual words and permute the phrases to achieve
linearization. Schmaltz et al. (2016) show that
strong surface-level language models are more ef-
fective than models trained with syntactic infor-
mation for the task of linearization. Transition-
based techniques have also been explored (Liu et
al., 2015; Liu and Zhang, 2015; Puduppully et
al., 2016). To our knowledge, we are the first to
use transition-based techniques for deep input lin-
earization.

There has been work done in the area of
sentence linearization using meaning-text theory
(Melčuk, 1988). Belz et al. (2011) organized a
shared task on both shallow and deep lineariza-
tion according to meaning-text theory, which pro-
vides a standard benchmark for system compari-
son. Song et al. (2014) achieved the best results
for the task of shallow-syntactic linearization. Us-
ing SVM models with rich features, Bohnet et al.
(2011) achieved state-of-art results on the task of
deep realization. While they built a pipeline sys-
tem, we show that joint models can be used to
overcome limitations of the pipeline approach giv-
ing the best results.

Joint models for NLP have shown effective-
ness in recent years. Though having to tackle in-
creased search space, they overcome issues with
error propagation in pipelined models. Joint mod-
els have been explored for grammar-based ap-
proaches to surface realisation using HPSG and
CCG (Carroll and Oepen, 2005; Velldal and
Oepen, 2006; Espinosa et al., 2008; White and Ra-
jkumar, 2009; White, 2006; Carroll et al., 1999).

644



be

meanwhile

,

price

think .

to have increase

ADV PSBJ

VC

P

INF

C-A1

VC

A1

A1
AM-TMP

Figure 3: Equivalent shallow graph for Figure 2.

Joint models have been proposed for word seg-
mentation and POS-tagging (Zhang and Clark,
2010), POS-tagging and syntactic chunking (Sut-
ton et al., 2007), segmentation and normaliza-
tion (Qian et al., 2015), syntactic linearization and
morphologization (Song et al., 2014), parsing and
NER (Finkel and Manning, 2009), entity and rela-
tion extraction (Li and Ji, 2014) and so on. We
propose a first joint model for deep realization,
integrating linearization, function word prediction
and morphological generation.

3 Baseline

We build a baseline following the pipeline in Fig-
ure 1a. Three stages are involved: 1. prediction
of function words, inserting the predicted function
words in the deep graph, resulting in a shallow
graph; 2. linearizing the shallow graph; 3. gen-
erating the inflection for each lemma in the string.

3.1 Function Word Prediction

In the First Surface Realisation Shared Task
dataset (Belz et al., 2011), there are three classes
of function words to predict: to infinitive, that
complementizer and comma. We implement clas-
sifiers to predict these classes of function words
locally at respective positions in the deep graph
resulting in a shallow graph (Figure 3). At each
location the input is a node and output is a class
indicating if to or that need to inserted under the
node or the count of comma to be introduced under
the node.

Table 1 shows the feature templates for classi-
fication of to infinitives and that complementizers
and Table 2 shows the feature templates for pre-
dicting the count of comma child nodes for each
non-leaf node in the graph. These feature tem-
plates are a subset of features used in the joint
model (Section 4) with the exceptions being word
order features, which are not available here for the
pipeline system, since earlier stages cannot lever-
age features in subsequent outcomes. We use av-

Features for predicting function words including
to infinitive, that complementizer
WORD(n); POS(n); WORD(c)

Table 1: Feature templates for the prediction of
function words- to infinitive and that complemen-
tizer. Indices on the surface string: n – word index;
c – child of n; Functions: WORD – word at index;
POS – part-of-speech at index.

Features for predicting count of comma
WORD(n); POS(n)
BAG(WORD-MOD(n))
BAG(LABEL-MOD(n))

Table 2: Feature templates for the comma predic-
tion system. Indices on the surface string: n –
word index; Functions: WORD – word at index;
POS – part-of-speech at index; WORD-MOD –
modifiers of index; LABEL-MOD – dependency
labels of modifiers; BAG – set.

eraged perceptron classifier (Collins, 2002) to pre-
dict function words, which is consistent with the
joint model.

3.2 Linearization

The next step is linearizing the graph, which we
solve using a novel transition-based algorithm.

3.2.1 Transition-Based Tree Linearization

Liu et al. (2015) introduce a transition-based
model for tree linearization. The approach extends
from transition-based parsers (Nivre and Scholz,
2004; Chen and Manning, 2014), where state con-
sists of stack to hold partially built outputs and a
queue to hold input sequence of words. In case of
linearization, the input is a set of words. Liu et
al. therefore use a set to hold the input instead of
a queue. State is represented by a tuple (σ, ρ, A),
where σ is stack to store partial derivations, ρ is
set of input words and A is the set of dependency
relations that have been built. There are three tran-
sition actions:
• SHIFT-Word-POS – shifts Word from ρ, as-

signs POS to it and pushes it to top of stack
as S0;
• LEFTARC-LABEL – constructs dependency

arc S1
LABEL←−−−−− S0 and pops out second ele-

ment from top of stack S1;
• RIGHTARC-LABEL – constructs depen-

dency arc S1
LABEL−−−−−→ S0 and pops out top

of stack S0.

645



Input lemmas: {think1, price2, .3, increase4, be5, have6, meanwhile7,
,8, to9}

Transition σ ρ A
0 [] {1...7} ∅
1 SH-meanwhile [7] {1...6,8,9}
2 SH-, [7 8] {1...6,9}
3 SH-price [7 8 2] {1,3,4,5,6,9}
4 SH-be [7 8 2 5] {1,3,4,6,9}
5 SH-think [7 8 2 5 1] {3,4,6,9}
6 SH-to [7 8 2 5 1 9] {3,4,6}
7 SH-have [7 8 2 5 1 9 6] {3,4}
8 SH-increase [7 8 2 5 1 9 6 4] {3}
9 RA [7 8 2 5 1 9 6] {3} A ∪ {6→ 4}
10 RA [7 8 2 5 1 9] {3} A ∪ {9→ 6}
11 RA [7 8 2 5 1] {3} A ∪ {1→ 9}
12 RA [7 8 2 5] {3} A ∪ {5→ 1}
13 SH-. [7 8 2 5 3] {}
14 RA [7 8 2 5] {} A ∪ {5→ 3}
15 LA [7 8 5] {} A ∪ {2← 5}
16 LA [7 5] {} A ∪ {8← 5}
17 LA [5] {} A ∪ {7← 5}

Table 3: Transition action sequence for lineariz-
ing the graph in Figure 3. SH - SHIFT, RA -
RIGHTARC, LA - LEFTARC. POS is not shown
in SHIFT actions.

The sequence of actions to linearize the set {he,
goes, home} is SHIFT-he, SHIFT-goes, SHIFT-
home, RIGHTARC-OBJ, LEFTARC-SBJ.

The full set of feature templates are shown in
Table 2 of Liu et al. (2015), partly shown in Table
4. The features include word(w), POS(p) and de-
pendency label (l) of elements on stack and their
descendants S0, S1, S0,l, S0,r etc. For example,
word on top of stack is S0w and word on first left
child of S0 is S0,lw. These are called configuration
features. They are combined with all possible ac-
tions to score the action. Puduppully et al. (2016)
extend Liu et al. (2015) by redefining features to
address feature sparsity and introduce lookahead
features, thereby achieving highest accuracies on
task of abstract word ordering.

3.2.2 Shallow Graph Linearization
Our transition based graph linearization system
extends from Puduppully et al. (2016). In our case,
the input is a shallow graph instead of a syntac-
tic tree, and hence the search space is larger. On
the other hand, the same set of actions can still
be applied, with additional constraints on valid ac-
tions given each configuration (Section 3.2.3). Ta-
ble 3 shows the sequence of transition actions to
linearize shallow graph in Figure 3.

3.2.3 Obtaining Possible Transition Actions
Given a Configuration

The purpose of a GETPOSSIBLEACTIONS func-
tion is to find out the set of transition actions that
can lead to a valid output given a certain state.

Algorithm 1: GETPOSSIBLEACTIONS for
shallow graph linearization

Input: A state s = ([σ|j i], ρ, A) and input graph C
Output: A set of possible transition actions T

1 T ← ∅
2 if s.σ == ∅ then
3 for k ∈ s.ρ do
4 T ← T ∪ (SHIFT, POS, k)
5 else
6 if ∃k, k ∈ (DIRECTCHILDREN(i) ∩ s.ρ) then
7 SHIFTSUBTREE(i, ρ)

8 else
9 if A.LEFTCHILD(i) is NIL then

10 SHIFTSUBTREE(i, ρ)

11 if {j → i} ∈ C∧ A.LEFTCHILD(j) is NIL
then

12 T ← T ∪ (RIGHTARC)
13 if i ∈ DESCENDANT(j) then
14 PROCESSDESCENDANT(i, j)

15 if i ∈ SIBLING(j) then
16 PROCESSSIBLING(i, j)

17 else if {j ← i} ∈ C then
18 T ← T ∪ (LEFTARC)
19 if i ∈ SIBLING(j) then
20 PROCESSSIBLING(i, j)

21 else
22 if size(s.σ) == 1 then
23 SHIFTPARENTANDSIBLINGS(i)

24 else
25 if i ∈ DESCENDANT(j) then
26 PROCESSDESCENDANT(i, j)

27 if i ∈ SIBLING(j) then
28 PROCESSSIBLING(i, j)

29 return T

Algorithm 2: DIRECTCHILDREN
Input: A state s=([σ|j i], ρ, A), input node and graph

C.
Output: DC direct child nodes of input node

1 DC← ∅
2 for k ∈ (C.CHILDREN(input node)) do
3 Parents← C.PARENTS(k)
4 if Parents.size == 1 then
5 DC← DC ∪ k
6 else
7 for m ∈ Parents do
8 if A.LEFTCHILD(m) is not NIL ∨ m ==

input node then
9 continue

10 if m ∩ s.ρ then
11 goto OutsideLoop

12 if m ∈ σ∧ σ.ISANCESTOR(m,C) then
13 goto OutsideLoop

14 DC← DC ∪ k
15 OutsideLoop:

16 return DC

646



be

meanwhile , price think .

to have increase

ADV

P
SBJ VC

P

INF

C-A1 VC

Figure 4: Equivalent syntactic tree for Figure 2.

Algorithm 3: SHIFTSUBTREE
Input: A state s = ([σ|j i], ρ, A), graph C, head k
Output: a set of possible Transition actions T

1 T← ∅
2 T← T ∪ (SHIFT, POS, k)
3 queue q
4 q.push(k)
5 while q is not empty do
6 front = q.pop()
7 for m ∈ (C.CHILDREN(front) ∩ s.ρ) do
8 q.push(m)
9 T← T ∪ (SHIFT, POS, m)

This is because not all sequences of actions corre-
spond to a well-formed output. Essentially, given
a state s = ([σ|j i], ρ, A) and an input graph C,
the Decoder extracts syntactic tree from the graph
(cf. Figure 4 extracted from Figure 3), outputting
RIGHTARC, LEFTARC only if the corresponding
arc exists in C. The corresponding pseudocode is
shown in Algorithm 1.

In particular, if node i has direct child nodes in
C, the descendants of i are shifted (line 6-7) (see
Algorithm 3). Here direct child nodes (see Algo-
rithm 2) include those child nodes of i for which
i is the only parent or if there is more than one
parent then every other parent is shifted on to the
stack without possibility to reduce the child node.
If no direct child node is in the buffer, then all
graph descendants of i are shifted. Now, there are
three configurations possible between i and j: 1.
i and j are directly connected in C. This results in
RIGHTARC or LEFTARC action; 2. i is descendant
of j. In this case the parents of i (such that they are
descendants of j) and siblings of i through such
parents are shifted. 3. i is sibling of j. In this case,
parents of i and their descendants are shifted such
that A remains consistent. Because the input is a
graph, more than one of the above configuration
can occur simultaneously. More detailed discus-
sion related to GETPOSSIBLEACTIONS is given in
Appendix A.

Unigrams
S0w; S0p; S0,lw; S0,lp; S0,ll; S0,rw; S0,rp; S0,rl;
Bigram
S0wS0,lw; S0wS0,lp; S0wS0,ll; S0pS0,lw;
Linearization
w0; p0; w−1w0; p−1p0; w−2w−1w0; p−2p−1p0

Table 4: Baseline linearization feature templates.
A subset is shown here. For the full feature set,
refer to Table 2 of Liu et al. (2015).

3.2.4 Feature Templates
There are three sets of features. The first is the
set of baseline linearization feature templates from
Table 2 in Liu et al. (2015), partly shown in Table
4. The second is a set of lookahead features simi-
lar to that of Puduppully et al. (2016), shown in Ta-
ble 5.1 Parent lookahead feature in Puduppully et
al. (2016) is defined for the only parent. For graph
linearization, however, the parent lookahead fea-
ture need to be defined for set of parents. The third
set of features in Table 6 are newly introduced for
Graph Linearization. Arcleft is a binary feature
indicating if there is left arc between S0 and S1,
whereas Arcright indicates if there is a right arc.
Lis descendant is a binary feature indicating if L is
descendant of S0, and Lis parent or sibling indicates
if it is a parent or sibling of S0. S0descendants shifted
is binary feature indicating if all the descendants
of S0 are shifted.

Not having POS in the input dataset, we com-
pute the feature templates for POS making use of
the most frequent POS of the lemma in the gold
training data. For the features with dependency la-
bels, we use the input graph labels.

3.2.5 Search and Learning
We follow Puduppully et al. (2016) and Liu et al.
(2015), applying the learning and search frame-
work of Zhang and Clark (2011). Pseudocode is
shown in Algorithm 4. It performs beam search
holding k best states in an agenda at each incre-
mental step. At the start of decoding, agenda holds
the initial state. At a step, for each state in the

1Here Lcls represents set of arc labels of child nodes (of
word to shift L) shifted on the stack, Lclns represents set of
arc labels of child nodes not shifted on the stack, Lcps the
POS set of shifted child nodes, Lcpns the POS set of un-
shifted child nodes, Lsls the set of arc labels of shifted sib-
lings, Lslns the set of arc labels of unshifted siblings, Lsps
the POS set of shifted siblings, Lcpns the POS set of un-
shifted siblings, Lpls the set of arc labels of shifted parents,
Lplns the set of arc labels of unshifted parents, Lpps the POS
set of shifted parents, Lppns the POS set of unshifted parents.

647



set of label and POS of child nodes of L
Lcls;Lclns;Lcps;Lcpns;
S0wLcls;S0pLcls;S1wLcls;S1pLcls;

set of label and POS of first-level siblings of L
Lsls;Lslns;Lsps;Lspns;
S0wLsls;S0pLsls;S1wLsls;S1pLsls;

set of label and POS of parents of L
Lpls;Lplns;Lpps;Lppns;
S0wLpls;S0pLpls;S1wLpls;S1pLpls;

Table 5: Lookahead linearization feature tem-
plates for the word L to shift. A subset is shown
here. For the full feature set, refer to Table 2 of
Puduppully et al. (2016). An identical set of fea-
ture templates are defined for S0.

arc features between S0 and S1
Arcleft ; Arcright ;
lookahead features for L
Lis descendant ; Lis parent or sibling ;
are all descendants of S0 shifted
S0descendants shifted ;
feature combination
S0descendants shiftedArcleft ;
S0descendants shiftedArcright ;
S0descendants shifted Lis descendant ;
S0descendants shifted Lis parent or sibling ;

Table 6: Graph linearization feature templates

agenda, each of transition actions in GETPOSSI-
BLEACTIONS is applied. The top-k states are up-
dated in the agenda for the next step. The pro-
cess repeats for 2n steps as each word needs to be
shifted once on to the stack and reduced once. Af-
ter 2n steps, the highest scoring state in agenda is
taken as the output. The complexity of algorithm
is n2, as it takes 2n steps to complete and during
each step, the number of transition actions is pro-
portional to ρ. Given a configuration C, the score
of a possible action a is calculated as:

Score(a) = ~θ · ~Φ(C, a),
where ~θ is the model parameter vector and ~Φ(C, a)
denotes a feature vector consisting of configura-
tion and action components. Given a set of labeled
training examples, the averaged perceptron with
early update (Collins and Roark, 2004) is used.

3.3 Morphological Generation
The last step is to inflate the lemmas in the sen-
tence. There are three POS categories, includ-
ing nouns, verbs and articles, for which we need
to generate morphological forms. We use Wik-
tionary2 as a basis and write a small set of rules

2https://en.wiktionary.org/

Algorithm 4: transition-based linearization
Input: C, a set of input syntactic constraints
Output: The highest-scored final state

1 candidates← ([ ], set(1..n), ∅)
2 agenda← ∅
3 for i← 1..2n do
4 for s in candidates do
5 for action in GETPOSSIBLEACTIONS(s, C)

do
6 agenda← APPLY(s, action)
7 candidates← TOP-K(agenda)
8 agenda← ∅
9 best← BEST(candidates)

10 return best

Rules for be
attr[‘partic’] == ‘pres’→ being
attr[‘partic’] == ‘past’→ been
attr[‘tense’] == ‘past’
sbj.attr[‘num’] == ‘sg’→ was
sbj.attr[‘num’] == ‘pl’→ were
other→ [was,were]

attr[‘tense’] == ‘pres’
sbj.attr[‘num’] == ‘sg’→ is
sbj.attr[‘num’] == ‘pl’→ are
other→ [am,is,are]

Rules for other verbs
attr[‘partic’] == ‘pres’→ wik.get(lemma, VBG)
attr[‘partic’] == ‘past’→ wik.get(lemma, VBN )
attr[‘tense’] == ‘past’→ wik.get(lemma, VBD)
attr[‘tense’] == ‘pres’
sbj.attr[‘num’] == ‘sg’→ wik.get(lemma, VBZ )
other→ wik.getall(lemma)

Rules for other types
lemma==a→ [a,an]
lemma==not→ [not,n’t]
attr[‘num’] == ‘sg’→ wik.get(lemma,NNP/NN)
attr[‘num’] == ‘pl’→ wik.get(lemma,NNPS/NNS)

Table 7: Lemma rules. All rules are in the format:
conditions→ candidate inflections. Nested condi-
tions are listed in multi-lines with indentation. wik
denotes english wiktionary.

similar to that used in Song et al. (2014), listed in
Table 7, to generate a candidate set of inflections.
An averaged perceptron classifier (Collins, 2002)
is trained for each lemma. For distinguishing be-
tween singular and plural candidate verb forms,
the feature templates in Table 8 are used.

4 Joint Method

We design a joint method for function word pre-
diction (Section 3.1), linearization (Section 3.2)
and morphological generation (Section 3.3) by
further extending the transition-based system of
Section 3.2, integrating actions for function word
prediction and morphological generation.

648



Features for predicting singular/ plural verb forms
WORD(n-1)WORD(n-2)WORD(n-3); COUNT SUBJ(n);
COUNT(n-1)COUNT(n-2)COUNT(n-3); SUBJ(n);
WORD(n-1)WORD(n-2); COUNT(n-1)COUNT(n-2);
WORD(n-1); COUNT(n-1); WORD(n+1); COUNT(n+1);

Table 8: Feature templates for predicting singular/
plural verb forms. Indices on the surface string:
n – word index; Functions: WORD – word at in-
dex n; COUNT – word at n is singular or plural
form; SUBJ – word at subject of n; COUNT SUBJ
– word at subject of n is singular or plural form.

think to have
C-A1

think have
C-A1 INF

Figure 5: Example for SPLITARC-to.

4.1 Transition Actions

In addition to SHIFT, LEFTARC and RIGHTARC
in Section 3.2.1, we use the following new transi-
tion actions for inserting function words:
• INSERT, inserts comma at the present posi-

tion;
• SPLITARC-Word, splits an arc in the input

graph C, inserting a function word between
the words connected by the arc. Here Word
specifies the function word being inserted
(Figure 5).

We generate a candidate set of inflections for
each lemma following the approach in Section 3.3.
For each candidate inflection, we generate a cor-
responding SHIFT transition action. The rules in
Table 7 are used to prune impossible inflections.3

Table 9 shows the transition actions to linearize
the graph in Figure 2. These newly introduced
transition actions result in variability in the num-
ber of transition actions. With function word
prediction, the number of transition actions for
a bag of n words is not necessarily 2n-1. For
example, considering an INSERT, SPLITARC-to
or SPLITARC-that action post each SHIFT action,
the maximum number of possible actions is 5n-1.
This variance in the number of actions can impact
the linear separability of state items. Following
Zhu et al. (2013), we use IDLE actions as a form of
padding method, which results in completed state
items being further expanded up to 5n-1 steps. The
joint model uses the same perceptron training al-

3For example in Figure 2, price is the subject of be and
if be is in present tense and price is in plural form, the in-
flections {am, is, was, were} are impossible and are is the
correct inflection for be. We therefore generate transition ac-
tion as SHIFT-are.

Input lemmas: {think1, price2, .3, increase4, be5, have6, meanwhile7}
Transition σ ρ A

0 [] {1...7} ∅
1 SH-meanwhile [7] {1...6}
2 IN [7] {1...6}
3 SH-prices [7 2] {1,3,4,5,6}
4 SH-are [7 2 5] {1,3,4,6}
5 SH-thought [7 2 5 1] {3,4,6}
6 SP-to [7 2 5 1] {3,4,6}
7 SH-have [7 2 5 1 6] {3,4}
8 SH-increased [7 2 5 1 6 4] {3}
9 RA [7 2 5 1 6] {3} A ∪ {6→ 4}
10 RA [7 2 5 1] {3} A ∪ {1→ 6}
11 RA [7 2 5] {3} A ∪ {5→ 1}
12 SH-. [7 2 5 3] {}
13 RA [7 2 5] {} A ∪ {5→ 3}
14 LA [7 5] {} A ∪ {2← 5}
15 LA [5] {} A ∪ {7← 5}

Table 9: Transition action sequence for lineariz-
ing the sentence in Figure 2. SH - SHIFT, SP -
SPLITARC, RA - RIGHTARC, LA - LEFTARC, IN
- INSERT. POS is not shown in SHIFT actions.

gorithm and similar features compared to the base-
line model.

4.2 Obtaining Possible Transition Actions
Given a Configuration

Given a state s = ([σ|j i], ρ, A) and an input
graph C, the possible transition actions include as
a subset the transition actions in Algorithm 1 for
shallow graph linearization. In addition, for each
lemma being shifted, we enumerate its inflections
and create SHIFT transition actions for each inflec-
tion. Further, we predict SPLITARC, INSERT and
IDLE actions to handle function words. If node
i has a child node in C, which is not shifted, we
predict SPLITARC and INSERT. If i is sibling to
j, we predict INSERT. If both the stack and buffer
are empty, we predict IDLE. Pseudocode for GET-
POSSIBLEACTIONS for the joint method is shown
in Algorithm 5.

5 Experiments

5.1 Dataset
We work on the deep dataset from the Sur-
face Realisation Shared Task (Belz et al., 2011)4.
Sentences are represented as sets of unordered
nodes with labeled semantic edges between them.
Semantic representation is obtained by merging
Nombank (Meyers et al., 2004), Propbank (Palmer
et al., 2005) and syntactic dependencies. Edge la-
beling follows PropBank annotation scheme such
as {A0, A1, ... An}. The nodes are annotated
with lemma and where appropriate number, tense
and participle features. Function words including

4http://www.nltg.brighton.ac.uk/research/sr-task/

649



Algorithm 5: GETPOSSIBLEACTIONS for
deep graph linearization, where C is a input
graph

Input: A state s = ([σ|j i], ρ, A) and graph C
Output: A set of possible transition actions T

1 T ← ∅
2 if s.σ == ∅ then
3 for k ∈ s.ρ do
4 T ← T ∪ (SHIFT, POS, k)
5 else
6 if ∃k, k ∈ (DIRECTCHILDREN(i) ∩ s.ρ) then
7 SHIFTSUBTREE(i, ρ)

8 else
9 if A.LEFTCHILD(i) is NIL then

10 SHIFTSUBTREE(i, ρ)

11 if {j → i} ∈ C∧ A.LEFTCHILD(j) is NIL
then

12 T ← T ∪ (RIGHTARC)
13 if i ∈ DESCENDANT(j) then
14 PROCESSDESCENDANT(i, j)

15 if i ∈ SIBLING(j) then
16 PROCESSSIBLING(i, j)

17 else if {j ← i} ∈ C then
18 T ← T ∪ (LEFTARC)
19 if i ∈ SIBLING(j) then
20 PROCESSSIBLING(i, j)

21 else
22 if size(s.σ) == 1 then
23 SHIFTPARENTANDSIBLINGS(i)

24 else
25 if i ∈ DESCENDANT(j) then
26 PROCESSDESCENDANT(i, j)

27 if i ∈ SIBLING(j) then
28 PROCESSSIBLING(i, j)

29 if C.Children(i) ∧s.ρ 6= ∅ then
30 T ← T ∪ (SPLITARC − to)
31 T ← T ∪ (SPLITARC − that)
32 if C.Children(i) ∧s.ρ 6= ∅ ∨ i ∈ SIBLING(j) then
33 T ← T ∪ (INSERT)
34 if s.σ == ∅ ∧ s.ρ == ∅ then
35 T ← T ∪ (IDLE)
36 return T

that complementizer, to infinitive and commas are
omitted from the input. There are two punctua-
tion features for information about brackets and
quotes. Table 10 shows a sample training instance.

Out of 39k total training instances, 2.8k are
non-projective, which we discard. We exclude in-
stances which result in non-projective dependen-
cies mainly because our transition actions predict
only projective dependencies. It has been derived
from the arc-standard system (Nivre, 2008). There
are 1.8k training instances with a mismatch be-

Input (unordered lemma-formed graph):
Sem ID PID Lemma Attr Lexeme
SROOT 1 0 be tense=pres are
ADV 2 1 meanwhile meanwhile
P 3 1 . .
SBJ 4 1 start.02 num=pl starts
A1 5 4 housing num=sg housing
AM-TMP 6 4 september num=sg september
VC 9 1 think.01 partic=past thought
A1 4 9
C-A1 10 9 have have
VC 11 10 inch.01 partic=past inched
A1 4 11
A5 12 11 upward upward

Table 10: Deep type training instance from Sur-
face Realisation Shared Task 2011. Sem – seman-
tic label, ID – unique ID of node within graph, PID
– the ID of the parent, Attr – Attributes such as
partic (participle), tense or number, Lexeme – lex-
eme which is resolved using wiktionary and rules
in Table 7.

tween edges in the input deep graph and gold out-
put tree. The gold output tree is the correspond-
ing shallow tree from the shared task. We ap-
proach the task of linearization as extracting a lin-
earized tree from the input semantic graph. So we
exclude those instances which do not have edges
corresponding to gold tree i.e mismatch between
edges of gold tree and input graph. After exclud-
ing these instances, we have 34.3k training in-
stances. We also exclude 800 training instances
where the function words to and that have more
than one child, and around 100 training instances
where function words’ parent and child nodes are
not connected by an arc in the deep graph. The
above cases are deemed annotation mistakes. We
thus train on a final subset of 33.4k training in-
stances. The development set comprises 1034 in-
stances and the test set comprises 2398 instances.
Evaluation is done using the BLEU metric (Pap-
ineni et al., 2002).

6 Development Results

6.1 Influence of Beam Size

We study the effect of beam size on the accura-
cies of joint model in Figure 6, by varying the
beam size and comparing the accuracies on de-
velopment dataset over training iterations. Beam
sizes of 64 and 128 perform the best. However,
beam size 128 does not improve the performance
significantly, yet is twice as slow compared to a
beam size 64. So we retain a 64 beam for further
experiments.

650



0 5 10 15 20 25 30

30

40

50

60

70

80

90

Iteration

B
L

E
U

Beam size

X1 X4 X16 X64 X128

Figure 6: Influence of beam sizes.

Pipeline Joint
to infinitive 92.7 94.1
that complementizer 70.6 76.5
count of comma 60.2 63.3

Table 11: Average F-measure for function word
prediction for development set.

6.2 Pipeline vs Joint Model
We compare the results of the joint model with the
pipeline baseline system. Table 11 shows the de-
velopment results of function word prediction, and
Table 12 shows the overall development results.
Our joint model of Transition-Based Deep Input
Linearization (TBDIL) achieves an improvement
of 5 BLEU points over the pipeline using the same
feature source and training algorithm. Thanks to
the sharing of word order information, the joint
model improves function word prediction com-
pared to the pipeline, which forbids such feature
integration because function word prediction is the
first step, taken before order becomes available.

7 Final Results

Table 13 shows the final results. The best perform-
ing system for the Shared Task was STUMABA-D
by Bohnet et al. (2011), which leverages a large-
scale n-gram language model. The joint model
TBDIL significantly outperforms the pipeline sys-
tem and achieves an improvement of 1 BLEU
point over STUMABA-D, obtaining 80.49 BLEU
without making use of external resources.

8 Analysis

Table 14 shows sample outputs from the Pipeline
system and the corresponding output from TBDIL.
In the first instance, the function word to is incor-
rectly predicted in the arc between nodes does and
yield in the pipeline system. In case of TBDIL,
the n-gram feature helps avoid incorrect insertion
of to which demonstrates the advantage of inte-
grating information across stages. In the second

System BLEU Score
Pipeline 75.86
TBDIL 80.77

Table 12: Development results.

System BLEU Score
STUMABA-D 79.43

Pipeline 70.99
TBDIL 80.49

Table 13: Test results.

output
ref. if it does n’t yield on these matters and even-

tually begin talking directly to the anc
Pipeline if it does not to yield on these matters and

eventually begin talking directly to the anc
TBDIL if it does n’t yield on these matters and even-

tually begin talking directly to the anc
ref. economists who read september ’s low level

of factory job growth as a sign of a slowdown
Pipeline september ’s low level of factory job growth

who as a sign of a slowdown reads economists
TBDIL economists who read september ’s low level

of factory job growth as a sign of a slowdown

Table 14: Example outputs.

instance, because of incorrect linearization, there
is error propagation to morphological generation
in the pipeline system. In particular, economists is
linearized to the object part of the sentence and the
subject is singular. This, in turn, results in the in-
correct prediction of morphological form of verb
read as its singular variant. In TBDIL, in contrast,
the joint modelling of linearization and morphol-
ogy helps ordering the sentence correctly.

9 Conclusion

We showed the usefulness of a joint model for the
task of Deep Linearization, by taking (Puduppully
et al., 2016) as the baseline and extending it to
perform joint graph linearization, function word
prediction and morphological generation. To our
knowledge, this is the first work to use Transition-
Based method for joint NLG from semantic struc-
ture. Our system gave the highest scores reported
for the NLG 2011 shared task on Deep Input Lin-
earization (Belz et al., 2011).

Acknowledgments

We thank Litton Kurisinkel for helpful discussions
and the anonymous reviewers for their detailed
and constructive comments. Yue Zhang is sup-
ported by the Singapore Ministry of Education
(MOE) AcRF Tier 2 grant T2MOE201301.

651



References
Laura Banarescu, Claire Bonial, Shu Cai, Madalina

Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin
Knight, Philipp Koehn, Martha Palmer, and Nathan
Schneider, 2013. Proceedings of the 7th Linguis-
tic Annotation Workshop and Interoperability with
Discourse, chapter Abstract Meaning Representa-
tion for Sembanking, pages 178–186. Association
for Computational Linguistics.

Srinivas Bangalore, Owen Rambow, and Steve Whit-
taker, 2000. INLG’2000 Proceedings of the First In-
ternational Conference on Natural Language Gen-
eration, chapter Evaluation Metrics for Generation.

Regina Barzilay and Kathleen R McKeown. 2005.
Sentence fusion for multidocument news summa-
rization. Computational Linguistics, 31(3):297–
328.

Anja Belz, Michael White, Dominic Espinosa, Eric
Kow, Deirdre Hogan, and Amanda Stent. 2011. The
first surface realisation shared task: Overview and
evaluation results. In Proceedings of the 13th Eu-
ropean workshop on natural language generation,
pages 217–226. Association for Computational Lin-
guistics.

Bernd Bohnet, Leo Wanner, Simon Mille, and Ali-
cia Burga. 2010. Broad coverage multilingual
deep sentence generation with a stochastic multi-
level realizer. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics,
pages 98–106. Association for Computational Lin-
guistics.

Bernd Bohnet, Simon Mille, Benoı̂t Favre, and Leo
Wanner. 2011. Stumaba: from deep representa-
tion to surface. In Proceedings of the 13th Eu-
ropean workshop on natural language generation,
pages 232–235. Association for Computational Lin-
guistics.

John Carroll and Stephan Oepen. 2005. High ef-
ficiency realization for a wide-coverage unification
grammar. In Second International Joint Conference
on Natural Language Processing: Full Papers.

John Carroll, Ann Copestake, and Dan Flickinger.
1999. An efficient chart generator for (semi-) lex-
icalist grammars.

Pi-Chuan Chang and Kristina Toutanova. 2007. A dis-
criminative syntactic word order model for machine
translation. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 9–16. Association for Computational Linguis-
tics.

Danqi Chen and Christopher D. Manning. 2014. A
fast and accurate dependency parser using neural
networks. Proceedings of the 2014 Conference on
EMNLP, 1:740–750.

Michael Collins and Brian Roark. 2004. Incremen-
tal parsing with the perceptron algorithm. In Pro-
ceedings of the 42nd Annual Meeting on Association
for Computational Linguistics, page 111. Associa-
tion for Computational Linguistics.

Michael Collins, 2002. Proceedings of the 2002 Con-
ference on EMNLP (EMNLP 2002), chapter Dis-
criminative Training Methods for Hidden Markov
Models: Theory and Experiments with Perceptron
Algorithms.

A. De Gispert, M. Tomalin, and W. Byrne. 2014. Word
ordering with phrase-based grammars. 14th Confer-
ence of the European Chapter of the Association for
Computational Linguistics 2014, EACL 2014, pages
259–268.

Dominic Espinosa, Michael White, and Dennis Mehay.
2008. Hypertagging: Supertagging for surface real-
ization with ccg. In Proceedings of ACL-08: HLT,
pages 183–191. Association for Computational Lin-
guistics.

Rose Jenny Finkel and Christopher D. Manning. 2009.
Joint parsing and named entity recognition. In Pro-
ceedings of Human Language Technologies: The
2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 326–334. Association for Computa-
tional Linguistics.

Alice H. Oh and Alexander I. Rudnicky, 2000. ANLP-
NAACL 2000 Workshop: Conversational Systems,
chapter Stochastic Language Generation for Spoken
Dialogue Systems.

Irene Langkilde and Kevin Knight. 1998. Generation
that exploits corpus-based statistical knowledge. In
COLING 1998 Volume 1: The 17th International
Conference on Computational Linguistics.

John Lee and Stephanie Seneff. 2006. Automatic
grammar correction for second-language learners.
In INTERSPEECH, pages 1978–1981.

Qi Li and Heng Ji. 2014. Incremental joint extrac-
tion of entity mentions and relations. In Proceed-
ings of the 52nd Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), pages 402–412. Association for Computa-
tional Linguistics.

Jiangming Liu and Yue Zhang. 2015. An empirical
comparison between n-gram and syntactic language
models for word ordering. In Proceedings of the
2015 Conference on EMNLP, pages 369–378, Lis-
bon, Portugal, September. Association for Compu-
tational Linguistics.

Yijia Liu, Yue Zhang, Wanxiang Che, and Bing Qin.
2015. Transition-based syntactic linearization. In
NAACL HLT 2015, The 2015 Conference of the
North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Denver, Colorado, USA, May 31 - June 5,
2015, pages 113–122.

652



Igor Aleksandrovič Melčuk. 1988. Dependency Syn-
tax: theory and practice. SUNY press.

Igor Aleksandrovič Melčuk. 2015. Semantics: From
meaning to text, volume 3. John Benjamins Pub-
lishing Company.

Adam Meyers, Ruth Reeves, Catherine Macleod,
Rachel Szekely, Veronika Zielinska, Brian Young,
and Ralph Grishman. 2004. Annotating noun argu-
ment structure for nombank. In Proceedings of the
Fourth International Conference on Language Re-
sources and Evaluation (LREC’04). European Lan-
guage Resources Association (ELRA).

Joakim Nivre and Mario Scholz. 2004. Determinis-
tic dependency parsing of english text. In Proceed-
ings of the 20th international conference on Compu-
tational Linguistics, page 64. Association for Com-
putational Linguistics.

Joakim Nivre. 2008. Algorithms for deterministic in-
cremental dependency parsing. Computational Lin-
guistics, Volume 34, Number 4, December 2008.

Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated corpus
of semantic roles. Computational Linguistics, Vol-
ume 31, Number 1, March 2005.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics.

Ratish Puduppully, Yue Zhang, and Manish Shrivas-
tava. 2016. Transition-based syntactic lineariza-
tion with lookahead features. In Proceedings of the
2016 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 488–493. Asso-
ciation for Computational Linguistics.

Tao Qian, Yue Zhang, Meishan Zhang, Yafeng Ren,
and Donghong Ji. 2015. A transition-based model
for joint segmentation, pos-tagging and normal-
ization. In Proceedings of the 2015 Conference
on EMNLP, pages 1837–1846, Lisbon, Portugal,
September. Association for Computational Linguis-
tics.

Ehud Reiter and Robert Dale. 1997. Building applied
natural language generation systems. Natural Lan-
guage Engineering, 3(01):57–87.

Allen Schmaltz, Alexander M. Rush, and Stuart M.
Shieber. 2016. Word ordering without syntax.
arXiv preprint arXiv:1604.08633.

Linfeng Song, Yue Zhang, Kai Song, and Qun Liu.
2014. Joint morphological generation and syntac-
tic linearization. Proceedings of the Twenty-Eighth
AAAI Conference on Artificial Intelligence, pages
1522–1528.

Charles Sutton, Andrew McCallum, and Khashayar
Rohanimanesh. 2007. Dynamic conditional random
fields: Factorized probabilistic models for labeling
and segmenting sequence data. Journal of Machine
Learning Research, 8(Mar):693–723.

Erik Velldal and Stephan Oepen, 2006. Proceedings
of the 2006 Conference on EMNLP, chapter Statisti-
cal Ranking in Tactical Generation, pages 517–525.
Association for Computational Linguistics.

Stephen Wan, Mark Dras, Robert Dale, and Cécile
Paris. 2009. Improving grammaticality in statisti-
cal sentence generation: Introducing a dependency
spanning tree algorithm with an argument satisfac-
tion model. In Proceedings of the 12th Conference
of the European Chapter of the ACL (EACL 2009),
pages 852–860. Association for Computational Lin-
guistics.

Michael White and Rajakrishnan Rajkumar. 2009.
Perceptron reranking for ccg realization. In Pro-
ceedings of the 2009 Conference on EMNLP, pages
410–419. Association for Computational Linguis-
tics.

Michael White. 2004. Reining in ccg chart realiza-
tion. In Natural Language Generation, pages 182–
191. Springer Berlin Heidelberg.

Michael White. 2006. Efficient realization of coordi-
nate structures in combinatory categorial grammar.
Research on Language and Computation, 4(1):39–
75.

Yue Zhang and Stephen Clark. 2010. A fast decoder
for joint word segmentation and pos-tagging using a
single discriminative model. In Proceedings of the
2010 Conference on EMNLP, pages 843–852. Asso-
ciation for Computational Linguistics.

Yue Zhang and Stephen Clark. 2011. Syntactic pro-
cessing using the generalized perceptron and beam
search. Computational linguistics, 37(1):105–151.

Yue Zhang and Stephen Clark. 2015. Discrimina-
tive syntax-based word ordering for text generation.
Computational Linguistics, 41(3):503–538.

Yue Zhang, Kai Song, Linfeng Song, Jingbo Zhu, and
Qun Liu. 2014. Syntactic smt using a discrimina-
tive text generation model. In Proceedings of the
2014 Conference on EMNLP, pages 177–182, Doha,
Qatar, October. Association for Computational Lin-
guistics.

Yue Zhang. 2013. Partial-tree linearization: gener-
alized word ordering for text synthesis. In Pro-
ceedings of the Twenty-Third international joint con-
ference on Artificial Intelligence, pages 2232–2238.
AAAI Press.

Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang,
and Jingbo Zhu. 2013. Fast and accurate shift-
reduce constituent parsing. In ACL (1), pages 434–
443.

653



A Obtaining possible transition actions
given a configuration for Shallow
Graph

During shallow linearization, a state is represented
by s = ([σ|j i], ρ, A) and C is the input graph.
Given C, the Decoder outputs actions which ex-
tract syntactic tree from the graph. Thus the De-
coder outputs RIGHTARC or LEFTARC only if
corresponding arc exists in C. The detailed pseu-
docode is given in Algorithm 1. If i has direct child
nodes in C, the descendants of i are shifted (line
6-7) (see Algorithm 3). Here, direct child nodes
(see Algorithm 2) include those child nodes of i
for which i is the only parent or if there is more
than one parent then every other parent is shifted
on to the stack without possibility to reduce the
child node. If no direct child node is in buffer, then
descendants of i are shifted (line 9-10). Now, there
are three configurations possible between i and j:
1. i and j are connected by arc in C. This results
in RIGHTARC or LEFTARC action; 2. i is descen-
dant of j. In this case the parents of i (such that
they are descendants of j) and siblings of i through
such parents are shifted. 3. i is sibling of j. In
this case, the parents of i and their descendants are
shifted such that A remains consistent. Addition-
ally, because the input is a graph structure, more
than one of the above configuration can occur si-
multaneously. We analyse the three configurations
in detail below.

Since the direct child nodes of i are shifted,
{j ← i} results in a LEFTARC action (line 18).
Also because the input is a graph, i can be a sib-
ling node of j. In this case, the valid parents and
siblings of i are shifted. We iterate through the
other elements in stack to identify the valid par-
ents and siblings. These conditions are encapsu-
lated in PROCESSSIBLING (line 20). Conditions
for RIGHTARC are similar to that of LEFTARC
with the following differences. We ensure that
there is no left arc relationship for j in A (line 11).
If there is a left arc relationship for j in A, it means
that in an arc-standard setting, the RIGHTARC ac-
tions for j have already been made. If i is a descen-
dant of j, valid parents and siblings of i are shifted.
We iterate through the parents of i and those par-
ents which are in turn descendants of j and not
shifted on to the stack are valid parents. We shift
the parent and the subtree through each such par-
ent. These conditions are denoted by PROCESS-
DESCENDANT (line 14).

C

B

X11 X12 X13

A

X21 X22 X23

D

X31 X32 X33

Figure 7: Sample graph to illustrate PROCESSSI-
BLING

If there is no arc between j and i and there is
only one element on the stack, then the parents
and siblings of i are shifted (line 22-23). If there
is more than one element on the stack, and if i is
descendant of j, then we use PROCESSDESCEN-
DANT (line 25-26). If i is sibling to j we use PRO-
CESSSIBLING (line 27-28).

Consider an example to see the working of
PROCESSSIBLING in detail. In PROCESSSIB-
LING, we need to ensure that i is in stack because
of sibling relation with j and we need to shift the
valid parent nodes of i and their descendants. We
call these valid nodes inflection points. Consider
the following stack entries [D, A, B, C] with C as
stack top. Assume that the input graph is as in
Figure 7. C is sibling of B through B’s parents
X11, X12, X13. Out of these, only X11 and X12 are
valid parents. X13 is sibling to A through A’s par-
ent X23. But X23 is in turn neither descendant of D
nor sibling of D. Thus X13 is not a valid inflection
point for C. Now, X12 is sibling of A through A’s
parent X22. X22 is in turn sibling of D through X32.
Thus there is a path to the stack bottom through a
path of siblings/ descendant. In case of X11, X11
is descendant of stack element A and is thus valid.
X11 and X12 are called valid inflection points. If
inflection point is a common parent to both S0 and
S1 then inflection point and its descendants are
shifted. Instead, if inflection point is ancestor to
S0, then parents of S0 (say P0) which are descen-
dants of inflection point are shifted. Additionally,
descendants of P0 are shifted.

654


