



















































Unsupervised Pretraining for Neural Machine Translation Using Elastic Weight Consolidation


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, pages 130–135
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

130

Unsupervised Pretraining for Neural Machine Translation
Using Elastic Weight Consolidation

Dušan Variš
Charles University,

Faculty of Mathematics and Physics
Malostranské náměstı́ 25

118 00 Prague, Czech Republic
varis@ufal.mff.cuni.cz

Ondřej Bojar
Charles University,

Faculty of Mathematics and Physics
Malostranské náměstı́ 25

118 00 Prague, Czech Republic
bojar@ufal.mff.cuni.cz

Abstract

This work presents our ongoing research of
unsupervised pretraining in neural machine
translation (NMT). In our method, we initial-
ize the weights of the encoder and decoder
with two language models that are trained with
monolingual data and then fine-tune the model
on parallel data using Elastic Weight Consoli-
dation (EWC) to avoid forgetting of the orig-
inal language modeling tasks. We compare
the regularization by EWC with the previous
work that focuses on regularization by lan-
guage modeling objectives.

The positive result is that using EWC with the
decoder achieves BLEU scores similar to the
previous work. However, the model converges
2-3 times faster and does not require the orig-
inal unlabeled training data during the fine-
tuning stage.

In contrast, the regularization using EWC is
less effective if the original and new tasks are
not closely related. We show that initializing
the bidirectional NMT encoder with a left-to-
right language model and forcing the model
to remember the original left-to-right language
modeling task limits the learning capacity of
the encoder for the whole bidirectional con-
text.

1 Introduction

Neural machine translation (NMT) using sequence
to sequence architectures (Sutskever et al., 2014;
Bahdanau et al., 2014; Vaswani et al., 2017) has
become the dominant approach to automatic ma-
chine translation. While being able to approach
human-level performance (Popel, 2018), it still re-
quires a huge amount of parallel data, otherwise it
can easily overfit. Such data, however, might not
always be available. At the same time, it is gener-
ally much easier to gather large amounts of mono-
lingual data, and therefore, it is interesting to find
ways of making use of such data. The simplest

strategy is to use backtranslation (Sennrich et al.,
2016), but it can be rather costly since it requires
training a model in the opposite translation direc-
tion and then translating the monolingual corpus.

It was suggested by Lake et al. (2017) that dur-
ing the development of a general human-like AI
system, one of the desired characteristics of such
a system is the ability to learn in a continuous
manner using previously learned tasks as build-
ing blocks for mastering new, more complex tasks.
Until recently, continuous learning of neural net-
works was problematic, among others, due to the
catastrophic forgetting (McCloskey and Cohen,
1989). Several methods were proposed (Li and
Hoiem, 2016; Aljundi et al., 2017; Zenke et al.,
2017), however, they mainly focus only on adapt-
ing the whole network (not just its parts) to new
tasks while maintaining good performance on the
previously learned tasks.

In this work, we present an unsupervised pre-
training method for NMT models using Elastic
Weight Consolidation (Kirkpatrick et al., 2017).
First, we initialize both encoder and decoder with
source and target language models respectively.
Then, we fine-tune the NMT model using the par-
allel data. To prevent the encoder and decoder
from forgetting the original language modeling
(LM) task, we regularize their weights individu-
ally using Elastic Weight Consolidation based on
their importance to that task. Our hypothesis is the
following: by forcing the network to remember the
original LM tasks we can reduce overfitting of the
NMT model on the limited parallel data.

We also provide a comparison of our approach
with the method proposed by Ramachandran et al.
(2017). They also suggest initialization of the en-
coder and decoder with a language model. How-
ever, during the fine-tuning phase they use the
original language modeling objectives as an ad-
ditional training loss in place of model regular-



131

ization. Their approach has two main drawbacks:
first, during the fine-tuning phase, they still require
the original monolingual data which might not be
available anymore in a life-long learning scenario.
Second, they need to compute both machine trans-
lation and language modeling losses which in-
creases the number of operations performed dur-
ing the update slowing down the fine-tuning pro-
cess. Our proposed method addresses both prob-
lems: it requires only a small held-out set to esti-
mate the EWC regularization term and converges
2-3 times faster than the previous method.1

2 Related Work

Several other approaches towards exploiting the
available monolingual data for NMT have been
previously proposed.

Currently, the most common method is creat-
ing synthetic parallel data by backtranslating the
target language monolingual corpora using ma-
chine translation (Sennrich et al., 2016). While
being consistently beneficial, this method requires
a pretrained model to prepare the backtranslations.
Additionally, Ramachandran et al. (2017) showed
that the unsupervised pretraining approach reaches
at least similar performance to the backtranslation
approach.

Recently, Lample and Conneau (2019) sug-
gested using a single cross-lingual language model
trained on multiple monolingual corpora as an ini-
tialization for various NLP tasks, including ma-
chine translation. While our work focuses strictly
on a monolingual language model pretraining, we
believe that our work can further benefit from us-
ing cross-lingual language models.

Another possible approach is to introduce an
additional reordering (Zhang and Zong, 2016) or
de-noising objectives, the latter being recently
employed in the unsupervised NMT scenarios
(Artetxe et al., 2018; Lample et al., 2017). These
approaches try to force the NMT model to learn
useful features by presenting it with either shuf-
fled or noisy sentences teaching it to reconstruct
the original input.

Furthermore, Khayrallah et al. (2018) show how
to prevent catastrophic forgeting during domain
adaptation scenarios. They fine-tune the general-
domain NMT model using in-domain data adding

1The speedup is with regard to the wall-clock time. In
our experiments both EWC and the LM-objective methods
require similar number of training examples to converge.

an additional cross-entropy objective to restrict the
distribution of the fine-tuned model to be similar
to the distribution of the original general-domain
model.

3 Elastic Weight Consolidation

Elastic Weight Consolidation (Kirkpatrick et al.,
2017) is a simple, statistically motivated method
for selective regularization of neural network pa-
rameters. It was proposed to counteract catas-
trophic forgetting in neural networks during a life-
long continuous training. The previous work de-
scribed the method in the context of adapting the
whole network for each new task. In this section,
we show that EWC can be also used to preserve
only parts of the network that were relevant for
the previous task, thus being potentially useful for
compositional learning.

To justify the choice of the parameter con-
straints, Kirkpatrick et al. (2017) approach the
neural network training as a Bayesian inference
problem. To put it into the context of NMT, we
would like to find the most probable network pa-
rameters θ, given a parallel data Dmt and mono-
lingual data Dsrc and Dtgt for source and target
languages, respectively:

p(θ|Dmt ∪Dsrc ∪Dtgt) =
p(Dmt|θ)p(θ|Dsrc ∪Dtgt)

p(Dmt)
(1)

Equation 1 holds, assuming datasets Dmt, Dsrc
and Dtgt being mutually exclusive. The probabil-
ity p(Dmt|θ) is the negative of the MT loss func-
tion and p(θ|Dsrc ∪ Dtgt) is the result of the un-
supervised pretraining. We can assume that dur-
ing the unsupervised pretraining, the parameters
θsrc of the encoder are independent of the param-
eters θtgt of the decoder. Furthermore, we as-
sume that the parameters of the encoder are in-
dependent of the target-side monolingual data and
the parameters of the decoder are independent of
the source-side monolingual data. Given these as-
sumptions, we can express the posterior probabil-
ity p(θ|Dsrc ∪Dtgt) in the following way:

p(θ|Dsrc∪Dtgt) = p(θsrc|Dsrc)p(θtgt|Dtgt) (2)

Probabilities p(θsrc|Dsrc) and p(θtgt|Dtgt) are
given by the pretrained source and target lan-
guage models respectively. The true posterior
probabilities given by the language models are in-
tractable during fine-tuning, however, similarly to



132

the work of Kirkpatrick et al. (2017), we can es-
timate p(θsrc|Dsrc) as Gaussian distribution us-
ing Laplace approximation (MacKay, 1992), with
mean given by the pretrained parameters θsrc and
variance given by a diagonal of the Fisher informa-
tion matrix Fsrc. Then, we can add the following
regularization term to our loss function:

Lewc−src(θ) =
∑

i,θi⊂θsrc

λ

2
Fsrc,i(θi − θ?src,i)2 (3)

The model parameters not present during the
language model pretraining are ignored by the
regularization term. Analogically, the same can
be applied for the target-side posterior probabil-
ity p(θtgt|Dtgt) giving a target-side regularization
term Lewc−tgt.

In the following section, we show that these reg-
ularization terms can be useful in a low-resource
machine translation scenario. Since we do not
necessarily need to preserve the knowledge of the
original language modeling tasks, we focus on us-
ing them only as prior knowledge to prevent over-
fitting during the fine-tuning.

4 Experiments

In this section, we present the results of our ex-
periments with EWC regularization and compare
them with the previously proposed regularization
by language modeling objectives.

4.1 Model Description
In all experiments, we use the self-attentive Trans-
former network (Vaswani et al., 2017) because it is
the current state-of-the-art NMT architecture, pro-
viding us with a strong baseline. In general, it fol-
lows the standard encoder-decoder paradigm, with
encoder creating hidden representations of the in-
put tokens based on their surrounding context and
decoder generating the output tokens autoregres-
sively while attending to the source sentence to-
ken representations and tokens it generated in the
previous decoding steps.2

We use Transformer with 6 layers in both en-
coder and decoder. We set the dimension of the
hidden states to 512 and the dimension of the feed-
forward layer to 2048. We use multi-head at-
tention with 16 attention heads. To simplify the
pretraining process, we use a separate vocabulary

2For more details about the architecture, see the original
paper.

for source and target languages, each containing
around 32k subwords. We use separate embed-
dings in the encoder and decoder. In the decoder,
we tie the embeddings with the output softmax
layer (Press and Wolf, 2017). During both pre-
training and fine tuning, we use Adam optimizer
(Kingma and Ba, 2014) and gradient clipping. We
set the initial learning rate to 3.1, use a linear
warm-up for 33500 training steps and then decay
the learning rate exponentially. We set the train-
ing batch size to a maximum of 2048 tokens per
batch together with sentence bucketing for more
efficient training. We set dropout to 0.1. During
the final evaluation, we use beam search decoding
with beam size of 8 and length normalization set
to 1.0.

When pretraining the encoder and decoder, we
use identical network parameters. We train each
language model to maximize the probability of
each word in a sentence using its leftward context.
To pretrain the decoder, we use the decoder archi-
tecture from Transformer with encoder-attention
sub-layer removed due to the lack of source sen-
tences. Later, we initialize the NMT decoder
with the language model weights and the encoder-
attention weights by a normal distribution. We
reset all training-related variables (learning rate,
Adam moments) during the NMT initialization.

For simplicity, we apply the same approach for
the encoder pretraining. In Section 4.2, we discuss
the drawbacks of our encoder pretraining and sug-
gest possible improvements. In all experiments,
we set the weight λ of each EWC regularization
term to 0.02.

The model implementation is available in Neu-
ral Monkey3 (Helcl and Libovický, 2017) frame-
work for sequence-to-sequence modeling.

4.2 Dataset and Evaluation

In our experiments, we focused on the low-
resource Basque-to-English machine translation
task featured in IWSLT 2018.4 We used the par-
allel data provided by IWSLT organizers, con-
sisting of 5,600 in-domain sentence pairs (TED
Talks) and around 940,000 general-domain sen-
tence pairs. During pretraining, we used Basque
Wikipedia for source language model and News-

3https://github.com/ufal/neuralmonkey
4https://sites.google.com/site/

iwsltevaluation2018/TED-tasks

https://github.com/ufal/neuralmonkey
https://sites.google.com/site/iwsltevaluation2018/TED-tasks
https://sites.google.com/site/iwsltevaluation2018/TED-tasks


133

SRC TGT ALL

Baseline 15.68 – – –
Backtrans. 19.65 – – –

LM best – 13.96 15.56 16.83
EWC best – 10.77 15.91 14.10

LM ens. – 15.16 16.60 17.14
EWC ens. – 10.73 16.63 14.66

Table 1: Comparison of the previous work (LM) with
the proposed method (EWC). We compared models
with only pretrained encoder (SRC), pretrained de-
coder (TGT) and both (ALL). All pretrained language
models contained 3 layers. We compared both single
best models and ensemble (using checkpoint averag-
ing) of 4 best checkpoints. Results where the proposed
method outperformed the previous work are in bold.

Commentary 2015 provided by WMT5 for target
language model. Both corpora contain close to 3
million sentences. We used UDPipe6 (Straka and
Straková, 2017) to split the monolingual data to
sentences and SentencePiece7 to prepare the sub-
word tokenization. We used the subword models
trained on the monolingual data to preprocess the
parallel data.

During training, we used development data pro-
vided by IWSLT 2018 organizers which contains
1,140 parallel sentences. To approximate the
Fisher Information Matrix diagonal of the pre-
trained Basque and English language models, we
used the respective parts of the IWSLT validation
set. For final evaluation, we used the IWSLT 2018
test data consisting of 1051 sentence pairs.

Table 1 compares the performance of the mod-
els fine-tuned using the LM objective regulariza-
tion and the EWC regularization. First, we can
see that using EWC when only the decoder was
pretrained slightly outperforms the previous work.
On the other hand, our method fails when used
in combination with the encoder initialization by
the source language model. The reason might be
a difference between the original LM task that
is trained in a left-to-right autoregressive fashion
while the strength of the Transformer encoder is
in modelling of the whole left-and-right context
for each source token. The learning capacity of

5http://www.statmt.org/wmt18/
translation-task.html

6http://ufal.mff.cuni.cz/udpipe
7https://github.com/google/

sentencepiece

0.5 1 1.5 2 2.5
·107

5

10

15

Number of Sentences

B
L

E
U

depth-2
depth-4
depth-6

no pretraining

Figure 1: Performance of MT models where only the
encoder was initialized by the language model of vary-
ing depths and then regularized by EWC. We include
the performance of the MT system that was not pre-
trained for comparison.

the encoder is therefore restricted by forcing it to
remember a task that is not so closely related to the
sentence encoding in Transformer NMT. Figure 1
supports our claim: the deeper the pretrained lan-
guage model and therefore more layers regularized
by EWC, the lower the performance of the fine-
tuned NMT system. We think that this behaviour
can be mitigated by initializing the encoder with
a language model that considers the whole bidi-
rectional context, e.g. a recently introduced BERT
encoder (Devlin et al., 2018). We leave this for our
future work.

In addition to improving model performance,
EWC converges much faster than the previously
introduced LM regularizer. Figure 2 shows that
the model fine-tuned without LM regularization
converged in about 10 hours, while it took around
20-30 hours to converge when LM regularization
was in place. Note, that all models converged af-
ter seeing a similar number of training examples,
however, computing the LM loss for regulariza-
tion introduces an additional computation over-
head. The main benefit of both EWC and LM-
based regularization is apparent here, too. The
non-regularized model quickly overfits.

As the comparison to the model trained on the
backtranslated monolingual corpus shows, none of
our regularization methods can match this sim-
ple but much more computationally demanding
benchmark.

http://www.statmt.org/wmt18/translation-task.html
http://www.statmt.org/wmt18/translation-task.html
http://ufal.mff.cuni.cz/udpipe
https://github.com/google/sentencepiece
https://github.com/google/sentencepiece


134

10 20 30 40 50

5.0

5.5

6.0

6.5

7.0

7.5

Training time (h)

Pe
rp

le
xi

ty
EWC-reg
LM-reg
no-reg

Figure 2: Comparison of relative convergence times
(measured by perplexity) of models where only the de-
coder was pretrained. The models were regularized us-
ing EWC, LM objective or were not using any regular-
ization (no reg.). All models were trained on the same
number of training examples (∼27M sentences). All
used a pretrained LM with 3 Transformer layers.

5 Conclusion

We introduced our work in progress, and explo-
ration of model regularization of NMT encoder
and decoder parameters based on their importance
for previously learned tasks and its application in
the unsupervised pretraining scenario. We doc-
umented that our method slightly improves the
NMT performance (compared to the baseline as
well as the previous work of LM-based regular-
ization) when combined with a pretrained target
language model. We achieve this improvement at
a reduced training time.

We also showed that the method is less effec-
tive if the original language modeling task used
to pretrain the NMT encoder is too different from
the task learned during the fine-tuning. We plan to
further investigate whether we can gain improve-
ments by using a different pretraining method for
the encoder and how much this task mismatch re-
lates to the learning capacity of the encoder.

Acknowledgments

This work has been in part supported by the
project no. 19-26934X (NEUREM3) of the Czech
Science Foundation, by the Charles University
SVV project number 260 453 and by the grant no.
1140218 of the Grant Agency of the Charles Uni-
versity.

References
R. Aljundi, P. Chakravarty, and T. Tuytelaars. 2017.

Expert gate: Lifelong learning with a network of ex-
perts. In 2017 IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), volume 00, pages
7120–7129.

Mikel Artetxe, Gorka Labaka, Eneko Agirre, and
Kyunghyun Cho. 2018. Unsupervised neural ma-
chine translation. In Proceedings of the Sixth Inter-
national Conference on Learning Representations.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua
Bengio. 2014. Neural machine translation by
jointly learning to align and translate. CoRR,
abs/1409.0473.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing.

Jindřich Helcl and Jindřich Libovický. 2017. Neural
Monkey: An Open-source Tool for Sequence Learn-
ing. The Prague Bulletin of Mathematical Linguis-
tics, (107):5–17.

Huda Khayrallah, Brian Thompson, Kevin Duh, and
Philipp Koehn. 2018. Regularized training objective
for continued training for domain adaptation in neu-
ral machine translation. In Proceedings of the 2nd
Workshop on Neural Machine Translation and Gen-
eration, pages 36–44, Melbourne, Australia. Asso-
ciation for Computational Linguistics.

Diederik P. Kingma and Jimmy Ba. 2014. Adam:
A Method for Stochastic Optimization. CoRR,
abs/1412.6980.

James Kirkpatrick, Razvan Pascanu, Neil C. Rabi-
nowitz, Joel Veness, Guillaume Desjardins, An-
drei A. Rusu, Kieran Milan, John Quan, Tiago Ra-
malho, Agnieszka Grabska-Barwinska, Demis Has-
sabis, Claudia Clopath, Dharshan Kumaran, and
Raia Hadsell. 2017. Overcoming catastrophic for-
getting in neural networks. Proceedings of the Na-
tional Academy of Sciences of the United States of
America, 114 13:3521–3526.

Brenden M. Lake, Tomer D. Ullman, Joshua B. Tenen-
baum, and Samuel J. Gershman. 2017. Building ma-
chines that learn and think like people. Behavioral
and Brain Sciences, 40:e253.

Guillaume Lample and Alexis Conneau. 2019. Cross-
lingual language model pretraining. arXiv preprint
arXiv:1901.07291.

Guillaume Lample, Ludovic Denoyer, and
Marc’Aurelio Ranzato. 2017. Unsupervised
machine translation using monolingual corpora
only. CoRR, abs/1711.00043.

Zhizhong Li and Derek Hoiem. 2016. Learning with-
out forgetting. In European Conference on Com-
puter Vision, pages 614–629. Springer.

https://doi.org/10.1109/CVPR.2017.753
https://doi.org/10.1109/CVPR.2017.753
http://arxiv.org/abs/1409.0473
http://arxiv.org/abs/1409.0473
http://arxiv.org/abs/1810.04805
http://arxiv.org/abs/1810.04805
http://arxiv.org/abs/1810.04805
https://doi.org/10.1515/pralin-2017-0001
https://doi.org/10.1515/pralin-2017-0001
https://doi.org/10.1515/pralin-2017-0001
https://www.aclweb.org/anthology/W18-2705
https://www.aclweb.org/anthology/W18-2705
https://www.aclweb.org/anthology/W18-2705
http://arxiv.org/abs/1412.6980
http://arxiv.org/abs/1412.6980
https://doi.org/10.1017/S0140525X16001837
https://doi.org/10.1017/S0140525X16001837
http://arxiv.org/abs/1711.00043
http://arxiv.org/abs/1711.00043
http://arxiv.org/abs/1711.00043


135

David J. C. MacKay. 1992. A practical bayesian frame-
work for backpropagation networks. Neural Com-
putation, 4(3):448–472.

Michael McCloskey and Neil J. Cohen. 1989. Catas-
trophic interference in connectionist networks: The
sequential learning problem. The Psychology of
Learning and Motivation, 24:104–169.

Martin Popel. 2018. CUNI transformer neural MT sys-
tem for WMT18. In Proceedings of the Third Con-
ference on Machine Translation, Volume 2: Shared
Tasks, volume 2, pages 486–491, Stroudsburg, PA,
USA. Association for Computational Linguistics,
Association for Computational Linguistics.

Ofir Press and Lior Wolf. 2017. Using the output em-
bedding to improve language models. In Proceed-
ings of the 15th Conference of the European Chap-
ter of the Association for Computational Linguistics:
Volume 2, Short Papers, pages 157–163, Valencia,
Spain. Association for Computational Linguistics.

Prajit Ramachandran, Peter J. Liu, and Quoc V. Le.
2017. Unsupervised pretraining for sequence to se-
quence learning. In Proceedings of the 2017 Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP 2017, Copenhagen, Denmark,
September 9-11, 2017, pages 383–391.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Improving neural machine translation mod-
els with monolingual data. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
86–96, Berlin, Germany. Association for Computa-
tional Linguistics.

Milan Straka and Jana Straková. 2017. Tokenizing,
pos tagging, lemmatizing and parsing ud 2.0 with
udpipe. In Proceedings of the CoNLL 2017 Shared
Task: Multilingual Parsing from Raw Text to Univer-
sal Dependencies, pages 88–99, Vancouver, Canada.
Association for Computational Linguistics.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in neural information process-
ing systems, pages 3104–3112.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In I. Guyon, U. V. Luxburg, S. Bengio,
H. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-
nett, editors, Advances in Neural Information Pro-
cessing Systems 30, pages 6000–6010. Curran As-
sociates, Inc.

Friedemann Zenke, Ben Poole, and Surya Ganguli.
2017. Continual learning through synaptic intel-
ligence. In Proceedings of the 34th International
Conference on Machine Learning, ICML 2017, Syd-
ney, NSW, Australia, 6-11 August 2017, pages 3987–
3995.

Jiajun Zhang and Chengqing Zong. 2016. Exploit-
ing source-side monolingual data in neural machine
translation. In Proceedings of the 2016 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 1535–1545. Association for Compu-
tational Linguistics.

https://doi.org/10.1162/neco.1992.4.3.448
https://doi.org/10.1162/neco.1992.4.3.448
https://www.aclweb.org/anthology/E17-2025
https://www.aclweb.org/anthology/E17-2025
https://aclanthology.info/papers/D17-1039/d17-1039
https://aclanthology.info/papers/D17-1039/d17-1039
http://www.aclweb.org/anthology/P16-1009
http://www.aclweb.org/anthology/P16-1009
http://www.aclweb.org/anthology/K/K17/K17-3009.pdf
http://www.aclweb.org/anthology/K/K17/K17-3009.pdf
http://www.aclweb.org/anthology/K/K17/K17-3009.pdf
http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf
http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf
http://proceedings.mlr.press/v70/zenke17a.html
http://proceedings.mlr.press/v70/zenke17a.html
https://doi.org/10.18653/v1/D16-1160
https://doi.org/10.18653/v1/D16-1160
https://doi.org/10.18653/v1/D16-1160

