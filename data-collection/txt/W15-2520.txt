



















































Novel Document Level Features for Statistical Machine Translation


Proceedings of the Second Workshop on Discourse in Machine Translation (DiscoMT), pages 153–157,
Lisbon, Portugal, 17 September 2015. c©2015 Association for Computational Linguistics.

Novel Document Level Features for Statistical Machine Translation

Rong Zhang and Abraham Ittycheriah
IBM T.J. Watson Research Center

Yorktown Heights, NY 10598, USA
{zhangr,abei}@us.ibm.com

Abstract

In this paper, we introduce document level
features that capture necessary informa-
tion to help MT system perform better
word sense disambiguation in the transla-
tion process. We describe enhancements
to a Maximum Entropy based translation
model, utilizing long distance contextual
features identified from the span of entire
document and from both source and tar-
get sides, to improve the likelihood of the
correct translation for words with multiple
meanings, and to improve the consistency
of the translation output in a document set-
ting. The proposed features have been ob-
served to achieve substantial improvement
of MT performance on a variety of stan-
dard test sets in terms of TER/BLEU score.

1 Introduction

Most statistical machine translation (MT) systems
use sentence as the processing unit for both train-
ing and decoding. This strategy, mainly the result
of pursuing efficiency, assumes that each sentence
is independent, and therefore suffers the loss of
missing many kinds of ”global” information, such
as domain, topic and inter-sentence dependency,
which are particularly important for word sense
disambiguation (Chan et al., 2007) and need be
learned from the span of entire document.

Table 1 shows the MT output of our sentence
level Arabic-to-English translation engine on two
sentences excerpted from a news article discussing
middle-east politics. The Arabic sentences are
displayed in Romanized form. The Arabic word
mrsy denotes the name of the former Egyptian
president Morsi in both sentences. In the first
sentence it is translated together with prior word
mHmd(Mohamed) as a phrase and mapped to the
name correctly. In the second sentence, where no
relevant local context is present, it is incorrectly
translated into the word thank, which is the most

frequent English word aligned to mrsy in our train-
ing data. This example shows that for ambigu-
ous words like mrsy, utilizing only local features
is insufficient to find them the correct translation
hypotheses. This example also illustrates another
weakness of sentence level MT. It has been ob-
served that a word tends to keep same meaning
within one document (Gale et al., 1992; Carpuat ,
2009). However, such consistency can’t be main-
tained by MT system working on isolated sen-
tences since all decisions are made locally.

AR: Alr}ys AlmSry AlmEzwl mHmd mrsy
ysf nfsh b〉nh r}ys Aljmhwryp

MT: The deposed Egyptian president Mo-
hamed Morsi describes himself as the
president of the republic

AR: mrsy ytHdY AlqADy fy mHAkmth
bthmp Alhrwb mn Alsjn

MT: Thank you defy the judge in his trial on
charges of escaping from prison

Table 1: Sentence level MT results of two sen-
tences excerpted from same document

To address these issues, this paper investigates
document level features to utilize useful informa-
tion from wider context. Three types of docu-
ment level features, including source and target
side long distance context, and ”quasi-topic”, are
integrated into our MT system via the framework
of Maximum Entropy, and lead to substantial im-
provement of translation performance.

2 A Practical Scheme to Approximate
Document Level Machine Translation

Let Df denote a document in source lan-
guage f consisting of N sentences: Df =<
f1, f2, ......, fN >. The goal of document level MT
is to search the best document hypothesis D∗e in
target language e that maximizes the translation
probability:

D∗e = arg max
De

Pr(De|Df ) (1)

153



We require that the number of sentences inDf and
De to be equal: De =< e1, e2, ......, eN >. Using
chain rule, Pr(De|Df ) is estimated as follows:

Pr(De|Df ) =
N∏

i=1

Pr(ei|fi, Df ,̄i, De,i−) (2)
where Df ,̄i denotes the source document ex-
cluding the current sentence fi, and De,i− =<
e1, e2, ......, ei−1 > which is the MT output up to
previous sentence. If we keep the i.i.d. assumption
of sentence generation that Df ,̄i and De,i− are ir-
relevant to< fi, ei >, Eq. (2) backs off to standard
sentence level translation that

Pr(De |Df ) =
N∏

i=1

Pr(ei|fi) (3)
In our document level MT experiments, the esti-
mate of Pr(ei|fi, Df ,̄i, De,i−) is divided into three
separate modules combined by a normalization
function:

Pr(ei|fi, Df ,̄i, De,i−) =
Θ{Pr(ei|fi),Pr(ei|Df ,̄i),Pr(ei|fi, De,i−)}

(4)
Eq. (4) provides a scheme to integrate docu-
ment level context features into translation pro-
cess. In (4), Pr(ei|fi) is the standard sentence
level translation model, Pr(ei|Df ,̄i) models how
source side long distance features, e.g. feature
regarding document topic or trigger word not in
current sentence, impact the generation of ei, and
Pr(ei|fi, De,i−) can be viewed as a module ex-
ploring target side cross-sentence dependency be-
tween ei and De,i− to maintain translation con-
sistency. Θ(.) is a normalized combination func-
tion that incorporates the three modules together to
generate a probabilistic estimate for each hypoth-
esis. Please note that in consideration of decod-
ing speed, the proposed scheme does not search
optimal hypothesis from document space directly,
but rather enhance sentence translation by utilizing
”global” information not limited to current sen-
tence fi.

3 Document Level Context Features

The MT system adopted in our experiments is a di-
rect translation model that utilizes the framework
of Maximum Entropy to combine multiple types
of lexical and syntactic features into translation
(Ittycheriah and Roukos , 2007). The model has
the following form:

Pr(t, j|s) = Pr0(t, j|s)
Z

exp
∑

k

αkφk(s, t) (5)

where s is a source side word or phrase, t is the
corresponding word or phrase translation, j is the
transition distance from last translated word, Pr0
is a prior distribution related to phrase to phrase
translation model and distortion model, and Z is a
normalizing term. In Eq. (5), feature φk(s, t) can
be viewed as a binary question regarding lexical
and syntactic attributes of s and t, e.g. the ques-
tion can be asked as if s and t share same POS
class. Weight αk is estimated using Iterative Scal-
ing algorithm. Testing results from many evalua-
tion tasks have shown that the MaxEnt system per-
forms significantly better than regular phrase sys-
tem and equally well to hierarchical system.

This section introduces three new types of doc-
ument level features to model Pr(ei|Df ,̄i) and
Pr(ei|fi, De,i−). All the three types of features
can be expressed as a triplet that φk(s, t) =<
s, c, t >, where c denotes a source or target side
context word, identified from the span of entire
document, which works as a bridge to connect s
and t. Please note that φk is still a binary feature
which indicates if a particular context word c of
certain type exists for s and t.

3.1 Source Side Long Distance Context
Feature

The first type of document level feature is mo-
tivated by the example shown in Table 1. The
ambiguous Arabic word mrsy in the second sen-
tence is mistranslated to English word thank be-
cause there is no local evidence to suggest it is
a person name rather than a verb which is more
common in training data and thus has higher trans-
lation probability in prior phrase model Pr0. In
this case, if the words co-occurring with mrsy in
the first sentence, i.e. mHmd(Mohamed), can be
identified and passed to subsequent sentences, the
probability of mrsy in the same document being
translated into Morsi is likely to be increased.
φLDC , the long distance context (LDC) feature,

is implemented as follows in training stage. Sup-
pose the questioned source word wf occur in sen-
tence i with translation we . To identify the rele-
vant LDC word cf , the entire document excluding
current sentence i is analyzed to find if the align-
ment (wf ,we) also occurs in other sentences. If
yes, the source words within a window centered
by wf at that place are collected as the candi-
dates for cf . For instance, if the two Arabic sen-
tences of Table 1 are in training data, the words

154



mHmd(Mohamed) and AlmSry(Egyptian) in the
first sentence will be viewed as the LDC word for
mrsy in the second sentence, which results in two
φLDC features i.e. < mrsy,mHmd ,Morsi > and
< mrsy,AlmSry ,Morsi >. As illustrated in Eq.
(5), the two features can boost the translation prob-
ability of Pr(Morsi|mrsy) for entire document if
their weights are properly learned.

In training stage the check of aligned target
word we is to ensure that only words with same
meaning can be grouped together to share context
features. In decoding where true we is unknown,
we only use wf instead of (wf ,we) to identify
LDC word cf . In our experiment function words
are not allowed to be cf , and tf ∗ idf score is used
to filter out irrelevant context word.

3.2 Target Side Long Distance Context
Feature

In order to improve the consistency of word choice
in hypothesis generation, φLDC can be extended to
target side to utilize the correlation betweenDe,i−,
the translation up to previous sentences, and ei,
the translation of current sentence.

In training stage, φtLDC , the target side long
distance context (tLDC) feature, is implemented
in the following way. For a questioned source
word wf which is aligned to target word we in sen-
tence i , we search their occurrence in all previous
sentences from 1 to i -1. If exists, the target side
words within the window centered by we in that
sentence are identified as the candidates of tLDC
word ce for wf . For the example used before,
the English side words Mohamed and president
are expected to make φtLDC features for the word
mrsy in the second sentence.

The feature in decoding stage is implemented
similarly by remembering previous translation
De,i− and its alignment to source words. Please
note we don’t use the hypothesized translation we
itself as the tLDC word for wf . This is because if it
is an incorrect translation, such error can be spread
to subsequent sentences to cause duplicated errors.

3.3 LSA based Quasi-Topic Feature
LDC and tLDC features are effective for repeated
words. For words occurring once in a document,
quasi-topic (QT) feature φQT is proposed as a
back-off model which utilizes underlying topic in-
formation to eliminate ambiguity for these words.

In training stage, Latent Semantic Analysis
(LSA) is performed on bilingual corpus consist-

ing of a large set of documents with parallel sen-
tences. Both source and target side words are
mapped to vectors locating in a unified high di-
mensional space. For a questioned word wf , its
QT feature words are selected as follows. First all
source side content words in the same document
are calculated tf ∗ idf score, and sorted by their
values from high to low. The top L words are then
collected as the indicators of the underlying docu-
ment topic. Next semantic similarity is measured
between wf and each of the L candidates based on
Cosine metric. Only words showing strong corre-
lation are selected as the QT feature ct for wf .

In decoding stage, MaxEnt model, as shown in
Eq. (5), is utilized to estimate the probability of a
hypothesis we being generated from wf and QT
feature words ct. Our preliminary experiments
found MaxEnt model performs better than com-
monly used vector based similarity metric. Gener-
ally speaking the QT features provide an implicit
way for topic adaptation. When applied to trans-
lation, it changes the lexical distribution of target
words to prefer the one more relevant to the hidden
topic represented by ct.

4 Related Work

Recent years witness a growing interest in exploit-
ing long distance dependency to improve MT per-
formance (Wong and Kit , 2012; Hardmeier et
al., 2013). Domain adaptation and topic adapta-
tion have attracted considerable attentions (Eidel-
man et al., 2012; Chen et al., 2013; Hewavitha-
rana et al., 2013; Xiong and Zhang, 2013a; Hasler
et al., 2014). There are also efforts that explore
lexical cohesions with the help of WordNet to
describe semantic co-occurrence from document
span (Ben et al., 2013; Xiong et al., 2013b).
Translation consistency, related to the observa-
tion of one sense per discourse (Gale et al., 1992;
Carpuat , 2009; Guillou , 2013), has been dis-
cussed recently as an additional metric to evalu-
ate translation quality (Xiao et al., 2011; Ture et
al., 2012). There are also efforts for Arabic proper
name disambiguation (Hermjakob et al., 2008).
This paper investigates novel document level fea-
tures to utilize lexical and semantic dependencies
between sentences. In contrast to (Ben et al.,
2013; Xiong et al., 2013b), our work doesn’t need
external resources e.g. WordNet or human efforts
to identify word cohesion and isn’t limited to cer-
tain word type. The advantage makes the proposed

155



Model MT03 MT04 MT05 MT06 MT08 MT09
Baseline 39.19 57.01 37.15 56.12 35.46 58.71 41.16 51.79 42.60 50.62 42.27 51.53
+LDC 38.99 57.58 37.19 56.43 35.41 59.20 41.29 52.27 42.45 51.45 42.18 52.02
+tLDC 39.02 57.32 37.16 56.41 35.32 58.91 41.12 52.03 42.56 51.14 42.16 51.77
+QT 39.10 57.16 37.03 56.23 35.37 58.79 41.14 51.80 42.52 50.75 42.22 51.68

+LDC+tLDC 38.46 57.83 36.83 56.50 34.97 59.64 40.91 52.23 42.25 51.26 41.92 52.26
+L+tL+QT 38.54 57.73 36.73 56.75 34.84 59.75 40.68 52.40 42.05 51.42 41.82 52.47

Table 2: MT performance on MT03-MT09 in terms of TER and BLEU.

features more suited to low-resource languages.

5 Experiments

Our system is primarily built for an Arabic di-
alect to English MT task. The training data con-
tains LDC-released parallel corpora for the BOLT
project. There are totally 6.9M sentence pairs
with 207M Arabic ATB tokens and 201M English
words, respectively. Three types of word align-
ments, maximum entropy, GIZA++ and HMM
alignment, are used to generate phrase pairs as the
prior model in Eq. (5). Approximately 1.8M in-
domain sentence pairs distributed in 106K docu-
ments, consisting of 36M Arabic ATB tokens and
38M English words, are selected to learn sentence
and document level MaxEnt features. The tun-
ing set contains 3700 sentences in 350 documents
which are mainly weblog and dialect data. Mod-
ule weights for prior model, sentence and docu-
ment level features, LM, and other components are
tuned with PRO algorithm (Hopkins and May ,
2011) to minimize the score of (TER-BLEU).

We select NIST Arabic MT03-MT09 as the test
sets. Results are shown in Table 2. The two num-
bers in each score column are TER followed by
BLEU. The best performance is illustrated in bold.
The result of MT system using only sentence level
features is listed as the baseline. The integra-
tions of the three document features are denoted
as +LDC, +tLDC and +QT, respectively. Table
2 shows that substantial improvements of transla-
tion quality, measured by both TER and BLEU,
are achieved for most of the test sets.

To understand the effectiveness of document
features on different type of data, we further split
MT09 set into newswire and weblog, and conduct
test on them. Table 3 shows that long distance con-
text features, φLDC and φtLDC , perform better on
newswire than on weblog respecting to the rela-
tive improvement of TER and BLEU. One reason
to explain this is that the rate of content word repe-

tition is different on the two types of data. Accord-
ing to our calculation, about 19% content words in
newswire repeat themselves while the ratio on we-
blog is about 13%.

Model MT09-nw MT09-wl
Baseline 33.85 61.15 50.46 41.35

+LDC+tLDC 33.38 61.98 50.23 42.02

Table 3: MT performance on MT09 newswire and
weblog in terms of TER and BLEU.

Table 4 shows the new MT output of the two ex-
ample sentences. Three LDC features are fired for
mrsy in the 2nd sentence: 〈mrsy,AlmSry,Morsi〉,
〈mrsy,mHmd,Morsi〉 and 〈mrsy,ySf,Morsi〉 where
the 3rd one is a false alarm. Items in the
triplets correspond to source word, context
word and hypothesized word, respectively.
Three tLDC features are also fired including
〈mrsy,Egyptian,Morsi〉, 〈mrsy,Mohamed,Morsi〉
and 〈mrsy,describes,Morsi〉 where the 3rd one is
also a false alarm. To our surprise, word Alr}ys
and its translation president aren’t fired as context
feature. Analysis found that this is due to the
fact that our LDC training data was collected
before Dr. Morsi was elected as president in
2012. Therefore no relevant feature is learned into
MaxEnt model.

AR: Alr}ys AlmSry AlmEzwl mHmd mrsy
ysf nfsh b〉nh r}ys Aljmhwryp

MT: The deposed Egyptian president Mo-
hamed Morsi describes himself as the
president of the republic

AR: mrsy ytHdY AlqADy fy mHAkmth
bthmp Alhrwb mn Alsjn

MT: Morsi defies the judge in his trial on
charges of escaping from prison

Table 4: New MT results using document level
features

156



References
Guosheng Ben, Deyi Xiong, Zhiyang Teng, Yajuan Lu

and Qun Liu. 2013. Bilingual Lexical Cohesion
Trigger Model for Document-Level Machine Trans-
lation. in Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguis-
tics. Sofia, Bulgaria.

Marine Carpuat. 2009. One Translation per Discourse.
in Proceedings of the NAACL HLT Workshop on Se-
mantic Evaluations: Recent Achievements and Fu-
ture Directions. Boulder, USA.

Yee Seng Chan, Hwee Tou Ng, and David Chiang.
2007. Word Sense Disambiguation Improves Sta-
tistical Machine Translation. in Proceedings of the
45th Annual Meeting of the Association for Compu-
tational Linguistics. Prague, Czech.

Boxing Chen, Roland Kuhn and George Foster. 2013.
Vector Space Model for Adaptation in Statistical
Machine Translation. in Proceedings of the 51th An-
nual Meeting of the Association for Computational
Linguistics. Sofia, Bulgaria.

Vladimir Eidelman, Jordan Boyd-Graber and Philip
Resnik. 2012. Topic Models for Dynamic Trans-
lation Model Adaptation. in Proceedings of the 50th
Annual Meeting of the Association for Computa-
tional Linguistics. Jeju Island, Korea.

William A. Gale, Kenneth W. Church, and David
Yarowsky. 1992. One sense per discourse. in Pro-
ceedings of the workshop on Speech and Natural
Language, HLT-91.

Liane Guillou. 2013. Analysing Lexical Consistency
in Translation. in ACL Proceedings of the Workshop
on Discourse in Machine Translation.

Christian Hardmeier, Sara Stymne, Jrg Tiedemann and
Joakim Nivre. 2013. Docent: A Document-
Level Decoder for Phrase-Based Statistical Machine
Translation. in Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics. Sofia, Bulgaria.

Eva Hasler, Phil Blunsom, Philipp Koehn, and Barry
Haddow. 2014. Dynamic Topic Adaptation for
Phrase-based MT. in Proceedings of the 14th Con-
ference of the European Chapter of the Association
for Computational Linguistics. Gothenburg, Swe-
den.

Ulf Hermjakob, Kevin Knight and Hal Daume III.
2008. Name Translation in Statistical Machine
Translation Learning When to Transliterate. in Pro-
ceedings of ACL-08: HLT. Columbus, Ohio, USA.

Sanjika Hewavitharana, Dennis N. Mehay, Sankara-
narayanan Ananthakrishnan and Prem Natarajan.
2013. Incremental Topic-Based Translation Model
Adaptation for Conversational Spoken Language
Translation. in Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics. Sofia, Bulgaria.

Mark Hopkins and Jonathan May. 2011. Tuning as
Ranking. in Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing. Edinburgh, UK.

Abraham Ittycheriah and Salim Roukos. 2007. Direct
Translation Model 2. in Proceedings of NAACL HLT
2007. Rochester, NY.

Ferhan Ture, Douglas W. Oard and Philip Resnik.
2012. Encouraging Consistent Translation Choices.
in Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies.
Montral, Canada.

Billy T. M. Wong and Chunyu Kit. 2012. Extending
Machine Translation Evaluation Metrics with Lexi-
cal Cohesion To Document Level. in Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning. Jeju Island, Korea.

Tong Xiao, Jingbo Zhu, Shujie Yao, and Hao Zhang.
2011. Document-level Consistency Verification in
Machine Translation. in Machine Translation Sum-
mit XIII. Xiamen, China.

Deyi Xiong and Min Zhang. 2013. A Topic-Based Co-
herence Model for Statistical Machine Translation.
in Proceedings of the Twenty-Seventh AAAI Confer-
ence on Artificial Intelligence. Bellevue, USA.

Deyi Xiong, Ding Yang, Min Zhang and Chew Lim
Tan. 2013. Lexical Chain Based Cohesion Mod-
els for Document-Level Statistical Machine Trans-
lation. in Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Process-
ing. Seattle, USA.

157


