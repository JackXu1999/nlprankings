



















































ExCL: Extractive Clip Localization Using Natural Language Descriptions


Proceedings of NAACL-HLT 2019, pages 1984–1990
Minneapolis, Minnesota, June 2 - June 7, 2019. c©2019 Association for Computational Linguistics

1984

ExCL: Extractive Clip Localization Using Natural Language Descriptions

Soham Ghosh1,∗ Anuva Agarwal1,∗ Zarana Parekh1,∗ Alexander Hauptmann1

1Language Technologies Institute
Carnegie Mellon University

{sohamg,anuvaa,zpp,alex}@cs.cmu.edu

Abstract

The task of retrieving clips within videos
based on a given natural language query re-
quires cross-modal reasoning over multiple
frames. Prior approaches such as sliding win-
dow classifiers are inefficient, while text-clip
similarity driven ranking-based approaches
such as segment proposal networks are far
more complicated. In order to select the most
relevant video clip corresponding to the given
text description, we propose a novel extrac-
tive approach that predicts the start and end
frames by leveraging cross-modal interactions
between the text and video - this removes the
need to retrieve and re-rank multiple proposal
segments. Using recurrent networks we en-
code the two modalities into a joint representa-
tion which is then used in different variants of
start-end frame predictor networks. Through
extensive experimentation and ablative analy-
sis, we demonstrate that our simple and ele-
gant approach significantly outperforms state
of the art on two datasets and has comparable
performance on a third.

1 Introduction

Clip Localization is the task of selecting the rel-
evant span of temporal frames in a video corre-
sponding to a natural language description and has
recently piqued interest in research that lies at the
intersection of visual and textual modalities. An
example of this task is demonstrated in Figure 1.
It requires cross-modal reasoning to ground free-
form text inside the video and calls for models ca-
pable of segmenting a video into action segments
(Singh et al., 2016; Yeung et al., 2016; Xu et al.,
2017) as well as measuring multi-modal semantic
similarity (Karpathy and Fei-Fei, 2015).

This task is inherently discriminative, i.e., there
is only a single most relevant clip pertaining to

∗Equal contribution, randomly ordered.

1.03 5.61

1.0 4.0 

Ground Truth

Predicted

Figure 1: Clip Extraction task for the given query ‘the
biker jumps to another ramp near the camera’

a given query in the corresponding video. How-
ever, most prior works (Hendricks et al., 2017,
2018; Liu et al., 2018; Chen et al., 2018; Zhang
et al., 2018) explore this as a ranking task over a
fixed number of moments by uniformly sampling
clips within a video. Moreover, these approaches
are restrictive in scope since they use predefined
clips as candidates for a video and cannot be eas-
ily extended to videos with considerable variance
in length.

Gao et al. (2017); Xu et al. (2019) apply two-
stage methods which rank candidate clips using a
learned similarity metric. Gao et al. (2017); Ge
et al. (2019) propose a sliding window approach
with alignment and offset regression learning ob-
jective, but it is limited by the coarseness of the
windows and is thus inefficient and inflexible. Xu
et al. (2019) address this through a query-guided
segment proposal network (QSPN). However, the
similarity metric used by these approaches is diffi-
cult to learn as it is sensitive to the choice of neg-
ative samples (Yu et al., 2018) and it still does not
consider the discriminative nature of the task.

Hence, we propose an elegant and fairly simple
extractive approach. Our technique is similar to
text-based Machine Comprehension (Chen et al.,
2017) but in a multimodal setting where the video
is analogous to the text passage and the target-
clip is analogous to the text span corresponding
to the correct answer. We verify empirically that



1985

our method significantly outperforms prior work
on two benchmark datasets - TACoS, ActivityNet
and comparably well on the third, Charades-STA.
Our flexible, modular approach to Extractive Clip
Localization (ExCL) can easily be extended to in-
corporate attention models and different variants
of encoders for both visual and text modality to
improve performance further.

2 Approach

Our model comprises of three modular parts - a
text encoder, a video encoder and a span pre-
dictor as shown in Figure 2. We use a bidirec-
tional text LSTM as the text encoder with pre-
trained GloVE (Pennington et al., 2014) embed-
dings as input, and use the last hidden state (hT )
as sentence-embedding. Simpler variants such as
bag-of-words, and other approaches such as In-
ferSent (Conneau et al., 2017) or Skip-Thought
(Kiros et al., 2015) could also be used. We use
a bidirectional LSTM with I3D features (Carreira
and Zisserman, 2017) as the video encoder which
captures temporal context at each time-step (hVt ).
Note that this could also be substituted by C3D
features (Tran et al., 2014) or self-attentive net-
works (Kim et al., 2018). Finally, we compare the
variants of span-predictor networks which output
scores Sstart(t), Send(t) ∈ R1 respectively.

2.1 Training objectives

We consider two modes of training our networks,
one which uses a classification loss (ExCL-clf)
and another which uses a regression loss (ExCL-
reg).

ExCL-clf: The scores are normalized using
SoftMax to give Pstart(t), Pend(t), and trained us-
ing negative log-likelihood loss:

L(θ) = − 1
N

N∑
i

log(Pstart(t
i
s)) + log(Pend(t

i
e))

(1)

where N is number of text-clip pairs and tis, t
i
e are

the ground-truth start and end frame indices for the
ith pair. During inference we predict the span of
frames (t̂s, t̂e) for each query by maximizing the

joint probability of start and end frames.

span(t̂s, t̂e) = argmax
t̂s,t̂e

Pstart(t̂s)Pend(t̂e) (2)

= argmax
t̂s,t̂e

Sstart(t̂s) + Send(t̂e) (3)

s.t. t̂s ≤ t̂e (4)

ExCL-reg: The classification approach is lim-
ited to predicting start and end times of clip at dis-
cretized intervals, while the true target is a con-
tinuous value. In order to directly model this as
a regression problem, we formulate start and end
time prediction by computing an expectation over
the probability distribution given by SoftMax out-
puts:

ts = EPstart
[
t
]

(5)

=

T∑
ts=1

tsPstart(ts) (6)

te = EPstart [EPend|start [t]] (7)

=
T∑

ts=1

Pstart(ts)
T∑

te=1

Pend|start(te) (8)

Here the above equations ts, te refer to the ac-
tual time values corresponding to each index.
Pend|start(te) is computed by a SoftMax over
masked logits:

Pend|start = SoftMax(1[te ≥ ts]Send(t))

Finally we train the networks using regression
losses such as mean squared error and absolute er-
ror. We find that using absolute error and first nor-
malizing the values of time to ts, te ∈ [0, 1] yields
better results.

2.2 Span Predictor Variants

We implement the following variants of the span
predictor network:

MLP predictor: At each time step t, we pass
concatenated video-encoder features with sen-
tence embeddings into two multi-layered percep-
trons (MLPs) to obtain scores Sstart(t), Send(t).

Sstart(t) = MLPstart([h
V
t ;h

T ]) (9)

Send(t) = MLPend([h
V
t ;h

T ]) (10)



1986

   

   

   

   

 

 

 

 

 

 

  

1

2

3

(a) MLP predictor

   

   

   

   

 

 

   

 

 

1

2

3

(b) Tied-LSTM predictor

   

   

   

   

   

  

  

   

1

2

3

(c) Conditioned-LSTM predictor

Figure 2: Our model consists of three modules: a text sentence encoder (orange, denoted by [1]), a video en-
coder (blue, denoted by [2]) and three variants of span predictor (green, denoted by [3]) - MLP, Tied-LSTM and
Conditioned-LSTM to predict start and end probabilities for each frame. We use bidirectional LSTMs for the text
and video encoders.

Tied LSTM predictor: Here, we send concate-
nated video encoder output and sentence embed-
ding to a bidirectional LSTM as input at each step
in order to capture recurrent cross-modal interac-
tions. The hidden states (hPt ) concatenated with
the original inputs are then fed to a MLP with tanh
activation in hidden layers to predict start and end
scores.

hPt = LSTM([h
V
t ;h

T ],hPt−1) (11)

Sstart(t) = MLPstart([h
P
t ;h

V
t ;h

T ]) (12)

Send(t) = MLPend([h
P
t ;h

V
t ;h

T ]) (13)

Conditioned LSTM predictor: Note that in the
previous two approaches the end-frame predictor
is not conditioned in any way on the start predictor.
Here we use two bidirectional LSTMs: LSTMend
takes as input the hidden states hP0t of LSTMstart
and produces hP1t as output. The respective hidden
states are then used in a similar way as the tied
LSTM method to generate start and end scores.

hP0t = LSTMstart([h
V
t ;h

T ],hP0t−1) (14)

hP1t = LSTMend(h
P0
t ,h

P1
t−1) (15)

Sstart(t) = Ws([h
P0
t ;h

V
t ;h

T ]) + bs (16)

Send(t) = We([h
P1
t ;h

V
t ;h

T ]) + be (17)

3 Datasets

We evaluate our models on three datasets.
Note that we do not evaluate our models on

DiDeMo/TEMPO (Hendricks et al., 2017, 2018)
as these datasets only provide coarse fixed-size
moments, thus reducing the problem essentially to
ranking a fixed set of candidates. We choose the
following datasets because each has unique prop-
erties such as visual variance, richness of vocabu-
lary and variance in query lengths.

MPII TACoS: This dataset (Rohrbach et al.,
2014) has been built on top of the MPII Cooking
Activities dataset. It consists of detailed tempo-
rally aligned text descriptions of cooking activi-
ties. The average length of videos is 5 minutes. A
significant challenge in TACoS dataset is that de-
scriptions span over only a few seconds because
of the atomic nature of queries such as ‘takes out
the knife’ and ‘chops the onion’ (8.4% of them are
less than 1.6s long). Such short queries allow a
smaller margin of error. We use the train/test split
as provided by Gao et al. (2017) here. Coupled
with lesser visual variance to distinguish activi-
ties, fine-grained actions and descriptions which
can often have high word overlap, this is a chal-
lenging dataset.

ActivityNet Captions: ActivityNet (Caba Heil-
bron et al., 2015) is a large-scale open domain
activity recognition, segmentation and prediction
dataset based on YouTube videos additionally aug-
mented with dense temporally annotated captions
(Krishna et al., 2017). The average length is
2 minutes, but they are much more diverse in
content, with videos that span over 200 activity

https://drive.google.com/file/d/1HF-hNFPvLrHwI5O7YvYKZWTeTxC5Mg1K/view?usp=sharing


1987

classes and annotations which use a richer vocab-
ulary. There are 10,024 and 5,044 train and test
set videos. We use the same train/test split as pro-
vided by the authors. Our reported results have
3,370 missing videos which could not be down-
loaded.

Charades-STA: The Charades dataset was in-
troduced with action classification, localization
and video description annotations (Sigurdsson
et al., 2016). Gao et al. (2017) extend this to in-
clude sentence level temporal annotation to create
the Charades-STA dataset which contains 12,408
training, and 3,720 test sentence level annotations.
In Charades-STA, average length of videos is 30
seconds and query length (number of frames) has
much lower variance (3.7s) as compared to TACoS
(39.5s) and ActivityNet (78.1s).

4 Experiments

4.1 Feature Extraction

We downsample the videos at a frame rate of 5
frames per second and extract I3D RGB (Carreira
and Zisserman, 2017) visual features pretrained on
Kinetics dataset. We use a fine-tuned version for
Charades available here.

4.2 Implementation Details

We use pre-trained GloVE embeddings of 300 di-
mensions. The extracted visual features have 1024
dimensions. For TACoS, Charades-STA, and Ac-
tivityNet we use a vocabulary size of 1438, 3720
and 10,000 respectively. We considered batch
sizes of 16, 32, 64 and single-layer LSTM hid-
den sizes of 128, 256, 512. The video and span
predictors are bidirectional LSTMs with 256 and
128 hidden units respectively while queries are en-
coded by a 256-dimensional bidirectional LSTM.
The MLP based span predictors have 256 hidden
dimensions and use tanh activation in hidden lay-
ers. For all datasets, we train the model with a
batch size of 32 for 30 epochs using Adam opti-
mizer with a learning rate of 0.001 and early stop-
ping. We apply a dropout of 0.5 to all the above-
mentioned LSTMs during training. We measure
our model performance primarily using localiza-
tion accuracy which is defined to be intersection
over union (IoU) at threshold values of 0.3, 0.5,
0.7 to compare with past work which reports Re-
call@1, IoU={0.3, 0.5, 0.7}.

4.3 Experimentation and Ablative Analysis

We compare the different training objectives (la-
beled ExCL-clf for classification and ExCL-reg
for regression) and evaluate the usefulness of re-
current encoders for video representations by re-
moving the video LSTM (labeled ExCL-clf/reg 1-
{a, b, c}). We also perform ablative analysis of
a range of span predictor networks. We compare
the performance of our proposed model with three
baselines which are the current SOTA for the dif-
ferent datsets. (i) Activity Concepts based Lo-
calizer (ACL) proposed in Ge et al. (2019) for
TACoS (ii) Segment Proposal Network approach
proposed in (Xu et al., 2019) for ActivityNet and
(iii) Moment Alignment Network (MAN) via It-
erative Graph Adjustment approach proposed in
(Zhang et al., 2018) for Charades-STA. While we
significantly beat the first two baselines for TACoS
and Activity Net respectively, we attain compara-
ble performance with the third for Charades-STA.
It is to be noted that since the approach in (Zhang
et al., 2018) depends on ranking fixed number of
segments in each video, it is not scalable to the
other two datasets which have longer videos with
greater variance in their lengths.

4.4 Results

Our results in Table 1 confirm our hypothesis that
extractive models work better.

We find that across all datasets, models with-
out recurrent architectures (ExCL 1-a) perform
significantly worse, demonstrating the importance
of temporal context provided by LSTMs in ei-
ther video encoder (ExCL 2-a) or span predictors
(ExCL 1-b). With ExCL 2-a we see that if the
visual context is captured well using recurrent ar-
chitectures in the video LSTM, then even with a
MLP span predictor we get a significant improve-
ment in performance. Furthermore, if we add
LSTM span predictors along with the video LSTM
(ExCL 2-{b, c}) we obtain an additional boost in
performance. However, without a recurrent vi-
sual encoder, a recurrent span predictor is essen-
tial to capture both uni-modal and cross-modal
interactions (ExCL 1-{b, c}). When comparing
the regression learning objective with classifica-
tion, we do not notice a substantial gain in per-
formance thereby indicating that not much infor-
mation is lost if the continuous nature of the labels
is not considered. In terms of span predictors, tied
LSTM generally performs well across all datasets,

https://github.com/piergiaj/pytorch-i3d/blob/master/models/rgb_charades.pt


1988

TACoS Charades-STA ActivityNet
IoU 0.3 0.5 0.7 0.3 0.5 0.7 0.3 0.5 0.7
Ge et al. (2019) 24.2 20.0 – – 30.5 12.2 – – –
Xu et al. (2019) – – – 54.7 35.6 15.8 45.3 27.7 13.6
Zhang et al. (2018) – – – – 46.5 22.7 – – –
ExCL-clf 1-a 22.6 12.6 5.1 55.4 30.4 14.8 42.5 23.8 12.1
ExCL-clf 1-b 42.0 25.0 12.3 64.7 43.8 22.1 61.7 40.4 23.0
ExCL-clf 1-c 41.9 25.5 13.6 64.2 43.9 23.3 60.7 40.9 23.4
ExCL-clf 2-a 41.7 26.0 12.9 64.6 41.5 20.3 60.4 40.5 23.1
ExCL-clf 2-b 44.2 28.0 14.6 65.1 44.1 22.6 61.1 41.3 23.4
ExCL-clf 2-c 44.4 27.8 14.6 61.4 41.2 21.3 62.1 41.6 23.9
ExCL-reg 1-a 26.2 11.9 4.8 54.7 34.0 14.5 48.4 27.0 11.0
ExCL-reg 1-b 45.2 27.5 12.9 60.1 42.6 21.6 63.0 43.6 23.6
ExCL-reg 1-c 41.4 24.8 11.4 59.0 43.1 20.7 61.5 42.7 23.4
ExCL-reg 2-a 42.2 27.2 11.7 59.6 41.9 20.2 61.5 41.9 23.3
ExCL-reg 2-b 45.5 28.0 13.8 61.5 44.1 22.4 62.3 42.7 24.1
ExCL-reg 2-c 42.3 27.3 12.5 58.0 41.8 20.5 61.4 41.7 22.4

Table 1: Clip Localization Accuracy at IoU = {0.3, 0.5, 0.7} for TACoS, Charades-STA and ActivityNet. Here
ExCL-clf represents the classification loss model and ExCL-reg represents the regression loss model. ExCL-
{clf/reg} 1-m refer to models run without a video LSTM encoder, while ExCL-{clf/reg} 2-m include the video
LSTM. m = a, b, c refer to MLP, tied LSTM and conditioned LSTM span predictor networks respectively.

and any difference with conditioned LSTM is neg-
ligible. This benefit is more pronounced when us-
ing the regression objective, possibly because con-
ditioning is already captured in the formulation as
given by Equation 7.

While Xu et al. (2019) note a significant differ-
ence in performance between Charades-STA and
ActivityNet captions, performance is similar on
both the datasets in our model. We hypothesize
that their model fails to work well when there is
large variability in the query lengths, as explained
in Section 3. We also find TACoS to be a signifi-
cantly more challenging benchmark, similar to Ge
et al. (2019).

5 Conclusion

In conclusion, our main contribution is an ex-
tractive model for clip localization based on text
queries as opposed to ranking driven approaches
used in the past. Our results show that this ele-
gant and fairly simple approach works much bet-
ter empirically on three very different benchmark
datasets, with tied LSTM span predictor gener-
ally giving best results. It is to be noted that
these datasets previously had three different archi-
tectures as their respective SOTA, and our work
naturally lays the foundation for training a sin-
gle generalizable model across datasets and pos-

sibly related tasks as the next step. Furthermore,
our approach is modular, making it trivial to insert
different architectures for the encoders and span-
predictors. Other future directions of this work in-
clude adding temporal attention in order to handle
more complicated temporal references and extend-
ing this approach to work for longer, and thereby
more challenging videos such as movies.

Acknowledgements

The authors would also like to thank anonymous
reviewers for their comments and Junwei Liang
for helpful discussions. This research was sup-
ported in part by DARPA grant FA8750-18-2-
0018 funded under the AIDA program. This work
is also supported in part through the financial as-
sistance award 60NANB17D156 from U.S. De-
partment of Commerce, National Institute of Stan-
dards and Technology and by the Intelligence Ad-
vanced Research Projects Activity (IARPA) via
Department of Interior/Interior Business Center
(DOI/IBC) contract number D17PC00340.

References
Fabian Caba Heilbron, Victor Escorcia, Bernard

Ghanem, and Juan Carlos Niebles. 2015. Activ-
itynet: A large-scale video benchmark for human
activity understanding. In Proceedings of the IEEE



1989

Conference on Computer Vision and Pattern Recog-
nition, pages 961–970.

Joao Carreira and Andrew Zisserman. 2017. Quo
vadis, action recognition? a new model and the
kinetics dataset. In Computer Vision and Pattern
Recognition (CVPR), 2017 IEEE Conference on,
pages 4724–4733. IEEE.

Danqi Chen, Adam Fisch, Jason Weston, and An-
toine Bordes. 2017. Reading wikipedia to an-
swer open-domain questions. arXiv preprint
arXiv:1704.00051.

Jingyuan Chen, Xinpeng Chen, Lin Ma, Zequn Jie, and
Tat-Seng Chua. 2018. Temporally grounding natural
sentence in video. In Proceedings of the 2018 Con-
ference on Empirical Methods in Natural Language
Processing, pages 162–171.

Alexis Conneau, Douwe Kiela, Holger Schwenk, Loı̈c
Barrault, and Antoine Bordes. 2017. Supervised
learning of universal sentence representations from
natural language inference data. In Proceedings of
the 2017 Conference on Empirical Methods in Nat-
ural Language Processing, pages 670–680, Copen-
hagen, Denmark. Association for Computational
Linguistics.

Jiyang Gao, Chen Sun, Zhenheng Yang, and Ram
Nevatia. 2017. Tall: Temporal activity lo-
calization via language query. arXiv preprint
arXiv:1705.02101.

Runzhou Ge, Jiyang Gao, Kan Chen, and Ram Nevatia.
2019. Mac: Mining activity concepts for language-
based temporal localization. In 2019 IEEE Win-
ter Conference on Applications of Computer Vision
(WACV), pages 245–253. IEEE.

Lisa Anne Hendricks, Oliver Wang, Eli Shechtman,
Josef Sivic, Trevor Darrell, and Bryan Russell. 2017.
Localizing moments in video with natural language.
In Proceedings of the IEEE International Confer-
ence on Computer Vision (ICCV), pages 5803–5812.

Lisa Anne Hendricks, Oliver Wang, Eli Shechtman,
Josef Sivic, Trevor Darrell, and Bryan Russell. 2018.
Localizing moments in video with temporal lan-
guage. arXiv preprint arXiv:1809.01337.

Andrej Karpathy and Li Fei-Fei. 2015. Deep visual-
semantic alignments for generating image descrip-
tions. In Proceedings of the IEEE conference
on computer vision and pattern recognition, pages
3128–3137.

Kyung-Min Kim, Seong-Ho Choi, Jin-Hwa Kim, and
Byoung-Tak Zhang. 2018. Multimodal dual atten-
tion memory for video story question answering.
Lecture Notes in Computer Science, page 698713.

Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,
Richard Zemel, Raquel Urtasun, Antonio Torralba,
and Sanja Fidler. 2015. Skip-thought vectors. In
Advances in neural information processing systems,
pages 3294–3302.

Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei,
and Juan Carlos Niebles. 2017. Dense-captioning
events in videos. In ICCV, pages 706–715.

Bingbin Liu, Serena Yeung, Edward Chou, De-An
Huang, Li Fei-Fei, and Juan Carlos Niebles. 2018.
Temporal modular networks for retrieving complex
compositional activities in videos. In European
Conference on Computer Vision, pages 569–586.
Springer.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. In Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1532–
1543.

Anna Rohrbach, Marcus Rohrbach, Wei Qiu, An-
nemarie Friedrich, Manfred Pinkal, and Bernt
Schiele. 2014. Coherent multi-sentence video de-
scription with variable level of detail. In German
conference on pattern recognition, pages 184–195.
Springer.

Gunnar A Sigurdsson, Gül Varol, Xiaolong Wang, Ali
Farhadi, Ivan Laptev, and Abhinav Gupta. 2016.
Hollywood in homes: Crowdsourcing data collec-
tion for activity understanding. In European Confer-
ence on Computer Vision, pages 510–526. Springer.

Bharat Singh, Tim K Marks, Michael Jones, Oncel
Tuzel, and Ming Shao. 2016. A multi-stream bi-
directional recurrent neural network for fine-grained
action detection. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recogni-
tion, pages 1961–1970.

Du Tran, Lubomir D. Bourdev, Rob Fergus, Lorenzo
Torresani, and Manohar Paluri. 2014. C3D: generic
features for video analysis. CoRR, abs/1412.0767.

Huijuan Xu, Abir Das, and Kate Saenko. 2017. R-c3d:
region convolutional 3d network for temporal activ-
ity detection. In IEEE Int. Conf. on Computer Vision
(ICCV), pages 5794–5803.

Huijuan Xu, Kun He, L Sigal, S Sclaroff, and
K Saenko. 2019. Multilevel language and vision
integration for text-to-clip retrieval. In AAAI, vol-
ume 2, page 7.

Serena Yeung, Olga Russakovsky, Greg Mori, and
Li Fei-Fei. 2016. End-to-end learning of action de-
tection from frame glimpses in videos. In Proceed-
ings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 2678–2687.

Baosheng Yu, Tongliang Liu, Mingming Gong,
Changxing Ding, and Dacheng Tao. 2018. Correct-
ing the triplet selection bias for triplet loss. In Pro-
ceedings of the European Conference on Computer
Vision (ECCV), pages 71–87.

https://www.aclweb.org/anthology/D17-1070
https://www.aclweb.org/anthology/D17-1070
https://www.aclweb.org/anthology/D17-1070
https://doi.org/10.1007/978-3-030-01267-0_41
https://doi.org/10.1007/978-3-030-01267-0_41
http://www.aclweb.org/anthology/D14-1162
http://www.aclweb.org/anthology/D14-1162
http://arxiv.org/abs/1412.0767
http://arxiv.org/abs/1412.0767


1990

Da Zhang, Xiyang Dai, Xin Wang, Yuan-Fang Wang,
and Larry S Davis. 2018. Man: Moment align-
ment network for natural language moment re-
trieval via iterative graph adjustment. arXiv preprint
arXiv:1812.00087.


