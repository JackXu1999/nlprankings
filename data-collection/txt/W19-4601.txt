



















































Incremental Domain Adaptation for Neural Machine Translation in Low-Resource Settings


Proceedings of the Fourth Arabic Natural Language Processing Workshop, pages 1–10
Florence, Italy, August 1, 2019. c©2019 Association for Computational Linguistics

1

Incremental Domain Adaptation for Neural Machine Translation in
Low-Resource Settings

Marimuthu Kalimuthu Michael Barz Daniel Sonntag
German Research Center for Artificial Intelligence (DFKI),

Saarland Informatics Campus D3 2, 66123 Saarbrücken
{marimuthu.kalimuthu,michael.barz,daniel.sonntag}@dfki.de

Abstract

We study the problem of incremental domain
adaptation of a generic neural machine trans-
lation model with limited resources (e.g., bud-
get and time) for human translations or model
training. In this paper, we propose a novel
query strategy for selecting “unlabeled” sam-
ples from a new domain based on sentence em-
beddings for Arabic. We accelerate the fine-
tuning process of the generic model to the tar-
get domain. Specifically, our approach esti-
mates the informativeness of instances from
the target domain by comparing the distance
of their sentence embeddings to embeddings
from the generic domain. We perform ma-
chine translation experiments (Ar-to-En direc-
tion) for comparing a random sampling base-
line with our new approach, similar to active
learning, using two small update sets for sim-
ulating the work of human translators. For the
prescribed setting we can save more than 50%
of the annotation costs without loss in qual-
ity, demonstrating the effectiveness of our ap-
proach.

1 Introduction

Neural Machine Translation (NMT) is the task
of translating text from one language (source)
to another (target) using, most commonly, Re-
current Neural Networks (RNN), specifically the
Encoder-Decoder or Sequence-to-Sequence mod-
els (Sutskever et al., 2014; Cho et al., 2014). Re-
cently, NMT has become a quite popular and
effective alternative to traditional Phrase-Based
Statistical Machine Translation (PBSMT) (Koehn
et al., 2003). Major problems that arise include
very high cost of training NMT models for new
domains and that abundant parallel corpora are re-
quired for this task: the standard encoder-decoder
models with attention have been shown to per-
form poorly in low-resource settings (Koehn and
Knowles, 2017). Sufficient data might not be

available for all languages due to resource restric-
tions, particularly for resource-poor languages.
Hence, we are in need of cost-effective adaptation
techniques that transfer existing knowledge to new
domains as much as possible.

A recently proposed approach for domain adap-
tation filters generic corpora based on sentence
embeddings of a potentially low amount of in-
domain samples to train domain-specific models
from scratch (Wang et al., 2017). However, the
problem of time- and resource-consuming training
still remains which is unsuitable for incremental
model updates.
Fine-tuning can accelerate the training process be-
cause it transfers knowledge from a pre-trained
generic model to a new domain and, hence, re-
quires less parallel training samples. However, re-
spective differences in contents and writing style
can reduce machine translation quality, if they are
not properly addressed.

Recent approaches include fine-tuning with
mixed batches containing in- and out-of-domain
samples (Chu et al., 2017) and with different regu-
larization methods for differing amounts of new
samples for English → German and English →
Russian (Barone et al., 2017). The findings of
Barone et al. (2017) suggest that there is an “ap-
proximately logarithmic relation between the size
of in-domain training set and improvement in
BLEU score”. We want to find out whether in-
cremental model training can be accelerated using
an advanced query strategy for sample selection.
Previous works on incremental machine transla-
tion include cache-based computer aided trans-
lation tools (Nepveu et al., 2004), active learn-
ing techniques for interactive statistical machine
translation (González-Rubio et al., 2012), interac-
tive visualizations for understanding and manipu-
lating attention weights and beam search parame-
ters in NMT (Lee et al., 2017), and domain adap-



2

tation through user interactions (Peris and Casacu-
berta, 2018).

In this work, we implement a new query strat-
egy for selecting “unlabeled” instances from a tar-
get domain and investigate its effect on fine-tuning
a generic NMT model. We borrow techniques and
terms from the active learning domain (Settles,
2010): a query strategy is a method for selecting
instances from a pool of unlabeled data that lead
to a high information gain when used for training
the machine learning model under consideration.
Selected instances are labeled by an oracle which
can be a human. Iteratively including the most in-
formative instances, labeled on demand, has been
shown to increase the model performance while
using the same amount of training data. Our pro-
posed methods for domain adaptation in NMT in-
clude query strategies that consider untranslated
sentences as unlabeled instances. We simulate a
human oracle by using parallel corpora in the eval-
uation, but we do not consider incremental updates
for the query strategy. This is of interest for crowd-
based domain-adaptation with limited resources as
described in (Barz et al., 2018b), in particular, be-
cause our method only requires monolingual data
for filtering (see Figure 1).

We compare random sampling as a naı̈ve base-
line strategy with our novel method based on dis-
tances between sentence embeddings. We esti-
mate the informativeness of instances from the tar-
get domain by comparing the distances of their
sentence embeddings to the embeddings of the
generic domain. For computing the sentence em-
beddings, we present AraSIF: we adapted the
methodology presented by Arora et al. (2017),
which is known to capture the semantics of sen-
tences well, to work with Arabic. In our experi-
ments, we use existing parallel corpora for simu-
lating human workers: The MEDAR1 and Glob-
alVoices dataset (Tiedemann, 2012) are consid-
ered as new target domains which mainly con-
cern the domain of climate change and politics,
respectively. The LDC Newswire parallel corpus
is used as the dataset for training generic domain
model. We fine-tune this generic NMT model us-
ing different amounts of samples from a new do-
main and varying training epoch settings while ob-
serving the BLEU score (Papineni et al., 2002) on
a held-out in-domain test set. Our hypothesis is
that the proposed novel query strategy can effec-

1http://medar.info

tively reduce the number of fine-tuning samples
required without hampering the translation quality
when compared to the baseline.

The remainder of this paper is organized as fol-
lows: Section 2 provides an overview on related
works, section 3 describes the NMT system and
considered query strategies. In section 4, we de-
scribe our experiment, and we report the results in
section 5. The results are discussed in section 6
and we conclude our work in section 7.

2 Related Work

Almahairi et al. (2016) presented their first result
on AR–EN bidirectional NMT, showing that NMT
models outperform traditional PBSMT models
when they are tested on out-of-domain test data.
This result motivates us to study domain adapta-
tion of NMT models rather than PBSMT models.

Several approaches are proposed for domain
adaptation in the context of statistical and neural
machine translation. Wang et al. (2017) show a
way to adapt existing corpora to new domains us-
ing learned sentence embeddings for the source
language of an NMT model to identify train-
ing samples that are close to the new domain.
This method allows us to train NMT models for
new domains without requiring a parallel corpus
in that domain, but models need to be trained
from scratch. Chu et al. (2017) present a method
called “mixed fine-tuning” where fine-tuning is
performed with mini-batches composed of a mix
of in- and out-of-domain parallel samples to ad-
dress the problem of overfitting to the new do-
main. Barone et al. (2017) investigate regular-
ization methods for domain adaptation in NMT.
Their findings indicate that BLEU scores increase
logarithmically with an increasing amount of in-
domain training data. Peris and Casacuberta
(2018) implement an online domain adaptation
method based on user interactions on the sub-word
level. In an experiment, simulating such inter-
actions with available public corpora, they could
show that their online learning approach success-
fully improves word error rates for EN-to-DE and
EN-to-FR translations.

González-Rubio et al. (2012) present different
active learning techniques that shall reduce human
workload in interactive statistical machine transla-
tion. They consider three query strategies for se-
lecting the most informative sentences for being
translated by humans: a random sampling base-



3

generic, parallel domain-specific; monolingual

generic corpus (𝑆𝑠𝑟𝑐
𝑔𝑒𝑛

, 𝑆𝑑𝑠𝑡
𝑔𝑒𝑛

) new domain corpus 𝑆𝑠𝑟𝑐
𝑛𝑒𝑤

𝑠𝑠𝑟𝑐
0

𝑠𝑠𝑟𝑐
𝑛

𝑠𝑑𝑠𝑡
0

𝑠𝑑𝑠𝑡
𝑛

𝑠𝑠𝑟𝑐
0

𝑠𝑠𝑟𝑐
𝑚

𝑠𝑑𝑠𝑡
0

𝑑𝑠𝑡𝑑𝑠𝑡
𝑚

… … … …

𝑀𝑔𝑒𝑛 𝑀𝑛𝑒𝑤

fine-tuningNMT
training

domain adaption

𝑓(𝑆𝑠𝑟𝑐
𝑛𝑒𝑤)

𝑠𝑠𝑟𝑐
0

𝑠𝑠𝑟𝑐
𝑚

𝑠𝑑𝑠𝑡
0

𝑠𝑑𝑠𝑡
𝒎−𝒋

… …

𝑠𝑠𝑟𝑐
0

𝑠𝑠𝑟𝑐
𝒎−𝒋

…

𝑓

𝑓

requires human labor which is 
expensive & time-consuming

𝑓 Filter samples whose distance to the generic 
domain corpus is small (sentence embeddings)

𝑀𝑛𝑒𝑤
𝑓

fine-tuning

domain-specific; monolingual

Figure 1: We focus on domain adaptation of a generic NMT model Mgen with humans-in-the-loop that translate
monolingual data of the new domain with limited resources. We simulate crowd-translated content using two
parallel corpora Snew representing data of new domains for training the adapted model Mnew. We propose an
advanced query strategy for selecting sentences from Snew that need to be translated by their similarity to the
generic corpus.

line, rare n-gram sampling, and a sampling based
on word confidences. In a recent work, Lam et al.
(2018) suggest to incorporate human judgments
on partial translations as reinforcement signal for
improving NMT models and evaluate it in a sim-
ulation experiment with existing parallel corpora.
For reducing human workload, they suggest an
entropy-based method to trigger human judgments
similar to active learning approaches with human
oracles.

We focus on a query strategy for domain adap-
tation of NMT models based on active learning.
We consider settings in which human workers pro-
vide new training data (Barz et al., 2018b,a; Green
et al., 2015) for domains with no or only little
parallel corpora due to, for instance, budget con-
straints. Our experiment includes random sam-
pling as a baseline similar to González-Rubio et al.
(2012) and an advanced sentence selection strat-
egy based on distances between sentence embed-
dings that also encode the semantics of a sentence
(Arora et al., 2017), adapted for Arabic.

3 Method

We implement a baseline query strategy (random
sampling) and an advanced query strategy (see
3.3) for selecting training samples which are used
for fine-tuning a generic NMT model. In this sec-
tion, we describe the applied NMT model and the
generic training process, as well as the two query
strategies used in the domain adaptation process.

3.1 Model Architecture and Training

We use the TensorFlow implementation of NMT2

(Luong et al., 2017) configured as an 8-layered
bidirectional RNN with standard LSTM cells in
each layer and residual connections between the
layers. We use the same architecture for both,
generic model training and fine-tuning tasks. The
model is trained3 with vanilla SGD for 350k itera-
tions with a batch size of 50 and a dropout rate of
0.2. The initial learning rate is set to 1.0 and a de-
cay factor of 0.5 is applied after every 1k iterations
starting from 170k iterations. We use the standard
hyperparameters provided in the NMT framework
and set the vocabulary size to 32k for both Arabic
and English. We train the generic model (Mgen)
for one week using the LDC corpus (Sgen) (see
Figure 1) and use the resulting checkpoint for all
of our fine-tuning experiments.

3.2 Datasets and Preprocessing

In our experiment, we use the LDC Newswire
corpus (Munteanu and Marcu, 2005) and two
publicly available datasets, MEDAR and Glob-
alVoices. The corpus statistics are summarized in
Table 1. The LDC Newswire parallel corpus (Ar-
En) is used for training a generic model and the
MEDAR and GlobalVoices datasets for domain-
specific fine-tuning. We include datasets on two

2https://github.com/tensorflow/nmt
3All experiments were performed on an Ubuntu machine

(Intel i7-5960X) with 8 cores and 2 GTX-1080 graphics cards



4

Corpus Sentence Pairs Domain Usage
LDC Newswire 1.3M Generic Generic model training
MEDAR 0.5k Climate Change Domain specific fine-tuning
GlobalVoices 37k Politics, Human Rights Domain specific fine-tuning

Table 1: Details of datasets that we used in our experiments.

1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00
cosine similarity between sentence embeddings of a dataset and eref

0.0

0.2

0.4

0.6

0.8

1.0

1.2

1.4

1.6

de
ns

ity

LDC Newswire
MEDAR
GlobalVoices

Figure 2: Kernel density estimates for the distributions
of distances (cosine similarity) between sentence em-
beddings of each considered dataset and eref , the mean
of sentence embeddings of the dataset used for training
of the generic translation model. The gray dotted lines
represent the 25% and the 75% percentile of the dis-
tance distribution for the generic model.

different genres to investigate whether our find-
ings generalize irrespective of the domain of the
fine-tuning set. To reduce noise in the data, we
clean the datasets by discarding instances with
mixed tokens (i.e. English sentences containing
Arabic words or Arabic sentences containing En-
glish words). This step removes around 0.01%,
1.2%, and 10.14% of sentence pairs from LDC
Newswire, MEDAR, and GlobalVoices datasets
respectively. Further preprocessing steps of our
system pipeline include normalization and tok-
enization4 of the sentences and generation of byte
pair encodings (BPE)5 (Sennrich et al., 2016) for
the tokenized sentences and the vocabulary.

3.3 Query Strategies for Sample Selection
For model adaptation in limited resource settings,
it is desirable to reduce the number of samples
from the target domain and, thus, the required
time and cost for receiving human translations.
Our goal is to develop a query strategy for se-
lecting the most informative update samples, sim-

4https://github.com/moses-smt/mosesdecoder
5https://github.com/rsennrich/subword-nmt

ilar to the active learning paradigm. We propose
a method that estimates the informativeness of a
sample based on its similarity to the generic cor-
pus using sentence embeddings. We exclude se-
mantically overlapping parts from the new corpus
which reduces the amount of training samples that
need to be translated by human labor and that need
to be included in model training (see Figure 1). We
refer to this method as fine-tuning with advanced
sampling. In addition, we implement a baseline
method which selects all samples from a new do-
main in random order (fine-tuning with random
sampling).

For our advanced sampling method, we use
smooth inverse frequency (SIF)-based sentence
embeddings (Arora et al., 2017) extended for Ara-
bic which we refer to as AraSIF (see Section 3.4).
It encodes sentences from the source language
s ∈ Ssrc into a 300-dimensional vector es:

esif : Ssrc → R300, s 7→ es

Arora et al. (2017) show that SIF-based embed-
dings perform well for many semantic textual sim-
ilarity tasks. This implies that the sentences which
are close to each other in the embedding space can
be considered to be semantically similar. We esti-
mate the informativeness of a sample for domain
adaptation based on the semantic similarity of two
sentences. We use the cosine distance d between
two sentence embeddings es and es′ as a proxy for
semantic similarity:

d : R300×2 → [−1, 1], (es, es′) 7→ ds,s′

Hereby, the mean of all sentence embeddings of
the generic corpus Sgensrc ⊂ Ssrc serves as the ref-
erence point eref in the sentence embedding space
for comparing sentences from other corpora:

eref = mean(esif (S
gen
src )), eref ∈ R300

Calculating the cosine similarity between all
samples of a new domain Snewsrc ⊂ Ssrc and this



5

reference point, results in a distribution of dis-
tances indicating the semantic similarity or dis-
similarity of samples from the new domain to the
generic domain. We show the distributions for all
considered datasets in terms of a kernel density es-
timate in Figure 2: MEDAR and GlobalVoices as
new domains and LDC Newswire as generic ref-
erence domain. Initially, we anticipated the tar-
get domain corpora to partially lie outside of the
reference distribution, but the new corpora rather
seem to be more specific subsets of the generic do-
main. Therefore, we select training samples for
our fine-tuning process from the new domains that
belong to the long-tail of the distance distribution
of the generic domain corpus. We expect the in-
formativeness of a new sample s to be high, if it
is underrepresented in the generic domain dataset
in terms of semantic similarity to eref , this is if
ds,eref is high. The interval boundaries that frame
the longtail are the only parameters that need to
be defined for this approach. We use the 25% and
75% percentiles of the distance distribution of the
generic domain to define these outer regions (see
dotted vertical lines in Figure 2).

3.4 AraSIF: Arabic Sentence Embeddings

To obtain sentence embeddings for Arabic sen-
tences we propose AraSIF. We use SIF6 with Ar-
aVec7 (Soliman et al., 2017), a Word2Vec pre-
trained model that is trained on 1.8M Arabic
Wikipedia articles with a total vocabulary size of
662k. SIF is based on word weights for comput-
ing embeddings, for which we consider all tokens
with a frequency count of at least 200. We prepro-
cess the Wikipedia articles on which AraVec was
trained on, for computing the word frequency. In
addition, SIF expects the word embedding to be
in GloVe embedding format. Hence, we convert
the AraVec word embeddings from Word2Vec to
GloVe format. The code for AraSIF is publicly
available at DFKI Interactive Machine Learning
repository on GitHub.

4 Experiment

We conduct a simulation experiment for inves-
tigating the effectiveness of our advanced query
strategy in reducing the required amount of update
samples for adapting an NMT model to a new do-
main. Our approach selects samples using mono-

6https://github.com/PrincetonML/SIF
7https://github.com/bakrianoo/aravec

epochs 1 5 10 20
nt Tsgd Tsgd Tsgd Tsgd
50 1 5 10 20

100 2 10 20 40
150 3 15 30 60
200 4 20 40 80
250 5 25 50 100
300 6 30 60 120
350 7 35 70 140
400 8 40 80 160

Table 2: Considered combinations of update set sizes
(nt) and SGD updates or iterations (Tsgd) used for fine-
tuning.

lingual information only, which can be assumed
to be available without investing resources. For
this, we compare the translation quality when fine-
tuning a model with random sampling and when
fine-tuning it with a reduced number of update
samples resulting from our advanced sampling.
Our generic NMT model Mgen is adapted to two
new domains, represented by the GlobalVoices
and the MEDAR datasets, using both query strate-
gies. We include a varying number of epochs for
identifying good training parameters. We hypoth-
esize that our advanced query strategy for sam-
ple selection can effectively reduce the number of
fine-tuning samples without hampering the trans-
lation quality when compared to the baseline.

5 Evaluation Procedure

We perform the simulation experiment with two
new domain datasets and observe the impact of
different parameters on domain adaptation of the
generic NMT model using small amounts of new
samples. These can be considered to stem from
human workers, e.g., professional translators or
crowdworkers. Considered parameters include the
number of training samples in the update set nt
and the number of training epochs e. The num-
ber of epochs defines the number of training itera-
tions: The number of Stochastic Gradient Descent
(SGD) updates, denoted by Tsgd, is computed by:

Tsgd =
⌈ nt
|S|

⌉
· e

where |S| is the mini-batch size which we set
to 50 throughout our experiments, nt is the num-
ber of sentence pairs in the update set, and e is the
number of epochs. Table 2 provides an overview
of all considered configurations.

https://github.com/DFKI-Interactive-Machine-Learning/AraSIF
https://github.com/DFKI-Interactive-Machine-Learning/AraSIF


6

The update sets used for model adaptation are
generated from either MEDAR or GlobalVoices
dataset, after excluding a static test set of 100
sentences for each. Both update sets are con-
strained to a maximum sample size of 400 to al-
low a fair comparison (this is the maximum size
for MEDAR, see table 1). Further, we assume that
the amount of data from the new domain might be
small due to resource constraints or scarcity.

For the random sampling case, we select all 400
samples from each of the datasets in a random or-
der and use it to adapt our generic model for all
parameter configurations in Table 2. Samples 1
to 50 of the update set are used for training the
nt = 50 model for all epochs. The model fine-
tuning is continued with samples 51 to 100 for
the nt = 100 model for all epochs, and so on.
Considering the stochastic nature of SGD, we re-
peat the experiment 5 times and report the average
scores on the respective test sets, instead of pro-
viding a point estimate. We observe the training
times on the update set and the BLEU scores (Pa-
pineni et al., 2002) on the test set as a dependent
variable.

For our advanced sampling strategy, we select a
subset of all training samples for both datasets us-
ing the filter mechanism described above. We in-
clude a sentence s, if its cosine distance d to eref is
smaller than the 25% percentile (−0.208) or larger
than the 75% percentile (0.317) of the generic dis-
tance distribution (see Figure 2). This leaves us
with 135 fine-tuning samples for MEDAR and 169
for GlobalVoices from the original 400 samples.
We consider the same set of parameters than be-
fore with the difference that the size of the update
set nt is limited to the reduced number of samples.

5.1 Results

In this section, we present the results of our fine-
tuning experiments for adapting the generic model
with different sampling strategies. We use an
increasing number of update samples (nt), dif-
ferent epoch configurations (e) and two new do-
main datasets. The generic baseline model Mgen
achieves a reference BLEU score of 18.6 for
MEDAR and 13.4 for GlobalVoices. We used the
same test set which we have used for evaluating
the fine-tuned model.

Figure 3 summarizes the BLEU scores for all
parameter settings and both new domains con-
cerning the random sampling condition. For

MEDAR, we can observe a monotonic improve-
ment in BLEU score for increasing numbers of
samples nt in the update set for all epoch con-
figurations. However, compared to the reference
score of 18.6, only e = 1 and e = 5 achieve
meaningful improvements: we can observe an im-
provement after fine-tuning with first two mini-
batches. Higher numbers for e (10, 20) result in
lower BLEU scores than the reference, also when
including all samples (nt = 400). Only for e = 10
and nt = [350, 400] we observe a BLEU score
slightly better than the reference model. The best
BLEU score on the MEDAR test set is achieved
using nt = 400 and e = 5 with a score of 19.39.
Averaged over 5 repetitions of the experiment, the
runtime ranges between 59s for nt = 50 and 68s
for nt = 400 for e = 1. All other configurations
require longer training times. For GlobalVoices,
we observe a monotonic improvement with in-
creasing number of samples nt for e ∈ {1, 5}.
Higher numbers for the epoch configuration result
in a monotonic deterioration of BLEU score. In
contrast to the models adapted to MEDAR, train-
ing with e ∈ {10, 20} yields better results than
the reference score of 13.4 for small nt. Yet, due
to the negative trend in BLEU scores, models with
these epoch configurations fall below the reference
score. The best BLEU score on the GlobalVoices
test set is achieved using nt = 100 and e = 10
with 14.36. Averaged over 5 trainings, the run-
time ranges between 62s for nt = 50 and 122s
for nt = 400. For nt ≥ 200, we observe better
BLEU scores than the reference model for e = 1
and e = 5, where the training times for e = 1
grow considerably slower than for e = 5. Here,
the training times range between 49s for nt = 50
and 66s for nt = 400.

Figure 4 summarizes the BLEU scores for all
considered settings and domains for our advanced
sampling condition. For MEDAR, we observe im-
provements in BLEU scores similar to the ran-
dom sampling condition. All epoch configura-
tions, except for e = 1, achieve scores higher than
the reference (18.6) starting from the first update
set. With the advanced sample selection, the best
score of 19.34 is achieved using nt = 135 and
e = 20. Using e = 1 for varying number of sam-
ples (nt) yields BLEU scores which are slightly
lower than the reference BLEU score of 18.6. Av-
eraged over 5 trainings, the runtime ranges be-
tween 54s and 58s for e = 1 and between 59s



7

50 100 150 200 250 300 350 400
nt

17.5

18.0

18.5

19.0

19.5
BL

EU
 s

co
re

1 epoch
5 epochs
10 epochs
20 epochs
Mgen

(a) MEDAR

50 100 150 200 250 300 350 400
nt

12.5

13.0

13.5

14.0

BL
EU

 s
co

re

1 epoch
5 epochs
10 epochs
20 epochs
Mgen

(b) GlobalVoices

Figure 3: BLEU scores of fine-tuned NMT models for MEDAR and GlobalVoices corpora with random sampling
for varying sizes of the update set (nt) and different number of training epochs (e).

50 100 135
nt

18.6

18.7

18.8

18.9

19.0

19.1

19.2

19.3

BL
EU

 s
co

re 1 epoch
5 epochs
10 epochs
20 epochs
Mgen

(a) MEDAR

50 100 150 169
nt

13.0

13.2

13.4

13.6

13.8

14.0

14.2

BL
EU

 s
co

re

1 epoch
5 epochs
10 epochs
20 epochs
Mgen

(b) GlobalVoices

Figure 4: BLEU scores of fine-tuned NMT models for MEDAR and GlobalVoices corpora with advanced sampling
for varying sizes of the update set (nt) and different number of training epochs (e).

and 70s for e = 5. For GlobalVoices, we obtain
the best score of 14.27 with e = 10 which is com-
parable to e = 10 and nt = 100 in the random
sampling condition. For e ∈ {5, 10}, we observe
better scores compared to the random sampling
condition, for all update set sizes nt. In addition,
with our advanced sampling, we always observe a
monotonic increase in BLEU score for all epoch
configurations and increasing number of samples
in the update set nt, in contrast to the epoch con-
figurations e ∈ {10, 20} for the random sampling
condition where we observe a decreasing trend in
BLEU scores. The runtimes are similar to training
times of MEDAR models.

6 Discussion

Our experiment shows that fine-tuning the generic
model Mgen with random sampling for small up-

date sets can improve BLEU scores (see Figure 3).
In particular, we observe improvements over the
baseline with MEDAR data for e = {1, 5} and
with GlobalVoices data for e = {1, 5} for update
set sizes larger than 200 and for e = {10, 20} with
update set sizes less than 200. We did not find log-
like relations similar to Barone et al. (2017). The
reason for this could be because we included less
data for the domain adaptation. For the random
sampling condition, with MEDAR dataset, we can
trade translation quality for faster training times
since e = 5 training yields only slightly better
BLEU scores when compared to e = 1 setting.
Analogously, for GlobalVoices dataset, e = 1, 5
achieves similar performance and perform better
than baseline model when nt > 200, which allows
to switch to a faster model training with e = 1
with a marginal loss in translation quality. Con-



8

cerning larger values of e for both the new do-
mains yield a slower gain in translation quality or
even a loss in translation quality for e ∈ {10, 20}
(GlobalVoices) after an initial improvement over
the baseline. This loss might be caused by overfit-
ting to the training samples due to a high number
of training iterations.

Using our advanced sampling for fine-tuning
Mgen to a new domain, significantly reduces the
amount of training samples without loss in transla-
tion quality compared to the commonly used fine-
tuning with random sampling. This allows to dra-
matically reduce the amount of data that needs to
be translated or post-edited by human labor, be-
cause the sampling of “unlabeled” instances is per-
formed using monolingual data only. In case of
MEDAR, our method reduces translation cost and
time by 66.25% compared to random sampling. In
addition, the BLEU scores improved overall: Ex-
cept for e = 1 training setting, none of the scores
is lower than the baseline score. An interesting
observation when compared to the random sam-
pling condition is that samples resulting from our
advanced sampling need more epochs to achieve
better BLEU scores. We believe this is due to
the following two reasons: (i) Domain mismatch:
the genre of samples of MEDAR dataset is signif-
icantly different from the domain of the samples
observed in Mgen (Almahairi et al., 2016). (ii)
Low amount of samples: our advanced sampling
approach removes 66% of samples from the orig-
inal 400. Both of these factors necessitates more
training epochs to achieve the best BLEU score as
with random sampling condition. In case of Glob-
alVoices, we can observe similar improvements in
BLEU score for e = {5, 10}: we achieve a sim-
ilar BLEU score as with random sampling base-
line although we excluded 57.75% of the training
data. All in all, we can confirm our hypothesis that
our advanced sampling query strategy for sample
selection effectively reduces the number of fine-
tuning samples without degrading the translation
quality compared to results of the baseline. A fur-
ther advantage of our approach is that it supports
continuous fine-tuning, in contrast to other meth-
ods which require a complete re-training of the
model whenever new samples of the target domain
become available (Wang et al., 2017).

Currently, there is one limitation in our work:
The update sets in our evaluation are quite small.
Hence, we want to investigate the performance of

our method using all 36k samples of the Glob-
alVoices parallel corpus.

A promising direction for future work would
be to investigate the impact of active learning in
NMT using our advanced sentence sampling on
translation time and quality of incremental model
improvements. In settings with human workers
that post-edit translation candidates, translations
that improve over time might reduce this post-
editing effort and, consequently reduce the over-
all time and budget required for model adapta-
tion to a new domain. In addition, this technol-
ogy can increase the efficiency of ubiquitous ma-
chine translation interfaces, e.g., for multimodal
post-editing (Herbig et al., 2019; Oviatt et al.,
2017), real-time translation systems in virtual re-
ality (Toyama et al., 2014), or medical cross-
language dialogue applications (Sonntag et al.,
2009b,a) As a follow-up work, we would like
to experiment with a clustering-based sample se-
lection instead of using a single reference vector
(eref ) for the whole generic domain and observe
the performance of domain-adapted sequence-to-
sequence models based on the chosen samples.

7 Conclusion

We investigate the problem of incremental domain
adaptation of a generic NMT model in a limited re-
sources setting. Our NMT models improve BLEU
score with only small amounts of data from a new
domain. Hereby, sentences from the source lan-
guage were randomly sampled for being used as
parallel training data after human translations. We
simulated the human translation task by using ex-
isting parallel corpora. Also, we introduced an
advanced sampling strategy, based on semantic
text similarity using a state-of-the-art technique,
after extending it for computing sentence embed-
dings for Arabic (AraSIF). We found that our
novel method achieves similar BLEU scores, com-
pared to fine-tuning with random sampling, but
using less than half of the initial training data.
This enables more efficient domain adaptation of
NMT models with humans-in-the-loop and with
resource constraints.

Acknowledgments

This work is supported by EIT Digital (H2020).



9

References
Amjad Almahairi, Kyunghyun Cho, Nizar Habash, and

Aaron C. Courville. 2016. First result on arabic neu-
ral machine translation. CoRR, abs/1606.02680.

Sanjeev Arora, Yingyu Liang, and Tengyu Ma. 2017.
A simple but tough-to-beat baseline for sentence em-
beddings.

Antonio Valerio Miceli Barone, Barry Haddow, Ulrich
Germann, and Rico Sennrich. 2017. Regularization
techniques for fine-tuning in neural machine trans-
lation. In Proceedings of the 2017 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1489–1494, Copenhagen, Denmark. As-
sociation for Computational Linguistics.

Michael Barz, Neslihan Büyükdemircioglu,
Rikhu Prasad Surya, Tim Polzehl, and Daniel
Sonntag. 2018a. Device-Type Influence in Crowd-
based Natural Language Translation Tasks (short
paper). In Proceedings of the 1st Workshop
on Subjectivity, Ambiguity and Disagreement in
Crowdsourcing, and Short Paper Proceedings of
the 1st Workshop on Disentangling the Relation Be-
tween Crowdsourcing and Bias Management (SAD)
2018 and CrowdBias 2018) co-locate, volume 2276
of CEUR Workshop Proceedings, pages 93–97.
CEUR-WS.org.

Michael Barz, Tim Polzehl, and Daniel Sonntag.
2018b. Towards hybrid human-machine translation
services. EasyChair Preprint no. 333.

Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using rnn encoder–decoder
for statistical machine translation. In Proceedings of
the 2014 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1724–
1734. Association for Computational Linguistics.

Chenhui Chu, Raj Dabre, and Sadao Kurohashi. 2017.
An empirical comparison of domain adaptation
methods for neural machine translation. In Pro-
ceedings of the 55th Annual Meeting of the Associ-
ation for Computational Linguistics (Short Papers),
pages 385–391. Association for Computational Lin-
guistics.

Jesús González-Rubio, Daniel Ortiz-Martı́nez, and
Francisco Casacuberta. 2012. Active learning for in-
teractive machine translation. In Proceedings of the
13th Conference of the European Chapter of the As-
sociation for Computational Linguistics, pages 245–
254. Association for Computational Linguistics.

Spence Green, Jeffrey Heer, and Christopher D. Man-
ning. 2015. Natural Language Translation at the In-
tersection of AI and HCI. Queue, 13(6):30.

Nico Herbig, Santanu Pal, Josef van Genabith, and An-
tonio Krüger. 2019. Multi-modal approaches for
post-editing machine translation. In Proceedings

of the 2019 CHI Conference on Human Factors in
Computing Systems, CHI ’19, pages 231:1–231:11,
New York, NY, USA. ACM.

Philipp Koehn and Rebecca Knowles. 2017. Six chal-
lenges for neural machine translation. In Pro-
ceedings of the First Workshop on Neural Machine
Translation, pages 28–39. Association for Compu-
tational Linguistics.

Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics on Human Language Technology - Vol-
ume 1, NAACL ’03, pages 48–54, Stroudsburg, PA,
USA. Association for Computational Linguistics.

Tsz Kin Lam, Julia Kreutzer, and Stefan Riezler. 2018.
A reinforcement learning approach to interactive-
predictive neural machine translation. CoRR,
abs/1805.01553.

Jaesong Lee, Joong-Hwi Shin, and Jun-Seok Kim.
2017. Interactive visualization and manipulation
of attention-based neural machine translation. In
Proceedings of the 2017 Conference on Empiri-
cal Methods in Natural Language Processing: Sys-
tem Demonstrations, pages 121–126. Association
for Computational Linguistics.

Minh-Thang Luong, Eugene Brevdo, and Rui Zhao.
2017. Neural machine translation (seq2seq) tutorial.
https://github.com/tensorflow/nmt.

Dragos Stefan Munteanu and Daniel Marcu. 2005.
Improving machine translation performance by ex-
ploiting non-parallel corpora. Comput. Linguist.,
31(4):477–504.

Laurent Nepveu, Guy Lapalme, Philippe Langlais, and
George Foster. 2004. Adaptive language and trans-
lation models for interactive machine translation. In
Proceedings of the 2004 Conference on Empirical
Methods in Natural Language Processing.

Sharon Oviatt, Björn Schuller, Philip R Cohen,
Daniel Sonntag, Gerasimos Potamianos, and Anto-
nio Krüger. 2017. Introduction: Scope, Trends, and
Paradigm Shift in the Field of Computer Interfaces.
In The Handbook of Multimodal-Multisensor Inter-
faces, pages 1–15. Association for Computing Ma-
chinery and Morgan & Claypool, New York, NY,
USA.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting on Association for Com-
putational Linguistics - ACL ’02, page 311, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.

Álvaro Peris and Francisco Casacuberta. 2018. Online
learning for effort reduction in interactive neural ma-
chine translation. CoRR, abs/1802.03594.

http://arxiv.org/abs/1606.02680
http://arxiv.org/abs/1606.02680
https://www.aclweb.org/anthology/D17-1156
https://www.aclweb.org/anthology/D17-1156
https://www.aclweb.org/anthology/D17-1156
http://ceur-ws.org/Vol-2276/paper12.pdf
http://ceur-ws.org/Vol-2276/paper12.pdf
http://ceur-ws.org/Vol-2276/paper12.pdf
https://doi.org/10.29007/kw5h
https://doi.org/10.29007/kw5h
https://doi.org/10.3115/v1/D14-1179
https://doi.org/10.3115/v1/D14-1179
https://doi.org/10.3115/v1/D14-1179
https://doi.org/10.18653/v1/P17-2061
https://doi.org/10.18653/v1/P17-2061
http://www.aclweb.org/anthology/E12-1025
http://www.aclweb.org/anthology/E12-1025
https://doi.org/10.1145/2791301.2798086
https://doi.org/10.1145/2791301.2798086
https://doi.org/10.1145/3290605.3300461
https://doi.org/10.1145/3290605.3300461
http://aclweb.org/anthology/W17-3204
http://aclweb.org/anthology/W17-3204
https://doi.org/10.3115/1073445.1073462
http://arxiv.org/abs/1805.01553
http://arxiv.org/abs/1805.01553
http://aclweb.org/anthology/D17-2021
http://aclweb.org/anthology/D17-2021
https://doi.org/10.1162/089120105775299168
https://doi.org/10.1162/089120105775299168
http://www.aclweb.org/anthology/W04-3225
http://www.aclweb.org/anthology/W04-3225
https://doi.org/10.1145/3015783.3015785
https://doi.org/10.1145/3015783.3015785
https://doi.org/10.3115/1073083.1073135
https://doi.org/10.3115/1073083.1073135
http://arxiv.org/abs/1802.03594
http://arxiv.org/abs/1802.03594
http://arxiv.org/abs/1802.03594


10

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Neural machine translation of rare words
with subword units. In Proceedings of the 54th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1715–
1725. Association for Computational Linguistics.

Burr Settles. 2010. Active learning literature survey.
Computer Sciences Technical Report 1648, Univer-
sity of Wisconsin–Madison.

Abu Bakr Soliman, Kareem Eissa, and Samhaa R. El-
Beltagy. 2017. Aravec: A set of arabic word embed-
ding models for use in arabic nlp. Procedia Com-
puter Science, 117:256 – 265. Arabic Computa-
tional Linguistics.

Daniel Sonntag, Robert Nesselrath, Gerhard Sonnen-
berg, and Gerd Herzog. 2009a. Supporting a rapid
dialogue system engineering process. Proceedings
of the 1st IWSDS.

Daniel Sonntag, Pinar Wennerberg, Paul Buitelaar, and
Sonja Zillner. 2009b. Pillars of ontology treatment
in the medical domain. J. Cases on Inf. Techn.,
11(4):47–73.

Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.
Sequence to Sequence Learning with Neural Net-
works. In Proceedings of the 27th International
Conference on Neural Information Processing Sys-
tems, pages 3104–3112. MIT Press.

Jörg Tiedemann. 2012. Parallel data, tools and in-
terfaces in opus. In Proceedings of the Eighth In-
ternational Conference on Language Resources and
Evaluation (LREC-2012). European Language Re-
sources Association (ELRA).

Takumi Toyama, Daniel Sonntag, Andreas Dengel,
Takahiro Matsuda, Masakazu Iwamura, and Koichi
Kise. 2014. A mixed reality head-mounted text
translation system using eye gaze input. In Proceed-
ings of the 19th international conference on Intelli-
gent User Interfaces - IUI ’14, pages 329–334, New
York, New York, USA. ACM Press.

Rui Wang, Andrew Finch, Masao Utiyama, and Ei-
ichiro Sumita. 2017. Sentence embedding for neu-
ral machine translation domain adaptation. In Pro-
ceedings of the 55th Annual Meeting of the Associ-
ation for Computational Linguistics (Short Papers),
pages 560–566. Association for Computational Lin-
guistics.

https://doi.org/10.18653/v1/P16-1162
https://doi.org/10.18653/v1/P16-1162
https://doi.org/https://doi.org/10.1016/j.procs.2017.10.117
https://doi.org/https://doi.org/10.1016/j.procs.2017.10.117
http://dl.acm.org/citation.cfm?id=2969033.2969173
http://dl.acm.org/citation.cfm?id=2969033.2969173
http://www.aclweb.org/anthology/L12-1246
http://www.aclweb.org/anthology/L12-1246
https://doi.org/10.1145/2557500.2557528
https://doi.org/10.1145/2557500.2557528
https://doi.org/10.18653/v1/P17-2089
https://doi.org/10.18653/v1/P17-2089

