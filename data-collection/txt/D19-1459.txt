



















































Taskmaster-1: Toward a Realistic and Diverse Dialog Dataset


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 4516–4525,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

4516

Taskmaster-1: Toward a Realistic and Diverse Dialog Dataset

Bill Byrne1∗ Karthik Krishnamoorthi1∗ Chinnadhurai Sankar2∗ Arvind Neelakantan1
Daniel Duckworth1 Semih Yavuz3 Ben Goodrich1
Amit Dubey1 Andy Cedilnik1 Kyu-Young Kim1

1Google LLC, Mountain View, California
2Mila, Université de Montréal

3University of California, Santa Barbara
{billb,krishnamoorthi,aneelakantan}@google.com

Abstract

A significant barrier to progress in data-driven
approaches to building dialog systems is the
lack of high quality, goal-oriented conversa-
tional data. To help satisfy this elementary
requirement, we introduce the initial release
of the Taskmaster-1 dataset which includes
13,215 task-based dialogs comprising six do-
mains. Two procedures were used to create
this collection, each with unique advantages.
The first involves a two-person, spoken “Wiz-
ard of Oz” (WOz) approach in which trained
agents and crowdsourced workers interact to
complete the task while the second is “self-
dialog” in which crowdsourced workers write
the entire dialog themselves. We do not re-
strict the workers to detailed scripts or to a
small knowledge base and hence we observe
that our dataset contains more realistic and di-
verse conversations in comparison to existing
datasets. We offer several baseline models in-
cluding state of the art neural seq2seq architec-
tures with benchmark performance as well as
qualitative human evaluations. Dialogs are la-
beled with API calls and arguments, a simple
and cost effective approach which avoids the
requirement of complex annotation schema.
The layer of abstraction between the dialog
model and the service provider API allows for
a given model to interact with multiple ser-
vices that provide similar functionally. Finally,
the dataset will evoke interest in written vs.
spoken language, discourse patterns, error han-
dling and other linguistic phenomena related
to dialog system research, development and de-
sign.1

1 Introduction

Voice-based “personal assistants” such as Apple’s
SIRI, Microsoft’s Cortana, Amazon Alexa, and the

∗Equal Contribution
1Dataset available at https://g.co/dataset/taskmaster-1

Google Assistant have finally entered the main-
stream. This development is generally attributed
to major breakthroughs in speech recognition and
text-to-speech (TTS) technologies aided by recent
progress in deep learning (Lecun et al., 2015), ex-
ponential gains in compute power (Steinkrau et al.,
2005; Jouppi et al., 2017), and the ubiquity of
powerful mobile devices. The accuracy of ma-
chine learned speech recognizers (Hinton et al.,
2012) and speech synthesizers (van den Oord et al.,
2016) are good enough to be deployed in real-world
products and this progress has been driven by pub-
licly available labeled datasets. However, conspic-
uously absent from this list is equal progress in
machine learned conversational natural language
understanding (NLU) and generation (NLG). The
NLU and NLG components of dialog systems start-
ing from the early research work (Weizenbaum,
1966) to the present commercially available per-
sonal assistants largely rely on rule-based systems.
The NLU and NLG systems are often carefully
programmed for very narrow and specific cases
(Google, 2019; Amazon, 2019). General under-
standing of natural spoken behaviors across mul-
tiple dialog turns, even in single task-oriented sit-
uations, is by most accounts still a long way off.
In this way, most of these products are very much
hand crafted, with inherent constraints on what
users can say, how the system responds and the or-
der in which the various subtasks can be completed.
They are high precision but relatively low cover-
age. Not only are such systems unscalable, but
they lack the flexibility to engage in truly natural
conversation.

Yet none of this is surprising. Natural language
is heavily context dependent and often ambiguous,
especially in multi-turn conversations across mul-
tiple topics. It is full of subtle discourse cues and
pragmatic signals whose patterns have yet to be
thoroughly understood. Enabling an automated



4517

system to hold a coherent task-based conversation
with a human remains one of computer science’s
most complex and intriguing unsolved problems
(Weizenbaum, 1966). In contrast to more tradi-
tional NLP efforts, interest in statistical approaches
to dialog understanding and generation aided by
machine learning has grown considerably in the last
couple of years (Rojas-Barahona et al., 2017; Bor-
des et al., 2017; Henderson et al., 2013). However,
the dearth of high quality, goal-oriented dialog data
is considered a major hindrance to more significant
progress in this area (Bordes et al., 2017; Lowe
et al., 2015).

To help solve the data problem we present
Taskmaster-1, a dataset consisting of 13,215 di-
alogs, including 5,507 spoken and 7,708 written
dialogs created with two distinct procedures. Each
conversation falls into one of six domains: ordering
pizza, creating auto repair appointments, setting up
ride service, ordering movie tickets, ordering coffee
drinks and making restaurant reservations. For the
spoken dialogs, we created a Wizard of Oz (WOz)
system (Kelley, 1984) to collect two-person, spo-
ken conversations. Crowdsourced workers playing
the “user” interacted with human operators playing
the digital assistant using a web-based interface. In
this way, users were led to believe they were inter-
acting with an automated system while it was in
fact a human, allowing them to express their turns
in natural ways but in the context of an automated
interface. We refer to this spoken dialog type as
“two-person dialogs”. For the written dialogs, we
engaged crowdsourced workers to write the full
conversation themselves based on scenarios out-
lined for each task, thereby playing roles of both
the user and assistant. We refer to this written di-
alog type as “self-dialogs”. In a departure from
traditional annotation techniques (Henderson et al.,
2013; Rojas-Barahona et al., 2017; Budzianowski
et al., 2018), dialogs are labeled with simple API
calls and arguments. This technique is much eas-
ier for annotators to learn and simpler to apply.
As such it is more cost effective and, in addition,
the same model can be used for multiple service
providers.

Taskmaster-1 has richer and more diverse lan-
guage than the current popular benchmark in task-
oriented dialog, MultiWOZ (Budzianowski et al.,
2018). Table 1 shows that Taskmaster-1 has more
unique words and is more difficult for language
models to fit. We also find that Taskmaster-1 is

Statistic Self-dialogs MultiWOZ

# unique words 21,894 19,175
# unique named 8,218 1,338
entities
# utterances 169,469 132,610
# dialogs 7,708 10,438
Avg. utterances 21.99 13.70
per dialog
Avg. tokens 8.62 13.82
per utterance
Perplexity 17.08 15.62
BLEU 6.53 11.02

Table 1: Statistics comparison: Self-dialogs vs Multi-
WOZ corpus both containing approximately 10k dia-
logues each.

more realistic than MultiWOZ. Specifically, the
two-person dialogs in Taskmaster-1 involve more
real-word entities than seen in MutliWOZ since we
do not restrict conversations to a small knowledge
base. Beyond the corpus and the methodologies
used to create it, we present several baseline models
including state-of-the-art neural seq2seq architec-
tures together with perplexity and BLEU scores.
We also provide qualitative human performance
evaluations for these models and find that auto-
matic evaluation metrics correlate well with human
judgments. We will publicly release our corpus
containing conversations, API call and argument
annotations, and also the human judgments.

2 Related work

2.1 Human-machine vs. human-human
dialog

Serban et al. (2017) discuss the major features and
differences among the existing offerings in an ex-
haustive and detailed survey of available corpora
for data driven learning of dialog systems. One im-
portant distinction covered is that of human-human
vs. human-machine dialog data, each having its
advantages and disadvantages. Many of the exist-
ing task-based datasets have been generated from
deployed dialog systems such as the Lets Go Bus
Information System (Raux et al., 2003) and the
various Dialog State Tracking Challenges (DSTCs)
(Williams et al., 2016). However, it is doubtful
that new data-driven systems built with this type of
corpus would show much improvement since they
would be biased by the existing system and likely



4518

mimic its limitations (Williams and Young, 2007).
Since the ultimate goal is to be able to handle com-
plex human language behaviors, it would seem
that human-human conversational data is the bet-
ter choice for spoken dialog system development
(Budzianowski et al., 2018). However, learning
from purely human-human based corpora presents
challenges of its own. In particular, human con-
versation has a different distribution of understand-
ing errors and exhibits turn-taking idiosyncrasies
which may not be well suited for interaction with a
dialog system (Williams and Young, 2007; Serban
et al., 2017).

2.2 The Wizard of Oz (WOz) Approach and
MultiWOZ

The WOz framework, first introduced by Kelley
(1984) as a methodology for iterative design of nat-
ural language interfaces, presents a more effective
approach to human-human dialog collection. In
this setup, users are led to believe they are interact-
ing with an automated assistant but in fact it is a
human behind the scenes that controls the system
responses. Given the human-level natural language
understanding, users quickly realize they can com-
fortably and naturally express their intent rather
than having to modify behaviors as is normally the
case with a fully automated assistant. At the same
time, the machine-oriented context of the interac-
tion, i.e. the use of TTS and slower turn taking
cadence, prevents the conversation from becom-
ing fully fledged, overly complex human discourse.
This creates an idealized spoken environment, re-
vealing how users would openly and candidly ex-
press themselves with an automated assistant that
provided superior natural language understanding.

Perhaps the most relevant work to consider
here is the recently released MultiWOZ dataset
(Budzianowski et al., 2018), since it is similar in
size, content and collection methodologies. Mul-
tiWOZ has roughly 10,000 dialogs which feature
several domains and topics. The dialogs are an-
notated with both dialog states and dialog acts.
MultiWOZ is an entirely written corpus and uses
crowdsourced workers for both assistant and user
roles. In contrast, Taskmaster-1 has roughly 13,000
dialogs spanning six domains and annotated with
API arguments. The two-person spoken dialogs in
Taskmaster-1 use crowdsourcing for the user role
but trained agents for the assistant role. The assis-
tant’s speech is played to the user via TTS. The

ASSISTANT: How can I help you?
USER: Hi, could you help me with booking movie

tickets for tonight?
ASSISTANT: What movie are you interested in?

USER: The Upside.
ASSISTANT: Did you have a theater in mind?

USER: Could you check if the Regal Neshaminy...
No, AMC Neshaminy in Neshaminy, PA is
playing it?

ASSISTANT: Could you spell that?
USER: Sure, n e s h a m i n y.

ASSISTANT: I have a showtime at 7:30 and at 10:30, is
that okay?

USER: Yes, could you get two tickets for the 7:30?
ASSISTANT: One moment. Okay so that’s 2 tickets for

7:30 at the AMC Neshaminy 24?
USER: Yes.

ASSISTANT: It’ll be twenty-four ninety-nine for your tick-
ets.

USER: That sounds great.
ASSISTANT: I’ve confirmed your tickets, they’ll arrive

via text shortly. Did you need any other
information?

USER: No, that was it. Thank you so much for your
help.

ASSISTANT: Great, no problem. I hope you have fun.
USER: I hope so, too. Thank you so much.

Figure 1: Sample Taskmaster-1 two-person dialog

remaining 7,708 conversations in Taskmaster-1 are
self-dialogs, in which crowdsourced workers write
the entire conversation themselves. As Krause et al.
(2017); Moghe et al. (2018) show, self dialogs are
surprisingly rich in content.

3 The Taskmaster Corpus

3.1 Overview

There are several key attributes that make
Taskmaster-1 both unique and effective for data-
driven approaches to building dialog systems and
for other research.

Spoken and written dialogs: While the spoken
sources more closely reflect conversational lan-
guage (Chafe and Tannen, 1987), written dialogs
are significantly cheaper and easier to gather. This
allows for a significant increase in the size of the
corpus and in speaker diversity.

Goal-oriented dialogs: All dialogs are based on
one of six tasks: ordering pizza, creating auto repair
appointments, setting up rides for hire, ordering
movie tickets, ordering coffee drinks and making
restaurant reservations.

Two collection methods: The two-person dialogs
and self-dialogs each have pros and cons, revealing
interesting contrasts.



4519

MAIN TASK: Users will pretend they are using a voice-
powered personal digital assistant to book movie tickets for a
film they ALREADY have in mind.

1. In several turns (not just one!), cover the following:

(a) Film name
(b) Number of people
(c) City
(d) Theater
(e) Time
(f) If applicable: 3D vs. IMAX vs. standard.

2. They may also want to know things like:

(a) Run time
(b) End time
(c) Director, actors, etc.

3. Make sure to CONFIRM all the relevant ticket details
before the end of the dialogue INCLUDING:

(a) Total cost for two tickets
(b) Time, location, theater

4. You can assume you have the users account info with the
ticket service–so no credit card information is necessary.

5. After confirming the details, end the conversation by
confirming that the tickets are being sent to the users
mobile device as a text message.

Figure 2: Sample instructions for agents playing “assis-
tant” role

MAIN TASK: Pretend you are using your voice-powered
digital assistant to book movie tickets.

1. Start by thinking of a particular movie PLAYING NOW
in theaters that you’d like to see. (Use the internet to
find one if necessary.)

2. Choose a DIFFERENT CITY from where you live,
work, or happen to be at the moment.

3. Pretend you’ve decided to see this movie tonight and
you’re taking a friend.

4. The assistant will ask about all relevant details BUT you
should make sure it covers all your needs.

5. You can assume you already have an account with the
ticket service–so no credit card information is necessary.

6. The assistant will end the conversation by confirming
that your tickets are being sent to your mobile device as
a text message. (And you can respond thanks, goodbye,
ok, etc. for a final closing turn, if you like).

Figure 3: Sample instructions for crowdsourced work-
ers playing “user” role

Multiple turns: The average number of utterances
per dialog is about 23 which ensures context-rich
language behaviors.

API-based annotation: The dataset uses a simple
annotation schema providing sufficient grounding
for the data while making it easy for workers to
apply labels consistently.

Size: The total of 13,215 dialogs in this corpus is
on par with similar, recently released datasets such
as MultiWOZ (Budzianowski et al., 2018).

3.2 Two-person, spoken dataset

In order to replicate a two-participant, automated
digital assistant experience, we built a WOz plat-
form that pairs agents playing the digital assistant
with crowdsourced workers playing the user in task-
based conversational scenarios. An example dialog
from this dataset is given in Figure 1.

3.2.1 WOz platform and data pipeline
While it is beyond the scope of this work to de-
scribe the entire system in detail, there are several
platform features that help illustrate how the pro-
cess works.

Modality: The agents playing the assistant type
their input which is in turn played to the user via
text-to-speech (TTS) while the crowdsourced work-
ers playing the user speak aloud to the assistant us-
ing their laptop and microphone. We use WebRTC
to establish the audio channel. This setup creates a
digital assistant-like communication style.

Conversation and user quality control: Once the
task is completed, the agents tag each conversation
as either successful or problematic depending on
whether the session had technical glitches or user
behavioral issues. We are also then able to root out
problematic users based on this logging.

Agent quality control: Agents are required to lo-
gin to the system which allows us to monitor per-
formance including the number and length of each
session as well as their averages.

User queuing: When there are more users trying
to connect to the system than available agents, a
queuing mechanism indicates their place in line
and connects them automatically once they move
to the front of the queue.

Transcription: Once complete, the user’s audio-
only portion of the dialog is transcribed by a second
set of workers and then merged with the assistant’s



4520

typed input to create a full text version of the di-
alog. Finally, these conversations are checked for
transcription errors and typos and then annotated,
as described in Section 3.4.

3.2.2 Agents, workers and training
Both agents and crowdsourced workers are given
written instructions prior to the session. Examples
of each are given in Figure 2 and Figure 3. The
instructions continue to be displayed on screen to
the crowdsourced workers while they interact with
the assistant. Instructions are modified at times
(for either participant or both) to ensure broader
coverage of dialog scenarios that are likely to occur
in actual user-assistant interactions. For example,
in one case users were asked to change their mind
after ordering their first item and in another agents
were instructed to tell users that a given item was
not available. Finally, in their instructions, crowd-
sourced workers playing the user are told they will
be engaging in conversation with a digital assistant.
However, it is plausible that some suspect human
intervention due to the advanced level of natural
language understanding from the assistant side.

Agents playing the assistant role were hired from
a pool of dialog analysts and given two hours of
training on the system interface as well as on how
to handle specific scenarios such as uncooperative
users and technical glitches. Uncooperative users
typically involve those who either ignored agent
input or who rushed through the conversation with
short phrases. Technical issues involved dropped
sessions (e.g. WebRTC connections failed) or cases
in which the user could not hear the agent or vice-
versa. In addition, weekly meetings were held with
the agents to answer questions and gather feed-
back on their experiences. Agents typically work
four hours per day with dialog types changing ev-
ery hour. Crowdsourced workers playing the user
are accessed using Amazon Mechanical Turk. Pay-
ment for a completed dialog session lasting roughly
five to seven minutes was typically in the range of
$1.00 to $1.30. Problematic users are detected ei-
ther by the agent involved in the specific dialog
or by post-session assessment and removed from
future requests.

3.3 Self-dialogs (one-person written dataset)

While the two-person approach to data collection
creates a realistic scenario for robust, spoken dia-
log data collection, this technique is time consum-
ing, complex and expensive, requiring considerable

1. Think of a particular movie PLAYING NOW in theaters
that you’d like to see. (Use the internet to find one if
necessary.)

2. Choose a DIFFERENT CITY from where you live,
work, or happen to be at the moment.

3. Pretend you’ve decided to see this movie tonight and
you’re taking a friend.

4. Use the internet to look up the details of the city, the
theater name, showtimes offered, ticket prices, and any
additional options like 3D, etc.

5. MAIN TASK: Pretend you call your personal assistant
on the phone who will book the ticket for you. Write
the conversation that would happen between you and
your assistant in order to buy two tickets.

6. MAKE SURE the assistant asks about all relevant de-
tails (see 4) INCLUDING the number of tickets needed.
BUT you should choose the order that makes sense to
you as far what details to ask (theater, times, etc)

7. You can assume you already have an account with the
ticket service–so no credit card information is necessary.

8. The assistant should end the conversation by confirming
that your tickets are being sent to your mobile device as
a text message. (And you can respond thanks, goodbye,
ok, etc. for a final closing turn, if you like).

• YOUR TASK: Write the conversation that results be-
tween you and your assistant. It must be at least 10
turns long (for both you and the assistant). Below we
have provided 15 turns in case you need more. KEEP IT
NEW AND FRESH! DON’T REPEAT DIALOGUES
FROM THE PAST!

Figure 4: Sample instructions for written “self-dialogs”

technical implementation as well as administrative
procedures to train and manage agents and crowd-
sourced workers. In order to extend the Taskmaster
dataset at minimal cost, we use an alternative self-
dialog approach in which crowdsourced workers
write the full dialogs themselves (i.e. interpreting
the roles of both user and assistant).

3.3.1 Task scenarios and instructions

Targeting the same six tasks used for the two-
person dialogs, we again engaged the Amazon Me-
chanical Turk worker pool to create self-dialogs,
this time as a written exercise. In this case, users
are asked to pretend they have a personal assistant
who can help them take care of various tasks in
real time. They are told to imagine a scenario in
which they are speaking to their assistant on the
phone while the assistant accesses the services for
one of the given tasks. They then write down the
entire conversation. Figure 4 shows a sample set of



4521

USER: Hi I would like to buy 2 tickets for
Shazam!

ASSISTANT: What city would you like to see this
movie?

USER: Ontario, California
ASSISTANT: Ok, I’ll check that location for you.

USER: I would prefer the Edwards Ontario
Mountain Village, since it’s closest to
me and my guest.

ASSISTANT: What time is best for you?
USER: Either 4 or 6 pm.

ASSISTANT: I’m sorry, but it looks like the 4:10 and
the 6:10 pm showings are sold out.

USER: That’s too bad. I really wanted to see
that movie.

ASSISTANT: I’m sorry. Is there another movie you
would like to see?

USER: How about Captain Marvel at the Ed-
wards Ontario Mountain theater.

ASSISTANT: Show times are 3:45, 7:10 and 10:10 pm.
Which would you like?

USER: I am interested in the 7:10 showing.
ASSISTANT: I’m sorry, it looks like the 7:10 showing

is also sold out.
USER: Wow, that’s too bad.

ASSISTANT: I’m sorry. Is there another movie you
would like me to look up?

USER: No, I think I’ll pass on the movies
tonight since those were the two I really
wanted to see.

ASSISTANT: If you want, I can check another theater.
USER: No, that’s fine. Thank you for your help.

ASSISTANT: You’re welcome.

Figure 5: Sample one-person, written dialog

instructions.

3.3.2 Pros and cons of self-dialogs

The self-dialog technique renders quality data and
avoids some of the challenges seen with the two-
person approach. To begin, since the same person
is writing both sides of the conversation, we never
see misunderstandings that lead to frustration as
is sometimes experienced between interlocutors in
the two-person approach. In addition, all the self-
dialogs follow a reasonable path even when the
user is constructing conversations that include un-
derstanding errors or other types of dialog glitches
such as when a particular choice is not available.
As it turns out, crowdsourced workers are quite
effective at recreating various types of interactions,
both error-free and those containing various forms
of linguistic repair. The sample dialog in Figure 5
shows the result of a self-dialog exercise in which
workers were told to write a conversation with var-
ious ticket availability issues that is ultimately un-
successful.

Two more benefits of the self-dialog approach
are its efficiency and cost effectiveness. We were

USER: Finally, I need the table to be for three
people and 8pm.

ASSISTANT: One moment....OK, I have your table
for three (num.guests.accept) at 8pm
(time.reservation.accept) reserved.

Figure 6: Indicating transaction status with “accept” or
“reject”

able to gather thousands of dialogs in just days
without transcription or trained agents, and spent
roughly six times less per dialog. Despite these ad-
vantages, the self-dialog written technique cannot
recreate the disfluencies and other more complex
error patterns that occur in the two-person spoken
dialogs which are important for model accuracy
and coverage.

3.4 Annotation

We chose a highly simplified annotation approach
for Taskmaster-1 as compared to traditional, de-
tailed strategies which require robust agreement
among workers and usually include dialog state
and slot information, among other possible labels.
Instead we focus solely on API arguments for each
type of conversation, meaning just the variables
required to execute the transaction. For example,
in dialogs about setting up UBER rides, we label
the “to” and “from” locations along with the car
type (UberX, XL, Pool, etc). For movie tickets,
we label the movie name, theater, time, number
of tickets, and sometimes screening type (e.g. 3D
vs. standard). A complete list of labels is included
with the corpus release.

As discussed in Section 3.2.2, to encourage di-
versity, at times we explicitly ask users to change
their mind in the middle of the conversation, and
the agents to tell the user that the requested item
is not available. This results in conversations hav-
ing multiple instances of the same argument type.
To handle this ambiguity, in addition to the labels
mentioned above, the convention of either “accept
or “reject” was added to all labels used to execute
the transaction, depending on whether or not that
transaction was successful.

In Figure 6, both the number of people and the
time variables in the assistant utterance would have
the “.accept” label indicating the transaction was
completed successfully. If the utterance describ-
ing a transaction does not include the variables by
name, the whole sentence is marked with the di-
alog type. For example, a statement such as The
table has been booked for you would be labeled as



4522

Statistic Self-dialogs Two Person

# unique words 17,275 13,490
# utterances 110,074 132,407
# dialogs 5000 5000
Avg. utterances 22.01 24.04
per dialog
Avg. tokens 8.62 7.54
per utterance
Perplexity 16.28 6.44
BLEU 4.73 15.16
Joint-Perplexity 16.44 6.04
Joint-BLEU 5.80 13.09

Table 2: Statistics comparison: Self-dialogs vs two
person corpus both containing 5k dialogs. Perplex-
ity and BLEU are reported for Transformer baseline.
Joint-Perplexity and Joint-BLEU are perplexity/BLEU
scores from the joint training of self-dialogs and two-
person but evaluated with their respective test sets.

reservation.accept.

4 Dataset Analysis

4.1 Self-dialogs vs MultiWOZ
We quantitatively compare our self-dialogs (Sec-
tion 3.3) with the MultiWOZ dataset in Table 1.
Compared to MultiWOZ, we do not ask the users
and assistants to stick to detailed scripts and do
not restrict them to have conversations surround-
ing a small knowledge base. Table 1 shows that
our dataset has more unique words, and has al-
most twice the number of utterances per dialog
than the MultiWOZ corpus. Finally, when trained
with the Transformer (Vaswani et al., 2017) model,
we observe significantly higher perplexities and
lower BLEU scores for our dataset compared to
MultiWOZ suggesting that our dataset conversa-
tions are difficult to model. Finally, Table 1 also
shows that our dataset contains close to 10 times
more real-world named entities than MultiWOZ
and thus, could potentially serve as a realistic base-
line when designing goal oriented dialog systems.
MultiWOZ has only 1338 unique named entities
and only 4510 unique values (including date, time
etc.) in their datatset.

4.2 Self-dialogs vs Two-person
In this section, we quantitatively compare 5k con-
versations each of self-dialogs (Section 3.3) and
two-person (Section 3.2). From Table 2, we find
that self-dialogs exhibit higher perplexity ( almost

3 times) compared to the two-person conversa-
tions suggesting that self-dialogs are more diverse
and contains more non-conventional conversational
flows which is inline with the observations in
Section-3.3.2. While the number of unique words
are higher in the case of self-dialogs, conversations
are longer in the two-person conversations. We
also report metrics by training a single model on
both the datasets together.

4.3 Baseline Experiments: Response
Generation

We evaluate various seq2seq architectures
(Sutskever et al., 2014) on our self-dialog corpus
using both automatic evaluation metrics and
human judgments. Following the recent line
of work on generative dialog systems (Vinyals
and Le, 2015), we treat the problem of response
generation given the dialog history as a conditional
language modeling problem. Specifically we
want to learn a conditional probability distribution
Pθ(Ut|U1:t−1) where Ut is the next response given
dialog history U1:t−1. Each utterance Ui itself is
comprised of a sequence of words wi1 , wi2 . . . wik .
The overall conditional probability is factorized
autoregressively as

Pθ(Ut|U1:t−1) =
n∏
i=1

Pθ(wti |wt1:i−1 , U1:t−1)

Pθ, in this work, is parameterized by a recurrent,
convolution or Transformer-based seq2seq model.

n-gram: We consider 3-gram and 4-gram condi-
tional language model baseline with interpolation.
We use random grid search for the best coefficients
for the interpolated model.

Convolution: We use the fconv architecture
(Gehring et al., 2017) and default hyperparam-
eters from the fairseq (Ott et al., 2019) frame-
work.2 We train the network with ADAM opti-
mizer (Kingma and Ba, 2015) with learning rate of
0.25 and dropout probability set to 0.2.

LSTM: We consider LSTM models (Hochreiter
and Schmidhuber, 1997) with and without attention
(Bahdanau et al., 2015) and use the tensor2tensor
(Vaswani et al., 2018) framework for the LSTM
baselines. We use a two-layer LSTM network for
both the encoder and the decoder with 128 dimen-
sional hidden vectors.

Transformer: As with LSTMs, we use the ten-
sor2tensor framework for the Transformer model.

2https://github.com/pytorch/fairseq



4523

Baseline PPL BLEU Ratings Rank
Models (LIKERT)

GPT-2 (117M) - 0.26 - -

3-gram 38.12 0.20 - -
4-gram 34.49 0.21 - -
LSTM 25.73 4.45 - -
Convolution 21.25 5.09 2.89 3
LSTM-attention 20.05 5.12 3.51 2
Transformer 18.19 6.11 3.22 1

Table 3: Evaluation of various seq2seq architectures
(Sutskever et al., 2014) on our self-dialog corpus us-
ing both automatic evaluation metrics and human judg-
ments. Human evaluation ratings in the 1-5 LIKERT
scale (higher the better), and human ranking are aver-
aged over 500 x 3 ratings (3 crowdsourced workers per
rating).

Our Transformer (Vaswani et al., 2017) model uses
256 dimensions for both input embedding and hid-
den state, 2 layers and 4 attention heads. For both
LSTMs and Transformer, we train the model with
ADAM optimizer (β1 = 0.85, β2 = 0.997) and
dropout probability set to 0.2.

GPT-2: Apart from supervised seq2seq mod-
els, we also include results from pre-trained GPT-2
(Radford et al., 2019) containing 117M parameters.

We evaluate all the models with perplexity and
BLEU scores (Table 3). Additionally, we per-
form two kinds of human evaluation - Ranking and
Rating (LIKERT scale) for the top-3 performing
models - Convolution, LSTM-attention and Trans-
former. For the ranking task, we randomly show
500 partial dialogs and generated responses of the
top-3 models from the test set to three different
crowdsourced workers and ask them to rank the
responses based on their relevance to the dialog
history. For the rating task, we show the model
responses individually to three different crowd-
sourced workers and ask them to rate the responses
on a 1-5 LIKERT scale based on their appropriate-
ness to the dialog history. From Table-4, we see
that inter-annotator reliability scores (Krippendorfs
Alpha) are higher for the ranking task compared to
the rating task. From Table 3, we see that Trans-
former is the best performing model on automatic
evaluation metrics. It is interesting to note that
there is a strong correlation between BLEU score
and human ranking judgments.

Evalation Inter-Annotator Reliability
method (Krippendorfs Alpha)

Rating (1-5 LIKERT) 0.21
Ranking 0.29

Table 4: Inter-Annotator Reliability scores of seq2seq
model responses computed for 500 self-dialogs from
the test set, each annotated by 3 crowdsourced workers.

Model Micro F1 (%)

Transformer 48.73
Transformer + copy 51.79

Table 5: API Argument prediction accuracy for Self-
dialogs. API arguments are annotated as spans in the
utterances.

4.4 Baseline Experiments: Argument
Prediction

Next, we discuss a set of baseline experiments for
the task of argument prediction. API arguments are
annotated as spans in the dialog (Section 3.4). We
formulate this problem as mapping text conversa-
tion to a sequence of output arguments. Apart from
the seq2seq Transformer baseline, we consider
an additional model - an enhanced Transformer
seq2seq model where the decoder can choose to
copy from the input or generate from the vocabu-
lary (Merity et al., 2017; Gu et al., 2016). Since all
the API arguments are input spans, the copy model
having the correct inductive bias achieves the best
performance.

5 Conclusion

To address the lack of quality corpora for data-
driven dialog system research and development,
this paper introduces Taskmaster-1, a dataset that
provides richer and more diverse language as com-
pared to current benchmarks since it is based on
unrestricted, task-oriented conversations involving
more real-word entities. In addition, we present
two data collection methodologies, both spoken
and written, that ensure both speaker diversity and
conversational accuracy. Our straightforward, API-
oriented annotation technique is much easier for
annotators to learn and simpler to apply. We give
several baseline models including state-of-the-art
neural seq2seq architectures, provide qualitative
human performance evaluations for these models,
and find that automatic evaluation metrics correlate
well with human judgments.



4524

References
Amazon. 2019. Alexa skills.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. ICLR.

Antoine Bordes, Y-Lan Boureau, and Jason Weston.
2017. Learning end-to-end goal-oriented dialog.
ICLR.

Paweł Budzianowski, Tsung-Hsien Wen, Bo-Hsiang
Tseng, Iñigo Casanueva, Stefan Ultes, Osman Ra-
madan, and Milica Gasic. 2018. Multiwoz - a large-
scale multi-domain wizard-of-oz dataset for task-
oriented dialogue modelling. EMNLP.

Wallace Chafe and Deborah Tannen. 1987. The rela-
tion between written and spoken language. Annual
Review of Anthropology.

J. Gehring, M. Auli, D. Grangier, D. Yarats, and Y. N.
Dauphin. 2017. Convolutional Sequence to Se-
quence Learning. Arxiv.

Google. 2019. Actions on google.

Jiatao Gu, Zhengdong Lu, Hang Li, and Victor O.K.
Li. 2016. Incorporating copying mechanism in
sequence-to-sequence learning. ACL.

Matthew Henderson, Blaise Thomson, and Steve
Young. 2013. Deep neural network approach for the
dialog state tracking challenge. SIGDIAL.

Geoffrey Hinton, Li Deng, Dong Yu, George Dahl,
Abdel rahman Mohamed, Navdeep Jaitly, Andrew
Senior, Vincent Vanhoucke, Patrick Nguyen, Tara
Sainath, and Brian Kingsbury. 2012. Deep neural
networks for acoustic modeling in speech recogni-
tion. Signal Processing Magazine.

Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long
short-term memory. Neural Computation.

Norman P. Jouppi, Cliff Young, Nishant Patil, David
Patterson, and Gaurav Agrawal et al. 2017. In-
datacenter performance analysis of a tensor process-
ing unit. ISCA.

John F Kelley. 1984. An iterative design methodology
for user-friendly natural language office information
applications. ACM Transactions on Information Sys-
tems (TOIS).

Diederik P. Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. ICLR.

Ben Krause, Marco Damonte, Mihai Dobre, Daniel
Duma, Joachim Fainberg, Federico Fancellu, Em-
manuel Kahembwe, Jianpeng Cheng, and Bonnie
Webber. 2017. Edina: Building an open domain so-
cialbot with self-dialogues. Arxiv.

Yann Lecun, Yoshua Bengio, and Geoffrey Hinton.
2015. Deep learning. Nature.

Ryan Lowe, Nissan Pow, Iulian V. Serban, and Joelle
Pineau. 2015. The ubuntu dialogue corpus: A large
dataset for research in unstructured multi-turn dia-
logue systems. SIGDIAL.

Stephen Merity, Caiming Xiong, James Bradbury, and
Richard Socher. 2017. Pointer sentinel mixture mod-
els. ICLR.

Nikita Moghe, Siddhartha Arora, Suman Banerjee, and
Mitesh M. Khapra. 2018. Towards exploiting back-
ground knowledge for building conversation sys-
tems. EMNLP.

Aron van den Oord, Sander Dieleman, Heiga Zen,
Karen Simonyan, Oriol Vinyals, Alexander Graves,
Nal Kalchbrenner, Andrew Senior, and Koray
Kavukcuoglu. 2016. Wavenet: A generative model
for raw audio. Arxiv.

Myle Ott, Sergey Edunov, Alexei Baevski, Angela
Fan, Sam Gross, Nathan Ng, David Grangier, and
Michael Auli. 2019. fairseq: A fast, extensible
toolkit for sequence modeling. NAACL Demonstra-
tions.

Alec Radford, Jeff Wu, Rewon Child, David Luan,
Dario Amodei, and Ilya Sutskever. 2019. Language
models are unsupervised multitask learners. Arxiv.

Antoine Raux, Brian Langner, Alan W Black, and Max-
ine Eskenazi. 2003. Lets go: Improving spoken di-
alog systems for the elderly and non-natives. Eu-
rospeech.

Lina Maria Rojas-Barahona, Milica Gasic, Nikola Mrk-
sic, Pei-Hao Su, Stefan Ultes, Tsung-Hsien Wen,
Steve J. Young, and David Vandyke. 2017. A
network-based end-to-end trainable task-oriented di-
alogue system. EACL.

Iulian Vlad Serban, Ryan Lowe, Peter Henderson, Lau-
rent Charlin, and Joelle Pineau. 2017. A survey of
available corpora for building data-driven dialogue
systems. nnn.

Dave Steinkrau, Patrice Y. Simard, and Ian Buck. 2005.
Using gpus for machine learning algorithms. IC-
DAR.

Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.
Sequence to sequence learning with neural networks.
NeurIPS.

Ashish Vaswani, Samy Bengio, Eugene Brevdo, Fran-
cois Chollet, Aidan N. Gomez, Stephan Gouws,
Llion Jones, Łukasz Kaiser, Nal Kalchbrenner, Niki
Parmar, Ryan Sepassi, Noam Shazeer, and Jakob
Uszkoreit. 2018. Tensor2tensor for neural machine
translation. CoRR.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. NeurIPS.

https://developer.amazon.com/alexa-skills-kit/tutorials
https://developers.google.com/actions/overview


4525

Oriol Vinyals and Quoc V. Le. 2015. A neural conver-
sational model. Arxiv.

Joseph Weizenbaum. 1966. Eliza a computer program
for the study of natural language communication be-
tween man and machine. Computational Linguis-
tics.

Jason Williams, Antoine Raux, and Matthew and Hen-
derson. 2016. The dialog state tracking challenge
series: A review. Dialog and Discourse.

Jason D. Williams and Steve J. Young. 2007. Partially
observable markov decision processes for spoken di-
alog systems. Computer Speech & Language.


