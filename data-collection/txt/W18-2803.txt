



















































Language Production Dynamics with Recurrent Neural Networks


Proceedings of the Eighth Workshop on Cognitive Aspects of Computational Language Learning and Processing, pages 17–26
Melbourne, Australia, July 19, 2018. c©2018 Association for Computational Linguistics

17

Language Production Dynamics with Recurrent Neural Networks

Jesús Calvillo1,2 and Matthew W. Crocker1
Saarland University1

Saarbrücken, Germany
Penn State Applied Cognitive Science Lab2

University Park, PA USA
{jesusc,crocker}@coli.uni-saarland.de

Abstract

We present an analysis of the inter-
nal mechanism of the recurrent neural
model of sentence production presented
by Calvillo et al. (2016). The results show
clear patterns of computation related to
each layer in the network allowing to in-
fer an algorithmic account, where the se-
mantics activates the semantically related
words, then each word generated at each
time step activates syntactic and seman-
tic constraints on possible continuations,
while the recurrence preserves informa-
tion through time. We propose that such
insights could generalize to other models
with similar architecture, including some
used in computational linguistics for lan-
guage modeling, machine translation and
image caption generation.

1 Introduction

A Recurrent Neural Network (RNN) is an artifi-
cial neural network that contains at least one layer
whose activation at a time step t serves as input
to itself at a time step t + 1. Theoretically, RNNs
have been shown to be at least as powerful as a
Turing Machine (Siegelmann and Sontag, 1995;
Siegelmann, 2012). Empirically, in computational
linguistics they achieve remarkable results in sev-
eral tasks, most notably in language modeling and
machine translation (e.g. Sutskever et al., 2014;
Mikolov et al., 2010). In the human language pro-
cessing literature, they have been used to model
language comprehension (e.g. Frank et al., 2009;
Brouwer, 2014; Rabovsky et al., 2016) and pro-
duction (e.g. Calvillo et al., 2016; Chang et al.,
2006).

In spite of their success, RNNs are often used
as a black box with little understanding of their

internal dynamics, and rather evaluating them in
terms of prediction accuracy. This is due to the
typically high dimensionality of the internal states
of the network, coupled with highly complex in-
teractions between layers.

Here we try to open the black box present-
ing an analysis of the internal behavior of the
sentence production model presented by Calvillo
et al. (2016). This model can be seen as a semanti-
cally conditioned language model that maps a se-
mantic representation onto a sequence of words
forming a sentence, by implementing an exten-
sion of a Simple Recurrent Network (SRN, El-
man, 1990). Because of its simple architecture
and its relatively low dimensionality, this model
can be analyzed as a whole, showing clear patterns
of computation, which could give insights into the
dynamics of larger language models with similar
architecture.

The method that we applied is based on Layer-
wise Relevance Propagation (Bach et al., 2015).
This algorithm starts at the output layer and moves
in the graph towards the input units, tracking the
amount of relevance that each unit in layer li−1 has
on the activation of units in layer li, back to the in-
put units, which are usually human-interpretable.
For a review of this and some other techniques for
interpreting neural networks, see Montavon et al.
(2017); for related work to this paper see Karpathy
et al. (2015); Li et al. (2015); Kádár et al. (2017);
Arras et al. (2016); Ding et al. (2017).

Our analysis reveals that the overall behavior
of the model is approximately as follows: the in-
put semantic representation activates the hidden
units related to all the semantically relevant words,
where words that are normally produced early in
the sentence receive relatively more activation; af-
ter producing a word, the word produced activates
syntactic and semantic constraints for the produc-
tion of the next word, for example, after a deter-



18

miner, all the nouns are activated, similarly, after
a given verb, only semantically fit objects are ac-
tivated; meanwhile, the recurrent units present a
tendency for self-activation, suggesting a mecha-
nism where activation is preserved over time, al-
lowing the model to implement dynamics over
multiple time steps. While some of the results pre-
sented here have been suggested previously, we
present a holistic integrative view of the internal
mechanics of the model, in contrast to previous
analyses that focus on specific examples.

The next subsection describes the semantic rep-
resentations used by the model. Section 2 de-
scribes the language production model. Section 3
presents the analysis. Discussion and Conclusion
are presented in sections 4 and 5 respectively.

1.1 Semantic Representations

The semantic representations were derived from
the Distributed Situation Space model (DSS,
Frank et al., 2003, 2009), which defines a
microworld in terms of a finite set of basic
events (e.g., play(charlie,chess)). Basic events
can be conjoined to form complex events (e.g.,
play(charlie,chess)∧win(charlie)). However, the
microworld poses both hard and probabilistic con-
straints on event co-occurrence, where some com-
plex events are very common, and some others im-
possible to happen.

Frank et al. (2009) defined a microworld con-
sisting of 44 basic events centered around three
people. Then they built a situation space by sam-
pling 25, 000 observations, where each observa-
tion is encoded by setting basic events that are the
case to 1 and 0 otherwise (see Table 1). The re-
sulting matrix encodes then all knowledge about
the microworld, where each column, also called
situation vector, represents the meaning of each
basic event in terms of the observations in which
the event is true. Finally, they reduced the dimen-
sionality of the situation vectors to 150 dimensions
using a competitive layer algorithm.

The language production model of Calvillo
et al. (2016) uses the same microworld as Frank
et al. (2009), however, the situation vectors were
converted to belief vectors. Each dimension of
the latter is equal to the conditional probabil-
ity of each basic event given the original 25k-
dimensional situation vector associated to each
sentence1. The result is a 44-dimensional vec-

1This vector is computed by calculating the dot prod-

pl
ay

(c
h,

ch
es

s)

pl
ay

(c
h,

hi
de

&
se

ek
)

pl
ay

(c
h,

so
cc

er
)

. . . m
an

ne
r(

w
in

,d
iff

)

observation1 1 0 0 . . . 1
observation2 0 1 0 . . . 1
. . . . . . . . . .
observation25000 0 1 0 . . . 0

Table 1: Situation Space.

tor where each dimension gives an intuition of
the state-of-affairs that is being represented. For
example, for the sentence “Charlie plays chess.”,
the dimension corresponding to the basic event
play(charlie,chess) would have a value of 1.0,
the basic event play(charlie,bedroom) would also
have a value of 1.0 because that is the only place
where chess can be played, nonetheless, the di-
mension of play(heidi,chess) would be less than
1.0 because Heidi does not always play chess
whenever Charlie does.

2 Language Production Model

The model architecture can be seen in Figure 1. It
consists of a 45-dimensional input layer, contain-
ing the semantic representation dss of the sentence
to be produced, plus one bit indicating the model
to produce an active (1) or passive (0) sentence.

At each time step t, activation of the input layer
propagates to a 120-dimensional hidden recurrent
(sigmoid) layer2. This layer also receives a copy
of its own activation ht−1 at time-step t− 1 (zeros
at t = 0) through context units; and the identity
of the word mont−1 produced at time-step t − 1
(zeros at t = 0) through monitoring units, where
only the unit corresponding to the word produced
at time-step t − 1 is activated. More formally, ac-
tivation of the hidden layer is given by:

ht = σ(Wih·dss+Whh·ht−1+Wmh·mont−1+bh)
(1)

where Wih is the weight matrix connecting the in-
put layer to the hidden layer, Whh is the weight

uct between the situation space matrix and the original 25k-
dimensional situation vector, and then normalizing each di-
mension of the resulting vector by the sum over the dimen-
sions of the original 25k-dimensional situation vector.

2While the model by Calvillo et al. (2016) uses an htan
activation function, here we use a sigmoid activation because
it simplifies the analysis, however there was no difference in
performance between the two configurations.



19

matrix connecting the hidden layer to itself, Wmh
is the matrix connecting the monitoring units to
the hidden layer, and bh corresponds to the bias
unit of the hidden layer.

Then, the activation of the hidden layer ht is
propagated to a 43-dimensional softmax output
layer, yielding a probability distribution over the
vocabulary:

outputt = softmax(Who · ht + bo) (2)

where Who is the weight matrix connecting the
hidden layer to the output layer and bo is the vector
corresponding to the output bias unit.

The word produced at time-step t is defined as
the one with highest probability. The model stops
when a period has been produced.

Figure 1: Model architecture.

2.1 Examples Set
The dataset that was used consists of a set of pairs
{(dss1, ϕ1), . . . , (dssn, ϕn))} where each dssi
corresponds to a belief vector plus one bit indicat-
ing the model to produce an active sentence (1) or
a passive one (0); and ϕi = {sent1, . . . , sentk}
where sentj is a sentence, a sequence of words
word1, . . . , wordn, expressing the information
contained in dssi. Each set ϕi represents all the
possible sentences that express the information
contained in dssi and in the expected voice.

The sentences are those generated by the mi-
crolanguage defined by Frank et al. (2009). This
microlanguage consists of 40 words that can be
combined into 13556 sentences according to its
grammar. The grammar was minimally modified
by introducing the determiners “a” and “the”, and
adding a period to the sentences, leaving a total of
43 vocabulary items.

Sentences that expressed unlawful situations ac-
cording to the microworld rules, and therefore
whose situation vectors were empty, were dis-
carded; leaving a total of 8201 lawful sentences

and 782 unique DSS representations. Each dssi is
related on average to 6.91 (σ = 7.13) sentences,
with a maximum of 130.

2.2 Training and Evaluation

The model was trained using cross-entropy back-
propagation (Rumelhart et al., 1986) with weight
updates after each word. All weights on the
projections between layers were initialized with
random values drawn from a normal distribution
N (0, 0.1). The weights on the bias projections
were initially set to zero.

During training, the monitoring units were set at
time t to what the model was supposed to produce
at time t− 1 (zeros for t = 0). During testing, the
monitoring units are set to 1.0 for the word that is
actually produced and 0.0 everywhere else.

The model was trained for a maximum of 200
epochs, each epoch consisting of a full presenta-
tion of the training set, which was randomized be-
fore each epoch. Each item in this set is a pair
(dssi, sent), where sent is a sentence related to
dssi, such that there is one training item per sen-
tence related to each dssi. We employed an initial
learning rate of 0.124 which was halved each time
there was no improvement of performance on the
training set during 15 epochs. No momentum was
used. Training halted if the maximum number of
epochs was reached or if there was no performance
improvement on the training set over 40 epochs.

The model was evaluated using a 10-fold cross-
validation schema, with 5 testing conditions as-
sessing different levels of generalization. A full
report can be seen in Calvillo et al. (2016). For a
given semantic representation dssi, a Levenshtein
similarity value was obtained comparing the sen-
tence produced with the most similar sentence in
ϕi. The performance was very high, obtaining an
average across conditions of 97.1% in similarity
scores, with 88.57% of perfect matches.

3 Production Dynamics

We based our analysis on Layer-wise Relevance
Propagation (Bach et al., 2015). The algorithm
consists of identifying for each unit in layer li, the
units in the layer immediately before li−1 that are
most important for the activation of that unit. The
process starts with the units in the output layer and
moves toward and up to the input units, similar to
the backpropagation algorithm.

An aspect that facilitates the analysis of this ar-



20

chitecture is that the activation of all layers is pos-
itive, ranging from 0 to 1. Then, the difference be-
tween activation or inhibition of any unit onto an-
other is given by the sign of the connection weight
between them. Thus, units inhibiting a particu-
lar unit ui will be those with a negative connec-
tion weight to ui, and activating units will be those
with a positive connection weight to ui.

Having this in mind, we performed the analy-
sis. In this architecture the output layer depends
solely on the activation of the recurrent hidden
layer. Thus, we will first analyze the influence of
the hidden layer onto the output layer, and later we
will see how monitoring, input and context units
affect production via the hidden layer.

3.1 Word-Producing Hidden Units

As the first step, we would like to know which
hidden units are most relevant for the production
of each word. We begin by identifying the hid-
den layer activation patterns that co-occur with the
production of each word. In order to do so, we fed
the model with the training set. For each training
item, the model was given as input the correspond-
ing semantic representation, and at each time step
the monitoring units were set according to the cor-
responding sentence of the training item. This is
very similar to one epoch of training, except that
no weight updates were made. During this pro-
cess, for each time a word had an activation greater
than 0.2, the activation of the hidden layer was
saved. This value was chosen in order to record ac-
tivation patterns where the target word was clearly
activated. At the end, for each word ok we ob-
tained a set of vectors, each vector corresponding
to a pattern of activation of the hidden layer that
led to the activation of ok. Then we averaged these
vectors, obtaining a vector that shows which hid-
den units are active/inactive during the production
of ok, in general and not just for a single instance,
providing us with a more general perspective of
the dynamics of the model for each word.

Having these patterns, we can further infer the
direction and magnitude of their effect by looking
at the connection weights that connect the hidden
layer to the output layer.

A hidden unit hj having a high average activa-
tion aj when producing a word ok means in gen-
eral that hj is relevant for ok. However, if the
weight connecting hj to ok is close to 0, then the
production of ok will not be so affected by hj . In

this case, it could be that hj is only indirectly af-
fecting the production of ok by activating/inhibit-
ing other words.

Intuitively, hidden units can lead to the produc-
tion of ok directly by activating ok or indirectly by
inhibiting other words. Similarly, they can lead to
the inhibition of ok directly by inhibiting ok, or
indirectly by activating other words that compete
against ok. Because of the large number of con-
figurations that can possibly influence production,
we will only focus on direct activation/inhibition.

For the case of activation, we obtain a score
Ahjok conveying the relevance of hidden unit hj
on the activation of word ok, equal to the average
activation that ok receives from hj when ok is pro-
duced, normalized by the sum of all activation that
ok receives:

Ahjok =
akjw

+
jk∑

j′
akj′w

+
j′k

(3)

where akj is the average activation of unit hj when
the word ok is produced, and w+jk is the positive
weight connecting hj to ok. This score is only de-
fined for hidden units with a positive connection
weight to ok, which we call activating units.

Inhibiting hidden units are units with negative
weights to a word ok. For inhibition the average
activation of an inhibiting hidden unit during the
production of ok is expected to be close to 0. Then,
the connection weight is irrelevant, as the product
would be close to 0 as well. Thus, for inhibition
we do not take into account the average activation,
but rather its complement. That is, for each hidden
unit hj , with average activation aj , we obtain 1−
aj and multiply it by the corresponding connection
weight. The result gives us the relevance regarding
inhibition of each hidden unit on a particular word:

Ihjok = −
(1− akj )w

−
jk∑

j′
(1− akj′)w

−
j′k

(4)

Based on these definitions, for each hidden unit
we obtained activation/inhibition relevance scores
for each word in the vocabulary. This gives us
an idea of the function of each hidden unit. Ex-
amples for some hidden units are shown in Fig-
ure 2, where columns represent hidden units and
rows are words in the output layer3. The first 5

3For better readability, all heatmaps presented
here are also available at https://plot.ly/
˜jesusctCogACLL

https://plot.ly/~jesusctCogACLL
https://plot.ly/~jesusctCogACLL


21

columns show a sample of the relevance patterns
in general, while the rest were chosen because they
show some kind of specialization. With the ex-
ception of Figure 4, the words in these heatmaps
are ordered intuitively according to syntactic and
semantic similarity, having in order: determiners,
nouns related to persons, nouns related to toys and
games, nouns related to locations, verbs, adverbs,
prepositions and the period.

Figure 2: Relevance scores of some hidden units
on output units. Red represents activation, blue
inhibition.

One can see that the model takes advantage of
redundancy and context sensitivity, where hidden
units activate many different words depending on
the context. As a result, production of a specific
word depends on the combined behavior of the
hidden units, where a word is produced if it re-
ceives support from several units.

Nonetheless, some units suggest a specializa-
tion (see also Karpathy et al., 2015), activating/in-
hibiting related words: there are units related to
games (e.g., 0, 80), toys (e.g., 4, 36), places
(e.g., 30, 34, 35, 69), people (e.g., 35, 115), win-
ning/losing (e.g., 10, 115), prepositions (e.g., 30,
36, 111) and adverbs (e.g., 36).

We can also see that similar words have simi-
lar relations with the hidden neurons, suggesting
syntactic/semantic categories. A clear example
are synonyms, with almost identical relevance pat-
terns, as shown by the rows corresponding to foot-
ball/soccer, jigsaw/puzzle and bathroom/shower.

3.2 Monitoring Units

Having the relevance values of the hidden layer,
we can infer the influence that monitoring units
have on the production of each word by looking at
their influence on the hidden layer.

The monitoring units feed the hidden layer with
the identity of the word produced at the previous
time step, where only the unit related to that word
is activated (set to 1). Consequently, their effect on
the hidden layer depends only on their connection
weights. Then, total relevance Rik of the monitor-
ing unit i on the output unit k, is given by:

Rik =
∑
j

wijRjk (5)

where wij is the weight connecting monitoring
unit i to the hidden unit j, and Rjk is the relevance
score of hidden unit j onto output unit k, which
can be activation (Ahjok ) or inhibition (Ihjok ).

Having this, we can further separate and nor-
malize, giving activation Aik and inhibition Iik:

Aik =
R+ik∑
k
R+ik

(6)

Iik = −
R−ik∑
k
R−ik

(7)

Figure 3 presents these scores. In general, each
monitoring unit promotes the activation of words
that are allowed after it. Determiners activate the
possible nouns that can follow them: “a” activates
all toys, “game” and “girl”; and “the” activates
“boy” and all locations. Nouns referring to peo-
ple (e.g., “charlie”) activate all present tense verbs
and the adverbs “inside” and “outside”. Games
and toys activate “is”, in order to form passive con-
structions. Given that locations appear always at
the end of the sentence, they activate the period “.”.
Verbs activate words that can serve as their com-
plements, for example “beats” activates all person-
related nouns. Similarly, prepositions activate all
their possible complements.



22

Figure 3: Relevance scores of monitoring units on output units. Red represents activation, blue inhibition.

Inhibition works very similarly, where mon-
itoring units inhibit words that should not fol-
low them. For example, determiners inhibit all
prepositions, nouns inhibit other nouns as two
nouns never occur together, prepositions inhibit
also other prepositions, etc. Finally, some words
inhibit themselves avoiding repetitions, for exam-
ple “well” and “badly”.

In general we can see that the monitoring units
enforce patterns related at least to bigrams in the
training set, with possibly more long distance de-
pendencies introduced via context units.

3.3 Input Units
Using equation 5, we also computed activation and
inhibition scores for the input units, where i would
be in this case the index of each input unit. In
contrast with monitoring units, many input units
can be active simultaneously. Because of that we
would like to infer not only the direction of their

effect, but also its magnitude in relation to other
input units. Hence, we skipped the normalization
introduced by equations 6 and 7. In this case ac-
tivation and inhibition correspond respectively to
positive and negative values of Rik in equation 5.
The resulting scores are shown in Figure 4.

In general, the input units activate words that
are related to their semantics. For example, the
input unit play(sophia,soccer) activates words re-
lated to sophia, soccer and places where soc-
cer is played (in the street). Similarly, the in-
put unit manner(win,difficulty) activates “beats”,
“difficulty” and “with”, which are used to convey
this aspect. At the same time, each input unit in-
hibits words that are in conflict with its semantics.
For example, the unit play(charlie,hide&seek) in-
hibits words concerning other games and the place
where that game is not allowed (in the street). This
behavior of activation and inhibition can be seen to



23

Figure 4: Relevance scores of input units on output units. Red represents activation, blue inhibition.

some degree in all input units.

Of special interest is the last input unit (actives
in Figure 4), which marks whether the model
should produce an active or a passive sentence.
When this unit is active, words concerning people
are activated (e.g., “charlie”, “someone”, “heidi”);
at the same time, this unit inhibits words concern-
ing games and passive constructions (e.g., “chess”,
“hide&seek”, “is”, “by”). Thus, production of
active or passive constructions seems to be de-
termined by giving relatively more activation to
words related to people for actives, or games for
passives. This seems to reflect experimental evi-
dence that shows that more conceptually available
elements are placed in more prominent grammati-
cal roles (Bock and Warren, 1985; Ferreira, 1994).
In this case, the actives unit learns to promote the
activation of hidden units related to specific con-
cepts depending on the voice of the sentence to be

produced.

At time step 0, the activation of monitoring and
context units is equal to 0. Consequently, the ac-
tivation of the hidden layer at this point only de-
pends on the input semantic representation. Then,
we would expect that the input units should acti-
vate more the words that can appear at the begin-
ning of a sentence, relative to other words. This
would ensure that the words starting a sentence are
correct, afterwards, monitoring and context units
would be able to enforce syntactic and semantic
sequential constraints, such that the resulting sen-
tence is coherent. The words shown in Figure 4
are ordered similarly to the other figures, except
that the first 10 words are those that can appear at
the beginning of a sentence. As one can see, those
words receive relatively more activation/inhibition
than the rest. Furthermore, the actives unit has
a very strong relevance, such that when an active



24

sentence is queried, the words that can start an ac-
tive sentence are more activated.

In sum, the input units influence production by
activating hidden units that are related to the se-
mantics that is to be encoded, while additionally
giving an idea of the word order that they should
follow, specially at time step 0.

3.4 Context Units

At each time step, context units feed the hidden
layer with its own activation at the previous time
step, providing the model with some kind of mem-
ory over possibly unlimited time steps.

We will use the notation hi to refer to a hid-
den unit i in the hidden layer, and ci to refer to
the corresponding context unit which contains the
activation of hi at the previous time step.

A way to preserve information over time is by
reverberating activation over different time steps.
For example, if the hidden unit ha gets active, then
the corresponding context unit ca will be active at
the next time step; if the weight connecting ca to
ha is such that the activation of ca causes the acti-
vation of ha, then this would form a cycle in which
ha will be active indefinitely or until other units in-
troduce inhibition, breaking the cycle.

We analyzed the connection weights between
the context and hidden layers in order to see if
these cycles were present. In such cases, the ef-
fect of ha in the current time step would be simi-
lar to the effect of ca in the next time step. Thus,
for each pair (hi, ci), if the effect of ci is similar
to the one of hi, it would mean that ci is mainly
activating hi or units similar to hi, forming a cy-
cle. Note that if ci does not activate hi directly but
other units similar to hi, it would mean that while
the activation of the specific unit might not be pre-
served, the model would still remain in the same
area within the hidden space.

As example, for the first 15 hidden units Fig-
ure 5 presents these values. For each pair of
columns, the first column represents the direct ef-
fect of each hidden unit on the output, identical to
the values in Figure 2, but normalized for each hid-
den unit; the second column represents the effect
of the corresponding context unit at the next time
step, calculated using the equations 5-7, where in
this case i is the index of each context unit.

The column of the right side (DimCorr) presents
for all hidden units, the correlations between the
relevance values of the hidden units and the rel-

Figure 5: Relevance scores of hidden and context
units. Right: correlations for each word between
the relevance values of the hidden units and the
context units. Bottom: correlations between all
relevance values of each hidden unit and the cor-
responding context unit.

evance values of the context units, related only
to each specific word; intuitively showing the de-
gree to which activation related to each word is
preserved by all hidden units. The results sug-
gest that the context units tend to preserve acti-
vation related to most words, but to different de-
grees, where activation of words related to toys,
locations and adverbs is preserved more than ac-
tivation of words related to people. Out of the 43
words, 11 presented moderate correlation (0.4 ≤
r < 0.6, n = 120, p < 0.00001), and 15 weak
correlation (0.2 ≤ r < 0.4, n = 120, p < 0.11).

The row at the bottom (UnitCorr) presents cor-
relations between all the relevance values of each
hidden unit and the corresponding context unit,
that is, between the values of the two columns
above. As we can see, some units seem to be-
have like memory, while others seem to erase their
content. For example, units 2, 3, 4, 5, 8, 10 and
14 have a high correlation between the hidden
and context relevances, implying a cycle as de-
scribed above, while units 9 and 11 present an an-



25

ticorrelation, which means that the context unit is
actually inhibiting its corresponding hidden unit.
Out of the 120 context units, 14 presented strong
correlation (r ≥ 0.6, n = 43, p < 0.00001),
26 moderate correlation (0.4 ≤ r < 0.6, n =
43, p < 0.006) and 20 weak correlation (0.2 ≤
r < 0.4, n = 43, p < 0.2). Regarding anticorrela-
tion, there were 3 units with moderate anticorrela-
tion (−0.6 ≤ r < −0.4, n = 43, p < 0.0036)
and 6 with weak anticorrelation (−0.4 ≤ r <
−0.2, n = 43, p < 0.2).

As we can see, about half of the context units
have a tendency to preserve their activation, which
varies according to each unit, and to the kind of
information. This suggests a tangible mechanism
that preserves information over time, which in the
case of language is necessary in order to enforce
long distance dependencies.

4 Discussion

In the above sections we separated the language
production model into its different modules in or-
der to see their function. Trying to integrate these
parts into a global explanation of the internal me-
chanics of the model, we arrive to the following:
production starts when the model is fed a seman-
tic representation at time step 0. At this point, the
semantic representation is the only source of in-
formation. Based on it, the model must produce
a word that is in accordance to the semantics and
that is syntactically plausible for the beginning of
the sentence. As we saw, the input units seem to
select the words necessary for production, and de-
pending on the voice expected (active or passive)
more activation is given to the words that can ful-
fill the first position. After the initial word has
been produced, monitoring and context units gain
influence. Monitoring units promote the produc-
tion of words that can follow the previous word,
and inhibit words that should not follow. At the
same time, context units keep information regard-
ing previous and current activation, suggesting a
sort of memory, where information remains latent
until the right time to be produced. This happens
until a period is produced, which halts production.

Considering the high architectural similarity of
the model of Calvillo et al. (2016) with other
models of human language production (e.g., Dell
et al., 1993; Chang et al., 1997), we expect that
these results would also reflect their internal me-
chanics. Furthermore, some models used in com-

putational linguistics also present an architecture
where the main paths of computation are largely
similar to the ones presented here: in language
models, at each time step the word previously pro-
duced is fed to a recurrence that in turn feeds an-
other layer yielding a probability distribution over
the vocabulary (e.g., Mikolov et al., 2010); ad-
ditionally, a semantics is fed into the recurrence
in semantically conditioned models, such as some
used in machine translation (e.g., Sutskever et al.,
2014) or image caption generation (e.g., Chen and
Lawrence Zitnick, 2015). One could argue that
larger language models implement more complex
interactions because of their higher dimensional-
ity or the use of more complex hidden units such
as LSTM (Hochreiter and Schmidhuber, 1997) or
GRU (Cho et al., 2014). Nonetheless, the individ-
ual results presented here are coherent with pre-
vious findings on larger architectures (for exam-
ple, similar words are known to have similar word
embeddings), suggesting that these results can be
generalized to such models.

While some adaptation might be needed for
larger models, the algorithm described above
might serve as intuition of how those models work,
and the methodology outlined here could serve to
test such a hypothesis in future work.

5 Conclusion

We presented an analysis of the internal mecha-
nism of a model of language production that uses
a recurrent neural network at its core. The re-
sults show clear patterns of computation that per-
mit to infer its internal mechanism. Because of
architectural similarity, we expect that this mech-
anism could be generalized to other models of hu-
man language production (e.g., Dell et al., 1993;
Chang et al., 1997), as well as models in compu-
tational linguistics, such as those used in language
modeling (e.g., Mikolov et al., 2010) or machine
translation (e.g., Sutskever et al., 2014). In future
work, the methodology outlined here could also
serve to test such a hypothesis.

Acknowledgments

This work was supported by DFG collaborative re-
search center SFB 1102 ‘Information Density and
Linguistic Encoding’. The first author was addi-
tionally supported by National Science Founda-
tion grant BCS-1734304.



26

References
Leila Arras, Franziska Horn, Grégoire Montavon,

Klaus-Robert Müller, and Wojciech Samek. 2016.
Explaining predictions of non-linear classifiers in
nlp. arXiv preprint arXiv:1606.07298.

Sebastian Bach, Alexander Binder, Grégoire Mon-
tavon, Frederick Klauschen, Klaus-Robert Müller,
and Wojciech Samek. 2015. On pixel-wise explana-
tions for non-linear classifier decisions by layer-wise
relevance propagation. PloS one, 10(7):e0130140.

J Kathryn Bock and Richard K Warren. 1985. Concep-
tual accessibility and syntactic structure in sentence
formulation. Cognition, 21(1):47–67.

Harm Brouwer. 2014. The Electrophysiology of
Language Comprehension: A Neurocomputational
Model. Ph.D. thesis, University of Groningen.

Jesús Calvillo, Harm Brouwer, and Matthew W
Crocker. 2016. Connectionist semantic systematic-
ity in language production. In Proceedings of the
38th Annual Conference of the Cognitive Science
Society. Austin, TX: Cognitive Science Society.

Franklin Chang, Gary S Dell, and Kathryn Bock.
2006. Becoming syntactic. Psychological review,
113(2):234.

Franklin Chang, Zenzi M Griffin, Gary S Dell, and
Kathryn Bock. 1997. Modeling structural priming
as implicit learning. Computational Psycholinguis-
tics, Berkeley, CA, 29:392–417.

Xinlei Chen and C Lawrence Zitnick. 2015. Mind’s
eye: A recurrent visual representation for image cap-
tion generation. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition,
pages 2422–2431.

Kyunghyun Cho, Bart van Merrienboer, Çaglar
Gülçehre, Fethi Bougares, Holger Schwenk, and
Yoshua Bengio. 2014. Learning phrase representa-
tions using RNN encoder-decoder for statistical ma-
chine translation. CoRR, abs/1406.1078.

Gary S Dell, Cornell Juliano, and Anita Govindjee.
1993. Structure and content in language produc-
tion: A theory of frame constraints in phonological
speech errors. Cognitive Science, 17(2):149–195.

Yanzhuo Ding, Yang Liu, Huanbo Luan, and Maosong
Sun. 2017. Visualizing and understanding neural
machine translation. In Proceedings of the 55th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), volume 1,
pages 1150–1159.

Jeffrey L Elman. 1990. Finding structure in time. Cog-
nitive Science, 14(2):179–211.

Fernanda Ferreira. 1994. Choice of passive voice is
affected by verb type and animacy. Journal of Mem-
ory and Language, 33(6):715–736.

Stefan L Frank, Willem FG Haselager, and Iris van
Rooij. 2009. Connectionist semantic systematicity.
Cognition, 110(3):358–379.

Stefan L Frank, Mathieu Koppen, Leo GM Noordman,
and Wietske Vonk. 2003. Modeling knowledge-
based inferences in story comprehension. Cognitive
Science, 27(6):875–910.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Akos Kádár, Grzegorz Chrupała, and Afra Alishahi.
2017. Representation of linguistic form and func-
tion in recurrent neural networks. Computational
Linguistics, 43(4):761–780.

Andrej Karpathy, Justin Johnson, and Fei-Fei Li. 2015.
Visualizing and understanding recurrent networks.
CoRR, abs/1506.02078.

Jiwei Li, Xinlei Chen, Eduard Hovy, and Dan Jurafsky.
2015. Visualizing and understanding neural models
in nlp. arXiv preprint arXiv:1506.01066.

Tomas Mikolov, Martin Karafit, Lukas Burget, Jan Cer-
nock, and Sanjeev Khudanpur. 2010. Recurrent neu-
ral network based language model. In Interspeech,
volume 2, page 3.

Grégoire Montavon, Wojciech Samek, and Klaus-
Robert Müller. 2017. Methods for interpreting and
understanding deep neural networks. arXiv preprint
arXiv:1706.07979.

Milena Rabovsky, Steven S Hansen, and James L Mc-
Clelland. 2016. N400 amplitudes reflect change in
a probabilistic representation of meaning: Evidence
from a connectionist model. In Proceedings of the
38th Annual Conference of the Cognitive Science
Society. Austin, TX: Cognitive Science Society.

David E Rumelhart, Geoffrey E Hinton, and Ronald J
Williams. 1986. Learning representations by back-
propagating errors. nature, 323(6088):533.

Hava T Siegelmann. 2012. Neural networks and ana-
log computation: beyond the Turing limit. Springer
Science & Business Media.

Hava T Siegelmann and Eduardo D Sontag. 1995. On
the computational power of neural nets. Journal of
computer and system sciences, 50(1):132–150.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in neural information process-
ing systems, pages 3104–3112.

http://arxiv.org/abs/1406.1078
http://arxiv.org/abs/1406.1078
http://arxiv.org/abs/1406.1078
http://arxiv.org/abs/1506.02078

