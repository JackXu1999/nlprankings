



















































Single Document Summarization as Tree Induction


Proceedings of NAACL-HLT 2019, pages 1745–1755
Minneapolis, Minnesota, June 2 - June 7, 2019. c©2019 Association for Computational Linguistics

1745

Single Document Summarization
as Tree Induction

Yang Liu, Ivan Titov and Mirella Lapata
Institute for Language, Cognition and Computation

School of Informatics, University of Edinburgh
10 Crichton Street, Edinburgh EH8 9AB

yang.liu2@ed.ac.uk, ititov@inf.ed.ac.uk, mlap@inf.ed.ac.uk

Abstract

In this paper we conceptualize single-
document extractive summarization as a tree
induction problem. In contrast to previous
approaches (Marcu, 1999; Yoshida et al.,
2014) which have relied on linguistically mo-
tivated document representations to generate
summaries, our model induces a multi-root
dependency tree while predicting the output
summary. Each root node in the tree is a
summary sentence, and the subtrees attached
to it are sentences whose content relates
to or explains the summary sentence. We
design a new iterative refinement algorithm: it
induces the trees through repeatedly refining
the structures predicted by previous iterations.
We demonstrate experimentally on two
benchmark datasets that our summarizer1

performs competitively against state-of-the-art
methods.

1 Introduction

Single-document summarization is the task of au-
tomatically generating a shorter version of a doc-
ument while retaining its most important informa-
tion. The task has received much attention in the
natural language processing community due to its
potential for various information access applica-
tions. Examples include tools which digest textual
content (e.g., news, social media, reviews), answer
questions, or provide recommendations.

Of the many summarization paradigms that
have been identified over the years (see Mani 2001
and Nenkova and McKeown 2011 for comprehen-
sive overviews), two have consistently attracted
attention. In abstractive summarization, various
text rewriting operations generate summaries us-
ing words or phrases that were not in the original
text, while extractive approaches form summaries
by copying and concatenating the most important
spans (usually sentences) in a document. Recent

1Our code is publicly available at https://github.
com/nlpyang/SUMO.

approaches to (single-document) extractive sum-
marization frame the task as a sequence labeling
problem taking advantage of the success of neu-
ral network architectures (Bahdanau et al., 2015).
The idea is to predict a label for each sentence
specifying whether it should be included in the
summary. Existing systems mostly rely on recur-
rent neural networks (Hochreiter and Schmidhu-
ber, 1997) to model the document and obtain a
vector representation for each sentence (Nallap-
ati et al., 2017; Cheng and Lapata, 2016). Inter-
sentential relations are captured in a sequential
manner, without taking the structure of the doc-
ument into account, although the latter has been
shown to correlate with what readers perceive as
important in a text (Marcu, 1999). Another prob-
lem in neural-based extractive models is the lack
of interpretability. While capable of identifying
summary sentences, these models are not able to
rationalize their predictions (e.g., a sentence is in
the summary because it describes important con-
tent upon which other related sentences elaborate).

The summarization literature offers examples
of models which exploit the structure of the un-
derlying document, inspired by existing theories
of discourse such as Rhetorical Structure The-
ory (RST; Mann and Thompson 1988). Most ap-
proaches produce summaries based on tree-like
document representations obtained by a parser
trained on discourse annotated corpora (Carlson
et al., 2003; Prasad et al., 2008). For instance,
Marcu (1999) argues that a good summary can
be generated by traversing the RST discourse tree
structure top-down, following nucleus nodes (dis-
course units in RST are characterized regarding
their text importance; nuclei denote central units,
whereas satellites denote peripheral ones). Other
work (Hirao et al., 2013; Yoshida et al., 2014) ex-
tends this idea by transforming RST trees into de-
pendency trees and generating summaries by tree
trimming. Gerani et al. (2014) summarize product
reviews; their system aggregates RST trees rep-

https://github.com/nlpyang/SUMO
https://github.com/nlpyang/SUMO


1746

1. One wily coyote traveled a bit too far from home, and its resulting 
adventure through Harlem had alarmed residents doing a double 
take and scampering to get out of its way Wednesday morning.

2. Police say frightened New Yorkers reported the coyote sighting 
around 9:30 a.m., and an emergency service unit was dispatched 
to find the animal. 

3. The little troublemaker was caught and tranquilized in Trinity 
Cemetery on 155th street and Broadway, and then taken to the 
Wildlife Conservation Society at the Bronx Zoo, authorities said.

4. "The coyote is under evaluation and observation," said Mary Dixon, 
spokesperson for the Wildlife Conservation Society.

5. She said the Department of Environmental Conservation will either 
send the animal to a rescue center or put it back in the wild.

6. According to Adrian Benepe, New York City Parks Commissioner, 
coyotes in Manhattan are rare, but not unheard of.

7. "This is actually the third coyote that has been seen in the last 10 
years," Benepe said.

8. Benepe said there is a theory the coyotes make their way to the 
city from suburban Westchester.

9. He said they probably walk down the Amtrak rail corridor along the 
Hudson River or swim down the Hudson River until they get to the 
city.

1 2 3 4 5 6 7 8 9

Figure 1: Dependency discourse tree for a docu-
ment from the CNN/DailyMail dataset (Hermann et al.,
2015). Blue nodes indicate the roots of the tree
(i.e., summary sentences) and parent-child links indi-
cate dependency relations.

resenting individual reviews into a graph, from
which an abstractive summary is generated. De-
spite the intuitive appeal of discourse structure for
the summarization task, the reliance on a parser
which is both expensive to obtain (since it must be
trained on labeled data) and error prone, presents
a major obstacle to its widespread use.

Recognizing the merits of structure-aware rep-
resentations for various NLP tasks, recent ef-
forts have focused on learning latent structures
(e.g., parse trees) while optimizing a neural net-
work model for a down-stream task. Various
methods impose structural constraints on the basic
attention mechanism (Kim et al., 2017; Liu and
Lapata, 2018), formulate structure learning as a
reinforcement learning problem (Yogatama et al.,
2017; Williams et al., 2018), or sparsify the set
of possible structures (Niculae et al., 2018). Al-
though latent structures are mostly induced for in-
dividual sentences, Liu and Lapata (2018) induce
dependency-like structures for entire documents.

Drawing inspiration from this work and ex-
isting discourse-informed summarization mod-
els (Marcu, 1999; Hirao et al., 2013), we frame
extractive summarization as a tree induction prob-
lem. Our model represents documents as multi-
root dependency trees where each root node is a
summary sentence, and the subtrees attached to it
are sentences whose content is related to and cov-

ered by the summary sentence. An example of a
document and its corresponding tree is shown in
Figure 1; tree nodes correspond to document sen-
tences; blue nodes represent those which should
be in the summary, dependent nodes relate to or
are subsumed by the parent summary sentence.

We propose a new framework that uses struc-
tured attention (Kim et al., 2017) as both the ob-
jective and attention weights for extractive sum-
marization. Our model is trained end-to-end, it in-
duces document-level dependency trees while pre-
dicting the output summary, and brings more inter-
pretability in the summarization process by help-
ing explain how document content contributes to
the model’s decisions. We design a new itera-
tive structure refinement algorithm, which learns
to induce document-level structures through re-
peatedly refining the trees predicted by previous
iterations and allows the model to infer complex
trees which go beyond simple parent-child rela-
tions (Liu and Lapata, 2018; Kim et al., 2017).
The idea of structure refinement is conceptually
related to recently proposed models for solving it-
erative inference problems (Marino et al., 2018;
Putzky and Welling, 2017; Lee et al., 2018). It
is also related to structured prediction energy net-
works (Belanger et al., 2017) which approach
structured prediction as iterative miminization of
an energy function. However, we are not aware
of any previous work considering structure refine-
ment for tree induction problems.

Our contributions in this work are three-fold: a
novel conceptualization of extractive summariza-
tion as a tree induction problem; a model which
capitalizes on the notion of structured attention to
learn document representations based on iterative
structure refinement; and large-scale evaluation
studies (both automatic and human-based) which
demonstrate that our approach performs competi-
tively against state-of-the-art methods while being
able to rationalize model predictions.

2 Model Description

Let d denote a document containing several sen-
tences [sent1, sent2, · · · , sentm], where senti is
the i-th sentence in the document. Extractive sum-
marization can be defined as the task of assigning a
label yi ∈ {0, 1} to each senti, indicating whether
the sentence should be included in the summary. It
is assumed that summary sentences represent the
most important content of the document.



1747

2.1 Baseline Model

Most extractive models frame summarization as a
classification problem. Recent approaches (Zhang
et al., 2018; Dong et al., 2018; Nallapati et al.,
2017; Cheng and Lapata, 2016) incorporate a neu-
ral network-based encoder to build representations
for sentences and apply a binary classifier over
these representations to predict whether the sen-
tences should be included in the summary. Given
predicted scores r and gold labels y, the loss func-
tion can be defined as:

L = −
m∑
i=1

(yi ln(ri) + (1− yi) ln(1− ri)) (1)

The encoder in extractive summarization mod-
els is usually a recurrent neural network with
Long-Short Term Memory (LSTM; Hochreiter
and Schmidhuber 1997) or Gated Recurrent Units
(GRU; Cho et al. 2014). In this paper, our
baseline encoder builds on the Transformer ar-
chitecture (Vaswani et al., 2017), a recently pro-
posed highly efficient model which has achieved
state-of-the-art performance in machine transla-
tion (Vaswani et al., 2017) and question answer-
ing (Yu et al., 2018). The Transformer aims at
reducing the fundamental constraint of sequential
computation which underlies most architectures
based on RNNs. It eliminates recurrence in favor
of applying a self-attention mechanism which di-
rectly models relationships between all words in a
sentence.

More formally, given a sequence of input vec-
tors {x1,x2, · · · ,xn}, the Transformer is com-
posed of a stack of N identical layers, each of
which has two sub-layers:

h̃l = LayerNorm(hl−1 + MHAtt(hl−1)) (2)

hl = LayerNorm(h̃l + FFN(h̃l)) (3)

where h0 = PosEmb(x) and PosEmb is the
function of adding positional embeddings to the
input; the superscript l indicates layer depth;
LayerNorm is the layer normalization operation
proposed in Ba et al. (2016); MHAtt represents
the multi-head attention mechanism introduced
in Vaswani et al. (2017) which allows the model
to jointly attend to information from different rep-
resentation subspaces (at different positions); and
FFN is a two-layer feed-forward network with
ReLU as hidden activation function.

For our extractive summarization task, the base-
line system is composed of a sentence-level Trans-
former (TS) and a document-level Transformer
(TD), which have the same structure. For each sen-
tence si = [wi1,wi2, · · · ,win] in the input doc-
ument, TS is applied to obtain a contextual repre-
sentation for each word:

[ui1,ui2, · · · ,uin] = TS([wi1,wi2, · · · ,win]) (4)

And the representation of a sentence is acquired
by applying weighted-pooling:

aij = W0u
T
ij (5)

si =
1

n

n∑
j=1

aijuij (6)

Document-level transformer TD takes si as input
and yields a contextual representation for each
sentence:

[v1,v2, · · · ,vm] = TD([s1, s2, · · · , sm]) (7)

Following previous work (Nallapati et al.,
2017), we use a sigmoid function after a linear
transformation to calculate the probability ri of se-
lecting si as a summary sentence:

ri = sigmoid(W1v
T
i ) (8)

2.2 Structured Summarization Model

In the Transformer model sketched above, inter-
sentence relations are modeled by multi-head at-
tention based on softmax functions, which only
capture shallow structural information. Our sum-
marizer, which we call SUMO as a shorthand for
Structured Summarization Model classifies sen-
tences as summary-worthy or not, and simultane-
ously induces the structure of the source document
as a multi-root tree. An overview of SUMO is il-
lustrated in Figure 2. The model has the same
sentence-level encoder TS as the baseline Trans-
former model (see the bottom box in Figure 2), but
differs in two important ways: (a) it uses struc-
tured attention to model the roots (i.e., summary
sentences) of the underlying tree (see the upper
box in Figure 2); and (b) through iterative refine-
ment it is able to progressively infer more complex
structures from past guesses (see the second and
third block in Figure 2).



1748

sent1 sent2 sent3 sent4 sent5 sent6

sent1 sent2 sent3 sent4 sent5 sent6

sent1 sent2 sent3 sent4 sent5 sent6

sent1 sent3

Sentence-level 
Encoder

transformer

Document-level 
Encoder

sent2

transformer transformer

Sructured Attention 
(Iteration 1)

Edges 
Distribution

Roots 
Distribution

Extractive 
Loss

Sructured Attention 
(Iteration 2)

1-Hop 
Propogation

Sructured Attention 
(Iteration 3)

2-Hop 
Propogation

3-Hop 
Propogation

......

Figure 2: Overview of SUMO. A Transformer-based sentence-level en-
coder (yellow box) builds a vector for each sentence. The blue box
presents the document-level encoder; dotted lines indicate iterative ap-
plication of structured attention, where at each iteration the model out-
puts a roots distribution and the extractive loss is calculated based on
gold summary sentences. si indicates the initial representation for
senti; vki indicates the sentence embedding for senti after iteration k.

Structured Attention Assuming document sen-
tences have been already encoded, SUMO first cal-
culates the unnormalized root score r̃i for senti
to indicate the extent to which it might be se-
lected as root in the document tree. It also cal-
culates the unnormalized edge score ẽij for sen-
tence pair 〈senti, sentj〉 indicating the extent to
which senti might be the head of sentj in that
tree (first upper block in Figure 2). To inject struc-
tural bias, SUMO normalizes these scores as the
marginal probabilities of forming edges in the doc-
ument dependency tree.

We use the Tree-Matrix-Theorem (TMT; Koo
et al. 2007; Tutte 1984) to calculate root marginal
probability ri and edge marginal probability eij ,
following the procedure introduced in Liu and La-
pata (2017). As illustrated in Algorithm 1, we
first build the Laplacian matrix L̄ based on un-
normalized scores and calculate marginal proba-
bilities by matrix inverse-based operations (L̄−1).
We refer the interested reader to Koo et al. (2007)
and Liu and Lapata (2017) for more details. In
contrast to Liu and Lapata (2017), who compute
the marginal probabilities of a single-root tree, our
tree has multiple roots since in our task the sum-
mary typically contains multiple sentences. Given
sentence vector si as input, SUMO computes:

r̃i = Wrsi (9)

ẽij = siWes
T
j (10)

ri, eij = TMT(r̃i, ẽij) (11)
Iterative Structure Refinement SUMO essen-
tially reduces summarization to a rooted-tree pars-
ing problem. However, accurately predicting a
tree in one shot is problematic. Firstly, when pre-
dicting the dependency tree, the model has solely

Algorithm 1: Calculate Tree Marginal Proba-

bilities based on Tree-Matrix-Theorem

Function TMT(r̃i, ẽij)l:

Aij =

{

0 if i = j
exp(r̂ij) otherwise

Lij =

{
∑n

i′=1Ai′j if i = j
−Aij otherwise

L̄ij =

{

Lij + exp(r̂i) i = j
Lij otherwise

eij = (1− δ1,j)Aij [L̄
−1]jj

− (1− δi,1)Aij [L̄
−1]ji

ri = exp(r̂i)[L̄
−1]i1

return ri, eij

access to labels for the roots (aka summary sen-
tences), while tree edges are latent and learned
without an explicit training signal. And as pre-
vious work (Liu and Lapata, 2017) has shown, a
single application of TMT leads to shallow tree
structures. Secondly, the calculation of r̃i and ẽij
would be based on first-order features alone, how-
ever, higher-order information pertaining to sib-
lings and grandchildren has proved useful in dis-
course parsing (Carreras, 2007).

We address these issues with an inference al-
gorithm which iteratively infers latent trees. In
contrast to multi-layer neural network architec-
tures like the Transformer or Recursive Neural
Networks (Tai et al., 2015) where word representa-
tions are updated at every layer based on the output
of previous layers, we refine only the tree struc-
ture during each iteration, word representations
are not passed across multiple layers. Empirically,
at early iterations, the model learns shallow and



1749

simple trees, and information propagates mostly
between neighboring nodes; as the structure gets
more refined, information propagates more glob-
ally allowing the model to learn higher-order fea-
tures.

Algorithm 2 provides the details of our refine-
ment procedure. SUMO takes K iterations to learn
the structure of a document. For each sentence,
we initialize a structural vector v0i with sentence
vector si. At iteration k, we use sentence embed-
dings from the previous iteration vk−1 to calculate
unnormalized root r̃ki and edge ẽ

k
ij scores using a

linear transformation with weight W kr and a bilin-
ear transformation with weight W ke , respectively.
Marginal root and edge probabilities are subse-
quently normalized with the TMT to obtain rki
and ekij (see lines 4–6 in Algorithm 2). Then, sen-
tence embeddings are updated with k-Hop Prop-
agation. The latter takes as input the initial sen-
tence representations s rather than sentence em-
beddings vk−1 from the previous layer. In other
words, new embeddings vk are computed from
scratch relying on the structure from the previ-
ous layer. Within the k-Hop-Propagation function
(lines 12–19), edge probabilities ekij are used as
attention weights to propagate information from a
sentence to all other sentences in k hops. pli and c

l
i

represent parent and child vectors, respectively,
while vector zli is updated with contextual infor-
mation at hop l. At the final iteration (lines 9 and
10), the top sentence embeddings vK−1 are used
to calculate the final root probabilities rK .

We define the model’s loss function as the sum-
mation of the losses of all iterations:

L =

K∑
k=1

[y log(rk) + (1− y) log(1− rk)] (12)

SUMO uses the root probabilities of the top layer
as the scores for summary sentences.

The k-Hop-Propagation function resembles the
computation used in Graph Convolution Networks
(Kipf and Welling, 2017; Marcheggiani and Titov,
2017). GCNs have been been recently applied to
latent trees (Corro and Titov, 2019), however not
in combination with iterative refinement.

3 Experiments

In this section we present our experimental setup,
describe the summarization datasets we used, dis-
cuss implementation details, our evaluation proto-
col, and analyze our results.

Algorithm 2: Structured Summarization

Model

Input: Document d

Output: Root probabilities rK after K
iterations

1 Calculate sentence vectors s using
sentence-level Transformer TS

2 v0← s
3 for k ← 1 to K − 1 do
4 Calculate unnormalized root scores:

r̃ki = W
k
r v

k−1
i

5 Calculate unnormalized edge scores:

ẽkij = v
k−1
i W

k
e v

k−1
j

T

6 Calculate marginal probabilities:

rk, ek = TMT(r̃k, ẽk)
7 Update sentence representations:

vk = k-Hop-Propagation(ek, s, k)
8 end
9 Calculate final unnormalized root and edge

scores: r̃Ki = W
K
r v

K−1
i ,

ẽKij = v
K−1
i W

K
e v

K−1
j

T

10 Calculate final root and edge probabilities:

rK , eK = TMT(r̃K , ẽK)
11

12 Function k-Hop-Propagation(e, s, k):

13 z0 ← s
14 for l← 1 to k do
15 pli =

1

n

∑n
j=1 ejiz

l−1
i

16 cli =
1

n

∑n
j=1 eijz

l−1
i

17 zli = tanh(W
k
v [p

l−1
i , c

l−1
i , z

l−1
i ])

18 end

19 return zk

3.1 Summarization Datasets

We evaluated SUMO on two benchmark datasets,
namely the CNN/DailyMail news highlights
dataset (Hermann et al., 2015) and the New
York Times Annotated Corpus (NYT; Sand-
haus 2008). The CNN/DailyMail dataset con-
tains news articles and associated highlights,
i.e., a few bullet points giving a brief overview
of the article. We used the standard splits
of Hermann et al. (2015) for training, valida-
tion, and testing (90,266/1,220/1,093 CNN docu-
ments and 196,961/12,148/10,397 DailyMail doc-
uments). We did not anonymize entities.

The NYT dataset contains 110,540 articles with
abstractive summaries. Following Durrett et al.
(2016), we split these into 100,834 training and
9,706 test examples, based on date of publication
(test is all articles published on January 1, 2007 or
later). We also followed their filtering procedure,
documents with summaries that are shorter than
50 words were removed from the raw dataset. The



1750

CNN DM CNN+DM NYT
Model R-1 R-2 R-L R-1 R-2 R-L R-1 R-2 R-L R-1 R-2 R-L

LEAD-3 29.2 11.2 26.0 40.7 18.3 37.2 39.6 17.7 36.2 35.5 17.3 32.0
Narayan et al. (2018) 30.4 11.7 26.9 41.0 18.8 37.7 40.0 18.2 36.6 41.3 22.0 37.8
Marcu (1999) 25.6 6.10 19.5 31.9 12.4 23.5 26.5 9.80 20.4 29.6 11.2 23.0
Durrett et al. (2016) — — — — — — — — — 40.8 22.3 36.7
See et al. (2017) — — — — — — 39.5 17.3 36.4 42.7 22.1 38.0
Celikyilmaz et al. (2018) — — — — — — 41.7 19.5 37.9 — — —
Transformer (no doc-att) 29.2 11.1 25.6 40.5 18.1 36.8 39.7 17.0 35.9 41.1 21.5 37.0
Transformer (1-layer doc-att) 29.5 11.4 26.0 41.5 18.7 38.0 40.6 18.1 36.7 41.8 22.1 37.8
Transformer (3-layer doc-att) 29.6 11.8 26.3 41.7 18.8 38.0 40.6 18.1 36.9 42.0 22.3 38.2
SUMO (1-layer) 29.5 11.6 26.2 41.6 18.8 37.6 40.5 18.0 36.8 42.2 22.1 38.1
SUMO (3-layer) 29.7 12.0 26.5 42.0 19.1 38.0 41.0 18.4 37.2 42.3 22.7 38.6

Table 1: Test set results on the CNN/DailyMail and NYT datasets using ROUGE F1 (R-1 and R-2 are shorthands
for unigram and bigram overlap, R-L is the longest common subsequence.

filtered test set includes 3,452 test examples out of
the original 9,706. Compared to CNN/DailyMail,
the NYT dataset contains longer and more elabo-
rate summary sentences.

Both datasets contain abstractive gold sum-
maries, which are not readily suited to training
extractive summarization models. A greedy algo-
rithm similar to Nallapati et al. (2017) was used
to generate an oracle summary for each document.
The algorithm explores different combinations of
sentences and generates an oracle consisting of
multiple sentences which maximize the ROUGE
score with the gold summary. We assigned label 1
to sentences selected in the oracle summary and 0
otherwise and trained SUMO on this data.

3.2 Implementation Details

We followed the same training procedure for
SUMO and various Transformer-based baselines.
The vocabulary size was set to 30K. We used
300D word embeddings which were initialized
randomly from N (0, 0.01). The sentence-level
Transformer has 6 layers and the hidden size of
FFN was set to 512. The number of heads in
MHAtt was set to 4. Adam was used for training
(β1 = 0.9, β2 = 0.999). We adopted the learn-
ing rate schedule from Vaswani et al. (2017) with
warming-up on the first 8,000 steps. SUMO and
related Transformer models produced 3-sentence
summaries for each document at test time (for both
CNN/DailyMail and NYT datasets).

3.3 Automatic Evaluation

We evaluated summarization quality using
ROUGE F1 (Lin, 2004). We report unigram and
bigram overlap (ROUGE-1 and ROUGE-2) as
a means of assessing informativeness and the
longest common subsequence (ROUGE-L) as a
means of assessing fluency.

Table 1 summarizes our results. We evalu-
ated two variants of SUMO, with one and three
structured-attention layers. We compared against
a baseline which simply selects the first three sen-
tences in each document (LEAD-3) and several
incarnations of the basic Transformer model in-
troduced in Section 2.1. These include a Trans-
former without document-level self-attention and
two variants with document-level self attention in-
stantiated with one and three layers. Several state-
of-the-art models are also included in Table 1, both
extractive and abstractive.

REFRESH (Narayan et al., 2018) is an extrac-
tive summarization system trained by globally op-
timizing the ROUGE metric with reinforcement
learning. The system of Marcu (1999) is an-
other extractive summarizer based on RST pars-
ing. It uses discourse structures and RST’s notion
of nuclearity to score document sentences in terms
of their importance and selects the most impor-
tant ones as the summary. Our re-implementation
of Marcu (1999) used the parser of Zhao and
Huang (2017) to obtain RST trees. Durrett et al.
(2016) develop a summarization system which in-
tegrates a compression model that enforces gram-
maticality and coherence. See et al. (2017) present
an abstractive summarization system based on



1751

an encoder-decoder architecture. Celikyilmaz et
al.’s (2018) system is state-of-the-art in abstrac-
tive summarization using multiple agents to repre-
sent the document as well a hierarchical attention
mechanism over the agents for decoding.

As far as SUMO is concerned, we observe that
it outperforms a simple Transformer model with-
out any document attention as well as variants
with document attention. SUMO with three layers
of structured attention overall performs best, con-
firming our hypothesis that document-level struc-
ture is beneficial for summarization. The re-
sults in Table 1 also reveal that SUMO and all
Transformer-based models with document atten-
tion (doc-att) outperform LEAD-3 across metrics.
SUMO (3-layer) is competitive or better than state-
of-the-art approaches. Examples of system output
are shown in Table 4.

Finally, we should point out that SUMO is su-
perior to Marcu (1999) even though the latter em-
ploys linguistically informed document represen-
tations.

3.4 Human Evaluation

In addition to automatic evaluation, we also as-
sessed system performance by eliciting human
judgments. Our first evaluation quantified the
degree to which summarization models retain
key information from the document following a
question-answering (QA) paradigm (Clarke and
Lapata, 2010; Narayan et al., 2018). We created a
set of questions based on the gold summary under
the assumption that it highlights the most impor-
tant document content. We then examined whether
participants were able to answer these questions
by reading system summaries alone without ac-
cess to the article. The more questions a system
can answer, the better it is at summarizing the doc-
ument as a whole.

We randomly selected 20 documents from the
CNN/DailyMail and NYT datasets, respectively
and wrote multiple question-answer pairs for each
gold summary. We created 71 questions in total
varying from two to six questions per gold sum-
mary. We asked participants to read the summary
and answer all associated questions as best they
could without access to the original document or
the gold summary. Examples of questions and
their answers are given in Table 4. We adopted
the same scoring mechanism used in Clarke and
Lapata (2010), i.e., a correct answer was marked

CNN+DM NYT
Model Rank QA Rank QA

LEAD 0.07 40.1 -0.18 36.3
Narayan et al. (2018) 0.21 62.4 0.12 46.1
Durrett et al. (2016) — — -0.11 40.1
See et al. (2017) -0.23 36.6 -0.44 35.3
Celikyilmaz et al. (2018) -0.64 37.5 — —
SUMO (3-layer) 0.15 65.3 0.33 57.2
GOLD 0.11 — -0.16 —
ORACLE 0.37 74.6 0.41 67.1

Table 2: System ranking according to human judg-
ments on summary quality and QA-based evaluation.

with a score of one, partially correct answers with
a score of 0.5, and zero otherwise. Answers were
elicited using Amazon’s Mechanical Turk plat-
form. Participants evaluated summaries produced
by the LEAD-3 baseline, our 3-layered SUMO
model and multiple state-of-the-art systems. We
elicited 5 responses per summary.

Table 2 (QA column) presents the results of the
QA-based evaluation. Based on the summaries
generated by SUMO, participants can answer
65.3% of questions correctly on CNN/DailyMail
and 57.2% on NYT. Summaries produced by
LEAD-3 and comparison systems fare worse, with
REFRESH (Narayan et al., 2018) coming close
to SUMO on CNN/DailyMail but not on NYT.
Overall, we observe there is room for improve-
ment since no system comes close to the extractive
oracle, indicating that improved sentence selec-
tion would bring further performance gains to ex-
tractive approaches. Between-systems differences
are all statistically significant (using a one-way
ANOVA with posthoc Tukey HSD tests; p < 0.01)
with the exception of LEAD-3 and See et al.
(2017) in both CNN+DM and NTY, Narayan et al.
(2018) and SUMO in both CNN+DM and NTY,
and LEAD-3 and Durrett et al. (2016) in NYT.

Our second evaluation study assessed the over-
all quality of the summaries by asking participants
to rank them taking into account the following
criteria: Informativeness , Fluency, and Succinct-
ness. The study was conducted on the Amazon
Mechanical Turk platform using Best-Worst Scal-
ing (Louviere et al., 2015), a less labor-intensive
alternative to paired comparisons that has been
shown to produce more reliable results than rat-
ing scales (Kiritchenko and Mohammad, 2017).
Participants were presented with a document and



1752

CNN+DM NYT
P H EA P H EA

Parser 24.8 8.9 — 18.7 10.6 —
SUMO (1-layer) 69.0 2.9 23.1 54.7 3.6 20.6
SUMO (3-layer) 52.7 3.7 25.3 45.1 6.2 21.6
Left Branching — — 21.4 — — 21.3
Right Branching — — 7.3 — — 6.7

Table 3: Descriptive statistics Projectivity(%), Height
and EdgeAgreement(%) for dependency trees pro-
duced by our model and the RST discourse parser
of Zhao and Huang (2017). Results are shown on the
CNN/DailyMail and NYT test sets.

summaries generated from 3 out of 7 systems and
were asked to decide which summary was better
and which one was worse, taking into account the
criteria mentioned above. We used the same 20
documents from each dataset as in our QA evalu-
ation and elicited 5 responses per comparison.

The rating of each system was computed as the
percentage of times it was chosen as best minus
the times it was selected as worst. Ratings range
from -1 (worst) to 1 (best). As shown in Ta-
ble 2 (Rank column), participants overwhelming
prefer the extractive oracle summaries followed
by SUMO and REFRESH (Narayan et al., 2018).
Abstractive systems (Celikyilmaz et al., 2018; See
et al., 2017; Durrett et al., 2016) perform rela-
tively poorly in this evaluation; we suspect that
humans are less forgiving to fluency errors and
slightly incoherent summaries. Interestingly, gold
summaries fare worse than the oracle and extrac-
tive systems. Albeit fluent, gold summaries natu-
rally contain less detail compared to oracle-based
ones; on virtue of being abstracts, they are writ-
ten in a telegraphic style, often in conversational
language while participants prefer the more lucid
style of the extracts. All pairwise comparisons
among systems are statistically significant (using
a one-way ANOVA with post-hoc Tukey HSD
tests; p < 0.01) except LEAD-3 and See et al.
(2017) in both CNN+DM and NTY, Narayan et al.
(2018) and SUMO in both CNN+DM and NTY,
and LEAD and Durrett et al. (2016) in NYT.

3.5 Evaluation of the Induced Structures

To gain further insight into the structures learned
by SUMO, we inspected the trees it produces.
Specifically, we used the Chu-Liu-Edmonds algo-
rithm (Chu and Liu, 1965; Edmonds, 1967) to ex-
tract the maximum spanning tree from the atten-

tion scores. We report various statistics on the
characteristics of the induced trees across datasets
in Table 3. We also examine the trees learned from
different SUMO variants (with different numbers
of iterations) in order to establish whether the iter-
ative process yields better structures.

Specifically, we compared the dependency trees
obtained from our model to those produced by a
discourse parser (Zhao and Huang, 2017) trained
on a corpus which combines annotations from
the RST treebank (Carlson et al., 2003) and the
Penn Treebank (Marcus et al., 1993). Unlike tra-
ditional RST discourse parsers (Feng and Hirst,
2014), which first segment a document into Ele-
mentary Discourse Units (EDUs) and then build a
discourse tree with the EDUs2 as leaves, Zhao and
Huang (2017) parse a document into an RST tree
along with its syntax subtrees without segmenting
it into EDUs. The outputs of their parser are ide-
ally suited for comparison with our model, since
we only care about document-level structures, and
ignore the subtrees within sentence boundaries.
We converted the constituency RST trees obtained
from the discourse parser into dependency trees
using Hirao et al.’s algorithm (2013).

As can be seen in Table 3, the dependency struc-
tures induced by SUMO are simpler compared to
those obtained from the discourse parser. Our
trees are generally shallower, almost half of them
are projective. We also calculated the percent-
age of head-dependency edges that are identical
between learned trees and parser generated ones.
Although SUMO is not exposed to any annotated
trees during training, a number of edges agree with
the outputs of the discourse parser. Moreover, we
observe that the iterative process involving multi-
ple structured attention layers helps generate bet-
ter discourse trees. We also compare SUMO trees
against a left- and right-branching baseline, where
the document is trivially parsed into a left- and
right-branching tree forming a chain-like struc-
ture. As shown in Table 3, SUMO outperforms
these baselines (with the exception of the one-
layered model on NYT). We should also point out
that the edge agreement between SUMO generated
trees and left/right branching trees is low (around
30% on both datasets), indicating that the trees we
learn are different from a simple chain.

2EDUs roughly correspond to clauses.



1753

CNN/DM NYT

G
O

L
D A company called CyArk specializes in digital preservation of threat-

ened ancient and historical architecture.
Founded by an Iraqi-born engineer, it plans to preserve 500 World Her-
itage sites within five years.

Louisiana officials set July 31 deadline for applicants for the Road
Home, grant program for homeowners who lost their houses to hurri-
canes Katrina and Rita.
Program is expected to cost far more than $7.5 billion provided by Fed-
eral Government, in part because many more families have applied than
officials anticipated.
With cutoff date, State hopes to figure out how much more money it
needs to pay for program.
Shortfall is projected to be $2.9 billion.

Q
A

Which company specializes in digital preservation of threatened ancient
and historical architecture? [CyArk]
How many World Heritage sites does the company plan to preserve?
[500]

What is Road Home? [the Louisiana grant program for homeowners
who lost their houses to hurricanes Katrina and Rita]
When is the applicants’ deadline for the Road Home? [July 31]
Why is the program expected to cost far more than $7.5 billion? [many
more families have applied than officials anticipated]
What is the shortfall projected to be? [$2.9 billion]

L
E

A
D

-3

In 2001, the Taliban wiped out 1700 years of history in a matter of
seconds, by blowing up ancient Buddha statues in central Afghanistan
with dynamite.
They proceeded to do so after an attempt at bringing down the 175-foot
tall sculptures with anti-aircraft artillery had failed.
Sadly, the event was just the first in a series of atrocities that have robbed
the world of some of its most prized cultural heritage.

The Road Home, the Louisiana grant program for homeowners who
lost their houses to hurricanes Katrina and Rita, is expected to cost far
more than the $7.5 billion provided by the Federal Government, in part
because many more families have applied than officials had anticipated.
As a result, Louisiana officials on Tuesday night set a July 31 deadline
for applicants, who can receive up to $150,000 to repair or rebuild their
houses.
With the cutoff date, the State hopes to be able to figure out how much
more money it needs to pay for the program.

Se
e

et
al

.(
20

17
)

The Taliban wiped out 1700 years of history in a matter of seconds.
The thought of losing a piece of our collective history is a bleak one.
But if loss can’t be avoided, technology can lend a hand.

Louisiana grant program for homeowners who lost their houses to hur-
ricanes Katrina and Rita is expected to cost far more than $7.5 billion
provided by federal government.
Louisiana officials set July 31 deadline for applicants, who can receive
up to $150,000 to repair or rebuild their houses.

N
ar

ay
an

et
al

.(
20

18
)

Sadly, the event was just the first in a series of atrocities that have robbed
the world of some of its most prized cultural heritage.
But historical architecture is also under threat from calamities which
might well escape our control, such as earthquakes and climate change.
The thought of losing a piece of our collective history is a bleak one.

The Road Home, the Louisiana grant program for homeowners who
lost their houses to hurricanes Katrina and Rita, is expected to cost far
more than the $7.5 billion provided by the federal government, in part
because many more families have applied than officials had anticipated.
With the cutoff date, the State hopes to be able to figure out how much
more money it needs to pay for the program.
The shortfall is projected to be $2.9 billion.

S
U

M
O

In 2001, the Taliban wiped out 1700 years of history in a matter of
seconds, by blowing up ancient Buddha statues in central Afghanistan
with dynamite.
Sadly, the event was just the first in a series of atrocities that have robbed
the world of some of its most prized cultural heritage.
Now Cyark, a non-profit company founded by an Iraqi-born engineer,
is using groundbreaking laser scanning to ensure that – at the very least
– incredibly accurate digital versions of the world’s treasures will stay
with us forever.

The Road Home, the Louisiana grant program for homeowners who
lost their houses to hurricanes Katrina and Rita, is expected to cost far
more than the $7.5 billion provided by the federal government, in part
because many more families have applied than officials had anticipated.
As a result, Louisiana officials on Tuesday night set a July 31 deadline
for applicants, who can receive up to $150,000 to repair or rebuild their
houses.
The shortfall is projected to be $2.9 billion.

Table 4: GOLD human authored summaries, questions based on them (answers shown in square brackets) and
automatic summaries produced by the LEAD-3 baseline, the abstractive system of See et al. (2017), REFRESH
(Narayan et al., 2018), and SUMO for a CNN and NYT (test) article.

4 Conclusions

In this paper we provide a new perspective on ex-
tractive summarization, conceptualizing it as a tree
induction problem. We present SUMO, a Struc-
tured Summarization Model, which induces a
multi-root dependency tree of a document, where
roots are summary-worthy sentences, and subtrees
attached to them are sentences which elaborate or
explain the summary content. SUMO generates
complex trees following an iterative refinement
process which builds latent structures while using
information learned in previous iterations. Exper-
iments on two datasets, show that SUMO performs
competitively against state-of-the-art methods and
induces meaningful tree structures.

In the future, we would like to generalize SUMO
to abstractive summarization (i.e., to learn latent
structure for documents and sentences) and per-
form experiments in a weakly-supervised setting
where summaries are not available but labels can
be extrapolated from the article’s title or topics.

Acknowledgments

We thank Serhii Havrylov for helpful suggestions.
This research is supported by a Google PhD Fel-
lowship to the first author. We gratefully acknowl-
edge the support of the European Research Coun-
cil (Lapata, award number 681760, “Translating
Multiple Modalities into Text”; Titov award num-
ber 678254, “Broad Coverage Semantic Parsing”).



1754

References
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-

ton. 2016. Layer normalization. arXiv preprint
arXiv:1607.06450.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In In Proceedings of
the 3rd International Conference on Learning Rep-
resentations, San Diego, California.

David Belanger, Bishan Yang, and Andrew McCallum.
2017. End-to-end learning for structured prediction
energy networks. ICML.

Lynn Carlson, Daniel Marcu, and Mary Ellen
Okurowski. 2003. Building a discourse-tagged cor-
pus in the framework of rhetorical structure theory.
In Current and new directions in discourse and dia-
logue, pages 85–112. Springer.

Xavier Carreras. 2007. Experiments with a higher-
order projective dependency parser. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL).

Asli Celikyilmaz, Antoine Bosselut, Xiaodong He, and
Yejin Choi. 2018. Deep communicating agents
for abstractive summarization. arXiv preprint
arXiv:1803.10357.

Jianpeng Cheng and Mirella Lapata. 2016. Neural
summarization by extracting sentences and words.
arXiv preprint arXiv:1603.07252.

Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using rnn encoder–decoder
for statistical machine translation. In Proceedings of
the 2014 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1724–
1734, Doha, Qatar.

Yoeng-Jin Chu and Tseng-Hong Liu. 1965. On shortest
arborescence of a directed graph. Scientia Sinica,
14(10):1396.

James Clarke and Mirella Lapata. 2010. Discourse
constraints for document compression. Computa-
tional Linguistics, 36(3):411–441.

Caio Corro and Ivan Titov. 2019. Differentiable
perturb-and-parse: Semi-supervised parsing with a
structured variational autoencoder. In ICLR.

Yue Dong, Yikang Shen, Eric Crawford, Herke van
Hoof, and Jackie Chi Kit Cheung. 2018. Banditsum:
Extractive summarization as a contextual bandit. In
Proceedings of the EMNLP Conference.

Greg Durrett, Taylor Berg-Kirkpatrick, and Dan Klein.
2016. Learning-based single-document summariza-
tion with compression and anaphoricity constraints.
arXiv preprint arXiv:1603.08887.

Jack Edmonds. 1967. Optimum branchings. Journal
of Research of the national Bureau of Standards B,
71(4):233–240.

Vanessa Wei Feng and Graeme Hirst. 2014. A linear-
time bottom-up discourse parser with constraints
and post-editing. In Proceedings of the 52nd Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), volume 1, pages
511–521.

Shima Gerani, Yashar Mehdad, Giuseppe Carenini,
Raymond T Ng, and Bita Nejat. 2014. Abstractive
summarization of product reviews using discourse
structure. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 1602–1613.

Karl Moritz Hermann, Tomas Kocisky, Edward
Grefenstette, Lasse Espeholt, Will Kay, Mustafa Su-
leyman, and Phil Blunsom. 2015. Teaching ma-
chines to read and comprehend. In Advances in Neu-
ral Information Processing Systems, pages 1693–
1701.

Tsutomu Hirao, Yasuhisa Yoshida, Masaaki Nishino,
Norihito Yasuda, and Masaaki Nagata. 2013.
Single-document summarization as a tree knapsack
problem. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1515–1520.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Yoon Kim, Carl Denton, Luong Hoang, and Alexan-
der M Rush. 2017. Structured attention networks.

Thomas N Kipf and Max Welling. 2017. Semi-
supervised classification with graph convolutional
networks. ICLR.

Svetlana Kiritchenko and Saif Mohammad. 2017.
Best-worst scaling more reliable than rating scales:
A case study on sentiment intensity annotation. In
Proceedings of the 55th Annual Meeting of the As-
sociation for Computational Linguistics, pages 465–
470, Vancouver, Canada.

Terry Koo, Amir Globerson, Xavier Carreras Pérez,
and Michael Collins. 2007. Structured prediction
models via the matrix-tree theorem. In Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL), pages 141–150.

Jason Lee, Elman Mansimov, and Kyunghyun Cho.
2018. Deterministic non-autoregressive neural se-
quence modeling by iterative refinement. EMNLP.

Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. In Text Summarization
Branches Out: Proceedings of the ACL-04 Work-
shop, pages 74–81, Barcelona, Spain. Association
for Computational Linguistics.



1755

Yang Liu and Mirella Lapata. 2017. Learning
structured text representations. arXiv preprint
arXiv:1705.09207.

Yang Liu and Mirella Lapata. 2018. Learning struc-
tured text representations. Transactions of the Asso-
ciation for Computational Linguistics, 6:63–75.

Jordan J Louviere, Terry N Flynn, and Anthony Al-
fred John Marley. 2015. Best-worst scaling: The-
ory, methods and applications. Cambridge Univer-
sity Press.

Inderjeet Mani. 2001. Automatic Summarization. John
Benjamins Pub Co.

William C Mann and Sandra A Thompson. 1988.
Rhetorical structure theory: Toward a functional the-
ory of text organization. Text-Interdisciplinary Jour-
nal for the Study of Discourse, 8(3):243–281.

Diego Marcheggiani and Ivan Titov. 2017. Encoding
sentences with graph convolutional networks for se-
mantic role labeling. In Proceedings of the 2017
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1507–1516, Copenhagen,
Denmark. Association for Computational Linguis-
tics.

Daniel Marcu. 1999. Discourse trees are good indica-
tors of importance in text. Advances in automatic
text summarization, 293:123–136.

Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large annotated
corpus of English: The Penn treebank. Computa-
tional Linguistics, 19(2):313–330.

Joseph Marino, Yisong Yue, and Stephan Mandt. 2018.
Iterative amortized inference. In Proceedings of the
35th International Conference on Machine Learn-
ing, pages 3403–3412, Stockholm, Sweden.

Ramesh Nallapati, Feifei Zhai, and Bowen Zhou. 2017.
Summarunner: A recurrent neural network based se-
quence model for extractive summarization of docu-
ments. In AAAI, pages 3075–3081.

Shashi Narayan, Shay B Cohen, and Mirella Lapata.
2018. Ranking sentences for extractive summariza-
tion with reinforcement learning. arXiv preprint
arXiv:1802.08636.

Ani Nenkova and Kathleen McKeown. 2011. Auto-
matic summarization. Foundations and Trends in
Information Retrieval, 5(2–3):103–233.

Vlad Niculae, André F. T. Martins, and Claire Cardie.
2018. Towards dynamic computation graphs via
sparse latent structure. In Proceedings of the 2018
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 905–911.

Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind K Joshi, and Bon-
nie L Webber. 2008. The penn discourse treebank
2.0. In LREC, Marrakech, Morocco. Citeseer.

Patrick Putzky and Max Welling. 2017. Recurrent
inference machines for solving inverse problems.
CoRR, abs/1706.04008.

Evan Sandhaus. 2008. The new york times annotated
corpus. Linguistic Data Consortium. Philadelphia.

Abigail See, Peter J Liu, and Christopher D Man-
ning. 2017. Get to the point: Summarization
with pointer-generator networks. arXiv preprint
arXiv:1704.04368.

Kai Sheng Tai, Richard Socher, and Christopher D.
Manning. 2015. Improved semantic representations
from tree-structured long short-term memory net-
works. In Proceedings of the 2015 Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 1556–1566, Beijing, China.

William Thomas Tutte. 1984. Graph theory.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems, pages 6000–6010.

Adina Williams, Andrew Drozdov, and Samuel R.
Bowman. 2018. Do latent tree learning models iden-
tify meaningful structure in sentences? Transac-
tions of the Association for Computational Linguis-
tics, 6:253–267.

Dani Yogatama, Phil Blunsom, Chris Dyer, Edward
Grefenstette, and Wang Ling. 2017. Learning to
compose words into sentences with reinforcement
learning. In Proceedings of the 5th International
Conference on Learning Representations, Toulon,
France.

Yasuhisa Yoshida, Jun Suzuki, Tsutomu Hirao, and
Masaaki Nagata. 2014. Dependency-based dis-
course parser for single-document summarization.
In Proceedings of the 2014 Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 1834–1839.

Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui
Zhao, Kai Chen, Mohammad Norouzi, and Quoc V
Le. 2018. Qanet: Combining local convolution
with global self-attention for reading comprehen-
sion. arXiv preprint arXiv:1804.09541.

Xingxing Zhang, Mirella Lapata, Furu Wei, and Ming
Zhou. 2018. Neural latent extractive document sum-
marization. In Proceedings of the EMNLP Confer-
ence.

Kai Zhao and Liang Huang. 2017. Joint syntacto-
discourse parsing and the syntacto-discourse tree-
bank. In Proceedings of the 2017 Conference on
Empirical Methods in Natural Language Process-
ing, pages 2117–2123, Copenhagen, Denmark.

https://www.aclweb.org/anthology/D17-1159
https://www.aclweb.org/anthology/D17-1159
https://www.aclweb.org/anthology/D17-1159

