



















































Tag-Enhanced Tree-Structured Neural Networks for Implicit Discourse Relation Classification


Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 496–505,
Taipei, Taiwan, November 27 – December 1, 2017 c©2017 AFNLP

Tag-Enhanced Tree-Structured Neural Networks for Implicit Discourse
Relation Classification

Yizhong Wang1 Sujian Li1,2 Jingfeng Yang1 Xu Sun1 Houfeng Wang1,2
1Key Laboratory of Computational Linguistics, Peking University, MOE, China
2Collaborative Innovation Center for Language Ability, Xuzhou, Jiangsu, China

{yizhong, lisujian, yjfllpyym, xusun, wanghf}@pku.edu.cn

Abstract

Identifying implicit discourse relations be-
tween text spans is a challenging task be-
cause it requires understanding the mean-
ing of the text. To tackle this task, re-
cent studies have tried several deep learn-
ing methods but few of them exploited the
syntactic information. In this work, we
explore the idea of incorporating syntac-
tic parse tree into neural networks. Specif-
ically, we employ the Tree-LSTM model
and Tree-GRU model, which are based
on the tree structure, to encode the argu-
ments in a relation. Moreover, we fur-
ther leverage the constituent tags to con-
trol the semantic composition process in
these tree-structured neural networks. Ex-
perimental results show that our method
achieves state-of-the-art performance on
PDTB corpus.

1 Introduction

It is widely agreed that text units such as clauses
or sentences are usually not isolated. Instead, they
correlate with each other to form coherent and
meaningful discourse together. To analyze how
text is organized, discourse parsing has gained
much attention from both the linguistic (Weiss and
Wodak, 2007; Tannen, 2012) and computational
(Marcu, 1997; Soricut and Marcu, 2003) commu-
nities, but the current performance is far from sat-
isfactory. The most challenging part is to identify
the discourse relations between text spans, espe-
cially when the discourse connectives (e.g., “be-
cause” and “but”) are not explicitly shown in the
text. Due to the absence of such evident linguistic
clues, trying to model and understand the mean-
ing of the text becomes the key point in identifying
such implicit relations.

economic performance

future

NP

JJ NN

JJ

NP

measure

VB

VP(a)

(b)

to expand

likely

VP

TO VB

JJ

ADJP

is

VBZ

S

the economy

DET NN

NN VP

(Expansion.Restatement.Specification, wsj_0233)

Arg2: A figure above 50 indicates the economy is 

likely to expand

Arg1: The index is intended to measure future 

economic performance

Figure 1: An example of two sentences
with their discourse relation as Expan-
sion.Restatement.Specification. Subfigure (a)
and (b) are partial parse trees of the two important
phrases with yellow background.

Previous studies in this field treat the task of
recognizing implicit discourse relations as a clas-
sification problem and various techniques in se-
mantic modeling have been adopted to encode the
arguments in each relation, ranging from tradi-
tional feature-based models (Lin et al., 2009; Pitler
et al., 2009) to the currently prevailing deep learn-
ing methods (Ji and Eisenstein, 2014; Liu and Li,
2016; Qin et al., 2017). Despite of the superior
ability of the deep learning models, the syntactic

496



information, which proves to be helpful for iden-
tifying discourse relations in many early studies
(Subba and Di Eugenio, 2009; Lin et al., 2009), is
seldom employed by recent work. Therefore we
are curious to explore whether such missing syn-
tactic information can be leveraged in deep learn-
ing methods to further improve the semantic mod-
eling for implicit discourse relation classification.

Tree-structured neural networks, which recur-
sively compose the representation of smaller text
units into larger text spans along the syntactic
parse tree, can tactfully combine syntatic tree
structure with neural network models and recently
achieve great success in several semantic mod-
eling tasks (Eriguchi et al., 2016; Kokkinos and
Potamianos, 2017; Chen et al., 2017). One useful
property of these models is that the representation
of phrases can be naturally captured while com-
puting the representations from bottom up. Taking
Figure 1 for an example, those highlighted phrases
could provide important signals for classifying the
discourse relation. Therefore, we will employ two
latest tree-structuerd models, i.e. the Tree-LSTM
model (Tai et al., 2015; Zhu et al., 2015) and
the Tree-GRU model (Kokkinos and Potamianos,
2017), in our work. Hopefully, these models can
learn to preserve or highlight such helpful phrasal
information while encoding the arguments.

Another important syntactic signal comes from
the constituent tags on the tree nodes (e.g., NP, VP,
ADJP). Those tags, derived from the production
rules, describe the generative process of text and
therefore could indicate which part is more impor-
tant in each constituent. For example, consider-
ing a node tagged with NP, its child node tagged
with DT is usually neglectable. Thus we pro-
pose to incorporate this tag information into the
tree-structured neural networks, where those con-
stituent tags can be used to control the semantic
compostion process.

Therefore, in this paper, we will approach
the discourse relation classification task with two
tree-structured neural networks proposed recently
(Tree-LSTM and Tree-GRU ). To our knowledge,
this is the first time these models are applied to dis-
course relation classification. Moreover, we fur-
ther enhance these models by leveraging the con-
stituent tags to compute the gates in these models.
Experiments on PDTB 2.0 (Prasad et al., 2008)
show that the models we propose can achieve
state-of-the-art results.

2 Related Work

2.1 Implicit Discourse Relation Classication
Discourse relation identification is an important
but difficult sub-component of discourse analysis.
One fundamental step forward recently is the re-
lease of the large-scale Penn Discourse TreeBank
(PDTB) (Prasad et al., 2008), which annotates dis-
course relations with their two textual arguments
over the 1 million word Wall Street Journal corpus.
The discourse relations in PDTB are broadly cat-
egorized as either “Explicit” or “Implicit” accord-
ing to whether there are connectives in the original
text that can indicate the sense of the relations. In
the absence of explicit connectives, identifying the
sense of the relations has proved to be much more
difficult (Park and Cardie, 2012; Rutherford and
Xue, 2014) since the inferring is solely based on
the arguments.

Prior work usually tackles this task of implicit
discourse relation identification as a classification
problem with the classes defined in PDTB cor-
pus. Early attempts use traditional various feature-
based methods and the work inspiring us most is
Lin et al. (2009), in which they show that the syn-
tactic parse structure can provide useful signals
for discourse relation classification. More specif-
ically they employ the production rules with con-
stituent tags (e.g., SBJ) as features and get com-
petitive performance. Recently, with the popular-
ity of deep learning methods, many cutting-edge
models are also applied to our task of implicit dis-
course relation classification. Qin et al. (2016)
tries to model the sentences with Convolutional
Neural Networks. Liu and Li (2016) encodes the
text with Long Short Term Memory model and
employ multi-level attention mechanism to cap-
ture important signals. Qin et al. (2017) proposes a
framework based on adversarial network to incor-
porate the connective information. To be noted,
Ji and Eisenstein (2014) adopts Recursive Neu-
ral Network to exploit the representation of sen-
tences and entities, which is the first yet simple
tree-structured neural network applied in this task.

2.2 Tree-Structured Neural Networks
Tree-structured neural networks are one of the
most widely-used deep learning models in natu-
ral language processing. Such neural networks
usually recursively computes the representation of
larger text spans from its constituent units accord-
ing to the syntactic parse tree. Thanks to this

497



compositional nature of text, tree-structured neu-
ral network models show superior ability in a vari-
ety of semantic modeling tasks, such as sentiment
classification (Kokkinos and Potamianos, 2017),
natural language inference (Chen et al., 2017) and
machine translation (Eriguchi et al., 2016).

The earliest and simplest tree-structure neural
network is the Recursive Neural Network pro-
posed by Socher et al. (2011), in which a global
matrix is learned to linearly combine the con-
tituent vectors. This work is further extended
by replacing the global matrix with a global ten-
sor to form the Recursive Neural Tensor Network
(Socher et al., 2013). Based on them, Qian et al.
(2015) first proposes to incorporate tag informa-
tion, which is very similar as our idea described
in Section 3.2, by either choosing a composition
function according to the tag of a phrase (Tag-
Guided RNN/RNTN) or combining the tag em-
beddings with word embeddings (Tag-Embedded
RNN/RNTN). Our method of incorporating tag
information improves from theirs and somewhat
combines these two methods by using the tag em-
bedding to dynamically determine the composi-
tion function via the gates in LSTM or GRU.

One fatal weakness of vanilla RNN/RNTN is
the well-known gradient exploding or vanishing
problem due to the multiple computation steps in
the vertical direction. Therefore Tai et al. (2015)
and Zhu et al. (2015) propose to import the Long
Short Term Memory into tree structured neural
networks and design a novel network architecture
called Tree-LSTM. The adoption of the memory
cell enables the Tree-LSTM model to preserve in-
formation even though the tree becomes very high.
Similar to Tree-LSTM, Kokkinos and Potamianos
(2017) introduces the so-called Tree-GRU net-
work, which replace the LSTM unit with Gated
Recurrent Unit (GRU). With less parameters to
train, Tree-GRU achieves better performance on
the sentiment analysis task. In this work, we will
experiment with these Tree-LSTM and Tree-GRU
models for the semantic modeling in implicit rela-
tion classification.

3 Our Method

This section details the models we use for implicit
discourse relation classification. Given two textual
arguments without explicit connectives, our task is
to classify the discourse relation between them. It
can be viewed as two parts: 1) modeling the se-

mantics of the two arguments; 2) classifying the
relations based on the semantics. Our main contri-
bution concentrates on the semantic modeling part
with two types of tree-structured neural networks
described in Section 3.1 and we further illustrate
how to leverage the constituent tags to enhance
these two models in Section 3.2. In Section 3.3,
we will shortly introduce the relation classifier and
the training procedure of our model. The architec-
ture of our system is illustrated in Figure 2.

Arg1 Word
Embedding

Arg2 Word
Embedding

Relation
Classifier

𝑟" 𝑟#

Pr	(𝑦)|𝑟", 𝑟#)

En
co
de
r Encoder

Figure 2: Architecture of our discourse relation
classification model. Layers with the same color
share the same parameters.

3.1 Modeling the Arguments with
Tree-Structured Neural Networks

In a typical tree-structured neural network, given a
parse tree of the text, the semantic representations
of smaller text units are recursively composed to
compute the representation of larger text spans and
finally compute the representation for the whole
text (e.g., sentence). In this work, we will con-
struct our models based on the constituency parse
tree, as is shown in Figure 1. Following previ-
ous convention (Eriguchi et al., 2016; Zhao et al.,
2017), we convert the general parse tree, where
the branching factor may be arbitrary, into a bi-
nary tree so that we only need to consider the left
and right children at each step. Then the following
Tree-LSTM and Tree-GRU models can be used to
obtain a vector representation of each argument.

Tree-LSTM Model. In a standard sequential
LSTM model, the LSTM unit is repeated at each

498



step to take the word at current step and previous
output as its input, update its memory cell and out-
put a new hidden vector. In the Tree-LSTM model,
a similar LSTM unit is applied to each node in the
tree in a bottom-up manner. Since each internal
node in the binary parse tree has two children, the
Tree-LSTM unit has to consider information from
two preceding nodes, as opposed to the single pre-
ceding node in the sequential LSTM model. Each
Tree-LSTM unit (indexed by j) contains an input
gate ij , a forget gate fj 1 and an output gate oj .
The computation equations at node j are as fol-
lows:

ij = σ
(
W (i)xj + U (i)

[
hLj , h

R
j

])
(1)

fj = σ
(
W (f)xj + U (f)

[
hLj , h

R
j

])
(2)

oj = σ
(
W (o)xj + U (o)

[
hLj , h

R
j

])
(3)

uj = tanh
(
W (u)xj + U (u)

[
hLj , h

R
j

])
(4)

cj = ij ⊙ uj + fj ⊙ cLj + fj ⊙ cRj (5)
hj = oj ⊙ tanh (cj) (6)

where xj is the embedded word input at current
node j, σ denotes the logistic sigmoid function
and ⊙ denotes element-wise multiplication. hLj ,
hRj are the output hidden vectors of the left and
right children, and cLj , c

R
j are the memory cell

states from them, respectively. To save space, we
leave out all the bias terms in affine transforma-
tions and the same is true for other affine transfor-
mations in this paper.

Intuitively, uj can be regarded as a summary of
the inputs at current node, which is then filtered
by ij . The memory from left and right children
are forgotten by fj and then we compose them to-
gether with the new inputs to form the the new
memory cj . At last, part of the information in
memory cj is exposed by oj to generate the output
vector hj for current step. Another thing to note is
that only leaf nodes in the constituency tree have
words as its input, so xj is set to a zero vector in
other cases.

Tree-GRU Model. Similar to Tree-LSTM, the
Tree-GRU model extends the sequential GRU
model to tree structures. The only difference be-
tween Tree-GRU and Tree-LSTM is how they

1The original Binary Tree-LSTM in (Tai et al., 2015) con-
tains separate forget gates for different child nodes but we
find single forget gate performs better in our task.

modulate the flow of information inside the unit.
Specifically, Tree-GRU unit removes the separate
memory cell and only uses two gates to simu-
late the reset and update procedure in informa-
tion gathering. The computation equations in each
Tree-GRU unit are the following:

rj = σ
(
W (r)xj + U (r)

[
hLj , h

R
j

])
(7)

zj = σ
(
W (z)xj + U (z)

[
hLj , h

R
j

])
(8)

h̃j = tanh
(
W (h)xj + U (h)

[
hLj ⊙ rj , hRj ⊙ rj

])
(9)

hj = zj ⊙ h̃j + (1− zj)⊙
(
hLj + h

R
j

)
(10)

where rj is the reset gate and zj is the update gate.
The reset gate allows the network to forget pre-
vious computed representations, while the update
gate decides the degree of update to the hidden
state. There is no memory cell in Tree-GRU, with
only hLj and h

R
j as the hidden states from the left

and right children.

3.2 Controlling the Semantic Composition
with Constituent Tags

The constituent tag in a parse tree describes the
grammatical role of its corresponding constituent
in the context. Bies et al. (1995) defines several
types of constituent tags, including clause-level
tags (e.g., SBAR, SINV, SQ), phrase-level tags
(e.g., NP, VP, PP) and word-level tags (e.g., NN,
VP, JJ). These constituent tags greatly interleave
with the semantics and in some ways can provide
determinant signals for the importance of a con-
stituent. For example, for most of the time, con-
stituents with PP (prepositional phrase) tag are less
important than those with VP (verb phrase) tag.
Therefore we argue that these tags are worth con-
sidering when we compose the semantics in the
tree-structured neural networks.

One way to leverage such tags is using tag-
specific composition functions but that would lead
to large number of parameters and some tags are
very sparse so it’s very hard to train their corre-
sponding parameters sufficiently. To solve this
problem, we propose to use tag embeddings and
dynamically control the composition process via
the gates in our model.

Gates in Tree-LSTM and Tree-GRU units con-
trol the flow of information and thus determine
how the semantics from child nodes are composed

499



to a new representation. Furthermore, these gates
are computed dynamically according to the inputs
at a certain step. Therefore, it’s natural to incor-
porate the tag embeddings in the computation of
these gates. Based on this idea, we propose the
Tag-Enhanced Tree-LSTM model, where the in-
put, forget and output gates in each unit are calcu-
lated as follows:

ij = σ
(
W (i)xj + M (i)tj + U (i)

[
hLj , h

R
j

])
(11)

fj = σ
(
W (f)xj + M (f)tj + U (f)

[
hLj , h

R
j

])
(12)

oj = σ
(
W (o)xj + M (o)tj + U (o)

[
hLj , h

R
j

])
(13)

Similarly, we can have the Tag-Enhanced Tree-
GRU model with new reset and update gates:

rj = σ
(
W (r)xj + M (r)tj + U (r)

[
hLj , h

R
j

])
(14)

zj = σ
(
W (z)xj + M (z)tj + U (z)

[
hLj , h

R
j

])
(15)

where tj is the embedding of the tag at current
node (indexed by j).

3.3 Relation Classification and Training
In our work, the two arguments are encoded with
the same network in order to reduce the number of
parameters. After that we get a vector representa-
tion for each argument, which can be denoted as r1
for argument 1 and r2 for argument 2. Supposing
that there are totally n relation types, the predicted
probability distribution ŷ ∈ Rn is calculated as:

ŷ = softmax
(
W (ŷ) [r1, r2] + b(ŷ)

)
(16)

To train our model, the training objective J is
dened as the cross-entropy loss with L2 regular-
ization:

E (ŷ, y) = −
n∑

j=1

yj × log ŷj (17)

J (θ) =
1
N

N∑
k=1

E (ŷ, y) +
λ

2
∥θ∥2 (18)

tanh

tanh𝝈

𝝈

𝝈

×

×

×

×

+

ℎ𝑗𝐿 ℎ𝑗
𝑅 𝑐𝑗𝐿 𝑐𝑗𝑅

ℎ𝑗 𝑐𝑗

𝑥𝑗

𝑡𝑗

Figure 3: Flow of Information in Tag-Enhanced
Tree-LSTM unit.

𝝈

+

ℎ𝑗

×

𝑡𝑗

×

1-

ℎ𝑗𝐿

𝝈

×

𝑥𝑗

tanh

+

ℎ𝑗𝑅

×

Figure 4: Flow of Information in Tag-Enhanced
Tree-GRU unit.

where ŷ is the predicted probability distribution, y
is the one-hot representation of the gold label and
N is the number of training samples.

4 Experiments

4.1 Experiment Setup

Corpus. We evaluate our method on the Penn
Discourse Treebank (PDTB) (Prasad et al., 2008),
which provides annotations of discourse relations
over the Wall Street Journal corpus. Each relation
instance consists of two arguments, typically adja-
cent pairs of sentences in a text. As is mentioned
in Section 2.1, the relations in PDTB are generally
categorized into either explicit or implicit and our
work focuses on the more challenging implicit re-
lation classification task. Totally, there are 16,224
implicit relation instances in PDTB dataset, with
a three-level hierarchy. The first level is defined
as 4 major classes of the relation, including: Tem-

500



Models
Level-1 Classification Level-2 Classification

Dev Test Dev Test
Bi-LSTM 55.10 56.88 35.02 42.44
Bi-GRU 55.21 57.01 35.34 42.46

Tree-LSTM 56.04 58.89 35.76 43.02
Tree-GRU 55.36 58.98 36.09 43.78

Tag-Enhanced Tree-LSTM 56.97 59.85 35.92 45.21
Tag-Enhanced Tree-GRU 56.63 59.75 36.93 44.55

Table 1: The accuracy score of multi-class classification

poral, Contingency, Comparison and Expansion.
Then for each class, it is further divided into dif-
ferent types, which is supposed to provide finer
pragmatic distinctions. This totally yields 16 rela-
tion types at the second level. At last, a third level
of subtypes is defined for some types according to
the semantic contribution of each argument.

Preprocesssing. Following the common setup
convention (Rutherford and Xue, 2014; Ji and
Eisenstein, 2014; Liu and Li, 2016), we split the
dataset into training set (Sections 2-20), develop-
ment set (Sections 0-1), and test set (Section 21-
22). For preprocessing, we employ the Stanford
CoreNLP toolkit (Manning et al., 2014) to lem-
matize all the words and get the constituency parse
tree for each sentence. Then we convert the parse
tree into binary with right branching and we re-
move the internal nodes that have only one child
so that our binary tree-structured models can be
applied. Small portion of the arguments in PDTB
are composed of multiple sentences. In such cases,
we add a new “Root” node and link the original
root nodes of those sentences to this shared “Root”
before converting the tree into binary.

Multi-Class Classification There are mainly
two ways to set up the classification tasks in pre-
vious work. Early studies (Pitler et al., 2009;
Park and Cardie, 2012) train and evaluate separate
“one-versus-all” classifiers for each discourse re-
lation since the classes are extremely imbalanced
in PDTB. However, recent work put more empha-
sis on the multi-class classification, where the goal
is to identify a discourse relation from all possible
choices. According to Rutherford and Xue (2014),
the multi-class classification setting is more natu-
ral and realistic. Moreover, the multi-class clas-
sifier can directly serve as one building block of
a complete discourse parser (Qin et al., 2017).
Therefore, in this work, we will focus on the multi-

class classification task. Moreover, we will ex-
periment on the classification of both the Level-
1 classes and the Level-2 types, so that we can
compare with most of previous systems and thor-
oughly analyze the performance of our method.

It should be noted that roughly 2% of the im-
plicit relation instances are annotated with more
than one types. Following Ji and Eisenstein
(2014), we treat these different types as multiple
instances while training our model. During test-
ing, if the classifier hits either of the annotated
types, we consider it to be correct. We also fol-
low previous studies (Lin et al., 2009; Ji and Eisen-
stein, 2014) to remove the instances of 5 very rare
relation types in the second level. Therefore we
have totally 11 types to classify for Level-2 classi-
fication and 4 classes for Level-1 classification.

4.2 Model Settings
We tune the hyper-parameters of our Tag-
Enhanced Tree-LSTM model based on the devel-
opment set and other models share the same set
of hyper-parameters. The best-validated hyper-
parameters, including the size of the word embed-
dings ω, the size of the tag embeddings τ , the di-
mension of the Tree-LSTM or Tree-GRU hidden
state d, the learning rate η, the weight of L2 regu-
larization term λ and the batch size b are shown in
Table 2.

ω τ d η λ b

50 50 250 0.01 0.0001 10

Table 2: Hyper-parameters of our model

The Pre-trained 50-dimentional Glove Vec-
tors (Pennington et al., 2014), which is case-
insensitive, are used for initializing the word em-
beddings and they are tuned together with other
parameters in the same learning rate during train-
ing.

501



We adopt the AdaGrad optimizer (Duchi et al.,
2011) for training our model and we validate the
performance every epoch. It takes around 5 hours
(5 epochs) for the Tag-Enhanced Tree-LSTM and
4 hours (6 epochs) for the Tag-Enhanced Tree-
GRU model to converge to the best performance,
using one INTEL(R) Core(TM) I7 3.4GHz CPU
and one NVIDIA GeForce GTX 1080 GPU.

4.3 Results

The evaluation results of our models on both the
Level-1 classification and the Level-2 classifica-
tion are reported in Table 1. Accuracy score is
used to measure the overall performance and we
present our performance on both the development
set and the test set. In addition to the four tree-
structured neural networks described in Section
3, we also implement two baseline models: the
bi-directional LSTM model and the bi-directional
GRU model. The hyper-parameters of these two
models are tuned separately from other models.
Due to the space limitation, we don’t present the
details here. Comparison of these sequential mod-
els with the tree-structured models are expected to
show the effects of tree structures.

From Table 1, we can see that the sequential Bi-
LSTM model and Bi-GRU model perform worst
for our task, which confirms our hypothesis that
the tree-structured neural networks can really cap-
ture some important signals that are missing in the
sequential models.

Furthermore, if we add the tag information
to tree-structured models, both the Tag-Enhanced
Tree-LSTM model and the Tag-Enhanced Tree-
GRU model provide conspicuous improvement
(around 1%) compared with the no-tag version.
This demonstrates the usefulness of those con-
stituent tags and the effectiveness of our method
to incorporate this important feature. Especially,
since both the Tree-LSTM model and Tree-GRU
model rely on the gating machanism to control the
flow of information, this double confirms that the
tag information can help with the computation of
such gates and therefore can be leveraged to con-
trol the semantic composition process.

Another discovery from our results is that the
GRU models performs similarly as its correspond-
ing variant of LSTM model. This conforms to
previous empirical observation in sequential mod-
els that LSTM and GRU have comparable capabil-
ity (Chung et al., 2014). However, the Tree-GRU

Systems Accuracy
Zhang et al. (2015) 55.39

Rutherford and Xue (2014) 55.50
Rutherford and Xue (2015) 57.10

Liu et al. (2016) 57.27
Liu and Li (2016) 57.57

Ji et al. (2016) 59.50
Tag-Enhanced Tree-LSTM 59.85
Tag-Enhanced Tree-GRU 59.75

Table 3: Accuracy (%) for Level-1 multi-class
classification on the test set, compared with other
state-of-the-art systems.

Systems Accuracy
Lin et al. (2009) 40.66

Ji and Eisenstein (2014) 44.59
Qin et al. (2016) 45.04
Qin et al. (2017) 46.23

Tag-Enhanced Tree-LSTM 45.21
Tag-Enhanced Tree-GRU 44.55

Table 4: Accuracy (%) for Level-2 multi-class
classification on the test set, compared with other
state-of-the-art systems.

model have less parameters to train which could
alleviate the problem of overfitting and also cost
less training time.

4.4 Comparison with Other Systems
For a comprehensive study, we compare our mod-
els with other state-of-the-art systems. The sys-
tems that conduct Level-1 classification are re-
ported in Table 3, including:

• Zhang et al. (2015) proposes to use convo-
lutional neural networks to encode the argu-
ments.

• Rutherford and Xue (2014) manually extracts
features to represent the arguments and use
a maximum entropy classifier for classifi-
cation. Rutherford and Xue (2015) further
exploits discourse connectives to enrich the
training data.

• Liu et al. (2016) employs a multi-task frame-
work that can leverage other discourse-
related data to help with the training of dis-
course relation classifier.

• Liu and Li (2016) represents arguments with
LSTM and introduces a multi-level attention

502



mechanism to model the interaction between
the two arguments.

• Ji et al. (2016) treats the discourse relation as
latent variable and proposes to model them
jointly with the sequences of words using a
latent variable recurrent neural network ar-
chitecture.

And in Table 4, we present the following sys-
tems, which focus on Level-2 classification:

• Lin et al. (2009) uses traditional feature-
based model to classify relations. Especially,
constituent and dependency parse trees are
exploited.

• Ji and Eisenstein (2014) models both the se-
mantics of argument and the meaning of en-
tity mention with two recursive neural net-
works, which are then combined to classify
relations.

• Qin et al. (2016) utilize convolutional neural
network for argument modeling and a collab-
orative gated neural network to model their
interaction.

• Qin et al. (2017) proposes to incorporate the
connective information via a novel pipelined
adversarial framework.

The comparison with these latest work shows
that our system achieves currently best perfor-
mance for the Level-1 classification and ranks sec-
ond for the Level-2. With the state-of-the-art per-
formance on both levels, we can verify the effec-
tiveness of our method.

4.5 Qualitative Analysis
To get a deeper insight into the proposed tag-
enhanced models, we project the constituent tag
embeddings learned in our Tag-Enhanced Tree-
LSTM model into two dimensions using the t-SNE
method (Maaten and Hinton, 2008) and normalize
the values in each dimension. The projected em-
beddings are visualized in Figure 5 and we high-
light some representative tags that may share some
kind of commonality in their functions.

According to the definition in Bies et al. (1995),
the tags with red color are all verb-related, while
those with blue color describes noun-related syn-
tax. Despite of some noisy points, we can see
that they are roughly separated into two groups.

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

S

PRP

VPVBD

NP

NN

PP

VBN

IN

PRP$

SBAR

VBG

CC

NNS

TO

VB

RP

ADVP

RB

CD

DT

NNP

POS

QP

ADJP

VBP

WHADJP

MD

JJR

VBZ

JJS

NP-TMPRBR

PRN

-LRB-

WP

NNPS

PDT

NAC

SINV

UCP

FRAG

SBARQ

EX

WHNP

WHPP

WP$

SQ

NX

RBS FW

WHADVP

UH

RRC

INTJ

LST

LS

PRT

Figure 5: t-SNE Visualization (Maaten and Hin-
ton, 2008) of the constituent tag embeddings

In addition, the purple tags correspond to words
or phrase that are wh-related (e.g., what, when,
where, which) and we can see they are distributed
similarly. Moreover, the green tags are, which are
located in the right-bottom corner, all describe the
clause-level constituents.

Therefore, from this visualization, we can con-
clude that the tag embeddings we learned are
somewhat meaningful and really capture some
functionalities of these tags. Since these em-
beddings are used to compute the gates and the
gates further determine the flow of information,
we argue that these tags can indeed help to con-
trol the semantic composition process in our tree-
structured networks.

5 Conclusion

In this work, we propose to use two latest tree-
structured neural networks to model the arguments
for discourse relation classification. The syntactic
parse tree are exploited from two aspects: first, we
leverage the tree structure to recursively compose
semantics in a bottom-up manner; second, the con-
stituent tags are used to control the semantic com-
position process at each step via gating mecha-
nism. Comprehensive experiments show the effec-
tiveness of our proposed method and our system
achieves state-of-the-art performance for the chal-
lenging task of implicit discourse relation classi-
fication. For future work, we will try other types
of syntax embeddings and we are also working on
incorporating structural attention mechanism into
our tree-based models.

503



Acknowledgments

We thank all the anonymous reviewers for their in-
sightful comments on this paper. This work was
partially supported by National Natural Science
Foundation of China (61572049 and 61333018).
The correspondence author of this paper is Sujian
Li.

References
Ann Bies, Mark Ferguson, Karen Katz, Robert Mac-

Intyre, Victoria Tredinnick, Grace Kim, Mary Ann
Marcinkiewicz, and Britta Schasberger. 1995.
Bracketing guidelines for treebank ii style penn tree-
bank project. University of Pennsylvania, 97:100.

Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei, Hui
Jiang, and Diana Inkpen. 2017. Enhanced lstm for
natural language inference. In Proc. ACL.

Junyoung Chung, Çaglar Gülçehre, KyungHyun Cho,
and Yoshua Bengio. 2014. Empirical evaluation of
gated recurrent neural networks on sequence model-
ing. CoRR, abs/1412.3555.

John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. Journal of Machine
Learning Research, 12(Jul):2121–2159.

Akiko Eriguchi, Kazuma Hashimoto, and Yoshi-
masa Tsuruoka. 2016. Tree-to-sequence atten-
tional neural machine translation. arXiv preprint
arXiv:1603.06075.

Yangfeng Ji and Jacob Eisenstein. 2014. One vec-
tor is not enough: Entity-augmented distributional
semantics for discourse relations. arXiv preprint
arXiv:1411.6699.

Yangfeng Ji, Gholamreza Haffari, and Jacob Eisen-
stein. 2016. A latent variable recurrent neural net-
work for discourse relation language models. arXiv
preprint arXiv:1603.01913.

Filippos Kokkinos and Alexandros Potamianos.
2017. Structural attention neural networks for
improved sentiment analysis. arXiv preprint
arXiv:1701.01811.

Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng. 2009.
Recognizing implicit discourse relations in the penn
discourse treebank. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing: Volume 1-Volume 1, pages 343–351.
Association for Computational Linguistics.

Yang Liu and Sujian Li. 2016. Recognizing implicit
discourse relations via repeated reading: Neural net-
works with multi-level attention. arXiv preprint
arXiv:1609.06380.

Yang Liu, Sujian Li, Xiaodong Zhang, and Zhifang Sui.
2016. Implicit discourse relation classification via
multi-task neural networks. In AAAI, pages 2750–
2756.

Laurens van der Maaten and Geoffrey Hinton. 2008.
Visualizing data using t-sne. Journal of Machine
Learning Research, 9(Nov):2579–2605.

Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David Mc-
Closky. 2014. The stanford corenlp natural lan-
guage processing toolkit. In Association for Compu-
tational Linguistics (ACL) System Demonstrations,
pages 55–60.

Daniel Marcu. 1997. The rhetorical parsing of natu-
ral language texts. In Proceedings of the 35th An-
nual Meeting of the Association for Computational
Linguistics and Eighth Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 96–103. Association for Computa-
tional Linguistics.

Joonsuk Park and Claire Cardie. 2012. Improving im-
plicit discourse relation recognition through feature
set optimization. In Proceedings of the SIGDIAL
2012 Conference, The 13th Annual Meeting of the
Special Interest Group on Discourse and Dialogue,
pages 108–112.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. In Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1532–
1543.

Emily Pitler, Annie Louis, and Ani Nenkova. 2009.
Automatic sense prediction for implicit discourse re-
lations in text. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP: Volume 2-Volume
2, pages 683–691. Association for Computational
Linguistics.

Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind K. Joshi, and Bon-
nie L. Webber. 2008. The penn discourse treebank
2.0. In Proceedings of the International Confer-
ence on Language Resources and Evaluation, LREC
2008, 26 May - 1 June 2008, Marrakech, Morocco.

Qiao Qian, Bo Tian, Minlie Huang, Yang Liu, Xuan
Zhu, and Xiaoyan Zhu. 2015. Learning tag embed-
dings and tag-specific composition functions in re-
cursive neural network. In ACL (1), pages 1365–
1374.

Lianhui Qin, Zhisong Zhang, and Hai Zhao. 2016. A
stacking gated neural architecture for implicit dis-
course relation classification. In EMNLP, pages
2263–2270.

504



Lianhui Qin, Zhisong Zhang, Hai Zhao, Zhiting Hu,
and Eric P Xing. 2017. Adversarial connective-
exploiting networks for implicit discourse relation
classification. arXiv preprint arXiv:1704.00217.

Attapol Rutherford and Nianwen Xue. 2014. Discover-
ing implicit discourse relations through brown clus-
ter pair representation and coreference patterns. In
Proceedings of the 14th Conference of the European
Chapter of the Association for Computational Lin-
guistics, 2014, pages 645–654.

Attapol Rutherford and Nianwen Xue. 2015. Improv-
ing the inference of implicit discourse relations via
classifying explicit discourse connectives. In The
2015 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 799–808.

Richard Socher, Cliff C Lin, Chris Manning, and An-
drew Y Ng. 2011. Parsing natural scenes and natu-
ral language with recursive neural networks. In Pro-
ceedings of the 28th international conference on ma-
chine learning (ICML-11), pages 129–136.

Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng,
Christopher Potts, et al. 2013. Recursive deep
models for semantic compositionality over a senti-
ment treebank. In Proceedings of the conference on
empirical methods in natural language processing
(EMNLP), volume 1631, page 1642.

Radu Soricut and Daniel Marcu. 2003. Sentence level
discourse parsing using syntactic and lexical infor-
mation. In Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology-Volume 1, pages 149–156. Association
for Computational Linguistics.

Rajen Subba and Barbara Di Eugenio. 2009. An effec-
tive discourse parser that uses rich linguistic infor-
mation. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 566–574. Association for
Computational Linguistics.

Kai Sheng Tai, Richard Socher, and Christopher D
Manning. 2015. Improved semantic representations
from tree-structured long short-term memory net-
works. arXiv preprint arXiv:1503.00075.

Deborah Tannen. 2012. Discourse analysis–what
speakers do in conversation. Linguistic Society of
America.

Gilbert Weiss and Ruth Wodak. 2007. Critical dis-
course analysis. Springer.

Biao Zhang, Jinsong Su, Deyi Xiong, Yaojie Lu, Hong
Duan, and Junfeng Yao. 2015. Shallow convolu-
tional neural network for implicit discourse relation
recognition. In Proceedings of the 2015 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 2230–2235.

Kai Zhao, Liang Huang, and Mingbo Ma. 2017. Tex-
tual entailment with structured attentions and com-
position. CoRR, abs/1701.01126.

Xiaodan Zhu, Parinaz Sobihani, and Hongyu Guo.
2015. Long short-term memory over recursive
structures. In International Conference on Machine
Learning, pages 1604–1612.

505


