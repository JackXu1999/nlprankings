



















































KSAnswer: Question-answering System of Kangwon National University and Sogang University in the 2016 BioASQ Challenge


Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 45–49,
Berlin, Germany, August 7-12, 2016. c©2016 Association for Computational Linguistics

KSAnswer: Question-answering System of Kangwon National University
and Sogang University in the 2016 BioASQ Challenge

Hyeon-gu Lee1, Minkyoung Kim1, Harksoo Kim1∗, Juae Kim2
Sunjae Kwon2, Jungyun Seo2∗, Jungkyu Choi3, Yi-reun Kim3

Kangwon National University, Chuncheon, Korea1

Sogang University, Seoul, Korea2

Intelligence Lab, LG Electronics, Korea3

{nlphglee, kmink0817, nlpdrkim}@kangwon.ac.kr1
{juaeKim, sj91kwon, seojy}@sogang.ac.kr2
{stanley.choi, yireun.kim }@lge.com3

Abstract

This paper describes a question–
answering system that returns relevant
documents and snippets (with particular
emphasis on snippets) from a large med-
ical document collection. The system is
implemented as part of our participation to
Phase A of Task 4b in the 2016 BioASQ
Challenge. The proposed system retrieves
candidate answer sentences using a
cluster–based language model. Then, it
re–ranks the retrieved top-n sentences
using five independent similarity models
based on shallow semantic analysis. The
experimental results show that the pro-
posed system is the first to find snippets
in batches 2 (MAP 0.0604), 3 (MAP
0.0728), 4 (MAP 0.1182), and 5 (MAP
0.1187).

1 Introduction

BioASQ 2016 is the fourth annual BioASQ chal-
lenge as an established international competition
for large–scale biomedical semantic indexing and
question–answering, since 2013 (Tsatsaronis et
al., 2015). The challenge consists of two tasks:
Task 4a on large–scale online biomedical seman-
tic indexing and Task 4b on biomedical semantic
question–answering. Task 4b is further divided
into two phases: Phase A and Phase B. In Phase
A, participating systems are required to return a
maximum of 10 relevant concepts, documents,
snippets, and triples during five batches. Partic-
ipation in Phase A can be partial, which means
that it is acceptable to participate in only some of
the batches and to return only relevant documents
without snippets, triples, and concepts. This paper

∗Corresponding author.

describes a questionanswering system of Kang-
won National University and Sogang University
submitted for Phase A of Task 4b in BioASQ
2016. The proposed system is focused on return-
ing relevant documents and snippets (with partic-
ular emphasis on snippets).

2 Question-answering system based on
sentence retrieval and re–ranking
techniques

KSAnswer consists of two submodules: A re-
trieval model for finding candidate answer sen-
tences from a large medical collection and a re–
ranking model for determining the final answer
among the retrieved candidate answer sentences.

2.1 Sentence retrieval model

Prior to indexing documents, KSAnswer first
splits documents into a sequence of sentences us-
ing LingPipe (Baldwin et al., 2003). Then, it per-
forms morphological analysis of the sentences and
extracts content words (i.e., proper noun, common
noun, verb, number, and so on) from the sentences.
This is followed by stemming of content words ex-
cept proper nouns using Porter Stemmer (Porter,
1980). Finally, KSAnswer uses the stemmed con-
tent words and the proper nouns as indexing terms.

For cluster–based sentence retrieval, KSAnswer
generates two types of indexing units from the
document collection comprising full data sets of
PubMed journals: a sentence trigram unit and a
document unit. The sentence trigram unit con-
sists of an indexing target sentence and its context
sentences (the previous and the next sentences)
to address the lexical disagreement between a
query and an indexed sentence. If a document
consists of three sentences, KSAnswer generates
three sentence trigrams (NULL–1st sentence–2nd

sentence, 1st sentence–2nd sentence–3rd sentence,

45



Figure 1: Relationship between a query and an answer sentence

2nd sentence–3rd sentence–NULL). The docu-
ment unit consists of a title sentence and abstract
sentences. The document unit assists in address-
ing the lexical disagreement between a query and
a sentence trigram. Then, KSAnswer performs
indexing of each unit and constructs two index-
ing databases using Lucene 4.0.0 (Białecki et al.,
2012).

To rank candidate answer sentences, KSAnswer
uses a cluster–based language model (Liu et al.,
2004; Merkel et al., 2007), as shown in Eq. (1):

SimIR(Q,S) =∝ Simtri(Q,T ) +
(1− ∝)Simdoc(Q,D) (1)

where Simtri(Q,T ) is the similarity of the lan-
guage model between the query Q and the sen-
tence trigram T in the document D. Then,
Simdoc(Q,D) is the similarity of the language
model between the query Q and the document D.
The weighting parameter ∝ has a value between
0 and 1. Finally, SimIR(Q,S) returns similari-
ties between the query Q and the indexing target
sentence S, which is located in the middle of the
sentence trigram T .

2.2 Sentence re–ranking model
Prior to re–ranking of candidate answer sentences,
KSAnswer selects top–n retrieved sentences and
normalizes their similarities, as shown in Eq. (2):

Sim
′
IR(Q,S) =

SimIR(Q,S)−m
σ

(2)

where m and σ are the average and standard de-
viation of similarity scores of top–n retrieved sen-
tences, respectively.

KSAnswer re–ranks the top–n retrieved sen-
tences using five independent similarity models,

namely, SimSNT (Q,S), SimEMB(Q,S),
SimEAT (Q,S), SimFOCUS(Q,S), and
SimME(Q,S). SimSNT (Q,S) is a similar-
ity model between the query Q and the sentence
S, which is located in the middle of the retrieved
sentence trigram. SimEMB(Q,S) is a simi-
larity model between the sentence embedding
of Q and the sentence embedding of S. The
sentence embeddings are constructed by the sum
of position–encoded word vectors in Word2Vec
(so–called position encoding) (Sukhbaatar et al.,
2015). SimEAT (Q,S) is a similarity model
between the expected answer type (EAT; a
category name of expected answer) of Q and
medical entity types (category names of medical
entities) in S. SimFOCUS(Q,S) is a similarity
model between focus words (FOCUS; a clue
word sequence to identify correct answers) in
Q and content words in S. SimME(Q,S) is a
similarity model between medical entities (MEs)
in Q and medical entities in S. For example, in
the sentence “Which drugs are utilized to treat
eosinophilic esophagitis?”, EAT, FOCUS, and
ME are [Chemicals & Drugs], [eosinophilic
esophagitis], and [drugs, eosinophilic esophagi-
tis], respectively. To obtain EAT, FOCUS, and
ME, KSAnswer uses a sentence analyzer based
on pattern matching and machine learning (Kim
et al., 2004). The sentence analyzer extracts
word chunks (generally noun phrases) from a
query using lexico–semantic patterns. Then, it
determines EAT and FOCUS by searching the
syntactic chunks in MetaMap (Aronson et al.,
2006). To obtain MEs, the sentence analyzer
uses a special version of named entity recognizer
based on Conditional Random Fields (CRFs),
which is trained for medical documents (Abacha

46



Figure 2: Vector-based similarity model based on a neural network

et al., 2012). The named entity recognizer extracts
medical entities from a sentence and annotates
them with predefined semantic categories. EAT
and MEs use the same semantic categories as
follows: ACTI (Activities & Behaviors), ANAT
(anatomy), CHEM (chemicals & drugs), CONC
(concepts & ideas), DEVI (devices), DISO (dis-
orders), GENE (genes & molecular sequences),
GEOG (geographic areas), LIVB (living beings),
OBJC (objects), OCCU (occupations), ORGA
(organizations), PHEN (phenomena), PHYS
(physiology), and PROC (procedures). Figure 1
shows a relationship between Q and S at the view
of EAT, FOCUS, and ME.

Eq. (3) shows the similarity scores between a
query and each top–n retrieved sentences for re–
ranking.

ReSim(Q,S) = αSim
′
IR

+(1− α){βSimsnt(Q,S)

+(1− β)
4∑

i=1

γiSim
i
sem(Q,S)},

where 0 ≤ α ≤ 1, 0 ≤ β ≤ 1,
4∑

i=1

γi = 1 (3)

where Simisem(Q,S) is the ith similarity
model among SimEMB(Q,S), SimEAT (Q,S),
SimFOCUS(Q,S), and SimME(Q,S). Then,
∝, β, and γ are the weighting parameters set by
experiments. The word–based similarity models
(i.e., models for calculating similarities between
words in Q and S), such as SimSNT (Q,S),
SimFOCUS(Q,S), and SimME(Q,S), are
calculated using the well-known Okapi BM25
(Robertson et al., 1999). Then, the category–based

similarity model (i.e., a model for calculating
similarities between category names in Q and S),
SimEAT (Q,S), is calculated using OR similarity
of the Paice model (Paice, 1984), as shown in Eq.
(4).

SimEAT (Q,S) =
∑n

i=1(r
i−1wi)∑n

i=1 r
i−1 ,

where 0 ≤ r ≤ 1 and w′is are
considered in descending order (4)

In Eq. (4), wi is a TF·IDF value of the ith word in
ME’s of S that have the same semantic category
with EAT of Q. Finally, the vector–based simi-
larity model, SimEMB(Q,S), is calculated using
a feed–forward neural network with one hidden
layer (Svozil et al., 1997), as shown in Figure 2.

The feed–forward neural network uses the sen-
tence embedding vectors of Q and S as input val-
ues and uses a degree of relevance (from 0 to 1)
between the two sentence embedding vectors as
an output value. It is trained using gold standard
answers as relevant snippets and by using top–n
retrieved sentences except gold standard answers
as irrelevant snippets.

3 Experiments

3.1 Experimental setting

We indexed the full data set of PubMed jour-
nals using Lucene 4.0.0. The number of docu-
ment units was 12,208,342 and the number of sen-
tence trigram units was 99,911,516. The language
model parameters (µ values) for the document and
sentence trigram units were set to 500 and 100, re-
spectively. The weighting parameter ∝ in Eq. (1)

47



was 0.8. Then, the weighting parameters∝, β, and
γi in Eq. (3) were 0.5, 0.9, and 0.3, respectively.

3.2 Experimental results

In Phase A of Task 4b, our best submission was
the first to find snippets in batches 2, 3, 4, and 5.
In batch 1, we indexed the limit set of PubMed and
achieved the second place in finding snippets. Ta-
ble 1 shows the best performances of KSAnswer.

Table 1: Evaluation results of submitted runs

Batch
Document

Precision Recall
F1 MAP

1
0.0840(0.0840) 0.2258(0.1664)
0.1065(0.1116) 0.0486(0.1223)

2
0.1675(0.1675) 0.4056(0.2758)
0.2122(0.2084) 0.0949(0.1905)

3
0.1380(0.1380) 0.3946(0.2686)
0.1786(0.1823) 0.0992(0.2095)

4
0.1720(0.1720) 0.5333(0.3470)
0.2247(0.2300) 0.1257(0.2871)

5
0.1103(0.1103) 0.3752(0.2560)
0.1546(0.1542) 0.0752(0.1742)

Batch
Snippet

Precision Recall
F1 MAP

1
0.0482(0.0418) 0.0952(0.1071)
0.0534(0.0602) 0.0266(0.0738)

2
0.1021(0.0967) 0.1615(0.1930)
0.1104(0.1288) 0.0604(0.1381)

3
0.0873(0.0823) 0.1208(0.1460)
0.0886(0.1053) 0.0728(0.1440)

4
0.1504(0.1377) 0.2023(0.2653)
0.1554(0.1813) 0.1182(0.2549)

5
0.0771(0.0773) 0.1272(0.1434)
0.0798(0.1004) 0.0582(0.1187)

The parenthesized values are informal per-
formances that are calculated using gold stan-
dard answers for each batch. In an ad-
ditional experiment, we found that the de-
gree of the sub-model importance in the re-
ranking model is as follows: SimSNT (Q,S)
>> SimEAT (Q,S) > SimFOCUS(Q,S) ≈
SimME(Q,S) ≈ SimEMB(Q,S)

4 Conclusion

We proposed a question-answering system for
finding candidate answer snippets from a large

medical document collection. The proposed sys-
tem retrieves candidate answer sentences using
cluster–based language model. Then, it re–ranks
top–n retrieved sentences using various similarity
models based on shallow semantic analysis of sen-
tences. In Phase A of task 4b, the proposed system
showed excellent performance by being the first to
find snippets in batches 2,3,4 and 5.

Acknowledgments

This research was supported by LG Electron-
ics. It was also supported by Basic Sci-
ence Research Program through the National Re-
search Foundation of Korea (NRF) funded by the
Ministry of Education, Science and Technology
(2013R1A1A4A01005074).

References
Alan R. Aronson. 2006. Metamap: Mapping text to

the umls metathesaurus. Bethesda, MD: NLM, NIH,
DHHS, 1–26.

Andreas Merkel, and Dietrich Klakow. 2007. Compar-
ing improved language models for sentence retrieval
in question answering. LOT Occasional Series 7,
35–50.

Andrzej Białecki, Robert Muir, and Grant Ingersoll.
2012. Apache lucene 4. SIGIR 2012 workshop on
open source information retrieval.

Ben Abacha, Asma, and Pierre Zweigenbaum. 2012.
Medical question answering: translating medical
questions into sparql queries. Proceedings of the
2nd ACM SIGHIT International Health Informatics
Symposium. ACM.

Breck Baldwin, and Bob Carpenter. 2003. Ling-
Pipe. Available from World Wide Web: http://alias-i.
com/lingpipe.

Chris D. Paice. 1984. Soft evaluation of Boolean
search queries in information retrieval systems. In-
formation Technology: Research and Development,
3(1):33-41.

Daniel Svozil, Vladimir Kvasnicka, and Jiri Pospichal.
1997. Introduction to multi–layer feed-forward neu-
ral networks. Chemometrics and intelligent labora-
tory systems, 39(1):43–62.

George Tsatsaronis, Georgios Balikas, Prodromos
Malakasiotis, Ioannis Partalas, Matthias Zschunke,
Michael R. Alvers, Dirk Weissenborn, Anastasia
Krithara, Sergios Petridis, Dimitris Polychronopou-
los, Yannis Almirantis, John Pavlopoulos, Nicolas
Baskiotis, Patrick Gallinari, Thierry Artires, Axel–
Cyrille N. Ngomo, Norman Heino, Eric Gaussier,

48



Liliana Barrio-Alvers, Michael Schroeder, Ion An-
droutsopoulos and Georgios Paliouras. 2015. An
overview of the BIOASQ large–scale biomedical se-
mantic indexing and question answering competi-
tion. BMC Bioinformatics.

Harksoo Kim and Jungyun Seo. 2004. A high per-
formance question-answering system based on a
two–pass answer indexing and lexico-syntactic pat-
tern matching. IEICE Information and Systems,
Vol.E87–D (12):2855–2862.

Martin F. Porter. 1980. An algorithm for suffix strip-
ping. Program, 14(3):130–137.

Sainbayar Sukhbaatar, Jason Weston, and Rob Fergus.
2015. End–to–end memory networks. Advances in
Neural Information Processing Systems.

Stephen E. Robertson, Steve Walker, and M. Beaulieu.
1999. Okapi at TREC–7: automatic ad hoc, filter-
ing, VLC and interactive track. Nist Special Publi-
cation SP, 253–264.

Xiaoyong Liu, and W. Bruce Croft. 2004. Cluster–
based retrieval using language models. Proceedings
of the 27th annual international ACM SIGIR confer-
ence on Research and development in information
retrieval. ACM.

49


