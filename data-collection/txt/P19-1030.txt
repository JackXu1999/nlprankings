



















































You Only Need Attention to Traverse Trees


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 316–322
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

316

You Only Need Attention to Traverse Trees

Mahtab Ahmed, Muhammad Rifayat Samee and Robert E. Mercer

Department of Computer Science, University of Western Ontario
{mahme255, msamee, rmercer}@uwo.ca

Abstract

In recent NLP research, a topic of interest
is universal sentence encoding, sentence rep-
resentations that can be used in any super-
vised task. At the word sequence level, fully
attention-based models suffer from two prob-
lems: a quadratic increase in memory con-
sumption with respect to the sentence length
and an inability to capture and use syntactic
information. Recursive neural nets can extract
very good syntactic information by travers-
ing a tree structure. To this end, we pro-
pose Tree Transformer, a model that captures
phrase level syntax for constituency trees as
well as word-level dependencies for depen-
dency trees by doing recursive traversal only
with attention. Evaluation of this model on
four tasks gets noteworthy results compared
to the standard transformer and LSTM-based
models as well as tree-structured LSTMs. Ab-
lation studies to find whether positional infor-
mation is inherently encoded in the trees and
which type of attention is suitable for doing
the recursive traversal are provided.

1 Introduction

Following the breakthrough in NLP research with
word embeddings by Mikolov et al. (2013), recent
research has focused on sentence representations.
Having good sentence representations can help ac-
complish many NLP tasks because we eventually
deal with sentences, e.g., question answering, sen-
timent analysis, semantic similarity, and natural
language inference.

Most of the existing task specific sequential
sentence encoders are based on recurrent neural
nets such as LSTMs or GRUs (Conneau et al.,
2017; Lin et al., 2017; Liu et al., 2016). All of
these works follow a common paradigm: use an
LSTM/GRU over the word sequence, extract con-
textual features at each time step, and apply some
kind of pooling on top of that. However, a few

works adopt some different methods. Kiros et al.
(2015) propose a skip-gram-like objective func-
tion at the sentence level to obtain the sentence
embeddings. Logeswaran and Lee (2018) refor-
mulate the task of predicting the next sentence
given the current one into a classification problem
where instead of a decoder they use a classifier to
predict the next sentence from a set of candidates.

The attention mechanism adopted by most of
the RNN based models require access to the hid-
den states at every time step (Yang et al., 2016;
Kumar et al., 2016). These models are ineffi-
cient and at the same time very hard to paral-
lelize. To overcome this, Parikh et al. (2016) pro-
pose a fully attention-based neural network which
can adequately model the word dependencies and
at the same time is parallelizable. Vaswani et al.
(2017) adopt the multi-head version in both the
encoder and decoder of their Transformer model
along with positional encoding. Ahmed et al.
(2017) propose a multi-branch attention frame-
work where each branch captures a different se-
mantic subspace and the model learns to combine
them during training. Cer et al. (2018) propose an
unsupervised sentence encoder by leveraging only
the encoder part of the Transformer where they
train on the large Stanford Natural Language In-
ference (SNLI) corpus and then use transfer learn-
ing on smaller task specific corpora.

Apart from these sequential models, there has
been extensive work done on the tree structure of
natural language sentences. Socher et al. (2011b,
2013, 2014) propose a family of recursive neural
net (RvNN) based models where a composition
function is applied recursively bottom-up on chil-
dren nodes to compute the parent node representa-
tion until the root is reached. Tai et al. (2015) pro-
pose two variants of sequential LSTM, child sum
tree LSTM and N-ary tree LSTM. The same gat-
ing structures as in standard LSTM are used except



317

A

B C

GFED

(a) Tree to traverse (c) Traversal technique

ED

Attention
Block

B GF

Attention
Block

C

C'B'

Attention
Block

A

A'

tanh
tanh

tanh

Multi Head
Attention

Linear

K K

Linear

PCNNPCNN

α α

Input

output

(b) Attention block used

Figure 1: Attention over the tree structure

the hidden and cell states of a parent are dependent
only on the hidden and cell states of its children.

Recently, Shen et al. (2018) propose a Parsing-
Reading-Predict Network (PRPN) which can in-
duce syntactic structure automatically from an
unannotated corpus and can learn a better lan-
guage model with that induced structure. Later,
Htut et al. (2018) test this PRPN under various
configurations and datasets and further verified its
empirical success for neural network latent tree
learning. Williams et al. (2018) also validate the
effectiveness of two latent tree based models but
found some issues such as being biased towards
producing shallow trees, inconsistencies during
negation handling, and a tendency to consider the
last two words of a sentence as constituents.

In this paper, we propose a novel recursive neu-
ral network architecture consisting of a decompos-
able attention framework in every branch. We call
this model Tree Transformer as it is solely depen-
dent on attention. In a subtree, the use of a com-
position function is justified by a claim of Socher
et al. (2011b, 2014). In this work, we replace this
composition function with an attention module.
While Socher et al. (2011b, 2014) consider only
the child representations for both dependency and
constituency syntax trees, in this work, for depen-
dency trees, the attention module takes both the
child and parent representations as input and pro-
duces weighted attentive copies of them. For con-
stituency trees, as the parent vector is entirely de-
pendent on the upward propagation, the attention
module works only with the child representations.
Our extensive evaluation proves that our model is

better or at least on par with the existing sequential
(i.e., LSTM and Transformer) and tree structured
(i.e., Tree LSTM and RvNN) models.

2 Proposed Model

Our model is designed to address the following
general problem. Given a dependency or con-
stituency tree structure, the task is to traverse ev-
ery subtree within it attentively and infer the root
representation as a vector. Our idea is inspired
by the RvNN models from Socher et al. (2013,
2011b, 2014) where a composition function is
used to transform a set of child representations
into one single parent representation. In this sec-
tion, we describe how we use the attention module
as a composition function to build our Tree Trans-
former. Figure 1 gives a sketch of our model.

A dependency tree contains a word at every
node. To traverse a subtree in a dependency tree,
we look at both the parent and child representa-
tions (Xd in Eqn. 1). In contrast, in a constituency
tree, only leaf nodes contain words. The non-
terminal vectors are calculated only after travers-
ing each subtree. Consequently, only the child rep-
resentations (Xc in Eqn. 1) are considered.

Xd =


pv
c1v

...
cnv

 Xc =

c1v
c2v

...
cnv

 (1)
Here, pv is the parent representation and the civ ’s
are the child representations. For both of these
trees, Eqn. 2 computes the attentive transformed



318

representation.

P̃ = f(x), where x ∈ {Xd,Xc} (2)

Here, f is the composition function using the
multi-branch attention framework (Ahmed et al.,
2017). This multi-branch attention is built upon
the multi-head attention framework (Vaswani
et al., 2017) which further uses scaled dot-product
attention (Parikh et al., 2016) as the building
block. It operates on a query Q, key K and value
V as follows

Attention(Q,K,V) = softmax

(
QKT√

dk

)
V (3)

where dk is the dimension of the key. As we are
interested in n branches, n copies are created for
each (Q, K, V), converted to a 3D tensor, and then
a scaled dot-product attention is applied using

Bi = Attention(QiW
Q
i ,KiW

K
i ,ViW

V
i ) (4)

where i ∈ [1, n] and the Wi’s are the parame-
ters that are learned. Note that WQi ,W

K
i and

WVi ∈ Rdm×dk . Instead of having separate pa-
rameters for the transformation of leaves, internal
nodes and parents (Socher et al., 2014), we keep
WQi ,W

K
i and W

V
i the same for all these compo-

nents. We then project each of the resultant tensors
into different semantic sub-spaces and employ a
residual connection (He et al., 2016; Srivastava
et al., 2015) around them. Lastly, we normalize
the resultant outputs using a layer normalization
block (Ba et al., 2016) and apply a scaling factor
κ to get the branch representation. All of these are
summarized in Eqn. 5.

Bi = LayerNorm(BiW
b
i +Bi)× κi (5)

Here, Wbi ∈ Rn×dv×dm and κ ∈ Rn are the
parameters to be learned. Note that we choose
dk = dq = dv = dm/n. Following this, we take
each of these B’s and apply a convolutional neural
network (see Eqn. 6) consisting of two transforma-
tions on each position separately and identically
with a ReLU activation (R) in between.

PCNN(x) = Conv(R(Conv(x) + b1)) + b2 (6)

We compute the final attentive representation
of these subspace semantics by doing a linearly
weighted summation (see Eqn. 7) where α ∈ Rn
is learned as a model parameter.

BranchAttn(Q,K,V) =
n∑

i=1

αiPCNN(Bi) (7)

Lastly, we employ another residual connection
with the output of Eqn. 7, transform it non-linearly
and perform an element-wise summation (EwS) to
get the final parent representation as in Eqn. 8.

P̃ = EwS(tanh((x̃+ x)W + b)) (8)

Here, x and x̃ depict the input and output of the
attention module.

3 Experiments

In this section, we present the effectiveness of our
Tree Transformer model by reporting its evalua-
tion on four NLP tasks. We present a detailed
ablation study on whether positional encoding is
important for trees and also demonstrate which at-
tention module is most suitable as a composition
function for the recursive architectures.
Experimental Setup: We initialize the word
embedding layer weights with GloVe 300-
dimensional word vectors (Pennington et al.,
2014). These embedding weights are not updated
during training. In the multi-head attention block,
the dimension of the query, key and value matrices
are set to 50 and we use 6 parallel heads on each
input. The multi-branch attention block is com-
posed of 6 position-wise convolutional layers. The
number of branches is also set to 6. We use two
layers of convolutional neural network as the com-
position function for the PCNN layer. The first
layer uses 341 1d kernels with no dropout and the
second layer uses 300 1d kernels with dropout 0.1.

During training, the model parameters are up-
dated using the Adagrad algorithm (Duchi et al.,
2011) with a fixed learning rate of 0.0002. We
trained our model on an Nvidia GeForce GTX
1080 GPU and used PyTorch 0.4 for the imple-
mentation under the Linux environment.
Datasets: Evaluation is done on four tasks: the
Stanford Sentiment Treebank (SST) (Socher et al.,
2011b) for sentiment analysis, Sentences Involv-
ing Compositional Knowledge (SICK) (Marelli
et al., 2014) for semantic relatedness (-R) and nat-
ural language inference (-E), and the Microsoft
Research Paraphrase (MSRP) corpus (Dolan et al.,
2004) for paraphrase identification.

The samples in the SST dataset are labelled for
both the binary and the 5-class classification task.
In this work we are using only the binary classifi-
cation labels. The MSRP dataset is labelled with
two classes. The samples in the SICK dataset are
labelled for both the 3-class SICK-E classification



319

Types of Models Model SICK-E SICK-R SST MSRP(Acc.) (MSE) (Acc.) (Acc.)

Tree Structured

SDT-RNN (Socher et al., 2014) - .3848 - -
RAE (Socher et al., 2011a) - - 82.40 76.80
MV-RNN (Socher et al., 2012) 58.14 † - 82.90 66.91 †
RNTN (Socher et al., 2013) 59.42 † - 85.40 66.91 †
DT-RNN (Socher et al., 2014) 63.38 † .3822 86.60 67.51 †
DT-LSTM (Tai et al., 2015) 83.11 † .2532/.2625 † 85.70/85.10 † 72.07 †
CT-LSTM (Tai et al., 2015) 82.00 † .2734/.2891 † 88.00/87.27 † 70.07 †

LSTM

LSTM (Tai et al., 2015) 76.80 .2831 84.90 71.70
Bi-LSTM (Tai et al., 2015) 82.11 † .2736 87.50 72.70
2-layer LSTM (Tai et al., 2015) 78.54 † .2838 86.30 69.35 †
2-layer Bi-LSTM (Tai et al., 2015) 79.66 † .2762 87.20 70.40 †
Infersent (Conneau et al., 2017) 84.62 .2732 86.00 74.46

Transformer
USE T (Cer et al., 2018) 81.15 .5241 † 85.38 74.96 †
USE T+DAN (Cer et al., 2018) - - 86.62 -
USE T+CNN (Cer et al., 2018) - - 86.69 -

Tree Transformer Dependency Tree Transformer (DTT) 82.95 .2774 83.12 70.34Constituency Tree Transformer (CTT) 82.72 .3012 86.66 71.73

Table 1: Performance comparison of the Tree Transformer against some state-of-the-art sentence encoders. Models
that we implemented are marked with †.

task and the SICK-R regression task which uses
real-valued labels between 1 and 5. Instead of do-
ing a regression on SICK-R to predict the score,
we are using the same setup as Tai et al. (2015)
who compute a target distribution p as a function
of the predicted score y given by Eqn. 9.

p̃i =


y − byc , if i = byc+ 1
byc − y + 1, if i = byc
0, otherwise

(9)

The SST dataset includes already generated de-
pendency and constituency trees. As the other two
datasets do not provide tree structures, we parsed
each sentence using the Stanford dependency and
constituency parser (Manning et al., 2014).

For the sentiment classification (SST), natu-
ral language inference (SICK-E), and paraphrase
identification (MSRP) tasks, accuracy, the stan-
dard evaluation metric, is used. For the seman-
tic relatedness task (SICK-R), we are using mean
squared error (MSE) as the evaluation metric.

We use KL-divergence as the loss function for
SICK-R to measure the distance between the pre-
dicted and target distribution. For the other three
tasks, we use cross entropy as the loss function.

Table 1 shows the results of the evaluation of the
model on the four tasks in terms of task specific
evaluation metrics. We compare our Tree Trans-
former against tree structured RvNNs, LSTM
based, and Transformer based architectures.

To do a fair comparison, we implemented both
variants of Tree LSTM and Transformer based ar-
chitectures and some of the RvNN and LSTM

based models which do not have reported re-
sults for every task. Instead of assessing on
transfer performance, the evaluation is performed
on each corpus separately following the standard
train/test/valid split.

For SICK-E, our model achieved 82.95% and
82.72% accuracy with dependency and con-
stituency tree, respectively, which is on par
with DT-LSTM (83.11%) as well as CT-LSTM
(82.00%) and somewhat better than the standard
Transformer (81.15%). As can be seen, all of the
previous recursive architectures are somewhat in-
ferior to the Tree Transformer results.

For SICK-R, we are getting .2774 and .3012
MSE whereas the reported MSE for DT-LSTM
and CT-LSTM are .2532 and .2734, respectively.
However, in our implementation of those models
with the same hyperparameters, we haven’t been
able to reproduce the reported results. Instead we
ended up getting .2625 and .2891 MSE for DT-
LSTM and CT-LSTM, respectively. On this task,
our model is doing significantly better than the
standard Transformer (.5241 MSE).

On the SST dataset, our model (86.66% Acc.) is
again on par with tree LSTM (87.27% Acc.) and
better than Transformer (85.38% Acc.) as well as
Infersent (86.00% Acc.)1.

On the MSRP dataset, our dependency tree ver-
sion (70.34% Acc.) is below DT-LSTM (72.07%

1The official implementation available at https:
//github.com/facebookresearch/InferSent is
used. Reported hyperparameters are used except LSTM hid-
den state, 1024d is chosen due to hardware limitations.

https://github.com/facebookresearch/InferSent
https://github.com/facebookresearch/InferSent


320

Model PE SICK-E SICK-R SST MSRP

DTT On 78.58 .3383 83.03 69.01Off 82.28 .2774 83.12 70.34

CTT On 81.83 .3088 83.96 71.73Off 82.72 .3012 86.66 68.62

Table 2: Effect of Positional Encoding (PE).

Acc.). However, for the constituency tree ver-
sion, we are getting better accuracy (71.73%) than
CT-LSTM (70.07%). It is to be noted that all of
the sequential models, i.e., Transformer, Infersent
and LSTMs, are doing better compared to the tree
structured models on this paraphrase identification
task.

Model S/M/B SICK-E SICK-R SST MSRP

DTT
S 82.95 .3004 81.71 68.62
M 82.86 .2955 82.97 69.07
B 82.28 .2774 83.12 70.34

CTT
S 80.17 .4657 84.58 69.35
M 79.66 .4346 83.74 70.01
B 82.72 .3012 86.32 71.73

Table 3: Effect of different attention modules as a com-
position function. S: single-head attention, M: multi-
head attention, B: multi-branch attention.

Since positional encoding is a crucial part of
the standard Transformer, Table 2 presents its ef-
fect on trees. In constituency trees, positional in-
formation is inherently encoded in the tree struc-
ture. However, this is not the case with depen-
dency trees. Nonetheless, our experiments suggest
that for trees, positional encoding is irrelevant in-
formation as the performance drops in all but one
case. We also did an experiment to see which
attention module is best suited as a composition
function and report the results in Table 3. As can
be seen, in almost all the cases, multi-branch at-
tention has much better performance compared to
the other two. This gain by multi-branch attention
is much more significant for CTT than for DTT.

Figure 2 visualizes how our CTT model puts
attention on different phrases in a tree to com-
pute the correct sentiment. Space limitations al-
low only portions of the tree to be visualized. As
can be seen, the sentiment is positive (+1) at the
root and the model puts more attention on the right
branch as it has all of the positive words, whereas
the left branch (NP) is neutral (0). The bottom
three trees are the phrases which contain the pos-
itive words. The model again puts more attention
on the relevant branches. The words ‘well’ and
‘sincere’ are inherently positive. In the corpus the

Doug Liman the director of Bourne directs the traffic well gets a nice wintry look from his locations 
absorbs us with the movie 's spycraft and uses Damon 's ability to be focused and sincere

Root

NP VP

.21 .79

+1

0 +1

X

VP

.13 .87

+1

0 +1

VP

.34 .66

+1

0 +1
X JJ

.23 .77

+1

0 +1

well sincereus

ADJP

VBZ PRPADVP

Figure 2: Attentive tree visualization (CTT)

word ‘us’ is tagged as positive for this sentence.

4 Conclusion

In this paper, we propose Tree Transformer which
successfully encodes natural language grammar
trees utilizing the modules designed for the stan-
dard Transformer. We show that we can effec-
tively use the attention module as the composi-
tion function together with grammar information
instead of just bag of words and can achieve per-
formance on par with Tree LSTMs and even better
performance than the standard Transformer.

Acknowledgements

This research is partially funded by The Natu-
ral Sciences and Engineering Research Council of
Canada (NSERC) through a Discovery Grant to
Robert E. Mercer. We also acknowledge the help-
ful comments provided by the reviewers.

References
Karim Ahmed, Nitish Shirish Keskar, and Richard

Socher. 2017. Weighted transformer net-
work for machine translation. arXiv preprint
arXiv:1711.02132.

Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-
ton. 2016. Layer normalization. arXiv preprint
arXiv:1607.06450.

Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua,
Nicole Limtiacob, Rhomni St John, Noah Constant,
Mario Guajardo-Céspedes, Steve Yuanc, Chris Tar,
Yun-Hsuan Sung, Brian Strope, and Ray Kurzweil.
2018. Universal sentence encoder. arXiv preprint
arXiv:1803.11175.

Alexis Conneau, Douwe Kiela, Holger Schwenk, Loı̈c
Barrault, and Antoine Bordes. 2017. Supervised
learning of universal sentence representations from
natural language inference data. In Proceedings of
the 2017 Conference on Empirical Methods in Nat-
ural Language Processing, pages 670–680.

https://doi.org/10.18653/v1/D17-1070
https://doi.org/10.18653/v1/D17-1070
https://doi.org/10.18653/v1/D17-1070


321

Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Un-
supervised construction of large paraphrase corpora:
Exploiting massively parallel news sources. In Pro-
ceedings of the 20th International Conference on
Computational Linguistics, pages 350–356.

John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. Journal of Machine
Learning Research, 12(Jul):2121–2159.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2016. Deep residual learning for image recog-
nition. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pages
770–778.

Phu Mon Htut, Kyunghyun Cho, and Samuel Bowman.
2018. Grammar induction with neural language
models: An unusual replication. In Proceedings
of the 2018 EMNLP Workshop BlackboxNLP: An-
alyzing and Interpreting Neural Networks for NLP,
pages 371–373.

Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,
Richard Zemel, Raquel Urtasun, Antonio Torralba,
and Sanja Fidler. 2015. Skip-thought vectors. In Ad-
vances in Neural Information Processing Systems,
pages 3294–3302.

Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit
Iyyer, James Bradbury, Ishaan Gulrajani, Victor
Zhong, Romain Paulus, and Richard Socher. 2016.
Ask me anything: Dynamic memory networks for
natural language processing. In International Con-
ference on Machine Learning, pages 1378–1387.

Zhouhan Lin, Minwei Feng, Cı́cero Nogueira dos San-
tos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua
Bengio. 2017. A structured self-attentive sentence
embedding. In 5th International Conference on
Learning Representations (ICLR) Conference Track
Proceedings.

Yang Liu, Chengjie Sun, Lei Lin, and Xiaolong Wang.
2016. Learning natural language inference using
bidirectional lstm model and inner-attention. arXiv
preprint arXiv:1605.09090.

Lajanugen Logeswaran and Honglak Lee. 2018. An ef-
ficient framework for learning sentence representa-
tions. In 6th International Conference on Learning
Representations (ICLR) Conference Track Proceed-
ings.

Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David Mc-
Closky. 2014. The Stanford CoreNLP natural lan-
guage processing toolkit. In Association for Compu-
tational Linguistics (ACL) System Demonstrations,
pages 55–60.

Marco Marelli, Luisa Bentivogli, Marco Baroni, Raf-
faella Bernardi, Stefano Menini, and Roberto Zam-
parelli. 2014. Semeval-2014 task 1: Evaluation of
compositional distributional semantic models on full

sentences through semantic relatedness and textual
entailment. In Proceedings of the 8th International
Workshop on Semantic Evaluation (SemEval 2014),
pages 1–8.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems, pages 3111–3119.

Ankur Parikh, Oscar Täckström, Dipanjan Das, and
Jakob Uszkoreit. 2016. A decomposable attention
model for natural language inference. In Proceed-
ings of the 2016 Conference on Empirical Methods
in Natural Language Processing, pages 2249–2255.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. GloVe: Global vectors for word
representation. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 1532–1543.

Yikang Shen, Zhouhan Lin, Chin-Wei Huang, and
Aaron Courville. 2018. Neural language model-
ing by jointly learning syntax and lexicon. In 6th
International Conference on Learning Representa-
tions (ICLR) Conference Track Proceedings.

Richard Socher, Eric H Huang, Jeffrey Pennington,
Christopher D Manning, and Andrew Y Ng. 2011a.
Dynamic pooling and unfolding recursive autoen-
coders for paraphrase detection. In Advances in
Neural Information Processing Systems, pages 801–
809.

Richard Socher, Brody Huval, Christopher D Manning,
and Andrew Y Ng. 2012. Semantic compositional-
ity through recursive matrix-vector spaces. In Pro-
ceedings of the 2012 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1201–1211.

Richard Socher, Andrej Karpathy, Quoc V. Le, Christo-
pher D. Manning, and Andrew Y. Ng. 2014.
Grounded compositional semantics for finding and
describing images with sentences. Transactions
of the Association for Computational Linguistics,
2:207–218.

Richard Socher, Cliff Chiung-Yu Lin, Andrew Y. Ng,
and Christopher D. Manning. 2011b. Parsing natu-
ral scenes and natural language with recursive neu-
ral networks. In Proceedings of the 28th Interna-
tional Conference on International Conference on
Machine Learning, ICML’11, pages 129–136.

Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. 2013. Recursive deep models
for semantic compositionality over a sentiment tree-
bank. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1631–1642.

https://www.aclweb.org/anthology/W18-5452
https://www.aclweb.org/anthology/W18-5452
http://www.aclweb.org/anthology/P/P14/P14-5010
http://www.aclweb.org/anthology/P/P14/P14-5010
https://doi.org/10.18653/v1/D16-1244
https://doi.org/10.18653/v1/D16-1244
https://iclr.cc/Conferences/2018/Schedule?showEvent=124
https://iclr.cc/Conferences/2018/Schedule?showEvent=124
http://aclweb.org/anthology/Q14-1017
http://aclweb.org/anthology/Q14-1017
http://dl.acm.org/citation.cfm?id=3104482.3104499
http://dl.acm.org/citation.cfm?id=3104482.3104499
http://dl.acm.org/citation.cfm?id=3104482.3104499
http://aclweb.org/anthology/D13-1170
http://aclweb.org/anthology/D13-1170
http://aclweb.org/anthology/D13-1170


322

Rupesh Kumar Srivastava, Klaus Greff, and Jürgen
Schmidhuber. 2015. Highway networks. arXiv
preprint arXiv:1505.00387.

Kai Sheng Tai, Richard Socher, and Christopher D.
Manning. 2015. Improved semantic representations
from tree-structured long short-term memory net-
works. In Proceedings of the 53rd Annual Meet-
ing of the Association for Computational Linguistics
and the 7th International Joint Conference on Natu-
ral Language Processing (Volume 1: Long Papers),
pages 1556–1566.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems, pages 5998–6008.

Adina Williams, Andrew Drozdov*, and Samuel R.
Bowman. 2018. Do latent tree learning models iden-
tify meaningful structure in sentences? Transac-
tions of the Association for Computational Linguis-
tics, 6:253–267.

Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He,
Alex Smola, and Eduard Hovy. 2016. Hierarchi-
cal attention networks for document classification.
In Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 1480–1489.

https://doi.org/10.3115/v1/P15-1150
https://doi.org/10.3115/v1/P15-1150
https://doi.org/10.3115/v1/P15-1150
https://doi.org/10.1162/tacl_a_00019
https://doi.org/10.1162/tacl_a_00019
https://doi.org/10.18653/v1/N16-1174
https://doi.org/10.18653/v1/N16-1174

