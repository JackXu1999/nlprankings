



















































A Compact and Language-Sensitive Multilingual Translation Method


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1213–1223
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

1213

A Compact and Language-Sensitive Multilingual Translation Method

Yining Wang1,2, Long Zhou1,2, Jiajun Zhang1,2∗, Feifei Zhai4,
Jingfang Xu4 and Chengqing Zong1,2,3

1National Laboratory of Pattern Recognition, CASIA, Beijing, China
2University of Chinese Academy of Sciences, Beijing, China

3CAS Center for Excellence in Brain Science and Intelligence Technology, Beijing, China
4Sogou Inc., Beijing, China

{yining.wang, long.zhou, jjzhang, cqzong}@nlpr.ia.ac.cn
{zhaifeifei,xujingfang}@sogou-inc.com

Abstract

Multilingual neural machine translation
(Multi-NMT) with one encoder-decoder
model has made remarkable progress due to
its simple deployment. However, this multi-
lingual translation paradigm does not make
full use of language commonality and param-
eter sharing between encoder and decoder.
Furthermore, this kind of paradigm cannot
outperform the individual models trained on
bilingual corpus in most cases. In this paper,
we propose a compact and language-sensitive
method for multilingual translation. To
maximize parameter sharing, we first present
a universal representor to replace both encoder
and decoder models. To make the representor
sensitive for specific languages, we further
introduce language-sensitive embedding,
attention, and discriminator with the ability
to enhance model performance. We verify
our methods on various translation scenarios,
including one-to-many, many-to-many and
zero-shot. Extensive experiments demon-
strate that our proposed methods remarkably
outperform strong standard multilingual
translation systems on WMT and IWSLT
datasets. Moreover, we find that our model
is especially helpful in low-resource and
zero-shot translation scenarios.

1 Introduction

Encoder-decoder based sequence-to-sequence ar-
chitecture (Kalchbrenner and Blunsom, 2013;
Sutskever et al., 2014; Zhang and Zong, 2015;
Vaswani et al., 2017; Gehring et al., 2017) fa-
cilitates the development of multilingual neural
machine translation (Multi-NMT) (Dong et al.,
2015; Luong et al., 2016; Firat et al., 2016; John-
son et al., 2017; Gu et al., 2018). The domi-

∗ Jiajun Zhang is the corresponding author and the work
is done while Yining Wang is doing research intern at Sogou
Inc.







N

Lang-Sensi 
Embedding

Lang-Sensi 
attention

Discrimnator

M Src N Tgt

EncoderDecoder
Sharing

Representor

N

Lang-Sensi Embedding

Lang-Sensi 
Attention

Lang-Sensi Discrimnator

M Src N Tgt

Encoder Decoder
Sharing

Representor

Figure 1: Our proposed compact representor, replacing
encoder and decoder, can perform multilingual trans-
lation from M source languages to N target languages.
We also introduce three specific modules consisting of
language-sensitive embedding, language-sensitive at-
tention, and language-sensitive discriminator.

nant paradigm of Multi-NMT contains one en-
coder to represent multiple languages and one de-
coder to generate output tokens of separate lan-
guages (Johnson et al., 2017; Ha et al., 2016). This
paradigm is widely used in Multi-NMT systems
due to simple implementation and convenient de-
ployment.

However, this paradigm has two drawbacks. For
one hand, using single encoder-decoder frame-
work for all language pairs usually yields infe-
rior performance compared to individually trained
single-pair models in most cases (Lu et al., 2018;
Platanios et al., 2018; Wang et al., 2018). For
the other hand, although this paradigm saves lots
of parameters compared to another Multi-NMT
framework which employs separate encoders and
decoders to handle different languages (Dong
et al., 2015; Zoph and Knight, 2016; Luong et al.,
2016; Firat et al., 2016), parameter sharing be-
tween encoder and decoder are not fully explored.
Since both encoder and decoder have similar



1214

structures but use different parameters, the com-
monality of languages cannot be fully exploited in
this paradigm. A natural question arises that why
not share the parameters between encoder and de-
coder on multilingual translation scenario?

To address these issues, we present a compact
and language-sensitive method in this work, as
shown in Figure 1. We first propose a unified
representor by tying encoder and decoder weights
in Multi-NMT model, which can not only re-
duce parameters but also make full use of lan-
guage commonality and universal representation.
To enhance the model ability to distinguish dif-
ferent languages, we further introduce language-
sensitive embedding, attention, and discriminator.

We conduct extensive experiments to verify
the effectiveness of our proposed model on vari-
ous Multi-NMT tasks including one-to-many and
many-to-many which is further divided into bal-
anced, unbalanced and zero-shot. Experimen-
tal results demonstrate that our model can sig-
nificantly outperform the strong standard base-
line multilingual systems and achieve even better
performance than individually trained models on
most of the language pairs.

Specifically, our contributions are three-fold in
this work:

(1) We present a universal representor to replace
encoder and decoder, leading to a compact transla-
tion model, which fully explores the commonality
between languages.

(2) We introduce language-sensitive embed-
ding, attention, and discriminator which augment
the ability of Multi-NMT model in distinguishing
different languages.

(3) Extensive experiments demonstrate the su-
periority of our proposed method on various trans-
lation tasks including one-to-many, many-to-many
and zero-shot scenarios. Moreover, for many-to-
many using unbalance translation pairs, we can
achieve the new state-of-the-art results on IWSLT-
15 English-Vietnamese. For zero-shot translation,
our methods can achieve even better results than
individually trained models with the parallel cor-
pus.

2 Background

In this section, we will introduce the background
of the encoder-decoder (Sutskever et al., 2014;
Cho et al., 2014) framework and self-attention-
based Transformer (Vaswani et al., 2017).

2.1 Encoder-Decoder Framework

Given a set of sentence pairs D = {(x, y)},
the encoder fenc with parameters θenc maps an
input sequence x = (x1, x2, · · · , xn) to a se-
quence of continuous representations henc =
(henc1 , henc2 , · · · , hencn ) whose size varies concern-
ing the source sentence length. The decoder
fdec with θdec generates an output sequence y =
(y1, y2, · · · , ym) by computing P(yt |y<t ) as fol-
lows:

P(yt |y<t ) = softmax( f (hdec, ct )) (1)

where hdec is a sequence of continuous represen-
tations for the decoder and ct is the context vector
which can be calculated as follows:

ct =
n∑
i=1

ai,t henci (2)

where ai,t is attention weight:

ai,t = softmax(ei,t ) =
exp ei,t∑n
j=1 exp ej,t

(3)

where ei,t is a similarity score between the source
and target representations. The parameters of cal-
culating cross-attention weight ai,t are denoted as
θattn.

The encoder and decoder are trained to maxi-
mize the conditional probability of target sequence
given a source sequence:

Lt (D;θ) =
|D |∑
d=1

M∑
t=1

log P(yt |y<t, x; θenc, θdec, θattn) (4)

where M is target sentence length. For simplicity,
we do not specify d in this formula.

Both the encoder and decoder can be imple-
mented by the different basic neural models struc-
tures, such as RNN (LSTM/GRU) (Sutskever
et al., 2014; Cho et al., 2014), CNN (Gehring
et al., 2017), and self-attention (Vaswani et al.,
2017). Our proposed method can be applied to any
encoder-decoder architecture. Considering the ex-
cellent translation performance of self-attention
based Transformer (Vaswani et al., 2017), we im-
plement our method based on this architecture.

2.2 Transformer Network

Transformer is a stacked network with several lay-
ers containing two or three basic blocks in each
layer. For a single layer in the encoder, it consists
of a multi-head self-attention and a position-wise



1215

feed-forward network. For the decoder model, be-
sides the above two basic blocks, a multi-head
cross-attention follows multi-head self-attention.
In this block, the calculation method of similar-
ity score et in Equation 3 is a little different from
Luong et al. (2015) and Bahdanau et al. (2015):

ei,t =
1√
dm

Wkh
enc
i ∗Wqh

dec
t (5)

where dm is the dimension of hidden units, Wk and
Wq are parameters of this cross-attention block,
which are denoted as θattn in Equation 4.

All the basic blocks are associated with resid-
ual connections, followed by layer normalization
(Ba et al., 2016). Since the Transformer network
contains no recurrence, positional embeddings are
used in the model to make use of sequence or-
der. More details regarding the architecture can
be found in Vaswani et al. (2017).

2.3 Multilingual Translation
In contrast to NMT models, multilingual models
perform the multi-task paradigm with some de-
gree of parameter sharing, in which models are
jointly trained on multiple language pairs. We
mainly focus on mainstream multilingual transla-
tion method proposed by Johnson et al. (2017),
which has a unified encoder-decoder framework
with a shared attention module for multiple lan-
guage pairs. They decompose the probability of
the target sequences into the products of per token
probabilities in all translation forms:

Lm−t (D;θ) =
L∑
l=1

|Dl |∑
d=1

M∑
t=1

log P(ylt |xl, yl<t ; θenc, θdec, θattn)

(6)

where L is the number of translation pairs and
P(ylt |xl, yl<t ;θ) denotes the translation probability
of t-th word of the d-th sentence in l-th translation
pair. Note that the translation process for all target
languages uses the same parameter set θ.

3 Our Method

In this section, we introduce our compact and
language-sensitive method for multilingual trans-
lation, which can compress the model by a repre-
sentor and improve model ability with language-
sensitive modules.

3.1 A Compact Representor
In Multi-NMT model, the encoder and decoder
are two key components, which play analogous

Embeddings 

Multi-head Self-Attention 

Residual&Norm

N

Softmax 

Linear

Output 
Probabilities

Language-Sensitive 
Discriminator

+Position 
Embdding

lang-1 lang-2 ... lang-n lang-1 lang-2 ... lang-n

lang-1 lang-2 ... lang-n

src tgt

Language-Sensitive 
Embedding

+ + + + + +
Representor

Residual&Norm 
Feed 

Forward

Residual&Norm
Multi-head  Language-Sensitive 

Cross-Attention 

Figure 2: The framework of Multi-NMT using our
compact and language-sensitive method.

roles and have a similar structure in each layer.
We argue that encoder and decoder can share the
same parameters if necessary. Thus, we introduce
a representor to replace both encoder and decoder
by sharing weight parameters of the self-attention
block, feed-forward block and the normalization
block, as shown in Figure 2. The representor pa-
rameters are denoted θrep. Therefore, the objective
function (Equation 6) becomes:

Lm−t (D;θ) =
L∑
l=1

|Dl |∑
d=1

M∑
t=1

log P(ylt |xl, yl<t ; θrep, θattn) (7)

This representor (θrep) coordinates the semantic
presentation of multiple languages in a closely re-
lated universal level, which also increases the uti-
lization of commonality for different languages.

3.2 Language-Sensitive Modules

The compact representor maximizes the sharing of
parameters and makes full use of language com-
monality. However, it lacks the ability to discrimi-
nate different languages. In our method, we intro-
duce three language-sensitive modules to enhance
our model as follows:

1) Language-Sensitive Embedding: Previ-
ously, Press and Wolf (2017) conduct the weight
tying of input and output embedding in NMT
model. Generally, a shared vocabulary is built



1216

upon subword units like BPE (Sennrich et al.,
2016b) and wordpiece (Wu et al., 2016; Schus-
ter and Nakajima, 2012). However, it remains
under-exploited which kind of embedding shar-
ing is best for Multi-NMT. We divide the sharing
manners into four categories including language-
based manner (LB, different languages have sep-
arate input embeddings), direction-based manner
(DB, languages in source side and target side
have different input embeddings), representor-
based manner (RB, shared input embeddings for
all languages) and three-way weight tying man-
ner (TWWT) proposed in Press and Wolf (2017),
in which the output embedding of the target side
is also shared besides representor-based sharing.
We compare these four sharing manners for Multi-
NMT in our experiments, and we will discuss the
results in Section 5.

Considering the last three sharing manners can-
not model a sense of which language a token be-
longs to, we propose a new language-sensitive em-
bedding in our method to specify different lan-
guages explicitly. Similar to the position embed-
dings described in Section 2, this kind of embed-
ding is added to the embedding of each token
for corresponding language, which can indicate
the translation direction on the source side and
guide the generation process for target languages.
This embedding is denoted as Elang ∈ R |K |∗dmodel ,
where |K | is the number of languages involved,
and dmodel is the dimension of hidden states in our
model. Note that this embedding can be learned
during training.

2) Language-Sensitive Attention: In NMT ar-
chitecture, cross-attention only appearing in the
decoder network locates the most-relevant source
part when generating each token in target lan-
guage. For Multi-NMT, we introduce three dif-
ferent ways to design the cross-attention mecha-
nism, consisting of i) shared-attention, ii) hybrid-
attention, and iii)) language-sensitive attention
utilized in our method.

i): In our proposed compact representor, we
share self-attention block between encoder and de-
coder. For the shared-attention, we make a fur-
ther step to share parameters of cross-attention and
self-attention, which can be regarded as coordina-
tion of information from both the source side and
target side.

ii): Different from the above attention mech-
anism, the hybrid-attention utilizes independent

cross-attention modules but it is shared for all
translation tasks.

iii): In the language-sensitive attention, it al-
lows the model to select the cross-attention param-
eters associated with specific translation tasks dy-
namically.

In our paper, we investigate these three atten-
tion mechanisms. We argue that both the shared
and hybrid mechanisms tend to be confused to ex-
tract information from different source languages
when decoding multiple source languages with
different word orders. Thus, we mainly focus
on languages-sensitive attention in our method.
To this end, we use multiple sets of parameters
θattn to represent cross-attention modules of differ-
ent translation tasks. However, language-sensitive
attention does not support zero-shot translation
because there is no explicit training set for this
specific translation task. Therefore, we employ
hybrid-attention mechanism in our zero-shot ex-
periments.

3) Language-Sensitive Discriminator: In our
method, the representor which shares encoder and
decoder makes full use of language common-
ality, but it weakens the model ability to dis-
tinguish different languages. Hence we intro-
duce a new language-sensitive discriminator to
strengthen model representation.

In NMT framework, the hidden states on the
top layer can be viewed as a fine-grained abstrac-
tion (Anastasopoulos and Chiang, 2018). For this
language-sensitive module, we first employ a neu-
ral model fdis on the top layer of reprensentor h

rep
top,

and the output of this model is a language judg-
ment score Plang.

hdis = fdis(hreptop)

Plang(d) = softmax(Wdis ∗ hdisd + bdis)
(8)

where Plang(d) is language judgment score for sen-
tence pair d, Wdis, bdis are parameters, which are
denoted as θdis. We test two different types of neu-
ral models for fdis, including convolutional net-
work with max pooling layer and two-layer feed-
forward network.

And then, we obtain an discriminant objective
function as follows:

Ldis (θdis) =
∑
k∈K

|D |∑
d=1
I {gd = k} ∗ logPlang (d) (9)

where I {·} is indicator function, and gd belongs to
language k.



1217

Finally, we incorporate the language-sensitive
discriminator into our Multi-NMT model, and it
can be optimized through an end-to-end manner
for all translation language pairs D with the fol-
lowing objective function.

L(D;θ) =L(D; θrep, θattn, θdis)
=(1 − λ)Lm−t (θrep, θattn) + λLdis(θdis)

(10)

where λ is learned or pre-defined weight to bal-
ance the translation task and language judgment
task.

4 Experimental Settings

4.1 Data

In this section, we describe the datasets using
in our experiments on one-to-many and many-to-
many multilingual translation scenarios.

One-to-Many: For this translation scenario,
we perform one-to-two, one-to-three, and one-
to-four multilingual translation on the combi-
nation of WMT-141 (English-to-German, briefly
En→De), WMT-172 datasets (English-to-Latvian,
briefly En→Lv) and WMT-183 (English-to-
Finnish, English-to-Chinese without UN part4,
briefly En→Fi and En→Zh) datasets.

Many-to-Many: For many-to-many transla-
tion, we test our methods on IWSLT-175 trans-
lation datasets, including English, Italian, Roma-
nian, Dutch (briefly, En, It, Ro, Nl). In order
to perform zero-shot translation, we discard some
particular language pairs. We also evaluate our
method on the unbalanced training corpus. To
this end, we construct the training corpus using
resource-rich En-De, En-Fi in WMT datasets and
low-resource English-Vietnamese (briefly, En-Vi)
in IWSLT-156.

The statistical information of all the datasets is
detailed in Table 1.

4.2 Training Details

We implement our compact and language-
sensitive method for Multi-NMT based on the ten-
sor2tensor7 library. We use wordpiece method
(Wu et al., 2016; Schuster and Nakajima, 2012) to

1http://www.statmt.org/wmt14/translation-task.html
2http://www.statmt.org/wmt17/translation-task.html
3http://www.statmt.org/wmt18/translation-task.html
4https://cms.unov.org/UNCorpus/
5https://sites.google.com/site/iwsltevaluation2017
6https://sites.google.com/site/iwsltevaluation2015
7https://github.com/tensorflow/tensor2tensor

Datasets Language pair Train Dev Test

WMT

En-De 4.50M 6003 3003
En-Lv 4.50M 2003 2001
En-Fi 3.25M 3000 3000
En-Zh 9.02M 2002 2001

IWSLT

En-It 231.6k 929 1566
En-Ro 220.5k 914 1678
En-Nl 237.2k 1003 1777
Ro-It 217.5k 914 1643
En-Vi 130.9k 768 1268

Table 1: The statistics of all the datasets including
WMT and IWSLT tasks.

encode the combination of both source side sen-
tences and target side sentences. The vocabulary
size is 37,000 for both sides. We train our mod-
els using configuration transformer base adopted
by Vaswani et al. (2017), which contains a 6-
layer encoder and a 6-layer decoder with 512-
dimensional hidden representations. Each mini-
batch contains roughly 3,072 source and 3,072 tar-
get tokens, which belongs to one translation di-
rection. We use Adam optimizer (Kingma and
Ba, 2014) with β1=0.9, β2=0.98, and �=10−9. For
evaluation, we use beam search with a beam size
of k = 4 and length penalty α = 0.6. All our meth-
ods are trained and tested on a single Nvidia P40
GPU.

5 Results and Analysis

In this section, we discuss the results of our exper-
iments about our compact and language-sensitive
method on Multi-NMT. The translation perfor-
mance is evaluated by character-level BLEU5
for En→Zh translation and case-sensitive BLEU4
(Papineni et al., 2002) for other translation tasks.
In our experiments, the models trained on individ-
ual language pair are denoted by NMT Baselines,
and the baseline Multi-NMT models are denoted
by Multi-NMT Baselines.

5.1 One-to-Many Translation

5.1.1 Main Results

The main results on the one-to-many translation
scenario, including one-to-two, one-to-three and
one-to-four translation tasks are reported in Ta-
ble 2. We present a typical Multi-NMT adopt-
ing Johnson et al. (2017) method on Transformer
as our Multi-NMT baselines model. Obviously,
Multi-NMT Baselines cannot outperform NMT
Baselines in all cases, among which four direc-
tions are comparable and twelve are worse.



1218

Task Tgt NMTBaselines
Multi-NMT Baselines
Johnson et al. (2017)

Three-Stgy
Wang et al. (2018) Rep+Emb

Rep+Emb
+Attn

Rep+Emb
+Attn+Dis

One-to-Two

De 27.50 27.26 27.35 26.60 26.96 27.74
Lv 16.28 16.32 16.38 15.37 15.87 16.79
De 27.50 27.88 27.89 26.96 27.32 27.96
Fi 16.83 16.47 16.70 15.78 16.58 16.89
De 27.50 26.80 26.99 26.08 26.68 27.45
Zh 26.04 25.54 25.78 24.48 25.33 26.17

One-to-Three

De 27.50 25.44 25.55 24.82 25.45 26.06
Zh 26.04 24.87 25.63 24.12 24.93 26.12
Fi 16.83 16.86 16.97 16.06 16.78 17.12
De 27.50 25.98 26.12 24.88 25.80 26.42
Lv 16.28 14.88 15.44 14.51 15.58 16.31
Fi 16.83 16.94 17.05 16.15 16.79 17.22

One-to-Four

De 27.50 23.59 22.88 22.88 23.58 24.08
Lv 16.28 15.57 16.02 15.00 16.21 16.57
Zh 26.04 25.24 25.83 24.15 25.27 26.29
Fi 16.83 13.45 14.12 12.99 14.11 15.03

Table 2: Translation performance on one-to-two, one-to-three and one-to-four translation tasks. Rep denotes our
proposed representor. Emb, Attn, and Dis represent our proposed language-sensitive methods to address multilin-
gual translation. Note that the source language of all our experiments is English.

100 100 100

41
27.3

20.5
29.8

19.2 14.5

35.2
22.5 18.7

35.4
22.6 18.8

0

20

40

60

80

100

120

one-to-two one-to-three one-to-four

Indiv Baselines Rep+Emb Rep+Emb+Attn Rep+Emb+Attn+Dis

Number of Parameters per 
Language Pair (M)

Tasks

Figure 3: The comparison of model scale among indi-
vidually trained system, baselines Multi-NMT system
and our methods. Y-axis represents the model parame-
ters per language pair, which is calculated by averaging
model parameters on all translation tasks involved.

With respect to our proposed method, it is clear
that our compact method consistently outperforms
the baseline systems. Compared with another
strong one-to-many translation model Three-Stgy
proposed by Wang et al. (2018), our compact
method can achieve better results as well. More-
over, our method can perform even better than in-
dividually trained systems in most cases (eleven
out of sixteen cases). The results demonstrate the
effectiveness of our method.

5.1.2 Model Size

Besides improving the translation results, we also
compress the model size by introducing the rep-
resentor. We investigate the scale of parameters
used on average in each translation direction. We
compare three models, including NMT Baselines
model, Multi-NMT Baselines model, and our com-
pact Multi-NMT model. As shown in Figure 3, all

Src→Tgt EmbManners Size Tgt-1 Tgt-2

En→De/Lv
LB 139M 26.58 15.76
DB 100M 27.22 16.26
RB 82M 27.26 16.32

TWWT 63M 26.82 16.02

En→De/Zh
LB 139M 27.34 25.61
DB 100M 27.15 25.22
RB 82M 27.22 25.38

TWWT 63M 26.91 24.99

Table 3: Size (number of parameters) and BLEU scores
of various embedding sharing manners. LB, DB, RB,
TWWT denote language-based manner, direction-
based manner, representor-based manner, and three-
way weight tying manner separately, as mentioned in
Section 3.2. Tgt-1 and Tgt-2 mean the results of the
first (De) and the second (Lv/Zh) target language.

the multilingual translation models reduce the pa-
rameters. Compared with Multi-NMT Baselines,
we can observe that our method further reduces
the model size of Multi-NMT. Considering Ta-
ble 2 and Figure 3 together, we note that even
though our proposed method in one-to-four trans-
lation task only uses 18.8% parameters of NMT
Baselines, we can achieve better performance on
En→Zh and En→Lv.

5.1.3 Discussion of Language-Sensitive
Modules

Table 2 shows that our proposed language-
sensitive modules are complementary with each
other. In this subsection, we will analyze each
module in detail.

Language-Sensitive Embedding: As men-
tioned in section 3.2, embedding sharing man-



1219

0.01 0.03 0.05 0.07 0.09
22.4

22.6

22.8

23.2

23.4

BL
EU

 S
co

re

22.57

22.76

23.19

23.07

22.67
22.63

22.91

23.35

23.19

22.74

CNN on En-De 
FFN on En-De

23.0

0.01 0.03 0.05 0.07 0.09

22.2

22.4

22.6

22.8

23.0

BL
EU

 S
co

re

22.21

22.37

22.97

22.78

22.43

22.15

22.28

22.72

22.56

22.24

CNN on En-Lv
FFN on En-Lv

Figure 4: The comparison of two neural models with different hyper-parameter λ. CNN and FFN denote convolu-
tion network and feed-forward network, respectively.

ners for Multi-NMT are divided into four cat-
egories. We show the results of these sharing
manners in Table 3. To make a fair compari-
son, we sample 4.5M sentence pairs from En-Zh
dataset. As shown in this table, our representor-
based sharing manner consistently outperforms
both the direction-based manner and three-way
weight tying manner. Furthermore, even though
the representor-based manner has about 40%
fewer parameters than the language-based man-
ner, it achieves comparable or even better per-
formance. We find that language-based sharing
manner is unstable because it achieves the highest
BLEU score on Multi-NMT of similar languages
(En→De/Zh), but the worst quality on dissimilar
languages (En→De/Lv). Taking into account of
translation quality and stability, we choose to use
representor-based sharing manner in our method.

As described in Section 3.2, our proposed
language-sensitive embedding is added to the in-
put embedding of each token, which is unlike
convention Multi-NMT method adding a special
token into source side sentences or vocabularies
(Johnson et al., 2017; Ha et al., 2016). There
exists a question, is this kind of embeddings es-
sential in our representor? To make a verifica-
tion, we do the ablation study without this module.
We observe that Multi-NMT model does not con-
verge during training, which demonstrates these
language-sensitive embeddings play a significant
role in our model.

Language-Sensitive Attention: We present
three types of cross-attention mechanisms in
Section 3.2. We adopt shared-attention and
language-sensitive attention for Rep+Emb and

Rep+Emb+Attn separately. Comparing these two
methods in Table 2, Rep+Emb+Attn method out-
performs Rep+Emb method in all cases, which
demonstrates the language-sensitive is useful for
multiple language pairs with different word order.
We also conduct the experiment of our representor
with the hybrid-attention mechanism. Since this
method has similar performance with Rep+Emb
but is larger in size, we ignore its results here.

Language-Sensitive Discriminator: In sec-
tion 3.2, we employ two different types of the
neural model as a language-sensitive discrimina-
tor, and there is a hyper-parameter λ in Equa-
tion 10. We present the effect of convolutional
network and feed-forward network with different
hyper-parameters on development datasets in Fig-
ure 4. Considering that distinguishing between
languages is only an auxiliary task in Multi-NMT,
we set the maximum of λ to be 0.1. As shown in
Figure 4, when we adopt the convolution network
as our discriminator with λ = 0.05, our language-
sensitive method performs best. We also conduct
the experiments in which the hyper-parameter λ
is learnable. The experiment results are similar to
the best settings mentioned above both on En→De
(23.35 vs. 23.19) and En→Lv (22.97 vs. 22.72).
For simplicity, all our experiments listed in Table 2
and 4 adopt convolution network as the language-
sensitive discriminator with λ = 0.05.

5.2 Many-to-Many Translation

Table 4 reports the detailed results of different
methods under the many-to-many translation sce-
nario. We will analyze the performance below.



1220

Task Src→Tgt NMTBaselines
Multi-NMT Baselines
Johnson et al. (2017) Rep+Emb

Rep+Emb
+Attn

Rep+Emb
+Attn+Dis

Many-to-Many for Balanced Corpus

I
Supervised

Four-to-Four

En→It 28.41 29.53 29.47 29.98 30.23
It→En 30.66 31.70 31.76 32.23 32.75

En→Ro 21.41 22.23 22.16 22.87 23.53
Ro→En 26.09 27.69 27.58 27.98 28.32
En→Nl 25.88 27.88 26.96 27.32 27.96
Nl→En 27.48 28.67 28.58 28.86 29.32
It→Ro 12.77 13.86 13.89 14.35 14.89
Ro→It 13.54 14.78 14.66 14.87 15.22

II
Zero-Shot

Nl→Ro 14.15 13.70 13.98 15.12 15.54
Ro→Nl 14.33 13.91 14.17 14.86 15.41
It→Nl 18.24 17.97 18.02 18.98 19.74
Nl→It 18.11 17.59 18.16 19.18 19.87

Many-to-Many for Unbalanced Corpus

III
Supervised

Three-to-Three

En→De 27.60 24.39 23.78 25.45 26.06
De→En 32.23 28.85 28.14 28.98 30.37
En→Fi 16.83 14.58 13.82 14.26 14.77
Fi→En 22.37 19.60 19.15 19.96 21.03
En→Vi 26.78 28.89 28.84 30.49 32.01
Vi→En 25.72 27.19 27.27 29.14 31.71

Table 4: Translation performance under the many-to-many scenario, consisting of supervised four-to-four and
zero-shot translation on the balanced corpus, and supervised three-to-three on the unbalanced corpus. Note that
we do not use the Nl-Ro and It-Nl language pairs in our many-to-many translation task for the balanced corpus.

5.2.1 Results of Balanced Corpus

In part I of Table 4, our compact and language-
sensitive method (Rep+Emb+Attn+Dis) performs
consistently better than corresponding Multi-NMT
Baselines, and it can achieve the improvements
up to 1.30 BELU points (23.53 vs. 22.23 on
En→Ro). Although Rep+Emb method dramati-
cally reduces the model parameters, it performs on
par with Multi-NMT Baselines. Compared with
NMT Baselines model, our method also achieves
better results, which is nearly 2 BLEU points
on average. Experimental results on our bal-
anced corpus demonstrate that our method is ro-
bust and valid under the many-to-many translation
scenario.

5.2.2 Results of Unbalanced Corpus

For unbalanced corpus, our method can achieve
better results than Multi-NMT Baselines as well,
as shown in part III of Table 4. Moreover, from
the last two lines of this part, we can observe
that compared with NMT Baselines, the transla-
tion quality of En↔Vi can achieve the improve-
ments up to 5.23/5.99 BLEU points (32.01/31.71
vs. 26.78/25.72), both of which are new state-of-
the-art on these translation tasks to the best of our
knowledge. The results show that our method is
more effective in low-resource language pairs, es-
pecially for the unbalanced corpus.

5.2.3 Zero-Shot Results
Part II in Table 4 shows the performance of zero-
shot translation. Note that we conduct experi-
ments of this translation scenario using hybrid-
attention mechanism. Compared with Multi-NMT
Baselines, our compact and language-sensitive
method performs significantly better with the im-
provement as large as 2.28 BLEU points on
Nl→It. Note that the training datasets do not con-
tain parallel data for Nl-Ro and It-Nl.

It is interesting to figure out the translation
performance of Nl↔Ro and It↔Nl when bilin-
gual training corpus is available. We conduct ex-
periments of NMT Baselines on Nl-Ro and It-Nl
with all sentence pairs in IWSLT-17 (about 200k),
which is similar to other training pairs in our bal-
anced corpus. As shown in part II, Multi-NMT
Baselines underperform the NMT Baselines on all
cases. However, our method performs better than
NMT Baselines, and it achieves the improvement
up to 1.76 BLEU points on Nl→It translation task.

6 Related Work

Our work is related to two lines of research, and
we describe each of them as follows:

Model Compactness and Multi-NMT: To re-
duce the model size in NMT, weight pruning,
knowledge distillation, quantization, and weight
sharing (Kim and Rush, 2016; See et al., 2016;
He et al., 2018; Zhou et al., 2018) have been ex-



1221

plored. Due to the benefit of compactness, multi-
lingual translation has been extensively studied in
Dong et al. (2015), Luong et al. (2016) and John-
son et al. (2017). Owing to excellent translation
performance and ease of use, many researchers
(Blackwood et al., 2018; Lakew et al., 2018) have
conducted translation based on the framework of
Johnson et al. (2017) and Ha et al. (2016). Zhou
et al. (2019) propose to perform decoding in two
translation directions synchronously, which can be
applied on different target languages and is a new
research area for Multi-NMT. In our method, we
present a compact method for Multi-NMT, which
can not only compress the model but also yield su-
perior performance.

Low-Resource and Zero-Shot NMT: Many
researchers have explored low-resource NMT us-
ing transfer learning (Zoph et al., 2016; Neu-
big and Hu, 2018) and data augmenting (Sen-
nrich et al., 2016a; Zhang and Zong, 2016) ap-
proaches. For zero-shot translation, Cheng et al.
(2017) and Chen et al. (2017) utilize a pivot-based
method, which bridges the gap between source-
to-pivot and pivot-to-target two steps. Multilin-
gual translation is another direction to deal with
both low-resource and zero-shot translation. Gu
et al. (2018) enable sharing of lexical and sen-
tence representation across multiple languages, es-
pecially for extremely low-resource Multi-NMT.
Firat et al. (2016), Lakew et al. (2017), and John-
son et al. (2017) propose to make use of multi-
linguality in Multi-NMT to address the zero-shot
problem. In this work, we propose a method for
Multi-NMT to boost the accuracy of the multi-
lingual translation, which better fits on both low-
resource scenario and zero-shot scenario.

7 Conclusion

In this paper, we have proposed a compact and
language-sensitive method for multilingual trans-
lation. We first introduce a representor for replac-
ing both encoder and decoder so as to fully ex-
plore the commonality among languages. Based
on the representor architecture, we then propose
three language-specific modules dealing with em-
bedding, attention and language discrimination re-
spectively, in order to enhance the multilanguage
translation model with the ability of distinguish-
ing among different languages. The empirical ex-
periments demonstrate that our proposed methods
can outperform strong standard multilingual trans-

lation systems on one-to-many and many-to-many
translation tasks. Moreover, our method is proved
to be especially helpful in the low-resource and
zero-shot translation scenarios.

Acknowledgments

The research work descried in this paper has been
supported by the National Key Research and De-
velopment Program of China under Grant No.
2016QY02D0303 and the Natural Science Foun-
dation of China under Grant No. U1836221 and
61673380. The research work in this paper has
also been supported by Beijing Advanced Inno-
vation Center for Language Resources and So-
gou Inc. We would like to thank Yang Zhao and
Yuchen Liu for their invaluable discussions on this
paper.

References
Antonios Anastasopoulos and David Chiang. 2018.

Tied multitask learning for neural speech translation.
In Proceedings of NAACL 2018, pages 82–91.

Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-
ton. 2016. Layer normalization. arXiv preprint
arXiv:1607.06450.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In Proceedings of
ICLR 2015.

Graeme Blackwood, Miguel Ballesteros, and Todd
Ward. 2018. Multilingual neural machine transla-
tion with task-specific attention. In Proceedings of
COLING 2018, pages 3112–3122.

Yun Chen, Yong Cheng, Yang Liu, and Li Victor,
O.K. 2017. A teacher-student framework for zero-
resource neural machine translation. In Proceedings
of ACL 2017, pages 1925–1935.

Yong Cheng, Qian Yang, Yang Liu, Maosong Sun, and
Wei Xu. 2017. Joint training for pivot-based neu-
ral machine translation. Proceedings of IJCAI 2017,
pages 3974–3980.

Kyunghyun Cho, Bart van Merriënboer Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares Holger
Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using rnn encoder–decoder
for statistical machine translation. In Proceedings
of EMNLP 2014, pages 1724–1734.

Daxiang Dong, Hua Wu, Wei He, Dianhai Yu, and
Haifeng Wang. 2015. Multi-task learning for mul-
tiple language translation. In Proceedings of ACL
2015, pages 1723–1732.



1222

Orhan Firat, Kyunghyun Cho, and Yoshua Bengio.
2016. Multi-way, multilingual neural machine
translation with a shared attention mechanism. In
Proceedings of NAACL 2016, pages 866–875.

Jonas Gehring, Michael Auli, David Grangier, De-
nis Yarats, and Yann N. Dauphin. 2017. Convolu-
tional sequence to sequence learning. arXiv preprint
arXiv:1601.03317.

Jiatao Gu, Hany Hassan, Jacob Devlin, and Victor OK
Li. 2018. Universal neural machine translation for
extremely low resource languages. In Proceedings
of NAACL 2018, pages 344–354.

Thanh-Le Ha, Jan Niehues, and Alexander Waibel.
2016. Toward multilingual neural machine trans-
lation with universal encoder and decoder. In Pro-
ceedings of IWSLT 2016.

Tianyu He, Xu Tan, Yingce Xia, Di He, Tao Qin, Zhibo
Chen, and Tie-Yan Liu. 2018. Layer-wise coordi-
nation between encoder and decoder for neural ma-
chine translation. In Proceedings of NIPS 2018,
pages 7944–7954.

Melvin Johnson, Mike Schuster, Quoc V. Le, Maxim
Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat,
Fernanda Viégas, Martin Wattenberg, Greg Corrado,
Macduff Hughes, and Jeffrey Dean. 2017. Google’s
multilingual neural machine translation system: En-
abling zero-shot translation. Transactions of the As-
sociation for Computational Linguistics, 5:339–351.

Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
continuous translation models. In Proceedings of
EMNLP 2013, pages 1700–1709.

Yoon Kim and Alexander M. Rush. 2016. Sequence-
level knowledge distillation. In Proceedings of
EMNLP 2016, pages 1317–1327.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Surafel M Lakew, ADG Mattia, and F Marcello. 2017.
Multilingual neural machine translation for low re-
source languages. CLiC-it.

Surafel Melaku Lakew, Mauro Cettolo, and Marcello
Federico. 2018. A comparison of transformer and
recurrent neural networks on multilingual neural
machine translation. In Proceedings of COLING
2018, pages 641–652.

Yichao Lu, Phillip Keung, Faisal Ladhak, Vikas Bhard-
waj, Shaonan Zhang, and Jason Sun. 2018. A neural
interlingua for multilingual machine translation. In
Proceedings of WMT 2018, pages 84–92.

Minh-Thang Luong, Quoc V Le, Ilya Sutskever, Oriol
Vinyals, and Lukasz Kaiser. 2016. Multi-task se-
quence to sequence learning. In Proceedings of
ICLR 2016.

Minh-Thang Luong, Hieu Pham, and Christopher D
Manning. 2015. Effective approaches to attention-
based neural machine translation. In Proceedings of
EMNLP, pages 1412–1421.

Graham Neubig and Junjie Hu. 2018. Rapid adaptation
of neural machine translation to new languages. In
Proceedings of EMNLP 2018, pages 875–880.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of
ACL, pages 311–318.

Emmanouil Antonios Platanios, Mrinmaya Sachan,
Graham Neubig, and Tom Mitchell. 2018. Contex-
tual parameter generation for universal neural ma-
chine translation. In Proceedings of EMNLP 2018,
pages 425–435.

Ofir Press and Lior Wolf. 2017. Using the output em-
bedding to improve language models. In Proceed-
ings of EACL 2017, pages 157–163.

Mike Schuster and Kaisuke Nakajima. 2012. Japanese
and korean voice search. In Proceedings of ICASSP
2012.

Abigail See, Minh-Thang Luong, and Christopher D.
Manning. 2016. Compression of neural machine
translation models via pruning. In Proceedings of
SIGNLL 2016, pages 291–301.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016a. Improving neural machine translation mod-
els with monolingual data. In Proceedings of ACL
2016, pages 86–96.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016b. Neural machine translation of rare words
with subword units. In Proceedings of ACL 2016,
pages 1715–1725.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Proceedings of NIPS, pages 3104–3112.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, and
Łukasz Kaiser. 2017. Attention is all you need. In
Proceedings of NIPS, pages 30–34.

Yining Wang, Jiajun Zhang, Feifei Zhai, Jingfang Xu,
and Chengqing Zong. 2018. Three strategies to im-
prove one-to-many multilingual translation. In Pro-
ceedings of EMNLP 2018, pages 2955–2960.

Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V
Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus
Macherey, et al. 2016. Google’s neural ma-
chine translation system: Bridging the gap between
human and machine translation. arXiv preprint
arXiv:1609.08144.



1223

Jiajun Zhang and Chengqing Zong. 2015. Deep neu-
ral networks in machine translation: An overview.
IEEE Intelligent Systems, 30(5):16–25.

Jiajun Zhang and Chengqing Zong. 2016. Exploit-
ing source-side monolingual data in neural machine
translation. In Proceedings of EMNLP, pages 1535–
1545.

Long Zhou, Yuchen Liu, Jiajun Zhang, Chengqing
Zong, and Guoping Huang. 2018. Language-
independent representor for neural machine transla-
tion. arXiv preprint arXiv:1811.00258.

Long Zhou, Jiajun Zhang, and Chengqing Zong. 2019.
Synchronous bidirectional neural machine transla-
tion. Transactions of the Association for Compu-
tational Linguistics, 7:91–105.

Barret Zoph and Kevin Knight. 2016. Multi-source
neural translation. In Proceedings of NAACL 2016,
pages 30–34.

Barret Zoph, Deniz Yuret, Jonathan May, and Kevin
Knight. 2016. Transfer learning for low-resource
neural machine translation. In Proceedings of
EMNLP 2016, pages 1568–1575.


