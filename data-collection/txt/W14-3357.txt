



















































Using Comparable Corpora to Adapt MT Models to New Domains


Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 437–444,
Baltimore, Maryland USA, June 26–27, 2014. c©2014 Association for Computational Linguistics

Using Comparable Corpora to Adapt MT Models to New Domains

Ann Irvine
Center for Language and Speech Processing

Johns Hopkins University

Chris Callison-Burch
Computer and Information Science Dept.

University of Pennsylvania

Abstract
In previous work we showed that when us-
ing an SMT model trained on old-domain
data to translate text in a new-domain,
most errors are due to unseen source
words, unseen target translations, and in-
accurate translation model scores (Irvine
et al., 2013a). In this work, we target er-
rors due to inaccurate translation model
scores using new-domain comparable cor-
pora, which we mine from Wikipedia. We
assume that we have access to a large old-
domain parallel training corpus but only
enough new-domain parallel data to tune
model parameters and do evaluation. We
use the new-domain comparable corpora
to estimate additional feature scores over
the phrase pairs in our baseline models.
Augmenting models with the new features
improves the quality of machine transla-
tions in the medical and science domains
by up to 1.3 BLEU points over very strong
baselines trained on the 150 million word
Canadian Hansard dataset.

1 Introduction

Domain adaptation for machine translation is
known to be a challenging research problem that
has substantial real-world application. In this set-
ting, we have access to training data in some old-
domain of text but very little or no training data
in the domain of the text that we wish to translate.
For example, we may have a large corpus of par-
allel newswire training data but no training data in
the medical domain, resulting in low quality trans-
lations at test time due to the mismatch.

In Irvine et al. (2013a), we introduced a tax-
onomy for classifying machine translation errors

related to lexical choice. Our ‘S4’ taxonomy in-
cludes seen, sense, score, and search errors. Seen
errors result when a source language word or
phrase in the test set was not observed at all during
training. Sense errors occur when the source lan-
guage word or phrase was observed during train-
ing but not with the correct target language trans-
lation. If the source language word or phrase was
observed with its correct translation during train-
ing, but an incorrect alternative outweighs the cor-
rect translation, then a score error has occurred.
Search errors are due to pruning in beam search
decoding. We measured the impact of each error
type in a domain adaptation setting and concluded
that seen and sense errors are the most frequent but
that there is also room for improving errors due to
inaccurate translation model scores (Irvine et al.,
2013a). In this work, we target score errors, using
comparable corpora to reduce their frequency in a
domain adaptation setting.

We assume the setting where we have an old-
domain parallel training corpus but no new domain
training corpus.1 We do, however, have access
to a mixed-domain comparable corpus. We iden-
tify new-domain text within our comparable cor-
pus and use that data to estimate new translation
features on the translation models extracted from
old-domain training data. Specifically, we focus
on the French-English language pair because care-
fully curated datasets exist in several domains for
tuning and evaluation. Following our prior work,
we use the Canadian Hansard parliamentary pro-
ceedings as our old-domain and adapt models to
both the medical and the science domains (Irvine
et al., 2013a). At over 8 million sentence pairs,

1Some prior work has referred to old-domain and new-
domain corpora as out-of-domain and in-domain, respec-
tively.

437



the Canadian Hansard dataset is one of the largest
publicly available parallel corpora and provides a
very strong baseline. We give details about each
dataset in Section 4.1.

We use comparable corpora to estimate sev-
eral signals of translation equivalence. In partic-
ular, we estimate the contextual, topic, and or-
thographic similarity of each phrase pair in our
baseline old-domain translation model. In Sec-
tion 3, we describe each feature in detail. Us-
ing just 5 thousand comparable new-domain doc-
ument pairs, which we mine from Wikipedia, and
five new phrase table features, we observe perfor-
mance gains of up to 1.3 BLEU points on the sci-
ence and medical translation tasks over very strong
baselines.

2 Related Work

Recent work on machine translation domain adap-
tation has focused on either the language model-
ing component or the translation modeling com-
ponent of an SMT model. Language modeling re-
search has explored methods for subselecting new-
domain data from a large monolingual target lan-
guage corpus for use as language model training
data (Lin et al., 1997; Klakow, 2000; Gao et al.,
2002; Moore and Lewis, 2010; Mansour et al.,
2011). Translation modeling research has typi-
cally assumed that either (1) two parallel datasets
are available, one in the old domain and one in the
new, or (2) a large, mixed-domain parallel training
corpus is available. In the first setting, the goal is
to effectively make use of both the old-domain and
the new-domain parallel training corpora (Civera
and Juan, 2007; Koehn and Schroeder, 2007; Fos-
ter and Kuhn, 2007; Foster et al., 2010; Haddow
and Koehn, 2012; Haddow, 2013). In the sec-
ond setting, it has been shown that, in some cases,
training a translation model on a subset of new-
domain parallel training data within a larger train-
ing corpus can be more effective than using the
complete dataset (Mansour et al., 2011; Axelrod
et al., 2011; Sennrich, 2012; Gascó et al., 2012).

For many language pairs and domains, no new-
domain parallel training data is available. Wu et
al. (2008) machine translate new-domain source
language monolingual corpora and use the syn-
thetic parallel corpus as additional training data.
Daumé and Jagarlamudi (2011), Zhang and Zong
(2013), and Irvine et al. (2013b) use new-domain
comparable corpora to mine translations for un-

seen words. That work follows a long line of re-
search on bilingual lexicon induction (e.g. Rapp
(1995), Schafer and Yarowsky (2002), Koehn and
Knight (2002), Haghighi et al. (2008), Irvine and
Callison-Burch (2013), Razmara et al. (2013)).
These efforts improve S4 seen, and, in some in-
stances, sense error types. To our knowledge,
no prior work has focused on fixing errors due
to inaccurate translation model scores in the set-
ting where no new-domain parallel training data is
available.

In Klementiev et al. (2012), we used compara-
ble corpora to estimate several features for a given
phrase pair that indicate translation equivalence,
including contextual, temporal, and topical simi-
larity. The definitions of phrasal and lexical con-
textual and topic similarity that we use here are
taken from our prior work, where we replaced
bilingually estimated phrase table features with
the new features and cited applications to low re-
source SMT. In this work we also focus on scoring
a phrase table using comparable corpora. How-
ever, here we work in a domain adaptation setting
and seek to augment, not replace, an existing set
of bilingually estimated phrase table features.

3 Phrase Table Scoring

We begin with a scored phrase table estimated us-
ing our old-domain parallel training corpus. The
phrase table contains about 201 million unique
source phrases up to length seven and about 479
million total phrase pairs. We use Wikipedia as a
source for comparable document pairs (details are
given in Section 4.1). We augment the bilingually
estimated features with the following: (1) lexical
and phrasal contextual similarity estimated over a
comparable corpus, (2) lexical and phrasal topi-
cal similarity estimated over a comparable corpus,
and (3) lexical orthographic similarity.

Contextual Similarity We estimate contextual
similarity2 by first computing a context vector for
each source and target word and phrase in our
phrase table using the source and target sides of
our comparable corpus, respectively. We begin by
collecting vectors of counts of words that appear
in the context of each source and target phrase, ps
and pt. We use a bag-of-words context consist-
ing of the two words to the left and two words to

2Similar to distributional similarity, which is typically de-
fined monolingually.

438



the right of each occurrence of each phrase. Vari-
ous means of computing the component values of
context vectors from raw context frequency counts
have been proposed (e.g. Rapp (1999), Fung and
Yee (1998)). Following Fung and Yee (1998), we
compute the value of the k-th component of ps’s
contextual vector, Cps , as follows:

Cpsk “ nps,k ˚ plogpn{nkq ` 1q

where nps,k and nk are the number of times the
k-th source word, sk, appears in the context of ps
and in the entire corpus, and n is the maximum
number of occurrences of any word in the data.
Intuitively, the more frequently sk appears with ps
and the less common it is in the corpus in general,
the higher its component value. The context vector
for ps, Cps , is M -dimensional, where M is the
size of the source language vocabulary. Similarly,
we compute N -dimensional context vectors for all
target language words and phrases, where N is the
size of the target language vocabulary.

We identify the most probable translation t for
each of the M source language words, s, as the
target word with the highest ppt|sq under our word
aligned old-domain training corpus. Given this
dictionary of unigram translations, we then project
each M -dimensional source language context vec-
tor into the N -dimensional target language context
vector space. To compare a given pair of source
and target context vectors, Cps and Cpt , respec-
tively, we compute their cosine similarity, or their
dot product divided by the product of their magni-
tudes:

simcontextualpps, ptq “
Cps ¨ Cpt
||Cps ||||Cpt ||

For a given phrase pair in our phrase table, we
estimate phrasal contextual similarity by directly
comparing the context vectors of the two phrases
themselves. Because context vectors for phrases,
which tend to be less frequent than words, can be
sparse, we also compute lexical contextual simi-
larity over phrase pairs. We define lexical con-
textual similarity as the average of the contextual
similarity between all word pairs within the phrase
pair.

Topic Similarity Phrases and their translations
are likely to appear in articles written about the
same topic in two languages. We estimate topic
similarity using the distribution of words and
phrases across Wikipedia pages, for which we

have interlingual French-English links. Specif-
ically, we compute topical vectors by counting
the number of occurrences of each word and
phrase across Wikipedia pages. That is, for each
source and target phrase, ps and pt, we collect M -
dimensional topic vectors, where M is the number
of Wikipedia page pairs used (in our experiments,
M is typically 5, 000). We use Wikipedia’s inter-
lingual links to align the French and English topic
vectors and normalize each topic vector by the to-
tal count. As with contextual similarity, we com-
pare a pair of source and target topic vectors, Tps
and Tpt , respectively, using cosine similarity:

simtopicpps, ptq “
Tps ¨ Tpt
||Tps ||||Tpt ||

We estimate both phrasal and lexical topic simi-
larity for each phrase pair. As before, lexical topic
similarity is estimated by taking an average topic
similarity across all word pairs in a given phrase
pair.

Orthographic Similarity We make use of one
additional signal of translation equivalence: ortho-
graphic similarity. In this case, we do not refer-
ence comparable corpora but simply compute the
edit distance between a given pair of phrases. This
signal is often useful for identifying translations
of technical terms, which appear frequently in our
medical and science domain corpora. However,
because of word order variation, we do not mea-
sure edit distance on phrase pairs directly. For ex-
ample, French embryon humain translates as En-
glish human embryo; embryon translates as em-
bryo and humain translates as human. Although
both word pairs are cognates, the words appear
in opposite orders in the two phrases. Therefore,
directly measuring string edit distance across the
phrase pair would not effectively capture the relat-
edness of the words. Hence, we only measure lex-
ical orthographic similarity, not phrasal. We com-
pute lexical orthographic similarity by first com-
puting the edit distance between each word pair,
ws and wt, within a given phrase pair, normalized
by the lengths of the two words:

simorthpws, wtq “
edpws, wtq

|ws||wt|
2

We then compute the average normalized edit dis-
tance across all word pairs.

The above similarity metrics all allow for scores
of zero, which can be problematic for our log-

439



Corpus Source Words Target Words
Training
Canadian Hansard 161.7 m 144.5 m
Tune-1 / Tune-2 / Test
Medical 53k / 43k / 35k 46k / 38k / 30k
Science 92k / 120k / 120k 75k / 101k / 101k
Language Modeling and Comparable Corpus Selection
Medical - 5.9 m
Science - 3.6 m

Table 1: Summary of the size of each corpus of text used
in this work in terms of the number of source and target word
tokens.

linear translation models. We describe our ex-
periments with different minimum score cutoffs in
Section 4.2.

4 Experimental Setup

4.1 Data
We assume that the following data is available in
our translation setting:

• Large old-domain parallel corpus for training
• Small new-domain parallel corpora for tun-

ing and testing

• Large new-domain English monolingual cor-
pus for language modeling and identifying
new-domain-like comparable corpora

• Large mixed-domain comparable corpus,
which includes some text from the new-
domain

These data conditions are typical for many real-
world uses of machine translation. A summary of
the size of each corpus is given in Table 1.

Our old-domain training data is taken from
the Canadian Hansard parliamentary proceedings
dataset, which consists of manual transcriptions
and translations of meetings of the Canadian par-
liament. The dataset is substantially larger than
the commonly used Europarl corpus, containing
over 8 million sentence pairs and about 150 mil-
lion word tokens of French and English.

For tuning and evaluation, we use new-domain
medical and science parallel datasets released by
Irvine et al. (2013a). The medical texts con-
sist of documents from the European Medical
Agency (EMEA), originally released by Tiede-
mann (2009). This data is primarily taken from
prescription drug label text. The science data is
made up of translated scientific abstracts from the

fields of physics, biology, and computer science.
For both the medical and science domains, we
use three held-out parallel datasets of about 40
and 100 thousand words,3 respectively, released
by Irvine et al. (2013a). We do tuning on dev1,
additional parameter selection on test2, and blind
testing on test1.

We use large new-domain monolingual English
corpora for language modeling and for selecting
new-domain-like comparable corpora from our
mixed domain comparable corpus. Specifically,
we use the English side of the medical and science
training datasets released by Irvine et al. (2013a).
We do not use the parallel French side of the train-
ing data at all; our data setting assumes that no
new-domain parallel data is available for training.

We use Wikipedia as a source of compara-
ble corpora. There are over half a million
pairs of inter-lingually linked French and English
Wikipedia documents.4 We assume that we have
enough monolingual new-domain data in one lan-
guage to rank Wikipedia pages according to how
new-domain-like they are. In particular, we use
our new-domain English language modeling data
to measure new-domain-likeness. We could have
targeted our learning even more by using our new-
domain French test sets to select comparable cor-
pora. Doing so may increase the similarity be-
tween our test data and comparable corpora. How-
ever, to avoid overfitting any particular test set, we
use our large English new-domain language mod-
eling corpus instead.

For each inter-lingually linked pair of French
and English Wikipedia documents, we compute
the percent of English phrases up to length four
that are observed in the English monolingual new-
domain corpus and rank document pairs by the ge-
ometric mean of the four overlap measures. More
sophisticated ways to identify new-domain-like
Wikipedia pages (e.g. (Moore and Lewis, 2010))
may yield additional performance gains, but, qual-
itatively, the ranked Wikipedia pages seem rea-
sonable for the purposes of generating a large set
of top-k new-domain document pairs. The top-10
ranked pages for each domain are listed in Table 2.
The top ranked science domain pages are primar-
ily related to concepts from the field of physics
but also include computer science and chemistry

3Or about 4 thousand lines each. The sentences in the
medical domain text are much shorter than those in the sci-
ence domain.

4As of January 2014.

440



Science Medical
Diagnosis (artificial intelligence) Pregabalin

Absorption spectroscopy Cetuximab
Spectral line Fluconazole

Chemical kinetics Calcitonin
Mahalanobis distance Pregnancy category

Dynamic light scattering Trazodone
Amorphous solid Rivaroxaban

Magnetic hyperthermia Spironolactone
Photoelasticity Anakinra

Galaxy rotation curve Cladribine

Table 2: Top 10 Wikipedia articles ranked by their similar-
ity to large new-domain English monolingual corpora.

topics. The top ranked medical domain pages are
nearly all prescription drugs, which makes sense
given the content of the EMEA medical corpus.

4.2 Phrase-based Machine Translation

We word align our old-domain training corpus
using GIZA++ and use the Moses SMT toolkit
(Koehn et al., 2007) to extract a translation gram-
mar. In this work, we focus on phrase-based
SMT models, however our approach to using new-
domain comparable corpora to estimate translation
scores is theoretically applicable to any type of
translation grammar.

Our baseline models use a phrase limit of seven
and the standard phrase-based SMT feature set, in-
cluding forward and backward phrase and lexical
translation probabilities. Additionally, we use the
standard lexicalized reordering model. We exper-
iment with two 5-gram language models trained
using SRILM with Kneser-Ney smoothing on (1)
the English side of the Hansard training corpus,
and (2) the relevant new-domain monolingual En-
glish corpus. We experiment with using, first, only
the old-domain language model and, then, both the
old-domain and the new-domain language models.

Our first comparison system augments the stan-
dard feature set with the orthographic similarity
feature, which is not based on comparable cor-
pora. Our second comparison system uses both
the orthographic feature and the contextual and
topic similarity features estimated over a random
set of comparable document pairs. The third sys-
tem estimates contextual and topic similarity using
new-domain-like comparable corpora. We tune
our phrase table feature weights for each model
separately using batch MIRA (Cherry and Fos-
ter, 2012) and new-domain tuning data. Results
are averaged over three tuning runs, and we use
the implementation of approximate randomization

released by Clark et al. (2011) to measure the
statistical significance of each feature-augmented
model compared with the baseline model that uses
the same language model(s).

As noted in Section 3, the features that we
estimate from comparable corpora may be zero-
valued. We use our second tuning sets5 to tune
a minimum threshold parameter for our new fea-
tures. We measure performance in terms of BLEU
score on the second tuning set as we vary the new
feature threshold between 1e´07 and 0.5 for each
domain. A threshold of 0.01, for example, means
that we replace all feature with values less than
0.01 with 0.01. For both new-domains, perfor-
mance drops when we use thresholds lower than
0.01 and higher than 0.25. We use a minimum
threshold of 0.1 for all experiments presented be-
low for both domains.

5 Results

Table 3 presents a summary of our results on the
test set in each domain. Using only the old-domain
language model, our baselines yield BLEU scores
of 22.70 and 21.29 on the medical and science test
sets, respectively. When we add the orthographic
similarity feature, BLEU scores increase signifi-
cantly, by about 0.4 on the medical data and 0.6 on
science. Adding the contextual and topic features
estimated over a random selection of comparable
document pairs improves BLEU scores slightly in
both domains. Finally, using the most new-domain
like document pairs to estimate the contextual and
topic features yields a 1.3 BLEU score improve-
ment over the baseline in both domains. For both
domains, this result is a statistically significant im-
provement6 over each of the first three systems.

In both domains, the new-domain language
models contribute substantially to translation qual-
ity. Baseline BLEU scores increase by about
6 and 5 BLEU score points in the medical and
science domains, respectively, when we add the
new-domain language models. In the medical do-
main, neither the orthographic feature nor the or-
thographic feature in combination with contextual
and topic features estimated over random docu-
ment pairs results in a significant BLEU score
improvement. However, using the orthographic
feature and the contextual and topic features es-
timated over new-domain document pairs yields a

5test2 datasets released by Irvine et al. (2013a)
6p-value ă 0.01

441



Language Model(s) System Medical Science

Old

Baseline 22.70 21.29
+ Orthographic Feature 23.09* (`0.4) 21.86* (`0.6)

+ Orthographic & Random CC Features 23.22* (`0.5) 21.88* (`0.6)
+ Orthographic & New-domain CC Features 23.98* (`1.3) 22.55* (`1.3)

Old+New

Baseline 28.82 26.18
+ Orthographic Feature 29.02 (`0.2) 26.40* (`0.2)

+ Orthographic & Random CC Features 28.86 (`0.0) 26.52* (`0.3)
+ Orthographic & New-domain CC Features 29.16* (`0.3) 26.50* (`0.3)

Table 3: Comparison between the performance of baseline old-domain translation models and domain-adapted models in
translating science and medical domain text. We experiment with two language models: old, trained on the English side of our
Hansard old-domain training corpus and new, trained on the English side of the parallel training data in each new domain. We
use comparable corpora of 5, 000 (1) random, and (2) the most new-domain-like document pairs to score phrase tables. All
results are averaged over three tuning runs, and we perform statistical significance testing comparing each system augmented
with additional features with the baseline system that uses the same language model(s). * indicates that the BLEU scores are
statistically significant with p ă 0.01.

small but significant improvement of 0.3 BLEU.
In the science domain, in contrast, all three aug-
mented models perform statistically significantly
better than the baseline. Contextual and topic fea-
tures yield only a slight improvement above the
model that uses only the orthographic feature, but
the difference is statistically significant. For the
science domain, when we use the new domain lan-
guage model, there is no difference between esti-
mating the contextual and topic features over ran-
dom comparable document pairs and those chosen
for their similarity with new-domain data.

Differences across domains may be due to the
fact that the medical domain corpora are much
more homogenous, containing the often boiler-
plate text of prescription drug labels, than the sci-
ence domain corpora. The science domain cor-
pora, in contrast, contain abstracts from several
different scientific fields; because that data is more
diverse, a randomly chosen mixed-domain set of
comparable corpora may still be relevant and use-
ful for adapting a translation model.

We experimented with varying the number of
comparable document pairs used for estimating
contextual and topic similarity but saw no sig-
nificant gains from using more than 5, 000 in ei-
ther domain. In fact, performance dropped in the
medical domain when we used more than a few
thousand document pairs. Our proposed approach
orders comparable document pairs by how new-
domain-like they are and augments models with
new features estimated over the top-k. As a result,
using more comparable document pairs means that
there is more data from which to estimate sig-
nals, but it also means that the data is less new-

domain like overall. Using a domain similarity
threshold to choose a subset of comparable doc-
ument pairs may prove useful in future work, as
the ideal amount of comparable data will depend
on the type and size of the initial mixed-domain
comparable corpus as well as the homogeneity of
the text domain of interest.

We also experimented with using a third lan-
guage model estimated over the English side of
our comparable corpora. However, we did not see
any significant improvements in translation qual-
ity when we used this language model in combina-
tion with the old and new domain language mod-
els.

6 Conclusion

In this work, we targeted SMT errors due
to translation model scores using new-domain
comparable corpora. Our old-domain French-
English baseline model was trained on the Cana-
dian Hansard parliamentary proceedings dataset,
which, at 8 million sentence pairs, is one of the
largest publicly available parallel datasets. Our
task was to adapt this baseline to the medical and
scientific text domains using comparable corpora.
We used new-domain parallel data only to tune
model parameters and do evaluation. We mined
Wikipedia for new-domain-like comparable docu-
ment pairs, over which we estimated several addi-
tional features scores: contextual, temporal, and
orthographic similarity. Augmenting the strong
baseline with our new feature set improved the
quality of machine translations in the medical and
science domains by up to 1.3 BLEU points.

442



7 Acknowledgements

This material is based on research sponsored by
DARPA under contract HR0011-09-1-0044 and
by the Johns Hopkins University Human Lan-
guage Technology Center of Excellence. The
views and conclusions contained in this publica-
tion are those of the authors and should not be
interpreted as representing official policies or en-
dorsements of DARPA or the U.S. Government.

References
Amittai Axelrod, Xiaodong He, and Jianfeng Gao.

2011. Domain adaptation via pseudo in-domain
data selection. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP).

Colin Cherry and George Foster. 2012. Batch tuning
strategies for statistical machine translation. In Pro-
ceedings of the Conference of the North American
Chapter of the Association for Computational Lin-
guistics (NAACL).

Jorge Civera and Alfons Juan. 2007. Domain adap-
tation in statistical machine translation with mixture
modelling. In Proceedings of the Workshop on Sta-
tistical Machine Translation (WMT).

Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis testing
for statistical machine translation: controlling for
optimizer instability. In Proceedings of the Confer-
ence of the Association for Computational Linguis-
tics (ACL).

Hal Daumé, III and Jagadeesh Jagarlamudi. 2011.
Domain adaptation for machine translation by min-
ing unseen words. In Proceedings of the Confer-
ence of the Association for Computational Linguis-
tics (ACL).

George Foster and Roland Kuhn. 2007. Mixture-
model adaptation for SMT. In Proceedings of
the Workshop on Statistical Machine Translation
(WMT).

G. Foster, C. Goutte, and R. Kuhn. 2010. Discrimi-
native instance weighting for domain adaptation in
SMT. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP).

Pascale Fung and Lo Yuen Yee. 1998. An IR approach
for translating new words from nonparallel, compa-
rable texts. In Proceedings of the Conference of the
Association for Computational Linguistics (ACL).

Jianfeng Gao, Joshua Goodman, Mingjing Li, and Kai-
Fu Lee. 2002. Toward a unified approach to sta-
tistical language modeling for chinese. ACM Trans-
actions on Asian Language Information Processing
(TALIP).

Guillem Gascó, Martha-Alicia Rocha, Germán
Sanchis-Trilles, Jesús Andrés-Ferrer, and Francisco
Casacuberta. 2012. Does more data always yield
better translations? In Proceedings of the Confer-
ence of the European Association for Computational
Linguistics (EACL).

Barry Haddow and Philipp Koehn. 2012. Analysing
the effect of out-of-domain data on SMT systems. In
Proceedings of the Workshop on Statistical Machine
Translation (WMT).

Barry Haddow. 2013. Applying pairwise ranked op-
timisation to improve the interpolation of transla-
tion models. In Proceedings of the Conference of
the North American Chapter of the Association for
Computational Linguistics (NAACL).

Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexi-
cons from monolingual corpora. In Proceedings of
the Conference of the Association for Computational
Linguistics (ACL).

Ann Irvine and Chris Callison-Burch. 2013. Su-
pervised bilingual lexicon induction with multiple
monolingual signals. In Proceedings of the Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics (NAACL).

Ann Irvine, John Morgan, Marine Carpuat, Hal Daumé
III, and Dragos Munteanu. 2013a. Measuring ma-
chine translation errors in new domains. Transac-
tions of the Association for Computational Linguis-
tics, 1(October).

Ann Irvine, Chris Quirk, and Hal Daume III. 2013b.
Monolingual marginal matching for translation
model adaptation. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP).

Dietrich Klakow. 2000. Selecting articles from the
language model training corpus. In Proceedings
of the IEEE International Conference on Acoustics,
Speech, and Signal Processing (ICASSP).

Alex Klementiev, Ann Irvine, Chris Callison-Burch,
and David Yarowsky. 2012. Toward statistical ma-
chine translation without parallel corpora. In Pro-
ceedings of the Conference of the European Associ-
ation for Computational Linguistics (EACL).

Philipp Koehn and Kevin Knight. 2002. Learning a
translation lexicon from monolingual corpora. In
ACL Workshop on Unsupervised Lexical Acquisi-
tion.

Philipp Koehn and Josh Schroeder. 2007. Experiments
in domain adaptation for statistical machine transla-
tion. In Proceedings of the Workshop on Statistical
Machine Translation (WMT).

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,

443



Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the Conference of the Association for
Computational Linguistics (ACL).

Sung-Chien Lin, Chi-Lung Tsai, Lee-Feng Chien, Ker-
Jiann Chen, and Lin-Shan Lee. 1997. Chinese lan-
guage model adaptation based on document classifi-
cation and multiple domain-specific language mod-
els. In Fifth European Conference on Speech Com-
munication and Technology.

Saab Mansour, Joern Wuebker, and Hermann Ney.
2011. Combining translation and language model
scoring for domain-specific data filtering. In Pro-
ceedings of the International Workshop on Spoken
Language Translation (IWSLT).

Robert C. Moore and William Lewis. 2010. Intelli-
gent selection of language model training data. In
Proceedings of the Conference of the Association for
Computational Linguistics (ACL).

Reinhard Rapp. 1995. Identifying word translations
in non-parallel texts. In Proceedings of the Confer-
ence of the Association for Computational Linguis-
tics (ACL).

Reinhard Rapp. 1999. Automatic identification of
word translations from unrelated English and Ger-
man corpora. In Proceedings of the Conference
of the Association for Computational Linguistics
(ACL).

Majid Razmara, Maryam Siahbani, Reza Haffari, and
Anoop Sarkar. 2013. Graph propagation for para-
phrasing out-of-vocabulary words in statistical ma-
chine translation. In Proceedings of the Confer-
ence of the Association for Computational Linguis-
tics (ACL).

Charles Schafer and David Yarowsky. 2002. Inducing
translation lexicons via diverse similarity measures
and bridge languages. In Proceedings of the Confer-
ence on Natural Language Learning (CoNLL).

Rico Sennrich. 2012. Perplexity minimization for
translation model domain adaptation in statistical
machine translation. In Proceedings of the Confer-
ence of the European Association for Computational
Linguistics (EACL).

Jörg Tiedemann. 2009. News from OPUS - A collec-
tion of multilingual parallel corpora with tools and
interfaces. In N. Nicolov, K. Bontcheva, G. An-
gelova, and R. Mitkov, editors, Recent Advances in
Natural Language Processing (RANLP).

Hua Wu, Haifeng Wang, and Chengqing Zong. 2008.
Domain adaptation for statistical machine transla-
tion with domain dictionary and monolingual cor-
pora. In Proceedings of the International Confer-
ence on Computational Linguistics (COLING).

Jiajun Zhang and Chengqing Zong. 2013. Learning
a phrase-based translation model from monolingual
data with application to domain adaptation. In Pro-
ceedings of the Conference of the Association for
Computational Linguistics (ACL).

444


