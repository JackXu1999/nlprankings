




































Rethinking Action Spaces for Reinforcement Learning in End-to-end Dialog Agents with Latent Variable Models


Proceedings of NAACL-HLT 2019, pages 1208–1218
Minneapolis, Minnesota, June 2 - June 7, 2019. c©2019 Association for Computational Linguistics

1208

Rethinking Action Spaces for Reinforcement Learning in End-to-end
Dialog Agents with Latent Variable Models

Tiancheng Zhao1, Kaige Xie2 and Maxine Eskenazi1
1Language Technologies Institute, Carnegie Mellon University

2Shanghai Jiao Tong University
{tianchez, max+}@cs.cmu.edu, lightyear0117@sjtu.edu.cn

Abstract

Defining action spaces for conversational
agents and optimizing their decision-making
process with reinforcement learning is an en-
during challenge. Common practice has been
to use handcrafted dialog acts, or the output
vocabulary, e.g. in neural encoder decoders, as
the action spaces. Both have their own limita-
tions. This paper proposes a novel latent action
framework that treats the action spaces of an
end-to-end dialog agent as latent variables and
develops unsupervised methods in order to in-
duce its own action space from the data. Com-
prehensive experiments are conducted exam-
ining both continuous and discrete action types
and two different optimization methods based
on stochastic variational inference. Results
show that the proposed latent actions achieve
superior empirical performance improvement
over previous word-level policy gradient meth-
ods on both DealOrNoDeal and MultiWoz di-
alogs. Our detailed analysis also provides in-
sights about various latent variable approaches
for policy learning and can serve as a founda-
tion for developing better latent actions in fu-
ture research. 1

1 Introduction

Optimizing dialog strategies in multi-turn dialog
models is the cornerstone of building dialog sys-
tems that more efficiently solve real-world chal-
lenges, e.g. providing information (Young, 2006),
winning negotiations (Lewis et al., 2017), improv-
ing engagement (Li et al., 2016) etc. A clas-
sic solution employs reinforcement learning (RL)
to learn a dialog policy that models the opti-
mal action distribution conditioned on the dialog
state (Williams and Young, 2007). However, since
there are infinite human language possibilities, an
enduring challenge has been to define what the

1Data and code are available at https://github.
com/snakeztc/NeuralDialog-LaRL

action space is. For traditional modular systems,
the action space is defined by hand-crafted seman-
tic representations such as dialog acts and slot-
values (Raux et al., 2005; Chen et al., 2013) and
the goal is to obtain a dialog policy that chooses
the best hand-crafted action at each dialog turn.
But it is limited because it can only handle simple
domains whose entire action space can be captured
by hand-crafted representations (Walker, 2000; Su
et al., 2017). This cripples a system’s ability to
handle conversations in complex domains.

Conversely, end-to-end (E2E) dialog systems
have removed this limit by directly learning a re-
sponse generation model conditioned on the dia-
log context using neural networks (Vinyals and Le,
2015; Sordoni et al., 2015). To apply RL to E2E
systems, the action space is typically defined as
the entire vocabulary; every response output word
is considered to be an action selection step (Li
et al., 2016), which we denote as the word-level
RL. Word-level RL, however, has been shown to
have several major limitations in learning dialog
strategies. The foremost one is that direct appli-
cation of word-level RL leads to degenerate be-
havior: the response decoder deviates from human
language and generates utterances that are incom-
prehensible (Lewis et al., 2017; Das et al., 2017;
Kottur et al., 2017). A second issue is that since
a multi-turn dialog can easily span hundreds of
words, word-level RL suffers from credit assign-
ment over a long horizon, leading to slow and sub-
optimal convergence (Kaelbling et al., 1996; He
et al., 2018).

This paper proposes Latent Action Reinforce-
ment Learning (LaRL), a novel framework that
overcomes the limitations of word-level RL for
E2E dialog models, marrying the benefits of a
traditional modular approach in an unsupervised
manner. The key idea is to develop E2E mod-
els that can invent their own discourse-level ac-



1209

tions. These actions must be expressive enough to
capture response semantics in complex domains
(i.e. have the capacity to represent a large num-
ber of actions), thus decoupling the discourse-
level decision-making process from natural lan-
guage generation. Then any RL technique can be
applied to this induced action space in the place
of word-level output. We propose a flexible latent
variable dialog framework and investigate several
approaches to inducing latent action space from
natural conversational data. We further propose
(1) a novel training objective that outperforms the
typical evidence lower bound used in dialog gen-
eration and (2) an attention mechanism for inte-
grating discrete latent variables in the decoder to
better model long responses.

We test this on two datasets, DealOrN-
oDeal (Lewis et al., 2017) and Multi-
Woz (Budzianowski et al., 2018), to answer
two key questions: (1) what are the advantages of
LaRL over Word-level RL and (2) what effective
methods can induce this latent action space.
Results show that LaRL is significantly more
effective than word-level RL for learning dialog
policies and it does not lead to incomprehensible
language generation. Our models achieve 18.2%
absolute improvement over the previous state-
of-the-art on MultiWoz and discover novel and
diverse negotiation strategies on DealOrNoDeal.
Besides strong empirical improvement, our model
analysis reveals novel insights, e.g. it is crucial to
reduce the exposure bias in the latent action space
and discrete latent actions are more suitable than
continuous ones to serve as action spaces for RL
dialog agents.

2 Related Work

Prior RL research in modular dialog manage-
ment has focused on policy optimization over
hand-crafted action spaces in task-oriented do-
mains (Walker, 2000; Young et al., 2007). A di-
alog manager is formulated as a Partially Observ-
able Markov Decision Process (POMDP) (Young
et al., 2013), where the dialog state is estimated
via dialog state tracking models from the raw di-
alog context (Lee, 2013; Henderson et al., 2014;
Ren et al., 2018). RL techniques are then used to
find the optimal dialog policy (Gasic and Young,
2014; Su et al., 2017; Williams et al., 2017). Re-
cent deep-learning modular dialog models have
also explored joint optimization over dialog pol-

icy and state tracking to achieve stronger per-
formance (Wen et al., 2016; Zhao and Eskenazi,
2016; Liu and Lane, 2017).

A related line of work is reinforcement learn-
ing for E2E dialog systems. Due to the flexibility
of encoder-decoder dialog models, prior work has
applied reinforcement learning to more complex
domains and achieved higher dialog-level rewards,
such as open-domain chatting (Li et al., 2016; Ser-
ban et al., 2017a), negotiation (Lewis et al., 2017),
visual dialogs (Das et al., 2017), grounded dia-
log (Mordatch and Abbeel, 2017) etc. As dis-
cussed in Section 1, these methods consider the
output vocabulary at every decoding step to be the
action space; they suffer from limitations such as
deviation from natural language and sub-optimal
convergence.

Finally, research in latent variable dialog mod-
els is closely related to our work, which strives
to learn meaningful latent variables for E2E di-
alog systems. Prior work has shown that learn-
ing with latent variables leads to benefits like di-
verse response decoding (Serban et al., 2017b;
Zhao et al., 2017; Cao and Clark, 2017), inter-
pretable decision-making (Wen et al., 2017; Zhao
et al., 2018) and zero-shot domain transfer (Zhao
and Eskenazi, 2018). Also, driven by similar mo-
tivations of this work, prior studies have explored
to utilize a coarse discrete node, either handcrafted
or learned, to decouple the word generation pro-
cess from dialog policy in E2E systems for better
dialog policy (He et al., 2018; Yarats and Lewis,
2017). Our work differs from prior work for
two reasons: (1) latent action in previous work
is only auxiliary, small-scale and mostly learned
in a supervised or semi-supervised setting. This
paper focuses on unsupervised learning of latent
variables and learns variables that are expressive
enough to capture the entire action space by itself.
(2) to our best knowledge, our work is the first
comprehensive study of the use of latent variables
for RL policy optimization in dialog systems.

3 Baseline Approach

E2E response generation can be treated as a con-
ditional language generation task, which uses neu-
ral encoder-decoders (Cho et al., 2014) to model
the conditional distribution p(x|c) where c is the
observed dialog context and x is the system’s re-
sponse to the context. The format of the dialog
context is domain dependent. It can vary from tex-



1210

Figure 1: High-level comparison between word-level and latent-action reinforcement learning in a sample multi-
turn dialog. The decoder network generates the response given the latent code z. Dashed line denotes places where
policy gradients from task rewards are applied to the model.

tual raw dialog history (Vinyals and Le, 2015) to
visual and textual context (Das et al., 2017). Train-
ing with RL usually has 2 steps: supervised pre-
training and policy gradient reinforcement learn-
ing (Williams and Zweig, 2016; Dhingra et al.,
2017; Li et al., 2016). Specifically, the supervised
learning step maximizes the log likelihood on the
training dialogs, where θ is the model parameter:

LSL(θ) = Ex,c[log pθ(x|c)] (1)

Then the following RL step uses policy gradients,
e.g. the REINFORCE algorithm (Williams, 1992)
to update the model parameters with respect to
task-dependent goals. We assume that we have
an environment that the dialog agent can interact
with and that there is a turn-level reward rt at ev-
ery turn t of the dialog. We can then write the ex-
pected discounted return under a dialog model θ as
J(θ) = E[

∑T
0 γ

trt], where γ ∈ [0, 1] is the dis-
counting factor and T is the length of the dialog.
Often a baseline function b is used to reduce the
variance of the policy gradient (Greensmith et al.,
2004), leading to Rt =

∑T−t
k=0 γ

k(rt+k − b).
Word-level Reinforcement Learning: as

shown in Figure 1, the baseline approach treats ev-
ery output word as an action step and its policy
gradient is:

∇θJ(θ) = Eθ[
T∑
t=0

Ut∑
j=0

Rtj∇θ log pθ(wtj |w<tj , ct)]

(2)
where Ut is the number of tokens in the response
at turn t and j is the word index in the response. It
is evident that Eq 2 has a very large action space,
i.e. |V | and a long learning horizon, i.e. TU . Prior
work has found that the direct application of Eq 2
leads to divergence of the decoder. The common
solution is to alternate with supervised learning
with Eq 2 at a certain ratio (Lewis et al., 2017).
We denote this ratio as RL:SL=A:B, which means

for every A policy gradient updates, we run B su-
pervised learning updates. We use RL:SL=off for
the case where only policy gradients are used and
no supervised learning is involved.

4 Latent Action Reinforcement Learning

We now describe the proposed LaRL framework.
As shown in Figure 1, a latent variable z is in-
troduced in the response generation process. The
conditional distribution is factorized into p(x|c) =
p(x|z)p(z|c) and the generative story is: (1) given
a dialog context c we first sample a latent action
z from pθe(z|c) and (2) generate the response by
sampling x based on z via pθd(x|z), where pθe is
the dialog encoder network and pθd is the response
decoder network. Given the above setup, LaRL
treats the latent variable z as its action space in-
stead of outputting words in response x. We can
now apply REINFORCE in the latent action space:

∇θJ(θ) = Eθ[
T∑
t=0

Rt log pθ(z|ct)] (3)

Compared to Eq 2, LaRL differs by:

• Shortens the horizon from TU to T .

• Latent action space is designed to be low-
dimensional, much smaller than V .

• The policy gradient only updates the encoder
θe and the decoder θd stays intact.

These properties reduce the difficulties for dia-
log policy optimization and decouple high-level
decision-making from natural language genera-
tion. The pθe are responsible for choosing the best
latent action given a context c while pθd is only re-
sponsible for transforming z into the surface-form
words. Our formulation also provides a flexible
framework for experimenting with various types
of model learning methods. In this paper, we fo-
cus on two key aspects: the type of latent variable



1211

z and optimization methods for learning z in the
supervised pre-training step.

4.1 Types of Latent Actions
Two types of latent variables have been used in
previous research: continuous isotropic Gaussian
distribution (Serban et al., 2017b) and multivariate
categorical distribution (Zhao et al., 2018). These
two types are both compatible with our LaRL
framework and can be defined as follows:

Gaussian Latent Actions follow M dimen-
sional multivariate Gaussian distribution with a di-
agonal covariance matrix, i.e. z ∼ N (µ,σ2I).
Let the encoder pθe consist of two parts: a context
encoder F , a neural network that encodes the dia-
log context c into a vector representation h, and a
feed forward network π that projects h into µ and
σ. The process is defined as follows:

h = F(c) (4)[
µ

log(σ2)

]
= π(h) (5)

p(x|z) = pθd(z) z ∼ N (µ,σ
2I) (6)

where the sampled z is used as the initial state
of the decoder for response generation. Also we
use pθ(z|c) = N (z;µ,σ2I) to compute the pol-
icy gradient update in Eq 3.

Categorical Latent Actions are M indepen-
dent K-way categorical random variables. Each
zm has its own token embeddings to map latent
symbols into vector space Em ∈ RK×D where
m ∈ [1,M ] and D is the embedding size. Thus
M latent actions can represent exponentially,KM ,
unique combinations, making it expressive enough
to model dialog acts in complex domains. Similar
to Gaussian Latent Actions, we have

h = F(c) (7)
p(Zm|c) = softmax(πm(h)) (8)
p(x|z) = pθd(E1:M (z1:M )) zm ∼ p(Zm|c)

(9)

For the computing policy gradient in Eq 3, we
have pθ(z|c) =

∏M
m=1 p(Zm = zm|c)

Unlike Gaussian latent actions, a matrix RM×D
comes after the embedding layers E1:M (z1:M ),
whereas the decoder’s initial state is a vector of
size RD. Previous work integrated this matrix
with the decoder by summing over the latent em-
beddings, i.e. x = pθd(

∑M
1 Em(zm)), denoted

as Summation Fusion for later discussion (Zhao

et al., 2018). A limitation of this method is that it
could lose fine-grained order information in each
latent dimension and have issues with long re-
sponses that involve multiple dialog acts. There-
fore, we propose a novel method, Attention Fu-
sion, to combine categorical latent actions with the
decoder. We apply the attention mechanism (Lu-
ong et al., 2015) over latent actions as the follow-
ing. Let i be the step index during decoding. Then
we have:

αmi = softmax(hTi WaEm(zm)) (10)

ci =
M∑
m=1

αmiEm(zm) (11)

h̃i = tanh(Ws

[
hi
ci

]
) (12)

p(wi|hi, ci) = softmax(Woh̃i) (13)
The decoder’s next state is updated by hi+1 =
RNN(hi, wi+1), h̃i) and h0 is computed via
summation-fusion. Thus attention fusion lets the
decoder focus on different latent dimensions at
each generation step.

4.2 Optimization Approaches
Full ELBO: Now given a training dataset {x, c},
our base optimization method is via stochastic
variational inference by maximizing the evidence
lowerbound (ELBO), a lowerbound on the data log
likelihood:

Lfull(θ) = pq(z|x,c)(x|z)−DKL[q(z|x, c)‖p(z|c)]
(14)

where qγ(z|x, c) is a neural network that is trained
to approximate the posterior distribution q(z|x, c)
and p(z|c) and p(x|z) are achieved by F , π
and pθd . For Gaussian latent actions, we use
the reparametrization trick (Kingma and Welling,
2013) to backpropagate through Gaussian latent
actions and the Gumbel-Softmax (Jang et al.,
2016) to backpropagate through categorical latent
actions.

Lite ELBO: a major limitation is that Full
ELBO can suffer from exposure bias at latent
space, i.e. the decoder only sees z sampled from
q(z|x, c) and never experiences z sampled from
pθ(z|c), which is always used at testing time.
Therefore, in this paper, we propose a simplified
ELBO for encoder-decoder models with stochas-
tic latent variables:

Llite(θ) = pp(z|c)(x|z)− βDKL[p(z|c))‖p(z)]
(15)



1212

Essentially this simplified objective sets the pos-
terior network the same as our encoder, i.e.
qγ(z|x, c) = pθe(z|c), which makes the KL term
in Eq 14 zero and removes the issue of expo-
sure bias. But this leaves the latent spaces un-
regularized and our experiments show that if we
only maximize pp(z|c)(x|z) there is overfitting.
For this, we add the additional regularization term
βDKL[p(z|c))‖p(z)] that encourages the posterior
be similar to certain prior distributions and β is
a hyper-parameter between 0 and 1. We set the
p(z) for categorical latent actions to be uniform,
i.e. p(z) = 1/K, and set the prior for Gaussian
latent actions to be N (0, I), which we will show
that are effective.

5 Experiment Settings

5.1 DealOrNoDeal Corpus and RL Setup

DealOrNoDeal is a negotiation dataset that con-
tains 5805 dialogs based on 2236 unique scenar-
ios (Lewis et al., 2017). We hold out 252 sce-
narios for testing environment and randomly sam-
ple 400 scenarios from the training set for valida-
tion. The results are evaluated from 4 perspec-
tives: Perplexity (PPL), Reward, Agree and Di-
versity. PPL helps us to identify which model pro-
duces the most human-like responses, while Re-
ward and Agree evaluate the model’s negotiation
strength. Diversity indicates whether the model
discovers a novel discourse-level strategy or just
repeats dull responses to compromise with the op-
ponent. We closely follow the original paper and
use the same reward function and baseline calcu-
lation. At last, to have a fair comparison, all the
compared models shared the identical judge model
and user simulator, which are a standard hierarchi-
cal encoder-decoder model trained with Maximum
Likelihood Estimation (MLE).

5.2 Multi-Woz Corpus and Novel RL Setup

Multi-Woz is a slot-filling dataset that contains
10438 dialogs on 6 different domains. 8438 di-
alogs are for training and 1000 each are for val-
idation and testing. Since no prior user simu-
lator exists for this dataset, for a fair compari-
son with the previous state-of-the-art we focus on
the Dialog-Context-to-Text Generation task pro-
posed in (Budzianowski et al., 2018). This task
assumes that the model has access to the ground-
truth dialog belief state and is asked to generate
the next response at every system turn in a di-

alog. The results are evaluated from 3 perspec-
tives: BLEU, Inform Rate and Success Rate. The
BLEU score checks the response-level lexical sim-
ilarity, while Inform and Success Rate measure
whether the model gives recommendations and
provides all the requested information at dialog-
level. Current state-of-the-art results struggle in
this task and MLE models only achieve 60% suc-
cess (Budzianowski et al., 2018). To transform
this task into an RL task, we propose a novel ex-
tension to the original task as follows:

1. For each RL episode, randomly sample a di-
alog from the training set

2. Run the model on every system turn, and do
not alter the original dialog context at every
turn given the generated responses.

3. Compute Success Rate based on the gener-
ated responses in this dialog.

4. Compute policy gradient using Eq 3 and up-
date the parameters.

This setup creates a variant RL problem that is
similar to the Contextual Bandits (Langford and
Zhang, 2008), where the goal is to adjust its pa-
rameters to generate responses that yield better
Success Rate. Our results show that this problem
is challenging and that word-level RL falls short.

5.3 Language Constrained Reward (LCR)
curve for Evaluation

It is challenging to quantify the performance of
RL-based neural generation systems because it is
possible for a model to achieve high task reward
and yet not generate human language (Das et al.,
2017). Therefore, we propose a novel measure, the
Language Constrained Reward (LCR) curve as an
additional robust measure. The basic idea is to use
an ROC-style curve to visualize the tradeoff be-
tween achieving higher reward and being faithful
to human language. Specifically, at each check-
point i over the course of RL training, we record
two measures: (1) the PPL of a given model on
the test data pi = PPL(θi) and (2) this model’s av-
erage cumulative task reward in the test environ-
ment Rti. After RL training is complete, we create
a 2D plot where the x-axis is the maximum PPL al-
lowed, and the y-axis is the best achievable reward
within the PPL budget in the testing environments:

y = maxiRti subject to pi < x (16)



1213

As a result, a perfect model should lie in the
upper left corner whereas a model that sacrifices
language quality for higher reward will lie in the
lower right corner. Our results will show that the
LCR curve is an informative and robust measure
for model comparison.

6 Results: Latent Actions or Words?

We have created 6 different variations of latent ac-
tion dialog models under our LaRL framework.
To demonstrate the advantages of LaRL, during

Model Var Type Loss Integration
Gauss Gaussian Lfull /
Cat Categorical Lfull sum
AttnCat Categorical Lfull attn
LiteGauss Gaussian Llite /
LiteCat Categorical Llite sum
LiteAttnCat Categorical Llite attn

Table 1: All proposed variations of LaRL models.

the RL training step, we set RL:SL=off for all la-
tent action models, while the baseline word-level
RL models are free to tune RL:SL for best per-
formance. For latent variable models, their per-
plexity is estimated via Monte Carlo p(x|c) ≈
Ep(z|c)[p(x|z)p(z|c)]. For the sake of clarity, this
section only compares the best performing latent
action models to the best performing word-level
models and focuses on the differences between
them. A detailed comparison of the 6 latent space
configurations is addressed in Section 7.

6.1 DealOrNoDeal

The baseline system is a hierarchical recur-
rent encoder-decoder (HRED) model (Serban
et al., 2016) that is tuned to reproduce results
from (Lewis et al., 2017). Word-level RL is
then used to fine-tune the pre-trained model with
RL:SL=4:1. On the other hand, the best perform-
ing latent action model is LiteCat. Best models are
chosen based on performance on the validation en-
vironment.

The results are summarized in Table 2 and Fig-
ure 2 shows the LCR curves for the baseline with
the two best models plus LiteAttnCat and baseline
without RL:SL. From Table 2, it appears that the
word-level RL baseline performs better than Lite-
Cat in terms of rewards. However, Figure 2 shows
that the two LaRL models achieve strong task re-
wards with a much smaller performance drop in
language quality (PPL), whereas the word-level

PPL Reward Agree% Diversity
Baseline 5.23 3.75 59 109
LiteCat 5.35 2.65 41 58
Baseline
+RL

8.23 7.61 86 5

LiteCat
+RL

6.14 7.27 87 202

Table 2: Results on DealOrNoDeal. Diversity is mea-
sured by the number of unique responses the model
used in all scenarios from the test data.

model can only increase its task rewards by de-
viating significantly from natural language.

Figure 2: LCR curves on DealOrNoDeal dataset.

Closer analysis shows the word-level baseline
severely overfits to the user simulator. The caveat
is that the word-level models have in fact discov-
ered a loophole in the simulator by insisting on
’hat’ and ’ball’ several times and the user model
eventually yields to agree to the deal. This is re-
flected in the diversity measure, which is the num-
ber of unique responses that a model uses in all
200 testing scenarios. As shown in Figure 3, af-
ter RL training, the diversity of the baseline model
drops to only 5. It is surprising that the agent can
achieve high reward with a well-trained HRED
user simulator using only 5 unique utterances. On
the contrary, LiteCat increases its response diver-
sity after RL training from 58 to 202, suggesting
that LiteCat discovers novel discourse-level strate-
gies in order to win the negotiation instead of ex-
ploiting local loopholes in the same user simulator.
Our qualitative analysis confirms this when we ob-
serve that our LiteCat model is able to use multi-
ple strategies in negotiation, e.g. elicit preference
question, request different offers, insist on key ob-
jects etc. See supplementary material for example
conversations.



1214

Figure 3: Response diversity and task reward learn-
ing curve over the course of RL training for both word
RL:SL=4:1 (left) and LiteCat (right).

6.2 MultiWoz
For MultiWoz, we reproduce results
from (Budzianowski et al., 2018) as the baseline.
After RL training, the best LaRL model is LiteAt-
tnCat and the best word-level model is word
RL:SL=off. Table 3 shows that LiteAttnCat is on

PPL BLEU Inform Success
Human / / 90% 82.3%
Baseline 3.98 18.9 71.33% 60.96%
LiteAttnCat 4.05 19.1 67.98% 57.36%
Baseline
+RL

17.11 1.4 80.5% 79.07%

LiteAttnCat
+RL

5.22 12.8 82.78% 79.2%

Table 3: Main results on MultiWoz test set. RL models
are chosen based on performance on the validation set.

par with the baseline in the supervised learning
step, showing that multivariate categorical latent
variables alone are powerful enough to match with
continuous hidden representations for modeling
dialog actions. For performance after RL training,
LiteAttnCat achieves near-human performance
in terms of success rate and inform rate, ob-
taining 18.24% absolute improvement over the
MLE-based state-of-the-art (Budzianowski et al.,
2018). More importantly, perplexity only slightly
increases from 4.05 to 5.22. On the other hand,
the word-level RL’s success rate also improves to
79%, but the generated responses completely de-
viate from natural language, increasing perplexity
from 3.98 to 17.11 and dropping BLEU from 18.9

to 1.4.

Figure 4: LCR curves on the MultiWoz dataset.

Figure 4 shows the LCR curves for MultiWoz,
with a trend similar to the previous section: the
word-level models can only achieve task reward
improvement by sacrificing their response decoder
PPL. Figure 4 also shows the LCR curve for the
baseline trained with RL:SL=100:1, hoping that
supervised learning can force the model to con-
form to natural language. While PPL and BLEU
are indeed improved, it also limits final reward
performance. The latent-level models, on the con-
trary, do not suffer from this tradeoff. We also
observe that LiteAttnCat consistently outperforms
LiteCat on MultiWoz, confirming the effective-
ness of Attention Fusion for handling long dialog
responses with multiple entities and dialog acts.
Lastly, Table 4 qualitatively exhibits the genera-
tion differences between the two approaches. The
RL:SL=off model learns to continuously output
entities to fool the evaluation script for high suc-
cess rate, whereas LiteCatAttn learns to give more
information while maintaining the language qual-
ity.

Context Sys I have [value count] trains matching
your request . Is there a specific day and
time you would like to travel? Usr I would
like to leave on [value day] and arrive by
[value time].

Model Generated Response
word
RL:SL=off

[train id] is leaving [value place] on
[value day] on [value day] on [train id]
[train id] [value count] [train id] leaving ...

word
RL:SL=100

[train id] leaves at [value time] . would you
like me to book you a ticket ?

LiteAttnCat [train id] leaves [value place] at
[value time] and arrives in [value place] at
[value time]. Would you like me to book
that for you ?

Table 4: Example responses from baselines and Lite-
CatAttn on MultiWoz.



1215

7 Model Analysis

We compare the 6 variants of latent action mod-
els on DealOrNoDeal and MultiWoz. Table 5

Deal PPL Reward Agree% Diversity
Baseline 3.23 3.75 59 109
Gauss 110K 2.71 43 176
LiteGauss 5.35 4.48 65 91
Cat 80.41 3.9 62 115
AttnCat 118.3 3.23 51 145
LiteCat 5.35 2.67 41 58
LiteAttnCat 5.25 3.69 52 75
MultiWoz PPL BLEU Inform% Succ%
Baseline 3.98 18.9 71.33 60.96
Gauss 712.3 7.54 60.5 23.0
LiteGauss 4.06 19.3 56.46 48.06
Cat 7.07 13.7 54.15 42.04
AttnCat 12.01 12.6 63.9 45.8
LiteCat 4.10 19.1 61.56 49.15
LiteAttnCat 4.05 19.1 67.97 57.36

Table 5: Comparison of 6 model variants with only su-
pervised learning training.

shows performance of the models that are pre-
trained only with supervised learning. Figure 5
shows LCR curves for the 3 models pre-trained
with Llite and fine-tuned with policy gradient re-
inforcement learning. The following are the main

Figure 5: LCR curves on DealOrNoDeal and Multi-
Woz. Models with Lfull are not included because their
PPLs are too poor to compare to the Lite models.

findings based on these results.
Llite outperforms Lfull as a pre-train ob-

jective. Table 5 shows that models with Lfull
fall behind their Lite counterparts on PPL and

BLEU. We attribute this to the exposure bias in
the latent space, i.e. the decoder is not trained
to consider the discrepancy between the poste-
rior network and actual dialog policy network.
Meanwhile, the full models tend to enjoy higher
diversity at pre-training, which agrees with the
diversity-promoting effect observed in prior re-
search (Zhao et al., 2017). However, our previous
discussion on Figure 3 shows that Lite models are
able to increase their response diversity in order to
win more in negotiation through RL training. This
is fundamentally different from diversity in pre-
training, since diversity in LaRL is optimized to
improve task reward, rather than to better model
the original data distribution. Table 6 shows the

β 0.0 0.01 β 0.0 0.01
LiteCat 4.23 7.27 LiteGauss 4.83 6.67

Table 6: Best rewards in test environments on
DealOrNoDeal with various β.

importance of latent space regularization. When β
is 0, both LiteCat and LiteGauss reach suboptimal
policies with final reward that are much smaller
than the regularized versions (β = 0.01). The
reason behind this is that the unregularized pre-
trained policy has very low entropy, which pro-
hibits sufficient exploration in the RL stage.

Categorical latent actions outperform Gaus-
sian latent actions. Models with discrete ac-
tions consistently outperform models with Gaus-
sian ones. This is surprising since continuously
distributed representations are a key reason for the
success of deep learning in natural language pro-
cessing. Our finding suggests that (1) multivari-
ate categorical distributions are powerful enough
to model complex natural dialog responses seman-
tics, and can achieve on par results with Gaussian
or non-stochastic continuous representations. (2)
categorical variables are a better choice to serve as
action spaces for reinforcement learning. Figure 5
shows that Lite(Attn)Cat easily achieves strong re-
wards while LiteGauss struggles to improve its re-
ward. Also, applying REINFORCE on Gaussian
latent actions is unstable and often leads to model
divergence. We suspect the reason for this is the
unbounded nature of continuous latent space: RL
exploration in the continuous space may lead to
areas in the manifold that are not covered in su-
pervised training, which causes undefined decoder
behavior given z in these unknown areas.



1216

8 Conclusion and Future Work

In conclusion, this paper proposes a latent vari-
able action space for RL in E2E dialog agents.
We present a general framework with a regularized
ELBO objective and attention fusion for discrete
variables. The methods are assessed on two dialog
tasks and analyzed using the proposed LCR curve.
Results show our models achieve superior perfor-
mance and create a new state-of-the-art success
rate on MultiWoz. Extensive analyses enable us
to gain insight on how to properly train latent vari-
ables that can serve as the action spaces for dialog
agents. This work is situated in the approach con-
cerning practical latent variables in dialog agents,
being able to create action abstraction in an unsu-
pervised manner. We believe that our findings are
a basic first step in this promising research direc-
tion.

References
Paweł Budzianowski, Tsung-Hsien Wen, Bo-Hsiang

Tseng, Iñigo Casanueva, Stefan Ultes, Osman Ra-
madan, and Milica Gasic. 2018. Multiwoz-a large-
scale multi-domain wizard-of-oz dataset for task-
oriented dialogue modelling. In Proceedings of the
2018 Conference on Empirical Methods in Natural
Language Processing, pages 5016–5026.

Kris Cao and Stephen Clark. 2017. Latent variable di-
alogue models and their diversity. In Proceedings of
the 15th Conference of the European Chapter of the
Association for Computational Linguistics: Volume
2, Short Papers, volume 2, pages 182–187.

Yun-Nung Chen, William Yang Wang, and Alexander I
Rudnicky. 2013. Unsupervised induction and fill-
ing of semantic slots for spoken dialogue systems
using frame-semantic parsing. In Automatic Speech
Recognition and Understanding (ASRU), 2013 IEEE
Workshop on, pages 120–125. IEEE.

Kyunghyun Cho, Bart Van Merriënboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using rnn encoder-decoder
for statistical machine translation. arXiv preprint
arXiv:1406.1078.

Abhishek Das, Satwik Kottur, José MF Moura, Stefan
Lee, and Dhruv Batra. 2017. Learning cooperative
visual dialog agents with deep reinforcement learn-
ing. In Computer Vision (ICCV), 2017 IEEE Inter-
national Conference on, pages 2970–2979. IEEE.

Bhuwan Dhingra, Lihong Li, Xiujun Li, Jianfeng Gao,
Yun-Nung Chen, Faisal Ahmed, and Li Deng. 2017.
Towards end-to-end reinforcement learning of dia-
logue agents for information access. In Proceed-
ings of the 55th Annual Meeting of the Association

for Computational Linguistics (Volume 1: Long Pa-
pers), volume 1, pages 484–495.

Milica Gasic and Steve Young. 2014. Gaussian pro-
cesses for pomdp-based dialogue manager optimiza-
tion. IEEE/ACM Transactions on Audio, Speech,
and Language Processing, 22(1):28–40.

Evan Greensmith, Peter L Bartlett, and Jonathan Bax-
ter. 2004. Variance reduction techniques for gradi-
ent estimates in reinforcement learning. Journal of
Machine Learning Research, 5(Nov):1471–1530.

He He, Derek Chen, Anusha Balakrishnan, and Percy
Liang. 2018. Decoupling strategy and generation in
negotiation dialogues. In Proceedings of the 2018
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 2333–2343.

Matthew Henderson, Blaise Thomson, and Steve
Young. 2014. Word-based dialog state tracking with
recurrent neural networks. In Proceedings of the
15th Annual Meeting of the Special Interest Group
on Discourse and Dialogue (SIGDIAL), pages 292–
299.

Eric Jang, Shixiang Gu, and Ben Poole. 2016. Categor-
ical reparameterization with gumbel-softmax. arXiv
preprint arXiv:1611.01144.

Leslie Pack Kaelbling, Michael L Littman, and An-
drew W Moore. 1996. Reinforcement learning: A
survey. Journal of artificial intelligence research,
4:237–285.

Diederik P Kingma and Max Welling. 2013. Auto-
encoding variational bayes. arXiv preprint
arXiv:1312.6114.

Satwik Kottur, José Moura, Stefan Lee, and Dhruv Ba-
tra. 2017. Natural language does not emerge nat-
urallyin multi-agent dialog. In Proceedings of the
2017 Conference on Empirical Methods in Natural
Language Processing, pages 2962–2967.

John Langford and Tong Zhang. 2008. The epoch-
greedy algorithm for multi-armed bandits with side
information. In Advances in neural information pro-
cessing systems, pages 817–824.

Sungjin Lee. 2013. Structured discriminative model
for dialog state tracking. In Proceedings of the SIG-
DIAL 2013 Conference, pages 442–451.

Mike Lewis, Denis Yarats, Yann Dauphin, Devi Parikh,
and Dhruv Batra. 2017. Deal or no deal? end-to-end
learning of negotiation dialogues. In Proceedings of
the 2017 Conference on Empirical Methods in Nat-
ural Language Processing, pages 2443–2453.

Jiwei Li, Will Monroe, Alan Ritter, Dan Jurafsky,
Michel Galley, and Jianfeng Gao. 2016. Deep rein-
forcement learning for dialogue generation. In Pro-
ceedings of the 2016 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1192–
1202.



1217

Bing Liu and Ian Lane. 2017. An end-to-end train-
able neural network model with belief tracking for
task-oriented dialog. Proc. Interspeech 2017, pages
2506–2510.

Thang Luong, Hieu Pham, and Christopher D Man-
ning. 2015. Effective approaches to attention-based
neural machine translation. In Proceedings of the
2015 Conference on Empirical Methods in Natural
Language Processing, pages 1412–1421.

Igor Mordatch and Pieter Abbeel. 2017. Emergence
of grounded compositional language in multi-agent
populations. arXiv preprint arXiv:1703.04908.

Antoine Raux, Brian Langner, Dan Bohus, Alan W
Black, and Maxine Eskenazi. 2005. Lets go pub-
lic! taking a spoken dialog system to the real world.
In in Proc. of Interspeech 2005. Citeseer.

Liliang Ren, Kaige Xie, Lu Chen, and Kai Yu. 2018.
Towards universal dialogue state tracking. In Pro-
ceedings of the 2018 Conference on Empirical Meth-
ods in Natural Language Processing, pages 2780–
2786.

Iulian V Serban, Chinnadhurai Sankar, Mathieu Ger-
main, Saizheng Zhang, Zhouhan Lin, Sandeep Sub-
ramanian, Taesup Kim, Michael Pieper, Sarath
Chandar, Nan Rosemary Ke, et al. 2017a. A
deep reinforcement learning chatbot. arXiv preprint
arXiv:1709.02349.

Iulian V Serban, Alessandro Sordoni, Yoshua Bengio,
Aaron Courville, and Joelle Pineau. 2016. Building
end-to-end dialogue systems using generative hier-
archical neural network models. In Proceedings of
the 30th AAAI Conference on Artificial Intelligence
(AAAI-16).

Iulian Vlad Serban, Alessandro Sordoni, Ryan Lowe,
Laurent Charlin, Joelle Pineau, Aaron Courville,
and Yoshua Bengio. 2017b. A hierarchical latent
variable encoder-decoder model for generating di-
alogues. In Thirty-First AAAI Conference on Artifi-
cial Intelligence.

Alessandro Sordoni, Michel Galley, Michael Auli,
Chris Brockett, Yangfeng Ji, Margaret Mitchell,
Jian-Yun Nie, Jianfeng Gao, and Bill Dolan. 2015.
A neural network approach to context-sensitive gen-
eration of conversational responses. In Proceed-
ings of the 2015 Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
196–205.

Pei-Hao Su, Paweł Budzianowski, Stefan Ultes, Mil-
ica Gasic, and Steve Young. 2017. Sample-efficient
actor-critic reinforcement learning with supervised
data for dialogue management. In Proceedings of
the 18th Annual SIGdial Meeting on Discourse and
Dialogue, pages 147–157.

Oriol Vinyals and Quoc Le. 2015. A neural conversa-
tional model. arXiv preprint arXiv:1506.05869.

Marilyn A. Walker. 2000. An application of reinforce-
ment learning to dialogue strategy selection in a spo-
ken dialogue system for email. Journal of Artificial
Intelligence Research, pages 387–416.

Tsung-Hsien Wen, Milica Gasic, Nikola Mrksic,
Lina M Rojas-Barahona, Pei-Hao Su, Stefan Ultes,
David Vandyke, and Steve Young. 2016. A network-
based end-to-end trainable task-oriented dialogue
system. arXiv preprint arXiv:1604.04562.

Tsung-Hsien Wen, Yishu Miao, Phil Blunsom, and
Steve Young. 2017. Latent intention dialogue mod-
els. In International Conference on Machine Learn-
ing, pages 3732–3741.

Jason D Williams, Kavosh Asadi, and Geoffrey Zweig.
2017. Hybrid code networks: practical and efficient
end-to-end dialog control with supervised and rein-
forcement learning. In Proceedings of the 55th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), volume 1,
pages 665–677.

Jason D Williams and Steve Young. 2007. Partially
observable markov decision processes for spoken
dialog systems. Computer Speech & Language,
21(2):393–422.

Jason D Williams and Geoffrey Zweig. 2016. End-
to-end lstm-based dialog control optimized with su-
pervised and reinforcement learning. arXiv preprint
arXiv:1606.01269.

Ronald J Williams. 1992. Simple statistical gradient-
following algorithms for connectionist reinforce-
ment learning. Machine learning, 8(3-4):229–256.

Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He,
Alex Smola, and Eduard Hovy. 2016. Hierarchi-
cal attention networks for document classification.
In Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 1480–1489.

Denis Yarats and Mike Lewis. 2017. Hierarchical
text generation and planning for strategic dialogue.
arXiv preprint arXiv:1712.05846.

Stephanie Young, Jost Schatzmann, Karl Weilhammer,
and Hui Ye. 2007. The hidden information state ap-
proach to dialog management. In Acoustics, Speech
and Signal Processing, 2007. ICASSP 2007. IEEE
International Conference on, volume 4, pages IV–
149. IEEE.

Steve Young, Milica Gašić, Blaise Thomson, and Ja-
son D Williams. 2013. Pomdp-based statistical spo-
ken dialog systems: A review. Proceedings of the
IEEE, 101(5):1160–1179.

Steve J Young. 2006. Using pomdps for dialog man-
agement. In SLT, pages 8–13.



1218

Tiancheng Zhao and Maxine Eskenazi. 2016. Towards
end-to-end learning for dialog state tracking and
management using deep reinforcement learning. In
17th Annual Meeting of the Special Interest Group
on Discourse and Dialogue, page 1.

Tiancheng Zhao and Maxine Eskenazi. 2018. Zero-
shot dialog generation with cross-domain latent ac-
tions. In Proceedings of the 19th Annual SIGdial
Meeting on Discourse and Dialogue, pages 1–10.

Tiancheng Zhao, Kyusong Lee, and Maxine Eskenazi.
2018. Unsupervised discrete sentence representa-
tion learning for interpretable neural dialog gener-
ation. In Proceedings of the 56th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), volume 1.

Tiancheng Zhao, Ran Zhao, and Maxine Eskenazi.
2017. Learning discourse-level diversity for neural
dialog models using conditional variational autoen-
coders. In Proceedings of the 55th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), volume 1, pages 654–664.

A Supplemental Material

A.1 Training Details

Supervised Pre-train
Word Embedding 256
Utterance Encoder Attn GRU (128)
Context (De)Encoder GRU (256)
Optimizer Adam (lr=1e-3)
Dropout 0.5
β 0.01
Categorical z M=10, K=20
Gaussian z M=200
Reinforce
Optimizer SGD (lr=0.2 grad clip=0.1)
γ 0.95

Table 7: Training details for DealOrNoDeal experi-
ments. Attn GRU refers to (Yang et al., 2016)

Supervised Pre-train
Word Embedding 256
Encoder Attn GRU (300)
Decoder LSTM (150)
Optimizer Adam (1e-3)
Dropout 0.5
β 0.01
Categorical z M=10, K=20
Gaussian z M=200
Reinforce
Optimizer SGD(lr=0.01 grad clip=0.5)
γ 0.99

Table 8: Training details for MultiWoz experiments

A.2 DealOrNoDeal Example Conversations
The followings are examples dialogs generated
from word-level models (Table 9) and latent-level
models (Table 10).

Scenario 1
1 book, 1 hat and 3 balls available.
Value Function
Agent: book=1, hat=6, ball=1.
User: book=1, hat=6, ball=1.
Dialog
Agent: I’d like the hat and 2 balls.
User: Deal.
Agent: SELECTION
Outcome: Agreement
Agent: 1 hat and 2 balls; User: 1 book and 1 ball
Agent reward = 8; User reward = 2
Scenario 2
4 books, 1 hat and 1 ball available.
Value Function
Agent: book=0, hat=7, ball=3;
User: book=1, hat=6, ball=0.
Dialog
User: I can offer you 1 hat and 2 books.
Agent: I need the hat and ball.
User: That won’t work for me. I can offer you one book
and the hat.
Agent: I need the hat and ball.
User: I have to have the hat.
Agent: I need the hat and ball.
User: Okay.
Agent: SELECTION
Outcome: Agreement
Agent: 1 hat and 1 ball; User: 4 books
Agent reward = 10; User reward = 4

Table 9: Example dialogs between baseline with the
user model. Agent is trained with word-level policy
gradient and the user is a supervised pre-trained model.

Scenario 1
2 books, 2 hats and 1 ball available.
Value Function
Agent: book=3, hat=0, ball=4;
User: book=2, hat=3, ball=0.
Dialog
Agent: Can i have the books and one ball?
User: I would like the books and the hats then.
Agent: I would like the ball and books.
User: Deal.
Agent: SELECTION
Outcome: Agreement
Agent: 2 books and 1 ball
User: 2 hats
Agent reward = 10, User reward = 6
Scenario 2
2 books, 1 hat and 3 ball available.
Value Function
Agent: book=0, hat=4, ball=2;
User: book=3, hat=1, ball=1.
Dialog
Agent: I would like the hat and balls.
User: I cannot accept that. I need the hat and books.
Agent: Can I have the hat and balls, you get the rest?
User: Okay, deal.
Agent: SELECTION.
Outcome: Agreement
Agent: 1 hat and 3 ball
User: 2 books
Agent reward = 10, Simulator reward = 6

Table 10: Example dialogs between LiteCat and the
user model. Agent is trained with latent-level policy
gradient and the user is a supervised pre-trained model.


