



















































Cross-lingual Learning of Semantic Textual Similarity with Multilingual Word Representations


Proceedings of the 21st Nordic Conference of Computational Linguistics, pages 211–215,
Gothenburg, Sweden, 23-24 May 2017. c©2017 Linköping University Electronic Press

Cross-lingual Learning of Semantic Textual Similarity
with Multilingual Word Representations

Johannes Bjerva
Center for Language and Cognition Groningen

University of Groningen
The Netherlands

j.bjerva@rug.nl

Robert Östling
Department of Linguistics

Stockholm University
Sweden

robert@ling.su.se

Abstract

Assessing the semantic similarity between
sentences in different languages is chal-
lenging. We approach this problem
by leveraging multilingual distributional
word representations, where similar words
in different languages are close to each
other. The availability of parallel data al-
lows us to train such representations on
a large amount of languages. This al-
lows us to leverage semantic similarity
data for languages for which no such data
exists. We train and evaluate on five lan-
guage pairs, including English, Spanish,
and Arabic. We are able to train well-
performing systems for several language
pairs, without any labelled data for that
language pair.

1 Introduction

Semantic Textual Similarity (STS) is the task of
assessing the degree to which two sentences are
semantically similar. Within the SemEval STS
shared tasks, this is measured on a scale ranging
from 0 (no semantic similarity) to 5 (complete se-
mantic similarity) (Agirre et al., 2016). Mono-
lingual STS is an important task, for instance for
evaluation of machine translation (MT) systems,
where estimating the semantic similarity between
a system’s translation and the gold translation can
aid both system evaluation and development. The
task is already a challenging one in a monolin-
gual setting, e.g., when estimating the similarity
between two English sentences. In this paper, we
tackle the more difficult case of cross-lingual STS,
e.g., estimating the similarity between an English
and an Arabic sentence.

Previous approaches to this problem have fo-
cussed on two main approaches. On the one hand,
MT approaches have been attempted (e.g. Lo et

al. (2016)), which allow for monolingual similar-
ity assessment, but suffer from the fact that involv-
ing a fully-fledged MT system severely increases
system complexity. Applying bilingual word rep-
resentations, on the other hand, bypasses this issue
without inducing such complexity (e.g. Aldarmaki
and Diab (2016)). However, bilingual approaches
do not allow for taking advantage of the increas-
ing amount of STS data available for more than
one language pair.

Currently, there are several methods available
for obtaining high quality multilingual word rep-
resentations. It is therefore interesting to investi-
gate whether language can be ignored entirely in
an STS system after mapping words to their re-
spective representations. We investigate the utility
of multilingual word representations in a cross-
lingual STS setting. We approach this by com-
bining multilingual word representations with a
deep neural network, in which all parameters are
shared, regardless of language combinations.

The contributions of this paper can be summed
as follows: i) we show that multilingual input
representations can be used to train an STS sys-
tem without access to training data for a given
language; ii) we show that access to data from
other languages improves system performance for
a given language.

2 Semantic Textual Similarity

Given two sentences, s1 and s2, the task in STS
is to assess how semantically similar these are to
each other. This is commonly measured using a
scale ranging from 0–5, with 0 indicating no se-
mantic overlap, and 5 indicating nearly identical
content. In the SemEval STS shared tasks, the fol-
lowing descriptions are used:

0. The two sentences are completely dissimilar.

1. The two sentences are not equivalent, but are on the
same topic.

211



2. The two sentences are not equivalent, but share some
details.

3. The two sentences are roughly equivalent, but some im-
portant information differs/missing.

4. The two sentences are mostly equivalent, but some
unimportant details differ.

5. The two sentences are completely equivalent, as they
mean the same thing.

This manner of assessing semantic content of
two sentences notably does not take important se-
mantic features such as negation into account, and
can therefore be seen as complimentary to textual
entailment. Furthermore, the task is highly related
to paraphrasing, as replacing an n-gram with a
paraphrase thereof ought to alter the semantic sim-
ilarity of two sentences to a very low degree. Suc-
cessful monolingual approaches in the past have
taken advantage of both of these facts (see, e.g.,
Beltagy et al. (2016)). Approaches similar to these
can be applied in cross-lingual STS, if the sen-
tence pair is translated to a language for which
such resources exist. However, involving a fully-
fledged MT system increases pipeline complexity,
which increases the risk of errors in cases of, e.g.,
mistranslations. Using bilingual word represen-
tations, in order to create truly cross-lingual sys-
tems, was explored by several systems in SemEval
2016 (Agirre et al., 2016). However, such sys-
tems are one step short of truly taking advantage
of the large amounts of multilingual parallel data,
and STS data, available. This work contributes to
previous work on STS by further exploring this as-
pect, by leveraging multilingual word representa-
tions.

3 Multilingual Word Representations

3.1 Multilingual Skip-gram

The skip-gram model has become one of the most
popular manners of learning word representations
in NLP (Mikolov et al., 2013). This is in part
owed to its speed and simplicity, as well as the per-
formance gains observed when incorporating the
resulting word embeddings into almost any NLP
system. The model takes a word w as its input,
and predicts the surrounding context c. Formally,
the probability distribution of c given w is defined
as

p(c|w;θ) = exp(~c
T~w)

Σc∈V exp(~cT~w)
, (1)

where V is the vocabulary, and θ the parameters of
word emeddings (~w) and context embeddings (~c).

The parameters of this model can then be learned
by maximising the log-likelihood over (w,c) pairs
in the dataset D,

J(θ) = ∑
(w,c)∈D

log p(c|w;θ). (2)

Guo et al. (2016) provide a multilingual exten-
sion for the skip-gram model, by requiring the
model to not only learn to predict English con-
texts, but also multilingual ones. This can be seen
as a simple adaptation of Firth (1957, p.11), i.e.,
you shall judge a word by the multilingual com-
pany it keeps. Hence, the vectors for, e.g., dog
and perro ought to be close to each other in such
a model. This assumes access to multilingual par-
allel data, as word alignments are used in order
to determine which words comprise the multilin-
gual context of a word. Whereas Guo et al. (2016)
only evaluate their approach on the relatively simi-
lar languages English, French and Spanish, we ex-
plore a more typological diverse case, as we ap-
ply this method to English, Spanish and Arabic.
We use the same parameter settings as Guo et al.
(2016).

3.2 Learning embeddings
We train multilingual embeddings on the Europarl
and UN corpora. Word alignment is performed
using the Efmaral word-alignment tool (Östling
and Tiedemann, 2016). This allows us to extract
a large amount of multilingual (w,c) pairs. We
then learn multilingual embeddings by applying
the word2vecf tool (Levy and Goldberg, 2014).

4 Method

4.1 System architecture
We use a relatively simple neural network ar-
chitecture, consisting of an input layer with pre-
trained word embeddings and a siamese network
of fully connected layers with shared weights. In
order to prevent any shift from occurring in the
embeddings, we do not update these during train-
ing. The intuition here, is that we do not want the
representation for, e.g., dog to be updated, which
might push it further away from that of perro.
We expect this to be especially important in cases
where we train on a single language, and evaluate
on another.

Given word representations for each word in our
sentence, we take the simplistic approach of aver-
aging the vectors across each sentence. The result-
ing sentence-level representation is then passed

212



through a single fully connected layer, prior to the
output layer. We apply dropout (p = 0.5) between
each layer (Srivastava et al., 2014). All weights
are initialised using the approach in Glorot and
Bengio (2010). We use the Adam optimisation
algorithm (Kingma and Ba, 2014), jointly moni-
toring the categorical cross entropy of a one-hot
representation of the (rounded) sentence similar-
ity score, as well as Pearson correlation using the
actual scores. All systems are trained using a batch
size of 40 sentence pairs, over a maximum of 50
epochs, using early stopping. Hyperparameters
are kept constant in all conditions.

4.2 Data
We use all available data from all previous editions
of the SemEval shared tasks on (cross-lingual)
STS. An overview of the available data is shown
in Table 1.

Table 1: Available data for (cross-lingual) STS
from the SemEval shared task series.

Language pair N sentences

English / English 3750
English / Spanish 1000
English / Arabic 2162
Spanish / Spanish 1620
Arabic / Arabic 1081

5 Experiments and Results

We aim to investigate whether using a multilin-
gual input representation and shared weights al-
low us to ignore languages in STS. We first train
and evaluate single-source trained systems (i.e. on
a single language pair), and evaluate this both us-
ing the same language pair as target, and on all
other target language pairs.1 Secondly, we inves-
tigate the effect of bundling training data together,
investigating which language pairings are helpful
for each other. We measure performance between
gold similarities and system output using the Pear-
son correlation measure, as this is standard in the
SemEval STS shared tasks.

5.1 Single-source training
Results when training on a single source cor-
pus are shown in Table 2. Training on the tar-
get language pair generally yields the highest

1This setting can be seen as a sort of model transfer.

results, except for one case. When evaluating
on Arabic/Arabic sentence pairs, training on En-
glish/Arabic texts yields comparable, or slightly
better, performance than when training on Ara-
bic/Arabic.

Table 2: Single-source training results (Pearson
correlations). Columns indicate training language
pairs, and rows indicate testing language pairs.
Bold numbers indicate best results per row.
HHHHHHTest

Train en/en en/es en/ar es/es ar/ar

en/en 0.69 0.07 -0.04 0.64 0.54
en/es 0.19 0.27 0.00 0.18 -0.04
en/ar -0.44 0.37 0.73 -0.10 0.62
es/es 0.61 0.07 0.12 0.65 0.50
ar/ar 0.59 0.52 0.73 0.59 0.71

5.2 Multi-source training

We combine training corpora in order to investi-
gate how this affects evaluation performance on
the language pairs in question. In the first con-
dition, we copy the single-source setup, except
for that we also add in the data belonging to the
source-pair at hand, e.g., training on both En-
glish/Arabic and Arabic/Arabic when evaluating
on Arabic/Arabic (see Table 3).

Table 3: Training results with one source in ad-
dition to in-language data (Pearson correlations).
Columns indicate added training language pairs,
and rows indicate testing language pairs. Bold
numbers indicate best results per row.
HHHHHHTest

Train en/en en/es en/ar es/es ar/ar

en/en 0.69 0.68 0.67 0.69 0.71
en/es 0.22 0.27 0.30 0.22 0.24
en/ar 0.72 0.72 0.73 0.71 0.72
es/es 0.63 0.60 0.63 0.65 0.66
ar/ar 0.71 0.72 0.75 0.70 0.71

We observe that the monolingual language pair-
ings (en/en, es/es, ar/ar) appear to be beneficial for
one another. We therefore run an ablation exper-
iment, in which we train on two out of three of
these language pairs, and evaluate on all three. Not
including any Spanish training data yields compa-
rable performance to including it (Table 4).

213



Table 4: Ablation results (Pearson correlations).
Columns indicate ablated language pairs, and rows
indicate testing language pairs. The none column
indicates no ablation, i.e., training on all three
monolingual pairs. Bold indicates results when
not training on the language pair evaluated on.
PPPPPPPPPTest

Ablated en/en es/es ar/ar none

en/en 0.60 0.69 0.69 0.65
es/es 0.64 0.64 0.67 0.60
ar/ar 0.68 0.66 0.58 0.72

5.3 Comparison with Monolingual
Representations

We compare multilingual embeddings with the
performance obtained using the pre-trained mono-
lingual Polyglot embeddings (Al-Rfou et al.,
2013). Training and evaluating on the same lan-
guage pair yields comparable results regardless
of embeddings. However, when using monolin-
gual embeddings, every multilingual language pair
combination yields poor results.

6 Discussion

In all cases, training on the target language pair is
beneficial. We also observe that using multilingual
embeddings is crucial for multilingual approaches,
as monolingual embeddings naturally only yield
on-par results in monolingual settings. This is due
to the fact that using the shared language-agnostic
input representation allows us to take advantage
of linguistic regularities across languages, which
we obtain solely from observing distributions be-
tween languages in parallel text. Using monolin-
gual word representations, however, there is no
similarity between, e.g., dog and perro to rely on
to guide learning.

For the single-source training, we in one case
observe somewhat better performance using other
training sets than the in-language one: training
on English/Arabic outperforms training on Ara-
bic/Arabic, when evaluating on Arabic/Arabic.
We expected this to be due to differing data set
sizes (English/Arabic is about twice as big). Con-
trolling for this does, indeed, bring the perfor-
mance of training on English/Arabic to the same
level as training on Arabic/Arabic. However, com-
bining these datasets increases performance fur-
ther (Table 3).

In single-source training, we also observe that
certain source languages do not offer any gener-
alisation over certain target languages. Interest-
ingly, certain combinations of training/testing lan-
guage pairs yield very poor results. For instance,
training on English/English yields very poor re-
sults when evaluating on English/Arabic, and vice
versa. The same is observed for the combination
Spanish/Spanish and English/Arabic. This may be
explained by domain differences in training and
evaluation data. A general trend appears to be that
either monolingual training pairs and evaluation
pairs, or cross-lingual pairs with overlap (e.g. En-
glish/Arabic and Arabic/Arabic) is beneficial.

The positive results on pairings without any lan-
guage overlap are particularly promising. Train-
ing on English/English yields results not too
far from training on the source language pairs,
for Spanish/Spanish and Arabic/Arabic. Simi-
lar results are observed when training on Span-
ish/Spanish and evaluating on English/English and
Arabic/Arabic, as well as when training on Ara-
bic/Arabic and evaluating on English/English and
Spanish/Spanish. This indicates that we can esti-
mate STS relatively reliably, even without assum-
ing any existing STS data for a given language.

7 Conclusions and Future Work

Multilingual word representations allow us to
leverage more available data for multilingual
learning of semantic textual similarity. We have
shown that relatively high STS performance can
be achieved for languages without assuming exist-
ing STS annotation, and relying solely on paral-
lel texts. An interesting direction for future work
is to investigate how multilingual character-level
representations can be included, perhaps learn-
ing morpheme-level representations and mappings
between these across languages. Leveraging ap-
proaches to learning multilingual word represen-
tations from smaller data sets would also be inter-
esting. For instance, learning such representations
from only the new testament, would allow for STS
estimation for more than 1,000 of the languages in
the world.

Acknowledgments

We would like to thank the Center for Informa-
tion Technology of the University of Groningen
for providing access to the Peregrine high perfor-
mance computing cluster.

214



References
Eneko Agirre, Carmen Banea, Daniel Cer, Mona Diab,

Aitor Gonzalez-Agirre, Rada Mihalcea, German
Rigau, and Janyce Wiebe. 2016. Semeval-2016
task 1: Semantic textual similarity, monolingual
and cross-lingual evaluation. In Proceedings of Se-
mEval, pages 497–511.

Rami Al-Rfou, Bryan Perozzi, and Steven Skiena.
2013. Polyglot: Distributed word representations
for multilingual nlp. CoNLL-2013.

Hanan Aldarmaki and Mona Diab. 2016. GWU NLP
at SemEval-2016 Shared Task 1: Matrix factoriza-
tion for crosslingual STS. In Proceedings of Se-
mEval 2016, pages 663–667.

Islam Beltagy, Stephen Roller, Pengxiang Cheng, Ka-
trin Erk, and Raymond J Mooney. 2016. Repre-
senting meaning with a combination of logical and
distributional models. Computational Linguistics.

John R Firth. 1957. A synopsis of linguistic theory,
1930-1955. Blackwell.

Xavier Glorot and Yoshua Bengio. 2010. Understand-
ing the difficulty of training deep feedforward neural
networks. In Aistats, volume 9, pages 249–256.

Jiang Guo, Wanxiang Che, David Yarowsky, Haifeng
Wang, and Ting Liu. 2016. A representation learn-
ing framework for multi-source transfer parsing. In
Proc. of AAAI.

Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Omer Levy and Yoav Goldberg. 2014. Dependency-
based word embeddings. In ACL, pages 302–308.

Chi-kiu Lo, Cyril Goutte, and Michel Simard. 2016.
Cnrc at semeval-2016 task 1: Experiments in
crosslingual semantic textual similarity. Proceed-
ings of SemEval, pages 668–673.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.

Robert Östling and Jörg Tiedemann. 2016. Efficient
word alignment with markov chain monte carlo.
The Prague Bulletin of Mathematical Linguistics,
106(1):125–146.

Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: a simple way to prevent neural networks
from overfitting. Journal of Machine Learning Re-
search, 15(1):1929–1958.

215


