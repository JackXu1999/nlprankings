



















































LIA: A Natural Language Programmable Personal Assistant


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (System Demonstrations), pages 145–150
Brussels, Belgium, October 31–November 4, 2018. c©2018 Association for Computational Linguistics

145

LIA: A Natural Language Programmable Personal Assistant

Igor Labutov Shashank Srivastava Tom Mitchell
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15217, USA

ilabutov@cs.cmu.edu ssrivastava@cmu.edu tom.mitchell@cmu.edu

Abstract
We present LIA, an intelligent personal as-
sistant that can be programmed using natural
language. Our system demonstrates multiple
competencies towards learning from human-
like interactions. These include (i) the ability
to be taught reusable conditional procedures,
(ii) ability to be taught new knowledge about
the world (concepts in an ontology) and (iii)
the ability to be taught how to ground that
knowledge in a set of sensors and effectors.
Building such a system highlights design ques-
tions regarding the overall architecture that
such an agent should have, as well as questions
about parsing and grounding language in situ-
ational contexts. We outline key properties of
this architecture, and demonstrate a prototype
that embodies them in the form of a personal
assistant on an Android device.

1 Introduction

Today’s conversational assistants such as Alexa
have the capacity to act on a small number
of pre-programmed natural language commands
(e.g., “What is the weather going to be like to-
day?”). However, advances in semantic pars-
ing and broader language technologies present the
possibility of designing conversational interfaces
that enable users to instruct (i.e., program) their
assistants using language, similar to how humans
teach new tasks to one another. For example, if a
user wants Alexa to have a new functionality such
as “whenever there is an important email I haven’t
seen within an hour, read it out to me”, she should
be able to instruct it verbally. This instruction may
include explaining what constitutes an “important
email”. This, in turn, may involve a description
such as “important emails are from colleagues”,
which may require further background knowledge
defining “colleagues”, “friends”, etc. When hu-
mans teach other humans, such knowledge is of-
ten imparted naturally through explanations, e.g.,

Figure 1: Architecture overview: LIA interacts with the
environment through a set of sensors and effectors, which
are mapped to API’s of other Android applications. End-
users interact with the agent through a text (or voice) in-
terface. User utterances are mapped through a Semantic
Parser to logical forms. A Dialog Manager module guides
user interactions by grounding logical forms to actions, or
asking questions based on possible control flow branches

“my colleagues would have a CMU affiliation” or
“Tom is a colleague”. If AI assistants could be
taught in a similar fashion, this could effectively
make every computer user a programmer.

Towards this end, we present a prototype for a
personal assistant, LIA (for Learning from In-
struction Agent), which demonstrates some of
these capabilities. LIA resides on a typical mo-
bile Android device. It can perceive the external
environment through a set of sensors (e.g., sen-
sors for detecting new emails, reading the calen-
dar, reading current time, etc.) and perform ac-
tions to change the environment through its effec-
tors (e.g., send a message, set an alarm, change the
calendar, etc.). The set of sensors and effectors are
mapped to functions calls of API’s for correspond-
ing Android applications (see Figure 1).



146

2 Core competencies

LIA demonstrates three core competencies that we
consider key for learning from instruction:

2.1 Learning Procedures
Among the main use-cases of being able to teach
an agent is being able to define condition-action
rules and procedures, such as the following:

> If there is an important email then forward
to my project team

> Whenever it snows at the night , set my alarm
to 30 minutes earlier

> Update my calendar when there is an important
meeting request

Here, the condition and the effect (action) are
highlighted in green and red respectively. The
condition in each example requires a check that
has to be grounded in the perception sensors. If the
condition is satisfied, the required processing con-
sists of calling the execution of actions grounded
in the effectors. Giving a conversational assistant
the capacity to learn rules verbally opens the pos-
sibility of teaching more complex and personal-
ized rules, especially compared to visual program-
ming tools such as IFTTT and Zapier1. LIA can
ask questions if it cannot parse specifics parts of a
user-statement (e.g., if it cannot understand the if-
condition, see Figure 2 for an example). Another
advantage of a conversational setting is that LIA
can take initiative when certain things are left am-
biguous by the user (e.g., ask the user what to do if
there is a conflict on the calendar for the last rule
in the list above) — an issue that cannot be coped
with in traditional programming environments.

2.2 Learning World Knowledge
LIA can be taught knowledge about the world by
the user (e.g., concepts and ontologies) that can be
used as building blocks in teaching new programs.

A key advantage of a conversational interface
for teaching new programs is that it allows the
user to be naturally expressive about data (i.e.,
variables/constants) by modelling them after real-
world concepts. For example, instead of saying:

> If there is an email from Tom, Justine, Oscar
or Igor, forward it to Mary

LIA allows a user to say:

> If there is an email from my project team forward
it to my assistant

1https://ifttt.com/, https://zapier.com/

By relying on the concept of “my project team”
(instead of listing its members), the second ex-
pression is more efficient and natural. It is also
better from a programming perspective: if team
members change later, the rule will not have to be
redefined. LIA enables users to refer to arbitrary
concepts such as a project team or colleague, by
declaratively teaching it about them. For example:
> Oscar is on my project team

> Everyone on my project team is a colleague

The above examples are akin to defining formal
data-structures, containing class and field defini-
tions, instance creation and definitions of the natu-
ral concept hierarchy (class inheritance). Because
the object-oriented programming (OOP) paradigm
is designed to model the real world, LIA uses it as
the underlying knowledge model that the user can
build and modify naturally using language.

2.3 Grounding Knowledge to Perception
Not all knowledge can be easily conveyed through
crisp extensional definitions such as in the exam-
ple of project team above. Common concepts such
as important email or meeting request are difficult
to declaratively define. In a conventional program-
ming paradigm, the developer may opt to create
special functions for grounding such “fuzzy” con-
cepts using machine learning models (i.e., classi-
fiers) that are grounded in attributes of examples
(e.g., emails) observed through perceptual sensors.

A conversational programming paradigm offers
a natural interface for teaching “fuzzy” and “crisp”
concepts alike. Instead of defining hard rules for
detecting important emails for example, the user
may instead opt to provide descriptions that char-
acterize the concept statistically:
> An important email will usually be from a

colleague’s email address

> its subject may contain words like urgent or
important

By grounding such natural language descriptions
to observable attributes of emails, such descrip-
tions can be used to build classification models
for concepts such as ‘important emails’.

Example Interaction: Figure 2 shows an exam-
ple interaction exemplifying these abilities in LIA,
and also outlining its working. A video demon-
stration of the system can be seen at http://y2u.
be/YfKqpT0apQw. Next, we describe how these
abilities are implemented in LIA, and highlight
salient features of its architecture.

https://ifttt.com/
https://zapier.com/
http://y2u.be/YfKqpT0apQw
http://y2u.be/YfKqpT0apQw


147
Figure 2: Example interaction sequence with LIA. Anno-
tations on the right summarize different parts of the con-
versation and outline LIA’s working

3 Architecture and System Overview

An instructable conversational assistant can be
thought of as a new type of programming inter-
face that allows end-users to compose core func-
tionalities (over domains such as email, calendar,
etc.) into programs through natural language di-
alog. Just like conventional programming lan-
guages, this needs answers to design questions
such as: “how are new components (functions)
imported into the instructable agent”, and “how
do these components communicate (e.g., what are
the data types, how do variables get created and
passed between functions)”. As examples in this
paper illustrate, allowing end-users teach an assis-
tant through conversation brings new challenges to
the design of a software architecture that facilitates
programming via dialog. We outline five features
that we see as fundamental to any system which
can be “programmed” through conversation, and
describe how LIA implements them:

3.1 Verbally Referencing and Passing Data
between Sensors and Effectors

In a conventional programming language, for-
mally declared variables allow one to explicitly
store and reference information later in the pro-
gram. A conversational programming interface
needs to allow for a similar mechanism by allow-
ing users to refer and reuse data during instruc-
tion through verbal references. For example, con-
sider the following instructions that the user can
give while teaching a new procedure “If there is a
meeting request, put it on my calendar”:

> Check if there is a time or date mentioned in
the message

> Then set the time of the new event to that time

This example illustrates the requirement for pass-
ing data between two components (email and cal-
endar APIs), which requires reference resolution
on the part of LIA’s semantic parser (e.g., which
“event” did the user refer to in this context?).

LIA solves the problem of interpreting users’
utterances and that of resolving references to vari-
ables (e.g., “subject of the received email” or
“assistant’s email address”) jointly. Reference
resolution is context-dependent, and is difficult
to solve using rule-based heuristics. The prob-
lem of semantic parsing and variable resolution
is addressed by LIA using a machine learning



148

based approach. For example, if the user men-
tions “Tom’s email”, and there are multiple con-
tacts named Tom, the agent can use its conversa-
tional context (e.g., most recently mentioned enti-
ties) and world knowledge to help and resolve the
reference – both are naturally incorporated as fea-
tures; weights for these are learned continuously
through interactions with the user.

LIA uses a synchronous CFG-based parser im-
plemented using SEMPRE (Berant et al., 2013),
and an underlying frame-based meaning repre-
sentation (i.e., a frame consists of an intent such
as CREATE NEW CONCEPT and any arguments
such as the name of the concept), allowing nested
frames for certain intents (currently the IF THEN
intent). The parser has an underlying log-linear
parameterization of the frame/utterance pairs, with
weights that can be learned offline and updated on-
line during interactions with the user. LIA uses
the following two classes of features to represent
utterance/frame pairs:
• Lexical/logical form features: these include

indicator features for derivation rules used in
the parse, as well as the conjunction of non-
terminals in the derivation with the part-of-
speech tags spanned by the derivation.

• Variable resolution features: these include in-
dicator features that fire if the resolved refer-
ence matches only partially to the variable name
(e.g., if the user mentions “affiliation” rather
than “university affiliation”), and a feature that
indicates whether the variable was mentioned
recently in the conversational context.

The variable resolution features are very power-
ful in that they allow the incorporation of external
context to help the agent resolve references by re-
lying on the aggregate information from all sen-
sor and effectors of the physical device. For ex-
ample, a reference to a particular person may be
ambiguous when interpreted in isolation, but may
naturally resolve to the person who recently sent
a text-message or an email. External information
such as this, can be incorporated into the variable
resolution features in a scalable way.

3.2 Generalizing Programs from a Single
Example

In conventional programming, explicit functions
serve as reusable building blocks and must be
expressed via specialized syntax to declare what
parts of the procedure can be generalized to differ-

ent arguments. In a conversational setting, the user
is not likely to be explicit about what parts of what
they teach should generalize – this knowledge is
often implicit based on the context of the taught
program. Thus, programs taught via conversation
need to be intelligent in automatically generaliz-
ing to future invocations with different arguments
where appropriate (e.g., if the user taught the agent
how to “tell colleagues to...”, the same procedure
should correctly generalize to “tell friends to...”.

When a user teaches a new procedure to LIA,
the interaction is always grounded in the specific
context within which the user was teaching it. To
explain, when teaching how to “tell my boss that
I will be late”, the user will narrate the sequence
of instructions to the agent that repeat arguments
from the original command, e.g.,
> Set the recipient to my boss ’s email address

> Then set its subject to I will be late

Here, “boss” and “I will be late” are arguments
repeated from the original utterance that is being
taught. In a conventional programming language,
the programmer would write a function that would
explicitly indicate which parts of the procedure
are placeholders and would be replaced with ar-
guments in any future invocations of the program.
In a conversational setting, the agent must have
the capacity to automatically identify what parts of
the taught program are placeholders and should be
substituted with different arguments in the future.
LIA’s algorithm is based on Azaria et al. (2016) –
it identifies matches between the command being
taught and the references to entities made in the
program; it then uses this information to store a
templated version of the taught program that can
be re-used for future invocations with different ar-
guments, e.g.,:“tell my friends I am on my way”.

3.3 Define New Knowledge
In a conventional programming language, data
structures and their relationships (e.g., inheri-
tance) must be declared formally. On the other
hand, LIA infers the data types and relationships
declared by the user from natural language state-
ments (e.g., “most colleagues have a university
affiliation” declares a new field ‘university affili-
ation’, and “everyone on the cmu team is a col-
league” creates an class-inheritance relation be-
tween a member of a cmu team and a colleague).
These are identified through a set of manually de-
fined syntactic patterns in the semantic parser.



149

LIA represents an agent’s knowledge in a tradi-
tional object-oriented paradigm: the agent’s world
consists of classes (referred to as concepts), and
instances (referred to as objects). Classes can ex-
tend (i.e., be inherited by) at most one other class,
while instances can instantiate multiple classes.
Further, if a concept (or instance) in LIA extends
other concepts, it also inherits all of its fields.

3.4 Grounding New Knowledge in Sensors
and Effectors

An important component of an intelligent assis-
tant is the ability to ground language and abstract
concepts in observable perception, through sen-
sors and effectors. We envision enabling the agent
to learn concepts (such as important emails) from
a combination of explanations, and examples of
the concept. This is motivated by our recent re-
search on using natural language to define fea-
ture functions for learning tasks (Srivastava et al.,
2017), and also work on using declarative knowl-
edge in natural language explanations to super-
vise training of classifiers (Srivastava et al., 2018).
Using semantic parsing, we can map natural lan-
guage statements to predicates in a logical lan-
guage, which are grounded in sensor-effector ca-
pabilities of the personal agent. These may enable
the user to:

1. Mention specific attributes that characterize a
concept (e.g., define a boolean feature that
checks whether an email comes from a col-
league)

2. Assert fuzzy statistical constraints specifying
relationships between such feature and labels
(e.g., ‘emails from my colleagues are usually
important’)

In combination, these capabilities can poten-
tially allow the agent to be taught classifiers for
fuzzy concepts from a blend of natural language
explanations of these concepts, and labeled or un-
labeled data.

Using these explanations and unlabeled data, an
automated learner can output a classifier that can
predict the class for a new instance. The system
can currently be used to train classifiers for a small
number of restricted domains. The classifier learn-
ing component is currently a standalone module
(separate from rest of LIA). We plan to make this
publicly accessible in the future.

Figure 3: Knowledge View in user interface, which dis-
plays procedures taught by a user, along with utilized sen-
sors and effectors

3.5 Mixed-Initiative Interactions
In a conventional programming language, the pro-
grammer anticipates all possible outcomes of var-
ious API calls made in a program, wrapping these
calls with control-flow statements (if/then/else
blocks) to account for different return flows. Con-
versely, end-user programmers must not be re-
quired to be explicit about all possible program
flows, but rather must inherently be in the form
of a mixed-initiative dialog. LIA does this by be-
ing pro-active in identifying possible control flow
branches based on the instructions the user has
provided while teaching. Consider an example:

> Check that I am available tomorrow at 2pm

...

> What should I do if you are not available?

Here, the question creates a control flow branch,
from which point on the user instructs a sequence
of actions that would be triggered only if the con-
dition that the agent asked about was true. One
of the key challenges in providing this mixed-
initiative strategy is scaling it to multiple sen-
sors and effectors, where the API for each sen-
sor/effector could trigger a set of potential con-
trol flow branches based on the internal execution
paths of the individual sensor/effector methods.

LIA’s architecture facilitates a generic way of
integrating new sensor/effector classes by auto-
matically discovering possible outputs of the API
method calls through a static analysis of the API
source code. This static code analysis registers this
information as possible control flow branches and
uses it during the dialog with the user to ask what
to do when these control flow branches are reached
during execution. Of course, not every possible
output of a particular API call (e.g., checking the
user’s availability) requires asking the user what to



150

Figure 4: Program flow with conditional branches for an
example program visualized in terms of the API method
calls. Dashed lines show data (variables) passed between
different method calls. White boxes denote branches that
were explicitly prompted by the agent by asking a ques-
tion on what to do in a given situation (see Fig 2)

do. It is up to the API developer to decide what ex-
ecution paths inside the API call warrant generat-
ing a question to the user. To communicate this in-
formation explicitly and in a standard way, we re-
quire that the API methods always return an object
of a special output return type that both (i) contains
the message to the user and (ii) encodes the infor-
mation about whether the execution path resulting
in this return value was successful or resulted in an
error, and whether the agent should probe the user
on what to do when this branch point is reached.

Figure 4 illustrates a full program taught by the
user (“if there is an important meeting request
then put it on my calendar”) with two conditional
branches; both branches were prompted by the
agent explicitly asking the user for what to do in
two scenarios: the user is not available at a partic-
ular time, and the original email does not contain
a time/date. See Figure 4 for more details.

4 Conclusion and Outlook

We have presented LIA, a natural language pro-
grammable assistant that allows users to teach it
new procedures, define new concepts and con-
cept hierarchies, and ground these to observable
attributes and actions in the agent’s repertoire of
sensors and effectors. Currently, LIA is limited
in some ways. For example, newly taught com-

Figure 5: Screenshot of LIA being taught a new procedure
on an Android mobile device

mands are incorporated as new grammar rules in
the semantic parser. As a result, future invocations
of the command that exhibit lexical and syntactic
variations may not correctly parse. Also, currently
LIA only uses conversational and lexical features
to resolve ambiguities in grounding frames to the
entities in the knowledge base. The mechanism of
incorporating features, however, is general and fu-
ture efforts can also incorporate external features
from multiple sensors (e.g., your location, person
you talked to most recently, etc.).

We believe that computers that can be interac-
tively instructed from natural language present an
exciting new area, which can have significant im-
plications for both learning and language research,
as well as engender a range of creative applica-
tions. We hope that through our demonstration,
we can engage the community in this direction.

References
Amos Azaria, Jayant Krishnamurthy, and Tom M

Mitchell. 2016. Instructable intelligent personal
agent. In AAAI, pages 2681–2689.

Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic parsing on freebase from
question-answer pairs. In Proceedings of the 2013
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1533–1544.

Shashank Srivastava, Igor Labutov, and Tom Mitchell.
2017. Joint concept learning and semantic parsing
from natural language explanations. In Proceedings
of the 2017 Conference on Empirical Methods in
Natural Language Processing, pages 1527–1536.

Shashank Srivastava, Igor Labutov, and Tom Mitchell.
2018. Zero-shot learning of classifiers from natu-
ral language quantification. In Proceedings of the
56th Annual Meeting of the Association for Compu-
tational Linguistics.


