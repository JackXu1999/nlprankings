



















































Distributional Modeling on a Diet: One-shot Word Learning from Text Only


Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 204–213,
Taipei, Taiwan, November 27 – December 1, 2017 c©2017 AFNLP

Distributional Modeling on a Diet:
One-shot Word Learning from Text Only

Su Wang♠ Stephen Roller♣ Katrin Erk♠
♠Department of Linguistics, ♣Department of Computer Science

The University of Texas at Austin
shrekwang@utexas.edu

roller@cs.utexas.edu, katrin.erk@mail.utexas.edu

Abstract
We test whether distributional models can
do one-shot learning of definitional prop-
erties from text only. Using Bayesian
models, we find that first learning overar-
ching structure in the known data, regular-
ities in textual contexts and in properties,
helps one-shot learning, and that individ-
ual context items can be highly informa-
tive. Our experiments show that our model
can learn properties from a single expo-
sure when given an informative utterance.

1 Introduction

When humans encounter an unknown word in text,
even with a single instance, they can often infer
approximately what it means, as in this example
from Lazaridou et al. (2014):

We found a cute, hairy wampimuk sleep-
ing behind the tree.

People who hear this sentence typically guess
that a wampimuk is an animal, or even that it is a
mammal. Distributional models, which describe
the meaning of a word in terms of its observed
contexts (Turney and Pantel, 2010), have been
suggested as a model for how humans learn word
meanings (Landauer and Dumais, 1997). How-
ever, distributional models typically need hun-
dreds of instances of a word to derive a high-
quality representation for it, while humans can of-
ten infer a passable meaning approximation from
one sentence only (as in the above example). This
phenomenon is known as fast mapping (Carey and
Bartlett, 1978), Our primary modeling objective in
this paper is to explore a plausible model for fast-
mapping learning from textual context.

While there is preliminary evidence that fast
mapping can be modeled distributionally (Lazari-
dou et al., 2016), it is unclear what enables it.

How do humans infer word meanings from so
little data? This question has been studied for
grounded word learning, when the learner per-
ceives an object in non-linguistic context that cor-
responds to the unknown word. The literature
emphasizes the importance of learning general
knowledge or overarching structure, which we de-
fine as the information that is learned by accumu-
lation across concepts (e.g. regularities in property
co-occurrence), across all concepts (Kemp et al.,
2007), In grounded word learning, overarching
structure that has been proposed includes knowl-
edge about which properties. For example knowl-
edge about which properties are most important to
object naming (Smith et al., 2002; Colunga and
Smith, 2005), or a taxonomy of concepts (Xu and
Tenenbaum, 2007).

In this paper we study models for fast mapping
in word learning1 from textual context alone, us-
ing probabilistic distributional models. Our task
differs from the grounded case in that we do not
perceive any object labeled by the unknown word.
In that context, learning word meaning means
learning the associated definitional properties and
their weights (see Section 3). For the sake of inter-
pretability, we focus on learning definitional prop-
erties We ask what kinds of overarching structure
in distributional contexts and in properties will be
helpful for one-shot word learning.

We focus on learning from syntactic context.
Distributional representations of syntactic context
are directly interpretable as selectional constraints,
which in manually created resources are typi-
cally characterized through high-level taxonomy
classes (Kipper-Schuler, 2005; Fillmore et al.,
2003). So they should provide good evidence for

1In this paper, we interchangeably use the terms unknown
word and unknown concept, as we learn properties, and prop-
erties belong to concepts rather than words, and we learn
them from text, where we observe words rather than concepts.

204



the meaning of role fillers. Also, it has been shown
that selectional constraints can be learned distri-
butionally (Erk et al., 2010; Ó Séaghdha and Ko-
rhonen, 2014; Ritter et al., 2010). However, our
point will not be that syntax is needed for fast word
learning, but that it helps to observe overarching
structure, with syntactic context providing a clear
test bed.

We test two types of overarching structure for
their usefulness in fast mapping. First, we hypoth-
esize that it is helpful to learn about commonali-
ties among context items, which enables mapping
from contexts to properties. For example the syn-
tactic contexts eat-dobj and cook-dobj should pre-
fer similar targets: things that are cooked are also
things that are eaten (Hypothesis H1).

The second hypothesis is that it will be use-
ful to learn co-occurrence patterns between prop-
erties. That is, we hypothesize that in learning
an entity is a mammal, we may also infer it is
four-legged (Hypothesis H2).

We do not intent to make strong cognitive
claims, for which additional experimentation will
be in order, and we leave this for future work. This
work sets its goal on building a plausible compu-
tational model that models human fast-mapping in
learning (i) well from limited grounded data, (ii)
effectively from only one instance.

2 Background

Fast mapping and textual context. Fast map-
ping (Carey and Bartlett, 1978) is the human abil-
ity to construct provisional word meaning repre-
sentations after one or few exposures. An impor-
tant reason for why humans can do fast mapping
is that they acquire overarching structure that con-
strains learning (Smith et al., 2002; Colunga and
Smith, 2005; Kemp et al., 2007; Xu and Tenen-
baum, 2007; Maas and Kemp, 2009). In this paper,
we ask what forms of overarching structure will be
useful for text-based word learning.

Lazaridou et al. (2014) consider fast mapping
for grounded word learning, mapping image data
to distributional representations, which is in a way
the mirror image of our task. Lazaridou et al.
(2016) were the first to explore fast mapping for
text-based word learning, using an extension to
word2vec with both textual and visual features.
However, they model the unknown word simply
by averaging the vectors of known words in the
sentence, and do not explore what types of knowl-

edge enable fast mapping.
Definitional properties. Feature norms are def-

initional properties collected from human partici-
pants. Feature norm datasets are available from
McRae et al. (2005) and Vigliocco et al. (2004). In
this paper we use feature norms as our target rep-
resentations of word meaning. There are several
recent approaches that learn to map distributional
representations to feature norms (Johns and Jones,
2012; Rubinstein et al., 2015; Făgărăşan et al.,
2015; Herbelot and Vecchi, 2015a). We also map
distributional information to feature norms, but we
do it based on a single textual instance (one-shot
learning).

In the current paper we use the Quantified
McRae (QMR) dataset (Herbelot and Vecchi,
2015b), which extends the McRae et al. (2005)
feature norms by ratings on the proportion of cate-
gory members that have a property, and the An-
imal dataset (Herbelot, 2013), which is smaller
but has the same shape. For example, most al-
ligators are dangerous. The quantifiers are given
probabilistic interpretations, so if most alligators
are dangerous, the probability for a random alli-
gator to be dangerous would be 0.95. This makes
this dataset a good fit for our probabilistic distri-
butional model. We discuss QMR and the Animal
data further in Section 4.

Bayesian models in lexical semantics. We
use Bayesian models for the sake of interpretabil-
ity and because the existing definitional property
datasets are small. The Bayesian models in lexi-
cal semantics that are most related to our approach
are Dinu and Lapata (2010), who represent word
meanings as distributions over latent topics that
approximate senses, and Andrews et al. (2009)
and Roller and Schulte im Walde (2013), who use
multi-modal extensions of Latent Dirichlet Allo-
cation (LDA) models (Blei et al., 2003) to repre-
sent co-occurrences of textual context and defini-
tional features. Ó Séaghdha (2010) and Ritter et al.
(2010) use Bayesian approaches to model selec-
tional preferences.

3 Models

In this section we develop a series of models to test
our hypothesis that acquiring general knowledge is
helpful to word learning, in particular knowledge
about similarities between context items (H1) and
co-occurrences between properties (H2). The
count-based model will implement neither hypoth-

205



esis, while the bimodal topic model will imple-
ment both. To test the hypotheses separately, we
employ two clustering approaches via Bernoulli
Mixtures, which we use as extensions to the count-
based model and bimodal topic model.

3.1 The Count-based Model

Independent Bernoulli condition. Let Q be a
set of definitional properties, C a set of concepts
that the learner knows about, and V a vocabulary
of context items. For most of our models, con-
text items w ∈ V will be predicate-role pairs such
as eat-dobj. The task is determine properties that
apply to an unknown concept u 6∈ C. Any con-
cept c ∈ C is associated with a vector cInd (where
“Ind” stands for “independent Bernoulli probabil-
ities”) of |Q| probabilities, where the i-th entry
of cInd is the probability that an instance of con-
cept c would have property qi. These probabil-
ities are independent Bernoulli probabilities. For
instance, alligatorInd would have an entry of 0.95
for dangerous. An instance c ∈ {0, 1}|Q| of a
concept c ∈ C is a vector of zeros and ones drawn
from cInd, where an entry of 1 at position i means
that this instance has the property qi.

The model proceeds in two steps. First it learns
property probabilities for context items w ∈ V .
The model observes instances c occurring textu-
ally with context itemw, and learns property prob-
abilities for w, where the probability that w has
for a property q indicates the probability that w
would appear as a context item with an instance
that has property q. In the second step the model
uses the acquired context item representations to
learn property probabilities for an unknown con-
cept u. When u appears withw, the context itemw
“imagines” an instance (samples it from its prop-
erty probabilities), and uses this instance to update
the property probabilities of u. Instead of mak-
ing point estimates, the model represents its uncer-
tainty about the probability of a property through
a Beta distribution, a distribution over Bernoulli
probabilities. As a Beta distribution is character-
ized by two parameters α and β, we associate each
context item w ∈ V with vectors wα ∈ R|Q| and
wβ ∈ R|Q|, where the i-th α and β values are the
parameters of the Beta distribution for property qi.
When an instance c is observed with context item
w, we do a Bayesian update on w simply as

wα = wα + c
wβ = wβ + (1− c) (1)

because the Beta distribution is the conjugate prior
of the Bernoulli. To draw an instance from w, we
draw it from the predictive posterior probabilities
of its Beta distributions, wInd = wα/(wα + wβ).

Likewise, we associate an unknown concept u
with vectors uα and uβ . When the model observes
u in the context of w, it draws an instance from
wInd, and performs a Bayesian update as in (1) on
the vectors associated with u. After training, the
property probabilities for u are again the posterior
predictive probabilities uInd = uα/(uα+uβ). The
model can be used for multi-shot learning and one-
shot learning in the same way.

Multinomial condition. We also test a multi-
nomial variant of the count-based model, for
greater comparability with the LDA model below.
Here, the concept representation cMult is a multi-
nomial distribution over the properties in Q. (That
is, all the properties compete in this model.) An
instance of concept c is now a single property,
drawn from c’s multinomial. The representation
of a context item w, and also the representation
of the unknown concept u, is a Dirichlet distribu-
tion with |Q| parameters. Bayesian update of the
representation ofw based on an occurrence with c,
and likewise Bayesian update of the representation
of u based on an occurrence with w, is straight-
forward again, as the Dirichlet distribution is the
conjugate prior of the multinomial.

The two count-based models do not implement
either of our two hypotheses. They compute sep-
arate selectional constraints for each context item,
and do not attend to co-occurrences between prop-
erties. In the experiments below, the count-based
models will be listed as Count Independent and
Count Multinomial.

3.2 The Bimodal Topic Model

We use an extension of LDA (Blei et al., 2003)
to implement our hypotheses on the usefulness
of overarching structure, both commonalities in
selectional constraints across predicates, and co-
occurrence of properties across concepts. In par-
ticular, we build on Andrews et al. (2009) in us-
ing a bimodal topic model, in which a single topic
simultaneously generates both a context item and
a property. We further build on Dinu and Lapata
(2010) in having a “pseudo-document” for each
concept c to represent its observed occurrences.
In our case, this pseudo-document contains pairs
of a context item w ∈ V and a property q ∈ Q,

206



α

θc

z

w qφz ψz

β γ

(w, q)

D

z z

Figure 1: Plate diagram for the Bimodal Topic
Model (bi-TM)

meaning that w has been observed to occur with
an instance of c that had q.

The generative story is as follows. For each
known concept c, draw a multinomial θc over top-
ics. For each topic z, draw a multinomial φz over
context items w ∈ V , and a multinomial ψz over
properties q ∈ Q. To generate an entry for c’s
pseudo-document, draw a topic z ∼ Mult(θc).
Then, from z, simultaneously draw a context item
from φz and a property from ψz . Figure 1 shows
the plate diagram for this model.

To infer properties for an unknown concept u,
we create a pseudo-document for u containing just
the observed context items, no properties, as those
are not observed. From this pseudo-document du
we infer the topic distribution θu. Then the proba-
bility of a property q given du is

P (q|du) =
∑
z

P (z|θu)P (q|ψz) (2)

For the one-shot condition, where we only observe
a single context item w with u, this simplifies to

P (q|w) =
∑
z

P (z|w)P (q|ψz) (3)

We refer to this model as bi-TM below. The
topics of this model implement our hypothesis
H1 by grouping context items that tend to occur
with the same concepts and the same properties.
The topics also implement our hypothesis H2 by
grouping properties that tend to occur with the
same concepts and the same context items. By
using multinomials ψz it makes the simplifying
assumption that all properties compete, like the
Count Multinomial model above.

3.3 Bernoulli Mixtures
With the Count models, we investigate word learn-
ing without any overarching structures. With the
bi-TMs, we investigate word learning with both
types of overarching structures at once. In order
to evaluate each of the two hypotheses separately,
we use clustering with Bernoulli Mixture models
of either the context items or the properties.

A Bernoulli Mixture model (Juan and Vidal,
2004) assumes that a population ofm-dimensional
binary vectors x has been generated by a set of
mixture components K, each of which is a vector
of m Bernoulli probabilities:

p(x) =
|K|∑
k=1

p(k)p(x|k) (4)

A Bernoulli Mixture can represent co-occurrence
patterns between the m random variables it mod-
els without assuming competition between them.

To test the effect of modeling cross-predicate
selectional constraints, we estimate a Bernoulli
Mixture model from n instances w for each w ∈
V , sampled from wInd (which is learned as in
the Count Independent model). Given a Bernoulli
Mixture model of |K| components, we then assign
each context item w to its closest mixture compo-
nent as follows. Say the instances of w used to es-
timate the Bernoulli Mixture were {w1, . . . ,wn},
then we assign w to the component

kw = argmaxk
n∑
j=1

p(k|wj) (5)

We then re-train the representations of context
items in the Count Multinomial condition, treating
each occurrence of c with context w as an occur-
rence of c with kw. This yields a Count Multino-
mial model called Count BernMix H1.

To test the effect of modeling property
co-occurrences, we estimate a |K|-component
Bernoulli Mixture model from n instances of each
known concept c ∈ C, sampled from cInd. We
then represent each concept c by a vector cMult, a
multinomial with |K| parameters, as follows. Say
the instances of c used to estimate the Bernoulli
Mixture were {c1, . . . , cn}, then the k-th entry in
cMult is the average probability, over all ci, of be-
ing generated by component k:

ck =
1
n

n∑
j=1

p(k|cj) (6)

207



This can be used as a Count Multinomial model
where the entries in cMult stand for Bernoulli Mix-
ture components rather than individual properties.
We refer to it as Count BernMix H2.2

Finally, we extend the bi-TM with the H2
Bernoulli Mixture in the same way as a Count
Multinomial model, and list this extension as bi-
TM BernMix H2. While the bi-TM already im-
plements both H1 and H2, its assumption of com-
petition between all properties is simplistic, and
bi-TM BernMix H2 tests whether lifting this as-
sumption will yield a better model. We do not
extend the bi-TM with the H1 Bernoulli Mixture,
as the assumption of competition between context
items that the bi-TM makes is appropriate.

4 Data and Experimental Setup

Definitional properties. As we use probabilis-
tic models, we need probabilities of properties
applying to concept instances. So the QMR
dataset (Herbelot and Vecchi, 2015b) is ideally
suited. QMR has 532 concrete noun concepts,
each associated with a set of quantified proper-
ties. The quantifiers have been given probabilistic
interpretations, mapping all→1, most→0.95,
some→0.35, few→0.05, none→0.3 Each con-
cept/property pair was judged by 3 raters. We
choose the majority rating when it exists, and
otherwise the minimum proposed rating. To ad-
dress sparseness, especially for the one-shot learn-
ing setting, we omit properties that are named
for fewer than 5 concepts. This leaves us with
503 concepts and 220 properties We intentionally
choose this small dataset: One of our main objec-
tives is to explore the possibility of learning effec-
tively from very limited training data. In addition,
while the feature norm dataset is small, our distri-
butional dataset (the BNC, see below) is not. The
latter essentially serves as a pivot for us to propa-
gate the knowledge from the feature norm data to
the wider semantic space.

It is a problem of both the original McRae et al.
(2005) data and QMR that if a property is not
named by participants, it is not listed, even if it ap-
plies. For example, the property four-legged

2We use the H2 Bernoulli Mixture as a soft clustering be-
cause it is straightforward to do this through concept repre-
sentations. For the H1 mixture, we did not see an obvious
soft clustering, so we use it as a hard clustering.

3The dataset also contains KIND properties that do not
have probabilistic interpretations. Following Herbelot and
Vecchi (2015a) we omit these properties.

is missing for alligator in QMR. So we addition-
ally use the Animal dataset of Herbelot (2013),
where every property has a rating for every con-
cept. The dataset comprises 72 animal concepts
with quantification information for 54 properties.

Distributional data. We use the British Na-
tional Corpus (BNC) (The BNC Consortium,
2007), with dependency parses from Spacy. 4 As
context items, we use pairs 〈pred, dep〉 of pred-
icates pred that are content words (nouns, verbs,
adjectives, adverbs) but not stopwords, where a
concept from the respective dataset (QMR, Ani-
mal) is a dependency child of pred via dep. In to-
tal we obtain a vocabulary of 500 QMR concepts
and 72 Animal concepts that appear in the BNC,
and 29,124 context items. We refer to this syn-
tactic context as Syn. For comparison, we also
use a baseline model with a bag-of-words (BOW)
context window of 2 or 5 words, with stopwords
removed.

Models. We test our probabilistic models as
defined in the previous section. While our focus
is on one-shot learning, we also evaluate a multi-
shot setting where we learn from the whole BNC,
as a sanity check on our models. (We do not test
our models in an incremental learning setting that
adds one occurrence at a time. While this is pos-
sible in principle, the computational cost is pro-
hibitive for the bi-TM.) We compare to the Partial
Least Squares (PLS) model of Herbelot and Vec-
chi (2015a)5 to see whether our models perform
at state of the art levels. We also compare to a
baseline that always predicts the probability of a
property to be its relative frequency in the set C of
known concepts (Baseline).

We can directly use the property probabilities in
QMR and the Animal data as concept representa-
tions cInd for the Count Independent model. For
the Count Multinomial model, we never explicitly
compute cMult. To sample from it, we first sample
an instance c ∈ {0, 1}|Q| from the independent
Bernoulli vector of c, cInd. From the properties
that apply to c, we sample one (with equal prob-
abilities) as the observed property. All priors for
the count-based models (Beta priors or Dirichlet
priors, respectively) are set to 1.

For the bi-TM, a pseudo-document for a known

4https://spacy.io
5Herbelot and Vecchi (2015a) is the only directly relevant

previous work on the subject. Further, to the best of our
knowledge, for one-shot property learning from text (only),
our work has been the first attempt.

208



Models QMR AnimalBOW5 Syn Syn
Baseline 0.12 0.16 0.63
PLS 0.24 0.35 0.71
Count Mult. 0.13 0.25 0.64

Ind. 0.11 0.23 0.64
BernMix H1 0.11 0.17 0.65
BernMix H2 0.10 0.18 0.63

bi-TM plain 0.23 0.36 0.80
BernMix H2 0.20 0.34 0.81

Table 1: MAP scores, multi-shot learning on the
QMR and Animal datasets

concept c is generated as follows: Given an occur-
rence of known concept c with context item w in
the BNC, we sample a property q from c (in the
same way as for the Count Multinomial model),
and add 〈w, q〉 to the pseudo-document for c. For
training the bi-TM, we use collapsed Gibbs sam-
pling (Steyvers and Griffiths, 2007) with 500 it-
erations for burn-in. The Dirichlet priors are uni-
formly set to 0.1 following Roller and Schulte im
Walde (2013). We use 50 topics throughout.

For all our models, we report the average per-
formance from 5 runs. For the PLS benchmark,
we use 50 components with otherwise default set-
tings, following Herbelot and Vecchi (2015a).

Evaluation. We test all models using 5-fold
cross validation and report average performance
across the 5 folds. We evaluate performance using
Mean Average Precision (MAP) , which tests to
what extent a model ranks definitional properties
in the same order as the gold data. Assume a sys-
tem that predicts a ranking of n datapoints, where
1 is the highest-ranked, and assume that each dat-
apoint i has a gold rating of I(i) ∈ {0, 1}. This
system obtains an Average Precision (AP) of

AP =
1∑n

i=1 I(i)

n∑
i=1

Preci · I(i)

where Preci is precision at a cutoff of i. Mean
Average Precision is the mean over multiple AP
values. In our case, n = |Q|, and we compare a
model-predicted ranking of property probabilities
with a binary gold rating of whether the property
applies to any instances of the given concept. For
the one-shot evaluation, we make a separate pre-
diction for each occurrence of an unknown con-
cept u in the BNC, and report MAP by averaging
over the AP values for all occurrences of u.

5 Results and Discussion

Multi-shot learning. While our focus in this pa-
per is on one-shot learning, we first test all mod-
els in a multi-shot setting. The aim is to see how
well they perform when given ample amounts of
training data, and to be able to compare their per-
formance to an existing multi-shot model (as we
will not have any related work to compare to for
the one-shot setting.) The results are shown in
Table 1, where Syn shows results that use syntac-
tic context (encoding selectional constraints) and
BOW5 is a bag-of-words context with a window
size of 5. We only compare our models to the
baseline and benchmark for now, and do an in-
depth comparison of our models when we get to
the one-shot task, which is our main focus.

Across all models, the syntactic context outper-
forms the bag-of-words context. We also tested
a bag-of-words context with window size 2 and
found it to have a performance halfway between
Syn and BOW5 throughout. This confirms our as-
sumption that it is reasonable to focus on syntactic
context, and for the rest of this paper, we test mod-
els with syntactic context only.

Focusing on Syn conditions now, we see that
almost all models outperform the property fre-
quency baseline, though the MAP scores for the
baseline do not fall far behind those of the weak-
est count-based models.6 The best of our models
perform on par with the PLS benchmark of Her-
belot and Vecchi (2015a) on QMR, and on the
Animal dataset they outperform the benchmark.
Comparing the two datasets, we see that all mod-
els show better performance on the cleaner (and
smaller) Animal dataset than on QMR. This is
probably because QMR suffers from many false
negatives (properties that apply but were not men-
tioned), while Animal does not. The Count In-
dependent model shows similar performance here
and throughout all later experiments to the Count
Multinomial (even though it matches the construc-
tion of the QMR and Animal datasets better), so to
avoid clutter we do not report on it further below.

One-shot learning. Table 2 shows the perfor-

6This is because MAP gives equal credit for all prop-
erties correctly predicted as non-zero. When we evaluate
with Generalized Average Precision (GAP) (Kishida, 2005),
which takes gold weights into account, the baseline model
is roughly 10 points below other models. This indicates our
models learn approximate property distributions. We omit
GAP scores because they correlate strongly with MAP for
non-baseline models.

209



Models oracle AvgCosall top20 top20
Q

M
R

Count Mult. 0.16 0.37 0.28
BernMix H1 0.14 0.33 0.21
BernMix H2 0.15 0.31 0.22

bi-TM plain 0.21 0.47 0.35
BernMix H2 0.18 0.45 0.34

A
ni

m
al

Count Mult. 0.58 0.77 0.61
BernMix H1 0.60 0.80 0.57
BernMix H2 0.59 0.81 0.59

bi-TM plain 0.64 0.88 0.63
BernMix H2 0.65 0.89 0.66

Table 2: MAP scores, one-shot learning on the
QMR and Animal datasets

mance of our models on the one-shot learning task.
We cannot evaluate the benchmark PLS as it is not
suitable for one-shot learning. The baseline is the
same as in Table 1. The numbers shown are Av-
erage Precision (AP) values for learning from a
single occurrence. Column all averages over all
occurrences of a target in the BNC (using only
context items that appeared at least 5 times in the
BNC), and column oracle top-20 averages over the
20 context items that have the highest AP for the
given target. As can be seen, AP varies widely
across sentences: When we average over all oc-
currences of a target in the BNC, performance is
close to baseline level.7 But the most informa-
tive instances yield excellent information about an
unknown concept, and lead to MAP values that
are much higher than those achieved in multi-shot
learning (Table 1). We explore this more below.

Comparing our models, we see that the bi-TM
does much better throughout than any of the count-
based models. Since the bi-TM model imple-
ments both cross-predicate selectional constraints
(H1) and property co-occurrence (H2), we find
both of our hypotheses confirmed by these re-
sults. The Bernoulli mixtures improved perfor-
mance on the Animal dataset, with no clear pattern
of which one improved performance more. On
QMR, adding a Bernoulli mixture model harms
performance across both the count-based and bi-
TM models. We suspect that this is because of
the false negative entries in QMR; an inspection
of Bernoulli mixture H2 components supports this
intuition, as the QMR ones were found to be of
poorer quality than those for the Animal data.

Comparing Tables 1 and 2 we see that they show
7Context items with few occurrences in the corpus per-

form considerably worse than baseline, as their property dis-
tributions are dominated by the small number of concepts
with which they appear.

Count
Mult.

clothing, made of metal, differ-
ent colours, an animal, is long

bi-TM clothing, made of material, has -
sleeves, different colours,
worn by women

bi-TM
one-shot

clothing, is long, made of -
material, different colours,
has sleeves

Table 3: QMR: top 5 properties of gown. Top 2
entries: multi-shot. Last entry: one-shot, context
undo-dobj

Top undo-dobj (0.70), nylon-nmod (0.66),
pink-amod (0.65), retie-dobj (0.64), silk-
amod (0.64)

Bottom sport-nsubj (0.01), contemplate-dobj
(0.01), comic-amod (0.01), wait-nsubj
(0.01), fibrous-amod (0.01)

Table 4: QMR one-shot: AP for top and bottom 5
context items of gown

the same patterns of performance: Models that do
better on the multi-shot task also do better on the
one-shot task. This is encouraging in that it sug-
gests that it should be possible to build incremen-
tal models that do well both in a low-data and an
abundant-data setting.

Table 3 looks in more detail at what it is that the
models are learning by showing the five highest-
probability properties they are predicting for the
concept gown. The top two entries are multi-
shot models, the third shows the one-shot re-
sult from the context item with the highest AP.
The bi-TM results are very good in both the
multi-shot and the one-shot setting, giving high
probability to some quite specific properties like
has sleeves. The count-based model shows
a clear frequency bias in erroneously giving high
probabilities to the two overall most frequent
properties, made of metal and an animal.
This is due to the additive nature of the Count
model: In updating unknown concepts from con-
text items, frequent properties are more likely
to be sampled, and their effect accumulates as
the model does not take into account interactions
among context items. The bi-TM, which models
these interactions, is much more robust to the ef-
fect of property frequency.

Informativity. In Table 2 we saw that one-shot
performance averaged over all context items in the
whole corpus was quite bad, but that good, infor-
mative context items can yield high-quality prop-
erty information. Table 4 illustrates this point fur-

210



Model Freq. Entropy AvgCos

Q
M

R
Count Mult. 0.09 -0.12 0.18
Count BernMix H1 0.07 -0.10 0.17
Count BernMix H2 0.10 -0.09 0.17
bi-TM plain 0.15 -0.09 0.41·
bi-TM BernMix H2 0.16 -0.10 0.39·

A
ni

. bi-TM plain 0.25 -0.40 0.49*
bi-TM BernMix H2 0.23· -0.37· 0.52*

Table 5: Correlation of informativity with AP,
Spearman’s ρ. * and · indicate significance at
p < 0.05 and p < 0.1

ther. For the concept gown, it shows the five con-
text items that yielded the highest AP values, at
the top undo-obj, with an AP as high as 0.7.

This raises the question of whether we can pre-
dict the informativity of a context item.8 We test
three measures of informativity. The first is simply
the frequency of the context item, with the ratio-
nale that more frequent context items should have
more stable representations. Our second measure
is based on entropy. For each context item w,
we compute a distribution over properties as in
the count-independent model, and measure the en-
tropy of this distribution. If the distribution has
few properties account for a majority of the prob-
ability mass, then w will have a low entropy, and
would be expected to be more informative. Our
third measure is based on the same intuition, that
items with more “concentrated” selectional con-
straints should be more informative. If a context
item w has been observed to occur with known
concepts c1, . . . , cn, then this measure is the av-
erage cosine (AvgCos) of the property distribu-
tions (viewed as vectors) of any pair of ci, cj ∈
{c1, . . . , cn}.

We evaluate the three informativity measures
using Spearman’s rho to determine the correlation
of the informativity of a context item with the AP
it produces for each unknown concept. We expect
frequency and AvgCos to be positively correlated
with AP, and entropy to be negatively correlated
with AP. The result is shown in Table 5. Again, all
measures work better on the Animal data than on
QMR, where they at best approach significance.
The correlation is much better on the bi-TM mod-
els than on the count-based models, which is prob-
ably due to their higher-quality predictions. Over-
all, AvgCos emerges as the most robust indicator

8Lazaridou et al. (2016), who use a bag-of-words context
in one-shot experiments, propose an informativity measure
based on the number of contexst that constitute properties.
we cannot do that with our syntactic context.

Type MAP
Function 0.45
Taxonomic 0.62
Visual 0.34
Encyclopaedic 0.35
Perc 0.40

Table 6: QMR, bi-TM, one-shot: MAP by prop-
erty type over (oracle) top 20 context items

for informativity.9 We now test AvgCos, as our
best informativity measure, on its ability to se-
lect good context items. The last column of Ta-
ble 2 shows MAP results for the top 20 context
items based on their AvgCos values. The results
are much below the oracle MAP (unsurprisingly,
given the correlations in Table 5), but for QMR
they are at the level of the multi-shot results of Ta-
ble 1, showing that it is possible to some extent
to automatically choose informative examples for
one-shot learning.

Properties by type. McRae et al. (2005) clas-
sify properties based on the brain region taxon-
omy of Cree and McRae (2003). This enables us
to test what types of properties are learned most
easily in our fast-mapping setup by computing av-
erage AP separately by property type. To com-
bat sparseness, we group property types into five
groups, function (the function or use of an entity),
taxonomic, visual, encyclopaedic, and other per-
ceptual (e.g., sound). Intuitively, we would expect
our contexts to best reflect taxonomic and function
properties: Predicates that apply to noun target
concepts often express functions of those targets,
and manually specified selectional constraints are
often characterized in terms of taxonomic classes.
Table 6 confirms this intuition. Taxonomic prop-
erties achieve the highest MAP by a large margin,
followed by functional properties. Visual proper-
ties score the lowest.

6 Conclusion

We have developed several models for one-shot
learning word meanings from single textual con-
texts. Our models were designed learn word prop-
erties using distributional contexts (H1) or about
co-occurrences of properties (H2). We find evi-
dence that both kinds of general knowledge are

9We also tested a binned variant of the frequency measure,
on the intuition that medium-frequency context items should
be more informative than either highly frequent or rare ones.
However, this measure did not show better performance than
the non-binned frequency measure.

211



helpful, especially when combined (in the bi-TM),
or when used on clean property data (in the Ani-
mal dataset). We further saw that some contexts
are highly informative, and preliminary expire-
ments in informativity measures found that aver-
age pairwise similarity of seen role fillers (Avg-
Cos) achieves some success in predicting which
contexts are most useful.

In the future, we hope to test with other
types of general knowledge, including a taxon-
omy of known concepts (Xu and Tenenbaum,
2007); wider-coverage property data (Baroni and
Lenci, 2010, Type-DM); and alternative modal-
ities (Lazaridou et al., 2016, image features as
“properties”). We expect our model will scale to
these larger problems easily.

We would also like to explore better informa-
tivity measures and improvements for AvgCos.
Knowledge about informative examples can be
useful in human-in-the-loop settings, for exam-
ple a user aiming to illustrate classes in an on-
tology with a few typical corpus examples. We
also note that the bi-TM cannot be used in for
truly incremental learning, as the cost of global
re-computation after each seen example is pro-
hibitive. We would like to explore probabilistic
models that support incremental word learning,
which would be interesting to integrate with an
overall probabilistic model of semantics (Good-
man and Lassiter, 2014).

Acknowledgments

This research was supported by the DARPA DEFT
program under AFRL grant FA8750-13-2-0026
and by the NSF CAREER grant IIS 0845925. Any
opinions, findings, and conclusions or recommen-
dations expressed in this material are those of the
author and do not necessarily reflect the view of
DARPA, DoD or the US government. We ac-
knowledge the Texas Advanced Computing Cen-
ter for providing grid resources that contributed to
these results.

References
Mark Andrews, Gabriella Vigliocco, and David Vin-

son. 2009. Integrating experiential and distribu-
tional data to learn semantic representations. Psy-
chological Review, 116(3):463–498.

Marco Baroni and Alexandero Lenci. 2010. Dis-
tributional memory: a general framework for

corpus-based semantics. Computational Linguis-
tics, 36(4):673–721.

David Blei, Andrew Ng, and Michael Jordan. 2003.
Latent Dirichlet Allocation. Journal of Machine
Learning Research, 3(4-5):993–1022.

Susan Carey and Elsa Bartlett. 1978. Acquiring a sin-
gle new word. Papers and Reports on Child Lan-
guage Development, 15:17–29.

Eliana Colunga and Linda B. Smith. 2005. From the
lexicon to expectations about kinds: A role for asso-
ciative learning. Psychological Review, 112(2):347–
382.

George S. Cree and Ken McRae. 2003. Analyzing the
factors underlying the structure and computation of
the meaning of chipmunk, cherry, chisel, cheese, and
cello (and many other such concrete nouns). Jour-
nal of Experimental Psychology: General, 132:163–
201.

Georgiana Dinu and Mirella Lapata. 2010. Measuring
distributional similarity in context. In Proceedings
of EMNLP, Cambridge, MA.

Katrin Erk, Sebastian Padó, and Ulrike Padó. 2010. A
flexible, corpus-driven model of regular and inverse
selectional preferences. Computational Linguistics,
36(4).

Luana Făgărăşan, Eva Maria Vecchi, and Stephen
Clark. 2015. From distributional semantics to fea-
ture norms: Grounding semantic models in human
perceptual data. In Proceedings of IWCS, London,
Great Britain.

C. J. Fillmore, C. R. Johnson, and M. Petruck. 2003.
Background to FrameNet. International Journal of
Lexicography, 16:235–250.

Noah D. Goodman and Daniel Lassiter. 2014. Prob-
abilistic semantics and pragmatics: Uncertainty in
language and thought. In Shalom Lappin and Chris
Fox, editors, Handbook of Contemporary Semantics.
Wiley-Blackwell.

Aurélie Herbelot. 2013. What is in a text, what isn’t
and what this has to do with lexical semantics. Pro-
ceedings of IWCS.

Aurélie Herbelot and Eva Vecchi. 2015a. Building
a shared world:mapping distributional to model-
theoretic semantic spaces. In Proceedings of
EMNLP.

Aurélie Herbelot and Eva Maria Vecchi. 2015b. Many
speakers, many worlds. Linguistic Issues in Lan-
guage Technology, 12(4):1–20.

Brendan T Johns and Michael N Jones. 2012. Percep-
tual inference through global lexical similarity. Top-
ics in Cognitive Science, 4(1):103–120.

212



Alfons Juan and Enrique Vidal. 2004. Bernoulli mix-
ture models for binary images. In Proceedings of
ICPR.

Charles Kemp, Amy Perfors, and Joshua B. Tenen-
baum. 2007. Learning overhypotheses with hier-
archical Bayesian models. Developmental Science,
10(3):307–321.

Karin Kipper-Schuler. 2005. VerbNet: A broad-
coverage, comprehensive verb lexicon. Ph.D. thesis,
Computer and Information Science Dept., Univer-
sity of Pennsylvania, Philadelphia, PA.

Kazuaki Kishida. 2005. Property of average precision
and its generalization: An examination of evaluation
indicator for information retrieval experiments. NII
Technical Reports, 2005(14):1–19.

Thomas Landauer and Susan Dumais. 1997. A solution
to Plato’s problem: The latent semantic analysis the-
ory of acquisition, induction, and representation of
knowledge. Psychological Review, pages 211–240.

Angeliki Lazaridou, Elia Bruni, and Marco Baroni.
2014. Is this a wampimuk? Cross-modal map-
ping between distributional semantics and the visual
world. In Proceedings of ACL.

Angeliki Lazaridou, Marco Marelli, and Marco Baroni.
2016. Multimodal word meaning induction from
minimal exposure to natural text. Cognitive Science,
pages 1–30.

Andrew L. Maas and Charles Kemp. 2009. One-shot
learning with Bayesian networks. In Proceedings of
the 31st Annual Conference of the Cognitive Science
Society, Amsterdam, The Netherlands.

Ken McRae, George S. Cree, Mark S. Seidenberg, and
Chris McNorgan. 2005. Semantic feature produc-
tion norms for a large set of living and nonliving
things. Behavior Research Methods, 37(4):547–
559.

Diarmuid Ó Séaghdha. 2010. Latent variable models
of selectional preference. In Proceedings of ACL.

Diarmuid Ó Séaghdha and Anna Korhonen. 2014.
Probabilistic distributional semantics with latent
variable models. Computational Linguistics,
40(3):587–631.

Alan Ritter, Mausam, and Oren Etzioni. 2010. A La-
tent Dirichlet Allocation method for selectional pref-
erences. In Proceedings of ACL.

Stephen Roller and Sabine Schulte im Walde. 2013. A
multimodal lda model integrating textual, cognitive
and visual modalities. In Proceedings of EMNLP.

Dana Rubinstein, Effi Levi, Roy Schwartz, and Ari
Rappoport. 2015. How well do distributional mod-
els capture different types of semantic knowledge?
In Proceedings of ACL, volume 2, pages 726–730.

Linda B. Smith, Susan S. Jones, Barbara Landau, Lisa
Gershkoff-Stowe, and Larissa Samuelson. 2002.
Object name learning provides on-the-job training
for attention. Psychological Science, 13(1):13–19.

Mark Steyvers and Tom Griffiths. 2007. Probabilistic
topic models. In T. Landauer, D.S. McNamara, S.
Dennis, and W. Kintsch, eds., Handbook of Latent
Semantic Analysis.

The BNC Consortium. 2007. The British Na-
tional Corpus, version 3 (BNC XML Edition).
Oxford University Computing Services, URL:
http://www.natcorp.ox.ac.uk/.

Peter Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37:141–188.

Gabriella Vigliocco, David Vinson, William Lewis, and
Merrill Garrett. 2004. Representing the meanings of
object and action words: The featural and unitary
semantic space hypothesis. Cognitive Psychology,
48:422–488.

Fei Xu and Joshua B. Tenenbaum. 2007. Word learn-
ing as Bayesian inference. Psychological Review,
114(2):245–272.

213


