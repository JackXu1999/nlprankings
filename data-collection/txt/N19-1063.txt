



















































On Measuring Social Biases in Sentence Encoders


Proceedings of NAACL-HLT 2019, pages 622–628
Minneapolis, Minnesota, June 2 - June 7, 2019. c©2019 Association for Computational Linguistics

622

On Measuring Social Biases in Sentence Encoders

Chandler May1 Alex Wang2 Shikha Bordia2
Samuel R. Bowman2 Rachel Rudinger1

1Johns Hopkins University 2New York University
{cjmay,rudinger}@jhu.edu {alexwang,sb6416,bowman}@nyu.edu

Abstract

The Word Embedding Association Test shows
that GloVe and word2vec word embed-
dings exhibit human-like implicit biases based
on gender, race, and other social con-
structs (Caliskan et al., 2017). Meanwhile,
research on learning reusable text representa-
tions has begun to explore sentence-level texts,
with some sentence encoders seeing enthusi-
astic adoption. Accordingly, we extend the
Word Embedding Association Test to measure
bias in sentence encoders. We then test sev-
eral sentence encoders, including state-of-the-
art methods such as ELMo and BERT, for the
social biases studied in prior work and two im-
portant biases that are difficult or impossible
to test at the word level. We observe mixed re-
sults including suspicious patterns of sensitiv-
ity that suggest the test’s assumptions may not
hold in general. We conclude by proposing di-
rections for future work on measuring bias in
sentence encoders.

1 Introduction

Word embeddings quickly achieved wide adoption
in natural language processing (NLP), precipitat-
ing the development of efficient, word-level neural
models of human language. However, prominent
word embeddings such as word2vec (Mikolov
et al., 2013) and GloVe (Pennington et al., 2014)
encode systematic biases against women and
black people (Bolukbasi et al., 2016; Garg et al.,
2018, i.a.), implicating many NLP systems in scal-
ing up social injustice. We investigate whether
sentence encoders, which extend the word embed-
ding approach to sentences, are similarly biased.1

The previously developed Word Embedding
Association Test (WEAT; Caliskan et al., 2017)
measures bias in word embeddings by comparing
two sets of target-concept words to two sets of at-
tribute words. We propose a simple generaliza-

1 While encoder training data may contain perspectives
from outside the U.S., we focus on biases in U.S. contexts.

tion of WEAT to phrases and sentences: the Sen-
tence Encoder Association Test (SEAT). We apply
SEAT to sentences generated by inserting individ-
ual words from Caliskan et al.’s tests into simple
templates such as “This is a[n] <word>.”

To demonstrate the new potential of a sentence-
level approach and advance the discourse on bias
in NLP, we also introduce tests of two biases that
are less amenable to word-level representation:
the angry black woman stereotype (Collins, 2004;
Madison, 2009; Harris-Perry, 2011; hooks, 2015;
Gillespie, 2016) and a double bind on women in
professional settings (Heilman et al., 2004).

The use of sentence-level contexts also facili-
tates testing the impact of different experimental
designs. For example, several of Caliskan et al.’s
tests rely on given names associated with Euro-
pean American and African American people or
rely on terms referring to women and men as
groups (such as “woman” and “man”). We explore
the effect of using given names versus group terms
by creating alternate versions of several bias tests
that swap the two. This is not generally feasible
with WEAT, as categories like African Americans
lack common single-word group terms.

We find varying evidence of human-like bias
in sentence encoders using SEAT. Sentence-to-
vector encoders largely exhibit the angry black
woman stereotype and Caliskan biases, and to
a lesser degree the double bind biases. Recent
sentence encoders such as BERT (Devlin et al.,
2018) display limited evidence of the tested bi-
ases. However, while SEAT can confirm the ex-
istence of bias, negative results do not indicate the
model is bias-free. Furthermore, discrepancies in
the results suggest that the confirmed biases may
not generalize beyond the specific words and sen-
tences in our test data, and in particular that cosine
similarity may not be a suitable measure of repre-
sentational similarity in recent models, indicating
a need for alternate bias detection techniques.



623

Target Concepts Attributes

European American names:
Adam, Harry, Nancy, Ellen,
Alan, Paul, Katie, . . .

Pleasant: love, cheer,
miracle, peace, friend,
happy, . . .

African American names:
Jamel, Lavar, Lavon, Tia,
Latisha, Malika, . . .

Unpleasant: ugly, evil,
abuse, murder, assault,
rotten, . . .

Table 1: Subsets of target concepts and attributes from
Caliskan Test 3. Concept and attribute names are in
italics. The test compares the strength of association
between the two target concepts and two attributes,
where all four are represented as sets of words.

Target Concepts Attributes

European American names:
“This is Katie.”, “This is
Adam.” “Adam is there.”, . . .

Pleasant: “There is
love.”, “That is happy.”,
“This is a friend.”, . . .

African American names:
“Jamel is here.”, “That is
Tia.”, “Tia is a person.”, . . .

Unpleasant: “This is
evil.”, “They are evil.”,
“That can kill.”, . . .

Table 2: Subsets of target concepts and attributes from
the bleached sentence version of Caliskan Test 3.

2 Methods

The Word Embedding Association Test
WEAT imitates the human implicit association
test (Greenwald et al., 1998) for word embed-
dings, measuring the association between two
sets of target concepts and two sets of attributes.
Let X and Y be equal-size sets of target concept
embeddings and let A and B be sets of attribute
embeddings. The test statistic is a difference
between sums over the respective target concepts,

s(X,Y,A,B) =
[∑

x∈Xs(x,A,B)−∑
y∈Y s(y,A,B)

]
,

where each addend is the difference between mean
cosine similarities of the respective attributes,

s(w,A,B) =
[
meana∈A cos(w, a)−
meanb∈B cos(w, b)

]
A permutation test on s(X,Y,A,B) is used to
compute the significance of the association be-
tween (A,B) and (X,Y ),

p = Pr [s(Xi, Yi, A,B) > s(X,Y,A,B)] ,

where the probability is computed over the space
of partitions (Xi, Yi) of X ∪ Y such that Xi and
Yi are of equal size, and a normalized difference of

means of s(w,A,B) is used to measure the mag-
nitude of the association (the effect size; Caliskan
et al., 2017),

d =
meanx∈Xs(x,A,B)−meany∈Y s(y,A,B)

std devw∈X∪Y s(w,A,B)
.

Controlling for significance, a larger effect size re-
flects a more severe bias. We detail our implemen-
tations in the supplement.

The Sentence Encoder Association Test SEAT
compares sets of sentences, rather than sets of
words, by applying WEAT to the vector repre-
sentation of a sentence. Because SEAT operates
on fixed-sized vectors and some encoders produce
variable-length vector sequences, we use pooling
as needed to aggregate outputs into a fixed-sized
vector. We can view WEAT as a special case of
SEAT in which the sentence is a single word. In
fact, the original WEAT tests have been run on the
Universal Sentence Encoder (Cer et al., 2018).

To extend a word-level test to sentence con-
texts, we slot each word into each of several se-
mantically bleached sentence templates such as
“This is <word>.”, “<word> is here.”, “This will
<word>.”, and “<word> are things.”. These tem-
plates make heavy use of deixis and are designed
to convey little specific meaning beyond that of the
terms inserted into them.2 For example, the word
version of Caliskan Test 3 is illustrated in Table 1
and the sentence version is illustrated in Table 2.
We choose this design to focus on the associations
a sentence encoder makes with a given term rather
than those it happens to make with the contexts of
that term that are prevalent in the training data; a
similar design was used in a recent sentiment anal-
ysis evaluation corpus stratified by race and gen-
der (Kiritchenko and Mohammad, 2018). To fa-
cilitate future work, we publicly release code for
SEAT and all of our experiments.3

3 Biases Tested

Caliskan Tests We first test whether the sen-
tence encoders reproduce the same biases that
word embedding models exhibited in Caliskan
et al. (2017). These biases correspond to past
social psychology studies of implicit associations
in human subjects.4 We apply both the original

2 See the supplement for further details and examples.
3 http://github.com/W4ngatang/sent-bias
4 See Greenwald et al. (2009) for a review of this work.

http://github.com/W4ngatang/sent-bias


624

word-level versions of these tests as well as our
generated sentence-level versions.

Angry Black Woman Stereotype In the Sap-
phire or angry black woman (ABW) stereotype,
black women are portrayed as loud, angry, and
imposing (Collins, 2004; Madison, 2009; Harris-
Perry, 2011; hooks, 2015; Gillespie, 2016). This
stereotype contradicts common associations made
with the ostensibly race-neutral (unmarked) cat-
egory of women (Bem, 1974), suggesting that
that category is implicitly white. Intersectional-
ity reveals that experiences considered common
to women are not necessarily shared by black
women, who are marginalized both among women
and among black people (Crenshaw, 1989). Re-
cently, intersectionality has been demonstrated in
English Wikipedia using distributional semantic
word representations (Herbelot et al., 2012), and
in the disparate error rates of machine learning
technologies like face recognition (Buolamwini
and Gebru, 2018).

To measure sentence encoders’ reproduction of
the angry black woman stereotype, we create a
test whose target concepts are black-identifying
and white-identifying female given names from
Sweeney (2013, Table 1) and whose attributes are
adjectives used in the discussion of the stereo-
type in Collins (2004, pp. 87-90) and their
antonyms. We also produce a version of the
test with attributes consisting of terms describing
black women and white women as groups, as well
as sentence versions in which attribute and target
concept terms are inserted in sentence templates.

Double Binds Women face many double binds,
contradictory or unsatisfiable expectations of fem-
ininity and masculinity (Stone and Lovejoy, 2004;
Harris-Perry, 2011; Mitchell, 2012). If women
clearly succeed in a male gender-typed job, they
are perceived less likable and more hostile than
men in similar positions; if success is am-
biguous, they are perceived less competent and
achievement-oriented than men. Both outcomes
can interfere in performance evaluations (Heilman
et al., 2004), contributing to the glass ceiling im-
peding women’s career advancement.5

We test this double bind in sentence encoders
by translating Heilman et al.’s experiment to two
SEAT tests. In the first, we represent the two tar-

5 See Schluter (2018) for a recent exposition of the glass
ceiling in the NLP research community.

Model Agg. Dim.

CBoW (GloVe), 840 billion token web
corpus version (Pennington et al., 2014)

mean 300

InferSent, AllNLI (Conneau et al., 2017) max 4096
GenSen, +STN +Fr +De +NLI +L +STP
+Par (Subramanian et al., 2018)

last 4096

Universal Sentence Encoder (USE),
DAN version (Cer et al., 2018)

N/A 512

ELMo (Peters et al., 2018), sum over
layers after mean-pooling over sequence

mean 1024

GPT (Radford et al., 2018) last 768
BERT, large, cased (Devlin et al., 2018) [CLS] 1024

Table 3: Models tested (disambiguated with notation
from cited paper), aggregation functions applied across
token representations, and representation dimensions.

get concepts by names of women and men, respec-
tively, in the single sentence template “<word> is
an engineer with superior technical skills.”; the at-
tributes are likable and non-hostile terms, based
on Heilman et al.’s design, in the sentence tem-
plate “The engineer is <word>.” In the sec-
ond, we use the shortened target concept sen-
tence template “<word> is an engineer” and fill
the attribute templates from before with competent
and achievement-oriented terms based on Heilman
et al.’s design.6 We refer to these tests as semanti-
cally unbleached because the context contains im-
portant information about the bias. We produce
two variations of these tests: word-level tests in
which target concepts are names in isolation and
attributes are adjectives in isolation, as well as cor-
responding semantically bleached sentence-level
tests. These control conditions allow us to probe
the extent to which observed associations are at-
tributable to gender independent of context.

4 Experiments and Results

We apply SEAT to seven sentence encoders (listed
in Table 3) including simple bag-of-words en-
coders, sentence-to-vector models, and state-of-
the-art sequence models.7 For all models, we use
publicly available pretrained parameters.

Table 4 shows effect size and significance
at 0.01 before and after applying the Holm-
Bonferroni multiple testing correction (Holm,
1979) for a subset of tests and models; com-
plete results are provided in the supplement.8

6 We consider other formulations in the supplement.
7 We provide further details and explore variations on

these model configurations in the supplement.
8 We use the full set of tests and models when comput-



625

Test Context CBoW InferSent GenSen USE ELMo GPT BERT

C1: Flowers/Insects word 1.50∗∗ 1.56∗∗ 1.24∗∗ 1.38∗∗ −0.03 0.20 0.22
C1: Flowers/Insects sent 1.56∗∗ 1.65∗∗ 1.22∗∗ 1.38∗∗ 0.42∗∗ 0.81∗∗ 0.62∗∗

C3: EA/AA Names word 1.41∗∗ 1.33∗∗ 1.32∗∗ 0.52 −0.40 0.60∗ −0.11
C3: EA/AA Names sent 0.52∗∗ 1.07∗∗ 0.97∗∗ 0.32∗ −0.38 0.19 0.05
C6: M/F Names, Career word 1.81∗ 1.78∗ 1.84∗ 0.02 −0.45 0.22 0.21
C6: M/F Names, Career sent 1.74∗∗ 1.69∗∗ 1.63∗∗ 0.83∗∗ −0.38 0.35 0.08
ABW Stereotype word 1.10∗ 1.18∗ 1.57∗∗ −0.39 0.53 0.08 −0.32
ABW Stereotype sent 0.62∗∗ 0.98∗∗ 1.05∗∗ −0.19 0.52∗ −0.07 −0.17
Double Bind: Competent word 1.62∗ 1.09 1.49∗ 1.51∗ −0.35 −0.28 −0.81
Double Bind: Competent sent 0.79∗∗ 0.57∗ 0.83∗∗ 0.25 −0.15 0.10 0.39
Double Bind: Competent sent (u) 0.84 1.42∗ 1.03 0.71 0.20 0.71 1.17∗

Double Bind: Likable word 1.29∗ 0.65 1.31∗ 0.16 −0.60 0.91 −0.55
Double Bind: Likable sent 0.69∗ 0.37 0.25 0.32 −0.45 −0.20 −0.35
Double Bind: Likable sent (u) 0.51 1.33∗ 0.05 0.48 −0.90 −0.87 0.99

Table 4: SEAT effect sizes for select tests, including word-level (word), bleached sentence-level (sent), and un-
bleached sentence-level (sent (u)) versions. CN : test from Caliskan et al. (2017, Table 1) row N ; *: significant at
0.01, **: significant at 0.01 after multiple testing correction.

Specifically, we select Caliskan Test 1 associat-
ing flowers/insects with pleasant/unpleasant, Test
3 associating European/African American names
with pleasant/unpleasant, and Test 6 associating
male/female names with career/family, as well as
the angry black woman stereotype and the com-
petent and likable double bind tests. We observe
that tests based on given names more often find a
significant association than those based on group
terms; we only show the given-name results here.

We find varying evidence of bias in sentence
encoders according to these tests. Bleached
sentence-level tests tend to elicit more significant
associations than word-level tests, while the latter
tend to have larger effect sizes. We find stronger
evidence for the Caliskan and ABW stereotype
tests than for the double bind. After the multi-
ple testing correction, we only find evidence of the
double bind in bleached, sentence-level competent
control tests; that is, we find women are associated
with incompetence independent of context.9

Some patterns in the results cast doubt on
the reasonableness of SEAT as an evaluation.
For instance, Caliskan Test 7 (association be-
tween math/art and male/female) and Test 8 (sci-
ence/art and male/female) elicit counterintuitive
results from several models. These tests have the
same sizes of target concept and attribute sets.
For CBoW on the word versions of those tests,
we see p-values of 0.016 and 10−2, respectively.

ing the multiple testing correction, including those only pre-
sented in the supplement.

9 However, the double bind results differ across models;
we show no significant associations for ELMo or GPT and
only one each for USE and BERT.

On the sentence versions, we see p-values of
10−5 for both tests. Observing similar p-values
agrees with intuition: The math/art association
should be similar to the science/art association
because they instantiate a disciplinary dichotomy
between math/science and arts/language (Nosek
et al., 2002). However, for BERT on the sentence
version, we see discrepant p-values of 10−5 and
0.14; for GenSen, 0.12 and 10−3; and for GPT,
0.89 and 10−4.

Caliskan Tests 3, 4, and 5 elicit even more
counterintuitive results from ELMo. These tests
measure the association between European Amer-
ican/African American and pleasant/unpleasant.
Test 3 has larger attribute sets than Test 4, which
has larger target concept sets than Test 5. Intu-
itively, we expect increasing p-values across Tests
3, 4, and 5, as well-designed target concepts and
attributes of larger sizes should yield higher-power
tests. Indeed, for CBoW, we find increasing p-
values of 10−5, 10−5, and 10−4 on the word ver-
sions of the tests and 10−5, 10−5, and 10−2 on the
sentence versions, respectively.10 However, for
ELMo, we find decreasing p-values of 0.95, 0.45,
and 0.08 on the word versions of the tests and 1,
0.97, and 10−4 on the sentence versions. We inter-
pret these results as ELMo producing substantially
different representations for conceptually similar
words. Thus, SEAT’s assumption that the sentence
representations of each target concept and attribute
instantiate a coherent concept appears invalid.

10 Our SEAT implementation uses sampling with a preci-
sion of 10−5, so 10−5 is the smallest p-value we can observe.



626

5 Conclusion

At face value, our results suggest recent sentence
encoders exhibit less bias than previous models
do, at least when “bias” is considered from a U.S.
perspective and measured using the specific tests
we have designed. However, we strongly caution
against interpreting the number of significant as-
sociations or the average significant effect size as
an absolute measure of bias. Like WEAT, SEAT
only has positive predictive ability: It can detect
presence of bias, but not its absence. Consider-
ing that these representations are trained without
explicit bias control mechanisms on naturally oc-
curring text, we argue against interpreting a lack
of evidence of bias as a lack of bias.

Moreover, the counterintuitive sensitivity of
SEAT on some models and biases suggests that
biases revealed by SEAT may not generalize be-
yond the specific words and sentences in our test
data. That is, our results invalidate the assumption
that each set of words or sentences in our tests rep-
resents a coherent concept/attribute (like African
American or pleasant) to the sentence encoders;
hence, we do not assume the encoders will ex-
hibit similar behavior on other potential elements
of those concepts/attributes (other words or sen-
tences representing, for example, African Ameri-
can or pleasant).

One possible explanation of the observed sen-
sitivity at the sentence level is that, from the sen-
tence encoders’ view, our sentence templates are
not as semantically bleached as we expect; small
variations in their relative frequencies and interac-
tions with the terms inserted into them may be un-
dermining the coherence of the concepts/attributes
they implement. Another possible explanation that
also accounts for the sensitivity observed in the
word-level tests is that cosine similarity is an in-
adequate measure of text similarity for sentence
encoders. If this is the case, the biases revealed by
SEAT may not translate to biases in downstream
applications. Future work could measure bias at
the application level instead, following Bailey and
Deery (2018)’s recommendation based on the ten-
sion between descriptive and normative correct-
ness in representations.

The angry black woman stereotype represents
an intersectional bias, a phenomenon not well an-
ticipated by an additive model of racism and sex-
ism (Crenshaw, 1989). Previous work has mod-
eled biases at the intersection of race and gender in

distributional semantic word representations (Her-
belot et al., 2012), natural language inference
data (Rudinger et al., 2017), and facial recogni-
tion systems (Buolamwini and Gebru, 2018), as
well as at the intersection of dialect and gender in
automatic speech recognition (Tatman, 2017). We
advocate for further consideration of intersection-
ality in future work in order to avoid reproducing
the erasure of multiple minorities who are most
vulnerable to bias.

We have developed a simple sentence-level ex-
tension of an established word embedding bias
instrument and used it to measure the degree
to which pretrained sentence encoders capture a
range of social biases, observing a large number
of significant effects as well as idiosyncrasies sug-
gesting limited external validity. This study is pre-
liminary and leaves open to investigation several
design choices that may impact the results; fu-
ture work may consider revisiting choices like the
use of semantically bleached sentence inputs, the
aggregation applied to models that represent sen-
tences with sequences of hidden states, and the
use of cosine similarity between sentence repre-
sentations. We challenge researchers of fairness
and ethics in NLP to critically (re-)examine their
methods; looking forward, we hope for a deeper
consideration of the social contexts in which NLP
systems are applied.

Acknowledgments

We are grateful to Carolyn Rosé, Jason Phang,
Sébastien Jean, Thibault Févry, Katharina Kann,
and Benjamin Van Durme for helpful conversa-
tions concerning this work and to our reviewers
for their thoughtful feedback.

CM is funded by IARPA MATERIAL; RR is
funded by DARPA AIDA; AW is funded by an
NSF fellowship. The U.S. Government is autho-
rized to reproduce and distribute reprints for Gov-
ernmental purposes. The views and conclusions
contained in this publication are those of the au-
thors and should not be interpreted as represent-
ing official policies or endorsements of IARPA,
DARPA, NSF, or the U.S. Government.

References

Katherine Bailey and Oisin Deery. 2018. How (and
how not!) to control for morally relevant bias in
NLP. The Second ACL Workshop on Ethics in Nat-



627

ural Language Processing, New Orleans, LA. In-
vited Talk.

Sandra L. Bem. 1974. The measurement of psycholog-
ical androgyny. Journal of Consulting and Clinical
Psychology, 42(2):155–162.

Tolga Bolukbasi, Kai-Wei Chang, James Y Zou,
Venkatesh Saligrama, and Adam T Kalai. 2016.
Man is to computer programmer as woman is to
homemaker? Debiasing word embeddings. In D. D.
Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and
R. Garnett, editors, Advances in Neural Information
Processing Systems 29, pages 4349–4357. Curran
Associates, Inc.

Joy Buolamwini and Timnit Gebru. 2018. Gender
shades: Intersectional accuracy disparities in com-
mercial gender classification. In Proceedings of
the 1st Conference on Fairness, Accountability and
Transparency, volume 81 of Proceedings of Ma-
chine Learning Research, pages 77–91, New York,
NY, USA. PMLR.

Aylin Caliskan, Joanna J. Bryson, and Arvind
Narayanan. 2017. Semantics derived automatically
from language corpora contain human-like biases.
Science, 356(6334):183–186.

Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua,
Nicole Limtiaco, Rhomni St. John, Noah Constant,
Mario Guajardo-Cespedes, Steve Yuan, Chris Tar,
Brian Strope, and Ray Kurzweil. 2018. Universal
sentence encoder for english. In Proceedings of the
2018 Conference on Empirical Methods in Natu-
ral Language Processing: System Demonstrations,
pages 169–174, Brussels, Belgium. Association for
Computational Linguistics.

Patricia Hill Collins. 2004. Black Sexual Politics:
African Americans, Gender, and the New Racism.
Routledge, New York.

Alexis Conneau, Douwe Kiela, Holger Schwenk, Loı̈c
Barrault, and Antoine Bordes. 2017. Supervised
learning of universal sentence representations from
natural language inference data. In Proceedings of
the 2017 Conference on Empirical Methods in Nat-
ural Language Processing, pages 670–680, Copen-
hagen, Denmark. Association for Computational
Linguistics.

Kimberle Crenshaw. 1989. Demarginalizing the inter-
section of race and sex: A black feminist critique of
antidiscrimination doctrine, feminist theory, and an-
tiracist politics. Feminist Legal Theory: Readings in
Law and Gender, 1989(1):57–80.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. ArXiv e-prints.

Nikhil Garg, Londa Schiebinger, Dan Jurafsky, and
James Zou. 2018. Word embeddings quantify 100
years of gender and ethnic stereotypes. Proceedings
of the National Academy of Sciences.

Andra Gillespie. 2016. Race, perceptions of feminin-
ity, and the power of the first lady: A comparative
analysis. In Nadia E. Brown and Sarah Allen Ger-
shon, editors, Distinct Identities: Minority Women in
U.S. Politics, chapter 15, pages 223–238. Routledge,
New York.

Anthony G. Greenwald, Debbie E. McGhee, and Jor-
dan L. K. Schwartz. 1998. Measuring individual
differences in implicit cognition: The implicit asso-
ciation test. Journal of Personality and Social Psy-
chology, 74(6):1464–1480.

Anthony G. Greenwald, T. Andrew Poehlman,
Eric Luis Uhlmann, and Mahzarin R. Banaji. 2009.
Understanding and using the implicit association
test: III. Meta-analysis of predictive validity. Jour-
nal of Personality and Social Psychology, 97(1):17–
41.

Melissa V. Harris-Perry. 2011. Sister Citizen: Shame,
Stereotypes, and Black Women in America. Yale
University Press, New Haven, Connecticut.

Madeline E. Heilman, Aaron S. Wallen, Daniella
Fuchs, and Melinda M. Tamkins. 2004. Penalties for
success: Reactions to women who succeed at male
gender-typed tasks. Journal of Applied Psychology,
89(3):416–427.

Aurélie Herbelot, Eva von Redecker, and Johanna
Müller. 2012. Distributional techniques for philo-
sophical enquiry. In Proceedings of the 6th Work-
shop on Language Technology for Cultural Her-
itage, Social Sciences, and Humanities, pages 45–
54, Avignon, France. Association for Computational
Linguistics.

Sture Holm. 1979. A simple sequentially rejective
multiple test procedure. Scandinavian Journal of
Statistics, 6(2):65–70.

bell hooks. 2015. Ain’t I a Woman: Black Women and
Feminism, 2 edition. Routledge, Taylor & Francis
Group, New York.

Svetlana Kiritchenko and Saif Mohammad. 2018. Ex-
amining gender and race bias in two hundred sen-
timent analysis systems. In Proceedings of the
Seventh Joint Conference on Lexical and Com-
putational Semantics, pages 43–53, New Orleans,
Louisiana. Association for Computational Linguis-
tics.

D. Soyini Madison. 2009. Crazy patriotism and an-
gry (post)black women. Communication and Criti-
cal/Cultural Studies, 6(3):321–326.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their composition-
ality. In C. J. C. Burges, L. Bottou, M. Welling,
Z. Ghahramani, and K. Q. Weinberger, editors, Ad-
vances in Neural Information Processing Systems
26, pages 3111–3119. Curran Associates, Inc.

https://doi.org/10.1037/h0036215
https://doi.org/10.1037/h0036215
http://papers.nips.cc/paper/6228-man-is-to-computer-programmer-as-woman-is-to-homemaker-debiasing-word-embeddings.pdf
http://papers.nips.cc/paper/6228-man-is-to-computer-programmer-as-woman-is-to-homemaker-debiasing-word-embeddings.pdf
http://proceedings.mlr.press/v81/buolamwini18a.html
http://proceedings.mlr.press/v81/buolamwini18a.html
http://proceedings.mlr.press/v81/buolamwini18a.html
https://doi.org/10.1126/science.aal4230
https://doi.org/10.1126/science.aal4230
http://www.aclweb.org/anthology/D18-2029
http://www.aclweb.org/anthology/D18-2029
https://www.aclweb.org/anthology/D17-1070
https://www.aclweb.org/anthology/D17-1070
https://www.aclweb.org/anthology/D17-1070
https://chicagounbound.uchicago.edu/uclf/vol1989/iss1/8
https://chicagounbound.uchicago.edu/uclf/vol1989/iss1/8
https://chicagounbound.uchicago.edu/uclf/vol1989/iss1/8
https://chicagounbound.uchicago.edu/uclf/vol1989/iss1/8
http://arxiv.org/abs/1810.04805
http://arxiv.org/abs/1810.04805
http://arxiv.org/abs/1810.04805
https://doi.org/10.1073/pnas.1720347115
https://doi.org/10.1073/pnas.1720347115
https://doi.org/10.1037/0022-3514.74.6.1464
https://doi.org/10.1037/0022-3514.74.6.1464
https://doi.org/10.1037/0022-3514.74.6.1464
https://doi.org/10.1037/a0015575
https://doi.org/10.1037/a0015575
https://doi.org/10.1037/0021-9010.89.3.416
https://doi.org/10.1037/0021-9010.89.3.416
https://doi.org/10.1037/0021-9010.89.3.416
http://www.aclweb.org/anthology/W12-1008
http://www.aclweb.org/anthology/W12-1008
https://www.jstor.org/stable/4615733
https://www.jstor.org/stable/4615733
http://www.aclweb.org/anthology/S18-2005
http://www.aclweb.org/anthology/S18-2005
http://www.aclweb.org/anthology/S18-2005
https://doi.org/10.1080/14791420903063810
https://doi.org/10.1080/14791420903063810
http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf
http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf
http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf


628

Kaye Mitchell. 2012. Raunch versus prude: contempo-
rary sex blogs and erotic memoirs by women. Psy-
chology & Sexuality, 3(1):12–25.

Brian A. Nosek, Mahzarin R. Banaji, and Anthony G.
Greenwald. 2002. Math = male, me = female,
therefore math 6= me. Journal of Personality and
Social Psychology, 83(1):44–59.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 1532–1543, Doha,
Qatar. Association for Computational Linguistics.

Matthew Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word rep-
resentations. In Proceedings of the 2018 Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, Volume 1 (Long Papers), pages
2227–2237, New Orleans, Louisiana. Association
for Computational Linguistics.

Alec Radford, Karthik Narasimhan, Tim Salimans, and
Ilya Sutskever. 2018. Improving language under-
standing by generative pre-training. Online; ac-
cessed November 28, 2018.

Rachel Rudinger, Chandler May, and Benjamin
Van Durme. 2017. Social bias in elicited natural lan-
guage inferences. In Proceedings of the First ACL
Workshop on Ethics in Natural Language Process-
ing, pages 74–79, Valencia, Spain. Association for
Computational Linguistics.

Natalie Schluter. 2018. The glass ceiling in nlp.
In Proceedings of the 2018 Conference on Em-
pirical Methods in Natural Language Processing,
pages 2793–2798, Brussels, Belgium. Association
for Computational Linguistics.

Pamela Stone and Meg Lovejoy. 2004. Fast-track
women and the “choice” to stay home. The ANNALS
of the American Academy of Political and Social Sci-
ence, 596(1):62–83.

Sandeep Subramanian, Adam Trischler, Yoshua Ben-
gio, and Christopher J Pal. 2018. Learning gen-
eral purpose distributed sentence representations via
large scale multi-task learning. In International
Conference on Learning Representations, Vancou-
ver, Canada.

Latanya Sweeney. 2013. Discrimination in online ad
delivery. Communications of the ACM, 56(5):44–
54.

Rachael Tatman. 2017. Gender and dialect bias in
youtube’s automatic captions. In Proceedings of the
First ACL Workshop on Ethics in Natural Language
Processing, pages 53–59, Valencia, Spain. Associa-
tion for Computational Linguistics.

https://doi.org/10.1080/19419899.2011.627692
https://doi.org/10.1080/19419899.2011.627692
https://doi.org/10.1037//0022-3514.83.1.44
https://doi.org/10.1037//0022-3514.83.1.44
http://www.aclweb.org/anthology/D14-1162
http://www.aclweb.org/anthology/D14-1162
http://www.aclweb.org/anthology/N18-1202
http://www.aclweb.org/anthology/N18-1202
https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf
https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf
http://www.aclweb.org/anthology/W17-1609
http://www.aclweb.org/anthology/W17-1609
http://www.aclweb.org/anthology/D18-1301
https://doi.org/10.1177/0002716204268552
https://doi.org/10.1177/0002716204268552
https://openreview.net/forum?id=B18WgG-CZ
https://openreview.net/forum?id=B18WgG-CZ
https://openreview.net/forum?id=B18WgG-CZ
https://doi.org/10.1145/2447976.2447990
https://doi.org/10.1145/2447976.2447990
http://www.aclweb.org/anthology/W17-1606
http://www.aclweb.org/anthology/W17-1606

