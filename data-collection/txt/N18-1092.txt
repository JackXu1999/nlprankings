



















































Deep Generative Model for Joint Alignment and Word Representation


Proceedings of NAACL-HLT 2018, pages 1011–1023
New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics

Deep Generative Model for Joint Alignment and Word Representation

Miguel Rios Wilker Aziz Khalil Sima’an
Institute for Logic, Language, and Computation

University of Amsterdam
{m.riosgaona, w.aziz, k.simaan}@uva.nl

Abstract
This work exploits translation data as a source
of semantically relevant learning signal for
models of word representation. In particular,
we exploit equivalence through translation as a
form of distributional context and jointly learn
how to embed and align with a deep gener-
ative model. Our EMBEDALIGN model em-
beds words in their complete observed context
and learns by marginalisation of latent lexical
alignments. Besides, it embeds words as pos-
terior probability densities, rather than point
estimates, which allows us to compare words
in context using a measure of overlap between
distributions (e.g. KL divergence). We inves-
tigate our model’s performance on a range of
lexical semantics tasks achieving competitive
results on several standard benchmarks includ-
ing natural language inference, paraphrasing,
and text similarity.

1 Introduction

Natural language processing applications often
count on the availability of word representations
trained on large textual data as a means to alleviate
problems such as data sparsity and lack of linguis-
tic resources (Collobert et al., 2011; Socher et al.,
2011; Tu et al., 2017; Bowman et al., 2015).

Traditional approaches to inducing word repre-
sentations circumvent the need for explicit seman-
tic annotation by capitalising on some form of in-
direct semantic supervision. A typical example is
to fit a binary classifier to detect whether or not a
target word is likely to co-occur with neighbour-
ing words (Mikolov et al., 2013). If the binary
classifier represents a word as a continuous vector,
that vector will be trained to be discriminative of
the contexts it co-occurs with, and thus words in
similar contexts will have similar representations.

Code available from https://github.com/
uva-slpl/embedalign

MR and WA contributed equally.

The underlying assumption is that context (e.g.
neighbouring words) stands for the meaning of the
target word (Harris, 1954; Firth, 1957). The suc-
cess of this distributional hypothesis hinges on the
definition of context and different models are based
on different definitions. Importantly, the nature
of the context determines the range of linguistic
properties the representations may capture (Levy
and Goldberg, 2014b). For example, Levy and
Goldberg (2014a) propose to use syntactic context
derived from dependency parses. They show that
their representations are much more discriminative
of syntactic function than models based on imme-
diate neighbourhood (Mikolov et al., 2013).

In this work, we take lexical translation as indi-
rect semantic supervision (Diab and Resnik, 2002).
Effectively we make two assumptions. First, that
every word has a foreign equivalent that stands for
its meaning. Second, that we can find this equiva-
lent in translation data through lexical alignments.1

For that we induce both a latent mapping between
words in a bilingual sentence pair and distributions
over latent word representations.

To summarise our contributions:

• we model a joint distribution over sentence
pairs that generates data from latent word rep-
resentations and latent lexical alignments;

• we embed words in context mining positive
correlations from translation data;

• we find that foreign observations are necessary
for generative training, but test time predic-
tions can be made monolingually;

• we apply our model to a range of semantic
natural language processing tasks showing its
usefulness.

1These assumptions are not new to the community, but in
this work they lead to a novel model which reaches more appli-
cations. §4 expands on the relation to other uses of bilingual
data for word representation.

1011



x

y

z

a

θ

m

n

|B|

Figure 1: A sequence xm1 is generated conditioned
on a sequence of random embeddings zm1 ; generating
the foreign sequence yn1 further requires latent lexical
alignments an1 .

2 EMBEDALIGN

In a nutshell, we model a distribution over pairs of
sentences expressed in two languages, namely, a
language of interest L1, and an auxiliary language
L2 which our model uses to mine some learning
signal. Our model, EMBEDALIGN, is governed by
a simple generative story:

1. sample a length m for a sentence in L1 and a
length n for a sentence in L2;

2. generate a sequence z1, . . . , zm of d-
dimensional random embeddings by sampling
independently from a standard Gaussian prior;

3. generate a word observation xi in the vocabu-
lary of L1 conditioned on the random embed-
ding zi;

4. generate a sequence ai, . . . , an of n random
alignments—each maps from a position aj in
xm1 to a position j in the L2 sentence;

5. finally, generate an observation yj in the vo-
cabulary of L2 conditioned on the random
embedding zaj that stands for xaj .

The model is parameterised by neural networks
and parameters are estimated to maximise a lower-
bound on log-likelihood of joint observations. In
the following, we present the model formally
(§2.1), discuss efficient training (§2.2), and con-
crete architectures (§2.3).

2.1 Probabilistic model

Notation We use block capitals (e.g. X) for ran-
dom variables, lowercase letters (e.g. x) for as-
signments, and the shorthand Xm1 for a sequence
X1, . . . , Xm. Boldface letters are reserved for de-
terministic vectors (e.g. v) and matrices (e.g. W).

Finally, E[f(Z);α] denotes the expected value of
f(z) under a density q(z|α).

We model a joint distribution over bilingual par-
allel data, i.e., L1–L2 sentence pairs. An obser-
vation is a pair of random sequences 〈Xm1 , Y n1 〉,
where a random variable X (Y ) takes on values in
the vocabulary of L1 (L2). For ease of exposition,
the length m (n) of each sequence is assumed ob-
served throughout. The L1 sentence is generated
one word at a time from a random sequence of la-
tent embeddings Zm1 , each Z taking on values in
Rd. The L2 sentence is generated one word at a
time given a random sequence of latent alignments
An1 , where Aj ∈ {1, . . . ,m} is the position in the
L1 sentence to which yj aligns.2

For i ∈ {1, . . . ,m} and j ∈ {1, . . . , n} the
generative story is

Zi ∼ N (0, I) (1a)
Xi|zi ∼ Cat(f(zi; θ)) (1b)
Aj |m ∼ U(1/m) (1c)

Yj |zm1 , aj ∼ Cat(g(zaj ; θ)) (1d)

and Figure 1 is a graphical depiction of our model.
We map from latent embeddings to categorical dis-
tributions over either vocabulary using a neural
network whose parameters are deterministic and
collectively denote by θ (the generative parame-
ters). The marginal likelihood of a sentence pair is
shown in Equation (2).

Pθ(x
m
1 , y

n
1 |m,n) =

∫
p(zm1 )

m∏

i=1

Pθ(xi|zi)

×
n∏

j=1

m∑

aj=1

P (aj |m)Pθ(yj |zaj )dzm1
(2)

Due to the conditional independences of our
model, it is trivial to marginalise lexical alignments
for any given latent embeddings zm1 , but marginalis-
ing the embeddings themselves is intractable. Thus,
we employ amortised mean field variational infer-
ence using the inference model

qφ(z
m
1 |xm1 ) ,

m∏

i=1

N (zi|ui, diag(si � si)) (3)

where each factor is a diagonal Gaussian. We map
from xm1 to a sequence u

m
1 of independent posterior

2We pad L1 sentences with NULL to account for untrans-
latable L2 words (Brown et al., 1993). Instead, Schulz et al.
(2016) generate untranslatable words from L2 context—an
alternative we leave for future work.

1012



mean (or location) vectors, where ui , µ(hi;φ),
as well as a sequence sm1 of independent standard
deviation (or scale) vectors, where si , σ(hi;φ),
and hm1 = enc(x

m
1 ;φ) is a deterministic encod-

ing of the L1 sequence (we discuss concrete ar-
chitectures in §2.3). All mappings are realised by
neural networks whose parameters are collectively
denoted by φ (the variational parameters). Note
that we choose to approximate the posterior with-
out conditioning on yn1 . This allows us to use the
inference model for monolingual prediction in ab-
sence of L2 data.

Variational φ and generative θ parameters are
jointly point-estimated to attain a local optimum of
the evidence lowerbound (Jordan et al., 1999):

logPθ(x
m
1 , y

n
1 |m,n) ≥

m∑

i=1

E [logPθ(xi|Zi);ui, si]

+

n∑

j=1

E


log

m∑

aj=1

P (aj |m)Pθ(yj |Zaj );um1 , sm1




−
m∑

i=1

KL [N (ui,diag(si � si))||N (0, I)] .

(4)
The variational family is location-scale, thus we
can rely on stochastic optimisation (Robbins and
Monro, 1951) and automatic differentiation (Bay-
din et al., 2015) with reparameterised gradient esti-
mates (Kingma and Welling, 2014; Rezende et al.,
2014; Titsias and Lázaro-Gredilla, 2014). More-
over, because the Gaussian density is an exponen-
tial family, the KL terms in (4) are available in
closed-form (Kingma and Welling, 2014, Appendix
B).

2.2 Efficient training
The likelihood terms in the ELBO (4) require eval-
uating two softmax layers over rather large vo-
cabularies. This makes training prohibitively slow
and calls for efficient approximation. We employ
an approximation proposed by Botev et al. (2017)
termed complementary sum sampling (CSS), which
we review in this section.

Consider the likelihood term logP (X = x|z)
that scores an observation x given a sampled em-
bedding z—we use serif font x to distinguish a par-
ticular observation from an arbitrary event x ∈ X
in the support. The exact class probability

P (X = x|z) = exp(u(z, x))∑
x∈X exp(u(z, x))

(5)

requires a normalisation over the complete support.
CSS works by splitting the support into two sets,
a set C that is explicitly summed over and must
include the positive class x, and another set N that
is a subset of the complement set X \C. We obtain
an estimate for the normaliser
∑

x∈C
exp(u(z, x)) +

∑

x∈N
κ(x) exp(u(z, x)) (6)

by importance- or Bernoulli-sampling from the sup-
port using a proposal distribution Q(X), where
κ(x) corrects for bias asN tends to the entire com-
plement set. In this paper, we design C and N
per training mini-batch: we take C to consist of all
unique words in a mini-batch of training samples
and N to consist of 103 negative classes uniformly
sampled from the complement set X \ C, in which
case κ(x) = 10−3|X \ C|.3

CSS makes it particularly easy to approximate
likelihood terms such as those with respect to L2
in Equation (4). Because those terms depend on
a marginalisation over alignments, an approxima-
tion must give support to all words in the sequence
yn1 . With CSS this is extremely simple, we just
need to make sure all unique words in yn1 are in the
set C—which our mini-batch procedure does guar-
antee. Botev et al. (2017) show that CSS is rather
stable and superior to the most popular softmax ap-
proximations. Besides being simple to implement,
CSS also addresses a few problems with other ap-
proximations. To name a few: unlike importance
sampling approximations, CSS converges to the
exact softmax with bounded computation (it takes
as many samples as there are classes). Unlike hier-
archical softmax, CSS only affects training, that is,
at test time we simply use the entire support instead
of the approximation.

Without a softmax approximation, inference
for our model would take time proportional to
O(m× vx +m× vy +m× n) where vx (vy) cor-
responds to the size of the vocabulary of L1 (L2).
The first term (m× vx) corresponds to projecting
from m latent embeddings to m categorical dis-
tributions over the vocabulary of L1. The second
term (m× vy) corresponds to projecting the same
m latent embeddings to m categorical distributions
over the vocabulary of L2. Finally, the third term
(m × n) is due to marginalisation of alignments.

3We sample uniformly from the complement set until we
have 103 unique classes. We realise this operation outside
the computation graph providing C and N as inputs to each
training iteration, but a GPU-based solution is also possible.

1013



Note, however, that with the CSS approximation
we drop the dependency on vocabulary sizes (as
the combined sizes of C and N is an independent
constant). Moreover, if inference is performed on
GPU, the squared term (m×n ≈ m2) is amortised
due to parallelism. Thus, while training our model
is somewhat slower than monolingual models of
word representation, which typically run in O(m),
it is not at all impracticably slower.

2.3 Architectures
Here we present the neural network architectures
that parameterise the different generative and vari-
ational components of §2.1. Refer to Appendix B
for an illustration.

Generative model We have two generative com-
ponents, namely, a categorical distribution over the
vocabulary of L1 and another over the vocabulary
of L2. We predict the parameter (event probabili-
ties) of each distribution with an affine transforma-
tion of a latent embedding followed by the softmax
nonlinearity to ensure normalisation:

f(zi; θ) = softmax (W1zi + b1) (7a)

g(zaj ; θ) = softmax
(
W2zaj + b2

)
(7b)

where W1 ∈ Rvx×d, b1 ∈ Rvx , W2 ∈ Rvy×d,
b2 ∈ Rvy , and vx (vy) is the size of the vocab-
ulary of L1 (L2). With the approximation of
§2.2, we replace the L1 softmax layer (7a) by
exp
(
z>i cx + bx

)
normalised by the CSS estimate

(6) at training, and similarly for the L2 softmax
layer (7b). In that case, we have parameters for
cx, cy ∈ Rd—deterministic embeddings for x and
y, respectively—as well as bias terms bx, by ∈ R.
Inference model We predict approximate poste-
rior parameters using two independent transforma-
tions

ui = M1hi + d1 (8a)

si = softplus(M2hi + d2) (8b)

of a shared representation hi ∈ Rdx of the ith word
in the L1 sequence xm1 —where M1,M2 ∈ Rd×dx
are projection matrices, d1,d2 ∈ Rd are bias vec-
tors, and the softplus nonlinearity ensures that stan-
dard deviations are non-negative. To obtain the
deterministic encoding hm1 , we employ two dif-
ferent architectures: (1) a bag-of-words (BOW)
encoder, where hi is a deterministic projection of
xi onto Rdx ; and (2) a bidirectional (BIRNN) en-
coder, where hi is the element-wise sum of two

LSTM hidden states (ith step) that process the se-
quence in opposite directions. We use 128 units
for deterministic embeddings, and 100 units for
LSTMs (Hochreiter and Schmidhuber, 1997) and
latent representations (i.e. d = 100).

3 Experiments

We start the section describing the data used to
estimate our model’s parameters as well as details
about the optimiser. The remainder of the section
presents results on various benchmarks.

Training data We train our model on bilingual
parallel data. In particular, we use parliament
proceedings (Europarl-v7) (Koehn, 2005) from
two language pairs: English-French and English-
German.4 We employed very minimal preprocess-
ing, namely, tokenisation and lowercasing using
scripts from MOSES (Koehn et al., 2007), and have
discarded sentences longer than 50 tokens. Table 1
lists more information about the training data, in-
cluding the English-French Giga web corpus (Bojar
et al., 2014) which we use in §3.4.5

Corpus Sentence pairs Tokens

Europarl EN-FR 1.7 42.5
Europarl EN-DE 1.7 43.5
Giga EN-FR 18.3 419.6

Table 1: Training data size (in millions).

Optimiser For all architectures, we use the
Adam optimiser (Kingma and Ba, 2014) with a
learning rate of 10−3. Except where explicitly in-
dicated, we

• train our models for 30 epochs using mini
batches of 100 sentence pairs;

• use validation alignment error rate for model
selection;

• train every model 10 times with random Glo-
rot initialisation (Glorot and Bengio, 2010)
and report mean and standard deviation;

• anneal the KL terms using the following
schedule: we use a scalar α from 0 to 1 with
additive steps of size 10−3 every 500 updates.

4The proposed model is not limited to these language pairs.
5As we investigate various configurations and train every

model 10 times to inspect variance in results, we conduct most
of the experiments on the more manageable Europarl.

1014



This means that at the beginning of the train-
ing, we allow the model to overfit to the like-
lihood terms, but towards the end we are opti-
mising the true ELBO (Bowman et al., 2016).

It is also important to highlight that we do not
employ regularisation techniques (such as batch
normalisation, dropout, or L2 penalty) for they did
not seem to yield consistent results.

3.1 Word alignment

Since our model leverages learning signal from
parallel data by marginalising latent lexical align-
ments, we use alignment error rate to double check
whether the model learns sensible word correspon-
dences. Intrinsic assessment of word alignment
quality requires manual annotation. For English-
French, we use the NAACL English-French hand-
aligned data (37 sentence pairs for validation and
447 for test) (Mihalcea and Pedersen, 2003). For
English-German, we use the data by Padó and La-
pata (2006) (98 sentence pairs for validation and
987 for test). Alignment quality is then measured
in terms of alignment error rate (AER) (Och and
Ney, 2000)—an F-measure over predicted align-
ment links. For prediction we condition on the
posterior means E[Zm1 ] which is just the predicted
variational means um1 and select the L1 position
for which P (yj , aj |um1 ) is maximum (a form of
approximate Viterbi alignment).

Model L1 accuracy L2 accuracy ↓AER
BOW 95.59± 2.22 5.69± 2.07 35.41± 1.16
BOWα 99.87± 0.22 6.16± 0.39 30.94± 2.49
BIRNN 95.72± 1.28 7.31± 0.64 34.32± 1.08
BIRNNα 99.97± 0.09 7.25± 0.62 29.18± 1.91

Table 2: English-French validation ↑accuracy and ↓
AER results.

Model L1 accuracy L2 accuracy ↓AER
BOW 93.51± 0.56 10.09± 0.20 53.66± 0.36
BOWα 97.72± 2.28 9.71± 0.63 52.81± 1.47
BIRNN 99.78± 0.18 8.63± 0.35 55.55± 0.67
BIRNNα 99.96± 0.05 8.32± 0.29 52.32± 1.77

Table 3: English-German validation ↑accuracy and
↓AER results.

We start by analysing validation results and se-
lecting amongst a few variants of EMBEDALIGN.
We investigate the use of annealing and the use of a

bidirectional encoder in the variational approxima-
tion. Table 2 (3) lists ↓AER for EN-FR (EN-DE)
as well as accuracy of word prediction. It is clear
that both annealing (systems decorated with sub-
script α) and bidirectional representations improve
the results across the board. In the rest of the pa-
per we still investigate whether or not recurrent
encoders help, but we always report results based
on annealing.

In order to establish baselines for our mod-
els we report IBM models 1 and 2 (Brown
et al., 1993). In a nutshell, IBM models 1 and
2 both estimate the conditional P (yj |xm1 ) =∑m

aj=1
P (aj |m)P (yj |xaj ) by marginalisation of

latent lexical alignments. The only difference be-
tween the two models is the prior over alignments,
which is uniform for IBM1 and categorical for
IBM2. An important difference between IBM mod-
els and EMBEDALIGN concerns the lexical distri-
bution. IBM models are parameterised with inde-
pendent categorical parameters, while our model
instead is parameterised by a neural network. IBM
models condition on a single categorical event xaj ,
namely, the word aligned to. Our model instead
conditions on the latent embedding zaj that stands
for the word aligned to.

In order to establish even stronger conditional
alignment models, we embed the conditioning
words and replace IBM1’s independent parame-
ters by a neural network (single hidden layer MLP).
We call this model a neural IBM1 (or NIBM for
short). Note that in an IBM model, the sequence
xm1 is never modelled, therefore we can condition
on it without restrictions. For that reason, we also
experiment with a bidirectional LSTM encoder and
condition lexical distributions on its hidden states.

Model En-Fr En-De

IBM1 32.45 46.71
IBM2 22.61 40.11
NIBMBoW 27.35± 0.19 46.22± 0.07
NIBMBiRNN 25.57± 0.40 43.37± 0.11
EMBALIGNBoW 30.97± 2.53 49.46± 1.72
EMBALIGNBiRNN 29.43± 1.84 48.09± 2.12

Table 4: Test ↓AER.

Table 4 shows AER for test predictions. First ob-
serve that neural models outperform classic IBM1
by far, some of them even approach IBM2’s perfor-
mance. Next, observe that bidirectional encodings
make NIBM much stronger at inducing good word-

1015



to-word correspondences. EMBEDALIGN cannot
catch up with NIBM, but that is not necessarily
surprising. Note that NIBM is a conditional model,
thus it can use all of its capacity to better explain L2
data. EMBEDALIGN, on the other hand, has to find
a compromise between generating both streams of
the data. To make that point a bit more obvious,
Table 5 (6) lists accuracy of word prediction for
EN-FR (EN-DE). Note that, without sacrificing L2
accuracy, and sometimes even improving it, EMBE-
DALIGN achieves very high L1 accuracy. This still
does not imply that induced representations have
captured aspects of lexical semantics such as word
senses. All this means is that we have induced fea-
tures that are jointly good at reconstructing both
streams of the data one word at time. Of course it
is tempting to conclude that our models must be
capturing some useful generalisations. For that, the
next sections will investigate a range of semantic
NLP tasks.

Model L1 accuracy L2 accuracy

NIBMBoW - 7.21± 0.16
NIBMBiRNN - 6.47± 0.45
EMBALIGNBoW 98.90± 0.41 7.08± 0.34
EMBALIGNBiRNN 99.21± 0.18 7.44± 0.61

Table 5: English-French ↑accuracy in test set

Model L1 accuracy L2 accuracy

NIBMBoW - 7.94± 0.03
NIBMBiRNN - 8.38± 0.10
EMBALIGNBoW 96.86± 2.89 8.72± 0.39
EMBALIGNBiRNN 99.32± 0.34 8.00± 0.12

Table 6: English-German ↑accuracy in test set

3.2 Lexical substitution task

The English lexical substitution task (LST) consists
in selecting a substitute word for a target word in
context (McCarthy and Navigli, 2009). In the most
traditional variant of the task, systems are presented
with a list of potential candidates and this list must
be sorted by relatedness.

Dataset The LST dataset includes 201 target
words present in 10 sentences/contexts each, along
with a manually annotated list of potential replace-
ments. The data are split in 300 instances for vali-
dation and 1, 710 for test. Systems are evaluated by

Model cos KL

RANDOM 30.0 -
SKIPGRAM 44.9 -
BSG - 46.1
ENBoW 29.75± 0.55 27.93± 0.25
ENBiRNN 21.31± 1.05 27.64± 0.40
EN-FRBoW 42.72± 0.36 41.90± 0.35
EN-FRBiRNN 42.19± 0.57 41.61± 0.55
EN-DEBoW 41.90± 0.58 40.63± 0.55
EN-DEBiRNN 42.07± 0.47 40.93± 0.59

Table 7: English ↑GAP on LST test data.

comparing the predicted ranking to the manual one
in terms of generalised average precision (GAP)
(Melamud et al., 2015).

Prediction We use EMBEDALIGN to encode
each candidate (in context) as a posterior Gaus-
sian density. Note that this task dispenses with
inferences about L2. Each candidate is compared
to the target word in context through a measure of
overlap between their inferred densities—we take
KL divergence. We then rank candidates using this
measure.

Table 7 lists GAP scores for variants of EM-
BEDALIN (bottom section) as well as some base-
lines and other established methods (top section).
For comparison, we also compute GAP by sorting
candidates in terms of cosine similarity, in which
case we take the Gaussian mean as a summary
of the density. The top section of the table con-
tains systems reported by Melamud et al. (2015)
(RANDOM and SKIPGRAM) and by Brazinskas
et al. (2017) (BSG). Note that both SKIPGRAM
(Mikolov et al., 2013) and BSG were trained on
the very large ukWaC English corpus (Ferraresi
et al., 2008). SKIPGRAM is known to perform
remarkably well regardless of its apparent insen-
sitivity to context (in terms of design). BSG is a
close relative of our model which gives SKIPGRAM
a Bayesian treatment (also by means of amortised
variational inference) and is by design sensitive to
context in a manner similar to EMBEDALIGN, that
is, through its inferred posteriors.

Our first observation is that cosine seems to out-
perform KL slightly. Others have shown that KL
can be used to predict directional entailment (Vil-
nis and McCallum, 2014; Brazinskas et al., 2017),
since LST is closer to paraphrasing than to entail-
ment directionality may be a distractor, but we

1016



Model MR CR SUBJ MPQA SST TREC MRPC SICK-R SICK-E SST14

W2VEC 77.7 79.8 90.9 88.3 79.7 83.6 72.5/81.4 0.80 78.7 0.65/0.64
NMT 64.7 70.1 84.9 81.5 - 82.8 -/- - - 0.43/0.42
EN 57.6 66.2 70.9 71.8 58.0 62.9 70.3/80.1 0.62 73.7 0.54/0.55
EN-FR 63.5 71.5 78.9 82.3 65.1 62.1 71.4/80.5 0.69 75.9 0.69/0.59
EN-DE 64.0 68.9 77.9 81.8 65.1 59.5 71.2/80.5 0.69 74.8 0.62/0.61
COMBO 66.7 73.1 82.4 84.8 69.2 67.7 71.8/80.7 0.73 77.4 0.62/0.61

Table 8: English sentence evaluation results: the last four rows correspond to the mean of 10 runs with EMBE-
DALIGN models. All models, but W2VEC, employ bidirectional encoders.

leave it as a rather speculative point. One addi-
tional point worth highlighting: the middle section
of Table 7. ENBoW and ENBiRNN show what hap-
pens when we do not give EMBEDALIGN L2 su-
pervision at training. That is, imagine the model
of Figure 1 without the bottom plate. In that case,
the model representations overfit for L1 word-by-
word prediction. Without the need to predict any
notion of context (monolingual or otherwise), the
representations drift away from semantic-driven
generalisations and fail at lexical substitution.

3.3 Sentence Evaluation
Conneau et al. (2017) developed a framework to
evaluate unsupervised sentence level representa-
tions trained on large amounts of data on a range of
supervised NLP tasks. We assess our induced repre-
sentations using their framework on the following
benchmarks evaluated on classification ↑accuracy
(MRPC is further evaluated on ↑F1)
MR classification of positive or negative movie

reviews;

SST fined-grained labelling of movie reviews from
the Stanford sentiment treebank (SST);

TREC classification of questions into k-classes;
CR classification of positive or negative product

reviews;

SUBJ classification of a sentence into subjective
or objective;

MPQA classification of opinion polarity;
SICK-E textual entailment classification;
MRPC paraphrase identification in the Microsoft

paraphrase corpus;

as well as the following benchmarks evaluated on
the indicated correlation metric(s)

SICK-R semantic relatedness between two sen-
tences (↑Pearson);

SST-14 semantic textual similarity
(↑Pearson/Spearman).

Prediction We use EMBEDALIGN to annotate
every word in the training set of the benchmarks
above with the posterior mean embedding in con-
text. We then average embeddings in a sentence
and give that as features to a logistic regression
classifier trained with 5-fold cross validation.6

For comparison, we report a SKIPGRAM model
(here indicated as W2VEC) as well as a model that
uses the encoder of a neural machine translation
system (NMT) trained on English-French Europarl
data. In both cases, we report results by Conneau
et al. (2017). Table 8 shows the results for all bench-
marks.7 We report EMBEDALIGN trained on either
EN-FR or EN-DE. The last line (COMBO) shows
what happens if we train logistic regression on the
concatenation of embeddings inferred by both EM-
BEDALIGN models, that is, EN-FR and EN-DE.
Note that these two systems perform sometimes
better sometimes worse depending on the bench-
mark. There is no clear pattern, but differences may
well come from some qualitative difference in the
induced latent space. It is a known fact that differ-
ent languages realise lexical ambiguities differently,
thus representations induced towards different lan-
guages are likely to capture different generalisa-
tions.8 As COMBO results show, the representa-
tions induced from different corpora are somewhat
complementary. That same observation has guided
paraphrasing models based on pivoting (Bannard
and Callison-Burch, 2005). Once more we report a
monolingual variant of EMBEDALIGN (indicated
by EN) in an attempt to illustrate how crucial the

6http://scikit-learn.org/stable/
7In Appendix A we provide bar plots marked with error

bars (2 standard deviations).
8We also acknowledge that our treatment of German is

likely suboptimal due to the lack of subword features, as it
can also be seen in AER results.

1017



translation signal is.

3.4 Word similarity

Word similarity benchmarks are composed of word
pairs which are manually ranked out of context. For
completeness, we also tried evaluating our embed-
dings in such benchmarks despite our work being
focussed on applications where context matters.

Prediction To assign an embedding for a word
type, we infer Gaussian posteriors for all training
instances of that type in context and aggregate the
posterior means through an average (effectively
collapsing all instances).

To cover the vocabulary of the typical bench-
mark, we have to use a much larger bilingual col-
lection than Europarl. Based on the results of §3.1,
we decided to proceed with English-French only—
recall that models based on that pair performed
better in terms of AER. Results in this section are
based on EMBEDALIGN (with bidirectional vari-
ational encoder) trained on the Giga web corpus
(see Table 1 for statistics). Due to the scale of the
experiment, we report on a single run.

We trained on Giga with the same hyperparam-
eters that we trained on Europarl, however, for 3
epochs instead of 30 (with this dataset an epoch
amounts to 183, 000 updates). Again, we per-
formed model selection on AER. Table 9 shows
the results for several datasets using the framework
of Faruqui and Dyer (2014a). Note that EMBE-
DALIGN was designed to make use of context in-
formation, thus this evaluation setup is a bit un-
natural for our model. Still, it outperforms SKIP-
GRAM on 5 out of 13 benchmarks, in particular, on
SIMLEX-999, whose relevance has been argued by
Upadhyay et al. (2016). We also remark that this
model achieves 0.25 test AER and 45.16 test GAP
on lexical substitution—a considerable improve-
ment compared to models trained on Europarl and
reported in Tables 4 (AER) and 7 (GAP).

4 Related work

Our model is inspired by lexical alignment mod-
els such as IBM1 (Brown et al., 1993), however,
we generate words yn1 from a latent vector repre-
sentation zm1 of x

m
1 , rather than directly from the

observation xm1 . IBM1 takes L1 sequences as con-
ditioning context and does not model their distribu-
tion. Instead, we propose a joint model, where L1
sentences are generated from latent embeddings.

Dataset SKIPGRAM EMBEDALIGN

MTurk-771 0.5679 0.5229
SIMLEX-999 0.3131 0.3887
WS-353-ALL 0.6392 0.3968
YP-130 0.3992 0.4784
VERB-143 0.2728 0.4593
MEN-TR-3k 0.6462 0.4191
SimVerb-3500 0.2172 0.3539
RG-65 0.5384 0.6389
WS-353-SIM 0.6962 0.4509
RW-STANFORD 0.3878 0.3278
WS-353-REL 0.6094 0.3494
MC-30 0.6258 0.5559
MTurk-287 0.6698 0.3965

Table 9: Evaluation of English word embeddings out of
context in terms of Spearman’s rank correlation coeffi-
cient (↑). The first column is from (Faruqui and Dyer,
2014a).

There is a vast literature on exploiting multilin-
gual context to strengthen the notion of synonymy
captured by monolingual models. Roughly, the lit-
erature splits into two groups, namely, approaches
that derive additional features and/or training objec-
tives based on pre-trained alignments (Klementiev
et al., 2012; Faruqui and Dyer, 2014b; Luong et al.,
2015; Šuster et al., 2016), and approaches that pro-
mote a joint embedding space by working with
sentence level representations that dispense with
explicit alignments (Hermann and Blunsom, 2014;
AP et al., 2014; Gouws et al., 2015; Hill et al.,
2014).

The work of Kočiský et al. (2014) is closer
to ours in that they also learn embeddings by
marginalising alignments, however, their model
is conditional—much like IBM models—and their
embeddings are not part of the probabilistic model,
but rather part of the architecture design. The joint
formulation allows our latent embeddings to har-
vest learning signal from L2while still being driven
by the learning signal from L1—in a conditional
model the representations can become specific to
alignment deviating from the purpose of well rep-
resenting the original language. In §3 we show
substantial evidence that our model performs better
when using both learning signals.

Vilnis and McCallum (2014) first propose to map
words into Gaussian densities instead of point esti-
mates for better word representation. For example,
a distribution can capture asymmetric relations that

1018



a point estimate cannot. Brazinskas et al. (2017)
recast the skip-gram model as a conditional varia-
tional auto-encoder. They induce a Gaussian den-
sity for each occurrence of a word in context, and
for that their model is the closest to ours. Addi-
tionally, they estimate a Gaussian prior per word
type thus representing both types and occurrences.
Unlike our model, the Bayesian skip-gram is not
trained generatively by reconstructing the data, but
rather discriminatively by prediction of overlapping
sets of neighbouring words.

5 Discussion

We have presented a generative model of word
representation that learns from positive correlations
implicitly expressed in translation data. In order
to make these correlations surface, we induce and
marginalise latent lexical alignments.

Embedding models such as CBOW and skip-
gram (Mikolov et al., 2013) are essentially speak-
ing supervised classifiers. This means they depend
on somewhat artificial strategies to derive labelled
data from monolingual corpora—words far from
the central word still have co-occurred with it even
though they are taken as negative evidence. Train-
ing our proposed model does not require a heuris-
tic notion of negative training data. However, the
model is also based on a somewhat artificial as-
sumption: L1 words do not necessarily need to
have an L2 equivalent and, even when they do, this
equivalent need not be realised as a single word.

We have shown with extensive experiments that
our model can induce representations useful to sev-
eral tasks including but not limited to alignment
(the task it most obviously relates to). We observed
interesting results on semantic natural language
processing benchmarks such as natural language
inference, lexical substitution, paraphrasing, and
sentiment classification.

We are currently expanding the notion of dis-
tributional context to multiple auxiliary foreign
languages at once. This seems to only require
minor changes to the generative story and could
increase the model’s disambiguation power dra-
matically. Another direction worth exploring is
to extend the model’s hierarchy with respect to
how parallel sentences are generated. For exam-
ple, modelling sentence level latent variables may
capture global constraints and expose additional
correlations to the model.

Acknowledgments

We thank Philip Schulz for comments on an ear-
lier version of this paper as well as the anonymous
NAACL reviewers. One of the Titan Xp cards used
for this research was donated by the NVIDIA Cor-
poration. This work was supported by the Dutch
Organization for Scientific Research (NWO) VICI
Grant nr. 277-89-002.

References

Sarath Chandar AP, Stanislas Lauly, Hugo
Larochelle, Mitesh Khapra, Balaraman Ravin-
dran, Vikas C Raykar, and Amrita Saha. 2014.
An autoencoder approach to learning bilingual
word representations. In Advances in Neural
Information Processing Systems. pages 1853–
1861.

Colin Bannard and Chris Callison-Burch. 2005.
Paraphrasing with bilingual parallel corpora.
In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Lin-
guistics. Association for Computational Lin-
guistics, Stroudsburg, PA, USA, ACL ’05,
pages 597–604. https://doi.org/10.
3115/1219840.1219914.

Atilim Gunes Baydin, Barak A Pearlmutter,
Alexey Andreyevich Radul, and Jeffrey Mark
Siskind. 2015. Automatic differentiation in
machine learning: a survey. arXiv preprint
arXiv:1502.05767 .

Ondrej Bojar, Christian Buck, Christian Feder-
mann, Barry Haddow, Philipp Koehn, Jo-
hannes Leveling, Christof Monz, Pavel Pecina,
Matt Post, Herve Saint-Amand, Radu Sori-
cut, Lucia Specia, and Aleš Tamchyna. 2014.
Findings of the 2014 workshop on statis-
tical machine translation. In Proceedings
of the Ninth Workshop on Statistical Ma-
chine Translation. Association for Computa-
tional Linguistics, Baltimore, Maryland, USA,
pages 12–58. http://www.aclweb.
org/anthology/W/W14/W14-3302.

Aleksandar Botev, Bowen Zheng, and David Bar-
ber. 2017. Complementary sum sampling for
likelihood approximation in large scale classi-
fication. In Artificial Intelligence and Statis-
tics. pages 1030–1038.

1019



Samuel R. Bowman, Gabor Angeli, Christopher
Potts, and Christopher D. Manning. 2015. A
large annotated corpus for learning natural
language inference. In Proceedings of the
2015 Conference on Empirical Methods in
Natural Language Processing. Association for
Computational Linguistics, Lisbon, Portugal,
pages 632–642. http://aclweb.org/
anthology/D15-1075.

Samuel R. Bowman, Luke Vilnis, Oriol Vinyals,
Andrew M. Dai, Rafal Józefowicz, and Samy
Bengio. 2016. Generating sentences from a
continuous space. In Proceedings of the 20th
SIGNLL Conference on Computational Natu-
ral Language Learning, CoNLL 2016, Berlin,
Germany, August 11-12, 2016. pages 10–21.

Arthur Brazinskas, Serhii Havrylov, and Ivan Titov.
2017. Embedding words as distributions
with a bayesian skip-gram model. Arxiv:
1711.11027 .

Peter F. Brown, Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine trans-
lation: parameter estimation. Computational
Linguistics 19(2):263–311.

Ronan Collobert, Jason Weston, Léon Bottou,
Michael Karlen, Koray Kavukcuoglu,
and Pavel Kuksa. 2011. Natural lan-
guage processing (almost) from scratch.
J. Mach. Learn. Res. 12:2493–2537.
http://dl.acm.org/citation.
cfm?id=1953048.2078186.

Alexis Conneau, Douwe Kiela, Holger Schwenk,
Loic Barrault, and Antoine Bordes. 2017. Su-
pervised learning of universal sentence rep-
resentations from natural language inference
data. arXiv preprint arXiv:1705.02364 .

Mona Diab and Philip Resnik. 2002. An unsuper-
vised method for word sense tagging using
parallel corpora. In Proceedings of 40th An-
nual Meeting of the Association for Compu-
tational Linguistics. Association for Compu-
tational Linguistics, Philadelphia, Pennsylva-
nia, USA, pages 255–262. https://doi.
org/10.3115/1073083.1073126.

Manaal Faruqui and Chris Dyer. 2014a. Commu-
nity evaluation and exchange of word vectors

at wordvectors.org. In Proceedings of the
52nd Annual Meeting of the Association for
Computational Linguistics: System Demon-
strations. Association for Computational Lin-
guistics, Baltimore, USA.

Manaal Faruqui and Chris Dyer. 2014b. Im-
proving vector space word representations
using multilingual correlation. In Proceed-
ings of the 14th Conference of the Euro-
pean Chapter of the Association for Com-
putational Linguistics. Association for Com-
putational Linguistics, Gothenburg, Sweden,
pages 462–471. http://www.aclweb.
org/anthology/E14-1049.

Adriano Ferraresi, Eros Zanchetta, Marco Baroni,
and Silvia Bernardini. 2008. Introducing and
evaluating ukwac, a very large web-derived
corpus of english. In In Proceedings of the
4th Web as Corpus Workshop (WAC-4.

J. R. Firth. 1957. A synopsis of linguistic the-
ory 1930-1955. Studies in Linguistic Analysis
pages 1–32.

Xavier Glorot and Yoshua Bengio. 2010. Under-
standing the difficulty of training deep feedfor-
ward neural networks. In Yee Whye Teh and
Mike Titterington, editors, Proceedings of the
Thirteenth International Conference on Artifi-
cial Intelligence and Statistics. PMLR, Chia
Laguna Resort, Sardinia, Italy, volume 9 of
Proceedings of Machine Learning Research,
pages 249–256. http://proceedings.
mlr.press/v9/glorot10a.html.

Stephan Gouws, Yoshua Bengio, and Greg Cor-
rado. 2015. Bilbowa: Fast bilingual dis-
tributed representations without word align-
ments. In Francis Bach and David Blei,
editors, Proceedings of the 32nd Interna-
tional Conference on Machine Learning.
PMLR, Lille, France, volume 37 of Proceed-
ings of Machine Learning Research, pages
748–756. http://proceedings.mlr.
press/v37/gouws15.html.

Zellig S. Harris. 1954. Distributional structure.
Word 10(23):146–162.

Karl Moritz Hermann and Phil Blunsom. 2014.
Multilingual models for compositional dis-
tributed semantics. In Proceedings of

1020



the 52nd Annual Meeting of the Associa-
tion for Computational Linguistics (Volume
1: Long Papers). Association for Compu-
tational Linguistics, Baltimore, Maryland,
pages 58–68. http://www.aclweb.
org/anthology/P14-1006.

Felix Hill, Kyunghyun Cho, Sebastien Jean, Coline
Devin, and Yoshua Bengio. 2014. Embedding
word similarity with neural machine transla-
tion. arXiv preprint arXiv:1412.6448 .

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural Comput.
9(8):1735–1780. https://doi.org/10.
1162/neco.1997.9.8.1735.

MichaelI. Jordan, Zoubin Ghahramani, TommiS.
Jaakkola, and LawrenceK. Saul. 1999. An
introduction to variational methods for graph-
ical models. Machine Learning 37(2):183–
233.

Diederik P. Kingma and Jimmy Ba. 2014. Adam:
A method for stochastic optimization. CoRR
abs/1412.6980.

Diederik P. Kingma and Max Welling. 2014. Auto-
encoding variational bayes. In International
Conference on Learning Representations.

Alexandre Klementiev, Ivan Titov, and Binod Bhat-
tarai. 2012. Inducing crosslingual distributed
representations of words. In Proceedings
of COLING 2012. The COLING 2012 Or-
ganizing Committee, Mumbai, India, pages
1459–1474. http://www.aclweb.org/
anthology/C12-1089.

Philipp Koehn. 2005. Europarl: A Parallel Corpus
for Statistical Machine Translation. In Confer-
ence Proceedings: the tenth Machine Trans-
lation Summit. AAMT, AAMT, Phuket, Thai-
land, pages 79–86. http://mt-archive.
info/MTS-2005-Koehn.pdf.

Philipp Koehn, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico,
Nicola Bertoldi, Brooke Cowan, Wade
Shen, Christine Moran, Richard Zens, Chris
Dyer, Ondřej Bojar, Alexandra Constantin,
and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine trans-
lation. In Proceedings of the 45th An-
nual Meeting of the ACL on Interactive

Poster and Demonstration Sessions. Associa-
tion for Computational Linguistics, Strouds-
burg, PA, USA, ACL ’07, pages 177–
180. http://dl.acm.org/citation.
cfm?id=1557769.1557821.

Tomáš Kočiský, Karl Moritz Hermann, and Phil
Blunsom. 2014. Learning Bilingual Word
Representations by Marginalizing Alignments.
In Proceedings of ACL.

Omer Levy and Yoav Goldberg. 2014a.
Dependency-based word embeddings.
In Proceedings of the 52nd Annual Meeting of
the Association for Computational Linguistics
(Volume 2: Short Papers). Association
for Computational Linguistics, Baltimore,
Maryland, pages 302–308. http://www.
aclweb.org/anthology/P14-2050.

Omer Levy and Yoav Goldberg. 2014b. Linguis-
tic regularities in sparse and explicit word
representations. In Proceedings of the Eigh-
teenth Conference on Computational Natural
Language Learning. Association for Compu-
tational Linguistics, Ann Arbor, Michigan,
pages 171–180. http://www.aclweb.
org/anthology/W14-1618.

Thang Luong, Hieu Pham, and Christopher D Man-
ning. 2015. Bilingual word representations
with monolingual quality in mind. In Pro-
ceedings of the 1st Workshop on Vector Space
Modeling for Natural Language Processing.
pages 151–159.

Diana McCarthy and Roberto Navigli. 2009. The
english lexical substitution task. Language
Resources and Evaluation 43(2):139–159.

Oren Melamud, Omer Levy, and Ido Dagan. 2015.
A simple word embedding model for lexi-
cal substitution. In Proceedings of the 1st
Workshop on Vector Space Modeling for Nat-
ural Language Processing, VS@NAACL-HLT
2015, June 5, 2015, Denver, Colorado, USA.
pages 1–7.

Rada Mihalcea and Ted Pedersen. 2003. An
evaluation exercise for word alignment.
In Proceedings of the HLT-NAACL 2003
Workshop on Building and Using Parallel
Texts: Data Driven Machine Translation
and Beyond - Volume 3. Association for

1021



Computational Linguistics, Stroudsburg, PA,
USA, HLT-NAACL-PARALLEL ’03, pages
1–10. https://doi.org/10.3115/
1118905.1118906.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S
Corrado, and Jeff Dean. 2013. Distributed
representations of words and phrases and their
compositionality. In C.J.C. Burges, L. Bot-
tou, M. Welling, Z. Ghahramani, and K.Q.
Weinberger, editors, Advances in Neural In-
formation Processing Systems 26, Curran As-
sociates, Inc., pages 3111–3119.

Franz Josef Och and Hermann Ney. 2000. Im-
proved statistical alignment models. In 38th
Annual Meeting of the Association for Compu-
tational Linguistics, Hong Kong, China, Octo-
ber 1-8, 2000..

Sebastian Padó and Mirella Lapata. 2006. Opti-
mal constituent alignment with edge covers
for semantic projection. In Proceedings of
the 21st International Conference on Compu-
tational Linguistics and 44th Annual Meet-
ing of the Association for Computational
Linguistics. Association for Computational
Linguistics, Sydney, Australia, pages 1161–
1168. https://doi.org/10.3115/
1220175.1220321.

Danilo Jimenez Rezende, Shakir Mohamed, and
Daan Wierstra. 2014. Stochastic backprop-
agation and approximate inference in deep
generative models. In Proceedings of the 31th
International Conference on Machine Learn-
ing, ICML 2014, Beijing, China, 21-26 June
2014. pages 1278–1286.

Herbert Robbins and Sutton Monro. 1951. A
stochastic approximation method. The Annals
of Mathematical Statistics 22(3):400–407.

Philip Schulz, Wilker Aziz, and Khalil Sima’an.
2016. Word alignment without null words.
In Proceedings of the 54th Annual Meeting
of the Association for Computational Linguis-
tics (Volume 2: Short Papers). Association for
Computational Linguistics, Berlin, Germany,
pages 169–174. http://anthology.
aclweb.org/P16-2028.

Richard Socher, Jeffrey Pennington, Eric H. Huang,
Andrew Y. Ng, and Christopher D. Man-

ning. 2011. Semi-supervised recursive au-
toencoders for predicting sentiment distribu-
tions. In Proceedings of the 2011 Confer-
ence on Empirical Methods in Natural Lan-
guage Processing. Association for Computa-
tional Linguistics, Edinburgh, Scotland, UK.,
pages 151–161. http://www.aclweb.
org/anthology/D11-1014.

Michalis Titsias and Miguel Lázaro-Gredilla. 2014.
Doubly stochastic variational bayes for non-
conjugate inference. In Proceedings of the
31st International Conference on Machine
Learning (ICML-14). pages 1971–1979.

Lifu Tu, Kevin Gimpel, and Karen Livescu.
2017. Learning to embed words in con-
text for syntactic tasks. In Proceedings
of the 2nd Workshop on Representation
Learning for NLP. Association for Computa-
tional Linguistics, Vancouver, Canada, pages
265–275. http://www.aclweb.org/
anthology/W17-2632.

Shyam Upadhyay, Manaal Faruqui, Chris Dyer,
and Dan Roth. 2016. Cross-lingual models of
word embeddings: An empirical comparison.
In Proceedings of the 54th Annual Meeting
of the Association for Computational Linguis-
tics (Volume 1: Long Papers). Association for
Computational Linguistics, Berlin, Germany,
pages 1661–1670. http://www.aclweb.
org/anthology/P16-1157.

Luke Vilnis and Andrew McCallum. 2014.
Word representations via gaussian embedding.
arXiv preprint arXiv:1412.6623 .

Simon Šuster, Ivan Titov, and Gertjan van No-
ord. 2016. Bilingual learning of multi-sense
embeddings with discrete autoencoders. In
Proceedings of the 2016 Conference of the
North American Chapter of the Association
for Computational Linguistics: Human Lan-
guage Technologies. Association for Compu-
tational Linguistics, San Diego, California,
pages 1346–1356. http://www.aclweb.
org/anthology/N16-1160.

1022



A Multiple runs sentence evaluation

Figure 2 shows multiple runs of our proposed
model on sentence evaluation. The first figure re-
ports the mean and two standard deviations (error
bars) for benchmarks based on accuracy (ACC), the
second figure reports benchmarks based on F1, and
finally the third figure reports benchmarks based
on correlation metrics Spearman (S) and Pearson
(P).

Figure 2: Mean and two standard deviations (error
bars) for 10 runs of EMBEDALIGN on the sentence eval-
uation benchmarks.

B Architecture

Figure 3 shows the architecture for the inference
and generative models in EMBEDALIGN, with
BiRNN encoder (h).

x

embedding
128d

h: BiRNN
100d

u 
100d

f(z)

x

s 
100d

sample z
100d

g(zaj)

y

Generative 
model

Inference 
model

Figure 3: Architecture for EmbedAlign.

1023


