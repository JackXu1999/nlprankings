



















































The ACCEPT Portal: An Online Framework for the Pre-editing and Post-editing of User-Generated Content


Workshop on Humans and Computer-assisted Translation, pages 66–71,
Gothenburg, Sweden, 26 April 2014. c©2014 Association for Computational Linguistics

The ACCEPT Portal: An Online Framework for the Pre-editing and
Post-editing of User-Generated Content

Violeta Seretan
FTI/TIM

University of Geneva
Switzerland

Violeta.Seretan@unige.ch

Johann Roturier
Symantec Ltd.
Dublin, Ireland

johann roturier@symantec.com

David Silva
Symantec Ltd.
Dublin, Ireland

David Silva@symantec.com

Pierrette Bouillon
FTI/TIM

University of Geneva
Switzerland

Pierrette.Bouillon@unige.ch

Abstract

With the development of Web 2.0, a
lot of content is nowadays generated on-
line by users. Due to its characteristics
(e.g., use of jargon and abbreviations, ty-
pos, grammatical and style errors), the
user-generated content poses specific chal-
lenges to machine translation. This pa-
per presents an online platform devoted to
the pre-editing of user-generated content
and its post-editing, two main types of hu-
man assistance strategies which are com-
bined with domain adaptation and other
techniques in order to improve the trans-
lation of this type of content. The plat-
form has recently been released publicly
and is being tested by two main types of
user communities, namely, technical fo-
rum users and volunteer translators.

1 Introduction

User-generated content – i.e., information posted
by Internet users in social communication chan-
nels like blogs, forum posts, social networks – is
one of the main sources of information available
today. Huge volumes of such content are created
each day, reach a very broad audience instantly.1

The democratisation of content creation due
to the emergence of the Web 2.0 paradigm also
means a diversification of the languages used on
the Internet.2 Despite its availability, the new con-
tent is only accessible to the speakers of the lan-
guage in which it was created. The automatic
translation of user-generated content is therefore
one of the key issues to be addressed in the field of
human language technologies. However, as stated

1For instance, 58 million tweets are sent on aver-
age per day (http://www.statisticbrain.com/
twitter-statistics/).

2See http://en.wikipedia.org/wiki/
Languages_used_on_the_Internet for statistics.

by Jiang et al. (2012), despite the obvious bene-
fits, there are relatively little attempts at translating
user-generated content.

The reason may lie in the fact that user-ge-
nerated content is very challenging for machine
translation. As shown, among others, by Nagara-
jan and Gamon (2011), there are several charac-
teristics of this content that pose new process-
ing challenges with respect to traditional content:
informal style, slang, abbreviations, specific ter-
minology, irregular grammar and spelling. In-
deed, Internet users are rarely professional writ-
ers.3 They often write in a language which is not
their own, and sacrifice quality for speed, not pay-
ing attention to spelling, punctuation, or grammar
rules.

The ACCEPT project4 addresses these chal-
lenges by developing a technology integrating
modules for automatic and manual content pre-
editing, statistical machine translation, as well
as output evaluation and post-editing. Thus, the
project aims to improve the translation of user-ge-
nerated content by proposing a full workflow, in
which the participation of humans is essential.

The application scenario considered in the
project are user communities sharing specific in-
formation on a given topic. The project focuses,
more specifically, on the following use cases:

1. the commercial use case, in which the tar-
get community is the user community built
around a software company in order for
members to help each other with issues re-
lated to products;

2. the NGO use case, in which non-go-
vernmental organisations such as Doctors
Without Borders produce health-care content
for distributions in areas of need.

3Even when they are, as in the case of government agen-
cies, the type of content produced (e.g., tweets) still poses
“multiple challenges” to translation (Gotti et al., 2013).

4http://www.accept-project.eu/

66



The language pairs considered in the project are
English to French, German and Japanese, as well
as French into English for the first use case (in-
volving technical forum information), and French
to and from English for the second use case (in-
volving healthcare information).

Past halfway into its research program, the
project has accomplished significant progress in
the main areas mentioned above (pre-editing, sta-
tistical machine translation, post-editing, and eval-
uation). The ACCEPT technology has recently
been released to the broad public as an on-
line framework, which demonstrates the different
modules of the workflow and provides access to
associated software components (plug-ins, APIs),
as well as to documentation. The pre-editing tech-
nology has been deployed on the targeted user fo-
rum5, allowing users to check their messages be-
fore posting them. The post-editing technology is
being used by a community of translators, which
provide pro-bono translation services to the NGOs
considered in our second use case.

In this paper, we describe the framework by pre-
senting its architecture and main modules (Sec-
tion 2). We discuss related work in Section 3 and
conclude in Section 4.

2 The Framework

The ACCEPT technology has been made acces-
sible to a broad audience in the form of an on-
line framework, i.e., an integrated environment
where registered users can perform pre-editing,
post-editing and evaluation work. The framework
– henceforth, the ACCEPT Portal – is hosted on a
cloud computing infrastructure and is available at
www.accept-portal.eu.

2.1 Architecture of the Framework
As explained in Section 1, the ACCEPT techno-
logy consists of the following main modules:

1. Pre-editing module;

2. Machine translation module,

3. Post-editing module,

4. Evaluation module.

The typical workflow is incremental, but the
modules are independent. They can be used both
within and outside the portal, as they are built on a
REST API facilitating integration.

5https://community.norton.com/

In the remaining of this section, we introduce
each of the framework modules.6

2.2 Pre-editing Module

The pre-editing module leverages existing ling-
ware which provides authoring support rules
aimed at language professionals, by relying on
shallow language processing (Bredenkamp et al.,
2000). The existing English checker and the lin-
guistic resources on which it relies have been ex-
tended and adapted to suit the type of data gener-
ated by community users. In particular, the soft-
ware extension consisted of designing a number
of pre-editing rules aimed at source normalisation,
for the purpose of making the input text easier
to handle by the SMT systems. In the case of
French, the pre-editing rules have been designed
from scratch. The pre-editing rules pertain to the
levels of spelling, grammar, style and terminology.
They are defined using the original lingware’s rule
formalism and are incorporated into a server dedi-
cated to the project.

The rule development was corpus-driven and
was performed on data collected for this purpose.
A stable set of pre-edition rules is available in
the portal for each of the domains and source
languages considered (i.e., technical forum and
heathcare data in English and French). The rules
are described in detail in the project deliverable
D 2.2 (2013).

The rules proposed have been evaluated individ-
ually and in combination (Roturier et al., 2012;
Gerlach et al., 2013; Seretan et al., 2014). As
a general observation, it is important to notice
that, for SMT, the improvement of the input text
does not go hand in hand with the improvement of
translation. For example, in French the rule for
correcting verbal forms to the subjunctive tense
had a negative impact since the subjunctive is not
frequent in the training data. Conversely, it was
possible to define lexical reformulations which de-
graded the quality of the input text, but had a po-
sitive impact on translation quality.

The combined impact of the rule applica-
tion was measured in a variety of settings in a
large-scale evaluation campaign involving transla-
tion students (Seretan et al., 2014). As the rules
are divided into two major groups, those automati-
cally applicable and those requiring human inter-

6The MT module will be omitted, as it is not part of the
portal. The interested reader is referred to D 4.2 (2013).

67



Figure 1: The ACCEPT Pre-edit plug-in in action (screen capture)

vention, the evaluation was carried out for the full
set of rules, as well as for the automatic rules only.
In addition, the evaluation was performed in both
a monolingual and a bilingual setting, i.e., with
the evaluators having or not access to the source
text, and it involved evaluation scales of different
granularities. The evaluation results showed a sys-
tematic statistically significant improvement over
the baseline when pre-editing is performed on the
source content. More details about the evalua-
tion methodology and results can be found in the
project deliverable D 9.2.2 (2013).

A data excerpt illustrating the impact of pre-
editing on translation quality is presented in Ex-
ample 1 below. The simple correction of an ac-
cented letter, du→ dû, leads to the change of seve-
ral target words, and to a much better translation of
the input sentence.

1. a) Source (original):
J’ai du m’absenter hier après midi.

b) Source (pre-edited):
J’ai dû m’absenter hier après midi.

c) Target (original):
I have the leave me yesterday afternoon.

d) Target (pre-edited):
I had to leave yesterday afternoon.

The pre-editing component of the ACCEPT
technology is available as a JQuery plug-in, which
can be downloaded and installed by Web applica-
tion owners, so that it can be used with text areas
and other text-bearing elements. APIs and ac-
companying documentation have also been made
available, so that the pre-editing rules can be
leveraged in automatic steps, without the plug-in,
across devices and platforms. A demo site illus-
trating the use of the plug-in in a TinyMCE envi-
ronment is available on the portal (see Figure 1).

The latest developments of the pre-editing mo-
dule include the possibility for users to customise
the application of rule sets, in particular, to ignore
specific rules and to manage their own dictionary,
in order to prevent the activation of checking flags.

2.3 Post-editing Module

The post-editing module of the framework (see
also Roturier et al., (2013)) is designed to fulfil
the project’s objective of collecting post-editing
data in order to learn correction rules and, through
feedback loops, to integrate them into the SMT
engines (with the goal of automating corrections
whenever possible). The project relies on the par-
ticipation of volunteer community members, who
are subject matter experts, native speakers of the

68



Figure 2: The ACCEPT Portal showing the post-editing demo (screen capture)

target language and, possibly, of the source lan-
guage. Accordingly, the post-editing environment
(see Figure 2) provides functionalities for both
monolingual and bilingual post-editing.

The post-editing text is organised in tasks be-
longing to post-editing projects. The latter are
created and managed by project administrators,
by defining the project settings (e.g., source and
target languages, monolingual or bilingual mode,
collaborative or non-collaborative type7), upload-
ing the text for each task8, inviting participants by
e-mail, and monitoring revision progress.

The post-editors edit the target text in a
sentence-by-sentence fashion. They have access
to the task guidelines and to help documentation.
The interface of the post-editing window displays
the whole text, through which they can navigate
with next-previous buttons or by clicking on a
specific sentence. Users can check the text they
are editing by accessing, with a button, the con-
tent checking technology described in Section 2.2.
Their actions – in terms of keystrokes and usage

7In a collaborative editing scenario, users may see edits
from other users and do not have to repeat them when work-
ing on the same project task. Conflicts are avoided by pre-
venting concurrent access.

8Currently, the JSON format is used for the input data.

of translation options – and time spent editing are
recorded in the portal.9 When they are done edi-
ting, they can click on a button marking the com-
pletion of the task. At any time, they can interrupt
their work and save their results for later.

Users can enter a comment on the post-editing
task they have performed. The feedback elicited
from users include the difficulty of the task and
their sentiment (Was it easy to post-edit? Did you
enjoy the post-editing task?). For systematically
collecting user feedback, the project administra-
tors can specify on the project configuration page
a link to a post-task survey, which will be sent to
users after completing their tasks.

The post-editing module includes a JQuery
plug-in for deployment in any Web-based envi-
ronment; a dedicated section of the portal; APIs
enabling the use of the post-editing functionality
outside the portal; and sample evaluation projects
for several language pairs.

The post-editing technology has been exten-
sively used in specific post-editing campaigns in-
volving translator volunteers and Amazon Me-
chanical Turk10 workers. The campaigns, includ-

9The post-editing data is exported in XLIFF format.
10The integration was done via the ACCEPT API.

69



ing reports on post-task surveys, are documented
inter alia in deliverable D 8.1.2 (2013). A notable
finding was that professional translators, who were
reticent towards MT before the task, had a more
positive sentiment after post-editing and their mo-
tivation to post-edit in the future increased.

2.4 Evaluation Module

The role of the evaluation module is to support the
collection of user ratings for assessing the quality
of source, machine-translated and post-edited con-
tent, and, ultimately, to support the development
of the technology created in the project.

This module groups several software compo-
nents: an evaluation environment available as a
section of the portal; APIs enabling the collection
of user evaluations in-context; and a third com-
ponent which is a customisation of the Appraise
toolkit for the collaborative collection of human
judgements (Federmann, 2012).

As in the case of post-editing module, this mod-
ule provides functionality for creating and man-
aging projects. Using the evaluation environ-
ment/APIs, project creators can define question
categories, add questions and possible answers,
and upload evaluation data (in JSON format). For
traditional evaluation projects, the Appraise sys-
tem is used instead.

3 Related Work

Transforming the source text in order to better
fit the needs of machine translation is a well-
investigated area of research. Strategies like
source control, source re-ordering, or source sim-
plification at the lexical or structural level have
been largely explored; for reviews, see, for in-
stance, Huhn (2013), Kazemi (2013), and Feng
(2008), respectively.

User-generated content has been investigated
in the context of machine translation in recent
work dealing specifically with spelling correc-
tion (Bertoldi et al., 2010; Formiga and Fonol-
losa, 2012); lexical normalisation by substituting
ill-formed words with their correct counterpart,
e.g., makn → making (Han and Baldwin, 2011);
missing word – e.g., zero-pronoun – recovery and
punctuation correction (Wang and Ng, 2013).

Rather than focusing on specific phenomena or
Web genres (i.e., tweets), we adopt a more gen-
eral approach in which we address the problem of
source normalisation at multiple levels – punctua-

tion, spelling, grammar, and style – for any type of
linguistically imperfect text.

Another peculiarity of our approach is that it
is rule-based and does not require parallel data
for learning corrections. In exchange, a limi-
tation of our pre-editing approach is that it is
language-dependent, as the underlying technology
is based on shallow analysis and is therefore time-
expensive to extend to a new language.

The post-editing technology differs from exist-
ing (standalone or Web-based) dedicated tools –
e.g., iOmegaT11 or MateCat12 – in that it is tai-
lored to community users, and, consequently, it
is lighter, it generates more concise reports, and
a simpler interface replaces the grid-like format
for presenting data. Another specificity is that it
is sufficiently flexible to be used in other environ-
ments (e.g., Amazon Mechanical Turk, cf. §2.3).

4 Conclusion

The technology outlined in this paper demon-
strates a specific case of human-computer interac-
tion, in which, for the first time, several modules
are integrated in a full process in which human
pre-editors, post-editors and evaluators play a key
role for improving the translation of community
content. The technology is freely accessible in the
online portal, has been deployed on a major user
forum, and can be downloaded for integration in
other Web-based environments. Since it is built on
top of a REST API, it is portable across devices
and platforms. The technology would be useful to
anyone who needs information instantly and relia-
bly translated, despite linguistic imperfections.

One of the main future developments concerns
the further improvement of SMT, by exploring,
in particular, the use of text analytics and senti-
ment detection. In addition, by incorporating post-
editing rules and developing techniques to change
the phrase table and system parameters dynam-
ically, it will be possible to reduce the amount
of error corrections that human post-editors have
to perform repeatedly. Another major develop-
ment (joint work with the CASMACAT European
project) will focus on novel types of assistance for
translators, aimed specifically at helping transla-
tors by identifying problematic parts of the ma-
chine translation output and signalling the para-
phrases that are more likely to be useful.

11http://try-and-see-mt.org/
12http://www.matecat.com/

70



Acknowledgments

The research leading to these results has received
funding from the European Community’s Seventh
Framework Programme (FP7/2007-2013) under
grant agreement no 288769.

References
Nicola Bertoldi, Mauro Cettolo, and Marcello Fe-

derico. 2010. Statistical machine translation of texts
with misspelled words. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Compu-
tational Linguistics, pages 412–419, Los Angeles,
California.

Andrew Bredenkamp, Berthold Crysmann, and Mirela
Petrea. 2000. Looking for errors: A declarative for-
malism for resource-adaptive language checking. In
Proceedings of the Second International Conference
on Language Resources and Evaluation, Athens,
Greece.

2013. ACCEPT deliverable D 2.2 Definition of
pre-editing rules for English and French (final
version). http://www.accept.unige.
ch/Products/D2_2_Definition_of_
Pre-Editing_Rules_for_English_and_
French_with_appendixes.pdf.

2013. ACCEPT deliverable D 9.2.2: Survey of
evaluation results. http://www.accept.
unige.ch/Products/D_9_2_Survey_of_
evaluation_results.pdf.

2013. ACCEPT deliverable D 4.2 Report on
robust machine translation: domain adap-
tation and linguistic back-off. http:
//www.accept.unige.ch/Products/
D_4_2_Report_on_robust_machine_
translation_domain_adaptation_and_
linguistic_back-off.pdf.

2013. ACCEPT deliverable D 8.1.2 Data and
report from user studies - Year 2. http:
//www.accept.unige.ch/Products/D_
8_1_2_Data_and_report_from_user_
studies_-_Year_2.pdf.

Christian Federmann. 2012. Appraise: An open-
source toolkit for manual evaluation of machine
translation output. The Prague Bulletin of Mathe-
matical Linguistics (PBML), 98:25–35.

Lijun Feng. 2008. Text simplification: A survey.
Technical report, CUNY.

Lluı́s Formiga and José A. R. Fonollosa. 2012. Dea-
ling with input noise in statistical machine transla-
tion. In Proceedings of COLING 2012: Posters,
pages 319–328, Mumbai, India.

Johanna Gerlach, Victoria Porro, Pierrette Bouillon,
and Sabine Lehmann. 2013. La préédition avec

des règles peu coûteuses, utile pour la TA statistique
des forums ? In Actes de la 20e conférence sur
le Traitement Automatique des Langues Naturelles
(TALN’2013), pages 539–546, Les Sables d’Olonne,
France.

Fabrizio Gotti, Philippe Langlais, and Atefeh Farzin-
dar. 2013. Translating government agencies’ tweet
feeds: Specificities, problems and (a few) solutions.
In Proceedings of the Workshop on Language Anal-
ysis in Social Media, pages 80–89, Atlanta, Georgia.

Bo Han and Timothy Baldwin. 2011. Lexical normali-
sation of short text messages: Makn sens a #twit-
ter. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 368–378, Port-
land, Oregon.

Jie Jiang, Andy Way, and Rejwanul Haque. 2012.
Translating user-generated content in the social net-
working space. In Proceedings of the Tenth Biennial
Conference of the Association for Machine Transla-
tion in the Americas (AMTA-2012), San Diego, Cali-
fornia.

Arefeh Kazemi, Amirhassan Monadjemi, and Moham-
madali Nematbakhsh. 2013. A quick review on re-
ordering approaches in statistical machine transla-
tion systems. IJCER, 2(4).

Tobias Kuhn. 2013. A survey and classification of con-
trolled natural languages. Computational Linguis-
tics.

Meenakshi Nagarajan and Michael Gamon, editors.
2011. Proceedings of the Workshop on Language
in Social Media (LSM 2011). Portland, Oregon.

Johann Roturier, Linda Mitchell, Robert Grabowski,
and Melanie Siegel. 2012. Using automatic ma-
chine translation metrics to analyze the impact of
source reformulations. In Proceedings of the Con-
ference of the Association for Machine Translation
in the Americas (AMTA), San Diego, California.

Johann Roturier, Linda Mitchell, and David Silva.
2013. The ACCEPT post-editing environment: a
flexible and customisable online tool to perform and
analyse machine translation post-editing. In Pro-
ceedings of MT Summit XIV Workshop on Post-edi-
ting Technology and Practice, Nice, France.

Violeta Seretan, Pierrette Bouillon, and Johanna Ger-
lach. 2014. A large-scale evaluation of pre-edi-
ting strategies for improving user-generated content
translation. In Proceedings of the 9th Edition of the
Language Resources and Evaluation Conference,
Reykjavik, Iceland.

Pidong Wang and Hwee Tou Ng. 2013. A beam-
search decoder for normalization of social media
text with application to machine translation. In Pro-
ceedings of the 2013 Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
471–481, Atlanta, Georgia.

71


