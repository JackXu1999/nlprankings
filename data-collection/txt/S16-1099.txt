



















































DLS@CU at SemEval-2016 Task 1: Supervised Models of Sentence Similarity


Proceedings of SemEval-2016, pages 650–655,
San Diego, California, June 16-17, 2016. c©2016 Association for Computational Linguistics

DLS@CU at SemEval-2016 Task 1: Supervised Models of Sentence
Similarity

Md Arafat Sultan† Steven Bethard‡ Tamara Sumner†
†Institute of Cognitive Science and Department of Computer Science,

University of Colorado Boulder
‡Department of Computer and Information Sciences,

University of Alabama at Birmingham,
arafat.sultan@colorado.edu, bethard@uab.edu, sumner@colorado.edu

Abstract

We describe a set of systems submitted to the
SemEval-2016 English Semantic Textual Sim-
ilarity (STS) task. Given two English sen-
tences, the task is to compute the degree of
their semantic similarity. Each of our systems
uses the SemEval 2012–2015 STS datasets to
train a ridge regression model that combines
different measures of similarity. Our best sys-
tem demonstrates 73.6% correlation with av-
erage human annotations across five test sets.

1 Introduction

Identification of short-text semantic similarity is an
important research problem with application in a
multitude of NLP tasks: question answering (Yao
et al., 2013; Severyn and Moschitti, 2013), short an-
swer grading (Mohler et al., 2011; Ramachandran
et al., 2015), text summarization (Dasgupta et al.,
2013; Wang et al., 2013), evaluation of machine
translation (Chan and Ng, 2008; Liu et al., 2011),
and so on. The SemEval Semantic Textual Similar-
ity (STS) task series (Agirre et al., 2012; Agirre et
al., 2013; Agirre et al., 2014; Agirre et al., 2015) is a
core platform for the task: a publicly available cor-
pus of more than 14,000 sentence pairs have been
developed over a span of four years with human an-
notations of similarity for each pair; and about 300
system runs have been evaluated.

In this article, we describe a set of systems that
participated in the SemEval-2016 English Seman-
tic Textual Similarity (STS) task. Given two En-
glish sentences, the objective is to compute their
semantic similarity in the range [0, 5], where the

score increases with similarity (i.e., 0 indicates no
similarity and 5 indicates identical meanings). The
official evaluation metric is the Pearson product-
moment correlation coefficient with human annota-
tions. Our systems leverage different measures of
sentence similarity and train ridge regression models
that learn to combine predictions from these differ-
ent sources using past SemEval data. The best of our
three system runs achieves 73.6% with human anno-
tations among all submitted systems on five test sets
(containing a total of 1186 test pairs).

Early work in sentence similarity (Mihalcea et al.,
2006; Li et al., 2006; Islam and Inkpen, 2008) estab-
lished the basic procedural framework under which
most modern algorithms operate: computing sen-
tence similarity as a mean of word similarities across
the two input sentences. With no human annotated
STS dataset available, these algorithms are unsuper-
vised and were evaluated extrinsically on tasks like
paraphrase detection and textual entailment recog-
nition. The SemEval STS task series has made an
important contribution through the large annotated
dataset, enabling intrinsic evaluation of STS systems
and making supervised STS systems a reality.

At SemEval 2012–2015, most of the top-
performing STS systems used a regression algorithm
to combine different measures of similarity (Bär et
al., 2012; Šarić et al., 2012; Wu et al., 2013; Lynum
et al., 2014; Sultan et al., 2015), with the notable
exception of a couple of unsupervised systems that
relied primarily on alignment of related words in
the two sentences (Han et al., 2013; Sultan et al.,
2014b).

Our models are based on the successful linear re-

650



Robin Warren was awarded    a      Nobel Prize .

«�$XVWUDOLDQ�GRFWRUV�5RELQ�:DUUHQ�DQG�%DUU\�0DUVKDOO�KDYH�UHFHLYHG�WKH������1REHO�3UL]H�LQ�«

Figure 1: Words aligned by our aligner across two sentences taken from the MSR alignment corpus (Brockett, 2007). (We show
only part of the second sentence.) Besides exact word/lemma matches, it identifies and aligns semantically similar word pairs using

PPDB (awarded↔ received in this example).

gression architecture of past SemEval systems in
general, and the winning system of SemEval-2015
(Sultan et al., 2015) in particular. We use the fea-
tures in the latter system unchanged in one of our
runs and augment them with simple word and char-
acter n-gram overlap features in the other two runs.

2 System Description

Our system employs a ridge regression model (linear
regression with L2 error and L2 regularization) to
combine a set of similarity measures. The model is
trained on SemEval 2012–2015 data. Our three runs
differ in the subset of features drawn from the fea-
ture pool. We describe the feature set in this section;
the individual runs will be discussed in Section 4.

2.1 Features

Word Alignment Proportion. This feature oper-
ationalizes the hypothesis that highly semantically
similar sentences should also have a high degree of
conceptual alignment between their semantic units,
i.e., words and phrases. To that end, we apply the
monolingual word aligner developed by Sultan et al.
(2014a) to input sentence pairs.1

This aligner aligns words based on their seman-
tic similarity and the similarity between their lo-
cal semantic contexts in the two sentences. It uses
the paraphrase database PPDB (Ganitkevitch et al.,
2013) to identify semantically similar words, and re-
lies on dependencies and surface-form neighbors of
the two words to determine their contextual similar-
ity. Word pairs are aligned in decreasing order of a
weighted sum of their semantic and contextual simi-
larity. Figure 1 shows an example set of alignments.

1https://github.com/ma-sultan/
monolingual-word-aligner

For more details, see (Sultan et al., 2014a).
Additionally, we also consider a Levenshtein dis-

tance2 of 1 between a misspelled word and a cor-
rectly spelled word (of length > 2) to be a match.

Given sentences S(1) and S(2), the alignment-
based similarity measure is computed as follows:

sim(S(1), S(2)) =
nac (S

(1)) + nac (S
(2))

nc(S(1)) + nc(S(2))

where nc(S(i)) and nac (S
(i)) are the number of con-

tent words and the number of aligned content words
in S(i), respectively.

Sentence Embedding. A fundamental limitation
of the above feature is that it only relies on PPDB to
identify semantically similar words; consequently,
similar word pairs are limited to only lexical para-
phrases. Hence it fails to utilize semantic similarity
or relatedness between non-paraphrase word pairs
(e.g., sister and related). In the current fea-
ture, we leverage neural word embeddings to over-
come this limitation. We use the 400-dimensional
vectors3 developed by Baroni et al. (2014). They
used the word2vec toolkit4 to extract these vectors
from a corpus of about 2.8 billion tokens. These
vectors perform well across different word similar-
ity datasets in their experiments. Details on their ap-
proach and findings can be found in (Baroni et al.,
2014).

Instead of comparing word vectors across the two
input sentences, we adopt a simple vector composi-
tion scheme to construct a vector representation of

2The minimum number of single-character edits needed to
change one word into the other, where an edit is an insertion, a
deletion or a substitution.

3http://clic.cimec.unitn.it/composes/
semantic-vectors.html

4https://code.google.com/p/word2vec/

651



Dataset Source of Text # of Pairs
answer-answer Q&A forums 254

headlines news headlines 249
plagiarism plagiarised answers 230
postediting post-edited MT pairs 244

question-question Q&A forums 209

Table 1: Test sets at SemEval STS 2016.

each input sentence and then take the cosine similar-
ity between the two sentence vectors as our second
feature for this run. The vector representing a sen-
tence is simply the sum of its content lemma vectors.

Word n-gram Overlap. This feature computes
the proportion of word n-grams (lemmatized) that
are in both S(1) and S(2). We employ separate in-
stances of this feature for n = 1, 2, 3. The goal is
to identify high local similarities in the two snippets
and learn the influence that such local similarities
might have on human judgment of sentence similar-
ity.

Character n-gram Overlap. This feature com-
putes the proportion of character n-grams that are in
both S(1) and S(2) in their surface form. We employ
separate instances of this feature for n = 3, 4. The
goal is to identify and correct for spelling errors as
well as incorrect lemmatizations.

Soft Cardinality. Soft Cardinality (Jimenez et
al., 2012) is a measure of set cardinality where simi-
lar items in a set contribute less to its cardinality than
dissimilar items. Jimenez et al. (2012) propose a
parameterized measure of semantic similarity based
on soft cardinality that computes sentence similar-
ity from word similarity and the latter from charac-
ter n-gram similarity. This measure was highly suc-
cessful at SemEval-2012 (Agirre et al., 2012). We
employ this measure with untuned parameter val-
ues as a feature for our model: p = 1, bias = 0,
α = 0.5, biassim = 0, αsim = 0.5, q1 = 2, and
q2 = 4. (Please see the original article for a detailed
description of these parameters as well as the simi-
larity measure.)

3 Data

The 1186 test sentence pairs at SemEval-2016 are
divided into five sets, each consisting of pairs from a
particular domain. Each pair is assigned a similarity
score in the range [0, 5] by human annotators (0: no
similarity, 5: identicality). Test sets are discussed

briefly in Table 1.
We train our supervised systems using data from

the past four years of SemEval STS (Agirre et al.,
2012; Agirre et al., 2013; Agirre et al., 2014; Agirre
et al., 2015). The selections vary by test set, which
we discuss in the next section.

4 Runs

We submit three runs at SemEval-2016. Each run
employs a ridge regression model; we use Scikit-
learn (Pedregosa et al., 2011) for model implemen-
tation. Different training data from SemEval 2012–
2015 are used for different test sets. For headlines,
we train the model on headlines (2013, 2014, 2015),
deft-news (2014), tweet-news (2014), and smtnews
(2012) pairs. For postediting, the model is trained on
smt (2013), smteuroparl (2012) and smtnews (2012)
pairs. These selections are based on the similar-
ity between the source and the target domains—
news data for headlines, machine translation data for
postediting. For the other three test sets, all past an-
notations (except those for fnwn (2013)) are used, as
we did not find any close matches for these test sets
in the SemEval 2012–2015 data.

4.1 Run 1

Run 1 is a ridge regression model based only on
the first two features—alignment and embeddings.
The regularization strength parameter α is set using
cross-validation on training data.

4.2 Run 2

Run 2 employs a model similar to the run 1 model,
but uses the entire feature set described in Sec-
tion 2.1. The same training sets are used for each
test set and the model parameter α is again set using
cross-validation on training data.

4.3 Run 3

Run 3 is identical to run 2, except that it assigns a
lower value to the regularization parameter α (100
as opposed to 500 in run 2).

5 Evaluation

Table 2 shows the performances of the three runs
(measured by Pearson’s r, the official evaluation
metric at SemEval STS) alongside the score for the

652



Dataset Runs Best
1 2 3 Score

answer-answer .5523 .5599 .5453 .6924
headlines .8008 .8033 .8033 .8275

plagiarism .8229 .8123 .8195 .8414
postediting .8426 .8442 .8442 .8669

question-question .6599 .6423 .6666 .7471
Weighted Mean .7356 .7330 .7355 .7781

Table 2: Performance on STS 2016 data. Each number in rows
1–5 is the correlation (Pearson’s r) between system output and

human annotations for the corresponding test set. The rightmost

column shows the best score by any system. The last row shows

the value of the final evaluation metric for each run as well as

the top-performing system.

best performing system for each test set. The last
row shows the official evaluation metric that com-
putes a weighted sum of correlations over all test
sets, where the weight of a test set is proportional
to the number of sentence pairs it contains.

Runs 1 and 3 have very similar overall perfor-
mances, slightly better than that of run 2. Among
the different test sets, the models perform well on
news headlines, plagiarism and machine translation
data, but poorly on the Q&A forums data.

5.1 Ablation Study
From the overall performances in Table 2, it is clear
that the three new features added to the Sultan et al.
(2015) model do not improve performance. There-
fore, we run a feature ablation study only on the
run 1 model. Table 3 shows the results. Similar
to the findings reported in (Sultan et al., 2015), the
alignment-based feature performs better across test
sets. However, the addition of the embedding fea-
ture improves performance on almost all test sets.

5.2 Relation between the Runs
We compute pairwise correlations between the pre-
dictions of our three runs to see how different they
are. As Table 4 shows, the predictions are highly
correlated, which is expected given the results in Ta-
ble 2.

6 Conclusions and Future Work

We present three supervised models of sentence sim-
ilarity based on the winning system at SemEval-
2015 (Sultan et al., 2015). Our additional features

Data Set Run 1 Alignment Embedding
answer-answer .5523 .5196 .4972

headlines .8008 .8013 .7240
plagiarism .8229 .8149 .7813
postediting .8426 .8226 .8214

question-question .6599 .5767 .6833
Weighted Mean .7356 .7084 .6994

Table 3: Performance of each individual feature of our best
run (run 1) on STS 2016 test sets. Combining the two features

improves performance on most test sets.

Runs Pearson’s r
1, 2 .9834
1, 3 .9952
2, 3 .9920

Table 4: Pairwise correlations between the three runs.

do not improve performance and results show simi-
lar influences of alignment and embedding features
as in SemEval-2015.

Besides high performance, the run 1 model has
the key advantage of simplicity and high replicabil-
ity. All the major design components are also avail-
able for free download (links provided in Section 2).

A key limitation of the system is its inability to
model semantics of units larger than words (phrasal
verbs, idioms, and so on). This is an important future
direction not only for our system but also for STS
and text comparison tasks in general. Incorporation
of stop word semantics is key to identifying similari-
ties and differences in subtle aspects of sentential se-
mantics like polarity and modality. Domain-specific
learning of the word vectors can also improve re-
sults.

References

Eneko Agirre, Mona Diab, Daniel Cer, and Aitor
Gonzalez-Agirre. 2012. SemEval-2012 Task 6: A Pi-
lot on Semantic Textual Similarity. In Proceedings of
the Sixth International Workshop on Semantic Evalua-
tion, SemEval ’12, pages 385–393, Montréal, Canada.

Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *SEM 2013 shared
task: Semantic Textual Similarity. In Second Joint
Conference on Lexical and Computational Semantics,
*SEM ’13, pages 32–43, Atlanta, Georgia, USA.

Eneko Agirre, Carmen Banea, Claire Cardie, Daniel
Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo,

653



Rada Mihalcea, German Rigau, and Janyce Wiebe.
2014. SemEval-2014 Task 10: Multilingual Seman-
tic Textual Similarity. In Proceedings of the 8th Inter-
national Workshop on Semantic Evaluation, SemEval
’14, pages 81–91, Dublin, Ireland.

Eneko Agirre, Carmen Banea, Claire Cardie, Daniel
Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo,
Inigo Lopez-Gazpio, Montse Maritxalar, Rada Mihal-
cea, German Rigau, Larraitz Uria, and Janyce Wiebe.
2015. SemEval-2015 Task 2: Semantic Textual Sim-
ilarity, English, Spanish and Pilot on Interpretability.
In Proceedings of the 9th International Workshop on
Semantic Evaluation, SemEval ’15, pages 252–263,
Denver, Colorado.

Daniel Bär, Chris Biemann, Iryna Gurevych, and Torsten
Zesch. 2012. UKP: Computing Semantic Textual
Similarity by Combining Multiple Content Similar-
ity Measures. In Proceedings of the Sixth Interna-
tional Workshop on Semantic Evaluation, SemEval
’12, pages 435–440, Montréal, Canada.

Marco Baroni, Georgiana Dinu, and Germán Kruszewski.
2014. Don’t count, predict! A systematic compari-
son of context-counting vs. context-predicting seman-
tic vectors. In Proceedings of the 52nd Annual Meet-
ing of the Association for Computational Linguistics,
ACL ’14, pages 238–247, Baltimore, Maryland.

Chris Brockett. 2007. Aligning the RTE 2006 Cor-
pus. Technical Report MSR-TR-2007-77, Microsoft
Research.

Yee Seng Chan and Hwee Tou Ng. 2008. MAXSIM: A
Maximum Similarity Metric for Machine Translation
Evaluation. In Proceedings of ACL-08: HLT, pages
55–62, Columbus, Ohio.

Anirban Dasgupta, Ravi Kumar, and Sujith Ravi. 2013.
Summarization Through Submodularity and Disper-
sion. In Proceedings of the 51st Annual Meeting of the
Association for Computational Linguistics, ACL ’13,
pages 1014–1022, Sofia, Bulgaria.

Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2013. PPDB: The Paraphrase
Database. In Proceedings of the 2013 Conference of
the North American Chapter of the Association for
Computational Linguistics – Human Language Tech-
nologies, NAACL ’13, pages 758–764, Atlanta, Geor-
gia, USA.

Lushan Han, Abhay L. Kashyap, Tim Finin,
James Mayfield, and Jonathan Weese. 2013.
UMBC EBIQUITY-CORE: Semantic Textual Simi-
larity Systems. In Second Joint Conference on Lexical
and Computational Semantics, *SEM ’13, pages
44–52, Atlanta, Georgia, USA.

Aminul Islam and Diana Inkpen. 2008. Semantic Text
Similarity Using Corpus-based Word Similarity and

String Similarity. ACM Transactions on Knowledge
Discovery from Data, 2(2):10:1–10:25.

Sergio Jimenez, Claudia Becerra, and Alexander Gel-
bukh. 2012. Soft Cardinality: A Parameterized Sim-
ilarity Function for Text Comparison. In Proceed-
ings of the Sixth International Workshop on Semantic
Evaluation, SemEval ’12, pages 449–453, Montréal,
Canada.

Yuhua Li, David McLean, Zuhair A. Bandar, James D.
O’Shea, and Keeley Crockett. 2006. Sentence Simi-
larity Based on Semantic Nets and Corpus Statistics.
IEEE Transactions on Knowledge and Data Engineer-
ing, 18(8):1138–1150.

Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng. 2011.
Better Evaluation Metrics Lead to Better Machine
Translation. In Proceedings of the 2011 Conference
on Empirical Methods in Natural Language Process-
ing, EMNLP ’13, pages 375–384, Edinburgh, Scot-
land, UK.

André Lynum, Partha Pakray, Björn Gambäck, and Ser-
gio Jimenez. 2014. NTNU: Measuring Semantic Sim-
ilarity with Sublexical Feature Representations and
Soft Cardinality. In Proceedings of the 8th Inter-
national Workshop on Semantic Evaluation, SemEval
’14, pages 448–453, Dublin, Ireland.

Rada Mihalcea, Courtney Corley, and Carlo Strapparava.
2006. Corpus-based and Knowledge-based Measures
of Text Semantic Similarity. In Proceedings of the 21st
National Conference on Artificial Intelligence, AAAI
’06, pages 775–780.

Michael Mohler, Razvan Bunescu, and Rada Mihalcea.
2011. Learning to Grade Short Answer Questions
Using Semantic Similarity Measures and Dependency
Graph Alignments. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies, ACL ’11, pages
752–762, Portland, Oregon, USA.

Fabian Pedregosa, Gaël Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier Grisel,
Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vin-
cent Dubourg, Jake Vanderplas, Alexandre Passos,
David Cournapeau, Matthieu Brucher, Matthieu Per-
rot, and Édouard Duchesnay. 2011. Scikit-learn: Ma-
chine Learning in Python. Journal of Machine Learn-
ing Research, 12:2825–2830.

Lakshmi Ramachandran, Jian Cheng, and Peter Foltz.
2015. Identifying Patterns For Short Answer Scor-
ing Using Graph-based Lexico-Semantic Text Match-
ing. In Proceedings of the Tenth Workshop on Inno-
vative Use of NLP for Building Educational Applica-
tions, NAACL-BEA ’15, pages 97–106.

Aliaksei Severyn and Alessandro Moschitti. 2013. Au-
tomatic Feature Engineering for Answer Selection and
Extraction. In Proceedings of the 2013 Conference

654



on Empirical Methods in Natural Language Process-
ing, EMNLP ’13, pages 458–467, Seattle, Washing-
ton, USA.

Md Arafat Sultan, Steven Bethard, and Tamara Sum-
ner. 2014a. Back to Basics for Monolingual Align-
ment: Exploiting Word Similarity and Contextual Ev-
idence. Transactions of the Association for Computa-
tional Linguistics, 2:219–230.

Md Arafat Sultan, Steven Bethard, and Tamara Sum-
ner. 2014b. DLS@CU: Sentence Similarity from
Word Alignment. In Proceedings of the 8th Inter-
national Workshop on Semantic Evaluation, SemEval
’14, pages 241–246, Dublin, Ireland.

Md Arafat Sultan, Steven Bethard, and Tamara Sum-
ner. 2015. DLS@CU: Sentence Similarity from Word
Alignment and Semantic Vector Composition. In Pro-
ceedings of the 9th International Workshop on Seman-
tic Evaluation, SemEval ’15, pages 148–153, Denver,
Colorado, USA.

Frane Šarić, Goran Glavaš, Mladen Karan, Jan Šnajder,
and Bojana Dalbelo Bašić. 2012. TakeLab: Systems
for Measuring Semantic Text Similarity. In Proceed-
ings of the Sixth International Workshop on Semantic
Evaluation, SemEval ’12, pages 441–448, Montréal,
Canada.

Lu Wang, Hema Raghavan, Vittorio Castelli, Radu Flo-
rian, and Claire Cardie. 2013. A Sentence Com-
pression Based Framework to Query-Focused Multi-
Document Summarization. In Proceedings of the 51st
Annual Meeting of the Association for Computational
Linguistics, ACL ’13, pages 1384–1394, Sofia, Bul-
garia.

Stephen Wu, Dongqing Zhu, Ben Carterette, and Hong-
fang Liu. 2013. MayoClinicNLP-CORE: Semantic
Representations for Textual Similarity. In Proceed-
ings of the Second Joint Conference on Lexical and
Computational Semantics, *SEM ’13, pages 148–154,
Atlanta, Georgia, USA.

Xuchen Yao, Benjamin Van Durme, Chris Callison-
Burch, and Peter Clark. 2013. Answer Extraction as
Sequence Tagging with Tree Edit Distance. In Pro-
ceedings of the 2013 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics – Human Language Technologies, NAACL
’13, pages 858–867, Atlanta, Georgia, USA.

655


