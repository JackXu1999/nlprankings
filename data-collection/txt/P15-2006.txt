



















































Deep Markov Neural Network for Sequential Data Classification


Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 32–37,

Beijing, China, July 26-31, 2015. c©2015 Association for Computational Linguistics

Deep Markov Neural Network for Sequential Data Classification

Min Yang1 Wenting Tu1 Wenpeng Yin2 Ziyu Lu1
1Department of Computer Science, The University of Hong Kong, Hong Kong

{myang,wttu,zylu}@cs.hku.hk
2Center for Information and Language Processing, University of Munich, Germany

wenpeng@cis.lmu.de

Abstract

We present a general framework for incor-
porating sequential data and arbitrary fea-
tures into language modeling. The general
framework consists of two parts: a hidden
Markov component and a recursive neural
network component. We demonstrate the
effectiveness of our model by applying it
to a specific application: predicting topics
and sentiments in dialogues. Experiments
on real data demonstrate that our method
is substantially more accurate than previ-
ous methods.

1 Introduction

Processing sequential data is a significant research
challenge for natural language processing. In
the past decades, numerous studies have been
conducted on modeling sequential data. Hidden
Markov Models (HMMs) and its variants are rep-
resentative statistical models of sequential data for
the purposes of classification, segmentation, and
clustering (Rabiner, 1989). For most aforemen-
tioned methods, only the dependencies between
consecutive hidden states are modeled. In natural
language processing, however, we find there are
dependencies locally and at a distance. Conser-
vatively using the most recent history to perform
prediction yields overfitting to short-term trends
and missing important long-term effects. Thus, it
is crucial to explore in depth to capture long-term
temporal dynamics in language use.

Numerous real world learning problems are
best characterized by interactions between mul-
tiple causes or factors. Taking sentiment analy-
sis for dialogues as an example, the topic of the
document and the author’s identity are both valu-
able for mining user’s opinions in the conversa-
tion. Specifically, each participant in the dialogue
usually has specific sentiment polarities towards

different topics. However, most existing sequen-
tial data modeling methods are not capable of in-
corporating the information from both the topic
and the author’s identity. More generally, there
is no sufficiently flexible sequential model that al-
lows incorporating an arbitrary set of features.

In this paper, we present a Deep Markov Neu-
ral Network (DMNN) for incorporating sequential
data and arbitrary features into language model-
ing. Our method learns from general sequential
observations. It is also capable of taking the or-
dering of words into account, and collecting in-
formation from arbitrary features associated with
the context. Comparing to traditional HMM-based
method, it explores deeply into the structure of
sentences, and is more flexible in taking exter-
nal features into account. On the other hand, it
doesn’t suffer from the training difficulties of re-
current neural networks, such as the vanishing gra-
dient problem.

The general framework consists of two parts:
a hidden Markov component and a neural net-
work component. In the training phase, the hid-
den Markov model is trained on the sequential ob-
servation, resulting in transition probabilities and
hidden states at each time step. Then, the neural
network is trained, taking words, features and hid-
den state at the previous time step as input, to pre-
dict the hidden states at the present time step. The
procedure is reversed in the testing phase: the neu-
ral network predicts the hidden states using words
and features, then the hidden Markov model pre-
dicts the observation using hidden states.

A key insight of our method is to use hid-
den states as an intermediate representation, as
a bridge to connect sentences and observations.
By using hidden states, we can deal with arbi-
trary observation, without worrying about the is-
sue of discretization and normalization. Hidden
states are robust with respect to the random noise
in the observation. Unlike recurrent neural net-

32



work which connects networks between consecu-
tive time steps, the recursive neural network in our
framework connects to the previous time step by
using its hidden states. In the training phase, since
hidden states are inferred by the hidden Markov
model, the training of recursive neural networks
at each time step can be performed separately,
preventing the difficulty of learning an extremely
deep neural network.

We demonstrate the effectiveness of our model
by applying it to a specific application: predicting
topics and sentiments in dialogues. In this exam-
ple, the sequential observation includes topics and
sentiments. The feature includes the identity of
the author. Experiments on real data demonstrate
that our method is substantially more accurate than
previous methods.

2 Related work

Modeling sequential data is an active research field
(Lewis and Gale, 1994; Jain et al., 2000; Ra-
biner, 1989; Baldi and Brunak, 2001; Kum et al.,
2005). The paper proposed by Kum et al. (2005)
describes most of the existing techniques for se-
quential data modeling. Hidden Markov Mod-
els (HMMs) is one of the most successful models
for sequential data that is best known for speech
recognition (Rabiner, 1989). Recently, HMMs
have been applied to a variety of applications out-
side of speech recognition, such as handwriting
recognition (Nag et al., 1986; Kundu and Bahl,
1988) and fault-detection (Smyth, 1994). The
variants and extensions of HMMs also include
language models (Guyon and Pereira, 1995) and
econometrics (Garcia and Perron, 1996).

In order to properly capture more complex lin-
guistic phenomena, a variety of neural networks
have been proposed, such as neural probabilistic
language model (Bengio et al., 2006), recurrent
neural network (Mikolov et al., 2010) and recur-
sive neural tensor network (Socher et al., 2013).
As opposed to the work that only focuses on the
context of the sequential data, some studies have
been proposed to incorporate more general fea-
tures associated with the context. Ghahramani and
Jordan (1997) proposes a factorial HMMs method
and it has been successfully utilized in natural lan-
guage processing (Duh, 2005), computer vision
(Wang and Ji, 2005) and speech processing (Gael
et al., 2009). However, exact inference and param-
eter estimation in factorial HMMs is intractable,

thus the learning algorithm is difficult to imple-
ment and is limited to the study of real-valued data
sets.

3 The DMNN Model

In this section, we describe our general framework
for incorporating sequential data and an arbitrary
set of features into language modeling.

3.1 Generative model

Given a time sequence t = 1, 2, 3, . . . , n, we as-
sociate each time slice with an observation (st, ut)
and a state label yt. Here, st represents the sen-
tence at time t, and ut represents additional fea-
tures. Additional features may include the author
of the sentence, the bag-of-word features and other
semantic features. The label yt is the item that we
want to predict. It might be the topic of the sen-
tence, or the sentiment of the author.

Given tuples (st, ut, yt), it is natural to build a
supervised classification model to predict yt. Re-
current neural networks have been shown effective
in modeling temporal NLP data. However, due to
the depth of the time sequence, training a single
RNN is difficult. When the time sequence length
n is large, the RNN model suffers from many prac-
tical problems, including the vanishing gradient is-
sue which makes the training process inefficient.

We propose a Deep Markov Neural Network
(DMNN) model. The DMNN model introduces
a hidden state variable Ht for each time slice. It
serves as an intermediate layer connecting the la-
bel yt and the observation (st, ut). These hidden
variables disentangle the correlation between neu-
ral networks for each sentence, but preserving time
series dependence. The time series dependence is
modeled by a Markov chain. In particular, we as-
sume that there is a labeling matrix L such that

P (yt = i|Ht = j) = Lij (1)
and a transition matrix T such that

P (Ht+1 = i|Ht = j) = Tij (2)

These two equations establish the relation be-
tween the hidden state and the labels. On the
other hand, we use a neural network model M to
model the relation between the hidden states and
the observations. The neural network model takes
(Ht−1, st, ut) as input, and predict Ht as its out-
put. In particular, we use a logistic model to define
the probability:

33



P (Ht = i|Ht−1, st, ut) ∝ (3)
exp((wih, φ(Ht−1)) + (w

i
u, ϕ(ut)) + (w

i
sN(st) + b))

The vectors wh, wu, ws are linear combination
coefficients to be estimated. The functions φ, ϕ
and function N turn Ht−1, ut and st into fea-
turized vectors. Among these functions, we rec-
ommend choosing φ(Ht−1) to be a binary vector
whose Ht−1-th coordinate is one and all other co-
ordinates are zeros. Both function ϕ and function
N are modeled by deep neural networks.

Since the sentence st has varied lengths and
distinct structures, choosing an appropriate neural
network to extract the sentence-level feature is a
challenge task. In this paper, we choose N to be
the recursive autoencoder (Socher et al., 2011a),
which explicitly takes structure of the sentence
into account. The network for defining ϕ can be
a standard fully connect neural network.

3.2 Estimating Model Parameters
There are two sets of parameters to be estimated:
the parameters L, T for the Markov chain model,
and the parameters wh, wu, ws, ϕ,N for the deep
neural networks. The training is performed in two
phases. In the first phase, the hidden states {Ht}
are estimated based on the labels {yt}. The emis-
sion matrix L and the transition matrix T are es-
timated at the same time. This step can be done
by using the Baum-Welch algorithm (Baum et al.,
1970; Baum, 1972) for learning hidden Markov
models.

When the hidden states {Ht} are obtained, the
second phase estimates the remaining parameters
for the neural network model in a supervised pre-
diction problem. First, we use available sentences
to train the structure of the recursive neural net-
workN . This step can be done without using other
information besides {st}. After the structure of N
is given, the remaining task is to train a supervised
prediction model to predict the hidden stateHt for
each time slice. In this final step, the parameters to
be estimated are wh, wu, wsand the weight coeffi-
cients in neural networks N and ϕ. By maximiz-
ing the log-likelihood of the prediction, all model
parameters can be estimated by stochastic gradient
descent.

3.3 Prediction
The prediction procedure is a reverse of the train-
ing procedure. For prediction, we only have the

sentence st and the additional feature ut. By equa-
tion (3), we use (s1, u1) to predict H1, then use
(H1, s2, u2) to predict H2. This procedure contin-
ues until we have reached Hn. Note that each Ht
is a random variable. Equation (3) yields

P (Ht = i|s, u) =
∑

j

P (Ht = i|st, ut, Ht−1 = j)

· P (Ht−1 = j|s, u) (4)
This recursive formula suggests inferring the

probability distribution P (Ht|s, u) one by one,
starting from t = 1 and terminate at t = n. After
P (Ht|s, u) is available, we can infer the probabil-
ity distribution of yt as

P (yt = i|s, u) =
∑

j

P (yt = i|Ht = j)P (Ht = j|s, u)

=
∑

j

Li,jP (Ht = j|s, u) (5)

which gives the prediction for the label of interest.

3.4 Application: Sentiment analysis in
conversation

Sentiment analysis for dialogues is a typical se-
quential data modeling problem.The sentiments
and topics expressed in a conversation affect
the interaction between dialogue participants
(Suin Kim, 2012). For example, given a user say
that “I have had a high fever for 3 days”, the user
may write back positive-sentiment response like “I
hope you feel better soon”, or it could be negative-
sentiment content when the response is “Sorry, but
you cannot join us today” (Hasegawa et al., 2013).
Incorporating the session’s sequential information
into sentiment analysis may improve the predic-
tion accuracy. Meanwhile, each participate in the
dialogue usually has specific sentiment polarities
towards different topics.

In this paper, the sequential labels available to
the framework include topics and sentiments. In
the training dataset, topics are obtained by run-
ning an LDA model, while the sentiment labels are
manually labeled. The feature includes the iden-
tity of the author. In the training phase, the hid-
den Markov model is trained on the sequential la-
bels, resulting in transition probabilities and hid-
den states at each time step. Then, the recursive
autoencoders (Socher et al., 2011a) is trained, tak-
ing words, the identity of the author and hidden
state at the previous time step as input, to predict
the hidden states at the present time step. The pro-
cedure is reversed in the testing phase: the neu-
ral network predicts the hidden states using words

34



and the identity of the author, then the hidden
Markov model predicts the observation using hid-
den states.

4 Experiments

To evaluate our model, we conduct experiments
for sentiment analysis in conversations.

4.1 Datasets

We conduct experiments on both English and Chi-
nese datasets. The detailed properties of the
datasets are described as follow.
Twitter conversation (Twitter): The original
dataset is a collection of about 1.3 million conver-
sations drawn from Twitter by Ritter et al. (2010).
Each conversation contains between 2 and 243
posts. In our experiments, we filter the data by
keeping only the conversations of five or more
tweets. This results in 64,068 conversations con-
taining 542,866 tweets.
Sina Weibo conversation (Sina): since there is
no authoritative publicly available Chinese short-
text conversation corpus, we write a web crawler
to grab tweets from Sina Weibo, which is the
most popular Twitter-like microblogging website
in China1. Following the strategy used in (Rit-
ter et al., 2010), we crawled Sina Weibo for a 3
months period from September 2013 to Novem-
ber 2013. Filtering the conversations that contain
less than five posts, we get a Chinese conversa-
tion corpus with 5,921 conversations containing
37,282 tweets.

For both datasets, we set the ground truth of sen-
timent classification of tweets by using human an-
notation. Specifically, we randomly select 1000
conversations from each datasets, and then invite
three researchers who work on natural language
processing to label sentiment tag of each tweet
(i.e., positive, negative or neutral) manually. From
3 responses for each tweet, we measure the agree-
ment as the number of people who submitted the
same response. We measure the performance of
our framework using the tweets that satisfy at least
2 out of 3 agreement.

For both datasets, data preprocessing is per-
formed. The words about time, numeral words,
pronoun and punctuation are removed as they are
unrelated to the sentiment analysis task.

1http://weibo.com

Dataset SVM NBSVM RAE Mesnil’s DMNN
Twitter 0.572 0.624 0.639 0.650 0.682

Sina 0.548 0.612 0.598 0.626 0.652

Table 1: Three-way classification accuracy

4.2 Baseline methods

To evaluate the effectiveness of our framework
on the application of sentiment analysis, we com-
pare our approach with several baseline methods,
which we describe below:
SVM: Support Vector Machine is widely-used
baseline method to build sentiment classifiers
(Pang et al., 2002). In our experiment, 5000 words
with greatest information gain are chosen as fea-
tures, and we use the LibLinear2 to implement
SVM.
NBSVM: This is a state-of-the-art performer on
many sentiment classification datasets (Wang and
Manning, 2012). The model is run using the pub-
licly available code3.
RAE: Recursive Autoencoder (Socher et al.,
2011b) has been proven effective in many senti-
ment analysis tasks by learning compositionality
automatically. The RAE model is run using the
publicly available code4 and we follow the same
setting as in (Socher et al., 2011b).
Mesnil’s method: This method is proposed in
(Mesnil et al., 2014), which achieves the strongest
results on movie reviews recently. It is a ensem-
ble of the generative technique and the discrimi-
native technique. We run this algorithm with pub-
licly available code 5.

4.3 Experiment results

In our HMMs component, the number of hidden
states is 80. We randomly initialize the matrix
of state transition probabilities and the initial state
distribution between 0 and 1. The emission prob-
abilities are determined by Gaussian distributions.
In our recursive autoencoders component, we rep-
resent each words using 100-dimensional vectors.
The hyperparameter used for weighing reconstruc-
tion and cross-entropy error is 0.1.

For each dataset, we use 800 conversations as
the training data and the remaining are used for
testing. We summarize the experiment results in

2http://www.csie.ntu.edu.tw/~cjlin/liblinear/
3http://nlp.stanford.edu/~sidaw
4https://github.com/sancha/jrae/zipball/stable
5https://github.com/mesnilgr/iclr15.

35



Table 1. According to Table 1, the proposed ap-
proach significantly and consistently outperforms
other methods on both datasets. This verifies the
effectiveness of the proposed approach. For exam-
ple, the overall accuracy of our algorithm is 3.2%
higher than Mesnil’s method and 11.0% higher
than SVM on Twitter conversations dataset. For
the Sina Weibo dataset, we observe similar results.
The advantage of our model comes from its capa-
bility of exploring sequential information and in-
corporating an arbitrary number of factors of the
corpus.

5 Conclusion and Future Work

In this paper, we present a general framework
for incorporating sequential data into language
modeling. We demonstrate the effectiveness of
our method by applying it to a specific appli-
cation: predicting topics and sentiments in dia-
logues. Experiments on real data demonstrate that
our method is substantially more accurate than
previous methods.

References
Pierre Baldi and Søren Brunak. 2001. Bioinformatics:

the machine learning approach. MIT press.

Leonard E Baum, Ted Petrie, George Soules, and Nor-
man Weiss. 1970. A maximization technique occur-
ring in the statistical analysis of probabilistic func-
tions of markov chains. The annals of mathematical
statistics, pages 164–171.

Leonard E Baum. 1972. An equality and associ-
ated maximization technique in statistical estimation
for probabilistic functions of markov processes. In-
equalities, 3:1–8.

Yoshua Bengio, Holger Schwenk, Jean-Sébastien
Senécal, Fréderic Morin, and Jean-Luc Gauvain.
2006. Neural probabilistic language models. In
Innovations in Machine Learning, pages 137–186.
Springer.

Kevin Duh. 2005. Jointly labeling multiple sequences:
A factorial hmm approach. In Proceedings of the
ACL Student Research Workshop, pages 19–24. As-
sociation for Computational Linguistics.

Jurgen V Gael, Yee W Teh, and Zoubin Ghahramani.
2009. The infinite factorial hidden markov model.
In Advances in Neural Information Processing Sys-
tems, pages 1697–1704.

René Garcia and Pierre Perron. 1996. An analysis of
the real interest rate under regime shifts. The Review
of Economics and Statistics, pages 111–125.

Zoubin Ghahramani and Michael I Jordan. 1997. Fac-
torial hidden markov models. Machine learning,
29(2-3):245–273.

Isabelle Guyon and Fernando Pereira. 1995. Design
of a linguistic postprocessor using variable memory
length markov models. In Document Analysis and
Recognition, 1995., Proceedings of the Third Inter-
national Conference on, volume 1, pages 454–457.
IEEE.

Takayuki Hasegawa, Nobuhiro Kaji, Naoki Yoshinaga,
and Masashi Toyoda. 2013. Predicting and eliciting
addressee’s emotion in online dialogue. In ACL (1),
pages 964–972.

Anil K Jain, Robert P. W. Duin, and Jianchang Mao.
2000. Statistical pattern recognition: A review.
Pattern Analysis and Machine Intelligence, IEEE
Transactions on, 22(1):4–37.

Hye-Chung Monica Kum, Susan Paulsen, and Wei
Wang. 2005. Comparative study of sequential pat-
tern mining models. In Foundations of Data Mining
and knowledge Discovery, pages 43–70. Springer.

Amlan Kundu and Paramrir Bahl. 1988. Recognition
of handwritten script: a hidden markov model based
approach. In Acoustics, Speech, and Signal Process-
ing, 1988. ICASSP-88., 1988 International Confer-
ence on, pages 928–931. IEEE.

David D Lewis and William A Gale. 1994. A se-
quential algorithm for training text classifiers. In
Proceedings of the 17th annual international ACM
SIGIR conference on Research and development in
information retrieval, pages 3–12. Springer-Verlag
New York, Inc.

Grégoire Mesnil, Marc’Aurelio Ranzato, Tomas
Mikolov, and Yoshua Bengio. 2014. Ensemble
of generative and discriminative techniques for sen-
timent analysis of movie reviews. arXiv preprint
arXiv:1412.5335.

Tomas Mikolov, Martin Karafiát, Lukas Burget, Jan
Cernockỳ, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In IN-
TERSPEECH, pages 1045–1048.

R Nag, K Wong, and Frank Fallside. 1986. Script
recognition using hidden markov models. In Acous-
tics, Speech, and Signal Processing, IEEE Interna-
tional Conference on ICASSP’86., volume 11, pages
2071–2074. IEEE.

Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using
machine learning techniques. In Proceedings of the
ACL-02 conference on Empirical methods in natural
language processing-Volume 10, pages 79–86. As-
sociation for Computational Linguistics.

Lawrence Rabiner. 1989. A tutorial on hidden markov
models and selected applications in speech recogni-
tion. Proceedings of the IEEE, 77(2):257–286.

36



Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Un-
supervised modeling of twitter conversations.

Padhraic Smyth. 1994. Hidden markov models for
fault detection in dynamic systems. Pattern recog-
nition, 27(1):149–164.

Richard Socher, Jeffrey Pennington, Eric H Huang,
Andrew Y Ng, and Christopher D Manning. 2011a.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 151–161. Association for
Computational Linguistics.

Richard Socher, Jeffrey Pennington, Eric H. Huang,
Andrew Y. Ng, and Christopher D. Manning. 2011b.
Semi-Supervised Recursive Autoencoders for Pre-
dicting Sentiment Distributions. In Proceedings of
the 2011 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP).

Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng,
and Christopher Potts. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 1631–1642. Citeseer.

Alice Oh Suin Kim, JinYeong Bak. 2012. Discover-
ing emotion influence patterns in online social net-
work conversations. In SIGWEB ACM Special In-
terest Group on Hypertext, Hypermedia, and Web.
ACM.

Peng Wang and Qiang Ji. 2005. Multi-view face track-
ing with factorial and switching hmm. In Applica-
tion of Computer Vision, 2005. WACV/MOTIONS’05
Volume 1. Seventh IEEE Workshops on, volume 1,
pages 401–406. IEEE.

Sida Wang and Christopher D Manning. 2012. Base-
lines and bigrams: Simple, good sentiment and topic
classification. In Proceedings of the 50th Annual
Meeting of the Association for Computational Lin-
guistics: Short Papers-Volume 2, pages 90–94. As-
sociation for Computational Linguistics.

37


