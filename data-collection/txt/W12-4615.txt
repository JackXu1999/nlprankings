



















































PLCFRS Parsing Revisited: Restricting the Fan-Out to Two


Proceedings of the 11th International Workshop on Tree Adjoining Grammars and Related Formalisms (TAG+11), pages 126–134,
Paris, September 2012.

PLCFRS Parsing Revisited: Restricting the Fan-Out to Two

Wolfgang Maier, Miriam Kaeshammer and Laura Kallmeyer
Institut für Sprache und Information

University of Düsseldorf
Universitätsstr. 1, 40225 Düsseldorf, Germany

{maierwo,kaeshammer,kallmeyer}@phil.hhu.de

Abstract

Linear Context-Free Rewriting System
(LCFRS) is an extension of Context-Free
Grammar (CFG) in which a non-terminal
can dominate more than a single continu-
ous span of terminals. Probabilistic LCFRS
have recently successfully been used for
the direct data-driven parsing of discontin-
uous structures. In this paper we present a
parser for binary PLCFRS of fan-out two,
together with a novel monotonous estimate
for A∗ parsing, with which we conduct ex-
periments on modified versions of the Ger-
man NeGra treebank and the Discontinuous
Penn Treebank in which all trees have block
degree two. The experiments show that
compared to previous work, our approach
provides an enormous speed-up while de-
livering an output of comparable richness.

1 Introduction

In many constituency treebanks, the syntactic an-
notation takes the form of Context-Free Grammar
(CFG) derivation trees, i.e., of trees with no cross-
ing branches. Discontinuous structures (Huck and
Ojeda, 1987) cannot be modeled with CFG and
are therefore handled by an additional mecha-
nism in such an annotation. In the Penn Tree-
bank (PTB) (Marcus et al., 1993), for instance, a
combination of trace nodes and co-indexation la-
bels is used in order to establish implicit edges.
In other treebanks, e.g., the German NeGra (Skut
et al., 1997) and TIGER (Brants et al., 2002)
treebanks, crossing branches are allowed.1 This
way, all parts of a discontinuous constituent can

1The annotation differences between TIGER and NeGra
are minor and can be neglected for the purpose of this work.

be grouped under a single node. There is no
fundamental difference between both represen-
tations: PTB-style annotation can be converted
into a NeGra/TIGER-style annotation. This has
been done in the Discontinuous Penn Treebank
(DPTB) (Evang and Kallmeyer, 2011).

For data-driven parsing with Probabilistic CFG
(PCFG), the annotation information concerning
discontinuities must be discarded, because it ex-
ceeds the expressivity of CFG. For NeGra, there
exist two methods, namely (i) attaching non-
head daughters of discontinuous constituents to
higher positions in the tree, such that the crossing
branches disappear (the NeGra distribution con-
tains a version of the treebank in which this trans-
formation is readily carried out), or (ii) introduc-
ing an additional non-terminal node for each con-
tinuous part of a discontinuous constituent (Boyd,
2007). As an example, figure 1 shows the annota-
tion of (1) before and after both transformations.

(1) Der
The

CD
CD

wird
will

bald
soon

ein
a

Buch
book

folgen
follow

“Soon, the CD will be followed by a book.”

For PCFG parsing with the PTB, trace nodes
and co-indexation are simply discarded. With ei-
ther of these transformations, discontinuities are
lost and cannot be restored from the parser output.
However, the fact that about 25%, resp. 20% of
all sentences in NeGra, resp. the PTB contain dis-
continuities (Maier and Lichte, 2011; Evang and
Kallmeyer, 2011) shows that this is an undesir-
able situation and that these structures warrant a
proper treatment.

Linear Context-Free Rewriting System
(LCFRS), an extension of CFG, has been estab-
lished as an appropriate candidate for modeling

126



Der
ART

CD
NN

wird
VAFIN

bald
ADV

ein
ART

Buch
NN

folgen
VVINF

NK NK

NP

DA MO HD

VP

NK NK

NP

OCHD SB

S

Crossing branches, original NeGra tree

Der
ART

CD
NN

wird
VAFIN

bald
ADV

ein
ART

Buch
NN

folgen
VVINF

NK NK

NP

DA MO

HD

VP

NK NK

NP

OCHD SB

S

Resolved crossing branches, NeGra distribution

Der
ART

CD
NN

wird
VAFIN

bald
ADV

ein
ART

Buch
NN

folgen
VVINF

NK NK

NP

DA MO HD

VP*VP*VP*

NK NK

NP

OCOCHD SB

S

OC

Resolved crossing branches, Boyd (2007)

Figure 1: Crossing branches removal for NeGra. Note
that the argument structure is changed as a result of the
removal of the crossing branches.

discontinuities (Maier and Lichte, 2011). In
LCFRS, a single non-terminal can span k ≥ 1
continuous blocks of a string. A CFG is simply
a special case of an LCFRS in which k = 1. k
is called the fan-out of the non-terminal, and a
corresponding constituent is said to have block
degree k. It has been shown that probabilistic
data-driven parsing on the basis of Probabilistic
LCFRS (PLCFRS) is feasible and gives good re-
sults while preserving discontinuity information
(Kallmeyer and Maier, 2010; Maier, 2010; van
Cranenburgh et al., 2011; Evang and Kallmeyer,
2011; van Cranenburgh, 2012; Maier, 2012).

The major problem of PLCFRS parsing is
its high computational complexity. A binarized
PCFG can be parsed in O(n3), parsing a bina-
rized LCFRS takes O(n3k) (Seki et al., 1991),
where k is the fan-out of the grammar (the max-
imal fan-out of any of its non-terminals). The
parsers from the literature allow for an unbounded
k. This leads to parsing times beyond practically
acceptable values for sentences longer than 25 to
30 words.

In this paper, our goal is to show that by re-

stricting the block degree, resp. the fan-out to two,
(i) one can express almost all the information con-
tained in the discontinuous treebank annotation of
NeGra and the DPTB, and (ii) one can obtain a
parser which is faster by an order of magnitude.

We proceed as follows. In section 2, we present
definitions of PLCFRS, as well as of trees and
the notion of block degree. In section 3, we de-
scribe how to bring the trees of both the DPTB
and NeGra to block degree two. Unlike the trans-
formations used for PCFG parsing, our transfor-
mations preserve the discontinuity information in
almost all cases. Section 4 introduces PLCFRS
as a formalism for data-driven parsing. In sec-
tion 5, we present a data-driven parser for binary
PLCFRS of fan-out two which uses an efficient
case-by-case strategy, together with a new outside
estimate for A∗ parsing. Section 6 contains exper-
iments on the transformed NeGra as well as on the
Discontinuous Penn Treebank. We use both the
new parser and rparse, the parser used in our
previous work (Kallmeyer and Maier, 2010). Our
experiments show that given equal conditions, we
achieve an enormous speed-up while obtaining an
output of a comparable richness. Finally, section
7 concludes the article.

2 Definitions

We notate LCFRS with the syntax of Simple
Range Concatenation Grammars (SRCG) (Boul-
lier, 1998), a formalism equivalent to LCFRS.

An LCFRS (Vijay-Shanker et al., 1987) is a
tuple G = (N,T, V, P, S) where a) N is a fi-
nite set of non-terminals with a function dim:
N → N determining the fan-out of each A ∈ N ;
b) T and V are disjoint finite sets of terminals
and variables; c) S ∈ N is the start symbol with
dim(S) = 1; d) P is a finite set of rewriting rules

A(α1, . . . , αdim(A))→ A1(X(1)1 , . . . , X
(1)
dim(A1)

)

· · ·Am(X(m)1 , . . . , X
(m)
dim(Am)

)

where A,A1, . . . , Am ∈ N , X(i)j ∈ V for 1 ≤
i ≤ m, 1 ≤ j ≤ dim(Ai) and αi ∈ (T ∪ V )∗ for
1 ≤ i ≤ dim(A), for a rank m ≥ 0. For all r ∈
P , every variable X occurring in r occurs exactly
once in the left-hand side (LHS) and exactly once
in the right-hand side (RHS). The rank of G is the
maximal rank of any of its rules, its fan-out is the
maximal fan-out of any of its non-terminals. If
G has rank u and fan-out v, then G is an (u, v)-
LCFRS.

127



A(ab, cd)→ ε (〈ab, cd〉 in yield of A)
A(aXb, cY d)→ A(X,Y ) (if 〈X,Y 〉 in yield of A,

then also 〈aXb, cY d〉 in
yield of A)

S(XY )→ A(X,Y ) (if 〈X,Y 〉 in yield of A,
then 〈XY 〉 in yield of
S)

L = {anbncndn |n > 0}

Figure 2: Sample LCFRS

A rewriting rule describes how to compute the
yield of the LHS non-terminal from the yields of
the RHS non-terminals. The yield of S is the lan-
guage of the grammar. See figure 2 for a sample
LCFRS.

A probabilistic LCFRS (PLCFRS) is a tuple
〈N,T, V, P, S, p〉 such that 〈N,T, V, P, S〉 is a
LCFRS and p : P → [0..1] a function such that
for all A ∈ N : ΣA(~x)→~Φ∈P p(A(~x)→ ~Φ) = 1.

A tree over a sentence w = w1 · · ·wn, n ≥ 1,
is a labeled ordered directed graph D = (V,E, r)
with V a set of nodes, E : V × V a set of edges
and r ∈ V a single dedicated root node, where
every v ∈ V \ {r} has exactly one incoming
edge and r has no incoming edges. All vl ∈ V
with no outgoing edges are called leaves or ter-
minals, and Vl is the set of all leaves or termi-
nals. The labeling of D is given by a function
Λ : V → N ∪ {1, . . . , n}, where N a set of
non-terminal labels, for all vi ∈ Vl, 1 ≤ i ≤ n,
Λ(vi) = i, and for all v ∈ V \ Vl, Λ(v) ∈ N .
The function π gives the yield of the node; more
precisely, for all v ∈ V , π(v) = {i ∈ Λ(u) |
u ∈ V is a leaf and there is a 〈v, u〉 ∈ E∗}. The
ordering of D is given by the relation ≺ which is
such that for all v1, v2, v1 ≺ v2 iff min(π(v1)) ≤
min(π(v2)).

The yield blocks of v are given by a partition
of π(v) into maximal continuous sequences of in-
tegers. The block degree of v is the number of
blocks of v, its gap degree is its block degree mi-
nus one. A gap of v is a tuple (i, k) such that
i ∈ π(v), k + 1 ∈ π(v) and j /∈ π(v) for
i+ 1 ≤ j ≤ k.

3 Treebanks with Block-Degree Two

3.1 Removing Spurious Gaps
In the DPTB as used by Evang and Kallmeyer
(2011),2 the maximal block degree is three. Mo-

2Thanks to Kilian Evang for providing us with his origi-
nal data.

tivated by the suspicion (Evang, p.c.) that the
cases of block degree three are spurious, i.e.,
caused only by punctuation, we move all punc-
tuation terminals to the least common ancestor of
their resp. left and right non-punctuation termi-
nal neighbors. This is essentially the algorithm
of Levy (2005), pp. 163. It leaves us with only
11 sentences containing nodes with more than one
(non-spurious) gap. For our experiments, we re-
move those sentences; an investigation of their
properties is left for future work.

In the NeGra annotation, punctuation and a
very small number of other elements such as parts
of ungrammatical sentences are not included in
the annotation, i.e., the corresponding nodes are
attached at the root node. They cause a very
high, linguistically meaningless block degree of
40. In order to avoid gaps which contain noth-
ing but those elements, we attach them lower.3

Since aside from (punctuation) terminals, non-
terminals may be concerned, we extend Levy’s
strategy as follows. Let n be a node origi-
nally attached to the root node, furthermore let
nl1 , . . . , nlk , nr1 , . . . , nrm , k,m ≥ 0, be all left,
resp. right siblings of n for which it holds that
both Sl = {min(π(n))} ∪ (

⋃k
i=1 π(nli)) and

Sr = {max (π(n))}∪(
⋃m
j=1 π(nrj )) are continu-

ous sequences of integers. We select as an attach-
ment target the least common ancestor node of the
terminals tl, tr with π(tl) = {(min(Sl)−1)} and
π(tr) = {(max (Sr) + 1)}. If tr or tl do not exist,
we do not move n. This algorithm improves over
the strategy from Maier (2012), pp. 189, in the
sense that the latter does not remove all spurious
gaps. We call the new strategy T1.

3.2 Block Degree Two for NeGra

For NeGra, we introduce a novel series of linguis-
tically motivated transformations which ensures
that all resulting trees have block degree two. The
block degrees of the treebank after each transfor-
mation are listed in table 1.

Verbs There is no consensus about the analysis
of German verb phrases (VPs) and auxiliaries in
particular, cf. Bouma and van Noord (1998) for a
discussion. In the interest of a small block degree
of the trees in NeGra, we change the VP anno-

3This is a necessary preprocessing step for PCFG parsing
as well since those elements are equally unattached in the
version of NeGra with resolved crossing branches.

128



Der
ART

CD
NN

wird
VAFIN

bald
ADV

ein
ART

Buch
NN

folgen
VVINF

NK NK

NP

DA MO HD

VP

NK NK

NP

HD

AUX

SB

S

Figure 3: Verb transformation on original tree from
fig. 1

tation principles and group auxiliary verbs under
the same VP as the corresponding full verb. We
furthermore insert a VP for all finite verbs and
their dependents except the subject (identified by
the labels in the treebank), since in the original
NeGra annotation, only non-finite verbs project to
a VP. See figure 3 for an example.

The positions of the newly introduced VPs in-
fluences the attachment points for the punctuation
attachment. As the second transformation T2 we
therefore first perform the verb transformation we
just described, followed by T1.

Parentheticals Parenthetical sentences such as
(2) are annotated as embedding the enclosing sen-
tence and therefore lead to an additional gap in the
latter.

(2) . . . ,
. . . ,

so
as

argumentierten
argued

die
the

Richter,
judges,

. . .

. . .
“. . . , the judges argued, . . . ”

We structurally identify them (not lexically) and
attach them as low as necessary such that they do
not create a gap. This is motivated by the anno-
tation of TüBa-D/Z (Telljohann et al., 2012), an-
other German treebank, where parenthetical sen-
tences are left unattached. T3 consists of T2, fol-
lowed by the parenthetical transformation.

Remainder Eventually, T4 consists of T3 fol-
lowed by a transformation inspired by the stan-
dard crossing branches resolution for NeGra. We
re-attach material to higher positions iff it causes
a block degree higher than two. Thereby, we first
consider sentential modifiers, then modifiers in
general and only finally constituents of any sort.
T4 only treats a tiny fraction of all sentences; how-
ever, it does change structures for which a block
degree higher than two can be linguistically jus-
tified, such as di-transitive adjectives and verbs
in particular word order configurations. A more

Blocks Orig. T1 T2 T3 T4

1 7,704 14,927 10,898 10,944 10,944
2 5,275 4,988 9,333 9,361 9,658

3 3,585 679 370 297 -
4 1,917 8 1 - -
5 998 - - - -
≥ 6 1123 - - - -

Table 1: Number of sentences in NeGra with a certain
block degree before and after transformations

careful investigation of T4 is left for future work.

4 PLCFRS for Data-Driven Parsing

LCFRSs can be extracted directly from treebanks
with a direct annotation of discontinuities (Maier
and Søgaard, 2008). The difference between
treebank PLCFRS and PCFG extraction is, intu-
itively, that in PLCFRS variables are used to de-
scribe the blocks which are dominated by a non-
terminal. In other words, an argument boundary
in a production corresponds to a block bound-
ary of the corresponding non-terminal in the tree,
and the fan-out of an extracted rule is equal to
the block degree of the treebank non-terminal
corresponding to the rule’s LHS non-terminal.
Consider again the original tree from figure 1.
From the discontinuous VP Der CD . . . bald
. . . folgen we extract the rule VP(X1, X2, X3)→
NP(X1)ADV (X2)VVINF (X3). The LHS non-
terminal has fan-out three due to the fact that the
VP has block degree three.

When applied to a treebank of block degree
two, the extraction algorithm yields grammars of
fan-out two. In order to obtain a (2, 2)-PLCFRS,
i.e., for rank reduction, we use the optimal bina-
rization algorithm of Kallmeyer (2010), p. 150,
which yields a minimal fan-out, resp. number of
variables per binarized rule. As in PCFG pars-
ing, we use markovization (Kallmeyer and Maier,
2010). We use standard Maximum Likelihood es-
timation. See section 6 for further experimental
details.

5 A CYK Parser for (2, 2)-PLCFRS

5.1 The Parser

Just as Kallmeyer and Maier (2010), we use a
probabilistic CYK parser (Seki et al., 1991). The
general CYK deduction system is shown in figure
4. Its items have the form [A, ~ρ], with A ∈ N and

129



Scan:
0 : [A, 〈〈i, i+ 1〉〉] A POS tag of wi+1

Unary:
in : [B, ~ρ]

in+ | log(p)| : [A, ~ρ] p : A(~ρ)→ B(~ρ) ∈ P

Binary:
inB : [B, ~ρB ], inC : [C, ~ρC ]

inB + inC + | log(p)| : [A, ~ρA]
where p : A( ~ρA) → B( ~ρB)C( ~ρC) is an instantiated
rule.
Goal: [S, 〈〈0, n〉〉]

Figure 4: Weighted CYK deduction system for LCFRS

ID Type G30T E30

1 A(X)→ B(X) 49 235
2 A(X,Y)→ B(X,Y) 1 4
3 A(XY)→ B(X) C(Y) 14,430 11,777
4 A(X,Y)→ B(X) C(Y) 1,644 312
5 A(XYZ)→ B(X,Z) C(Y) 621 205
6 A(X,YZ)→ B(X,Y) C(Z) 100 45
7 A(X,YZ)→ B(X,Z) C(Y) 142 94
8 A(XY,Z)→ B(X,Z) C(Y) 172 10
9 A(XY,Z)→ B(X) C(Y,Z) 582 108
10 A(XY,ZU)→ B(X,Z) C(Y,U) 7 0
11 A(XY,ZU)→ B(X,U) C(Y,Z) 0 0
12 A(X,YZU)→ B(X,Z) C(Y,U) 12 3
13 A(XYZ,U)→ B(X,Z) C(Y,U) 12 2
14 A(XYZU)→ B(X,Z) C(Y,U) 13 6

Figure 5: LCFRS rule types and numbers of occur-
rence in binarized grammars (cf. section 6)

~ρ a vector of ranges characterizing all components
of the span of A. We specify a simpler, special-
ized deduction system which takes advantage of
the fact that due to our maximum fan-out of two,
we can rely on only encountering rules of certain
forms. The second column of figure 5 schemati-
cally displays all 14 different rule types the parser
must handle.

In the specialized deduction system, unary
items now take the form [A, i, j] and binary items
take the form [A, i, j, k, l], where A ∈ N and
i, j, resp. k, l are spans dominated by A with
0 ≤ i < j < k < l ≤ n. The goal item is
[S, 0, n]. We replace the old Unary and Binary
deduction rules in figure 4 with 14 new rules, one
per production type. Figure 6 shows the new scan
rule and the complete rules for type 1, type 6 and
type 10, which should make the basic idea clear.
Note that there is no need to refer to instantiations
anymore. Our case-by-case strategy is similar to
the one employed by Kato et al. (2006).

As in our previous work, we specify the set

Scan:
0 : [A, i, i+ 1]

A POS tag of wi+1

Complete1:
in : [B, i, j]

in+ | log(p)| : [A, i, j]
where p : A(X)→ B(X) ∈ P .

Complete6:
inB : [B, i, j, k, l], inC : [C, l, u]

inB + inC + | log(p)| : [A, i, j, k, u]
where p : A(X,Y Z)→ B(X,Y )C(Z) ∈ P .

Complete10:
inB : [B, i, x, k, y], inC : [C, x, j, y, l]
inB + inC + | log(p)| : [A, i, j, k, l]

where p : A(XY,ZU)→ B(X,Z)C(Y,U) ∈ P

Figure 6: Weighted CYK deduction rules for 2-LCFRS

of parse items using the algorithm of weighted
deductive parsing (WDP) (Nederhof, 2003). In
WDP, one maintains a priority queue of items,
sorted by the resp. Viterbi inside scores. The top-
most item is always processed first. WDP guaran-
tees optimality, i.e., that the best parse is found.

5.2 A Novel Outside Estimate

One can speed up parsing by adding to the Viterbi
inside score of an item an estimate of its Viterbi
outside score, in other words, an estimate of the
cost of completion of the item to a complete
parse. This has proven to be successful for both
PCFG (Klein and Manning, 2003) and PLCFRS
(Kallmeyer and Maier, 2010). As outside esti-
mate, one uses the outside probability of a sum-
mary of the item, i.e., of an equivalence class
of parse items. The difficulty for PLCFRS is to
choose the summary such that optimality is main-
tained through the two estimate properties ad-
missibility and monotonicity (Klein and Manning,
2003).

Here, we present the novel LN estimate, which
is based on a summary that records only the sum
of the span lengths and the length of the entire
sentence. It is the first practically computable es-
timate which allows for maintaining optimality.

The estimate is computed offline up to a cer-
tain maximal sentence length lenmax. We spec-
ify the estimate computation with the deduction
system in figure 7.4 The items have the form
[X, len, slen] with X ∈ N , dim(X) ≤ len ≤
slen . The value in(X, l) for a non-terminal X
and a length l, 0 ≤ l ≤ lenmax is an estimate of

4A simpler deduction system for the estimate computa-
tion for (2, 2)-LCFRS would be possible as well, along the
lines of the simplification of the CYK parser.

130



Axiom :
0:[S,len,len]

1≤len≤lenmax

Unary:
w:[X,lX ,slen]

w+| log(p)|:[A,lX ,slen]
where p : X(~α)→ A(~α) ∈ P

Binary-right:
w:[X,lX ,slen]

w+in(A,lX−lB)+| log(p)|:[B,lB ,slen]

Binary-left:
w:[X,lX ,slen]

w+in(B,lX−lA)+| log(p)|:[A,lA,slen]
where, for both rules,

p : X(~α)→ A( ~αA)B( ~αB) ∈ P .

Figure 7: LN estimate (span and sentence length)

POS tags:
0:[A,1]

A a POS tag

Unary:
in:[B,l]

in+| log(p)|:[A,l]
p:A(~α)→B(~α)∈P

Binary:
inB :[B,lB ],inC :[C,lC ]

inB+inC+| log(p)|:[A,lB+lC ]
where either p : A( ~αA)→ B( ~αB)C( ~αC) ∈ P or

p : A( ~αA)→ C( ~αC)B( ~αB) ∈ P .

Figure 8: Inside estimate with total span length

the inside score of an X category with a span of
length l. Its computation is specified in figure 8.

The outside estimate for a sentence length n
and for some predicate C with a span ~ρ =
〈〈l1, r1〉, . . . , 〈ldim(C), rdim(C)〉〉 where len =
Σ
dim(C)
i=1 (ri − li) is then the minimal weight of

[C, len, n].

We will show in the following that the LN esti-
mate maintains optimal search by being both ad-
missible and monotonic. Since the weight of the
outside estimate for an item is always lower or
equal to the actual outside probability, given the
input, the weight of an item in the agenda is al-
ways lower or equal to the log of the actual prod-
uct of inside and outside probability of the con-
stituent represented by the item. Therefore, the
LN estimate is admissible. In order to prove that
the estimate is also monotonic, we look at the
CYK deduction rules when being augmented with
the estimate. Only Unary and Binary are relevant
since Scan does not have antecedent items. The
two rules are now as follows:

Unary:
inB+outB :[B,~ρ]

inB+| log(p)|+outA:[A,~ρ]
where p : A(~α)→ B(~α) ∈ P .

Binary:
inB+outB :[B, ~ρB ],inC+outC :[C, ~ρC ]

inB+inC+| log(p)|+outA:[A, ~ρA]
where p : A( ~ρA)→ B( ~ρB)C( ~ρC) is an instan-

tiated rule. (Here, outA, outB and outC are the
respective outside estimates of [A, ~ρA], [B, ~ρB]
and [C, ~ρC ].)

We have to show that for every rule, if this rule
has an antecedent item with weight w and a con-
sequent item with weight w′, then w ≤ w′.

We start with Unary. To show: inB + outB ≤
inB + | log(p)| + outA. Because of the Unary
rule for computing the outside estimate and be-
cause of the unary production, we obtain that,
given the outside estimate outA of [A, ~ρ], the out-
side estimate outB of the item [B, ~ρ] is at most
outA+ | log(p)|, i.e., outB ≤ | log(p)|+outA. 2

Now we consider the rule Binary. We treat
only the relation between the weight of the C
antecedent item and the consequent. The treat-
ment of the antecedent B is symmetric. To show:
inC + outC ≤ inB + inC + | log(p)| + outA.
Assume that lB is the length of the components
of the B item and n is the sentence length. Then,
because of the Binary-right rule in the computa-
tion of the outside estimate and because of our
instantiated rule p : A( ~ρA) → B( ~ρB)C( ~ρC),
we have that the outside estimate outC of the
C-item is at most outA + in(B, lB) + | log(p)|.
Furthermore, in(B, lB) ≤ inB . Consequently
outC ≤ inB + | log(p)|+ outA. 2

6 Experiments

We have implemented the parser within the API
of rparse in order to provide equal conditions.
The new parser will be made available under
GNU GPL.5 For all experiments, we have used
the newest Oracle Java 7, running on Debian
Linux on a series of Intel Xeon X5690 nodes at
3.46GHz.

6.1 Data and Experimental Setup

We perform experiments with both the English
DPTB and the German NeGra. The names of the
data sets will have the prefixes E (for the DPTB)

5See http://www.phil.hhu.de/rparse for
more information.

131



and G (for NeGra). We create two versions of Ne-
Gra in which we limit the sentence lengths to 30
and 40 words respectively and investigate the tree-
bank after T4 (data set name suffix T) (only for 30
words) and after T1 (data set name suffix O) (for
30 and 40 words). The names of the data sets are
consequently: G30O (for the 30-word data set af-
ter T1) and G30T, resp. G40T (for the 30- and 40-
word data sets after T4). As for the DPTB, we cre-
ate one data set E30 with a sentence length limit
of 30. In E30, we reattach punctuation tokens as
described in section 3.1. For training, resp. test-
ing we use the first 90%, resp. the last 10% of
each data set. The parser is provided with gold
POS tags.

We extract PLCFRSs from our data sets as
described before and binarize them using the
optimal binarization algorithm from Kallmeyer
(2010). For E30 we cannot resort to determinis-
tic left-to-right binarization as done by Evang and
Kallmeyer, since it results in a binarized gram-
mar of fan-out three. Note that in general, given
an unbinarized LCFRS production with a fan-out
of two, finding a binarization which does not in-
crease the fan-out cannot be guaranteed if its RHS
has a length > 3 (Gómez-Rodrı́guez et al., 2010;
Rambow and Satta, 1999). However, with the
optimal algorithm, we have not observed an in-
creased fan-out in practice, neither for NeGra,
nor for the DPTB. Figure 5 shows the occurrence
counts of the 14 different production types in the
binarized grammars of G30T and E30. For the
choice of the remaining parsing parameters, we
exploit the results of Maier (2012): We do not use
unary rules during binarization and markovize the
binarized grammars with v = 1, h = 2.

6.2 Parsing Speed

We first investigate the speed of the new parser on
both NeGra and the DPTB.

NeGra The upper graph in figure 9 shows the
average parsing times of both parsers on G40T.
The speed-up provided by the case-by-case strat-
egy of the new parser is enormous. The average
parsing time for a sentence of length 40 (a com-
mon upper length limit in PCFG parsing litera-
ture, see, e.g., Klein and Manning (2003)) drops
from several hours with rparse to slightly under
3 minutes with the new parser. Note that the pars-
ing complexity is not changed. The speed gain

 0.001

 0.01

 0.1

 1

 10

 100

 1000

 10000

 100000

 5  10  15  20  25  30  35  40

pa
rs

in
g 

tim
e 

(s
ec

)

sentence length

Average parsing time (G40T)

rparse

new parser

0

500

1000

1500

2000

 20  25  30  35  40

ite
m

s 
pr

od
uc

ed

sentence length

Average items produced (G40T)

new parser 

new parser (LN) 

Figure 9: Average parsing times and items for G40T

 0.01

 0.1

 1

 10

 100

 1000

 5  10  15  20  25  30

pa
rs

in
g 

tim
e 

(s
ec

)

sentence length

Average parsing time (E30)

rparse

new parser

0

200

400

600

800

1000

1200

1400

 20  22  24  26  28  30

ite
m

s 
pr

od
uc

ed

sentence length

Average items produced (E30)

new parser 

new parser (LN) 

Figure 10: Average parsing times and items for E30

132



Precision Recall F1

G30O 74.6 74.5 74.5
G30T 71.8 71.7 71.8

G30O-S 73.6 73.8 73.7
G30T-V 73.5 73.6 73.5

Table 2: Parsing results for NeGra

can be attributed to the fact that it is much cheaper
to perform the simple integer comparisons of the
specialized Complete rules (fig. 6) than to pro-
vide a comparison operation for range vectors of
an arbitrary length (Maier, 2012, p. 176). This
becomes more clear when regarding the pseudo-
code formulation of the similar case-by-case strat-
egy of Kato et al. (2006).

As for the LN estimate, we can observe that it
effectively reduces the number of items which are
produced (cf. the lower graph in fig. 9). However,
it has less effect than the estimates presented in
previous work (Kallmeyer and Maier, 2010). This
indicates that the context summary consisting of
the sum of the span lengths and the total sentence
length provides too few information. For (2, 2)-
LCFRS, unlike for full LCFRS, the full SX esti-
mate from Kallmeyer and Maier should be com-
putable and should deliver better results. We post-
pone this to future work.

DPTB The upper graph in figure 10 shows the
average parsing times for both parsers on E30. We
can see that the speed gain with the new parser
is similar to the one we obtain on NeGra. The
behavior of the LN estimate is also similar to its
behavior in the NeGra experiments (cf. the lower
graph in fig. 10).

6.3 Output Quality

For the qualitative evaluation of the parser output,
we use the extended evalbmeasure for PLCFRS
(Maier, 2010). We report labeled precision, recall
and F1.

NeGra In order to investigate how the trans-
formed treebank behaves compared to the unmod-
ified treebank, we run rparse on G30O and the
new parser on G30T. Intuitively, one might ex-
pect that the less flat annotation of the transformed
treebank leads to better results (Rehbein and van
Genabith, 2007), however, as can be seen in table
2, the results on G30T are worse. We can identify

two major reasons for this: The status of subjects
and different types of verb phrases.

Subjects can be identified structurally in the
transformed treebank, because they are attached
below S while other arguments are part of the
newly introduced VPs. In the original treebank,
when disregarding grammatical functions (such
as we do), subject NPs are indistinguishable from
other NPs. In other words, with the transformed
treebank, the parser must cope with the addi-
tional tasks of identifying subjects. We therefore
produce a minimally modified version of G30O,
G30O-S, in which subjects can be identified by
node labels. In the original annotation, the edge
label SB designates subjects. We rename all NPs
with an SB edge to NP-SB. Subjects which con-
sists only of a single word are attached directly to
the sentence in the original annotation, we project
them to a new single NP-SB node instead. The
results get about 0.8 points worse (cf. tab. 2), re-
flecting the difficulty of the task.

Verb phrases also have a different status in the
transformed treebank. While per definition in the
original annotation, the VP label only designates
non-finite VPs, in the transformed treebank, we
have both finite and non-finite VPs. We therefore
produce a modified version of G30T, G30T-V, in
which we change the label of a VP to VPFIN if
it has a finite lexical head. Similar linguistically
motivated splits have successfully been used be-
fore (Maier, 2010). It turns out that the results
for G30T-V and G30O-S lie very close together
(again cf. tab. 2).

DPTB For the sake of completeness we report
the results for the DPTB as well. On E30, we
obtain LP 76.15, LR 70.94, and therefore a LF1
of 73.45. Our parameter settings have not been
tried before on the DPTB (Evang, 2011; Evang
and Kallmeyer, 2011), therefore there are no pre-
vious result to compare to.

7 Conclusion

The goal of this paper on data-driven PLCFRS
parsing was to show that by restricting the block
degree of trees used for grammar extraction,
resp. the fan-out of the resulting grammars to two,
(i) one can express almost all the information con-
tained in the discontinuous treebank annotation of
NeGra and the DPTB, and (ii) one obtains a parser
which is much faster than a parser for general

133



LCFRS on the same data. The first contribution of
this paper is a series of treebank transformations
for NeGra and the DPTB which produces trees of
a block degree of at most two. Unlike transforma-
tions for PCFG parsing, our transformations al-
most completely preserve the annotation informa-
tion on discontinuities. The second contribution is
an efficient data-driven parser for (2, 2)-PLCFRS,
to be extracted from the converted treebanks. The
evaluation of experiments with this parser on both
NeGra and the Penn Treebank shows that an enor-
mous speed-up has been achieved in comparison
to earlier PLCFRS parsers, all while obtaining an
output of comparable richness.

References

Pierre Boullier. 1998. Proposal for a Natural Lan-
guage Processing syntactic backbone. Technical
Report 3342, INRIA.

Gosse Bouma and Gertjan van Noord. 1998. Word
Order Constraints on Verb Clusters in German and
Dutch. In Erhard Hinrichs, Tsuneko Nakazawa,
and Andreas Kathol, editors, Complex predicates in
Nonderivational Syntax. Academic Press.

Adriane Boyd. 2007. Discontinuity revisited: An im-
proved conversion to context-free representations.
In Proceedings of The Linguistic Annotation Work-
shop.

Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius, and George Smith. 2002. The TIGER
Treebank. In Proceedings of TLT.

Kilian Evang and Laura Kallmeyer. 2011. PLCFRS
parsing of English discontinuous constituents. In
Proceedings of IWPT.

Kilian Evang. 2011. Parsing discontinuous con-
stituents in English. Master’s thesis, University of
Tübingen.

Carlos Gómez-Rodrı́guez, Marco Kuhlmann, and
Giorgio Satta. 2010. Efficient parsing of well-
nested Linear Context-Free Rewriting Systems. In
Proceedings of HLT-NAACL.

Geoffrey Huck and Almerindo Ojeda, editors. 1987.
Discontinuous constituency. Academic Press.

Laura Kallmeyer and Wolfgang Maier. 2010. Data-
driven parsing with Probabilistic Linear Context-
Free Rewriting Systems. In Proceedings of COL-
ING.

Laura Kallmeyer. 2010. Parsing beyond Context-Free
Grammar. Springer.

Yuki Kato, Hiroyuki Seki, and Tadao Kasami. 2006.
Stochastic multiple Context-Free Grammar for
RNA pseudoknot modeling. In Proceedings of
TAG+8.

Dan Klein and Christopher D. Manning. 2003. A∗

parsing: Fast exact viterbi parse selection. In Pro-
ceedings of NAACL.

Roger Levy. 2005. Probabilistic Models of Word Or-
der and Syntactic Discontinuity. Ph.D. thesis, Stan-
ford University.

Wolfgang Maier and Timm Lichte. 2011. Charac-
terizing discontinuity in constituent treebanks. In
FG 2009, Revised Selected Papers, volume 5591 of
LNAI. Springer.

Wolfgang Maier and Anders Søgaard. 2008. Tree-
banks and mild context-sensitivity. In Philippe
de Groote, editor, Proceedings of Formal Grammar.
CSLI.

Wolfgang Maier. 2010. Direct parsing of discontin-
uous constituents in German. In Proceedings of
SPMRL.

Wolfgang Maier. 2012. Parsing Discontinuous Struc-
tures. Ph.D. thesis, University of Tübingen.

Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313–330.

Mark-Jan Nederhof. 2003. Weighted deductive pars-
ing and Knuth’s algorithm. Computational Linguis-
tics, 29(1):1–9.

Owen Rambow and Giorgio Satta. 1999. Independent
parallelism in finite copying parallel rewriting sys-
tems. Theoretical Computer Science, 223(1-2):87–
120.

Ines Rehbein and Josef van Genabith. 2007. Eval-
uating evaluation measures. In Proceedings of
NODALIDA-2007.

Hiroyuki Seki, Takashi Matsumura, Mamoru Fujii,
and Tadao Kasami. 1991. On Multiple Context-
Free Grammars. Theoretical Computer Science,
88(2):191–229.

Wojciech Skut, Brigitte Krenn, Thorsten Brants, and
Hans Uszkoreit. 1997. An annotation scheme
for free word order languages. In Proceedings of
ANLP, pages 88–95.

Heike Telljohann, Erhard W. Hinrichs, Sandra Kübler,
Heike Zinsmeister, and Kathrin Beck. 2012. Style-
book for the Tübingen Treebank of Written Ger-
man (TüBa-D/Z). Technical report, University of
Tübingen.

Andreas van Cranenburgh, Remko Scha, and Federico
Sangati. 2011. Discontinuous data-oriented pars-
ing: A mildly context-sensitive all-fragments gram-
mar. In Proceedings of SPMRL.

Andreas van Cranenburgh. 2012. Efficient parsing
with linear context-free rewriting systems. In Pro-
ceedings of EACL.

K. Vijay-Shanker, David Weir, and Aravind K. Joshi.
1987. Characterising structural descriptions used
by various formalisms. In Proceedings of ACL.

134


