



















































User Embedding for Scholarly Microblog Recommendation


Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 449–453,
Berlin, Germany, August 7-12, 2016. c©2016 Association for Computational Linguistics

User Embedding for Scholarly Microblog Recommendation 

 

Yang Yu, Xiaojun Wan and Xinjie Zhou 

Institute of Computer Science and Technology, The MOE Key Laboratory of Computational 

Linguistics, Peking University, Beijing 100871, China 
{yu.yang, wanxiaojun, xinjiezhou}@pku.edu.cn 

 

 

Abstract 

Nowadays, many scholarly messages are 

posted on Chinese microblogs and more 

and more researchers tend to find schol-

arly information on microblogs. In order 

to exploit microblogging to benefit scien-

tific research, we propose a scholarly mi-

croblog recommendation system in this 

study. It automatically collects and mines 

scholarly information from Chinese mi-

croblogs, and makes personalized rec-

ommendations to researchers. We pro-

pose two different neural network models 

which learn the vector representations for 

both users and microblog texts. Then the 

recommendation is accomplished based 

on the similarity between a user’s vector 

and a microblog text’s vector. We also 

build a dataset for this task. The two em-

bedding models are evaluated on the da-

taset and show good results compared to 

several baselines.  

1 Introduction 

Online social networks such as microblogs have 

drawn growing attention in recent years, and 

more and more researchers are involved in mi-

croblogging websites. Besides expressing their 

own emotions and exchanging their life experi-

ences just like other users, these researchers also 

write from time to time about their latest findings 

or recommend useful research resources on their 

microblogs, which may be insightful to other 

researchers in the same field. We call such mi-

croblog texts scholarly microblog texts. The vol-

ume of scholarly microblog texts is huge, which 

makes it time-consuming for a researcher to 

browse and find the ones that he or she is inter-

ested in. 

In this study, we aim to build a personalized 

recommendation system for recommending 

scholarly microblogs. With such a system a re-

searcher can easily obtain the scholarly mi-

croblogs he or she has interests in. The system 

first collects the latest scholarly microblogs by 

crawling from manually selected microblog users 

or by applying scholarly microblog classification 

methods, as introduced in (Yu and Wan, 2016).  

Second, the system models the relevance of each 

scholarly microblog to a researcher and make 

personalized recommendation. In this study, we 

focus on the second step of the system and aim to 

model the interest and preference of a researcher 

by embedding the researcher into a dense vector. 

We also embed each scholarly microblog into a 

dense vector, and thus the relevance of a scholar-

ly microblog to a researcher can be estimated 

based on their vector representations.  

In this paper, we propose two neural embed-

ding algorithms for learning the vector represen-

tations for both users (researchers) and mi-

croblog texts. By extending the paragraph vector 

representation method proposed by (Le and 

Mikolov, 2014), the vector representations are 

jointly learned in a single framework. By model-

ing the user preferences into the same vector 

space with the words and texts, we can obtain the 

similarity between them in a straightforward way, 

and use this relevance for microblog recommen-

dation. We build a real evaluation dataset from 

Sina Weibo. Evaluation results on the dataset 

show the efficacy of our proposed methods.   

2 Related Work 

There have been a few previous studies focusing 

on microblog recommendation. Chen et al. (2012) 

proposed a collaborative ranking model. Their 

approach takes advantage of collaborative filter-

ing based recommendation by collecting prefer-

ence information from many users. Their ap-

proach takes into account the content of the tweet, 

user’s social relations and certain other explicitly 

defined features. Ma et al. (2011) generated  rec-

ommendations by adding additional social regu-

larization terms in MF to constrain the user latent 

feature vectors to be similar to his or her friends' 

average latent features. Bhattacharya et al. (2014) 

449



proposed a method benefiting from knowing the 

user’s topics of interest, inferring the topics of 

interest for an individual user. Their idea is to 

infer them from the topical expertise of the users 

whom the user follows. Khater and Elmongu 

(2015) proposed a dynamic personalized tweet 

recommendation system capturing the user’s in-

terests, which change over the time.  Their sys-

tem shows the messages that correspond to such 

dynamic interests.  Kuang et al. (2016) consid-

ered three major aspects in their proposed tweet 

recommending model, including the popularity 

of a tweet itself, the intimacy between the user 

and the tweet publisher, and the interest fields of 

the user. They also divided the users into three 

types by analyzing their behaviors, using differ-

ent weights for the three aspects when recom-

mending tweets for different types of users.  

    Most of the above studies make use of the re-

lationships between users, while in this study, we 

focus on leveraging only the microblog texts for 

addressing the task.  

3 Our Approach 

3.1 Task Definition 

We denote a set of users by  1 2 , , , mu u u u  ,  
and a set of microblog texts by

 1 2 , , , nd d d d  . We assume that a user 
tweeting, retweeting or commenting on a mi-

croblog text reflects that the user is interested in 

that microblog. Given tu u , we denote the set 
of microblogs that tu  is interested in by  td u .  
In our task, the entire sets of d  and u  are given, 

while given a user tu u , only a subset of 

 td u  is known. This subset is used as the train-
ing set, denoted as  td u . Our task aims to re-
trieve a subset 'd  of d , that 'd  is as similar to 

   t td u d u  as possible. 
In this section, we introduce one baseline 

method and then propose two different neural 

network methods for user and microblog embed-

ding. The baseline averages the vector represen-

tation of microblog texts into a user vector repre-

sentation. Our proposed two methods learn user 

vector representations jointly with word and text 

vectors, either indirectly or directly from word 

vectors. 

3.2 Paragraph Vector 

As our methods are mainly based on the Para-

graph Vector model proposed by (Le and 

Mikolov, 2014), we start by introducing this 

framework first.  

Paragraph Vector is an unsupervised frame-

work that learns continuous distributed vector 

representations for pieces of texts. In this ap-

proach, every paragraph is mapped to a unique 

vector, represented by a column in matrix D  and 
every word is also mapped to a unique vector, 

represented by a column in matrix W . This ap-
proach is similar to the Word2Vec approach pro-

posed in (Mikolov et al., 2013), except that a 

paragraph token is added to the paragraph and is 

treated as a special word. The paragraph vector is 

asked to contribute to the prediction work in ad-

dition to the word vectors in the context of the 

word to be predicted. The   paragraph vector and 

word vectors are averaged to predict the next 

word in a context. 

Formally speaking, given a paragraph 

 1 2,  , , ,i Td w w w  with id  as the paragraph 
token, k  as the window size, the Paragraph Vec-
tor model applies hierarchical softmax to maxim-

ize the average log probability 

1
log ( | , ,..., )t i t k t k

t

p w d w w
T

   

3.3 Averaging Microblog Text Vectors as 
User Vector 

An intuitive baseline approach to map a mi-

croblog user into a vector space is to build such 

representation from the vector representations of 

the microblogs he or she likes. 

We treat microblog texts as paragraphs, and 

then apply the Paragraph Vector model intro-

duced in Section 3.2 to learn vector representa-

tions of the microblog texts. After learning all 

vector representations of microblog texts, for 

each user, we average all vectors of microblog 

Figure 1. The proposed User2Vec#1 framework for 

learning user vector representation. In this frame-

work, the word vectors do not directly contribute to 

the user vectors.  

 

 

wt

wt-3 wt-2 wt-1di

ui1 ui2 ui3

Word Matrix W

Microblog text 
Matrix D

User Matrix U

Average

Average

450



texts he or she likes in the training set as the user 

vector.  

 

3.4 Learning User Vectors Indirectly From 
Word Vectors 

Besides the above-mentioned baseline approach 

we further consider to jointly learn the vectors of 

users and microblog texts. In this framework, 

every user is mapped to a vector represented in a 

column in matrix U , in addition to the mi-

croblog text matrix D  and the word matrixW . 

Given a microblog text 1 2,  , , ,i Td w w w , be-
sides predicting words in the microblog texts 

using the microblog token id  and words in the 

sliding window, we also try to predict id   using 

the users related to it. Denoting the set of all us-

ers related to id  in the training set as  

1 2
( ) { , ..., }

hi i i i
u d u u u  , we maximize the aver-

age log probability 

1

1
[log ( | , ,..., ) log ( | ,..., )]

ht i t k t k i i i

t

p w d w w p d u u
T

  
 

The structure of this framework is shown in 

Figure 1. We name this framework User2Vec#1. 

3.5 Learning User Vectors Directly From 
Word Vectors 

In the above framework, the user vectors are 

learned only from microblog text vectors, not 

directly from word vectors. Another framework 

we proposed for learning user vector representa-

tion is to put user vectors and microblog vectors 

in the same layer. Unlike User2Vec#1, we do not 

use user vectors to predict microblog text vector. 

Instead, we directly add user vectors into the in-

put layer of word vector prediction task, along 

with the microblog text vector. 

In this framework, the average log probability 

we want to maximize is 

1

1
( log ( | , ,..., , ,..., )

ht i t k t k i i

t

p w d w w u u
T

   

In practical tasks, we modify the dataset by 

copying each microblog once for each user in 

�̃�(𝑑𝑖), and make each copied microblog text only 
relate to one user. All copies of the same mi-

croblog text share a same vector representation. 

The structure of the framework is shown in 

Figure 2. We name this framework User2Vec#2. 

 

 

 

 

3.6 Recommending Microblogs 

When recommending microblogs, given a mi-

croblog 
jd and a user ku  , we compute the cosine 

distance between their vector representations, 

and use the cosine distance to determine whether 

jd  should be recommended to ku  or not. 

4 Evaluation 

4.1 Data Preparation 

To evaluate our proposed user embedding meth-

ods in a scholarly microblog recommending sys-

tem, we built a dataset by crawling from the 

website Machine Learning Daily1.  

The Machine Learning Daily is a Chinese 

website which focuses on collecting and labeling 

scholarly microblogs related to machine learning, 

natural language processing, information retriev-

al and data mining on Sina Weibo. These mi-

croblog texts were collected by a combination of 

manual and automatic methods, and each mi-

croblog text is annotated with multiple tags by 

experts, yielding an excellent dataset for our ex-

periment. The microblog texts in our dataset can 

be written in a mixture of both Chinese and Eng-

lish. We removed stop words from   the raw texts, 

leaving 16,797 words in our corpus. The texts 

were then segmented with the Jieba Chinese text 

segmentation tool2. 

                                                 
1 http://ml.memect.com/ 
2 https://github.com/fxsjy/jieba 

Figure 2. The proposed User2Vec#2 framework for 

learning user vector representation. In this frame-

work, the word vectors contribute directly to the 

user vectors, along with the microblog text vectors. 

wt

wt-3 wt-2 wt-1diui

Word Matrix W

Microblog text 
Matrix D

User 
Matrix U

Word Matrix W

Average

k

451



0.340

0.360

0.380

0.400

0.420

0.440

50 100 150 200 250 300

Average Embedding User2Vec#1

User2Vec#2

After crawling the microblogs from the Ma-

chine Learning Daily, we used Sina Weibo API 

to retrieve the list of users who retweeted or 

commented on those microblogs. These retweet-

ing and commenting actions indicated that those 

users have interests in the microblogs they re-

tweeted or commented, and such microblogs 

were considered the gold-standard (positive) mi-

croblogs for the users in the recommendation 

system. Then we filtered out the users who have 

less than two hundred positive samples to avoid 

the data sparseness problem. This left us with 

711 users and 10,620 microblog texts in our cor-

pus. Each user was associated with 282.3 posi-

tive microblogs on average. 

4.2 Evaluation Setup 

Because there is no API that can directly grant us 

the access to the follower and followee list for 

each user without authorization on Sina Weibo, 

when evaluating the effectiveness of our methods, 

we randomly choose one hundred positive sam-

ples and another four hundred negative samples 

randomly selected from the crawled microblogs, 

to simulate the timeline of a user, and use this 

simulated timeline as the test dataset. The re-

maining positive samples are used for training. 

 We adopt two additional baselines: Bag-of-

Words and SVM on Bag-of-Words. For the Bag-

of-Words baseline, we use the Bag-of-Words 

vector of each microblog text as the microblog 

text vector, and average them to obtain user vec-

tors. For the SVM on Bag-of-Words baseline, we 

randomly choose the same amount of negative 

samples as that of positive samples for training. 

We use the Bag-of-Words vector of each mi-

croblog text as the features, and run the SVM 

algorithm implemented in LibSVM 3  once for 

every user. Note that the Average Embedding 

                                                 
3 https://www.csie.ntu.edu.tw/~cjlin/libsvm/ 

method introduced in Section 3.3 is considered a 

strong baseline for comparison.  

For each method and each user, we sort the 

microblog texts according to their similarity with 

the user and select the top k microblog texts as 

recommendation results, where k varies from 10 

to 100.  

Besides precision and recall values, we also 

compute mean reciprocal rank (MRR) to meas-

ure the recommendation results in our experi-

ments, which is the average of the multiplicative 

inverse of the rank of the positive samples in the 

output of the recommending system, and then 

averaged again across all users. Note that when k 

is set to 100, the precision and recall value will 

be equal to each other. 

4.3 Evaluation Results 

The comparison results with respect to differ-

ent k are shown in Table 1. As we can see, the 

two proposed joint learning methods outperform 

the simple average embedding method and the 

two other baselines, indicating the effectiveness 

of the proposed methods. Moreover, User2Vec#2 

yields better results than User2Vec#1.We believe 

this is because in User2Vec#2, the word vectors 

have a direct contribution  to the user vectors, 

which improves the learning effect of the user 

 

k=10 k=20 k=50 k=100 

Preci-
sion 

Recall MRR 
Preci-
sion 

Recall MRR 
Preci-
sion 

Recall MRR 
Preci-
sion 

Recall MRR 

Bag-of-

Words 
0.5036 0.0504 0.0153 0.4917 0.0983 0.0185 0.4461 0.2231 0.0223 0.3204 0.3204 0.0246 

SVM on 
BoW 

0.5774 0.0577 0.0172 0.5662 0.1132 0.0212 0.5122 0.2561 0.0256 0.3675 0.3675 0.0282 

Average 

Embedding 
0.5963 0.0596 0.0183 0.5824 0.1165 0.0219 0.5266 0.2633 0.0264 0.3793 0.3793 0.0291 

User2Vec#1 0.6246 0.0625 0.0189 0.6055 0.1211 0.0228 0.5511 0.2756 0.0275 0.3953 0.3953 0.0304 

User2Vec#2 0.6652 0.0665 0.0201 0.6498 0.1300 0.0244 0.5883 0.2942 0.0295 0.4231 0.4231 0.0325 

Figure 3. Precision/Recall@k=100 w.r.t. vector di-

mension. 

Table 1. Overview of results. 

452



vectors learnt in the framework. Furthermore, the 

precision/recall scores of the embedding methods 

(k=100) with respect to different vector dimen-

sions are shown in Figure 3. We can see that the 

dimension size has little impact on the recom-

mendation performance, and our proposed two 

methods always outperform the strong baseline. 

5   Conclusion 

In this paper, we proposed two neural embedding 

methods for learning the vector representations 

for both the users and the microblog texts. We 

tested their performance by applying them to 

recommending scholarly microblogs. In future 

work, we will investigate leveraging user rela-

tionships and temporal information to further 

improve the recommendation performance.  

Acknowledgments 

The work was supported by National Natural 

Science Foundation of China (61331011), Na-

tional Hi-Tech Research and Development Pro-

gram (863 Program) of China (2015AA015403) 

and IBM Global Faculty Award Program. We 

thank the anonymous reviewers and mentor for 

their helpful comments.  Xiaojun Wan is the cor-

responding author.  

References 

Michal Barla. 2011. Towards social-based user mod-

eling and personalization. Information Sciences 

and Technologies Bulletin of the ACM Slovakia, 

3(1).  

Parantapa Bhattacharya, Muhammad Bilal Zafar, Ni-

loy Ganguly, Saptarshi Ghosh, and Krishna P. 

Gummadi. 2014. Inferring user interests in the twit-

ter social network. In Proceedings of the 8th ACM 

Conference on Recommender systems. ACM. 

Kailong Chen, Tianqi Chen, Guoqing Zheng, Ou Jin, 

Enpeng Yao, and Yong Yu. 2012. Collaborative 

personalized tweet recommendation. In Proceed-

ings of the 35th international ACM SIGIR confer-

ence on Research and development in information 

retrieval. ACM. 

Shaymaa Khater and Hicham G. Elmongui. 2015. 

Tweets You Like: Personalized Tweets Recom-

mendation based on Dynamic Users Interests. In 

2014 ASE Conference. 

Li Kuang, Xiang Tang, Meiqi Yu, Yujian Huang and 

Kehua Guo. 2016. A comprehensive ranking model 

for tweets big data in online social network. EUR-

ASIP Journal on Wireless Communications and 

Networking, 2016(1). 

Quoc V. Le and Tomas Mikolov. 2014. Distributed 

representations of sentences and documents. arXiv 

preprint arXiv:1405.4053. 

Hao Ma, Dengyong Zhou, Chao Liu, Michael R. Lyu 

and Irwin King. 2011. Recommender systems with 

social regularization. In Proceedings of the fourth 

ACM international conference on Web search and 

data mining. ACM. 

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey 

Dean. 2013. Efficient estimation of word represen-

tations in vector space. arXiv preprint 

arXiv:1301.3781. 

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. 

Corrado and Jeff Dean. 2013. Distributed represen-

tations of words and phrases and their composi-

tionality. In Advances in neural information pro-

cessing systems. 

Hongzhi Yin, Bin Cui, Ling Chen, Zhiting Hu and 

Xiaofang Zhou. 2015. Dynamic user modeling in 

social media systems. ACM Transactions on In-

formation Systems (TOIS), 33(3). 

Jianjun Yu, Yi Shen and Zhenglu Yang. 2014. Topic-

STG: Extending the session-based temporal graph 

approach for personalized tweet recommendation. 

In Proceedings of the companion publication of the 

23rd international conference on World Wide Web 

companion. International World Wide Web Con-

ferences Steering Committee. 

Yang Yu and Xiaojun Wan. 2016. MicroScholar: 

Mining Scholarly Information from Chinese Mi-

croblogs. In Thirtieth AAAI Conference on Artifi-

cial Intelligence. 

 

453


