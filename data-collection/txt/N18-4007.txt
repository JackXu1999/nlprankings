



















































Learning Word Embeddings for Data Sparse and Sentiment Rich Data Sets


Proceedings of NAACL-HLT 2018: Student Research Workshop, pages 46–53
New Orleans, Louisiana, June 2 - 4, 2018. c©2017 Association for Computational Linguistics

Learning Word Embeddings for Data Sparse and Sentiment Rich Data
Sets

Prathusha Kameswara Sarma
Electrical and Computer Engineering

UW - Madison

Abstract

This research proposal describes two al-
gorithms that are aimed at learning word
embeddings for data sparse and senti-
ment rich data sets. The goal is to
use word embeddings adapted for domain
specific data sets in downstream appli-
cations such as sentiment classification.
The first approach learns word embed-
dings in a supervised fashion via SWESA
(Supervised Word Embeddings for Sen-
timent Analysis), an algorithm for senti-
ment analysis on data sets that are of mod-
est size. SWESA leverages document la-
bels to jointly learn polarity-aware word
embeddings and a classifier to classify un-
seen documents. In the second approach
domain adapted (DA) word embeddings
are learned by exploiting the specificity of
domain specific data sets and the breadth
of generic word embeddings. The new
embeddings are formed by aligning cor-
responding word vectors using Canoni-
cal Correlation Analysis (CCA) or the re-
lated nonlinear Kernel CCA. Experimental
results on binary sentiment classification
tasks using both approaches for standard
data sets are presented.

1 Introduction

Generic word embeddings such as Glove and
word2vec (Pennington et al., 2014; Mikolov et al.,
2013) which are pre-trained on large sets of raw
text, in addition to having desirable structural
properties have demonstrated remarkable success
when used as features to a supervised learner in
various applications such as the sentiment classi-
fication of text documents. There are, however,
many applications with domain specific vocabu-

laries and relatively small amounts of data. The
performance of word embedding approaches in
such applications is limited, since word embed-
dings pre-trained on generic corpora do not cap-
ture domain specific semantics/knowledge, while
embeddings trained on small data sets are of low
quality. Since word embeddings are used to ini-
tialize most algorithms for sentiment analysis etc,
generic word embeddings further make for poor
initialization of algorithms for tasks on domain
specific data sets.

A concrete example of a small-sized domain
specific corpus is the Substances User Disorders
(SUDs) data set (Quanbeck et al., 2014; Litvin
et al., 2013), which contains messages from dis-
cussion forums for people with substance addic-
tions. These forums are part of mobile health in-
tervention treatments that encourages participants
to engage in sobriety-related discussions. The
aim with digital intervention treatments is to an-
alyze the daily content of participants’ messages
and predicit relapse risk. This data is both do-
main specific and limited in size. Other examples
include customer support tickets reporting issues
with taxi-cab services, reviews of restaurants and
movies, discussions by special interest groups, and
political surveys. In general they are common in
fields where words have different sentiments from
what they would have elsewhere.

Such data sets present significant challenges for
algorithms based on word embeddings. First, the
data is on specific topics and has a very different
distribution from generic corpora, so pre-trained
generic word embeddings such as those trained on
Common Crawl or Wikipedia are unlikely to yield
accurate results in downstream tasks. When per-
forming sentiment classification using pre-trained
word embeddings, differences in domains of train-
ing and test data sets limit the applicability of the
embedding algorithm. For example, in SUDs, dis-

46



cussions are focused on topics related to recov-
ery and addiction; the sentiment behind the word
‘party’ may be very different in a dating context
than in a substance abuse context. Similarly seem-
ingly neutral words such as ‘holidays’, ‘alcohol’
etc are indicative of stronger negative sentiment in
these domains, while words like ‘clean’ are indica-
tive of stronger positive sentiment. Thus domain
specific vocabularies and word semantics may be
a problem for pre-trained sentiment classification
models (Blitzer et al., 2007).

Second, there is insufficient data to completely
train a word embedding. The SUD data set con-
sists of a few hundred people and only a fraction
of these are active (Firth et al., 2017) and (Naslund
et al., 2015). This results in a small data set of
text messages available for analysis. Furthermore,
the content is generated spontaneously on a day to
day basis, and language use is informal and un-
structured. Running the generic word embedding
constructions algorithms on such a data set leads
to very noisy outputs that are not suitable as input
for downstream applications like sentiment classi-
fication. Fine-tuning the generic word embedding
also leads to noisy outputs due to the highly non-
convex training objective and the small amount of
the data.

This proposal briefly describes two possible so-
lutions to address this problem. Section 3 de-
scribes a Canonical Correlation Analysis (CCA)
based approach to obtain domain adapted word
embeddings. Section 2 describes an biconvex op-
timization algorithm that jointly learns polarity
aware word embeddings and a classifier. Section 4
discusses results from both approaches and out-
lines potential future work.

2 Supervised Word Embeddings for
Sentiment Analysis on Small Sized
Data Sets

Supervised Word Embedding for Sentiment Anal-
ysis (SWESA) algorithm is an iterative algorithm
that minimizes a cost function for both a classifier
and word embeddings under unit norm constraint
on the word vectors. SWESA incorporates doc-
ument label information while learning word em-
beddings from small sized data sets.

2.1 Mathematical model and optimization

Text documents di in this framework are repre-
sented as a weighted linear combination of words

in a given vocabulary. Weights φi used are term
frequencies. SWESA aims to find vector repre-
sentations for words, and by extension of text doc-
uments such that applying a nonlinear transforma-
tion f to the product (θ>Wφ) results in a binary
label y indicating the polarity of the document.
Mathematically we assume that,

P[Y = 1|d = Wφ,θ] = f(θ>Wφ) (1)

for some function f The optimization problem
in (1) can be solved as the following minimization
problem,

J(θ,W)
def
=
−1
N

[
C+

∑

yi=+1

logP(Y = yi|Wφi,θ)

+ C−
∑

yi=−1
logP(Y = yi|Wφi,θ)

]

+λθ||θ ||22.
This optimization problem can now be written

as

min
θ∈Rk,

W∈Rk×V

J(θ,W) (2)

s.t. ||wj ||2 = 1 ∀j = 1, . . . V.

Class imbalance is accounted for by using mis-
classification costs C−,C+ as in (Lin et al., 2002).
The unit norm constraint in the optimization prob-
lem shown in (2) is enforced on word embeddings
to discourage degenerate solutions of wj . This
optimization problem is bi-convex. Algorithm 1
shows the algorithm that we use to solve the op-
timization problem in (2). This algorithm is an
alternating minimization procedure that initializes
the word embedding matrix W with W0 and then
alternates between minimizing the objective func-
tion w.r.t. the weight vector θ and the word em-
beddings W.

The probability model used in this work is lo-
gistic regression. Under this assumption the min-
imization problem in Step 3 of Algorithm 1 is
a standard logistic regression problem. In order
to solve the optimization problem in line 4 of
Algorithm 1 a projected stochastic gradient de-
scent (SGD) with suffix averaging (Rakhlin et al.,
2011). Algorithm 2 implements the SGD algo-
rithm (with stochastic gradients instead of full gra-
dients) for solving the optimization problem in
step 4 of Algorithm 1. W0 is initialized via pre-
trained word2vec embeddings and Latent Seman-
tic Analysis (LSA) (Dumais, 2004) based word

47



Algorithm 1 Supervised Word Embeddings for
Sentiment Analysis (SWESA)
Require: W0, Φ, C+, C−, λθ, 0 < k < V , La-

bels: y = [y1, . . . , yN ], Iterations: T > 0,

1: Initialize W = W0.
2: for t = 1, . . . , T do
3: Solve θt ← arg minθ J(θ,Wt−1).
4: Solve Wt ← arg minW J(θt,W).
5: end for
6: Return θT ,WT

Algorithm 2 Stochastic Gradient Descent for W
Require: θ, γ,W0, Labels: y = [y1, . . . , yN ], It-

erations: N, step size: η > 0, and suffix pa-
rameter: 0 < τ ≤ N .

1: Randomly shuffle the dataset.
2: for t = 1, . . . , N do
3: Set Ct = C+ if yt = +1, Ct = C− if

yt = −1.
4: W̃t+1 = Wt− ηCt

1+eyi(θ
>Wφi)

×(−yi θ φ>i )

5: Wt+1,j = Wt+1,j /||Wt+1,j ||2 ∀j =
1, 2, . . . , V

6: η ← ηt
7: end for
8: Return W = 1τ

∑N
t=N−τ Wt

embeddings obtained form a matrix of term fre-
quencies from the given data. Dimension k of
word vectors is determined empirically by select-
ing the dimension that provides the best perfor-
mance across all pairs of training and test data sets.

2.2 Experiment evaluation and results

SWESA is evaluated against the following base-
lines and data sets,
Datasets: 3 balanced data sets (Kotzias et al.,
2015) of 1000 reviews from Amazon, IMDB and
Yelp with binary ‘positive’ and ‘negative’ senti-
ment labels are considered. One imbalanced data
set with 2500 text messages obtained from a study
involving subjects with alcohol addiction is con-
sidered. Only 8% of the messages are indicative of
‘relapse risk’ while the rest are ‘benign’. Note that
this imbalance influences the performance metrics
and can be seen by comparing against the scores
achieved by the balanced data sets. Additional in-
formation such as number of word tokens etc can

Data Set Method Avg Precision Avg AUC

Yelp

SWESA (LSA)
SWESA (word2vec)

TS (LSA)
TS (word2vec)

NB
RNTN (pre-trained)
RNTN (re-trained)

78.09±2.84
78.35±4.62
76.27±3.0
65.22±4.4
70.31±5.6
83.31±1.1
51.15±4.3

86.06±2.4
86.03±3.5
83.05±5.0
69.08±3.5
57.07±3.3

-
-

Amazon

SWESA (LSA)
SWESA (word2vec)

TS (LSA)
TS (word2vec)

NB
RNTN (pre-trained)
RNTN (re-trained)

80.31±3.3
80.36±2.8
77.32±4.6
71.09±6.2
72.54±6.4
82.84±0.6
49.15±2.1

87.54±4.2
87.19±3.3
85.00±6.2
77.09±5.3
61.16±4.5

-
-

IMDB

SWESA (LSA)
SWESA (word2vec)

TS (LSA)
TS (word2vec)

NB
RNTN (pre-trained)
RNTN (re-trained)

76.40±5.2
77.27±5.4
70.36±5.5
56.87±7.6
73.31±5.6
80.88±0.7
53.95±1.9

81.08±7.6
81.04±6.8
77.54±6.8
59.34±8.9
48.40±2.9

-
-

A-CHESS

SWESA (LSA)
SWESA (word2vec)

TS (LSA)
TS (word2vec)

NB
RNTN (pre-trained)
RNTN (re-trained)

35.80±2.5
35.40±2.0
32.20±3.2
23.60±2.4
30.30±3.8

-
-

83.80±3.1
83.40±2.6
83.80±3.1
68.00±1.2
45.23±3.3

-
-

Table 1: This table shows results from a standard
sentiment classification task on all four data sets.
Results from SWESA are in boldface and results
from pre-trained RNTN are in blue.

be found in the supplemental section.

• Naive Bayes: This is a standard baseline that
is best suited for classification in small sized
data sets.

• Recursive Neural Tensor Network: RNTN
is a dependency parser based sentiment anal-
ysis algorithm. Both pre-trained RNTN and
the RNTN algorithm retrained on the data
sets considered here are used to obtain clas-
sification accuracy. Note that with the RNTN
we do not get probabilities for classes hence
we do not compute AUC.

• Two-Step (TS): In this set up, embeddings
obtained via word2vec on the test data sets
and LSA are used to obtain document rep-
resentation via weighted averaging. Docu-
ments are then classified using a Logistic Re-
gressor.

Hyperparameters: Parameters such as dimen-
sion of word embeddings, regularization on the
logistic regressor etc are determined via 10-fold
cross validation.

48



Figure 1: This figure depicts word embeddings on
a unit circle. Cosine angle between embeddings
is used to show dissimilar word pairs learned via
SWESA and word2vec.

Results: Average Precision and AUC are re-
ported in table 2. Note that, the word2vec em-
beddings used in TS are obtained by retraining
the word2vec algorithm on the test data sets. To
reinforce the point that retraining neural network
based algorithms on sparse data sets depreciates
their performance, results from pre-trained and re-
trained RNTN are presented to further support this
fact. Since SWESA makes use of document labels
when learning word embeddings, resulting word
embeddings are polarity aware. Using cosine sim-
ilarity, word antonym pairs are observed. Given
words ‘Good,’‘fair’ and ‘Awful,’ the antonym pair
‘Good/Awful’ is determined via cosine similarity
between wGood and wAwful. Figure 1 shows a
small sample of word embeddings learned on the
Amazon data set by SWESA and word2vec. The
cosine similarity (angle) between the most dissim-
ilar words is calculated and words are depicted as
points on the unit circle. These examples illustrate
that SWESA captures sentiment polarity at word
embedding level despite limited data.

3 Domain Adapted Word Embeddings
for Improved Sentiment Classification

While SWESA learns embeddings from do-
main specific data alone, this approach proposes
a method for obtaining high quality Domain
Adapted (DA) embeddings by combining generic
embeddings and Domain Specific (DS) embed-
dings via CCA/KCCA. Generic embeddings are
trained on large corpora and do not capture do-
main specific semantics, while DS embeddings are
obtained from the domain specific data set via al-
gorithms such as Latent Semantic Analysis (LSA)
or other embedding methods. Thus DA embed-

dings exploit the breath of generic embeddings
and the specificity of DS embeddings. The two
sets of embeddings are combined using a linear
CCA (Hotelling, 1936) or a nonlinear kernel CCA
(KCCA) (Hardoon et al., 2004). They are pro-
jected along the directions of maximum correla-
tion, and a new (DA) embedding is formed by av-
eraging the projections of the generic embeddings
and DS embeddings. The DA embeddings are then
evaluated in a sentiment classification setting. Em-
pirically, it is shown that the combined DA em-
beddings improve substantially over the generic
embeddings, DS embeddings and a concatenation-
SVD (conc-SVD) based baseline.

3.1 Brief description of CCA/KCCA
Let WDS ∈ R|VDS |×d1 be the matrix whose
columns are the domain specific word embeddings
(obtained by, e.g., running the LSA algorithm on
the domain specific data set), where VDS is its
vocabulary and d1 is the dimension of the em-
beddings. Similarly, let WG ∈ R|VG|×d2 be the
matrix of generic word embeddings (obtained by,
e.g., running the GloVe algorithm on the Com-
mon Crawl data), where VG is the vocabulary
and d2 is the dimension of the embeddings. Let
V∩ = VDS∩VG. Let wi,DS be the domain specific
embedding of the word i ∈ V∩, and wi,G be its
generic embedding. For one dimensional CCA, let
φDS and φG be the projection directions of wi,DS
and wi,G respectively. Then the projected values
are,

w̄i,DS = wi,DS φDS

w̄i,G = wi,G φG. (3)

CCA maximizes the correlation ρ between w̄i,DS
and w̄i,G to obtain φDS and φG such that

ρ(φDS , φG) = max
φDS ,φG

E[w̄i,DSw̄i,G]√
E[w̄2i,DS ]E[w̄2i,G]

(4)

where the expectation is over all words i ∈ V∩.
The d-dimensional CCA with d > 1 can be de-

fined recursively. Suppose the first d − 1 pairs
of canonical variables are defined. Then the dth

pair is defined by seeking vectors maximizing the
same correlation function subject to the constraint
that they be uncorrelated with the first d − 1
pairs. Equivalently, matrices of projection vec-
tors ΦDS ∈ Rd1×d and ΦG ∈ Rd2×d are ob-
tained for all vectors in WDS and WG where d ≤

49



Data Set Embedding Avg Precision Avg F-score Avg AUC

Yelp

WDA

WG

WDS

KCCA(Glv, LSA)
CCA(Glv, LSA)

KCCA(w2v, LSA)
CCA(w2v, LSA)

KCCA(GlvCC, LSA)
CCA(GlvCC, LSA)

KCCA(w2v, DSw2v)
CCA(w2v, DSw2v)
concSVD(Glv, LSA)
concSVD(w2v, LSA)

concSVD(GlvCC, LSA)
GloVe

GloVe-CC
word2vec

LSA
word2vec

85.36± 2.8
83.69± 4.7
87.45± 1.2
84.52± 2.3
88.11± 3.0
83.69± 3.5
78.09± 1.7
86.22± 3.5
80.14± 2.6
85.11± 2.3
84.20± 3.7
77.13± 4.2
82.10± 3.5
82.80± 3.5
75.36± 5.4
73.08± 2.2

81.89±2.8
79.48±2.4
83.36±1.2
80.02±2.6
85.35±2.7
78.99±4.2
76.04±1.7
84.35±2.4
78.50±3.0
83.51±2.2
80.39±3.7
72.32±7.9
76.74±3.4
78.28±3.5
71.17±4.3
70.97±2.4

82.57±1.3
80.33±2.9
84.10±0.9
81.04±2.1
85.80±2.4
80.03±3.7
76.66±1.5
84.65±2.2
78.92±2.7
83.80±2.0
80.83±3.9
74.17±5.0
78.17±2.7
79.35±3.1
72.57±4.3
71.76±2.1

Amazon

WDA

WG

WDS

KCCA(Glv, LSA)
CCA(Glv, LSA)

KCCA(w2v, LSA)
CCA(w2v, LSA)

KCCA(GlvCC, LSA)
CCA(GlvCC, LSA)

KCCA(w2v, DSw2v)
CCA(w2v, DSw2v)
concSVD(Glv, LSA)
concSVD(w2v, LSA)

concSVD(GlvCC, LSA)
GloVe

GloVe-CC
word2vec

LSA
word2vec

86.30±1.9
84.68±2.4
87.09±1.8
84.80±1.5
89.73±2.4
85.67±2.3
85.68±3.2
83.50±3.4
82.36±2.0
87.28±2.9
84.93±1.6
81.58±2.5
79.91±2.7
84.55±1.9
82.65±4.4
74.20±5.8

83.00±2.9
82.27±2.2
82.63±2.6
81.42±1.9
85.47±2.4
83.83±2.3
81.23±3.2
81.31±4.0
81.30±3.5
86.17±2.5
77.81±2.3
77.62±2.7
81.63±2.8
80.52±2.5
73.92±3.8
72.49±5.0

83.39±3.2
82.78±1.7
83.50±2.0
82.12±1.3
85.56±2.6
84.21±2.1
82.20±2.9
81.86±3.7
81.51±2.5
86.42±2.0
79.52±1.7
78.72±2.7
81.46±2.6
81.45±2.0
76.40±3.2
73.11±4.8

IMDB

DA

WG

WDS

KCCA(Glv, LSA)
CCA(Glv, LSA)

KCCA(w2v, LSA)
CCA(w2v, LSA)

KCCA(GlvCC, LSA)
CCA(GlvCC, LSA)

KCCA(w2v, DSw2v)
CCA(w2v, DSw2v)
concSVD(Glv, LSA)
concSVD(w2v, LSA)

concSVD(GlvCC, LSA)
GloVe

GloVe-CC
word2vec

LSA
word2vec

73.84±1.3
73.35±2.0
82.36±4.4
80.66±4.5
54.50±2.5
54.08±2.0
60.65±3.5
58.47±2.7
73.25±3.7
53.87±2.2
78.28±3.2
64.44±2.6
50.53±1.8
78.92±3.7
67.92±1.7
56.87±3.6

73.07±3.6
73.00±3.2
78.95±2.7
75.95±4.5
54.42±2.9
53.03±3.5
58.95±3.2
57.62±3.0
74.55±3.2
51.77±5.8
77.67±3.7
65.18±3.5
62.39±3.5
74.88±3.1
69.79±5.3
56.04±3.1

73.17±2.4
73.06±2.0
79.66±2.6
77.23±3.8
53.91±2.0
54.90±2.1
58.95±3.7
58.03±3.9
73.02±4.7
53.54±1.9
74.55±2.9
64.62±2.6
49.96±2.3
75.60±2.4
69.71±3.8
59.53±8.9

A-CHESS

DA

WG

WDS

KCCA(Glv, LSA)
CCA(Glv, LSA)

KCCA(w2v, LSA)
CCA(w2v, LSA)

KCCA(GlvCC, LSA)
CCA(GlvCC, LSA)

KCCA(w2v, DSw2v)
CCA(w2v, DSw2v)
concSVD(Glv, LSA)
concSVD(w2v, LSA)

concSVD(GlvCC, LSA)
GloVe

GloVe-CC
word2vec

LSA
word2vec

32.07±1.3
32.70±1.5
33.45±1.3
33.06±3.2
36.38±1.2
32.11±2.9
25.59±1.2
24.88±1.4
27.27±2.9
29.84±2.3
28.09±1.9
30.82±2.0
38.13±0.8
32.67±2.9
27.42±1.6
24.48±0.8

39.32±2.5
35.48±4.2
39.81±1.0
34.02±1.1
34.71±4.8
36.85±4.4
28.27±3.1
29.17±3.1
34.45±3.0
36.32±3.3
35.06±1.4
33.67±3.4
27.45±3.1
31.72±1.6
34.38±2.3
27.97±3.7

65.96±1.3
62.15±2.9
65.92±0.6
60.91±0.9
61.36±2.6
62.99±3.1
57.25±1.7
57.76±2.0
61.59±2.3
62.94±1.1
62.13±2.6
60.80±2.3
57.49±1.2
59.64±0.5
61.56±1.9
57.08±2.5

Table 2: This table shows results from the classi-
fication task using sentence embeddings obtained
from weighted averaging of word embeddings.
Metrics reported are average Precision, F-score
and AUC and the corresponding standard devia-
tions (STD). Best results are attained by KCCA
(GlvCC, LSA) and are highlighted in boldface.

min {d1, d2}. Embeddings obtained by w̄i,DS =
wi,DS ΦDS and w̄i,G = wi,G ΦG are projections
along the directions of maximum correlation.

The final domain adapted embedding for word i
is given by ŵi,DA = αw̄i,DS + βw̄i,G, where the
parameters α and β can be obtained by solving the

Data Set Embedding Avg Precision Avg F-score Avg AUC

Yelp

GlvCC
KCCA(GlvCC, LSA)

CCA(GlvCC, LSA)
concSVD(GlvCC,LSA)

RNTN

86.47±1.9
91.06±0.8
86.26±1.4
85.53±2.1
83.11±1.1

83.51±2.6
88.66±2.4
82.61±1.1
84.90±1.7

-

83.83±2.2
88.76±2.4
83.99±0.8
84.96±1.5

-

Amazon

GlvCC
KCCA(GlvCC, LSA)

CCA(GlvCC, LSA)
concSVD(GlvCC, LSA)

RNTN

87.93±2.7
90.56±2.1
87.12±2.6
85.73±1.9
82.84±0.6

82.41±3.3
86.52±2.0
83.18±2.2
85.19±2.4

-

83.24±2.8
86.74±1.9
83.78±2.1
85.17±2.6

-

IMDB

GlvCC
KCCA(GlvCC, LSA)

CCA(GlvCC, LSA)
concSVD(GlvCC, LSA)

RNTN

54.02±3.2
59.76±7.3
53.62±1.6
52.75±2.3
80.88±0.7

53.03±5.2
53.26±6.1
50.62±5.1
53.05±6.0

-

53.01±2.0
56.46±3.4
58.75±3.7
53.54±2.5

-

A-CHESS

GlvCC
KCCA(GlvCC, LSA)

CCA(GlvCC, LSA)
concSVD(GlvCC, LSA)

RNTN

52.21±5.1
55.37±5.5
54.34±3.6
40.41±4.2

-

55.26±5.6
50.67±5.0
48.76±2.9
44.75±5.2

-

74.28±3.6
69.89±3.1
68.78±2.4
68.13±3.8

-

Table 3: This table shows results obtained by us-
ing sentence embeddings from the InferSent en-
coder in the sentiment classification task. Met-
rics reported are average Precision, F-score and
AUC along with the corresponding standard devi-
ations (STD). Best results are obtained by KCCA
(GlvCC, LSA) and are highlighted in boldface.

following optimization,

min
α,β
‖w̄i,DS − (αw̄i,DS + βw̄i,G)‖22+

‖w̄i,G − (αw̄i,DS + βw̄i,G)‖22. (5)
Solving (5) gives a weighted combination with
α = β = 12 , i.e., the new vector is equal to the
average of the two projections:

ŵi,DA =
1

2
w̄i,DS +

1

2
w̄i,G. (6)

Because of its linear structure, the CCA in (4)
may not always capture the best relationships be-
tween the two matrices. To account for nonlinear-
ities, a kernel function, which implicitly maps the
data into a high dimensional feature space, can be
applied. For example, given a vector w ∈ Rd, a
kernel function K is written in the form of a fea-
ture map ϕ defined by ϕ : w = (w1, . . . ,wd) 7→
ϕ(w) = (ϕ1(w), . . . , ϕm(w))(d < m) such that
given wa and wb

K(wa,wb) = 〈ϕ(wa), ϕ(wb)〉.
In kernel CCA, data is first projected onto a
high dimensional feature space before performing
CCA. In this work the kernel function used is a
Gaussian kernel, i.e.,

K(wa,wb) = exp
(
− ||wa−wb ||

2

2σ2

)
.

The implementation of kernel CCA follows the
standard algorithm described in several texts such
as (Hardoon et al., 2004); see reference for details.

50



3.2 Experimental evaluation and results

DA embeddings are evaluated in binary sentiment
classification tasks on four data sets described
in Section 2.2. Document embeddings are ob-
tained via i)a standard framework that expresses
documents as weighted combination of their con-
stituent word embeddings and ii) by initializing a
state of the art sentence encoding algorithm In-
ferSent (Conneau et al., 2017) with word embed-
dings to obtain sentence embeddings. Encoded
sentences are then classified using a Logistic Re-
gressor.
Word embeddings and baselines:

• Generic word embeddings: Generic word
embeddings used are GloVe1 from both
Wikipedia and common crawl and the
word2vec (Skip-gram) embeddings2. These
generic embeddings will be denoted as Glv,
GlvCC and w2v.

• DS word embeddings: DS embeddings are
obtained via Latent Semantic Analysis (LSA)
and via retraining word2vec on the test data
sets using the implementation in gensim3.
DS embeddings via LSA are denoted by LSA
and DS embeddings via word2vec are de-
noted by DSw2v.

• concatenation-SVD baseline: Generic and
DS embeddings are concatenated to form a
single embeddings matrix. SVD is performed
on this matrix and the resulting singular vec-
tors are projected onto the d largest singular
values to form resultant word embeddings.
These meta-embeddings proposed by (Yin
and Schütze, 2016) have demonstrated con-
siderable success in intrinsic tasks such as
similarities, analogies etc.

Details about dimensions of the word embeddings
and kernel hyperparameter tuning are found in the
supplemental material.

Note that InferSent is fine-tuned with a combi-
nation of GloVe common crawl embeddings and
DA embeddings, and concSVD. Since the data
sets at hand do not contain all the tokens re-
quired to retrain InferSent, we replace word tokens

1https://nlp.stanford.edu/projects/
glove/

2https://code.google.com/archive/p/
word2vec/

3https://radimrehurek.com/gensim/

that are common across our test data sets and In-
ferSent training data with the DA embeddings and
concSVD.

3.2.1 Discussion of results
From tables 2 and 3 we see that DA embed-
dings perform better than concSVD as well as the
generic and DS word embeddings, when used in a
standard classification task as well as when used
to initialize a sentence encoding algorithm. As
expected LSA DS embeddings provide better re-
sults than word2vec DS embeddings. Also since
the A-CHESS dataset is imbalanced, we look at
precision closely over the other metric since the
positive class is in minority. These results are be-
cause i) CCA/KCCA provides an intuitively bet-
ter technique to preserve information from both
the generic and DS embeddings. On the other
hand the concSVD based embeddings do not ex-
ploit information in both the generic and DS em-
beddings. ii) Furthermore, in their work (Yin
and Schütze, 2016) propose to learn an ‘ensem-
ble’ of meta-embeddings by learning weights to
combine different generic word embeddings via a
simple neural network. Via the simple optimiza-
tion problem we propose in equation (5), we de-
termine the proper weight for combination of DS
and generic embeddings in the CCA/KCCA space.
Thus, task specific DA embeddings formed by a
proper weighted combination of DS and generic
word embeddings are expected to do better than
the concSVD and other embeddings and this is
verified empirically. Also note that the LSA DS
embeddings do better than the word2vec DS em-
beddings. This is expected due to the size of the
test sets and the nature of the word2vec algorithm.
We expect similar observations when using GloVe
DS embeddings owing to the similarities between
word2vec and GloVe.

4 Future work and Conclusions

From these initial preliminary results we can see
that while SWESA learns embeddings from the
domain specific data sets along, DA embeddings
combine both generic and domain specific embed-
dings thereby achieving better performance met-
rics than SWESA or DS embeddings alone. How-
ever, SWESA imparts potentially desirable struc-
tural properties to its word embeddings. As a
next step we would like to infer from both these
approaches to learn better polarized and domain
adapted word embeddings.

51



References
John Blitzer, Mark Dredze, Fernando Pereira, et al.

2007. Biographies, bollywood, boom-boxes and
blenders: Domain adaptation for sentiment classi-
fication. In ACL. volume 7, pages 440–447.

Alexis Conneau, Douwe Kiela, Holger Schwenk, Loic
Barrault, and Antoine Bordes. 2017. Supervised
learning of universal sentence representations from
natural language inference data. arXiv preprint
arXiv:1705.02364 .

Susan T Dumais. 2004. Latent semantic analysis. An-
nual review of information science and technology
38(1):188–230.

Joseph Firth, John Torous, Jennifer Nicholas, Re-
bekah Carney, Simon Rosenbaum, and Jerome Sar-
ris. 2017. Can smartphone mental health interven-
tions reduce symptoms of anxiety? a meta-analysis
of randomized controlled trials. Journal of Affective
Disorders .

David R Hardoon, Sandor Szedmak, and John Shawe-
Taylor. 2004. Canonical correlation analysis: An
overview with application to learning methods.
Neural computation 16(12):2639–2664.

Harold Hotelling. 1936. Relations between two sets of
variates. Biometrika 28(3/4):321–377.

Dimitrios Kotzias, Misha Denil, Nando De Freitas, and
Padhraic Smyth. 2015. From group to individual la-
bels using deep features. In Proceedings of the 21th
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining. ACM, pages 597–
606.

Yi Lin, Yoonkyung Lee, and Grace Wahba. 2002. Sup-
port vector machines for classification in nonstan-
dard situations. Machine learning 46(1-3):191–202.

Erika B Litvin, Ana M Abrantes, and Richard A
Brown. 2013. Computer and mobile technology-
based interventions for substance use disorders:
An organizing framework. Addictive behaviors
38(3):1747–1756.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in neural information processing
systems. pages 3111–3119.

John A Naslund, Lisa A Marsch, Gregory J McHugo,
and Stephen J Bartels. 2015. Emerging mhealth and
ehealth interventions for serious mental illness: a
review of the literature. Journal of mental health
24(5):321–332.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 confer-
ence on empirical methods in natural language pro-
cessing (EMNLP). pages 1532–1543.

Andrew Quanbeck, Ming-Yuan Chih, Andrew Isham,
Roberta Johnson, and David Gustafson. 2014. Mo-
bile delivery of treatment for alcohol use disorders:
A review of the literature. Alcohol research: current
reviews 36(1):111.

Alexander Rakhlin, Ohad Shamir, and Karthik Srid-
haran. 2011. Making gradient descent optimal
for strongly convex stochastic optimization. arXiv
preprint arXiv:1109.5647 .

Wenpeng Yin and Hinrich Schütze. 2016. Learning
word meta-embeddings. In Proceedings of the 54th
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers). vol-
ume 1, pages 1351–1360.

A Supplemental Material

Dimensions of CCA and KCCA projections.
Using both KCCA and CCA, generic embeddings
and DS embeddings are projected onto their d
largest correlated dimensions. By construction,
d ≤ min (d1, d2). The best d for each data set
is obtained via 10 fold cross validation on the
sentiment classification task. Table 2 provides
dimensions of all word embeddings considered.
Note that for LSA and DA, average word embed-
ding dimension across all four data sets are re-
ported. Generic word embeddings such as GloVe
and word2vec are of fixed dimensions across all
four data sets.

Kernel parameter estimation. Parameter σ of
the Gaussian kernel used in KCCA is obtained em-
pirically from the data. The median (µ) of pair-
wise distances between data points mapped by the
kernel function is used to determine σ. Typically
σ = µ or σ = 2µ. In this section both values are
considered for σ and results with the best perform-
ing σ are reported.

Word tokens and word embeddings dimen-
sions:

Table 4 provide the number of unique word to-
kens in all four data sets used in SWESA as well
as in learning DA embeddings.

Data Set Word Tokens
Yelp 2049

Amazon 1865
IMDB 3075

A-CHESS 3400

Table 4: This table presents the unique tokens
present in each of the four data sets considered in
the experiments.

52



Table 5 presents the dimensions of DS and
generic word embeddings used to obtain DA em-
beddings.

Word embedding Dimension
GloVe 100

word2vec 300
LSA 70

CCA-DA 68
KCCA-DA 68

GloVe common crawl 300
AdaptGloVe 300

Table 5: This table presents the average dimen-
sions of LSA, generic and DA word embeddings.

53


