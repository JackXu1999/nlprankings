



















































Multi-Team: A Multi-attention, Multi-decoder Approach to Morphological Analysis.


Proceedings of the 16th Workshop on Computational Research in Phonetics, Phonology, and Morphology, pages 35–49
Florence, Italy. August 2, 2019 c©2019 Association for Computational Linguistics

35

Multi-Team: A Multi-attention, Multi-decoder Approach to
Morphological Analysis.

Ahmet Üstün Rob van der Goot Gosse Bouma Gertjan van Noord

University of Groningen
{a.ustun, r.van.der.goot, g.bouma, g.j.m.van.noord}@rug.nl

Abstract
This paper describes our submission to SIG-
MORPHON 2019 Task 2: Morphological
analysis and lemmatization in context. Our
model is a multi-task sequence to sequence
neural network, which jointly learns morpho-
logical tagging and lemmatization. On the
encoding side, we exploit character-level as
well as contextual information. We introduce
a multi-attention decoder to selectively focus
on different parts of character and word se-
quences. To further improve the model, we
train on multiple datasets simultaneously and
use external embeddings for initialization. Our
final model reaches an average morphological
tagging F1 score of 94.54 and a lemma accu-
racy of 93.91 on the test data, ranking respec-
tively 3rd and 6th out of 13 teams in the SIG-
MORPHON 2019 shared task.

1 Introduction

This paper presents our model for the SIGMOR-
PHON 2019 Task 2 on morphological analysis and
lemmatization in context (McCarthy et al., 2019).
The task is to generate a lemma and a sequence of
morphological tags, which are called morphosyn-
tactic descriptions (MSD), for each word in a
given sentence. This task is important because it
can be used to improve several downstream NLP
applications such as grammatical error correc-
tion (Ng et al., 2014), machine translation (Con-
forti et al., 2018) and multilingual parsing (Zeman
et al., 2018). Table 1 shows the lemma and mor-
phological tags of: Johnny likes cats.

The first sub-task, Lemmatization, is to trans-
form an inflected word form to its lemma which is
its base-form (or dictionary form), as in the exam-
ple of likes to like. The second sub-task, morpho-
logical tagging, is to predict morphological prop-
erties of words as a sequence of tags, including a
part of speech tag. These morphological tags spec-
ify the inflections encoded in word-forms. In the

Orig Johnny likes cats .

Lemma Johnny like cat .
MSD PROPN;SG V;SG;3;IND;PRS N;PL

Table 1: Example sentence, annotated with lemmas and
morphological tags.

example sentence, the word likes is annotated with
a morphological tag set of {V,SG,3,IND,PRS}.
Both tasks are dependent on context. For example,
while walking is annotated with the lemma walk
and tag set {N,SG} in the sentence: The beach is
within walking distance; it is annotated with walk-
ing and {V.PTCP;PRS;V} in: I was walking.

These two tasks have a clear relation; in most
languages the categories found in the morpholog-
ical tags indicate how the lemma of the word was
inflected to the word-form. In other words, syn-
tactic inflections have a strong correlation with the
morphological properties of the words.

Our approach to solve both of these tasks
consists of an encoder and two separate de-
coders within a multi-task architecture based on
a sequence-to-sequence network. The shared
encoder reads words and sentences to learn
character-level and word-level representations.
The decoders then separately generate lemmas and
morphological tags using these representations by
using multiple attention mechanisms. Our contri-
butions are threefold:

• We introduce the use of multiple attention
mechanisms that selectively focus character and
word sequences in the sentence context.

• We evaluate the effect of a variety of types of ex-
ternal embeddings for lemmatization and mor-
phological tagging.

• We evaluate the effect of combining annotated
datasets from related languages for both tasks



36

using dataset embeddings.

2 Related work

Our system is based on three main approaches
which are heavily studied in existing literature.
These are sequence-to-sequence learning, multi-
task learning and multi-lingual learning.

Recent work on computational morphol-
ogy showed that neural sequence-to-sequence
(seq2seq) models (Sutskever et al., 2014;
Bahdanau et al., 2014) have yielded new state-
of-the-art performance on various tasks including
morphological reinflection and lemmatiza-
tion (Cotterell et al., 2016, 2017, 2018). Building
on this, Dayanık et al. (2018) utilize different
levels of representations such as character-level,
word-level and sentence-level in the encoder of
their seq2seq architecture based on previous work
(Heigold et al., 2017).

Multi-task learning approaches for jointly learn-
ing related tasks have been successfully employed
on syntactic and semantic tasks (Søgaard and
Goldberg, 2016; Plank et al., 2016). In the con-
text of morphological analysis, this has been used
by Kementchedjhieva et al. (2018), who jointly
learn morphosyntactic tags and inflections for a
word in a given context, and use a shared encoder
within a multi-task architecture consisting of mul-
tiple decoder similar to our model.

Multi-lingual learning approaches which ben-
efit from joint learning for multiple languages is
also studied on various tasks with different archi-
tectures. Ammar et al. (2016) uses a language em-
bedding that contains information considering the
language, word-order properties and typological
properties for dependency parsing. In multilingual
neural machine translation, Johnson et al. (2017)
use a special token to indicate the target language.
In this work, our model uses the approach of Smith
et al. (2018) who introduce the treebank embed-
ding approach to combine several treebanks for a
single language or closely related languages.

Most similar to our model, Kondratyuk et al.
(2018) use a joint decoder approach for morpho-
logical tagging and lemmatization. However, our
model differs from theirs in substantial ways. Our
model employs an encoder-decoder architecture
which utilizes different levels of attention com-
ponents with a multi-lingual/multi-dataset signal.
Moreover, our model solves the tagging problem
as a sequential prediction task instead of multi-
layer classification so that we can use the same

architecture for both lemmatization and tagging
which are described in Section 3.2 and 3.3.

3 System Description

Our model is inspired by the architecture
of Dayanık et al. (2018). We employ an encoder-
decoder model over the character and word se-
quences. Following Dayanık et al. (2018), the en-
coder in our model consists of two parts. First, a
word encoder which runs on the character level, is
used to generate embeddings for each word (Sec-
tion 3.1.1). Second, a context encoder is initialized
with these word embeddings, and runs on the sen-
tence level (Section 3.1.4). We also experiment
with two methods to complement the word-level
embeddings (Section 3.1.2 and 3.1.3).

The representations at the different levels which
are generated by the encoder are then passed into
the decoders. Unlike Dayanık et al. (2018) which
uses one decoder for both the lemmas and the mor-
phological tags, we use two different decoders in
a multi-task architecture. The tag decoder pro-
duces a set of morphological tags by using word
representations and joint attention mechanism that
one attention focuses on words and other focuses
on characters (Section 3.2). The lemma decoder
produces a lemma by using the same information
complemented with output embeddings of the tag
decoder (Section 3.3).

Multi-task Learning The decoders work jointly
in a multi-task fashion and they share all internal
representations of the encoder. The whole network
is trained by backpropagating the sum of the losses
of the decoders without any weighting:

L(θ) = Ltag + Llemma (1)

where the morphological tag loss Ltag and the
lemma lossLlemma are separately computed as the
negative log likelihood loss over their softmax out-
puts.

Notation Given a sentence S = w1, ..., wn and
wi = c1, ..., cm where w denotes words and c de-
notes characters, our model processes S and w
in encoders and jointly produces a set of mor-
phological tags ti = ti,1, ..., ti,γ and a lemma
li = li,1, ..., li,φ which is a sequence of characters.

3.1 Encoder
In the following subsections, we explain the dif-
ferent parts of the encoder. An overview of the
encoder architecture is shown in Figure 1.



37

esiw1 w2 w3 w4

eci e
w
i e

d
i

DatasetExt

−−→
hci,1

←−−−
hci,m

eci

<w> c a t s <\w>

Figure 1: Overview of the encoder when processing
the third word of the sentence: “Johnny likes cats .”.
Red:word level embeddings. Green: Character level
embeddings.

3.1.1 Word Encoder
We use a bidirectional GRU layer (Cho et al.,
2014) to encode character sequences in the word
encoder. We first pass each character of a word wi
to an embedding layer to map them into the fixed
dimensional character embeddings. The bi-GRUs
process character embeddings in both directions
and produce the hidden states hci,1, ..., h

c
i,m. The

resulting word embedding eci is computed by con-
catenating the final states of forward and backward
GRUs for the given word:

hci,1:m = bi-GRU(ci,1:m) (2)

eci = [
−−→
hci,m;

←−
hci,1] (3)

3.1.2 Word-Surface Embeddings
In addition to the character-level word embed-
dings, we use surface-level word embeddings
which are either learned in a standalone embed-
ding layer or taken from the pre-trained external
embeddings. Word-surface embeddings are de-
noted by ewi . For the unknown words, we used
a word droupout to overcome the sparsity issue.
Following Kiperwasser and Goldberg (2016), we
replace unknown tokens with a probability that is
inversely proportional to the frequency of the word
so that the word representation for an unknown to-
ken is learned based on infrequent words and their
context.

3.1.3 Dataset Embeddings
In order to train our model on multiple datasets
at once, we use dataset embedding eda for each

N PL <\w>

character attention

context attention

eci

esn

ti,1:γ

Figure 2: An overview of the morphological tag de-
coder.

dataset awhich is mapped into a fixed dimensional
vector in an embedding layer. The idea of dataset
embeddings is introduced by Smith et al. (2018).
These embeddings enable us to combine multiple
datasets without losing their monolingual and het-
erogeneous characters. The strategy that we use
to pick and combine datasets is described in Sec-
tion 4.2

3.1.4 Context Encoder
In order to encode sentence level contextual infor-
mation, we use another bidirectional GRU layer.
For a given sentence, we first concatenate the out-
put of the word encoder eci , the word-surface em-
bedding ewi and the dataset embedding e

d
a, for each

word in the sentence. The resulting embedding se-
quence ein1 , ..., e

in
n is then passed into the bi-GRU.

The output of the bi-GRU is a sequence of em-
beddings es1, ..., e

s
n each representing a word in the

sentence:

eini = [e
c
i ; e

w
i ; e

d
a] (4)

es1:n = bi-GRU(e
in
1:n) (5)

3.2 Tag Decoder
As the tag decoder shows in Figure 2, we use a 2
layer stacked bidirectional GRU as the tag decoder
to generate morphological tags ti = ti,1, ..., ti,γ
for the target word wi in a given sentence. In or-
der to utilize both character-level representations
and contextual representations during decoding,
we initialize the first layer of the decoder with
the context-level word embedding esi and the sec-
ond layer of the decoder with the character-level
word embedding eci after passing them through a
relu layer. The decoder outputs the morphological



38

tags over a softmax layer based on the final hidden
states h̃t, which are computed in a joint attention
mechanism described in the following section.

h̃t = decoder(ht, cct , c
s
t ) (6)

p(ti,t|h̃t) = softmax(h̃t) (7)

3.2.1 Joint Context and Character Attention
We employ two different attention mechanisms to
allow the decoder to focus on multiple parts of the
sentence and the target word at the same time. We
use the attention mechanism introduced by Bah-
danau et al. (2014) for the context attention layer.
In the context attention, the alignment vector ast ,
which consists of weights for each word in the
sentence, is computed based on the previous hid-
den state ht−1 at the top layer of the stacked bi-
GRU and context-level embeddings es of words
by using the concat function described in Luong
et al. (2015). The sentence-level context vector
cst which is calculated as a weighted average over
word embeddings, is then passed into a simple
concatenation layer W sc to produce the new hid-
den state ht through the stacked bi-GRU:

ast (i) = align
s(ht−1, e

s
i ) (8)

cst =
∑
i

aste
s
i (9)

ht = bi-GRU(W sc [c
s
t ;ht−1], ht−1) (10)

Together with the context attention, we also em-
ploy a character-level attention model to take into
account the entire output of the word encoder.
We use the global attention mechanism with the
general score function for alignment vectors (Lu-
ong et al., 2015), for the character attention. The
source-side character-level attention vector cct is
computed as a weighted average of the outputs
of the word encoder, each denoted by hci,j . The
resulting output state h̃t of the tag decoder is
then generated by concatenating the current hid-
den state at the top of the stacked bi-GRU ht
and the context vector cct in a concatenation layer
which has a tanh activation:

act(j) = align
c(ht, h

c
i,j) (11)

cct =
∑
j

acth
c
i,j (12)

h̃t = tanh(W cc [c
c
t ;ht]) (13)

3.3 Lemma Decoder
The lemma decoder (Figure 3) produces one char-
acter at a time to sequentially form a lemma li =

c a t <\w>

character attention

eci

esn

ti,1:γ

Figure 3: An overview of the lemma decoder.

li,1, ..., li,φ for a target word wi. Similar to the
tag encoder, we use a 2 layer stacked bi-GRU as
lemma decoder. The initial states of the decoder
layers are taken from the word encoder output eci
and the context encoder output esi through a relu
layer similarly as in the tag decoder. The output of
the lemma decoder li,t is conditioned on the cur-
rent state of the decoder ht, the character attention
cct and the morphological tags ti,1:γ of the target
word. The probability of the output lemma char-
acters are then predicted through a softmax layer.

h̃t = decoder(ht, cct , ti,1:γ) (14)

p(li,t|h̃t) = softmax(h̃t) (15)

In order to exploit morphological features during
lemmatization, we give the morphological tags ti:γ
which are predicted by the tag decoder, as part of
input to the lemma decoder. Independent of their
order, the entire set of the tags are encoded by a
simple feed-forward layer as described in the base-
line model (Malaviya et al., 2019) and the result-
ing vector is concatenated with the input embed-
dings for each target word.

The last part of the lemma decoder is the atten-
tion network which is the same character-level at-
tention model as in the tag decoder. The character
attention mechanism allows the lemma decoder to
compute an attention vector cct based on the output
states of the word encoder. The attention vector is
then passed into a concatenation layer to generate
the output state h̃t of the decoder for each lemma
character li,t.

act(j) = align
c(ht, h

c
i,j) (16)

cct =
∑
j

acth
c
i,j (17)

h̃t = tanh(W cc [c
c
t ;ht]) (18)



39

Parameter Val. Parameter Val.

teacher forcing ratio 0.5 dataset embbedding size (eda) 32
dropout 0.25 word enc. hidden size (hci ) 1,024
patience 4 context enc. hidden size (hsi ) 1,024
word enc. input size 128 dec. input size 128
word embedding size (ewi ) 256 dec. hidden size (ht) 1,024

Table 2: Default hyperparameter settings. Encoder and
decoder are denoted by enc and dec respectively.

4 Setup

In this section we will give the details regarding
our experimental setup. The hyperparameters we
used in our experiments are shown in Table 2.
These hyperparameters have been tuned on the
datasets described in Section 5.1. For the train-
ing, we used ADAM (Kingma and Ba, 2014) and
we applied an early stopping strategy with a min-
imum number of 100 epochs. We stop training if
there is no improvement in the development set for
4 consecutive epochs (patience).

4.1 External Embeddings

Because of time-constraints and the large num-
ber of languages in the dataset, we used out-of-
the-box embeddings. We compared the perfor-
mance of three well-known pre-trained embedding
repositories for different training methods. We
use two word-based embeddings: Polyglot em-
beddings (Al-Rfou et al., 2013), and FastText em-
beddings (Grave et al., 2018). For FastText, two
sets of pre-trained embeddings are available: one
is trained only on Wikipedia (Bojanowski et al.,
2017), whereas the newer versions are also trained
on CommonCrawl (Grave et al., 2018). Whenever
available, we pick the newer embeddings, but for
many low-resource languages we fall back to the
older, smaller version. We also experiment with
context-based embeddings, namely ELMo embed-
dings (Peters et al., 2018), we use the pre-trained
models from Che et al. (2018).

All of these embeddings have been trained us-
ing the default settings for the embedding type,
hence their dimensions are substantially different
(Polyglot; 64, FastText:300, ELMo:1,024) . We
decided not to transform these, as their default di-
mensions are tuned towards their training algo-
rithm and we want to provide a fair comparison
for all out-of-the-box settings.

4.2 Dataset Embeddings

For the dataset embeddings, we only consider
combining pairs of two for efficiency reasons. To
ensure that we match datasets which are informa-
tive, we use word overlap (excluding numberals
and punctuation). As this method is expected to be
most benficial for small datasets, we searched for
datasets which are closest (ie. have a large word
overlap) to the 50 smallest datasets. The final pairs
of datasets can be found in Appendix A.

5 Experiments

In this section, we will describe the data used in
our experiments as well as evaluate the effective-
ness of our external embeddings setup and the
dataset embeddings with in a variety of settings.
In all experiments we use +E and -E to indicate
the model with and without external embeddings,
and +D and -D for dataset embeddings.

5.1 Data

The test data of SIGMORPHON 2019 task 2 con-
sists of a collection of datasets released in the Uni-
versal Dependencies project (Nivre et al., 2018),
which are automatically converted to the Uni-
Morph Schema (McCarthy et al., 2018). In total,
we evaluate our model on 107 datasets, covering
66 languages.

After empirically looking at the trade-off be-
tween data-size and training time, we decided to
limit each dataset to its first 250,000 tokens for all
experiments. This speeded up the training consid-
erably, with almost no loss in performance.

For the tuning of our model, we selected a
sub-set of datasets from the main benchmark.
More specifically, we aimed to get a diversion of
language-family, size, and morphological richness
(here proxied by the average amount of morpho-
logical tags per word). To ensure we do not overfit
on a specific dataset/annotation, we selected two
datasets for each of these languages. The selected
datasets are shown in Table 3.

5.2 Baseline

The baseline consists of two separate parts: a mor-
phological tagger and a lemmatizer. The morpho-
logical tagger, which predicts a set of morpholog-
ical features (as one tag) for each word, is a biL-
STM model with character level layers. The k-
best predicted morphological tags are then used



40

no
ne po

ly fas
t

elm
o

90

91

92

93

94

95

96
lemma acc
morph f1

Figure 4: Results of our model when using a variety of
types of external embeddings.

as extra information to improve the lemmatiza-
tion. The lemmatizer, which is based on Wu et al.
(2018), uses a hard attention mechanism within an
encoder-decoder model. Unlike the previous mod-
els, the morphological tags are explicitly given to
the lemmatizer to indicate the morpho-syntactic
features of words. The lemmatizer combines the
given morphological tags with a character encod-
ing to predict the lemma.

5.3 External Embeddings

In Figure 4, we plotted the average performance
of our model when the different types of embed-
dings are used to initialize the word-surface em-
beddings (detailed results are in Appendix B). The
results show that a performance boost of approx-
imately 2.5% can be obtained for lemmatization
and 5% for morphological tagging. Especially the
ELMo embeddings perform very well, and result
in an improvement of 3.77 and 6.35 percentage
points. The Polyglot embeddings perform surpris-
ingly well, considering they only have an embed-
ding size of 64. In addition to the reported settings,
we also experimented with concatenating the vec-
tors from all types of external embeddings. How-
ever, our empirical results showed that this per-
formed worse compared to using any of the em-

Dataset Language Family Sents words tag/word

en ewt IE,Germanic 13,297 204,857 1.95
en pud IE,Germanic 800 16,927 1.88
tr imst Turkic,Southwestern 4,508 46,417 3.58
tr pud Turkic,Southwestern 800 13,380 2.78
zh cfl Sino-Tibetan 360 5,688 1.00
zh gsd Sino-Tibetan 3,997 98,734 1.06
fi pud Uralic,Finnic 800 12,556 2.97
fi ftb Uralic,Finnic 14,978 127,536 3.07

Table 3: The datasets which we used to tune our mod-
els, with data properties based on the training split. IE:
Indo-European

Sm
all 

(co
nca

t)

Sm
all 

(+
D)

Lar
ge

 (c
on

cat
)

Lar
ge

 (+
D)

90

91

92

93

94

95

96
lemma acc
morph f1

Figure 5: Average results of our model when using sim-
ple dataset concatenation versus using dataset embed-
dings (+D) on 4 small datasets and 4 large datasets

beddings in isolation.
Because not all types of embeddings are avail-

able for all languages, we use fallback options for
the test data. We choose embeddings in the fol-
lowing order: ELMo, Polyglot, FastText. After
this selection, three languages still have no embed-
dings (Akkadian, Coptic and Naija), we omitted
datasets in these languages from the external em-
bedding experiments.

5.4 Dataset Embeddings
To test whether the dataset embeddings are neces-
sary, we compare them with a naive approach to
combine datasets: simply training on the concate-
nation of both datasets. The average results on 4
small datasets and 4 large datasets which are given
in Table 3, are compared separately in Figure 5. In
both small and large settings, using dataset embed-
dings improves the performance in both morpho-
logical tagging and lemmatization, however the
effect of dataset embeddings is higher on small
datasets, especially in the morphological tagging
task. For the detailed results on our tune datasets,
we refer to Appendix C.

6 Results

In this section, we will compare our final results
for two settings with the baseline. In general, we
compare two setups: use of external data (exter-
nal embeddings, +E) and a constrained setup (-E),
which only uses training data. For the dataset em-
beddings, we could only run for the smallest 50
datasets because of time limitations, so for the de-
velopment data, we only report results for these
datasets. For the test data, we used dataset embed-
dings for datasets for which they have shown to be
beneficial on the development data. Our average
results are shown in Table 7. For the results for all



41

four settings per dataset, we refer to Appendix D;
here we see that the best setting is generally to use
dataset embeddings when available.

6.1 Morphological Tagging

For the morphological tagging task, external em-
beddings show to be more beneficial for the tag-
ging task, whereas the dataset embeddings are par-
ticularly beneficial for lemmatization, but combin-
ing them leads to the best scores for both tasks.
Furthermore, our model outperforms the baseline
by a large margin. This is because, while the
baseline has a separate component for morpholog-
ical tagging, our model learns both tasks jointly.
This approach implicitly enables the decoder to
access lemma information for morphological tag-
ging. Besides, we use a multi-attention strategy
which combines word level and character level at-
tentions which improves the tagging performance.

6.2 Lemmatization

In contrast to the results on the development
data, the baseline outperforms our model on the
test data (Table 7). Especially on small datasets
which are not paired with another dataset, such
as UD Akkadian-PISANDUB, the baseline per-
forms better with a large margin.

There are two main reasons for this perfor-
mance difference. First, the baseline uses a hard
attention to model alignment distribution explic-
itly, whereas, our model uses soft attention for
both tasks. The results show that a hard attention
mechanism performs better on the lemmatization,
confirming the findings of Wu et al. (2018). In-
tegrating a lemma decoder having hard attention
with a morphological tag decoder which employs
soft attention, could be explored in future studies.
Second, as explained in the previous section, we
optimize for both tasks jointly without any weight-
ing. Although this is more elegant, as only one
model is trained, it might not lead to the most op-
timal performance.

7 Conclusion

In this paper, we presented our model for the Sig-
morphon 2019 Task 2 on morphological analysis
and lemmatization. We use an encoder-decoder
model by utilizing multi-task learning approach.
A shared encoder runs on the character and sen-
tence level and two separate decoders jointly learn
to generate morphological tags and the lemma for

Morph. tags Lemma

Models Acc F1 Acc Lev

dev (small)

base 69.66 85.38 91.53 0.19
-E -D 83.16 89.45 86.75 0.29
+E -D 85.84 91.54 87.65 0.28
-E+D 85.58 91.26 89.70 0.27
+E+D 88.03 92.96 91.29 0.24

test (all)

base 73.16 87.92 94.17 0.13
-E 89.00 93.35 93.05 0.16
+E 90.61 94.57 93.94 0.15

Table 7: Average results for all evaluation metrics
for development and test data. +E: use external em-
beddings for initialization, +D: use dataset embedding
strategy. On the development data, we report the aver-
age over the datasets where predictions of all settings
were available.

each word.
Our system achieved an average morphological

tagging F1 score of 94.57 and an average lemma
accuracy score of 93.94 on the test data. The ex-
perimental analysis showed that:

• Employing a multi-task achitecture having mul-
tiple levels of attention mechanism improved
the morphological tagging over the baseline
strategy.

• Using the pre-trained embeddings substantially
improved our scores for both tasks.

• Applying a multi-lingual/dataset strategy by
learning special embeddings also improved our
scores, specifically for small datasets. On 50
datasets (Table 7), the multi-dataset strategy im-
proved the performance of our model substan-
tially, by 2.95 (accuracy) on lemmatization and
1.81 (F1) on morphological tagging.

• Furthermore, these improvements are highly
complementary: using dataset embeddings si-
multaneously with external embeddings leads to
superior performance.

The code to re-run all experiments can
be found on: https://bitbucket.org/
ahmetustunn/morphology_in_context

https://bitbucket.org/ahmetustunn/morphology_in_context
https://bitbucket.org/ahmetustunn/morphology_in_context


42

Lemma Morph. tags Lemma Morph. tags
Dataset Acc. Lev. F1 Acc. +E +D Dataset Acc. Lev. F1 Acc. +E +D

af afribooms 96.44 0.10 98.05 98.45 + + it postwita 95.20 0.18 95.42 96.64 + -
akk pisandub 47.52 1.35 76.24 75.84 - + it pud 97.11 0.06 93.73 96.96 - +
am att 98.49 0.02 87.81 91.52 - - ja gsd 98.98 0.01 98.00 97.76 + +
ar padt 94.65 0.14 94.16 96.90 + + ja modern 96.87 0.04 96.74 96.80 - +
ar pud 82.53 0.41 83.61 94.16 + + ja pud 99.01 0.02 98.56 98.39 + +
be hse 90.28 0.18 82.20 91.52 + + kmr mg 91.31 0.14 83.51 89.44 + -
bg btb 97.19 0.08 97.27 98.79 + - ko gsd 90.09 0.21 95.93 95.35 + -
bm crb 87.47 0.21 91.42 93.77 + - ko kaist 94.62 0.09 96.84 96.46 + -
br keb 92.24 0.18 86.57 89.50 + + ko pud 99.13 0.01 92.38 95.59 + +
bxr bdt 87.12 0.31 83.65 86.57 + + kpv ikdp 85.94 0.26 66.41 75.96 - +
ca ancora 99.00 0.02 97.94 99.04 + - kpv lattice 81.87 0.46 69.23 82.21 + +
cop scriptorium 96.13 0.08 94.67 96.31 - - la ittb 98.33 0.04 95.01 97.77 + -
cs cac 98.39 0.03 95.21 98.36 + - la perseus 92.73 0.15 83.75 93.01 + +
cs cltt 97.60 0.29 93.30 97.59 + + la proiel 96.76 0.07 90.28 96.60 + -
cs fictree 97.78 0.04 93.84 97.57 + - lt hse 80.14 0.46 67.23 83.26 + +
cs pdt 97.94 0.04 94.36 97.97 + - lv lvtb 95.02 0.09 92.96 96.91 + +
cs pud 96.84 0.05 91.19 97.21 + + mr ufal 72.63 0.67 62.33 76.02 + +
cu proiel 95.54 0.10 88.67 95.48 - - nl alpino 96.25 0.08 95.10 96.05 + -
da ddt 96.96 0.05 96.05 97.49 + + nl lassysmall 94.30 0.12 93.45 94.26 - -
de gsd 95.24 0.10 84.99 93.71 + - no bokmaal 97.72 0.04 95.21 97.05 + -
el gdt 94.64 0.11 92.79 97.47 + + no nynorsk 95.86 0.08 94.05 96.27 - -
en ewt 98.39 0.08 96.18 97.24 + + no nynorsklia 97.58 0.04 94.53 96.62 + +
en gum 97.85 0.04 95.95 96.95 + + pcm nsc 99.48 0.02 94.79 93.01 + +
en lines 97.96 0.04 96.45 97.32 + - pl lfg 97.06 0.06 94.55 97.76 + +
en partut 97.97 0.03 95.40 96.27 + + pl sz 97.11 0.05 90.88 96.56 + +
en pud 97.20 0.04 95.44 96.85 + + pt bosque 98.24 0.03 94.83 97.53 + -
es ancora 99.03 0.02 97.83 98.91 + - pt gsd 98.14 0.10 98.24 98.37 + -
es gsd 98.75 0.02 94.60 97.37 - + ro nonstandard 96.44 0.07 92.74 96.18 - +
et edt 95.07 0.11 94.51 97.24 + - ro rrt 98.29 0.03 97.47 98.42 + -
eu bdt 96.03 0.09 90.15 95.38 + - ru gsd 96.79 0.05 90.69 96.05 + -
fa seraji 95.20 0.23 97.76 98.23 + - ru pud 94.31 0.10 87.93 95.50 + +
fi ftb 94.65 0.12 95.17 97.37 + - ru syntagrus 96.76 0.07 95.10 97.71 + -
fi pud 89.35 0.28 95.24 97.51 + + ru taiga 93.44 0.15 86.33 93.83 + +
fi tdt 93.61 0.14 95.31 97.52 + - sa ufal 52.26 1.18 42.21 64.45 + +
fo oft 85.59 0.29 80.60 90.62 - + sk snk 95.61 0.08 91.49 96.75 + -
fr gsd 98.12 0.04 97.31 98.43 + - sl ssj 97.84 0.03 93.65 97.13 + -
fr partut 96.54 0.05 94.96 97.71 + + sl sst 96.24 0.07 90.72 95.09 + +
fr sequoia 98.27 0.03 97.18 98.77 + - sme giella 87.54 0.27 86.22 91.38 + +
fr spoken 99.52 0.01 98.15 98.18 + + sr set 96.09 0.07 92.38 96.27 + -
ga idt 89.07 0.26 83.95 90.82 + - sv lines 96.43 0.08 93.13 97.03 + -
gl ctg 98.31 0.03 97.80 97.59 + - sv pud 94.19 0.11 94.97 97.09 + +
gl treegal 96.56 0.06 93.97 96.93 + - sv talbanken 96.65 0.07 96.32 98.20 + -
got proiel 95.04 0.10 85.99 94.39 - - ta ttb 88.17 0.28 81.14 91.29 + -
grc perseus 92.42 0.18 88.90 95.69 + - tl trg 75.68 2.24 86.49 91.30 + +
grc proiel 96.70 0.08 91.15 97.37 + - tr imst 96.09 0.07 90.79 95.52 + +
he htb 96.61 0.06 95.86 97.35 + - tr pud 86.46 0.34 87.63 94.96 + +
hi hdtb 98.61 0.02 91.80 97.30 + - uk iu 95.45 0.09 91.92 96.42 + -
hr set 94.18 0.11 89.41 96.02 + - ur udtb 95.91 0.07 77.31 92.02 + -
hsb ufal 87.11 0.21 77.12 86.73 + + vi vtb 99.20 0.03 89.55 88.18 - +
hu szeged 94.17 0.12 87.95 96.22 + + yo ytb 98.06 0.02 92.64 93.27 - -
hy armtdp 92.15 0.15 84.64 91.66 + + yue hk 99.29 0.01 92.32 90.23 - +
id gsd 99.09 0.02 89.32 93.04 - - zh cfl 96.57 0.04 91.61 90.35 + +
it isdt 97.83 0.04 96.78 98.01 - - zh gsd 99.02 0.01 94.61 94.59 + +
it partut 98.25 0.04 97.30 98.45 - + average 93.94 0.15 90.61 94.57

Table 6: All four evaluation metrics for the test data of our best system. E: use of external embeddings. D: use of
dataset embeddings. Results might be different compared to the ones in the overview paper, as we did not have
enough time to run all experiments before the deadline. +E: whether external embeddings were used. +D: whether
dataset embeddings were used.



43

Acknowledgements

We would like to thank all our colleagues and
the anonymous reviewers. Furthermore, we would
like to thank the Center for Information Technol-
ogy of the University of Groningen for their sup-
port and for providing access to the Peregrine high
performance computing cluster.

References
Rami Al-Rfou, Bryan Perozzi, and Steven Skiena.

2013. Polyglot: Distributed word representations
for multilingual NLP. In Proceedings of the Seven-
teenth Conference on Computational Natural Lan-
guage Learning, pages 183–192, Sofia, Bulgaria.
Association for Computational Linguistics.

Waleed Ammar, George Mulcaire, Miguel Ballesteros,
Chris Dyer, and Noah A Smith. 2016. Many lan-
guages, one parser. Transactions of the Association
for Computational Linguistics, 4:431–444.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
learning to align and translate. arXiv preprint
arXiv:1409.0473.

Piotr Bojanowski, Edouard Grave, Armand Joulin, and
Tomas Mikolov. 2017. Enriching word vectors with
subword information. Transactions of the Associa-
tion for Computational Linguistics, 5:135–146.

Wanxiang Che, Yijia Liu, Yuxuan Wang, Bo Zheng,
and Ting Liu. 2018. Towards better UD parsing:
Deep contextualized word embeddings, ensemble,
and treebank concatenation. In Proceedings of the
CoNLL 2018 Shared Task: Multilingual Parsing
from Raw Text to Universal Dependencies, pages
55–64, Brussels, Belgium. Association for Compu-
tational Linguistics.

Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using rnn encoder–decoder
for statistical machine translation. In Proceedings of
the 2014 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1724–
1734.

Costanza Conforti, Matthias Huck, and Alexander
Fraser. 2018. Neural morphological tagging of
lemma sequences for machine translation. In Pro-
ceedings of the 13th Conference of the Association
for Machine Translation in the Americas (Volume 1:
Research Papers), volume 1, pages 39–53.

Ryan Cotterell, Christo Kirov, John Sylak-Glassman,
Géraldine Walther, Ekaterina Vylomova, Arya D
McCarthy, Katharina Kann, Sebastian Mielke, Gar-
rett Nicolai, Miikka Silfverberg, et al. 2018.

The conll–sigmorphon 2018 shared task: Uni-
versal morphological reinflection. arXiv preprint
arXiv:1810.07125.

Ryan Cotterell, Christo Kirov, John Sylak-Glassman,
Géraldine Walther, Ekaterina Vylomova, Patrick
Xia, Manaal Faruqui, Sandra Kübler, David
Yarowsky, Jason Eisner, et al. 2017. Conll-
sigmorphon 2017 shared task: Universal morpho-
logical reinflection in 52 languages. arXiv preprint
arXiv:1706.09031.

Ryan Cotterell, Christo Kirov, John Sylak-Glassman,
David Yarowsky, Jason Eisner, and Mans Hulden.
2016. The sigmorphon 2016 shared taskmorpholog-
ical reinflection. In Proceedings of the 14th SIG-
MORPHON Workshop on Computational Research
in Phonetics, Phonology, and Morphology, pages
10–22.

Erenay Dayanık, Ekin Akyürek, and Deniz Yuret.
2018. Morphnet: A sequence-to-sequence model
that combines morphological analysis and disam-
biguation. arXiv preprint arXiv:1805.07946.

Edouard Grave, Piotr Bojanowski, Prakhar Gupta, Ar-
mand Joulin, and Tomas Mikolov. 2018. Learning
word vectors for 157 languages. In Proceedings
of the International Conference on Language Re-
sources and Evaluation (LREC 2018).

Georg Heigold, Guenter Neumann, and Josef van Gen-
abith. 2017. An extensive empirical evaluation of
character-based morphological tagging for 14 lan-
guages. In Proceedings of the 15th Conference of
the European Chapter of the Association for Compu-
tational Linguistics: Volume 1, Long Papers, pages
505–513.

Melvin Johnson, Mike Schuster, Quoc V Le, Maxim
Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat,
Fernanda Viégas, Martin Wattenberg, Greg Corrado,
et al. 2017. Googles multilingual neural machine
translation system: Enabling zero-shot translation.
Transactions of the Association for Computational
Linguistics, 5:339–351.

Yova Kementchedjhieva, Johannes Bjerva, and Is-
abelle Augenstein. 2018. Copenhagen at conll–
sigmorphon 2018: Multilingual inflection in context
with explicit morphosyntactic decoding. Proceed-
ings of the CoNLL SIGMORPHON 2018 Shared
Task: Universal Morphological Reinflection, pages
93–98.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Eliyahu Kiperwasser and Yoav Goldberg. 2016. Sim-
ple and accurate dependency parsing using bidirec-
tional LSTM feature representations. Transactions
of the Association for Computational Linguistics,
4:313–327.

http://www.aclweb.org/anthology/W13-3520
http://www.aclweb.org/anthology/W13-3520
http://www.aclweb.org/anthology/K18-2005
http://www.aclweb.org/anthology/K18-2005
http://www.aclweb.org/anthology/K18-2005


44

Daniel Kondratyuk, Tomáš Gavenčiak, Milan Straka,
and Jan Hajič. 2018. Lemmatag: Jointly tagging
and lemmatizing for morphologically rich languages
with brnns. In Proceedings of the 2018 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 4921–4928.

Thang Luong, Hieu Pham, and Christopher D Man-
ning. 2015. Effective approaches to attention-based
neural machine translation. In Proceedings of the
2015 Conference on Empirical Methods in Natural
Language Processing, pages 1412–1421.

Chaitanya Malaviya, Shijie Wu, and Ryan Cot-
terell. 2019. A simple joint model for improved
contextual neural lemmatization. arXiv preprint
arXiv:1904.02306.

Arya D. McCarthy, Miikka Silfverberg, Ryan Cotterell,
Mans Hulden, and David Yarowsky. 2018. Mar-
rying Universal Dependencies and Universal Mor-
phology. In Proceedings of the Second Workshop
on Universal Dependencies (UDW 2018), pages 91–
101, Brussels, Belgium. Association for Computa-
tional Linguistics.

Arya D. McCarthy, Ekaterina Vylomova, Shijie Wu,
Chaitanya Malaviya, Lawrence Wolf-Sonkin, Gar-
rett Nicolai, Christo Kirov, Miikka Silfverberg, Se-
bastian Mielke, Jeffrey Heinz, Ryan Cotterell, and
Mans Hulden. 2019. The SIGMORPHON 2019
shared task: Crosslinguality and context in morphol-
ogy. In Proceedings of the 16th SIGMORPHON
Workshop on Computational Research in Phonetics,
Phonology, and Morphology, Florence, Italy. Asso-
ciation for Computational Linguistics.

Hwee Tou Ng, Siew Mei Wu, Ted Briscoe, Christian
Hadiwinoto, Raymond Hendy Susanto, and Christo-
pher Bryant. 2014. The conll-2014 shared task on
grammatical error correction. In Proceedings of the
Eighteenth Conference on Computational Natural
Language Learning: Shared Task, pages 1–14.

Joakim Nivre, Mitchell Abrams, Željko Agić, Lars
Ahrenberg, Lene Antonsen, Katya Aplonova,
Maria Jesus Aranzabe, Gashaw Arutie, Masayuki
Asahara, Luma Ateyah, Mohammed Attia, Aitz-
iber Atutxa, Liesbeth Augustinus, Elena Bad-
maeva, Miguel Ballesteros, Esha Banerjee, Sebas-
tian Bank, Verginica Barbu Mititelu, Victoria Bas-
mov, John Bauer, Sandra Bellato, Kepa Bengoetxea,
Yevgeni Berzak, Irshad Ahmad Bhat, Riyaz Ah-
mad Bhat, Erica Biagetti, Eckhard Bick, Rogier
Blokland, Victoria Bobicev, Carl Börstell, Cristina
Bosco, Gosse Bouma, Sam Bowman, Adriane
Boyd, Aljoscha Burchardt, Marie Candito, Bernard
Caron, Gauthier Caron, Gülsen Cebiroğlu Eryiğit,
Flavio Massimiliano Cecchini, Giuseppe G. A.
Celano, Slavomı́r Čéplö, Savas Cetin, Fabricio
Chalub, Jinho Choi, Yongseok Cho, Jayeol Chun,
Silvie Cinková, Aurélie Collomb, Cağrı Cöltekin,
Miriam Connor, Marine Courtin, Elizabeth David-
son, Marie-Catherine de Marneffe, Valeria de Paiva,

Arantza Diaz de Ilarraza, Carly Dickerson, Pe-
ter Dirix, Kaja Dobrovoljc, Timothy Dozat, Kira
Droganova, Puneet Dwivedi, Marhaba Eli, Ali
Elkahky, Binyam Ephrem, Tomaž Erjavec, Aline
Etienne, Richárd Farkas, Hector Fernandez Al-
calde, Jennifer Foster, Cláudia Freitas, Katarı́na
Gajdošová, Daniel Galbraith, Marcos Garcia, Moa
Gärdenfors, Sebastian Garza, Kim Gerdes, Filip
Ginter, Iakes Goenaga, Koldo Gojenola, Memduh
Gökırmak, Yoav Goldberg, Xavier Gómez Guino-
vart, Berta Gonzáles Saavedra, Matias Grioni, Nor-
munds Grūzı̄tis, Bruno Guillaume, Céline Guillot-
Barbance, Nizar Habash, Jan Hajič, Jan Hajič jr.,
Linh Hà Mỹ, Na-Rae Han, Kim Harris, Dag Haug,
Barbora Hladká, Jaroslava Hlaváčová, Florinel
Hociung, Petter Hohle, Jena Hwang, Radu Ion,
Elena Irimia, O. lájı́dé Ishola, Tomáš Jelı́nek, An-
ders Johannsen, Fredrik Jørgensen, Hüner Kasıkara,
Sylvain Kahane, Hiroshi Kanayama, Jenna Kan-
erva, Boris Katz, Tolga Kayadelen, Jessica Ken-
ney, Václava Kettnerová, Jesse Kirchner, Kamil
Kopacewicz, Natalia Kotsyba, Simon Krek, Sooky-
oung Kwak, Veronika Laippala, Lorenzo Lam-
bertino, Lucia Lam, Tatiana Lando, Septina Dian
Larasati, Alexei Lavrentiev, John Lee, Phng
Lê H`ông, Alessandro Lenci, Saran Lertpradit, Her-
man Leung, Cheuk Ying Li, Josie Li, Keying
Li, KyungTae Lim, Nikola Ljubešić, Olga Logi-
nova, Olga Lyashevskaya, Teresa Lynn, Vivien
Macketanz, Aibek Makazhanov, Michael Mandl,
Christopher Manning, Ruli Manurung, Cătălina
Mărănduc, David Mareček, Katrin Marheinecke,
Héctor Martı́nez Alonso, André Martins, Jan
Mašek, Yuji Matsumoto, Ryan McDonald, Gus-
tavo Mendonca, Niko Miekka, Margarita Misir-
pashayeva, Anna Missilä, Cătălin Mititelu, Yusuke
Miyao, Simonetta Montemagni, Amir More, Laura
Moreno Romero, Keiko Sophie Mori, Shinsuke
Mori, Bjartur Mortensen, Bohdan Moskalevskyi,
Kadri Muischnek, Yugo Murawaki, Kaili Müürisep,
Pinkey Nainwani, Juan Ignacio Navarro Horñiacek,
Anna Nedoluzhko, Gunta Nešpore-Bērzkalne, Lng
Nguy˜ên Thi., Huy`ên Nguy˜ên Thi. Minh, Vi-
taly Nikolaev, Rattima Nitisaroj, Hanna Nurmi,
Stina Ojala, Adédayo. Olúòkun, Mai Omura, Petya
Osenova, Robert Östling, Lilja Øvrelid, Niko
Partanen, Elena Pascual, Marco Passarotti, Ag-
nieszka Patejuk, Guilherme Paulino-Passos, Siyao
Peng, Cenel-Augusto Perez, Guy Perrier, Slav
Petrov, Jussi Piitulainen, Emily Pitler, Barbara
Plank, Thierry Poibeau, Martin Popel, Lauma
Pretkalnina, Sophie Prévost, Prokopis Proko-
pidis, Adam Przepiórkowski, Tiina Puolakainen,
Sampo Pyysalo, Andriela Rääbis, Alexandre Rade-
maker, Loganathan Ramasamy, Taraka Rama, Car-
los Ramisch, Vinit Ravishankar, Livy Real, Siva
Reddy, Georg Rehm, Michael Rießler, Larissa Ri-
naldi, Laura Rituma, Luisa Rocha, Mykhailo Ro-
manenko, Rudolf Rosa, Davide Rovati, Valentin
Roca, Olga Rudina, Jack Rueter, Shoval Sadde,
Benoı̂t Sagot, Shadi Saleh, Tanja Samardžić,
Stephanie Samson, Manuela Sanguinetti, Baiba
Saulı̄te, Yanin Sawanakunanon, Nathan Schnei-

https://www.aclweb.org/anthology/W18-6011
https://www.aclweb.org/anthology/W18-6011
https://www.aclweb.org/anthology/W18-6011


45

der, Sebastian Schuster, Djamé Seddah, Wolfgang
Seeker, Mojgan Seraji, Mo Shen, Atsuko Shi-
mada, Muh Shohibussirri, Dmitry Sichinava, Na-
talia Silveira, Maria Simi, Radu Simionescu, Katalin
Simkó, Mária Šimková, Kiril Simov, Aaron Smith,
Isabela Soares-Bastos, Carolyn Spadine, Antonio
Stella, Milan Straka, Jana Strnadová, Alane Suhr,
Umut Sulubacak, Zsolt Szántó, Dima Taji, Yuta
Takahashi, Takaaki Tanaka, Isabelle Tellier, Trond
Trosterud, Anna Trukhina, Reut Tsarfaty, Francis
Tyers, Sumire Uematsu, Zdeňka Urešová, Larraitz
Uria, Hans Uszkoreit, Sowmya Vajjala, Daniel van
Niekerk, Gertjan van Noord, Viktor Varga, Eric
Villemonte de la Clergerie, Veronika Vincze, Lars
Wallin, Jing Xian Wang, Jonathan North Washing-
ton, Seyi Williams, Mats Wirén, Tsegay Wolde-
mariam, Tak-sum Wong, Chunxiao Yan, Marat M.
Yavrumyan, Zhuoran Yu, Zdeněk Žabokrtský, Amir
Zeldes, Daniel Zeman, Manying Zhang, and Hanzhi
Zhu. 2018. Universal dependencies 2.3. LIN-
DAT/CLARIN digital library at the Institute of For-
mal and Applied Linguistics (ÚFAL), Faculty of
Mathematics and Physics, Charles University.

Matthew Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word rep-
resentations. In Proceedings of the 2018 Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, Volume 1 (Long Papers), pages
2227–2237, New Orleans, Louisiana. Association
for Computational Linguistics.

Barbara Plank, Anders Søgaard, and Yoav Goldberg.
2016. Multilingual part-of-speech tagging with
bidirectional long short-term memory models and
auxiliary loss. arXiv preprint arXiv:1604.05529.

Aaron Smith, Bernd Bohnet, Miryam de Lhoneux,
Joakim Nivre, Yan Shao, and Sara Stymne. 2018. 82
treebanks, 34 models: Universal dependency pars-
ing with multi-treebank models. In Proceedings of
the CoNLL 2018 Shared Task: Multilingual Pars-
ing from Raw Text to Universal Dependencies, pages
113–123, Brussels, Belgium. Association for Com-
putational Linguistics.

Anders Søgaard and Yoav Goldberg. 2016. Deep
multi-task learning with low level tasks supervised
at lower layers. In Proceedings of the 54th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 2: Short Papers), volume 2, pages
231–235.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in neural information process-
ing systems, pages 3104–3112.

Shijie Wu, Pamela Shapiro, and Ryan Cotterell. 2018.
Hard non-monotonic attention for character-level
transduction. In Proceedings of the 2018 Confer-
ence on Empirical Methods in Natural Language

Processing, pages 4425–4438, Brussels, Belgium.
Association for Computational Linguistics.

Daniel Zeman, Jan Hajič, Martin Popel, Martin Pot-
thast, Milan Straka, Filip Ginter, Joakim Nivre, and
Slav Petrov. 2018. CoNLL 2018 shared task: Mul-
tilingual parsing from raw text to universal depen-
dencies. In Proceedings of the CoNLL 2018 Shared
Task: Multilingual Parsing from Raw Text to Univer-
sal Dependencies, pages 1–21, Brussels, Belgium.
Association for Computational Linguistics.

http://hdl.handle.net/11234/1-2895
https://doi.org/10.18653/v1/N18-1202
https://doi.org/10.18653/v1/N18-1202
https://www.aclweb.org/anthology/K18-2011
https://www.aclweb.org/anthology/K18-2011
https://www.aclweb.org/anthology/K18-2011
https://www.aclweb.org/anthology/D18-1473
https://www.aclweb.org/anthology/D18-1473
https://www.aclweb.org/anthology/K18-2001
https://www.aclweb.org/anthology/K18-2001
https://www.aclweb.org/anthology/K18-2001


46

A Matching of Datasets

Src Data Additional Emb. type Src Data Additional Emb. type

af afribooms nl alpino poly it postwita it isdt elmo
akk pisandub cs pdt elmo it pud it isdt elmo
am att ja gsd ja pud elmo
ar padt ar pud elmo ja modern ja gsd elmo
ar pud ar padt elmo ja pud ja gsd elmo
be hse ru syntagrus poly kmr mg es gsd poly
bg btb ru syntagrus elmo ko gsd ko kaist elmo
bm crb cs pdt fast ko kaist ko gsd elmo
br keb no bokmaal poly ko pud ko kaist elmo
bxr bdt ru syntagrus fast kpv ikdp ru syntagrus fast
ca ancora es ancora elmo kpv lattice ru syntagrus fast
cop scriptorium la ittb la proiel elmo
cs cac cs pdt elmo la perseus la proiel elmo
cs cltt cs pdt elmo la proiel la ittb elmo
cs fictree cs pdt elmo lt hse lv lvtb poly
cs pdt cs cac elmo lv lvtb hr set elmo
cs pud cs pdt elmo mr ufal hi hdtb poly
cu proiel ru syntagrus elmo nl alpino nl lassysmall elmo
da ddt no bokmaal elmo nl lassysmall nl alpino elmo
de gsd fr gsd elmo no bokmaal no nynorsk elmo
el gdt grc proiel elmo no nynorsk no bokmaal elmo
en ewt en gum elmo no nynorsklia no nynorsk elmo
en gum en ewt elmo pcm nsc en ewt elmo
en lines en ewt elmo pl lfg pl sz elmo
en partut en ewt elmo pl sz pl lfg elmo
en pud en ewt elmo pt bosque pt gsd elmo
es ancora es gsd elmo pt gsd pt bosque elmo
es gsd es ancora elmo ro nonstandard ro rrt elmo
et edt cs pdt elmo ro rrt ro nonstandard elmo
eu bdt es ancora elmo ru gsd ru syntagrus elmo
fa seraji ur udtb elmo ru pud ru syntagrus elmo
fi ftb fi tdt elmo ru syntagrus ru gsd elmo
fi pud fi tdt elmo ru taiga ru syntagrus elmo
fi tdt fi ftb elmo sa ufal hi hdtb poly
fo oft no nynorsk poly sk snk cs pdt elmo
fr gsd fr sequoia elmo sl ssj hr set elmo
fr partut fr gsd elmo sl sst sl ssj elmo
fr sequoia fr gsd elmo sme giella no nynorsk poly
fr spoken fr gsd elmo sr set hr set poly
ga idt cs pdt elmo sv lines sv talbanken elmo
gl ctg es ancora elmo sv pud sv talbanken elmo
gl treegal gl ctg elmo sv talbanken sv lines elmo
got proiel no nynorsk none ta ttb
grc perseus grc proiel elmo tl trg es gsd poly
grc proiel grc perseus elmo tr imst tr pud elmo
he htb ru gsd elmo tr pud tr imst elmo
hi hdtb mr ufal poly uk iu ru syntagrus elmo
hr set sr set poly ur udtb fa seraji elmo
hsb ufal cs pdt poly vi vtb en ewt elmo
hu szeged et edt elmo yo ytb es gsd poly
hy armtdp ru pud poly yue hk zh gsd poly
id gsd es gsd elmo zh cfl zh gsd elmo
it isdt it partut elmo zh gsd ja gsd elmo
it partut it isdt elmo

Table 8: This shows for each dataset, with which dataset it has the highest word overlap, and what their best
common embeddings type is. Three datasets could not be paired, as they had 0% overlap with all other datasets
(ignoring punctuation and numericals).



47

B External Embeddings per Dataset

no
ne po

ly fas
t

elm
o

75

80

85

90

95

100 UD_English-EWT

lemma acc
morph f1

no
ne po

ly fas
t

elm
o

75

80

85

90

95

100 UD_English-PUD

no
ne po

ly fas
t

elm
o

75

80

85

90

95

100 UD_Turkish-IMST

no
ne po

ly fas
t

elm
o

75

80

85

90

95

100 UD_Turkish-PUD

no
ne po

ly fas
t

elm
o

75

80

85

90

95

100 UD_Chinese-CFL

no
ne po

ly fas
t

elm
o

75

80

85

90

95

100 UD_Chinese-GSD

no
ne po

ly fas
t

elm
o

75

80

85

90

95

100 UD_Finnish-PUD

no
ne po

ly fas
t

elm
o

75

80

85

90

95

100 UD_Finnish-FTB

lemma acc
morph f1

Figure 6: Results of different types of embeddings on the development splits of our tune datasets.



48

C Dataset Embeddings per Dataset

con
cat

 (-D
)

+D
85.0

87.5

90.0

92.5

95.0

97.5

100.0 UD_English-EWT

lemma acc
morph f1

con
cat

 (-D
)

+D
85.0

87.5

90.0

92.5

95.0

97.5

100.0 UD_English-PUD

con
cat

 (-D
)

+D
85.0

87.5

90.0

92.5

95.0

97.5

100.0 UD_Turkish-IMST

con
cat

 (-D
)

+D
85.0

87.5

90.0

92.5

95.0

97.5

100.0 UD_Turkish-PUD

con
cat

 (-D
)

+D
85.0

87.5

90.0

92.5

95.0

97.5

100.0 UD_Chinese-CFL

con
cat

 (-D
)

+D
85.0

87.5

90.0

92.5

95.0

97.5

100.0 UD_Chinese-GSD

con
cat

 (-D
)

+D
85.0

87.5

90.0

92.5

95.0

97.5

100.0 UD_Finnish-PUD

con
cat

 (-D
)

+D
85.0

87.5

90.0

92.5

95.0

97.5

100.0 UD_Finnish-FTB

lemma acc
morph f1

Figure 7: Results of dataset embeddings on the development splits of our tune datasets. We compare the dataset
embeddings with a simple concatenation of the datasets.



49

D Results of External and Treebank Embeddings on Development Data

Dataset Base -E-D -E+D +E-D +E+D Lem Mor Dataset Base -E-D -E+D +E-D +E+D Lem Mor

af afribooms 95.48 95.14 95.95 96.63 97.03 96.49 97.57 it postwita 92.19 95.12 0.00 96.19 0.00 95.24 97.14
akk pisandub 74.76 63.65 64.76 59.64 63.07 46.67 82.85 it pud 94.14 94.06 97.02 90.29 96.80 97.25 96.79
am att 93.53 95.51 0.00 93.77 0.00 98.90 92.12 ja gsd 93.98 96.96 97.27 98.08 98.21 98.91 97.52
ar padt 93.40 93.74 90.05 93.93 95.92 94.90 96.94 ja modern 94.26 94.61 96.55 94.19 96.47 96.45 96.65
ar pud 86.58 84.28 76.56 88.02 88.65 82.70 94.60 ja pud 92.91 95.16 98.34 97.02 99.02 99.46 98.59
be hse 84.82 80.69 88.10 81.32 89.13 87.48 90.78 kmr mg 89.17 87.36 0.00 87.81 0.00 88.57 87.06
bg btb 95.57 97.43 0.00 97.97 0.00 97.15 98.79 ko gsd 89.38 91.49 0.00 92.82 0.00 90.45 95.19
bm crb 88.86 91.34 0.00 91.50 0.00 88.70 94.30 ko kaist 91.99 94.89 0.00 95.57 0.00 94.69 96.45
br keb 91.00 86.51 0.00 89.14 91.39 91.49 91.29 ko pud 93.89 94.02 96.57 96.32 97.55 98.80 96.30
bxr bdt 83.66 83.46 86.34 84.33 86.36 86.83 85.89 kpv ikdp 67.44 61.19 73.16 60.82 72.83 71.08 75.24
ca ancora 96.91 98.43 0.00 99.08 0.00 99.11 99.06 kpv lattice 75.14 63.65 74.16 62.89 76.29 78.57 74.01
cop scriptorium 94.53 95.53 0.00 94.54 0.00 95.12 95.94 la ittb 95.85 97.89 0.00 98.17 0.00 98.39 97.94
cs cac 95.86 97.31 0.00 98.30 0.00 98.34 98.26 la perseus 83.11 82.97 89.34 87.26 91.25 90.80 91.70
cs cltt 95.53 94.32 97.20 93.19 97.67 97.82 97.51 la proiel 94.03 95.05 0.00 96.74 0.00 96.88 96.59
cs fictree 94.10 96.09 0.00 97.85 0.00 97.99 97.72 lt hse 76.09 70.49 74.93 76.93 81.48 80.69 82.27
cs pdt 95.26 96.96 0.00 98.00 0.00 98.02 97.98 lv lvtb 92.38 93.88 0.00 95.67 95.70 94.73 96.67
cs pud 89.85 88.22 96.04 94.17 96.94 97.05 96.83 mr ufal 76.22 74.79 76.64 74.80 77.87 76.71 79.02
cu proiel 93.47 94.94 0.00 94.78 0.00 95.00 94.87 nl alpino 94.25 95.77 0.00 96.17 0.00 96.35 95.99
da ddt 93.74 91.53 95.66 97.19 97.35 97.01 97.68 nl lassysmall 92.62 95.05 0.00 94.19 0.00 95.16 94.95
de gsd 0.00 93.59 94.35 94.44 0.00 95.23 93.64 no bokmaal 95.53 97.45 0.00 97.62 0.00 97.91 97.32
el gdt 95.23 95.83 95.36 95.76 96.00 94.50 97.50 no nynorsk 0.00 97.23 0.00 96.26 0.00 97.34 97.13
en ewt 93.99 94.49 96.41 97.66 97.78 98.26 97.29 no nynorsklia 91.80 95.52 95.25 95.61 96.99 97.79 96.18
en gum 93.74 92.04 96.04 93.90 97.53 97.80 97.26 pcm nsc 89.24 95.87 96.10 95.86 96.17 100.00 92.35
en lines 94.61 96.24 0.00 97.60 0.00 98.01 97.20 pl lfg 92.07 95.56 95.66 96.82 97.43 97.03 97.84
en partut 93.33 95.15 96.05 96.11 97.24 98.24 96.25 pl sz 91.10 93.78 95.00 96.15 97.09 97.37 96.80
en pud 91.62 92.70 95.00 96.23 97.07 97.20 96.94 pt bosque 94.88 97.08 0.00 97.91 0.00 98.32 97.50
es ancora 96.87 98.27 98.38 98.89 0.00 98.96 98.83 pt gsd 0.00 97.44 0.00 98.14 0.00 97.94 98.35
es gsd 0.00 97.62 98.14 98.13 0.00 98.68 97.59 ro nonstandard 93.62 95.73 96.20 96.11 0.00 96.32 96.09
et edt 93.31 94.50 0.00 96.20 0.00 94.99 97.42 ro rrt 95.56 97.39 97.46 98.20 0.00 98.23 98.16
eu bdt 91.94 94.76 0.00 95.80 0.00 96.03 95.57 ru gsd 55.99 94.85 0.00 96.93 0.00 97.10 96.75
fa seraji 0.00 95.90 0.00 96.61 0.00 95.27 97.96 ru pud 89.25 88.21 94.70 94.07 95.94 95.06 96.82
fi ftb 92.27 94.23 94.65 96.13 0.00 94.81 97.45 ru syntagrus 94.37 96.66 0.00 97.25 0.00 96.75 97.75
fi pud 88.69 86.82 92.50 90.42 93.12 87.97 98.27 ru taiga 85.09 85.38 92.94 91.17 94.70 94.28 95.13
fi tdt 89.32 94.30 94.31 95.56 0.00 93.43 97.70 sa ufal 68.45 61.87 64.32 62.96 67.69 63.92 71.46
fo oft 88.93 88.18 89.65 87.98 89.28 88.11 91.19 sk snk 92.44 93.12 0.00 96.52 0.00 96.20 96.85
fr gsd 96.43 97.93 97.73 98.30 0.00 98.13 98.47 sl ssj 92.97 95.59 0.00 97.35 0.00 97.60 97.09
fr partut 94.09 94.62 96.89 96.54 97.47 97.10 97.83 sl sst 89.78 89.19 94.26 93.20 95.98 96.89 95.06
fr sequoia 95.59 96.64 97.98 98.49 0.00 98.43 98.54 sme giella 89.69 89.60 87.56 88.27 90.44 88.32 92.56
fr spoken 96.34 96.71 97.92 97.94 98.63 99.41 97.84 sr set 93.82 95.64 0.00 95.91 0.00 95.84 95.98
ga idt 86.92 84.13 0.00 90.14 0.00 89.15 91.13 sv lines 93.54 93.97 95.07 96.98 0.00 96.90 97.07
gl ctg 95.09 97.48 97.97 98.15 0.00 98.38 97.92 sv pud 91.08 89.84 93.61 95.03 95.68 94.48 96.88
gl treegal 92.77 93.38 95.64 96.29 94.91 96.03 96.55 sv talbanken 0.00 96.13 95.95 97.61 0.00 96.85 98.38
got proiel 94.19 95.40 0.00 95.00 0.00 95.63 95.17 ta ttb 93.31 89.73 0.00 91.46 0.00 91.15 91.76
grc perseus 0.00 93.48 0.00 94.02 0.00 92.46 95.57 tl trg 68.66 73.36 70.36 69.13 78.62 76.00 81.25
grc proiel 0.00 95.73 0.00 97.06 0.00 96.72 97.41 tr imst 90.73 93.46 93.81 95.40 95.43 95.50 95.35
he htb 94.31 95.82 96.55 96.87 0.00 96.47 97.27 tr pud 87.86 88.63 90.39 90.83 91.74 87.96 95.52
hi hdtb 96.36 97.43 97.55 97.93 97.71 98.53 97.32 uk iu 91.39 92.41 94.06 95.75 0.00 95.24 96.25
hr set 93.21 93.27 0.00 95.25 0.00 94.20 96.30 ur udtb 92.12 92.85 0.00 93.59 0.00 95.61 91.57
hsb ufal 84.64 82.83 82.98 84.67 84.79 86.28 83.31 vi vtb 89.39 93.37 94.48 94.13 0.00 99.40 89.56
hu szeged 91.06 91.65 91.03 90.94 94.70 93.42 95.98 yo ytb 88.72 92.41 91.38 89.07 0.00 94.40 90.42
hy armtdp 0.00 92.02 0.00 92.69 93.08 93.16 93.01 yue hk 85.19 90.97 94.10 89.32 93.94 98.97 89.22
id gsd 92.75 96.05 0.00 95.89 0.00 99.08 93.03 zh cfl 85.31 89.34 93.05 91.33 93.86 96.26 91.46
it isdt 95.73 97.83 97.62 97.12 97.80 97.63 98.02 zh gsd 91.34 94.15 95.04 96.57 96.79 99.06 94.52
it partut 95.36 95.59 98.11 97.84 97.73 97.85 98.37

Table 9: Results on all development datasets. The average of lemma accuracy and morphological F1 score is used
as main metric. base: baseline. E: external embeddings. D: dataset embeddings. Bold indicates which model is
used on the test data. Lem: lemma accuracy of the bold model. Mor: morphologic tagging F1 score of bold model.
A score of 0.00 means that we did not have time to run the model for this setting.


