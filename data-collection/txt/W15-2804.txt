



















































Lingusitic Analysis of Multi-Modal Recurrent Neural Networks


Proceedings of the 2015 Workshop on Vision and Language (VL’15), pages 8–9,
Lisbon, Portugal, 18 September 2015. c©2015 Association for Computational Linguistics.

Lingusitic Analysis of multi-modal Recurrent Neural Networks

Ákos Kádár
Tilburg University
a.kadar@uvt.nl

Grzegorz Chrupała
Tilburg University

g.a.chrupala@uvt.nl

Afra Alishahi
Tilburg University

a.alishahi@uvt.nl

1 Introduction

Recurrent neural networks (RNN) have gained a
reputation for beating state-of-the-art results on
many NLP benchmarks and for learning repre-
sentations of words and larger linguistic units
that encode complex syntactic and semantic struc-
tures. However, it is not straight-forward to un-
derstand how exactly these models make their de-
cisions. Recently Li et al. (2015) developed meth-
ods to provide linguistically motivated analysis for
RNNs trained for sentiment analysis. Here we fo-
cus on the analysis of a multi-modal Gated Recur-
rent Neural Network (GRU) architecture trained to
predict image-vectors - extracted from images us-
ing a CNN trained on ImageNet - from their cor-
responding descriptions. We propose two meth-
ods to explore the importance of grammatical cat-
egories with respect to the model and the task.
We observe that the model pays most attention
to head-words, noun subjects and adjectival modi-
fiers and least to determiners and coordinations.

2 Method

We used the IMAGINET model from Chrupała et
al. (2015), trained on the MSCOCO dataset (Lin
et al., 2014). It learns visually grounded mean-
ing representations from textual and visual input
and consists of two GRU pathways, TEXTUAL
and VISUAL, with a shared word-embedding ma-
trix. The inputs to the model are pairs of captions
and their corresponding images. Each sentence is
mapped to two sequences of hidden states: one by
TEXTUAL and another by VISUAL. At each time-
step TEXTUAL predicts the next word in the sen-
tence from its current hidden state hTt , while VI-
SUAL predicts the image vector from its last hid-
den representation hVfull. The model is trained us-
ing a multi-task objective which combines cross-
entropy loss for the word predictions and a mean
squared error for the image predictions.

We focus our analysis on the hidden states and
update-gate activations of VISUAL to assess the
impact of syntactic structure on the learned mean-
ing representations of sentences used to predict
images. For each input sentence of length n, VI-
SUAL produces n hidden activations hV1 , ..., h

V
n

and n update-gate activations zV1 , ..., z
V
n . We

associate each word in the input sentence with
their part-of-speech (POS) and dependency rela-
tion (DepRel) labels1, and assess the contribution
of the (word, POS, DepRel) tuples by estimating
the following two scores:

1. dred measures the distance reduction at each
step by calculating the cosine distance be-
tween the current ht and the last hidden state
hfull and subtracting it from the previous dis-
tance: dtred = d

t−1
red − cos(ht, hfull). The idea

is to see how much each word brings the cur-
rent state closer to, or further away from, the
final interpretation.

2. zmean assigns the average activation of the
update-gate z at time step t to the tuple at
positions t. The activation function for z is
sigmoid, therefore it has values between 0-1.
High values of zmean indicate that the model
places more importance on the previous tu-
ples until t− 1 than on the current one at t.

3 Results

We measure dred and zmean for every position in the
first 5000 captions from the validation portion of
MSCOCO and use them to analyze the importance
of both POS and DepRel categories. We only re-
port results on the grammatical categories that ap-
pear at least 500 times. Figure 1 demonstrates the
dred measurements for each word in an example
sentence. A large distance between two adjacent

1We used the dependency parser from Martins et al.
(2013) for both the POS and DepRel tags.

8



Figure 1: An example of the impact of each word in the sentence Stop sign with words written on it with
a black marker, measured by dred. Left: best retrieved image; middle: reduction of distance from hVfull;
right: dred scores for each word.

Figure 2: Importance of dependency relations as measured by zmean on the left chart. Contribution of
POS categories (middle) and DepRel categories (right) measured by dred.

words signals the arrival of a highly informative
word. Figure 2 shows the impact of both measures
for each grammatical category. The low zmean
scores (left) for the roots, adjectival modifiers
(amod), direct objects (dobj), noun compound
modifiers (nn), noun subjects (nsubj), conjuncts
(conj) and objects of prepositions (pobj) suggest
that the model remembers words of these cate-
gories, while prefers to forget determiners (det),
coordinations (cc), prepositions (prep) and auxil-
iaries. As indicated by the high dred scores (mid-
dle graph), nouns (N), adjectives (JJ) verbs (V)
and prepositions (PRP) provide the largest contri-
bution to the meaning representations of the sen-
tences, while determiners (DET) and conjunctions
(C) provide the least. The dred scores for DepRels
are in line with the zmean scores; they highlight the
importance of nsubj, nn, amod, pobj and dobj.

4 Conclusions

We propose two measures to assess the impact
of grammatical categories on sentence representa-
tions learned for predicting images. The observed
patterns likely reflect the visual salience and in-

formativeness of the lexical items associated with
each category. They also provide insights into the
details of the task e.g.: nouns came out signifi-
cantly more important then other content word cat-
egories, indicating that predicting the correct enti-
ties is the most important aspect of the task.

References
[Chrupała et al.2015] Grzegorz Chrupała, Ákos Kádár,

and Afra Alishahi. 2015. Learning language
through pictures. arXiv preprint arXiv:1506.03694.

[Li et al.2015] Jiwei Li, Xinlei Chen, Eduard Hovy,
and Dan Jurafsky. 2015. Visualizing and un-
derstanding neural models in nlp. arXiv preprint
arXiv:1506.01066.

[Lin et al.2014] Tsung-Yi Lin, Michael Maire, Serge
Belongie, James Hays, Pietro Perona, Deva Ra-
manan, Piotr Dollár, and C Lawrence Zitnick.
2014. Microsoft coco: Common objects in context.
In Computer Vision–ECCV 2014, pages 740–755.
Springer.

[Martins et al.2013] André FT Martins, Miguel
Almeida, and Noah A Smith. 2013. Turning on the
turbo: Fast third-order non-projective turbo parsers.
In ACL (2), pages 617–622. Citeseer.

9


