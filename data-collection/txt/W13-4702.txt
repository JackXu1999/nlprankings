










































LexToPlus: A Thai Lexeme Tokenization and Normalization Tool


The 4th Workshop on South and Southeast Asian NLP (WSSANLP), International Joint Conference on Natural Language Processing, pages 9–16,
Nagoya, Japan, 14-18 October 2013.

LexToPlus: A Thai Lexeme Tokenization and Normalization Tool

Choochart Haruechaiyasak and Alisa Kongthon
Speech and Audio Technology Laboratory (SPT)

National Electronics and Computer Technology Center (NECTEC)
Thailand Science Park, Klong Luang, Pathumthani 12120, Thailand

{choochart.har, alisa.kon}@nectec.or.th

Abstract

The increasing popularity of social me-
dia has a large impact on the evolution of
language usage. The evolution includes
the transformation of some existing terms
to enhance the expression of the writer’s
emotion and feeling. Text processing tasks
on social media texts have become much
more challenging. In this paper, we pro-
pose LexToPlus, a Thai lexeme tokenizer
with term normalization process. Lex-
ToPlus is designed to handle the inten-
tional errors caused by the repeated char-
acters at the end of words. LexToPlus is a
dictionary-based parser which detects ex-
isting terms in a dictionary. Unknown to-
kens with repeated characters are merged
and removed. We performed statistical
analysis and evaluated the performance of
the proposed approach by using a Twit-
ter corpus. The experimental results show
that the proposed algorithm yields an ac-
curacy of 96.3% on a test data set. The
errors are mostly caused by the out-of-
vocabulary problem which can be solved
by adding newly found terms into the dic-
tionary.

1 Introduction

Thailand is among the top countries having a large
population on social networking websites such as
Facebook and Twitter. The recent statistics show
that the number of social media users in Thai-
land has reached 18 millions (approximately 25%
of the total population) as of the first quarter of
20131. In addition to the enormous amount of
texts being created daily, another challenging is-
sue is the language usage on social media is much

1Zocial Inc. blog, http://blog.zocialinc.com/thailand-
zocial-award-2013-summary/

different from the traditional and formal language.
Social media texts include chat message, SMS,
comments and posts. These texts are usually short
and noisy, i.e., contain some ill-formed, out-of-
vocabulary, abbreviated, transliterated and homo-
phonic transformed terms. These special charac-
teristics are due to many reasons including incon-
venience in typing on virtual keyboards of smart-
phones and intentional transformation of existing
terms to better express the emotion and feeling
of the writers. As a result, performing basic text
processing tasks such as term tokenization has be-
come much more challenging.

Tokenizing Thai written texts is more difficult
than languages in which word boundary mark-
ers are placed between words. Thai language is
considered as an unsegmented language in which
words are written continuously without the use of
word delimiters. Word segmentation is considered
a basic yet very important NLP task in many un-
segmented languages. The main goal of word seg-
mentation task is to assign correct word bound-
aries on given text strings. Previous approaches
applied to Thai word segmentation can be broadly
classified as dictionary-based and machine learn-
ing. The dictionary-based approach relies on a set
of terms from a dictionary for parsing and seg-
menting input texts into word tokens. During the
parsing process, series of characters are looked up
on the dictionary for matching terms. The perfor-
mance of the dictionary-based approach depends
on the quality and size of the word set in the dic-
tionary. Recent works in Thai word segmenta-
tion have adopted machine learning algorithms.
The machine learning approach relies on a model
trained from a corpus by using sequential labeling
algorithms. Using the annotated corpus in which
word boundaries are explicitly marked with a spe-
cial character, the algorithm could be applied to
train a model based on the features (e.g., character
types) surrounding these boundaries.

9



The errors caused during the tokenization pro-
cess can be categorized into two classes, uninten-
tional and intentional. The unintentional errors are
the typographical errors caused by careless typing
(Peterson, 1980; Brill and Moore., 2000). This
type of errors has been rigorously studied in the
area of word editing and optical character recogni-
tion (OCR). There are three cases of typographical
errors: insertion, deletion and transposition. In-
sertion error is caused by additional characters in
a word. Deletion error is caused by missing char-
acters in a word. Transposition error are caused by
swapping of characters in the adjacent positions.
Table 1 shows some examples of Thai word errors
for all cases. The correct spellings are shown in
parentheses with translations.

Table 1: Unintentional spelling error types and ex-
amples

The scope of this paper does not include the
unintentional errors which have been well stud-
ied. Instead we focus on intentional errors, i.e.,
words in which users intentionally create and
type. Based on our preliminary study, the in-
tentional errors can be classified into four cate-
gories: insertion, transformation, transliteration
and onomatopoeia. The intentional insertion er-
ror is caused by typing repeated characters at the
end of a word to emphasize the emotion or feeling.
The transformation error is caused by alteration
of existing terms and can be categorized into two
subtypes: homophonic and syllable trimming. The
homophonic terms refer to terms with the same or
similar pronunciation to existing terms. The sylla-
ble trimming is a transformed term by deleting one
or more syllables from an existing term for the pur-
pose of reducing the keystrokes. The transliterated
terms are created by using the Thai character set to
create new terms from other languages. The last
intentional error is the onomatopoeia terms which
are created to phonetically imitate various sounds.

In this paper, we propose a solution for tokeniz-
ing and normalizing texts with the intentional in-
sertion errors, i.e., users insert repeated charac-

ters at the end of words. The statistics on a 2-
million Twitter corpus show that this type of er-
rors accounts for approximately 4.8% of corpus
size. Our proposed method is a longest matching
dictionary-based approach with a rule-based nor-
malization process. From our initial evaluation,
the dictionary-based approach can handle the case
of repeated characters better than the machine-
learning based. More analysis and discussion will
be given in the paper.

The remainder of this paper is organized as fol-
lows. In next section, we review some related
works in word segmentation, text tokenization and
term normalization for both segmented and unseg-
mented languages. In Section 3, we first give a for-
mal definition of the tokenization task. Then we
present the proposed algorithm for implementing
LexToPlus. In Section 4, we give the performance
evaluation by using a data set collected from Twit-
ter. Some examples of errors are presented with
some discussion. Section 5 concludes the paper
with the future work.

2 Related work

Many techniques for word segmentation and mor-
phological analysis have been reported for un-
segmented languages. Peng et al. applied the
linear-chain CRFs model for Chinese word seg-
mentation (Peng et al., 2004). Their proposed
model included a probabilistic new word detection
method to further improve the performance. For
Thai word segmentation, many previous works
also applied machine learning algorithms to train
the models. Meknavin et al. combined the model
of word segmentation with the POS tagging (Mek-
navin et al., 1997). Their proposed model solved
the ambiguity problem by using a feature-based
model. Kruengkrai and Isahara applied the CRFs
to train a word segmentation model for Thai lan-
guage (Kruengkrai and Isahara, 2006). Two path
selection schemes based on confidence estimation
and Viterbi were proposed to solve the ambiguity
problem. Haruechaiyasak et al. compared the per-
formance among the dictionary-based approach
and many machine learning techniques such as
CRFs and the Support Vector Machines (SVMs)
(Haruechaiyasak et al., 2008). The CRFs model
was reported to outperform the dictionary-based
approach and other machine learning algorithms.

While the majority of previous works focused
on formal written texts, some works in text to-

10



Figure 1: Thai Unicode Characters

kenization have expanded the scope into real-
world texts which often contains many out-of-
vocabulary terms. Tokenization for short and
noisy texts requires an additional process of text
normalization. Early works for text normalization
were focused on news text, newsgroups posts and
classified ads. Sprout et al. performed a study
on non-standard words (NSWs), including num-
bers, abbreviations, dates, currency amounts and
acronyms (Sprout et al., 2001). They applied sev-
eral techniques including n-gram language mod-
els, decision trees and weighted finite-state trans-
ducers and showed that the machine learning ap-
proaches yielded better performance than the rule-
based approach.

More recent works in text normalization fo-
cused on real world texts which contain informal
language, such as SMS, chat and social media.
Aw et al. proposed an approach for normaliz-
ing SMS texts for machine translation task (Aw
et al., 2006). The normalization task is viewed
as a translation problem from the SMS language
to the English language. The results showed that
normalizing SMS texts before performing transla-
tion significantly improved the performance based
on the BLEU score. Costa-Jussa and Banchs pro-
posed an approach for normalizing short texts, i.e.,
SMS (Costa-Jussa and Banchs, 2013). The pro-
posed approach adopted the idea from statistical
machine translation (SMT) by combining statisti-
cal and rule-based techniques.

Han et al. proposed a classification-based ap-
proach to detect ill-formed words, and generates
correction candidates based on morphophonemic
similarity (Han and Baldwin, 2011; Han et al.,
2013). The proposed approach was evaluated on
both SMS and Twitter corpus. The best perfor-
mance was achieved with the combination of dic-
tionary lookup, word similarity and context sup-
port modelling. Hirst and Budanitsky proposed

a method for detecting and correcting spelling er-
rors (Hirst and Budanitsky, 2005). The proposed
method is based on the identification of tokens
that are semantically unrelated to their context.
The method also detects tokens which are spelling
variations of words that would be related to the
context. Liu et al. identified and distinguished
nonstandard tokens found in social media texts
as intentionally and unintentionally (Liu et al.,
2012). A normalization system was proposed by
integrating different techniques including the en-
hanced letter transformation, visual priming, and
string/phonetic similarity. The proposed system
was evaluated on SMS and Twitter data sets. The
results showed that the proposed system achieved
over 90% word-coverage across all data sets.

Another related research is the study of ono-
matopoeia in which terms are created to pho-
netically imitate different sounds. Research in
onomatopoeia has recently gained much atten-
tion for Japanese language. Asaga et al. pre-
sented an online onomatopoeia example-based
dictionary called ONOMATOPEDIA (Asaga et
al., 2008). The proposed approach includes the
extraction and clustering of sentences containing
onomatopoeias as learning examples. Uchida et
al. studied some Japanese onomatopoeias which
contain various emotions (Uchida et al., 2001).
Users are asked to rate emotion in each ono-
matopoeia. Correct identification of emotion from
onomatopoeia could be used in advanced seman-
tic analysis. Kato et al. proposed an approach
to extract onomatopoeia found in food reviews
(Kato et al., 2012). The extracted onomatopoeia
terms can help users search for food or restaurants.
In Thai language, many onomatopoeia terms are
found in chat and social media texts. We classify
onomatopoeia as another type of intentional errors
while performing tokenization. Details are given
and discussed in later section of the paper.

11



Table 2: Intentional spelling error types and examples

3 Tokenization and normalization for
Thai texts

Thai language has its own set of characters or al-
phabets. It has 44 consonants, 15 vowel symbols
and four tone marks. Consonants are written hor-
izontally from left to right, with vowels placed
in four positions: above, below, left and right of
the corresponding consonant. Tone marks can be
placed in the position above of consonants and
some vowels. There are 87 valid characters in Uni-
code system (shown in Figure 1).

3.1 Problem definition

The problem of tokenization for unsegmented lan-
guages can be defined as follows. Given a string of
N words, w0w1 . . . wN−1, each word wi consists
of a series of characters, ci0c

i
1 . . . c

i
|wi|−1, where

|wi| is the number of characters in wi. The to-
kenization task is to assign a word boundary be-
tween two words, e.g., wi|wj , where | represents a
word boundary character.

From our preliminary study, the errors from tok-
enizing Thai texts from social media texts are due
to four cases: insertion, transformation, translit-
eration and onomatopoeia. These error types are
considered as intentional errors as opposed to the
unintentional errors previously mentioned. Inten-
tional errors are caused by users intentionally cre-
ate, alter and transform existing words on different
purposes. Table 2 lists all error types with some

examples. The original terms or brief descriptions
are shown in parentheses with translations. Each
error type is fully explained as follows.

1. Insertion: This type of error is caused
by repeated characters at the end of a
word. Using the above problem defini-
tion, the error can be described as fol-
lows, ci0c

i
1 . . . c

i
|wi|−1c

i
|wi|−1c

i
|wi|−1c

i
|wi|−1, in

which the last character ci|wi|−1 is repeated
more than once. This error type also ap-
pears in English, e.g., whatttt, sleepyyy and
loveeee.

2. Transformation: This error type is caused
by transformation of existing terms and can
be categorized into two following types.

Homophonic: The homophonic terms
refer to terms with the same or similar pro-
nunciation. A homophonic term is normally
created by replacing an original vowel with a
new vowel which has similar sound. Some
examples in English are luv (love), kinda
(kind of) and gal (girl).

Syllable trimming: The syllable trim-
ming is a transformed term by deleting one
or more syllables from an existing term for
the purpose of reducing the keystrokes.

3. Transliteration: Thai Transliterated terms
are newly created terms converted from other

12



language scripts. Transliterated terms are
commonly found in modern Thai written
texts, e.g., chat and social media. Most of the
terms are transliterated from English terms
including named entities such as company
and product names.

4. Onomatopoeia: Thai onomatopoeia terms
are created by using the Thai character set
to form new terms to imitate different sounds
in nature and environment including humans
and animals. Onomatopoeia terms are typ-
ically used in chat and social media texts
to make the communications between users
more vivid. For example, to make the kissing
action sound more realistic, the word joob in
Thai (or smooch in English) which imitates
the kissing sound is normally used.

3.2 The proposed solution

To select an appropriate approach for tokeniz-
ing and normalizing social media texts, we first
perform a comparison between two approaches,
dictionary-based (DCB) and machine learning
based (MLB) (Haruechaiyasak et al., 2008). For
machine learning based approach, we adopt the
conditional random fields (CRFs) algorithm to
train the tokenization model. The dictionary-
based approach is a lexicon-based parser which
solves the ambiguity with a longest matching
heuristic. Table 3 shows tokenized results from
different approaches. The input text consists of
three words. Each word contains the insertion of
some repeated characters at the end. The correct
terms are bolded and underlined. The MLB ap-
proach cannot correctly assign the word bound-
aries between words. The first problem is due to
the repeated characters at the end of each word.
All repeated characters are recognized as part of
the word. The second problem is due to the out-of-
vocabulary which results in a word is incorrectly
tokenized as two separating words.

The DCB approach can correctly tokenize all
the terms which are included in the dictionary.
The repeated word-ending characters are merged
into a chunk. To further normalize the out-
put words, we propose an algorithm DCB-Norm,
which is a dictionary-based tokenization with a
term normalization process. The DCB-Norm al-
gorithm is shown in Figure 2. The algorithm per-
forms text parsing with longest matching strat-
egy (LM PARSE). The strategy is used to solve

Table 3: An example of tokenized results

the ambiguity problem in which there are more
than one possible path to select in the parsing tree.
The longest matching uses the heuristic such that
longer terms contain better semantic than shorter
terms. Figure 3 illustrates the dictionary-based to-
kenization with longest matching. From the ex-
ample, there are four possible path to select. The
longest matching strategy select the path (shown
with a thick solid line) which contains a term with
the largest length.

Figure 2: DCB-Norm algorithm

Figure 3: An example of tokenizing parse tree.

13



A set of terms in a lexicon is stored in a trie,
an efficient data structure in terms of storage space
and retrieval time (Frakes and Baeza-Yates, 1992).
Figure 4 illustrates a trie storing an example set of
terms. The algorithm begins by parsing the input
text. A chunk of characters are looked up with
terms stored in TRIE. If the character chunk are
not found in TRIE and consists of same charac-
ters, it will be neglected. On the other hand, if a
character chunk is found in TRIE, a word bound-
ary marker (denoted with |) will be assigned at the
end of the chunk.

Figure 4: An example of trie

4 Experiments and discussion

In this section, we first perform a statistical analy-
sis on a Twitter corpus to observe language usage
characteristics. An experiment is then performed
to evaluate the proposed tokenization and normal-
ization solution.

4.1 Statistical analysis

To understand the characteristics of the language
usage among Thai users on Twitter, we analyze
a Twitter corpus consisting of 2,388,649 posts in
Thai language. The total number of all words in
the corpus is 25,683,296. The number of unique
tokens in the corpus is equal to 81,136.

We run the DCB-Norm algorithm on the corpus
and collect all repeated character chunks. Table 4
summarizes statistics of number of grams in re-
peated word-ending characters. Figure 5 shows
the plot of the gram statistics. It can be ob-
served that the occurrence of repeated characters
is gradually decreased with the number of grams.
From the corpus, we observe that some of the

Table 4: Number of grams in repeated characters

Figure 5: Number of grams in repeated characters

Table 5: Top ranked terms with repeated charac-
ters

14



Figure 6: Top ranked terms with repeated charac-
ters

posts containing more than 10 repeated character
grams. Another observation is the repeated char-
acters mostly occur at the end of the words as we
expected.

Next, we analyze the high frequent terms which
contains the repeated characters. Table 5 shows
top ranked terms with repeated characters. The to-
tal number of posts which contain some repeated
characters is 115,020 which is equal to 4.8% of
corpus size. Figure 6 shows the plot of top ranked
terms. From the figure, we observe the Zipf (or
long-tail) distribution over the terms with repeated
characters. Most of the top terms are very informal
and often used in conversational language. Some
of the terms are used to express the feeling and
emotion of the users.

4.2 Experiments and discussion

To evaluate the performance of the proposed ap-
proach, we perform an experiment by using a set
of 1,000 randomly selected Twitter posts written
in Thai language. Each post contains some re-
peated characters and is manually assigned with
correct word boundary markers. For the pro-
posed DCB-Norm algorithm, we use a lexicon
from LEXiTRON2 which contains 35,328 general
terms. We also include another lexicon consist-
ing of 1,341 terms frequently found in Twitter cor-
pus. Words obtained from Twitter lexicon include
chat, slangs and transliterated words from other
languages.

The performance evaluation is carried out on a
notebook with a 2 GHz Intel Core 2 Duo CPU, 4
GB RAM running under Mac OS X. We evaluate
the algorithm in terms of accuracy, i.e., the number

2LEXiTRON, http://lexitron.nectec.or.th

of correctly tokenized texts over the total number
of test texts. We also evaluate the running time
efficiency. The results are summarized in Table 6.
The overall accuracy is equal to 96.3%. To analyze
the errors, we manually look at the incorrectly to-
kenized results. We observe that in the case of all
words in the text are in the dictionary, the words
are recognized and the repeated characters are cor-
rectly removed. However, the problem is mostly
due to out-of-vocabulary (OOV) which causes the
incorrect assignment of word boundary markers.
As a result, the words with repeated characters at
the end could be not normalized correctly. Two er-
ror types associated with OOV problem is homo-
phonic transformation and transliteration. Table 7
shows some examples from the error analysis. The
simplest solution to the OOV problem is to manu-
ally collect newly created terms from the corpus.

Table 6: Evaluation results

Table 7: Examples of tokenized errors

5 Conclusion and future work

We proposed a tool called LexToPlus for tokeniz-
ing and normalizing Thai written texts. LexTo-
Plus is designed to handle the intentional word
errors commonly found in social media texts. In
this paper, we focus on solving the tokenization
errors caused by repeated characters at the end of
words. The proposed algorithm DCB-Norm is a
dictionary-based parser with a rule-based exten-
sion to merge and remove repeated characters. We

15



performed some statistical analysis on a Twitter
corpus consisting of over 2 million posts written in
Thai language. One interesting result is the rank-
ing distribution of top terms with repeated charac-
ters follows the Zipf or long-tail distribution. We
evaluated the proposed algorithm by using a cor-
pus of 1,000 manually tokenized texts. The accu-
racy is equal to 96.3% with the average throughput
of 435,596 words/second.

From the error analysis, we found that the
major problem is the out-of-vocabulary (OOV)
which comes from homophonic transformation
and transliteration. Although the OOV problem
can be partially solved by adding new terms into
the dictionary. However, it is labor intensive and
ineffective in long terms. For future work, we plan
to improve the performance of the proposed algo-
rithm by constructing a machine learning model to
automatically detect new terms based on the con-
textual information.

References
Chisato Asaga, Yusuf Mukarramah and Chiemi Watan-

abe. 2008. ONOMATOPEDIA: onomatopoeia on-
line example dictionary system extracted from data
on the web. Proc. of the 10th Asia-Pacific web conf.
on Progress in WWW research and development ,
601-612.

AiTi Aw, Min Zhang, Juan Xiao, and Jian Su. 2006. A
phrase-based statistical model for SMS text normal-
ization. Proc. of the COLING/ACL on Main confer-
ence poster sessions, 33–40.

Eric Brill and Robert C. Moore. 2000. An improved
error model for noisy channel spelling correction.
Proc. of the 38th Annual Meeting on Association for
Computational Linguistics, 286–293.

Marta R. Costa-Jussa and Rafael E. Banchs. 2013. Au-
tomatic normalization of short texts by combining
statistical and rule-based techniques. Language Re-
sources and Evaluation , 47(1):179–193.

William B. Frakes and Ricardo Baeza-Yates (Eds.).
1992. Information Retrieval: Data Structures and
Algorithms , Prentice-Hall, Englewood Cliffs, NJ.

Bo Han and Timothy Baldwin. 2011. Lexical normal-
isation of short text messages: makn sens a #twitter.
Proc. of the 49th Annual Meeting of the Association
for Computational Linguistics, 368–378.

Bo Han, Paul Cook, and Timothy Baldwin. 2013.
Lexical normalization for social media text. ACM
Transactions on Intelligent Systems and Technology,
4(1): 1–27.

Choochart Haruechaiyasak, Sarawoot Kongyoung and
Matthew Dailey. 2008. A comparative study on
Thai word segmentation approaches. Proc. of the
ECTI-CON 2008, 1:125-128.

Graeme Hirst and Alexander Budanitsky. 2005. Cor-
recting real-word spelling errors by restoring lexical
cohesion. Natural Language Engineering, 11:87–
111.

Ayumi Kato, Yusuke Fukazawa, Tomomasa Sato and
Taketoshi Mori. 2012. Extraction of onomatopoeia
used for foods from food reviews and its application
to restaurant search. Proc. of the 21st int. conf. com-
panion on World Wide Web , 719-728.

Canasai Kruengkrai and Hitoshi Isahara. 2006. A con-
ditional random field framework for thai morpholog-
ical analysis. Proc. of the Fifth Int. Conf. on Lan-
guage Resources and Evaluation (LREC-2006).

Fei Liu, Fuliang Weng, and Xiao Jiang. 2012. A
broad-coverage normalization system for social me-
dia language. Proc. of the 50th Annual Meeting
of the Association for Computational Linguistics,
1035-1044.

Surapant Meknavin, Paisarn Charoenpornsawat, and
Boonserm Kijsirikul. 1997. Feature-Based Thai
Word Segmentation. Proc. of the Natural Language
Processing Pacific Rim Symposium, 289–296.

Fuchun Peng, Fangfang Feng, and Andrew McCallum.
2004. Chinese Segmentation and New Word Detec-
tion Using Conditional Random Fields. Proc. of the
20th COLING, 562–568.

James L. Peterson. 1980. Computer programs for de-
tecting and correcting spelling errors. Communica-
tions of ACM, 23:676–687.

Richard Sproat, Alan W. Black, Stanley Chen, Shankar
Kumar, Mari Ostendorf, and Christopher Richards.
2001. Normalization of non-standard words. Com-
puter Speech and Language, 15(3):287 333.

Yuzu Uchida, Kenji Araki, and Jun Yoneyama. 2012.
Classification of Emotional Onomatopoeias Based
on Questionnaire Surveys. Proc. of the 2012 Inter-
national Conference on Asian Language Processing,
1–4.

16


