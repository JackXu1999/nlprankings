











































Language Modeling with Sparse Product of Sememe Experts


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4642–4651
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

4642

Language Modeling with Sparse Product of Sememe Experts

Yihong Gu
1,2,⇤

Jun Yan
1,3,⇤

Hao Zhu
1,2,⇤

Zhiyuan Liu
1,2,†

Ruobing Xie
4

Maosong Sun
1,2

Fen Lin
4

Leyu Lin
4

1Institute for Artificial Intelligence
State Key Lab on Intelligent Technology and Systems

2Department of CST, 3Department of EE, Tsinghua University, Beijing, China
4Search Product Center, WeChat Search Application Department, Tencent
{gyh15,j-yan15,zhuhao15}@mails.tsinghua.edu.cn,
{lzy,sms}@tsinghua.edu.cn, xrbsnowing@163.com,

{felicialin,goshawklin}@tencent.com

Abstract

Most language modeling methods rely on
large-scale data to statistically learn the se-
quential patterns of words. In this pa-
per, we argue that words are atomic lan-
guage units but not necessarily atomic seman-
tic units. Inspired by HowNet, we use se-
memes, the minimum semantic units in hu-
man languages, to represent the implicit se-
mantics behind words for language model-
ing, named Sememe-Driven Language Model
(SDLM). More specifically, to predict the next
word, SDLM first estimates the sememe dis-
tribution given textual context. Afterwards, it
regards each sememe as a distinct semantic ex-
pert, and these experts jointly identify the most
probable senses and the corresponding word.
In this way, SDLM enables language mod-
els to work beyond word-level manipulation to
fine-grained sememe-level semantics, and of-
fers us more powerful tools to fine-tune lan-
guage models and improve the interpretabil-
ity as well as the robustness of language mod-
els. Experiments on language modeling and
the downstream application of headline gener-
ation demonstrate the significant effectiveness
of SDLM. Source code and data used in the
experiments can be accessed at https://
github.com/thunlp/SDLM-pytorch.

1 Introduction

Language Modeling (LM) aims to measure the
probability of a word sequence, reflecting its flu-
ency and likelihood as a feasible sentence in a
human language. Language Modeling is an es-
sential component in a wide range of natural lan-
guage processing (NLP) tasks, such as Machine
Translation (Brown et al., 1990; Brants et al.,
2007), Speech Recognition (Katz, 1987), Informa-
tion Retrieval (Berger and Lafferty, 1999; Ponte

⇤ Equal contribution.
† Correspondence author.

(a) Conventional Decoder

context 
vector

word  
distribution

context 
vector

Sememe-Driven Decoder
word 

distributionsememe distributionSememe 
Predictor

Sense 
Predictor 

Word 
Predictor

sense 
distribution(b)

Figure 1: Decoder of (a) Conventional Language
Model, (b) Sememe-Driven Language Model.

and Croft, 1998; Miller et al., 1999; Hiemstra,
1998) and Document Summarization (Rush et al.,
2015; Banko et al., 2000).

A probabilistic language model calculates the
conditional probability of the next word given
their contextual words, which are typically learned
from large-scale text corpora. Taking the sim-
plest language model for example, N-Gram es-
timates the conditional probabilities according to
maximum likelihood over text corpora (Jurafsky,
2000). Recent years have also witnessed the ad-
vances of Recurrent Neural Networks (RNNs) as
the state-of-the-art approach for language model-
ing (Mikolov et al., 2010), in which the context is
represented as a low-dimensional hidden state to
predict the next word.

Those conventional language models including
neural models typically assume words as atomic
symbols and model sequential patterns at word
level. However, this assumption does not neces-
sarily hold to some extent. Let us consider the fol-
lowing example sentence for which people want to
predict the next word in the blank,

The U.S. trade deficit last year is initially

estimated to be 40 billion .

People may first realize a unit should be filled in,
then realize it should be a currency unit. Based on
the country this sentence is talking about, the U.S.,
one may confirm it should be an American cur-

https://github.com/thunlp/SDLM-pytorch
https://github.com/thunlp/SDLM-pytorch


4643

rency unit and predict the word dollars. Here, the
unit, currency, and American can be regarded as
basic semantic units of the word dollars. This pro-
cess, however, has not been explicitly taken into
consideration by conventional language models.
That is, although in most cases words are atomic
language units, words are not necessarily atomic
semantic units for language modeling. We ar-
gue that explicitly modeling these atomic semantic
units could improve both the performance and the
interpretability of language models.

Linguists assume that there is a limited close
set of atomic semantic units composing the se-
mantic meanings of an open set of concepts (i.e.
word senses). These atomic semantic units are
named sememes (Dong and Dong, 2006).i Since
sememes are naturally implicit in human lan-
guages, linguists have devoted much effort to ex-
plicitly annotate lexical sememes for words and
build linguistic common-sense knowledge bases.
HowNet (Dong and Dong, 2006) is one of the
representative sememe knowledge bases, which
annotates each Chinese word sense with its se-
memes. The philosophy of HowNet regards the
parts and attributes of a concept can be well rep-
resented by sememes. HowNet has been widely
utilized in many NLP tasks such as word similar-
ity computation (Liu, 2002) and sentiment analy-
sis (Fu et al., 2013). However, less effort has been
devoted to exploring its effectiveness in language
models, especially neural language models.

It is non-trivial for neural language models to
incorporate discrete sememe knowledge, as it is
not compatible with continuous representations
in neural models. In this paper, we propose
a Sememe-Driven Language Model (SDLM) to
leverage lexical sememe knowledge. In order to
predict the next word, we design a novel sememe-
sense-word generation process: (1) We first esti-
mate sememes’ distribution according to the con-
text. (2) Regarding these sememes as experts, we
propose a sparse product of experts method to se-
lect the most probable senses. (3) Finally, the dis-
tribution of words could be easily calculated by
marginalizing out the distribution of senses.

We evaluate the performance of SDLM on the
language modeling task using a Chinese news-

i Note that although sememes are defined as the mini-
mum semantic units, there still exist several sememes for
capturing syntactic information. For example, the word
å “with” corresponds to one specific sememe ü˝Õ
“FunctWord”.

paper corpus People’s Daily ii (Renmin Ribao),
and also on the headline generation task using the
Large Scale Chinese Short Text Summarization
(LCSTS) dataset (Hu et al., 2015). Experimen-
tal results show that SDLM outperforms all those
data-driven baseline models. We also conduct case
studies to show that our model can effectively pre-
dict relevant sememes given context, which can
improve the interpretability and robustness of lan-
guage models.

2 Background

Language models target at learning the
joint probability of a sequence of words
P (w1, w2, · · · , wn), which is usually factor-
ized as P (w1, w2, · · · , wn) =

Q
n

t=1 P (w
t|w<t).

Bengio et al. (2003) propose the first Neural Lan-
guage Model as a feed-forward neural network.
Mikolov et al. (2010) use RNN and a softmax
layer to model the conditional probability. To be
specific, it can be divided into two parts in series.
First, a context vector gt is derived from a deep
recurrent neural network. Then, the probability
P (wt+1|wt) = P (wt+1;gt) is derived from a
linear layer followed by a softmax layer based
on gt. Let RNN(·, ·;✓NN) denote the deep
recurrent neural network, where ✓NN denotes the
parameters. The first part can be formulated as

gt = RNN(xwt , {ht�1l }
L

l=1;✓NN). (1)

Here we use subscripts to denote layers and su-
perscripts to denote timesteps. Thus ht

l
represents

the hidden state of the L-th layer at timestep t.
xwt 2 RH0 is the input embedding of word wt
where H0 is the input embedding size. We also
have gt 2 RH1 , where H1 is the dimension of the
context vector.

Supposing that there are N words in the lan-
guage we want to model, the second part can be
written as

P (wt+1;gt) =
exp(gtTwwt+1)P
w0 exp(g

tTww0)
, (2)

where ww is the output embedding of word w and
w1,w2, · · ·wN 2 RH2 . Here H2 is the output
embedding size. For a conventional neural lan-
guage model, H2 always equals to H1.

ii http://paper.people.com.cn/rmrb/



4644

( ) 
“apple(fruit)”

( ) 
“apple(computer)”

( ) 
“pear(fruit)”

 
“apple”

…

…
 

“pear”P(word)

P(sense)

 
“fruit”

 
“bring”

 
“computer”

 
“SpeBrand”

 
“able”

 
 “PatternVal”

… x
0.2

×

sememe 
experts

context 
vector

 
“I”

 
“in”

 
“orchard”

 
“pick”

LSTM LSTM LSTM LSTM

… x
0.1

×

x
0.9

×

x
0.1

×

x
0.3

×

x
0.2

×

Figure 2: An example of the architecture of our model.

Given the corpus {wt}n
t=1, the loss function is

defined by the negative log-likelihood:

L(✓) = � 1
n

nX

t=1

logP (wt|w<t;✓), (3)

where ✓ = {{xi}Ni=1, {wi}Ni=1,✓NN} is the set of
parameters that are needed to be trained.

3 Methodology

In this section, we present our SDLM which uti-
lizes sememe information to predict the probabil-
ity of the next word. SDLM is composed of three
modules in series: Sememe Predictor, Sense Pre-
dictor and Word Predictor. The Sememe Predictor
first takes the context vector as input and assigns a
weight to each sememe. Then each sememe is re-
garded as an expert and makes predictions about
the probability distribution over a set of senses
in the Sense Predictor. Finally, the probability of
each word is obtained in the Word Predictor.

Here we use an example shown in Figure 2 to
illustrate our architecture. Given context ⌘(ú
ÌX “In the orchard, I pick”, the actual next word
could be ˘ú “apples”. From the context, espe-
cially the wordúÌ “orchard” andX “pick”, we
can infer that the next word probably represents
a kind of fruit. So the Sememe Predictor assigns
a higher weight to the sememe 4ú “fruit” (0.9)
and lower weights to irrelevant sememes like 5
⌘ “computer” (0.1). Therefore in the Sense Pre-
dictor, the sense˘ú (4ú) “apple (fruit)” is as-
signed a much higher probability than the sense˘
ú (5⌘) “apple (computer)”. Finally, the prob-
ability of the word ˘ú “apple” is calculated as
the sum of the probabilities of its senses˘ú (4

 
“apple”

 
“PatternVal”

 
“computer”

 
“able”

 
“bring”

 
“SpeBrand”

 
“fruit”

mod
ifier

modifier

Sense #1:  
( ) 

“apple(computer)”

Sense #2:  
( ) 

“apple(fruit)”

word

Figure 3: An example of the word-sense-sememe hier-
archy.

ú) “apple(fruit)” and ˘ú (5⌘) “apple (com-
puter)”.

In the following subsections, we first introduce
the word-sense-sememe hierarchy in HowNet, and
then give details about our SDLM.

3.1 Word-Sense-Sememe Hierarchy

We also use the example of “apple” to illustrate
the word-sense-sememe hierarchy. As shown in
Figure 3, the word ˘ú “apple” has two senses,
one is the Apple brand, the other is a kind of fruit.
Each sense is annotated with several sememes or-
ganized in a hierarchical structure. More specifi-
cally, in HowNet, sememes “PatternVal”, “bring”,
“SpeBrand”, “computer” and “able” are annotated
with the word “apple” and organized in a tree
structure. In this paper, we ignore the structural
relationship between sememes. For each word, we
group all its sememes as an unordered set.

We present the notations that we use in the fol-
lowing subsections as follows. We define the over-
all sememe, sense, and word set as E , S and W .
And we suppose the corpus contains K = |E| se-
memes, M = |S| senses and N = |W| words.
For word w 2 W , we denote its corresponding
sense set as S(w). For sense s 2 S(w), we de-
note its corresponding sememes as an unordered
set E(s) = {en1 , en2 , · · · , enk} ⇢ E = {ek}Kk=1.

3.2 Sememe Predictor

The Sememe Predictor takes the context vec-
tor g 2 RH1 as input and assigns a weight to
each sememe. We assume that given the context
w

1
, w

2
, · · · , wt�1, the events that word wt con-

tains sememe ek (k 2 {1, 2, · · · ,K}) are indepen-
dent, since the sememe is the minimum semantic
unit and there is no semantic overlap between any
two different sememes. For simplicity, we ignore



4645

the superscript t. We design the Sememe Predic-
tor as a linear decoder with the sigmoid activation
function. Therefore, qk, the probability that the
next word contains sememe ek, is formulated as

qk = P (ek|g) = �(gTvk + bk), (4)

where vk 2 RH1 , bk 2 R are trainable parameters,
and �(·) denotes the sigmoid activation function.

3.3 Sense Predictor and Word Predictor

The architecture of the Sense Predictor is moti-
vated by Product of Experts (PoE) (Hinton, 1999).
We regard each sememe as an expert that only
makes predictions on the senses connected with it.
Let D(ek) denote the set of senses that contain se-
meme ek, the k-th expert. Different from conven-
tional neural language models, which directly use
the inner product of the context vector g 2 RH1
and the output embedding ww 2 RH2 for word
w to generate the score for each word, we use
�
(k)(g,w) to calculate the score given by expert

ek. And we choose a bilinear function parame-
terized with a matrix Uk 2 RH1⇥H2 as a straight
implementation of �(k)(·, ·):

�(k)(g,w) = gTUkw. (5)

Let ws denote the output embedding of sense
s. The score of sense s provided by sememe ex-
pert ek can be written as �(k)(g,ws). Therefore,
P

(ek)(s|g), the probability of sense s given by ex-
pert ek, is formulated as

P (ek)(s|g) = exp(qkCk,s�
(k)(g,ws))P

s02D(ek) exp(qkCk,s0�
(k)(g,ws0))

, (6)

where Ck,s is a normalization constant because
sense s is not connected to all experts (the
connections are sparse with approximately �N
edges, � < 5). Here we can choose either
Ck,s = 1/|E(s)| (left normalization) or Ck,s =
1/
p

|E(s)||D(ek)| (symmetric normalization).
In the Sense Predictor, qk can be viewed as

a gate which controls the magnitude of the term
Ck,s�

(k)(g,wws), thus control the flatness of the
sense distribution provided by sememe expert ek.
Consider the extreme case when qk ! 0, the pre-
diction will converge to the discrete uniform dis-
tribution. Intuitively, it means that the sememe ex-
pert will refuse to provide any useful information
when it is not likely to be related to the next word.

Finally, we summarize the predictions on sense
s by taking the product of the probabilities given

by relevant experts and then normalize the result;
that is to say, P (s|g), the probability of sense s,
satisfies

P (s|g) /
Y

ek2E(s)
P (ek)(s|g). (7)

Using Equation 5 and 6, we can formulate
P (s|g) as

P (s|g) =
exp(

P
ek2E(s) qkCk,sg

TUkws)P
s0 exp(

P
ek2E(s

0) qkCk,s0gTUkws0)
. (8)

It should be emphasized that all the supervision
information provided by HowNet is embodied in
the connections between the sememe experts and
the senses. If the model wants to assign a high
probability to sense s, it must assign a high prob-
ability to some of its relevant sememes. If the
model wants to assign a low probability to sense
s, it can assign a low probability to its relevant
sememes. Moreover, the prediction made by se-
meme expert ek has its own tendency because of
its own �(k)(·, ·). Besides, the sparsity of con-
nections between experts and senses is also de-
termined by HowNet itself. For our dataset, on
average, a word is connected with 3.4 sememe ex-
perts and each sememe expert will make predic-
tions about 22 senses.

As illustrated in Figure 2, in the Word Predic-
tor, we get P (w|g), the probability of word w,
by summing up probabilities of corresponding s
given by the Sense Predictor, that is

P (w|g) =
X

s2S(w)
P (s|g). (9)

3.4 Implementation Details

Basis Matrix Actually, HowNet contains K ⇡
2000 sememes. In practice, we cannot directly in-
troduce K ⇥ H1 ⇥ H2 parameters, which might
be computationally infeasible and lead to overfit-
ting. To address this problem, we apply a weight-
sharing trick called the basis matrix. We use R
basis matrices and their weighted sum to estimate
Uk:

Uk =
RX

r=1

↵k,rQr, (10)

where Qr 2 RH1⇥H2 , ↵k,r > 0 are trainable pa-
rameters, and

P
R

r=1 ↵k,r = 1.
Weight Tying To incorporate the weight tying
strategy (Inan et al., 2017; Press and Wolf, 2017),
we use the same output embedding for multiple



4646

senses of a word. To be specific, the sense output
embedding ws for each s 2 S(w) is the same as
the word input embedding xw.

4 Experiments

We evaluate our SDLM on a Chinese language
modeling dataset, namely People’s Daily based on
perplexity.iii Furthermore, to show that our SDLM
structure can be a generic Chinese word-level de-
coder for sequence-to-sequence learning, we con-
duct a Chinese headline generation experiment on
the LCSTS dataset. Finally, we explore the inter-
pretability of our model with cases, showing the
effectiveness of utilizing sememe knowledge.

4.1 Language Modeling

Dataset

We choose the People’s Daily Corpus, which is
widely used for Chinese NLP tasks, as the re-
source. It contains one month’s news text from
People’s Daily (Renmin Ribao). Taking Penn
Treebank (PTB) (Marcus et al., 1993) as a ref-
erence, we build a dataset for Chinese language
modeling based on the People’s Daily Corpus with
734k, 10k and 19k words in the training, valida-
tion and test set. After the preprocessing similar
to (Mikolov et al., 2010) (see Appendix A), we get
our dataset and the final vocabulary size is 13,476.

Baseline

As for baselines, we consider three kinds of neural
language modeling architectures with LSTM cells:
simple LSTM, Tied LSTM and AWD-LSTM.
LSTM and Tied LSTM Zaremba et al. (2014)
use the dropout strategy to prevent overfitting for
neural language models and adopt it to two-layer
LSTMs with different embedding and hidden size:
650 for medium LSTM, and 1500 for large LSTM.
Employing the weight tying strategy, we get Tied
LSTM with better performance. We set LSTM and
Tied LSTM of medium and large size as our base-
line models and use the code from PyTorch exam-
plesiv as their implementations.
AWD-LSTM Based on several strategies for reg-
ularizing and optimizing LSTM-based language
models, Merity et al. (2018) propose AWD-LSTM

iii Although we only conduct experiments on Chinese cor-
pora, we argue that this model has the potential to be ap-
plied to other languages in the light of works on construc-
tion sememe knowledge bases for other languages, such
as (Qi et al., 2018).

iv https://github.com/pytorch/examples/
tree/master/word_language_model

as a three-layer neural network, which serves as a
very strong baseline for word-level language mod-
eling. We build it with the code released by the
authorsv.
Variants of Softmax Meanwhile, to compare our
SDLM with other language modeling decoders,
we set cHSM (Class-based Hierarchical Softmax)
(Goodman, 2001), tHSM (Tree-based Hierarchi-
cal Softmax) (Mikolov et al., 2013) and MoS
(Mixture of Softmaxes) (Yang et al., 2018) as
the baseline add-on structures to the architectures
above.

Experimental Settings

We apply our SDLM and other variants of softmax
structures to the architectures mentioned above:
LSTM (medium / large), Tied LSTM (medium /
large) and AWD-LSTM. MoS and SDLM are only
applied on the models that incorporate weight ty-
ing, while tHSM is only applied on the models
without weight tying, since it is not compatible
with this strategy.

For a fair comparison, we train these mod-
els with same experimental settings and conduct
a hyper-parameter search for baselines as well
as our models (the search setting and the opti-
mal hyper-parameters can be found in Appendix
C.1). We keep using these hyper-parameters in our
SDLM for all architectures. It should be empha-
sized that we use the SGD optimizer for all archi-
tectures, and we decrease the learning rate by a
factor of 2 if no improvement is observed on the
validation set. We uniformly initialize the word
embeddings, the class embeddings for cHSM and
the non-leaf embeddings for tHSM in [�0.1, 0.1].
In addition, we set R, the number of basis matri-
ces, to 5 in Tied LSTM architecture and to 10 in
AWD-LSTM architecture. We choose the left nor-
malization strategy because it performs better.

Experimental Results

Table 1 shows the perplexity on the validation and
test set of our models and the baseline models.
From Table 1, 2, and 3, we can observe that:
1. Our models outperform the corresponding base-
line models of all structures, which indicates the
effectiveness of our SDLM. Moreover, our SDLM
not only consistently outperforms state-of-the-art
MoS model, but also offers much better inter-
pretability (as described in Sect. 4.3), which

v https://github.com/salesforce/
awd-lstm-lm

https://github.com/pytorch/examples/tree/master/word_language_model
https://github.com/pytorch/examples/tree/master/word_language_model
https://github.com/salesforce/awd-lstm-lm
https://github.com/salesforce/awd-lstm-lm


4647

makes it possible to interpret the prediction pro-
cess of the language model. Note that under a fair
comparison, we do not see MoS’s improvement
over AWD-LSTM while our SDLM outperforms
it by 1.20 with respect to perplexity on the test set.
2. To further locate the performance improve-
ment of our SDLM, we study the perplexity of
the single-sense words and multi-sense words sep-
arately on Tied LSTM (medium) and Tied LSTM
(medium) + SDLM. Improvements with respect to
perplexity are presented in Table 2. The perfor-
mance on both single-sense words and multi-sense
words gets improved while multi-sense words
benefit more from SDLM structure because they
have richer sememe information.
3. In Table 3 we study the perplexity of words
with different mean number of sememes. We can
see that our model outperforms baselines in all
cases and is expected to benefit more as the mean
number of sememes increases.

Model #Paras Validation Test
LSTM (medium) 24M 116.46 115.51

+ cHSM 24M 129.12 128.12
+ tHSM 24M 151.00 150.87

Tied LSTM (medium) 15M 105.35 104.67
+ cHSM 15M 116.78 115.66
+ MoS 17M 98.47 98.12
+ SDLM 17M 97.75 97.32

LSTM (large) 76M 112.39 111.66
+ cHSM 76M 120.07 119.45
+ tHSM 76M 140.41 139.61

Tied LSTM (large) 56M 101.46 100.71
+ cHSM 56M 108.28 107.52
+ MoS 67M 94.91 94.40
+ SDLM 67M 94.24 93.60

AWD-LSTMiv 26M 89.35 88.86
+ MoS 26M 92.98 92.76
+ SDLM 27M 88.16 87.66

Table 1: Single model perplexity on validation and test
sets on the People’s Daily dataset.

#senses = 1 #senses > 1
Baseline ppl 93.21 121.18
SDLM ppl 87.22 111.88
�ppl 5.99 9.29
�ppl/Baseline ppl 6.4% 7.8%

Table 2: Perplexity of words with different number of
senses on the test set.

We also test the robustness of our model by ran-
domly removing 10% sememe-sense connections
in HowNet. The test perplexity for Tied LSTM

iv We find that multi-layer AWD-LSTM has problems con-
verging when adopting cHSM, so we skip that result.

[1, 2) [2, 4) [4, 7) [7, 14)
Baseline ppl 71.56 161.32 557.26 623.71
SDLM ppl 68.47 114.95 465.29 476.45
�ppl 3.09 16.36 91.98 147.25
�ppl/Baseline ppl 4.3% 10.1% 16.5% 23.61%

Table 3: Perplexity of words with different mean num-
ber of sememes on the test set.

(medium) + SDLM slightly goes up to 97.67, com-
pared to 97.32 with a complete HowNet, which
shows that our model is robust to tiny incomplete-
ness of annotations. However, the performance
of out model is still largely dependent upon the
accuracy of sememe annotations. As HowNet
is continuously updated, we expect our model to
perform better with sememe knowledge of higher
quality.

4.2 Headline Generation

Dataset

We use the LCSTS dataset to evaluate our SDLM
structure as the decoder of the sequence-to-
sequence model. As its author suggests, we di-
vide the dataset into the training set, the validation
set and the test set, whose sizes are 2.4M, 8.7k
and 725 respectively. Details can be found in Ap-
pendix B.

Models

For this task, we consider two models for compar-
ison.

RNN-context As described in (Bahdanau et al.,
2015), RNN-context is a basic sequence-to-
sequence model with a bi-LSTM encoder, an
LSTM decoder and attention mechanism adopted.
The context vector is concatenated with the word
embedding at each timestep when decoding. It’s
widely used for sequence-to-sequence learning, so
we set it as the baseline model.

RNN-context-SDLM Based on RNN-context,
we substitute the decoder with our proposed
SDLM and name it RNN-context-SDLM.

Experimental Settings

We implement our models with PyTorch, on top of
the OpenNMT librariesv. For both models, we set
the word embedding size to 250, the hidden unit
size to 250, the vocabulary size to 40000, and the
beam size of the decoder to 5. For RNN-context-
SDLM, we set the number of basis matrices to
3. We conduct a hyper-parameter search for both

v http://opennmt.net

http://opennmt.net


4648

models (see Appendix C.2 for settings and optimal
hyper-parameters).

Experimental Results

Following previous works, we report the F1-score
of ROUGE-1, ROUGE-2, and ROUGE-L on the
test set. Table 4 shows that our model outperforms
the baseline model on all metrics. We attribute the
improvement to the use of SDLM structure.

Words in headlines do not always appear in the
corresponding articles. However, words with the
same sememes have a high probability to appear in
the articles intuitively. Therefore, a probable rea-
son for the improvement is that our model could
predict sememes highly relevant to the article, thus
generate more accurate headlines. This could be
corroborated by our case study.

Model Rouge-1 Rouge-2 Rouge-L
RNN-context 37.5 25.0 34.9
RNN-context-SDLM 38.9 26.2 36.2

Table 4: ROUGE scores of both models on the LCSTS
test set.

4.3 Case Study

The above experiments demonstrate the effective-
ness of our SDLM. Here we present some samples
from the test set of the People’s Daily Corpus in
Table 5 as well as the LCSTS dataset in Table 6
and conduct further analysis.

For each example of language modeling, given
the context of previous words, we list the Top
5 words and Top 5 sememes predicted by our
SDLM. The target words and the sememes anno-
tated with them in HowNet are blackened. Note
that if the target word is an out-of-vocabulary

Example (1)
ªté˝8◆⌃Ó�e0°: <N> ⇥
The U.S. trade deficit last year is initially estimated to be <N> .

Top 5 word prediction
éééCCC “dollar” � “,” ⇥ “.”
ÂC “yen” å “and”

Top 5 sememe prediction
FFF⇢⇢⇢ “commerce” ———ççç “finance” UUUMMM “unit”
⇢⌘ “amount” ◆ “proper name”

Example (2)
? ;⌃ Ú~rÜ�y}‰⇥
Albanian Prime Minister has signed an order.

Top 5 word prediction
Ö “inside” <unk> ( “at”
T “tower” å “and”

Top 5 sememe prediction
??? “politics” ∫∫∫ “person” ±I “flowers”
≈≈≈˚̊̊ “undertake” 4fl “waters”

Table 5: Some examples of word and sememe predic-
tions on the test set of the People’s Daily Corpus.

(OOV) word, helpful sememes that are related to
the target meaning are blackened.

Sememes annotated with the corresponding
sense of the target word éC “dollar” are UM
“unit”, F⇢ “commerce”, —ç “finance”, '�
“money” andé˝ “US”. In Example (1), the tar-
get word “dollar” is predicted correctly and most
of its sememes are activated in the predicting pro-
cess. It indicates that our SDLM has learned the
word-sense-sememe hierarchy and used sememe
knowledge to improve language modeling.

Example (2) shows that our SDLM can provide
interpretable results on OOV word prediction with
sememe information associated with it. The tar-
get word here should be the name of the Albanian
prime minister, which is out of vocabulary. But
with our model, one can still conclude that this
word is probably relevant to the sememe “poli-
tics”, “person”, “flowers”, “undertake” and “wa-
ters”, most of which characterize the meaning of
this OOV word – the name of a politician. This
feature can be helpful when the vocabulary size is
limited or there are many terminologies and names
in the corpus.

For the example of headline generation, given
the article and previous words, when generating
the word � “student”, except the sememe Ñô
“predict”, all other Top 5 predicted sememes have
high relevance to either the predicted word or the
context. To be specific, the sememef` “study”
is annotated with � “student” in HowNet. ⇤
’ “exam” indicates “college entrance exam”. y
öLP “brand” indicates “BMW”. And ÿI
“higher” indicates “higher education”, which is
the next step after this exam. We can conclude that
with sememe knowledge, our SDLM structure can
extract critical information from both the given ar-
ticle and generated words explicitly and produce
better summarization based on it.

5 Related Work

Neural Language Modeling. RNNs have
achieved state-of-the-art performance in the
language modeling task since Mikolov et al.
(2010) first apply RNNs for language modeling.
Much work has been done to improve RNN-based
language modeling. For example, a variety of
work (Zaremba et al., 2014; Gal and Ghahramani,
2016; Merity et al., 2017, 2018) introduces many
regularization and optimization methods for
RNNs. Based on the observation that the word



4649

Article
8Â�⌧∞��ùlf¬†ÿ⇤Ñ7⇤�
⇤: \⌦´ì�‡�·—⇤��°6\⌦K
:�ŒÃ��⇢⌃s—⇤��Œ���í9
0≤⇥v„˙¬�⇢ “`ÂS⌘8/�J
�`1Â⌘� ”ÓM�S∫⇤�Ú´ÿY⇥
On the 8th in Fuxin, a male student drove a BMW to
take the college entrance exam and was caught cheating.
Because the teacher confiscated his mobile phone, he
kicked the teacher from the last row to the podium and
shouted: ”Do you know who my dad is? How dare you
catch me!” Currently, this student has been detained.

Gold
7�ÿ⇤\⌦˝S—⇤��⇢`ÂS⌘8/
��
In the college entrance exam, a male student caught
cheating hit the teacher: Do you know who my dad is?

RNN-context-SDLM
ÿ⇤�\⌦´ì⇢`ÂS⌘8/�J�
In the college entrance exam, a student was caught
cheating: Do you know who my dad is?

Top 5 sememe prediction
⇤⇤⇤’’’ “exam” fff`̀̀ “study” yyyöööLLLPPP “brand”
Ñô “predict” ÿÿÿIII “higher”

Table 6: An example of generated headlines on the LC-
STS test set.

appearing in the previous context is more likely to
appear again, some work (Grave et al., 2017a,b)
proposes to use cache for improvements. In this
paper, we mainly focus on the output decoder,
the module between the context vector and the
predicted probability distribution. Similar to our
SDLM, Yang et al. (2018) propose a high-rank
model which adopts a Mixture of Softmaxes
structure for the output decoder. However,
our model is sememe-driven with each expert
corresponding to an interpretable sememe.

Hierarchical Decoder Since softmax computa-
tion on large vocabulary is time-consuming, there-
fore being a dominant part of the model’s com-
plexity, various hierarchical softmax models have
been proposed to address this issue. These mod-
els can be categorized to class-based models and
tree-based models according to their hierarchi-
cal structure. Goodman (2001) first proposes the
class-based model which divides the whole vocab-
ulary into different classes and uses a hierarchi-
cal softmax decoder to model the probability as
P(word) = P(word|class)P(class), which is sim-
ilar to our model. For the tree-based models, all
words are organized in a tree structure and the
word probability is calculated as the probability of
always choosing the correct child along the path
from the root node to the word node. While Morin
and Bengio (2005) utilize knowledge from Word-

Net to build the tree, Mnih and Hinton (2008)
build it in a bootstrapping way and Mikolov et al.
(2013) construct a Huffman Tree based on word
frequencies. Recently, Jiang et al. (2017) reform
the tree-based structure to make it more efficient
on GPUs. The major differences between our
model and theirs are the purpose and the moti-
vation. Our model targets at improving the per-
formance and interpretability of language model-
ing using external knowledge in HowNet. There-
fore, we take its philosophy of the word-sense-
sememe hierarchy to design our hierarchical de-
coder. Meanwhile, the class-based and tree-based
models are mainly designed to speed up the soft-
max computation in the training process.

Sememe. Recently, there are a lot of works con-
centrating on utilizing sememe knowledge in tra-
ditional natural language processing tasks. For ex-
ample, Niu et al. (2017) use sememe knowledge
to improve the quality of word embeddings and
cope with the problem of word sense disambigua-
tion. Xie et al. (2017) apply matrix factorization to
predict sememes for words. Jin et al. (2018) im-
prove their work by incorporating character-level
information. Our work extends the previous works
and tries to combine word-sense-sememe hierar-
chy with the sequential model. To be specific,
this is the first work to improve the performance
and interpretability of Neural Language Modeling
with sememe knowledge.

Product of Experts. As Hinton (1999, 2002)
propose, the final probability can be calculated as
the product of probabilities given by experts.Gales
and Airey (2006) apply PoE to the speech recog-
nition where each expert is a Gaussian mixture
model. Unlike their work, in our SDLM, each
expert is mapped to a sememe with better inter-
pretability. Moreover, as the final distribution is
a categorical distribution, each expert is only re-
sponsible for making predictions on a subset of
the categories (usually less than 10), so we call it
Sparse Product of Experts.

Headline Generation. Headline generation is
a kind of text summarization tasks. In recent
years, with the advances of RNNs, a lot of works
have been done in this domain. The encoder-
decoder models (Sutskever et al., 2014; Cho et al.,
2014) have achieved great success in sequence-
to-sequence learning. Rush et al. (2015) pro-
pose a local attention-based model for abstractive



4650

sentence summarization. Gu et al. (2016) intro-
duce the copying mechanism which is close to the
rote memorization of the human being. Ayana
et al. (2016) employ the minimum risk training
strategy to optimize model parameters. Different
from these works, we focus on the decoder of the
sequence-to-sequence model, and adopt SDLM to
utilize sememe knowledge for sentence genera-
tion.

6 Conclusion and Further Work

In this paper, we propose an interpretable
Sememe-Driven Language Model with a hier-
archical sememe-sense-word decoder. Besides
interpretability, our model also achieves state-
of-the-art performance in the Chinese Language
Modeling task and shows improvement in the
Headline Generation task. These results indicate
that SDLM can successfully take advantages of se-
meme knowledge.

As for future work, we plan the following re-
search directions: (1) In language modeling, given
a sequence of words, a sequence of correspond-
ing sememes can also be obtained. We will uti-
lize the context sememe information for better se-
meme and word prediction. (2) Structural infor-
mation about sememes in HowNet is ignored in
our work. We will extend our model with the hi-
erarchical sememe tree for more accurate relations
between words and their sememes. (3) It is imag-
inable that the performance of SDLM will be sig-
nificantly influenced by the annotation quality of
sememe knowledge. We will also devote to fur-
ther enrich the sememe knowledge for new words
and phrases, and investigate its effect on SDLM.

Acknowledgement

This work is supported by the 973 Program (No.
2014CB340501), the National Natural Science
Foundation of China (NSFC No. 61572273)
and the research fund of Tsinghua University-
Tencent Joint Laboratory for Internet Innova-
tion Technology. This work is also funded by
China Association for Science and Technology
(2016QNRC001). Hao Zhu and Jun Yan are sup-
ported by Tsinghua University Initiative Scientific
Research Program. We thank all members of Ts-
inghua NLP lab. We also thank anonymous re-
viewers for their careful reading and their insight-
ful comments.

References

Shiqi Shen Ayana, Zhiyuan Liu, and Maosong Sun.
2016. Neural headline generation with minimum
risk training. arXiv preprint arXiv:1604.01904.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In Proceedings of
ICLR.

Michele Banko, Vibhu O Mittal, and Michael J Wit-
brock. 2000. Headline generation based on statisti-
cal translation. In Proceedings of ACL, pages 318–
325. Association for Computational Linguistics.

Yoshua Bengio, Rejean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3:1137–1155.

Adam Berger and John Lafferty. 1999. Information re-
trieval as statistical translation. In Proceedings of
SIGIR, pages 222–229. ACM.

Thorsten Brants, Ashok C Popat, Peng Xu, Franz J
Och, and Jeffrey Dean. 2007. Large language
models in machine translation. In Proceedings of
EMNLP.

Peter F Brown, John Cocke, Stephen A Della Pietra,
Vincent J Della Pietra, Fredrick Jelinek, John D Laf-
ferty, Robert L Mercer, and Paul S Roossin. 1990. A
statistical approach to machine translation. Compu-
tational linguistics, 16(2):79–85.

Kyunghyun Cho, Bart Van Merriënboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using rnn encoder-decoder
for statistical machine translation. In Proceedings
of EMNLP.

Zhendong Dong and Qiang Dong. 2006. Hownet and
the computation of meaning (with Cd-rom). World
Scientific.

Xianghua Fu, Guo Liu, Yanyan Guo, and Zhiqiang
Wang. 2013. Multi-aspect sentiment analysis for
chinese online social reviews based on topic model-
ing and hownet lexicon. Knowledge-Based Systems,
37:186–195.

Yarin Gal and Zoubin Ghahramani. 2016. A theoret-
ically grounded application of dropout in recurrent
neural networks. In Proceedings of NIPS.

M. J. F. Gales and S. S. Airey. 2006. Product of gaus-
sians for speech recognition. Computer Speech and
Language, 20(1):22–40.

J Goodman. 2001. Classes for fast maximum entropy
training. In Proceedings of ICASSP, pages 561–564
vol.1.



4651

Edouard Grave, Moustapha Cisse, and Armand Joulin.
2017a. Unbounded cache model for online language
modeling with open vocabulary. In Proceedings of
NIPS.

Edouard Grave, Armand Joulin, and Nicolas Usunier.
2017b. Improving neural language models with a
continuous cache. In Proceedings of ICLR.

Jiatao Gu, Zhengdong Lu, Hang Li, and Victor OK
Li. 2016. Incorporating copying mechanism in
sequence-to-sequence learning. In Proceedings of
ACL, pages 1631–1640.

Djoerd Hiemstra. 1998. A linguistically motivated
probabilistic model of information retrieval. In Pro-
ceedings of TPDL, pages 569–584. Springer.

G. E Hinton. 1999. Products of experts. In Artificial
Neural Networks, 1999. ICANN 99. Ninth Interna-

tional Conference on, pages 1–6 vol.1.

G. E. Hinton. 2002. Training products of experts by
minimizing contrastive divergence. MIT Press.

Baotian Hu, Qingcai Chen, and Fangze Zhu. 2015. Lc-
sts: A large scale chinese short text summarization
dataset. In Proceedings of EMNLP.

Hakan Inan, Khashayar Khosravi, and Richard Socher.
2017. Tying word vectors and word classifiers: A
loss framework for language modeling. In Proceed-
ings of ICLR.

Nan Jiang, Wenge Rong, Min Gao, Yikang Shen,
Zhang Xiong, Nan Jiang, Wenge Rong, Min Gao,
Yikang Shen, and Zhang Xiong. 2017. Exploration
of tree-based hierarchical softmax for recurrent lan-
guage models. In Proceedings of IJCAI.

Huiming Jin, Hao Zhu, Zhiyuan Liu, Ruobing Xie,
Maosong Sun, Fen Lin, and Leyu Lin. 2018. In-
corporating chinese characters of words for lexical
sememe prediction. In Proceedings of ACL, pages
2439–2449. Association for Computational Linguis-
tics.

Dan Jurafsky. 2000. Speech & language processing.
chapter 4. Pearson Education India.

Slava Katz. 1987. Estimation of probabilities from
sparse data for the language model component of a
speech recognizer. IEEE transactions on acoustics,
speech, and signal processing, 35(3):400–401.

Qun Liu. 2002. Word similarity computing based on
hownet. Computational linguistics and Chinese lan-
guage processing, 7(2):59–76.

Mitchell P. Marcus, Beatrice Santorini, and Ann
Marcinkiewicz, Mary. 1993. Building a large anno-
tated corpus of English: The Penn Treebank. Com-
putational Linguistics, 19:313–330.

Stephen Merity, Nitish Shirish Keskar, and Richard
Socher. 2018. Regularizing and optimizing LSTM
language models. In Proceedings of ICLR.

Stephen Merity, Bryan Mccann, and Richard Socher.
2017. Revisiting activation regularization for lan-
guage rnns. arXiv preprint arXiv:1708.01009.

Tomáš Mikolov, Martin Karafiát, Lukáš Burget, Jan
Černockỳ, and Sanjeev Khudanpur. 2010. Recurrent
neural network based language model. In Proceed-
ings of INTERSPEECH.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeffrey Dean. 2013. Distributed represen-
tations of words and phrases and their composition-
ality. In Proceedings of NIPS, pages 3111–3119.

David RH Miller, Tim Leek, and Richard M Schwartz.
1999. A hidden markov model information retrieval
system. In Proceedings of SIGIR, pages 214–221.
ACM.

Andriy Mnih and Geoffrey Hinton. 2008. A scalable
hierarchical distributed language model. In Pro-
ceedings of NIPS, pages 1081–1088.

Frederic Morin and Yoshua Bengio. 2005. Hierarchi-
cal probabilistic neural network language model. In
Proceedings of AISTATS.

Yilin Niu, Ruobing Xie, Zhiyuan Liu, and Maosong
Sun. 2017. Improved word representation learning
with sememes. In Proceedings of ACL, volume 1,
pages 2049–2058.

Jay M Ponte and W Bruce Croft. 1998. A language
modeling approach to information retrieval. In Pro-
ceedings of SIGIR, pages 275–281. ACM.

Ofir Press and Lior Wolf. 2017. Using the output em-
bedding to improve language models. In Proceed-
ings of EACL.

Fanchao Qi, Yankai Lin, Maosong Sun, Hao Zhu,
Ruobing Xie, and Zhiyuan Liu. 2018. Cross-
lingual lexical sememe prediction. In Proceedings
of EMNLP.

Alexander M Rush, Sumit Chopra, and Jason Weston.
2015. A neural attention model for abstractive sen-
tence summarization. In Proceedings of EMNLP.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Proceedings of NIPS, pages 3104–3112.

Ruobing Xie, Xingchi Yuan, Zhiyuan Liu, and
Maosong Sun. 2017. Lexical sememe prediction via
word embeddings and matrix factorization. In Pro-
ceedings of IJCAI, pages 4200–4206. AAAI Press.

Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and
William W. Cohen. 2018. Breaking the softmax bot-
tleneck: A high-rank rnn language model. In Pro-
ceedings of ICLR.

Wojciech Zaremba, Ilya. Sutskever, and Oriol. Vinyals.
2014. Recurrent neural network regularization.
arXiv preprint arXiv:1409.2329.


