
























































Persuasion of the Undecided: Language vs. the Listener


Proceedings of the 6th Workshop on Argument Mining, pages 167–176
Florence, Italy, August 1, 2019. c©2019 Association for Computational Linguistics

167

Persuasion of the Undecided: Language vs. the Listener

Liane Longpré
Cornell University

lfl42@cornell.edu

Esin Durmus
Cornell University

ed459@cornell.edu

Claire Cardie
Cornell University

cardie@cs.cornell.edu

Abstract

This paper examines the factors that govern
persuasion for a priori UNDECIDED versus
DECIDED audience members in the context of
on-line debates. We separately study two types
of influences: linguistic factors — features of
the language of the debate itself; and audi-
ence factors — features of an audience mem-
ber encoding demographic information, prior
beliefs, and debate platform behavior. In a
study of users of a popular debate platform,
we find first that different combinations of lin-
guistic features are critical for predicting per-
suasion outcomes for UNDECIDED versus DE-
CIDED members of the audience. We addi-
tionally find that audience factors have more
influence on predicting the side (PRO/CON)
that persuaded UNDECIDED users than for DE-
CIDED users that flip their stance to the op-
posing side. Our results emphasize the im-
portance of considering the undecided and de-
cided audiences separately when studying lin-
guistic factors of persuasion.

1 Introduction

Understanding the factors that influence persua-
sion in the context of argumentation (e.g. debates)
has been an important focus in a variety of re-
search areas. Natural language processing (NLP)
research on persuasion has focused for the most
part on uncovering the linguistic factors that de-
termine and define persuasive arguments — fea-
tures of the language of the argument itself. For
example, Tan et al. (2016) and Zhang et al. (2016)
have found that the language used in arguments
and the patterns of interaction between debaters
are important predictors of persuasiveness. Re-
cently, however, studies have emerged that begin
to study the effects of audience characteristics on
persuasion, e.g. features that encode demographic
information, the prior beliefs, and debate platform

behavior of individual listeners of a debate or read-
ers of an argument. Lukin et al. (2017), for ex-
ample, find that different types of people are per-
suaded by different types of arguments. And Dur-
mus and Cardie (2018) show that the prior beliefs
of the audience have a significant impact on pre-
dicting whether or not a particular audience mem-
ber will be persuaded to flip their stance on a de-
bated topic.

Research in psychology and political science
moreover suggests that there are key differences
in the persuasion of undecided versus decided vot-
ers/audience members. For example, Petty and
Cacioppo (1996) find that prior experiences and
beliefs can lead to the re-framing of a message
perceived by a person to maintain consistency be-
tween their prior beliefs and their attitudes to-
wards the topic of the message. In particular,
studies show that a priori decided voters sim-
ply ignore certain information in order to main-
tain this consistency (Sweeney and Gruber, 1984;
Vecchione et al., 2013; Kosmidis, 2014). In con-
trast, an undecided voter is asked to make a deci-
sion on an issue for which previously received in-
formation was somehow unconvincing; and Kos-
midis (2014), Kosmidis and Xezonakis (2010),
and Schill and Kirk (2014) show that, as a result,
these voters are likely to rely heavily on informa-
tion conveyed in a new message.

The undecided voter group furthermore holds
the highest potential for persuasion (Kosmidis and
Xezonakis, 2010; Shehryar et al., 2017). Public
support for social and political causes often criti-
cally depends on the undecided decision makers.
To the best of our knowledge, computational stud-
ies of persuasion in NLP have not yet studied this
important subset of the audience separately.

This paper studies argumentation in the context
of online debate to better understand the factors
that govern persuasion for a priori UNDECIDED



168

versus DECIDED members of the audience. We
study persuasion at the individual (i.e. audience
member) level, and find that the linguistic fea-
tures most important for persuasion differ for the
UNDECIDED and DECIDED audience subgroups.
Consistent with results of social and political psy-
chology research, the linguistic feature differences
correspond to rhetorical styles found to be effec-
tive on undecided and decided audiences. Ad-
ditionally, we find that certain audience features
are more important for predicting undecided cases
of persuasion than for predicting decided cases of
persuasion.

The remainder of this paper is organized as fol-
lows. Related work is described in Section 2. We
describe the dataset in Section 3 and experiment
methodology in Section 4. Results and analysis is
in Section 5, and conclusions are in Section 6.

2 Related Work

Language and persuasion. Extensive work has
been done in cognitive and social psychology on
the linguistic influence on persuasion. Some of
the most critical elements of persuasive text in-
clude lexical complexity, language intensity, and
power of speech style (Dillard and Pfau, 2002).
Studies on linguistic factors effecting the persua-
sion of the listener have shown that language is
a key factor in predicting the outcome of debates
(Paxton and Dale, 2014; Jorgensen et al., 1998).
These studies find the importance of various lan-
guage features: lexical qualities such as personal
pronoun use, word sentiment, and hedging (Pax-
ton and Dale, 2014), and rhetoric qualities such
as precision, firmness, energy, and commitment
(Jorgensen et al., 1998). These works in psychol-
ogy highlight the importance of studying linguistic
features in arguments and persuasion.

Argument mining. Much recent work in argu-
mentation has focused on the automatic detection
of argument structures in text (Lippi and Torroni,
2016; Schulz et al., 2018; Stab et al., 2018; Morio
and Fujita, 2018). Research has shown promis-
ing results on using extracted argument structures
as features on tasks that involve predicting con-
vincingness (Ghosh et al., 2016; Yunfan Gu and
Huang, 2018; Cano-Basave and He, 2016).

Specific to debates, work has been done on de-
tecting the stance of the speaker. Walker et al.
(2012), for example, find that structuring the de-
bates in terms of agreement relations between

speakers improves prediction. Lexical and syntac-
tic argument features are shown to improve pre-
dictive performance in Somasundaran and Wiebe
(2010). More relevant to our work, recent studies
have examined the role of language in predicting
persuasion outcomes in debates. For example, Tan
et al. (2016) find that the linguistic interaction be-
tween an opinion holder and opposing debater are
highly predictive of persuasiveness. And Zhang
et al. (2016) find that debaters who target and ad-
dress their opponent’s points are more likely to
win the debate.

While these studies motivate the linguistic fea-
tures examined in our study, they do not take fac-
tors corresponding to audience characteristics into
consideration. Our work aims to study the linguis-
tic characteristics of persuasive text, while also
considering audience characteristics such as prior
beliefs and decidedness.

Prior views of the audience. Persuasion of an
audience is not solely dependent on the language
used by the speaker. Research in psychology em-
phasizes the significance of people’s prior views
on their perception of new information. The ef-
fectiveness of a message depends significantly on
the prior beliefs and the strengths of beliefs of the
message recipient (Johnson et al., 1995; Lau et al.,
1991).

Recent work has analyzed the influence of au-
dience characteristics on predicting persuasion
(Lukin et al., 2017; Durmus and Cardie, 2018).
Lukin et al. (2017) examine the effects of audience
factors and argumentation types in belief change.
They study dialogs from 4forums.com1, which
contain argument type annotations. Their results
show that information on prior beliefs and person-
ality type improves the ability of the model to pre-
dict belief change; more conscientious, open, and
agreeable people tend to respond more to emo-
tional argument types.

The importance of considering audience-
specific prior belief factors is further illustrated
in Durmus and Cardie (2018). Using debate and
user data from debate.org, they study the effects of
prior beliefs on various controversial issues along
with linguistic factors on predicting the outcome
of debates. Importantly, they find that the linguis-
tic features most important for prediction differ
when audience features are considered from when

1http://www.4forums.com/political/
forum.php/

http://www.4forums.com/political/forum.php/
http://www.4forums.com/political/forum.php/


169

they are not. To the best of our knowledge, this
work is most relevant to ours because it studies
debate text and considers prior beliefs of both the
audience and the debaters. Our work differs from
this study in that we separately consider persua-
sion of audience members who were undecided
before the debate from audience members who
switched sides.

The undecided audience. There has been a
substantial amount of research effort in the social
and political sciences on undecided and decided
voters. A study on the 2005 British general elec-
tion finds that undecided voters are more suscep-
tible to campaign persuasion (Kosmidis and Xe-
zonakis, 2010). This result, elaborated on in Kos-
midis (2014), is because decided voters rely more
on their prior beliefs while undecided voters place
higher weight on information conveyed in cam-
paigns.

Consistent with this account, studies by Schill
and Kirk (2014) on 2008 and 2012 U.S. presi-
dential debate outcomes find that the most criti-
cal portions of the debate to undecided voters were
the content-rich statements, and that the rhetorical
strategies shown to be effective to undecideds are
strategies that “transcended the personalities of the
candidates”. In contrast, studies by Adams et al.
(2011) on European election campaigns find that
in response to policy statements of political parties
during elections, voters adjust their Left-Right po-
sitions based on their subjective perceptions of the
party’s campaign and not on the campaign’s actual
policy statements. Research on selective exposure
(favoring information that aligns with an individ-
ual’s prior beliefs and attitudes) provides insight
into the mechanisms behind this tendency. Voters
already decided on an issue tend to avoid infor-
mation that is inconsistent with their attitudes and
are receptive to information consistent with their
attitudes (Sweeney and Gruber, 1984; Vecchione
et al., 2013).

3 Data Description

The debate dataset from Durmus and Cardie
(2018) consists of 67,315 debates and user infor-
mation on 36,294 users obtained from debate.org.

3.1 Debates

Debates span over 23 different categories (e.g.
‘Politics’, ‘Education’, ‘Movies’). Each debate
consists of multiple rounds, where a round con-

ROUND 1
PRO: ... this reason, you are not free to

make threats or defamatory state-
ments against another person in ...

CON: ... laws violate the fundamental
freedom of speech which democ-
racy is founded upon ...

ROUND 2
PRO: ... has ignored my point about hate

speech breeding an “us vs them”
mentality, and how such ...

CON: ... question is, does our govern-
ment have the right to tell us what
our opinions are, and to define ...

ROUND 3
PRO: ... evidenced by the rise in vio-

lence against Hispanics and Mus-
lims I cited in my second round ...

CON: ... courts to be able to decide
which opinions are “moral” and
which are not? How fascist do ...

Table 1: An example debate titled ‘HATE SPEECH
LAWS ARE A GOOD IDEA’.

tains text from the PRO debater and the CON
debater. An example debate is shown in Ta-
ble 1. Other examples of debate titles are: “THE
DEATH PENALTY IS A SUITABLE PUNISHMENT”
and “ANIMAL TESTING SHOULD BE BANNED”.

Users can interact with debates by voting on
them. Votes include “AGREE WITH BEFORE THE
DEBATE” and “AGREE WITH AFTER THE DE-
BATE” for each debater/side (users can respond
with PRO, CON, or TIE). We focus our analysis
on two distinct cases of persuasion based on this
vote data.

Case 1: voters persuaded from the middle.
This category constitutes voters who indicate TIE
between PRO and CON for “AGREE WITH BE-
FORE THE DEBATE” and indicate one side, PRO
or CON, for “AGREE WITH AFTER THE DEBATE”.
We keep instances of persuasion that correspond
to this category and refer to this case as FROM-
MIDDLE.

Case 2: voters persuaded from the opposite
side. This category constitutes voters who indicate
one side for “AGREE WITH BEFORE THE DEBATE”
and indicate the opposite side (PRO or CON) for
“AGREE WITH AFTER THE DEBATE”. We keep
instances that correspond to this category, referred



170

Persuasion Case #instances #debates
FROM-MIDDLE 4360 3652
FROM-OPPOSING 2642 2183

Table 2: Dataset statistics.

to as FROM-OPPOSING. In our prediction task, the
original side of the voter is not given to the model.

Figure 1 illustrates example user votes for each
of the two cases. Distinguishing instances of vot-
ers being persuaded into these case groupings al-
lows us to examine what makes an argument per-
suasive to audience members who are undecided
versus decided with respect to a particular debate
topic. Table 2 summarizes the dataset statistics rel-
evant to the voter cases.

3.2 User Information

User profiles contain self-identified demographic
information, such as GENDER and RELIGIOUS
IDEOLOGY. Profiles additionally contain users’
opinions on current controversial debate topics
(denoted by BIG-ISSUES), such as ABORTION,
SOCIAL SECURITY, and MINIMUM WAGE2. Users
can respond with PRO (in favor), CON (against),
UND (undecided), N/O (no opinion), or N/S (not
saying).

4 Prediction Task

We aim to study what factors are most important
in influencing audience members to be persuaded
to one side or the other for each of the cases (a
priori undecided or decided) of persuasion. En-
coding audience-level and linguistic factors as fea-
tures, we structure the prediction task as follows:

Given an individual voter, predict which
debater/side (PRO or CON) the voter
will be convinced by after the debate.

We consider only samples from the data where (1)
a voter was undecided before the debate and then
adopted a stance, i.e. voted for one of the debaters
as the winner; and (2) a voter was (seemingly) de-
cided beforehand and then flipped their stance. We
do not consider samples where (1) a voter declared
a “tie” between the debaters after the debate; and
(2) a voter was decided beforehand, and voted for
the debater with the stance that they agreed with
beforehand.

2https://www.debate.org/big-issues/

To study the effect of each of the debaters’ lin-
guistic and user-based features on persuasion, in
this setting, we specifically look at which side
(PRO vs. CON) did the convincing for a partic-
ular voter. We believe that restricting the samples
in the way described above allows us to best study
what influences persuasion when voters are suc-
cessfully convinced.

4.1 Features

Audience features. User profile data is used to
generate a number of features for a voter and the
PRO and CON debaters for a given debate.

The gender of a voter is one-hot encoded to ac-
count for the user’s option to not include gender
in their profile; the elements of the vector corre-
spond to FEMALE, MALE, and OTHER/DID NOT
INDICATE. Additionally, information about the
debaters’ genders are encoded as whether or not
the debater’s gender is the same as the voter’s.

User profile data is also used to capture the prior
opinion similarities of the voter and debaters in
two ways, as in Durmus and Cardie (2018). First,
the political and religious ideologies are encoded
as whether or not each of the debaters’ ideologies
is the same as each of the voter’s. We denote this
feature by matching ideology. Second, the sim-
ilarity of the voter and debaters’ BIG-ISSUES re-
sponses are encoded as follows. Each issue in
BIG-ISSUES is represented as a one-hot encoding
corresponding to PRO, CON, UND, and N/O.
The encoding of an example user can be seen in
Figure 2. All issue encodings are concatenated
to create a BIG-ISSUES vector for each user. The
cosine similarity between the voter’s BIG-ISSUES
vector and each debaters’ BIG-ISSUES vector is
used as a feature. We denote this feature by opin-
ion similarity.

The number of elements in the voter’s BIG-
ISSUES vector corresponding to PRO and CON,
and the number of elements in the vector corre-
sponding to UND and N/O are used to encode
the voter’s decidedness or undecidedness, respec-
tively. We denote the feature by decidedness.
An example of the encoding is shown in Fig-
ure 2. This feature captures the degree to which
the voter’s opinions are established on widely dis-
cussed topics.

The frequency of a voter being persuaded is en-
coded as the percentage of other training debates
in which the voter changed their stance, out of all

https://www.debate.org/big-issues/


171

Figure 1: Example votes for a debate showing each case of persuasion.

Figure 2: Example user profile and corresponding feature encodings.

training debates on which the voter made a vote.
We denote the feature by persuadability. This fea-
ture is an indication of how persuadable a voter is,
in general.

Linguistic features. We process debate text
and use linguistic features as is done in Durmus
and Cardie (2018). The text from all rounds of
PRO are concatenated before feature processing.
The same is done for the rounds of CON. We use
the same set of linguistic features from Durmus
and Cardie (2018), described as follows.

Lexical features include TF-IDF, modal verbs,
swear words, spelling errors, and punctuation. A
speaker’s word choice (i.e. use of hedging, and
particular causal connectors and modal particles)
are indicative of the mode of argumentation (Gold
et al., 2015; Paxton and Dale, 2014).

Style features include length, personal pro-
nouns, referring to opponent, use of citations, and
links. Using citations and addressing an oppo-
nent’s points are critical components of justifica-
tion that affect the reception of an argument. Addi-
tionally, the length of a speaker’s utterance and the
language used when referring to self and the oppo-

nent exhibit characteristics of respect and partici-
pation between the debaters, which are important
aspects for communication outcomes (Tan et al.,
2016; Gold et al., 2015; Paxton and Dale, 2014).

Semantic features include sentiment, subjectiv-
ity (Wilson et al., 2005), connotation (Feng and
Hirst, 2011), and politeness. The sentiment and
subjectivity of an argument impacts the reception
of the message, and are predictive of argument
stance (Somasundaran and Wiebe, 2010). In ad-
dition to these attributes, connotation and polite-
ness cues contribute to the patterns of interaction
of debaters, which are critical in predicting per-
suasiveness (Tan et al., 2016).

Argumentation features, as in (Somasundaran
et al., 2007), have been shown to predict the stance
and opinion of a speaker. These include the fol-
lowing: assessment, authority, conditioning, con-
trasting, emphasizing, generalizing, empathy, in-
consistency, necessity, possibility, priority, rhetor-
ical questions, desire, and difficulty.



172

4.2 Hypotheses

We hypothesize that there are key differences in
the linguistic features important for persuasion of
an a priori undecided audience member and the
persuasion of an a priori seemingly decided audi-
ence member to change their mind. Drawing from
social and political science studies, we hypoth-
esize that the persuasion of undecided audience
members will critically depend on content-centric
language features, while the persuasion of seem-
ingly decided audience members will be more in-
fluenced by stylistic language features. Addition-
ally, we hypothesize that audience features will
provide important context, improving predictive
performance.

4.3 Methodology

We use Logistic Regression to perform the classi-
fication task. Prediction accuracy is evaluated us-
ing 5-fold cross validation. We use 3-fold cross
validation on the training set to select model pa-
rameters. We perform ablation analysis first on
audience features only and linguistic features only,
then on combinations of the best-performing au-
dience and linguistic features. This analysis is
done separately for the subsets of data correspond-
ing to undecided and decided cases of persua-
sion (FROM-MIDDLE and FROM-OPPOSING, re-
spectively). We use majority classifier as a base-
line.

5 Results and Analysis

Results for models and feature ablation experi-
ments are show in Table 3. Majority baseline pro-
duces 57.43% and 59.42% accuracy for FROM-
MIDDLE and FROM-OPPOSING, respectively. This
baseline predicts the majority debater/side be-
tween PRO and CON in the training set of ex-
amples.

Linguistic vs. audience features. As shown
in Table 3, the best performance is achieved
when both audience and linguistic features are in-
cluded, obtaining 69.01% and 67.22% accuracy
for FROM-MIDDLE and FROM-OPPOSING, respec-
tively. We find that linguistic features are more
important for predictive accuracy than audience
features. Relying only on audience features ob-
tains accuracies of 61.47% for FROM-MIDDLE and
61.54% for FROM-OPPOSING. Using all linguistic
features produces a significant improvement over
baseline accuracy, achieving 66.95% and 66.65%

Accuracy of Models
FROM-
MIDDLE

FROM-
OPPOSING

Majority Baseline 57.43% 59.42%
All Features 69.01% 67.22%
Audience Features 61.47% 61.54%
- persuadability 61.46% 61.51%
- gender 61.44% 61.47%
- matching ideology 61.42% 61.39%
- decidedness 61.33% 61.13%
- opinion similarity 59.04% 59.80%
Linguistic Features 66.95% 66.65%
- unigram TF-IDF 65.25% 64.54%
- use of citations and
referring to opponent 67.20% 66.12%
- subjectivity 66.03% 67.79%

Table 3: Accuracy results, for majority class base-
line, all features, audience features, and linguis-
tic features. Remaining results are ablation studies,
where ‘- feature’ denotes the removal of the feature.
Underlined results are feature combinations that im-
prove performance over including all features.

for FROM-MIDDLE and FROM-OPPOSING, respec-
tively. This result is surprising and in contrast
to results from Durmus and Cardie (2018), who
find that audience features improve accuracy more
than linguistic features. We suspect that this dif-
ference arises because our experiments consider
debates from all categories, while Durmus and
Cardie (2018) restrict analysis to political and reli-
gious debate categories. Political and religious de-
bate topics tend to be more controversial in nature
Fichman and Hara (2014), and correspond more
closely to the issues encoded in the audience fea-
tures; the BIG-ISSUES elements consist primarily
of political and religious issues 3. As such, these
features will be more informative in political and
religious debate settings.

Audience features. Feature ablation across
user-based features shows that all audience fea-
tures are helpful in predicting vote outcomes for
both voter groups. We find that the most important
feature is opinion similarity4; removing this fea-
ture decreases prediction accuracy from 61.47%
to 59.04% for FROM-MIDDLE, and from 61.54%
to 59.80% for FROM-OPPOSING. This result is

3https://www.debate.org/big-issues/
4For UserA and UserB, the cosine similarity of

BIG-ISSUESA and BIG-ISSUESB .

https://www.debate.org/big-issues/


173

consistent with research on voter behavior from
Arcuri et al. (2008) and Friese et al. (2012), who
find that despite reporting uncertainty, undecided
voters have implicit attitudes that are predictive of
voting behavior.

Linguistic features. The most important lin-
guistic feature for both voter groups is uni-
gram TF-IDF5, whose removal decreases perfor-
mance to from 66.95% to 65.25% for FROM-
MIDDLE, and from 66.65% to 64.54% for FROM-
OPPOSING. However, not all linguistic features
are helpful in predictive accuracy. For instance,
removing use of citations6 and referring to oppo-
nent7 features increases accuracy from 66.95% to
67.20% for FROM-MIDDLE. Similarly, removal
of the subjectivity8 feature improves accuracy for
FROM-OPPOSING from 66.65% to 67.79%.

It should be noted that the linguistic fea-
tures whose removal improves performance for
FROM-MIDDLE and FROM-OPPOSING are differ-
ent, showing that there are distinctions in the im-
portant factors for persuasion between the voter
groups. These differences are further explored in
the following sections.

5.1 Differences Between Persuasion Groups

5.1.1 Linguistic Feature Differences
We find distinct differences in the important fea-
tures for predicting the vote outcome for voter
groups FROM-MIDDLE and FROM-OPPOSING. Ta-
ble 4 shows that the best-performing set of linguis-
tic features for FROM-MIDDLE includes all fea-
tures minus use of citations, referring to opponent,
and swear words, while the best-performing set of
linguistic features for FROM-OPPOSING includes
all features minus subjectivity, modals9, and bi-
/tri-gram TF-IDF10. These linguistic feature sets
are denoted by MIDDLE* and OPPOSING*, re-
spectively. Using features OPPOSING* increases
accuracy for FROM-OPPOSING from 67.22% to
68.39%, while decreasing accuracy for FROM-
MIDDLE from 69.01% to 68.51%. Conversely,
using features MIDDLE* increases accuracy for
FROM-MIDDLE from 69.01% to 69.17%, while
decreasing accuracy from 67.22% to 66.92%.

5Calculated with a maximum of 50 terms.
6The number of explicit source citations.
7The usage of phrases like “according to my opponent”.
8Number of words with negative strong, negative weak,

positive strong, and positive weak subjectivity.
9The usage of modal verbs, i.e. can, should, will, and may.

10Calculated with a maximum of 30 terms.

Accuracy of Models
FROM-
MIDDLE

FROM-
OPPOSING

All Features 69.01% 67.22%
- persuadability 68.33% 67.52%
- matching ideology 68.99% 67.30%
User+MIDDLE* 69.17% 66.92%
- persuadability 69.16% 66.84%
- matching ideology 68.60% 66.92%
User+OPPOSING* 68.51% 68.21%
- persuadability 68.46% 68.32%
- matching ideology 67.96% 68.39%

Table 4: Accuracy results, for all features and best-
performing linguistic feature sets. Remaining results
are ablation studies, where ‘- feature’ denotes the re-
moval of the feature. Underlined results are feature
combinations that improve performance over includ-
ing all features. MIDDLE* denotes the best-performing
combination of linguistic features for FROM-MIDDLE,
which includes all linguistic features minus use of cita-
tions, referring to opponent, and swear words. OPPOS-
ING* denotes the best-performing combination of lin-
guistic features for FROM-OPPOSING, which includes
all linguistic features minus subjectivity, modals, and
bi-/tri-gram TF-IDF.

The linguistic feature differences of the two
groups have subtle differences in nature. A pos-
sible analysis that distinguishes the groups is that
there is a difference in the rhetorical strategies
most effective for undecided versus decided audi-
ences. Use of modals, subjectivity, and general
word choice are semantic features of an argument
that affect the perception of the content of the ar-
gument. Based on our results, these content-based
features are more important for undecided voters
than they are for decided voters. In comparison,
use of swear words, citing sources, and referring to
the opponent are stylistic features of an argument
that affect the perception of the debater producing
the argument. Based on our results, these style-
based features are not as important for undecided
voters as they are for decided voters. This ac-
count is consistent with the findings of Schill and
Kirk (2014) that undecided voters respond most to
content-rich rhetorical strategies, and the findings
of Vecchione et al. (2013); Sweeney and Gruber
(1984) that decided voters tend to selectively at-
tend to information in a message based on prior
attitudes. The account is also in line with exper-
iments conducted by Adams et al. (2011), which



174

find that affiliated voters do not adjust their posi-
tions in response to a party’s actual policy state-
ments, but rather do adjust their positions based
on their subjective perceptions of the party.

5.1.2 Audience Feature Differences
The inclusion of certain audience features has
different effects on prediction accuracy be-
tween FROM-MIDDLE and FROM-OPPOSING voter
groups. As shown in Table 4, removing the
persuadability feature improves the accuracy for
FROM-OPPOSING from 67.22% to 67.52% when
all linguistic features are included, and improves
the accuracy from 68.21% to 68.32% when OP-
POSING* linguistic features are used. Similarly,
removing the matching ideology feature improves
the accuracy for FROM-OPPOSING from 67.22%
to 67.30% when all linguistic features are in-
cluded, and improves the accuracy from 68.21%
to 68.39% when OPPOSING* linguistic features
are used. The reverse is true for FROM-MIDDLE.
For this voter group, removing the persuadabil-
ity and matching ideology features decreases ac-
curacy from 69.01% to 68.33% and 68.99%, re-
spectively, when all lingusitic features are in-
cluded, and decreases the accuracy from 69.17%
to 69.16% and 68.60%, respectively, when MID-
DLE* features are included.

It should be noted that the best-performing over-
all feature set for FROM-OPPOSING includes nei-
ther the persuadability feature nor the matching
ideology feature. In contrast, all audience features
are present in the best-performing overall feature
set for FROM-MIDDLE. This difference suggests
that certain audience-level aspects are compara-
tively more predictive of vote outcomes for un-
decided voters. The result emphasizes the impor-
tance of considering audience factors for people
who are undecided with respect to an issue; in or-
der to understand vote behavior of the undecided
audience, it is critical to consider audience factors.

5.2 Influence of Audience Features

We perform ablation across linguistic features sep-
arately for when audience features are included
and for when they are not. Results in Table 5 show
that the linguistic features most important for
model performance differ when audience features
are present. For instance, experiments on voter
group FROM-OPPOSING show that including argu-
ment lexicon features improves performance from
67.22% to 67.52% when audience features are not

Accuracy of Models
FROM-
MIDDLE

FROM-
OPPOSING

Linguistic Features 66.95% 66.65%
- argument lexicon 66.22% 65.90%
- use of citations and
referring to opponent 67.20% 66.12%
- swear words 66.65% 66.65%
- subjectivity 66.03% 67.79%
All Features 69.01% 67.22%
- argument lexicon 68.46% 67.52%
- use of citations and
referring to opponent 69.17% 66.99%
- swear words 69.08% 67.20%
- subjectivity 68.76% 67.90%

Table 5: Accuracy results, for all features and lin-
guistic features. Remaining results are ablation stud-
ies, where ‘- feature’ denotes the removal of the fea-
ture. Underlined results are feature combinations that
improve performance over including all features.

included, while performance is decreased from
66.65% to 65.90% when audience features are in-
cluded. Comparatively, inclusion of the swear
words feature improves performance for FROM-
MIDDLE from 69.01% to 69.08% when audience
features are not included, but negatively impacts
performance from 66.95% to 66.65% when audi-
ence features are included.

We find that the best-performing sets of lin-
guistic features for FROM-OPPOSING and FROM-
MIDDLE differ when audience features are in-
cluded versus when they are not. The best-
performing set of linguistic features for FROM-
OPPOSING when audience features are not con-
sidered includes modals and bi-/tri-gram TF-IDF,
while these features are not present in the best-
performing set of features when all features are
considered (denoted by OPPOSING*). Similarly
for FROM-MIDDLE, the swear words feature is
not in MIDDLE*, while it is present in the best-
performing set of linguistic features when audi-
ence features are not considered.

These results are consistent with findings from
Durmus and Cardie (2018) and re-affirm the im-
portance of considering audience features when
analyzing linguistic effects of persuasion.



175

6 Conclusion

In this paper, we separately examine what linguis-
tic and audience-level factors are most important
for predicting vote outcomes of previously unde-
cided and decided audiences. We show that differ-
ent linguistic features are critical for predicting the
successful side of persuasion of undecided versus
decided voters. We find that some audience fea-
tures that are important for predicting the side of
persuasion of undecided voters are not as helpful
in predicting persuasion of decided voters.

This paper examines the differences between
the undecided and decided audiences in persua-
sion, which has been under-studied in a computa-
tional framework. The results of our work validate
the importance of analyzing the undecided versus
decided audience separately.

Acknowledgments

This work was supported in part by NSF grants
IIS-1815455 and SES-1741441. The views and
conclusions contained herein are those of the au-
thors and should not be interpreted as necessarily
representing the official policies or endorsements,
either expressed or implied, of NSF or the U.S.
Government.

References
James Adams, Lawrence Ezrow, and Zeynep Somer-

Topcu. 2011. Is anybody listening? Evidence that
voters do not respond to European parties’ policy
statements during elections. American Journal of
Political Science, 55(2):370–382.

Luciano Arcuri, Luigi Castelli, Silvia Galdi, Cristina
Zogmaister, and Alessandro Amadori. 2008. Pre-
dicting the vote: Implicit attitudes as predictors of
the future behavior of decided and undecided voters.
Political Psychology, 29(3):369–387.

Amparo Elizabeth Cano-Basave and Yulan He. 2016.
A study of the impact of persuasive argumentation
in political debates. In Proceedings of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies
2016, pages 1405–1413. Association for Computa-
tional Linguistics.

James P. Dillard and Michael Pfau. 2002. The Persua-
sion Handbook: Developments in Theory and Prac-
tice, pages 371–380. Sage Publications, Inc., Thou-
sand Oaks, CA.

Esin Durmus and Claire Cardie. 2018. Exploring the
role of prior beliefs for argument persuasion. In Pro-
ceedings of the North American Chapter of the Asso-

ciation for Computational Linguistics: Human Lan-
guage Technologies 2018, pages 1035–1045. Asso-
ciation for Computational Linguistics.

Vanessa Wei Feng and Graeme Hirst. 2011. Clas-
sifying arguments by scheme. In Proceedings
of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language
Technologies-Volume 1, pages 987–996. Association
for Computational Linguistics.

Pnina Fichman and Noriko Hara. 2014. Global
Wikipedia: International and Cross-Cultural Issues
in Online Collaboration, pages 25–41. Rowman &
Littlefield Publishers, Lanham, Maryland.

Malte Friese, Colin Tucker Smith, Thomas Plischke,
Matthias Bluemke, and Brian A. Nosek. 2012. Do
implicit attitudes predict actual voting behavior par-
ticularly for undecided voters? Public Library of
Science One, 7:1–14.

Debanjan Ghosh, Aquila Khanam, Yubo Han, and
Smaranda Muresan. 2016. Coarse-grained argu-
mentation features for scoring persuasive essays. In
Proceedings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics, pages 549–
554. Association for Computational Linguistics.

Valentin Gold, Mennatallah El-Assady, Annette Hautli-
Janisz, Tina Bgel, Christian Rohrdantz, Miriam
Butt, Katharina Holzinger, and Daniel Keim. 2015.
Visual linguistic analysis of political discussions:
Measuring deliberative quality. Digital Scholarship
in the Humanities, 32(1):141–158.

Blair T. Johnson, Hung-Yu Lin, Cynthia S. Symons,
Laura Ann Campbell, and Geoffrey Ekstein. 1995.
Initial beliefs and attitudinal latitudes as factors in
persuasion. Personality and Social Psychology Bul-
letin, 21(5):502–511.

Charlotte Jorgensen, Christian Kock, and Lone Ror-
bech. 1998. Rhetoric that shifts votes: An ex-
ploratory study of persuasion in issue-oriented pub-
lic debates. Political Communication, 15(3):283–
299.

Spyros Kosmidis. 2014. Heterogeneity and the calcu-
lus of turnout: Undecided respondents and the cam-
paign dynamics of civic duty. Electoral Studies,
33:123 – 136.

Spyros Kosmidis and Georgios Xezonakis. 2010. The
undecided voters and the economy: Campaign het-
erogeneity in the 2005 British general election.
Electoral Studies, 29(4):604 – 616.

Richard R. Lau, Richard A. Smith, and Susan T.
Fiske. 1991. Political beliefs, policy interpretations,
and political persuasion. The Journal of Politics,
53(3):644–675.

Marco Lippi and Paolo Torroni. 2016. Argumentation
mining: State of the art and emerging trends. ACM
Transactions on Internet Technology, 16(2):10:1–
10:25.

https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1540-5907.2010.00489.x
https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1540-5907.2010.00489.x
https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1540-5907.2010.00489.x
https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9221.2008.00635.x
https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9221.2008.00635.x
https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9221.2008.00635.x
https://www.aclweb.org/anthology/N16-1166
https://www.aclweb.org/anthology/N16-1166
https://aclweb.org/anthology/N18-1094
https://aclweb.org/anthology/N18-1094
https://www.aclweb.org/anthology/P11-1099
https://www.aclweb.org/anthology/P11-1099
https://books.google.com/books?id=L0HcDQAAQBAJ
https://books.google.com/books?id=L0HcDQAAQBAJ
https://books.google.com/books?id=L0HcDQAAQBAJ
https://doi.org/10.1371/journal.pone.0044130
https://doi.org/10.1371/journal.pone.0044130
https://doi.org/10.1371/journal.pone.0044130
https://www.aclweb.org/anthology/P16-2089
https://www.aclweb.org/anthology/P16-2089
https://doi.org/10.1093/llc/fqv033
https://doi.org/10.1093/llc/fqv033
https://doi.org/10.1177/0146167295215008
https://doi.org/10.1177/0146167295215008
https://doi.org/10.1080/105846098198902
https://doi.org/10.1080/105846098198902
https://doi.org/10.1080/105846098198902
https://doi.org/10.1016/j.electstud.2013.08.008
https://doi.org/10.1016/j.electstud.2013.08.008
https://doi.org/10.1016/j.electstud.2013.08.008
https://doi.org/10.1016/j.electstud.2010.04.015
https://doi.org/10.1016/j.electstud.2010.04.015
https://doi.org/10.1016/j.electstud.2010.04.015
https://doi.org/10.2307/2131574
https://doi.org/10.2307/2131574
https://dl.acm.org/citation.cfm?doid=2909066.2850417
https://dl.acm.org/citation.cfm?doid=2909066.2850417


176

Stephanie Lukin, Pranav Anand, Marilyn Walker, and
Steve Whittaker. 2017. Argument strength is in the
eye of the beholder: Audience effects in persuasion.
In Proceedings of the 15th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics: Volume 1, Long Papers, pages 742–753,
Valencia, Spain. Association for Computational Lin-
guistics.

Gaku Morio and Katsuhide Fujita. 2018. End-to-end
argument mining for discussion threads based on
parallel constrained pointer architecture. In Pro-
ceedings of the 5th Workshop on Argument Min-
ing, pages 11–21, Brussels, Belgium. Association
for Computational Linguistics.

Alexandra Paxton and Rick Dale. 2014. Leveraging
linguistic content and debater traits to predict debate
outcomes. In Proceedings of the Annual Meeting of
the Cognitive Science Society, pages 592–596. Cog-
nitive Science Society.

Richard E. Petty and John T. Cacioppo. 1996. Atti-
tudes and Persuasion: Classic and Contemporary
Approaches, pages 95–160. Westview Press, New
York, NY.

Dan Schill and Rita Kirk. 2014. Courting the swing
voter: “Real time” insights into the 2008 and 2012
U.S. presidential debates. American Behavioral Sci-
entist, 58(4):536–555.

Claudia Schulz, Steffen Eger, Johannes Daxenberger,
Tobias Kahse, and Iryna Gurevych. 2018. Multi-
task learning for argumentation mining in low-
resource settings. In Proceedings of the 2018 Con-
ference of the North American Chapter of the As-
sociation for Computational Linguistics: Human
Language Technologies, Volume 2 (Short Papers),
pages 35–41, New Orleans, Louisiana. Association
for Computational Linguistics.

Omar Shehryar, Kelly Weidner, and Dan Moshavi.
2017. Persuading the undecided: An interdisci-
plinary approach to increase public support for the
arts. Journal of Public Affairs, 18(2):e1652.

Swapna Somasundaran, Josef Ruppenhofer, and Janyce
Wiebe. 2007. Detecting arguing and sentiment in
meetings. In Proceedings of the SIGdial Workshop
on Discourse and Dialogue, volume 6.

Swapna Somasundaran and Janyce Wiebe. 2010. Rec-
ognizing stances in ideological on-line debates. In
Proceedings of the the North American Chapter
of the Association for Computational Linguistics:
Human Language Technologies 2010 Workshop on
Computational Approaches to Analysis and Genera-
tion of Emotion in Text, pages 116–124. Association
for Computational Linguistics.

Christian Stab, Tristan Miller, Benjamin Schiller,
Pranav Rai, and Iryna Gurevych. 2018. Cross-
topic argument mining from heterogeneous sources.
In Proceedings of the 2018 Conference on Em-
pirical Methods in Natural Language Processing,

pages 3664–3674, Brussels, Belgium. Association
for Computational Linguistics.

Paul D. Sweeney and Kathy L. Gruber. 1984. Selec-
tive exposure: Voter information preferences and the
Watergate affair. Journal of Personality and Social
Psychology, 46(6):1208–1221.

Chenhao Tan, Vlad Niculae, Cristian
DanescuNiculescu-Mizil, and Lillian Lee. 2016.
Winning arguments: Interaction dynamics and per-
suasion strategies in good-faith online discussions.
In Proceedings of the 25th International Conference
on World Wide Web, pages 613–624. International
Conference on World Wide Web.

Michele Vecchione, Gianvittorio Caprara, Francesco
Dentale, and Shalom H. Schwartz. 2013. Voting and
values: Reciprocal effects over time. Political Psy-
chology, 34(4):465–485.

Marilyn A. Walker, Pranav Anand, Robert Abbott, and
Ricky Grant. 2012. Stance classification using dia-
logic properties of persuasion. In 2012 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, pages 592–596. Association for Com-
putational Linguistics.

Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the Con-
ference on Human Language Technology and Em-
pirical Methods in Natural Language Processing,
pages 347–354. Association for Computational Lin-
guistics.

Maoran Xu Hao Fu Yang Liu Yunfan Gu, Zhongyu Wei
and Xuanjing Huang. 2018. Incorporating topic as-
pects for online comment convincingness evalua-
tion. In Proceedings of the 5th Workshop on Argu-
ment Mining, pages 97–104. Association for Com-
putational Linguistics.

Justine Zhang, Ravi Kumar, Sujith Ravi, and Cris-
tian Danescu-Niculescu-Mizil. 2016. Conversa-
tional flow in Oxford-style debates. In Proceedings
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies 2016, pages 136–141, San Diego, Cal-
ifornia. Association for Computational Linguistics.

https://www.aclweb.org/anthology/E17-1070
https://www.aclweb.org/anthology/E17-1070
https://aclweb.org/anthology/W18-5202
https://aclweb.org/anthology/W18-5202
https://aclweb.org/anthology/W18-5202
https://escholarship.org/uc/item/79d1s576
https://escholarship.org/uc/item/79d1s576
https://escholarship.org/uc/item/79d1s576
https://doi.org/10.1177/0002764213506204
https://doi.org/10.1177/0002764213506204
https://doi.org/10.1177/0002764213506204
https://www.aclweb.org/anthology/N18-2006
https://www.aclweb.org/anthology/N18-2006
https://www.aclweb.org/anthology/N18-2006
https://doi.org/10.1002/pa.1652
https://doi.org/10.1002/pa.1652
https://doi.org/10.1002/pa.1652
https://www.aclweb.org/anthology/W10-0214
https://www.aclweb.org/anthology/W10-0214
https://www.aclweb.org/anthology/D18-1402
https://www.aclweb.org/anthology/D18-1402
http://dx.doi.org/10.1037/0022-3514.46.6.1208
http://dx.doi.org/10.1037/0022-3514.46.6.1208
http://dx.doi.org/10.1037/0022-3514.46.6.1208
http://arxiv.org/abs/1602.01103
http://arxiv.org/abs/1602.01103
https://doi.org/10.1111/pops.12011
https://doi.org/10.1111/pops.12011
https://aclweb.org/anthology/papers/N/N12/N12-1072/
https://aclweb.org/anthology/papers/N/N12/N12-1072/
https://www.aclweb.org/anthology/H05-1044
https://www.aclweb.org/anthology/H05-1044
https://aclweb.org/anthology/W18-5212
https://aclweb.org/anthology/W18-5212
https://aclweb.org/anthology/W18-5212
https://www.aclweb.org/anthology/N16-1017
https://www.aclweb.org/anthology/N16-1017

