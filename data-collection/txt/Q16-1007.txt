








































Learning Tier-based Strictly 2-Local Languages

Adam Jardine and Jeffrey Heinz
University of Delaware

{ajardine,heinz}@udel.edu

Abstract

The Tier-based Strictly 2-Local (TSL2) lan-
guages are a class of formal languages which
have been shown to model long-distance
phonotactic generalizations in natural lan-
guage (Heinz et al., 2011). This paper in-
troduces the Tier-based Strictly 2-Local In-
ference Algorithm (2TSLIA), the first non-
enumerative learner for the TSL2 languages.
We prove the 2TSLIA is guaranteed to con-
verge in polynomial time on a data sample
whose size is bounded by a constant.

1 Introduction

This work presents the Tier-based Strictly 2-Local
Inference Algorithm (2TSLIA), an efficient learn-
ing algorithm for a class of Tier-based Strictly Lo-
cal (TSL) formal languages (Heinz et al., 2011). A
TSL class is determined by two parameters: the tier,
or subset of the alphabet, and the permissible tier
k-factors, which are the legal sequences of length
k allowed in the string, once all non-tier symbols
have been removed. The Tier-based Strictly 2-Local
(TSL2) languages are those in which k = 2.

As will be discussed below, the TSL languages
are of interest to phonology because they can model
a wide variety of long-distance phonotactic patterns
found in natural language (Heinz et al., 2011; Mc-
Mullin and Hansson, forthcoming). One example
is derived from Latin liquid dissimilation, in which
two ls cannot appear in a word unless there is an
r intervening, regardless of distance. For exam-
ple, floralis ‘floral’ is well-formed but not *mili-
talis (cf. militaris ‘military’). As explained in sec-

tions 2 and 4, this can be modeled with permissible
2-factors over a tier consisting of the liquids {l, r}.

For long-distance phonotactics, k can be fixed to
2, but it is does not appear that the tier can be fixed
since languages employ a variety of different tiers.
This presents an interesting learning problem: Given
a fixed k, how can an algorithm induce both a tier
and a set of permissible tier k-factors from positive
data?

There is some related work which addresses this
question. Goldsmith and Riggle (2012), building
on work by Goldsmith and Xanthos (2009), present
a method based on mutual information for learn-
ing tiers and subsequently learning harmony pat-
terns. This paper differs in that its methods are
rooted firmly in grammatical inference and formal
language theory (de la Higuera, 2010). For instance,
in contrast to the results presented there, we prove
the kinds of patterns 2TSLIA succeeds on and the
kind of data sufficient for it to do so.

Nonetheless, there is relevant work in computa-
tional learning theory: Gold (1967) proved that any
finite class of languages is identifiable in the limit
via an enumeration method. Given a fixed alphabet
and a fixed k, the number of possible tiers and per-
missible tier k-factors is finite, and thus learnable in
this way. However, such learners are grossly inef-
ficient. No provably-correct, non-enumerative, ef-
ficient learner for both the tier and permissible tier
k-factor parameters has previously been proposed.
This work fills this gap with an algorithm which
learns these parameters when k = 2 from positive
data in time polynomial in the size of the data.

Finally, Jardine (2016) presents a simplified ver-

87

Transactions of the Association for Computational Linguistics, vol. 4, pp. 87–98, 2016. Action Editor: Alexander Clark.
Submission batch: 7/2015; Revision batch: 11/2015; Published 4/2016.

c©2016 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license.



sion of 2TSLIA and reports the results of some sim-
ulations. Unlike that paper, the present work pro-
vides the full mathematical details and proofs. The
simulations are discussed in the discussion section.

This paper is structured as follows. §2 moti-
vates the work with examples from natural language
phonology. §3 outlines the basic concepts and def-
initions to be used throughout the paper. §4 defines
the TSL languages and discusses their properties. §5
details the 2TSLIA and proves it learns the TSL2
class in polynomial time and data. §6 discusses fu-
ture work, and §7 concludes.

2 Linguistic motivation

The primary motivation for studying the TSL lan-
guages and their learnability comes from their rel-
evance to phonotactic (word well-formedness) pat-
terns in natural language phonology. Many phono-
tactic patterns belong to either the Strictly Local
(SL) languages (McNaughton and Papert, 1971) or
the Strictly Piecewise (SP) languages (Rogers et
al., 2010).1 An example of phonotactic knowledge
which is SL is Chomsky and Halle’s (Chomsky and
Halle, 1965) observation that English speakers will
classify blick as a possible word of English while
rejecting bnick as impossible. This is SL because
it can be described as *bn being unacceptable as a
word-initial sequence. SP languages can describe
long-distance dependencies based on precedence re-
lationships between sounds (such as consonant har-
mony in Sarcee, in which s may not follow a S any-
where in a word, but may precede one (Cook, 1984))
(Heinz, 2010a). Also, the SL and SP languages
are efficiently learnable (Garcı́a et al., 1990; Heinz,
2010a; Heinz, 2010b; Heinz and Rogers, 2013).

However, there are some long-distance patterns
which cannot be described purely with precedence
relationships. One example is a pattern from Latin,
in which in certain cases an l cannot follow another
l unless an r intervenes, no matter the distance be-
tween them (Jensen, 1974; Odden, 1994; Heinz,
2010a). This can be seen in the -alis adjectival suf-
fix (Example 1), which appears as -aris if the word
it attaches to already contains an l ((d) through (f)

1The relationship between these formal language classes and
human cognition (linguistic and otherwise) is discussed in more
detail in Rogers and Pullum (2011), Rogers and Hauser (2010),
Rogers et al. (2013), and Lai (2013; 2015).

in Example 1), except in cases where there is an in-
tervening r, in which it appears again as -alis ((g)
through (i) in Example 1). In the examples, the
sounds in question are underlined for emphasis (data
from Heinz (2010a)), and for (d) through (f), illicit
forms are given, marked with a star (*).

Example 1.
a. navalis ‘naval’
b. episcopalis ‘episcopal’
c. infinitalis ‘negative’
d. solaris ‘solar’ (*solalis)
e. lunaris ‘lunar’ (*lunalis)
f. militaris ‘military’ (*militalis)
g. floralis ‘floral’
h. sepulkralis ‘funereal’
i. litoralis ‘of the shore’

This non-local alternating pattern is not SP be-
cause SP languages cannot express blocking effects
(Heinz, 2010a). However, it can be described with
a TSL grammar in which the tier is {r, l} and the
permissible tier 2-factors do not include *ll and *rr.
This yields exactly the set of strings in which an l is
always immediately (disregarding all sounds besides
{r, l}) followed by an r, and vice versa.

Formal learning algorithms for SL and SP lan-
guages can provide a model for human learning of
SL and SP sound patterns (Heinz, 2010a). TSL lan-
guages are also similarly learnable, given the stip-
ulation that both the tier and k are fixed. For nat-
ural language, the value for k never seems to go
above 2 (Heinz et al., 2011). However, tiers vary
in human language—TSL patterns occur both with
different kinds of vowels (Nevins, 2010) and con-
sonants (Suzuki, 1998; Bennett, 2013). For exam-
ple, in Turkish the tier is the entire vowel inven-
tory (Clements and Sezer, 1982), while in Finnish
it is vowels except /i,e/ (Ringen, 1975). In Samala
consonant harmony, the tier is sibilants (Rose and
Walker, 2004), whereas in Koorete, the tier is sibi-
lants and {b,r,g,d} (McMullin and Hansson, forth-
coming). Thus, it is of interest to understand how
both the tier and permissible tier 2-factors for TSL2
grammars might be learned efficiently.

3 Preliminaries

Basic knowledge of set theory is assumed. Let S1 −
S2 denote the set theoretic difference of sets S1 and

88



S2. Let P(S) denote the powerset of S and Pfin(S)
be the set of all finite subsets of S.

Let Σ denote a set of symbols, referred to as the
alphabet, and let a string over Σ be a finite se-
quence of symbols from that alphabet. The length
of a string w will be denoted |w|. Let λ denote the
empty string; |λ| = 0. Let Σ∗ (Kleene star) repre-
sent all strings over this alphabet, and Σk represent
all strings of length k. Concatenation of two strings
w and v (or symbols σ1 and σ2) will be written wv
(σ1σ2). Special beginning (o) and end (n) symbols
(o,n 6∈ Σ) will often be used to mark the begin-
ning and end of words; the alphabet Σ augmented
with these, Σ ∪ {o,n}, will be denoted Σon.

A string u ∈ Σ∗ is said to be a factor or sub-
string of another string w if there are two other
strings v, v′ ∈ Σ∗ such that w = vuv′. We call u
a k-factor of w if it is a factor of w and |u| = k.
Let fack : Σ∗ → P(Σ≤k) be a function mapping
strings to their k-factors, where fack(w) equals
{u|u is a k-factor of w} if |w| > k and equals {w}
otherwise. For example, fac2(aab) = {aa, ab} and
fac8(aab) = {aab}. We extend the k-factor func-
tion to languages; fack(L) =

⋃
w∈L fack(w).

3.1 Grammars, languages, and learning
A language (or stringset) L is a subset of Σ∗. If
L is finite, let |L| denote the cardinality of L, and
let ||L|| denote the size of L, which is defined to be∑

w∈L |w|. Let L1 · L2 denote the concatenation of
the languages L1 and L2, i.e., the pairwise concate-
nation of each word in L1 to each word in L2. For
notational simplicity, the concatenation of a single-
ton language {w} to another language L2 (or L1 to
{w}) will be written wL2 (L1w).

A grammar is a finite representation of a pos-
sibly infinite language. A class L of languages is
represented by a class R of representations if every
r ∈ R is of finite size and there is a naming function
L : R→ L which is both total and surjective.

The learning paradigm used in this paper is identi-
fication in the limit learning paradigm (Gold, 1967),
with polynomial bounds on time and data (de la
Higuera, 1997). This paradigm has two complemen-
tary aspects. First, it requires that the information
which distinguishes a learning target from other po-
tential targets be present in the input for algorithms
to successfully learn. Second, it requires successful

algorithms to return a hypothesis in time polynomial
of the size of the sample, and that the size of the sam-
ple itself must be polynomial in the size of grammar.

The definition of the learning paradigm (Defini-
tion 3) depends on some preliminary notions.
Definition 1. Let L be a class of languages repre-
sented by some class R of representations.

1. An input sample I for a language L ∈ L is a
finite set of data consistent with L, that is to say
I ⊆ L.

2. A (L,R)-learning algorithm A is a program
that takes as input a sample for a language
L ∈ L and outputs a representation from R.

The notion of characteristic sample is integral.
Definition 2 (Characteristic sample). For a (L,R)-
learning algorithm A, a sample CS is a characteris-
tic sample of a language L ∈ L if for all samples I
for L such that CS ⊆ I , A returns a representation
r such that L(r) = L.

Now the learning paradigm can be defined.
Definition 3 (Identification in polynomial time and
data). A class L of languages is identifiable in
polynomial time and data if there exists a (L,R)-
learning algorithm A and two polynomials p() and
q() such that:

1. For any sample I of size m for L ∈ L, A re-
turns a hypothesis r ∈ R in O(p(m)) time.

2. For each representation r ∈ R of size n, there
exists a characteristic sample of r for A of size
at most O(q(n)).

4 Tier-based Strictly Local Languages

This section introduces the Tier-based Strictly Local
(TSL) languages (Heinz et al., 2011). The TSL lan-
guages generalize the SL languages (McNaughton
and Papert, 1971; Garcı́a et al., 1990; Caron, 2000),
and as such these will briefly be discussed first.

4.1 The Strictly Local Languages
The SL languages can be defined as follows (Heinz
et al., 2011):
Definition 4 (SL languages). A language L
is Strictly k-Local (SLk) iff there exists a fi-
nite set S ⊆ fack(oΣ∗n) such that L =
{w ∈ Σ∗|fack(own) ⊆ S}

89



Such a set S is sometimes referred to as the per-
missible k-factors or permissible substrings of L.
For example, let L = {λ, ab, abab, ababab, ...}.
This L can be described with a set of permissible
2-factors S = {on,oa, ab, ba, bn} because every
2-factor of every word in L is in S; thus, L is Strictly
2-Local (abbreviated SL2).

As a set S of permissible k-factors is finite it can
also be viewed as a SL grammar where L(S) =
{w ∈ Σ∗|fack(own) ⊆ S}. An element s of an
SL grammar S for a language L is useful (resp. use-
less) iff s ∈ fack(L) (s 6∈ fack(L)). A canonical
SL grammar contains no useless elements.

In the example above, aa 6∈ S and aa 6∈ fac2(w)
for any w ∈ L. Such a string is referred to as a
forbidden k-factor or a restriction on L. The set of
forbidden k-factors R is fack(oΣ∗n)− S. Think-
ing about the grammar in terms of S or in terms ofR
is equivalent, but in some cases it is simpler to refer
to one rather than the other, so we shall use both.

Any SLk class of languages is learnable with
polynomial bounds on time and data if k is known
in advance (Garcı́a et al., 1990; Heinz, 2010b).

The class of SLk languages (for each k) belongs
to a collection of language classes called string ex-
tension language classes (Heinz, 2010b). The dis-
cussion above presents SLk languages from this per-
spective. These language classes have many desir-
able learning properties, due to their underlying lat-
tice structure (Heinz et al., 2012).

4.2 The TSL languages
The TSL languages can be thought of as a further pa-
rameterization on the k-factor function where a cer-
tain subset of the alphabet takes part in the grammar
and all other symbols in the alphabet are ignored.
This special subset is referred to as a tier T ⊆ Σ.
Symbols not on the tier are removed from consider-
ation of the grammar by an erasing function ET :
Σ∗ → T ∗ defined as ET (σ0σ1...σn) = u0u1...un
where ui = σi if σi ∈ T ; else ui = λ.

For example, if Σ = {a, b, c} and T = {a, c}
then ET (bbabbcbba) = aca. We can then define
a tier version facT-k of fack as facT-k(w) =
fack(oET (w)n).

Here, o and n are built into the function as they
are always treated as part of the tier. Continu-
ing the example from above, facT-2(bbabbcbba) =

{oa, ac, ca, an}. facT-k can be extended to lan-
guages as with fack above.

The TSL languages can now be defined parallel to
the SL languages (the following definition is equiv-
alent to the one in Heinz et al. (2011)):

Definition 5 (TSL languages). A languageL is Tier-
based Strictly k-Local iff there exists a subset T ⊆ Σ
of the alphabet and a finite set S ⊆ fack(oT∗n)
such that L = {w ∈ Σ∗|facT-k(w) ⊆ S}

Parallel to SL grammars above, 〈T, S〉 can be
thought of as a TSL grammar of L. Likewise, the
forbidden tier substrings (or tier restrictions) R is
simply the set facT-k(Σ∗) − S. Finally, 〈T, S〉 is
canonical if S contains no useless elements (i.e., s ∈
S ⇔ s ∈ facT-k(L(〈T, S〉))) and R is nonempty
(this second restriction is explained below).

For example, let Σ = {a, b, c} and T =
{a, c} as above and let S = {on,oa,oc,
ac, an, ca, cc, cn}. Plugging these into Definition
5, we obtain a language L which only contains
those strings without the forbidden 2-factor aa on
tier T . These are words which may contain bs in-
terspersed with as and cs provided that no a pre-
cedes another without an intervening c. For exam-
ple, bbabbcbba ∈ L but bbabbbbabb 6∈ L, because
ET (bbabbbbabb) = aa and aa ∈ fac2(oaan) but
aa 6∈ S.

Like the class of SLk languages, the class of TSL
languages (for fixed T and k) is a string extension
language class. The relationship of the TSL lan-
guages to other sub-Regular language classes (Mc-
Naughton and Papert, 1971; Rogers et al., 2013) is
studied in Heinz et al. (2011).

Given a fixed k and T , S is easily learnable in the
same way as the SL languages (Heinz et al., 2011).
However, as discussed above, in the case of natural
language phonology, it is not clear that information
about T is available a priori. Learning both T and
S simultaneously is thus an interesting problem.

This problem admits a technical, but unsatisfying,
solution. The number of subsets T such that T ⊆ Σ
is finite, so the number of TSLk languages given a
fixed k is finite. It is already known that any finite
class of languages which can be enumerated can be
identified in the limit by an algorithm which checks
through the enumeration (Gold, 1967). However,
given the cognitive relevance of TSL languages, it

90



is of interest to pursue a smarter, computationally
efficient method for learning them.

What are the consequences of varying T ? When
T = Σ, the result is an SLk language, because the
erasing function operates vacuously (Heinz et al.,
2011). Conversely, when T = ∅, by Definition 5, S
is either ∅ or {on}. The former obtains the empty
language while the latter obtains Σ∗. By Definition
5, Σ∗ can be described with any T ⊆ Σ as long as
S = fac2(oT ∗n). In such a grammar, none of the
members of T serve any purpose; thus we stipulate
that for a canonical grammar R is nonempty.

Importantly, a member of T may fail to belong
to a string in R and still serve a purpose. For ex-
ample, let Σ = {a, b, c}, T = {a, c}, and S =
fac2(oT ∗n)− {aa}. Because c appears in no for-
bidden tier substrings in R, it is freely distributed in
L = L(〈T, S〉). However, it makes a difference in
the language, because aca ∈ L but aba 6∈ L. If
c (and the relevant tier substrings in S) were miss-
ing from the tier, neither aba nor aca would be in L.
This can be thought of a ‘blocking’ function of c, be-
cause it allows sequences a...a even though aa ∈ R.

We may now return to the Latin example from
§2 in a little more detail. The Latin pattern, in
which rs and ls must alternate, regardless of other
intervening sounds. This can be captured by a
TSL grammar in which T = {l, r} and S =
{ol,or, lr, rl, rr, rn, ln}. This captures the gen-
eralization that, ignoring all sounds besides l and r,
l and l are never allowed to be adjacent. The re-
mainder of the paper discusses how such a grammar,
including T , may be induced from positive data.

5 Algorithm

This section introduces the Tier-based Strictly 2-
Local Inference Algorithm (2TSLIA), which in-
duces both a tier and a set of permissible tier 2-
factors from positive data. First, in §5.1, the con-
cept of a path, which is crucial to the learner, is
defined. §5.3 introduces and describes the algo-
rithm, and §5.4 defines a distinguishing example and
proves that it is a characteristic sample for which the
algorithm is guaranteed to converge. Time and data
complexity for the algorithm are discussed there.

5.1 Paths
First, we define the concept of 2-paths. How this
concept might be generalized to k is discussed in §6.
Paths denote precedence relations between symbols
in a string, but they are further augmented with sets
of intervening symbols. Formally, a 2-path is a 3-
tuple 〈x, Z, y〉 where x and y are a symbol in Σon
and Z is a subset of Σ. The 2-paths of a string w =
σ0σ1 . . . σn, denoted paths2(w), are

paths2(w) =
{
〈σi, Z, σj〉

∣∣ i < j and
Z = {σz|i < z < j}

}

Essentially, the 2-paths are pairs of symbols in a
string ordered by precedence and interpolated with
the sets of symbols intervening between them. For
example, 〈a, {b, c}, d〉 is in paths2(abbcd) because
a precedes d and {b, c} is the set of symbols that
come between them. Intuitively, it gives the set of
symbols one must ‘travel over’ in order to get from
one symbol to another in a string. As shall be seen
shortly, this information is essential for how the al-
gorithm works. Let paths2() be extended to lan-
guages such that paths2(L) =

⋃
w∈L paths2(w).

As we are here only concerned with 2-paths, we
henceforth simply refer to them as ‘paths’.

Remark 1. The paths of a string w can be calcu-
lated in time at most quadratic in the size of w =
σ1 · · ·σn.

To see why, consider a n×n table and consider i, j
such that 1 ≤ i < j ≤ n. The idea is each nonempty
cell in the table contains the set of intervening sym-
bols between σi and σj . Let p(x, y) denote the entry
in the x-th row and the y-th column. The following
hold: p(i, i + 1) = ∅; p(i, i + 2) = {σi+1}; and
p(i, j) =

⋃
i≤x≤j−2 p(x, x + 2) for any j ≥ i + 3.

Since each of these operations is linear time or less,
the size of the table, which equals n2, provides an
upper bound on the time complexity of paths2(w).
(This bound is not tight since half the table is empty.)

5.2 Terminology
Before introducing the algorithm we define some
terms which are useful for understanding its oper-
ation. For a TSL2 grammar G = 〈T, S〉, T ⊆ Σ
will be referred to as the tier, S the allowed tier sub-
strings, and R = facT-2(Σ∗) − S as the forbid-
den tier substrings. Let H = Σ − T be the non-

91



tier elements of G. For any elements σi, σj ∈ T ,
their tier adjacency with respect to a set L of strings
refers to whether or not they may appear adjacent
on the tier; formally, σi, σj are tier adjacent in L iff
σiσj ∈ facT-2(w) for some w ∈ L.

An exclusive blocker is a σ ∈ T which does not
appear in R. That is, ∀w ∈ R, σ 6∈ fac1(w). An
exclusive blocker is thus not restricted in its distri-
bution but may intervene between other elements on
the tier. The free elements of a grammar G will refer
to the union of the set of the nontier elements of G
and exclusive blockers given G.

Given an order σ0, σ1, ..., σn on Σ, let Σi =
{σh|h < i} refer to the elements of Σ less than σi
and Ji = Σ−Σi be the elements σi and greater. Note
that Σ0 = ∅ and J0 = Σ. Let Hi = H ∩ Σi be the
non-tier elements less than σi and Ti = (T ∩Σi)∪Ji
be the expanded tier given σi, or the tier plus ele-
ments from Ji. Note thatHi and Ti are complements
of each other with respect to Σ. In the context of the
algorithm, Ti will represent the algorithm’s current
hypothesis for the tier. When referring not to the or-
der on Σ but to the index of positions in a string or
path, τ1, τ2, ...,∈ Σ shall be used.

For a path 〈τ1, X, τ2〉, we refer to τ1 and τ2 as the
symbols on the path, and X as the intervening set.
For a set of paths P , the term tier paths will refer
to the paths whose symbols are on Ton. Formally,
the tier paths are PT = {〈τ1, X, τ2〉 ∈ P |τ1, τ2 ∈
Ton, X ⊆ Σ}. Note that PT is restricted only by
the symbols on the path; the intervening set X may
be any subset of Σ.

Example 2. Let Σ = {a, b, c, d} with the order a <
b < c < d and let T = {b, d}. Referring to b as σ2
in the order, Σ2 = H2 = {a}, and T2 = {b, c, d}. If
w = bacd, and P = paths2(w), then

P =
{
〈o, {}, b〉, 〈o, {b}, a〉,

〈o, {a, b}, c〉, 〈o, {a, b, c}, d〉, 〈o, {a, b, c, d},n〉,
〈b, {}, a〉, 〈b, {a}, c〉, 〈b, {a, c}, d〉, 〈b, {a, c, d},n〉,
〈a, {}, c〉, 〈a, {c}, d〉, 〈a, {c, d},n〉, 〈c, {d},n〉,
〈c, {}, d〉, 〈d, {},n〉

}
,

PT2 =
{
〈o, {}, b〉, 〈o, {a, b}, c〉, 〈o, {a, b, c}, d〉,

〈o, {a, b, c, d}n〉, 〈b, {a}, c〉, 〈b, {a, c}, d〉,
〈b, {a, c, d},n〉, 〈c, {d},n〉, 〈c, {}, d〉, 〈d, {},n〉

}
,

and PT =
{
〈o, {}, b〉, 〈o, {a, b, c}, d〉,

〈o, {a, b, c, d}n〉, 〈b, {a, c}, d〉, 〈b, {a, c, d},n〉,
〈d, {},n〉

}

5.3 Algorithm

The 2TSLIA2 (Algorithm 1 on the following page)
takes an ordered alphabet Σ and a set of input strings
I and returns a TSL grammar 〈T, S〉. It has two
main functions: get tier, which calculates T , and
main, which calls get tier and uses the resulting
tier to determine S.

First, get tier takes as arguments an index i, an
expanded tier Ti, and a set of paths P . The expanded
tier is the algorithm’s hypothesis for the tier at stage
i. It is first called with Ti = Σ (the most conser-
vative hypothesis for the tier) and i = 0. The goal
of get tier is to whittle down Ti, which is the set
of elements known to be in T plus the set of ele-
ments whose membership in T has not yet been de-
termined, down to only elements known to be in T .

The get tier function recursively iterates
through Ti, starting with σ0, to determine which
members of Ti should be in the final hypothesis
T . Two other pieces of data are important for this:
PTi , or the tier paths of Ti whose symbols are in
Ti ∪ {o,n}, and Hi, or the set of non-tier elements
less than σi. The set Hi is the algorithm’s largest
safe hypothesis for non-tier elements, and it will rea-
son about restrictions on σi using paths from PTi .

Elements of Σ are checked for membership on
the tier in order; thus, when checking σi, get tier
has already checked every other σh for h < i. For
each σi, membership in T is decided on two condi-
tions labeled (a) and (b) in the if-then statement of
get tier. Condition (a) tests whether σi is a free
element, and condition (b) further tests whether σi
is an exclusive blocker. If σi is found to be a free
element and not an exclusive blocker, then it is a
non-tier element (see §5.2), and should be removed
from T . In detail:

Condition (a). To test whether σi is a free ele-
ment, this condition checks to see if there are any
restrictions on σi given PTi and Hi. It searches
through PTi for 〈o, X, σi〉, 〈σi, X ′,n〉, and, for all
σ′ ∈ Ti, 〈σi, Y, σ′〉 and 〈σ′, Y ′, σi〉, where the in-
tervening sets X,X ′, Y, Y ′ are all subsets of Hi. If
all of these appear in PTi , it means that σi is a free
element with respect to Ti.

92



Data: An alphabet Σ = {σ0, σ1, ..., σn}; finite
set I of input strings in Σ∗

Result: A TSL2 grammar 〈T, S〉
function get tier(Ti, P, i):

Initialize PTi = {〈τ1, X, τ2〉 ∈ P |τ1, τ2 ∈
Ti ∪ {o,n}};
Initialize Hi = Σ− Ti;
for i ≤ n do

Let σ = σi;
if a.) ∃X,X ′ ⊆ Hi such that
〈o, X, σ〉, 〈σ,X ′,n〉 ∈ PTi and
∀σ′ ∈ Ti,∃Y, Y ′ ⊆ Hi such that
〈σ, Y, σ′〉, 〈σ′, Y ′, σ〉 ∈ PTi and

b.) for each 〈τ1, Z, τ2〉 ∈ PTi with
τ1, τ2 ∈ ((Ti ∪ {o,n})− {σ}), σ ∈
Z,Z − {σ} ⊆ Hi, ∃Z ′ ⊆ Hi such that
〈τ1, Z ′, τ2〉 ∈ PTi then

Return get tier(Ti −
{σ}, PTi , i+ 1);

else
i = i+1

end
end
Return 〈Ti, PTi〉;

function main(I,Σ):
Initialize P = paths2(I);
Initialize S = ∅;
Set T, PT = get tier(Σ, P, 0);
for p = 〈τ1, X, τ2〉 ∈ PT do

if X ⊆ Σ− T then
Add τ1τ2 to S;

end
end
Return 〈T, S〉;

Algorithm 1: The TSL2 Learning Algorithm
(2TSLIA)

Example 3. Let Σ = {a, b, c, d} from Example 2,
with that order, and let I = {aaa, bab, cac, dad}.
In the initial condition of the algorithm, i = 0, so
σi = a, Ti = Σ, Hi = ∅, and PTi = paths2(I).

Because Hi = ∅, a satisifes condition (a)
if 〈o, ∅, a〉 ∈ PTi , 〈a, ∅,n〉 ∈ PTi , and for
each σ ∈ Ti = {a, b, c, d}, 〈σ, ∅, a〉 ∈ PTi and
〈a, ∅, σ〉 ∈ PTi . This is true given this partic-
ular I , because 〈o, ∅, a〉, 〈a, ∅,n〉, 〈a, ∅, a〉,∈

paths2(aaa), 〈b, ∅, a〉, 〈a, ∅, b〉 ∈ paths2(bab),
〈c, ∅, a〉, 〈a, ∅, c〉 ∈ paths2(cac), and
〈d, ∅, a〉, 〈a, ∅, d〉 ∈ paths2(dad).

Condition (b). This condition checks whether σi
is an exclusive blocker, and thus should not be re-
moved from Ti. It does this by looking for a pair
τ1, τ2 of members of Ti distinct from σi whose tier-
adjacency is dependent on σi. It searches through
paths of the form 〈τ1, Z, τ2〉, where Z includes σi
and some subset of Hi. For each such 〈τ1, Z, τ2〉, it
searches for a 〈τ1, Z ′, τ2〉 where Z ′ is only a subset
of Hi. If such a 〈τ1, Z ′, τ2〉 exists, then τ1 and τ2 are
tier-adjacent in the data regardless of σi. However,
if no such path exists, then it may be that τ1 and τ2
can only appear with σi in between them, and thus
get tier infers that σi is an exclusive blocker. If
any such pair is found, then condition (b) fails, and
σi must remain in Ti.

Example 4. Continuing with Example 3, a would
not satisfy condition (b) given I . Given that i =
0, in order for a to satisfy condition (b), for all
τ1, τ2 ∈ {o, b, c, d,n}, if 〈τ1, {a}, τ2〉 is in PTi ,
then 〈τ1, ∅, τ2〉 must also be in PTi . For this I , this
is false for τ1 = τ2 = b, because 〈b, {a}, b〉 ∈
paths2(bab), but 〈b, ∅, b〉 6∈ paths2(I). Thus
get tier would infer that a is an exclusive blocker.

However, the reader can confirm that a would
satisfy condition (b) for an input I ′ = I ∪
{λ, bb, cc, dd}.

If both conditions (a) and (b) are met, then the
algorithm determines that σi should not appear on
the final hypothesis T for the tier, and so get tier
is recursively called with Ti − {σi} and i + 1 as
arguments. Because the hypothesis for the tier has
changed, PTi is also passed, so on the next call when
get tier calculates PTi+1 , it only has to search
through PTi instead of the full set of paths P .

If neither condition is met, no change is made
to Ti, i is increased, and the next σi is checked by
continuing through the for loop. This process is re-
peated for each member of the alphabet, after which
Ti is returned as the final answer for the tier. Its tier
paths PTi are also returned.

The function main calls get tier and takes its
result as T and PT . Using this, it finds each τ1τ2 ∈
S. It does this by searching through PT and finding
paths 〈τ1, X, τ2〉 whose intervening set X is a sub-

93



set of the non-tier elements Σ − T . Such τ1τ2 pairs
are thus tier-adjacent in I . The resulting grammar
〈T, S〉 is then returned.

5.4 Identification in polynomial time and data

Here we establish the main result of the paper that
2TSLIA identifies the TSL2 class in polynomial
time and data. As is typical, the proof relies on a
establishing a characteristic sample for the 2TSLIA.

Lemma 1 establishes 2TSLIA runs efficiently.

Lemma 1. Given an input sample I of size n,
2TSLIA outputs a grammar in time polynomial in the
size of n.

Proof. The paths are calculated for each word once
at the beginning of main. This takes time quadratic
in the size of the sample (Remark 1), soO(n2). Call
this set of paths P (note P is also bounded in size
by n2).

Additionally, the loop in get tier is called
exactly |Σ| times. Checking condition (a) in
get tier requires a single pass through the paths.
On other hand, condition (b) requires one search
through P for every path element p ∈ P in the worst
case, which is O(n4). Thus the time complexity for
get tier is O(|Σ|(n2 + n4)) = O(n4) since |Σ| is
a constant.

Lastly, finding the permissible substrings on the
tier also requires a single pass through P , which also
takesO(n2). Altogether then an upper bound on the
time complexity of 2TSLIA is given byO(n2+n4+
n2) = O(n4), which is polynomial.

Next we define a distinguishing sample for a tar-
get language for the 2TSLIA. We first show it is
polynomial in the size of the target grammar. We
then show that it is a characteristic sample.

Definition 6 (Distinguishing sample). Given an al-
phabet Σ, with some order σ1, σ2, ..., σn on Σ, the
target language L ∈ TSL2, and the canonical gram-
mar G = 〈T, S〉 for L, a distinguishing sample D
forG is the set meeting the following conditions. Re-
call that H = Σ − T are the non-tier elements and
Hi refers to the non-tier elements less than σi.

1. The non-tier element condition. For all non-
tier elements σi ∈ H ,

i. ∃w1, w2 ∈ D s.t. 〈o, X, σ〉 ∈
paths2(w1) and 〈σ,X ′,n〉 ∈
paths2(w2), X,X ′ ⊆ Hi
∀σ′ ∈ Ti, ∃v1, v2 ∈ D s.t.
〈σ, Y, σ′〉 ∈ paths2(v1) and
〈σ′, Y ′, σ〉 ∈ paths2(v2), Y, Y ′ ⊆ Hi.

ii. ∀τ1τ2 ∈ (facTi-2((Σ−{σi})∗)−R),w ∈
D s.t. 〈τ1, X, τ2〉 ∈ paths2(w), X ⊆ Hi.

2. The exclusive blocker condition. For each
σi ∈ T which is an exclusive blocker, w ∈ D
s.t. 〈τ1, X, τ2〉 ∈ paths2(w), where τ1τ2 ∈ R,
τ1 6= σi, τ2 6= σi, σi ∈ X , and X − σi ⊆ Hi

3. The allowed tier substring condition. ∀τ1τ2 ∈
S, some w s.t. 〈τ1, X, τ2〉 ∈ paths2(w) where
X ⊆ H

Essentially, item (1) ensures that all symbols σi
not on the target T will meet conditions (a) and (b)
in the for loop and be removed from the algorithm’s
hypothesis for T . Item (2) ensures that, in the situa-
tion an exclusive blocker σi ∈ T meets condition (a)
for removal from the tier hypothesis, it will not meet
condition (b). Item (3) ensures that the sample con-
tains every τ1τ2 in S. These points will be discussed
more detail in the proof that the distinguishing ex-
ample is a characteristic sample.

Lemma 2. Given a grammar G = 〈T, S〉 whose
size is |T |+ ||S||, the size of a distinguishing sample
D for G is polynomial in the size of G.

Proof. Recall that T ⊆ Σ and S ⊆ Σ2on, that
H = Σ − T and R = Σ2 − S. The non-tier el-
ement condition requires that for each σ ∈ H and
σ′ ∈ T , the sample minimally contains the words
σ, σ′σ, and σ′σ, whose total length is |H|+2|H||T |.
The exclusive blocker condition requires for each
exclusive blocker σ ∈ T and each τ1τ2 ∈ R that
minimally τ1στ2 is contained in the sample. Letting
B denote the set of exclusive blockers, we have the
total length of words in the characteristic sample is
||B|| × ||R||. Finally, the allowed tier substring con-
dition is requires for each τ1τ2 ∈ S that minimally
τ1τ2 is contained in the sample. Hence, the length of
these words equals |S|.

Altogether this means there is a characteristic
sampleD such that ||D|| = |H|+2|H||T |+ ||B||×
||R|| + |S|. Since H,T,B ⊆ Σ and R,S ⊆

94



Σ2, ||D|| ≤ |Σ| + 2|Σ||Σ| + |Σ||Σ|2 + |Σ|2 =
|Σ| + 3|Σ|2 + |Σ|3. Thus, ||D|| is bounded by
O(|Σ|3) which, since |Σ| is a constant, is effectively
O(1).

Next we prove Lemma 3, which shows that the
tier conjectured by the 2TSLIA at step i is correct
for all symbols up to σi. Thus, once all symbols are
treated by the 2TSLIA, its conjecture for the tier is
correct. Let T ′i correspond to the algorithm’s current
tier hypothesis when σi is being checked in the for
loop of the get tier function, let H ′i = Σ − T ′i
be the algorithm’s hypothesis for the set of non-tier
elements less than σi, and let PT ′i be the set of paths
under consideration (i.e., the set of paths from the
initialization of PTi before the for loop). As above,
τ0τ1...τm index positions in a string or path.

Lemma 3. Let Σ = {σ0, . . . , σn} and consider any
G = 〈T, S〉. Given any finite input sample I which
contains a distinguishing sample D for G, it is the
case that for all i (0 ≤ i ≤ n), T ′i = Ti.

Proof. The proof is by recursion on i. The base case
is when i = 0. By definition, T0 = Σ. The algo-
rithm starts with T ′0 = Σ, so T

′
0 = T0.

Next we assume the recursive hypothesis (RH):
for some i ∈ N that T ′i = Ti. We prove that T ′i+1 =
Ti+1. Specifically, we show that if RH is true for
i, then (Case 1) σi ∈ Hi+1 implies σi ∈ H ′i+1 and
(Case 2) σi 6∈ Hi+1 implies σi 6∈ H ′i+1.

Case 1. This is the case in which σi is a non-tier
element. The non-tier element condition in Defini-
tion 6 for D ensures that the data in I will meet both
conditions (a) and (b) in the for loop in get tier()
for removing σi from the tier hypothesis.

Condition (a) requires that 〈o, X, σi〉 ∈ PT ′i and
〈σi, X ′,n〉 ∈ PT ′i for some X,X

′ ⊆ H ′i, and
∀σ′ ∈ Ti, 〈σi, Y, σ′〉 ∈ PT ′i and 〈σ

′, Y ′, σi〉 ∈ PT ′i
for some Y, Y ′ ⊆ H ′i. Part (i) of the non-tier ele-
ment condition in Defintion (6) ensures that for σi,
there are words w1, w2 ∈ I such that 〈o, X, σi〉 ∈
paths2(w1), 〈σi, X ′,n〉 ∈ paths2(w2), and, for all
σ′ ∈ Ti, v1, v2 ∈ I s.t. 〈σi, Y, σ′〉 ∈ paths2(v1)
and 〈σ′, Y ′, σi〉 ∈ paths2(v2), where the interven-
ing sets X,X ′, Y, Y ′ are all subsets of Hi. Because
by RH H ′i = Hi, condition (a) for removing σi from
the tier hypothesis is satisfied.

For σi to satisfy condition (b), for any path
〈τ1, X, τ2〉 ∈ PT ′i such that σi ∈ X and X−{σi} ⊆
H ′i, there must be another path 〈τ1, X ′, τ2〉 ∈ PT ′i
whereX ′ ⊆ H ′i. If τ1τ2 ∈ R, then such a 〈τ1, X, τ2〉
is guaranteed not to exist in PT ′i , because τ1 and
τ2 will, by the definition of R, not appear in the
data with only non-tier elements between them. For
τ1τ2 6∈ R, part (ii) of the nontier element condi-
tion in Definition 6 ensures that some 〈τ1, Y, τ2〉
wher e Y ⊆ Hi exists in PT ′i , as it requires that
for each such τ1, τ2 there is some w ∈ I such that
〈τ1, X, τ2〉 ∈ paths2(w) where X ⊆ Hi. By hy-
pothesis H ′i = Hi, and so there is always guaran-
teed to be some 〈τ1, X ′, τ2〉 ∈ PT ′i where X

′ ⊆ H ′i.
Thus, condition (b) will always be satisfied for σi.

Thus, assuming the RH, σi ∈ Hi+1 is guaranteed
to satisfy conditions (a) and (b) and be removed from
the algorithm’s hypothesis for the tier, so σi ∈ H ′i+1.

Case 2. This is the case in which σi ∈ T . There
are two mutually exclusive possibilities. The first
possibility is that σi is not a free element. Here,
σi is guaranteed to not be taken off of the tier hy-
pothesis, because condition (a) for removing a sym-
bol from the tier requires that there exists some path
〈σj , X, σi〉 and 〈σi, X ′, σj〉 where X,X ′ ⊆ H ′i.
From the definition of a TSL grammar, there will ex-
ist no 〈σj , Y, σi〉 and 〈σi, Y ′, σj〉, where Y, Y ′ ⊆ H ,
in the paths of I . Because Hi ⊆ H and by hypoth-
esis H ′i = Hi, H

′
i ⊆ H , so the algorithm will cor-

rectly register that σjσi ∈ R or σiσj ∈ R and σi
will remain on the tier hypothesis. Thus σi 6∈ H ′i+1.

The other possibility is that σi is an exclusive
blocker. If so, σi may satisfy condition (a) for tier
removal (as discussed above for the case in which
σi ∈ H). However, the exclusive blocker condi-
tion in Definition 6 for D guarantees that σi will
not meet condition (b). As discussed above, con-
dition (b) will fail if there is a 〈τ1, X, τ2〉 ∈ PT ′i
such that X includes σi and zero or more elements
ofH ′i and no other path 〈τ1, X ′, τ2〉 ∈ PT ′i whereX

′

only includes elements ofH ′i. The exclusive blocker
condition requires that there will be some τ1τ2 ∈ R
such that there is a word w ∈ D s.t. 〈τ1, Y, τ2〉 ∈
paths2(w) where σi ∈ Y and Y − {σi} = Hi.
From the definition of a TSL grammar, no w such
that 〈τ1, Y ′, τ2〉 ∈ paths2(w) where Yi ⊆ Hi will
appear in I , because Hi ⊆ H . Because by hy-
pothesis H ′i = Hi, the algorithm will correctly find

95



〈τ1, Y, τ2〉 and also find that there is no 〈τ1, Y ′, τ2〉.
Thus when σi is an exclusive blocker, it will not be
removed from the tier hypothesis, and σi 6∈ H ′i.

We now know that both Cases (1) and (2) are
true; thus, assuming the RH, Hi+1 = H ′i+1. Thus,
by induction, (∀i)[Hi = H ′i]. Because Hi and
H ′i are the complements of Ti and T

′
i , respectively,

(∀i)[Ti = T ′i ].
Lemma 4 (Distinguishing sample = characteristic
sample). A distinguishing sample for a TSL2 lan-
guage as defined in Definition 6 is a characteristic
sample for that language for the 2TSLIA.

Proof. As above, let G′ = (T ′, S′) be the output of
the algorithm. From Lemma 3, we know that for any
language L(G), G = 〈T, S〉, and a characteristic
sample D for G, given any sample I of L such that
D ⊆ I , (∀i)[Ti = T ′i ]. That T ′ = T immediately
follows from this fact.

That S′ = S follows directly from T ′ = T and
the allowed tier substring condition of Definition 6
for D. The allowed tier substring condition states
that for all τ1τ2 ∈ S, the distinguishing sample will
contain some w s.t. 〈τ1, X, τ2〉 ∈ paths2(w) where
X ⊆ H . Because T = T ′, the for loop of main
will correctly find all such τ1τ2. Thus, S = S′, and
G = G′.

Theorem 1 (Identification in the limit in polynomial
time and data). The 2TSLIA identifies the TSLk lan-
guages in polynomial time and data.

Proof. Immediate from Lemmas 1, 2, and 4.

We note further that in the worst case the time
complexity is polynomial (degree four Lemma 1)
and the data complexity is constant (Lemma 2).

6 Discussion

This algorithm opens up multiple paths for future
work. The most immediate theoretical question is
whether the algorithm here can be (efficiently) gen-
eralized to any TSL class as long as the learner
knows k a priori. We believe that it can. The no-
tion of 2-paths can be straightforwardly extended to
k such that a k-path is a 2k − 1 tuple of the form
〈σ0, X0, σ1, X1, ..., Xk−1, σk〉, where each set Xi
represents the symbols between σi and σi+1. The
algorithm presented here can then be modified to

check a set of such k-paths. We believe such an al-
gorithm could be shown to be provably correct using
a proof of similar structure to the one here, although
time and data complexity will likely increase.

However, in terms of applying these algorithms
to natural language phonology, it is likely that a k
value of 2 is sufficient. Heinz et al. (2011) argue that
TSL2 can describe both long-distance dissimilation
and assimilation patterns. One potential exception
to this claim comes from Sundanese, where whether
liquids must agree or disagree partly depends on the
syllable structure (Bennett, 2015).

Additional issues arise with natural language data.
One is that natural language corpora often include
some exceptions to phonotactic generalizations. Al-
gorithms which take as input such noisy data and
output grammars that are guaranteed to be represen-
tations of languages ‘close’ to the target language
have been obtained and studied in the PAC learning
paradigm (Angluin and Laird, 1988). It would be in-
teresting to apply such techniques and other similar
ones to adapt 2TSLIA into an algorithm that remains
effective despite noisy input data.

Another area of future research is how to gen-
eralize over multiple tiers. Jardine (2016), in run-
ning versions of the 2TSLIA on natural language
corpora, shows that it fails because local dependen-
cies (which again can be modeled where T = Σ)
prevent crucial information in the CS from appear-
ing in the data. Furthermore, natural languages can
have separate long-distance phonotactics which hold
over distinct tiers. For example, KiYaka has both a
vowel harmony pattern (Hyman, 1998) and a liquid-
nasal harmony pattern over the tier {l,m,n,N} (Hy-
man, 1995). Thus, words in KiYaka exhibit a pattern
corresponding to the intersection of two TSL2 gram-
mars, one with a vowel tier and one with a nasal-
liquid tier. The problem of learning a finite inter-
section of TSL2 languages is thus another relevant
learning problem.

One final way this result can be extended is
to study the nature of long-distance processes in
phonology. Chandlee (2014) extends the notion of
SL languages to the Input- and Output-Strictly Local
string functions, which are sufficient to model local
phonological processes. Subsequent work (Chan-
dlee et al., 2014; Chandlee et al., 2015; Jardine et
al., 2014) has shown how these classes of functions

96



can be efficiently learned, building on ideas on the
learning of SL functions. An open question, then, is
how these ideas can be used to develop a functional
version of the TSL languages to model long-distance
processes. The central result in this paper may then
help to understand how the tiers over which these
processes apply can be learned.

7 Conclusion

This paper has presented an algorithm which can
learn a grammar for any TSL2 language in time
polynomial in the size of the input sample, whose
size is bounded by a constant. As the TSL2 lan-
guages can model long-distance phonotactics in nat-
ural language, this represents a step towards under-
standing how humans internalize such patterns.

Acknowledgments

We thank Rémi Eyraud, Kevin McMullin, and two
anonymous reviewers for useful comments. This re-
search was supported by NSF#1035577.

References
Dana Angluin and Philip Laird. 1988. Learning from

noisy examples. Machine Learning, 2:343–370.
William G. Bennett. 2013. Dissimilation, Consonant

Harmony, and Surface Correspondence. Ph.D. thesis,
Rutgers, the State University of New Jersey.

William G. Bennett. 2015. Assimilation, dissimilation,
and surface correspondence in sundanese. Natural
Language and Linguistic Theory, 33(2):371–415.

Pascal Caron. 2000. Families of locally testable lan-
guages. Theoretical Computer Science, 242:361–376.

Jane Chandlee, Rémi Eyraud, and Jeffrey Heinz. 2014.
Learning Strictly Local Subsequential Functions.
Transactions of the Association for Computational
Linguistics, 2:491–503.

Jane Chandlee, Rémi Eyraud, and Jeffrey Heinz. 2015.
Output Strictly Local functions. In Proceedings of the
14th Meeting on the Mathematics of Language (MoL
14), Chicago, IL, July.

Jane Chandlee. 2014. Strictly Local Phonological Pro-
cesses. Ph.D. thesis, University of Delaware.

Noam Chomsky and Morris Halle. 1965. Some contro-
versial questions in phonological theory. Journal of
Linguistics, 1(2):pp. 97–138.

George N. Clements and Engin Sezer. 1982. Vowel and
consonant disharmony in Turkish. In Harry van der
Hulst and Norval Smith, editors, The Structure of

Phonological Representations (Part II). Foris, Dor-
drecht.

Eung-Do Cook. 1984. A Sarcee Grammar. Vancouver:
University of British Columbia Press.

Colin de la Higuera. 1997. Characteristic sets for poly-
nomial grammatical inference. Machine Learning,
27(2):125–138.

Colin de la Higuera. 2010. Grammatical Inference:
Learning Automata Grammars. Cambridge University
Press.

Pedro Garcı́a, Enrique Vidal, and José Oncina. 1990.
Learning locally testable languages in the strict sense.
In Proceedings of the Workshop on Algorithmic Learn-
ing Theory, pages 325–338.

Mark E. Gold. 1967. Language identification in the
limit. Information and Control, 10:447–474.

John Goldsmith and Jason Riggle. 2012. Information
theoretic approaches to phonological structure: The
case of Finnish vowel harmony. Natural Language &
Linguistic Theory, 30(3):859–896.

John Goldsmith and Aris Xanthos. 2009. Learning
phonological categories. Language, 85(1):4–38.

Jeffrey Heinz and James Rogers. 2013. Learning sub-
regular classes of languages with factored determinis-
tic automata. In Andras Kornai and Marco Kuhlmann,
editors, Proceedings of the 13th Meeting on the Math-
ematics of Language (MoL 13), pages 64–71, Sofia,
Bulgaria, August. Association for Computational Lin-
guistics.

Jeffrey Heinz, Chetan Rawal, and Herbert G. Tanner.
2011. Tier-based strictly local constraints for phonol-
ogy. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics, pages
58–64, Portland, Oregon, USA, June. Association for
Computational Linguistics.

Jeffrey Heinz, Anna Kasprzik, and Timo Kötzing. 2012.
Learning with lattice-structured hypothesis spaces.
Theoretical Computer Science, 457:111–127, October.

Jeffrey Heinz. 2010a. Learning long-distance phonotac-
tics. Linguistic Inquiry, 41:623–661.

Jeffrey Heinz. 2010b. String extension learning. In Pro-
ceedings of the 48th Annual Meeting of the Association
for Computational Linguistics, pages 897–906. Asso-
ciation for Computational Linguistics, July.

Larry Hyman. 1995. Nasal consonant harmony at a dis-
tance: The case of Yaka. Studies in African Linguis-
tics, 24:5–30.

Larry Hyman. 1998. Positional prominence and the
‘prosodic trough’ in Yaka. Phonology, 15:14–75.

Adam Jardine, Jane Chandlee, Rémi Eyraud, and Jeffrey
Heinz. 2014. Very efficient learning of structured
classes of subsequential functions from positive data.
In Proceedings of the 12th International Conference

97



on Grammatical Inference (ICGI 2014), JMLR Work-
shop Proceedings, pages 94–108.

Adam Jardine. 2016. Learning tiers for long-distance
phonotactics. In Laurel Perkins, Rachel Dudley, Ju-
liana Gerard, and Kasia Hitczenko, editors, Proceed-
ings of the 6th Conference on Generative Approaches
to Language Acquisition North America (GALANA
2015).

John Jensen. 1974. Variables in phonology. Language,
50:675–686.

Regine Lai. 2013. Domain Specificity in Learning
Phonology. Ph.D. thesis, University of Delaware.

Regine Lai. 2015. Learnable versus unlearnable har-
mony patterns. Linguistic Inquiry, 46(3):425–451.

Kevin McMullin and Gunnar Ólafur Hansson. forth-
coming. Long-distance phonotactics as Tier-Based
Strictly 2-Local languages. In Proceedings of the An-
nual Meeting on Phonology 2015.

Robert McNaughton and Seymour Papert. 1971.
Counter-Free Automata. MIT Press.

Andrew Nevins. 2010. Locality in Vowel Harmony.
Number 55 in Linguistic Inquiry Monographs. MIT
Press.

David Odden. 1994. Adjacency parameters in phonol-
ogy. Language, 70(2):289–330.

Catherine Ringen. 1975. Vowel Harmony: Theoretical
Implications. Ph.D. thesis, Indiana University.

James Rogers and Marc D. Hauser. 2010. The use of
formal language theory in studies of artificial language
learning: A proposal for distinguishing the differences
between human and nonhuman animal learners. In
Harry van der Hulst, editor, Recursion and Human
Language (Studies in Generative Grammar [SGG])
104. de Gruyter.

James Rogers and Geoffrey Pullum. 2011. Aural pattern
recognition experiments and the subregular hierarchy.
Journal of Logic, Language and Information, 20:329–
342.

James Rogers, Jeffrey Heinz, Gil Bailey, Matt Edlefsen,
Molly Visscher, David Wellcome, and Sean Wibel.
2010. On languages piecewise testable in the strict
sense. In Christian Ebert, Gerhard Jäger, and Jens
Michaelis, editors, The Mathematics of Language, vol-
ume 6149 of Lecture Notes in Artifical Intelligence,
pages 255–265. Springer.

James Rogers, Jeffrey Heinz, Margaret Fero, Jeremy
Hurst, Dakotah Lambert, and Sean Wibel. 2013. Cog-
nitive and sub-regular complexity. In Formal Gram-
mar, volume 8036 of Lecture Notes in Computer Sci-
ence, pages 90–108. Springer.

Sharon Rose and Rachel Walker. 2004. A typology of
consonant agreement as correspondence. Language,
80:475–531.

Keiichiro Suzuki. 1998. A typological investigation of
dissimilation. Ph.D. thesis, University of Arizona,
Tucson.

98


