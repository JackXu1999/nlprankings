










































Untitled


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3231–3241
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

3231



3232

embeddings (Nickel and Kiela, 2017, 2018) as

follows: Roller et al. (2018) showed that Hearst

patterns can provide important constraints for hy-

pernymy extraction from distributional contexts.

However, it is also well-known that Hearst patterns

suffer from missing and incorrect extractions, as

words must co-occur in exactly the right pattern to

be detected successfully. For this reason, we first

extract potential is-a relationships from a corpus

using Hearst patterns and build a directed weighted

graph from these extractions. We then embed this

Hearst Graph in hyperbolic space to infer missing

hypernymy relations and remove wrong extractions.

By using hyperbolic space for the embedding, we

can exploit the following important advantages:

Consistency Hyperbolic entailment cones (Ganea

et al., 2018) allow us to enforce transitivity of

is-a-relations in the entire embedding space.

This improves the taxonomic consistency of

the model, as it enforces that (x,is-a, z) if
(x,is-a, y) and (y,is-a, z). To improve
optimization properties, we also propose a

new method to compute hyperbolic entailment

cones in the Lorentz model of hyperbolic space.

Efficiency Hyperbolic space allows for very low

dimensional embeddings of graphs with latent

hierarchies and heavy-tailed degree distribu-

tions. For embedding large Hearst graphs –

which exhibit both properties (e.g., see Fig-

ure 2) – this is an important advantage. In our

experiments, we will show that hyperbolic em-

beddings allow us to decrease the embedding

dimension by over an order of magnitude while

outperforming SVD-based methods.

Interpretability In hyperbolic embeddings, simi-

larity is captured via distance while the gener-

ality of terms is captured through their norms.

This makes it easy to interpret the embeddings

with regard to their hierarchical structure and

allows us to get additional insights, e.g., about

a term’s degree of generality.

Figure 1 shows an example of a two-dimensional

embedding of the Hearst graph that we use in our

experiments. Although we will use higher dimen-

sionalities for our final embedding, the visualiza-

tion serves as a good illustration of the hierarchical

structure that is obtained through the embedding.

2 Related Work

Hypernym detection Detecting is-a-relations

from text is a long-standing task in natural language

processing. A popular approach is to exploit high-

precision lexico-syntactic patterns as first proposed

by Hearst (1992). These patterns may be prede-

fined or learned automatically (Snow et al., 2005;

Shwartz et al., 2016; Nakashole et al., 2012). How-

ever, it is well known that such pattern-based meth-

ods suffer significantly from missing extractions as

terms must occur in exactly the right configuration

to be detected (Shwartz et al., 2016; Roller et al.,

2018). Recent works improve coverage by lever-

aging search engines (Kozareva and Hovy, 2010)

or by exploiting web-scale corpora (Seitner et al.,

2016); but also come with precision trade-offs.

To overcome the sparse extractions of pattern-

based methods, focus has recently shifted to dis-

tributional approaches which provide rich repre-

sentations of lexical meaning. These methods al-

leviate the sparsity issue but also require special-

ized similarity measures to distinguish different

lexical relationships. To date, most measures are

inspired by the Distributional Inclusion Hypothe-

sis (DIH; Geffet and Dagan 2005) which hypoth-

esizes that for a subsumption relation (cat, is-a,

mammal) the subordinate term (cat) should appear

in a subset of the contexts in which the superior

term (mammal) occurs. Unsupervised methods for

hypernymy detection based on distributional ap-

proaches include WeedsPrec (Weeds et al., 2004),

invCL (Lenci and Benotto, 2012), SLQS (Santus

et al., 2014), and DIVE (Chang et al., 2018). Dis-

tributional representations that are based on posi-

tional or dependency-based contexts may also cap-

ture crude Hearst-pattern-like features (Levy et al.,

2015; Roller and Erk, 2016). Shwartz et al. (2017)

showed that such contexts plays an important role

for the success of distributional methods. Camacho-

Collados et al. (2018) proposed a new shared task

for hypernym retrieval from text corpora.

Recently, Roller et al. (2018) performed a sys-

tematic study of unsupervised distributional and

pattern-based approaches for hypernym detection.

Their results showed that pattern-based methods

are able to outperform DIH-based methods on sev-

eral challenging hypernymy benchmarks. Key as-

pects to good performance were the extraction of

patterns from large text corpora and using embed-

ding methods to overcome the sparsity issue. Our

work builds on these findings by replacing their



3233

1

2

3

4

5

0 1 2 3 4 5

Rank (log scale)

F
re

q
u
e
n
c
y
 (

lo
g
 s

c
a
le

)

Figure 2: Frequency distribution of words appearing

in the Hearst pattern corpus (on a log-log scale).

embeddings with ones with a natural hierarchical

structure.

Taxonomy induction Although detecting hyper-

nymy relationships is an important and difficult

task, these systems alone do not produce rich taxo-

nomic graph structures (Camacho-Collados, 2017),

and complete taxonomy induction may be seen as

a parallel and complementary task.

Many works in this area consider a taxonomic

graph as the starting point, and consider a variety

of methods for growing or discovering areas of the

graph. For example, Snow et al. (2006) train a clas-

sifier to predict the likelihood of an edge in Word-

Net, and suggest new undiscovered edges, while

Kozareva and Hovy (2010) propose an algorithm

which repeatedly crawls for new edges using a web

search engine and an initial seed taxonomy. Cimi-

ano et al. (2005) considered learning ontologies

using Formal Concept Analysis. Similar works

consider noisy graphs discovered from Hearst pat-

terns, and provide algorithms for pruning edges

until a strict hierarchy remains (Velardi et al., 2005;

Kozareva and Hovy, 2010; Velardi et al., 2013).

Maedche and Staab (2001) proposed a method to

learn ontologies in a Semantic Web context.

Embeddings Recently, works have proposed a

variety of graph embedding techniques for rep-

resenting and recovering hierarchical structure.

Order-embeddings (Vendrov et al., 2016) repre-

sent text and images with embeddings where the

ordering over individual dimensions forms a par-

tially ordered set. Hyperbolic embeddings repre-

sent words in hyperbolic manifolds such as the

Poincaré ball and may be viewed as a continuous

analogue to tree-like structures (Nickel and Kiela,

2017, 2018). Recently, Tifrea et al. (2018) also pro-

posed an extension of GLOVE (Pennington et al.,

Pattern

X which is a (example | class | kind | . . . ) of Y
X (and | or) (any | some) other Y
X which is called Y
X is JJS (most)? Y
X a special case of Y
X is an Y that
X is a !(member | part | given) Y
!(features | properties) Y such as X1, X2, . . .
(Unlike | like) (most | all | any | other) Y, X
Y including X1, X2, . . .

Table 1: Hearst patterns used in this study. Patterns are

lemmatized, but listed as inflected for clarity.

2014) to hyperbolic space. In addition, works have

considered how distributional co-occurrences may

be used to augment order-embeddings (Li et al.,

2018) and hyperbolic embeddings (Dhingra et al.,

2018). Further methods have focused on the of-

ten complex overlapping structure of word classes,

and induced hierarchies using box-lattice structures

(Vilnis et al., 2018) and Gaussian word embeddings

(Athiwaratkun and Wilson, 2018). Compared to

many of the purely graph-based works, these meth-

ods generally require supervision of hierarchical

structure, and cannot learn taxonomies using only

unstructured noisy data.

3 Methods

In the following, we discuss our method for un-

supervised learning of concept hierarchies. We

first discuss the extraction and construction of the

Hearst graph, followed by a description of the Hy-

perbolic Embeddings.

3.1 Hearst Graph

The main idea introduced by Hearst (1992) is to

exploit certain lexico-syntactic patterns to detect

is-a relationships in natural language. For in-

stance, patterns like “NPy such as NPx” or “NPx
and other NPy” often indicate a hypernymy rela-

tionship (u,is-a, v). By treating unique noun
phrases as nodes in a large, directed graph, we may

construct a Hearst Graph using only unstructured

text and very limited prior knowledge in the form of

patterns. Table 1 lists the only patterns that we use

in this work. Formally, let E = {(u, v)}Ni=1 denote
the set of is-a relationships that have been ex-

tracted from a text corpus. Furthermore, let w(u, v)
denote how often we have extracted the relationship

(u,is-a, v). We then represent the extracted pat-
terns as a weighted directed graph G = (V,E,w)



3234



3235



3236

let Θ = {vi}
M
i=1 be the set of embeddings. To find

an embedding that minimizes the overall energy,

we then solve the optimization problem

Θ̂ = argmin
Θ∈Hn

∑

u,v ∈V

L(u, v) (4)

where

L(u, v) =

{

E(u, v) if (u, v) ∈ E

max(0, γ − E(u, v)) otherwise

is the max-margin loss as used in (Ganea et al.,

2018; Vendrov et al., 2016). The goal of Equa-

tion (4) is to find a joint embedding of all terms

that best explains the observed Hearst patterns.

To solve Equation (4), we follow Nickel and

Kiela (2018) and perform stochastic optimization

via Riemannian SGD (RSGD; Bonnabel 2013). In

RSGD, updates to the parameters v are computed

via

vt+1 = expvt(−η grad f(vt)) (5)

where grad f(vt) denotes the Riemannian gradient
and η denotes the learning rate. In Equation 5, the
Riemannian gradient of f at v is computed via

grad f(vt) = projvt
(

g−1ℓ ∇f(v)
)

where ∇f(v) denotes the Euclidean gradient of f
and where

proj
v
(x) = v + 〈v,x〉Lv

g−1ℓ (v) = diag([−1, 1, . . . , 1])

denote the projection from the ambient space Rn+1

onto the tangent space TvL
n and the inverse of the

metric tensor, respectively. Finally, the exponential

map for Ln is computed via

exp
v
(x) = cosh(‖x‖L)v + sinh(‖x‖L)

x

‖x‖L

where ‖v‖L =
√

〈v,v〉L and v ∈ TxL
n.

As suggested by Nickel and Kiela (2018),

we initialize the embeddings close to the ori-

gin of Ln by sampling from the uniform dis-

tribution U(−0.001, 0.001) and by setting v0 to
√

1 + ||v′||2, what ensures that the sampled points
are located on the surface of the hyperboloid.

4 Experiments

To evaluate the efficacy of our method, we evaluate

on several commonly-used hypernymy benchmarks

(as described in (Roller et al., 2018)) as well as in a

reconstruction setting (as described in (Nickel and

Kiela, 2017)). Following Roller et al. (2018), we

compare to the following methods for unsupervised

hypernymy detection:

Pattern-Based Models Let E = {(x, y)}Ni=1 be
the set of Hearst patterns in our corpus, w(x, y) be
the count of how many times (x, y) occurs in E,
and W =

∑

(x,y)∈E w(x, y). We then consider the
following pattern-based methods:

Count Model (p) This model simply outputs the

count, or equivalently, the extraction probabilities

of Hearst patterns, i.e.,

p(x, y) =
w(x, y)

W

PPMI Model (ppmi) To correct for skewed oc-

currence probabilities, the PPMI model predicts

hypernymy relations based on the Positive Point-

wise Mutual Information over the Hearst pattern

corpus. Let p−(x) = Σ(x,y)∈Ew(x, y)/W and
p+(x) = Σ(y,x)∈Ew(y, x)/W , then:

ppmi(x, y) = max

(

0, log
p(x, y)

p−(x)p+(y)

)

SVD Count (sp) To account for missing relations,

we also compare against low-rank embeddings of

the Hearst corpus using Singular Value Decompo-

sition (SVD). Specifically, let X ∈ RMxM , such
that Xij = w(i, j)/W and UΣV

⊤ be the singular

value decomposition of X , then:

sp(x, y) = u⊤xΣrvy

SVD PPMI (spmi) We also evaluate against the

SVD of the PPMI matrix, which is identical to

sp(i, j), with the exception that Xij = ppmi(i, j),
instead of p(i, j). Roller et al. (2018) showed that
this method provides state-of-the-art results for un-

supervised hypernymy detection.

Hyperbolic Embeddings (HypeCones) We embed

the Hearst graph into hyperbolic space as described

in Section 3.2. At evaluation time, we predict the

likelihood using the model energy E(u, v).

Distributional Models The distributional mod-

els in our evaluation are based on the DIH, i.e., the

idea that contexts in which a narrow term x (ex:
cat) may appear should be a subset of the contexts

in which a broader term y (ex: animal) may appear.



3237

Detection (AP) Direction (Acc.) Graded (ρ)

BLESS EVAL LEDS SHWARTZ WBLESS BLESS WBLESS BIBLESS HYPERLEX

Cosine .12 .29 .71 .31 .53 .00 .54 .52 .14
WeedsPrec .19 .39 .87 .43 .68 .63 .59 .45 .43
invCL .18 .37 .89 .38 .66 .64 .60 .47 .43
SLQS .15 .35 .60 .38 .69 .75 .67 .51 .16

p(x, y) .49 .38 .71 .29 .74 .46 .69 .62 .62
ppmi(x, y) .45 .36 .70 .28 .72 .46 .68 .61 .60
sp(x, y) .66 .45 .81 .41 .91 .96 .84 .80 .51
spmi(x, y) .76 .48 .84 .44 .96 .96 .87 .85 .53

HypeCones .81 .50 .89 .50 .98 .94 .90 .87 .59

Table 2: Experimental results comparing distributional and pattern-based methods in all settings.

WeedsPrec The first distributional model we con-

sider is WeedsPrec (Weeds et al., 2004), which

captures the features of x which are included in the
set of more general term’s features, y:

WeedsPrec(x, y) =

∑n
i=1 xi · ✶yi>0
∑n

i=1 xi

invCL Lenci and Benotto (2012), introduce the

idea of distributional exclusion by also measuring

the degree to which the broader term contains con-

texts not used by the narrower term. The degree of

inclusion is denoted as:

CL(x, y) =

∑n
i=1min(xi, yi)
∑n

i=1 xi

To measure the inclusion of x and y and the non-
inclusion of y in x, invCL is then computed as

invCL(x, y) =
√

CL(x, y) · (1− CL(y, x))

SLQS The SLQS model is based on the informa-

tiveness hypothesis (Santus et al., 2014; Shwartz

et al., 2017), i.e., the idea that general words appear

mostly in uninformative contexts, as measured by

entropy. SLQS depends on the median entropy of

a term’s top k contexts:

Ex = median
k
i=1[H(ci)]

where H(ci) is the Shannon entropy of context ci
across all terms. SLQS is then defined as:

SLQS(x, y) = 1− Ex/Ey

Corpora and Preprocessing We construct our

Hearst graph using the same data, patterns, and pro-

cedure as described in (Roller et al., 2018): Hearst

patterns are extracted from the concatenation of

GigaWord and Wikipedia. The corpus is tokenized,

lemmatized, and POS-tagged using CoreNLP 3.8.0

(Manning et al., 2014). The full set of Hearst pat-

terns is provided in Table 1. These include proto-

typical Hearst patterns, like “animals [such as] big

cats”, as well as broader patterns like “New Year [is

the most important] holiday.” Noun phrases were

allowed to match limited modifiers, and produced

additional hits for the head of the noun phrase. The

final corpus contains circa 4.5M matched pairs,

431K unique pairs, and 243K unique terms.

Hypernymy Tasks We consider three distinct

subtasks for evaluating the performance of these

models for hypernymy prediction:

• Detection: Given a pair of words (u, v), deter-
mine if v is a hypernym of u.

• Direction: Given a pair (u, v), determine if u
is more general than v or vise versa.

• Graded Entailment: Given a pair of words (u,
v), determine the degree to which u is a v.

For detection, we evaluate all models on five

commonly-used benchmark datasets: BLESS (Ba-

roni and Lenci, 2011), LEDS (Baroni et al., 2012),

EVAL (Santus et al., 2015), SHWARTZ (Shwartz

et al., 2016), and WBLESS (Weeds et al., 2014),

In addition to positive hypernymy relations, these

datasets include negative samples in the form of

random pairs, co-hyponymy, antonymy, meronymy,

and adjectival relations. For directionality and

graded entailment, we also use BIBLESS (Kiela

et al., 2015) and HYPERLEX (Vulic et al., 2016).

We refer to Roller et al. (2018) for an in-depth dis-

cussion of these datasets. For all models, we use

the identical text corpus and tune hyperparameters

on the validation sets.



3238

Animals Plants Vehicles

All Missing Transitive All Missing Transitive All Missing Transitive

p(x, y) 350.18 512.28 455.27 271.38 393.98 363.73 43.12 82.57 66.10
ppmi(x, y) 350.47 512.28 455.38 271.40 393.98 363.76 43.20 82.57 66.16
sp(x, y) 56.56 77.10 11.22 43.40 64.70 17.88 9.19 26.98 14.84
spmi(x, y) 58.40 102.56 12.37 40.61 71.81 14.80 9.62 17.96 3.03

HypeCones 25.33 37.60 4.37 17.00 31.53 6.36 5.12 10.28 2.74
∆% 56.6 51.2 61.1 58.1 51.3 57.0 44.3 42.8 9.6

Table 3: Reconstruction of Animals, Plants, and Vehicles subtrees in WORDNET.

Table 2 shows the results for all tasks. It can

be seen that our proposed approach provides sub-

stantial gains on the detection and directionality

tasks and, overall, achieves state of the art re-

sults on seven of nine benchmarks. In addition,

our method clearly outperforms other embedding-

based approaches on HYPERLEX, although it can

not fully match the count-based methods. As Roller

et al. (2018) noted, this might be an artifact of the

evaluation metric, as count-based methods benefit

from their sparse-predictions in this setting.

Our method achieves also strong performance

when compared to Poincaré GLOVE on the task

of hypernymy prediction. While Tifrea et al.

(2018) report Spearman’s rho ρ = 0.421 on HY-
PERLEX and accuracy ACC = 0.790 on WBLESS,
our method achieves ρ = 0.59 (HYPERLEX) and
ACC = 0.909 (WBLESS). This illustrates the im-
portance of the distributional constraints that are

provided by Hearst patterns.

An additional benefit is the efficiency of our

embeddings. For all tasks, we have used a 20-

dimensional embedding for HYPECONES, while

the best results for SVD-based methods have been

achieved with 300 dimensions. This reduction in

parameters by over an order of magnitude clearly

highlights the efficiency of hyperbolic embeddings

for representing hierarchical structures.

Reconstruction In the following, we compare

embedding and pattern-based methods on the task

of reconstructing an entire subtree of WORDNET,

i.e., the animals, plants, and vehicles taxonomies,

as proposed by Kozareva and Hovy (2010). In

addition to predicting the existence of single hy-

pernymy relations, this allows us to evaluate the

performance of these models for inferring full tax-

onomies and to perform an ablation for the pre-

diction of missing and transitive relations. We fol-

low previous work (Bordes et al., 2013; Nickel

and Kiela, 2017) and report for each observed

relation (u, v) in WORDNET, its score ranked
against the score of the ground-truth negative

edges. In Table 3, All refers to the ranking of

all edges in the subtree, Missing to edges that are

not included in the Hearst graph G, Transitive to
missing transitive edges in G (i.e. for all edges
{(x, z) : (x, y), (y, z) ∈ E ∧ (x, z) /∈ E}).

It can be seen that our method clearly outper-

forms the SVD and count-based models with a rel-

ative improvement of typically over 40% over the
best non-hyperbolic model. Furthermore, our abla-

tion shows that HYPECONES improves the consis-

tency of the embedding due to its transitivity prop-

erty. For instance, in our Hearst Graph the relation

(male horse, is-a, equine) is missing. However,

since we correctly model that (male horse, is-a,

horse) and (horse, is-a, equine), by transitivity,

we also infer (male horse, is-a, equine), which

SVD fails to do.

5 Conclusion

In this work, we have proposed a new approach

for inferring concept hierarchies from large text

corpora. For this purpose, we combine Hearst pat-

terns with hyperbolic embeddings which allows

us to set appropriate constraints on the distribu-

tional contexts and to improve the consistency in

the embedding space. By computing a joint embed-

ding of all terms that best explains the extracted

Hearst patterns, we can then exploit these prop-

erties for improved hypernymy prediction. The

natural hierarchical structure of hyperbolic space

allows us also to learn very efficient embeddings

that reduce the required dimensionality substan-

tially over SVD-based methods. To improve op-

timization, we have furthermore proposed a new

method to compute entailment cones in the Lorentz

model of hyperbolic space. Experimentally, we

show that our embeddings achieve state-of-the-art

performance on a variety of commonly-used hyper-

nymy benchmarks.



3239

References

Rolf Apweiler, Amos Bairoch, Cathy H Wu, Winona C
Barker, Brigitte Boeckmann, Serenella Ferro, Elis-
abeth Gasteiger, Hongzhan Huang, Rodrigo Lopez,
Michele Magrane, et al. 2004. Uniprot: the univer-
sal protein knowledgebase. Nucleic acids research,
32(suppl 1):D115–D119.

Michael Ashburner, Catherine A Ball, Judith A Blake,
David Botstein, Heather Butler, J Michael Cherry,
Allan P Davis, Kara Dolinski, Selina S Dwight,
Janan T Eppig, et al. 2000. Gene ontology: tool
for the unification of biology. Nature genetics,
25(1):25.

Ben Athiwaratkun and Andrew Gordon Wilson. 2018.
Hierarchical density order embeddings. In Proceed-
ings of the International Conference on Learning
Representations.

Sören Auer, Christian Bizer, Georgi Kobilarov, Jens
Lehmann, Richard Cyganiak, and Zachary Ives.
2007. Dbpedia: A nucleus for a web of open data.
In The semantic web, pages 722–735. Springer.

Marco Baroni, Raffaella Bernardi, Ngoc-Quynh Do,
and Chung-chieh Shan. 2012. Entailment above the
word level in distributional semantics. In Proceed-
ings of the 13th Conference of the European Chap-
ter of the Association for Computational Linguis-
tics, pages 23–32. Association for Computational
Linguistics.

Marco Baroni and Alessandro Lenci. 2011. How we
BLESSed distributional semantic evaluation. In
Proceedings of the 2011 Workshop on GEometrical
Models of Natural Language Semantics, pages 1–10,
Edinburgh, UK.

Tim Berners-Lee, James Hendler, and Ora Lassila.
2001. The semantic web. Scientific american,
284(5):34–43.

Silvere Bonnabel. 2013. Stochastic gradient descent
on Riemannian manifolds. IEEE Trans. Automat.
Contr., 58(9):2217–2229.

Antoine Bordes, Nicolas Usunier, Alberto Garcia-
Duran, Jason Weston, and Oksana Yakhnenko.
2013. Translating embeddings for modeling multi-
relational data. In Advances in neural information
processing systems, pages 2787–2795.

Samuel R. Bowman, Gabor Angeli, Christopher Potts,
and Christopher D. Manning. 2015. A large anno-
tated corpus for learning natural language inference.
In Proceedings of the 2015 Conference on Empiri-
cal Methods in Natural Language Processing, pages
632–642, Lisbon, Portugal. Association for Compu-
tational Linguistics.

Jose Camacho-Collados. 2017. Why we have switched
from building full-fledged taxonomies to simply
detecting hypernymy relations. arXiv preprint
arXiv:1703.04178.

Jose Camacho-Collados, Claudio Delli Bovi, Luis Es-
pinosa Anke, Sergio Oramas, Tommaso Pasini, En-
rico Santus, Vered Shwartz, Roberto Navigli, and
Horacio Saggion. 2018. Semeval-2018 task 9: hy-
pernym discovery. In Proceedings of The 12th Inter-
national Workshop on Semantic Evaluation, pages
712–724.

Haw-Shiuan Chang, Ziyun Wang, Luke Vilnis, and An-
drew McCallum. 2018. Distributional inclusion vec-
tor embedding for unsupervised hypernymy detec-
tion. In Proceedings of the 2018 Conference of the
North American Chapter of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, Volume 1 (Long Papers), pages 485–495, New
Orleans, Louisiana. Association for Computational
Linguistics.

Philipp Cimiano, Andreas Hotho, and Steffen Staab.
2005. Learning concept hierarchies from text cor-
pora using formal concept analysis. Journal of arti-
ficial intelligence research, 24:305–339.

Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan
Roth. 2010. Recognizing textual entailment: Ra-
tional, evaluation and approaches–erratum. Natural
Language Engineering, 16(1):105–105.

Jeffrey Dean and Sanjay Ghemawat. 2004. Mapre-
duce: Simplified data processing on large clusters.
In OSDI’04: Sixth Symposium on Operating System
Design and Implementation, pages 137–150, San
Francisco, CA.

Bhuwan Dhingra, Christopher Shallue, Mohammad
Norouzi, Andrew Dai, and George Dahl. 2018. Em-
bedding text in hyperbolic spaces. In Proceedings
of the Twelfth Workshop on Graph-Based Methods
for Natural Language Processing (TextGraphs-12),
pages 59–69, New Orleans, Louisiana, USA. Asso-
ciation for Computational Linguistics.

Octavian-Eugen Ganea, Gary Bécigneul, and Thomas
Hofmann. 2018. Hyperbolic entailment cones for
learning hierarchical embeddings. arXiv preprint
arXiv:1804.01882.

Maayan Geffet and Ido Dagan. 2005. The distribu-
tional inclusion hypotheses and lexical entailment.
In Proceedings of the 43rd Annual Meeting on As-
sociation for Computational Linguistics, pages 107–
114. Association for Computational Linguistics.

Gene Ontology Consortium. 2016. Expansion of the
gene ontology knowledgebase and resources. Nu-
cleic acids research, 45(D1):D331–D338.

Marti A Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings
of the 14th conference on Computational linguistics-
Volume 2, pages 539–545. Association for Computa-
tional Linguistics.

Johannes Hoffart, Fabian M. Suchanek, Klaus
Berberich, and Gerhard Weikum. 2013. YAGO2: A
spatially and temporally enhanced knowledge base
from wikipedia. Artif. Intell., 194:28–61.



3240

Douwe Kiela, Laura Rimell, Ivan Vulic, and Stephen
Clark. 2015. Exploiting image generality for lexical
entailment detection. In Proceedings of the 53rd An-
nual Meeting of the Association for Computational
Linguistics (ACL 2015), pages 119–124. ACL.

Zornitsa Kozareva and Eduard Hovy. 2010. A
semi-supervised method to learn and construct tax-
onomies using the web. In Proceedings of the
2010 conference on empirical methods in natural
language processing, pages 1110–1118. Association
for Computational Linguistics.

Douglas B. Lenat. 1995. Cyc: a large-scale investment
in knowledge infrastructure. Communications of the
ACM, 38(11):33–38.

Alessandro Lenci and Giulia Benotto. 2012. Identify-
ing hypernyms in distributional semantic spaces. In
Proceedings of the First Joint Conference on Lexical
and Computational Semantics-Volume 1: Proceed-
ings of the main conference and the shared task, and
Volume 2: Proceedings of the Sixth International
Workshop on Semantic Evaluation, pages 75–79. As-
sociation for Computational Linguistics.

Omer Levy, Steffen Remus, Chris Biemann, and Ido
Dagan. 2015. Do supervised distributional meth-
ods really learn lexical inference relations? In Pro-
ceedings of the 2015 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
970–976.

Xiang Li, Luke Vilnis, and Andrew McCallum. 2018.
Improved representation learning for predicting
commonsense ontologies. In International Confer-
ence on Machine Learning Workshop on Deep Struc-
tured Prediction.

Dekang Lin. 1998. An information-theoretic definition
of similarity. In Proceedings of the 14th Interna-
tional Conference on Machine Learning, volume 98,
pages 296–304.

Carolus Linnaeus et al. 1758. Systema naturae, vol. 1.
Systema naturae, Vol. 1.

Alexander Maedche and Steffen Staab. 2001. Ontol-
ogy learning for the semantic web. IEEE Intelligent
systems, 16(2):72–79.

Christopher Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven Bethard, and David McClosky.
2014. The stanford corenlp natural language pro-
cessing toolkit. In Proceedings of 52nd annual meet-
ing of the association for computational linguistics:
system demonstrations, pages 55–60.

George Miller and Christiane Fellbaum. 1998. Word-
net: An electronic lexical database.

George A Miller, Richard Beckwith, Christiane Fell-
baum, Derek Gross, and Katherine J Miller. 1990.
Introduction to wordnet: An on-line lexical database.
International journal of lexicography, 3(4):235–
244.

Ndapandula Nakashole, Gerhard Weikum, and Fabian
Suchanek. 2012. Patty: a taxonomy of relational
patterns with semantic types. In Proceedings of the
2012 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning, pages 1135–1145. Associa-
tion for Computational Linguistics.

Maximilian Nickel and Douwe Kiela. 2017. Poincaré
embeddings for learning hierarchical representa-
tions. In Advances in Neural Information Process-
ing Systems 30, pages 6338–6347. Curran Asso-
ciates, Inc.

Maximilian Nickel and Douwe Kiela. 2018. Learning
continuous hierarchies in the lorentz model of hyper-
bolic geometry. In Proceedings of the Thirty-fifth
International Conference on Machine Learning.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word rep-
resentation. In Proceedings of the 2014 conference
on empirical methods in natural language process-
ing (EMNLP), pages 1532–1543.

Philip Stuart Resnik. 1993. Selection and informa-
tion: a class-based approach to lexical relationships.
IRCS Technical Reports Series, page 200.

FB Rogers. 1963. Medical subject headings. Bulletin
of the Medical Library Association, 51:114–116.

Stephen Roller and Katrin Erk. 2016. Relations such
as hypernymy: Identifying and exploiting hearst pat-
terns in distributional vectors for lexical entailment.
arXiv preprint arXiv:1605.05433.

Stephen Roller, Douwe Kiela, and Maximilian Nickel.
2018. Hearst patterns revisited: Automatic hy-
pernym detection from large text corpora. arXiv
preprint arXiv:1806.03191.

Enrico Santus, Alessandro Lenci, Qin Lu, and
S Schulte im Walde. 2014. Chasing hypernyms in
vector spaces with entropy. In 14th Conference of
the European Chapter of the Association for Com-
putational Linguistics, pages 38–42. EACL (Euro-
pean chapter of the Association for Computational
Linguistics).

Enrico Santus, Frances Yung, Alessandro Lenci, and
Chu-Ren Huang. 2015. Evalution 1.0: an evolving
semantic dataset for training and evaluation of distri-
butional semantic models. In Proceedings of the 4th
Workshop on Linked Data in Linguistics: Resources
and Applications, pages 64–69.

Julian Seitner, Christian Bizer, Kai Eckert, Stefano
Faralli, Robert Meusel, Heiko Paulheim, and Si-
mone Paolo Ponzetto. 2016. A large database of
hypernymy relations extracted from the web. In
Proceedings of the Tenth International Conference
on Language Resources and Evaluation LREC 2016,
Portorož, Slovenia, May 23-28, 2016.



3241

Vered Shwartz, Yoav Goldberg, and Ido Dagan.
2016. Improving hypernymy detection with an inte-
grated path-based and distributional method. arXiv
preprint arXiv:1603.06076.

Vered Shwartz, Enrico Santus, and Dominik
Schlechtweg. 2017. Hypernyms under siege:
Linguistically-motivated artillery for hypernymy
detection. In Proceedings of the 15th Conference
of the European Chapter of the Association for
Computational Linguistics: Volume 1, Long Papers,
pages 65–75, Valencia, Spain. Association for
Computational Linguistics.

GO Simms. 1992. The ICD-10 classification of men-
tal and behavioural disorders: clinical descriptions
and diagnostic guidelines, volume 1. World Health
Organization.

Rion Snow, Daniel Jurafsky, and Andrew Y Ng. 2005.
Learning syntactic patterns for automatic hypernym
discovery. In Advances in neural information pro-
cessing systems, pages 1297–1304.

Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2006.
Semantic taxonomy induction from heterogenous ev-
idence. In Proceedings of the 21st International
Conference on Computational Linguistics and 44th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 801–808. Association for
Computational Linguistics.

Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: a core of semantic knowl-
edge. In Proceedings of the 16th International Con-
ference on World Wide Web, WWW 2007, Banff, Al-
berta, Canada, May 8-12, 2007, pages 697–706.

Alexandru Tifrea, Gary Bécigneul, and Octavian-
Eugen Ganea. 2018. Poincaré glove: Hy-
perbolic word embeddings. arXiv preprint
arXiv:1810.06546.

Paola Velardi, Stefano Faralli, and Roberto Navigli.
2013. Ontolearn reloaded: A graph-based algorithm
for taxonomy induction. Computational Linguistics,
39(3):665–707.

Paola Velardi, Roberto Navigli, Alessandro Cuchiarelli,
and R Neri. 2005. Evaluation of ontolearn, a
methodology for automatic learning of domain on-
tologies. Ontology Learning from Text: Methods,
evaluation and applications, 123(92).

Ivan Vendrov, Ryan Kiros, Sanja Fidler, and Raquel Ur-
tasun. 2016. Order-embeddings of images and lan-
guage. In Proceedings of the International Confer-
ence on Learning Representations (ICLR), volume
abs/1511.06361.

Luke Vilnis, Xiang Li, Shikhar Murty, and Andrew Mc-
Callum. 2018. Probabilistic embedding of knowl-
edge graphs with box lattice measures. In Proceed-
ings of the 56th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), pages 263–272. Association for Computa-
tional Linguistics.

Ivan Vulic, Daniela Gerz, Douwe Kiela, Felix Hill, and
Anna Korhonen. 2016. Hyperlex: A large-scale eval-
uation of graded lexical entailment. arXiv preprint
arXiv:1608.02117.

Julie Weeds, Daoud Clarke, Jeremy Reffin, David Weir,
and Bill Keller. 2014. Learning to distinguish hyper-
nyms and co-hyponyms. In Proceedings of COL-
ING 2014, the 25th International Conference on
Computational Linguistics: Technical Papers, pages
2249–2259. Dublin City University and Association
for Computational Linguistics.

Julie Weeds, David Weir, and Diana McCarthy. 2004.
Characterising measures of lexical distributional
similarity. In Proceedings of the 20th interna-
tional conference on Computational Linguistics,
page 1015. Association for Computational Linguis-
tics.

Wentao Wu, Hongsong Li, Haixun Wang, and Kenny Q
Zhu. 2012. Probase: A probabilistic taxonomy for
text understanding. In Proceedings of the 2012 ACM
SIGMOD International Conference on Management
of Data, pages 481–492. ACM.

Amir R Zamir, Alexander Sax, William Shen,
Leonidas J Guibas, Jitendra Malik, and Silvio
Savarese. 2018. Taskonomy: Disentangling task
transfer learning. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recogni-
tion, pages 3712–3722.


