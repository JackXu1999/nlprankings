




































m_y at SemEval-2019 Task 9: Exploring BERT for Suggestion Mining


Proceedings of the 13th International Workshop on Semantic Evaluation (SemEval-2019), pages 888–892
Minneapolis, Minnesota, USA, June 6–7, 2019. ©2019 Association for Computational Linguistics

888

m y at SemEval-2019 Task 9: Exploring BERT for Suggestion Mining

Masahiro Yamamoto
Sony Corporation, Tokyo, Japan.

Masahiro.A.Yamamoto@sony.com

Toshiyuki Sekiya
Sony Corporation, Tokyo, Japan.

Toshiyuki.Sekiya@sony.com

Abstract

This paper presents our system to the
SemEval-2019 Task 9, Suggestion Mining
from Online Reviews and Forums. The goal
of this task is to extract suggestions such as the
expressions of tips, advice, and recommenda-
tions. We explore Bidirectional Encoder Rep-
resentations from Transformers (BERT) focus-
ing on target domain pre-training in Subtask A
which provides training and test datasets in the
same domain. In Subtask B, the cross domain
suggestion mining task, we apply the idea of
distant supervision. Our system obtained the
third place in Subtask A and the fifth place in
Subtask B, which demonstrates its efficacy of
our approaches.

1 Introduction

In SemEval2019 Task 9, participants are required
to build a model which can classify given sen-
tences into suggestion and non-suggestion classes.
We participate in two sub-tasks: domain specific
suggestion mining task (Subtask A1) and cross do-
main suggestion mining task (Subtask B2). In Sub-
task A, the test dataset belongs to the same domain
as the training and development datasets. These
datasets are extracted from the suggestion forum
for windows platform. In Subtask B, training and
test datasets belong to separate domains. More
specifically, the domain of the training dataset is
entries from the windows forum and that of the
test dataset is hotel reviews. Example sentences
used in these tasks are listed in Table 1. For a de-
scription of these tasks please refer to (Negi et al.,
2019).

Subtask A can be viewed as a binary text classi-
fication task. Recently, pre-training models, such
as OpenAI GPI (Radford et al., 2018) and BERT

1https://competitions.codalab.org/competitions/19955
2https://competitions.codalab.org/competitions/19956

Subtask A (windows platform)
P: xbox dev mode companion work ipv6 please...
N: I do not want to convert MSIs.
Subtask B (hotel review)
P: If you can, upgrade to an Ocean Front Room.
N: it doesn’t have a very clean look.

Table 1: Example sentences in the test dataset. P means
a positive sentence, i.e. suggestion sentence and N de-
notes a negative sentence, i.e. non-suggestion sentence.

(Devlin et al., 2018), have gained much attention
with their ability to improve a number of down-
stream tasks. These models are pre-trained using
unlabeled corpora and then fine-tuned on labeled
datasets. We apply BERT to this task because it
has achieved state-of-the-art performance in sev-
eral text classification tasks. The difference to the
original BERT model is that we further pre-train
BERT model using an unlabeled corpus related to
this domain. More concretely, we extract docu-
ments from a windows forum and run additional
steps of pre-training using these documents, start-
ing from the pre-trained BERT model.

In Subtask B, we apply the idea of distant
supervision which has been firstly proposed by
(Mintz et al., 2009). Distant supervision is a
weakly supervised learning framework which tries
to automatically generate noisy training examples.
Specifically, we use the rule based system which is
provided by the task organizer for creating a noisy
training dataset and train the model based on them.

Our system significantly outperforms baseline
methods on two subtasks. These results demon-
strate its efficacy of our approaches, target domain
pre-training in Subtask A and distant supervision
in Subtask B.



889

2 System Description

Our model is built using a recent development of
pre-training model, BERT. In the following, we
describe details of this model first and then explain
our systems in Subtask A and Subtask B.

2.1 BERT

BERT, proposed by (Devlin et al., 2018), has been
shown to improve several tasks such as sentiment
classification, calculating semantic textual similar-
ity task, and recognizing textual entailment task.
This model consists of several Transformer mod-
els (Vaswani et al., 2017) whose parameters are
pre-trained on unlabeled corpora, Wikipedia and
BooksCorpus (Zhu et al., 2015). Pre-training con-
sists of two tasks, masked language modeling and
next sentence prediction, and trained models of
these unsupervised tasks are available.3

For the classification task, we added one layer to
output predictions, suggestion or non-suggestion.
These model parameters were then fine-tuned
based on the labeled training dataset.

2.2 Subtask A

Subtask A is a classical document classifica-
tion task, where training/development/test datasets
consist of the same domain. The data has been col-
lected from feedback posts on universal windows
platform. We mainly use BERT in this task, how-
ever, in the following we describe several tech-
niques to improve the score.

2.2.1 Target Domain Pre-training
The major difference between BERT and our sys-
tem is the target domain pre-training. Typically,
BERT training consists of two parts: pre-training
on the general domain corpus and fine-tuning on
the target task. On the other hand, our system
consists of three training steps: pre-training on
the general domain corpus, pre-training on the
target domain corpus, and fine-tuning on the tar-
get task. More concretely, we further pre-train
the model using a task specific unlabeled corpus
scraped from the universal windows platform de-
veloper feedback site.4 These documents are split
into sentences and the model is then trained on two
unsupervised tasks (i.e. masked language model-
ing and next sentence prediction). We should note

3https://github.com/google-research/bert
4https://wpdev.uservoice.com/forums/110705-universal-

windows-platform

here that we use officially provided pre-trained pa-
rameters as initial model parameters and further
train the model based on target domain documents
to obtain better network parameters.

This target domain pre-training can be consid-
ered as a similar framework of Universal Lan-
guage Model Fine-Tuning (ULMFiT), proposed
by (Howard and Ruder, 2018). In their frame-
work, the model is firstly trained on general do-
main corpus to capture general features. In ad-
dition, the model is further trained on target task
data to learn task specific features, and fine-tuned
on the target task. This procedure is similar to our
system. In other words, our work can be viewed
as the extension of BERT model by using the idea
of ULMFiT.

2.2.2 Model Averaging
(Reimers and Gurevych, 2017) showed that train-
ing deep neural network is sensitive to the initial
weights and (Che et al., 2018) showed the effec-
tiveness of ensembling models trained with dif-
ferent initialization. Furthermore, (Devlin et al.,
2018) empirically showed that ensembling BERT
models trained with different pre-training check-
point leads to performance improvement. We fol-
low this work and train three models with differ-
ent pre-training checkpoint, then ensemble these
models by simply averaging output scores.

2.3 Subtask B

Subtask B is cross-domain suggestion mining,
where train and development datasets belong to
windows platform domain, while the test dataset
belongs to the hotel review domain. In this task,
we do not use the datasets provided in Subtask
A, because we find that models trained on these
datasets tend to have poor performance (see also
Sec. 3.2.2). We instead apply the paradigm of dis-
tant supervision.

Distant supervision is a framework to generate
noisy annotated data automatically and use them
as a training dataset. This idea has been firstly pro-
posed by (Mintz et al., 2009) and well studied in
the field of relation extraction (Riedel et al., 2010;
Hoffmann et al., 2011).

2.3.1 Rule Based Labeling
For distant supervision, the initial labeling method
is needed. In this work, we make use of the of-
ficially provided baseline method as an initial la-
beling tool. This system is based on a rule based



890

Subtask A Subtask B
Train Dev Test Train Dev Test

Suggestions 2,085 296 87 - 404 348
Non Suggestions 6,415 296 746 - 404 476

All 8,500 592 833 - 808 824

Table 2: Statistics of datasets.

method. For example, if a specific word such as
”suggest” is in the sentence, then the sentence is
predicted as a suggestion sentence. For more de-
tails, please see the code in the official repository.5

2.3.2 Model Training Procedure
After the labeled corpus is generated, we trained
a model using this corpus. Here, we do not label
all of the unlabeled sentences via the rule based
system. Instead, we split the sentences into sev-
eral pieces and label one of them by using the rule
based system. A model is trained on the noisy la-
beled dataset and we label another piece through
the trained model. We apply this procedure itera-
tively until the model is trained on the last piece.
More specifically, our training procedure can be
summarized as follows:

1. We prepare the unlabeled hotel review corpus
and split it into N pieces (corpus C1, C2, C3,
..., CN ).

2. We apply the baseline method which is pro-
vided by the task organizer to the corpus C1
and treat predicted labels as true labels.

3. The model is then trained using the labeled
corpus C ′1.

4. We apply the trained model to the corpus C2
and treat predicted labels as true labels.

5. A new model is trained using the labeled cor-
pus C ′2 and labels of sentences in C3 are pre-
dicted by this model.

6. We iteratively apply the above procedure un-
til the model is trained on the corpus CN .

3 Experiments

Table 2 shows the statistics of datasets which are
used in Subtask A and Subtask B. These datasets
are available at the official repository.6 For evalu-
ating systems, F1 score for the positive class, i.e.
the suggestion class, is employed.

5https://github.com/Semeval2019Task9/Subtask-
B/blob/master/semeval-task9-baseline.py

6https://github.com/Semeval2019Task9

System Dev F1 Test F1
Baseline 0.721 0.267
BERT BASE (Single) 0.845 0.731
BERT BASE (Sgl. + Tgt.) 0.866 0.755
BERT LARGE (Single) 0.867 0.737
BERT LARGE (Sgl. + Tgt.) 0.882 0.759
BERT LARGE (Ens. + Tgt.) 0.890 0.776

Table 3: Results of Subtask A. Single or Sgl. denotes
the single model and Ens. means the ensembled mod-
els. Tgt. denotes the pre-training on the target domain
corpus.

3.1 Subtask A

3.1.1 Settings
We employed the official BERT model. We used
both BASE model which has 12 Transformer lay-
ers, 12 self-attention heads, and 768 hidden size,
and LARGE model which has 24 Transformer
layers, 16 self-attention heads, and 1024 hidden
size. There are 110 million parameters in total
in BASE model and 340 million parameters in
LARGE model.

As for the target domain pre-training, we ob-
tained the windows review corpus and split it into
sentences using NLTK.7 The number of scraped
documents is 2,325 and we used these documents
as the unlabeled corpus for further pre-training the
BERT model.

3.1.2 Results and Discussions
Table 3 shows the results of the experiment. Here,
the baseline method is a rule based method as ex-
plained in Sec. 2.3.1. From Table 3, we can con-
clude the following four things:

First, BERT LARGE model outperformed
BERT BASE model despite of the small size of
the dataset. In general, it has been known that in-
creasing the model size leads to an improvement
on large scale tasks such as machine translation,
and this does not be applied to small scale tasks

7https://www.nltk.org/



891

except for (Devlin et al., 2018). They showed a
similar tendency in another small scale task. These
results demonstrate that large size models improve
results not only on large scale tasks but also on
small scale tasks, if the model has been well pre-
trained.

Second, the effect of the target domain pre-
training is not trivial. The improvement can be
observed in both BASE and LARGE model. In
BASE model, the F1 score is pushed from 0.845 to
0.866 in the development dataset and from 0.731
to 0.755 in the test dataset. In LARGE model, the
score is improved from 0.867 to 0.882 in the de-
velopment dataset and from 0.737 to 0.759 in the
test dataset. These results show that better models
are produced by target domain pre-training even if
we do not have large, domain-specific documents.

Third, ensembling models leads to further per-
formance improvement.

Fourth, Test F1 score is much lower than Dev F1
score. This has also been observed by other teams.
In concrete, many teams have achieved over 0.850
F1 score on the development dataset, however, no
team has achieved over 0.800 F1 score on the test
dataset. It might have something to do with the
small size of the development dataset size or the
difference in the label distribution.

3.2 Subtask B

3.2.1 Settings

In Subtask B, we got unlabeled hotel review docu-
ments provided by (Wachsmuth et al., 2014).8 We
split documents into sentences using the NLTK
package and randomly extracted 500,000 sen-
tences. These sentences were further split into 5
pieces.

We firstly ran the baseline method and automat-
ically labeled 100,000 sentences. These sentences
were used as a training dataset to train the machine
learning model. We used the BERT BASE model
and set the default hyperparameters except for the
training epochs. To avoid over-fitting, we trained
the model for only one epoch.

As described in Sec. 2.3, another 100,000
sentences were automatically labeled using the
trained model and we trained a new model using
this labeled data. We iteratively applied this pro-
cedure and submitted results predicted by the final
model.

8http://argumentation.bplaced.net/arguana/data

System Dev F1 Test F1
Baseline 0.774 0.732
BERT BASE (windows) 0.308 0.419
BERT BASE (1st model) 0.800 0.785
BERT BASE (5th model) 0.817 0.793

Table 4: Results of Subtask B. BERT BASE (win-
dows) is the model trained on windows corpus pro-
vided in Subtask A.

3.2.2 Results and Discussions
Table 4 shows the results of the experiment.
As you can see in Table 4, BERT BASE (win-
dows), the model trained on the other domain cor-
pus, shows poor performance while the baseline
method has achieved a much better score. This
motivated us to apply the idea of distant supervi-
sion.

The model based on distant supervision signif-
icantly outperformed the baseline model. This re-
sult shows that the distant supervision idea can be
applied successfully in the cross domain sugges-
tion mining task. Furthermore, we can see that
our iterative approach leads to more performance
improvement (from 0.800 to 0.817 in the develop-
ment dataset and 0.785 to 0.793 in the test dataset).
We currently do not know why the F1 score has
been improved, however, one interpretation could
be that our iterative framework avoids over-fitting
to the one model and learns more general decision
boundaries. A detailed study of this effect is future
work.

4 Conclusion

This paper explains our submission to Se-
mEval2019 Task 9, Subtask A and Subtask B. We
explored BERT models focusing on the target do-
main pre-training in Subtask A and the idea of
the distant supervision in Subtask B. Our approach
obtained the third place in Subtask A and the fifth
place in Subtask B.

In the future, we will further investigate the ef-
fect of these approaches in other tasks.

References

Wanxiang Che, Yijia Liu, Yuxuan Wang, Bo Zheng,
and Ting Liu. 2018. Towards better ud pars-
ing: Deep contextualized word embeddings, ensem-
ble, and treebank concatenation. arXiv preprint
arXiv:1807.03121.



892

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. arXiv preprint arXiv:1810.04805.

Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke
Zettlemoyer, and Daniel S Weld. 2011. Knowledge-
based weak supervision for information extraction
of overlapping relations. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies-
Volume 1, pages 541–550. Association for Compu-
tational Linguistics.

Jeremy Howard and Sebastian Ruder. 2018. Universal
language model fine-tuning for text classification. In
Proceedings of the 56th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), volume 1, pages 328–339.

Mike Mintz, Steven Bills, Rion Snow, and Dan Juraf-
sky. 2009. Distant supervision for relation extrac-
tion without labeled data. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Vol-
ume 2-Volume 2, pages 1003–1011. Association for
Computational Linguistics.

Sapna Negi, Tobias Daudert, and Paul Buitelaar. 2019.
Semeval-2019 task 9: Suggestion mining from on-
line reviews and forums. In Proceedings of the
13th International Workshop on Semantic Evalua-
tion (SemEval-2019).

Alec Radford, Karthik Narasimhan, Time Salimans,
and Ilya Sutskever. 2018. Improving language un-
derstanding with unsupervised learning. Technical
report, OpenAI.

Nils Reimers and Iryna Gurevych. 2017. Reporting
score distributions makes a difference: Performance
study of lstm-networks for sequence tagging. arXiv
preprint arXiv:1707.09861.

Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions with-
out labeled text. In Joint European Conference
on Machine Learning and Knowledge Discovery in
Databases, pages 148–163. Springer.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems, pages 5998–6008.

Henning Wachsmuth, Martin Trenkmann, Benno Stein,
Gregor Engels, and Tsvetomira Palakarska. 2014. A
review corpus for argumentation analysis. In In-
ternational Conference on Intelligent Text Process-
ing and Computational Linguistics, pages 115–127.
Springer.

Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-
dinov, Raquel Urtasun, Antonio Torralba, and Sanja
Fidler. 2015. Aligning books and movies: Towards
story-like visual explanations by watching movies
and reading books. In Proceedings of the IEEE
international conference on computer vision, pages
19–27.


