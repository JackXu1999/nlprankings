

















































Multi Sense Embeddings from Topic Models

Shobhit Jain
Amazon Web Services

jainshob@amazon.com

Sravan Babu Bodapati
Amazon Web Services

sravanb@amazon.com

Ramesh Nallapati
Amazon Web Services

rnallapa@amazon.com

Abstract
Distributed word embeddings have yielded
state-of-the-art performance in many NLP
tasks, mainly due to their success in captur-
ing useful semantic information. These repre-
sentations assign only a single vector to each
word whereas a large number of words are pol-
ysemous (i.e., have multiple meanings). In
this work, we approach this critical problem
in lexical semantics, namely that of represent-
ing various senses of polysemous words in
vector spaces. We propose a topic modeling
based skip-gram approach for learning multi-
prototype word embeddings. We also intro-
duce a method to prune the embeddings de-
termined by the probabilistic representation of
the word in each topic. We use our embed-
dings to show that they can capture the context
and word similarity strongly and outperform
various state-of-the-art implementations.

1 Introduction

Representing words as dense, low dimensional em-
beddings (Mikolov et al., 2013a,b; Pennington
et al., 2014) allow the representations to capture
useful syntactic & semantic information making
them useful in downstream Natural Language Pro-
cessing tasks. However, these embedding models
ignore the lexical ambiguity among different mean-
ings of the same word. They assign only a single
vector representative of all the different meanings
of a word. In this work, we attempt to address
this problem by capturing the multiple senses of a
word using the global semantics of the document
in which the word appears. Li and Jurafsky (2015)
indicated that such sense specific vectors improve
the performance of applications related to semantic
understanding, such as Named Entity Recognition,
Part-Of-Speech tagging.

In this work, we first train a topic model on our
corpus to extract the topic distribution for each doc-
ument. We treat these extracted topics as a heuristic

to model word senses. We hypothesize that these
word senses correlate quite well with the human
notion of word senses, and validate it through our
rigorous experiments as we demonstrate in our re-
sults section. We then use this topic distribution
to train sense-specific word embeddings for each
sense. We train these embeddings by weighing
the learning procedure in proportion to the corre-
sponding topic representation for each document.
However, a word need not strongly correlate with
each of these extracted senses. To address it, we
propose a variant of this model which restricts the
learning to only those embeddings where the word
has a strong correlation with the topic extracted,
i.e., high p(word|topic).

The major contributions of our work are (i) train-
ing multi-sense word embeddings based on struc-
tured skip gram using topic models as a precursor
(ii) non-parametric approach which prunes the em-
beddings to capture variability in the number of
word senses.

2 Prior Work

Recently, learning multi-sense word embedding
models has been an active area of research and
has gained a lot of interest. TF-IDF (Reisinger
and Mooney, 2010), SaSA (Wu and Giles, 2015),
MSSG (Neelakantan et al., 2015), Huang et al.
(2012) used cluster-based techniques to cluster the
context of a word and comprehend word senses
from the cluster centroids. Tian et al. (2014) pro-
posed to use EM-based probabilistic clustering to
assign word senses. Li and Jurafsky (2015) used
Chinese Restaurant Process to model the word
senses. All these techniques are just local con-
text based and thus ignore the essential correlations
amongst words and phrases in a broader document-
level context. In contrast, our method enriches the
embeddings with the document level information,



capturing word interactions in a broader document-
level context.

AutoExtend (Rothe and Schütze, 2015), Sensem-
bed (Iacobacci et al., 2015), Nasari (Camacho-
Collados et al., 2016), Deconf (Pilehvar and Col-
lier, 2016), Chen et al. (2014); Jauhar et al. (2015);
Pelevina et al. (2017) have used multi-step ap-
proach to learn sense & word embeddings but re-
quire an external lexical database like WordNet
to achieve it. SW2V(Mancini et al., 2016) train
the embeddings in a single joint training phase.
Nonetheless, all these methods assign same weight
to every sense of a word, ignoring the extent to
which each sense is associated with it’s context.

MSWE (Nguyen et al., 2017) trained sense and
word embeddings separately, with sense specific
word embeddings computed as a weighted sum of
the two, where the weights are calculated using
topic modeling. Similarly, Liu et al. (2015a,b);
Cheng et al. (2015); Zhang and Zhong (2016)
used skip-gram based approach to obtain separate
word & topic embeddings. Lau et al. (2013) also
used topic models to distinguish between different
senses of a word. All these techniques express the
sense-specific word representation as a function of
word & sense embeddings which essentially be-
longs to two different domains. Our work trains
more robust compositional word embeddings for-
mulated as a weighted sum of sense specific word
embeddings, thus, taking into consideration all the
different word senses while operating in the same
vector space.

More recent techniques like ELMo (Peters et al.,
2018), BERT (Devlin et al., 2018) compute the
contextual representations of a word based on the
sentence in which the word appears, whereas, our
method yields precomputed embeddings for each
sense of a word within the same vector space.

3 Multi Sense Embeddings Model

3.1 Topic Modeling

Mixed membership models like topic models allow
us to discover topics that occur in a collection of
documents. A topic is defined as a distribution over
words and consists of cluster of words that occur
frequently. This formulation benefits us in infer-
ring the probability distribution over different con-
texts(topics) the word can occur in. Latent Dirichlet
Allocation(LDA) (Blei et al., 2003) is a topic mod-
eling technique that assigns multiple topics in dif-
ferent proportions to each document along with the

Figure 1: We feed our model a central word as input
and predict the context word from it. First we learn
separate word embeddings corresponding to each topic,
Ewt,zi . Each of these embeddings are then multiplied
with global word embeddings, vg(w), weighed in pro-
portion to the topic distribution of the document from
which the words have been chosen, and summed up to
predict the neighboring context word.

probability distribution over words for each of the
topics. Topic models based on Gibbs Sampling (Ge-
man and Geman, 1987) achieve this by computing
the posterior for a word based on the topic pro-
portion at document level coupled with how often
the word appears together with other words in the
topic. We use Gibbs Sampling based approach to
compute the topic distribution for each document.
We use the LDA implementation from MALLET
topic modeling toolkit (McCallum, 2002) for our
experiments.

3.2 Embeddings from Topic Models (ETMo)

In this section we present our baseline approach for
training sense-specific word embeddings. We for-
mulate our approach as follows. Let Ew ∈ Rk×n
represent the embedding matrix for word w, where
k is the number of topics(treated as number of word
senses) and n is the dimensionality of embeddings.
We represent the embedding of word w correspond-
ing to topic zi as Ew,zi . Let vg(w) be the output
vector representation for word w, which is shared
across senses, and enforces the embeddings of dif-
ferent senses to be within the same vector space.

We introduce a latent variable z, representing
the topic dimension, to model separate embedding
for each topic. Inline with the skip-gram(Mikolov
et al., 2013a) approach, we maximize the proba-
bility of predicting the context word wt+j , given a



central word wt for a document d as:

p(wt+j |wt, d) =
k∑

i=1

p(wt+j |wt, zi, d) ∗ p(zi|d)
(1)

p(zi|d) represents the topic distribution of the doc-
ument d, obtained from the trained topic model.
In the above equation, we reasonably make the as-
sumption, p(zi|wt, d) = p(zi|d), owing to the fact
that the topic distribution is computed at the doc-
ument level. Using Negative Sampling (Mikolov
et al., 2013b), we reduce the first term in the above
equation as:

p(wt+j |wt, zi) =

σ(Ewt,zi ∗ vg(wt+j)) +
∑
w∈S

σ(− Ewt,zi ∗ vg(w))

(2)

Formally, given a large corpus of docu-
ments, with size D, having a words sequence
w1, w2, ..., wNd−1, wNd , where Nd is the number
of words in document d, skip-window size c, num-
ber of topics k, the objective is to maximize the
following log likelihood:

L =
D∑

d=1

Nd∑
t=1

c∑
j=−c

log p(wt+j |wt, d) =

D∑
d=1

Nd∑
t=1

c∑
j=−c

log
k∑

i=1

p(wt+j |wt, zi, d) ∗ p(zi|d)

(3)

As shown in Figure 1, we use a neural network
architecture to compute the log likelihood. We
feed the central word, in its BoW representation,
as input to the model and compute the probability
of the context word. Refer to the figure for detailed
explanation.

During inference, we first compute the topic dis-
tribution for the given document, p(zi|d), using our
pre-trained topic model. Finally, for a document d
and for each word w, we infer the word embedding
as:

vw,d =

k∑
i=1

p(zi|d) ∗ Ew,zi (4)

Model avgSim globalSim
GloVe - 63.2
(Pennington et al., 2014)
Huang et al. (2012) 64.2 71.3
csmRNN - 64.58
(Luong et al., 2013)
GC-SINGLE 62.3 -
(Jauhar et al., 2015)
NP-MSSG 69.1 68.6
(Neelakantan et al., 2015)
MSWE-I - 72.40
(Nguyen et al., 2017)
Gensense 54.0 -
(Lee et al., 2018)
ETMo (Ours) 68.5 68.2
ETMo + NP (Ours) 69.3 69.1

Table 1: Spearman’s correlation ρ× 100 on WS-353

3.3 ETMo + Non-parametric

In this section, we substantiate the flaws in our
baseline approach and present our non-parametric
method to learn the embeddings.
Our previous approach assigns an embedding to
every word corresponding to each topic. As one
can see, this method would undesirably accumu-
late a fair amount of noisy updates to those word
embeddings that have minimal representation in a
topic. Hence, we extend our model by exploiting
the information from topic models to learn only
those embeddings where the word has a strong cor-
relation with the topic.
In particular, we train only those embedding Ewt,zi
such that p(wt|zi) > pthres, where pthres is chosen
empirically, which we will explain later. For the
words where none of the senses satisfy the above
condition (might be the case for some monose-
mous words), we chose the embedding Ewt,x to be
trained, such that x = argmaxzi p(wt|zi).

4 Experimental Setup

We use the English Wikipedia corpus dump
(Shaoul and Westbury, 2010) for training both,
topic models and embedding models. Though
many previous research works have used a larger
training corpus, but for a fair comparison, we only
compare our results with those works which have
used the same corpus. We could also improve ob-
tained results by using a larger training corpus, but
this is not central point of our paper. The main aim
of our work is to compute sense specific embed-



Model avgSim avgSimC
TF-IDF 60.4 -
Huang et al. (2012) 62.8 65.7
Tian et al. (2014) - 65.4
Chen et al. (2014) 66.2 68.9
Cheng et al. (2015) - 65.9
GC-MULTI - 65.9
(Jauhar et al., 2015)
SENSEMBED 62.4 -
(Iacobacci et al., 2015)
SaSA - 66.4
(Wu and Giles, 2015)
TWE-I (Liu et al., 2015b) - 68.1
NP-MSSG 67.2 69.3
(Neelakantan et al., 2015)
SG+Greedy - 69.1
(Li and Jurafsky, 2015)
MSWE 66.7 66.6
(Nguyen et al., 2017)
ETMo (Ours) 65.4 65.8
ETMo + NP (Ours) 67.5 69.1

Table 2: Spearman’s correlation ρ× 100 on SCWS

dings for a word using topic models and demon-
strate the strength of our model empirically.

The raw dataset consists of nearly 3.2 million
documents and 1 billion tokens. Training topic
models on such a large and diverse corpus helps in
obtaining clearly demarcated senses for each topic.

To tune the hyper parameters of our neural net-
work model, we sample 20% of our corpus as val-
idation data and chose those parameters that give
the lowest validation loss. Later, we use these pa-
rameter values for training on the entire corpus. For
all our experiments, we use a skip-window of size
2, 8 negative samples, embeddings of dimension-
ality 200, and fix the number of topics to 10. A
detailed analysis on how we chose the number of
topics, using perplexity score, can be found later in
the analysis section. We initialize the embeddings
using pre-trained GloVe embeddings to ensure all
our target embeddings are in the same vector space.
We choose the value for pthres as 1e-4 and give an
analysis on how we chose the parameter value in
the results section.

5 Results

We evaluate our model on two tasks, namely, word
similarity and word analogy. For word similarity
evaluation, we evaluate our embeddings on stan-
dard word similarity benchmark datasets including
WS-353 (Finkelstein et al., 2001) & SCWS-2003
(Huang et al., 2012). WS-353 includes 353 pairs

Model Accuracy(%)
Word2Vec 67
Huang et al. (2012) 12
Neelakantan et al. (2015) 64
ETMo (Ours) 67
ETMo + NP (Ours) 66

Table 3: Results on Word Analogy task

of words and a human judgment score of the simi-
larity measure between the two words. Similarly,
SCWS-2003 consists of 2003 pairs of words, but,
given with a context.

We note that our embeddings can capture only
those senses that are represented by the extracted
topics, and due to the restricted number of topics
extracted, they might not be able to capture all the
senses for a word. However, at a specific number
of topics, our model is effective in capturing var-
ious senses of words in standard word similarity
datasets. We demonstrate this effect qualitatively
and quantitatively in this section.

For each of the datasets, we report the Spearman
correlation between the human judgment score and
model’s similarity score computed between two
words w and w′. We follow Reisinger and Mooney
(2010) to compute the following similarity mea-
sures. For a pair of words w and w′ and given
their respective contexts c and c′, we represent the
cosine distance between the embeddings Ew,i and
Ew′,j as d(Ew,i, Ew′,j).

globalSim = d(vg(w), )

avgSim =
1

N1 ∗N2

N1∑
i=1

N2∑
j=1

d(Ew,i, Ew′,j)

avgSimC =

N1∑
i=1

N2∑
j=1

p(zi|c) ∗ p(zj |c′)∗

d(Ew,i, Ew′,j)

(5)

N1 and N2 are chosen such that they satisfy
p(wt|zi) > pthres. vg(w) represents the output
vector for word w, as mentioned in section 3.2. We
infer the probabilities, p(zi|c) & p(zj |c′) using our
pre-trained topic model.

In contrast to our model, methods such as ELMo,
BERT requires a document context to compute an
embedding, which makes it unfair to compare on
avgSim metric since it doesn’t take any context
into account. Additionally, ELMo gives a set of 3
different embeddings making it unclear to compare



on the avgSimC metric as well.

5.1 Quantitative Results

We present the results of our approach in Tables 1
and 2. A higher Spearman’s correlation translates
to a better model.

As can be seen in Table 1, our non-parametric
approach clearly outperforms other multi-sense em-
beddings models using the avgSim metric on WS-
353. Further, though our method focusses on sense
specific embeddings and not on the global word
embeddings, for the purpose of completeness, we
also report our results on the globalSim metric. Us-
ing globalSim, expectedly we obtain slightly lower
results since globalSim is more suited for global
word embeddings.

In Table 2, we compare our models on the SCWS
dataset. Using avgSim metric, our model obtain
state-of-the-art results, outperforming other embed-
dings model. Using the avgSimC metric, we pro-
duce competitive results and perform better than
most of the models, including Nguyen et al. (2017)
which also uses topic models.

These superior results indicate the usefulness of
our method to accurately capture word represen-
tations that can take into account different word
senses. Additionally, our non-parametric approach
consistently outperforms our baseline ETMo ap-
proach, validating our hypothesis to threshold the
topics.

We also evaluate our model on the word anal-
ogy task (Mikolov et al., 2013a). 1 Our answer is
correct if this word matches the correct word given
in the dataset. As can seen in Table 3, our ETMo
approach obtains similar results as the baseline
word2vec model, and we beat other implementa-
tions.

5.2 Qualitative Comparison

We show a qualitative comparison of some polyse-
mous words in Table 4, with the nearest neighbors
of words in the table, for Glove embeddings and
the embeddings trained from our model. For each
of the words in Table 4, we can clearly see that
the different senses of words are being effectively
captured by our model whereas Glove embeddings
could only capture most frequently used meaning

1 The word analogy task aims to answer the question of the
form: a is to b as c is to ?. To answer the question, we compute
the word vector nearest to ‘vg(b) − vg(a) + vg(c)’, where
vg(w) represents the output vector for word w, as mentioned
in section 3.2.

for the word. Moreover, each of these senses can
be easily correlated with the topic that these embed-
dings correspond to which can be seen from Table
5. Consider the word Play. The first sense for
play corresponds to Music (topic 2). The second
embedding corresponds to Sports (topic 7).

An interesting qualitative result is shown for the
word Network. The nearest neighbors to Glove
embeddings show that they are only able to capture
one meaning which is in the subject of Television
Network. However, our model is able to capture 3
different meanings for the word quite powerfully.
The first one, which corresponds to topic 2, occurs
in the context of Television Network which is the
sense Glove was able to capture. The second sense,
which corresponds to topic 5, occurs in the context
of Computer Networks. The third sense, which
corresponds to topic 6, remarkably relates to the
context Geography.

5.3 Number of Topics Analysis
In this section, we perform a study on choosing
the right number of topics(k) in Table 6. Here,
topic uniqueness refers to the proportion of unique
words in a topic, computed over the top words in
the vocabulary. Higher the topic uniqueness score,
more distinct are the obtained topics. We compute
the Spearman correlation on the avgSim metric us-
ing the word pairs from RG-65 (Rubenstein and
Goodenough, 1965). With k = 10, we obtained a
topic uniqueness of 32.23, which dropped to 27.12
for k=20 topics. Thus increasing the number of
topics increases overlap which harms our model
as the topic weight gets divided while training the
embeddings. This effect can be clearly seen in the
correlation coefficient which drops from 68.5 to
66.9 for 10 & 20 topics respectively. Using k=5
improved the topic uniqueness score to 34.05, but
the perplexity score (Blei et al., 2003) reduced, in-
dicating that the topic model requires more degrees
of freedom to fit the corpus. We also observed not
very distinct topics at k=5 (i.e. a topic could be
mixture of sports and history), resulting in reduced
correlation coefficient of 67.1.

5.4 Threshold Parameter Analysis
In this section, we study the effect of pthres on the
model performance. We tune its value by compar-
ing the Spearman correlation on the avgSim metric
using the word pairs from RG-65 (Rubenstein and
Goodenough, 1965). However, we hypothesize that
the threshold parameter depends only on the output



Word Topic # Nearest Neighbors
Glove playing, played, plays, game, players, player, match, matches, games

play 2 played. performance, musical, performed, plays, stage, release, song, work, time
7 season, players, played, one, game, first, football, teams, last, year, clubs
Glove band, punk, pop, bands, album, rocks, music, indie, singer, albums, songs, rockers

rock 2 metal, pop, punk, members, jazz, alternative, indie, folk, band, hard, recorded, blues
6 island, point, valley, hill, large, creek, granite, railroad, river, lake
Glove banks, banking, central, credit, bankers, financial, investment, lending, citibank

bank 6 river, tributary, flows, valley, side, banks, mississippi, south, north, mouth, branch
8 company, established, central, first, group, one, investment, organisation, development
Glove plants, factory, facility, flowering, produce, reactor, factories, production

plant 1 plants, bird, genus, frog, rodent, flowering, fish, species, tree, endemic, asteraceae
5 design, plants, modern, power, process, technology, standard, substance, production
Glove wars, conflict, battle, civil, military, invasion, forces, fought, fighting, wartime

war 4 combat, first, world, army, served, american, battle, civil, outbreak, forces
7 series, championship, cup, fifa, champion, chess, records, wrestling, championships
Glove cable, channel, television, broadcast, internet, stations, programming, radio

network 2 series, program, shows, bbc, broadcast, station, channel, aired, nbc, radio, episode
5 data, information, computer, system, applications, technology, control, standard, design
6 light, station, car, stations, railway, commuter, lines, rail, trains, commute

Table 4: Nearest neighbours of some polysemous words for Glove, and for each sense identified by our algorithm,
based on the cosine similarility. We take only those senses corresponding to topics where p(wt|j) > pthres.

TOPIC # TOPIC KEYS TOPIC NAME

1
species south india island north found small indian region family

district water large east long spanish central village west area Agriculture

2
film music album released band series show song time television

single songs live rock records video release appeared episode films Music/Television

3
party government state states united president law member general
court house election served political elected national born council Politics

4
war air army force british battle service aircraft japanese forces

world military time ship fire navy command attack september car Military

5
system formula number time called form data systems process high

energy type common set space based power similar standard Technology

6
city age county area population town located years north river
south west station park line road district village income living Geography

7
team season game league played club football games world year

career player born time final cup play championship national Sports

8
school university college company students education, public program

business national research development services million service Education

9
church book work life published century time english works

art people world books language great written god early death called Religion

10
french german france war king germany century russian part

italian son chinese empire soviet republic born died emperor paris History

Table 5: The top words for each topics according to topic modeling



# of topics uniqueness perplexity ρ× 100
5 34.05 9.88 67.1
10 32.23 9.78 68.5
15 29.57 9.70 67.8
20 27.12 9.65 66.9

Table 6: effect of number of topics on Spearman corre-
lation on 50 word pairs from WordSim-353

pthres ρ× 100 senses captured for network
1e-3 68.3 television, IT
1e-4 69.1 television, IT, transportation
1e-5 68.4 mixed senses

Table 7: Spearman’s correlation ρ × 100 on 50 word
pairs from WS-353 and the word senses captured for
network. The word senses are adjudged qualitatively.

of topic modeling, particularly p(word|topic), and
thus is independent of the this chosen subset, as can
be seen in the results on other datasets. In Table
7, we can see that the optimal value for pthres is
1e-4 for the non-parametric model at which it can
strongly differentiate between the different senses
for network. A higher threshold value of 1e-3 cap-
tures a fewer number of senses. A lower threshold
value of 1e-5 allows training of more than the ac-
tual number of true senses leading to noisy updates,
thus becoming ineffective in capturing any sense.
The corresponding lower correlation coefficients in
Table 7 confirm these effects quantitatively.

6 Conclusion & Future Work

In this work, we presented our approach to learn
word embeddings to capture the different senses
of a word. Unlike previous sense-based models,
our model exploits knowledge from topic modeling
to induce mixture weights in structured skip-gram
approach, for learning sense specific representa-
tions. We extend this model further by pruning the
embeddings conditioned on the number of word
senses. Finally, we showed our model achieves
state-of-the-art results on word similarity tasks, and
demonstrated the strength of our model in captur-
ing multiple word senses qualitatively. Future work
should aim towards using these embeddings for
downstream tasks.

References
David M Blei, Andrew Y Ng, and Michael I Jordan.

2003. Latent dirichlet allocation. Journal of ma-
chine Learning research, 3(Jan):993–1022.

José Camacho-Collados, Mohammad Taher Pilehvar,
and Roberto Navigli. 2016. Nasari: Integrating ex-
plicit knowledge and corpus statistics for a multilin-
gual representation of concepts and entities. Artifi-
cial Intelligence, 240:36–64.

Xinxiong Chen, Zhiyuan Liu, and Maosong Sun. 2014.
A unified model for word sense representation and
disambiguation. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 1025–1035.

Jianpeng Cheng, Zhongyuan Wang, Ji-Rong Wen, Jun
Yan, and Zheng Chen. 2015. Contextual text under-
standing in distributional semantic space. In Pro-
ceedings of the 24th ACM International on Confer-
ence on Information and Knowledge Management,
pages 133–142. ACM.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. arXiv preprint arXiv:1810.04805.

Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2001. Placing search in context: The
concept revisited. In Proceedings of the 10th inter-
national conference on World Wide Web, pages 406–
414. ACM.

Stuart Geman and Donald Geman. 1987. Stochas-
tic relaxation, gibbs distributions, and the bayesian
restoration of images. In Readings in computer vi-
sion, pages 564–584. Elsevier.

Eric H Huang, Richard Socher, Christopher D Man-
ning, and Andrew Y Ng. 2012. Improving word
representations via global context and multiple word
prototypes. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguistics:
Long Papers-Volume 1, pages 873–882. Association
for Computational Linguistics.

Ignacio Iacobacci, Mohammad Taher Pilehvar, and
Roberto Navigli. 2015. Sensembed: Learning sense
embeddings for word and relational similarity. In
Proceedings of the 53rd Annual Meeting of the Asso-
ciation for Computational Linguistics and the 7th In-
ternational Joint Conference on Natural Language
Processing (Volume 1: Long Papers), volume 1,
pages 95–105.

Sujay Kumar Jauhar, Chris Dyer, and Eduard Hovy.
2015. Ontologically grounded multi-sense represen-
tation learning for semantic vector space models. In
Proceedings of the 2015 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 683–693.



Jey Han Lau, Paul Cook, and Timothy Baldwin. 2013.
unimelb: Topic modelling-based word sense induc-
tion for web snippet clustering. In Second Joint
Conference on Lexical and Computational Seman-
tics (* SEM), Volume 2: Proceedings of the Seventh
International Workshop on Semantic Evaluation (Se-
mEval 2013), volume 2, pages 217–221.

Yang-Yin Lee, Ting-Yu Yen, Hen-Hsen Huang, Yow-
Ting Shiue, and Hsin-Hsi Chen. 2018. Gensense: A
generalized sense retrofitting model. In Proceedings
of the 27th International Conference on Computa-
tional Linguistics, pages 1662–1671.

Jiwei Li and Dan Jurafsky. 2015. Do multi-sense em-
beddings improve natural language understanding?
arXiv preprint arXiv:1506.01070.

Pengfei Liu, Xipeng Qiu, and Xuanjing Huang. 2015a.
Learning context-sensitive word embeddings with
neural tensor skip-gram model. In Twenty-Fourth
International Joint Conference on Artificial Intelli-
gence.

Yang Liu, Zhiyuan Liu, Tat-Seng Chua, and Maosong
Sun. 2015b. Topical word embeddings. In AAAI,
pages 2418–2424.

Thang Luong, Richard Socher, and Christopher Man-
ning. 2013. Better word representations with re-
cursive neural networks for morphology. In Pro-
ceedings of the Seventeenth Conference on Computa-
tional Natural Language Learning, pages 104–113.

Massimiliano Mancini, Jose Camacho-Collados, Igna-
cio Iacobacci, and Roberto Navigli. 2016. Em-
bedding words and senses together via joint
knowledge-enhanced training. arXiv preprint
arXiv:1612.02703.

Andrew Kachites McCallum. 2002. Mallet:
A machine learning for language toolkit.
Http://mallet.cs.umass.edu.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space. CoRR, abs/1301.3781.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in neural information processing
systems, pages 3111–3119.

Arvind Neelakantan, Jeevan Shankar, Alexandre Pas-
sos, and Andrew McCallum. 2015. Efficient
non-parametric estimation of multiple embeddings
per word in vector space. arXiv preprint
arXiv:1504.06654.

Dai Quoc Nguyen, Dat Quoc Nguyen, Ashutosh Modi,
Stefan Thater, and Manfred Pinkal. 2017. A mixture
model for learning multi-sense word embeddings.
arXiv preprint arXiv:1706.05111.

Maria Pelevina, Nikolay Arefyev, Chris Biemann, and
Alexander Panchenko. 2017. Making sense of word
embeddings. arXiv preprint arXiv:1708.03390.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word rep-
resentation. In Proceedings of the 2014 conference
on empirical methods in natural language process-
ing (EMNLP), pages 1532–1543.

Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word repre-
sentations. arXiv preprint arXiv:1802.05365.

Mohammad Taher Pilehvar and Nigel Collier. 2016.
De-conflated semantic representations. arXiv
preprint arXiv:1608.01961.

Joseph Reisinger and Raymond J Mooney. 2010.
Multi-prototype vector-space models of word mean-
ing. In Human Language Technologies: The 2010
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 109–117. Association for Computational Lin-
guistics.

Sascha Rothe and Hinrich Schütze. 2015. Au-
toextend: Extending word embeddings to embed-
dings for synsets and lexemes. arXiv preprint
arXiv:1507.01127.

Herbert Rubenstein and John B Goodenough. 1965.
Contextual correlates of synonymy. Communica-
tions of the ACM, 8(10):627–633.

C. Shaoul and Edmonton AB: University of Al-
berta Westbury, C. (2010) The Westbury
Lab Wikipedia Corpus. 2010. The west-
bury lab wikipedia dataset. Data downloaded
from http://www.psych.ualberta.ca/ westbury-
lab/downloads/westburylab.wikicorp.download.html.

Fei Tian, Hanjun Dai, Jiang Bian, Bin Gao, Rui Zhang,
Enhong Chen, and Tie-Yan Liu. 2014. A probabilis-
tic model for learning multi-prototype word embed-
dings. In Proceedings of COLING 2014, the 25th
International Conference on Computational Linguis-
tics: Technical Papers, pages 151–160.

Zhaohui Wu and C Lee Giles. 2015. Sense-aaware se-
mantic analysis: A multi-prototype word representa-
tion model using wikipedia. In Twenty-Ninth AAAI
Conference on Artificial Intelligence.

Heng Zhang and Guoqiang Zhong. 2016. Improving
short text classification by learning vector represen-
tations of both words and hidden topics. Knowledge-
Based Systems, 102:76–86.

http://arxiv.org/abs/1301.3781
http://arxiv.org/abs/1301.3781

