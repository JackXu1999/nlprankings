



















































A Refined Notion of Memory Usage for Minimalist Parsing


Proceedings of the 14th Meeting on the Mathematics of Language (MoL 14), pages 1–14,
Chicago, USA, July 25–26, 2015. c©2015 Association for Computational Linguistics

A Refined Notion of Memory Usage for Minimalist Parsing

Thomas Graf Brigitta Fodor James Monette
Gianpaul Rachiele Aunika Warren Chong Zhang

Stony Brook University, Department of Linguistics
mail@thomasgraf.net, {firstname.lastname}@stonybrook.edu

Abstract

Recently there has been a lot of interest in
testing the processing predictions of a spe-
cific top-down parser for Minimalist gram-
mars (Stabler, 2013). Most of this work relies
on memory-based difficulty metrics that relate
the shape of the parse tree to processing be-
havior. We show that none of the difficulty
metrics proposed so far can explain why sub-
ject relative clauses are more easily processed
than object relative clauses in Chinese, Ko-
rean, and Japanese. However, a minor tweak
to how memory load is determined is suffi-
cient to fully capture the data. This result
thus lends further support to the hypothesis
that very simple notions of resource usage are
powerful enough to explain a variety of pro-
cessing phenomena.

1 Introduction

One of the great advantages of mathematical linguis-
tics is that its formal rigor allows for the exploration
of ideas and questions that could not even be pre-
cisely formulated otherwise. A promising project
along these lines is the investigation of syntactic pro-
cessing from a computationally informed perspec-
tive (Joshi, 1990; Rambow and Joshi, 1995; Steed-
man, 2001; Hale, 2011; Yun et al., 2014). This
requires I) an articulated theory of syntax that has
sufficient empirical coverage to be applicable to a
wide range of constructions, II) a sound and com-
plete parser for the syntactic formalism, and III)
a linking theory that derives psycholinguistic pre-
dictions from these two components. A successful

model along these lines provides profound insights
into the mechanisms of linguistic performance, and
it can also rule out certain syntactic proposals as psy-
cholinguistically inadequate. Unfortunately there
are multiple choices for each one of the three com-
ponents, which raises the question of which combi-
nations are empirically adequate.

This paper explores this issue for Minimalist
grammars (MGs), a formalization of the Chomskyan
variety of generative grammar that informs a lot of
psycholinguistic research nowadays. Taking as our
vantage point Kobele et al. (2012; henceforth KGH)
and their method for deriving structure-sensitive
processing predictions from Stabler’s (2013) MG
top-down parser, we evaluate how well the parser
captures the processing difficulty of relative clauses
in Chinese, Japanese, and Korean — a phenomenon
that escapes many processing models in the litera-
ture. By carefully modulating the set of syntactic
assumptions as well as the linking hypotheses, we
show that none of the memory-based proposals in
the tradition of KGH yield the right predictions. The
correct results are obtained, however, if the size of
parse items also counts towards their memory usage.
Our paper thus serves a dual purpose: it provides a
positive result in the form of a more refined notion of
memory usage that explains the observed processing
behavior, and a negative one by eliminating many
combinations of the three factors listed above.

Our discussion starts with two introductory sec-
tions that familiarize the reader with the research
this paper follows up on. We first discuss MGs,
the MG top-down parser, and how this parser has
been used to model processing phenomena in re-

1



cent years. This is followed by a brief review of a
long standing problem in syntactic processing: the
preference for subject relative clauses (SRCs) over
object relative clauses (ORCs) irrespective of cross-
linguistic word order differences. We present two
prominent relative clause analyses from the syntac-
tic literature, and we discuss why the preference for
SRCs over ORCs is surprising given current psy-
cholinguistic models. In Sec. 4 we finally demon-
strate that the MG parser cannot make the right pre-
dictions with any of the proposed metrics unless one
refines their conception of memory load.

2 Minimalist Grammars for Processing

2.1 Minimalist Grammars

MGs (Stabler, 1997) are a formalization of the most
recent iteration of transformational grammar, known
as Minimalism. Since they formalize ideas that form
the underpinning for the majority of contemporary
research in theoretical syntax and syntactic process-
ing, they act as a form of glue that makes these ideas
amenable to more rigorous study. The main purpose
of MGs in this paper is to provide a specific type of
structure for the parser to operate on — derivation
trees. Consequently the technical machinery is of
interest only to the extent that it illuminates the con-
nection between derivations and MG parsing, and
we thus omit formal details where possible.

An MG is a finite set of lexical items (LIs), where
every LI consists of a phonetic exponent and a finite,
non-empty string of features. Each feature has a pos-
itive or negative polarity, and it is either a Merge fea-
ture (written in upper caps) or a Move feature (writ-
ten in lower caps). MGs assemble LIs into trees via
the structure-building operations Merge and Move
according to the feature specifications of the LIs.
Intuitively, Merge may combine two LIs if their re-
spective first unchecked features are Merge features
and differ only in their polarity. The LI with the pos-
itive polarity feature acts as the head of the assem-
bled phrase. Move, on the other hand, removes a
phrase from an already assembled tree and puts it in
a different position; see Stabler (2011) for a formal
definition. Figure 1 shows a simplified tree for John,
the girl likes, with dashed lines indicating which po-
sitions certain phrases were displaced from.

The structure of a sentence is also fully encoded

by its derivation tree, i.e. the record of how its phrase
structure tree was assembled from the LIs via appli-
cations of Move and Merge. Every derivation tree
corresponds to exactly one phrase structure tree, but
the reverse does not necessarily hold. The main dif-
ference between the two types of tree is that moving
phrases remain in their base position in the deriva-
tion tree — compare, for instance, the positions of
John and the girl in the two trees in Fig. 1 (for the
sake of clarity interior nodes have the same label as
their counterpart in the phrase structure tree). As
a result, derivation trees do not directly reflect the
word order of a sentence, which must be derived by
carrying out the movement steps.

In addition, an MG’s set of well-formed deriva-
tion trees forms a regular tree language thanks to
a specific restriction on Move that is known as the
Shortest Move Constraint (Michaelis, 2001; Kobele
et al., 2007; Salvati, 2011; Graf, 2012). The set
of well-formed phrase structure trees, on the other
hand, is supra-regular — a corollary of MGs’ weak
equivalence to MCFGs (Harkema, 2001; Michaelis,
2001). The fact that derivation trees do not need to
directly encode linear order thus reduces their com-
plexity significantly in comparison to phrase struc-
ture trees. Since derivation trees offer a complete
regular description of the structure of a sentence,
and because regular tree languages can be viewed as
context-free grammars (CFGs) with an ancillary hid-
den alphabet (Thatcher, 1967), MGs turn out to be
close relatives of CFGs with a more complex map-
ping from trees to strings. It is this close connec-
tion to CFGs that forms the foundation of Stabler
(2013)’s top-down parser.

2.2 MG Parsing as Tree Traversal

Stabler (2013)’s parser for MGs builds on standard
depth-first, top-down parsing strategies for CFGs
but modifies them in three important respects: I)
the parser is equipped with a search beam that dis-
cards the most unlikely analyses, thus avoiding the
usual problems with left recursion, II) the parser
constructs derivation trees rather than phrase struc-
ture trees, and III) since derivation trees do not di-
rectly reflect linear order, the parser moves through
them in a particular fashion that would approximate
a left-most, depth-first search in the corresponding
phrase structure trees.

2



CP

C′

TP

T′

vP

vP

v′

VP

likes :: D+ V−

ε :: V+ D+ acc+ v−

ε :: v+ nom+ T−

DP

girl :: N−the :: N+ D− nom−

ε :: T+ top+ C−

John :: D− acc− top−

CP

C′

TP

T′

vP

vP

v′

VP

John :: D− acc− top−likes :: D+ V−

ε :: V+ D+ acc+ v−

DP

girl :: N−the :: N+ D− nom−

ε :: v+ nom+ T−

ε :: T+ top+ C−

Figure 1: MG phrase structure tree and derivation tree for John, the girl likes; dashed branches indicate movement

We completely ignore the beam in this paper and
instead adopt KGH’s assumption that the parser is
equipped with a perfect oracle so that it never makes
any wrong guesses during the construction of the
derivation tree. While psychologically implausible,
this idealization is meant to stake out a specific re-
search goal: processing effects must be explained
purely in terms of the syntactic complexity of the
involved structures, rather than the difficulty of find-
ing these structures in a large space of alternatives.
More pointedly, we assume that parsing difficulty
modulo non-determinism is sufficient to account for
the processing phenomena under discussion.

With non-determinism completely eliminated
from the picture, the parse of some sentence s re-
duces to a specific traversal of the derivation tree of
s. In general, the parser follows a left-most, depth-
first strategy, where a node is left-most if it is a spec-
ifier or if it is a head with a complement. However,
when a Move node is encountered, two things can
happen, depending on whether the Move node is an
intermediary landing site or a final one. Let p be
a moving phrase and m1, . . . ,mn the Move nodes
that denote an instance of Move displacing p. Then
mi is a final landing site (or simply final) iff there is

no mj , 1 ≤ j ≤ n, that properly dominates mi in
the derivation tree. A Move node is an intermediary
landing site (or intermediary) iff there is no phrase
in the derivation tree for which it is a final landing
site. An intermediary Move node does not affect the
parser’s tree traversal strategy. A final Move node,
on the other hand, causes the parser to take the short-
est path to the phrase that will be displaced by this
instance of Move. Once the root of that phrase has
been reached, the parser traverses its subtree in the
usual fashion and then returns to the point where it
veered off the standard path.

The traversal is made fully explicit via a notation
adopted from KGH where each node in the deriva-
tion tree has a superscripted index and a subscripted
outdex. The index lists the point at which the parse
item corresponding to the node is inserted into the
parser’s memory queue, whereas the outdex gives
the point at which said parse item is removed from
the queue. Both values can be computed in a purely
tree-geometric fashion. Let s[urface]-precedence be
the relation that holds between nodes m and n in
a derivation tree iff their counterparts m′ and n′ in
the corresponding phrase structure tree stand in the
precedence relation (if m undergoes movement, its

3



counterpart m′ is the final landing site rather than
its base position). Then indices and outdices can be
inferred without knowledge of the parser by the fol-
lowing procedure (cf. Fig. 2 on page 7):

• The index of the root is 1. For every other node,
its index is identical to the outdex of its mother.

• If nodes n and n′ are distinct nodes with index
i, and n reflexively dominates a node that is not
s-preceded by any node reflexively dominated
by n′, then n has outdex i+ 1.

• Otherwise, the outdex of node n with index i
is max(i + 1, j + 1), where j ≥ 0 is greatest
among the outdices of all nodes that s-precede
n but are not reflexively dominated by n.

2.3 Parsing Metrics

In order to allow for psycholinguistic predictions,
the behavior of the parser must be related to pro-
cessing difficulty via a parsing metric. There is no a
priori limit on the complexity of metrics one may
entertain, but the methodologically soundest posi-
tion is to explore simple metrics before moving on
to more complicated ones.

Extending KGH, Graf and Marcinek (2014;
henceforth GM) evaluate a variety of memory-based
metrics that measure I) how long a node is kept in
memory (tenure), or II) how many nodes must be
kept in memory (payload), or III) specific combi-
nations of these two factors. Tenure and payload
are easily defined using the node indexation scheme.
A node’s tenure is the difference between its index
and outdex, and the payload of the derivation tree is
equal to the number of nodes with a tenure strictly
greater than 2 (in the derivation trees in Figs. 2–5,
these nodes are boxed to highlight their contribution
to the payload).

GM define three metrics, the first of which is
adopted directly from KGH. Depending on the met-
ric, the difficulty of a parse is given by

Max max({t | t is the tenure of some node n})

Box | {n | n is a node with tenure > 2} |

Sum
∑

n has tenure >2 tenure-of(n)

GM define an additional six variants by restricting
the choice of nodes n to LIs and pronounced LIs,
respectively. They then compare the predictions of
these nine metrics with respect to right embedding
VS center embedding, and nested dependencies VS
crossing dependencies (both of which were origi-
nally analyzed in KGH), as well as two phenomena
involving relative clauses: I) sentential complements
containing a relative clause VS a relative clause con-
taining a sentential complement, and II) the prefer-
ence for subject relative clauses (SRCs) over object
relative clauses (ORCs) in English. They conclude
that the only metric that makes the right predictions
in all four constructions is Max restricted to pro-
nounced LIs.

Irrespective of the choice of metric, though, the
psycholinguistic predictions of the MG parser vary
with the choice of syntactic analysis. KGH use this
fact for a persuasive demonstration of how process-
ing data can be brought to bear on the distinction be-
tween so-called phrasal movement and head move-
ment. It is unclear, however, whether this should be
interpreted as support for a specific movement anal-
ysis or as evidence against the assumed difficulty
metric. GM’s comparison sheds little light on this
because it presupposes a specific syntactic analysis
for each phenomenon. A more elaborate comparison
is required that varies both the parsing metric and
the choice of syntactic analysis, ideally resulting in
only a few empirically adequate combinations. The
processing contrast between prenominal SRCs and
ORCs is exactly such a case.

3 Surveying Relative Clauses

3.1 Syntax

The main idea of this paper is that the space of pos-
sible combinations of syntactic analyses and parsing
metrics can be narrowed down quite significantly by
looking at processing phenomena that have proven
difficult to account for. As we will see next, the
fact that SRCs are easier to parse than ORCs in
Chinese, Korean, and Japanese constitutes such a
problem. We first discuss how the two have been
analyzed in the syntactic literature, while the next
section explains why many well-known processing
models have a hard time capturing the data.

Relative clauses (RCs) can be categorized accord-

4



ing to two parameters. First, the head noun, i.e. the
noun modified by the RC, may be the subject or the
object of the RC, in which case we speak of an SRC
and an ORC, respectively. Second, an RC is post-
nominal if it is linearly preceded by its head noun,
and prenominal otherwise. Note that in prenomi-
nal languages the complementizer (if it is realized
overtly) usually occurs at the right edge of the RC
rather than the left edge. Whether RCs have such an
overt complementizer is an ancillary parameter.

Most analyses of RCs were developed for lan-
guages like English, French, and German, where
RCs are postnominal and have overt complementiz-
ers (which might be optional). The general template
is [DP Det head-noun [RC complementizer subject
verb object]], with either the subject or the object
unrealized and the position of the verb depending
on language-specific word order constraints.

(1) a. [DP The mayor [RC who _ invited the ty-
coon]] likes wine.

b. [DP The mayor [RC who the tycoon in-
vited _]] likes wine.

The canonical account is the wh-movement analy-
sis, according to which the complementizer fills the
subject or object position, depending on the type of
RC, and then moves into Spec,CP (Chomsky, 1965;
Heim and Kratzer, 1998). Alternatively, the comple-
mentizer starts out as the C-head and instead a silent
operator undergoes movement from the base posi-
tion to Spec,CP. For the purposes of this paper the
two variants of the wh-movement analysis are fully
equivalent.

The promotion analysis is a well-known compet-
ing proposal (Vergnaud, 1974; Kayne, 1994). It
combines the ideas above and posits that the com-
plementizer starts out as the C-head, but instead of
a silent operator it is the head noun that moves from
the embedded subject/object position into Spec,CP.
In contrast to the wh-movement analysis, the head
noun is thus part of the RC. Crucially, though,
all three proposals involve an element that fills the
seemingly empty argument position of the verb and
subsequently moves to Spec,CP.

Languages with prenominal RCs, such as Chi-
nese, Japanese, and Korean, can be analyzed along
these lines, but differences in word order lead to a
significant increase in analytic complexity. Below is

an example of the English sentence in (1) with Chi-
nese word order.

(2) a. [DP [RC _ invited the tycoon who] the
mayor] likes wine.

b. [DP [RC the tycoon invited _ who] the
mayor] likes wine.

On a theoretical level, there are two major com-
plications. First, while Chinese is an SVO language
like English, Japanese and Korean are SOV lan-
guages, which requires movement of the object to
Spec,vP, thereby adding at least one more move-
ment step within each RC in these two languages.
More importantly, the prenominal word order must
be derived from the postnominal one via movement,
which causes the wh-movement analysis and the
promotion analysis to diverge more noticeably.

In the promotion analysis, the RC is no longer a
CP, but rather a RelP that contains a CP (see also
Yun et al., 2014). The head noun still moves from
within the RC to Spec,CP, but this is followed by
the TP moving to Spec,RelP so that one gets the de-
sired word order with the complementizer between
the rest of the RC in Spec,RelP and the head noun
in Spec,CP. In the wh-movement analysis, the head
noun is once again outside the RC, which is just a
CP instead of a RelP. The complementizer starts out
in subject or object position depending on the type
of RC, and then moves into a right specifier of the
CP. The CP subsequently moves to the specifier of
the DP of the head noun, once again yielding the de-
sired word order with the complementizer between
the RC and the head noun.

In sum, the promotion analysis needs to posit
a new phrase RelP but all movement is leftward
and takes place within this phrase, whereas the
wh-movement analysis sticks with a single CP but
invokes one instance of rightward movement and
moves the RC into Spec,DP, a higher position than
Spec,RelP. Both accounts are fairly complicated due
to the sheer number and intricate timing of move-
ment steps — the reader is advised to carefully study
the derivations in Figures 2 through 5.

Involved as they might be, both the promotion
analysis and the wh-movement analysis are work-
able solutions for the kind of prenominal SRCs and
ORCs found in Chinese, Korean, and Japanese. The
latter two only add an additional movement step for

5



each object to Spec,vP, and Japanese differs from
Chinese and Korean in that the RC complementizer
is never pronounced.

3.2 Psycholinguistics

SRCs and ORCs have been the subject of extensive
psycholinguistic research, with overwhelming evi-
dence pointing towards SRCs being easier to process
than ORCs irrespective of whether RCs are prenom-
inal or postnominal in a given language (Mecklinger
et al., 1995; Gibson and Pearlmutter, 1998; Mak
et al., 2002; Miyamoto and Nakamura, 2003; Gor-
don et al., 2006; Kwon et al., 2006; Mak et al.,
2006; Ueno and Garnsey, 2008; Kwon et al., 2010;
Miyamoto and Nakamura, 2013). The data is less
clear-cut in Chinese (Lin and Bever, 2006), but it has
recently been argued that this is only because of cer-
tain structural ambiguities (Gibson and Wu, 2013).
Yun et al. (2014) even show how such an ambiguity-
based account can be formalized via the MG parser.
Recall, though, that we deliberately ignore ambigui-
ties in this paper in an effort to find the simplest em-
pirically adequate linking between derivations and
processing behavior. For this reason, we assume that
Chinese would also exhibit a uniform preference for
SRCs over ORCs if it were not for the confound of
structural ambiguity.

That language-specific differences in word order
have no effect on the difficulty of SRCs relative to
ORCs is unexpected under a variety of psycholin-
guistic models. Dependency Locality Theory (Gib-
son, 1998) and the Active-Filler strategy (Frazier,
1987), for example, contend that parsing difficulty
increases with the distance between a filler and its
gap due to a concomitant increase in memory load
— an idea that is also implicit in KGH’s Max met-
ric. However, both models calculate distance over
strings rather than trees. Since prenominal RCs put
the object position (i.e. the gap) linearly closer to
the head noun (the filler), while the subject is farther
away, ORCs should be easier than SRCs.

The failure of string-based memory load models
can be remedied in two ways. One is to abandon the
notion that the SRC-ORC asymmetry derives from
structural factors, replacing it by functional con-
cepts such as Keenan and Comrie’s (1977) accessi-
bility hierarchy, which claims that objects are harder
to manipulate than subjects irrespective of the con-

struction involved. While certainly a valid hypothe-
sis, a computationally informed perspective has little
light to shed on it. We thus discard this option and
focus instead on how a more elaborate concept of
sentence structure may interact with memory-based
concepts of parsing difficulty. More precisely: can
the MG parser, when coupled with a suitable RC
analysis and one of the metrics discussed in Sec. 2.3,
explain why SRCs are easier to parse than ORCs?

4 Parser Predictions

4.1 Overview of Data

The annotated derivation trees for Chinese and Ko-
rean RCs are given in Figures 2 through 5. Japanese
is omitted since it has exactly the same analysis as
Korean except that the RC complementizer remains
unpronounced. Interior nodes are labeled with pro-
jections instead of Merge and Move for the sake of
increased readability, and a dashed branch spanning
from node m to node n indicates movement of the
whole subtree rooted in m to the specifier of n. For
the wh-analysis, we use a dotted line instead of a
dashed one if movement is to a right specifier rather
than a left one. Since these notational devices make
features redundant, they are omitted completely.

The tenure values for Chinese and Korean are
summarized in Tables 1 and 2, respectively. The
table subgroups nodes according to whether they
are pronounced LIs, unpronounced LIs, or interior
nodes. It also includes the summed tenure values for,
respectively, pronounced LIs, all LIs, and all listed
nodes. Once again we omit Japanese since it shows
exactly the same behavior as Korean, except that the
complementizer would be grouped under “lexical”
and not “pronounced”.

4.2 Evaluation of Metrics

All the metrics discussed in Sec. 2.3 fail insofar as
they do not predict a consistent preference for SRC
over ORC. On the other hand, some metrics fare
worse than others because they predict the very op-
posite, ORC being easier than SRC. This is the case
for Sum, which adds the tenure of all nodes that con-
tribute to the derivation’s payload. The problem is
that ORCs have a smaller total tenure than SRCs in
Korean and Japanese irrespective of the choice of
analysis. Furthermore, if the tenure of phrasal nodes

6



1CP2

2TP4

4T′5

5vP6

6v′ 26

26VP28

28wine3028likes29

26v27

6DP7

7RelP9

9Rel′10

10CP11

11C′12

12TP13

13T′14

14vP16

16v′17

17VP19

19tycoon2119invite20

17v18

16mayor 23

14T15

12C 24

10who 22

7D8

5T 25

2C3

1CP2

2TP4

4T′5

5vP6

6v′ 26

26VP28

28wine3028likes29

26v27

6DP7

7RelP9

9Rel′10

10CP11

11C′12

12TP13

13T′14

14vP15

15v′ 18

18VP20

20tycoon 2320invite21

18v19

15mayor16

14T 17

12C 24

10who 22

7D8

5T 25

2C3

Figure 2: SRC and ORC in Chinese, promotion analysis

is ignored, then Sum also makes the wrong predic-
tions for Chinese. This shows that all variants of
Sum are completely unsuitable to account for the
observed processing differences, corroborating pre-
vious findings by GM.

A more complicated picture emerges with pure
payload, formalized as Box. Depending on the
choice of analysis and which nodes count towards
payload, Box predicts a preference for SRC, for
ORC, or a tie. The unwanted preference for ORCs
emerges I) with the wh-movement analysis in Ko-
rean if all nodes are taken into consideration, II) with
both analyses in Korean if only LIs matter, and III)
with both analyses in Korean and the wh-movement
analysis in Chinese if only pronounced LIs are taken
into account. The only defensible variant of Box,
then, is the one that considers the full payload rather
than its restriction to lexical or pronounced nodes. In
combination with the promotion analysis, this pre-
dicts an SRC preference in Chinese and a tie in Ko-
rean.

Unfortunately, it has been shown by GM that Box
fails to make a distinction in processing difficulty

for crossing and nested dependencies, the latter of
which are harder to parse despite their reduced com-
putational complexity (Bach et al., 1986). Unless
the relative ease of crossing dependencies can be
explained by some other mechanism, an MG parser
with Box cannot model all the phenomena that were
already accounted for in KGH and GM.

Crossing dependencies were actually one of
KGH’s main arguments in support of Max— the
maximum tenure among all nodes determines over-
all parsing difficulty — so if this metric fares just
as well as Box for SRCs and ORCs, it is the prefer-
able choice. Unfortunately, Max is ill-suited for the
problem at hand. If one simply looks at the highest
tenure value, Max predicts ties for SRCs and ORCs
no matter which analysis or type of node is consid-
ered. If the metric is applied recursively such that
derivation d is easier than d′ iff they agree on the n
highest tenure values and the n + 1-th value of d is
lower than the n + 1-th value of d′, then Max pre-
dicts ORC preferences under all combinations. So
recursive application of Max leads from universal
ties to a universal ORC preference.

7



1CP2

2TP4

4T′5

5vP6

6v′ 26

26VP28

28wine3028likes29

26v27

6DP7

7D′8

8NP9

9CP10

10C′11

11TP13

13T′14

14vP16

16v′17

17VP19

19tycoon2119invite20

17v18

16who 22

14T15

11C12

9mayor 24 ;

8D 23

5T 25

2C3

1CP2

2TP4

4T′5

5vP6

6v′ 26

26VP28

28wine3028likes29

26v27

6DP7

7D′8

8NP9

9CP10

10C′11

11TP13

13T′14

14vP15

15v′ 18

18VP20

20who2220invite21

18v19

15mayor16

14T 17

11C12

9tycoon 24

8D 23

5T 25

2C3

Figure 3: SRC and ORC in Chinese, wh-movement analysis

It seems, then, that we have a choice between
an unrestricted version of Box, which works only
with the promotion analysis and treats crossing and
nested dependencies the same, and a non-recursive
unrestricted version of Max, which treats prenom-
inal SRCs and ORCs the same irrespective of the
chosen analysis. Either metric needs to be supple-
mented by some additional principle to handle these
cases. Recall, though, that Box predicts a tie for
Korean under the promotion analysis. Furthermore,
GM showed that the non-recursive version of Max
is also unsuitable for postnominal RCs and fails to
make a clear distinction between the easy case of
a sentential complement containing an RC and the
much harder case of an RC containing a sentential
complement. So whatever additional principle one
might propose, it must establish parsing preferences
for a diverse range of phenomena.

4.3 A Refined Tenure Metric
On an intuitive level it is rather surprising that no
metric grants a clear advantage to SRCs across the
board. After all, SRC and ORC derivations differ
only in the movement branch to the CP, which is

much longer for ORCs than for SRCs as subjects oc-
cupy a higher structural position than objects. Since
all the metrics home in on some aspect of memory
load, one would expect at least one of them to pick
up on this difference. That this does not happen is
due to the very nature of tenure.

A node has high tenure if its corresponding parse
item enters the parser’s queue early but cannot be
worked on for a long time. In the case of RCs,
the complementizer (or alternatively the head noun
in the wh-movement analysis) occupies a very high
structural position, so that it is encountered early
during the construction of the RC. At the same
time, it cannot be removed from the queue until the
full RC has been constructed, which means that the
parser has to move all the way down to the verb and
the object. But as long as the complementizer has
not been removed from the queue, none of the nodes
following it can be removed, either. The result is
a “parsing bottle neck” that leads to high tenure on
a large number of nodes. The difference between
SRCs and ORCs has no effect because it does not
change the need for the parser to build the entire RC

8



1CP2

2TP4

4T′5

5vP6

6vP7

7v′ 28

28VP29

29money3029loves 32

28v 31

7DP8

8RelP10

10Rel′11

11CP12

12C′13

13TP14

14T′15

15vP17

17vP18

18v′19

19VP20

20tycoon2120invited 23

19v 22

18mayor 25

15T16

13C 26

11who 24

8D9

5T 27

2C3

1CP2

2TP4

4T′5

5vP6

6vP7

7v′ 28

28VP29

29money3029loves 32

28v 31

7DP8

8RelP10

10Rel′11

11CP12

12C′13

13TP14

14T′15

15vP16

15vP17

17v′ 20

20VP22

22tycoon 2522invited23

20v21

17mayor18

15T 19

13C 26

11who 24

8D9

5T 27

2C3

Figure 4: SRC and ORC in Korean, promotion analysis

before it can work on the complementizer, which is
the actual cause for the bottle neck.

The central problem, then, is that the structural
differences between SRC and ORC are too marginal
to outweigh the effects of their shared structure on
tenure. There are many conceivable ways around
this, e.g. by combining payload and tenure so that
each node’s tenure from steps i to j is scaled relative
to the overall payload from i to j. The most natural
idea of multiplying tenure and payload leads to an
ORC preference, but division seems to produce the
correct results, even for the phenomena discussed in
KGH and GM. However, such a step would take us
away from the ideal of a simple metric. A less in-
volved solution is to refine the granularity of tenure
in a particular way.

Tenure measures how long a parse items remains
in memory, but it does not take into account how
much memory a given parse item consumes. Con-

sider the parse item corresponding to the embedded
CP of the SRC derivation in Fig. 2 on page 7. The
step from CP to C′ corresponds to a specific infer-
ence rule in the parser that constructs the C′ parse
item from the one for CP by adding a movement
feature f− to the list of movers that still need to be
found. From here on out, f− has to be passed around
from parse item to parse item until it is finally in-
stantiated on the object. All the parse items along
this path would have been smaller if they did not
have to carry along f− in the list of movers. There-
fore movement dependencies increase memory load
to the extent that they increase the size of parse items
(and thus the number of bits that are required for the
encoding of said items).

From this perspective, the processing difference
between SRCs and ORCs is due to the fact that the
longer movement branch in ORCs means that some
parse items are bigger in the ORC than their SRC

9



1CP2

2TP4

4T′5

5vP6

6vP7

7v′ 28

28VP29

29money3029loves 32

28v 31

7DP8

8D′9;

9NP10

10CP11

11C′12

12TP14

14T′15

15vP17

17vP18

18v′19

19VP20

20tycoon2120invited 23

19v 22

18who 24

15T16

12C13

10mayor 26

9D 25

5T 27

2C3

1CP2

2TP4

4T′5

5vP6

6vP7

7v′ 28

28VP29

29money3029loves 32

28v 31

7DP8

8D′9;

9NP10

10CP11

11C′12

12TP14

14T′15

15vP16

16vP17

17v′ 20

20VP22

22who2422invited23

20v21

17mayor18

15T 19

12C13

10tycoon 26

9D 25

5T 27

2C3

Figure 5: SRC and ORC in Korean, wh-movement analysis

counterparts. One must be careful, though, because
only the features of final landing sites are passed
along in this fashion — as defined in Stabler (2013),
the parser handles the features of intermediary land-
ing sites without increased memory usage. Once one
controls for the fact that some final landing sites in
the SRC are intermediate in the ORC, and the other
way round, there still remains a small advantage for
the SRC even in Korean. In both the SRC and the
ORC in Figs. 4 and 5, all the interior nodes inside
the embedded CP have to pass along at least one fea-
ture. More precisely, C′, TP, v′ and VP pass along
exactly one feature, while both vPs carry exactly two
features. Only T′ shows a difference: in the SRC it
hosts only the negative feature that triggers move-
ment of the subject, whereas in the ORC it must also
pass along the feature for the object.

This comparison is rather involved, but it can be
approximated via the index-based metric Gap (in-

spired by filler-gap dependencies), where ip is the
index of moving phrase p and fp the index of the
final landing site:

Gap
∑

p a moving phrase fp − ip
Both Box and non-recursive Max as discussed
above now make the right predictions in conjunction
with Gap as a secondary metric to resolve ties (this
includes also the constructions investigated in KGH
and GM). Such a system will grant an advantage to
SRCs as long as subjects occur in a higher position
than objects. Consequently, it argues against pro-
posals where subjects start out lower than objects
(Sigurðsson, 2006). Box furthermore favors the pro-
motion analysis over wh-movement, while Max re-
mains agnostic.

Conclusion

We showed that the MG parser does not make the
right predictions for prenominal SRCs and ORCs

10



Language Analysis RC Type Node Type Node Index Outdex Tenure

Chinese Promotion SRC pronounced who 10 22 12
mayor 16 23 7

lexical matrix T 5 25 20
C 12 24 12

interior matrix v′ 6 26 20

Summed tenure: 19 51 71

ORC pronounced who 10 22 12
mayor 20 23 3

lexical matrix T 5 25 20
C 12 24 12
embedded T 14 17 3

interior matrix v′ 6 26 20
embedded v′ 15 18 3

Summed tenure: 15 50 73

Wh SRC pronounced mayor 9 24 15
who 16 22 6

lexical matrix T 5 25 20
D 8 23 15

interior matrix v′ 5 25 20

Summed tenure: 21 56 76

ORC pronounced mayor 9 24 15
lexical matrix T 5 25 20

D 8 23 15
embedded T 14 17 3

interior matrix v′ 6 26 20
embedded v′ 15 18 3

Summed tenure: 15 53 76

Table 1: Tenure of nodes for Chinese, grouped by analysis; maximum tenure values are in italics

under any of the tree-geometric metrics that have
been proposed in the literature so far. However,
the observed processing effects can be explained if
one also take the memory requirements of move-
ment dependencies into account, formalized via the
metric Gap. The next step will be to test this hy-
pothesis against recent data from Basque (Carreiras
et al., 2010), where a uniform preference for ORCs
has been observed. Basque is an ergative language,
for which it has been argued that subject and ob-
ject might occur in different positions. If so, the ob-
served behavior may fall out naturally from slightly
different movement patterns and their effect on the
size of parse items.

A more pressing concern, though, is the mathe-
matical investigation of the parser — a sentiment
that is also expressed by KGH. The current method
of testing various metrics against numerous con-

structions is essential for mapping out the space of
empirically pertinent alternatives, but it is needlessly
labor intensive due to the usual pitfalls of combi-
natorial explosion. Nor does it enjoy the elegance
and generality of a proof-based approach. We be-
lieve that true progress in this area hinges on a so-
phisticated understanding of the tree traversal algo-
rithm instantiated by the parser and how exactly this
tree traversal interacts with specific metrics to pre-
fer particular tree shapes over others. Our insistence
on simple metrics, free from complicating aspects
like probabilities, stems from this desire to keep the
parser as open to future mathematical inquiry as pos-
sible.

Acknowledgments

We are greatly indebted to John Drury, Jiwon Yun,
and the three anonymous reviewers for their com-

11



Language Analysis RC Type Node Type Node Index Outdex Tenure

Korean Promotion SRC pronounced who 11 24 13
tycoon 18 25 7
invited 20 23 3
loves 29 32 3

lexical matrix T 5 27 22
C 13 26 13
embedded v 19 22 3
matrix v 28 31 3

interior matrix v′ 7 28 21

Summed tenure: 26 67 88

ORC pronounced who 11 24 13
mayor 22 25 3
loves 29 32 3

lexical matrix T 5 27 22
C 13 26 13
embedded T 15 19 4
matrix v 28 31 3

interior embedded v′ 17 20 3
matrix v′ 7 28 21

Summed tenure: 19 61 85

Wh SRC pronounced tycoon 10 26 16
who 18 24 6
loves 28 31 3
invited 20 23 3

lexical matrix T 5 27 22
D 9 25 16
embedded v 19 22 3
matrix v 27 30 3

interior matrix v′ 7 28 21

Summed tenure: 28 72 93

ORC pronounced tycoon 10 26 16
loves 29 32 3

lexical matrix T 5 27 22
D 9 25 16
embedded T 15 19 4
matrix v 28 31 3

interior embedded v′ 17 20 3
matrix v′ 7 28 21

Summed tenure: 19 64 88

Table 2: Tenure of nodes for Korean, grouped by analysis; maximum tenure values are in italics

ments and remarks that allowed us to streamline es-
sential parts of this work and improve the presenta-
tion of the material.

12



References
Emmon Bach, Colin Brown, and William Marslen-

Wilson. 1986. Crossed and nested dependencies in
German and Dutch: A psycholinguistic study. Lan-
guage and Cognitive Processes, 1:249–262.

Manuel Carreiras, Jon Andoni Duñabeitia, Marta Ver-
gara, Irene de la Cruz-Pavía, and Itziar Laka. 2010.
Subject relative clauses are not universally easier to
process: Evidence from Basque. Cognition, 115:79–
92.

Noam Chomsky. 1965. Aspects of the Theory of Syntax.
MIT Press, Cambridge, Mass.

Lyn Frazier. 1987. Sentence processing: A tutorial re-
view.

Edward Gibson and Neal J. Pearlmutter. 1998. Con-
straints on sentence comprehension. Trends in Cog-
nitive Sciences, 2(7):262–268.

Edward Gibson and H.-H. Iris Wu. 2013. Processing
Chinese relative clauses in context. Language and
Cognitive Processes, 28(1-2):125–155.

Edward Gibson. 1998. Linguistic complexity: Locality
of syntactic dependencies. Cognition, 68:1–76.

Peter C. Gordon, Randall Hendrick, Marcus Johnson, and
Yoonhyoung Lee. 2006. Similarity-based interfer-
ence during language comprehension: Evidence from
eye tracking during reading. Journal of Experimen-
tal Psychology: Learning, Memory, and Cognition,
32(6):1304.

Thomas Graf and Bradley Marcinek. 2014. Evaluating
evaluation metrics for minimalist parsing. In Proceed-
ings of the 2014 ACL Workshop on Cognitive Modeling
and Computational Linguistics, pages 28–36.

Thomas Graf. 2012. Locality and the complexity of min-
imalist derivation tree languages. In Philippe de Groot
and Mark-Jan Nederhof, editors, Formal Grammar
2010/2011, volume 7395 of Lecture Notes in Com-
puter Science, pages 208–227, Heidelberg. Springer.

John Hale. 2011. What a rational parser would do. Cog-
nitive Science, 35:399–443.

Henk Harkema. 2001. A characterization of minimalist
languages. In Philippe de Groote, Glyn Morrill, and
Christian Retoré, editors, Logical Aspects of Compu-
tational Linguistics (LACL’01), volume 2099 of Lec-
ture Notes in Artificial Intelligence, pages 193–211.
Springer, Berlin.

Irene Heim and Angelika Kratzer. 1998. Semantics in
Generative Grammar. Blackwell, Oxford.

Aravind Joshi. 1990. Processing crossed and nested
dependencies: An automaton perspective on the psy-
cholinguistic results. Language and Cognitive Pro-
cesses, 5:1–27.

Richard S. Kayne. 1994. The Antisymmetry of Syntax.
MIT Press, Cambridge, Mass.

Edward L. Keenan and Bernard Comrie. 1977. Noun
phrase accessiblity and universal grammar. Linguistic
Inquiry, 8:63–99.

Gregory M. Kobele, Christian Retoré, and Sylvain Sal-
vati. 2007. An automata-theoretic approach to mini-
malism. In James Rogers and Stephan Kepser, editors,
Model Theoretic Syntax at 10, pages 71–80.

Gregory M. Kobele, Sabrina Gerth, and John T. Hale.
2012. Memory resource allocation in top-down min-
imalist parsing. In Proceedings of Formal Grammar
2012.

Nayoung Kwon, Maria Polinsky, and Robert Kluender.
2006. Subject preference in Korean. In Proceedings
of the 25th West Coast Conference on Formal Lin-
guistics, pages 1–14. Cascadilla Proceedings Project
Somerville, MA.

Nayoung Kwon, Peter C. Gordon, Yoonhyoung Lee,
Robert Kluender, and Maria Polinsky. 2010. Cog-
nitive and linguistic factors affecting subject/object
asymmetry: An eye-tracking study of prenominal rel-
ative clauses in Korean. Language, 86(3):546–582.

Chien-Jer Charles Lin and Thomas G. Bever. 2006. Sub-
ject preference in the processing of relative clauses in
Chinese. In Proceedings of the 25th West Coast Con-
ference on Formal Linguistics, pages 254–260. Cas-
cadilla Proceedings Project Somerville, MA.

Willem M. Mak, Wietske Vonk, and Herbert Schriefers.
2002. The influence of animacy on relative clause pro-
cessing. Journal of Memory and Language, 47(1):50–
68.

Willem M. Mak, Wietske Vonk, and Herbert Schriefers.
2006. Animacy in processing relative clauses: The
hikers that rocks crush. Journal of Memory and Lan-
guage, 54(4):466–490.

Axel Mecklinger, Herbert Schriefers, Karsten Steinhauer,
and Angela D. Friederici. 1995. Processing relative
clauses varying on syntactic and semantic dimensions:
An analysis with event-related potentials. Memory &
Cognition, 23(4):477–494.

Jens Michaelis. 2001. Transforming linear context-free
rewriting systems into minimalist grammars. Lecture
Notes in Artificial Intelligence, 2099:228–244.

Edson T. Miyamoto and Michiko Nakamura. 2003. Sub-
ject/object asymmetries in the processing of relative
clauses in Japanese. In Proceedings of WCCFL, vol-
ume 22, pages 342–355.

Edson T. Miyamoto and Michiko Nakamura. 2013.
Unmet expectations in the comprehension of relative
clauses in Japanese. In Proceedings of the 35th An-
nual Meeting of the Cognitive Science Society.

Owen Rambow and Aravind Joshi. 1995. A processing
model for free word order languages. Technical Re-
port IRCS-95-13, University of Pennsylvania.

13



Sylvain Salvati. 2011. Minimalist grammars in the light
of logic. In Sylvain Pogodalla, Myriam Quatrini, and
Christian Retoré, editors, Logic and Grammar — Es-
says Dedicated to Alain Lecomte on the Occasion of
His 60th Birthday, number 6700 in Lecture Notes in
Computer Science, pages 81–117. Springer, Berlin.

Halldór Ármann Sigurðsson. 2006. The nominative puz-
zle and the low nominative hypothesis. Linguistic In-
quiry, 37:289–308.

Edward P. Stabler. 1997. Derivational minimalism. In
Christian Retoré, editor, Logical Aspects of Compu-
tational Linguistics, volume 1328 of Lecture Notes in
Computer Science, pages 68–95. Springer, Berlin.

Edward P. Stabler. 2011. Computational perspectives on
minimalism. In Cedric Boeckx, editor, Oxford Hand-
book of Linguistic Minimalism, pages 617–643. Ox-
ford University Press, Oxford.

Edward P. Stabler. 2013. Two models of minimalist, in-
cremental syntactic analysis. Topics in Cognitive Sci-
ence, 5:611–633.

Mark Steedman. 2001. The Syntactic Process. MIT
Press, Cambridge, Mass.

James W. Thatcher. 1967. Characterizing derivation
trees for context-free grammars through a generaliza-
tion of finite automata theory. Journal of Computer
and System Sciences, 1:317–322.

Mieko Ueno and Susan M Garnsey. 2008. An ERP
study of the processing of subject and object relative
clauses in Japanese. Language and Cognitive Pro-
cesses, 23(5):646–688.

Jean-Roger Vergnaud. 1974. French Relative Clauses.
Ph.D. thesis, MIT.

Jiwon Yun, Zhong Chen, Tim Hunter, John Whitman, and
John Hale. 2014. Uncertainty in processing relative
clauses across East Asian languages. Journal of East
Asian Linguistics, pages 1–36.

14


