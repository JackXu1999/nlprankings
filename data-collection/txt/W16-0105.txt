



















































Neural Enquirer: Learning to Query Tables in Natural Language


Proceedings of 2016 NAACL Human-Computer Question Answering Workshop, pages 29‚Äì35,
San Diego, California, June 12-17, 2016. c¬©2016 Association for Computational Linguistics

Neural Enquirer: Learning to Query Tables in Natural Language

Pengcheng Yin
The University of Hong Kong

pcyin@cs.hku.hk

Zhengdong Lu
Huawei Noah‚Äôs Ark Lab

Lu.Zhengdong@huawei.com

Hang Li
Huawei Noah‚Äôs Ark Lab

HangLi.HL@huawei.com

Ben Kao
The University of Hong Kong

kao@cs.hku.hk

Abstract

We propose NEURAL ENQUIRER ‚Äî a neu-
ral network architecture for answering natural
language (NL) questions given a knowledge
base (KB) table. Unlike previous work on
end-to-end training of semantic parsers, NEU-
RAL ENQUIRER is fully ‚Äúneuralized‚Äù: it gives
distributed representations of queries and KB
tables, and executes queries through a series
of differentiable operations. The model can be
trained with gradient descent using both end-
to-end and step-by-step supervision. During
training the representations of queries and the
KB table are jointly optimized with the query
execution logic. Our experiments show that
the model can learn to execute complex NL
queries on KB tables with rich structures.

1 Introduction

Natural language dialogue and question answering
often involve querying a knowledge base (Wen et
al., 2015; Berant et al., 2013). The traditional ap-
proach involves two steps: First, a given query QÃÉ is
semantically parsed into an ‚Äúexecutable‚Äù represen-
tation, which is often expressed in certain logical
form ZÃÉ (e.g., SQL-like queries). Second, the rep-
resentation is executed against a KB from which an
answer is obtained. For queries that involve com-
plex semantics and logic (e.g., ‚ÄúWhich city hosted
the longest Olympic Games before the Games in
Beijing?‚Äù), semantic parsing and query execution
become extremely complex. For example, care-
fully hand-crafted features and rules are needed to
correctly parse a complex query into its logical

form (see example in the lower-left corner of Fig-
ure 1). To partially overcome this complexity, recent
works (Clarke et al., 2010; Liang et al., 2011; Pa-
supat and Liang, 2015) attempt to ‚Äúbackpropagate‚Äù
query execution results to revise the semantic rep-
resentation of a query. This approach, however, is
greatly hindered by the fact that traditional seman-
tic parsing mostly involves rule-based features and
symbolic manipulation, and is subject to intractable
search space incurred by the great flexibility of nat-
ural language.

In this paper we propose NEURAL ENQUIRER
‚Äî a neural network system that learns to under-
stand NL queries and execute them on a KB table
from examples of queries and answers. Unlike sim-
ilar efforts along this line of research (Neelakan-
tan et al., 2015), NEURAL ENQUIRER is a fully
neuralized, end-to-end differentiable network that
jointly models semantic parsing and query execu-
tion. It encodes queries and KB tables into dis-
tributed representations, and executes compositional
queries against the KB through a series of differen-
tiable operations. The model is trained using query-
answer pairs, where the distributed representations
of queries and the KB are optimized together with
the query execution logic in an end-to-end fash-
ion. We demonstrate using a synthetic QA task that
NEURAL ENQUIRER is capable of learning to exe-
cute complex compositional NL questions.

2 Model

Given an NL query Q and a KB table T , NEU-
RAL ENQUIRER executes Q against T and outputs a
ranked list of answers. The execution is done by first

29



Which city hosted the longest Olympic Games before the Games in Beijing?

query ‡∑®ùëÑ
Query Encoder

Executor-1 Memory Layer-1

Executor-2 Memory Layer-2

Executor-3 Memory Layer-3

Executor-4 Memory Layer-4

Executor-5

Athens (probability distribution over table entries)

year host_city #_duration #_medals

2000 Sydney 20 2,000

2004 Athens 35 1,500

2008 Beijing 30 2,500

2012 London 40 2,300

query embedding

table embedding

Tab
le En

co
d

er

where year < (select year, where host_city = Beijing),
argmax(host_city, #_duration)

Find row r1 where host_city=Beijing

Select year of r1 as a

Find row sets R where year < a

Find r2 in R with max(#_duration)

Select host_city of r2

logical form ‡∑®ùëç

Figure 1: An overview of NEURAL ENQUIRER with five executors

using Encoders to encode the query and table into
distributed representations, which are then sent to a
cascaded pipeline of Executors to derive the answer.
Figure 1 gives an illustrative example. It consists of
the following components:

2.1 Query Encoder

Query Encoder abstracts the semantics of an NL
query Q and encodes it into a query embedding
q ‚àà RdQ . Let {x1,x2, . . . ,xT } be the embeddings
of the words in Q, where xt ‚àà RdW is from an em-
bedding matrix L. We employ a bidirectional Gated
Recurrent Unit (GRU) (Bahdanau et al., 2015) to
summarize the sequence of word embeddings in for-
ward and reverse orders. q is formed by concatenat-
ing the last hidden states in the two directions.

We remark that Query Encoder can find the rep-
resentation of a rather general class of symbol se-
quences, agnostic to the actual representation of
the query (e.g., natural language, SQL, etc). The
model is able to learn the semantics of input queries
through end-to-end training, making it a generic
model for query understanding and query execution.

2.2 Table Encoder

Table Encoder converts a KB table T into a dis-
tributed representation, which is used as an input to
executors. Suppose T has M rows and N columns.
In our model, the n-th column is associated with a
field name (e.g., host city). Each cell value is a
word (e.g., Beijing) in the vocabulary. We use wmn
to denote the cell value in row m column n, and

wmn to denote its embedding. Let fn be the em-
bedding of the field name for column n. For each
entry (cell) wmn, Table Encoder computes a „Äàfield,
value„Äâ composite embedding emn ‚àà RdE by fusing
fn and wmn through a non-linear transformation:

emn = tanh(W ¬∑ [fn;wmn] + b),
where [¬∑; ¬∑] denotes vector concatenation. The out-
put of Table Encoder is an M √óN √ó dE tensor that
consists of M √óN embeddings, each of length dE .
2.3 Executor
NEURAL ENQUIRER executes an input query on a
KB table through layers of execution. Each layer
consists of an executor that, after learning, performs
certain operation (e.g., select, max) relevant to the
input query. An executor outputs intermediate exe-
cution results, referred to as annotations, which are
saved in the external memory of the executor. A
query is executed sequentially through a stack of ex-
ecutors. Such a cascaded architecture enables the
model to answer complex, compositional queries.
An example is given in Figure 1 in which descrip-
tions of the operation each executor is assumed to
perform for the query QÃÉ are shown. We will demon-
strate in Section 4 that the model is capable of learn-
ing the operation logic of each executor via end-to-
end training.

As illustrated in Figure 2, an executor at Layer-`
(denoted as Executor-`) consists of two major neu-
ral network components: a Reader and an Annota-
tor. The executor processes a table row-by-row. The

30



Reader

table embedding
read vectors

pooling

Annotator

row annotations

table annotation
Memory Layer-(‚Ñì-1)

query embedding Memory Layer-‚Ñì

Figure 2: Overview of an Executor-`

Reader reads in data from each row m in the form
of a read vector r`m, which is then sent to the An-
notator to perform the actual execution. The output
of the Annotator is a row annotation a`m, which cap-
tures the row-wise local computation result. Once
all row annotations are obtained, Executor-` gener-
ates a table annotation g` to summarize the global
computation result on the whole table by pooling
all row annotations. All the row and table annota-
tions are saved in the memory of Layer-`: M` =
{a`1,a`2, . . . ,a`M ,g`}. Intuitively, row annotations
handle operations that require only row-wise, lo-
cal information (e.g., select, where), while table
annotations model superlative operations (e.g., max,
min) by aggregating table-wise, global execution re-
sults. A combination of row and table annotations
enables the model to perform a wide variety of query
operations in real world scenarios.

2.3.1 Reader
As illustrated in Figure 3, for the m-th row with

N „Äàfield, value„Äâ composite embeddings Rm =
{em1, em2, . . . , emN}, the Reader fetches a read
vector r`m from Rm via an attentive reading oper-
ation:

r`m = f
`
R(Rm,FT ,q,M`‚àí1) =

N‚àë
n=1

œâÃÉ(fn,q,g`‚àí1)emn

whereM`‚àí1 denotes the content of memory Layer-
(`‚àí1), and FT = {f1, f2, . . . , fN} is the set of field
name embeddings. œâÃÉ(¬∑) is the normalized attention
weights given by:

œâÃÉ(fn,q,g`‚àí1)=
exp(œâ(fn,q,g`‚àí1))‚àëN

n‚Ä≤=1 exp(œâ(fn‚Ä≤ ,q,g`‚àí1))
(1)

where œâ(¬∑) is modeled as a Deep Neural Network
(denoted as DNN(`)1 ). Since each executor models a
specific type of computation, it should only attend
to a subset of entries that are pertinent to its execu-
tion. This is modeled by the Reader. Our approach

DNN1
+

query embedding

table annotation

read vector

year host_city #_duration #_medalsrow m

year host_city #_duration #_medals

(‚Ñì)

composite embeddings

Figure 3: Illustration of the Reader in Executor-`.

is related to the content-based addressing of Neural
Turing Machines (Graves et al., 2014) and the atten-
tion mechanism in neural machine translation mod-
els (Bahdanau et al., 2015).

2.3.2 Annotator
The Annotator of Executor-` computes row and

table annotations based on read vectors fetched by
the Reader. The results are stored in the `-th mem-
ory layerM` accessible to Executor-( +ÃÄ1). The last
executor is the only exception, which outputs the fi-
nal answer.
[Row annotations] Capturing row-wise execution
result, the annotation a`m for row m in Executor-`
is given by

a`m=f
`
A(r

`
m,q,M`‚àí1)=DNN(`)2 ([r`m;q;a`‚àí1m ;g`‚àí1]).

DNN(`)2 fuses the corresponding read vector r
`
m,

the results saved in the previous memory layer (row
and table annotations a`‚àí1m , g`‚àí1), and the query em-
bedding q. Specifically,

‚Ä¢ row annotation a`‚àí1m represents the local status of
execution before Layer-`;

‚Ä¢ table annotation g`‚àí1 summarizes the global sta-
tus of execution before Layer-`;

‚Ä¢ read vector r`m stores the value of attentive read-
ing;

‚Ä¢ query embedding q encodes the overall execution
agenda.

All the above values are combined through
DNN(`)2 to form the annotation of row m in the cur-
rent layer.
[Table annotations] Capturing global execution
state, a table annotation summarizes all row anno-
tations via a global max pooling operation:

g` = fMAXPOOL(a`1,a
`
2, . . . ,a

`
M ) = [g1, g2, . . . , gdG ]

>

31



year host city # participants # medals # duration # audience host country GDP country size population
2008 Beijing 4,200 2,500 30 67,000 China 2,300 960 130

Figure 4: An example table in the synthetic QA task (only one row shown)

1¬© SELECT WHERE [select Fa, where Fb = wb] 3¬©WHERE SUPERLATIVE [where Fa >|< wa, argmax/min(Fb, Fc)]
. How many people participated in the game in Beijing? . How long was the game with the most medals that had fewer than 3,000 participants?
. In which city was the game hosted in 2012? . How many medals were in the first game after 2008?
2¬© SUPERLATIVE [argmax/min(Fa, Fb)] 4¬© NEST [where Fa >|< (select Fa,where Fb=wb), argmax/min(Fc, Fd)]
. When was the latest game hosted? . Which country hosted the longest game before the game in Athens?
. How big is the country which hosted the shortest game? . How many people watched the earliest game that lasts for more days than the game in 1956?

Table 1: Example queries for each query type, with annotated SQL-like logical form templates

where gk = max({a`1(k),a`2(k), . . . ,a`M (k)}) is
the maximum value among the k-th elements of all
row annotations.

2.3.3 Last Layer Executor

Instead of computing annotations based on read
vectors, the last executor in NEURAL ENQUIRER di-
rectly outputs the probability of an entry wmn in ta-
ble T being the answer a:

p(a = wmn|Q, T ) =
exp(f `ANS(emn,q,a

`‚àí1
m ,g

`‚àí1))‚àëM,N
m‚Ä≤=1,n‚Ä≤=1 exp(f

`
ANS(em‚Ä≤n‚Ä≤ ,q,a

`‚àí1
m‚Ä≤ ,g

`‚àí1))
(2)

where f `ANS(¬∑) is modeled as a DNN (DNN(`)3 ). Note
that the last executor, which is devoted to returning
answers, could still carry out execution in DNN(`)3 .

3 Learning

NEURAL ENQUIRER can be trained in an end-to-
end (N2N) fashion. Given a set of ND query-table-
answer triples D = {(Q(i), T (i), y(i))}, the model is
optimized by maximizing the log-likelihood of gold-
standard answers:

LN2N(D) =
‚àëND

i=1
log p(a = y(i)|Q(i), T (i)) (3)

The training can also be carried out with stronger
guidance, i.e., step-by-step (SbS) supervision, by
softly guiding the learning process via controlling
the attention weights wÃÉ(¬∑) in Eq. (1). As an example,
for Executor-1 in Figure 1, by biasing the attention
weight of the host city field towards 1.0, only the
value of host city will be fetched and sent to the
Annotator. In this way we can ‚Äúforce‚Äù the executor
to learn the where operation to find the row whose

host city is Beijing. Formally, this is done by in-
troducing additional supervision signal to Eq. (3):

LSbS(D) =
‚àëND

i=1
[log p(a = y(i)|Q(i), T (i))

+ Œ±
‚àëL‚àí1

`=1
log wÃÉ(f?i,`, ¬∑, ¬∑)] (4)

where Œ± is a tuning weight, and L is the number of
executors. f?i,` is the embedding of the field known
a priori to be used by Executor-` in answering the
i-th example.

4 Experiments

In this section we evaluate NEURAL ENQUIRER on
synthetic QA tasks with NL queries of varying com-
positional depths.

4.1 Synthetic QA Task
We present a synthetic QA task with a large num-
ber of QA examples at various levels of complex-
ity to evaluate the performance of NEURAL EN-
QUIRER. Starting with ‚Äúartificial‚Äù tasks acceler-
ates the development of novel deep models (Weston
et al., 2015), and has gained increasing popularity
in recent research on modeling symbolic computa-
tion using DNNs (Graves et al., 2014; Zaremba and
Sutskever, 2014).

Our synthetic dataset consists of query-table-
answer triples {(Q(i), T (i), y(i))}. To generate a
triple, we first randomly sample a table T (i) of size
10√ó10 from a synthetic schema of Olympic Games.
The cell values of T (i) are drawn from a vocabulary
of 120 location names and 120 numbers. Figure 4
gives an example table. Next, we sample a query
Q(i) generated using NL templates, and obtain its
gold-standard answer y(i) on T (i). Our task consists
of four types of NL queries, with examples given
in Table 1. We also give the logical form template

32



MIXED-25K MIXED-100K
SEMPRE N2N SbS N2N SbS

SELECT WHERE 93.8% 96.2% 99.7% 99.3% 100.0%
SUPERLATIVE 97.8% 98.9% 99.5% 99.9% 100.0%
WHERE SUPERLATIVE 34.8% 80.4% 94.3% 98.5% 99.8%
NEST 34.4% 60.5% 92.1% 64.7% 99.7%
Overall Accuracy 65.2% 84.0% 96.4% 90.6% 99.9%

Table 2: Accuracies on MIXED datasets

for each type of query. The templates define the se-
mantics and compositionality of queries. We gener-
ate queries at various compositional depths, ranging
from simple SELECT WHERE queries to more com-
plex NEST ones. This makes the dataset have sim-
ilar complexity as a real-world one, except for the
relatively small vocabulary. The queries are flexible
enough to involve complex matching between NL
phrases and logical constituents, which makes query
understanding nontrivial: (1) the same field is de-
scribed by different NL phrases (e.g., ‚ÄúHow big is
the country ...‚Äù and ‚ÄúWhat is the size of the country
...‚Äù for the country size field); (2) different fields
may be referred to by the same NL pattern (e.g,
‚Äúin China‚Äù for host country and ‚Äúin Beijing‚Äù for
host city); (3) simple NL constituents may be
grounded to complex logical operations (e.g., ‚Äúafter
the game in Beijing‚Äù implies comparing between the
values of year fields).

To simulate the read-world scenario where
queries of various types are issued to the model, we
construct two MIXED datasets, with 25K and 100K
training examples respectively, where four types of
queries are sampled with the ratio 1 : 1 : 1 : 2. Both
datasets share the same testing set of 20K examples,
5K for each type of query. We enforce that no tables
and queries are shared between training/testing sets.

4.2 Setup
[Tuning] We adopt a model with five executors. The
lengths of hidden states for GRU and DNNs are 150,
50. The numbers of layers for DNN(`)1 , DNN

(`)
2 and

DNN
(`)
3 are 2, 3, 3. The length of word embeddings

and annotations is 20. Œ± is 0.2. We train the model
using ADADELTA (Zeiler, 2012) on a Tesla K40
GPU. The training converges fast within 2 hours.
[Metric] We evaluate in terms of accuracy, defined
as the fraction of correctly answered queries.
[Models] We compare the results of the following
settings:

‚Ä¢ Sempre (Pasupat and Liang, 2015) is a state-of-
the-art semantic parser and serves as the baseline;

‚Ä¢ N2N, NEURAL ENQUIRER model trained using
end-to-end setting (Sec 4.3);

‚Ä¢ SbS, NEURAL ENQUIRER model trained using
step-by-step setting (Sec 4.4).

4.3 End-to-End Evaluation

Table 2 summarizes the results of SEMPRE and
NEURAL ENQUIRER under different settings. We
show both the individual performance for each query
type and the overall accuracy. We evaluate SEM-
PRE only on MIXED-25K because of its long train-
ing time even on this small dataset (about 3 days).

In this section we discuss the results under end-
to-end (N2N) training setting. On MIXED-25K,
the relatively low performance of SEMPRE indi-
cates that our QA task, although synthetic, is highly
nontrivial. Surprisingly, our model outperforms
SEMPRE on all types of queries, with a marginal
gain on simple queries (SELECT WHERE, SU-
PERLATIVE), and significant improvement on com-
plex queries (WHERE SUPERLATIVE, NEST). On
MIXED-100K, our model achieves a decent over-
all accuracy of 90.6%. These results show that in
our QA task, NEURAL ENQUIRER is very effective
in answering compositional NL queries, especially
those with complex semantic structures compared
with the state-of-the-art system.

To further understand why our model is capable
of answering compositional queries, we study the
attention weights wÃÉ(¬∑) of Readers (Eq. 1) for execu-
tors in intermediate layers, and the answer probabil-
ity (Eq. 2) the last executor outputs for each entry in
the table. Those statistics are obtained on MIXED-
100K. We sample two queries (Q1 and Q2) in the
testing set that our model answers correctly and vi-
sualize their corresponding values in Figure 5. To

33



Q1: How long was the game with the most medals that had fewer than 3,000 participants?
Z1: where # participants < 3,000, argmax(# duration, # medals)

ye
ar

ho
st

_c
ity

#
_p

ar
tic

ip
an

ts

#
_m

ed
al
s

#
_d

ur
at

io
n

#
_a

ud
ie
nc

e

ho
st

_c
ou

nt
ry

GD
P

co
un

try
_s

ize

po
pu

la
tio

n
0.0
0.2
0.4
0.6
0.8
1.0

Executor-1

ye
ar

ho
st

_c
ity

#
_p

ar
tic

ip
an

ts

#
_m

ed
al
s

#
_d

ur
at

io
n

#
_a

ud
ie
nc

e

ho
st

_c
ou

nt
ry

GD
P

co
un

try
_s

ize

po
pu

la
tio

n
0.0
0.2
0.4
0.6
0.8
1.0

Executor-2

ye
ar

ho
st

_c
ity

#
_p

ar
tic

ip
an

ts

#
_m

ed
al
s

#
_d

ur
at

io
n

#
_a

ud
ie
nc

e

ho
st

_c
ou

nt
ry

GD
P

co
un

try
_s

ize

po
pu

la
tio

n
0.0
0.2
0.4
0.6
0.8
1.0

Executor-3

ye
ar

ho
st

_c
ity

#
_p

ar
tic

ip
an

ts

#
_m

ed
al
s

#
_d

ur
at

io
n

#
_a

ud
ie
nc

e

ho
st

_c
ou

nt
ry

GD
P

co
un

try
_s

ize

po
pu

la
tio

n
0.0
0.2
0.4
0.6
0.8
1.0

Executor-4

ye
ar

ho
st

_c
ity

#
_p

ar
tic

ip
an

ts

#
_m

ed
al
s

#
_d

ur
at

io
n

#
_a

ud
ie
nc

e

ho
st

_c
ou

nt
ry

GD
P

co
un

try
_s

ize

po
pu

la
tio

n

Executor-5

Q2: Which country hosted the longest game before the game in Athens?
Z2: where year < (select year,where host city=Athens), argmax(host country, # duration)

ye
ar

ho
st

_c
ity

#
_p

ar
tic

ip
an

ts

#
_m

ed
al
s

#
_d

ur
at

io
n

#
_a

ud
ie
nc

e

ho
st

_c
ou

nt
ry

GD
P

co
un

try
_s

ize

po
pu

la
tio

n
0.0
0.2
0.4
0.6
0.8
1.0

Executor-1

ye
ar

ho
st

_c
ity

#
_p

ar
tic

ip
an

ts

#
_m

ed
al
s

#
_d

ur
at

io
n

#
_a

ud
ie
nc

e

ho
st

_c
ou

nt
ry

GD
P

co
un

try
_s

ize

po
pu

la
tio

n
0.0
0.2
0.4
0.6
0.8
1.0

Executor-2

ye
ar

ho
st

_c
ity

#
_p

ar
tic

ip
an

ts

#
_m

ed
al
s

#
_d

ur
at

io
n

#
_a

ud
ie
nc

e

ho
st

_c
ou

nt
ry

GD
P

co
un

try
_s

ize

po
pu

la
tio

n
0.0
0.2
0.4
0.6
0.8
1.0

Executor-3

ye
ar

ho
st

_c
ity

#
_p

ar
tic

ip
an

ts

#
_m

ed
al
s

#
_d

ur
at

io
n

#
_a

ud
ie
nc

e

ho
st

_c
ou

nt
ry

GD
P

co
un

try
_s

ize

po
pu

la
tio

n
0.0
0.2
0.4
0.6
0.8
1.0

Executor-4

ye
ar

ho
st

_c
ity

#
_p

ar
tic

ip
an

ts

#
_m

ed
al
s

#
_d

ur
at

io
n

#
_a

ud
ie
nc

e

ho
st

_c
ou

nt
ry

GD
P

co
un

try
_s

ize

po
pu

la
tio

n

Executor-5

Figure 5: Weights visualization of queries Q1 and Q2

better understand the query execution process, we
also give the logical forms (Z1 and Z2) of the two
queries. Note that the logical forms are just for ref-
erence purpose and unknown by the model. We find
that each executor actually learns its execution logic
from just the correct answers in N2N training, which
is in accordance with our assumption. The model
executes Q1 in three steps, with each of the last
three executors performs a specific type of opera-
tion. For each row, Executor-3 takes the value of the
# participants field as input, while Executor-4
attends to the # medals field. Finally, Executor-5
outputs a high probability for the # duration field
in the 3-rd row. The attention weights for Executor-
1 and Executor-2 appear to be meaningless because
Q1 requires only three steps of execution, and the
model learns to defer the meaningful execution to
the last three executors. Compared with the logical
form Z1 of Q1, we can deduce that Executor-3 ‚Äúex-
ecutes‚Äù the where clause in Z1 to find row sets R
satisfying the condition, and Executor-4 performs
the first part of argmax to find the row r ‚àà R with
the maximum value of # medals, while Executor-5
outputs the value of # duration in row r.

Compared with Q1, Q2 is more complicated. Ac-
cording to Z2, Q2 involves an additional nest sub-
query to be solved by two extra executors, and re-
quires a total of five steps of execution. The last
three executors function similarly as in answering
Q1, yet the execution logic for the first two execu-
tors (devoted to solving the sub-query) is a bit ob-
scure, since their attention weights are scattered in-

stead of being perfectly centered on the ideal fields
as highlighted in red dashed rectangles. We posit
that this is because during the end-to-end training,
the supervision signal propagated from the top layer
has decayed along the long path down to the first two
executors, which causes vanishing gradients.

4.4 With Additional Step-by-Step Supervision
To alleviate the vanishing gradient problem when
training on complex queries like Q2, we train the
model using step-by-step (SbS) setting (Eq. 4),
where we encourage each intermediate executor to
attend to the field that is known a priori to be rele-
vant to its execution logic. Results are shown in Ta-
ble 2 (column SbS). With stronger supervision sig-
nal, the model significantly outperforms the N2N
setting, and achieves perfect accuracy on MIXED-
100K. This shows that NEURAL ENQUIRER is ca-
pable of leveraging the additional supervision signal
given to intermediate layers in SbS training. Let us
revisit the query Q2 in SbS setting. In contrast to
the result in N2N setting (Figure 5) where the atten-
tion weights for the first two executors are obscure,
now the weights are perfectly skewed towards each
relevant field with a value of 1.0, which corresponds
with the highlighted ideal weights.

5 Conclusion

We propose NEURAL ENQUIRER, a fully neural,
end-to-end differentiable network that learns to ex-
ecute compositional natural language queries on
knowledge-base tables.

34



References
[Bahdanau et al.2015] Dzmitry Bahdanau, Kyunghyun

Cho, and Yoshua Bengio. 2015. Neural machine
translation by jointly learning to align and translate.
In ICLR.

[Berant et al.2013] Jonathan Berant, Andrew Chou, Roy
Frostig, and Percy Liang. 2013. Semantic parsing
on freebase from question-answer pairs. In EMNLP,
pages 1533‚Äì1544.

[Clarke et al.2010] James Clarke, Dan Goldwasser, Ming-
Wei Chang, and Dan Roth. 2010. Driving semantic
parsing from the world‚Äôs response. In CoNLL, pages
18‚Äì27.

[Graves et al.2014] Alex Graves, Greg Wayne, and Ivo
Danihelka. 2014. Neural turing machines. CoRR,
abs/1410.5401.

[Liang et al.2011] Percy Liang, Michael I. Jordan, and
Dan Klein. 2011. Learning dependency-based com-
positional semantics. In ACL (1), pages 590‚Äì599.

[Neelakantan et al.2015] Arvind Neelakantan, Quoc V.
Le, and Ilya Sutskever. 2015. Neural programmer: In-
ducing latent programs with gradient descent. CoRR,
abs/1511.04834.

[Pasupat and Liang2015] Panupong Pasupat and Percy
Liang. 2015. Compositional semantic parsing on
semi-structured tables. In ACL (1), pages 1470‚Äì1480.

[Wen et al.2015] Tsung-Hsien Wen, Milica Gasic, Nikola
Mrksic, Pei hao Su, David Vandyke, and Steve J.
Young. 2015. Semantically conditioned lstm-based
natural language generation for spoken dialogue sys-
tems. In EMNLP, pages 1711‚Äì1721.

[Weston et al.2015] Jason Weston, Antoine Bordes, Sumit
Chopra, and Tomas Mikolov. 2015. Towards ai-
complete question answering: A set of prerequisite toy
tasks. CoRR, abs/1502.05698.

[Zaremba and Sutskever2014] Wojciech Zaremba and
Ilya Sutskever. 2014. Learning to execute. CoRR,
abs/1410.4615.

[Zeiler2012] Matthew D. Zeiler. 2012. ADADELTA: an
adaptive learning rate method. CoRR, abs/1212.5701.

35


