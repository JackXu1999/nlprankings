



















































The Interplay of Semantics and Morphology in Word Embeddings


Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 422–426,
Valencia, Spain, April 3-7, 2017. c©2017 Association for Computational Linguistics

The Interplay of Semantics and Morphology in Word Embeddings

Oded Avraham and Yoav Goldberg
Computer Science Department

Bar-Ilan University

Ramat-Gan, Israel

{oavraham1,yoav.goldberg}@gmail.com

Abstract

We explore the ability of word embed-
dings to capture both semantic and mor-
phological similarity, as affected by the
different types of linguistic properties
(surface form, lemma, morphological tag)
used to compose the representation of each
word. We train several models, where
each uses a different subset of these prop-
erties to compose its representations. By
evaluating the models on semantic and
morphological measures, we reveal some
useful insights on the relationship between
semantics and morphology.

1 Introduction

Word embedding models learn a space of continu-
ous word representations, in which similar words
are expected to be close to each other. Tradi-
tionally, the term similar refers to semantic sim-
ilarity (e.g. walking should be close to hiking,
and happiness to joy), hence the model perfor-
mance is usually evaluated using semantic simi-
larity datasets. Recently, several works introduced
morphology-driven models motivated by the poor
performance of traditional models on morpholog-
ically complex words. Such words are often rare,
and there is not enough evidence to model them
correctly. The morphology-driven models allow
pooling evidence from different words which have
the same base form. These models work by learn-
ing per-morpheme representations rather than just
per-word ones, and compose the representing vec-
tor of each word from those of its morphemes – as
derived from a supervised or unsupervised mor-
phological analysis – and (optionally) its surface
form (e.g. walking = f(vwalk, ving, vwalking)).

The works differ in the way they acquire mor-
phological knowledge (from using linguistically

derived morphological analyzers on one end, to
approximating morphology using substrings while
relying on the concatenative nature of morphol-
ogy, on the other) and in the model form (cDSMs
(Lazaridou et al., 2013), RNN (Luong et al.,
2013), LBL (Botha and Blunsom, 2014), CBOW
(Qiu et al., 2014), SkipGram (Soricut and Och,
2015; Bojanowski et al., 2016), GGM (Cotterell
et al., 2016)). But essentially, they all show that
breaking a word into morphological components
(base form, affixes and potentially also the com-
plete surface form), learning a vector for each
component, and representing a word as a composi-
tion of these vectors improves the models semantic
performance, especially on rare words.

In this work we argue that these models capture
two distinct aspects of word similarity, semantic
(e.g. sim(walking, hiking) > sim(walking, eating))
and morphological (e.g. sim(walking, hiking) >
sim(walking, hiked)), and that these two aspects
are at odds with each other (should sim(walking,
hiking) be lower or higher than sim(walking,
walked)?). The base form component of the com-
positional models is mostly responsible for seman-
tic aspects of the similarity, while the affixes are
mostly responsible for morphological similarity.

This analysis brings about several natural ques-
tions: is the combination of semantic and morpho-
logical components used in previous work ideal
for every purpose? For example, if we exclude
the morphological component from the represen-
tations, wouldn’t it improve the semantic perfor-
mance? What is the contribution of using the
surface form? And do the models behave dif-
ferently on common and rare words? We ex-
plore these questions in order to help the users of
morphology-driven models choose the right con-
figuration for their needs: semantic or morpholog-
ical performance, on common or rare words.

422



We compare different configurations of
morphology-driven models, while controlling for
the components composing the representation.
We then separately evaluate the semantic and
morphological performance of each model, on
rare and on common words. We focus on inflec-
tional (rather than derivational) morphology. This
is due to the fact that derivations (e.g. affected→
unaffected) often drastically change the meaning
of the word, and therefore the benefit of having
similar representations for words with the same
derivational base is questionable, as discussed by
Lazaridou et al (2013) and Luong et al (2013).
Inflections (e.g. walked → walking), in contrast,
preserve the word lexical meaning, and only
change its grammatical categories values.

Our experiments are performed on Modern He-
brew, a language with rich inflectional morpho-
logical system. We build on a recently intro-
duced evaluation dataset for semantic similarity in
Modern Hebrew (Avraham and Goldberg, 2016),
which we further extend with a collection of rare
words. We also create datasets for morphologi-
cal similarity, for common and rare words. He-
brew’s morphology is not concatenative, so un-
like most previous work we do not break the
words into base and affixes, but instead rely on a
morphological analyzer and represent words using
their lemmas (corresponding to the base form) and
their morphological tags (from which the mor-
phological forms are derived, corresponding to af-
fixes). This allow us to have a finer grained con-
trol over the composition, separating inflectional
from derivational processes. We also compare to
a strong character ngram based model, that mixes
the different components and does not allow finer-
grained distinctions.

We observe a clear trade-off between the mor-
phological and semantic performance – models
that excel on one metric perform badly on the
other. We present the strengths and weaknesses
of the different configurations, to help the users
choose the one that best fits their needs. To the
best of our knowledge, this work is the first to
make a comprehensive comparison between var-
ious configurations of morphology-driven mod-
els,1 as well as the first to evaluate both seman-

1Among the previous work mentioned above, only few
explored configurations other than (base + affixes) or (sur-
face + base + affixes). Lazaridou et al (2013) and Luong et
al (2013) trained models which represent a word by its base
only, and showed that these models performs worse than the

tic and morphological performance of such mod-
els. While our experiments focus on Modern He-
brew due to the availability of a reliable semantic
similarity dataset, we believe our conclusions hold
more generally.

2 Models

Our model form is a generalization of the fast-
Text model (Bojanowski et al., 2016), which in
turn extends the skip-gram model of Mikolov et
al (2013). The skip-gram model takes a sequence
of words w1, ..., wT and a function s assigning
scores to (word, context) pairs, and maximizes

T∑
t=1

 ∑
wc∈Ct

`(s(wt, wc)) +
∑

w′c∈Nt
`(−s(wt, w′c))


where ` is the log-sigmoid loss function, Ct is a
set of context words, and Nt is a set of negative
examples sampled from the vocabulary. s(wt, wc)
is defined as s(wt, wc) = u>wtvwc (where uwt and
vwc are the embeddings of the focus and the con-
text words).

Bojanowski et al (2016) replace the word rep-
resentation vwt with the set of character ngrams
appearing in it: vwc =

∑
g∈G(wt) vg where G(wt)

is the set of n-grams appearing in wt. The n-grams
are used to approximate the morphemes in the tar-
get word.

We generalize Bojanowski et al (2016) by re-
placing the set of ngrams G(w) with a set P(w)
of explicit linguistic properties. Each word wt
is then composed as the sum of the vectors of
its linguistic properties: vwt =

∑
p∈P(wt) vp.

The linguistic properties we consider are the sur-
face form of the word (W), it’s lemma (L) and
its morphological tag (M)2. The lemma corre-
sponds to the base-form, and the morphologi-
cal tag encodes the grammatical properties of the
word, from which its inflectional affixes are de-
rived (a similar approach was taken by Cotterell
and Schütze (2015)). Moving from a set of n-
grams to a set of explicit linguistic properties, al-
lows finer control of the kinds of information in
compositional ones (base + affixes). However, the poor re-
sults for the base-only models were mainly attributed to un-
desirable capturing of derivational similarity, e.g. (affected,
unaffected). Working with a more linguistically informed
morphological analyzer allows us to tease apart inflectional
from derivational processes, leading to different results.

2The lemma and morphological tag for a word in context
are obtained using a morphological analyzer and disambigua-
tor. Then, each value of lemma/tag/surface from is associated
with a trainable embedding vector.

423



the word representation. We train models with dif-
ferent subsets of {W, L, M}.
3 Experiments and Results

Our implementation is based on the fastText3 li-
brary (Bojanowski et al., 2016), which we modify
as described above. We train the models on the
Hebrew Wikipedia (∼4M sentences), using a win-
dow size of 2 to each side of the focus word, and
dimensionality of 200. We use the morphologi-
cal disambiguator of Adler (2007) to assign words
with their morphological tags, and the inflection
dictionary of MILA (Itai and Wintner, 2008) to
find their lemmas. For example, for the words
נסתכל ([we will] look [at]), הסתכלה ([she] looked
[at]) and הסתכל ([he] looked [at]) are assigned
the tags VB.MF.P.1.FUTURE, VB.F.S.3.PAST and
VB.M.S.3.PAST respectively, and share the lemma
.הסתכל We train the models for the subsets {W},
{L}, {W, L}, {W, M} and {W, L,M}, as well
as the original fastText (n-grams) model. Finally,
we evaluate each model on several datasets, us-
ing both semantic and morphological performance
measures.4

Semantic Evaluation Measure The common
datasets for semantic similarity5 have some no-
table shortcomings as noted in (Avraham and
Goldberg, 2016; Faruqui et al., 2016; Batchkarov
et al., 2016; Linzen, 2016). We use the eval-
uation method (and corresponding Hebrew sim-
ilarity dataset) that we have introduced in a
previous work (Avraham and Goldberg, 2016)
(AG). The AG method defines an annotation task
which is more natural for human judges, result-
ing in datasets with improved annotator-agreement
scores. Furthermore, the AG’s evaluation metric
takes annotator agreement into account, by putting
less weight on similarities that have lower annota-
tor agreement.

An AG dataset is a collection of target-groups,
where each group contains a target word (e.g.
singer) and three types of candidate words: pos-
itives which are words “similar” to the target (e.g.
musician), distractors which are words “related
but dissimilar” to the target (e.g. microphone), and
randoms which are not related to the target at all

3https://github.com/facebookresearch/fastText
4Our code is available on https://github.com/

oavraham1/prop2vec, our datasets on https://
github.com/oavraham1/ag-evaluation

5E.g., WordSim353 (Finkelstein et al., 2001), RW (Luong
et al., 2013) and SimLex999 (Hill et al., 2015)

(e.g laptop). The human annotators are asked to
rank the positive words by their similarity to the
target word (distractor and random words are not
annotated by humans and are automatically ranked
below the positive words). This results in a set
of triples of a target word w and two candidate
words c1, c2, coupled with a value indicating the
confidence of ranking sim(w, c1) > sim(w, c2)
by the annotators. A model is then scored based
on its ability to correctly rank each triple, giv-
ing more weight to highly-confident triples. The
scores range between 0 (all wrong answers) to 1
(perfect match with human annotators).

We use this method on two datasets: the AG
dataset from (Avraham and Goldberg, 2016) (Se-
manticSim, containing 1819 triples), and a new
dataset we created in order to evaluate the mod-
els on rare words (similar to RW (Luong et al.,
2013)). The rare-words dataset (SemanticSim-
Rare) follows the structure of SemanticSim, but in-
cludes only target words that occur less than 100
times in the corpus. It contains a total of 163
triples, all of the type positive vs. random (we find
that for rare words, distinguishing similar words
from random ones is a hard enough task for the
models).

Morphological Evaluation Measure Cotterrel
and Schütze (2015) introduced the MorphoDistk
measure, which quantifies the amount of mor-
phological difference between a target word
and a list of its k most similar words.
We modify MorphDistk measure to derive
MorphSimk, a measure that ranges between
0 and 1, where 1 indicates total morpho-
logical compatibility. The MorphDist mea-
sure is defined as: MorphoDistk(w) =∑

w′∈Kw minmw,mw′ dh(mw, mw′) where Kw is
the set of top-k similarities of w, mw and mw′
are possible morphological tags of w and w′ re-
spectively (there may be more than one possi-
ble morphological interpretation per word), and
dh is the Hamming distance between the morpho-
logical tags. MorphoDist counts the total num-
ber of incompatible morphological components.
MorphSimk calculates the average rate of com-
patible morphological values. More formally,
MorphoSimk(w) = 1 − MorphoDistk(w)k·|mw| , where
|mw| is the number of grammatical components
specified in w’s morphological tag.

We use k=10 and calculate the average Mor-
phoSim score over 100 randomly chosen words.

424



1st 2nd 3rd
W gaze:VB.F.S.3.PAST:הביטה smile:VB.F.S.3.PAST:חייכה cry:VB.F.S.3.PRESENT:מתייפחת
L gaze:VB.F.S.2.IMPERATIVE:הביטי watch:VB.M.S.3.PAST:התבונן stare:VB.MF.P.3.PAST:בהו

WL gaze:VB.MF.P.1.FUTURE:נביט watch:VB.F.S.3.PAST:התבוננה stare:VB.F.S.3.PRESENT:בוהה
WM smile:VB.F.S.3.PAST:חייכה injure:VB.F.S.3.PAST:נחבלה blow:VB.F.S.3.PAST:נשפה
LM gaze:VB.F.S.3.PAST:הביטה watch:VB.F.S.3.PAST:התבוננה move:VB.F.S.3.PAST:זזה

WLM gaze:VB.F.S.3.PAST:הביטה watch:VB.F.S.3.PAST:התבוננה walk:VB.F.S.3.PAST:פסעה

Table 1: Top-3 similarities for the word הסתכלה ([she] looked [at]).
Each entry is of the form [word:lexical meaning:morphological tag]. Green-colored items share the semantic/inflection of the

target word, while red-colored indicate a divergence. In the morphological tags: M/F/MF indicate masculine/feminine/both,
P/S indicate plural/singular, 1/2/3 indicate 1st/2nd/3rd person.

To evaluate the morphological performance on
rare words, we run another benchmark (Mor-
phoSimRare) in which we calculate the average
MorphoSim score over the 35 target words of the
SemanticSimRare dataset.

Qualitative Results To get an impression of the
differences in behavior between the models, we
queried each model for the top similarities of sev-
eral words (calculated by cosine similarity be-
tween words vectors), focusing on rare words. Ta-
ble 1 presents the top-3 similarities for the word
הסתכלה ([she] looked [at]), which occurs 17 times
in the corpus, under the different models. Unsur-
prisingly, the lemma component has a positive ef-
fect on semantics, while the tag component im-
proves the morphological performance. It also
shows a clear trade-off between the two aspects
– as models which perform the best on semantics
are the worst on morphology. This behavior is rep-
resentative of the dozens of words we examined.

Quantitative Results We compare the different
models on the different measures, and also com-
pare to the state-of-the-art n-gram based fastText
model of Bojanowski et al (2016) that does not
require morphological analysis. The results (Ta-
ble 2) highlight the following:

1. There is a trade-off between semantic and mor-
phological performance – improving one aspect
comes at the expense of the other: the lemma com-
ponent improves semantics but hurts morphology,
while the opposite is true for the tag component.
The common practice of using both components
together is a kind of compromise: the LM, WLM
and n-grams models are not the best nor the worst
on any measure.

2. The impacts of the lemma and the tag com-
ponents are much larger when dealing with rare

SS SSR MS MSR
W 0.707 0.675 0.626 0.569
L 0.713 0.816 0.491 0.339

WL 0.719 0.785 0.602 0.501
WM 0.687 0.528 0.907 1
LM 0.707 0.693 0.887 0.996

WLM 0.716 0.748 0.882 1
n-grams 0.712 0.767 0.71 0.866

Table 2: Results on SemanticSim (SS), SemanticSimRare
(SSR), MorphoSim (MS) and MorphoSimRare (MSR). The

best result for each measure is green, the worst is red.

words: comparing to W, WL is only 1.7% better on
SS and 3.8% worse on MS, while it’s 16.3% better
and 11.9% worse on SSR and MSR (respectively).
Similarly, WM is only 2.8% worse than W on SS
and 44.9% better on MS, while it’s 21.8% worse
and 75.7% better on SSR and MSR (respectively).

3. Simply lemmatizing the words is very effec-
tive for capturing semantic similarity. This is es-
pecially true for the rare words, in which the L
model clearly outperform all others. For the com-
mon words, we see a small drop compared to in-
cluding the surface form as well (WL, WLM). This
is attributed to cases in which some of the seman-
tics lies within the word’s morphological template,
for example: in W model, most similar words for
the masculine verb נפל (fell) are associated with
a soldier (which is a masculine noun): נהרג (was
killed), נפגע (was injured), while the similarities of
the feminine form נפלה are associated with a land
or a state (both are feminine nouns): סופחה (was
annexed), נכבשה (was occupied). In L model – נפלה
and נפל share a single, less accurate representation
(somewhat similarly to representations of ambigu-
ous words). This suggests using different compo-
sitions for common and rare words.

425



4 Conclusions
Our key message is that users of morphology-
driven models should consider the trade-off be-
tween the different components of their repre-
sentations. Since the goal of most works on
morphology-driven models was to improve se-
mantic similarity, the configurations they used
(which combine both semantic and morphological
components) were probably not the best choices:
we show that using the lemma component (either
alone or together with the surface form) is better.
Indeed, excluding the morphological component
will make the morphological similarity drop, but
it’s not necessarily a problem for every task. One
should include the morphological component in
the embeddings only for tasks in which morpho-
logical similarity is required and cannot be han-
dled by other means. A future work can be to per-
form an extrinsic evaluation of the different mod-
els in various downstream applications. This may
reveal which kinds of tasks benefit from morpho-
logical information, and which can be done better
by a pure semantic model.

Acknowledgements

The work was supported by the Israeli Science
Foundation (grant number 1555/15).

References
Menahem Meni Adler. 2007. Hebrew morphological

disambiguation: An unsupervised stochastic word-
based approach. Ph.D. thesis, Ben-Gurion Univer-
sity of the Negev.

Oded Avraham and Yoav Goldberg. 2016. Improving
reliability of word similarity evaluation by redesign-
ing annotation task and performance measure. ACL
Workshop on Evaluating Vector Space Representa-
tions for NLP, page 106.

Miroslav Batchkarov, Thomas Kober, Jeremy Reffin,
Julie Weeds, and David Weir. 2016. A critique of
word similarity as a method for evaluating distribu-
tional semantic models. ACL Workshop on Evaluat-
ing Vector Space Representations for NLP, page 7.

Piotr Bojanowski, Edouard Grave, Armand Joulin,
and Tomas Mikolov. 2016. Enriching word vec-
tors with subword information. arXiv preprint
arXiv:1607.04606.

Jan A Botha and Phil Blunsom. 2014. Compositional
morphology for word representations and language
modelling. In ICML, pages 1899–1907.

Ryan Cotterell and Hinrich Schütze. 2015. Morpho-
logical word-embeddings. In Proc. of NAACL.

Ryan Cotterell, Hinrich Schütze, and Jason Eisner.
2016. Morphological smoothing and extrapolation
of word embeddings. In Proceedings of the 54th An-
nual Meeting of the Association for Computational
Linguistics, volume 1, pages 1651–1660.

Manaal Faruqui, Yulia Tsvetkov, Pushpendre Rastogi,
and Chris Dyer. 2016. Problems with evaluation of
word embeddings using word similarity tasks. ACL
Workshop on Evaluating Vector Space Representa-
tions for NLP, page 30.

Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2001. Placing search in context: The
concept revisited. In Proceedings of the 10th inter-
national conference on World Wide Web, pages 406–
414. ACM.

Felix Hill, Roi Reichart, and Anna Korhonen. 2015.
Simlex-999: Evaluating semantic models with (gen-
uine) similarity estimation. Computational Linguis-
tics, 41(4).

Alon Itai and Shuly Wintner. 2008. Language re-
sources for Hebrew. Language Resources and Eval-
uation, 42(1):75–98, March.

Angeliki Lazaridou, Marco Marelli, Roberto Zampar-
elli, and Marco Baroni. 2013. Compositional-ly
derived representations of morphologically complex
words in distributional semantics. In ACL (1), pages
1517–1526. Citeseer.

Tal Linzen. 2016. Issues in evaluating semantic spaces
using word analogies. ACL Workshop on Evaluating
Vector Space Representations for NLP, page 13.

Thang Luong, Richard Socher, and Christopher D
Manning. 2013. Better word representations
with recursive neural networks for morphology. In
CoNLL, pages 104–113.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.

Siyu Qiu, Qing Cui, Jiang Bian, Bin Gao, and Tie-Yan
Liu. 2014. Co-learning of word representations and
morpheme representations. In COLING, pages 141–
150.

Radu Soricut and Franz Och. 2015. Unsupervised
morphology induction using word embeddings. In
Proc. NAACL.

426


