



















































Learning to Ask for Conversational Machine Learning


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 4164–4174,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

4164

Learning to Ask for Conversational Machine Learning

Shashank Srivastava1∗ Igor Labutov2 Tom Mitchell3
1UNC Chapel Hill, 2LAER AI

3Machine Learning Department, Carnegie Mellon University
ssrivastava@cs.unc.edu, igor.labutov@laer.ai, tom.mitchell@cmu.edu

Abstract

Natural language has recently been increas-
ingly explored as a medium of supervision for
training machine learning models. Here, we
explore learning classification tasks using lan-
guage in a conversational setting – where the
automated learner does not simply receive lan-
guage input from a teacher, but can proactively
engage the teacher by asking template-based
questions. We experiment with a reinforce-
ment learning framework, where the learner’s
actions correspond to question types and the
reward for asking a question is based on how
the teacher’s response changes performance
of the resulting machine learning model on
the learning task. In this framework, learning
good question-asking strategies corresponds to
asking sequences of questions that maximize
the cumulative (discounted) reward, and hence
quickly lead to effective classifiers. Empirical
analysis shows that learned question-asking
strategies can expedite classifier training by
asking appropriate questions at different points
in the learning process. The approach allows
learning using a blend of strategies, including
learning from observations, explanations and
clarifications.

1 Introduction

The ability to learn new tasks and behaviors from
language is characteristic of human intelligence.
In recent years, the fields of machine learning and
NLP have seen an renewed interest in incorpo-
rating natural language supervision in models of
machine intelligence (Narasimhan et al., 2015; El-
hoseiny et al., 2013; Goldwasser and Roth, 2014;
Andreas et al., 2018; Fried et al., 2018; Wang
et al., 2016). In particular, methods such as Bab-
bleLabble (Hancock et al., 2018) and LNL (Srivas-
tava et al., 2017) show progress towards realistic

∗* Work done while the first and second authors were at
CMU

Identify all important emails in my 
inbox

Can you give me an explanation of
what important emails might look like?

Emails from nlp-corp.com are usually 
important

Did you mean that emails where the
sender’s address has a string match with
“nlp-corp.com” are usually important?

Yes, that’s right

Can you label this new email as 
important/not important?

This email is not important

Asking for 
Explanations

Requesting 
clarification

Seeking 
labels

Figure 1: Question-Answer dialog can enable learning
from a mix of strategies, including label observations
(traditional supervised learning), explanations and clar-
ifications (to overcome parsing limitations). The out-
put from the teacher-learner interaction is a classifica-
tion model (here, for important emails). We present a
framework that (a) enables learning classifiers from a
mix of such supervision; (b) learns to ask appropriate
sequences of questions to accelerate this.

applications of supervised learning from language
on tasks such as information extraction and email
categorization. However, until now, such methods
have been limited in two ways.

First, despite a body of work on leveraging lan-
guage for tasks involving human robot interac-
tion (She and Chai, 2017; Cakmak and Thomaz,
2012; Krishnamurthy and Kollar, 2013) and inter-
active learning in non-linguistic settings (see Sec-
tion 2), existing approaches for training machine
learning models from language are largely non-
interactive, i.e. the learner agent receives stati-
cally collected text-based advice from a teacher
as input, but does not directly engage with the



4165

teacher.1 In comparison, when humans learn, they
do not rely only on passively receiving instruc-
tion from a teacher. Rather, the interaction takes
the form of a mixed-initiative dialog, where they
ask questions and proactively seek clarifications to
simplify learning. These questions can generalize
learning to novel situations, explore hypotheses, or
fill information gaps. The ability to ask questions
can, thus, fundamentally facilitate learning.

Second, existing approaches have focused on
using language either as a standalone replacement
for labeled data (Hancock et al., 2018), or to drive
learning such as through specifying features for
learning tasks (Eisenstein et al., 2009). In contrast,
many realistic scenarios of learning from language
would involve not learning from language alone,
but learning from a mix of supervision, including
both traditional labeled data, and natural language
advice. Thus, automated learners should be capa-
ble of learning from a blend of observations, ex-
planations and clarification.

In this work, we introduce a framework for
learning from language in a conversational set-
ting (LiD, for Learning with Interactive Dialog),
which is a step towards alleviating these short-
comings. Language provides a natural medium
for conversational interactions between a learner
and a teacher, specifically in the form of question-
answer dialog. The premise driving our work is
that the ability to ask questions can be leveraged
by an automated learner to accelerate its learning.
We explore a data-driven approach for learning ef-
fective question-asking strategies in the specific
context of learning classification tasks. The sig-
nal for learning to ask questions is grounded in
the learning task itself. i.e., the value of a ques-
tion is evaluated in utilitarian terms of how it af-
fects performance on a downstream classification
task. This follows a Wittgensteinian view of lan-
guage as a cooperative game (Wittgenstein, 1953)
between agents (here, the teacher and a learner)
with a shared goal (here, building an effective clas-
sifier). While the space of questions that an inter-
active learner can ask can be vast in general, here
we specifically focus on leveraging interactivity
for three specific aspects (highlighted in Figure 1):

1. Seeking labels for specific examples.

1 Zhang et al. (2018) diverge from prior work in this re-
spect, and model language games between teachers and learn-
ers. However, their learning tasks are toylike, and the method
does not generalize to realistic scenarios.

2. Asking for explanations of a concept.
3. Requesting clarifications about explanations.

As illustrated in Figure 1, these dimensions can
facilitate multiple aspects of the learning process:
including learning from labeled examples (similar
to traditional supervised learning), learning from
natural language explanations (similar to recent
work on learning from explanations) and allevi-
ating limitations in the learner’s semantic parsing
abilities (in vein with work such as Labutov et al.
(2018)). Learning systems that reify these abilities
can enable users to interactively teach new con-
cepts using a blend of traditional and language-
based supervision. Our contributions are:

• A reinforcement learning formulation to guide
question-asking strategies for learning from lan-
guage.

• A method for interactively training classifiers
using a mix of labeled data, natural language
explanations and clarifications. Our exploration
highlights some of the challenges involved in in-
teractive learning from language.

2 Challenges in Relation to Previous
Work

From the perspective of traditional supervised
learning, the problem of asking questions can be
seen as cognate with active learning. Methods in
active learning have explored various criteria for
choosing which of a set of unlabeled examples
to label next while training supervised machine
learning models (Settles, 2012; Collins et al.,
2008). This can be seen as asking a specific kind
of question (as illustrated in Figure 1). Learning to
ask questions generalizes active learning in multi-
ple ways by possibly soliciting a wider range of
data measurements. These include feature labels
(‘Are emails with subject “urgent” usually impor-
tant?’), label proportions (‘Around what fraction
of emails are important?’), constraints on model
expectations (‘Are you more likely to reply to im-
portant emails?’), etc. Approaches such as Srivas-
tava et al. (2018) map such language to data mea-
surements that computational models can reason
over.2 Statistical frameworks such as Generalized
Expectation (Druck et al., 2008), Posterior Reg-
ularization (Ganchev et al., 2010) and Bayesian

2For example, a statement such as ‘Emails from my boss
are usually important’ may be mapped to a data measurement
of form P (y = important|sender = boss) ≈ Pusually .



4166

Measurements (Liang et al., 2009) then allow for
model training from a broad range of such data
measurements in conjunction with unlabeled data,
rather than using labeled examples. Other recent
approaches such as Huang et al. (2015) and Sid-
diquie and Gupta (2010) have expanded pool-
based active learning to learning from multiple
types of queries, especially in the context of multi-
label and multi-class learning. Similarly, Parikh
and Grauman (2011) explore feature space con-
struction for visual tasks in an interactive setting.
Although in principle soliciting different types of
data measurements can help learning, each type
requires its own interface. The advantage of using
natural language as a medium is that it allows us
to unify the different modes of supervision into a
single, familiar user interface. However, using nat-
ural language as a medium of supervision comes
with its own set of challenges, as we discuss next.

2.1 Dependence on Language Interpreter

Since both generation and transmission of lan-
guage advice can be noisy, the optimal question
asking strategy may depend not only on the infor-
mation content of data measurements, but also fac-
tors such as the quality of the learner’s semantic
parsing model and the teacher’s skill.3 To explain,
while useful from an information theoretic sense, a
teacher’s explanations may be too complex to han-
dle for the learning agent’s parser, in which case
it might be preferable to stick to asking about the
teacher about instance labels (which would require
minimal parsing). Thus, question-asking strate-
gies need to be sensitive to the learner’s own se-
mantic parsing ability, which may also change dur-
ing the course of interactions with users.

2.2 Context Dependence

Rather than learning a static criterion for choosing
what question to ask (as in active learning), our
focus is on asking questions in conversational set-
tings, which are inherently dynamic processes. To
explain, asking a teacher to rephrase an explana-
tion only makes sense in specific contexts (when
the interpretation of something said previously is
unclear). Further, the question to ask can depend
on factors such as the task domain, supervision

3In this work, we are not interested in learning seman-
tic parsing models. We presume the existence of pretrained
semantic parsers for learning agents. Our focus is rather on
whether some question asking strategies may be more effec-
tive than others for a learning agent with given capabilities.

previously received, etc. These factors motivated
our choice of a reinforcement learning approach
for learning question-asking strategies.

Relation to Question Generation approaches:
The problem of learning to ask questions has pre-
viously been explored by several approaches. Van-
derwende (2008) and Olney et al. (2012) explore
generating reading comprehension questions con-
ditioned on a given text. More recently, Romeo
et al. (2016) and Rao and Daumé III (2018)
present neural network models that rank questions
in community QA forums, whereas Misra et al.
(2018) generate questions for visual scene un-
derstanding. Our framework significantly differs
from these in its sequential framework, and that
the questions to be asked are grounded in quanti-
tative performance on a downstream task.

3 Approach

In this section, we describe our framework for
interactive learning from question-answer dialog.
We first describe an approach to learn classifiers
using a mix of explanations and labeled exam-
ples in Section 3.1. This is a preliminary to-
wards question-asking strategies that subsume ac-
tive learning as well as language advice; and con-
stitutes a subroutine that is repeatedly invoked in
our approach. Section 3.2 describes our reinforce-
ment learning formulation for learning question-
asking strategies in simulated conversational set-
tings. The space of actions consists of a vocabu-
lary of question types that a learner can ask, and
reward is based on improvements in the classifica-
tion model that the teacher’s response to an asked
question leads to.

3.1 Learning classifiers from a mix of
observations and explanations

We base our learning framework on previous work
by Srivastava et al. (2018), who train loglinear
classifiers (with parameters θ) using natural lan-
guage explanations of the individual classes and
unlabeled data. Further, they use the semantics of
linguistic quantifiers (such as ‘usually’,‘always’,
etc.) as priors in a Posterior Regularization objec-
tive to drive the model training. In particular, their
training objective takes the following form:

JQ(θ) = L(θ)︸︷︷︸
Explain data

−min
q∈Q

KL(q | pθ(Y |X))︸ ︷︷ ︸
Emulate human advice

(1)



4167

Identify all important emails in my inbox

Can you give me an explanation of what 
important emails might look like?

Emails from nlp-corp.com are usually 
important

Did you mean that emails where the 
sender’s address has a string match with 

“nlp-corp.com” are usually important?

Yes, that’s right

Can you label this new email as 
important/not important?

This email is not important

rt
<latexit sha1_base64="Bmobwtq6d1VdnrQp0oh1JwbYUlg=">AAAB6nicbVBNS8NAEJ3Ur1q/oh69LBbBU0lE0GPRi8eK9gPaUDbbTbt0swm7E6GE/gQvHhTx6i/y5r9x2+agrQ8GHu/NMDMvTKUw6HnfTmltfWNzq7xd2dnd2z9wD49aJsk0402WyER3Qmq4FIo3UaDknVRzGoeSt8Px7cxvP3FtRKIecZLyIKZDJSLBKFrpQfex71a9mjcHWSV+QapQoNF3v3qDhGUxV8gkNabreykGOdUomOTTSi8zPKVsTIe8a6miMTdBPj91Ss6sMiBRom0pJHP190ROY2MmcWg7Y4ojs+zNxP+8bobRdZALlWbIFVssijJJMCGzv8lAaM5QTiyhTAt7K2EjqilDm07FhuAvv7xKWhc136v595fV+k0RRxlO4BTOwYcrqMMdNKAJDIbwDK/w5kjnxXl3PhatJaeYOYY/cD5/AGgYjd0=</latexit><latexit sha1_base64="Bmobwtq6d1VdnrQp0oh1JwbYUlg=">AAAB6nicbVBNS8NAEJ3Ur1q/oh69LBbBU0lE0GPRi8eK9gPaUDbbTbt0swm7E6GE/gQvHhTx6i/y5r9x2+agrQ8GHu/NMDMvTKUw6HnfTmltfWNzq7xd2dnd2z9wD49aJsk0402WyER3Qmq4FIo3UaDknVRzGoeSt8Px7cxvP3FtRKIecZLyIKZDJSLBKFrpQfex71a9mjcHWSV+QapQoNF3v3qDhGUxV8gkNabreykGOdUomOTTSi8zPKVsTIe8a6miMTdBPj91Ss6sMiBRom0pJHP190ROY2MmcWg7Y4ojs+zNxP+8bobRdZALlWbIFVssijJJMCGzv8lAaM5QTiyhTAt7K2EjqilDm07FhuAvv7xKWhc136v595fV+k0RRxlO4BTOwYcrqMMdNKAJDIbwDK/w5kjnxXl3PhatJaeYOYY/cD5/AGgYjd0=</latexit><latexit sha1_base64="Bmobwtq6d1VdnrQp0oh1JwbYUlg=">AAAB6nicbVBNS8NAEJ3Ur1q/oh69LBbBU0lE0GPRi8eK9gPaUDbbTbt0swm7E6GE/gQvHhTx6i/y5r9x2+agrQ8GHu/NMDMvTKUw6HnfTmltfWNzq7xd2dnd2z9wD49aJsk0402WyER3Qmq4FIo3UaDknVRzGoeSt8Px7cxvP3FtRKIecZLyIKZDJSLBKFrpQfex71a9mjcHWSV+QapQoNF3v3qDhGUxV8gkNabreykGOdUomOTTSi8zPKVsTIe8a6miMTdBPj91Ss6sMiBRom0pJHP190ROY2MmcWg7Y4ojs+zNxP+8bobRdZALlWbIFVssijJJMCGzv8lAaM5QTiyhTAt7K2EjqilDm07FhuAvv7xKWhc136v595fV+k0RRxlO4BTOwYcrqMMdNKAJDIbwDK/w5kjnxXl3PhatJaeYOYY/cD5/AGgYjd0=</latexit><latexit sha1_base64="Bmobwtq6d1VdnrQp0oh1JwbYUlg=">AAAB6nicbVBNS8NAEJ3Ur1q/oh69LBbBU0lE0GPRi8eK9gPaUDbbTbt0swm7E6GE/gQvHhTx6i/y5r9x2+agrQ8GHu/NMDMvTKUw6HnfTmltfWNzq7xd2dnd2z9wD49aJsk0402WyER3Qmq4FIo3UaDknVRzGoeSt8Px7cxvP3FtRKIecZLyIKZDJSLBKFrpQfex71a9mjcHWSV+QapQoNF3v3qDhGUxV8gkNabreykGOdUomOTTSi8zPKVsTIe8a6miMTdBPj91Ss6sMiBRom0pJHP190ROY2MmcWg7Y4ojs+zNxP+8bobRdZALlWbIFVssijJJMCGzv8lAaM5QTiyhTAt7K2EjqilDm07FhuAvv7xKWhc136v595fV+k0RRxlO4BTOwYcrqMMdNKAJDIbwDK/w5kjnxXl3PhatJaeYOYY/cD5/AGgYjd0=</latexit>

+2%

0%

+1%

Semantic 
Parser

Example/Data 
measurement

Update classifier (   ) Evaluate performance✓
<latexit sha1_base64="0hx53wE9o81N9wB5RihMEgjgI3U=">AAAB7XicbVDLSgNBEJyNrxhfUY9eBoPgKeyKoMeAF48R8oJkCbOT3mTM7Mwy0yuEkH/w4kERr/6PN//GSbIHTSxoKKq66e6KUiks+v63V9jY3NreKe6W9vYPDo/KxyctqzPDocm11KYTMQtSKGiiQAmd1ABLIgntaHw399tPYKzQqoGTFMKEDZWIBWfopFYPR4CsX674VX8Buk6CnFRIjnq//NUbaJ4loJBLZm038FMMp8yg4BJmpV5mIWV8zIbQdVSxBGw4XVw7oxdOGdBYG1cK6UL9PTFlibWTJHKdCcORXfXm4n9eN8P4NpwKlWYIii8XxZmkqOn8dToQBjjKiSOMG+FupXzEDOPoAiq5EILVl9dJ66oa+NXg4bpSa+RxFMkZOSeXJCA3pEbuSZ00CSeP5Jm8kjdPey/eu/exbC14+cwp+QPv8wepYY86</latexit><latexit sha1_base64="0hx53wE9o81N9wB5RihMEgjgI3U=">AAAB7XicbVDLSgNBEJyNrxhfUY9eBoPgKeyKoMeAF48R8oJkCbOT3mTM7Mwy0yuEkH/w4kERr/6PN//GSbIHTSxoKKq66e6KUiks+v63V9jY3NreKe6W9vYPDo/KxyctqzPDocm11KYTMQtSKGiiQAmd1ABLIgntaHw399tPYKzQqoGTFMKEDZWIBWfopFYPR4CsX674VX8Buk6CnFRIjnq//NUbaJ4loJBLZm038FMMp8yg4BJmpV5mIWV8zIbQdVSxBGw4XVw7oxdOGdBYG1cK6UL9PTFlibWTJHKdCcORXfXm4n9eN8P4NpwKlWYIii8XxZmkqOn8dToQBjjKiSOMG+FupXzEDOPoAiq5EILVl9dJ66oa+NXg4bpSa+RxFMkZOSeXJCA3pEbuSZ00CSeP5Jm8kjdPey/eu/exbC14+cwp+QPv8wepYY86</latexit><latexit sha1_base64="0hx53wE9o81N9wB5RihMEgjgI3U=">AAAB7XicbVDLSgNBEJyNrxhfUY9eBoPgKeyKoMeAF48R8oJkCbOT3mTM7Mwy0yuEkH/w4kERr/6PN//GSbIHTSxoKKq66e6KUiks+v63V9jY3NreKe6W9vYPDo/KxyctqzPDocm11KYTMQtSKGiiQAmd1ABLIgntaHw399tPYKzQqoGTFMKEDZWIBWfopFYPR4CsX674VX8Buk6CnFRIjnq//NUbaJ4loJBLZm038FMMp8yg4BJmpV5mIWV8zIbQdVSxBGw4XVw7oxdOGdBYG1cK6UL9PTFlibWTJHKdCcORXfXm4n9eN8P4NpwKlWYIii8XxZmkqOn8dToQBjjKiSOMG+FupXzEDOPoAiq5EILVl9dJ66oa+NXg4bpSa+RxFMkZOSeXJCA3pEbuSZ00CSeP5Jm8kjdPey/eu/exbC14+cwp+QPv8wepYY86</latexit><latexit sha1_base64="0hx53wE9o81N9wB5RihMEgjgI3U=">AAAB7XicbVDLSgNBEJyNrxhfUY9eBoPgKeyKoMeAF48R8oJkCbOT3mTM7Mwy0yuEkH/w4kERr/6PN//GSbIHTSxoKKq66e6KUiks+v63V9jY3NreKe6W9vYPDo/KxyctqzPDocm11KYTMQtSKGiiQAmd1ABLIgntaHw399tPYKzQqoGTFMKEDZWIBWfopFYPR4CsX674VX8Buk6CnFRIjnq//NUbaJ4loJBLZm038FMMp8yg4BJmpV5mIWV8zIbQdVSxBGw4XVw7oxdOGdBYG1cK6UL9PTFlibWTJHKdCcORXfXm4n9eN8P4NpwKlWYIii8XxZmkqOn8dToQBjjKiSOMG+FupXzEDOPoAiq5EILVl9dJ66oa+NXg4bpSa+RxFMkZOSeXJCA3pEbuSZ00CSeP5Jm8kjdPey/eu/exbC14+cwp+QPv8wepYY86</latexit>

Figure 2: We assume that the dialog between the learner and the teacher is in the form of turn-wise conversations –
consisting of a sequence of questions asked by the learner, and the teacher’s responses to those questions. At each
step in this process, the teacher’s response is parsed by the learner (using a pre-trained semantic parser), and can
be incorporated into the learner’s concept model as either a labeled example or a data measurement (the learner
can also choose to seek a clarification). A reward (denoted by rt) can be computed at each step, which denotes
the marginal change in classification performance on a held-out set of examples due to the last response. In this
framework, learning good question-asking strategies corresponds to asking sequences of questions that maximize
the cumulative (discounted) reward, and hence quickly lead to effective concept models. The framework also
allows for asking sequences of multiple questions before seeing a major jump in model performance.

The objective reflects a tension between explain-
ing the unlabeled data (likelihood term) and em-
ulating the natural language advice provided by a
teacher. The KL divergence represents difference
between predictions from the trained model on un-
labeled data pθ(Y |X) and language advice (each
explanation is incorporated as a data measure-
ment; the conjunction of these defines the ‘valid
set’ of posterior distributionsQ that perfectly con-
cur with the natural language advice). The second
term essentially computes the minimum distance
between the model posterior and the set Q.

Here, we show that we can naturally extend this
approach to learn classifiers from a mix of both
labeled and unlabeled data, and natural language
explanations. To do this, we simply append a log-
likelihood term for the labeled examples to the ob-
jective in Equation 1. The updated objective is:

JQ(θ) = Llabeled(θ)

+ µ
(
Lunlabeled(θ)−min

q∈Q
KL(q | pθ(Y |X))

)
(2)

Here, Llabeled(θ) denotes the log-likelihood

term for a set of nlabeled labeled examples
Xlabeled = {(xk, yk)}nlabeled (normalized by
nlabeled), whereas the other two terms are as be-
fore: Lunlabeled(θ) denoting log-likelihood over a
set of nunlabeled unlabeled examples, and a poste-
rior regularizer term (KL-divergence) penalizing
violations of the parse natural language advice. In
the E-step of the Posterior Regularization train-
ing (Ganchev et al., 2010), the computation of the
posterior regularizer remains unchanged. How-
ever, the M-step is modified so that the classifier
parameters θ are learned using both the inferred
labels for the unlabeled data, and provided labels
for the labeled examples.

In Equation 2, µ > 0 determines the relative
weights of provided example labels and natural
language advice in the optimization objective and
is a hyper-parameter for the method. In learn-
ing scenarios where there is little labeled data, we
would like to rely primarily on constraints speci-
fied from natural language explanations, and un-
labeled data. On the other hand, in scenarios
where there is a lot of labeled data available en-
abling robust inductive inference, we would like



4168

to primarily rely on it rather than explanations.4

While setting up the optimization problem, the
value of µ can be adapted to reflect this intu-
ition. In our experiments, we found setting µ =
1/max(nlabeled, 1) to work well across settings.

3.2 RL formulation for learning to ask

Figure 2 illustrates our framework for learning
classification tasks in a question-answer dialog
setting. We assume the presence of a teacher to
answer questions posed by the learner. We restrict
the structure of dialog to a sequence of questions
(q1 . . . qT ) asked by the learner, and the teacher’s
responses (u1 . . . uT ) to them. We further assume
the presence of a held-out set of labeled examples
of the concept, which can be used to evaluate the
learner’s classification performance as the dialog
progresses. At each step t, the learner’s action at
consists of choosing a question to ask the teacher.
The teacher’s response to the learner’s question is
parsed (in the form of a labeled example, or a data
measurement), which is then incorporated into the
learner’s concept model (by retraining with the ad-
ditional labeled example or the new data measure-
ment). The classification performance, ct, of the
updated model is evaluated on the held-out set.
The change in classification performance from the
previous step, rt approximates the marginal value
of the question in learning the task, and constitutes
the learner’s reward at that step.

Our approach for learning question asking
strategies models the dialog as a simple Markov
Decision Process. Since our state and action
spaces are discrete (as described in the follow-
ing sections), we can use a table-based SARSA-
learning procedure (which allows for on-policy
learning over Q-learning) to estimate the state-
action values Q(s, a) of different question types
in different contexts. We next describe the state-
space, actions and rewards, and the learning pro-
cedure.

3.2.1 Action Space

As mentioned earlier, there can be a multitude of
questions that a learner can ask a teacher. Here,
we are interested in exploring three specific types
of questions that are specially germane for facili-
tating learning from a mix of labeled examples and
explanations. These consist of the following:

4In the asymptotic case of infinite data with labels, an in-
ductively learned Bayes Classifier would be optimal.

1. Seeking labels for specific examples: This is
similar to traditional active learning. In partic-
ular, we can have a different action correspond-
ing to every active learning criterion, which
chooses which example to label next. In our
experiments, we use two active learning tech-
niques (with a corresponding action for each):

• Random: Ask for class label for a randomly
chosen unlabeled instance.
• Maximum Uncertainty: Ask for class label

for the instance in the data for which the cur-
rent concept model is most uncertain (high-
est entropy). If there are multiple such in-
stances, randomly pick one among them.

2. Asking for an explanation for the concept:
This action seeks out from the teacher a short
natural language explanation of the concept.
This is then incorporated in the concept model
as a quantitative constraint. In general, this can
encompass several types of questions:

• Asking for probability estimates about spe-
cific labels and features. e.g., ‘How often are
emails about meetings important?’
• Asking for discriminative features for partic-

ular concept labels. e.g., ‘Can you think of a
feature that if present always denotes that an
email is important?’
• Asking about class probabilities. e.g.,

‘Around what fraction of emails in your in-
box are important?’

In principle, each of the above provide ad-
missible constraints which the classifier train-
ing procedure (from Section 3.1) can handle.
However, to simplify analysis, in our experi-
ments, we conflate these actions into one cat-
egory, and ask for general explanations of the
form ‘Can you give me an explanation of the
concept?’, which could return a variety of data
measurements (subsuming P (y),P (y|x) and
P (x|y), previously explored in Srivastava et al.
(2018)).

3. Requesting clarification about the previous
explanation: This action asks for a clarifica-
tion about the interpretation of a previous ex-
planation (which can be helpful in cases when
the learner is uncertain about the interpreta-
tion). For this, the learner verifies if the in-
terpretation of the previous explanation (using



4169

the learner’s semantic parser) was correct or
not. To do this, we generate a question of the
form ‘Did you mean . . .’ using a synchronous
grammar which deterministically maps logical
forms to natural language descriptions (see Fig-
ure 1 for an example). The teacher responds
with yes, if the parsed logical form matches the
gold annotated logical form, and with no oth-
erwise. In case the teacher responds with no,
the current explanation is discarded (not used
in model training), and the learner moves ahead
to ask for a new label or explanation.

Simulating Interactions: We note that each of
the question types described above – (1) asking
for labels for examples, (2) asking for concept ex-
planations, and (3) verifying interpretations of lan-
guage explanations – can be simulated with corre-
sponding statically collected data – consisting of
(1) labeled examples for classification, (2) natu-
ral language explanations of classes, and (3) anno-
tations of those explanations with logical forms.
This has a significant implication: rather than re-
lying on questioning human users in real-time, we
can simulate the conversational exchange by ask-
ing questions to an oracle, which has access to pre-
viously pre-collected data of the above-described
form for each classification task. While this is
a coarse approximation of actual dialog between
an automated learner and human teachers, it can
serve as a useful proof-of-concept, and allows for
quick experimentation. We rely on this simulated
setting for learning policies for question asking.

3.2.2 Rewards
The reward, rt, evaluates the change in classifica-
tion performance due to an asked question at each
step t of the dialog. The performance of the clas-
sification model is evaluated on a held out set of
nheldout = 50 labeled examples for each learning
task. In our experiments, we use the model’s F1
score as the metric for classification performance,
ct. We define the reward as the absolute change in
model performance from the previous step:

rt = ct − ct−1 (3)

3.2.3 State-space
Next, we describe the featurized state space for
our reinforcement learning formulation. The best
question to ask at a particular point can likely
depend on the state of the conversation. This
could include factors such as the pedagogical

phase in the learning process (exploratory vs con-
firmatory), previous questions asked, etc. Thus,
defining a rich enough state space is an important
consideration for a formulation of conversational
learning. In our treatment, we assume a discrete
state-space, which is defined as the cross product
of the following (also discrete) features.

• Curricular stage in the Learning process: We
use a discrete variable to model the curricular
stage of the learner, approximating it by the
number of steps (questions previously asked) in
the interaction at any point. We cluster the num-
ber of steps in the following five bins of values:
BEGINNER (0 steps), NOVICE (1-5 steps), IN-
TERMEDIATE (6-10 steps), ADVANCED (11-15
steps) and MATURE (> 15 steps).5

• Reward in the previous state: We discretize
the value of reward as belonging to one of four
ranges (abstractly named GOOD, INCREASING,
FLAT, and DECREASING), with thresholds cho-
sen to correspond to the inter-quartile ranges for
the value of rewards observed in evaluating a
random policy.

• Velocity of reward: This is a ternary variable
indicating whether the value of the discrete vari-
able for the reward (above) is BETTER,WORSE
or the SAME than the previous step.

• Type of the previous two actions: As men-
tioned in the description of the action space.

• Domain of learning task: Indicates the do-
main of the current classification task. We
use datasets corresponding to three domains:
EMAIL CATEGORIZATION, SHAPE CLASSIFI-
CATION and BIRD SPECIES IDENTIFICATION;
hence this variable can take these three values.

• Confidence of previous parse: We model the
learner’s confidence in parsing the response ut
from a teacher as the ratio of the probabil-
ity of the highest probability (predicted) logi-
cal form from the learner’s semantic parser and
the next best logical form. We discretize this
ratio into three values, corresponding to the up-
per (HIGH), lower (LOW) and middle two inter-
quartile ranges (MEDIUM) for the value of the
ratio over all explanations in our data.
5These threshold values were heuristically chosen based

on the observation that for many datasets that we experiment
with, classifier performance roughly begins to taper at around
20 training examples.



4170

While our state space captures several facets, it
does not model some other important factors:

• Teacher behavior: Whether the teacher pro-
vides correct information, and uses easily inter-
pretable language.

• Task difficulty: This refers to how expressible
a classification task is using language explana-
tions. For example, some concept maybe sig-
nificantly easier to explain using language than
others, depending on the logical language avail-
able to the semantic parser. For example, it
maybe impossible to explain digit recognition
using pixel level features using language.

3.3 Model training
Since the action and state-spaces are discrete
and not prohibitively large, we use the on-policy
SARSA learning algorithm for policy control
(Rummery and Niranjan, 1994), where we repre-
sent the state-action Q-values for pairs of states
s and actions a as a table. We use an �-greedy
strategy (� = 0.20). i.e., the strategy balances be-
tween exploitation and exploration by picking the
next action to be the estimated optimal one (hav-
ing the maximum estimated Q-value for a state)
with probability 1 − �, and choosing the next ac-
tion randomly with a probability of �. The initial
policy is defined by uniform randomly initializing
Q(s, a) values between 0 and 1.

4 Data

Our empirical analysis uses existing datasets for
learning classifiers from language. These are
datasets for email categorization from natural lan-
guage explanations from Srivastava et al. (2017);
and bird species classification and synthetic shape
classification tasks from Srivastava et al. (2018).
In all, thesee consist of 67 classification tasks be-
longing to these three domains.6 For each task,
the corresponding data consisting of natural lan-
guage explanations of classes as well as annotated
logical forms for these explanations are available.
For each task, we hold out a random sample of
50 examples for evaluating classifier performance.
The rest of the examples (ranging between 50 and
100 for individual tasks) are considered as unla-
beled data at the start of each interaction. At each
step in a simulated interaction, either (1) the label

6The complexity of these problems varies considerably,
ranging from marginally above chance to perfectly learnable.

for a new example from this set is revealed by the
teacher, (2) a new data measurement is provided
as a natural language explanation by the teacher,
or (3) the learner seeks a clarification about the
parse of the previous explanation. Thus, from the
learner’s perspective, over time the size of the un-
labeled data decreases, and the labeled examples
increase (see Equation 2).

5 Experiments

In this section, we evaluate LiD’s performance
for three domains of classification tasks. As pre-
viously mentioned, for policy learning we sim-
ulate conversational interactions between teach-
ers and learners by asking questions to a ora-
cle, which has access to previously collected data
about each classification task. One limitation of
learning question-asking strategies from simulated
interactions is that for some classification tasks,
we may run out of explanations in the course of
model training (since the number of explanations
of a concept are limited). In these cases, we end
the interaction as soon as all explanations are al-
ready provided to the learner.

5.1 Learned vs Random policy evaluation

First, we compare learned policies for question
asking with a naive policy that randomly takes a
new action at each step in the learning process.

Figure 3 shows averaged cumulative reward for
question asking strategies on 20 unseen classifica-
tion tasks, after SARSA learning for 10 epochs on
the remaining 37 classification tasks. In the fig-
ure, the x-axis corresponds to the number of steps
in a dialog, and the y-axis denotes the cumulative
reward (averaged over 20 tasks) for a learned pol-
icy vs a random policy. We note that policy learn-
ing leads to consistently superior performance (on
average, LiD achieves any given level of classifi-
cation performance in fewer steps), which unam-
biguously indicates value in asking the right se-
quences of questions. We observed that the trend
was also similar in most individual learning tasks.
For example, the cumulative reward after 10 steps
was higher for the learned policy than the random
policy for 18 of the 20 learning tasks. The dif-
ference in performance was statistically significant
at p < 0.05 using a signed permutation test. We
characterize some learned behaviors that drive this
improved performance in Section 5.4.



4171

Figure 3: Cumulative Reward (averaged over 20 tasks)
for interactive concept learning for learned question
asking policy vs random policy.

Parsing Accuracy

%
 o

f A
ct

io
ns

 s
ee

ki
ng

 N
L 

Ex
pl

an
at

io
ns

0

0.2

0.4

0.6

00.250.50.751

Reliance on NL Explanations vs Parsing Accuracy

Figure 4: Fraction of actions seeking Natural Language
Explanations vs competence of the learner’s semantic
parser.

5.2 Reliance on NL vs Parsing accuracy

Intuitively, semantic parsing competence of a
learner should be a significant consideration in
whether it should rely on explanations. To test
this, we simulate scenarios of learners with differ-
ent levels of semantic parsing ability by choosing
the true logical form for any explanation with the
corresponding probability, and choosing an alter-
native logical form from the remaining candidates
in the k-best list from the semantic parser other-
wise. Figure 4 depicts the effect of parsing com-
petence on learned question-asking strategies. The
empirical behavior largely corroborates our expec-
tation, as the learned strategies increasingly avoid
seeking natural language explanations of concepts
as the parsing performance worsens. In the base
case where the learner has no parsing competence,
the model learns to exclusively ask for labeled ex-
amples only (In the figure, the fraction is seen to
converge close to 0.1 rather than 0 due to the �-
greedy nature of the policy).

Rank (in sequence of instruction by human teacher)

Av
g 

Re
w

ar
d 

pe
r 

0

0.01

0.02

0.03

0.04

0.05

Rank 1 Rank 2 Ranks 3-5 Rank>5

Relative value of explanations

Figure 5: Average increase in classification perfor-
mance from explanations of different positional orders.

5.3 Differential value of explanations

A notable issue in learning from explanations,
which we do not model here is that a teacher’s
multiple explanations of a concept can have a large
variance in their utility to a learner. In particu-
lar, we might expect that teachers would be more
likely to provide the most useful explanations first,
and minor explanations subsequently. From an ab-
lation study, we observe that this is indeed a valid
concern. Figure 5 shows the average marginal in-
crease in classification performance over 50 vi-
sual shape classification tasks from explanations
with different rank (based on the actual order
of providing from human users). This indicates
that explanations from teachers provided later con-
tribute significantly less towards classification per-
formance.

5.4 Examples of Learned behavior

From a qualitative analysis, learned policies are
seen to be intuitive and interpretable. In partic-
ular, the policies overwhelmingly seek clarifica-
tions when confidence in the parser is low. On the
other hand, there are strong inclinations to con-
tinue using an action type as long as it yields high
returns. Interestingly, the optimal policy differs
significantly in behavior across domains. As ex-
ample, learned policies rely nearly twice as much
on explanations for bird species identification as
for email classification tasks. The probable reason
is that parsing is harder for email explanations, as
features in this domain are often compositional.

5.5 User study

We perform a small user-study to also evaluate
performance of the learned policies on an email
categorization task with actual human teachers.
We still train the question-asking strategy using
the simulated teacher framework (since learning
the policy from crowdsourced human users would
be expensive and slow). 20 users were asked to in-



4172

Avg. Reward Natural Avg. Rew (simulated)
LiD 0.524 3.2 0.607
Random 0.493 2.9 0.551

Table 1: Human teacher evaluation for learned and ran-
dom question asking policy.

teract with the learned LiD policy to teach a cho-
sen email-classification task. For each task, the
system asked a sequence of 10 questions, and the
human teacher’s responses were incorporated into
the system to update the classification model. The
users were also asked to teach another task with
questions asked through a random policy. Table 1
shows the average cumulative reward for humans
interacting with LiD vs a random policy for this
experiment. We note that LiD leads to better per-
formance on average. This trend is the same as in
the simulated analysis, although we note that the
learning is slower with real teachers than in the
simulated setting on the same tasks, and the gain in
performance is substantially smaller. A contribut-
ing reason for this is likely annotator bias (Geva
et al., 2019), since in the simulated testing sce-
narios, the teacher’s explanations can often likely
come from a small set of turkers whose language
explanations for teaching other tasks were used for
training the learner’s semantic parsing model. We
note that the learned policy was rated by human
users as more natural than a random policy on a
Likert scale (with range 1-5).

6 Conclusion

In this paper, we have provided a reinforcement
learning formulation for learning to ask questions
for interactive training of machine learning mod-
els. This framework is attractive in grounding the
value of questions asked in a measurable down-
stream task. Further, change in model perfor-
mance is a natural reward to drive this learning.
While this provides a conceptually useful frame-
work for framing question generation, in its cur-
rent form the approach makes simplistic assump-
tions on the types of questions that can be asked,
as well as on the structure of the dialog between
the teacher and the learner. While the system
outperforms a random policy on learning clas-
sification tasks, the dialog looks contrived from
a human perspective. An interesting direction
could be to pair the framework with neural text
generation methods to model fine-grained ques-
tion types, and generate more natural-looking in-

teractions through dialog. An important scien-
tific question is to characterize learning tasks for
which learning from language is likely to outper-
form pure inductive learning. Future work can
also extend the approach to other supervised learn-
ing tasks, as well as bootstrap from natural dialog
data.

References

Jacob Andreas, Dan Klein, and Sergey Levine. 2018.
Learning with latent language. In Proceedings of
the 2018 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, Volume 1 (Long Pa-
pers), pages 2166–2179. Association for Computa-
tional Linguistics.

Maya Cakmak and Andrea L. Thomaz. 2012. Design-
ing robot learners that ask good questions. In Pro-
ceedings of the Seventh Annual ACM/IEEE Inter-
national Conference on Human-Robot Interaction,
HRI ’12, pages 17–24, New York, NY, USA. ACM.

Brendan Collins, Jia Deng, Kai Li, and Li Fei-Fei.
2008. Towards scalable dataset construction: An ac-
tive learning approach. In European conference on
computer vision, pages 86–98. Springer.

Gregory Druck, Gideon Mann, and Andrew McCal-
lum. 2008. Learning from labeled features using
generalized expectation criteria. In Proceedings of
the 31st annual international ACM SIGIR confer-
ence on Research and development in information
retrieval, pages 595–602. ACM.

Jacob Eisenstein, James Clarke, Dan Goldwasser, and
Dan Roth. 2009. Reading to learn: Constructing
features from semantic abstracts. In Proceedings
of the 2009 Conference on Empirical Methods in
Natural Language Processing: Volume 2-Volume 2,
pages 958–967. Association for Computational Lin-
guistics.

Mohamed Elhoseiny, Babak Saleh, and Ahmed Elgam-
mal. 2013. Write a classifier: Zero-shot learning us-
ing purely textual descriptions. In The IEEE Inter-
national Conference on Computer Vision (ICCV).

Daniel Fried, Ronghang Hu, Volkan Cirik, Anna
Rohrbach, Jacob Andreas, Louis-Philippe Morency,
Taylor Berg-Kirkpatrick, Kate Saenko, Dan Klein,
and Trevor Darrell. 2018. Speaker-follower mod-
els for vision-and-language navigation. CoRR,
abs/1806.02724.

Kuzman Ganchev, João Graça, Jennifer Gillenwater,
and Ben Taskar. 2010. Posterior regularization for
structured latent variable models. The Journal of
Machine Learning Research, 11:2001–2049.

https://doi.org/10.18653/v1/N18-1197
https://doi.org/10.1145/2157689.2157693
https://doi.org/10.1145/2157689.2157693


4173

Mor Geva, Yoav Goldberg, and Jonathan Berant. 2019.
Are we modeling the task or the annotator? an inves-
tigation of annotator bias in natural language under-
standing datasets. arXiv preprint arXiv:1908.07898.

Dan Goldwasser and Dan Roth. 2014. Learning from
natural instructions. Mach. Learn., 94(2):205–232.

Braden Hancock, Paroma Varma, Stephanie Wang,
Martin Bringmann, Percy Liang, and Christopher
Ré. 2018. Training classifiers with natural lan-
guage explanations. In Proceedings of the 56th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1884–
1895. Association for Computational Linguistics.

Sheng-Jun Huang, Songcan Chen, and Zhi-Hua Zhou.
2015. Multi-label active learning: Query type mat-
ters. In Proceedings of the 24th International Con-
ference on Artificial Intelligence, IJCAI’15, pages
946–952. AAAI Press.

Jayant Krishnamurthy and Thomas Kollar. 2013.
Jointly learning to parse and perceive: Connecting
natural language to the physical world. Transactions
of Association for Computational Linguistics.

Igor Labutov, Bishan Yang, and Tom Mitchell. 2018.
Learning to learn semantic parsers from natural lan-
guage supervision. In Proceedings of the 2018 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1676–1690.

Percy Liang, Michael I Jordan, and Dan Klein. 2009.
Learning from measurements in exponential fami-
lies. In Proceedings of the 26th annual interna-
tional conference on machine learning, pages 641–
648. ACM.

Ishan Misra, Ross Girshick, Rob Fergus, Martial
Hebert, Abhinav Gupta, and Laurens van der
Maaten. 2018. Learning by asking questions. In
2018 IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR), pages 11–20.
IEEE.

Karthik Narasimhan, Tejas Kulkarni, and Regina
Barzilay. 2015. Language understanding for text-
based games using deep reinforcement learning. In
Proceedings of the 2015 Conference on Empirical
Methods in Natural Language Processing, pages 1–
11.

Andrew M Olney, Arthur C Graesser, and Natalie K
Person. 2012. Question generation from concept
maps. Dialogue & Discourse, 3(2):75–99.

Devi Parikh and Kristen Grauman. 2011. Interactively
building a discriminative vocabulary of nameable at-
tributes. In Proceedings of the 2011 IEEE Confer-
ence on Computer Vision and Pattern Recognition,
pages 1681–1688. IEEE Computer Society.

Sudha Rao and Hal Daumé III. 2018. Learning to ask
good questions: Ranking clarification questions us-
ing neural expected value of perfect information. In

Proceedings of the 56th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), volume 1, pages 2737–2746.

Salvatore Romeo, Giovanni Da San Martino, Alberto
Barrón-Cedeno, Alessandro Moschitti, Yonatan Be-
linkov, Wei-Ning Hsu, Yu Zhang, Mitra Mohtarami,
and James Glass. 2016. Neural attention for learn-
ing to rank questions in community question answer-
ing. In Proceedings of COLING 2016, the 26th In-
ternational Conference on Computational Linguis-
tics: Technical Papers, pages 1734–1745.

G. A. Rummery and M. Niranjan. 1994. On-line Q-
learning using connectionist systems. Technical Re-
port TR 166, Cambridge University Engineering De-
partment, Cambridge, England.

Burr Settles. 2012. Active learning. Synthesis Lec-
tures on Artificial Intelligence and Machine Learn-
ing, 6(1):1–114.

Lanbo She and Joyce Chai. 2017. Interactive learning
of grounded verb semantics towards human-robot
communication. In Proceedings of the 55th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), volume 1, pages
1634–1644.

Behjat Siddiquie and Abhinav Gupta. 2010. Beyond
active noun tagging: Modeling contextual interac-
tions for multi-class active learning. In 2010 IEEE
Computer Society Conference on Computer Vision
and Pattern Recognition, pages 2979–2986. IEEE.

Shashank Srivastava, Igor Labutov, and Tom Mitchell.
2017. Joint concept learning and semantic parsing
from natural language explanations. In Proceedings
of the 2017 Conference on Empirical Methods in
Natural Language Processing, pages 1527–1536.

Shashank Srivastava, Igor Labutov, and Tom Mitchell.
2018. Zero-shot learning of classifiers from natu-
ral language quantification. In Proceedings of the
56th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
306–316. Association for Computational Linguis-
tics.

Lucy Vanderwende. 2008. The importance of being
important: Question generation. In Proceedings
of the 1st Workshop on the Question Generation
Shared Task Evaluation Challenge, Arlington, VA.

Sida I. Wang, Percy Liang, and Christopher D. Man-
ning. 2016. Learning language games through in-
teraction. In Proceedings of the 54th Annual Meet-
ing of the Association for Computational Linguis-
tics, ACL 2016, August 7-12, 2016, Berlin, Ger-
many, Volume 1: Long Papers.

Ludwig Wittgenstein. 1953. Philosophical investiga-
tions. Philosophische Untersuchungen. Macmillan.

https://doi.org/10.1007/s10994-013-5407-y
https://doi.org/10.1007/s10994-013-5407-y
http://aclweb.org/anthology/P18-1175
http://aclweb.org/anthology/P18-1175
http://dl.acm.org/citation.cfm?id=2832249.2832380
http://dl.acm.org/citation.cfm?id=2832249.2832380
http://aclweb.org/anthology/P18-1029
http://aclweb.org/anthology/P18-1029
http://aclweb.org/anthology/P/P16/P16-1224.pdf
http://aclweb.org/anthology/P/P16/P16-1224.pdf


4174

Haichao Zhang, Haonan Yu, and Wei Xu. 2018. In-
teractive language acquisition with one-shot visual
concept learning through a conversational game. In
Proceedings of the 56th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 2609–2619. Association for
Computational Linguistics.

http://aclweb.org/anthology/P18-1243
http://aclweb.org/anthology/P18-1243
http://aclweb.org/anthology/P18-1243

