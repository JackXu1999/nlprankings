















































Source Error-Projection for Sample Selection in Phrase-Based SMT for Resource-Poor Languages


Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 819–827,
Chiang Mai, Thailand, November 8 – 13, 2011. c©2011 AFNLP

Source Error-Projection for Sample Selection in Phrase-Based SMT for
Resource-Poor Languages

Sankaranarayanan Ananthakrishnan, Shiv Vitaladevuni, Rohit Prasad, and Prem Natarajan
Raytheon BBN Technologies

10 Moulton Street
Cambridge, MA 02138, U.S.A.

{sanantha,svitalad,rprasad,pnataraj}@bbn.com

Abstract

The unavailability of parallel training cor-
pora in resource-poor languages is a ma-
jor bottleneck in cost-effective and rapid
deployment of statistical machine transla-
tion (SMT) technology. This has spurred
significant interest in active learning for
SMT to select the most informative sam-
ples from a large candidate pool. This
is especially challenging when irrelevant
outliers dominate the pool. We propose
two supervised sample selection methods,
viz. greedy selection and integer lin-
ear programming (ILP), based on a novel
measure of benefit derived from error anal-
ysis. These methods support the selec-
tion of diverse and high-impact, yet rel-
evant batches of source sentences. Com-
parative experiments on multiple test sets
across two resource-poor language pairs
(English-Pashto and English-Dari) reveal
that the proposed approaches achieve
BLEU scores comparable to the full sys-
tem using a very small fraction of all avail-
able training data (ca. 6% for E-P and 13%
for E-D). We further demonstrate that the
ILP method supports global constraints of
significant practical value.

1 Introduction

The laborious and time-consuming nature of pro-
ducing parallel training corpora for the develop-
ment of high-quality SMT systems cannot be over-
stated. Barring a few mainstream languages, the
vast majority of language pairs can be classified

This paper is based upon work supported by the DARPA TRANSTAC Program. The views
expressed are those of the author and do not reflect the official policy or position of the
Department of Defense or the U.S. Government.

as “resource-poor” as far as availability of a usable
SMT system is concerned. Active learning can re-
duce human labor, turn-around time and monetary
cost of developing SMT systems with little or no
loss in translation accuracy.

In its simplest form, active learning for building
parallel corpora involves selecting “high-value”
samples from a large monolingual corpus of
source sentences (the candidate pool) for transla-
tion by a bilingual human expert. The notion of
“value” depends on the selection method, and can
be derived using unsupervised, semi-supervised,
or supervised techniques. For instance, Eck et al.
(2005) define high-value source sentences as those
that contain a large number of previously unseen
n-grams. While it aims to increase coverage of the
training set, the main deficiency of this approach
is its tendency to pick irrelevant outliers if the can-
didate pool contains data from unrelated regimes.

Haffari et al. (2009) propose a number of fea-
tures, such as similarity to the seed corpus, trans-
lation probability, n-gram and phrase coverage as
unsupervised measures of the value of candidate
samples. Additionally, a linear combination of
these features is proposed as a supervised measure
of value for ranking candidate sentences. The pa-
rameters of this model are optimized on two sep-
arate held-out bilingual development sets. The
disadvantage of this approach is that it relies on
the candidate pool having the same distributional
characteristics as the development sets used for pa-
rameter estimation. Haffari and Sarkar (2009) ex-
plore active learning in a multilingual setting (dif-
ferent source languages fd to be translated to a
single target language e) using disagreement be-
tween target hypotheses generated by each of the
SMT systems. For single language active learn-

Distribution Statement “A” (Approved for Public Release, Distribution Unlimited)

819



ing, they use OOV phrases, i.e. source n-grams
without translation choices.

Ananthakrishnan et al. (2010) propose an error-
driven approach that identifies translation errors
on a held-out development set, and uses this to
train a discriminative pairwise comparator func-
tion that preferentially selects candidate sentences
with constructs that are incorrectly translated in
the development set. The chosen sentences pro-
vide maximum potential reduction in translation
error. The advantage of this method over other un-
supervised and semi-supervised selection strate-
gies is that it favors domain-relevant sentences that
are difficult to translate. However, its granular-
ity is low, because it considers errors only at the
sentence level. Further, the diversity constraint is
implemented in a non-optimal, ad-hoc manner by
deleting feature functions from the pairwise clas-
sification model.

Bloodgood and Callison-Burch (2010) explored
active learning to augment training data for high
resource language pairs. They experimented with
random, shortest, and longest sentence selection,
as well as a technique they refer to as Vocab-
Growth. The latter prefers a candidate sentence
that contains the most frequent n-gram not seen in
the labeled training data. Again, this approach is
unsuitable if the pool contains a lot of irrelevant
sentences. Moreover, their system is based en-
tirely on source language statistics, and does not
use any feedback from the SMT system. As one
of their examples indicates, this makes it suscepti-
ble to selecting sentences containing n-grams that
were already correctly translatable.

This paper introduces a novel, fine-grained,
error-driven measure of value for candidate sen-
tences obtained by translation error analysis on
a domain-relevant held-out development set. Er-
rors identified in translation hypotheses are pro-
jected back on to the corresponding source sen-
tences through phrase derivations from the SMT
decoder. This projected error is used to obtain a
“benefit value” for each source n-gram that serves
as a measure of its translation difficulty. Sentence
selection is posed as the problem of choosing K
sentences from the candidate pool that maximize
the sum of the benefit values of n-grams covered
by the choice. This is a generalization of the
set-covering problem, known to be NP-Complete.
We present two approximate solutions: (a) an ef-
ficient greedy algorithm and (b) an integer lin-

ear programming (ILP) formulation. We compare
these two methods and demonstrate their superi-
ority to numerous competing selection strategies
described in the literature.

2 Translation Error Projection

The principal advantage of error-driven sample se-
lection (Cohn et al., 1996; Meng and Lee, 2008)
over traditional unsupervised or semi-supervised
active learning (Hwa, 2004; Tang et al., 2002;
Shen et al., 2004) is its ability to choose instances,
which, when annotated, potentially maximize er-
ror reduction of the learner on a reference set.

We assume the following data configuration for
error-driven sample selection for SMT. A seed par-
allel corpus S is required to bootstrap an initial
translation system. However, we do not require
that this corpus be drawn from the same distribu-
tion as the testing condition. This relaxed assump-
tion is particularly useful for developing SMT sys-
tems for resource-poor language pairs for which
in-domain parallel training data may not be read-
ily available, but a (low quality) translation sys-
tem may be built using data from other domains,
genres or dialects. We also assume a phrase-based
SMT architecture (Koehn et al., 2003).

A held-out development (tuning) set D is used
for optimizing the parameters of the SMT system
using MERT (Och, 2003), as well as for error-
analysis in guiding the proposed sample selection
algorithms. System performance is evaluated on
a fair test set T. We assume that the tuning set
is derived from the same distribution as the test
set. The selection algorithms operate on a large
pool of monolingual source sentences P to extract
high-value samples for translation by a human ex-
pert. The candidate pool may contain any mixture
of relevant and irrelevant sentences, and may also
possess significant redundancy.

2.1 Error Analysis

The SMT system is bootstrapped using the seed
training corpus S. The held-out set D is decoded
by the SMT to obtain 1-best translation hypothe-
ses. Translation edit rate (TER) analysis (Snover
et al., 2006) is used to identify errors in the hy-
potheses by aligning them to the target references.
The TER alignment identifies a set of insertions,
substitutions, deletions, and shifts that is required
to transform a hypothesis to its corresponding ref-
erence. Large values of TER indicate greater dis-

820



Figure 1: Example illustrating evaluation of back-projected error for a source sentence given phrase
derivations and error analysis of its translation hypothesis (English-Pashto). Incorrectly hypothesized
words are underlined in red.

similarity between hypotheses and references, cor-
responding to poor translations.

An individual target word in the hypothesis is
deemed “correct” if it aligns to itself in the TER
alignment. Conversely, hypothesized words cor-
responding to substitution or insertion errors are
considered “incorrect” translations, while deletion
errors are ignored. Thus, each hypothesized word
can be labeled “correct” or “incorrect” based on
the TER alignment, providing a fine-grained view
of translation difficulty on the held-out set.

2.2 Benefit/Objective Function

TER analysis enables us to label errors at the word
level for each SMT hypothesis. However, it is the
knowledge of which source words were translated
incorrectly that is useful for sample selection. As-
suming there is a way to attach a label (or real
value) to each source word in the held-out set D
indicating whether it was correctly translated (or
to what degree it was translated), we can compute
a benefit value over each source n-gram. The sum
of benefit values over all source n-grams can be
used as an objective function that must be maxi-
mized by selecting suitable samples from the can-
didate pool.

The SMT decoder produces phrase derivations
specifying the origin of each target phrase in a hy-
pothesis, providing a convenient mechanism for
approximate projection of target error labels back
on to the source words. We refer to this as er-
ror back-projection. We compute, for each target
phrase in the phrase derivations, the target phrase
error as a ratio of the number of words labeled “in-
correct” to the total number of words within that
phrase. This quantity is then equally distributed
among constituent source words using the geomet-

ric mean with respect to the number of words in
the containing source phrase (obtained from the
phrase derivation), to give us a back-projected er-
ror at the level of individual source words. Since
the phrase derivations form a mutually exclusive
partition over the source sentence, we can com-
pute an unambiguous back-projected error value
for each source word in the held-out set D. Figure
1 illustrates this procedure with an example.

We then compute a benefit value for each source
n-gram as the sum of back-projected error of
the constituent source words. The set of source
phrases in the SMT decoder derivations is typi-
cally a very small subset of the set of all pos-
sible n-grams of equal length. By distributing
back-projected phrase error over individual target
words, we are able to compute benefit values for
n-grams not covered by the phrase table inven-
tory. Finally, we sum the benefit values of source
n-grams that occur multiple times in D to generate
a table of benefit values hashed by the correspond-
ing n-grams.

3 Sentence Selection Problem

Given a set of n-grams N = {ni}mi=1 from source
sentences in D and associated benefit values bi ≥
0 computed by error back-projection, the goal of
sample selection is to choose a batch of K sen-
tences that maximizes the cumulative benefit value
of n-grams covered by the chosen sentences. The
contribution of each sentence towards the objec-
tive function is equal to the sum of benefit values
of all unique n-grams it contains.

The sentence selection problem is closely re-
lated to the classical set covering problem, one
of 21 NP-complete problems described in Karp’s

821



seminal paper on reducibility (Karp, 1972). The
decision version of set covering is as follows:
given a set of elements T and a set of subsets
within T , say T = {Tj ⊆ T}, is it possible to
select k subsets such that their union is the super-
set T ? This problem is known to be NP-Complete.

To visualize the similarity of set covering to the
sentence selection problem, note that the elements
of T are akin to n-grams, and the subsets Tj’s cor-
respond to sentences. It is easy to show that the
set covering problem can be reduced to the sen-
tence selection problem. Thus, there does not exist
a polynomial time algorithm for optimal sentence
selection unless P = NP .

There is a standard greedy approximation algo-
rithm for the minimum set covering problem (Cor-
men et al., 2001): at each iteration, choose the sub-
set Tj that has the largest number of as yet uncov-
ered elements of T . The procedure is repeated un-
til all elements in T are covered. We next present a
variant of this algorithm for the sentence selection
problem. Our algorithm address two differences
between sentence selection and set covering: (a)
each n-gram provides a distinct benefit value on
covering, and (b) we must select only K sentences
and maximize the cumulative benefit.

4 Greedy Sample Selection

The greedy solution constructs batches iteratively
by choosing, at each step, the sentence whose total
current benefit is the largest. Each sentence in the
candidate pool is decomposed into its constituent
n-grams, and the sum of benefit values of these n-
grams is computed. The sentence that scores high-
est on this criterion is chosen. Resetting the benefit
values of n-grams in previously chosen sentences
ensures diversity. The greedy selection technique
is illustrated in Algorithm 1. Each iteration of the
master loop selects one sentence based on the local
maximum of the objective function. The indicator
function Ii(·) returns unity if n-gram ni is present
in the argument sentence, and zero otherwise.

The greedy algorithm provides a highly-
scalable approximation to the solution and can be
applied to systems with millions of n-grams and
candidate sentences. It is, however, sub-optimal
in general; potentially better solutions can be ob-
tained by casting it in an integer linear program-
ming (ILP) framework, as discussed below.

Algorithm 1 Greedy Sample Selection
B← ()
for k = 1 to K do
p∗ ← argmaxp∈P

∑m
i=1 biIi(p)

B(k)← p∗
P← P− {p∗}
bi ← 0 ∀ {i | Ii(p∗) = 1}

end for
return B

5 Integer Linear Programming (ILP)

We define a set of indicator variables, xj , for the
sentences, xj = 1 if sentences pj is selected and
0 otherwise. Similarly, there are a set of indicator
variables, yi, for the n-grams, yi = 1 if n-gram ni
is covered by some selected sentence and 0 other-
wise. Sentence selection can be expressed as the
following integer linear program (ILP):

max :
∑

i

biyi

subj. to. : yi ≤
∑

j|Ii(pj)=1
xj ∀ i

∑

j

xj ≤ K

0 ≤ yi ≤ 1 ∀ i , xj ∈ {0, 1} ∀ j
(1)

Notice that since bi ≥ 0, in order to maximize
the optimization function each yi will be set to 1
whenever at least one of the sentence covering its
n-gram is selected, xj = 1.

In general, exact optimization of ILP is NP-
Hard. However, there are several publicly avail-
able solvers for ILPs with thousands of variables.
For instance, the open-source lp-solve program
uses the Branch-and-Bound technique to solve
ILPs and in our experiments handles systems with
thousands of sentences.

5.1 Including Application Constraints

Unlike greedy selection, ILP allows us to impose
additional application or domain specific global
constraints within the optimization framework.
One example is to bound the total number of
words in the selected sentences rather than number
of chosen sentences. This is useful because when
the number of sentences is constrained, the system
is biased to choose longer sentences as they would
cover more n-grams. Assuming manual transla-
tion cost to be linear in the number of words, we

822



can put a bound on the total length of the cho-
sen sentences. Let lj be the length of sentence
pj . We can put a bound

∑
j ljxj ≤ L. Impos-

ing such a bound is similar to the Knapsack prob-
lem, a standard NP-Complete problem (Cormen et
al., 2001). We can also incorporate prior infor-
mation on the goodness of sentences by including
sentence costs (e.g. syntactic well-formedness,
length, etc.), cj’s, within the optimization func-
tion:

∑
i biyi +

∑
j cjxj . In contrast to the greedy

algorithm, ILP provides a natural framework to
perform joint optimization over multiple types of
constraints.

5.2 Solving the ILP for Practical Problems

Even moderate size SMT applications involve tens
of thousands of n-grams and sentences, e.g., one
of our test conditions has approximately 79, 000
n-grams and 100, 000 sentences. Each sentence
would result in an integer variable in the ILP.
State-of-the-art solvers have difficulty handling
such large problems, e.g., lp-solve was unable to
solve the ILP for a system with 100, 000 sen-
tences. We propose a two-step solution to achieve
scalability. If k sentences must be selected in a
given active learning iteration, we use the greedy
algorithm to prune the problem by choosing k′ >
k sentences from the corpus, and subsequently
construct the ILP on this smaller problem to select
the required k sentences. While ILP is optimal,
greedily pruning the problem for ILP may result
in sub-optimality.

We observed the run time and optimization
value computed by ILP for different k′, keeping
k constant at 16. We chose smaller (k′, k) for
these simulations to allow ILP to run to comple-
tion. Figure 2 summarizes our findings. Note that
the optimum improves with larger prune size and
requires more iterations. Larger prune sizes allow
the ILP to choose from a larger pool of sentences,
and are therefore likely to improve the optimum.
However, these typically require more iterations.
In this paper, we used the greedy algorithm to
prune the problem to K ′ = 1000 sentences, and
then select K = 400 sentences using ILP (we re-
stricted ILP run-time to 20 minutes per batch).

6 Experimental Results

We demonstrate the effectiveness of greedy and
ILP-based sample selection by conducting sim-
ulation experiments on two resource-poor lan-

0 5 10 15 20 25
2.55

2.56

2.57

2.58

2.59

2.6
x 10

4

Number of iterations

O
pt

im
iz

at
io

n 
fu

nc
tio

n 
va

lu
e

 

 

k’=64

k’=512k’=256k’=128

Figure 2: Optimization function value for different
iterations of ILP branch-and-bound, for different
sizes of pruned problems (k′ = 64, 128, 256, 512)
for constant choice cardinality, k = 16.

guage pairs commissioned under the DARPA
Transtac speech-to-speech translation initiative,
viz. English-Pashto (E2P) and English-Dari
(E2D). In both cases, only a small fraction of the
available training data is pertinent to the transla-
tion task. This simulates a condition where a large
source language corpus (e.g. English) is harvested
from the Web, of which only a small fraction is
relevant to the target SMT system.

We simulate low-resource conditions by seques-
tering the majority of available parallel training
data. A seed translation model is bootstrapped
with a very small subset of the training corpus;
source sentences of the remainder constitute the
candidate pool. Because obtaining monolingual
text in the target language is usually not a con-
straint, we train the target language model (LM)
from all available target language sentences in the
training corpus.

We then apply the proposed selection algo-
rithms to choose fixed-size batches from the pool.
Translations for the selected sentences are ob-
tained from the sequestered parallel corpus (thus
simulating a human oracle). The chosen batch and
its translation is appended to the seed corpus S
for retraining the SMT. At each iteration, we in-
dependently decode the test set and evaluate trans-
lation accuracy in order to compare the trajectory
of BLEU for these and other competing selection
strategies:

• Random: Source sentences are uniformly sam-
pled from the candidate pool P.

• Dissimilarity: Select sentences from P with
the largest number of n-grams not seen in S

823



(Eck et al., 2005; Haffari et al., 2009).

• Longest: Pick the longest sentences from the
candidate pool P.

• Discriminative: Choose sentences that po-
tentially minimize translation error using
a maximum-entropy pairwise comparator
(Ananthakrishnan et al., 2010).

• Greedy: Simple greedy selection with pro-
posed error-projection benefit objective.

• ILP: Integer linear programming optimization
with error-projection benefit objective.

English-to-Pashto Simulation: The E2P data
originates from a two-way collection of spoken di-
alogues, and consists of two parallel sub-corpora:
a directional E2P corpus and a directional Pashto-
English (P2E) corpus. Each sub-corpus has its
own independent training, development, and test
partitions. The directional E2P training, develop-
ment, and test sets consist of 33.9k, 2.4k, and 1.1k
sentence pairs, respectively. The directional P2E
training set consists of 76.5k sentence pairs. In ad-
dition, DARPA has made available to all Transtac
participants an open 564-sentence E2P test set
with four target references for each input.

We trained a baseline E2P SMT system from
all available E2P and reversed P2E data. The full-
system BLEU scores on the single-reference in-
ternal test set and on the multi-reference DARPA
evaluation test set were 10.8 and 24.4, respec-
tively. We set up active learning simulation by ran-
domly sampling 1,000 sentence pairs from the di-
rectional E2P training partition to obtain the seed
training corpus. The remainder of this set, and the
entire reversed P2E training partition were com-
bined to create the pool. The reversed directional
P2E data is considered irrelevant as far as the E2P
test sets are concerned. The pool thus consists of
30% in-domain and 70% irrelevant sentence pairs.
We simulated 35 iterations with batches of 400
sentences each; the seed corpus grows to 15,000
sentence pairs at the end of the simulation.

English-to-Dari Simulation: The E2D data is
also derived from a two-way collection of spoken
dialogues. The directional E2D training, develop-
ment, and test sets consist of 11.6k, 3.2k, and 2.8k
sentence pairs, respectively. The directional D2E
training set consists of 52.9k sentence pairs. The
full-system BLEU on the E2D test set was 15.1.

As with E2P, the seed training corpus was ob-
tained by randomly sampling 1,000 sentence pairs

from the directional E2D training partition. All re-
maining parallel training data were designated as
the candidate pool. Thus, only about 17% of the
candidate pool is considered relevant with respect
to the E2D test set. Again, we simulated 35 itera-
tions with batches of 400 sentences each.

BLEU Trajectories: The trajectories of BLEU
scores for the E2P and E2D test sets are shown
in Figures 3(a), 3(b), and 3(c), respectively. The
horizontal line near the top of each plot represents
the corresponding full-system BLEU score. The
following observations are noteworthy:

• BLEU scores using the proposed greedy and
ILP-based selection methods ramp up very
quickly to the full-system level using a small
fraction of available training data. On the E2P
single-reference test set, the top-line BLEU
score of 10.8 is attained after just 12 iterations
(5.8k sentence pairs), as against 100k sentence
pairs for the full system (6% of the corpus).
Likewise for E2D, the full-system BLEU score
of 15.1 is attained after only 18 iterations (8.2k
sentence pairs), as opposed to 65k sentence
pairs for the full system (13% of the corpus).

• In some cases, BLEU scores with training cor-
pora constructed using active learning exceed
those obtained with the full system. This is
because our selection algorithms are biased to
choose relevant, in-domain sentences from the
candidate pool. Initially, the training corpus
is kept free of outliers that cause performance
degradation in the full system. With more it-
erations of selection, the latter eventually find
their way into the training set, causing transla-
tion performance to settle around the top-line
BLEU scores.

• Under identical initial conditions at the first it-
eration of active learning, the ILP benefit op-
timum exceeds the greedy optimum by 444.5
units for E2P, and by 420.3 units for E2D. This
confirms the theoretical superiority of ILP over
greedy selection.

We computed total area under the BLEU curves
for the various selection techniques as a single fig-
ure of merit. Summarized in Table 1, the BLEU-
Iteration product shows source error-projection
with the ILP selection algorithm outperforming
all competing techniques, including discriminative
sample selection (Ananthakrishnan et al., 2010).

Global Length Constraint: The above simula-

824



(a) Trajectory of BLEU (E2P-SingleRef) (b) Trajectory of BLEU (E2P-MultiRef)

(c) Trajectory of BLEU (E2D-SingleRef) (d) Maximum benefit for greedy and ILP selection

Figure 3: Simulation results for E2P and E2D.

Method E2P-SR E2P-MR E2D-SR
Random 83.03 78.76 144.65
Dissim. 82.34 81.18 154.17
Longest 89.89 93.65 155.72
Discrim. 113.54 145.32 205.56
Greedy 120.83 155.64 204.95
ILP 121.16 156.70 210.51

Table 1: Area under the BLEU curve with respect
to 0-th iteration baseline.

tions choose a fixed number of sentences at each
iteration. However, the ILP optimization frame-
work also permits the integration of global con-

straints, such as the number of words in a batch.
This is an important practical benefit, as most pro-
fessional translators charge by word rather than by
sentence. Other selection methods can only sup-
port such constraints in an ad-hoc fashion. To eval-
uate this feature of ILP selection, we implemented
a variant of the greedy algorithm where the stop-
ping criterion is number of words selected. We
then imposed a global length constraint for ILP
as described in Section 5.1, and compared BLEU
scores across 35 simulation iterations for E2P with
a limit of 5,000 words per batch (Figure 4(a)). The
trend in BLEU indicates that ILP provides a better
framework for integration of such constraints.

825



(a) Length-capped BLEU trajectory (E2P-SingleRef). Full system
training data contains 1.46M source words.

0 20 40 60 80 100
0

0.05

0.1

0.15

0.2

0.25

Number of words

N
or

m
al

iz
ed

 fr
eq

ue
nc

y

 

 

Greedy − cap length
ILP − cap length

#words=26

#words=16

(b) Normalized histogram of sentence length

Figure 4: Comparison of greedy and ILP with global length constraint of 5,000 words per batch.

Moreover, the overall sentence length distribu-
tion shown in Figure 4(b) indicates that the greedy
algorithm prefers a smaller number of very long
sentences, whereas ILP prefers a larger number
of shorter sentences. The latter is an important
practical benefit for applications such as crowd-
sourcing, because annotators often find it difficult
to translate long sentences. Secondly, automatic
word alignment quality for long sentence pairs is
often poor. Finally, larger number of sentences in
the training pool is likely to increase diversity.

7 Discussion and Future Work

Active learning provides a useful framework for
alleviating the significant costs associated with de-
veloping SMT systems for resource-poor language
pairs. In this paper, we introduced a novel criterion
for active sample selection, viz. back-projected
translation error. Candidate instances that score
well on this criterion are chosen for translation
by a bilingual human expert. We showed that the
problem of maximizing the error-projection objec-
tive function is closely related to the set-covering
problem, known to be NP-complete.

We used a simple greedy selection algorithm as
a first approximation to the solution. BLEU trajec-
tories from simulation experiments demonstrated
the superiority of this scheme to competing ac-
tive learning algorithms. We then proposed an op-
timization framework to maximize the objective
function for sample selection, and provided a so-
lution via ILP. The ILP-based selection algorithm

also supports global constraints on the optimiza-
tion problem, e.g. overall corpus size in words,
which neither greedy selection nor other compet-
ing strategies can implement in a principled fash-
ion. We also showed that ILP was superior to
the greedy approach when constraining selected
batches by total number of words, rather than by
number of sentences.

The proposed approach is shown to outper-
form competing active learning strategies when
the candidate pool contains a small number of
high-impact samples buried within a large corpus
of mostly irrelevant text. Guided by an in-domain
development set, our approach always selects rel-
evant samples that are likely to provide maximal
benefit to the SMT system. Pilot experiments
suggest that this approach may not be as effec-
tive when the candidate pool is completely task-
relevant. Another weakness of our technique is
the reliance on a relatively small development set
to guide selection. Performance may saturate once
all sentences in the development set can be trans-
lated accurately. Generally, a large, rich develop-
ment set will tend to give better results. In the fu-
ture, we plan to experiment with random sampling
to vary the development set at each active learning
iteration.

We have shown the effectiveness of our ap-
proach in the context of phrase-based SMT sys-
tems. The same principles can be extended to hi-
erarchical or syntax-based SMT architectures with
minimal effort.

826



References

Sankaranarayanan Ananthakrishnan, Rohit Prasad,
David Stallard, and Prem Natarajan. 2010. Dis-
criminative sample selection for statistical machine
translation. In Proceedings of the 2010 Conference
on Empirical Methods in Natural Language Pro-
cessing, EMNLP ’10, pages 626–635, Morristown,
NJ, USA. Association for Computational Linguis-
tics.

Michael Bloodgood and Chris Callison-Burch. 2010.
Bucking the trend: large-scale cost-focused active
learning for statistical machine translation. In Pro-
ceedings of the 48th Annual Meeting of the Associa-
tion for Computational Linguistics, ACL ’10, pages
854–864, Stroudsburg, PA, USA. Association for
Computational Linguistics.

David A. Cohn, Zoubin Ghahramani, and Michael I.
Jordan. 1996. Active learning with statistical mod-
els. Journal of Artificial Intelligence Research,
4(1):129–145.

Thomas H. Cormen, Charles E. Lerserson, and
Ronald L. Rivest. 2001. Introduction to Algorithms.
Prentice Hall of India.

Matthias Eck, Stephan Vogel, and Alex Waibel. 2005.
Low cost portability for statistical machine transla-
tion based in N-gram frequency and TF-IDF. In
Proceedings of IWSLT, Pittsburgh, PA, October.

Gholamreza Haffari and Anoop Sarkar. 2009. Active
learning for multilingual statistical machine transla-
tion. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Pro-
cessing of the AFNLP: Volume 1 - Volume 1, ACL
’09, pages 181–189, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.

Gholamreza Haffari, Maxim Roy, and Anoop Sarkar.
2009. Active learning for statistical phrase-based
machine translation. In NAACL ’09: Proceedings
of Human Language Technologies: The 2009 An-
nual Conference of the North American Chapter
of the Association for Computational Linguistics,
pages 415–423, Morristown, NJ, USA. Association
for Computational Linguistics.

Rebecca Hwa. 2004. Sample selection for statistical
parsing. Computational Linguistics, 30:253–276.

Richard M. Karp. 1972. Reducibility among com-
binatorial problems. In R. E. Miller and J. W.
Thatcher, editors, Complexity of Computer Compu-
tations, pages 85–103. Plenum.

Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
NAACL ’03: Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology, pages 48–54, Morristown, NJ, USA.
Association for Computational Linguistics.

Qinggang Meng and Mark Lee. 2008. Error-driven
active learning in growing radial basis function net-
works for early robot learning. Neurocomputing,
71(7-9):1449–1461.

Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In ACL ’03:
Proceedings of the 41st Annual Meeting on Asso-
ciation for Computational Linguistics, pages 160–
167, Morristown, NJ, USA. Association for Compu-
tational Linguistics.

Dan Shen, Jie Zhang, Jian Su, Guodong Zhou, and
Chew-Lim Tan. 2004. Multi-criteria-based active
learning for named entity recognition. In ACL ’04:
Proceedings of the 42nd Annual Meeting on Associ-
ation for Computational Linguistics, page 589, Mor-
ristown, NJ, USA. Association for Computational
Linguistics.

Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings AMTA, pages 223–231, August.

Min Tang, Xiaoqiang Luo, and Salim Roukos. 2002.
Active learning for statistical natural language pars-
ing. In ACL ’02: Proceedings of the 40th Annual
Meeting on Association for Computational Linguis-
tics, pages 120–127, Morristown, NJ, USA. Associ-
ation for Computational Linguistics.

827


