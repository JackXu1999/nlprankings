










































Detecting Deceptive Opinions with Profile Compatibility


International Joint Conference on Natural Language Processing, pages 338–346,
Nagoya, Japan, 14-18 October 2013.

Detecting deceptive opinions with profile compatibility

Vanessa Wei Feng
University of Toronto

Toronto, ON, M5S 3G4, Canada
weifeng@cs.toronto.edu

Graeme Hirst
University of Toronto

Toronto, ON, M5S 3G4, Canada
gh@cs.toronto.edu

Abstract

We propose using profile compatibility to
differentiate genuine and fake product re-
views. For each product, a collective
profile is derived from a separate col-
lection of reviews. Such a profile con-
tains a number of aspects of the prod-
uct, together with their descriptions. For
a given unseen review about the same
product, we build a test profile using the
same approach. We then perform a bidi-
rectional alignment between the test and
the collective profile, to compute a list
of aspect-wise compatible features. We
adopt Ott et al. (2011)’s op spam v1.3
dataset for identifying truthful vs. decep-
tive reviews. We extend the recently pro-
posed N-GRAM+SYN model of Feng et al.
(2012a) by incorporating profile compat-
ibility features, showing such an addition
significantly improves upon their state-of-
art classification performance.

1 Introduction

With the rapid development of e-commerce and
the increasing popularity of various product re-
view websites, people are more and more used to
making purchase decisions based on the reported
experience of other customers. A product rated
positively by its previous users is able to attract po-
tential new customers, while a poorly rated prod-
uct is certainly not a good option for most new
customers. Given this influential power of prod-
uct reviews, there comes a huge potential for de-
ceptive opinion spam to distort the true evaluation
of a product. The promoters of a product may
post false complimentary reviews, and competi-
tors may post false derogatory reviews.

Although the task of detecting deceptive opin-
ion spam can be formulated as a traditional bi-

nary classification problem with two classes, de-
ceptive and truthful, where supervised learning
can be applied, it is essentially very challenging.
Its major difficulty arises from the lack of reli-
ably labeled data, because it is extremely difficult
for humans to identify through reading (see dis-
cussion in Section 4). Therefore, much previous
work on detecting deceptive opinions usually re-
lies on some meta-information, such as the IP ad-
dress of the reviewer or the average rating of the
product, rather than the actual content of the re-
view Liu (2012). However, thanks to the release
of the op spam v1.3 dataset by Ott et al. (2011),
a gold-standard dataset composed of 400 truthful
and 400 deceptive hotel reviews (see Section 3),
we are now at an appropriate stage for conducting
supervised learning and reliable evaluation of the
task.

In their work, Ott et al. proposed to use n-grams
as features, and trained an SVM classifier to clas-
sify whether a given review is deceptive or truth-
ful, achieving nearly 90% accuracy. More re-
cently, Feng et al. (2012a) extended Ott et al.’s
n-gram feature set by incorporating deep syntax
features, i.e., syntactic production rules derived
from Probabilistic Context Free Grammar (PCFG)
parse trees, and obtained 91.2% accuracy on the
same dataset.

While both Ott et al. and Feng et al.’s cues of de-
ception come purely from the surface realization
in the reviews, we propose the use of additional
signals of truthfulness by characterizing the de-
gree of compatibility between the personal experi-
ence described in a test review and a product pro-
file derived from a collection of reference reviews
about the same product. Our intuition comes from
the hypothesis that since the writer of a deceptive
review usually does not have any actual experience
with that product, the resulting review might con-
tain some contradictions with facts about the prod-
uct, or the writer might fail to mention those as-

338



pects of the product that are commonly mentioned
in truthful reviews. Conversely, a writer of a truth-
ful review is more likely to make similar com-
ments about some particular aspects of the prod-
uct as other truthful reviewers. In other words, we
postulate that by aligning a the profile of a product
and the description of the writer’s personal expe-
rience, some useful clues can be revealed to help
identify possible deception.

In line with the intuition above, we want to cap-
ture two types of compatibility: (1) Compatibility
with the existence of some distinct aspect of the
product, such as the mention of the famous art mu-
seum nearby the hotel; (2) Compatibility with the
description of some general aspect of the product,
such as commenting that the breakfast at the hotel
is charged for.

We show that, by incorporating our computed
profile alignment features with Feng et al.’s n-
grams and deep syntax features, we significantly
improve on the state-of-art performance presented
by their N-GRAM+SYN model.

2 Related work

Before the existence of gold-standard datasets
which include both truthful and deceptive product
reviews, several attempts were made to detect de-
ceptive opinions.

With regard to unsupervised approaches, pre-
vious work was primarily based on various pat-
terns of atypical behaviors. Lim et al. (2010) pro-
posed several behavior models to identify unusual
reviewer patterns; e.g., spammers may contribute
a fake review soon after product launch to maxi-
mally affect subsequent reviewers. Each of these
individual models assigns a numeric score indicat-
ing the extent to which the reviewer has engaged in
apparent spam behaviors, and these scores are then
combined to product a final spam score. Jindal
et al. (2010) employed data-mining techniques to
discover unexpected class association rules; e.g.,
reviewers who give all high ratings to products
of a brand while most other reviews are generally
negative about the brand. Wu et al. (2010) pro-
posed to use the distortion of popularity rankings
as an indicator of opinion spamming, in the sense
that deleting a set of random reviews should not
overly disrupt the popularity ranking of a list of
entities, while deleting fake reviews should signif-
icantly distort the overall rankings, under the as-
sumption that deceptive reviews usually express a

sentiment at odds with legitimate reviews. Feng
et al. (2012b) postulated that for a given domain,
there exists a set of representative distributions of
review rating scores, and fake reviews written by
hired spammers will distort such natural distribu-
tions. Experimenting with a range of pseudo-gold-
standard datasets, they provided quantitative in-
sights into the characteristics of natural distribu-
tions of opinions in various product review do-
mains.

With respect to supervised approaches, Jindal
and Liu (2008) first conducted a tentative study
of detecting deceptive opinions. In the absence
of gold-standard datasets, they trained models us-
ing features from the review texts, as well as from
meta-information on the reviewer and the prod-
uct, to distinguish between duplicate reviews (re-
garded as deceptive), i.e., reviews whose major
content appears more than once in the corpus,
and non-duplicate reviews (regarded as truthful).
However, these (near-)duplicate reviews are often
not sophisticated in their composition, and there-
fore are relatively easy to identify, even by off-
the-shelf plagiarism detection software (Ott et al.,
2011).

Yoo and Gretzel (2009) constructed a small
gold-standard dataset with 40 truthful and 42 de-
ceptive hotel reviews, and manually inspected the
statistical differences of psychologically relevant
linguistic features for truthful and deceptive re-
views.

Ott et al. (2011) released the op spam v1.3
dataset (see Section 3), a much larger dataset with
400 truthful and 400 deceptive hotel reviews with
positive comments, which allows the application
of machine learning techniques for training mod-
els to automatically detect deceptive opinions. In
their work, Ott et al. focused on the textual con-
tent of the reviews, and approached the task of de-
tecting deceptive opinions by finding any stretch
of imagination in the text — they treated fake re-
views as imaginative writing, and used standard
computational approaches of genre identification,
psycholinguistic deception detection, and text cat-
egorization to identify them. Among the set of fea-
tures explored in their classification experiments,
they discovered that the features traditionally em-
ployed in either psychological studies of decep-
tion or genre identification were both significantly
outperformed by a much simpler N-GRAM model
with n-grams only as features, which achieved

339



nearly 90% accuracy. Later, Feng et al. (2012a)
proposed a strengthened model, N-GRAM+SYN,
by incorporating syntactic production rules de-
rived from Probabilistic Context Free Grammar
(PCFG) parse trees. They obtained 91.2% accu-
racy on the op spam v1.3 dataset, which is a 14%
error reduction.

More recently, Ott et al. released a new version,
the op spam v1.4 dataset, with gold-standard neg-
ative reviews included as well (Ott et al., 2013),
which offers an opportunity to more extensively
study the problem of detecting deceptive reviews.
However, the work described in this paper is fo-
cused on positive review spam, and based di-
rectly on the work of Feng et al. We extend their
N-GRAM+SYN model by incorporating potential
signals of truthfulness, derived from aligning the
description in a test review with the product profile
constructed from a large collection of reference
reviews about the same product. Somewhat or-
thogonal to Feng et al.’s n-grams and deep syntax
features, which are focused on the surface realiza-
tion of a given product review, our alignment fea-
tures are able to employ useful information from
the product itself.

3 The op spam v1.3 Dataset

Due to the difficulty for humans to identify de-
ceptive opinions, traditional approaches to con-
structing annotated corpora by recruiting human
judges to label a given set of texts does not apply
to the task of deception detection. Consequently,
crowdsourcing services such as Amazon Mechan-
ical Turk1 (AMT) have been adopted as a better
solution (Ott et al., 2011; Rubin and Vashchilko,
2012). By asking paid subjects on AMT to com-
pose deceptive and/or truthful texts, corpora with
reliable labels can be constructed.

In this work, we use Ott et al.’s op spam v1.3
dataset2, which contains positive reviews for the
20 most-rated hotels on TripAdvisor3 (TA) in
Chicago. For each hotel, Ott et al. selected 20 de-
ceptive reviews from submissions on AMT, and
20 truthful reviews from 5-star reviews on TA,
resulting in 800 reviews in total for 20 hotels.
The average length of deceptive reviews is 115.75
words, while the truthful reviews are chosen to
have roughly the same average length.

1http://mturk.com.
2http://www.cs.cornell.edu/˜myleott
3http://tripadvisor.com.

4 Difficulty of the Task

It has been shown that humans can differenti-
ate truthful reviews from deceptive ones with
only a modest accuracy. With respect to the
op spam v1.3 dataset used in our work, human
judges were only able to achieve 60% accuracy,
and the inter-annotator agreement was low: 0.11
computed by Fleiss’s kappa (Ott et al., 2011).
Since as humans, we do not have a tangible in-
tuition of what signals deception and what char-
acterizes truthfulness, we have great difficulty de-
signing features for automatic identifying decep-
tive opinions.

In addition, there is difficulty with regard to the
construction of gold-standard datasets for the task
of deception detection. On one hand, although
crowdsourcing provides a relatively cheap and re-
liable approach, gathering deceptive data is still
nevertheless laborious. On the other hand, since
for truthful reviews, we are able to select a small
subset for which we have high confidence only by
using a combination of various heuristics, the con-
structed dataset is inherently biased. Therefore,
given the limited number and size of currently
available gold-standard datasets, in the process of
developing more sophisticated models involving a
larger number of features, we may inevitably face
the issue of overfitting on a relatively small set of
data.

5 Methodology

5.1 Baseline features

We adopt Feng et al.’s N-GRAM+SYN model as the
baseline system, which employs two distinct sets
of features.

N-GRAM features: As shown by Ott et al., the
bag-of-words model is effective for identifying de-
ceptive reviews. However, the optimal choice of
n is not consistent in Ott et al.’s original imple-
mentation and Feng et al.’s strengthened model:
Ott et al. used the union of unigrams and bigrams,
while Feng et al. obtained their best performance
using unigrams alone, together with deep syntax
features. Therefore, for a fair comparison, we con-
sider using unigrams, bigrams, and the union of
both, and choose the best combination with deep
syntax features as our baseline system.

SYN features: Following Feng et al., deep syn-
tax features are encoded as production rules de-
rived from PCFG parse trees. These production

340



rules include lexicalized ones, i.e., POS-tag-to-
word rules, and are combined with the grandparent
node.

All baseline features are encoded as their tf-idf
weights, with features appearing only once in the
dataset eliminated.

5.2 Product profile construction

5.2.1 Aspects and compatibility
As introduced in Section 1, we postulate that by
aligning the profile of a product with the descrip-
tion of the writer’s personal experience, we can
characterize the degree of compatibility between
them. Such alignment features could serve as use-
ful signals to differentiate between deception and
truthfulness.

In particular, we define compatibility for two
types of aspects of a product — distinct aspects
and general aspects. Distinct aspects are special
features of the product. For hotels, those dis-
tinct aspects are usually realized as proper noun
phrases in the text, typically the names of the
landmarks nearby the hotel, including museums,
parks, restaurants, bars, and shopping malls. On
the other hand, general aspects are common fea-
tures which typically appear in any product of this
particular kind. For hotels, location, service, and
breakfast are typical general aspects. The method
we use to identify and extract distinct and general
aspects from reviews is described in Section 5.2.4.

Compatibility for distinct and general aspects is
defined as follows:

1. Compatibility with the existence of some dis-
tinct aspect of the product, e.g., truthful re-
views of this Chicago hotel often mention the
famous nearby Field Museum, and the test re-
view also mentions this museum.

2. Compatibility with the description of some
general aspect of the product. For example,
breakfast at most Chicago hotels is compli-
mentary; however, the breakfast at this hotel
is charged for, and the test review describes it
as “the breakfast is kinda expensive”.

5.2.2 Definition of product profile
A product’s profile P is composed of a set of its
aspects A = {a1, . . . ,aN}, where each ai is an indi-
vidual aspect of the product. Each aspect ai is as-
sociated with a description Di = {p1 : w1, . . . , pm :
wm}, in which each element is a unique word p j

Aspect Description

Bathroom {clean : 3.0, comfortable : 3.0,
pleasant : 5.0, high-end : 1.0,
European : 1.0}

Room {wonderful : 4.0, deluxe : 2.0,
huge : 2.0}

Service {average : 2.0, slow : 2.0}
Michigan Ave. {existence : 5.0}

Table 1: An example fragment of the profile of a
hotel.

to describe the aspect, along with the weight w j
assigned to that word.

Distinct aspects, ai,d , and general aspects, ai,g,
are treated differently when it comes to their de-
scriptions. For a distinct aspect, its description
can contain only one word, existence, because we
only care about whether this aspect is mentioned
in the text, not the particular words used to de-
scribe this aspect. In fact, most of these distinct
aspects do not occur with any associated adjectives
or adverbs.

An example fragment of a hotel’s profile is
shown in Table 1, in which the last aspect Michi-
gan Ave. is a distinct aspect, and thus only one
word, existence, appears in its description.

5.2.3 Data for profile construction
To establish a profile P for each target hotel in
the op spam v1.3 dataset, we first gather all its re-
views on TripAdvisor, from which we choose up
to 200 reviews, subject to the following criteria:

1. It is not present in the op spam v1.3 dataset.

2. Its rating is five stars (the highest).

3. Its language is English.

4. It contains at least 150 characters.

5. The author has written at least 10 reviews.

6. The author has written reviews for at least 5
different hotels.

7. The author has received at least 5 helpfulness
votes from other users.

If there are more than 200 reviews satisfying the
above criteria, we choose the 200 with the high-
est number of helpfulness votes received by their
authors. Most of the above criteria are consistent

341



Reviews

Min Max Mean

Reviews per hotel 44 200 160.6
Characters 150 8,995 848.3

Authors

Min Max Mean

Total reviews 10 506 37.8
Reviewed hotels 5 278 17.8
Helpfulness votes 5 2,280 33.3

Table 2: The statistics of the collection of reviews
used in constructing profiles and their distinct au-
thors.

with the original setup of collecting the truthful re-
views in the op spam v1.3 dataset. However, we
add the last three strong criteria to ensure the qual-
ity of the collected reviews.

Table 2 lists the statistics of the reviews used
in our profile construction. The statistics are cate-
gorized into (1) the information about the reviews
themselves, including the total number of qualified
reviews for each hotel, and the length (in charac-
ters) of each review; and (2) the information about
the distinct authors of these reviews, including the
total number of reviews each author has written
on TripAdvisor, the total number of hotels each
author has reviewed, and the total number of help-
fulness votes that each author has received from
other users.

5.2.4 Aspect extraction

Raw aspect extraction For each particular hotel
h, from the set of its corresponding reviews Rh, we
extract the aspects Ah = {a1,t1,h, . . . ,aM,tM ,h} (e.g.,
room, service, and Michigan Ave. in Table 1) to
be included into h’s profile Ph, where ti is the type,
distinct or general, of each aspect. For the sake
of notational clarity, from now on, when talking
about a specific hotel h, we will omit the subscript
h. We first extract all noun phrases present in R,
and use these noun phrases as raw aspects. For
each raw aspect extracted, we identify whether it
is a distinct or general aspect: if the aspect is ex-
tracted as a proper noun phrase (as tagged by the
Stanford parser (Klein and Manning, 2003)), then
the aspect is labeled as distinct, and general other-
wise.

Noun phrase extraction is performed solely on

the syntactic parse trees. We do not attempt to
resolve coreferences for the following two rea-
sons: (1) since most coreference resolution tools
are trained on news texts, their performance on
product reviews might not be reliable; (2) we ob-
serve that authors of these product reviews rarely
employ coreference in their writing, and therefore
the impact of resolving coreference might be mi-
nor after all.

Aspect clustering Due to the flexibility of natu-
ral languages, it is common to see people express
the same concept through different lexical usage.
For example, both “price” and “rate” can refer to
the price of the hotel. In addition to synonyms,
semantically related words can also be adopted to
describe the same aspect of the product, for exam-
ple, “lighting” and “painting” can both describe
the decoration of the hotel.

In order to deal with the use of these (near-) syn-
onyms in text, we perform separate clustering for
the sets of distinct and general aspects.

For distinct aspects {ai,d}, the distance between
a pair of aspects is defined by the length of their
longest common substring divided by the aver-
age length of the two aspects. The motivation
for this definition is to bring distinct aspects such
as Michigan Ave. and Michigan Avenue into the
same cluster.

For general aspects {ai,g}, the distance between
a pair of aspects is defined as their Lin Similar-
ity (Lin, 1998), implemented by NLTK’s similar-
ity package (Bird et al., 2009).

For both types of aspects, we perform hierar-
chical agglomerative clustering with average link-
age. Cluster merging is terminated once the dis-
tance between all pairs of clusters is greater than
0.3.

Sorting aspects by occurrences After cluster-
ing distinct and general aspects separately, we
keep track of the total occurrences of each aspect
across the entire set R. Then, we let A be the set
of the most common N aspects4. N is a parameter
identical for all hotels, indicating how many as-
pects are included in each profile. We expect that
the choice of N is affected by the particular type
of product, and we experiment with several values
of N.

4N is usually much smaller than M, the total number of
raw aspects.

342



5.2.5 Description extraction
For a hotel h, each of its aspects ai is then associ-
ated with a description

Di = {pi,1 : wi,1, . . . , pi,m : wi,m},

which has two components: description words pi, j
and their weights wi, j, for 1≤ j ≤ m.

Description words The list {pi,1, . . . , pi,m} is
composed of the description words used to modify
the particular aspect ai, e.g., {wonderful, deluxe,
and huge} for the aspect room in Table 1.

For each distinct aspect ai,d , as discussed in
Section 5.2.2, there can be only one description
word, existence, indicating the presence of this as-
pect in the profile.

For each general aspect ai,g, its description
words are automatically extracted from the depen-
dencies in the set of reviews R. From the depen-
dencies output by the Stanford dependency parser
(de Marneffe et al., 2006), for a particular aspect
ai, we first locate all relevant dependency relations
which have ai (or the head noun of ai if ai is a noun
phrase) as the governors (or dependents, depend-
ing on the particular dependency types). Then
from these relevant dependency relations, we ex-
tract the words that appear in the dependent (or
governor) slots to be the description words, pro-
vided that those words are tagged as adjectives or
adverbs.

Weight assignment Each description word pi, j
is assigned a weight wi, j to indicate the relia-
bility of that particular description word. There
are many possibilities for these weight assignment
functions. Here, as a preliminary study, for dis-
tinct aspects {ai,d}, we simply use the number of
occurrences of the aspect in the review collection
R as the assigned weight wi, j, and for general as-
pects {ai,d}, we use the number of occurrences of
pi, j in R as wi, j.

5.3 Computing profile compatibility
Once we establish a profile P for each hotel h from
the set of relevant reviews R, then given an un-
seen test review r about h, we can perform a bidi-
rectional alignment with P, and compute a list of
compatibility features.

Using the same approach as described in Sec-
tions 5.2.4 and 5.2.5, we construct a profile Q from
this single review. For the sake of clarity, we call
Q the test profile, in contrast to P, the collective

profile, which is constructed from a collection of
reviews.

5.3.1 Bidirectional alignment
We perform a bidirectional alignment between the
test profile P and the collective profile Q. The di-
rection of an alignment is defined as the following.

When aligning the test profile Q with the collec-
tive profile P (Q→ P), we compute aspect-wise
compatibility features (to be defined in Section
5.3.2) for each aspect ai in P. Similarly, when
aligning the collective profile P with the test pro-
file Q (P→ Q), we compute for each aspect ai in
Q.

The Q→ P alignment captures whether the as-
pects that most reviewers would describe in their
reviews are mentioned in the test review, and
whether there exist any similarity or conflicts be-
tween the common opinions represented in the
collective profile P and the test review.

The P→ Q alignment captures whether the test
review falsely includes some imaginary extra as-
pects about the product, such as an indoor pool in
the hotel, while the collective profile P does not
include this aspect.

5.3.2 Aspect-wise compatibility features
For each direction of the bidirectional alignment,
we compute a list of aspect-wise compatibility fea-
tures. Without loss of generality, assume that the
direction of the alignment is Q → P. Thus, for
each aspect ai in P, we include three features to
indicate the compatibility between P and Q:

1. mai : a boolean value indicating whether ai is
mentioned in Q or not. If ai does appear in
Q, we then compute the following two val-
ues, based on ai’s description Di in P, and its
description D′i in Q.

2. sai ∈ [0,1]: a numeric value indicating the
similarity between P and Q with respect to
ai. Sai is computed as the cosine similar-
ity between Di and D′i. We treat Di and D

′
i

as two vectors following the bag-of-words
model, where the description words are the
words, and the weights are their correspond-
ing frequencies.

3. cai ∈ [0,1]: a numeric value indicating the
conflicts between P and Q with respect to
ai. For each description word pi in Di, we
determine whether D′i includes its antonym.

343



Antonyms are determined using the lexical
relations defined in WordNet5. cai is com-
puted as the fraction of description words in
Di that have corresponding antonyms in D′i.

Clearly, when computing aspect-wise similarity
score sai , we do not consider any synonyms or se-
mantic similarity between two description words
— the similarity is based solely on the use of ex-
act words. Our strategy of exact word matching is
a simplification, but since similarity of adjectives
and adverbs is not as well-studied as that of nouns
(various similarity metrics for nouns are available
for WordNet, but none of those work on adjectives
and adverbs), clustering description words is not
as trivial as clustering aspects6.

5.4 C+N-GRAM+SYN model
To test whether a given review r about the ho-
tel h is deceptive or not, we extend Feng et al.’s
N-GRAM+SYN model by incorporating our aspect-
wise compatibility features into it. We call this
extension the C+N-GRAM+SYN model, which in-
cludes the following two categories of features:

1. Alignment compatibility features

(a) Compatibility features for alignment
Q → P, including mai , sai , and cai for
each of the N aspects {a1, . . . ,aN} with
the highest occurrences in P.

(b) Compatibility features for alignment
P → Q, including ma′i , sa′i , and ca′i for
each of the N aspects {a′1, . . . ,a′N} with
the highest occurrences in Q.

2. Baseline features: 3,009 unigrams, 10,538
bigrams, and 6,571 syntactic production
rules, as described in Section 5.1.

The resulting dimension of our full feature set
is 20,118 + 2N, where N ranges from 10 to 100 in
our experiments (see Section 6.2). For all experi-
ments, we use SVMperf (Joachims, 2006) to train
all the linear SVM classifiers.

6 Results and discussion

6.1 Reimplementing baseline models
We first demonstrate the performance of our reim-
plemented baseline models. We conduct 5-fold

5http://wordnet.princeton.edu/wordnet/
6In fact, we have experimented with using LSA-based

word vectors to cluster description words, but the perfor-
mance is slightly lower than using exact word matching.

Features Prec Rec F1 Acc

SYN 87.2 88.8 88.0 87.9
N-GRAM 89.7 89.5 89.6 89.6
1-GRAM+SYN 88.0 90.0 89.0 88.9
2-GRAM+SYN 87.9 90.5 89.2 89.0
N-GRAM+SYN 89.2 91.3 90.2 90.1

Table 3: Results (in %) of our reimplemented
baseline models. Performance is reported us-
ing 5-fold cross-validation over the op spam v1.3
dataset (800 reviews of 20 hotels).

cross-validation experiments over the 800 reviews
of 20 hotels, in which each fold contains all re-
views from four hotels, such that the learned mod-
els are always tested against unseen hotels. Per-
formance is reported by accuracy and the micro-
averaged precision, recall, and F-score of re-
trieving deceptive reviews (see Ott et al. (2013)
for details). We experiment with five different
baseline feature sets: (1) SYN: syntax features,
(2) N-GRAM: unigram and bigram features, (3)
1-GRAM+SYN: unigram and syntax features, (4)
2-GRAM+SYN: bigram and syntax features, and
(5) N-GRAM+SYN: unigram, bigram and syntax
features, where the second is Ott et al.’s model7,
and the third is Feng et al. (2012a)’s model.

The results are shown in Table 3. Our reim-
plementation of Ott et al.’s model (N-GRAM) ob-
tains 89.6% accuracy, while the accuracy of our
reimplemented Feng et al. model (1-GRAM+SYN)
is noticeably lower (88.9%). Though its im-
provement over the N-GRAM model is not signifi-
cant, the N-GRAM+SYN model performs the best
(90.1%) among all baseline models, and thus is
chosen to be the baseline in our later experiments.

6.2 Comparison with baseline models

We now demonstrate the performance achieved by
our profile alignment compatibility features. We
experiment with using our profile alignment fea-
tures in combination with baseline features, i.e.,
the C+N-GRAM+SYN model. We first study the

7Actually, the best performance reported by Ott et al.
was obtained using their LIWC + N-GRAM model, which is
the combination of N-GRAM features and 80 features output
by the Linguistic Inquiry and Word Count (LIWC) software
(Pennebaker et al., 2007). However, since the improvement
over n-gram features alone was small and insignificant (0.2%
absolute improvement in accuracy), and we were not able
to successfully reproduce the improvement, the comparison
with our model is based on using N-GRAM features alone.

344



effect of the number of reviews used in construct-
ing collective profiles (see Section 5.2.3). We ex-
periment with using the top 50, 100, 150, and 200
reviews written by authors with the highest help-
fulness votes (for some hotels, the total number of
available reviews is less than 200). For each set of
constructed profiles, we tried different values of
N, the number of aspects included in each profile
(both collective and test profiles), ranging from 10
to 100, and determine the optimal number by con-
ducting 4-fold cross-validation experiments using
80% of the training data.

The results are shown in Table 4. First, we see
that the best overall performance, in terms of F1
score and accuracy, is achieved using the top 50 or
100 reviews for profile construction. This suggests
that using a fairly small amount of reliable data is
sufficient to recognize compatibility between col-
lective profiles and test reviews. In fact, using a
larger number of reviews is slightly detrimental to
the overall performance, since more noise might
be included in the process.

Moreover, we see that our best model, achieved
using 50 or 100 reviews for profile con-
struction, outperforms the best baseline model,
N-GRAM+SYN, on all four metrics, by over 1%,
which is a 12.1% error reduction rate. The im-
provement is confirmed to be statistically signifi-
cant (p < .05) using the Wilcoxon sign-rank test.
In addition, we inspect each model’s predictions
on individual test instances, and discover that our
C+N-GRAM+SYN model correctly classifies some
instances on which the baseline model makes an
error, and at the same time does not make any
additional errors that the baseline model does not
make.

Finally, with respect to the value of N, which is
the number of aspects included in each profile, we
discover that smaller values of N, i.e., 10 or 20,
consistently give the most satisfying results.

7 Conclusions and perspectives

In this paper we proposed the use of profile align-
ment compatibility as an indicator of truthfulness
in product reviews. We defined two types of com-
patibility between product profiles, and designed
a methodology to tackle them by extracting as-
pects and associated descriptions from reviews.
We adopted Ott et al.’s op spam v1.3 dataset of
hotel reviews, and improved the N-GRAM+SYN
model of Feng et al. (2012a). Our approach was

Reviews
used

N Prec Rec F1 Acc

50 10 90.2* 92.5* 91.4* 91.3*
100 10 90.0* 92.8* 91.4* 91.3*
150 20 90.6* 92.0* 91.3* 91.3*
200 20 90.4* 91.8* 91.1* 91.0*

Table 4: Results (in %) of our profile alignment
compatibility models. Performance is reported us-
ing 5-fold cross-validation over the op spam v1.3
dataset (800 reviews of 20 hotels). The best result
of each metric is shown in bold face. All num-
bers are significantly better (denoted by *) than the
baseline in Table 3, using the Wilcoxon sign-rank
test (p < .05).

shown to significantly improve the performance of
identifying deceptive reviews.

In future work, it is critical to know how well
our methodology of extracting aspects and their
descriptions from hotel reviews generalizes on
other kinds of reviews. We suspect that the ap-
proach should work equally well on other do-
mains, as long as the aspects are realized mainly
by noun phrases, especially that distinct aspects
are realized by proper noun phrases.

Another particularly interesting direction is to
explore how our C+N-GRAM+SYN model per-
forms on identifying fake negative reviews, as re-
cently released by Ott et al. (2013), rather than
the positive reviews used in this work. While
negative opinion spam is more hazardous to a
brand’s fame compared to positive ones, and thus
identifying fake negative reviews might be more
crucial, one potential difficulty of our approach
is that genuine extremely negative reviews writ-
ten by renowned reviewers are much more sparse
than extremely positive ones, especially for fa-
mous products, such as the most popular Chicago
hotels in the op spam v1.3 dataset.

Acknowledgments

This work was financially supported by the Nat-
ural Sciences and Engineering Research Council
of Canada and by the University of Toronto. We
would like to thank Myle Ott from Cornell Univer-
sity for kindly providing us with the op spam v1.3
dataset.

345



References
Steven Bird, Ewan Klein, and Edward Loper.

2009. Natural Language Processing with Python.
O’Reilly Media Inc.

Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of the 5th International Conference on
Language Resources and Evaluation (LREC 2006).

Song Feng, Ritwik Banerjee, and Yejin Choi. 2012a.
Syntactic stylometry for deception detection. In
Proceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 2:
Short Papers), pages 171–175, Jeju Island, Korea,
July. Association for Computational Linguistics.

Song Feng, Longfei Xing, Anupam Gogar, and Yejin
Choi. 2012b. Distributional footprints of decep-
tive product reviews. In Proceedings of Interna-
tional AAAI Conference on Weblogs and Social Me-
dia (ICWSM 2012), June.

Stephanie Gokhman, Jeff Hancock, Poornima Prabhu,
Myle Ott, and Claire Cardie. 2012. In search of
a gold standard in studies of deception. In Pro-
ceedings of the Workshop on Computational Ap-
proaches to Deception Detection, pages 23–30, Avi-
gnon, France, April. Association for Computational
Linguistics.

Nitin Jindal and Bing Liu. 2008. Opinion spam and
analysis. In Proceedings of First ACM Interna-
tional Conference on Web Search and Data Mining
(WSDM-2008), pages 219–230.

Nitin Jindal, Bing Liu, and Ee-Peng Lim. 2010. Find-
ing unusual review patterns using unexpected rules.
In Proceedings of the 19th ACM International Con-
ference on Information and Knowledge Manage-
ment (CIKM 2010), pages 1549–1552.

Thorsten Joachims. 2006. Training linear SVMs in lin-
ear time. In Proceedings of the 12th ACM SIGKDD
international conference on Knowledge discovery
and data mining, KDD ’06, pages 217–226, New
York, NY, USA. ACM.

Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting of the Association for Com-
putational Linguistics (ACL 2003), pages 423–430,
Sapporo, Japan, July.

Ee-Peng Lim, Viet-An Nguyen, Nitin Jindal, Bing Liu,
and Hady Wirawan Lauw. 2010. Detecting prod-
uct review spammers using rating behaviors. In Pro-
ceedings of the 19th ACM International Conference
on Information and Knowledge Management (CIKM
2010), pages 939–948.

Dekang Lin. 1998. An information-theoretic defini-
tion of similarity. In Proceedings of International
Conference on Machine Learning, Madison, Wis-
consin, USA, July.

Bing Liu. 2012. Sentiment Analysis and Opinion Min-
ing, volume 5 of Synthesis Lectures on Human Lan-
guage Technologies. Morgan & Claypool Publish-
ers, May.

Bin Lu, Myle Ott, Claire Cardie, and Benjamin K.
Tsou. 2011. Multi-aspect sentiment analysis with
topic models. In ICDM Workshops, pages 81–88.

Myle Ott, Yejin Choi, Claire Cardie, and Jeffrey T.
Hancock. 2011. Finding deceptive opinion spam
by any stretch of the imagination. In Proceedings
of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 309–319, Portland, Oregon, USA,
June.

Myle Ott, Claire Cardie, and Jeffrey T. Hancock. 2013.
Negative deceptive opinion spam. In Proceedings of
the 2013 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, Short Papers, At-
lanta, Georgia, USA, June. Association for Compu-
tational Linguistics.

James W. Pennebaker, Cindy K. Chung, Molly
Ireland, Amy Gonzales, and Roger J. Booth.
2007. The development and psychometric proper-
ties of LIWC2007. Technical report, Austin, TX,
LIWC.Net.

Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of Hu-
man Language Technologies 2007: The Conference
of the North American Chapter of the Association
for Computational Linguistics (NAACL-HLT 2007),
pages 404–411, Rochester, New York, April. Asso-
ciation for Computational Linguistics.

Victoria L. Rubin and Tatiana Vashchilko. 2012. Iden-
tification of truth and deception in text: Application
of vector space model to rhetorical structure theory.
In Proceedings of the Workshop on Computational
Approaches to Deception Detection, pages 97–106,
Avignon, France, April.

Ivan Titov and Ryan McDonald. 2008. A joint model
of text and aspect ratings for sentiment summariza-
tion. In Proceedings of the 46th Annual Meeting of
the Association for Computational Linguistcs: Hu-
man Language Technologies (ACL-08: HLT), pages
308–316, Columbus, Ohio, June. Association for
Computational Linguistics.

Guangyu Wu, Derek Greene, Barry Smyth, and Pádraig
Cunningham. 2010. Distortion as a validation cri-
terion in the identification of suspicious reviews. In
Proceedings of the First Workshop on Social Media
Analytics (SOMA 2010), pages 10–13, New York,
NY, USA.

Kyung-Hyan Yoo and Ulrike Gretzel. 2009. Com-
parison of deceptive and truthful travel reviews. In
Information and Communication Technologies in
Tourism 2009, pages 37–47. Springer Vienna.

346


