



















































MulCode: A Multiplicative Multi-way Model for Compressing Neural Language Model


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 5257–5266,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

5257

MulCode: A Multiplicative Multi-way Model for Compressing Neural
Language Model

Yukun Ma1∗, Pei-Hung(Patrick) Chen2*, Cho-Jui Hsieh2
1AIR Labs, Continental Automotive Group, Singapore 2Department of Computer Science, UCLA

Abstract

It is challenging to deploy deep neural nets on
memory-constrained devices due to the explo-
sion of numbers of parameters. Especially, the
input embedding layer and Softmax layer usu-
ally dominate the memory usage in an RNN-
based language model. For example, input
embedding and Softmax matrices in IWSLT-
2014 German-to-English data set account for
more than 80% of the total model parame-
ters. To compress these embedding layers, we
propose MulCode, a novel multi-way multi-
plicative neural compressor. MulCode learns
an adaptively created matrix and its multi-
plicative compositions. Together with a prior
weighted loss, MulCode is more effective than
the state-of-the-art compression methods. On
the IWSLT-2014 machine translation data set,
MulCode achieved 17 times compression rate
for the embedding and Softmax matrices, and
when combined with quantization technique,
our method can achieve 41.38 times compres-
sion rate with very little loss in performance.

1 Introduction

Deep neural language models with a large number
of parameters have achieved great successes in fa-
cilitating a wide range of Natural Language Pro-
cessing (NLP) applications. However, the over-
whelming size of parameters used by these models
stands out as the main obstacle hindering their de-
ployment on memory-constrained devices. Given
the over-parameterization of deep neural nets, the
effective compression of them has been receiving
increasing attention from the research community.
The neural language models typically consist of
three major components: the recurrent layers (e.g.,
a LSTM cell), an embedding layer for representing
input tokens, and a Softmax layer for generating
output tokens. The dimension of recurrent layers
is typically small and independent of the size of

∗ The two authors contribute equally

input/output vocabulary. In contrast, the embed-
ding layer and Softmax layer use a dense layer to
map the vocabulary to a lower dimensional space,
which grows linearly with the vocabulary size and
contributes mostly to memory consumption. For
example, the One Billion Word language model-
ing task (Chelba et al., 2013) has a vocabulary size
around 800k, and more than 90% of the memory
usage is used to store the input and Softmax matri-
ces. Compressing the word embedding, therefore,
is the key to reducing the memory usage of deep
neural language models.

There exist several studies for compressing neu-
ral language models. In particular, quantization
(Lin et al., 2016; Hubara et al., 2016) and group-
wise low rank approximation (Chen et al., 2018a)
achieve the state-of-the-art performance on tasks
like language modeling and machine translation.
However, these methods are rather computation-
ally inspired than being motivated by intuitions
of the semantic composition and thus suffer from
poor interpretability. In contrast, deep composi-
tional models (Shu and Nakayama, 2017; Chen
et al., 2018b) have been explored to represent the
original word vector with a set of discrete pseudo
words and thus decompose the word vector to the
addition of the corresponding pseudo word vectors
learned in an end-to-end manner. These methods
are seen as embracing the long-existing yet still,
the most popular assumption (Foltz et al., 1998)
on compositionality of semantics which states that
the meaning of an element (e.g., a word, phrase, or
sentence) can be obtained by taking the addition of
its constituent parts.

Despite it’s more semantically meaningful, cur-
rent deep compositional models have several lim-
itations. First, additive composition might turn
to be counter-intuitive for that the addition does
not bring new meaning beyond the individual el-
ement to the constructed element (Pinker, 2003;
Mitchell and Lapata, 2009, 2008). Besides, exist-



5258

ing deep code compressor is found hard to com-
press the output Softmax layer for that the con-
flicting codes (i.e. different words having same
encoding) might make the language model less
discriminative. Last, it is well-known that the
distribution of word frequencies can be approx-
imated by a power law. Defining importance of
words in terms of frequencies in the corpus is also
verified in (Chen et al., 2018a). However, cur-
rent deep compositional frameworks encode every
word equally without considering the frequency
information.

In this paper, we propose MulCode, a novel and
effective deep neural compressor to address the
above issues. MulCode uses a multiplicative com-
position instead of addition. The multiplicative
factors introduce a larger capacity empowering the
encoding of more complicated semantic. From the
perspective of semantic composition, this allows
introducing new information to the base code-
book vectors (sub-elements to compose a word
vector), taking frequency information of each to-
ken into consideration. Technically, MulCode em-
braces the research line of dense matrix decompo-
sition that has been investigated in many applica-
tions (Memisevic, 2011; Novikov et al., 2015). It
trains the composition with weighted loss propor-
tional to the frequency of each word. MulCode
also adopts an adaptive way of constructing code-
books based on the frequency of each word.

MulCode outperforms state-of-the-arts meth-
ods on compressing language models and neu-
ral machine translation models. For example,
on One-Billion-Word data set, MulCode achieves
18.2 times compression rate on embedding and
Softmax matrices without losing much of perfor-
mance. On IWSLT-14 DE-EN machine transla-
tion task, MulCode achieves 17 times compres-
sion with BLEU score difference smaller than
one percent. Results can be further improved to
41.38 times compression rate when combined with
quantization technique.

2 Related Work

2.1 Model Compression for Convolutional
Neural Network

Low-rank Approximation The original word
embedding vectors can be viewed as a large ma-
trix with the rows being the words and columns
being the vector values. One natural way to ap-
proximate it is to obtain the low-rank approxi-

mation of the matrix using SVD. Based on this
idea, (Sainath et al., 2013) compressed the fully
connected layers in neural nets. For convolu-
tion layers, (Jaderberg et al., 2014; Denton et al.,
2014) applied higher-order tensor decomposition
to compress CNN. Similarly, (Howard et al., 2017)
developed another structural approximation. (Kim
et al., 2016) proposed an algorithm to select rank
for each layer. (Yu et al., 2017) reconstructed the
weight matrices by using sparse plus low-rank ap-
proximation.

Pruning Many algorithms have been proposed
to remove those unimportant weights in deep neu-
ral nets. To do this, one needs to define the im-
portance of each weight. (LeCun et al., 1990)
showed that the importance can be estimated by
using the Hessian of loss function. (Han et al.,
2015b) considered adding `1 or `2 regularization
and applied iterative thresholding approaches to
achieve very good compression rates. Later on,
(Han et al., 2015a) demonstrated that CNNs can be
compressed by combining pruning, weight sharing
and quantization.

Quantization Using lower precision representa-
tions to store parameters has been used for model
compression. (Hubara et al., 2016) showed that
a simple uniform quantization scheme effectively
reduces both the model size and the prediction
time of a deep neural net. (Lin et al., 2016)
showed that non-uniform quantization can further
improve the performance. Recently, several ad-
vanced quantization techniques have been pro-
posed for CNN compression (Xu et al., 2018; Choi
et al., 2018).

2.2 Model Compression for Neural Language
Models

Despite model compression has been studied ex-
tensively for CNN models, fewer work have fo-
cused on the compression for deep neural nets
for NLP applications. In fact, directly apply-
ing popular approaches to NLP problems does
not provide a satisfactory result. (Hubara et al.,
2016) showed that the naive quantization can only
achieve less than 3X compression rate on PTB
data with no performance loss. (Lobacheva et al.,
2017) showed that for word-level LSTM models,
the pruning approach can only achieve 4 times
compression with more than 5% performance loss.
Most recently, (Chen et al., 2018a) proposed a
decomposition scheme based on block-wise low



5259

rank approximation of embedding matrix. Al-
though this method achieves competitive empir-
ical results, the learned model does not have a
strong semantic interpretation.

Additive Composition One of the most popu-
lar assumptions about the composition of seman-
tics is called the additive composition stating that
the meaning of a unit (e.g., word, phrase, or sen-
tences) can be obtained by summing up the mean-
ing of its constituents. At the word level, a word
might be decomposed into a set of subword units.
For example, “disagree” = “dis”+“agree”. Alter-
natively, a word can also be represented by a set of
relevant words, e.g., “king” = “man” + “crown”.
(Gittens et al., 2017) has validated this particu-
lar assumption for a popular word embedding ap-
proach (i.e., Skip-Gram (Mikolov et al., 2013)).

Based on this assumption, (Chen et al., 2016)
created a codebook by splitting the vocabulary
into two disjoint sets based on the word frequency:
the most frequent words and the rest. Less fre-
quent words are represented using a sparse lin-
ear combination of the vectors of more frequent
words. Instead of using an explicit set of words,
(Shu and Nakayama, 2017) designed the code-
books in a more data-driven fashion where the
selection of pseudo words and code vectors are
learned automatically using the Gumbel-Softmax
trick (Jang et al., 2016). Following the same ap-
proach, (Chen et al., 2018b) propose additional
training objective to integrate the learning of dis-
crete codes with the training of the language
model. Our method, based on the multiplicative
composition rather than addition, follows this re-
search line. The effectiveness of multiplicative
composition is verified in some language model-
ing tasks (Mitchell and Lapata, 2009, 2008).

3 Proposed Method

3.1 Multi-way Multiplicative Codes

Compositional models start with defining a set
of dm dimensional vectors, which serve as basic
codes to compose the targeted embedding matrix.
These vectors could further be separated into M
groups and say each group containsK vectors. We
call each group a codebook, and each dm dimen-
sional vector in the codebook a codeword.

An 1-way codebook then is defined as a
R1×M×K×dm tensor. Correspondingly, we de-
fine N -way codebooks as U ∈ RN×M×K×dm ,

where each of the N ways consists of a set of
M codebooks, each codebook contains K words,
and each codeword is associated with a dm dimen-
sional vector representing its semantics.

Since U is a high-order tensor which is not
memory-friendly, we model the composition of
U as applying a customized multiplicative oper-
ator on two tensors C ∈ RM×K×dm and S ∈
RN×M×dm as

U = C � S,

where � is a multiplicative operator defining that

Ui,j,k = Cj,k ◦ σ(Si,j),
∀i ∈ [1, N ], j ∈ [1,M ], and k ∈ [1,K],

σ(·) stands for the tangent hyperbolic function,
and ◦ is the Hadamard product (entry-wise prod-
uct). Tensor C is referred to as the base codebook,
consisting of M codebooks where each codebook
contains K codeword vectors of dm dimensions.
S is called rescaling codebooks. Each codeword
in the base codebook is injected with new mean-
ings by a code vector in the rescaling codebook.

Despite rescaling codebook uses much less
memory (O(NMdm)) than simply creating a N
times larger set of vectors (O(NMKdm)), us-
ing rescaling codebook still introduces additional
costs of the memory. We propose to further reduce
the cost by allowing rescaling code vectors to be
shared. We replace S with a S̃ ∈ RN×dm . That is,
for each of the N way codebook, we use only one
dm-dimensional vector to rescale the base code-
book C. The number of parameters in S can thus
be reduced and the computation of U becomes

Ui,j,k = Cj,k ◦ σ(S̃i),
∀i ∈ [1, N ], j ∈ [1,M ], and k ∈ [1,K].

Now, with the N -way codebook defined by C and
S̃, given an embedding matrix E with V vocabu-
laries (i.e. E ∈ RV×dm), we could represent E by
finding an encoding Q ∈ RV×N×M to compose
E from C and S̃. Qi contains encoding of cor-
responding vocabulary Vi. From each of N -way
n, and each of M codebooks m, Qi,n,m indicates
which codeword to compose. Let the word vector
for the ith word be ei. We could construct ei by

ei ≈ êi =
N∑

n=1

M∑
m=1

Un,m,Qi,n,m

=
N∑

n=1

M∑
m=1

Cm,Qi,n,m � σ(S̃n).



5260

Word embedding (!)

Gumbel Softmax Samples

Reconstructed
Word embedding

N-way Codebooks (")

#×%&'×(×%&

Base 
Codebooks ())

Rescaling 
Codebook ( *+)

Multiplicative Composition of Code Books

N-way Discrete Codes

MLP Encoder

Train

Test Lookup

Linear 
Combination

Reconstruct

Figure 1: Overall architecture of the multi-way multiplicative compressor

TheN -way discrete code could be learned in an
end-to-end manner by using the Gumbel-Softmax
trick (Jang et al., 2016). We first compute an en-
coding vector for the original word vector ei by
feeding it to a neural network

ai = Softmax(σ(W
>φ(W ′>ei + b) + b

′)),

where W , W ′, b and b′ are the parameters of the
network, and φ is the softplus function. ai repre-
sents a real value tensor in a shape N ×M ×K,
which is then fed to the Gumbel-Softmax to gen-
erate a continuous approximation of drawing dis-
crete samples with respect to the last dimension

D̂i = Softmax((log ai + g)/τ),

where g is a random noise vector sampled from
Gumbel distribution, and τ is the Softmax temper-
ature controlling how close is the sample vector
to a uniform distribution. D̂i is an approximately
one-hot out of K drawing for each way N and
codebook M . We can then compute approxima-
tion of each dimension of ei as

êi,d = U:,:,:,d � D̂i

Note that during training D̂i is generated as a
continuous approximation of the N -way discrete
code. At the testing phase, the fixed encoding
Qi of vector ei is directly computed as Qi,n,m =
arg maxk an,m,k. More details on Gumbel-trick
could be found in (Jang et al., 2016).

3.2 Group Adaptive Coding
Given the rescaling codebooks S̃ is small, the
memory consumption mainly consists of two
parts: code vectorsC and discrete codesQ. On the
one hand, the code vectors normally account for
the major memory usage when dealing with rela-
tively smaller vocabulary size. On the other hand,

the size of discrete codes grows linearly with the
size of vocabulary and the logarithm of K. To fur-
ther reduce the memory usage, we propose to use
codes of adaptive length and dimensions to deal
with the linear dependency with vocabulary size.
The intuition is to encode frequent words with
a longer code length to achieve a relative lower
reconstruction loss while representing rare words
with fewer codes. To achieve this, we first sort
the words according to their frequency and then
split words into fixed number of groups G. The
ith group could access only γGi = M × (1− i−1|G| )
codebooks.

In the same time, we store the low-rank ver-
sion of code vectors for some codebooks that were
mainly used for representing rare words.

Ci,j = WGici,

where ci ∈ RdGi and WGi ∈ RdGi×dm is the lin-
ear transformation matrix to be shared by all the
codebooks in the ith group. We resolve dGi in an
intuitive way as shown in Algorithm 1.

In practice, high frequency words tend to get
accesses to codebooks with higher rank while
fewer frequent words can only access codebooks
of lower dimension. Thanks to the long tail of rare
words, this could actually help save considerable
memory space. Normally, using a low compres-
sion rate results in dGi � γGi × K so that it is
guaranteed to reduce the number of parameters in
the base codebook.

3.3 Prior-weighted Reconstruction Loss

According to Zipf’s law, the frequency of words
conforms to a power law distribution. It means
that word which falls to the long tail may rarely
appear in the sentence. It motivates the modifi-
cation on the learning objective so that the com-
pressor will be more focused on words with high
frequency. In this paper, we use the distribution of



5261

Sort set G according to frequency;
d’: the minimum dimension ;
δ : targeted compression rate;
fGi : frequency of group Gi;
oGi : bits consumed by discrete code matrix
Q for each word in Gi;
n←δ × V × dm × 32(bits) ;
for i← 1 to |G| do

compute a ratio based on frequency
∆Gi ←

fGi∑|G|
i=1 fGi

;

calculate the dimension;
dGi ← (n− oGi)×∆Gi/(γGi + dm);
ensure dimension is valid within (d′, dm);
dGi ←min(max(dGi , d′), dm);
update n by subtracting used bits;
n← n− oGi − (γGi − γGi+1 + dm)× dGi

end
Algorithm 1: Algorithm to resolve the dimen-
sion of adaptive codebooks towards achieving a
targeted compression rate.

the words in the training set as a prior knowledge
to guide the learning of the compressor. In addi-
tion, similar to (Lin et al., 2017) we also want to
let each of the N way encoding focus on differ-
ent aspects of the original word embedding. The
proposed training objective function then becomes

L =
V∑
i=1

(− log p̂i‖êi − ei‖22

+ λ‖v̂iv̂>i − I‖22 + λ‖v̂>i v̂i − I‖22),

where p̂i represents the empirical distribution of
word Vi in the training set, ei is original word vec-
tor, êi is the reconstructed vector and v̂i ∈ RN×dm
is a compilation of reconstrcuted vectors from all
N -way codebooks in a matrix form (i.e. v̂i,n =∑M

m=1Cm,Qi,n,m�σ(S̃n) is the reconstructed vec-
tor from nth-way codebook). The new objective
function thus focuses on high-frequency words
and in the meanwhile allows the N-way coding to
encode different information.

4 Experiment

4.1 Data Sets and Models
Following the experimental protocol of (Chen
et al., 2018a), we evaluate our proposed method
with two important NLP tasks: language mod-
eling and machine translation. Table 1 summa-
rizes the key characteristics of the four data sets

Table 1: Statistics of data sets. The number in paren-
thesis shows the ratio of embedding layers (input plus
Softmax) respective to the entire model size

Models Vocab. Size Dimension Model Size
PTB-small 10k 200 17.7MB(85.8%)
PTB-large 10k 1500 251MB(45.4%)

OBW 793k 1024 6.8GB(91.2%)
NMT: DE-EN 30k 500 195MB(83.1%)

and models. PTB-small is using 2-layer LSTM-
based language model built on Penn Tree Bank
(PTB) data set. The vocabulary size is 10k. In-
put embedding layer and output Softmax layer are
both set to 200 dimensions. PTB-large is trained
on the same PTB data set as PTB-small except
the dimensions for input and output are increased
to 1500. Neural machine translation (NMT: DE-
EN) is a Seq2seq model initialized using Pytorch
checkpoints provided by Open-NMT (Klein et al.,
2017). The model is to perform German to En-
glish translation tasks on IWSLT-14 (Cettolo et al.,
2014) data set. One billion words (OBW) uses a
2-layer LSTM (referred to as ”2-LAYER LSTM-
8192-1024” by (Jozefowicz et al., 2016)) trained
on the OBW data set and the vocabulary size is
793,471. For LSTM-based language models, the
input embedding matrix and Softmax embedding
matrix account for the major memory usage (up
to 91.2%). Therefore, we target compressing both
the input and Softmax embedding matrices.

4.2 Implementation Details

We compress both input embedding and Softmax
matrices. We trained MulCode by using Adam op-
timizer with learning rate 0.001. For PTB data
set, we group the vocabulary using 3 groups and
8 groups for OBW. To resolve the dimension of
codebooks for adaptive coding, we use targeted
compression rate δ = 0.2 for PTB-small and
δ = 0.05 for the rest three models1.

After approximation, we retrain the rest of pa-
rameters by SGD optimizer with initial learning
rate 0.01. Whenever the validation perplexity does
not drop down, we decrease the learning rate to
an order smaller. We did not include results of
fine-tuning on OBW for that the re-training pro-
cess takes too long (few days) which is not com-
pliant with our motivation to compress the given
pre-trained embeddings.

1The original language models can be downloaded from
https://github.com/mayktian/MulCode.git

https://github.com/mayktian/MulCode.git


5262

The compression rate and corresponding per-
formance could certainly be plotted as a spec-
trum graph. The more we compress, the larger
the performance drop. In this paper, as far as
BLEU score is concerned, we report results of
compressed models when the BLEU falls within
3 percent difference from the original score. For
PTB data set, we target 3 percent drop of per-
plexity (PPL) after retrain. For OBW data set,
since it has a larger vocabulary size, we report re-
sults within 10 percent difference from the PPL
achieved by the uncompressed model. For each
method we tested various parameters and report
the smallest model size of the compression fulfill-
ing above criteria.

Notice that some previous methods compress
model directly during training phase (Khrulkov
et al., 2019; Wen et al., 2017). In contrast, our
problem setup follows (Chen et al., 2018a; Shu
and Nakayama, 2017; Chen et al., 2018b) that
given a pre-trained model, we want to compress
the model with limited fine-tuning.

4.3 Comparison with Baseline Models
We refer to our proposed method as MulCode
(Mul stands for both multi-way and multiplicative
composition). We mainly compare with two state-
of-the-art baseline compressors targeting com-
pressing the embedding layer.

1. GroupReduce We refer to the results re-
ported in (Chen et al., 2018a).

2. DeepCode The additive composition model
by (Shu and Nakayama, 2017). We use
the pytorch code2 released by (Shu and
Nakayama, 2017) to produce the results.

Table 2 summarizes the comparison between
the proposed methods and state-of-the-art base-
lines for the four benchmark data sets and LSTM
models. MulCode manages to compress the input
embedding layer and Softmax embedding layer 6
to 18 times without suffering a significant loss in
the performance.

In comparison, all the baseline models achieve
much lower compression rate with PTB-small
which has only 200 dimensions. It is reason-
able since embedding layers of PTB-small con-
tains less redundant information and thus can be
hardly compressed. As compared with DeepCode,

2https://github.com/zomux/
neuralcompressor

our method achieves much higher compression
rate3 for all the four models. Our method also con-
sistently and significantly outperforms GroupRe-
duce.

4.4 Comparison with Quantization

Quantization has been proven to be a strong base-
line. In fact, the discrete coding of MulCode can
be considered equivalent to a trainable quantiza-
tion. On the other hand, we need to point out
that quantization is not orthogonal to MulCode.
MulCode could be combined with quantization
to achieve better performance. Specifically, the
M × K base codebooks as well as the rescaling
codebook could be quantized to further reduce the
memory usage. We summarize the results in Ta-
ble 3. Quantized MulCode could achieve more
than 30.8 times compression for both input em-
bedding and Softmax matrix in OBW. In addition,
on machine translation task, it achieves 41.38X
with BLEU score drops around only 1% after re-
training. In particular, we observe that the effect
of retraining is more prominent for MulCode and
simple quantization compared to GroupReduce.
This implies that local precision lost for low-rank
basis in GroupReduce is more difficult to be re-
covered. In contrast, the collective information
of MulCode due to the compositional property is
more robust when imprecise local vectors present.

4.5 Ablation Analysis

We summarize the ablation analysis in Table 4.
The major technical features of MulCode are
rescaling codebooks (RC), prior-weighted recon-
struction loss (PRL), and the group adaptive cod-
ing (GAC). Besides, we also consider using full
rescaling books (Full) in place of the one shared
acrossM codebooks. We remove or add these fea-
tures one at a time. First, the rescaling codebook
is shown contributing largely to the performance.
On the other hand, since removing the rescaling
codebook is equivalent to degrading to using pure
additive composition, it also suggests the multi-
plicative composition is a better alternative to the
additive composition. In the meantime, removing
the rescaling codebook is equivalent to repeatedly
selecting from the base codebook. It may force the
codeword vector to encode the original meaning of

3Aligned with what has been mentioned by (Shu and
Nakayama, 2017) in their rebuttal, we found the DeepCode
unable to work with the Softmax layer of OBW no matter
how hard we have tuned it (99.5 as shown in Table 2).

https://github.com/zomux/neuralcompressor
https://github.com/zomux/neuralcompressor


5263

Table 2: Compressed model evaluation with 3 language models and 1 machine translation model. Memory usage
is reported as compared with the original input embedding and Softmax embedding. For example, 30x means the
compressed embeddings use only 1/30 the memory space of the original embedding.

Model Metric Original GroupReduce DeepCode MulCode

PTB-small
Memory 1X 4X 3.9X 6.5X

PPL(before retrain) 112.28 115.38 120.05 115.38
PPL(after retrain) - 113.81 115.57 113.34

PTB-large
Memory 1X 8X 3.8X 17.1X

PPL(before retrain) 78.32 84.79 84.73 83.85
PPL(after retrain) - 79.83 81.80 79.89

NMT: DE-EN
Memory 1X 8X 5.7X 17X

BLEU(before retrain) 30.33 29.31 26.58 29.48
BLEU(after retrain) - 29.96 29.37 30.08

OBW Memory 1X 6.6X 5.7X 18.2XPPL 31.04 32.47 99.59 32.66

Table 3: Results of compressing all input and Softmax embedding layers on three data sets. Experiments compare
proposed Quantized MulCode with traditional Quantization and Quantized GroupReduce. 20x means approxi-
mated embedding uses 20 times smaller memory compared to original input embedding layer and Softmax layer..

Model Metric Original Quantization Quantized GroupReduce Quantized MulCode

PTB-small
Memory 1X 6.4X 16X 16.3X

PPL(before retrain) 112.28 115.81 116.54 116.33
PPL(after retrain) - 114.14 114.39 113.34

PTB-large
Memory 1X 6.4X 20X 28.2X

PPL(before retrain) 78.32 81.69 81.53 81.55
PPL(after retrain) - 79.22 78.61 78.91

NMT:
ED-EN

Memory 1X 6.4X 32X 41.38X
BLEU(before retrain) 30.33 27.41 29.33 29.32
BLEU(after retrain) - 30.19 29.65 29.94

OBW Memory 1X 6.4X 26X 30.8XPPL(before retrain) 31.04 32.63 34.43 34.36

words as a mix of different aspects of the seman-
tics, which may turn out hard to be recovered. In
fact, we find it difficult to train a model that can
achieve low perplexity loss (33) on OBW with the
rescaling codebooks removed.

Removing the prior-weighted loss from the
model results in only marginal loss in the per-
formance except for PTB-small. It indicates that,
given the limited capacity of codebooks, the per-
formance of compressed model could benefit more
from lower reconstruction loss for those words
with the highest frequencies. For example, Mul-
Code achieves 3.5% less perplexity loss compared
to the model without using a prior-weighted re-
construction loss. Group adaptive coding is an-
other key factor to reduce memory usage. It shows
that it works well with all the four models. It han-
dles the memory reduction in both ways: reducing
the parameters in the code vectors and reducing
the number of discrete codes to represent a word.
For model with a very large vocabulary, it seems
to achieve higher memory reduction than with a
smaller model. We conjecture it is largely due
to the prominent presence of rare words in data

set like OBW. The significance of PRL and GAC
verifies the importance of frequency in compress-
ing natural words. Lastly, using the full tensor for
rescaling codebook does not seem to be necessary.
It fails to produce any improved performance of
the compressed model at the expense of a non-
trivial compression rate drop.

4.6 Selection of M ,N and K

6 8 10 12 14 16
Memory

115.0

117.5

120.0

122.5

125.0

127.5

130.0

132.5

PP
L

M
N
K

Figure 2: Influence of M,N,K for PTB-small.

To understand how the choices of M,N,K
would affect the performance of the compressed
model, we test the proposed model for PTB-small



5264

Table 4: Ablation analysis of MulCode. We test the compression model by removing (or adding) one feature at a
time. -RC stands for removing the rescaling codebook;-PRL stands for removing the prior-weighted reconstruc-
tion loss; -GAC means removing the group adaptive coding; + Full means to use the untied tensor for rescaling
codebook.

Model Metric -RC -PRL -GAC + Full MulCode

PTB-small Memory 3.95X 6.47X 5.68X 5.50X 6.47XPPL 124.78 115.58 115.65 115.96 115.38

PTB-large Memory 5.64X 15.42X 13.08X 10.20X 17.1XPPL 84.72 84.82 84.60 84.36 83.85

NMT: DE-EN Memory 9.02X 17.4X 11.24X 16X 17.44XBLEU 26.16 28.12 28.60 29.02 29.11

OBW Memory 9.4X 23.8X 16.8X 23.4X 23.8XPPL 37.42 34.05 33.45 32.81 32.95

with varying setting of M , N , and K. The de-
parture point of this experiment is using M =
32,K = 32, N = 8. We then adjust the three pa-
rameters one at a time while fixing the other two.
In order to plot the results in a single figure, we
set the x-axis as the memory usage instead of dif-
ferent values of the three parameters. As shown
in Figure 2, adjusting the values of M , N , and K
have similar effects on the PPL when the compres-
sion rate is low (≤7X). With a larger compression
rate, the performance becomes much more sensi-
tive to the change of K. It suggests that it is safer
to maintain a high value for K while tuning the
rest two parameters for the purpose of securing a
reasonable performance of the compressed model.

4.7 Understanding Composed Codes

At the core of our approach is the multiplicative
composition from two sets of codebooks. Hence,
it is interesting to investigate what has been en-
coded in the N -way codebooks. One assumption
is that each code in the base codebooks encodes
a mix of information which can be disentangled
by the rescaling codebook. We encode all the
words of OBW corpus by a 32× 8× 4 codes. We
compute the hamming distance of example query
words (shown in Table 5) for each of the N -way
codes. We select the top ones with the smallest
hamming distance from the 10,000 most frequent
words that are likely to have low reconstruction er-
rors.

Since the N -way codings are generated by se-
lecting from the base codebooks and modified by
rescaling codebooks, each channel can be seen as
meaningful subspace. It shows that each of theN -
way codings might have encoded a different sub-
space of the original meaning of words, including
tenses (e.g., halt v.s. halted), plurals (e.g., bank
v.s. banks), synonyms (e.g., soccer v.s. football),

Word 1 2 3 4
tomorrow today Sunday prompt Tuesday
beautiful elegant iconic great fields
soccer football hockey Ghana basketball
where when experiencing who learn
bank company police companies banks
halt stop halted afternoon cover
like just Like such is

Table 5: Most similar word computed using Hamming
distance for each of the N-way codings.

co-occurrence (like v.s. just), topical relatedness
(soccer v.s. hockey). It verifies that the multiplica-
tive composition used in our approach is able to
introduce new information to the base codebook.

5 Conclusion

In this paper, we propose a novel compres-
sion method for neural language models. Our
method applies multiplicative factors on end-to-
end learned codebooks. Our method also con-
siders the frequency information in the corpus by
adding weighted loss according to the importance.
At the same time, the coding scheme is made
adaptive based on the frequency information. Ex-
perimental results show that our method outper-
forms the state-of-the-art compression methods by
a large margin. In particular, on the IWSLT-14
data set, our method combined with quantization
achieves 41.38 times compression rate for both the
embedding and Softmax matrices. It will facili-
tate deployment of large neural language models
on memory-constrained devices.

6 Acknowledgements

Pei-Hung Chen and Cho-Jui Hsieh acknowledge
the support from NSF via IIS1719097, Intel and
Google Cloud.



5265

References
Mauro Cettolo, Jan Niehues, Sebastian Stüker, Luisa

Bentivogli, and Marcello Federico. 2014. Report on
the 11th iwslt evaluation campaign, iwslt 2014. In
Proceedings of the International Workshop on Spo-
ken Language Translation, Hanoi, Vietnam.

Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,
Thorsten Brants, Phillipp Koehn, and Tony Robin-
son. 2013. One billion word benchmark for measur-
ing progress in statistical language modeling. arXiv
preprint arXiv:1312.3005.

Patrick Chen, Si Si, Yang Li, Ciprian Chelba, and
Cho-Jui Hsieh. 2018a. Groupreduce: Block-wise
low-rank approximation for neural language model
shrinking. In Advances in Neural Information Pro-
cessing Systems 31, pages 10988–10998. Curran As-
sociates, Inc.

Ting Chen, Martin Renqiang Min, and Yizhou Sun.
2018b. Learning k-way d-dimensional discrete
codes for compact embedding representations. In
International Conference on Machine Learning,
pages 853–862.

Yunchuan Chen, Lili Mou, Yan Xu, Ge Li, and Zhi
Jin. 2016. Compressing neural language models by
sparse word representations. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), vol-
ume 1, pages 226–235.

Yoojin Choi, Mostafa El-Khamy, and Jungwon Lee.
2018. Universal deep neural network compression.
CoRR, abs/1802.02271.

Emily L Denton, Wojciech Zaremba, Joan Bruna, Yann
LeCun, and Rob Fergus. 2014. Exploiting linear
structure within convolutional networks for efficient
evaluation. In Advances in neural information pro-
cessing systems, pages 1269–1277.

Peter W Foltz, Walter Kintsch, and Thomas K Lan-
dauer. 1998. The measurement of textual coherence
with latent semantic analysis. Discourse processes,
25(2-3):285–307.

Alex Gittens, Dimitris Achlioptas, and Michael W Ma-
honey. 2017. Skip-gram- zipf+ uniform= vector ad-
ditivity. In Proceedings of the 55th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), pages 69–76.

Song Han, Huizi Mao, and William J. Dally. 2015a.
Deep compression: Compressing deep neural net-
work with pruning, trained quantization and huff-
man coding. CoRR, abs/1510.00149.

Song Han, Jeff Pool, John Tran, and William J. Dally.
2015b. Learning both weights and connections for
efficient neural networks. CoRR, abs/1506.02626.

Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry
Kalenichenko, Weijun Wang, Tobias Weyand,

Marco Andreetto, and Hartwig Adam. 2017. Mo-
bilenets: Efficient convolutional neural networks
for mobile vision applications. arXiv preprint
arXiv:1704.04861.

Itay Hubara, Matthieu Courbariaux, Daniel Soudry,
Ran El-Yaniv, and Yoshua Bengio. 2016. Quantized
neural networks: Training neural networks with low
precision weights and activations. arXiv preprint
arXiv:1609.07061.

Max Jaderberg, Andrea Vedaldi, and Andrew Zisser-
man. 2014. Speeding up convolutional neural net-
works with low rank expansions. arXiv preprint
arXiv:1405.3866.

Eric Jang, Shixiang Gu, and Ben Poole. 2016. Categor-
ical reparameterization with gumbel-softmax. arXiv
preprint arXiv:1611.01144.

Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam
Shazeer, and Yonghui Wu. 2016. Exploring
the limits of language modeling. arXiv preprint
arXiv:1602.02410.

Valentin Khrulkov, Oleksii Hrinchuk, Leyla Mir-
vakhabova, and Ivan Oseledets. 2019. Tensorized
embedding layers for efficient model compression.
arXiv preprint arXiv:1901.10787.

Yong-Deok Kim, Eunhyeok Park, Sungjoo Yoo, Taelim
Choi, Lu Yang, and Dongjun Shin. 2016. Compres-
sion of deep convolutional neural networks for fast
and low power mobile applications. In ICLR.

Guillaume Klein, Yoon Kim, Yuntian Deng, Jean
Senellart, and Alexander Rush. 2017. OpenNMT:
Open-source toolkit for neural machine translation.
In Proceedings of ACL 2017, System Demonstra-
tions, pages 67–72, Vancouver, Canada. Association
for Computational Linguistics.

Yann LeCun, John S Denker, and Sara A Solla. 1990.
Optimal brain damage. In Advances in neural infor-
mation processing systems, pages 598–605.

Darryl Lin, Sachin Talathi, and Sreekanth Anna-
pureddy. 2016. Fixed point quantization of deep
convolutional networks. In International Confer-
ence on Machine Learning, pages 2849–2858.

Zhouhan Lin, Minwei Feng, Cicero Nogueira dos San-
tos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua
Bengio. 2017. A structured self-attentive sentence
embedding. arXiv preprint arXiv:1703.03130.

Ekaterina Lobacheva, Nadezhda Chirkova, and Dmitry
Vetrov. 2017. Bayesian sparsification of recurrent
neural networks. arXiv preprint arXiv:1708.00077.

Roland Memisevic. 2011. Learning to relate im-
ages: Mapping units, complex cells and simultane-
ous eigenspaces. arXiv preprint arXiv:1110.0107.



5266

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in neural information processing
systems, pages 3111–3119.

Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. proceedings of
ACL-08: HLT, pages 236–244.

Jeff Mitchell and Mirella Lapata. 2009. Language
models based on semantic composition. In Proceed-
ings of the 2009 Conference on Empirical Methods
in Natural Language Processing: Volume 1-Volume
1, pages 430–439. Association for Computational
Linguistics.

Alexander Novikov, Dmitrii Podoprikhin, Anton Os-
okin, and Dmitry P Vetrov. 2015. Tensorizing neural
networks. In Advances in neural information pro-
cessing systems, pages 442–450.

Steven Pinker. 2003. The language instinct: How the
mind creates language. Penguin UK.

Tara N Sainath, Brian Kingsbury, Vikas Sindhwani,
Ebru Arisoy, and Bhuvana Ramabhadran. 2013.
Low-rank matrix factorization for deep neural net-
work training with high-dimensional output tar-
gets. In Acoustics, Speech and Signal Processing
(ICASSP), 2013 IEEE International Conference on,
pages 6655–6659. IEEE.

Raphael Shu and Hideki Nakayama. 2017. Compress-
ing word embeddings via deep compositional code
learning. arXiv preprint arXiv:1711.01068.

Wei Wen, Yuxiong He, Samyam Rajbhandari, Wen-
han Wang, Fang Liu, Bin Hu, Yiran Chen, and Hai
Li. 2017. Learning intrinsic sparse structures within
long short-term memory. CoRR, abs/1709.05027.

Yuhui Xu, Yongzhuang Wang, Aojun Zhou, Weiyao
Lin, and Hongkai Xiong. 2018. Deep neural net-
work compression with single and multiple level
quantization. CoRR.

Xiyu Yu, Tongliang Liu, Xinchao Wang, and Dacheng
Tao. 2017. On compressing deep models by low
rank and sparse decomposition. 2017 IEEE Confer-
ence on Computer Vision and Pattern Recognition
(CVPR), pages 67–76.


