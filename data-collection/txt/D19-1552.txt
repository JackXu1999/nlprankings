



















































Emotion Detection with Neural Personal Discrimination


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 5499–5507,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

5499

Emotion Detection with Neural Personal Discrimination

Xiabing Zhou1, Zhongqing Wang1∗, Shoushan Li1,
Guodong Zhou1, Min Zhang1

1School of Computer Scienceand Technology, Soochow University, China
wangzq.antony@gmail.com,

{zhouxiabing, lishoushan, gdzhou, minzhang}@suda.edu.cn

Abstract

There have been a recent line of works to auto-
matically predict the emotions of posts in so-
cial media. Existing approaches consider the
posts individually and predict their emotion-
s independently. Different from previous re-
searches, we explore the dependence among
relevant posts via the authors’ backgrounds,
since the authors with similar backgrounds,
e.g., gender, location, tend to express similar
emotions. However, such personal attributes
are not easy to obtain in most social media
websites, and it is hard to capture attributes-
aware words to connect similar people. Ac-
cordingly, we propose a Neural Personal Dis-
crimination (NPD) approach to address above
challenges by determining personal attributes
from posts, and connecting relevant posts with
similar attributes to jointly learn their emo-
tions. In particular, we employ adversarial
discriminators to determine the personal at-
tributes, with attention mechanisms to aggre-
gate attributes-aware words. In this way, social
correlationship among different posts can be
better addressed. Experimental results show
the usefulness of personal attributes, and the
effectiveness of our proposed NPD approach
in capturing such personal attributes with sig-
nificant gains over the state-of-the-art models.

1 Introduction

The advent of social media and its prosperity en-
able the creation of massive online user-generated
content including opinions and product reviews.
Analyzing such user-generated contents allows to
detect the users’ emotional states, which are useful
for various downstream applications.

In the literature, there are a large number of
works on emotion detection (Roberts et al., 2012;
Abdul-Mageed and Ungar, 2017; Gupta et al.,
2017), both discrete and neural models have been

∗ corresponding auther

used to predict the emotions of posts in social me-
dia. For example, Roberts et al. (2012) used a
series of binary SVM classifiers to detect the e-
motion of a post, while Gupta et al. (2017) used
sentiment based semantic embedding and a LST-
M model to learn the representation of a post for
emotion detection.

Different from previous researches, which con-
sider each post individually, we think that post-
s in social media are much correlated by the au-
thors’ backgrounds. Motivated by the principle
of homophily (Lazarsfeld et al., 1954), the idea
that similarity and connection tend to co-occur, or
“birds of a feather flock together”, suggests that
users, connected by mutual personal backgrounds,
may hold similar opinions toward a post (Thelwal-
l, 2010). In the literature, the personal attributes,
such as gender, location, age, have been proved
useful in personal background construction (Li
et al., 2016): people with different attributes tend
to express their emotions through different ways.
For example, the happiness emotion in [E1] is
expressed through some femininity sense word-
s, such as “little brother”, “handsome”, while the
happiness emotion in [E2] is expressed using a
dialectal word “bashi(comfortable)”, which con-
tains strong characteristic of the Sichuan dialect1.
Therefore, it is necessary to jointly detect the emo-
tion of posts with the personal attributes.

[E1] ]:-ý7î: �°�å
å��/��
(Congratulations to Chinese basketball
team on becoming the champion, the
little brother of Xinjiang is really hand-
some.)

1Sichuan is a southwest province in China, Sichuan di-
alect(Sichuanese) is quite specific and different from standard
Mandarin.



5500

[E2]ÙÍ)��Ù*�/ô��
(It is just bashi to eat this in such
weather!)

However, the personal attributes are not easy to
obtain in most social media websites. On one
hand, most websites may not contain useful per-
sonal information. On the other hand, people
are normally not willing to attach their person-
al information in social media. Besides, inte-
grating personal attributes into emotion detec-
tion is challenging, since it is hard to capture
attributes-aware words, such as “little brother” and
“bashi”(comfortable), to connect the posts with
similar backgrounds. Although there are some
related works on either personal attribute extrac-
tion (Wang et al., 2014) or emotion detection with
personal attributes (Li et al., 2016), none of them
address both challenges at the same time.

In this paper, we propose a Neural Personal Dis-
crimination (NPD) model with both adversarial
discriminators and attention mechanisms to tack-
le above challenges. Here, the Adversarial dis-
criminators (Goodfellow et al., 2014) are used to
determine the personal attributes, e.g., gender or
location, of a post, providing the inherent corre-
lationship between emotions and personal back-
grounds, while the Attention mechanisms (Wang
et al., 2016) are utilized to aggregate the repre-
sentation of informative attributes-aware words in-
to a vector for emotion prediction, providing in-
sights into which words contribute to a personal
background. Experimental results show the use-
fulness of personal attributes in emotion detection,
and the effectiveness of our proposed NPD model
with both adversarial discriminators and attention
mechanisms over the state-of-the-art discrete and
neural models.

2 Related Work

Earlier works on emotion detection are based on
discrete models. For example, Yang et al. (2007)
built a support vector machine (SVM) model and
a conditional random field (CRF) model for the e-
motion detection. Bhowmick et al. (2009) used
a multi-label kNN model to classify a new sen-
tence into multiple emotion categories. Quan et al.
(2015) proposed a logistic regression model for
social emotion detection. Recently, with the devel-
opment of artificial intelligence, neural network
models have been successfully applied to vari-
ous NLP tasks (Collobert et al., 2011; Goldberg,

2016). However, few works use neural network
models for emotion detection. Abdul-Mageed and
Ungar (2017) used a gated recurrent neural net-
work model for emotion detection with a large-
scale dataset. Zhang et al. (2018) used an auxiliary
and attention based LSTM to detect emotion on a
cross-lingual dataset.

Lexicon and social information are very impor-
tant for emotion detection, and there are many
researches focus on this topic. For example, S-
trapparava and Mihalcea (2008) used WordNet-
Affect to compute the sentimental score of a post.
More recently, In addition, Hovy (2015) used both
the age and gender information of the authors to
improve the performance of sentiment analysis.
Vosoughi et al. (2016) explored the relationship a-
mong locations, date time, authors and sentiments.

Different from previous works which consid-
er each post individually, we think that the post-
s in social media can be connected through the
authors’ backgrounds and should be better ad-
dressed. On the basis, we propose a neural person-
al discrimination model to determine the person-
al background attributes from each post through
adversarial discriminators, and aggregate the rep-
resentation of informative attributes-aware words
through attention mechanisms.

3 Vanilla Model for Emotion Detection

In this section, we propose a vanilla model. In
the next section, we show how to utilize the neu-
ral personal discrimination model to improve the
vanilla model by capturing personal attributes.

3.1 Document Representation

In general, we denote a post as a document d with
n words {w1, w2, ..., wn}. Given the post, we
use a standard Long Short-Term Memory (LST-
M) model to learn the shared document represen-
tation. Specially, we transform each token wi into
a real-valued vector xi using the word embedding
vector of wi, obtained by looking up a pre-trained
word embedding table D via the skip-gram algo-
rithm to train embeddings (Mikolov et al., 2013).
We then employ the LSTM model over d to gen-
erate a hidden vector sequence {h1, h2, ..., hn}.
At each step t, the hidden vector ht of the LST-
M model is computed based on the current vec-
tor xt and the previous vector ht−1 with ht =
LSTM(xt, ht−1). The initial state and all stand
LSTM parameters are randomly initialized and



5501

tuned during training.

3.2 Multi-label Emotion Detection

Emotion detection aims to predict the emotion la-
bels of posts. We follow (Wang et al., 2016) which
adopts five kinds of emotions2 in the study. S-
ince there may be more than one emotion in a
post, emotion detection can be considered as a
multi-label classification task: we use K emotion-
specific binary perceptions (K = 5) to predict
if the post has the corresponding emotion or not.
The advantage of multi-label classification is that
it learns and predicts all the emotion labels jointly.

Formally, giving an input vector H , a hidden
layer is first used to induce a set of high-level fea-
tures for each emotion j:

H̃j = σ(W jH + bj), (1)

and then, Hj is used as inputs to a softmax output
layer:

ŷj = softmax(W̃ jH̃j + b̃j) (2)

Here, W j , bj , W̃ j , and b̃j are model parameters.

3.3 Training

Given the word sequence in a post, our training ob-
jective is to minimize the cross-entropy loss over
a set of training examples (xi, yi)|Ni=1, with a `2-
regularization term,

Jy(θy) = −
N∑
i=1

K∑
j=1

yji log ŷ
j
i +

λ

2
||θy||2 (3)

where yji represents the label of the j-th emotion
for xi, θy is the set of model parameters and λ is
the parameter for `2 regularization.

In this paper, the model parameters are opti-
mized by AdaGrad (Duchi et al., 2011), and Skip-
gram algorithm (Mikolov et al., 2013) is used for
word embedding.

4 Neural Personal Discrimination Model

The drawback of above vanilla model is that it
does not consider the deep personal correlation-
ship among different posts.

2The five emotions include happiness, sadness, anger,
fear, and surprise, please refer to “Experiments Section” of
the paper for more details.

In this study, we think that the posts in social
media can be connected through the authors’ back-
grounds. Therefore, we propose a Neural Person-
al Discrimination (NPD) model to connect peo-
ple and learn their emotions collectively. We use
adversarial discriminators to determine the per-
sonal attributes to construct the personal profiles,
and employ attention mechanisms to aggregate
attributes-aware words. In this way, the social cor-
relationship between different posts can be well
addressed.

Figure 1 illustrates our proposed neural person-
al discrimination model for emotion detection. In
particular, we first learn the representation of each
post using a LSTM model, same as the vanilla
joint model. Then, we use adversarial discrimi-
nators to determine the personal attributes of each
post. Finally, we employ attention mechanism-
s to aggregate the representation of informative
attributes-aware words into a vector for the emo-
tion prediction. In the following of this section,
we illustrate the details of the infrastructure one
by one.

4.1 Personal Adversarial Discriminators

A straightforward way to jointly detect personal
attributes and the emotion of a post is to treat emo-
tion detection as a multi-label classification. How-
ever, such model may not be able to separate the
posts from different attributes directly, and thus
fail to learn the correlationship between the emo-
tion of a post and the personal backgrounds of the
authors. To address this issue, we utilize adver-
sarial discriminators to determine the personal at-
tributes of the authors, and to learn the emotion
and the attributes of the authors collectively. Ad-
versarial networks have achieved much success in
various studies, especially in image and text gen-
eration (Goodfellow et al., 2014; Wang and Wan,
2018; Fedus et al., 2018). In this part, we pro-
pose two adversarial discriminators, i.e, a gender
discriminator and a location discriminator, to de-
termine the personal attributes of each post.

Gender Discriminator. The gender discrimi-
nator is employed to determine the author’s gender
of each post. Let gi ∈ [0, 1] represents the proba-
bility of the gender label (female or male) for the
gender discriminator, and f is the function param-
eterized by θf which maps an embedding vector to
a hidden representation hgi from the post xi. Here,
the gender discriminator G(hgi ; θg) → ĝi param-



5502

Figure 1: Overview of the neural personal discrimination model.

eterized by θg maps a hidden representation vec-
tor hgi to a predicted gender label ĝi with the loss
function is:

Jgend =

N∑
i=1

{gi log ĝi+(1−gi) log(1− ĝi)}, (4)

where ĝi = G(f(xi)).
In this study, the gender discriminator is trained

towards a saddle point of the loss function through
maximizing the loss over θg while minimizing the
loss over θf (Ganin et al., 2017).

Location Discriminator. The location discrim-
inator is employed to determine the authors’ loca-
tion of a post3. Let `ji ∈ [0, 1] represents probabil-
ity of the j-th location information of the i-th post
and j ∈ {1, 2, . . . ,m}, where, m is the number of
provinces. The loss function is:

Jloc =

N∑
i=1

m∑
j=1

`ji log
ˆ̀j
i , (5)

where, ˆ̀ji = L(f(xi)) and L(h
`
i ; θ`) are the pa-

rameters of the location discriminator, h`i = f(xi)
is the hidden represent from the post xi

From the optimization of both discriminators,
we can find that both hg and h` represent the la-
tent feature representation of posts, which inte-
grate the discrimination of various personal infor-
mation. With the goal at G(θg) and L(θ`) try best

3In this study, location discriminator is used to detect the
author’s province of a post.

to determine the gender and location of the au-
thors. The adversarial network makes use of min-
max optimization.

4.2 Personal Attention Mechanisms

In emotion detection, not all words contribute e-
qually to the representation of emotions and per-
sonal attributes. Hence, we employ attention
mechanisms to extract the words that are impor-
tant to the personal backgrounds of posts, and to
aggregate the representations of those informative
attributes-aware words. With regard to the two
adversarial discriminators, we propose attention
mechanisms to build two representation (vg and
v`) from the gender and location discriminators
respectively, and then concatenate them togeth-
er to construct the overall personal representation
through the informative attributes-aware words .

Gender Attention. We use an attention func-
tion to aggregate the gender-aware representation
of the salient words to formulate the gender atten-
tion vector vg. Here, the gender attention mod-
el outputs a continuous vector vg ∈ Rd×1 recur-
rently by feeding the hidden representation vec-
tors {hg1, h

g
2, · · · , h

g
nt} as inputs. Specifically, vg

is computed as a weighted sum of hgi (0 ≤ i ≤ nt),
namely

vg =

nt∑
i

αih
g
i (6)

where nt is the hidden variable size, αi ∈ [0, 1]
is the weight of hgi , and

∑
i αi = 1. For each



5503

piece of hidden state hgi , the scoring function is
calculated as follows:

vgi = tanh(Wgh
g
i + b

g) (7)

αi =
exp(vg)∑
j exp(v

g
j )

(8)

Location Attention. Similar with the gender
attention mechanism, the location attention mod-
el outputs a continuous vector v` ∈ Rd×1 recur-
rently by feeding the hidden representation vec-
tors {h`1, h`2, · · · , h`nk} as inputs. Specifically, v

`

is computed as a weighted sum of h`i (0 ≤ i ≤ nt),
namely

v` =

nk∑
i

βih
`
i (9)

v`i = tanh(W`h
`
i + b

`) (10)

βi =
exp(v`)∑
j exp(v

`
j)

(11)

where, nk is the location hidden variable size, and
βi is the same setting as αi.

Finally, we concatenate vg and v` to capture the
overall personal representation through all the per-
sonal attributes discriminators.

V = vg ⊕ v` (12)

4.3 Adversarial Training with Neural
Personal Discrimination

The proposed NPD model can be trained in a end-
to-end manner once we obtain the loss function of
the emotion detector and the attribute discrimina-
tors. Our ultimate training goal is to minimize the
loss function with parameters θ = {θf , θy, θg, θ`}
as follow:

J(θ) = λ1Jy + λ2Jgend + λ3Jloc (13)

Whereλ1, λ2 and λ3 are the weight parameters to
balance the importance of losses between the emo-
tion detection and the two personal attribute dis-
criminators.

Specifically, Eq. 13 is defined by finding a sad-
dle point θ̂y, θ̂f , θ̂g, θ̂` such that

(θ̂f , θ̂y) = arg min
θf ,θy

J(θf , θy, θ̂g, θ̂`) (14)

θ̂g = argmax
θg

J(θ̂f , θ̂y, θg, θ̂`) (15)

θ̂` = argmax
θ`

J(θ̂f , θ̂y, θ̂g, θ`) (16)

As suggested previously, a saddle point is de-
fined by Eq. 14−Eq. 16, and can be achieved as a
stationary point the gradient updates:

θf ← θf − µ(λ1
∂J iy
∂θf
− λ2

∂J igend
∂θf

− λ3
∂J iloc
∂θf

)

(17)

θy ← θy − µλ1
∂J iy
∂θy

(18)

θg ← θg − µλ2
∂J igend
∂θg

(19)

θ` ← θ` − µλ3
∂J iloc
∂θ`

(20)

where µ is the learning rate.

5 Experimentation

5.1 Experimental Settings

We collect the data from Weibo.com, one of the
most popular SNS websites in China. We crawl
all the posts and corresponding personal profiles
from the website. The dataset contains 11,157 mi-
croblog posts from 839 users. We employed six
graduated students to annotate the corpus with a
well-defined annotation guideline. Every two an-
notators annotate a same part of corpus, if they
have disagreement on some posts, we ask anoth-
er annotator to vote with them. The annotation
guideline is based on Lee et al. (2013). Five basic
emotions are annotated, namely happiness, sad-
ness, fear, anger and surprise (Lee et al., 2013;
Wang et al., 2017). Table 1 illustrates the statis-
tics of each emotion. From the table, we find that
the frequency of happiness and sadness are simi-
lar. Moreover, the frequency of fear and anger is
much less than other three emotions.

We randomly select 70% posts as the training
data, and remaining 30% posts as the test data.
For evaluation, F1-measure is used to evaluate the
performance of proposed model in each emotion.
Average F1-measure is used to evaluate the overall
performance of all emotions.

The setting of hyper-parameters is: vocabulary
size is 2000, batch size is set to 32, dropout ratio
is 0.2, learning rate µ is set 0.0001, and λ1 : λ2 :
λ3 = 1 : 1 : 1



5504

Table 2: Experimental results of different models.

Method F1.
Happiness Sadness Anger Surprise Fear Average

SVM 0.628 0.462 0.390 0.117 0.091 0.338
Abdul17 0.656 0.492 0.429 0.103 0.111 0.358

Vaswani17 0.644 0.494 0.392 0.113 0.121 0.353
NPD 0.657 0.510 0.459 0.135 0.127 0.378

Table 1: The statistics of emotion distribution in the
dataset

Emtion Post Number
Happiness 2,915
Sadness 2,454

Fear 359
Anger 153

Surprise 601
None 4,675

5.2 Experimental Results

We compare the proposed Neural Personal Dis-
crimination (NPD) model with several represen-
tative baselines models in Table 2, where,1) SVM
is a widely used baseline to predict the emotion of
a post in social media (Yang et al., 2007).2) Ab-
dul17 is a standard LSTM model which consist of
a LSTM layer and a fully connected layer, and it
is modified from the model in Abdul-Mageed and
Ungar (2017). The LSTM model yields the state-
of-the-art performance on emotion detection in re-
cent researches.3)Vaswani17 is an improved LST-
M model with a self-attention mechanism. The
self-attention mechanism is used to capture the
structural information and has been successful-
ly applied in various natural language processing
tasks recently (Cheng et al., 2016; Vaswani et al.,
2017)

From Table 2, we find that all of the neural mod-
els outperform SVM significantly. This indicates
that neural models are much more effective than
discrete models in emotion detection. In addition,
our proposed NPD model outperforms both the s-
tandard LSTM model (Abdul17) and the improved
LSTM model with self-attention (Vaswani17) sig-
nificantly. This shows the effectiveness of our pro-
posed NPD model with both adversarial discrimi-
nators and attention mechanisms. This also shows
the usefulness of personal attributes for emotion
detection. Moreover, we find that the performance
of Vaswani17 is even lower than the standard L-

STM model. This shows that simply integrating a
self-attention mechanism may not be able to well
capture informative words for emotion detection
in social media.

5.3 Analysis and Discussion

In this subsection, we analyze the influence of d-
ifferent factors in the proposed NPD model, and
give some statistics and examples to illustrate the
effectiveness of the proposed NPD model with d-
ifferent personal attributes.

5.3.1 Influence of Personal Attributes

We illustrate the influence of personal attributes
in the proposed NPD model in Table 3, where,
1)LSTM-attributes is a LSTM based multi-label
classification model, which predicts both the e-
motion and the attribute labels of each post col-
lectively.2)NPD-gender ablates the location at-
tribute, i.e., only considering the gender attributes
in the NPD model.3) NPD-location ablates the
gender attribute, i.e., only considering the location
attributes in the NPD model.

From the table, we can find that the perfor-
mance of LSTM-attributes is much lower than L-
STM model. This indicates that simple multi-label
classification setting is not effective for integrat-
ing personal attributes. This may be due to the
fact that basic multi-label setting fails to learn the
correlationship between the emotions of posts and
the personal attributes of the authors well. In addi-
tion, both the NPD-gender and the NPD-location
perform better than the LSTM model respectively.
This shows the effectiveness of the proposed N-
PD model with both adversarial and attention net-
works, and the usefulness of both gender and lo-
cation attributes. Finally, the proposed NPD mod-
el with all the attributes significantly outperforms
all the other models. This suggests that we should
integrate all the personal attributes for emotion de-
tection in social media.



5505

Table 3: Comparison of various models with different
personal attributes.

Method Average F1.
LSTM 0.358

LSTM-attributes 0.341
NPD-gender 0.360

NPD-location 0.363
NPD 0.378

Table 4: Influence of network structures.

Method Average F1.
LSTM 0.358

LSTM-attention 0.363
LSTM-adversarial 0.375

NPD 0.378

5.3.2 Influence of Network Structures
After analyzing the influence of different at-
tributes, we analyze the influence of network
structures in Table 4, where LSTM-attention ab-
lates the adversarial discriminators, and only uti-
lizes attention mechanisms with a multi-label clas-
sification setting, and LSTM-adversarial ablates
the attention mechanisms and only utilizes adver-
sarial discriminators for emotion detection.

From Table 4, we can see that both the attention
mechanisms (LSTM-attention) and the adversar-
ial discriminators (LSTM-adversarial) are effec-
tive in emotion detection. Moreover, the adver-
sarial discriminators are much more effective than
the attention mechanisms. This shows that the
personal attribute discriminator is more important
than learning informative attributes-aware words.
Moreover, the NPD model obtains the best results
by integrating both adversarial discriminators and
attention mechanisms.

5.3.3 Statistics
We give the statistics of gender and location to
explore the correlationship between emotion and
personal attributes.

Distribution of Gender. Figure 2 illustrates the
distribution of emotions between genders. Here,
the Y-axis is conditional probability of each emo-
tion given gender. From the figure, we can find
that women tend to express the sadness emotion,
while men tend to express anger emotion. This
may be due to the fact that the different person-
ality has different emotion expressions, i.e., senti-
mentality of the female and the impulsion of the

Figure 2: Distribution of gender.

Figure 3: Distribution of location.

male.
Distribution of Location. Figure 3 is an ex-

ample of the distribution of emotions between lo-
cations. As discussed in the above section, we
use the province of authors as location attributes.
Here, the Y-axis represents the conditional prob-
ability of each emotion given location. From the
figure, we can find that the authors’ location can
may influence their emotions in many aspects. For
example, people tend to express the positive (i.e.,
happiness) emotion than the negative emotion in
Jiangsu. One of the most comfortable and devel-
oped regions in China. Due to air pollution and
populations, people tend to express the negative e-
motion than the positive emotion in Beijing. In
addition, people in Hong Kong always feel crowd-
ing and tend to express the sadness emotion. Fi-
nally, people always feel happy and comfortable
in Sichuan, well known as “country of paradise”
in China.

From the results of statistics, we can see that
the personal attributes, like gender, location, can
affect the emotion detection. Also, the experimen-



5506

Table 5: Examples predicted labels of LSTM and NPD

Post LSTM NPD
[E3]@$$Ê,Þ]ý�ý�Z,ô�è,°°ÉQ94�Á.
(@Momo Jun, this guy can even make Taro Balls from Jiufen of
Taiwan. It is so bash. This ice fruit dessert is so soft and chewy.)

Sadness Happiness

[E4]��î,¹�Ç,	�7^ú:.
(Next tv show is not to be missed, because my Mr. Mcdreamy
will be coming.)

None Happiness

[E5]�Ù«SbmôJ,�}�°,ÓÛÏ',v'�
¹¢ÖÜ6.
(I can�t compliment my body. Eating too much ice leads to my
dysmenorrhea. I must to drink more brown sugar ginger tea)

None Sadness

tal results of personal attribute influence shows the
same conclusion.

5.3.4 Case Study
We select three examples from the test set to eval-
uate the effectiveness of the proposed NPD mod-
el for better comparison with the LSTM model in
Table 5. In [E3], through “bashi(conformable)”
is a strong cue word of the happiness emotion,
it is treated as a general word without indicating
any location information from the LSTM mod-
el. At the same time, the location information
(Sichuan Province) determined by the location
discriminator and the attention mechanism makes
“bashi(conformable)” critical in the NPD model,
and these enable the NPD model determines the
correct happiness emotion. In [E4] and [E5], both
posts contain implicit gender information. For ex-
ample, “Mr. Mcdreamy” implies the female’s af-
fection for the handsome male, and “dysmenor-
rhea” is a female physiological disease. Without
the background of such gender information, it is
impossible to infer any potential emotional infor-
mation from these two words. This explains why
the LSTM model fails to detect any emotion from
the examples (None of emotion). However, our N-
PD model successfully determines the gender of
these two posts by the gender discriminator, and
improves the weight of these two words for emo-
tion detection by the attention mechanisms. In this
way, the emotions of the three examples are cor-
rectly detected through the proposed NPD model.

6 Conclusion

Most of previous studies consider each post in-
dividually in emotion detection, one of the most
important tasks in sentiment analysis. However,
since the posts in social media are generated by

users, it is natural that these posts can be con-
nected through authors’ personal background at-
tributes. In this paper, we propose a neural person-
al discrimination model with both adversarial dis-
criminators and attention mechanisms to connect
posts with similar personal attributes. Here, the
adversarial discriminators are used to determine
the personal attributes, and the attention mecha-
nisms are employed to aggregate attributes-aware
words. In this way the social correlations be-
tween different posts can be captured. The exper-
imental results show the usefulness of the person-
al attributes and the effectiveness of our proposed
neural personal discrimination model in modeling
such personal attributes with significant perfor-
mance improvement over the state-of-the-art base-
lines.

Acknowledgment

This work was support by National Natural Sci-
ence Foundation of China (No. 61702518,
No.61806137, No.61525205) and a project fund-
ed by the Priority Academic Program Develop-
ment of Jiangsu Higher Education Institutions..
This work was also supported by the joint research
project of Alibaba and Soochow University. Final-
ly, we would like to thank the anonymous review-
ers for their insightful comments and suggestions.

References
Muhammad Abdul-Mageed and Lyle H. Ungar. 2017.

Emonet: Fine-grained emotion detection with gated
recurrent neural networks. In ACL, pages 718–728.

Plaban Kumar Bhowmick, Anupam Basu, Pabitra Mi-
tra, and Abhishek Prasad. 2009. Multi-label text
classification approach for sentence level news emo-
tion analysis. In ICPRAI, pages 261–266.

https://doi.org/10.18653/v1/P17-1067
https://doi.org/10.18653/v1/P17-1067
https://doi.org/10.1007/978-3-642-11164-8_42
https://doi.org/10.1007/978-3-642-11164-8_42
https://doi.org/10.1007/978-3-642-11164-8_42


5507

Jianpeng Cheng, Li Dong, and Mirella Lapata. 2016.
Long short-term memory-networks for machine
reading. pages 551–561.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel P. Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research,
12:2493–2537.

Niko Colneriĉ and Janez Demsar. 2018. Emotion
recognition on twitter: Comparative study and train-
ing a unison model. IEEE Transactions on Affective
Computing, PP(99):1–14.

John C. Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. Journal of Machine
Learning Research, 12:2121–2159.

William Fedus, Ian J. Goodfellow, and Andrew M. Dai.
2018. Maskgan: Better text generation via filling in
the . In ICLR, pages 1–16.

Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan,
Pascal Germain, Hugo Larochelle, François Lavi-
olette, Mario Marchand, and Victor S. Lempitsky.
2017. Domain-adversarial training of neural net-
works. pages 189–209.

Yoav Goldberg. 2016. A primer on neural network
models for natural language processing. J. Artif. In-
tell. Res., 57:345–420.

Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,
Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron C. Courville, and Yoshua Bengio. 2014. Gen-
erative adversarial nets. In NIPS, pages 2672–2680.

Umang Gupta, Ankush Chatterjee, Radhakrishnan S-
rikanth, and Puneet Agrawal. 2017. A sentiment-
and-semantics-based approach for emotion de-
tection in textual conversations. CoRR, ab-
s/1707.06996. Version 4.

Dirk Hovy. 2015. Demographic factors improve clas-
sification performance. In ACL, pages 752–762.

Paul F Lazarsfeld, Robert K Merton, et al. 1954.
Friendship as a social process: A substantive and
methodological analysis. Freedom and control in
modern society, 18(1):18–66.

Sophia Yat Mei Lee, Ying Chen, Chu-Ren Huang, and
Shoushan Li. 2013. Detecting emotion causes with
a linguistic rule-based approach. Computational In-
telligence, 29(3):390–416.

Junjie Li, Haitong Yang, and Chengqing Zong. 2016.
Sentiment classification of social media text con-
sidering user attributes. In NLPCC/ICCPOL, pages
583–594.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.
Corrado, and Jeffrey Dean. 2013. Distributed repre-
sentations of words and phrases and their composi-
tionality. In NIPS, pages 3111–3119.

Xiaojun Quan, Qifan Wang, Ying Zhang, Luo Si, and
Wenyin Liu. 2015. Latent discriminative models for
social emotion detection with emotional dependen-
cy. ACM Trans. Inf. Syst., 34(1):2:1–2:19.

Kirk Roberts, Michael A. Roach, Joseph Johnson,
Josh Guthrie, and Sanda M. Harabagiu. 2012. Em-
patweet: Annotating and detecting emotions on twit-
ter. In LREC, pages 3806–3813.

Carlo Strapparava and Rada Mihalcea. 2008. Learning
to identify emotions in text. In SAC, pages 1556–
1560.

Mike Thelwall. 2010. Emotion homophily in social
network site messages. First Monday, 15(4):1–8.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In NIPS, pages 6000–6010.

Soroush Vosoughi, Helen Zhou, and Deb Roy. 2016.
Enhanced twitter sentiment classification using con-
textual information. CoRR, abs/1605.05195. Ver-
sion 1.

Ke Wang and Xiaojun Wan. 2018. Sentigan: Gener-
ating sentimental texts via mixture adversarial net-
works. In IJCAI, pages 4446–4452.

Xuren Wang and Qiuhui Zheng. 2013. Text emotion
classification research based on improved latent se-
mantic analysis algorithm. In ICCSEE, pages 210–
213.

Zhongqing Wang, Sophia Yat Mei Lee, Shoushan Li,
and Guodong Zhou. 2017. Emotion analysis in
code-switching text with joint factor graph model.
IEEE/ACM Trans. Audio, Speech & Language Pro-
cessing, 25(3):469–480.

Zhongqing Wang, Shoushan Li, Hanxiao Shi, and
Guodong Zhou. 2014. Skill inference with personal
and skill connections. In COLING, pages 520–529.

Zhongqing Wang, Yue Zhang, Sophia Yat Mei Lee,
Shoushan Li, and Guodong Zhou. 2016. A bilingual
attention network for code-switched emotion predic-
tion. In COLING, pages 1624–1634.

Changhua Yang, Kevin Hsin-Yih Lin, and Hsin-Hsi
Chen. 2007. Emotion classification using web blog
corpora. In WI-IAT, pages 275–278.

Lu Zhang, Liangqing Wu, Shoushan Li, Zhongqing
Wang, and Guodong Zhou. 2018. Cross-lingual
emotion classification with auxiliary and attention
neural networks. In NLPCC, pages 429–441.

http://aclweb.org/anthology/D/D16/D16-1053.pdf
http://aclweb.org/anthology/D/D16/D16-1053.pdf
http://dl.acm.org/citation.cfm?id=2078186
http://dl.acm.org/citation.cfm?id=2078186
http://dl.acm.org/citation.cfm?id=2021068
http://dl.acm.org/citation.cfm?id=2021068
https://doi.org/10.1007/978-3-319-58347-1_10
https://doi.org/10.1007/978-3-319-58347-1_10
https://doi.org/10.1613/jair.4992
https://doi.org/10.1613/jair.4992
http://papers.nips.cc/paper/5423-generative-adversarial-nets
http://papers.nips.cc/paper/5423-generative-adversarial-nets
http://arxiv.org/abs/1707.06996
http://arxiv.org/abs/1707.06996
http://arxiv.org/abs/1707.06996
http://aclweb.org/anthology/P/P15/P15-1073.pdf
http://aclweb.org/anthology/P/P15/P15-1073.pdf
https://doi.org/10.1111/j.1467-8640.2012.00459.x
https://doi.org/10.1111/j.1467-8640.2012.00459.x
https://doi.org/10.1007/978-3-319-50496-4_52
https://doi.org/10.1007/978-3-319-50496-4_52
http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality
http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality
http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality
https://doi.org/10.1145/2749459
https://doi.org/10.1145/2749459
https://doi.org/10.1145/2749459
http://www.lrec-conf.org/proceedings/lrec2012/summaries/201.html
http://www.lrec-conf.org/proceedings/lrec2012/summaries/201.html
http://www.lrec-conf.org/proceedings/lrec2012/summaries/201.html
https://doi.org/10.1145/1363686.1364052
https://doi.org/10.1145/1363686.1364052
http://firstmonday.org/htbin/cgiwrap/bin/ojs/index.php/fm/article/view/2897
http://firstmonday.org/htbin/cgiwrap/bin/ojs/index.php/fm/article/view/2897
http://papers.nips.cc/paper/7181-attention-is-all-you-need
http://papers.nips.cc/paper/7181-attention-is-all-you-need
http://arxiv.org/abs/1605.05195
http://arxiv.org/abs/1605.05195
https://doi.org/10.24963/ijcai.2018/618
https://doi.org/10.24963/ijcai.2018/618
https://doi.org/10.24963/ijcai.2018/618
https://doi.org/10.1109/TASLP.2016.2637280
https://doi.org/10.1109/TASLP.2016.2637280
http://aclweb.org/anthology/C/C14/C14-1050.pdf
http://aclweb.org/anthology/C/C14/C14-1050.pdf
http://aclweb.org/anthology/C/C16/C16-1153.pdf
http://aclweb.org/anthology/C/C16/C16-1153.pdf
http://aclweb.org/anthology/C/C16/C16-1153.pdf
https://doi.org/10.1109/WI.2007.51
https://doi.org/10.1109/WI.2007.51
https://doi.org/10.1007/978-3-319-99495-6_36
https://doi.org/10.1007/978-3-319-99495-6_36
https://doi.org/10.1007/978-3-319-99495-6_36

