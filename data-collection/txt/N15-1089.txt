



















































Early Gains Matter: A Case for Preferring Generative over Discriminative Crowdsourcing Models


Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 882–891,
Denver, Colorado, May 31 – June 5, 2015. c©2015 Association for Computational Linguistics

Early Gains Matter: A Case for Preferring Generative over Discriminative
Crowdsourcing Models

Paul Felt, Eric Ringger, Kevin Seppi, Kevin Black, Robbie Haertel
Brigham Young University

Provo, UT 84602, USA
{paul felt,kevin black}@byu.edu, {ringger,kseppi}@cs.byu.edu, robbie.haertel@gmail.com

Abstract

In modern practice, labeling a dataset of-
ten involves aggregating annotator judgments
obtained from crowdsourcing. State-of-the-
art aggregation is performed via inference on
probabilistic models, some of which are data-
aware, meaning that they leverage features of
the data (e.g., words in a document) in addi-
tion to annotator judgments. Previous work
largely prefers discriminatively trained condi-
tional models. This paper demonstrates that
a data-aware crowdsourcing model incorpo-
rating a generative multinomial data model
enjoys a strong competitive advantage over
its discriminative log-linear counterpart in the
typical crowdsourcing setting. That is, the
generative approach is better except when the
annotators are highly accurate in which case
simple majority vote is often sufficient. Ad-
ditionally, we present a novel mean-field vari-
ational inference algorithm for the generative
model that significantly improves on the previ-
ously reported state-of-the-art for that model.
We validate our conclusions on six text clas-
sification datasets with both human-generated
and synthetic annotations.

1 Introduction

The success of supervised machine learning has cre-
ated an urgent need for manually-labeled training
datasets. Crowdsourcing allows human label judg-
ments to be obtained rapidly and at relatively low
cost. Micro-task markets such as Amazon’s Me-
chanical Turk and CrowdFlower have popularized
crowdsourcing by reducing the overhead required to

distribute a job to a community of annotators (the
“crowd”). However, crowdsourced judgments often
suffer from high error rates. A common solution to
this problem is to obtain multiple redundant human
judgments, or annotations,1 relying on the obser-
vation that, in aggregate, the ability of non-experts
often rivals or exceeds that of experts by averag-
ing over individual error patterns (Surowiecki, 2005;
Snow et al., 2008; Jurgens, 2013).

For the purposes of this paper a crowdsourcing
model is a model that infers, at a minimum, class
labels y based on the evidence of one or more im-
perfect annotations a. A common baseline method
aggregates annotations by majority vote but by so
doing ignores important information. For exam-
ple, some annotators are more reliable than others,
and their judgments ought to be weighted accord-
ingly. State-of-the-art crowdsourcing methods for-
mulate probabilistic models that account for such
side information and then apply standard inference
techniques to the task of inferring ground truth la-
bels from imperfect annotations.

Data-aware crowdsourcing models additionally
account for the features x comprising each data in-
stance (e.g., words in a document). The data can be
modeled generatively by proposing a joint distribu-
tion p(y,x,a). However, because of the challenge
of accurately modeling complex data x, most previ-
ous work uses a discriminatively trained conditional
model p(y,a|x), hereafter referred to as a discrim-
inative model. As Ng and Jordan (2001) explain,
maximizing conditional log likelihood is a compu-

1 We use the term annotation to identify human judgments
and distinguish them from gold standard class labels.

882



tationally convenient approximation to minimizing
a discriminative 0-1 loss objective, giving rise to the
common practice of referring to conditional models
as discriminative.

Contributions. This paper challenges the popu-
lar preference for discriminative data models in the
crowdsourcing literature by demonstrating that in
typical crowdsourcing scenarios a generative model
enjoys a strong advantage over its discriminative
counterpart. We conduct, on both real and syn-
thetic annotations, the first empirical comparison
of structurally comparable generative and discrim-
inative crowdsourcing models. The comparison is
made fair by developing similar mean-field varia-
tional inference algorithms for both models. The
generative model is considerably improved by our
variational algorithm compared with the previously
reported state-of-the-art for that model.

2 Previous Work

Dawid and Skene (1979) laid the groundwork for
modern annotation aggregation by proposing the
item-response model: a probabilistic crowdsourcing
model p(y,a|γ) over document labels y and annota-
tions a parameterized by confusion matrices γ for
each annotator. A growing body of work extends
this model to account for such things as correlation
among annotators, annotator trustworthiness, item
difficulty, and so forth (Bragg et al., 2013; Hovy et
al., 2013; Passonneau and Carpenter, 2013; Paster-
nack and Roth, 2010; Smyth et al., 1995; Welinder et
al., 2010; Whitehill et al., 2009; Zhou et al., 2012).

Of the crowdsourcing models that are data-aware,
most model the data discriminatively (Carroll et al.,
2007; Liu et al., 2012; Raykar et al., 2010; Yan et
al., 2014). A smaller line of work models the data
generatively (Lam and Stork, 2005; Simpson and
Roberts, In Press). We are aware of no papers that
compare a generative crowdsourcing model with a
similar discriminative model. In the larger context of
supervised machine learning, Ng and Jordan (2001)
observe that generative models parameters tend to
converge with fewer training examples than their
discriminatively trained counterparts, but to lower
asymptotic performance levels. This paper explores
those insights in the context of crowdsourcing mod-
els.

3 Models

At a minimum, a probabilistic crowdsourcing model
predicts ground truth labels y from imperfect anno-
tations a (i.e., argmaxy p(y|a)). In this section we re-
view the specifics of two previously-proposed data-
aware crowdsourcing models. These models are best
understood as extensions to a Bayesian formulation
of the item-response model that we will refer to as
ITEMRESP. ITEMRESP, illustrated in Figure 1a, is
defined by the joint distribution

p(θ ,γ,y,a) (1)

= p(θ)
[
∏
j∈J

∏
k∈K

p(γ jk)
]
∏
i∈N

p(yi|θ)∏
j∈J

p(ai j|γ j,yi)

where J is the set of annotators, K is the set of class
labels, N is the set of data instances in the corpus, θ
is a stochastic vector in which θk is the probability
of label class k, γ j is a matrix of stochastic vector
rows in which γ jkk′ is the probability that annotator
j annotates with k′ items whose true label is k, yi is
the class label associated with the ith instance in the
corpus, and ai jk is the number of times that instance
i was annotated by annotator j with label k. The
fact that ai j is a count vector allows for the general
case where annotators express their uncertainty over
multiple class values. Also, θ ∼ Dirichlet(b(θ)),
γ jk ∼ Dirichlet(b(γ)jk ), yi|θ ∼ Categorical(θ), and
ai j|yi,γ j ∼ Multinomial(γ jyi ,Mi) where Mi is the
number of times annotator j annotated instance i.
We need not define a distribution over Mi because in
practice Mi = |ai j|1 is fixed and known during pos-
terior inference. A special case of this model formu-
lates ai j as a categorical distribution assuming that
annotators will provide at most one annotation per
item. All hyperparameters are designated b and are
disambiguated with a superscript (e.g., the hyperpa-
rameters for p(θ) are b(θ)). When ITEMRESP pa-
rameters are set with uniform θ values and diagonal
confusion matrices γ , majority vote is obtained.

Inference in a crowdsourcing model involves a
corpus with an annotated portion NA = {i : |ai|1 > 0}
and also potentially an unannotated portion NU =
{i : |ai|1 = 0}. ITEMRESP can be written as
p(γ,y,a) = p(γ,yA,yU ,a) where yA = {yi : i ∈ NA}
and yU = {yi : i ∈ NU}. However, because ITEM-
RESP has no model of the data x, it receives no ben-
efit from unannotated data NU .

883



θ

γ jk

b(θ)

b(γ)

yi

ai j

i ∈ N

k ∈ K
j ∈ J

j ∈ J

(a) ITEMRESP

γ jk

φk
b(γ)

Σ

yi

xi ai j

i ∈ N

k ∈ K
j ∈ J

j ∈ J

k ∈ K

(b) LOGRESP

θ

γ jkφk

b(θ)

b(φ) b
(γ)

yi

xi ai j

i ∈ N

k ∈ K
j ∈ J

j ∈ J

k ∈ K

(c) MOMRESP

Figure 1: Directed graphical model depictions of the models discussed in this paper. Round nodes are
variables with distributions. Rectangular nodes are hyperparameters (without distributions). Shaded nodes
have known values (although some a values may be unobserved).

3.1 Log-linear data model (LOGRESP)

One way to make ITEMRESP data-aware is by
adding a discriminative log-linear data component
(Raykar et al., 2010; Liu et al., 2012). For short, we
refer to this model as LOGRESP, illustrated in Fig-
ure 1b. Concretely,

p(γ,φ ,y,a|x) =
[
∏
j∈J

∏
k∈K

p(γ jk)
]

(2)

∏
k∈K

p(φk)∏
i∈N

p(yi|xi,φ)∏
j∈J

p(ai j|γ j,yi)

where xi f is the value of feature f in data instance
i (e.g., a word count in a text classification prob-
lem), φk f is the probability of feature f occurring
in an instance of class k, φk ∼ Normal(0,Σ), and
yi|xi,φ ∼ LogLinear(xi,φ). That is, p(yi|xi,φ) =
exp[φ Tyi xi]/∑k exp[φ

T
k xi].

In the special case that each γ j is the identity ma-
trix (each annotator is perfectly accurate), LOGRESP
reduces to a multinomial logistic regression model.
Because it is a conditional model, LOGRESP lacks
any built-in capacity for semi-supervised learning.

3.2 Multinomial data model (MOMRESP)

An alternative way to make ITEMRESP data-aware
is by adding a generative multinomial data compo-
nent (Lam and Stork, 2005; Felt et al., 2014). We re-

fer to the model as MOMRESP, shown in Figure 1c.

p(θ ,γ,φ ,y,x,a) = p(θ)
[
∏
j∈J

∏
k∈K

p(γ jk)
]

(3)

∏
k∈K

p(φk)∏
i∈N

p(yi|θ)p(xi|yi,φ)∏
j∈J

p(ai j|γ j,yi)

where φk f is the probability of feature f occurring
in an instance of class k, φk ∼ Dirichlet(b(φ)k ), xi ∼
Multinomial(φyi ,Ti), and Ti is a number-of-trials pa-
rameter (e.g., for text classification Ti is the number
of words in document i). Ti = |xi|1 is observed dur-
ing posterior inference p(θ ,γ,φ ,y|x,a).

Because MOMRESP is fully generative over
the data features x, it naturally performs semi-
supervised learning as data from unannotated in-
stances NU inform inferred class labels yA of an-
notated instances via φ . This can be seen by ob-
serving that p(x) terms prevent terms involving
yU from summing out of the marginal distribu-
tion p(θ ,γ,φ ,yA,x,a) = ∑yU p(θ ,γ,φ ,yA,yU ,x,a) =
p(θ ,γ,φ ,yA,xA,a)∑yU p(yU |θ)p(xU |yU).

When N = NU (the unsupervised setting) the pos-
terior distribution p(θ ,γ,φ ,yU |x,a) = p(θ ,φ ,yU |x)
is a mixture of multinomials clustering model.
Otherwise, the model resembles a semi-supervised
naı̈ve Bayes classifier (Nigam et al., 2006). How-
ever, naı̈ve Bayes is supervised by trustworthy labels
whereas MOMRESP is supervised by imperfect an-
notations mediated by inferred annotator error char-
acteristic γ . In the special case that γ is the identity
matrix (each annotator is perfectly accurate), MOM-
RESP reduces to a possibly semi-supervised naı̈ve

884



Bayes classifier where each annotation is a fully
trusted label.

3.3 A Generative-Discriminative Pair
MOMRESP and LOGRESP are a generative-
discriminative pair, meaning that they belong to the
same parametric model family but with parameters
fit to optimize joint likelihood and conditional likeli-
hood, respectively. This relationship is seen via the
equivalence of the conditional probability of LOG-
RESP pL(y,a|x) and the same expression accord-
ing to MOMRESP pM(y,a|x). For simplicity in this
derivation we omit priors and consider φ , θ , and γ
to be known values. Then

pM(y,a|x) = p(y)p(x|y)p(a|y)∑y′ ∑a′ p(y′)p(x|y′)p(a′|y′)
(4)

=
p(y)p(x|y)

∑y′ p(y′)p(x|y′)
· p(a|y) (5)

=
exp[ew

T
y x+z]

∑k exp[ew
T
k x+z]

· p(a|y) (6)

= pL(y,a|x) (7)
Equation 4 follows from Bayes Rule and conditional
independence in the model. In Equation 5 p(a′|y)
sums to 1. The first term of Equation 6 is the pos-
terior p(y|x) of a naı̈ve Bayes classifier, known to
have the same form as a logistic regression classi-
fier where parameters w and z are constructed from
φ and θ .2

4 Mean-field Variational Inference (MF)

In this section we present novel mean-field (MF)
variational algorithms for LOGRESP and MOM-
RESP. Note that Liu et al. (2012) present (in an ap-
pendix) variational inference for LOGRESP based on
belief propagation (BP). They do not test their algo-
rithm for LOGRESP; however, their comparison of
MF and BP variational inference for the ITEMRESP
model indicates that the two flavors of variational
inference perform very similarly. Our MF algorithm
for LOGRESP has not been designed with the idea
of outperforming its BP analogue, but rather with
the goal of ensuring that the generative and discrim-
inative model use the same inference algorithm. We

2http://cs.cmu.edu/˜tom/mlbook/NBayesLogReg.pdf

gives a proof of this property in the continuous case and hints
about the discrete case proof.

expect that we would achieve the same results if our
comparison used variational BP algorithms for both
MOMRESP and LOGRESP, although such an addi-
tional comparison is beyond the scope of this work.

Broadly speaking, variational approaches to pos-
terior inference transform inference into an opti-
mization problem by searching within some family
of tractable approximate distributions Q for the dis-
tribution q ∈ Q that minimizes distributional diver-
gence from an intractable target posterior p∗. In par-
ticular, under the mean-field assumption we confine
our search to distributions Q that are fully factorized.

4.1 LOGRESP Inference
We approximate LOGRESP’s posterior
p∗(γ,φ ,y|x,a) using the fully factorized approxima-
tion q(γ,φ ,y) =

[
∏ j ∏k q(γ jk)

]
∏k q(φk)∏i q(yi).

Approximate marginal posteriors q are disam-
biguated by their arguments.

Algorithm. Initialize each q(yi) to the em-
pirical distribution observed in the annotations ai.
The Kullback-Leibler divergence KL(q||p∗) is min-
imized by iteratively updating each variational dis-
tribution in the model as follows:

q(γ jk) ∝ ∏
k′∈K

γ
b(γ)jkk′+∑i∈N ai jk′q(yi=k)−1
jkk′ = Dirichlet(α

(γ)
jk )

q(φk) ∝ exp
[
φ Tk Σ

−1φk + ∑
i∈N

q(yi = k)φ Tk xi
]

q(yi) ∝ ∏
k∈K

exp
[
∑
j∈J

∑
k′∈K

ai jk′Eq(γ jk)[logγ jkk′ ]+

∑
f∈F

xi f Eq(φk)[φk f ]
]
1(yi=k)

∝ ∏
k∈K

α(y)1(yi=k)ik = Categorical(α
(y)
i )

Approximate distributions are updated by calcu-
lating variational parameters α(·), disambiguated by
a superscript. Because q(γ jk) is a Dirichlet distri-
bution the term Eq(γ jk)[logγ jkk′ ] appearing in q(yi)
is computed analytically as ψ(α(γ)jkk′)−ψ(∑k′ α(γ)jkk′)
where ψ is the digamma function.

The distribution q(φk) is a logistic normal distri-
bution. This means that the expectations Eq(φk)[φk f ]
that appear in q(yi) cannot be computed analyti-
cally. Following Liu et al. (2012), we approxi-
mate the distribution q(φk) with the point estimate

885



φ̂k = argmaxφk q(φk) which can be calculated using
existing numerical optimization methods for log-
linear models. Such maximization can be under-
stood as embedding the variational algorithm inside
of an outer EM loop such as might be used to tune
hyperparameters in an empirical Bayesian approach
(where φ are treated as hyperparameters).

4.2 MOMRESP Inference
MOMRESP’s posterior p∗(y,θ ,γ,φ |x,a) is ap-
proximated with the fully factorized distribution
q(y,θ ,γ,φ) = q(θ)

[
∏ j ∏k q(γ jk)

]
∏k q(φk)∏i q(yi).

Algorithm. Initialize each q(yi) to the em-
pirical distribution observed in the annotations ai.
The Kullback-Leibler divergence KL(q||p∗) is min-
imized by iteratively updating each variational dis-
tribution in the model as follows:

q(θ) ∝ ∏
k∈K

θ b
(θ)
k +∑i∈N q(yi=k)−1

k = Dirichlet(α
(θ))

q(γ jk) ∝ ∏
k′∈K

γ
b(γ)jkk′+∑i∈N ai jk′q(yi=k)−1
jkk′ = Dirichlet(α

(γ)
jk )

q(φk) ∝ ∏
f∈F

φ
b(φ)k f +∑i∈N xi f q(yi=k)−1
k f = Dirichlet(α

(φ)
k )

q(yi) ∝ ∏
k∈K

exp
[
∑
j∈J

∑
k′∈K

ai jk′Eq(γ jk)[logγ jkk′ ]+

Eq(θk)[logθk]+ ∑
f∈F

xi f Eq(φk)[logφk f ]
]
1(yi=k)

∝ ∏
k∈K

α(y)1(yi=k)ik = Categorical(α
(y)
i )

Approximate distributions are updated by calcu-
lating the values of variational parameters α(·), dis-
ambiguated by a superscript. The expectations of
log terms in the q(yi) update are all with respect to
Dirichlet distributions and so can be computed ana-
lytically as explained previously.

4.3 Model priors and implementation details
Computing a lower bound on the log likelihood
shows that in practice the variational algorithms pre-
sented above converge after only a dozen or so
updates. We compute argmaxφk q(φk) for LOG-
RESP using the L-BFGS algorithm as implemented
in MALLET (McCallum, 2002). We choose unin-
formed priors b(θ)k = 1 for MOMRESP and identity

matrix Σ = 1 for LOGRESP. We set b(φ)k f = 0.1 for
MOMRESP to encourage sparsity in per-class word
distributions. Liu et al. (2012) argue that a uniform
prior over the entries of each confusion matrix γ j can
lead to degenerate performance. Accordingly, we
set the diagonal entries of each b(γ)j to a higher value

b(γ)jkk =
1+δ
K+δ and off-diagonal entries to a lower value

b(γ)jkk′ =
1

K+δ with δ = 2.
Both MOMRESP and LOGRESP are given full ac-

cess to all instances in the dataset, annotated and
unannotated. However, as explained in Section 3.1,
LOGRESP is conditioned on the data and thus is
structurally unable to make use of unannotated data.
We experimented briefly with self-training for LOG-
RESP but it had little effect. With additional effort
one could likely settle on a heuristic scheme that al-
lowed LOGRESP to benefit from unannotated data.
However, since such an extension is external to the
model itself, it is beyond the scope of this work.

5 Experiments with Simulated Annotators

Models which learn from error-prone annotations
can be challenging to evaluate in a systematic way.
Simulated annotations allow us to systematically
control annotator behavior and measure the perfor-
mance of our models in each configuration.

5.1 Simulating Annotators
We simulate an annotator by corrupting ground truth
labels according to that annotator’s accuracy param-
eters. Simulated annotators are drawn from the an-
notator quality pools listed in Table 1. Each row
is a named pool and contains five annotators A1–
A5, each with a corresponding accuracy parameter
(the number five is chosen arbitrarily). In the pools
HIGH, MED, and LOW, annotator errors are dis-
tributed uniformly across the incorrect classes. Be-
cause there are no patterns among errors, these set-
tings approximate situations in which annotators are
ultimately in agreement about the task they are do-
ing, although some are better at it than others. The
HIGH pool represents a corpus annotation project
with high quality annotators. In the MED and LOW
pools annotators are progressively less reliable.

The CONFLICT annotator pool in Table 1 is spe-
cial in that annotator errors are made systemati-
cally rather than uniformly. Systematic errors are

886



WEBKB

0.4

0.6

0.8

1.0

M
E

D

 0  1  2  3

Number of annotated instances x 1,000

A
c
c
u

ra
c
y algorithm

MomResp+MF

MomResp+Gibbs

MomResp Inference
R52

0.0

0.2

0.4

0.6

L
O

W
0 2 4 6 8

Number of annotated instances x 1,000

A
c
c
u

ra
c
y algorithm

LogResp+MF

LogResp+EM

LogResp Inference

Number of annotated instances ×1000.
Figure 2: Mean field (MF) variational inference outperforms previous inference methods for both models.
Left: MOMRESP with MF (MOMRESP+MF) versus with Gibbs sampling (MOMRESP+Gibbs) on the We-
bKB dataset using annotators from the MED pool. Right: LOGRESP with MF (LOGRESP+MF) versus with
EM (LOGRESP+EM) on the Reuters52 dataset using annotators from the LOW pool.

produced at simulation time by constructing a per-
annotator confusion matrix (similar to γ j) whose di-
agonal is set to the desired accuracy setting, and
whose off-diagonal row entries are sampled from
a symmetric Dirichlet distribution with parameter
0.1 to encourage sparsity and then scaled so that
each row properly sums to 1. These draws from
a sparse Dirichlet yield consistent error patterns.
The CONFLICT pool approximates an annotation
project where annotators understand the annotation
guidelines differently from one another. For the sake
of example, annotator A5 in the CONFLICT setting
will annotate documents with the true class B as B
exactly 10% of the time but might annotate B as C
85% of the time. On the other hand, annotator A4
might annotate B as D most of the time. We choose
low agreement rates for CONFLICT to highlight a
case that violates majority vote’s assumption that an-
notators are basically in agreement.

A1 A2 A3 A4 A5
HIGH 90 85 80 75 70
MED 70 65 60 55 50
LOW 50 40 30 20 10
CONFLICT 50† 40† 30† 20† 10†

Table 1: For each simulated annotator quality pool
(HIGH, MED, LOW, CONFLICT), annotators A1-
A5 are assigned an accuracy. † indicates that errors
are systematically in conflict as described in the text.

5.2 Datasets and Features
We simulate the annotator pools from Table
1 on each of six text classification datasets.
The datasets 20 Newsgroups, WebKB, Cade12,
Reuters8, and Reuters52 are described by Cardoso-
Cachopo (2007). The LDC-labeled Enron emails
dataset is described by Berry et al. (2001). Each
dataset is preprocessed via Porter stemming and by
removal of the stopwords from MALLET’s stop-
word list. Features occurring fewer than 5 times in
the corpus are discarded. Features are fractionally
scaled so that |xi|1 is equal to the average document
length since document scaling has been shown to be
beneficial for multinomial document models (Nigam
et al., 2006).

Each dataset is annotated according to the follow-
ing process: an instance is selected at random (with-
out replacement) and annotated by three annotators
selected at random (without replacement). Because
annotation simulation is a stochastic process, each
simulation is repeated five times.

5.3 Validating Mean-field Variational Inference
Figure 2 compares mean-field variational inference
(MF) with alternative inference algorithms from pre-
vious work. For variety, the left and right plots are
calculated over arbitrarily chosen datasets and an-
notator pools, but these trends are representative of
other settings. MOMRESP using MF is compared
with MOMRESP using Gibbs sampling estimating
p(y|x,a) from several hundred samples (an improve-
ment to the method used by Felt et al. (2014)).

887



CONFLICT LOW MED HIGH

0.4

0.6

0.8

1.0

 0  5 10 15  0  5 10 15  0  5 10 15  0  5 10 15

Number of annotated instances x 1,000

A
c
c
u
ra

c
y

algorithm

MomResp

LogResp

Majority

20 Newsgroups

CONFLICT LOW MED HIGH

0.4

0.6

0.8

1.0

 0  5 10 15  0  5 10 15  0  5 10 15  0  5 10 15

Number of annotated instances x 1,000

T
e
s
t 
A

c
c
u
ra

c
y

algorithm

MomResp

LogResp

20 Newsgroups

Figure 3: Top row: Inferred label accuracy on three-deep annotations. A majority vote baseline is shown for
reference. Bottom row: Generalization accuracy on a test set. Majority vote is not shown since it does not
generate test set predictions. Each column uses the indicated simulated annotator pool.

MOMRESP benefits significantly from MF. We sus-
pect that this disparity could be reduced via hyper-
parameter optimization as indicated by Asuncion et
al. (2009). However, that investigation is beyond the
scope of the current work. LOGRESP using MF is
compared with LOGRESP using expectation maxi-
mization (EM) as in (Raykar et al., 2010). LOG-
RESP with MF displays minor improvements over
LOGRESP with EM. This is consistent with the mod-
est gains that Liu et al. (2012) reported when com-
paring variational and EM inference for the ITEM-
RESP model.

5.4 Discriminative (LOGRESP) versus
Generative (MOMRESP)

We run MOMRESP and LOGRESP with MF infer-
ence on the cross product of datasets and annotator
pools. Inferred label accuracy on items that have
been annotated is the primary task of crowdsourc-
ing; we track this measure accordingly. However,
the ability of these models to generalize on unanno-
tated data is also of interest and allows better com-
parison with traditional non-crowdsourcing models.
Figure 3 plots learning curves for each annotator
pool on the 20 Newsgroups dataset; results on other
datasets are summarized in Table 2. The first row of

Figure 3 plots the accuracy of labels inferred from
annotations. The second row of Figure 3 plots gen-
eralization accuracy using the inferred model param-
eters φ (and θ in the case of MOMRESP) on held-out
test sets with no annotations. The generalization ac-
curacy curves of MOMRESP and LOGRESP may be
compared with those of naı̈ve Bayes and logistic re-
gression, respectively. Recall that in the special case
where annotations are both flawless and trusted (via
diagonal confusion matrices γ) then MOMRESP and
LOGRESP simplify to semi-supervised naı̈ve Bayes
and logistic regression classifiers, respectively.

Notice that MOMRESP climbs more steeply than
LOGRESP in all cases. This observation is in keep-
ing with previous work in supervised learning. Ng
and Jordan (2001) argue that generative and discrim-
inative models have complementary strengths: gen-
erative models tend to have steeper learning curves
and converge in terms of parameter values after
only logn training examples, whereas discriminative
models tend to achieve higher asymptotic levels but
converge more slowly after n training examples. The
second row of Figure 3 shows that even after training
on three-deep annotations over the entire 20 news-
groups dataset, LOGRESP’s data model does not ap-
proach its asymptotic level of performance. The

888



early steep slope of the generative model is more
desirable in this setting than the eventually superior
performance of the discriminative model given large
numbers of annotations. Figure 4 additionally plots
MOMRESPA, a variant of MOMRESP deprived of
all unannotated documents, showing that the early
generative advantage is not attributable entirely to
semi-supervision.

The generative model is more robust to annotation
noise than the discriminative model, seen by com-
paring the LOW, MED, and HIGH columns in Fig-
ure 3. This robustness is significant because crowd-
sourcing tends to yield noisy annotations, making
the LOW and MED annotator pools of greatest prac-
tical interest. This assertion is borne out by an ex-
periment with CrowdFlower, reported in Section 6.

To validate that LOGRESP does, indeed, asymp-
totically surpass MOMRESP we ran inference
on datasets with increasing annotation depths.
Crossover does not occur until 20 Newsgroups is an-
notated nearly 12-deep for LOW, 5-deep for MED,
and 3.5-deep (on average) for HIGH. Additionally,
for each combination of dataset and annotator pool
except those involving CONFLICT, by the time
LOGRESP surpasses MOMRESP, the majority vote
baseline is extremely competitive with LOGRESP.
The CONFLICT setting is the exception to this rule:
CONFLICT annotators are particularly challenging
for majority vote since they violate the implicit as-
sumption that annotators are basically aligned with
the truth. The CONFLICT setting is of practical
interest only when annotators have dramatic deep-
seated differences of opinion about what various la-
bels should mean. For most crowdsourcing projects
this issue may be avoided with sufficient up-front
orientation of the annotators. For reference, in Fig-
ure 4 we show that a less extreme variant of CON-
FLICT behaves more similarly to LOW.

Table 2 reports the percent of the dataset that
must be annotated three-deep before LOGRESP’s in-
ferred label accuracy surpasses that of MOMRESP.
Crossover tends to happen later when annotation
quality is low and earlier when annotator quality is
high. Cases reported as NA were too close to call;
that is, the dominating algorithm changed depend-
ing on the random run.

Unsurprisingly, MOMRESP is not well suited to
all classification datasets. The 0% entries in Table

CONFLICT_MILD

0.4

0.6

0.8

1.0

 0  5 10 15

Number of annotated instances x 1,000

A
c
c
u
ra

c
y

algorithm

MomResp

MomRespA

LogResp

Majority

20 Newsgroups

Figure 4: Inferred label accuracy for a variant of
the CONFLICT annotator pool in which the off-
diagonals of each annotator confusion matrix are
drawn from a Dirichlet parameterized by 1 rather
than 0.1. Also adds the algorithm MOMRESPA to
show the effect of removing MOMRESP’s access to
unannotated documents.

2 mean that LOGRESP dominates the learning curve
for that annotator pool and dataset. These cases are
likely the result of the MOMRESP model making
the same strict inter-feature independence assump-
tions as naı̈ve Bayes, rendering it tractable and ef-
fective for many classification tasks but ill-suited for
datasets where features are highly correlated or for
tasks in which class identity is not informed by doc-
ument vocabulary. The CADE12 dataset, in particu-
lar, is known to be challenging. A supervised naı̈ve
Bayes classifier achieves only 57% accuracy on this
dataset (Cardoso-Cachopo, 2007). We would expect
MOMRESP to perform similarly poorly on sentiment
classification data. Although we assert that gener-
ative models are inherently better suited to crowd-
sourcing than discriminative models, a sufficiently
strong mismatch between model assumptions and
data can negate this advantage.

6 Experiments with Human Annotators

In the previous section we used simulations to con-
trol annotator error. In this section we relax that con-
trol. To assess the effect of real-world annotation er-
ror on MOMRESP and LOGRESP, we selected 1000
instances at random from 20 Newsgroups and paid
annotators on CrowdFlower to annotate them with
the 20 Newsgroups categories, presented as human-
readable names (e.g., “Atheism” for alt.atheism).
Annotators were allowed to express uncertainty by

889



CONFLICT LOW MED HIGH
20 News 21% X X X
WebKB NA X X 0%
Reuters8 NA X X X
Reuters52 X X X X
CADE12 0% X 0% 0%
Enron X X X 18%

Table 2: The percentage of the dataset that must be
annotated (three-deep) before the generative model
MOMRESP is surpassed by LOGRESP. Xindicates
that MOMRESP dominates the entire learning curve;
0% indicates that LOGRESP dominates. NA indi-
cates high variance cases that were too close to call.

selecting up to three unique categories per docu-
ment. During the course of a single day we gath-
ered 7,265 annotations, with each document having
a minimum of 3 and a mean of 7.3 annotations.3 Fig-
ure 5 shows learning curves for the CrowdFlower
annotations. The trends observed previously are un-
changed. MOMRESP enjoys a significant advantage
when relatively few annotations are available. Pre-
sumably LOGRESP would still dominate if we were
able to explore later portions of the curve or curves
with greater annotation depth.

0.4

0.5

0.6

0.7

 0  2  4  6

Number of human annotations x 1,000

A
c
c
u
ra

c
y

algorithm

MomResp

LogResp

Majority

Crowdflower Annotations

Figure 5: Inferred label accuracy on annotations
gathered from CrowdFlower over a subset of 1000
instances of the 20 Newsgroups dataset. At the last
plotted point there are 7,265/1,000 ≈ 7.3 annota-
tions per instance.

3 This dataset and the scripts that produced it are available
via git at git://nlp.cs.byu.edu/plf1/crowdflower-newsgroups.git

7 Conclusions and Future Work

We have argued that generative models are better
suited than discriminative models to the task of an-
notation aggregation since they tend to be more
robust to annotation noise and to approach their
asymptotic performance levels with fewer annota-
tions. Also, in settings where a discriminative model
would usually shine, there are often enough annota-
tions that a simple baseline of majority vote is suffi-
cient.

In support of this argument, we developed
comparable mean-field variational inference for
a generative-discriminative pair of crowdsourcing
models and compared them on both crowdsourced
and synthetic annotations on six text classification
datasets. In practice we found that on classification
tasks for which generative models of the data work
reasonably well, the generative model greatly out-
performs its discriminative log-linear counterpart.

The generative multinomial model we employed
makes inter-feature independence assumptions ill
suited to some classification tasks. Document topic
models (Blei, 2012) could be used as the basis
of a more sophisticated generative crowdsourcing
model. One might also transform the data to make
it more amenable to a simple model using docu-
ments assembled from distributed word represen-
tations (Mikolov et al., 2013). Finally, although
we expect these results to generalize, we have only
experimented with text classification. Similar ex-
periments could be performed on other commonly
crowdsourced tasks such as sequence labeling.

Acknowledgments

We thank Alex Smola and the anonymous reviewers
for their insightful comments. This work was sup-
ported by the collaborative NSF Grant IIS-1409739
(BYU) and IIS-1409287 (UMD).

References

[Asuncion et al.2009] A. Asuncion, M. Welling,
P. Smyth, and Y. W. Teh. 2009. On smoothing
and inference for topic models. In Proceedings of the
Twenty-Fifth Conference on Uncertainty in Artificial
Intelligence, pages 27–34. AUAI Press.

[Berry et al.2001] M. W. Berry, M. Browne, and

890



B. Signer. 2001. Topic annotated Enron email data
set. Linguistic Data Consortium, Philadelphia.

[Blei2012] D. Blei. 2012. Probabilistic topic models.
Communications of the ACM, 55(4):77–84.

[Bragg et al.2013] J. Bragg, Mausam, and D. Weld. 2013.
Crowdsourcing multi-label classification for taxon-
omy creation. In First AAAI Conference on Human
Computation and Crowdsourcing.

[Cardoso-Cachopo2007] A. Cardoso-Cachopo. 2007.
Improving Methods for Single-label Text Categoriza-
tion. Ph.D. thesis, Universidade Tecnica de Lisboa.

[Carroll et al.2007] J. Carroll, R. Haertel, P. McClanahan,
E. Ringger, and K. Seppi. 2007. Modeling the anno-
tation process for ancient corpus creation. In Proceed-
ings of ECAL 2007, pages 25–42. Charles University.

[Dawid and Skene1979] A.P. Dawid and A.M. Skene.
1979. Maximum likelihood estimation of observer
error-rates using the EM algorithm. Applied Statistics,
pages 20–28.

[Felt et al.2014] P. Felt, R. Haertel, E. Ringger, and
K. Seppi. 2014. MomResp: A Bayesian model for
multi-annotator document labeling. In Proceedings of
LREC.

[Hovy et al.2013] D. Hovy, T. Berg-Kirkpatrick,
A. Vaswani, and E. Hovy. 2013. Learning whom to
trust with MACE. In Proceedings of HLT-NAACL
2013, pages 1120–1130.

[Jurgens2013] D. Jurgens. 2013. Embracing ambigu-
ity: A comparison of annotation methodologies for
crowdsourcing word sense labels. In Proceedings of
NAACL-HLT 2013, pages 556–562.

[Lam and Stork2005] C. P. Lam and D. G. Stork. 2005.
Toward optimal labeling strategy under multiple unre-
liable labelers. In AAAI Spring Symposium: Knowl-
edge Collection from Volunteer Contributors.

[Liu et al.2012] Q. Liu, J. Peng, and A. Ihler. 2012. Vari-
ational inference for crowdsourcing. In NIPS, pages
692–700.

[McCallum2002] Andrew Kachites McCallum. 2002.
Mallet: A machine learning for language toolkit.
http://mallet.cs.umass.edu.

[Mikolov et al.2013] T. Mikolov, I. Sutskever, K. Chen,
G. Corrado, and J. Dean. 2013. Distributed represen-
tations of words and phrases and their composition-
ality. In Advances in Neural Information Processing
Systems, pages 3111–3119.

[Ng and Jordan2001] A. Ng and M. Jordan. 2001. On
discriminative vs. generative classifiers: A comparison
of logistic regression and naive Bayes. NIPS, 14:841–
848.

[Nigam et al.2006] K. Nigam, A. McCallum, and
T. Mitchell. 2006. Semi-supervised text classification
using EM. Semi-Supervised Learning, pages 33–56.

[Passonneau and Carpenter2013] R. Passonneau and
B. Carpenter. 2013. The benefits of a model of
annotation. In Proceedings of the 7th Linguistic Anno-
tation Workshop and Interoperability with Discourse,
pages 187–195. Citeseer.

[Pasternack and Roth2010] J. Pasternack and D. Roth.
2010. Knowing what to believe (when you already
know something). In COLING, Beijing, China.

[Raykar et al.2010] V. Raykar, S. Yu, L. Zhao,
G. Valadez, C. Florin, L. Bogoni, and L. Moy.
2010. Learning from crowds. The Journal of Machine
Learning Research, 11:1297–1322.

[Simpson and RobertsIn Press] E. Simpson and
S. Roberts. In Press. Bayesian methods for in-
telligent task assignment in crowdsourcing systems.
In Decision Making: Uncertainty, Imperfection,
Deliberation and Scalability. Springer.

[Smyth et al.1995] P. Smyth, U. Fayyad, M. Burl, P. Per-
ona, and P. Baldi. 1995. Inferring ground truth from
subjective labelling of Venus images. NIPS, pages
1085–1092.

[Snow et al.2008] R. Snow, B. O’Connor, D. Jurafsky,
and A. Ng. 2008. Cheap and fast—but is it good?:
Evaluating non-expert annotations for natural lan-
guage tasks. In Proceedings of EMNLP. ACL.

[Surowiecki2005] J. Surowiecki. 2005. The Wisdom of
Crowds. Random House LLC.

[Welinder et al.2010] P. Welinder, S. Branson, P. Perona,
and S. Belongie. 2010. The multidimensional wisdom
of crowds. In NIPS, pages 2424–2432.

[Whitehill et al.2009] J. Whitehill, P. Ruvolo, T. Wu,
J. Bergsma, and J. Movellan. 2009. Whose vote
should count more: Optimal integration of labels from
labelers of unknown expertise. NIPS, 22:2035–2043.

[Yan et al.2014] Y. Yan, R. Rosales, G. Fung, R. Subra-
manian, and J. Dy. 2014. Learning from multiple an-
notators with varying expertise. Machine Learning,
95(3):291–327.

[Zhou et al.2012] D. Zhou, J. Platt, S. Basu, and Y. Mao.
2012. Learning from the wisdom of crowds by mini-
max entropy. In NIPS, volume 25, pages 2204–2212.

891


