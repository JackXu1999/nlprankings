Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 537–545,

Beijing, August 2010

537

Data-Driven Parsing with Probabilistic Linear Context-Free Rewriting

Systems

Laura Kallmeyer and Wolfgang Maier

SFB 833, University of T¨ubingen

{lk,wmaier}@sfs.uni-tuebingen.de

Abstract

This paper presents a ﬁrst efﬁcient imple-
mentation of a weighted deductive CYK
parser for Probabilistic Linear Context-
Free Rewriting Systems (PLCFRS), to-
gether with context-summary estimates
for parse items used to speed up pars-
ing. LCFRS, an extension of CFG, can de-
scribe discontinuities both in constituency
and dependency structures in a straight-
forward way and is therefore a natural
candidate to be used for data-driven pars-
ing. We evaluate our parser with a gram-
mar extracted from the German NeGra
treebank. Our experiments show that data-
driven LCFRS parsing is feasible with
a reasonable speed and yields output of
competitive quality.

1 Introduction

Data-driven parsing has largely been dominated
by Probabilistic Context-Free Grammar (PCFG).
The use of PCFG is tied to the annotation princi-
ples of popular treebanks, such as the Penn Tree-
bank (PTB) (Marcus et al., 1994), which are used
as a data source for grammar extraction. Their an-
notation generally relies on the use of trees with-
out crossing branches, augmented with a mech-
anism that accounts for non-local dependencies.
In the PTB, e.g., labeling conventions and trace
nodes are used which establish additional implicit
edges in the tree beyond the overt phrase struc-
ture. In contrast, some other treebanks, such as the
German NeGra and TIGER treebanks allow anno-
tation with crossing branches (Skut et al., 1997).

Non-local dependencies can then be expressed di-
rectly by grouping all dependent elements under a
single node.

However, given the expressivity restrictions of
PCFG, work on data-driven parsing has mostly
excluded non-local dependencies. When us-
ing treebanks with PTB-like annotation,
label-
ing conventions and trace nodes are often dis-
carded, while in NeGra, resp. TIGER, tree trans-
formations are applied which resolve the crossing
branches (K¨ubler, 2005; Boyd, 2007, e.g.). Espe-
cially for these treebanks, such a transformation is
questionable, since it is non-reversible and implies
information loss.

Some research has gone into incorporating non-
local information into data-driven parsing. Levy
and Manning (2004) distinguish three approaches:
1. Non-local information can be incorporated di-
rectly into the PCFG model (Collins, 1999), or
can be reconstructed in a post-processing step af-
ter PCFG parsing (Johnson, 2002; Levy and Man-
ning, 2004).
2. Non-local information can be
incorporated into complex labels (Hockenmaier,
2003). 3. A formalism can be used which accom-
modates the direct encoding of non-local informa-
tion (Plaehn, 2004). This paper pursues the third
approach.

Our work is motivated by the following re-
cent developments: Linear Context-Free Rewrit-
ing Systems (LCFRS) (Vijay-Shanker et al., 1987)
have been established as a candidate for mod-
eling both discontinuous constituents and non-
projective dependency trees as they occur in tree-
banks (Kuhlmann and Satta, 2009; Maier and
Lichte, 2009). LCFRS extend CFG such that
non-terminals can span tuples of possibly non-

538

CFG:

LCFRS:

A

•

γ1

A

γ

•

γ2

•

γ3

Figure 1: Different domains of locality

adjacent strings (see Fig. 1). PCFG techniques,
such as Best-First Parsing (Charniak and Cara-
ballo, 1998), Weighted Deductive Parsing (Neder-
hof, 2003) and A∗ parsing (Klein and Manning,
2003a), can be transferred to LCFRS. Finally,
German has attracted the interest of the parsing
community due to the challenges arising from its
frequent discontinuous constituents (K¨ubler and
Penn, 2008).

We bring together these developments by pre-
senting a parser for probabilistic LCFRS. While
parsers for subclasses of PLCFRS have been pre-
sented before (Kato et al., 2006), to our knowl-
edge, our parser is the ﬁrst for the entire class of
PLCFRS. We have already presented an applica-
tion of the parser on constituency and dependency
treebanks together with an extensive evaluation
(Maier, 2010; Maier and Kallmeyer, 2010). This
article is mainly dedicated to the presentation of
several methods for context summary estimation
of parse items, and to an experimental evaluation
of their usefulness. The estimates either act as
ﬁgures-of-merit in a best-ﬁrst parsing context or
as estimates for A∗ parsing. Our evaluation shows
that while our parser achieves a reasonable speed
already without estimates, the estimates lead to a
great reduction of the number of produced items,
all while preserving the output quality.

Sect. 2 and 3 of the paper introduce probabilis-
tic LCFRS and the parsing algorithm. Sect. 4
presents different context summary estimates. In
Sect. 5, the implementation and evaluation of the
work is discussed.

2 Probabilistic LCFRS

LCFRS are an extension of CFG where the non-
terminals can span not only single strings but, in-
stead, tuples of strings. We will notate LCFRS
with the syntax of simple Range Concatenation
Grammars (SRCG) (Boullier, 1998), a formalism

that is equivalent to LCFRS.

A LCFRS (Vijay-Shanker et al., 1987) is a tu-
ple hN, T, V, P, Si where a) N is a ﬁnite set of
non-terminals with a function dim: N → N that
determines the fan-out of each A ∈ N; b) T and V
are disjoint ﬁnite sets of terminals and variables;
c) S ∈ N is the start symbol with dim(S) = 1; d)
P is a ﬁnite set of rules

A(α1, . . . , αdim(A)) → A1(X (1)
· ·· Am(X (m)

1 , . . . , X (1)
, . . . , X (m)

dim(A1 ))
dim(Am ))

1

for m ≥ 0 where A, A1, . . . , Am ∈ N, X (i)
j ∈
V for 1 ≤ i ≤ m, 1 ≤ j ≤ dim(Ai) and
αi ∈ (T ∪ V )∗ for 1 ≤ i ≤ dim(A). For all
r ∈ P , it holds that every variable X occurring in
r occurs exactly once in the left-hand side (LHS)
and exactly once in the right-hand side (RHS).

A rewriting rule describes how the yield of
the LHS non-terminal can be computed from
the yields of the RHS non-terminals. The rules
A(ab, cd) → ε and A(aXb, cY d) → A(X, Y )
for instance specify that 1. hab, cdi is in the yield
of A and 2. one can compute a new tuple in the
yield of A from an already existing one by wrap-
ping a and b around the ﬁrst component and c and
d around the second.

For every A ∈ N in a LCFRS G, we deﬁne the
yield of A, yield(A) as follows:
a) For every A(~α) → ε, ~α ∈ yield(A);
b) For every rule
1 , . . . , X (1)
, . . . , X (m)

A(α1, . . . , αdim(A)) → A1(X (1)
·· · Am(X (m)

dim(A1 ))
dim(Am ))

1

and all ~τi ∈ yield(Ai) for 1 ≤ i ≤ m,
hf (α1), . . . , f (αdim(A))i ∈ yield(A) where f
is deﬁned as follows: (i) f (t) = t for all t ∈ T ,
(ii) f (X (i)
j ) = ~τi(j) for all 1 ≤ i ≤ m, 1 ≤
j ≤ dim(Ai) and (iii) f (xy) = f (x)f (y) for
all x, y ∈ (T ∪V )+. f is the composition func-
tion of the rule.

c) Nothing else is in yield(A).
The language is then {w |hwi ∈ yield(S)}.
The fan-out of an LCFRS G is the maximal fan-
out of all non-terminals in G. Furthermore, the
RHS length of a rewriting rules r ∈ P is called the
rank of r and the maximal rank of all rules in P
is called the rank of G. We call a LCFRS ordered
if for every r ∈ P and every RHS non-terminal A
in r and each pair X1, X2 of arguments of A in

539

the RHS of r, X1 precedes X2 in the RHS iff X1
precedes X2 in the LHS.

A probabilistic LCFRS (PLCFRS) (Kato et
al., 2006) is a tuple hN, T, V, P, S, pi such that
hN, T, V, P, Si is a LCFRS and p : P →
[0..1] a function such that for all A ∈ N:
ΣA(~x)→~Φ∈P p(A(~x) → ~Φ) = 1.
3 The CYK Parser

We use a probabilistic version of the CYK parser
from (Seki et al., 1991), applying techniques of
weighted deductive parsing (Nederhof, 2003).

LCFRS can be binarized (G´omez-Rodr´ıguez et
al., 2009) and ε-components in the LHS of rules
can be removed (Boullier, 1998). We can there-
fore assume that all rules are of rank 2 and do not
contain ε components in their LHS. Furthermore,
we assume POS tagging to be done before pars-
ing. POS tags are non-terminals of fan-out 1. The
rules are then either of the form A(a) → ε with A
a POS tag and a ∈ T or of the form A(~α) → B(~x)
or A(~α) → B(~x)C(~y) where ~α ∈ (V +)dim(A),
i.e., only the rules for POS tags contain terminals
in their LHSs.

For every w ∈ T ∗, where w = w1 . . . wn with
wi ∈ T for 1 ≤ i ≤ n, we deﬁne: P os(w) :=
{0, . . . , n}. A pair hl, ri ∈ P os(w) × P os(w)
with l ≤ r is a range in w. Its yield hl, ri(w) is
the string wl+1 . . . wr. The yield ~ρ(w) of a vec-
tor of ranges ~ρ is the vector of the yields of the
single ranges. For two ranges ρ1 = hl1, r1i, ρ2 =
hl2, r2i: if r1 = l2, then ρ1 · ρ2 = hl1, r2i; other-
wise ρ1 · ρ2 is undeﬁned.
For a given rule p : A(α1, . . . , αdim(A)) →
we
B(X1, . . . , Xdim(B))C(Y1, . . . , Xdim(C))
now extend the composition function f to ranges,
given an input w: for all range vectors ~ρB and
~ρC of dimensions dim(B) and dim(C) respec-
tively, fr( ~ρB, ~ρC ) = hg(α1), . . . , g(αdim(A))i
is deﬁned as follows: g(Xi) = ~ρB(i) for all
1 ≤ i ≤ dim(B), g(Yi) = ~ρC(i) for all
1 ≤ i ≤ dim(C) and g(xy) = g(x) · g(y) for all
x, y ∈ V +. p : A(fr( ~ρB, ~ρC)) → B( ~ρB)C( ~ρC )
is then called an instantiated rule.
For a given input w, our items have the
form [A, ~ρ] where A ∈ N, ~ρ ∈ (P os(w) ×
P os(w))dim(A). The vector ~ρ characterizes the
span of A. We specify the set of weighted parse

Scan:

Unary:

0 : [A,hhi, i + 1ii]
in : [B, ~ρ]

A POS tag of wi+1

p : A(~α) → B(~α) ∈ P

in + |log(p)| : [A, ~ρ]
inB : [B, ~ρB], inC : [C, ~ρC]
inB + inC + log(p) : [A, ~ρA]

Binary:
where p : A( ~ρA) → B( ~ρB)C( ~ρC) is an instantiated rule.
Goal: [S, hh0, nii]

Figure 2: Weighted CYK deduction system

add SCAN results to A
while A 6= ∅
remove best item x : I from A
add x : I to C
if I goal item
then stop and output true
else
for all y : I′ deduced from x : I and items in C:
if there is no z with z : I′ ∈ C ∪ A
then add y : I′ to A
else if z : I′ ∈ A for some z
then update weight of I′ in A to max (y, z)
Figure 3: Weighted deductive parsing

items via the deduction rules in Fig. 2. Our parser
performs a weighted deductive parsing (Nederhof,
2003), based on this deduction system. We use a
chart C and an agenda A, both initially empty, and
we proceed as in Fig. 3.

4 Outside Estimates

In order to speed up parsing, we add an estimate of
the log of the outside probabilities of the items to
their weights in the agenda. All our outside esti-
mates are admissible (Klein and Manning, 2003a)
which means that they never underestimate the ac-
tual outside probability of an item. However, most
of them are not monotonic. In other words, it can
happen that we deduce an item I2 from an item I1
where the weight of I2 is greater than the weight
of I1. The parser can therefore end up in a local
maximum that is not the global maximum we are
searching for. In other words, our outside weights
are only ﬁgures of merit (FOM). Only for the full
SX estimate, the monotonicity is guaranteed and
we can do true A∗ parsing as described in (Klein
and Manning, 2003a) that always ﬁnds the best
parse.

All outside estimates are computed for a certain

maximal sentence length len max.

540

POS tags:

A a POS tag

0 : [A,h1i]
in : [B,~l]

Unary:

in + log(p) : [A,~l]

p : A(~α) → B(~α) ∈ P

inB : [B,~lB], inC : [C,~lC ]
inB + inC + log(p) : [A,~lA]

Binary:
where p : A( ~αA) → B( ~αB)C( ~αC ) ∈ P and the follow-
ing holds: we deﬁne B(i) as {1 ≤ j ≤ dim(B)| ~αB(j)
occurs in ~αA(i)} and C(i) as {1 ≤ j ≤ dim(C) | ~αC (j)
occurs in ~αA(i)}. Then for all i, 1 ≤ i ≤ dim(A):
~lA(i) = Σj∈B(i)

~lC (j).

~lB(j) + Σj∈C(i)
Figure 4: Inside estimate

4.1 Full SX estimate

The full SX estimate, for a given sentence length
n, is supposed to give the minimal costs (maxi-
mal probability) of completing a category X with
a span ρ into an S with span hh0, nii.
For the computation, we need an estimate of
the inside probability of a category C with a span
ρ, regardless of the actual terminals in our in-
put. This inside estimate is computed as shown
in Fig. 4. Here, we do not need to consider the
number of terminals outside the span of C (to
the left or right or in the gaps), they are not rel-
evant for the inside probability. Therefore the
items have the form [A,hl1, . . . , ldim(A)i], where
A is a non-terminal and li gives the length of its
ith component.
It holds that Σ1≤i≤dim(A)li ≤
len max − dim(A) + 1.
A straight-forward extension of the CFG algo-
rithm from (Klein and Manning, 2003a) for com-
puting the SX estimate is given in Fig. 5. For a
given range vector ρ = hhl1, r1i, . . . ,hlk, rkii and
a sentence length n, we deﬁne its inside length
vector lin(ρ) as hr1 − l1, . . . , rk − lki and its
outside length vector lout(ρ) as hl1, r1 − l1, l2 −
r1, . . . , lk − rk−1, rk − lk, n − rki.
This algorithm has two major problems: Since
it proceeds top-down, in the Binary rules, we must
compute all splits of the antecedent X span into
the spans of A and B which is very expensive.
Furthermore, for a category A with a certain num-
ber of terminals in the components and the gaps,
we compute the lower part of the outside estimate
several times, namely for every combination of
number of terminals to the left and to the right
(ﬁrst and last element in the outside length vec-

Axiom :

Unary:

0 : [S, h0, len, 0i]
w + log(p) : [B,~l]

w : [A,~l]

1 ≤ len ≤ len max
p : A(~α) → B(~α) ∈ P

Binary-right:

w : [X,~lX ]

w + in(A,~l′A) + log(p) : [B,~lB]
Binary-left:

w : [X,~lX ]

w + in(B,~l′B) + log(p) : [A,~lA]
where, for both rules,
there is an instantiated rule p :
X(~ρ) → A( ~ρA)B( ~ρB) such that ~lX = lout(ρ), ~lA =
lout(ρA),~l′A = lin(ρA), ~lB = lout(ρB,~lB = lin(ρB.

Figure 5: Full SX estimate top-down

tor).
In order to avoid these problems, we now
abstract away from the lengths of the part to the
left and the right, modifying our items such as to
allow a bottom-up strategy.

The idea is to compute the weights of items rep-
resenting the derivations from a certain lower C
up to some A (C is a kind of “gap” in the yield of
A) while summing up the inside costs of off-spine
nodes and the log of the probabilities of the corre-
sponding rules. We use items [A, C, ρA, ρC, shift ]
where A, C ∈ N and ρA, ρC are range vectors,
both with a ﬁrst component starting at position 0.
The integer shift ≤ len max tells us how many po-
sitions to the right the C span is shifted, compared
to the starting position of the A. ρA and ρC repre-
sent the spans of C and A while disregarding the
number of terminals to the left the right. I.e., only
the lengths of the components and of the gaps are
encoded. This means in particular that the length
n of the sentence does not play a role here. The
right boundary of the last range in the vectors is
limited to len max. For any i, 0 ≤ i ≤ len max,
and any range vector ρ, we deﬁne shift(ρ, i) as the
range vector one obtains from adding i to all range
boundaries in ρ and shift(ρ,−i) as the range vec-
tor one obtains from subtracting i from all bound-
aries in ρ.

The weight of [A, C, ρA, ρC, i] estimates the
costs for completing a C tree with yield ρC into
an A tree with yield ρA such that, if the span of A
starts at position j, the span of C starts at position
i + j. Fig. 6 gives the computation. The value of
in(A,~l) is the inside estimate of [A,~l].

The SX-estimate for some predicate C with

541

C a POS tag

Axiom :

0 : [S, len, 0, 0, 0]

1 ≤ len ≤ len max

POS tags:

Unary:
Binary-right:

0 : [C, C,h0, 1i,h0, 1i, 0]
0 : [B, B, ρB, ρB, 0]

log(p) : [A, B, ρB, ρB, 0]

p : A(~α) → B(~α) ∈ P

0 : [A, A, ρA, ρA, 0], 0 : [B, B, ρB, ρB, 0]
in(A, l(ρA)) + log(p) : [X, B, ρX, ρB, i]

Binary-left:

0 : [A, A, ρA, ρA, 0], 0 : [B, B, ρB, ρB, 0]
in(B, l(ρB)) + log(p) : [X, A, ρX, ρA, 0]
where i is such that for shift(ρB, i) = ρ′B p : X(ρX ) →
A(ρA)B(ρ′B) is an instantiated rule.
Starting sub-trees with larger gaps:

w : [B, C, ρB, ρC , i]
0 : [B, B, ρB, ρB, 0]

Transitive closure of sub-tree combination:

w1 : [A, B, ρA, ρB, i], w2 : [B, C, ρB, ρC , j]

w1 + w2 : [A, C, ρA, ρC , i + j]

Figure 6: Full SX estimate bottom-up

span ρ where i is the left boundary of the
ﬁrst component of ρ and with sentence length
n is then given by the maximal weight of
[S, C,h0, ni, shift (−i, ρ), i]. Among our esti-
mates, the full SX estimate is the only one that
is monotonic and that allows for true A∗ parsing.

4.2 SX with Left, Gaps, Right, Length

A problem of the previous estimate is that with
a large number of non-terminals the computation
of the estimate requires too much space. Our ex-
periments have shown that for treebank parsing
where we have, after binarization and markoviza-
tion, appr. 12,000 non-terminals, its computation
is not feasible. We therefore turn to simpler es-
timates with only a single non-terminal per item.
We now estimate the outside probability of a non-
terminal A with a span of a length length (the
sum of the lengths of all the components of the
span), with left terminals to the left of the ﬁrst
component, right terminals to the right of the
last component and gaps terminals in between the
components of the A span, i.e., ﬁlling the gaps.
Our items have the form [X, len, left, right , gaps]
with X ∈ N, len + left + right + gaps ≤ len max,
len ≥ dim(X), gaps ≥ dim(X) − 1.
in the rule X(~α) →
A( ~αA)B( ~αB), when looking at the vector ~α, we
have lef tA variables for A-components preceding
the ﬁrst variable of a B component, rightA vari-
ables for A-components following the last vari-

Let us assume that,

w : [X, len, l, r, g]

w + log(p) : [A, len, l, r, g]

Unary:
where p : X(~α) → A(~α) ∈ P .
Binary-right:

w : [X, len, l, r, g]

w + in(A, len − lenB) + log(p) : [B, len B, lB, rB, gB]
Binary-left:

w : [X, len, l, r, g]

w + in(B, len − len A) + log(p) : [A, lenA, lA, rA, gA]
where, for both rules, p : X(~α) → A( ~αA)B( ~αB) ∈ P .
Figure 7: SX with length, left, right, gaps

POS tags:

0 : [A, 1]

A a POS tag

Unary:

in : [B, l]

in + log(p) : [A, l]

p : A(~α) → B(~α) ∈ P

inB : [B, lB], inC : [C, lC ]

inB + inC + log(p) : [A, lB + lC]

Binary:
where either p : A( ~αA) → B( ~αB)C( ~αC ) ∈ P or p :
A( ~αA) → C( ~αC )B( ~αB) ∈ P .
Figure 8: Inside estimate with total span length

able of a B component and rightB variables for
B-components following the last variable of a A
component. (In our grammars, the ﬁrst LHS argu-
ment always starts with the ﬁrst variable from A.)
Furthermore, gapsA = dim(A)−lef tA−rightA,
gapsB = dim(B) − rightB.
Fig. 7 gives the computation of the estimate.
The following side conditions must hold: For
Binary-right to apply, the following constraints
must be satisﬁed: a) len + l + r + g = len B +
lB +rB +gB, b) lB ≥ l +lef tA, c) if rightA > 0,
then rB ≥ r+rightA, else (rightA = 0), rB = r,
d) gB ≥ gapsA. Similarly, for Binary-left to ap-
ply, the following constraints must be satisﬁed: a)
len + l + r + g = len A + lA + rA + gA, b) lA = l,
c) if rightB > 0, then rA ≥ r + rightB, else
(rightB = 0), rA = r d) gA ≥ gapsB.
The value in(X, l) for a non-terminal X and a
length l, 0 ≤ l ≤ len max is an estimate of the
probability of an X category with a span of length
l. Its computation is speciﬁed in Fig. 8.

The SX-estimate for a sentence length n and
for some predicate C with a range characterized
by ~ρ = hhl1, r1i, . . . ,hldim (C), rdim (C)ii where
len = Σdim(C)
(ri − li) and r = n − rdim(C)
is then given by the maximal weight of the item
[C, len, l1, r, n − len − l1 − r].

i=1

542

Axiom :

0 : [S, len, 0, 0]

1 ≤ len ≤ len max

w : [X, len, lr , g]

w + log(p) : [A, len, lr , g]

Unary:
where p : X(~α) → A(~α) ∈ P .
Binary-right:

w : [X, len, lr , g]

w + in(A, len − len B) + log(p) : [B, len B, lr B, gB]
Binary-left:

w : [X, len, lr , g]

w + in(B, len − len A) + log(p) : [A, len A, lr A, gA]
where, for both rules, p : X(~α) → A( ~αA)B( ~αB) ∈ P .
Figure 9: SX estimate with length, LR, gaps

4.3 SX with LR, Gaps, Length
In order to further decrease the space complex-
ity, we can simplify the previous estimate by sub-
suming the two lengths left and right in a sin-
gle length lr. I.e., the items now have the form
[X, len, lr , gaps] with X ∈ N, len + lr + gaps ≤
len max, len ≥ dim(X), gaps ≥ dim(X) − 1.
The computation is given in Fig. 9. Again, we
deﬁne lef tA, gapsA, rightA and gapsB, rightB
for a rule X(~α) → A( ~αA)B( ~αB) as above. The
side conditions are as follows: For Binary-right to
apply, the following constraints must be satisﬁed:
a) len + lr + g = len B + lr B + gB, b) lr < lr B,
and c) gB ≥ gapsA. For Binary-left to apply, the
following must hold: a) len + lr + g = len A +
lr A + gA, b) if rightB = 0 then lr = lr A, else
lr < lr A and c) gA ≥ gapsB.
The SX-estimate for a sentence length n
and for some predicate C with a span ~ρ =
hhl1, r1i, . . . ,hldim (C), rdim (C)ii where len =
Σdim(C)
(ri − li) and r = n − rdim(C) is then the
maximal weight of [C, len, l1 +r, n−len−l1−r].
5 Evaluation

i=1

The goal of our evaluation of our parser is to
show that, ﬁrstly, reasonable parser speed can be
achieved and, secondly, the parser output is of
promising quality.

5.1 Data
Our data source is the German NeGra treebank
(Skut et al., 1997).
In a preprocessing step,
following common practice (K¨ubler and Penn,
2008), we attach punctuation (not included in the
NeGra annotation) as follows: In a ﬁrst pass, us-

ing heuristics, we attach punctuation as high as
possible while avoiding to introduce new crossing
branches. In a second pass, parentheses and quo-
tation marks preferably attach to the same node.
Grammatical function labels on the edges are dis-
carded.

We create data sets of different sizes in order
to see how the size of the training set relates to
the gain using context summary estimates and to
the output quality of the parser. The ﬁrst set uses
the ﬁrst 4000 sentences and the second one all
sentences of NeGra. Due to memory limitations,
in both sets, we limit ourselves to sentences of a
maximal length of 25 words. We use the ﬁrst 90%
of both sets as training set and the remaining 10%
as test set. Tab. 1 shows the resulting sizes.

NeGra-small
test
training
316

2839

size

NeGra

training
14858

test
1651

Table 1: Test and training sets

5.2 Treebank Grammar Extraction

S

VP

VP

PROAV VMFIN
dar¨uber
about it

muß
must
“It must be thought about it”

thought

VVPP

VAINF
nachgedacht werden

be

Figure 10: A sample tree from NeGra

As already mentioned, in NeGra, discontinu-
ous phrases are annotated with crossing branches
(see Fig. 10 for an example with two discontin-
uous VPs). Such discontinuities can be straight-
forwardly modelled with LCFRS. We use the al-
gorithm from Maier and Søgaard (2008) to extract
LCFRS rules from NeGra and TIGER. It ﬁrst cre-
ates rules of the form P (a) → ε for each pre-
terminal P dominating some terminal a. Then
for all other nonterminals A0 with the children
A1 ··· Am, a clause A0 → A1 ··· Am is cre-
ated. The arguments of the A1 ··· Am are sin-
gle variables where the number of arguments is
the number of discontinuous parts in the yield of
a predicate. The arguments of A0 are concate-
nations of these variables that describe how the

543

discontinuous parts of the yield of A0 are ob-
tained from the yields of its daughters. Differ-
ent occurrences of the same non-terminal, only
with different fan-outs, are distinguished by corre-
sponding subscripts. Note that this extraction al-
gorithm yields only monotone LCFRS (equivalent
to ordered simple RCG). See Maier and Søgaard
(2008) for further details. For Fig. 10, we obtain
for instance the rules in Fig. 11.

VMFIN(muß) → ε
PROAV(Dar¨uber) → ε
VVPP(nachgedacht) → ε VAINF(werden) → ε
S1(X1X2X3) → VP2(X1, X3) VMFIN(X2)
VP2(X1, X2X3) → VP2(X1, X2) VAINF(X3)
VP2(X1, X2) → PROAV(X1) VVPP(X2)
Figure 11: LCFRS rules for the tree in Fig. 10

5.3 Binarization and Markovization

Before parsing, we binarize the extracted LCFRS.
For this we ﬁrst apply Collins-style head rules,
based on the rules the Stanford parser (Klein and
Manning, 2003b) uses for NeGra, to mark the
resp. head daughters of all non-terminal nodes.
Then, we reorder the RHSs such that the sequence
γ of elements to the right of the head daughter is
reversed and moved to the beginning of the RHS.
We then perform a binarization that proceeds from
left to right. The binarization works like the trans-
formation into Chomsky Normal Form for CFGs
in the sense that for RHSs longer than 2, we in-
troduce a new non-terminal that covers the RHS
without the ﬁrst element. The rightmost new rule,
which covers the head daughter, is binarized to
unary. We do not use a unique new non-terminal
for every new rule. Instead, to the new symbols
introduced during the binarization (VPbin in the
example), a variable number of symbols from the
vertical and horizontal context of the original rule
is added in order to achieve markovization. Fol-
lowing the literature, we call the respective quan-
tities v and h. For reasons of space we restrict
ourselves here to the example in Fig. 12. Refer to
Maier and Kallmeyer (2010) for a detailed presen-
tation of the binarization and markovization.

The probabilities are then computed based on
the rule frequencies in the transformed treebank,
using a Maximum Likelihood estimator.

S

VP

PDS
das
that

VMFIN

muß
must

PIS
man
one

AD V VVINF
jetzt
machen
now

do

“One has to do that now”

Tree after binarization:

S

Sbin

VP

Sbin

VPbin

VPbin

PDS

VMFIN PIS ADV VVINF

Figure 12: Sample binarization

5.4 Evaluation of Parsing Results
In order to assess the quality of the output of
our parser, we choose an EVALB-style metric,
i.e., we compare phrase boundaries. In the con-
text of LCFRS, we compare sets of items [A, ~ρ]
that characterize the span of a non-terminal A in
a derivation tree. One set is obtained from the
parser output, and one from the corresponding
treebank trees. Using these item sets, we compute
labeled and unlabeled recall (LR/UR), precision
(LP/UP), and the F1 measure (LF1/UF1). Note
that if k = 1, our metric is identical to its PCFG
equivalent.We are aware of the recent discussion
about the shortcomings of EVALB. A discussion
of this issue is presented in Maier (2010).

For

5.5 Experiments
In all experiments, we provide the parser with
gold part-of-speech tags.
the experi-
ments with NeGra-small, the parser is given the
markovization settings v = 1 and h = 1. We com-
pare the parser performance without estimates
(OFF) with its performance with the estimates de-
scribed in 4.2 (SIMPLE) and 4.3 (LR). Tab. 2
shows the results. Fig. 13 shows the number of
items produced by the parser, indicating that the
estimates have the desired effect of preventing un-
necessary items from being produced. Note that it
is even the case that the parser produces less items
for the big set with LR than for the small set with-
out estimate.

We can see that the estimates lead to a slightly

544

UP/UR
UF1
LP/LR
LF1
Parsed

OFF

SIMPLE

LR

72.29/72.40

70.49/71.81

72.10/72.60

68.31/68.41

64.93/66.14

67.35/66.14

72.35

68.36

71.14

65.53

72.35

65.53

313 (99.05%)

313 (99.05%)

313 (99.05%)

Table 2: Experiments with NeGra-small

OFF (NeGra)
LR (NeGra)
OFF (NeGra-small)
SIMPLE (NeGra-small)
LR (NeGra-small)

 500

 450

 400

 350

 300

 250

 200

 150

 100

 50

)
0
0
0
1
 
n
i
(
 
s
m
e
t
i
 
f
o

 
.
o
N

 16

 18

 20

 22

 24

Sentence length

Figure 13: Items produced by the parser

lower F-score. However, while the losses in terms
of F1 are small, the gains in parsing time are sub-
stantial, as Fig. 13 shows.

Tab. 3 shows the results of experiments with
NeGra, with the markovization settings v = 2
and h = 1 which have proven to be successful
for PCFG parsing of NeGra (Rafferty and Man-
ning, 2008). Unfortunately, due to memory re-
strictions, we were not able to compute SIMPLE
for the large data set.1 Resp. LR, the ﬁndings
are comparable to the ones for NeGra-short. The
speedup is paid with a lower F1.

OFF

LR

77.12

73.03/73.46

76.89/77.35

75.22/75.99

UP/UR
UF1
LP/LR
LF1
Parsed
1642 (99.45%)
Table 3: Experiments with NeGra

1642 (99.45%)

70.98/71.70

75.60

71.33

73.25

Our results are not directly comparable with
PCFG parsing results, since LCFRS parsing is a

harder task. However, since the EVALB met-
ric coincides for constituents without crossing
branches, in order to place our results in the con-
text of previous work on parsing NeGra, we cite
some of the results from the literature which were
obtained using PCFG parsers2: K¨ubler (2005)
(Tab. 1, plain PCFG) obtains 69.4, Dubey and
Keller (2003) (Tab. 5, sister-head PCFG model)
71.12, Rafferty and Manning (2008) (Tab. 2, Stan-
ford parser with markovization v = 2 and h = 1)
77.2, and Petrov and Klein (2007) (Tab. 1, Berke-
ley parser) 80.1. Plaehn (2004) obtains 73.16 La-
beled F1 using Probabilistic Discontinuous Phrase
Structure Grammar (DPSG), albeit only on sen-
tences with a length of up to 15 words. On those
sentences, we obtain 81.27.

The comparison shows that our system deliv-
ers competitive results. Additionally, when com-
paring this to PCFG parsing results, one has
to keep in mind that LCFRS parse trees con-
tain non-context-free information about disconti-
nuities. Therefore, a correct parse with our gram-
mar is actually better than a correct CFG parse,
evaluated with respect to a transformation of Ne-
Gra into a context-free treebank where precisely
this information gets lost.

6 Conclusion

We have presented the ﬁrst parser for unrestricted
Probabilistic Linear Context-Free Rewriting Sys-
tems (PLCFRS), implemented as a CYK parser
with weighted deductive parsing. To speed up
parsing, we use context summary estimates for
parse items. An evaluation on the NeGra treebank,
both in terms of output quality and speed, shows
that data-driven parsing using PLCFRS is feasi-
ble. Already in this ﬁrst attempt with a straight-
forward binarization, we obtain results that are
comparable to state-of-the-art PCFG results in
terms of F1, while yielding parse trees that are
richer than context-free trees since they describe
discontinuities. Therefore, our approach demon-
strates convincingly that PLCFRS is a natural and
tractable alternative for data-driven parsing which
takes non-local dependencies into consideration.

1SIMPLE also proved to be infeasible to compute for the
small set for the markovization settings v = 2 and h = 1
due to the greatly increased label set with this settings.

2Note that these results were obtained on sentences with
a length of ≤ 40 words and that those parser possibly would
deliver better results if tested on our test set.

545

References
Boullier, Pierre. 1998. A Proposal for a Natural Lan-
guage Processing Syntactic Backbone. Technical
Report 3342, INRIA.

Boyd, Adriane. 2007. Discontinuity revisited: An im-
proved conversion to context-free representations.
In The Linguistic Annotation Workshop at ACL
2007.

Charniak, Eugene and Sharon A. Caraballo. 1998.
New ﬁgures of merit for best-ﬁrst probabilistic chart
parsing. Computational Linguistics, 24.

Collins, Michael. 1999. Head-driven statistical mod-
els for natural language parsing. Ph.D. thesis, Uni-
versity of Pennsylvania.

Dubey, Amit and Frank Keller. 2003. Probabilistic
parsing for German using sisterhead dependencies.
In Proceedings of ACL.

G´omez-Rodr´ıguez, Carlos, Marco Kuhlmann, Giorgio
Satta, and David Weir. 2009. Optimal reduction of
rule length in linear context-free rewriting systems.
In Proceedings of NAACL-HLT.

Hockenmaier, Julia. 2003. Data and models for Statis-
tical Parsing with Combinatory Categorial Gram-
mar. Ph.D. thesis, University of Edinburgh.

Johnson, Mark. 2002. A simple pattern-matching al-
gorithm for recovering empty nodes and their an-
tecedents. In Proceedings of ACL.

Kato, Yuki, Hiroyuki Seki, and Tadao Kasami. 2006.
Stochastic multiple context-free grammar for rna
pseudoknot modeling. In Proceedings of TAG+8.

Klein, Dan and Christopher D. Manning. 2003a. A*
Parsing: Fast Exact Viterbi Parse Selection. In Pro-
ceedings of NAACL-HLT.

Klein, Dan and Christopher D. Manning. 2003b. Fast
exact inference with a factored model for natural
language parsing. In In Advances in Neural Infor-
mation Processing Systems 15 (NIPS).

K¨ubler, Sandra and Gerald Penn, editors. 2008. Pro-
ceedings of the Workshop on Parsing German at
ACL 2008.

K¨ubler, Sandra. 2005. How do treebank annotation
schemes inﬂuence parsing results? Or how not to
compare apples and oranges.
In Proceedings of
RANLP 2005.

Kuhlmann, Marco and Giorgio Satta. 2009. Treebank
grammar techniques for non-projective dependency
parsing. In Proceedings of EACL.

Levy, Roger and Christopher D. Manning. 2004. Deep
dependencies from context-free statistical parsers:
correcting the surface dependency approximation.
In Proceedings of ACL.

Maier, Wolfgang and Laura Kallmeyer. 2010. Discon-
tinuity and non-projectivity: Using mildly context-
sensitive formalisms for data-driven parsing.
In
Proceedings of TAG+10.

Maier, Wolfgang and Timm Lichte. 2009. Charac-
terizing Discontinuity in Constituent Treebanks. In
Proceedings of Formal Grammar 2009.

Maier, Wolfgang and Anders Søgaard. 2008. Tree-
banks and mild context-sensitivity. In Proceedings
of Formal Grammar 2008.

Maier, Wolfgang. 2010. Direct parsing of discontin-
uous constituents in german. In Proceedings of the
SPMRL workshop at NAACL HLT 2010.

Marcus, Mitchell, Grace Kim, Mary Ann
Marcinkiewicz, Robert MacIntyre, Ann Bies,
Mark Ferguson, Karen Katz, and Britta Schas-
berger. 1994. The Penn Treebank: Annotating
predicate argument structure.
In Proceedings of
HLT.

Nederhof, Mark-Jan. 2003. Weighted Deductive Pars-
ing and Knuth’s Algorithm. Computational Lin-
guistics, 29(1).

Petrov, Slav and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of HLT-
NAACL 2007.

Plaehn, Oliver. 2004. Computing the most proba-
ble parse for a discontinuous phrase-structure gram-
mar.
In New developments in parsing technology.
Kluwer.

Rafferty, Anna and Christopher D. Manning, 2008.
Parsing Three German Treebanks: Lexicalized and
Unlexicalized Baselines.
In K¨ubler and Penn
(2008).

Seki, Hiroyuki, Takahashi Matsumura, Mamoru Fujii,
and Tadao Kasami. 1991. On multiple context-free
grammars. Theoretical Computer Science, 88(2).

Skut, Wojciech, Brigitte Krenn, Thorten Brants, and
Hans Uszkoreit. 1997. An Annotation Scheme
for Free Word Order Languages. In Proceedings of
ANLP.

Vijay-Shanker, K., David J. Weir, and Aravind K.
Joshi. 1987. Characterizing structural descriptions
produced by various grammatical formalisms.
In
Proceedings of ACL.

