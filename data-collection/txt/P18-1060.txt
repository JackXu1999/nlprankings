















































The price of debiasing automatic metrics in natural language evalaution


Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 643–653
Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics

643

The price of debiasing automatic metrics in natural language evaluation

Arun Tejasvi Chaganty∗ and Stephen Mussmann∗ and Percy Liang
Computer Science Department, Stanford University

{chaganty,mussmann,pliang}@cs.stanford.edu

Abstract

For evaluating generation systems, auto-
matic metrics such as BLEU cost noth-
ing to run but have been shown to corre-
late poorly with human judgment, leading
to systematic bias against certain model
improvements. On the other hand, av-
eraging human judgments, the unbiased
gold standard, is often too expensive. In
this paper, we use control variates to com-
bine automatic metrics with human evalu-
ation to obtain an unbiased estimator with
lower cost than human evaluation alone.
In practice, however, we obtain only a 7–
13% cost reduction on evaluating summa-
rization and open-response question an-
swering systems. We then prove that our
estimator is optimal: there is no unbi-
ased estimator with lower cost. Our the-
ory further highlights the two fundamen-
tal bottlenecks—the automatic metric and
the prompt shown to human evaluators—
both of which need to be improved to ob-
tain greater cost savings.

1 Introduction

In recent years, there has been an increasing in-
terest in tasks that require generating natural lan-
guage, including abstractive summarization (Nal-
lapati et al., 2016), open-response question an-
swering (Nguyen et al., 2016; Kočisky et al.,
2017), image captioning (Lin et al., 2014), and
open-domain dialogue (Lowe et al., 2017b). Un-
fortunately, the evaluation of these systems re-
mains a thorny issue because of the diversity of
possible correct responses. As the gold standard
of performing human evaluation is often too ex-
pensive, there has been a large effort develop-

∗Authors contributed equally.

ing automatic metrics such as BLEU (Papineni
et al., 2002), ROUGE (Lin and Rey, 2004), ME-
TEOR (Lavie and Denkowski, 2009; Denkowski
and Lavie, 2014) and CiDER (Vedantam et al.,
2015). However, these have shown to be biased,
correlating poorly with human metrics across dif-
ferent datasets and systems (Liu et al., 2016b;
Novikova et al., 2017).

Can we combine automatic metrics and human
evaluation to obtain an unbiased estimate at lower
cost than human evaluation alone? In this paper,
we propose a simple estimator based on control
variates (Ripley, 2009), where we average differ-
ences between human judgments and automatic
metrics rather than averaging the human judg-
ments alone. Provided the two are correlated, our
estimator will have lower variance and thus reduce
cost.

We prove that our estimator is optimal in the
sense that no unbiased estimator using the same
automatic metric can have lower variance. We
also analyze its data efficiency (equivalently, cost
savings)—the factor reduction in number of hu-
man judgments needed to obtain the same accu-
racy versus naive human evaluation—and show
that it depends solely on two factors: (a) the an-
notator variance (which is a function of the hu-
man evaluation prompt) and (b) the correlation be-
tween human judgments and the automatic met-
ric. This factorization allows us to calculate typi-
cal and best-case data efficiencies and accordingly
refine the evaluation prompt or automatic metric.

Finally, we evaluate our estimator on state-
of-the-art systems from two tasks, summariza-
tion on the CNN/Daily Mail dataset (Hermann
et al., 2015; Nallapati et al., 2016) and open-
response question answering on the MS MAR-
COv1.0 dataset (Nguyen et al., 2016). To study
our estimators offline, we preemptively collected
10,000 human judgments which cover several



644

0.5 0.6 0.7 0.8
Human judgement

0.2

0.3

0.4

R
O

U
G

E
-L

fastqa

fastqa ext

snet

snet.ens

(a) System-level correlation on the MS MARCO task

0.0 0.5 1.0
Human judgment

0.0

0.2

0.4

0.6

0.8

1.0

R
O

U
G

E
-L

(b) Instance-level correlation for the fastqa system

Figure 1: (a) At a system-level, automatic metrics (ROUGE-L) and human judgment correlate well, but
(b) the instance-level correlation plot (where each point is a system prediction) shows that the instance-
level correlation is quite low (ρ = 0.31). As a consequence, if we try to locally improve systems to
produce better answers (. in (a)), they do not significantly improve ROUGE scores and vice versa (M).

tasks and systems.1 As predicted by the theory,
we find that the data efficiency depends not only
on the correlation between the human and auto-
matic metrics, but also on the evaluation prompt.
If the automatic metric had perfect correlation, our
data efficiency would be around 3, while if we had
noiseless human judgments, our data efficiency
would be about 1.5. In reality, the reduction in
cost we obtained was only about 10%, suggesting
that improvements in both automatic metric and
evaluation prompt are needed. As one case study
in improving the latter, we show that, when com-
pared to a Likert survey, measuring the amount of
post-editing needed to fix a generated sentence re-
duced the annotator variance by three-fold.

2 Bias in automatic evaluation

It is well understood that current automatic met-
rics tend to correlate poorly with human judg-
ment at the instance-level. For example, Novikova
et al. (2017) report correlations less than 0.3 for
a large suite of word-based and grammar-based
evaluation methods on a generation task. Sim-
ilarly, Liu et al. (2016b) find correlations less
than 0.35 for automatic metrics on a dialog gen-
eration task in one domain, but find correlations
with the same metric dropped significantly to less
than 0.16 when used in another domain. Still,
somewhat surprisingly, several automatic metrics

1An anonymized version of this data and the annota-
tion interfaces used can be found at https://bit.ly/
price-of-debiasing.

have been found to have high system-level correla-
tions (Novikova et al., 2017). What, then, are the
implications of having a low instance-level corre-
lation?

As a case study, consider the task of open-
response question answering: here, a system re-
ceives a human-generated question and must gen-
erate an answer from some given context, e.g. a
document or several webpages. We collected the
responses of several systems on the MS MAR-
COv1 dataset (Nguyen et al., 2016) and crowd-
sourced human evaluations of the system output
(see Section 4 for details).

The instance-level correlation (Figure 1b) is
only ρ = 0.31. A closer look at the instance-level
correlation reveals that while ROUGE is able to
correctly assign low scores to bad examples (lower
left), it is bad at judging good examples and often
assigns them low ROUGE scores (lower right)—
see Table 1 for examples. This observation agrees
with a finding reported in Novikova et al. (2017)
that automatic metrics correlate better with human
judgments on bad examples than average or good
examples.

Thus, as Figure 1(a) shows, we can improve
low-scoring ROUGE examples without improving
their human judgment (M) and vice versa (.). In-
deed, Conroy and Dang (2008) report that sum-
marization systems were optimized for ROUGE
during the DUC challenge (Dang, 2006) until they
were indistinguishable from the ROUGE scores
of human-generated summaries, but the systems



645

Question and reference answer System answer (System; Corr / ROUGE-L)

Examples where system is correct and ROUGE-L > 0.5 (19.6% or 285 of 1455 unique responses)

Q. what is anti-mullerian hormone
A. Anti-Mullerian Hormone (AMH) is a protein hormone
produced by granulosa cells (cells lining the egg sacs or fol-
licles) within the ovary.

it is a protein hormone produced by granulosa cells
(cells lining the egg sacs or follicles) within the ovary.
(snet.ens;X / 0.86)

Examples where system is incorrect and ROUGE-L > 0.5 (1.3% or 19 of 1455 unique responses)

Q. at what gestational age can you feel a fetus move
A. 37 to 41 weeks (incorrect reference answer)

37 to 41 weeks (fastqa, fastqa.ext; × / 1.0)

Examples where system is correct and ROUGE-L < 0.5 (56.0% or 815 of 1455 unique responses)

Q. what is the definition of onomatopoeia
A. It is defined as a word, which imitates the natural sounds
of a thing.

the naming of a thing or action by a vocal imitation of the
sound associated with it (as buzz, hiss). (fastqa;X / 0.23)

Examples where system is incorrect and ROUGE-L < 0.5 (23.1% or 336 of 1455 unique responses)

Q. what kind root stem does a dandelion have
A. Fibrous roots and hollow stem.

vitamin a, vitamin c, vitamin d and vitamin b complex, as
well as zinc, iron and potassium. (snet, snet.ens; × /
0.09)

(a) MS MARCO. Human annotators rated answer correctness (AnyCorrect) and the automatic metric used is ROUGE-L
(higher is better).

Reference summary System summary (System; Edit / VecSim)

Examples where system Edit < 0.3 and VecSim > 0.5 (53.9% or 1078 of 2000 responses)

Bhullar is set to sign a�-day contract with the Kings.
The�-year-old will become the NBA’s first player of
Indian descent. Bhullar will be on the roster when the
Kings host New Orleans Pelicans.

Bhullar andThe Kings are signing Bhullar to a �-day contract.
The �-year-old will be on the roster on friday when David Wear’s
�-season contract expires thursday. Bhullar is set to become the
NBA’s first player of Indian descent. (ml; 0.13 / 0.82)

Examples where system Edit > 0.3 and VecSim > 0.5 (18.0% or 360 of 2000 responses)

The Direct Marketing Commission probing B2C
Data and Data Bubble. Investigating whether they
breached rules on the sale of private data. Chief com-
missioner described allegations made about firms as
‘serious’.

� Data obtained by the Mail’s marketing commission said it
would probe both companies over claims that they had breached
the rules on the sale of private data. The FSA said it would probe
both companies over claims they had breached the rules on the
sale of private data. (se2seq; 1.00 / 0.72)

Examples where system Edit < 0.3 and VecSim < 0.5 (14.5% or 290 of 2000 responses)

Death toll rises to more than �. Pemba Tamang, �,
shows no apparent signs of serious injury after rescue.
Americans special forces helicopter �, including �
Americans, to safety.

Six of Despite Nepal’s tragedy, life triumphed in Kathmandu’s
hard-hit neighborhoods. Rescuers pulled an 15-year-old from the
rubble of a multistory residential building. He was wearing a New
York shirt and a blue neck brace. (pointer; 0.04 / 0.27)

Examples where system Edit > 0.3 and VecSim < 0.5 (13.6% or 272 of 2000 responses)

“Mad Men’s” final seven episodes begin airing April
�. The show has never had high ratings but is con-
sidered one of the great TV series. It’s unknown what
will happen to characters, but we can always guess.

‘This’s “Mad Men” is the end of a series of an era’, This he says.
Stores have created fashion lines inspired by the show.“The So-
pranos”. The in � the Kent State shootings in may � or Richard
Nixonś � re-election.. (ml+rl; 0.95 / 0.24)

(b) CNN/Daily Mail. Human judgment scores used are post-edit distance (Edit) (lower is better) and the automatic metric
used is sentence vector similarity with the reference (higher is better).

Table 1: Examples highlighting the different modes in which the automatic metric and human judgments
may agree or disagree. On the MS MARCO task, a majority of responses from systems were actually
correct but poorly scored according to ROUGE-L. On the CNN/Daily Mail task, a significant number of
examples which are scored highly by VecSim are poorly rated by humans, and likewise many examples
scored poorly by VecSim are highly rated by humans.



646

had hardly improved on human evaluation. Hill-
climbing on ROUGE can also lead to a system
that does worse on human scores, e.g. in machine
translation (Wu et al., 2016). Conversely, genuine
quality improvements might not be reflected in im-
provements in ROUGE. This bias also appears in
pool-based evaluation for knowledge base popula-
tion (Chaganty et al., 2017). Thus the problems
with automatic metrics clearly motivate the need
for human evaluation, but can we still use the au-
tomatic metrics somehow to save costs?

3 Statistical estimation for unbiased
evaluation

We will now formalize the problem of combining
human evaluation with an automatic metric. Let
X be a set of inputs (e.g., articles), and let S be
the system (e.g. for summarization), which takes
x ∈ X and returns output S(x) (e.g. a summary).
Let Z = {(x, S(x)) : x ∈ X} be the set of system
predictions. Let Y (z) be the random variable rep-
resenting the human judgment according to some
evaluation prompt (e.g. grammaticality or correct-
ness), and define f(z) = E[Y (z)] to be the (un-
known) human metric corresponding to averaging
over an infinite number of human judgments. Our
goal is to estimate the average across all examples:

µ
def
= Ez[f(z)] =

1

|Z|
∑

z∈Z
f(z) (1)

with as few queries to Y as possible.
Let g be an automatic metric (e.g. ROUGE),

which maps z to a real number. We assume eval-
uating g(z) is free. The central question is how
to use g in conjunction with calls to Y to produce
an unbiased estimate µ̂ (that is, E[µ̂] = µ). In this
section, we will construct a simple estimator based
on control variates (Ripley, 2009), and prove that
it is minimax optimal.

3.1 Sample mean

We warm up with the most basic unbiased esti-
mate, the sample mean. We sample z(1), . . . , z(n)

independently with replacement from Z . Then,
we sample each human judgment y(i) = Y (z(i))
independently.2 Define the estimator to be
µ̂mean =

1
n

∑n
i=1 y

(i). Note that µ̂mean is unbiased
(E[µ̂mean] = µ).

2Note that this independence assumption isn’t quite true
in practice since we do not control who annotates our data.

We can define σ2f
def
= Var(f(z)) as the variance

of the human metric and σ2a
def
= Ez[Var(Y (z))] as

the variance of human judgment averaged over Z .
By the law of total variance, the variance of our
estimator is

Var(µ̂mean) =
1

n
(σ2f + σ

2
a). (2)

3.2 Control variates estimator
Now let us see how an automatic metric g can re-
duce variance. If there is no annotator variance
(σ2a = 0) so that Y (z) = f(z), we should ex-
pect the variance of f(z) − g(z) to be lower than
the variance of f(z), assuming g is correlated with
f—see Figure 2 for an illustration.

The actual control variates estimator needs to
handle noisy Y (z) (i.e. σ2a > 0) and guard against
a g(z) with low correlation. Let us standardize g
to have zero mean and unit variance, because we
have assumed it is free to evaluate. As before, let
z(1), . . . , z(n) be independent samples from Z and
draw y(i) = Y (z(i)) independently as well. We
define the control variates estimator as

µ̂cv =
1

n

n∑

i=1

y(i) − αg(z(i)), (3)

where

α
def
= Cov(f(z), g(z)). (4)

Intuitively, we have averaged over y(i) to handle
the noise introduced by Y (z), and scaled g(z) to
prevent an uncorrelated automatic metric from in-
troducing too much noise.

An important quantity governing the quality of
an automatic metric g is the correlation between
f(z) and g(z) (recall that g has unit variance):

ρ
def
=

α

σf
. (5)

We can show that among all distributions with
fixed σ2f , σ

2
a, and α (equivalently ρ), this estimator

is minimax optimal, i.e. it has the least variance
among all unbiased estimators:

Theorem 3.1. Among all unbiased estimators that
are functions of y(i) and g(z(i)), and for all distri-
butions with a given σ2f , σ

2
a, and α,

Var(µ̂cv) =
1

n
(σ2f (1− ρ2) + σ2a), (6)

and no other estimator has a lower worst-case
variance.



647

Samples of 𝑓(𝑧) Samples of 𝑓 𝑧 − 𝑔(𝑧)

𝑧

𝑓(𝑧)

𝑔(𝑧)

𝜇

Figure 2: The samples from f(z) have a higher
variance than the samples from f(z) − g(z) but
the same mean. This is the key idea behind using
control variates to reduce variance.

0.00 0.25 0.50 0.75 1.00
Normalized annotator variance (γ)

0.0

0.2

0.4

0.6

0.8

1.0

A
ut

om
at

ic
m

et
ri

c
co

rr
el

at
io

n
(ρ

)

0.0

0.2

0.4

0.6

0.8

1.0
In

ve
rs

e
da

ta
effi

ci
en

cy

Figure 3: Inverse data efficiency for various val-
ues of γ and ρ. We need both low γ and high ρ to
obtain significant gains.

Comparing the variances of the two estimators
((2) and (6)), we define the data efficiency as the
ratio of the variances:

DE def=
Var(µ̂mean)

Var(µ̂cv)
=

1 + γ

1− ρ2 + γ , (7)

where γ def= σ2a/σ
2
f is the normalized annotator

variance. Data efficiency is the key quantity in
this paper: it is the multiplicative reduction in the
number of samples required when using the con-
trol variates estimator µ̂cv versus the sample mean
µ̂mean. Figure 3 shows the inverse data efficiency
contours as a function of the correlation ρ and γ.

When there is no correlation between human
and automatic metrics (ρ = 0), the data efficiency
is naturally 1 (no gain). In order to achieve a
data efficiency of 2 (half the labeling cost), we
need |ρ| ≥

√
2/2 ≈ 0.707. Interestingly, even

for an automatic metric with perfect correlation

(ρ = 1), the data efficiency is still capped by
1+γ
γ : unless γ → 0 the data efficiency cannot in-

crease unboundedly. Intuitively, even if we knew
that ρ = 1, f(z) would be undetermined up to a
constant additive shift and just estimating the shift
would incur a variance of 1nσ

2
a.

3.3 Using the control variates estimator

The control variates estimator can be easily inte-
grated into an existing evaluation: we run human
evaluation on a random sample of system outputs,
automatic evaluation on all the system outputs,
and plug in these results into Algorithm 1.

It is vital that we are able to evaluate the au-
tomatic metric on a significantly larger set of ex-
amples than those with human evaluations to reli-
ably normalize g(z): without these additional ex-
amples, it be can shown that the optimal minimax
estimator for µ is simply the naive estimate µ̂mean.
Intuitively, this is because estimating the mean of
g(z) incurs an equally large variance as estimating
µ. In other words, g(z) is only useful if we have
additional information about g beyond the samples
{z(i)}.

Algorithm 1 shows the estimator. In practice,
we do not know α = Cov(f(z), g(z)), so we use
a plug-in estimate α̂ in line 3 to compute the esti-
mate µ̃ in line 4. We note that estimating α from
data does introduce aO(1/n) bias, but when com-
pared to the standard deviation which decays as
Θ(1/

√
n), this bias quickly goes to 0.

Proposition 3.1. The estimator µ̃ in Algorithm 1
has O(1/n) bias.

Algorithm 1 Control variates estimator

1: Input: n human evaluations y(i) on system
outputs z(i), normalized automatic metric g

2: y = 1n
∑

i y
(i)

3: α̂ = 1n
∑

i(y
(i) − y)g(z(i))

4: µ̃ = 1n
∑

i y
(i) − α̂g(z(i))

5: return µ̃

An additional question that arises when apply-
ing Algorithm 1 is figuring out how many samples
n to use. Given a target variance, the number of
samples can be estimated using (6) with conserva-
tive estimates of σ2f , σ

2
a and ρ. Alternatively, our

estimator can be combined with a dynamic stop-
ping rule (Mnih et al., 2008) to stop data collection
once we reach a target confidence interval.



648

Task Eval. σ2a σ
2
f γ =

σ2a
σ2f

CDM Fluency 0.32 0.26 1.23
CDM Redund. 0.26 0.43 0.61
CDM Overall 0.28 0.28 1.00
CDM Edit 0.07 0.18 0.36

MS MARCO AnyCorr. 0.14 0.15 0.95
MS MARCO AvgCorr. 0.12 0.13 0.91

Table 2: A summary of the key statistics, human
metric variance (σ2f ) and annotator variance (σ

2
a)

for different datasets, CNN/Daily Mail (CDM)
and MS MARCO in our evaluation benchmark.
We observe that the relative variance (γ) is fairly
high for most evaluation prompts, upper bounding
the data efficiency on these tasks. A notable ex-
ception is the Edit prompt wherein systems are
compared on the number of post-edits required to
improve their quality.

3.4 Discussion of assumptions

We will soon see that empirical instantiations of γ
and ρ lead to rather underwhelming data efficien-
cies in practice. In light of our optimality result,
does this mean there is no hope for gains? Let us
probe our assumptions. We assumed that the hu-
man judgments are uncorrelated across different
system outputs; it is possible that a more accurate
model of human annotators (e.g. Passonneau and
Carpenter (2014)) could offer improvements. Per-
haps with additional information about g(z) such
as calibrated confidence estimates, we would be
able to sample more adaptively. Of course the
most direct routes to improvement involve increas-
ing the correlation of g with human judgments and
reducing annotator variance, which we will dis-
cuss more later.

4 Tasks and datasets

In order to compare different approaches to evalu-
ating systems, we first collected human judgments
for the output of several automatic summariza-
tion and open-response question answering sys-
tems using Amazon Mechanical Turk. Details of
instructions provided and quality assurance steps
taken are provided in Appendix A of the supple-
mentary material. In this section, we’ll briefly de-
scribe how we collected this data.

Evaluating language quality in automatic sum-
marization. In automatic summarization, sys-
tems must generate a short (on average two or
three sentence) summary of an article: for our
study, we chose articles from the CNN/Daily Mail
(CDM) dataset (Hermann et al., 2015; Nallapati
et al., 2016) which come paired with reference
summaries in the form of story highlights. We
focus on the language quality of summaries and
leave evaluating content selection to future work.

For each summary, we collected human judg-
ments on a scale from 1–3 (Figure 4a) for flu-
ency, (lack of) redundancy, and overall quality of
the summary using guidelines from the DUC sum-
marization challenge (Dang, 2006). As an alter-
nate human metric, we also asked workers to post-
edit the system’s summary to improve its qual-
ity, similar to the post-editing step in MT evalu-
ations (Snover et al., 2006). Obtaining judgments
costs about $0.15 per summary and this cost rises
to about $0.40 per summary for post-editing.

We collected judgments on the summaries gen-
erated by the seq2seq and pointer models
of See et al. (2017), the ml and ml+rl mod-
els of Paulus et al. (2018), and the reference
summaries.3 Before presenting the summaries to
human annotators, we performed some minimal
post-processing: we true-cased and de-tokenized
the output of seq2seq and pointer using
Stanford CoreNLP (Manning et al., 2014) and re-
placed “unknown” tokens in each system with a
special symbol (�).

Evaluating answer correctness. Next, we look
at evaluating the correctness of system outputs
in question answering using the MS MARCO
question answering dataset (Nguyen et al., 2016).
Here, each system is provided with a question and
up to 10 paragraphs of context. The system gener-
ates open-response answers that do not need to be
tied to a span in any paragraph.

We first ask annotators to judge if the output
is even plausible for the question, and if yes, ask
them identify if it is correct according to each con-
text paragraph. We found that requiring annotators
to highlight regions in the text that support their
decision substantially improved the quality of the
output without increasing costs. Annotations cost
$0.40 per system response.4

3All system output was obtained from the original authors
through private communication.

4This cost could be significantly reduced if systems also



649

(a) Interface to evaluate language quality on CNN/Daily
Mail

(b) Interface to judge answer correctness on MS MARCO

Figure 4: Screenshots of the annotation interfaces we used to measure (a) summary language quality on
CNN/Daily Mail and (b) answer correctness on MS MARCO tasks.

While our goal is to evaluate the correctness of
the provided answer, we found that there are of-
ten answers which may be correct or incorrect de-
pending on the context. For example, the question
“what is a pothole” is typically understood to refer
to a hole in a roadway, but also refers to a geo-
logical feature (Figure 4b). This is reflected when
annotators mark one context paragraph to support
the given answer but mark another to contradict it.
We evaluated systems based on both the average
correctness (AvgCorrect) of their answers across
all paragraphs as well as whether their answer is
correct according to any paragraph (AnyCorrect).

We collected annotations on the systems gen-
erated by the fastqa and fastqa ext from
Weissenborn et al. (2017) and the snet and
snet.ens(emble) models from Tan et al. (2018),
along with reference answers. The answers gener-
ated by the systems were used without any post-
processing. Surprisingly, we found that the cor-
rectness of the reference answers (according to
the AnyCorrect metric) was only 73.5%, only 2%
above that of the leading system (snet.ens).
We manually inspected 30 reference answers
which were annotated incorrectly and found that
of those, about 95% were indeed incorrect. How-
ever, 62% are actually answerable from some
paragraph, indicating that the real ceiling perfor-
mance on this dataset is around 90% and that there
is still room for improvement on this task.

5 Experimental results

We are now ready to evaluate the performance
of our control variates estimator proposed in Sec-
tion 3 using the datasets presented in Section 4.

specify which passage they used to generate the answer.

Recall that our primary quantity of interest is data
efficiency, the ratio of the number of human judg-
ments required to estimate the overall human eval-
uation score for the control variates estimator ver-
sus the sample mean. We’ll briefly review the au-
tomatic metrics used in our evaluation before ana-
lyzing the results.

Automatic metrics. We consider the follow-
ing frequently used automatic word-overlap based
metrics in our work: BLEU (Papineni et al.,
2002), ROUGE (Lin and Rey, 2004) and ME-
TEOR (Lavie and Denkowski, 2009). Following
Novikova et al. (2017) and Liu et al. (2016b), we
also compared a vector-based sentence-similarity
using sent2vec (Pagliardini et al., 2017) to
compare sentences (VecSim). Figure 5 shows how
each of these metrics is correlated with human
judgment for the systems being evaluated. Un-
surprisingly, the correlation varies considerably
across systems, with token-based metrics correlat-
ing more strongly for systems that are more ex-
tractive in nature (fastqa and fastqa ext).

Results.5 In Section 3 we proved that the con-
trol variates estimator is not only unbiased but also
has the least variance among other unbiased esti-
mators. Figure 6 plots the width of the 80% con-
fidence interval, estimated using bootstrap, mea-
sured as a function of the number of samples col-
lected for different tasks and prompts. As ex-
pected, the control variates estimator reduces the
width of the confidence interval. We measure data
efficiency by the averaging of the ratio of squared
confidence intervals between the human baseline

5Extended results for other systems, metrics
and prompts can be found at https://bit.ly/
price-of-debiasing/.



650

fa
stq

a

fa
stq

a
ex

t
sn

et

sn
et

.en
s Al

l

Systems

ROUGE-L

ROUGE-1

ROUGE-2

METEOR

BLEU-2

VecSim
M

et
ri

cs

0.10

0.15

0.20

0.25

0.30

0.35

0.40

0.45

0.50

P
ea

rs
on
ρ

(a) MS MARCO with the AnyCorrect prompt

se
q2

se
q

po
int

er m
l

m
l+

rl Al
l

Systems

ROUGE-L

ROUGE-1

ROUGE-2

METEOR

BLEU-2

VecSim

M
et

ri
cs

0.10

0.15

0.20

0.25

0.30

0.35

0.40

0.45

0.50

P
ea

rs
on
ρ

(b) CNN/Daily Mail with the Edit prompt

Figure 5: Correlations of different automatic metrics on the MS MARCO and CNN/Daily Mail tasks.
Certain systems are more correlated with certain automatic metrics than others, but overall the correlation
is low to moderate for most systems and metrics.

and control variates estimates. We observe that the
data efficiency depends on the task, prompt and
system, ranging from about 1.08 (a 7% cost reduc-
tion) to 1.15 (a 13% cost reduction) using current
automatic metrics.

As we showed in Section 3, further gains are
fundamentally limited by the quality of the evalu-
ation prompts and automatic metrics. Figures 6a
and 6b show how improving the quality of the
evaluation prompt from a Likert-scale prompt for
quality (Overall) to using post-editing (Edit)
noticeably decreases variance and hence allows
better automatic metrics to increase data effi-
ciency. Likewise, Figure 6c shows how using
a better automatic metric (ROUGE-L instead of
VecSim) also reduces variance.

Figure 6 also shows the conjectured confidence
intervals if we were able to eliminate noise in hu-
man judgments (noiseless humans) or have a au-
tomatic metric that correlated perfectly with aver-
age human judgment (perfect metric). In particu-
lar, we use the mean of all (2–3) humans on each
z for the perfect g(z) and use the mean of all hu-
mans on each z for the “noiseless” Y (z).

In both cases, we are able to significantly in-
crease data efficiency (i.e. decrease estimator vari-
ance). With zero annotator variance and using ex-
isting automatic metrics, the data efficiency ranges
from 1.42 to 1.69. With automatic metrics with
perfect correlation and current variance of human
judgments, it ranges from 2.38 to 7.25. Thus,
we conclude that it is important not only to im-
prove our automatic metrics but also the evalua-
tion prompts we use during human evaluation.

6 Related work

In this work, we focus on using existing automatic
metrics to decrease the cost of human evaluations.
There has been much work on improving the qual-
ity of automatic metrics. In particular, there is
interest in learning models (Lowe et al., 2017a;
Dusek et al., 2017) that are able to optimize for im-
proved correlations with human judgment. How-
ever, in our experience, we have found that these
learned automatic metrics have trouble generaliz-
ing to different systems. The framework we pro-
vide allows us to safely incorporate such models
into evaluation, exploiting them when their corre-
lation is high but also not introducing bias when it
is low.

Our key technical tool is control variates, a stan-
dard statistical technique used to reduce the vari-
ance of Monte Carlo estimates (Ripley, 2009).
The technique has also been used in machine
learning and reinforcement learning to lower vari-
ance estimates of gradients (Greensmith et al.,
2004; Paisley et al., 2012; Ranganath et al., 2014).
To the best of our knowledge, we are the first to ap-
ply this technique in the context of language eval-
uation.

Our work also highlights the importance of hu-
man evaluation. Chaganty et al. (2017) identified
a similar problem of systematic bias in evaluation
metrics in the setting of knowledge base popula-
tion and also propose statistical estimators that re-
lies on human evaluation to correct bias. Unfortu-
nately, their technique relies on having a structured
output (relation triples) that are shared between



651

0 100 200 300 400 500
Number of samples

0.06

0.08

0.10

0.12

0.14

0.16

0.18

0.20

80
%

co
nfi

de
nc

e
in

te
rv

al
Humans

Humans + VecSim

Noiseless humans + VecSim

Humans + perfect metric

(a) seq2seq on CNN/Daily Mail using
the Overall

0 100 200 300 400 500
Number of samples

0.06

0.08

0.10

0.12

0.14

0.16

0.18

0.20

80
%

co
nfi

de
nc

e
in

te
rv

al

Humans

Humans + VecSim

Noiseless humans + VecSim

Humans + perfect metric

(b) seq2seq on CNN/Daily Mail using
Edit

0 100 200 300 400 500
Number of samples

0.050

0.075

0.100

0.125

0.150

0.175

0.200

80
%

co
nfi

de
nc

e
in

te
rv

al

Humans

Humans + VecSim

Humans + ROUGE-1

Humans + perfect metric

(c) fastqa ext on MS MARCO using
AnyCorrect

Figure 6: 80% bootstrap confidence interval length as a function of the number of human judgments
used when evaluating the indicated systems on their respective datasets and prompts. (a) We see a modest
reduction in variance (and hence cost) relative to human evaluation by using the VecSim automatic metric
with the proposed control variates estimator to estimate Overall scores on the CNN/Daily Mail task;
the data efficiency (DE) is 1.06. (b) By improving the evaluation prompt to use Edits instead, it is
possible to further reduce variance relative to humans (DE is 1.15). (c) Another way to reduce variance
relative to humans is to improve the automatic metric evaluation; here using ROUGE-1 instead of VecSim
improves the DE from 1.03 to 1.16.

systems and does not apply to evaluating natu-
ral language generation. In a similar vein, Chang
et al. (2017) dynamically collect human feedback
to learn better dialog policies.

7 Discussion

Prior work has shown that existing automatic
metrics have poor instance-level correlation with
mean human judgment and that they score many
good quality responses poorly. As a result, the
evaluation is systematically biased against genuine
system improvements that would lead to higher
human evaluation scores but not improve auto-
matic metrics. In this paper, we have explored us-
ing an automatic metric to decrease the cost of hu-
man evaluation without introducing bias. In prac-
tice, we find that with current automatic metrics
and evaluation prompts data efficiencies are only
1.08–1.15 (7–13% cost reduction). Our theory
shows that further improvements are only possi-
ble by improving the correlation of the automatic
metric and reducing the annotator variance of the
evaluation prompt. As an example of how evalu-
ation prompts could be improved, we found that
using post-edits of summarizes decreased normal-
ized annotator variance by a factor of three relative
to using a Likert scale survey. It should be noted
that changing the evaluation prompt also changes
the underlying ground truth f(z): it is up to us
to find a prompt that still captures the essence of
what we want to measure.

Without making stronger assumptions, the con-
trol variates estimator we proposed outlines the
limitations of unbiased estimation. Where do we
go from here? Certainly, we can try to improve
the automatic metric (which is potentially as diffi-
cult as solving the task) and brainstorming alterna-
tive ways of soliciting evaluation (which has been
less explored). Alternatively, we could give up on
measuring absolute scores, and seek instead to find
techniques stably rank methods and thus improve
them. As the NLP community tackles increasingly
difficult tasks, human evaluation will only become
more important. We hope our work provides some
clarity on to how to make it more cost effective.

Reproducibility

All code, data, and experiments for this paper are
available on the CodaLab platform at https://
bit.ly/price-of-debiasing.

Acknowledgments

We are extremely grateful to the authors of the
systems we evaluated for sharing their systems’
output with us. We also would like to thank Ur-
vashi Khandelwal and Peng Qi for feedback on
an earlier draft of the paper, the crowdworkers
on Amazon Mechanical Turk and TurkNation for
their work and feedback during the data collection
process, and the anonymous reviewers for their
constructive feedback.



652

References
A. Chaganty, A. Paranjape, P. Liang, and C. Man-

ning. 2017. Importance sampling for unbiased on-
demand evaluation of knowledge base population.
In Empirical Methods in Natural Language Process-
ing (EMNLP).

C. Chang, R. Yang, L. Chen, X. Zhou, and K. Yu.
2017. Affordable on-line dialogue policy learning.
In Empirical Methods in Natural Language Process-
ing (EMNLP). pages 223–231.

J. M. Conroy and H. T. Dang. 2008. Mind the gap :
Dangers of divorcing evaluations of summary con-
tent from linguistic quality. In International Con-
ference on Computational Linguistics (COLING).
pages 145–152.

H. T. Dang. 2006. Overview of DUC 2006. In Docu-
ment Understanding Conference.

M. Denkowski and A. Lavie. 2014. Meteor universal:
Language specific translation evaluation for any tar-
get language. In Workshop on Statistical Machine
Translation.

O. Dusek, J. Novikova, and V. Rieser. 2017. Refer-
enceless quality estimation for natural language gen-
eration. arXiv .

E. Greensmith, P. L. Bartlett, and J. Baxter. 2004. Vari-
ance reduction techniques for gradient estimates in
reinforcement learning. Journal of Machine Learn-
ing Research (JMLR) 5:1471–1530.

K. M. Hermann, T. Koisk, E. Grefenstette, L. Espe-
holt, W. Kay, M. Suleyman, and P. Blunsom. 2015.
Teaching machines to read and comprehend. In Ad-
vances in Neural Information Processing Systems
(NIPS).

T. Kočisky, J. Schwarz, P. Blunsom, C. Dyer, K. M.
Hermann, G. Melis, and E. Grefenstette. 2017.
The NarrativeQA reading comprehension challenge.
arXiv preprint arXiv:1712.07040 .

A. Lavie and M. Denkowski. 2009. The meteor met-
ric for automatic evaluation of machine translation.
Machine Translation 23.

C. Lin and M. Rey. 2004. Looking for a few good met-
rics: ROUGE and its evaluation. In NTCIR Work-
shop.

T. Lin, M. Maire, S. Belongie, J. Hays, P. Perona,
D. Ramanan, P. Doll’ar, and C. L. Zitnick. 2014.
Microsoft COCO: Common objects in context. In
European Conference on Computer Vision (ECCV).
pages 740–755.

A. Liu, S. Soderland, J. Bragg, C. H. Lin, X. Ling, and
D. S. Weld. 2016a. Effective crowd annotation for
relation extraction. In North American Association
for Computational Linguistics (NAACL). pages 897–
906.

C. Liu, R. Lowe, I. V. Serban, M. Noseworthy, L. Char-
lin, and J. Pineau. 2016b. How NOT to evaluate
your dialogue system: An empirical study of un-
supervised evaluation metrics for dialogue response
generation. In Empirical Methods in Natural Lan-
guage Processing (EMNLP).

R. Lowe, M. Noseworthy, I. V. Serban, N. Angelard-
Gontier, Y. Bengio, and J. Pineau. 2017a. Towards
an automatic turing test: Learning to evaluate dia-
logue responses. In Association for Computational
Linguistics (ACL).

R. T. Lowe, N. Pow, I. Serban, L. Charlin, C. Liu, and
J. Pineau. 2017b. Training end-to-end dialogue sys-
tems with the ubuntu dialogue corpus. Dialogue and
Discourse 8.

C. D. Manning, M. Surdeanu, J. Bauer, J. Finkel, S. J.
Bethard, and D. McClosky. 2014. The stanford
coreNLP natural language processing toolkit. In
ACL system demonstrations.

V. Mnih, C. Szepesv’ari, and J. Audibert. 2008. Empir-
ical berstein stopping. In International Conference
on Machine Learning (ICML).

R. Nallapati, B. Zhou, C. Gulcehre, B. Xiang,
et al. 2016. Abstractive text summarization us-
ing sequence-to-sequence rnns and beyond. arXiv
preprint arXiv:1602.06023 .

T. Nguyen, M. Rosenberg, X. Song, J. Gao, S. Tiwary,
R. Majumder, and L. Deng. 2016. MS MARCO:
A human generated machine reading comprehension
dataset. In Workshop on Cognitive Computing at
NIPS.

J. Novikova, O. Duek, A. C. Curry, and V. Rieser. 2017.
Why we need new evaluation metrics for NLG. In
Empirical Methods in Natural Language Processing
(EMNLP).

M. Pagliardini, P. Gupta, and M. Jaggi. 2017. Unsuper-
vised learning of sentence embeddings using com-
positional n-gram features. arXiv .

J. Paisley, D. M. Blei, and M. I. Jordan. 2012. Vari-
ational Bayesian inference with stochastic search.
In International Conference on Machine Learning
(ICML). pages 1363–1370.

K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
BLEU: A method for automatic evaluation of ma-
chine translation. In Association for Computational
Linguistics (ACL).

R. J. Passonneau and B. Carpenter. 2014. The benefits
of a model of annotation. In Association for Com-
putational Linguistics (ACL).

R. Paulus, C. Xiong, and R. Socher. 2018. A deep re-
inforced model for abstractive summarization. In
International Conference on Learning Representa-
tions (ICLR).



653

R. Ranganath, S. Gerrish, and D. Blei. 2014. Black
box variational inference. In Artificial Intelligence
and Statistics (AISTATS). pages 814–822.

B. D. Ripley. 2009. Stochastic simulation. John Wiley
& Sons.

A. See, P. J. Liu, and C. D. Manning. 2017. Get to the
point: Summarization with pointer-generator net-
works. In Association for Computational Linguis-
tics (ACL).

M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate
with targeted human annotation. In Association for
Machine Translation in the Americas. pages 223–
231.

C. Tan, F. Wei, N. Yang, W. Lv, and M. Zhou. 2018.
S-Net: From answer extraction to answer genera-
tion for machine reading comprehension. In Associ-
ation for the Advancement of Artificial Intelligence
(AAAI).

R. Vedantam, C. L. Zitnick, and D. Parikh. 2015.
CIDEr: Consensus-based image description evalu-
ation. In Computer Vision and Pattern Recognition
(CVPR). pages 4566–4575.

D. Weissenborn, G. Wiese, and L. Seiffe. 2017. Mak-
ing neural QA as simple as possible but not sim-
pler. In Computational Natural Language Learning
(CoNLL).

Y. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi,
W. Macherey, M. Krikun, Y. Cao, Q. Gao,
K. Macherey, et al. 2016. Google’s neural ma-
chine translation system: Bridging the gap between
human and machine translation. arXiv preprint
arXiv:1609.08144 .


