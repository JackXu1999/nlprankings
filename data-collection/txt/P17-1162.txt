



















































Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics


Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1766–1776
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1162

Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1766–1776
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1162

Learning Symmetric Collaborative Dialogue Agents with Dynamic
Knowledge Graph Embeddings

He He and Anusha Balakrishnan and Mihail Eric and Percy Liang
Computer Science Department, Stanford University

{hehe,anusha28,meric,pliang}@cs.stanford.edu

Abstract

We study a symmetric collaborative dia-
logue setting in which two agents, each
with private knowledge, must strategically
communicate to achieve a common goal.
The open-ended dialogue state in this set-
ting poses new challenges for existing di-
alogue systems. We collected a dataset
of 11K human-human dialogues, which
exhibits interesting lexical, semantic, and
strategic elements. To model both struc-
tured knowledge and unstructured lan-
guage, we propose a neural model with dy-
namic knowledge graph embeddings that
evolve as the dialogue progresses. Au-
tomatic and human evaluations show that
our model is both more effective at achiev-
ing the goal and more human-like than
baseline neural and rule-based models.

1 Introduction

Current task-oriented dialogue systems (Young
et al., 2013; Wen et al., 2017; Dhingra et al., 2017)
require a pre-defined dialogue state (e.g., slots
such as food type and price range for a restau-
rant searching task) and a fixed set of dialogue acts
(e.g., request, inform). However, human conversa-
tion often requires richer dialogue states and more
nuanced, pragmatic dialogue acts. Recent open-
domain chat systems (Shang et al., 2015; Serban
et al., 2015b; Sordoni et al., 2015; Li et al., 2016a;
Lowe et al., 2017; Mei et al., 2017) learn a map-
ping directly from previous utterances to the next
utterance. While these models capture open-ended
aspects of dialogue, the lack of structured dialogue
state prevents them from being directly applied
to settings that require interfacing with structured
knowledge.

In order to bridge the gap between the two types

Friends of agent A:

Name School Major Company

Jessica Columbia Computer Science Google
Josh Columbia Linguistics Google
... ... ... ...

A: Hi! Most of my friends work for Google
B: do you have anyone who went to columbia?
A: Hello?
A: I have Jessica a friend of mine
A: and Josh, both went to columbia
B: or anyone working at apple?
B: SELECT (Jessica, Columbia, Computer Science, Google)
A: SELECT (Jessica, Columbia, Computer Science, Google)

Figure 1: An example dialogue from the Mutual-
Friends task in which two agents, A and B, each
given a private list of a friends, try to identify their
mutual friend. Our objective is to build an agent
that can perform the task with a human. Cross-
talk (Section 2.3) is italicized.

of systems, we focus on a symmetric collabora-
tive dialogue setting, which is task-oriented but
encourages open-ended dialogue acts. In our set-
ting, two agents, each with a private list of items
with attributes, must communicate to identify the
unique shared item. Consider the dialogue in Fig-
ure 1, in which two people are trying to find their
mutual friend. By asking “do you have anyone
who went to columbia?”, B is suggesting that she
has some Columbia friends, and that they probably
work at Google. Such conversational implicature
is lost when interpreting the utterance as simply
an information request. In addition, it is hard to
define a structured state that captures the diverse
semantics in many utterances (e.g., defining “most
of”, “might be”; see details in Table 1).

To model both structured and open-ended con-
text, we propose the Dynamic Knowledge Graph
Network (DynoNet), in which the dialogue state is
modeled as a knowledge graph with an embedding

1766

https://doi.org/10.18653/v1/P17-1162
https://doi.org/10.18653/v1/P17-1162


for each node (Section 3). Our model is similar
to EntNet (Henaff et al., 2017) in that node/entity
embeddings are updated recurrently given new
utterances. The difference is that we structure
entities as a knowledge graph; as the dialogue
proceeds, new nodes are added and new context
is propagated on the graph. An attention-based
mechanism (Bahdanau et al., 2015) over the node
embeddings drives generation of new utterances.
Our model’s use of knowledge graphs captures the
grounding capability of classic task-oriented sys-
tems and the graph embedding provides the repre-
sentational flexibility of neural models.

The naturalness of communication in the sym-
metric collaborative setting enables large-scale
data collection: We were able to crowdsource
around 11K human-human dialogues on Amazon
Mechanical Turk (AMT) in less than 15 hours.1

We show that the new dataset calls for more flex-
ible representations beyond fully-structured states
(Section 2.2).

In addition to conducting the third-party human
evaluation adopted by most work (Liu et al., 2016;
Li et al., 2016b,c), we also conduct partner evalu-
ation (Wen et al., 2017) where AMT workers rate
their conversational partners (other workers or our
models) based on fluency, correctness, coopera-
tion, and human-likeness. We compare DynoNet
with baseline neural models and a strong rule-
based system. The results show that DynoNet can
perform the task with humans efficiently and nat-
urally; it also captures some strategic aspects of
human-human dialogues.

The contributions of this work are: (i) a new
symmetric collaborative dialogue setting and a
large dialogue corpus that pushes the boundaries
of existing dialogue systems; (ii) DynoNet, which
integrates semantically rich utterances with struc-
tured knowledge to represent open-ended dialogue
states; (iii) multiple automatic metrics based on
bot-bot chat and a comparison of third-party and
partner evaluation.

2 Symmetric Collaborative Dialogue

We begin by introducing a collaborative task be-
tween two agents and describe the human-human
dialogue collection process. We show that our data
exhibits diverse, interesting language phenomena.

1The dataset is available publicly at https://
stanfordnlp.github.io/cocoa/.

2.1 Task Definition

In the symmetric collaborative dialogue setting,
there are two agents, A and B, each with a pri-
vate knowledge base—KBA and KBB, respec-
tively. Each knowledge base includes a list of
items, where each item has a value for each at-
tribute. For example, in the MutualFriends set-
ting, Figure 1, items are friends and attributes are
name, school, etc. There is a shared item that A
and B both have; their goal is to converse with
each other to determine the shared item and select
it. Formally, an agent is a mapping from its pri-
vate KB and the dialogue thus far (sequence of ut-
terances) to the next utterance to generate or a se-
lection. A dialogue is considered successful when
both agents correctly select the shared item. This
setting has parallels in human-computer collabora-
tion where each agent has complementary exper-
tise.

2.2 Data collection

We created a schema with 7 attributes and approx-
imately 3K entities (attribute values). To elicit lin-
guistic and strategic variants, we generate a ran-
dom scenario for each task by varying the num-
ber of items (5 to 12), the number attributes (3 or
4), and the distribution of values for each attribute
(skewed to uniform). See Appendix A and B for
details of schema and scenario generation.

Figure 2: Screenshot of the chat interface.

We crowdsourced dialogues on AMT by ran-
domly pairing up workers to perform the task
within 5 minutes.2 Our chat interface is shown in
Figure 2. To discourage random guessing, we pre-
vent workers from selecting more than once every
10 seconds. Our task was very popular and we col-

2If the workers exceed the time limit, the dialogue is
marked as unsuccessful (but still logged).

1767



Type % Easy example Hard example

Inform 30.4 I know a judy. / I have someone whostudied the bible in the afternoon.
About equal indoor and outdoor friends / me too. his
major is forestry / might be kelly

Ask 17.7 Do any of them like Poi? / What does your henrydo?
What can you tell me about our friend? / Or maybe
north park college?

Answer 7.4 None of mine did / Yup / They do. / Same here. yes 3 of them / No he likes poi / yes if boston college

Table 1: Main utterance types and examples. We show both standard utterances whose meaning can
be represented by simple logical forms (e.g., ask(indoor)), and open-ended ones which require more
complex logical forms (difficult parts in bold). Text spans corresponding to entities are underlined.

Phenomenon Example

Coreference (I know one Debra) does she like the indoors? / (I have two friends named TIffany) at World airways?
Coordination keep on going with the fashion / Ok. let’s try something else. / go by hobby / great. select him. thanks!

Chit-chat Yes, that is good ole Terry. / All indoorsers! my friends hate nature
Categorization same, most of mine are female too / Does any of them names start with B

Correction I know one friend into Embroidery - her name is Emily. Sorry – Embroidery friend is named Michelle

Table 2: Communication phenomena in the dataset. Evident parts is in bold and text spans corresponding
to an entity are underlined. For coreference, the antecedent is in parentheses.

lected 11K dialogues over a period of 13.5 hours.3

Of these, over 9K dialogues are successful. Un-
successful dialogues are usually the result of either
worker leaving the chat prematurely.

2.3 Dataset statistics
We show the basic statistics of our dataset in Ta-
ble 3. An utterance is defined as a message sent
by one of the agents. The average utterance length
is short due to the informality of the chat, how-
ever, an agent usually sends multiple utterances in
one turn. Some example dialogues are shown in
Table 6 and Appendix I.

# dialogues 11157
# completed dialogues 9041
Vocabulary size 5325
Average # of utterances 11.41
Average time taken per task (sec.) 91.18
Average utterance length (tokens) 5.08
Number of linguistic templates4 41561

Table 3: Statistics of the MutualFriends dataset.

We categorize utterances into coarse types—
inform, ask, answer, greeting, apology—by pattern
matching (Appendix E). There are 7.4% multi-
type utterances, and 30.9% utterances contain
more than one entity. In Table 1, we show exam-
ple utterances with rich semantics that cannot be
sufficiently represented by traditional slot-values.

3Tasks are put up in batches; the total time excludes inter-
vals between batches.

4Entity names are replaced by their entity types.

Some of the standard ones are also non-trivial due
to coreference and logical compositionality.

Our dataset also exhibits some interesting com-
munication phenomena. Coreference occurs fre-
quently when people check multiple attributes
of one item. Sometimes mentions are dropped,
as an utterance simply continues from the part-
ner’s utterance. People occasionally use exter-
nal knowledge to group items with out-of-schema
attributes (e.g., gender based on names, location
based on schools). We summarize these phenom-
ena in Table 2. In addition, we find 30% utter-
ances involve cross-talk where the conversation
does not progress linearly (e.g., italic utterances
in Figure 1), a common characteristic of online
chat (Ivanovic, 2005).

One strategic aspect of this task is choosing the
order of attributes to mention. We find that people
tend to start from attributes with fewer unique val-
ues, e.g., “all my friends like morning” given the
KBB in Table 6, as intuitively it would help ex-
clude items quickly given fewer values to check.5

We provide a more detailed analysis of strategy in
Section 4.2 and Appendix F.

3 Dynamic Knowledge Graph Network

The diverse semantics in our data motivates us
to combine unstructured representation of the di-
alogue history with structured knowledge. Our

5Our goal is to model human behavior thus we do not dis-
cuss the optimal strategy here.

1768



B: anyone went to columbia?

columbia

google

KB + Dialogue history

Dynamic knowledge graph Graph 
embedding

Generator

Name School Company
Jessica Columbia Google

Josh Columbia Google
Item 1
Item 2

2

1

josh

jessica

S

N

C

Message passing path of columbia

anyone went columbia

……

columbia

google

jessica
josh

… …

Yes and joshjessica

Attention + Copy

Figure 3: Overview of our approach. First, the KB and dialogue history (entities in bold) is mapped to
a graph. Here, an item node is labeled by the item ID and an attribute node is labeled by the attribute’s
first letter. Next, each node is embedded using relevant utterance embeddings through message passing.
Finally, an LSTM generates the next utterance based on attention over the node embeddings.

model consists of three components shown in Fig-
ure 3: (i) a dynamic knowledge graph, which rep-
resents the agent’s private KB and shared dialogue
history as a graph (Section 3.1), (ii) a graph em-
bedding over the nodes (Section 3.2), and (iii) an
utterance generator (Section 3.3).

The knowledge graph represents entities and re-
lations in the agent’s private KB, e.g., item-1’s
company is google. As the conversation unfolds,
utterances are embedded and incorporated into
node embeddings of mentioned entities. For in-
stance, in Figure 3, “anyone went to columbia”
updates the embedding of columbia. Next, each
node recursively passes its embedding to neigh-
boring nodes so that related entities (e.g., those
in the same row or column) also receive informa-
tion from the most recent utterance. In our exam-
ple, jessica and josh both receive new context
when columbia is mentioned. Finally, the utter-
ance generator, an LSTM, produces the next utter-
ance by attending to the node embeddings.

3.1 Knowledge Graph

Given a dialogue of T utterances, we construct
graphs (Gt)Tt=1 over the KB and dialogue history
for agent A.6 There are three types of nodes: item
nodes, attribute nodes, and entity nodes. Edges
between nodes represent their relations. For ex-
ample, (item-1, hasSchool, columbia) means
that the first item has attribute school whose value

6 It is important to differentiate perspectives of the two
agents as they have different KBs. Thereafter we assume the
perspective of agent A, i.e., accessing KBA for A only, and
refer to B as the partner.

is columbia. An example graph is shown in Fig-
ure 3. The graph Gt is updated based on utterance
t by taking Gt�1 and adding a new node for any
entity mentioned in utterance t but not in KBA.7

3.2 Graph Embedding
Given a knowledge graph, we are interested in
computing a vector representation for each node
v that captures both its unstructured context from
the dialogue history and its structured context in
the KB. A node embedding Vt(v) for each node
v 2 Gt is built from three parts: structural prop-
erties of an entity defined by the KB, embeddings
of utterances in the dialogue history, and message
passing between neighboring nodes.

Node Features. Simple structural properties of
the KB often govern what is talked about; e.g.,
a high-frequency entity is usually interesting to
mention (consider “All my friends like dancing.”).
We represent this type of information as a fea-
ture vector Ft(v), which includes the degree and
type (item, attribute, or entity type) of node v, and
whether it has been mentioned in the current turn.
Each feature is encoded as a one-hot vector and
they are concatenated to form Ft(v).

Mention Vectors. A mention vector Mt(v) con-
tains unstructured context from utterances relevant
to node v up to turn t. To compute it, we first de-
fine the utterance representation ũt and the set of
relevant entities Et. Let ut be the embedding of
utterance t (Section 3.3). To differentiate between

7 We use a rule-based lexicon to link text spans to entities.
See details in Appendix D.

1769



the agent’s and the partner’s utterances, we repre-
sent it as ũt =

⇥
ut · {ut2Uself}, ut · {ut2Upartner}

⇤
,

where Uself and Upartner denote sets of utterances
generated by the agent and the partner, and [·, ·]
denotes concatenation. Let Et be the set of entity
nodes mentioned in utterance t if utterance t men-
tions some entities, or utterance t � 1 otherwise.8
The mention vector Mt(v) of node v incorporates
the current utterance if v is mentioned and inherits
Mt�1(v) if not:

Mt(v) = �tMt�1(v) + (1 � �t)ũt; (1)

�t =

(
�
�
W inc [Mt�1(v), ũt]

�
if v 2 Et,

1 otherwise.

Here, � is the sigmoid function and W inc is a pa-
rameter matrix.

Recursive Node Embeddings. We propagate
information between nodes according to the struc-
ture of the knowledge graph. In Figure 3, given
“anyone went to columbia?”, the agent should fo-
cus on her friends who went to Columbia Univer-
sity. Therefore, we want this utterance to be sent
to item nodes connected to columbia, and one step
further to other attributes of these items because
they might be mentioned next as relevant informa-
tion, e.g., jessica and josh.

We compute the node embeddings recursively,
analogous to belief propagation:

V kt (v) = max
v02Nt(v)

tanh (2)
⇣
W mp

h
V k�1t (v

0), R(ev!v0)
i⌘

,

where V kt (v) is the depth-k node embedding at
turn t and Nt(v) denotes the set of nodes adjacent
to v. The message from a neighboring node v0 de-
pends on its embedding at depth-(k � 1), the edge
label ev!v0 (embedded by a relation embedding
function R), and a parameter matrix W mp. Mes-
sages from all neighbors are aggregated by max,
the element-wise max operation.9 Example mes-
sage passing paths are shown in Figure 3.

The final node embedding is the concatenation
of embeddings at each depth:

Vt(v) =
⇥
V 0t (v), . . . , V

K
t (v)

⇤
, (3)

where K is a hyperparameter (we experiment with
K 2 {0, 1, 2}) and V 0t (v) = [Ft(v), Mt(v)].

8 Relying on utterance t � 1 is useful when utterance t
answers a question, e.g., “do you have any google friends?”
“No.”

9Using sum or mean slightly hurts performance.

3.3 Utterance Embedding and Generation
We embed and generate utterances using Long
Short Term Memory (LSTM) networks that take
the graph embeddings into account.

Embedding. On turn t, upon receiving an
utterance consisting of nt tokens, xt =
(xt,1, . . . , xt,nt), the LSTM maps it to a vector as
follows:

ht,j = LSTMenc(ht,j�1, At(xt,j)), (4)

where ht,0 = ht�1,nt�1 , and At is an entity ab-
straction function, explained below. The final hid-
den state ht,nt is used as the utterance embed-
ding ut, which updates the mention vectors as de-
scribed in Section 3.2.

In our dialogue task, the identity of an entity
is unimportant. For example, replacing google
with alphabet in Figure 1 should make little dif-
ference to the conversation. The role of an entity
is determined instead by its relation to other en-
tities and relevant utterances. Therefore, we de-
fine the abstraction At(y) for a word y as follows:
if y is linked to an entity v, then we represent an
entity by its type (school, company etc.) embed-
ding concatenated with its current node embed-
ding: At(y) = [Etype(y), Vt(v)]. Note that Vt(v)
is determined only by its structural features and its
context. If y is a non-entity, then At(y) is the word
embedding of y concatenated with a zero vector
of the same dimensionality as Vt(v). This way,
the representation of an entity only depends on its
structural properties given by the KB and the dia-
logue context, which enables the model to gener-
alize to unseen entities at test time.

Generation. Now, assuming we have embedded
utterance xt�1 into ht�1,nt�1 as described above,
we use another LSTM to generate utterance xt.
Formally, we carry over the last utterance embed-
ding ht,0 = ht�1,nt�1 and define:

ht,j = LSTMdec(ht,j�1, [At(xt,j), ct,j ]), (5)

where ct,j is a weighted sum of node embeddings
in the current turn: ct,j =

P
v2Gt ↵t,j,vVt(v),

where ↵t,j,v are the attention weights over the
nodes. Intuitively, high weight should be given to
relevant entity nodes as shown in Figure 3,. We
compute the weights through standard attention
mechanism (Bahdanau et al., 2015):

↵t,j = softmax(st,j),

st,j,v = w
attn · tanh

�
W attn [ht,j�1, Vt(v)]

�
,

1770



where vector wattn and W attn are parameters.
Finally, we define a distribution over both words

in the vocabulary and nodes in Gt using the copy-
ing mechanism of Jia and Liang (2016):

p(xt,j+1 = y | Gt, xt,j) / exp
�
W vocabht,j + b

�
,

p(xt,j+1 = r(v) | Gt, xt,j) / exp (st,j,v) ,

where y is a word in the vocabulary, W vocab and
b are parameters, and r(v) is the realization of the
entity represented by node v, e.g., google is real-
ized to “Google” during copying.10

4 Experiments

We compare our model with a rule-based sys-
tem and a baseline neural model. Both automatic
and human evaluations are conducted to test the
models in terms of fluency, correctness, coopera-
tion, and human-likeness. The results show that
DynoNet is able to converse with humans in a co-
herent and strategic way.

4.1 Setup
We randomly split the data into train, dev, and test
sets (8:1:1). We use a one-layer LSTM with 100
hidden units and 100-dimensional word vectors
for both the encoder and the decoder (Section 3.3).
Each successful dialogue is turned into two exam-
ples, each from the perspective of one of the two
agents. We maximize the log-likelihood of all ut-
terances in the dialogues. The parameters are opti-
mized by AdaGrad (Duchi et al., 2010) with an ini-
tial learning rate of 0.5. We trained for at least 10
epochs; after that, training stops if there is no im-
provement on the dev set for 5 epochs. By default,
we perform K = 2 iterations of message passing
to compute node embeddings (Section 3.2). For
decoding, we sequentially sample from the output
distribution with a softmax temperature of 0.5.11

Hyperparameters are tuned on the dev set.
We compare DynoNet with its static cou-

sion (StanoNet) and a rule-based system (Rule).
StanoNet uses G0 throughout the dialogue, thus
the dialogue history is completely contained in
the LSTM states instead of being injected into
the knowledge graph. Rule maintains weights for
each entity and each item in the KB to decide

10 We realize an entity by sampling from the empirical dis-
tribution of its surface forms found in the training data.

11 Since selection is a common ‘utterance’ in our dataset
and neural generation models are susceptible to over-
generating common sentences, we halve its probability dur-
ing sampling.

what to talk about and which item to select. It
has a pattern-matching semantic parser, a rule-
based policy, and a templated generator. See Ap-
pendix G for details.

4.2 Evaluation

We test our systems in two interactive settings:
bot-bot chat and bot-human chat. We perform both
automatic evaluation and human evaluation.

Automatic Evaluation. First, we compute the
cross-entropy (`) of a model on test data. As
shown in Table 4, DynoNet has the lowest test
loss. Next, we have a model chat with itself on
the scenarios from the test set.12 We evaluate the
chats with respect to language variation, effective-
ness, and strategy.

For language variation, we report the average
utterance length Lu and the unigram entropy H
in Table 4. Compared to Rule, the neural mod-
els tend to generate shorter utterances (Li et al.,
2016b; Serban et al., 2017b). However, they are
more diverse; for example, questions are asked
in multiple ways such as “Do you have ...”, “Any
friends like ...”, “What about ...”.

At the discourse level, we expect the distribu-
tion of a bot’s utterance types to match the distri-
bution of human’s. We show percentages of each
utterance type in Table 4. For Rule, the decision
about which action to take is written in the rules,
while StanoNet and DynoNet learned to behave in
a more human-like way, frequently informing and
asking questions.

To measure effectiveness, we compute the over-
all success rate (C) and the success rate per turn
(CT ) and per selection (CS). As shown in Table 4,
humans are the best at this game, followed by Rule
which is comparable to DynoNet.

Next, we investigate the strategies leading to
these results. An agent needs to decide which
entity/attribute to check first to quickly reduce
the search space. We hypothesize that humans
tend to first focus on a majority entity and an
attribute with fewer unique values (Section 2.3).
For example, in the scenario in Table 6, time and
location are likely to be mentioned first. We
show the average frequency of first-mentioned en-
tities (#Ent1) and the average number of unique
values for first-mentioned attributes (|Attr1|) in Ta-

12 We limit the number of turns in bot-bot chat to be the
maximum number of turns humans took in the test set (46
turns).

1771



System ` # Lu H C " CT " CS " Sel Inf Ask Ans Greet #Ent1 |Attr1| #Ent #Attr
Human - 5.10 4.57 .82 .07 .38 .21 .31 .17 .08 .08 .55 .35 6.1 2.6

Rule - 7.61 3.37 .90 .05 .29 .18 .34 .23 .00 .12 .24 .61 9.9 3.0
StanoNet 2.20 4.01 4.05 .78 .04 .18 .19 .26 .12 .23 .09 .61 .19 7.1 2.9
DynoNet 2.13 3.37 3.90 .96 .06 .25 .22 .26 .13 .20 .12 .55 .18 5.2 2.5

Table 4: Automatic evaluation on human-human and bot-bot chats on test scenarios. We use " / # to
indicate that higher / lower values are better; otherwise the objective is to match humans’ statistics. Best
results (except Human) are in bold. Neural models generate shorter (lower Lu) but more diverse (higher
H) utterances. Overall, their distributions of utterance types match those of the humans’. (We only show
the most frequent speech acts therefore the numbers do not sum to 1.) Rule is effective in completing
the task (higher CS), but it is not information-efficient given the large number of attributes (#Attr) and
entities (#Ent) mentioned.

ble 4.13 Both DynoNet and StanoNet successfully
match human’s starting strategy by favoring enti-
ties of higher frequency and attributes of smaller
domain size.

To examine the overall strategy, we show the
average number of attributes (#Attr) and entities
(#Ent) mentioned during the conversation in Ta-
ble 4. Humans and DynoNet strategically focus on
a few attributes and entities, whereas Rule needs
almost twice entities to achieve similar success
rates. This suggests that the effectiveness of Rule
mainly comes from large amounts of unselective
information, which is consistent with comments
from their human partners.

Partner Evaluation. We generated 200 new
scenarios and put up the bots on AMT using the
same chat interface that was used for data col-
lection. The bots follow simple turn-taking rules
explained in Appendix H. Each AMT worker is
randomly paired with Rule, StanoNet, DynoNet,
or another human (but the worker doesn’t know
which), and we make sure that all four types of
agents are tested in each scenario at least once. At
the end of each dialogue, humans are asked to rate
their partner in terms of fluency, correctness, co-
operation, and human-likeness from 1 (very bad)
to 5 (very good), along with optional comments.

We show the average ratings (with significance
tests) in Table 5 and the histograms in Appendix J.
In terms of fluency, the models have similar per-
formance since the utterances are usually short.
Judgment on correctness is a mere guess since the
evaluator cannot see the partner’s KB; we will an-
alyze correctness more meaningfully in the third-
party evaluation below.

13 Both numbers are normalized to [0, 1] with respect to all
entities/attributes in the corresponding KB.

Noticeably, DynoNet is more cooperative than
the other models. As shown in the example dia-
logues in Table 6, DynoNet cooperates smoothly
with the human partner, e.g., replying with rel-
evant information about morning/indoor friends
when the partner mentioned that all her friends
prefer morning and most like indoor. StanoNet
starts well but doesn’t follow up on the morn-
ing friend, presumably because the morning node
is not updated dynamically when mentioned by
the partner. Rule follows the partner poorly. In
the comments, the biggest complaint about Rule
was that it was not ‘listening’ or ‘understanding’.
Overall, DynoNet achieves better partner satisfac-
tion, especially in cooperation.

Third-party Evaluation. We also created a
third-party evaluation task, where an independent
AMT worker is shown a conversation and the KB
of one of the agents; she is asked to rate the same
aspects of the agent as in the partner evaluation
and provide justifications. Each agent in a dia-
logue is rated by at least 5 people.

The average ratings and histograms are shown
in Table 5 and Appendix J. For correctness, we see
that Rule has the best performance since it always
tells the truth, whereas humans can make mistakes
due to carelessness and the neural models can gen-
erate false information. For example, in Table 6,
DynoNet ‘lied’ when saying that it has a morning
friend who likes outdoor.

Surprisingly, there is a discrepancy between the
two evaluation modes in terms of cooperation and
human-likeness. Manual analysis of the com-
ments indicates that third-party evaluators focus
less on the dialogue strategy and more on linguis-
tic features, probably because they were not fully
engaged in the dialogue. For example, justification

1772



System C CT CS
Partner eval Third-party eval

Flnt Crct Coop Human Flnt Crct Coop Human

Human .89 .07 .36 4.2rds 4.3rds 4.2rds 4.1rds 4.0 4.3ds 4.0ds 4.1rds

Rule .88 .06 .29 3.6 4.0 3.5 3.5 4.0 4.4hds 3.9s 4.0s
StanoNet .76 .04 .23 3.5 3.8 3.4 3.3 4.0 4.0 3.8 3.8
DynoNet .87 .05 .27 3.8s 4.0 3.8rs 3.6s 4.0 4.1 3.9 3.9

Table 5: Results on human-bot/human chats. Best results (except Human) in each column are in bold.
We report the average ratings of each system. For third-party evaluation, we first take mean of each
question then average the ratings. DynoNet has the best partner satisfaction in terms of fluency (Flnt),
correctness (Crct), cooperation (Coop), human likeness (Human). The superscript of a result indicates that
its advantage over other systems (r: Rule, s: StanoNet, d: DynoNet) is statistically significant with
p < 0.05 given by paired t-tests.

for cooperation often mentions frequent questions
and timely answers, less attention is paid to what
is asked about though.

For human-likeness, partner evaluation is
largely correlated with coherence (e.g., not repeat-
ing or ignoring past information) and task suc-
cess, whereas third-party evaluators often rely on
informality (e.g., usage of colloquia like “hiya”,
capitalization, and abbreviation) or intuition. In-
terestingly, third-party evaluators noted most phe-
nomena listed in Table 2 as indicators of human-
beings, e.g., correcting oneself, making chit-chat
other than simply finishing the task. See example
comments in Appendix K.

4.3 Ablation Studies

Our model has two novel designs: entity abstrac-
tion and message passing for node embeddings.
Table 7 shows what happens if we ablate these.
When the number of message passing iterations,
K, is reduced from 2 to 0, the loss consistently
increases. Removing entity abstraction—meaning
adding entity embeddings to node embeddings and
the LSTM input embeddings—also degrades per-
formance. This shows that DynoNet benefits from
contextually-defined, structural node embeddings
rather than ones based on a classic lookup table.

Model `

DynoNet (K = 2) 2.16
DynoNet (K = 1) 2.20
DynoNet (K = 0) 2.26
DynoNet (K = 2) w/o entity abstraction 2.21

Table 7: Ablations of our model on the dev
set show the importance of entity abstraction and
message passing (K = 2).

5 Discussion and Related Work

There has been a recent surge of interest in
end-to-end task-oriented dialogue systems, though
progress has been limited by the size of available
datasets (Serban et al., 2015a). Most work focuses
on information-querying tasks, using Wizard-of-
Oz data collection (Williams et al., 2016; Asri
et al., 2016) or simulators (Bordes and Weston,
2017; Li et al., 2016d), In contrast, collaborative
dialogues are easy to collect as natural human
conversations, and are also challenging enough
given the large number of scenarios and diverse
conversation phenomena. There are some in-
teresting strategic dialogue datasets—settlers of
Catan (Afantenos et al., 2012) (2K turns) and the
cards corpus (Potts, 2012) (1.3K dialogues), as
well as work on dialogue strategies (Keizer et al.,
2017; Vogel et al., 2013), though no full dialogue
system has been built for these datasets.

Most task-oriented dialogue systems follow the
POMDP-based approach (Williams and Young,
2007; Young et al., 2013). Despite their suc-
cess (Wen et al., 2017; Dhingra et al., 2017; Su
et al., 2016), the requirement for handcrafted slots
limits their scalability to new domains and bur-
dens data collection with extra state labeling. To
go past this limit, Bordes and Weston (2017) pro-
posed a Memory-Networks-based approach with-
out domain-specific features. However, the mem-
ory is unstructured and interfacing with KBs relies
on API calls, whereas our model embeds both the
dialogue history and the KB structurally. Williams
et al. (2017) use an LSTM to automatically infer
the dialogue state, but as they focus on dialogue
control rather than the full problem, the response is
modeled as a templated action, which restricts the
generation of richer utterances. Our network ar-

1773



Friends of A

ID Name Company Time Location

1 Kathy TRT Holdings afternoon indoor
2 Jason Dollar General afternoon indoor
3 Johnny TRT Holdings afternoon outdoor
4 Frank SFN Group afternoon indoor
5 Catherine Dollar General afternoon indoor
6 Catherine Weis Markets afternoon indoor
7 Kathleen TRT Holdings morning indoor
8 Lori TRT Holdings afternoon indoor
9 Frank L&L Hawaiian Barbecue afternoon outdoor

Friends of B

ID Name Company Time Location

1 Justin New Era Tickets morning indoor
2 Kathleen TRT Holdings morning indoor
3 Gloria L&L Hawaiian Barbecue morning indoor
4 Kathleen Advance Auto Parts morning outdoor
5 Justin Arctic Cat morning indoor
6 Anna Dollar General morning indoor
7 Steven SFN Group morning indoor
8 Wayne R.J. Corman Railroad Group morning indoor
9 Alexander R.J. Corman Railroad Group morning indoor

A: Human B: Human A: DynoNet B: Human

A: Hi
B: hey || i have one outdoor
A: I have 4 TRT Holdings || I have 2 outdoor one
Johnny, other Frank
B: i only have one TRT Holdings - Kathleen
A: SELECT 7 B: SELECT 2

A: hi
B: Hello || all my friends prefer morning
A: 1 of my morning likes the outdoors
B: and all like indoor except for one
A: do they work for trt holdings?
B: Kathleen?
A: SELECT 7 B: SELECT 2

A: StanoNet B: Human A: Human B: Rule

A: Hello
B: hi
A: Hello || I have one morning person.
B: all of my friends like mornings
A: My friend prefers afternoon works at trt holdings.
B: what is their name?
A: Likes indoors.
B: what is your fiend who likes morning name?
A: They work for trt holdings.
B: SELECT 2 A: SELECT 7

B: hiya
A: hEY
B: I have 1 indoors and kathleen.
A: Most of mine are indoors.
B: SELECT 1
A: I have one morning and rest afternoon.
B: Do you have any friend working at l hawaiian?
A: I don’t know Justin
B: I have 1 alexander.
...

Table 6: Examples of human-bot chats. The mutual friend is highlighted in blue in each KB. Bots’
utterances are in bold and selected items are represented by item IDs. Only the first half of the human-
Rule chat is shown due to limited space. Multiple utterances of one agent rae separated by ||.

chitecture is most similar to EntNet (Henaff et al.,
2017), where memories are also updated by input
sentences recurrently. The main difference is that
our model allows information to be propagated be-
tween structured entities, which is shown to be
crucial in our setting (Section 4.3).

Our work is also related to language generation
conditioned on knowledge bases (Mei et al., 2016;
Kiddon et al., 2016). One challenge here is to
avoid generating false or contradicting statements,
which is currently a weakness of neural models.
Our model is mostly accurate when generating
facts and answering existence questions about a
single entity, but will need a more advanced at-
tention mechanism for generating utterances in-
volving multiple entities, e.g., attending to items
or attributes first, then selecting entities; generat-
ing high-level concepts before composing them to
natural tokens (Serban et al., 2017a).

In conclusion, we believe the symmetric col-
laborative dialogue setting and our dataset pro-

vide unique opportunities at the interface of tra-
ditional task-oriented dialogue and open-domain
chat. We also offered DynoNet as a promising
means for open-ended dialogue state representa-
tion. Our dataset facilitates the study of prag-
matics and human strategies in dialogue—a good
stepping stone towards learning more complex di-
alogues such as negotiation.

Acknowledgments. This work is supported by
DARPA Communicating with Computers (CwC)
program under ARO prime contract no. W911NF-
15-1-0462. Mike Kayser worked on an early ver-
sion of the project while he was at Stanford. We
also thank members of the Stanford NLP group for
insightful discussions.

Reproducibility. All code, data, and
experiments for this paper are avail-
able on the CodaLab platform: https:
//worksheets.codalab.org/worksheets/

0xc757f29f5c794e5eb7bfa8ca9c945573.

1774



References

S. Afantenos, N. Asher, F. Benamara, A. Cadilhac,
C. Dégremont, P. Denis, M. Guhe, S. Keizer, A. Las-
carides, O. Lemon, P. Muller, S. Paul, V. Rieser, and
L. Vieu. 2012. Developing a corpus of strategic con-
versation in the settlers of catan. In SeineDial 2012 -
The 16th Workshop on the Semantics and Pragmat-
ics of Dialogue.

L. E. Asri, H. Schulz, S. Sharma, J. Zumer, J. Har-
ris, E. Fine, R. Mehrotra, and K. Suleman. 2016.
Frames: A corpus for adding memory to goal-
oriented dialogue systems. Maluuba Technical Re-
port .

D. Bahdanau, K. Cho, and Y. Bengio. 2015. Neural
machine translation by jointly learning to align and
translate. In International Conference on Learning
Representations (ICLR).

A. Bordes and J. Weston. 2017. Learning end-to-end
goal-oriented dialog. In International Conference
on Learning Representations (ICLR).

B. Dhingra, L. Li, X. Li, J. Gao, Y. Chen, F. Ahmed,
and L. Deng. 2017. End-to-end reinforcement learn-
ing of dialogue agents for information access. In As-
sociation for Computational Linguistics (ACL).

J. Duchi, E. Hazan, and Y. Singer. 2010. Adaptive sub-
gradient methods for online learning and stochastic
optimization. In Conference on Learning Theory
(COLT).

M. Henaff, J. Weston, A. Szlam, A. Bordes, and Y. Le-
Cun. 2017. Tracking the world state with recur-
rent entity networks. In International Conference
on Learning Representations (ICLR).

E. Ivanovic. 2005. Dialogue act tagging for instant
messaging chat sessions. In Association for Com-
putational Linguistics (ACL).

R. Jia and P. Liang. 2016. Data recombination for neu-
ral semantic parsing. In Association for Computa-
tional Linguistics (ACL).

S. Keizer, M. Guhe, H. Cuayahuitl, I. Efstathiou,
K. Engelbrecht, M. Dobre, A. Lascarides, and
O. Lemon. 2017. Evaluating persuasion strategies
and deep reinforcement learning methods for nego-
tiation dialogue agents. In European Association for
Computational Linguistics (EACL).

C. Kiddon, L. S. Zettlemoyer, and Y. Choi. 2016. Glob-
ally coherent text generation with neural checklist
models. In Empirical Methods in Natural Language
Processing (EMNLP).

J. Li, M. Galley, C. Brockett, J. Gao, and B. Dolan.
2016a. A persona-based neural conversation model.
In Association for Computational Linguistics (ACL).

J. Li, M. Galley, C. Brockett, J. Gao, and W. B. Dolan.
2016b. A diversity-promoting objective function
for neural conversation models. In Human Lan-
guage Technology and North American Association
for Computational Linguistics (HLT/NAACL).

J. Li, W. Monroe, A. Ritter, D. Jurafsky, M. Galley, and
J. Gao. 2016c. Deep reinforcement learning for dia-
logue generation. In Empirical Methods in Natural
Language Processing (EMNLP).

X. Li, Z. C. Lipton, B. Dhingra, L. Li, J. Gao,
and Y. Chen. 2016d. A user simulator for task-
completion dialogues. arXiv .

C. Liu, R. Lowe, I. V. Serban, M. Noseworthy, L. Char-
lin, and J. Pineau. 2016. How NOT to evaluate your
dialogue system: An empirical study of unsuper-
vised evaluation metrics for dialogue response gen-
eration. In Empirical Methods in Natural Language
Processing (EMNLP).

R. T. Lowe, N. Pow, I. Serban, L. Charlin, C. Liu, and
J. Pineau. 2017. Training End-to-End dialogue sys-
tems with the ubuntu dialogue corpus. Dialogue and
Discourse 8.

H. Mei, M. Bansal, and M. R. Walter. 2016. What
to talk about and how? selective generation using
LSTMs with coarse-to-fine alignment. In Human
Language Technology and North American Associa-
tion for Computational Linguistics (HLT/NAACL).

H. Mei, M. Bansal, and M. R. Walter. 2017. Coherent
dialogue with attention-based language models. In
Association for the Advancement of Artificial Intel-
ligence (AAAI).

C. Potts. 2012. Goal-driven answers in the Cards dia-
logue corpus. In Proceedings of the 30th West Coast
Conference on Formal Linguistics.

I. Serban, T. Klinger, G. Tesauro, K. Talamadupula,
B. Zhou, Y. Bengio, and A. C. Courville. 2017a.
Multiresolution recurrent neural networks: An ap-
plication to dialogue response generation. In Asso-
ciation for the Advancement of Artificial Intelligence
(AAAI).

I. Serban, A. Sordoni, R. Lowe, L. Charlin, J. Pineau,
A. C. Courville, and Y. Bengio. 2017b. A hierarchi-
cal latent variable encoder-decoder model for gener-
ating dialogues. In Association for the Advancement
of Artificial Intelligence (AAAI).

I. V. Serban, R. Lowe, L. Charlin, and J. Pineau.
2015a. A survey of available corpora for build-
ing data-driven dialogue systems. arXiv preprint
arXiv:1512.05742 .

I. V. Serban, A. Sordoni, Y. Bengio, A. Courville, and
J. Pineau. 2015b. Building end-to-end dialogue sys-
tems using generative hierarchical neural network
models. arXiv preprint arXiv:1507.04808 .

1775



L. Shang, Z. Lu, and H. Li. 2015. Neural responding
machine for short-text conversation. In Association
for Computational Linguistics (ACL).

A. Sordoni, M. Galley, M. Auli, C. Brockett, Y. Ji,
M. Mitchell, J. Nie, J. Gao, and B. Dolan. 2015.
A neural network approach to context-sensitive
generation of conversational responses. In North
American Association for Computational Linguis-
tics (NAACL).

P. Su, M. Gasic, N. Mrksic, L. M. Rojas-Barahona,
S. Ultes, D. Vandyke, T. Wen, and S. J. Young. 2016.
Continuously learning neural dialogue management.
arXiv preprint arXiv:1606.02689 .

A. Vogel, M. Bodoia, C. Potts, and D. Jurafsky. 2013.
Emergence of gricean maxims from multi-agent de-
cision theory. In North American Association for
Computational Linguistics (NAACL). pages 1072–
1081.

T. Wen, M. Gasic, N. Mrksic, L. M. Rojas-Barahona,
P. Su, S. Ultes, D. Vandyke, and S. Young. 2017.
A network-based end-to-end trainable task-oriented
dialogue system. In European Association for Com-
putational Linguistics (EACL).

J. D. Williams, K. Asadi, and G. Zweig. 2017. Hy-
brid code networks: Practical and efficient end-to-
end dialog control with supervised and reinforce-
ment learning. In Association for Computational
Linguistics (ACL).

J. D. Williams, A. Raux, and M. Henderson. 2016. The
dialog state tracking challenge series: A review. Di-
alogue and Discourse 7.

J. D. Williams and S. Young. 2007. Partially observ-
able Markov decision processes for spoken dialog
systems. Computer Speech & Language 21(2):393–
422.

S. Young, M. Gasic, B. Thomson, and J. D. Williams.
2013. POMDP-based statistical spoken dialog
systems: A review. Proceedings of the IEEE
101(5):1160–1179.

1776


	Learning Symmetric Collaborative Dialogue Agents with Dynamic Knowledge Graph Embeddings

