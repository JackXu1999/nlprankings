



















































SPIED: Stanford Pattern based Information Extraction and Diagnostics


Proceedings of the Workshop on Interactive Language Learning, Visualization, and Interfaces, pages 38–44,
Baltimore, Maryland, USA, June 27, 2014. c©2014 Association for Computational Linguistics

SPIED: Stanford Pattern-based Information Extraction and Diagnostics

Sonal Gupta Christopher D. Manning
Department of Computer Science

Stanford University
{sonal, manning}@cs.stanford.edu

Abstract

This paper aims to provide an effective
interface for progressive refinement of
pattern-based information extraction sys-
tems. Pattern-based information extrac-
tion (IE) systems have an advantage over
machine learning based systems that pat-
terns are easy to customize to cope with
errors and are interpretable by humans.
Building a pattern-based system is usually
an iterative process of trying different pa-
rameters and thresholds to learn patterns
and entities with high precision and recall.
Since patterns are interpretable to humans,
it is possible to identify sources of errors,
such as patterns responsible for extract-
ing incorrect entities and vice-versa, and
correct them. However, it involves time
consuming manual inspection of the ex-
tracted output. We present a light-weight
tool, SPIED, to aid IE system develop-
ers in learning entities using patterns with
bootstrapping, and visualizing the learned
entities and patterns with explanations.
SPIED is the first publicly available tool to
visualize diagnostic information of multi-
ple pattern learning systems to the best of
our knowledge.

1 Introduction

Entity extraction using rules dominates commer-
cial industry, mainly because rules are effective,
interpretable by humans, and easy to customize to
cope with errors (Chiticariu et al., 2013). Rules,
which can be hand crafted or learned by a sys-
tem, are commonly created by looking at the con-
text around already known entities, such as surface
word patterns (Hearst, 1992) and dependency pat-
terns (Yangarber et al., 2000). Building a pattern-
based learning system is usually a repetitive pro-
cess, usually performed by the system developer,

of manually examining a system’s output to iden-
tify improvements or errors introduced by chang-
ing the entity or pattern extractor. Interpretabil-
ity of patterns makes it easier for humans to iden-
tify sources of errors by inspecting patterns that
extracted incorrect instances or instances that re-
sulted in learning of bad patterns. Parameters
range from window size of the context in surface
word patterns to thresholds for learning a candi-
date entity. At present, there is a lack of tools
helping a system developer to understand results
and to improve results iteratively.

Visualizing diagnostic information of a system
and contrasting it with another system can make
the iterative process easier and more efficient. For
example, consider a user trying to decide on the
context’s window size in surface words patterns.
And the user deliberates that part-of-speech (POS)
restriction of context words might be required for
a reduced window size to avoid extracting erro-
neous mentions.1 By comparing and contrasting
extractions of two systems with different parame-
ters, the user can investigate the cases in which the
POS restriction is required with smaller window
size, and whether the restriction causes the system
to miss some correct entities. In contrast, compar-
ing just accuracy of two systems does not allow
inspecting finer details of extractions that increase
or decrease accuracy and to make changes accord-
ingly.

In this paper, we present a pattern-based entity
learning and diagnostics tool, SPIED. It consists
of two components: 1. pattern-based entity learn-
ing using bootstrapping (SPIED-Learn), and 2. vi-
sualizing the output of one or two entity learning
systems (SPIED-Viz). SPIED-Viz is independent
of SPIED-Learn and can be used with any pattern-
based entity learner. For demonstration, we use
the output of SPIED-Learn as an input to SPIED-

1A shorter context size usually extracts entities with
higher recall but lower precision.

38



Viz. SPIED-Viz has pattern-centric and entity-
centric views, which visualize learned patterns
and entities, respectively, and the explanations for
learning them. SPIED-Viz can also contrast two
systems by comparing the ranks of learned enti-
ties and patterns. In this paper, as a concrete ex-
ample, we learn and visualize drug-treatment (DT)
entities from unlabeled patient-generated medical
text, starting with seed dictionaries of entities for
multiple classes. The task was proposed and fur-
ther developed in Gupta and Manning (2014b)
and Gupta and Manning (2014a).

Our contributions in this paper are: 1. we
present a novel diagnostic tool for visual-
ization of output of multiple pattern-based
entity learning systems, and 2. we release the
code of an end-to-end pattern learning sys-
tem, which learns entities using patterns in a
bootstrapped system and visualizes its diag-
nostic output. The pattern learning code is
available at http://nlp.stanford.edu/
software/patternslearning.shtml.
The visualization code is available at
http://nlp.stanford.edu/software/
patternviz.shtml.

2 Learning Patterns and Entities

Bootstrapped systems have been commonly used
to learn entities (Riloff, 1996; Collins and Singer,
1999). SPIED-Learn is based on the system de-
scribed in Gupta and Manning (2014a), which
builds upon the previous bootstrapped pattern-
learning work and proposed an improved mea-
sure to score patterns (Step 3 below). It learns
entities for given classes from unlabeled text by
bootstrapping from seed dictionaries. Patterns
are learned using labeled entities, and entities are
learned based on the extractions of learned pat-
terns. The process is iteratively performed until
no more patterns or entities can be learned. The
following steps give a short summary of the itera-
tive learning of entities belonging to a class DT:

1. Data labeling: The text is labeled using the
class dictionaries, starting with the seed dic-
tionaries in the first iteration. A phrase
matching a dictionary phrase is labeled with
the dictionary’s class.

2. Pattern generation: Patterns are generated us-
ing the context around the positively labeled
entities to create candidate patterns for DT.

3. Pattern learning: Candidate patterns are
scored using a pattern scoring measure and
the top ones are added to the list of learned
patterns for DT. The maximum number of
patterns learned is given as an input to the
system by the developer.

4. Entity learning: Learned patterns for the class
are applied to the text to extract candidate en-
tities. An entity scorer ranks the candidate
entities and adds the top entities to DT’s dic-
tionary. The maximum number of entities
learned is given as an input to the system by
the developer.

5. Repeat steps 1-4 for a given number of itera-
tions.

SPIED provides an option to use any of the pat-
tern scoring measures described in (Riloff, 1996;
Thelen and Riloff, 2002; Yangarber et al., 2002;
Lin et al., 2003; Gupta and Manning, 2014b). A
pattern is scored based on the positive, negative,
and unlabeled entities it extracts. The positive and
negative labels of entities are heuristically deter-
mined by the system using the dictionaries and the
iterative entity learning process. The oracle labels
of learned entities are not available to the learning
system. Note that an entity that the system consid-
ered positive might actually be incorrect, since the
seed dictionaries can be noisy and the system can
learn incorrect entities in the previous iterations,
and vice-versa. SPIED’s entity scorer is the same
as in Gupta and Manning (2014a).

Each candidate entity is scored using weights of
the patterns that extract it and other entity scoring
measures, such as TF-IDF. Thus, learning of each
entity can be explained by the learned patterns that
extract it, and learning of each pattern can be ex-
plained by all the entities it extracts.

3 Visualizing Diagnostic Information

SPIED-Viz visualizes learned entities and patterns
from one or two entity learning systems, and the
diagnostic information associated with them. It
optionally uses the oracle labels of learned enti-
ties to color code them, and contrast their ranks
of correct/incorrect entities when comparing two
systems. The oracle labels are usually determined
by manually judging each learned entity as cor-
rect or incorrect. SPIED-Viz has two views: 1. a
pattern-centric view that visualizes patterns of one

39



S
co

re
 o

f 
th

e 
en

ti
ty

 i
n

 
th

is
 s

y
st

em
 a

n
d

 t
h

e 
o

th
er

 s
y

st
em

, 
a

lo
n

g
 

w
it

h
 a

 l
in

k
 t

o
 s

ea
rc

h
 

it
 o

n
 G

o
o

g
le

.

A
n

 s
ta

r 
si

g
n

 f
o

r 
a

n
 

en
ti

ty
 i

n
d

ic
a

te
s 

th
e 

en
ti

ty
 l

a
b

el
 i

s 
n

o
t 

p
ro

v
id

ed
 a

n
d

 i
t 

w
a

s 
n

o
t 

ex
tr

a
ct

ed
 b

y
 t

h
e 

o
th

er
 s

y
st

em
.

A
 t

ro
p

h
y

 s
ig

n
 

in
d

ic
a

te
s 

th
a

t 
th

e 
en

ti
ty

 i
s 

co
rr

ec
t 

a
n

d
 w

a
s 

n
o

t 
ex

tr
a

ct
ed

 b
y

 t
h

e 
o

th
er

 s
y

st
em

.

L
is

t 
o

f 
en

ti
ti

es
 

le
a

rn
ed

 a
t 

ea
ch

 
it

er
a

ti
o

n
. 

G
re

en
 

co
lo

r 
in

d
ic

a
te

s 
th

a
t 

th
e 

en
ti

ty
 i

s 
co

rr
ec

t 
a

n
d

 r
ed

 
co

lo
r 

in
d

ic
a

te
s 

th
a

t 
th

e 
en

ti
ty

 i
s 

in
co

rr
ec

t.

L
is

t 
o

f 
p

a
tt

er
n

s 
th

a
t 

ex
tr

a
ct

ed
 t

h
e 

en
ti

ty
. 

T
h

ei
r 

d
et

a
il

s 
a

re
 s

im
il

a
r 

to
 t

h
e 

d
et

a
il

s 
sh

o
w

n
 i

n
 t

h
e 

p
a

tt
er

n
-c

en
tr

ic
 

v
ie

w
.

Fi
gu

re
1:

E
nt

ity
ce

nt
ri

c
vi

ew
of

SP
IE

D
-V

iz
.

T
he

in
te

rf
ac

e
al

lo
w

s
th

e
us

er
to

dr
ill

do
w

n
th

e
re

su
lts

to
di

ag
no

se
ex

tr
ac

tio
n

of
co

rr
ec

ta
nd

in
co

rr
ec

te
nt

iti
es

,a
nd

co
nt

ra
st

th
e

de
ta

ils
of

th
e

tw
o

sy
st

em
s.

T
he

en
tit

ie
s

th
at

ar
e

no
tl

ea
rn

ed
by

th
e

ot
he

r
sy

st
em

ar
e

m
ar

ke
d

w
ith

ei
th

er
a

tr
op

hy
(c

or
re

ct
en

tit
y)

,a
th

um
bs

do
w

n
(i

nc
or

re
ct

en
tit

y)
,o

ra
st

ar
ic

on
(o

ra
cl

e
la

be
lm

is
si

ng
),

fo
re

as
y

id
en

tifi
ca

tio
n.

40



L
is

t 
o

f 
en

ti
ti

es
 

co
n

si
d

er
ed

 a
s 

p
o

si
ti

v
e,

 n
eg

a
ti

v
e,

 
a

n
d

 u
n

la
b

el
ed

 b
y

 
th

e 
sy

st
em

 w
h

en
 i

t 
le

a
rn

ed
 t

h
is

 
p

a
tt

er
n

.

A
n

 e
x

cl
a

m
a

ti
o

n
 

si
g

n
 i

n
d

ic
a

te
s 

th
a

t 
le

ss
 t

h
a

n
 h

a
lf

 o
f 

th
e 

u
n

la
b

el
ed

 
en

ti
ti

es
 w

er
e 

ev
en

tu
a

ll
y

 l
ea

rn
ed

 
w

it
h

 c
o

rr
ec

t 
la

b
el

.

D
et

a
il

s 
o

f 
th

e 
p

a
tt

er
n

.

G
re

en
 c

o
lo

r 
o

f 
en

ti
ty

 i
n

d
ic

a
te

s 
th

a
t 

th
e 

en
ti

ty
 w

a
s 

le
a

rn
ed

 b
y

 t
h

e 
sy

st
em

 a
n

d
 t

h
e 

o
ra

cl
e 

a
ss

ig
n

ed
 i

t 
th

e 
‘c

o
rr

ec
t’

 l
a

b
el

.

L
is

t 
o

f 
p

a
tt

er
n

s 
le

a
rn

ed
 a

t 
ea

ch
 

it
er

a
ti

o
n

. 
B

lu
e 

p
a

tt
er

n
 i

n
d

ic
a

te
s 

th
a

t 
th

e 
p

a
tt

er
n

 
w

a
s 

n
o

t 
le

a
rn

ed
 b

y
 

th
e 

o
th

er
 s

y
st

em
.

Fi
gu

re
2:

Pa
tte

rn
ce

nt
ri

c
vi

ew
of

SP
IE

D
-V

iz
.

41



Figure 3: When the user click on the compare icon for an entity, the explanations of the entity extraction
for both systems (if available) are displayed. This allows direct comparison of why the two systems
learned the entity.

to two systems, and 2. an entity centric view that
mainly focuses on the entities learned. Figure 1
shows a screenshot of the entity-centric view of
SPIED-Viz. It displays following information:

Summary: A summary information of each sys-
tem at each iteration and overall. It shows
for each system the number of iterations, the
number of patterns learned, and the number
of correct and incorrect entities learned.

Learned Entities with provenance: It shows
ranked list of entities learned by each system,
along with an explanation of why the entity
was learned. The details shown include the
entity’s oracle label, its rank in the other sys-
tem, and the learned patterns that extracted
the entity. Such information can help the user
to identify and inspect the patterns responsi-
ble for learning an incorrect entity. The inter-
face also provides a link to search the entity
along with any user provided keywords (such
as domain of the problem) on Google.

System Comparison: SPIED-Viz can be used to
compare entities learned by two systems. It
marks entities that are learned by one system
but not by the other system, by either display-
ing a trophy sign (if the entity is correct), a
thumbs down sign (if the entity is incorrect),
or a star sign (if the oracle label is not pro-
vided).

The second view of SPIED-Viz is pattern-
centric. Figure 2 shows a screenshot of the pattern-
centric view. It displays the following informa-
tion.

Summary: A summary information of each sys-
tem including the number of iterations and

number of patterns learned at each iteration
and overall.

Learned Patterns with provenance: It shows
ranked list of patterns along with the entities
it extracts and their labels. Note that each pat-
tern is associated with a set of positive, neg-
ative and unlabeled entities, which were used
to determine its score.2 It also shows the per-
centage of unlabeled entities extracted by a
pattern that were eventually learned by the
system and assessed as correct by the oracle.
A smaller percentage means that the pattern
extracted many entities that were either never
learned or learned but were labeled as incor-
rect by the oracle.

Figure 3 shows an option in the entity-centric
view when hovering over an entity opens a win-
dow on the side that shows the diagnostic informa-
tion of the entity learned by the other system. This
direct comparison is to directly contrast learning
of an entity by both systems. For example, it can
help the user to inspect why an entity was learned
at an earlier rank than the other system.

An advantage of making the learning entities
component and the visualization component inde-
pendent is that a developer can use any pattern
scorer or entity scorer in the system without de-
pending on the visualization component to provide
that functionality.

2Note that positive, negative, and unlabeled labels are dif-
ferent from the oracle labels, correct and incorrect, for the
learned entities. The former refer to the entity labels consid-
ered by the system when learning the pattern, and they come
from the seed dictionaries and the learned entities. A positive
entity considered by the system can be labeled as incorrect
by the human assessor, in case the system made a mistake in
labeling data, and vice-versa.

42



4 System Details

SPIED-Learn uses TokensRegex (Chang and
Manning, 2014) to create and apply surface word
patterns to text. SPIED-Viz takes details of
learned entities and patterns as input in a JSON
format. It uses Javascript, angular, and jquery to
visualize the information in a web browser.

5 Related Work

Most interactive IE systems focus on annotation
of text, labeling of entities, and manual writing
of rules. Some annotation and labeling tools are:
MITRE’s Callisto3, Knowtator4, SAPIENT (Li-
akata et al., 2009), brat5, Melita (Ciravegna et al.,
2002), and XConc Suite (Kim et al., 2008). Akbik
et al. (2013) interactively helps non-expert users
to manually write patterns over dependency trees.
GATE6 provides the JAPE language that recog-
nizes regular expressions over annotations. Other
systems focus on reducing manual effort for de-
veloping extractors (Brauer et al., 2011; Li et al.,
2011). In contrast, our tool focuses on visualizing
and comparing diagnostic information associated
with pattern learning systems.

WizIE (Li et al., 2012) is an integrated environ-
ment for annotating text and writing pattern ex-
tractors for information extraction. It also gener-
ates regular expressions around labeled mentions
and suggests patterns to users. It is most similar
to our tool as it displays an explanation of the re-
sults extracted by a pattern. However, it is focused
towards hand writing and selection of rules. In ad-
dition, it cannot be used to directly compare two
pattern learning systems.

What’s Wrong With My NLP?7 is a tool for
jointly visualizing various natural language pro-
cessing formats such as trees, graphs, and entities.
It can be used alongside our system to visualize
the patterns since we mainly focus on diagnostic
information.

6 Future Work and Conclusion

We plan to add a feature for a user to provide
the oracle label of a learned entity using the in-
terface. Currently, the oracle labels are assigned
offline. We also plan to extend SPIED to visualize

3http://callisto.mitre.org
4http://knowtator.sourceforge.net
5http://brat.nlplab.org
6http://gate.ac.uk
7https://code.google.com/p/whatswrong

diagnostic information of learned relations from a
pattern-based relation learning system. Another
avenue of future work is to evaluate SPIED-Viz
by studying its users and their interactions with
the system. In addition, we plan to improve the
visualization by summarizing the diagnostic infor-
mation, such as which parameters led to what mis-
takes, to make it easier to understand for systems
that extract large number of patterns and entities.

In conclusion, we present a novel diagnostic
tool for pattern-based entity learning that visual-
izes and compares output of one to two systems.
It is light-weight web browser based visualization.
The visualization can be used with any pattern-
based entity learner. We make the code of an end-
to-end system freely available for research pur-
pose. The system learns entities and patterns using
bootstrapping starting with seed dictionaries, and
visualizes the diagnostic output. We hope SPIED
will help other researchers and users to diagnose
errors and tune parameters in their pattern-based
entity learning system in an easy and efficient way.

References
Alan Akbik, Oresti Konomi, and Michail Melnikov.

2013. Propminer: A workflow for interactive infor-
mation extraction and exploration using dependency
trees. In ACL (Conference System Demonstrations),
pages 157–162.

Falk Brauer, Robert Rieger, Adrian Mocan, and Woj-
ciech M. Barczynski. 2011. Enabling information
extraction by inference of regular expressions from
sample entities. In CIKM, pages 1285–1294.

Angel X. Chang and Christopher D. Manning. 2014.
TokensRegex: Defining cascaded regular expres-
sions over tokens. In Stanford University Technical
Report.

Laura Chiticariu, Yunyao Li, and Frederick R. Reiss.
2013. Rule-based information extraction is dead!
long live rule-based information extraction systems!
In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, EMNLP
’13, pages 827–832.

Fabio Ciravegna, Alexiei Dingli, Daniela Petrelli, and
Yorick Wilks. 2002. User-system cooperation
in document annotation based on information ex-
traction. In In Proceedings of the 13th Interna-
tional Conference on Knowledge Engineering and
Knowledge Management, EKAW02, pages 122–137.
Springer Verlag.

Michael Collins and Yoram Singer. 1999. Unsuper-
vised models for named entity classification. In Pro-
ceedings of the Joint SIGDAT Conference on Empir-

43



ical Methods in Natural Language Processing and
Very Large Corpora, pages 100–110.

Sonal Gupta and Christopher D. Manning. 2014a. Im-
proved pattern learning for bootstrapped entity ex-
traction. In Proceedings of the Eighteenth Confer-
ence on Computational Natural Language Learning
(CoNLL).

Sonal Gupta and Christopher D. Manning. 2014b. In-
duced lexico-syntactic patterns improve information
extraction from online medical forums. Under Sub-
mission.

Marti A Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
the 14th International Conference on Computational
linguistics, COLING ’92, pages 539–545.

Jin-Dong Kim, Tomoko Ohta, and Jun ichi Tsujii.
2008. Corpus annotation for mining biomedical
events from literature. BMC Bioinformatics.

Yunyao Li, Vivian Chu, Sebastian Blohm, Huaiyu
Zhu, and Howard Ho. 2011. Facilitating pat-
tern discovery for relation extraction with semantic-
signature-based clustering. In Proceedings of the
20th ACM International Conference on Informa-
tion and Knowledge Management, CIKM ’11, pages
1415–1424.

Yunyao Li, Laura Chiticariu, Huahai Yang, Freder-
ick R. Reiss, and Arnaldo Carreno-fuentes. 2012.
Wizie: A best practices guided development envi-
ronment for information extraction. In Proceedings
of the ACL 2012 System Demonstrations, ACL ’12,
pages 109–114.

Maria Liakata, Claire Q, and Larisa N. Soldatova.
2009. Semantic annotation of papers: Interface &
enrichment tool (sapient). In Proceedings of the
BioNLP 2009 Workshop, pages 193–200.

Winston Lin, Roman Yangarber, and Ralph Grishman.
2003. Bootstrapped learning of semantic classes
from positive and negative examples. In Proceed-
ings of the ICML 2003 Workshop on The Continuum
from Labeled to Unlabeled Data in Machine Learn-
ing and Data Mining.

Ellen Riloff. 1996. Automatically generating extrac-
tion patterns from untagged text. In Proceedings
of the 13th National Conference on Artificial Intelli-
gence, AAAI’96, pages 1044–1049.

Michael Thelen and Ellen Riloff. 2002. A bootstrap-
ping method for learning semantic lexicons using
extraction pattern contexts. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP ’02, pages 214–221.

Roman Yangarber, Ralph Grishman, and Pasi
Tapanainen. 2000. Automatic acquisition of
domain knowledge for information extraction. In
Proceedings of the 18th International Conference
on Computational Linguistics, COLING ’00, pages
940–946.

Roman Yangarber, Winston Lin, and Ralph Grishman.
2002. Unsupervised learning of generalized names.
In Proceedings of the 19th International Conference
on Computational Linguistics, COLING ’02.

44


