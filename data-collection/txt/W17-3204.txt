



















































Six Challenges for Neural Machine Translation


Proceedings of the First Workshop on Neural Machine Translation, pages 28–39,
Vancouver, Canada, August 4, 2017. c©2017 Association for Computational Linguistics

Six Challenges for Neural Machine Translation

Philipp Koehn
Computer Science Department

Johns Hopkins University
phi@jhu.edu

Rebecca Knowles
Computer Science Department

Johns Hopkins University
rknowles@jhu.edu

Abstract

We explore six challenges for neural
machine translation: domain mismatch,
amount of training data, rare words, long
sentences, word alignment, and beam
search. We show both deficiencies and
improvements over the quality of phrase-
based statistical machine translation.

1 Introduction

Neural machine translation has emerged as the
most promising machine translation approach in
recent years, showing superior performance on
public benchmarks (Bojar et al., 2016) and rapid
adoption in deployments by, e.g., Google (Wu
et al., 2016), Systran (Crego et al., 2016), and
WIPO (Junczys-Dowmunt et al., 2016). But there
have also been reports of poor performance, such
as the systems built under low-resource conditions
in the DARPA LORELEI program.1

In this paper, we examine a number of chal-
lenges to neural machine translation (NMT) and
give empirical results on how well the technology
currently holds up, compared to traditional statis-
tical machine translation (SMT).

We find that:

1. NMT systems have lower quality out of do-
main, to the point that they completely sacri-
fice adequacy for the sake of fluency.

2. NMT systems have a steeper learning curve
with respect to the amount of training data,
resulting in worse quality in low-resource
settings, but better performance in high-
resource settings.

1https://www.nist.gov/itl/iad/mig/lorehlt16-
evaluations

3. NMT systems that operate at the sub-word
level (e.g. with byte-pair encoding) perform
better than SMT systems on extremely low-
frequency words, but still show weakness in
translating low-frequency words belonging to
highly-inflected categories (e.g. verbs).

4. NMT systems have lower translation quality
on very long sentences, but do comparably
better up to a sentence length of about 60
words.

5. The attention model for NMT does not al-
ways fulfill the role of a word alignment
model, but may in fact dramatically diverge.

6. Beam search decoding only improves trans-
lation quality for narrow beams and deterio-
rates when exposed to a larger search space.

We note a 7th challenge that we do not exam-
ine empirically: NMT systems are much less in-
terpretable. The answer to the question of why
the training data leads these systems to decide on
specific word choices during decoding is buried in
large matrices of real-numbered values. There is a
clear need to develop better analytics for NMT.

Other studies have looked at the comparable
performance of NMT and SMT systems. Ben-
tivogli et al. (2016) considered different lin-
guistic categories for English–German and Toral
and Sánchez-Cartagena (2017) compared different
broad aspects such as fluency and reordering for
nine language directions.

2 Experimental Setup

We use common toolkits for neural machine trans-
lation (Nematus) and traditional phrase-based sta-
tistical machine translation (Moses) with common
data sets, drawn from WMT and OPUS.

28



2.1 Neural Machine Translation

While a variety of neural machine translation ap-
proaches were initially proposed — such as the
use of convolutional neural networks (Kalchbren-
ner and Blunsom, 2013) — practically all re-
cent work has been focused on the attention-based
encoder-decoder model (Bahdanau et al., 2015).

We use the toolkit Nematus2 (Sennrich et al.,
2017) which has been shown to give state-of-the-
art results (Sennrich et al., 2016a) at the WMT
2016 evaluation campaign (Bojar et al., 2016).

Unless noted otherwise, we use default settings,
such as beam search and single model decoding.
The training data is processed with byte-pair en-
coding (Sennrich et al., 2016b) into subwords to
fit a 50,000 word vocabulary limit.

2.2 Statistical Machine Translation

Our machine translation systems are trained using
Moses3 (Koehn et al., 2007). We build phrase-
based systems using standard features that are
commonly used in recent system submissions to
WMT (Williams et al., 2016; Ding et al., 2016a).

While we use the shorthand SMT for these
phrase-based systems, we note that there are other
statistical machine translation approaches such as
hierarchical phrase-based models (Chiang, 2007)
and syntax-based models (Galley et al., 2004,
2006) that have been shown to give superior per-
formance for language pairs such as Chinese–
English and German–English.

2.3 Data Conditions

We carry out our experiments on English–Spanish
and German–English. For these language pairs,
large training data sets are available. We use
datasets from the shared translation task organized
alongside the Conference on Machine Translation
(WMT)4. For the domain experiments, we use the
OPUS corpus5 (Tiedemann, 2012).

Except for the domain experiments, we use the
WMT test sets composed of news stories, which
are characterized by a broad range of topic, for-
mal language, relatively long sentences (about 30
words on average), and high standards for gram-
mar, orthography, and style.

2https://github.com/rsennrich/nematus/
3http://www.stat.org/moses/
4http://www.statmt.org/wmt17/
5http://opus.lingfil.uu.se/

Corpus Words Sentences W/S
Law (Acquis) 18,128,173 715,372 25.3
Medical (EMEA) 14,301,472 1,104,752 12.9
IT 3,041,677 337,817 9.0
Koran (Tanzil) 9,848,539 480,421 20.5
Subtitles 114,371,754 13,873,398 8.2

Table 1: Corpora used to train domain-specific
systems, taken from the OPUS repository. IT
corpora are GNOME, KDE, PHP, Ubuntu, and
OpenOffice.

3 Challenges

3.1 Domain Mismatch

A known challenge in translation is that in dif-
ferent domains,6 words have different transla-
tions and meaning is expressed in different styles.
Hence, a crucial step in developing machine trans-
lation systems targeted at a specific use case is
domain adaptation. We expect that methods for
domain adaptation will be developed for NMT. A
currently popular approach is to train a general do-
main system, followed by training on in-domain
data for a few epochs (Luong and Manning, 2015;
Freitag and Al-Onaizan, 2016).

Often, large amounts of training data are only
available out of domain, but we still seek to have
robust performance. To test how well NMT and
SMT hold up, we trained five different systems us-
ing different corpora obtained from OPUS (Tiede-
mann, 2012). An additional system was trained on
all the training data. Statistics about corpus sizes
are shown in Table 1. Note that these domains are
quite distant from each other, much more so than,
say, Europarl, TED Talks, News Commentary, and
Global Voices.

We trained both SMT and NMT systems for all
domains. All systems were trained for German-
English, with tuning and test sets sub-sampled
from the data (these were not used in training). A
common byte-pair encoding is used for all training
runs.

See Figure 1 for results. While the in-domain
NMT and SMT systems are similar (NMT is better
for IT and Subtitles, SMT is better for Law, Med-
ical, and Koran), the out-of-domain performance
for the NMT systems is worse in almost all cases,
sometimes dramatically so. For instance the Med-

6We use the customary definition of domain in machine
translation: a domain is defined by a corpus from a specific
source, and may differ from other domains in topic, genre,
style, level of formality, etc.

29



System ↓ Law Medical IT Koran Subtitles

All Data 30.5 32.8 45.1 42.2 35.3 44.7 17.9 17.9 26.4 20.8

Law 31.1 34.4 12.1 18.2 3.5 6.9 1.3 2.2 2.8 6.0

Medical 3.9 10.2 39.4 43.5 2.0 8.5 0.6 2.0 1.4 5.8

IT 1.9 3.7 6.5 5.3 42.1 39.8 1.8 1.6 3.9 4.7

Koran 0.4 1.8 0.0 2.1 0.0 2.3 15.9 18.8 1.0 5.5

Subtitles 7.0 9.9 9.3 17.8 9.2 13.6 9.0 8.4 25.9 22.1

Figure 1: Quality of systems (BLEU), when trained on one domain (rows) and tested on another domain
(columns). Comparably, NMT systems (left bars) show more degraded performance out of domain.

ical system leads to a BLEU score of 3.9 (NMT)
vs. 10.2 (SMT) on the Law test set.

Figure 2 displays an example. When translating
the sentence Schaue um dich herum. (reference:
Look around you.) from the Subtitles corpus,
we see mostly non-sensical and completely unre-
lated output from the NMT system. For instance,
the translation from the IT system is Switches to
paused.

Note that the output of the NMT system is often
quite fluent (e.g., Take heed of your own souls.)
but completely unrelated to the input, while the
SMT output betrays its difficulties with coping
with the out-of-domain input by leaving some
words untranslated (e.g., Schaue by dich around.).
This is of particular concern when MT is used for
information gisting — the user will be mislead by
hallucinated content in the NMT output.

3.2 Amount of Training Data

A well-known property of statistical systems is
that increasing amounts of training data lead to
better results. In SMT systems, we have previ-
ously observed that doubling the amount of train-
ing data gives a fixed increase in BLEU scores.
This holds true for both parallel and monolin-
gual data (Turchi et al., 2008; Irvine and Callison-
Burch, 2013).

Source Schaue um dich herum.
Ref. Look around you.
All NMT: Look around you.

SMT: Look around you.
Law NMT: Sughum gravecorn.

SMT: In order to implement dich Schaue .
Medical NMT: EMEA / MB / 049 / 01-EN-Final Work

progamme for 2002
SMT: Schaue by dich around .

IT NMT: Switches to paused.
SMT: To Schaue by itself . \t \t

Koran NMT: Take heed of your own souls.
SMT: And you see.

Subtitles NMT: Look around you.
SMT: Look around you .

Figure 2: Examples for the translation of a sen-
tence from the Subtitles corpus, when translated
with systems trained on different corpora. Per-
formance out-of-domain is dramatically worse for
NMT.

30



106 107 108
0

10

20

30

21.8
23.4

24.9
26.2 26.9

27.9 28.6
29.2 29.6

30.1 30.4

16.4
18.1

19.6
21.2

22.2
23.5

24.7
26.1 26.9

27.8 28.6

1.6

7.2

11.9
14.7

18.2

22.4

25.7
27.4 29.2

30.3 31.1

Corpus Size (English Words)

BLEU Scores with Varying Amounts of Training Data

Phrase-Based with Big LM
Phrase-Based

Neural

Figure 3: BLEU scores for English-Spanish sys-
tems trained on 0.4 million to 385.7 million
words of parallel data. Quality for NMT starts
much lower, outperforms SMT at about 15 mil-
lion words, and even beats a SMT system with a
big 2 billion word in-domain language model un-
der high-resource conditions.

How do the data needs of SMT and NMT com-
pare? NMT promises both to generalize better (ex-
ploiting word similary in embeddings) and condi-
tion on larger context (entire input and all prior
output words).

We built English-Spanish systems on WMT
data,7 about 385.7 million English words paired
with Spanish. To obtain a learning curve, we used

1
1024 ,

1
512 , ...,

1
2 , and all of the data. For SMT, the

language model was trained on the Spanish part of
each subset, respectively. In addition to a NMT
and SMT system trained on each subset, we also
used all additionally provided monolingual data
for a big language model in contrastive SMT sys-
tems.

Results are shown in Figure 3. NMT ex-
hibits a much steeper learning curve, starting with
abysmal results (BLEU score of 1.6 vs. 16.4 for

1
1024 of the data), outperforming SMT 25.7 vs.
24.7 with 116 of the data (24.1 million words), and
even beating the SMT system with a big language
model with the full data set (31.1 for NMT, 28.4
for SMT, 30.4 for SMT+BigLM).

7Spanish was last represented in 2013, we used data from
http://statmt.org/wmt13/translation-task.html

Src: A Republican strategy to counter the re-election
of Obama

1
1024

Un órgano de coordinación para el anuncio de
libre determinación

1
512

Lista de una estrategia para luchar contra la
elección de hojas de Ohio

1
256

Explosión realiza una estrategia divisiva de
luchar contra las elecciones de autor

1
128

Una estrategia republicana para la eliminación
de la reelección de Obama

1
64

Estrategia siria para contrarrestar la reelección
del Obama .

1
32

+ Una estrategia republicana para contrarrestar la
reelección de Obama

Figure 4: Translations of the first sentence of
the test set using NMT system trained on varying
amounts of training data. Under low resource con-
ditions, NMT produces fluent output unrelated to
the input.

The contrast between the NMT and SMT learn-
ing curves is quite striking. While NMT is able to
exploit increasing amounts of training data more
effectively, it is unable to get off the ground with
training corpus sizes of a few million words or
less.

To illustrate this, see Figure 4. With 11024 of the
training data, the output is completely unrelated to
the input, some key words are properly translated
with 1512 and

1
256 of the data (estrategia for strat-

egy, elección or elecciones for election), and start-
ing with 164 the translations become respectable.

3.3 Rare Words

Conventional wisdom states that neural machine
translation models perform particularly poorly on
rare words, (Luong et al., 2015; Sennrich et al.,
2016b; Arthur et al., 2016) due in part to the
smaller vocabularies used by NMT systems. We
examine this claim by comparing performance on
rare word translation between NMT and SMT
systems of similar quality for German–English
and find that NMT systems actually outperform
SMT systems on translation of very infrequent
words. However, both NMT and SMT systems
do continue to have difficulty translating some
infrequent words, particularly those belonging to
highly-inflected categories.

For the neural machine translation model, we
use a publicly available model8 with the training
settings of Edinburgh’s WMT submission (Sen-
nrich et al., 2016a). This was trained using Ne-

8https://github.com/rsennrich/wmt16-scripts/

31



0
1—

2—
— 4 8—

16
—

— 32 64 12
8

25
6

51
2

99
9

19
99

39
99

79
99

15
99

9

31
99

9

63
99

9

64
00

0+

40%

50%

60%

70%

0%

5%

Figure 5: Precision of translation and deletion rates by source words type. SMT (light blue) and NMT
(dark green). The horizontal axis represents the corpus frequency of the source types, with the axis labels
showing the upper end of the bin. Bin width is proportional to the number of word types in that frequency
range. The upper part of the graph shows the precision averaged across all word types in the bin. The
lower part shows the proportion of source tokens in the bin that were deleted.

matus9 (Sennrich et al., 2017), with byte-pair en-
codings (Sennrich et al., 2016b) to allow for open-
vocabulary NMT.

The phrase-based model that we used was
trained using Moses (Koehn et al., 2007), and
the training data and parameters match those de-
scribed in Johns Hopkins University’s submission
to the WMT shared task (Ding et al., 2016b).

Both models have case-sensitive BLEU scores
of 34.5 on the WMT 2016 news test set (for the
NMT model, this reflects the BLEU score re-
sulting from translation with a beam size of 1).
We use a single corpus for computing our lexi-
cal frequency counts (a concatenation of Common
Crawl, Europarl, and News Commentary).

We follow the approach described by Koehn
and Haddow (2012) for examining the effect of
source word frequency on translation accuracy.10

9https://github.com/rsennrich/nematus/
10First, we automatically align the source sentence and the

machine translation output. We use fast-align (Dyer et al.,
2013) to align the full training corpus (source and reference)
along with the test source and MT output. We use the sug-
gested standard options for alignment and then symmetrize
the alignment with grow-diag-final-and.

Each source word is either unaligned (“dropped”) or
aligned to one or more target language words. For each tar-
get word to which the source word is aligned, we check if
that target word appears in the reference translation. If the
target word appears the same number of times in the MT out-
put as in the reference, we award that alignment a score of
one. If the target word appears more times in the MT output
than in the reference, we award fractional credit. If the target
word does not appear in the reference, we award zero credit.

The overall average precision is quite similar
between the NMT and SMT systems, with the
SMT system scoring 70.1% overall and the NMT
system scoring 70.3%. This reflects the similar
overall quality of the MT systems. Figure 5 gives
a detailed breakdown. The values above the hor-
izontal axis represent precisions, while the lower
portion represents what proportion of the words
were deleted. The first item of note is that the
NMT system has an overall higher proportion of
deleted words. Of the 64379 words examined, the
NMT system is estimated to have deleted 3769 of
them, while the SMT system deleted 2274. Both
the NMT and SMT systems delete very frequent
and very infrequent words at higher proportions
than words that fall into the middle range. Across
frequencies, the NMT systems delete a higher pro-
portion of words than the SMT system does. (The
related issue of translation length is discussed in
more detail in Section 3.4.)

The next interesting observation is what hap-
pens with unknown words (words which were
never observed in the training corpus). The SMT
system translates these correctly 53.2% of the
time, while the NMT system translates them cor-
rectly 60.1% of the time. This is reflected in Fig-
ure 5, where the SMT system shows a steep curve

We then average these scores over the full set of target words
aligned to the given source word to compute the precision for
that source word. Source words can then be binned by fre-
quency and average translation precisions can be computed.

32



Label Unobserved Observed Once
Adjective 4 10
Named Entity 40 42
Noun 35 35
Number 12 4
Verb 3 6
Other 6 3

Table 2: Breakdown of the first 100 tokens that
were unobserved in training or observed once in
training, by hand-annotated category.

up from the unobserved words, while the NMT
system does not see a great jump.

Both SMT and NMT systems actually have
their worst performance on words that were ob-
served a single time in the training corpus, drop-
ping to 48.6% and 52.2%, respectively; even
worse than for unobserved words. Table 2 shows
a breakdown of the categories of words that were
unobserved in the training corpus or observed only
once. The most common categories across both
are named entity (including entity and location
names) and nouns. The named entities can of-
ten be passed through unchanged (for example,
the surname “Elabdellaoui” is broken into “E@@
lab@@ d@@ ell@@ a@@ oui” by the byte-
pair encoding and is correctly passed through un-
changed by both the NMT and SMT systems).
Many of the nouns are compound nouns; when
these are correctly translated, it may be attributed
to compound-splitting (SMT) or byte-pair encod-
ing (NMT). The factored SMT system also has ac-
cess to the stemmed form of words, which can
also play a similar role to byte-pair encoding in
enabling translation of unobserved inflected forms
(e.g. adjectives, verbs). Unsurprisingly, there are
many numbers that were unobserved in the train-
ing data; these tend to be translated correctly (with
occasional errors due to formatting of commas and
periods, resolvable by post-processing).

The categories which involve more extensive
inflection (adjectives and verbs) are arguably the
most interesting. Adjectives and verbs have worse
accuracy rates and higher deletion rates than nouns
across most word frequencies. We show examples
in Figure 6 of situations where the NMT system
succeeds and fails, and contrast it with the fail-
ures of the SMT system. In Example 1, the NMT
system successfully translates the unobserved ad-
jective choreographiertes (choreographed), while
the SMT system does not. In Example 2, the
SMT system simply passes the German verb

Src. (1) ... choreographiertes Gesamtkunstwerk ...
(2) ... die Polizei ihn einkesselte.

BPE (1) chore@@ ograph@@ iertes
(2) ein@@ kes@@ sel@@ te

NMT (1) ... choreographed overall artwork ...
(2) ... police stabbed him.

SMT (1) ... choreographiertes total work of art ...
(2) ... police einkesselte him.

Ref. (1) ... choreographed complete work of art ...
(2) ... police closed in on him.

Figure 6: Examples of words that were unob-
served in the training corpus, their byte-pair en-
codings, and their translations.

einkesselte (closed in on) unchanged into the out-
put, while the NMT system fails silently, selecting
the fluent-sounding but semantically inappropriate
“stabbed” instead.

While there remains room for improvement,
NMT systems (at least those using byte-pair en-
coding) perform better on very low-frequency
words then SMT systems do. Byte-pair encoding
is sometimes sufficient (much like stemming or
compound-splitting) to allow the successful trans-
lation of rare words even though it does not nec-
essarily split words at morphological boundaries.
As with the fluent-sounding but semantically inap-
propriate examples from domain-mismatch, NMT
may sometimes fail similarly when it encounters
unknown words even in-domain.

3.4 Long Sentences

A well-known flaw of early encoder-decoder
NMT models was the inability to properly trans-
late long sentences (Cho et al., 2014; Pouget-
Abadie et al., 2014). The introduction of the at-
tention model remedied this problem somewhat.
But how well?

We used the large English-Spanish system from
the learning curve experiments (Section 3.2), and
used it to translate a collection of news test sets
from the WMT shared tasks. We broke up these
sets into buckets based on source sentence length
(1-9 subword tokens, 10-19 subword tokens, etc.)
and computed corpus-level BLEU scores for each.

Figure 7 shows the results. While overall NMT
is better than SMT, the SMT system outperforms
NMT on sentences of length 60 and higher. Qual-
ity for the two systems is relatively close, except
for the very long sentences (80 and more tokens).
The quality of the NMT system is dramatically
lower for these since it produces too short trans-
lations (length ratio 0.859, opposed to 1.024).

33



0 10 20 30 40 50 60 70 80
25

30

35

27.1

28.5

29.6

31

33

34.7

34.1

31.3

27.7
26.9

27.6

28.7

30.3

32.3

33.8

34.7

31.5

33.9

Sentence Length (source, subword count)

B
L

E
U

BLEU Scores with Varying Sentence Length

Neural
Phrase-Based

Figure 7: Quality of translations based on sen-
tence length. SMT outperforms NMT for sen-
tences longer than 60 subword tokens. For very
long sentences (80+) quality is much worse due to
too short output.

3.5 Word Alignment

The key contribution of the attention model in neu-
ral machine translation (Bahdanau et al., 2015)
was the imposition of an alignment of the output
words to the input words. This takes the shape
of a probability distribution over the input words
which is used to weigh them in a bag-of-words
representation of the input sentence.

Arguably, this attention model does not func-
tionally play the role of a word alignment between
the source in the target, at least not in the same
way as its analog in statistical machine translation.
While in both cases, alignment is a latent variable
that is used to obtain probability distributions over
words or phrases, arguably the attention model has
a broader role. For instance, when translating a
verb, attention may also be paid to its subject and
object since these may disambiguate it. To fur-
ther complicate matters, the word representations
are products of bidirectional gated recurrent neu-
ral networks that have the effect that each word
representation is informed by the entire sentence
context.

But there is a clear need for an alignment mech-
anism between source and target words. For in-
stance, prior work used the alignments provided
by the attention model to interpolate word transla-
tion decisions with traditional probabilistic dictio-
naries (Arthur et al., 2016), for the introduction of
coverage and fertility models (Tu et al., 2016), etc.

But is the attention model in fact the proper

re
la

tio
ns

be
tw

ee
n

O
ba

m
a

an
d

N
et

an
ya

hu

ha
ve

be
en

st
ra

in
ed

fo
r

ye
ar

s
.

die
Beziehungen

zwischen

Obama

und
Netanjahu

sind

seit

Jahren
angespannt

.

56

89

72

16

26

96

79

98

42

11

11

14

38

22

84

23

54 10

98

49

Figure 8: Word alignment for English–German:
comparing the attention model states (green boxes
with probability in percent if over 10) with align-
ments obtained from fast-align (blue outlines).

means? To examine this, we compare the soft
alignment matrix (the sequence of attention vec-
tors) with word alignments obtained by traditional
word alignment methods. We use incremental
fast-align (Dyer et al., 2013) to align the input and
output of the neural machine system.

See Figure 8 for an illustration. We compare
the word attention states (green boxes) with the
word alignments obtained with fast align (blue
outlines). For most words, these match up pretty
well. Both attention states and fast-align align-
ment points are a bit fuzzy around the function
words have-been/sind.

However, the attention model may settle on
alignments that do not correspond with our intu-
ition or alignment points obtained with fast-align.
See Figure 9 for the reverse language direction,
German–English. All the alignment points appear
to be off by one position. We are not aware of any
intuitive explanation for this divergent behavior —
the translation quality is high for both systems.

We measure how well the soft alignment (atten-
tion model) of the NMT system match the align-
ments of fast-align with two metrics:

• a match score that checks for each output
if the aligned input word according to fast-
align is indeed the input word that received
the highest attention probability, and

• a probability mass score that sums up the

34



da
s

V
er

hä
ltn

is

zw
is

ch
en

O
ba

m
a

un
d

N
et

an
ya

hu

is
t

se
it

Ja
hr

en
ge

sp
an

nt
.

the
relationship

between

Obama

and
Netanyahu

has

been

stretched

for
years

. 11

47

81

72

87

93

95

38

21

17

16

14

38

19

33

90

32

26

54

77

12

17

Figure 9: Mismatch between attention states and
desired word alignments (German–English).

probability mass given to each alignment
point obtained from fast-align.

In these scores, we have to handle byte pair encod-
ing and many-to-many alignments11

In out experiment, we use the neural machine
translation models provided by Edinburgh12 (Sen-
nrich et al., 2016a). We run fast-align on the same
parallel data sets to obtain alignment models and
used them to align the input and output of the
NMT system. Table 3 shows alignment scores for
the systems. The results suggest that, while dras-
tic, the divergence for German–English is an out-
lier. We note, however, that we have seen such
large a divergence also under different data condi-
tions.

Note that the attention model may produce bet-
ter word alignments by guided alignment training
(Chen et al., 2016; Liu et al., 2016) where super-
vised word alignments (such as the ones produced
by fast-align) are provided to model training.

11(1) NMT operates on subwords, but fast-align is run on
full words. (2) If an input word is split into subwords by
byte pair encoding, then we add their attention scores. (3)
If an output word is split into subwords, then we take the
average of their attention vectors. (4) The match scores and
probability mass scores are computed as average over output
word-level scores. (5) If an output word has no fast-align
alignment point, it is ignored in this computation. (6) If an
output word is fast-aligned to multiple input words, then (6a)
for the match score: count it as correct if the n aligned words
among the top n highest scoring words according to attention
and (6b) for the probability mass score: add up their attention
scores.

12https://github.com/rsennrich/wmt16-scripts

Language Pair Match Prob.
German–English 14.9% 16.0%
English–German 77.2% 63.2%
Czech–English 78.0% 63.3%
English–Czech 76.1% 59.7%
Russian–English 72.5% 65.0%
English–Russian 73.4% 64.1%

Table 3: Scores indicating overlap between at-
tention probabilities and alignments obtained with
fast-align.

3.6 Beam Search

The task of decoding is to find the full sentence
translation with the highest probability. In statis-
tical machine translation, this problem has been
addressed with heuristic search techniques that ex-
plore a subset of the space of possible translation.
A common feature of these search techniques is a
beam size parameter that limits the number of par-
tial translations maintained per input word.

There is typically a straightforward relationship
between this beam size parameter and the model
score of resulting translations and also their qual-
ity score (e.g., BLEU). While there are dimin-
ishing returns for increasing the beam parameter,
typically improvements in these scores can be ex-
pected with larger beams.

Decoding in neural translation models can be
set up in similar fashion. When predicting the next
output word, we may not only commit to the high-
est scoring word prediction but also maintain the
next best scoring words in a list of partial trans-
lations. We record with each partial translation
the word translation probabilities (obtained from
the softmax), extend each partial translation with
subsequent word predictions and accumulate these
scores. Since the number of partial translation ex-
plodes exponentially with each new output word,
we prune them down to a beam of highest scoring
partial translations.

As in traditional statistical machine translation
decoding, increasing the beam size allows us to
explore a larger set of the space of possible transla-
tion and hence find translations with better model
scores.

However, as Figure 10 illustrates, increasing the
beam size does not consistently improve transla-
tion quality. In fact, in almost all cases, worse
translations are found beyond an optimal beam
size setting (we are using again Edinburgh’s WMT

35



1 2 4 8 12 20 30 50 100 200 500 1,000

29

30

31

29.7

30.4 30.5 30.430.3
30

29.8

29.4

28.5

29.7

30.4
30.6

30.830.930.930.930.9 30.9 30.7

30.3

29.9

Beam Size

B
L

E
U

Czech–English

Unnormalized
Normalized

1 2 4 8 12 20 30 50 100 200 500 1,000

20

21

22

23

24

22

23.2

23.9 24
24.124.124

23.8
23.5

22.7

19.9

23.1

23.6
23.8

24.1 24.2 24 23.9
23.6

23.2

Beam Size

B
L

E
U

English-Czech

Unnormalized
Normalized

1 2 4 8 12 20 30 50 100 200 500 1 000

35

36

37

35.7

36.4

36.9 36.936.836.736.6
36.3

36.1

35.7

34.6

35.7

36.6

37.2
37.537.537.637.637.6 37.6 37.6 37.6 37.6

Beam Size

B
L

E
U

German–English

Unnormalized
Normalized

1 2 4 8 12 20 30 50 100 200 500 1 000

27

28

29

26.8

27.9

28.4 28.428.528.528.528.4
28.1

27.6

26.726.8

28

28.6
28.929

29.129.129.2 29.2 29.2 29.1

28.7

Beam Size

B
L

E
U

English–German

Unnormalized
Normalized

1 2 4 8 12 20 30 50 100 200 500 1 000
15

16

17

15.8

16.4
16.6 16.616.7

16.916.916.9

17.3

16
15.8

16.4 16.5 16.416.416.416.416.3 16.2

15.9

15.6

15.3

Beam Size

B
L

E
U

Romanian–English

Unnormalized
Normalized

1 2 4 8 12 20 30 50 100 200 500 1 000

24

25

26

25.4
25.5

25.6 25.6
25.825.825.825.8 25.8

25.6

24.7

24

25.4
25.6 25.6

25.7
25.6

25.7
25.6

25.7
25.6 25.6 25.6 25.6

Beam Size

B
L

E
U

English–Romanian

Unnormalized
Normalized

1 2 4 8 12 20 30 50 100 200 500 1 000

25

26

27

25.5

26.6
26.9

26.6
26.4

25.9

25.5

24.1

25.5

26.9

27.5
27.727.827.827.727.7 27.6

27.1

25.9

24.8

Beam Size

B
L

E
U

Russian–English

Unnormalized
Normalized

1 2 4 8 12 20 30 50 100 200 500 1 000

20

21

22

23

19.9

21.8

22.3
22.622.5

22.222.2
21.9

21.4

20.7

19.9

21.7

22.1
22.422.522.422.422.4 22.3

22.1
21.8

21.3

Beam Size

B
L

E
U

English–Russian

Unnormalized
Normalized

Figure 10: Translation quality with varying beam sizes. For large beams, quality decreases, especially
when not normalizing scores by sentence length.

36



2016 systems). The optimal beam size varies from
4 (e.g., Czech–English) to around 30 (English–
Romanian).

Normalizing sentence level model scores by
length of the output alleviates the problem some-
what and also leads to better optimal quality in
most cases (5 of the 8 language pairs investigated).
Optimal beam sizes are in the range of 30–50 in
almost all cases, but quality still drops with larger
beams. The main cause of deteriorating quality are
shorter translations under wider beams.

4 Conclusions

We showed that, despite its recent successes, neu-
ral machine translation still has to overcome vari-
ous challenges, most notably performance out-of-
domain and under low resource conditions. We
hope that this paper motivates research to address
these challenges.

What a lot of the problems have in common
is that the neural translation models do not show
robust behavior when confronted with conditions
that differ significantly from training conditions —
may it be due to limited exposure to training data,
unusual input in case of out-of-domain test sen-
tences, or unlikely initial word choices in beam
search. The solution to these problems may hence
lie in a more general approach of training that
steps outside optimizing single word predictions
given perfectly matching prior sequences.

Acknowledgment

This work was partially supported by a Amazon
Research Award (to the first author) and a Na-
tional Science Foundation Graduate Research Fel-
lowship under Grant No. DGE-1232825 (to the
second author).

References

Philip Arthur, Graham Neubig, and Satoshi Nakamura.
2016. Incorporating discrete translation lexicons
into neural machine translation. In Proceedings of
the 2016 Conference on Empirical Methods in Natu-
ral Language Processing. Association for Computa-
tional Linguistics, Austin, Texas, pages 1557–1567.
https://aclweb.org/anthology/D16-1162.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua
Bengio. 2015. Neural machine translation by
jointly learning to align and translate. In ICLR.
http://arxiv.org/pdf/1409.0473v6.pdf.

Luisa Bentivogli, Arianna Bisazza, Mauro Cettolo, and
Marcello Federico. 2016. Neural versus phrase-
based machine translation quality: a case study. In
Proceedings of the 2016 Conference on Empirical
Methods in Natural Language Processing. Associ-
ation for Computational Linguistics, Austin, Texas,
pages 257–267. https://aclweb.org/anthology/D16-
1025.

Ondřej Bojar, Rajen Chatterjee, Christian Feder-
mann, Yvette Graham, Barry Haddow, Matthias
Huck, Antonio Jimeno Yepes, Philipp Koehn,
Varvara Logacheva, Christof Monz, Matteo Negri,
Aurelie Neveol, Mariana Neves, Martin Popel,
Matt Post, Raphael Rubino, Carolina Scarton,
Lucia Specia, Marco Turchi, Karin Verspoor,
and Marcos Zampieri. 2016. Findings of the
2016 conference on machine translation. In
Proceedings of the First Conference on Ma-
chine Translation. Association for Computational
Linguistics, Berlin, Germany, pages 131–198.
http://www.aclweb.org/anthology/W/W16/W16-
2301.

Wenhu Chen, Evgeny Matusov, Shahram Khadivi,
and Jan-Thorsten Peter. 2016. Guided align-
ment training for topic-aware neural ma-
chine translation. CoRR abs/1607.01628.
http://arxiv.org/abs/1607.01628.

David Chiang. 2007. Hierarchical phrase-based
translation. Computational Linguistics 33(2).
http://www.aclweb.org/anthology-new/J/J07/J07-
2003.pdf.

Kyunghyun Cho, Bart van Merrienboer, Dzmitry Bah-
danau, and Yoshua Bengio. 2014. On the proper-
ties of neural machine translation: Encoder–decoder
approaches. In Proceedings of SSST-8, Eighth
Workshop on Syntax, Semantics and Structure in
Statistical Translation. Association for Computa-
tional Linguistics, Doha, Qatar, pages 103–111.
http://www.aclweb.org/anthology/W14-4012.

Josep Maria Crego, Jungi Kim, Guillaume Klein, An-
abel Rebollo, Kathy Yang, Jean Senellart, Egor
Akhanov, Patrice Brunelle, Aurelien Coquard,
Yongchao Deng, Satoshi Enoue, Chiyo Geiss,
Joshua Johanson, Ardas Khalsa, Raoum Khiari,
Byeongil Ko, Catherine Kobus, Jean Lorieux, Leid-
iana Martins, Dang-Chuan Nguyen, Alexandra Pri-
ori, Thomas Riccardi, Natalia Segal, Christophe Ser-
van, Cyril Tiquet, Bo Wang, Jin Yang, Dakun Zhang,
Jing Zhou, and Peter Zoldan. 2016. Systran’s
pure neural machine translation systems. CoRR
abs/1610.05540. http://arxiv.org/abs/1610.05540.

Shuoyang Ding, Kevin Duh, Huda Khayrallah,
Philipp Koehn, and Matt Post. 2016a. The jhu
machine translation systems for wmt 2016. In
Proceedings of the First Conference on Ma-
chine Translation. Association for Computational
Linguistics, Berlin, Germany, pages 272–280.
http://www.aclweb.org/anthology/W/W16/W16-
2310.

37



Shuoyang Ding, Kevin Duh, Huda Khayrallah, Philipp
Koehn, and Matt Post. 2016b. The JHU machine
translation systems for WMT 2016. In Proceed-
ings of the First Conference on Machine Translation
(WMT).

Chris Dyer, Victor Chahuneau, and Noah A. Smith.
2013. A simple, fast, and effective reparameteriza-
tion of ibm model 2. In Proceedings of the 2013
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies. Association for Computa-
tional Linguistics, Atlanta, Georgia, pages 644–648.
http://www.aclweb.org/anthology/N13-1073.

Markus Freitag and Yaser Al-Onaizan. 2016. Fast
domain adaptation for neural machine translation.
arXiv preprint arXiv:1612.06897 .

Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Igna-
cio Thayer. 2006. Scalable inference and train-
ing of context-rich syntactic translation models.
In Proceedings of the 21st International Confer-
ence on Computational Linguistics and 44th An-
nual Meeting of the Association for Computa-
tional Linguistics. Association for Computational
Linguistics, Sydney, Australia, pages 961–968.
http://www.aclweb.org/anthology/P/P06/P06-1121.

Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What’s in a translation rule?
In Proceedings of the Joint Conference on Hu-
man Language Technologies and the Annual Meet-
ing of the North American Chapter of the Associ-
ation of Computational Linguistics (HLT-NAACL).
http://www.aclweb.org/anthology/N04-1035.pdf.

Ann Irvine and Chris Callison-Burch. 2013. Com-
bining bilingual and comparable corpora for
low resource machine translation. In Pro-
ceedings of the Eighth Workshop on Statistical
Machine Translation. Association for Computa-
tional Linguistics, Sofia, Bulgaria, pages 262–270.
http://www.aclweb.org/anthology/W13-2233.

Marcin Junczys-Dowmunt, Tomasz Dwojak, and Hieu
Hoang. 2016. Is neural machine translation ready
for deployment? a case study on 30 translation di-
rections. In Proceedings of the International Work-
shop on Spoken Language Translation (IWSLT).
http://workshop2016.iwslt.org/downloads/IWSLT
2016 paper 4.pdf.

Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
continuous translation models. In Proceedings of
the 2013 Conference on Empirical Methods in Natu-
ral Language Processing. Association for Computa-
tional Linguistics, Seattle, Washington, USA, pages
1700–1709. http://www.aclweb.org/anthology/D13-
1176.

Philipp Koehn and Barry Haddow. 2012. Interpolated
backoff for factored translation models. In Proceed-
ings of the Tenth Conference of the Association for
Machine Translation in the Americas (AMTA).

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Christopher J. Dyer, Ondřej Bo-
jar, Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine
translation. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics
Companion Volume Proceedings of the Demo and
Poster Sessions. Association for Computational Lin-
guistics, Prague, Czech Republic, pages 177–180.
http://www.aclweb.org/anthology/P/P07/P07-2045.

Lemao Liu, Masao Utiyama, Andrew Finch, and
Eiichiro Sumita. 2016. Neural machine trans-
lation with supervised attention. In Proceed-
ings of COLING 2016, the 26th International
Conference on Computational Linguistics: Tech-
nical Papers. The COLING 2016 Organizing
Committee, Osaka, Japan, pages 3093–3102.
http://aclweb.org/anthology/C16-1291.

Minh-Thang Luong and Christopher D Manning. 2015.
Stanford neural machine translation systems for spo-
ken language domains. In Proceedings of the In-
ternational Workshop on Spoken Language Transla-
tion.

Thang Luong, Ilya Sutskever, Quoc Le, Oriol Vinyals,
and Wojciech Zaremba. 2015. Addressing the
rare word problem in neural machine transla-
tion. In Proceedings of the 53rd Annual Meet-
ing of the Association for Computational Lin-
guistics and the 7th International Joint Con-
ference on Natural Language Processing (Vol-
ume 1: Long Papers). Association for Computa-
tional Linguistics, Beijing, China, pages 11–19.
http://www.aclweb.org/anthology/P15-1002.

Jean Pouget-Abadie, Dzmitry Bahdanau, Bart van
Merrienboer, KyungHyun Cho, and Yoshua Ben-
gio. 2014. Overcoming the curse of sentence
length for neural machine translation using au-
tomatic segmentation. CoRR abs/1409.1257.
http://arxiv.org/abs/1409.1257.

Rico Sennrich, Orhan Firat, Kyunghyun Cho, Alexan-
dra Birch, Barry Haddow, Julian Hitschler, Marcin
Junczys-Dowmunt, Samuel Läubli, Antonio Vale-
rio Miceli Barone, Jozef Mokry, and Maria Nade-
jde. 2017. Nematus: a toolkit for neural ma-
chine translation. In Proceedings of the Soft-
ware Demonstrations of the 15th Conference of
the European Chapter of the Association for Com-
putational Linguistics. Association for Computa-
tional Linguistics, Valencia, Spain, pages 65–68.
http://aclweb.org/anthology/E17-3017.

Rico Sennrich, Barry Haddow, and Alexandra
Birch. 2016a. Edinburgh neural machine trans-
lation systems for WMT 16. In Proceedings
of the First Conference on Machine Transla-
tion (WMT). Association for Computational
Linguistics, Berlin, Germany, pages 371–376.

38



http://www.aclweb.org/anthology/W/W16/W16-
2323.

Rico Sennrich, Barry Haddow, and Alexandra
Birch. 2016b. Neural machine translation of
rare words with subword units. In Proceed-
ings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics (Volume
1: Long Papers). Association for Computational
Linguistics, Berlin, Germany, pages 1715–1725.
http://www.aclweb.org/anthology/P16-1162.

Jörg Tiedemann. 2012. Parallel data, tools and in-
terfaces in opus. In Nicoletta Calzolari (Con-
ference Chair), Khalid Choukri, Thierry Declerck,
Mehmet Ugur Dogan, Bente Maegaard, Joseph
Mariani, Jan Odijk, and Stelios Piperidis, edi-
tors, Proceedings of the Eight International Con-
ference on Language Resources and Evaluation
(LREC’12). European Language Resources Associ-
ation (ELRA), Istanbul, Turkey.

Antonio Toral and Vı́ctor M. Sánchez-Cartagena. 2017.
A multifaceted evaluation of neural versus phrase-
based machine translation for 9 language directions.
In Proceedings of the 15th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics: Volume 1, Long Papers. Association for
Computational Linguistics, Valencia, Spain, pages
1063–1073. http://www.aclweb.org/anthology/E17-
1100.

Zhaopeng Tu, Zhengdong Lu, Yang Liu, Xiao-
hua Liu, and Hang Li. 2016. Modeling cov-
erage for neural machine translation. In Pro-
ceedings of the 54th Annual Meeting of the
Association for Computational Linguistics (Vol-
ume 1: Long Papers). Association for Computa-
tional Linguistics, Berlin, Germany, pages 76–85.
http://www.aclweb.org/anthology/P16-1008.

Marco Turchi, Tijl De Bie, and Nello Cristianini. 2008.
Learning performance of a machine translation
system: a statistical and computational analysis. In
Proceedings of the Third Workshop on Statistical
Machine Translation. Association for Computa-
tional Linguistics, Columbus, Ohio, pages 35–43.
http://www.aclweb.org/anthology/W/W08/W08-
0305.

Philip Williams, Rico Sennrich, Maria Nadejde,
Matthias Huck, Barry Haddow, and Ondřej
Bojar. 2016. Edinburgh’s statistical machine
translation systems for wmt16. In Proceed-
ings of the First Conference on Machine
Translation. Association for Computational
Linguistics, Berlin, Germany, pages 399–410.
http://www.aclweb.org/anthology/W/W16/W16-
2327.

Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V.
Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus
Macherey, Jeff Klingner, Apurva Shah, Melvin
Johnson, Xiaobing Liu, Lukasz Kaiser, Stephan

Gouws, Yoshikiyo Kato, Taku Kudo, Hideto
Kazawa, Keith Stevens, George Kurian, Nishant
Patil, Wei Wang, Cliff Young, Jason Smith, Ja-
son Riesa, Alex Rudnick, Oriol Vinyals, Greg
Corrado, Macduff Hughes, and Jeffrey Dean.
2016. Google’s neural machine translation sys-
tem: Bridging the gap between human and
machine translation. CoRR abs/1609.08144.
http://arxiv.org/abs/1609.08144.pdf.

39


