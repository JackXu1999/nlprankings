



















































Argument Mining for Understanding Peer Reviews


Proceedings of NAACL-HLT 2019, pages 2131–2137
Minneapolis, Minnesota, June 2 - June 7, 2019. c©2019 Association for Computational Linguistics

2131

Argument Mining for Understanding Peer Reviews

Xinyu Hua, Mitko Nikolov, Nikhil Badugu, Lu Wang
Khoury College of Computer Sciences

Northeastern University
Boston, MA 02115

{hua.x, nikolov.m, badugu.n}@husky.neu.edu
luwang@ccs.neu.edu

Abstract

Peer-review plays a critical role in the scien-
tific writing and publication ecosystem. To as-
sess the efficiency and efficacy of the review-
ing process, one essential element is to un-
derstand and evaluate the reviews themselves.
In this work, we study the content and struc-
ture of peer reviews under the argument min-
ing framework, through automatically detect-
ing (1) argumentative propositions put forward
by reviewers, and (2) their types (e.g., evalu-
ating the work or making suggestions for im-
provement). We first collect 14.2K reviews
from major machine learning and natural lan-
guage processing venues. 400 reviews are an-
notated with 10, 386 propositions and corre-
sponding types of EVALUATION, REQUEST,
FACT, REFERENCE, or QUOTE. We then
train state-of-the-art proposition segmentation
and classification models on the data to evalu-
ate their utilities and identify new challenges
for this new domain, motivating future di-
rections for argument mining. Further ex-
periments show that proposition usage varies
across venues in amount, type, and topic.

1 Introduction

Peer review is a process where domain experts
scrutinize the quality of research work in their
field, and it is a cornerstone of scientific discov-
ery (Hettich and Pazzani, 2006; Kelly et al., 2014;
Price and Flach, 2017). In 2015 alone, approxi-
mately 63.4 million hours were spent on peer re-
views (Kovanis et al., 2016). To maximize their
benefit to the scientific community, it is crucial to
understand and evaluate the construction and lim-
itation of reviews themselves. However, minimal
work has been done to analyze reviews’ content
and structure, let alone to evaluate their qualities.

As seen in Figure 1, peer reviews resemble
arguments: they contain argumentative propo-
sitions (henceforth propositions) that convey re-

Review #1 (rating: 5, # sentences: 11)
[Quality: This paper demonstrates that convolutional and re-
lational neural networks fail to solve visual relation prob-
lems . . . ]FACT [This points at important limitations of cur-
rent neural network architectures where architectures depend
mainly on rote memorization.]EVAL . . . [Significance: This
work demonstrates failures of relational networks on relational
tasks. . .]FACT [Pros: Important message about network limita-
tions.]EVAL [Cons: Straightforward testing of network perfor-
mance on specific visual relation tasks.]EVAL . . .
Review #2 (rating: 5, # sentences: 10)
[The authors present two autoregressive models . . .]FACT. . . [In
that context , this work can be viewed as applying deep autore-
gressive density estimators to policy gradient methods.]EVAL. . .
[At least one of those papers ought to be cited.]REQ [It also seems
like a simple, obvious baseline is missing from their experi-
ments . . .]EVAL. . . [The method could even be made to capture
dependencies between different actions by adding a latent prob-
abilistic layer . . .]EVAL. . .[A direct comparison against one of the
related methods in the discussion section would help]REQ. . .

Figure 1: Sample ICLR review excerpts. Propositions
are annotated with types, such as FACT (fact), EVAL
(evaluation), and REQ (request). Review #2 contains
in-depth evaluation and actionable suggestion, thus is
perceived to be of a higher quality.

viewers’ interpretation and evaluation of the re-
search. Constructive reviews, e.g., review #2, of-
ten contain in-depth analysis as well as concrete
suggestions. As a result, automatically identify-
ing propositions and their types would be useful
to understand the composition of peer reviews.

Therefore, we propose an argument mining-
based approach to understand the content and
structure of peer reviews. Argument mining
studies the automatic detection of argumentative
components and structure within discourse (Peld-
szus and Stede, 2013). Specifically, argument
types (e.g. evidence and reasoning) and their
arrangement are indicative of argument qual-
ity (Habernal and Gurevych, 2016; Wachsmuth
et al., 2017). In this work, we focus on two specific
tasks: (1) proposition segmentation—detecting
elementary argumentative discourse units that are



2132

propositions, and (2) proposition classification—
labeling the propositions according to their types
(e.g., evaluation vs. request).

Since there was no annotated dataset for peer re-
views, as part of this study, we first collect 14.2K
reviews from major machine learning (ML) and
natural language processing (NLP) venues. We
create a dataset, AMPERE (Argument Mining for
PEer REviews), by annotating 400 reviews with
10, 386 propositions and labeling each proposition
with the type of EVALUATION, REQUEST, FACT,
REFERENCE, QUOTE, or NON-ARG.1 Significant
inter-annotator agreement is achieved for propo-
sition segmentation (Cohen’s κ = 0.93), with
good consensus level for type annotation (Krip-
pendorf’s αU = 0.61).

We benchmark our new dataset with state-of-
the-art and popular argument mining models to
better understand the challenges posed in this new
domain. We observe a significant drop of perfor-
mance for proposition segmentation on AMPERE,
mainly due to its different argument structure. For
instance, 25% of the sentences contain more than
one proposition, compared to that of 8% for es-
says (Stab and Gurevych, 2017), motivating new
solutions for segmentation and classification.

We further investigate review structure differ-
ence across venues based on proposition usage,
and uncover several patterns. For instance, ACL
reviews tend to contain more propositions than
those in ML venues, especially with more re-
quests but fewer facts. We further find that re-
views with extreme ratings, i.e., strong reject or
accept, tend to be shorter and make much fewer
requests. Moreover, we probe the salient words
for different proposition types. For example, ACL
reviewers ask for more “examples” when making
requests, while ICLR reviews contain more evalu-
ation of “network” and how models are “trained”.

2 AMPERE Dataset

We collect review data from three sources: (1)
openreview.net—an online peer reviewing
platform for ICLR 2017, ICLR 2018, and UAI
2018 2; (2) reviews released for accepted papers
at NeurIPS from 2013 to 2017; and (3) opted-in
reviews for ACL 2017 from Kang et al. (2018).

1Dataset and annotation guideline can be found at http:
//xinyuhua.github.io/Resources/naacl19/.

2ICLR reviews are downloaded from the public API:
https://github.com/iesl/openreview-py.
UAI reviews are collected by the OpenReview team.

EVALUATION: Subjective statements, often containing qual-
itative judgment. Ex: “This paper shows nice results on a
number of small tasks.”
REQUEST: Statements suggesting a course of action. Ex:
“The authors should compare with the following methods.”
FACT: Objective information of the paper or commonsense
knowledge. Ex: “Existing works on multi-task neural net-
works typically use hand-tuned weights. . .”
REFERENCE: Citations and URLs. Ex: “see MuseGAN
(Dong et al), MidiNet (Yang et al), etc ”
QUOTE: Quotations from the paper. Ex: “The author wrote
‘where r is lower bound of feature norm’.”
NON-ARG: Non-argumentative statements. Ex: “Aha, now I
understand.”

Table 1: Proposition types and examples.

Dataset #Doc #Sent #Prop
Comments (Park and Cardie, 2018) 731 3,994 4,931
Essays (Stab and Gurevych, 2017) 402 7,116 6,089
News (Al Khatib et al., 2016) 300 11,754 14,313
Web (Habernal and Gurevych, 2017) 340 3,899 1,882
AMPERE 400 8,030 10,386

Table 2: Statistics for AMPERE and some argument
mining corpora, including # of annotated propositions.

In total, 14, 202 reviews are collected (ICLR:
4, 057; UAI: 718; ACL: 275; and NeurIPS:
9, 152). All venues except NeurIPS have paper
rating scores attached to the reviews.

Annotation Process. For proposition segmenta-
tion, we adopt the concepts from Park et al. (2015)
and instruct the annotators to identify elementary
argumentative discourse units on sentence or sub-
sentence level, based on their discourse functions
and topics. They then classify the propositions
into five types with an additional non-argument
category, as explained in Table 1.
400 ICLR 2018 reviews are sampled for annota-

tion, with similar distributions of length and rating
to those of the full dataset. Two annotators who
are fluent English speakers first label the 400 re-
views with proposition segments and types, and a
third annotator then resolves disagreements.

We calculate the inter-annotator agreement be-
tween the two annotators. A Cohen’s κ of 0.93 is
achieved for proposition segmentation, with each
review treated as a BIO sequence. For classifi-
cation, unitized Krippendorf’s αU (Krippendorff,
2004), which considers disagreements among seg-
mentation, is calculated per review and then av-
eraged over all samples, and the value is 0.61.
Among the exactly matched proposition segments,
we report a Cohen’s κ of 0.64.

Statistics. Table 2 shows comparison be-
tween AMPERE and some other argument min-

openreview.net
http://xinyuhua.github.io/Resources/naacl19/
http://xinyuhua.github.io/Resources/naacl19/
https://github.com/iesl/openreview-py


2133

ing datasets of different genres. We also show the
number of propositions in each category in Table
3. The most frequent types are evaluation (38.3%)
and fact (36.5%).

EVAL REQ FACT REF QUOT NON-A Total
3,982 1,911 3,786 207 161 339 10,386

Table 3: Number of propositions per type in AMPERE.

3 Experiments with Existing Models

We benchmark AMPERE with popular and state-
of-the-art models for proposition segmentation
and classification. Both tasks can be treated as se-
quence tagging problems with the setup similar to
Schulz et al. (2018). For experiments, 320 reviews
(7, 999 propositions) are used for training and 80
reviews (2, 387 propositions) are used for testing.
Following Niculae et al. (2017), 5-fold cross vali-
dation on the training set is used for hyperparam-
eter tuning. To improve the accuracy of tokeniza-
tion, we manually replace mathematical formulas,
variables, URL links, and formatted citation with
special tokens such as <EQN>, <VAR>, <URL>,
and <CIT>. Parameters, lexicons, and features
used for the models are described in the supple-
mentary material.

3.1 Task I: Proposition Segmentation
We consider three baselines. FullSent: treat-
ing each sentence as a proposition. PDTB-conn:
further segmenting sentences when any discourse
connective (collected from Penn Discourse Tree-
bank (Prasad et al., 2007)) is observed. RST-
parser: segmenting discourse units by the RST
parser in Feng and Hirst (2014).

For learning-based methods, we start with Con-
ditional Random Field (CRF) (Lafferty et al.,
2001) with features proposed by Stab and
Gurevych ((2017), Table 7), and BiLSTM-CRF,
a bidirectional Long Short-Term Memory network
(BiLSTM) connected to a CRF output layer and
further enhanced with ELMo representation (Pe-
ters et al., 2018). We adopt the BIO scheme for
sequential tagging (Ramshaw and Marcus, 1999),
with O corresponding to NON-ARG. Finally, we
consider jointly modeling segmentation and clas-
sification by appending the proposition types to
BI tags, e.g., B-fact, with CRF (CRF-joint) and
BiLSTM-CRF (BiLSTM-CRF-joint).

Table 4 shows that BiLSTM-CRF outperforms
other methods in F1. More importantly, the perfor-

Prec. Rec. F1
FullSent 73.68 56.00 63.64
PDTB-conn 51.11 49.71 50.40
RST-parser 30.28 43.00 35.54
CRF 66.53 52.92 58.95
BiLSTM-CRF 82.25 79.96 81.09∗

CRF-joint 74.99 63.33 68.67
BiLSTM-CRF-joint 81.12 78.42 79.75

Table 4: Proposition segmentation results. Result that
is significantly better than all comparisons is marked
with ∗ (p < 10−6, McNemar test).

Overall EVAL REQ FACT REF QUOT
With Gold-Standard Segments
Majority 40.75 57.90 – – – –
PropLexicon 36.83 40.42 36.07 32.23 59.57 31.28
SVM 60.98 63.88 69.02 54.74 69.47 7.69
CNN 66.56∗ 69.02 63.26 66.17 67.44 52.94

With Predicted Segments
Majority 33.30 47.60 – – – –
PropLexicon 23.21 22.45 23.97 23.73 35.96 16.67
SVM 51.46 54.05 48.16 52.77 52.27 4.71
CNN 55.48 57.75 53.71 55.19 48.78 33.33
CRF-joint 50.69 46.78 55.74 52.27 55.77 26.47
BiLSTM-CRF-
joint

62.64∗ 62.36∗ 67.31∗ 61.86 54.74 37.36

Table 5: Proposition classification F1 scores. Re-
sults that are significant better than other methods are
marked with ∗ (p < 10−6, McNemar test).

mance on reviews is lower than those reached on
existing datasets, e.g., an F1 of 86.7 is obtained by
CRF for essays (Stab and Gurevych, 2017). This
is mostly due to essays’ better structure, with fre-
quent use of discourse connectives.

3.2 Task II: Proposition Classification
With given proposition segments, predicted or
gold-standard, we experiment with proposition-
level models to label proposition types.

We utilize two baselines. Majority simply as-
signs the majority type in the training set. Pro-
pLexicon matches the following lexicons for dif-
ferent proposition types in order, and returns the
first corresponding type with a match; if no lexi-
con is matched, the proposition is labeled as NON-
ARG:

• REFERENCE: <URL>, <CIT>
• QUOTE: “, ”, ’
• REQUEST: should, would be nice, why, please, would

like to, need
• EVALUATION: highly, very, unclear, clear, interesting,

novel, well, important, similar, clearly, quite, good
• FACT: author, authors, propose, present, method, pa-

rameters, example, dataset, same, incorrect, correct



2134

For supervised models, we employ linear SVM
with a squared hinge loss and group Lasso regu-
larizer (Yuan and Lin, 2006). It is trained with the
top 500 features selected from Table 9 in (Stab and
Gurevych, 2017) by χ2 test. We also train a convo-
lutional neural network (CNN) proposed by Kim
(2014), with the same setup and pre-trained
word embeddings from word2vec (Mikolov et al.,
2013). Finally, results by joint models of CRF and
BiLSTM-CRF are also reported.

F1 scores for all propositions and each type are
reported in Table 5. A prediction is correct when
both segment and type are matched with the true
labels. CNN performs better for types with sig-
nificantly more training samples, i.e., evaluation
and fact, indicating the effect of data size on neu-
ral model’s performance. Joint models (CRF-joint
and BiLSTM-CRF-joint) yield the best F1 scores
for all categories when gold-standard segmenta-
tion is unavailable.

4 Proposition Analysis by Venues

Here we leverage the BiLSTM-CRF-joint model
trained on the annotated AMPERE data to iden-
tify propositions and their types in unlabeled re-
views from the four venues (ICLR, UAI, ACL, and
NeurIPS), to understand the content and structure
of peer reviews at a larger scale.
Proposition Usage by Venue and Rating. Fig-
ure 2 shows the average number of propositions
per review, grouped by venue and rating. Scores
in 1 − 10 are scaled to 1 − 5 by dx/2e, with 1
as strong reject and 5 as strong accept. ACL and
NeurIPS have significantly more propositions than
ICLR and UAI. Ratings, which reflect a reviewer’s
judgment of paper quality, also affect proposition
usage. We find that reviews with extreme ratings,
i.e., 1 and 5, tend to have fewer propositions.

ICLR UAI ACL NeurIPS
0

10
20
30
40
50
60
70 # Propositions per Review

1 2 3 4 50
10

20

30

40

# Propositions by Rating and Venue
ICLR UAI ACL

Figure 2: Proposition number in reviews. Differences
among venues are all significant except UAI vs. ICLR
and ACL vs. NeurIPS (p < 10−6, unpaired t-test).

We further study the distribution of proposition
type in each venue. As observed in Figure 3, ACL

Evaluation Request Fact Reference Quote Non-Arg0

10

20

30

40
% of Proposition Type

ICLR
UAI

ACL
NeurIPS

Figure 3: Distribution of proposition type per venue.

1 2 3 4 50

15

30

45 Evaluation

1 2 3 4 50

10

20

Request

1 2 3 4 50

10

20

30

40 Fact

1 2 3 4 50

1

2

3 Reference

1 2 3 4 50.0
0.5
1.0
1.5
2.0
2.5 Quote

1 2 3 4 50

5

10

15 Non-Arg

Figure 4: Distribution of proposition type per rating (in
%) on AMPERE.

reviews contain more requests but fewer facts than
other venues. Specifically, we find that 94.6% of
ACL reviews have at least one REQUEST propo-
sition, compared to 81.5% for ICLR and 84.7%
for UAI. We also show proposition type distribu-
tion based on ratings in Figure 4. Reviews with
the highest rating tend to use fewer evaluation and
reference, while reviews with ratings of 3−4 (bor-
derline or weak accept) contain more requests. We
further observe a sharp decrease of QUOTE usage
in rating group 4, and a surge of NON-ARG for rat-
ing group 5, while FACT remains consistent across
rating ranges.

Proposition Structure. Argumentative structure,
which is usually studied as support and attack
relations, reveals how propositions are organized
into coherent text. According to Park and Cardie
(2018), 75% of support relations happen between
adjacent propositions in user comments. We thus
plot the proposition transition probability matrix
in Figure 5, to show the argument structure in AM-
PERE. The high probabilities along the diagonal
line imply that propositions of the same type are
often constructed consecutively, with the excep-
tion of quote, which is more likely to be followed
by evaluation.

Proposition Type and Content. We also probe
the salient words used for each proposition type,
and the difference of their usage across venues.
For each venue, we utilize log-likelihood ratio
test (Lin and Hovy, 2000) to identify the represen-



2135

EVALUATION REQUEST FACT REFERENCE QUOTE
All Venues overall, unclear, not,

contribution, seem, in-
teresting

please, could, should,
if, why, would, more,
suggest

think, each, some,
data, useful, written,
proposes

<URL>, et, al., confer-
ence, paper, proceed-
ings, arxiv

”, paper, we, :, our

ICLR network, general, ac-
ceptance, convinced,
trained

network, appendix,
recommend, because,
novelty

training, results, work,
then, image

deep, ;, nips, pp.,
speech

not, section, 4, 5,
agent

UAI quality, relevant,
found, presentation,
major

<VAR>, model,
method, nice, column

stochastic, called, con-
siders, sense, writing

artificial, discovery,
etc., via, systems

–, second, column,
processes, connec-
tions

ACL weaknesses, strengths,
so, word, main

consider, examples,
further, models,
proposed

word, method, words,
proposed, embeddings

language, extraction,
emnlp, computational,
linguistics

NeurIPS theoretical, <EQN>,
interest, practical, nips

following, clarity,
address, significance,
quality

<EQN>, maximum,
may, comments,
characters

for, see, class, de-
tailed, guidelines

of, in, which, <EQN>,
reviewer

Table 6: Salient words (α = 0.001, χ2 test) per proposition type. Top 5 frequent words that are unique for each
venue are shown. “<EQN>”, “<URL>”, and “<VAR>” are equations, URL links, and variables.

EVAL REQ FACT REF QUOT NON-A
EVAL 50.3 17.2 27.3 1.0 1.4 2.9
REQ 32.2 41.6 19.4 1.8 2.3 2.8
FACT 33.5 11.0 51.2 1.3 0.9 2.0
REF 15.0 10.8 18.0 50.9 3.6 1.8
QUOT 31.2 23.6 25.5 1.3 12.1 6.4
NON-A 31.9 15.5 22.7 1.3 2.8 25.9

Figure 5: Proposition transition prob. on AMPERE.

tative words in each proposition type compared to
other types. Table 6 shows both the commonly
used salient words across venues and the unique
words with top frequencies for each venue (α =
0.001, χ2 test). For evaluation, all venues tend
to focus on clarity and contribution, with ICLR
discussing more about “network” and NeurIPS of-
ten mentioning equations. ACL reviews then fre-
quently request for “examples”.

5 Related Work

There is a growing interest in understanding the
content and assessing the quality of peer re-
views. Authors’ feedback such as satisfaction and
helpfulness have been adopted as quality indica-
tors (Latu and Everett, 2000; Hart-Davidson et al.,
2010; Xiong and Litman, 2011). Nonetheless,
they suffer from author subjectivity and are often
influenced by acceptance decisions (Weber et al.,
2002). Evaluation by experts or editors proves
to be more reliable and informative (van Rooyen
et al., 1999), but requires substantial work and
knowledge of the field. Shallow linguistic fea-
tures, e.g., sentiment words, are studied in Born-
mann et al. (2012) for analyzing languages in peer
reviews. To the best of our knowledge, our work
is the first to understand the content and structure

of peer reviews via argument usage.
Our work is also in line with the growing body

of research in argument mining (Teufel et al.,
1999; Palau and Moens, 2009). Most of the work
focuses on arguments in social media posts (Park
and Cardie, 2014; Wei et al., 2016; Habernal and
Gurevych, 2016), online debate portals or Oxford-
style debates (Wachsmuth et al., 2017; Hua and
Wang, 2017; Wang et al., 2017), and student es-
says (Persing and Ng, 2015; Ghosh et al., 2016).
We study a new domain of peer reviews, and iden-
tify new challenges for existing models.

6 Conclusion

We study the content and structure of peer reviews
under the argument mining framework. AM-
PERE, a new dataset of peer reviews, is collected
and annotated with propositions and their types.
We benchmark AMPERE with state-of-the-art ar-
gument mining models for proposition segmenta-
tion and classification. We leverage the classifiers
to analyze the proposition usage in reviews across
ML and NLP venues, showing interesting patterns
in proposition types and content.

Acknowledgements

This research is based upon work supported in part
by National Science Foundation through Grants
IIS-1566382 and IIS-1813341. We are grateful to
the OpenReview team, especially Michael Spec-
tor, for setting up the API to facilitate review data
collection. We also thank three anonymous re-
viewers for their constructive suggestions.



2136

References
Khalid Al Khatib, Henning Wachsmuth, Johannes

Kiesel, Matthias Hagen, and Benno Stein. 2016.
A news editorial corpus for mining argumentation
strategies. In Proceedings of COLING 2016, the
26th International Conference on Computational
Linguistics: Technical Papers, pages 3433–3443.
The COLING 2016 Organizing Committee.

Lutz Bornmann, Markus Wolf, and Hans-Dieter
Daniel. 2012. Closed versus open reviewing of jour-
nal manuscripts: how far do comments differ in lan-
guage use? Scientometrics, 91(3):843–856.

Vanessa Wei Feng and Graeme Hirst. 2014. A linear-
time bottom-up discourse parser with constraints
and post-editing. In Proceedings of the 52nd Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), pages 511–521.
Association for Computational Linguistics.

Debanjan Ghosh, Aquila Khanam, Yubo Han, and
Smaranda Muresan. 2016. Coarse-grained argu-
mentation features for scoring persuasive essays.
In Proceedings of the 54th Annual Meeting of the
Association for Computational Linguistics (Volume
2: Short Papers), pages 549–554. Association for
Computational Linguistics.

Ivan Habernal and Iryna Gurevych. 2016. Which ar-
gument is more convincing? analyzing and predict-
ing convincingness of web arguments using bidirec-
tional lstm. In Proceedings of the 54th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), pages 1589–1599. Asso-
ciation for Computational Linguistics.

Ivan Habernal and Iryna Gurevych. 2017. Argumenta-
tion mining in user-generated web discourse. Com-
putational Linguistics, 43(1):125–179.

William Hart-Davidson, Michael McLeod, Christopher
Klerkx, and Michael Wojcik. 2010. A method for
measuring helpfulness in online peer review. In Pro-
ceedings of the 28th ACM international conference
on design of communication, pages 115–121. ACM.

Seth Hettich and Michael J Pazzani. 2006. Mining for
proposal reviewers: lessons learned at the national
science foundation. In Proceedings of the 12th
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 862–871.
ACM.

Xinyu Hua and Lu Wang. 2017. Understanding and
detecting supporting arguments of diverse types. In
Proceedings of the 55th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 2:
Short Papers), pages 203–208, Vancouver, Canada.
Association for Computational Linguistics.

Dongyeop Kang, Waleed Ammar, Bhavana Dalvi,
Madeleine van Zuylen, Sebastian Kohlmeier, Ed-
uard Hovy, and Roy Schwartz. 2018. A dataset of
peer reviews (peerread): Collection, insights and nlp

applications. In Proceedings of the 2018 Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, Volume 1 (Long Papers), pages
1647–1661. Association for Computational Linguis-
tics.

Jacalyn Kelly, Tara Sadeghieh, and Khosrow Adeli.
2014. Peer review in scientific publications: ben-
efits, critiques, & a survival guide. EJIFCC,
25(3):227.

Yoon Kim. 2014. Convolutional neural networks for
sentence classification. In Proceedings of the 2014
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 1746–1751. As-
sociation for Computational Linguistics.

Michail Kovanis, Raphaël Porcher, Philippe Ravaud,
and Ludovic Trinquart. 2016. The global burden
of journal peer review in the biomedical literature:
Strong imbalance in the collective enterprise. PLoS
One, 11(11):e0166387.

Klaus Krippendorff. 2004. Measuring the reliability of
qualitative text analysis data. Quality and Quantity,
38:787–800.

John Lafferty, Andrew McCallum, and Fernando CN
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data.

Tavite M Latu and André M Everett. 2000. Review of
satisfaction research and measurement approaches.
Citeseer.

Chin-Yew Lin and Eduard Hovy. 2000. The automated
acquisition of topic signatures for text summariza-
tion. In COLING 2000 Volume 1: The 18th Interna-
tional Conference on Computational Linguistics.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in neural information processing
systems, pages 3111–3119.

Vlad Niculae, Joonsuk Park, and Claire Cardie. 2017.
Argument mining with structured svms and rnns. In
Proceedings of the 55th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 985–995. Association for Com-
putational Linguistics.

Raquel Mochales Palau and Marie-Francine Moens.
2009. Argumentation mining: the detection, clas-
sification and structure of arguments in text. In Pro-
ceedings of the 12th international conference on ar-
tificial intelligence and law, pages 98–107. ACM.

Joonsuk Park, Cheryl Blake, and Claire Cardie. 2015.
Toward machine-assisted participation in erulemak-
ing: An argumentation model of evaluability. In
Proceedings of the 15th International Conference
on Artificial Intelligence and Law, pages 206–210.
ACM.

http://aclweb.org/anthology/C16-1324
http://aclweb.org/anthology/C16-1324
https://doi.org/10.3115/v1/P14-1048
https://doi.org/10.3115/v1/P14-1048
https://doi.org/10.3115/v1/P14-1048
https://doi.org/10.18653/v1/P16-2089
https://doi.org/10.18653/v1/P16-2089
https://doi.org/10.18653/v1/P16-1150
https://doi.org/10.18653/v1/P16-1150
https://doi.org/10.18653/v1/P16-1150
https://doi.org/10.18653/v1/P16-1150
https://doi.org/10.1162/COLI_a_00276
https://doi.org/10.1162/COLI_a_00276
http://aclweb.org/anthology/P17-2032
http://aclweb.org/anthology/P17-2032
https://doi.org/10.18653/v1/N18-1149
https://doi.org/10.18653/v1/N18-1149
https://doi.org/10.18653/v1/N18-1149
https://doi.org/10.3115/v1/D14-1181
https://doi.org/10.3115/v1/D14-1181
http://aclweb.org/anthology/C00-1072
http://aclweb.org/anthology/C00-1072
http://aclweb.org/anthology/C00-1072
https://doi.org/10.18653/v1/P17-1091


2137

Joonsuk Park and Claire Cardie. 2014. Identifying
appropriate support for propositions in online user
comments. In Proceedings of the First Workshop
on Argumentation Mining, pages 29–38. Association
for Computational Linguistics.

Joonsuk Park and Claire Cardie. 2018. A corpus of
erulemaking user comments for measuring evalua-
bility of arguments. In Proceedings of the Eleventh
International Conference on Language Resources
and Evaluation (LREC-2018). European Language
Resource Association.

Andreas Peldszus and Manfred Stede. 2013. From ar-
gument diagrams to argumentation mining in texts:
A survey. International Journal of Cognitive Infor-
matics and Natural Intelligence (IJCINI), 7(1):1–31.

Isaac Persing and Vincent Ng. 2015. Modeling ar-
gument strength in student essays. In Proceedings
of the 53rd Annual Meeting of the Association for
Computational Linguistics and the 7th International
Joint Conference on Natural Language Processing
(Volume 1: Long Papers), pages 543–552. Associa-
tion for Computational Linguistics.

Matthew Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word repre-
sentations. In Proceedings of the 2018 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume 1 (Long Papers), pages 2227–
2237. Association for Computational Linguistics.

Rashmi Prasad, Eleni Miltsakaki, Nikhil Dinesh, Alan
Lee, Aravind Joshi, Livio Robaldo, and Bonnie L
Webber. 2007. The penn discourse treebank 2.0 an-
notation manual.

Simon Price and Peter A Flach. 2017. Computational
support for academic peer review: A perspective
from artificial intelligence. Communications of the
ACM, 60(3):70–79.

Lance A Ramshaw and Mitchell P Marcus. 1999. Text
chunking using transformation-based learning. In
Natural language processing using very large cor-
pora, pages 157–176. Springer.

Susan van Rooyen, Nick Black, and Fiona Godlee.
1999. Development of the review quality instru-
ment (rqi) for assessing peer reviews of manuscripts.
Journal of clinical epidemiology, 52(7):625–629.

Claudia Schulz, Steffen Eger, Johannes Daxenberger,
Tobias Kahse, and Iryna Gurevych. 2018. Multi-
task learning for argumentation mining in low-
resource settings. In Proceedings of the 2018 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, Volume 2 (Short Papers), pages
35–41. Association for Computational Linguistics.

Christian Stab and Iryna Gurevych. 2017. Parsing ar-
gumentation structures in persuasive essays. Com-
putational Linguistics, 43(3):619–659.

Simone Teufel et al. 1999. Argumentative zoning: In-
formation extraction from scientific text. Ph.D. the-
sis, Citeseer.

Henning Wachsmuth, Nona Naderi, Ivan Habernal,
Yufang Hou, Graeme Hirst, Iryna Gurevych, and
Benno Stein. 2017. Argumentation quality assess-
ment: Theory vs. practice. In Proceedings of the
55th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 2: Short Papers), pages
250–255. Association for Computational Linguis-
tics.

Lu Wang, Nick Beauchamp, Sarah Shugars, and
Kechen Qin. 2017. Winning on the merits: The
joint effects of content and style on debate outcomes.
Transactions of the Association for Computational
Linguistics, 5:219–232.

Ellen J Weber, Patricia P Katz, Joseph F Waeckerle,
and Michael L Callaham. 2002. Author perception
of peer review: impact of review quality and accep-
tance on satisfaction. JAMA, 287(21):2790–2793.

Zhongyu Wei, Yang Liu, and Yi Li. 2016. Is this post
persuasive? ranking argumentative comments in on-
line forum. In Proceedings of the 54th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 2: Short Papers), pages 195–200. Associa-
tion for Computational Linguistics.

Wenting Xiong and Diane Litman. 2011. Automat-
ically predicting peer-review helpfulness. In Pro-
ceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 502–507. Association for
Computational Linguistics.

Ming Yuan and Yi Lin. 2006. Model selection and es-
timation in regression with grouped variables. Jour-
nal of the Royal Statistical Society: Series B (Statis-
tical Methodology), 68(1):49–67.

https://doi.org/10.3115/v1/W14-2105
https://doi.org/10.3115/v1/W14-2105
https://doi.org/10.3115/v1/W14-2105
http://aclweb.org/anthology/L18-1257
http://aclweb.org/anthology/L18-1257
http://aclweb.org/anthology/L18-1257
https://doi.org/10.3115/v1/P15-1053
https://doi.org/10.3115/v1/P15-1053
https://doi.org/10.18653/v1/N18-1202
https://doi.org/10.18653/v1/N18-1202
https://doi.org/10.18653/v1/N18-2006
https://doi.org/10.18653/v1/N18-2006
https://doi.org/10.18653/v1/N18-2006
https://doi.org/10.1162/COLI_a_00295
https://doi.org/10.1162/COLI_a_00295
https://doi.org/10.18653/v1/P17-2039
https://doi.org/10.18653/v1/P17-2039
http://aclweb.org/anthology/Q17-1016
http://aclweb.org/anthology/Q17-1016
https://doi.org/10.18653/v1/P16-2032
https://doi.org/10.18653/v1/P16-2032
https://doi.org/10.18653/v1/P16-2032
http://aclweb.org/anthology/P11-2088
http://aclweb.org/anthology/P11-2088

