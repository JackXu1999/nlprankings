



















































Learning Sentence Embeddings with Auxiliary Tasks for Cross-Domain Sentiment Classification


Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 236–246,
Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics

Learning Sentence Embeddings with Auxiliary Tasks
for Cross-Domain Sentiment Classification

Jianfei Yu
School of Information Systems

Singapore Management University
jfyu.2014@phdis.smu.edu.sg

Jing Jiang
School of Information Systems

Singapore Management University
jingjiang@smu.edu.sg

Abstract

In this paper, we study cross-domain senti-
ment classification with neural network archi-
tectures. We borrow the idea from Structural
Correspondence Learning and use two auxil-
iary tasks to help induce a sentence embedding
that supposedly works well across domains for
sentiment classification. We also propose to
jointly learn this sentence embedding together
with the sentiment classifier itself. Experi-
ment results demonstrate that our proposed
joint model outperforms several state-of-the-
art methods on five benchmark datasets.

1 Introduction

With the growing need of correctly identifying the
sentiments expressed in subjective texts such as
product reviews, sentiment classification has re-
ceived continuous attention in the NLP community
for over a decade (Pang et al., 2002; Pang and Lee,
2004; Hu and Liu, 2004; Choi and Cardie, 2008;
Nakagawa et al., 2010). One of the big challenges
of sentiment classification is how to adapt a senti-
ment classifier trained on one domain to a different
new domain. This is because sentiments are often
expressed with domain-specific words and expres-
sions. For example, in the Movie domain, words
such as moving and engaging are usually positive,
but they may not be relevant in the Restaurant do-
main. Since labeled data is expensive to obtain, it
would be very useful if we could adapt a model
trained on a source domain to a target domain.

Much work has been done in sentiment analysis
to address this domain adaptation problem (Blitzer

et al., 2007; Pan et al., 2010; Bollegala et al.,
2011; Ponomareva and Thelwall, 2012; Bollegala
et al., 2016). Among them, an appealing method
is the Structural Correspondence Learning (SCL)
method (Blitzer et al., 2007), which uses pivot fea-
ture prediction tasks to induce a projected feature
space that works well for both the source and the tar-
get domains. The intuition behind is that these pivot
prediction tasks are highly correlated with the orig-
inal task. For sentiment classification, Blitzer et al.
(2007) first chose pivot words which have high mu-
tual information with the sentiment labels, and then
set up the pivot prediction tasks to be the predictions
of each of these pivot words using the other words.

However, the original SCL method is based on
traditional discrete feature representations and lin-
ear classifiers. In recent years, with the advances
of deep learning in NLP, multi-layer neural net-
work models such as RNNs and CNNs have been
widely used in sentiment classification and achieved
good performance (Socher et al., 2013; Dong et
al., 2014a; Dong et al., 2014b; Kim, 2014; Tang
et al., 2015). In these models, dense, real-valued
feature vectors and non-linear classification func-
tions are used. By using real-valued word embed-
dings pre-trained from a large corpus, these mod-
els can take advantage of the embedding space that
presumably better captures the syntactic and se-
mantic similarities between words. And by using
non-linear functions through multi-layer neural net-
works, these models represent a more expressive hy-
pothesis space. Therefore, it would be interesting to
explore how these neural network models could be
extended for cross-domain sentiment classification.

236



There has been some recent studies on neural
network-based domain adaptation (Glorot et al.,
2011; Chen et al., 2012; Yang and Eisenstein, 2014).
They use Stacked Denoising Auto-encoders (SDA)
to induce a hidden representation that presumably
works well across domains. However, SDA is fully
unsupervised and does not consider the end task we
need to solve, i.e., the sentiment classification task.
In contrast, the idea behind SCL is to use carefully-
chosen auxiliary tasks that correlate with the end
task to induce a hidden representation. Another line
of work aims to learn a low dimensional represen-
tation for each feature in both domains based on
predicting its neighboring features (Yang and Eisen-
stein, 2015; Bollegala et al., 2015). Different from
these methods, we aim to directly learn sentence em-
beddings that work well across domains.

In this paper, we aim to extend the main idea be-
hind SCL to neural network-based solutions to sen-
timent classification to address the domain adapta-
tion problem. Specifically, we borrow the idea of
using pivot prediction tasks from SCL. But instead
of learning thousands of pivot predictors and per-
forming singular value decomposition on the learned
weights, which all relies on linear transformations,
we introduce only two auxiliary binary prediction
tasks and directly learn a non-linear transformation
that maps an input to a dense embedding vector.
Moreover, different from SCL and the auto-encoder-
based methods, in which the hidden feature repre-
sentation and the final classifier are learned sequen-
tially, we propose to jointly learn the hidden feature
representation together with the sentiment classifi-
cation model itself, and we show that joint learning
works better than sequential learning.

We conduct experiments on a number of different
source and target domains for sentence-level sen-
timent classification. We show that our proposed
method is able to achieve the best performance com-
pared with a number of baselines for most of these
domain pairs.

2 Related Work

Domain Adaptation: Domain adaptation is a gen-
eral problem in NLP and has been well studied
in recent years (Blitzer et al., 2006; Daumé III,
2007; Jiang and Zhai, 2007; Dredze and Crammer,

2008; Titov, 2011; Yu and Jiang, 2015). For sen-
timent classification, most existing domain adap-
tation methods are based on traditional discrete
feature representations and linear classifiers. One
line of work focuses on inducing a general low-
dimensional cross-domain representation based on
the co-occurrences of domain-specific and domain-
independent features (Blitzer et al., 2007; Pan et al.,
2010; Pan et al., 2011). Another line of work tries to
derive domain-specific sentiment words (Bollegala
et al., 2011; Li et al., 2012). Our proposed method
is similar to the first line of work in that we also aim
to learn a general, cross-domain representation (sen-
tence embeddings in our case).
Neural Networks for Sentiment Classification:
A recent trend of deep learning enhances various
kinds of neural network models for sentiment clas-
sification, including Convolutional Neural Networks
(CNNs), Recursive Neural Network (ReNNs) and
Recurrent Neural Network (RNNs), which have
been shown to achieve competitive results across
different benchmarks (Socher et al., 2013; Dong et
al., 2014a; Dong et al., 2014b; Kim, 2014; Tang et
al., 2015). Inspired by their success in standard in-
domain settings, it is intuitive for us to apply these
neural network models to domain adaptation set-
tings.
Denoising Auto-encoders for Domain Adapta-
tion: Denoising Auto-encoders have been exten-
sively studied in cross-domain sentiment classifica-
tion, since the representations learned through multi-
layer neural networks are robust against noise during
domain adaptation. The initial application of this
idea is to directly employ stacked denoising auto-
encoders (SDA) by reconstructing the original fea-
tures from data that are corrupted with noise (Glo-
rot et al., 2011), and Chen et al. (2012) proposed
to analytically marginalize out the corruption during
SDA training. Later Yang and Eisenstein (2014) fur-
ther showed that their proposed structured dropout
noise strategy can dramatically improve the effi-
ciency without sacrificing the accuracy. However,
these methods are still based on traditional discrete
representation and do not exploit the idea of using
auxiliary tasks that are related to the end task. In
contrast, the sentence embeddings learned from our
method are derived from real-valued feature vectors
and rely on related auxiliary tasks.

237



3 Method

In this section we present our sentence embedding-
based domain adaptation method for sentiment clas-
sification. We first introduce the necessary notation
and an overview of our method. we then delve into
the details of the method.

3.1 Notation and Method Overview

We assume that each input is a piece of text consist-
ing of a sequence of words. For the rest of this paper,
we assume each input is a sentence, although our
method is general enough for longer pieces of text.
Let x = (x1, x2, . . .) denote a sentence where each
xi ∈ {1, 2, . . . , V } is a word in the vocabulary and
V is the vocabulary size. Let the sentiment label ofx
be y ∈ {+,−}where + denotes a positive sentiment
and − a negative sentiment. We further assume that
we are given a set of labeled training sentences from
a source domain, denoted by Ds = {(xsi , ysi )}N

s

i=1.
Also, we have a set of unlabeled sentences from a
target domain, denoted by Dt = {xti}N

t

i=1. Our goal
is to learn a good sentiment classifier from both Ds
and Dt such that the classifier works well on the tar-
get domain.

A baseline solution without considering any do-
main difference is to simply train a classifier using
Ds, and with the recent advances in neural network-
based methods to sentence classification, we con-
sider a baseline that uses a multi-layer neural net-
work such as a CNN or an RNN to perform the clas-
sification task. To simplify the discussion and focus
on the domain adaptation ideas we propose, we will
leave the details of the neural network model we use
in Section 3.5. For now, we assume that a multi-
layer neural network is used to transform each input
x into a sentence embedding vector z. Let us use
fΘ to denote the transformation function parameter-
ized by Θ, that is, z = fΘ(x). Next, we assume
that a linear classifier such as a softmax classifier is
learned to map z to a sentiment label y.

We introduce two auxiliary tasks which presum-
ably are highly correlated with the sentiment classi-
fication task itself. Labels for these auxiliary tasks
can be automatically derived from unlabeled data in
both the source and the target domains. With the
help of the two auxiliary tasks, we learn a non-linear
transformation function fΘ′ from unlabeled data and

use it to derive a sentence embedding vector z′ from
sentence x, which supposedly works better across
domains. Finally we use the source domain’s train-
ing data to learn a linear classifier on the represen-
tation z ⊕ z′, where ⊕ is the operator that concate-
nates two vectors. Figure 1 gives the outline of our
method.

3.2 Auxiliary Tasks

Our two auxiliary tasks are about whether an in-
put sentence contains a positive or negative domain-
independent sentiment word. The intuition is the
following. If we have a list of domain-independent
positive sentiment words, then an input sentence that
contains one of these words, regardless of the do-
main the sentence is from, is more likely to contain
an overall positive sentiment. For example, a sen-
tence containing the word good is likely to be over-
all positive. Moreover, the rest of the sentence ex-
cluding the word good may contain domain-specific
words or expressions that also convey a positive sen-
timent. For example, in the sentence “The laptop
is good and goes really fast,” we can see that the
word fast is a domain-specific sentiment word, and
its sentiment polarity correlates with that of the word
good, which is domain-independent. Therefore, we
can hide the domain-independent positive words in
a sentence and try to use the other words in the sen-
tence to predict whether the original sentence con-
tains a domain-independent positive word. There are
two things to note about this auxiliary task: (1) The
label of the task can be automatically derived pro-
vided that we have the domain-independent positive
word list. (2) The task is closely related to the orig-
inal task of sentence-level sentiment classification.
Similarly, we can introduce a task to predict the ex-
istence of a domain-independent negative sentiment
word in a sentence.

Formally, let us assume that we have two domain-
independent sentiment word lists, one for the posi-
tive sentiment and the other for the negative senti-
ment. Details of how these lists are obtained will
be given in Section 3.5. Borrowing the term from
SCL, we refer to these sentiment words as pivot
words. For each sentence x, we replace all the oc-
currences of these pivot words with a special token
UNK. Let g(·) be a function that denotes this pro-
cedure, that is, g(x) is the resulting sentence with

238



Sentiment Classification

Sentence 

Embedding

Word 

Embedding

31

The  laptop is     good and    goes  really  fast The  laptop is     UNK   and    goes  really  fast

Shared 

Lookup 

Table

Auxiliary Tasks

Original Sentence New Sentence without Pivots

CNN/RNN CNN/RNN

Figure 1: The Outline of our Proposed Method.

UNK tokens. We then introduce two binary labels
for g(x). The first label u indicates whether the
original sentence x contains at least one domain-
independent positive sentiment word, and the sec-
ond label v indicates whether x contains at least one
domain-independent negative sentiment word. Fig-
ure 1 shows an example sentence x, its modified ver-
sion g(x) and the labels u and v for x. We further
use Da = {(xi, ui, vi)}Nai=1 to denote a set of train-
ing sentences for the auxiliary tasks. Note that the
sentences in Da can be from the sentences in Ds
and Dt, but they can also be from other unlabeled
sentences.

3.3 Sentence Embeddings for Domain
Adaptation

With the two auxiliary tasks, we can learn a neural
network model in a standard way to produce sen-
tence embeddings that work well for the auxiliary
tasks. Specifically, we still use Θ′ to denote the pa-
rameters of the neural network that produces the sen-
tence embeddings (and fΘ′ the corresponding trans-
formation function), and we use β+ and β− to de-
note the parameters of two softmax classifiers for
the two auxiliary tasks, respectively. Using cross-
entropy loss, we can learn Θ′ by minimizing the fol-
lowing loss function:

J(Θ′,β+,β−)

= −
∑

(x,u,v)∈Da

(
log p(u|fΘ′(g(x));β+)

+ log p(v|fΘ′(g(x));β−)
)
,

where p(y|z;β) is the probability of label y given
vector z and parameter β under softmax regression.

With the learned Θ′, we can derive a sentence em-
bedding z′ from any sentence. Although we could
simply use this embedding z′ for sentiment classi-
fication through another softmax classifier, this may
not be ideal because z′ is transformed from g(x),
which has the domain-independent sentiment words
removed. Similar to SCL and some other previous
work, we concatenate the embedding vector z′ with
the standard embedding vector z for the final classi-
fication.

3.4 Joint Learning

Although we can learn Θ′ using Da as a first step,
here we also explore a joint learning setting. In this
setting, Θ′ is learned together with the neural net-
work model used for the end task, i.e., sentiment
classification. This way, the learning of Θ′ depends
not only on Da but also on Ds, i.e., the sentiment-
labeled training data from the source domain.

Specifically, we use Θ to denote the parameters
for a neural network that takes the original sentence
x and transforms it to a sentence embedding (and fΘ
the corresponding transformation function). We use
γ to denote the parameters of a softmax classifier
that operates on the concatenated sentence embed-
ding z ⊕ z′ for sentiment classification. With joint
learning, we try to minimize the following loss func-

239



tion:

J(Θ,Θ′,γ,β+,β−)

= −
∑

(x,y)∈Ds

(
log p(y|fΘ(x)⊕ fΘ′(g(x));γ)

)

−
∑

(x,u,v)∈Da

(
log p(u|fΘ′(g(x));β+)

+ log p(v|fΘ′(g(x));β−)
)
.

We can see that this loss function contains two parts.
The first part is the cross-entropy loss based on the
true sentiment labels of the sentences in Ds. The
second part is the loss based on the auxiliary tasks
and the data Da, which are derived from unlabeled
sentences.

Finally, to make a prediction on a sentence, we
use the learned Θ and Θ′ to derive a sentence embed-
ding fΘ(x) ⊕ fΘ′(g(x)), and then use the softmax
classifier parameterized by the learned γ to make the
final prediction.

3.5 Implementation Details

In this section we explain some of the model details.

Pivot Word Selection
Recall that the two auxiliary tasks depend on two

domain-independent sentiment word lists, i.e., pivot
word lists. Different from Blitzer et al. (2007), we
employ weighted log-likelihood ratio (WLLR) to se-
lect the most positive and negative words in both do-
mains as pivots. The reason is that in our prelimi-
nary experiments we observe that mutual informa-
tion (used by Blitzer et al. (2007)) is biased towards
low frequency words. Some high frequency words
including good and great are scored low. In com-
parison, WLLR does not have this issue. The same
observation was also reported previously by Li et al.
(2009).

More specifically, we first tokenize the sentences
in Ds and Dt and perform part-of-speech tagging
using the NLTK toolkit. Next, we extract only ad-
jectives, adverbs and verbs with a frequency of at
least 3 in the source domain and at least 3 in the tar-
get domain. We also remove negation words such as
not and stop words using a stop word list. We then
measure each remaining candidate word’s relevance
to the positive and the negative classes based on Ds

by computing the following scores:

r(w, y) = p̃(w|y) log p̃(w|y)
p̃(w|ȳ) ,

where w is a word, y ∈ {+,−} is a sentiment label,
ȳ is the opposite label of y, and p̃(w|y) is the empir-
ical probability of observing w in sentences labeled
with y. We can then rank the candidate words in de-
creasing order of r(w,+) and r(w,−). Finally, we
select the top 25% from each ranked list as the final
lists of pivot words for the positive and the nega-
tive sentiments. Some manual inspection shows that
most of these words are indeed domain-independent
sentiment words.

Neural Network Model
Our framework is general and potentially we can

use any neural network model to transform an in-
put sentence to a sentence embedding vector. In this
paper, we adopt a CNN-based approach because it
has been shown to work well for sentiment classi-
fication. Specifically, each word (including the to-
ken UNK) is represented by a word embedding vec-
tor. Let W ∈ Rd×V denote the lookup table for
words, where each column is a d-dimensional em-
bedding vector for a word type. Two separate CNNs
are used to process x and g(x), and their mecha-
nisms are the same. For a word xi in each CNN, the
embedding vectors inside a window of size n cen-
tered at i are concatenated into a new vector, which
we refer to as ei ∈ Rnd. A convolution operation
is then performed by applying a filter F ∈ Rh×nd
on ei to produce a hidden vector hi = m(Fei + b),
where b ∈ Rh is a bias vector and m is an element-
wise non-linear transformation function. Note that
we pad the original sequence in front and at the back
to ensure that at each position i we have n vectors
to be combined into hi. After the convolution op-
eration is applied to the whole sequence, we obtain
H = [h1,h2, . . .], and we apply a max-over-time
pooling operator to take the maximum value of each
row of H to obtain an overall hidden vector, i.e., z
for x and z′ for g(x).

It is worth noting that the two neural networks
corresponding to fΘ and fΘ′ share the same word
embedding lookup table. This lookup table is ini-
tialized with word embeddings from word2vec1 and

1https://code.google.com/p/word2vec/

240



is updated during our learning process. Note that the
token UNK is initialized as a zero vector and never
updated.

3.6 Differences from SCL
Although our method is inspired by SCL, there are
a number of major differences: (1) Our method is
based on neural network models with continuous,
dense feature representations and non-linear trans-
formation functions. SCL is based on discrete,
sparse feature vectors and linear transformations.
(2) Although our pivot word selection is similar to
that of SCL, in the end we only use two auxiliary
tasks while SCL uses much more pivot prediction
tasks. (3) We can directly learn the transformation
function f ′Θ that produces the hidden representation,
while SCL relies on SVD to learn the projection
function. (4) We perform joint learning of the auxil-
iary tasks and the end task, i.e., sentiment classifica-
tion, while SCL performs the learning in a sequential
manner.

4 Experiments

4.1 Data Sets and Experiment Settings

Data Set # Sentences # Words

Movie1(MV1) 10662 18765
Movie2(MV2) 9613 16186
Camera(CR) 3770 5340
Laptop(LT) 1907 2837
Restaurant(RT) 1572 2930

Table 1: Statistics of our data sets.

To evaluate our proposed method, we conduct
experiments using five benchmark data sets. The
data sets are summarized in Table 1. Movie12 and
Movie23 are movie reviews labeled by Pang and Lee
(2005) and Socher et al. (2013), respectively. Cam-
era4 are reviews of digital products such as MP3
players and cameras (Hu and Liu, 2004). Laptop and
Restaurant5 are laptop and restaurant reviews taken

2https://www.cs.cornell.edu/people/pabo/
movie-review-data/

3http://nlp.stanford.edu/sentiment/
4http://www.cs.uic.edu/˜liub/FBS/

sentiment-analysis.html
5Note that the original data set is for aspect-level sentiment

analysis. We remove sentences with opposite polarities towards
different aspects, and use the consistent polarity as the sentence-
level sentiment of each remaining sentence.

from SemEval 2015 Task 12.
We consider 18 pairs of data sets where the two

data sets come from different domains.6 For neural
network-based methods, we randomly pick 200 sen-
tences from the target domain as the development set
for parameter tuning, and the rest of the data from
the target domain as the test data.

4.2 Baselines and Hyperparameters

We consider the following baselines:
Naive is a non-domain-adaptive baseline based on
bag-of-word representations.
SCL is our implementation of the Structural Corre-
spondence Learning method. We set the number of
induced features K to 100 and rescale factor α = 5,
and we use 1000 pivot words based on our prelimi-
nary experiments.
mDA is our implementation of marginalized De-
noising Auto-encoders (Chen et al., 2012), one
of the state-of-the-art domain adaptation methods,
which learns a shared hidden representation by re-
constructing pivot features from corrupted inputs.
Following Yang and Eisenstein (2014), we employ
the efficient and effective structured dropout noise
strategy without any parameter. The top 500 fea-
tures are chosen as pivots based on our preliminary
experiments.
NaiveNN is a non-domain-adaptive baseline based
on CNN, as described in Section 3.5.
Aux-NN is a simple combination of our auxiliary
tasks with NaiveNN, which treats the derived la-
bel of two auxiliary tasks as two features and then
appends them to the hidden representation learned
from CNN, followed by a softmax classifier.
SCL-NN is a naive combination of SCL with
NaiveNN, which appends the induced representation
from SCL to the hidden representation learned from
CNN, followed by a softmax classifier.
mDA-NN is similar to SCL-NN but uses the hidden
representation derived from mDA.
Sequential is our proposed method without joint
learning, which first learns Θ′ based on Da and then
learns Θ and γ based on Ds with fixed Θ′.
Joint is our proposed joint learning method, that is,
we jointly learn Θ and Θ′.

6Because Movie1 and Movie2 come from the same domain,
we do not take this pair.

241



Task Method

Source Target Naive Naive++ SCL++ mDA++ NaiveNN Aux-NN SCL-NN mDA-NN Sequential Joint

MV1 LT 0.656 0.739 0.742 0.742 0.773 0.779 0.776 0.780 0.774 0.804∗

MV1 RT 0.625 0.742 0.750 0.761 0.802 0.794 0.817 0.819 0.814 0.825∗

MV1 CR 0.609 0.684 0.688 0.688 0.721 0.717 0.734 0.730 0.717 0.747∗

MV2 LT 0.699 0.760 0.765 0.772 0.805 0.811 0.800 0.811 0.808 0.827∗

MV2 RT 0.696 0.761 0.768 0.778 0.813 0.819 0.824 0.825 0.833 0.840∗

MV2 CR 0.644 0.697 0.705 0.706 0.738 0.732 0.736 0.756 0.745 0.768∗

CR LT 0.780 0.791 0.802 0.806 0.848 0.848 0.846 0.850 0.856 0.858∗

CR RT 0.746 0.784 0.782 0.789 0.827 0.835 0.841 0.839 0.835 0.844∗

CR MV1 0.593 0.597 0.612 0.612 0.685 0.689 0.689 0.692 0.687 0.696∗

CR MV2 0.609 0.629 0.644 0.640 0.735 0.726 0.734 0.731 0.735 0.736
LT RT 0.736 0.781 0.800 0.810 0.819 0.820 0.823 0.852 0.841 0.840
LT MV1 0.574 0.601 0.612 0.630 0.711 0.703 0.702 0.709 0.705 0.707
LT MV2 0.588 0.632 0.645 0.663 0.742 0.745 0.739 0.747 0.746 0.747
LT CR 0.736 0.762 0.768 0.780 0.791 0.796 0.803 0.819 0.803 0.817
RT LT 0.732 0.777 0.777 0.799 0.817 0.822 0.831 0.826 0.828 0.834∗

RT MV1 0.580 0.604 0.618 0.643 0.721 0.726 0.724 0.734 0.722 0.724
RT MV2 0.605 0.630 0.633 0.664 0.761 0.762 0.756 0.772 0.757 0.765
RT CR 0.689 0.708 0.704 0.732 0.764 0.772 0.759 0.774 0.772 0.779∗

Average 0.661 0.704 0.712 0.723 0.770 0.772 0.774 0.781 0.777 0.787

Table 2: Comparison of classification accuracies of different methods. ∗ indicates that our joint method is significantly better than
NaiveNN, Aux-NN, SCL-NN and mDA-NN with p < 0.05 based on McNemar’s paired significance test.

For Naive, SCL and mDA, we use LibLinear7 to
train linear classifiers and use its default hyperpa-
rameters. In all the tasks, we use unigrams and bi-
grams with a frequency of at least 4 as features for
classification. For the word embeddings, we set the
dimension d to 300. For CNN, we set the window
size to 3. Also, the size of the hidden representa-
tions z and z′ is set to 100. Following Kim (2014),
the non-linear activation function in CNN is Relu,
the mini-batch size is 50, the dropout rate α equals
0.5, and the hyperparameter for the l2 norms is set
to be 3. For Naive, SCL and mDA, we do not use
the 200 sentences in the development set for tuning
parameters. Hence, for fair comparison, we also in-
clude settings where the 200 sentences are added to
the training set. We denote these settings by ++.

4.3 Results

In Table 2, we report the results of all the methods.
It is easy to see that the performance of Naive is
very limited, and the incorporation of 200 reviews
in the development set (Naive++) brings in 4.3% of
improvement on average. SCL++ and mDA++ can
further improve the average accuracy respectively

7http://www.csie.ntu.edu.tw/cjlin/
liblinear/

by 0.8% and 1.9%, which verifies the usefulness of
these two domain adaptation methods. However, we
can easily see that the performance of these domain
adaptation methods based on discrete, bag-of-word
representations is even much lower than the non-
domain-adaptive method on continuous representa-
tions (NaiveNN). This confirms that it is useful to
develop domain adaptation methods based on em-
bedding vectors and neural network models.

Moreover, we can find that the performance of
simply appending two features from auxiliary tasks
to NaiveNN (i.e., Aux-NN) is quite close to that
of NaiveNN on most data set pairs, which shows
that it is not ideal for domain adaptation. In addi-
tion, although the shared hidden representations de-
rived from SCL and mDA are based on traditional
bag-of-word representations, SCL-NN and mDA-
NN can still improve the performance of NaiveNN
on most data set pairs, which indicates that the de-
rived shared hidden representations by SCL and by
mDA can generalize better across domains and are
generally useful for domain adaptation.

Finally, it is easy to see that our method with
joint learning outperforms SCL-NN on almost all
the data set pairs. And in comparison with mDA-
NN, our method with joint learning can also outper-

242



Task Method

Source Target NaiveNN mDA-NN Joint

MV1 LT 0.802 0.799 0.816∗

MV1 RT 0.816 0.820 0.838∗

MV1 CR 0.744 0.757 0.767∗

MV2 LT 0.823 0.830 0.839∗

MV2 RT 0.837 0.829 0.850∗

MV2 CR 0.753 0.769 0.773∗

CR LT 0.853 0.863 0.870∗

CR RT 0.840 0.856 0.851
CR MV1 0.699 0.701 0.704∗

CR MV2 0.745 0.741 0.745
LT RT 0.839 0.849 0.849
LT MV1 0.714 0.710 0.720∗

LT MV2 0.759 0.767 0.766
LT CR 0.803 0.815 0.814
RT LT 0.825 0.839 0.841
RT MV1 0.724 0.737 0.732
RT MV2 0.762 0.771 0.768
RT CR 0.773 0.777 0.783∗

Average 0.784 0.790 0.796

Table 3: Comparison of our method Joint with NaiveNN and
mDA-NN in a setting where some labeled target data is used.

form it on most data set pairs, especially when the
size of the labeled data in the source domain is rela-
tively large. Furthermore, we can easily observe that
for our method, joint learning generally works bet-
ter than sequential learning. All these observations
show the advantage of our joint learning method.

In Table 3, we also show the comparison between
mDA-NN and our model under a setting some la-
beled target data is used. Specifically, we randomly
select 100 sentences from the development set and
mix them with the training set. We can observe that
our method Joint outperforms NaiveNN and mDA-
NN by 1.2% and 0.6%, respectively, which further
confirms the effectiveness of our model. But, in
comparison with the setting where no target data is
available, the average improvement of our method
over NaiveNN is relatively small.

Hence, to give a deeper analysis, we further show
the comparison of Joint and NaiveNN with respect
to the number of labeled target data in Figure 2. Note
that for space limitation, we only present the results
on MV2→ RT and MV2→ CR. Similar trends have
been observed on other data set pairs. As we can
see from Figure 2, the difference between the per-
formance of NaiveNN and that of Joint gradually
decreases with the increase of the number of labeled

0 20 40 60 80 100
0.72

0.74

0.76

0.78

0.8

0.82

0.84

0.86

the number of labeled target data

A
cc

ur
ac

y

 

 

MV2RT−Joint

MV2RT−NaiveNN

MV2CR−Joint

MV2CR−NaiveNN

Figure 2: The influence of the number of labeled target data.

target data. This indicates that our joint model is
much more effective when no or small number of
labeled target data is available.

4.4 Case Study

To obtain a better understanding of our method, we
conduct a case study where the source is CR and the
target is RT.

For each sentiment polarity, we try to extract the
most useful trigrams for the final predictions. Re-
call that our CNN models use a window size of 3,
which corresponds to trigrams. By tracing the final
prediction scores back through the neural network,
we are able to locate the trigrams which have con-
tributed the most through max-pooling. In Table 4,
we present the most useful trigrams of each polarity
extracted by NaiveNN and by the two components
of our sequential and joint method. Sequential-
original and Joint-original refer to the CNN cor-
responding to fΘ while Sequential-auxiliary and
Joint-auxiliary refer to the CNN corresponding to
fΘ′ , which is related to the auxiliary tasks.

In Table 4, we can easily observe that for
NaiveNN, the most important trigrams are domain-
independent, which contain some general senti-
ment words like good, great and disappointing.
For our sequential model, the most important tri-
grams captured by Sequential-original are simi-
lar to NaiveNN, but due to the removal of the
pivot words in each sentence, the most impor-
tant trigrams extracted by Sequential-auxiliary are
domain-specific, including target-specific sentiment
words like oily, friendly and target-specific aspect
words like flavor, atmosphere. But since aspect
words are irrelevant to our sentiment classification

243



Method Negative Sentiment Positive Sentiment

disappointing * *, disgusting * *, it is not, * * great, good * *,* * best, * i love,
NaiveNN slow * *, * too bad, * * terrible, place is not, was very good,* * excellent,

unpleasant experience *, would not go, * the only wonderful * *, * * amazing, * * nice
disgusting * *, disappointing * *, * * terrible * * great, good * *, * * best, * i love,

Sequential-original expensive * *, it is not, unpleasant experience *, * highly recommended, * * excellent,
slow * *, * too bad, probably would not, awful * * wonderful * *, is amazing *, is the perfect
disgusting * *, never go back, money * *, delicious * *, friendly * *, food * *,

Sequential-auxiliary rude * *, flavor * *, * this place, oily * *, food is UNK, * highly UNK, fresh * *,
prices * *, inedible ! *, this place survives atmosphere * *, * i highly, nyc * *
disgusting * *, soggy * *, disappointing * *, * * great, good * *, * * best, * i love,

Joint-original * too bad, * would never, it is not, rude * *, * * amazing, delicious * *,
* * terrible, place is not, disappointment * * back * *, * i highly, of my favorite
soggy * *, disgusting * *, rude * *, delicious * *, go back *, is always fresh,

Joint-auxiliary disappointment * *, not go back, was not fresh, friendly * *, to die for, also very UNK,
prices * *, inedible ! *, oily * *, overpriced * * of my favorite, food * *, * i highly, delicious ! *

Table 4: Comparison of the most useful trigrams chosen by our method and by NaiveNN on CR → RT. Here * denotes a “padding”,
which we added at the beginning and the end of each sentence. The domain-specific sentiment words are in bold.

task, it might bring in some noise and affect the
performance of our sequential model. In contrast
to Sequential-auxiliary, Joint-auxiliary is jointly
learnt with the sentiment classification task, and it is
easy to see that most of its extracted trigrams are
target-specific sentiment words. Also, for Joint-
original, since we share the word embeddings of
two components and do not remove any pivot, it is
intuitive to see that the extracted trigrams contain
both domain-independent and domain-specific sen-
timent words. These observations agree with our
motivations behind the model.

Finally, we also sample several sentences from
the test dataset, i.e., RT, to get a deeper insight of
our joint model. Although NaiveNN and Sequen-
tial correctly predict sentiments of the following two
sentences:

1. “I’ve also been amazed at all the new addi-
tions in the past few years: A new Jazz Bar, the most
fantastic Dining Garden, the Best Thin Crust Pizzas,
and now a Lasagna Menu which is to die for!”

2. “The have a great cocktail with Citrus Vodka
and lemon and lime juice and mint leaves that is to
die for!”
Both of them give wrong predictions on another
three sentences containing to die for:

3. “Try their chef’s specials– they are to die for.”
4. “Their tuna tartar appetizer is to die for.”
5. “It’s to die for!”.
However, since to die for co-occurs with some

general sentiment words like fantastic, best and
great in previous two sentences, our joint model can
implicitly learn that to die for is highly correlated
with the positive sentiment via our auxiliary tasks,
and ultimately make correct predictions for the lat-
ter three sentences. This further indicates that our
joint model can identify more domain-specific sen-
timent words in comparison with NaiveNN and Se-
quential, and therefore improve the performance.

5 Conclusions

We presented a domain adaptation method for senti-
ment classification based on sentence embeddings.
Our method induces a sentence embedding that
works well across domains, based on two auxil-
iary tasks. We also jointly learn the cross-domain
sentence embedding and the sentiment classifier.
Experiment results show that our proposed joint
method can outperform several highly competi-
tive domain adaptation methods on 18 source-target
pairs using five benchmark data sets. Moreover, fur-
ther analysis confirmed that our method is able to
pick up domain-specific sentiment words.

Acknowledgment

This research is supported by the Singapore Na-
tional Research Foundation under its International
Research Centre@Singapore Funding Initiative and
administered by the IDM Programme Office, Media
Development Authority (MDA).

244



References

John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proceedings of the 2006 Con-
ference on Empirical Methods in Natural Language
Processing, pages 120–128. Association for Compu-
tational Linguistics.

John Blitzer, Mark Dredze, and Fernando Pereira. 2007.
Biographies, bollywood, boom-boxes and blenders:
Domain adaptation for sentiment classification. In
Proceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics, pages 440–447,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.

Danushka Bollegala, David Weir, and John Carroll.
2011. Using multiple sources to construct a sentiment
sensitive thesaurus for cross-domain sentiment clas-
sification. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies-Volume 1, pages 132–
141. Association for Computational Linguistics.

Danushka Bollegala, Takanori Maehara, and Ken-ichi
Kawarabayashi. 2015. Unsupervised cross-domain
word representation learning. In Proceedings of the
53rd Annual Meeting of the Association for Computa-
tional Linguistics and the 7th International Joint Con-
ference on Natural Language Processing (Volume 1:
Long Papers), pages 730–740, Beijing, China, July.
Association for Computational Linguistics.

Danushka Bollegala, Tingting Mu, and John Goulermas.
2016. Cross-domain sentiment classification using
sentiment sensitive embeddings. IEEE Transactions
on Knowledge & Data Engineering, 6(2):398–410.

Minmin Chen, Zhixiang Eddie Xu, Kilian Q. Weinberger,
and Fei Sha. 2012. Marginalized denoising autoen-
coders for domain adaptation. In Proceedings of the
29th International Conference on Machine Learning.

Yejin Choi and Claire Cardie. 2008. Learning with com-
positional semantics as structural inference for subsen-
tential sentiment analysis. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 793–801. Association for Compu-
tational Linguistics.

Hal Daumé III. 2007. Frustratingly easy domain adapta-
tion. In Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics, pages 256–
263.

Li Dong, Furu Wei, Chuanqi Tan, Duyu Tang, Ming
Zhou, and Ke Xu. 2014a. Adaptive recursive neural
network for target-dependent twitter sentiment classi-
fication. In Proceedings of the 52nd Annual Meeting
of the Association for Computational Linguistics (Vol-

ume 2: Short Papers), pages 49–54, Baltimore, Mary-
land, June. Association for Computational Linguistics.

Li Dong, Furu Wei, Ming Zhou, and Ke Xu. 2014b.
Adaptive multi-compositionality for recursive neural
models with applications to sentiment analysis. In
Twenty-Eighth AAAI Conference on Artificial Intelli-
gence.

Mark Dredze and Koby Crammer. 2008. Online meth-
ods for multi-domain learning and adaptation. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, pages 689–697.

Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Domain adaptation for large-scale sentiment
classification: A deep learning approach. In In Pro-
ceedings of the Twenty-eight International Conference
on Machine Learning.

Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 168–177.
ACM.

Jing Jiang and ChengXiang Zhai. 2007. Instance weight-
ing for domain adaptation in nlp. In Proceedings of
the 45th Annual Meeting of the Association of Compu-
tational Linguistics, pages 264–271.

Yoon Kim. 2014. Convolutional neural networks for sen-
tence classification. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 1746–1751. Association
for Computational Linguistics, October.

Shoushan Li, Rui Xia, Chengqing Zong, and Chu-Ren
Huang. 2009. A framework of feature selection meth-
ods for text categorization. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natural
Language Processing of the AFNLP: Volume 2-Volume
2, pages 692–700. Association for Computational Lin-
guistics.

Fangtao Li, Sinno Jialin Pan, Ou Jin, Qiang Yang, and Xi-
aoyan Zhu. 2012. Cross-domain co-extraction of sen-
timent and topic lexicons. In Proceedings of the 50th
Annual Meeting of the Association for Computational
Linguistics: Long Papers-Volume 1, pages 410–419.
Association for Computational Linguistics.

Tetsuji Nakagawa, Kentaro Inui, and Sadao Kurohashi.
2010. Dependency tree-based sentiment classification
using crfs with hidden variables. In Human Language
Technologies: The 2010 Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics, pages 786–794. Association for
Computational Linguistics.

Sinno Jialin Pan, Xiaochuan Ni, Jian-Tao Sun, Qiang
Yang, and Zheng Chen. 2010. Cross-domain senti-
ment classification via spectral feature alignment. In

245



Proceedings of the 19th international conference on
World wide web, pages 751–760. ACM.

Sinno Jialin Pan, Ivor W Tsang, James T Kwok, and
Qiang Yang. 2011. Domain adaptation via transfer
component analysis. Neural Networks, IEEE Transac-
tions on, 22(2):199–210.

Bo Pang and Lillian Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In Proceedings of the 42nd
annual meeting on Association for Computational Lin-
guistics, page 271. Association for Computational Lin-
guistics.

Bo Pang and Lillian Lee. 2005. Seeing stars: Ex-
ploiting class relationships for sentiment categoriza-
tion with respect to rating scales. In Proceedings of
the 43rd Annual Meeting on Association for Compu-
tational Linguistics, pages 115–124. Association for
Computational Linguistics.

Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using ma-
chine learning techniques. In Proceedings of the ACL-
02 conference on Empirical methods in natural lan-
guage processing-Volume 10, pages 79–86. Associa-
tion for Computational Linguistics.

Natalia Ponomareva and Mike Thelwall. 2012. Do
neighbours help?: an exploration of graph-based al-
gorithms for cross-domain sentiment classification. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning, pages
655–665. Association for Computational Linguistics.

Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang,
Christopher D. Manning, Andrew Ng, and Christopher
Potts. 2013. Recursive deep models for semantic
compositionality over a sentiment treebank. In Pro-
ceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1631–
1642, Seattle, Washington, USA, October. Association
for Computational Linguistics.

Duyu Tang, Bing Qin, and Ting Liu. 2015. Docu-
ment modeling with gated recurrent neural network
for sentiment classification. In Proceedings of the
2015 Conference on Empirical Methods in Natural
Language Processing, pages 1422–1432, Lisbon, Por-
tugal, September. Association for Computational Lin-
guistics.

Ivan Titov. 2011. Domain adaptation by constraining
inter-domain variability of latent feature representa-
tion. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 62–71.

Yi Yang and Jacob Eisenstein. 2014. Fast easy unsuper-
vised domain adaptation with marginalized structured
dropout. In Proceedings of the 52nd Annual Meeting

of the Association for Computational Linguistics (Vol-
ume 2: Short Papers), pages 538–544.

Yi Yang and Jacob Eisenstein. 2015. Unsupervised
multi-domain adaptation with feature embeddings. In
Proceedings of the North American Chapter of the As-
sociation for Computational Linguistics, pages 672–
682.

Jianfei Yu and Jing Jiang. 2015. A hassle-free unsuper-
vised domain adaptation method using instance sim-
ilarity features. In Proceedings of the 53rd Annual
Meeting of the Association for Computational Linguis-
tics and the 7th International Joint Conference on Nat-
ural Language Processing (Volume 2: Short Papers),
pages 168–173, Beijing, China, July. Association for
Computational Linguistics.

246


