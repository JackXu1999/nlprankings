



















































A Parallel Corpus of Python Functions and Documentation Strings for Automated Code Documentation and Code Generation


Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 314–319,
Taipei, Taiwan, November 27 – December 1, 2017 c©2017 AFNLP

A Parallel Corpus of Python Functions and Documentation Strings for
Automated Code Documentation and Code Generation

Antonio Valerio Miceli Barone and Rico Sennrich
School of Informatics, The University of Edinburgh

amiceli@inf.ed.ac.uk
rico.sennrich@ed.ac.uk

Abstract

Automated documentation of program-
ming source code and automated code
generation from natural language are chal-
lenging tasks of both practical and scien-
tific interest. Progress in these areas has
been limited by the low availability of par-
allel corpora of code and natural language
descriptions, which tend to be small and
constrained to specific domains.

In this work we introduce a large and di-
verse parallel corpus of a hundred thou-
sands Python functions with their doc-
umentation strings (”docstrings”) gener-
ated by scraping open source reposito-
ries on GitHub. We describe baseline re-
sults for the code documentation and code
generation tasks obtained by neural ma-
chine translation. We also experiment with
data augmentation techniques to further
increase the amount of training data. We
release our datasets and processing scripts
in order to stimulate research in these ar-
eas.

1 Introduction

Joint processing of natural languages and pro-
gramming languages is a research area concerned
with tasks such as automated source code docu-
mentation, automated code generation from natu-
ral language descriptions and code search by natu-
ral language queries. These tasks are of great prac-
tical interest, since they could increase the produc-
tivity of programmers, and also of scientific in-
terest due to their difficulty and the conjectured
connections between natural language, computa-
tion and reasoning (Chomsky, 1956; Miller, 2003;
Graves et al., 2014).

1.1 Existing corpora

Major breakthroughs have been recently achieved
in machine translation and other hard natural lan-
guage processing tasks by using neural networks,
such as sequence-to-sequence transducers (Bah-
danau et al., 2014). In order to properly gener-
alize, neural networks need to be trained on large
and diverse datasets.

These techniques have also been applied with
some success to code documentation (Iyer et al.,
2016) and code generation (Ling et al., 2016; Yin
and Neubig, 2017), but these works trained and
evaluated their models on datasets which are small
or limited to restricted domains, in some cases sin-
gle software projects.

Source code can be collected by scraping open
source repositories from code hosting services
such as GitHub1 (Allamanis and Sutton, 2013;
Bhoopchand et al., 2016), but the main difficulty
is finding natural language annotations that docu-
ment the code in sufficient detail.

Some existing corpora, such as the the
DJANGO dataset and the Project Euler dataset
(Oda et al., 2015) have been created by human an-
notators, who can produce high accuracy exam-
ples, but this annotation process is expensive and
relatively slow, resulting in small (from a few hun-
dreds to less than 20,000 examples) and homoge-
neous datasets. Other corpora have been assem-
bled from user-generated descriptions matched to
code fragments mined from public websites such
as StackOverflow2 (Allamanis et al., 2015b; Iyer
et al., 2016) or IFTTT3 (Quirk et al., 2015). These
datasets can be large (> 100, 000 examples) but
often very noisy. Another approach is to tar-
get a very specific domain, namely trading card

1github.com
2stackoverflow.com
3ifttt.com

314



games (Magic the Gathering and Hearthstone)
(Ling et al., 2016), where code is very repetitive
and contains a natural language description (the
card text) that can be extracted using simple hand-
coded rules. Like the human-annotated corpora,
these corpora have high accuracy but are small and
very domain-specific.

In practice the existing low-noise corpora seem
to have drawbacks which cause them to be unusu-
ally easy. The published evaluation scores on these
dataset are are surprisingly high even for baseline
systems (Oda et al., 2015; Yin and Neubig, 2017),
with BLEU scores more than twice those of ma-
chine translation between natural languages (Cet-
tolo et al., 2016), a task that we would expect to
be no more difficult than code documentation or
code generation, especially given the much larger
amount of available data.

The DJANGO and and Project Euler corpora
use pseudo-code rather than true natural language
as a code description, resulting in code fragments
and descriptions being similar and easy to align.
The Magic the Gathering and Hearthstone code
fragments are repetitive, with most code of an ex-
ample being either boilerplate or varying in a lim-
ited number of ways that correspond to specific
keywords in the description. We conjecture that,
as a consequence of these structural properties,
these corpora don’t fully represent the complex-
ity of code documentation and code generation as
typically done by human programmers, and may
be thus of limited use in practical applications.

Therefore we identify the need for a more chal-
lenging corpus that better represents code and doc-
umentation as they occur in the wild.

1.2 Our proposal

In this work we seek to address these limitations
by introducing a parallel corpus of over a hundred
thousands diverse Python code fragments with de-
scriptions written by their own programmers.

The Python programming language allows each
source code object to contain a ”docstring” (docu-
mentation string), which is retained at runtime as
metadata. Programmers use docstrings to describe
the functionality and interface of code objects, and
sometimes also usage examples. Docstrings can
be extracted by automatic tools to generate, for in-
stance, HTML documentation or they can be ac-
cessed at runtime when running Python in interac-
tive mode.

We propose the use of docstrings as natural lan-
guage descriptions for code documentation and
code generation tasks. As the main contribution
of this work, we release code-docstring-corpus:
a parallel corpus of Python function declarations,
bodies and descriptions collected from publicly
available open source repositories on GitHub.

Current approaches to sequence transduction
work best on short and ideally independent frag-
ments, while source code can have complex de-
pendencies between functions and classes. There-
fore we only extract top-level functions since they
are usually small and relatively self-contained,
thus we conjecture that they constitute meaningful
units as individual training examples. However,
in order to support research on project-level code
documentation and code generation, we anno-
tate each sample with metadata (repository owner,
repository name, file name and line number), en-
abling users to reconstruct dependency graphs and
exploit contextual information.

Class definitions and class methods are not in-
cluded in our main corpus but will be released in
an extended version of the corpus, which will also
include the commit hash metadata for all the col-
lected repositories.

We train and evaluate baseline neural machine
translation systems for the code documentation
and the code generation tasks. In order to sup-
port comparisons using different evaluation met-
rics, we also release the test and validation outputs
of these systems.

We additionally release a corpus of Python
functions without docstrings which we auto-
matically annotated with synthetic docstrings
created by our code documentation system.
The corpora, extraction scripts and baseline
system configurations are available online at
https://github.com/EdinburghNLP/
code-docstring-corpus.

2 Dataset

2.1 Extraction and preparation
We used the GitHub scraper4 by Bhoopchand
et al. (2016) with default settings to download
source code from repositories on GitHub, retain-
ing Python 2.7 code.

We split each top-level function in a declaration
(decorators, name and parameters), a docstring (if

4https://github.com/uclmr/
pycodesuggest

315



Dataset Examples Tokens LoCs
Parallel decl. 150,370 556,461 167,344
Parallel bodies 150,370 12,601,929 1,680,176
Parallel docstrings 150,370 5,789,741 -
Code-only decl. 161,630 538,303 183,935
Code-only bodies 161,630 13,009,544 1,696,594

Table 1: Number of examples, tokens and lines of
code in the corpora.

Corpus Element Mean Std. Median
Parallel Declarations 3.70 7.62 3
Parallel Bodies 83.81 254.47 40
Parallel Docstrings 38.50 71.87 16
Code-only Declarations 3.33 5.04 2
Code-only Bodies 80.49 332.75 37

Table 2: Tokens per example statistics.

present) and the rest of the function body. If the
docstring is present, the function is included in the
main parallel corpus, otherwise it is included in
the ”monolingual” code-only corpus for which we
later generate synthetic docstrings.

We further process the the data by removing the
comments, normalizing the code syntax by pars-
ing and unparsing, removing semantically irrele-
vant spaces and newlines and escaping the rest and
removing empty or non-alphanumeric lines from
the docstrings. Preprocessing removes empty lines
and decorative elements from the docstrings but it
is functionally reversible on the code5.

An example of an extracted function based on
scikit-learn (Pedregosa et al., 2011) (with doc-
string shortened for brevity) is provided in fig. 1.

2.2 Dataset description

The extraction process resulted in a main parallel
corpus of 150,370 triples of function declarations,
docstrings and bodies.

We partition the main parallel corpus in a
training/validation/test split, consisting of 109,108
training examples, 2,000 validation examples and
2,000 test examples (the total size is smaller than
the full corpus due to duplicate example removal).

The code-only corpus consists of 161,630 pairs
of function declarations and bodies. The synthetic
docstring corpus consists of docstrings generated
using from the code-only corpus using our NMT
code documentation model, described in the next
section.

We report corpora summary statistics in tables
1 and 2.

5except in the rare cases where the code accesses its own
docstring or source code string

3 Baseline results

Since we are releasing a novel dataset, it is useful
to assess its difficulty by providing baseline results
for other researchers to compare to and hopefully
improve upon.

3.1 Setup

In order to obtain these baseline results, we train
Neural Machine Translation (NMT) models in
both direction using Nematus6 (Sennrich et al.,
2017). Our objective here is not to compete with
syntax-aware techniques such as Yin and Neubig
(2017) but to assess a lower bound on the task per-
formance on this dataset without using knowledge
of the structure of the programming language.

We prepare our datasets considering the func-
tion declarations as part of the input for both
the documentation and generation tasks. In or-
der to reduce data sparsity, we sub-tokenize with
the Moses (Koehn et al., 2007) tokenization script
(which splits some source code identifiers that
contain punctuation) followed by Byte-Pair En-
coding (BPE) (Sennrich et al., 2016b). BPE sub-
tokenization has been shown to be effective for
natural language processing, and for code process-
ing it can be considered a data-driven alternative
to the heuristic identifier sub-tokenization of Al-
lamanis et al. (2015a). We train our models with
the Adam optimizer (Kingma and Ba, 2015) with
learning rate 10−4, batch size 20. We use a vo-
cabulary size of 89500 tokens and we cap training
sequence length to 300 tokens for both the source
side and the target side. We apply ”Bayesian” re-
current dropout (Gal and Ghahramani, 2016) with
drop probability 0.2 and word drop probability
0.1. We perform early stopping by computing
the likelihood every 10000 on the validation set
and terminating when no improvement is made for
more than 10 times. For the code documentation
task, we use word embedding size 500, state size
500 and no backpropagation-through-time gradi-
ent truncation. For the code generation task, we
use word embedding size 400, state size 800 and
BPTT gradient truncation at 200 steps. These dif-
ferences are motivated by GPU memory consider-
ations.

After training the code documentation model,
we apply it to the corpus-only datasets to gener-
ate synthetic docstrings. We then combine this

6https://github.com/EdinburghNLP/
nematus

316



d e f i n t e r c e p t d o t (w, X, y ) :
””” Computes y ∗ np . d o t (X, w) .
I t t a k e s i n t o c o n s i d e r a t i o n i f t h e i n t e r c e p t s h o u l d be f i t o r n o t .
P a r a m e t e r s
−−−−−−−−−−
w : n d a r r a y , n d a r r a y , shape ( n f e a t u r e s , ) o r ( n f e a t u r e s + 1 , )

C o e f f i c i e n t v e c t o r .
[ . . . ]
”””
c = 0 .
i f w. s i z e == X. shape [ 1 ] + 1 :

c = w[−1]
w = w[: −1]

z = s a f e s p a r s e d o t (X, w) + c
yz = y ∗ z
r e t u r n w, c , yz

d e f i n t e r c e p t d o t (w, X, y ) :

’ Computes y ∗ np . d o t (X, w) . DCNL I t t a k e s i n t o c o n s i d e r a t i o n i f t h e i n t e r c e p t s h o u l d
be f i t o r n o t . DCNL P a r a m e t e r s DCNL w : n d a r r a y , shape ( n f e a t u r e s , ) o r (

n f e a t u r e s + 1 , ) DCNL C o e f f i c i e n t v e c t o r . DCNL [ . . . ] ’

DCSP c = 0 . 0 DCNL DCSP i f (w. s i z e == (X. shape [ 1 ] + 1) ) : DCNL DCSP DCSP c = w[(−1) ]
DCNL DCSP DCSP w = w[ : ( −1 ) ] DCNL DCSP z = ( s a f e s p a r s e d o t (X, w) + c ) DCNL

DCSP yz = ( y ∗ z ) DCNL DCSP r e t u r n (w, c , yz )
g i t h u b / s c i k i t −l e a r n / s c i k i t −l e a r n / s k l e a r n / l i n e a r m o d e l / l o g i s t i c . py 39

Figure 1: A Python function with its extracted declaration, docstring, body and repository metadata.

System BLEU
valid. test

Code-to-docstring 14.03 13.84
Docstring-to-code (base) 10.32 10.24
Docstring-to-code (backtransl.) 10.85 10.90

Table 3: Code documentation and code genera-
tion accuracy (multi-bleu.perl).

semi-synthetic corpus to the main parallel cor-
pus to train another code generation model, with
the same hyperparameters as above, according to
the backtranslation approach of Sennrich et al.
(2016a).

3.2 Results

We report BLEU scores for our models in table 3.
Backtranslation provides a moderate improvement
of 0.5− 0.6 BLEU points over the base model.

Both tasks on this dataset appear to be very
challenging, in comparison with the previously
published results in the 60 − 85 BLEU range by
Oda et al. (2015) and Yin and Neubig (2017)
on other Python corpora (DJANGO and Hearth-
stone), which are unusually high compared to
machine translation between natural languages,
where reaching 40 BLEU points is challenging.
While BLEU is only a shallow approximation of
model accuracy, these large differences are suffi-

cient to demonstrate the challenging nature of our
dataset compared to the existing datasets. We con-
jecture that this indicative of the strength of our
dataset at representing the true complexity of the
tasks.

4 Conclusions

We argue that the challenging nature of code doc-
umentation and code generation is not well rep-
resented by the existing corpora because of their
drawbacks in terms of noise, size and structural
properties.

We introduce a large and diverse parallel corpus
of Python functions with their docstrings scraped
from public repositories. We report baseline re-
sults on this dataset using Neural Machine Trans-
lation, noting that it is much more challenging
than previously published corpora as evidenced by
translation scores. We argue that our corpus bet-
ter captures the complexity of code documentation
and code generation as done by human program-
mers and may enable practical applications.

We believe that our contribution may stimu-
late research in this area by promoting the devel-
opment of more advanced models that can fully
tackle the complexity of these tasks. Such mod-
els could be, for instance, integrated into IDEs to
provide documentation stubs given the code, code

317



stubs given the documentation or context-aware
autocomplete suggestions. As future work, we en-
courage the creation of similar corpora for other
programming languages which support standard-
ized code documentation, such as Java.

Finally, we hope that this research area even-
tually improves the understanding and possible
replication of the human ability to reason about
algorithms.

References
Miltiadis Allamanis, Earl T Barr, Christian Bird, and

Charles Sutton. 2015a. Suggesting accurate method
and class names. In Proceedings of the 2015 10th
Joint Meeting on Foundations of Software Engineer-
ing, pages 38–49. ACM.

Miltos Allamanis and Charles Sutton. 2013. Mining
source code repositories at massive scale using lan-
guage modeling. In Working Conference on Mining
Software Repositories (MSR).

Miltos Allamanis, Daniel Tarlow, Andrew Gordon, and
Yi Wei. 2015b. Bimodal modelling of source code
and natural language. In Proceedings of the 32nd In-
ternational Conference on Machine Learning, pages
2123–2132, Lille, France. PMLR.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
learning to align and translate. arXiv preprint
arXiv:1409.0473.

Avishkar Bhoopchand, Tim Rocktäschel, Earl Barr,
and Sebastian Riedel. 2016. Learning python code
suggestion with a sparse pointer network. arXiv
preprint arXiv:1611.08307.

Mauro Cettolo, Jan Niehues, Sebastian Stüker, Luisa
Bentivogli, and Marcello Federico. 2016. Report on
the 13th IWSLT Evaluation Campaign. In IWSLT
2016, Seattle, USA.

Noam Chomsky. 1956. Three models for the descrip-
tion of language. IRE Transactions on information
theory, 2(3):113–124.

Yarin Gal and Zoubin Ghahramani. 2016. A Theoreti-
cally Grounded Application of Dropout in Recurrent
Neural Networks. In Advances in Neural Informa-
tion Processing Systems 29 (NIPS).

Alex Graves, Greg Wayne, and Ivo Danihelka.
2014. Neural turing machines. arXiv preprint
arXiv:1410.5401.

Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and
Luke Zettlemoyer. 2016. Summarizing source code
using a neural attention model. In Proceedings of
the 54th Annual Meeting of the Association for Com-
putational Linguistics, pages 2073–2083.

Diederik P. Kingma and Jimmy Ba. 2015. Adam: A
Method for Stochastic Optimization. In The Inter-
national Conference on Learning Representations,
San Diego, California, USA.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al. 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of the 45th annual meeting of the ACL on
interactive poster and demonstration sessions, pages
177–180.

Wang Ling, Edward Grefenstette, Karl Moritz Her-
mann, Tomáš Kočiskỳ, Andrew Senior, Fumin
Wang, and Phil Blunsom. 2016. Latent predic-
tor networks for code generation. arXiv preprint
arXiv:1603.06744.

George A Miller. 2003. The cognitive revolution: a
historical perspective. Trends in cognitive sciences,
7(3):141–144.

Yusuke Oda, Hiroyuki Fudaba, Graham Neubig,
Hideaki Hata, Sakriani Sakti, Tomoki Toda, and
Satoshi Nakamura. 2015. Learning to generate
pseudo-code from source code using statistical ma-
chine translation (t). In Automated Software Engi-
neering (ASE), 2015 30th IEEE/ACM International
Conference on, pages 574–584. IEEE.

F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine learning
in Python. Journal of Machine Learning Research,
12:2825–2830.

Chris Quirk, Raymond Mooney, and Michel Galley.
2015. Language to code: Learning semantic parsers
for if-this-then-that recipes. In Proceedings of the
53rd Annual Meeting of the Association for Compu-
tational Linguistics and the 7th International Joint
Conference on Natural Language Processing, pages
878–888, Beijing, China.

Rico Sennrich, Orhan Firat, Kyunghyun Cho, Alexan-
dra Birch, Barry Haddow, Julian Hitschler, Marcin
Junczys-Dowmunt, Samuel Läubli, Antonio Vale-
rio Miceli Barone, Jozef Mokry, and Maria Nade-
jde. 2017. Nematus: a Toolkit for Neural Machine
Translation. In Proceedings of the Demonstrations
at the 15th Conference of the European Chapter of
the Association for Computational Linguistics, Va-
lencia, Spain.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016a. Improving Neural Machine Translation
Models with Monolingual Data. In Proceedings of
the 54th Annual Meeting of the Association for Com-
putational Linguistics, pages 86–96, Berlin, Ger-
many.

318



Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016b. Neural Machine Translation of Rare Words
with Subword Units. In Proceedings of the 54th An-
nual Meeting of the Association for Computational
Linguistics, pages 1715–1725, Berlin, Germany.

Pengcheng Yin and Graham Neubig. 2017. A syntac-
tic neural model for general-purpose code genera-
tion. In The 55th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL), Vancou-
ver, Canada.

319


