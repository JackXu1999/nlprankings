



















































A Feature Structure Algebra for FTAG


Proceedings of the 13th International Workshop on Tree Adjoining Grammars and Related Formalisms (TAG+13), pages 1–10,
Umeå, Sweden, September 4–6, 2017. c© 2017 Association for Computational Linguistics

A Feature Structure Algebra for FTAG

Alexander Koller
Saarland University

koller@coli.uni-saarland.de

Abstract

FTAG, the extension of TAG with fea-
ture structures, lags behind other feature-
based grammar formalisms in the avail-
ability of efficient chart parsers. This is in
part because of the complex interaction of
adjunction and unification, which makes
such parsers inconvenient to implement.
We present a novel, simple algebra for fea-
ture structures and show how FTAG can
be encoded as an Interpreted Regular Tree
Grammar using this algebra. This yields a
straightforward, efficient chart parsing al-
gorithm for FTAG.

1 Introduction

Like many other grammar formalisms, tree-
adjoining grammars (TAG) have been extended
with feature structures to model linguistic phe-
nomena such as agreement conveniently. The
FTAG formalism of Vijay-Shanker and Joshi
(1988) equips each node in each elementary tree
with a “top” and “bottom” feature structure. These
are unified with each other at the end of the deriva-
tion if no auxiliary tree is adjoined at this node;
otherwise they are unified with feature structures
from the root and foot node of such an auxiliary
tree. FTAG has been used successfully in large-
scale grammar engineering, such as in the XTAG
grammar (XTAG Research Group, 2001).

One aspect in which FTAG has lagged behind
other feature grammar formalisms, such as HPSG
and LFG, is in parsing. Recent efficient parsers
for TAG, such as MICA (Bangalore et al., 2009)
and Alto (Koller and Kuhlmann, 2012; Groschwitz
et al., 2016), do not support feature structures.
The recent TuLiPA parser (Kallmeyer et al., 2010)
does support feature structures in FTAG parsing,
but can be inefficient in practice because it enu-

merates all TAG derivation trees and then checks
each of them for feature structure violations indi-
vidually, instead of checking features on the parse
chart directly. On the theoretical side, Schmitz
and Le Roux (2008) explain FTAG through a
feature-based formalism for describing languages
of derivation trees, with unclear implications for
parsing efficiency. Overall, the sense is that be-
cause of the complex interaction of unification and
adjunction in FTAG, implementing efficient FTAG
parsers is uncomfortable and something that tends
to get avoided.

In this paper, we offer a simple and modular
approach to efficient parsing with FTAG. We en-
code an FTAG grammar into an Interpreted Reg-
ular Tree Grammar (IRTG, Koller and Kuhlmann
(2011)) by extending the TAG-to-IRTG encoding
of Koller and Kuhlmann (2012) with an additional
interpretation into feature structures. This inter-
pretation maps each derivation tree into a term
over a novel algebra of feature structures, in a
similar way as c-structures are mapped into f-
structures in LFG (Kaplan and Bresnan, 1982).
This term can be evaluated to a value in the al-
gebra if and only if all unifications required by the
grammar succeed. We then show how known al-
gorithms for IRTG chart parsing – which can be
efficient for TAG (Groschwitz et al., 2016) – ex-
tend naturally to FTAG parsing.

We offer a view on FTAG which brings it
more in line with other feature-based grammar for-
malisms, in that the distinction between top and
bottom feature structures is represented correctly,
but requires no special treatment by the parser.
This simplifies, for instance, the use of existing
unification algorithms. At the same time, we offer
a very general and modular approach to checking
feature unification on a parse chart; no unpacking
of the individual derivation trees is required in our
algorithm. This approach generalizes straightfor-

1






WORD boy

AGR

[
NUM sg
GEN masc

]






SUBJ
[

AGR 1
]

AGR 1
[

NUM pl
]




(a) (b)

Figure 1: Two example feature structures.

wardly to other mechanisms for filtering deriva-
tion trees, as long as they can be expressed in
terms of finite-state constraints on trees.

Plan of the paper. We will review FTAG in Sec-
tion 2 and IRTGs and the TAG-to-IRTG encoding
in Section 3. We will then define the feature struc-
ture algebra and show how it can be used to encode
FTAG in Section 4. We show how to do efficient
and modular chart parsing for FTAG in Section 5.
Section 6 concludes.

2 TAG and feature structures

2.1 Feature structures
Feature structures (Shieber, 1986; Kasper and
Rounds, 1986; Carpenter, 1992) are used in many
grammar formalisms to represent grammatical in-
formation. Intuitively, a feature structure (FS) as-
signs values to a finite set of features; these val-
ues may either be atomic, or they may be fea-
ture structures themselves. For instance, the FS
in Fig. 1a specifies that the WORD feature has the
atomic value ‘boy’, and the value of the AGR fea-
ture is a feature structure with the features NUM
and GEN.

Technically, feature structures represent di-
rected, acyclic graphs in which both the top-level
FS and all of the FSs it contains are nodes. If F
has a feature FT, and the value of this feature in F
is G, then there is an edge (with label FT) from the
node F to the node G. We define a feature path to
be a sequence F1.F2. . . . .Fn of features Fi. Then
if F is a feature structure, we write F.f1. . . . .fn
for feature selection, i.e. for the node in F that is
reached by the given feature path. We define the
depth of an FS as the length of the longest defined
feature path.

As a special case, it can happen that the same
node is reachable via two different feature paths,
i.e. we have F.p1 = F.p2 for feature paths p1 6=
p2. This is called a reentrancy in F . We repre-
sent reentrancies in the attribute-value matrix no-
tation of Fig. 1 by marking the endpoints of the
reentrant feature paths with the same number. In
Fig. 1b, the marker 1 indicates that F.SUBJ.AGR

S

NP↓ VP

sleeps
NP

boy

NP

the NP*

bot: [num: X = sg]

top: [num: X]

bot: [num: sg, det: -]
top: [det: +]

top: [num: X, det: +]

bot: [num: X, det: -]

α1

α2

β1

Figure 2: An example FTAG derivation.

and F.AGR are the same node. Thus, we have
F.SUBJ.AGR.NUM = pl.

We say that the FS F subsumes the FS G if all
the information that F specifies, including reen-
trancies, is also present in G. The unification
F tG of F and G is the least informative FS that
is subsumed both by F and G. Not all FSs can
be unified. For instance, the two example FSs in
Fig. 1 cannot be unified because they specify con-
tradictory values for the feature path AGR.NUM, so
no feature structure can be subsumed by both.

2.2 Feature TAG

Vijay-Shanker and Joshi (1988) extend TAG with
feature structures, obtaining Feature-Based TAG
(FTAG). The core point about FTAG is that every
node in every elementary tree is decorated with
two feature structures (FSs) – one for the “top”
half of the node and one for the “bottom” half.
If no auxiliary tree is adjoined at a given node u,
these two feature structures must be unified with
each other. However, if an auxiliary tree β is ad-
joined, then the top FS of u will be unified with
the top FS of the root of β, and the bottom FS of
u will be unified with the bottom FS of the foot
node. Finally, substitution unifies the top FSs of
the substitution node and the root of the initial tree.
This makes unification consistent with adjunction,
and allows an elegant encoding of obligatory and
selective adjunction constraints in terms of FSs.

FTAG assumes that the value of each feature in
each top or bottom FS is an atomic value, ensuring
that all FSs have depth one. This makes the set of
possible FSs finite, and thus the expressive capac-
ity of FTAG is the same as that of TAG without
features.

An example derivation of FTAG is shown in
Fig. 2. This derivation combines three elemen-
tary trees α1, α2, β1 by substituting α2 into the
substitution node of α1 and then adjoining β1 into
the root of α2. Every node in these elementary
trees is annotated with a top and bottom feature

2



structure; feature structures whose value is >, the
trivial feature structure which subsumes all other
feature structures, are not shown in the figure. The
top and bottom FSs of the root of α2 cannot be uni-
fied with each other because they require different
values for the “det” feature; thus a derivation tree
in which nothing is adjoined into this node would
be ungrammatical. By adjoining β1 into this node,
we instead unify the top FS of α2’s root with the
top FS of β1’s root, and the bottom FS of α2’s root
with the bottom FS of β1’s foot node. Both unifi-
cations succeed.

FTAG requires that multiple occurrences of the
same variable (such as X) within the FSs of the
same elementary tree must be instantiated with the
same value. Thus when the occurrence ofX in the
bottom FS of β1’s foot node is bound to ‘sg’, the
occurrence of X in the top FS of β1’s root gets
the value ‘sg’ too. Further unifications percolate
this value into the occurrence of X in α1, which is
consistent with the requirement defined at α1’s V
node. By contrast, an elementary tree for “boys”
would assign ‘pl’ to the “num” feature at its root
node, making this unification fail and establishing
ungrammaticality of “boys sleeps”.

The decision to assign feature structures to
nodes (and not elementary trees), and to assign
two of them to each node, is necessary to make
unification consistent with adjunction. However,
it makes implementing parsers inconvenient, be-
cause we do not even know what FSs need to be
unified with each other before we have decided
whether and what to adjoin at each node. One way
to read this paper is as an illustration that these
decisions are not so inextricably linked with each
other as it may seem at first glance. Instead, unifi-
cations can simply be performed bottom-up at the
level of the derivation tree, enabling efficient chart
parsing.

We identify the nodes in an elementary tree α
through their node addresses, i.e. words π ∈ N∗
such that � is the address of the root, and πk is the
address of the k-th child of π. Given an elemen-
tary tree α in an FTAG grammar, we write topα(π)
for the top FS at the node π and botα(π) for the
bottom FS. We combine the two into the feature
structure

φα(π) =

[
TOP topα(π)

BOT botα(π)

]

We drop the subscript α if the elementary tree
is clear from the context.

3 Interpreted Regular Tree Grammars

We tackle FTAG parsing below by encoding it
as an Interpreted Regular Tree Grammar (IRTG,
Koller and Kuhlmann (2011)). IRTG is a grammar
formalism in which a language of derivation trees
is described using a regular tree grammar (RTG,
(Comon et al., 2008)) or, equivalently, a finite tree
automaton, and each derivation tree is then inter-
preted in one or more algebras. Different grammar
formalisms can be encoded into IRTG by varying
the algebras; for instance, (Koller and Kuhlmann,
2012) encoded TAG into IRTG by introducing two
novel algebras for strings and trees.

An IRTG which encodes the TAG grammar un-
derlying the derivation in Fig. 2 is shown in Fig. 3.
The first column contains an RTG G which de-
scribes a language L(G) of derivation trees. The
RTG contains one rule for each elementary tree
α in the TAG grammar, expanding a nontermi-
nal symbol of the form XS if α is an initial tree
with root symbol X , or of the form XA if α is an
auxiliary tree (cf. Schmitz and Le Roux (2008)).
Each rule also specifies the substitutions that must
and the adjunctions that may take place at this el-
ementary tree. Each of these nonterminals must
be expanded by applying another RTG rule, corre-
sponding to performing the respective substitution
and adjunction. We can choose not to adjoin an
auxiliary tree at a node where an adjunction could
take place by expanding the nonterminal XA with
the rule nopX . One derivation tree t ∈ L(G) is
shown in Fig. 4; note the obvious correspondence
to the derivation tree underlying the TAG deriva-
tion in Fig. 2.

This derivation tree can now be mapped into
a term over an algebra, and evaluated there into
an object of this algebra. In general, an algebra
over the signature Σ of operation symbols is a
structure A = (A, I) consisting of a set A (the
domain of A) and an interpretation function I.
This function assigns to each operation symbol
f ∈ Σ of arity k a function I(f) that takes k ar-
guments from A and returns a value in A. Thus, a
term τ over the signature Σ can be evaluated to a
value JτK recursively by letting Jf(τ1, . . . , τk)K =
I(f)(Jτ1K, . . . , JτkK).

The second column of Fig. 3 shows how the ex-
ample IRTG interprets derivation trees into strings.
This assumes the TAG string algebra As =
(As, Is) defined by Koller and Kuhlmann (2012).
The values As of this algebra are all strings and

3



RTG rule string homomorphism hs FS homomorphism hF
SS → α1(NPS , SA,VPA) wrap21(x2, conc11(x1,wrap21(x3, sleeps))) unify(cα1 , embi1(x1), emba�(x2), emba2(x3))
NPS → α2(NPA) wrap21(x1, boy) unify(cα2 , emba�(x1))
NPA → β1(NPA) wrap21(x1, conc12(the, ∗)) unify(cβ1 , emba�(x1))
XA → nopX ∗ cnop

Figure 3: The FTAG from Fig. 2, encoded as an IRTG. There is a nop rule for each nonterminal X ∈
{S,VP,NP}.

string pairs over a given alphabet; in the example,
the alphabet contains the words “sleeps”, “boy”,
and “the”. The signature ∆s contains a constant
for each of these words, a constant * denoting
the pair (�, �) of empty strings, plus a number
of binary operations which combine strings and
string pairs. In particular, Is(conc11) is a func-
tion which takes two strings as arguments and
concatenates them; Is(conc12) takes a string v
and a string pair (w1, w2) as arguments and con-
catenates them into the string pair (vw1, w2); and
Is(wrap21) takes a string pair (v1, v2) and a string
w as arguments, and then “wraps” the string pair
around the string, yielding the string v1wv2. Now
the IRTG G has an interpretation (hs,As). A
derivation tree t is recursively mapped to a term
hs(t) by the tree homomorphism hs, and then
hs(t) is evaluated to a value Jhs(t)K in As. In
this way, t describes a string. For instance, for the
derivation tree t in Fig. 4, we obtain Jhs(t)K =
“the boy sleeps”.

In general, an IRTG G =
(G, (h1,A1), . . . , (hn,An)) consists of an
RTG G and n ≥ 1 interpretations. It describes
the language L(G) = {(Jh1(t)K, . . . , Jhn(t)K) |
t ∈ L(G)}. Thus, if we consider only the first and
second column in the example grammar in Fig. 3,
we have a grammar which describes a language
of strings, and captures precisely the TAG part of
the grammar underlying Fig. 2. The third column
is for the feature structure interpretation we define
below, and extends the IRTG into an encoding of
an FTAG grammar.

4 A feature structure algebra for FTAG

As the grammar in Fig. 3 illustrates, the basic intu-
ition of the TAG-to-IRTG encoding is to traverse
the derivation tree bottom-up, calculating an in-
terpretation for each node in the derivation tree.
Each rule of the IRTG specifies the function by
which the interpretation for the parent is calcul-
cated from those of the children; so for instance,
the second column of Fig. 3 expresses for each el-

α1

nopVPnopSα2

β1

nopNP

Figure 4: Derivation tree of the IRTG in Fig. 3,
representing the (F)TAG derivation of Fig. 2.

ementary tree of the TAG grammar how it com-
bines the strings of its children with each other.

We will follow the same intuition here and spec-
ify how the feature structure for a subtree of the
derivation tree is computed out of the FSs of its
parts. We will first introduce an algebra F for fea-
ture structures, with novel operations that are suit-
able for FTAG, and then show how an FTAG can
be converted into an IRTG over this algebra.

4.1 A feature structure algebra

We define an algebra F = (F , I) consisting of
a set F of feature structures and an interpretation
function I. We let F contain all feature structures
whose feature paths have the form π tb f , where
π ∈ N∗ ∪ {RT, FT} is either a node address in an
elementary tree or a special feature for the root or
foot node, respectively; tb ∈ {TOP, BOT}; and f is
a feature from an FTAG grammar, as in Section 2.
The value reached under each feature path is either
an atomic value, such as ‘sg’, or the placeholder
>, which unifies with any atomic value. We as-
sume that every feature structure in F contains a
RT feature; it may or may not contain a FT feature
as well.

We define I to provide interpretations for the
following four classes of function symbols.

• Any constant c is interpreted as a feature
structure I(c) ∈ F . The construction in
Section 4.2 will yield a finite set of feature
structures F ∈ F , one for each elementary

4



tree. We assume that F has a constant cF with
I(cF ) = F for each of these.

• For any path π ∈ N∗, we have a func-
tion symbol embiπ for initial-tree embed-
ding. Given a feature structure F ∈ F , we let
I(embiπ) = EIπ with EIπ(F ) = [π F.RT].

• For any path π ∈ N∗, we have a function
symbol embaπ for auxiliary-tree embedding.
Given a feature structure F ∈ F , we let
I(embaπ) = EAπ with

EAπ(F ) =


π

[
TOP F .RT.TOP
BOT F .FT.BOT

]


• The binary function symbol unify represents
unification of two feature structures. For
any two arguments F,G ∈ F , we let
I(unify)(F,G) = F t G if F and G can
be unified; otherwise I(unify)(F,G) is un-
defined.

Note that F is a partial algebra, in that not every
term over the algebra has a value. This happens in
particular if the unify operation attempts to unify
two feature structures which cannot be unified. We
say that JtK is undefined in such a case.

4.2 Encoding FTAG with the feature
structure algebra

We will now use this algebra to encode arbitrary
FTAG grammars into IRTGs. We extend the TAG-
to-IRTG encoding of Koller and Kuhlmann (2012)
with an additional interpretation (hF ,F) into the
feature structure algebra (see the third column of
Fig. 3). This means that we need to define, for
each elementary tree α in the FTAG grammar, an
image hF (α) for the homomorphism into the FS
algebra, which specifies how α combines the FSs
of its children.

Let us say that the elementary tree α
was encoded into an IRTG rule N →
α(N

(1)
S , . . . , N

(k)
S , N

(k+1)
A , . . . N

(n)
A ) by the con-

version in Section 3, with k child nonterminals for
substitution and n − k for adjunction. The homo-
morphic image hF (α) will select the RT and FT
entries of the FSs for the child nonterminals and
unify them with one large FS T (α) representing
the functional behavior of α; we will define T (α)
below. Let cα be the constant with I(cα) = T (α)

(assumed to exist above). Then we let

hF (α) = unify(cα,
embiπ1(X1), . . . , embiπk(Xk),
embaπk+1(Xk+1), . . . , embaπn(Xn)),

where we abbreviate the n− 1 binary unify opera-
tions that are needed to combine the arguments by
simply writing unify once.

We construct T (α) = F (α) t I(α) t N(α) t
R(α) by unifying four smaller feature structures,
each of which encodes one specific aspect of α:

1. A feature structure F (α) which captures the
top and bottom feature structures at each
node of α. For any node π of α, F (α) con-
tains an entry [π φα(π)].

2. A feature structure I(α) which enforces the
coindexations for features in α that share
variables. If a variable X occurs both at fea-
ture f1 in the tb1 feature structure of the node
π1 (where tb1 ∈ {TOP, BOT}) and at feature
f2 in the tb2 FS of the node π2, we let




π1

[
tb1

[
f1 1

]]

π2

[
tb2

[
f2 1

]]




subsume I(α).

3. A feature structure N(α) which unifies the
top and bottom feature structures at nodes
where nothing can be adjoined. This includes
all foot nodes, as well as all nodes explicitly
marked with a null adjunction constraint. The
IRTG rule contains no child nonterminal for
adjoining at this node; thus without N(α),
the top and bottom FSs at these nodes would
not get unified at all. For all such nodes π,
we let 

π
[

TOP 1

BOT 1

]


subsume N(α). Note that substitution nodes
do not generate entries in N(α), as this
would unify the top and bottom FS of the root
of an initial tree substituted there.

4. A feature structure R(α) which makes the
correct sub-feature-structures accessible un-
der the RT and FT features. If α is an initial

5



I(cα1) = T (α1) I(cα2) = T (α2) I(cβ1) = T (β1)



� 1

[
TOP >
BOT >

]

1


TOP

[
NUM 2

]

BOT >




2




TOP >
BOT

[
NUM 2 sg

]



RT 1







� 1




TOP
[

DET +
]

BOT

[
NUM sg
DET -

]




RT 1







� 1




TOP >

BOT

[
DET +
NUM 2

]



2 3




TOP 4

[
DET -
NUM 2

]

BOT 4 >




RT 1

FT 3




Figure 5: Feature structure encodings for the elementary trees in Fig. 2.

tree, we let

R(α) =

[
RT 1

� 1

]

If α is an auxiliary tree with its foot node at
address π, we let

R(α) =




RT 1

FT 2

� 1

π 2




In addition to rules that encode elementary
trees, the IRTG also contains rules XA → nopX
for every nonterminal symbol X . For these sym-
bols, we let hF (nopN ) = cnop, with a constant
cnop which evaluates to the feature structure

I(cnop) = Fnop =




RT
[

TOP 1
]

FT
[

BOT 1
]




Thus, whenever we choose not to adjoin an aux-
iliary tree at a certain node, the IRTG encodes this
with a nop operation. When the FS for this nop
operation is unified into the parent FS using an
embaπ operation, this unifies the top and bottom
feature structures of π, as required in FTAG.

4.3 An example

We illustrate this construction with the example
FTAG grammar from Fig. 2. The homomorphism
hF is shown in the third column of Fig. 3; it uses
constants cα1 , cα2 , cβ1 , whose values are the FSs
shown in Fig. 5.

unify

emba2

cnop

unify

emba�

cnop

unify

embi1

unify

emba�

unify

emba�

cnop

cβ1

cα2

cα1

Figure 6: The term hF (t) over the feature structure
algebra for the derivation tree t from Fig. 4.

Note first that all three FSs have one feature for
each (non-lexical) node in the respective elemen-
tary tree, with a TOP and BOT feature nested below,
representing the upper and lower feature structure
of that node. This part of the feature structure is
contributed by F (α). Any two occurrences of the
same variable are coindexed through I(α); see e.g.
the index 2 in T (α1) and the index 2 in T (β1).

Furthermore, all nodes where no adjunction can
take place have had their top and bottom feature
structure coindexed byN(α). The index 4 for the
foot node at position 2 in β1 was introduced like
this. Finally, all three FSs have a RT feature, con-
tributed by R(α) and coindexed with the � feature
for the root node of the elementary tree. Because
β1 is an auxiliary tree, T (β1) also has a FT feature,
coindexed with the foot node at position 2.

Now consider the IRTG derivation tree t in
Fig. 4, which encodes the TAG derivation in Fig. 2.

6



T (α1) t




1




TOP

[
DET +
NUM 2

]

BOT

[
DET -
NUM 2 sg

]






t


�

[
TOP 3

BOT 3

]
t


2

[
TOP 4

BOT 4

]
 =




� 1

[
TOP 3

BOT 3

]

1




TOP

[
DET +
NUM 2

]

BOT

[
DET -
NUM 2 sg

]




2




TOP 5

BOT 5
[

NUM 2 sg
]



RT 1




Figure 7: Computing the value JhF (t)K of the term in Fig. 6.

The homomorphism hF maps t to a term hF (t)
over the FS algebra F, shown in Fig. 6. We have
marked with a box the root of each subtree hF (t′)
where t′ is a subtree of t. Observe that each
“block” between two boxed nodes has a consis-
tent shape, in that it unifies cα with a number of
FSs, each of which is obtained by embedding the
FS for a subtree into the features of T (α).

The term hF (t) can then be evaluated to a fea-
ture structure JhF (t)K in the algebra F. The last
step of computing JhF (t)K for the entire term
hF (t) in Fig. 6 is shown in Fig. 7. We per-
form three unification operations, with the first
argument being I(cα1) = T (α1) and the sec-
ond being the value of the sub-derivation-tree
α2(β1(nopNP)), embedded at the substitution
node 1. The other two arguments come from the
nop nodes, embedded at the appropriate features
through embaπ operations. Notice that the top and
bottom feature structures at 1 are not coindexed,
because we adjoined β1 into the root of α2, and
the top and bottom features are not coindexed in
T (β1).

5 Parsing

We now turn to the issue of FTAG parsing. By en-
coding an FTAG grammar as an IRTG with an in-
terpretation into the FS algebra, we can apply stan-
dard methods from IRTG parsing to FTAG pars-
ing. We will first compute a parse chart for the
input string while ignoring the feature structures,
and then intersect it with an RTG that carries out
the unifications.

The specific parsing problem we solve is as fol-
lows: Given an IRTG G = (G, (hs,As), (hF ,F))
and an input string w, find a compact represen-

tation Cw of the set parses(w) = {t ∈ L(G) |
Jhs(t)K = w and JhF (t)K is defined} – that is, the
derivation t must be grammatical according to G;
it must map to the input string under the string in-
terpretation; and all unifications required by the
feature structure interpretation succeed. We will
represent this parse chart Cw as an RTG such that
L(Cw) = parses(w).

5.1 Parsing without feature structures

In a first step, we apply standard methods
from IRTG parsing (Koller and Kuhlmann, 2011;
Groschwitz et al., 2016) to obtain a parse chart of
all grammatical derivation trees that interpret tow.
We do this by computing a decomposition gram-
mar Dw for w in the TAG string algebra – i.e., an
RTG such that L(Dw) is the set of all terms over
the TAG string algebra that evaluate to w. We then
look at the set LI = h−1s (L(Dw)) of derivation
trees t such that hs(t) ∈ L(Dw), i.e. of deriva-
tion trees that interpret to w. Because regular tree
languages are closed under inverse tree homomor-
phisms (Comon et al., 2008), we can calculate an
RTG Iw such that L(Iw) = LI . Because regular
tree languages are also closed under intersection,
we can then intersect Iw with G to obtain an RTG
C0w such that L(C

0
w) = {t ∈ L(G) | Jhs(t)K =

w}. We call C0w the pre-chart for w.
Consider, by way of example, the pre-chart C0w

for the input string w = “the boy sleeps” and
our example grammar; see Koller and Kuhlmann
(2012) for details. The nonterminals of C0w are
pairs of nonterminals of the derivation tree RTG
G and nonterminals of Iw. Nonterminals of Iw of
the form i−k can derive terms which evaluate to
the substring of w from position i to k − 1; we
write pairs of nonterminals Ns with such nonter-

7



minals as [Ns, i−k]. Nonterminals of Iw can also
be of the form (i−j, k−l), for terms which evalu-
ate to a pair of substrings of w (from position i to
j − 1 and from k to l − 1 respectively). We write
pairs of nonterminals NA with such nonterminals
as 〈Na, i−j, k−l〉. Note that a span i−i represents
an empty string at position i. Some of the rules in
C0w are as follows:

〈NPA, 1−1, 3−3〉 → nopNP
〈NPA, 2−2, 3−3〉 → nopNP
〈NPA, 1−2, 3−3〉 → β1(〈NPA, 1−1, 3−3〉)

[NPS , 2−3] → α2(〈NPA, 2−2, 3−3〉)
[NPS , 1−3] → α2(〈NPA, 1−2, 3−3〉)

Using these rules, we can derive both the sub-
derivation-tree t1 = α2(nopNP), which evaluates
to the string “boy” (as an NP) on the string in-
terpretation of the IRTG, and the sub-derivation-
tree t2 = α2(β1(nopNP)), which evaluates to “the
boy”. Notice that while both of these subtrees
are allowed by the underlying TAG grammar, only
hF (t2) can be evaluated over the FS algebra. To
evaluate hF (t1), we would have to unify the top
and bottom FS at the root of α2, which fails.

5.2 Feature structure filtering

In order to exclude t1 from the language of the
chart, while retaining t2, we can define an RTG
FG, which tracks feature structures and filters out
trees with unification failures. The nonterminals
of FG are symbols [F ], where F is any feature
structure in F ; this is a finite set because we have
limited the depth of these feature structures to
three. We can then define rules for FG which sim-
ply interpret the function symbols of F:

[I(c)] → c for constants c
[EIπ(F )] → embiπ([F ])

[EAπ(F )] → embaπ([F ])
[F tG] → unify([F ], [G]) if defined

We add a start symbol SFG and rules SFG → [F ]
for all feature structures F . Then the language
L(FG) consists exactly of all terms τ over F such
that JτK is defined. As a consequence, we can in-
tersect C0w with h

−1
F (FG) to obtain the chart Cw,

which describes only derivation trees that can be
interpreted in the FS algebra, i.e. where all unifi-
cations succeed.

In our example, the intersection Cw of C0w with

FG contains (among others) the following rules:

〈NPA, 1−1, 3−3, Fnop〉 → nopN
〈NPA, 2−2, 3−3, Fnop〉 → nopNP
〈NPA, 1−2, 3−3, F1〉 → β1(〈NPA, 1−1, 3−3, Fnop〉)

[NPS , 1−3, F2] → α2(〈NPA, 1−2, 3−3, F1〉)
[NPS , 1−3, SFG] → [NPS , 1−3, F2],

where F1 = T (β1) t EA�(Fnop) and F2 =
T (α2) t EA�(F1); we have already seen F2 as
the second argument of the unification in Fig. 7.
One can think of these rules as copies of the rules
from C0w, each decorated with a feature structure.
Observe that the rule for [NPS , 1−3], correspond-
ing to “the boy”, is simply extended with the FS
F2. The chart has further rules (not shown above)
which derive the derivation tree t from Fig. 4,
which contains t2 as a subtree, we we find that
t ∈ L(Cw) = parses(w).

By contrast, Cw does not contain decorated
rules for [NPS , 2−3]. This is because JhF (t1)K is
undefined in F, and therefore t1 is not in L(FG),
and the pre-chart rule for [NPS , 2−3] is filtered out
by the intersection algorithm.

5.3 Discussion

Our parsing algorithm computes the full parse
chart in two steps: We first parse the input into
the pre-chart and then refine the pre-chart into a
chart by intersecting it with an RTG which fil-
ters out derivation trees that do not unify. The
filter grammar FG and the intersection algorithm
can be implemented in a way that does not require
us to compute all rules of FG beforehand (which
would be extremely expensive). Instead, the in-
tersection algorithm can query the rules of FG as
it goes along, which essentially amounts to eval-
uating the operations of F on the feature structure
decorations.

In addition, this algorithm does not require enu-
merating all derivation trees before the unifica-
tions are checked; all unifications are performed
on chart items. This is in line with parsers for
grammar formalisms such as HPSG and especially
LFG, and in contrast to other parsers for FTAG.
At the same time, our algorithm avoids the expo-
nential blowup of lexical ambiguity which would
result from compiling the FTAG grammar into an
ordinary TAG grammar. The filtering method pre-
sented here could be used more generally to en-
force any well-formedness condition on derivation
trees which can be expressed by an RTG, such as
well-typedness given a type system.

8



5.4 Implementation

We have implemented the FS algebra and the fil-
ter grammar described above in the context of
the Alto IRTG parser (Gontrum et al., 2017).
Alto is available open-source from https://
bitbucket.org/tclup/alto.

Our implementation uses well-known efficient
algorithms for unification (Tomabechi, 1991) and
subsumption checking (Malouf et al., 2000). Sub-
sumption checking is needed because as the inter-
section algorithm discovers new candidate rules
for Cw, it has to check whether another rule
with an equal feature structure already exists; this
equality checking is performed by testing whether
each FS subsumes the other one. Parsing effi-
ciency could be improved further by replacing this
with asymmetric subsumption checks and retain-
ing only the chart rule with the less informative
FS (Zhang et al., 2007).

6 Conclusion

We have defined an algebra of feature structures
and used it to encode FTAG grammars into IRTG
grammars with two interpretations – one over a
TAG string algebra and one over the feature struc-
ture algebra. We have shown how to do FTAG
parsing by intersecting a parse chart with respect
to the input string with an RTG which attempts to
carry out the unifications required by the FTAG
grammar. This yields a clean, LFG-style sepa-
ration of the derivation process of TAG (includ-
ing adjunction) from unification, and an efficient
parsing algorithm that performs unifications on the
chart and not the individual derivation trees.

One advantage of our FS algebra is that it can be
combined freely with any other grammar formal-
ism that can be encoded in IRTG. It should, for in-
stance, be straightforward to define feature-based
LCFRS by adding variants of embaπ for rules
of higher fan-out. By adding further interpreta-
tions, we can also build feature-based synchronous
grammars, e.g. for semantic parsing (Peng et al.,
2015; Groschwitz et al., 2015). An interesting
challenge would be to attempt to encode LFG
into IRTG, using a string interpretation for the c-
structure and a variant of the FS algebra intro-
duced here for the f-structure. This would require
dealing with arbitrarily deep feature structures (as
opposed to ones of bounded depth as in FTAG),
and encoding LFG’s completeness and coherence
constraints into a filter grammar.

Acknowledgments. I am grateful to the review-
ers for their insightful and constructive com-
ments, and to Jessica Grasso, Jonas Groschwitz,
Christoph Teichmann, and Stefan Thater for dis-
cussions. I am also indebted to the students in
my Grammar Formalisms classes at the University
of Potsdam for demanding a faster FTAG parser,
and those at Saarland University for being the first
users of the system described here.

References
Srinivas Bangalore, Pierre Boullier, Alexis Nasr, Owen

Rambow, and Benoit Sagot. 2009. MICA: A prob-
abilistic dependency parser based on Tree Insertion
Grammars. In Proceedings of NAACL HLT 2009:
Short Papers.

Bob Carpenter. 1992. The logic of typed feature struc-
tures. Cambridge University Press.

Hubert Comon, Max Dauchet, Remi Gilleron, Florent
Jacquemard, Denis Lugiez, Christof Lding, Sophie
Tison, and Marc Tommasi. 2008. Tree automata
techniques and applications. Available on http:
//tata.gforge.inria.fr/.

Johannes Gontrum, Jonas Groschwitz, Alexander
Koller, and Christoph Teichmann. 2017. Alto:
Rapid prototyping for parsing and translation. In
Proceedings of the 15th Conference of the European
Chapter of the Association for Computational Lin-
guistics (EACL): Demo Session.

Jonas Groschwitz, Alexander Koller, and Mark John-
son. 2016. Efficient techniques for parsing with tree
automata. In Proceedings of the 54th Annual Meet-
ing of the Association for Computational Linguistics
(ACL).

Jonas Groschwitz, Alexander Koller, and Christoph Te-
ichmann. 2015. Graph parsing with s-graph gram-
mars. In Proceedings of the 53rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL-IJCNLP).

Laura Kallmeyer, Wolfgang Maier, Yannick Parmen-
tier, and Johannes Dellert. 2010. TuLiPA – Pars-
ing extensions of TAG with Range Concatenation
Grammars. Bulletin of the Polish Academy of Sci-
ences – Technical Sciences 58(3):377–391.

Ronald Kaplan and Joan Bresnan. 1982. Lexical-
functional grammar: A formal system for grammat-
ical representation. In Joan Bresnan, editor, The
Mental Representation of Grammatical Relations,
MIT Press, Cambridge, MA, pages 173–381.

Robert Kasper and William Rounds. 1986. A logical
semantics for feature structures. In Proceedings of
the 24th Annual Meeting of the Association for Com-
putational Linguistics (ACL).

9



Alexander Koller and Marco Kuhlmann. 2011. A gen-
eralized view on parsing and translation. In Pro-
ceedings of the 12th International Conference on
Parsing Technologies (IWPT).

Alexander Koller and Marco Kuhlmann. 2012. De-
composing TAG algorithms using simple alge-
braizations. In Proceedings of the 11th TAG+ Work-
shop.

Robert Malouf, John Carroll, and Ann Copestake.
2000. Efficient feature structure operations with-
out compilation. Natural Language Engineering
6(1):29–46.

Xiaochang Peng, Linfeng Song, and Daniel Gildea.
2015. A synchronous hyperedge replacement gram-
mar based approach for AMR parsing. In Proceed-
ings of the Nineteenth Conference on Computational
Natural Language Learning (CoNLL).

Sylvain Schmitz and Joseph Le Roux. 2008. Feature
unification in TAG derivation trees. In Proceedings
of the 9th TAG+ Workshop.

Stuart Shieber. 1986. An introduction to unification-
based approaches to grammar. CSLI Publications.

Hideto Tomabechi. 1991. Quasi-destructive graph uni-
fication. In Proceedings of the 29th Annual Meet-
ing of the Association for Computational Linguistics
(ACL).

K. Vijay-Shanker and Aravind Joshi. 1988. Feature
structures based tree-adjoining grammar. In Pro-
ceedings of COLING.

XTAG Research Group. 2001. A lexicalized tree
adjoining grammar for english. Technical Report
IRCS-01-03, IRCS, University of Pennsylvania.
ftp://ftp.cis.upenn.edu/pub/xtag/
release-2.24.2001/tech-report.pdf.

Yi Zhang, Stephan Oepen, and John Carroll. 2007. Ef-
ficiency in unification-based n-best parsing. In Pro-
ceedings of the 10th International Conference on
Parsing Technologies (IWPT).

10


