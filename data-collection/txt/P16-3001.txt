



















































Controlled and Balanced Dataset for Japanese Lexical Simplification


Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics – Student Research Workshop, pages 1–7,
Berlin, Germany, August 7-12, 2016. c©2016 Association for Computational Linguistics

Controlled and Balanced Dataset for Japanese Lexical Simplification

Tomonori Kodaira Tomoyuki Kajiwara
Tokyo Metropolitan University

Hino City, Tokyo, Japan
{kodaira-tomonori, kajiwara-tomoyuki}@ed.tmu.ac.jp, komachi@tmu.ac.jp

Mamoru Komachi

Abstract

We propose a new dataset for evaluating
a Japanese lexical simplification method.
Previous datasets have several deficien-
cies. All of them substitute only a sin-
gle target word, and some of them extract
sentences only from newswire corpus. In
addition, most of these datasets do not al-
low ties and integrate simplification rank-
ing from all the annotators without consid-
ering the quality. In contrast, our dataset
has the following advantages: (1) it is the
first controlled and balanced dataset for
Japanese lexical simplification with high
correlation with human judgment and (2)
the consistency of the simplification rank-
ing is improved by allowing candidates to
have ties and by considering the reliability
of annotators.

1 Introduction

Lexical simplification is the task to find and sub-
stitute a complex word or phrase in a sentence
with its simpler synonymous expression. We de-
fine complex word as a word that has lexical and
subjective difficulty in a sentence. It can help in
reading comprehension for children and language
learners (De Belder and Moens, 2010). This task
is a rather easier task which prepare a pair of com-
plex and simple representations than a challeng-
ing task which changes the substitute pair in a
given context (Specia et al., 2012; Kajiwara and
Yamamoto, 2015). Construction of a benchmark
dataset is important to ensure the reliability and
reproducibility of evaluation. However, few re-
sources are available for the automatic evaluation
of lexical simplification. Specia et al. (2012) and
De Belder and Moens (2010) created benchmark
datasets for evaluating English lexical simplifica-

tion. In addition, Horn et al. (2014) extracted sim-
plification candidates and constructed an evalua-
tion dataset using English Wikipedia and Simple
English Wikipedia. In contrast, such a parallel cor-
pus does not exist in Japanese. Kajiwara and Ya-
mamoto (2015) constructed an evaluation dataset
for Japanese lexical simplification1 in languages
other than English.

However, there are four drawbacks in the
dataset of Kajiwara and Yamamoto (2015): (1)
they extracted sentences only from a newswire
corpus; (2) they substituted only a single target
word; (3) they did not allow ties; and (4) they
did not integrate simplification ranking consider-
ing the quality.

Hence, we propose a new dataset addressing
the problems in the dataset of Kajiwara and Ya-
mamoto (2015). The main contributions of our
study are as follows:

• It is the first controlled and balanced dataset
for Japanese lexical simplification. We ex-
tract sentences from a balanced corpus and
control sentences to have only one com-
plex word. Experimental results show that
our dataset is more suitable than previous
datasets for evaluating systems with respect
to correlation with human judgment.

• The consistency of simplification ranking is
greatly improved by allowing candidates to
have ties and by considering the reliability of
annotators.

Our dataset is available at GitHub2.

2 Related work

The evaluation dataset for the English Lexical
Simplification task (Specia et al., 2012) was an-

1http://www.jnlp.org/SNOW/E4
2https://github.com/KodairaTomonori/EvaluationDataset

1



sentence 「技を出し合い、気分が高揚するのがたまらない」とはいえ、技量で相手を上回りたい気持ちも強い。
Although using their techniques makes you feel exalted, I strongly feel I want to outrank my competitors in terms of skill.

paraphrase list 盛り上がる 高まる高ぶる 上がる 高揚する 興奮する 熱を帯びる 活性化する
come alive raised, excited up exalted excited heated revitalized

Figure 1: A part of the dataset of Kajiwara and Yamamoto (2015).

notated on top of the evaluation dataset for English
lexical substitution (McCarthy and Navigli, 2007).
They asked university students to rerank substi-
tutes according to simplification ranking. Sen-
tences in their dataset do not always contain com-
plex words, and it is not appropriate to evaluate
simplification systems if a test sentence does not
include any complex words.

In addition, De Belder and Moens (2012) built
an evaluation dataset for English lexical simplifi-
cation based on that developed by McCarthy and
Navigli (2007). They used Amazon’s Mechanical
Turk to rank substitutes and employed the relia-
bility of annotators to remove outlier annotators
and/or downweight unreliable annotators. The re-
liability was calculated on penalty based agree-
ment (McCarthy and Navigli, 2007) and Fleiss’
Kappa. Unlike the dataset of Specia et al. (2012),
sentences in their dataset contain at least one com-
plex word, but they might contain more than one
complex word. Again, it is not adequate for the
automatic evaluation of lexical simplification be-
cause the human ranking of the resulting simpli-
fication might be affected by the context contain-
ing complex words. Furthermore, De Belder and
Moens’ (2012) dataset is too small to be used for
achieving a reliable evaluation of lexical simplifi-
cation systems.

3 Problems in previous datasets for
Japanese lexical simplification

Kajiwara and Yamamoto (2015) followed Specia
et al. (2012) to construct an evaluation dataset for
Japanese lexical simplification. Namely, they split
the data creation process into two steps: substitute
extraction and simplification ranking.

During the substitute extraction task, they col-
lected substitutes of each target word in 10 differ-
ent contexts. These contexts were randomly se-
lected from a newswire corpus. The target word
was a content word (noun, verb, adjective, or ad-
verb), and was neither a simple word nor part of
any compound words. They gathered substitutes
from five annotators using crowdsourcing. These
procedures were the same as for De Belder and

Moens (2012).
During the simplification ranking task, annota-

tors were asked to reorder the target word and its
substitutes in a single order without allowing ties.
They used crowdsourcing to find five annotators
different from those who performed the substitute
extraction task. Simplification ranking was inte-
grated on the basis of the average of the simplifi-
cation ranking from each annotator to generate a
gold-standard ranking that might include ties.

During the substitute extraction task, agreement
among the annotators was 0.664, whereas during
the simplification ranking task, Spearman’s rank
correlation coefficient score was 0.332. Spear-
man’s score of this work was lower than that of
Specia et al. (2012) by 0.064. Thus, there was a
big blur between annotators, and the simplifica-
tion ranking collected using crowdsourcing tended
to have a lower quality.

Figure 1 shows a part of the dataset of Kajiwara
and Yamamoto (2015). Our discussion in this pa-
per is based on this example.

Domain of the dataset is limited. Because Ka-
jiwara and Yamamoto (2015) extracted sentences
from a newswire corpus, their dataset has a poor
variety of expression. English lexical simplifica-
tion datasets (Specia et al., 2012; De Belder and
Moens, 2012) do not have this problem because
both of them use a balanced corpus of English
(Sharoff, 2006).

Complex words might exist in context. In Fig-
ure 1, even when a target word such as “高揚す
る (feel exalted)” is simplified, another complex
word “技量 (skill)” is left in a sentence. Lexi-
cal simplification is a task of simplifying complex
words in a sentence. Previous datasets may in-
clude multiple complex words in a sentence but
target only one complex word. Not only the tar-
get word but also other complex words should be
considered as well, but annotation of substitutes
and simplification ranking to all complex words
in a sentence produces a huge number of pat-
terns, therefore takes a very high cost of anno-
tation. For example, when three complex words

2



Figure 2: Process of constructing the dataset.

which have 10 substitutes each in a sentence, an-
notators should consider 103 patterns. Thus, it is
desired that a sentence includes only simple words
after the target word is substituted. Therefore, in
this work, we extract sentences containing only
one complex word.

Ties are not permitted in simplification rank-
ing. When each annotator assigns a simplifica-
tion ranking to a substitution list, a tie cannot be
assigned in previous datasets (Specia et al., 2012;
Kajiwara and Yamamoto, 2015). This deterio-
rates ranking consistency if some substitutes have
a similar simplicity. De Belder and Moens (2012)
allow ties in simplification ranking and report con-
siderably higher agreement among annotators than
Specia et al. (2012).

The method of ranking integration is naı̈ve.
Kajiwara and Yamamoto (2015) and Specia et al.
(2012) use an average score to integrate rankings,
but it might be biased by outliers. De Belder and
Moens (2012) report a slight increase in agreement
by greedily removing annotators to maximize the
agreement score.

4 Balanced dataset for evaluation of
Japanese lexical simplification

We create a balanced dataset for the evaluation of
Japanese lexical simplification. Figure 2 illustrates
how we constructed the dataset. It follows the data
creation procedure of Kajiwara and Yamamoto’s
(2015) dataset with improvements to resolve the
problems described in Section 3.

We use a crowdsourcing application, Lancers,3

3http://www.lancers.jp/

Figure 3: Example of annotation of extracting sub-
stitutes. Annotators are provided with substitutes
that preserve the meaning of target word which is
shown bold in the sentence. In addition, annota-
tors can write a substitute including particles.

to perform substitute extraction, substitute evalua-
tion, and substitute ranking. In each task, we re-
quested the annotators to complete at least 95% of
their previous assignments correctly. They were
native Japanese speakers.

4.1 Extracting sentences

Our work defines complex words as “High Level”
words in the Lexicon for Japanese Language Edu-
cation (Sunakawa et al., 2012).4 The word level is
calculated by five teachers of Japanese, based on
their experience and intuition. There were 7,940
high-level words out of 17,921 words in the lex-
icon. In addition, target words of this work com-
prised content words (nouns, verbs, adjectives, ad-
verbs, adjectival nouns, sahen nouns,5 and sahen
verbs6).

Sentences that include a complex word were
randomly extracted from the Balanced Corpus of
Contemporary Written Japanese (Maekawa et al.,
2010). Sentences shorter than seven words or
longer than 35 words were excluded. We excluded
target words that appeared as a part of compound
words. Following previous work, 10 contexts of
occurrence were collected for each complex word.
We assigned 30 complex words for each part of
speech. The total number of sentences was 2,100
(30 words × 10 sentences × 7 parts of speech).
We used a crowdsourcing application to annotate
1,800 sentences, and we asked university students
majoring in computer science to annotate 300 sen-
tences to investigate the quality of crowdsourcing.

4.2 Extracting substitutes

Simplification candidates were collected using
crowdsourcing techniques. For each complex
word, five annotators wrote substitutes that did not

4http://jhlee.sakura.ne.jp/JEV.html
5Sahen noun is a kind of noun that can form a verb by

adding a generic verb “suru (do)” to the noun. (e.g. “修理
repair”)

6Sahen verb is a sahen noun that accompanies with
“suru”. (e.g. “修理する (do repair)”)

3



Dataset balanced lang sents. noun (％) verb (％) adj. (％) adv. (％) outlier
De Belder and Moens (2012) yes En 430 100 (23.3) 60 (14.0) 160 (37.2) 110 (25.6) excluded
Specia et al. (2012) yes En 2,010 580 (28.9) 520 (25.9) 560 (27.9) 350 (17.6) included
Kajiwara and Yamamoto (2015) no Ja 2,330 630 (27.0) 720 (30.9) 500 (21.5) 480 (20.6) included
This work yes Ja 2,010 570 (28.3) 570 (28.3) 580 (28.8) 290 (14.4) excluded

Table 1: Comparison of the datasets. In this work, nouns include sahen nouns, verbs include sahen verbs,
and adjectives include adjectival nouns.

Figure 4: Example of annotation of evaluating
substitutes. Annotators choose substitutes that fit
into the sentence from substitutes list.

Figure 5: Example of annotation of ranking sub-
stitutes. Annotators write rank in blank. Addition-
ally, they are allowed to write a tie.

change the sense of the sentence. Substitutions
could include particles in context. Conjugation
was allowed to cover variations of both verbs and
adjectives. Figure 3 shows an example of annota-
tion.

To improve the quality of the lexical substi-
tution, inappropriate substitutes were deleted for
later use, as described in the next subsection.

4.3 Evaluating substitutes

Five annotators selected an appropriate word to
include as a substitution that did not change the
sense of the sentence. Substitutes that won a ma-
jority were defined as correct. Figure 4 shows an
example of annotation.

Nine complex words that were evaluated as not
having substitutes were excluded at this point. As
a result, 2,010 sentences were annotated, as de-
scribed in next subsection.

4.4 Ranking substitutes

Five annotators arranged substitutes and complex
words according to the simplification ranking. An-
notators were permitted to assign a tie, but they
could select up to four items to be in a tie because
we intended to prohibit an insincere person from
selecting a tie for all items. Figure 5 shows an ex-

ample of annotation.

4.5 Integrating simplification ranking

Annotators’ rankings were integrated into one
ranking, using a maximum likelihood estimation
(Matsui et al., 2014) to penalize deceptive annota-
tors as was done by De Belder and Moens (2012).
This method estimates the reliability of annotators
in addition to determining the true order of rank-
ings. We applied the reliability score to exclude
extraordinary annotators.

5 Result

Table 1 shows the characteristics of our dataset. It
is about the same size as previous work (Specia
et al., 2012; Kajiwara and Yamamoto, 2015). Our
dataset has two advantages: (1) improved correla-
tion with human judgment by making a controlled
and balanced dataset, and (2) enhanced consis-
tency by allowing ties in ranking and removing
outlier annotators. In the following subsections,
we evaluate our dataset in detail.

5.1 Intrinsic evaluation

To evaluate the quality of the ranking integration,
the Spearman rank correlation coefficient was cal-
culated. The baseline integration ranking used an
average score (Kajiwara and Yamamoto, 2015).
Our proposed method excludes outlier annotators
by using a reliability score calculated using the
method developed by Matsui et al. (2014).

1
|P |

∑
p1,p2∈P

p1 ∩ p2
p1 ∪ p2 (1)

Pairwise agreement is calculated between each
pair of sets (p1, p2 ∈ P ) from all the possible pair-
ings (P) (Equation 1). The agreement among an-
notators from the substitute evaluation phase was
0.669, and agreement among the students is 0.673,
which is similar to the level found in crowdsourc-
ing. This score is almost the same as that from
Kajiwara and Yamamoto (2015). On the contrary,

4



sentence 最も安上りにサーファーを装う方法は，ガラムというインドネシア産のタバコを，これ見よがしに吸うことです．
The most simplest method that is imitating safer is pretentiously smoke that Garam which is Indonesian cigarette.

paraphrase list 1.のふりをする 2.に見せかける 3.の真似をする,の振りをする 4.を真似る 5.に成りすます 6.を装う 7.を偽る
professing counterfeiting playing, professing playing pretending imitating falsifying

Figure 6: A part of our dataset.

genre PB PM PN LB OW OT OP OB OC OY OV OL OM all
sentence 0 64 628 6 161 90 170 700 1 0 6 9 175 2,010

average of substitutes 0 4.12 4.36 5.17 4.41 4.22 3.9 4.28 4 0 5.5 4.11 4.45 4.3

Table 3: Detail of sentences and substitutes in our dataset. (BCCWJ comprise three main subcorpora:
publication (P), library (L), special-purpose (O). PB = book, PM = magazine, PN = newswire, LB = book,
OW = white paper, OT = textbook, OP =PR paper, OB = bestselling books, OC = Yahoo! Answers,
OY = Yahoo! Blogs, OL = Law, OM = Magazine)

baseline outlier removal
Average 0.541 0.580

Table 2: Correlation of ranking integration.

the Spearman rank correlation coefficient of the
substitute ranking phase was 0.522. This score
is higher than that from Kajiwara and Yamamoto
(2015) by 0.190. This clearly shows the impor-
tance of allowing ties during the substitute ranking
task.

Table 2 shows the results of the ranking inte-
gration. Our method achieved better accuracy in
ranking integration than previous methods (Specia
et al., 2012; Kajiwara and Yamamoto, 2015) and
is similar to the results from De Belder and Moens
(2012). This shows that the reliability score can be
used for improving the quality.

Table 3 shows the number of sentences and av-
erage substitutes in each genre. In our dataset, the
number of acquired substitutes is 8,636 words and
the average number of substitutes is 4.30 words
per sentence.

Figure 6 illustrates a part of our dataset. Substi-
tutes that include particles are found in 75 context
(3.7%). It is shown that if particles are not permit-
ted in substitutes, we obtain only two substitutes (4
and 7). By permitting substitutes to include parti-
cles, we are able to obtain 7 substitutes.

In ranking substitutes, Spearman rank correla-
tion coefficient is 0.729, which is substantially
higher than crowdsourcing’s score. Thus, it is nec-
essary to consider annotation method.

5.2 Extrinsic evaluation
In this section, we evaluate our dataset using five
simple lexical simplification methods. We calcu-

This work K & Y annotated
Frequency 41.6 35.8 41.0
# of Users 32.9 25.0 31.5
Familiarity 30.4 31.5 32.5
JEV 38.2 35.7 38.7
JLPT 42.0 40.9 43.3
Pearson 0.963 0.930 N/A

Table 4: Accuracy and correlation of the datasets.

late 1-best accuracy in our dataset and the dataset
of Kajiwara and Yamamoto (2015). Annotated
data is collected by our and Kajiwara and Ya-
mamoto (2015)’s work in ranking substitutes task,
and which size is 21,700 ((2010 + 2330) × 5)
rankings. Then, we calculate correlation between
the accuracies of annotated data and either those
of Kajiwara and Yamamoto (2015) or those of our
dataset.

5.2.1 Lexical simplification systems

We used several metrics for these experiments:

Frequency Because it is said that a high fre-
quent word is simple, most frequent word is se-
lected as a simplification candidate from substi-
tutes using uni-gram frequency of Japanese Web
N-gram (Kudo and Kazawa, 2007). This uni-gram
frequency is counted from two billion sentences in
Japanese Web text.

Number of Users Aramaki et al. (2013) claimed
that a word used by many people is simple, so we
pick the word used by the most of users. Number
of Users were estimated from the Twitter corpus
created by Aramaki et al. (2013). The corpus con-
tains 250 million tweets from 100,000 users.

5



Familiarity Assuming that a word which is
known by many people is simple, replace a target
word with substitutes according to the familiarity
score using familiarity data constructed by Amano
and Kondo (2000). The familiarity score is an av-
eraged score 28 annotators with seven grades.

JEV We hypothesized a word which is low dif-
ficulty for non-native speakers is simple, so we
select a word using a Japanese learner dictionary
made by Sunakawa et al. (2012). The word in
dictionary has a difficulty score averaged by 5
Japanese teachers with their subjective annotation
according to six grade system.

JLPT Same as above, but uses a different source
called Japanese Language Proficient Test (JLPT).
We choose the lowest level word using levels of
JLPT. These levels are a scale of one to five.

5.2.2 Evaluation

We ranked substitutes according to the metrics,
and calculated the 1-best accuracy for each tar-
get word. Finally, to compare two datasets, we
used the Pearson product-moment correlation co-
efficient between our dataset and the dataset of
Kajiwara and Yamamoto (2015) against the anno-
tated data.

Table 4 shows the result of this experiment. The
Pearson coefficient shows that our dataset corre-
lates with human annotation better than the dataset
of Kajiwara and Yamamoto (2015), possibly be-
cause we controlled each sentence to include only
one complex word. Because our dataset is bal-
anced, the accuracy of Web corpus-based metrics
(Frequency and Number of Users) closer than the
dataset of Kajiwara and Yamamoto (2015).

6 Conclusion

We have presented a new controlled and balanced
dataset for the evaluation of Japanese lexical sim-
plification. Experimental results show that (1)
our dataset is more consistent than the previous
datasets and (2) lexical simplification methods us-
ing our dataset correlate with human annotation
better than the previous datasets. Future work in-
cludes increasing the number of sentences, so as
to leverage the dataset for machine learning-based
simplification methods.

References
Shigeaki Amano and Kimihisa Kondo. 2000. On the

NTT psycholinguistic databases “lexical properties
of Japanese”. Journal of the Phonetic Society of
Japan 4(2), pages 44–50.

Eiji Aramaki, Sachiko Maskawa, Mai Miyabe, Mizuki
Morita, and Sachi Yasuda. 2013. Word in a dic-
tionary is used by numerous users. In Proceeding
of International Joint Conference on Natural Lan-
guage Processing, pages 874–877.

Jan De Belder and Marie-Francine Moens. 2010. Text
simplification for children. In Proceedings of the SI-
GIR Workshop on Accessible Search Systems, pages
19–26.

Jan De Belder and Marie-Francine Moens. 2012. A
dataset for the evaluation of lexical simplification.
In Proceedings of the 13th International Conference
on Computational Linguistics and Intelligent Text
Processing, pages 426–437.

Colby Horn, Cathryn Manduca, and David Kauchak.
2014. Learning a lexical simplifier using Wikipedia.
In Proceedings of the 52nd Annual Meeting of the
Association for Computational Linguistics (Volume
2: Short Papers), pages 458–463.

Tomoyuki Kajiwara and Kazuhide Yamamoto. 2015.
Evaluation dataset and system for Japanese lexical
simplification. In Proceedings of the ACL-IJCNLP
2015 Student Research Workshop, pages 35–40.

Taku Kudo and Hideto Kazawa. 2007. Japanese Web
N-gram Version 1. Linguistic Data Consoritium.

Kikuo Maekawa, Makoto Yamazaki, Takehiko
Maruyama, Masaya Yamaguchi, Hideki Ogura,
Wakako Kashino, Toshinobu Ogiso, Hanae Koiso,
and Yasuharu Den. 2010. Design, compilation,
and preliminary analyses of balanced corpus of
contemporary written Japanese. In Proceedings of
the Seventh International Conference on Language
Resources and Evaluation, pages 1483–1486.

Toshiko Matsui, Yukino Baba, Toshihiro Kamishima,
and Hisashi Kashima. 2014. Crowdordering. In
Proceedings of the 18th Pacific-Asia Conference on
Knowledge Discovery and Data Mining, pages 336–
347.

Diana McCarthy and Roberto Navigli. 2007. Semeval-
2007 task 10: English lexical substitution task. In
Proceedings of the 4th International Workshop on
Semantic Evaluations, pages 48–53.

Serge Sharoff. 2006. Open-source corpora: Using the
net to fish for linguistic data. Journal of Corpus Lin-
guistics, 11(4), pages 435–462.

Lucia Specia, Sujay Kumar Jauhar, and Rada Mihalcea.
2012. SemEval-2012 task 1: English lexical sim-
plification. In Proceedings of the 6th International
Workshop on Semantic Evaluation, pages 347–355.

6



Yuriko Sunakawa, Jae-ho Lee, and Mari Takahara.
2012. The construction of a database to support the
compilation of Japanese learners dictionaries. Jour-
nal of the Acta Linguistica Asiatica 2(2), pages 97–
115.

7


