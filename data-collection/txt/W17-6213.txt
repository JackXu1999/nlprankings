



















































Linguistically Rich Vector Representations of Supertags for TAG Parsing


Proceedings of the 13th International Workshop on Tree Adjoining Grammars and Related Formalisms (TAG+13), pages 122–131,
Umeå, Sweden, September 4–6, 2017. c© 2017 Association for Computational Linguistics

Linguistically Rich Vector Representations of Supertags for TAG Parsing
Dan Friedman∗1 Jungo Kasai∗1 R. Thomas McCoy∗1

Robert Frank1 Forrest Davis2 Owen Rambow3
1Department of Linguistics, Yale University

2 Columbia University
3 DSI, Columbia University

{dan.friedman,jungo.kasai,richard.mccoy,robert.frank}@yale.edu
fld2111@columbia.edu

rambow@ccls.columbia.edu

Abstract

In this paper, we explore several tech-
niques for producing vector representa-
tions of TAG supertags that can be used
as inputs to a neural network-based TAG
parser. In the simplest case, the supertag is
encoded as a 1-hot vector that is projected
to a dense vector. Secondly, we use a
tree-recursive neural network that is given
as input the structure of the elementary
tree. Thirdly, we use hand-crafted feature
vectors that describe the syntactic features
of each supertag, and project these to a
dense vector. These three representations
are learned during the training of a neu-
ral network TAG parser with a layer that
embeds supertags in a low-dimensional
space. Finally, we consider an embedding
that is trained only on patterns of linear
co-occurrence among supertags. By test-
ing the resulting vector representations on
the task of completing syntactic analogies,
we show that these vector representations
capture syntactically relevant information.
While our linguistically-informed embed-
dings outperform atomic embeddings on
the syntactic analogy task, we find that the
same embeddings lead to only a slight im-
provement on the task of TAG parsing, in-
dicating that the neural parser is able to
induce useful representations of supertags
from the data alone.

1 Introduction

In a Tree Adjoining Grammar (TAG), the set of
elementary trees can be thought of as the pos-
sible lexical grammatical category assignments,
much like the part of speech tags in a Context

∗Equal Contribution.

Free Grammar (CFG). However, the number of
elementary trees is considerably larger than the
category set typically found in other formalisms.
In the TAG that is extracted from the Penn Tree-
bank by Chen (2001), there are more than 4,700
distinct elementary trees, as compared to the 48
POS tags found in the Penn Treebank or even the
1,286 categories found in the Combinatory Cat-
egorial Grammar (CCG) bank (Hockenmaier and
Steedman, 2007). While this is indeed a large
number, the set of elementary trees in a linguis-
tically adequate TAG is finely structured, with
systematic relationships holding between elemen-
tary trees. Past work on grammar development in
TAG has explored a variety of methods for captur-
ing the relationships among and within so-called
tree families (Vijay-Shanker and Schabes, 1992;
Evans et al., 1995; XTAG Research Group, 1998;
Becker, 2000), where all members of a tree family
have the same basic argument structure (or have
the same value for some other syntactic dimen-
sion) but differ from each other based on transfor-
mations such as passivization or wh-movement.

Under the approach suggested by Bangalore
and Joshi (1999), the first step of TAG parsing,
called supertagging, involves the assignment of el-
ementary trees to lexical items. Given the high
degree of supertag ambiguity and the fact that
state-of-the-art TAG supertagging accuracy is only
around 90% (using the bi-LISTM supertagger re-
ported in Kasai et al. (2017)) as compared to 95%
for CCG supertagging (Lewis et al., 2016), it is
useful indeed if the parser can be made sensi-
tive to the relationships between elementary trees.
Certain errors in the supertagger might then not
prove fatal to a parser if a single supertag can be
interpreted as related to other elementary trees,
providing potentially useful derivational options.
Furthermore, if relations among supertags are en-
coded, problems of data sparsity during training

122



might be overcome; nearly half of the supertags
present in the PTB WSJ Sections 1-22 appear only
once, but they may be related to other supertags
that occur more frequently.

Previous work by Chung et al. (2016) has pro-
posed a way of exploiting relationships among su-
pertags in a transition-based parser, by adding a
series of hand-coded linguistic features that char-
acterize properties of the elementary trees in the
grammar. Such features had a beneficial effect on
parser performance when used in conjunction with
lexical identity, supertag identity, and POS tag.

In this paper, we demonstrate how the use of
a neural network TAG parsing model, proposed
by Kasai et al. (2017), facilitates the representa-
tion of similarity among supertags. The input to
this parser is a sequence of 1-best supertags output
by a bidirectional LSTM supertagger. As the first
step in computation, the parsing network maps
each supertag into a vector via an embedding ma-
trix. Given a set of supertag vectors, we can study
the similarity relations among them using methods
similar to those that have been applied to lexical
vectors by Mikolov et al. (2013a,b). For example,
we can consider analogies between elementary
trees that correspond to an operation of detransi-
tivization, by asking whether an elementary tree
representing a clause headed by a transitive verb
(t27) stands in the same relationship to an elemen-
tary tree headed by an intransitive verb (t81) that a
subject relative clause elementary tree headed by a
transitive verb (t99) stands in to a subject relative
headed by an intransitive verb (t109). By inter-
preting these elementary trees as vectors, we can
express this analogy by t27 − t81 + t109 ≈ t99.
As we will demonstrate below, this formalization
allows us to study the degree to which a represen-
tational scheme successfully captures a wide range
of linguistic relationships among elementary trees.

Our discussion will compare four alternatives
for constructing supertag embeddings. Three of
these are trained in conjunction with parser, and
differ only in the representation of the supertag
input to the parser: atomic encodings of supertag
identity, recursive encoding of the structure of the
elementary tree, and the hand-coded linguistic fea-
tures from Chung et al. (2016). The fourth derives
embeddings via a GloVe-type model of distribu-
tional similarity (Pennington et al., 2014).

Recent work by McMahan and Stone (2016) has
proposed a method to embed TAG supertags in

the context of natural langauge generation. They
utilize structural information of elementary trees
through convolutional neural networks. Our recur-
sive encoding is similar to their approach in that
the embedding procedure respects tree structure of
supertags. It should be noted, however, that our in-
duction process runs in the opposite direction from
that in McMahan and Stone (2016). In their appli-
cation to natural language generation, the objec-
tive is surface realization from (unlabeled) depen-
dency trees, whereas the problem of interest in this
paper is derivation of dependency trees from sur-
face realization.

In the next section, we briefly describe the pars-
ing model that is the foundation of our experi-
ments, along with our four methods for construct-
ing supertag embeddings. Section 3 lays out our
experimental set-up and Section 4 explains how
we perform evaluation for supertag similarity and
for parsing. Section 5 reports the results and dis-
cusses their implications.

2 Parsing Model and Embedding
Construction

2.1 Neural Network TAG Parser

In our experiments, we make use of the neural net-
work TAG parser from Kasai et al. (2017). This is
an arc-eager shift-reduce parser that uses a feed-
forward neural network as an oracle. At each time
step, the oracle takes as input the configuration of
the parser, which consists of a fixed number of
cells from the top of the stack and the front of the
buffer, each containing a supertag. The parser’s
task is to construct a derivation tree from the indi-
vidual supertags. This derivation is constructed in
the usual way for transition-based parsers, namely
through a series of actions (shift, reduce, left-arc,
and right-arc). Left-arc and right-arc create links
in the derivation tree, and these operations are fur-
ther specified by the type of operation (substitution
and adjoining) as well as the node within the ele-
mentary tree to which the operation applies (spec-
ified for substitution as 0-4, encodings of the deep
grammatical role of the substitution site). The out-
put of the network is a softmax layer, whose acti-
vations can be interpreted as a probability distribu-
tion over actions and labels. The parser is unlex-
icalized; the only information that the parser uses
to determine its action is the supertags in the rel-
evant cells of the stack and buffer. For the cur-
rent work, we make use of the bi-LSTM supertag-

123



ger discussed in Kasai et al. (2017), which is pre-
trained on the WSJ Penn Treebank and does not
vary across the experiments reported here. This
supertagger provides as output a distribution over
possible supertags for each word in a sentence.

Our focus in this paper is the first layer of
the neural network, which maps each supertag
in a parser configuration to a vector in a low-
dimensional space. The input to the subsequent
feed-forward layer is the concatenation of the
dense vectors associated with the relevant cells in
the stack and buffer. In our experiments, we vary
the representation of the supertag that is provided
as input to the parser, and retrain. Our hypothesis
is that using a more linguistically informed em-
bedding function will produce linguistically inter-
pretable vector representations and improve pars-
ing accuracy.

2.2 Atomic Embedding
The first type of embedding function we consider
is the one from Chen and Manning (2014) (POS
tags), Lewis et al. (2016); Xu (2016) (CCG su-
pertags), and Kasai et al. (2017) (TAG supertags):
here, each supertag in a parse configuration is rep-
resented as a one-hot column vector in R|V |, where
|V | is the total number of supertags. That is, each
supertag is represented as a vector in which all en-
tries are 0 except for a single entry, which is 1,
corresponding to the integer ID of the supertag.
The supertags are then projected into a lower-
dimensional space by multiplying the one-hot vec-
tors with an embedding matrix L ∈Md×|V |. Thus
each supertag t is associated with a distinct vector
xt ∈ Rd, corresponding to a column in the em-
bedding matrix L. The embeddings are then con-
catenated and passed to the feed-forward network.
The embedding matrix L is trained jointly with the
parser via the back-propagation algorithm to opti-
mize the negative log-likelihood of outputting the
correct parser actions. As a result, although the
1-hot encoding does not convey any information
about relationships between supertags, the opti-
mization process may favor an embedding matrix
where the vector encoding of the supertag does
convey such information to the degree to which
similar supertags are best treated similarly by the
parser.

2.3 Recursive Tree-Based Embedding
There are several theoretical drawbacks to using
an atomic representation of supertags for parsing.

First, the parser makes no use of the linguistic
meaning of supertags. Each supertag is consid-
ered to be a distinct entity, and the only way for
the parser to associate similar supertags is to learn
associations in the process of optimizing the train-
ing objective. Second, supertag data is sparse:
of the 4,724 supertags in the TAG-annotated ver-
sion of the Penn Treebank, 2,165 occur only once
(Chen, 2001; Kasai et al., 2017). Because each
supertag is considered to be distinct in the input
layer, the parser will learn little information about
nearly half of all supertags.

For these reasons we next consider a tree-
recursive embedding layer that associates each su-
pertag with a low-dimensional vector by passing
the corresponding elementary tree through a recur-
sive neural network. A recursive neural network
(RNN)1 in a bottom-up fashion, by using a neu-
ral network layer to combine the hidden states of
the node’s constituents (Goller and Kuchler, 1996;
Socher et al., 2011). This recursive model has
the advantage of both encoding linguistic informa-
tion about supertags and also making efficient use
of sparse data: even if a supertag appears infre-
quently in the corpus, the parser learns the param-
eters for embedding that supertag from encounter-
ing other, structurally similar supertags.

We use a syntactically untied RNN, similar to
the model described in Socher et al. (2014). First,
for each leaf node j in an elementary tree, the hid-
den state hj is obtained by taking the jth column
of an embedding matrix E ∈ Md×|L|(R), where
|L| is the size of the vocabulary of labels used in
TAG trees. Then, for each non-terminal node i, let
C(i) denote the set of children of node i and let
rel(i, j) denote the relation between node i and its
child j, defined by the label of i and the left-to-
right position of j. For example, if i is a VP node
and j is its leftmost child, then rel(i, j) is VP0.
The hidden state of node i is then

hi = f


 ∑

j∈C(i)
Wrel(i,j)hj


 ,

where f is a nonlinear activation function. We use
tanh as the activation function in all of our experi-
ments. Wrel(i,j) is a square matrix in Md×d(R), so
the resulting hi is a vector in Rd.

1Note that throughout this paper we use the abbreviation
RNN to refer to recursive neural networks, not recurrent neu-
ral networks.

124



S(0)

NP0↓(1) VP(2)

V♦(3) NP1↓(4)

(a) The elementary tree for t27, a transitive verb. Nodes are
indexed by their position in the breadth-first traversal of the
tree.

xt27 =WROOTh0

h0 = f(WS0h1 +WS1h2)

h1 = eNP0↓ h2 = f(WV P0h3 +WV P1h4)

h3 = eV♦ h4 = eNP1↓

(b) The recursive activation states for t27. el is the column in
the embedding matrix E corresponding to label l.

Figure 1: Example of an RNN structure for generating a vector for a supertag.

Once the hidden state for each node in the tree
has been computed, we obtain the final vector rep-
resentation of the supertag by multiplying the hid-
den state of the root node by a special weight ma-
trix WROOT ∈ Md×d(R) and applying the activa-
tion function. Figure 1 gives an example of how
the RNN is used to generate a vector representa-
tion for a transitive verb.

As with the Atomic Embeddings, the weight
matricesW ’s andE can be learned during training
of the parser via backpropogation.

2.4 Feature-based Embeddings

Our third representation of supertags and cor-
responding embedding derives from the hand-
selected features associated with elementary trees
in the grammar used in MICA (Bangalore et al.,
2009). Each elementary tree is associated with
a set of dimensions describing phrase structure,
interpretation (e.g., subcategorization frame), and
linguistic transformations (e.g., dative shift). The
features include binary- and ternary-valued di-
mensions, whose values can be “yes”, “no”, or
“NA,” as well as dimensions whose values are a
part of speech tag or a list of part of speech tags.
See Chung et al. (2016) for the complete list of
features.

To feed these feature vectors to a neural net-
work, and to allow us to compare the vectors di-
rectly with our recursive embeddings, we con-
vert each feature vector into a d-dimensional real-
valued vector. We use two approaches to encod-
ing features. First, we encode binary- and ternary-
valued features as one-hot vectors. Since the other
fields take on a much larger range of possible val-
ues, we choose to embed those features in a low-
dimensional space. Specifically, we randomly ini-
tialize an embedding matrix E ∈ Mk×|L|(R),

where |L| is the number of part of speech labels,
and associate each part of speech label with a col-
umn in the matrix. We set k equal to 8. For
fields with list values–for example, the list of ad-
junction nodes–we pad the list with zeros to en-
sure a fixed width representation. We concatenate
the one-hot vectors and the label embeddings to
obtain an n-dimensional feature representation of
the supertag. In order to compare these vectors
more directly with our d-dimensional recursive
embeddings, we then multiply each vector by a
weight matrix W ∈Mn×d(R) to obtain a final, d-
dimensional vector for each supertag. Once again,
these embedding matrices are trained in conjunc-
tion with training of the parser.

2.5 GloVe Model
As a final point of comparison, we generate vector
representations of supertags by training a GloVe
model on our training corpus of supertags. The
GloVe model is a widely used method for gener-
ating dense representations of words from corpus
co-occurrence counts (Pennington et al., 2014). A
GloVe model is trained on a co-occurrence table
X ∈ M|V |×|V |(R), where Xij is the number of
times word i appears in the context of word j.
Context is determined by a hyperparameter c: an
occurrence of word i is in the context of an oc-
currence of word j if that occurrence of i appears
within the window of cwords on either side of that
occurrence of j. The cost function is

J =

|V |∑

i,j=1

f(Xij)(w
T
i w̃j + bi + b̃j − logWij)2.

f is a weighting function defined f(x) =
min(1, (x/xmax)

α), where xmax and α are hyper-
parameters to be tuned. wi and w̃j are the word

125



vector and the context vector for words i and j, re-
spectively. Similarly, bi and b̃j are word and con-
text biases for words i and j. Intuitively, the GloVe
model attempts to learn a dense representation of
words that will allow it to estimate the likelihood
that any pair of words co-occur.

We train a GloVe model to convergence on the
training portion of the TAG-annotated PTB train-
ing corpus, from which we were able to extract a
co-occurrence table for supertags. We then use the
resulting vectors to initialize an embedding matrix
for the parser.

3 Experimental Setups

The experiments we conducted employ the same
experimental setups as Kasai et al. (2017). Specif-
ically, we follow the protocol from Chung et al.
(2016) and Bangalore et al. (2009), and use the
grammar and the TAG-annotated WSJ Penn Tree
Bank extracted by Chen (2001). We use Sec-
tions 01-22 as the training set, Section 00 as the
development set, and Section 23 as the test set.
The training, development, and test sets comprise
39832, 1921, and 2415 sentences, respectively.

We use a publicly available string representa-
tion of supertags to associate each supertag with
an elementary tree.2 We consider the label of a
node in an elementary tree to consist of the part of
speech tag as well as the deep argument position
and the node type, if relevant. For example, an
NP node marked for substitution with a deep argu-
ment position of 0 will be labeled “NP0s” and will
be considered distinct from, for example, an NP
foot node (“NPf”). The relation between a node
i and a child j (rel(i, j)) is then considered to be
the label of i subscripted by the index of j in the
ordered list of children of i.

We implement the recursive neural network in
TensorFlow and TensorFlow Fold, a library for
creating TensorFlow models that take dynamically
sized, structure inputs, like trees (Looks et al.,
2017).3 For the sake of comparison with the re-
sults in Kasai et al. (2017), we use an embedding
size of 50 in all of our experiments and set all of
the hyperparameters of the parser to be the same
as the best performing ones. Specifically, we use
two fully-connected layers with 200 hidden units
each, dropout rates of 0.2 for the input and 0.3 for

2The grammar is available here: http://mica.lif.
univ-mrs.fr

3https://github.com/tensorflow/fold

the hidden layer, and 3 for the stack and buffer
scope. We train stochastically using the Adam op-
timization algorithm and minibatches of size 100.

We use a publicly available TensorFlow im-
plementation of the GloVe model (available
at https://github.com/GradySimon/
tensorflow-glove) training for 50 itera-
tions. The hyperparameters are the same as those
reported in Pennington et al. (2014). We use a
context size of 5, which we found performed
better than larger context windows.

4 Methods of Evaluation

4.1 Supertag Similarity and the Analogy
Task

In the literature on word embeddings (e.g.,
Mikolov et al. (2013a) and Pennington et al.
(2014)), word analogies are often used to evalu-
ate whether the word embeddings have captured
relevant semantic relationships between words.
Similarly, we use syntactic analogies to evalu-
ate whether our supertag embeddings have cap-
tured relevant syntactic relationships between su-
pertags. To create our test set, we considered 9
different syntactic transformations that can relate
two supertags either within or across tree fam-
ilies. These transformations are (i) subject rel-
ativization, (ii) object relativization, (iii) subject
wh-movement, (iv) object wh-movement, (v) tran-
sitivization, (vi) infinitivization,4 (vii) passiviza-
tion with a by phrase, (viii) passivization without
a by phrase, and (ix) dative shift.

For each of these transformations, we identified
all pairs of supertags (tag1, tag2) such that apply-
ing the transformation to tag1 yields tag2. For ex-
ample, if the transformation in question is subject
relativization, an example of such a pair would be
t27 (the supertag for the verb heading a transitive
clause, such as found in the boy found the trea-
sure) and t99 (the supertag for the verb heading a
transitive subject relative clause, such as found in
the boy who found the treasure). We then gener-
ated syntactic analogies by matching up two such
pairs that were both generated based on the same
transformation; for example, the pairs (t27, t99)
and (t81, t109) were both generated based on sub-
ject relativization, so combining these pairs gives

4Infinitivization is the process of turning a verbal tree with
a substitution node in subject position into a tree with an
empty category in its subject position. It is called infinitiviza-
tion because typically trees with empty subjects are anchored
on infinitival verbs.

126



the analogy “t27 is to t99 as t81 is to t109”, since
conducting subject relativization on t27 generates
t99 and conducting subject relativization on t81
generates t109. These trees are shown in Figure 2.

We wish to test if the linear relationships be-
tween our supertag embeddings properly express
the syntactic relationships between the elementary
trees these embeddings represent. To test if this
is the case, we represent each of our analogies in
the form of an equation; for example, the analogy
“t27 is to t99 as t81 is to t109” would be written
as t27 − t99 ≈ t81 − t109. We then rearrange
the equation so that there is only one term on the
right-hand side to get t27−t99+t109 ≈ t81. Each
such equation becomes one test in the test set, and
to test the equation we perform the arithmetic on
the left hand side (here, t27−t99+t109) using the
embeddings we have generated and evaluate how
similar the resulting vector is to the desired right
hand side (here t81).

We use two basic metrics for evaluating perfor-
mance. The first metric (% correct in Table 1) is
the proportion of analogies in the test set for which
the most similar vector to the result of the com-
putation is the desired one as measured by cosine
distance. The second metric (Avg. position in Ta-
ble 1) is the average rank of the correct answer
within the ranked list of the embeddings that have
the smallest cosine distance from the result of the
computation.

In addition, it may be the case that the embed-
dings we generate are better for more frequent su-
pertags due to a larger number of training exam-
ples on which to train the embedding. In order to
ascertain the effects of frequency, we also compute
both of the aforementioned metrics upon various
restricted test sets consisting of only those analo-
gies for which all four supertags in the analogy are
among the n most common supertags in the train-
ing set for some value n, and where the ranked
list of nearest neighbors is filtered to only include
the n most common supertags. These metrics are
listed as % correct (top n) and Avg. position (top
n) in Table 1.

4.2 Parsing

We also present parsing results for models trained
using each embedding scheme. We use the same
neural TAG parser in each experiment, varying
only the embedding layer. In all cases the param-
eters for the embedding layer are trained jointly

with the parser. We give labeled and unlabeled
attachment scores given gold supertags and pre-
dicted supertags from the supertagger in Kasai
et al. (2017), using first a greedy decoding strat-
egy and then beam search with a beam size of 16.

5 Results and Discussion

5.1 Analogy Task

Our results for the analogy task are given in Ta-
ble 1. The GloVe vectors perform significantly
worse than any other embedding, suggesting that
co-occurrence information alone in the absence
of information about grammatical structure is not
enough to learn the kind of syntactic information
tested in the analogy task.

The atomic embeddings trained with the parser
learn a surprising amount of syntactic information
about supertags. Despite initially having no infor-
mation about the underlying syntactic meaning of
supertags, the parser is able to discover syntactic
features in the process of optimizing parsing per-
formance on the training set. As we would ex-
pect, however, the performance of the atomic em-
beddings degrades quickly as n increases. The
atomic embeddings get around two thirds of the
analogy tests correct when the tests and answers
are restricted to the top 300 supertags, but the em-
beddings perform considerably worse with larger
values of n; the embeddings get only 4.62% ac-
curacy on analogy tests drawn from the full set of
supertags. This is consistent with what we would
expect: the atomic embedding scheme treats each
supertag as distinct from the others, and the major-
ity of supertags are very sparse in the training data,
so the parser has scarce information with which to
learn better representations for rare supertags.

In contrast, both the recursive embeddings and
the feature-based embeddings achieve very high
accuracy on the analogy tests, with hardly any
degradation with larger values of n. Of particu-
lar note is average position of the correct supertag
in the ranked list of possible answers. For both
the recursive and the feature-based embeddings,
the target supertag is, on average, within the top
2 supertags most similar to the result of the anal-
ogy tests, even using tests drawn from the full
set of supertags. This is not surprising given that
both embedding schemes encode syntactic infor-
mation about supertags by construction, so, un-
like atomic embeddings, they produce meaning-
ful representations of supertags even when the su-

127



S

NP0↓ VP

V♦ NP1↓
(a) The elementary tree for t27.

NP

NP* S

NP0↓ S

NP

-NONE-

VP

V♦ NP1↓

(b) The elementary tree for t99.

S

NP0↓ VP

V♦
(c) The elementary tree for t81.

NP

NP* S

NP0↓ S

NP

-NONE-

VP

V♦
(d) The elementary tree for t109.

Figure 2: The supertags involved in the subject-relativization-based analogy t27− t99 + t109 ≈ t81.

Embedding n # equations % correct % correct (top n) Avg. position Avg. position (top n)
GloVe 300 246 0.00 0.00 71.21 101.30
Atomic 300 246 50.40 67.07 7.98 2.36
RNN 300 246 83.33 93.50 1.83 1.08
Features 300 246 97.56 100 1.02 1.00
GloVe 500 776 0.13 0.13 128.04 158.16
Atomic 500 776 36.47 41.62 22.15 5.67
RNN 500 776 82.09 91.36 1.68 1.15
Features 500 776 95.62 99.87 1.05 1.00
GloVe 4724 57220 0.02 – 2086.35 –
Atomic 4724 57220 4.62 – 289.48 –
RNN 4724 57220 83.34 – 1.61 –
Features 4724 57220 94.14 – 1.10 –

Table 1: Analogy task results.

pertags appear infrequently in the training data.
The feature-based vectors in particular directly en-
code many of the dimensions we inspect with the
analogy tests since several of the transformations
used in the analogy tests, such as dative shift and
passivization, are among the features derived from
Chung et al. (2016) that we used.

The feature-based embeddings outperform the
recursive embeddings. In particular, the feature-
based embeddings seem to be even more robust
with larger values of n. The recursive embed-
dings achieve above 90% accuracy when the space
of possible answers is limited to the n most com-
mon supertags, but accuracy decreases by around
10% when we are allowed to consider all possible
supertags. Using feature-based embeddings, how-

ever, accuracy remains above 94% for all tests, and
is 100% for low values of n. One reason for this
disparity might be that the recursive neural net-
work, like the atomic embedding matrix, contains
parameters that might be updated rarely; the RNN
includes a weight matrix W for each parent-child
relation, and some relations (e.g., VP6) appear in
only a few rare elementary trees. By contrast, the
parameters trained to produce the feature-based
embeddings are shared across all elementary trees,
so the information learned by the parser general-
izes better to rare or even unseen supertags.

Figures 3 and 4 provide a visual representa-
tion of certain atomic embeddings by plotting their
first two principal components. They show how
even the comparatively syntactically uninformed

128



Figure 3: Graph of the first two principal components
of the atomic embeddings for supertag pairs related by
infinitivization.

Figure 4: Graph of the first two principal components
of the atomic embeddings for supertag pairs related by
subject relativization.

method of using atomic embeddings still creates
relatively consistent linear structure between pairs
of vectors related by the same syntactic transfor-
mations.

5.2 Parsing

The parsing results are in Table 2. Although both
of our linguistically informed representations of
supertags (the RNN embeddings and the feature-
based embeddings) significantly outperform the
atomic embeddings at the analogy task, these large
increases in performance do not carry over to
the task of parsing. When parsing with gold su-
pertags, the atomic embeddings outperform both
the RNN embeddings and the feature-based em-
beddings. With predicted supertags, the feature-
based embeddings do slightly overtake the atomic
embeddings, but the increase in performance is
quite small, while the RNN embeddings continue
to achieve lower scores than the atomic embed-
dings.5

The fact that the parser achieves state-of-the-
art performance even with the atomic embeddings
suggests that the parser may be able to induce
meaningful linguistic relationships from the data
alone. If this is the case, it could explain why the
linguistically informed embeddings do not make
much of a difference in parsing: If the parser can
learn the syntactic relationships between supertags
that are most relevant to parsing solely from the
data, then it does not require the additional linguis-
tic information that the RNN-based and feature-
based embeddings provide.

The fact remains that the linguistically informed

5To see if the two types of linguistically informed embed-
dings are complementary, we also trained a model combin-
ing both RNN embeddings and feature-based embeddings,
but there was no significant improvement over either of the
models with only one form of embedding.

embeddings do perform much better at the anal-
ogy task than the atomic embeddings. This may
be because the analogies contain some syntactic
constructions (such as relativization of an indi-
rect object) that are relatively uncommon. Since
these constructions are uncommon, it may be pos-
sible to attain high parsing performance without
being able to properly handle such constructions.
Thus, the atomic embeddings are able to yield high
parsing performance despite having poorer anal-
ogy task performance. It would be interesting to
parse only those sentences containing uncommon
syntactic constructions to see if the linguistically
informed embeddings outperform the atomic em-
beddings in such cases, since we would expect that
the greatest improvements in performance from
linguistically informed embeddings would come
with the parsing of infrequent supertags, since the
parser would not have much training data with
which to learn representations of these supertags.

6 Conclusions and Future Work

We presented two techniques for computing real-
valued vector representations of TAG supertags
and applied these vector representations to the
shift-reduce parsing system. We showed that
the vectors produced by all but the non-syntactic
GloVe embeddings performed reasonably well on
the syntactic analogy task. The two representa-
tions which were built on the basis of explicitly
represented linguistic structure, namely recursive
tree-based embedding and feature-based embed-
ding, on average produced the correct answer to
an analogy question as one of its top two guesses,
thereby outperforming both the distribution-based
embeddings trained using the GloVe method and
the atomic embeddings. On the parsing task, our
two linguistically-rich embedding methods per-

129



Dev Results Test Results
Gold Stags Predicted Stags Gold Stags Predicted Stags

Parsing Model Beam size UAS LAS UAS LAS UAS LAS UAS LAS
bi-LSTM Stagger + MICA Parser – 97.60 97.30 90.05 88.32 96.97 96.59 90.20 88.66
GloVe 1 93.01 91.97 88.35 86.46 – – – –
Atomic 1 96.82 96.45 89.48 88.00 – – – –
RNN 1 95.53 95.21 89.55 88.05 – – – –
Features 1 94.90 94.74 89.63 88.19 – – – –
GloVe 16 93.91 92.96 89.14 87.32 94.10 93.16 88.83 87.15
Atomic 16 97.67 97.45 90.23 88.77 97.87 97.64 90.25 88.90
RNN 16 96.39 96.10 90.30 88.81 96.73 96.46 90.24 88.85
Features 16 95.78 95.62 90.36 88.91 96.42 96.21 90.31 88.96

Table 2: Parsing results on the development and test sets. In each cell, shown is the mean of 5 trials with different initialization;
the standard deviation for these values ranges from 0.01 to 0.26.

formed comparably with the atomic embedding
method; the fact that the linguistically informed
embeddings did not significantly improve perfor-
mance compared to the atomic embeddings is an
interesting indication that the parser can learn
most relevant syntactic information purely from
its training data. In the future, we will explore
the application of these linguistically-rich embed-
dings in different systems of parsing such as the
graph-based parsing system that has recently been
successful in dependency grammar parsing.

Acknowledgments

We thank the anonymous reviewers for their help-
ful suggestions on this work.

References
Srinivas Bangalore, Pierre Boullier, Alexis Nasr, Owen

Rambow, and Benoı̂t Sagot. 2009. MICA: A Proba-
bilistic Dependency Parser Based on Tree Insertion
Grammars. In NAACL HLT 2009 (Short Papers).

Srinivas Bangalore and Aravind K. Joshi. 1999. Su-
pertagging: An Approach to Almost Parsing. Com-
putational Linguistics 25:237–266.

Tilman Becker. 2000. Patterns in metarules for TAG.
In Tree Adjoining Grammars, page 331342.

Danqi Chen and Christopher Manning. 2014. A
fast and accurate dependency parser using neural
networks. In Proceedings of the 2014 Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP). Association for Computa-
tional Linguistics, Doha, Qatar, pages 740–750.
http://www.aclweb.org/anthology/D14-1082.

John Chen. 2001. Towards efficient statistical parsing
using lexicalized grammatical information. Ph.D.
thesis, Ph. D. thesis, University of Delaware.

Wonchang Chung, Suhas Siddhesh Mhatre, Alexis
Nasr, Owen Rambow, and Srinivas Bangalore. 2016.
Revisiting supertagging and parsing: How to use su-
pertags in transition-based parsing. In Proceedings
of the 12th International Workshop on Tree Adjoin-
ing Grammars and Related Formalisms (TAG+12).
pages 85–92.

Roger Evans, Gerald Gazdar, and David Weir. 1995.
Encoding lexicalized tree adjoining grammars with
a nonmonotonic inheritance hierarchy. In Proceed-
ings of the 33rd Annual Meeting of the Association
for Computational Linguistics. pages 77–84.

Christoph Goller and Andreas Kuchler. 1996. Learning
task-dependent distributed representations by back-
propagation through structure. In IEEE Interna-
tional Conference on Neural Networks. volume 1,
pages 347–352.

Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: a corpus of CCG derivations and dependency
structures extracted from the penn treebank. Com-
putational Linguistics 33(3):355–396.

Jungo Kasai, Robert Frank, R. Thomas McCoy, Owen
Rambow, and Alexis Nasr. 2017. TAG Parsing with
Neural Networks and Vector Representations of Su-
pertags. In Proceedings of EMNLP.

Mike Lewis, Kenton Lee, and Luke Zettlemoyer. 2016.
Lstm ccg parsing. In Proceedings of the 2016 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies. Association for Computational
Linguistics, San Diego, California, pages 221–231.
http://www.aclweb.org/anthology/N16-1026.

Moshe Looks, Marcello Herreshoff, DeLesley
Hutchins, and Peter Norvig. 2017. Deep learning
with dynamic computation graphs. Proceedings of
the International Conference on Learning Represen-
tations https://openreview.net/pdf?id=ryrGawqex.

Brian McMahan and Matthew Stone. 2016. Syntac-
tic realization with data-driven neural tree gram-
mars. In Proceedings of COLING 2016, the 26th In-

130



ternational Conference on Computational Linguis-
tics: Technical Papers. The COLING 2016 Orga-
nizing Committee, Osaka, Japan, pages 224–235.
http://aclweb.org/anthology/C16-1022.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S
Corrado, and Jeff Dean. 2013a. Distributed
Representations of Words and Phrases and their
Compositionality. In C. J. C. Burges, L. Bottou,
M. Welling, Z. Ghahramani, and K. Q. Wein-
berger, editors, Advances in Neural Information
Processing Systems 26, Curran Associates, Inc.,
pages 3111–3119. http://papers.nips.cc/paper/5021-
distributed-representations-of-words-and-phrases-
and-their-compositionality.pdf.

Tomas Mikolov, Wen tau Yih, and Geoffrey Zweig.
2013b. Linguistic regularities in continuous space
word representations. In Proceedings of the 2013
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies (NAACL HLT 2013). At-
lanta, page 746751.

Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP). pages 1532–1543.

Richard Socher, Andrej Karpathy, Quoc V Le, Christo-
pher D Manning, and Andrew Y Ng. 2014.
Grounded compositional semantics for finding and
describing images with sentences. Transactions
of the Association for Computational Linguistics
2:207–218.

Richard Socher, Cliff C Lin, Chris Manning, and An-
drew Y Ng. 2011. Parsing natural scenes and nat-
ural language with recursive neural networks. In
Proceedings of the 28th International Conference on
Machine Learning (ICML-11). pages 129–136.

K Vijay-Shanker and Yves Schabes. 1992. Struc-
ture sharing in lexicalized tree-adjoining grammars.
In Proceedings of the 14th conference on Compu-
tational linguistics. Association for Computational
Linguistics, volume 1, pages 205–211.

XTAG Research Group. 1998. A lexicalized tree ad-
joining grammar for English. Technical report, De-
partment of Computer and Information Sciences,
University of Pennsylvania.

Wenduan Xu. 2016. LSTM shift-reduce CCG
parsing. In Proceedings of the 2016 Con-
ference on Empirical Methods in Natural Lan-
guage Processing. Association for Computational
Linguistics, Austin, Texas, pages 1754–1764.
https://aclweb.org/anthology/D16-1181.

131


