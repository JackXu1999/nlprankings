



















































Sense Contextualization in a Dependency-Based Compositional Distributional Model


Proceedings of the 2nd Workshop on Representation Learning for NLP, pages 1–9,
Vancouver, Canada, August 3, 2017. c©2017 Association for Computational Linguistics

Sense Contextualization in a Dependency-Based Compositional
Distributional Model

Pablo Gamallo
Centro Singular de Investigación en

Tecnoloxı́as da Información (CiTIUS)
Universidade de Santiago de Compostela, Galiza

pablo.gamallo@usc.es

Abstract

Little attention has been paid to distribu-
tional compositional methods which em-
ploy syntactically structured vector mod-
els. As word vectors belonging to differ-
ent syntactic categories have incompatible
syntactic distributions, no trivial compo-
sitional operation can be applied to com-
bine them into a new compositional vec-
tor. In this article, we generalize the
method described by Erk and Padó (2009)
by proposing a dependency-base frame-
work that contextualize not only lem-
mas but also selectional preferences. The
main contribution of the article is to ex-
pand their model to a fully compositional
framework in which syntactic dependen-
cies are put at the core of semantic com-
position. We claim that semantic compo-
sition is mainly driven by syntactic depen-
dencies. Each syntactic dependency gen-
erates two new compositional vectors rep-
resenting the contextualized sense of the
two related lemmas. The sequential ap-
plication of the compositional operations
associated to the dependencies results in
as many contextualized vectors as lemmas
the composite expression contains. At the
end of the semantic process, we do not
obtain a single compositional vector rep-
resenting the semantic denotation of the
whole composite expression, but one con-
textualized vector for each lemma of the
whole expression. Our method avoids the
troublesome high-order tensor representa-
tions by defining lemmas and selectional
restrictions as first-order tensors (i.e. stan-
dard vectors). A corpus-based experiment
is performed to both evaluate the quality

of the compositional vectors built with our
strategy, and to compare them to other ap-
proaches on distributional compositional
semantics. The experiments show that our
dependency-based compositional method
performs as (or even better than) the state-
of-the-art.

1 Introduction

Erk and Padó (2008) proposed a method in which
the combination of two words, a and b, returns
two vectors: a vector a’ representing the sense of
a given the selectional preferences imposed by b,
and a vector b’ standing for the sense of b given
the (inverse) selectional preferences imposed by a.
The main problem is that this approach does not
propose any compositional model for sentences.
Its objective is to simulate word sense disambigua-
tion, but not to model semantic composition at any
level of analysis. In Erk and Padó (2009), the au-
thors briefly describe an extension of their model
by proposing a recursive application of the com-
positional function. However, they only formalize
the recursive application when the composite ex-
pression consits of two dependent words linked to
the same head. So, they only explain how the head
is contextualized by its dependents, but not the
other way around. In addition, they do not model
the influence of context on the selectional prefer-
ences. In other terms, their recursive model does
not make use of contextualized selectional prefer-
ences.

In this article, we generalize the method de-
scribed in Erk and Padó (2009) by proposing
a dependency-base framework that contextualize
both lemmas and selectional preferences. The
main contribution of the article is to expand their
model to a fully compositional framework in
which syntactic dependencies are put at the core
of semantic composition.

1



In our model, lemmas and selectional prefer-
ences are defined as unary-tensors (standard vec-
tors), while syntactic dependencies are binary
functions combining vectors in an iterative and in-
cremental way.

For dealing with any sequence with N (lexical)
words and N − 1 dependencies linking them, the
compositional process can be applied N − 1 times
dependency-by-dependency in two different ways:
from left-to-right and from right-to-left. Figure 1
illustrates the incremental process of building the
sense of words dependency-by-dependency from
left-to-right. Given the composite expression “a b
c” and its dependency analysis depicted in the first
row of the figure, several compositional processes
are driven by the two dependencies involved in
the analysis (m and n). First, m is decomposed
into two functions: the head function m↑, and
the dependent one, m↓. The head function m↑
takes as input the sense of the head word b and
the selectional preferences of a, noted here as a◦,
and returns a new denotation of the head word,
bm↑, which represents the contextualized sense of
b given a at the m relation. Similarly, the depen-
dent function m↓ takes as input the sense of the
dependent word a and the selectional preferences
b◦, and returns a new denotation of the dependent
word: am↓. The green box is used to highlight the
result of each function. Next, the dependency n
between b and c is also decomposed into the head
and dependent functions: n↑ and n↓. Function
n↑ combines the already contextualized head bm↑
with the selectional preferences c◦, and returns a
still more specific sense of the head: bm↑+n↑. Fi-
nally, function n↓ takes as input the sense of the
dependent word c and the already contextualized
selectional preferences b◦m↓, and builds a contex-
tualized sense of the dependent word: cm↓+n↓. At
the end of the process, we have not obtained one
single sense for the whole expression “a b c”, but
one contextualized sense per word: am↓, bm↑+n↑,
and cm↓+n↓. Notice that the two words involved in
the direct object dependency, b and c, have been
contextualized twice since they inherit the restric-
tions of the subject dependency. The root word, b,
is directly involved in the two dependencies and,
then, is assigned an intermediate contextualized
sense, bm↑, in the first combination with a.

In the second case, from right-to-left, the se-
mantic process is applied in a similar way, but
starting from the rightmost dependency, n, and

ending by the leftmost one, m. At the end of the
process, three contextualized word senses are also
obtained which might be slightly different from
those obtained by the left-to-right algorithm. The
main difference is that a is now contextualized by
both b and c, while c is just contextualized by b.

The iterative application of the syntactic depen-
dencies found in a sentence is actually the pro-
cess of building the contextualized sense of all the
content words constituting that sentence. So, the
whole sentence is not assigned only one mean-
ing - which could be the contextualized sense of
the root word-, but one sense per word, being the
sense of the root just one of them, as in the work
described in Weir et al. (Weir et al., 2016). This
allows us to retrieve the contextualized sense of
all constituent words within a sentence. The con-
textualized sense of any word might be required
in further semantic processes, namely for dealing
with co-reference resolution involving anaphoric
pronouns. Such an elementary operation is pre-
vented if the sense of the phrase is just one com-
plex sense, as in most compositional approaches.

The rest of the article is organized as follows.
In Section 2, several distributional compositional
approaches are introduced and discussed. Next,
in Section 3, our dependency-based compositional
model is described. In Section 4, a corpus-based
experiment is performed to build and evaluate the
quality of compositional vectors. Finally, relevant
conclusions are addressed in Section 5.

2 Related Work

To take into account “the mode of combination”,
some distributional approaches follow a strategy
aligned with the formal semantics perspective in
which functional words are represented as high-
dimensional tensors (Coecke et al., 2010; Baroni
and Zamparelli, 2010; Grefenstette et al., 2011;
Krishnamurthy and Mitchell, 2013; Kartsaklis and
Sadrzadeh, 2013; Baroni, 2013; Baroni et al.,
2014). Using the abstract mathematical frame-
work of category theory, they provide the distribu-
tional models of meaning with the elegant mecha-
nism expressed by the principle of composition-
ality, where words interact with each other ac-
cording to their type-logical identities (Kartsak-
lis, 2014). The categorial-based approaches de-
fine arguments as vectors while functions taking
arguments (e.g., verbs or adjectives that combine
with nouns) are n-order tensors, with the number

2



a b c

m n

a bm↑

m↑(b, a◦)

am↓ b

m↓(b◦, a)

bm↑+n↑ c

n↑(bm↑, c◦)

bm↑ cm↓+n↓

n↓(b◦m↓, c)

Figure 1: Syntactic analysis of the expression “a b c” and left-to-right construction of the contextualized
word senses.

of arguments determining their order. Function
application is the general composition operation.
This is formalized as the tensor product, which is
nothing more than a generalization of matrix mul-
tiplication in higher dimensions. However, this
method results in an information scalability prob-
lem, since tensor representations grow exponen-
tially (Kartsaklis et al., 2014).

In our approach, by contrast, we operate with
only two types of semantic objects: first-order ten-
sors (or standard vectors) for lemmas and pref-
erences, and second-order functions for syntactic
dependencies. This solves the scalability problem
of high-order tensors. In addition, it also prevent
us giving different categorical representations to
verbs in different syntactic contexts. A verb is rep-
resented as a single vector which is contextualized
as it is combined with its arguments.

Some of the approaches cited above induce the
compositional meaning of the functional words
from examples adopting regression techniques
commonly used in machine learning (Baroni and
Zamparelli, 2010; Krishnamurthy and Mitchell,
2013; Baroni, 2013; Baroni et al., 2014). In our
approach, by contrast, functions associated with
dependencies are just basic arithmetic operations
on vectors, as in the case of the first arithmetic
approaches to composition (Mitchell and Lapata,
2008, 2009, 2010; Guevara, 2010; Zanzotto et al.,
2010). Arithmetic approaches are easy to imple-
ment and produce high-quality compositional vec-
tors, which makes them a good choice for practical
applications (Baroni et al., 2014).

However, given that our vector space is struc-
tured and enriched with syntactic information, the
vectors built by composition cannot be a sim-

ple mixture of the input vectors as in the bag-
of-words approaches (Mitchell and Lapata, 2008).
Our syntax-based vector representation of two re-
lated words encodes incompatible information and
there is no direct way of combining the informa-
tion encoded in their respective vectors. Vectors
of content words (nouns, verbs, adjectives, and ad-
verbs) live into different and incompatible spaces
because they are constituted by different types of
syntactic contexts. So, they cannot be merged.
To combine them, on the basis of previous work
(Thater et al., 2010; Erk and Padó, 2008; Melamud
et al., 2015), we distinguish between direct deno-
tation and selectional preferences within a depen-
dency relation. Our approach is an attempt to join
the main ideas of these syntax-based and struc-
tured vector space models into an entirely compo-
sitional model. More precisely, we generalize the
recursive model introduced by Erk and Pado (Erk
and Padó, 2009) with the addition of contextual-
ized selection preferences.

Finally, recent works make use of deep learn-
ing strategies to build compositional vectors, such
as recursive neural network models (Socher et al.,
2012; Hashimoto and Tsuruoka, 2015). Still in
the deep learning paradigm, special attention de-
serves a syntax-based compositional version of C-
BOW algorithm (Pham et al., 2015). Our method,
however, requires transparent and structured vec-
tor spaces to model compositionality.

3 The Method

In our approach, composition is modeled in terms
of recursive function application on word vectors
driven by binary dependencies. Each dependency
stands for two functions on vectors: the head func-

3



tion and the dependent one. Let us consider the
nominal subject syntactic dependency, which de-
notes two functions represented by the following
binary λ-expressions:

λxλy◦ nsubj↑(x, y◦) (1)
λx◦ λy nsubj↓(x◦, y) (2)

where nsubj↑ and nsubj↓ represent the head and
dependent functions, respectively; x, x◦, y, and
y◦ stand for vector variables. On the one hand,
x and y represent the denotation of the head and
dependent lemmas, respectively. They represent
standard context distributions. On the other hand,
x◦ represents the selectional preferences imposed
by the head, while y◦ stands for the selectional
preferences imposed by the dependent lemma. Se-
lectional preferences are also vectors and the way
we build them is described later.

Consider now the vectors of two specific lem-
mas, cat and chase, and their respective selec-
tional preferences at the subject position. Each
function application consists of multiplying the di-
rect vector associated with a lemma and the selec-
tional preferences imposed by the other lemma:

nsubj↑(chase, cat
◦) = chase� cat◦ = chasensubj↑

(3)

nsubj↓(chase
◦, cat) = cat� chase◦ = catnsubj↓ (4)

Each multiplicative operation results in a com-
positional vector which represents the contextual-
ized sense of one of the two lemmas (either the
head or the dependent). Component-wise multi-
plication has an intersective effect: the selectional
preferences restricts the direct vector by assigning
frequency 0 to those contexts that are not shared
by both vectors. Here, cat◦ and chase◦ are se-
lectional preferences resulting from the following
vector addition:

cat◦ =
∑

w∈ S↓(cat)
w (5)

chase◦ =
∑

w∈ S↑(chase)
w (6)

where S↓(cat) returns the vector set of those verbs
having cat as subject (except the verb chase).

More precisely, given the nominal subject posi-
tion, the new vector cat◦ is obtained by adding
the vectors {w|w ∈ S↓(cat)} of those verbs (eat,
jump, etc) that are combined with the noun cat in
that syntactic context. Component-wise addition
of vectors has an union effect. In more intuitive
terms, cat◦ stands for the inverse selectional pref-
erences imposed by cat on any verb at the sub-
ject position. As this new vector consists of ver-
bal contexts, it lives in the same vector space than
verbs and, therefore, it can be combined with the
direct vector of chase.

On the other hand, S↑(chase) in equation 6 rep-
resents the vector set of nouns occurring as sub-
jects of chase (except the noun cat). Given the
subject position, the vector chase◦ is obtained by
adding the vectors {w|w ∈ S↑(chase)} of those
nouns (e.g. dog, man, tiger, etc) that might be at
the subject position of the verb chase.

The incremental application of head and de-
pendent functions contextualize the representa-
tion of each word in the phrase. Incremental-
ity also model the influence of context on the
selectional preferences. The incremental left-to-
right interpretation of “the cat chased a mouse”
is illustrated in Figure 2 (without considering the
meaning of determiners nor verbal tense): First,
the head and dependent functions associated with
the subject dependency nsubj build the composi-
tional vectors chasensubj↑ and catnsubj↓. Then,
the head function associated with dobj produces a
more elaborate chasing event, chasensubj↑+dobj↑,
which stands for the final contextualized sense
of the root verb. In addition, the dependent
function of dobj yields a new nominal vector,
mousensubj↓+dobj↓, whose internal information
only can refer to a specific animal: “the mouse
chased by the cat”. Notice that contextualization
may disambiguate ambiguous words: in the con-
text of a chasing event, mouse does not refer to a
computer’s device. In fact, to interpret “the cat
chased a mouse”, it is required to interpret “cat
chased” as a fragment that restricts the type of
nouns that can appear at the direct object position:
mouse, rat, bird, etc. In the same way “police
chases” restricts the entities that can be chased by
police officers: thieves, robbers, and so on.

In our approach, not only the lemmas are
contextualized but also the selectional prefer-
ences. The contextualized selectional preferences,

4



cat chase mouse

nsubj dobj

cat chasensubj↑

nsubj↑(chase, cat◦)

catnsubj↓ chase

nsubj↓(chase◦, cat)

chasensubj↑+dobj↑ mouse

dobj↑(chasensubj↑,mouse◦)

chasensubj↑ mousensubj↓+dobj↓

dobj↓(chase◦nsubj↑,mouse)

nsubj↑(chase, cat◦) = chasensubj↑
nsubj↓(chase◦, cat) = catnsubj↓
dobj↑(chasensubj↑,mouse◦) = chasensubj↑+dobj↑
dobj↓(chase◦nsubj↑,mouse) = mousensubj↓+dobj↓

Figure 2: Syntactic analysis of the expression “the cat chased a mouse” and left-to-right construction of
the contextualized word senses.

chase◦nsubj↑, are obtained as follows:

chase◦nsubj↑ = catnsubj↓ �
∑

w∈ D↑(chase)
w

(7)
where D↑(chase) returns the vector set of those
nouns that are in the direct object role of chase
(except the noun mouse). The new vector result-
ing by this addition is combined by multiplication
(intersection) with the contextualized dependent
vector, catnsubj↓, to build the contextualized se-
lectional preferences. In more intuitive terms, the
selectional preferences built in equation 7 are con-
stituted by selecting the contexts of the nouns ap-
pearing as direct object of chase, which are also
part of cat after having been contextualized by the
verb at the subject position. This is the major con-
tribution with regard to the work described in Erk
and Padó (2009).

The dependency-by-dependency functional
application results in three contextualized word
senses: catnsubj↓, chasensubj↑+dobj↑ and
mousensubj↓+dobj↓. They all together represent
the meaning of the sentence in the left-to-right
direction.

In the opposite direction, from right-to-left, the
incremental process starts with the direct object
dependency:

dobj↑(chase,mouse◦) = chasedobj↑
dobj↓(chase◦,mouse) = mousedobj↓
nsubj↑(chasedobj↑, cat◦) = chasedobj↑+nsubj↑
nsubj↓(chase◦dobj↑, cat) = catdobj↓+nsubj↓

(8)

In Equation 8, the verb chase is first restricted
by mouse at the direct object position, and then
by its subject cat. In addition, this noun is re-
stricted by the vector chase◦dobj↓, which repre-
sents the contextualized selectional preferences
built by combining mousedobj↓ with the vectors
of the nouns that are in the subject position of
chase (except cat). This new compositional vec-
tor represents a very contextualized nominal con-
cept: “the cat that chased a mouse”. The word cat
and its specific sense can be related to anaphorical
expressions by making use of co-referential rela-
tionships at the discourse level: e.g., pronoun it,
other definite expressions (“that cat”, “the cat”),
and so on.

4 Experiments

We carried out a corpus-based experiment based
on compositional distributional similarity to check
the quality of composite expressions, namely
NOUN-VERB-NOUN constructions (NVN) incre-

5



mentally composed with nsubj and dobj depen-
dencies.

4.1 The Corpus and the Structured Vector
Model

Our working corpus consists of both the English
Wikipedia (dump file of November 20151) and the
British National Corpus (BNC)2. In total, the cor-
pus contains about 2.5 billion word tokens. We
used the rule-based dependency parser DepPattern
(Gamallo and González, 2011; Gamallo, 2015) to
perform syntactic analysis on the whole text.

Word vectors were built by computing their
co-occurrences in syntactic contexts. Two dif-
ferent types of vectors were built from the cor-
pus: nominal and verbal vectors. Then, for each
word we filtered out non relevant contexts using
simple count-based techniques inspired by those
described in (Bordag, 2008; Padró et al., 2014;
Gamallo, 2016), where matrices are stored in hash
tables with only non-zero values. More precisely,
the association between words and their contexts
were weighted with the Dunning’s likelihood ra-
tio (Dunning, 1993) and then, for each word, only
theN contexts with highest likelihood scores were
stored in the hash table (where N = 500). So, the
remaining contexts were removed from the hash
(in standard vector/matrix representations, instead
of removing contexts we should assign them zero
values).

The process of matrix reduction resulted in the
selection of 330, 953 nouns (most of them proper
names) with 236, 708 different nominal contexts;
and 6, 618 verbs with 140, 695 different verbal
contexts. As the contexts of nouns and verbs
are not compatible, we created two different vec-
tor spaces. Words and their contexts were stored
in two hashes, one per vector space, which rep-
resent matrices containing only non-zero values.
To build compositional vectors from these matri-
ces, the strategy defined in the previous section
was implemented in PERL giving rise to the soft-
ware Depfunc3. Distributional similarity between
pairs of composite expressions was performed us-
ing Cosine.

1https://dumps.wikimedia.org/enwiki/
2http://www.natcorp.ox.ac.uk
3Software and models are available at http://

gramatica.usc.es/˜gamallo/prototypes.htm

4.2 NVN Composite Expressions
This experiment consists of evaluating the qual-
ity of compositional vectors built by means of the
consecutive application of head and dependency
functions associated with nominal subject and di-
rect object. The experiment is performed on the
dataset developed by Grefenstette and Sadrzadeh
(2011a). The dataset was built using transi-
tive verbs paired with subjects and direct objects:
NVN composites.

Given our compositional strategy, we are able
to compositional build several vectors that some-
how represent the meaning of the whole NVN
composite expression. Take the expression “the
coach runs the team”. If we follow the left-to-
right strategy (noted nv-n), at the end of the com-
positional process, we obtain two fully contextu-
alized senses:

nv-n head The sense of the head run, as a result
of being contextualized first by the prefer-
ences imposed by the subject and then by the
preferences required by the direct object. We
note nv-n head the final sense of the head in
a NVN composite expression following the
left-to-right strategy.

nv-n dep The sense of the object team, as a re-
sult of being contextualized by the prefer-
ences imposed by run previously combined
with the subject coach. We note nv-n dep the
final sense of the direct object in a NVN com-
posite expression following the left-to-right
strategy.

If we follow the right-to-left strategy (noted n-
vn), at the end of the compositional process, we
obtain two fully contextualized senses:

n-nv head The sense of the head run as a result of
being contextualized first by the preferences
imposed by the object and then by the sub-
ject.

n-nv dep The sense of the subject coach, as a
result of being contextualized by the prefer-
ences imposed by run previously combined
with the object team.

Table 1 shows the Spearman’s correlation val-
ues (ρ) between individual human similarity
scores and the similarity values predicted by the
different versions built from our Depfunc system.
The best score was achieved by averaging the

6



Systems ρ
non-compositional (V) 0.27
Depfunc (nv head) 0.33
Depfunc (nv dep) 0.19
Depfunc (vn head) 0.36
Depfunc (vn dep) 0.38
Depfunc (nv-n head+dep) 0.35
Depfunc (nv-n head) 0.33
Depfunc (nv-n dep) 0.20
Depfunc (n-vn head+dep) 0.46
Depfunc (n-vn head) 0.36
Depfunc (n-vn dep) 0.42
Depfunc (n-vn+nv-n) 0.44
Grefenstette and Sadrzadeh (2011) 0.28
Hashimoto and Tsuruoka (2014) 0.43
Polajnar et al. (2015) 0.35

Table 1: Spearman correlation for transitive ex-
pressions using the benchmark by Grefenstette
and Sadrzadeh (2011)

head and dependent similarity values derived from
the n-vn (right-to-left) strategy. Let us note that,
for NVN composite expressions, the left-to-right
strategy seems to build less reliable compositional
vectors than the right-to-left counterpart. Besides,
the combination of the two strategies (n-vn+nv-n)
does not improve the results of the best one (n-
vn).4

The score value obtained by our n-vn head+dep
right-to-left strategy outperforms other systems
tested for this dataset: Grefenstette and Sadrzadeh
(2011b); Polajnar et al. (2015), which are two
works based on the categorical compositional dis-
tributional model of meaning of Coecke et al.
(2010), and the neural network strategy described
in Hashimoto and Tsuruoka (2015).

At the top of Table 1, we show the non-
compositional baseline we have created for this
dataset: similarity beteween single verbs. The
table also shows four intermediate values result-
ing from comparing partial compositional con-
structions: the noun-verb (nv head and nv dep)
and the verb-noun (vn head and vn dep) combina-
tions. Two interesting remarks can be made from
these values when they are compared with the full
compositional constructions.

First, there is no clear improvement of perfor-
mance if we compare the full compositional infor-
mation of the two transitive constructions with the
partial combinations. On the one hand, the full
nv-n construction does not improve the scores ob-
tained by the partial intransitive nv. On the other

4n-vn+nv-n is computed by averaging the similarities of
both n-vn head+dep and nv-n head+dep

hand, n-vn performs slightly better than vn but
only in the case of the dependent function which
makes use of contextualized selectional prefer-
ences: n-vn dep = 0.42 / vn dep = 0.38. The low
performance at the second level of composition
might call into question the use of contextualized
vectors to build still more contextualized senses.
The scarcity problem derived from the recursive
combination of contextualized vectors is an impor-
tant issue which could be faced with more corpus,
and which we should analyze with more complex
evaluation tests.

The second remark is about the difference be-
tween the two algorithms: left-to-righ and right-
to-left. The scores achieved by the left-to-
right algorithm (nv, nv-n) are clearly below those
achieved by right-to-left (vn, n-vn) . This might be
due to the weak semantic motivation of the selec-
tional preferences involved in the subject depen-
dency of transitive constructions in comparison to
the direct object one. In fact, right-to-left and left-
to-right function application produces quite differ-
ent vectors because each algorithm corresponds
to a particular hierarchy of constituents. Change
of constituency implies different semantic entail-
ments such as we can easily observe if we consider
the different levels of constituency of noun mod-
ifiers (e.g. “fastest American runner” 6= “Amer-
ican fastest runner”). Finally, the poor results of
nv in this dataset might be explained because the
subject role is less meaningful in transitive clauses
than in intransitive ones. The subject of intransi-
tive clauses is assigned a complex semantic role
that tends to merge the notions of agent and pa-
tient. By contrast, the subject of transitive con-
structions tends to be just the agent of an action
with an external patient.

5 Conclusions

In this paper, we described a distributional compo-
sitional model based on a transparent and syntacti-
cally structured vector space. The combination of
two related lemmas gives rise to two vectors which
represent the senses of the two contextualized lem-
mas. This process can be repeated until no syntac-
tic dependency is found in the analyzed composite
expression. The compositional interpretation of a
composite expression builds the sense of each con-
stituent lemma in an incremental way.

Substantial problems still remain unsolved. For
instance there is no clear borderline between

7



compositional and non-compositional expressions
(collocations, compounds, or idioms). It seems to
be obvious that vectors of full compositional units
should be built by means of compositional oper-
ations and predictions based on their constituent
vectors. It is also evident that vectors of entirely
frozen expressions should be totally derived from
corpus co-occurrences of the whole expressions
without considering internal constituency. How-
ever, there are many expressions, in particular col-
locations (such as “save time”, “go mad”, “heavy
rain”, . . . ) which can be considered as both com-
positional and non-compositional. In those cases,
it is not clear which is the best method to build
their distributional representation: predicted vec-
tors by compositionality or corpus-observed vec-
tors of the whole expression?

Another problem that has not been considered
is how to represent the semantics of some gram-
matical words, namely determiners and auxiliary
verbs (i.e., noun and verb specifiers). For this pur-
pose, we think that it would be required a different
functional approach, probably closer to the work
described by Baroni (2014), who defines functions
as linear transformations on vector spaces.

Finally, as we have outlined above, generated
vectors tend to be too scarce when they are derived
from the recursive combination of already contex-
tualized vectors. Further experiments with more
complex phrases and larger training corpora are
required in order to deeply analyse this issue. For
this purpose, we will explore the strategy defined
in Kober et al. (2016) to improve sparse distribu-
tional representations.

In current work, we are defining richer semantic
word models by combining WordNet features with
semantic spaces based on distributional contexts
(Gamallo and Pereira-Fariña, 2017). This hybrid
method might also help overcome scarcity.

Acknowledgments

This work has received financial support from a
2016 BBVA Foundation Grant for Researchers
and Cultural Creators, TelePares (MINECO,
ref:FFI2014-51978-C2-1-R), the Consellerı́a de
Cultura, Educación e Ordenación Universitaria
(accreditation 2016-2019, ED431G/08) and the
European Regional Development Fund (ERDF).

References
Marco Baroni. 2013. Composition in distributional se-

mantics. Language and Linguistics Compass 7:511–
522.

Marco Baroni, Raffaella Bernardi, and Roberto Zam-
parelli. 2014. Frege in space: A program for compo-
sitional distributional semantics. LiLT 9:241–346.

Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing. Strouds-
burg, PA, USA, EMNLP’10, pages 1183–1193.

Stefan Bordag. 2008. A Comparison of Co-occurrence
and Similarity Measures as Simulations of Context.
In 9th CICLing. pages 52–63.

B. Coecke, M. Sadrzadeh, and S. Clark. 2010. Math-
ematical foundations for a compositional distribu-
tional model of meaning. Linguistic Analysis 36(1-
4):345–384.

Ted Dunning. 1993. Accurate methods for the statistics
of surprise and coincidence. Computational Lin-
guistics 19(1):61–74.

Katrin Erk and Sebastian Padó. 2008. A structured vec-
tor space model for word meaning in context. In
Proceedings of EMNLP. Honolulu, HI.

Katrin Erk and Sebastian Padó. 2009. Paraphrase as-
sessment in structured vector space: Exploring pa-
rameters and datasets. In Proceedings of the EACL
Workshop on Geometrical Methods for Natural Lan-
guage Semantics. Athens, Greece.

Pablo Gamallo. 2015. Dependency parsing with com-
pression rules. In International Workshop on Pars-
ing Technology (IWPT 2015). Bilbao, Spain.

Pablo Gamallo. 2016. Comparing explicit and predic-
tive distributional semantic models endowed with
syntactic contexts. Language Resources and Eval-
uation First online: 13 May 2016.

Pablo Gamallo and Isaac González. 2011. A grammat-
ical formalism based on patterns of part-of-speech
tags. International Journal of Corpus Linguistics
16(1):45–71.

Pablo Gamallo and Martı́n Pereira-Fariña. 2017. Com-
positional semantics using feature-based models
from wordnet. In 1st Workshop on Sense, Con-
cept and Entity Representations and their Applica-
tions, co-located at 15th Conference of the European
Chapter of the Association for Computational Lin-
guistics. Association of Computational Linguistics
(ACL), pages 1–10.

Edward Grefenstette and Mehrnoosh Sadrzadeh.
2011a. Experimental support for a categorical com-
positional distributional model of meaning. In Con-
ference on Empirical Methods in Natural Language
Processing.

8



Edward Grefenstette and Mehrnoosh Sadrzadeh.
2011b. Experimenting with transitive verbs in a dis-
cocat. In Workshop on Geometrical Models of Nat-
ural Language Semantics (EMNLP-2011).

Edward Grefenstette, Mehrnoosh Sadrzadeh, Stephen
Clark, Bob Coecke, and Stephen Pulman. 2011.
Concrete sentence spaces for compositional distri-
butional models of meaning. In Proceedings of the
Ninth International Conference on Computational
Semantics. IWCS ’11, pages 125–134.

Emiliano Guevara. 2010. A regression model of
adjective-noun compositionality in distributional se-
mantics. In Proceedings of the 2010 Workshop on
GEometrical Models of Natural Language Seman-
tics. GEMS ’10.

Kazuma Hashimoto and Yoshimasa Tsuruoka.
2015. Learning embeddings for transitive verb
disambiguation by implicit tensor factoriza-
tion. In Proceedings of the 3rd Workshop on
Continuous Vector Space Models and their
Compositionality. Association for Computa-
tional Linguistics, Beijing, China, pages 1–11.
http://www.aclweb.org/anthology/W15-4001.

Dimitri Kartsaklis. 2014. Compositional operators in
distributional semantics. Springer Science Reviews
2(1-2):161–177.

Dimitri Kartsaklis, Nal Kalchbrenner, and Mehrnoosh
Sadrzadeh. 2014. Resolving lexical ambiguity in
tensor regression models of meaning. In Proceed-
ings of the 52nd Annual Meeting of the Associa-
tion for Computational Linguistics (Vol. 2: Short
Papers). Association for Computational Linguistics,
Baltimore, USA, pages 212–217.

Dimitri Kartsaklis and Mehrnoosh Sadrzadeh. 2013.
Prior disambiguation of word tensors for construct-
ing sentence vectors. In Conference on Empirical
Methods in Natural Language Processing (EMNLP
2013).

Thomas Kober, Julie Weeds, Jeremy Reffin, and
David J. Weir. 2016. Improving sparse word rep-
resentations with distributional inference for se-
mantic composition. In Proceedings of EMNLP
2016, Austin, Texas, USA. pages 1691–1702.
http://aclweb.org/anthology/D/D16/D16-1175.pdf.

Jayant Krishnamurthy and Tom Mitchell. 2013. Pro-
ceedings of the Workshop on Continuous Vector
Space Models and their Compositionality, Associ-
ation for Computational Linguistics, chapter Vector
Space Semantic Parsing: A Framework for Compo-
sitional Vector Space Models, pages 1–10.

Oren Melamud, Ido Dagan, and Jacob Gold-
berger. 2015. Modeling word meaning in con-
text with substitute vectors. In NAACL HLT
2015, Denver, Colorado, USA. pages 472–482.
http://aclweb.org/anthology/N/N15/N15-1050.pdf.

Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL-08: HLT . pages 236–244.

Jeff Mitchell and Mirella Lapata. 2009. Language
models based on semantic composition. In Proceed-
ings of EMNLP. pages 430–439.

Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence 34(8):1388–1439.

Muntsa Padró, Marco Idiart, Aline Villavicencio, and
Carlos Ramisch. 2014. Nothing like good old fre-
quency: Studying context filters for distributional
thesauri. In Proceedings of EMNLP 2014, Doha,
Qatar. pages 419–424.

Nghia The Pham, Germán Kruszewski, Angeliki
Lazaridou, and Marco Baroni. 2015. Jointly op-
timizing word representations for lexical and sen-
tential tasks with the C-PHRASE model. In Pro-
ceedings of the 53rd Annual Meeting of the Associ-
ation for Computational Linguistics and the 7th In-
ternational Joint Conference on Natural Language
Processing of the Asian Federation of Natural Lan-
guage Processing, ACL 2015, July 26-31, 2015, Bei-
jing, China, Volume 1: Long Papers. pages 971–981.
http://aclweb.org/anthology/P/P15/P15-1094.pdf.

Tamara Polajnar, Laura Rimell, and Stephen Clark.
2015. An exploration of discourse-based sentence
spaces for compositional distributional semantics.
In Proceedings of the First Workshop on Linking
Computational Models of Lexical, Sentential and
Discourse-level Semantics. Association for Compu-
tational Linguistics, Lisbon, Portugal, pages 1–11.
http://aclweb.org/anthology/W15-2701.

Richard Socher, Brody Huval, Christopher D.
Manning, and Andrew Y. Ng. 2012. Seman-
tic compositionality through recursive matrix-
vector spaces. In Proceedings of the EMNLP-
CoNLL’12. Association for Computational Lin-
guistics, Stroudsburg, PA, USA, pages 1201–1211.
http://dl.acm.org/citation.cfm?id=2390948.2391084.

Stefan Thater, Hagen Fürstenau, and Manfred Pinkal.
2010. Contextualizing semantic representations us-
ing syntactically enriched vector models. In Pro-
ceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics. Stroudsburg,
PA, USA, pages 948–957.

David J. Weir, Julie Weeds, Jeremy Reffin, and Thomas
Kober. 2016. Aligning packed dependency trees: A
theory of composition for distributional semantics.
Computational Linguistics 42(4):727–761.

Fabio Massimo Zanzotto, Ioannis Korkontzelos,
Francesca Fallucchi, and Suresh Manandhar. 2010.
Estimating linear models for compositional distribu-
tional semantics. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics.
COLING ’10, pages 1263–1271.

9


