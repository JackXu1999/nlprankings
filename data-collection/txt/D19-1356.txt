



















































Leveraging Adjective-Noun Phrasing Knowledge for Comparison Relation Prediction in Text-to-SQL


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 3515–3520,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

3515

Leveraging Adjective-Noun Phrasing Knowledge
for Comparison Relation Prediction in Text-to-SQL

Haoyan Liu1∗, Lei Fang2, Qian Liu3, Bei Chen2, Jian-Guang Lou2, Zhoujun Li1
1State Key Lab of Software Development Environment, Beihang University, China

2Microsoft Research, Beijing, China
3State Key Lab of Virtual Reality Technology and Systems, Beihang University, China
{haoyan.liu, qian.liu, lizj}@buaa.edu.cn; {leifa, beichen, jlou}@microsoft.com

Abstract

One key component in text-to-SQL is to pre-
dict the comparison relations between column-
s and their values. To the best of our knowl-
edge, no existing models explicitly introduce
external common knowledge to address this
problem, thus their capabilities of predicting
comparison relations are limited beyond train-
ing data. In this paper, we propose to lever-
age adjective-noun phrasing knowledge mined
from the web to predict the comparison rela-
tions in text-to-SQL. Experimental results on
both the original and the re-split Spider dataset
show that our approach achieves significant
improvement over state-of-the-art methods on
comparison relation prediction.

1 Introduction

Text-to-SQL (Yaghmazadeh et al., 2017; Zhong
et al., 2017), which aims at mapping natural lan-
guage to SQL queries, is one of the most important
tasks in natural language processing. Most state-
of-the-art models are end-to-end neural network
based models (Zhang et al., 2017; Xu et al., 2017;
Yu et al., 2018a; Herzig and Berant, 2018; Dong
and Lapata, 2018; Yu et al., 2018b), which mainly
extend the Seq2Seq architecture with some com-
plicated network structures. As shown by Yu et al.
(2018b), the performances of most methods are
overstated, because they just match semantic pars-
ing results, rather than truly understand the mean-
ings of inputs (Finegan-Dollak et al., 2018). In this
paper, we study the comparison relation predic-
tion problem, as the comparison relations between
columns and their values are not well understood
by existing methods.

In SQL queries, comparison relations are ex-
pressed by the comparison operators (=, 6=, <,≤
, >,≥) and value ordering keywords (ASC, DESC

∗This work was done when the first author was an intern
at Microsoft Research Asia.

in ORDER BY expression). In most text-to-SQL
models, the comparison relations are either gen-
erated using Seq2Seq architectures (Zhong et al.,
2017; Dong and Lapata, 2018), or predicted us-
ing classifiers trained with output decoding fea-
tures (Xu et al., 2017; Yu et al., 2018b). We give
an example to show that external common knowl-
edge is indispensable to truly understand the com-
parison relations on unseen data.

rank name time birth date
1 Martina 11′9 19800930
2 Mirjana 12′5 19820309
3 Justine 12′9 19820601

Table 1: Player Basic Information.

Table 1 shows the basic information about the
athletes of the 100-meter sprint. Given the query
“what is the name of the oldest player ?”, the
goal is to generate the SQL “SELECT name
FROM players ORDER BY birth date
ASC LIMIT 1”. It should be noted that ASC
represents the common knowledge that small
value in birth date column means “old”.
Supposing that there are columns named age
and some queries containing “old” in the training
data, it’s easy for the trained models to remember
that “old” means selecting a large value from
column age. But if birth date is unseen in
the training data, there is little chance to predict
the comparison relation correctly without the
common knowledge that “age” and “birth date”
both represent age but have opposite value polari-
ties. Similarly, for query “fast runner”, models
should select a large value from column speed
or a small value from column time. There are
some related works that address the intensity of
adjectives (De Melo and Bansal, 2013; Ruppen-
hofer et al., 2014; Sharma et al., 2017), however,



3516

no existing work studies the relations between the
value polarities of adjective-noun phrasing pairs.

In this paper, we propose to explicitly incorpo-
rate the column value polarities as external knowl-
edge in text-to-SQL models. Our goal is to scale
the capabilities of existing models on comparison
relation prediction to unseen data. The colum-
n value polarities are named as “adjective-noun
phrasing knowledge”, which can be easily con-
structed from the web. We further formulate the
phrasing knowledge as feature vectors, which can
be generically fed to existing models. Experimen-
tal results on both the original and the re-split Spi-
der dataset show that our approach achieves signif-
icant improvement over state-of-the-art methods
on comparison relation prediction.

2 Adjective-Noun Phrasing Knowledge

There are 3 steps to construct the knowledge: 1)
adjective-noun pair candidate extraction, 2) value
polarity mining, and 3) adjective-noun pair clus-
tering. The adjective-noun phrasing knowledge
will be two clusters, each of which is a list of
adjective-noun pairs with the same value polarity.

2.1 Adjective-Noun Pair Candidate
Extraction

As the value polarities depend on both adjectives
and nouns, we first extract adjective-noun pairs
that could be candidates of the phrasing knowl-
edge. Typically, table column names could be
considered as hypernyms of the corresponding cell
values. Motivated by this, we gather a list of noun
candidates, which consists of the concepts (hy-
pernyms) from Microsoft Concept Graph1 (Cheng
et al., 2015; Wu et al., 2012) and the column
names whose value types are number or date in
the Web Table Corpora2 (Lehmberg et al., 2016).

To extract the adjectives that modify the noun
candidates, we introduce two POS tag patterns:
(1) [ADJ] [NOUN]

(2) [NOUN] is|are|was|were|be [ADJ]

where [ADJ] and [NOUN] are the POS tags
for adjectives and nouns, respectively. We apply
these two patterns to Wikipedia dump3 to extrac-
t adjective-noun pair candidates. Taking price as
an example, we would obtain the adjectives: high,
expensive, and cheap, etc.

1https://concept.research.microsoft.
com

2http://webdatacommons.org/webtables/
3https://dumps.wikimedia.org/enwiki/

2.2 Value Polarity Mining
As shown in Table 1, the value polarities of age
and birth date are opposite though both words
are about “age”. We propose to mine the value po-
larities from the Web Table Corpora automatically.
We assume that for two columns in the same table,
if their corresponding values have negative corre-
lations, they have opposite value polarities4. We
use the Spearman’s ρ coefficients to measure the
correlations between two columns. For each two
columns with value type number or date, if the fre-
quency of their co-occurrences is above 20 and the
coefficients ρ are below −0.9 for more than 50%
co-occurrences in the corpora, the two columns
are determined to have opposite value polarities.

2.3 Adjective-Noun Pair Clustering
So far, we have obtained the adjective-noun pairs
and value polarity relations for nouns. However,
it is still unclear whether the polarity is positive or
negative. Positive (negative) polarity means that
large (small) column values shall be selected when
the noun is modified by an adjective. For example,
the polarities of <age, old> and <price, ex-
pensive> are positive, while <age, young> and
<price, cheap> are negative. Our goal is to
separate the adjective-noun pairs into two clusters
based on the value polarities.

Supposing that we know the polarity of <age,
old> is positive, we can derive that the polari-
ties of <age, young> and <birth date, old>
are negative, because “old” and “young” are
antonyms, and the value polarities of age and
birth date are opposite. Motivated by this,
we extend and group the adjective-noun pairs into
clusters based on the following rules:

• the polarities of two pairs are the same if the
adjectives are synonyms and the nouns are
the same, or the nouns are synonyms and the
adjectives are the same;

• the polarities of two pairs are the opposite if
the adjectives are antonyms and the nouns are
the same, or the nouns have opposite value
polarities and the adjectives are the same.

It should be noted that each cluster contains t-
wo sets of pairs with opposite polarities. We man-
ually assign polarity labels to clusters in top sizes

4We also studied the assumption that columns with posi-
tive correlations have the same value polarities, but find that
it introduces much noise.

https://concept.research.microsoft.com
https://concept.research.microsoft.com
http://webdatacommons.org/webtables/
https://dumps.wikimedia.org/enwiki/


3517

or with high total frequencies. One potential is-
sue is that there might be conflicts due to the syn-
onyms and antonyms resources, i.e., there may be
two pairs assigned opposite polarities in the same
cluster. In practice, there are only a few such cas-
es, and we separate them into individual cluster-
s and manually label these pairs. After that, we
obtain the adjective-noun phrasing knowledge or-
ganized in two clusters, each of which is a list of
adjective-noun pairs with the same polarity.

3 Phrasing Knowledge As Features

We propose to formulate knowledge as features
to incorporate the adjective-noun phrasing knowl-
edge. It is very generic, because knowledge fea-
ture can be easily combined with the input or hid-
den output of existing neural models.

3.1 Baseline Models
We use Spider (Yu et al., 2018c) as the dataset. We
do not use other datasets like WikiSQL (Zhong
et al., 2017) because WikiSQL queries that con-
tain comparison relations are very simple, and do
not require additional knowledge for understand-
ing. We introduce the knowledge feature to t-
wo baseline models, namely syntaxSQLNet5 (Yu
et al., 2018b) and SQLNet6 (Xu et al., 2017; Yu
et al., 2018c). We do not use other syntax tree-
based or sequence-based baselines like coarse-to-
fine (Dong and Lapata, 2018) because they cannot
handle the Spider dataset.

Currently7, syntaxSQLNet achieves the best
performance on Spider. SyntaxSQLNet and SQL-
Net solve the text-to-SQL task using a sequence-
to-set structure. They decompose the SQL gen-
eration procedure into multiple individual mod-
ules. The comparison relation prediction con-
sists of two parts: the comparison operator pre-
diction in OP module of WHERE and GROUP
Having components and ordering prediction in
DESC/ASC/LIMIT(DAL) module of ORDER
BY component. Both the two modules are clas-
sifiers defined as:

p(y) = F(V tanh(WX)), (1)

whereX represents the input features; y is the out-
put label; and W and V are trainable parameter-
s. Function F is defined as sigmoid in SQLNet

5https://github.com/taoyds/syntaxSQL
6https://github.com/taoyds/spider/

tree/master/baselines/sqlnet
7As of mid May when we prepare this submission.

and softmax in syntaxSQLNet. The OP module
predicts the comparison operator y ∈ {=, >, <, >=,
<=, !=, LIKE, NOTIN, IN, BETWEEN}, while
the DAL module predicts the ordering keyword
y ∈ {DESC, ASC, DESC LIMIT, ASC LIMIT}.

3.2 Knowledge as Features
The adjective-noun phrasing knowledge is formu-
lated as additional input features to existing mod-
els. Let XK denote the knowledge feature, we re-
write the classifier for OP and DAL module as:

p(y) = F(V tanh(W [X : XK ])), (2)

where [:] denotes the concatenation of feature vec-
tors. The intuition is that even if X is unseen, the
knowledge feature XK will help to make the cor-
rect prediction. We will give an example to show
how to construct the knowledge feature.

As the adjective-noun phrasing knowledge con-
sists of two clusters, we use two fixed random vec-
tors to represent the two clusters, denoted as po-
larity features. We define the knowledge feature
as the attention weighted polarity features with
knowledge masks.

0.05 0.01 0.02 0.80 0.10 0.02

Id

Name

Age

Gender

Who is the oldest singer ?

0 0 0 1 0 0
Mask

Question

C
o
lu
m
n
s

0.00×

●

0 0 0 0 0 0

●

0.80 ×Knowledge
Feature

POS NEG

POS

NEG

Figure 1: Knowledge as feature.

In Figure 1, the up palette shows the attention
matrix between column names and question to-
kens. In syntaxSQLNet and SQLNet, the attention
between column names and question tokens is de-
fined by:

αQ/CS = softmax(HQWH
>
CS), (3)

where HCS and HQ represent the hidden state
of LSTM for column names and question token-
s, respectively. Suppose that we are predicting
the comparison operator for column age using the
DAL module. We first construct the positive and

https://github.com/taoyds/syntaxSQL
https://github.com/taoyds/spider/tree/master/baselines/sqlnet
https://github.com/taoyds/spider/tree/master/baselines/sqlnet


3518

negative knowledge mask vectors, whose length-
s are equal to the number of question tokens. In
mask vectors, 1 means that the pair of the column
name and the corresponding question token exist-
s in phrasing knowledge. Then, we calculate the
weight of positive and negative polarity features
using the inner product (�) of knowledge mask
vectors and the attention vector. For example, in
Figure 1, the weight of positive and negative polar-
ity features are 0.8 and 0, respectively. After that,
we obtain the knowledge feature by the element-
wise sum (⊕) of the weighted positive and nega-
tive polarity features, denoted by +weighted.

To obtain the mask vectors, we need to match
the nouns in phrasing pairs with column names in
the Spider dataset. For example, column names
birth date, birthdate, birth date and
date of birth shoud be matched with the
same noun “date of birth” in the phrasing knowl-
edge. We heuristically define the matching score
for column name n1 and noun n2 as:

Score(n1, n2) =0.2Rt(n1, n2) + 0.2Rc(n1, n2)

+0.6 cos(
∑

t1∈n1

idft1vt1 ,
∑

t2∈n2

idft2vt2), (4)

whereRt andRc are the token and char level fuzzy
matching ratios, which is calculated by the output
of fuzzywuzzy8 divided by 100; and the last part
is the cosine similarity between the inverse docu-
ment frequency (idf) weighted sum of token em-
beddings, where the “document” refers to one col-
umn name in dataset. Given a column name in OP
or DAL module, we select phrasing pairs whose
noun matching score is ranked in top 10 as the
knowledge to obtain the knowledge mask vectors.

Another straightforward way is to define the po-
larity feature as the knowledge feature directly, de-
noted as +direct, where the polarity is determined
by the attention weighted knowledge mask score.
In the example above, the positive polarity feature
will be considered as knowledge feature since the
positive mask has a higher attention weighted s-
core (0.8 > 0). We will evaluate both the +weighted
and +direct knowledge features in the experiments.

4 Experiments

4.1 Data & Settings
For quality assurance, we manually choose 91 ad-
jectives that have value polarities from top 200 fre-
quent adjectives in Wikipedia. The adjectives with

8https://github.com/seatgeek/
fuzzywuzzy

no value polarities like “important”, “happy” and
“reasonable” are filtered out. Then we use the ad-
jective synonyms crawled from Bing Dictionary9,
the noun synonyms and adjective antonyms from
WordNet (Miller, 1995) to cluster the adjective-
noun pairs. We obtain 4, 133 distinct phrasing
pairs in 2, 689 clusters. After that, we manually la-
bel the clusters with total sum of pair frequencies
in top 50 or total number of pairs in top 50. Fi-
nally, we obtain 970 distinct adjective-noun pairs,
which cover about 79.65% usages among all pairs.
We manually evaluate the value polarities of all the
970 pairs, and the accuracy is 87.8%. The pairs af-
ter manually revision are served as the adjective-
noun phrasing knowledge.

For Spider dataset, we remove the nested SQL
queries and queries containing JOIN, because
the performance of baseline models on complex
queries is rather low, and we would more like to
see the result that whether the comparison rela-
tion is correctly predicted given the right colum-
n. There are 4, 490 questions after removing the
complex queries, with 3, 946 training and 544 de-
velopment data. We manually review all the 4, 490
questions and the corresponding tables, and find
that there are 1, 212 questions containing compar-
ison relations (excluding =, 6=), among which 668
questions require adjective phrasing knowledge.
The total number of required distinct adjective-
noun pairs is 728, which is considered as ground-
truth knowledge (denoted as gt). After match-
ing the mined adjective phrasing knowledge (970
pairs) with the Spider column names using Equa-
tion 4, we obtain 631 distinct pairs where 598 pairs
exist in the ground-truth knowledge, the precision
of knowledge matching using Equation 4 is 94.8%
(598/631), and the recall is 82.1% (598/728).

To further evaluate effectiveness of phrasing
knowledge, we re-split the training and develop-
ment data of Spider, to ensure that the knowledge
applied in the development set has not been seen or
applied in the training set. The sizes of the re-split
training and development sets are 3, 906 and 584,
respectively. For model training, the dimension of
knowledge feature is 50 with other parameters re-
maining the same as baseline models.

4.2 Phrasing Knowledge Examples

In the step of value polarity mining (described in
Section 2.2), we obtain 758 noun pairs with oppo-

9https://www.bing.com/dict

https://github.com/seatgeek/fuzzywuzzy
https://github.com/seatgeek/fuzzywuzzy
https://www.bing.com/dict


3519

Select -AGG Where -OP Group -Having Order -DAL Overall

Original

SQLNet 62.7 64.9 40.7 45.0 58.1 73.1 62.4 66.9 39.9
+weighted 63.2 63.9 40.8 44.6 48.1 59.2 61.1 67.2 40.4
+direct 65.0 65.9 36.4 37.9 60.0 65.5 64.4 68.9 42.8
+weighted gt 65.7 67.0 40.8 42.7 58.9 68.4 64.4 68.9 43.0
+direct gt 65.8 67.3 40.3 44.8 54.2 69.3 65.9 70.5 42.1

syntaxSQL 62.1 64.0 30.1 38.8 66.9 69.4 46.3 54.8 39.5
+weighted 65.0 67.6 33.3 40.3 59.0 63.1 49.1 58.7 41.0
+direct 61.8 64.6 39.5 46.7 66.4 68.9 48.7 57.7 40.1
+weighted gt 65.9 68.1 37.6 47.2 60.6 63.1 53.5 58.9 42.5
+direct gt 63.5 64.8 30.4 39.0 64.4 69.5 50.6 52.9 41.2

Re-Split

SQLNet 64.4 65.7 38.4 42.4 52.1 64.5 60.9 68.3 40.8
+weighted 63.2 63.9 40.8 44.6 48.1 59.2 61.1 67.2 40.4
+direct 65.0 65.9 36.4 37.9 60.0 65.5 64.4 68.9 42.8
+weighted gt 65.2 66.7 37.4 42.3 54.2 69.2 63.2 73.1 39.0
+direct gt 62.8 64.4 40.8 44.5 60.0 72.2 60.7 70.0 40.1

syntaxSQL 66.0 69.0 30.0 41.2 64.8 68.0 50.5 60.4 39.7
+weighted 61.6 63.5 36.6 41.1 56.9 62.2 60.2 71.5 38.2
+direct 65.7 67.1 37.9 41.6 59.5 69.8 62.5 71.3 40.8
+weighted gt 67.5 69.2 33.2 45.0 67.7 69.3 48.0 58.6 41.6
+direct gt 64.1 66.2 38.5 49.9 66.7 68.2 52.7 60.5 41.4

Table 2: F1-scores on the original and re-split Spider dataset. gt means introducing the ground-truth knowledge.
The bold numbers highlight the best results when introducing the matched mined phrasing knowledge outperforms
the baselines, while the underlined numbers highlight that when introducing the ground truth knowledge.

Positive Negative
age old, eld, high young, low

birth date young old, eld

date
fresh, new,

old, early
recent, late

score high low
rank low high

price
costly, expensive, inexpensive

high, pricy cheap, low

Table 3: Examples of value polarity.

site polarities. For example, the opposite-polarity
words for age are {birth year, date of birth, birth-
day}, and for rank are {point, vote, score}.

Table 3 shows some examples of the adjective-
noun phrasing knowledge. It can be seen that the
value polarities are opposite for age and birth
date, score and rank.

4.3 Results on Comparison Relation
Prediction

Table 2 shows the F1-scores on the original and
re-split Spider dataset. -OP, -Having, and -DAL
represent WHERE component without OP module,

Group component without Having module, and
ORDER BY component without DAL module, re-
spectively. The results show that both the +weighted
and the +direct knowledge features are effective,
they significantly improve the performances of
components that have comparison relations. The
results also show that the overall performances are
improved, which demonstrates that adjective-noun
phrasing knowledge is effective and generic for
existing state-of-the-art models.

5 Conclusion & Future Work

In this paper, we introduce the adjective-noun
phrasing knowledge as feature to improve com-
parison relation prediction on unseen data. Exper-
imental results show that our apporach achieves
promising performance. For future work, we will
further improve the quality of phrasing knowledge
or incorporate other concept-level knowledge in
text-to-SQL.

Acknowledgment

This work is supported in part by the National
Natural Science Foundation of China (Grand Nos.
U1636211, 61672081, 61370126).



3520

References
Jianpeng Cheng, Zhongyuan Wang, Ji-Rong Wen, Jun

Yan, and Zheng Chen. 2015. Contextual text under-
standing in distributional semantic space. In Pro-
ceedings of the 24th ACM International on Confer-
ence on Information and Knowledge Management,
pages 133–142. ACM.

Gerard De Melo and Mohit Bansal. 2013. Good, great,
excellent: Global inference of semantic intensities.
Transactions of the Association for Computational
Linguistics, 1:279–290.

Li Dong and Mirella Lapata. 2018. Coarse-to-fine de-
coding for neural semantic parsing. In Proceedings
of the 56th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Paper-
s), volume 1, pages 731–742.

Catherine Finegan-Dollak, Jonathan K Kummerfeld,
Li Zhang, Karthik Ramanathan, Sesh Sadasivam,
Rui Zhang, and Dragomir Radev. 2018. Improving
text-to-sql evaluation methodology. In Proceedings
of the 56th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Paper-
s), volume 1, pages 351–360.

Jonathan Herzig and Jonathan Berant. 2018. Decou-
pling structure and lexicon for zero-shot semantic
parsing. In Proceedings of the 2018 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1619–1629.

Oliver Lehmberg, Dominique Ritze, Robert Meusel,
and Christian Bizer. 2016. A large public corpus
of web tables containing time and context metadata.
In Proceedings of the 25th International Conference
Companion on World Wide Web, pages 75–76. In-
ternational World Wide Web Conferences Steering
Committee.

George A Miller. 1995. Wordnet: a lexical database for
english. Communications of the ACM, 38(11):39–
41.

Josef Ruppenhofer, Michael Wiegand, and Jasper
Brandes. 2014. Comparing methods for deriving in-
tensity scores for adjectives. In Proceedings of the
14th Conference of the European Chapter of the As-
sociation for Computational Linguistics, volume 2:
Short Papers, pages 117–122.

Raksha Sharma, Arpan Somani, Lakshya Kumar, and
Pushpak Bhattacharyya. 2017. Sentiment intensity
ranking among adjectives using sentiment bearing
word embeddings. In Proceedings of the 2017 Con-
ference on Empirical Methods in Natural Language
Processing, pages 547–552.

Wentao Wu, Hongsong Li, Haixun Wang, and Kenny Q
Zhu. 2012. Probase: A probabilistic taxonomy for
text understanding. In Proceedings of the 2012 ACM
SIGMOD International Conference on Management
of Data, pages 481–492. ACM.

Xiaojun Xu, Chang Liu, and Dawn Song. 2017. Sqlnet:
Generating structured queries from natural language
without reinforcement learning. arXiv preprint arX-
iv:1711.04436.

Navid Yaghmazadeh, Yuepeng Wang, Isil Dillig, and
Thomas Dillig. 2017. Sqlizer: query synthesis from
natural language. Proceedings of the ACM on Pro-
gramming Languages, 1(OOPSLA):63.

Tao Yu, Zifan Li, Zilin Zhang, Rui Zhang, and
Dragomir Radev. 2018a. Typesql: Knowledge-
based type-aware neural text-to-sql generation. In
Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Volume 2 (Short Papers), volume 2, pages 588–594.

Tao Yu, Michihiro Yasunaga, Kai Yang, Rui Zhang,
Dongxu Wang, Zifan Li, and Dragomir Radev.
2018b. Syntaxsqlnet: Syntax tree networks for com-
plex and cross-domain text-to-sql task. In Proceed-
ings of the 2018 Conference on Empirical Methods
in Natural Language Processing, pages 1653–1663.

Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga,
Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingn-
ing Yao, Shanelle Roman, et al. 2018c. Spider: A
large-scale human-labeled dataset for complex and
cross-domain semantic parsing and text-to-sql task.
In Proceedings of the 2018 Conference on Empiri-
cal Methods in Natural Language Processing, pages
3911–3921.

Yuchen Zhang, Panupong Pasupat, and Percy Liang.
2017. Macro grammars and holistic triggering for
efficient semantic parsing. In Proceedings of the
2017 Conference on Empirical Methods in Natural
Language Processing, pages 1214–1223.

Victor Zhong, Caiming Xiong, and Richard Socher.
2017. Seq2sql: Generating structured queries from
natural language using reinforcement learning. arX-
iv preprint arXiv:1709.00103.


