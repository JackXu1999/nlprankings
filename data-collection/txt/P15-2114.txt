



















































Learning Hybrid Representations to Retrieve Semantically Equivalent Questions


Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 694–699,

Beijing, China, July 26-31, 2015. c©2015 Association for Computational Linguistics

Learning Hybrid Representations to Retrieve
Semantically Equivalent Questions

Cı́cero dos Santos1, Luciano Barbosa1, Dasha Bogdanova2, Bianca Zadrozny1
1IBM Research, 138/146 Av. Pasteur, Rio de Janeiro, Brazil
{cicerons,lucianoa,biancaz}@br.ibm.com

2ADAPT centre, School of Computing, Dublin City University, Dublin, Ireland
dbogdanova@computing.dcu.ie

Abstract

Retrieving similar questions in online
Q&A community sites is a difficult task
because different users may formulate the
same question in a variety of ways, us-
ing different vocabulary and structure.
In this work, we propose a new neural
network architecture to perform the task
of semantically equivalent question re-
trieval. The proposed architecture, which
we call BOW-CNN, combines a bag-of-
words (BOW) representation with a dis-
tributed vector representation created by a
convolutional neural network (CNN). We
perform experiments using data collected
from two Stack Exchange communities.
Our experimental results evidence that: (1)
BOW-CNN is more effective than BOW
based information retrieval methods such
as TFIDF; (2) BOW-CNN is more robust
than the pure CNN for long texts.

1 Introduction

Most Question-answering (Q&A) community
sites advise users before posting a new question
to search for similar questions. This is not always
an easy task because different users may formulate
the same question in a variety of ways.

We define two questions as semantically equiv-
alent if they can be adequately answered by the
exact same answer. Here is an example of a pair
of such questions from Ask Ubuntu community,
which is part of the Stack Exchange Q&A com-
munity site: (q1)“I have downloaded ISO files re-
cently. How do I burn it to a CD or DVD or mount
it?” and (q2)“I need to copy the iso file for Ubuntu
12.04 to a CD-R in Win8. How do I do so?”.
Retrieving semantically equivalent questions is a
challenging task due to two main factors: (1) the
same question can be rephrased in many different

ways; and (2) two questions may be different but
may refer implicitly to a common problem with
the same answer. Therefore, traditional similarity
measures based on word overlap such as shingling
and Jaccard coefficient (Broder, 1997) and its vari-
ations (Wu et al., 2011) are not able to capture
many cases of semantic equivalence. To capture
the semantic relationship between pair of ques-
tions, different strategies have been used such as
machine translation (Jeon et al., 2005; Xue et al.,
2008), knowledge graphs (Zhou et al., 2013) and
topic modelling (Cai et al., 2011; Ji et al., 2012).

Recent papers (Kim, 2014; Hu et al., 2014; Yih
et al., 2014; dos Santos and Gatti, 2014; Shen et
al., 2014) have shown the effectiveness of convo-
lutional neural networks (CNN) for sentence-level
analysis of short texts in a variety of different nat-
ural language processing and information retrieval
tasks. This motivated us to investigate CNNs for
the task of semantically equivalent question re-
trieval. However, given the fact that the size of a
question in an online community may vary from a
single sentence to a detailed problem description
with several sentences, it was not clear that the
CNN representation would be the most adequate.

In this paper, we propose a hybrid neural net-
work architecture, which we call BOW-CNN. It
combines a traditional bag-of-words (BOW) rep-
resentation with a distributed vector representa-
tion created by a CNN, to retrieve semantically
equivalent questions. Using a ranking loss func-
tion in the training, BOW-CNN learns to represent
questions while learning to rank them according to
their semantic similarity. We evaluate BOW-CNN
over two different Q&A communities in the Stack
Exchange site, comparing it against CNN and 6
well-established information retrieval algorithms
based on BOW. The results show that our proposed
solution outperforms BOW-based information re-
trieval methods such as the term frequency - in-
verse document frequency (TFIDF) in all evalu-

694



Figure 1: Representing and scoring questions with
weighted bag-of-words.

ated scenarios. Moreover, we were able to show
that for short texts (title of the questions), an ap-
proach using only CNN obtains the best results,
whereas for long texts (title and body of the ques-
tions), our hybrid approach (BOW-CNN) is more
effective.

2 BOW-CNN

2.1 Feed Forward Processing

The goal of the feed forward processing is to cal-
culate the similarity between a pair of questions
(q1, q2). To perform this task, each question
q follows two parallel paths (BOW and CNN),
each one producing a distinct vector representa-
tions of q. The BOW path produces a weighted
bag-of-words representation of the question, rbowq ,
where the weight of each word in the vocabu-
lary V is learned by the neural network. The
CNN path, uses a convolutional approach to con-
struct a distributed vector representations, rconvq ,
of the question. After producing the BOW and
CNN representations for the two input questions,
the BOW-CNN computes two partial similarity
scores sbow(q1, q2), for the CNN representations,
and sconv(q1, q2), for the BOW representations.
Finally, it combines the two partial scores to create
the final score s(q1, q2).

2.2 BOW Path

The generation of the bag-of-words representation
for a given question q is quite straightforward. As
detailed in Figure 1, we first create a sparse vec-
tor qbow ∈ R|V | that contains the frequency in q of
each word of the vocabulary. Next, we compute
the weighted bag-of-words representation by per-

Figure 2: Representing and scoring questions with
a convolutional approach.

forming the element-wise vector multiplication:

rbowq = q
bow ∗ t (1)

where the vector t ∈ R|V |, contains a weight for
each word in the vocabulary V . The vector t is a
parameter to be learned by the network. This is
closely related to the TFIDF text representation.
In fact, if we fix t to the vector of IDFs, this corre-
sponds to the exact TFIDF representation.

2.3 CNN Path
As detailed in Figure 2, the first layer of the
CNN path transforms words into representations
that capture syntactic and semantic information
about the words. Given a question consisting of
N words q = {w1, ..., wN}, every word wn is
converted into a real-valued vector rwn . There-
fore, for each question, the input to the next NN
layer is a sequence of real-valued vectors qemb =
{rw1 , ..., rwN }. Word representations are encoded
by column vectors in an embedding matrix W 0 ∈
Rd×|V |, where V is a fixed-sized vocabulary.

The next step in the CNN path consists in cre-
ating distributed vector representations rconvq1 and
rconvq2 from the word embedding sequencies q

emb
1

and qemb2 . We perform this by using a convolu-
tional layer in the same way as used in (dos Santos
and Gatti, 2014) to create sentence-level represen-
tations.

More specifically, given a question q1, the con-
volutional layer applies a matrix-vector operation
to each window of size k of successive windows

695



in qemb1 = {rw1 , ..., rwN }. Let us define the vector
zn ∈ Rdk as the concatenation of a sequence of k
word embeddings, centralized in the n-th word:

zn = (rwn−(k−1)/2 , ..., rwn+(k−1)/2)
T

The convolutional layer computes the j-th ele-
ment of the vector rconvq1 ∈ Rclu as follows:

[rconvq1 ]j = f
(

max
1<n<N

[
W 1zn + b1

]
j

)
(2)

where W 1 ∈ Rclu×dk is the weight matrix of the
convolutional layer and f is the hyperbolic tangent
function. Matrices W 0 and W 1, and the vector b1

are parameters to be learned. The word embedding
size d, the number of convolutional units clu, and
the size of the word context window k are hyper-
parameters to be chosen by the user.

2.4 Question Pair Scoring
After the bag-of-words and convolutional-based
representations are generated for the input pair (q1,
q2), the partial scores are computed as the cosine
similarity between the respective vectors:

sbow(q1, q2) =
rbowq1 .r

bow
q2

‖rbowq1 ‖‖rbowq2 ‖

sconv(q1, q2) =
rconvq1 .r

conv
q2

‖rconvq1 ‖‖rconvq2 ‖
The final score for the input questions (q1, q2) is

given by the following linear combination

s(q1, q2) = β1 ∗ sbow(q1, q2) + β2 ∗ sconv(q1, q2)
where β1 and β2 are parameters to be learned.

2.5 Training Procedure
Our network is trained by minimizing a ranking
loss function over the training set D. The input in
each round is two pairs of questions (q1, q2)+ and
(q1, qx)− where the questions in the first pair are
semantically equivalent (positive example), and
the ones in the second pair are not (negative ex-
ample). Let ∆ be the difference of their similarity
scores, ∆ = sθ(q1, q2) − sθ(q1, qx), generated by
the network with parameter set θ. As in (Yih et al.,
2011), we use a logistic loss over ∆

L(∆, θ) = log(1 + exp(−γ∆))
where γ is a scaling factor that magnifies ∆ from
[-2,2] (in the case of using cosine similarity) to a

larger range. This helps to penalize more on the
prediction errors. Following (Yih et al., 2011), in
our experiments we set γ to 10.

Sampling informative negative examples can
have a significant impact in the effectiveness of the
learned model. In our experiments, before train-
ing, we create 20 pairs of negative examples for
each positive pair (q1,q2)+. To create a negative
example we (1) randomly sample a question qx
that is not semantically equivalent to q1 or q2; (2)
then create negative pairs (q1,qx)− and (q2,qx)−.
During training, at each iteration we only use the
negative example x that produces the smallest dif-
ferent sθ(q1, q2)+ − sθ(q1, qx)−. Using this strat-
egy, we select more representative negative exam-
ples.

We use stochastic gradient descent (SGD) to
minimize the loss function with respect to θ.
The backpropagation algorithm is used to com-
pute the gradients of the network. In our exper-
iments, BOW-CNN architecture is implemented
using Theano (Bergstra et al., 2010).

3 Experimental Setup

3.1 Data

A well-structured source of semantically equiv-
alent questions is the Stack Exchange site. It
is composed by multiple Q&A communities,
whereby users can ask and answer questions, and
vote up and down both questions and answers.
Questions are composed by a title and a body.
Moderators can mark questions as duplicates, and
eventually a question can have multiple duplicates.

For this evaluation, we chose two highly-
accessed Q&A communities: Ask Ubuntu and En-
glish. They differ in terms of content and size.
Whereas Ask Ubuntu has 29510 duplicated ques-
tions, English has 6621. We performed exper-
iments using only the title of the questions as
well as title + body, which we call all for the
rest of this section. The average size of a title
is very small (about 10 words), which is at least
10 times smaller than the average size of all for
both datasets. The data was tokenized using the
tokenizer available with the Stanford POS Tag-
ger (Toutanova et al., 2003), and all links were re-
placed by a unique string. For Ask Ubuntu, we
did not consider the content inside the tag code,
which contains some specific Linux commands or
programming code.

For each community, we created training, vali-

696



Community Training Validation Test
Ask Ubuntu 9802 1991 3800
English 2235 428 816

Table 1: Partition of training, validation and test
sets for the experiments.

dation and test sets. In Table 1, we inform the size
of each set. The number of instances in the train-
ing set corresponds to the number of positive pairs
of semantically equivalent questions. The number
of instances in the validation and the test sets cor-
respond to the number of questions which are used
as queries. All questions in the validation and test
set contain at least one duplicated question in the
set of all questions. In our experiments, given a
query question q, all questions in the Q&A com-
munity are evaluated when searching for a dupli-
cate of q.

3.2 Baselines and Neural Network Setup

In order to verify the impact of jointly using
BOW and CNN representations, we perform ex-
periments with two NN architectures: the BOW-
CNN and the CNN alone, which consists in us-
ing only the CNN path of BOW-CNN and, con-
sequently, computing the score for a pair of ques-
tions using s(q1, q2) = sconv(q1, q2).

Additionally, we compare BOW-CNN with six
well-established IR algorithms available on the
Lucene package (Hatcher et al., 2004). Here we
provide a brief overview of them. For further de-
tails, we refer the reader to the citation associated
with the algorithm.

• TFIDF (Manning et al., 2008) uses the tradi-
tional Vector Space Model to represent docu-
ments as vectors in a high-dimensional space.
Each position in the vector represents a word
and the weight of words are calculated using
TFIDF.

• BM25 (Robertson and Walker, 1994) is a
probabilistic weighting method that takes
into consideration term frequency, inverse
document frequency and document length.
Its has two free parameters: k1 to tune term-
frequency saturation; and b to calibrate the
document-length normalization.

• IB (Clinchant and Gaussier, 2010) uses
information-based models to capture the im-
portance of a term by measuring how much

Param. Name BOW-CNN CNN
Word Emb. Size 200 200
Context Winow Size 3 3
Convol. Units 400 1000
Learning Rate 0.01 0.05

Table 2: Neural Network Hyper-Parameters

its behavior in a document deviates from its
behavior in the whole collection.

• DFR (Amati and Van Rijsbergen, 2002) is
based on divergence from randomness frame-
work. The relevance of a term is measured by
the divergence between its actual distribution
and the distribution from a random process.

• LMDirichlet and LMJelinekMercer apply
probabilistic language model approaches for
retrieval (Zhai and Lafferty, 2004). They dif-
fer in the smoothing method: LMDirichlet
uses Dirichlet priors and LMJelinekMercer
uses the Jelinek-Mercer method.

The word embeddings used in our experiments
are initialized by means of unsupervised pre-
training. We perform pre-training using the skip-
gram NN architecture (Mikolov et al., 2013) avail-
able in the word2vec tool. We use the En-
glish Wikipedia to train word embeddings for
experiments with the English dataset. For the
AskUbuntu dataset, we use all available Ask-
Ubuntu community data to train word embed-
dings.

The hyper-parameters of the neural networks
and the baselines are tuned using the development
sets. In Table 2, we show the selected hyper-
parameter values. In our experiments, we initialize
each element [t]i of the bag-of-word weight vector
t with the IDF of i−th word wi computed over the
respective set of questions Q as follows

[t]i = IDF (wi, Q) = log
|Q|

|q ∈ Q : wi ∈ q|

4 Experimental Results

Comparison with Baselines. In Tables 3 and
4, we present the question retrieval performance
(Accuracy@k) of different algorithms over the
AskUbuntu and English datasets for the title and
all settings, respectively. For both datasets, BOW-
CNN outperforms the six IR algorithms for both
title and all settings. For the AskUbuntu all,
BOW-CNN is four absolute points larger than the

697



AskUbuntu English
Algorithm @1 @5 @10 @1 @5 @10
TFIDF 8.3 17.5 22.5 10.0 18.1 21.6
BM25 7.3 17.1 21.8 10.0 18.9 23.2
IB 8.1 18.1 22.6 10.1 18.4 22.7
DFR 7.7 17.8 22.4 10.5 19.0 23.0
LMD 5.6 14.1 19.0 10.9 20.1 24.2
LMJ 8.3 17.5 22.5 10.3 18.5 22.1
CNN 11.5 24.8 31.4 11.6 23.0 26.9
BOW-CNN 10.9 22.6 28.7 11.3 21.4 26.0

Table 3: Question title retrieval performance (Ac-
curacy@k) for different algorithms.

AskUbuntu English
Algorithm @1 @5 @10 @1 @5 @10
TFIDF 16.9 31.3 38.3 25.9 42.0 48.1
BM25 18.2 33.1 39.8 29.4 45.7 52.5
IB 14.9 28.2 34.8 25.4 42.3 48.0
DFR 18.0 32.6 39.2 28.6 45.4 52.5
LMD 13.7 26.8 34.4 23.0 40.2 46.0
LMJ 18.3 33.4 40.7 28.5 45.7 52.3
CNN 20.0 33.8 40.1 17.2 29.6 33.8
BOW-CNN 22.3 39.7 46.4 30.8 47.7 54.9

Table 4: Question title + body (all) retrieval per-
formance for different algorithms.

best IR baseline (LMJ) in terms of Accuracy@1,
which represents an improvement of 21.9%. Since
the BOW representation we use is closely related
to TFIDF, an important comparison is the perfor-
mance of BOW-CNN vs. TFIDF. In Tables 3 and
4, we can see that BOW-CNN consistently outper-
forms the TFIDF model in the two datasets for
both cases title and all. These findings suggest
that BOW-CNN is indeed combining the strong
semantic representation power conveyed by the
convolutional-based representation to, jointly with
the BOW representation, construct a more effec-
tive model.

Another interesting finding is that CNN out-
performs BOW-CNN for short texts (Table 3)
and, conversely, BOW-CNN outperforms CNN for
long texts (Table 4). This demonstrates that, when
dealing with large input texts, BOW-CNN is an
effective approach to combine the strengths of
convolutional-based representation and BOW.

Impact of Initialization of BOW Weights. In
the BOW-CNN experiments whose results are pre-
sented in tables 3 and 4 we initialize the elements
of the BOW weight vector t with the IDF of each
word in V computed over the question set Q. In
this section we show some experimental results
that indicate the contribution of this initialization.

In Table 5, we present the performance of

BOW-CNN for the English dataset when differ-
ent configurations of the BOW weight vector t are
used. The first column of Table 5 indicates the
type of initialization, where ones means that t is
initialized with the value 1 (one) in all positions.
The second column informs whether t is allowed
to be updated (Yes) by the network or not (No).
The numbers suggest that letting BOW weights
free to be updated by the network produces better
results than fixing them to IDF values. In addition,
using IDF to initialize the BOW weight vector is
better than using the same weight (ones) to initial-
ize it. This is expected, since we are injecting a
prior knowledge known to be helpful in IR tasks.

Title All
t initial t updated @1 @10 @1 @10

IDF Yes 11.3 26.0 30.8 54.9
IDF No 10.6 25.3 29.7 54.9
Ones Yes 10.7 24.2 26.3 51.2

Table 5: BOW-CNN performance using different
methods to initialize the BOW weight vector t.

5 Conclusions

In this paper, we propose a hybrid neural network
architecture, BOW-CNN, that combines bag-of-
words with distributed vector representations cre-
ated by a CNN, to retrieve semantically equivalent
questions. Our experimental evaluation showed
that: our approach outperforms traditional bow ap-
proaches; for short texts, a pure CNN obtains the
best results, whereas for long texts, BOW-CNN is
more effective; and initializing the BOW weight
vector with IDF values is beneficial.

Acknowledgments

Dasha Bogdanova’s contributions were made
during an internship at IBM Research. Her
work was partially supported by Science Foun-
dation Ireland through the CNGL Programme
(Grant 12/CE/I2267) in the ADAPT Centre
(www.adaptcentre.ie) at DCU.

References
Gianni Amati and Cornelis Joost Van Rijsbergen.

2002. Probabilistic models of information retrieval
based on measuring the divergence from random-
ness. ACM Transactions on Information Systems
(TOIS), 20(4):357–389.

James Bergstra, Olivier Breuleux, Frédéric Bastien,
Pascal Lamblin, Razvan Pascanu, Guillaume Des-

698



jardins, Joseph Turian, David Warde-Farley, and
Yoshua Bengio. 2010. Theano: a CPU and GPU
math expression compiler. In Proceedings of the
Python for Scientific Computing Conference.

A. Broder. 1997. On the resemblance and containment
of documents. In Proceedings of the Compression
and Complexity of Sequences 1997, SEQUENCES
’97, pages 21–, Washington, DC, USA. IEEE Com-
puter Society.

Li Cai, Guangyou Zhou, Kang Liu, and Jun Zhao.
2011. Learning the latent topics for question re-
trieval in community qa. In IJCNLP, volume 11,
pages 273–281.

Stéphane Clinchant and Eric Gaussier. 2010.
Information-based models for ad hoc ir. In Proceed-
ings of the 33rd international ACM SIGIR confer-
ence on Research and development in information
retrieval, pages 234–241. ACM.

Cı́cero Nogueira dos Santos and Maı́ra Gatti. 2014.
Deep convolutional neural networks for sentiment
analysis of short texts. In Proceedings of the 25th In-
ternational Conference on Computational Linguis-
tics (COLING), Dublin, Ireland.

Erik Hatcher, Otis Gospodnetic, and Michael McCan-
dless. 2004. Lucene in action.

Baotian Hu, Zhengdong Lu, Hang Li, and Qingcai
Chen. 2014. Convolutional neural network archi-
tectures for matching natural language sentences. In
Advances in Neural Information Processing Systems
27: Annual Conference on Neural Information Pro-
cessing Systems 2014, December 8-13 2014, Mon-
treal, Quebec, Canada, pages 2042–2050.

Jiwoon Jeon, W Bruce Croft, and Joon Ho Lee. 2005.
Finding similar questions in large question and an-
swer archives. In Proceedings of the 14th ACM in-
ternational conference on Information and knowl-
edge management, pages 84–90.

Zongcheng Ji, Fei Xu, Bin Wang, and Ben He. 2012.
Question-answer topic model for question retrieval
in community question answering. In Proceedings
of the 21st ACM international conference on Infor-
mation and knowledge management, pages 2471–
2474.

Yoon Kim. 2014. Convolutional neural networks for
sentence classification. In Proceedings of the 2014
Conference on Empirical Methods for Natural Lan-
guage Processing, pages 1746–1751.

Christopher D Manning, Prabhakar Raghavan, and
Hinrich Schütze. 2008. Introduction to informa-
tion retrieval, volume 1. Cambridge university press
Cambridge.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. In In Proceedings of Work-
shop at ICLR.

Stephen E Robertson and Steve Walker. 1994. Some
simple effective approximations to the 2-poisson
model for probabilistic weighted retrieval. In Pro-
ceedings of the 17th annual international confer-
ence on Research and development in information
retrieval, pages 232–241.

Yelong Shen, Xiaodong He, Jianfeng Gao, Li Deng,
and Grégoire Mesnil. 2014. A latent semantic
model with convolutional-pooling structure for in-
formation retrieval. In Proceedings of the 23rd ACM
International Conference on Conference on Infor-
mation and Knowledge Management, pages 101–
110.

Kristina Toutanova, Dan Klein, Christopher D Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the North American Chapter of the
Association for Computational Linguistics on Hu-
man Language Technology, pages 173–180.

Yan Wu, Qi Zhang, and Xuanjing Huang. 2011. Ef-
ficient near-duplicate detection for q&a forum. In
Proceedings of 5th International Joint Conference
on Natural Language Processing, pages 1001–1009,
Chiang Mai, Thailand, November. Asian Federation
of Natural Language Processing.

Xiaobing Xue, Jiwoon Jeon, and W. Bruce Croft. 2008.
Retrieval models for question and answer archives.
In Proceedings of the 31st Annual International
ACM SIGIR Conference on Research and Devel-
opment in Information Retrieval, SIGIR ’08, pages
475–482.

Wen-tau Yih, Kristina Toutanova, John C. Platt, and
Christopher Meek. 2011. Learning discrimina-
tive projections for text similarity measures. In
Proceedings of the Fifteenth Conference on Com-
putational Natural Language Learning, CoNLL’11,
pages 247–256.

Wen-tau Yih, Xiaodong He, and Christopher Meek.
2014. Semantic parsing for single-relation ques-
tion answering. In Proceedings of the 52nd Annual
Meeting of the Association for Computational Lin-
guistics (Volume 2: Short Papers), pages 643–648.
Association for Computational Linguistics.

Chengxiang Zhai and John Lafferty. 2004. A study of
smoothing methods for language models applied to
information retrieval. ACM Transactions on Infor-
mation Systems (TOIS), 22(2):179–214.

Guangyou Zhou, Yang Liu, Fang Liu, Daojian Zeng,
and Jun Zhao. 2013. Improving question retrieval in
community question answering using world knowl-
edge. In Proceedings of the Twenty-Third inter-
national joint conference on Artificial Intelligence,
pages 2239–2245.

699


