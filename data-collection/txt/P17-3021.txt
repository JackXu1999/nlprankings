



















































Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics


Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics- Student Research Workshop, pages 128–135
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-3021

Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics- Student Research Workshop, pages 128–135
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-3021

An Empirical Study on End-to-End Sentence Modelling

Kurt Junshean Espinosa
National Centre for Text Mining, School of Computer Science, University of Manchester, UK

Department of Computer Science, University of the Philippines Cebu, Philippines
kurtjunshean.espinosa@postgrad.manchester.ac.uk

kpespinosa@up.edu.ph

Abstract
Accurately representing the meaning of a
piece of text, otherwise known as sen-
tence modelling, is an important compo-
nent in many natural language inference
tasks. We survey the spectrum of these
methods, which lie along two dimensions:
input representation granularity and com-
position model complexity. Using this
framework, we reveal in our quantitative
and qualitative experiments the limitations
of the current state-of-the-art model in the
context of sentence similarity tasks.

1 Introduction
Accurately representing the meaning of a piece of
text remains an open problem. To illustrate why it
is difficult, consider the pair of sentences A and B
below in the context of a sentence similarity task.
A:The shares of the company dropped.
B:The organisation’s stocks slumped.

If we use a very naı̈ve model such as bag-
of-words to represent a sentence and use dis-
crete counting of common words between the two
sentences to determine their similarity, the score
would be very low although they are highly sim-
ilar. How then do we represent the meaning of
sentences?

Firstly, we must be able to represent them in
ways that computers can understand. Based on
the Principle of Compositionality (Frege, 1892),
we define the meaning of a sentence as a func-
tion of the meaning of its constituents (i.e., words,
phrases, morphemes). Generally, there are two
main approaches to representing constituents: lo-
calist and distributed representations. With the
localist representation1, we represent each con-
stituent with a unique representation usually taken

1The best example of this sparse representation is the
“one-hot” representation (see Appendix A for details)

from its position in a vocabulary V. However,
this kind of representation suffers from the curse
of dimensionality and does not consider the syn-
tactic relationship of a constituent with other
constituents. These two shortcomings are ad-
dressed by the distributed representation (Hinton,
1984) which encodes a constituent based on its
co-occurrence with other constituents appearing
within its context, into a dense n-dimensional vec-
tor where n ⌧ |V |. Estimating the distributed rep-
resentation has been an active research topic in
itself. Baroni et al. (2014) conducted a system-
atic comparative evaluation of context-counting
and context-predicting models for generating dis-
tributed representations and concluded that the lat-
ter outperforms the former, but Levy et al. (2015)
later have shown that simple pointwise mutual
information (PMI) methods also perform simi-
larly if they are properly tuned. To date, the
most popular architectures to efficiently estimate
these distributed representations are word2vec
(Mikolov et al., 2013a) and GloVe (Pennington
et al., 2014). Subsequent developments estimate
distributed representations at other levels of gran-
ularity (see Section 2.1).

While much research has been directed into
constructing representations for constituents, there
has been far less consensus regarding the represen-
tation of larger semantic structures such as phrases
and sentences (Blacoe and Lapata, 2012). A sim-
ple approach is based on looking up the vector rep-
resentation of the constituents (i.e., embeddings)
and taking their sum or average which yields a sin-
gle vector of the same dimension. This strategy
is effective in simple tasks but loses word order
information and syntactic relations in the process
(Mitchell and Lapata, 2008; Turney et al., 2010).
Most modern neural network models have a sen-
tence encoder that learns the representation of sen-
tences more efficiently while preserving word or-

128

https://doi.org/10.18653/v1/P17-3021
https://doi.org/10.18653/v1/P17-3021


der and compositionality (see Section 2.1).
In this work, we present a generalised frame-

work for sentence modelling based on a survey of
state-of-the-art methods. Using the framework as
a guide, we conducted preliminary experiments by
implementing an end-to-end version of the state-
of-the-art model in which we reveal its limitations
after evaluation on sentence similarity tasks.

2 Related Work

The best way to evaluate sentence models is to
assess how they perform on actual natural lan-
guage inference (NLI) tasks. In this work, we
examine three related tasks which are central to
natural language understanding: paraphrase detec-
tion (Dolan et al., 2004; Xu et al., 2015), seman-
tic similarity measurement (Marelli et al., 2014;
Xu et al., 2015; Agirre et al., 2016a) and inter-
pretable semantic similarity measurement (Agirre
et al., 2016b). (We refer the reader to the respec-
tive papers for the task description and dataset de-
tails).

Among the four broad types of methods we
have identified in the literature (see Appendix
C.1), we focus in this paper on deep learning (DL)
methods because they support end-to-end learn-
ing, i.e., they use few hand-crafted features—or
none at all, making them easier to adapt to new
domains. More importantly, these methods have
obtained comparable performance relative to other
top-ranking methods.

2.1 Sentence Modelling Framework

As a contribution of this work, we survey the spec-
trum of DL methods, which lie on two dimensions:
input representation granularity and composition
model complexity, which are both central to sen-
tence modelling (see Appendix Figure C.2 for a
graphical illustration).

The first dimension (see horizontal axis of Ap-
pendix Figure C.2) is the granularity of input rep-
resentation. This dimension characterises a trade-
off between syntactic dependencies captured in the
representation and data sparsity. On the one hand,
character-based methods (Vosoughi et al., 2016;
dos Santos and Zadrozny, 2014; Wieting et al.,
2016) are not faced with the data sparsity prob-
lem; however, it is not straightforward to deter-
mine whether composing sentences based on in-
dividual character representations would represent
the originally intended semantics. On the other

hand, while sentence embeddings (Kiros et al.,
2015), which are learned by predicting the previ-
ous and next sentences given the current sentence,
could intuitively represent the actual semantics, it
suffers from data sparsity.

The second dimension (see vertical axis of Ap-
pendix Figure C.2) is the spectrum of composi-
tion models ranging from bag-of-items-driven2 ar-
chitectures to compositionality-driven ones to ac-
count for the morphological, lexical, syntactic,
and compositional aspects of a sentence. Some
of the popular methods are based on Bag-of-Item
models, which represent a sentence by perform-
ing algebraic operations (e.g., addition or aver-
aging) over the vector representations of individ-
ual constituents (Blacoe and Lapata, 2012). How-
ever, these models have received criticism as they
use linear bag-of-words context and thus do not
take into account syntax. Spatial neural net-
works, e.g., Convolutional Neural Networks or
ConvNets (LeCun et al., 1998), have been shown
to capture morphological variations in short sub-
sequences (dos Santos and Zadrozny, 2014; Chiu
and Nichols, 2016). However, this architecture
still does not capture the overall syntactic informa-
tion. Thus Sutskever et al. (2014) proposed the use
of sequence-based neural networks, e.g., Recur-
rent Neural Networks, Long Short Term Memory
models (Hochreiter and Schmidhuber, 1997), be-
cause they can capture long-range temporal depen-
dencies. Tai et al. (2015) introduced Tree-LSTM,
a generalisation of LSTMs to tree-structured net-
work topologies, e.g., Recursive Neural Networks
(Socher et al., 2011). However, this type of net-
work requires input from an external resource (i.e.,
dependency/constituency parser).

More complex models involved stacked archi-
tectures of the three basic forms above (He and
Lin, 2016; Yin et al., 2015; Cheng and Kartsaklis,
2015; Zhang et al., 2015; He et al., 2015) which
capture the syntactic and semantic structure of a
language. However, in addition to being com-
putationally intensive, most of these architectures
model sentences as vectors with a fixed size, they
risk losing information especially when input sen-
tence vectors are of varying lengths. Recently,
Bahdanau et al. (2014) introduced the concept
of attention, originally in the context of machine
translation, where the network learns to align parts

2We use items instead of words to generalise amongst var-
ious representations.

129



of the source sentence that match the constituents
of the target sentence, without having to explicitly
form these parts as hard segments. This enables
phrase-alignments between sentences as described
by Yin and Schütze (2016) in the context of a tex-
tual entailment recognition task.

3 Preliminary Experiments

In this section, we describe the preliminary experi-
ments we conducted in order to gain deeper under-
standing on the limitations of the state-of-the-art
model.

Firstly, we define sentence similarity as
a supervised learning task where each train-
ing example consists of a pair of sentences
(xa1, ..., x

a
Ta

), (xb1, ..., x
b
Tb

) of fixed-sized vectors
(where xai , x

b
j 2 Rdinput denoting constituent vec-

tors from each sentence, respectively, which may
be of different lengths Ta 6= Tb) along with a sin-
gle real-valued label y for the pair. We evaluated
the performance of the state-of-the-art model on
this task.

3.1 Model Overview

Since we focus on end-to-end sentence modelling,
we implement a simplified (see Table 1) version of
MaLSTM (Mueller and Thyagarajan, 2016), i.e.,
the state-of-the-art model on this task (see Ap-
pendix Figure C.1). The model uses a siamese ar-
chitecture of Long-Short Term Memory (LSTM)
to read word vectors representing each input sen-
tence. Each LSTM cell has four components: in-
put gate it, forget gate ft, memory state ct, and
output gate ot; which decides the information to
retain or forget in a sequence of inputs. Equa-
tions 1-6 are the updates performed at each LSTM
cell for a sequence of input (x1, ..., xT ) at each
timestep t 2 {1, ..., T}, parameterised by weight
matrices Wi, Wf , Wc, Wo, Ui, Uf , Uc, Uo and bias
vectors bi, bf , bc, bo.

it = �(Wixt + Uiht�1 + bi) (1)
ft = �(Wfxt + Ufht�1 + bf ) (2)

c̃t = tanh(Wcxt + Ucht�1 + bc) (3)
ct = it � c̃t + ft � ct�1 (4)

ot = �(Woxt + Uoht�1 + bo) (5)
ht = ot � tanh(ct) (6)

This model computes the sentence similarity
based on the Manhattan distance between the fi-
nal hidden state representations for each sentence:
g(haTa , h

b
Tb

) = exp(�||haTa � hbTb ||1) 2 [0, 1],
which was found to perform better empirically

Model Feature MaLSTM Ours

pre-training yes no
synonym augmentation yes no
prediction calibration yes no
optimisation method Adadelta Adam

Table 1: Model comparison between MaLSTM
and our implementation

than other simple similarity functions such as co-
sine similarity (Mueller and Thyagarajan, 2016).

3.2 Training Details
We use the 300-dimensional pre-trained
word2vec3(Mikolov et al., 2013b) word em-
beddings and compare the performance with that
of GloVe4 (Pennington et al., 2014) embeddings.
Out-of-embedding-vocabulary (OOEV) words
are replaced with an <unk> token. We retain
the word cases and keep the digits. For character
representation, we fine-tune the 50-dimensional
initial embeddings, modifying them during gra-
dient updates of the neural network model by
back-propagating gradients. The chosen size of
the embeddings was found to perform best after
initial experiments with different sizes.

Our model uses 50-dimensional hidden repre-
sentations ht and memory cells ct. Optimisation of
the parameters is done using the SGD-based Adam
method (Kingma and Ba, 2014) and we perform
gradient clipping to prevent exploding gradients.
We tune the hyper-parameters on the validation
set by random search since it is infeasible to do
a random search across the full hyper-parameter
space due to time constraints. After conducting
initial experiments, we found the optimal training
parameters to be the following: batch size = 30,
learning rate = 0.01, learning rate decay = 0.98,
dropout = 0.5, number of LSTM layers = 1, maxi-
mum epochs = 10, patience = 5 epochs. Patience is
the early stopping condition based on performance
on validation sets. We used the Tensorflow5 li-
brary to implement and train the model.

3.3 Dataset and Evaluation
We measure the model’s performance on three
benchmark datasets, i.e., SICK 2014 (Marelli
et al., 2014), STS 2016 (Agirre et al., 2016a) and

3code.google.com/p/word2vec
4https://nlp.stanford.edu/projects/glove/
5https://www.tensorflow.org/

130



Dataset Baseline
LSTM Vector Sum

GloVe word2vec char GloVe word2vec char

SICK 2014 0.5675 0.7430 0.7355 0.3487 0.4903 0.5099 0.0178
PIT 2015 0.4001 0.1187 0.0581 0.0086 0.1263 0.0845 0.0000
STS 2016 0.4757 0.3768 0.2592 0.1067 0.5052 0.4865 -0.0100

Table 2: Pearson Correlation. Performance comparison across input representations and composition
models. Baseline method uses cosine similarity measure to predict similarity between sentences.

Dataset Vocab Size
% OOEV

word2vec GloVe

SICK 2014 2,423 1.4 1.1
PIT 2015 15,156 16.5 9.6
STS 2016 18,061 11.1 7.3

Table 3: Percentage of Out-of-Embedding-
Vocabulary (OOEV) words

PIT 2015 (Xu et al., 2015), using Pearson correla-
tion. We assert that a robust model should perform
consistently well in these three datasets.

Furthermore, using the framework described in
Section 2.1, we chose to compare the model per-
formance at two levels of input representation
(i.e., character-level vs word-level) and composi-
tion models (i.e., LSTM vs vector sum) in order
to eliminate the need for external tools such as
parsers.

4 Results and Discussion

Table 2 shows the performance across input repre-
sentations and composition models. As expected,
our simplified model performs relatively worse
(Pearson correlation = 0.7355) when compared to
what was reported in the original MaLSTM pa-
per (Pearson correlation = 0.8822) on the SICK
dataset (using word2vec). This performance dif-
ference (around 15%) could be attributed to the
additional features (see Table 1) that the state-of-
the-art model added to their system.

With respect to input representation, the word-
based one yields better performance in all datasets
over character-level representation for the obvi-
ous reason that it carries more semantic infor-
mation. Furthermore, the character-level repre-
sentation using LSTM performs better than us-
ing Vector Sum (VS) because it is able to retain
sequential information. Regarding word embed-
dings, GloVe resulted in higher performance com-

pared to word2vec in all datasets and models ex-
cept with VS on the SICK dataset where word2vec
is slightly better. Table 3 shows the percentage
of OOEV words in each dataset with respect to
its vocabulary size. Upon closer inspection, we
found out that word2vec does not have embed-
dings for stopwords (e.g., a, to, of, and). With re-
spect to token-based statistics, these OOEVs com-
prised 95% (SICK), 67% (PIT) and 44% (STS) re-
spectively in each dataset. Although further work
is needed to ascertain the effect of this type of OO-
EVs, we hypothesise that GloVe’s superior perfor-
mance could be attributed to it, if not to its word
vector quality as claimed by Pennington et al.
(2014).

-30 -10 10 30 50 70 90

Answer-Answer

Headlines

Plagiarism

Post-editing

Question-Question

Overall

Pearson correlation x 100

C
at

eg
or

y

L+G

L+W

L+C

V+G

V+W

V+C

Figure 1: Pearson correlation in STS 2016 evalua-
tion sets (Key: L=LSTM, V=Vector Sum, C=char,
W=word2vec, G=GloVe)

131



Dataset Sentence Pair Gold Label Vector Sum MaLSTM

SICK
A person is scrubbing a zucchini
The woman is cutting cooked octopus

0.046 0.523 0.694

STS Question-
Question

What kind of socket is this ?
What kind of bug is this ?

0.000 0.668 0.778

STS Answer-
Answer

You should do it
You should never do it

0.200 0.818 0.900

PIT
That s what were watching in Europe
If only England was in Europe

0.000 0.566 0.519

Table 4: Examples of difficult sentence pairs. Compositional models use GloVe embeddings.

With respect to the composition model, LSTM
performs better than VS but only in the SICK
dataset while VS dominates in both the PIT and
STS datasets. Specifically, Figure 1 shows the
overall and the per-category performance of the
model on the STS dataset. Overall, we can clearly
see that VS outperforms LSTM by a considerable
margin and also in each category except in Post-
editing and Headlines. On the one hand, this sug-
gests that simple compositional models can per-
form competitively on clean and noisy datasets
(e.g., less OOEVs). On the other hand, this shows
the ability of LSTM models to capture long term
dependencies especially on clean datasets (e.g.,
SICK dataset) because they contain sufficient se-
mantic information while their performance de-
creases dramatically on noisy data or on datasets
with high proportion of OOEVs (e.g., PIT and STS
datasets).

The worst performance was obtained on the
PIT dataset in both the baseline6 and composi-
tion models. Aside from PIT dataset’s compara-
tively higher percentage of OOEV words (see Ta-
ble 3), its diverse, short and noisy user-generated
text (Strauss et al., 2016)—typical of social media
text—make it a very challenging dataset.

To better understand the reason behind the per-
formance drop of the model, we extracted the 100
most difficult sentence pairs in each dataset by
ranking all of the pairs in the test set according to
the absolute difference between the gold standard
and predicted similarity scores.

We observed that around 60% of the difficult
sentence pairs share many similar words (except
for a word or two) or contain OOEV words that led
to a complete change in meaning. Meanwhile the

6We represent each sentence with term-frequency vectors.

rest are sentence pairs which are topically similar
but completely mean different.

In Table 4, we show examples from each dataset
and their corresponding scores (i.e., Pearson cor-
relation) from the gold standard and the compo-
sition models. The two sentences come from an
actual pair in the dataset.

Example 1 (from SICK dataset) shows a pair
of sentences which, although can be interpreted to
come from the same domain food preparation, are
semantically different in their verb, subject, and
direct object, for which, presumably, they were
labelled in the gold standard as highly dissimi-
lar. However, both of the word-based models pre-
dicted them to be highly similar (in varying de-
grees). This limitation can be attributed to the re-
latedness of their words (e.g., person vs woman,
cutting vs scrubbing). Under the distributional hy-
pothesis assumption (Harris, 1940; Firth, 1957),
two words will have high similarity if they oc-
cur in similar contexts even if they neither have
the same nor similar meanings. Since word em-
beddings are typically generated based on this as-
sumption, the relatedness aspect is captured more
than genuine similarity. Furthermore, the higher
similarity obtained by the LSTM model over Vec-
tor Sum can be attributed to its ability to capture
syntactic structure in sequences such as sentences.

Examples 2 and 3 (from STS dataset) show sen-
tence pairs which were labelled as completely dis-
similar but were predicted with high similarity in
both models. This shows the inability of the mod-
els to put more weight on semantically rich words
which change the overall meaning of a sentence
when compared with another.

Example 4 (from PIT dataset) shows a sentence
pair which was labelled as completely dissimi-

132



lar, presumably because it lacks sufficient con-
text for meaningful interpretation. However, they
were predicted to some degree as similar possi-
bly because some words are common to both sen-
tences and some are likely related by virtue of co-
occurrence in the same context (e.g., England, Eu-
rope). See Appendix B for more examples.

5 Future Work

This work is intended to serve as an initial study
on end-to-end sentence modelling to identify the
limitations associated with it. The models and
representations compared, while typical of current
sentence modelling methods, are not an exhaustive
set and some variations exist. A natural extension
to this study is to explore other input granularity
representations and composition models presented
in the framework. For example, in this study we
did not go beyond word representations; however,
multi-word expressions are common occurrences
in the English language. This could be addressed
by modelling sentence constituents using recur-
sive tree structures (Tai et al., 2015) or by learning
phrase representations (Wieting et al., 2015).

The limitations of the current word embeddings
as revealed in this paper has been studied in the
context of word similarity tasks (Levy and Gold-
berg, 2014; Hill et al., 2016) but to our knowl-
edge had never been investigated explicitly in the
context of sentence similarity tasks. For exam-
ple, Kiela et al. (2015) have shown that specialis-
ing semantic spaces to downstream tasks and ap-
plications requiring similarity or relatedness can
improve performance. Furthermore, some studies
(Faruqui et al., 2014; Yu and Dredze, 2014; Ono
et al., 2015; Ettinger et al., 2016) have proposed to
learn word embeddings by going beyond the dis-
tributional hypothesis assumption either through a
retrofitting or joint-learning process with some us-
ing semantic resources such as ontologies and en-
tity relation databases. Thus, we will explore this
direction as this will be particularly important in
semantic processing since entities encode much of
the semantic information in a language.

Furthermore, the inability of the state-of-the-
art model to encode semantically rich words (e.g.,
socket, bug in Example 2) with higher weights rel-
ative to other words, supports the assertion of Bla-
coe and Lapata (2012) that distributive semantic
representation and composition must be mutually
learned. Wieting et al. (2015) have showed that

this kind of weighting for semantic importance can
be learned automatically when training on a para-
phrase database. Recent models (Hashimoto et al.,
2016) proposed end-to-end joint modelling at dif-
ferent linguistic levels of a sentence (i.e. morphol-
ogy, syntax, semantics) on a hierarchy of tasks
(i.e., POS tagging, dependency parsing, seman-
tic role labelling)—often done separately—with
the assumption that higher-level tasks benefit from
lower-level ones.

References
Naveed Afzal, Yanshan Wang, and Hongfang Liu.

2016. Mayonlp at semeval-2016 task 1: Semantic
textual similarity based on lexical semantic net and
deep learning semantic model. Proceedings of Se-
mEval pages 674–679.

Eneko Agirre, Carmen Banea, Daniel Cer, Mona Diab,
Aitor Gonzalez-Agirre, Rada Mihalcea, and Janyce
Wiebe. 2016a. Semeval-2016 task 1: Semantic tex-
tual similarity, monolingual and cross-lingual eval-
uation. In Proceedings of the 10th International
Workshop on Semantic Evaluation. pages 509–523.

Eneko Agirre, Aitor Gonzalez-Agirre, Inigo Lopez-
Gazpio, Montse Maritxalar, German Rigau, and Lar-
raitz Uria. 2016b. Semeval-2016 task 2: Inter-
pretable semantic textual similarity. Proceedings of
SemEval pages 512–524.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
learning to align and translate. arXiv preprint
arXiv:1409.0473 .

Rajendra Banjade, Nabin Maharjan, Nobal B Niraula,
and Vasile Rus. 2016. Dtsim at semeval-2016 task 2:
Interpreting similarity of texts based on automated
chunking, chunk alignment and semantic relation
prediction. Proceedings of SemEval pages 809–813.

Marco Baroni, Georgiana Dinu, and Germán
Kruszewski. 2014. Don’t count, predict! a
systematic comparison of context-counting vs.
context-predicting semantic vectors. In ACL (1).
pages 238–247.

Dario Bertero and Pascale Fung. 2015. Hltc-hkust:
A neural network paraphrase classifier using trans-
lation metrics, semantic roles and lexical similarity
features. Proceedings of SemEval .

Ergun Biçici. 2015. Rtm-dcu: Predicting seman-
tic similarity with referential translation machines.
SemEval-2015 page 56.

William Blacoe and Mirella Lapata. 2012. A com-
parison of vector-based representations for semantic
composition. In Proceedings of the 2012 Joint Con-
ference on Empirical Methods in Natural Language

133



Processing and Computational Natural Language
Learning. Association for Computational Linguis-
tics, pages 546–556.

Tomáš Brychcın and Lukáš Svoboda. 2016. Uwb at
semeval-2016 task 1: Semantic textual similarity
using lexical, syntactic, and semantic information.
Proceedings of SemEval pages 588–594.

Jianpeng Cheng and Dimitri Kartsaklis. 2015. Syntax-
aware multi-sense word embeddings for deep com-
positional models of meaning. arXiv preprint
arXiv:1508.02354 http://arxiv.org/abs/1508.02354.

Jason PC Chiu and Eric Nichols. 2016. Named
entity recognition with bidirectional LSTM-
CNNs. Transactions of the Association
for Computational Linguistics 4:357–370.
https://transacl.org/ojs/index.php/tacl/article/view/792.

Bill Dolan, Chris Quirk, and Chris Brockett. 2004.
Unsupervised construction of large paraphrase cor-
pora: Exploiting massively parallel news sources. In
Proceedings of the 20th international conference on
Computational Linguistics. Association for Compu-
tational Linguistics, page 350.

Cı́cero Nogueira dos Santos and Bianca Zadrozny.
2014. Learning character-level representations for
part-of-speech tagging. In ICML. pages 1818–1826.

Allyson Ettinger, Philip Resnik, and Marine Carpuat.
2016. Retrofitting sense-specific word vectors using
parallel text. In Proceedings of NAACL-HLT . pages
1378–1383.

Asli Eyecioglu and Bill Keller. 2015. ASOBEK: Twit-
ter paraphrase identification with simple overlap fea-
tures and SVMs. Proceedings of SemEval .

Manaal Faruqui, Jesse Dodge, Sujay K Jauhar, Chris
Dyer, Eduard Hovy, and Noah A Smith. 2014.
Retrofitting word vectors to semantic lexicons.
arXiv preprint arXiv:1411.4166 .

John Rupert Firth. 1957. Papers in linguistics, 1934-
1951. Oxford University Press.

Gottlob Frege. 1892. Ubersicht und bedeutung.
Journal of Philosophy and Philosophical Criticism
100:25–50.

Z. S. Harris. 1940. Review of Louis H. Gray, Foun-
dations of Language (New York: Macmillan, 1939).
Language 16(3):216–231.

Kazuma Hashimoto, Caiming Xiong, Yoshimasa Tsu-
ruoka, and Richard Socher. 2016. A Joint Many-
Task Model: Growing a Neural Network for Mul-
tiple NLP Tasks. arXiv preprint arXiv:1611.01587
https://arxiv.org/abs/1611.01587.

Hua He, Kevin Gimpel, and Jimmy Lin. 2015. Multi-
perspective sentence similarity modeling with con-
volutional neural networks. In Proceedings of the
2015 Conference on Empirical Methods in Natural
Language Processing. pages 1576–1586.

Hua He and Jimmy Lin. 2016. Pairwise word inter-
action modeling with deep neural networks for se-
mantic similarity measurement. In Proceedings of
NAACL-HLT . pages 937–948.

Hua He, John Wieting, Kevin Gimpel, Jinfeng Rao, and
Jimmy Lin. 2016. Umd-ttic-uw at semeval-2016
task 1: Attention-based multi-perspective convolu-
tional neural networks for textual similarity mea-
surement .

Felix Hill, Roi Reichart, and Anna Korhonen. 2016.
Simlex-999: Evaluating semantic models with (gen-
uine) similarity estimation. Computational Linguis-
tics .

Geoffrey E Hinton. 1984. Distributed representations .

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation
9(8):1735–1780.

Yangfeng Ji and Jacob Eisenstein. 2013. Dis-
criminative Improvements to Distributional Sen-
tence Similarity. In EMNLP. pages 891–896.
http://jiyfeng.github.io/papers/ji-emnlp-2013.pdf.

Douwe Kiela, Felix Hill, and Stephen Clark. 2015.
Specializing word embeddings for similarity or re-
latedness. In EMNLP. pages 2044–2048.

Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980 .

Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,
Richard Zemel, Raquel Urtasun, Antonio Torralba,
and Sanja Fidler. 2015. Skip-thought vectors. In
Advances in neural information processing systems.
pages 3294–3302.

Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick
Haffner. 1998. Gradient-based learning applied to
document recognition. Proceedings of the IEEE
86(11):2278–2324.

Omer Levy and Yoav Goldberg. 2014. Dependency-
based word embeddings. In ACL (2). pages 302–
308.

Omer Levy, Yoav Goldberg, and Ido Dagan. 2015. Im-
proving distributional similarity with lessons learned
from word embeddings. Transactions of the Associ-
ation for Computational Linguistics 3:211–225.

Simone Magnolini, Anna Feltracco, and Bernardo
Magnini. 2016. Fbk-hlt-nlp at semeval-2016 task
2: A multitask, deep learning approach for inter-
pretable semantic textual similarity. Proceedings of
SemEval pages 783–789.

Marco Marelli, Stefano Menini, Marco Baroni, Luisa
Bentivogli, Raffaella Bernardi, and Roberto Zam-
parelli. 2014. A sick cure for the evaluation of com-
positional distributional semantic models. In LREC.
pages 216–223.

134



Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013a. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781 .

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in neural information processing
systems. pages 3111–3119.

Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In ACL. pages
236–244.

Jonas Mueller and Aditya Thyagarajan. 2016. Siamese
recurrent architectures for learning sentence similar-
ity. In AAAI. pages 2786–2792.

Masataka Ono, Makoto Miwa, and Yutaka Sasaki.
2015. Word embedding-based antonym detection
using thesauri and distributional information. In
HLT-NAACL. pages 984–989.

Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. Glove: Global vectors for word
representation. In EMNLP. volume 14, pages 1532–
43.

Piotr Przybyła, Nhung TH Nguyen, Matthew Shard-
low, Georgios Kontonatsios, and Sophia Ananiadou.
2016. Nactem at semeval-2016 task 1: Inferring
sentence-level semantic similarity from an ensem-
ble of complementary lexical and sentence-level fea-
tures. Proceedings of SemEval pages 614–620.

Barbara Rychalska, Katarzyna Pakulska, Krystyna
Chodorowska, Wojciech Walczak, and Piotr An-
druszkiewicz. 2016. Samsung poland nlp team at
semeval-2016 task 1: Necessity for diversity; com-
bining recursive autoencoders, wordnet and ensem-
ble methods to measure semantic similarity. Pro-
ceedings of SemEval pages 602–608.

Richard Socher, Eric H Huang, Jeffrey Pennin, Christo-
pher D Manning, and Andrew Y Ng. 2011. Dy-
namic pooling and unfolding recursive autoencoders
for paraphrase detection. In Advances in Neural In-
formation Processing Systems. pages 801–809.

Benjamin Strauss, Bethany E Toma, Alan Ritter,
Marie-Catherine de Marneffe, and Wei Xu. 2016.
Results of the wnut16 named entity recognition
shared task. In Proceedings of the 2nd Workshop
on Noisy User-generated Text. pages 138–144.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in neural information process-
ing systems. pages 3104–3112.

Kai Sheng Tai, Richard Socher, and Christopher D
Manning. 2015. Improved semantic representations
from tree-structured long short-term memory net-
works. arXiv preprint arXiv:1503.00075 .

Peter D Turney, Patrick Pantel, et al. 2010. From
frequency to meaning: Vector space models of se-
mantics. Journal of artificial intelligence research
37(1):141–188.

Soroush Vosoughi, Prashanth Vijayaraghavan, and
Deb Roy. 2016. Tweet2vec: Learning Tweet
Embeddings Using Character-level CNN-LSTM
Encoder-Decoder. ACM Press, pages 1041–1044.
https://doi.org/10.1145/2911451.2914762.

John Wieting, Mohit Bansal, Kevin Gimpel, and
Karen Livescu. 2015. Towards universal para-
phrastic sentence embeddings. arXiv preprint
arXiv:1511.08198 .

John Wieting, Mohit Bansal, Kevin Gimpel, and Karen
Livescu. 2016. Charagram: Embedding words and
sentences via character n-grams. arXiv preprint
arXiv:1607.02789 .

Wei Xu, Chris Callison-Burch, and William B Dolan.
2015. Semeval-2015 task 1: Paraphrase and seman-
tic similarity in twitter (pit). Proceedings of Se-
mEval .

Wenpeng Yin and Hinrich Schutze. 2016. Dis-
criminative Phrase Embedding for Paraphrase
Identification. arXiv preprint arXiv:1604.00503
http://arxiv.org/abs/1604.00503.

Wenpeng Yin and Hinrich Schütze. 2016. Why and
how to pay different attention to phrase align-
ments of different intensities. arXiv preprint
arXiv:1604.06896 .

Wenpeng Yin, Hinrich Schütze, Bing Xiang, and
Bowen Zhou. 2015. Abcnn: Attention-based convo-
lutional neural network for modeling sentence pairs.
arXiv preprint arXiv:1512.05193 .

Mo Yu and Mark Dredze. 2014. Improving lexical
embeddings with semantic knowledge. In ACL (2).
pages 545–550.

Guido Zarrella, John Henderson, Elizabeth M.
Merkhofer, and Laura Strickhart. 2015. MITRE:
Seven systems for semantic similarity in tweets.
Proceedings of SemEval .

Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015.
Character-level convolutional networks for text clas-
sification. In Advances in Neural Information Pro-
cessing Systems. pages 649–657.

Jiang Zhao, Tian Tian Zhu, and Man Lan. 2014. Ecnu:
One stone two birds: Ensemble of heterogenous
measures for semantic relatedness and textual entail-
ment. Proceedings of the SemEval pages 271–277.

Yao Zhou, Cong Liu, and Yan Pan. 2016. Mod-
elling Sentence Pairs with Tree-structured Atten-
tive Encoder. arXiv preprint arXiv:1610.02806
https://arxiv.org/abs/1610.02806.

135


	An Empirical Study on End-to-End Sentence Modelling

