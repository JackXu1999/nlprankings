















































Exploring self training for Hindi dependency parsing


Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 1452–1456,
Chiang Mai, Thailand, November 8 – 13, 2011. c©2011 AFNLP

Exploring self training for Hindi dependency parsing 

 
    Rahul Goutam           Bharat Ram Ambati 

Language Technologies Research Centre   Language Technologies Research Centre 

            IIIT-Hyderabad, India     IIIT-Hyderabad, India 

   rahul.goutam@research.iiit.ac.in             ambati@research.iiit.ac.in  

  

 

  

Abstract 

In this paper we explore the effect of self-

training on Hindi dependency parsing. We 

consider a state-of-the-art Hindi dependency 

parser and apply self-training by using a large 

raw corpus. We consider two types of raw 
corpus, one from same domain as of training 

and testing data and the other from different 

domain. We also do an experiment, where we 

add small gold-standard data to the training 

set. Comparing these experiments, we show 

the impact of adding small, but gold-standard 

data to training data versus large, but automat-

ically parsed data on Hindi parser.  

1 Introduction 

Parsing morphologically rich free-word-order 
languages like Czech, Hindi, Turkish, etc., is a 

challenging task. Unlike English, most of the 

parsers for such languages have adopted the de-
pendency grammatical framework. It is well 

known that for these languages, dependency 

framework is better suited (Shieber, 1985; 

Mel’čuk, 1988, Bharati et al., 1995). Due to the 
availability of annotated corpora in recent years, 

data driven dependency parsing has achieved 

considerable success. In spite of availability of 
annotated treebanks, state-of-the-art parsers for 

these languages have not reached the perfor-

mance obtained for English (Nivre et al., 2007a). 
Frequently stated reasons for low performance 

are small treebank size, complex linguistic phe-

nomenon, long distance dependencies, and non-

projective structures (Nivre et al., 2007a; Nivre 
et al., 2007b; Bharati et al., 2008). 

In this paper, we try to address the problem of 

small treebank size. We have lots of un-
annotated data. One way to increase treebanks’ 

size is to manually annotate this data. But it is 

very time consuming task. Other way is to auto-

matically parse this data and consider highly re-
liable parses. But, what criteria should be used 

for extracting reliable parses is a really challeng-

ing task. In this paper, we explore a bootstrap-

ping technique called self training and see its 

impact on dependency parsing accuracy. We 
consider a state-of-the-art Hindi dependency 

parser and analyze its performance using self-

training. We consider two types of raw corpus, 
one from the same domain as of training and 

testing data and the other from a different do-

main. We also show the impact of adding small, 
but gold-standard data to training data versus 

large, but automatically parsed data on Hindi 

dependency parsing. 

The paper is arranged as follows. In section 2, 
we describe the related work in this field. In sec-

tion 3, we present the current state-of-the-art of 

Hindi dependency parser. Section 3, talks about 
different experiments conducted and presents the 

results. We conclude with possible future work 

in section 4. 

2 Related Work 

In this section, we briefly describe major works 

on bootstrapping using statistical parsers. 
Steedman et al. (2003) did experiments to 

show that raw data can be used to improve the 

performance of statistical parsers by bootstrap-

ping. Although their main focus was on co-
training between two statistical parsers, they 

have also performed self-training for each parser 

but the results are not that promising with self-
training. They have also done cross-genre expe-

riments to show that co-training is beneficial 

even when the seed set was from a different do-
main compared to the raw data.  

Reichart and Rappoport (2007) also perform 

similar cross-genre experiments to improve the 

quality of their parser and to adapt the parser to a 
different domain. They have also reported signif-

icant reduction in annotation cost and amount of 

work because only small amount of manually 
annotated seed data was used. 

1452



McClosky et al. (2006) used a two phase pars-

er-reranker system for self-training using readily 

available raw data. In their approach, instead of 

adding the raw data in steps, they have added the 
entire data in one go. They have reported signifi-

cant improvement in accuracy over the previous 

state-of-the-art accuracy for Wall Street Journal 
parsing. They have also performed sentence 

length analysis to show that there is a general 

improvement in intermediate-length sentences, 
but no improvement at the extremes. 

All the above mentioned works are on phrase 

structure parsing of English. There is an attempt 

at exploring usefulness of large raw corpus for 
dependency parsing by Chen et al. (2008). They 

could achieve considerable improvement over 

baseline for Chinese using only high confident 
edges instead of entire sentences. In our work the 

focus is dependency parsing of Hindi. We also 

explore how domain and quality of data affects 
the parser performance.  

3 Hindi Dependency Parsing 

In ICON 2009 and 2010, two tools contests were 
held that focused on Indian Language dependen-

cy parsing (Husain, 2009; Husain et al., 2010). In 

these contests, rule-based, constraint based, sta-

tistical and hybrid approaches were explored to-
wards building dependency parsers for Hindi. In 

2009 contest, given the gold standard chunk 

heads, the task was to find dependencies between 
them. But in 2010 contest, given words with gold 

features like part-of-speech (POS) and morph 

information, the task was to find word level de-
pendency parse. Table 1, gives the basic statistics 

about the data. 
 

Type Sentences Words Average Sen-

tence Length 
Training 2,972 64632 22.69 

Development 543 12617 23.28 

Testing 320 6589 20.59 
 

Table 1: Hindi ICON 2010 data statistics 

3.1 Baseline (State-of-the-art) System 

We consider the best system (Kosaraju et al., 

2010) in ICON 2010 tools contest as the starting 
point. Kosaraju et al. (2010) used MaltParser 

(Nivre et al., 2007b) and achieved 94.5% Unla-

beled Attachment Score (UAS) and 88.6% La-

beled Attachment Score (LAS). They could 
achieve this using liblinear learner and nivres-

tandard parsing algorithm. But, as mentioned 

above, POS and other features used in this sys-

tem were gold standard. The only available sys-

tem which uses automatically extracted features 

and does complete word level parsing for Hindi 
is Ambati et al. (2010). Though both Ambati et 

al. (2010) and Kosaraju et al. (2010) used Malt-

Parser, the data used is the subset of the one used 
by the latter and the parser settings were slightly 

different.  

Taking training data and parser settings of Ko-
saraju et al. (2010) and automatic features similar 

to Ambati et al. (2010), we developed a parser 

and evaluated it on the ICON 2010 tools contest 

test data. We could achieve LAS of 77.9% and 
UAS of 86.5% on test set. This is the state-of-

the-art system for Hindi dependency paring using 

automatic features. We consider this system as 
our baseline and try to explore self-training tech-

nique. 

 
System UAS LAS LS 

1)  Ambati et. al. (2010);  

     automatic features  

85.5% 75.4% 78.9% 

2)  Kosaraju et. al. (2010);  

     gold features 

94.5% 88.6% 90.0% 

    

3)  Kosaraju et. al. (2010) + 

     automatic features  

86.5% 77.9% 81.7% 

 

Table 2: Comparison of different systems 

4 Experiments and Analysis 

We have modified the Malt parser used in base-
line system so that it gives a confidence value for 

each arc-decision taken. We have taken the aver-

age confidence value for all the arcs in the sen-
tence to be the confidence value of a sentence. 

This system was first trained on ICON 2010 

tools contest training data for Hindi. The model 
generated was then used to parse the large raw 

corpus. The output sentences were then sorted in 

descending order based on their scores. 

In the self-training experiments, in each itera-
tion, we have added 1000 sentences from the 

sorted output generated above, to the training 

data and re-trained the parser. The resulting 
model was then used to parse the test data.  

Hindi data released in ICON-2010 tools con-

test is a portion of large treebank (Bhatt et al., 
2009), which is under development. This is a 

news corpus taken from well-known Hindi news 

daily. Self-training experiments were performed 

using two types of data: one from the same news 
domain (in-domain) and another from a different 

domain (out-of-domain).  

1453



77.2

77.4

77.6

77.8

78

78.2

78.4

78.6

78.8

0 10 20 30 40 50

          

85.5

86

86.5

87

87.5

88

0 10 20 30 40 50

 
 

Figure 1a. Self training in-domain (LAS)  Figure 1b. Self training in-domain (UAS) 

 
 

76.6

76.8

77

77.2

77.4

77.6

77.8

78

78.2

0 10 20 30 40 50

          

85.4

85.6

85.8

86

86.2

86.4

86.6

86.8

87

0 10 20 30 40 50

 
 

      Figure 2a. Self training out-of-domain (LAS)  Figure 2b. Self training out-of-domain (UAS) 

 

 

4.1 Self training: In-domain 

We have taken raw news corpus of about 
108,000 sentences. As a first step, we have 

cleaned the data. In this process, we removed the 

repeated sentences, and very large sentences 
(>100 words per sentence). Using the above 

mentioned approach of self-training, we added 

top 1000 sentences one by one to the training 
data. Performance of the resulting system on test 

data for the first 50 iterations is shown in Figures 

1a and 1b. After 50 iterations, there was steady 

drop in the accuracy of the system. This could be 
because of less confidence values of the sen-

tences after 50th iteration. As the confidence 

values are low, major arcs in these sentences 
might be wrong. As a result, these sentences 

were giving negative impact on the parser per-

formance. There were slight fluctuations in the 
initial iteration and peeked at 23rd iteration. At 

this iteration, accuracy of 78.6% LAS and 86.9% 

UAS was achieved. With this data, we could 

achieve 0.7% and 0.4% improvement in LAS 
and UAS respectively over the baseline.  

4.2 Self training: Out-of-domain 

In this experiment, raw data of a domain differ-
ent from the actual training, and testing data was 

used for self-training. For this purpose, we have 

taken raw non-news corpus of about 700,000 
sentences. Major part of this data is from tourism 

domain. Similar to in-domain data, we first 

cleaned the data. Apart from repeated, and very 
large, there were a few non-Hindi sentences. We 

also removed them during the process of clean-

ing. Using the above mentioned approach of self-

training (see section 4), we added top 1000 sen-
tences one by one to the training data. Perfor-

mance of the resulting system on test data for the 

first 50 iterations is shown in Figures 2a and 2b. 
After 50 iterations, there was sharp drop in the 

accuracy of the system. There isn’t any im-

provement in LAS over the baseline. But in case 
of UAS after initial fluctuations, accuracy 

peeked at 17th iteration. At this iteration, accura-

cy of 77.8% LAS and 86.8% UAS was observed. 

We could achieve an improvement of 0.3% in 
UAS, but a decrement 0.1% in LAS. 

1454



   

77.6
77.8
78

78.2
78.4
78.6
78.8
79

79.2
79.4

0 2 4 6 8 10

                   

86.2

86.4

86.6

86.8

87

87.2

87.4

0 2 4 6 8 10

 
 

         Figure 3a. Gold-standard data (LAS)         Figure 3b. Gold-standard data (UAS) 

 

4.3 Gold-Standard Data 

In the previous two experiments (sections 4.1 
and 4.2), we have taken large amount of raw data 

from same and different domains and applied 

self-training technique. In both these experi-

ments, impact of large automatically annotated 
data was observed. In this experiment, we shall 

observe the impact of small but, gold-standard 

data. We have taken the development data of 
ICON 2010 tools contest.  We have divided the 

data into sets of 50 sentences and added one by 

one similar to above experiments. We could 
achieve the accuracy of 79.2% LAS and 87.2% 

UAS at the final iteration. With this gold stan-

dard data, we could achieve an improvement of 

0.7% in UAS and 1.3% in LAS. 

4.4 Analysis 

Table 3, gives the summary comparing all the 

experiments performed. “*” mark in the table 

shows that, accuracy is statistically significant 
over the baseline. Significance is calculated us-

ing McNemar’s test (p <= 0.05) made available 

with MaltEval (Nilsson and Nivre, 2008). 
 

System UAS LAS LS 
1) Baseline System 86.5% 77.9% 81.7% 

2) In-domain self-training 87.0%* 78.6%* 82.3%* 

3) Out-of-domain self-training 86.8% 77.8% 81.6% 

4) Gold-Standard Data 87.2%* 79.2%* 82.9%* 
 

Table 2. Summary of Experiments.  
 

We could achieve significant improvement in 

the accuracy when the raw data is from the same 

domain. But, when the data is from different do-
main, we haven’t seen any significant increase in 

the performance. This clearly shows the impor-

tance of domain of the training data. One can get 

better accuracies when training data is similar to 
testing data. As expected, adding gold-standard 

data outperformed both the self-training experi-

ments. Our experiments show that gold-standard 

data is the best solution for improving the parser 

performance. When this is not possible, raw data 
from same domain seems to be a better option. 

Interesting observation is that even when gold 

data is being added, there isn’t steady increase. 

Slight drop was observed when some sentences 
are added. This clearly shows that nature of the 

sentences being added to training data is very 

important. Currently, criterion being used to ex-
tract reliable sentences from automatically 

parsed ones is average confidence score given by 

the parser. We are considering all the nodes in 
the sentences for calculating confidence score of 

sentence. It was shown by Ambati et al. (2010) 

that accuracy for intra-chunk dependencies is 

pretty high and that of inter-chunk dependencies 
is low. We can explore considering sentences 

with high confidence scores for inter-chunk 

nodes, rather than average score considering all 
the nodes. We can also explore considering only 

high confidence nodes rather than entire sen-

tence, similar to works of Chen et al. (2008) and 
Mannem and Dara (2011). 

5 Conclusions and Future Work 

We explored the effect of self-training on Hindi 
dependency parsing. We showed the impact of 

adding small, but gold-standard data to training 

data versus large, but automatically parsed data 
on Hindi dependency parsing.  

We are planning to explore the importance of 

co-training technique also using another parser 

like MSTParser, as the parser can learn new in-
formation in case of co-training. We did experi-

ments on Hindi. There are several other languag-

es like Telugu, Bangla etc. for which annotated 
data is less but large amount of raw corpus is 

available. We are also planning to explore the 

importance of self-training and co-training tech-

niques for parsing these languages. 

 

1455



References 

B. R. Ambati, M. Gupta, S. Husain and D. M. Shar-

ma. 2010. A high recall error identification tool for 

Hindi treebank validation. In Proceedings of The 

7th International Conference on Language Re-

sources and Evaluation (LREC), Valleta, Malta. 

A. Bharati, V. Chaitanya and R. Sangal. 1995. Natu-

ral Language Processing: A Paninian Perspective, 

Prentice-Hall of India, New Delhi. 

A. Bharati, S. Husain, B. Ambati, S. Jain, D. Sharma, 

and R. Sangal. 2008. Two semantic features make 

all the difference in parsing accuracy. In Proceed-

ings of ICON-08. 

R. Bhatt, B. Narasimhan, M. Palmer, O. Rambow, D. 

M.Sharma and F.Xia. 2009. Multi-Representational 

and Multi-Layered Treebank for Hindi/Urdu. In 

Proceedings of the Third Linguistic Annotation 

Workshop at 47th ACL and 4th IJCNLP. 

W. Chen, Y. Wu, and H. Isahara. 2008. Learning reli-

able information for dependency parsing adapta-

tion. In Proceedings of the 22nd International Con-

ference on Computational Linguistics (COLING). 

S. Husain. 2009. Dependency Parsers for Indian Lan-

guages. In Proceedings of ICON09 NLP Tools 

Contest: Indian Language Dependency Parsing. 

Hyderabad, India. 

S. Husain, P. Mannem, B. Ambati and P. Gadde. 

2010. The ICON-2010 Tools Contest on Indian 

Language Dependency Parsing. In Proceedings of 
ICON-2010 Tools Contest on Indian Language 

Dependency Parsing. Kharagpur, India. 

P. Kosaraju, S. R. Kesidi, V. B. R. Ainavolu and P. 

Kukkadapu. 2010. Experiments on Indian Lan-

guage Dependency Parsing. In Proceedings of the 

ICON10 NLP Tools Contest: Indian Language De-

pendency Parsing. 

P. Mannem and A. Dara. 2011. Partial Parsing from 

Bitext Projections. Accepted at the 49th Annual 

Meeting of the Association of Computational Lin-

guistics. 

D. McClosky, E. Charniak and M. Johnson. 2006. 
Effective Self-Training for Parsing. In Proceedings 

of the main conference on Human Language Tech-

nology Conference of the North American Chapter 

of the Association of Computational Linguistics. 

R. McDonald, K. Lerman, and F. Pereira. 2006. Mul-

tilingual dependency analysis with a two-stage dis-

criminative parser. In Proceedings of the Tenth 

Conference on Computational Natural Language 

Learning (CoNLL-X), pp. 216–220. 

I. A. Mel’čuk. 1988. Dependency Syntax: Theory and 

Practice, State University Press of New York. 

J. Nilsson and J. Nivre. 2008. Malteval:An evaluation 

and visualization tool for dependency parsing. In 

Proceedings of the Sixth LREC, Marrakech, Mo-

rocco.  

J. Nivre, J. Hall, S. Kubler, R. McDonald, J. Nilsson, 

S. Riedel and D. Yuret. 2007a. The CoNLL 2007 

Shared Task on Dependency Parsing. In Proceed-

ings of EMNLP/CoNLL-2007. 

J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryigit, S. 

Kübler, S. Marinov and E Marsi. 2007b. MaltPars-

er: A language-independent system for data-driven 
dependency parsing. Natural Language Engineer-

ing, 13(2), 95-135. 

R. Reichart, and A. Rappoport. 2007. Self-Training 

for Enhancement and Domain Adaptation of Statis-

tical Parsers Trained on Small Datasets. In Pro-

ceedings of the 45th Annual Meeting of the Associ-

ation of Computational Linguistics. 

S. M. Shieber. 1985. Evidence against the context-

freeness of natural language. In Linguistics and 

Philosophy, p. 8, 334–343. 

M. Steedman, M. Osborne, A. Sarkar, S. Clark, R. 
Hwa, J. Hockenmaier, P. Ruhlen, S. Baker and J. 

Crim. 2003. Bootstrapping Statistical Parsers from 

Small Datasets. In Proceedings of the tenth confe-

rence on European chapter of the Association for 

Computational Linguistics – Volume 1. 

1456


