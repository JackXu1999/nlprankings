



















































Distributional Learning as a Theory of Language Acquisition


Proc. of 5th Workshop on Cognitive Aspects of Computational Language Learning (CogACLL) @ EACL 2014, page 29,
Gothenburg, Sweden, April 26 2014. c©2014 Association for Computational Linguistics

Distributional Learning as a Theory of Language Acquisition
(Extended Abstract)

Alexander Clark
Department of Philosophy
King’s College, London

Strand, London
alexander.clark@kcl.ac.uk

1 Abstract

In recent years, a theory of distributional learning
of phrase structure grammars has been developed
starting with the simple algorithm presented in
(Clark and Eyraud, 2007). These ideas are based
on the classic ideas of American structuralist lin-
guistics (Wells, 1947; Harris, 1954). Since that
initial paper, the algorithms have been extended to
large classes of grammars, notably to the class of
Multiple Context-Free grammars by (Yoshinaka,
2011).

In this talk we will sketch a theory of language
acquisition based on these techniques, and con-
trast it with other proposals, such as the semantic
bootstrapping and parameter setting models. This
proposal is based on three recent results: first, a
weak learning result for a class of languages that
plausibly includes all natural languages (Clark and
Yoshinaka, 2013), secondly, a strong learning re-
sult for some context-free grammars, that includes
a general strategy for converting weak learners to
strong learners (Clark, 2013a), and finally a theo-
retical result that all minimal grammars for a lan-
guage will have distributionally definable syntac-
tic categories (Clark, 2013b). We argue that we
now have all of the pieces for a complete and ex-
planatory theory of language acquisition based on
distributional learning and sketch some of the non-
trivial predictions of this theory about the syntax
and syntax-semantics interface.

2 Biography

Alexander Clark is a Lecturer in Logic and Lin-
guistics in the Department of Philosophy at King’s
College London; before that he taught for sev-
eral years in the Computer Science department of
Royal Holloway, University of London. His first
degree was in Mathematics from the University
of Cambridge, and his Ph.D. is from the Univer-
sity of Sussex. He did postdoctoral research at the

University of Geneva. He is currently President
of SIGNLL and chair of the steering committee of
the International Conference on Grammatical In-
ference. His research is on unsupervised learn-
ing in computational linguistics, grammatical in-
ference, and theoretical and mathematical linguis-
tics.

References
Alexander Clark and Rémi Eyraud. 2007. Polynomial

identification in the limit of substitutable context-
free languages. Journal of Machine Learning Re-
search, 8:1725–1745, August.

Alexander Clark and Ryo Yoshinaka. 2013. Distri-
butional learning of parallel multiple context-free
grammars. Machine Learning, pages 1–27.

Alexander Clark. 2013a. Learning trees from strings:
A strong learning algorithm for some context-free
grammars. Journal of Machine Learning Research,
14:3537–3559.

Alexander Clark. 2013b. The syntactic concept lat-
tice: Another algebraic theory of the context-free
languages? Journal of Logic and Computation.

Zellig Harris. 1954. Distributional structure. Word,
10(2-3):146–62.

R. S. Wells. 1947. Immediate constituents. Language,
23(2):81–117.

R. Yoshinaka. 2011. Efficient learning of multiple
context-free languages with multidimensional sub-
stitutability from positive data. Theoretical Com-
puter Science, 412(19):1821 – 1831.

29


