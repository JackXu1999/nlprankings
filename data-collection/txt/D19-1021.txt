
























































EMNLP2019_OpenRE__camera_ready_.pdf


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 219–228,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

219

Open Relation Extraction: Relational Knowledge Transfer from
Supervised Data to Unsupervised Data

Ruidong Wu1∗ , Yuan Yao1∗, Xu Han1, Ruobing Xie2,
Zhiyuan Liu1† , Fen Lin2, Leyu Lin2, Maosong Sun1

1Department of Computer Science and Technology, Tsinghua University, Beijing, China
Institute for Artificial Intelligence, Tsinghua University, Beijing, China

State Key Lab on Intelligent Technology and Systems, Tsinghua University, Beijing, China
2Search Product Center, WeChat Search Application Department, Tencent, China

mooninsiderain@gmail.com

Abstract

Open relation extraction (OpenRE) aims to
extract relational facts from the open-domain
corpus. To this end, it discovers relation pat-
terns between named entities and then clusters
those semantically equivalent patterns into a
united relation cluster. Most OpenRE meth-
ods typically confine themselves to unsuper-
vised paradigms, without taking advantage of
existing relational facts in knowledge bases
(KBs) and their high-quality labeled instances.
To address this issue, we propose Relational
Siamese Networks (RSNs) to learn similar-
ity metrics of relations from labeled data of
pre-defined relations, and then transfer the
relational knowledge to identify novel rela-
tions in unlabeled data. Experiment results on
two real-world datasets show that our frame-
work can achieve significant improvements as
compared with other state-of-the-art methods.
Our code is available at https://github.
com/thunlp/RSN.

1 Introduction

Relation extraction (RE) aims to extract relational
facts between two entities from plain texts. For
example, with the sentence “Hayao Miyazaki is
the director of the film ‘The Wind Rises’", we can
extract a relation “director_of" between two
entities “Hayao Miyazaki" and “The Wind Rises".

Recent progress in supervised methods to RE
has achieved great successes. Supervised meth-
ods can effectively learn significant relation se-
mantic patterns based on existing labeled data,
but the data constructions are time-consuming and
human-intensive. To lower the level of super-
vision, several semi-supervised approaches have
been developed, including bootstrapping, active
learning, label propagation (Pawar et al., 2017).

∗ indicates equal contribution
† Corresponding author: Z.Liu(liuzy@tsinghua.edu.cn)

Relational
Siamese Network

Novel Relations
(Unlabeled)

Pre-defined Relations
(Auto-labeled/Labeled)

1

…

2

…

n

…

…

Testing Instances

Novel Relations
(Extracted)

n+1

…

n+2

…

n+m

…

…

clustering

…
every rectangle indicates 
an unlabeled instance of a 
novel relation

…

metric learning

every rectangle indicates 
a pre-defined relation 
instance

relational 
knowledge transfer

Figure 1: A flowchart of our framework. Our model
RSN learns from both labeled instances of pre-defined
relations and unlabeled instances of new relations, and
tries to cluster testing instances of new relations.1

Mintz (2009) also proposes distant supervision to
generate training data automatically. It assumes
that if two entities have a relation in KBs, all sen-
tences that contain these two entities will express
this relation. Still, all these approaches can only
extract pre-defined relations that have already ap-
peared either in human-annotated datasets or KBs.
It is hard for them to cover the great variety of
novel relational facts in the open-domain corpora.

Open relation extraction (OpenRE) aims to ex-
tract relational facts on the open-domain cor-
pus, where the relation types may not be pre-
defined. There are some efforts concentrating on
extracting triples with new relation types. Banko
(2008) directly extracts words or phrases in sen-
tences to represent new relation types. How-
ever, some relations cannot be explicitly repre-
sented with tokens in sentences, and it is hard
to align different relational tokens that exactly
have the same meanings. Yao (2011) consid-

1To highlight our model’s ability to extract new relations,
testing instances only contain new relations.



220

ers OpenRE as a clustering task for extracting
triples with new relation types. However, previ-
ous clustering-based OpenRE methods (Yao et al.,
2011, 2012; Marcheggiani and Titov, 2016; Elsa-
har et al., 2017) are mostly unsupervised, and can-
not effectively select meaningful relation patterns
and discard irrelevant information.

In this paper, we propose to take advantage of
high-quality supervised data of pre-defined rela-
tions for OpenRE. The approach is non-trivial,
however, due to the considerable gap between the
pre-defined relations and novel relations of inter-
est in open domain. To bridge the gap, we propose
Relational Siamese Networks (RSNs) to learn
transferable relational knowledge from supervised
data for OpenRE. Specifically, RSNs learn re-
lational similarity metrics from labeled data of
pre-defined relations, and then transfer the met-
rics to measure the similarity of unlabeled sen-
tences for open relation clustering. We describe
the flowchart of our framework in Figure 1.

Moreover, we show that RSNs can also be
generalized to various weakly-supervised scenar-
ios. We propose Semi-supervised RSN to learn
from both supervised data of pre-defined rela-
tions and unsupervised data with novel relations,
and Distantly-supervised RSN to learn from
distantly-supervised data and unsupervised data.

We conduct experiments on real-world RE
datasets, FewRel and FewRel-distant, by split-
ting relations into seen and unseen set, and eval-
uate our models in supervised, semi-supervised,
and distantly-supervised scenarios. The results
demonstrate that our models significantly outper-
form state-of-the-art baseline methods in all sce-
narios without using external linguistic tools. To
summarize, the main contributions of this work are
as follows:

(1) We develop a novel relational knowledge
transfer framework RSN for OpenRE, which can
effectively transfer existing relational knowledge
to novel-relation data and accurately identify
novel relations. To the best of our knowledge,
RSN is the first model to consider knowledge
transfer in clustering-based OpenRE task.

(2) We further propose Semi-supervised RSNs
and Distantly-supervised RSNs that can learn
from various weakly supervised scenarios. The
experimental results show that all these RSN mod-
els achieve significant improvements in F-measure
compared with state-of-the-art baselines.

2 Related Work

Open Relation Extraction. Relation extraction
(RE) is an important task in NLP. Traditional RE
methods mainly concentrate on classifying rela-
tional facts into pre-defined relation types (Mintz
et al., 2009; Yu et al., 2017). Zeng (2014) utilizes
CNN encoders to build sentence representations
with the help of position embeddings. Lin (2016)
further improves RE performance on distantly-
supervised data via instance-level attention. These
methods take advantage of supervised or distantly-
supervised data to learn neural sentence encoders
for distributed representations, and have achieved
promising results. However, these methods can-
not handle the open-ended growth of new relation
types in the open-domain corpora.

To solve this problem, recently many efforts
have been invested in exploring methods for
open relation extraction (OpenRE), which aims
to discover new relation types from unsupervised
open-domain corpora. OpenRE methods can be
roughly divided into two categories: tagging-
based and clustering-based. Tagging-based meth-
ods cast OpenRE as a sequence labeling prob-
lem, and extract relational phrases consisting of
words from sentences in unsupervised (Banko
et al., 2007; Banko and Etzioni, 2008) or super-
vised paradigms (Jia et al., 2018; Cui et al., 2018;
Stanovsky et al., 2018). However, tagging-based
methods often extract multiple overly-specific re-
lational phrases for the same relation type, and
cannot be readily utilized for downstream tasks.

In comparison, conventional clustering-based
OpenRE methods extract rich features for relation
instances via external linguistic tools, and clus-
ter semantic patterns into several relation types
(Lin and Pantel, 2001; Yao et al., 2011, 2012).
Marcheggiani (2016) proposes a reconstruction-
based model discrete-state variational autoencoder
for OpenRE via unlabeled instances. Elsahar
(2017) utilizes a clustering algorithm over lin-
guistic features. In this paper, we focus on the
clustering-based OpenRE methods, which have
the advantage of discovering highly distinguish-
able relation types.

Few-shot Learning. Few-shot learning aims to
classify instances with a handful of labeled sam-
ples. Many efforts are devoted to few-shot image
classification (Koch et al., 2015) and relation clas-
sification (Yuan et al., 2017; Han et al., 2018).
Notably, (Koch et al., 2015) introduces Convolu-



221

Twain 
was 

a 
writer 

of 
America 

FC 

distance classifier 0.7 

vl 

vr 

vd p 

max 

word 
embeddings 

position 
embeddings 

FC max 

Kenji 
was 

a 
poet 

of 
Japan 

Figure 2: The architecture of Relational Siamese Net-
works. The output is the similarity between two rela-
tional instances.

tional Siamese Neural Network for image metric
learning, which inspires us to learn relational sim-
ilarity metrics for OpenRE.

Semi-supervised Clustering. Semi-supervised
clustering aims to cluster semantic patterns given
instance seeds of target categories (Bair, 2013;
Hongtao Lin, 2019). Differently, our proposed
Semi-supervised RSN only leverages labeled in-
stances of pre-defined relations, and does not need
any seed of new relations.

3 Methodology

Our OpenRE framework mainly consists of two
modules, the relation similarity calculation mod-
ule and the relation clustering module. For rela-
tion similarity calculation, we propose Relational
Siamese Networks (RSNs), which learn to pre-
dict whether two sentences mention the same re-
lation. To utilize large-scale unsupervised data
and distantly-supervised data, we further propose
Semi-supervised RSN and Distantly-supervised
RSN. Finally, in the relation clustering module,
with the learned relation metric, we utilize hierar-
chical agglomerative clustering (HAC) and Lou-
vain clustering algorithms to cluster target relation
instances of new relation types.

3.1 Relational Siamese Network (RSN)

The architecture of our Relational Siamese Net-
works is shown in Figure 2. CNN modules encode
a pair of relational instances into vectors, and sev-
eral shared layers compute their similarity.

Sentence Encoder. We use a CNN module
as the sentence encoder. The CNN module in-
cludes an embedding layer, a convolutional layer,

a max-pooling layer, and a fully-connected (FC)
layer. The embedding layer transforms the words
in a sentence x and the positions of entities ehead
and etail into pre-trained word embeddings and
random-initialized position embeddings. Follow-
ing (Zeng et al., 2014), we concatenate these em-
beddings to form a vector sequence. Next, a
one-dimensional convolutional layer and a max-
pooling layer transform the vector sequence into
features. Finally, an FC layer with sigmoid ac-
tivation maps features into a relational vector v.
To summarize, we obtain a vector representation
v for a relational sentence with our CNN module:

v = CNN(s), (1)

in which we denote the joint information of a sen-
tence x and two entities in it ehead and etail as a
data sample s. And with paired input relational
instances, we have:

vl = CNN(sl),vr = CNN(sr), (2)

in which two CNN modules are identical and share
all the parameters.

Similarity Computation. Next, to measure the
similarity of two relational vectors, we calculate
their absolute distance and transform it into a real-
number similarity p ∈ [0, 1]. First, a distance layer
computes the element-wise absolute distance of
two vectors:

vd = |vl − vr|. (3)

Then, a classifier layer calculates a metric p for
relation similarity. The layer is a one-dimensional-
output FC layer with sigmoid activation:

p = σ(kvd + b), (4)

in which σ denotes the sigmoid function, k and b
denote the weights and bias. To summarize, we
obtain a good similarity metric p of relational in-
stances.

Cross Entropy Loss. The output of RSN p can
also be explained as the probability of two sen-
tences mentioning two different relations. Thus,
we can use binary labels q and binary cross en-
tropy loss to train our RSN:

Ll = Edl∼Dl [q ln(pθ(dl)) + (1− q) ln(1− pθ(dl))], (5)

in which θ indicates all the parameters in the RSN.



222

labeled data dl 
 

p = 0.7 

q = 0 
Cross Entropy 

Relational 
Siamese 
Network 

… 

(a) Supervised RSN

(auto)-labeled data dl 
 

q = 0 
Cross Entropy 

(+VAT) Relational 
Siamese 
Network 

… 

 
unlabeled data du 

… 

p = 0.7 

p = 0.6 

Conditional Entropy 
+VAT 

(b) Weakly-supervised RSNs

Figure 3: The comparison of (a) Supervised RSN
and (b) Weakly-supervised RSNs. Weakly-supervised
RSNs, including Semi-supervised RSN and Distantly-
supervised RSN, further learn from unlabeled data with
conditional entropy minimization and virtual adversar-
ial training (VAT). In figures, p indicates the predicted
similarity of two relational sentences, while q indicates
the ground-truth label between them.

3.2 Semi-supervised RSN

To discover relation clusters in the open-domain
corpus, it is beneficial to not only learn from la-
beled data, but also capture the manifold of unla-
beled data in the semantic space. To this end, we
need to push the decision boundaries away from
high-density areas, which is known as the cluster
assumption (Chapelle and Zien, 2005).

We try to achieve this goal with several addi-
tional loss functions. In the following paragraphs,
we denote the labeled training dataset as Dl and a
couple of labeled relational instances as dl. Sim-
ilarly, we denote the unlabeled training dataset as
Du and a couple of unlabeled instances as du.

Conditional Entropy Loss. In classification
problems, a well-classified embedding space usu-
ally reserves large margins between different clas-
sified clusters, and optimizing margin can be a
promising way to facilitate training. However, in
clustering problems, type labels are not available
during training. To optimize margin without ex-
plicit supervision, we can push the data points
away from the decision boundaries. Intuitively,
when the distance similarity p between two rela-
tional instances equals 0.5, there is a high prob-

ability that at least one of two instances is near
the decision boundary between relation clusters.
Thus, we use the conditional entropy loss (Grand-
valet and Bengio, 2005), which reaches the max-
imum when p = 0.5, to penalize close-boundary
distribution of data points:

Lu = Edu∼Du [pθ(du) ln(pθ(du))+
(1− pθ(du)) ln(1− pθ(du))]. (6)

Virtual Adversarial Loss. Despite its theo-
retical promise, conditional entropy minimization
suffers from shortcomings in practice. Due to neu-
ral networks’ strong fitting ability, a very complex
decision hyperplane might be learned so as to keep
away from all the training samples, which lacks
generalizability. As a solution, we can smooth
the relational representation space with locally-
Lipschitz constraint.

To satisfy this constraint, we introduce virtual
adversarial training (Miyato et al., 2016) on both
branches of RSN. Virtual adversarial training can
search through data point neighborhoods, and pe-
nalize most sharp changes in distance prediction.
For labeled data, we have

Lvl = Edl∼Dl [DKL(pθ(dl)||pθ(dl, t1, t2))], (7)

in which DKL indicates the Kullback-Leibler di-
vergence, pθ(dl, t1, t2) indicates a new distance es-
timation with perturbations t1 and t2 on both in-
put instances respectively. Specifically, t1 and t2
are worst-case perturbations that maximize the KL
divergence between pθ(dl) and pθ(dl, t1, t2) with a
limited length. Empirically, we approximate the
perturbations the same as the original paper (Miy-
ato et al., 2016). Specifically, we first add a ran-
dom noise to the input, and calculate the gradient
of the KL-divergence between the outputs of the
original input and the noisy input. We then add
the normalized gradient to the original input and
get the perturbed input. And for unlabeled data,
we have

Lvu = Edu∼Du [DKL(pθ(du)||pθ(du, t1, t2))], (8)

in which the perturbations t1 and t2 are added
to word embeddings rather than the words them-
selves.

To summarize, we use the following loss func-
tion to train Semi-supervised RSN, which learns
from both labeled and unlabeled data:

Lall = Ll + λvLvl + λu(Lu + λvLvu), (9)

in which λv and λu are two hyperparameters.



223

3.3 Distantly-supervised RSN

To alleviate the intensive human labor for annota-
tion, the topic of distantly-supervised learning has
attracted much attention in RE. Here, we propose
Distantly-supervised RSN, which can learn from
both distantly-supervised data and unsupervised
data for relational knowledge transfer. Specifi-
cally, we use the following loss function:

Lall = Ll + λu(Lu + λvLvu), (10)

which treats auto-labeled data as labeled data but
removes the virtual adversarial loss on the auto-
labeled data.

The reason to remove the loss is simple: vir-
tual adversarial training on auto-labeled data can
amplify the noise from false labels. Indeed, we
do find that the virtual adversarial loss on auto-
labeled data can harm our model’s performance in
experiments.

We do not use more denoising methods, since
we think RSN has some inherent advantages of
tolerating such noise. Firstly, the noise will be
overwhelmed by the large proportion of negative
sampling during training. Secondly, during clus-
tering, the prediction of a new relation cluster is
based on areas where the density of relational in-
stances is high. Outliers from noise, as a result,
will not influence the prediction process so much.

3.4 Open Relation Clustering

After RSN is learned, we can use RSN to calculate
the similarity matrix of testing instances. With this
matrix, several clustering methods can be applied
to extract new relation clusters.

Hierarchical Agglomerative Clustering. The
first clustering method we adopt is hierarchical ag-
glomerative clustering (HAC). HAC is a bottom-
up clustering algorithm. At the start, every testing
instance is regarded as a cluster. For every step, it
agglomerates two closest instances. There are sev-
eral criteria to evaluate the distance between two
clusters. Here, we adopt the complete-linkage cri-
terion, which is more robust to extreme instances.

However, there is a significant shortcoming of
HAC: it needs the exact number of clusters in ad-
vance. A potential solution is to stop agglomerat-
ing according to an empirical distance threshold,
but it is hard to determine such a threshold. This
problem leads us to consider another clustering al-
gorithm Louvain (Blondel et al., 2008).

Louvain. Louvain is a graph-based clustering
algorithm traditionally used for detecting commu-
nities. To construct the graph, we use the binary
approximation of RSN’s output, with 0 indicat-
ing an edge between two nodes. The advantage
of Louvain is that it does not need the number of
potential clusters beforehand. It will automatically
find proper sizes of clusters by optimizing commu-
nity modularity. According to the experiments we
conduct, Louvain performs better than HAC.

After running, Louvain might produce a number
of singleton clusters with few instances. It is not
proper to call these clusters new relation types, so
we label these instances the same as their closest
labeled neighbors.

Finally, we want to explain the reason why we
do not use some other common clustering methods
like K-Means, Mean-Shift and Ward’s (Ward Jr,
1963) method of HAC: these methods calculate
the centroid of several points during clustering by
merely averaging them. However, the relation vec-
tors in our model are high-dimensional, and the
distance metric described by RSN is non-linear.
Consequently, it is not proper to calculate the cen-
troid by simply averaging the vectors.

4 Experiments

In this section, we conduct several experiments on
real-world RE datasets to show the effectiveness
of our models, and give a detailed analysis to show
its advantages.

4.1 Dataset

In experiments, we use FewRel (Han et al., 2018)
as our first dataset. FewRel is a human-annotated
dataset containing 80 types of relations, each with
700 instances. An advantage of FewRel is that ev-
ery instance contains a unique entity pair, so RE
models cannot choose the easy way to memorize
the entities.

We use the original train set of FewRel, which
contains 64 relations, as labeled set with pre-
defined relations, and the original validation set of
FewRel, which contains 16 new relations, as the
unlabeled set with novel relations to extract. We
then randomly choose 1, 600 instances from the
unlabeled set as the test set, with the rest labeled
and unlabeled instances considered as the train set.

The second dataset we use is FewRel-distant,
which contains the distantly-supervised data ob-
tained by the authors of FewRel before human an-



224

notation. We follow the split of FewRel to obtain
the auto-labeled train set and unlabeled train set.
For evaluation, we use the human-annotated test
set of FewRel with 1, 600 instances. Unlabeled in-
stances already existing in this test set are removed
from the unlabeled train set of FewRel-distant. Fi-
nally, the auto-labeled train set contains 323, 549
relational instances, and the unlabeled train set
contains 60, 581 instances.

A previous OpenRE work reports performance
on an unpublic dataset called NYT-FB (Marcheg-
giani and Titov, 2016). However, it has sev-
eral shortcomings compared with FewRel-distant.
First, NTY-FB’s test set is distantly-supervised
and is noisy for instance-level RE. Moreover, in-
stances in NYT-FB often share entity pairs or re-
lational phrases, which makes it much easier for
relation clustering. Therefore, we think the re-
sults on FewRel-distant are convincing enough for
Distantly-supervised OpenRE.

4.2 Implementation Details

Data Sampling. The input of RSN should be
a pair of sampled instances. For the unlabeled
set, the only possible sampling method is to select
two instances randomly. For the labeled set, how-
ever, random selection would result in too many
different-relation pairs, and cause severe biases
for RSN. To solve this problem, we use down-
sampling. In our experiments, we fix the percent-
age of same-relation pairs in every labeled data
batch as 6%.

Let us denote this percentage number as the
sample ratio for convenience. Experimental re-
sults show that the sample ratio decides RSN’s
tendency to predict larger or smaller clusters. In
other words, it controls the granularity of the pre-
dicted relation types. This phenomenon suggests a
potential application of our model in hierarchical
relation extraction. However, we leave any serious
discussion to future work.

Hyperparameter Settings. Following (Lin
et al., 2016) and (Zeng et al., 2014), we fix the
less influencing hyperparameters for sentence en-
coding as their reported optimal values. For word
embeddings, we use pre-trained 50-dimensional
Glove (Pennington et al., 2014) word embed-
dings. For position embeddings, we use random-
initialized 5-dimensional position embeddings.
During training, all the embeddings are trainable.
For the neural network, the number of feature

maps in the convolutional layer is 230. The fil-
ter length is 3. The activation function after the
max-pooling layer is ReLU, and the activation
functions after FC layers are sigmoid. Besides,
we adopt two regularization methods in the CNN
module. We put a dropout layer right after the
embedding layer as (Miyato et al., 2016). The
dropout rate is 0.2. We also impose L2 regulariza-
tion on the convolutional layer and the FC layer,
with parameters of 0.0002 and 0.001 respectively.
Hyperparameters for virtual adversarial training
are just the same as (Miyato et al., 2016) proposed.

At the same time, major hyperparameters are
selected with grid search according to the model
performance on a validation set. Specifically, the
validation set contains 10,000 randomly chosen
sentence pairs from the unlabeled set (i.e. 16 novel
relations) and does not overlap with the test set.
The model is evaluated according to the precision
of binary classification of sentence pairs on the
validation set, which is an estimation for models’
clustering ability. We do not use F1 during model
validation because the clustering steps are time-
consuming.

For optimization, we use Adam opti-
mizer (Kingma and Ba, 2014) with a learn-
ing rate of 0.0001, which is selected from
{0.1, 0.01, 0.001, 0.0001, 0.00001}. The batch
size is 100 selected from {25, 50, 100}. For hy-
perparameters in Equation 9 and Equation 10, λv
is 1.0 selected from {0.1, 0.5, 1.0, 2.0} and λu is
0.03 selected from {0.01, 0.02, 0.03, 0.04, 0.05}.

For baseline models, original papers do grid
search for all possible hyperparameters and report
the best result during testing. We follow their set-
tings and do grid search directly on the test set.

4.3 Experiment Results on OpenRE

In this section, we demonstrate the effective-
ness of our RSN models by comparing our mod-
els with state-of-the-art clustering-based OpenRE
methods. We also conduct ablation experiments
to detailedly investigate the contributions of dif-
ferent mechanisms of Semi-supervised RSN and
Distantly-supervised RSN.

Baselines. Conventional clustering-based
OpenRE models usually cluster instances by either
clustering their linguistic features (Lin and Pantel,
2001; Yao et al., 2012; Elsahar et al., 2017) or im-
posing reconstruction constraints (Yao et al., 2011;
Marcheggiani and Titov, 2016). To demonstrate



225

the effectiveness of our RSN models, we compare
our models with two state-of-the-art models:

(1) HAC with re-weighted word embeddings
(RW-HAC) (Elsahar et al., 2017): RW-HAC is
the state-of-the-art feature clustering model for
OpenRE. The model first extracts KB types and
NER tags of entities as well as re-weighted word
embeddings from sentences, then adopts principal
component analysis (PCA) to reduce feature di-
mensionality, and finally uses HAC to cluster the
concatenation of reduced feature representations.

(2) Discrete-state variational autoencoder
(VAE) (Marcheggiani and Titov, 2016): VAE is
the state-of-the-art reconstruction-based model
for OpenRE via unlabeled instances. It optimizes
a relation classifier by reconstructing entities from
pairing entities and predicted relation types. Rich
features including entity words, context words,
trigger words, dependency paths, and context
POS tags are used to predict the relation type.

RW-HAC and VAE both rely on external lin-
guistic tools to extract rich features from plain
texts. Specifically, we first align entities to Wiki-
data and get their KB types. Next, we preprocess
the instances with part-of-speech (POS) tagging,
named-entity recognition (NER), and dependency
parsing with Stanford CoreNLP (Manning et al.,
2014). It is worth noting that these features are
only used by baseline models. Our models, in con-
trast, only use sentences and entity pairs as inputs.

Evaluation Protocol. In evaluation, we use B3
metric (Bagga and Baldwin, 1998) as the scor-
ing function. B3 metric is a standard measure
to balance the precision and recall of clustering
tasks, and is commonly used in previous OpenRE
works (Marcheggiani and Titov, 2016; Elsahar
et al., 2017). To be specific, we use F1 measure,
the harmonic mean of precision and recall.

First, we report the result of supervised RSN
with different clustering methods. Specifically,
SN represents the original RSN structure, HAC
and L indicate HAC and Louvain clustering intro-
duced in Sec. 3.3. The result shows that Louvain
performs better than HAC, so in the following ex-
periments we focus on using Louvain clustering.

Next, for Semi-supervised and Distantly-
supervised RSN, we conduct various combina-
tions of different mechanisms to verify the contri-
bution of each part. (+C) indicates that the model
is powered up with conditional entropy minimiza-
tion, while (+V) indicates that the model is pow-

FewRel FewRel-distant
Approach P R F1 P R F1

VAE 17.9 69.7 28.5 17.9 69.7 28.5
RW-HAC 31.8 46.0 37.6 31.8 46.0 37.6

SN-HAC 36.2 53.3 43.1 34.5 53.3 41.5
SN-L 36.5 69.2 47.8 34.6 59.8 43.9
SN-L+V 46.1 77.3 57.8 40.7 52.4 45.8
SN-L+C 47.1 78.1 58.8 42.3 66.0 51.5
SN-L+CV1 48.9 77.5 59.9 40.8 74.0 52.6

Table 1: Precision, recall and F1 results (%) for differ-
ent models. The first two models are baselines. The
next five models are different variants of our model.

ered up with virtual adversarial training.
Experimental Result Analysis. Table 1 shows

the experimental results, from which we can ob-
serve that:

(1) RSN models outperform all baseline models
on precision, recall, and F1-score, among which
Weakly-supervised RSN (SN-L+CV) achieves
state-of-the-art performances. This indicates that
RSN is capable of understanding new relations’
semantic meanings within sentences.

(2) Supervised and distantly-supervised rela-
tional representations improve clustering perfor-
mances. Compared with RW-HAC, SN-HAC
achieves better clustering results because of its
supervised relational representation and similar-
ity metric. Specifically, unsupervised baselines
mainly use sparse one-hot features. RW-HAC uses
word embeddings, but integrates them in a rule-
based way. In contrast, RSN uses distributed fea-
ture representations, and can optimize information
integration process according to supervision.

(3) Louvain outperforms HAC for clustering
with RSN, comparing SN-HAC with SN-L. One
explanation is that our model does not put addi-
tional constraints on the prior distribution of rela-
tional vectors, and therefore the relation clusters
might have odd shapes in violation of HAC’s as-
sumption. Moreover, when representations are not
distinguishable enough, forcing HAC to find fine-
grained clusters may harm recall while contribut-
ing minimally to precision. In practice, we do ob-
serve that the number of relations SN-L extracts is
constantly less than the true number 16.

(4) Both SN-L+V and SN-L+C improve the
performance of supervised or distantly-supervised

1Here for FewRel-distant we use Equation 10 rather
than Equation 9 as loss, which corresponds to Distantly-
supervised RSN, and this brings a minor improvement on F1
from 52.0% to 52.6%.



226

(a) RSN (b) Semi-supervised RSN (c) Supervised CNN

Figure 4: The t-SNE visualization of the output vectors of CNN modules in our (a) OpenRE model RSN, (b)
Semi-supervised RSN facilitated by unlabeled novel-relation data and in (c) a classical RE baseline trained with
labeled novel-relation data. All figures visualize the clustering result for 402 instances of 4 novel relations.

40 48 56 64

Number of Pre-defined Training Relations

40.0

42.5

45.0

47.5

50.0

52.5

55.0

57.5

60.0

F1
 S

co
re

SN-L+CV
SN-L+V
SN-L+C
SN-L

Figure 5: The clustering results with different numbers
of pre-defined training relations.

RSN by further utilizing unsupervised corpora.
Both semi-supervised approaches bring significant
improvements for F1 scores by increasing the pre-
cision and recall, and combining both can further
increase the F1 score.

(5) One interesting observation is that SN-L+V
does not outperform SN-L so much on FewRel-
distant. This is probably because VAT on the noisy
data might amplify the noise. In further exper-
iments, we perform VAT only on unlabeled set
and observe improvements on F1, with SN-L+V
from 45.8% to 49.2% and SN-L+CV from 52.0%
to 52.6%, which proves this conjecture.

4.4 The Influence of Pre-defined Relation
Diversity on Generalizability

In this subsection, we mainly focus on analyzing
the influence of pre-defined relation diversity, i.e.,
the number of relations in the labeled train set. To
study this influence, we use FewRel for evaluation
and change the number of relations in the labeled
train set from 40 to 64 while fixing the total num-

ber of labeled instances to 25, 000, and report the
clustering results in Figure 5.

Several conclusions can be drawn according to
Figure 5. Firstly, a rich variety of labeled rela-
tions do improve the performance of our models,
especially RSN. The models trained on 64 rela-
tions perform better than those trained on 40 rela-
tions constantly. Secondly, while the performance
of supervised RSN is very sensitive to pre-defined
relation diversity, its semi-supervised counterparts
suffer much less from the relation number limit.
This phenomenon suggests that Semi-supervised
RSNs succeed in learning from unlabeled novel-
relation data and are more generalizable to novel
relations.

4.5 Relational Knowledge Representation
Visualization

To intuitively evaluate the knowledge transfer ef-
fects of RSN and Semi-supervised RSN, we vi-
sualize their relational knowledge representation
spaces in the last layer of CNN encoders with t-
SNE(Maaten and Hinton, 2008) in Figure 4. We
also compare with a supervised CNN trained on
9, 600 labeled instances of novel relations, which
suggests the optimal relational knowledge repre-
sentation. In each figure, we plot 402 relation in-
stances of 4 randomly-chosen relation types in the
test set, and points are colored according to their
ground-truth labels.

As we can see from Figure 4, RSN is able
to roughly distinguish different relations, and
Semi-supervised RSN further facilitated knowl-
edge transfer by optimizing the margin between
potential relation clusters during training. As a re-
sult, Semi-supervised RSN can extract more dis-
tinguishable novel relations, and gains comparable



227

relational knowledge representation ability with
supervised CNN.

5 Conclusions and Future Work

In this paper, we propose a new model Rela-
tional Siamese Network (RSN) for OpenRE. Dif-
ferent from conventional unsupervised models,
our model learns to measure relational similarity
from supervised/distantly-supervised data of pre-
defined relations, as well as unsupervised data of
novel relations. There are mainly two innovative
points in our model. First, we propose to transfer
relational similarity knowledge with RSN struc-
ture. To the best of our knowledge, we are the first
to propose knowledge transfer for OpenRE. Sec-
ond, we propose Semi/Distantly-supervised RSN,
to further perform semi-supervised and distantly-
supervised transfer learning. Experiments show
that our models significantly surpass conventional
OpenRE models and achieve new state-of-the-art
performance.

For future research, we plan to explore the
following directions: (1) Besides CNN, there
are some other popular sentence encoder struc-
tures like piecewise convolutional neural network
(PCNN) and Long Short-Term Memory (LSTM)
for RE. In the future, we can try different sentence
encoders in our model. (2) As mentioned above,
our model has the potential ability to discover the
hierarchical structure of relations. In the future,
we will try to explore this application with addi-
tional experiments.

6 Acknowledgement

This work is supported by the National Key Re-
search and Development Program of China (No.
2018YFB1004503) and the National Natural Sci-
ence Foundation of China (NSFC No. 61572273,
61661146007). Ruidong Wu is also supported by
Tsinghua University Initiative Scientific Research
Program.

References
Amit Bagga and Breck Baldwin. 1998. Algorithms

for scoring coreference chains. In Proceedings of
the First Iternational Conference on Language Re-
sources and Evaluation Workshop on Linguistics
Coreference.

Eric Bair. 2013. Semi-supervised clustering meth-
ods. Wiley Interdisciplinary Reviews: Computa-
tional Statistics.

Michele Banko, Michael J Cafarella, Stephen Soder-
land, Matthew Broadhead, and Oren Etzioni. 2007.
Open information extraction from the web. In Pro-
ceedings of IJCAI.

Michele Banko and Oren Etzioni. 2008. The tradeoffs
between open and traditional relation extraction. In
Proceedings of ACL-HLT.

Vincent D Blondel, Jean Loup Guillaume, Renaud
Lambiotte, and Etienne Lefebvre. 2008. Fast un-
folding of communities in large networks. Journal
of Statistical Mechanics.

Olivier Chapelle and Alexander Zien. 2005. Semi-
supervised classification by low density separation.
In Proceedings of AISTATS.

Lei Cui, Furu Wei, and Ming Zhou. 2018. Neural open
information extraction. arXiv.

Hady Elsahar, Elena Demidova, Simon Gottschalk,
Christophe Gravier, and Frederique Laforest. 2017.
Unsupervised open relation extraction. In Proceed-
ings of European Semantic Web Conference.

Yves Grandvalet and Yoshua Bengio. 2005. Semi-
supervised learning by entropy minimization. In
Proceedings of NIPS.

Xu Han, Hao Zhu, Pengfei Yu, Ziyun Wang, Yuan Yao,
Zhiyuan Liu, and Maosong Sun. 2018. Fewrel: A
large-scale supervised few-shot relation classifica-
tion dataset with state-of-the-art evaluation. In Pro-
ceedings of EMNLP.

Meng Qu Xiang Ren Hongtao Lin, Jun Yan. 2019.
Learning dual retrieval module for semi-supervised
relation extraction. In Proceedings of WWW.

Shengbin Jia, Yang Xiang, and Xiaojun Chen. 2018.
Supervised neural models revitalize the open rela-
tion extraction. arXiv.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv.

Gregory Koch, Richard Zemel, and Ruslan Salakhut-
dinov. 2015. Siamese neural networks for one-shot
image recognition. In Proceedings of ICML Deep
Learning Workshop, volume 2.

Dekang Lin and Patrick Pantel. 2001. Dirt - discovery
of inference rules from text. In Proceedings of KDD.

Yankai Lin, Shiqi Shen, Zhiyuan Liu, Huanbo Luan,
and Maosong Sun. 2016. Neural relation extraction
with selective attention over instances. In Proceed-
ings of ACL.

Laurens van der Maaten and Geoffrey Hinton. 2008.
Visualizing data using t-sne. JMLRJournal of Sta-
tistical Mechanics.

Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David Mc-
Closky. 2014. The Stanford CoreNLP natural lan-
guage processing toolkit. In Proceedings of ACL.



228

Diego Marcheggiani and Ivan Titov. 2016. Discrete-
state variational autoencoders for joint discovery and
factorization of relations. Transactions of ACL.

Mike Mintz, Steven Bills, Rion Snow, and Daniel Ju-
rafsky. 2009. Distant supervision for relation extrac-
tion without labeled data. In Proceedings of ACL-
IJCNLP.

Takeru Miyato, Andrew M Dai, and Ian Goodfel-
low. 2016. Adversarial training methods for semi-
supervised text classification. arXiv.

Sachin Pawar, Girish K. Palshikar, and Pushpak Bhat-
tacharyya. 2017. Relation extraction : A survey.
arXiv.

Jeffrey Pennington, Richard Socher, and Christoper
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of EMNLP.

Gabriel Stanovsky, Julian Michael, Luke Zettlemoyer,
and Ido Dagan. 2018. Supervised open information
extraction. In Proceedings of NAACL.

Joe H Ward Jr. 1963. Hierarchical grouping to opti-
mize an objective function. Journal of the American
Statistical Association.

Limin Yao, Aria Haghighi, Sebastian Riedel, and An-
drew Mccallum. 2011. Structured relation discov-
ery using generative models. In Proceedings of
EMNLP.

Limin Yao, Sebastian Riedel, and Andrew McCallum.
2012. Unsupervised relation discovery with sense
disambiguation. In Proceedings of ACL.

Dian Yu, Lifu Huang, and Heng Ji. 2017. Open re-
lation extraction and grounding. In Proceedings of
IJCNLP.

Jianbo Yuan, Han Guo, Zhiwei Jin, Hongxia Jin, Xian-
chao Zhang, and Jiebo Luo. 2017. One-shot learn-
ing for fine-grained relation extraction via convolu-
tional siamese neural network. In Proceedings of
BigData.

Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou,
Jun Zhao, et al. 2014. Relation classification via
convolutional deep neural network. In Proceedings
of COLING.


