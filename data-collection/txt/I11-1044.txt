















































Extracting Relation Descriptors with Conditional Random Fields


Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 392–400,
Chiang Mai, Thailand, November 8 – 13, 2011. c©2011 AFNLP

Extracting Relation Descriptors with Conditional Random Fields

Yaliang Li†, Jing Jiang†, Hai Leong Chieu‡, Kian Ming A. Chai‡
†School of Information Systems, Singapore Management University, Singapore

‡DSO National Laboratories, Singapore
{ylli,jingjiang}@smu.edu.sg, {chaileon,ckianmin}@dso.org.sg

Abstract

In this paper we study a novel relation
extraction problem where a general rela-
tion type is defined but relation extrac-
tion involves extracting specific relation
descriptors from text. This new task can
be treated as a sequence labeling problem.
Although linear-chain conditional random
fields (CRFs) can be used to solve this
problem, we modify this baseline solution
in order to better fit our task. We propose
two modifications to linear-chain CRFs,
namely, reducing the space of possible la-
bel sequences and introducing long-range
features. Both modifications are based on
some special properties of our task. Using
two data sets we have annotated, we eval-
uate our methods and find that both modi-
fications to linear-chain CRFs can signif-
icantly improve the performance for our
task.

1 Introduction

Relation extraction is the task of identifying and
characterizing the semantic relations between en-
tities in text. Depending on the application and the
resources available, relation extraction has been
studied in a number of different settings. When
relation types are well defined and labeled relation
mention instances are available, supervised learn-
ing is usually applied (Zelenko et al., 2003; Zhou
et al., 2005; Bunescu and Mooney, 2005; Zhang
et al., 2006). When relation types are known but
little training data is available, bootstrapping has
been used to iteratively expand the set of seed ex-
amples and relation patterns (Agichtein and Gra-
vano, 2000). When no relation type is pre-defined
but there is a focused corpus of interest, unsu-
pervised relation discovery tries to cluster en-
tity pairs in order to identify interesting relation

types (Hasegawa et al., 2004; Rosenfeld and Feld-
man, 2006; Shinyama and Sekine, 2006). More
recently, open relation extraction has also been
proposed where there is no fixed domain or pre-
defined relation type, and the goal is to identify
all possible relations from an open-domain cor-
pus (Banko and Etzioni, 2008; Wu and Weld,
2010; Hoffmann et al., 2010).

These different relation extraction settings suit
different applications. In this paper, we focus on
another setting where the relation types are defined
at a general level but a more specific relation de-
scription is desired. For example, in the widely
used ACE1 data sets, relation types are defined at
a fairly coarse granularity. Take for instance the
“employment” relation, which is a major relation
type defined in ACE. In ACE evaluation, extrac-
tion of this relation only involves deciding whether
a person entity is employed by an organization en-
tity. In practice, however, we often also want to
find the exact job title or position this person holds
at the organization if this information is mentioned
in the text. Table 1 gives some examples. We refer
to the segment of text that describes the specific
relation between the two related entities (i.e., the
two arguments) as the relation descriptor. This pa-
per studies how to extract such relation descriptors
given two arguments.

One may approach this task as a sequence label-
ing problem and apply methods such as the linear-
chain conditional random fields (CRFs) (Lafferty
et al., 2001). However, this solution ignores a use-
ful property of the task: the space of possible la-
bel sequences is much smaller than that enumer-
ated by a linear-chain CRF. There are two impli-
cations. First, the normalization constant in the
linear-chain CRF is too large because it also enu-
merates the impossible sequences. Second, the re-
striction to the correct space of label sequence per-

1Automatic Content Extraction http://www.itl.
nist.gov/iad/mig/tests/ace/

392



Relation Candidate Relation Instance Relation Descriptor

Employment ... said ARG-1 , a vice president at ARG-2 , which ... a vice president
(PER, ORG) A ARG-2 spokesman , ARG-1 , said the company now ... spokesman

At ARG-2 , by contrast , ARG-1 said customers spend on ... Nil

Personal/Social ARG-1 had an elder brother named ARG-2 . an elder brother
(PER, PER) ARG-1 was born at ... , as the son of ARG-2 of Sweden ... the son

ARG-1 later married ARG-2 in 1973 , ... married
Through his contact with ARG-1 , ARG-2 joined the Greek Orthodox Church . Nil

Table 1: Some examples of candidate relation instances and their relation descriptors.

mits the use of long-range features without an ex-
ponential increase in computational cost.

We compare the performance of the baseline
linear-chain CRF model and our special CRF
model on two data sets that we have manually an-
notated. Our experimental results show that both
reducing the label sequence space and introducing
long-range features can significantly improve the
baseline performance.

The rest of the paper is organized as follows. In
Section 2 we review related work. We then for-
mally define our task in Section 3. In Section 4 we
present a baseline linear-chain CRF-based solu-
tion and our modifications to the baseline method.
We discuss the annotation of our data sets and
show our experimental results in Section 5. We
conclude in Section 6.

2 Related Work

Most existing work on relation extraction stud-
ies binary relations between two entities. For
supervised relation extraction, existing work of-
ten uses the ACE benchmark data sets for eval-
uation (Bunescu and Mooney, 2005; Zhou et al.,
2005; Zhang et al., 2006). In this setting, a set of
relation types are defined and the task is to iden-
tify pairs of entities that are related and to clas-
sify their relations into one of the pre-defined re-
lation types. It is assumed that the relation type
itself is sufficient to characterize the relation be-
tween the two related entities. However, based on
our observation, some of the relation types defined
in ACE such as the “employment” relation and the
“personal/social” relation are very general and can
be further characterized by more specific descrip-
tions.

Recently open relation extraction has been
proposed for open-domain information extrac-
tion (Banko and Etzioni, 2008). Since there are no
fixed relation types, open relation extraction aims
at extracting all possible relations between pairs of

entities. The extracted results are (ARG-1, REL,
ARG-2) tuples. The TextRunner system based
on (Banko and Etzioni, 2008) extracts a diverse
set of relations from a huge Web corpus. These
extracted predicate-argument tuples are presum-
ably the most useful to support Web search scenar-
ios where the user is looking for specific relations.
However, because of the diversity of the extracted
relations and the domain independence, open rela-
tion extraction is probably not suitable for popu-
lating relational databases or knowledgebases. In
contrast, the task of extracting relation descriptors
as we have proposed still assumes a pre-defined
general relation type, which ensures that the ex-
tracted tuples follow the same relation definition
and thus can be used in applications such as popu-
lating relational databases.

In terms of models and techniques, we use stan-
dard linear-chain CRF as our baseline, which is the
main method used in (Banko and Etzioni, 2008)
as well as for many other information extraction
problems. The major modifications we propose
for our task are the reduction of the label sequence
space and the incorporation of long-range features.
We note that these modifications are closely re-
lated to the semi-Markov CRF models proposed
by Sarawagi and Cohen (2005). In fact, the modi-
fied CRF model for our task can be considered as a
special case of semi-Markov CRF where we only
consider label sequences that contain at most one
relation descriptor sequence.

3 Task Definition

In this section we define the task of extracting re-
lation descriptors for a given pre-defined class of
relations such as “employment.” Given two named
entities occurring in the same sentence, one acting
as ARG-1 and the other as ARG-2, we aim to ex-
tract a segment of text from the sentence that best
describes a pre-defined general relation between
the two entities. Formally, let (w1, w2, . . . , wn)

393



denote the sequence of tokens in a sentence, where
wp is ARG-1 and wq is ARG-2 (1 ≤ p, q ≤ n,
p 6= q). Our goal is to locate a subsequence
(wr, . . . , ws) (1 ≤ r ≤ s ≤ n) that best describes
the relation between ARG-1 and ARG-2. If ARG-1
and ARG-2 are not related through the pre-defined
general relation, Nil should be returned.

The above definition constrains ARG-1 and
ARG-2 to single tokens. In our experiments, we
will replace the original lexical strings of ARG-
1 and ARG-2 with the generic tokens ARG1 and
ARG2. Examples of sentences with the named en-
tities replaced with argument tokens are shown in
the second column of Table 1.

4 Method

4.1 Representation

The relation descriptor extraction task can be
treated as a sequence labeling problem. Let x =
(x1, x2, . . . , xn) denote the sequence of observa-
tions in a relation instance, where xi is wi aug-
mented with additional information such as the
POS tag of wi, and the phrase boundary infor-
mation. Each observation xi is associated with
a label yi ∈ Y which indicates whether wi is
part of the relation descriptor. Following the com-
monly used BIO notation (Ramshaw and Mar-
cus, 1995) in sequence labeling, we define Y =
{B-REL, I-REL,O}. Let y = (y1, y2, . . . , yn) de-
note the sequence of labels for x. Our task can
be reduced to finding the best label sequence ŷ
among all the possible label sequences for x.

4.2 A Linear-Chain CRF Solution

For sequence labeling tasks in NLP, linear-chain
CRFs have been rather successful. It is an undi-
rected graphical model in which the conditional
probability of a label sequence y given the obser-
vation sequence x is

p(y|x,Λ) =
exp

“P
i

P
k λkfk(yi−1, yi,x)

”
Z(x,Λ)

, (1)

where Λ = {λk} is the set of model parameters,
fk is an arbitrary feature function defined over two
consecutive labels and the whole observation se-
quence, and

Z(x,Λ) =
X
y′

exp
“X

i

X
k

λkfk(y
′
i−1, y

′
i,x)

”
(2)

is the normalization constant.
Given a set of training instances {xj ,y∗j} where

y∗j is the correct label sequence for xj , we can
learn the best model parameters Λ̂ as follows:

Λ̂ = argmin
Λ

 
−
X
j

log p(y∗j |xj ,Λ) + β
X
k

λ2k

!
. (3)

Here β
∑

k λ
2
k is a regularization term.

4.3 Improvement over Linear-Chain CRFs

We note that while we can directly apply linear-
chain CRFs to extract relation descriptors, there
are some special properties of our task that allow
us to modify standard linear-chain CRFs to better
suit our needs.

Label sequence constraint
In linear-chain CRFs, the normalization constant
Z considers all possible label sequences y. For
the relation descriptor extraction problem, how-
ever, we expect that there is either a single relation
descriptor sequence or no such sequence. In other
words, for a given relation instance, we only ex-
pect two kinds of label sequences: (1) All yi are O,
and (2) exactly one yi is B-REL followed by zero
or more consecutive I-REL while all other yi are
O. Therefore the space of label sequences should
be reduced to only those that satisfy the above con-
straint.

One way to exploit this constraint within linear-
chain CRFs is to enforce it only during testing.
We can pick the label sequence that has the high-
est probability in the valid label sequence space
instead of the entire label sequence space. For a
candidate relation instance x, let Ỹ denote the set
of valid label sequences, i.e., those that have ei-
ther one or no relation descriptor sequence. We
then choose the best sequence ŷ as follows:

ŷ = argmax
y∈Ỹ

p(y|x, Λ̂). (4)

Arguably, the more principled way to exploit
the constraint is to modify the probabilistic model
itself. So at the training stage, we should also con-
sider only Ỹ by defining the normalization term Z̃
as follows:

Z̃(x,Λ) =
X
y′∈Ỹ

exp
“X

i

X
k

λkfk(y
′
i−1, y

′
i,x)

”
. (5)

394



The difference between Equation (5) and Equa-
tion (2) is the set of label sequences considered. In
other words, while in linear-chain CRFs the cor-
rect label sequence competes with all possible la-
bel sequences for probability mass, for our task the
correct label sequence should compete with only
other valid label sequences. In Section 5 we will
compare these two different normalization terms
and show the advantage of using Equation (5).

Adding long-range features
In linear-chain CRF models, only first-order la-
bel dependencies are considered because features
are defined over two consecutive labels. Inference
in linear-chain CRFs can be done efficiently us-
ing dynamic programming. More general higher-
order CRF models also exist, allowing long-range
features defined over more than two consecutive
labels. But the computational cost of higher-order
CRFs also increases exponentially with the order
of dependency.

For our task, because of the constraint on the
space of label sequences, we can afford to use
long-range features. In our case, inference is still
efficient because the number of sequences to be
enumerated has been drastically reduced due to the
constraint. Let g(y,x) denote a feature function
defined over the entire label sequence y and the
observation sequence x. We can include such fea-
ture functions in our model as follows:

p(y|x,Θ) = 1
Z̃(x,Θ)

"
exp

“P
i

P
k λkfk(yi−1, yi,x)

+
P

l µlgl(y,x)
”#

, (6)

where Θ = {{λk}, {µl}} is the set of all model
parameters. Both {λk} and {µl} are regular-
ized as in Equation (3). Note that although each
f(yi−1, yi,x) may be subsumed under a g(y,x),
here we group all features that can be captured by
linear-chain CRFs under f and other real long-
range features under g. In Section 5 we will see
that with the additional feature functions g, rela-
tion extraction performance can also be further im-
proved.

4.4 Features
We now describe the features we use in the base-
line linear-chain CRF model and our modified
model.

Linear-chain features
The linear-chain features are those that can be for-
mulated as f(yi−1, yi,x), i.e., those that depend
on x and two consecutive labels only. We use
typical features that include tokens, POS tags and
phrase boundary information coupled with label
values. Let ti denote the POS tag of wi and pi
denote the phrase boundary tag of wi. The phrase
boundary tags also follow the BIO notation. Ex-
amples include B-NP, I-VP, etc. Table 2 shows the
feature templates covering only the observations.
Each feature shown in Table 2 is further combined
with either the value of the current label yi or the
values of the previous and the current labels yi−1
and yi to form zeroth order and first order features.
For example, a zeroth order feature is “yi is B-REL
and wi is the and wi+1 is president”, and a
first order feature is “yi−1 is O and yi is B-REL
and ti is N”.

Long-range features
Long-range features are those that cannot be
defined based on only two consecutive labels.
When defining long-range features, we treat the
whole relation descriptor sequence as a single
unit, denoted as REL. Given a label sequence y
that contains a relation descriptor sequence, let
(wr, wr+1, . . . , ws) denote the relation descriptor,
that is, yr = B-REL and yt = I-REL where
r + 1 ≤ t ≤ s. The long-range features we
use are categorized and summarized in Table 3.
These features capture the context of the entire re-
lation descriptor, its relation to the two arguments,
and whether the boundary of the relation descrip-
tor conforms to the phrase boundaries (since we
expect that most relation descriptors consist of a
single or a sequence of phrases).

5 Experiments

5.1 Data Preparation

Since the task of extracting relation descriptors is
new, we are not aware of any data set that can be
directly used to evaluate our methods. We there-
fore annotated two data sets for evaluation, one for
the general “employment” relation and the other
for the general “personal/social” relation.2

The first data set contains 150 business articles
from New York Times. The articles were crawled
from the NYT website between November 2009

2http://www.mysmu.edu/faculty/
jingjiang/data/IJCNLP2011.zip

395



Description Feature Template Example

single token wi+j (−2 ≤ j ≤ 2) wi+1 (next token) is president
single POS tag ti+j (−2 ≤ j ≤ 2) ti (current POS tag) is DET
single phrase tag pi+j (−2 ≤ j ≤ 2) pi−1 (previous phrase boundary tag) is I-NP
two consecutive tokens wi+j−1&wi+j (−1 ≤ j ≤ 2) wi is the and wi+1 is president
two consecutive POS tags ti+j−1&ti+j (−1 ≤ j ≤ 2) ti is DET and ti+1 is N
two consecutive phrase tags pi+j−1&pi+j (−1 ≤ j ≤ 2) pi is B-NP and pi+1 is I-NP

Table 2: Linear-chain feature templates. Each feature is defined with respect to a particular (current)
position in the sequence. i indicates the current position and j indicates the position relative to the
current position. All features are defined using observations within a window size of 5 of the current
position.

Category Feature Template Description Example

Contextual Features word wr−1 or POS tag tr−1 preceding relation descriptor , REL
word ws+1 or POS tag ts+1 following relation descriptor REL PREP

Path-based Features word or POS tag sequence between ARG1 and relation descriptor ARG1 is REL
word or POS tag sequence between ARG2 and relation descriptor REL PREP ARG2
word or POS tag sequence containing ARG1, ARG2 and relation descriptor ARG2 ’s REL , ARG1

Phrase Boundary whether relation descriptor violates phrase boundaries 1 or 0
Feature

Table 3: Long-range feature templates. r and s are the indices of the first word and the last word of the
relation descriptor, respectively.

and January 2010. After sentence segmentation
and tokenization, we used the Stanford NER tag-
ger (Finkel et al., 2005) to identify PER and ORG
named entities from each sentence. For named en-
tities that contain multiple tokens we concatenated
them into a single token. We then took each pair
of (PER, ORG) entities that occur in the same sen-
tence as a single candidate relation instance, where
the PER entity is treated as ARG-1 and the ORG
entity is treated as ARG-2.

The second data set comes from a Wikipedia
personal/social relation data set previously used
in (Culotta et al., 2006). The original data set
does not contain annotations of relation descrip-
tors such as “sister” or “friend” between the two
PER arguments. We therefore also manually an-
notated this data set. Similarly, we performed
sentence segmentation, tokenization and NER tag-
ging, and took each pair of (PER, PER) entities
occurring in the same sentence as a candidate re-
lation instance. Because both arguments involved
in the “personal/social” relation are PER entities,
we always treat the first PER entity as ARG-1 and
the second PER entity as ARG-2.3

3Since many personal/social relations are asymmetric,
ideally we should assign ARG-1 and ARG-2 based on their
semantic meanings rather than their positions. Here we take
a simple approach.

We go through each candidate relation instance
to find whether there is an explicit sequence of
words describing the relation between ARG-1 and
ARG-2, and label the sequence of words, if any.
Note that we only consider explicitly stated rela-
tion descriptors. If we cannot find such a rela-
tion descriptor, even if ARG-1 and ARG-2 actu-
ally have some kind of relation, we still label the
instance as Nil. For example, in the instance “he
is the son of ARG1 and ARG2”, although we can
infer that ARG-1 and ARG-2 have some family re-
lation, we regard this as a negative instance.

A relation descriptor may also contain multi-
ple relations. For example, in the instance “ARG1
is the CEO and president of ARG2”, we label
“the CEO and president” as the relation descrip-
tor, which actually contains two job titles, namely,
CEO and president.

Note that our annotated relation descriptors are
not always nouns or noun phrases. An example
is the third instance for personal/social relation in
Table 1, where the relation descriptor “married” is
a verb and indicates a spouse relation.

The total number of relation instances, the num-
ber of positive and negative instances as well as the
number of distinct relation descriptors in each data
set are summarized in Table 4.

396



Data Set total positive negative distinct
descriptors

NYT 536 208 328 140
Wikipedia 700 122 578 70

Table 4: Number of instances in each data set.
Positive instances are those that have an explicit
relation descriptor. The last column shows the
number of distinct relation descriptors.

5.2 Experiment Setup

We compare the following methods in our experi-
ments:

• LC-CRF: This is the standard linear-chain
CRF model with features described in Ta-
ble 2.

• M-CRF-1: This is our modified linear-chain
CRF model with the space of label sequences
reduced but with features fixed to the same as
those used in LC-CRF.

• M-CRF-2: This is M-CRF-1 with the addi-
tion of the contextual long-range features de-
scribed in Table 3.

• M-CRF-3: This is M-CRF-2 with the addi-
tion of the path-based long-range features de-
scribed in Table 3.

• M-CRF-4: This is M-CRF-3 with the addi-
tion of the phrase boundary long-range fea-
ture described in Table 3.

For the standard linear-chain CRF model, we
use the package CRF++4. We implement our own
version of the modified linear-chain CRF models.

We perform 10-fold cross validation for all our
experiments. For each data set we first randomly
divide it into 10 subsets. Each time we take 9 sub-
sets for training and the remaining subset for test-
ing. We report the average performance across the
10 runs.

Based on our preliminary experiments, we have
found that using a smaller set of general POS
tags instead of the Penn Treebank POS tag set
could slightly improve the overall performance.
We therefore only report the performance obtained
using our POS tags. For example, we group NN,
NNP, NNS and NNPS of the Penn Treebank set
under a general tag N.

4http://crfpp.sourceforge.net/

We evaluate the performance using two differ-
ent criteria: overlap match and exact match. Over-
lap match is a more relaxed criterion: if the ex-
tracted relation descriptor overlaps with the true
relation descriptor (i.e., having at least one to-
ken in common), it is considered correct. Ex-
act match is a much stricter criterion: it requires
that the extracted relation descriptor be exactly the
same as the true relation descriptor in order to be
considered correct. Given these two criteria, we
can define accuracy, precision, recall and F1 mea-
sures. Accuracy is the percentage of candidate re-
lation instances whose label sequence is consid-
ered correct. Both positive and negative instances
are counted when computing accuracy. Because
our data sets are quite balanced, it is reasonable to
use accuracy. Precision, recall and F1 are defined
in the usual way at the relation instance level.

5.3 Method Comparison

In Table 5, we summarize the performance in
terms of the various measures on the two data
sets. For both the baseline linear-chain CRF model
and our modified linear-chain CRF models, we
have tuned the regularization parameters and show
only the results using the optimal parameter val-
ues for each data set, chosen from β = 10γ for
γ ∈ [−3,−2, . . . , 2, 3].

First, we can see from the table that by reduc-
ing the label sequence space, M-CRF-1 can signif-
icantly outperform the baseline LC-CRF in terms
of F1 in all cases. In terms of accuracy, there is
significant improvement for the NYT data set but
not for the Wikipedia data set. We also notice that
for both data sets the advantage of M-CRF-1 is
mostly evident in the improvement of recall. This
shows that a larger number of true relation descrip-
tors are extracted when the label sequence space is
reduced.

Next we see from the table that long-range fea-
tures are also useful, and the improvement comes
mostly from the path-based long-range features.
In terms of both accuracy and F1, M-CRF-3 can
significantly outperform M-CRF-1 in all settings.
In this case, the improvement is a mixture of both
precision and recall. This shows that by explicitly
capturing the patterns between the two arguments
and the relation descriptor, we can largely improve
the extraction performance. On the other hand,
neither the contextual long-range features nor the
phrase boundary long-range features exhibit any

397



New York Times Overlap Match Exact Match

Accu. Prec. Rec. F1 Accu. Prec. Rec. F1

LC-CRF 0.8173 0.8407 0.6548 0.7303 0.8117 0.8373 0.6394 0.7186
M-CRF-1 0.8491† 0.8640 0.7202† 0.7830† 0.8454† 0.8625 0.7124† 0.7774†

M-CRF-2 0.8491 0.8627 0.7202 0.7819 0.8454 0.8617 0.7124 0.7763
M-CRF-3 0.8659† 0.9000† 0.7364 0.8070† 0.8640† 0.8992† 0.7319† 0.8038†
M-CRF-4 0.8659 0.9000 0.7364 0.8070 0.8640 0.8992 0.7319 0.8038

Wikipedia Overlap Match Exact Match

Accu. Prec. Rec. F1 Accu. Prec. Rec. F1

LC-CRF 0.8486 0.6513 0.3140 0.4137 0.8457 0.6489 0.2980 0.3931
M-CRF-1 0.8414 0.5648 0.4233† 0.4778† 0.8386 0.5530 0.4072† 0.4609†

M-CRF-2 0.8471 0.5859 0.4260 0.4873 0.8443 0.5741 0.4099 0.4704
M-CRF-3 0.8657† 0.6847† 0.4488 0.5318† 0.8628† 0.6823† 0.4327 0.5144†
M-CRF-4 0.8671 0.6966 0.4388 0.5278 0.8643 0.6942 0.4228 0.5105

Table 5: Comparison of different methods on the New York Times data set and Wikipedia data set. Accu.,
Prec., Rec. and F1 stand for accuracy, precision, recall and F1 measures, respectively. † indicates that
the current value is statistically significantly better than the value in the previous row at a 0.95 level of
confidence by one-tailed paired T-test.

significant impact. We hypothesize the following.
For contextual long-range features, they have al-
ready been captured in the linear-chain features.
For example, the long-range feature “is REL” is
similar to the linear-chain feature “wi−1 = is & yi
= B-R”. For the phrase boundary long-range fea-
ture, since phrase boundary tags have also been
used in the linear-chain features, this feature does
not provide additional information. In addition,
we have found that a large percentage of relation
descriptors violate phrase boundaries: 22% in the
NYT data set, and 29% in the Wikipedia data set.
Therefore, it seems that phrase boundary informa-
tion is not important for relation descriptor extrac-
tion.

Overall, performance is much higher on the
NYT data set than on the Wikipedia data set.
Based on our observations during annotation, this
is due to the fact that the “employment” relations
expressed in the NYT data set often follow some
standard patterns, whereas in Wikipedia the “per-
sonal/social” relations can be expressed in more
varied ways. The lower performance achieved on
the Wikipedia data set suggests that extracting re-
lation descriptors is not an easy task even under a
supervised learning setting.

Presumably relation descriptors that are not
seen in the training data are harder to extract.
We would therefore also like to see how well our
model works on such unseen relation descriptors.
We find that with 10-fold cross validation, for the

NYT data set, on average our model is able to ex-
tract approximately 67% of the unseen relation de-
scriptors in the test data using exact match criteria.
For the Wikipedia data set this percentage is ap-
proximately 27%. Both numbers are lower than
the overall recall values the model can achieve on
the entire test data, showing that unseen relation
descriptors are indeed harder to extract. However,
our model is still able to pick up new relation de-
scriptors.

5.4 The Effect of Training Data Size

In the previous experiments, we have used 90%
of the data for training and the remaining 10% for
testing. We now take a look at how the perfor-
mance changes with different numbers of training
instances. We vary the training data size from only
a few instances (2, 5, and 10) to 20%, 40%, 60%
and 80% of the entire data set. The results are
shown in Figure 1.

As we can expect, when the number of train-
ing instances is small, the performance on both
data sets is low. The figure also shows that the
Wikipedia data set is the more difficult than the
NYT data set. This is consistent with our observa-
tion in the previous section.

The modified linear-chain CRF model consis-
tently outperforms the baseline linear-chain CRF
model. For similar level of performance, the mod-
ified linear-chain CRF model requires less train-
ing data than the baseline linear-chain CRF model.

398



 0

 0.2

 0.4

 0.6

 0.8

 1

 1  10  100  1000

O
ve

rla
p 

F
1 

m
ea

su
re

Size of Training Data

LC-CRF
M-CRF-3

(a) New York Times

 0

 0.2

 0.4

 0.6

 0.8

 1

 1  10  100  1000

E
xa

ct
 F

1 
m

ea
su

re

Size of Training Data

LC-CRF
M-CRF-3

(b) New York Times

 0

 0.2

 0.4

 0.6

 0.8

 1

 1  10  100  1000

O
ve

rla
p 

F
1 

m
ea

su
re

Size of Training Data

LC-CRF
M-CRF-3

(c) Wikipedia

 0

 0.2

 0.4

 0.6

 0.8

 1

 1  10  100  1000
E

xa
ct

 F
1 

m
ea

su
re

Size of Training Data

LC-CRF
M-CRF-3

(d) Wikipedia

Figure 1: Performance of LC-CRF and M-CRF-3 as the training data size increases.

For example, Figure 1(b) shows that the modi-
fied linear-chain CRF model achieve 0.72 F1 with
about 215 training instances, while the baseline
linear-chain CRF model requires about 480 train-
ing instances for a similar F1.

6 Conclusions

In this paper, we studied relation extraction un-
der a new setting: the relation types are defined
at a general level but more specific relation de-
scriptors are desired. Based on the special prop-
erties of this new task, we found that standard
linear-chain CRF models have some potential lim-
itations for this task. We subsequently proposed
some modifications to linear-chain CRFs in order
to suit our task better. We annotated two data sets
to evaluate our methods. The experiments showed
that by restricting the space of possible label se-
quences and introducing certain long-range fea-
tures, the performance of the modified linear-chain
CRF model can perform significantly better than
standard linear-chain CRFs.

Currently our work is only based on evaluation
on two data sets and on two general relations. In
the future we plan to evaluate the methods on other
general relations to test its robustness. We also

plan to explore how this new relation extraction
task can be used within other NLP or text mining
applications.

Acknowledgments

This material is based on research sponsored by
the Air Force Research Laboratory, under agree-
ment number FA2386-09-1-4123. The U.S. Gov-
ernment is authorized to reproduce and distribute
reprints for Governmental purposes notwithstand-
ing any copyright notation thereon. The views and
conclusions contained herein are those of the au-
thors and should not be interpreted as necessarily
representing the official policies or endorsements,
either expressed or implied, of the Air Force Re-
search Laboratory or the U.S. Government.

References
Eugene Agichtein and Luis Gravano. 2000. Snow-

ball: Extracting relations from large plain-text col-
lections. In Proceedings of the Fifth ACM Confer-
ence on Digital Libraries, pages 85–94, June.

Michele Banko and Oren Etzioni. 2008. The tradeoffs
between open and traditional relation extraction. In
Proceedings of the 46th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 28–36.

399



Razvan Bunescu and Raymond Mooney. 2005. A
shortest path dependency kernel for relation extrac-
tion. In Proceedings of the Human Language Tech-
nology Conference and the Conference on Empiri-
cal Methods in Natural Language Processing, pages
724–731, October.

Aron Culotta, Andrew McCallum, and Jonathan Betz.
2006. Integrating probabilistic extraction models
and data mining to discover relations and patterns in
text. In Proceedings of the Human Language Tech-
nology Conference of the North American Chapter
of the Association for Computational Linguistics,
pages 296–303, June.

Jenny Rose Finkel, Trond Grenager, and Christo-
pher D. Manning. 2005. Incorporating non-local
information into information extraction systems by
gibbs sampling. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Lin-
guistics, pages 363–370, June.

Takaaki Hasegawa, Satoshi Sekine, and Ralph Grish-
man. 2004. Discovering relations among named
entities from large corpora. In Proceedings of the
42nd Meeting of the Association for Computational
Linguistics, pages 415–422, July.

Raphael Hoffmann, Congle Zhang, and Daniel S.
Weld. 2010. Learning 5000 relational extractors. In
Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics, pages 286–
295, July.

John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In Proceedings of the 18th Interna-
tional Conference on Machine Learning, pages 282–
289, June.

Lance A. Ramshaw and Mitchell P. Marcus. 1995.
Text chunking using transformation-based learning.
In Proceedings of the Third ACL Workshop on Very
Large Corpora, pages 82–94.

Benjamin Rosenfeld and Ronen Feldman. 2006.
URES : An unsupervised Web relation extraction
system. In Proceedings of the 21st International
Conference on Computational Linguistics and 44th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 667–674, July.

Sunita Sarawagi and William W. Cohen. 2005. Semi-
Markov conditional random fields for information
extraction. In Advances in Neural Information Pro-
cessing Systems 17, pages 1185–1192.

Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted rela-
tion discovery. In Proceedings of the Human Lan-
guage Technology Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics, pages 304–311, June.

Fei Wu and Daniel S. Weld. 2010. Open information
extraction using Wikipedia. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 118–127, July.

Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2003. Kernel methods for relation ex-
traction. Journal of Machine Learning Research,
3:1083–1106.

Min Zhang, Jie Zhang, and Jian Su. 2006. Explor-
ing syntactic features for relation extraction using a
convolution tree kernel. In Proceedings of the Hu-
man Language Technology Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 288–295, June.

GuoDong Zhou, Jian Su, Jie Zhang, and Min Zhang.
2005. Exploring various knowledge in relation ex-
traction. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 427–434, June.

400


