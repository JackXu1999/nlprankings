



















































A Joint Framework for Coreference Resolution and Mention Head Detection


Proceedings of the 19th Conference on Computational Language Learning, pages 12–21,
Beijing, China, July 30-31, 2015. c©2015 Association for Computational Linguistics

A Joint Framework for Coreference Resolution and Mention Head Detection

Haoruo Peng Kai-Wei Chang Dan Roth
University of Illinois, Urbana-Champaign

Urbana, IL, 61801
{hpeng7,kchang10,danr}@illinois.edu

Abstract

In coreference resolution, a fair amount of
research treats mention detection as a pre-
processed step and focuses on developing
algorithms for clustering coreferred men-
tions. However, there are significant gaps
between the performance on gold mentions
and the performance on the real problem,
when mentions are predicted from raw text
via an imperfect Mention Detection (MD)
module. Motivated by the goal of reduc-
ing such gaps, we develop an ILP-based
joint coreference resolution and mention
head formulation that is shown to yield sig-
nificant improvements on coreference from
raw text, outperforming existing state-of-
art systems on both the ACE-2004 and the
CoNLL-2012 datasets. At the same time,
our joint approach is shown to improve men-
tion detection by close to 15% F1. One
key insight underlying our approach is that
identifying and co-referring mention heads
is not only sufficient but is more robust than
working with complete mentions.

1 Introduction

Mention detection is rarely studied as a stand-alone
research problem (Recasens et al. (2013) is one
key exception). Most coreference resolution work
simply mentions it in passing as a module in the
pipelined system (Chang et al., 2013; Durrett and
Klein, 2013; Lee et al., 2011; Björkelund and Kuhn,
2014). However, the lack of emphasis is not due to
this being a minor issue, but rather, we think, its dif-
ficulty. Indeed, many papers report results in terms
of gold mentions versus system generated mentions,
as shown in Table 1. Current state-of-the-art sys-
tems show a very significant drop in performance
when running on system generated mentions. These
performance gaps are worrisome, since the real goal
of NLP systems is to process raw data.

System Dataset Gold Predict Gap
Illinois CoNLL-12 77.05 60.00 17.05
Illinois CoNLL-11 77.22 60.18 17.04
Illinois ACE-04 79.42 68.27 11.15
Berkeley CoNLL-11 76.68 60.42 16.26
Stanford ACE-04 81.05 70.33 10.72

Table 1: Performance gaps between using gold mentions
and predicted mentions for three state-of-the-art corefer-
ence resolution systems. Performance gaps are always larger
than 10%. Illinois’s system (Chang et al., 2013) is evaluated
on CoNLL (2012, 2011) Shared Task and ACE-2004 datasets.
It reports an average F1 score of MUC, B3 and CEAFe met-
rics using CoNLL v7.0 scorer. Berkeley’s system (Durrett and
Klein, 2013) reports the same average score on the CoNLL-
2011 Shared Task dataset. Results of Stanford’s system (Lee et
al., 2011) are for B3 metric on ACE-2004 dataset.

This paper focuses on improving end-to-end
coreference performance. We do this by: 1) De-
veloping a new ILP-based joint learning and infer-
ence formulation for coreference and mention head
detection. 2) Developing a better mention head can-
didate generation algorithm. Importantly, we focus
on heads rather than mention boundaries since those
can be identified more robustly and used effectively
in an end-to-end system. As we show, this results
in a dramatic improvement in the quality of the MD
component and, consequently, a significant reduc-
tion in the performance gap between coreference on
gold mentions and coreference on raw data.

Existing coreference systems usually consider a
pipelined system, where the mention detection step
is followed by that of clustering mentions into coref-
erence chains. Higher quality mention identification
naturally leads to better coreference performance.
Standard methods define mentions as boundaries of
text, and expect exact boundaries as input in the
coreference step. However, mentions have an intrin-
sic structure, in which mention heads carry the cru-
cial information. Here, we define a mention head as
the last token of a syntactic head, or the whole syn-
tactic head for proper names.1 For example, in “the

1Here, we follow the ACE annotation guideline. Note that

12



incumbent [Barack Obama]” and “[officials] at the
Pentagon”, “Barack Obama” and “officials” serve
as mention heads, respectively. Mention heads can
be used as auxiliary structures for coreference. In
this paper, we first identify mention heads, and then
detect mention boundaries based on heads. We rely
heavily on the first, head identification, step, which
we show to be sufficient to support coreference deci-
sions. Moreover, this step also provides enough in-
formation for “understanding” the coreference out-
put, and can be evaluated more robustly (since mi-
nor disagreements on mention boundaries are often
a reason for evaluation issues when dealing with
predicted mentions). We only identify the mention
boundaries at the end, after we make the coreference
decisions, to be consistent with current evaluation
standards in the corefernce resolution community.
Consider the following example2:

[Multinational companies investing in [China]]
had become so angry that [they] recently
set up an anti-piracy league to pressure [the
[Chinese] government] to take action. [Do-
mestic manufacturers, [who] are also suffering],
launched a similar body this month. [They] hope
[the government] can introduce a new law in-
creasing fines against [producers of fake goods]
from the amount of profit made to the value of the
goods produced.

Here, phrases in the brackets are mentions and
the underlined simple phrases are mention heads.
Moreover, mention boundaries can be nested (the
boundary of a mention is inside the boundary of
another mention), but mention heads never overlap.
This property also simplifies the problem of mention
head candidate generation. In the example above,
the first “they” refers to “Multinational companies
investing in China” and the second “They” refers
to “Domestic manufacturers, who are also suffer-
ing”. In both cases, the mention heads are sufficient
to support the decisions: ”they” refers to ”compa-
nies”, and ”They” refers to ”manufacturers”. In
fact, most of the features3 implemented in existing
coreference resolution systems rely solely on men-
tion heads (Bengtson and Roth, 2008).

Furthermore, consider the possible mention can-
didate “league” (italic in the text). It is not cho-
sen as a mention because the surrounding context
is not focused on “anti-piracy league”. So, mention

the CoNLL-2012 dataset is built from OntoNotes-5.0 corpus.
2This example is chosen from the ACE-2004 corpus.
3All features except for those that rely on modifiers.

Figure 1: Comparison between a traditional pipelined sys-
tem and our proposed system. We split up mention detection
into two steps: mention head candidate generation and (an op-
tional) mention boundary detection. We feed mention heads
rather than complete mentions into the coreference model. Dur-
ing the joint head-coreference process, we reject some mention
head candidates and then recover complete mention boundaries
after coreference decisions are made.

detection can be viewed as a global decision prob-
lem, which involves considering the relevance of a
mention to its context. The fact that the coreference
decision provides a way to represent this relevance,
further motivates considering mention detection and
coreference jointly. The insight here is that a men-
tion candidate will be more likely to be valid when
it has more high confidence coreference links.

This paper develops a joint coreference resolution
and mention head detection framework as an Inte-
ger Linear Program (ILP) following Roth and Yih
(2004). Figure 1 compares a traditional pipelined
system with our proposed system. Our joint for-
mulation includes decision variables both for coref-
erence links between pairs of mention heads, and
for all mention head candidates, and we simultane-
ously learn the ILP coefficients for all these vari-
ables. During joint inference, some of the mention
head candidates will be rejected (that is, the corre-
sponding variables will be assigned ’0’), contribut-
ing to improvement both in MD and in coreference
performance. The aforementioned joint approach
builds on an algorithm that generates mention head
candidates. Our candidate generation process con-
sists of a statistical component and a component that
makes use of existing resources, and is designed to
ensure high recall on head candidates.

Ideally, after making coreference decisions, we
extend the remaining mention heads to complete
mentions; we employ a binary classifier, which
shares all features with the mention head detection
model in the joint step.

Our proposed system can work on both ACE and
OntoNotes datasets, even though their styles of an-
notation are different. There are two main differ-

13



ences to be addressed. First, OntoNotes removes
singleton mentions, even if they are valid mentions.
This causes additional difficulty in learning a good
mention detector in a pipelined framework. How-
ever, our joint framework can adapt to it by rejecting
those singleton mentions. More details will be dis-
cussed in Sec. 2. Second, ACE uses shortest deno-
tative phrases to identify mentions while OntoNotes
tends to use long text spans. This makes identifying
mention boundaries unnecessarily hard. Our system
focuses on mention heads in the coreference stage to
ensure robustness. As OntoNotes does not contain
head annotations, we preprocess the data to extract
mention heads which conform with the ACE style.

Results on ACE-2004 and CoNLL-2012 datasets
show that our system4 reduces the performance gap
for coreference by around 25% (measured as the ra-
tio of performance improvement over performance
gap) and improves the overall mention detection by
over 10 F1 points. With such significant improve-
ments, we achieve the best end-to-end coreference
resolution results reported so far.

The main contributions of our work can be sum-
marized as follows:

1. We develop a new, end-to-end, coreference ap-
proach that is based on a joint learning and in-
ference model for mention heads and corefer-
ence decisions.

2. We develop an improved mention head candi-
date generation module and a mention bound-
ary detection module.

3. We achieve the best coreference results on pre-
dicted mentions and reduce the performance
gap compared to using gold mentions.

The rest of the paper is organized as follows. We
explain the joint head-coreference learning and in-
ference framework in Sec. 2. Our mention head
candidate generation module and mention boundary
detection module are described in Sec. 3. We report
our experimental results in Sec. 4, review related
work in Sec. 5 and conclude in Sec. 6.

2 A Joint Head-Coreference Framework

This section describes our joint coreference resolu-
tion and mention head detection framework. Our
work is inspired by the latent left-linking model in
Chang et al. (2013) and the ILP formulation from
Chang et al. (2011). The joint learning and infer-
ence model takes as input mention head candidates

4Available at http://cogcomp.cs.illinois.
edu/page/software_view/Coref

(Sec. 3) and jointly (1) determines if they are indeed
mention heads and (2) learns a similarity metric be-
tween mentions. This is done by simultaneously
learning a binary mention head detection classifier
and a mention-pair coreference classifier. The men-
tion head detection model here is mainly trained to
differentiate valid mention heads from invalid ones.
By learning and making decisions jointly, it also
serves as a singleton mention head classifier, build-
ing on insights from Recasens et al. (2013). This
joint framework aims to improve performance on
both mention head detection and on coreference.

We first describe the formualtion of the men-
tion head detection and the ILP-based mention-pair
coreference separately, and then propose the joint
head-coreference framework.

2.1 Mention Head Detection
The mention head detection model is a binary classi-
fier gm = w>1 ϕ(m), in which ϕ(m) is a feature vector
for mention head candidate m and w1 is the corre-
sponding weight vector. We identify a candidate m
as a mention head if gm > 0. The features utilized in
the vector ϕ(m) consist of: 1) Gazetteer features 2)
Part-Of-Speech features 3) Wordnet features 4) Fea-
tures from the previous and next tokens 5) Length of
mention head. 6) Normalized Pointwise Mutual In-
formation (NPMI) on the tokens across a mention
head boundary 7) Feature conjunctions. Altogether
there are hundreds of thousands of sparse features.

2.2 ILP-based Mention-Pair Coreference
Let M be the set of all mentions. We train a corefer-
ence model by learning a pairwise mention scoring
function. Specifically, given a mention-pair (u,v)
(u,v ∈M, u is the antecedent of v), we learn a left-
linking scoring function fu,v = w>2 φ(u,v), where
φ(u,v) is a pairwise feature vector and w2 is the
weight vector. The inference algorithm is inspired
by the best-left-link approach (Chang et al., 2011),
where they solve the following ILP problem:

argmax
y ∑u<v,u,v∈M

fu,vyu,v,

s.t. ∑
u<v

yu,v ≤ 1, ∀v ∈M,

yu,v ∈ {0,1} ∀u,v ∈M.

(1)

Here, yu,v = 1 iff mentions u,v are directly linked.
Thus, we can construct a forest and the mentions
in the same connected component (i.e., in the same
tree) are co-referred. For this mention-pair corefer-
ence model φ(u,v), we use the same set of features
used in Bengtson and Roth (2008).

14



2.3 Joint Inference Framework

We extend expression (1) to facilitate joint inference
on mention heads and coreference as follows:

argmax
y ∑u<v,u,v∈M

fu,vyu,v + ∑
m∈M

gmym,

s.t. ∑
u<v

yu,v ≤ 1, ∀v ∈M′,

∑
u<v

yu,v ≤ yv, ∀v ∈M′,

yu,v ∈ {0,1}, ym ∈ {0,1} ∀u,v,m ∈M′.

Here, M′ is the set of all mention head candidates.
ym is the decision variable for mention head candi-
date m. ym = 1 if and only if the mention head m
is chosen. To consider coreference decisions and
mention head decisions together, we add the con-
straint ∑u<v yu,v ≤ yv, which ensures that if a candi-
date mention head v is not chosen, then it will not
have coreference links with other mention heads.

2.4 Joint Learning Framework

To support joint learning of the parameters w1 and
w2 described above, we define a joint training objec-
tive function C(w1,w2) for mention head detection
and coreference, which uses a max-margin approach
to learn both weight vectors. Suppose we have a col-
lection of documents D, and we generate nd men-
tion head candidates for each document d (d ∈ D).
We use an indicator function δ (u,m) to represent
whether mention heads u,m are in the same corefer-
ence cluster based on gold annotations (δ (u,m) = 1
iff they are in the same cluster). Similarly, Ω(m) is
an indicator funtion representing whether mention
head m is valid in the gold annotations.

For simplicity, we first define

u′ = argmax
u<m

(w>2 φ(u,m)−δ (u,m)),
u′′ = arg max

u<m,δ (u,m)=1
w>2 φ(u,m)Ω(m).

We then minimize the following joint training ob-
jective function C(w1,w2).

C(w1,w2) =
1
|D| ∑d∈D

1
nd

∑
m

(Ccore f ,m(w2)

+Clocal,m(w1)+Ctrans,m(w1))+R(w1,w2).

C(w1,w2) is composed of four parts. The first part
is the loss function for coreference, where we have

Ccore f ,m(w2) =−w>2 φ(u′′,m)Ω(m)
+(w>2 φ(u

′,m)−δ (u′,m))(Ω(m)∨Ω(u′)).

It is similar to the loss function for a latent left-
linking coreference model5. As the second com-
ponent, we have the quadratic loss for the mention
head detection model,

Clocal,m(w1) =
1
2
(w>1 ϕ(m)−Ω(m))2.

Using the third component, we further maximize the
margin between valid and invalid mention head can-
didates when they are selected as the best-left-link
mention heads for any valid mention head. It can be
represented as

Ctrans,m(w1) =
1
2
(w>1 ϕ(u

′)−Ω(u′))2Ω(m).

The last part is the regularization term

R(w1,w2) =
λ1
2
||w1||2 + λ22 ||w2||

2.

2.5 Stochastic Subgradient Descent for Joint
Learning

For joint learning, we choose stochastic subgradi-
ent descent (SGD) approach to facilitate performing
SGD on a per mention head basis. Next, we de-
scribe the weight update algorithm by defining the
subgradients.

The partial subgradient w.r.t. mention head m for
the head weight vector w1 is given by

∇w1,mC(w1,w2) =
1
|D|nd (∇Clocal,m(w1)+∇Ctrans,m(w1))+λ1w1, (2)

where
∇Clocal,m(w1) = (w>1 ϕ(m)−Ω(m))ϕ(m),
∇Ctrans,m(w1) = (w>1 ϕ(u

′)−Ω(u′))ϕ(u′)Ω(m).
The partial subgradient w.r.t. mention head m for

the coreference weight vector w2 is given by

∇w2,mC(w1,w2) = λ2w2+
φ(u′,m)−φ(u′′,m) if Ω(m) = 1,
φ(u′,m) if Ω(m) = 0 and Ω(u′) = 1,
0 if Ω(m) = 0 and Ω(u′) = 0.

(3)

Here λ1 and λ2 are regularization coefficients
which are tuned on the development set. To learn
the mention head detection model, we consider two
different parts of the gradient in expression (2).
∇Clocal,m(w1) is exactly the local gradient of men-
tion head m while we add ∇Ctrans,m(w1) to represent

5More details can be found in Chang et al. (2013). The
difference here is that we also consider the validity of mention
heads using Ω(u),Ω(m)

15



the gradient for mention head u′, the mention head
chosen by the current best-left-linking model for m.
This serves to maximize the margin between valid
mention heads and invalid ones. As invalid mention
heads will not be linked to any other mention head,
∇trans is zero when m is invalid. When training the
mention-pair coreference model, we only consider
gradients when at least one of the two mention heads
m,u′ is valid, as shown in expression (3). When
mention head m is valid (Ω(m) = 1), the gradient
is the same as local training for best-left-link of m
(first condition in expression (3)). When m is not
valid while u′ is valid, we only demote the coref-
erence link between them (second condition in ex-
pression (3)). We consider only the gradient from
the regularization term when both m,u′ are invalid.

As mentioned before, our framework can han-
dle annotations with or without singletion mentions.
When the gold data contains no singleton mentions,
we have Ω(m) = 0 for all singleton mention heads
among mention head candidates. Then, our men-
tion head detection model partly serves as a single-
ton head detector, and tries to reject singletons in
the joint decisions with coreference. When the gold
data contains singleton mentions, we have Ω(m) = 1
for all valid singleton mention heads. Our mention
head detection model then only learns to differenti-
ate invalid mention heads from valid ones, and thus
has the ability to preserve valid singleton heads.

Most of the head mentions proposed by the al-
gorithms described in Sec. 3 are positive exam-
ples. We ensure a balanced training of the men-
tion head detection model by adding sub-sampled
invalid mention head candidates as negative exam-
ples. Specifically, after mention head candidate gen-
eration (described in Sec. 3), we train on a set of
candidates with precision larger than 50%. We then
use Illinois Chunker (Punyakanok and Roth, 2001)6

to extract more noun phrases from the text and em-
ploy Collins head rules (Collins, 1999) to identify
their heads. When these extracted heads do not
overlap with gold mention heads, we treat them as
negative examples.

We note that the aforementioned joint framework
can take as input either complete mention candi-
dates or mention head candidates. However, in this
paper we only feed mention heads into it. Our ex-
perimental results support our intuition that this pro-
vides better results.

6http://cogcomp.cs.illinois.edu/page/
software_view/Chunker

3 Mention Detection Modules

This section describes the module that generates our
mention head candidates, and then how the mention
heads are expanded to complete mentions.

3.1 Mention Head Candidate Generation

The goal of the mention head candidate genera-
tion process is to acquire candidates from multiple
sources to ensure high recall, given that our joint
framework acts as a filter and increases precision.
We view the sources as independent components
and merge all mention heads generated. A sequence
labelling component and a named entity recogni-
tion component employ statistical learning methods.
These are augmented by additional heads that we
acquire from Wikipedia and a “known heads” re-
source, which we incorporate utilizing string match-
ing algorithms.

3.1.1 Statistical Components
Sequence Labelling Component We use the fol-
lowing notations. Let O =< o1,o2, · · · ,on > repre-
sent an input token sequence over an alphabet Ω. A
mention is a substring of consecutive input tokens
mi, j =< oi,oi+1, · · · ,o j > for 1 ≤ i ≤ j ≤ n. We
consider the positions of mentions in the text: two
mentions with an identical sequence of tokens that
differ in position are considered different mentions.

The sequence labeling component builds on the
following assumption:
Assumption Different mentions have different
heads, and heads do not overlap with each other.
That is, for each mi, j, we have a corresponding
head ha,b where i ≤ a ≤ b ≤ j. Moreover, for an-
other head ha′,b′ , we have the satisfying condition
a−b′ > 0 or b−a′ < 0 ∀ha,b,ha′,b′ .

Based on this assumption, the problem of
identifying mention heads is a sequential phrase
identification problem, and we choose to em-
ploy the BILOU-representation as it has advan-
tages over traditional BIO-representation, as shown,
e.g. in Ratinov and Roth (2009). The BILOU-
representation suggests learning classifiers that
identify the Beginning, Inside and Last tokens of
multi-token chunks as well as Unit-length chunks.
The problem is then transformed into a simple, but
constrained, 5-class classification problem.

The BILOU-classifier shares all features with the
mention head detection model described in Sec. 2.1
except for two: length of mention heads and NPMI
over head boundary. For each instance, the feature

16



vector is sparse and we use sparse perceptron (Jack-
son and Craven, 1996) for supervised training. We
also apply a two layer prediction aggregation. First,
we apply a baseline BILOU-classifier, and then use
the resulting predictions as additional features in a
second level of inference to take interactions into
account in an efficient manner. A similar technique
has been applied in Ratinov and Roth (2009), and
has shown favorable results over other ”standard”
sequential prediction models.
Named Entity Recognition Component We use
existing tools to extract named entities as additional
mention head candidates. We choose the state-of-
the-art “Illinois Named Entity Tagger” package7.
It uses distributional word representations that im-
prove its generalization. This package gives the
standard Person/Location/Organization/Misc labels
and we take all output named entities as candidates.

3.1.2 Resource-Driven Matching Components

Wikipedia Many mention heads can be directly
matched to a Wikipedia title. We get 4,045,764
Wikipedia titles from Wikipedia dumps and use all
of them as potential mention heads. The Wikipedia
matching component includes an efficient hashing
algorithm implemented via a DJB2 hash function8.
One important advantage of using Wikipedia is
that it keeps updating. This component can con-
tribute steadily to ensure a good coverage of men-
tion heads. We first run this matching component
on training documents and compute the precision of
entries that appear in the text (the probability of ap-
pearing as mention heads). We then get the set of en-
tries with precision higher than a threshold α , which
is tuned on the development set using F1-score. We
use them as candidates for mention head matching.
Known Head Some mention heads appear repeat-
edly in the text. To fully utilize the training data, we
construct a known mention head candidate set and
identify them in the test documents. To balance be-
tween recall and precision, we set a parameter β > 0
as a precision threshold and only allow those men-
tion heads with precision larger than β on the train-
ing set. Please note that threshold β is also tuned on
the development set using F1-score.

We also employ a simple word variation tolerance
algorithm in our matching components, to general-
ize over small variations (plural/singular, etc.).

7http://cogcomp.cs.illinois.edu/page/
software_view/NETagger

8http://www.cse.yorku.ca/˜oz/hash.html

3.2 Mention Boundary Detection

Once the joint learning and inference process deter-
mines the set of mention heads (and their corefer-
ence chains), we extend the heads to complete men-
tions. Note that this process may not be necessary,
since in many applications, the head clusters often
provide enough information. However, for consis-
tency with existing coreference resolution systems,
we describe below how we expand the heads to
complete mentions.

We learn a binary classifier to expand mentions,
which determines if the mention head should in-
clude the token to its left and to its right. We fol-
low the notations in Sec. 2.1. We construct pos-
itive examples as (op,ha,b,dir), ∀mi, j(ha,b). Here
p ∈ {i, i + 1, · · · ,a− 1} ∪ {b + 1,b + 2, · · · , j} and
when p = i, i + 1, · · · ,a− 1, dir = L; when p =
b + 1,b + 2, · · · , j, dir = R. We construct negative
examples as (oi−1,ha,b,L) and (o j+1,ha,b,R). Once
trained, the binary classifier takes in the head, a to-
ken and the direction of the token relative to the
head, and decides whether the token is inside or out-
side the mention corresponding to the head. At test
time, this classifier is used around each confirmed
head to determine the mention boundaries. The fea-
tures used here are similar to the mention head de-
tection model described in Sec. 2.1.

4 Experiments

We present experiments on the two standard coref-
erence resolution datasets, ACE-2004 (NIST, 2004)
and OntoNotes-5.0 (Hovy et al., 2006). Our ap-
proach results in a substantial reduction in the coref-
erence performance gap between gold and pre-
dicted mentions, and significantly outperforms ex-
isting stat-of-the-art results on coreference resolu-
tion; in addition, it achieves significant performance
improvement on MD for both datasets.

4.1 Experimental Setup

Datasets The ACE-2004 dataset contains 443 doc-
uments. We use a standard split of 268 training doc-
uments, 68 development documents, and 106 test-
ing documents (Culotta et al., 2007; Bengtson and
Roth, 2008). The OntoNotes-5.0 dataset, which is
released for the CoNLL-2012 Shared Task (Prad-
han et al., 2012), contains 3,145 annotated docu-
ments. These documents come from a wide range of
sources which include newswire, bible, transcripts,
magazines, and web blogs. We report results on the
test documents for both datasets.

17



MUC B3 CEAFe AVG
GoldM/H 78.17 81.64 78.45 79.42
StanfordM 63.89 70.33 70.21 68.14
PredictedM 64.28 70.37 70.16 68.27
H-M-CorefM 65.81 71.97 71.14 69.64
H-Joint-MM 67.28 73.06 73.25 71.20
StanfordH 70.28 73.93 73.04 72.42
PredictedH 71.35 75.33 74.02 73.57
H-M-CorefH 71.81 75.69 74.45 73.98
H-Joint-MH 72.74 76.69 75.18 74.87

Table 2: Performance of coreference resolution for all sys-
tems on the ACE-2004 dataset. Subscripts (M , H ) indicate
evaluations on (mentions, mention heads) respectively. For
gold mentions and mention heads, they yield the same per-
formance for coreference. Our proposed H-Joint-M system
achieves the highest performance. Parameters of our proposed
system are tuned as α = 0.9, β = 0.8, λ1 = 0.2 and λ2 = 0.3.

The ACE-2004 dataset is annotated with both
mention and mention heads, while the OntoNotes-
5.0 dataset only has mention annotations. There-
fore, we preprocess Ontonote-5.0 to derive men-
tion heads using Collins head rules (Collins, 1999)
with gold constituency parsing information and gold
named entity information. The parsing information9

is only needed to generate training data for the men-
tion head candidate generator and named entities are
directly set as heads. We set these extracted heads
as gold, which enables us to train the two layer
BILOU-classifier described in Sec. 3.1.1. The non-
overlapping mention head assumption in Sec. 3.1.1
can be verified empirically on both ACE-2004 and
OntoNotes-5.0 datasets.
Baseline Systems We choose three publicly avail-
able state-of-the-art end-to-end coreference systems
as our baselines: Stanford system (Lee et al., 2011),
Berkeley system (Durrett and Klein, 2014) and
HOTCoref system (Björkelund and Kuhn, 2014).
Developed Systems Our developed system is built
on the work by Chang et al. (2013), using Con-
strained Latent Left-Linking Model (CL3M) as our
mention-pair coreference model in the joint frame-
work10. When the CL3M coreference system uses
gold mentions or heads, we call the system Gold;
when it uses predicted mentions or heads, we call
the system Predicted. The mention head candidate
generation module along with mention boundary
detection module can be grouped together to form
a complete mention detection system, and we call
it H-M-MD. We can feed the predicted mentions
from H-M-MD directly into the mention-pair coref-

9No parsing information is needed at evaluation time.
10We use Gurobi v5.0.1 as our ILP solver.

MUC B3 CEAFe AVG
GoldM/H 82.03 70.59 66.76 73.12
StanfordM 64.62 51.89 48.23 54.91
HotCorefM 70.74 58.37 55.47 61.53
BerkeleyM 71.24 58.71 55.18 61.71
PredictedM 69.63 57.46 53.16 60.08
H-M-CorefM 70.95 59.11 54.98 61.68
H-Joint-MM 72.22 60.50 56.37 63.03
StanfordH 68.53 56.68 52.36 59.19
HotCorefH 72.94 60.27 57.53 63.58
BerkeleyH 73.05 60.39 57.43 63.62
PredictedH 72.11 60.12 55.68 62.64
H-M-CorefH 73.22 61.42 56.21 63.62
H-Joint-MH 74.83 62.77 57.93 65.18

Table 3: Performance of coreference resolution for all sys-
tems on the CoNLL-2012 dataset. Subscripts (M , H ) indi-
cate evaluations on (mentions, mention heads) respectively. For
gold mentions and mention heads, they yield the same per-
formance for coreference. Our proposed H-Joint-M system
achieves the highest performance. Parameters of our proposed
system are tuned as α = 0.9, β = 0.9, λ1 = 0.25 and λ2 = 0.2.

erence model that we implemented, resulting in a
traditional pipelined end-to-end coreference system,
namely H-M-Coref. We name our new proposed
end-to-end coreference resolution system incorpo-
rating both the mention head candidate generation
module and the joint framework as H-Joint-M.
Evaluation Metrics We compare all systems us-
ing three popular metrics for coreference resolution:
MUC (Vilain et al., 1995), B3 (Bagga and Bald-
win, 1998), and Entity-based CEAF (CEAFe) (Luo,
2005). We use the average F1 scores (AVG) of these
three metrics as the main metric for comparison.
We use the v7.0 scorer provided by CoNLL-2012
Shared Task11. We also evaluate the mention de-
tection performance based on precision, recall and
F1 score. As mention heads are important for both
mention detection and coreference resolution, we
also report results evaluated on mention heads.

4.2 Performance for Coreference Resolution
Performance of coreference resolution for all sys-
tems on the ACE-2004 and CoNLL-2012 datasets is
shown in Table 2 and Table 3 respectively.12 These
results show that our developed system H-Joint-M

11The latest scorer is version v8.01, but MUC, B3, CEAFe
and CoNLL average scores are not changed. For evaluation on
ACE-2004, we convert the system output and gold annotations
into CoNLL format.

12We do not provide results from Berkeley and HOTCoref on
ACE-2004 dataset as they do not directly support ACE input.
Results for HOTCoref are slightly different from the results re-
ported in Björkelund and Kuhn (2014). For Berkeley system,
we use the reported results from Durrett and Klein (2014).

18



shows significant improvement on all metrics for
both datasets. Existing systems only report results
on mentions. Here, we also show their performance
evaluated on mention heads. When evaluated on
mention heads rather than mentions13, we can al-
ways expect a performance increase for all systems
on both datasets. Even though evaluating on men-
tions is more common in the literature, it is often
enough to identify just mention heads in corefer-
ence chains (as shown in the example from Sec.
1). H-M-Coref can already bring substantial perfor-
mance improvement, which indicates that it is help-
ful for coreference to just identify high quality men-
tion heads. Our proposed H-Joint-M system out-
performs all baselines and achieves the best results
reported so far.

4.3 Performance for Mention Detection

The performance of mention detection for all sys-
tems on the ACE-2004 and CoNLL-2012 datasets
is shown in Table 4. These results show that our
developed system exhibits significant improvement
on precision and recall for both datasets. H-M-MD
mainly improves on recall, indicating, as expected,
that the mention head candidate generation mod-
ule ensures high recall on mention heads. H-Joint-
M mainly improves on precision, indicating, as ex-
pected, that the joint framework correctly rejects
many of the invalid mention head candidates during
joint inference. Our joint model can adapt to anno-
tations with or without singleton mentions. Based
on training data, our system has the ability to pre-
serve true singleton mentions in ACE while reject-
ing many singleton mentions in OntoNotes14. Note
that we have better mention detection results on
ACE-2004 dataset than on OntoNotes-5.0 dataset.
We believe that this is due to the fact that extract-
ing mention heads in the OntoNotes dataset is some-
what noisy.

4.4 Analysis of Performance Improvement

The improvement of our H-Joint-M system is due to
two distinct but related modules: the mention head
candidate generation module (“Head”) and the joint
learning and inference framework (“Joint”).15 We

13Here, we treat mention heads as mentions. Thus, in the
evaluation script, we set the boundary of a mention to be the
boundary of its correponding mention head.

14Please note that when evaluating on OntoNotes, we even-
tually remove all singleton mentions from the output.

15“Joint” rows are computed as “H-Joint-M” rows minus
“Head” rows. They reflect the contribution of the joint frame-
work to mention detection (by rejecting some mention heads).

Systems Precision Recall F1-score
ACE-2004

PredictedM 75.11 73.03 74.06
H-M-MDM 77.45 92.97 83.90
H-Joint-MM 85.34 91.73 88.42
PredictedH 76.84 86.99 79.87
H-M-MDH 80.82 93.45 86.68
H-Joint-MH 88.85 92.27 90.53

CoNLL-2012
PredictedM 65.28 63.41 64.33
H-M-MDM 70.09 76.72 73.26
H-Joint-MM 78.51 75.52 76.99
PredictedH 76.38 74.02 75.18
H-M-MDH 77.73 83.99 80.74
H-Joint-MH 85.07 82.31 83.67

Table 4: Performance of mention detection for all systems
on the ACE-2004 and CoNLL-2012 datasets. Subscripts (M ,
H ) indicate evaluations on (mentions, mention heads) respec-
tively. Our proposed H-Joint-M system dramatically improves
the MD performance.

evaluate the effect of these two modules in terms
of Mention Detection Error Reduction (MDER) and
Performance Gap Reduction (PGR) for coreference.
MDER is computed as the ratio of performance im-
provement for mention detection over the original
mention detection error rate, while PGR is com-
puted as the ratio of performance improvement for
coreference over the performance gap for corefer-
ence. Results on the ACE-2004 and CoNLL-2012
datasets are shown in Table 5.16

The mention head candidate generation module
has a bigger impact on MDER compared to the joint
framework. However, they both have the same level
of positive effects on PGR for coreference resolu-
tion. On both datasets, we achieve more than 20%
performance gap reduction for coreference.

5 Related Work

Coreference resolution has been extensively stud-
ied, with several state-of-the-art approaches ad-
dressing this task (Lee et al., 2011; Durrett and
Klein, 2013; Björkelund and Kuhn, 2014; Song et
al., 2012). Many of the early rule-based systems
like Hobbs (1978) and Lappin and Leass (1994)
gained considerable popularity. The early designs
were easy to understand and the rules were designed
manually. Machine learning approaches were intro-
duced in many works (Connolly et al., 1997; Ng and

16We use bootstrapping resampling (10 times from the test
data) with signed rank test. All the improvements shown are
statistically significant.

19



ACE-2004 MDER PGR(AVG)
HeadM 37.93 12.29
JointM 17.43 13.99
H-Joint-MM 55.36 26.28
HeadH 34.00 7.01
JointH 19.22 15.21
H-Joint-MH 53.22 22.22
CoNLL-2012 MDER PGR(AVG)
HeadM 25.04 12.16
JointM 10.45 10.44
H-Joint-MM 35.49 22.60
HeadH 22.40 10.58
JointH 11.81 13.75
H-Joint-MH 34.21 24.33

Table 5: Analysis of performance improvement in terms
of Mention Detection Error Reduction (MDER) and Perfor-
mance Gap Reduction (PGR) for coreference resolution on
the ACE-2004 and CoNLL-2012 datasets. “Head” represents
the mention head candidate generation module, “Joint” repre-
sents the joint learning and inference framework, and “H-Joint-
M” indicates the end-to-end system.

Cardie, 2002; Bengtson and Roth, 2008; Soon et al.,
2001). The introduction of ILP methods has influ-
enced the coreference area too (Chang et al., 2011;
Denis and Baldridge, 2007). In this paper, we use
the Constrained Latent Left-Linking Model (CL3M)
described in Chang et al. (2013) in our experiments.

The task of mention detection is closely related
to Named Entity Recognition (NER). Punyakanok
and Roth (2001) thoroughly study phrase identifica-
tion in sentences and propose three different general
approaches. They aim to learn several different lo-
cal classifiers and combine them to optimally satisfy
some global constraints. Cardie and Pierce (1998)
propose to select certain rules based on a given
corpus, to identify base noun phrases. However,
the phrases detected are not necessarily mentions
that we need to discover. Ratinov and Roth (2009)
present detailed studies on the task of named entity
recognition, which discusses and compares different
methods on multiple aspects including chunk repre-
sentation, inference method, utility of non-local fea-
tures, and integration of external knowledge. NER
can be regarded as a sequential labeling problem,
which can be modeled by several proposed mod-
els, e.g. Hidden Markov Model (Rabiner, 1989) or
Conditional Random Fields (Sarawagi and Cohen,
2004). The typical BIO representation was intro-
duced in Ramshaw and Marcus (1995); OC repre-
sentations were introduced in Church (1988), while
Finkel and Manning (2009) further study nested
named entity recognition, which employs a tree

structure as a representation of identifying named
entities within other named entities.

The most relevant study on mentions in the con-
text of coreference was done in Recasens et al.
(2013); this work studies distinguishing single men-
tions from coreferent mentions. Our joint frame-
work provides similar insights, where the added
mention decision variable partly reflects if the men-
tion is singleton or not.

Several recent works suggest studying corefer-
ence jointly with other tasks. Lee et al. (2012)
model entity coreference and event coreference
jointly; Durrett and Klein (2014) consider joint
coreference and entity-linking. The work closest
to ours is that of Lassalle and Denis (2015), which
studies a joint anaphoricity detection and corefer-
ence resolution framework. While their inference
objective is similar, their work assumes gold men-
tions are given and thus their modeling is very dif-
ferent.

6 Conclusion

This paper proposes a joint inference approach to
the end-to-end coreference resolution problem. By
moving to identify mention heads rather than men-
tions, and by developing an ILP-based, joint, online
learning and inference approach, we close a signif-
icant fraction of the existing gap between corefer-
ence systems’ performance on gold mentions and
their performance on raw data. At the same time,
we show substantial improvements in mention de-
tection. We believe that our approach will gener-
alize well to many other NLP problems, where the
performance on raw data (the result that really mat-
ters) is still significantly lower than the performance
on gold data.

Acknowledgments

This work is partly supported by NSF grant #SMA
12-09359 and by DARPA under agreement number
FA8750-13-2-0008. The U.S. Government is autho-
rized to reproduce and distribute reprints for Gov-
ernmental purposes notwithstanding any copyright
notation thereon. The views and conclusions con-
tained herein are those of the authors and should
not be interpreted as necessarily representing the of-
ficial policies or endorsements, either expressed or
implied, of DARPA or the U.S. Government.

20



References
A. Bagga and B. Baldwin. 1998. Algorithms for scoring

coreference chains. In In The First International Con-
ference on Language Resources and Evaluation Work-
shop on Linguistics Coreference.

E. Bengtson and D. Roth. 2008. Understanding the value
of features for coreference resolution. In EMNLP.

A. Björkelund and J Kuhn. 2014. Learning structured
Perceptrons for coreference resolution with latent an-
tecedents and non-local features. In ACL.

C. Cardie and D. Pierce. 1998. Error-driven pruning of
Treebanks grammars for base noun phrase identifica-
tion. In Proceedings of ACL-98.

K. Chang, R. Samdani, A. Rozovskaya, N. Rizzolo,
M. Sammons, and D. Roth. 2011. Inference proto-
cols for coreference resolution. In CoNLL.

K.-W. Chang, R. Samdani, and D. Roth. 2013. A con-
strained latent variable model for coreference resolu-
tion. In EMNLP.

K. Church. 1988. A stochastic parts program and noun
phrase parser for unrestricted text. In ANLP.

M. Collins. 1999. Head-driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, Computer
Science Department, University of Pennsylvenia.

D. Connolly, J. Burger, and D. Day. 1997. A machine
learning approach to anaphoric reference. In New
Methods in Language Processing.

A. Culotta, M. Wick, and A. McCallum. 2007. First-
order probabilistic models for coreference resolution.
In NAACL.

P. Denis and J. Baldridge. 2007. Joint determination of
anaphoricity and coreference resolution using integer
programming. In NAACL.

G. Durrett and D. Klein. 2013. Easy victories and uphill
battles in coreference resolution. In EMNLP.

G. Durrett and D. Klein. 2014. A joint model for entity
analysis: Coreference, typing, and linking.

J. Finkel and C. Manning. 2009. Nested named entity
recognition. In EMNLP.

J. R. Hobbs. 1978. Resolving pronoun references. Lin-
gua.

E. Hovy, M. Marcus, M. Palmer, L. Ramshaw, and
R. Weischedel. 2006. Ontonotes: The 90% solution.
In Proceedings of HLT/NAACL.

J. Jackson and M. Craven. 1996. Learning sparse per-
ceptrons. Proceedings of the 1996 Advances in Neural
Information Processing Systems.

S. Lappin and H. Leass. 1994. An algorithm for pronom-
inal anaphora resolution. Computational linguistics.

E. Lassalle and P. Denis. 2015. Joint anaphoricity detec-
tion and coreference resolution with constrained latent
structures.

H. Lee, Y. Peirsman, A. Chang, N. Chambers, M. Sur-
deanu, and D. Jurafsky. 2011. Stanford’s multi-
pass sieve coreference resolution system at the conll-
2011 shared task. In Proceedings of the CoNLL-2011
Shared Task.

H. Lee, M. Recasens, A. Chang, M. Surdeanu, and D. Ju-
rafsky. 2012. Joint entity and event coreference reso-
lution across documents. In EMNLP.

X. Luo. 2005. On coreference resolution performance
metrics. In EMNLP.

V. Ng and C. Cardie. 2002. Improving machine learning
approaches to coreference resolution. In ACL.

NIST. 2004. The ACE evaluation plan.

S. Pradhan, A. Moschitti, N. Xue, O. Uryupina, and
Y. Zhang. 2012. CoNLL-2012 shared task: Modeling
multilingual unrestricted coreference in OntoNotes.
In CoNLL.

V. Punyakanok and D. Roth. 2001. The use of classifiers
in sequential inference. In NIPS.

L. R. Rabiner. 1989. A tutorial on hidden Markov mod-
els and selected applications in speech recognition.
Proceedings of the IEEE.

L. A. Ramshaw and M. P. Marcus. 1995. Text chunking
using transformation-based learning. In Proceedings
of the Third Annual Workshop on Very Large Corpora.

L. Ratinov and D. Roth. 2009. Design challenges and
misconceptions in named entity recognition. In Proc.
of the Annual Conference on Computational Natural
Language Learning (CoNLL).

M. Recasens, M.-C. de Marneffe, and C. Potts. 2013.
The life and death of discourse entities: Identifying
singleton mentions. In NAACL.

D. Roth and W. Yih. 2004. A linear programming
formulation for global inference in natural language
tasks. In Hwee Tou Ng and Ellen Riloff, editors,
CoNLL.

S. Sarawagi and W. Cohen. 2004. Semi-Markov con-
ditional random fields for information extraction. In
NIPS.

Y. Song, J. Jiang, W.-X. Zhao, S. Li, and H. Wang. 2012.
Joint learning for coreference resolution with markov
logic. In Proceedings of the 2012 Joint Conference of
EMNLP-CoNLL.

W. M. Soon, H. T. Ng, and D. C. Y. Lim. 2001. A ma-
chine learning approach to coreference resolution of
noun phrases. Comput. Linguist.

M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and
L. Hirschman. 1995. A model-theoretic coreference
scoring scheme. In Proceedings of the 6th conference
on Message understanding.

21


