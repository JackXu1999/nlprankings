



















































Entity-balanced Gaussian pLSA for Automated Comparison


Proceedings of NAACL-HLT 2016, pages 69–79,
San Diego, California, June 12-17, 2016. c©2016 Association for Computational Linguistics

Entity-balanced Gaussian pLSA for Automated Comparison

Danish Contractor∗
IIT Delhi & IBM Research

New Delhi, India
dcontrac@in.ibm.com

Mausam and Parag Singla
IIT Delhi

New Delhi, India
{mausam,parags}@cse.iitd.ac.in

Abstract
Community created content (e.g., product de-
scriptions, reviews) typically discusses one
entity at a time and it can be hard as well
as time consuming for a user to compare
two or more entities. In response, we de-
fine a novel task of automatically generating
entity comparisons from text. Our output is
a table that semantically clusters descriptive
phrases about entities. Our clustering algo-
rithm is a Gaussian extension of probabilis-
tic latent semantic analysis (pLSA), in which
each phrase is represented in word vector em-
bedding space. In addition, our algorithm at-
tempts to balance information about entities in
each cluster to generate meaningful compar-
ison tables, where possible. We test our sys-
tem’s effectiveness on two domains, travel ar-
ticles and movie reviews, and find that entity-
balanced clusters are strongly preferred by
users.

1 Introduction

The proliferation of Web 2.0 has enabled ready ac-
cess to large amounts of community created content,
such as status messages, blogs, wikis, and reviews.
These form an important source of knowledge in
our day to day decision making, such as deciding
which restaurant to try, or which movie to watch,
or which city to visit etc. Unfortunately, such con-
tent typically focuses on one real world entity at
a time, whereas, a user deciding between alterna-
tives is most interested in a comparative analysis of
strengths and weaknesses of each.

∗This work was carried out as part of PhD research at IIT
Delhi. The author is also a regular employee at IBM Research.

There have been some recent attempts to create
comparisons using expert knowledge, but generat-
ing such comparisons manually does not scale –
even pairwise comparisons are quadratic in the num-
ber of entities. Few automated comparisons for spe-
cific products with pre-defined attributes (e.g., lap-
tops, cameras) exist; they are typically powered by
existing structured knowledge bases. To the best of
our knowledge, prior work on automatically gener-
ating comparisons for arbitrary domains from un-
structured text, does not exist.

We define a novel task of generating entity com-
parisons from textual corpora in which each docu-
ment describes one entity at a time. For broad appli-
cability, we do not restrict ourselves to a pre-defined
ontology; instead, we use textual phrases that de-
scribe entities as our unit of information. We call
these descriptive phrases – they encompass general
attribute-value phrases, opinion phrases, and other
descriptions of the facets of an entity. We gener-
ate entity comparisons in a tabular form where the
phrases are organized semantically, thus, allowing
for direct comparisons. Figure 1 shows a sample city
comparison generated by our system for tourism.

Our comparison generation algorithm extracts de-
scriptive phrases per entity and clusters them into
semantic groups. We perform clustering via a topic
model, where phrases from an entity are combined
into one document. The topics identify prominent
facets of the entities. Unfortunately, since the num-
ber of entities being compared is usually small, just
statistical co-occurrence of words and phrases is not
sufficient to identify good topics. In response, we
use vector embeddings of descriptive phrases and

69



employ a Gaussian extension of probabilistic latent
semantic analysis (pLSA) over these vectors.

We also modify Gaussian pLSA to additionally
incorporate an entity-balance term, preferring topics
in which phrases from the entities are represented in
a proportionate measure. The balance term trades off
the discovery of unique facets for each entity with
that of common facets. This enables direct compari-
son between entities leading to an overall improved
comparison table. Since the balance term is only a
preference (not a constraint), it still allows the al-
gorithm to exhibit clusters which may be sparsely
represented (or not represented at all) in one of the
entities.

We demonstrate the usefulness of our ideas on
two domains – tourism and movies. Based on user
experiments, we find that the entity-balanced model
outputs much better comparisons as compared to an
entity-oblivious model such as GMM. In summary,
our paper makes the following contributions:
• We define a novel task of generating entity

comparisons from a corpus that describes en-
tities individually.

• We present the first system to output such
a comparison. Our system runs Gaussian
pLSA over the vector embeddings of extracted
phrases, and preferentially tries to balance the
entities in each topic.

• Human subject evaluations using Amazon
Mechanical Turk (AMT) demonstrate that
AMT workers overwhelmingly prefer compar-
isons generated using entity-balanced Gaussian
pLSA compared to entity-oblivious clustering.

2 Related Work

Recently, the internet has seen a growth in websites
offering comparisons for different entities. Prod-
uct websites such as eBay maintain comparisons
for products. Google also outputs pre-built compar-
isons between common entities when queried with
the word “vs.” between them. Both of these output
purely structured attribute-value information and are
unable to compare along more qualitative and de-
scriptive dimensions such as ease of living or qual-
ity of nightlife when comparing cities, for exam-
ple. Other websites such as WikiVS1 contain user-

1http://www.wikivs.com/wiki/Main Page

Cluster Granada (Spain) New York City (U.S.)Labels

art,
moorish architecture contemporary art
religious art modern american art

arch. fine art medieval art
beautiful architecture egyptian art

palace,
brick-walled courtyard
lovely courtyard area

courtyard nasrid royal palace
alhambra palace

museum,
alhambra museum fine art museums
archaeological museum guggenheim museum

finest world heritage site islamic art collection
splendid arabic shops metropolitan museum

gardens,
partal gardens flushing meadows park
palace gardens central park

park pleasant gardens renowned gardens
moorish style gardens natl. recreational area

Figure 1: Sample comparison (abridged) between Granada and
New York generated by our system. A quick look reveals that

that both cities have a nice set of museums and gardens to visit,

while palaces and courtyards are only in Granada. Granada’s

art and architecture are more ornamental, whereas New York’s

might be more contemporary.

contributed comparisons that have been categorized
based on the nature of the entities being compared.
These are manually curated and therefore do not
scale to the quadratic number of entity pairs.

Perhaps the most closely related work to ours is
the field of contrastive opinion mining and summa-
rization (Kim et al., 2011; Liu and Zhang, 2012).
Examples include extraction of contrastive senti-
ments on a product (Lerman and McDonald, 2009)
and summarization of opinionated political articles
(Paul et al., 2010). Contrastive opinion mining ex-
tracts contrasting view points about a single entity
or event instead of comparing multiple ones. A re-
cent preliminary study extends this for comparing
reviews of two products (Sipos and Joachims, 2013).
It uses a supervised method for learning sentence
alignments per product-type, and does not organize
various opinions for an entity via clustering.

Other related work includes comparative text
mining tasks where document collections are ana-
lyzed to extract shared topics or themes (Zhai et al.,
2004). Since such methods only identify latent top-
ics for the full document collection, they can’t be
directly used for a specific comparison task.

Since our system is a combination of IE and clus-
tering, we briefly describe related approaches for
these subtasks.

Information Extraction: Our work is related to

70



the vast literature in information extraction, in par-
ticular Open IE (Banko et al., 2007). Our use of
POS patterns for extracting domain-specific descrip-
tive phrases is similar in spirit to ReVerb’s pat-
terns for relation extraction (Etzioni et al., 2011) and
adjective-noun bigrams for fine grained attribute ex-
traction (Huang et al., 2012; Yatani et al., 2011).
Adapting the literature on entity set expansion (Pan-
tel et al., 2009; Voorhees, 1994; Natsev et al., 2007),
our system expands seed nouns for broader cover-
age. We use Wordnet and distributional similarity-
based approaches for this (Curran, 2003; Voorhees,
1994).

Clustering: Our entity-balanced clustering algo-
rithm is related but different from previous work
on balanced clustering. Prior work (Banerjee and
Ghosh, 2006; Yuepeng et al., 2011) has focused on
generating different clusters to be equi-sized. Other
work (Zhu et al., 2010; Ganganath et al., 2014) en-
forces size constraints on clusters. Our idea of bal-
ance, on the other hand, is targeted towards a better
comparison and prefers that entities are well repre-
sented (balanced) in each cluster.

3 Task & System Description

Our motivation is to concisely compare two or more
entities to aid a user’s decision making. We make
several choices in our task definition to help with this
goal. First, we decide to output comparisons using a
succinct tabular representation (see Figure 1). It has
higher information density compared to, say, writing
a natural language comparison summary.

Second, our unit of information is a descriptive
phrase. We define it as any short phrase that de-
scribes an entity – these include attribute-value pairs
(e.g., “Greek art”), opinion phrases (e.g., “spectacu-
lar views”), as well as other descriptions (e.g., “old-
est church of Europe”).

Third, for better readability, our table must or-
ganize the information coherently along various as-
pects relevant for a comparison. We achieve this by
grouping related descriptive phrases. The choice of
aspects should be dependent on the specific entities
being compared, e.g., the facet of “beaches” may
split into “water activities” and “beach types” for
Jamaica v.s. Hawaii, but not for San Francisco v.s.
Bombay.

Moreover, comparisons are meant to highlight
both the similarities and the differences between en-
tities. We therefore need to trade-off the discovery
of unique facets of an entity with those which are
common to the entities being compared. Thus, while
clusters that balance the entities are preferable, it is
also acceptable to have clusters where one of the en-
tities is sparsely represented (or not represented at
all). This would happen in situations where that en-
tity does not express a particular aspect and other
entities do. Comparisons must trade off semantic co-
herence of facets with entity-balance in each facet.

Last, but not the least, since the comparisons are
targeted to aiding user’s decision making, under-
standing her intent is important. As an example, the
user may be interested in city-comparison for the
purpose of tourism, or for choosing a city to live in.
Descriptive phrases for the former could be related
to sightseeing, shopping, etc., but for the latter they
may cover aspects such as living expenses, trans-
portation, and pollution. We accommodate this ne-
cessity by allowing minimal human supervision for
specifying user intent. This supervision can come in
forms such as an intent-relevant seed noun list, or
topic-level annotation following unsupervised topic
modeling, etc. This supervision further guides de-
scriptive phrase extraction.

System Architecture: Our system consists of a
pipeline of information extraction, clustering, clus-
ter labeling and phrase ordering. IE extracts descrip-
tive phrases relevant to user-intent and we develop a
new clustering algorithm that produces better com-
parisons by balancing the entities in each cluster. We
identify cluster labels based on the most frequent
words in a cluster. We order phrases within a clus-
ter based on the distance from the centroid. We now
describe our IE and clustering techniques in detail.

3.1 Information Extraction

Our IE pipeline works in two steps. We first extract
descriptive phrases via POS patterns and then fil-
ter out the non-topical phrases. For filtering, first we
create a seed list of relevant nouns via minimal hu-
man supervision, which are then expanded by item-
set expansion. Descriptive phrases with a noun in the
expanded list are retained, and rest are filtered.

Preliminary analysis on a devset revealed that

71



a large fraction of descriptive phrases are noun
phrases (NPs). We first extract all NP chunks from
the collection and, additionally, using POS tags, ex-
tract any adjective-noun bigrams that are part of a
bigger NP chunk, or missed due to chunking errors.
This forms the initial set of descriptive phrases.

Filtering for User Intent:
These descriptive phrases include those that are

not relevant for user intent such as “excellent
schools” for tourism. We filter these phrases by
matching them to a list of intent-specific nouns. This
list is created by first curating a seed list and then ex-
panding it using item-set expansion. We employ two
methods to obtain a seed list for specifying user in-
tent: (1) a list of user-specified seed nouns, and (2) a
labeling of LDA topics based on top words in each
topic.

In the first approach we get the seed nouns di-
rectly from the domain expert. Our system sup-
ports the process by identifying frequent nouns and
showing those to the annotator to annotate. For our
tourism system, an author spent about three hours to
produce a list of 100 seed nouns.

Since this process requires significant effort per
user intent, we also investigate a semi-automatic ap-
proach in which we run Latent Dirichlet Allocation
(LDA) (Blei et al., 2003) on the whole phrase list.
We then show the top 20 words in each topic and
ask the annotator to provide only topic-level annota-
tions. We treat the top 15 words from each positively
labeled topic to be in the seed set. Since the number
of topics is usually not that large, this significantly
reduces the time required for annotation. E.g., we
ran LDA with 20 topics and it took about 10 mins.
for an author to annotate them. However, the seed
nouns are noisier due to noise in LDA.

Seed List Expansion: Finally, we use ideas from
item-set expansion to expand the seed list for im-
proved coverage. We implement two approaches for
this step. In the first method (WN) we use Wordnet
(Miller, 1995) to include words that are a direct hop
away from the seed nouns. In the second approach
(WV), we use word-vector embeddings (Collobert
et al., 2011) and include top 10 neighbors of each
seed in our expanded list. The expansions capture
near-synonyms and topically related words.

IE Experiments: We now present comparisons of

Method Prec. Recall F1
All nouns 0.53 0.67 0.59
Seed Nouns only (Manual) 0.77 0.32 0.44
Seed (Manual) + WN 0.71 0.35 0.46
Seed (Manual) + WV 0.70 0.40 0.49
Seed Nouns only (LDA) 0.74 0.19 0.30
Seed (LDA) + WN 0.74 0.20 0.31
Seed (LDA) + WV 0.74 0.26 0.38

Table 1: Quality of extracted descriptive phrases on a devset

various IE methods on a small development set. We
selected seven WikiTravel2 articles (each article is
on one city) and manually annotated an exhaustive
set of descriptive phrases. This forms our devset for
IE comparisons.

We chose various parameters in our IE systems so
that our precision never drops below 0.70. For exam-
ple, we used K=15 for choosing the top words from
LDA into seed list. We use this target precision, be-
cause we believe that for any human-facing system
the precision needs to be high for it to be considered
acceptable by people.

Table 1 compares the performance of the various
IE methods. Not surprisingly, we find that manual
seed lists obtain a much higher recall as compared to
LDA seeds, at approximately the same level of pre-
cision. Both Wordnet and word-vector improve the
recall substantially, though vectors are more effec-
tive. The recall of all nouns is only 0.67 because a
large number of descriptive phrases were larger n-
grams (not just adjective-noun bigrams) and were
missed due to chunking errors.

3.2 Building Clusters for Comparison

Our next task is to construct meaningful compar-
isons using these phrases. A useful comparison of
entities should organize the available information in
a way that is easy to comprehend by the user. To-
wards this goal, we group the related descriptive
phrases across a number of clusters. But simply hav-
ing a good clustering of descriptive phrases may not
be enough. We would like to have a clustering that
explicitly captures the individual characteristics of
each of the entities as well as makes the relative
strengths and weaknesses of each entity apparent.
For example, Figure 2 (Right) shows three different
clusterings of phrases from two cities; phrases from
each city are in a different color. Here, the third clus-

2www.wikitravel.com

72



x

z

e

x

z q

m

|E|

|z|

(i) (ii)

f

S

q

m

S
|E|

|z|

|E|

white sand beaches 
beach resorts 
secluded beaches

sandy beaches 

beach volleyball

wind surfing
scuba diving

fast moving rivers 
strong currents

serene lakes 
fresh water lake

(a) Entity Oblivious Clusters
(GMM)

Cluster 1 Cluster 2

white sand beaches 
beach resorts 
secluded beaches

sandy beaches 

beach volleyball

wind surfing
scuba diving

paddle boats 
water activities

fast moving rivers 
strong currents

serene lakes 
fresh water lake

jet skiing 
fishing spot

(b) Entity Aware Clusters
(G-pLSA)

Beach Related 
clusters Lake/River Related 

cluster

paddle boats 
water activities

jet skiing 
fishing spot

white sand beaches 
beach resorts 
secluded beaches

sandy beaches 

beach volleyball

wind surfing
scuba diving

fast moving rivers 
strong currents

serene lakes 
fresh water lake

(c) Entity Balanced clusters
(EB G-pLSA)

Water body 
clustersWater activity 

related cluster

paddle boats 
water activities

jet skiing 
fishing spot

|Xj|
|Xj|

Figure 2: Left: Plate Notation of (i) Standard Gaussian Mixture Model (ii) Gaussian pLSA (and entity balanced Gaussian pLSA)
Right: Three alternative clusterings (a), (b), (c) for descriptive phrases from two cities – each color is a different city. We prefer
clusters shown in (c) as they balance information from both entities

tering is most appropriate for comparison, because
not only is it a good clustering of descriptive phrases
from each city considered separately, but the clusters
produced also have entity-balance, i.e., the clusters
produced have a good balance of both cities; both of
these are key elements of comparison.

We first observe that a topic model such as Prob-
abilistic Latent Semantic Analysis (pLSA) is a good
fit to our clustering problem. In pLSA documents
are characterized as mixtures of topics and topics as
distributions over words. For our problem, we could
combine all phrases for an entity into one document,
and run pLSA to identify a coherent set of topics,
which can then be used as clusters. Such a model
will allow different entities to express topics in dif-
ferent proportions.

We note that LDA, which is a strict generaliza-
tion of pLSA3, is, in general, not a good fit for our
task. LDA typically uses a sparse Dirichlet prior on
document-topic distribution, which would not be ap-
propriate since for comparison we would like to rep-
resent each entity in as many topics as possible.

Unfortunately, a direct application of pLSA may
not yield good results. This is because typically the
number of entities being compared (i.e., the number
of documents in pLSA) is very small (often 2), there-
fore, there isn’t enough statistical regularity to find
good coherent topics. The alternative proposition of
learning topics on the whole corpus isn’t very ap-
pealing either, since that will learn global topics and
not the topics particularly meaningful for the current
comparison at hand.

3LDA with uniform Dirichlet prior is equivalent to pLSA

In response, we exploit the availability of pre-
trained word vectors as a source of background se-
mantic knowledge for every phrase, and generalize
the pLSA model to Gaussian pLSA (G-pLSA). We
construct a vector representation for each descriptive
phrase by averaging the word-vectors of individual
words in a phrase (Mikolov et al., 2013)4. Thus, this
model is pLSA with each topic-word distribution
represented as a Gaussian distribution over descrip-
tive phrases in the embedding space. This model
is also similar to the recently introduced Gaussian
LDA model (Das et al., 2015), but without LDA’s
Dirichlet priors as discussed above.

Gaussian pLSA has several advantages for our
task. First, it can meaningfully learn topics only for
the entities being compared, instead of needing to
learn a global topic model over the whole corpus.
Second, due to additional context from word vec-
tors, the topics are expected to be much more coher-
ent compared to traditional topic models for cases
when the underlying corpus is small, as in our case.
Finally, in our model the vectors are generated from
a Gaussian distribution and that helps capture the
theme of the cluster directly by enabling a centroid
computation in the embedding space. This is espe-
cially useful for identifying and ranking important
descriptive phrases per cluster while generating the
comparison table.

Let x(i)j , z
(i)
j denote the values of the i

th phrase
and the corresponding cluster (topic) id, respec-
tively, for the jth entity ej . Then, the log-likelihood

4We use the pre-trained 300 dimension vectors available at
http://code.google.com/p/word2vec/

73



L(Θ) of the observed data can be written as:

|E|∑
j=1

|Xj |∑
i=1

log
[ |Z|∑
z
(i)
j =1

P (x
(i)
j |z(i)j ; Θ) ·P (z(i)j |ej ; Θ) ·P (ej ; Θ)

]
(1)

Here, |Xj | and |Z| are the total number of phrases
and clusters5 respectively, for a given entity ej and,
|E| is the total number of entities being considered
for comparison. Θ denotes the vector of all the pa-
rameters. We optimize the expression L(Θ) using
EM and estimate the parameters of the model. As
can be seen, the clusters are shared across entities,
and the phrases generated are independent of the en-
tity given, a cluster and the entities themselves are
free to exhibit clusters in different proportions.

We also note just as pLSA can be seen as a nat-
ural extension of mixture of unigrams (Blei et al.,
2003), Gaussian pLSA is an extension from the
Gaussian Mixture Model (GMM) which is entity-
oblivious. GMM generates each phrase independent
of the entity it came from and hence, distributes en-
tity phrases arbitrarily across clusters. We use GMM
as a baseline for our experiments. Figure 2 (left) il-
lustrates the two models in plate notation.
Entity-Balanced Gaussian pLSA: Vanilla Gaus-
sian pLSA may not always lead to a good clustering
for comparison since the expression above does not
involve any term to balance the entity-information in
clusters, as motivated earlier. Thus, we incorporate a
regularizer term to have a good balance (proportion)
of entities in each cluster (see Figure 2 (right) (c))
resulting in our final model for comparison called
Entity-Balanced Gaussian pLSA (EB G-pLSA). The
plate notation for EB G-pLSA is identical to G-
pLSA.

Our regularizer is a function of the KL-divergence
between multinomial distributions for every pair
of entities. KL-divergence KL(P ||Q) between two
discrete distributions P (x) and Q(x) is defined as∑

l P (xl)log
(
P (xl)
Q(xl)

)
. Its an asymmetric measure

of similarity and is equal to 0 when the two dis-
tributions are identical (and greater than 0 oth-
erwise). Symmetric KL-divergence is defined as
Sym-KL(P,Q) = KL(P ||Q) +KL(Q||P ).

Let Pθj (z|ej) and Pθk(z|ek) denote the multi-
5Note that number of clusters for all entities will be the same

i.e |Zj | = Z for all j

nomial distributions for generating the cluster id z
given the entities ej and ek, respectively. Here, θj
and θk denote the respective multinomial parame-
ters. We add a regularizer term to the log-likelihood
minimizing the sum of symmetric KL-divergence
between the distributions Pθj (z|ej) and Pθk(z|ek)
for every pair of entities ej and ek. Adding this reg-
ularizer requires the multinomial distributions to be
similar to each other, thereby preferring balanced
clusters over unbalanced ones. Our regularized av-
erage log-likelihood can be written as:

Lavgreg (Θ) =
1

|M |L(Θ)−α ·
 |E|∑
j,k=1|j<k

Sym-KL(Pθj , Pθk )


(2)

L(Θ) is the total log-likelihood as defined in the pre-
vious equation. M =

∑|E|
j=1 |Xj | and |E| is the to-

tal number of entities being compared. α is a con-
stant controlling the weight of the regularizer. Note
that we add the regularizer term to the average log-
likelihood (instead of the total log-likelihood) in or-
der to have the same regularizer value for compar-
isons having varying number of data points (descrip-
tive phrases). This is important to obtain a single
value of α which would work well across different
entity comparisons. In our experiments, α was tuned
using held-out data and was found to be robust to
small perturbations.

We use standard EM to optimize the regularized
log-likelihood. Since the regularizer does not have
any hidden variables, E-step is identical to the one
for the unregularized case. During M -step, the val-
ues maximizing the mean parameters µz and the φ
parameter can be obtained analytically. There is no
closed form solution for the parameters θj , θk. We
perform gradient descent to optimize these parame-
ters during the M -step. In our experiments, we did
not estimate the co-variance matrices Σz and kept
them fixed as a diagonal matrix with the diagonal
entry (variance) being 0.1. We did not learn the co-
variance matrices as that would have increased the
number of parameters substantially, and thus, had
the danger of over fitting. The small value of the
variance chosen was to ensure less overlap between
different clusters.
Clustering Experiments We conducted preliminary
experiments to compare the performance of GMM
(vanilla Gaussian mixture modeling using word vec-

74



GMM G-pLSA EB G-pLSA
f-measure 0.42 0.43 0.44
pairwise accuracy 0.66 0.65 0.76
Table 2: Comparing clustering methods on development set

tors) with G-pLSA and EB G-pLSA on a develop-
ment set consisting of 5 random city pairs. The de-
scriptive phrases were constructed using the auto-
mated seed list as described in IE Section. We manu-
ally created the gold standard clusterings. The num-
ber of clusters was set to the number in the gold set
for each of the city pairs.

We used f-measure and pairwise accuracy to eval-
uate the deviation from the gold standard for the
clusterings produced by each of the algorithms. Ta-
ble 2 shows the results. EB G-pLSA performs better
than the other two algorithms on both the metrics,
and especially on pairwise accuracy. Performance of
G-pLSA is very similar to GMM.

4 Human Subject Evaluations

In order to evaluate the usefulness of our system we
conducted extensive experiments on Amazon Me-
chanical Turk (AMT). Our experiments answer the
following questions. (1) Are comparisons generated
using our clustering methods G-pLSA and EB G-
pLSA preferred by users against the entity oblivi-
ous baseline of GMM? (2) Are our system-generated
comparison tables helpful to people for the task of
entity comparison?
Datasets & System Settings: We experiment6 on
two datasets – tourism and movies. For tourism,
we downloaded a collection of 16,785 travel arti-
cles from WikiTravel. The website contains arti-
cles that have been collaboratively written by Web
users. Each article describes a city or a larger geo-
graphic area that is of interest to tourists. In addition,
all articles contain sections7 describing different as-
pects of a city from a tourism point of view (e.g.,
places to see, transportation, shopping and eating).
For our proof of concept, we performed IE only on
the ‘places to see’ sections.

For Movies dataset, we used the Amazon review
data set (Leskovec and Krevl, 2014). It has over 7.9
million reviews for 250,000 movies. We combined
all the reviews for a movie, thus, generating a large

6Code and data available on request
7http://wikitravel.org/en/Wikitravel:Article templates/Sections

review document per movie. This dataset is much
noisier compared to WikiTravel due to presence of
slang, incorrect grammar, sarcasm, etc. In addition,
users also tend to compare and contrast while re-
viewing movies so there are even references to other
movies. As a result, the descriptive phrases extracted
were much more noisy.

For the time consuming manual seed list setting
of our IE system, we only use the tourism dataset.
For movies, we generate seeds using annotation over
LDA topics only. For all systems we use word-
vectors to expand the seed list.

For each table, we generated k clusters where k
was determined using a heuristic8 (Mardia et al.,
1980), and we displayed at most 30 phrases per clus-
ter. We did not display any cluster that had less than
4 phrases.

4.1 Evaluation of Clustering Algorithms

In order to examine whether clustering using EB
G-pLSA indeed produces best comparison tables,
we conducted a human evaluation task on Amazon
Mechanical Turk (AMT) where users of our sys-
tem were asked to indicate their preference between
two comparison tables. Since we have three systems
we performed this pairwise study thrice. In each
study, two comparison tables were generated from
different systems. For each entity-pair we asked four
workers each to select which comparison table they
preferred. The order of the tables was randomized to
remove any biasing effect. We paid $0.3 for each ta-
ble comparison. Table 3 reports the results for both
domains where descriptive phrases were generated
using LDA+WV.

On 30 city-pairs in the Tourism domain, work-
ers preferred the comparison tables generated using
EB G-pLSA 53% of the time and GMM was pre-
ferred only 13% (the rest were ties). It is worth-
while to note that whereas in 20% of the compar-
isons, EB G-pLSA had a clear 4-0 margin, there was
no such comparison where all the workers preferred
the GMM model. We also requested users to provide
the reasons for their preferences. While most users
specified a non-informative reason such as “like it
better”, some users gave specific reasons such as
“subdivides the parts I find useful into more specific

8No. of clusters = square root of half the number of phrases

75



Domain Total pairs EB G-pLSA Win GMM Win EB G-pLSA Win G-pLSA Win G-pLSA Win GMM Win

4-0 3-1 1-3 0-4 4-0 3-1 1-3 0-4 4-0 3-1 1-3 0-4

Tourism 30 20% 33% 13% 0% 13% 30% 30% 0% 17% 27% 13% 3%
Movies 20 20% 35% 10% 0% 5% 35% 15% 5% 5% 45% 15% 5%

Table 3: User preference win-loss statistics for different clustering methods on both city and movie comparison task using the same
IE system. Both EB G-pLSA and G-pLSA significantly outperform the baseline GMM model. EB G-pLSA has some edge over the

G-pLSA model. Note: Ties have not been shown in the table.

categories” and “easy to understand and more spe-
cific points of comparison”. Our results also show
that G-pLSA is a distinct improvement over GMM
(44% vs. 16%). EB G-pLSA had a marginal edge
over G-pLSA (43% vs. 30%).

On movies domain, we report results on 20
movie-pairs and we again found an overwhelming
preference for the system using EB G-pLSA for
clustering. 55% of the time, the output of EB G-
pLSA was preferred over GMM’s 10%. Other com-
parisons between G-pLSA and GMM, and between
our G-pLSA and EB G-pLSA systems also fol-
low trends similar to tourism domain. The perfor-
mance of EB G-pLSA is statistically significantly
better than GMM for both the tourism and the movie
datasets, with p values being less than 0.00004 and
0.002, respectively, using a one-sided students t-test.
This strong preference suggests that the clustering
induced by incorporating entity balance in the clus-
ters produces much better comparison tables.

4.2 Value of Comparison Tables

The goal of our experiments in this section was to
assess whether our comparison tables add value to
some realistic task and to understand the overall
usefulness of our system. To our knowledge there
are no other automated systems comparing cities
for tourism (or movies), hence we could not eval-
uate our system against existing approaches. There-
fore, we decided to evaluate the benefit of the out-
put generated by our system (i.e., comparison tables)
against reading the original WikiTravel articles. For
fairness we only use the ‘places to see’ sections from
WikiTravel, since that was the raw text used in gen-
erating comparison tables in the first place.

Since the comparisons are generated automati-
cally, people may not find them understandable, or
there may be missing valuable information. We test
this in a human subject evaluation. We adapt the

evaluation methodology developed recently for con-
trasting multiple ways of presenting information and
testing the overall learning of the subjects (Shahaf
et al., 2012; Christensen et al., 2014). The evalua-
tion is divided into two parts. In the first part the
workers are given a limited time to read the informa-
tion provided (articles or comparison tables) for an
entity-pair. They are then asked to write a short 150-
300 word summary contrasting different aspects of
the two entities. Each user writes two summaries,
one based on articles and the other based on our ta-
ble. Our study pairs two users such that if user1 read
the articles for city pair 1 and the table for city pair
2, their partner user will see the reverse. The work-
ers were additionally asked which knowledge source
they preferred and why.

Making a worker create summaries using both in-
formation sources helps reduce the effect of worker
comprehension and skill in the evaluation of our
task, as each worker contributes to summaries cre-
ated using our system as well as the baseline. In or-
der to reduce the effects of any sequence bias, half
the mechanical turk workers were first shown the
output of our system followed by the articles and the
other half (partners) were shown content the other
way around.

In the second part of this experiment we directly
compare the knowledge acquisition of these work-
ers. In particular, we ask a different set of work-
ers to evaluate the summaries created by the part-
nered workers. In each task, a worker has to compare
two summaries for the same entity-pair, one created
using tables by one worker and other created us-
ing articles by their partner. Each summary pair was
shown to four different users and each of them was
asked to select the summary they preferred for com-
paring and contrasting the entities. Since we perform
this experiment on Tourism data, the MTurk task de-
scriptions explained that the intent of the compari-

76



son is tourism and their summaries or preferences
must be from that perspective.

4.2.1 Results
We performed this evaluation on twenty city

pairs using both our information extraction methods
i.e. Manual+WV expansion (referred as TABLE-
M) and LDA+WV expansion (referred as TABLE-
LDA) along with the EB G-pLSA method for clus-
tering. The city pairs were chosen such that the
cities are related but not too similar, and the workers
would likely not have thought of the specific com-
parisons before.

We found that in the first part where workers were
given 10 minutes to create the summaries, they on
average asked for 30% more time to create the sum-
maries when information was presented as article.
This supports our belief that our system-generated
tables successfully reduce information overload. It
also suggests that the structure added by the system
(clusters) was useful for the comparison task and re-
duced workers’ cognitive load.

We now present the results for the second part
of the study in which workers evaluated the com-
parison summaries written by the workers in the
first part. Within 20 city-pairs, summaries for 5 city
pairs (25%) generated based on TABLE-M were
preferred and 5 (25%) generated based on original
articles were chosen. The workers were indifferent
in 10 of the city pairs (both summaries got two votes
each). This shows that despite having a very high
compression ratio, workers still managed to create
summaries that were comparable in quality to those
created by reading original documents. We repeated
the same study using TABLE-LDA and found that
summaries for 8 city pairs (40%) generated based
on TABLE-LDA were preferred and 5 (25%) gen-
erated based on original articles were chosen. The
workers were indifferent in 6 of the city pairs (both
summaries got two votes each).

We did not repeat this experiment using the
Movies data set as the source articles were concate-
nated reviews with no structure and it would not be
surprising that users prefer our system. In summary,
we find that both our systems convey adequate and
useful information in the comparisons and the sum-
maries generated by users using our systems were
found to be as good as the ones created by users

reading the full articles.

5 Conclusions

We define a novel task of automatically generating
tabular entity comparisons from unstructured text.
We also implement the first system for this task that
first extracts descriptive phrases from text and then
clusters them to generate comparison tables. Our
clustering algorithm is a Gaussian extension of p-
LSA, where the descriptive phrases are represented
using embeddings in the word vector space. In or-
der to have a better comparison between entities,
we incorporate a balance term which prefers clus-
ters where entities are proportionately represented.

We perform extensive human-subject evaluations
for our systems over Amazon Mechanical Turk
(AMT) on two datasets – tourism and movies. We
find that AMT workers overwhelmingly prefer EB
G-pLSA based comparisons over GMM-based. We
also assess the value of our generated comparisons
over reading the original articles. We find that while
both sets of workers learned as much, the workers
viewing tables asked for less additional time to nar-
rate a comparison in words. Overall, we believe that
comparison tables add value for users deciding be-
tween multiple entities. In the future we wish to per-
form joint extraction and clustering instead of our
current pipelined approach.

Acknowledgments

We would like to thank the users of our system:
Bhadra Mani, Dinesh Khandelwal, Eshita Sharma,
Kuntal Dey, Leela Muthana, Noira Khan, Samuel
Kumar, Seher Contractor and Prachi Jain, and the
anonymous Amazon mechnical turk workers for
their evaluation and insights. We would also like to
thank Ankit Anand, Happy Mittal and anonymous
reviewers for their suggestions on improving the
paper. The work was supported by IBM Research,
Google language understanding and knowledge dis-
covery focused research grants to Mausam, a KISTI
grant and a Bloomberg grant also to Mausam. We
would also like to acknowledge the IBM Research
India PhD program that enables the first author to
pursue the PhD at IIT Delhi.

77



References
Arindam Banerjee and Joydeep Ghosh. 2006. Scalable

clustering algorithms with balancing constraints. Data
Min. Knowl. Discov., 13(3):365–395, November.

Michele Banko, Michael J Cafarella, Stephen Soderland,
Matt Broadhead, and Oren Etzioni. 2007. Open infor-
mation extraction from the web. In IN IJCAI, pages
2670–2676.

David M. Blei, Andrew Y. Ng, Michael I. Jordan, and
John Lafferty. 2003. Latent dirichlet allocation. Jour-
nal of Machine Learning Research, 3:2003.

Janara Christensen, Stephen Soderland, Gagan Bansal,
and Mausam. 2014. Hierarchical summarization:
Scaling up multi-document summarization. In Pro-
ceedings of the 52nd Annual Meeting of the Associ-
ation for Computational Linguistics, ACL 2014, June
22-27, 2014, Baltimore, MD, USA, Volume 1: Long Pa-
pers, pages 902–912.

R. Collobert, J. Weston, L. Bottou, M. Karlen,
K. Kavukcuoglu, and P. Kuksa. 2011. Natural lan-
guage processing (almost) from scratch. Journal of
Machine Learning Research, 12:2493–2537.

James Richard Curran. 2003. From Distributional to Se-
mantic Similarity. Ph.D. thesis, Institute for Commu-
nicating and Collaborative Systems School of Infor-
matics University of Edinburgh.

Rajarshi Das, Manzil Zaheer, and Chris Dyer. 2015.
Gaussian lda for topic models with word embeddings.
In Proceedings of the 53rd Annual Meeting of the As-
sociation for Computational Linguistics and the 7th
International Joint Conference on Natural Language
Processing (Volume 1: Long Papers), pages 795–804,
Beijing, China, July. Association for Computational
Linguistics.

Oren Etzioni, Anthony Fader, Janara Christensen,
Stephen Soderland, and Mausam. 2011. Open In-
formation Extraction: the Second Generation. In In-
ternational Joint Conference on Artificial Intelligence
(IJCAI), Barcelona, Spain, July.

N. Ganganath, Chi-Tsun Cheng, and C.K. Tse. 2014.
Data clustering with cluster size constraints using a
modified k-means algorithm. In Cyber-Enabled Dis-
tributed Computing and Knowledge Discovery (Cy-
berC), 2014 International Conference on, pages 158–
161, Oct.

Jeff Huang, Oren Etzioni, Luke Zettlemoyer, Kevin
Clark, and Christian Lee. 2012. Revminer: An extrac-
tive interface for navigating reviews on a smartphone.
In Proceedings of the 25th Annual ACM Symposium
on User Interface Software and Technology, UIST ’12,
pages 3–12, New York, NY, USA. ACM.

Hyun Duk Kim, Kavita Ganesan, Parikshit Sondhi, and
ChengXiang Zhai. 2011. Comprehensive review of
opinion summarization.

Kevin Lerman and Ryan McDonald. 2009. Con-
trastive summarization: An experiment with consumer
reviews. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, Companion Volume: Short Papers,
NAACL-Short ’09, pages 113–116, Stroudsburg, PA,
USA. Association for Computational Linguistics.

Jure Leskovec and Andrej Krevl. 2014. SNAP Datasets:
Stanford large network dataset collection. http://
snap.stanford.edu/data, June.

Bing Liu and Lei Zhang. 2012. A survey of opinion
mining and sentiment analysis. pages 415–463.

K. V. Mardia, J. T. Kent, and J. M. Bibby. 1980. Multi-
variate Analysis (Probability and Mathematical Statis-
tics). Academic Press, 1 edition.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado,
and Jeffrey Dean. 2013. Distributed representations of
words and phrases and their compositionality. CoRR,
abs/1310.4546.

George A. Miller. 1995. Wordnet: A lexical database for
english. Commun. ACM, 38(11):39–41, November.

Apostol (Paul) Natsev, Alexander Haubold, Jelena Tešić,
Lexing Xie, and Rong Yan. 2007. Semantic concept-
based query expansion and re-ranking for multime-
dia retrieval. In Proceedings of the 15th International
Conference on Multimedia, MULTIMEDIA ’07, pages
991–1000, New York, NY, USA. ACM.

Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-
Maria Popescu, and Vishnu Vyas. 2009. Web-scale
distributional similarity and entity set expansion. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing: Volume 2
- Volume 2, EMNLP ’09, pages 938–947, Stroudsburg,
PA, USA. Association for Computational Linguistics.

Michael J. Paul, ChengXiang Zhai, and Roxana Girju.
2010. Summarizing contrastive viewpoints in opin-
ionated text. In Proceedings of the 2010 Conference
on Empirical Methods in Natural Language Process-
ing, EMNLP ’10, pages 66–76, Stroudsburg, PA, USA.
Association for Computational Linguistics.

Dafna Shahaf, Carlos Guestrin, and Eric Horvitz. 2012.
Trains of thought: Generating information maps. In
International World Wide Web Conference (WWW).

Ruben Sipos and Thorsten Joachims. 2013. Generat-
ing comparative summaries from reviews. In 22nd
ACM International Conference on Information and
Knowledge Management, CIKM’13, San Francisco,
CA, USA, October 27 - November 1, 2013, pages
1853–1856.

EllenM. Voorhees. 1994. Query expansion using lexical-
semantic relations. In BruceW. Croft and C.J. Rijsber-
gen, editors, SIGIR ?94, pages 61–69. Springer Lon-
don.

78



Koji Yatani, Michael Novati, Andrew Trusty, and Khai N.
Truong. 2011. Review spotlight: A user interface for
summarizing user-generated reviews using adjective-
noun word pairs. In Proceedings of the SIGCHI Con-
ference on Human Factors in Computing Systems, CHI
’11, pages 1541–1550, New York, NY, USA. ACM.

Sun Yuepeng, Liu Min, and Wu Cheng. 2011. A
modified k-means algorithm for clustering problem
with balancing constraints. In Proceedings of the
2011 Third International Conference on Measuring
Technology and Mechatronics Automation - Volume
01, ICMTMA ’11, pages 127–130, Washington, DC,
USA. IEEE Computer Society.

ChengXiang Zhai, Atulya Velivelli, and Bei Yu. 2004.
A cross-collection mixture model for comparative text
mining. In Proceedings of the Tenth ACM SIGKDD In-
ternational Conference on Knowledge Discovery and
Data Mining, KDD ’04, pages 743–748, New York,
NY, USA. ACM.

Shunzhi Zhu, Dingding Wang, and Tao Li. 2010. Data
clustering with size constraints. Knowledge-Based
Systems, 23(8):883 – 889.

79


