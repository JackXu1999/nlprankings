



















































WECA：A WordNet-Encoded Collocation-Attention Network for Homographic Pun Recognition


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2507–2516
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

2507

WECA: A WordNet-Encoded Collocation-Attention Network for
Homographic Pun Recognition

Yufeng Diao1,2, Hongfei Lin1∗, Di Wu1, Liang Yang1, Kan Xu1,
Zhihao Yang1, Jian Wang1, Shaowu Zhang1, Bo Xu1, DongyuZhang1

1DaLian University of Technology, Da Lian, China
2Inner Mongolia University for Nationalities, Tong Liao, China

diaoyufeng@mail.dlut.edu.cn, hflin@dlut.edu.cn
wudi@dlut.edu.cn, liang@dlut.edu.cn
xukan@dlut.edu.cn, yangzh@dlut.edu.cn

wangjian@dlut.edu.cn, zhangsw@dlut.edu.cn
xubo2011@mail.dlut.edu.cn, zhangdongyu@dlut.edu.cn

Abstract

Homographic puns have a long history in hu-
man writing, widely used in written and spo-
ken literature, which usually occur in a certain
syntactic or stylistic structure. How to rec-
ognize homographic puns is an important re-
search. However, homographic pun recogni-
tion does not solve very well in existing work.
In this work, we first use WordNet to under-
stand and expand word embedding for settling
the polysemy of homographic puns, and then
propose a WordNet-Encoded Collocation-
Attention network model (WECA) which
combined with the context weights for recog-
nizing the puns. Our experiments on the Se-
mEval2017 Task7 and Pun of the Day demon-
strate that the proposed model is able to dis-
tinguish between homographic pun and non-
homographic pun texts. We show the effec-
tiveness of the model to present the capability
of choosing qualitatively informative words.
The results show that our model achieves the
state-of-the-art performance on homographic
puns recognition.

1 Introduction

A pun is a writers use of a word in an ambiguous
and inconsistent way in language, often to play on
the different meanings of the word or utilize simi-
larly pronounced sounds for a common humorous
effect. Puns are widely used in written and spo-
ken literature, which intended as jokes. For ex-
ample, Tom Swifty by(Lippman and Dunn, 2000),
in which puns usually occur in a certain syntac-
tic or stylistic structure. From literature, speeches
and oral storytelling, puns are also a standard
rhetorical device, which also can be applied non-
humorously. For instance, Shakespeare is well
known for his puns, which continually appeared in
his non-comedic works by (Tanaka, 1992). Both

∗Corresponding author

humorous and non-humorous puns have been the
theme of extensive and attractive works that has
led to discernment for the nature of puns with dou-
ble meaning.

There are many relevant studies on pun recogni-
tion in natural language processing. Many schol-
ars attempted to classify puns according to the
similar relationship between the pronunciations
and double meanings of the words. For exam-
ple, (Pafford, 1987) categorizes pun into homo-
phonic puns and homographic puns, which used
homonyms and polysemy of words respectively.
The research on pun recognition has carried out
according to this classification system of Redfern.
Our work also considers that puns consist of ho-
mophonic and homographic puns.

Type of Pun Example Pun Word
Homographic I used to be a banker interest

but I lost interest.
Homophonic When the church bought propane

gas for their annual
barbecue, proceeds went

from sacred to the propane.

Table 1: Pun Examples

Both homographic puns and homophonic puns
have double meanings to increase deep impression
in a certain environment. However, two types of
puns have their own features, respectively. Homo-
graphic puns, as an important class of puns, which
the words for two senses of puns share the same
orthographic form. While homophonic puns have
the similarity in pronunciations with double senses
that distinguished from homographic puns. The
former one mainly settles synonyms, while the lat-
ter one solves homonyms. Because of the differ-
ence, we can not use the unified model to distin-
guish. Table 1 illustrates the examples of homo-
graphic pun and homophonic pun.

In this study, we mainly focus on homographic



2508

puns since they widely used everywhere (Miller,
Tristan and Turković, Mladen, 2016) and easily
obtain in existing corpus. However, homographic
puns recognition in the current works does not
solve very well because of their confused double
meanings.

To solve the mentioned problem, we propose
a computational WordNet-Encoded Collocation-
Attention network model (WECA) to recognize
homographic puns. Our model takes semantic
word embedding and collocation into account for
homographic puns recognition. Based on the ex-
periments, the results show that our work will
improve the performance of homographic puns
recognition. This work is the first to recognize ho-
mographic puns with improved word representa-
tion and attention mechanism to the best of knowl-
edge. Here, our contributions are as follows.

• The paper applies the lexical ontology Word-
Net to understand and extend the word em-
bedding for solving the polysemy of homo-
graphic puns.

• The paper proposes a neural attention mech-
anism to extract the collocation for homo-
graphic puns classification, which combined
with Bi-LSTM to obtain the context weights.

• Experimental results on the datasets of Se-
meval2017 Task7 and Pun of the Day demon-
strate our method outperforms several base-
lines for recognition homographic puns. Fur-
thermore, visualization of selected examples
show the reasons that this model works well.

The rest of this paper is structured in the follow-
ing. Section 2 mainly reviews the related work on
word representation and puns classification. Sec-
tion 3 presents our proposed word embedding and
collocation attention-based network model. Sec-
tion 4 shows our experiments and discusses eval-
uation results. Finally, Section 5 concludes our
research contributions and offers the future work.

2 Related Work

In this section, we will review related works on
word representation and homographic pun recog-
nition for homographic puns classification briefly.

2.1 Word Representation
In recent years, word representation has the great
improvement because it solves data sparsity prob-

lem and obtain more semantic relations between
words compared with one-hot representation.

(Rumelhart et al., 1986) proposed the idea of
word distributed representation, which converts
all the words into a low-dimensional continu-
ous semantic space. This space took each word
as a vector. These distributed low-dimensional
word representation have been widely applied
in many NLP tasks, including machine transla-
tion(Sutskever et al., 2014; Bahdanau et al., 2014),
text classification (Niu et al., 2017; Du et al.,
2017), neural language models (Mikolov et al.,
2010, 2013) and parsing (Chen and Manning,
2014; Chen et al., 2015). Word embedding is taken
as the essential and available inputs for NLP tasks,
which enables encoding semantic representation
in meaningful vector space.

The studies show that word representations
are useful to achieve a good balance between
effectiveness and efficiency, such as Word2Vec
(Mikolov et al., 2013) and GloVe(Pennington
et al., 2014). Therefore, the semantic meanings
of words can reflect in the contexts according to
these distributed representation models.

However, homographic puns always have multi-
ple meanings. The word representation, consider-
ing as only one vector for each word, which puz-
zled by the understanding for polysemy of puns.
This paper combines the representations of lem-
mas, synsets and words from WordNet1(Miller,
2002) to understand multiple meanings of homo-
graphic puns. The lemma and synset annotation in
WordNet provide helpful semantic information for
detecting homographic puns.

2.2 Homographic Pun Recognition

In recent years, homographic puns have increas-
ingly become a respectable research topic, which
widely appears in rhetoric and literary criticism.
However, there were little related works in the
fields of computational linguistics and natural lan-
guage processing by (Miller, Tristan and Turković,
Mladen, 2016). In this subsection, we mainly in-
troduce some puns detecting methods.

There are many useful methods to classify the
puns in NLP. For example, (Kao et al., 2016;
Huang et al., 2017) used a probability statisti-
cal model to capture the latent semantic infor-
mation between words for detecting homographic
puns. (Jaech et al., 2016) proposed a new prob-

1WordNet: http://wordnet.princeton.edu/



2509

ability model to learn phoneme edit probabilities
for classifying the homophonic puns. The sys-
tem ECNU(Xiu et al., 2017) applied a supervised
training classifier, which helpful features derived
from WordNet and Word2Vec embeddings to dis-
tinguish between homographic puns. The sys-
tem Fermi (Indurthi and Oota, 2017) employed
a supervised approach for the detection of ho-
mographic puns. It used a bi-directional RNN
for a classification model and adopted the dis-
tributed semantic word embeddings as input fea-
tures. These methods do not consider the colloca-
tion between words in homographic puns.

The attention mechanism proposed by (Bah-
danau et al., 2014) to settle machine translation
problem, which was used to select the reference
words for words before translation. (Xu et al.,
2015) used attention model for image generation
to select the similar image regions. For text clas-
sification, (Yang et al., 2016) applied attention
mechanism into solving document-level classifica-
tion. Many other tasks in NLP used this mecha-
nism, including natural language question answer-
ing (Kumar et al., 2015), parsing (Vinyals et al.,
2014), image question answering(Yang et al.,
2015), and classification(Shen et al., 2018; Tan
et al., 2018). Therefore, this model is capable of
discovering the important and semantic informa-
tion. Meanwhile, attention mechanism can also
improve the performance of classification tasks.

Hence, we explore an attention mechanism for
collocation to mine the latent semantic informa-
tion between the part of speech words to achieve
good result for homographic puns recognition.

3 Methods

Homographic puns recognition could influence
by considering both the semantic word embed-
ding and collocation with the context weights
for homographic puns. In this section, we pro-
pose our model as WordNet-Encoded Collocation-
Attention network (WECA). Figure 1 demon-
strates the overall structure of our model.

It consists of three main components: an im-
proved word embedding with WordNet-Encoded
as inputs, a Bidirectional Long Short-Term Mem-
ory (Bi-LSTM) as context weights in a sentence
for homographic puns and a fully-connected net-
work as the collocation-attention mechanism. The
attention networks combined by a concatenate op-
eration to discover the collocation. Then the con-

text weights and attention networks combined by
an element-wise multiplication operation in the
classification layer. We describe the details of
three components as follows.

3.1 WordNet-Encoded Word Embedding

The homographic pun is a clever trick to let one
word relate to two aspects or multiple meanings.
For example, “Before he sold Christmas trees, he
got himself spruced up” The pun word spruced
has two meaning: one meaning is spruce tree,
while the other is making oneself or something
look neater and tidier. We find this word spruced
means the last meaning in this situation. There-
fore, the polysemy of the ambiguity from homo-
graphic puns need additional large lexical ontol-
ogy. Thus, we apply WordNet for computational
linguistics and natural language processing.

Polysemy is critical factor for recognizing ho-
mographic puns. To combine the information of
multiple meanings, we propose giving a WordNet-
Encoded model (WE) to obtain the word embed-
ding for each word. WordNet is a lexical ontology
of words. Each word has multiple semantics corre-
sponding with respect to different senses and each
sense corresponds to multiple words.

We introduce lemmas, synsets (senses) and
words in WordNet. For example, the word is
“interest”. The word “interest” has three main
synsets: sake (a reason for wanting something
done), pastime (a diversion that occupies one’s
time and thoughts) and interest (a sense of con-
cern with and curiosity about someone or some-
thing). The lemmas eliminate the ambiguity of
each sense. For instance, the synset pastime rep-
resents a diversion that occupies one’s time and
thoughts, which contains lemmas pastime, inter-
est and pursuit. Then we propose two strategies to
generate the Word-Net-Encoded embedding based
on the information of lemmas and synsets in-
formation, Average Lemma Aggregation Model
(ALA) and Weighted Lemma Aggregation Model
(WLA).

Average Lemma Aggregation Model (ALA)
adopts a strategy of equal weight according to
meanings of homographic puns. ALA model
mixes all the lemmas of all the senses of a word to-
gether for each word. Hence, it represents the tar-
get word by using the average of its whole lemma
embedding and puts this together on the original



2510

Figure 1: WordNet-Encoded Collocation-Attention network model(WECA)

vector of target word. The formula is as follows:

w =
1

m

∑
si(w)∈S(w)

∑
li
(sj)∈Li(sj)

wlj
sj (1)

which means the new embedding vector of w is
determined by the average of all its lemma embed-
ding. Here, m represents the number of lemmas
with overlapping senses with respect to the word
w, si is the sense i, lj is the lemma j. Finally,
word embedding of w is the concatenation of the
original vector and above new vector.

ALA model can apply lemmas to encoding la-
tent semantic relationship because lemmas share
the information by multiple words and senses.
Therefore, words sharing the same lemmas are
likely to obtain the similar representations.

Weighted Lemma Aggregation Model (WLA)
The ALA Model takes the lemma embedding to
encode lemma information for word representa-
tion. Although ALA model represents the aver-
age of all the lemma, which does not consider the
importance of certain lemmas. Hence, we con-
struct embedding for a target word with the help
of word senses and lemmas in WordNet that we
called WLA model. The formula is as follows:

w =
∑

si(w)∈S(w)

|Li(si)|
m

∑
li
(sj)∈Li(sj)

wlj
sj (2)

where m represents for all the number of lemmas
with overlapping senses with respect to the word

w, si is the sense i, lj is the lemma j, li(sj) is the
number of lemmas in each sense with target word,
w is the new embedding considering the weighted
lemma information. Then, the target word embed-
ding concatenates new vector to original vector.

The weighted lemma strategy assumes one
sense of word obtains more attention if this sense
of word has more lemmas. We can show each
word as a special distribution on the sense. From
the results, WLA model is the best representation.

3.2 Bidirectional Long Short Term
Memory(Bi-LSTM) for Recognizing
Homographic Puns

Long Short Term Memory (LSTM) was proposed
by Hochreiter and Schmiduber (1997), which has
been widely adopted for text processing. There are
three gates and one cell in LSTM: an input gate it,
a forget gate ft, an output gate ot and a memory
cell ct. They are all vector in Rd. The equations
of transition are:

it = σ(Wixt + Uiht−1 + Vict−1) (3)

ft = σ(Wfxt + Ufht−1 + Vfct−1) (4)

ot = σ(Woxt + Uoht−1 + Voct−1) (5)

c̃t = tanh(Wcxt + Ucht−1) (6)



2511

ct = ft�ct−1 + it � c̃t (7)

ht = ot�tanh(ct) (8)

where xt is an input vector at the current time step,
σ is the sigmoid function and � is the element-
wise multiplication operation, W{i,f,o,c} ,U{i,f,o,c}
,V{i,f,o,c} are learned weight parameters, ht is the
hidden state vector. In LSTM, the hidden state ht
only encodes the front context in a forward direc-
tion but not consider the backward context.

Figure 2: The Structure of Bi-LSTM

In this study, we apply Bi-LSTM model(Graves,
2012) to capture the latent semantic information
of homographic puns for obtaining the context
weights. For each sentence, it has a forward
LSTM

−→
h and a backward LSTM

←−
h to con-

catenate the hidden states of two LSTMs as the
representation of corresponding word. Figure 2
illustrates the architecture of Bi-LSTM model.
{w1, w2, · · · , wn} represent the word vector in a
sentence whose length is N . Then, the forward
and backward contexts can take into account si-
multaneously. The equations of transition are:

−→
h t = H(Wx

−→
h
xt +W−→h

−→
h

−→
h t−1 + b−→h ) (9)

←−
h t = H(Wx

←−
h
xt +W←−h

←−
h

←−
h t−1 + b←−h ) (10)

hout =W−→h y
−→
h t +W←−h y

←−
h t + by (11)

where
−→
h t is a forward LSTM,

←−
h t is a backward

LSTM, hout is the output of Bi-LSTM.

3.3 Collocation-Attention Mechansim

It proposes that not all the words provide the same
contribution of word representation for the sen-
tence. Especially for homographic puns recogni-
tion, the collocation between candidate pun words
in a sentence offers more clues for getting the col-
locational word weights. Miller points out that
the candidate pun words mainly consist of nouns,
verbs, adjectives and adverbs in each pun. For ex-
ample, “The money doesn’t grow on the tree, but it
can grow on the branch.” The word “branch” is the
pun word. From this example, we know that the
collocation of candidate pun words {money, grow,
tree, branch}, which should be more important for
recognizing homographic puns.

Therefore, it is necessary to learn about the la-
tent relationship in collocation of words. We de-
sign an attention mechanism to obtain the colloca-
tional weights by extracting such words of collo-
cation from nouns, verbs, adjectives and adverbs,
respectively. Then we concentrate on the four
parts in sentences with pun to aggregate the in-
formative words for classifying the homographic
puns. This model uses an attention network taking
word embedding with WordNet-Encoded as input
then to extract polysemy attention signal, which
made use of polysemy to understand ambiguity of
homographic puns. The formula is as follows:

uijt = V · tanh(Wuhijt + bw) (12)

αijt =
exp(uijt)∑Tx
t=1 exp(uijt)

(13)

cij =

Tx∑
t=1

αijthijt (14)

where hijt is a hidden state at each
time step for each part of speech,
j ∈ {nouns, verbs, adjectives, adverbs},
uijt is a hidden representation of hijt through a
one-layer MLP, αijt is a normalized importance
weight through a softmax function with each part
of speech, cij is a context vector as a high level
representation over the words from attention-
based model by the weighted mean of the hidden
state sequence hijt for each part of speech.

After combining the attention networks with the
context weights in a sentence, we merge all the
cij vectors from collocation attention model and
take the uniformed context weights in a sentence.



2512

Then we mix the two parts results with element
wise multiplication operation to recognize the ho-
mographic puns. The formula is as follows:

ci = [cinouns; civerbs; ciadjectives; ciadverbs] (15)

lout = Softmax(hout) (16)

si = ci · lout (17)

where ci is merged by cij , j ∈
{nouns, verbs, adjectives, adverbs}, lout is
the softmax function of hout, si is the result with
the multiplication operation of ci and lout.

The model can be trained in an end-to-end way
by backpropagation, where objective function is
the cross-entropy loss. Let y be the target distribu-
tion and ŷ be the predicted distribution. The goal
of training is to minimize the cross-entropy error
between y and ŷ for all sentences.

loss = −
∑
i

∑
j

yji logŷi
j + λ‖θ‖2 (18)

where i is the index of sentence, j is the index of
class. Our classification is two way. λ is the L2
regularization term. θ is the parameter set.

4 Experiments and Evaluation

In this section, we first evaluate the effective-
ness of our WordNet-Encode model (WE) on two
tasks to detect the polysemy of homographic puns.
Then, we examine the performance of our WECA
model compared with existing methods. Finally,
we show the effectiveness of our model.

4.1 Experimental Setting
In this section, we introduce datasets, evaluation
metrics, baseline methods, and present the details
of the training process of our model.

Datasets To verify the effectiveness of our pro-
posed model, we use two datasets: SemEval2017
Task72 and Pun of the Day3 .

SemEval2017 Task7. This dataset is composed
of homographic and heterographic puns for rec-
ognizing and interpreting puns. We focus on ho-
mographic puns detection in semantic rather than
phonology. The homographic pun word will have
at least two words sense in the WordNet(Miller

2SemEval2017 Task7: htto://alt.qcri.org/semeval2017/task7/
3Pun of the Day: htto://www.punoftheday.com/

and Gurevych, 2015). Table 1 shows a detailed
statistical distribution of our datasets.

Pun of the Day. This dataset only includes pun
content in the beginning. Then it collects the neg-
ative samples from Yahoo! Answer4 , AP News5 ,
Proverb, and New York Times in order to balance
the distribution of positive and negative examples,
which adapt to decrease the domain discrepancy.
Table 2 provides a complete statistical description
of our dataset.

Dataset Positive Negative Average
Length

Task7 1607 643 13.1
Pun of the Day 2423 2403 13.5

Table 2: Statistics of Datasets

Metrics We apply the standard measures pre-
cision, recall, accuracy and F1-score to evaluate
the effectiveness for homographic puns recogni-
tion, which also adopted as metrics in SemEval
2017 Task7 evaluation.

Baselines We compare several strong baselines
as follows.

LSTM: LSTM without WordNet-Encoded em-
bedding and Collocation-Attention mechanism.

Bi-LSTM: Bi-LSTM without WordNet-
Encoded embedding and Collocation-Attention.

Bi-LSTME: Bi-LSTM with WordNet-Encoded
embedding used the WLA model.

Bi-LSTM-Attention: Bi-LSTM with single at-
tention mechanism.

Fermi and N-Hance are the good performing
model in the SemEval2017 task7.

Top1 Fermi: Fermi took a supervised ap-
proach for homographic puns detection. It did
not construct own train data set, but rather split
the shared task data set into train sets and test
sets(Miller, Tristan and Hempelmann, Christian
and Gurevych, Iryna, 2017). It used a Bi-RNN
to learn a classification model and treat the word
embedding as the input features.

Top2 N-Hance: It assumed every pun had a
particularly strong association with exactly one
other word in context(Miller, Tristan and Hempel-
mann, Christian and Gurevych, Iryna, 2017).
Then it calculated PMI between words in context
to detect and locate puns. If the score exceeded

4htto://answers.yahoo.com/
5htto://hosted.ap.org/dynamic/fronts/HOME?SITE=AP



2513

a certain threshold, the text assumed to contain a
pun. Otherwise, the text assumed to have no pun.

WECA: Here, we use Bi-LSTM with WordNet-
Encoded embedding with WLA model and
Collocation-Attention mechanism.

Training Details In experiments, our model is
tuned with 5-fold cross validation. All word vec-
tors are initialized by GloVe. We use 50, 100, 200
and 300 dimension to verify the performance, re-
spectively. Here, 200 dimension is the best perfor-
mance. Therefore, we set the dimensions of word,
synset and lemma embedding to be 200. The size
of units in LSTM is 800. RMSprop is used for our
optimization method. We use learning rate decay
and early stop in the training process. All models
are trained by mini-batch of 64 instances.

4.2 The Effectiveness of WordNet-Encoded
Word Embedding

Comparing the GloVe model with our ALA model
and WLA model, we evaluate the quality of our
improved word representations to detect the ho-
mographic puns. In this experiment, we use the
same classifier Bi-LSTM and parameters to verify
the effectiveness of our word embedding.

Figure 3 and Figure 4 show the results of dif-
ferent word embedding for detecting homographic
puns. From the results we can observe that:

(1) Our models ALA and WLA, which out-
perform the original vector GloVe on both two
datasets. It indicates that our model can better cap-
ture the semantic relations of words by utilizing
lemma annotation properly based on the WordNet.

(2) The ALA model represents each word with
the average of its lemma embedding. In gen-
eral, the ALA model performs better than GloVe,
showed that lemma and synset of WordNet is very
useful. The reason is the words sharing mutual
lemma representation are helpful with each other.

(3) The WLA model mostly performs better
than GloVe and ALA model. This model can
obtain a weighted distribution according to the
senses and lemmas. The results show that their
different senses are commonly different from oth-
ers, but share certain representation.

4.3 Pun Recognition with WordNet-Encoded
Collocation-Attention Network (WECA)

Our model WECA, combination of WordNet-
Encoded word embedding and Collocation-
Attention network with context weight, which
performs compared with the suggested baselines.

Figure 3: Comparison of Different Word Embedding
on SemEval2017 Task7

Figure 4: Comparison of Different Word Embedding
on Pun of the Day

Here, we use Pun of the Day as the training set
to obtain all the parameters, and test the results
of homographic pun recognition in SemEval2017
task7. The results are shown in Table 3.

Precision Recall F1
LSTM 81.80 83.7 82.43

Bi-LSTM 85.40 83.64 84.51
Bi-LSTME 85.87 85.07 85.46

Bi-LSTM-Attention 84.92 85.62 85.26
N-Hance 75.53 93.34 83.50
WECA 89.19 90.64 89.21

Table 3: Comparison of Different Models of Homo-
graphic Puns Recognition

(1) Bi-LSTM has the better performance for ho-
mographic puns detection compared with LSTM
(84.51% vs.82.43%). It shows that Bi-LSTM ex-
ploits two parallels to discover more context in-
formation. At the same time, Bi-LSTME out-
performs Bi-LSTM (85.46% vs.84.51%), which
demonstrating the effectiveness of the WordNet-
Encoded word embedding.

(2) Bi-LSTM-Attention performs slightly better



2514

Precision Recall F1
Fermi 90.24 89.70 89.97

WECA 91.43 90.53 90.98

Table 4: Comparison of WECA and Fermi of Homo-
graphic Puns Recognition

than Bi-LSTM and LSTM (85.26% vs. 84.51%,
82.43%). The reason is that the attention mech-
anism can assign the weight to the whole words
according to the context information.

(3) Our model WECA has a better performance
compared with Bi-LSTME , Bi-LSTM-Attention
and N-Hance (87.45% vs. 85.46%, 85.26%,
83.50%). N-Hance is the second place in Se-
mEval2017 task7. It shows the WordNet-Encoded
word embedding can capture more semantic in-
formation between words with the help of lemma
and synsets in WordNet. Meanwhile, it presents
that the attention network mechanism combined
with collocation of the specific part of speech of
puns, which capture the characteristic information
to recognize the homographic puns.

The best perform of SemEval2017 task7 is
Fermi. However, Fermi only evaluates on 675
of 2250 homographic contexts(Miller, Tristan and
Hempelmann, Christian and Gurevych, Iryna,
2017) in SemEval2017 task7. Thus, our model
uses 675 as a test set and rest of data as a train-
ing set. The results are shown in Table 4. Experi-
ment results present our model outperforms Fermi
under the same data distribution. It shows the ef-
fectiveness of our model again.

4.4 Visualization of Model

In order to verify our model is enable to select
the valuable information of words that reflected
the collocation, we visualize the attention layers
for several sentences in Pun of the Day and Se-
mEval2017 Task7 data sets whose labels are cor-
rectly predicted by our model in Figure 5. We
choose two examples. One presents collocation
between nouns, the other presents collocation be-
tween verbs.

Each line is a sentence. Blue denotes word
weight. If color of word is darker, the word is more
important. Figure 5 shows our WECA model can
select words carrying ambiguous meanings from
the collocation of homographic puns. For exam-
ple, in the first sentence, it highlights “interest”,
which is worthy attracting more attention because
of multiple meanings for the pun word. In the

Figure 5: Visualization of attention layers

Figure 6: Average context weight of all sentences by
Bi-LSTM

second sentence, “sleep” is selected word by our
attention model as related to homographic puns.
Therefore, attention networks of collocation is ef-
fective for recognizing homographic puns.

We apply the Bi-LSTM to capture latent seman-
tic context for weighting part of speech which in-
cluded nouns, verbs, adjectives and adverbs from
forward and backward direction. Figure 6 shows
Bi-LSTM model distributes weights to the four
part of speeches. The weights of verbs occupy
the first place, then second one is nouns, adjec-
tives and adverbs are lower. Meanwhile, it demon-
strates the importance of part of speech.

Thus, we choose two examples to illustrate
weights with the four part of speeches according
to context information by Bi-LSTM in Figure7.
For the first example, the word “cured”, as a verb,
which is a pun word. It shows weights of verbs are
highest allocation by Bi-LSTM. For the second ex-
ample, the word “cinch”, as a noun, which is a pun
word. It illustrates that Bi-LSTM distributes the
higher weights to nouns, which presents the im-
portance of nouns from the context semantic in-
formation. Hence, context weights providing by
Bi-LSTM are helpful of the collocation for recog-
nizing the homographic puns.



2515

Figure 7: Visualization of Bi-LSTM

5 Conclusion and Future Work

In this study, we propose a computational model
WECA combined with WordNet-Encoded word
embedding and Collocation-Attention network.
We extend the semantic information of word em-
bedding by lemma and synset according to Word-
Net. We also apply a neural attention network,
combined with Bi-LSTM, which captures the col-
location of homographic puns. Experimental re-
sults show our model achieves the best perfor-
mance and outperforms several baselines.

In future work, we would like to find an appro-
priate way in incorporating the external linguistic
knowledge to improve the performance of homo-
graphic puns recognition. We also focus on au-
tomatically generating homographic puns. Those
are all promising jobs we can pursue in the future.

Acknowledgements

This work is partially supported by grant
from the Natural Science Foundation of China
(No.61632011, 61702080, 61572102, 61602079,
61602078), the Fundamental Research Funds
for the Central Universities (No.DUT18ZD102,
DUT17RC(3)016).

References

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
learning to align and translate. Computer Science.

D. Chen and C. D. Manning. 2014. A fast and accurate
dependency parser using neural networks. In Con-
ference on Empirical Methods in Natural Language
Processing, pages 740–750.

Wenliang Chen, Min Zhang, and Yue Zhang. 2015.
Distributed feature representations for dependency
parsing. IEEE Press.

Jiachen Du, Ruifeng Xu, Yulan He, and Lin Gui. 2017.
Stance classification with target-specific neural at-
tention. In Twenty-Sixth International Joint Confer-
ence on Artificial Intelligence, pages 3988–3994.

Alex Graves. 2012. Long Short-Term Memory.
Springer Berlin Heidelberg.

Yu Hsiang Huang, Hen Hsen Huang, and Hsin Hsi
Chen. 2017. Identification of homographic pun lo-
cation for pun understanding. In International Con-
ference on World Wide Web Companion, pages 797–
798.

Vijayasaradhi Indurthi and Subba Reddy Oota. 2017.
Fermi at semeval-2017 task 7: Detection and inter-
pretation of homographic puns in english language.
In International Workshop on Semantic Evaluation,
pages 457–460.

Aaron Jaech, Rik Koncel-Kedziorski, and Mari Osten-
dorf. 2016. Phonological pun-derstanding. In Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 654–663.

Justine T. Kao, Roger Levy, and Noah D. Goodman.
2016. A computational model of linguistic humor
in puns. Cognitive Science, 40(5):1270–1285.

Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit
Iyyer, James Bradbury, Ishaan Gulrajani, Victor
Zhong, Romain Paulus, and Richard Socher. 2015.
Ask me anything: dynamic memory networks for
natural language processing. pages 1378–1387.

L. G. Lippman and M. L. Dunn. 2000. Contextual
connections within puns: effects on perceived hu-
mor and memory. Journal of General Psychology,
127(2):185–197.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. Computer Science.

Tomas Mikolov, Martin Karafit, Lukas Burget, Jan
Cernock, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In IN-
TERSPEECH 2010, Conference of the International
Speech Communication Association, Makuhari,
Chiba, Japan, September, pages 1045–1048.

G. A. Miller. 2002. Wordnet: A lexical database
for the english language. Contemporary Review,
241(1):206–208.

Tristan Miller and Iryna Gurevych. 2015. Automatic
disambiguation of english puns. In Proceedings
of the 53rd Annual Meeting of the Association for
Computational Linguistics and the 7th International
Joint Conference on Natural Language Processing
(Volume 1: Long Papers), volume 1, pages 719–729.

Miller, Tristan and Hempelmann, Christian and
Gurevych, Iryna. 2017. Semeval-2017 task 7: De-
tection and interpretation of english puns. In Inter-
national Workshop on Semantic Evaluation, pages
58–68.



2516

Miller, Tristan and Turković, Mladen. 2016. Towards
the automatic detection and identification of english
puns. The European Journal of Humour Research,
4(1):59–75.

Yilin Niu, Ruobing Xie, Zhiyuan Liu, and Maosong
Sun. 2017. Improved word representation learning
with sememes. In Meeting of the Association for
Computational Linguistics, pages 2049–2058.

J. H. P. Pafford. 1987. Redfern, w., puns. Notes
Queries, (3).

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Conference on Empirical Meth-
ods in Natural Language Processing, pages 1532–
1543.

David E Rumelhart, Geoffrey E Hinton, and Ronald J
Williams. 1986. Learning representations by back-
propagating errors. Parallel Distributed Process-
ing Explorations in the Microstructure of Cognition,
323(6088):399–421.

Tao Shen, Tianyi Zhou, Guodong Long, Jing Jiang,
Shirui Pan, and Chengqi Zhang. 2018. Disan: Di-
rectional self-attention network for rnn/cnn-free lan-
guage understanding. In AAAI.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. 4:3104–3112.

Zhixing Tan, Mingxuan Wang, Jun Xie, Yidong Chen,
and Xiaodong Shi. 2018. Deep semantic role label-
ing with self-attention. In AAAI.

Keiko Tanaka. 1992. The pun in advertising: A prag-
matic approach. Lingua, 87(1-2):91–102.

Oriol Vinyals, Lukasz Kaiser, Terry Koo, Slav Petrov,
Ilya Sutskever, and Geoffrey Hinton. 2014. Gram-
mar as a foreign language. Eprint Arxiv, pages
2773–2781.

Yuhuan Xiu, Man Lan, and Yuanbin Wu. 2017. Ecnu
at semeval-2017 task 7: Using supervised and unsu-
pervised methods to detect and locate english puns.
In International Workshop on Semantic Evaluation,
pages 453–456.

Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho,
Aaron Courville, Ruslan Salakhutdinov, Richard
Zemel, and Yoshua Bengio. 2015. Show, attend and
tell: Neural image caption generation with visual at-
tention. Computer Science, pages 2048–2057.

Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng,
and Alex Smola. 2015. Stacked attention networks
for image question answering. pages 21–29.

Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He,
Alex Smola, and Eduard Hovy. 2016. Hierarchical
attention networks for document classification. In
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 1480–1489.


