



















































Transition-based Dependency DAG Parsing Using Dynamic Oracles


Proceedings of the ACL-IJCNLP 2015 Student Research Workshop, pages 22–27,
Beijing, China, July 28, 2015. c©2015 Association for Computational Linguistics

Transition-based Dependency DAG Parsing Using Dynamic Oracles

Alper Tokgöz
Istanbul Technical University

Department of Computer Engineering
Istanbul, Turkey

tokgoza@itu.edu.tr

Gülşen Eryiǧit
Istanbul Technical University

Department of Computer Engineering
Istanbul, Turkey

gulsen.cebiroglu@itu.edu.tr

Abstract

In most of the dependency parsing stud-
ies, dependency relations within a sen-
tence are often presented as a tree struc-
ture. Whilst the tree structure is suf-
ficient to represent the surface relations,
deep dependencies which may result to
multi-headed relations require more gen-
eral dependency structures, namely Di-
rected Acyclic Graphs (DAGs). This
study proposes a new dependency DAG
parsing approach which uses a dynamic
oracle within a shift-reduce transition-
based parsing framework. Although there
is still room for improvement on per-
formance with more feature engineer-
ing, we already obtain competitive perfor-
mances compared to static oracles as a re-
sult of our initial experiments conducted
on the ITU-METU-Sabancı Turkish Tree-
bank (IMST).

1 Introduction

Syntactic parsing is the process of determining
the grammatical structure of a sentence as con-
forming to the grammatical rules of the relevant
natural language. The structure of the sentence
is determined according to the grammar formal-
ism that the parser is built upon. Phrase struc-
ture parsers, also known as constituency parsers,
parse a sentence by splitting it into its smaller
constituents. On the other hand, in dependency
parsers, the structure of the sentence is represented
as dependency trees consisting of directed depen-
dency links between a dependent and a head word.

Data-driven dependency parsing frameworks
have gained increasing popularity in recent years

and been used in a wide range of applications such
as machine translation (Ding and Palmer, 2005),
textual entailment (Yuret et al., 2013) and question
answering (Xu et al., 2014). Most data-driven de-
pendency parsers achieve state-of-the art parsing
performances with a language agnostic approach
on the different syntactic structures of different
languages (Buchholz and Marsi, 2006). Mod-
ern data-driven dependency parsers can be catego-
rized into two groups: graph-based and transition-
based parsers. Graph-based parsers rely on the
global optimization of models aiming to find span-
ning trees over dependency graphs. On the other
hand, transition-based parsers work basically with
greedy local decisions that are deterministically
selected by oracles, which are generic machine
learning models trained to make decisions about
the next transition action. In a recent study,
Zhang and Nivre (2012) propose a new approach
on transition-based parsing that aims to provide
global learning instead of greedy local decisions.

Despite the high performances of both graph-
based and transition-based dependency parsers,
these are generally bounded by the constraint that
each dependent may not have multiple heads.
Therefore, the resulting parsing output is a tree
where words correspond to nodes and dependency
relations correspond to edges. Although depen-
dency trees yield satisfactory performances, they
are inadequate in capturing dependencies at dif-
ferent levels of semantic interpretations or more
complicated linguistic phenomena (e.g. relative
clauses, anaphoric references) which could result
in multi-head dependencies together with exist-
ing surface dependency relations. An example is
given in Figure 1 which is taken from the Turk-
ish IMST Treebank (Sulubacak and Eryiğit, 2015).
In Figure 1, the dependent token “Umut” depends

22



on more than one head token with SUBJECT re-
lations: 1) the verb “koşmak” (to run) and 2) the
verb “düşmek” (to fall). Adding a second rela-
tion (emphasized with a dash-dotted line in the fig-
ure) to the token ”Umut” breaks the condition that
each token may have at most one head, and ren-
ders existing dependency tree parsers incompati-
ble for this setup. It is also worth mentioning that
the deep dependencies in the IMST are not dis-
criminated from surface dependencies by the use
of different labels.

Umut koşar +ken düştü

‘Umut’ ‘[he] runs’ (WHILE) ‘[he] fell’

“Umut fell as [he was] running.”

SUBJECT

SUBJECT DERIV MODIFIER PREDICATE

Figure 1: Example for Turkish multi-head depen-
dencies.

In this paper, for the first time in the litera-
ture, we investigate the impact of using dynamic
oracles for parsing multi-head dependency struc-
tures by extending the approach of Goldberg and
Nivre (2012). We provide comparisons with the
replication of the basic shift-reduce DAG pars-
ing algorithm of Sagae and Tsujii (2008) and a
first time implementation of their proposed arc-
eager parsing algorithm. The remainder of the pa-
per first gives a background information about the
topic, then introduces the DAG parsing framework
and the proposed algorithms together with experi-
ments and results.

2 Background

Although it is possible to discover the syntactic re-
lations with a two stage model by first finding the
regular surface dependencies and then finding the
deep relations with post-processing as in Nivre et
al. (2010), it is not always straightforward to de-
cide which dependencies should be treated as sur-
face relations or deep relations as in the case of
Turkish. Thus, in this study, we focus on single
stage models and aim to discover the entire set of
relations in a single pass. McDonald and Pereira
(2006) use graph-based algorithms for DAG pars-
ing simply using approximate interference in an

edge-factored dependency model starting from de-
pendency trees. On the other hand, Sagae and
Tsujii (2008) propose a transition-based counter-
part for DAG parsing which made available for
parsing multi-headed relations. They modified the
existing shift-reduce bottom-up dependency pars-
ing algorithm of Nivre and Nilsson (2005) to al-
low multiple heads per token by the use of cycle
removal and pseudo-projectivization as a prepro-
cessing stage. They report higher performance
scores on the Danish treebank compared to Mc-
Donald and Pereira (2006).

A standard way of determining transition ac-
tions in a shift-reduce dependency parser is us-
ing static oracles. During the training stage, the
learning instances for the oracle are prepared by
the use of manually annotated gold-standard parse
trees and the current parsing configuration. Dur-
ing the parsing stage, the already trained oracle
decides on the next transition operation. One of
the problems with static oracles lays beneath the
spurious ambiguity, which implies there might be
more than one transition sequence for a given sen-
tence and the sequence proposed by an oracle may
not be the easiest to learn. The second problem oc-
curs when the parser makes a parsing error which
leads to a parser configuration from which the cor-
rect parse tree is not derivable. The algorithm does
not provide any solutions for dealing with the er-
ror propagation caused by such situations. The
idea of dynamic oracles introduced by Goldberg
and Nivre (2012) rises for handling the aforemen-
tioned handicaps of static oracles. Rather than re-
turning a unique transition for a given configura-
tion, a dynamic oracle returns a set of valid tran-
sitions regarding the current configuration, which
would allow the algorithm to explore non-optimal
configurations during the training procedure.

3 Transition-Based Solutions for
Dependency DAG Parsing

Transition-based parsing frameworks consider the
transition system to be an abstract machine that
processes input sentences and produces corre-
sponding parsing graphs. The tokens of the in-
put sequence and the partially created dependency
structures are kept within the following data struc-
tures:

1. a buffer β which includes the remaining un-
processed tokens in the input sequence in a
queue,

23



2. a stack σ which consists of the tokens being
processed,

3. a set A of assigned dependency arcs.

The transition actions explained in the follow-
ing subsections are basic stack and queue opera-
tions that correspond to parsing actions marking
dependency relations. The algorithm starts with a
buffer β initialized with all tokens of a sentence
preserving their sequence, and an empty stack σ.
The parsing process finishes when there are no
nodes left in β and only the artificial root in σ.

3.1 Basic shift-reduce parsing with multiple
heads

The first algorithm that is capable of parsing DAG
structures is the standard algorithm of Sagae and
Tsujii (2008). The complete list of the transitions
of this algorithm is as follows:

• Shift: Pops the first item of the buffer and
pushes it onto the top of the stack.

• Left-Reduce: Pops the top two items of the
stack, creates a left arc between them where
the top item is assigned as the head of the
item below, and pushes the head token back
onto the stack.

• Right-Reduce: Pops the top two items of
the stack, creates a right arc between them,
where the item below is assigned as the head
of the top item, and pushes the head token
back onto the stack.

• Left-Attach: Creates a left arc between the
top two items of the stack, where the top item
is assigned as the head of the one below. The
stack and the buffer remain unchanged.

• Right-Attach: Creates a right dependency arc
between the two top items on the stack and
assigns the top token as the dependent of the
token below. As the second step, it pops the
top of the stack and places it into the buffer
β.

3.2 Multi-Head Arc-Eager Parsing
Algorithm

The second transition algorithm introduced but not
implemented by Sagae and Tsujii (2008) is a vari-
ation of the Arc-Eager algorithm of Nivre et al.
(2007) and has the following transition operations:

• Shift: Pops the first item of the buffer and
pushes it onto the top token of the stack.

• Left-Arc: Creates a left dependency arc be-
tween the top token of the stack and the first
token of the input buffer, where the first token
in the buffer becomes the head and the one at
the top of the stack becomes the dependent.
It is also worth noticing that the stack and the
input buffer remains unchanged.

• Right-Arc: Creates a right dependency arc
between the top token of the stack and the
first token on the input buffer, where the to-
ken in the stack becomes the head, and the
token which is in front of the buffer becomes
the dependent. It is also worth noticing that
the stack and the input buffer remains un-
changed.

• Reduce: Pops the top item of the stack if and
only if it was previously associated with at
least one head.

3.3 Multi-Head Arc Eager Parsing with a
Dynamic Oracle

In order to design a dynamic oracle with the ca-
pability of parsing multi-head dependencies, we
need an efficient method for computing the cost
of each transition. To this end, we extend the
dynamic oracle defined by Goldberg and Nivre
(2012), considering DAG parsing arc-eager sys-
tem of Sagae and Tsujii (2008). Extended arc-
eager transition system will operate in the same
way as previously defined in Section 3.2, within a
dynamic oracle system whose cost function is de-
fined with a transition operation, the current con-
figuration c = (σ|s, b|β,A)1 and the gold parse
of the sentence (Ggold). Differing from Goldberg
and Nivre (2012), for ease of computation, we pre-
fer marking transitions as zero cost or costly in-
stead of computing the exact cost:

• Cost(LeftAttach, c,Ggold) Attaching s to b
with a left arc is costly, if there is a right arc
between s and b, or it is already attached with
a left arc.

• Cost(RightAttach, c,Ggold) Attaching s to
b by creating right arc is costly, if there is a
left arc between s and b, or it is already at-
tached with a right arc.

1In c = (σ|s, b|β,A), s denotes the top token of the stack
σ, b denotes first item of buffer β, A denotes revealed arcs

24



• Cost(Reduce, c,Ggold) Popping s from the
stack means it will be no longer possible to
associate it with any head or dependent from
buffer β, therefore it is costly if it has heads
or dependents in the β.

• Cost(Shift, c,Ggold) Pushing b onto the
stack means it will be no longer possible to
associate it with any heads or dependents in
stack σ, therefore it is costly if it has a head
or dependent token in the σ.

Since left attach and right attach operations do
not change the parser configuration (i.e. these op-
erations cannot lead to a parser configuration from
which the gold tree is not reachable), their cost is
measured according to the validity of the attach-
ment. The only difference of our multi-head vari-
ant from the single head arc-eager transition sys-
tem is that the left and right arc operations do not
change the parser state. As such, it is essentially a
relaxed version of the single-head system. There-
fore, since the arc-decomposition property holds
for the single-head system (as proven by Goldberg
and Nivre (2013)), it also holds for our variant.

We use the same online training procedure (with
the perceptron algorithm) as Goldberg and Nivre
(2012) given in Algorithm 1.

Algorithm 1 Online training with dynamic oracle
1: procedure TRAIN
2: w ← 0
3: for I ← 1, ITERATIONS do
4: c← cs(x)
5: for sentence x do
6: while c is not terminal do
7: tp ← argmaxtw.Φ(c, t)
8: ZC ←
{t|o(t; c;Ggold) = true}

9: to ← argmaxt�ZCw.Φ(c, t)
10: if tp 6∈ ZC then
11: w ← w + Φ(c, to) −

Φ(c, tp)

12: tn ←NEXT(I, tp, ZC)
13: c← tn(c)
14: procedure NEXT(I, t, ZC)
15: if t ∈ ZC then
16: return t
17: else
18: RANDOM ELEMENT (ZC)

The proposed oracle will return a set of zero
cost transition operations (denoted as ZC at line
8) where the costs are calculated according to the
cost function defined above. Feature weights will
be updated only if the perceptron model makes a
transition prediction that does not belong to the
zero cost transitions (lines 10 and 11). After that,
the next transition operation is chosen by the func-
tion NEXT, which returns the transition that is pre-
dicted by the model if it belongs to zero cost tran-
sitions; if not, it returns a random transition which
belongs to the zero cost transition set.

4 Experiments

In order to apply the specified DAG parsing al-
gorithm to non-projective sentences, a pseudo-
projective transformation operation is applied to
the IMST. For that aim, we apply Head scheme2

described by Nivre (2005). Moreover, before the
application of this pseudo-projective transforma-
tion, the cyclic dependency paths are handled as
described by Sagae and Tsujii (2008), by reversing
the shortest arc within the cyclic dependency path
until no cyclic path remains. 99.3% precision and
99.2% recall are acquired on IMST by applying
the pseudo-projective transformation and detrans-
formation operations. As a learning component,
we follow the work of Sagae and Tsujii (2008)
and use a Maximum Entropy model for the clas-
sification with the greedy search algorithm. For
the dynamic oracle experiment, we use an aver-
aged perceptron algorithm iterating 15 times over
the training data.

The following features are used in all of the ex-
periments:

• The POS tag, lemma and morphological fea-
tures of the top 3 tokens on the stack and the
first 3 tokens in the buffer.

• The POS tag and dependency relations of the
rightmost and leftmost modifiers of the top 2
items on the stack.

• The number of heads and dependents of the
top item of the stack and the first item of the
buffer.

• The dependency relations of the top of the
stack.

2Although other schemes could be tried for DAGs for bet-
ter performance, this is left for future work due to time con-
straints.

25



• Whether the top 2 tokens on the stack have a
dependency relation between them or not.

• Whether the top token of the stack and the
first of the buffer have a dependency relation
between them or not, and if so the direction
and the type of the relation.

• Combination of the surface form of the top
token of the stack and its number of left and
right modifiers.

• Combination of the surface form of the first
token of the buffer and its number of left and
right modifiers.

• The surface forms and POS tags of heads of
the top token of the stack and the first token
of the buffer.

• The previous two parsing actions.
For training and evaluation purposes, we use the

IMST with ten-fold cross validation. Experiment
results are given in Table 4.

Table 1: Unlabeled scores of experiments with
using IMST.

Experiment Precision Recall F1
Static-Standard 79.42 77.56 78.50
Static-Eager 78.90 76.79 77.83
Dynamic-Eager 79.68 81.17 80.42

As shown in Table 4, the static arc-eager DAG
implementation for Turkish performs slightly
worse than the arc-standard algorithm. This is not
surprising in the light of previous studies (Nivre,
2008; Eryiğit et al., 2008) reporting that the arc
standard algorithm performs better in tree parsing
due to the smaller number of classes to be learned
by the oracle. However, the proposed multi-head
arc-eager algorithm with dynamic oracle (referred
to as Dynamic-Eager) yields the best precision, re-
call and F1 scores among the three experiments.3

In this study, although there is still room for
improvement on performance with more feature
engineering, we obtain better results on Turkish
IMST treebank between static and dynamic ora-
cles with our newly proposed method for parsing

3The difference of this model from the runner-up models
are found to be statistically significant according to McNe-
mar’s test (p < 0.0001)

DAGs. This encourages us to test our system with
different languages as future work with the expec-
tation that the ameliorations will be much higher
than the reported ones in the single-head scenario.

5 Conclusion and Future Works

In this paper, we experimented with three differ-
ent transition-based algorithms for DAG parsing
which eliminate the single-head constraint of tra-
ditional algorithms and allows multi-head depen-
dency relations to represent more complicated lin-
guistic phenomena along with surface relations.
We present the first results for arc-eager DAG
parsing with static oracles and propose a new arc-
eager DAG parsing algorithm using dynamic ora-
cles. Our initial experiments conducted on Turkish
pave the way for future research on the usage of
the dynamic arc-eager DAG parsing for other lan-
guages. For future work, we will first conduct ex-
periments on how well the Dynamic-Eager algo-
rithm performs on different treebanks, including
multi-head dependencies (such as the Danish tree-
bank (Kromann, 2003)). Secondly, we will con-
duct experiments on previously described static-
oracle parsing algorithms by using different clas-
sifiers such as Support Vector Machines.

Acknowledgments

We hereby acknowledge that this study is part of
a research project named ”Parsing Web 2.0 Sen-
tences” that is supported by TÜBİTAK (Turkish
Scientific and Technological Research Council)
1001 program (grant number 112E276) and part
of the ICT COST Action IC 1207.

References

Sabine Buchholz and Erwin Marsi. 2006. Conll-x
shared task on multilingual dependency parsing. In
Proceedings of the Tenth Conference on Computa-
tional Natural Language Learning, pages 149–164.
Association for Computational Linguistics.

Yuan Ding and Martha Palmer. 2005. Machine trans-
lation using probabilistic synchronous dependency
insertion grammars. In Proceedings of the 43rd An-
nual Meeting on Association for Computational Lin-
guistics, pages 541–548. Association for Computa-
tional Linguistics.

Gülşen Eryiğit, Joakim Nivre, and Kemal Oflazer.
2008. Dependency parsing of Turkish. Computa-
tional Linguistics, 34(3):357–389.

26



Yoav Goldberg and Joakim Nivre. 2012. A dynamic
oracle for arc-eager dependency parsing. In COL-
ING, pages 959–976.

Yoav Goldberg and Joakim Nivre. 2013. Training
deterministic parsers with non-deterministic oracles.
Transactions of the association for Computational
Linguistics, 1:403–414.

Matthias T Kromann. 2003. The danish dependency
treebank and the underlying linguistic theory. In
Proc. of the Second Workshop on Treebanks and Lin-
guistic Theories (TLT).

Ryan T McDonald and Fernando CN Pereira. 2006.
Online learning of approximate dependency parsing
algorithms. In EACL. Citeseer.

Joakim Nivre and Jens Nilsson. 2005. Pseudo-
projective dependency parsing. In Proceedings of
the 43rd Annual Meeting on Association for Compu-
tational Linguistics, pages 99–106. Association for
Computational Linguistics.

Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, Gülsen Eryigit, Sandra Kübler, Svetoslav
Marinov, and Erwin Marsi. 2007. Maltparser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13(02):95–135.

Joakim Nivre, Laura Rimell, Ryan McDonald, and Car-
los Gomez-Rodriguez. 2010. Evaluation of depen-
dency parsers on unbounded dependencies. In Pro-
ceedings of the 23rd International Conference on
Computational Linguistics, pages 833–841. Associ-
ation for Computational Linguistics.

Joakim Nivre. 2008. Algorithms for deterministic in-
cremental dependency parsing. Computational Lin-
guistics, 34(4):513–553.

Kenji Sagae and Jun’ichi Tsujii. 2008. Shift-reduce
dependency DAG parsing. In Proceedings of the
22nd International Conference on Computational
Linguistics-Volume 1, pages 753–760. Association
for Computational Linguistics.

Umut Sulubacak and Gülşen Eryiğit. 2015. A rede-
fined Turkish dependency grammar and its imple-
mentations: A new Turkish web treebank & the re-
vised Turkish treebank. under review.

Kun Xu, Sheng Zhang, Yansong Feng, and Dongyan
Zhao. 2014. Answering natural language questions
via phrasal semantic parsing. In Natural Language
Processing and Chinese Computing, pages 333–344.
Springer.

Deniz Yuret, Laura Rimell, and Aydın Han. 2013.
Parser evaluation using textual entailments. Lan-
guage resources and evaluation, 47(3):639–659.

Yue Zhang and Joakim Nivre. 2012. Analyzing
the effect of global learning and beam-search on
transition-based dependency parsing. In COLING
(Posters), pages 1391–1400.

27


