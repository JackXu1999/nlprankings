



















































The Error-driven Ranking Model of the Acquisition of Phonotactics: How to Keep the Faithfulness Constraints at Bay


Proceedings of the 2014 Joint Meeting of SIGMORPHON and SIGFSM, pages 10–18,
Baltimore, Maryland USA, June 27 2014. c©2014 Association for Computational Linguistics

The error-driven ranking model of the acquisition of phonotactics:
how to keep the faithfulness constraints at bay

Giorgio Magri
SFL (CNRS and University of Paris 8)

UiL OTS (Utrecht University)
magrigrg@gmail.com

Abstract

A problem which arises in the theory of
the error-driven ranking model of the ac-
quisition of phonotactics is that the faith-
fulness constraints need to be promoted
but should not be promoted too high. This
paper motivates this technical problem and
shows how to tune the promotion compo-
nent of the re-ranking rule so as to keep the
faithfulness constraints at bay.

Sections 1-2 introduce the algorithmic frame-
work considered in the paper, namely the error-
driven ranking model of the acquisition of phono-
tactics. Section 3 motivates a specific problem
which arises in the design and analysis of this
model, namely the problem of controlling the
height reached by the faithfulness (F) constraints.
Sections 4-6 sketch the theory of F-controlling.
Magri (2014a) presents the theory in more detail.

1 The acquisition of phonotactics

Generative linguistics assumes that the learner is
provided with a typology of grammars G1, G2...
The language-learning problem thus consists of
individuating the target adult grammar G∗ within
the typology, on the basis of a finite set of data
generated by that grammar. Various formulations
of this problem differ for the structural assump-
tions about the underlying typology, for the type
of data fed to the learner, and for the criteria of
success used to evaluate the grammar Ĝ chosen
by the learner relative to the target grammar G∗.

In this paper, I focus on the following spe-
cific formulation of this general language learn-
ing problem. The typology consists of the phono-
logical grammars defined in Optimality Theoretic
(OT) terms through the rankings of a given set of
constraints (Prince and Smolensky, 2004). The
data fed to the learner consist of surface forms
sampled from the language L∗ generated by the

target OT grammar G∗, namely the set of surface
forms which are the phonological realizations of
some underlying forms according to G∗. The cri-
teria for success is that the OT grammar Ĝ chosen
by the learner generates a language L̂ which coin-
cides with the target one: L̂ = L∗.

This specific formulation is called the problem
of the acquisition of phonotactics. In fact, phono-
tactics is the knowledge of the distinction between
licit and illicit forms. Assuming that the distinc-
tion is categorical (Gorman, 2013), knowledge of
phonotactics reduces to knowledge of the set of
licit forms (the set of illicit forms is just the com-
plement). And the set of licit forms relative to an
OT grammarG is the corresponding language LG.

2 The EDRA model

In this paper, I focus on a specific algorithmic ap-
proach to the problem of the acquisition of phono-
tactics, based on error-driven ranking algorithms
(EDRAs). This approach is summarized below
and explained in the rest of this section.

Algorithm 1 The EDRA model
Initialize

the ranking values of F constraints to zero
the ranking values ofM constraints to θinit>0

Repeat
1 get a surface form [y] from the target language
2 pick a loser form [z]
3 check whether the current ranking vector θ is

consistent with the underlying/winner/loser
form triplet (/y/, [y], [z])

4 if it isn’t, update the current ranking vector θ
until no more mistakes are made at step 3

The EDRA model maintains a current hypothe-
sis of the target adult grammar, namely a current
constraint ranking. This ranking is represented nu-
merically through a ranking vector θ = (θ1, , θn)

10



which assigns to each constraint Ck a numerical
ranking value θk. A constraint Ck is ranked above
another constraint Ch according to a ranking vec-
tor θ provided the ranking value θk of the former
is (strictly) larger than the ranking value θh of the
latter (Boersma, 1998; Boersma, 2009).

The current ranking (vector) is initialized in
such a way that the corresponding initial language
is as small as possible. OT constraints come in two
varieties: faithfulness (F) and markedness (M)
constraints. A smallest language corresponds to
a ranking which assigns all F constraints under-
neath all M constraints. Thus, the F constraints
are assigned a small initial ranking value, say zero
for concreteness; and theM constraints start with
a large positive initial ranking value θinit > 0. The
algorithm then loops through the three steps 1-4.

At step 1, the EDRA model receives a piece of
data, namely a surface form [y] sampled from the
target language L∗. Assuming that the underly-
ing typology satisfies Tesar’s (2013) Surface Ori-
entedness Condition, this piece of data provides
evidence that the target grammar G∗ maps this
phonological form (construed as the underlying
form /y/) into itself (construed as the surface form
[y]) rather than reducing it to some non-faithful
candidate [z] (as a mnemonic, I strike out a candi-
date construed as a loser). In other words, the tar-
get adult ranking (vector) θ∗ is consistent with the
underlying/winner/loser form triplet (/y/, [y], [z])
for any loser [z], namely it satisfies condition (1).
Here,W (L) is the set of winner-preferring (loser-
preferring) constraints, namely those which assign
less (more) violations to the faithful mapping of
/y/ to [y] than to the neutralization of /y/ to [z].

(1) max
Ck∈W

θ∗k > max
Ch∈L

θ∗h

This consistency condition (1) says that there is
at least a winner-preferring constraint which is
ranked above all loser-preferring constraints by
the target ranking (vector) θ∗ = (θ∗1, . . . , θ∗n).

At steps 2 and 3, the EDRA model thus picks
a specific loser [z] and checks whether its cur-
rent ranking vector θ satisfies the corresponding
consistency condition (1). Failure to satisfy this
condition means that the current ranking values
of the loser-preferring (winner-preferring) con-
straints are too large (too small). The algorithm
thus promotes the winner-preferring constraints
by a small promotion amount and demotes the
loser-preferring constraints by a small demotion

amount. What matters is not the actual values of
the promotion and demotion amounts, but rather
their ratio. Thus, the demotion amount can be
set equal to 1 for concreteness, letting instead the
promotion amount be equal to an arbitrary non-
negative constant p ≥ 0, as in (2).

(2) a. Increase the ranking value of each winner-
preferring constraint by p ≥ 0;

b. decrease the ranking value of each undom-
inated loser-preferring constraint by 1.

Crucially, not all loser-preferring constraints are
demoted by (2b), but only those that need to be
demoted, namely the undominated ones (Tesar and
Smolensky, 1998), whose current ranking value is
at least as large as the ranking value of all winner-
preferring constraints and thus are responsible for
flouting the consistency condition (1).

3 The problem of F-control
The crucial implementation parameter of the
EDRA model is the promotion amount p ≥ 0 used
in the promotion component (2a) of the re-ranking
rule. How should this parameter be tuned so as to
optimize the performance of the EDRA model of
the acquisition of phonotactics? This section ex-
plains how this question leads to the problem of
controlling the height of the F constraints.
3.1 Some initial guarantees
The problem of the acquisition of phonotactics
in OT is intractable: no algorithm can solve effi-
ciently an arbitrary instance of the problem cor-
responding to an arbitrary constraint set (Magri,
2013a). Prompted by this intractability result, Ma-
gri (2013b) starts to tackle the problem by looking
at a class of “easy” cases.

The intuitive idea is that the relative ranking
of the F constraints might often be irrelevant for
phonotactics, namely for drawing the line between
licit and illicit forms (although it is of course al-
ways crucial for phonology, namely for the spe-
cific way in which illicit forms are repaired). This
intuition that the relative ranking of the F con-
straints is not relevant to describe a certain phono-
tactic pattern can be formalized as follows. A
partial constraint ranking is any partial order on
the constraint set. A partial ranking generates
a language provided each one of its total refine-
ments generates that language in the usual OT
sense (Yanovich, 2012). A language is called F-
irrelevant provided it can be generated in this tech-

11



nical sense by a partial ranking which does not
rank any two F constraints relative to each other
(see subsection 3.2 for an example).

Suppose that the EDRA model is trained on a
target language L∗ which is F-irrelevant. The
M constraints start out high, with an initial rank-
ing value θinit usually larger than the number m
of markedness constraints. The F constraints in-
stead start out low, with a null initial ranking
value. Throughout learning, the F constraints will
raise, if the algorithm adopts a non-null promotion
amount p > 0. Theorem 1 provides guarantees
that the EDRA model learns the target phonotac-
tics, as long as the F constraints don’t raise too
high, namely their ranking values remain smaller
by at least m than the initial ranking value θinit of
theM constraints, as stated in (3).
Theorem 1 Suppose that the underlying OT ty-
pology satisfies the following two assumptions.
First, if a surface form [y] is a non-faithful candi-
date of an underlying form /x/, then there exists
at least one faithfulness constraint which assigns
at least one violation to the mapping of /x/ into
[y] (F-discernibility assumption). Second, a form
[y] is a candidate of an underlying form /x/ if and
only if the latter form construed as the surface
form [x] is vice versa a candidate of the former
form construed as the underlying form /y/ (sym-
metric candidacy assumption). Consider a lan-
guage in this OT typology which is F-irrelevant.
Suppose that the EDRA model only makes a fi-
nite number of errors and then converges to a fi-
nal ranking vector which is never updated again.
Suppose furthermore that the ranking value θF of
any F constraint F at any time in the run satisfies
condition (3), where m is the number ofM con-
straints and θinit > m their initial ranking value.

(3) θF ≤ θinit −m
Then, the language generated by (an arbitrary re-
finement of) the final ranking vector learned by the
EDRA model coincides with the target language
the EDRA model has been trained on. �

The two assumptions of F-discernibility and
symmetric candidacy required by theorem 1 are
extremely mild. Magri (2013b; 2014b) conjec-
tures that the relative ranking of the faithfulness
constraints turns out to matter for phonotactics
only in very special configurations, so that the
F-irrelevancy assumption might plausibly hold in
the vast majority of cases. Theorem 1 thus pro-

vides guarantees that the EDRA model succeeds
at the problem of the acquisition of phonotactics
in a large class of cases under two crucial assump-
tions. One assumption is that it can only make a
finite number of errors before it converges to a fi-
nal ranking which is consistent with any form and
thus never updated. The other assumption is the
condition (3) that the height of the F-constraints
can be properly controlled.

3.2 Some examples

To illustrate the issues raised by convergence and
F-control, consider the following OT typology.
The set of forms consists of only four forms
{apsa, apza, absa, abza}. The faithfulness con-
straints are the two identity constraints for voicing
in stops and fricatives (F1, F2). The markedness
constraints are the two corresponding constraints
against stop and fricative voicing (M1,M2) plus
an additional constraint M which bans sequences
of stops and fricatives which agree in voicing,
namely it is violated by the two forms apsa and
abza. The candidacy relation is total: the four
forms are all candidates of each other.

The OT typology just described contains in par-
ticular the language L = {[absa], [apza]}. This
language is generated by any ranking which satis-
fies the ranking conditions (4).

(4) M

F2 F1

M1 M2

These ranking conditions (4) say nothing about the
relative ranking of the two F constraints F1 and
F2. The language L thus qualifies as F-irrelevant.

When trained on this language, the EDRA
model will be provided at step 1 with a sequence
of the two licit forms [absa] and [apza]. It will then
complete them into an underlying/winner/loser
form triplet at steps 2 and 3 by assuming a faith-
ful underlying form and a non-faithful loser form.
The list of all possible such triplets that the algo-
rithm can consider is provided in (5).

(5) 

F1 F2 M1 M2 M

(/absa/, [absa], [apsa]) W e L e W
(/absa/, [absa], [abza]) e W e W W

(/absa/, [absa], [apza]) W W L W e
(/apza/, [apza], [abza]) W e W e W
(/apza/, [apza], [apsa]) e W e L W

(/apza/, [apza], [absa]) W W W L e


12



Each triplet is described here in ERC notation
(Prince, 2002): constraints which are winner- or
loser-preferring or even relative to a triplet are
marked with a corresponding W or L or e.

The triplets where the constraint M is winner-
preferring will trigger virtually no update, since
that constraint starts high and is never demoted,
and will thus always ensure consistency with those
triplets. The learning run is thus driven by the two
remaining triplets, boldfaced in (5), which I as-
sume are fed one after the other to the algorithm.
Suppose the promotion amount is non-null, say
equal to the demotion amount: p = 1. The re-
sulting learning run is described in (6).

(6) 
F1 0
F2 0
M1 5
M2 5
M 5

→

1
1
4
6
5

→

2
2
5
5
5

→

3
3
4
6
5

→

4
4
5
5
5

→

5
5
4
6
5

→

6
6
5
5
5


The two F constraints end up too high, namely
with a final ranking value θF1 = θF2 = 6 which
is larger than the initial ranking value θinit = 5
of theM constraints. And indeed the EDRA has
failed at learning the target phonotactics: since the
F constraints are ranked at the top, the model has
incorrectly learned that any form is licit.

A trivial strategy to enforce the F-control con-
dition (3) would be to threshold the promotion
component (2a) of the re-ranking rule, as in (2a′).

(2) a′. Increase the ranking value of each
winner-preferring constraint by p, except
for an F constraint which is already
close to the forbidden threshold θinit−m.

Yet, suppose we tried to remedy to the failure
in (6) by thresholding the promotions as in (2a′).
When an F constraint reaches the height θinit −
m = 5 − 3 = 2, we stop promoting it, as bold-
faced in the learning run (7).

(7) 
F1 0
F2 0
M1 5
M2 5
M 5

→

1
1
4
6
5

→

2
2
5
5
5

→

2
2
4
6
5

→

2
2
5
5
5

→

2
2
4
6
5

→

2
2
5
5
5

 . . .
In this run, the constraint M stays put at its initial
position. The constraints M1 and M2 oscillate up
and down, because promoted and demoted by the
two boldfaced triplets in (5). The constraints F1
and F2 raise a bit until they hit the threshold, and
then settle. The EDRA model will thus keep mak-
ing mistakes forever, without ever converging to a
ranking vector consistent with the data.

3.3 Against a null promotion amount
These difficulties with convergence and the F-
control condition (3) would disappear if the pro-
motion amount p was set equal to zero, so that
the EDRA performs no constraint promotion at
all. In fact, Tesar and Smolensky (1998) guarantee
convergence for the demotion-only case. And the
F constraints could not possibly be promoted too
high, as they would not be promoted at all.

Unfortunately, the option of a null promotion
amount is not viable, as argued in Magri (2012;
2014b). In fact, recall that the EDRA model at
step 3 always considers underlying/winner/loser
form triplets (/y/, [y], [z]) which have an underly-
ing form /y/ faithful to the winner [y]. This means
that the F constraints are never loser-preferring
and are therefore never demoted. If the promotion
amount is set equal to zero, then they will not be
promoted either. In the end, the F constraints will
thus never be re-ranked. This hampers the ability
of the EDRA model to learn the correct relative
ranking of the F constraints when trained on a F-
relevant language, namely when it needs to learn a
phonotactic pattern which crucially does require a
specific relative ranking of the F constraints.
3.4 Convergence through calibration
As recalled above, Tesar and Smolensky (1998)
show that the EDRA model converges when the
promotion amount is null and the algorithm per-
forms only constraint demotion. It could in princi-
ple be the case that convergence does not extend to
the demotion/promotion case, because any amount
of promotion disrupts convergence. But Magri
(2012) shows that is not the case: convergence ex-
tends to EDRAs which perform constraint promo-
tion as well, as long as the promotion amount is
small enough. In particular, consider a promotion
amount p which scales as in (8) with the numbers
` and w of currently undominated loser-preferring
constraints and of winner-preferring constraints.

(8) p =
`

w + σ
It turns out that the EDRA model converges ef-
ficiently if (and only if) the promotion amount
is calibrated, namely has the shape in (8) corre-
sponding to some strictly positive calibration con-
stant σ > 0. The larger the calibration constant σ,
the smaller the promotion amount. The case of a
null promotion amount corresponds to the limiting
case σ =∞.

13



3.5 F-control through calibration as well
Let’s take stock. Theorem 1 provides some ini-
tial guarantees of success of the EDRA model of
the acquisition of phonotactics. These guarantees
hold under two crucial assumptions: convergence
and theF-control condition (3). Do these assump-
tions hold when the promotion amount is non-
null? Convergence does hold, if the promotion
amount, although not null, is nonetheless small,
namely calibrated as in (8). What about the F-
control condition (3)? Can we play the same trick
of a small promotion amount? Or is it the case
that, no matter how small the promotion amount,
as soon as it is allowed to be non-null, the F con-
straints raise too high through a long sequence of
very small promotions? Section 4 shows that the
latter scenario can never arise: the F constraints
can never raise too high if the promotion amount
is small enough. More precisely, it assumes a cal-
ibrated promotion amount as in (8). And it shows
that the F-control condition (3) holds when the
calibration constant σ is large enough, namely it
grows as the number m ofM constraints.

As the calibration constant increases as the
number m of markedness constraints, the promo-
tion amount decreases quickly. Is it possible to
improve on the analysis of section 4 and guaran-
tee the F-control condition (3) with a calibration
constant σ which does not grow with the number
m of markedness constraints? Unfortunately, sec-
tion 5 shows that the calibration constant must in-
crease with m. More precisely, this section con-
siders the very simple case where there is a single
F constraint and where theM constraints are al-
ways loser-preferring (or even) but never winner-
preferring. In this case, the F-control condition
fails if the calibration constant σ does not grow
with m at least as mlogm .

Interestingly, the derivative of the function mlogm
goes to zero as m grows. In other words, although
the function increases with m, the rate of increase
becomes smaller and smaller, making this func-
tion as close as possible to a constant. Is this par-
ticularly favorable choice of the calibration con-
stant only possible in the peculiar case considered
in section 5? or does this favorable choice of the
calibration constant ensure F-calibration also in
the general case? Section 6 shows how to relax at
least one of the two restrictive assumptions made
in section 5, namely the assumption that the M
constraints are never winner-preferring.

4 F-controlling with a non-null
promotion amount

The most basic question of the theory ofF-control
is as follows: is it possible to guarantee the F-
control condition (3) despite a non-null promotion
amount? This section provides a positive answer
to this question. In particular, assume the promo-
tion amount p is calibrated as in (8), through the
calibration constant σ. The F-control condition
then holds provided the calibration constant σ sat-
isfies the bound (9), where m is the number ofM
constraints and θinit is their initial ranking value.

(9) σ ≥ 2m+mθ
init

θinit −m
To get a sense of the bound (9), assume that the
initial ranking value θinit of the M constraints is
some power of the number m of M constraints:
θinit = mk for some k > 1. The bound (9) thus
becomes m

k+2
mk−1−1 , which is approximately m.

At each update, each of the ` currently undom-
inated loser-preferring constraints is demoted by
1 and each of the w winner-preferring constraints
is promoted by p. Because of the specific shape
(8) of the promotion amount p, the sum of the
current ranking values decreases by ` − wp =
` − w`w+σ = `σw+σ . And the latter is at least σw+σ ,
as every update requires at least one undominated
loser-preferring constraint, namely ` ≥ 1. Let αi
be the number of updates triggered by the ith ERC
in the run considered up to the time considered
(note that there is only a finite number of ERCs
relative to a finite number of constraints). Thus,
the sum

∑
k θk of the current ranking values has

overall decreased by at least
∑

i αi
σ

wi+σ
relative

to the the sum
∑

k θ
init
k of the initial ranking val-

ues, as stated in (10).

(10)
∑
k

θk ≤
∑
k

θinitk −
∑
i

αi
σ

wi + σ

The sum
∑

k θ
init
k of the initial ranking values

can be computed explicitly as in (11), as the m
M constraints start with the initial ranking value
θinit while theF constraints start with a null initial
ranking value.

(11)
∑
k

θinitk = mθ
init

The sum
∑

k θk of the current ranking values can
be lower bounded as in (12).

14



(12)
∑
k

θk
(a)
=

∑
F

θF +
∑
M

θM

(b)

≥ 0 +
∑
M

θM

(c)
> 0 +m(−2) = −2m

In step (11a), I have split the sum over all con-
straints into the sum over the faithfulness and the
markedness constraints. In step (11b), I have noted
that the ranking value θF of any faithfulness con-
straint F is always at least as large as 0. In fact,
the faithfulness constraints start with a null ini-
tial ranking value and are never demoted, because
the EDRA model always assumes an underlying
form faithful to the winner, so that the faithful-
ness constraints are never loser-preferring. In step
(11c), I have noted that the ranking value θM of
a markedness constraint M can never get smaller
than −2. In fact, suppose by contradiction that M
managed to be demoted that low. That would im-
ply that some ERC triggers an update that demotes
M despite the fact that its current ranking value
is strictly smaller than 0. And that is impossible.
In fact, at least one faithfulness constraint F must
be winner-preferring relative to that ERC, because
of the F-discernibility assumption. Furthermore,
that constraint F must already dominate M , be-
cause F has a non-negative current ranking value
while M has a negative current ranking value.

Using the expressions for the sum of the ini-
tial and the current ranking values obtained in (11)
and (12) respectively, the original inequality (10)
yields the bound in (13).

(13)
∑
i

αi
1

wi + σ
<

2m+mθinit

σ

The ranking value θF of a generic faithfulness
constraint F can now be bound as in (14).

(14) θF
(a)

≤
∑
i

αi
1

wi + σ
(b)
<

2m+mθinit

σ
(c)

≤ θinit −m
In step (14a), I have used the fact that the faithful-
ness constraint F starts with a null initial ranking
value and is promoted by 1wi+σ for each one of the
αi updates triggered by the ith ERC, as long as F
is winner-preferring relative to that ERC. In step
(14b), I have used the bound computed in (13).

And in step (14c), I have used the choice (9) of the
calibration constant σ.

The bound obtained in (14) guarantees that
the generic faithfulness constraint F never raises
above the forbidden threshold θinit−m, thus com-
plying with the F-control condition (3). In other
words, we have obtained the following sufficient
solution to the problem of F-controlling.
Theorem 2 Suppose the underlying typology sat-
isfies the F-discernibility assumption. Consider a
run of the EDRA model on an arbitrary language
in that typology. Assume that the F constraints
start out with a null initial ranking value while the
m M constraints start out with an initial rank-
ing value θinit > m. Assume furthermore that the
promotion amount is calibrated as in (8) and that
the calibration constant σ is large enough to sat-
isfy the bound (9). Then, the ranking values of the
F constraints remain smaller than the forbidden
threshold θinit −m throughout the entire run. �

5 F-controlling on the diagonal case
The preceding section has established the F-
control condition (3) when the promotion amount
is not null, provided it is small enough, namely it
corresponds to a calibration constant which grows
as the numberm ofM constraints. Is it possible to
do better? In particular, is it possible to guarantee
F-control when the calibration constant does not
increase with m? This section sketches a coun-
terexample which provides a negative answer to
this question; see Magri (2014a) for details.

At every iteration, the EDRA model receives
a winner form sampled from the target language,
assumes a corresponding faithful underlying form
and picks a corresponding loser candidate. At ev-
ery iteration, the model thus constructs an under-
lying/winner/loser form triplet, which can be de-
scribed in terms of the corresponding ERC, as ex-
emplified in (5) above. Since there are only a finite
number of ERCs corresponding to a finite number
of constraints, the ERCs considered in a run of the
model can be stacked one on top of the other into
an input ERC matrix.

Without loss of generality, assume that each
input ERC has a unique loser-preferring con-
straint. Next, let me make two crucial assump-
tions. First, assume that the constraint set contains
a single faithfulness constraint F – plus of course
a certain number m of markedness constraints
M1, . . . ,Mm. Second, assume that M1, . . . ,Mm

15



Figure 1: First three stages of the learning dynamics where each diagonal ERC is fed persistently in turn

ERC 1

θinit/3

0

θinit

F

M1

M2, , Mm

ERC 1 ERC 2
0

θinit

θinit/3

2
9 θ

init

F

M1

M2

M3, , Mm

ERC 1 ERC 2 ERC 3
0

θinit

1
3 θ

init

2
9 θ

init

4
27 θ

init

F

M1

M2
M3

M4, , Mm

are either loser-preferring or even in the input
ERCs, but never winner-preferring. The input
ERC matrix thus is (a subset of) the matrix (15).

(15) 
F M1 ... Mm

ERC 1 W L e
... | . . .

ERC m W e L


The column corresponding to F consists of all
W’s. The entries corresponding to M1, . . . ,Mm
are all equal to e’s but for the diagonal of L’s. This
ERC matrix is thus called diagonal.

What is the maximum height that the constraint
F can reach in a run of the EDRA model on the
input diagonal ERC matrix (15)? To address this
question, consider the following special run. To
start, we persistently feed ERC 1 to the algorithm,
until the markedness constraint M1 is demoted
underneath the faithfulness constraint F and that
ERC cannot trigger any further update. Only at
that point, we stop feeding ERC 1 to the algo-
rithm, and persistently feed ERC 2 instead, again
until it cannot trigger any further update. Only at
that point, we stop feeding ERC 2 and persistently
feed ERC 3. And so on.

Assume that the promotion amount has the
shape (8) and suppose for concreteness that the
calibration constant is σ = 1, so that the faith-
fulness constraint is promoted by 1/2 with each
update. The dynamics of the ranking values is
depicted in Figure 1 for the first three learning
stages. Throughout stage 1, it is ERC 1 that trig-
gers updates, whereby the markedness constraint
M1 is demoted and the faithfulness constraint is
promoted by 13θ =

1
2+σθ

init, until the two con-
straints meet. Throughout stage 2, it is ERC 2 that
triggers updates, whereby the markedness con-
straint M2 is demoted and the faithfulness con-
straint is promoted by another 29θ =

1+σ
(2+σ)2

θinit,
until the two constraints meet. Throughout the
generic kth stage, it is the kth ERC that trig-
gers updates, whereby the markedness constraint
Mk is demoted and the faithfulness constraint pro-

moted by an amount that turns out to be equal to
(1+σ)k−1
(2+σ)k

θinit. The height θF reached by the faith-
fulness constraint at the end of the special run con-
sidered is thus

∑m
k=1

(1+σ)k−1
(2+σ)k

θinit. It turns out
that this is indeed the maximum height reacheable
by the faithfulness constraint F on any run on the
diagonal ERC matrix (15).

The F-control condition (3) thus boils down to
the inequality

∑m
k=1

(1+σ)k−1
(2+σ)k

θinit ≤ θinit − m.
Assume that the m markedness constraints start
out with the initial ranking value θinit = mk. This
inequality can then be solved analytically yielding
σ(m) = (1− exp {(−k logm)/m})−1. By a first
order Taylor expansion exp(x) ∼ 1+x+o(x2) of
the exponential function, the latter expression can
be approximated as in (16).

(16) σ = σ(m) ∼ m
k logm

The latter bound for the calibration threshold
is substantially smaller than the linear bound
σ(m) ∼ m obtained through the elementary anal-
ysis of section 6. In particular, although (16) is not
bounded as a function of m, its derivative goes to
zero as 1/ logm.

6 F-controlling when the promotion
amount decreases slowly

The preceding section has made two restrictive as-
sumptions. First, that there is a unique F con-
straint. Second, that the M constraints are never
winner-preferring. Under these assumptions, it
has shown that the F-control condition (3) holds
when the calibration constant grows only very
slowly with m, namely as in (16). Does this favor-
able result also hold when we relax the two restric-
tive assumptions? This section shows how to relax
one of the two assumptions, namely the assump-
tion that the M constraints cannot be winner-
preferring. At this stage, I do know how to relax
the other assumption that there is a unique F con-
straint. Again, the reasoning here is only sketched;
see Magri (2014a) for details.

16



To illustrate the core idea, suppose that the
EDRA model is trained on the input ERC matrix
(17a) and walks through the run (18a). Here, I
am assuming that the promotion amount p has the
shape in (8), with the calibration constant σ = 0
set equal to zero for concreteness.

(17)a. b.[ F M1 M2
ERC 1 W L e
ERC 2 W W L

] [ F M1 M2
ERC 1 W L e
ERC 2 W e L

]
(18) a.

F

M1

M2

 010
10

  19
10

  28
10

 2.58.5
9

 39
8

ERC 1 ERC 1 ERC 2 ERC 2
b.

F

M1

M2

 010
10

  19
10

  29
9

 39
8

ERC 1 ERC 2 ERC 2

Consider the diagonal ERC matrix (17b) corre-
sponding to m = 2 markedness constraints. The
original run (18a) on the original ERC matrix
(17a) can be simulated with the run (18b) on the
diagonal ERC matrix (17b) in such a way that all
constraints end up at the same high in the two runs.

This reasoning holds in complete generality. In-
deed, under the assumption that there is a unique
F constraint but no restrictions on the M con-
straints, the input ERC matrix looks like (19).

(19) 
F1 M1 ... Mm

| . . . . . .
W L, e,W

| . . . . . .


Any run of the EDRA model on this input ERC
matrix (19) can be mimicked by a corresponding
run on the diagonal ERC matrix (15). This reduc-
tion to the diagonal case holds provided the pro-
motion amount is calibrated, namely has the shape
in (8), no matter the choice of the calibration con-
stant σ ≥ 0. This reduction fails if the promotion
amount is not calibrated.

Another crucial condition needed for the re-
duction to the diagonal case is the following: in
the original run, the markedness constraints are
allowed to raise only slightly above their initial
ranking value θinit. Indeed, if a markedness con-
straint could raise arbitrarily high above its initial
ranking value in the original run, there would be
no way to mimic that increasing ranking dynam-
ics with a derived run on the diagonal ERC ma-
trix (15), as the latter only demotes but never pro-
motes the markedness constraints. The fact that

the markedness constraints can raise by a small
amount does not threaten the reduction to the diag-
onal case, because the markedness constraints can
be assigned a slightly larger initial ranking value
in the derived run on the diagonal ERC matrix.

Fortunately, the markedness constraints
M1, . . . ,Mm indeed can raise above their initial
ranking value θinit only by a small amount,
namely never by more than m, as stated in (20).

(20) θ1, . . . , θm ≤ θinit +m
Obviously, this bound (20) holds at the beginning
of the run. It is thus sufficient to prove that this
bound is an invariant of the algorithm: if it holds
of the current ranking values at some time t − 1,
then it also holds at the subsequent time t. The
challenge is that a winner-preferring markedness
constraint M1 sitting right at θinit + m at time
t − 1 could in principle be promoted above that
forbidden threshold, so that the bound (20) would
hold at time t − 1 but fail at time t. Yet, in order
for such an update to happen, there has got to ex-
ist another constraintM2 which is loser-preferring
and is ranked at time t − 1 at least as high as
the winer-preferring constraint M1. This means
in turn that the sum θt−11 + θ

t−1
2 of the two rank-

ing values of M1 and M2 at time t − 1 is at least
(θinit + m) + (θinit + m). This suggests to cope
with the difficulty just highlighted by strengthen-
ing the invariant. Not only a single ranking value
cannot get larger than θinit +m, but also the sum
of any two ranking values can never reach (θinit +
m)+(θinit+m). For instance, let’s say it can never
get larger than (θinit +m) + (θinit +m− 1). But
now again, in order to prove that the latter bound
on the sum of two ranking values holds at time t,
I need an assumption about the sum of three rank-
ing values at time t − 1. And so on. Indeed, the
sum θi1 + . . .+θik of the current ranking values of
any number k of different markedness constraints
Mi1 , . . . ,Mik can be bound as in (21). This bound
holds for any promotion amount with the shape (8)
corresponding to a calibration constant σ which is
not too small, namely σ ≥ 1.

(21)
k∑

h=1

θih ≤
k∑

h=1

(θinit +m− h+ 1)

For k = 1, (21) yields the desired bound (20).

Acknowledgments

This research was supported by a Marie Curie In-
tra European Fellowship within the 7th European

17



Community Framework Programme.

References
Paul Boersma. 1998. Functional Phonology. Ph.D.

thesis, University of Amsterdam, The Netherlands.
The Hague: Holland Academic Graphics.

Paul Boersma. 2009. Some correct error-driven ver-
sions of the constraint demotion algorithm. Linguis-
tic Inquiry, 40:667–686.

Kyle Gorman. 2013. Generative phonotactics. Ph.D.
thesis, University of Pennsylvania.

Giorgio Magri. 2012. Convergence of error-driven
ranking algorithms. Phonology, 29(2):213–269.

Giorgio Magri. 2013a. The complexity of learning in
OT and its implications for the acquisition of phono-
tactics. Linguistic Inquiry, 44.3:433468.

Giorgio Magri. 2013b. An initial result on the re-
strictiveness of the error-driven ranking model of the
early stage of the acquisition of phonotactics. In
Hsin-Lun Huang, Ethan Poole, and Amanda Rys-
ling, editors, Proceedings of NELS 43: the 43rd an-
nual meeting of the North East Linguistic Society.

Giorgio Magri. 2014a. The error-driven ranking model
of the acquisition of phonotactics: how to control the
height of the faithfulness constraints. CNRS, UiL-
OTS ms.

Giorgio Magri. 2014b. Error-driven versus batch mod-
els of the acquisition of phonotactics: David defeats
Goliath. In John Kingston, Claire Moore-Cantwell,
Joe Pater, and Robert Staubs, editors, Supplemental
Proceedings of Phonology 2013, Washington DC.
Linguistic Society of America.

Joe Pater. 2009. Weighted constraints in Generative
Linguistics. Cognitive Science, 33:999–1035.

Alan Prince and Paul Smolensky. 2004. Optimality
Theory: Constraint Interaction in generative gram-
mar. Blackwell, Oxford. As Technical Report CU-
CS-696-93, Department of Computer Science, Uni-
versity of Colorado at Boulder, and Technical Report
TR-2, Rutgers Center for Cognitive Science, Rut-
gers University, New Brunswick, NJ, April 1993.
Also available as ROA 537 version.

Alan Prince. 2002. Entailed ranking arguments.
Ms., Rutgers University, New Brunswick, NJ. Rut-
gers Optimality Archive, ROA 500. Available at
http://www.roa.rutgers.edu.

Bruce Tesar and Paul Smolensky. 1998. Learnability
in Optimality Theory. Linguistic Inquiry, 29:229–
268.

Bruce Tesar. 2013. Output-Driven Phonology: Theory
and Learning. Cambridge Studies in Linguistics.

Igor Yanovich. 2012. The logic of OT rankings. MIT
manuscript.

18


