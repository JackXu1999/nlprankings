



















































A State-transition Framework to Answer Complex Questions over Knowledge Base


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2098–2108
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

2098

A State-transition Framework to Answer Complex Questions over
Knowledge Base

Sen Hu1, Lei Zou1,2, Xinbo Zhang1
1Peking University, China; 2Beijing Institute of Big Data Research, China;

{husen,zoulei,zhangxinbo}@pku.edu.cn

Abstract

Although natural language question answer-
ing over knowledge graphs have been studied
in the literature, existing methods have some
limitations in answering complex questions.
To address that, in this paper, we propose a
State Transition-based approach to translate
a complex natural language question N to a
semantic query graph (SQG) QS , which is
used to match the underlying knowledge graph
to find the answers to question N . In or-
der to generate QS , we propose four prim-
itive operations (expand, fold, connect and
merge) and a learning-based state transition
approach. Extensive experiments on several
benchmarks (such as QALD, WebQuestions
and ComplexQuestions) with two knowledge
bases (DBpedia and Freebase) confirm the su-
periority of our approach compared with state-
of-the-arts.

1 Introduction

Complex Question Answering, in which the ques-
tion corresponds to multiple triples in knowledge
base, has attracted researchers’ attentions recently
(Bao et al., 2014; Xu et al., 2016; Berant and
Liang, 2014; Bast and Haussmann, 2015; Yih
et al., 2015). However, most of existing solu-
tions employ the predefined patterns or templates
in understanding complex questions. For exam-
ple, (Bao et al., 2014; Xu et al., 2016) use de-
pendency tree patterns to decompose a complex
question to several simple questions. (Berant and
Liang, 2014; Bast and Haussmann, 2015) rely on
templates to translate natural language sentences
to predefined logical forms. Obviously, it is im-
possible to cover all real-world cases using those
handcrafted templates. The recent work STAGG
(Yih et al., 2015) proposes a state transition strat-
egy to tackle complex questions. However, the
generated query graph’s structure in STAGG is

limited, as shown in Figure 1. Generally, the struc-
ture is formed by linking the topic entity e and the
answer node t through a core relation. It only al-
lows entity constraints (such as c) and one vari-
able (the answer node t), the expressivity is quite
limited. It cannot cover all types of complex ques-
tions such as the question in Figure 2, whose query
graph contains variable constraint (v4) and multi-
edges (between v1 and v4). Therefore, in this pa-
per, we propose a more flexible strategy to answer
complex questions without handcrafted templates
or restricting the query graph’s structure.

1.1 Properties and Challenges of Complex
Questions

We first analyze the inherent challenges for com-
plex question answering, which has not been well
studied in existing literatures.
Multi or Implicit relations. A complex ques-
tion has multiple relations and some relations may
be multi-hop or implicit. Consider the question
N1 = “Which Russian astronauts died in the
same place they were born in?”. Obviously there
are two explicit relations represented by the two
phrases “died in” and “born in”, but many tradi-
tional methods only extract a single relation be-
tween topic entity and the answer node. On the
other hand, there is an implicit relation between
“Russian” and “astronauts” (i.e., 〈Nationality〉),
which lacks a corresponding relation phrase.
Multi or No entities. Different from simple (one-
hop) questions that only contain a single topic en-
tity in the question sentenceN , complex questions
may have multiple entities. For example, “Which
Russian astronauts were died in Moscow and born
in Soviet Union”. There are three entities “Rus-
sian”, “Moscow” and “Soviet Union”. In some
cases, the question sentence does not include any
entity, such as “Who died in the same place they
were born in”. Therefore, existing methods that



2099

e t c 

Topic Entity Answer Constraint

Figure 1: The query graph structure of STAGG. Node e is
the “topic entity”, path e-t is the “core inferential chain” and
c represents “augmenting constraints”. Notice we omit the
CVT node as it is a virtual node only existed in Freebase.

Russia 
?cosmonauts

v1

v2v3

Which cosmonauts died in the same place they were born in?

<birthPlace>

?place

v4

Astronaut 
<nationality> <type>

<deathPlace>
(died in) (born in)

Figure 2: The example of complex questions with semantic
query graph

only care about a single topic entity cannot work
for such types of complex questions.
Variables and Co-reference. Existing solutions
assume that there is a single variable in the ques-
tion N , i.e., the wh-word, which is focus of the
question (representing the answer node). How-
ever, complex questions may have other variables.
Consider the question in Figure 2, there are four
variables “Which”, “cosmonauts”, “place” and
“they” recognized. While the three in yellow grid
(“Which”, “cosmonaut”, “they”) refer to the same
thing, which is the co-reference phenomenon.
Composition. In the simple questions, the query
graph’s structure is determinate (one-hop edge).
For complex questions, even when all entities,
variables and relations are recognized, how to as-
semble them to logical forms or executable queries
(query graphs) is still a challenge. As mentioned
earlier, existing solutions that depend on prede-
fined templates (such as (Xu et al., 2016; Bast and
Haussmann, 2015)) or query graph structure pat-
terns (such as (Yih et al., 2015)) may not be suit-
able due to the diversity of complex questions.

1.2 Our Solution

Considering the above challenges, we propose
a state transition based framework to translate
users’ question N into a semantic query graph
(SQG) QS , further the answers of N can be found
by matching QS over the knowledge graph G.
Specifically, we first recognize nodes (such as en-
tities and variables) from N as the initial state
(Section 2.1). Then, to enable and speed up the

state transition process, we propose four primitive
operations with corresponding conditions (Section
2.2). The entity/relation candidates are extracted
using existing entity linking algorithms and our
proposed MCCNN model (Section 2.3). To guide
the state transition, we learn a reward function us-
ing SVM ranking algorithm (Section 3) to greedily
select a subsequent state.

Compared with existing solutions, our frame-
work has stronger expressivity to solve more types
of complex questions (considering the challenges
introduced in Section 1.1) and does not rely on
handcrafted templates.

2 Semantic Query Graph Generation

This section outlines the whole framework of our
system. Given a natural language question N , we
aim to translate N into a semantic query graph
(SQG) (see Definition 1).

Definition 1 (Semantic Query Graph). A seman-
tic query graph (denoted as QS) is a connected
graph, in which each vertex vi corresponds a
phrase in the question sentence N and associated
with a list of entity/type/literal candidates; and
each edge vivj is associated with a list of relation
candidates, 1 ≤ i, j ≤ |V (QS)|.

Given a question sentence N1 = “Which cos-
monauts died in the same place they were born
in?”, the corresponding semantic query graph QS1
is given in Figure 2. In QS1 , the subgraph {v1,
v2 and v3} corresponds “cosmonauts”, v4 corre-
sponds “place”. The relation phrases “born in” and
“died in” denote two relations between v1 and v4.
Each edge in QS has a list of relation(predicate)
candidates with confidence probabilities. In this
paper, we only draw one relation candidate of
each edge for simplicity. Notice that the edge
v1v3 and v1v2 have no relation phrases, however,
they still have corresponding relation candidates
〈nationality〉 and 〈type〉.

After obtaining semantic query graph QS , we
find matches of QS over the underlying knowl-
edge graph G to find answers to the original ques-
tion N . This stage is identical to gAnswer (Zou
et al., 2014) and NFF (Hu et al., 2018). Thus, in
this paper, we concentrate ourselves on discussing
how to generate semantic query graph QS , which
is different from NFF that relies on the paraphras-
ing dictionaries and heuristic rules to build QS .



2100

2.1 Node Recognition

The first step is to detect the “nodes” (for build-
ing SQG QS) in users’ question sentences. Ac-
cording to Definition 1, each node in the semantic
query graph is a phrase corresponding to a subject
or object in the SPARQL query. Generally, con-
stant nodes (e.g., Titanic) can be divided into three
categories according to their roles (entity, type, lit-
eral) in knowledge graph. Besides, a variable node
(e.g., ?place) has the potential to map any vertices
in knowledge graph.

Generally, the node recognition task is regarded
as a sequence labeling problem and we adopt a
BLSTM-CRF model (Huang et al., 2015) to re-
alize it. However, this model doesn’t work very
well on recognizing entity/type nodes as the entity
phrases may be too long or too complex. There-
fore, we first detect entity and type nodes by uti-
lize existing entity linking algorithms of specific
knowledge bases (see section 2.3), and then detect
variable/literal nodes by the BLSTM-CRF model.
A phrase wij , which contains the words from i to
j in the question, is recognized as entity/type node
if we can find entity/type mappings of wij through
the entity linking algorithm. We check all possible
phrases and select the longer one if two candidate
nodes share any words. The detected entity/type
nodes will be replaced by special tokens 〈E〉 or
〈T〉 before calling the BLSTM-CRF model.

In practice, we find that some nodes have hid-
den information, which can be expanded to a
subgraph (i.e, one or multiple triples). For ex-
ample, the hidden information of “?cosmonauts”
in question N1 in Figure 2 can extends two
triples: 〈?cosmonauts rdf:type dbo:Astronaut〉 and
〈?cosmonauts dbo:nationality dbr:Russia〉. Such
information can help us to reduce the matching
space and make the answers more accurate.

We mine those nodes with hidden information
from several QA benchmarks1 and build a transla-
tion dictionary DT in offline. Given a question N
with the gold query graphQG, we try to matchQG

to the SQG QS generated by our approach. Each
unmatched connected subgraph in QG is regarded
as hidden information of the nearest matched node
v. For the benchmarks only have gold question-
answers pairs, we utilize the method in (Zhang and
Zou, 2017) to mine the gold query graph first.

1QALD, WebQuestions and ComplexQuestions, only the
training sets are used to build the translation dictionary DT .

type country

?cosmonautsAstronaut 

v'2 v'1 v'3

(c) Expand 

?cosmonauts

v1

Russia 

compose

?soundtrack?Who

v2 v1 v3

(d) Fold 

Titanic

for

musicComposer

Titanic?Who

v'2 v'3

Give me all cosmonauts. Who composed the soundtrack for 
Cameron's Titanic?

?they?place

v3 v4
?presidents?Which

v2 v1

Which presidents died in the same place 
they were born in?

deathPlace birthPlace

?place
v'3

?presidents
v'1 deathPlace

birthPlace
type

v'1 v'2

(a) Connect 

Trump

v1

Is Trump a president?

president

v2

Trump president

(b) Merge 

Figure 3: Samples of Operations

2.2 SQG’s Structure Construction

As mentioned earlier, our SQG’s structure is gen-
erated in a state-transition paradigm. Initially, the
state only contains several isolated nodes that are
recognized in the first step. To enable the state
transition, we propose four primitive operations as
follows.

1. Connect. Given two operate nodes u1 and
u2, we introduce an edge u1u2 between them
by the connect operation. Note that the rela-
tion mention and candidate relations (predi-
cates) of the edge u1u2 can be found through
relation extraction model (see section 2.3).

2. Merge. Given a SQG QS = {V,E} and two
operate nodes u,v, this operation is to merge
the node u into v. The new SQG Q

′S = {V \
{u}, (E \ Ed) ∪ Ea}, Ed = {uw ∈ E} and
Ea = {vw|uw ∈ E ∧ w 6= v}. Notice that
v also inherits the properties of u. The merge
operation is designed to support co-reference
resolution. The Figure 3(b) gives an example.
For the question “Which presidents died in
the same place they were born in?”, the node
v2(“which”) is redundant and can be merged
into v1(“presidents”). The node v4(“they”)
and v1(“presidents”) are co-reference, we can
merge v4 to v1. The edge v3v4(birthPlace) is
replaced by edge v′3v

′
1(birthPlace).

3. Expand. Given a SQG QS = {V,E} and
the operate node u ∈ V , this operation is
to expand u to a subgraph QSu = {Vu, Eu}.
The new SQG Q

′S = {V ∪ Vu, E ∪ Eu}.
The expand operation is designed to those



2101

Russia 

?cosmonauts v'2v'4

?place

Astronaut 

died in born in?cosmonauts

?they?place
v3

v4

?cosmonauts ?Which

v2
?cosmonauts

v'1

?place

v'3

died in born in

v1

Expand(v1)

Merge(v1,v2)
Merge(v1,v4)

born in

died in

?Which

?place

?they

Connect(v1,v3)

v1

v2

v3

v4

Connect(v3,v4)

Connect(v1,v2)
v'1

v'3

Figure 4: Semantic Query Graph Generation

nodes that have hidden information. Let us
see Figure 3(c). The word “cosmonauts”(v1)
means “Russia astronauts”, but it cannot be
mapped to a certain entity or type in the
knowledge base directly. To match “cosmo-
nauts”, we can expand it to an equivalent sub-
graph {v′1,v

′
2,v

′
3} (according to the transition

dictionaryDT ) that can match the underlying
knowledge base.

4. Fold. Given a SQG QS = {V,E} and an op-
erate node u ∈ V , this operation is to delete
u and merge the associated relations. For-
mally, the new SQG Q

′S = {V \ {u}, (E \
Ed) ∪ Ea}, in which Ed = {uvi ∈ E}
and Ea = {vivj |uvi ∈ E ∧ uvj ∈ E ∧
i 6= j}. The fold operation is designed
to eliminate those nodes which are useless
or mis-recognized. Figure 3(d) gives an ex-
ample. For the question “Who composed
the soundtrack for Cameron’s Titanic?”, SQG
{v1, v2, v3} cannot find matches in knowl-
edge base because there are no correct rela-
tions of v1v2 and v1v3. By applying a fold
operation, these two edges with the redundant
node v1 are removed, while the new edge
v′1v′2 can be matched to the correct relation
〈musicComposer〉 (according to the relation
extraction model).

We focus on generating the correct SQG with
the above operations. Obviously, it is impossible
to enumerate all possible transitions in each step
due to exponential search space. Therefore, we
propose a greedy search algorithm2 inspired by
(Yih et al., 2015). The intention is that an better
intermediate state should have more opportunities
to be visited and produce subsequent states. As
each state can be seen as a partial SQG, we pro-
pose a reward function using a log-linear model
(see Section 3) to estimate the likelihood of each
state. A priority queue is utilized to maintain the

2The formal algorithm can be found in Appendix A.

order of unvisited states. Each turn we choose the
state with maximum correctness likelihood and try
all possible transitions by each operation. Specif-
ically, the connect and merge operations try every
pair of nodes in current state as the two operate
nodes, while the expand and fold operations try
each node as the operate node. The newly gener-
ated states are estimated by the learning model and
pushed into the priority queue. Note that redupli-
cated states will not be added. The algorithm ends
when a complete SQG QS is found and QS can
not generates any better subsequent states.

To reduce the search space of the state-
transition process, we also propose several con-
straints for each operation. Only when the condi-
tions are satisfied, the corresponding operation can
be executed. Experiments show that those condi-
tions not only speeds up the QA process but also
improves the precision (see Section 4.4). The con-
ditions are listed as follows.

Condition 1 (Condition for Connect) Two nodes
v1 and v2 can be connected if and only if there
exists no other node v∗ that occurs in the shortest
path between v1 and v2 of the dependency parse
tree3 of question sentence N .

Condition 2 (Condition for Merge) For the
merge operation, there should be at least one vari-
able v in the two operate nodes. And v should be
a wh-word or pronoun, which may co-reference3

with an other node.

Condition 3 (Condition for Expand) For the ex-
pand operation, the operate node u should be a
variable, and we need to be able to find u and
its hidden information in the transition dictionary
DT .

Condition 4 (Condition for Fold) For the fold
operation, we require at least one of the connected
edges with the operate node have no relation can-
didates or the confidence probabilities of the cor-
responding relations are less than a threshold τ .

3We utilize the Stanford CoreNLP dependency parser and
coreference annotator (Manning et al., 2014)



2102

The following example illustrates the process of
SQG generation. Note that it can have other tran-
sition sequences to get the final state.

Example 1 Consider Figure 4. Given a question
N1 = “Which cosmonauts died in the same place
they were born in?”, we first extract four nodes v1,
v2, v3, v4, which are all variable nodes. Accord-
ing the condition 1, we connect (v1,v2), (v1,v3),
and (v3,v4). The simple path between v1 and v3 in
the dependency parse tree can be regarded as the
relation phrase of v1v3, which is “died in”. Simi-
larly, the relation phrase of v3v4 is “born in”. As
v1 and v2 are neighbors in the dependency parse
tree, the corresponding relation phrase is empty.
Then we merge (v1,v2) as they are recognized co-
reference. Similarly v1 and v4 are merged. Notice
that the associated edge v3v4 have been reserved,
then there are two edges between v′1 and v

′
3 in the

new SQG. In the end, we expand v′1 to the sub-
graph {v′1, v′4, v′2} and get the final SQG.

2.3 Finding entity/relation candidates and
matches of SQG

During SQG construction, there are two sub-
tasks: entity extraction and relation extraction. We
briefly discuss the two steps as follows.

Given a node phrase in QS , we find the candi-
date entities/types with confidence probabilities by
using existing entity linking approaches. Specifi-
cally, we use S-MART (Yang and Chang, 2015)
for Freebase dataset and DBpeida Lookup4 for
DBpedia dataset.

Given two connected nodes vi and vj in QS ,
we need to find the relation candidates between
them. Inspired by the recent success of neural rela-
tion extraction models in KBQA (Yih et al., 2015;
Dong et al., 2015; Xu et al., 2016), we propose
a Multi-Channel Convolutional Neural Network
(MCCNN) to extract all relations in the question
N exploiting both syntactic and sentential infor-
mation. We use the simple path between vi and vj
in the dependency tree Y (N) as input to the first
channel. The input of second channel is the whole
question sentence excluded all nodes, which rep-
resents the context of the relation we want to pre-
dict. If vi and vj are neighbors in Y (N), we need
to extract an implicit relation. To tackle this task,
we use vi, vj and their types as input to the third
channel. For vi = “Chinese” and vj = “actor”, the
input is “Chinese-Country-actor-Actor”.

4http://wiki.dbpedia.org/projects/dbpedia-lookup

Different from (Xu et al., 2016) which can only
extract one explicit relation, our model can not
only extract multiple relations from N by taking
different node pairs, but also extract the implicit
relation considering the embedded information in
the two nodes vi and vj .

After generating the SQG QS , we employ the
subgraph matching algorithm (Hu et al., 2018)
to find matches of QS over the underlying RDF
knowledge graph G. Once the matches are found,
it is straightforward to derive the answers.

3 Learning the reward function

Given an intermediate state s during SQG gener-
ation process, we may transit to multiple subse-
quent states s′ by applying different operations.
Thus, we greedily select a subsequent state s′ with
the maximum γ(s′), where γ() is a reward func-
tion taking the features and outputting the reward
of corresponding state. In this work, the reward
function γ() is a linear model trained with the
SVM ranking loss. Below we describe the fea-
tures, learning process and how to generate the
training data.

3.1 Features

Note that for any intermediate state s, we also re-
gard it as a SQG QS(s) even though it is a dis-
connected graph. For a given SQG QS , we extract
the following features which are passed as an input
vector to the SVM ranker.

Nodes. For each entity/type node v, we utilize
entity linking systems to find the candidate entities
or types and corresponding confidence probabili-
ties. We use the largest confidence probability as
the score of v, and use the average score of all en-
tity nodes as the feature. In addition, the number
of constant nodes and variable nodes in currentQS

are considered as features.
Edges. For each edge vivj , the relation extrac-

tion model returns a list of candidate predicates
with the corresponding confidence probabilities.
We use the largest confidence probability as the
score of vivj , and use the average score of all
edges as the feature. We also consider the to-
tal number of edges in QS , the number of edges
which have relation phrase and which have no re-
lation phrase.

Matches. We also consider the matching be-
tween QS and the knowledge graph G. A new
state s would get a low score if its corresponding



2103

SQG QS could not find any matches in G. We
do not discard s directly from the search queue as
we allow the fold operation of it. To speed up the
matching process, we build the index of all entities
and relations in offline. For a given SQG QS , we
use the offline index to check whether it is valid.

Given a valid state s, if all of its possible subse-
quent states s′ have smaller reward function value
than that of s, we will terminate the state transi-
tion process and return s as the final state. The
corresponding SQG QS(s) is used to match the
knowledge base to retrieve answers.

3.2 Learning

The task of reward function can be viewed as a
ranking problem, in which the appropriate SQGs
should be ranked in the front. In this work, we
rank them using a SVM rank classifier (Joachims,
2006).

Given a SQG QS = {V,E}, the ranking score
that we use to train our reward function γ() is cal-
culated by the following function.

R(QS) =
|P (V )|
|V ′|

+
|P (E)|
|E′|

+max(F (Ai, A
′))

(1)
V ′ andE′ are the gold node set and relation set ex-
tracted from the gold SPARQL query. P (V ) con-
tains the nodes existing in both V and V ′. P (E)
contains the relations existing in both E and E′.
F (Ai, A

′) is the F-1 measure where A′ is the gold
answer set and Ai is one of the subgraph matches
between QS and knowledge graph G obtained by
the executing method in (Hu et al., 2018). No-
tice that for the benchmark without gold SPARQL
queries, R(QS) = max(F (Ai, A′)).

3.3 Generating Training Data

To generate the training data of the reward func-
tion, given a question N = {w1, w2, ..., wn}, we
first check all possible phrases wij by existing en-
tity linking system and get the candidate entity list
Cvi = {e1, e2, ..., em} for each entity node vi. If
two entity node vi and vj have conflict, i.e, vi and
vj share one or more words, we reserve the longer
one. We replace each entity node vi with a spe-
cial token to denote the type of entity ej , where
ej ∈ Cvi and has the largest confidence possi-
bility. Then the whole node set V can be pre-
dicted by the well-trained node recognition model.
QS = {V, φ} is the initial state. Further we do
state transition by applying the predefined opera-

tions using a BFS algorithm. Meanwhile, only the
states satisfied with corresponding conditions are
considered if enabling the condition mechanism.
Once we get a new QS we call the well-trained
relation extraction model to get the relation can-
didate list of each new edge vivj ∈ E, while the
features are collected according Section 3.1 and
the ranking score is calculated using Equation 1.

4 Evaluation

We evaluate our system on several benchmarks
with two knowledge base DBpedia and Freebase.
For DBpedia, we use the QALD-6 benchmark. We
compare our method STF (State Transition Frame-
work) with all systems in QALD-6 competition
as well as Aqqu (Bast and Haussmann, 2015),
gAnswer (Zou et al., 2014) and NFF (Hu et al.,
2018). For Freebase, we use WebQuestions (Be-
rant et al., 2013) and ComplexQuestions (Abuja-
bal et al., 2017) as benchmarks and compare our
method with corresponding state-of-art systems.

4.1 Setup
4.1.1 Knowledge Bases
DBpedia RDF repository is a knowledge base
derived from Wikipedia (Bizer et al., 2009). We
use the version of DBpedia 2014 and the statistics
are given in Table 1.
Freebase is a collaboratively edited knowledge
base. We use the version of Freebase 2013, which
is same with (Berant and Liang, 2014). The statis-
tics are given in Table 1.

4.1.2 Benchmarks
QALD is a series of open-domain question an-
swering campaigns, which have several different
tasks. We only consider the task of English ques-
tion answering over DBpedia in QALD-6 (Unger
et al., 2016), which has 350 training questions and
100 test questions, with gold SPARQL queries and
answer sets.
WebQuestions (Berant et al., 2013) consists of
3778 training questions and 2032 test questions,
which are collected using the Google Suggest API
and crowdsourcing. It only provides the pairs of
question and answer set.
ComplexQuestions (Abujabal et al., 2017) is
composed of 150 test questions that exhibit
compositionality through multiple clauses, which
is constructed using the crawl of WikiAnswers
(http://wiki.answers.com) and human annotator. It
has no training questions and SPARQL queries.



2104

Table 1: Statistics of Knowledge Bases
DBpedia Freebase

Number of Entities 5.4 million 41 million
Number of Triples 110 million 596 million

Number of Predicates 9708 19456
Size of KBs (in GB) 8.7 56.9

Table 2: The average F1 score of WebQuestions and
ComplexQuestions benchmark

WQ CQ
STF (Our approach) 53.6% 54.3%

STAGG (Yih et al., 2015) 52.5% -
QUINT (Abujabal et al., 2017) 51.0% 49.2%

NFF (Hu et al., 2018) 49.6% -
Aqqu (Bast and Haussmann, 2015) 49.4% 27.8%

Aqqu++ (Bast and Haussmann, 2015) 49.4% 46.7%

4.2 Results on WebQuestions and
ComplexQuestions

Table 2 shows the results on the test set of
WebQuestions(WQ) and ComplexQuestions(CQ),
which are based on the Freebase. We com-
pare our system with STAGG (Yih et al., 2015),
QUINT (Abujabal et al., 2017), NFF (Hu et al.,
2018) and Aqqu(Bast and Haussmann, 2015).
QUINT is the model used in the original Com-
plexQuestions paper and Aqqu is the best publicly
available QA system on WebQuestions. The aver-
age F1 of our system (53.6% for WQ and 54.3%
for CQ) are better than the other systems.

Aqqu defines three query templates for We-
bQuestions and try to match test questions to pre-
defined templates. It has a poor performance
(27.8%) on ComplexQuestions when answering
the test questions directly. Aqqu++ shows the
result (46.7%) by taking manually decomposed
subquestions as input and getting the intersection
of subquestions’ answer sets. QUINT is a sys-
tem that automatically learns utterance-query tem-
plates from the pairs of question and answer set.
NFF builds a relation paraphrase dictionary and
leverages it to extract relations from the questions.
STAGG proposes a state-transition based query
graph generation method. However, its query
graph structure is limited as in the Figure 1, which
is very similar with the templates of Aqqu.

To generate the training data of relation extrac-
tion model on the WebQuestions, for each two
nodes u and v in the training question, we explore
all simple relations between u and v in Freebase.
Note that if u or v is the answer node, we will
anchor the answer entity first and select the rela-
tion which can handle most of answer entities as
the gold relation. Each entity will be replaced in
text by a special token representing its type before

Table 3: Evaluating QALD-6 Testing Questions
Processed Right Recall Precision F-1

Our approach 100 70 0.72 0.89 0.80
CANaLI 100 83 0.89 0.89 0.89
UTQA 100 63 0.69 0.82 0.75

KWGAnswer 100 52 0.59 0.85 0.70
SemGraphQA 100 20 0.25 0.70 0.37

UIQA1 44 21 0.63 0.54 0.25
UIQA2 36 14 0.53 0.43 0.17

NFF 100 68 0.70 0.89 0.78
gAnswer 100 40 0.43 0.77 0.55

Aqqu 100 36 0.37 0.39 0.38

training. As there are no training data in Com-
plexQuestions, we use the models trained on We-
bQuestions directly.

Generally, WebQuestions benchmark has low
diversity and contains some incorrect or incom-
plete labeled answers. Most of questions in We-
bQuestions are simple questions, which can be
translated to a “one-triple” query. The results
on WebQuestions benchmark prove our approach
is able to obtain good performance on simple
questions. ComplexQuestions benchmark is more
complex than WebQuestions. The experiments re-
sults show that the two template based methods
QUINT and Aqqu can solve a part of the complex
questions. However, it still lack of the capacity to
tackle all questions due to the error of dependency
parse or the mismatch of the templates. On the
other hand, our system outperforms them because
we do not rely on templates and have better gener-
alization ability to solve complex questions.

4.3 Results on QALD

Table 3 shows the evaluation results on the test
set of QALD-6. We compare our system with all
participants of QALD-6 as well as gAnswer (Zou
et al., 2014), NFF (Hu et al., 2018) and Aqqu (Bast
and Haussmann, 2015). To enable the comparison,
we show the experiment results in the QALD com-
petition report format. “Processed” denotes the
number of questions can be processed and “Right”
refers to the number of questions that were an-
swered correctly. Our approach can answer 70
questions correctly, and get the 0.80 F-1 score.

The experiment results show that we can beat
all systems in QALD-6 campaign in F-1 ex-
cept for CANaLI (Mazzeo and Zaniolo). Note
that CANaLI is not an automatic QA system,
as it needs users to specify the precise entities
and predicates in the question sentences. gAn-
swer (Zou et al., 2014) proposes a relation first
framework to generate the semantic query graph
while it can not detect implicit relations. NFF
(Hu et al., 2018) proposes a node first framework
which performs well on answering complex ques-



2105

Table 4: The performance comparison of state transi-
tion with conditions or without conditions in QALD-6
test set

Right Recall Precision F-1
with condition 70 0.72 0.89 0.80
no condition 62 0.64 0.84 0.72

10

100

1000

10000

Q3 Q14 Q25 Q30 Q42 Q57 Q66 Q71 Q81 Q91 AVG

use condition w/o condition

SQ
G

 C
o

n
st

ru
ct

io
n

 T
im

e 
(i

n
 m

s)

Figure 5: The elapsed time comparison of state transi-
tion with conditions or without conditions in QALD-6
test set

tions (0.78 F1 score on QALD-6). However it re-
lies on the predefined paraphrasing dictionaries to
extract relations and could not tackle with redun-
dant nodes.

As QALD has the gold SPARQL queries, we
can obtain the training data of node recognition
and relation extraction model directly. Different
from the WebQuestions and ComplexQuestions
which have only special questions, the QALD
benchmark also has general questions and imper-
ative sentence. Besides, there are some questions
that have no entities or multiple entities.

Generally, QALD benchmark has both high di-
versity and quality. The performance of Aqqu
in QALD is worse than its performance in We-
bQuestions and ComplexQuestions as it has only
designed the templates to WebQuestions. In con-
trast, our approach do not rely specific benchmark
or knowledge base. The evaluation results show
that our approach performs well on various types
of complex questions.

4.4 Comparison experiments of the four
conditions

We run experiments on QALD-6 test set to verify
the effectiveness of the four conditions proposed
in Section 2.2. We first compare the elapsed time
of state transition with conditions or without con-
ditions. Figure 5 shows the results of 10 ques-

Table 5: Failure Analysis on QALD-6
Reason # (Ratio) Sample question
Structure
Failure

6 (20%) Q55: In which countries do
people speak Japanese?

Entity
Mapping

5 (17%) Q88: What color expresses
loyalty?

Relation
Mapping

10 (33%) Q44: What is the full name
of Prince Charles?

Complex
Aggregation

9 (30%) Q80: How short is the
shortest active NBA player?

tions selected randomly and AVG means the av-
erage time consumption among all 100 test ques-
tions. The average elapsed time with conditions
is 700 ms while it rises to 1200 ms without the
conditions. The results declare that using the con-
ditions in the search process can avoid unneces-
sary searches and save SQG construction time. In
some questions (such as Q66 and Q81), the gap
of elapsed time are very little because the number
of recognized nodes are less than three. In other
words, those simple questions do not need the con-
ditions to speed up the SQG construction process.

On the other hand, using conditions can im-
prove the evaluation performance (see Table 4).
That because the conditions help system to re-
duce search space and avoid local optimum, es-
pecially the condition of connect operation. When
the question has multiple nodes, if we do not use
the condition of connect operation, it tries to con-
nect each two nodes and enrolls too many mis-
taken states.

4.5 Error Analysis
We provide the error analysis of our approach on
QALD-6 test set (100 questions), which contains
gold SPARQL queries and more diversified ques-
tions. The ratio of each error type and the corre-
sponding examples are given in Table 5.

There are mainly four reasons for the failure of
some questions in our approach. The first reason
is the structure failure, which means we generate a
wrong SQG. That usually occurred when the cor-
rect SQG needs a fold operation. For example, the
correct SQG of question “In which countries do
people speak Japanese” has only two nodes “coun-
tries”, “Japanese”. However, we recognized the
phrase “people” as a variable node by mistake and
connected it to both “countries” and “Japanese” to
generate a wrong SQG QS

′
. It should be elimi-

nated by the fold operation to get the correct SQG
QS . However, the score of QS

′
that we get from

the reward function is higher than the score ofQS .
The inherent reason is the lack of training data.



2106

The second reason is the failure of entity link-
ing, for example, we could not find the correct en-
tity of the node “loyalty” in the question “What
color expresses loyalty?”. It has smallest propor-
tion as the existing entity linking algorithms al-
ready have high accuracy on QA task.

The third reason is the failure of relation ex-
traction. For example, the correct relation be-
tween “What” and “Prince Charles” in the ques-
tion ‘What is the full name of Prince Charles?” is
〈alias〉. However, we got a wrong relation 〈name〉.
The relation extraction problem is very important
to QA task, which can be improved by designing
more elegant models and combining more useful
information.

The last one is that our method cannot answer
some complex aggregation questions, which need
to detect the aggregation constraint first and then
infer the related node and corresponding relation.
Using predefined templates or utilizing textual ev-
idence maybe works but not good enough.

5 Related Work

Generally, the solutions of KBQA can be di-
vided into information retrieval-based and seman-
tic parsing-based methods. The general process
of information retrieval-based methods is to se-
lect candidate answers first and then rank them
by various methods(Veyseh, 2016; Dong et al.,
2015; Xu et al., 2016; Bordes et al., 2014; Yao
and Durme, 2014). The main difference among in-
formation retrieval-based methods is how to rank
the candidate answers. (Bordes et al., 2014) uti-
lizes subgraph embedding to predict the confi-
dence of candidate answers. (Dong et al., 2015)
maximize the similarity between the distributed
representation of a question and its answer can-
didates using Multi-Column Convolutional Neural
Networks (MCCNN), while (Xu et al., 2016) aims
to predicate the correct relations between topic en-
tity and answer candidates with text evidence.

The semantic parsing-based methods try to
translate the natural language question into seman-
tic equivalent logical forms, such as simple λ-
DCS (Berant et al., 2013; Berant and Liang, 2014),
query graph (Yih et al., 2015; Zou et al., 2014),
or executable queries such as SPARQL (Bast and
Haussmann, 2015; Unger et al., 2012; Yahya et al.,
2012). Then the logical forms are executed by
corresponding technique and get the answers from
knowledge base. For answering complex ques-

tions, the key is how to construct the logical form.
Different from existing systems relying on the pre-
defined templates, we build the query graph by
state transition, which has stronger representation
power and more robustness.

(Dong and Lapata, 2016) trains a sequence to
tree model to translate natural language to logical
forms directly, which needs a lot of training data.
(Yu et al., 2017) uses deep bidirectional LSTMs
to learn both relation-level and word-level ques-
tion/relation representations and match the ques-
tion to candidate relations. The pipeline of (Yu
et al., 2017) is similar with (Yih et al., 2015),
which detect topic entity and relation first and
then recognize constraints by enumerating all con-
nected entities of answer node or CVT node in
knowledge base. The issue is they assume the
question N only have one relation, which is from
the topic entity to the answer. Different from (Yu
et al., 2017), we consider the question can have
many nodes (including entities and variables) and
recognize them by a BLSTM model. We also al-
low multi relations between these nodes and ex-
tract them by the MCCNN model.

6 Conclusions

In this paper, we propose a state transition frame-
work to utilize neural networks to answer complex
questions, which generates semantic query graph
based on four primitive operations. We train a
BLSTM-CRF model to recognize nodes including
entities and variables from the question sentence.
To extract the relations between those nodes, we
propose a MCCNN model which can tackle both
explicit and implicit relations. Comparing with
existing solutions, our framework do not rely on
handcrafted templates and has the potential to gen-
erate all kinds of SQGs given enough training
data. Extensive experiments on multi benchmark
datasets confirm that our approach have the state-
of-art performance.

Acknowledgments

This work was supported by The National Key Re-
search and Development Program of China under
grant 2016YFB1000603 and NSFC under grant
61622201 and 61532010. Lei Zou is the corre-
sponding author of this work. This work is also
supported by Key Laboratory of Science, Technol-
ogy and Standard in Press Industry (Key Labora-
tory of Intelligent Press Media Technology).



2107

References
Abdalghani Abujabal, Mohamed Yahya, Mirek Riede-

wald, and Gerhard Weikum. 2017. Automated tem-
plate generation for question answering over knowl-
edge graphs. In Proceedings of the 26th Interna-
tional Conference on World Wide Web, WWW 2017,
Perth, Australia, April 3-7, 2017, pages 1191–1200.

Jun-Wei Bao, Nan Duan, Ming Zhou, and Tiejun Zhao.
2014. Knowledge-based question answering as ma-
chine translation. In Proceedings of the 52nd An-
nual Meeting of the Association for Computational
Linguistics, ACL 2014, June 22-27, 2014, Baltimore,
MD, USA, Volume 1: Long Papers, pages 967–976.

Hannah Bast and Elmar Haussmann. 2015. More accu-
rate question answering on freebase. In Proceedings
of the 24th ACM International Conference on Infor-
mation and Knowledge Management, CIKM 2015,
Melbourne, VIC, Australia, October 19 - 23, 2015,
pages 1431–1440.

Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic parsing on freebase from
question-answer pairs. In EMNLP, pages 1533–
1544.

Jonathan Berant and Percy Liang. 2014. Semantic
parsing via paraphrasing. In Proceedings of the
52nd Annual Meeting of the Association for Com-
putational Linguistics, ACL 2014, June 22-27, 2014,
Baltimore, MD, USA, Volume 1: Long Papers, pages
1415–1425.

Christian Bizer, Jens Lehmann, Georgi Kobilarov,
Sören Auer, Christian Becker, Richard Cyganiak,
and Sebastian Hellmann. 2009. Dbpedia - a crys-
tallization point for the web of data. J. Web Sem.,
7(3):154–165.

Antoine Bordes, Sumit Chopra, and Jason Weston.
2014. Question answering with subgraph embed-
dings. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP 2014, October 25-29, 2014, Doha,
Qatar, A meeting of SIGDAT, a Special Interest
Group of the ACL, pages 615–620.

Li Dong and Mirella Lapata. 2016. Language to logi-
cal form with neural attention. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics, ACL 2016, August 7-12, 2016,
Berlin, Germany, Volume 1: Long Papers.

Li Dong, Furu Wei, Ming Zhou, and Ke Xu.
2015. Question answering over freebase with multi-
column convolutional neural networks. In Proceed-
ings of the 53rd Annual Meeting of the Association
for Computational Linguistics and the 7th Interna-
tional Joint Conference on Natural Language Pro-
cessing of the Asian Federation of Natural Language
Processing, ACL 2015, July 26-31, 2015, Beijing,
China, Volume 1: Long Papers, pages 260–269.

Sen Hu, Lei Zou, Jeffrey Xu Yu, Haixun Wang, and
Dongyan Zhao. 2018. Answering natural language
questions by subgraph matching over knowledge
graphs. IEEE Trans. Knowl. Data Eng., 30(5):824–
837.

Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidi-
rectional LSTM-CRF models for sequence tagging.
CoRR, abs/1508.01991.

Thorsten Joachims. 2006. Training linear svms in
linear time. In Proceedings of the Twelfth ACM
SIGKDD International Conference on Knowledge
Discovery and Data Mining, Philadelphia, PA, USA,
August 20-23, 2006, pages 217–226.

Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Rose Finkel, Steven Bethard, and David Mc-
Closky. 2014. The stanford corenlp natural language
processing toolkit. In Proceedings of the 52nd An-
nual Meeting of the Association for Computational
Linguistics, ACL 2014, June 22-27, 2014, Baltimore,
MD, USA, System Demonstrations.

Giuseppe M. Mazzeo and Carlo Zaniolo. Answer-
ing controlled natural language questions on RDF
knowledge bases. In Proceedings of the 19th Inter-
national Conference on Extending Database Tech-
nology, EDBT 2016, Bordeaux, France, March 15-
16, 2016, Bordeaux, France, March 15-16, 2016.,
pages 608–611.

Christina Unger, Lorenz Bühmann, Jens Lehmann,
Axel-Cyrille Ngonga Ngomo, Daniel Gerber, and
Philipp Cimiano. 2012. Template-based question
answering over rdf data. In WWW, pages 639–648.

Christina Unger, Axel-Cyrille Ngonga Ngomo, and
Elena Cabrio. 2016. 6th open challenge on question
answering over linked data (QALD-6). In Seman-
tic Web Challenges - Third SemWebEval Challenge
at ESWC 2016, Heraklion, Crete, Greece, May 29 -
June 2, 2016, Revised Selected Papers, pages 171–
177.

Amir Pouran Ben Veyseh. 2016. Cross-lingual ques-
tion answering using common semantic space. In
Proceedings of TextGraphs@NAACL-HLT 2016:
the 10th Workshop on Graph-based Methods for
Natural Language Processing, June 17, 2016, San
Diego, California, USA, pages 15–19.

Kun Xu, Siva Reddy, Yansong Feng, Songfang Huang,
and Dongyan Zhao. 2016. Question answering on
freebase via relation extraction and textual evidence.
In Proceedings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics, ACL 2016,
August 7-12, 2016, Berlin, Germany, Volume 1:
Long Papers.

Mohamed Yahya, Klaus Berberich, Shady Elbas-
suoni, Maya Ramanath, Volker Tresp, and Gerhard
Weikum. 2012. Natural language questions for the
web of data. In EMNLP-CoNLL, pages 379–390.



2108

Yi Yang and Ming-Wei Chang. 2015. S-MART: novel
tree-based structured learning algorithms applied to
tweet entity linking. In Proceedings of the 53rd An-
nual Meeting of the Association for Computational
Linguistics and the 7th International Joint Confer-
ence on Natural Language Processing of the Asian
Federation of Natural Language Processing, ACL
2015, July 26-31, 2015, Beijing, China, Volume 1:
Long Papers, pages 504–513.

Xuchen Yao and Benjamin Van Durme. 2014. Infor-
mation extraction over structured data: Question an-
swering with freebase. In Proceedings of the 52nd
Annual Meeting of the Association for Computa-
tional Linguistics, ACL 2014, June 22-27, 2014,
Baltimore, MD, USA, Volume 1: Long Papers, pages
956–966.

Wen-tau Yih, Ming-Wei Chang, Xiaodong He, and
Jianfeng Gao. 2015. Semantic parsing via staged
query graph generation: Question answering with
knowledge base. In Proceedings of the 53rd Annual
Meeting of the Association for Computational Lin-
guistics and the 7th International Joint Conference
on Natural Language Processing of the Asian Fed-
eration of Natural Language Processing, ACL 2015,
July 26-31, 2015, Beijing, China, Volume 1: Long
Papers, pages 1321–1331.

Mo Yu, Wenpeng Yin, Kazi Saidul Hasan,
Cı́cero Nogueira dos Santos, Bing Xiang, and
Bowen Zhou. 2017. Improved neural relation
detection for knowledge base question answering.
In Proceedings of the 55th Annual Meeting of the
Association for Computational Linguistics, ACL
2017, Vancouver, Canada, July 30 - August 4,
Volume 1: Long Papers, pages 571–581.

Xinbo Zhang and Lei Zou. 2017. Improving the pre-
cision of RDF question/answering systems: A why
not approach. In Proceedings of the 26th Interna-
tional Conference on World Wide Web Companion,
Perth, Australia, April 3-7, 2017, pages 877–878.

Lei Zou, Ruizhe Huang, Haixun Wang, Jeffrey Xu Yu,
Wenqiang He, and Dongyan Zhao. 2014. Natu-
ral language question answering over RDF: a graph
data driven approach. In International Conference
on Management of Data, SIGMOD 2014, Snowbird,
UT, USA, June 22-27, 2014, pages 313–324.


