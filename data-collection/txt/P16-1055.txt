



















































How Much is 131 Million Dollars? Putting Numbers in Perspective with Compositional Descriptions


Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 578–587,
Berlin, Germany, August 7-12, 2016. c©2016 Association for Computational Linguistics

How Much is 131 Million Dollars? Putting Numbers in Perspective with
Compositional Descriptions

Arun Tejasvi Chaganty
Computer Science Department

Stanford University
chaganty@cs.stanford.edu

Percy Liang
Computer Science Department

Stanford University
pliang@cs.stanford.edu

Abstract

How much is 131 million US dollars? To
help readers put such numbers in con-
text, we propose a new task of automati-
cally generating short descriptions known
as perspectives, e.g. “$131 million is about
the cost to employ everyone in Texas
over a lunch period”. First, we collect a
dataset of numeric mentions in news arti-
cles, where each mention is labeled with
a set of rated perspectives. We then pro-
pose a system to generate these descrip-
tions consisting of two steps: formula con-
struction and description generation. In
construction, we compose formulae from
numeric facts in a knowledge base and
rank the resulting formulas based on fa-
miliarity, numeric proximity and seman-
tic compatibility. In generation, we con-
vert a formula into natural language us-
ing a sequence-to-sequence recurrent neu-
ral network. Our system obtains a 15.2%
F1 improvement over a non-compositional
baseline at formula construction and a 12.5
BLEU point improvement over a baseline
description generation.

1 Introduction

When posed with a mention of a number, such
as “Cristiano Ronaldo, the player who Madrid ac-
quired for [. . . ] a $131 million” (Figure 1), it is
often difficult to comprehend the scale of large (or
small) absolute values like $131 million (Paulos,
1988; Seife, 2010). Studies have shown that pro-
viding relative comparisons, or perspectives, such
as “about the cost to employ everyone in Texas
over a lunch period” significantly improves com-
prehension when measured in terms of memory re-
tention or outlier detection (Barrio et al., 2016).

Cristiano Ronaldo, the player who Madrid
acquired for [. . . ] $131 million.

$1.3e8 ≈ 71e3 $/per/yr︸ ︷︷ ︸
1

× 27e6 per︸ ︷︷ ︸
2

× 30 min︸ ︷︷ ︸
3

1 = cost of an employee
2 = population of Texas
3 = time taken for lunch

about the cost to employ everyone in Texas
over a lunch period.

construction

generation

mention

formula

perspective

Figure 1: An overview of the perspective gen-
eration task: given a numeric mention, generate
a short description (a perspective) that allows the
reader to appreciate the scale of the mentioned
number. In our system, we first construct a for-
mula over facts in our knowledge base and then
generate a description of that formula.

Previous work in the HCI community has relied
on either manually generated perspectives (Barrio
et al., 2016) or present a fact as is from a knowl-
edge base (Chiacchieri, 2013). As a result, these
approaches are limited to contexts in which a rele-
vant perspective already exists.

In this paper, we generate perspectives by com-
posing facts from a knowledge base. For example,
we might describe $100,000 to be “about twice the
median income for a year”, and describe $5 mil-
lion to be the “about how much the average per-
son makes over their lifetime”. Leveraging com-
positionality allows us to achieve broad coverage
of numbers from a relatively small collection of
familiar facts, e.g. median income and a person’s

578



lifetime.
Using compositionality in perspectives is also

concordant with our understanding of how people
learn to appreciate scale. Jones and Taylor (2009)
find that students learning to appreciate scale do so
mainly by anchoring with familiar concepts, e.g.
$50,000 is slightly less than the median income
in the US, and by unitization, i.e. improvising a
system of units that is more relatable, e.g. using
the Earth as a measure of mass when describing
the mass of Jupiter to be that of 97 Earths. Here,
compositionality naturally unitizes the constituent
facts: in the examples above, money was unitized
in terms of median income, and time was unitized
in a person’s lifetime. Unitization and anchoring
have also been proposed by Chevalier et al. (2013)
as the basis of a design methodology for construct-
ing visual perspectives called concrete scales.

When generating compositional perspectives,
we must address two key challenges: construct-
ing familiar, relevant and meaningful formulas
and generating easy-to-understand descriptions or
perspectives. We tackle the first challenge us-
ing an overgenerate-and-rank paradigm, selecting
formulas using signals from familiarity, composi-
tionality, numeric proximity and semantic similar-
ity. We treat the second problem of generation
as a translation problem and use a sequence-to-
sequence recurrent neural network (RNN) to gen-
erate perspectives from a formula.

We evaluate individual components of our sys-
tem quantitatively on a dataset collected using
crowdsourcing. Our formula construction method
improves on F1 over a non-compositional baseline
by about 17.8%. Our generation method improves
over a simple baseline by 12.5 BLEU points.

2 Problem statement

The input to the perspective generation task is a
sentence s containing a numeric mention x: a span
of tokens within the sentence which describes a
quantity with value x.value and of unit x.unit. In
Figure 1, the numeric mention x is “$131 million”,
x.value = 1.31e8 and x.unit = $. The output is
a description y that puts x in perspective.

We have access to a knowledge baseK with nu-
meric tuples t = (t.value, t.unit, t.description).
Table 1 has a few examples of tuples in our knowl-
edge base. Units (e.g. $/per/yr) are fractions com-
posed either of fundamental units (length, area,
volume, mass, time) or of ordinal units (e.g. cars,

Description Value Unit

cost of an employee 71e3
$/year/person

population of Texas 27e3 person
number of employees at Google 57e3 person
average household size 2.54 person
time taken for a basketball game 60 minute
average lifetime for a person 79 year
a week 1 week
time taken for lunch 30 minute
cost of property in the Bay area 1e3 $/ft2

area of a city block 10e3 m2

Table 1: A subset of our knowledge base of nu-
meric tuples. Tuples with fractional units (e.g.
$/ft2) can be combined with other tuples to create
formulas.

people, etc.).
The first step of our task, described in Section 4,

is to construct a formula f over numeric tuples
in K that has the same value and unit as the nu-
meric mention x. A valid formula comprises of an
arbitrary multiplier f.m and a sequence of tuples
f.tuples. The value of a formula, f.value, is sim-
ply the product of the multiplier and the values of
the tuples, and the unit of the formula, f.unit, is
the product of the units of the tuples. In Figure 1,
the formula has a multiplier of 1 and is composed
of tuples 1 , 2 and 3 ; it has a value of 1.3e8
and a unit of $.

The second step of our task, described in Sec-
tion 5, is to generate a perspective y, a short noun
phrase that realizes f . Typically, the utterance will
be formed using variations of the descriptions of
the tuples in f.tuples.

3 Dataset construction

We break our data collection task into two steps,
mirroring formula selection and description gen-
eration: first, we collect descriptions of formulas
constructed exhaustively from our knowledge base
(for generation), and then we use these descrip-
tions to collect preferences for perspectives (for
construction).

Collecting the knowledge base. We manually
constructed a knowledge base with 142 tuples and
9 fundamental units1 from the United States Bu-

1Namely, length, area, volume, time, weight, money, peo-
ple, cars and guns. These units were chosen because they

579



reau of Statistics, the orders of magnitude topic on
Wikipedia and other Wikipedia pages. The facts
chosen are somewhat crude; for example, though
“the cost of an employee” is a very context depen-
dent quantity, we take its value to be the median
cost for an employer in the United States, $71,000.
Presenting facts at a coarse level of granularity
makes them more familiar to the general reader
while still being appropriate for perspective gen-
eration: the intention is to convey the right scale,
not necessarily the precise quantity.

Collecting numeric mentions. We collected
53,946 sentences containing numeric mentions
from the newswire section of LDC2011T07
using simple regular expression patterns like
$([0-9]+(,[0-9]+)*(.[0-9]+)?
((hundred)|(thousand)|(million)|
(billion)|(trillion))). The values
and units of the numeric mentions in each
sentence were normalized and converted to
fundamental units (e.g. from miles to length).
We then randomly selected up to 200 mentions
of each of the 9 types in bins with boundaries
10−3, 1, 103, 106, 109, 1012 leading to 4,931
mentions that are stratified by unit and magni-
tude.2 Finally, we chose mentions which could
be described by at least one numeric expression,
resulting in the 2,041 mentions that we use in
our experiments (Figure 2). We note that there
is a slight bias towards mentions of money and
people because these are more common in the
news corpus.

Generating formulas. Next, we exhaustively
generate valid formulas from our knowledge base.
We represent the knowledge base as a graph over
units with vertices and edges annotated with tu-
ples (Figure 3). Every vertex in this graph is la-
beled with a unit u and contains the set of tuples
with this unit: {t ∈ K : t.unit = u}. Addition-
ally, for every vertex in the graph with a unit of
the form u1/u2, where u2 has no denominator, we
add an edge from u1/u2 to u1, annotated with all
tuples of type u2: in Figure 3 we add an edge from
money/person to money annotated with the three
person tuples in Table 1. The set of formulas with
unit u is obtained by enumerating all paths in the
graph which terminate at the vertex u. The mul-
tiplier of the formula is set so that the value of

were well represented in the corpus.
2Some types had fewer than 200 mentions for some bins.

10−2 10−1 100 101 102 103 104 105 106 107 108 109 1010

Mention value
0

50

100

150

200

250

300

350

400

450

N
um

be
ro

fm
en

tio
ns

weight
area
car
time
gun
volume
person
length
money

Figure 2: A histogram of the absolute values of
numeric mentions by type. There are 100–300
mentions of each unit.

person
(3 tuples)

time
(4 tuples)

area
(1 tuple)

money
(0 tuples)

money/time/person
(1 tuples)

money/person
(0 tuples)

money/area
(1 tuples)

time (4 tuples)

person (3 tuples) area (1 tuples)

Figure 3: The graph over tuples generated from
the knowledge base subset in Table 1.

the formula matches the value of the mention. For
example, the formula in Figure 1 was constructed
by traversing the graph from money/time/person to
money: we start with a tuple in money/time/person
(cost of an employee) and then multiply by a tu-
ple with unit time (time for lunch) and then by unit
person (population of Texas), thus traversing two
edges to arrive at money.

Using the 142 tuples in our knowledge base, we
generate a total of 1,124 formulas sans multiplier.

Collecting descriptions of formulas. The main
goal of collecting descriptions of formulas is to
train a language generation system, though these
descriptions will also be useful while collecting
training data for formula selection. For every unit
in our knowledge base and every value in the set
{10−7, 10−6 . . . , 1010}, we generated all valid for-
mulas. We further restricted this set to formulas
with a multiplier between 1/100 and 100, based on
the rationale that human cognition of scale sharply
drops beyond an order of magnitude (Tretter et al.,

580



Figure 4: A screenshot of the crowdsourced task
to generate natural language descriptions, or per-
spectives, from formulas.

2006). In total, 5000 formulas were presented to
crowdworkers on Amazon Mechanical Turk, with
a prompt asking them to rephrase the formula as an
English expression (Figure 4).3 We obtained 5–7
descriptions of each formula, leading to a total of
31,244 unique descriptions.

Collecting data on formula preference. Fi-
nally, given a numeric mention, we ask crowd-
workers which perspectives from the description
dataset they prefer. Note that formulas gener-
ated for a particular mention may differ in mul-
tiplier with a formula in the description dataset.
We thus relax our constraints on factual accuracy
while collecting this formula preference dataset:
for each mention x, we choose a random perspec-
tive from the description dataset described above
corresponding to a formula whose value is within
a factor of 2 from the mention’s value, x.value. A
smaller factor led to too many mentions without a
valid comparison, while a larger one led to blatant
factual inaccuracies. The perspectives were par-
titioned into sets of four and displayed to crowd-
workers along with a “None of the above” option
with the following prompt: “We would like you to
pick up to two of these descriptions that are useful
in understanding the scale of the highlighted num-
ber” (Figure 5). A formula is rated to be useful by
simple majority.4

Figure 6 provides a summary of the dataset col-
lected, visualizing how many formulas are use-
ful, controlling for the size of the formula. The
exhaustive generation procedure produces a large
number of spurious formulas like “20× trash gen-
erated in the US× a minute× number of employ-
ees on Medicare”. Nonetheless, compositional

3Crowdworkers were paid $0.08 per description.
4Crowdworkers were paid $0.06 to vote on each set of

perspectives.

Figure 5: A screenshot of the crowdsourced task
to identify which formulas are useful to crowd-
workers in understanding the highlighted men-
tioned number.

formulas are quite useful in the appropriate con-
text; Table 2 presents some mentions with highly
rated perspectives and formulas.

4 Formula selection

We now turn to the first half of our task: given a
numeric mention x and a knowledge baseK, select
a formula f over K with the same value and unit
as the mention. It is easy to generate a very large
number of formulas for any mention. For the ex-
ample, “Cristiano Ronaldo, the player who Madrid
acquired for [. . . ] $131 million.”, the small knowl-
edge base in Table 1 can generate the 12 different
formulas,5 including the following:

1. 1 × the cost of an employee × the population of Texas
× the time taken for lunch.

2. 400 × the cost of an employee × average household
size × a week.

3. 1 × the cost of an employee × number of employees
at Google × a week.

4. 1 × cost of property in the Bay Area × area of a city
block.

Some of the formulas above are clearly worse
than others: the key challenge is picking a for-
mula that will lead to a meaningful and relevant
perspective.

Criteria for ranking formulas. We posit the
following principles to guide our choice in fea-
tures (Table 3).

5The full knowledge base described in Section 3 can gen-
erate 242 formulas with the unit money (sans multiplier).

581



Sentence That’s about . . . Formula

The Billings-based Stillwater Min-
ing produced 601,000 ounces of
platinum.

4 times the weight of an ele-
phant.

4 × weight of an elephant.

Authorities estimate there are about
60 million guns in Yemen.

twice the gun ownership of the
population of Texas

2 × gun ownership × popula-
tion of Texas

Water is flowing into Taihu lake at
a rate of 150 cubic meters per sec-
ond.

how much water would flow
from a tap left on for a week.

rate of flow of water from tap
× a week

The bank had held auctions, sell-
ing around US$1 billion worth of
three-month bills.

half the cost of employing the
population of Texas for a work
day.

1/2 × cost of an employee ×
time taken for a work day ×
population of Texas

The government[s] have promised
to rent about 1.2 million sq. feet.

the area of forest logged in a
single minute

90× area of forest logged× a
minute

Table 2: Examples of numeric mentions, perspectives and their corresponding formulas in the dataset.
All the examples except the last one are rated to be useful by crowdworkers.

0.00.10.20.30.40.50.60.70.80.91.0

Fraction of times formula is rated useful

0

2

4

6

8

10

12

14

16

N
um

be
ro

ff
or

m
ul

as

Formulas of length 1
Formulas of length 2
Formulas of length 3

Figure 6: A histogram comparing formula length
to ratings of usefulness (clipped for readability).
Non-compositional perspectives with a single tu-
ple are broadly useful. Useful compositional per-
spectives tend to be more context-specific than
non-compositional ones, and many of the formu-
las that can be generated from the knowledge base
are spurious.

Proximity: A numeric perspective should be
within an order of magnitude of the mentioned
value. Conception of scale quickly fails with
quantities that exceed “human scales” (Tretter
et al., 2006): numbers that are significantly away
from 1/10 and 10. We use this principle to
prune formulas with multipliers not in the range
[1/100, 100] (e.g. example 2 above) and introduce
features for numeric proximity.

Type Features #

Proximity sign(log(f.m)), | log(f.m)| 1
Familiarity I[t] 142
Compatibility I[t, t′] 20022
Similarity wvec(s)>

wvec(t.description)
1

Table 3: Feature templates used to score a formu-
las f and their counts (#), where f.m is the for-
mula’s multiplier and t, t′ ∈ f.tuples are tuples in
the formula.

Familiarity: A numeric perspective should be
composed of concepts familiar to the reader. The
most common technique cited by those who do
well at scale cognition tests is reasoning in terms
of familiar objects (Tretter et al., 2006; Jones and
Taylor, 2009; Chevalier et al., 2013). Intuitively,
the average American reader may not know ex-
actly how many people are in Texas, but is familiar
enough with the quantity to effectively reason us-
ing Texas’ population as a unit. On the other hand,
it is less likely that the same reader is familiar with
even the concept of Angola’s population.

Of course, because it is so personal, familiarity
is difficult to capture. With additional information
about the reader, e.g. their location, it is possible
to personalize the chosen tuples (Kim et al., 2016).
Without this information, we back off to a global
preference on tuples by using indicator features for
each tuple in the formula.

582



Formula Score

Studies estimate 36,000 people die on aver-
age each year from seasonal flu.

1/4 × global death rate × a day 0.67
5 × death rate in the US × a day 0.64
1/3 × number of employees at Mi-
crosoft

0.60

Gazprom’s exports to Europe [. . . ] will total
60 billion cubic meters . . .

oil produced by the US × average life-
time

0.78

average coffee consumption × popula-
tion of the world × average lifetime

0.78

2 × average coffee consumption × pop-
ulation of Asia × average lifetime

0.73

Table 4: The top three examples outputted by
the ranking system with the scores reported by the
system.

Compatibility: Similarly, some tuple combi-
nations are more natural (“median income × a
month”) while others are less so (“weight of a per-
son × population of Texas”). We model compati-
bility between tuples in a formula using an indica-
tor feature.

Similarity: A numeric perspective should be
relevant to the context. Apart from helping with
scale cognition, a perspective should also place the
mentioned quantity in appropriate context: for ex-
ample, NASA’s budget of $17 billion could be de-
scribed as 0.1% of the United States’ budget or the
amount of money it could cost to feed Los Angeles
for a year. While both perspectives are appropri-
ate, the former is more relevant than the latter.

We model context relevance using word vector
similarity between the tuples of the formula and
the sentence containing the mention as a proxy for
semantic similarity. Word vectors for a sentence or
tuple description are computed by taking the mean
of the word vectors for every non-stop-word token.
The word vectors at the token level are computed
using word2vec (Mikolov et al., 2013).

Evaluation. We train a logistic regression clas-
sifier using the features described in Table 3 using
the perspective ratings collected in Section 3. Re-
call that the formula for each perspective in the
dataset is assigned a positive (“useful”) label if

it was labeled to be useful to the majority of the
workers. Table 5a presents results on classifying
formulas as useful with a feature ablation.6

Familiarity and compatibility are the most use-
ful features when selecting formulas, each hav-
ing a significant increase in F1 over the proximity
baseline. There are minor gains from combining
these two features. On the other hand, semantic
similarity does not affect performance relative to
the baseline. We find that this is mainly due to
the disproportionate number of unfamiliar formu-
las present in the dataset that drown out any sig-
nal. Table 4 presents two examples of the system’s
ranking of formulas.

5 Perspective generation

Our next goal is to generate natural language de-
scriptions, also known as perspectives, given a
formula. Our approach models the task as a
sequence-to-sequence translation task from for-
mulas to natural language. We first describe a rule-
based baseline and then describe a recurrent neural
network (RNN) with an attention-based copying
mechanism (Jia and Liang, 2016).

Baseline. As a simple approach to generate per-
spectives, we just combine tuples in the formula
with the neutral prepositions of and for, e.g. “1/5th
of the cost of an employee for the population of
Texas for the time taken for lunch.”

Sequence-to-sequence RNN. We use formula-
perspective pairs from the dataset to create a
sequence-to-sequence task: the input is composed
using the formula’s multiplier and descriptions of
its tuples connected with the symbol ‘*’; the out-
put is the perspective (Figure 7).

Our system is based on the model described in
Jia and Liang (2016). Given a sequence of input
tokens (x = (xi)), the model computes a context-
dependent vector (b = (bi)) for each token using
a bidirectional RNN with LSTM units. We then
generate the output sequence (yj) left to right as
follows. At each output position, we have a hid-
den state vector (sj) which is used to produce an
“attention” distribution (αj = (αji)) over input
tokens: αji = Attend(sj , bi). This distribution is
used to generate the output token and update the
hidden state vector. To generate the token, we ei-

6Significance results are computed by the bootstrap test as
described in Berg-Kirkpatrick et al. (2012) using the output
of classifiers trained on the entire training set.

583



Feature set Train Dev
P R F1 P R F1

Proximity 56.4 48.7 52.2 56.3 48.8 52.3
Similarity 65.1 34.9 45.4 65.1 34.9 45.4
Familiarity∗ 70.5 63.5 66.8 69.6 62.9 66.1
Compatibility+ 66.9 74.4 70.4 65.4 73.1 69.0
F + C† 73.8 70.3 72.1 71.5 68.9 70.1
F + C + P† 73.8 70.3 72.1 71.5 68.9 70.1
F + C + P + S† 73.8 70.3 72.0 71.4 68.6 69.9

(a) the formula construction system. Precision, Recall and F1 are cross-
validated on 10-folds. ∗significant F1 versus P and S with p < 0.01.
+significant F1 versus P, S and F with p < 0.01. †significant F1 versus
P, S, F and C with p < 0.05.

System Train
BLEU

Test
BLEU

Baseline 65.00 57.32
RNN∗ 81.50 69.79

(b) the description generation system.
∗significant BLEU score versus the baseline
with p < 0.01.

Table 5: Evaluation of perspective generation subsystems.

7 * the cost of an employee * a week xi
input

bidirectional RNN

bi

sj attend

αji

output copy

7 times the cost of employing one person for one week
yj

output

input encoding

state vector

attention vector

Figure 7: We model description generation as a
sequence transduction task, with input as formu-
las (at bottom) and output as perspectives (at top).
We use a RNN with an attention-based copying
mechanism.

ther sample a word from the current state or copy
a word from the input using attention. Allowing
our model to copy from the input is helpful for our
task, since many of the entities are repeated verba-
tim in both input and output. We refer the reader
to Jia and Liang (2016) for more details.

Evaluation. We split the perspective descrip-
tion dataset into a training and test set such that
no formula in the test set contains the same set
of tuples as a formula in the training set.7 Ta-
ble 5b compares the performance of the base-
line and sequence-to-sequence RNN using BLEU.

7Note that formulas with the same set of tuples can oc-
cur multiple times in the either the training or test set with
different multipliers.

The sequence-to-sequence RNN performs signif-
icantly better than the baseline, producing more
natural rephrasings. Table 6 shows some output
generated by the system (see Table 6).

6 Human evaluation

In addition to the automatic evaluations for each
component of the system, we also ran an end-to-
end human evaluation on an independent set of
211 mentions collected using the same methodol-
ogy described in Section 3. Crowdworkers were
asked to choose between perspectives generated
by our full system (LR+RNN) and those generated
by the baseline of picking the numerically closest
tuple in the knowledge base (BASELINE). They
could also indicate if either both or none of the
shown perspectives appeared useful.8

Table 7 summarizes the results of the evalua-
tion and an error analysis conducted by the au-
thors. Errors were characterized as either be-
ing errors in generation (e.g. Table 6) or viola-
tions of the criteria in selecting good formulas de-
scribed in Section 4 (Table 7c). The other category
mostly contains cases where the output generated
by LR+RNN appears reasonable by the above cri-
teria but was not chosen by a majority of workers.
A few of the mentions shown did not properly de-
scribe a numeric quantity, e.g. “. . . claimed respon-
sibility for a 2009 gun massacre . . . ” and were la-
beled invalid mentions. The most common error
is the selection of a formula that is not contextu-
ally relevant to the mentioned text because no such

8Crowdworkers were paid $0.06 per to choose a perspec-
tive for each mention. Each mention and set of perspectives
were presented to 5 crowdworkers.

584



Input formula Generated perspective

7 × the cost of an employee × a week 7 times the cost of employing one person for
one week

1/10× the cost of an employee× the population of
California × the time taken for a football game

one tenth the cost of an employee during a foot-
ball game by the population of California

1 × coffee consumption × a minute × population
of the world

the amount of coffee consumed in one minute
on the world

6 × weight of a person × population of California six times the weight of the people who is worth

Table 6: Examples of perspectives generated by the sequence-to-sequence RNN. The model is able to
capture rephrasings of fact descriptions and reordering of the facts. However, it often confuses preposi-
tions and, very rarely, can produce nonsensical utterances.

LR+RNN BASELINE
perspective rated useful? #

Yes Yes 31
Yes No 63
No Yes 61
No No 56

(a) A summary of the number of times the
perspective generated by LR+RNN or BASE-
LINE was rated useful by a majority of crowd-
workers.

Cause of error #

Proximity 9
Familiarity 6
Compatibility 8
Similarity 49

Generation 24
Other 14
Invalid mention 7

Total 117

(b) An analysis of errors produced by
LR+RNN when its perspectives were not rated
useful. Errors caused by poor formula selec-
tion are further categorized by selection crite-
ria violated.

Cat. Mention

LR+RNN perspective (vs. BASELINE)

Prox. . . . ready to ship about 2,300 miles across the
Pacific to the mainland . . .

three times the distance from San Francisco to
Los Angeles (vs. the distance from San Fran-
cisco to Dallas TX).

Sim. China had disposed of about 100,000 tons of
CFCs” . . .

one fifth of the weight of garbage produced in
the United States by the population of Texas in
one week. (vs. the average food wasted every
year).

Fam. . . . the project could save New England ratepay-
ers $4.6 billion in energy costs over 25 years.

one eighth the cost of employing the population
of Asia for one hour. (vs. the construction cost
of The Cosmopolitan in Las Vegas.)

Comp. Hominids started shaping stone tools about 2.6
million years ago.

5 times the total time taken to build the number
of cars registered. (vs. 17000 times the average
lifetime for a tree).

(c) Examples of errors categorized by the criteria defined in Section 4.

Table 7: Results of an end-to-end human evaluation of the output produced by our perspective generation
system (LR+RNN) and a baseline (BASELINE) that picks the numerically closest tuple in the knowledge
base for each mention.

585



Mention Perspective (that’s about. . . )

+ In 2007, Turkmenistan exported 50 billion cu-
bic meters of gas to Russia.

the amount of oil produced by the US during a
lifetime

+ It can carry up to 10 nuclear warheads and has
a range of 8,000 km.

the distance from San Francisco to Beijing

- the 2.7 million square feet that Mission Bay’s
largest developer is entitled to build

twice the area of forest logged in a minute

- Las Vegas Sands claims the 10.5 million
square feet is the largest building in Asia.

one half of an area of an average farm

Table 8: Examples of perspectives generated by our system that frame the mentioned quantity to be
larger or smaller (top to bottom) than initially the authors thought.

formula exists within the knowledge base (within
an order of magnitude of the mentioned value):
a larger knowledge base would significantly de-
crease these errors.

7 Related work and discussion

We have proposed a new task of perspective gen-
eration. Compositionality is the key ingredient of
our approach, which allows us synthesize informa-
tion across multiple sources of information. At the
same time, compositionality also poses problems
for both formula selection and description genera-
tion.

On the formula selection side, we must com-
pose facts that make sense. For semantic com-
patibility between the mention and description, we
have relied on simple word vectors (Mikolov et al.,
2013), but more sophisticated forms of semantic
relations on larger units of text might yield better
results (Bowman et al., 2015).

On the description generation side, there is a
long line of work in generating natural language
descriptions of structured data or logical forms
Wong and Mooney (2007); Chen and Mooney
(2008); Lu and Ng (2012); Angeli et al. (2010).
We lean on the recent developments of neural
sequence-to-sequence models (Sutskever et al.,
2014; Bahdanau et al., 2014; Luong et al., 2015).
Our problem bears some similarity to the semantic
parsing work of Wang et al. (2015), who connect
generated canonical utterances (representing logi-
cal forms) to real utterances.

If we return to our initial goal of helping peo-
ple understand numbers, there are two important
directions to explore. First, we have used a small
knowledge base, which limits the coverage of per-
spectives we can generate. Using Freebase (Bol-

lacker et al., 2008) or even open information ex-
traction (Fader et al., 2011) would dramatically in-
crease the number of facts and therefore the scope
of possible perspectives.

Second, while we have focused mostly on ba-
sic compatibility, it would be interesting to explore
more deeply how the juxtaposition of facts affects
framing. Table 8 presents several examples gener-
ated by our system that frame the mentioned quan-
tities to be larger or smaller than the authors orig-
inally thought. We think perspective generation
is an exciting setting to study aspects of numeric
framing (Teigen, 2015).

Reproducibility All code, data, and
experiments for this paper are avail-
able on the CodaLab platform at https:
//worksheets.codalab.org/worksheets/

0x243284b4d81d4590b46030cdd3b72633/.

Acknowledgments

We would like to thank Glen Chiacchieri for
providing us information about the Dictionary
of Numbers, Maneesh Agarwala for useful dis-
cussions and references, Robin Jia for sharing
code for the sequence-to-sequence RNN, and the
anonymous reviewers for their constructive feed-
back. This work was partially supported by the
Sloan Research fellowship to the second author.

References

G. Angeli, P. Liang, and D. Klein. 2010. A sim-
ple domain-independent probabilistic approach
to generation. In Empirical Methods in Natural
Language Processing (EMNLP).

D. Bahdanau, K. Cho, and Y. Bengio. 2014.
Neural machine translation by jointly learn-

586



ing to align and translate. arXiv preprint
arXiv:1409.0473 .

P. J. Barrio, D. G. Goldstein, and J. M. Hofman.
2016. Improving the comprehension of num-
bers in the news. In Conference on Human Fac-
tors in Computing Systems (CHI).

T. Berg-Kirkpatrick, D. Burkett, and D. Klein.
2012. An empirical investigation of statistical
significance in NLP. In Empirical Methods in
Natural Language Processing (EMNLP). pages
995–1005.

K. Bollacker, C. Evans, P. Paritosh, T. Sturge, and
J. Taylor. 2008. Freebase: a collaboratively
created graph database for structuring human
knowledge. In International Conference on
Management of Data (SIGMOD). pages 1247–
1250.

S. Bowman, G. Angeli, C. Potts, and C. D. Man-
ning. 2015. A large annotated corpus for learn-
ing natural language inference. In Empiri-
cal Methods in Natural Language Processing
(EMNLP).

D. L. Chen and R. J. Mooney. 2008. Learning to
sportscast: A test of grounded language acqui-
sition. In International Conference on Machine
Learning (ICML). pages 128–135.

F. Chevalier, R. Vuillemot, and G. Gali. 2013. Us-
ing concrete scales: A practical framework for
effective visual depiction of complex measures.
IEEE Transactions on Visualization and Com-
puter Graphics 19:2426–2435.

G. Chiacchieri. 2013. Dictionary of numbers.
http://www.dictionaryofnumbers.
com/.

A. Fader, S. Soderland, and O. Etzioni. 2011.
Identifying relations for open information ex-
traction. In Empirical Methods in Natural Lan-
guage Processing (EMNLP).

R. Jia and P. Liang. 2016. Data recombination
for neural semantic parsing. In Association for
Computational Linguistics (ACL).

M. G. Jones and A. R. Taylor. 2009. Developing
a sense of scale: Looking backward. Journal of
Research in Science Teaching 46:460–475.

Y. Kim, J. Hullman, and M. Agarwala. 2016. Gen-
erating personalized spatial analogies for dis-
tances and areas. In Conference on Human Fac-
tors in Computing Systems (CHI).

W. Lu and H. T. Ng. 2012. A probabilistic forest-
to-string model for language generation from
typed lambda calculus expressions. In Empir-
ical Methods in Natural Language Processing
(EMNLP). pages 1611–1622.

M. Luong, H. Pham, and C. D. Manning. 2015.
Effective approaches to attention-based neural
machine translation. In Empirical Methods in
Natural Language Processing (EMNLP). pages
1412–1421.

T. Mikolov, K. Chen, G. Corrado, and Jeffrey.
2013. Efficient estimation of word representa-
tions in vector space. arXiv .

J. A. Paulos. 1988. Innumeracy: Mathematical
illiteracy and its consequences. Macmillan.

C. Seife. 2010. Proofiness: How you’re being
fooled by the numbers. Penguin.

I. Sutskever, O. Vinyals, and Q. V. Le. 2014. Se-
quence to sequence learning with neural net-
works. In Advances in Neural Information Pro-
cessing Systems (NIPS). pages 3104–3112.

K. H. Teigen. 2015. Framing of numeric quanti-
ties. The Wiley Blackwell Handbook of Judg-
ment and Decision Making pages 568–589.

T. R. Tretter, M. G. Jones, and J. Minogue. 2006.
Accuracy of scale conceptions in science: Men-
tal maneuverings across many orders of spa-
tial magnitude. Journal of Research in Science
Teaching 43:1061–1085.

Y. Wang, J. Berant, and P. Liang. 2015. Building
a semantic parser overnight. In Association for
Computational Linguistics (ACL).

Y. W. Wong and R. J. Mooney. 2007. Generation
by inverting a semantic parser that uses statisti-
cal machine translation. In Human Language
Technology and North American Association
for Computational Linguistics (HLT/NAACL).
pages 172–179.

587


