



















































Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics


Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 72–78
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-2012

Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 72–78
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-2012

Learning to Parse and Translate Improves Neural Machine Translation

Akiko Eriguchi†, Yoshimasa Tsuruoka†, and Kyunghyun Cho‡
†The University of Tokyo, 7-3-1 Hongo, Bunkyo-ku, Tokyo, Japan
{eriguchi, tsuruoka}@logos.t.u-tokyo.ac.jp

‡New York University, New York, NY 10012, USA
kyunghyun.cho@nyu.edu

Abstract

There has been relatively little attention
to incorporating linguistic prior to neu-
ral machine translation. Much of the
previous work was further constrained to
considering linguistic prior on the source
side. In this paper, we propose a hybrid
model, called NMT+RNNG, that learns
to parse and translate by combining the
recurrent neural network grammar into
the attention-based neural machine trans-
lation. Our approach encourages the neu-
ral machine translation model to incorpo-
rate linguistic prior during training, and
lets it translate on its own afterward. Ex-
tensive experiments with four language
pairs show the effectiveness of the pro-
posed NMT+RNNG.

1 Introduction

Neural Machine Translation (NMT) has enjoyed
impressive success without relying on much, if
any, prior linguistic knowledge. Some of the most
recent studies have for instance demonstrated that
NMT systems work comparably to other systems
even when the source and target sentences are
given simply as flat sequences of characters (Lee
et al., 2016; Chung et al., 2016) or statistically, not
linguistically, motivated subword units (Sennrich
et al., 2016; Wu et al., 2016). Shi et al. (2016)
recently made an observation that the encoder of
NMT captures syntactic properties of a source sen-
tence automatically, indirectly suggesting that ex-
plicit linguistic prior may not be necessary.

On the other hand, there have only been a
couple of recent studies showing the potential
benefit of explicitly encoding the linguistic prior
into NMT. Sennrich and Haddow (2016) for in-
stance proposed to augment each source word with
its corresponding part-of-speech tag, lemmatized

form and dependency label. Eriguchi et al. (2016)
instead replaced the sequential encoder with a
tree-based encoder which computes the represen-
tation of the source sentence following its parse
tree. Stahlberg et al. (2016) let the lattice from a
hierarchical phrase-based system guide the decod-
ing process of neural machine translation, which
results in two separate models rather than a single
end-to-end one. Despite the promising improve-
ments, these explicit approaches are limited in that
the trained translation model strictly requires the
availability of external tools during inference time.
More recently, researchers have proposed meth-
ods to incorporate target-side syntax into NMT
models. Alvarez-Melis and Jaakkola (2017) have
proposed a doubly-recurrent neural network that
can generate a tree-structured sentence, but its ef-
fectiveness in a full scale NMT task is yet to be
shown. Aharoni and Goldberg (2017) introduced
a method to serialize a parsed tree and to train the
serialized parsed sentences.

We propose to implicitly incorporate linguis-
tic prior based on the idea of multi-task learn-
ing (Caruana, 1998; Collobert et al., 2011). More
specifically, we design a hybrid decoder for NMT,
called NMT+RNNG1, that combines a usual con-
ditional language model and a recently pro-
posed recurrent neural network grammars (RN-
NGs, Dyer et al., 2016). This is done by plugging
in the conventional language model decoder in the
place of the buffer in RNNG, while sharing a sub-
set of parameters, such as word vectors, between
the language model and RNNG. We train this hy-
brid model to maximize both the log-probability of
a target sentence and the log-probability of a parse
action sequence. We use an external parser (An-
dor et al., 2016) to generate target parse actions,
but unlike the previous explicit approaches, we do
not need it during test time.

1Our code is available at https://github.com/
tempra28/nmtrnng.

72

https://doi.org/10.18653/v1/P17-2012
https://doi.org/10.18653/v1/P17-2012


We evaluate the proposed NMT+RNNG on four
language pairs ({JP, Cs, De, Ru}-En). We observe
significant improvements in terms of BLEU scores
on three out of four language pairs and RIBES
scores on all the language pairs.

2 Neural Machine Translation

Neural machine translation is a recently proposed
framework for building a machine translation sys-
tem based purely on neural networks. It is of-
ten built as an attention-based encoder-decoder
network (Cho et al., 2015) with two recurrent
networks—encoder and decoder—and an atten-
tion model. The encoder, which is often imple-
mented as a bidirectional recurrent network with
long short-term memory units (LSTM, Hochre-
iter and Schmidhuber, 1997) or gated recurrent
units (GRU, Cho et al., 2014), first reads a source
sentence represented as a sequence of words x =
(x1, x2, . . . , xN ). The encoder returns a sequence
of hidden states h = (h1, h2, . . . , hN ). Each hid-
den state hi is a concatenation of those from the
forward and backward recurrent network: hi =[−→
h i;
←−
h i

]
, where

−→
h i =

−→
f enc(

−→
h i−1, Vx(xi)),

←−
h i =

←−
f enc(

←−
h i+1, Vx(xi)).

Vx(xi) refers to the word vector of the i-th source
word.

The decoder is implemented as a conditional re-
current language model which models the target
sentence, or translation, as

log p(y|x) =
∑

j

log p(yj |y<j ,x),

where y = (y1, . . . , yM ). Each of the conditional
probabilities in the r.h.s is computed by

p(yj = y|y<j ,x) = softmax(W>y s̃j), (1)
s̃j = tanh(Wc[sj ; cj ]), (2)

sj = fdec(sj−1, [Vy(yj−1); s̃j−1]), (3)

where fdec is a recurrent activation function, such
as LSTM or GRU, and Wy is the output word vec-
tor of the word y.
cj is a time-dependent context vector that is

computed by the attention model using the se-
quence h of hidden states from the encoder. The
attention model first compares the current hidden

state sj against each of the hidden states and as-
signs a scalar score: βi,j = exp(h>i Wdsj) (Lu-
ong et al., 2015). These scores are then normal-
ized across the hidden states to sum to 1, that is
αi,j =

βi,j∑
i βi,j

. The time-dependent context vector
is then a weighted-sum of the hidden states with
these attention weights: cj =

∑
i αi,jhi.

3 Recurrent Neural Network Grammars

A recurrent neural network grammar (RNNG,
Dyer et al., 2016) is a probabilistic syntax-based
language model. Unlike a usual recurrent lan-
guage model (see, e.g., Mikolov et al., 2010), an
RNNG simultaneously models both tokens and
their tree-based composition. This is done by
having a (output) buffer, stack and action his-
tory, each of which is implemented as a stack
LSTM (sLSTM, Dyer et al., 2015). At each time
step, the action sLSTM predicts the next action
based on the (current) hidden states of the buffer,
stack and action sLSTM. That is,

p(at = a|a<t) ∝ eW
>
a faction(h

buffer
t ,h

stack
t ,h

action
t ), (4)

where Wa is the vector of the action a. If the se-
lected action is shift, the word at the beginning of
the buffer is moved to the stack. When the re-
duce action is selected, the top-two words in the
stack are reduced to build a partial tree. Addi-
tionally, the action may be one of many possible
non-terminal symbols, in which case the predicted
non-terminal symbol is pushed to the stack.

The hidden states of the buffer, stack and action
sLSTM are correspondingly updated by

hbuffert = StackLSTM(h
buffer
top , Vy(yt−1)), (5)

hstackt = StackLSTM(h
stack
top , rt),

hactiont = StackLSTM(h
action
top , Va(at−1)),

where Vy and Va are functions returning the target
word and action vectors. The input vector rt of the
stack sLSTM is computed recursively by

rt = tanh(Wr[r
d; rp;Va(at)]),

where rd and rp are the corresponding vectors
of the parent and dependent phrases, respec-
tively (Dyer et al., 2015). This process is iter-
ated until a complete parse tree is built. Note that
the original paper of RNNG (Dyer et al., 2016)
uses constituency trees, but we employ depen-
dency trees in this paper. Both types of trees are

73



represented as a sequence of the three types of ac-
tions in a transition-based parsing model.

When the complete sentence is provided, the
buffer simply summarizes the shifted words.
When the RNNG is used as a generator, the buffer
further generates the next word when the selected
action is shift. The latter can be done by replacing
the buffer with a recurrent language model, which
is the idea on which our proposal is based.

4 Learning to Parse and Translate

4.1 NMT+RNNG

Our main proposal in this paper is to hybridize the
decoder of the neural machine translation and the
RNNG. We continue from the earlier observation
that we can replace the buffer of RNNG to a recur-
rent language model that simultaneously summa-
rizes the shifted words as well as generates future
words. We replace the RNNG’s buffer with the
neural translation model’s decoder in two steps.

Construction First, we replace the hidden state
of the buffer hbuffer (in Eq. (5)) with the hidden
state of the decoder of the attention-based neural
machine translation from Eq. (3). As is clear from
those two equations, both the buffer sLSTM and
the translation decoder take as input the previous
hidden state (hbuffertop and sj−1, respectively) and
the previously decoded word (or the previously
shifted word in the case of the RNNG’s buffer),
and returns its summary state. The only difference
is that the translation decoder additionally consid-
ers the state s̃j−1. Once the buffer of the RNNG
is replaced with the NMT decoder in our proposed
model, the NMT decoder is also under control of
the actions provided by the RNNG.2 Second, we
let the next word prediction of the translation de-
coder as a generator of RNNG. In other words,
the generator of RNNG will output a word, when
asked by the shift action, according to the condi-
tional distribution defined by the translation de-
coder in Eq. (1). Once the buffer sLSTM is re-
placed with the neural translation decoder, the ac-
tion sLSTM naturally takes as input the translation
decoder’s hidden state when computing the action
conditional distribution in Eq. (4). We call this hy-
brid model NMT+RNNG.

2The j-th hidden state in Eq. (3) is calculated only when
the action (shift) is predicted by the RNNG. This is why our
proposed model can handle the sequences of words and ac-
tions which have different lengths.

Learning and Inference After this integration,
our hybrid NMT+RNNG models the conditional
distribution over all possible pairs of transla-
tion and its parse given a source sentence, i.e.,
p(y,a|x). Assuming the availability of parse
annotation in the target-side of a parallel cor-
pus, we train the whole model jointly to maxi-
mize E(x,y,a)∼data [log p(y,a|x)]. In doing so, we
notice that there are two separate paths through
which the neural translation decoder receives er-
ror signal. First, the decoder is updated in or-
der to maximize the conditional probability of the
correct next word, which has already existed in
the original neural machine translation. Second,
the decoder is updated also to maximize the con-
ditional probability of the correct parsing action,
which is a novel learning signal introduced by the
proposed hybridization. Furthermore, the second
learning signal affects the encoder as well, encour-
aging the whole neural translation model to be
aware of the syntactic structure of the target lan-
guage. Later in the experiments, we show that this
additional learning signal is useful for translation,
even though we discard the RNNG (the stack and
action sLSTMs) in the inference time.

4.2 Knowledge Distillation for Parsing

A major challenge in training the proposed hybrid
model is that there is not a parallel corpus aug-
mented with gold-standard target-side parse, and
vice versa. In other words, we must either parse
the target-side sentences of an existing parallel
corpus or translate sentences with existing gold-
standard parses. As the target task of the proposed
model is translation, we start with a parallel cor-
pus and annotate the target-side sentences. It is
however costly to manually annotate any corpus
of reasonable size (Table 6 in Alonso et al., 2016).

We instead resort to noisy, but automated an-
notation using an existing parser. This approach
of automated annotation can be considered along
the line of recently proposed techniques of knowl-
edge distillation (Hinton et al., 2015) and distant
supervision (Mintz et al., 2009). In knowledge dis-
tillation, a teacher network is trained purely on a
training set with ground-truth annotations, and the
annotations predicted by this teacher are used to
train a student network, which is similar to our ap-
proach where the external parser could be thought
of as a teacher and the proposed hybrid network’s
RNNG as a student. On the other hand, what we

74



Train. Dev. Test Voc. (src, tgt, act)
Cs-En 134,453 2,656 2,999 (33,867, 27,347, 82)
De-En 166,313 2,169 2,999 (33,820, 30,684, 80)
Ru-En 131,492 2,818 2,998 (32,442, 27,979, 82)
Jp-En 100,000 1,790 1,812 (23,509, 28,591, 80)

Table 1: Statistics of parallel corpora.

propose here is a special case of distant supervi-
sion in that the external parser provides noisy an-
notations to otherwise an unlabeled training set.

Specifically, we use SyntaxNet, released by An-
dor et al. (2016), on a target sentence.3 We convert
a parse tree into a sequence of one of three tran-
sition actions (SHIFT, REDUCE-L, REDUCE-R).
We label each REDUCE action with a correspond-
ing dependency label and treat it as a more fine-
grained action.

5 Experiments

5.1 Language Pairs and Corpora

We compare the proposed NMT+RNNG against
the baseline model on four different language
pairs–Jp-En, Cs-En, De-En and Ru-En. The ba-
sic statistics of the training data are presented in
Table 1. We mapped all the low-frequency words
to the unique symbol “UNK” and inserted a spe-
cial symbol “EOS” at the end of both source and
target sentences.

Ja We use the ASPEC corpus (“train1.txt”) from
the WAT’16 Jp-En translation task. We tokenize
each Japanese sentence with KyTea (Neubig et al.,
2011) and preprocess according to the recommen-
dations from WAT’16 (WAT, 2016). We use the
first 100K sentence pairs of length shorter than 50
for training. The vocabulary is constructed with
all the unique tokens that appear at least twice in
the training corpus. We use “dev.txt” and “test.txt”
provided by WAT’16 respectively as development
and test sets.

Cs, De and Ru We use News Commentary v8.
We removed noisy metacharacters and used the to-
kenizer from Moses (Koehn et al., 2007) to build a
vocabulary of each language using unique tokens
that appear at least 6, 6 and 5 times respectively for
Cs, Ru and De. The target-side (English) vocab-
ulary was constructed with all the unique tokens

3When the target sentence is parsed as data preprocessing,
we use all the vocabularies in a corpus and do not cut off
any words. We use the plain SyntaxNet and do not train it
furthermore.

appearing more than three times in each corpus.
We also excluded the sentence pairs which include
empty lines in either a source sentence or a target
sentence. We only use sentence pairs of length 50
or less for training. We use “newstest2015” and
“newstest2016” as development and test sets re-
spectively.

5.2 Models, Learning and Inference

In all our experiments, each recurrent network has
a single layer of LSTM units of 256 dimensions,
and the word vectors and the action vectors are
of 256 and 128 dimensions, respectively. To re-
duce computational overhead, we use BlackOut (Ji
et al., 2015) with 2000 negative samples and α =
0.4. When employing BlackOut, we shared the
negative samples of each target word in a sen-
tence in training time (Hashimoto and Tsuruoka,
2017), which is similar to the previous work (Zoph
et al., 2016). For the proposed NMT+RNNG, we
share the target word vectors between the decoder
(buffer) and the stack sLSTM.

Each weight is initialized from the uniform dis-
tribution [−0.1, 0.1]. The bias vectors and the
weights of the softmax and BlackOut are initial-
ized to be zero. The forget gate biases of LSTMs
and Stack-LSTMs are initialized to 1 as recom-
mended in Józefowicz et al. (2015). We use
stochastic gradient descent with minibatches of
128 examples. The learning rate starts from 1.0,
and is halved each time the perplexity on the de-
velopment set increases. We clip the norm of the
gradient (Pascanu et al., 2012) with the thresh-
old set to 3.0 (2.0 for the baseline models on Ru-
En and Cs-En to avoid NaN and Inf). When the
perplexity of development data increased in train-
ing time, we halved the learning rate of stochastic
gradient descent and reloaded the previous model.
The RNNG’s stack computes the vector of a de-
pendency parse tree which consists of the gener-
ated target words by the buffer. Since the complete
parse tree has a “ROOT” node, the special token of
the end of a sentence (“EOS”) is considered as the
ROOT. We use beam search in the inference time,
with the beam width selected based on the devel-
opment set performance.

It took about 15 minutes per epoch and about 20
minutes respectively for the baseline and the pro-
posed model to train a full JP-EN parallel corpus
in our implementation.4

4We run all the experiments on multi-core CPUs (10

75



De-En Ru-En Cs-En Jp-En
BLEU

NMT 16.61 12.03 11.22 17.88
NMT+RNNG 16.41 12.46† 12.06† 18.84†

RIBES
NMT 73.75 69.56 69.59 71.27
NMT+RNNG 75.03† 71.04† 70.39† 72.25†

Table 2: BLEU and RIBES scores by the baseline
and proposed models on the test set. We use the
bootstrap resampling method from Koehn (2004)
to compute the statistical significance. We use † to
mark those significant cases with p < 0.005.

Jp-En (Dev) BLEU
NMT+RNNG 18.60
w/o Buffer 18.02
w/o Action 17.94
w/o Stack 17.58
NMT 17.75

Table 3: Effect of each component in RNNG.

5.3 Results and Analysis
In Table 2, we report the translation qualities of
the tested models on all the four language pairs.
We report both BLEU (Papineni et al., 2002) and
RIBES (Isozaki et al., 2010). Except for De-
En, measured in BLEU, we observe the statis-
tically significant improvement by the proposed
NMT+RNNG over the baseline model. It is worth-
while to note that these significant improvements
have been achieved without any additional param-
eters nor computational overhead in the inference
time.

Ablation Since each component in RNNG may
be omitted, we ablate each component in the pro-
posed NMT+RNNG to verify their necessity.5 As
shown in Table 3, we see that the best performance
could only be achieved when all the three compo-
nents were present. Removing the stack had the
most adverse effect, which was found to be the
case for parsing as well by Kuncoro et al. (2017).

Generated Sentences with Parsed Actions
The decoder part of our proposed model consists
of two components: the NMT decoder to gener-

threads on Intel(R) Xeon(R) CPU E5-2680 v2 @2.80GHz)
5 Since the buffer is the decoder, it is not possible to com-

pletely remove it. Instead we simply remove the dependency
of the action distribution on it.

Figure 1: An example of translation and its depen-
dency relations obtained by our proposed model.

ate a translated sentence and the RNNG decoder
to predict its parsing actions. The proposed model
can therefore output a dependency structure along
with a translated sentence. Figure 1 shows an
example of JP-EN translation in the development
dataset and its dependency parse tree obtained by
the proposed model. The special symbol (“EOS”)
is treated as the root node (“ROOT”) of the parsed
tree. The translated sentence was generated by
using beam search, which is the same setting of
NMT+RNNG shown in Table 3. The parsing ac-
tions were obtained by greedy search. The re-
sulting dependency structure is mostly correct but
contains a few errors; for example, dependency re-
lation between “The” and “ transition” should not
be “pobj”.

6 Conclusion

We propose a hybrid model, to which we refer
as NMT+RNNG, that combines the decoder of an
attention-based neural translation model with the
RNNG. This model learns to parse and translate si-
multaneously, and training it encourages both the
encoder and decoder to better incorporate linguis-
tic priors. Our experiments confirmed its effec-
tiveness on four language pairs ({JP, Cs, De, Ru}-
En). The RNNG can in principle be trained with-
out ground-truth parses, and this would eliminate
the need of external parsers completely. We leave
the investigation into this possibility for future re-
search.

Acknowledgments

We thank Yuchen Qiao and Kenjiro Taura for their
help to speed up the implementations of training
and also Kazuma Hashimoto for his valuable com-
ments and discussions. This work was supported
by JST CREST Grant Number JPMJCR1513 and
JSPS KAKENHI Grant Number 15J12597 and

76



16H01715. KC thanks support by eBay, Face-
book, Google and NVIDIA.

References
Roee Aharoni and Yoav Goldberg. 2017. Towards

string-to-tree neural machine translation. In Pro-
ceedings of the 55th Annual Meeting of the Asso-
ciation for Computational Linguistics. to appear.

Héctor Martı́nez Alonso, Djamé Seddah, and Benoı̂t
Sagot. 2016. From noisy questions to minecraft
texts: Annotation challenges in extreme syntax sce-
nario. In Proceedings of the 2nd Workshop on Noisy
User-generated Text (WNUT). pages 13–23.

David Alvarez-Melis and Tommi S. Jaakkola. 2017.
Tree-structured decoding with doubly-recurrent
neural networks. In Proceedings of International
Conference on Learning Representations 2017.

Daniel Andor, Chris Alberti, David Weiss, Aliaksei
Severyn, Alessandro Presta, Kuzman Ganchev, Slav
Petrov, and Michael Collins. 2016. Globally nor-
malized transition-based neural networks. In Pro-
ceedings of the 54th Annual Meeting of the Asso-
ciation for Computational Linguistics. pages 2442–
2452.

Rich Caruana. 1998. Multitask learning. In Learning
to learn, Springer, pages 95–133.

Kyunghyun Cho, Aaron Courville, and Yoshua Ben-
gio. 2015. Describing multimedia content using
attention-based encoder-decoder networks. IEEE
Transactions on Multimedia 17(11):1875–1886.

Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using rnn encoder–decoder
for statistical machine translation. In Proceedings of
the 2014 Conference on Empirical Methods in Nat-
ural Language Processing. pages 1724–1734.

Junyoung Chung, Kyunghyun Cho, and Yoshua Ben-
gio. 2016. A character-level decoder without ex-
plicit segmentation for neural machine translation.
In Proceedings of the 54th Annual Meeting of the
Association for Computational Linguistics. Associ-
ation for Computational Linguistics, pages 1693–
1703.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research
12:2493–2537.

Chris Dyer, Miguel Ballesteros, Wang Ling, Austin
Matthews, and A. Noah Smith. 2015. Transition-
based dependency parsing with stack long short-
term memory. In Proceedings of the 53rd Annual

Meeting of the Association for Computational Lin-
guistics and the 7th International Joint Conference
on Natural Language Processing. pages 334–343.

Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros,
and A. Noah Smith. 2016. Recurrent neural net-
work grammars. In Proceedings of the 2016 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies. pages 199–209.

Akiko Eriguchi, Kazuma Hashimoto, and Yoshimasa
Tsuruoka. 2016. Tree-to-sequence attentional neu-
ral machine translation. In Proceedings of the 54th
Annual Meeting of the Association for Computa-
tional Linguistics. pages 823–833.

Kazuma Hashimoto and Yoshimasa Tsuruoka.
2017. Neural Machine Translation with Source-
Side Latent Graph Parsing. arXiv preprint
arXiv:1702.02265 .

Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.
Distilling the knowledge in a neural network. arXiv
preprint arXiv:1503.02531 .

Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long
short-term memory. Neural Comput. 9(8):1735–
1780.

Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito
Sudoh, and Hajime Tsukada. 2010. Automatic eval-
uation of translation quality for distant language
pairs. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Process-
ing. pages 944–952.

Shihao Ji, S. V. N. Vishwanathan, Nadathur Satish,
Michael J. Anderson, and Pradeep Dubey. 2015.
Blackout: Speeding up recurrent neural network lan-
guage models with very large vocabularies. Pro-
ceedings of International Conference on Learning
Representations 2015 .

Rafal Józefowicz, Wojciech Zaremba, and Ilya
Sutskever. 2015. An empirical exploration of recur-
rent network architectures. In Proceedings of the
32nd International Conference on Machine Learn-
ing. pages 2342–2350.

Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
the 2004 Conference on Empirical Methods in Nat-
ural Language Processing. pages 388–395.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the As-
sociation for Computational Linguistics Companion
Volume Proceedings of the Demo and Poster Ses-
sions. pages 177–180.

77



Adhiguna Kuncoro, Miguel Ballesteros, Lingpeng
Kong, Chris Dyer, Graham Neubig, and Noah A.
Smith. 2017. What do recurrent neural network
grammars learn about syntax? In Proceedings of
the 15th Conference of the European Chapter of the
Association for Computational Linguistics: Volume
1, Long Papers. pages 1249–1258.

Jason Lee, Kyunghyun Cho, and Thomas Hofmann.
2016. Fully character-level neural machine trans-
lation without explicit segmentation. arXiv preprint
arXiv:1610.03017 .

Thang Luong, Hieu Pham, and Christopher D. Man-
ning. 2015. Effective approaches to attention-based
neural machine translation. In Proceedings of the
2015 Conference on Empirical Methods in Natural
Language Processing. pages 1412–1421.

Tomáš Mikolov, Martin Karafiát, Lukáš Burget, Jan
Černocký, and Sanjeev Khudanpur. 2010. Recurrent
neural network based language model. In Proceed-
ings of the 11th Annual Conference of the Interna-
tional Speech Communication Association (INTER-
SPEECH 2010). International Speech Communica-
tion Association, pages 1045–1048.

Mike Mintz, Steven Bills, Rion Snow, and Dan Juraf-
sky. 2009. Distant supervision for relation extrac-
tion without labeled data. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP. pages
1003–1011.

Graham Neubig, Yosuke Nakata, and Shinsuke Mori.
2011. Pointwise prediction for robust, adaptable
japanese morphological analysis. In Proceedings of
the 49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies. pages 529–533.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics. pages 311–318.

Razvan Pascanu, Tomas Mikolov, and Yoshua Ben-
gio. 2012. Understanding the exploding gra-
dient problem. arXiv preprint arXiv:1211.5063
abs/1211.5063.

Rico Sennrich and Barry Haddow. 2016. Linguistic
input features improve neural machine translation.
In Proceedings of the First Conference on Machine
Translation. pages 83–91.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Neural machine translation of rare words with
subword units. In Proceedings of the 54th Annual
Meeting of the Association for Computational Lin-
guistics. pages 1715–1725.

Xing Shi, Inkit Padhi, and Kevin Knight. 2016. Does
string-based neural mt learn source syntax? In Pro-
ceedings of the 2016 Conference on Empirical Meth-
ods in Natural Language Processing. pages 1526–
1534.

Felix Stahlberg, Eva Hasler, Aurelien Waite, and Bill
Byrne. 2016. Syntactically guided neural machine
translation. In Proceedings of the 54th Annual Meet-
ing of the Association for Computational Linguis-
tics. pages 299–305.

WAT. 2016. http://lotus.kuee.
kyoto-u.ac.jp/WAT/baseline/
dataPreparationJE.html.

Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V.
Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus
Macherey, Jeff Klingner, Apurva Shah, Melvin
Johnson, Xiaobing Liu, Lukasz Kaiser, Stephan
Gouws, Yoshikiyo Kato, Taku Kudo, Hideto
Kazawa, Keith Stevens, George Kurian, Nishant
Patil, Wei Wang, Cliff Young, Jason Smith, Jason
Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado,
Macduff Hughes, and Jeffrey Dean. 2016. Google’s
neural machine translation system: Bridging the
gap between human and machine translation. arXiv
preprint arXiv:1609.08144 .

Barret Zoph, Ashish Vaswani, Jonathan May, and
Kevin Knight. 2016. Simple, Fast Noise-Contrastive
Estimation for Large RNN Vocabularies. In Pro-
ceedings of the 2016 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies. pages
1217–1222.

78


	Learning to Parse and Translate Improves Neural Machine Translation

