



















































The SIGMORPHON 2019 Shared Task: Morphological Analysis in Context and Cross-Lingual Transfer for Inflection


Proceedings of the 16th Workshop on Computational Research in Phonetics, Phonology, and Morphology, pages 229–244
Florence, Italy. August 2, 2019 c©2019 Association for Computational Linguistics

229

The SIGMORPHON 2019 Shared Task:
Morphological Analysis in Context and Cross-Lingual Transfer

for Inflection
Arya D. McCarthy♣ and Ekaterina Vylomova♥ and

Shijie Wu♣ and Chaitanya Malaviya♠ and Lawrence Wolf-Sonkin♦ and
Garrett Nicolai♣ and Miikka Silfverberg] and Sebastian Mielke♣ and

Jeffrey Heinz[ and Ryan Cotterell♣ and Mans Hulden\

♣Johns Hopkins University ♥University of Melbourne ♠Allen Institute for AI
♦Google ]University of Helsinki [Stony Brook University \University of Colorado

Abstract
The SIGMORPHON 2019 shared task on
cross-lingual transfer and contextual analysis
in morphology examined transfer learning of
inflection between 100 language pairs, as well
as contextual lemmatization and morphosyn-
tactic description in 66 languages. The first
task evolves past years’ inflection tasks by ex-
amining transfer of morphological inflection
knowledge from a high-resource language to a
low-resource language. This year also presents
a new second challenge on lemmatization and
morphological feature analysis in context. All
submissions featured a neural component and
built on either this year’s strong baselines or
highly ranked systems from previous years’
shared tasks. Every participating team im-
proved in accuracy over the baselines for the
inflection task (though not Levenshtein dis-
tance), and every team in the contextual analy-
sis task improved on both state-of-the-art neu-
ral and non-neural baselines.

1 Introduction

While producing a sentence, humans combine vari-
ous types of knowledge to produce fluent output—
various shades of meaning are expressed through
word selection and tone, while the language is
made to conform to underlying structural rules via
syntax and morphology. Native speakers are often
quick to identify disfluency, even if the meaning of
a sentence is mostly clear.

Automatic systems must also consider these
constraints when constructing or processing lan-
guage. Strong enough language models can often
reconstruct common syntactic structures, but are
insufficient to properly model morphology. Many
languages implement large inflectional paradigms
that mark both function and content words with
a varying levels of morphosyntactic information.
For instance, Romanian verb forms inflect for per-
son, number, tense, mood, and voice; meanwhile,

Archi verbs can take on thousands of forms (Kib-
rik, 1998). Such complex paradigms produce large
inventories of words, all of which must be pro-
ducible by a realistic system, even though a large
percentage of them will never be observed over
billions of lines of linguistic input. Compounding
the issue, good inflectional systems often require
large amounts of supervised training data, which is
infeasible in many of the world’s languages.

This year’s shared task is concentrated on en-
couraging the construction of strong morphologi-
cal systems that perform two related but different
inflectional tasks. The first task asks participants
to create morphological inflectors for a large num-
ber of under-resourced languages, encouraging sys-
tems that use highly-resourced, related languages
as a cross-lingual training signal. The second task
welcomes submissions that invert this operation in
light of contextual information: Given an unanno-
tated sentence, lemmatize each word, and tag them
with a morphosyntactic description. Both of these
tasks extend upon previous morphological competi-
tions, and the best submitted systems now represent
the state of the art in their respective tasks.

2 Tasks and Evaluation

2.1 Task 1: Cross-lingual transfer for
morphological inflection

Annotated resources for the world’s languages are
not distributed equally—some languages simply
have more as they have more native speakers will-
ing and able to annotate more data. We explore
how to transfer knowledge from high-resource lan-
guages that are genetically related to low-resource
languages.

The first task iterates on last year’s main task:
morphological inflection (Cotterell et al., 2018).
Instead of giving some number of training exam-
ples in the language of interest, we provided only



230

a limited number in that language. To accompany
it, we provided a larger number of examples in
either a related or unrelated language. Each test
example asked participants to produce some other
inflected form when given a lemma and a bundle
of morphosyntactic features as input. The goal,
thus, is to perform morphological inflection in
the low-resource language, having hopefully ex-
ploited some similarity to the high-resource lan-
guage. Models which perform well here can aid
downstream tasks like machine translation in low-
resource settings. All datasets were resampled from
UniMorph, which makes them distinct from past
years.

The mode of the task is inspired by Zoph et al.
(2016), who fine-tune a model pre-trained on a
high-resource language to perform well on a low-
resource language. We do not, though, require that
models be trained by fine-tuning. Joint modeling or
any number of methods may be explored instead.

Example The model will have access to type-
level data in a low-resource target language, plus
a high-resource source language. We give an ex-
ample here of Asturian as the target language with
Spanish as the source language.

Low-resource target training data (Asturian)
facer “fechu” V;V.PTCP;PST
aguar “aguà” V;PRS;2;PL;IND
...

...
...

High-resource source language training data
(Spanish)
tocar “tocando” V;V.PTCP;PRS
bailar “bailaba” V;PST;IPFV;3;SG;IND
mentir “mintió” V;PST;PFV;3;SG;IND
...

...
...

Test input (Asturian)
baxar V;V.PTCP;PRS
Test output (Asturian)

“baxando”

Table 1: Sample language pair and data format for
Task 1

Evaluation We score the output of each system
in terms of its predictions’ exact-match accuracy
and the average Levenshtein distance between the
predictions and their corresponding true forms.

2.2 Task 2: Morphological analysis in context

Although inflection of words in a context-agnostic
manner is a useful evaluation of the morphological
quality of a system, people do not learn morphol-
ogy in isolation.

In 2018, the second task of the CoNLL–
SIGMORPHON Shared Task (Cotterell et al.,
2018) required submitting systems to complete an
inflectional cloze task (Taylor, 1953) given only
the sentential context and the desired lemma – an
example of the problem is given in the following
lines: A successful system would predict the plu-
ral form “dogs”. Likewise, a Spanish word form
“ayuda” may be a feminine noun or a third-person
verb form, which must be disambiguated by con-
text.

The are barking.
(dog)

This year’s task extends the second task from last
year. Rather than inflect a single word in context,
the task is to provide a complete morphological
tagging of a sentence: for each word, a successful
system will need to lemmatize and tag it with a
morphsyntactic description (MSD).

The dogs are barking .

the dog be bark .
DET N;PL V;PRS;3;PL V;V.PTCP;PRS PUNCT

Context is critical—depending on the sentence,
identical word forms realize a large number of po-
tential inflectional categories, which will in turn
influence lemmatization decisions. If the sen-
tence were instead “The barking dogs kept us up
all night”, “barking” is now an adjective, and its
lemma is also “barking”.

3 Data

3.1 Data for Task 1

Language pairs We presented data in 100 lan-
guage pairs spanning 79 unique languages. Data
for all but four languages (Basque, Kurmanji, Mur-
rinhpatha, and Sorani) are extracted from English
Wiktionary, a large multi-lingual crowd-sourced
dictionary with morphological paradigms for many
lemmata.1 20 of the 100 language pairs are either

1The Basque language data was extracted from a manually
designed finite-state morphological analyzer (Alegria et al.,
2009). Murrinhpatha data was donated by John Mansfield; it



231

distantly related or unrelated; this allows specula-
tion into the relative importance of data quantity
and linguistic relatedness.

Data format For each language, the basic data
consists of triples of the form (lemma, feature bun-
dle, inflected form), as in Table 1. The first fea-
ture in the bundle always specifies the core part of
speech (e.g., verb). For each language pair, sepa-
rate files contain the high- and low-resource train-
ing examples.

All features in the bundle are coded according to
the UniMorph Schema, a cross-linguistically con-
sistent universal morphological feature set (Sylak-
Glassman et al., 2015a,b).

Extraction from Wiktionary For each of the
Wiktionary languages, Wiktionary provides a num-
ber of tables, each of which specifies the full in-
flectional paradigm for a particular lemma. As in
the previous iteration, tables were extracted using a
template annotation procedure described in (Kirov
et al., 2018).

Sampling data splits From each language’s col-
lection of paradigms, we sampled the training, de-
velopment, and test sets as in 2018.2 Crucially,
while the data were sampled in the same fashion,
the datasets are distinct from those used for the
2018 shared task.

Our first step was to construct probability distri-
butions over the (lemma, feature bundle, inflected
form) triples in our full dataset. For each triple, we
counted how many tokens the inflected form has
in the February 2017 dump of Wikipedia for that
language. To distribute the counts of an observed
form over all the triples that have this token as its
form, we follow the method used in the previous
shared task (Cotterell et al., 2018), training a neu-
ral network on unambiguous forms to estimate the
distribution over all, even ambiguous, forms. We
then sampled 12,000 triples without replacement
from this distribution. The first 100 were taken as
training data for low-resource settings. The first
10,000 were used as high-resource training sets. As
these sets are nested, the highest-count triples tend
to appear in the smaller training sets.3

is discussed in Mansfield (2019). Data for Kurmanji Kurdish
and Sorani Kurdish were created as part of the Alexina project
(Walther et al., 2010; Walther and Sagot, 2010).

2These datasets can be obtained from https://
sigmorphon.github.io/sharedtasks/2019/

3Several high-resource languages had necessarily fewer,
but on a similar order of magnitude. Bengali, Uzbek, Kannada,

The final 2000 triples were randomly shuffled
and then split in half to obtain development and
test sets of 1000 forms each.4 The final shuffling
was performed to ensure that the development set is
similar to the test set. By contrast, the development
and test sets tend to contain lower-count triples
than the training set.5

Other modifications We further adopted some
changes to increase compatibility. Namely, we cor-
rected some annotation errors created while scrap-
ing Wiktionary for the 2018 task, and we standard-
ized Romanian t-cedilla and t-comma to t-comma.
(The same was done with s-cedilla and s-comma.)

3.2 Data for Task 2

Our data for task 2 come from the Universal Depen-
dencies treebanks (UD; Nivre et al., 2018, v2.3),
which provides pre-defined training, development,
and test splits and annotations in a unified anno-
tation schema for morphosyntax and dependency
relationships. Unlike the 2018 cloze task which
used UD data, we require no manual data prepa-
ration and are able to leverage all 107 monolin-
gual treebanks. As is typical, data are presented in
CoNLL-U format,6 although we modify the mor-
phological feature and lemma fields.

Data conversion The morphological annotations
for the 2019 shared task were converted to the Uni-
Morph schema (Kirov et al., 2018) according to
McCarthy et al. (2018), who provide a determin-
istic mapping that increases agreement across lan-
guages. This also moves the part of speech into
the bundle of morphological features. We do not
attempt to individually correct any errors in the UD
source material. Further, some languages received
additional pre-processing. In the Finnish data, we
removed morpheme boundaries that were present
in the lemmata (e.g., puhe#kieli 7→ puhekieli
‘spoken+language’). Russian lemmata in the GSD
treebank were presented in all uppercase; to match

Swahili. Likewise, the low-resource language Telugu had
fewer than 100 forms.

4When sufficient data are unavailable, we instead use 50
or 100 examples.

5This mimics a realistic setting, as supervised training is
usually employed to generalize from frequent words that ap-
pear in annotated resources to less frequent words that do not.
Unsupervised learning methods also tend to generalize from
more frequent words (which can be analyzed more easily by
combining information from many contexts) to less frequent
ones.

6https://universaldependencies.org/format.
html

https://sigmorphon.github.io/sharedtasks/2019/
https://sigmorphon.github.io/sharedtasks/2019/
https://universaldependencies.org/format.html
https://universaldependencies.org/format.html


232

the 2018 shared task, we lowercased these. In de-
velopment and test data, all fields except for form
and index within the sentence were struck.

4 Baselines

4.1 Task 1 Baseline
We include four neural sequence-to-sequence mod-
els mapping lemma into inflected word forms: soft
attention (Luong et al., 2015), non-monotonic hard
attention (Wu et al., 2018), monotonic hard atten-
tion and a variant with offset-based transition distri-
bution (Wu and Cotterell, 2019). Neural sequence-
to-sequence models with soft attention (Luong
et al., 2015) have dominated previous SIGMOR-
PHON shared tasks (Cotterell et al., 2017). Wu
et al. (2018) instead models the alignment between
characters in the lemma and the inflected word
form explicitly with hard attention and learns this
alignment and transduction jointly. Wu and Cot-
terell (2019) shows that enforcing strict monotonic-
ity with hard attention is beneficial in tasks such
as morphological inflection where the transduction
is mostly monotonic. The encoder is a biLSTM
while the decoder is a left-to-right LSTM. All mod-
els use multiplicative attention and have roughly
the same number of parameters. In the model, a
morphological tag is fed to the decoder along with
target character embeddings to guide the decoding.
During the training of the hard attention model, dy-
namic programming is applied to marginalize all
latent alignments exactly.

4.2 Task 2 Baselines
Non-neural (Müller et al., 2015): The Lemming
model is a log-linear model that performs joint
morphological tagging and lemmatization. The
model is globally normalized with the use of a sec-
ond order linear-chain CRF. To efficiently calculate
the partition function, the choice of lemmata are
pruned with the use of pre-extracted edit trees.

Neural (Malaviya et al., 2019): This is a state-
of-the-art neural model that also performs joint
morphological tagging and lemmatization, but also
accounts for the exposure bias with the applica-
tion of maximum likelihood (MLE). The model
stitches the tagger and lemmatizer together with
the use of jackknifing (Agić and Schluter, 2017) to
expose the lemmatizer to the errors made by the
tagger model during training. The morphological
tagger is based on a character-level biLSTM em-
bedder that produces the embedding for a word,

Team Avg. Accuracy Avg. Levenshtein

AX-01 18.54 3.62
AX-02 24.99 2.72
CMU-03 58.79 1.52
IT-IST-01 49.00 1.29
IT-IST-02 50.18 1.32
Tuebingen-01† 34.49 1.88
Tuebingen-02† 20.86 2.36
UAlberta-01* 48.33 1.23
UAlberta-02*† 54.75 1.03
UAlberta-03*† 8.45 4.06
UAlberta-04*† 11.00 3.86
UAlberta-05* 4.10 3.08
UAlberta-06*† 26.85 2.65

Baseline 48.55 1.33

Table 2: Task 1 Team Scores, averaged across all Lan-
guages; * indicates submissions were only applied to a
subset of languages, making scores incomparable. † in-
dicates that additional resources were used for training.

and a word-level biLSTM tagger that predicts a
morphological tag sequence for each word in the
sentence. The lemmatizer is a neural sequence-
to-sequence model (Wu and Cotterell, 2019) that
uses the decoded morphological tag sequence from
the tagger as an additional attribute. The model
uses hard monotonic attention instead of standard
soft attention, along with a dynamic programming
based training scheme.

5 Results

The SIGMORPHON 2019 shared task received
30 submissions—14 for task 1 and 16 for task 2—
from 23 teams. In addition, the organizers’ baseline
systems were evaluated.

5.1 Task 1 Results

Five teams participated in the first Task, with a
variety of methods aimed at leveraging the cross-
lingual data to improve system performance.

The University of Alberta (UAlberta) performed
a focused investigation on four language pairs,
training cognate-projection systems from exter-
nal cognate lists. Two methods were considered:
one which trained a high-resource neural encoder-
decoder, and projected the test data into the HRL,
and one that projected the HRL data into the LRL,
and trained a combined system. Results demon-
strated that certain language pairs may be amenable
to such methods.



233

HRL–LRL Baseline Best Team HRL–LRL Baseline Best Team

adyghe–kabardian 96.0 97.0 Tuebingen-02 hungarian–livonian 29.0 44.0 it-ist-01
albanian–breton 40.0 81.0 CMU-03 hungarian–votic 19.0 34.0 it-ist-01
arabic–classical-syriac 66.0 92.0 CMU-03 irish–breton 39.0 79.0 CMU-03
arabic–maltese 31.0 41.0 CMU-03 irish–cornish 24.0 34.0 it-ist-01
arabic–turkmen 74.0 84.0 CMU-03 irish–old-irish 2.0 6.0 it-ist-02
armenian–kabardian 83.0 87.0 it-ist-01 irish–scottish-gaelic 64.0 66.0 CMU-03
asturian–occitan 48.0 77.0 CMU-03 italian–friulian 56.0 78.0 CMU-03
bashkir–azeri 39.0 69.0 it-ist-02 italian–ladin 55.0 74.0 CMU-03
bashkir–crimean-tatar 70.0 70.0 CMU-03 italian–maltese 26.0 45.0 CMU-03
bashkir–kazakh 80.0 90.0 it-ist-01 italian–neapolitan 80.0 83.0 CMU-03
bashkir–khakas 86.0 96.0 it-ist-02 kannada–telugu 82.0 94.0 CMU-03
bashkir–tatar 68.0 74.0 it-ist-02 kurmanji–sorani 15.0 69.0 CMU-03
bashkir–turkmen 94.0 88.0 it-ist-01 latin–czech 20.1 71.4 CMU-03
basque–kashubian 40.0 76.0 CMU-03 latvian–lithuanian 17.1 48.4 CMU-03
belarusian–old-irish 2.0 10.0 CMU-03 latvian–scottish-gaelic 48.0 68.0 CMU-03
bengali–greek 17.7 74.6 CMU-03 persian–azeri 46.0 69.0 CMU-03
bulgarian–old-church-slavonic 44.0 56.0 CMU-03 persian–pashto 27.0 48.0 CMU-03
czech–kashubian 52.0 78.0 CMU-03 polish–kashubian 74.0 78.0 CMU-03
czech–latin 8.4 42.0 CMU-03 polish–old-church-slavonic 40.0 58.0 CMU-03
danish–middle-high-german 72.0 82.0 it-ist-02 portuguese–russian 27.5 76.3 CMU-03
danish–middle-low-german 36.0 44.0 it-ist-01 romanian–latin 6.7 41.3 CMU-03
danish–north-frisian 28.0 46.0 CMU-03 russian–old-church-slavonic 34.0 64.0 CMU-03
danish–west-frisian 42.0 43.0 CMU-03 russian–portuguese 50.5 88.4 CMU-03
danish–yiddish 76.0 67.0 it-ist-01 sanskrit–bengali 33.0 65.0 CMU-03
dutch–middle-high-german 76.0 78.0 it-ist-01 / it-ist-02 sanskrit–pashto 34.0 43.0 CMU-03
dutch–middle-low-german 42.0 52.0 it-ist-02 slovak–kashubian 54.0 76.0 CMU-03
dutch–north-frisian 32.0 46.0 CMU-03 slovene–old-saxon 10.6 53.2 CMU-03
dutch–west-frisian 38.0 51.0 it-ist-02 sorani–irish 27.6 66.3 CMU-03
dutch–yiddish 78.0 64.0 it-ist-01 spanish–friulian 53.0 81.0 CMU-03
english–murrinhpatha 22.0 42.0 it-ist-02 spanish–occitan 57.0 78.0 CMU-03
english–north-frisian 31.0 42.0 CMU-03 swahili–quechua 13.9 92.1 CMU-03
english–west-frisian 35.0 43.0 CMU-03 turkish–azeri 80.0 87.0 it-ist-02
estonian–ingrian 30.0 44.0 it-ist-02 turkish–crimean-tatar 83.0 89.0 CMU-03 / it-ist-02
estonian–karelian 74.0 68.0 it-ist-01 turkish–kazakh 76.0 86.0 it-ist-02
estonian–livonian 36.0 40.0 it-ist-02 turkish–khakas 76.0 94.0 it-ist-01
estonian–votic 25.0 35.0 it-ist-01 turkish–tatar 73.0 83.0 it-ist-02
finnish–ingrian 54.0 48.0 it-ist-02 turkish–turkmen 86.0 98.0 it-ist-01
finnish–karelian 70.0 78.0 it-ist-01 urdu–bengali 49.0 67.0 CMU-03
finnish–livonian 22.0 34.0 CMU-03 / it-ist-01 urdu–old-english 20.8 40.3 CMU-03
finnish–votic 42.0 40.0 it-ist-02 uzbek–azeri 57.0 70.0 CMU-03
french–occitan 50.0 80.0 CMU-03 uzbek–crimean-tatar 67.0 67.0 CMU-03
german–middle-high-german 72.0 82.0 CMU-03 uzbek–kazakh 84.0 72.0 CMU-03
german–middle-low-german 42.0 52.0 it-ist-02 uzbek–khakas 86.0 92.0 it-ist-01
german–yiddish 77.0 68.0 it-ist-01 uzbek–tatar 69.0 72.0 CMU-03
greek–bengali 51.0 67.0 CMU-03 uzbek–turkmen 80.0 78.0 CMU-03
hebrew–classical-syriac 89.0 95.0 CMU-03 welsh–breton 45.0 86.0 CMU-03
hebrew–maltese 37.0 47.0 CMU-03 welsh–cornish 22.0 42.0 it-ist-01
hindi–bengali 54.0 68.0 CMU-03 welsh–old-irish 6.0 6.0 CMU-03
hungarian–ingrian 12.0 40.0 it-ist-01 welsh–scottish-gaelic 40.0 64.0 CMU-03
hungarian–karelian 62.0 70.0 it-ist-02 zulu–swahili 44.0 81.0 CMU-03

Table 3: Task 1 Accuracy scores



234

HRL–LRL Baseline Best Team HRL–LRL Baseline Best Team

adyghe–kabardian 0.04 0.03 Tuebingen-02 hungarian–livonian 2.56 1.81 it-ist-02
albanian–breton 1.30 0.44 it-ist-02 hungarian–votic 2.47 1.11 it-ist-01
arabic–classical-syriac 0.46 0.10 CMU-03 irish–breton 1.57 0.38 CMU-03
arabic–maltese 1.42 1.37 CMU-03 irish–cornish 2.00 1.56 it-ist-01
arabic–turkmen 0.46 0.32 CMU-03 irish–old-irish 3.30 3.12 it-ist-02
armenian–kabardian 0.21 0.14 CMU-03 / it-ist-01 irish–scottish-gaelic 0.96 1.06 CMU-03
asturian–occitan 1.74 0.80 it-ist-01 italian–friulian 1.03 0.72 it-ist-02
bashkir–azeri 1.64 0.69 it-ist-02 italian–ladin 0.79 0.60 CMU-03
bashkir–crimean-tatar 0.39 0.42 CMU-03 italian–maltese 1.39 1.23 CMU-03
bashkir–kazakh 0.32 0.10 it-ist-01 italian–neapolitan 0.40 0.36 it-ist-02
bashkir–khakas 0.18 0.04 it-ist-02 kannada–telugu 0.60 0.14 CMU-03
bashkir–tatar 0.46 0.33 CMU-03 kurmanji–sorani 2.56 0.65 CMU-03
bashkir–turkmen 0.10 0.12 it-ist-01 latin–czech 2.77 1.14 CMU-03
basque–kashubian 1.16 0.42 CMU-03 latvian–lithuanian 2.21 1.69 CMU-03
belarusian–old-irish 3.90 3.14 CMU-03 latvian–scottish-gaelic 1.16 1.00 CMU-03
bengali–greek 2.86 0.59 CMU-03 persian–azeri 1.35 0.74 CMU-03
bulgarian–old-church-slavonic 1.14 1.06 CMU-03 persian–pashto 1.70 1.54 CMU-03
czech–kashubian 0.84 0.36 CMU-03 polish–kashubian 0.34 0.34 CMU-03
czech–latin 2.95 1.36 CMU-03 polish–old-church-slavonic 1.22 0.96 CMU-03
danish–middle-high-german 0.50 0.38 it-ist-02 portuguese–russian 1.70 1.16 CMU-03
danish–middle-low-german 1.44 1.26 it-ist-01 romanian–latin 3.05 1.35 CMU-03
danish–north-frisian 2.78 2.11 CMU-03 russian–old-church-slavonic 1.33 0.86 CMU-03
danish–west-frisian 1.57 1.27 it-ist-02 russian–portuguese 1.04 0.66 CMU-03
danish–yiddish 0.91 0.72 Tuebingen-01 sanskrit–bengali 1.79 1.13 CMU-03
dutch–middle-high-german 0.44 0.36 it-ist-02 sanskrit–pashto 1.54 1.27 it-ist-02
dutch–middle-low-german 1.34 1.16 it-ist-02 slovak–kashubian 0.60 0.34 CMU-03
dutch–north-frisian 2.67 1.99 CMU-03 slovene–old-saxon 2.23 1.14 CMU-03
dutch–west-frisian 2.18 1.18 it-ist-02 sorani–irish 2.40 0.99 CMU-03
dutch–yiddish 0.53 0.72 Tuebingen-01 spanish–friulian 1.01 0.61 CMU-03
english–murrinhpatha 1.68 1.10 it-ist-02 spanish–occitan 1.14 0.57 it-ist-01
english–north-frisian 2.73 2.22 it-ist-02 swahili–quechua 3.90 0.56 CMU-03
english–west-frisian 1.48 1.26 it-ist-02 turkish–azeri 0.35 0.22 it-ist-01
estonian–ingrian 1.56 1.24 it-ist-02 turkish–crimean-tatar 0.24 0.14 CMU-03
estonian–karelian 0.52 0.62 it-ist-02 turkish–kazakh 0.34 0.16 it-ist-02
estonian–livonian 1.87 1.47 it-ist-02 turkish–khakas 0.80 0.06 it-ist-01
estonian–votic 1.55 1.17 it-ist-02 turkish–tatar 0.37 0.21 it-ist-02
finnish–ingrian 1.08 1.20 it-ist-02 turkish–turkmen 0.24 0.02 it-ist-01
finnish–karelian 0.64 0.42 it-ist-01 urdu–bengali 1.12 0.98 CMU-03
finnish–livonian 2.48 1.71 it-ist-01 urdu–old-english 1.72 1.20 CMU-03
finnish–votic 1.25 1.02 it-ist-02 uzbek–azeri 1.23 0.70 CMU-03
french–occitan 1.22 0.69 it-ist-01 uzbek–crimean-tatar 0.49 0.45 CMU-03
german–middle-high-german 0.44 0.32 it-ist-02 uzbek–kazakh 0.20 0.32 CMU-03
german–middle-low-german 1.24 1.16 it-ist-02 uzbek–khakas 0.24 0.18 it-ist-01
german–yiddish 0.46 0.72 Tuebingen-01 uzbek–tatar 0.48 0.35 CMU-03
greek–bengali 1.21 1.02 CMU-03 uzbek–turkmen 0.32 0.42 CMU-03
hebrew–classical-syriac 0.14 0.06 CMU-03 welsh–breton 0.90 0.31 CMU-03
hebrew–maltese 1.24 1.10 CMU-03 welsh–cornish 2.44 1.50 it-ist-01
hindi–bengali 1.18 0.72 UAlberta-02 welsh–old-irish 3.36 3.08 CMU-03
hungarian–ingrian 2.60 1.46 it-ist-01 welsh–scottish-gaelic 1.22 1.08 CMU-03
hungarian–karelian 0.90 0.50 it-ist-01 zulu–swahili 1.24 0.33 CMU-03

Table 4: Task 1 Levenshtein scores



235

The Tuebingen University submission (Tuebin-
gen) aligned source and target to learn a set of edit-
actions with both linear and neural classifiers that
independently learned to predict action sequences
for each morphological category. Adding in the
cross-lingual data only led to modest gains.

AX-Semantics combined the low- and high-
resource data to train an encoder-decoder seq2seq
model; optionally also implementing domain adap-
tation methods to focus later epochs on the target
language.

The CMU submission first attends over a decou-
pled representation of the desired morphological
sequence before using the updated decoder state to
attend over the character sequence of the lemma.
Secondly, in order to reduce the bias of the de-
coder’s language model, they hallucinate two types
of data that encourage common affixes and charac-
ter copying. Simply allowing the model to learn to
copy characters for several epochs significantly out-
performs the task baseline, while further improve-
ments are obtained through fine-tuning. Making
use of an adversarial language discriminator, cross
lingual gains are highly-correlated to linguistic sim-
ilarity, while augmenting the data with hallucinated
forms and multiple related target language further
improves the model.

The system from IT-IST also attends separately
to tags and lemmas, using a gating mechanism to
interpolate the importance of the individual atten-
tions. By combining the gated dual-head attention
with a SparseMax activation function, they are able
to jointly learn stem and affix modifications, im-
proving significantly over the baseline system.

The relative system performance is described
in Table 5, which shows the average per-language
accuracy of each system. The table reflects the fact
that some teams submitted more than one system
(e.g. Tuebingen-1 & Tuebingen-2 in the table).

5.2 Task 2 Results

Nine teams submitted system papers for Task 2,
with several interesting modifications to either the
baseline or other prior work that led to modest
improvements.

Charles-Saarland achieved the highest overall
tagging accuracy by leveraging multi-lingual BERT
embeddings fine-tuned on a concatenation of all
available languages, effectively transporting the
cross-lingual objective of Task 1 into Task 2. Lem-
mas and tags are decoded separately (with a joint

encoder and separate attention); Lemmas are a se-
quence of edit-actions, while tags are calculated
jointly. (There is no splitting of tags into features;
tags are atomic.)

CBNU instead lemmatize using a transformer
network, while performing tagging with a mul-
tilayer perceptron with biaffine attention. Input
words are first lemmatized, and then pipelined to
the tagger, which produces atomic tag sequences
(i.e., no splitting of features).

The team from Istanbul Technical University
(ITU) jointly produces lemmatic edit-actions and
morphological tags via a two level encoder (first
word embeddings, and then context embeddings)
and separate decoders. Their system slightly im-
proves over the baseline lemmatization, but signifi-
cantly improves tagging accuracy.

The team from the University of Groningen
(RUG) also uses separate decoders for lemmati-
zation and tagging, but uses ELMo to initialize the
contextual embeddings, leading to large gains in
performance. Furthermore, joint training on related
languages further improves results.

CMU approaches tagging differently than the
multi-task decoding we’ve seen so far (baseline is
used for lemmatization). Making use of a hierar-
chical CRF that first predicts POS (that is subse-
quently looped back into the encoder), they then
seek to predict each feature separately. In partic-
ular, predicting POS separately greatly improves
results. An attempt to leverage gold typological
information led to little gain in the results; experi-
ments suggest that the system is already learning
the pertinent information.

The team from Ohio State University
(OHIOSTATE) concentrates on predicting
tags; the baseline lemmatizer is used for lemma-
tization. To that end, they make use of a dual
decoder that first predicts features given only the
word embedding as input; the predictions are fed to
a GRU seq2seq, which then predicts the sequence
of tags.

The UNT HiLT+Ling team investigates a low-
resource setting of the tagging, by using parallel
Bible data to learn a translation matrix between
English and the target language, learning morpho-
logical tags through analogy with English.

The UFAL-Prague team extends their submis-
sion from the UD shared task (multi-layer LSTM),
replacing the pretrained embeddings with BERT,
to great success (first in lemmatization, 2nd in tag-



236

Team Lemma Accuracy Lemma Levenshtein Morph Accuracy Morph F1

CBNU-01† 94.07 0.13 88.09 91.84
CHARLES-MALTA-01 74.95 0.62 50.37 58.81
CHARLES-SAARLAND-02† 95.00 0.11 93.23 96.02
CMU-02 92.20 0.17 85.06 88.97
CMU-DataAug-01‡ 92.51 0.17 86.53 91.18
Edinburgh-01 94.20 0.13 88.93 92.89
ITU-01 94.46 0.11 86.67 90.54
NLPCUBE-01 91.43 2.43 84.92 88.67
OHIOSTATE-01 93.43 0.17 87.42 92.51
RUG-01† 93.91 0.14 90.53 94.54
RUG-02 93.06 0.15 88.80 93.22
UFALPRAGUE-01† 95.78 0.10 93.19 95.92
UNTHILTLING-02† 83.14 0.55 15.69 51.87
EDINBURGH-02* 97.35 0.06 93.02 95.94
CMU-Monolingual* 88.31 0.27 84.60 91.18
CMU-PolyGlot-01*† 76.81 0.54 60.98 75.42
Baseline 94.17 0.13 73.16 87.92

Table 5: Task 2 Team Scores, averaged across all treebanks; * indicates submissions were only applied to a subset
of languages, making scores incomparable. † indicates that additional external resources were used for training,
and ‡ indicates that training data were shared across languages or treebanks.

ging). Although they predict complete tags, they
use the individual features to regularize the decoder.
Small gains are also obtained from joining multi-
lingual corpora and ensembling.

CUNI–Malta performs lemmatization as oper-
ations over edit actions with LSTM and ReLU.
Tagging is a bidirectional LSTM augmented by the
edit actions (i.e., two-stage decoding), predicting
features separately.

The Edinburgh system is a character-based
LSTM encoder-decoder with attention, imple-
mented in OpenNMT. It can be seen as an extension
of the contextual lemmatization system Lematus
(Bergmanis and Goldwater, 2018) to include mor-
phological tagging, or alternatively as an adaptation
of the morphological re-inflection system MED
(Kann and Schütze, 2016) to incorporate context
and perform analysis rather than re-inflection. Like
these systems it uses a completely generic encoder-
decoder architecture with no specific adaptation
to the morphological processing task other than
the form of the input. In the submitted version
of the system, the input is split into short chunks
corresponding to the target word plus one word of
context on either side, and the system is trained to
output the corresponding lemmas and tags for each
three-word chunk.

Several teams relied on external resources to

improve their lemmatization and feature analysis.
Several teams made use of pre-trained embeddings.
CHARLES-SAARLAND-2 and UFALPRAGUE-
1 used pretrained contextual embeddings (BERT)
provided by Google (Devlin et al., 2019). CBNU-
1 used a mix of pre-trained embeddings from the
CoNLL 2017 shared task and fastText. Further,
some teams trained their own embeddings to aid
performance.

6 Future Directions

In general, the application of typology to natu-
ral language processing (e.g., Gerz et al., 2018;
Ponti et al., 2018) provides an interesting avenue
for multilinguality. Further, our shared task was
designed to only leverage a single helper language,
though many may exist with lexical or morpholog-
ical overlap with the target language. Techniques
like those of Neubig and Hu (2018) may aid in de-
signing universal inflection architectures. Neither
task this year included unannotated monolingual
corpora. Using such data is well-motivated from
an L1-learning point of view, and may affect the
performance of low-resource data settings.

In the case of inflection an interesting future
topic could involve departing from orthographic
representation and using more IPA-like representa-
tions, i.e. transductions over pronunciations. Differ-



237

L
an

gu
ag

e
(T

re
eb

an
k)

B
as

el
in

e
B

es
t

Te
am

L
an

gu
ag

e
(T

re
eb

an
k)

B
as

el
in

e
B

es
t

Te
am

U
D

A
fr

ik
aa

ns
-A

fr
iB

oo
m

s
98

.4
1

99
.1

5
U

FA
L

PR
A

G
U

E
-0

1
U

D
It

al
ia

n-
Po

ST
W

IT
A

95
.6

0
97

.9
5

U
FA

L
PR

A
G

U
E

-0
1

U
D

A
kk

ad
ia

n-
PI

SA
N

D
U

B
66

.8
3

67
.8

2
C

B
N

U
-0

1
/E

D
IN

B
U

R
G

H
-0

1
U

D
It

al
ia

n-
PU

D
95

.5
9

98
.0

6
U

FA
L

PR
A

G
U

E
-0

1
U

D
A

m
ha

ri
c-

A
T

T
98

.6
8

10
0.

00
M

ul
tip

le
U

D
Ja

pa
ne

se
-G

SD
97

.7
1

99
.6

5
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
U

D
A

nc
ie

nt
G

re
ek

-P
er

se
us

94
.4

4
95

.2
4

E
D

IN
B

U
R

G
H

-0
1

U
D

Ja
pa

ne
se

-M
od

er
n

94
.2

0
98

.6
7

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

A
nc

ie
nt

G
re

ek
-P

R
O

IE
L

96
.6

8
97

.4
9

E
D

IN
B

U
R

G
H

-0
1

U
D

Ja
pa

ne
se

-P
U

D
95

.7
5

99
.3

6
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
U

D
A

ra
bi

c-
PA

D
T

94
.4

9
96

.0
8

U
FA

L
PR

A
G

U
E

-0
1

U
D

K
om

i
Z

yr
ia

n-
IK

D
P

78
.9

1
89

.8
4

R
U

G
-0

2
U

D
A

ra
bi

c-
PU

D
85

.2
4

87
.1

3
E

D
IN

B
U

R
G

H
-0

1
U

D
K

om
i

Z
yr

ia
n-

L
at

tic
e

82
.9

7
87

.9
1

U
FA

L
PR

A
G

U
E

-0
1

U
D

A
rm

en
ia

n-
A

rm
T

D
P

95
.3

9
95

.9
6

U
FA

L
PR

A
G

U
E

-0
1

U
D

K
or

ea
n-

G
SD

92
.2

5
94

.2
1

U
FA

L
PR

A
G

U
E

-0
1

U
D

B
am

ba
ra

-C
R

B
87

.0
2

92
.7

1
U

FA
L

PR
A

G
U

E
-0

1
U

D
K

or
ea

n-
K

ai
st

94
.6

1
95

.7
8

E
D

IN
B

U
R

G
H

-0
1

U
D

B
as

qu
e-

B
D

T
96

.0
7

97
.1

9
U

FA
L

PR
A

G
U

E
-0

1
U

D
K

or
ea

n-
PU

D
96

.4
1

99
.5

7
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
U

D
B

el
ar

us
ia

n-
H

SE
89

.7
0

92
.5

1
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
U

D
K

ur
m

an
ji-

M
G

92
.2

9
94

.8
0

U
FA

L
PR

A
G

U
E

-0
1

U
D

B
re

to
n-

K
E

B
93

.5
3

93
.8

3
O

H
IO

ST
A

T
E

-0
1

U
D

L
at

in
-I

T
T

B
98

.1
7

99
.2

0
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
U

D
B

ul
ga

ri
an

-B
T

B
97

.3
7

98
.3

6
U

FA
L

PR
A

G
U

E
-0

1
U

D
L

at
in

-P
er

se
us

89
.5

4
93

.4
9

U
FA

L
PR

A
G

U
E

-0
1

U
D

B
ur

ya
t-

B
D

T
88

.5
6

90
.1

9
U

FA
L

PR
A

G
U

E
-0

1
U

D
L

at
in

-P
R

O
IE

L
96

.4
1

97
.3

7
U

FA
L

PR
A

G
U

E
-0

1
U

D
C

an
to

ne
se

-H
K

91
.6

1
10

0.
00

M
ul

tip
le

U
D

L
at

vi
an

-L
V

T
B

95
.5

9
97

.2
3

U
FA

L
PR

A
G

U
E

-0
1

U
D

C
at

al
an

-A
nC

or
a

98
.0

7
99

.3
8

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

L
ith

ua
ni

an
-H

SE
86

.4
2

87
.4

4
O

H
IO

ST
A

T
E

-0
1

U
D

C
hi

ne
se

-C
FL

93
.2

6
99

.7
6

C
B

N
U

-0
1

/U
FA

L
PR

A
G

U
E

-0
1

U
D

M
ar

at
hi

-U
FA

L
75

.6
1

76
.6

9
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
U

D
C

hi
ne

se
-G

SD
98

.4
4

99
.9

8
C

B
N

U
-0

1
/C

M
U

-0
2

/U
FA

L
PR

A
G

U
E

-0
1

U
D

N
ai

ja
-N

SC
99

.3
3

10
0.

00
M

ul
tip

le
U

D
C

op
tic

-S
cr

ip
to

ri
um

95
.8

0
97

.3
1

U
FA

L
PR

A
G

U
E

-0
1

U
D

N
or

th
Sa

m
i-

G
ie

lla
93

.0
4

93
.4

7
O

H
IO

ST
A

T
E

-0
1

U
D

C
ro

at
ia

n-
SE

T
95

.3
2

97
.5

2
U

FA
L

PR
A

G
U

E
-0

1
U

D
N

or
w

eg
ia

n-
B

ok
m

aa
l

98
.0

0
99

.1
9

U
FA

L
PR

A
G

U
E

-0
1

U
D

C
ze

ch
-C

A
C

97
.8

2
99

.4
5

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

N
or

w
eg

ia
n-

N
yn

or
sk

97
.8

5
99

.0
0

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

C
ze

ch
-C

LT
T

98
.2

1
99

.4
7

U
FA

L
PR

A
G

U
E

-0
1

U
D

N
or

w
eg

ia
n-

N
yn

or
sk

L
IA

96
.6

6
98

.2
2

U
FA

L
PR

A
G

U
E

-0
1

U
D

C
ze

ch
-F

ic
Tr

ee
97

.6
6

99
.0

1
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
U

D
O

ld
C

hu
rc

h
Sl

av
on

ic
-P

R
O

IE
L

96
.3

8
97

.2
3

E
D

IN
B

U
R

G
H

-0
1

U
D

C
ze

ch
-P

D
T

96
.0

6
99

.4
2

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

Pe
rs

ia
n-

Se
ra

ji
96

.0
8

96
.8

9
U

FA
L

PR
A

G
U

E
-0

1
U

D
C

ze
ch

-P
U

D
93

.5
8

98
.1

3
U

FA
L

PR
A

G
U

E
-0

1
U

D
Po

lis
h-

L
FG

95
.8

2
97

.9
4

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

D
an

is
h-

D
D

T
96

.1
6

98
.3

3
U

FA
L

PR
A

G
U

E
-0

1
U

D
Po

lis
h-

SZ
95

.1
8

97
.4

3
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
U

D
D

ut
ch

-A
lp

in
o

97
.3

5
98

.6
2

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

Po
rt

ug
ue

se
-B

os
qu

e
97

.0
8

98
.6

9
U

FA
L

PR
A

G
U

E
-0

1
U

D
D

ut
ch

-L
as

sy
Sm

al
l

96
.6

3
98

.2
1

U
FA

L
PR

A
G

U
E

-0
1

U
D

Po
rt

ug
ue

se
-G

SD
93

.7
0

99
.1

1
U

FA
L

PR
A

G
U

E
-0

1
U

D
E

ng
lis

h-
E

W
T

97
.6

8
99

.1
9

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

R
om

an
ia

n-
N

on
st

an
da

rd
95

.8
6

96
.7

4
U

FA
L

PR
A

G
U

E
-0

1
U

D
E

ng
lis

h-
G

U
M

97
.4

1
98

.6
3

U
FA

L
PR

A
G

U
E

-0
1

U
D

R
om

an
ia

n-
R

R
T

96
.9

4
98

.6
0

U
FA

L
PR

A
G

U
E

-0
1

U
D

E
ng

lis
h-

L
in

E
S

98
.0

0
98

.6
2

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

R
us

si
an

-G
SD

95
.6

7
97

.7
7

U
FA

L
PR

A
G

U
E

-0
1

U
D

E
ng

lis
h-

Pa
rT

U
T

97
.6

6
98

.5
2

U
FA

L
PR

A
G

U
E

-0
1

U
D

R
us

si
an

-P
U

D
91

.8
5

95
.7

6
U

FA
L

PR
A

G
U

E
-0

1
U

D
E

ng
lis

h-
PU

D
95

.2
9

97
.8

9
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
U

D
R

us
si

an
-S

yn
Ta

gR
us

95
.9

2
99

.0
1

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

E
st

on
ia

n-
E

D
T

94
.8

4
97

.0
9

E
D

IN
B

U
R

G
H

-0
1

U
D

R
us

si
an

-T
ai

ga
89

.8
6

10
0.

00
U

N
T

H
IL

T
L

IN
G

-0
2

U
D

Fa
ro

es
e-

O
FT

88
.8

6
89

.5
3

U
FA

L
PR

A
G

U
E

-0
1

U
D

Sa
ns

kr
it-

U
FA

L
64

.3
2

67
.3

4
C

M
U

-M
on

ol
in

gu
al

-0
1

U
D

Fi
nn

is
h-

FT
B

94
.8

8
96

.6
4

E
D

IN
B

U
R

G
H

-0
2

U
D

Se
rb

ia
n-

SE
T

96
.7

2
98

.1
9

U
FA

L
PR

A
G

U
E

-0
1

U
D

Fi
nn

is
h-

PU
D

88
.2

7
89

.9
8

U
FA

L
PR

A
G

U
E

-0
1

U
D

Sl
ov

ak
-S

N
K

96
.1

4
97

.5
7

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

Fi
nn

is
h-

T
D

T
95

.5
3

96
.6

0
U

FA
L

PR
A

G
U

E
-0

1
U

D
Sl

ov
en

ia
n-

SS
J

96
.4

3
98

.8
7

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

Fr
en

ch
-G

SD
97

.9
7

99
.0

1
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
U

D
Sl

ov
en

ia
n-

SS
T

94
.0

6
97

.2
0

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

Fr
en

ch
-P

ar
T

U
T

95
.6

9
96

.6
6

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

Sp
an

is
h-

A
nC

or
a

98
.5

4
99

.4
6

U
FA

L
PR

A
G

U
E

-0
1

U
D

Fr
en

ch
-S

eq
uo

ia
97

.6
7

99
.0

1
U

FA
L

PR
A

G
U

E
-0

1
U

D
Sp

an
is

h-
G

SD
98

.4
2

99
.3

0
U

FA
L

PR
A

G
U

E
-0

1
U

D
Fr

en
ch

-S
po

ke
n

97
.9

8
99

.5
2

po
st

de
ad

lin
e

R
U

G
-0

1
U

D
Sw

ed
is

h-
L

in
E

S
95

.8
5

98
.3

0
U

FA
L

PR
A

G
U

E
-0

1
U

D
G

al
ic

ia
n-

C
T

G
98

.2
2

98
.9

6
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
U

D
Sw

ed
is

h-
PU

D
93

.1
2

96
.6

3
U

FA
L

PR
A

G
U

E
-0

1
U

D
G

al
ic

ia
n-

Tr
ee

G
al

96
.1

8
98

.6
5

U
FA

L
PR

A
G

U
E

-0
1

U
D

Sw
ed

is
h-

Ta
lb

an
ke

n
97

.2
3

98
.6

2
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
U

D
G

er
m

an
-G

SD
96

.2
6

97
.6

5
IT

U
-0

1
U

D
Ta

ga
lo

g-
T

R
G

78
.3

8
91

.8
9

M
ul

tip
le

U
D

G
ot

hi
c-

PR
O

IE
L

96
.5

3
97

.0
3

E
D

IN
B

U
R

G
H

-0
1

U
D

Ta
m

il-
T

T
B

93
.8

6
96

.4
3

U
FA

L
PR

A
G

U
E

-0
1

U
D

G
re

ek
-G

D
T

96
.7

6
97

.2
4

E
D

IN
B

U
R

G
H

-0
1

U
D

Tu
rk

is
h-

IM
ST

96
.4

1
96

.8
4

U
FA

L
PR

A
G

U
E

-0
1

U
D

H
eb

re
w

-H
T

B
96

.7
2

98
.1

7
U

FA
L

PR
A

G
U

E
-0

1
U

D
Tu

rk
is

h-
PU

D
86

.0
2

89
.0

3
U

FA
L

PR
A

G
U

E
-0

1
U

D
H

in
di

-H
D

T
B

98
.6

0
98

.8
7

U
FA

L
PR

A
G

U
E

-0
1

U
D

U
kr

ai
ni

an
-I

U
95

.5
3

97
.8

5
U

FA
L

PR
A

G
U

E
-0

1
U

D
H

un
ga

ri
an

-S
ze

ge
d

95
.1

7
97

.4
7

U
FA

L
PR

A
G

U
E

-0
1

U
D

U
pp

er
So

rb
ia

n-
U

FA
L

91
.6

9
93

.7
4

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

In
do

ne
si

an
-G

SD
99

.3
7

99
.6

1
U

FA
L

PR
A

G
U

E
-0

1
U

D
U

rd
u-

U
D

T
B

96
.1

9
96

.9
8

U
FA

L
PR

A
G

U
E

-0
1

U
D

Ir
is

h-
ID

T
91

.6
9

92
.0

2
O

H
IO

ST
A

T
E

-0
1

U
D

V
ie

tn
am

es
e-

V
T

B
99

.7
9

10
0.

00
C

M
U

-0
2

/U
N

T
H

IL
T

L
IN

G
-0

2
U

D
It

al
ia

n-
IS

D
T

97
.3

8
98

.8
8

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

/U
FA

L
PR

A
G

U
E

-0
1

U
D

Y
or

ub
a-

Y
T

B
98

.8
4

98
.8

4
M

ul
tip

le
U

D
It

al
ia

n-
Pa

rT
U

T
96

.8
4

98
.8

7
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2

Table 6: Task 2 Lemma Accuracy scores



238

L
an

gu
ag

e
(T

re
eb

an
k)

B
as

el
in

e
B

es
t

Te
am

L
an

gu
ag

e
(T

re
eb

an
k)

B
as

el
in

e
B

es
t

Te
am

U
D

A
fr

ik
aa

ns
-A

fr
iB

oo
m

s
0.

03
0.

02
M

ul
tip

le
U

D
It

al
ia

n-
Po

ST
W

IT
A

0.
11

0.
05

U
FA

L
PR

A
G

U
E

-0
1

U
D

A
kk

ad
ia

n-
PI

SA
N

D
U

B
0.

87
0.

85
O

H
IO

ST
A

T
E

-0
1

U
D

It
al

ia
n-

PU
D

0.
08

0.
04

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

/U
FA

L
PR

A
G

U
E

-0
1

U
D

A
m

ha
ri

c-
A

T
T

0.
02

0.
00

M
ul

tip
le

U
D

Ja
pa

ne
se

-G
SD

0.
04

0.
01

M
ul

tip
le

U
D

A
nc

ie
nt

G
re

ek
-P

er
se

us
0.

14
0.

12
E

D
IN

B
U

R
G

H
-0

1
U

D
Ja

pa
ne

se
-M

od
er

n
0.

07
0.

01
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
U

D
A

nc
ie

nt
G

re
ek

-P
R

O
IE

L
0.

08
0.

06
E

D
IN

B
U

R
G

H
-0

1
/E

D
IN

B
U

R
G

H
-0

2
U

D
Ja

pa
ne

se
-P

U
D

0.
07

0.
01

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

/U
FA

L
PR

A
G

U
E

-0
1

U
D

A
ra

bi
c-

PA
D

T
0.

16
0.

11
U

FA
L

PR
A

G
U

E
-0

1
U

D
K

om
i

Z
yr

ia
n-

IK
D

P
0.

38
0.

23
R

U
G

-0
1

/R
U

G
-0

2
U

D
A

ra
bi

c-
PU

D
0.

41
0.

37
E

D
IN

B
U

R
G

H
-0

1
U

D
K

om
i

Z
yr

ia
n-

L
at

tic
e

0.
34

0.
25

U
FA

L
PR

A
G

U
E

-0
1

U
D

A
rm

en
ia

n-
A

rm
T

D
P

0.
08

0.
07

U
FA

L
PR

A
G

U
E

-0
1

U
D

K
or

ea
n-

G
SD

0.
18

0.
11

M
ul

tip
le

U
D

B
am

ba
ra

-C
R

B
0.

27
0.

10
U

FA
L

PR
A

G
U

E
-0

1
U

D
K

or
ea

n-
K

ai
st

0.
09

0.
06

E
D

IN
B

U
R

G
H

-0
1

U
D

B
as

qu
e-

B
D

T
0.

09
0.

06
U

FA
L

PR
A

G
U

E
-0

1
U

D
K

or
ea

n-
PU

D
0.

06
0.

01
M

ul
tip

le
U

D
B

el
ar

us
ia

n-
H

SE
0.

17
0.

12
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
U

D
K

ur
m

an
ji-

M
G

0.
39

0.
10

U
FA

L
PR

A
G

U
E

-0
1

U
D

B
re

to
n-

K
E

B
0.

16
0.

13
IT

U
-0

1
U

D
L

at
in

-I
T

T
B

0.
04

0.
02

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

/U
FA

L
PR

A
G

U
E

-0
1

U
D

B
ul

ga
ri

an
-B

T
B

0.
07

0.
05

IT
U

-0
1

/U
FA

L
PR

A
G

U
E

-0
1

U
D

L
at

in
-P

er
se

us
0.

21
0.

13
U

FA
L

PR
A

G
U

E
-0

1
U

D
B

ur
ya

t-
B

D
T

0.
27

0.
22

U
FA

L
PR

A
G

U
E

-0
1

U
D

L
at

in
-P

R
O

IE
L

0.
08

0.
05

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

C
an

to
ne

se
-H

K
0.

28
0.

00
M

ul
tip

le
U

D
L

at
vi

an
-L

V
T

B
0.

07
0.

05
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
/U

FA
L

PR
A

G
U

E
-0

1
U

D
C

at
al

an
-A

nC
or

a
0.

04
0.

01
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
/U

FA
L

PR
A

G
U

E
-0

1
U

D
L

ith
ua

ni
an

-H
SE

0.
25

0.
24

U
FA

L
PR

A
G

U
E

-0
1

U
D

C
hi

ne
se

-C
FL

0.
10

0.
01

N
L

PC
U

B
E

-0
1

U
D

M
ar

at
hi

-U
FA

L
0.

86
0.

57
C

M
U

-M
on

ol
in

gu
al

-0
1

U
D

C
hi

ne
se

-G
SD

0.
02

0.
01

M
ul

tip
le

U
D

N
ai

ja
-N

SC
0.

01
0.

00
M

ul
tip

le
U

D
C

op
tic

-S
cr

ip
to

ri
um

0.
09

0.
06

U
FA

L
PR

A
G

U
E

-0
1

U
D

N
or

th
Sa

m
i-

G
ie

lla
0.

14
0.

13
E

D
IN

B
U

R
G

H
-0

1
/O

H
IO

ST
A

T
E

-0
1

U
D

C
ro

at
ia

n-
SE

T
0.

09
0.

05
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
/U

FA
L

PR
A

G
U

E
-0

1
U

D
N

or
w

eg
ia

n-
B

ok
m

aa
l

0.
03

0.
01

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

/U
FA

L
PR

A
G

U
E

-0
1

U
D

C
ze

ch
-C

A
C

0.
05

0.
01

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

/U
FA

L
PR

A
G

U
E

-0
1

U
D

N
or

w
eg

ia
n-

N
yn

or
sk

0.
04

0.
01

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

C
ze

ch
-C

LT
T

0.
04

0.
01

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

/U
FA

L
PR

A
G

U
E

-0
1

U
D

N
or

w
eg

ia
n-

N
yn

or
sk

L
IA

0.
08

0.
03

U
FA

L
PR

A
G

U
E

-0
1

U
D

C
ze

ch
-F

ic
Tr

ee
0.

04
0.

02
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
/U

FA
L

PR
A

G
U

E
-0

1
U

D
O

ld
C

hu
rc

h
Sl

av
on

ic
-P

R
O

IE
L

0.
08

0.
06

E
D

IN
B

U
R

G
H

-0
1

U
D

C
ze

ch
-P

D
T

0.
06

0.
01

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

/U
FA

L
PR

A
G

U
E

-0
1

U
D

Pe
rs

ia
n-

Se
ra

ji
0.

19
0.

15
U

FA
L

PR
A

G
U

E
-0

1
U

D
C

ze
ch

-P
U

D
0.

10
0.

03
U

FA
L

PR
A

G
U

E
-0

1
U

D
Po

lis
h-

L
FG

0.
08

0.
04

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

/U
FA

L
PR

A
G

U
E

-0
1

U
D

D
an

is
h-

D
D

T
0.

06
0.

03
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
/U

FA
L

PR
A

G
U

E
-0

1
U

D
Po

lis
h-

SZ
0.

08
0.

04
U

FA
L

PR
A

G
U

E
-0

1
U

D
D

ut
ch

-A
lp

in
o

0.
05

0.
03

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

/U
FA

L
PR

A
G

U
E

-0
1

U
D

Po
rt

ug
ue

se
-B

os
qu

e
0.

05
0.

02
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
/U

FA
L

PR
A

G
U

E
-0

1
U

D
D

ut
ch

-L
as

sy
Sm

al
l

0.
06

0.
03

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

/U
FA

L
PR

A
G

U
E

-0
1

U
D

Po
rt

ug
ue

se
-G

SD
0.

18
0.

05
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
/U

FA
L

PR
A

G
U

E
-0

1
U

D
E

ng
lis

h-
E

W
T

0.
12

0.
01

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

R
om

an
ia

n-
N

on
st

an
da

rd
0.

08
0.

06
M

ul
tip

le
U

D
E

ng
lis

h-
G

U
M

0.
05

0.
02

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

/U
FA

L
PR

A
G

U
E

-0
1

U
D

R
om

an
ia

n-
R

R
T

0.
05

0.
02

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

E
ng

lis
h-

L
in

E
S

0.
04

0.
02

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

/U
FA

L
PR

A
G

U
E

-0
1

U
D

R
us

si
an

-G
SD

0.
07

0.
04

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

/U
FA

L
PR

A
G

U
E

-0
1

U
D

E
ng

lis
h-

Pa
rT

U
T

0.
04

0.
02

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

/U
FA

L
PR

A
G

U
E

-0
1

U
D

R
us

si
an

-P
U

D
0.

18
0.

08
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
/U

FA
L

PR
A

G
U

E
-0

1
U

D
E

ng
lis

h-
PU

D
0.

07
0.

03
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
U

D
R

us
si

an
-S

yn
Ta

gR
us

0.
08

0.
02

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

/U
FA

L
PR

A
G

U
E

-0
1

U
D

E
st

on
ia

n-
E

D
T

0.
11

0.
05

E
D

IN
B

U
R

G
H

-0
1

U
D

R
us

si
an

-T
ai

ga
0.

21
0.

00
U

W
T

H
IL

T
L

IN
G

U
D

Fa
ro

es
e-

O
FT

0.
20

0.
18

IT
U

-0
1

U
D

Sa
ns

kr
it-

U
FA

L
0.

85
0.

82
C

M
U

-M
on

ol
in

gu
al

-0
1

U
D

Fi
nn

is
h-

FT
B

0.
11

0.
08

M
ul

tip
le

U
D

Se
rb

ia
n-

SE
T

0.
06

0.
03

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

/U
FA

L
PR

A
G

U
E

-0
1

U
D

Fi
nn

is
h-

PU
D

0.
24

0.
18

U
FA

L
PR

A
G

U
E

-0
1

U
D

Sl
ov

ak
-S

N
K

0.
06

0.
04

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

Fi
nn

is
h-

T
D

T
0.

10
0.

07
U

FA
L

PR
A

G
U

E
-0

1
U

D
Sl

ov
en

ia
n-

SS
J

0.
06

0.
02

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

/U
FA

L
PR

A
G

U
E

-0
1

U
D

Fr
en

ch
-G

SD
0.

04
0.

02
M

ul
tip

le
U

D
Sl

ov
en

ia
n-

SS
T

0.
12

0.
05

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

Fr
en

ch
-P

ar
T

U
T

0.
07

0.
05

R
U

G
-0

2
/p

os
t

de
ad

lin
e

R
U

G
-0

1
U

D
Sp

an
is

h-
A

nC
or

a
0.

03
0.

01
M

ul
tip

le
U

D
Fr

en
ch

-S
eq

uo
ia

0.
05

0.
02

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

/U
FA

L
PR

A
G

U
E

-0
1

U
D

Sp
an

is
h-

G
SD

0.
03

0.
01

M
ul

tip
le

U
D

Fr
en

ch
-S

po
ke

n
0.

04
0.

01
po

st
de

ad
lin

e
R

U
G

-0
1

U
D

Sw
ed

is
h-

L
in

E
S

0.
08

0.
03

U
FA

L
PR

A
G

U
E

-0
1

U
D

G
al

ic
ia

n-
C

T
G

0.
04

0.
02

M
ul

tip
le

U
D

Sw
ed

is
h-

PU
D

0.
10

0.
05

U
FA

L
PR

A
G

U
E

-0
1

U
D

G
al

ic
ia

n-
Tr

ee
G

al
0.

06
0.

03
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
/U

FA
L

PR
A

G
U

E
-0

1
U

D
Sw

ed
is

h-
Ta

lb
an

ke
n

0.
05

0.
02

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

/U
FA

L
PR

A
G

U
E

-0
1

U
D

G
er

m
an

-G
SD

0.
08

0.
04

IT
U

-0
1

U
D

Ta
ga

lo
g-

T
R

G
0.

49
0.

19
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
/I

T
U

-0
1

U
D

G
ot

hi
c-

PR
O

IE
L

0.
07

0.
06

O
H

IO
ST

A
T

E
-0

1
U

D
Ta

m
il-

T
T

B
0.

14
0.

07
U

FA
L

PR
A

G
U

E
-0

1
U

D
G

re
ek

-G
D

T
0.

07
0.

06
E

D
IN

B
U

R
G

H
-0

1
U

D
Tu

rk
is

h-
IM

ST
0.

08
0.

06
E

D
IN

B
U

R
G

H
-0

1
/I

T
U

-0
1

/U
FA

L
PR

A
G

U
E

-0
1

U
D

H
eb

re
w

-H
T

B
0.

06
0.

03
U

FA
L

PR
A

G
U

E
-0

1
U

D
Tu

rk
is

h-
PU

D
0.

34
0.

28
IT

U
-0

1
U

D
H

in
di

-H
D

T
B

0.
02

0.
01

M
ul

tip
le

U
D

U
kr

ai
ni

an
-I

U
0.

10
0.

03
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
U

D
H

un
ga

ri
an

-S
ze

ge
d

0.
10

0.
05

U
FA

L
PR

A
G

U
E

-0
1

U
D

U
pp

er
So

rb
ia

n-
U

FA
L

0.
12

0.
10

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

In
do

ne
si

an
-G

SD
0.

01
0.

01
M

ul
tip

le
U

D
U

rd
u-

U
D

T
B

0.
07

0.
06

M
ul

tip
le

U
D

Ir
is

h-
ID

T
0.

18
0.

16
O

H
IO

ST
A

T
E

-0
1

U
D

V
ie

tn
am

es
e-

V
T

B
0.

02
0.

00
C

M
U

-0
2

/U
N

T
H

IL
T

L
IN

G
U

D
It

al
ia

n-
IS

D
T

0.
05

0.
02

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

/U
FA

L
PR

A
G

U
E

-0
1

U
D

Y
or

ub
a-

Y
T

B
0.

01
0.

01
M

ul
tip

le
U

D
It

al
ia

n-
Pa

rT
U

T
0.

08
0.

02
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2

Table 7: Task 2 Lemma Levenshtein scores



239

L
an

gu
ag

e
(T

re
eb

an
k)

B
as

el
in

e
B

es
t

Te
am

L
an

gu
ag

e
(T

re
eb

an
k)

B
as

el
in

e
B

es
t

Te
am

U
D

A
fr

ik
aa

ns
-A

fr
iB

oo
m

s
84

.9
0

99
.2

3
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
/U

FA
L

PR
A

G
U

E
-0

1
U

D
It

al
ia

n-
Po

ST
W

IT
A

70
.0

9
96

.8
8

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

A
kk

ad
ia

n-
PI

SA
N

D
U

B
78

.2
2

89
.1

1
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
U

D
It

al
ia

n-
PU

D
80

.7
8

96
.3

7
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
U

D
A

m
ha

ri
c-

A
T

T
75

.4
3

89
.7

9
U

FA
L

PR
A

G
U

E
-0

1
U

D
Ja

pa
ne

se
-G

SD
85

.4
7

98
.4

1
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
U

D
A

nc
ie

nt
G

re
ek

-P
er

se
us

69
.8

8
91

.9
4

U
FA

L
PR

A
G

U
E

-0
1

U
D

Ja
pa

ne
se

-M
od

er
n

94
.9

4
97

.4
7

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

A
nc

ie
nt

G
re

ek
-P

R
O

IE
L

84
.5

5
92

.9
4

U
FA

L
PR

A
G

U
E

-0
1

U
D

Ja
pa

ne
se

-P
U

D
84

.3
3

98
.6

3
U

FA
L

PR
A

G
U

E
-0

1
U

D
A

ra
bi

c-
PA

D
T

76
.7

8
95

.6
6

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

K
om

i
Z

yr
ia

n-
IK

D
P

35
.9

4
75

.7
8

U
FA

L
PR

A
G

U
E

-0
1

U
D

A
ra

bi
c-

PU
D

63
.0

7
85

.0
4

U
FA

L
PR

A
G

U
E

-0
1

U
D

K
om

i
Z

yr
ia

n-
L

at
tic

e
45

.0
5

69
.7

8
U

FA
L

PR
A

G
U

E
-0

1
U

D
A

rm
en

ia
n-

A
rm

T
D

P
64

.3
8

93
.3

4
U

FA
L

PR
A

G
U

E
-0

1
U

D
K

or
ea

n-
G

SD
79

.7
3

96
.7

7
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
U

D
B

am
ba

ra
-C

R
B

76
.9

9
93

.9
3

U
FA

L
PR

A
G

U
E

-0
1

U
D

K
or

ea
n-

K
ai

st
84

.3
0

97
.8

5
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
U

D
B

as
qu

e-
B

D
T

67
.7

6
92

.5
2

U
FA

L
PR

A
G

U
E

-0
1

U
D

K
or

ea
n-

PU
D

76
.7

8
94

.6
7

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

B
el

ar
us

ia
n-

H
SE

54
.2

2
89

.9
3

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

K
ur

m
an

ji-
M

G
68

.1
0

85
.5

7
U

FA
L

PR
A

G
U

E
-0

1
U

D
B

re
to

n-
K

E
B

76
.5

2
91

.1
4

U
FA

L
PR

A
G

U
E

-0
1

U
D

L
at

in
-I

T
T

B
77

.6
8

97
.6

4
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
U

D
B

ul
ga

ri
an

-B
T

B
79

.6
4

98
.0

1
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
U

D
L

at
in

-P
er

se
us

55
.0

6
87

.7
6

U
FA

L
PR

A
G

U
E

-0
1

U
D

B
ur

ya
t-

B
D

T
64

.2
3

88
.5

6
U

FA
L

PR
A

G
U

E
-0

1
U

D
L

at
in

-P
R

O
IE

L
82

.1
6

93
.6

8
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
U

D
C

an
to

ne
se

-H
K

68
.5

7
94

.2
9

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

L
at

vi
an

-L
V

T
B

70
.3

3
95

.7
8

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

C
at

al
an

-A
nC

or
a

85
.5

7
98

.8
2

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

L
ith

ua
ni

an
-H

SE
41

.4
3

80
.1

4
U

FA
L

PR
A

G
U

E
-0

1
U

D
C

hi
ne

se
-C

FL
76

.7
1

94
.0

9
U

FA
L

PR
A

G
U

E
-0

1
U

D
M

ar
at

hi
-U

FA
L

40
.1

1
67

.7
5

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

C
hi

ne
se

-G
SD

75
.9

7
97

.1
3

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

N
ai

ja
-N

SC
66

.4
2

96
.5

7
U

FA
L

PR
A

G
U

E
-0

1
U

D
C

op
tic

-S
cr

ip
to

ri
um

87
.7

3
96

.2
2

U
FA

L
PR

A
G

U
E

-0
1

U
D

N
or

th
Sa

m
i-

G
ie

lla
66

.8
7

92
.4

6
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
U

D
C

ro
at

ia
n-

SE
T

71
.4

2
94

.4
2

U
FA

L
PR

A
G

U
E

-0
1

U
D

N
or

w
eg

ia
n-

B
ok

m
aa

l
81

.2
7

98
.2

5
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
U

D
C

ze
ch

-C
A

C
77

.2
6

98
.4

8
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
U

D
N

or
w

eg
ia

n-
N

yn
or

sk
81

.7
5

98
.1

1
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
U

D
C

ze
ch

-C
LT

T
72

.6
0

95
.8

1
U

FA
L

PR
A

G
U

E
-0

1
U

D
N

or
w

eg
ia

n-
N

yn
or

sk
L

IA
74

.2
0

96
.8

0
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
U

D
C

ze
ch

-F
ic

Tr
ee

68
.3

4
97

.1
3

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

O
ld

C
hu

rc
h

Sl
av

on
ic

-P
R

O
IE

L
84

.1
3

93
.0

1
U

FA
L

PR
A

G
U

E
-0

1
U

D
C

ze
ch

-P
D

T
76

.7
0

98
.5

4
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
U

D
Pe

rs
ia

n-
Se

ra
ji

86
.8

4
98

.3
1

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

/U
FA

L
PR

A
G

U
E

-0
1

U
D

C
ze

ch
-P

U
D

60
.6

7
95

.0
3

U
FA

L
PR

A
G

U
E

-0
1

U
D

Po
lis

h-
L

FG
65

.7
2

97
.1

3
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
U

D
D

an
is

h-
D

D
T

77
.2

2
97

.9
8

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

Po
lis

h-
SZ

63
.1

5
95

.1
1

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

D
ut

ch
-A

lp
in

o
82

.0
7

98
.1

2
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
U

D
Po

rt
ug

ue
se

-B
os

qu
e

78
.0

5
96

.2
2

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

D
ut

ch
-L

as
sy

Sm
al

l
76

.7
8

98
.5

0
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
U

D
Po

rt
ug

ue
se

-G
SD

83
.8

7
99

.0
3

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

E
ng

lis
h-

E
W

T
80

.1
7

97
.8

5
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
U

D
R

om
an

ia
n-

N
on

st
an

da
rd

74
.7

1
95

.0
1

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

E
ng

lis
h-

G
U

M
79

.5
7

97
.5

2
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
U

D
R

om
an

ia
n-

R
R

T
81

.6
2

98
.1

9
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
U

D
E

ng
lis

h-
L

in
E

S
80

.3
0

97
.7

7
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
U

D
R

us
si

an
-G

SD
63

.3
7

94
.9

2
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
U

D
E

ng
lis

h-
Pa

rT
U

T
80

.3
1

96
.6

5
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
U

D
R

us
si

an
-P

U
D

60
.6

8
91

.1
5

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

E
ng

lis
h-

PU
D

77
.5

9
96

.6
7

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

R
us

si
an

-S
yn

Ta
gR

us
73

.6
4

98
.3

8
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
U

D
E

st
on

ia
n-

E
D

T
74

.0
3

97
.2

3
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
U

D
R

us
si

an
-T

ai
ga

52
.0

6
92

.0
9

U
FA

L
PR

A
G

U
E

-0
1

U
D

Fa
ro

es
e-

O
FT

65
.3

2
87

.7
0

U
FA

L
PR

A
G

U
E

-0
1

U
D

Sa
ns

kr
it-

U
FA

L
29

.6
5

50
.7

5
U

FA
L

PR
A

G
U

E
-0

1
U

D
Fi

nn
is

h-
FT

B
72

.8
9

96
.8

5
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
U

D
Se

rb
ia

n-
SE

T
77

.0
5

97
.0

2
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
U

D
Fi

nn
is

h-
PU

D
70

.0
7

95
.6

2
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
/U

FA
L

PR
A

G
U

E
-0

1
U

D
Sl

ov
ak

-S
N

K
64

.0
4

95
.4

1
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
U

D
Fi

nn
is

h-
T

D
T

74
.8

4
97

.1
5

U
FA

L
PR

A
G

U
E

-0
1

U
D

Sl
ov

en
ia

n-
SS

J
73

.8
2

97
.0

4
U

FA
L

PR
A

G
U

E
-0

1
U

D
Fr

en
ch

-G
SD

84
.2

0
98

.3
1

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

Sl
ov

en
ia

n-
SS

T
69

.5
7

92
.7

6
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
U

D
Fr

en
ch

-P
ar

T
U

T
81

.6
7

95
.7

8
U

FA
L

PR
A

G
U

E
-0

1
U

D
Sp

an
is

h-
A

nC
or

a
84

.3
5

98
.7

9
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
U

D
Fr

en
ch

-S
eq

uo
ia

81
.5

0
98

.1
5

U
FA

L
PR

A
G

U
E

-0
1

U
D

Sp
an

is
h-

G
SD

81
.9

0
95

.8
8

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

Fr
en

ch
-S

po
ke

n
94

.4
8

98
.6

0
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
U

D
Sw

ed
is

h-
L

in
E

S
76

.9
3

94
.7

5
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
U

D
G

al
ic

ia
n-

C
T

G
86

.6
5

98
.4

4
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
U

D
Sw

ed
is

h-
PU

D
79

.9
7

95
.8

5
U

FA
L

PR
A

G
U

E
-0

1
U

D
G

al
ic

ia
n-

Tr
ee

G
al

76
.4

0
96

.2
1

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

Sw
ed

is
h-

Ta
lb

an
ke

n
81

.3
7

98
.0

9
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
U

D
G

er
m

an
-G

SD
68

.3
5

90
.4

3
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
U

D
Ta

ga
lo

g-
T

R
G

67
.5

7
91

.8
9

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

/U
FA

L
PR

A
G

U
E

-0
1

U
D

G
ot

hi
c-

PR
O

IE
L

81
.0

0
91

.0
2

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

Ta
m

il-
T

T
B

73
.3

3
91

.6
3

U
FA

L
PR

A
G

U
E

-0
1

U
D

G
re

ek
-G

D
T

77
.4

4
95

.9
5

U
FA

L
PR

A
G

U
E

-0
1

U
D

Tu
rk

is
h-

IM
ST

62
.9

4
92

.2
7

U
FA

L
PR

A
G

U
E

-0
1

U
D

H
eb

re
w

-H
T

B
81

.1
5

97
.6

7
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
U

D
Tu

rk
is

h-
PU

D
66

.3
0

87
.6

3
po

st
de

ad
lin

e
R

U
G

-0
1

U
D

H
in

di
-H

D
T

B
80

.6
0

93
.6

5
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
U

D
U

kr
ai

ni
an

-I
U

63
.5

9
95

.7
8

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

H
un

ga
ri

an
-S

ze
ge

d
65

.9
0

95
.0

3
U

FA
L

PR
A

G
U

E
-0

1
U

D
U

pp
er

So
rb

ia
n-

U
FA

L
57

.7
0

87
.0

2
U

FA
L

PR
A

G
U

E
-0

1
U

D
In

do
ne

si
an

-G
SD

71
.7

3
92

.4
8

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

U
rd

u-
U

D
T

B
69

.9
7

80
.9

0
U

FA
L

PR
A

G
U

E
-0

1
U

D
Ir

is
h-

ID
T

67
.6

6
86

.3
7

U
FA

L
PR

A
G

U
E

-0
1

U
D

V
ie

tn
am

es
e-

V
T

B
69

.4
2

94
.5

4
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
U

D
It

al
ia

n-
IS

D
T

83
.7

2
98

.4
9

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

Y
or

ub
a-

Y
T

B
73

.2
6

93
.8

0
C

M
U

-D
at

aA
ug

-0
1

U
D

It
al

ia
n-

Pa
rT

U
T

83
.5

1
98

.7
2

U
FA

L
PR

A
G

U
E

-0
1

Table 8: Task 2 Morph Accuracy scores



240

L
an

gu
ag

e
(T

re
eb

an
k)

B
as

el
in

e
B

es
t

Te
am

L
an

gu
ag

e
(T

re
eb

an
k)

B
as

el
in

e
B

es
t

Te
am

U
D

A
fr

ik
aa

ns
-A

fr
iB

oo
m

s
92

.8
7

99
.4

0
U

FA
L

PR
A

G
U

E
-0

1
U

D
It

al
ia

n-
Po

ST
W

IT
A

87
.9

8
97

.9
0

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

A
kk

ad
ia

n-
PI

SA
N

D
U

B
80

.4
1

89
.0

6
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
U

D
It

al
ia

n-
PU

D
92

.2
4

98
.4

2
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
U

D
A

m
ha

ri
c-

A
T

T
87

.5
7

93
.1

5
U

FA
L

PR
A

G
U

E
-0

1
U

D
Ja

pa
ne

se
-G

SD
90

.6
4

98
.2

1
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
U

D
A

nc
ie

nt
G

re
ek

-P
er

se
us

88
.9

7
96

.7
2

U
FA

L
PR

A
G

U
E

-0
1

U
D

Ja
pa

ne
se

-M
od

er
n

95
.6

4
97

.5
0

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

A
nc

ie
nt

G
re

ek
-P

R
O

IE
L

93
.5

5
97

.8
8

U
FA

L
PR

A
G

U
E

-0
1

U
D

Ja
pa

ne
se

-P
U

D
89

.6
4

98
.4

9
U

FA
L

PR
A

G
U

E
-0

1
U

D
A

ra
bi

c-
PA

D
T

91
.8

2
97

.6
5

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

K
om

i
Z

yr
ia

n-
IK

D
P

59
.5

2
82

.9
9

U
FA

L
PR

A
G

U
E

-0
1

U
D

A
ra

bi
c-

PU
D

86
.3

5
94

.6
6

R
U

G
-0

1
U

D
K

om
i

Z
yr

ia
n-

L
at

tic
e

74
.1

2
82

.9
9

R
U

G
-0

1
/R

U
G

-0
2

U
D

A
rm

en
ia

n-
A

rm
T

D
P

86
.7

4
96

.6
6

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

K
or

ea
n-

G
SD

85
.9

0
96

.2
7

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

B
am

ba
ra

-C
R

B
88

.9
4

95
.5

5
U

FA
L

PR
A

G
U

E
-0

1
U

D
K

or
ea

n-
K

ai
st

89
.4

5
97

.5
8

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

B
as

qu
e-

B
D

T
87

.5
4

96
.3

0
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
U

D
K

or
ea

n-
PU

D
88

.1
5

96
.7

6
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
U

D
B

el
ar

us
ia

n-
H

SE
78

.8
0

95
.6

8
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
U

D
K

ur
m

an
ji-

M
G

86
.5

4
91

.2
8

U
FA

L
PR

A
G

U
E

-0
1

U
D

B
re

to
n-

K
E

B
88

.3
4

93
.7

9
U

FA
L

PR
A

G
U

E
-0

1
U

D
L

at
in

-I
T

T
B

93
.1

2
98

.9
6

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

B
ul

ga
ri

an
-B

T
B

93
.8

5
99

.1
8

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

L
at

in
-P

er
se

us
78

.9
1

94
.6

5
U

FA
L

PR
A

G
U

E
-0

1
U

D
B

ur
ya

t-
B

D
T

80
.9

4
90

.5
0

U
FA

L
PR

A
G

U
E

-0
1

U
D

L
at

in
-P

R
O

IE
L

91
.4

2
97

.8
7

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

C
an

to
ne

se
-H

K
76

.8
0

92
.8

3
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
U

D
L

at
vi

an
-L

V
T

B
89

.5
5

98
.0

4
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
U

D
C

at
al

an
-A

nC
or

a
95

.7
3

99
.4

5
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
U

D
L

ith
ua

ni
an

-H
SE

67
.3

9
87

.9
7

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

C
hi

ne
se

-C
FL

82
.0

5
93

.2
1

U
FA

L
PR

A
G

U
E

-0
1

U
D

M
ar

at
hi

-U
FA

L
69

.7
1

80
.1

9
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
U

D
C

hi
ne

se
-G

SD
83

.7
9

97
.0

4
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
U

D
N

ai
ja

-N
SC

76
.7

3
95

.4
7

U
FA

L
PR

A
G

U
E

-0
1

U
D

C
op

tic
-S

cr
ip

to
ri

um
93

.5
6

97
.1

7
U

FA
L

PR
A

G
U

E
-0

1
U

D
N

or
th

Sa
m

i-
G

ie
lla

85
.4

5
95

.3
3

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

C
ro

at
ia

n-
SE

T
90

.3
9

97
.8

2
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
U

D
N

or
w

eg
ia

n-
B

ok
m

aa
l

93
.1

7
99

.0
2

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

C
ze

ch
-C

A
C

93
.9

4
99

.4
8

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

N
or

w
eg

ia
n-

N
yn

or
sk

92
.8

5
98

.9
7

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

C
ze

ch
-C

LT
T

92
.6

1
98

.3
2

U
FA

L
PR

A
G

U
E

-0
1

U
D

N
or

w
eg

ia
n-

N
yn

or
sk

L
IA

89
.2

1
97

.3
9

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

C
ze

ch
-F

ic
Tr

ee
90

.3
2

98
.9

0
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
U

D
O

ld
C

hu
rc

h
Sl

av
on

ic
-P

R
O

IE
L

91
.1

7
97

.1
3

U
FA

L
PR

A
G

U
E

-0
1

U
D

C
ze

ch
-P

D
T

94
.2

3
99

.4
7

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

Pe
rs

ia
n-

Se
ra

ji
93

.7
6

98
.6

8
U

FA
L

PR
A

G
U

E
-0

1
U

D
C

ze
ch

-P
U

D
85

.7
3

98
.2

3
U

FA
L

PR
A

G
U

E
-0

1
U

D
Po

lis
h-

L
FG

88
.7

3
98

.8
6

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

D
an

is
h-

D
D

T
90

.1
9

98
.6

8
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
U

D
Po

lis
h-

SZ
86

.2
4

98
.1

1
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
U

D
D

ut
ch

-A
lp

in
o

91
.2

5
98

.6
2

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

Po
rt

ug
ue

se
-B

os
qu

e
92

.3
6

98
.2

6
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
U

D
D

ut
ch

-L
as

sy
Sm

al
l

87
.9

7
98

.8
3

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

Po
rt

ug
ue

se
-G

SD
91

.7
3

99
.1

0
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
U

D
E

ng
lis

h-
E

W
T

90
.9

1
98

.5
2

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

R
om

an
ia

n-
N

on
st

an
da

rd
91

.7
0

97
.6

5
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
U

D
E

ng
lis

h-
G

U
M

89
.8

1
98

.1
1

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

R
om

an
ia

n-
R

R
T

93
.8

8
98

.8
9

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

E
ng

lis
h-

L
in

E
S

90
.5

8
98

.3
0

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

R
us

si
an

-G
SD

87
.4

9
97

.9
5

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

E
ng

lis
h-

Pa
rT

U
T

89
.4

6
97

.3
5

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

R
us

si
an

-P
U

D
84

.3
1

96
.2

7
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
U

D
E

ng
lis

h-
PU

D
87

.7
0

97
.5

8
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
U

D
R

us
si

an
-S

yn
Ta

gR
us

92
.7

3
99

.2
3

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

E
st

on
ia

n-
E

D
T

91
.5

2
98

.6
9

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

R
us

si
an

-T
ai

ga
76

.7
7

95
.5

6
U

FA
L

PR
A

G
U

E
-0

1
U

D
Fa

ro
es

e-
O

FT
85

.7
3

93
.9

8
U

FA
L

PR
A

G
U

E
-0

1
U

D
Sa

ns
kr

it-
U

FA
L

57
.8

0
69

.6
3

R
U

G
-0

1
/R

U
G

-0
2

U
D

Fi
nn

is
h-

FT
B

89
.0

8
98

.3
8

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

Se
rb

ia
n-

SE
T

91
.7

5
98

.6
4

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

Fi
nn

is
h-

PU
D

87
.7

7
97

.9
8

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

Sl
ov

ak
-S

N
K

88
.0

4
98

.2
4

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

Fi
nn

is
h-

T
D

T
90

.6
6

98
.5

4
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
U

D
Sl

ov
en

ia
n-

SS
J

90
.1

2
98

.8
0

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

Fr
en

ch
-G

SD
94

.6
3

99
.0

7
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
U

D
Sl

ov
en

ia
n-

SS
T

82
.2

8
96

.2
0

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

Fr
en

ch
-P

ar
T

U
T

92
.1

9
97

.9
7

U
FA

L
PR

A
G

U
E

-0
1

U
D

Sp
an

is
h-

A
nC

or
a

95
.3

5
99

.4
0

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

Fr
en

ch
-S

eq
uo

ia
93

.0
4

99
.1

1
U

FA
L

PR
A

G
U

E
-0

1
U

D
Sp

an
is

h-
G

SD
93

.9
5

98
.0

8
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
U

D
Fr

en
ch

-S
po

ke
n

94
.8

0
98

.6
5

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

Sw
ed

is
h-

L
in

E
S

89
.9

9
97

.6
7

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

G
al

ic
ia

n-
C

T
G

91
.3

5
98

.2
9

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

Sw
ed

is
h-

PU
D

90
.4

9
97

.4
0

U
FA

L
PR

A
G

U
E

-0
1

U
D

G
al

ic
ia

n-
Tr

ee
G

al
89

.3
3

97
.8

8
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
U

D
Sw

ed
is

h-
Ta

lb
an

ke
n

92
.6

5
99

.0
5

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

G
er

m
an

-G
SD

88
.9

1
95

.9
0

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

Ta
ga

lo
g-

T
R

G
87

.0
7

95
.0

4
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
/U

FA
L

PR
A

G
U

E
-0

1
U

D
G

ot
hi

c-
PR

O
IE

L
90

.0
2

96
.6

4
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
U

D
Ta

m
il-

T
T

B
89

.2
2

96
.0

0
U

FA
L

PR
A

G
U

E
-0

1
U

D
G

re
ek

-G
D

T
93

.4
5

98
.3

7
U

FA
L

PR
A

G
U

E
-0

1
U

D
Tu

rk
is

h-
IM

ST
86

.1
0

96
.3

0
U

FA
L

PR
A

G
U

E
-0

1
U

D
H

eb
re

w
-H

T
B

91
.7

9
98

.4
7

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

Tu
rk

is
h-

PU
D

87
.6

2
94

.9
6

po
st

de
ad

lin
e

R
U

G
-0

1
U

D
H

in
di

-H
D

T
B

93
.9

2
98

.0
4

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

U
kr

ai
ni

an
-I

U
86

.8
1

98
.1

0
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
U

D
H

un
ga

ri
an

-S
ze

ge
d

87
.6

2
98

.2
5

U
FA

L
PR

A
G

U
E

-0
1

U
D

U
pp

er
So

rb
ia

n-
U

FA
L

81
.0

4
93

.5
1

U
FA

L
PR

A
G

U
E

-0
1

U
D

In
do

ne
si

an
-G

SD
86

.1
2

95
.1

6
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
U

D
U

rd
u-

U
D

T
B

89
.4

6
93

.4
5

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

Ir
is

h-
ID

T
81

.5
8

91
.4

6
U

FA
L

PR
A

G
U

E
-0

1
U

D
V

ie
tn

am
es

e-
V

T
B

78
.0

0
94

.0
2

C
H

A
R

L
E

S-
SA

A
R

L
A

N
D

-0
2

U
D

It
al

ia
n-

IS
D

T
94

.4
6

99
.1

9
C

H
A

R
L

E
S-

SA
A

R
L

A
N

D
-0

2
U

D
Y

or
ub

a-
Y

T
B

85
.4

7
94

.1
9

C
M

U
-D

at
aA

ug
-0

1
U

D
It

al
ia

n-
Pa

rT
U

T
93

.8
8

99
.2

1
U

FA
L

PR
A

G
U

E
-0

1

Table 9: Task 2 Morph F1 scores



241

ent languages, in particular those with idiosyncratic
orthographies, may offer new challenges in this re-
spect.7

Only one team tried to learn inflection in a multi-
lingual setting—i.e. to use all training data to train
one model. Such transfer learning is an interest-
ing avenue of future research, but evaluation could
be difficult. Whether any cross-language transfer
is actually being learned vs. whether having more
data better biases the networks to copy strings is an
evaluation step to disentangle.8

Creating new data sets that accurately reflect
learner exposure (whether L1 or L2) is also an
important consideration in the design of future
shared tasks. One pertinent facet of this is informa-
tion about inflectional categories—often the inflec-
tional information is insufficiently prescribed by
the lemma, as with the Romanian verbal inflection
classes or nominal gender in German.

As we move toward multilingual models for mor-
phology, it becomes important to understand which
representations are critical or irrelevant for adapt-
ing to new languages; this may be probed in the
style of (Thompson et al., 2018), and it can be used
as a first step toward designing systems that avoid
“catastrophic forgetting” as they learn to inflect new
languages (Thompson et al., 2019).

Future directions for Task 2 include exploring
cross-lingual analysis—in stride with both Task 1
and Malaviya et al. (2018)—and leveraging these
analyses in downstream tasks.

7 Conclusions

The SIGMORPHON 2019 shared task provided a
type-level evaluation on 100 language pairs in 79
languages and a token-level evaluation on 107 tree-
banks in 66 languages, of systems for inflection and
analysis. On task 1 (low-resource inflection with
cross-lingual transfer), 14 systems were submitted,
while on task 2 (lemmatization and morphological
feature analysis), 16 systems were submitted. All
used neural network models, completing a trend in
past years’ shared tasks and other recent work on
morphology.

In task 1, gains from cross-lingual training were
generally modest, with gains positively correlating
with the linguistic similarity of the two languages.

7Although some work suggests that working with IPA or
phonological distinctive features in this context yields very
similar results to working with graphemes (Wiemerslage et al.,
2018).

8This has been addressed by Jin and Kann (2017).

In the second task, several methods were im-
plemented by multiple groups, with the most suc-
cessful systems implementing variations of multi-
headed attention, multi-level encoding, multiple
decoders, and ELMo and BERT contextual embed-
dings.

We have released the training, development, and
test sets, and expect these datasets to provide a
useful benchmark for future research into learn-
ing of inflectional morphology and string-to-string
transduction.

Acknowledgments

MS has received funding from the European Re-
search Council (ERC) under the European Union’s
Horizon 2020 research and innovation programme
(grant agreement No 771113).

References
Željko Agić and Natalie Schluter. 2017. How (not) to

train a dependency parser: The curious case of jack-
knifing part-of-speech taggers. In Proceedings of
the 55th Annual Meeting of the Association for Com-
putational Linguistics (Volume 2: Short Papers),
pages 679–684, Vancouver, Canada. Association for
Computational Linguistics.

Inaki Alegria, Izaskun Etxeberria, Mans Hulden, and
Montserrat Maritxalar. 2009. Porting Basque mor-
phological grammars to foma, an open-source tool.
In International Workshop on Finite-State Methods
and Natural Language Processing, pages 105–113.
Springer.

Toms Bergmanis and Sharon Goldwater. 2018. Con-
text sensitive neural lemmatization with Lematus. In
Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Volume 1 (Long Papers), pages 1391–1400, New
Orleans, Louisiana. Association for Computational
Linguistics.

Ryan Cotterell, Christo Kirov, John Sylak-Glassman,
Géraldine Walther, Ekaterina Vylomova, Arya D.
McCarthy, Katharina Kann, Sebastian Mielke, Gar-
rett Nicolai, Miikka Silfverberg, David Yarowsky,
Jason Eisner, and Mans Hulden. 2018. The CoNLL–
SIGMORPHON 2018 shared task: Universal mor-
phological reinflection. In Proceedings of the
CoNLL–SIGMORPHON 2018 Shared Task: Univer-
sal Morphological Reinflection, pages 1–27, Brus-
sels. Association for Computational Linguistics.

Ryan Cotterell, Christo Kirov, John Sylak-Glassman,
Gėraldine Walther, Ekaterina Vylomova, Patrick
Xia, Manaal Faruqui, Sandra Kübler, David
Yarowsky, Jason Eisner, and Mans Hulden. 2017.

https://doi.org/10.18653/v1/P17-2107
https://doi.org/10.18653/v1/P17-2107
https://doi.org/10.18653/v1/P17-2107
https://doi.org/10.18653/v1/N18-1126
https://doi.org/10.18653/v1/N18-1126
https://www.aclweb.org/anthology/K18-3001
https://www.aclweb.org/anthology/K18-3001
https://www.aclweb.org/anthology/K18-3001


242

CoNLL-SIGMORPHON 2017 shared task: Uni-
versal morphological reinflection in 52 languages.
Proceedings of the CoNLL SIGMORPHON 2017
Shared Task: Universal Morphological Reinflection,
pages 1–30.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers),
pages 4171–4186, Minneapolis, Minnesota. Associ-
ation for Computational Linguistics.

Daniela Gerz, Ivan Vulić, Edoardo Ponti, Jason Narad-
owsky, Roi Reichart, and Anna Korhonen. 2018.
Language modeling for morphologically rich lan-
guages: Character-aware modeling for word-level
prediction. Transactions of the Association for Com-
putational Linguistics, 6:451–465.

Huiming Jin and Katharina Kann. 2017. Exploring
cross-lingual transfer of morphological knowledge
in sequence-to-sequence models. In Proceedings of
the First Workshop on Subword and Character Level
Models in NLP, pages 70–75, Copenhagen, Den-
mark. Association for Computational Linguistics.

Katharina Kann and Hinrich Schütze. 2016. MED: The
LMU system for the SIGMORPHON 2016 shared
task on morphological reinflection. In Proceedings
of the 14th SIGMORPHON Workshop on Computa-
tional Research in Phonetics, Phonology, and Mor-
phology, pages 62–70, Berlin, Germany. Associa-
tion for Computational Linguistics.

Aleksandr E. Kibrik. 1998. Archi. In Andrew Spencer
and Arnold M. Zwicky, editors, The Handbook of
Morphology, pages 455–476. Oxford: Blackwell
Publishers.

Christo Kirov, Ryan Cotterell, John Sylak-Glassman,
Géraldine Walther, Ekaterina Vylomova, Patrick
Xia, Manaal Faruqui, Sebastian J. Mielke, Arya D.
McCarthy, Sandra Kübler, David Yarowsky, Jason
Eisner, and Mans Hulden. 2018. UniMorph 2.0:
Universal Morphology. In Proceedings of the 11th
Language Resources and Evaluation Conference,
Miyazaki, Japan. European Language Resource As-
sociation.

Thang Luong, Hieu Pham, and Christopher D. Man-
ning. 2015. Effective approaches to attention-based
neural machine translation. In Proceedings of the
2015 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1412–1421, Lis-
bon, Portugal. Association for Computational Lin-
guistics.

Chaitanya Malaviya, Matthew R. Gormley, and Gra-
ham Neubig. 2018. Neural factor graph models for
cross-lingual morphological tagging. In Proceed-
ings of the 56th Annual Meeting of the Association

for Computational Linguistics (Volume 1: Long Pa-
pers), pages 2653–2663, Melbourne, Australia. As-
sociation for Computational Linguistics.

Chaitanya Malaviya, Shijie Wu, and Ryan Cotterell.
2019. A simple joint model for improved contextual
neural lemmatization. In Proceedings of the 2019
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, Volume 1 (Long and Short
Papers), pages 1517–1528, Minneapolis, Minnesota.
Association for Computational Linguistics.

John Mansfield. 2019. Murrinhpatha morphology and
phonology, volume 653. Walter de Gruyter GmbH
& Co KG.

Arya D. McCarthy, Miikka Silfverberg, Ryan Cotterell,
Mans Hulden, and David Yarowsky. 2018. Marrying
Universal Dependencies and Universal Morphology.
In Proceedings of the Second Workshop on Univer-
sal Dependencies (UDW 2018), pages 91–101, Brus-
sels, Belgium. Association for Computational Lin-
guistics.

Thomas Müller, Ryan Cotterell, Alexander Fraser, and
Hinrich Schütze. 2015. Joint lemmatization and
morphological tagging with LEMMING. In Proceed-
ings of the 2015 Conference on Empirical Meth-
ods in Natural Language Processing, pages 2268–
2274, Lisbon, Portugal. Association for Computa-
tional Linguistics.

Graham Neubig and Junjie Hu. 2018. Rapid adapta-
tion of neural machine translation to new languages.
In Proceedings of the 2018 Conference on Empiri-
cal Methods in Natural Language Processing, pages
875–880, Brussels, Belgium. Association for Com-
putational Linguistics.

Joakim Nivre, Mitchell Abrams, Željko Agić, Lars
Ahrenberg, Lene Antonsen, Katya Aplonova,
Maria Jesus Aranzabe, Gashaw Arutie, Masayuki
Asahara, Luma Ateyah, Mohammed Attia, Aitz-
iber Atutxa, Liesbeth Augustinus, Elena Badmaeva,
Miguel Ballesteros, Esha Banerjee, Sebastian Bank,
Verginica Barbu Mititelu, Victoria Basmov, John
Bauer, Sandra Bellato, Kepa Bengoetxea, Yev-
geni Berzak, Irshad Ahmad Bhat, Riyaz Ahmad
Bhat, Erica Biagetti, Eckhard Bick, Rogier Blok-
land, Victoria Bobicev, Carl Börstell, Cristina
Bosco, Gosse Bouma, Sam Bowman, Adriane
Boyd, Aljoscha Burchardt, Marie Candito, Bernard
Caron, Gauthier Caron, Gülşen Cebiroğlu Eryiğit,
Flavio Massimiliano Cecchini, Giuseppe G. A.
Celano, Slavomı́r Čéplö, Savas Cetin, Fabricio
Chalub, Jinho Choi, Yongseok Cho, Jayeol Chun,
Silvie Cinková, Aurélie Collomb, Çağrı Çöltekin,
Miriam Connor, Marine Courtin, Elizabeth David-
son, Marie-Catherine de Marneffe, Valeria de Paiva,
Arantza Diaz de Ilarraza, Carly Dickerson, Pe-
ter Dirix, Kaja Dobrovoljc, Timothy Dozat, Kira
Droganova, Puneet Dwivedi, Marhaba Eli, Ali
Elkahky, Binyam Ephrem, Tomaž Erjavec, Aline

https://www.aclweb.org/anthology/N19-1423
https://www.aclweb.org/anthology/N19-1423
https://www.aclweb.org/anthology/N19-1423
https://doi.org/10.1162/tacl_a_00032
https://doi.org/10.1162/tacl_a_00032
https://doi.org/10.1162/tacl_a_00032
https://doi.org/10.18653/v1/W17-4110
https://doi.org/10.18653/v1/W17-4110
https://doi.org/10.18653/v1/W17-4110
https://doi.org/10.18653/v1/W16-2010
https://doi.org/10.18653/v1/W16-2010
https://doi.org/10.18653/v1/W16-2010
https://www.aclweb.org/anthology/L18-1293
https://www.aclweb.org/anthology/L18-1293
https://doi.org/10.18653/v1/D15-1166
https://doi.org/10.18653/v1/D15-1166
https://www.aclweb.org/anthology/P18-1247
https://www.aclweb.org/anthology/P18-1247
https://www.aclweb.org/anthology/N19-1155
https://www.aclweb.org/anthology/N19-1155
https://www.aclweb.org/anthology/W18-6011
https://www.aclweb.org/anthology/W18-6011
https://doi.org/10.18653/v1/D15-1272
https://doi.org/10.18653/v1/D15-1272
https://www.aclweb.org/anthology/D18-1103
https://www.aclweb.org/anthology/D18-1103


243

Etienne, Richárd Farkas, Hector Fernandez Al-
calde, Jennifer Foster, Cláudia Freitas, Katarı́na
Gajdošová, Daniel Galbraith, Marcos Garcia, Moa
Gärdenfors, Sebastian Garza, Kim Gerdes, Filip
Ginter, Iakes Goenaga, Koldo Gojenola, Memduh
Gökırmak, Yoav Goldberg, Xavier Gómez Guino-
vart, Berta Gonzáles Saavedra, Matias Grioni, Nor-
munds Grūzı̄tis, Bruno Guillaume, Céline Guillot-
Barbance, Nizar Habash, Jan Hajič, Jan Hajič jr.,
Linh Hà Mỹ, Na-Rae Han, Kim Harris, Dag Haug,
Barbora Hladká, Jaroslava Hlaváčová, Florinel
Hociung, Petter Hohle, Jena Hwang, Radu Ion,
Elena Irimia, O. lájı́dé Ishola, Tomáš Jelı́nek, An-
ders Johannsen, Fredrik Jørgensen, Hüner Kaşıkara,
Sylvain Kahane, Hiroshi Kanayama, Jenna Kan-
erva, Boris Katz, Tolga Kayadelen, Jessica Ken-
ney, Václava Kettnerová, Jesse Kirchner, Kamil
Kopacewicz, Natalia Kotsyba, Simon Krek, Sooky-
oung Kwak, Veronika Laippala, Lorenzo Lam-
bertino, Lucia Lam, Tatiana Lando, Septina Dian
Larasati, Alexei Lavrentiev, John Lee, Phuong
Lê H`ông, Alessandro Lenci, Saran Lertpradit, Her-
man Leung, Cheuk Ying Li, Josie Li, Keying
Li, KyungTae Lim, Nikola Ljubešić, Olga Logi-
nova, Olga Lyashevskaya, Teresa Lynn, Vivien
Macketanz, Aibek Makazhanov, Michael Mandl,
Christopher Manning, Ruli Manurung, Cătălina
Mărănduc, David Mareček, Katrin Marheinecke,
Héctor Martı́nez Alonso, André Martins, Jan
Mašek, Yuji Matsumoto, Ryan McDonald, Gus-
tavo Mendonça, Niko Miekka, Margarita Misir-
pashayeva, Anna Missilä, Cătălin Mititelu, Yusuke
Miyao, Simonetta Montemagni, Amir More, Laura
Moreno Romero, Keiko Sophie Mori, Shinsuke
Mori, Bjartur Mortensen, Bohdan Moskalevskyi,
Kadri Muischnek, Yugo Murawaki, Kaili Müürisep,
Pinkey Nainwani, Juan Ignacio Navarro Horñiacek,
Anna Nedoluzhko, Gunta Nešpore-Bērzkalne, Lu-
ong Nguy˜ên Thi., Huy`ên Nguy˜ên Thi. Minh, Vitaly
Nikolaev, Rattima Nitisaroj, Hanna Nurmi, Stina
Ojala, Adédayo. Olúòkun, Mai Omura, Petya Osen-
ova, Robert Östling, Lilja Øvrelid, Niko Partanen,
Elena Pascual, Marco Passarotti, Agnieszka Pate-
juk, Guilherme Paulino-Passos, Siyao Peng, Cenel-
Augusto Perez, Guy Perrier, Slav Petrov, Jussi Piitu-
lainen, Emily Pitler, Barbara Plank, Thierry Poibeau,
Martin Popel, Lauma Pretkalniņa, Sophie Prévost,
Prokopis Prokopidis, Adam Przepiórkowski, Ti-
ina Puolakainen, Sampo Pyysalo, Andriela Rääbis,
Alexandre Rademaker, Loganathan Ramasamy,
Taraka Rama, Carlos Ramisch, Vinit Ravishankar,
Livy Real, Siva Reddy, Georg Rehm, Michael
Rießler, Larissa Rinaldi, Laura Rituma, Luisa
Rocha, Mykhailo Romanenko, Rudolf Rosa, Davide
Rovati, Valentin Roca, Olga Rudina, Jack Rueter,
Shoval Sadde, Benoı̂t Sagot, Shadi Saleh, Tanja
Samardžić, Stephanie Samson, Manuela Sanguinetti,
Baiba Saulı̄te, Yanin Sawanakunanon, Nathan
Schneider, Sebastian Schuster, Djamé Seddah, Wolf-
gang Seeker, Mojgan Seraji, Mo Shen, Atsuko Shi-
mada, Muh Shohibussirri, Dmitry Sichinava, Na-
talia Silveira, Maria Simi, Radu Simionescu, Katalin
Simkó, Mária Šimková, Kiril Simov, Aaron Smith,

Isabela Soares-Bastos, Carolyn Spadine, Antonio
Stella, Milan Straka, Jana Strnadová, Alane Suhr,
Umut Sulubacak, Zsolt Szántó, Dima Taji, Yuta
Takahashi, Takaaki Tanaka, Isabelle Tellier, Trond
Trosterud, Anna Trukhina, Reut Tsarfaty, Francis
Tyers, Sumire Uematsu, Zdeňka Urešová, Larraitz
Uria, Hans Uszkoreit, Sowmya Vajjala, Daniel van
Niekerk, Gertjan van Noord, Viktor Varga, Eric
Villemonte de la Clergerie, Veronika Vincze, Lars
Wallin, Jing Xian Wang, Jonathan North Washing-
ton, Seyi Williams, Mats Wirén, Tsegay Wolde-
mariam, Tak-sum Wong, Chunxiao Yan, Marat M.
Yavrumyan, Zhuoran Yu, Zdeněk Žabokrtský, Amir
Zeldes, Daniel Zeman, Manying Zhang, and Hanzhi
Zhu. 2018. Universal dependencies 2.3. LIN-
DAT/CLARIN digital library at the Institute of For-
mal and Applied Linguistics (ÚFAL), Faculty of
Mathematics and Physics, Charles University.

Edoardo Maria Ponti, Helen O’Horan, Yevgeni Berzak,
Ivan Vulic, Roi Reichart, Thierry Poibeau, Ekaterina
Shutova, and Anna Korhonen. 2018. Modeling lan-
guage variation and universals: A survey on typo-
logical linguistics for natural language processing.
CoRR, abs/1807.00914.

John Sylak-Glassman, Christo Kirov, Matt Post, Roger
Que, and David Yarowsky. 2015a. A universal
feature schema for rich morphological annotation
and fine-grained cross-lingual part-of-speech tag-
ging. In Cerstin Mahlow and Michael Piotrowski,
editors, Proceedings of the 4th Workshop on Sys-
tems and Frameworks for Computational Morphol-
ogy (SFCM), Communications in Computer and In-
formation Science, pages 72–93. Springer, Berlin.

John Sylak-Glassman, Christo Kirov, David Yarowsky,
and Roger Que. 2015b. A language-independent
feature schema for inflectional morphology. In Pro-
ceedings of the 53rd Annual Meeting of the Associ-
ation for Computational Linguistics and the 7th In-
ternational Joint Conference on Natural Language
Processing (Volume 2: Short Papers), pages 674–
680, Beijing, China. Association for Computational
Linguistics.

Wilson L Taylor. 1953. “Cloze procedure”: A new
tool for measuring readability. Journalism Bulletin,
30(4):415–433.

Brian Thompson, Jeremy Gwinnup, Huda Khayrallah,
Kevin Duh, and Philipp Koehn. 2019. Overcoming
catastrophic forgetting during domain adaptation of
neural machine translation. In Proceedings of the
2019 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, Volume 1 (Long and
Short Papers), pages 2062–2068, Minneapolis, Min-
nesota. Association for Computational Linguistics.

Brian Thompson, Huda Khayrallah, Antonios Anasta-
sopoulos, Arya D. McCarthy, Kevin Duh, Rebecca
Marvin, Paul McNamee, Jeremy Gwinnup, Tim An-
derson, and Philipp Koehn. 2018. Freezing subnet-
works to analyze domain adaptation in neural ma-

http://hdl.handle.net/11234/1-2895
http://arxiv.org/abs/1807.00914
http://arxiv.org/abs/1807.00914
http://arxiv.org/abs/1807.00914
https://link.springer.com/chapter/10.1007/978-3-319-23980-4_5
https://link.springer.com/chapter/10.1007/978-3-319-23980-4_5
https://link.springer.com/chapter/10.1007/978-3-319-23980-4_5
https://link.springer.com/chapter/10.1007/978-3-319-23980-4_5
http://www.aclweb.org/anthology/P15-2111
http://www.aclweb.org/anthology/P15-2111
https://www.aclweb.org/anthology/N19-1209
https://www.aclweb.org/anthology/N19-1209
https://www.aclweb.org/anthology/N19-1209
https://www.aclweb.org/anthology/W18-6313
https://www.aclweb.org/anthology/W18-6313


244

chine translation. In Proceedings of the Third Con-
ference on Machine Translation: Research Papers,
pages 124–132, Belgium, Brussels. Association for
Computational Linguistics.

Géraldine Walther and Benoı̂t Sagot. 2010. Develop-
ing a large-scale lexicon for a less-resourced lan-
guage: General methodology and preliminary exper-
iments on Sorani Kurdish. In Proceedings of the 7th
SaLTMiL Workshop on Creation and use of basic lex-
ical resources for less-resourced languages (LREC
2010 Workshop), Valetta, Malta.

Géraldine Walther, Benoı̂t Sagot, and Karën Fort. 2010.
Fast development of basic NLP tools: Towards a lex-
icon and a POS tagger for Kurmanji Kurdish. In In-
ternational conference on lexis and grammar.

Adam Wiemerslage, Miikka Silfverberg, and Mans
Hulden. 2018. Phonological features for morpho-
logical inflection. In Proceedings of the Fifteenth
Workshop on Computational Research in Phonetics,
Phonology, and Morphology, pages 161–166, Brus-
sels, Belgium. Association for Computational Lin-
guistics.

Shijie Wu and Ryan Cotterell. 2019. Exact hard
monotonic attention for character-level transduction.
arXiv preprint arXiv:1905.06319v1.

Shijie Wu, Pamela Shapiro, and Ryan Cotterell. 2018.
Hard non-monotonic attention for character-level
transduction. In Proceedings of the 2018 Confer-
ence on Empirical Methods in Natural Language
Processing, pages 4425–4438, Brussels, Belgium.
Association for Computational Linguistics.

Barret Zoph, Deniz Yuret, Jonathan May, and Kevin
Knight. 2016. Transfer learning for low-resource
neural machine translation. In Proceedings of the
2016 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1568–1575, Austin,
Texas. Association for Computational Linguistics.

https://www.aclweb.org/anthology/W18-6313
https://www.aclweb.org/anthology/W18-6313
https://halshs.archives-ouvertes.fr/halshs-00751634
https://halshs.archives-ouvertes.fr/halshs-00751634
https://halshs.archives-ouvertes.fr/halshs-00751634
https://halshs.archives-ouvertes.fr/halshs-00751634
https://www.aclweb.org/anthology/W18-5818
https://www.aclweb.org/anthology/W18-5818
http://arxiv.org/abs/1905.06319v1
http://arxiv.org/abs/1905.06319v1
https://www.aclweb.org/anthology/D18-1473
https://www.aclweb.org/anthology/D18-1473
https://doi.org/10.18653/v1/D16-1163
https://doi.org/10.18653/v1/D16-1163

