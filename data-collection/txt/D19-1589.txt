



















































Linguistic Versus Latent Relations for Modeling Coherent Flow in Paragraphs


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 5809–5815,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

5809

Linguistic Versus Latent Relations
for Modeling Coherent Flow in Paragraphs

Dongyeop Kang Hiroaki Hayashi Alan W Black Eduard Hovy
Carnegie Mellon University, Pittsburgh, PA, USA

{dongyeok,hiroakih,awb,hovy}@cs.cmu.edu

Abstract
Generating a long, coherent text such as a
paragraph requires a high-level control of dif-
ferent levels of relations between sentences
(e.g., tense, coreference). We call such a log-
ical connection between sentences as a (para-
graph) flow. In order to produce a coherent
flow of text, we explore two forms of inter-
sentential relations in a paragraph: one is a
human-created linguistical relation that forms
a structure (e.g., discourse tree) and the other
is a relation from latent representation learned
from the sentences themselves. Our two pro-
posed models incorporate each form of re-
lations into document-level language models:
the former is a supervised model that jointly
learns a language model as well as discourse
relation prediction, and the latter is an unsu-
pervised model that is hierarchically condi-
tioned by a recurrent neural network (RNN)
over the latent information. Our proposed
models with both forms of relations outper-
form the baselines in partially conditioned
paragraph generation task. Our codes and data
are publicly available1.

1 Introduction

When composing multiple sentences into a para-
graph, as in novels or academic papers, we often
make design decisions in advance (Byrne, 1979)
such as topic introduction and content ordering to
ensure better coherence of the text. For instance,
McKeown (1985); Swan (2002) proposed effec-
tive patterns for scientific writing: a hypothesis
at first, followed by supporting sentences to val-
idate the hypothesis, and lastly a concluding sen-
tence. We call such a logical connection between
sentences in a written paragraph as a flow. A co-
herent flow between sentences requires an under-
standing of various factors including tense, coref-
erence, plans (Appelt, 1982; Hovy, 1991), scripts

1https://github.com/dykang/flownet

(Tomkins, 1978) and several others. We focus on
the paragraph-level plan between sentences.

In text planning, underlying relations in text are
broadly categorized into two forms: an explicit
human-defined relation (e.g., a discourse tree)
(Reiter and Dale, 2000) or an implicitly learned la-
tent relation (Yang et al., 2016). While the former
is defined and manuallly annotated based on lin-
guistic theories, the latter is simply determinable
from how people in fact put sentences together. In
this work, we provide an empirical comparison be-
tween a linguistically-informed and a latent form
of relations in context of a paragraph generation.

We compare the effectiveness of the two forms
of relations using language modeling for para-
graph generation. Due to the different charac-
teristics of the two forms, we employ compara-
ble but different components in addition to the
base language model. For linguistic relations (e.g.,
discourse), we cast the problem into multi-task
learning of supervised language modeling and dis-
course relation prediction. On the other hand, for
latent relations, we learn an unsupervised hierar-
chical language model that is hierarchically con-
ditioned by RNNs over linear operations between
sentences.

We evaluate our models on partial paragraph
generation task; producing the rest of text in a
paragraph given some context of text. We ob-
serve that linguistically annotated discourse rela-
tions help produce more coherent text than the la-
tent relations, followed by other baselines.

2 Related Work

There has been a variety of NLG systems that
incorporate additional information between sen-
tences (Appelt, 1982; Reiter and Dale, 2000; Gatt
and Krahmer, 2018) which can be broadly catego-
rized into two forms: linguistic and latent.

https://github.com/dykang/flownet


5810

Linguistic relations are explicitly represented
as external labels in the form of predefined rules or
plans, formats, knowledge base, discourse parses,
and more. Hovy (1985, 1990); Dalianis and
Hovy (1996) integrated text planning in genera-
tion, where the plans are considered in knowl-
edge, formatted rules and so forth. However,
they are limited to small scale (i.e. few exam-
ples) and hand-written rules. Kang et al. (2017);
Gardent et al. (2017); Kang et al. (2018b); Wang
et al. (2018) used an external knowledge base
to micro-planning for generating a corresponding
text, while our work focuses on comparing two
forms of relations from the text itself.

Moore and Paris (1993); Young and Moore
(1994) utilized discourse structures such as rhetor-
ical structure theory (RST) (Mann and Thompson,
1988) for parsing a document. A script (Tomkins,
1978) is another structured representation that de-
scribes a typical sequence of events in a partic-
ular context. Zhang et al. (2016); Ji and Eisen-
stein (2014) proposed better discourse parsers us-
ing neural networks. The prior works, however,
used the discourse representations to describe the
structure of the paragraph, while we focus on ap-
plicability of the discourse relations to language
generation.

Latent relations use implicit information in a
document such as hierarchical structure of the doc-
ument: Lin et al. (2015); Chung et al. (2016)
used hierarchical RNN for modeling a document.
Similarly, the hierarchical model can be extended
to other variants such as attention (Yang et al.,
2016), encoder-decoder framework (Serban et al.,
2017; Sordoni et al., 2015), auto-encoding (Li
et al., 2015), and multiscale (Chung et al., 2016).
However, the hierarchical recurrence of sentences,
which is dependent on topics, are less likely mod-
eling a flow of a document.

We further summarize the fundamental differ-
ences between the two forms of relations in Ap-
pendix.

3 FlowNet: Language Modeling with
Inter-sentential Relations

We propose language models that incorporate each
relation to capture a high-level flow of text.

3.1 Discourse-driven FlowNet

As a linguistic relation, we employ RST (Mann
and Thompson, 1988) trees to represent discourse

connections in the text. For simplicity, we limit
usage of the discourse trees by only considering
relations between adjacent phrases2: relations are
inserted between adjacent phrases and represented
as a flattened sequence of phrases and relations.
If two consecutive RST relations are given, the
deeper level of relation is chosen. If the central
elementary discourse unit (EDU) or phrase is af-
ter its dependent, the relation is excluded. We
consider each sequence of the flattened discourse
relations as a writing flow. For example, peo-
ple often write a text by elaborating basic infor-
mation (Elaboration) and then describing a
following statement attributed to the information
(Attribution).

We view discourse relations as additional labels
to predict at the same time we predict next words
in language modeling. Specifically, we propose to
jointly train a model that predicts a sequence of
words and a sequence of RST labels by taking ad-
vantage of shared representations, following pre-
vious sequence labeling problems such as named
entity recognition (Collobert et al., 2011) and part-
of-speech tagging (Huang et al., 2015). Note that
the RST relations are only used during training to
obtain better representation for the two tasks, but
not at test time.

Figure 1(a) shows our FlowNet using discourse
relations. Let a paragraph be a sequence of sen-
tences D={s1, s2, . . . , sM}. This model treats ad-
jacent sentences as pairs for learning the standard
seq2seq model. The first objective is to maximize
the likelihood of the current sentence given the
previous sentence. Hence, we maximize the fol-
lowing:

Ls2s =
∑
j

logP (wij |wi,<j , si−1) (1)

where si={wi1, wi2, . . . , wiTi}, and Ti is the num-
ber of tokens of si.

To better guide the model with discourse con-
text, we use the shared representations to pre-
dict RST relations at the same time. For
each paragraph, we run the pre-trained RST
parser (Ji and Eisenstein, 2014) and flatten the
parse tree to obtain RST relations for each sen-
tence Yi=(y1, . . . , yKi), where Ki is the number
of discourse relations in si. We then make a
label sequence over tokens in the sentence with

2The full discourse tree can be incorporated using other
types of language model such as Tai et al. (2015).



5811

was dead because she

CAUSEO O

CAUSE

O

w31 w32w21 w22
......

(a) Discourse-driven (b) Delta-driven

Figure 1: FlowNet with linguistic (i.e., discourse) versus latent (i.e., delta) relation. (a) For each word, a form
of discourse relation and next word are jointly predicted using CRF (}) and language model, respectively. (b)
Decoding wi is conditioned on previous word (wi−1), previous sentence (si−1), and delta between two previous
sentences (di−2). Best viewed in color.

by placing y at the first word of EDUs and fill-
ing up the rest with a null relation o: Y ′i =
(o, . . . , o,y1, o, . . . ,yKi , o, . . . , o). We incorpo-
rate a sequence labeling objective by employing
conditional random field (Lafferty et al., 2001)
to find the label sequence that maximizes the
score function for each sentence si: S(si, Y ′i ) =∑Ti−1

j=1 W
T
y′j ,y

′
j+1
hj + by′j ,y′j+1 where hj , W and b

are the hidden representation of wij , weight ma-
trix, and the bias vector corresponding to the pair
of labels (y′i, y

′
i+1), respectively. For training, we

maximize the conditional likelihood:

LCRF = S(si, y′i)−
∑
y∈Yx

logS(si,y) (2)

where Yx represents all possible discourse label
sequences. Decoding is done by greedily pre-
dicting the output sequence with maximum score.
Both training and decoding can be computed us-
ing dynamic programming. The final objective is
represented as the sum of two objective functions:

Ldisc = Ls2s + α ∗ LCRF (3)

where α is a scaling parameter to control the im-
pact of CRF objective. The value is chosen empir-
ically by searching based on validation set.

3.2 Delta-driven FlowNet
In this model, we aim to utilize latent representa-
tions to characterize the flow between sentences.
Specifically we define delta, subtractions of hid-
den represenations of adjacent sentences as such
latent information. Figure 1(b) shows how we hi-
erarchically model different levels of information:
words, sentences, and deltas.

Each word is encoded using a RNN encoder
gword. We take the last hidden representation of

word as sentence embeddings s1, ..., sM . Simi-
lar to hierarchical RNN (Lin et al., 2015), each
sentence representation is encoded using another
RNN encoder gsent. While discourse flow pro-
vides an explicit relation symbols, delta flow cal-
culates a latent relation by subtracting previous
representation si−1 from current representation
si

3:

d(si−1, si) = di−1 = si − si−1 (4)

Given a sequence of M -1 delta relations
d1, ..., dM−1 for a paragraph of M sentences, we
again encode them using another RNN encoder
gdelta. The model takes the word, sentence and
delta information altogether to predict the next (t-
th) word in the m-th sentence:

ht = f(ht−1, xt, sm−1, dm−2) (5)

where xt is a word representation, sm−1 is a sen-
tence representation and dm−2 is a delta informa-
tion. Note that sentence representation is from the
previous sentence, and delta information is calcu-
lated by two previous sentences. If there is no pre-
vious information given, the parameters are ran-
domly initialized.

4 Experiment

Due to the absence of goal-oriented language gen-
eration task, we collect paragraph data and define
a new task of generating partial text of a paragraph
given some context.

4.1 Data
We collect paragraphs from three different do-
mains: Papers are paragraphs extracted from

3Our experiment includes a comparison among other
types of linear operations between sentences such as addition
or a learnable function.



5812

Train Valid Test

Papers 16,173 899 899
SciFi 157,031 8,724 8,724
Fantasy 317,654 17,649 17,649

Table 1: Number of paragraphs in our dataset.

academic manuscripts in computer science do-
main from the PeerRead (Kang et al., 2018a), and
Fantasy and SciFi are paragraphs of two fre-
quent categories extracted from the BookCorpus
(Zhu et al., 2015), where paragraphs are extracted
using the line breaker in the dataset.

We only use paragraphs whose lengths are from
4 to 7, in order to measure the performance change
according to paragraph length. The dataset is ran-
domly split by 0.9/0.05/0.05 for train, valid, and
test set, respectively. Table 1 shows the numbers
of paragraphs for each domain. All paragraphs
are parsed into RST trees using the state-of-the-art
discourse parser by Ji and Eisenstein (2014).

4.2 Bridging: Partial Paragraph Generation
We evaluate our models on partial text generation
task; given a partial information (e.g., some sen-
tences), producing the rest of text.

[1] Inside the club we moved straight for the bar.
[2] Devlin ordered a beer for himself and a glass
of my favorite wine for me. [3] I love that I didn’t
have to tell him what I wanted. [4] He knew me
well and always thought about what I wanted or
needed, in and out of bed.

Figure 2: Bridging task: given [1] and [4] sentences,
guessing [2,3] sentences (red, underlined).

Figure 2 shows our bridging task. It requires a
generation of masked sentences in the middle of
a paragraph given the first and the last sentences.
If only the first sentence is given, the generation
can be too divergent. The existence of the last
sentence makes the generation more coherent and
converged to some point.

We evaluate it with one hard and one soft au-
tomatic metrics: METEOR (M) (Banerjee and
Lavie, 2005) and VectorExtrema (VE) (Liu et al.,
2016) by calculating cosine similarity of averaged
word embeddings (Pennington et al., 2014), and
human performance.

4.3 Models and Setup
We compare various baseline seq2seq models
which encode the context; a concatenated first

and last sentences, and decode the intermediate
words: S2S is attentional seq2seq model (Bah-
danau et al., 2014), and HS2S: is a hierarchical
version of the S2S by combining two baselines:
HRNN (Lin et al., 2015) hierarchically models se-
quence of words and sentences, and HRED (Ser-
ban et al., 2017; Sordoni et al., 2015) encodes the
given context and decodes the words. FlowNet
(delta/disc.) is our proposed language model with
delta and discourse relations, respectively.

We find the best hyper-parameters on validation
set using grid search. Here are the final parame-
ters used: 32 for batch size, 25 for maximum sen-
tence length, 300 for word embedding size initial-
ized by GloVe (Pennington et al., 2014), 1 LSTM
layer (Hochreiter and Schmidhuber, 1997) with
512 size, clipping by 0.25, 0.2 learning rate and
0.5 decay rate with Adagrad (Duchi et al., 2011)
optimizer, and 50, 000 for the vocabulary size.
The total number of distinct discourse relations is
44.

4.4 Results

Papers SciFi Fantasy

M VE M VE M VE

S2S 3.7 56.3 3.5 71.0 3.3 66.3
HS2S 3.7 54.7 3.4 73.0 3.0 69.7

FlowNet (delta) 3.1 58.5 3.6 69.7 3.6 73.9
FlowNet (disc.) 4.0 57.2 4.2 70.3 3.9 71.8

Table 2: Performance on bridging task. METEOR and
VectorExtrema are used. The higher the better.

In Table 2, both discourse and delta driven
FlowNet outperform the baseline models across
most of the metrics except for VecterExtrema on
SciFi. Especially, as the number of training
size increases (Papers<<SciFi<Fantasy),
the improvements gained from the FlowNet be-
come bigger. This is probably because the model
learns more information of the (discourse or la-
tent) relations from the larger data.

M VE

SUBTRACT 3.35 67.20
ADD 3.45 65.35
MLP 3.32 62.97

Table 3: Comparison
of different delta func-
tions.

Figure 3: Comparison of
paragraph lengths. Best
viewed in color.

Table 3 shows performance comparison among
different delta operations: SUBTRACT, ADD, and



5813

FIRST: Satyrs never wear armor, including helmets, Newel began, using his hands expressively.
LAST: Anyhow, as we actors were laying siege, a big chunk of the battlement dislodged from atop the tower.

REF: [M1] ”But years ago I was in a play, and the helm was part of my costume. [M2] During the big battle scene, a few of us
were assailing a castle. [M3] We had quite a set. [M4] The main tower must have been fifteen feet tall, fashioned from real stone.
HUMAN: [M1] Actually he needed to wear any protectors to prevent him from a big accident. [M2] We planned to make a prank
cam to make him wear those always. [M3] ”I have a good idea,” Newel kept talking continuously. [M4] ”Let’s play a role like we
are under the attack.

S2S: [M1] he’s a good man [M2] the UNK, the one who’s a man who’s a man and the other [M3] and the other, the one who ’s a
good friend [M4] he’s a good man
HS2S: [M1] i’m not sure that,” he said [M2] i’m not sure that i’m not sure [M3] i’m not sure that i’m not a fool [M4] ”i’m not sure
that,” he said
FLOWNET (DELTA): [M1] he’s a good man [M2] i’m not sure what to do [M3] i’m not sure that i’m not going to be a vampire [M4]
he’s a good man
FLOWNET (DISC.): [M1] perhaps they were not quite good, but he was not a master, and they were the most powerful [M2] the
only way to do not like a little, but i’ d been in the world [M3] ”you’re right,” he said ”i am not a fool you’re here [M4] you’re
going to be a bit more than the other

Table 4: An example paragraph and predicted texts in Fantasy dataset. Given FIRST and LAST sentences,
the models generate middle sentences (e.g., [M1] → [M2]..). REF and HUMAN are reference middle sentences and
sentences written by human annotator, respectively. Please find more examples in the appendix.

MLP which is a multi-layer perceptron network.
All scores are macro-averaged across datasets.
While ADD shows good performance on ME-
TEOR, SUBTRACT does on the soft metric (i.e.,
VecExt), indicating that subtraction can help the
model capture the better semantics than the other
functions. Figure 3 shows how performance
changes on Fantasy as the paragraph lengths in-
crease. Both of FlowNet achieve more improve-
ments when generating longer paragraphs. Espe-
cially, discourse relations achieve the best perfor-
mance at length 6 and 7.

(a) By paragraph lengths (b) By domains

Figure 4: Comparison (METEOR) with human perfor-
mance (black bars): S2S (blue), HS2S (red), Flow:delta
(yellow), and Flow:disc. (green). Best viewed in color.

We conduct a comparison with human perfor-
mance (See Figure 4). We randomly choose 100
samples per dataset and per paragraph length and
ask an annotator to perform the bridging task on
the final 1,000 samples. Human outperforms the
models by large margins. FlowNet with discourse
relations outperforms the FlowNet with latent rela-
tions and other baselines by a large margin. As the
paragraph length increases or more data is trained,
discourse relations become more useful.

Table 4 shows an example paragraph with text

produced by the models as well as reference and
human annotation. Given only the partial con-
text (i.e., first and last sentences), bridging task is
very challenging even for human. The reference
sentences and human annotations are semantically
very different indeed. Among the latent mod-
els, FlowNet (delta) produces more coherent flow
of text compared to S2S and HS2S. Surprisingly,
FlowNet (discourse) enables generating more di-
verse sentences with a bit of coherence, because
each sentence is generated based on the represen-
tation conditioned on the predicted RST discourse
relation.

5 Conclusion and Discussion

We explore two forms of inter-sentential relations:
linguistic relation such as discourse relations and
a latent representation learned from the text. The
proposed models for both relations achieve signif-
icant improvements over the baselines on partial
paragraph generation task.

Despite the empirical effectiveness and differ-
ence between the linguistic and latent relations,
they are not directly aligned for comparison. A
potential direction for future study is to directly
couple them together and see whether one form
contains the other, or vice versa. Another direc-
tion is to check their effectiveness on top of the
recent pre-trained language models.

Acknowledgements

We also thank Jason Weston, Dan Jurafsky, and
anonymous reviewers for their helpful comments.



5814

References
Douglas E Appelt. 1982. Planning natural-language ut-

terances to satisfy multiple goals. Technical report,
SRI INTERNATIONAL MENLO PARK CA ARTI-
FICIAL INTELLIGENCE CENTER.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
learning to align and translate. arXiv preprint
arXiv:1409.0473.

Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An
automatic metric for mt evaluation with improved
correlation with human judgments. In Proceedings
of the acl workshop on intrinsic and extrinsic evalu-
ation measures for machine translation and/or sum-
marization, pages 65–72.

Donn Byrne. 1979. Teaching writing skills. Longman.

Junyoung Chung, Sungjin Ahn, and Yoshua Bengio.
2016. Hierarchical multiscale recurrent neural net-
works. CoRR, abs/1609.01704.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research,
12(Aug):2493–2537.

Hercules Dalianis and Eduard Hovy. 1996. Aggrega-
tion in natural language generation. In Trends in
Natural Language Generation An Artificial Intelli-
gence Perspective, pages 88–105. Springer.

John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. Journal of Machine
Learning Research, 12(Jul):2121–2159.

Claire Gardent, Anastasia Shimorina, Shashi Narayan,
and Laura Perez-Beltrachini. 2017. Creating train-
ing corpora for nlg micro-planning. In 55th annual
meeting of the Association for Computational Lin-
guistics (ACL).

Albert Gatt and Emiel Krahmer. 2018. Survey of the
state of the art in natural language generation: Core
tasks, applications and evaluation. Journal of Artifi-
cial Intelligence Research, 61:65–170.

Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long
short-term memory. Neural Computation, 9:1735–
1780.

Eduard H. Hovy. 1985. Integrating text planning and
production in generation. In IJCAI.

Eduard H Hovy. 1990. Pragmatics and natural lan-
guage generation. Artificial Intelligence, 43(2):153–
197.

Eduard H Hovy. 1991. Approaches to the planning of
coherent text. In Natural language generation in
artificial intelligence and computational linguistics,
pages 83–102. Springer.

Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidirec-
tional lstm-crf models for sequence tagging. CoRR,
abs/1508.01991.

Yangfeng Ji and Jacob Eisenstein. 2014. Represen-
tation learning for text-level discourse parsing. In
Proceedings of the 52nd Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), volume 1, pages 13–24.

Dongyeop Kang, Waleed Ammar, Bhavana Dalvi,
Madeleine van Zuylen, Sebastian Kohlmeier, Ed-
uard Hovy, and Roy Schwartz. 2018a. A dataset of
peer reviews (peerread): Collection, insights and nlp
applications. In Proceedings of NAACL-HLT.

Dongyeop Kang, Varun Gangal, Ang Lu, Zheng Chen,
and Eduard Hovy. 2017. Detecting and explaining
causes from text for a time series event. In Con-
ference on Empirical Methods on Natural Language
Processing.

Dongyeop Kang, Tushar Khot, Ashish Sabharwal, and
Eduard Hovy. 2018b. Adventure: Adversarial train-
ing for textual entailment with knowledge-guided
examples. In The 56th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), Mel-
bourne, Australia.

John D. Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In ICML.

Jiwei Li, Minh-Thang Luong, and Daniel Jurafsky.
2015. A hierarchical neural autoencoder for para-
graphs and documents. In ACL.

Rui Lin, Shujie Liu, Muyun Yang, Mu Li, Ming Zhou,
and Sheng Li. 2015. Hierarchical recurrent neural
network for document modeling. In Proceedings of
the 2015 Conference on Empirical Methods in Nat-
ural Language Processing, pages 899–907.

Chia-Wei Liu, Ryan Lowe, Iulian V Serban, Michael
Noseworthy, Laurent Charlin, and Joelle Pineau.
2016. How not to evaluate your dialogue system:
An empirical study of unsupervised evaluation met-
rics for dialogue response generation. arXiv preprint
arXiv:1603.08023.

William C Mann and Sandra A Thompson. 1988.
Rhetorical structure theory: Toward a functional the-
ory of text organization. Text-Interdisciplinary Jour-
nal for the Study of Discourse, 8(3):243–281.

Kathleen R McKeown. 1985. Discourse strategies for
generating natural-language text. Artificial Intelli-
gence, 27(1):1–41.

Johanna D Moore and Cécile L Paris. 1993. Planning
text for advisory dialogues: Capturing intentional
and rhetorical information. Computational linguis-
tics, 19(4):651–694.



5815

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 confer-
ence on empirical methods in natural language pro-
cessing (EMNLP), pages 1532–1543.

Ehud Reiter and Robert Dale. 2000. Building natural
language generation systems. Cambridge university
press.

Iulian Vlad Serban, Alessandro Sordoni, Ryan Lowe,
Laurent Charlin, Joelle Pineau, Aaron C Courville,
and Yoshua Bengio. 2017. A hierarchical latent
variable encoder-decoder model for generating di-
alogues. In AAAI, pages 3295–3301.

Alessandro Sordoni, Yoshua Bengio, Hossein Vahabi,
Christina Lioma, Jakob Grue Simonsen, and Jian-
Yun Nie. 2015. A hierarchical recurrent encoder-
decoder for generative context-aware query sugges-
tion. In Proceedings of the 24th ACM International
on Conference on Information and Knowledge Man-
agement, pages 553–562. ACM.

Judith A. Swan. 2002. The science of scientific writ-
ing. In Book.

Kai Sheng Tai, Richard Socher, and Christopher D
Manning. 2015. Improved semantic representations
from tree-structured long short-term memory net-
works. arXiv preprint arXiv:1503.00075.

Silvan S Tomkins. 1978. Script theory: Differential
magnification of affects. In Nebraska symposium on
motivation. University of Nebraska Press.

Qingyun Wang, Xiaoman Pan, Lifu Huang, Boliang
Zhang, Zhiying Jiang, Heng Ji, and Kevin Knight.
2018. Describing a knowledge base. In CoRR, vol-
ume abs/1809.01797.

Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He,
Alex Smola, and Eduard Hovy. 2016. Hierarchi-
cal attention networks for document classification.
In Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies.

R Michael Young and Johanna D Moore. 1994. Dpocl:
A principled approach to discourse planning. In
Proceedings of the Seventh International Workshop
on Natural Language Generation, pages 13–20. As-
sociation for Computational Linguistics.

Biao Zhang, Deyi Xiong, Jinsong Su, Qun Liu, Ron-
grong Ji, Hong Duan, and Min Zhang. 2016. Vari-
ational neural discourse relation recognizer. In
EMNLP.

Yukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan
Salakhutdinov, Raquel Urtasun, Antonio Torralba,
and Sanja Fidler. 2015. Aligning books and movies:
Towards story-like visual explanations by watching
movies and reading books. 2015 IEEE International
Conference on Computer Vision (ICCV), pages 19–
27.


