



















































Extended Translation Models in Phrase-based Decoding


Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 282–293,
Lisboa, Portugal, 17-18 September 2015. c©2015 Association for Computational Linguistics.

Extended Translation Models in Phrase-based Decoding

Andreas Guta, Joern Wuebker, Miguel Graça, Yunsu Kim, Hermann Ney
Human Language Technology and Pattern Recognition Group

RWTH Aachen University
Aachen, Germany

{surname}@cs.rwth-aachen.de

Abstract

We propose a novel extended translation
model (ETM) to counteract some prob-
lems in phrase-based translation: The lack
of translation context when using single-
word phrases and uncaptured dependen-
cies beyond phrase boundaries. The ETM
operates on word-level and augments the
IBM models by an additional bilingual
word pair and a reordering operation. Its
implementation in a phrase-based decoder
introduces translation and reordering de-
pendencies for single-word phrases and
dependencies across phrase boundaries.
More, the model incorporates an explicit
treatment of multiple and empty align-
ments. Its integration outperforms com-
petitive systems that include lexical and
phrase translation models as well as hier-
archical reordering models on 4 language
pairs significantly by +0.7% BLEU on av-
erage. Although simpler and using fewer
dependencies, the ETM proves to be on
par with 7-gram operation sequence mod-
els (Durrani et al., 2013b).

1 Introduction

The first successful steps in Statistical Ma-
chine Translation have been taken by applying
word-based models in a source-channel approach
(Brown et al., 1990; Brown et al., 1993). Within
this framework, the language model (LM) is esti-
mated on monolingual n-grams, whereas the trans-
lation models IBM-1 to IBM-5 are trained on
bilingual data using word alignments. The dis-
advantage of word-to-word translation is over-
come by phrase-based translation (PBT) (Och et
al., 1999; Zens et al., 2002; Koehn et al., 2003)
and log-linear model combination (Och and Ney,
2002).

 0

 10

 20

 30

 40

1 2 3 4 5 6 7

re
la

ti
ve

 w
or

d 
fr

eq
ue

nc
y 

[%
]

phrase length

De-En (source)
De-En (target)
En-Fr (source)
En-Fr (target)

Figure 1: Relative frequency of words dependent
on the length of the phrase they were decoded with
for the IWSLT dev2010 German→English and
English→French corpora.

Nevertheless, phrase-based translation models
have several drawbacks: (i) Single-word phrases
are translated without any context. (ii) Dependen-
cies beyond phrase boundaries are not modelled at
all. (iii) Phrase-based translation models have dif-
ficulties modelling long-distance dependencies on
source words with large gaps inbetween.

The open question is how much actual lex-
ical context is included in decoding. Figure
1 depicts the relative word frequencies plotted
against the length of the phrase they were trans-
lated with for the IWSLT 20141 German→English
and English→French tasks. For English→French,
more than 40% of the words are translated us-
ing single- or two-word phrases, i.e. with a
lexical context of at most one word. For the
German→English task, more reorderings occur
and lead to less monotone alignments. Here, even
60% of all words are translated with a lexical con-
text of at most one single word and over 20% are
translated without any lexical context at all.

1http://www.iwslt2014.org

282



We address this problem by developing two
variants of extended translation models (ETM),
the direct (EdTM) for the Source→Target and the
inverse (EiTM) for the Target→Source direction.
They operate on word-level and augment the IBM
models by an additional bilingual word pair and
a reordering operation. We introduce them into
the log-linear framework of a PBT system. Thus,
the decoding of single-word phrases can benefit
from lexical and reordering context. Moreover,
the ETM allows to capture dependencies across
phrase boundaries and long-range source depen-
dencies. It incorporates reordering information for
non-monotone and multiple alignments including
unaligned words.

As a first step, we implement the ETM as
a count model with interpolated Kneser-Ney
smoothing (Chen and Goodman, 1998) using the
Viterbi alignment and apply it in phrase-based de-
coding. Nevertheless, the long-term goal of this
approach is to replace the phrases used in decod-
ing by translation units that predict a single target
word, but may depend on several source words,
previously translated target words and the reorder-
ing context.

2 Previous Work

Various approaches have been taken to compen-
sate the downside of the phrase translation model.
Mariño et al. (2006) introduce a translation model
based on n-grams of bilingual word pairs, i.e. a
bilingual language model (BILM), with an n-gram
decoder that requires monotone alignments. In
(Niehues et al., 2011), this is further advanced
by BILMs operating on non-monotone alignments
within a PBT framework.

However, this differs from our approach:
BILMs treat jointly aligned source words as
atomic units, ignore source deletions and do not
include reordering context.

The Operation Sequence Model (OSM) intro-
duced in (Durrani et al., 2011; Durrani et al.,
2013a) includes n-grams of both translation and
reordering operations in a consistent framework.
It utilizes minimal translation units (MTUs) and
is applied in a corresponding OSM decoder. Ex-
periments in (Durrani et al., 2013b) show that a
slightly enhanced version of OSM performs best
when integrated into the log-linear framework of a
phrase-based decoder. Both the BILM (Stewart et
al., 2014) and the OSM (Durrani et al., 2014) can

be smoothed using word clusters.

In comparison, the ETM is much simpler: Since
it predicts probabilities of single words, it has a
lower vocabulary size. More, it does not make
use of reordering gaps, i.e. it utilizes a simpler
reordering approach. The OSM uses one joint
model for reorderings and translations. In con-
trast, the ETM incorporates separate models to
estimate the probability of words and the proba-
bility of reorderings. Furthermore, the OSM has
the drawback that it extracts the MTUs sentence-
wise, thus one word can appear in several MTUs
extracted from different sentence pairs. Since an
MTUs is treated as an atomic unit, this results in
a distribution of probability mass on overlapping
events. The ETM overcomes this drawback by op-
erating on single words.

Guta et al. (2015) propose the conversion of
bilingual sentence pairs and word alignments into
joint translation and reordering (JTR) sequences.
They investigate n-gram models with modified
Kneser-Ney smoothing, feed-forward and recur-
rent neural networks trained on JTR sequences. In
comparison to the OSM, JTR models have smaller
vocabulary sizes, as they operate on words, and
incorporate simpler reordering structures. Never-
theless, they are shown to perform slightly better
than the OSM when included into the log-linear
framework of a phrase-based decoder.

Although our approach is similar, there are
the following significant differences: On the one
hand, the ETM estimates the probability of single
words conditioned on an extended lexical and re-
ordering context, whereas the JTR n-gram model
predicts the probability of bilingual word pairs.
On the other hand, we do not assume linear se-
quences of dependencies, but propose and explicit
treatment of multiply aligned words.

Deng and Byrne (2005) present an HMM ap-
proach for word-to-phrase alignments, which per-
forms similar to IBM-4 on the task of bitext align-
ment and can also be applied for more powerful
phrase induction. Feng et al. (2013) introduce an
reordering model based on sequence labeling tech-
niques by converting the reordering problem into a
a tagging task. Zhang et al. (2013) explore differ-
ent Markov chain orderings for an n-gram model
on MTUs. These are not integrated into decod-
ing, but used in N-best rescoring. Another gener-
ative, word-based Markov chain translation model
is presented by Feng and Cohn (2013). It exploits

283



a hierarchical Pitman-Yor process for smoothing,
but is only applied to induce word alignments.
Their follow-up work (Feng et al., 2014) intro-
duces a Markov-model on MTUs, similar to the
OSM described above.

Finally, there has been recent research on apply-
ing neural network models for extended context
(Le et al., 2012; Auli et al., 2013; Hu et al., 2014;
Devlin et al., 2014; Sundermeyer et al., 2014). All
of these papers focus on lexical context and ignore
the reordering aspect covered in our work.

3 Extended Translation Models

Given a source sentence f J1 and its translation e
I
1,

EiTM models the inverse probability p( f J1 |eI1) and
EdTM the direct probability p(eI1| f J1 ). We allow
for source words to be translated to multiple target
words and vice versa. The inverted alignment bi
denotes the sequence of source positions j aligned
to target position i for i = 1, . . . , I. Its subsequence
b< ji includes all source positions in bi preceding a
given source position j:

b< ji =
{

j̄ ∈ bi : j̄ < j
}

.

Unaligned target words are aligned to the empty
source word f0, unaligned source words to the
empty target word e0. b0 denotes the unaligned
source positions. We introduce the fertility φi of a
target word ei. It determines the number of source
words aligned to the target word ei:

φi =

{
0, bi = {0}
|bi|, else

By analogy, we use φ< ji to denote the number of
source positions in b< ji . Similar to the approach in
(Feng and Cohn, 2013), we generalize reorderings
to the following jump classes ∆φij′, j:

∆φij′, j =



↓ (’insert’), φi = 0
• (’stay’), φi > 0, j = j′
→ (’forward’), φi > 0, j = j′+1
y (’jump forward’), φi > 0, j > j′+1
← (’backward’), φi > 0, j = j′−1
x (’jump backward’), φi > 0, j < j′−1.

Figure 2 outlines the jump classes for subsequent
target positions i′ and i. As shown in Figure 3,
for source positions j̄ < j which are aligned to the

(a) forward (b) jump forward

Figure 3: Overview of the jump classes ∆ j̄, j.

same target position i, there are two possible jump
classes:

∆ j̄, j =

{
→ (’step forward’), j = j̄ +1
y (’jump forward’), j > j̄ +1.

In the following, we depict the derivations of the
EiTM and the EdTM. Although they operate in op-
posite translation directions, both models incorpo-
rate the inverted alignment bI1.

3.1 Extended Inverse Translation Model
In order to model the inverse probability p( f J1 |eI1),
the unknown inverted alignment bI1 is introduced
as a hidden variable and approximated by the
Viterbi alignment.

p( f J1 |eI1) = ∑
bI1

p( f J1 ,b
I
1|eI1)

u max
bI1

{
p( f bIb0 ,b

I
1|eI1)

}
= max

bI1

{
p( f bIb1 ,b

I
1|eI1) · p( fb0 | f bIb1 ,bI1,eI1)︸ ︷︷ ︸

deletion probability

}

The inverse probability has been decomposed into
the deletion probability p( fb0 | f bIb1 ,bI1,eI1) and the
joint probability p( f bIb1 ,b

I
1|eI1). The latter is refor-

mulated using the Markov chain rule:

p( f bIb1 ,b
I
1|eI1) =

I

∏
i=1

p( fbi ,bi|eI1, f bi−1b1 ,bi−11 ).

In order to restrict the history, we assume the prob-
ability of ( fbi ,bi) to be dependent only on the cur-
rent target word ei, its last aligned predecessor
ei′ , the corresponding alignment bi′ and the source
words fbi′ :

p( f bIb1 ,b
I
1|eI1) =

I

∏
i=1

p( fbi ,bi|ei′ ,ei, fbi′ ,bi′).

The conditional joint probability is factorized as

p( fbi ,bi|ei′ ,ei, fbi′ ,bi′) =
p( fbi |ei′ ,ei, fbi′ ,bi′ ,bi)︸ ︷︷ ︸

lexicon probability

· p(bi|ei′ ,ei, fbi′ ,bi′)︸ ︷︷ ︸
alignment probability

,

284



(a) insert (b) stay (c) forward (d) jump forward (e) backward (f) jump backward

Figure 2: Overview of the jump classes ∆φij′, j.

resulting in the lexicon probability of fbi and the
alignment probability of bi. In a nutshell, we have
decomposed the inverse probability into the fol-
lowing three probabilities:

• deletion: p( fb0 | f bIb1 ,bI1,eI1)

• lexicon:
I

∏
i=1

p( fbi |ei′ ,ei, fbi′ ,bi′ ,bi)

• alignment:
I

∏
i=1

p(bi|ei′ ,ei, fbi′ ,bi′)

Below, we show how to estimate these probabil-
ities using the EiTM deletion, lexicon and align-
ment models.

3.1.1 EiTM: Deletion Model
Due to its artificiality, e0 has no preceding target
word. We condition the deletion of fb0 only on e0
and assume conditional independence between the
unaligned source words fb0 :

p( fb0 | f bIb1 ,bI1,eI1) = ∏
j∈b0

p( f j|e0).

3.1.2 EiTM: Lexicon Model
Firstly, we apply the Markov chain rule to obtain
the factorized probabilities of single words f j.

p( fbi |ei′ ,ei, fbi′ ,bi′ ,bi) =
∏
j∈bi

p( f j|ei′ ,ei, fbi′ , fb< ji ,bi′ ,bi)

Each source word f j is dependent on all predeces-
sors fb< ji aligned to the same target word ei and
all previously aligned source words fbi′ . If we
modelled the probability conditioned on the sets
of source words fbi′ and fb< ji , this would lead to
sparsity problems due to the arbitrary number of
source words contained in the sets.

In order to avoid this, we therefore condition the
probability on the individual words contained in
fbi′ , fb< ji

. Without any additional information, we
assume all words fbi′ , fb< ji to be equally important

for the prediction of f j. Thus, we average over the
probabilities conditioned on:

• all source words f j′ aligned to the preceding
target word ei′ ,

• all preceding source words f j̄ aligned to the
current target word ei.

Moreover, we reduce the alignments (bi′ ,bi) to
their corresponding jump classes. As a final result
we obtain:

p( f j|ei′ ,ei, fbi′ , fb< ji ,bi′ ,bi) =
1

φi′ +φ
< j
i

(
∑

j′∈bi′
p( f j|ei, f j′ ,ei′ ,∆φij′, j)

+ ∑
j̄∈b< ji

p( f j|ei, f j̄,∆ j̄, j)
)

.

3.1.3 EiTM: Alignment Model
In principle, we follow the same derivation as for
the lexicon model above. The probability of a
source position j ∈ bi is computed as the average
probability of a jump from a previously aligned
source position, which either has to be aligned to
the target predecessor i′ or is a preceeding source
position aligned to the same target word ei.

p(bi|ei′ ,ei, fbi′ ,bi′) =

∏
j∈bi

1

φi′ +φ
< j
i

(
∑

j′∈bi′
p(∆φij′, j|ei, f j′ ,ei′)

+ ∑
j̄∈b< ji

p(∆ j̄, j|ei, f j̄)
)

.

To emphasize the core idea, Figure 4 demonstrates
the application on a German→English translation
example. Thin blue arcs denote the probabilities
conditioned on distinct target words ei′ and ei, the
thick red arc denotes the probabilities conditioned
on a previous source word f j̄ aligned to the cur-
rent target word. The shape of an arc symbolizes

285



Figure 4: EiTM scoring for a sentence from the
IWSLT German→English corpus including the
word alignment.

the jump class, see Figures 2 and 3. The empty
words are shown at positions j, i = 0. The deletion
is indicated by a violet circle. The EiTM proba-
bility for the whole sentence pair is computed as
follows:

p( f 91 ,b
9
1|e91) =

p( f1|e1,<s>,<s>,→) · p(→ |e1,<s>,<s>)
· p( f5|e2, f1,e1,y) · p(y |e2, f1,e1) (1)
· p( f6|e2, f1,e1,y)+ p( f6|e2, f5,→)

2

· p(y |e2, f1,e1)+ p(→ |e2, f5)
2

(2)

· p( f8|e3, f5,e2,y)+ p( f8|e3, f6,e2,y)
2

· p(y |e3, f5,e2)+ p(y |e3, f6,e2)
2

(3)

·p( f7|e4, f8,e3,←) · p(← |e4, f8,e3)
· p( f0|e5, f7,e4,↓) · p(↓ |e5, f7,e4) (4)
· p( f2|e6, f7,e4,x) · p(x |e6, f7,e4) (5)
·p( f4|e7, f2,e6,y) · p(y |e7, f2,e6)
· p( f4|e8, f4,e7,•) · p(•|e8, f4,e7) (6)
· p( f9|e9, f4,e8,y) · p(y |e9, f4,e8) (7)
·p(</s>|</s>, f9,e9,→) · p(→ |</s>, f9,e9)
· p( f3|e0). (8)

Lines (1), (2), (3) and (7) are dependencies in-
cluded in the EiTM but not in phrase translation
models due to the phrase extraction heuristics. The
dependency on multiple preceding word pairs is
exemplified in (2) and (3). (4) depicts the inser-
tion of the target word e5 = up conditioned on
the word pair (e4 = us, f7 = uns). Note that in
(5) there is no dependency of e6 = is on its pre-
decessor e5 = up and the empty word f0, but on
its last aligned predecessor e4 = us and the cor-
responding source word f7 = uns. (6) shows an
example of a source word aligned to multiple tar-
get words. The deletion probability of the source
word f3 = es is presented in (8).

3.2 Extended Direct Translation Model

So far, we have introduced the EiTM, which mod-
els the inverse translation probability p( f J1 |eI1).
Besides modelling p( f J1 |eI1) using extended trans-
lation models, our aim is to employ them to model
the direct probability p(eI1| f J1 ) as well.

For a start, the direct probability p(eI1| f J1 ) can be
modelled using the EiTM: Simply put, source and
target corpora have to be swapped for the training
of the EiTM. By doing so, the alignment has to
be inverted as well, i.e. one has to use the direct
alignment a j which denotes the sequence of target
positions i aligned to source position j. As a result,
the EiTM models p(eaJa0 ,a

J
1| f J1 ) when trained with

inverted corpora and alignments.
During the decoding process, the partial hy-

potheses are generated successively. Thus, for
each target word ei that is hypothesized, all its
predecessors have already been translated, i.e. its
last aligned predecessor ei′ and the corresponding
alignment bi′ and source words fbi′ are known.

Nevertheless, source words do not have to be
translated in monotone order. In general, it cannot
be guaranteed that the predecessor f j−1 of the first
word f j of a source phrase has been translated yet.
Therefore, the last aligned predecessor of f j and
its aligned target words are generally unknown.

As a result, when applying the EiTM within
phrase-based decoding for modelling the direct
probability p(eI1| f J1 ), dependencies beyond phrase
boundaries cannot be captured.

Thus, we additionally develop the EdTM which
models the direct translation probability p(eI1| f J1 ).
In comparison to the EiTM trained with swapped
corpora and alignments, EdTM incorporates de-
pendencies beyond phrase boundaries by keep-

286



ing the inverted alignment bI1 instead of using a
J
1.

Analogue to the EiTM, the hidden alignment bI1 is
approximated by the Viterbi alignment.

p(eI1| f J1 ) u max
bI1

{
p(e0| f bIb0 )︸ ︷︷ ︸

deletion probability

·p(eI1,bI1| f bIb0 ,e0)
}

Applying the Markov chain rule and assuming
(ei,bi) to be dependent only on the aligned source
words fbi , the previously aligned target word ei′
as well as the corresponding alignment bi′ and the
source words fbi′ , we obtain:

p(eI1,b
I
1| f bIb0 ,e0) =

I

∏
i=1

p(ei,bi| fbi′ , fbi ,ei′ ,bi′).

We factorize the joint probability to obtain the lex-
icon probability of ei and the alignment probabil-
ity of bi.

p(ei,bi| fbi′ , fbi ,ei′ ,bi′) =
p(ei| fbi′ , fbi ,ei′ ,bi′ ,bi)︸ ︷︷ ︸

lexicon probability

· p(bi| fbi′ , fbi ,ei′ ,bi′)︸ ︷︷ ︸
alignment probability

The direct probability has been decomposed into
the following three probabilities.

• deletion: p(e0| f bIb0 )

• lexicon:
I

∏
i=1

p(ei| fbi′ , fbi ,ei′ ,bi′ ,bi)

• alignment:
I

∏
i=1

p(bi| fbi′ , fbi ,ei′ ,bi′)

Next, we introduce the corresponding EdTM dele-
tion, lexicon and alignment models.

3.2.1 EdTM: Deletion Model
The EdTM deletion model approximates the prob-
ability of e0 conditioned on all unaligned source
words fb0 and is obtained by averaging over all
unaligned source words:

p(e0| f bIb0 ) = ∑
j∈b0

p(e0| f j)
φ0

.

3.2.2 EdTM: Lexicon Model
In contrast to the derivation of EiTM, the Markov
chain rule cannot be applied at this point, since we
do not model the probability of fbi , but the prob-
ability of ei conditioned on fbi . Thus, we average

over all aligned source words fbi , which results in:

p(ei| fbi′ , fbi ,ei′ ,bi′ ,bi) =
1
φi ∑j∈bi

1

φi′ +φ
< j
i

(
∑

j′∈bi′
p(ei| f j,ei′ , f j′ ,∆φij′, j)

+ ∑
j̄∈b< ji

p(ei| f j, f j̄,∆ j̄, j)
)

.

3.2.3 EdTM: Alignment Model
Applying the same assumptions as for the lexicon
model, the EdTM alignment model results in:

p(bi| fbi′ , fbi ,ei′ ,bi′) =
1
φi ∑j∈bi

1

φi′ +φ
< j
i

(
∑

j′∈bi′
p(∆φij′, j| f j,ei′ , f j′)

+ ∑
j̄∈b< ji

p(∆ j̄, j| f j, f j̄)
)

.

3.3 Count Models and Smoothing

So far, we have introduced the ETM and shown
how to include unaligned words and multiple word
dependencies. However, there are various possi-
bilities to train the lexicon and alignment proba-
bilities derived in Subsections 3.1 and 3.2.

As a starting point, we apply relative frequen-
cies obtained from bilingual training data, where
the Viterbi alignment is estimated using GIZA++
(Och and Ney, 2003). In order to address data
sparseness, we apply interpolated Kneser-Ney
smoothing as described in (Chen and Goodman,
1998). In comparison to monolingual n-grams
used in LMs, we lack any clear order of e, f , e′,
f ′ and ∆, since they include bilingual and reorder-
ing information. Similar to the approach taken by
Mariño et al. (2006), we model the probability of
the bilingual word pair (e, f ) given its predecessor
(e′, f ′,∆) which also includes the jump class. The
EdTM lexicon model for dependencies on previ-
ously aligned target words is computed as

p(e| f ,e′, f ′,∆) = p(e, f |e
′, f ′,∆)

p(·, f |e′, f ′,∆) , (9)

where p(e, f |e′, f ′,∆) is the bigram distribution
of (e, f ) given its predecessor (e′, f ′,∆) with in-
terpolated Kneser-Ney smoothing. The denomi-
nator p(·, f |e′, f ′,∆) is obtained by marginalizing
p(e, f |e′, f ′,∆) over all target words e. We follow
the same approach for all other models in analogy.

287



IWSLT IWSLT BOLT BOLT
German English English French Chinese English Arabic English

Sentences 4.32M 26.05M 4.08M 0.92M
Run. Words 108M 109M 698M 810M 78M 86M 14M 16M
Vocabulary 836K 792K 2119K 2139K 384K 817K 285K 203K

Table 1: Statistics for the bilingual training data of the IWSLT 2014 German→English, English→French
and the DARPA BOLT Chinese→English, Arabic→English translation tasks.

4 Integration into Phrase-based
Decoding

In this work, we apply a standard phrase-based
translation system (Koehn et al., 2003). The de-
coding process is implemented as a beam search
for the best translation given a set of models
hm(eI1,s

K
1 , f

J
1 ). The goal of search is to maximize

the log-linear feature score (Och and Ney, 2004):

êÎ1 = argmax
I,eI1,s

K
1

{
M

∑
m=1

λmhm(eI1,s
K
1 , f

J
1 )

}
, (10)

where sK1 = s1 . . .sK is the hidden phrase align-
ment. The feature weights λm are tuned with mini-
mum error rate training (MERT) (Och, 2003). The
models hm, that are part of all baselines presented
in this work, are phrasal and lexical translation
scores in both directions, an n-gram LM, a sim-
ple distance-based distortion model and word and
phrase penalties. All phrase pairs that are licensed
by the word alignment are extracted from the train-
ing corpus and their probabilities estimated as rel-
ative frequencies. Moreover, the word alignment
each phrase pair has been extracted from is mem-
orized in the phrase table.

Our extended translation models are integrated
into this framework as additional features hm.
They are trained in both directions on a bilin-
gual corpus and the Viterbi alignment, result-
ing in four additional features. When train-
ing in the Target→Source direction, the align-
ment direction is also swapped. Thus, EiTM and
EdTM have the advantage of including context be-
yond phrase boundaries only when trained in the
Source→Target direction.

To include the extended translation models into
the phrasal decoder, the source position aligned
to the last (not inserted) target word of the pre-
viously translated phrase has to be memorized in
the search state of a partial hypothesis. Although
this slightly affects hypothesis recombination and

therefore leads to a larger search space, in prac-
tice it does not degrade the search accuracy, as
experiments with relaxed pruning parameters have
shown.

5 Evaluation

We perform experiments on the large-
scale IWSLT 20142 (Cettolo et al., 2014)
German→English, English→French and the
large-scale DARPA BOLT Chinese→English,
Arabic→English tasks. As mentioned in Section
4, all baseline systems include phrasal and lexical
smoothing scores trained in both directions.
Word alignments are trained with GIZA++, by
sequentially running 5 iterations each for the
IBM-1, HMM and IBM-4 alignment models.

The domain of IWSLT consists of lecture-type
talks presented at TED conferences which are
also available online3. The baseline systems are
trained on all provided bilingual data. All sys-
tems are optimized on the dev2010 and eval-
uated on the test2010 corpus. The ETM is
trained on the TED portions of the data: 138K sen-
tences for German→English and 185K sentences
for English→French.

For German→English, to estimate the 4-gram
LM, we additionally make use of parts of the
Shuffled News, LDC English Gigaword and 109-
French-English corpora, selected by a cross-
entropy difference criterion (Moore and Lewis,
2010). In total, 1.7 billion running words are
taken for LM training. For English→French, we
use a large general domain 5-gram LM and an in-
domain 5-gram LM. Both are estimated with the
KenLM toolkit (Heafield et al., 2013) using inter-
polated Kneser-Ney smoothing. For the general
domain LM, we first select 12 of the English Shuf-
fled News, 14 of the French Shuffled News as well
as both the English and French Gigaword corpora

2http://www.iwslt2014.org
3http://www.ted.com/

288



by the same cross-entropy difference criterion. By
concatenating this selection with all available re-
maining monolingual data, we build an unpruned
LM.

The BOLT tasks are evaluated on the ”discus-
sion forum” domain. For Chinese→English, the
baseline is trained on 4.08M general domain sen-
tence pairs and the 5-gram LM on 2.9 billion run-
ning words in total. The ETM is trained on an in-
domain subset of 67.8K sentences and the test set
contains 1844 sentences. For the Arabic→English
BOLT task, we use only the in-domain data for
training the baseline and the ETM. The training
and test sets contain text drawn from discussion
forums in Egyptian Arabic. The evaluation set
contains 1510 bilingual sentence pairs.

The baseline systems for all tasks - except the
Arabic→English BOLT task, where preliminary
experiments showed no improvement - contain a
7-gram word cluster language model (Wuebker et
al., 2013) and for comparison, we also experiment
with a hierarchical reordering model (HRM) (Gal-
ley and Manning, 2008). When integrated into a
phrase-based decoder, Durrani et al. (2013b) have
shown the OSM to outperform bilingual LMs on
MTUs. Therefore, we directly compare ourselves
with a 7-gram OSM implemented into our phrase-
based decoder as an additional feature. The OSM
is trained on the same data as the ETM for all
tasks. Bilingual data statistics for all tasks are
shown in Table 1. For each system setting we eval-
uate three MERT runs using multeval (Clark et
al., 2011). Results are reported in BLEU (Papineni
et al., 2001) and TER (Snover et al., 2006). The
optimization criterion for all experiments is BLEU.

5.1 Model parameters

To measure the complexity of the extended trans-
lation models in comparison to the phrase-based
translation model, we count the number of param-
eters to be trained for each.

Table 2 illustrates the phrase-table and ETM
count table entries for the BOLT Arabic→English
translation task, where both the phrase-based base-
line and the ETM are trained on the same bilin-
gual data consisting of 0.92M bilingual sentence
pairs. Here, we only show the numbers for the
Source→Target direction, as the numbers for the
Target→Source direction are similar. The EdTM
and EiTM each have roughly 35M parameters to
be trained, i.e. there are approximately 70M pa-

model # parameters

phrase-based translation 57,155,149

EdTM 35,511,396
lexicon 19,899,812
alignment 15,276,718
deletion 334,866

EiTM 34,994,534
lexicon 20,153,114
alignment 14,791,722
deletion 49,698

Table 2: The number of model parameters for the
BOLT Arabic→English bilingual training data af-
ter filtering.

BLEU TER

Baseline + HRM 30.7 49.3

+ EiTM + EdTM
Ge↔En none 31.4 48.3
none Ge↔En 31.6 48.1
Ge→En Ge→En 31.6 48.2
Ge↔En Ge↔En 31.8 48.2

Table 3: Results for the German→English IWSLT
data. The systems are optimized with MERT on
the dev2010 set. All results are statistically sig-
nificant with ≥99% confidence.

rameters to be trained for the ETM in total. This
is slightly more than the 57M parameters for the
phrase translation model.

5.2 Results
In order to compare the effect of the EiTM and
EdTM used in a phrase-based decoder, we have
trained the baseline including the HRM as de-
scribed above on the full German→English bilin-
gual data of the IWSLT task and the extended
translation models on the TED data. The results
evaluated on test2010 are shown in Table 3.

Including the EiTM trained in both
German→English and English→German di-
rections into the phrasal decoder yields an
absolute improvement of +0.7 BLEU and -1.0
TER, whereas including the EdTM yields +0.9
BLEU and -1.2 TER. This underlines that the
EdTM is more suitable for translation than the
EiTM because it predicts the direct probability

289



Ge-En En-Fr Zh-En Ar-En

Baseline 30.6 32.8 16.5 23.8

+ ETM 31.4 33.8 16.8 24.1
+ OSM 31.6 34.1 17.3 24.1
+ HRM 30.7 33.1 17.0 24.0

+ ETM 31.8 33.9 17.5 24.4
+ OSM 31.8 34.5 17.6 24.1

Table 4: Comparison of ETM to the HRM and
OSM measured in BLEU. Statistically significant
improvements with ≥99% confidence are printed
in boldface.

of a target word, which corresponds to the actual
translation direction. Note, that both EiTM and
EdTM lose the advantage of modelling dependen-
cies beyond phrase boundaries when trained in the
inverse direction English→German. Therefore,
we have evaluated their joint performance when
trained only in German→English direction,
which is similar to the performance of EdTM
trained in both directions. This can be due to
the fact that even though the EiTM trained in
German→English direction incorporates depen-
dencies beyond phrase boundaries, the EdTM
trained in English→German direction profits from
the better suited direct translation probability. The
full ETM, i.e. EiTM and EdTM trained in both
directions, yields the best overall performance
gain of +1.1 BLEU and -1.1 TER over the baseline.

Moreover, we evaluate the performance of the
(full) ETM compared to the HRM and a 7-gram
OSM, which are all introduced as additional fea-
tures into the log-linear framework of the base-
line phrase-based decoder. The results are pre-
sented in Table 4. The ETM performs simi-
larly to the HRM for the Chinese→English and
Arabic→English tasks, resulting in +0.3 BLEU
over the PBT baseline. For both IWSLT tasks,
the ETM outperforms the HRM by +0.7 BLEU,
gaining +0.8 BLEU for the German→English and
+1.0 BLEU for the German→English task over the
PBT baseline. The context captured by the ETM
corresponds roughly to the context captured by a
3-gram OSM. Bearing this in mind, we compare
the ETM to a 7-gram OSM, which yields +0.25
BLEU more than the ETM averaged over the four
language pairs. Comparing the OSM vocabulary
of 1.5M words for the Arabic→English task to the

285K words in the Arabic corpus, this results in
an ETM vocabulary 5-times smaller than the OSM
vocabulary.

We also compare the ETM to the OSM on top of
a PBT system that also includes the HRM, which
is shown in the last two lines of Table 4. The per-
formance of the ETM benefits from the informa-
tion introduced by the HRM, as the gain of us-
ing the ETM is further increased by +0.15 BLEU
on average. Overall, the ETM gains consistent
and statistically significant improvements of +0.7
BLEU on average for all four language pairs over
a state-of-the-art phrase-based decoder including
the HRM. On the other hand, OSM seems to have
a higher overlap with HRM, as the gain of OSM
compared to ETM is reduced to +0.1 BLEU on av-
erage. Thus, on top of the phrase-based system in-
cluding the HRM, the ETM including a bilingual
word pair and the corresponding reordering jump
class proves to be competitive to a 7-gram OSM.

6 Discussion

We have integrated two variants of a novel ex-
tended translation model into a state-of-the-art
phrase-based decoder. The ETM captures lexical
and reordering context beyond phrase boundaries
in both the Source→Target and Target→Source
directions. Further, the model potentially captures
long-range reorderings and utilizes multiple and
empty alignments, allowing for target insertions
and source deletions. As an initial step, we have
implemented the ETM using relative frequencies
with interpolated Kneser-Ney smoothing. Its con-
sistent and statistically significant improvement
of up to +1.1 BLEU and -1.1 TER respectively
+0.7 BLEU on average has been shown for four
large-scale translation tasks, outperforming com-
petitive phrase-based systems that include lexical
and phrase translation models and hierarchical re-
ordering models.

Compared to a 7-gram OSM, the ETM is much
simpler in design: It uses a smaller vocabulary
size, estimates the probability of single words in-
stead of bilingual MTUs, avoids the need of re-
ordering gaps and includes less lexical and re-
ordering context, thus being less sparse. For
all that, it performs competitively to a 7-gram
OSM on top of phrase-based systems including
the HRM. This fact underlines the advantages in-
troduced by the ETM: It operates on words rather
than MTUs, explicitly models multiple alignments

290



instead of incorporating linear dependencies and
models reorderings in a less complex way.

So far we have used the ETM as an additional
feature in a phrase-based decoder, but we believe
that the usage of such a decoder is a limitation.
First, the ETM is estimated on alignments, which
themselves are optimized for the IBM models.
Second, decoding is performed using phrases that
are extracted from the alignments using heuristics.
Therefore, the potential of a phrase-based decoder
is also limited by these heuristics.

Based on these facts, we believe that the ETM
will show its full potential when it is also inte-
grated into the training of the alignment, leading
not only to a higher alignment quality, but also to a
joint optimization of the alignments and the ETM.
Further, directly applying the ETM within a word-
based decoder utilizing an extended translation
and reordering context will redundantize phrases
and thus any extraction heuristics. We believe that
a consistent framework where the ETM is applied
in both training the alignments and decoding will
significantly advance machine translation.

For the short term, we will investigate better
smoothing strategies and the possibilities of using
neural networks instead of count models.

Acknowledgements

This work has received funding from the Euro-
pean Union’s Horizon 2020 research and innova-
tion programme under grant agreement no 645452
(QT21). This material is partially based upon
work supported by the DARPA BOLT project un-
der Contract No. HR0011- 12-C-0015. Any opin-
ions, findings and conclusions or recommenda-
tions expressed in this material are those of the
authors and do not necessarily reflect the views of
DARPA.

References
Michael Auli, Michel Galley, Chris Quirk, and Geof-

frey Zweig. 2013. Joint Language and Translation
Modeling with Recurrent Neural Networks. In Con-
ference on Empirical Methods in Natural Language
Processing, pages 1044–1054, Seattle, USA, Octo-
ber.

Peter F. Brown, John Cocke, Stephan A. Della Pietra,
Vincent J. Della Pietra, Fredrick Jelinek, John D.
Lafferty, Robert L. Mercer, and Paul S. Rossin.
1990. A Statistical Approach to Machine Transla-
tion. Computational Linguistics, 16(2):79–85, June.

Peter F. Brown, Stephan A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
Mathematics of Statistical Machine Translation: Pa-
rameter Estimation. Computational Linguistics,
19(2):263–311, June.

Mauro Cettolo, Jan Niehues, Sebastian Stüker, Luisa
Bentivogli, and Marcello Federico. 2014. Report on
the 11th iwslt evaluation campaign, iwslt 2014. In
International Workshop on Spoken Language Trans-
lation, pages 2–11, Lake Tahoe, CA, USA, Decem-
ber.

Stanley F. Chen and Joshuo Goodman. 1998. An
Empirical Study of Smoothing Techniques for Lan-
guage Modeling. Technical Report TR-10-98, Com-
puter Science Group, Harvard University, Cam-
bridge, MA, August.

Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis test-
ing for statistical machine translation: Controlling
for optimizer instability. In 49th Annual Meet-
ing of the Association for Computational Linguis-
tics:shortpapers, pages 176–181, Portland, Oregon,
June.

Yonggang Deng and William Byrne. 2005. Hmm word
and phrase alignment for statistical machine transla-
tion. In Proceedings of Human Language Technol-
ogy Conference and Conference on Empirical Meth-
ods in Natural Language Processing, pages 169–
176, Vancouver, British Columbia, Canada, October.
Association for Computational Linguistics.

Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas
Lamar, Richard Schwartz, and John Makhoul. 2014.
Fast and Robust Neural Network Joint Models for
Statistical Machine Translation. In 52nd Annual
Meeting of the Association for Computational Lin-
guistics, pages 1370–1380, Baltimore, MD, USA,
June.

Nadir Durrani, Helmut Schmid, and Alexander Fraser.
2011. A joint sequence translation model with in-
tegrated reordering. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
1045–1054, Portland, Oregon, USA, June. Associa-
tion for Computational Linguistics.

Nadir Durrani, Alexander Fraser, and Helmut Schmid.
2013a. Model with minimal translation units, but
decode with phrases. In Proceedings of the 2013
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 1–11, Atlanta, Geor-
gia, June. Association for Computational Linguis-
tics.

Nadir Durrani, Alexander Fraser, Helmut Schmid,
Hieu Hoang, and Philipp Koehn. 2013b. Can
markov models over minimal translation units help
phrase-based smt? In Proceedings of the 51st An-
nual Meeting of the Association for Computational

291



Linguistics (Volume 2: Short Papers), pages 399–
405, Sofia, Bulgaria, August. Association for Com-
putational Linguistics.

Nadir Durrani, Philipp Koehn, Helmut Schmid, and
Alexander Fraser. 2014. Investigating the useful-
ness of generalized word representations in smt. In
COLING, Dublin, Ireland, aug.

Yang Feng and Trevor Cohn. 2013. A markov
model of machine translation using non-parametric
bayesian inference. In 51st Annual Meeting of the
Association for Computational Linguistics, pages
333–342, Sofia, Bulgaria, August.

Minwei Feng, Jan-Thorsten Peter, and Hermann Ney.
2013. Advancements in reordering models for sta-
tistical machine translation. In Annual Meeting
of the Assoc. for Computational Linguistics, pages
322–332, Sofia, Bulgaria, August.

Yang Feng, Trevor Cohn, and Xinkai Du. 2014. Fac-
tored markov translation with robust modeling. In
Proceedings of the Eighteenth Conference on Com-
putational Natural Language Learning, pages 151–
159, Ann Arbor, Michigan, June. Association for
Computational Linguistics.

Michel Galley and Christopher D. Manning. 2008.
A simple and effective hierarchical phrase reorder-
ing model. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP ’08, pages 848–856, Stroudsburg, PA,
USA. Association for Computational Linguistics.

Andreas Guta, Tamer Alkhouli, Jan-Thorsten Peter, Jo-
ern Wuebker, and Hermann Ney. 2015. A com-
parison between count and neural network models
based on joint translation and reordering sequences.
In Proceedings of the 2015 Conference on Empirical
Methods in Natural Language Processing, Lisboa,
Portugal, September. Association for Computational
Linguistics. to appear.

Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013. Scalable modi-
fied Kneser-Ney language model estimation. In Pro-
ceedings of the 51st Annual Meeting of the Associa-
tion for Computational Linguistics, pages 690–696,
Sofia, Bulgaria, August.

Yuening Hu, Michael Auli, Qin Gao, and Jianfeng Gao.
2014. Minimum translation modeling with recur-
rent neural networks. In Proceedings of the 14th
Conference of the European Chapter of the Associ-
ation for Computational Linguistics, pages 20–29,
Gothenburg, Sweden, April. Association for Com-
putational Linguistics.

P. Koehn, F. J. Och, and D. Marcu. 2003. Statisti-
cal Phrase-Based Translation. In Proceedings of the
2003 Meeting of the North American chapter of the
Association for Computational Linguistics (NAACL-
03), pages 127–133, Edmonton, Alberta.

Hai Son Le, Alexandre Allauzen, and François Yvon.
2012. Continuous Space Translation Models with
Neural Networks. In Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
39–48, Montreal, Canada, June.

José B Mariño, Rafael E Banchs, Josep M Crego, Adrià
de Gispert, Patrik Lambert, José A R Fonollosa, and
Marta R Costa-jussà. 2006. N-gram-based Machine
Translation. Comput. Linguist., 32(4):527–549, De-
cember.

R.C. Moore and W. Lewis. 2010. Intelligent Selection
of Language Model Training Data. In ACL (Short
Papers), pages 220–224, Uppsala, Sweden, July.

Jan Niehues, Teresa Herrmann, Stephan Vogel, and
Alex Waibel, 2011. Proceedings of the Sixth Work-
shop on Statistical Machine Translation, chapter
Wider Context by Using Bilingual Language Mod-
els in Machine Translation, pages 198–206. Associ-
ation for Computational Linguistics.

Franz J. Och and Hermann Ney. 2002. Discriminative
Training and Maximum Entropy Models for Statis-
tical Machine Translation. In Proc. of the 40th An-
nual Meeting of the Association for Computational
Linguistics (ACL), pages 295–302, Philadelphia, PA,
July.

Franz J. Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19–51,
March.

Franz Josef Och and Hermann Ney. 2004. The Align-
ment Template Approach to Statistical Machine
Translation. Computational Linguistics, 30(4):417–
449, December.

Franz J. Och, Christoph Tillmann, and Hermann Ney.
1999. Improved Alignment Models for Statistical
Machine Translation. In Proc. Joint SIGDAT Conf.
on Empirical Methods in Natural Language Pro-
cessing and Very Large Corpora, pages 20–28, Uni-
versity of Maryland, College Park, MD, June.

Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proc. of the
41th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 160–167, Sapporo,
Japan, July.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a Method for Automatic
Evaluation of Machine Translation. IBM Research
Report RC22176 (W0109-022), IBM Research Di-
vision, Thomas J. Watson Research Center, P.O. Box
218, Yorktown Heights, NY 10598, September.

Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study of
Translation Edit Rate with Targeted Human Annota-
tion. In Proceedings of the 7th Conference of the As-
sociation for Machine Translation in the Americas,

292



pages 223–231, Cambridge, Massachusetts, USA,
August.

Darelene Stewart, Roland Kuhn, Eric Joanis, and
George Foster. 2014. Coarse split and lump bilin-
gual languagemodels for richer source information
in smt. In AMTA, Vancouver, BC, Canada, oct.

Martin Sundermeyer, Tamer Alkhouli, Joern Wuebker,
and Hermann Ney. 2014. Translation modeling
with bidirectional recurrent neural networks. In
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 14–25, Doha, Qatar, Octo-
ber.

Joern Wuebker, Stephan Peitz, Felix Rietig, and Her-
mann Ney. 2013. Improving statistical machine
translation with word class models. In Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 1377–1381, Seattle, USA, October.

Richard Zens, Franz Josef Och, and Hermann Ney.
2002. Phrase-Based Statistical Machine Transla-
tion. In 25th German Conf. on Artificial Intelligence
(KI2002), pages 18–32, Aachen, Germany, Septem-
ber. Springer Verlag.

Hui Zhang, Kristina Toutanova, Chris Quirk, and Jian-
feng Gao. 2013. Beyond left-to-right: Multiple de-
composition structures for smt. In Proceedings of
the 2013 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, pages 12–21, At-
lanta, Georgia, June. Association for Computational
Linguistics.

293


