



















































YiSi - a Unified Semantic MT Quality Evaluation and Estimation Metric for Languages with Different Levels of Available Resources


Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 2: Shared Task Papers (Day 1) pages 507–513
Florence, Italy, August 1-2, 2019. c©2019 Association for Computational Linguistics

507

YiSi - A unified semantic MT quality evaluation and estimation metric
for languages with different levels of available resources

Chi-kiu Lo
NRC-CNRC

Multilingual Text Processing
National Research Council Canada

1200 Montreal Road, Ottawa, ON K1A 0R6, Canada
chikiu.lo@nrc-cnrc.gc.ca

Abstract

We present YiSi, a unified automatic seman-
tic machine translation quality evaluation and
estimation metric for languages with different
levels of available resources. Underneath the
interface with different language resources set-
tings, YiSi uses the same representation for
the two sentences in assessment. Besides, we
show significant improvement in the correla-
tion of YiSi-1’s scores with human judgment is
made by using contextual embeddings in mul-
tilingual BERT–Bidirectional Encoder Repre-
sentations from Transformers to evaluate lex-
ical semantic similarity. YiSi is open source
and publicly available.

1 Introduction

A good automatic MT quality metric is one that
closely reflect the usefulness of the translation, in
terms of assisting human readers to understand the
meaning of the input sentence. BLEU (Papineni
et al., 2002) has long been shown not to correlate
well with human judgment on translation qual-
ity (Machacek and Bojar, 2014; Stanojević et al.,
2015; Bojar et al., 2016, 2017; Ma et al., 2018).
However, it is still the most commonly used metric
for reporting quality of machine translation sys-
tems. One of the major reasons is that BLEU is
ready-to-deploy to all languages due to its simplic-
ity. Semantic MT evaluation metrics, such as ME-
TEOR (Denkowski and Lavie, 2014) and MEANT
(Lo, 2017), require additional linguistic resources
to more accurately evaluate the meaning similarity
between the MT output and the reference transla-
tion. The lower portability hinders the wide adop-
tion of these metrics.

We, therefore, propose a unified framework,
YiSi, for MT quality evaluation and estimation
that take advantage of both metric paradigms by
providing options to fallback to surface-level lexi-

cal similarity when semantic models are not avail-
able for the languages in assessment.

YiSi were first used in WMT 2018 metrics
shared task (Ma et al., 2018) and performed well
and consistently at segment-level across the tested
language pairs in correlating with human judg-
ment. An YiSi based system successfully served in
WMT2018 parallel corpus filtering task (Lo et al.,
2018).

This year, instead of using word2vec
(Mikolov et al., 2013) to evaluate lexical seman-
tic similarity in YiSi, we use BERT –Bidirectional
Encoder Representation from Transformers (De-
vlin et al., 2018). YiSi is open source and publicly
available.1

2 YiSi

YiSi2 is a unified semantic MT quality evaluation
and estimation metric for languages with different
levels of available resources. Inspired by MEANT
(Lo, 2017), YiSi-1 is a MT quality evaluation met-
ric that measures the similarity between a machine
translation and human references by aggregating
the weighted distributional lexical semantic sim-
ilarities and optionally incorporating shallow se-
mantic structures. YiSi-0 is the degenerate ver-
sion of YiSi-1 that is ready-to-deploy to any lan-
guages. It uses longest common character sub-
string to measure the lexical similarity. YiSi-2 is
the bilingual, reference-less version, which uses
bilingual embeddings to evaluate crosslingual lex-
ical semantic similarity between the input and MT
output. Like YiSi-1, YiSi-2 can exploit shallow
semantic structures as well.

YiSi-0 and YiSi-1 were first used in WMT
2018 metrics shared task (Ma et al., 2018) and
performed well and consistently at segment-level

1http://chikiu-jackie-lo.org/home/index.php/yisi
2YiSi is the romanization of the Cantonese word 意思

(‘meaning’).



508

Figure 1: Graphical representation of the computation of YiSi.

across the tested language pairs in correlating with
human judgment. While YiSi-1 also successfully
served in WMT2018 parallel corpus filtering task,
YiSi-2 showed comparable accuracy in our inter-
nal experiments (Lo et al., 2018).

2.1 Overview
Following the guiding principle that a good MT
quality metric reflects how well human readers un-
derstand the meaning of the input sentence, YiSi is
the weighted f-scores over corresponding seman-
tic frames and role fillers in the two sentences E
and F in assessment. The procedure of computing
YiSi is described as follow:

1. Apply a shallow semantic parser to both E
and F .

2. Apply the maximum weighted bipartite
matching algorithm to align the semantic
frames between E and F according to the
lexical similarities of the predicates.

3. For each pair of aligned frames, apply the
maximum weighted bipartite matching algo-
rithm to align the arguments between E and
F according to the lexical similarity of role
fillers.

4. Compute the weighted f-score over the

matching role labels of these aligned predi-
cates and role fillers according to the follow-
ing definitions: (Figure 1 is the graphical rep-
resentation of the following computation.)

w (e) = lexical weight of e

s(e, f) = lexical similarity of e and f

where s(e, f) is the lexical similarity and it is
weighted byw(e) andw(f) for computing phrasal
precision and recall respectively. Different vari-
ants of YiSi have different definition of lexical
similarities and weights depend on the resources
available for the assessment settings. By aggregat-
ing the weighted lexical similarities into n-gram
similarities, we then align the bag of n-grams in
the two sentences using maximum alignment on
the n-gram similarities. The phrasal similarity pre-
cision, sp, and recall, sr, (as defined below) are the
weighted average of the similarities of the aligned
n-gram.

sp(
−→e ,
−→
f ) =

∑
a

max
b

n−1∑
k=0

w (ea+k) · s (ea+k, fb+k)

∑
a

n−1∑
k=0

w (ea+k)

sr(
−→e ,
−→
f ) =

∑
b

max
a

n−1∑
k=0

w (fb+k) · s (ea+k, fb+k)

∑
b

n−1∑
k=0

w (fb+k)



509

With the phrasal semantic precision and recall,
we compute the structural semantic precision and
recall as follow:

qEi,j = argument j of aligned frame i in E

qFi,j = argument j of aligned frame i in F

wEi =
#units filled in aligned frame i of E

total #units in E

wFi =
#units filled in aligned frame i of F

total #units in F

wj = count (argument j in F)
wt = 0.25 ∗ count (predicate in F)

srlp =

∑
i
wei

wtsp(
−→ei,t,
−→
fi,t)+

∑
j
wjsp(

−→ei,j ,
−→
fi,j)

wt+
∑
j
wj |qei,j |∑

i
wei

srlr =

∑
i
wfi

wtsr(
−→ei,t,
−→
fi,t)+

∑
j
wjsr(

−→ei,j ,
−→
fi,j)

wt+
∑
j
wj |qfi,j |∑

i
wfi

wherewt is the weight of the lexical similarities of
the aligned predicates in step 2. wj is the weight
of the phrasal similarities of the role fillers of the
arguments of role type j of the aligned frames be-
tween the reference translations and the MT out-
put in step 3 if their role types are matching. As
in (Lo, 2017), we merge the semantic role labels
into 8 role types (who, did, what, whom, when,
where, why, how) for more robust performance.
Thus, there is a total of 8 weights for the set of se-
mantic role types in YiSi estimated by type counts
in the document F. The frame precision/recall is
the weighted sum of the phrasal precision/recall
of the aligned role fillers. The token coverage wei
and wfi estimate the importance of frame i in the
sentence E and F . The structural semantic preci-
sion and recall is the weighted average of all the
aligned frames in sentence E and F respectively.

Now, the overall precision and recall is the
weighted sum of the phrasal precision and recall
of the whole sentence of −−−→esent and

−−−→
fsent, like in

the following:

precision = β · srlp + (1− β) · sp(−−−→esent,
−−−→
fsent)

recall = β · srlr + (1− β) · sr(−−−→esent,
−−−→
fsent)

It is important to note that the weight β should
NOT be interpreted as the importance of the struc-
tural semantic similarity in YiSi because there is a

Figure 2: Resources used in YiSi-0.

huge overlap in the structural semantic similarity
and the phrasal semantic similarity. Instead, we
should pay attention to the significant difference
in the performance of YiSi with and without struc-
tural semantic similarity, especially in YiSi-2, the
crosslingual variant. In this experiment, β is set to
0.1.

Finally, the weight α for the precision and recall
is introduced for different usages of YiSi. α should
be set to 0.7 to make YiSi more recall-oriented
when it is used for MT evaluation. When used for
MT system optimization, α should be set to 0.5 to
balance precision and recall.

YiSi =
precision · recall

α · precision + (1− α) · recall

In the following, we describe how we estimate
the lexical similarity s(e, f) and lexical weight
w(e) under different resource conditions.

2.1.1 YiSi-0: quality evaluation metric for
extremely low resource languages

YiSi-0 is the degenerate resource-free variant of
YiSi for MT quality evaluation, where sentence E
is the MT output and sentence F is the reference.
Figure 2 shows the resources used in YiSi-0.

YiSi-0 uses the longest common character sub-
string accuracy to evaluate lexical similarity be-
tween the MT output and human reference. Since
the MT output and the human reference are both
in the same language, the lexical weight w(e) of
word e in the translation and the lexical weight
w(f) of word f in the reference are both estimated
by the inverse-document-frequency of those words
in the reference document F. Thus, formally YiSi-



510

Figure 3: Resources used in YiSi-1. The dash arrow
means that the semantic parser is optional.

0 is defined as follow:

l(e, f) = longest common substring of e and f

s0(e, f) =
2 ∗ l(e, f)
|e|+ |f |

w (e) = idf(e) = log(1 +
|F|+ 1
|F∃e|+ 1

)

YiSi-0 = YiSi(s=s0, β=0.0, E=MT, F=REF)

2.1.2 YiSi-1: quality evaluation metric with
access to an embedding model

YiSi-1 is the monolingual variant of YiSi for MT
quality evaluation, where sentence E is the MT
output and sentence F is the reference. Figure 3
shows the resources used in YiSi-1.

YiSi-1 requires an embedding model to evalu-
ate lexical semantic similarity and optionally re-
quires a semantic role labeler in the output lan-
guage for evaluating structural semantic similarity.
The lexical semantic similarity is the cosine simi-
larity of the embeddings from the lexical represen-
tation model. Similar to YiSi-0, the lexical weight
w(u) of word unit u in the MT and the reference
are estimated by the inverse-document-frequency
of that word in the reference document F. Thus,
formally YiSi-1 is defined as follow:

v(u) = embedding of unit u

s1(e, f) = cos(v(e), v(f))

w (u) = idf(u) = log(1 +
|F|+ 1
|F∃u|+ 1

)

YiSi-1 = YiSi(s=s1, β=0.0, E=MT, F=REF)

YiSi-1 srl = YiSi(s=s1, β=0.1, E=MT, F=REF)

Figure 4: Resources used in YiSi-2. Arrows in green
depict resources in target language and arrows in or-
ange depict resources in source language. The dash ar-
rows mean that the semantic parsers are optional.

2.1.3 YiSi-2: quality estimation metric for
languages with access to a bilingual
embedding model

YiSi-2 is the cross-lingual variant of YiSi for MT
quality estimation, where sentence E is the MT
output and sentence F is the input. Figure 4 shows
the resources used in YiSi-2.

YiSi-2 requires a cross-lingual embedding
model for evaluating cross-lingual lexical seman-
tic similarity and optionally requires a semantic
role labeler in both the input and the output lan-
guages for evaluating structural semantic similar-
ity. The lexical semantic similarity is the co-
sine similarity of the embeddings from the cross-
lingual lexical representation model. The lexical
weight w(e) of word unit e in the MT is estimated
by the inversion-document-frequency of the word
in the MT document E while the lexical weight
w(f) of word unit f in the MT is estimated by the
inversion-document-frequency of the word in the
MT document F. Thus, formally YiSi-2 is defined
as follow:

v(u) = embedding of unit u

s2(e, f) = cos(v(e), v(f))

w (e) = idf(e) = log(1 +
|E|+ 1
|E∃e|+ 1

)

w (f) = idf(f) = log(1 +
|F|+ 1
|F∃f |+ 1

)

YiSi-2 = YiSi(s=s2, β=0.0, E=MT, F=IN)

YiSi-2 srl = YiSi(s=s2, β=0.1, E=MT, F=IN)



511

2.2 Using BERT for lexical unit semantic
similarity

In WMT 2018 metrics shared task, YiSi-1 uses
word2vec (Mikolov et al., 2013) to evaluate lex-
ical semantic similarity between the MT output
and the human reference at word level. The short-
comings of this kind of static embedding models
(also including but not limited to GloVe (Pen-
nington et al., 2014)) is that they provide the
same embedding representation for the same word
without reflecting context of different sentences.
In contrast, BERT (Devlin et al., 2018) uses a
bidirectional transformer encoder (Vaswani et al.,
2017) to capture the sentence context in the out-
put embeddings (at subword unit level), such that
the embedding for the same word/subword unit in
different sentences would be different and better
represented in the embedding space. Zhang et al.
(2019) provided an extensive study on the perfor-
mance of the output embeddings of difference lay-
ers of BERT model in correlation with human ad-
equacy. Following the recommendation from their
studies, we use embeddings extracted from BERT
models with the following settings:

• the 18th layer of the pretrained English cased
BERT-Large model to represent the subword
units in the reference and MT output in En-
glish for computing YiSi-1;

• the 9th layer of the pretrained Chinese BERT-
Base model to represent the characters in the
reference and MT output in Chinese for com-
puting YiSi-1; and

• the 9th layer of the pretrained multilingual
cased BERT-Base model to represent the sub-
word units in the reference and MT output in
languages other than Chinese and English for
computing YiSi-1 and to represent the sub-
word units in the original input and MT out-
put in all language pairs for computing YiSi-
2.

2.3 Using MATE/MATEPLUS for structural
semantic similarity

There are a handful of shallow semantic parsers
available publicly. mate-tools (Björkelund
et al., 2009) is an SVM classifier based on features
extracted from a dependency parse. Its succes-
sor mateplus (Roth and Woodsend, 2014) also
uses features extracted from distributional word
embeddings. mate-tools and mateplus are

integrated into YiSi because of their support for
languages other than English. We use mateplus
for German’s and English’s semantic role labeling
and mate-tools for Chinese’s semantic role la-
beling.

3 Experiments and results

We use WMT 2018 metrics task evaluation set
(Ma et al., 2018) for our development experi-
ments.

The official human judgments of translation
quality in WMT 2018 were collected using direct
assessment. The direct assessment evaluation pro-
tocol in WMT2018 gave the annotators the refer-
ence and a MT output and asked them to evaluate
the translation adequacy of the MT output on an
absolute scale.

Due to space limitations, we only report the re-
sults of YiSi, chrF (Popović, 2015), BLEU and the
best correlation in each of the individual language
pairs. Since we use exactly the same correlation
analysis as the official task for each of the test sets,
our reported results are directly comparable with
those reported in the task’s overview paper. We
summarize our observations in the following sec-
tions.

3.1 Correlation with human judgment at
system-level

Table 1 shows the Pearson’s correlation with
WMT 2018 official aggregated human direct as-
sessment of translation adequacy at system-level.

YiSi-0 performs more stably than chrF and
BLEU in correlating with human on translation
quality across all translation directions. YiSi-0
achieves comparable results with chrF and BLEU
in most of the translation directions while signif-
icantly outperforms chrF and BLEU in correlat-
ing with human in evaluating Turkish-English and
English-Turkish translations.

YiSi-1 beats all the WMT2018 participants
in correlation with human at system level for
evaluating Czech-English, German-English,
Chines-English, English-German, English-
Estonian, English-Finnish and English-Russian
translations. In addition, YiSi-1 srl further
improves YiSi-1’s correlation with human at
system level for evaluating German-English,
Chinese-English translations.

For the quality estimation variants, YiSi-2
achieves reasonably good results (with less than



512

input lang. cs de et fi ru tr zh en en en en en en en
output lang. en en en en en en en cs de et fi ru tr zh
individual best .981 .997 .991 .996 .995 .958 .982 .999 .991 .984 .974 .992 .990 .983
chrF .966 .994 .981 .987 ,990 .452 .960 .990 .990 .981 .969 .989 .948 .944
BLEU .970 .971 .986 .973 .979 .657 .978 .995 .981 .975 .962 .983 .826 .947
YiSi-0 .962 .995 .982 .986 .985 .857 .972 .984 .989 .984 .954 .989 .980 .956
YiSi-1 .990 .998 .986 .994 .993 .830 .988 .993 .995 .988 .979 .993 .929 .977
YiSi-1 srl .989 .999 .987 .993 .993 .793 .989 – .995 – – – – .976
Quality estimation as a metric
YiSi-2 .919 .946 .865 .927 .566 .061 .797 .710 .862 .156 .475 .204 .389 .417
YiSi-2 srl – .948 – – – – .781 – .902 – – – – .472

Table 1: Pearson’s correlation of the metric scores with WMT 2018 aggregated human direct assessment scores at
system-level.

input lang. cs de et fi ru tr zh en en en en en en en
output lang. en en en en en en en cs de et fi ru tr zh
individual best .347 .498 .368 .273 .311 .259 .218 .518 .696 .573 .525 .407 .418 .323
chrF .288 .479 .328 .229 .269 .210 .208 .516 .677 .572 .520 .383 .409 .328
sentBLEU .233 .415 .285 .154 .228 .145 .178 .389 .320 .414 .355 .330 .261 .311
YiSi-0 .308 .480 .330 .210 .284 .213 .216 .454 .670 .530 .468 .396 .362 .316
YiSi-1 .391 .544 .397 .299 .352 .301 .254 .548 .734 .599 .549 .427 .402 .371
YiSi-1 srl .396 .543 .390 .303 .351 .297 .253 – .719 – – – – .368
Quality estimation as a metric
YiSi-2 .014 .279 .186 .151 .088 .066 .091 -.043 .359 .106 .172 .061 .103 .101
YiSi-2 srl – .281 – – – – .085 – .380 – – – – .103

Table 2: Kendall’s correlation of metric scores with the rankings at segment-level human direct assessment in
WMT 2018.

0.1 degradation in correlation with human) in eval-
uating Czech-English , German-English, Finnish-
English translation without using the human trans-
lation as reference. At the same time, YiSi-2 srl
improves YiSi-2’s correlation with human at sys-
tem level for evaluating English-German, English-
Chinese translations.

3.2 Correlation with human judgment at
segment-level

Table 2 shows the Kendall’s correlation with the
rankings at segment-level human direct assess-
ment obtained in the WMT 2018.

YiSi-0 achieves comparable results with chrF
and BLEU for evaluating all translation directions
at segment level. YiSi-1 beats all the WMT2018
participants in correlation with human at segment
level for evaluating almost all translation direc-
tions, except English-Turkish. In addition, YiSi-
1 srl further improves YiSi-1’s correlation with
human at segment level for evaluating Czech-
English and Finnish-English translations.

For the quality estimation variants, YiSi-2 per-
forms significantly worse than YiSi-1 due to the
lacking of a reference translation in the same lan-
guage for evaluating fluency. Therefore, We can
see that as shown by the significant improvement
in YiSi-2 srl for evaluating English-German trans-

lation without reference translation, using seman-
tic parsers to extract the semantic frames of the in-
put sentence and machine translation become very
helping in evaluating translation fluency.

4 Conclusion

We have presented the on-going work in devel-
oping a unified semantic MT quality evaluation
and estimation metric for languages with differ-
ent levels of available resources. Initial experi-
ment results show that the improved variants of
YiSi that use BERT contextual embeddings corre-
late with human judgment significantly better than
other trained metrics.

References
Anders Björkelund, Love Hafdell, and Pierre Nugues.

2009. Multilingual semantic role labeling. In Pro-
ceedings of the Thirteenth Conference on Computa-
tional Natural Language Learning (CoNLL 2009):
Shared Task, pages 43–48, Boulder, Colorado. As-
sociation for Computational Linguistics.

Ondřej Bojar, Yvette Graham, and Amir Kamran.
2017. Results of the WMT17 metrics shared task.
In Proceedings of the Second Conference on Ma-
chine Translation, pages 489–513, Copenhagen,
Denmark. Association for Computational Linguis-
tics.

http://www.aclweb.org/anthology/W09-1206
https://doi.org/10.18653/v1/W17-4755


513

Ondřej Bojar, Yvette Graham, Amir Kamran, and
Miloš Stanojević. 2016. Results of the WMT16
Metrics Shared Task. In Proceedings of the First
Conference on Machine Translation, pages 199–
231, Berlin, Germany. Association for Computa-
tional Linguistics.

Michael Denkowski and Alon Lavie. 2014. METEOR
universal: Language specific translation evaluation
for any target language. In 9th Workshop on Statis-
tical Machine Translation (WMT 2014).

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. BERT: pre-training of
deep bidirectional transformers for language under-
standing. CoRR, abs/1810.04805.

Chi-kiu Lo. 2017. MEANT 2.0: Accurate semantic
MT evaluation for any output language. In Proceed-
ings of the Second Conference on Machine Transla-
tion, pages 589–597, Copenhagen, Denmark. Asso-
ciation for Computational Linguistics.

Chi-kiu Lo, Michel Simard, Darlene Stewart, Samuel
Larkin, Cyril Goutte, and Patrick Littell. 2018. Ac-
curate semantic textual similarity for cleaning noisy
parallel corpora using semantic machine translation
evaluation metric: The NRC supervised submissions
to the parallel corpus filtering task. In Proceedings
of the Third Conference on Machine Translation:
Shared Task Papers, pages 908–916, Belgium, Brus-
sels. Association for Computational Linguistics.

Qingsong Ma, Ondřej Bojar, and Yvette Graham. 2018.
Results of the WMT18 metrics shared task: Both
characters and embeddings achieve good perfor-
mance. In Proceedings of the Third Conference on
Machine Translation: Shared Task Papers, pages
671–688, Belgium, Brussels. Association for Com-
putational Linguistics.

Matous Machacek and Ondrej Bojar. 2014. Results of
the WMT14 Metrics Shared Task. In Proceedings
of the Ninth Workshop on Statistical Machine Trans-
lation, pages 293–301, Baltimore, Maryland, USA.
Association for Computational Linguistics.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeffrey Dean. 2013. Distributed repre-
sentations of words and phrases and their compo-
sitionality. In Proceedings of the 26th International
Conference on Neural Information Processing Sys-
tems, NIPS’13, pages 3111–3119, USA. Curran As-
sociates Inc.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In 40th An-
nual Meeting of the Association for Computational
Linguistics (ACL-02), pages 311–318, Philadelphia,
Pennsylvania.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language

Processing (EMNLP), pages 1532–1543, Doha,
Qatar. Association for Computational Linguistics.

Maja Popović. 2015. chrF: character n-gram f-score
for automatic MT evaluation. In Proceedings of the
Tenth Workshop on Statistical Machine Translation,
pages 392–395, Lisbon, Portugal. Association for
Computational Linguistics.

Michael Roth and Kristian Woodsend. 2014. Composi-
tion of word representations improves semantic role
labelling. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 407–413, Doha, Qatar. Association
for Computational Linguistics.

Miloš Stanojević, Amir Kamran, Philipp Koehn, and
Ondřej Bojar. 2015. Results of the WMT15 Met-
rics Shared Task. In Proceedings of the Tenth Work-
shop on Statistical Machine Translation, pages 256–
273, Lisbon, Portugal. Association for Computa-
tional Linguistics.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in neural information pro-
cessing systems, pages 5998–6008.

Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.
Weinberger, and Yoav Artzi. 2019. Bertscore:
Evaluating text generation with BERT. CoRR,
abs/1904.09675.

http://www.aclweb.org/anthology/W16-2302
http://www.aclweb.org/anthology/W16-2302
http://arxiv.org/abs/1810.04805
http://arxiv.org/abs/1810.04805
http://arxiv.org/abs/1810.04805
https://doi.org/10.18653/v1/W17-4767
https://doi.org/10.18653/v1/W17-4767
https://www.aclweb.org/anthology/W18-6481
https://www.aclweb.org/anthology/W18-6481
https://www.aclweb.org/anthology/W18-6481
https://www.aclweb.org/anthology/W18-6481
https://www.aclweb.org/anthology/W18-6481
https://www.aclweb.org/anthology/W18-6450
https://www.aclweb.org/anthology/W18-6450
https://www.aclweb.org/anthology/W18-6450
http://www.aclweb.org/anthology/W14-3336
http://www.aclweb.org/anthology/W14-3336
http://dl.acm.org/citation.cfm?id=2999792.2999959
http://dl.acm.org/citation.cfm?id=2999792.2999959
http://dl.acm.org/citation.cfm?id=2999792.2999959
https://doi.org/10.3115/1073083.1073135
https://doi.org/10.3115/1073083.1073135
https://doi.org/10.3115/v1/D14-1162
https://doi.org/10.3115/v1/D14-1162
https://doi.org/10.18653/v1/W15-3049
https://doi.org/10.18653/v1/W15-3049
http://www.aclweb.org/anthology/D14-1045
http://www.aclweb.org/anthology/D14-1045
http://www.aclweb.org/anthology/D14-1045
http://aclweb.org/anthology/W15-3031
http://aclweb.org/anthology/W15-3031
http://arxiv.org/abs/1904.09675
http://arxiv.org/abs/1904.09675

