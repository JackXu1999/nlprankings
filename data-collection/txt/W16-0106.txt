



















































Neural Generative Question Answering


Proceedings of 2016 NAACL Human-Computer Question Answering Workshop, pages 36–42,
San Diego, California, June 12-17, 2016. c©2016 Association for Computational Linguistics

Neural Generative Question Answering

Jun Yin1∗Xin Jiang2 Zhengdong Lu2, Lifeng Shang2, Hang Li2, Xiaoming Li1
1School of Electronic Engineering and Computer Science, Peking University

2Noah’s Ark Lab, Huawei Technologies
{jun.yin,lxm}@pku.edu.cn, {jiang.xin, lu.zhengdong, shang.lifeng, hangli.hl}@huawei.com

Abstract

This paper presents an end-to-end neural net-
work model, named Neural Generative Ques-
tion Answering (GENQA), that can generate
answers to simple factoid questions, based on
the facts in a knowledge-base. More specif-
ically, the model is built on the encoder-
decoder framework for sequence-to-sequence
learning, while equipped with the ability to en-
quire the knowledge-base, and is trained on a
corpus of question-answer pairs, with their as-
sociated triples in the knowledge-base. Em-
pirical study shows the proposed model can
effectively deal with the variations of ques-
tions and answers, and generate right and nat-
ural answers by referring to the facts in the
knowledge-base. The experiment on ques-
tion answering demonstrates that the proposed
model can outperform an embedding-based
QA model as well as a neural dialogue model
trained on the same data.

1 Introduction

Question answering (QA) can be viewed as a special
case of single-turn dialogue: QA aims at providing
correct answers to the questions in natural language,
while dialogue emphasizes on generating relevan-
t and fluent responses to the messages also in natural
language (Shang et al., 2015; Vinyals and Le, 2015).
Recent progress in deep learning has raised the pos-
sibility of realizing generation-based QA in a purely
neutralized way. That is, the answer is generated by
a neural network (e.g., recurrent neural network, or
∗ The work is done when the first author worked as intern at

Noah’s Ark Lab, Huawei Technologies.

RNN) based on the question, which is able to handle
the flexibility and diversity of language. More im-
portantly, the model is trained in an end-to-end fash-
ion, and thus there is no need in building the system
using linguistic knowledge, e.g., creating a semantic
parser.

There is however one serious limitation of this
generation-based approach to QA. It is practically
impossible to store all the knowledge in a neural
network to achieve a desired precision and cover-
age in real world QA. This is a fundamental diffi-
culty, rooting deeply in the way in which knowledge
is acquired, represented and stored. The neural net-
work, and more generally the fully distributed way
of representation, is good at representing smooth
and shared patterns, i.e., modeling the flexibility and
diversity of language, but improper for representing
discrete and isolated concepts, i.e., depicting the lex-
icon of language.

On the other hand, the recent success of memory-
based neural network models has greatly extended
the ways of storing and accessing text information,
in both short-term memory (e.g., in (Bahdanau et
al., 2015)) and long-term memory (e.g., in (West-
on et al., 2015)). It is hence a natural choice to
connect a neural model for QA with a neural model
of knowledge-base on an external memory, which is
also related to the traditional approach of template-
based QA from knowledge-base.

In this paper, we report our exploration in this di-
rection, with a proposed model called Neural Gen-
erative Question Answering (GENQA).

Learning Task: We formalize generative question
answering (GENQA) as a supervised learning task or

36



Table 1: Examples of training instances for GENQA. The KB-words in the training instances are underlined in the examples.
Question & Answer Triple (subject, predicate, object)
Q: How tall is Yao Ming?
A: He is 2.29m and is visible from space.

(Yao Ming, height, 2.29m)

Q: Which country was Beethoven from?
A: He was born in what is now Germany.

(Ludwig van Beethoven, place
of birth, Germany)

Q: Which club does Messi play for?
A: Lionel Messi currently plays for FC Barcelona in the Spanish
Primera Liga.

(Lionel Messi, team, FC
Barcelon)

more specifically a sequence-to-sequence learning
task. A GENQA system takes a sequence of words
as input question and generates another sequence of
words as answer. In order to provide right answers,
the system is connected with a knowledge-base (K-
B), which contains facts. During the process of an-
swering, the system queries the KB, retrieves a set of
candidate facts and generates a correct answer to the
question using the right fact. The generated answer
may contain two types of “words”: one is common
words for composing the answer (referred to as com-
mon word) and the other is specialized words in the
KB denoting the answer (referred to as KB-word).

To learn a GENQA model, we assume that each
training instance consists of a question-answer pair
with the KB-word specified in the answer. In this
paper, we only consider the case of simple factoid
question, which means each question-answer pair is
associated with a single fact (i.e., one triple) of the
KB. Without loss of generality, we mainly focus on
forward relation QA, where the question is on sub-
ject and predicate and the answer points to object.
Tables 1 shows some examples of the training in-
stances.
Dataset: To facilitate research on the task of gen-
erative QA, we create a new dataset by collecting da-
ta from the web. We first build a knowledge-base by
mining from three Chinese encyclopedia web sites1.
Specifically we extract entities and associated triples
(subject, predicate, object) from the structured parts
(e.g. HTML tables) of the web pages. Then the ex-
tracted data is normalized and aggregated to form a
knowledge-base. In this paper we sometimes refer to
the items of a triple as a constituent of knowledge-
base. Second, we collect question-answer pairs by
extracting from two Chinese community QA sites2.
1 Baidu Baike, Baike.com, Douban.com
2 Baidu Zhidao, Sogou Wenwen

Table 2: Statistics of the QA data and the knowledge-base.
Community QA Knowledge-base

#QA pairs #entities #triples
235,171,463 8,935,028 11,020,656

Table 2 shows the statistics of the knowledge-base
and QA-pairs.

We construct the training and test data for GEN-
QA by “grounding” the QA pairs with the triples
in knowledge-base. Specifically, for each QA pair,
a list of candidate triples with the subject field-
s appearing in the question, is retrieved by using
the Aho-Corasick string searching algorithm. The
triples in the candidate list are then judged by a se-
ries of rules for relevance to the QA pair. The basic
requirement for relevance is that the answer contains
the object of the triple, which specifies the KB-word
in the answer. Besides, we use additional scoring
and filtering rules, attempting to find out the triple
that truly matches the QA pair, if there is any. As
the result of processing, 720K instances (tuples of
question, answer, triple) are finally obtained with an
estimated 80% of instances being truly positive. The
data are publicly available online3.

In order to test the generalization ability of the
GENQA model, the data is randomly partitioned in-
to training dataset and test dataset by using triple as
the partition key. In that way, all the questions in the
test data are regarding to the unseen facts (triples) in
the training data. Table 3 shows some statistics of
the datasets. By comparing the numbers of triples
in Table 2 and Table 3, we can see that a large por-
tion of facts in the knowledge-base are not present
in the training and test data, which demonstrates the
necessity for the model to generalize to unseen facts.

3 https://github.com/jxfeb/Generative_QA

37



Table 3: Statistics of the training and test dataset for GENQA
Training Data Test Data

#QA pairs #triples #QA pairs #triples
696,306 58,019 23,364 1,974

2 The Neural Model

Let Q = (x1, . . . , xTQ) and Y = (y1, . . . , yTY ) de-
note the natural language question and answer re-
spectively. The knowledge-base is organized as a
set of triples (subject, predicate, object), each de-
noted as τ = (τs, τp, τo). Inspired by the work on
the encoder-decoder framework for neural machine
translation (Cho et al., 2014; Sutskever et al., 2014;
Bahdanau et al., 2015) and neural natural language
dialogue (Shang et al., 2015; Vinyals and Le, 2015;
Serban et al., 2015), and the work on question an-
swering with knowledge-base embedding (Bordes et
al., 2014b; Bordes et al., 2014a; Bordes et al., 2015),
we propose an end-to-end neural network model for
GENQA, which is illustrated in Figure 1.

The GENQA model consists of Interpreter, En-
quirer, Answerer, and an external knowledge-base.
Answerer further consists of Attention Model and
Generator. Basically, Interpreter transforms the
natural language question Q into a representation
HQ and saves it in the short-term memory. Enquir-
er takes HQ as input to interact with the knowledge-
base in the long-term memory, retrieves relevant
facts (triples) from the knowledge-base, and summa-
rizes the result in a vector rQ. The Answerer feeds
on the question representation HQ (through the At-
tention Model) as well as the vector rQ and gener-
ates the answer with Generator. We elaborate each
component hereafter.

Interpreter: Given the question represented as
word sequence Q = (x1, . . . , xTQ), Interpreter en-
codes it to the array of vector representations. In
our implementation, we adopt a bi-directional RN-
N as in (Bahdanau et al., 2015), which process-
es the sequence in forward and reverse order by
using two independent RNNs (here we use gat-
ed recurrent unit (GRU) (Chung et al., 2014)).
By concatenating the hidden states (denoted as
(h1, · · · ,hTQ)), the embeddings of the words (de-
noted as (x1, · · · ,xTQ)), and the original one-hot
representations of the words, we obtain an array

Figure 1: The diagram for GENQA.

Figure 2: The Enquirer of GENQA.

of vectors HQ = (h̃1, · · · , h̃TQ), where h̃t =
[ht; xt;xt]. This array of vectors is saved in the
short-term memory, allowing for further processing
by Enquirer and Answerer for different purposes.

Enquirer: Enquirer “fetches” the relevant facts
from the knowledge-base with Q and HQ (as illus-
trated by Figure 2). Enquirer first performs term-
level matching to retrieve a list of relevant candidate
triples, denoted as TQ = {τk}KQk=1. KQ is the num-
ber of candidate triples, which is usually less than
several hundreds in our data. This first round filter-
ing, although fairly simple, is important in making
the following step of differentiable operations (e.g.,
the weighting on the candidate set and the answer
generation) and optimization feasible. After obtain-
ing TQ, the task reduces to evaluating the relevance
of each candidate triple with the question in the em-
bedded space(Bordes et al., 2014b; Bordes et al.,
2014a).

More specifically Enquirer calculates the match-
ing scores between the question and the KQ triples.

38



For question Q, the scores are represented in a KQ-
dimensional vector rQ where the kth element of rQ
is defined as the probability

rQk =
eS(Q,τk)∑KQ

k′=1 e
S(Q,τk′ )

,

where S(Q, τk) denotes the matching score between
question Q and triple τk.

The probability in rQ will be further taken into
the probabilistic model in Answerer for generating a
particular answering sentence. Since rQ is of mod-
est size, after the filtering step, and differentiable
with respect to its parameters, it can be effective-
ly optimized by the supervision signal in recovering
the original answers through back-propagation.

In this work, we provide two implementations for
Enquirer to calculate the matching scores between
question and triples.

Bilinear Model: The first implementation simply
takes the average of the word embedding vectors in
HQ as the representation of the question (with the
result denoted as x̄Q) . For each triple τ in the
knowledge-base, it takes the mean of the embed-
dings of its subject and predicate as the represen-
tation of the triple (denoted as uτ ). Then we define
the matching score as

S̄(Q, τ) = x̄>QMuτ ,

where M is the matrix parameterizing the matching
between the question and the triple.

CNN-based Matching Model: The second im-
plementation employs the convolutional neural net-
work (CNN) for modeling the matching score be-
tween question and triple, as in (Hu et al., 2014)
and (Shen et al., 2014). Specifically, the question
is fed to a convolutional layer followed by a max-
pooling layer, and summarized as a fixed-length vec-
tor, denoted as ĥQ. Then ĥQ and uτ (again as the
mean of the embedding of the corresponding subject
and predicate) are concatenated as input to a multi-
layer perceptron (MLP) to produce their matching
score

Ŝ(Q, τ) = fMLP([ĥQ; uτ ]).

For this model the parameters consist of both the C-
NN for question representation and the MLP for the
final matching decision.

Answerer: Answerer uses an RNN to generate the
answer sentence based on the information of ques-
tion saved in the short-term memory (represented by
HQ) and the relevant knowledge retrieved from the
long-term memory (indexed by rQ), as illustrated in
Figure 3. The probability of generating the answer
sentence Y = (y1, y2, . . . , yTY ) is defined as

p(y1, · · · , yTY |HQ, rQ; θ) =

p(y1|HQ, rQ; θ)
TY∏
t=2

p(yt|y1, . . . , yt−1,HQ, rQ; θ)

where θ represents the parameters in the GEN-
QA model. The conditional probability in
the RNN model (with hidden state s1, · · · , sTY )
is specified by p(yt|y1, . . . , yt−1,HQ, rQ; θ) =
p(yt|yt−1, st,HQ, rQ; θ).

In generating the tth word yt in the answer sen-
tence, the probability is given by the following mix-
ture model

p(yt|yt−1, st,HQ, rQ; θ) =
p(zt = 0|st; θ)p(yt|yt−1, st,HQ, zt = 0; θ)+
p(zt = 1|st; θ)p(yt|rQ, zt = 1; θ),

which sums the contributions from the “language”
part and the “knowledge” part, with the coefficien-
t p(zt|st; θ) being realized by a logistic regression
model with st as input. Here the latent variable zt
indicates whether the tth word is generated from a
common vocabulary (for zt = 0) or a KB vocab-
ulary (zt = 1). In this work, the KB vocabulary
contains all the objects of the candidate triples as-
sociated with the particular question. For any word
y that is only in the KB vocabulary, e.g., “2.29m”,
we have p(yt|yt−1, st,HQ, zt = 0; θ) = 0, while for
y that does not appear in KB, e.g., “and”, we have
p(yt|rQ, zt = 1; θ) = 0. There are some words (e.g.,
“Shanghai”) that appear in both common vocab-
ulary and KB vocabulary, for which the probability
contains nontrivial contributions of both bodies.

In generating common words, Answerer acts in
the same way as the decoder RNN in (Bahdanau
et al., 2015) with information from HQ selected
by the attention model. Specifically, the hidden s-
tate at t step is computed as st = fs(yt−1, st−1, ct)
and p(yt|yt−1, st,HQ, zt = 0; θ) = fy(yt−1, st, ct),

39



Figure 3: The Answerer of GENQA.

where ct is the context vector computed as weight-
ed sum of the hidden states stored in the short-term
memory HQ.

In generating KB-words via p(yt|rQ, zt = 1; θ),
Answerer simply employs the model p(yt =
k|rQ, zt = 1; θ) = rQk. The better a triple matched
with the question, the more likely the object of the
triple is selected.
Training: The parameters to be learned include
the weights in the RNNs for Interpreter and An-
swerer, parameters in Enquirer, and the word-
embeddings which are shared by the Interpreter RN-
N and the knowledge-base. GENQA, although es-
sentially containing a retrieval operation, can be
trained in an end-to-end fashion by maximizing the
likelihood of observed data, since the mixture form
of probability in Answerer provides a unified way to
generate words from common vocabulary and (dy-
namic) KB vocabulary. In practice the model is
trained on machines with GPUs by using stochastic
gradient-descent with mini-batch.

3 Experiments

3.1 Comparison Models
To our best knowledge there is no previous work on
generative QA, we choose three baseline methods:
a neural dialogue model, a retrieval-based QA mod-
el and the embedding based QA model, respectively
corresponding to the generative aspect and the KB-

retrieval aspect of GENQA:
Neural Responding Machine (NRM): NR-

M (Shang et al., 2015) is a neural network based
generative model specially designed for short-text
conversation. We train the NRM model on the
question-answer pairs in the training data with the
same vocabulary as the vocabulary of GENQA. S-
ince NRM does not access the knowledge-base dur-
ing training and test, it actually remembers all the
knowledge from the QA pairs in the weights of the
model.

Retrieval-based QA: the knowledge-base is in-
dexed by an information retrieval system (we use
Apache Solr), in which each triple is deemed as a
document. At test phase, a question is used as the
query and the top-retrieved triple is returned as the
answer. Note that in general this method cannot gen-
erate natural language answers.
Embedding-based QA: as proposed by (Bordes et

al., 2014a; Bordes et al., 2014b), the model is learn-
t from the question-triple pairs in the training data.
The model learns to map questions and knowledge-
base constituents into the same embedding space,
where the similarities between questions and triples
are computed as the inner product of the two embed-
ding vectors.

Since we have two implementations of matching
score in Enquirer of the GENQA model, we denote
the one using the bilinear model as GENQA and the
other using CNN and MLP as GENQACNN.

3.2 Results

We evaluate the performance of the models in terms
of 1) accuracy, i.e., the ratio of correctly answered
questions, and 2) the fluency of answers. In order
to ensure an accurate evaluation, we randomly se-
lect 300 questions from the test set, and manually
remove the nearly duplicate cases and filter out the
mistaken cases (e.g., non-factoid questions).

Accuracy: Table 4 shows the accuracies of the
models in the test set. NRM has the lowest ac-
curacy, showing the lack of ability to remem-
ber the answers accurately and generalize to ques-
tions unseen in the training data. For example,
to question “Which country does Xavi play
for as a midfielder?” (Translated from Chi-
nese), NRM gives the wrong answer “He plays
for France” (Translated from Chinese), since the

40



Figure 4: Examples of the generated answers by GENQA.

Table 4: Test accuracies
Models Test
Retrieval-based QA 36%
NRM 19%
Embedding-based QA 45%
GENQA 47%
GENQACNN 52%

athlete actually plays for Spain. The retrieval-based
method achieves a moderate accuracy, but like most
string-matching methods it suffers from word mis-
match between the question and the triples in K-
B. The embedding-based QA model achieves higher
accuracy on test set, thanks to its generalization a-
bility from distributed representations. GENQA and
GENQACNN are both better than the competitors,
showing that GENQA can further benefit from the
end-to-end training for sequence-to-sequence learn-
ing. For example, as we conjecture, the task of gen-
erating the appropriate answer may help the learning
of word-embeddings of the question. Among the t-
wo GENQA variants, GENQACNN achieves the best
accuracy, getting over half of the questions right.
An explanation for that is that the convolution lay-
er helps to capture salient features in matching. The
experiment results demonstrate the ability of GEN-
QA models to find the right answer from KB even
with regard to new facts. For example, to the ex-
ample question mentioned above, GENQA gives the
correct answer “He plays for Spain”.
Fluency: We make some empirical comparison-
s and find no significant differences between NRM
and GENQA in terms of the fluency of answers. In
general, all the three models based on sequence gen-
eration yield correct patterns in most of the time.

3.3 Case Study

Figure 4 gives some examples of generated answers
to the questions in the test set by our GENQA mod-
els, with the underlined words generated from K-
B. Clearly it can smoothly blend the KB-words and
common words in the sentence, thanks to the uni-
fied neural model that can learn to determine the
right time to place a KB-word or a common word.
We notice that most of the generated answers are
short sentences, for which there are two possible
reasons: 1) many answers to the factoid questions
on the Community QA sites are usually short, and
2) we select the answers by beam-searching the se-
quence with maximum log-likelihood normalized by
its length, which generally prefers short answers.
Examples 1 to 4 show the correctly generated an-
swers, where the model not only matches the right
triples (and thus generate the right KB-words), but
also generates suitable common words surrounding
them. However, in some cases like examples 5 and
6 even the right triples are found, the surrounding
common words are improper or incorrect from the
knowledge-base point of view (e.g., in example 6
the author “Jonathan Swift” is from Ireland
rather than France). By investigating the correct-
ly generated answers on test data, we find roughly
8% of them having improper surrounding words. In
some other cases, the model fails to match the cor-
rect triples with the questions, which produces com-
pletely wrong answers. For the instance in example
7, the question is about the release date of a movie,
while the model finds its distributor and generates an
answer incorrect both in terms of fact and language.

41



References
[Bahdanau et al.2015] Dzmitry Bahdanau, Kyunghyun

Cho, and Yoshua Bengio. 2015. Neural machine
translation by jointly learning to align and translate.
In ICLR.

[Bordes et al.2014a] Antoine Bordes, Jason Weston, and
Sumit Chopra. 2014a. Question answering with sub-
graph embeddings. EMNLP.

[Bordes et al.2014b] Antoine Bordes, Jason Weston, and
Nicolas Usunier. 2014b. Open question answering
with weakly supervised embedding models. In ECML
PKDD, pages 165–180.

[Bordes et al.2015] Antoine Bordes, Nicolas Usunier,
Sumit Chopra, and Jason Weston. 2015. Large-scale
simple question answering with memory networks.
arXiv preprint arXiv:1506.02075.

[Cho et al.2014] Kyunghyun Cho, Bart Van Merriënboer,
Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares,
Holger Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using rnn encoder-decoder for
statistical machine translation. EMNLP.

[Chung et al.2014] Junyoung Chung, Caglar Gulcehre,
KyungHyun Cho, and Yoshua Bengio. 2014. Empir-
ical evaluation of gated recurrent neural networks on
sequence modeling. arXiv preprint arXiv:1412.3555.

[Hu et al.2014] Baotian Hu, Zhengdong Lu, Hang Li, and
Qingcai Chen. 2014. Convolutional neural network
architectures for matching natural language sentences.
In Advances in Neural Information Processing System-
s, pages 2042–2050.

[Serban et al.2015] Iulian V Serban, Alessandro Sordoni,
Yoshua Bengio, Aaron Courville, and Joelle Pineau.
2015. Building end-to-end dialogue systems using
generative hierarchical neural network models. arX-
iv preprint arXiv:1507.04808.

[Shang et al.2015] Lifeng Shang, Zhengdong Lu, and
Hang Li. 2015. Neural responding machine for short-
text conversation. In Association for Computational
Linguistics (ACL), pages 1577–1586.

[Shen et al.2014] Yelong Shen, Xiaodong He, Jianfeng
Gao, Li Deng, and Grégoire Mesnil. 2014. Learn-
ing semantic representations using convolutional neu-
ral networks for web search. In Proceedings of the
companion publication of the 23rd international con-
ference on World wide web companion, pages 373–
374. International World Wide Web Conferences S-
teering Committee.

[Sutskever et al.2014] Ilya Sutskever, Oriol Vinyals, and
Quoc VV Le. 2014. Sequence to sequence learning
with neural networks. In NIPS, pages 3104–3112.

[Vinyals and Le2015] Oriol Vinyals and Quoc Le. 2015.
A neural conversational model. arXiv preprint arX-
iv:1506.05869.

[Weston et al.2015] Jason Weston, Sumit Chopra, and
Antoine Bordes. 2015. Memory networks. In In-
ternational Conference on Learning Representations
(ICLR).

42


