



















































Predicting the Usefulness of Amazon Reviews Using Off-The-Shelf Argumentation Mining


Proceedings of the 5th Workshop on Argument Mining, pages 35–39
Brussels, Belgium, November 1, 2018. c©2018 Association for Computational Linguistics

35

Predicting the Usefulness of Amazon Reviews
Using Off-The-Shelf Argumentation Mining

Marco Passon†, Marco Lippi‡, Giuseppe Serra†, Carlo Tasso†

†Università degli Studi di Udine
‡Università degli Studi di Modena e Reggio Emilia

marco.passon@spes.uniud.it
marco.lippi@unimore.it

{giuseppe.serra, carlo.tasso}@uniud.it

Abstract

Internet users generate content at unprece-
dented rates. Building intelligent systems ca-
pable of discriminating useful content within
this ocean of information is thus becoming a
urgent need. In this paper, we aim to predict
the usefulness of Amazon reviews, and to do
this we exploit features coming from an off-
the-shelf argumentation mining system. We
argue that the usefulness of a review, in fact,
is strictly related to its argumentative content,
whereas the use of an already trained system
avoids the costly need of relabeling a novel
dataset. Results obtained on a large publicly
available corpus support this hypothesis.

1 Introduction

In our digital era, reviews affect our everyday de-
cisions. More and more people resort to digital re-
views before buying a good or deciding where to
eat or stay. In fact, helpful reviews allow users to
grasp more clearly the features of a product they
are about to buy, and thus to understand whether
it fits their needs. The same can be said for users
who want to book hotels or restaurants.

Companies have started to exploit the impor-
tance of reviews. For example, when browsing
for a specific product, we are usually presented re-
views that have been judged helpful by other users.
Moreover, we are often given the possibility to sort
reviews according to the number of people who
judged them as helpful. That said, a review can
also be helpful for companies who want to mon-
itor what people think about their brand. Being
able to identify helpful reviews has thus many im-
portant applications, both for users and for com-
panies, and in multiple domains.

The automatic identification of helpful reviews
is not as easy as it may seem, because the review
content has to be semantically analyzed. There-

fore, this process is traditionally done by asking
users for a judgment.

To overcome this issue, some approaches have
been proposed. One of the earliest studies (Kim
et al., 2006) aims to rank Amazon reviews by their
usefulness by training a regressor with a combina-
tion of different features extracted from text and
metadata of the reviews, as well as features of the
product. Similar approaches employ different sets
of features (Ngo-Ye and Sinha, 2012), for exam-
ple including the reputation of reviewers too (Baek
et al., 2012). Another significant work (Mudambi
and Schuff, 2010) builds a customer model that de-
scribes which features of an Amazon review affect
its perceived usefulness, and then it uses such fea-
tures to build a regression model to predict the use-
fulness, expressed as the percentage of the number
of people who judged a review to be useful. A hy-
brid regression model (Ngo-Ye and Sinha, 2014)
combines text and additional features describing
users (recency, frequency, monetary value) to pre-
dict the number of people who judged as useful
reviews taken from Amazon and Yelp. A more
complete work considers both regression and clas-
sification (Ghose and Ipeirotis, 2011). It proves
different hypotheses, starting with expressing the
usefulness of an Amazon review as a function of
readability and subjectivity cues, and then convert-
ing the usefulness, expressed with a continuous
value, into a binary usefulness, that is predicting
if a review is useful or not useful.

Another recent work (Liu et al., 2017) presents
an approach that explores an similar assumption
to ours: helpful reviews are typically argumen-
tative. In fact, what we hope to read in a re-
view is something that goes beyond plain opin-
ions or sentiment, being rather a collection of rea-
sons and evidence that motivate and support the
overall judgment of the product or service that is
reviewed. These characteristics are usually cap-



36

tured by an argumentation analysis, and could be
automatically detected by an argumentation min-
ing system (Lippi and Torroni, 2016a). The work
in (Liu et al., 2017) considers a set of 110 hotel re-
views, it presents a complete and manual labeling
of the arguments in such reviews, and it exploits
such information as additional features for a ma-
chine learning classifier that predicts usefulness.
In this paper, instead, we investigate the possibil-
ity to predict the usefulness of Amazon reviews
by using features coming from an automatic ar-
gumentation mining system, thus not directly us-
ing human-annotated arguments. A preliminary
experimental study conducted on a large publicly
dataset (117,000 Amazon reviews) confirms that
this could be really doable and a very fruitful re-
search direction.

2 Background

Argumentation is the discipline that studies the
way in which humans debate and articulate their
opinions and beliefs (Walton, 2009). Argumenta-
tion mining (Lippi and Torroni, 2016a) is a rapidly
expanding area, at the cross-road of many research
fields, such as computational linguistics, machine
learning, artificial intelligence. The main goal of
argumentation mining is to automatically extract
arguments and their relations from plain textual
documents.

Among the many approaches developed in re-
cent years for argumentation mining, based on
advanced machine learning and natural language
processing techniques, the vast majority is in fact
genre-dependent, or domain-dependent, as they
exploit information that is highly specific of the
application scenario. Due to the complexity of
these tasks, building general systems capable of
processing unstructured documents of any genre,
and of automatically reconstructing the relations
between the arguments contained in them, still re-
mains an open challenge.

In this work, we consider a simple definition
of argument, inspired by the work by Douglas
Walton (2009), that is the so-called claim/premise
model. A claim can be defined as an assertion re-
garding a certain topic, and it is typically consid-
ered as the conclusion of an argument. A premise
is a piece of evidence that supports the claim, by
bringing a contribution in favor of the thesis that is
contained within the claim itself.

3 Methodology

Our goal is to develop a machine learning sys-
tem capable of predicting the usefulness of a re-
view, by exploiting information related to its ar-
gumentative content. In particular, we consider
to enrich the features of a standard text classifi-
cation algorithm with features coming from an ar-
gumentation mining system. To this aim, we use
MARGOT (Lippi and Torroni, 2016b), a publicly
available argumentation mining system1 that em-
ploys the claim/premise model (to our knowledge,
there are no other off-the-shelf systems that per-
form argumentation mining). Two distinct classi-
fiers, based on Tree Kernels (Moschitti, 2006) are
trained to detect claims and premises (also called
evidence), respectively. When processing a docu-
ment, MARGOT returns two scores for each sen-
tence, one computed by each kernel machine, that
are used to predict the presence of a claim or a
premise within that sentence (by default, MAR-
GOT uses a decision threshold equal to zero).

Consider for example the following excerpt of
a review, where the proposition in italics is identi-
fied by MARGOT as a claim:

The only jam band I ever listen to now
is Cream, simply because they were ge-
niuses. They were geniuses because the
spontaneity, melodicism, and fearless-
ness in their improvisation has never
been equaled in rock, and rarely so in
jazz.

Clearly, such a review is very informative, since it
comments on very specific aspects of the product,
bringing motivations that can greatly help users in
taking their decisions. Similarly, the following ex-
cerpt of another review brings very convincing ar-
guments in favor of an overall positive judgment
of the product. In this case, both sentences are
classified by MARGOT as argumentative.

The music indeed seems to transcend
so many moods that most pianists have
a very hard time balancing this act
and there is an immense discography of
these concertos of disjoint and loosely-
knit performances. Pletnev pushes a
straightforward bravura approach with
lyrical interludes – and his performance
pays off brilliantly.

1http://margot.disi.unibo.it

http://margot.disi.unibo.it


37

0 10 20 30
Sentences w/ argument score > 0

0.0

0.2

0.4

0.6

0.8

1.0
U

se
fu

ln
es

s

: 0.411 (p < 10 12)
: 0.323 (p < 10 14)

Figure 1: Relation between usefulness and number of
sentences whose claim or evidence score is above zero
for category “CDs and Vinyl”.

Within this work, we compute simple statis-
tics from the output of MARGOT: the aver-
age claim/evidence/argument score, the maxi-
mum claim/evidence/argument score, the num-
ber and the percentage of sentences whose
claim/evidence/argument score is above 0 (that is,
the number and the percentage of sentences that
contain a claim, an evidence or simply one of
those). From a preliminary analysis, in fact, we
observed how the presence of arguments within
a review is highly informative of its usefulness.
Figure 1, for example, shows the correlation of
the number of sentences whose claim or evidence
score, according to MARGOT, is above 0, with
the usefulness for a subset of 200 reviews in the
Amazon category “CDs and Vinyl”. While it is
true that a low number of sentences that contain
a claim or an evidence does not necessarily mean
that the review is useless, yet the figure shows that
a review with a high number of sentences contain-
ing a claim or an evidence is most likely a use-
ful review, which confirms our intuition that use-
ful reviews are in fact argumentative. We use these
simple statistics as an additional set of features to
be used within a standard text classification algo-
rithm, in order to assess whether the presence of
argumentative content can help in predicting how
useful is a review.

We hereby remark that using MARGOT within
this framework is not optimal, because MARGOT
was trained on a completely different genre of doc-
uments, that is Wikipedia articles. Therefore, we
are dealing with a transfer learning task, where the
argumentation mining system is tested on a differ-

ent domain with respect to the one it was original-
ity trained on. Using such a classifier adds a chal-
lenge to our approach, but it has the advantage of
not needing a labeled corpus of argumentative re-
views to train a new argumentation mining system
from scratch. Indeed, more sophisticated systems
that take into account argumentation could be de-
veloped: here, we just want to exploit a straight-
forward combination of features in order to test
our hypothesis.

4 Experimental Results

To evaluate the proposed approach we use the
public Amazon Reviews dataset (McAuley and
Leskovec, 2013), in particular, we worked with the
so called “5-core” subset, that is, a subset of the
data in which all users and items have at least five
reviews. Each element of this dataset contains a
product review and metadata related to it.

Since we aim to predict usefulness, for each re-
view we compute the ratio between the number of
people who voted and judged that review as use-
ful, and the total number of people who expressed
a judgment about that review. Then, we define
useful reviews as the ones whose percentage of
usefulness is equal or above 0.7 (that means that
at least 70% of the people who judged a review,
judged it as useful), while the remaining are con-
sidered not to be useful, and thus they represent
our negative class.

The Amazon Review dataset is split into prod-
uct categories. For our experiments we picked
three of them, chosen among those with the high-
est number of reviews. Our choice has fallen upon
the “CDs and Vinyl” “Electronics” and “Movies
and TV” categories. We further selected only the
reviews having at least 75 rates, in order to assess
usefulness on a reasonably large set of samples.
Finally, we randomly selected 39,000 reviews for
each category, ending up with an almost balanced
number of helpful and unhelpful reviews.

Our goal in executing the experiments is to pre-
dict whether a review is considered useful, by tak-
ing into account either its textual content only, or,
additionally, also the argumentation mining data
coming from MARGOT. In other words, we are
working in a binary classification scenario.

In these experiments we use a stochastic gradi-
ent descent classifier2 with a hinge loss, which is a
classic solution in binary classification tasks. We

2We used SGDClassifier in scikit-learn.



38

Table 1: Performance on three Amazon categories us-
ing different sets of features: Margot features (M),
Bag-of-Words (BoW), Bag-of-Words weighted by TF-
IDF (TF-IDF), and combinations thereof.
Category Data A P R F1

CDs and Vinyl M .600 .544 .772 .638
BoW .756 .716 .769 .742
BoW + M .784 .744 .799 .771
TF-IDF .769 .736 .767 .752
TF-IDF + M .787 .751 .797 .773

Electronics M .583 .529 .744 .618
BoW .676 .639 .656 .648
BoW + M .689 .640 .714 .675
TF-IDF .672 .651 .612 .631
TF-IDF + M .689 .649 .684 .666

Movies and TV M .564 .517 .792 .625
BoW .745 .705 .748 .726
BoW + M .773 .741 .767 .754
TF-IDF .757 .719 .761 .740
TF-IDF + M .777 .739 .784 .761

performed the tuning of the α and � parameters
with a 5-fold cross validation over the training set,
and we then used the best model to predict over
the test set. From the original set of 39,000 re-
views, 50% of them is used as training set, and the
other half as the test set. Each category is treated
singularly.

We run experiments both employing a plain
Bag-of-Words model, and with TF-IDF features.
Both preprocessing variants perform tokenization
and stemming3 and exclude stopwords and words
that do not appear more than five times in the
whole training set. To regularize the different
magnitude of the features, both textual features
and argumentation mining features are normal-
ized using the L2 normalization in all our exper-
iments. Textual and argumentative features are
simply concatenated into a single vector. The per-
formance is measured in terms of accuracy (A),
precision (P), recall (R), and F1, as in standard text
classification applications.

Table 1 shows that, even using only the features
obtained from MARGOT, thus completely ignor-
ing the textual content of the review, the accuracy
of the classifier is far above a random baseline.
Moreover, results clearly highlights how the im-
provement obtained by using argumentative fea-
tures is consistent across all product categories,
both using plain BoW and TF-IDF weighting. For
the “CDs and Vinyl”and “Electronics”categories

3We used Snowball from python nltk library.

the difference between the classifier exploiting
TF-IDF with MARGOT and the one using TF-
IDF only is statistically significant according to a
McNemar’s test, with p-value < 0.01. The same
holds for the BOW classifier, for the “Electron-
ics”and “Movies and TV”categories.

It is interesting to notice that, while the “CDs
and Vinyl” and the “Movies and TV” categories
have similar performance, even when using tex-
tual data only, the category “Electronics” results to
be the most difficult to predict. One plausible ex-
planation for this is the heterogeneity of such cat-
egory, that includes many different types of elec-
tronic devices. The other two categories, instead,
include more homogeneous products. It would be
very interesting to further investigate whether cer-
tain product categories result to be more suitable
for argumentation studies.

5 Conclusions

When reading online reviews of products, restau-
rants, and hotels, we typically appreciate those
that bring motivations and reasons rather than
plain opinions. In other words, we often look for
argumentative reviews. In this paper, we proposed
a first experimental study that aims to show how
features coming from an off-the-shelf argumenta-
tion mining system can help in predicting whether
a given review is useful.

We remark that this is just a preliminary study,
which yet opens the doors to several research di-
rections that we aim to investigate in future works.
First, we certainly plan to use more advanced ma-
chine learning systems, such as deep learning ar-
chitectures, that have achieved significant results
in many applications related to natural language
processing. In addition, we aim to address dif-
ferent learning problems, for example moving to
multi-class classification, or directly to regression.

The combination of textual and argumentative
features exploited in this work was effective in
confirming our intuition, but it can certainly be im-
proved. While building a dedicated argumentation
mining system for product reviews could require
an effort in terms of corpus annotation, we believe
that transfer learning here could play a crucial role.
Beyond using statistics obtained from the output
of an argumentation mining system as an addi-
tional input for a second-stage classifier, a unified
model combining the two steps could result to be
a smart compromise for this kind of application.



39

References
Hyunmi Baek, JoongHo Ahn, and Youngseok Choi.

2012. Helpfulness of online consumer reviews:
Readers’ objectives and review cues. International
Journal of Electronic Commerce, 17(2):99–126.

A. Ghose and P. G. Ipeirotis. 2011. Estimating the
helpfulness and economic impact of product re-
views: Mining text and reviewer characteristics.
IEEE Transactions on Knowledge and Data Engi-
neering, 23(10):1498–1512.

Soo-Min Kim, Patrick Pantel, Tim Chklovski, and
Marco Pennacchiotti. 2006. Automatically assess-
ing review helpfulness. In Proceedings of the 2006
Conference on empirical methods in natural lan-
guage processing, pages 423–430. Association for
Computational Linguistics.

Marco Lippi and Paolo Torroni. 2016a. Argumentation
mining: State of the art and emerging trends. ACM
Trans. Internet Technol., 16(2):10:1–10:25.

Marco Lippi and Paolo Torroni. 2016b. MARGOT: A
web server for argumentation mining. Expert Syst.
Appl., 65:292–303.

Haijing Liu, Yang Gao, Pin Lv, Mengxue Li, Shiqiang
Geng, Minglan Li, and Hao Wang. 2017. Using
argument-based features to predict and analyse re-
view helpfulness. In Proceedings of the 2017 Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP 2017, Copenhagen, Denmark,
September 9-11, 2017, pages 1358–1363. Associa-
tion for Computational Linguistics.

Julian McAuley and Jure Leskovec. 2013. Hidden fac-
tors and hidden topics: understanding rating dimen-
sions with review text. In Proceedings of the 7th
ACM conference on Recommender systems, pages
165–172. ACM.

Alessandro Moschitti. 2006. Efficient convolution
kernels for dependency and constituent syntactic
trees. In Johannes Frnkranz, Tobias Scheffer,
and Myra Spiliopoulou, editors, Machine Learning:
ECML 2006, volume 4212 of LNCS, pages 318–329.
Springer Berlin Heidelberg.

Susan M Mudambi and David Schuff. 2010. Research
note: What makes a helpful online review? a study
of customer reviews on amazon. com. MIS quar-
terly, pages 185–200.

Thomas L Ngo-Ye and Atish P Sinha. 2012. An-
alyzing online review helpfulness using a regres-
sional relieff-enhanced text mining method. ACM
Transactions on Management Information Systems
(TMIS), 3(2):10.

Thomas L Ngo-Ye and Atish P Sinha. 2014. The in-
fluence of reviewer engagement characteristics on
online review helpfulness: A text regression model.
Decision Support Systems, 61:47–58.

Douglas Walton. 2009. Argumentation theory: A very
short introduction. In Guillermo Simari and Iyad
Rahwan, editors, Argumentation in Artificial Intel-
ligence, pages 1–22. Springer US.


