



















































Neural Machine Translation with Source Dependency Representation


Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2846–2852
Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics

Neural Machine Translation with Source Dependency Representation
Kehai Chen1∗, Rui Wang2†, Masao Utiyama2, Lemao Liu3,

Akihiro Tamura4, Eiichiro Sumita2 and Tiejun Zhao1
1Machine Intelligence & Translation Laboratory, Harbin Institute of Technology

2ASTREC, National Institute of Information and Communications Technology (NICT)
3Tencent AI Lab

4Graduate School of Science and Engineering, Ehime University
{khchen and tjzhao}@hit.edu.cn

{wangrui, mutiyama and eiichiro.sumita}@nict.go.jp
lemaoliu@gmail.com and tamura@cs.ehime-u.ac.jp

Abstract

Source dependency information has been
successfully introduced into statistical
machine translation. However, there
are only a few preliminary attempts for
Neural Machine Translation (NMT), such
as concatenating representations of source
word and its dependency label together. In
this paper, we propose a novel attentional
NMT with source dependency represen-
tation to improve translation performance
of NMT, especially on long sentences.
Empirical results on NIST Chinese-to-
English translation task show that our
method achieves 1.6 BLEU improvements
on average over a strong NMT system.

1 Introduction

Neural Machine Translation (NMT) (Kalch-
brenner and Blunsom, 2013; Bahdanau et al.,
2014; Sutskever et al., 2014) relies heavily on
source representations, which encode implicitly
semantic information of source words by neural
networks (Mikolov et al., 2013a,b). Recently,
several research works have been proposed to
learn richer source representation, such as multi-
source information (Zoph and Knight, 2016; Firat
et al., 2016), and particularly source syntactic
information (Eriguchi et al., 2016; Li et al., 2017;
Huadong et al., 2017; Eriguchi et al., 2017), thus
improving the performance of NMT.

In this paper, we enhance source representations
by dependency information, which can capture
source long-distance dependency constraints for
word prediction. Actually, source dependency
information has been shown greatly effective in

∗Kehai Chen was an internship research fellow at NICT
when conducting this work.

†Corresponding author.

Statistical Machine Translation (SMT) (Garmash
and Monz, 2014; Kazemi et al., 2015; Hadiwinoto
et al., 2016; Chen et al., 2017; Hadiwinoto
and Ng, 2017). In NMT, there has been a
quite recent preliminary exploration (Sennrich and
Haddow, 2016), in which vector representations of
source word and its dependency label are simply
concatenated as source input, achieving state-of-
the-art performance in NMT (Bojar et al., 2016).

In this paper, we propose a novel NMT
with source dependency representation to improve
translation performance. Compared with the
simple approach of vector concatenation, we
learn the Source Dependency Representation
(SDR) to compute dependency context vectors
and alignment matrices in a more sophisticated
manner, which has the potential to make full use
of source dependency information. To this end,
we create a dependency unit for each source word
to capture long-distance dependency constraints.
Then we design an Encoder with convolutional
architecture to jointly learn SDRs (Section 3) and
source dependency annotations, thus computing
dependency context vectors and hidden states by
a novel double-context based Decoder for word
prediction (Section 4). Empirical results on NIST
Chinese-to-English translation task show that the
proposed approach achieves significant gains over
the method by Sennrich and Haddow (2016), and
thus delivers substantial improvements over the
standard attentional NMT (Section 5).

2 Background

An NMT model consists of an Encoder process
and a Decoder process, and hence it is often
called Encoder-Decoder model (Sutskever et al.,
2014; Bahdanau et al., 2014). Typically, each
unit of source input xj ∈ (x1, . . . , xJ) is firstly
embedded as a vector Vxj , and then represented as

2846



x3
x1
x4
x7
ε
/
/
/
/
/

Convolution layer 1

Max-pooling layer 1

Output layer VU2

Input layer

Convolution layer 2
Max-pooling layer 2

Figure 1: The CNN architecture for learning SRD.

an annotation vector hj by

hj = fenc(Vxj , hj−1), (1)

where fenc is a bidirectional Recurrent Neural
Network (RNN) (Bahdanau et al., 2014). These
annotation vectors H = (h1, . . . , hJ) are used to
generate the target word in the Decoder.

An RNN Decoder is used to compute the target
word yi probability by a softmax layer g:

p(yi|y<i, x) = g(ŷi−1, si, ci), (2)

where ŷi−1 is the previously emitted word, and si
is an RNN hidden state for the current time step:

si = ϕ(ŷi−1, si−1, ci), (3)

and the context vector ci is computed as a
weighted sum of these source annotations hj :

ci =
J∑

j=1

αijhj , (4)

where the normalized alignment weight αij is
computed by

αij =
exp(eij)∑J

k=1 exp(eik)
, (5)

where eij is an alignment which indicates how
well the inputs around position j and the output
at the position i match:

eij = f(si−1, hj). (6)

where f is a feedforward neural network.

3 Source Dependency Representation

In order to capture source long-distance depen-
dency constraints, we extract a dependency unit
Uj for each source word xj from dependency
tree, inspired by a dependency-based bilingual
composition sequence for SMT (Chen et al.,
2017). The extracted Uj is defined as the
following:

Uj = 〈PAxj , SIxj , CHxj 〉, (7)

where PAxj , SIxj , CHxj denote the parent,
siblings and children words of source word xj
in a dependency tree. Take x2 in Figure 2
as an example, the blue solid box U2 denotes
its dependency unit: PAx2 = 〈x3〉, SIx2 =
〈x1, x4, x7〉 and CHx2 = 〈ε〉 (no child), that is, U2
= 〈x3, x1, x4, x7, ε〉.

We design a simplified neural network follow-
ing Chen et al. (2017)’s Convolutional Neural
Network (CNN) method, to learn the SDR for
each source dependency unit Uj , as shown in
Figure 1. Our neural network consists of an input
layer, two convolutional layers, two pooling layers
and an output layer:

• Input layer: the input layer takes words of a
dependency unitUj in the form of embedding
vectors n×d, where n is the number of
words in a dependency unit and d is vector
dimension of each word. In our experiments,
we set n to 10,1 and d is 620. For dependency
units shorter than 10, we perform “/” padding
at the ending of Uj . For example, the padded
U2 is 〈x3, x1, x4, x7, ε, /, /, /, /, /〉.

1We find that 99% of all the source dependency units
contain no more than 10 words. So if the length is more than
10, the extra words are abandoned; if the length is less than
10, the rest positions are padded with “/”.

2847



• Convolutional layer: the first convolution
consists of one 3×d convolution kernels (the
stride is 1) to output an (n-2)×d matrix;
the second convolution consists of one 3×d
convolution kernels to output a n−22 ×d
matrix.

• Max-Pooling layer: the first pooling layer
performs row-wise max over the two con-
secutive rows to output a n−24 ×d matrix; the
second pooling layer performs row-wise max
over the two consecutive rows to output a
n−2

8 ×d matrix.
• Output layer: the output layer performs

row-wise average based on the output of
the second pooling layer to learn a compact
d-dimension vector VUj for Uj . In our
experiment, the output of the output layer is
1× d-dimension vector.

It should be noted that the dependency unit
is similar to the source dependency feature of
Sennrich and Haddow (2016) and the SDR
is the same to the source-side representation
of Chen et al. (2017). In comparison with
Sennrich and Haddow (2016), who concatenate
the source dependency labels and word together
to enhance the Encoder of NMT, we adapt a
separate attention mechanism together with a
CNN dependency Encoder. Compared with Chen
et al. (2017), which expands the famous neural
network joint model (Devlin et al., 2014) with
source dependency information to improve the
phrase pair translation probability estimation for
SMT, we focus on source dependency information
to enhance attention probability estimation and to
learn corresponding dependency context and RNN
hidden state for improving translation.

4 NMT with SDR

In this section, we propose two novel NMT mod-
els SDRNMT-1 and SDRNMT-2, both of which
can make use of source dependency information
SDR to enhance Encoder and Decoder of NMT.

4.1 SDRNMT-1
Compared with standard attentional NMT, the
Encoder of SDRNMT-1 model consists of a
convolutional architecture and an bidirectional
RNN, as shown in Figure 2. Therefore, the
proposed Encoder can not only learn composition-
al representations for dependency units but also

greatly tackle the sparsity issues associated with
large dependency units.

Motivated by (Sennrich and Haddow, 2016),
we concatenate the Vxj and VUj as input of the
Encoder, as shown in the black dotted box in
Figure 2. Source annotation vectors are learned
based on the concatenated representation with
dependency information:

hj = fenc(Vxj : VUj , hj−1), (8)

where “:” denotes the operation of vectors
concatenation. Finally, these learned annotation
vectors are as the input of the standard NMT
Decoder to jointly learn alignment and translation.
The only difference between our method and (Sen-
nrich and Haddow, 2016)’s method is that they
only use dependency label representation instead
of VUj .

4.2 SDRNMT-2

In SDRNMT-1, a single annotation, learned over
concatenating word representation and SDR, is
used to compute the context vector and the RNN
hidden state for the current time step. To relieve
more translation performance for NMT from the
SDR, we propose a double-context mechanism, as
shown in Figure 3.

First, the Encoder of SDRNMT-2 consists of
two independent annotations hj and dj :

hj = fenc(Vxj , hj−1),
dj = fenc(VUj , dj−1),

(9)

where H = [h1, · · · , hJ ] and D = [d1, · · · , dJ ]
encode source sequential and long-distance depen-
dency information, respectively.

The Decoder learns the corresponding align-
ment matrices and context vectors over the H
and D, respectively. That is, according to eq.(6),
given the previous hidden state ssi−1 and s

d
i−1, the

current alignments esi,j and e
d
i,j are computed over

source annotation vectors hj and dj , respectively:

esi,j = f(s
s
i−1 + hj),

edi,j = f(s
d
i−1 + dj).

(10)

According to eq.(5), we further compute the
current alignment α̃:

α̃i,j =
exp(λesi,j + (1− λ)edi,j)∑J

j=1 exp(λe
s
i,j + (1− λ)edi,j)

, (11)

2848



Src dep

Src

U2=<x3, x1, x4, x7 , ε>Dep Tuples U1
E
nc
od
er

Vx1 VU1

h1

Vx2 VU2

h2

VxJ VUJ

hJ

……

…

yi

D
ec
od
er

…

si…

ci

yi-1

si-1

…

…

UJ

CNNCNN CNN CNN

x1 x2 x3 x4 x5 x6 x7

root

…

,1i
,2i … ,i J

Figure 2: SDRNMT-1 for the i-th time step.
D
ec
od
er

sdi-1

yi-1

ssi-1
sdi

csi

yi

cdi

ssi

x1 x2 x3 x4 x5 x6 x7

Src dep

Src

root

U2=<x3, x1, x4, x7 , ε>Dep Tuples U1

E
nc
od
er

Vx1 VU1

h1

Vx2 VU2

h2

VxJ VUJ

hJ

……

…

UJ

CNNCNN CNN CNN

d1 d2 dJ…

…

… …

…
…

…
…

෤𝛼i,1 ෤𝛼i,1 ෤𝛼i,2
෤𝛼i,2 ෤𝛼i,J෤𝛼i,J

Figure 3: SDRNMT-2 for the i-th time step.

where λ is a hyperparameter2 to control the
importance of H and D. Note that compared with
the original alignment model only depending on
the sequential annotation vectorsH , the alignment
weight α̃i,j jointly compute statistic over source
sequential annotation vectors H and dependency
annotation vectors D.

The current context vector csi and c
d
i are

compute by eq.(4), respectively:

csi =
J∑

j=1

α̃i,jhj , and cdi =
J∑

j=1

α̃i,jdj . (12)

The current hidden state ssi and s
d
i are computed

by eq.(3), respectively:

ssi = ϕ(s
s
i−1, yi−1, c

s
i ),

sdi = ϕ(s
d
i−1, yi−1, c

d
i ).

(13)

Finally, according to eq.(2), the probabilities
for the next target word are computed using two
hidden states ssi and s

d
i , the previously emitted

word ŷi−1, the current sequential context vector
csi and dependency context vector cdi :

p(yi|y<i, x, T ) = g(ŷi−1, ssi , sdi , csi , cdi ). (14)
5 Experiment

5.1 Setting up
We carry out experiments on Chinese-to-English
translation. The training dataset consists of 1.42M

2λ can be tuned according to a subset FBIS of training
data and be set as 0.6 in the experiments.

sentence pairs extract from LDC corpora.3 We
use the Stanford dependency parser (Chang et al.,
2009) to generate the dependency tree for Chinese.
We choose the NIST 2002 (MT02) and the NIST
2003-2008 (MT03-08) datasets as the validation
set and test sets, respectively. Case-insensitive 4-
gram NIST BLEU score (Papineni et al., 2002) is
used as an evaluation metric, and signtest (Collins
et al., 2005) is as statistical significance test.

The baseline systems include the standard
Phrase-Based Statistical Machine Translation
(PBSMT) implemented in Moses (Koehn
et al., 2007) and the standard Attentional NMT
(AttNMT) (Bahdanau et al., 2014), where only
source word representation is utilized. We also
compare with a state-of-the-art syntax enhanced
NMT method (Sennrich and Haddow, 2016). For
a fair comparison, we only utilize dependency
information for (Sennrich and Haddow, 2016),
called Sennrich-deponly. We try our best to
re-implement the baseline methods on Nematus
toolkit 4 (Sennrich et al., 2017).

For all NMT systems, we limit the source and
target vocabularies to 30K, and the maximum
sentence length is 80. The word embedding
dimension is 620,5 and the hidden layer dimension

3LDC2002E18, LDC2003E07, LDC2003E14, Hansards
portion of LDC2004T07, LDC2004T08, and LDC2005T06.

4https://github.com/EdinburghNLP/nematus
5For SDRNMT-1, the 360 dimensions are from Vxj and

the 260 dimensions are from VUj .

2849



System Dev (MT02) MT03 MT04 MT05 MT06 MT08 AVG
PBSMT 33.15 31.02 33.78 30.33 29.62 23.53 29.66
AttNMT 36.31 34.02 37.11 32.86 32.54 25.44 32.40
Sennrich-deponly 36.68 34.51 38.09 33.37 32.96 26.96 32.98
SDRNMT-1 36.88 34.98* 38.14 34.61 33.58 27.06 33.32
SDRNMT-2 37.34 35.91** 38.73* 34.18** 33.76** 27.64* 34.04

Table 1: Results on NIST Chinese-to-English Translation Task. “*” indicates statistically significant
better than “Sennrich-deponly” at p-value < 0.05 and “**” at p-value < 0.01. AVG = average BLEU
scores for test sets.

is 1000, and all the layers use the dropout training
technique (Hinton et al., 2012). We shuffle
training set before training and the mini-batch size
is 80. Training is conducted on a single Tesla P100
GPU. All NMT models train for 15 epochs using
ADADELTA (Zeiler, 2012), and the train time is 6
days, which is 25% slower than the standard NMT.

5.2 Results and Analyses
Table 1 shows the translation performances on
test sets measured in BLEU score. The AttNMT
significantly outperforms PBSMT by 2.74 BLEU
points on average, indicating that it is a strong
baseline NMT system. The baseline Sennrich-
deponly improves the performance over the
AttNMT by 0.58 BLEU points on average. This
indicates that the proposed source dependency
constraint is beneficial for improving the perfor-
mance of NMT.

Moreover, SDRNMT-1 gains improvements of
0.92 and 0.34 BLEU points on average than the
AttNMT and Sennrich-deponly. These show that
the proposed SDR can more effectively capture
source dependency information than vector con-
catenation. Especially, the proposed SDRNMT-
2 outperforms the AttNMT and Sennrich-deponly
on average by 1.64 and 1.03 BLEU points. These
verify that the proposed double-context method is
effective for word prediction.

5.3 Effect of Translating Long Sentences
We follow (Bahdanau et al., 2014) to group
sentences of similar lengths all the test sets
(MT03-08), for example, “40” indicates that
the length of sentences is between 30 and 40,
and compute a BLEU score per group. As
demonstrated in Figure 4, the proposed models
outperform other baseline systems, especially in
translating long sentences. These results show that
the proposed models can effective encode long-
distance dependencies to improve translation.

25

30

35

10 20 30 40 50 60 70 80 80+

BL
EU

SENT LENGTHS

PBSMT
AttNMT
Sennrich-deponly
SDSNMT-1
SDSNMT-2

Figure 4: Translation qualities for different
sentence lengths.

6 Conclusion and Future Work

In this paper, we explored the source dependency
information to improve the performance of NMT.
We proposed a novel attentional NMT with source
dependency representation to capture source long-
distance dependencies. In the future, we will try
to exploit a general framework for utilizing richer
syntax knowledge.

Acknowledgments

We are grateful to the anonymous reviewers
for their insightful comments and suggestions.
This work is partially supported by the program
“Promotion of Global Communications Plan: Re-
search, Development, and Social Demonstration
of Multilingual Speech Translation Technology”
of MIC, Japan. Tiejun Zhao is supported by
the National Natural Science Foundation of China
(NSFC) via grant 91520204 and National High
Technology Research & Development Program of
China (863 program) via grant 2015AA015405.

2850



References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua

Bengio. 2014. Neural machine translation by
jointly learning to align and translate. CoRR,
abs/1409.0473.

Ondřej Bojar, Rajen Chatterjee, Christian Feder-
mann, Yvette Graham, Barry Haddow, Matthias
Huck, Antonio Jimeno Yepes, Philipp Koehn,
Varvara Logacheva, Christof Monz, Matteo Negri,
Aurelie Neveol, Mariana Neves, Martin Popel,
Matt Post, Raphael Rubino, Carolina Scarton,
Lucia Specia, Marco Turchi, Karin Verspoor, and
Marcos Zampieri. 2016. Findings of the 2016
conference on machine translation. In Proceedings
of the First Conference on Machine Translation,
pages 131–198, Berlin, Germany. Association for
Computational Linguistics.

Pi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, and
Christopher D. Manning. 2009. Discriminative
reordering with Chinese grammatical relations
features. In Proceedings of the Third Workshop on
Syntax and Structure in Statistical Translation at
NAACL HLT 2009, pages 51–59, Boulder, Colorado.
Association for Computational Linguistics.

Kehai Chen, Tiejun Zhao, Muyun Yang, and Lemao
Liu. 2017. Translation prediction with source
dependency-based context representation. In
Proceedings of the Thirty-First AAAI Conference on
Artificial Intelligence, pages 3166–3172, California,
USA. AAAI Press.

Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proceedings of the 43rd Annual
Meeting of the Association for Computational
Linguistics, pages 531–540, Ann Arbor, Michigan.
Association for Computational Linguistics.

Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas
Lamar, Richard Schwartz, and John Makhoul. 2014.
Fast and robust neural network joint models for
statistical machine translation. In Proceedings
of the 52nd Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long
Papers), pages 1370–1380, Baltimore, Maryland.
Association for Computational Linguistics.

Akiko Eriguchi, Kazuma Hashimoto, and Yoshimasa
Tsuruoka. 2016. Tree-to-sequence attentional
neural machine translation. In Proceedings of
the 54th Annual Meeting of the Association for
Computational Linguistics, pages 823–833, Berlin,
Germany. Association for Computational Linguis-
tics.

Akiko Eriguchi, Yoshimasa Tsuruoka, and Kyunghyun
Cho. 2017. Learning to parse and translate improves
neural machine translation. In Proceedings of
the 55th Annual Meeting of the Association for
Computational Linguistics, Vancouver, Canada.
Association for Computational Linguistics.

Orhan Firat, Kyunghyun Cho, and Yoshua Bengio.
2016. Multi-way, multilingual neural machine
translation with a shared attention mechanism. In
Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 866–875, San Diego, California. Association
for Computational Linguistics.

Ekaterina Garmash and Christof Monz. 2014.
Dependency-based bilingual language models for
reordering in statistical machine translation. In
Proceedings of the 2014 Conference on Empirical
Methods in Natural Language Processing,
pages 1689–1700, Doha, Qatar. Association
for Computational Linguistics.

Christian Hadiwinoto, Yang Liu, and Hwee Tou Ng.
2016. To swap or not to swap? exploiting
dependency word pairs for reordering in statistical
machine translation. In Proceedings of the Thirtieth
AAAI Conference on Artificial Intelligence, pages
2943–2949, Arizona, USA. AAAI Press.

Christian Hadiwinoto and Hwee Tou Ng. 2017.
A dependency-based neural reordering model for
statistical machine translation. In Proceedings
of the Thirty-First AAAI Conference on Artificial
Intelligence, California, USA. AAAI Press.

Geoffrey E. Hinton, Nitish Srivastava, Alex
Krizhevsky, Ilya Sutskever, and Ruslan
Salakhutdinov. 2012. Improving neural networks
by preventing co-adaptation of feature detectors.
CoRR.

Chen Huadong, Huang Shujian, Chiang David, and
Chen Jiajun. 2017. Improved neural machine
translation with a syntax-aware encoder and de-
coder. In Proceedings of the 55th Annual Meeting
of the Association for Computational Linguistics,
Vancouver, Canada. Association for Computational
Linguistics.

Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
continuous translation models. In Proceedings
of the 2013 Conference on Empirical Methods
in Natural Language Processing, pages 1700–
1709, Seattle, Washington, USA. Association for
Computational Linguistics.

Arefeh Kazemi, Antonio Toral, Andy Way, Amirhas-
san Monadjemi, and Mohammadali Nematbakhsh.
2015. Dependency-based reordering model for
constituent pairs in hierarchical smt. In Proceedings
of the 18th Annual Conference of the European
Association for Machine Translation, pages 43–
50, Antalya, Turkey. Association for Computational
Linguistics.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open

2851



source toolkit for statistical machine translation.
In Proceedings of the 45th Annual Meeting of the
Association for Computational Linguistics Compan-
ion Volume Proceedings of the Demo and Poster
Sessions, pages 177–180, Prague, Czech Republic.
Association for Computational Linguistics.

Junhui Li, Xiong Deyi, Tu Zhaopeng, Zhu Muhua,
and Zhou Guodong. 2017. Modeling source syntax
for neural machine translation. In Proceedings
of the 55th Annual Meeting of the Association
for Computational Linguistics, Vancouver, Canada.
Association for Computational Linguistics.

Tomas Mikolov, Kai Chen, Greg Corrado, and
Jeffrey Dean. 2013a. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg
Corrado, and Jeffrey Dean. 2013b. Distributed
representations of words and phrases and their
compositionality. In Proceedings of the 26th
International Conference on Neural Information
Processing Systems, pages 3111–3119, USA. Cur-
ran Associates Inc.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311–318, Philadelphia,
Pennsylvania, USA. Association for Computational
Linguistics.

Rico Sennrich, Orhan Firat, Kyunghyun Cho, Alexan-
dra Birch, Barry Haddow, Julian Hitschler, Marcin
Junczys-Dowmunt, Samuel Läubli, Antonio Valerio
Miceli Barone, Jozef Mokry, and Maria Nadejde.
2017. Nematus: a toolkit for neural machine transla-
tion. In Proceedings of the Software Demonstrations
of the 15th Conference of the European Chapter
of the Association for Computational Linguistics,
pages 65–68, Valencia, Spain. Association for
Computational Linguistics.

Rico Sennrich and Barry Haddow. 2016. Linguistic
input features improve neural machine translation.
In Proceedings of the First Conference on Machine
Translation, pages 83–91, Berlin, Germany. Associ-
ation for Computational Linguistics.

Ilya Sutskever, Oriol Vinyals, and Quoc V. Le.
2014. Sequence to sequence learning with
neural networks. In Proceedings of the 27th
International Conference on Neural Information
Processing Systems, pages 3104–3112, Cambridge,
MA, USA. MIT Press.

Matthew D. Zeiler. 2012. ADADELTA: an adaptive
learning rate method. CoRR, abs/1212.5701.

Barret Zoph and Kevin Knight. 2016. Multi-
source neural translation. In Proceedings of the
2016 Conference of the North American Chapter

of the Association for Computational Linguistics:
Human Language Technologies, pages 30–34, San
Diego, California. Association for Computational
Linguistics.

2852


