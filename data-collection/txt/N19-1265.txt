



















































Beyond task success: A closer look at jointly learning to see, ask, and GuessWhat


Proceedings of NAACL-HLT 2019, pages 2578–2587
Minneapolis, Minnesota, June 2 - June 7, 2019. c©2019 Association for Computational Linguistics

2578

Beyond Task Success:
A Closer Look at Jointly Learning to See, Ask, and GuessWhat

Ravi Shekhar†, Aashish Venkatesh∗, Tim Baumgärtner∗, Elia Bruni∗,
Barbara Plank♥, Raffaella Bernardi† and Raquel Fernández∗

†University of Trento, ∗University of Amsterdam,♥IT University of Copenhagen
ravi.shekhar@unitn.it aashishv@outlook.com

baumgaertner.t@gmail.com elia.bruni@gmail.com bapl@itu.dk
raffaella.bernardi@unitn.it raquel.fernandez@uva.nl

Abstract

We propose a grounded dialogue state encoder
which addresses a foundational issue on how
to integrate visual grounding with dialogue
system components. As a test-bed, we fo-
cus on the GuessWhat?! game, a two-player
game where the goal is to identify an object in
a complex visual scene by asking a sequence
of yes/no questions. Our visually-grounded
encoder leverages synergies between guessing
and asking questions, as it is trained jointly
using multi-task learning. We further enrich
our model via a cooperative learning regime.
We show that the introduction of both the joint
architecture and cooperative learning lead to
accuracy improvements over the baseline sys-
tem. We compare our approach to an alter-
native system which extends the baseline with
reinforcement learning. Our in-depth analysis
shows that the linguistic skills of the two mod-
els differ dramatically, despite approaching
comparable performance levels. This points
at the importance of analyzing the linguistic
output of competing systems beyond numeric
comparison solely based on task success.1

1 Introduction

Over the last few decades, substantial progress has
been made in developing dialogue systems that ad-
dress the abilities that need to be put to work dur-
ing conversations: Understanding and generating
natural language, planning actions, and tracking
the information exchanged by the dialogue partic-
ipants. The latter is particularly critical since, for
communication to be effective, participants need
to represent the state of the dialogue and the com-
mon ground established through the conversation
(Stalnaker, 1978; Lewis, 1979; Clark, 1996).

In addition to the challenges above, dialogue
is often situated in a perceptual environment. In

1Equal contribution by R. Shekhar and A. Venkatesh.

Figure 1: Our questioner model with a single visually
grounded dialogue state encoder.

this study, we develop a dialogue agent that builds
a representation of the context and the dialogue
state by integrating information from both the vi-
sual and linguistic modalities. We take the Guess-
What?! game (de Vries et al., 2017) as our test-
bed, a two-player game where a Questioner faces
the task of identifying a target object in a visual
scene by asking a series of yes/no questions to an
Oracle. We model the agent in the Questioner’s
role.

To model the Questioner, previous work relies
on two independent models to learn to ask ques-
tions and to guess the target object, each equipped
with its own encoder (de Vries et al., 2017; Strub
et al., 2017; Zhu et al., 2017; Lee et al., 2017;
Shekhar et al., 2018; Zhang et al., 2018). We
propose an end-to-end architecture with a single
visually-grounded dialogue state encoder (cf. Fig-
ure 1). Our system is trained jointly in a super-
vised learning setup, extended with a cooperative
learning (CL) regime: By letting the model play
the game with self-generated dialogues, the com-
ponents of the Questioner agent learn to better per-
form the overall Questioner’s task in a cooperative
manner. Das et al. (2017b) have explored the use
of CL to train two visual dialogue agents that re-
ceive joint rewards when they play a game suc-
cessfully. To our knowledge, ours is the first ap-



2579

proach where cooperative learning is applied to the
internal components of a grounded conversational
agent.

Our cooperative learning regime can be seen as
an interesting alternative to reinforcement learning
(RL)—which was first applied to GuessWhat?!
by Strub et al. (2017)—because it is entirely dif-
ferentiable and computationally less expensive to
train than RL. Little is known on how this learning
approach compares to RL not only regarding task
success, but also in terms of the quality of the lin-
guistic output, a gap we seek to fill in this paper.
In particular, our contributions are:2

• The introduction of a single visually-
grounded dialogue state encoder jointly
trained with the guesser and question gener-
ator modules to address a foundational ques-
tion of how to integrate visual grounding with
dialogue system components; this yields up
to 9% improvement on task success.

• The effectiveness of cooperative learning,
which yields an additional increase of 8.7%
accuracy, while being easier to train than RL.

• A first in-depth study to compare cooperative
learning to a state-of-the-art RL system. Our
study shows that the linguistic skills of the
models differ dramatically, despite approach-
ing comparable task success levels. This un-
derlines the importance of linguistic analysis
to complement solely numeric evaluation.

2 Related Work

Task-oriented dialogue systems The conven-
tional architecture of task-oriented dialogue sys-
tems includes a pipeline of components, and the
task of tracking the dialogue state is typically
modelled as a partially-observable Markov deci-
sion process (Williams et al., 2013; Young et al.,
2013; Kim et al., 2014) that operates on a sym-
bolic dialogue state consisting of predefined vari-
ables. The use of symbolic representations to char-
acterise the state of the dialogue has some ad-
vantages (e.g., ease of interfacing with knowledge
bases), but it has also some key disadvantages: the
variables to be tracked have to be defined in ad-
vance and the system needs to be trained on data
annotated with explicit state configurations.

2Code and supplementary material are available at
https://vista-unitn-uva.github.io.

Given these limitations, there has been a shift
towards neural end-to-end systems that learn their
own representations. Early works focus on non-
goal-oriented chatbots (Vinyals and Le, 2015; Sor-
doni et al., 2015; Serban et al., 2016; Li et al.,
2016a,b). Bordes et al. (2017) propose a mem-
ory network to adapt an end-to-end system to task-
oriented dialogue. Recent works combine conven-
tional symbolic with neural approaches (Williams
et al., 2017; Zhao and Eskenazi, 2016; Rastogi
et al., 2018), but all focus on language-only di-
alogue. We propose a visually grounded task-
oriented end-to-end dialogue system which, while
maintaining the crucial aspect of the interaction
of the various modules at play in a conversational
agent, grounds them through vision.

Visual dialogue agents In recent years, re-
searchers in computer vision have proposed tasks
that combine visual processing with dialogue in-
teraction. Pertinent datasets created by Das et al.
(2017a) and de Vries et al. (2017) include Vis-
Dial and GuessWhat?!, respectively, where two
participants ask and answer questions about an im-
age. While impressive progress has been made
in combining vision and language, current mod-
els make simplifications regarding the integration
of these two modalities and their exploitation for
task-related actions. For example, the models pro-
posed for VisDial by Das et al. (2017a) concern
an image guessing game where one agent does not
see the target image (thus, no multimodal under-
standing) and is required to ‘imagine’ it by asking
questions. The other agent does see the image, but
only responds to questions without the need to per-
form additional actions.

In GuessWhat?!, the Questioner agent sees an
image and asks questions to identify a target object
in it. The Questioner’s role hence involves a com-
plex interaction of vision, language, and guessing
actions. Most research to date has investigated
approaches consisting of different models trained
independently (de Vries et al., 2017; Strub et al.,
2017; Zhu et al., 2017; Lee et al., 2017; Shekhar
et al., 2018; Zhang et al., 2018). We propose
the first multimodal dialogue agent for the Guess-
What?! task where all components of the Ques-
tioner agent are integrated into a joint architecture
that has at its core a visually-grounded dialogue
state encoder (cf. Figure 1).

Reinforcement learning for visual dialogue
agents was introduced by Das et al. (2017b) for

https://vista-unitn-uva.github.io


2580

VisDial and by Strub et al. (2017) for Guess-
What?!. Our joint architecture allows us to explore
a simpler solution based on cooperative learning
between the agent’s internal modules (see Sec-
tion 5 for details).

3 Task and Data

The GuessWhat?! game (de Vries et al., 2017) is
a simplified instance of a referential communica-
tion task where two players collaborate to identify
a referent—a setting used extensively in human-
human collaborative dialogue (Clark and Wilkes-
Gibbs, 1986; Yule, 1997; Zarrieß et al., 2016).

The GuessWhat?! dataset3 was collected via
Amazon Mechanical Turk by de Vries et al.
(2017). The task involves two human participants
who see a real-world image, taken from the MS-
COCO dataset (Lin et al., 2014). One of the par-
ticipants (the Oracle) is assigned a target object
in the image and the other participant (the Ques-
tioner) has to guess it by asking Yes/No questions
to the Oracle. There are no time constraints to play
the game. Once the Questioner is ready to make
a guess, the list of candidate objects is provided
and the game is considered successful if the Ques-
tioner picks the target object. The dataset consists
of around 155k English dialogues about approxi-
mately 66k different images. Dialogues contain on
average 5.2 questions-answer pairs.

4 Models

We focus on developing an agent who plays the
role of the Questioner in GuessWhat?!.

4.1 Baseline Model
As a baseline model (BL), we consider our own
implementation of the best performing system put
forward by de Vries et al. (2017). It consists of
two independent models: a Question Generator
(QGen) and a Guesser. For the sake of simplic-
ity, QGen asks a fixed number of questions before
the Guesser predicts the target object.

QGen is implemented as an Recurrent Neural
Network (RNN) with a transition function handled
with Long-Short-Term Memory (LSTM) (Hochre-
iter and Schmidhuber, 1997), on which a prob-
abilistic sequence model is built with a Softmax
classifier. At each time step in the dialogue, the
model receives as input the raw image and the
dialogue history and generates the next question

3Dataset: https://guesswhat.ai/download.

one word at a time. The image is encoded by
extracting its VGG-16 features (Simonyan and
Zisserman, 2014). In our new joint architec-
ture (described below in Section 4.2), we use
ResNet152 (He et al., 2016) features instead of
VGG, because they tend to yield better perfor-
mance in image classification and are more ef-
ficient to compute. For the baseline model it
turns out that the original VGG-16 features lead
to better performance (41.8% accuracy for VGG-
16 vs. 37.3% with ResNet152 features). While we
use ResNet152 features in our models, we keep the
original VGG-16 feature configuration as de Vries
et al. (2017), which constitutes a stronger baseline.

The Guesser model exploits the annotations in
the MS-COCO dataset (Lin et al., 2014) to repre-
sent candidate objects by their object category and
their spatial coordinates. This yields better perfor-
mance than using raw image features in this case,
as reported by de Vries et al. (2017). The objects’
categories and coordinates are passed through a
Multi-Layer Perceptron (MLP) to get an embed-
ding for each object. The Guesser also takes as in-
put the dialogue history processed by its own ded-
icated LSTM. A dot product between the hidden
state of the LSTM and each of the object embed-
dings returns a score for each candidate object.

The model playing the role of the Oracle is in-
formed about the target object otarget. Like the
Guesser, the Oracle does not have access to the
raw image features. It receives as input embed-
dings of the target object’s category, its spatial co-
ordinates, and the current question asked by the
Questioner, encoded by a dedicated LSTM. These
three embeddings are concatenated and fed to an
MLP that gives an answer (Yes or No).

4.2 Visually-Grounded Dialogue State
Encoder

In line with the baseline model, our Questioner
agent includes two sub-modules, a QGen and a
Guesser. As in the baseline, the Guesser guesses
after a fixed number of questions, which is a pa-
rameter tuned on the validation set. Our agent
architecture differs from the baseline model by
de Vries et al.: Rather than operating indepen-
dently, the language generation and guessing mod-
ules are connected through a common grounded
dialogue state encoder (GDSE) which combines
linguistic and visual information as a prior for the
two modules. Given this representation, we will

https://guesswhat.ai/download


2581

MLP

<person> <car>

⊙
softmax (                                                              ) 

MLPMLP
ht

visually grounded 
dialogue state

⊙⊙

category and coordinates of candidate objects

<bat>

<sos>

is theit batter

ResNet 152

QGen

Guesser

Figure 2: Question Generation and Guesser modules.

refer to our Questioner agent as GDSE.
As illustrated in Figure 1, the encoder receives

as input representations of the visual and linguis-
tic context. The visual representation consists of
the second to last layer of ResNet152 trained on
ImageNet. The linguistic representation is ob-
tained by an LSTM (LSTMe) which processes
each new question-answer pair in the dialogue. At
each question-answer QAt, the last hidden state of
LSTMe is concatenated with the image features I ,
passed through a linear layer and a tanh activation
to result in the final layer ht:

ht = tanh (W · [LSTMe(qa1:t−1); I]) (1)

where [·; ·] represents concatenation, I ∈ R2048×1,
LSTMe ∈ R1024×1 and W ∈ R512×3072 (identical
to prior work except for tuning the ResNet-specific
parameters). We refer to this final layer as the dia-
logue state, which is given as input to both QGen
and Guesser.

As illustrated in Figure 2, our QGen and
Guesser modules are like the corresponding mod-
ules by de Vries et al. (2017), except for the crucial
fact that they receive as input the same grounded
dialogue state representation. QGen employs an
LSTM (LSTMq) to generate the token sequence
for each question conditioned on ht, which is used
to initialise the hidden state of LSTMq. As input
at every time step, QGen receives a dense embed-
ding of the previously generated token wi−1 and
the image features I:

p(wi) = p(wi|w1, ..., wi−1, ht, I) (2)

We optimise QGen by minimising the Negative
Log Likelihood (NLL) of the human dialogues and

use the Adam optimiser (Kingma and Ba, 2015):

LQ =
∑
i

− log p(wi) (3)

Thus, in our architecture the LSTMq of QGen
in combination with the LSTMe of the Encoder
form a sequence-to-sequence model (Sutskever
et al., 2014), conditioned on the visual and linguis-
tic context — in contrast to the baseline model,
where question generation is performed by a sin-
gle LSTM on its own.

The Guesser consists of an MLP which is eval-
uated for each candidate object in the image. It
takes the dense embedding of the category and the
spatial information of the object to establish a rep-
resentation rj ∈ R512×1 for each object. A score
is calculated for each object by performing the dot
product between the dialogue state ht and the ob-
ject representation. Finally, a softmax over the
scores results in a probability distribution over the
candidate objects:

p(oj) =
eh

T
t ·rj∑

j e
hTt ·rj

(4)

We pick the object with the highest probability and
the game is successful if oguess = otarget , where
oguess = argmaxj p(oj). As with QGen, we op-
timise the Guesser by minimising the NLL and
again make use of Adam:

LG = − log p(otarget) (5)

The resulting architecture is fully differentiable.
In addition, the GDSE agent faces a multi-task
optimisation problem: While the QGen optimises
LQ and the Guesser optimises LG, the parame-
ters of the Encoder (W , LSTMe) are optimised via
both LQ and LG. Hence, both tasks faced by the
Questioner agent contribute to the optimisation of
the dialogue state ht, and thus to a more effective
encoding of the input context.

5 Learning Approach

We first introduce the supervised learning ap-
proach used to train both BL and GDSE, then our
cooperative learning regime, and finally the rein-
forcement learning approach we compare to.

5.1 Supervised Learning
In the baseline model, the QGen and the Guesser
modules are trained autonomously with super-
vised learning (SL): QGen is trained to replicate



2582

human questions and, independently, the Guesser
is trained to predict the target object. Our new ar-
chitecture with a common dialogue state encoder
allows us to formulate these two tasks as a multi-
task problem, with two different losses (Eq. 3 and
5 in Section 4.2). These two tasks are not equally
difficult: While the Guesser has to learn the prob-
ability distribution of the set of possible objects
in the image, QGen needs to fit the distribution
of natural language words. Thus, QGen has a
harder task to optimize and requires more param-
eters and training iterations. We address this issue
by making the learning schedule task-dependent.
We call this setup modulo-n training, where n in-
dicates after how many epochs of QGen training
the Guesser is updated together with QGen.

Using the validation set, we experimented with
n from 5 to 15 and found that updating the Guesser
every 7 epochs worked best. With this opti-
mal configuration, we then train GDSE for 100
epochs (batch size of 1024, Adam, learning rate of
0.0001) and select the Questioner module best per-
forming on the validation set (henceforth, GDSE-
SL or simply SL).

5.2 Cooperative Learning

Once the model has been trained with SL, new
training data can be generated by letting the agent
play new games. Given an image from the train-
ing set used in the SL phase, we generate a new
training instance by randomly sampling a target
object from all objects in the image. We then let
our Questioner agent and the Oracle play the game
with that object as target, and further train the
common encoder using the generated dialogues by
backpropagating the error with gradient descent
through the Guesser. After training the Guesser
and the encoder with generated dialogues, QGen
needs to ‘readapt’ to the newly arranged encoder
parameters. To achieve this, we re-train QGen on
the human data with SL, but using the new encoder
states. Also here, the error is backpropagated with
gradient descent through the common encoder.

Regarding modulo-n, in this case QGen is up-
dated at every nth epoch, while the Guesser is up-
dated at all other epochs; we experimented with
n from 3-7 and set it to the optimal value of 5.
The GDSE previously trained with SL is further
trained with this cooperative learning regime for
100 epochs (batch size of 256, Adam, learning rate
of 0.0001), and we select the Questioner module

performing best on the validation set (henceforth,
GDSE-CL or simply CL).

5.3 Reinforcement Learning
Strub et al. (2017) proposed the first extension of
BL (de Vries et al., 2017) with deep reinforce-
ment learning (RL). They present an architecture
for end-to-end training using an RL policy. First,
the Oracle, Guesser, and QGen models are trained
independently using supervised learning. Then,
QGen is further trained using a policy gradient.

We use the publicly available code and pre-
trained model based on Sampling (Strub et al.,
2017), which resulted in the closest performance
to what was reported by the authors.4 This is the
RL model we use throughout the rest of the paper.

5.4 Experimental Details
We use the same train (70%), validation (15%),
and test (15%) splits as de Vries et al. (2017). The
test set contains new images not seen during train-
ing. We use two experimental setups for the num-
ber of questions to be asked by the question gen-
erator, motivated by prior work: 5 questions (5Q)
following de Vries et al. (2017), and 8 questions
(8Q) as in Strub et al. (2017). As noted in Sec-
tion 3, on average, there are 5.2 questions per dia-
logue in the GuessWhat?! data set.

For evaluation, we report task success in terms
of accuracy (Strub et al., 2017). To neutralize
the effect of random sampling in training CL, we
trained the model 3 times. RL is tested 3 times
with sampling. We report means and standard de-
viation (for some tables these are provided in the
supplementary material; see footnote 2).

6 Results

Table 1 reports the results for all models. There
are several take-aways.

Grounded joint architecture First of all, our
visually-grounded dialogue state encoder is ef-
fective. GDSE-SL outperforms the baseline
by de Vries et al. (2017) significantly in both
setups (absolute accuracy improvements of 6.6%

4Their result of 53.3% accuracy published in Strub et al.
(2017) is obsolete, as stated on their GitHub page (https:
//github.com/GuessWhatGame/guesswhat)
where they report 56.5% for sampling and 58.4% for greedy
search. By running their code, we could only replicate
their results with sampling, obtaining 56%, while greedy
and beam search resulted in similar or worse performance.
Our analysis showed that greedy and beam search have the
additional disadvantage of learning a smaller vocabulary.

https://github.com/GuessWhatGame/guesswhat
https://github.com/GuessWhatGame/guesswhat


2583

Model 5Q 8Q

Baseline 41.2 40.7
GDSE-SL 47.8 49.7
GDSE-CL 53.7 (±.83) 58.4 (±.12)
RL 56.2 (±.24) 56.3 (±.05)

Table 1: Test set accuracy for each model (for setups
with 5 and 8 questions). GDSE-SL is our grounded
supervised learning system, GDSE-CL the cooperative
learning setup, and RL the results we obtain with the
reinforcement learning system by Strub et al. (2017).

and 9%). To evaluate the impact of the multi-
task learning aspect, we did an ablation study and
used the encoder-decoder architecture to train the
QGen and Guesser modules independently. With
such a decoupled training we obtain lower results:
44% and 43.7% accuracy for 5Q and 8Q, respec-
tively. Hence, the multi-task component brings an
increase of up to 6% over the baseline.5

Cooperative learning and RL The introduction
of the cooperative learning approach results in a
clear improvement over GDSE-SL: +8.7% (8Q:
from 49.7 to 58.4) and +5.9% (with 5Q). Despite
its simplicity, our GDSE-CL model achieves a task
success rate which is comparable to RL: In the 8Q
setup, GDSE-CL reaches an average accuracy of
58.4 versus 56.3 for RL, giving CL a slight edge
in this setup (+2.1%), while in the 5Q setup RL is
slightly better (+2.5%). Overall, the accuracy of
the CL and RL models is close. The interesting
question is how the linguistic skills and strategy of
these two models differ, to which we turn in the
next section.

We compared to Strub et al. (2017), but RL has
also been put forward by Zhang et al. (2018), who
report 60.7% accuracy (5Q). This result is close to
our highest GDSE-CL result (60.8 ±0.51, when
optimized for 10Q).6 Their RL system integrates
several partial reward functions to increase coher-
ence, which is an interesting aspect. Yet their code
is not publicly available. We leave the comparison
to Zhang et al. (2018) and adding RL to GDSE to
future work.

5While de Vries et al. (2017) originally report an accuracy
of 46.8%, this result was later revised to 40.8%, as clarified
on their GitHub page. Our own implementation of the base-
line system achieves an accuracy of 41.2%.

6Since our aim is to compare to the best setup for BL (5Q)
and RL (8Q), we do not report our results with 10Q in Table 1.

7 Analysis

In this section, we present a range of analyses that
aim to shed light on the performance of the mod-
els. They are carried out on the test set data using
the 8Q setting, which yields better results than the
5Q setting for the GDSE models and RL. Given
that there is only a small difference in accuracy for
the baseline with 5Q and 8Q, for comparability we
analyse dialogues with 8Q also for BL.

7.1 Quantitative Analysis of Linguistic
Output

We analyse the language produced by the Ques-
tioner agent with respect to three factors: (1) lexi-
cal diversity, measured as type/token ratio over all
games, (2) question diversity, measured as the per-
centage of unique questions over all games, and
(3) the number of games with questions repeated
verbatim. We compute these factors on the test set
for the models and for the human data (H).

As shown in Table 2, the linguistic output of SL
& CL is closer to the language used by humans:
Our agent is able to produce a much richer and less
repetitive output than both BL and RL. In particu-
lar, it learns to use a more diverse vocabulary, gen-
erates more unique questions, and repeats ques-
tions within the same dialogue at a much lower
rate than the baseline and RL: 93.5% of the games
played by BL contain at least one verbatim ques-
tion repetition, for RL this happens in 96.47% of
the cases, whereas for SL and CL this is for only
55.8% and 52.19% of the games, respectively.

Lexical
diversity

Question
diversity

% Games with
repeated Q’s

BL 0.030 1.60 93.50
SL 0.101 13.61 55.80
CL 0.115 (±.02) 14.15 (±3.0) 52.19 (±4.7)
RL 0.073 (±.00) 1.04 (±.03) 96.47 (±.04)
H 0.731 47.89 —

Table 2: Statistics of the linguistic output of all models
with the 8Q setting and of humans (H) in all test games.

7.2 Dialogue Strategy

To further understand the variety of questions
asked by the agents, we classify questions into dif-
ferent types. We distinguish between questions
that aim at getting the category of the target ob-
ject (ENTITY questions, e.g., ‘is it a vehicle?’) and
questions about properties of the queried objects
(ATTRIBUTE questions, e.g., ‘is it square?’ or ‘are



2584

Humans [success]
1. does it have cereal on it? no
2. does it have pink frosting? no
3. does it have cookies? no
4. is it a donut? yes
5. does it have maple frosting? no
6. is there candy? yes
7. is it the doughnut with M&Ms? no
8. is it dark brown? no

GDSE-CL [success]
1. is it a donut? yes
2. is it on the left? no
3. is it on the right? yes
4. is it the whole donut? yes
5. does it have pink icing? no
6. is it touching the donut with the sprinkles? yes
7. does it have chocolate icing? no
8. is it the third donut? yes

RL [failure]
1. is it food? yes
2. is it a donut? yes
3. is it in left? no
4. is it in top? yes
5. is it in top? yes
6. is it in top? yes
7. is it in top? yes
8. is it top? yes

Figure 3: Game example where GDSE-CL succeeds and RL fails at guessing the target object (framed).

they standing?’). Within ATTRIBUTE questions,
we make a distinction between color, shape, size,
texture, location, and action questions. Within EN-
TITY questions, we distinguish questions whose
focus is an object category or a super-category
(see the supplementary material for example ques-
tions). The classification is done by manually ex-
tracting keywords for each question type from the
human dialogues, and then applying an automatic
heuristic that assigns a class to a question given
the presence of the relevant keywords.7 This pro-
cedure allows us to classify 91.41% of the ques-
tions asked by humans. The coverage is higher for
the questions asked by the models: 98.88% (BL),
94.72% (SL), 94.11% (CL) and 99.51 % (RL).8

The statistics are shown in Table 3. We use
Kullback-Leibler (KL) divergence to measure how
the output of each model differs from the human
distribution of fine-grained question classes. The
baseline’s output has the highest degree of diver-
gence: For instance, the BL model does never ask
any SHAPE or TEXTURE questions, and hardly any
SIZE questions. The output of the RL model also
differs substantially from the human dialogues: It
asks a very large number of LOCATION questions
(74.8% vs. 40% for humans). Our model, in con-
trast, generates question types that resemble the
human distribution more closely.

We also analyse the structure of the dialogues in
terms of the sequences of question types asked. As
expected, both humans and models almost always
start with an ENTITY question (around 97% for
BL, SL and CL, 98.7% for RL, and 78.48% for hu-
mans), in particular a SUPER-CATEGORY (around
70% for BL, SL and CL, 84% for RL, and 52.32%
for humans). In some cases, humans start by ask-

7A question may be tagged with several attribute classes if
keywords of different types are present. E.g., “Is it the white
one on the left?” is classified as both COLOR and LOCATION.

8In the supplementary material we provide details on the
question classification procedure: the lists of keywords by
class, the procedure used to obtain these lists, as well as the
pseudo-code of the heuristics used to classify the questions.

Question type BL SL CL RL H

ENTITY 49.00 48.07 46.51 23.99 38.11
SUPER-CAT 19.6 12.38 12.58 14.00 14.51
OBJECT 29.4 35.70 33.92 9.99 23.61
ATTRIBUTE 49.88 46.64 47.60 75.52 53.29
COLOR 2.75 13.00 12.51 0.12 15.50
SHAPE 0.00 0.01 0.02 0.003 0.30
SIZE 0.02 0.33 0.39 0.024 1.38
TEXTURE 0.00 0.13 0.15 0.013 0.89
LOCATION 47.25 37.09 38.54 74.80 40.00
ACTION 1.34 7.97 7.60 0.66 7.59
Not classified 1.12 5.28 5.90 0.49 8.60

KL (wrt human) 0.953 0.042 0.038 0.396 0.0

Table 3: Percentage of questions per question type in
all the test set games played by humans (H) and the
models with the 8Q setting, and KL divergence from
human distribution of fine-grained question types.

ing questions directly about an attribute that may
easily distinguish an object from others, while this
is very uncommon for models. Figure 3 shows an
example: The human dialogue begins with an AT-
TRIBUTE question (‘does it have cereal on it?’),
which in this case is not very effective and leads to
a change in strategy at turn 4. The CL model starts
by asking an OBJECT question (‘is it a donut?’)
while the RL model begins with a more generic
SUPER-CATEGORY question (‘is it food?’).

We check how the answer to a given question
type affects the type of the follow-up question. In
principle, we expect to find that question types
that are answered positively will be followed by
more specific questions. This is indeed what we
observe in the human dialogues, as shown in Ta-
ble 4. For example, when a SUPER-CATEGORY
question is answered positively, humans follow up
with an OBJECT or ATTRIBUTE question 89.56%
of the time. This trend is mirrored by all models.
Overall, the models also learn the strategy to move
from an OBJECT to an ATTRIBUTE question when
an OBJECT question receives a Yes answer. The
BL, SL, and CL models do this to a lesser extent
than humans, while the RL model systematically



2585

(a) Lexical diversity (b) Question diversity (c) % Games w/ repeated Q’s (d) KL-distance from human

Figure 4: Evolution of linguistic factors over 100 training epochs for our GDSE-CL model. Note: lexical and
question diversity of the human data fall outside the range in (a) / (b). The same is the case with KL for BL in (d).

Question type shift BL SL CL RL H

SUPER-CAT→ OBJ/ATT 89.05 92.61 89.75 95.63 89.56
OBJECT→ ATTRIBUTE 67.87 60.92 65.06 99.46 88.70

Table 4: Proportion of question type shift vs. no type
shift in consecutive questions Qt → Qt+1 where Qt
has received a Yes answer.

transitions to attributes (in 99.46% of cases), using
mostly LOCATION questions as pointed out above.
For example (Figure 3), after receiving an affirma-
tive answer to the OBJECT question ‘is it a donut?’
both CL and RL shift to a LOCATION question.
Once location is established, CL moves on to other
attributes while RL keeps asking the same LOCA-
TION question, which leads to failure. Further il-
lustrative examples are given in the supplementary
material.

7.3 Analysis of the CL Learning Process

In order to better understand the effect of the co-
operative learning regime, we trace the evolution
of linguistic factors identified above over the CL
epochs. As illustrated in Figure 4 (a) and (b),
through the epochs the CL model learns to use
a richer vocabulary and more diverse questions,
moving away from the levels achieved by BL and
RL, overpassing SL and moving toward humans.

The CL model progressively produces fewer
repeated questions within a dialogue, improving
over SL in the last few epochs, cf. Figure 4 (c). Fi-
nally, (d) illustrates the effect of modulo-n train-
ing: As the model is trained on generated dia-
logues, its linguistic output drifts away from the
human distribution of question types; every 5th

epoch QGen is trained via supervision, which
brings the model’s behaviour closer back to hu-
man linguistic style and helps decrease the drift.

8 Conclusion

We present a new visually-grounded joint Ques-
tioner agent for goal-oriented dialogue. First, we
show that our architecture archives 6–9% accuracy
improvements over the GuessWhat?! baseline sys-
tem (de Vries et al., 2017). This way, we address
a foundational limitation of previous approaches
that model guessing and questioning separately.

Second, our joint architecture allows us to pro-
pose a two-phase cooperative learning approach
(CL), which further improves accuracy. It results
in our overall best model and reaches state-of-the-
art results (cf. Section 6). We compare CL to
the system proposed by Strub et al. (2017) which
extends the baseline with reinforcement learning
(RL). We find that the two approaches (CL and
RL) achieve overall relatively similar task success
rates. However, evaluating on task success is only
one side of the coin. Finally and most importantly,
we propose to pursue an in-depth analysis of the
quality of the dialogues by visual conversational
agents, which is an aspect often neglected in the
literature. We analyze the linguistic output of the
two models across three factors (lexical diversity,
question diversity, and repetitions) and find them
to differ substantially. The CL model uses a richer
vocabulary and inventory of questions, and pro-
duces fewer repeated questions than RL. In con-
trast, RL highly relies on asking location ques-
tions, which might be explained by a higher re-
liance on spatial and object-type information ex-
plicitly given to the Guesser and Oracle models.
Limiting rewards to task success or other rewards
not connected to the language proficiency does not
stimulate the model to learn rich linguistic skills,
since a reduced vocabulary and simple linguistic
structures may be an efficient strategy to succeed
at the game.



2586

Overall, the presence of repeated questions re-
mains an important weakness of all models, re-
sulting in unnatural dialogues. This shows that
there is still a considerable gap to human-like con-
versational agents. Looking beyond task success
can provide a good basis for extensions of cur-
rent architectures, e.g., Shekhar et al. (2018) add a
decision-making component that decides when to
stop asking questions which results in less repet-
itive and more human-like dialogues. Our joint
architecture could easily be extended with such a
component.

Acknowledgements

The work carried out by the Amsterdam team
was partially funded by the Netherlands Organi-
sation for Scientific Research (NWO) under VIDI
grant nr. 276-89-008, Asymmetry in Conversa-
tion. We thank the University of Trento for gen-
erously funding a research visit by Barbara Plank
to CIMeC that led to part of the work presented in
this paper. In addition, we kindly acknowledge the
support of NVIDIA Corporation with the donation
to the University of Trento of the GPUs used in our
research.

References
Antoine Bordes, Y-Lan Boureau, and Jason Weston.

2017. Learning end-to-end goal oriented dialog. In
Proceedings of ICLR.

Herbert H Clark. 1996. Using Language. Cambridge
University Press.

Herbert H Clark and Deanna Wilkes-Gibbs. 1986.
Referring as a collaborative process. Cognition,
22(1):1–39.

Abhishek Das, Satwik Kottur, Khushi Gupta, Avi
Singh, Deshraj Yadav, José M.F. Moura, Devi
Parikh, and Dhruv Batra. 2017a. Visual Dialog. In
Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition.

Abhishek Das, Satwik Kottur, José M.F. Moura, Ste-
fan Lee, and Dhruv Batra. 2017b. Learning coop-
erative visual dialog agents with deep reinforcement
learning. In International Conference on Computer
Vision (ICCV).

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2016. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pages 770–
778.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Dongho Kim, Catherine Breslin, Pirros Tsiakoulis,
M Gašić, Matthew Henderson, and Steve Young.
2014. Inverse reinforcement learning for micro-turn
management. In Fifteenth Annual Conference of the
International Speech Communication Association.

D. Kingma and J. Ba. 2015. Adam: A method for
stochastic optimization. In Proceedings of ICLR.

Sang-Woo Lee, Yujung Heo, and Byoung-Tak Zhang.
2017. Answerer in questioner’s mind for goal-
oriented visual dialogue. In NIPS Workshop
on Visually-Grounded Interaction and Language
(ViGIL).

David Lewis. 1979. Scorekeeping in a language game.
Journal of Philosophical Logic, 8(1):339–359.

Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,
and Bill Dolan. 2016a. A diversity-promoting ob-
jective function for neural conversation models. In
Proceedings of NAACL-2016, pages 110–119.

Jiwei Li, Will Monroe, Alan Ritter, Michel Galley,
Jianfeng Gao, and Dan Jurafsky. 2016b. Deep rein-
forcement learning for dialogue generation. In Pro-
ceedings of EMNLP.

T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona,
D. Ramanan, Dollar, P., and C. L. Zitnick. 2014. Mi-
crosoft COCO: Common objects in context. In Pro-
ceedings of ECCV (European Conference on Com-
puter Vision).

Abhinav Rastogi, Raghav Gupta, and Dilek Hakkani-
Tur. 2018. Multi-task learning for joint language
understanding and dialogue state tracking. In Pro-
ceedings of SIGdial.

Iulian V Serban, Alessandro Sordoni, Yoshua Bengio,
Aaron Courville, and Joelle Pineau. 2016. Building
end-to-end dialogue systems using generative hier-
archical neural network models. In Proceedings of
the Thirtieth AAAI Conference on Artificial Intelli-
gence.

Ravi Shekhar, Tim Baumgärtner, Aashish Venkatesh,
Elia Bruni, Raffaella Bernardi, and Raquel
Fernández. 2018. Ask no more: Deciding when
to guess in referential visual dialogue. In Pro-
ceedings of the 27th International Conference
on Computational Linguistics (COLING), pages
1218–1233.

Karen Simonyan and Andrew Zisserman. 2014. Very
deep convolutional networks for large-scale image
recognition. Technical Report arXiv:1409.1556, Vi-
sual Geometry Group, University of Oxford.

Alessandro Sordoni, Michel Galley, Michael Auli,
Chris Brockett, Yangfeng Ji, Margaret Mitchell,
Jian-Yun Nie, Jianfeng Gao, and Bill Dolan. 2015.



2587

A neural network approach to context-sensitive gen-
eration of conversational responses. In Proceedings
of NAACL-HLT, pages 196–205.

Robert Stalnaker. 1978. Assertion. In P. Cole, edi-
tor, Pragmatics, volume 9 of Syntax and Semantics.
New York Academic Press.

Florian Strub, Harm de Vries, Jeremie Mary, Bilal
Piot, Aaron Courville, and Olivier Pietquin. 2017.
End-to-end optimization of goal-driven and visually
grounded dialogue systems. In Proceedings of the
International Joint Conference on Artificial Intelli-
gence (IJCAI).

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in Neural Information Process-
ing Systems, pages 3104–3112.

Oriol Vinyals and Quoc V. Le. 2015. A neural conver-
sational model. In ICML Deep Learning Workshop.

Harm de Vries, Florian Strub, Sarath Chandar, Olivier
Pietquin, Hugo Larochelle, and Aaron C. Courville.
2017. Guesswhat?! Visual object discovery through
multi-modal dialogue. In Conference on Computer
Vision and Pattern Recognition (CVPR).

Jason Williams, Kavosh Asadi, and Geoffrey Zweig.
2017. Hybrid code networks: Practical and efficient
end-to-end dialog control with supervised and rein-
forcement learning. In Proceedings of ACL-2017.
Association for Computational Linguistics.

Jason Williams, Antoine Raux, Deepak Ramachan-
dran, and Alan Black. 2013. The dialog state track-
ing challenge. In Proceedings of the SIGDIAL 2013
Conference, pages 404–413.

Steve Young, Milica Gašić, Blaise Thomson, and Ja-
son D Williams. 2013. POMDP-based statistical
spoken dialog systems: A review. Proceedings of
the IEEE, 101(5).

George Yule. 1997. Referential communication tasks.
Routledge.

Sina Zarrieß, Julian Hough, Casey Kennington,
Ramesh Manuvinakurike, David DeVault, Raquel
Fernández, and David Schlangen. 2016. Pentoref:
A corpus of spoken references in task-oriented dia-
logues. In 10th edition of the Language Resources
and Evaluation Conference.

Junjie Zhang, Qi Wu, Chunhua Shen, Jian Zhang, Jian-
feng Lu, and Anton van den Hengel. 2018. Goal-
oriented visual question generation via intermediate
rewards. In Proceedings of the European Confer-
ence of Computer Vision (ECCV).

Tiancheng Zhao and Maxine Eskenazi. 2016. Towards
end-to-end learning for dialog state traching and
management using deep reinforcement learning. In
Proceedings of SIGDIAL-2016.

Yan Zhu, Shaoting Zhang, and Dimitris Metaxas.
2017. Interactive reinforcement learning for ob-
ject grounding via self-talking. In NIPS Work-
shop on Visually-Grounded Interaction and Lan-
guage (ViGIL).


