Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 572–580,

Beijing, August 2010

572

Evaluating N-gram based Evaluation Metrics for

Automatic Keyphrase Extraction

Su Nam Kim, Timothy Baldwin

CSSE

University of Melbourne

sunamkim@gmail.com, tb@ldwin.net

Min-Yen Kan

School of Computing

National University of Singapore
kanmy@comp.nus.edu.sg

Abstract

This paper describes a feasibility study
of n-gram-based evaluation metrics for
automatic keyphrase extraction. To ac-
count for near-misses currently ignored
by standard evaluation metrics, we adapt
various evaluation metrics developed for
machine translation and summarization,
and also the R-precision evaluation metric
from keyphrase evaluation. In evaluation,
the R-precision metric is found to achieve
the highest correlation with human anno-
tations. We also provide evidence that
the degree of semantic similarity varies
with the location of the partially-matching
component words.

1 Introduction

Keyphrases are noun phrases (NPs) that are repre-
sentative of the main content of documents. Since
they represent the key topics in documents, ex-
tracting good keyphrases beneﬁts various natu-
ral language processing (NLP) applications such
as summarization, information retrieval (IR) and
question-answering (QA). Keyphrases can also be
used in text summarization as semantic metadata
(Barzilay and Elhadad, 1997; Lawrie et al., 2001;
D’Avanzo and Magnini, 2005). In search engines,
keyphrases supplement full-text indexing and as-
sist users in creating good queries.

In the past, a large body of work on keyphrases
has been carried out as an extraction task, uti-
lizing three types of cohesion:
(1) document
cohesion, i.e. cohesion between documents and
keyphrases (Frank et al., 1999; Witten et al., 1999;

Matsuo and Ishizuka, 2004; Medelyan and Wit-
ten, 2006; Nguyen and Kan, 2007; Wan and
Xiao, 2008); (2) keyphrase cohesion, i.e. cohe-
sion among keyphrases (Turney, 2003); and (3)
term cohesion, i.e. cohesion among terms in a
keyphrase (Park et al., 2004).

Despite recent successes in keyphrase extrac-
tion (Frank et al., 1999; Turney, 2003; Park et al.,
2004; Medelyan and Witten, 2006; Nguyen and
Kan, 2007), current work is hampered by the in-
ﬂexibility of standard metrics in evaluating differ-
ent approaches. As seen in other ﬁelds, e.g. ma-
chine translation (MT) and multi-document sum-
marization, the advent of standardized automatic
evaluation metrics, combined with standardized
datasets, has enabled easy comparison of sys-
tems and catalyzed the respective research ar-
eas. Traditionally, the evaluation of automatic
keyphrase extraction has relied on the number
of exact matches in author-assigned keyphrases
and reader-assigned keyphrases. The main prob-
lem with this approach is that even small vari-
ants in the keyphrases are not given any credit.
For example, given the gold-standard keyphrase
effective grid computing algorithm, grid com-
puting algorithm is a plausible keyphrase candi-
date and should be scored appropriately, rather
than being naively evaluated as wrong. Addition-
ally, author-assigned keyphrases and even reader-
assigned keyphrases often have their own prob-
lems in this type of evaluation (Medelyan and Wit-
ten, 2006). For example, some keyphrases are
often partly or wholly subsumed by other can-
didates or may not even occur in the document.
Therefore, counting the exactly-matching candi-
dates has been shown to be suboptimal (Jarmasz

573

and Barriere, 2004).

Our goal in this paper is to evaluate the relia-
bility of automatic evaluation metrics that better
account for near-misses. Prior research based on
semantic similarity (Jarmasz and Barriere, 2004;
Mihalcea and Tarau, 2004; Medelyan and Wit-
ten, 2006) has taken the approach of using ex-
ternal resources such as large corpora, Wikipedia
or manually-curated index words. While we ac-
knowledge that these methods can help address
the near-miss problem, they are impractical due
to the effort required to compile the requisite re-
sources for each individual evaluation exercise,
and furthermore, the resources tend to be domain-
speciﬁc. In order to design a cheap, practical and
stable keyphrase evaluation metric, our aim is to
properly account for these near-misses without re-
liance on costly external resources.

According to our analysis, the degree of se-
mantic similarity of keyphrase candidates varies
relative to the location of overlap. For exam-
ple, the candidate grid computing algorithm has
higher semantic similarity than computing algo-
rithm with the gold-standard keyphrase effective
grid computing algorithm. Also, computing algo-
rithm is closer than effective grid to the same gold-
standard keyphrase. From these observations, we
infer that n-gram-based evaluation metrics can
be applied to evaluating keyphrase extraction, but
also that candidates with the same relative n-gram
overlap are not necessarily equally good.

Our primary goal is to test the utility of n-gram
based evaluation metrics to the task of keyphrase
extraction evaluation. We test the following eval-
uation metrics: (1) evaluation metrics from MT
and multi-document summarization (BLEU, NIST,
METEOR and ROUGE); and (2) R-precision (Zesch
and Gurevych, 2009), an n-gram-based evalua-
tion metric developed speciﬁcally for keyphrase
extraction evaluation which has yet to be evalu-
ated against humans at the extraction task. Sec-
ondarily, we attempt to shed light on the bigger
question of whether it is feasible to expect that
n-gram-based metrics without access to external
resources should be able to capture subtle seman-
tic differences in keyphrase candidates. To this
end, we experimentally verify the impact of lex-
ical overlap of different types on keyphrase sim-

ilarity, and use this as the basis for proposing a
variant of R-precision.

In the next section, we present a brief primer on
keyphrases. We then describe the MT and sum-
marization evaluation metrics trialled in this re-
search, along with R-precision, modiﬁed R-precision
and a semantic similarity-based evaluation metric
for keyphrase evaluation (Section 3). In Section 4,
we discuss our gold-standard and candidate ex-
traction method. We compare the evaluation met-
rics with human assigned scores for suitability in
Section 5, before concluding the paper.

2 A Primer on Keyphrases

Keyphrases can be either simplex words (e.g.
query, discovery, or context-awareness)1 or larger
N-bars/noun phrases (e.g.
intrusion detection,
mobile ad-hoc network, or quality of service).
The majority of keyphrases are 1–4 words long
(Paukkeri et al., 2008).

Keyphrases are normally composed of nouns
and adjectives, but may occasionally contain ad-
verbs (e.g. dynamically allocated task, or partially
observable Markov decision process) or other
parts of speech. They may also contain hyphens
(e.g. sensor-grouping or multi-agent system) and
apostrophes for possessives (e.g. Bayes’ theorem
or agent’s goal).

Keyphrases can optionally incorporate PPs (e.g.
service quality vs. quality of service). A variety of
prepositions can be used (e.g. incentive for coop-
eration, inequality in welfare, agent security via
approximate policy), although the genetive of is
the most common.

Keyphrases can also be coordinated, either as
simple nouns at the top level (e.g. performance
and scalability or group and partition) or within
more complex NPs or between N-bars (e.g. his-
tory of past encounter and transitivity or task and
resource allocation in agent system).

When candidate phrases get too long, abbre-
viations also help to form valid keyphrases (e.g.
computer support collaborative work vs. CSCW,
or partially observable Markov decision process
vs. POMDP).

1All examples in this section are taken from the data set

outlined in Section 4.

574

3 Evaluation Metrics

There have been various evaluation metrics de-
veloped and validated for reliability in ﬁelds such
as MT and summarization (Callison-Burch et al.,
2009). While n-gram-based metrics don’t cap-
ture systematic alternations in keyphrases, they do
support partial match between keyphrase candi-
dates and the reference keyphrases.

In this section, we ﬁrst introduce a range of
popular n-gram-based evaluation metrics from
the MT and automatic summarization literature,
which we naively apply to the task of keyphrase
evaluation. We then present R-precision, an n-
gram-based evaluation metric developed specif-
ically for keyphrase evaluation, and propose a
modiﬁed version of R-precision which weights n-
grams according to their relative position in the
keyphrase. Finally, we present a semantic similar-
ity method.

3.1 Machine Translation and Summarization

Evaluation Metrics

In this research, we experiment with four popu-
lar n-gram-based metrics from the MT and au-
tomatic summarization ﬁelds — BLEU, METEOR,
NIST and ROUGE. The basic task performed by the
respective evaluation metrics is empirical determi-
nation of how good an approximation is string1 of
string2?, which is not far removed from the re-
quirements of keyphrase evaluation. We brieﬂy
outline each of the methods below.

One subtle property of keyphrase evaluation is
that there is no a priori preference for shorter
keyphrases over longer keyphrases, unlike MT
where shorter strings tend to be preferred. Hence,
we use the longer NP as reference and the shorter
NP as a translation, to avoid the length penalty in
most MT metrics.2

BLEU (Papineni et al., 2002) is an evaluation
metric for measuring the relative similarity be-
tween a candidate translation and a set of ref-
erence translations, based on n-gram composi-
tion. It calculates the number of overlapping n-
grams between the candidate translation and the

2While we don’t present the numbers in this paper, the
results were lower for the MT evaluation metrics without this
reordering of the reference and candidate keyphrases.

set of reference translations. In order to avoid hav-
ing very short translations receive artiﬁcially high
scores, BLEU adds a brevity penalty to the scoring
equation.

METEOR (Agarwal and Lavie, 2008) is similar
to BLEU, in that it measures string-level similarity
between the reference and candidate translations.
The difference is that it allows for more match
ﬂexibility, including stem variation and WordNet
synonymy. The basic metric is based on the num-
ber of mapped unigrams found between the two
strings, the total number of unigrams in the trans-
lation, and the total number of unigrams in the ref-
erence.

NIST (Martin and Przybocki, 1999) is once
again similar to BLEU, but integrates a propor-
tional difference in the co-occurrences for all n-
grams while weighting more heavily n-grams that
occur less frequently, according to their informa-
tion value.

ROUGE (Lin and Hovy, 2003) — and its vari-
ants including ROUGE-N and ROUGE-L — is simi-
larly based on n-gram overlap between the can-
didate and reference summaries. For example,
ROUGE-N is based on co-occurrence statistics,
using higher-order n-grams (n > 1) to esti-
mate the ﬂuency of summaries. ROUGE-L uses
longest common subsequence (LCS)-based statis-
tics, based on the assumption that the longer the
substring overlap between the two strings,
the
greater the similar Saggion et al. (2002). ROUGE-
W is a weighted LCS-based statistic that priori-
tizes consecutive LCSes. In this research, we ex-
periment exclusively with the basic ROUGE met-
ric, and unigrams (i.e. ROUGE-1).

3.2 R-precision
In order to analyze near-misses in keyphrase ex-
traction evaluation, Zesch and Gurevych (2009)
proposed R-precision, an n-gram-based evalua-
tion metric for keyphrase evaluation.3 R-precision
contrasts with the majority of previous work on
keyphrase extraction evaluation, which has used
semantic similarity based on external resources

3Zesch and Gurevych’s R-precision has nothing to do with
the information retrieval evaluation metric of the same name,
where P@N is calculated for N equal to the number of rele-
vant documents.

575

(Jarmasz and Barriere, 2004; Mihalcea and Tarau,
2004; Medelyan and Witten, 2006). As our inter-
est is in fully automated evaluation metrics which
don’t require external resources and are domain
independent (for maximal reproducibility of re-
sults), we experiment only with R-precision in this
paper.

R-precision is based on the number of overlap-
ping words between a keyphrase and a candi-
date, as well as the length of each. The met-
ric differentiates three types of near-misses: IN-
CLUDE, PARTOF and MORPH. The ﬁrst two
types are based on an n-gram approach, while
the third relies on lexical variation. As we use
stemming, in line with the majority of previous
work on keyphrase extraction evaluation, we fo-
cus exclusively on the ﬁrst two cases, namely IN-
CLUDE, and PARTOF. The ﬁnal score returned
by R-precision is:

number of overlapping word(s)
length of keyphrase/candidate

where the denominator is the longer of
keyphrase and candidate.

the

Zesch and Gurevych (2009) evaluated R-
precision over three corpora (Inspec, DUC and SP)
based on 566 non-exact matching candidates. In
order to evaluate the human agreement, they hired
4 human annotators to rate the near-miss candi-
dates, and reported agreements of 80% and 44%
for the INCLUDE and PARTOF types, respec-
tively. They did not, however, perform holistic
evaluation with human scores to verify its relia-
bility in full system evaluation. This is one of our
contributions in this paper.

3.3 Modiﬁed R-precision
In this section, we describe a modiﬁcation to
R-precision which assigns different weights for
component words based on their position in the
keyphrase (unlike R-precision which assigns the
same score for each matching component word).
The head noun generally encodes the core seman-
tics of the keyphrase, and as a very rough heuris-
tic, the further a word is from the head noun,
the less semantic import on the keyphrase it has.
As such, modiﬁed R-precision assigns a score to
each component word relative to its position as

1

N−i+1 where N is the number of com-
CW =
ponent words in the keyphrase and i is the posi-
tion of the component word in the keyphrase (1 =
leftmost word).

1

1

1

1

1

1

2

1

2 + 1

3 + 1

2 + 1

3 + 1

3 + 1

2 + 1

= 5

= 9

11 and

For example, AB and BC from ABC would be
scored as
11, re-
spectively. Thus, with the keyphrase effective
grid computing algorithm and candidates effec-
tive grid, grid computing and computing algo-
rithm, modiﬁed R-precision assigns different scores
for each candidate (computing algorithm > grid
computing > effective grid). In contrast, the orig-
inal R-precision assigns the same score to all can-
didates.

3.4 Semantic Similarity
In Jarmasz and Barriere (2004) and Mihalcea and
Tarau (2004), the authors used a large data set
to compute the semantic similarity of two NPs
to assign partial credits for semantically similar
candidate keyphrases. To simulate these meth-
ods, we adopted the distributional semantic simi-
larity using web documents. That is, we computed
the similarity between a keyphrase and its sub-
string by cosine measure over collected the snip-
pets from Yahoo! BOSS.4 We use the computed
similarity as our score for near-misses.

4 Data

4.1 Data Collection
We constructed a keyphrase extraction dataset us-
ing papers across 4 different categories5 of the
ACM Digital Library.6
In addition to author-
assigned keyphrases provided as part of the ACM
Digital Library, we generated reader-assigned
keyphrases by assigning 250 students 5 papers
each, a list of candidate keyphrases (see below for
details), and standardized instructions on how to
assign keyphrases. It took them an average of 15
minutes to annotate each paper. This is the same

4http://developer.yahoo.com/search/

boss/

5C2.4 (Distributed Systems), H3.3 (Information Search
and Retrieval), I2.11 (Distributed Artiﬁcial Intelligence –
Multiagent Systems) and J4 (Social and Behavioral Sciences
– Economics).

6http://portal.acm.org/dl.cfm

576

Author

Reader

Total

1298/1305

3110/3221

3816/3962

3.85/4.01

12.44/12.88

15.26/15.85

2537

2509

3027

2864

Total
NPs
Average
Found

937

769

Table 1: Details of the keyphrase dataset

(Rule1) NBAR = (NN*|JJ*)∗(NN*)
e.g. complexity, effective algorithm,
distributed web-service discovery architecture
(Rule2) NBAR IN NBAR
e.g. quality of service, sensitivity of VOIP trafﬁc,
simpliﬁed instantiation of zebroid

Table 2: Regular expressions for candidate selec-
tion

document collection and set of keyphrase annota-
tions as was used in the SemEval 2010 keyphrase
extraction task (Kim et al., 2010).

Table 1 shows the details of the ﬁnal dataset.
The numbers after the slashes indicate the number
of keyphrases after including alternate keyphrases
based on of -PPs. Despite the reliability of author-
assigned keyphrases discussed in Medelyan and
Witten (2006), many author-assigned keyphrases
and some reader-assigned keyphrases are not
found verbatim in the source documents because:
(1) many of them are substrings of the candidates
or vice versa (about 75% of the total keyphrases
are found in the documents); and (2) our candi-
date selection method does not extract keyphrases
in forms such as coordinated NPs or adverbial
phrases.

4.2 Candidate Selection
During preprocessing, we ﬁrst converted the
PDF versions of
the papers into text using
pdftotext. We then lemmatized and POS
tagged all words using morpha and the Lingua
POS tagger. Next, we applied the regular expres-
sions in Table 2 to extract candidates, based on
Nguyen and Kan (2007). Finally, we selected can-
didates in terms of their frequency: simplex words
with frequency ≥ 2 and NPs with frequency ≥ 1.
We observed that for reader-assigned keyphrases,
NPs were often selected regardless of their fre-

quency in the source document. In addition, we
allowed variation in the possessive form, noun
number and abbreviations.

Rule1 detects simplex nouns or N-bars as candi-
dates. Rule2 extracts N-bars with post-modifying
PPs. In Nguyen and Kan (2007), Rule2 was not
used to additionally extract N-bars inside modify-
ing PPs. For example, our rules extract not only
performance of grid computing as a candidate, but
also grid computing. However, we did not extend
the candidate selection rules to cover NPs includ-
ing adverbs (e.g. partially-observable Markov de-
cision process) or conjunctions (e.g. behavioral
evolution and extrapolation), as they are rare.

4.3 Human Assigned Score
We hired four graduate students working in NLP
to assign human scores to substrings in the gold-
standard data. The scores are between 0 and 4
(0 means no semantic overlap between a NP and
its substring, while 4 means semantically indistin-
guishable).

We broke down the candidate–keyphrases pairs
into subtypes, based on where the overlap oc-
curs relative to the keyphrase (e.g. ABCD): (1)
Head:
the candidate contains the head noun of
the keyphrase (e.g. CD); (2) First:
the candi-
date contains the ﬁrst word of the keyphrase (e.g.
AB); and (3) Middle: the candidate overlaps with
the keyphrase, but contains neither its ﬁrst word
nor its head word (e.g. BC). The average human
scores are 1.94 and 2.11 for First and Head, re-
spectively, when the candidate is shorter, while
they are 2.00, 1.89 and 2.15 for First, Middle, and
Head, respectively when the candidate is longer.
Note that we did not have Middle instances with
candidates as the shorter string. The scores are
slightly higher for the keyphrases as substrings
than for the candidates as substrings.

5 Correlation
To check the feasibility of metrics for keyphrase
evaluation, we checked the Spearman rank corre-
lation between the machine-generated score and
the human-assigned score for each keyphrase–
candidate pairing.

As the percentage of annotators who agree on
the exact score is low (i.e. 2 subjects agree ex-

577

Human

.4506
.4510
.4551
.4603
.4604
.4638

R-precision
Orig Mod
.2840
.4763
.2806
.5264
.4834
.2893
.3438
.4763
.3434
.5264
.4838
.3547

BLEU METEOR

NIST

ROUGE

.3250
.3242
.3439
.3407
.3423
.3679

.3246
.3238
.3437
.3403
.3421
.3675

.3366
.3369
.3584
.3514
.3547
.3820

.3246
.3240
.3437
.3404
.3422
.3676

Semantic
Similarity

.2116
.2050
.1980
.2224
.2168
.2123

Average

Majority

All
L ≤ 4
L ≤ 3
All
L ≤ 4
L ≤ 3

Table 3: Rank correlation between humans and the different evaluation metrics, based on the human
average (top half) and majority (bottom half)

LOCATION

COMPLEXITY

POS

First
Middle
Head
Simple

PP
CC
AdjN
NN

Human

.5508
.5329
.3783
.4452
.4771
.3645
.4616
.4467

R-precision
Orig Mod
.5033
.5032
.5741
.5988
.4838
.4838
.4715
.2790
.4814
.1484
.3810
.3140
.4844
.3507
.4586
.2581

BLEU METEOR NIST ROUGE

.3844
.4669
.3865
.3653
.3367
.3748
.3147
.3321

.3844
.4669
.3860
.3445
.3122
.3446
.3132
.3321

.4057 .3844
.4055 .4669
.3780 .3864
.3527 .3445
.3443 .3123
.3384 .3748
.3115 .3133
.3488 .3322

Table 4: Rank correlation between human average judgments and n-gram-based metrics

actly on 55%-70% of instances, 3 subjects agree
exactly on 25%-35% of instances), we require a
method for combining the annotations. We ex-
periment with two combination methods: major-
ity and average. The majority is simply the label
with the majority of annotations associated with
it; in the case of a tie, we break the tie by select-
ing that annotation which is closest to the median.
The average is simply the average score across all
annotators.

5.1 Overall Correlation with Human Scores
Table 3 presents the correlations between the hu-
man scores (acting as an upper bound for the
task), as well as those between human scores
with machine-generated scores. We ﬁrst present
the overall results, then results over the subset of
keyphrases of length 4 words or less, and also 3
words or less. We present the results for the anno-
tator average and majority in top and bottom half,
respectively, of the table.

To compute the correlation between the hu-
man annotators, we used leave-one-out cross-

validation, holding out one annotator, and com-
paring them to the combination of the remaining
annotators (using either the majority or average
method to combine the remaining annotations).
This was repeated across all annotators, and the
Spearman’s ρ was averaged across the annotators.
Overall, we found that R-precision achieved the
highest correlation with humans, above the inter-
annotator correlation in all instances. That is,
based on the evaluation methodology employed,
it is performing slightly above the average level
of a single annotator. The relatively low inter-
annotator correlation is, no doubt, due to the dif-
ﬁculty of the task, as all of our near-misses have
2 or more terms, and the annotators have to make
very ﬁne-grained, and ultimately subjective, deci-
sions about the true quality of the candidate.

Comparing the n-gram-based methods with the
semantic similarity-based method,
the n-gram-
based metrics achieved higher correlations across
the board, with BLEU, METEOR, NIST and ROUGE
all performing remarkably consistently, but well

578

LOCATION

COMPLEXITY

POS

First
Middle
Head
Simple

PP
CC
AdjN
NN

Human

.5642
.5510
.4147
.4580
.4715
.5777
.4501
.4631

R-precision
Orig Mod
.5163
.5162
.5320
.4991
.5073
.5074
.4869
.3394
.5068
.3724
.5513
.3841
.4861
.3968
.4733
.3244

BLEU METEOR NIST ROUGE

.4032
.4175
.4156
.3653
.3367
.5745
.3266
.3499

.4032
.4175
.4153
.3651
.3367
.5571
.3251
.3499

.4297 .4032
.3653 .4175
.4042 .4156
.3715 .3651
.3652 .3367
.5600 .5745
.3246 .3252
.3648 .3500

Table 5: Rank correlation between human majority and n-gram-based metrics

below the level of R-precision. Due to the markedly
lower performance of the semantic similarity-
based method, we do not consider it for the re-
mainder of our experiments. A general ﬁnding
was that as the length of the keyphrase (L) got
longer, the correlation tended to be higher across
all n-gram-based metrics.

One disappointment at this stage is that the re-
sults for modiﬁed R-precision are well below those
of the original, especially over the average of the
human annotators.

5.2 Correlation with Different NP Subtypes
To get a clearer sense of how the different eval-
uation metrics are performing, we broke down
the keyphrases according to three syntactic sub-
classiﬁcations:
(1) the location of overlap (see
Section 4.3); (2) the complexity of the NP (does
the keyphrase contain a preposition [PP], a con-
junction [CC] or neither a preposition nor a con-
junction [Simple]?); and (3) the word class se-
quence of the keyphrase (is the keyphrase an NN
[NN] or an AdjN sequence [AdjN]?). We present
the results in Tables 4 and Table 4 for the human
average and majority, respectively, presenting re-
sults in boldface when the correlation for a given
method is higher than for that same method in
our holistic evaluation in Table 3 (i.e. .4506 and
.4603, for the average and majority human scores,
respectively).

All methods, including inter-annotator correla-
tion, improve in raw numbers over the subsets
of the data based on overlap location, indicating
that the data was partitioned into more internally-

consistent subsets. Encouragingly, modiﬁed R-
precision equalled or bettered the performance of
the original R-precision over each subset of the
data based on overlap location. Where modiﬁed
R-precision appears to fall down most noticeably
is over keyphrases including prepositions, as our
assumption about the semantic import based on
linear ordering clearly breaks down in the face of
post-modifying PPs. It is also telling that it does
worse over noun–noun sequences than adjective–
noun sequences. In being agnostic to the effects
of syntax, the original R-precision appears to bene-
ﬁt overall. Another interesting effect is that the
performance of BLEU, METEOR and ROUGE is
notably better over candidates which match with
non-initial and non-ﬁnal words in the keyphrase.

We conclude from this analysis that keyphrase
scoring should be sensitive to overlap location.
Furthermore, our study also shows that n-gram-
based MT and summarization metrics are sur-
prisingly adept at capturing partial matches in
keyphrases, despite them being much shorter than
the strings they are standardly applied to. More
compellingly, we found that R-precision is the best
overall performer, and that it matches the perfor-
mance of our human annotators across the board.
This is the ﬁrst research to establish this fact. Our
ﬁndings for modiﬁed R-precision were more sober-
ing, but its location sensitivity was shown to im-
prove over R-precision for instances of overlap in
the middle or with the head of the keyphrase.

579

6 Conclusion
In this work, we have shown that preexisting n-
gram-based evaluation metrics from MT, summa-
rization and keyphrase extraction evaluation are
able to handle the effects of near-misses, and that
R-precision performs at or above the average level
of a human annotator. We have also shown that
a semantic similarity-based method which uses
web data to model distributional similarity per-
formed below the level of all of the n-gram-based
methods, despite them requiring no external re-
sources (web or otherwise). We proposed a mod-
iﬁcation to R-precision based on the location of
match, but found that while it could achieve better
performance over certain classes of keyphrases,
its net effect was to drag the performance of R-
precision down. Other methods were found to be
remarkably consistent across different subtypes of
keyphrase.

Acknowledgements
Many thanks to the anonymous reviewers for their
insightful comments. We wish to acknowledge
the generous funding from National Research
Foundation grant R 252-000-279-325 in support-
ing Min-Yen Kan’s work.

References
Abhaya Agrwal and Alon Lavie. METEOR, M-
BLEU and M-TER: Evaluation Metrics for High-
Correlation with Human Rankings of Machine
Translation Output. In Proceedings of ACL Work-
shop on Statistical Machine Translation. 2008.

Ken Barker and Nadia Corrnacchia. Using noun
phrase heads to extract document keyphrases.
In
Proceedings of BCCSCSI : Advances in Artiﬁcial
Intelligence. 2000, pp.96–103.

Regina Barzilay and Michael Elhadad. Using lexi-
cal chains for text summarization. In Proceedings
of ACL/EACL Workshop on Intelligent Scalable Text
Summarization. 1997, pp. 10–17.

Chris Callison-Burch, Philipp Koehn, Christof Monz
and Josh Schroeder. Proceedings of 4th Workshop
on Statistical Machine Translation. 2009.

Eibe Frank, Gordon W. Paynter, Ian H. Witten, Carl
Gutwin and Craig G. Nevill-Manning. Domain
Speciﬁc Keyphrase Extraction.
In Proceedings of
IJCAI. 1999, pp.668–673.

Mario Jarmasz and Caroline Barriere. Using semantic
similarity over Tera-byte corpus, compute the per-
formance of keyphrase extraction. In Proceedings
of CLINE. 2004.

Su Nam Kim, Olena Medelyan, Min-Yen Kan and
Timothy Baldwin. SemEval-2010 Task 5: Auto-
matic Keyphrase Extraction from Scientiﬁc Arti-
cles. In Proceedings of SemEval-2: Evaluation Ex-
ercises on Semantic Evaluation. to appear.

Dawn Lawrie, W. Bruce Croft and Arnold Rosenberg.
Finding Topic Words for Hierarchical Summariza-
tion. In Proceedings of SIGIR. 2001, pp. 349–357.

Chin-Yew Lin and Edward H. Hovy. Automatic Eval-
uation of Summaries Using N-gram Co-occurrence
Statistics. In In Proceedings of HLT-NAACL. 2003.

Alvin Martin and Mark Przybocki. The 1999 NIST
Speaker Recognition Evaluation, Using Summed
Two-Channel Telephone Data for Speaker Detec-
tion and Speaker Tracking. In Proceedings of Eu-
roSpeech. 1999.

Yutaka Matsuo and Mitsuru Ishizuka. Keyword Ex-
traction from a Single Document using Word Co-
International
occurrence Statistical Information.
Journal on Artiﬁcial Intelligence Tools.
2004,
13(1), pp. 157–169.

Olena Medelyan and Ian Witten. Thesaurus based
In Proceedings of

automatic keyphrase indexing.
ACM/IEED-CS JCDL. 2006, pp. 296–297.

Rada Mihalcea and Paul Tarau. TextRank: Bringing
Order into Texts. In Proceedings of EMNLP 2004.
2004, pp. 404–411.

Guido Minnen, John Carroll and Darren Pearce. Ap-
plied morphological processing of English. NLE.
2001, 7(3), pp. 207–223.

Thuy Dung Nguyen and Min-Yen Kan. Key phrase
Extraction in Scientiﬁc Publications. In Proceeding
of ICADL. 2007, pp. 317–326.

Sebastian Pad´o, Michel Galley, Dan Jurafsky and
Christopher D. Manning. Textual Entailment Fea-
tures for Machine Translation Evaluation. In Pro-
ceedings of ACL Workshop on Statistical Machine
Translation. 2009, pp. 37–41.

Ernesto D’Avanzo and Bernado Magnini. A Key-
the
In Proceedings of

phrase-Based Approach to Summarization:
LAKE System at DUC-2005.
DUC. 2005.

Kishore Papineni, Salim Roukos, Todd Ward and Wei-
Jing Zhu. BLEU: a method for automatic evalua-
tion of machine translation. In Proceedings of ACL.
2001, pp. 311–318.

580

Youngja Park, Roy J. Byrd and Branimir Boguraev.
Automatic Glossary Extraction Beyond Terminol-
ogy Identiﬁcation.
In Proceedings of COLING.
2004, pp. 48–55.

Mari-Sanna Paukkeri, Ilari T. Nieminen, Matti Polla
and Timo Honkela. A Language-Independent Ap-
proach to Keyphrase Extraction and Evaluation. In
Proceedings of COLING. 2008, pp. 83–86.

Horacio Saggion, Dragomir Radev, Simon Teufel,
Wai Lam and Stephanie Strassel. Meta-evaluation
of Summaries in a Cross-lingual Environment us-
ing Content-based Metrics. In Proceedings of COL-
ING. 2002, pp. 1–7.

Peter Turney. Coherent keyphrase extraction via Web
mining. In Proceedings of IJCAI. 2003, pp. 434–
439.

Xiaojun Wan and Jianguo Xiao. CollabRank:
to-
wards a collaborative approach to single-document
keyphrase extraction. In Proceedings of COLING.
2008, pp. 969–976.

Ian Witten, Gordon Paynter, Eibe Frank, Car Gutwin
and Craig Nevill-Manning. KEA:Practical Auto-
matic Key phrase Extraction.
In Proceedings of
ACM conference on Digital libraries. 1999, pp.
254–256.

Torsten Zesch and Iryna Gurevych. Approximate
Matching for Evaluating Keyphrase Extraction. In-
ternational Conference on Recent Advances in Nat-
ural Language Processing. 2009.

