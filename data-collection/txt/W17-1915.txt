



















































Supervised and Unsupervised Word Sense Disambiguation on Word Embedding Vectors of Unambigous Synonyms


Proceedings of the 1st Workshop on Sense, Concept and Entity Representations and their Applications, pages 120–125,
Valencia, Spain, April 4 2017. c©2017 Association for Computational Linguistics

Supervised and Unsupervised Word Sense Disambiguation on Word
Embedding Vectors of Unambiguous Synonyms

Aleksander Wawer
Institute of Computer Science

PAS
Jana Kazimierza 5

01-248 Warsaw, Poland
axw@ipipan.waw.pl

Agnieszka Mykowiecka
Institute of Computer Science

PAS
Jana Kazimierza 5

01-248 Warsaw, Poland
agn@ipipan.waw.pl

Abstract

This paper compares two approaches to
word sense disambiguation using word
embeddings trained on unambiguous syn-
onyms. The first one is an unsupervised
method based on computing log proba-
bility from sequences of word embedding
vectors, taking into account ambiguous
word senses and guessing correct sense
from context. The second method is super-
vised. We use a multilayer neural network
model to learn a context-sensitive transfor-
mation that maps an input vector of am-
biguous word into an output vector repre-
senting its sense. We evaluate both meth-
ods on corpora with manual annotations of
word senses from the Polish wordnet.

1 Introduction

Ambiguity is one of the fundamental features of
natural language, so every attempt to understand
NL utterances has to include a disambiguation
step. People usually do not even notice ambi-
guity because of the clarifying role of the con-
text. A word market is ambiguous, and it is
still such in the phrase the fish market while in
a longer phrase like the global fish market it is
unequivocal because of the word global, which
cannot be used to describe physical place. Thus,
distributional semantics methods seem to be a
natural way to solve the word sense discrimina-
tion/disambiguation task (WSD). One of the first
approaches to WSD was context-group sense dis-
crimination (Schütze, 1998) in which sense rep-
resentations were computed as groups of simi-
lar contexts. Since then, distributional semantic
methods were utilized in very many ways in su-
pervised, weekly supervised and unsupervised ap-
proaches.

Unsupervised WSD algorithms aim at resolv-
ing word ambiguity without the use of annotated
corpora. There are two popular categories of
knowledge-based algorithms. The first one orig-
inates from the Lesk (1986) algorithm, and ex-
ploit the number of common words in two sense
definitions (glosses) to select the proper meaning
in a context. Lesk algorithm relies on the set of
dictionary entries and the information about the
context in which the word occurs. In (Basile et
al., 2014) the concept of overlap is replaced by
similarity represented by a DSM model. The au-
thors compute the overlap between the gloss of
the meaning and the context as a similarity mea-
sure between their corresponding vector represen-
tations in a semantic space. A semantic space is
a co-occurrences matrix M build by analysing the
distribution of words in a large corpus, later re-
duced using Latent Semantic Analysis (Landauer
and Dumais, 1997). The second group of algo-
rithms comprises graph-based methods which use
structure of semantic nets in which different types
of word sense relations are represented and linked
(e.g. WordNet, BabelNet). They used various
graph-induced information, e.g. Page Rank algo-
rithm (Mihalcea et al., 2004).

In this paper we present a method of word sense
disambiguation, i.e. inferring an appropriate word
sense from those listed in Polish wordnet, using
word embeddings in both supervised and unsuper-
vised approaches. The main tested idea is to cal-
culate sense embeddings using unambiguous syn-
onyms (elements of the same synsets) for a par-
ticular word sense. In section 2 we shortly present
existing results for WSD for Polish as well as other
works related to word embeddings for other lan-
guages, while section 3 presents annotated data
we use for evaluation and supervised model train-
ing. Next sections describe the chosen method of
calculating word sense embeddings, our unsuper-

120



vised and supervised WSD experiments and some
comments on the results.

2 Existing Work

2.1 Polish WSD

There was very little research done in WSD for
Polish. The first one from the few more vis-
ible attempts comprise a small supervised ex-
periment with WSD in which machine learn-
ing techniques and a set of a priori defined fea-
tures were used, (Kobyliński, 2012). Next, in
(Kobyliński and Kopeć, 2012), extended Lesk
knowledge-based approach and corpus-based sim-
ilarity functions were used to improve previous
results. These experiments were conducted on
the corpora annotated with the specially designed
set of senses. The first one contained general
texts with 106 polysemous words manually anno-
tated with 2.85 sense definitions per word on av-
erage. The second, smaller, WikiEcono corpus
(http://zil.ipipan.waw.pl/plWikiEcono) was anno-
tated by another set of senses for 52 polysemous
words. It contains 3.62 sense definitions per word
on average. The most recent work on WSD for
Polish (Kędzia et al., 2015) utilizes graph-based
approaches of (Mihalcea et al., 2004) and (Agirre
et al., 2014). This method uses both plWordnet
and SUMO ontology and was tested on KPWr data
set (Broda et al., 2012) annotated with plWord-
net senses — the same data set which we use in
our experiments. The highest precision of 0.58
was achieved for nouns. The results obtained by
different WSD approaches are very hard to com-
pare because of different set of senses and test
data used and big differences in results obtained
by the same system on different data. (Tripodi
and Pelillo, 2017) reports the results obtained by
the best systems for English at the level of 0.51-
0.85% depending on the approach (supervised or
unsupervised) and the data set. The only system
for Polish to which to some extend we can com-
pare our approach is (Kędzia et al., 2015).

2.2 WSD and Word Embeddings

The problem of WSD has been approached from
various perspectives in the context of word embed-
dings.

Popular approach is to generate multiple em-
beddings per word type, often using unsupervised
automatic methods. For example, (Reisinger and
Mooney, 2010; Huang et al., 2012) cluster con-

texts of each word to learn senses for each word,
then re-label them with clustered sense for learn-
ing embeddings. (Neelakantan et al., 2014) intro-
duce flexible number of senses: they extend sense
cluster list when a new sense is encountered by a
model.

(Iacobacci et al., 2015) use an existing WSD
algorithm to automatically generate large sense-
annotated corpora to train sense-level embeddings.
(Taghipou and Ng, 2015) prepare POS-specific
embeddings by applying a neural network with
trainable embedding layer. They use those embed-
dings to extend feature space of a supervised WSD
tool named IMS.

In (Bhingardive et al., 2015), the authors pro-
pose to exploit word embeddings in an unsuper-
vised method for most frequent sense detection
from the untagged corpora. Like in our work, the
paper explores creation of sense embeddings with
the use of WordNet. As the authors put it, sense
embeddings are obtained by taking the average of
word embeddings of each word in the sense-bag.
The sense-bag for each sense of a word is obtained
by extracting the context words from the WordNet
such as synset members (S), content words in the
gloss (G), content words in the example sentence
(E), synset members of the hypernymy-hyponymy
synsets (HS), and so on.

3 Word-Sense Annotated Treebank

The main obstacle in elaborating WSD method for
Polish is lack of semantically annotated resources
which can be applied for training and evaluation.
In our experiment we used an existing one which
use wordnet senses – semantic annotation (Ha-
jnicz, 2014) of Składnica (Woliński et al., 2011).
The set is a rather small but carefully prepared re-
source and contains constituency parse trees for
Polish sentences. The adapted version of Skład-
nica (0.5) contains 8241 manually validated trees.
Sentence tokens are annotated with fine-grained
semantic types represented by Polish wordnet
synsets from plWordnet 2.0 plWordnet, Piasecki et
al., 2009, http://plwordnet.pwr.wroc.pl/wordnet/).
The set contains lexical units of three open parts
of speech: adjectives, nouns and verbs. Therefore,
only tokens belonging to these POS are annotated
(as well as abbreviations and acronyms). Skład-
nica contains about 50K nouns, verbs and adjec-
tives for annotation, and 17410 of them belong-
ing to 2785 (34%) sentences has been already an-

121



notated. For 2072 tokens (12%), the lexical unit
appropriate in the context has not been found in
plWordnet.

4 Obtaining Sense Embeddings

In this section we describe the method of obtaining
sense-level word embeddings. Unlike most of the
approaches described in Section 2.2, our method
is applied to manually sense-labeled corpora.

In Wordnet, words either occur in multiple
synsets (are therefore ambiguous and subject of
WSD), or in one synset (are unambiguous). Our
approach is to focus on synsets that contain both
ambiguous and unambiguous words. In Skad-
nica 2.0 (Polish WordNet) we found 28766 synsets
matching these criteria and therefore potentially
suitable for our experiments.

Let us consider a synset containing following
words: ‘blemish’, ‘deface’, ‘disfigure’. Word
‘blemish’ appears also in other synsets (is ambigu-
ous) while words ‘deface’ and ‘disfigure’ are spe-
cific for this synset and do not appear in any other
synset (are unambiguous).

We assume that embeddings specific to a sense
or synset can be approximated by unambiguous
part of the synset. While some researchers such
as (Bhingardive et al., 2015) take average em-
beddings of all synset-specific words, even us-
ing glosses and hyperonymy, we use unambiguous
words to generate word2vec embedding vector of
a sense.

During training, each occurrence of unambigu-
ous word in corpus is substituted for a synset
identifier. As in the provided example, each oc-
currence of ‘deface’ and ‘disfigure’ would be re-
placed by its sense identifier, the same for both
unambiguous words. We’ll later use these sense
vectors to distinguish between senses of ambigu-
ous ‘blemish’ given their contexts.

We train word2vec vectors using substitution
mechanism described above on a dump of all Pol-
ish language Wikipedia and 300-million subset of
the National Corpus of Polish (Przepiórkowski et
al., 2012). The embedding size is set to 100, all
other word2vec parameters have the default value
as in (Řehůřek and Sojka, 2010). The model is
based on lemmatized (base word forms) so only
the occurrences of forms with identical lemmas
are taken into account.

5 Unsupervised Word Sense Recognition

In this section we are proposing a simple unsuper-
vised approach to WSD. The key idea is to use
word embeddings in probabilistic interpretation
and application comparable to language model-
ing, however without building any additional mod-
els or parameter-rich systems. The method is de-
rived from (Taddy, 2015), where it was used with
a bayesian classifier and vector embedding inver-
sion to classify documents.

(Mikolov et al., 2013) describe two alterna-
tive methods of generating word embeddings: the
skip-gram, which represents conditional probabil-
ity for a word’s context (surrounding words) and
CBOW, which targets the conditional probability
for each word given its context. None of these
corresponds to a likelihood model, but as (Taddy,
2015) note they can be interpreted as components
in a composite likelihood approximation. Let w
= [w1. . . wT ] denote an ordered vector of words.
The skip-gram in (Mikolov et al., 2013) yields the
pairwise composite log likelihood:

logpV(w) =
T∑

j=1

T∑
i=1

1[1≤|k−j|≤b]logpV(wk|wj)

(1)
We use the above formula to compute probabil-

ity of a sentence. Unambiguous words are repre-
sented as their word2vec representations derived
directly from corpus. In case of ambiguous words,
we substitute them for each possible sense vector
(generated from unambiguous parts of synsets, as
has been previously described). Therefore, for an
ambiguous word to be disambiguated, we gener-
ate as many variants of a sentence as there are its
senses, and compute each variant’s likelihood us-
ing formula 1. Ambiguous words which occur in
the context are omitted (although we might also
replace them with an averaged vector representing
all their meanings). Finally, we select the most
probable variant.

Because the method involves no model train-
ing, we evaluate it directly over the whole data
set without dividing it into train and test sets for
cross-validation.

6 Supervised Word Sense Recognition

In the supervised approach, we train neural net-
work models to predict word senses. In our exper-
iment, neural network model acts as a regression

122



function F transforming word embeddings pro-
vided at input into sense (synset identifiers) vec-
tors.

As the network architecture we selected LSTM
(Hochreiter and Schmidhuber, 1997). Neural net-
work model consists of one LSTM layer followed
by a dense (perceptron) layer at the output. We
train the network using mean standard error loss
function.

Input data consists of the sequences of five
word2vec embeddings: of two words that make
left and right symmetric contexts of each input
word to be disambiguated, and the word itself rep-
resented by the average vector of vectors repre-
senting all its senses. Ambiguous words for which
there are no embeddings are represented by zero
vectors (padded). Zero vectors are also added if
the context is too short. This data is used to train
LSTM model (Keras 1.0.1 https://keras.
io/) linked with the subsequent dense layer with
sigmoid activation function.

At the final step, we transform the output into
synsets rather than vectors. We select the most ap-
propriate sense from a set of possible sense inven-
tory, taking into account continuous output struc-
ture. In this step, neural network output layer
(which is a vector of the same size as input em-
beddings, but transformed) is compared with each
possible sense vector. To compare vectors, we use
cosine similarity measure, defined between any
two vectors.

We compute cosine similarity between neural
network output vector nnv and each sense from
possible sense inventory S, and select the sense
with the maximum cosine similarity towards nnv.

To test each neural network set-up we use 30-
fold cross-validation.

7 Results

In this section we put summary of the results
obtained on our test set, as well as two base-
line results. The corpus consisted of 2785 sen-
tences and 303 occurences of annotated ambigu-
ous words which could be disambiguated by our
algorithms, i.e. there were unambiguous equiva-
lents of its senses and there were appropiate word
embeddings for at least one of the other senses of
this word. There were 5571 occurences of words
which occurred only in one sense.

Table 1 presents precision of both tested meth-
ods computed over the Skladnica dataset. The

set contains 344 occurrences of ambiguous words
which were eligible for our method. For the unsu-
pervised approach we tested a window of 5 and 10
words around the analyzed word.

The ambiguous words from the sentence other
than the one being disambiguated at the moment
are either omitted or represented as a vector rep-
resenting all their occurrences. The uniq variant
omit all other ambiguous words from the sentence
while in the all variant we use not disambiguated
representation of these words.

Method Settings Precision

random baseline N/A 0.47

MFS baseline N/A 0.73

pagerank N/A 0.52

unsupervised

5 word, all 0.507
5 word, uniq 0.507
10 word, uniq 0.529
10 word, all 0.513

supervised

750 epochs 0.673
1000 epochs 0.680
2000 epochs 0.690
4000 epochs 0.667

Table 1: Precision of word-sense disambiguation
methods for Polish.

In the supervised approach the best results were
obtained for 2000 epochs but they did not differ
much from these obtained after 1000 epochs.
For comparison, we include two baseline values:

• random baseline select random sense from
uniform random probability distribution,

• MFS baseline use most frequent sense as
computed from the same corpus (There is no
other available sense frequency data for Pol-
ish, that could be obtained from manually an-
notated sources.)

The table also includes results computed using
pagerank WSD algorithm developed at the PWR
(Kędzia et al., 2015). These results were obtained
for all the ambiguous words occurring within the
sample, so cannot be directly compared to our re-
sults.

As the results indicate, unsupervised method
performs at the level of random sense selection.

123



Below there are two examples of the analyzed sen-
tences.

• lęk przed nicością łączy się z doświadczeniem
pustki ‘fear of nothingness combines with the
experience of emptiness’: in this sentence,
Polish ambiguous words ‘nothingness’ and
‘emptiness’ were resolved correctly while an
ambiguous words ‘experience’ does not have
unambiguous equivalents.

• na tym nie kończą się problemy ‘that does not
stop problems’: in this example ambiguous
word ‘problem’ was not resolved correctly,
but this case is difficult also for humans.

The low quality of the results might be the ef-
fect of a relatively short context available as the
analysed text is not continuous.

It might have also pointed out to the difficulty
of the test set. Senses in plWodnet are very numer-
ous and hard to differentiate even for human. But
the results of the supervised method falsify this as-
sumption.

Our supervised approach gave much better re-
sults although they are also not very good as the
amount of annotated data is rather small. In this
approach more epochs resulted in a slight model
over-fitting.

8 Conclusions

Our work introduced two methods of word sense
disambiguation based on word embeddings, su-
pervised and unsupervised. The first approach as-
sumes probabilistic interpretation of embeddings
and computes log probability from sequences of
word embedding vectors. In place of ambiguous
word we put embeddings specific for each possible
sense and evaluate the likelihood of thus obtained
sentences. Finally we select the most probable
sentence. The second supervised method is based
on a neural network trained to learn a context-
sensitive transformation that maps an input vector
of ambiguous word into an output vector repre-
senting its sense. We compared the performance
of both methods on corpora with manual anno-
tations of word senses from the Polish wordnet
(plWordnet). The results show the low quality of
the unsupervised method and suggest the superior-
ity of the supervised version in comparison to the
pagerank method on the set of words which were
eligible for our approach. Although the baseline

in which just the most frequent sense is chosen is
still a little better, this is probably due to a very
limited training set available for Polish.

Acknowledgments

The paper is partially supported by the Polish Na-
tional Science Centre project Compositional dis-
tributional semantic models for identification, dis-
crimination and disambiguation of senses in Pol-
ish texts number 2014/15/B/ST6/05186.

References
Eneko Agirre, Oier López de Lacalle, and Aitor Soroa.

2014. Random walks for knowledge-based word
sense disambiguation. Comput. Linguist., 40(1):57–
84, March.

Pierpaolo Basile, Annalina Caputo, and Giovanni Se-
meraro. 2014. An enhanced lesk word sense dis-
ambiguation algorithm through a distributional se-
mantic model. In Proceedings of COLING 2014,
the 25th International Conference on Computational
Linguistics, Dublin, Irleand. Association for Com-
putational Linguistics.

Sudha Bhingardive, Dhirendra Singh, Rudra Murthy,
Hanumant Redkar, and Pushpak Bhattacharyya.
2015. Unsupervised most frequent sense detection
using word embeddings. In DENVER.

Bartosz Broda, Michał Marcinczuk, Marek Maziarz,
Adam Radziszewski, and Adam Wardynski. 2012.
KPWr: Towards a free corpus of polish. Proceed-
ings of LREC’12.

Elżbieta Hajnicz. 2014. Lexico-semantic annotation of
składnica treebank by means of PLWN lexical units.
In Heili Orav, Christiane Fellbaum, and Piek Vossen,
editors, Proceedings of the 7th International Word-
Net Conference (GWC 2014), pages 23–31, Tartu,
Estonia. University of Tartu.

Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long
short-term memory. Neural Comput., 9(8):1735–
1780, November.

Eric H. Huang, Richard Socher, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Improving word
representations via global context and multiple word
prototypes. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics: Long Papers - Volume 1, ACL ’12, pages 873–
882, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.

Ignacio Iacobacci, Mohammad Taher Pilehvar, and
Roberto Navigli. 2015. Sensembed: Learning
sense embeddings for word and relational similarity.
In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the

124



7th International Joint Conference on Natural Lan-
guage Processing of the Asian Federation of Natural
Language Processing, ACL 2015, July 26-31, 2015,
Beijing, China, Volume 1: Long Papers, pages 95–
105.

Łukasz Kobyliński and Mateusz Kopeć. 2012. Se-
mantic similarity functions in word sense dis-
ambiguation. In Petrand Horák Sojka, Alešand
Kopeček, and Karel Ivanand Pala, editors, Text,
Speech and Dialogue: 15th International Confer-
ence, TSD 2012, Brno, Czech Republic, September
3-7, 2012. Proceedings, pages 31–38, Berlin, Hei-
delberg. Springer.

Łukasz Kobyliński. 2012. Mining class association
rules for word sense disambiguation. In Pascal Bou-
vry, Mieczysław A. Kłopotek, Franck Leprevost,
Małgorzata Marciniak, Agnieszka Mykowiecka, and
Henryk Rybiński, editors, Security and Intelligent
Information Systems: International Joint Confer-
ence, SIIS 2011, Warsaw, Poland, June 13-14, 2011,
Revised Selected Papers, volume 7053 of Lecture
Notes in Computer Science, pages 307–317, Berlin,
Heidelberg. Springer.

Paweł Kędzia, Maciej Piasecki, and Marlena Or-
lińska. 2015. Word sense disambiguation based
on large scale Polish CLARIN heterogeneous lexi-
cal resources. Cognitive Studies| Études cognitives,
15:269–292.

Rada Mihalcea, Paul Tarau, and Elizabeth Figa. 2004.
Pagerank on semantic networks, with application
to word sense disambiguation. In Proceedings of
the 20th International Conference on Computational
Linguistics, COLING ’04, Stroudsburg, PA, USA.
Association for Computational Linguistics.

Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013. Linguistic regularities in continuous space
word representations. In Human Language Tech-
nologies: Conference of the North American Chap-
ter of the Association of Computational Linguis-
tics, Proceedings, June 9-14, 2013, Westin Peachtree
Plaza Hotel, Atlanta, Georgia, USA, pages 746–751.

Arvind Neelakantan, Jeevan Shankar, Alexandre Pas-
sos, and Andrew McCallum. 2014. Efficient non-
parametric estimation of multiple embeddings per
word in vector space. In Proceedings of the 2014
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP 2014, October 25-29,
2014, Doha, Qatar, A meeting of SIGDAT, a Special
Interest Group of the ACL, pages 1059–1069.

Adam Przepiórkowski, Mirosław Bańko, Rafał L.
Górski, and Barbara Lewandowska-Tomaszczyk,
editors. 2012. Narodowy Korpus Języka Polskiego.
Wydawnictwo Naukowe PWN, Warsaw.

Radim Řehůřek and Petr Sojka. 2010. Software
Framework for Topic Modelling with Large Cor-
pora. In Proceedings of the LREC 2010 Workshop

on New Challenges for NLP Frameworks, pages 45–
50, Valletta, Malta, May. ELRA. http://is.
muni.cz/publication/884893/en.

Joseph Reisinger and Raymond J Mooney. 2010.
Multi-prototype vector-space models of word mean-
ing. In Human Language Technologies: The 2010
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 109–117. Association for Computational Lin-
guistics.

Hinrich Schütze. 1998. Automatic word sense
discrimination. Computational Linguistics, 24
(1):97–123.

Matt Taddy. 2015. Document classification by inver-
sion of distributed language representations. CoRR,
abs/1504.07295.

Kaveh Taghipou and Hwee Tou Ng. 2015. Semi-
supervised word sense disambiguation using word
embeddings in general and specific domains. In
Human Language Technologies: The 2015 Annual
Conference of the North American Chapter of the
ACL, page 314–323. Association for Computational
Linguistics.

Rocco Tripodi and Marcello Pelillo. 2017. A game-
theoretic approach to word sense disambiguation.
Computational Linguistics.

Marcin Woliński, Katarzyna Głowińska, and Marek
Świdziński. 2011. A preliminary version of Skład-
nica—a treebank of Polish. In Zygmunt Vetulani,
editor, Proceedings of the 5th Language & Technol-
ogy Conference: Human Language Technologies as
a Challenge for Computer Science and Linguistics,
pages 299–303, Poznań, Poland.

125


