




















































Long and Diverse Text Generation with Planning-based Hierarchical Variational Model


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 3257–3268,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

3257

Long and Diverse Text Generation with Planning-based Hierarchical
Variational Model

Zhihong Shao1, Minlie Huang1∗, Jiangtao Wen1, Wenfei Xu2, Xiaoyan Zhu1

1 Institute for Artificial Intelligence, State Key Lab of Intelligent Technology and Systems
1 Beijing National Research Center for Information Science and Technology

1 Department of Computer Science and Technology, Tsinghua University, Beijing 100084, China
2 Baozun, Shanghai, China

szh19@mails.tsinghua.edu.cn, aihuang@tsinghua.edu.cn

jtwen@tsinghua.edu.cn, xuwenfeilittle@gmail.com

zxy-dcs@tsinghua.edu.cn

Abstract

Existing neural methods for data-to-text gen-

eration are still struggling to produce long and

diverse texts: they are insufficient to model

input data dynamically during generation, to

capture inter-sentence coherence, or to gener-

ate diversified expressions. To address these

issues, we propose a Planning-based Hierar-

chical Variational Model (PHVM). Our model

first plans a sequence of groups (each group

is a subset of input items to be covered by a

sentence) and then realizes each sentence con-

ditioned on the planning result and the pre-

viously generated context, thereby decompos-

ing long text generation into dependent sen-

tence generation sub-tasks. To capture expres-

sion diversity, we devise a hierarchical latent

structure where a global planning latent vari-

able models the diversity of reasonable plan-

ning and a sequence of local latent variables

controls sentence realization. Experiments

show that our model outperforms state-of-the-

art baselines in long and diverse text genera-

tion.

1 Introduction

Data-to-text generation is to generate natural lan-

guage texts from structured data (Gatt and Krah-

mer, 2018), which has a wide range of applications

(for weather forecast, game report, product de-

scription, advertising document, etc.). Most neu-

ral methods focus on devising encoding scheme

and attention mechanism, namely, (1) exploiting

input structure to learn better representation of in-

put data (Lebret et al., 2016; Liu et al., 2018), and

(2) devising attention mechanisms to better em-

ploy input data (Mei et al., 2016; Liu et al., 2018;

Nema et al., 2018) or to dynamically trace which

part of input has been covered in generation (Kid-

don et al., 2016). These models are able to pro-

∗*Corresponding author: Minlie Huang.

duce fluent and coherent short texts in some appli-

cations.

However, to generate long and diverse texts

such as product descriptions, existing methods

are still unable to capture the complex semantic

structures and diversified surface forms of long

texts. First, existing methods are not good at

modeling input data dynamically during genera-

tion. Some neural methods (Kiddon et al., 2016;

Feng et al., 2018) propose to record the accumu-

lated attention devoted to each input item. How-

ever, these records may accumulate errors in rep-

resenting the state of already generated prefix, thus

leading to wrong new attention weights. Second,

inter-sentence coherence in long text generation

is not well captured (Wiseman et al., 2017) due

to the lack of high-level planning. Recent stud-

ies propose to model planning but still have much

space for improvement. For instance, in (Pudup-

pully et al., 2019) and (Sha et al., 2018), planning

is merely designed for ordering input items, which

is limited to aligning input data with the text to

be generated. Third, most methods fail to gener-

ate diversified expressions. Existing data-to-text

methods inject variations at the conditional out-

put distribution, which is proved to capture only

low-level variations of expressions (Serban et al.,

2017).

To address the above issues, we propose a novel

Planning-based Hierarchical Variational Model

(PHVM). To better model input data and alleviate

the inter-sentence incoherence problem, we design

a novel planning mechanism and adopt a compati-

ble hierarchical generation process, which mimics

the process of human writing. Generally speaking,

to write a long text, a human writer first arranges

contents and discourse structure (i.e., high-level

planning) and then realizes the surface form of

each individual part (low-level realization). Mo-

tivated by this, our proposed model first performs



3258

1 . <类型,裙>
<Category, Dress / Skirt>

2 . <版型,显瘦>
<Design, Figure Flattering>

3 . <材质,棉>
<Material, Cotton>

4 . <风格,文艺>
<Style, Aesthetic>

5 . <风格,青春>
<Style, Youthful>

6 . <风格,清新>
<Style, Fresh>

7 . <图案,格子>
<Pattern, Plaid>

8 . <裙下摆,荷叶边>
<Hem, Flounce>

9 . <裙腰型,高腰>
<Waist, High-rise>

10. <裙长,半身裙>
<Length, Skirt>

11. <裙款式,不规则>
<Element, irregular>

Input:

Group 1 3, 10

4, 6, 7

2, 9

5, 8, 11

这款半身裙，纯棉的面料，舒适透气；
This skirt is made of pure cotton，which is comfortable
and breathable;

融入清新文艺的格纹元素，展现出俏皮可爱的学院风；
The fresh and aesthetic plaid brings about lovely preppy

style;

修身的版型，高腰的设计，提高腰线，修饰完美身材；
The figure flattering design -- in particular, the high-rise

design -- heightens your waistline and perfects your body

shape;

不规则荷叶边裙摆的设计，灵动温婉中又多了几分洒
脱随性的格调，洋溢着青春的气息。
The irregular flounce is youthful, making the mild you

look a bit more free and easy.

Generation:

Group 2

Group 3

Group 4

Figure 1: Generation process of PHVM. After encod-

ing a list of input attribute-value pairs, PHVM first

conducts planning by generating a sequence of groups,

each of which is a subset of input items. Each sentence

is then realized conditioned on the corresponding group

and its previous generated sentences.

planning by segmenting input data into a sequence

of groups, and then generates a sentence condi-

tioned on the corresponding group and preceding

generated sentences. In this way, we decompose

long text generation into a sequence of dependent

sentence generation sub-tasks where each sub-task

depends specifically on an individual group and

the previous context. By this means, the input data

can be well modeled and inter-sentence coherence

can be captured. Figure 1 depicts the process.

To deal with expression diversity, this model

also enables us to inject variations at both high-

level planning and low-level realization with a hi-

erarchical latent structure. At high level, we in-

troduce a global planning latent variable to model

the diversity of reasonable planning. At low level,

we introduce local latent variables for sentence

realization. Since our model is based on Con-

ditional Variational Auto-Encoder (CVAE) (Sohn

et al., 2015), expression diversity can be captured

by the global and local latent variables.

We evaluate the model on a new advertising

text1 generation task which requires the system to

generate a long and diverse advertising text that

covers a given set of attribute-value pairs describ-

ing a product (see Figure 1). We also evaluate

1An advertising text describes a product with attractive
wording. The goal of writing such texts is to advertise a prod-
uct and attract users to buy it.

our model on the recipe text generation task from

(Kiddon et al., 2016) which requires the system

to correctly use the given ingredients and main-

tain coherence among cooking steps. Experiments

on advertising text generation show that our model

outperforms state-of-the-art baselines in automatic

and manual evaluation. Our model also general-

izes well to long recipe text generation and out-

performs the baselines. Our contributions are two-

fold:

• We design a novel Planning-based Hierar-
chical Variational Model (PHVM) which in-

tegrates planning into a hierarchical latent

structure. Experiments show its effectiveness

in coverage, coherence, and diversity.

• We propose a novel planning mechanism
which segments the input data into a se-

quence of groups, thereby decomposing long

text generation into dependent sentence gen-

eration sub-tasks. Thus, input data can be

better modeled and inter-sentence coherence

can be better captured. To capture expres-

sion diversity, we devise a hierarchical la-

tent structure which injects variations at both

high-level planning and low-level realization.

2 Related Work

Traditional methods (Reiter and Dale, 1997; Stent

et al., 2004) for data-to-text generation consist

of three components: content planning, sentence

planning, and surface realization. Content plan-

ning and sentence planning are responsible for

what to say and how to say respectively; they

are typically based on hand-crafted (Kukich,

1983; Dalianis and Hovy, 1993; Hovy, 1993) or

automatically-learnt rules (Duboue and McKe-

own, 2003). Surface realization generates natu-

ral language by carrying out the plan, which is

template-based (McRoy et al., 2003; van Deemter

et al., 2005) or grammar-based (Bateman, 1997;

Espinosa et al., 2008). As these models are shal-

low and the two stages (planning and realization)

often function separately, traditional methods are

unable to capture rich variations of texts.

Recently, neural methods have become the

mainstream models for data-to-text generation due

to their strong ability of representation learning

and scalability. These methods perform well in

generating weather forecasts (Mei et al., 2016) or

very short biographies (Lebret et al., 2016; Liu



3259

d1 d2 dN

…
~zp <SG> bow(g1) bow(gT-1)

g1 g2 gT

…

~   …

Input Encoder

Plan Decoder

Plan Encoder

Sentence Decoder

Word Decoder

h1 h2 hN

s1 s2 sT

         

0.7 0.2 0.8
…

0.2

0.3 0.8 0.2 … 0.8

0.5

d1 d2 dNd3 …

d1 d3

gt

group

…

bow(gT-1)

gt

st

   …

…

…d3

h3

Probability of discarding

Probability of selecting

   
~   ~   ~   

Concatenation

Sampling~

Figure 2: Architecture of PHVM. The model controls planning with a global latent variable zp. The plan decoder

conducts planning by generating a sequence of groups g = g1g2...gT where gt is a subset of input items and
specifies the content of sentence st to be generated. The sentence decoder controls the realization of st with a local

latent variable zst ; dependencies among z
s
t are explicitly modeled to better capture inter-sentence coherence.

et al., 2018; Sha et al., 2018; Nema et al., 2018)

using well-designed data encoder and attention

mechanisms. However, as demonstrated in Wise-

man et al. (2017) (a game report generation task),

existing neural methods are still problematic for

long text generation: they often generate incoher-

ent texts. In fact, these methods also lack the abil-

ity to model diversity of expressions.

As for long text generation, recent studies tackle

the incoherence problem from different perspec-

tives. To keep the decoder aware of the crucial

information in the already generated prefix, Shao

et al. (2017) appended the generated prefix to the

encoder, and Guo et al. (2018) leaked the extracted

features of the generated prefix from the discrim-

inator to the generator in a Generative Adversar-

ial Nets (Goodfellow et al., 2014). To model

dependencies among sentences, Li et al. (2015)

utilized a hierarchical recurrent neural network

(RNN) decoder. Konstas and Lapata (2013) pro-

posed to plan content organization with grammar

rules while Puduppully et al. (2019) planned by

reordering input data. Most recently, Moryossef

et al. (2019) proposed to select plans from all pos-

sible ones, which is infeasible for large inputs.

As for diverse text generation, existing meth-

ods can be divided into three categories: enrich-

ing conditions (Xing et al., 2017), post-processing

with beam search and rerank (Li et al., 2016), and

designing effective models (Xu et al., 2018). Some

text-to-text generation models (Serban et al., 2017;

Zhao et al., 2017) inject high-level variations with

latent variables. Variational Hierarchical Conver-

sation RNN (VHCR) (Park et al., 2018) is a most

similar model to ours, which also adopts a hierar-

chical latent structure. Our method differs from

VHCR in two aspects: (1) VHCR has no plan-

ning mechanism, and the global latent variable is

mainly designed to address the KL collapse prob-

lem, while our global latent variable captures the

diversity of reasonable planning; (2) VHCR in-

jects distinct local latent variables without direct

dependencies, while our method explicitly mod-

els the dependencies among local latent variables

to better capture inter-sentence connections. Shen

et al. (2019) proposed ml-VAE-D with multi-level

latent variables. However, the latent structure of

ml-VAE-D consists of two global latent variables:

the top-level latent variable is introduced to learn a

more flexible prior of the bottom-level latent vari-

able which is then used to decode a whole para-

graph. By contrast, our hierarchical latent struc-

ture is tailored to our planning mechanism: the top

level latent variable controls planning results and a

sequence of local latent variables is introduced to

obtain fine-grained control of sentence generation

sub-tasks.

We evaluated our model on a new advertising

text generation task which is to generate a long

and diverse text that covers all given specifica-

tions about a product. Different from our task,

the advertising text generation task in (Chen et al.,

2019) is to generate personalized product descrip-

tion based on product title, product aspect (e.g.,

“appearance”), and user category.

3 Task Definition

Given input data x = {d1, d2, ..., dN} where each
di can be an attribute-value pair or a keyword,

our task is to generate a long and diverse text

y = s1s2...sT (st is the t
th sentence) that cov-

ers x as much as possible. For the advertising



3260

text generation task, x consists of specifications

about a product where each di is an attribute-value

pair < ai, vi >. For the recipe text generation

task, x is an ingredient list where each di is an in-

gredient. Since the recipe title r is also used for

generation, we abuse the symbol x to represent

< {d1, d2, ..., dN}, r > for simplification.

4 Approach

4.1 Overview

Figure 2 shows the architecture of PHVM. PHVM

first samples a global planning latent variable zp

based on the encoded input data; zp serves as a

condition variable in both planning and hierarchi-

cal generation process. The plan decoder takes

zp as initial input. At time step t, it decodes

a group gt which is a subset of input items (di)

and specifies the content of the tth sentence st.

When the plan decoder finishes planning, the hier-

archical generation process starts, which involves

the high-level sentence decoder and the low-level

word decoder. The sentence decoder models inter-

sentence coherence in semantic space by comput-

ing a sentence representation hst and sampling a

local latent variable zst for each group. h
s
t and z

s
t ,

along with gt, guide the word decoder to realize

the corresponding sentence st.

The planning process decomposes the long text

generation task into a sequence of dependent sen-

tence generation sub-tasks, thus facilitating the hi-

erarchical generation process. With the hierar-

chical latent structure, PHVM is able to capture

multi-level variations of texts.

4.2 Input Encoding

We first embed each input item di into vector

e(di). The recipe title r is also embedded as e(r).
We then encode x2 with a bidirectional Gated Re-

current Unit (GRU) (Cho et al., 2014). For ad-

vertising text generation, x is represented as the

concatenation of the last hidden states of the for-

ward and backward GRU enc(x) = [
−→
hN ;
←−
h1]; for

recipe text generation, enc(x) = [
−→
hN ;
←−
h1; e(r)].

hi = [
−→
hi ;
←−
hi ] is the context-aware representation

of di. Note that input encoder is not necessarily an

RNN; other neural encoders or even other encod-

ing schemes are also feasible, such as multi-layer

perceptron (MLP) and bag of words.

2For advertising text generation, x is first ordered by at-
tributes so that general attributes are ahead of specific ones;
for recipe text generation, we retain the order in the dataset

4.3 Planning Process

The planning process generates a subset of input

items to be covered for each sentence, thus decom-

posing long text generation into easier dependent

sentence generation sub-tasks. Due to the flexi-

bility of language, there may exist more than one

reasonable text that covers the same input but in

different order. To capture such variety, we model

the diversity of reasonable planning with a global

planning latent variable zp. Different samples of

zp may lead to different planning results which

control the order of content. This process can be

formulated as follows:

g = argmaxgP (g|x, z
p) (1)

where g = g1g2...gT is a sequence of groups, and
each group gt is a subset of input items which is a

main condition when realizing the sentence st.

The global latent variable zp is assumed to fol-

low the isotropic Gaussian distribution, and is

sampled from its prior distribution pθ(z
p|x) =

N (µp, σp2I) during inference and from its ap-
proximate posterior distribution qθ′ (z

p|x, y) =

N (µp
′

, σp
′2I) during training:

[µp; log σp2] = MLPθ(x) (2)

[µp
′

; log σp
′2] = MLPθ′ (x, y) (3)

We solve Eq. 1 greedily by computing gt =
argmaxgtP (gt|g<t, x, z

p) with the plan decoder
(a GRU). Specifically, at time step t, the plan de-

coder makes a binary prediction for each input

item by estimating P (di ∈ gt|g<t, x, z
p):

P (di ∈ gt) = σ(v
T
p tanh(Wp[hi;h

p
t ] + bp)) (4)

where σ denotes the sigmoid function, hi is the

vector of input item di, and h
p
t is the hidden

state of the plan decoder. Each group is therefore

formed as gt = {di|P (di ∈ gt) > 0.5} (If this is
empty, we set gt as {argmaxdiP (di ∈ gt)}.).

We feed bow(gt) (the average pooling of
{hi|di ∈ gt}) to the plan decoder at the next time
step, so that h

p
t+1 is aware of what data has been

selected and what has not. The planning process

proceeds until the probability of stopping at the

next time step is over 0.5:

P
stop
t = σ(Wch

p
t + bc) (5)

The hidden state is initialized with enc(x) and
zp. The plan decoder is trained with full super-

vision, which is applicable to those tasks where



3261

reference plans are available or can be approxi-

mated. For both tasks we evaluate in this paper,

we approximate the reference plans by recogniz-

ing the subset of input items covered by each sen-

tence with string match heuristics. The loss func-

tion at time step t is given by:

− logP (gt = g̃t|g̃<t, x, z
p)

=−
∑

di∈g̃t

logP (di ∈ gt)

−
∑

di /∈g̃t

log(1− P (di ∈ gt))

(6)

where g̃t is the reference group. As a result, z
p is

forced to capture features of reasonable planning.

4.4 Hierarchical Generation Process

The generation process produces a long text y =
s1s2...sT in alignment with the planning result

g = g1g2...gT , which is formulated as follows:

c = {x, zp} (7)

y = argmaxyP (y|g, c) (8)

We perform sentence-by-sentence generation

and solve Eq. 8 greedily by computing st =
argmaxstP (st|s<t, g, c). st focuses more on gt
than on the entire plan g. The generation pro-

cess is conducted hierarchically, which consists

of sentence-level generation and word-level gen-

eration. Sentence-level generation models inter-

sentence dependencies at high level, and interac-

tively controls word-level generation which con-

ducts low-level sentence realization.

Sentence-level Generation The sentence de-

coder (a GRU) performs sentence-level genera-

tion; for each sentence st to be generated, it pro-

duces a sentence representation hst and introduces

a local latent variable zst to control sentence real-

ization.

The latent variable zst is assumed to follow the

isotropic Gaussian distribution. At time step t, the

sentence decoder samples zst from the prior dis-

tribution pφ(z
s
t |s<t, g, c) = N (µ

s
t , σ

s2
t I) during

inference and from the approximate posterior dis-

tribution qφ′ (z
s
t |s≤t, g, c) = N (µ

s′
t , σ

s′2
t I) during

training. hst and the distribution of z
s
t are given by:

hst = GRU s([z
s
t−1;h

w
t−1], h

s
t−1) (9)

[µst ; log σ
s2
t ] = MLPφ(h

s
t , bow(gt)) (10)

[µs
′

t ; log σ
s′2
t ] =MLPφ′ (h

s
t , bow(gt), st) (11)

where hwt−1 is the last hidden state of the word de-

coder after decoding sentence st−1, and GRUs
denotes the GRU unit of the sentence decoder. By

this means, we constrain the distribution of zst in

two aspects. First, to strengthen the connection

from the planning result g, we additionally condi-

tion zst on gt to keep z
s
t focused on gt. Second,

to capture the dependencies on s<t, we explicitly

model the dependencies among local latent vari-

ables by inputting zst−1 to the sentence decoder, so

that zst is conditioned on z
s
<t and is expected to

model smooth transitions in a long text.

We initialize the hidden state hs0 by encoding

the input x, the global planning latent variable zp

and the planning result g:

h
g
t = GRUg(bow(gt), h

g
t−1) (12)

hs0 = Ws[enc(x); z
p;hgT ] + bs (13)

where h
g
T is the last hidden state of GRUg that

encodes the planning result g.

Word-level Generation The word decoder (a

GRU) conducts word-level generation; it decodes

a sentence st = argmaxstP (st|s<t, z
s
t , g, c) con-

ditioned on {hst , z
s
t , gt}. Specifically, we sample

word wtk of st as follows:

wtk ∼ P (w
t
k|w

t
<k, s<t, z

s
t , g, c) (14)

4.5 Loss Function

We train our model end-to-end. The loss func-

tion has three terms: the negative evidence lower

bound (ELBO) of logP (y|x) (L1), the loss of pre-
dicting the stop signal (L2) and the bag-of-word
loss (Zhao et al., 2017) (L3).

We first derive the ELBO:

logP (y|x) ≥ Eq
θ
′ (zp|x,y)[logP (y|x, z

p)]

−DKL(qθ′ (z
p|x, y)||pθ(z

p|x))
(15)

logP (y|x, zp) = logP (g, y|x, zp)

=

T∑

t=1

logP (gt|g<t, x, z
p)

+ logP (st|s<t, g, x, z
p)

(16)

logP (st|s<t, g, x, z
p)

≥ Eq
φ
′ (zst |s≤t,g,x,z

p)[logP (st|s<t, z
s
t , g, x, z

p)]

−DKL(qφ′ (z
s
t |s≤t, g, x, z

p)||pφ(z
s
t |s<t, g, x, z

p))

(17)

We can obtain the ELBO by unfolding the right

hand side of Eq. 15 with Eq. 16 and 17. During



3262

training, we use linear KL annealing technique to

alleviate the KL collapse problem(Bowman et al.,

2016).

L2 is given by:

L2 =
T−1∑

t=1

logP stopt + log(1− P
stop
T ) (18)

L3 is the sum of bag-of-word loss (Zhao et al.,
2017) applied to each sentence, which is another

technique to tackle the KL collapse problem.

5 Experiments

5.1 Dataset

We evaluated PHVM on two generation tasks. The

first task is the new advertising text generation task

which is to generate a long advertising text that

covers all given attribute-value pairs for a piece of

clothing. The second task is the recipe generation

task from (Kiddon et al., 2016) which is to gener-

ate a correct recipe for the given recipe title and

ingredient list.

Advertising Text Generation We constructed

our dataset from a Chinese e-commerce platform.

The dataset consists of 119K pairs of advertising

text and clothing specification table. Each table

is a set of attribute-value pairs describing a piece

of clothing. We made some modifications to the

original specification tables. Specifically, if some

attribute-value pairs from a table do not occur in

the corresponding text, the pairs are removed from

the table. We also recognized attribute values by

string matching with a dictionary of attribute val-

ues. If a pair occurs in the text but not in the table,

the pair is added to the table.

The statistics are shown in Table 1 and Table 2.

Category Tops Dress / Skirt Pants

# Type 22 23 9
# Attr. 13 16 11
# Val. 264 284 203

Avg. # Input Pairs 7.7 7.7 6.6
Avg. Len. 110 111 108
# Instances 48K 47K 24K

Table 1: Detailed statistics of our dataset. # Attr. / #

Val.: the total number of attributes / attribute values.

Our dataset consists of three categories of cloth-

ing: tops, dress / skirt, and pants, which are fur-

ther divided into 22, 23, and 9 types respectively

(E.g., shirt, sweater are two types of tops). Other

# Attr. # Val. Vocab Avg. # Input Pairs Avg. # Len.

28 633 54.9K 7.5 110.2

Table 2: General statistics of our dataset. We counted

the size of vocabulary after removing brand names.

categories (e.g., hats and socks) are discarded be-

cause these categories have insufficient data for

training. The average length of advertising text

is about 110 words. To evaluate the expression

diversity of our dataset, we computed distinct-4

(see Section 5.3) on 3,000 randomly sampled texts

from our dataset. The distinct-4 score is 85.35%,

much higher than those of WIKIBIO (Lebret et al.,

2016) and ROTOWIRE (Wiseman et al., 2017)

(two popular data-to-text datasets). Therefore, our

dataset is suitable for evaluating long and diverse

text generation3.

We left 1,070 / 3,127 instances for validation /

test, and used the remainder for training.

Recipe Text Generation We used the same

train-validation-test split (82,590 / 1,000 / 1,000)

and pre-processing from (Kiddon et al., 2016). In

the training set, the average recipe length is 102 to-

kens, and the vocabulary size of recipe title / text

is 3,793 / 14,103 respectively. The recipe dataset

covers a wide variety of recipe types indicated by

the vocabulary size of recipe title.

5.2 Baselines

We compared our model with four strong base-

lines where the former two do not perform plan-

ning and the latter two do:

Checklist: This model utilizes an attention-based

checklist mechanism to record what input data has

been mentioned, and focuses more on what has not

during generation (Kiddon et al., 2016).

CVAE: The CVAE model proposed by Zhao et al.

(2017) uses a latent variable to capture the di-

versity of responses in dialogue generation. We

adapted it to our task by replacing the hierarchical

encoder with a one-layer bidirectional GRU.

Pointer-S2S: A two-stage method (Puduppully

et al., 2019) that decides the order of input

data with Pointer Network (Vinyals et al., 2015)

before generation with Sequence-to-Sequence

(Seq2Seq) (Bahdanau et al., 2015).

Link-S2S Link-S2S (Sha et al., 2018) is a

Seq2Seq with link-based attention mechanism

3We presented a detailed comparison with other bench-
mark corpora in Appendix A.2.



3263

Models BLEU (%) Coverage (%) Length Distinct-4 (%) Repetition-4 (%)

Checklist 4.17 84.52** 83.61** 21.95** 46.40**
CVAE 4.02 77.65** 80.96** 41.69** 36.58**

Pointer-S2S 3.88** 85.97** 74.88** 18.16** 36.78**
Link-S2S 3.90** 70.49** 95.65 16.64** 59.83**

PHVM (ours) 2.85** 87.05 89.20** 72.87 3.90
w/o zp 3.07** 84.74** 91.97** 70.51** 4.19
w/o zst 3.38** 84.89** 75.28** 42.32** 20.88**

Table 3: Automatic evaluation for advertising text generation. We applied bootstrap resampling (Koehn, 2004) for

significance test. Scores that are significantly worse than the best results (in bold) are marked with ** for p-value

< 0.01.

where a link matrix parameterizes the probability

of describing one type of input item after another.

5.3 Implementation Details

For both advertising text generation and recipe text

generation, the settings of our model have many

in common. The dimension of word embedding

is 300. All embeddings were randomly initialized.

We utilized GRU for all RNNs. All RNNs, except

the input encoder, the plan decoder, and the plan

encoder, have a hidden size of 300. The global

planning latent variable and local latent variables

have 200 dimensions. We set batch size to 32

and trained our model using the Adam optimizer

(Kingma and Ba, 2015) with a learning rate of

0.001 and gradient clipping threshold at 5. We se-

lected the best model in terms of L1 + L2 on the
validation set.

As we need to train the plan decoder with full

supervision, we extracted plans from the texts

by recognizing attribute values (or ingredients) in

each sentence with string match heuristics. Some

sentences do not mention any input items; we as-

sociated these sentences with a special tag, which

is treated as a special input item for Pointer-S2S,

Link-S2S, and our model. Although our extraction

method can introduce errors, the extracted plans

are sufficient to train a good plan decoder4.

Advertising Text Generation We embedded an

attribute-value pair by concatenating the embed-

ding of attribute and the embedding of attribute

value. Embedding dimensions for attribute and

attribute value are 30 and 100 respectively. The

input encoder, the plan decoder, and the plan en-

coder all have a hidden size of 100.

Recipe Text Generation We embedded a multi-

word title (ingredient) by taking average pooling

4Our corpus and code are available at
https://github.com/ZhihongShao/Planning-based-
Hierarchical-Variational-Model.

of the embeddings of its constituent words. Em-

bedding dimensions for title word and ingredient

word are 100 and 200 respectively. The input en-

coder, the plan decoder, and the plan encoder all

have a hidden size of 200.

5.4 Automatic Evaluation Metrics

We adopted the following automatic metrics. (1)

Corpus BLEU: BLEU-4 (Papineni et al., 2002).

(2) Coverage: This metric measures the average

proportion of input items that are covered by a

generated text. We recognized attribute values (in-

gredients) with string match heuristics. For the

advertising text generation task, synonyms were

also considered. (3) Length: The average length

of the generated texts. (4) Distinct-4: Distinct-

n (Li et al., 2016) is a common metric for diver-

sity which measures the ratio of distinct n-grams

in generated tokens. We adopted distinct-4. (5)

Repetition-4: This metric measures redundancy

with the percentage of generated texts that repeat

at least one 4-gram.

5.5 Advertising Text Generation

5.5.1 Automatic Evaluation

Table 3 shows the experimental results. As our

dataset possesses high expression diversity, there

are many potential expressions for the same con-

tent, which leads to the low BLEU scores of all

models. Our model outperforms the baselines in

terms of coverage, indicating that it learns to ar-

range more input items in a long text. With content

ordering, Pointer-S2S outperforms both Checklist

and CVAE in coverage. By contrast, our plan-

ning mechanism is even more effective in con-

trolling generation: each sentence generation sub-

task is specific and focused, and manages to cover

95.16% of the corresponding group on average.

Noticeably, Link-S2S also models planning but



3264

Models
Grammaticality

κ
Coherence

κ

Win (%) Lose (%) Tie (%) Win (%) Lose (%) Tie (%)

PHVM vs. Checklist 59.0** 23.5 17.5 0.484 54.5* 42.5 3.0 0.425
PHVM vs. CVAE 69.5** 13.5 17.0 0.534 60.0** 37.0 3.0 0.426

PHVM vs. Pointer-S2S 76.5** 17.0 6.5 0.544 56.5** 39.0 4.5 0.414
PHVM vs. Link-S2S 66.0** 28.5 5.5 0.462 62.5** 31.5 6.0 0.415

Table 4: Manual pair-wise evaluation for advertising text generation. We conducted Sign Test for significance test.

Scores marked with * mean p-value < 0.05 and ** for p-value < 0.01. κ denotes Fleiss’ kappa, all indicating

moderate agreement.

has the lowest coverage, possibly because a static

link matrix is unable to model flexible content ar-

rangement in long text generation. As for diver-

sity, our model has substantially lower repetition-

4 and higher distinct-4, indicating that our gener-

ated texts are much less redundant and more di-

versified. Notably, Link-S2S has the longest texts

but with the highest repetition-4, which produces

many redundant expressions.

To investigate the influence of each component

in the hierarchical latent structure, we conducted

ablation tests which removed either global latent

variable zp or local latent variables zst . As ob-

served, removing zp leads to significantly lower

distinct-4, indicating that zp contributes to expres-

sion diversity. The lower coverage is because the

percentage of input items covered by a planning

result decreases from 98.4% to 94.4% on average,

which indicates that zp encodes useful information

for planning completeness. When removing zst ,

distinct-4 drops substantially, as the model tends

to generate shorter and more common sentences.

This indicates that zst contributes more to captur-

ing variations of texts. The significantly higher

repetition-4 is because removing zst weakens the

dependencies among sentences so that the word

decoder is less aware of the preceding generated

context. The lower coverage is because each gen-

erated sentence covers less planned items (from

95.16% to 93.07% on average), indicating that zst
keeps sentence st more focused on its group.

5.5.2 Manual Evaluation

To better evaluate the quality of the generated

texts, we conducted pair-wise comparisons manu-

ally. Each model generates texts for 200 randomly

sampled inputs from the test set. We hired five

annotators to give preference (win, lose or tie) to

each pair of texts (ours vs. a baseline, 800 pairs in

total).

Metrics Two metrics were independently eval-

uated during annotation: grammaticality which

measures whether a text is fluent and grammati-

cal, and coherence which measures whether a text

is closely relevant to input, logically coherent, and

well-organized.

Results The annotation results in Table 4 show

that our model significantly outperforms baselines

in both metrics. Our model produces more logi-

cally coherent and well-organized texts, which in-

dicates the effectiveness of the planning mecha-

nism. It is also worth noting that our model per-

forms better in terms of grammaticality. The rea-

son is that long text generation is decomposed into

sentence generation sub-tasks which are easier to

control, and our model captures inter-sentence de-

pendencies through modeling the dependencies

among local latent variables.

5.5.3 Diversity of Planning

3.7

3.9

4.1

4.3

4.5

4.7

8.4

8.8

9.2

9.6

10

[3, 6) [6, 9) [9, 12) [12, 15)

A
v

g
. 
S

c
o
re

A
v

g
. 

#
 D

is
ti

n
c
t 
P

la
n

n
in

g

# Input Pairs

Figure 3: Average number of distinct planning results

(left) / average score of generation quality (right) when

the number of input pairs varies.

To evaluate how well our model can capture the

diversity of planning, we conducted another man-

ual evaluation. We randomly sampled 100 test in-

puts and generated 10 texts for each input by re-

peatedly sampling latent variables. Five annota-

tors were hired to score (a Likert scale ∈ [1, 5])



3265

Models BLEU (%) Coverage (%) Length Distinct-4 (%) Repetition-4 (%)

Checklist § 3.0 67.9 N/A N/A N/A
Checklist 2.6** 66.9* 67.59 30.67** 39.1**

CVAE 4.6 63.0** 57.49** 52.53** 38.7**
Pointer-S2S 4.3 70.4** 59.18** 30.72** 36.4**
Link-S2S 1.9** 53.8** 40.34** 24.93** 31.6**

PHVM (ours) 4.6 73.2 70.92 67.86 17.3

Table 5: Automatic evaluation for recipe text generation. Checklist was trained with its own source code. We also

re-printed results from (Kiddon et al., 2016) (i.e., Checklist §). We applied bootstrap resampling (Koehn, 2004) for
significance test. Scores that are significantly worse than the best results (in bold) are marked with * for p-value <

0.05 or ** for p-value < 0.01.

a text about whether it is a qualified advertising

text, which requires comprehensive assessment in

terms of fluency, redundancy, content organiza-

tion, and coherence. We computed the average of

five ratings as the final score of a generated text.

Results The average score of a generated text is

4.27. Among the 1,000 generated texts, 79.0% of

texts have scores above 4. These results demon-

strate that our model is able to generate multiple

high-quality advertising texts for the same input.

We further analyzed how our model performs

with different numbers of input attribute-value

pairs (see Figure 3). A larger number of input

items indicates more potential reasonable ways

of content arrangement. As the number of input

items increases, our model produces more distinct

planning results while still obtaining high scores

(above 4.2). It indicates that our model captures

the diversity of reasonable planning. The aver-

age score drops slightly when the number of in-

put pairs is more than 12. This is due to insuf-

ficient training data for this range of input length

(accounting for 6.5% of the entire training set).

To further verify the planning diversity, we also

computed self-BLEU (Zhu et al., 2018) to evalu-

ate how different planning results (or texts) for the

same input overlap (by taking one planning result

(or text) as hypothesis and the rest 9 for the same

input as reference and then computing BLEU-4).

The average self-BLEU of the planning results is

43.37% and that of the texts is 16.87%, which

demonstrates the much difference among the 10

results for the same input.

Annotation Statistics The Fleiss’ kappa is 0.483,

indicating moderate agreement.

5.6 Recipe Text Generation

Table 5 shows the experimental results. Our model

outperforms baselines in terms of coverage and

diversity; it manages to use more given ingredi-

ents and generates more diversified cooking steps.

We also found that Checklist / Link-S2S produces

the general phrase “all ingredients” in 14.9% /

24.5% of the generated recipes, while CVAE /

Pointer-S2S / PHVM produce the phrase in 7.8% /

6.3% / 5.0% of recipes respectively. These results

demonstrate that our model may generalize well to

other data-to-text generation tasks.

6 Case Study

We present examples for both tasks in Appendix

B.

7 Conclusion and Future Work

We present the Planning-based Hierarchical Vari-

ational Model (PHVM) for long and diverse text

generation. A novel planning mechanism is pro-

posed to better model input data and address the

inter-sentence incoherence problem. PHVM also

leverages a hierarchical latent structure to capture

the diversity of reasonable planning and sentence

realization. Experiments on two data-to-text cor-

pora show that our model is more competitive to

generate long and diverse texts than state-of-the-

art baselines.

Our planning-based model may be inspiring to

other long text generation tasks such as long text

machine translation and story generation.

Acknowledgements

This work was supported by the National

Science Foundation of China (Grant No.

61936010/61876096) and the National Key R&D

Program of China (Grant No. 2018YFC0830200).

We would like to thank THUNUS NExT Joint-Lab

for the support.



3266

References

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In International Con-
ference on Learning Representations.

John A. Bateman. 1997. Enabling technology for mul-
tilingual natural language generation: the KPML de-
velopment environment. Natural Language Engi-
neering, 3(1):15–55.

Samuel R. Bowman, Luke Vilnis, Oriol Vinyals, An-
drew M. Dai, Rafal Józefowicz, and Samy Ben-
gio. 2016. Generating sentences from a continuous
space. In Proceedings of the 20th SIGNLL Confer-
ence on Computational Natural Language Learning,
CoNLL 2016, Berlin, Germany, August 11-12, 2016,
pages 10–21.

Qibin Chen, Junyang Lin, Yichang Zhang, Hongxia
Yang, Jingren Zhou, and Jie Tang. 2019. Towards
knowledge-based personalized product description
generation in e-commerce. In Proceedings of the
25th ACM SIGKDD International Conference on
Knowledge Discovery & Data Mining, KDD 2019,
Anchorage, AK, USA, August 4-8, 2019., pages
3040–3050.

Kyunghyun Cho, Bart van Merrienboer, Dzmitry Bah-
danau, and Yoshua Bengio. 2014. On the properties
of neural machine translation: Encoder-decoder ap-
proaches. In Proceedings of SSST@EMNLP 2014,
Eighth Workshop on Syntax, Semantics and Struc-
ture in Statistical Translation, Doha, Qatar, 25 Oc-
tober 2014, pages 103–111.

Hercules Dalianis and Eduard H. Hovy. 1993. Ag-
gregation in natural language generation. In Trends
in Natural Language Generation, An Artificial In-
telligence Perspective, Fourth European Workshop,
EWNLG ’93, Pisa, Italy, April 28-30, 1993, Selected
Papers, pages 88–105.

Kees van Deemter, Mariët Theune, and Emiel Krahmer.
2005. Real versus template-based natural language
generation: A false opposition? Computational Lin-
guistics, 31(1):15–24.

Pablo A Duboue and Kathleen R McKeown. 2003. Sta-
tistical acquisition of content selection rules for nat-
ural language generation. In Proceedings of the
2003 conference on Empirical methods in natural
language processing, pages 121–128. Association
for Computational Linguistics.

Dominic Espinosa, Michael White, and Dennis Mehay.
2008. Hypertagging: Supertagging for surface real-
ization with CCG. In ACL 2008, Proceedings of the
46th Annual Meeting of the Association for Compu-
tational Linguistics, June 15-20, 2008, Columbus,
Ohio, USA, pages 183–191.

Xiaocheng Feng, Ming Liu, Jiahao Liu, Bing Qin, Yibo
Sun, and Ting Liu. 2018. Topic-to-essay generation

with neural networks. In Proceedings of the Twenty-
Seventh International Joint Conference on Artificial
Intelligence, IJCAI 2018, July 13-19, 2018, Stock-
holm, Sweden., pages 4078–4084.

Albert Gatt and Emiel Krahmer. 2018. Survey of the
state of the art in natural language generation: Core
tasks, applications and evaluation. J. Artif. Intell.
Res., 61:65–170.

Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,
Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron C. Courville, and Yoshua Bengio. 2014. Gen-
erative adversarial nets. In Advances in Neural
Information Processing Systems 27: Annual Con-
ference on Neural Information Processing Systems
2014, December 8-13 2014, Montreal, Quebec,
Canada, pages 2672–2680.

Jiaxian Guo, Sidi Lu, Han Cai, Weinan Zhang, Yong
Yu, and Jun Wang. 2018. Long text generation
via adversarial training with leaked information.
In (McIlraith and Weinberger, 2018), pages 5141–
5148.

Eduard H. Hovy. 1993. Automated discourse genera-
tion using discourse structure relations. Artif. Intell.,
63(1-2):341–385.

Chloé Kiddon, Luke Zettlemoyer, and Yejin Choi.
2016. Globally coherent text generation with neural
checklist models. In (Su et al., 2016), pages 329–
339.

Diederik P. Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. In International
Conference on Learning Representations.

Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In EMNLP, pages
388–395. ACL.

Ioannis Konstas and Mirella Lapata. 2013. Inducing
document plans for concept-to-text generation. In
Proceedings of the 2013 Conference on Empirical
Methods in Natural Language Processing, EMNLP
2013, 18-21 October 2013, Grand Hyatt Seattle,
Seattle, Washington, USA, A meeting of SIGDAT,
a Special Interest Group of the ACL, pages 1503–
1514. ACL.

Karen Kukich. 1983. Design of a knowledge-based
report generator. In 21st Annual Meeting of the
Association for Computational Linguistics, Mas-
sachusetts Institute of Technology, Cambridge, Mas-
sachusetts, USA, June 15-17, 1983., pages 145–150.

Rémi Lebret, David Grangier, and Michael Auli. 2016.
Neural text generation from structured data with ap-
plication to the biography domain. In (Su et al.,
2016), pages 1203–1213.

Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,
and Bill Dolan. 2016. A diversity-promoting ob-
jective function for neural conversation models. In
NAACL HLT 2016, The 2016 Conference of the

http://arxiv.org/abs/1409.0473
http://arxiv.org/abs/1409.0473
https://doi.org/10.1017/S1351324997001514
https://doi.org/10.1017/S1351324997001514
https://doi.org/10.1017/S1351324997001514
http://aclweb.org/anthology/K/K16/K16-1002.pdf
http://aclweb.org/anthology/K/K16/K16-1002.pdf
https://doi.org/10.1145/3292500.3330725
https://doi.org/10.1145/3292500.3330725
https://doi.org/10.1145/3292500.3330725
http://aclweb.org/anthology/W/W14/W14-4012.pdf
http://aclweb.org/anthology/W/W14/W14-4012.pdf
http://aclweb.org/anthology/W/W14/W14-4012.pdf
https://doi.org/10.1007/3-540-60800-1_25
https://doi.org/10.1007/3-540-60800-1_25
https://doi.org/10.1162/0891201053630291
https://doi.org/10.1162/0891201053630291
http://www.aclweb.org/anthology/P08-1022
http://www.aclweb.org/anthology/P08-1022
https://doi.org/10.24963/ijcai.2018/567
https://doi.org/10.24963/ijcai.2018/567
https://doi.org/10.1613/jair.5477
https://doi.org/10.1613/jair.5477
https://doi.org/10.1613/jair.5477
http://papers.nips.cc/paper/5423-generative-adversarial-nets
http://papers.nips.cc/paper/5423-generative-adversarial-nets
https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16360
https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16360
https://doi.org/10.1016/0004-3702(93)90021-3
https://doi.org/10.1016/0004-3702(93)90021-3
http://aclweb.org/anthology/D/D16/D16-1032.pdf
http://aclweb.org/anthology/D/D16/D16-1032.pdf
http://arxiv.org/abs/1412.6980
http://arxiv.org/abs/1412.6980
http://aclweb.org/anthology/D/D13/D13-1157.pdf
http://aclweb.org/anthology/D/D13/D13-1157.pdf
http://aclweb.org/anthology/P/P83/P83-1022.pdf
http://aclweb.org/anthology/P/P83/P83-1022.pdf
http://aclweb.org/anthology/D/D16/D16-1128.pdf
http://aclweb.org/anthology/D/D16/D16-1128.pdf
http://aclweb.org/anthology/N/N16/N16-1014.pdf
http://aclweb.org/anthology/N/N16/N16-1014.pdf


3267

North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, San Diego California, USA, June 12-17,
2016, pages 110–119.

Jiwei Li, Minh-Thang Luong, and Dan Jurafsky. 2015.
A hierarchical neural autoencoder for paragraphs
and documents. In Proceedings of the 53rd Annual
Meeting of the Association for Computational Lin-
guistics and the 7th International Joint Conference
on Natural Language Processing of the Asian Fed-
eration of Natural Language Processing, ACL 2015,
July 26-31, 2015, Beijing, China, Volume 1: Long
Papers, pages 1106–1115.

Tianyu Liu, Kexiang Wang, Lei Sha, Baobao Chang,
and Zhifang Sui. 2018. Table-to-text generation by
structure-aware seq2seq learning. In (McIlraith and
Weinberger, 2018), pages 4881–4888.

Sheila A. McIlraith and Kilian Q. Weinberger, edi-
tors. 2018. Proceedings of the Thirty-Second AAAI
Conference on Artificial Intelligence, (AAAI-18),
the 30th innovative Applications of Artificial Intel-
ligence (IAAI-18), and the 8th AAAI Symposium
on Educational Advances in Artificial Intelligence
(EAAI-18), New Orleans, Louisiana, USA, February
2-7, 2018. AAAI Press.

Susan W McRoy, Songsak Channarukul, and Syed S
Ali. 2003. An augmented template-based approach
to text realization. Natural Language Engineering,
9(4):381–420.

Hongyuan Mei, Mohit Bansal, and Matthew R. Wal-
ter. 2016. What to talk about and how? selec-
tive generation using lstms with coarse-to-fine align-
ment. In NAACL HLT 2016, The 2016 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, San Diego California, USA, June 12-17,
2016, pages 720–730.

Amit Moryossef, Yoav Goldberg, and Ido Dagan. 2019.
Step-by-step: Separating planning from realization
in neural data-to-text generation. In Proceedings of
the 2019 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies.

Preksha Nema, Shreyas Shetty, Parag Jain, Anirban
Laha, Karthik Sankaranarayanan, and Mitesh M.
Khapra. 2018. Generating descriptions from struc-
tured data using a bifocal attention mechanism and
gated orthogonalization. In Proceedings of the 2018
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, NAACL-HLT 2018, New
Orleans, Louisiana, USA, June 1-6, 2018, Volume
1 (Long Papers), pages 1539–1550.

Martha Palmer, Rebecca Hwa, and Sebastian Riedel,
editors. 2017. Proceedings of the 2017 Confer-
ence on Empirical Methods in Natural Language
Processing, EMNLP 2017, Copenhagen, Denmark,

September 9-11, 2017. Association for Computa-
tional Linguistics.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In ACL, pages 311–
318. ACL.

Yookoon Park, Jaemin Cho, and Gunhee Kim. 2018. A
hierarchical latent structure for variational conversa-
tion modeling. In Proceedings of the 2018 Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, NAACL-HLT 2018, New Or-
leans, Louisiana, USA, June 1-6, 2018, Volume 1
(Long Papers), pages 1792–1801.

Ratish Puduppully, Li Dong, and Mirella Lapata. 2019.
Data-to-text generation with content selection and
planning. In Proceedings of the Thirty-Third AAAI
Conference on Artificial Intelligence.

Ehud Reiter and Robert Dale. 1997. Building applied
natural language generation systems. Natural Lan-
guage Engineering, 3(1):57–87.

Iulian Vlad Serban, Alessandro Sordoni, Ryan Lowe,
Laurent Charlin, Joelle Pineau, Aaron C. Courville,
and Yoshua Bengio. 2017. A hierarchical latent
variable encoder-decoder model for generating di-
alogues. In Proceedings of the Thirty-First AAAI
Conference on Artificial Intelligence, February 4-9,
2017, San Francisco, California, USA., pages 3295–
3301. AAAI Press.

Lei Sha, Lili Mou, Tianyu Liu, Pascal Poupart, Sujian
Li, Baobao Chang, and Zhifang Sui. 2018. Order-
planning neural text generation from structured data.
In (McIlraith and Weinberger, 2018), pages 5414–
5421.

Yuanlong Shao, Stephan Gouws, Denny Britz, Anna
Goldie, Brian Strope, and Ray Kurzweil. 2017.
Generating high-quality and informative conversa-
tion responses with sequence-to-sequence models.
In (Palmer et al., 2017), pages 2210–2219.

Dinghan Shen, Asli Çelikyilmaz, Yizhe Zhang, Liqun
Chen, Xin Wang, Jianfeng Gao, and Lawrence
Carin. 2019. Towards generating long and coherent
text with multi-level latent variable models. In Pro-
ceedings of the 57th Conference of the Association
for Computational Linguistics, ACL 2019, Florence,
Italy, July 28- August 2, 2019, Volume 1: Long Pa-
pers, pages 2079–2089.

Kihyuk Sohn, Honglak Lee, and Xinchen Yan. 2015.
Learning structured output representation using
deep conditional generative models. In NIPS, pages
3483–3491.

Amanda Stent, Rashmi Prasad, and Marilyn A. Walker.
2004. Trainable sentence planning for complex in-
formation presentations in spoken dialog systems.
In Proceedings of the 42nd Annual Meeting of the
Association for Computational Linguistics, 21-26
July, 2004, Barcelona, Spain., pages 79–86.

http://aclweb.org/anthology/P/P15/P15-1107.pdf
http://aclweb.org/anthology/P/P15/P15-1107.pdf
https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16599
https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16599
https://www.aaai.org/ocs/index.php/AAAI/AAAI18/schedConf/presentations
https://www.aaai.org/ocs/index.php/AAAI/AAAI18/schedConf/presentations
https://www.aaai.org/ocs/index.php/AAAI/AAAI18/schedConf/presentations
https://www.aaai.org/ocs/index.php/AAAI/AAAI18/schedConf/presentations
https://www.aaai.org/ocs/index.php/AAAI/AAAI18/schedConf/presentations
https://www.aaai.org/ocs/index.php/AAAI/AAAI18/schedConf/presentations
https://www.aaai.org/ocs/index.php/AAAI/AAAI18/schedConf/presentations
http://aclweb.org/anthology/N/N16/N16-1086.pdf
http://aclweb.org/anthology/N/N16/N16-1086.pdf
http://aclweb.org/anthology/N/N16/N16-1086.pdf
https://aclanthology.info/papers/N18-1139/n18-1139
https://aclanthology.info/papers/N18-1139/n18-1139
https://aclanthology.info/papers/N18-1139/n18-1139
https://aclanthology.info/volumes/proceedings-of-the-2017-conference-on-empirical-methods-in-natural-language-processing
https://aclanthology.info/volumes/proceedings-of-the-2017-conference-on-empirical-methods-in-natural-language-processing
https://aclanthology.info/volumes/proceedings-of-the-2017-conference-on-empirical-methods-in-natural-language-processing
https://aclanthology.info/volumes/proceedings-of-the-2017-conference-on-empirical-methods-in-natural-language-processing
https://aclanthology.info/papers/N18-1162/n18-1162
https://aclanthology.info/papers/N18-1162/n18-1162
https://aclanthology.info/papers/N18-1162/n18-1162
http://arxiv.org/abs/1809.00582
http://arxiv.org/abs/1809.00582
https://doi.org/10.1017/S1351324997001502
https://doi.org/10.1017/S1351324997001502
http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14567
http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14567
http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14567
https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16203
https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16203
https://aclanthology.info/papers/D17-1235/d17-1235
https://aclanthology.info/papers/D17-1235/d17-1235
https://www.aclweb.org/anthology/P19-1200/
https://www.aclweb.org/anthology/P19-1200/
http://aclweb.org/anthology/P/P04/P04-1011.pdf
http://aclweb.org/anthology/P/P04/P04-1011.pdf


3268

Jian Su, Xavier Carreras, and Kevin Duh, editors. 2016.
Proceedings of the 2016 Conference on Empirical
Methods in Natural Language Processing, EMNLP
2016, Austin, Texas, USA, November 1-4, 2016. The
Association for Computational Linguistics.

Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly.
2015. Pointer networks. In Advances in Neural
Information Processing Systems 28: Annual Con-
ference on Neural Information Processing Systems
2015, December 7-12, 2015, Montreal, Quebec,
Canada, pages 2692–2700.

Sam Wiseman, Stuart M. Shieber, and Alexander M.
Rush. 2017. Challenges in data-to-document gener-
ation. In (Palmer et al., 2017), pages 2253–2263.

Chen Xing, Wei Wu, Yu Wu, Jie Liu, Yalou Huang,
Ming Zhou, and Wei-Ying Ma. 2017. Topic aware
neural response generation. In Proceedings of the
Thirty-First AAAI Conference on Artificial Intelli-
gence.

Jingjing Xu, Xuancheng Ren, Junyang Lin, and
Xu Sun. 2018. Diversity-promoting gan: A cross-
entropy based generative adversarial network for di-
versified text generation. In Proceedings of the 2018
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 3940–3949. Association
for Computational Linguistics.

Tiancheng Zhao, Ran Zhao, and Maxine Eskénazi.
2017. Learning discourse-level diversity for neural
dialog models using conditional variational autoen-
coders. In Proceedings of the 55th Annual Meet-
ing of the Association for Computational Linguis-
tics, ACL 2017, Vancouver, Canada, July 30 - August
4, Volume 1: Long Papers, pages 654–664.

Yaoming Zhu, Sidi Lu, Lei Zheng, Jiaxian Guo,
Weinan Zhang, Jun Wang, and Yong Yu. 2018.
Texygen: A benchmarking platform for text genera-
tion models. In The 41st International ACM SIGIR
Conference on Research & Development in Infor-
mation Retrieval, SIGIR 2018, Ann Arbor, MI, USA,
July 08-12, 2018, pages 1097–1100.

http://aclweb.org/anthology/D/D16/
http://aclweb.org/anthology/D/D16/
http://aclweb.org/anthology/D/D16/
http://papers.nips.cc/paper/5866-pointer-networks
https://aclanthology.info/papers/D17-1239/d17-1239
https://aclanthology.info/papers/D17-1239/d17-1239
https://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14563
https://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14563
http://aclweb.org/anthology/D18-1428
http://aclweb.org/anthology/D18-1428
http://aclweb.org/anthology/D18-1428
https://doi.org/10.18653/v1/P17-1061
https://doi.org/10.18653/v1/P17-1061
https://doi.org/10.18653/v1/P17-1061
https://doi.org/10.1145/3209978.3210080
https://doi.org/10.1145/3209978.3210080

