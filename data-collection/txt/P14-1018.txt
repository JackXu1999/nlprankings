



















































Inferring User Political Preferences from Streaming Communications


Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 186–196,
Baltimore, Maryland, USA, June 23-25 2014. c©2014 Association for Computational Linguistics

Inferring User Political Preferences from Streaming Communications

Svitlana Volkova,1 Glen Coppersmith2 and Benjamin Van Durme1,2
1Center for Language and Speech Processing,

2Human Language Technology Center of Excellence,
Johns Hopkins University, Baltimore, MD 21218

svitlana@jhu.edu, coppersmith@jhu.edu, vandurme@cs.jhu.edu

Abstract

Existing models for social media per-
sonal analytics assume access to thou-
sands of messages per user, even though
most users author content only sporadi-
cally over time. Given this sparsity, we:
(i) leverage content from the local neigh-
borhood of a user; (ii) evaluate batch mod-
els as a function of size and the amount
of messages in various types of neighbor-
hoods; and (iii) estimate the amount of
time and tweets required for a dynamic
model to predict user preferences. We
show that even when limited or no self-
authored data is available, language from
friend, retweet and user mention commu-
nications provide sufficient evidence for
prediction. When updating models over
time based on Twitter, we find that polit-
ical preference can be often be predicted
using roughly 100 tweets, depending on
the context of user selection, where this
could mean hours, or weeks, based on the
author’s tweeting frequency.

1 Introduction

Inferring latent user attributes such as gender, age,
and political preferences (Rao et al., 2011; Za-
mal et al., 2012; Cohen and Ruths, 2013) auto-
matically from personal communications and so-
cial media including emails, blog posts or public
discussions has become increasingly popular with
the web getting more social and volume of data
available. Resources like Twitter1 or Facebook2

become extremely valuable for studying the un-
derlying properties of such informal communica-
tions because of its volume, dynamic nature, and
diverse population (Lunden, 2012; Smith, 2013).

1http://www.demographicspro.com/
2http://www.wolframalpha.com/facebook/

The existing batch models for predicting latent
user attributes rely on thousands of tweets per
author (Rao et al., 2010; Conover et al., 2011;
Pennacchiotti and Popescu, 2011a; Burger et al.,
2011; Zamal et al., 2012; Nguyen et al., 2013).
However, most Twitter users are less prolific than
those examined in these works, and thus do not
produce the thousands of tweets required to obtain
their levels of accuracy e.g., the median number of
tweets produced by a random Twitter user per day
is 10. Moreover, recent changes to Twitter API
querying rates further restrict the speed of access
to this resource, effectively reducing the amount of
data that can be collected in a given time period.

In this paper we analyze and go beyond static
models formulating personal analytics in social
media as a streaming task. We first evaluate batch
models that are cognizant of low-resource predic-
tion setting described above, maximizing the effi-
ciency of content in calculating personal analytics.
To the best of our knowledge, this is the first work
that makes explicit the tradeoff between accuracy
and cost (manifest as calls to the Twitter API),
and optimizes to a different tradeoff than state-of-
the-art approaches, seeking maximal performance
when limited data is available. In addition, we
propose streaming models for personal analytics
that dynamically update user labels based on their
stream of communications which has been ad-
dressed previously by Van Durme (2012b). Such
models better capture the real-time nature of evi-
dence being used in latent author attribute predic-
tions tasks. Our main contributions include:

- develop low-resource and real-time dynamic
approaches for personal analytics using as an
example the prediction of political preference
of Twitter users;

- examine the relative utility of six different
notions of “similarity” between users in an
implicit Twitter social network for personal
analytics;

186



- experiments are performed across multiple
datasets supporting the prediction of politi-
cal preference in Twitter, to highlight the sig-
nificant differences in performance that arise
from the underlying collection and annota-
tion strategies.

2 Identifying Twitter Social Graph

Twitter users interact with one another and en-
gage in direct communication in different ways
e.g., using retweets, user mentions e.g., @youtube
or hashtags e.g., #tcot, in addition to having ex-
plicit connections among themselves such as fol-
lowing, friending. To investigate all types of social
relationships between Twitter users and construct
Twitter social graphs we collect lists of followers
and friends, and extract user mentions, hashtags,
replies and retweets from communications.3

2.1 Social Graph Definition
Lets define an attributed, undirected graph G =
(V,E), where V is a set of vertices and E is a set
of edges. Each vertex vi represents someone in
a communication graph i.e., communicant: here
a Twitter user. Each vertex is attributed with a
feature vector ~f(vi) which encodes communica-
tions e.g., tweets available for a given user. Each
vertex is associated with a latent attribute a(vi),
in our case it is binary a(vi) ∈ {D,R}, where
D stands for Democratic and R for Republican
users. Each edge eij ∈ E represents a connec-
tion between vi and vj , eij = (vi, vj) and defines
different social circles between Twitter users e.g.,
follower (f), friend (b), user mention (m), hash-
tag (h), reply (y) and retweet (w). Thus, E ∈
V (2)×{f, b, h,m,w, y}. We denote a set of edges
of a given type as φr(E) for r ∈ {f, b, h,m,w, y}.
We denote a set of vertices adjacent to vi by so-
cial circle type r as Nr(vi) which is equivalent to
{vj | eij ∈ φr(E)}. Following Filippova (2012)
we refer to Nr(vi) as vi’s social circle, otherwise
known as a neighborhood. In most cases, we only
work with a sample of a social circle, denoted by
N ′r(vi) where |N ′r(vi)| = k is its size for vi.

Figure 1 presents an example of a social graph
derived from Twitter. Notably, users from differ-
ent social circles can be shared across the users of
the same or different classes e.g., a user vj can be

3The code and detailed explanation on how we col-
lected all six types of user neighbors and their com-
munications using Twitter API can be found here:
http://www.cs.jhu.edu/ svitlana/

Figure 1: An example of a social graph with follower, friend,
@mention, reply, retweet and hashtag social circles for each
user of interest e.g., blue: Democratic, red: Republican.

in both follower circle vj ∈ Nf (vi), vi ∈ D and
retweet circle vj ∈ Nw(vk), vk ∈ R.
2.2 Candidate-Centric Graph
We construct candidate-centric graph Gcand by
looking into following relationships between the
users and Democratic or Republican candidates
during the 2012 US Presidential election. In the
Fall of 2012, leading up to the elections, we ran-
domly sampled n = 516 Democratic and m =
515 Republican users. We labeled users as Demo-
cratic if they exclusively follow both Democratic
candidates4 – BarackObama and JoeBiden but
do not follow both Republican candidates – Mit-
tRomney and RepPaulRyan and vice versa. We
collectively refer to D and R as our “users of in-
terest” for which we aim to predict political prefer-
ence. For each such user we collect recent tweets
and randomly sample their immediate k = 10
neighbors from follower, friend, user mention, re-
ply, retweet and hashtag social circles.

2.3 Geo-Centric Graph
We construct a geo-centric graph Ggeo by col-
lecting n = 135 Democratic and m = 135 Re-
publican users from the Maryland, Virginia and
Delaware region of the US with self-reported po-
litical preference in their biographies. Similar to
the candidate-centric graph, for each user we col-
lect recent tweets and randomly sample user social
circles in the Fall of 2012. We collect this data to
get a sample of politically less active users com-
pared to the users from candidate-centric graph.

2.4 ZLR Graph
We also consider a GZLR graph constructed from
a dataset previously used for political affiliation

4As of Oct 12, 2012, the number of followers for Obama,
Biden, Romney and Ryan were 2m, 168k, 1.3m and 267k.

187



classification (Zamal et al., 2012). This dataset
consists of 200 Republican and 200 Democratic
users associated with 925 tweets on average per
user.5 Each user has on average 6155 friends with
642 tweets per friend. Sharing restrictions and rate
limits on Twitter data collection only allowed us to
recreate a semblance of ZLR data6 – 193 Demo-
cratic and 178 Republican users with 1K tweets
per user, and 20 neighbors of four types including
follower, friends, user mention and retweet with
200 tweets per neighbor for each user of interest.

3 Batch Models

Baseline User Model As input we are given a set
of vertices representing users of interest vi ∈ V
along with feature vectors ~f(vi) derived from con-
tent authored by the user of interest. Each user
is associated with a non-zero number of publicly
posted tweets. Our goal is assign to a category
each user of interest vi based on ~f(vi). Here we
focus on a binary assignment into the categories
Democratic D or Republican R. The log-linear
model7 for such binary classification is:

Φvi =
{
D (1 + exp[−~θ · ~f(vi)])−1 ≥ 0.5,
R otherwise.

(1)
where features are normalized word ngram counts
extracted from vi’s tweets ~ft(vi) : D×t(vi)→ R.

The proposed baseline model follows the same
trends as the existing state-of-the-art approaches
for user attribute classification in social media as
described in Section 8. Next we propose to ex-
tend the baseline model by taking advantage of
language in user social circles as describe below.

Neighbor Model As input we are given user-local
neighborhood Nr(vi), where r is a neighborhood
type. Besides the neighborhood’s type r, each is
characterized by:
• the number of communications per neighbor
~ft(Nr), t = {5, 10, 15, 25, 50, 100, 200};

5The original dataset was collected in 2012 and has
been recently released at http://icwsm.cs.mcgill.ca/. Politi-
cal labels are extracted from http://www.wefollow.com as de-
scribed by Pennacchiotti and Popescu (2011b).

6This inability to perfectly replicate prior work based on
Twitter is a recognized problem throughout the community of
computational social science, arising from the data policies of
Twitter itself, it is not specific to this work.

7We use log-linear models over reasonable alternatives
such as perceptron or SVM, following the practice of a wide
range of previous work in related areas (Smith, 2004; Liu et
al., 2005; Poon et al., 2009) including text classification in so-
cial media (Van Durme, 2012b; Yang and Eisenstein, 2013).

• the order of the social circle – the num-
ber of neighbors per user of interest |Nr| =
deg(vi), n = {1, 2, 5, 10}.

Our goal is to classify users of interest using
evidence (e.g., communications) from their local
neighborhood

∑
n

~ft[Nr(vi)] ≡ ~f(Nr) as Demo-
cratic or Republican. The corresponding log-
linear model is defined as:

ΦNr =
{
D (1 + exp[−~θ · ~f(Nr)])−1 ≥ 0.5,
R otherwise.

(2)
To check whether our static models are cog-

nizant of low-resource prediction settings we com-
pare the performance of the user model from Eq.1
and the neighborhood model from Eq.2. Follow-
ing the streaming nature of social media, we see
the scarce available resource as the number of re-
quests allowed per day to the Twitter API. Here
we abstract this to a model assumption where we
receive one tweet tk at a time and aim to maximize
classification performance with as few tweets per
user as possible:8

• for the baseline user model:

minimize
k

∑
k

tk(vi), (3)

• for the neighborhood model:

minimize
k

∑
n

∑
k

tk[Nr(vi)]. (4)

4 Streaming Models

We rely on straightforward Bayesian rule update
to our batch models in order to simulate a real-
time streaming prediction scenario as a first step
beyond the existing models as shown in Figure 2.

The model makes predictions of a latent user at-
tribute e.g., Republican under a model assumption
of sequentially arriving, independent and identi-
cally distributed observations T = (t1, . . . , tk)9.
The model dynamically updates posterior proba-
bility estimates p(a(vi) = R|tk) for a given user

8The separate issue is that many authors simply don’t
tweet very often. For instance, 85.3% of all Twitter
users post less than one update per day as reported at
http://www.sysomos.com/insidetwitter/. Thus, their commu-
nications are scare even if we could get all of them without
rate limiting from Twitter API.

9Given the dynamic character of online discourse it will
clearly be of interest in the future to consider models that go
beyond the iid assumption.

188



p(R|t1)
0.6

vi vi vi

p(R|t1, t2)
0.7

p(R|t1, . . . tk)
0.9

Nr Nr Nr

Time, ττ2τ1 τk

Figure 2: Stream-based classification of an attribute a(vi) ∈
{R,D} given a stream of communications t1, t2, . . . , tk au-
thored by a user vi or user immediate neighbors from Nr
social circles at time τ1, τ2, . . . , τk.

vi as an additional evidence tk is acquired, as de-
fined in a general form below for any latent at-
tribute a(vi) ∈ A given the tweets T of user vi:

p(a(vi) = x ∈ A | T ) =
p(T | a(vi) = x) · p(a(vi) = x)∑
y∈A p(T | a(vi) = y) · p(a(vi) = y)

=∏
k p(tk | a(vi) = x) · p(a(vi) = x)∑

y∈A
∏
k p(tk | a(vi) = y) · p(a(vi) = y),

(5)
where y is the number of all possible attribute val-
ues, and k is the number of tweets per user.

For example, to predict user political prefer-
ence, we start with a prior P (R) = 0.5, and se-
quentially update the posterior p(R | T ) by accu-
mulating evidence from the likelihood p(tk|R):

p(R | T ) = ∏
k p(tk|R) · p(R)∏

k P (tk|R) · p(R) +
∏
k P (tk|D) · p(D).

(6)

Our goal is to maximize posterior probability
estimates given a stream of communications for
each user in the data over (a) time τ and (b) the
number of tweets T . For that, for each user we
take tweets that arrive continuously over time and
apply two different streaming models:
• User Model with Dynamic Updates: re-

lies exclusively on user tweets t(vi)1 , . . . , t
(vi)
k

following the order they arrive over time τ ,
where for each user vi we dynamically up-
date the posterior p(R | t(vi)1 , . . . , t(vi)k ).
• User-Neighbor Model with Dynamic Up-

dates: relies on both neighbor Nr commu-
nications including friend, follower, retweet,
user mention and user tweets t(vi)1 , . . . , t

(Nr)
k

following the order they arrive over time τ ;
here we dynamically update the posterior
probability p(R | t(vi)1 , . . . , t(Nr)k ).

5 Experimental Setup

We design a set of experiments to analyze static
and dynamic models for political affiliation classi-
fication defined in Sections 3 and 4.

5.1 Batch Classification Experiments

We first answer whether communications from
user-local neighborhoods can help predict politi-
cal preference for the user. To explore the con-
tribution of different neighborhood types we learn
static user and neighbor models on Gcand, Ggeo
and GZLR graphs. We also examine the ability of
our static models to predict user political prefer-
ences in low-resource setting e.g., 5 tweets.

The existing models follow a standard setup
when either user or neighbor tweets are available
during train and test. For a static neighbor model
we go beyond that, and train our the model on all
data available per user, but only apply part of the
data at the test time, pushing the boundaries of
how little is truly required for classification. For
example, we only use follower tweets for Gtest,
but we use tweets from all types of neighbors for
Gtrain. Such setup will simulate different real-
world prediction scenarios which have not been
previously explored, to our knowledge e.g., when
a user has a private profile or has not tweeted yet,
and only user neighbor tweets are available.

We experiment with our static neighbor model
defined in Eq.2 with the aim to:

1. evaluate neighborhood size influence, we
change the number of neighbors and try n =
[1, 2, 5, 10] neighbor(s) per user;

2. estimate neighbor content influence, we alter-
nate the amount of content per neighbor and
try t = [5, 10, 15, 25, 50, 100, 200] tweets.

We perform 10-fold cross validation10 and run
100 random restarts for every n and t parame-
ter combination. We compare our static neigh-
bor and user models using the cost functions
from Eq.3 and Eq.4. For all experiments we use
LibLinear (Fan et al., 2008), integrated in the
Jerboa toolkit (Van Durme, 2012a). Both mod-
els defined in Eq.1 and Eq.2 are learned using
normalized count-based word ngram features ex-
tracted from either user or neighbor tweets.11

10For each fold we split the data into 3 parts: 70% train,
10% development and 20% test.

11For brevity we omit reporting results for bigram and tri-
gram features, since unigrams showed superior performance.

189



5.2 Streaming Classification Experiments

We evaluate our models with dynamic Bayesian
updates on a continuous stream of communica-
tions over time as shown in Figure 2. Unlike static
model experiments, we are not modeling the in-
fluence of the number of neighbors or the amount
of content per neighbor. Here, we order user and
neighbor communication streams by real world
time of posting and measure changes in posterior
probabilities over time. The main purpose of these
experiments is to quantitatively evaluate (1) the
number of tweets and (2) the amount of real world
time it takes to observe enough evidence on Twit-
ter to make reliable predictions.

We experiment with log-linear models defined
in Eq. 1 and 2 and continuously estimate the poste-
rior probabilities P (R | T ) as defined in Eq.6. We
average the posterior probability results over the
users in Gcand, Ggeo and GZLR graphs. We train
streaming models on an attribute balanced subset
of tweets for each user vi excluding vi’s tweets (or
vi’s neighbor tweets for a joint model). This setup
is similar to leave-one-out classification. The clas-
sifier is learned using binary word ngram features
extracted from user or user-neighbor communi-
cations. We prefer binary to normalized count-
based features to overcome sparsity issues caused
by making predictions on each tweet individually.

6 Static Classification Results

6.1 Modeling User Content Influence

We investigate classification decision probabilities
for our static user model Φvi by making predic-
tions on a random set of 5 vs. 100 tweets per user.
To our knowledge only limited work on personal

0 20 40 60 80 100

0.
0

0.
2

0.
4

0.
6

0.
8

1.
0

User

C
la

ss
ifi

ca
tio

n 
de

ci
si

on
 (p

ro
ba

bi
lit

y)

misclassified

misclassifiedcorrect

correct

Figure 3: Classification probabilities for Φvi estimated over
100 users in Gcand tested on 5 (blue) vs. 100 (green) tweets
per user where Republican = 1, Democratic = 0, filled mark-
ers = correctly classified, not filled = misclassified users.

●

●
● ●

● ●
●

5 10 20 50 100 200

0.
50

0.
55

0.
60

0.
65

0.
70

log(Tweets Per Neighbor)

A
cc

ur
ac

y

10 50 100 200 400

Friend
Follower
Hashtag
Usermention

Retweet
Reply
User

●
●

●
●

●
● ●

10 50 100 200 400

Friend
Follower
Hashtag
Usermention

Retweet
Reply
User

●

● ●
●

●

●
●

10 50 100 200 400

Friend
Follower
Hashtag
Usermention

Retweet
Reply
User

●

●
●

●

●
●

●

10 50 100 200 400

Friend
Follower
Hashtag
Usermention

Retweet
Reply
User

● ● ●

● ●
●

●

10 50 100 200 400

Friend
Follower
Hashtag
Usermention

Retweet
Reply
User

●
●

●
● ●

●
●

10 50 100 200 400

Friend
Follower
Hashtag
Usermention

Retweet
Reply
User

(a) Ggeo: 2 neighbors

●

●
● ●

● ● ●

5 10 20 50 100 200

0.
50

0.
55

0.
60

0.
65

0.
70

log(Tweets Per Neighbor)

A
cc

ur
ac

y

50 100 250 500 1000 2000

Friend
Follower
Hashtag
Usermention

Retweet
Reply
User

●
●

● ● ● ● ●

50 100 250 500 1000 2000

Friend
Follower
Hashtag
Usermention

Retweet
Reply
User

●

●
● ●

● ●

●

50 100 250 500 1000 2000

Friend
Follower
Hashtag
Usermention

Retweet
Reply
User

●

●
●

●
●

●
●

50 100 250 500 1000 2000

Friend
Follower
Hashtag
Usermention

Retweet
Reply
User

●

●
●

●
●

● ●

50 100 250 500 1000 2000

Friend
Follower
Hashtag
Usermention

Retweet
Reply
User

●

●
● ●

●
●

●

50 100 250 500 1000 2000

Friend
Follower
Hashtag
Usermention

Retweet
Reply
User

(b) Ggeo: 10 neighbors

●

●

●

●

●

●
●

5 10 20 50 100 200

0.
50

0.
55

0.
60

0.
65

0.
70

0.
75

log(Tweets Per Neighbor)

A
cc

ur
ac

y

10 50 100 200 400

Friend
Follower
Hashtag
Usermention

Retweet
Reply
User

●

●

●

●

●

●
●

10 50 100 200 400

Friend
Follower
Hashtag
Usermention

Retweet
Reply
User

●

●

●
●

●
●

●

10 50 100 200 400

Friend
Follower
Hashtag
Usermention

Retweet
Reply
User

●

●

●

●

●

●
●

10 50 100 200 400

Friend
Follower
Hashtag
Usermention

Retweet
Reply
User

●

●

●

●

●

●
●

10 50 100 200 400

Friend
Follower
Hashtag
Usermention

Retweet
Reply
User

●

●

●

●

●

●
●

10 50 100 200 400

Friend
Follower
Hashtag
Usermention

Retweet
Reply
User

(c) Gcand: 2 neighbors

●

●

●

●

●
●

●

5 10 20 50 100 200

0.
50

0.
55

0.
60

0.
65

0.
70

0.
75

log(Tweets Per Neighbor)

A
cc

ur
ac

y

50 100 250 500 1000 2000

Friend
Follower
Hashtag
Usermention

Retweet
Reply
User

●

●

●

●

●

●
●

50 100 250 500 1000 2000

Friend
Follower
Hashtag
Usermention

Retweet
Reply
User

●

● ●
●

● ●
●

50 100 250 500 1000 2000

Friend
Follower
Hashtag
Usermention

Retweet
Reply
User

●

●

●

●

●

●
●

50 100 250 500 1000 2000

Friend
Follower
Hashtag
Usermention

Retweet
Reply
User

●

●

●

●

●
●

●

50 100 250 500 1000 2000

Friend
Follower
Hashtag
Usermention

Retweet
Reply
User

●

●

●

●

●

●
●

50 100 250 500 1000 2000

Friend
Follower
Hashtag
Usermention

Retweet
Reply
User

(d) Gcand: 10 neighbors

Figure 4: Modeling the influence of the number of tweets per
neighbor t=[5, .., 200] for Gcand and Ggeo graphs.

analytics (Burger et al., 2011; Van Durme, 2012b)
have performed this straight-forward comparison.
For that purpose, we take a random partition con-
taining 100 users ofGcand graph and perform four
independent classification experiments – two runs
using 5 and two runs using 100 tweets per user.

Figure 3 demonstrates that more tweets during
prediction time lead to higher accuracy by show-
ing that more users with 100 tweets are correctly
classified e.g., filled green markers in the right up-
per quadrant are true Republicans and in the left
lower quadrant are true Democrats. Moreover, a
lot of users with 100 tweets are close to 0.5 deci-
sion probability which suggests that the classifier
is just uncertain rather then being completely off,
e.g., misclassified Republican users with 5 tweets
(not filled blue markers in the right lower quad-
rant) are close to 0. These results follow natu-
rally from the underlying feature representation:
having more tweets per user leads to a lower vari-
ance estimate of a target multinomial distribution.
The more robustly this distribution is estimated
(based on having more tweets) the more confident
we should be in the classifier output.

6.2 Modeling Neighbor Content Influence

Here we discuss the results for our static neighbor-
hood model. We study the influence of the neigh-
borhood type r and size in terms of the number of
neighbors n and tweets t per neighbor.

190



●

●

●

●

1 2 5 10

0.
50

0.
55

0.
60

0.
65

0.
70

0.
75

log(Number of Neighbors)

A
cc

ur
ac

y
5 10 25 50

Friend
Follower
Hashtag

Usermention
Retweet
Reply

●

●

●

●

5 10 25 50

Friend
Follower
Hashtag

Usermention
Retweet
Reply

●

●

●

●

5 10 25 50

Friend
Follower
Hashtag

Usermention
Retweet
Reply

●

●

●

●

5 10 25 50

Friend
Follower
Hashtag

Usermention
Retweet
Reply

●

●

●

●

5 10 25 50

Friend
Follower
Hashtag

Usermention
Retweet
Reply

●

●

●

●

5 10 25 50

Friend
Follower
Hashtag

Usermention
Retweet
Reply

(a) Gcand: 5 tweets

●

●

●

●

1 2 5 10

0.
50

0.
55

0.
60

0.
65

0.
70

0.
75

log(Number of Neighbors)

A
cc

ur
ac

y

200 400 1000 2000

Friend
Follower
Hashtag

Usermention
Retweet
Reply

●

●

●

●

200 400 1000 2000

Friend
Follower
Hashtag

Usermention
Retweet
Reply

●

●

●

●

200 400 1000 2000

Friend
Follower
Hashtag

Usermention
Retweet
Reply

●

●

●

●

200 400 1000 2000

Friend
Follower
Hashtag

Usermention
Retweet
Reply

●

●

●

●

200 400 1000 2000

Friend
Follower
Hashtag

Usermention
Retweet
Reply

●

●

●

●

200 400 1000 2000

Friend
Follower
Hashtag

Usermention
Retweet
Reply

(b) Gcand: 200 tweets

●

●

●

●

1 2 5 10

0.
50

0.
55

0.
60

0.
65

0.
70

log(Number of Neighbors)

A
cc

ur
ac

y

5 10 25 50

Friend
Follower
Hashtag

Usermention
Retweet
Reply

●

●

●

●

5 10 25 50

Friend
Follower
Hashtag

Usermention
Retweet
Reply

●

●

●

●

5 10 25 50

Friend
Follower
Hashtag

Usermention
Retweet
Reply

●

●

●

●

5 10 25 50

Friend
Follower
Hashtag

Usermention
Retweet
Reply

●

●

●

●

5 10 25 50

Friend
Follower
Hashtag

Usermention
Retweet
Reply

●

●

● ●

5 10 25 50

Friend
Follower
Hashtag

Usermention
Retweet
Reply

(c) Ggeo: 5 tweets

●

●

●

●

1 2 5 10

0.
50

0.
55

0.
60

0.
65

0.
70

log(Number of Neighbors)

A
cc

ur
ac

y

200 400 1000 2000

Friend
Follower
Hashtag

Usermention
Retweet
Reply

●

●

● ●

200 400 1000 2000

Friend
Follower
Hashtag

Usermention
Retweet
Reply

●

●

●

●

200 400 1000 2000

Friend
Follower
Hashtag

Usermention
Retweet
Reply

●

●

●

●

200 400 1000 2000

Friend
Follower
Hashtag

Usermention
Retweet
Reply

●

●

●

●

200 400 1000 2000

Friend
Follower
Hashtag

Usermention
Retweet
Reply

●

●

●
●

200 400 1000 2000

Friend
Follower
Hashtag

Usermention
Retweet
Reply

(d) Ggeo: 200 tweets

Figure 5: Modeling the influence of the number of neighbors
per user n=[1, .., 10] for Gcand and Ggeo graphs.

In Figure 4 we present accuracy results for
Gcand and Ggeo graphs. Following Eq.3 and 4, we
spent an equal amount of resources to obtain 100
user tweets and 10 tweets from 10 neighbors. We
annotate these ‘points of equal number of commu-
nications’ with a line on top marked with a corre-
sponding number of user tweets.

We show that three of six social circles – friend,
retweet and user-mention yield better accuracy
compared to the user model for all graphs when
t ≥ 250. Thus, for effectively classifying a given
user vi it is better to take 200 tweets each from 10
neighbors rather than 2,000 tweets from the user.

The best accuracy for Gcand is 0.75 for friend,
follower, retweet and user-mention neighborhoods
which is 0.03 higher than the user baseline; for
Ggeo is 0.67 for user-mention and 0.64 for retweet
circles compared to 0.57 for the user model; for
GZLR is 0.863 for retweet and 0.849 for friend
circles which is 0.11 higher that the user baseline.
Finally, similarly to the results for the user model
given in Figure 3, increasing the number of tweets
per neighbor from 5 to 200 leads to a significant
gain in performance for all neighborhood types.

6.3 Modeling Neighborhood Size

In Figure 5 we present accuracy results to show
neighborhood size influence on classification per-
formance for Ggeo and Gcand graphs. Our re-
sults demonstrate that even small changes to the

neighborhood size n lead to better performance
which does not support the claims by Zamal et al.
(2012). We demonstrate that increasing the size
of the neighborhood leads to better performance
across six neighborhood types. Friend, user men-
tion and retweet neighborhoods yield the highest
accuracy for all graphs. We observe that when the
number of neighbors is n = 1, the difference in
accuracy across all neighborhood types is less sig-
nificant but for n ≥ 2 it becomes more significant.

7 Streaming Classification Results

7.1 Modeling Dynamic Posterior Updates
from a User Stream

Figures 6a and 6b demonstrate dynamic user
model prediction results averaged over users from
Gcand and GZLR graphs. Each figure outlines
changes in sequential average probability esti-
mates pµ(R | T ) for each individual self-authored
tweet tk as defined in Eq. 6. The average proba-
bility estimates pµ(R | T ) are reported for every 5
tweets in a stream T = (t1, . . . tk) as

∑
nP (R|tk)
n ,

where n is the total number of users with the same
attribute R or D. We represent pµ(R | T ) as a
box and whisker plot with the median, lower and
upper quantiles to show the variance; the length of
whiskers indicate lower and upper extreme values.

We find similar behavior across all three graphs.
In particular, the posterior estimates converge
faster when predicting Democratic than Republi-
can users but it has been trained on an equal num-
ber of tweets per class. We observe that average
posterior estimates Pµ(R | T ) converge faster to 0

0.5

0.6

0.7

0.8

0.9

1.0

0 20 40 60

p(
Re
pu
bl
ic
an
|T
)

0.0

0.1

0.2

0.3

0.4

0.5

0 20 40 60
Tweet Stream (T)

p(
Re
pu
bl
ic
an
|T
)

(a) User Gcand

0.5

0.6

0.7

0.8

0.9

1.0

0 50 100 150

p(
Re
pu
bl
ic
an
|T
)

0.0

0.1

0.2

0.3

0.4

0.5

0 50 100 150
Tweet Stream (T)

p(
Re
pu
bl
ic
an
|T
)

(b) User GZLR

Figure 6: Streaming classification results from user commu-
nications for Gcand and GZLR graphs averaged over every 5
tweets (red - Republican, blue - Democratic).

191



300

400

500

0 5 10 15
Time in Weeks

U
se
rs

(a) User Gcand

0

100

200

0 20 40 60 80
Time in Weeks

U
se
rs

(b) User GZLR

300

400

500

0 1 2 3 4 5
Time in Weeks

U
se
rs

(c) User-Neigh Gcand

0

100

200

0 10 20 30 40
Time in Weeks

U
se
rs

(d) User-Neigh GZLR

Figure 7: Time needed for (a) - (b) dynamic user model and
(c) - (d) joint user-neighbor model to infer political prefer-
ences of Democratic (blue) and Republican (red) users at
75% (dotted line) and 95% (solid line) accuracy levels.

(Democratic) than to 1 (Republican) in Figures 6a
and 6b. It suggests that language of Democrats is
more expressive of their political preference than
language of Republicans. For example, frequent
politically influenced terms used widely by Demo-
cratic users include faith4liberty, constitutionally,
pass, vote2012, terroristic.

The variance for average posterior estimates
decreases when the number of tweets increases
for all three datasets. Moreover, we detect that
Pµ(R|T ) estimates for users in Gcand converge 2-
3 times faster in terms of number of tweets than
for users in GZLR. The lowest convergence is de-
tected for Ggeo where after tk = 250 tweets the
average posterior estimate Pµ(R | tk) = 0.904 ±
0.044 and Pµ(D | tk) = 0.861 ± 0.008. It means
that users inGcand are more politically vocal com-
pared to users in GZLR and Ggeo. As a result,
less active users in Ggeo just need more than 250
tweets to converge to a true 0 or 1 class. These re-
sults are coherent with the outcomes for our static
models shown in Figures 4 and 5. These findings
further confirm that differences in performance are
caused by various biases present in the data due to
distinct sampling and annotation approaches.

Figure 7a and 7b illustrate the amount of time
required for the user model to infer political pref-
erences estimated for 1,031 users inGcand and 371
users inGZLR. The amount of time needed can be
evaluated for different accuracy levels e.g., 0.75
and 0.95. Thus, with 75% accuracy we classify:
• 100 (∼20%) Republican users in 3.6 hours

and Democratic users in 2.2 hours for Gcand;
• 100 (∼56%) R users in 20 weeks and 100

(∼52%) D users in 8.9 weeks for GZLR
which is 800 times longer that for Gcand;
• 100 (∼75%) R users in 12 weeks and 80

(∼60%) D users in 19 weeks for Ggeo.
Such extreme divergences in the amount of time

required for classification across all graphs should
be of strong interest to researchers concerned with
latent attribute prediction tasks because Twitter
users produce messages with extremely different
frequencies. In our case, users in GZLR tweet ap-
proximately 800 times less frequently than users
in Gcand.

7.2 Modeling Dynamic Posterior Updates
from a Joint User-Neighbor Stream

We estimate dynamic posterior updates from a
joint stream of user and neighbor communications
in Ggeo, Gcand and GZLR graphs. To make a fair
comparison with a streaming user model, we start
with the same user tweet t0(vi). Then instead of
waiting for the next user tweet we rely on any
neighbor tweets that appear until the user produces
the next tweet t1(vi). We rely on communications
from four types of neighbors such as friends, fol-
lowers, retweets and user mentions.

The convergence rate for the average posterior
probability estimates Pµ(R|T ) depending on the
number of tweets is similar to the user model re-
sults presented in Figure 6. However, for Ggeo
the variance for Pµ(R|T ) is higher for Democratic
users; for GZLR Pµ(R|T ) → 1 for Republicans
in less than 110 tweets which is ∆t = 40 tweets
faster than the user model; for Gcand the conver-
gence for both Pµ(R|T ) → 1 and Pµ(D|T ) → 0
is not significantly different than the user model.

Figures 7c and 7d show the amount of time re-
quired for a joint user-neighbor model to infer po-
litical preferences estimated for users inGcand and
GZLR. We find that with 75% accuracy we can
classify 100 users for:
• Gcand: Republican users in 23 minutes and

Democratic users in 10 minutes;
• GZLR: R users in 3.2 weeks and D users in

1.1 weeks which is 7 times faster on average
across attributes than for the user model;
• Ggeo: R users in 1.2 weeks and D users in

3.5 weeks which is on average 6 times faster
across attributes than for the user model.

Similar or better Pµ(R|T ) convergence in terms
of the number of tweets and, especially, in the
amount of time needed for user and user-neighbor

192



models further confirms that neighborhood con-
tent is useful for political preference prediction.
Moreover, communications from a joint stream al-
low to make an inference up to 7 times faster.

8 Related Work

Supervised Batch Approaches The vast major-
ity of work on predicting latent user attributes in
social media apply supervised static SVM mod-
els for discrete categorical e.g., gender and re-
gression models for continuous attributes e.g., age
with lexical bag-of-word features for classifying
user gender (Garera and Yarowsky, 2009; Rao et
al., 2010; Burger et al., 2011; Van Durme, 2012b),
age (Rao et al., 2010; Nguyen et al., 2011; Nguyen
et al., 2013) or political orientation. We present an
overview of the existing models for political pref-
erence prediction in Table 1.

Bergsma et al. (2012) following up on Rao’s
work (2010) on adding socio-linguistic features
to improve gender, ethnicity and political prefer-
ence prediction show that incorporating stylistic
and syntactic information to the bag-of-word fea-
tures improves gender classification.

Other methods characterize Twitter users by ap-
plying limited amounts of network structure in-
formation in addition to lexical features. Con-

Approach Users Tweets Features Accur.

Rao et al.
(2010) 1K 2M

ngrams
socio-ling

stacked

0.824
0.634
0.809

Pennacchiotti
and Popescu

(2011a)
10.3K –

ling-all
soc-all

full

0.770
0.863
0.889

Conover et
al. (2011) 1,000 1M

full-text
hashtags
clusters

0.792
0.908
0.949

Zamal et al.
(2012) 400

400K
3.85M
4.25M

UserOnly
Nbr

User-Nbr11

0.890
0.920
0.932

Cohen and
Ruths
(2013)

397
1.8K
262
196

397K
1.8M
262K
196K

features
from (Za-
mal et al.,

2012)

0.910
0.840
0.680
0.870

This paper
(batch clas-
sification)

Gcand
1,031
Ggeo
270
GZLR
371

206K
2M
54K

540K
371K
1.5M

user ngrams
neighbor

user ngrams
neighbor

user ngrams
neighbor

0.720
0.750
0.570
0.670
0.886
0.920

This paper
(dynamic
Bayesian

update clas-
sification)

Gcand
1,031
Ggeo
270
GZLR
371

103K
130K
54K
67K
74K

185K

user stream
user-neigh.
user stream
user-neigh.
user stream
user-neigh.

0.995
0.999
0.843
0.882
0.892
0.999

Table 1: Overview of the existing approaches for political
preference classification in Twitter.

nover et al. (2011) rely on identifying strong parti-
san clusters of Democratic and Republican users
in a Twitter network based on retweet and user
mention degree of connectivity, and then combine
this clustering information with the follower and
friend neighborhood size features. Pennacchiotti
et al. (2011a; 2011b) focus on user behavior, net-
work structure and linguistic features. Similar to
our work, they assume that users from a partic-
ular class tend to reply and retweet messages of
the users from the same class. We extend this as-
sumption and study other relationship types e.g.,
friends, user mentions etc. Recent work by Wong
et al. (2013) investigates tweeting and retweet-
ing behavior for political learning during 2012 US
Presidential election. The most similar work to
ours is by Zamal et al. (2012), where the authors
apply features from the tweets authored by a user’s
friend to infer attributes of that user. In this paper,
we study different types of user social circles in
addition to a friend network.

Additionally, using social media for mining po-
litical opinions (O’Connor et al., 2010a; May-
nard and Funk, 2012) or understanding socio-
political trends and voting outcomes (Tumasjan
et al., 2010; Gayo-Avello, 2012; Lampos et al.,
2013) is becoming a common practice. For in-
stance, Lampos et al. (2013) propose a bilinear
user-centric model for predicting voting intentions
in the UK and Australia from social media data.
Other works explore political blogs to predict what
content will get the most comments (Yano et al.,
2013) or analyze communications from Capitol
Hill12 to predict campaign contributors based on
this content (Yano and Smith, 2013).

Unsupervised Batch Approaches Bergsma et
al. (2013) show that large-scale clustering of user
names improves gender, ethnicity and location
classification on Twitter. O’Connor et al. (2010b)
following the work by Eisenstein (2010) propose
a Bayesian generative model to discover demo-
graphic language variations in Twitter. Rao et
al. (2011) suggest a hierarchical Bayesian model
which takes advantage of user name morphology
for predicting user gender and ethnicity. Golbeck
et al. (2010) incorporate Twitter data in a spatial
model of political ideology.

Streaming Approaches Van Durme (2012b)
proposed streaming models to predict user gen-
der in Twitter. Other works suggested to process

12http://www.tweetcongress.org

193



text streams for a variety of NLP tasks e.g., real-
time opinion mining and sentiment analysis in so-
cial media (Pang and Lee, 2008), named entity
disambiguation (Sarmento et al., 2009), statistical
machine translation (Levenberg et al., 2011), first
story detection (Petrović et al., 2010), and unsu-
pervised dependency parsing (Goyal and Daumé,
2011). Massive Online Analysis (MOA) toolkit
developed by Bifet et al. (2010) is an alternative to
the Jerboa package used in this work developed
by Van Durme (2012a). MOA has been effec-
tively used to detect sentiment changes in Twitter
streams (Bifet et al., 2011).

9 Conclusions and Future Work

In this paper, we extensively examined state-of-
the-art static approaches and proposed novel mod-
els with dynamic Bayesian updates for streaming
personal analytics on Twitter. Because our stream-
ing models rely on communications from Twitter
users and content from various notions of user-
local neighborhood they can be effectively applied
to real-time dynamic data streams. Our results
support several key findings listed below.

Neighborhood content is useful for personal
analytics. Content extracted from various notions
of a user-local neighborhood can be as effective
or more effective for political preference classifi-
cation than user self-authored content. This may
be an effect of ‘sparseness’ of relevant user data,
in that users talk about politics very sporadically
compared to a random sample of their neighbors.

Substantial signal for political preference
prediction is distributed in the neighborhood.
Querying for more neighbors per user is more ben-
eficial than querying for extra content from the
existing neighbors e.g., 5 tweets from 10 neigh-
bors leads to higher accuracy than 25 tweets from
2 neighbors or 50 tweets from 1 neighbor. This
may be also the effect of data heterogeneity in
social media compared to e.g., political debate
text (Thomas et al., 2006). These findings demon-
strate that a substantial signal is distributed over
the neighborhood content.

Neighborhoods constructed from friend,
user mention and retweet relationships are
most effective. Friend, user mention and retweet
neighborhoods show the best accuracy for predict-
ing political preferences of Twitter users. We think
that friend relationships are more effective than
e.g., follower relationships because it is very likely

that users share common interests and preferences
with their friends, e.g. Facebook friends can even
be used to predict a user’s credit score.13 User
mentions and retweets are two primary ways of in-
teraction on Twitter. They both allow to share in-
formation e.g., political news, events with others
and to be involved in direct communication e.g.,
live political discussions, political groups.

Streaming models are more effective than
batch models for personal analytics. The predic-
tions made using dynamic models with Bayesian
updates over user and joint user-neighbor commu-
nication streams demonstrate higher performance
with lower resources spent compared to the batch
models. Depending on user political involvement,
expressiveness and activeness, the perfect predic-
tion (approaching 100% accuracy) can be made
using only 100 - 500 tweets per user.

Generalization of the classifiers for political
preference prediction. This work raises a very
important but under-explored problem of the gen-
eralization of classifiers for personal analytics in
social media, also recently discussed by Cohen
and Ruth (2013). For instance, the existing models
developed for political preference prediction are
all trained on Twitter data but report significantly
different results even for the same baseline mod-
els trained using bag-of-word lexical features as
shown in Table 1. In this work we experiment with
three different datasets. Our results for both static
and dynamic models show that the accuracy in-
deed depends on the way the data was constructed.
Therefore, publicly available datasets need to be
released for a meaningful comparison of the ap-
proaches for personal analytics in social media.

In future work, we plan to incorporate itera-
tive model updates from newly classified com-
munications similar to online perceptron-style up-
dates. In addition, we aim to experiment with
neighborhood-specific classifiers applied towards
the tweets from neighborhood-specific streams
e.g., friend classifier used for friend tweets,
retweet classifier applied to retweet tweets etc.

Acknowledgments

The authors would like to thank the anonymous
reviewers for their helpful comments.

13http://money.cnn.com/2013/08/26/technology/social/
facebook-credit-score/

194



References
Shane Bergsma, Matt Post, and David Yarowsky. 2012.

Stylometric analysis of scientific articles. In Pro-
ceedings of the Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies (NAACL-
HLT), pages 327–337.

Shane Bergsma, Mark Dredze, Benjamin Van Durme,
Theresa Wilson, and David Yarowsky. 2013.
Broadly improving user classification via
communication-based name and location clus-
tering on Twitter. In Proceedings of the Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies (NAACL-HLT), pages 1010–1019.

Albert Bifet, Geoff Holmes, Bernhard Pfahringer,
Philipp Kranen, Hardy Kremer, Timm Jansen, and
Thomas Seidl. 2010. MOA: Massive online analy-
sis, a framework for stream classification and clus-
tering. Journal of Machine Learning Research,
11:44–50.

Albert Bifet, Geoffrey Holmes, Bernhard Pfahringer,
and Ricard Gavaldà. 2011. Detecting sentiment
change in Twitter streaming data. Journal of Ma-
chine Learning Research, 17:5–11.

John D. Burger, John Henderson, George Kim, and
Guido Zarrella. 2011. Discriminating gender on
Twitter. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 1301–1309.

Raviv Cohen and Derek Ruths. 2013. Classifying Po-
litical Orientation on Twitter: It’s Not Easy! In Pro-
ceedings of the International AAAI Conference on
Weblogs and Social Media (ICWSM), pages 91–99.

Michael D. Conover, Bruno Gonçalves, Jacob
Ratkiewicz, Alessandro Flammini, and Filippo
Menczer. 2011. Predicting the political alignment
of Twitter users. In Proceedings of Social Comput-
ing, pages 192–199.

Jacob Eisenstein, Brendan O’Connor, Noah A. Smith,
and Eric P. Xing. 2010. A latent variable model for
geographic lexical variation. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 1277–1287.

Rong En Fan, Kai Wei Chang, Cho Jui Hsieh, Xi-
ang Rui Wang, and Chih Jen Lin. 2008. LIBLIN-
EAR: A library for large linear classification. Jour-
nal of Machine Learning Research, 9:1871–1874.

Katja Filippova. 2012. User demographics and lan-
guage in an implicit social network. In Proceed-
ings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 1478–1488.

Nikesh Garera and David Yarowsky. 2009. Modeling
latent biographic attributes in conversational genres.
In Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
of the AFNLP, pages 710–718.

Daniel Gayo-Avello. 2012. No, you cannot predict
elections with Twitter. Internet Computing, IEEE,
16(6):91–94.

Jennifer Golbeck, Justin M. Grimes, and Anthony
Rogers. 2010. Twitter use by the u.s. congress.
Journal of the American Society for Information Sci-
ence and Technology, 61(8):1612–1621.

Amit Goyal and Hal Daumé, III. 2011. Approxi-
mate scalable bounded space sketch for large data
NLP. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 250–261.

Vasileios Lampos, Daniel Preotiuc-Pietro, and Trevor
Cohn. 2013. A user-centric model of voting inten-
tion from social media. In Proceedings of the Asso-
ciation for Computational Linguistics (ACL), pages
993–1003.

Abby Levenberg, Miles Osborne, and David Matthews.
2011. Multiple-stream language models for statis-
tical machine translation. In Proceedings of the
Sixth Workshop on Statistical Machine Translation
(WMT), pages 177–186.

Yang Liu, Qun Liu, and Shouxun Lin. 2005. Log-
linear models for word alignment. In Proceedings
of the Annual Meeting on Association for Computa-
tional Linguistics (ACL), pages 459–466.

Ingrid Lunden. 2012. Analyst: Twitter
passed 500M users in june 2012, 140m of
them in US; Jakarta ‘biggest tweeting’ city.
http://techcrunch.com/2012/07/30/analyst-twitter-
passed-500m-users-in-june-2012-140m-of-them-in-
us-jakarta-biggest-tweeting-city/.

Diana Maynard and Adam Funk. 2012. Automatic de-
tection of political opinions in tweets. In Proceed-
ings of the 8th International Conference on The Se-
mantic Web (ESWC), pages 88–99.

Felix Ming Fai Wong, Chee Wei Tan, Soumya Sen, and
Mung Chiang. 2013. Quantifying political leaning
from tweets and retweets. In Proceedings of the In-
ternational AAAI Conference on Weblogs and Social
Media (ICWSM).

Dong Nguyen, Noah A. Smith, and Carolyn P. Rosé.
2011. Author age prediction from text using lin-
ear regression. In Proceedings of the 5th ACL-
HLT Workshop on Language Technology for Cul-
tural Heritage, Social Sciences, and Humanities
(LaTeCH), pages 115–123.

195



Dong Nguyen, Rilana Gravel, Dolf Trieschnigg, and
Theo Meder. 2013. ”How old do you think I am?”
A study of language and age in Twitter. In Proceed-
ings of the AAAI Conference on Weblogs and Social
Media (ICWSM), pages 439–448.

Brendan O’Connor, Ramnath Balasubramanyan,
Bryan R. Routledge, and Noah A. Smith. 2010a.
From tweets to polls: Linking text sentiment to
public opinion time series. In Proceedings of the
International AAAI Conference on Weblogs and
Social Media (ICWSM), pages 122–129.

Brendan O’Connor, Jacob Eisenstein, Eric P. Xing, and
Noah A. Smith. 2010b. A mixture model of de-
mographic lexical variation. In Proceedings of the
NIPS Workshop on Machine Learning and Social
Computing.

Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations of Trends in Infor-
mation Retrieval, 2(1-2):1–135, January.

Marco Pennacchiotti and Ana-Maria Popescu. 2011a.
Democrats, republicans and starbucks afficionados:
user classification in twitter. In Proceedings of the
17th ACM SIGKDD Conference on Knowledge Dis-
covery and Data Mining (KDD), pages 430–438.

Marco Pennacchiotti and Ana Maria Popescu. 2011b.
A machine learning approach to Twitter user clas-
sification. In Proceedings of the International AAAI
Conference on Weblogs and Social Media (ICWSM),
pages 281–288.

Saša Petrović, Miles Osborne, and Victor Lavrenko.
2010. Streaming first story detection with appli-
cation to Twitter. In Proceedings of Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics (NAACL).

Hoifung Poon, Colin Cherry, and Kristina Toutanova.
2009. Unsupervised morphological segmentation
with log-linear models. In Proceedings of Human
Language Technologies: The Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 209–217.

Delip Rao, David Yarowsky, Abhishek Shreevats, and
Manaswi Gupta. 2010. Classifying latent user at-
tributes in Twitter. In Proceedings of the 2nd In-
ternational Workshop on Search and Mining User-
generated Contents (SMUC), pages 37–44.

Delip Rao, Michael Paul, Clay Fink, David Yarowsky,
Timothy Oates, and Glen Coppersmith. 2011. Hier-
archical Bayesian models for latent attribute detec-
tion in social media. In Proceedings of the Inter-
national AAAI Conference on Weblogs and Social
Media (ICWSM).

Luı́s Sarmento, Alexander Kehlenbeck, Eugénio
Oliveira, and Lyle Ungar. 2009. An approach to

web-scale named-entity disambiguation. In Pro-
ceedings of the 6th International Conference on Ma-
chine Learning and Data Mining in Pattern Recog-
nition (MLDM), pages 689–703.

Noah A. Smith. 2004. Log-linear models.

Craig Smith. 2013. May 2013 by the
numbers: 16 amazing Twitter stats.
http://expandedramblings.com/index.php/march-
2013-by-the-numbers-a-few-amazing-twitter-stats/.

Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get
out the vote: determining support or opposition from
congressional floor-debate transcripts. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing, pages 327–335.

A. Tumasjan, T. O. Sprenger, P. G. Sandner, and I. M.
Welpe. 2010. Predicting elections with Twitter:
What 140 characters reveal about political senti-
ment. In Proceedings of the International AAAI
Conference on Weblogs and Social Media, pages
178–185.

Benjamin Van Durme. 2012a. Jerboa: A toolkit for
randomized and streaming algorithms. Technical re-
port, Human Language Technology Center of Excel-
lence.

Benjamin Van Durme. 2012b. Streaming analysis of
discourse participants. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 48–58.

Yi Yang and Jacob Eisenstein. 2013. A log-linear
model for unsupervised text normalization. In Pro-
ceedings of Conference on Empirical Methods in
Natural Language Processing, pages 61–72.

Tao Yano and Noah A. Smith. 2013. What’s worthy
of comment? content and comment volume in po-
litical blogs. In International AAAI Conference on
Weblogs and Social Media (ICWSM).

Tao Yano, Dani Yogatama, and Noah A. Smith. 2013.
A penny for your tweets: Campaign contributions
and capitol hill microblogs. In Proceedings of the
International AAAI Conference on Weblogs and So-
cial Media (ICWSM).

Faiyaz Al Zamal, Wendy Liu, and Derek Ruths. 2012.
Homophily and latent attribute inference: Inferring
latent attributes of Twitter users from neighbors. In
Proceedings of the International AAAI Conference
on Weblogs and Social Media, pages 387–390.

196


