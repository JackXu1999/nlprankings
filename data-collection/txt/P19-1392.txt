



















































Pay Attention when you Pay the Bills. A Multilingual Corpus with Dependency-based and Semantic Annotation of Collocations.


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4012–4019
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

4012

Pay attention when you pay the bills. A multilingual corpus with
dependency-based and semantic annotation of collocations.

Marcos Garcia
Universidade da Coruña

Grupo LyS, Dpto. de Letras
Campus da Zapateira, Coruña

Marcos Garcı́a-Salido
Universidade da Coruña

Grupo LyS, Dpto. de Letras
Campus da Zapateira, Coruña

Susana Sotelo
Universidade da Coruña

Grupo LyS, Dpto. de Letras
Campus da Zapateira, Coruña

Estela Mosqueira
Universidade da Coruña

Grupo LyS, Dpto. de Letras
Campus da Zapateira, Coruña

{marcos.garcia.gonzalez,marcos.garcias
susana.sotelo,estela.mosqueira,margarita.alonso}udc.gal

Margarita Alonso-Ramos
Universidade da Coruña

Grupo LyS, Dpto. de Letras
Campus da Zapateira, Coruña

Abstract
This paper presents a multilingual corpus with
semantic annotation of collocations in En-
glish, Portuguese, and Spanish. The whole
resource contains 155k tokens and 1, 526
collocations labeled in context. The anno-
tated examples belong to three syntactic struc-
tures (adjective-noun, verb-object, and nomi-
nal compounds), and represent 60 lexical func-
tions in the Meaning-Text Theory (e.g., Oper,
Magn, Bon, etc.). Each collocation was anno-
tated by three linguists and the final resource
was revised by a team of experts. The resulting
corpora, which are freely available, can serve
as a basis to evaluate different approaches for
collocation identification and classification in
three languages, which in turn can be useful
for different NLP tasks such as natural lan-
guage generation or understanding.

1 Introduction

The automatic identification of collocations, as
well as other multiword expressions (MWEs),
is crucial for many natural language processing
(NLP) tasks, since their linguistic behaviour dif-
fers from other combinations of words (Mel’čuk,
1995; Sag et al., 2002; Ramisch and Villavicen-
cio, 2018). On the one hand, approaches to natural
language generation may take advantage of collo-
cational information to produce natural utterances
with the desired meanings (Wanner et al., 2010;
Lareau et al., 2011). In this regard, while different
adjectives such as heavy and strong can convey ba-
sically the same meaning (e.g., ‘intensification’ in
heavy load and in strong fragrance), great has dif-
ferent senses in great loss and in great time (with
‘intensification’ and ‘positive’ meanings, respec-
tively). On the other hand, to interpret the meaning
of a sentence, a system should take into account

the properties of these expressions: for instance,
the meaning of the verb [to] take in the colloca-
tion take [a] cab is different from the same verb
in a free combination such as take [a] pencil, so
natural language understanding or abstract mean-
ing representation systems could benefit from the
correct identification of collocations (Bonial et al.,
2014; O’Gorman et al., 2018). It is worth mention-
ing that collocations are pervasive and frequent in
all domains and text typologies, so their correct
interpretation should be critical to progress in the
automatic processing of natural languages.

The concept of collocation was formalized in
the Meaning-Text Theory as a combination of two
lexical units (LUs) where one of them (the BASE,
e.g., attention in the collocation pay attention) is
freely selected due to its meaning, while the selec-
tion of the other one (the COLLOCATE, e.g., [to]
pay) is restricted by the former (Mel’čuk, 1995).
Under this theory, lexical functions (LF) repre-
sent a relation between a LU (the base) and a set
of expressions (the potential collocates) (Mel’čuk,
1996, 1998; Wanner, 1996). For instance, the
LF Oper means ‘to carry out’, so we could de-
fine Oper(picture)=[to] take to formalize the col-
location take a picture. Similarly, the adjective–
noun collocation loud screech can be represented
as Magn(screech)=loud, where the lexical func-
tion Magn denotes ‘intensification’.

The automatic identification of collocations has
deserved a substantial number of works of dif-
ferent researchers from NLP and computational
linguistics as well as from lexicography and cor-
pus linguistics (Evert, 2008; Pecina, 2010; Gries,
2013). Most approaches rely on statistical associa-
tion measures (AMs), both symmetrical and direc-
tional, and recent works incorporate distributional



4013

semantics to automatically identify the collocate
of a given base and LF, or to classify the compo-
sitionality of MWEs including collocations (Wan-
ner et al., 2006; Carlini et al., 2014; Rodrı́guez-
Fernández et al., 2016; Cordeiro et al., 2019). To
evaluate the extraction, some researchers use man-
ual selection of true collocations from ranked lists,
while others take advantage of examples extracted
from collocation dictionaries. However, most of
these approaches are carried out only in one lan-
guage, and they do not always permit to obtain
precise recall values. Moreover, they usually do
not include semantic information.

Taking the above into account, this paper at-
tempts to fill this gap by releasing a freely avail-
able multilingual corpus of English, Portuguese,
and Spanish with manual annotation of colloca-
tions and their lexical functions. The whole re-
source, annotated by five experts, has more than
155k tokens and 1, 526 collocations classified into
60 lexical functions. For each language, we pro-
vide both the labeled data of each annotator as well
as the gold-standard data.1

2 Related Work

Different statistical methods have been applied
to automatically identify and classify collocations
from corpora. Studies such as Wanner et al.
(2006), Wanner et al. (2016), or Gelbukh and
Kolesnikova (2010) train statistical models using
Spanish data (from EuroWordNet, from the DiCE
dictionary, and using a Spanish corpus, respec-
tively). For French, Fonseca et al. (2017) explore
the combination of dependency parsing with a lex-
ical network based on lexical functions.

The semantic classification of base-collocate
pairs allowed for implementing multilingual nat-
ural language generation systems which take ad-
vantage of lexical functions to select the most ap-
propriate combinations for each context (Wanner
et al., 2010). In this regard, Lareau et al. (2011)
propose a methodology to use lexical functions in
Lexical Functional Grammar.

With respect to the extraction process, there
have been a large number of studies focusing on
the automatic identification of collocations in cor-
pora. In this regard, most approaches have relied
on statistical association measures applied both to
windows of n-grams (Church and Hanks, 1990;

1The corpora are freely available at the following url:
http://www.grupolys.org/˜marcos/pub/collocations.zip

Smadja, 1993; Pecina, 2010), and to syntax-based
dependency triples (Seretan, 2011; Carlini et al.,
2014; Garcia et al., 2017; Uhrig et al., 2018). In
Rodrı́guez-Fernández et al. (2016) it is presented
a method to retrieve potential collocates of a given
LF and a base. Other studies address the identifi-
cation process as a classification problem. Karan
et al. (2012) take advantage of a set of true positive
and true negative collocations to evaluate machine
learning algorithms which use, among others, fea-
tures based on association values.

To evaluate such methods, some authors carry
out a manual review of the n best combinations of
candidate collocations lists, ranked by a given AM
(Seretan and Wehrli, 2006; Garcia, 2018). A dif-
ferent approach consists of collecting an inventory
of true collocations (usually from existing dictio-
naries), which is then used to compare the perfor-
mance of various AMs (Evert and Krenn, 2001;
Pearce, 2002; Pecina, 2010; Kilgarriff et al., 2014;
Evert et al., 2017). Concerning the available data
with collocational information, it is worth noting
that a vast majority of the resources are dictionar-
ies and lexicons often targeted at language learn-
ers (Benson et al., 1986; Alonso-Ramos et al.,
2010). From a different perspective, initiatives
such as PropBank and abstract meaning represen-
tation also provide corpora with semantic annota-
tion of MWEs, some of which may be considered
collocations (Banarescu et al., 2013; Bonial et al.,
2014; O’Gorman et al., 2018).

The approach to evaluate the process of col-
location extraction proposed here consists of us-
ing a gold-standard corpus with manual annota-
tion of such expressions. On the one hand, this
allows for accurate precision and recall values to
be obtained, also taking into account ambiguous
combinations which may be collocations or not
depending on the context. On the other hand, a
gold-standard enables the research community to
evaluate different strategies in a more comparable
way. In this regard, the 2017 and 2018 PARSEME
Shared Tasks released multilingual corpora with
annotation of verbal MWEs (Savary et al., 2017;
Ramisch et al., 2018). Even if the initial objec-
tives of these shared tasks differ from ours (they
annotate idioms, verb-particle constructions and
other non-collocation MWEs), some of the units
actually intersect with the expressions we want to
identify. Thus, we rely on these corpora to initi-
ate the construction of a multilingual corpus with

http://www.grupolys.org/~marcos/pub/collocations.zip


4014

dependency-based and semantic annotation of col-
locations.

3 Source Data and Annotation

This section describes both the source data used to
build our multilingual corpora as well as the anno-
tation guidelines and procedure.

3.1 Corpora
We decided to take advantage of three subcorpora
of the edition 1.1 of the PARSEME Shared Task,
which include annotation of different verbal multi-
word expressions in 20 languages (Ramisch et al.,
2018). Since we understand collocations as lex-
ically restricted combinations of words, some of
the MWEs annotated in the PARSEME corpora
are also useful for our objectives (see Section 3.2).

Our main purpose is to provide datasets to eval-
uate unsupervised strategies for extracting collo-
cations, so we selected the test datasets for Por-
tuguese (58k tokens) and Spanish (39k tokens),
and the train corpus for English (53k tokens), be-
cause the test data for this language are fewer.
These corpora are annotated with Universal De-
pendencies (Nivre, 2015) and released in .cupt for-
mat2 (an extension of .conllu).3

3.2 Annotation Guidelines
In general, our annotation follows Mel’čuk (1996)
with specific guidelines for each collocation
type. Also, we tried to be compatible with the
PARSEME principles with a view to combine both
annotations. As our strategy relies on dependency
analysis to obtain candidate combinations (which
are subsequently revised), we defined annotation
guidelines for three syntactic patterns of colloca-
tions (for each pattern, a set of tests for identify-
ing collocations was included). In the following
we present some examples of the most productive
lexical functions in each pattern (see Appendix A
for the whole list of LFs):

Adjective-noun (amod): collocations where the
adjective has a function of intensification and at-
tenuation (Magn: high priority, or AntiMagn:
weak resource), expresses a positive or negative
consideration from the speaker (Bon: great event,
AntiBon: unfortunate mistake), or conveys a spe-
cific sense (NonStandard) in combination with
the noun (e.g., dark sorcerer) (Mel’čuk, 1996).

2
http://multiword.sourceforge.net/cupt-format/

3
https://universaldependencies.org/format.html

Figure 1: Annotation workflow.

Nominal compounds (nmod and compound):
nominal compounds where the head of the relation
may express the notion of ‘head of a collective’
(Cap: police chief ), ‘a part of’ (Sing: member
[of the] government), or of a ‘group’ or the ‘total-
ity’ of the dependent (Mult: wolf pack).

Verb-object (obj): verb-object collocations oc-
cur between a predicative noun (Polguère, 2011)
depending of a verb which either do not contribute
to the meaning of the combination (Oper: [to]
have breakfast), or express causation (CausOper,
[to] cause damage) or a specific meaning in com-
bination with the base (NonStandard: [to] shake
hands). As some of these types were covered by
PARSEME (labeled as light verb constructions),
we revised each case and added their LFs. Some
structures such as verb-obj candidates occurring
in passive voice as subjects (e.g., [the] damage
was caused) or relative constructions (which do
not have a direct dependency between the lexical
base and the collocate) were not extracted.

A simplified version of the guidelines
can be accessed at the following url:
http://www.grupolys.org/˜marcos/
collocations/guidelines.html.

3.3 Annotation Procedure
In order to facilitate the labeling by each annota-
tor as well as to speed up the whole process we
defined the following procedure (see Figure 1):

We start by extracting the instances of the de-
sired relations (amod, nmod, compound and obj)
from the referred corpora, and arrange them into
triples (base;collocate;relation). Despite the fact
that most collocation extraction approaches set up
a frequency threshold to avoid noisy and less fre-
quent combinations, we revised every single in-
stance of each dependency relation.

Then, for each language and collocation pattern,

http://multiword.sourceforge.net/cupt-format/
https://universaldependencies.org/format.html
http://www.grupolys.org/~marcos/collocations/guidelines.html
http://www.grupolys.org/~marcos/collocations/guidelines.html


4015

Figure 2: Example of an annotation sheet (top) and HTML view (bottom) of the examples.

we created a sheet including the triples, a link to
an automatically generated HTML site with exam-
ples from the corpora, and annotation fields (see
Figure 2). Each collocation candidate was revised
by three experts (native or near-native speakers of
the target languages) which classified them as col-
location, non-collocation, or doubt.4 Doubts in-
clude (i) combinations which are collocations in
some examples but not in others, (ii) collocations
which include internal MWEs (e.g., verb-particle
constructions), and consequently they need a spe-
cific annotation, and (iii) cases in which the anno-
tator is not sure about the collocational status of
the candidate. After that, we created a final sheet
including the most frequent classification for each
candidate. Those cases in which there is no agree-
ment (i.e., each annotator selected a different la-
bel, or there is more than one doubt) were also
marked as doubt. The dubious cases in these fi-
nal sheets were revised in common by the whole
team of language experts, who decided on the LF
of each collocation.

Finally, we automatically transferred the anno-
tation to a new version of the initial corpora, and
convert it to the .tsv format required by WebAnno
(Eckart de Castilho et al., 2016). Using this tool,
we corrected those special cases (MWEs bases and
collocates, combinations which are collocations
only in some contexts, etc.) and performed a gen-
eral revision of the corpus. At the end of this pro-
cess, we generated the final corpus in .conllu for-
mat using the original resources and the .tsv files.

It is worth mentioning that we did not perform
a systematic evaluation of the syntactic analysis

4Light verb constructions already annotated in the original
corpus were initially marked as doubt, so each annotator also
revised again these cases.

id token h dep collocational information
1 He 2 nsubj
2 took 0 root 1
3 a 5 det
4 deep 5 amod 2
5 breath 2 obj 1:obj Oper1;2:amod Magn

Figure 3: Example of two annotated collocations (in
the last column) with the base breath in a simpli-
fied .conllu format (where h refers to the id of the
syntactic head, and dep to the dependency relation):
Oper1(breath)=[to] take, Magn(breath)=deep.

of each corpus. In this respect, we could miss
some true collocations incorrectly labeled with a
wrong dependency relation. However, the anno-
tated cases were manually checked, and therefore
they have a correct syntactic analysis (except for
human errors).

This resulting corpus contains the collocational
annotation in the last column of the .conllu file
(see an example in Figure 3). On the one hand,
the base of each collocation has a numerical id
followed by the syntactic pattern (e.g., obj, amod)
and by its lexical function. On the other hand, the
collocate is labeled with the same id as the base it
depends on. In blended collocations (as in the ex-
ample), the base contains information about both
combinations separated by a semicolon.

4 Final Resources and Results

The final multilingual corpus has 155, 794 tokens
and 1, 526 annotated collocations (1, 394 unique)
Table 1 includes the number of revised candidates
and annotated collocations for each language and
dependency relation. As expected, adjective-noun
and verb-object collocations were the most pro-



4016

Pattern English Portuguese Spanish Total
Cand. Colloc. Cand. Colloc. Cand. Colloc. Cand. Colloc.

amod 1,841 272 1,540 199 1,557 149 4,938 620
nmod 813 28 1,529 91 1,235 76 3,577 195

obj 1,495 184 1,572 250 914 145 3,981 579
Total 4,149 484 4,641 540 3,706 370 12,496 1,394

Table 1: Collocation candidates and unique annotated combinations per language and dependency pattern.

Pattern English Port. Spa. Total
amod 0.548 0.482 0.537 0.526
nmod 0.400 0.388 0.330 0.370

obj 0.541 0.706 0.630 0.632
Total 0.532 0.566 0.571 0.541

Table 2: Multi-k inter-annotator agreement per depen-
dency pattern and language.

ductive ones, and nominal compounds combina-
tions were less frequent.

We computed multi-k inter-annotator agree-
ment (Davies and Fleiss, 1982; Artstein and Poe-
sio, 2008) for each language and relation (Table 2),
with values between k = 0.370 and k = 0.706.
The higher agreement occurs in verb-object collo-
cations, while in nominal compounds it was lower.

Once the final sheets (for each relation and lan-
guage) were created, a total of 447 combinations
(3.6%) were labeled as doubt (there was no agree-
ment between the annotators). Out of these, 260
(58.2%) were finally considered true collocations
by the team of experts. Among the most frequent
disagreements we found adjective-noun pairs for
which the annotators doubted whether they were
technical terms (e.g., light cluster), nominal com-
pounds in which one of the nouns seems lexically
selected by the other (e.g., golf tournament), and
verb-object combinations in which the noun could
be predicative and the verb has scarce lexical con-
tent, but lacks a single-word verb equivalent (e.g.,
tener velocidad ‘have speed’ in Spanish). In the
latter group, we harmonized their annotation in the
three languages.

The final resource includes a total of 60 lexical
functions, some of them complex (e.g., Magn +
AntiBon), and not all of them in every lan-
guage (i.e., less frequent LFs appear only in one
or two corpora). The most frequent ones are
Oper1, Magn, Bon, and NonStandard (see
Appendix A for the full list of LFs per language).

5 Conclusions and Further Work

This paper presented a multilingual corpus with
manual annotation of collocations and their lexi-
cal functions in English, Portuguese, and Spanish.
The resource contains 155k tokens and 1, 526 col-
locations classified into 60 lexical functions. Each
collocation candidate was revised by three lan-
guage experts, and those which were dubious were
corrected by the whole team of annotators.

We release both the final corpus of each anno-
tator as well as the gold-standard resource in .con-
llu format. This dataset can serve as a basis to
evaluate systems designed to automatically extract
collocations and identify their lexical functions,
which in turn may be useful for different NLP
and corpus linguistics tasks. As we provide re-
sources for three languages (and with different de-
pendency relations), the corpora can be also useful
to verify whether some methods behave similarly
or not in each language and syntactic pattern.

In further work we plan to carry out a multi-
lingual alignment of the collocations in each lan-
guage. This process, also enlarged with other mul-
tilingual equivalents, will generate a new dataset
for evaluating the automatic translation of this type
of multiword expressions.

Acknowledgments

This research was supported by a 2017 Leonardo
Grant for Researchers and Cultural Creators
(BBVA Foundation), by Ministerio de Economı́a,
Industria y Competitividad (project with reference
FFI2016-78299-P), and by the Galician Govern-
ment (Xunta de Galicia grant ED431B-2017/01).
Marcos Garcia has been funded by a Juan de
la Cierva-incorporación grant (IJCI-2016-29598),
and Marcos Garcı́a-Salido by a post-doctoral grant
from Xunta de Galicia (ED481D-2017-009).



4017

References
Margarita Alonso-Ramos, Alfonso Nishikawa, and Or-

solya Vincze. 2010. DiCE in the web: An online
Spanish collocation dictionary. In ELexicography
in the 21st Century: New Challenges, New Applica-
tions: Proceedings of ELex 2009, volume 7, pages
369–374. Presses univ. de Louvain.

Ron Artstein and Massimo Poesio. 2008. Survey ar-
ticle: Inter-coder agreement for computational lin-
guistics. Computational Linguistics, 34(4).

Laura Banarescu, Claire Bonial, Shu Cai, Madalina
Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin
Knight, Philipp Koehn, Martha Palmer, and Nathan
Schneider. 2013. Abstract meaning representation
for sembanking. In Proceedings of the 7th Linguis-
tic Annotation Workshop and Interoperability with
Discourse, pages 178–186. Association for Compu-
tational Linguistics.

Morton Benson, Evelyn Benson, and Robert Ilson.
1986. The BBI combinatory dictionary of English:
A guide to word combinations. John Benjamins
Publishing.

Claire Bonial, Meredith Green, Jenette Preciado, and
Martha Palmer. 2014. An approach to take multi-
word expressions. In Proceedings of the 10th Work-
shop on Multiword Expressions (MWE), pages 94–
98. Association for Computational Linguistics.

Roberto Carlini, Joan Codina-Filba, and Leo Wanner.
2014. Improving collocation correction by ranking
suggestions using linguistic knowledge. In Proceed-
ings of the third workshop on NLP for computer-
assisted language learning, pages 1–12, Uppsala.
LiU Electronic Press.

Richard Eckart de Castilho, Éva Mújdricza-Maydt,
Seid Muhie Yimam, Silvana Hartmann, Iryna
Gurevych, Anette Frank, and Chris Biemann. 2016.
A web-based tool for the integrated annotation of se-
mantic and syntactic structures. In Proceedings of
the Workshop on Language Technology Resources
and Tools for Digital Humanities (LT4DH), pages
76–84. The COLING 2016 Organizing Committee.

Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicog-
raphy. Computational Linguistics, 16(1):22–29.

Silvio Cordeiro, Aline Villavicencio, Marco Idiart, and
Carlos Ramisch. 2019. Unsupervised composition-
ality prediction of nominal compounds. Computa-
tional Linguistics, 45(1):1–57.

Mark Davies and Joseph L Fleiss. 1982. Measuring
agreement for multinomial data. Biometrics, pages
1047–1051.

Stefan Evert. 2008. Corpora and collocations. In Anke
Lüdeling and Merja Kytö, editors, Corpus Linguis-
tics. An international handbook, volume 2, pages
1212–1248. Mouton de Gruyter, Berlin.

Stefan Evert and Brigitte Krenn. 2001. Methods for
the qualitative evaluation of lexical association mea-
sures. In Proceedings of 39th Annual Meeting of the
Association for Computational Linguistics, pages
188–195, Toulouse, France. Association for Com-
putational Linguistics.

Stefan Evert, Peter Uhrig, Sabine Bartsch, and Thomas
Proisl. 2017. E-VIEW-affilation–A large-scale eval-
uation study of association measures for colloca-
tion identification. In Proceedings of eLex 2017–
Electronic lexicography in the 21st century: Lexi-
cography from Scratch, pages 531–549.

Alexsandro Fonseca, Fatiha Sadat, and François
Lareau. 2017. Combining dependency parsing and
a lexical network based on lexical functions for
the identification of collocations. In International
Conference on Computational and Corpus-Based
Phraseology, pages 447–461. Springer.

Marcos Garcia. 2018. Comparing bilingual word
embeddings to translation dictionaries for extract-
ing multilingual collocation equivalents. In Stella
Markantonatou, Carlos Ramisch, Agata Savary, and
Veronika Vincze, editors, Multiword expressions at
length and in depth: Extended papers from the MWE
2017 workshop, pages 319–342. Language Science
Press, Berlin.

Marcos Garcia, Marcos Garcı́a-Salido, and Mar-
garita Alonso-Ramos. 2017. Using bilingual word-
embeddings for multilingual collocation extraction.
In Proceedings of the 13th Workshop on Multiword
Expressions (MWE 2017), pages 21–30, Valencia,
Spain. Association for Computational Linguistics.

Alexander Gelbukh and Olga Kolesnikova. 2010. Su-
pervised learning for semantic classification of span-
ish collocations. In Mexican Conference on Pattern
Recognition, volume 6256 of Lecture Notes in Com-
puter Science, pages 362–371. Springer-Verlag.

Stefan Th. Gries. 2013. 50-something years of work on
collocations. International Journal of Corpus Lin-
guistics, 18(1):137–165.

Mladen Karan, Jan Šnajder, and Bojana Dalbelo Bašić.
2012. Evaluation of classification algorithms and
features for collocation extraction in croatian. In
Proceedings of the Eighth International Conference
on Language Resources and Evaluation (LREC-
2012), pages 657–662, Istanbul, Turkey. European
Language Resources Association (ELRA).

Adam Kilgarriff, Pavel Rychlý, Milos Jakubicek,
Vojtěch Kovář, Vit Baisa, and Lucia Kocincová.
2014. Extrinsic corpus evaluation with a colloca-
tion dictionary task. In Proceedings of the Ninth In-
ternational Conference on Language Resources and
Evaluation (LREC’14), pages 545–552, Reykjavik,
Iceland. European Language Resources Association
(ELRA).

http://www.dicesp.com/app/webroot/files/file/eLex%202009%20DiCE%20in%20the%20Web.pdf
http://www.dicesp.com/app/webroot/files/file/eLex%202009%20DiCE%20in%20the%20Web.pdf
https://doi.org/10.1162/coli.07-034-R2
https://doi.org/10.1162/coli.07-034-R2
https://doi.org/10.1162/coli.07-034-R2
http://aclweb.org/anthology/W13-2322
http://aclweb.org/anthology/W13-2322
https://doi.org/10.3115/v1/W14-0816
https://doi.org/10.3115/v1/W14-0816
http://www.ep.liu.se/ecp/107/001/ecp14107001.pdf
http://www.ep.liu.se/ecp/107/001/ecp14107001.pdf
http://aclweb.org/anthology/W16-4011
http://aclweb.org/anthology/W16-4011
https://doi.org/10.1162/coli_a_00341
https://doi.org/10.1162/coli_a_00341
https://doi.org/10.3115/1073012.1073037
https://doi.org/10.3115/1073012.1073037
https://doi.org/10.3115/1073012.1073037
https://elex.link/elex2017/wp-content/uploads/2017/09/paper32.pdf
https://elex.link/elex2017/wp-content/uploads/2017/09/paper32.pdf
https://elex.link/elex2017/wp-content/uploads/2017/09/paper32.pdf
https://doi.org/10.1007/978-3-319-69805-2_31
https://doi.org/10.1007/978-3-319-69805-2_31
https://doi.org/10.1007/978-3-319-69805-2_31
https://doi.org/10.5281/zenodo.1469571
https://doi.org/10.5281/zenodo.1469571
https://doi.org/10.5281/zenodo.1469571
https://doi.org/10.18653/v1/W17-1703
https://doi.org/10.18653/v1/W17-1703
https://doi.org/10.1075/ijcl.18.1.09gri
https://doi.org/10.1075/ijcl.18.1.09gri
http://www.lrec-conf.org/proceedings/lrec2012/pdf/796_Paper.pdf
http://www.lrec-conf.org/proceedings/lrec2012/pdf/796_Paper.pdf
http://www.lrec-conf.org/proceedings/lrec2014/pdf/52_Paper.pdf
http://www.lrec-conf.org/proceedings/lrec2014/pdf/52_Paper.pdf


4018

Francois Lareau, Mark Dras, Benjamin Borschinger,
and Robert Dale. 2011. Collocations in multilin-
gual natural language generation: Lexical functions
meet lexical functional grammar. In Proceedings of
the Australasian Language Technology Association
Workshop 2011, pages 95–104.

Igor Mel’čuk. 1995. Phrasemes in language and
phraseology in linguistics. In Martin Everaert, Erik-
Jan van der Linden, André Schenk, and Rob Schreu,
editors, Idioms: Structural and psychological per-
spectives, pages 167–232. Hillsdale: Lawrence Erl-
baum Associates.

Igor Mel’čuk. 1996. Lexical functions: a tool for the
description of lexical relations in a lexicon. In Leo
Wanner, editor, Lexical Functions in Lexicography
and Natural Language Processing, volume 31 of
Studies in Corpus Linguistics, pages 37–102. John
Benjamins Publishing.

Igor Mel’čuk. 1998. Collocations and lexical func-
tions. In Anthony Paul Cowie, editor, Phraseol-
ogy. Theory, analysis and applications, pages 23–
53. Clarendon Press, Oxford.

Joakim Nivre. 2015. Towards a universal grammar for
natural language processing. In International Con-
ference on Intelligent Text Processing and Computa-
tional Linguistics, volume 9041 of Lecture Notes in
Computer Science, pages 3–16. Springer.

Tim O’Gorman, Sameer Pradhan, Martha Palmer, Julia
Bonn, Kathryn Conger, and James Gung. 2018. The
new propbank: Aligning propbank with amr through
pos unification. In Proceedings of the Eleventh In-
ternational Conference on Language Resources and
Evaluation (LREC-2018). European Language Re-
source Association.

Darren Pearce. 2002. A comparative evaluation of col-
location extraction techniques. In Proceedings of
the Third International Conference on Language Re-
sources and Evaluation (LREC’02), Las Palmas, Ca-
nary Islands - Spain. European Language Resources
Association (ELRA).

Pavel Pecina. 2010. Lexical association measures and
collocation extraction. Language Resources and
Evaluation, 44(1-2):137–158.

Alain Polguère. 2011. Propriétés sémantiques et com-
binatoires des quasi-prédicats sémantiques. Scolia,
26:131–152.

Carlos Ramisch, Silvio Ricardo Cordeiro, Agata
Savary, Veronika Vincze, Verginica Barbu Mititelu,
Archna Bhatia, Maja Buljan, Marie Candito, Polona
Gantar, Voula Giouli, Tunga Güngör, Abdelati
Hawwari, Uxoa Iñurrieta, Jolanta Kovalevskaitė, Si-
mon Krek, Timm Lichte, Chaya Liebeskind, Jo-
hanna Monti, Carla Parra Escartı́n, Behrang Qasem-
iZadeh, Renata Ramisch, Nathan Schneider, Ivelina
Stoyanova, Ashwini Vaidya, and Abigail Walsh.
2018. Edition 1.1 of the PARSEME Shared Task on

Automatic Identification of Verbal Multiword Ex-
pressions. In Proceedings of the Joint Workshop on
Linguistic Annotation, Multiword Expressions and
Constructions (LAW-MWE-CxG-2018), pages 222–
240. Association for Computational Linguistics.

Carlos Ramisch and Aline Villavicencio. 2018. Com-
putational treatment of multiword expressions. In
Ruslan Mitkov, editor, Oxford Handbook on Compu-
tational Linguistics, 2nd edition. Oxford University
Press.

Sara Rodrı́guez-Fernández, Luis Espinosa Anke,
Roberto Carlini, and Leo Wanner. 2016. Semantics-
driven recognition of collocations using word em-
beddings. In Proceedings of the 54th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 2: Short Papers), pages 499–505. Associa-
tion for Computational Linguistics.

Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann A.
Copestake, and Dan Flickinger. 2002. Multiword
expressions: A pain in the neck for NLP. In Pro-
ceedings of the 3rd International Conference on
Computational Linguistics and Intelligent Text Pro-
cessing, volume 2276/2010 of CICLing ’02, pages
1–15, London, UK. Springer-Verlag.

Agata Savary, Carlos Ramisch, Silvio Cordeiro, Fed-
erico Sangati, Veronika Vincze, Behrang Qasem-
iZadeh, Marie Candito, Fabienne Cap, Voula Giouli,
Ivelina Stoyanova, and Antoine Doucet. 2017. The
PARSEME Shared Task on Automatic Identification
of Verbal Multiword Expressions. In Proceedings of
the 13th Workshop on Multiword Expressions (MWE
2017), pages 31–47. Association for Computational
Linguistics.

Violeta Seretan. 2011. Syntax-based collocation ex-
traction, volume 44 of Text, Speech and Language
Technology. Springer Science & Business Media.

Violeta Seretan and Eric Wehrli. 2006. Accurate col-
location extraction using a multilingual parser. In
Proceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 953–960, Sydney, Australia. Association
for Computational Linguistics.

Frank Smadja. 1993. Retrieving collocations from text:
Xtract. Computational Linguistics, 19(1):143–177.

Peter Uhrig, Stefan Evert, and Thomas Proisl.
2018. Collocation Candidate Extraction from
Dependency-Annotated Corpora: Exploring Differ-
ences across Parsers and Dependency Annotation
Schemes. In Lexical Collocation Analysis, Quan-
titative Methods in the Humanities and Social Sci-
ences, pages 111–140. Springer.

Leo Wanner. 1996. Lexical Functions in Lexicogra-
phy and Natural Language Processing, volume 31
of Studies in Corpus Linguistics. John Benjamins
Publishing.

http://aclweb.org/anthology/U11-1013
http://aclweb.org/anthology/U11-1013
http://aclweb.org/anthology/U11-1013
https://doi.org/10.1007/978-3-319-18111-0_1
https://doi.org/10.1007/978-3-319-18111-0_1
http://aclweb.org/anthology/L18-1231
http://aclweb.org/anthology/L18-1231
http://aclweb.org/anthology/L18-1231
http://www.lrec-conf.org/proceedings/lrec2002/pdf/169.pdf
http://www.lrec-conf.org/proceedings/lrec2002/pdf/169.pdf
https://doi.org/10.1007/s10579-009-9101-4
https://doi.org/10.1007/s10579-009-9101-4
http://aclweb.org/anthology/W18-4925
http://aclweb.org/anthology/W18-4925
http://aclweb.org/anthology/W18-4925
https://doi.org/10.1093/oxfordhb/9780199573691.013.56
https://doi.org/10.1093/oxfordhb/9780199573691.013.56
https://doi.org/10.18653/v1/P16-2081
https://doi.org/10.18653/v1/P16-2081
https://doi.org/10.18653/v1/P16-2081
https://doi.org/10.1007/3-540-45715-1_1
https://doi.org/10.1007/3-540-45715-1_1
https://doi.org/10.18653/v1/W17-1704
https://doi.org/10.18653/v1/W17-1704
https://doi.org/10.18653/v1/W17-1704
https://doi.org/10.1007/978-94-007-0134-2
https://doi.org/10.1007/978-94-007-0134-2
https://doi.org/10.3115/1220175.1220295
https://doi.org/10.3115/1220175.1220295
https://doi.org/10.1007/978-3-319-92582-0_6
https://doi.org/10.1007/978-3-319-92582-0_6
https://doi.org/10.1007/978-3-319-92582-0_6
https://doi.org/10.1007/978-3-319-92582-0_6
https://doi.org/10.1075/slcs.31
https://doi.org/10.1075/slcs.31


4019

Leo Wanner, Bernd Bohnet, Nadjet Bouayad-Agha,
François Lareau, and Daniel Nicklaß. 2010. MAR-
QUIS: Generation of user-tailored multilingual air
quality bulletins. Applied Artificial Intelligence,
24(10):914–952.

Leo Wanner, Bernd Bohnet, and Mark Giereth. 2006.
Making sense of collocations. Computer Speech &
Language, 20(4):609–624.

Leo Wanner, Gabriela Ferraro, and Pol Moreno. 2016.
Towards Distributional Semantics-Based Classifica-
tion of Collocations for Collocation Dictionaries.
International Journal of Lexicography, 30(2):167–
186.

A Appendices

Lexical Function Eng. Pt. Sp. Total Lexical Function Eng. Pt. Sp. Total
Oper1 151 181 106 438 FinOper1 – 1 2 3
Magn 89 81 69 239 Centr – – 3 3
Bon 58 49 14 121 Caus1Manif – 3 – 3
A NonStandard 55 28 30 113 AntiMagn temp 2 – 1 3
S NonStandard 8 31 16 55 SLoc 1 1 – 2
AntiMagn 18 14 16 48 SIncepPredP lus 2 – – 2
CausOper1 16 12 16 44 S2 – 2 – 2
Sing 8 20 11 39 AntiV er – 2 – 2
Mult 4 9 19 32 AntiMagn quant – 1 1 2
AntiBon 20 9 1 30 S SingCaus1Manif – 1 – 1
Cap 2 13 13 28 SOper1 – 1 – 1
V NonStandard 4 11 6 21 SOper – – 1 1
CausFunc 9 5 5 19 SManif – 1 – 1
Oper2 1 14 2 17 SLiqu1Func – – 1 1
Magn quant 9 2 6 17 SIncepFunc – – 1 1
Magn temp 8 3 3 14 SFinOper1 – 1 – 1
V er 3 5 1 9 SFinOper – – 1 1
Germ 3 5 1 9 SFinFunc – 1 – 1
Magn+AntiBon 4 1 3 8 SContOper1 – 1 – 1
CausFunc1 – 7 – 7 SCausOper1 – – 1 1
Real1 – 6 – 6 SCausFunc – – 1 1
Magn+Bon 3 1 2 6 Real 1 – – 1
Liqu1Func 1 2 2 5 Loc – – 1 1
Gener 1 2 2 5 Culm – – 1 1
SReal – 1 3 4 ContOper1 – 1 – 1
Pos 1 2 1 4 CausPredP lus – 1 – 1
NonStandard Oper1 – 3 1 4 CausManif – 1 – 1
LiquFunc – – 3 3 AntiPos – 1 – 1
IncepOper1 1 1 1 3 A NonStd./Magn 1 – – 1
Func – 2 1 3 A1IncepPredP lus – – 1 1

Table 3: Number of unique collocations per lexical
function and language in the final corpus.

https://doi.org/10.1080/08839514.2010.529258
https://doi.org/10.1080/08839514.2010.529258
https://doi.org/10.1080/08839514.2010.529258
https://doi.org/10.1016/j.csl.2005.10.002
https://doi.org/10.1093/ijl/ecw002
https://doi.org/10.1093/ijl/ecw002

