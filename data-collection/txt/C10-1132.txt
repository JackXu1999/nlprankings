Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1173–1181,

Beijing, August 2010

1173

 A Character-Based Joint Model for Chinese Word Segmentation

Kun Wang and Chengqing Zong 

National Laboratory of Pattern Recognition 

Institute of Automation, Chinese Academy of Science

{kunwang,cqzong}@nlpr.ia.ac.cn 

Keh-Yih Su 

Behavior Design Corporation

 

Kysu@bdc.com.tw 

 

Abstract 

The  character-based  tagging  approach 
is  a  dominant  technique  for  Chinese 
word segmentation, and both discrimi-
native  and  generative  models  can  be 
adopted  in  that  framework.  However, 
generative  and  discriminative  charac-
ter-based  approaches  are  significantly 
different  and  complement  each  other. 
A  simple  joint  model  combining  the 
character-based  generative  model  and 
the discriminative one is thus proposed 
in this paper to take advantage of both 
approaches.  Experiments  on  the  Sec-
ond  SIGHAN  Bakeoff  show  that  this 
joint  approach  achieves  21%  relative 
error reduction over the discriminative 
model and 14% over the generative one. 
In addition, closed tests also show that 
the  proposed  joint  model  outperforms 
all  the  existing  approaches  reported  in 
the  literature  and  achieves  the  best  F-
score in four out of five corpora. 

Introduction 

1 
Chinese  word  segmentation  (CWS)  plays  an 
important  role  in  most  Chinese  NLP  applica-
tions such as machine translation, information 
retrieval and question answering. Many statis-
tical methods for CWS have been proposed in 
the last two decades, which can be classified as 
either  word-based  or  character-based.  The 
word-based  approach  regards  the  word  as  the 
basic unit, and the desired segmentation result 
is the best word sequence found by the search 
process. On the other hand, the character-based 
approach treats the word segmentation task as 
a character tagging problem. The final segmen-

tation  result  is  thus  indirectly  generated  ac-
cording to the tag assigned to each associated 
character. Since the vocabulary size of possible 
character-tag-pairs  is  limited,  the  character-
based  models  can  tolerate  out-of-vocabulary 
(OOV)  words and have become the dominant 
technique for CWS in recent years. 

On the other hand, statistical approaches can 
also be classified as either adopting a genera-
tive model or adopting a discriminative model. 
The generative model learns the joint probabil-
ity  of  the  given  input  and  its  associated  label 
sequence,  while 
the  discriminative  model 
learns  the  posterior  probability  directly.  Gen-
erative  models  often  do  not  perform  well  be-
cause they make strong independence assump-
tions  between  features  and  labels.  However, 
(Toutanova, 2006) shows that generative mod-
els can also achieve very similar or better per-
formance  than  the  corresponding  discrimina-
tive models if they have a structure that avoids 
unrealistic independence assumptions.  

In  terms  of  the  above  dimensions,  methods 

for CWS can be classified as:  

1) The word-based generative model (Gao et 
al., 2003; Zhang et al., 2003), which is a well-
known  approach  and  has  been  used  in  many 
successful applications;  

2)  The  word-based  discriminative  model 
(Zhang  and  Clark,  2007),  which  generates 
word candidates with both word and character 
features and is the only word-based model that 
adopts the discriminative approach； 

3) The character-based discriminative model 
(Xue,  2003;  Peng  et  al.,  2004;  Tseng  et  al., 
2005;  Jiang  et  al.,  2008),  which  has  become 
the  dominant  method  as  it  is  robust  on  OOV 
words  and  is  capable  of  handling  a  range  of 
different  features,  and  it  has  been  adopted  in 
many previous works;  

1174

4)  The  character-based  generative  model 
(Wang et al., 2009), which adopts a character-
tag-pair-based  n-gram  model  and  achieves 
comparable results with the popular character-
based discriminative model. 

In general, character-based models are much 
more  robust  on  OOV  words  than  word-based 
approaches do, as the vocabulary size of char-
acters  is  a  closed  set  (versus  the  open  set  of 
that  of  words).  Furthermore,  among  those 
character-based  approaches, 
the  generative 
model and the discriminative one complement 
each  other  in  handling  in-vocabulary  (IV) 
words and OOV words. Therefore, a character-
based joint model is proposed to combine them. 
This  proposed  joint  approach  has  achieved 
good  balance  between  IV  word  recognition 
and OOV word identification. The experiments 
of closed tests on the second SIGHAN Bakeoff 
(Emerson,  2005)  show  that  the  joint  model 
significantly  outperforms  the  baseline  models 
of  both  generative  and  discriminative  ap-
proaches.  Moreover,  statistical  significance 
tests also show that the joint model is signifi-
cantly better than all those state-of-the-art sys-
tems reported in the literature and achieves the 
best F-score in four of the five corpora tested. 
2  Character-Based Models for CWS 
The goal of CWS is to find the corresponding 
word sequence for a given character sequence. 
Character-based model is to find out the corre-
sponding tags for given character sequence. 
2.1  Character-Based Discriminative Model 
The  character-based  discriminative  model 
(Xue,  2003)  treats  segmentation  as  a  tagging 
problem, which assigns a corresponding tag to 
each character. The model is formulated as: 
P t c
(
n
1

       (1) 
Where tk is a member of {Begin, Middle, End, 
Single}  (abbreviated  as  B,  M,  E  and  S  from 
now on) to indicate the corresponding position 
of character ck in its associated word. For ex-
ample,  the  word  “北京市  (Beijing  City)”  will 
be assigned with the corresponding tags as: “北
/B (North) 京/M (Capital) 市/E (City)”.  

P t c
(
k
k

∏

∏

Since this tagging approach treats characters 
as  basic  units,  the  vocabulary  size  of  those 
possible  character-tag-pairs  is  limited.  There-

2 )
+
2
−

P t
(

c
n
1

t
k
1

≈

=

n
1

1
−

1
=

1
=

)

)

,

n

n

k

k

k

k

n

n

 

1
+

n

2, 1,0,1);

= − −
(

2, 1,0,1,2);
= − −

fore, this method is robust to OOV words and 
could  possess  a  high  recall  of  OOV  words 
(ROOV). Although the dependency between ad-
jacent  tags/labels  can  be  addressed,  the  de-
pendency between adjacent characters within a 
word  cannot  be  directly  modeled  under  this 
framework. Lower recall of IV words (RIV) is 
thus usually accompanied (Wang et al., 2009).  
In this work, the character-based discrimina-
tive model is implemented by adopting the fea-
ture  templates  given  by  (Ng  and  Low,  2004), 
but excluding those ones that are forbidden by 
the  closed  test  regulation  of  SIGHAN  (e.g., 
Pu(C0):  whether  C0  is  a  punctuation).  Those 
feature templates adopted are listed below: 
a C n
( )
(
b C C
( )
n
c C C
( )
1
1
For  example,  when  we  consider  the  third 
character  “奥”  in  the  sequence  “北京奥运会”, 
template (a) results in the features as following: 
C-2=北, C-1=京, C0=奥, C1=运, C2=会, and tem-
plate (b) generates the features as: C-2C-1=北京, 
C-1C0=京奥,  C0C1=奥运,  C1C2=运会,  and  tem-
plate (c) gives the feature C-1C1=京运. 
2.2  Character-Based Generative Model 
To  incorporate  the  dependency  between  adja-
cent characters in the character-based approach, 
(Wang et al., 2009) proposes a character-based 
generative model. In this approach, word wi is 
first replaced with its corresponding sequence 
of [character, tag] (denoted as [c, t]), where tag 
is the same as that adopted in the above char-
acter-based  discriminative  model.  With  this 
representation, this model can be expressed as:  

−

 

=

×
n
1

                   (2) 

c
P c t
P w c
([ , ]
)
)
(
n
n
n
m
≡
1
1
1
1
c t
P c t
P c
(
([ , ] )
[ , ] )
n
n
n
1
1
1
 is the same for 
≡  and 
Since 
c t
P c
[ , ] ) 1
(
n
1
various  candidates,  only 
should  be 
1] )nt
considered.  It  can  be  further  simplified  with 
Markov Chain assumption as: 
 
P c t
c t −
([ , ] [ , ]
).
i
1
i k
−

P c
(
)
n
1
)nP c
1(
([ ,P c

                    (3) 

P c t
([ , ] )
n
1

n

i

≈ ∏

i

1
=

Compared  with  the  character-based  dis-
criminative model, this generative model keeps 
the capability to handle OOV words because it 
also regards the character as basic unit. In ad-
dition, the dependency between adjacent 

1175

1 

-1 

1C2 C

1C2 C

Feature 

Feature 

E/0.8138 

E/0.2236 

-0.7207
2.8300
0.0000
1.8223

0.4175 
0.0687 
-0.4330 
-0.0532 

Gold and Discriminative Tag: M 

C-1C0
0.0000
0.0921
0.0000
0.0000

C0 C
0.0000
4.5381
1.8847
2.7360

C1 C
0.2282
-1.2696
2.9422
-1.9008

C-2 C
0.3586
0.3666
-0.5657
-0.1595

2 
0.7709
-0.5970
0.4636
-0.6375

0 
0.0800
0.7229
-0.3174
-0.4856

C2 C
0.4626
-0.0846
-0.0918
-0.2862

-1 C
0.1572 
0.1910 
-0.5527 
0.2046 

宿 
Tag probability:   B/0.0333 

-1C1
-0.6718  0.0000
0.8049  0.0000
-0.9700  0.0000
0.8368  0.0000

Tag 
B 
E 
M 
S 
者 
Tag probability:   B/0.0009 

C-2 C
-1.4375
1.3558
1.1071
-1.0254
Gold and Discriminative Tag: E 

Generative Trigram Tag: E 
S/0.0030 
M/0.7401 
C0C1 C
C-2C-1
0.0000 
0.2741
0.0049
0.0000 
0.0000 
-0.1708
0.0000
0.0000 
Generative Trigram Tag: S 
M/0.0012 
S/0.1841 
-1C1
C0C1 C
Tag 
0.0000 
0.0000  0.0000
B 
-1.0279  0.6127  0.0000
E 
0.0000  0.0000
0.0000 
M 
S 
1.0494 
0.7113  0.0000
Table 1: The corresponding lambda weight of features for “露宿者” in the sentence “[該] [處] [的] [露宿者] 
[只] [有] [數] [人]”. In the Feature column and Tag row, the value is the corresponding lambda weight for 
the feature and tag under ME framework. The meanings of those features are explained in Section 2.1. 
 
characters  is  now  directly  modeled.  This  will 
give  sharper  preference  when  the  history  of 
assignment  is  given.  Therefore,  this  approach 
not only holds robust IV performance but also 
achieves comparable results with the discrimi-
native model. However, the OOV performance 
of this approach is still lower than that of the 
discriminative  model  (see  in  Table  5),  which 
would be discussed in the next section. 
3  Problems with the Character-Based 

-2C-1
0.0085
0.0000
0.0000
-0.0024

C-1C0
0.0000
0.0000
0.0000
0.0000

/M者/E(street  sleeper)”  is  observed  to  be  an 
OOV  word,  while  “ 露 /B 宿 /E(sleep  on  the 
street)” is an IV word, where the associated tag 
of each character is given after the slash sym-
bol.  The  character-based  generative  model 
wrongly splits “露宿者” into two words “露/B
宿/E”  and  “者/S  (person)”,  as  the  associated 
trigram for “露宿者” is not seen in the training 
set.  However,  the  discriminative  model  gives 
the correct result for “宿/M” and the dominant 
features come from its future context “者” and 
“只”.  Similarly,  the  future  context  “只”  helps 
to give the correct tag to “者/E”. Table 1 gives 
the corresponding lambda feature weights (un-
der the Maximum Entropy (ME) (Ratnaparkhi, 
1998)  framework)  for  “ 露 宿 者”  in  the  dis-
criminative model. It shows that in the column 
of  “C1”  below  “宿”,  the  lambda  value  associ-
ated with the correct tag “M” is 2.9422, which 
is  the  highest  value  in  that  column  and  is  far 
greater  than  that  of  the  wrong  tag  “E”  (i.e.,  -
1.2696)  assigned  by  the  generative  model. 
Which indicates that the future feature “C1” is 
the most useful feature for tagging “宿”. 

The  above  example  shows  the  character-
based  generative  model  fails  to  handle  some 
OOV words such as “露宿者” because this ap-
proach cannot utilize future context when it is 
indeed  required.  However,  the  future  context 
for the generative model scanning from left to 
right is just its past context when it scans from 
right to left. It is thus expected that this kind of 

Generative Model 

The  character-based  generative  model  can 
handle the dependency between adjacent char-
acters  and  thus  performs  well  on  IV  words. 
However, this generative trigram model is de-
rived  under  the  second  order  Markov  Chain 
assumption.  Future  character  context  (i.e.,  C1 
and C2) is thus not utilized in the model when 
the tag of the current character (i.e., t0) is de-
termined.  Nevertheless, 
the  future  context 
would  help  to  select  the  correct  tag  when  the 
associated trigram has not been observed in the 
training-set,  which  is  just  the  case  for  those 
OOV  words.  In  contrast,  the  discriminative 
one  could  get  help  from  the  future  context  in 
this case. The example given in the next para-
graph clearly shows the above situation. 

At the sentence “該(that) 處(place) 的(of) 露
宿者(street sleeper) 只(only) 有(have) 數(some) 
人(person) (There are only some street sleepers 
in that place)” in the CITYU corpus, “露/B宿

1176

errors  will  be  fixed  if  we  let  the  model  scans 
from  both  directions,  and  then  combine  their 
results. Unfortunately, it is observed that these 
two  scanning  modes  share  over  90%  of  their 
errors.  For  example,  in  CITYU  corpus,  the 
left-to-right scan generates 1,958 wrong words 
and  the  right-to-left  scan  results  1,947  ones, 
while 1,795 of them are the same. Similar be-
havior can also be observed on other corpora. 

To find out what are the problems, 10 errors 
that are similar to “露宿者” are selected to ex-
amine. Among those errors, only one of them 
is fixed, and “露宿者” still cannot be correctly 
segmented. Having analyzed the scores of the 
model scanning from both directions, we found 
that the original scores (from left-to-right scan) 
at the stages “者” and “宿” indeed get better if 
the  model  scans  from  right-to-left.  However, 
the score at the stage “露” deteriorates because 
the  useful  feature  “ 者”  (a  past  non-adjacent 
character  for  “露”  when  scans  form  right-to-
left) still cannot be utilized when the past con-
text “宿者” as a whole is unseen, when the re-
lated  probabilities  are  estimated  via  modified 
Kneser-Ney  smoothing  (Chen  and  Goodman, 
1998) technique. 

Two scanning modes seem not complement-
ing each other, which is out of our original ex-
pectation. However, we found that the charac-
ter-based generative model and the discrimina-
tive  one  complement  each  other  much  more 
than the two scanning modes do. It is observed 
that these two approaches share less than 50% 
of their errors. For example, in CITYU corpus, 
the generative approach generates 1,958 wrong 
words and the discriminative one results 2,338 
ones, while only 835 of them are the same. 

The  statistics  of  the  remaining  errors  re-
sulted from the generative model and the dis-
criminative  model  is  shown  in  Table  2.  As 
shown in the table, it can be seen that the gen-
erative  model  and  the  discriminative  model 
complement each other on handling IV words 
and  OOV  words  (In  the  “IV  Errors”  column, 
the number of “G+D-” is much more than the 
“G-D+”, while the behavior is reversed in the 
“OOV Errors” column). 
4  Proposed Joint Model 
Since  the  performance  of  both  IV  words  and 
OOV words are important for real applications, 

OOV Errors 

IV Errors 
G+D-
12,027

7,481

2,384 

6,139 

G-D+ G-D- G+D-  G-D+  G-D-
4,723
3,975
Table 2: Statistics for remaining errors of the char-
acter-based generative model and the discriminative 
one  on  the  second  SIGHAN  Bakeoff  (“G+D-”  in 
the  “IV  Errors”  column  means  that  the  generative 
model segments the IV words correctly but the dis-
criminative one gives wrong results. The meanings 
of other abbreviations are similar with this one.). 
we  need  to  combine  the  strength  from  both 
models.  Among  various  combining  methods, 
log-linear  interpolation  combination  is  a  sim-
ple but effective one (Bishop, 2006). Therefore, 
the  following  character-based  joint  model  is 
proposed, and a parameter α is used to weight 
the generative model in a cross-validation set. 

Score t
(

)

k

 

log(
= ×
α
(1
+ −

c t
P c t
[ , ]
([ , ]
k
k
k
P t c
)
(
log(
k
α
k

×

k

))
1
−
2
−
))

2
+
2
−

           (4) 

α

1.0)

(0.0

α≤
≤

Where  tk  indicates  the  corresponding  position 
of  character  ck,  and 
 is  the 
weight for the generative model. Score(tk) will 
be used during searching the best sequence. It 
can  be  seen  that  these  two  models  are  inte-
grated naturally as both are character-based. 

Generally speaking, if the “G(or D)+” has a 
strong preference on the desired candidate, but 
the  “D(or  G)-”  has  a  weak  preference  on  its 
top-1 incorrect candidate, then this combining 
method  would  correct  most  “G+D-  (also    G-
D+)” errors. On the other hand, the advantage 
of combining two models would vanish if the 
“G(or  D)+”  has  a  weak  preference  while  the 
“D(or  G)-”  has  a  strong  preference  over  their 
top-1 candidates. In our observation, these two 
models meet this requirement quite well. 
5  Weigh Various Features Differently 
For  a  given  observation,  intuitively  each 
feature  should  be  trained only  once  under  the 
ME  framework  and  its  associated  weight  will 
be automatically learned from the training cor-
pus.  However,  when  we  repeat  the  work  of 
(Jiang  et  al.,  2008),  which  reports  to  achieve 
the  state-of-art  performance  in  the  data-sets 
that we adopt, it has been found that some fea-
tures (e.g., C0) are unnoticeably trained several 
times in their model (which are implicitly gen-
erated from different feature templates used in 
the paper). For example, the feature C0 actually 

1177

Abbrev. 
AS 

Corpus 
Academia Sinica (Taipei) 
City University of Hong Kong  CITYU 
Microsoft Research (Beijing)  MSR 
Peking University 

Encoding 
Unicode/Big5 
Unicode/Big5 
Unicode/CP936
PKU(ucvt.) Unicode/CP936
Unicode/CP936
PKU(cvt.) 

Training Size
(Words/Type)
5.45M/141K 
1.46M/69K 
2.37M/88K 
1.1M/55K 
1.1M/55K 

Test Size 
(Words/Type)  OOV Rate
122K/19K 
41K/9K 
107K/13K 
104K/13K 
104K/13K 

0.046 
0.074 
0.026 
0.058 
0.035 

Table 3: Corpus statistics for the second SIGHAN Bakeoff 

 

appears  twice,  which  is  generated  from  two 
different templates Cn (with n=0, generates C0) 
and  [C0Cn]  (used  in  (Jiang  et  al.,  2008),  with 
n=0,  generates  [C0C0]).  The  meanings  of  fea-
tures  are  illustrated  in  Section  2.1.  Those  re-
petitive  features  also  include  [C-1C0]  and 
[C0C1],  which  implicitly  appear  thrice.  And  it 
is surprising to discover that its better perform-
ance is mainly due to this implicit feature repe-
tition but the authors do not point out this fact. 
As  all  the  features  adopted  in  (Jiang  et  al., 
2008) possess binary values, if a binary feature 
is repeated n times, then it should behave like a 
real-valued feature with its value to be “n”, at 
least  in  principle.  Inspired  by  the  above  dis-
covery, accordingly, we convert all the binary-
value  features  into  their  corresponding  real-
valued  features.  After  having  transformed  bi-
nary  features  into  their  corresponding  real-
valued ones, the original discriminative model 
is re-trained under the ME framework. 

This  new  implementation,  which  would  be 
named  as  the  character-based  discriminative-
plus  model,  just  weights  various  features  dif-
ferently  before  conducting  ME  training.  Af-
terwards, it is further combined with the gen-
erative trigram model, and is called the charac-
ter-based joint-plus model. 
6  Experiments 
The corpora provided by the second SIGHAN 
Bakeoff (Emerson, 2005) were used in our ex-
periments.  The  statistics  of  those  corpora  are 
shown in Table 3. 

Note that the PKU corpus is a little different 
from  others.  In  the  training  set,  Arabic  num-
bers  and  English  characters  are  in  full-width 
form  occupying  two  bytes.  However,  in  the 
testing  set,  these  characters  are  in  half-width 
form occupying only one byte. Most research-
ers  in  the  SIGHAN  Bakeoff  competition  per-
formed  a  conversion  before  segmentation 
(Xiong et al., 2009). In this work, we conduct 

the tests on both unconverted (ucvt.) case and 
converted (cvt.) case. After the conversion, the 
OOV  rate  of  converted  corpus  is  obviously 
lower than that of unconverted corpus. 

To  fairly  compare  the  proposed  approach 
with  previous  works,  we  only  conduct  closed 
tests1.  The  metrics  Precision  (P),  Recall  (R), 
F-score  (F)  (F=2PR/(P+R)),  Recall  of  OOV 
(ROOV)  and  Recall  of  IV  (RIV)  are  used  to 
evaluate the results. 
6.1  Character-Based  Generative  Model 

and Discriminative Model 

For 

As shown in (Wang et al., 2009), the character-
based  generative  trigram  model  significantly 
exceeds its related bigram model and performs 
the same as its 4-gram model. Therefore,  SRI 
Language Modeling  Toolkit2 (Stolcke, 2002) 
is  used  to  train  the  trigram  model  with  modi-
fied  Kneser-Ney  smoothing  (Chen  and  Good-
man,  1998).  Afterwards,  a  beam  search  de-
coder is applied to find out the best sequence. 

the  character-based  discriminative 
model, the ME Package3 given by Zhang Le is 
used to conduct the experiments. Training was 
done with Gaussian prior 1.0 and 300, 150 it-
erations for AS and other corpora respectively.  
ble 5 gives the segmentation results of both 
Ta
the  character-based  generative  model  and  the 
discriminative  model.  From  the  results,  it  can 
be  seen  that  the  generative  model  achieves 
comparable results with the discriminative one 
and  they  outperform  each  other  on  different 
corpus.  However,  the  generative  model  ex-
ceeds the discriminative one on RIV (0.973 vs. 
0.956) but loses on ROOV (0.511 vs. 0.680). It 
illustrates that they complement each other. 
                                                
1 According to the second Sighan Bakeoff regulation, the 
closed test could only use the training data directly pro-
vided. Any other data or information is forbidden, includ-
ing the knowledge of characters set, punctuation set, etc. 
2 http://www.speech.sri.com/projects/srilm/ 
3 http://homepages.inf.ed.ac.uk/lzhang10/maxent_toolkit.html 

 

1178

Joint model performance on Development sets

e
r
o
c
s
-
F

0.9900

0.9800

0.9700

0.9600

0.9500

0.9400

0.9300

0.00

0.10

0.20

0.30

0.40

AS

CITYU

MSR

PKU

0.70

0.80

0.90

1.00

0.50
0.60
alpha

 
Figure 1: Development sets performance of Charac-
ter-based joint model. 
Corpus Set 
AS 

Words  OOV Num OOV Rate

0.026  

122,610 5,308/5,311 0.043/0.043

Development  17,243  445 
Testing 
Development  17,324  355 
MSR  Testing 
CITYU Testing 

Development  12,075  537 

106,873 2,829/2,833 0.026/0.027

40,936  3,028/3,034 0.074/0.074

0.020 

0.044 

PKU 

0.039 

Development  13,576  532 
Testing (ucvt.)  104,372 6,006/6,054 0.058/0.058
Testing (cvt.)  104,372 3,611/3,661 0.035/0.035
Table 4: Corpus statistics for Development sets and 
Testing sets. A “/” separates the OOV number (or 
OOV rate) with respect to the original training sets 
and the new training sets. 
6.2  Character-Based Joint Model 
For  the  character-based  joint  model,  a  devel-
opment set is required to obtain the weight α 
for  its  associated  generative  model.  A  small 
portion of each original training corpus is thus 
extracted  as  the  development  set  and  the  re-
maining  data  is  regarded  as  the  new  training-
set, which is used to train two new parameter-
sets  for  both  generative  and  discriminative 
models associated.  

The last 2,000, 600, 400, and 300 sentences 
for AS, MSR, CITYU, and PKU are extracted 
from the original training corpora as their cor-
responding development sets. The statistics for 
new data sets are shown in Table 4. It can be 
seen that the variation of the OOV rate could 
be  hardly  noticed.  The  F-scores  of  the  joint 
model,  versus  different α,  evaluated  on  four 
development sets are shown in Figure 1. It can 
be  seen  that  the  curves  are  not  sharp  but  flat 
near the top, which indicates that the character-
based  joint  model  is  not  sensitive  to  the  α 
value  selected.  From  those  curves,  the  best 
suitable α for AS, CITYU, MSR and PKU are 
found to be 0.30, 0.60, 0.60 and 0.60, respec-
tively. Those alpha values will then be adopted 
to conduct the experiments on the testing sets. 

 

P 

R 

AS 

MSR 

CITYU

0.946  0

.951 0.707 

PKU 
(ucvt.) 

Corpus Model 

G 
D 
D-Plus 
J 
J-Plus 
G 
D 
D-Plus 
J 
J-Plus 
G 
D 
D-Plus 
J 
J-Plus 
G 
D 
D-Plus 
J 
J-Plus 
G 
D 
D-Plus 
J 
J-Plus 
G 
D 
D-Plus 
J 
J-Plus 
 Segm
odels o

F  ROOV  RIV
0.958 0.938  0.948  0.518  0.978
0
.955
0.967
0.960 0.948  0.954  0.680  0.973
0.962 0.950  0.956  0.679  0.975
0.963 0.949  0.956  0.652  0.977
0.951 0.937  0.944  0.609  0.978
0.941 0.944  0.942  0.708  0.959
0.951 0.952  0.952  0.720  0.970
0.957 0.951  0.954  0.691  0.979
0.959 0.952  0.956  0.700  0.980
0.974 0.967  0.970  0.561  0.985
0.957 0.962  0.960  0.719  0.964
0.965 0.967  0.966  0.675  0.973
0.974 0.971  0.972  0.659  0.983
0.975 0.970  0.972  0.632  0.984
0.929 0.933  0.931  0.435  0.959
0.922 0.941  0.932  0.620  0.941
0.934 0.949  0.941  0.649  0.951
0.935 0.946  0.941  0.561  0.958
0.937 0.947  0.942  0.556  0.960
0.952 0.951  0.952  0.503  0.968
0.940 0.951  0.946  0.685  0.949
0.949 0.958  0.953  0.674  0.958
0.954 0.958  0.956  0.616  0.966
0.955 0.958  0.957  0.610  0.967
0.953 0.946  0.950  0.511  0.973
0.944 0.950  0.947  0.680  0.956
0.952 0.955  0.953  0.676  0.965
0.957 0.955  0.956  0.633  0.971
0.958 0.955  0.957  0.621  0.973
Table 5:
ation r sults of various character-
based m
 
he second SI HAN Bakeoff, the
generative  trigram  model  (G),  the  discriminative 
model (D), the discriminative-plus model (D-Plus), 
the joint model (J) and the joint-plus model (J-Plus). 
 

PKU 
(cvt.) 

ent
n t

Overall

G

e

As  shown  in  Table  5,  the  joint  model  sig-
ificantly  outperforms  both 
the  character-
n
ba
sed generative model and the discriminative 
one in F-score on all the testing corpora. Com-
pared  with  the  generative  approach,  the  joint 
model increases the overall ROOV from 0.510 to 
0.633,  with  the  cost  of  slightly  degrading  the 
overall  RIV  from  0.973  to  0.971.  This  shows 
that the joint model holds the advantage of the 
generative model on IV words. Compared with 
the  discriminative  model,  the  proposed  joint 
model improves the overall RIV from 0.956 to 
0.971,  with  the  cost  of  degrading  the  overall 
ROOV from 0.680 to 0.633. It clearly shows that 
the  joint  model  achieves  a  good  balance  be-
tween IV words and OOV words and achieves 
the best F-scores obtained so far (21% relative 
error  reduction  over  the  discriminative  model 
and 14% over the generative model). 

1179

 

if 

to  be  equal 

6.3  Weigh Various Features Differently 
Inspired by (Jiang et al., 2008), we set the real-
value  of  C0  to  be  2.0,  the  value  of  C-1C0  an
d 
C0C1 to be 3.0, and the values of all other fea-
tures  to  be  1.0  for  the  character-based  dis-
criminative-plus model. Although it seems rea-
sonable  to  weight  those  closely  relevant  fea-
tures more (C0 should be the most relevant fea-
ture for assigning tag t0), both implementations 
seem 
their  corresponding 
lambda-values  are  also  updated  accordingly. 
However,  Table  5  shows  that  this  new  dis-
criminative-plus  implementation  (D-Plus)  sig-
nificantly outperforms the original one (overall 
F-score  is  raised  from  0.947  to  0.953)  when 
both  of  them  adopt  real-valued  features.  It  is 
not clear how this change makes the difference. 
Similar improvements can be observed with 
two  other  ME  packages.  One  anonymous  re-
viewer pointed out that the duplicated features 
should not make difference if there is no regu-
larization.  However,  we  found  that  the  dupli-
cated features would improve the performance 
whether we give Gaussian penalty or not. 

Afterwards,  this  new  implementation  and 
the generative trigram model are further com-
bined (named as the joint-plus model). Table 5 
shows that this joint-plus model also achieves 
better  results  compared  with  the  discrimina-
tive-plus model, which illustrates that our joint 
approach is an effective and robust method for 
CWS.  However,  compared  with  the  original 
joint model, the new joint-plus approach does 
not show much improvement, regardless of the 
significant improvement made by the discrimi-
native-plus  model,  as  the  additional  benefit 
generated by the discriminative-plus model has 
already  covered  by  the  generative  approach 
(Among the 6,965 error words corrected by the 
discriminative-plus  model,  6,292  (90%)  of 
them are covered by the generative model). 
7  Statistical Significance Tests 
Although Table 5 has shown that the proposed 
all  the 
joint  (joint-plus)  model  outperforms 
baselines  mentioned  above,  we  want  to  know 
if  the  difference  is  statistically  significant 
enough  to  make  such  a  claim.  Since  there  is 
only  one  testing  set  for  each  training  corpus, 
the bootstrapping technique (Zhang et al., 2004) 
is adopted to conduct the tests: Giving an  

AS CITYU  MSR  PKU 
Models  
(ucvt.) 
B 
A 
~ 
<  
G 
D 
>  
>  
D-Plus G 
>  
>  
D-Plus D 
>  
>  
J 
G 
>  
>  
J 
D 
>  
>  
J-Plus  G 
~ 
>  
J-Plus  D-Plus
~ 
>  
J 
J-Plus 
Table  6
of 
al 
:  St
among arious charact
 v

>  
<  
>  
>  
>  
>  
>  
~ 
e  t
ode

~ 
>  
>  
>  
>  
>  
>  
>  
ific
ase

anc
est 
d m ls. 

sign
er-b

atistic

PKU
(cvt.)
>  
>  
>  
>  
>  
>  
>  
>  
scor

F-

e 

testing-set T0,  additional M-1  new  testing-sets 
f T0) will 
T0,…,TM-1 (each with the same size o
be  generated  by  repeatedly  re-sampling  data 
from  T0.  Then,  we  will  have  a  total  of  M 
testing-sets (M=2000 in our experiments). 
7.1  Comparisons with Baselines 
eas-
We then follow (Zhang et al., 2004) to m
  the  dis-
ure  the  95%  confidence  interval  for
he  confi-
crepancy  between  two  models.  If  t
 
dence interval does not include the origin point,
we  then  claim  that  system  A  is  significantly 
different from system B. Table 6 gives the re-
sults of significant tests among various models 
mentioned above. In this table, “>” means that 
system A is significantly better than B, where 
as  “<”  denotes  that  system  A  is  significantly 
worse than B, and “~” indicates that these two 
systems are not significantly different. 

As  shown  in  Table  6,  the  proposed  joint 
model is significantly better than the two base-
line models on all corpora. Similarly, 
the pro-
sed  joint-plus  model  also  significantly  out-
po
performs  the  generative  model  and  the  dis-
criminative-plus  model  on  all  corpora  except 
on the PKU(ucvt.). The comparison shows that 
the  proposed  joint  (also  joint-plus)  model  in-
deed exceeds each of its component models. 
7.2  Comparisons with Previous Works 
The above comparison mainly shows the sup
e-
riority  of  the  proposed  joint  model  amo
ng 
those approaches that have been implemente
d. 
However, it would be interesting to know if the 
joint  (and  joint-plus)  model  also  outperforms 
those previous state-of-the-art systems.  

The systems that performed best for at least 
one corpus in the second SIGHAN Bakeoff are 
first  selected  for  comparison.  This  ca
tegory 
includes  (Asahara  et  al.,  2005)  (denoted  as 

1180

sahara05)  and  (Tseng  et  al.,  2005) 4  
A
(Tseng05). (Asahara et al., 2005) achieves the 
best result in the AS corpus, and (Tseng et al., 
2005)  performs  best  in  the  remaining  three 
corpora.  Besides,  those  systems  that  are  re-
ported  to  exceed  the  above  two  systems  are 
also selected. This category includes (Zhang et 
al., 2006) (Zhang06), (Zhang and Clark, 2007) 
(Z&C07)  and  (Jiang  et  al.,  2008)  (Jiang08). 
They  are  briefly  summarized  as  follows. 
(Zhang et al., 2006) is based on sub-word tag-
ging and uses a confidence measure method to 
combine  the  sub-word  CRF  (Lafferty  et  al., 
2001)  and  rule-based  models.  (Zhang  and 
Clark, 2007) uses perceptron (Collins, 2002) to 
generate word candidates  with both word and 
character  features.  Last,  (Jiang  et  al.,  2008)5 
adds repeated features implicitly based on (Ng 
and Low, 2004). All of the above models, ex-
cept (Zhang and Clark, 2007), adopt the char-
acter-based discriminative approach. 

All  the  results  of  the  systems  mentioned 
above are shown in Table 7. Since the systems 
are  not  re-implemented,  we  cannot  generate 
paired  samples  from  those  M  testing
-sets.  In-
ead,  we  calculate  the  95%  confidence  inter-
st
val of the joint (also joint-plus) model. After-
wards,  those  systems  can  be  compared  with 
our proposed models. If the F-score of system 
B does not fall within the 95% confidence in-
terval  of  system  A  (joint  or  joint-plus),  then 
they are statistically significantly different. 

Table 8 gives the results of significant tests 
for those systems mentioned in this section. It 
shows that both our joint-plus model and joint 
model exceed (or are comparable to) almost all 
e state-of-the-art systems across all corpora, 
th
except (Zhang and Clark, 2007) at PKU(ucvt.). 
In that special case, (Zhang and Clark, 2007) 
                                                
4 We  are  not  sure  whether  (Asahara  et  al.,  2005)  and 
(Tseng et al., 2005) performed a conversion before seg-
mentation  in  PKU  corpus.  In  this  paper,  we  followed 
previous works, which cited and compared with them. 
5 The  data  for  (Jiang  et  al.,  2008)  given  at  Table  7  are 
different  from  what  were  reported  at  their  paper.  In  the 
communication with the authors, it is found that the script 
for  evaluating  performance,  provided  by  the  SIGHAN 
Bakeoff, does not work correctly in their platform. After 
the problem is fixed, the re-evaluated real performances 
reported  here  deteriorate  from  their  original  version. 
Please  see  the  announcement  in  Jiang’s  homepage 
(http://mtgroup.ict.ac.cn/~jiangwenbin/papers/error_corre
ction.pdf). 

 

PKU
AS CITYU  MSR  PKU 
(cvt.)
(ucvt.) 
0.952 0.941  0.958  N/A  0.941
0.947 0.943  0.964  N/A  0.950
0.951 0.951  0.971  N/A  0.951
0.946 0.951  0.972  0.945  N/A
0.953 0.948  0.966  0.937  N/A
0.956 0.954  0.972  0.941  0.956
0.956 0.956  0.972  0.942  0.957
sons of F-sco e with revio s 
u
stems. 

 p

r

Corpus

Participants 
Asahara05 
Tseng05 
Zhang06 
Z&C07 
Jiang08 
Our Joint 
Our Joint-Plus 
Table 7: Compari
the-art sy
state-of-
Systems 
B 
A 
Asahara05
Tseng05 
Zhang06 
Z&C07 
Jiang08 
Asahara05
Tseng05 
Zhang06 
Z&C07 
Jiang08 
8:  Statistic

J-Plus

J 

of

  0

e  te
s. 

al  s
f-the
he  jo

> 
> 
~ 
~ 
> 
> 
> 
~ 
~ 
> 
st 

PKU
  (cvt.)
> 
> 
> 
N/A
N/A
> 
> 
> 
N/A
N/A
r 
 F-score  fo

PKU 
AS CITYU MSR  (ucvt.)
N/A 
> 
> 
N/A 
N/A 
> 
< 
> 
> 
> 
> 
N/A 
N/A 
> 
N/A 
> 
< 
> 
~ 
> 
ign
-art
int

> 
> 
~ 
> 
> 
> 
> 
> 
> 
> 
ific
Table 
anc
 syst
previous state-o
em
outpe
-plu model  by .3%  
  on
rforms  t
s 
F- score (0.4% for the joint model). However, 
our joint-plus model exceeds it more over AS 
and  CITYU  corpora  by  1.0%
  and  0.5%,  re-
spectively (1.0% and 0.3% for the joint model). 
Thus, it is fair to say that both our joint model 
and joint-plus model are superior to the state-
of-the-art systems reported in the literature. 
8  Conclusion 
From the error analysis of the character-based 
generative  model  and  the  discriminative  o
ne, 
we  found  that  thes
e  two  models  complement 
each  other  on  han
dling  IV  words  and  OOV 
words.  To  take  advantage  of  these  two  ap-
proaches,  a  joint  model  is  thus  proposed  to 
combine  them.  Experiments  on  the  Second 
SIGHAN  Bakeoff  show  that  the  joint  model 
achieves  21%  error  reduction  over  the  dis-
criminative  model  (14%  over  the  generative 
model).  Moreover,  closed  tests  on  the  second 
SIGHAN Bakeoff corpora show that this joint 
model  significantly  outperforms  all  the  state-
of-the-art systems reported in the literature. 

Last, it is found that weighting various fea-
tures differently would give better result. How-
ever,  further  study  is  required  to  find  out  the 
true reason for this strange but interesting ph
e-
nomenon.  

1181

cknowledgement 

A
The authors extend sincere thanks to Wenbing 
Jiang for his helps with our experiments. Also, 
we  thank  Behavior  Design  Corporation  for 
using  their
  Generic-Beam-Search  code  and 
show  special  thanks  t
o  Ms.  Nanyan  Kuo  for 
her helps with the Gen
eric-Beam-Search code.  
The research work has been partially funded 
by  the  Natural  Science  Foundation  of  China 
under  Grant  No.  60975053,  90820303  and 
60736014, the National Key Technology R&D 
Program  under  Grant  No.  2006BAH03B02, 
and  also the  Hi-Tech  Research  and  Develop-
ent Program (“863” Program) of China under 
m
Grant No. 2006AA010108-4 as well. 

References 
Masayuki  Asahara,  Kenta  Fukuoka,  Ai  Azuma, 
Chooi-Ling  Goh,  Yotaro  Watanabe,  Yuji  Ma-
tsumoto  and  Takashi  Tsuzuki,  2005.  Combina-
optimum 
tion  of  machine  learning  methods  for 
entation.  In  Proceedings  of 
Chinese  word  segm
GHAN Workshop on Chinese Lan-
the Fourth SI
guage Processing, pages 134–137, Jeju, Korea. 

Christopher  M.  Bishop,  2006.  Pattern  recognition 

and machine learning. New York: Springer  
anley  F.  Chen  and  Joshua  Goodman,  1998.  An 
empirical study of smoothing techniques for lan-
guage  modeling.  Technical  Report  TR-10-98, 
Harvard  University  Center  for  Research  in 
Computing Technology. 

St

Jia

Th

Michael  Collins,  2002.  Discriminative  training 
methods for hidden markov models: theory and 
experiments with perceptron algorithms. In Pro-
ceedings of EMNLP, pages 1-8, Philadelphia. 
omas  Emerson,  2005.  The  second  international 
Chinese word segmentation bakeoff. In Proceed-
ings  of  the  Fourth  SIGHAN  Workshop  on  Chi-
nese Language Processing, pages 123-133. 
nfeng Gao, Mu Li and Chang-Ning Huang, 2003. 
Improved  Source-Channel  Models  for  Chinese 
Word  Segmentation.  In  Proceedings  of  ACL, 
pages 272-279. 
enbin  Jiang,  Liang Huang,  Qun  Liu  and Yajuan 
Lu,  2008.  A  Cascaded  Linear  Model  for  Joint 
Chinese Word Segmentation and Part-of-Speech 
Tagging. In Proceedings of ACL, pages 897-904. 
hn  Lafferty,  Andrew  McCallum  and  Fernando 
Pereira, 2001. Conditional Random Fields: Prob-
abilistic  Models  for  Segmenting  and  Labeling 
Sequence Data. In Proceedings of ICML, pages 
282-289. 
ee  Tou  Ng  and  Jin  Kiat  Low,  2004.  Chinese 
part-of-speech  tagging:  one-at-a-time  or  all-at-

Hw

W

Jo

Fu

Ad

once?  word-based  or  character-based.  In  Pro-
ceedings of EMNLP, pages 277-284. 
chun  Peng,  Fangfang  Feng  and  Andrew 
McCallum, 2004. Chinese segmentation and new 
ction using conditional random fields. 
word dete
In Proceedings of COLING, pages 562–568. 
wait  Ratnaparkhi,  1998.  Maximum  entropy 
models  for  natural  language  ambiguity  resolu-
tion. University of Pennsylvania. 

Yi

Hu

Ku

Andreas  Stolcke,  2002.  SRILM-an  extensible  lan-
guage  modeling  toolkit.  In  Proceedings  of  the 
International  Conference  on  Spoken  Language 
Processing, pages 311-318. 

Kristina  Toutanova,  2006.  Competitive  generative 
models with structure learning for NLP classifi-
MNLP,  pages 
cation  tasks.  In  Proceedings  of  E
576-584, Sydney, Australia. 
ihsin  Tseng,  Pichuan  Chang,  Galen  Andrew, 
Daniel Jurafsky and Christopher Manning, 2005. 
ld  Word  Segmenter 
A  Conditional  Random  Fie
for Sighan Bakeoff 2005. In Proceedings of the 
Fourth  SIGHAN  Workshop  on  Chinese  Lan-
guage Processing, pages 168-171. 
d Keh-Yih Su, 2009. 
n Wang, Chengqing Zong an
Which  is  more  suitable  for  Chinese  word  seg-
mentation, the generative model or the discrimi-
native  one?  In  Proceedings  of  PACLIC,  pages 
827-834, Hong Kong, China. 
ng Xiong, Jie Zhu, Hao Huang and Haihua Xu, 
scriminative 
2009.  Minimum  tag  error  for  di
training  of  conditional  random  fields.  Informa-
tion Sciences, 179 (1-2). pages 169-179. 
anwen  Xue,  2003.  Chinese  Word  Segmentation 
as Character Tagging. Computational Linguistics 
ssing, 8 (1). pages 
and Chinese Language Proce
29-48. 
aping Zhang, Hongkui Yu, Deyi Xiong and Qun 
Liu,  2003.  HHMM-based  Chinese  lexical  ana-
Second 
lyzer  ICTCLAS.  In  Proceedings  of  the 
SIGHAN Workshop on Chinese Language Proc-
essing, pages 184–187. 
iqiang  Zhang,  Genichiro  Kikui  and  Eiichiro 
2006. Subword-based Tagging for Con-
Sumita, 
fidence-dependent Chinese Word Segmentation. 
In Proceedings of the COLING/ACL, pages 961-
968, Sydney, Australia. 
 
ng Zhang, Stephan Vogel and Alex Waibel, 2004.
 scores: How much im-
Interpreting BLEU/NIST
provement do we need to have a better system. 
In Proceedings of LREC, pages 2051–2054. 
e Zhang and Stephen Clark, 2007. Chinese Seg-
mentation with a Word-Based Perceptron Algo-
f  ACL,  pages  840-847, 
rithm.  In  Proceedings  o
Prague, Czech Republic. 

Hu

Yu

Ru

Ni

Yi

