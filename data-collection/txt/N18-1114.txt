



















































Tensor Product Generation Networks for Deep NLP Modeling


Proceedings of NAACL-HLT 2018, pages 1263–1273
New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics

Tensor Product Generation Networks for Deep NLP Modeling

Qiuyuan Huang1, Paul Smolensky1, Xiaodong He2, Li Deng1∗, Dapeng Wu3
1Microsoft Research, WA, USA; 2JD AI Research; 3University of Florida, FL, USA

{qihua,psmo}@microsoft.com; xiaodong.he@jd.com; l.deng@ieee.org
dpwu@ufl.edu

Abstract

We present a new approach to the design of
deep networks for natural language processing
(NLP), based on the general technique of Ten-
sor Product Representations (TPRs) for encod-
ing and processing symbol structures in dis-
tributed neural networks. A network architec-
ture — the Tensor Product Generation Net-
work (TPGN) — is proposed which is ca-
pable in principle of carrying out TPR com-
putation, but which uses unconstrained deep
learning to design its internal representations.
Instantiated in a model for image-caption
generation, TPGN outperforms LSTM base-
lines when evaluated on the COCO dataset.
The TPR-capable structure enables interpreta-
tion of internal representations and operations,
which prove to contain considerable gram-
matical content. Our caption-generation model
can be interpreted as generating sequences of
grammatical categories and retrieving words
by their categories from a plan encoded as a
distributed representation.

1 Introduction

In this paper we introduce a new architecture for
natural language processing (NLP). On what type
of principles can a computational architecture be
founded? It would seem a sound principle to re-
quire that the hypothesis space for learning which
an architecture provides include network hypothe-
ses that are independently known to be suitable
for performing the target task. Our proposed ar-
chitecture makes available to deep learning net-
work configurations that perform natural language
generation by use of Tensor Product Represen-
tations (TPRs) (Smolensky and Legendre, 2006).
Whether learning will create TPRs is unknown in
advance, but what we can say with certainty is that
the hypothesis space being searched during learn-

∗LD is currently at Citadel.

ing includes TPRs as one appropriate solution to
the problem.

TPRs are a general method for generating
vector-space embeddings of complex symbol
structures. Prior work has proved that TPRs en-
able powerful symbol processing to be carried
out using neural network computation (Smolen-
sky, 2012). This includes generating parse trees
that conform to a grammar (Cho et al., 2017), al-
though incorporating such capabilities into deep
learning networks such as those developed here re-
mains for future work. The architecture presented
here relies on simpler use of TPRs to generate sen-
tences; grammars are not explicitly encoded here.

We test the proposed architecture by applying
it to image-caption generation (on the MS-COCO
dataset, (COCO, 2017)). The results improve upon
a baseline deploying a state-of-the-art LSTM ar-
chitecture (Vinyals et al., 2015), and the TPR
foundations of the architecture provide greater in-
terpretability.

Section 2 of the paper reviews TPR. Section 3
presents the proposed architecture, the Tensor
Product Generation Network (TPGN). Section 4
describes the particular model we study for im-
age captioning, and Section 5 presents the exper-
imental results. Importantly, what the model has
learned is interpreted in Section 5.3. Section 6 dis-
cusses the relation of the new model to previous
work and Section 7 concludes.

2 Review of tensor product
representation

The central idea of TPRs (Smolensky, 1990) can
be appreciated by contrasting the TPR for a word
string with a bag-of-words (BoW) vector-space
embedding. In a BoW embedding, the vector that
encodes Jay saw Kay is the same as the one
that encodes Kay saw Jay: J + K + s where

1263



J,K, s are respectively the vector embeddings of
the words Jay, Kay, saw.

A TPR embedding that avoids this confusion
starts by analyzing Jay saw Kay as the set
{Jay/SUBJ, Kay/OBJ, saw/VERB}. (Other anal-
yses are possible: see Section 3.) Next we choose
an embedding in a vector space VF for Jay, Kay,
saw as in the BoW case: J,K, s. Then comes the
step unique to TPRs: we choose an embedding in
a vector space VR for the roles SUBJ, OBJ, VERB:
rSUBJ, rOBJ, rVERB. Crucially, rSUBJ 6= rOBJ. Finally,
the TPR for Jay saw Kay is the following vec-
tor in VF ⊗ VR:

vJay saw Kay = J⊗ rSUBJ +K⊗ rOBJ + s⊗ rVERB
(1)

Each word is tagged with the role it fills in the sen-
tence; Jay and Kay fill different roles.

This TPR avoids the BoW confusion:
vJay saw Kay 6= vKay saw Jay because
J⊗ rSUBJ + K⊗ rOBJ 6= J ⊗ rOBJ + K⊗ rSUBJ.
In the terminology of TPRs, in Jay saw Kay,
Jay is the filler of the role SUBJ, and J ⊗ rSUBJ
is the vector embedding of the filler/role binding
Jay/SUBJ. In the vector space embedding, the
binding operation is the tensor — or generalized
outer — product ⊗; i.e., J⊗ rSUBJ is a tensor with
2 indices defined by: [J⊗ rSUBJ]ϕρ ≡ [J]ϕ[rSUBJ]ρ.

The tensor product can be used recursively,
which is essential for the TPR embedding of recur-
sive structures such as trees and for the computa-
tion of recursive functions over TPRs. However, in
the present context, recursion will not be required,
in which case the tensor product can be regarded
as simply the matrix outer product (which cannot
be used recursively); we can regard J⊗rSUBJ as the
matrix product Jr>SUBJ. Then Equation 1 becomes

vJay saw Kay = Jr
>
SUBJ + Kr

>
OBJ + sr

>
VERB (2)

Note that the set of matrices (or the set of ten-
sors with any fixed number of indices) is a vec-
tor space; thus Jay saw Kay 7→ vJay saw Kay
is a vector-space embedding of the symbol struc-
tures constituting sentences. Whether we regard
vJay saw Kay as a 2-index tensor or as a matrix,
we can call it simply a ‘vector’ since it is an el-
ement of a vector space: in the context of TPRs,
‘vector’ is used in a general sense and should not
be taken to imply a single-indexed array.

Crucial to the computational power of TPRs and
to the architecture we propose here is the notion of
unbinding. Just as an outer product — the tensor

product — can be used to bind the vector embed-
ding a filler Jay to the vector embedding a role
SUBJ, J⊗ rSUBJ or Jr>SUBJ, so an inner product can
be used to take the vector embedding a structure
and unbind a role contained within that structure,
yielding the symbol that fills the role.

In the simplest case of orthonormal role vec-
tors ri, to unbind role SUBJ in Jay saw
Kay we can compute the matrix-vector product:
vJay saw KayrSUBJ = J (because r>i rj = δij
when the role vectors are orthonormal). A simi-
lar situation obtains when the role vectors are not
orthonormal, provided they are not linearly depen-
dent: for each role such as SUBJ there is an un-
binding vector uSUBJ such that r>i uj = δij so we
get: vJay saw KayuSUBJ = J. A role vector such
as rSUBJ and its unbinding vector uSUBJ are said
to be duals of each other. (If R is the matrix in
which each column is a role vector rj , thenR is in-
vertible when the role vectors are linearly indepen-
dent; then the unbinding vectors ui are the rows of
R−1. When the rj are orthonormal, ui = ri. Re-
placing the matrix inverse with the pseudo-inverse
allows approximate unbinding if the role vectors
are linearly dependent.)

We can now see how TPRs can be used to gener-
ate a sentence one word at a time. We start with the
TPR for the sentence, e.g., vJay saw Kay. From
this vector we unbind the role of the first word,
which is SUBJ: the embedding of the first word
is thus vJay saw KayuSUBJ = J, the embedding
of Jay. Next we take the TPR for the sentence
and unbind the role of the second word, which is
VERB: the embedding of the second word is then
vJay saw KayuVERB = s, the embedding of saw.
And so on.

To accomplish this, we need two representa-
tions to generate the tth word: (i) the TPR of the
sentence, S (or of the string of not-yet-produced
words, St) and (ii) the unbinding vector for the tth

word, ut. The architecture we propose will there-
fore be a recurrent network containing two subnet-
works: (i) a subnet S hosting the representation St,
and a (ii) a subnet U hosting the unbinding vector
ut. This is shown in Fig. 1.

3 A TPR-capable generation
architecture

As Fig. 1 shows, the proposed Tensor Product
Generation Network architecture (the dashed box
labeled N ) is designed to support the technique

1264



Figure 1: Architecture of TPGN, a TPR-capable generation network. “2×” denotes the matrix-vector product.

for generation just described: the architecture is
TPR-capable. There is a sentence-encoding sub-
network S which could host a TPR of the sen-
tence to be generated, and an unbinding subnet-
work U which could output a sequence of unbind-
ing vectors ut; at time t, the embedding ft of the
word produced, xt, could then be extracted from
St via the matrix-vector product (shown in the fig-
ure by “2×”): ft = Stut. The lexical-decoding sub-
network L converts the embedding vector ft to the
1-hot vector xt corresponding to the word xt.

Unlike some other work (Palangi et al., 2017),
TPGN is not constrained to literally learn TPRs.
The representations that will actually be housed in
S and U are determined by end-to-end deep learn-
ing on a task: the bubbles in Fig. 1 show what
would be the meanings of St,ut and ft if an ac-
tual TPR scheme were instantiated in the archi-
tecture. The learned representations St will not be
proven to literally be TPRs, but by analyzing the
unbinding vectors ut the network learns, we will
gain insight into the process by which the learned
matrices St give rise to the generated sentence.

The task studied here is image captioning; Fig.
1 shows that the input to this TPGN model is an
image, preprocessed by a CNN which produces
the initial representation in S, S0. This vector
S0 drives the entire caption-generation process:
it contains all the image-specific information for
producing the caption. (We will call a caption a
“sentence” even though it may in fact be just a
noun phrase.)

The two subnets S and U are mutually-
connected LSTMs (Hochreiter and Schmidhuber,
1997): see Fig. 2. The internal hidden state of U ,
pt, is sent as input to S; U also produces output,
the unbinding vector ut. The internal hidden state

of S, St, is sent as input to U , and also produced
as output. As stated above, these two outputs are
multiplied together to produce the embedding vec-
tor ft = Stut of the output word xt. Furthermore,
the 1-hot encoding xt of xt is fed back at the next
time step to serve as input to both S and U .

What type of roles might the unbinding vec-
tors be unbinding? A TPR for a caption could
in principle be built upon positional roles, syn-
tactic/semantic roles, or some combination of the
two. In the caption a man standing in a
room with a suitcase, the initial a and
man might respectively occupy the positional
roles of POS(ITION)1 and POS2; standing
might occupy the syntactic role of VERB; in the
role of SPATIAL-P(REPOSITION); while a room
with a suitcase might fill a 5-role schema
DET(ERMINER)1 N(OUN)1 P DET2 N2. In fact
we will provide evidence in Sec. 5.3.2 that our net-
work learns just this kind of hybrid role decompo-
sition; further evidence for these particular roles is
presented elsewhere.

What form of information does the sentence-
encoding subnetwork S need to encode in S? Con-
tinuing with the example of the previous para-
graph, S needs to be some approximation to the
TPR summing several filler/role binding matrices.
In one of these bindings, a filler vector fa — which
the lexical subnetwork L will map to the article a
— is bound (via the outer product) to a role vec-
tor rPOS1 which is the dual of the first unbinding
vector produced by the unbinding subnetwork U :
uPOS1 . In the first iteration of generation the model
computes S1uPOS1 = fa, which L then maps to a.
Analogously, another binding approximately con-
tained in S2 is fmanr>POS2 . There are correspond-
ing approximate bindings for the remaining words

1265



of the caption; these employ syntactic/semantic
roles. One example is fstandingr>V . At iteration
3, U decides the next word should be a verb, so
it generates the unbinding vector uV which when
multiplied by the current output of S , the matrix
S3, yields a filler vector fstanding which L maps
to the output standing. S decided the caption
should deploy standing as a verb and included
in S an approximation to the binding fstandingr>V .
It similarly decided the caption should deploy in
as a spatial preposition, approximately including
in S the binding finr>SPATIAL-P; and so on for the
other words in their respective roles in the caption.

4 System Description

As stated above, the unbinding subnetwork U and
the sentence-encoding subnetwork S of Fig. 1
are each implemented as (1-layer, 1-directional)
LSTMs (see Fig. 2); the lexical subnetwork L is
implemented as a linear transformation followed
by a softmax operation.

In the equations below, the LSTM variables in-
ternal to the S subnet are indexed by 1 (e.g., the
forget-, input-, and output-gates are respectively
f̂1, î1, ô1) while those of the unbinding subnet U
are indexed by 2.

Thus the state updating equations for S are, for
t = 1, · · · , T = caption length:

f̂1,t = σg(W1,fpt−1 −D1,fWext−1 +U1,f Ŝt−1) (3)
î1,t = σg(W1,ipt−1 −D1,iWext−1 +U1,iŜt−1) (4)

ô1,t = σg(W1,opt−1 −D1,oWext−1 +U1,oŜt−1) (5)
g1,t = σh(W1,cpt−1 −D1,cWext−1 +U1,cŜt−1) (6)

c1,t = f̂1,t � c1,t−1 + î1,t � g1,t (7)
Ŝt = ô1,t � σh(c1,t) (8)

Here f̂1,t, î1,t, ô1,t, g1,t, c1,t, Ŝt ∈ Rd×d, pt ∈ Rd;
σg(·) is the (element-wise) logistic sigmoid func-
tion; σh(·) is the hyperbolic tangent function; the
operator � denotes the Hadamard (element-wise)
product; W1,f ,W1,i,W1,o,W1,c ∈ R(d×d)×d,
D1,f , D1,i, D1,o, D1,c ∈ R(d×d)×d, U1,f , U1,i,
U1,o, U1,c ∈ R(d×d)×(d×d). For clarity, biases
— included throughout the model — are omitted
from all equations in this paper. The initial state
Ŝ0 is initialized by:

Ŝ0 = Cs(v − v̄) (9)

where v ∈ R2048 is the vector of visual features
extracted from the current image by ResNet (Gan
et al., 2017) and v̄ is the mean of all such vectors;
Cs ∈ R(d×d)×2048. On the output side, xt ∈ RV is

a 1-hot vector with dimension equal to the size of
the caption vocabulary, V , and We ∈ Rd×V is a
word embedding matrix, the i-th column of which
is the embedding vector of the i-th word in the vo-
cabulary; it is obtained by the Stanford GLoVe al-
gorithm with zero mean (Pennington et al., 2017).
x0 is initialized as the one-hot vector correspond-
ing to a “start-of-sentence” symbol.

For U in Fig. 1, the state updating equations are:

f̂2,t = σg(Ŝt−1w2,f −D2,fWext−1 +U2,fpt−1) (10)
î2,t = σg(Ŝt−1w2,i −D2,iWext−1 +U2,ipt−1) (11)

ô2,t = σg(Ŝt−1w2,o −D2,oWext−1 +U2,opt−1) (12)
g2,t = σh(Ŝt−1w2,c −D2,cWext−1 +U2,cpt−1) (13)

c2,t = f̂2,t � c2,t−1 + î2,t � g2,t (14)
pt = ô2,t � σh(c2,t) (15)

Here w2,f ,w2,i,w2,o,w2,c ∈ Rd, D2,f , D2,i,
D2,o, D2,c ∈ Rd×d, and U2,f , U2,i, U2,o, U2,c
∈ Rd×d. The initial state p0 is the zero vector.

The dimensionality of the crucial vectors shown
in Fig. 1, ut and ft, is increased from d×1 to d2×1
as follows. A block-diagonal d2 × d2 matrix St is
created by placing d copies of the d× d matrix Ŝt
as blocks along the principal diagonal. This matrix
is the output of the sentence-encoding subnetwork
S . Now the ‘filler vector’ ft ∈ Rd2 — ‘unbound’
from the sentence representation St with the ‘un-
binding vector’ ut — is obtained by Eq. (16).

ft = Stut (16)

Here ut ∈ Rd2 , the output of the unbinding sub-
network U , is computed as in Eq. (17), where
Wu ∈ Rd2×d is U’s output weight matrix.

ut = σh(Wupt) (17)

Finally, the lexical subnetworkL produces a de-
coded word xt ∈ RV by

xt = σs(Wxft) (18)

where σs(·) is the softmax function and Wx ∈
RV×d2 is the overall output weight matrix. Since
Wx plays the role of a word de-embedding matrix,
we can set

Wx = (We)
> (19)

where We is the word-embedding matrix. Since
We is pre-defined, we directly set Wx by Eq. (19)
without training L through Eq. (18). Note that S
and U are learned jointly through end-to-end train-
ing as shown in Algorithm 1.

1266



Figure 2: The sentence-encoding subnet S and the unbinding subnet U are inter-connected LSTMs; v encodes the
visual input while the xt encode the words of the output caption.

Algorithm 1 End-to-end training of S and U
Input: Image feature vector v(i) and corresponding caption
X(i) = [x

(i)
1 , · · · , x(i)T ] (i = 1 , · · · , N ), where N is the

total number of samples.
Output: W1,f ,W1,i,W1,o,W1,c,Cs,D1,f ,D1,i,D1,o,
D1,c,U1,f ,U1,i,U1,o,U1,c,w2,f ,w2,i,w2,o,w2,c,D2,f ,
D2,i,D2,o,D2,c,U2,f ,U2,i,U2,o,U2,c,Wu,Wx.

1: Initialize S0 by (9);
2: Initialize x0 as the one-hot vector corresponding to the

start-of-sentence symbol;
3: Initialize p0 as the zero vector;
4: Randomly initialize weights W1,f ,W1,i,W1,o,

W1,c,Cs,D1,f ,D1,i,D1,o,D1,c,U1,f ,U1,i,U1,o,
U1,c,w2,f ,w2,i,w2,o,w2,c,D2,f ,D2,i,D2,o,
D2,c,U2,f ,U2,i,U2,o,U2,c,Wu,Wx;

5: for n from 1 to N do
6: for t from 1 to T do
7: Calculate (3) – (8) to obtain St;
8: Calculate (10) – (15) to obtain pt;
9: Calculate (17) to obtain ut;

10: Calculate (16) to obtain ft;
11: Calculate (18) to obtain xt;
12: Update weights W1,f ,W1,i,W1,o,W1,c,Cs,

D1,f ,D1,i,D1,o,D1,c,U1,f ,U1,i,U1,o,U1,c,
w2,f ,w2,i,w2,o,w2,c,D2,f ,D2,i,D2,o,D2,c,
U2,f ,U2,i,U2,o,U2,c,Wu
by the back-propagation algorithm;

13: end for
14: end for

5 Experimental results

5.1 Dataset

To evaluate the performance of our proposed
model, we use the COCO dataset (COCO, 2017).
The COCO dataset contains 123,287 images, each
of which is annotated with at least 5 captions. We
use the same pre-defined splits as in (Karpathy
and Fei-Fei, 2015; Gan et al., 2017): 113,287 im-
ages for training, 5,000 images for validation, and
5,000 images for testing. We use the same vocabu-

lary as that employed in (Gan et al., 2017), which
consists of 8,791 words.

5.2 Evaluation
For the CNN of Fig. 1, we used ResNet-152 (He
et al., 2016), pretrained on the ImageNet dataset.
The feature vector v has 2048 dimensions. Word
embedding vectors in We are downloaded from
the web (Pennington et al., 2017). The model is
implemented in TensorFlow (Abadi et al., 2015)
with the default settings for random initialization
and optimization by backpropagation.

In our experiments, we choose d = 25 (where d
is the dimension of vector pt). The dimension of
St is 625 × 625 (while Ŝt is 25 × 25); the vocab-
ulary size V = 8, 791; the dimension of ut and ft
is d2 = 625.

The main evaluation results on the MS COCO
dataset are reported in Table 5.2. The widely-
used BLEU (Papineni et al., 2002), METEOR
(Banerjee and Lavie, 2005), and CIDEr (Vedan-
tam et al., 2015) metrics are reported in our quan-
titative evaluation of the performance of the pro-
posed model. In evaluation, our baseline is the
widely used CNN-LSTM captioning method orig-
inally proposed in (Vinyals et al., 2015). For com-
parison, we include results in that paper in the
first line of Table 5.2. We also re-implemented the
model using the latest ResNet features and report
the results in the second line of Table 5.2. Our
re-implementation of the CNN-LSTM method
matches the performance reported in (Gan et al.,
2017), showing that the baseline is a state-of-the-
art implementation. For TPGN, we use parameter
settings in a similar range to those in (Gan et al.,
2017). TPGN has comparable, although slightly

1267



Methods METEOR BLEU-1 BLEU-2 BLEU-3 BLEU-4 CIDEr
NIC (Vinyals et al., 2015) 0.237 0.666 0.461 0.329 0.246 0.855
CNN-LSTM 0.238 0.698 0.525 0.390 0.292 0.889
TPGN 0.243 0.709 0.539 0.406 0.305 0.909

Table 1: Performance of the proposed TPGN model on the COCO dataset.

more, parameters than the CNN-LSTM. The train-
ing time of TPGN is roughly 50% more than the
CNN-LSTM model. The weights in TPGN are
updated at every mini-batch; in the experiments,
we use a batch size of 64 images. As shown
in Table 5.2, compared to the CNN-LSTM base-
line, the proposed TPGN appreciably outperforms
the benchmark schemes in all metrics across the
board. The improvement in BLEU-n is greater for
greater n; TPGN particularly improves generation
of longer subsequences. The results attest to the
effectiveness of the TPGN architecture.

It is worth mentioning that this paper is aimed
at developing a Tensor Product Representation
(TPR) inspired network to replace the core lay-
ers in an LSTM; therefore, it is directly compa-
rable to an LSTM baseline. So in the experiments,
we focus on comparison to a strong CNN-LSTM
baseline. We acknowledge that more recent papers
(Xu et al., 2017; Rennie et al., 2017; Yao et al.,
2017; Lu et al., 2017; Gan et al., 2017) reported
better performance on the task of image caption-
ing. Performance improvements in these more re-
cent models are mainly due to using better image
features such as those obtained by Region-based
Convolutional Neural Networks (R-CNN), or us-
ing reinforcement learning (RL) to directly opti-
mize metrics such as CIDEr, or using more com-
plex attention mechanisms (Gan et al., 2017) to
provide a better context vector for caption gener-
ation, or using an ensemble of multiple LSTMs,
among others. However, the LSTM is still play-
ing a core role in these works and we believe im-
provement over the core LSTM, in both perfor-
mance and interpretability, is still very valuable;
that is why we compare the proposed TPGN with
a state-of-the-art native LSTM (the second line of
Table 5.2).

5.3 Interpretation of learned unbinding
vectors

To get a sense of how the sentence encodings St
learned by TPGN approximate TPRs, we now in-
vestigate the meaning of the role-unbinding vec-

tor ut the model uses to unbind from St — via
Eq. (16) — the filler vector ft that produces — via
Eq. (18) — the one-hot vector xt of the tth gener-
ated caption word. The meaning of an unbinding
vector is the meaning of the role it unbinds. Inter-
preting the unbinding vectors reveals the meaning
of the roles in a TPR that S approximates.

Figure 3: Unbinding vectors of 1000 words; different
POS tags of words are represented by different colors.

5.3.1 Visualization of ut
We run the TPGN model with 5,000 test images
as input, and obtain the unbinding vector ut used
to generate each word xt in the caption of a test
image. We plot 1,000 unbinding vectors ut, which
correspond to the first 1,000 words in the result-
ing captions of these 5,000 test images. There are
17 parts of speech (POS) in these 1,000 words.
The POS tags are obtained by the Stanford Parser
(Manning, 2017).

We use the Embedding Projector in Tensor-
Board (Google, 2017) to plot 1,000 unbinding vec-
tors ut with a custom linear projection in Tensor-
Board to reduce 625 dimensions of ut to 2 dimen-
sions shown in Fig. 3 through Fig. 7.

Fig. 3 shows the unbinding vectors of 1000
words; different POS tags of words are represented
by different colors. In fact, we can partition the
625-dim space of ut into 17 regions, each of which

1268



contains 76.3% words of the same type of POS on
average; i.e., each region is dominated by words
of one POS type. This clearly indicates that each
unbinding vector contains important grammatical
information about the word it generates. As exam-
ples, Fig. 4 to Fig. 7 show the distribution of the
unbinding vectors of nouns, verbs, adjectives, and
prepositions, respectively. Furthermore, we show
that the subject and the object of a sentence can be
distinguished based on ut in (Huang et al., 2018).

Figure 4: Unbinding vectors of 360 nouns in red and
640 words of other types of POS in grey.

Figure 5: Unbinding vectors of 81 verbs in red and 919
words of other types of POS in grey.

5.3.2 Clustering of ut
Since the previous section indicates that there is a
clustering structure for ut, in this section we parti-
tion ut into Nu clusters and examine the grammar
roles played by ut.

First, we run the trained TPGN model on
the 113,287 training images, obtaining the role-

Figure 6: Unbinding vectors of 55 adjectives in red and
945 words of other types of POS in grey.

Figure 7: Unbinding vectors of 169 prepositions in red
and 831 words of other types of POS in grey.

unbinding vector ut used to generate each word xt
in the caption sentence. There are approximately
1.2 million ut vectors over all the training im-
ages. We apply the K-means clustering algorithm
to these vectors to obtain Nu clusters and the cen-
troid µi of each cluster i (i = 0, · · · , Nu − 1).

Then, we run the TPGN model with 5,000 test
images as input, and obtain the role vector ut of
each word xt in the caption sentence of a test im-
age. Using the nearest neighbor rule, we obtain the
index i of the cluster that each ut is assigned to.

The partitioning of the unbinding vectors ut
into Nu = 2 clusters exposes the most funda-
mental distinction made by the roles. We find that
the vectors assigned to Cluster 1 generate words
which are nouns, pronouns, indefinite and definite
articles, and adjectives, while the vectors assigned
to Cluster 0 generate verbs, prepositions, conjunc-
tions, and adverbs. Thus Cluster 1 contains the
noun-related words, Cluster 0 the verb-like words

1269



Category Nw Nr Pc
Nouns 16683 16115 0.969
Pronouns 462 442 0.957
Indefinite articles 7248 7107 0.981
Definite articles 797 762 0.956
Adjectives 2543 2237 0.880
Verbs 3558 3409 0.958
Prepositions & conjunctions 8184 7859 0.960
Adverbs 13 8 0.615

Table 2: Conformity to N/V generalization (Nu = 2).

ID Interpretation (proportion)
2 Position 1 (1.00)
3 Position 2 (1.00)
1 Noun (0.54), Determiner (0.43)
5 Determiner (0.50), Noun (0.19), Preposition (0.15)
7 Noun (0.88), Adjective (0.09)
9 Determiner (0.90), Noun (0.10)
0 Preposition (0.64), . (0.16), V (0.14)
4 Preposition: spatial (0.72) non-spatial (0.19)
6 Preposition (0.59), . (0.14)
8 Verb (0.37), Preposition (0.36), . (0.20)

Table 3: Interpretation of unbinding clusters (Nu = 10)

(verbs, prepositions and conjunctions are all po-
tentially followed by noun-phrase complements,
for example). Cross-cutting this distinction is an-
other dimension, however: the initial word in a
caption (always a determiner) is sometimes gen-
erated with a Cluster 1 unbinding vector, some-
times with a Cluster 0 vector. Outside the caption-
initial position, exceptions to the nominal/verbal
∼ Cluster 1/0 generalization are rare, as attested
by the high rates of conformity to the generaliza-
tion shown in Table 5.3.1.

Table 5.3.1 shows the likelihood of correctness
of this ‘N/V’ generalization for the words in 5,000
sentences captioned for the 5,000 test images; Nw
is the number of words in the category, Nr is the
number of words conforming to the generaliza-
tion, and Pc = Nr/Nw is the proportion conform-
ing. We use the Natural Language Toolkit (NLTK,
2017) to identify the part of speech of each word
in the captions.

A similar analysis with Nu = 10 clusters re-
veals the results shown in Table 5.3.1; these re-
sults concern the first 100 captions, which were
inspected manually to identify interpretable pat-
terns. (More comprehensive results will be dis-
cussed elsewhere.)

The clusters can be interpreted as falling into
3 groups (see Table 5.3.1). Clusters 2 and 3 are
clearly positional roles: every initial word is gen-
erated by a role-unbinding vector from Cluster 2,

and such vectors are not used elsewhere in the
string. The same holds for Cluster 3 and the sec-
ond caption word.

For caption words after the second word, po-
sition is replaced by syntactic/semantic proper-
ties for interpretation purposes. The vector clus-
ters aside from 2 and 3 generate words with a
dominant grammatical category: for example, un-
binding vectors assigned to the cluster 4 gener-
ate words that are 91% likely to be prepositions,
and 72% likely to be spatial prepositions. Cluster
7 generates 88% nouns and 9% adjectives, with the
remaining 3% scattered across other categories.
As Table 5.3.1 shows, clusters 1, 5, 7, 9 are pri-
marily nominal, and 0, 4, 6, and 8 primarily verbal.
(Only cluster 5 spans the N/V divide.)

6 Related work

This work follows a great deal of recent caption-
generation literature in exploiting end-to-end deep
learning with a CNN image-analysis front end
producing a distributed representation that is then
used to drive a natural-language generation pro-
cess, typically using RNNs (Mao et al., 2015;
Vinyals et al., 2015; Devlin et al., 2015; Chen
and Zitnick, 2015; Donahue et al., 2015; Karpa-
thy and Fei-Fei, 2015; Kiros et al., 2014a,b; Xu
et al., 2017; Rennie et al., 2017; Yao et al., 2017;
Lu et al., 2017). Our grammatical interpretation of
the structural roles of words in sentences makes
contact with other work that incorporates deep
learning into grammatically-structured networks
(Tai et al., 2015; Kumar et al., 2016; Kong et al.,
2017; Andreas et al., 2015; Yogatama et al., 2016;
Maillard et al., 2017; Socher et al., 2010; Pollack,
1990). Here, the network is not itself structured
to match the grammatical structure of sentences
being processed; the structure is fixed, but is de-
signed to support the learning of distributed repre-
sentations that incorporate structure internal to the
representations themselves — filler/role structure.

TPRs are also used in NLP in (Palangi et al.,
2017) but there the representation of each individ-
ual input word is constrained to be a literal TPR
filler/role binding. (The idea of using the outer
product to construct internal representations was
also explored in (Fukui et al., 2016).) Here, by
contrast, the learned representations are not them-
selves constrained, but the global structure of the
network is designed to display the somewhat ab-
stract property of being TPR-capable: the archi-

1270



tecture uses the TPR unbinding operation of the
matrix-vector product to extract individual words
for sequential output.

7 Conclusion

Tensor Product Representation (TPR) (Smolen-
sky, 1990) is a general technique for construct-
ing vector embeddings of complex symbol struc-
tures in such a way that powerful symbolic func-
tions can be computed using hand-designed neu-
ral network computation. Integrating TPR with
deep learning is a largely open problem for which
the work presented here proposes a general ap-
proach: design deep architectures that are TPR-
capable — TPR computation is within the scope
of the capabilities of the architecture in princi-
ple. For natural language generation, we proposed
such an architecture, the Tensor Product Genera-
tion Network (TPGN): it embodies the TPR op-
eration of unbinding which is used to extract par-
ticular symbols (e.g., words) from complex struc-
tures (e.g., sentences). The architecture can be in-
terpreted as containing a part that encodes a sen-
tence and a part that selects one structural role at
a time to extract from the sentence. We applied
the approach to image-caption generation, devel-
oping a TPGN model that was evaluated on the
COCO dataset, on which it outperformed LSTM
baselines on a range of standard metrics. Unlike
standard LSTMs, however, the TPGN model ad-
mits a level of interpretability: we can see which
roles are being unbound by the unbinding vec-
tors generated internally within the model. We find
such roles contain considerable grammatical infor-
mation, enabling POS tag prediction for the words
they generate and displaying clustering by POS.

References

Martı́n Abadi, Ashish Agarwal, Paul Barham, Eugene
Brevdo, Zhifeng Chen, Craig Citro, Greg S. Cor-
rado, Andy Davis, Jeffrey Dean, Matthieu Devin,
Sanjay Ghemawat, Ian Goodfellow, Andrew Harp,
Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal
Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh
Levenberg, Dan Mané, Rajat Monga, Sherry Moore,
Derek Murray, Chris Olah, Mike Schuster, Jonathon
Shlens, Benoit Steiner, Ilya Sutskever, Kunal Tal-
war, Paul Tucker, Vincent Vanhoucke, Vijay Va-
sudevan, Fernanda Viégas, Oriol Vinyals, Pete War-
den, Martin Wattenberg, Martin Wicke, Yuan Yu,
and Xiaoqiang Zheng. 2015. TensorFlow: Large-
scale machine learning on heterogeneous systems.

Software available from tensorflow.org. https:
//www.tensorflow.org/.

Jacob Andreas, Marcus Rohrbach, Trevor Darrell,
and Dan Klein. 2015. Deep compositional ques-
tion answering with neural module networks. arxiv
preprint. arXiv preprint arXiv:1511.02799 2.

Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An
automatic metric for mt evaluation with improved
correlation with human judgments. In Proceed-
ings of the ACL workshop on intrinsic and extrinsic
evaluation measures for machine translation and/or
summarization. Association for Computational Lin-
guistics, pages 65–72.

Xinlei Chen and Lawrence Zitnick. 2015. Mind’s eye:
A recurrent visual representation for image caption
generation. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition. pages
2422–2431.

Pyeong Whan Cho, Matthew Goldrick, and Paul
Smolensky. 2017. Incremental parsing in a continu-
ous dynamical system: Sentence processing in Gra-
dient Symbolic Computation. Linguistics Vanguard
3. DOI:10.1515/lingvan-2016-0105.

COCO. 2017. Coco dataset for image cap-
tioning. http://mscoco.org/dataset/
#download.

Jacob Devlin, Hao Cheng, Hao Fang, Saurabh Gupta,
Li Deng, Xiaodong He, Geoffrey Zweig, and Mar-
garet Mitchell. 2015. Language models for im-
age captioning: The quirks and what works. arXiv
preprint arXiv:1505.01809 .

Jeffrey Donahue, Lisa Anne Hendricks, Sergio Guadar-
rama, Marcus Rohrbach, Subhashini Venugopalan,
Kate Saenko, and Trevor Darrell. 2015. Long-term
recurrent convolutional networks for visual recogni-
tion and description. In Proceedings of the IEEE
conference on computer vision and pattern recogni-
tion. pages 2625–2634.

Akira Fukui, Dong Huk Park, Daylen Yang, Anna
Rohrbach, Trevor Darrell, and Marcus Rohrbach.
2016. Multimodal compact bilinear pooling for
visual question answering and visual grounding.
arXiv preprint arXiv:1606.01847 .

Zhe Gan, Chuang Gan, Xiaodong He, Yunchen Pu,
Kenneth Tran, Jianfeng Gao, Lawrence Carin, and
Li Deng. 2017. Semantic compositional networks
for visual captioning. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recog-
nition.

Google. 2017. Embedding projector in tensor-
board. https://www.tensorflow.org/
programmers_guide/embedding.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2016. Deep residual learning for image recog-
nition. In Proceedings of the IEEE Conference

1271



on Computer Vision and Pattern Recognition. pages
770–778.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation
9(8):1735–1780.

Qiuyuan Huang, Li Deng, Dapeng Wu, Chang Liu, and
Xiaodong He. 2018. Attentive tensor product learn-
ing for language generation and grammar parsing.
arXiv preprint arXiv:1802.07089 .

Andrej Karpathy and Li Fei-Fei. 2015. Deep visual-
semantic alignments for generating image descrip-
tions. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition. pages
3128–3137.

Ryan Kiros, Ruslan Salakhutdinov, and Rich Zemel.
2014a. Multimodal neural language models. In
Proceedings of the 31st International Conference on
Machine Learning (ICML-14). pages 595–603.

Ryan Kiros, Ruslan Salakhutdinov, and Richard S
Zemel. 2014b. Unifying visual-semantic embed-
dings with multimodal neural language models.
arXiv preprint arXiv:1411.2539 .

Lingpeng Kong, Chris Alberti, Daniel Andor, Ivan Bo-
gatyy, and David Weiss. 2017. Dragnn: A transition-
based framework for dynamically connected neural
networks. arXiv preprint arXiv:1703.04474 .

Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer,
James Bradbury, Ishaan Gulrajani, Victor Zhong,
Romain Paulus, and Richard Socher. 2016. Ask
me anything: Dynamic memory networks for natu-
ral language processing. In International Confer-
ence on Machine Learning. pages 1378–1387.

Jiasen Lu, Caiming Xiong, Devi Parikh, and Richard
Socher. 2017. Knowing when to look: Adaptive at-
tention via a visual sentinel for image captioning. In
Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition (CVPR). volume 6.

Jean Maillard, Stephen Clark, and Dani Yogatama.
2017. Jointly learning sentence embeddings and
syntax with unsupervised tree-lstms. arXiv preprint
arXiv:1705.09189 .

Christopher Manning. 2017. Stanford parser.
https://nlp.stanford.edu/software/
lex-parser.shtml.

Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, Zhiheng
Huang, and Alan Yuille. 2015. Deep captioning with
multimodal recurrent neural networks (m-rnn). In
Proceedings of International Conference on Learn-
ing Representations.

NLTK. 2017. Natural language toolkit (nltk). http:
//www.nltk.org.

Hamid Palangi, Paul Smolensky, Xiaodong He, and
Li Deng. 2017. Deep learning of grammatically-
interpretable representations through question-
answering. arXiv preprint arXiv:1705.08432
.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of
the 40th annual meeting on association for compu-
tational linguistics. Association for Computational
Linguistics, pages 311–318.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2017. Stanford glove: Global vec-
tors for word representation. https://nlp.
stanford.edu/projects/glove/.

Jordan B Pollack. 1990. Recursive distributed repre-
sentations. Artificial Intelligence 46(1):77–105.

Steven J. Rennie, Etienne Marcheret, Youssef Mroueh,
Jerret Ross, and Vaibhava Goel. 2017. Self-critical
sequence training for image captioning. In Proceed-
ings of the IEEE Conference on Computer Vision
and Pattern Recognition.

Paul Smolensky. 1990. Tensor product variable bind-
ing and the representation of symbolic structures in
connectionist systems. Artificial intelligence 46(1-
2):159–216.

Paul Smolensky. 2012. Symbolic functions from neu-
ral computation. Philosophical Transactions of the
Royal Society — A: Mathematical, Physical and En-
gineering Sciences 370:3543 – 3569.

Paul Smolensky and Géraldine Legendre. 2006.
The harmonic mind: From neural computation to
optimality-theoretic grammar. Volume 1: Cognitive
architecture. MIT Press.

Richard Socher, Christopher D Manning, and An-
drew Y Ng. 2010. Learning continuous phrase
representations and syntactic parsing with recursive
neural networks. In Proceedings of the NIPS-2010
Deep Learning and Unsupervised Feature Learning
Workshop. pages 1–9.

Kai Sheng Tai, Richard Socher, and Christopher D
Manning. 2015. Improved semantic representations
from tree-structured long short-term memory net-
works. arXiv preprint arXiv:1503.00075 .

Ramakrishna Vedantam, C Lawrence Zitnick, and Devi
Parikh. 2015. Cider: Consensus-based image de-
scription evaluation. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recog-
nition. pages 4566–4575.

Oriol Vinyals, Alexander Toshev, Samy Bengio, and
Dumitru Erhan. 2015. Show and tell: A neural im-
age caption generator. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recog-
nition. pages 3156–3164.

1272



Kaisheng Xu, Hanli Wang, and Pengjie Tang. 2017.
Image captioning with deep lstm based on sequen-
tial residual. In Proceedings of IEEE International
Conference on Multimedia and Expo (ICME). pages
361–366.

Ting Yao, Yingwei Pan, Yehao Li, Zhaofan Qiu, and
Tao Mei. 2017. Boosting image captioning with at-
tributes. In Proceedings of International Conference
on Computer Vision.

Dani Yogatama, Phil Blunsom, Chris Dyer, Edward
Grefenstette, and Wang Ling. 2016. Learning to
compose words into sentences with reinforcement
learning. arXiv preprint arXiv:1611.09100 .

1273


