



















































FBK-IRST: Semantic Relation Extraction Using Cyc


Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 214–217,
Uppsala, Sweden, 15-16 July 2010. c©2010 Association for Computational Linguistics

FBK-IRST: Semantic Relation Extraction using Cyc

Kateryna Tymoshenko and Claudio Giuliano
FBK-IRST

I-38050, Povo (TN), Italy
tymoshenko@fbk.eu, giuliano@fbk.eu

Abstract

We present an approach for semantic re-
lation extraction between nominals that
combines semantic information with shal-
low syntactic processing. We propose to
use the ResearchCyc knowledge base as
a source of semantic information about
nominals. Each source of information
is represented by a specific kernel func-
tion. The experiments were carried out
using support vector machines as a clas-
sifier. The system achieves an overall F1
of 77.62% on the “Multi-Way Classifica-
tion of Semantic Relations Between Pairs
of Nominals” task at SemEval-2010.

1 Introduction

The SemEval-2010 Task 8 “Multi-Way Classifi-
cation of Semantic Relations Between Pairs of
Nominals” consists in identifying which seman-
tic relation holds between two nominals in a sen-
tence (Hendrickx et al., 2010). The set of rela-
tions is composed of nine mutually exclusive se-
mantic relations and the Other relation. Specifi-
cally, the task requires to return the most informa-
tive relation between the specified pair of nomi-
nals e1 and e2 taking into account their order. An-
notation guidelines show that semantic knowledge
about e1 and e2 plays a very important role in dis-
tinguishing among different relations. For exam-
ple, relations Cause-Effect and Product-Producer
are closely related. One of the restrictions which
might help to distinguish between them is that
products must be concrete physical entities, while
effects must not.

Recently, there has emerged a large number of
freely available large-scale knowledge bases. The
ground idea of our research is to use them as
source of semantic information. Among such re-

sources there are DBpedia,1 YAGO,2 and Open-
Cyc.3 On the one hand, DBpedia and YAGO have
been automatically extracted from Wikipedia.
They have a good coverage of named entities, but
their coverage of common nouns is poorer. They
seem to be more suitable for relation extraction be-
tween named entities. On the other hand, Cyc is
a manually designed knowledge base, which de-
scribes actions and entities both in common life
and in specific domains (Lenat, 1995). Cyc has
a good coverage of common nouns, making it in-
teresting for our task. The full version of Cyc is
freely available to the research community as Re-
searchCyc.4

We approached the task using the system intro-
duced by Giuliano et al. (2007) as a basis. They
exploited two information sources: the whole sen-
tence where the relation appears, and WordNet
synonymy and hyperonymy information. In this
paper, we (i) investigate usage of Cyc as a source
of semantic knowledge and (ii) linguistic infor-
mation, which give useful clues to semantic re-
lation extraction. From Cyc, we obtain informa-
tion about super-classes (in the Cyc terminology
generalizations) of the classes which correspond
to nominals in a sentence. The sentence itself
provides linguistic information, such as local con-
texts of entities, bag of verbs and distance between
nominals in the context.

The different sources of information are rep-
resented by kernel functions. The final system
is based on four kernels (i.e., local context ker-
nel, distance kernel, verbs kernel and generaliza-
tion kernel). The experiments were carried out us-
ing support vector machines (Vapnik, 1998) as a
classifier. The system achieves an overall F1 of

1http://dbpedia.org/
2http://www.mpi-inf.mpg.de/yago-naga/

yago/
3http://www.cyc.com/opencyc
4http://research.cyc.com/

214



77.62%.

2 Kernel Methods for Relation
Extraction

In order to implement the approach based on shal-
low syntactic and semantic information, we em-
ployed a linear combination of kernels, using the
support vector machines as a classifier. We de-
veloped two types of basic kernels: syntactic and
semantic kernels. They were combined by exploit-
ing the closure properties of kernels. We define the
composite kernel KC(x1, x2) as follows.

n∑
i=1

Ki(x1, x2)√
Ki(x1, x1)Ki(x2, x2)

. (1)

Each basic kernel Ki is normalized.
All the basic kernels are explicitly calculated as

follows

Ki(x1, x2) = 〈ϕ(x1), ϕ(x2)〉 , (2)

where ϕ(·) is the embedding vector. The resulting
feature space has high dimensionality. However,
Equation 2 can be efficiently computed explicitly
because the representations of input are extremely
sparse.

2.1 Local context kernel

Local context is represented by terms, lemmata,
PoS tags, and orthographic features extracted
from a window around the nominals considering
the token order. Formally, given a relation ex-
ample R, we represent a local context LC =
t−w, ..., t−1, t0, t+1, ..., t+w as a row vector

ψLC(R) = (tf1(LC), tf2(LC), ..., tfm(LC) ) ∈ {0, 1}m,
(3)

where tfi is a feature function which returns 1
if the feature is active in the specified position
of LC; 0 otherwise. The local context kernel
KLC (R1, R2) is defined as

KLC e1(R1, R2) +KLC e2(R1, R2), (4)

where KLC e1 and KLC e2 are defined by substi-
tuting the embedding of the local contexts of e1
and e2 into Equation 2, respectively.

2.2 Verb kernel

The verb kernel operates on the verbs present in
the sentence,5 representing it as a bag-of-verbs.

5On average there are 2.65 verbs per sentence

More formally, given a relation example R, we
represent the verbs from it as a row vector

ψV (R) = (vf(v1, R), ..., vf(vl, R)) ∈ {0, 1}l, (5)

where the binary function vf(vi, R) shows if a
particular verb is used in R. By substituting
ψV (R) into Equation 2 we obtain the bag-of-verbs
kernel KV .

2.3 Distance kernel

Given a relation example R(e1, e2), we repre-
sent the distance between the nominals as a one-
dimensional vector

ψD(R) =
1

dist(e1, e2)
∈ <1, (6)

where dist(e1, e2) is number of tokens between
the nominals e1 and e2 in a sentence. By substitut-
ing ψD(R) into Equation 2 we obtain the distance
kernel KD.

2.4 Cyc-based kernel

Cyc is a comprehensive, manually-build knowl-
edge base developed since 1984 by CycCorp. Ac-
cording to Lenat (1995) it can be considered as
an expert system with domain spanning all ev-
eryday actions and entities, like Fish live in wa-
ter. The open-source version of Cyc named Open-
Cyc, which contains the full Cyc ontology and re-
stricted number of assertions, is freely available
on the web. Also the full power of Cyc has been
made available to the research community via Re-
searchCyc. Cyc knowledge base contains more
than 500,000 concepts and more than 5 million as-
sertions about them. They may refer both to com-
mon human knowledge like food or drinks and to
specialized knowledge in domains like physics or
chemistry. The knowledge base has been formu-
lated using CycL language. A Cyc constant repre-
sents a thing or a concept in the world. It may be
an individual, e.g. BarackObama, or a collection,
e.g. Gun, Screaming.

2.4.1 Generalization kernel
Given a nominal e, we map it to a set of Cyc
constants EC = {ci}, using the Cyc function
denotation-mapper. Nominals in Cyc usually de-
note constants-collections. Notice that we do not
perform word sense disambiguation. For each ci ∈
EC, we query Cyc for collections which general-
ize it. In Cyc collection X generalizes collection

215



Y if each element of Y is also an element of col-
lectionX . For instance, collection Gun is general-
ized by Weapon, ConventionalWeapon, Mechani-
calDevice and others.

The semantic kernel incorporates the data from
Cyc described above. More formally, given a rela-
tion example R each nominal e is represented as

ψEC(R) = (fc(c1, e), ..., fc(ck, e)) ∈ {0, 1}k, (7)

where the binary function fc(ci, e) shows if a par-
ticular Cyc collection ci is a generalization of e.

The bag-of-generalizations kernel
Kgenls (R1, R2) is defined as

Kgenls e1 (R1, R2) +Kgenls e2 (R1, R2) , (8)

whereKgenls e1 andKgenls e2 are defined by sub-
stituting the embedding of generalizations e1 and
e2 into Equation 2 respectively.

3 Experimental setup and Results

Sentences have been tokenized, lemmatized and
PoS tagged with TextPro.6 Information for gener-
alization kernel has been obtained from Research-
Cyc. All the experiments were performed using
jSRE customized to embed our kernels.7 jSRE
uses the SVM package LIBSVM (Chang and Lin,
2001). The task is casted as multi-class classifica-
tion problem with 19 classes (2 classes for each
relation to encode the directionality and 1 class
to encode Other). The multiple classification task
is handled with One-Against-One technique. The
SVM parameters have been set as follows. The
cost-factor Wi for a given class i is set to be the
ratio between the number of negative and positive
examples. We used two values of regularization
parameter C: (i) Cdef = 1∑K(x,x) where x are
all examples from the training set, (ii) optimized
Cgrid value obtained by brute-force grid search
method. The default value is used for the other
parameters.

Table 1 shows the performance of different ker-
nel combinations, trained on 8000 training exam-
ples, on the test set. The system achieves the
best overall macro-average F1 of 77.62% using
KLC + KV + KD + Kgenls. Figure 1 shows the
learning curves on the test set. Our experimen-
tal study has shown that the size of the training

6http://textpro.fbk.eu/
7jSRE is a Java tool for relation extraction avail-

able at http://tcc.itc.it/research/textec/
tools-resources/jsre.html.

1000 2000 4000 8000
0.40

0.45

0.50

0.55

0.60

0.65

0.70

0.75

0.80

0.85

0.90

Cause-Effect
Component-Whole
 Content-Container
Entity-Destination
 Entity-Origin
 Instrument-Agency
 Member-Collection
Message-Topic
Product-Producer
All

Number of training examples

F1

Figure 1: Learning curves on the test set per rela-
tion

Kernels P R F1
KLC +KV +KD +Kgenls 74.98 80.69 77.62
KLC +KV +KD +Kgenls* 78.51 76.03 77.11
KLC +KD +Kgenls* 78.14 75.93 76.91
KLC +Kgenls* 78.19 75.70 76.81
KLC +KD +Kgenls 72.98 80.28 76.39
KLC +Kgenls 73.05 79.98 76.28

Table 1: Performance on the test set. Combina-
tions marked with * were run with Cgrid, others
with Cdef .

set influences the performance of the system. We
observe that when the system is trained on 8000
examples the overall F1 increases for 14.01% as
compared to the case of 1000 examples.

4 Discussion and error analysis

The experiments have shown that KLC is the core
kernel of our approach. It has good performance
on its own. For instance, it achieves precision of
66.16%, recall 72.67% and F1 of 69.13% evalu-
ated using 10-fold cross-validation on the training
set.

Relation KLC KLC +Kgenls ∆F1
Cause-Effect 74.29 76.41 2.12
Component-Whole 61.24 66.13 4.89
Content-Container 76.36 79.12 2.76
Entity-Destination 82.85 83.95 1.10
Entity-Origin 72.09 74.13 2.04
Instrument-Agency 57.71 65.51 7.80
Member-Collection 81.30 83.40 2.10
Message-Topic 60.41 69.09 8.68
Product-Producer 55.95 63.52 7.57

Table 2: The contribution of Cyc evaluated on the
training set.

216



Generalization kernel combined with local con-
text kernel gives precision of 70.38%, recall of
76.96%, and F1 73.47% with the same exper-
imental setting. The increase of F1 per re-
lation is shown in the Table 2 in the col-
umn ∆F1. The largest F1 increase is ob-
served for Instrument-Agency (+7.80%), Message-
Topic (+8.68%) and Product-Producer (+7.57%).
Kgenls reduces the number of misclassifications
between the two directions of the same rela-
tion, like Product-Producer(artist,design). It
also captures the differences among relations,
specified in the annotation guidelines. For in-
stance, the system based only on KLC misclass-
fied “The<e1>species</e1>makes a squelching
<e2>noise</e2>” as Product-Producer(e2,e1).
Generalizations for <e2>noise</e2> provided
by Cyc include Event, MovementEvent, Sound.
According to the annotation guidelines a product
must not be an event. A system based on the com-
bination of KLC and Kgenls correctly labels this
example as Cause-Effect(e1,e2).
Kgenls improves the performance in general.

However, in some cases using Cyc as a source of
semantic information is a source of errors. Firstly,
sometimes the set of constants for a given nom-
inal is empty (e.g., disassembler, babel) or does
not include the correct one (noun surge is mapped
to the constant IncreaseEvent). In other cases,
an ambiguous nominal is mapped to many con-
stants at once. For instance, notes is mapped
to a set of constants, which includes Musical-
Note, Note-Document and InformationRecording-
Process. Word sense disambiguation should help
to solve this problem. Other knowledge bases like
DBpedia and FreeBase8 can be used to overcome
the problem of lack of coverage.

Bag-of-word kernel with all words from the
sentence did not impact the final result.9 However,
the information about verbs present in the sentence
represented by KV helped to improve the perfor-
mance. A preliminary error analysis shows that a
deeper syntactic analysis could help to further im-
prove the performance.

For comparison purposes, we also exploited
WordNet information by means of the supersense
kernel KSS (Giuliano et al., 2007). In all exper-
iments, KSS was outperformed by Kgenls. For
instance, KLC + KSS gives overall F1 measure

8http://www.freebase.com/
9This kernel has been evaluated only on the training data.

of 70.29% with the same experimental setting as
described in the beginning of this section.

5 Conclusion

The paper describes a system for semantic rela-
tions extraction, based on the usage of semantic
information provided by ResearchCyc and shal-
low syntactic features. The experiments have
shown that the external knowledge, encoded as
super-class information from ResearchCyc with-
out any word sense disambiguation, significantly
contributes to improve overall performance of the
system. The problem of the lack of coverage may
be overcome by the usage of other large-scale
knowledge bases, such as DBpedia. For future
work, we will try to use the Cyc inference en-
gine to obtain implicit information about nominals
in addition to the information about their super-
classes and perform word sense disambiguation.

Acknowledgments
The research leading to these results has received funding
from the ITCH project (http://itch.fbk.eu), spon-
sored by the Italian Ministry of University and Research and
by the Autonomous Province of Trento and the Copilosk
project (http://copilosk.fbk.eu), a Joint Research
Project under Future Internet - Internet of Content program
of the Information Technology Center, Fondazione Bruno
Kessler.

References
Chih-Chung Chang and Chih-Jen Lin, 2001. LIB-

SVM: a library for support vector machines. Soft-
ware available at http://www.csie.ntu.edu.tw/
˜cjlin/libsvm.

Claudio Giuliano, Alberto Lavelli, Daniele Pighin, and
Lorenza Romano. 2007. Fbk-irst: Kernel methods
for semantic relation extraction. In Proceedings of the
Fourth International Workshop on Semantic Evaluations
(SemEval-2007), Prague, Czech Republic, June. Associa-
tion for Computational Linguistics.

Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav
Nakov, Diarmuid Ó Séaghdha, Sebastian Padó, Marco
Pennacchiotti, Lorenza Romano, and Stan Szpakowicz.
2010. Semeval-2010 task 8: Multi-way classification of
semantic relations between pairs of nominals. In Proceed-
ings of the 5th SIGLEX Workshop on Semantic Evaluation,
Uppsala, Sweden.

Douglas B. Lenat. 1995. CYC: A large-scale investment in
knowledge infrastructure. Communications of the ACM,
38(11):33–38.

Vladimir N. Vapnik. 1998. Statistical Learning Theory.
Wiley-Interscience, September.

217


