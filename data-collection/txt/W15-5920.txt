



















































Proceedings of the...


D S Sharma, R Sangal and E Sherly. Proc. of the 12th Intl. Conference on Natural Language Processing, pages 130–137,
Trivandrum, India. December 2015. c©2015 NLP Association of India (NLPAI)

Domain Sentiment Matters: A Two Stage Sentiment Analyzer

Raksha Sharma and Pushpak Bhattacharyya
Dept. of Computer Science and Engineering

IIT Bombay, Mumbai, India
{raksha,pb}@cse.iitb.ac.in

Abstract

There are words that change its polar-
ity from domain to domain. For exam-
ple, the word deadly is of positive po-
larity in the cricket domain as in “Shane
Warne is a ‘deadly’ leg spinner”. How-
ever, ‘I witnessed a deadly accident’ car-
ries negative polarity and going by the sen-
timent in cricket domain will be mislead-
ing. In addition to this, there exist domain-
specific words, which have the same polar-
ity across domains, but are used very fre-
quently in a particular domain. For exam-
ple, blockbuster, is specific to the movie
domain. We combine such words as Do-
main Dedicated Polar Words (DDPW).

A concise feature set made up of prin-
cipal polarity clues makes the classifier
less expensive in terms of time complex-
ity and enhances the accuracy of classifi-
cation. In this paper, we show that DDPW
make such a concise feature set for senti-
ment analysis in a domain. Use of domain-
dedicated polar words as features beats the
state of art accuracies achieved indepen-
dently with unigrams, adjectives or Uni-
versal Sentiment Lexicon (USL).

1 Introduction

The general approach of Sentiment Analysis (SA)
is to summarize the semantic polarity (i.e., positive
or negative) of sentences/documents by analysis
of the orientation of the individual words (Riloff
and Wiebe, 2003; Pang and Lee, 2004; Danescu-
Niculescu-Mizil et al., 2009; Kim and Hovy, 2004;
Takamura et al., 2005). In the real world, most
sentiment analysis applications are domain ori-
ented. All business organizations are interested
in sentiment information about the product they
deal with. For instance, an automobile organiza-

tion is concerned only about recognizing the sen-
timent information received for automobiles only.
Therefore, a list of Domain Dedicated Polar Words
(DDPW) can be proved as the best lexical resource
for domain oriented sentiment analysis.

Most sentiment analysis applications rely on
the Universal Sentiment Lexicons (USL) as a key
feature along with additional features (Riloff and
Wiebe, 2003). There are many USL resources like
senti-word-net1, subjectivity lexicon2 by Wiebe
and a list of positive and negative opinion words3

by Liu. These lexicons contain only those words
that are usual and have the same polarity across all
the domains. These universal sentiment lexicons
have the following problems:

• The words that have fluctuating polarity
across domains, but have fixed polarity in a
domain are strong candidate for the sentiment
analysis in that domain. We call such words
chameleon words. Consider the following ex-
ample of fluctuating polarity phenomenon.

1. The cars steering was unpredictable
while driving. (-ve sentiment)

2. The story line of Palmetto was unpre-
dictable. (+ve sentiment)

The word unpredictable bears negative polar-
ity in the automobile domain, but it is positive
in the movie domain. Hence, unpredictable
assigns negative polarity to the first sentence
and positive polarity to the second sentence.
Due to the absence of chameleon words like
unpredictable, the USL based classifier fails
to determine the correct polarity of the sen-
tences that contain chameleon words.

• On the other hand, consistency in use of a po-
lar word in a particular domain, makes it a

1http://sentiwordnetisti.cnr.it/
2http://mpqa.cs.pitt.edu/
3http://www.cs.uic.edu/l̃iub/FBS/sentiment-analysis.html130



very strong candidate for sentiment analysis
in that domain. Consider the following ex-
ample of fluctuating regularity (frequency of
usage) phenomenon.

1. It’s another summer blockbuster with
plot points that are beyond unbelievable.
(+ve sentiment)

2. The main Characters were miscast (-ve
sentiment)

The words blockbuster and miscast are used
very frequently in the movie domain to ex-
press the opinion in comparison with other
domains. The absenteeism of such strong po-
larity clues for a domain, makes the USL im-
poverished for sentiment analysis in a partic-
ular domain.

We combine the chameleon words and the do-
main specific regular words into a single unit:
Domain-Dedicated polar words (DDPW). The
DDPW are missing from the USL, because of their
fluctuating polarity and regularity across domains.

In this paper, we present sentiment analysis in
a domain as a two stage process. Identification of
domain-dedicated polar words prior to implemen-
tation of classification algorithms leads to less ex-
pensive and more efficient sentiment analysis sys-
tem (Section 2). We examine the role of domain-
dedicated polar words in three domains: movie,
tourism and product. Our results show that use of
domain-dedicated polar words as features either
beats or equates the accuracy achieved indepen-
dently with unigrams, adjectives or USL in all the
three domains (Section 6). The two stage approach
is depicted in the figure 1.

The first stage implements the Chi-Square4 test
on the difference in the counts of the word in pos-
itive and negative documents to detect domain-
dedicated polar words. The second stage ac-
complishes the sentiment analysis task using the
output of the first stage as features. We exper-
imented with three standard classification algo-
rithms: Neural Network (NN) classification, Lo-
gistic Regression (LR) classification and Support
Vector Machine (SVM) classification. Accuracy
figures (Section 6) substantiate the effectiveness
of the two stage sentiment analysis in a domain in
comparison with the single stage sentiment anal-
ysis that relies on standard features like, universal
sentiment lexicons, adjectives or unigrams.

4http://math.hws.edu/ javamath/ryan/ChiSquare.html

Figure 1: The two stage approach for SA in a do-
main

In this paper, we use subjectivity lexicon given
by Wiebe as universal sentiment lexicon. Section
2 helps illustrate the reason behind improvement
in accuracy with a very small feature set: domain-
dedicated polar words. In section 3, we formu-
late the Chi-Square test to depict the generation of
DDPW. Section 4 expands on the ML based clas-
sification algorithms that are used in stage-2. Sec-
tion 5 and 6 illustrate the experimental set up and
results respectively. Sections 7 and 8 discuss re-
lated work and conclusion.

2 Artifacts of Domain Significance in
Sentiment Analysis

A feature is a piece of information that is poten-
tially useful for prediction. Coming up with a
big feature set increases the time complexity of
the classifier. In addition to this, presence of ir-
relevant or redundant features misleads the clas-
sifier, hence, results in a poor accuracy. Pang et
al. (2002) observed that the top 2633 unigrams
are better features than unigrams or adjectives for
sentiment classification of a document. They also
proved that ‘term presence’ is more informative
than ‘term frequency’ for sentiment prediction of
a document.

A concise feature set made up of principal po-
larity clues makes the classifier less expensive in
terms of time complexity and enhances the ac-
curacy of classification. Our work falls in the
same series, that is, identification of decisive com-
ponents for sentiment analysis in a domain. As131



most of the applications of SA are domain specific,
therefore, we can restrict ourselves to a domain for
SA. For domain oriented sentiment analysis, we
can come up with more prominent features, such
as domain dedicated polar words. The following
examples5 from the movie domain help illustrate
the problem we attempt to address.

juvenile, surreal, unpredictable, predictable,
timeless, thrilling, well-made, well-written

The exemplified words are highly polar in the
movie domain, but because of their fluctuating
polarity and regularity in use are not included
in most of the existing universal sentiment lexi-
cons. We tested for the universal sentiment lexicon
given by Wiebe and find 664 words that are not
present in the USL, but are extracted as DDPW.
The words shown in the example are a few of
them. On the other hand, these features can be ex-
tracted as a subset of unigrams or adjectives from
the input corpus, but at the cost of higher train-
ing time complexity and poor generalization in
classification. Therefore, we define identification
of domain-dedicated polar words as the foremost
step (stage-1) for sentiment analysis in a domain.

In literature, Unigrams are considered as state-
of-art features for sentiment classification, we are
able to achieve the same level of accuracy with
DDPW as features. However, instead of words,
one uses word senses (synset ids in WordNets) as
features, the accuracy improves dramatically. Bal-
amurali et al. (2011) reported accuracy above 85%
with (sense + words) as features. However, ac-
curacy accomplishment is a function of investment
in annotation. This improvement is not significant
enough to justify the cost of annotation.

However, the criterion of domain-dedication
does not equally exist with all the polar words.
There are words that have uniform polarity and
regularity (frequency of usage) across domains.
This phenomenon is considered implicitly by our
proposed approach of DDPW extraction. Conse-
quently, we are able to extract deterministic po-
lar words that have uniform polarity and regularity
across domains. Consider the following example
from the movie domain.

enjoyable, entertaining, magnificent, impres-
sive, irritating, awful, annoying, weakest

5All the examples reported in the paper are part of the
output obtained through the Chi-Square test in stage-1.

The significant occurrence of such deterministic
polar words in a particular class (positive or neg-
ative) in the movie review corpus assures the sat-
isfiability criterion of the Chi-Square test, hence,
their extraction as DDPW in stage-1 of the pro-
posed SA system.

3 Identification of Domain Dedicated
Polar Words

The orientation of the polarity of a word and its
frequency of usage vary from domain to domain.
Such domain-dedicated polar words are the im-
portant clues for sentiment analysis in that do-
main. This section illustrates the stage-1 of the
two-stage sentiment analysis approach that gener-
ates domain-dedicated polar words. We have per-
formed an extensive evaluation of DDPW partici-
pation in sentiment analysis using three domains:
movie, product and tourism.

3.1 Domain and Dataset
Providing polarity information about movie re-
views is a very useful service (Turney, 2002). Its
proof is the continuously growing popularity of
the several film review websites67. For movie do-
main, we use 1000 positive and 1000 negative re-
views8. Product reviews directly affect the busi-
ness of e-commerce organizations. For product
domain, we use 1000 positive and 1000 negative
reviews (music instruments) from Amazon, used
by Blitzer et al. (2007). The third domain is the
tourism domain, a more accurate sentiment analy-
sis in tourism domain can suggest a more accurate
place for visit. We use 700 positive and 700 nega-
tive tourism reviews, used by Khapra et al. (2010)
to train a word sense disambiguation system. In
this paper, we report domain-dedicated words for
the movie, product and tourism domain and show
that these words are better features than universal
sentiment lexicons, unigrams and adjectives for
sentiment analysis in the respective domain.

3.2 Chi-Square Test
The Chi-Square test is a statistical test to identify
the class (positive/negative) of the encountered
word. We use Chi-Square test to extract domain-
dedicated words from the corpus in stage-1 of the
proposed sentiment analyzer. As Chi-square test

6www.rottentomatoes.com
7www.imdb.com
8Available at: www.cs.cornell.edu/people/pabo/movie-

review-data/(review corpus version 2.0).132



requires values of two parameters, that are, ex-
pected count and observed count, we consider the
arithmetic mean of the count in positive and nega-
tive files as expected count of the word in positive
and negative classes, which is also a null hypoth-
esis. The alternative hypothesis states that there
is a statistically significant difference between the
observed count and the mean value.

On the basis of the deviation of the observed
count from the mean value (expected count), Chi-
square test decides the polarity of a word for a par-
ticular domain to which documents belong. The
statistically significant deviation resulted from the
Chi-Square test shows that the word appears in
a particular class of documents more frequently.
This appearance is not by chance, rather, there is
some reason behind its occurrence in that class of
documents. The reason behind this significant de-
viation is the polarity orientation of the word that
makes it a part of positive or negative documents
more frequently. For example, thought-provoking,
superb, thrilling, tremendous have positive polar-
ity in the movie domain, so they would occur more
frequently in the positive reviews rather than neg-
ative reviews (Sharma and Bhattacharyya, 2013).
The Chi-Square test is formulated as follows:

X2(W ) = ((Cp − µ)2 + (Cn − µ)2)/µ (1)

Here, Cp is the count of a word W in the pos-
itive documents and Cn is the count in negative
documents. µ represents an average of the word’s
count in positive and negative documents. µ is
the expected count or null hypothesis, while Cp
and Cn are the observed count of W . If the Chi-
square test results in a value that is greater than
the threshold value, then there is a significant dif-
ference between the expected and the observed.
Since there is an inverse relation9 between the Chi-
Square value and the probability of word given
NULL hypothesis is true, a high Chi-Square value
indicates that the probability is very poor. There-
fore, we reject NULL hypothesis, that is, we re-
ject the uniform distribution of the word in posi-
tive and negative class. However, we assume that
the word W belongs to a particular class (alterna-
tive hypothesis), either positive or negative.

To understand the identification of DDPW from
the corpus, Consider the example of Chi-Square

9The Chi-square score and probability table is given at
http://faculty.southwest.tn.edu/jiwilliams/probab2.gif

test performed for the word unpredictable in the
movie domain. The count of the word unpre-
dictable in positive (22) and negative files (10) are
taken from the considered movie review corpus.

X2(Unpredictable) = ((22−16)2+(10−16)2)/16
(2)

The word “unpredictable” results in a Chi-
Square value of 4.5, that is greater than 3.84 (Stan-
dard Threshold Value in Statistics). This relation
implies that P(Data|NULL-Hypothesis is true) is
less than 0.05 (5%) for the word unpredictable.
Hence, reject NULL hypothesis and accept the
alternative hypothesis, that is, the word unpre-
dictable belongs to a particular class.

Bruce and Wiebe (1999) proved that ‘adjec-
tives’ are the best candidate to adhere the po-
larity. They established a statistically signifi-
cant correlation between sentence subjectivity and
the presence of adjectives. At this stage, we
also have considered adjectives10 only as domain-
dedicated words, but we believe that domain-
dedicated words are not limited to adjective only.
The same approach can be applied to find domain-
dedicated words from other part of speeches.

4 Sentiment Classification in stage-2

The final class of the document is predicted in
stage-2 of the proposed sentiment analysis system.
Our utmost goal is to examine the behavior of SA
system using a concise feature set: Domain Ded-
icated Polar Words (DDPW), which are extracted
as output of stage-1. For this purpose, we experi-
mented with three machine learning based classi-
fication algorithms, that are, Neural Network, Lo-
gistic Regression and SVM.

4.1 Neural Network (NN)

Neural networks are able to produce a com-
plex non linear hypothesis function for classifica-
tion. Nowadays, NN has become “state-of-the-
art” technique for many applications because of
the fast computers that can solve a big network,
(Yanagimoto et al., 2013; Hui, 2011). In our case
also, the classification accuracy attained by Neural
Network surpasses SVM and LR.

10Bidirectional Standford POS tagger is used to find the
part of speech of the word.133



4.2 Logistic Regression (LR)

LR classifier is a non linear classifier. Non linear-
ity is achieved through sigmoid function (equation
3) that estimates the probability of the document
belonging to a class. If LR results in a probability
value higher than 0.5, it implies that the document
has more than 50% chance of being positive, else
the document bears negative polarity. The Logistic
(Sigmoid) function simulated by LR is as follows.

Hypothesis(X) = 1/(1 + e(−Theta
′∗X)) (3)

Here, X is the input feature vector of a docu-
ment (Section 4), Theta is also a vector that con-
tains the weights assigned to X .

4.3 Support Vector Machine (SVM)

The support vector machine (SVM) has been
proven to be highly effective at traditional
text categorization and sentiment classification
(Joachims, 1998; Pang et al., 2002). SVM predicts
the class of the document on the basis of a linear
function, that is, z’s score. LR takes a decision at z
equals to zero, while SVM takes a decision at two
boundaries: z equals to +1 and z equals to −1.

z = Theta′ ∗X (4)

5 Experimental Setup

For all the three observed domains: movie, prod-
uct and tourism, we divide the data into five equal
size folds, maintaining the balance of the negative
and positive files in each fold. All the results re-
ported in section 6 are the average five-fold cross
validation results on the dataset.

In our work, we compare the performance of
sentiment analysis system based on features, iden-
tified by Chi-Square test with the systems that are
based on universal sentiment lexicon, Union of
DDPW and USL, top-adjectives, adjectives and
unigrams. Since training Logistic Regression is
expensive with a large set of features, we consid-
ered only those unigrams that appear at least four
times in the corpus. In the same way, top adjec-
tives are chosen such that they appear at least five
times in the corpus. These constants are chosen
such that it satisfy both the conditions; it fetches
maximum number of words which will be used as
features and all those words occur frequently in
the corpus.

Since all the three techniques, NN, LR, and
SVM take input as a feature vector, each docu-
ment is represented as a feature vector. Let pj(d)
denotes the presence of feature j in the document
d and n be the size of the feature set. Then, each
document d is represented as a feature vector as
shown here.
−→
d = (p1(d), p2(d), p3(d), ..........., pn(d))

6 Results and Discussion

The table 1 shows the classification accuracies for
the movie domain obtained with various feature
sets and techniques separately. Accuracy is calcu-
lated as a fraction of total input documents that are
correctly classified by the classifiers. The accura-
cies resulting from using only DDPW as features
are shown in row (1) of table 1.

In literature, unigrams are considered as state-
of-art features (Ng et al., 2006; Pang et al.,
2002) for sentiment analysis, we also experi-
mented with unigrams. Domain-dedicated words
as features perform better than unigrams with all
the three classification algorithms. At the same
time, domain-dedicated words speed up the clas-
sification process with a small feature set of size
920. Since adjectives have been crucial clues in
sentiment prediction (Hatzivassiloglou and Wiebe,
2000), we experimented with all the adjectives and
top adjectives. We find that both the feature sets
are not as effective as DDPW.

We experimented with a universal sentiment
lexicon provided by Wiebi to capture more con-
text in general. Such sentiment lexicons are good
source of polar words with a compact size, but
are independent of any domain. The absence of
polar words from USL that are crucial for movie
domain (e.g., blockbuster) causes accuracy to de-
cline by 5% to 8%. On the other hand, inclusion of
DDPW with the USL leads to a big increment in
accuracy in comparison with USL only. Yet, the
results shown in row (5) of table 1 are relatively
poor: the feature set, consisting of 920 domain-
dedicated polar words provide more information
than the intersection of DDPW and USL.

Figure 2 shows the maximum classification ac-
curacy obtained in the three domains using six
feature sets. From figure 2, we can observe that
DDPW outperform the accuracy obtained with the
other features in the movie domain. In case of
tourism domain, DDPW equate the accuracy ob-
tained with unigrams, which is the highest accu-134



Features Number of features NN LR SVM
(1) DDPW 920 85.50 83.50 84.50
(2) Unigrams 18345 83.50 80.00 82.50
(3) Adjectives 11151 83.00 82.75 80.25
(4) Top-Adjectives 2500 82.50 82.00 81.50
(5) DDPW ∪ USL 2220 81.50 81.75 81.75
(6) USL 1946 76.50 75.50 76.00

Table 1: Average five-fold cross-validation accuracies, in percentage. Boldface: best performance
achieved through NN, LR and SVM, for the given feature set

Figure 2: Sentiment classification accuracy in all
three domains

racy for the tourism domain. For product domain,
DDPW equate the accuracy obtained with adjec-
tives, which is the highest accuracy for the product
domain.

In terms of relative performance of classifica-
tion techniques, Neural Network tends to perform
the best, although the differences are not very
large. As a whole, accuracy figures validate the
prominence of identification of domain-dedicated
polar words prior to the implementation of classi-
fication algorithm.

7 Related Work

Several works use the universal sentiment lexicons
to decide whether a sentence expresses a sentiment
(Riloff and Wiebe, 2003; Whitelaw et al., 2005;
Mukherjee et al., 2012). Considering that the USL
solely is not sufficient to achieve satisfactory per-
formance, there are some more works that com-
bine additional feature types for sentiment classi-
fication exist (Yu and Hatzivassiloglou, 2003; Kim
and Hovy, 2004; McDonald et al., 2007; Melville
et al., 2009; Ng et al., 2006).

Wiebe (2000), for the first time, worked in the

area of sentiment lexicon. She focused on the
problem of identifying subjective adjectives with
the help of the corpus. She proposed an ap-
proach to find subjective adjectives based on the
distributional similarity from the Lin (1998) the-
saurus. Her approach was seeded by manually
provided strong subjectivity clues. She used this
new set of adjectives to find subjectivity in sen-
tences, just by the presence of an adjective from
the new set. However, the approach was unable to
predict sentiment orientations of newly found sub-
jective adjectives and sentences. Moreover, they
did not take into account the domain-dedicated po-
lar words and domain-dedicated sentiment analy-
sis.

Pang et al. (2002) for the first time applied
machine learning techniques for sentiment classi-
fication. They implemented Naive Bayes, max-
imum entropy classification, and support vector
machines. They used frequency or the presence
of unigrams or bigrams as features. In addition
to this, they used combinations of unigrams and
bigrams, unigrams and Part of Speech, unigrams
and their position as features. They got the high-
est accuracy of 82.9% with SVM using a feature
vector of size 16165. Besides this, they showed
that simply using the 2633 most frequent unigrams
are better choice. The feature vector made up of
2633 most frequent unigrams yielded performance
comparable to that of using all unigrams (16165)
from corpus. Our approach based on Chi-Square
test identifies key features from unigrams and en-
hances the performance.

There are a few researchers who have worked
for domain oriented sentiment analysis. The work
of Qiu et al. (2009) exploited the relationship
between sentiment words and product features.
Their method begins with a seed set, then they ex-
tract product features that are modified by some
sentiment word in the domain dependent corpus.135



For example zoom, flash, resolution in the camera
domain can be modified by high, poor, nice, re-
spectively. The process was executed iteratively.
The extraction rules are defined based on the rela-
tionships described in the dependency trees. They
proposed that a feature should receive the same
polarity throughout the review and the words ex-
tracted by the features will receive the polarity of
the feature. However, the reviewer may associate
polarity towards a feature with time. To under-
stand this fact consider the following scenario.

When I purchased this camera, the battery
was good, but now it is disastrous.

The change in time changes the user’s views for a
feature in the same review.

8 Conclusion

In this paper, we propose that if we restrict the
sentiment analysis task to a domain, then domain-
dedicated polar words are the best features for sen-
timent prediction. For this purpose, we present
the SA system as a two stage process, stage-
1 identifies decisive words from a domain spe-
cific corpus for sentiment analysis in that domain.
Stage-2 uses these words as features for classifica-
tion task. Use of domain-dedicated polar words
as features outperforms or equates the accura-
cies achieved independently with unigrams, adjec-
tives, top-adjectives, Universal Sentiment Lexicon
(USL) and union of USL and DDPW. Besides im-
provement in sentiment analysis, the research can
be useful for creating writing aids for authors and
in natural language generation.

References
A. R. Balamurali, Aditya Joshi, and Pushpak Bhat-

tacharyya. 2011. Harnessing wordnet senses for
supervised sentiment classification. In Proceedings
of the Conference on Empirical Methods in Natu-
ral Language Processing, EMNLP ’11, pages 1081–
1091, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.

John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, bollywood, boom-boxes and
blenders: Domain adaptation for sentiment classifi-
cation. In ACL, volume 7, pages 440–447. Citeseer.

Rebecca F. Bruce and Janyce M. Wiebe. 1999. Recog-
nizing subjectivity: a case study in manual tagging.
Nat. Lang. Eng., 5(2):187–205, June.

Cristian Danescu-Niculescu-Mizil, Gueorgi Kossinets,
Jon Kleinberg, and Lillian Lee. 2009. How opinions
are received by online communities: A case study
on amazon.com helpfulness votes. In Proceedings
of the 18th International Conference on World Wide
Web, WWW ’09, pages 141–150, New York, NY,
USA. ACM.

Vasileios Hatzivassiloglou and Janyce M. Wiebe.
2000. Effects of adjective orientation and grad-
ability on sentence subjectivity. In Proceedings of
the 18th Conference on Computational Linguistics
- Volume 1, COLING ’00, pages 299–305, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.

Wei Hui. 2011. A neurual dynamic model based on ac-
tivation diffusion and a micro-explanation for cogni-
tive operations. In Cognitive Informatics Cognitive
Computing (ICCI*CC ), 2011 10th IEEE Interna-
tional Conference on, pages 397–404.

Thorsten Joachims. 1998. Text categorization with
support vector machines: Learning with many rel-
evant features. Springer.

Mitesh M Khapra, Sapan Shah, Piyush Kedia, and
Pushpak Bhattacharyya. 2010. Domain-specific
word sense disambiguation combining corpus based
and wordnet based parameters. In In 5th Interna-
tional Conference on Global Wordnet (GWC2010.
Citeseer.

Soo-Min Kim and Eduard Hovy. 2004. Determin-
ing the sentiment of opinions. In Proceedings of
the 20th international conference on Computational
Linguistics, page 1367. Association for Computa-
tional Linguistics.

Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 17th inter-
national conference on Computational linguistics -
Volume 2, COLING ’98, pages 768–774, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.

Ryan McDonald, Kerry Hannan, Tyler Neylon, Mike
Wells, and Jeff Reynar. 2007. Structured mod-
els for fine-to-coarse sentiment analysis. In An-
nual Meeting-Association For Computational Lin-
guistics, page 432.

Prem Melville, Wojciech Gryc, and Richard D
Lawrence. 2009. Sentiment analysis of blogs by
combining lexical knowledge with text classifica-
tion. In Proceedings of the 15th ACM SIGKDD in-
ternational conference on Knowledge discovery and
data mining, pages 1275–1284. ACM.

Subhabrata Mukherjee, Akshat Malu, Balamurali A.R.,
and Pushpak Bhattacharyya. 2012. Twisent: a mul-
tistage system for analyzing sentiment in twitter. In
Proceedings of the 21st ACM international confer-
ence on Information and knowledge management,
CIKM ’12, pages 2531–2534, New York, NY, USA.
ACM.136



Vincent Ng, Sajib Dasgupta, and S. M. Niaz Arifin.
2006. Examining the role of linguistic knowledge
sources in the automatic identification and classifica-
tion of reviews. In Proceedings of the COLING/ACL
on Main Conference Poster Sessions, COLING-
ACL ’06, pages 611–618, Stroudsburg, PA, USA.
Association for Computational Linguistics.

Bo Pang and Lillian Lee. 2004. A sentimental educa-
tion: Sentiment analysis using subjectivity summa-
rization based on minimum cuts. In Proceedings of
the 42Nd Annual Meeting on Association for Com-
putational Linguistics, ACL ’04, Stroudsburg, PA,
USA. Association for Computational Linguistics.

Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using
machine learning techniques. In Proceedings of the
ACL-02 conference on Empirical methods in natural
language processing-Volume 10, pages 79–86. As-
sociation for Computational Linguistics.

Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen.
2009. Expanding domain sentiment lexicon through
double propagation. In Proceedings of the 21st in-
ternational jont conference on Artifical intelligence,
pages 1199–1204.

Ellen Riloff and Janyce Wiebe. 2003. Learning ex-
traction patterns for subjective expressions. In Pro-
ceedings of the 2003 conference on Empirical meth-
ods in natural language processing, EMNLP ’03,
pages 105–112, Stroudsburg, PA, USA. Association
for Computational Linguistics.

Raksha Sharma and Pushpak Bhattacharyya. 2013.
Detecting domain dedicated polar words. In Pro-
ceedings of the Sixth International Joint Conference
on Natural Language Processing, pages 661–666,
Nagoya, Japan, October. Asian Federation of Nat-
ural Language Processing.

Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2005. Extracting semantic orientations of words us-
ing spin model. In Proceedings of the 43rd An-
nual Meeting on Association for Computational Lin-
guistics, ACL ’05, pages 133–140, Stroudsburg, PA,
USA. Association for Computational Linguistics.

Peter D. Turney. 2002. Thumbs up or thumbs down?:
semantic orientation applied to unsupervised classi-
fication of reviews. In Proceedings of the 40th An-
nual Meeting on Association for Computational Lin-
guistics, ACL ’02, pages 417–424, Stroudsburg, PA,
USA. Association for Computational Linguistics.

Casey Whitelaw, Navendu Garg, and Shlomo Arga-
mon. 2005. Using appraisal groups for sentiment
analysis. In Proceedings of the 14th ACM inter-
national conference on Information and knowledge
management, CIKM ’05, pages 625–631, New York,
NY, USA. ACM.

Janyce Wiebe. 2000. Learning subjective adjectives
from corpora. In Proceedings of the Seventeenth

National Conference on Artificial Intelligence and
Twelfth Conference on Innovative Applications of
Artificial Intelligence, pages 735–740. AAAI Press.

Hidekazu Yanagimoto, Mika Shimada, and Akane
Yoshimura. 2013. Document similarity estima-
tion for sentiment analysis using neural network.
In Computer and Information Science (ICIS), 2013
IEEE/ACIS 12th International Conference on, pages
105–110.

Hong Yu and Vasileios Hatzivassiloglou. 2003. To-
wards answering opinion questions: separating facts
from opinions and identifying the polarity of opinion
sentences. In Proceedings of the 2003 conference
on Empirical methods in natural language process-
ing, EMNLP ’03, pages 129–136, Stroudsburg, PA,
USA. Association for Computational Linguistics.

137


