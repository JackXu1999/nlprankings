



















































SentiSys at SemEval-2016 Task 5: Opinion Target Extraction and Sentiment Polarity Detection


Proceedings of SemEval-2016, pages 350–355,
San Diego, California, June 16-17, 2016. c©2016 Association for Computational Linguistics

SentiSys at SemEval-2016 Task 5: Opinion Target Extraction and Sentiment
Polarity Detection

Hussam Hamdan
Aix-Marseille University

hamdan.hussam@gmail.com

Abstract

This paper describes our contribution in Opin-
ion Target Extraction and Sentiment Polarity
sub-tasks of SemEval 2016 ABSA task. A
Conditional Random Field model has been
adopted for opinion target extraction. A
Logistic Regression model with a weighting
schema of positive and negative labels has
been used for sentiment polarity. Our sub-
mission for opinion target extraction is ranked
second among the constrained systems which
do not use additional resources and sixth over
19 submissions among the constrained and un-
constrained systems in English restaurant re-
views. Our submission for Sentiment Polarity
is ranked eighth over 22 submissions on the
laptop reviews.

1 Introduction

Classifying opinion texts at document or sentence
levels is not sufficient for applications which need
to identify the opinion targets. Even if the docu-
ment is about one entity, many applications need to
determine the opinion about each aspect of the en-
tity. A user may express a positive opinion towards
a restaurant, but he may have a negative opinion to-
wards some aspects as the ambiance. Therefore, we
need to identify the aspects and determine whether
the sentiment is positive, negative or neutral towards
each one. This task is called Aspect-Based Senti-
ment Analysis or Feature-Based opinion mining as
called in the early work (Hu and Liu, 2004).
Aspect-Based Sentiment Analysis is composed of
four subtasks:

1. Opinion Target Expression Extraction

Opinion Target Expression is a linguistic ex-
pression used in a given text to refer to an as-
pect of the reviewed entity. This subtask aims
at identifying all the aspect terms present in a
given set of sentences with pre-identified en-
tities such as restaurants, laptops. An opinion
target names a particular aspect of the target en-
tity. For example:

”I liked the service and the staff, but not the
food”

”The hard disk is very noisy”

The service, staff and food are opinion target
expressions. hard disk is multi-word opinion
target expression which will be treated as a sin-
gle term.

2. Aspect Sentiment Detection

Each identified opinion target has to be as-
signed to one of the following polarity labels:
positive, negative or neutral.

For example:

Input: ”I hated their fajitas, but their salads
were great”

Output: {fajitas: negative, salads: positive}

3. Aspect Category Detection

This subtask aims at identifying the aspect
categories discussed in a given sentence from
a predefined set of aspect categories such as

350



price and food. Aspect categories are typically
coarser than the opinion targets, and they do not
necessarily occur as terms in a given sentence.

For example, given the set of aspect categories
of restaurant entity {food, service, price, am-
biance}:

”The restaurant was too expensive”

The aspect category is {price}.

”The restaurant was expensive, but the menu
was great”

The aspect categories are {price, food}.

4. Aspect Category Polarity

Given a set of pre-identified aspect categories
such as {food, price}, this subtask aims at de-
termining the polarity of each aspect category.
For example:

”The restaurant was too expensive”

{price: negative}
”The restaurant was expensive, but the menu

was great”

{price: negative, food: positive}

In this paper, we focus on Opinion Target Extrac-
tion (OTE) and Sentiment Polarity towards a target
or a category. The description of each subtask is pro-
vided by ABSA organizers (Pontiki et al., 2016). For
OTE, a CRF model is proposed with several groups
of features including syntactic and lexical features.
For polarity detection, a logistic regression classi-
fier is trained with the weighting schema for positive
and negative labels and several groups of features
are extracted including lexical, syntactic, semantic
and sentiment lexicon features.

The rest of this paper is organised as follows. Sec-
tion 2 outlines existing work in target extraction and
polarity detection. Section 3 describes our system
for opinion target extraction. Polarity detection is
presented in Section 4. Section 5 shows the conclu-
sion and the future work.

2 Related Work

Aspect-Based Sentiment Analysis consists of sev-
eral subtasks. Some studies have proposed different
methods for aspect detection and sentiment polarity
analysis, others have proposed joint models in or-
der to obtain the aspect and their polarities from the
same model, these last models are generally unsu-
pervised.

The early work on opinion target detection from
on-line reviews presented by (Hu and Liu, 2004)
used association rule mining based on Apriori al-
gorithm (Agrawal and Srikant, 1994) to extract fre-
quent noun phrases as product features. For polarity
detection, they used two seed sets of 30 positive and
negative adjectives, then WordNet has been used to
find and add the synonyms of the seed words. Infre-
quent product features or opinion targets had been
processed by finding the noun related to an opinion-
ated word.

Opinion Digger (Moghaddam and Ester, 2010)
also used the Apriori algorithm to extract the fre-
quent opinion targets. The kNN algorithm is ap-
plied to estimate the aspect rating scaling from 1 to
5 stands for (Excellent, Good, Average, Poor, Terri-
ble).

Supervised methods use normally Conditional
Random Fields (CRF) or Hidden Markov models
(HMM). (Jin and Ho, 2009) applied a HMM model
to extract opinion targets using the words and their
part-of-speech tags in order to learn a model, then
unsupervised algorithm for determining the opinion
targets polarity using the nearest opinion word to the
opinion target and taking into account the polarity
reversal words (such as not).

A CRF model was used by (Jakob and Gurevych,
2010) with the following features: tokens, POS tags,
syntactic dependency (if the opinion target has a
relation with the opinionated word), word distance
(the distance between the word in the closest noun
phrase and the opinionated word), and opinion sen-
tences (each token in the sentence containing an
opinionated expression is labeled by this feature),
the input of this method is also the opinionated ex-
pressions, they use these expressions for predicting
the opinion target polarity using dependency pars-
ing for retrieving the pair target-expression from the
training set. (Hamdan et al., 2014; Hamdan et al.,

351



2015a) also applied a CRF model with different fea-
tures .

Unsupervised methods based on LDA (Latent
Dirichlet allocation) have been proposed. (Brody
and Elhadad, 2010) used LDA to figure out the opin-
ion targets, determined the number of topics by ap-
plying a clustering method, then they used a similar
method proposed by (Hatzivassiloglou and McKe-
own, 1997) to extract the conjunctive adjectives, but
not the disjunctive due to the specificity of the do-
main.

(Lin et al., 2012) proposed Joint model of Sen-
timent and Topic (JST) which extends the state-of-
the-art topic model (LDA) by adding a sentiment
layer, this model is fully unsupervised and it can de-
tect sentiment and topic simultaneously.

(Wei and Gulla, 2010) modeled the hierarchi-
cal relation between product aspects. They de-
fined Sentiment Ontology Tree (SOT) to formulate
the knowledge of hierarchical relationships among
product attributes and tackled the problem of senti-
ment analysis as a hierarchical classification prob-
lem. Unsupervised hierarchical aspect Sentiment
model (HASM) was proposed by (Kim et al., 2013)
to discover a hierarchical structure of aspect-based
sentiments from unlabeled online reviews.

3 Opinion Target Expression (OTE)

The objective of opinion target extraction is to ex-
tract all opinion target expressions in a restaurant
review, opinion target could be a word or multi-
ple words. This extraction consists of the following
steps:

1. Review Segmentation

This step segments each review into sentences.
In restaurant dataset, we already have the sen-
tences.

2. Sentence Tokenizing

Each sentence is tokenized to get the terms.
One can consider the spaces as separators or
use a more complex tokenizer. We tokenize
each sentence using NTLK tokenizer1 which
extracts the words, numbers and punctuations.

1http://www.nltk.org/api/nltk.tokenize.html

3. Sentence Tagging

Each term in the sentence should be tagged in
order to be presented to a tagging classifier. We
choose the IOB notation for representing each
sentence in the review. Therefore, we distin-
guish the terms at the Beginning, the Inside and
the Outside of opinion target expression. For
example, for the following review sentence:

”But the staff was so horrible to us.”

Where staff is opinion target. The tag of each
term will be:

But:O the:O staff:B was:O so:O horrible:O
to:O us:O.

4. Feature Extraction

This is the main step of opinion target extrac-
tion. For representing each term, we extract the
following features:

• The term itself.
• POS: We use NLTK parser2 to attach a

part of speech tag to each term.
• word shape: the shape of each character in

the word (capital letter, small letter, digit,
punctuation, other symbol)
• word type: the type of the word (upper-

case, digit, symbol, combination )
• Prefixes (all prefixes having length be-

tween one to four).
• Suffixes (all suffixes having length be-

tween one to four).
• Stop word: if the word is a stop word or

not.

In addition to the previous features, we extract
for each term the following features:

• The two preceding and three subsequent
terms of the actual term.
• The value of each two successive features

in the the range -2,2 (the previous and sub-
sequent two terms of actual term) for the
following features: term, word POS, word

2http://www.nltk.org/book/ch05.html

352



shape, word type. For example, for POS
feature we extract:
pos[-2]—pos[-1]=DT—JJ
pos[-1]—pos[0]=JJ—NN
pos[0]—pos[1]=NN—,
pos[1]—pos[2]=,—C
• We also extract the value of each three

successive features in the the range -1,1
for the two features: term POS and term.
For example, for the feature term we ex-
tract:
term[-2]—term[-
1]—term[0]=a—good—place
term[-1]—term[0]—term[1]=good—place—,

5. Training Method

we have used a Conditional Random Fields
(CRF) which receives the feature representa-
tion of each term in each sentence and builds
a tagging model in order to use it for predicting
the tags of the new sentences.

3.1 Experiments

The data set is extracted from restaurant reviews,
provided by SemEval 2016 ABSA organisers (Pon-
tiki et al., 2016) where each review is composed
of several sentences and each sentence may contain
several OTEs. CRFsuite tool is used for this ex-
periment. This tool is fast in training and tagging
(Okazaki, 2007).

Our submission is ranked second among the con-
strained systems and sixth over all 19 systems with
the F1 score. Table 2 shows the result of our system.

Experiment F1 Score
Our System 66.545
Baseline 44.071

Table 1. The results of OTE slot.

4 Sentiment Polarity

For a given set of aspect terms within a sentence, we
determine whether the polarity of each aspect term
is positive, negative or neutral. For example, the sys-
tem should extract the polarity of fajitas and sal-
ads in the following sentence: ”I hated their fajitas,
but their salads were great”, fajitas: negative and
salads: positive.

This sub-task can be seen as sentence level
or phrase level sentiment Analysis. We should
determine the polarity, which could be positive,
negative, neutral. We use the system described by
(Hamdan et al., 2015b). Thus,We propose to use a
logistic regression classifier with weighting schema
of positive and negative labels with the following
features:

- Word n-grams Features
Unigrams and bigrams are extracted for each

word in the text without any stemming or stop-word
removing, all terms with occurrence less than 3 are
removed from the feature space.

- Sentiment Lexicon-based Features
The system extracts four features from the

manually constructed lexicons (Bing Liu Lexicon
(Hu and Liu, 2004) and MPQA subjectivity Lexicon
(Wilson et al., 2005)) and six features from the
automatic ones (NRC Hashtag Sentiment Lexicon
(Mohammad, 2012), Sentiment Lexicon (Hamdan
et al., 2015b) ). For each text the number of positive
words, the number of negative ones, the number of
positive words divided by the number of negative
ones and the polarity of the last word are extracted
from manual constructed lexicons. In addition to
the sum of the positive scores and the sum of the
negative scores from the automatic constructed
lexicons.

- Negation Features
The rule-based algorithm presented in Christo-

pher Potts’ Sentiment Symposium Tutorial is
implemented. This algorithm appends a negation
suffix to all words that appear within a negation
scope which is determined by the negation key and
a certain punctuation. All these words are added to
the feature space.

- Brown Cluster Features
Each word in the text is mapped to its cluster in

Brown clusters, 1000 features are added to feature
space where each feature represents the number
of words in the text mapped to each cluster. The
1000 clusters is provided in Twitter Word Clusters
of CMU ARK group which were constructed from
approximately 56 million tweets.

353



- Category Feature
We also added the category of each OTE as a fea-

ture to the feature space.

4.1 Experiments

We trained a L1-regularized Logistic regression
classifier implemented in LIBLINEAR, which has
given good results in several papers (Hamdan et al.,
2015b) (Hamdan et al., 2015a). The classifier is
trained on the training data set using the previous
features with the three polarities (positive, nega-
tive, and neutral) as labels. A weighting schema is
adapted for each class, we use the weighting option
-wi which enables a use of different cost parameter
C for different classes. Since the training data is un-
balanced, this weighting schema adjusts the proba-
bility of each label. Thus we tuned the classifier in
adjusting the cost parameter C of Logistic Regres-
sion, weight wpos of positive class and weight wneg
of negative class.

We used the 1/10 of training data set for tuning
the three parameters in Laptop reviews, all combi-
nations of C in range 0.1 to to 4 by step 0.1, wpos
in range 1 to 8 by step 0.1, wneg in range 1 to 8
by step 0.1 are tested. The combination C=0.3,
textitwpos=1.2, wneg=1.9 have been chosen. Table
2 shows the results of our system on the Laptop
data set. Our system is ranked eighth over 22
submissions.

Experiment Accuracy
Laptops
Our system 74.282
Baseline 44.071

Table 2. Results of sentiment polarity in laptops
reviews.

5 Conclusion

We have built two systems for opinion target extrac-
tion of restaurant data set, and sentiment polarity
analysis for laptops. We have used supervised tagger
for OTE, trained a CRF model with several features.
A Logistic regression classifier is used for sentiment
polarity where we adopted a weighting schema.

References
Agrawal, R. and Srikant, R. (1994). Fast Algorithms for

Mining Association Rules in Large Databases. In Pro-
ceedings of the 20th International Conference on Very
Large Data Bases, VLDB ’94, pages 487–499, San
Francisco, CA, USA. Morgan Kaufmann Publishers
Inc.

Brody, S. and Elhadad, N. (2010). An Unsupervised
Aspect-sentiment Model for Online Reviews. In Hu-
man Language Technologies: The 2010 Annual Con-
ference of the North American Chapter of the As-
sociation for Computational Linguistics, HLT ’10,
pages 804–812, Stroudsburg, PA, USA. Association
for Computational Linguistics.

Hamdan, H., Bellot, P., and Bechet, F. (2014). Super-
vised Methods for Aspect-Based Sentiment Analysis.
In Proceedings of the Eighth International Workshop
on Semantic Evaluation (SemEval 2014).

Hamdan, H., Bellot, P., and Bechet, F. (2015a). Lsislif:
CRF and Logistic Regression for Opinion Target Ex-
traction and Sentiment Polarity Analysis. In Proceed-
ings of the 9th International Workshop on Semantic
Evaluation (SemEval 2015), pages 753–758, Denver,
Colorado. Association for Computational Linguistics.

Hamdan, H., Bellot, P., and Bechet, F. (2015b). Senti-
ment Lexicon-Based Features for Sentiment Analysis
in Short Text. In Proceeding of the 16th International
Conference on Intelligent Text Processing and Compu-
tational Linguistics, Cairo, Egypt.

Hatzivassiloglou, V. and McKeown, K. R. (1997). Pre-
dicting the Semantic Orientation of Adjectives. In
Proceedings of the Eighth Conference on European
Chapter of the Association for Computational Lin-
guistics, EACL ’97, pages 174–181, Stroudsburg, PA,
USA. Association for Computational Linguistics.

Hu, M. and Liu, B. (2004). Mining and Summariz-
ing Customer Reviews. In Proceedings of the Tenth
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, KDD ’04, pages
168–177, New York, NY, USA. ACM.

Jakob, N. and Gurevych, I. (2010). Extracting Opinion
Targets in a Single- and Cross-domain Setting with
Conditional Random Fields. In Proceedings of the
2010 Conference on Empirical Methods in Natural
Language Processing, EMNLP ’10, pages 1035–1045,
Stroudsburg, PA, USA. Association for Computational
Linguistics.

Jin, W. and Ho, H. H. (2009). A Novel Lexicalized
HMM-based Learning Framework for Web Opinion
miningNOTE FROM ACM: A Joint ACM Conference
Committee Has Determined That the Authors of This
Article Violated ACM’s Publication Policy on Simul-
taneous Submissions. Therefore ACM Has Shut off

354



Access to This Paper. In Proceedings of the 26th
Annual International Conference on Machine Learn-
ing, ICML ’09, pages 465–472, New York, NY, USA.
ACM.

Kim, S., Zhang, J., Chen, Z., Oh, A., and Liu, S. (2013).
A Hierarchical Aspect-Sentiment Model for Online
Reviews. In Proceedings of The Twenty-Seventh
AAAI Conference on Artificial Intelligence (AAAI-13).
AAAI.

Lin, C., He, Y., Everson, R., and Ruger, S. (2012).
Weakly Supervised Joint Sentiment-Topic Detection
from Text. IEEE Trans. on Knowl. and Data Eng.,
24(6):1134–1145.

Moghaddam, S. and Ester, M. (2010). Opinion Digger:
An Unsupervised Opinion Miner from Unstructured
Product Reviews. In Proceedings of the 19th ACM
International Conference on Information and Knowl-
edge Management, CIKM ’10, pages 1825–1828, New
York, NY, USA. ACM.

Mohammad, S. (2012). #Emotional Tweets. In *SEM
2012: The First Joint Conference on Lexical and
Computational Semantics – Volume 1: Proceedings
of the main conference and the shared task, and Vol-
ume 2: Proceedings of the Sixth International Work-
shop on Semantic Evaluation (SemEval 2012), pages
246–255, Montréal, Canada. Association for Compu-
tational Linguistics.

Okazaki, N. (2007). CRFsuite: a fast implementation of
Conditional Random Fields (CRFs).

Pontiki, M., Galanis, D., Papageorgiou, H., Androut-
sopoulos, I., Manandhar, S., AL-Smadi, M., Al-
Ayyoub, M., Zhao, Y., Qin, B., Clercq, O. D., Hoste,
V., Apidianaki, M., Tannier, X., Loukachevitch, N.,
Kotelnikov, E., Bel, N., Jimenez-Zafra, S. M., and
Eryigit, G. (2016). SemEval-2016 task 5: Aspect
based sentiment analysis. In Proceedings of the 10th
International Workshop on Semantic Evaluation, Se-
mEval ’16, San Diego, California. Association for
Computational Linguistics.

Wei, W. and Gulla, J. A. (2010). Sentiment learning on
product reviews via sentiment ontology tree. In Pro-
ceedings of the 48th Annual Meeting of the ACL, pages
404–413. ACL.

Wilson, T., Hoffmann, P., Somasundaran, S., Kessler, J.,
Wiebe, J., Choi, Y., Cardie, C., Riloff, E., and Pat-
wardhan, S. (2005). OpinionFinder: A System for
Subjectivity Analysis. In Proceedings of HLT/EMNLP
on Interactive Demonstrations, HLT-Demo ’05, pages
34–35, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.

355


