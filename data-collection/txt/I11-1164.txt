















































Finding Problem Solving Threads in Online Forum


Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 1413–1417,
Chiang Mai, Thailand, November 8 – 13, 2011. c©2011 AFNLP

Finding Problem Solving Threads in Online Forum

Zhonghua Qu and Yang Liu
The University of Texas at Dallas

{qzh,yangl}@hlt.utdallas.edu

Abstract

Online forum is an important source that
people use to find answers to questions.
Most search engines simply retrieve “rele-
vant” threads, but those are not necessarily
good threads in terms of providing qual-
ity answers. In this paper we propose a
two-step approach to classify online forum
threads according to their informativeness
in terms of question answering. We use
statistical models to first categorize posts
inside a thread. Then, a variety of features
including post level information and other
meta-data information are used to classify
the thread. We show promising results us-
ing the online support forum data we have
collected.

1 Introduction

Online forums often contain useful information,
such as answers to questions. When people use
search engines to search the forum for answers,
the returned threads can be of very low quality
in terms of providing informative answers. Some
threads are either long conversations without any
final conclusive answers or contain answers that
do not work. This is especially the case in tech-
nical forums. There has been some work on ex-
tracting information from online forums. For ex-
ample, Cong et al. (Cong et al., 2008) used la-
beled sequential patterns to detect question sen-
tences in online forums. Ding et al. (Ding et al.,
2008) used Conditional Random Fields to extract
context of questions for answer detection. Huang
et al. (Huang et al., 2007) used SVM to automati-
cally extract and rank title-reply pairs from online
discussion forums for chatbot knowledge. These
previous studies provide a relatively good founda-
tion for answer finding and extraction. In online
environments like mailing list or forums, question

answering is carried out in conversations. In this
paper we take advantage of the structure of con-
versation and language cues to answer a basic yet
important question: Does the thread actually solve
the problem?

We will make use of the conversation within
the thread to determine how users think of the an-
swers. To classify the usefulness of a thread, we
propose a two-step approach using statistical mod-
els. First we classify the posts inside a thread into
different categories (e.g., problem description, so-
lution, feedback). Different models using both
content and contextual information are evaluated.
Then, we develop features generated from the post
categories together with forum meta-data to clas-
sify a thread’s usefulness. In our collection of fo-
rum threads, we show that our proposed methods
achieve good results, significantly better than the
rule-based baseline.

2 Data Collection and Annotation

We created our own data collection and annota-
tion.1 We crawled 20,000 threads from an ac-
tive online forum (Oracle database support forum
– general section). As an initial study, we se-
lected 200 threads randomly and asked two anno-
tators to label the threads and posts in them. The
two annotators are computer science students with
adequate knowledge to understand the content of
those posts. For thread level annotation, we asked
the annotators to label the thread based on whether
they think it solves the problem and to what ex-
tent. We did not give annotators detailed instruc-
tions, but rather let them read the threads and make
their own judgment. Each thread is given a score
from 1 to 5, 1 being least helpful and 5 being most
helpful. In the 200 threads we used, 50 have a use-
fulness score of 1, 19 with 2, 7 with 3, 61 with 4,
and 83 with a score of 5. The distribution has a U

1Please contact the authors for data sharing.

1413



shape – many threads are annotated as absolutely
solved the problem (usefulness of 5), or absolutely
not helpful at all (usefulness of 1). This also shows
that the confidence of annotators is very high.

Another annotation we performed is for the
posts inside each thread. We defined four classes
for the posts based on their main purpose. The cat-
egories and brief explanation for them are shown
in Table 1, along with the number of posts for each
category. Note that sentences inside a post may
have different purposes, however, we instructed
the annotators to label it with its main purpose.
The post classes are very imbalanced in the an-
notated corpus. The feedback classes account for
only about 10% of all the posts, while the majority
classes are problems and solutions.

Post category Description number
Problem Ask a question or ask for

answer clarification.
399

Solution Give answers/clarifications
to previous questions.

557

Good Feedback State that the problem is
solved.

78

Bad Feedback State that the answer did not
solve the problem.

18

Table 1: Annotated categories for posts inside
threads.

Table 2 shows the Kappa statistics on thread and
post annotation using 20 threads that we randomly
selected and let both annotators label them. For
the thread annotation agreement, we use binary la-
bels – usefulness greater than 3 is considered as 1,
and usefulness less than 3 is considered as 0. The
Kappa scores are very reasonable, suggesting that
humans do not have much trouble with this task
definition.

Classes Kappa Coefficient
Thread Usefulness 0.93
Post - Problem 0.86
Post - Solution 0.70
Post - Good Feedback 0.93

Table 2: Kappa statistics between two annotators
for thread and post category annotation.

3 Approaches

For the ultimate goal of classifying how likely a
thread provides a good solution (its usefulness),
we propose to use a two-step approach. First we
determine the categories for the posts, then we

classify the threads using information from the
first step along with other information.

3.1 Post Level Classification
We use the content of a post as well as its context
for post classification. First we used Naive Bayes
classifier with a bag-of-word model for post classi-
fication. Naive Bayes classifiers have been proved
to be robust and perform well in document classifi-
cation. After performing tokenization using space,
we use the top 600 words from the training set as
the dictionary.2 Each post is thus represented by
a feature vector of tokens that appear in the dic-
tionary. We use binary features for these words.
This has been shown to perform better than using
word frequency information for many classifica-
tion tasks. We use the implementation of multi-
nomial Naive Bayes classifier provided by Weka
(Hall et al., 2009).

We expect that posts in a thread, like turns in di-
alogs, are very dependent on the context. For ex-
ample, a “problem description” post is more likely
to be followed by “solution” posts than a “feed-
back” one. People are more likely to give “feed-
back” after some “answers” are provided rather af-
ter a “question” is asked. Hence we evaluate using
an HMM for this post categorization task, in order
to take advantage of context information. Figure 1
shows a first order HMM for this setup.

c2

d2

c3

d3

c1

d1

Figure 1: Illustration of HMM for post categoriza-
tion. An observation di is a post, and ci corre-
sponds to its category.

The generative process is as follows. The cat-
egory of current post is generated according to
a multinomial distribution P (ci|ci−1) conditioned
on its previous post category. Then, given the
category of current post, words in the post are
generated according to multinomial distribution
P (di|ci) based on the post’s category.

The problem can be formulated to find the most
likely state sequence C (post categories) given all

2We varied the dictionary size, but observed performance
degradation.

1414



the words in all the posts D. The posterior proba-
bility of C given D is:

P (C|D) = P (c1, .., cn|d1, .., dn)
∝ P (d1, .., dn|c1, .., cn)× P (c1, .., cn)

=
n∏

i=1

P (di|ci)× P (ci|ci−1) (1)

The last equation is due to the first order HMM
assumption where a post document is only depen-
dent on its category (in a generative process), and
a post category is only dependent on its previous
category. For parameters in this HMM, the tran-
sition probabilities are estimated from the training
set, similar to training a bigram language model
(LM) for the post categories.

It is possible to use an n-gram language model
to model the generation of text in posts given its
category. However, since our data set is rather
small, there is a data sparsity issue for these n-
gram LMs. Hence, we only use a unigram-like
model for a subset of the words (same as those in
the Naive Bayes model), and each word is repre-
sented in binary form: whether or not it appeared.

In practice, when combining the state transition
probabilities and emission probabilities, we use a
weighting factor γ that is determined empirically.
This is needed because the two scores are in dif-
ferent scales and contribute to the final hypothesis
differently.

3.2 Thread Level Classification
The second step in our system is thread level clas-
sification, where the system determines whether
a thread is useful or not. Using our annotated
threads, we grouped them into binary classes: use-
ful when the label is equal to or greater than 3; and
not useful otherwise. We use a variety of features
for thread level classification. Table 3 lists all the
features we use for thread classification.

Since the post level classifier is optimized for
the classification accuracy for that particular task,
its hypotheses might not be optimal for subse-
quent thread classification. For example, if a post
“looks” like both a “good feedback” and a “prob-
lem”, we may prefer “good feedback” to “prob-
lem” since the former is more related to our final
decision for the thread usefulness. To achieve this,
we adjust the priors in the post classification pro-
cess for different categories. We found that by
increasing the prior for “good feedback” by 1.5
times, the final performance is improved – even

though the classification precision decreases, the
improved recall seems to be helping.

Name Description
num solution The number of solution posts
feedback p Number of positive feedbacks
num prob sol Times of back and forth be-

tween problem and solutions
author post Number of posts posted by au-

thor
author postend How far the author’s last post is

from the end
length Length of a thread (measured

using number of posts)

Table 3: Features used for thread level classifica-
tion. ‘Author’ means the user who first posted the
problem and started the thread.

We use a maximum entropy model for thread
classification. This classifier is chosen because of
its good performance in many language process-
ing tasks, as well as our own preliminary experi-
ments when comparing to other classifiers includ-
ing decision trees and SVMs.

4 Experiments

4.1 Post Level Classification Results

We performed leave-one-out-cross-validation (us-
ing threads as the units) for post classification.
Table 4 shows the results using different meth-
ods. We developed a rule-based baseline system
for this task. Posts containing typical question
words like “what”,“where” are classified as “prob-
lem” posts. Those containing cue words for pos-
itive feedbacks, like “thanks”, “solved”, and hav-
ing length of less than 50 words are classified as
“positive feedback” posts. The rest are classified
as solutions.

The results shown in the table include the F1
score for 3 individual post categories: problem
(F-P), solution (F-S), and good feedback (F-F), as
well as the micro averaged F1 score. Here we ig-
nored the classification results for “bad feedback”
category since it is not very useful in later thread
classification.

We can see that the basic Naive Bayes (NB)
classifier can achieve reasonable performance al-
ready. It is significantly better than the base-
line. This shows that lexicon features (bag-of-
word model) are strong and reliable in classify-
ing post type. We also tried using an SVM (with
linear kernels) for post level classification, but its
performance is worse than the Naive Bayes clas-

1415



Method Fmicro F1-P F1-S F1-F
Baseline: rule 44.58 49.59 46.53 18.58
NB 77.57 76.81 85.52 28.57
HMM 83.17 83.21 87.75 36.04

Table 4: Post classification results using different
methods. Results are the F-measures for problem,
solution, and good feedback categories, and the
overall performance.

sifier. After using context information through
HMM, post level classification performance in-
creased substantially. There is consistent improve-
ment for all the categories. In particular, there is
a relatively larger gain for the “feedback” class,
which we expect may have a great impact on sub-
sequent thread level classification. This perfor-
mance gain using HMM demonstrates that con-
textual information is useful and HMM can well
model such information (e.g., an ‘answer’ post is
likely to follow a ‘problem’ one). We also in-
creased HMM to higher order in order to model
the transitions using more previous post cate-
gories, but found there is no additional improve-
ment. In fact, it is slightly worse than using the
first order HMM. This can be explained by either
that the long distance relationship is not very cru-
cial for this task, or more likely that we do not
have a large training set to learn high order transi-
tion information reliably.

In general, we can see that the classifiers per-
form relatively well for the majority categories,
such as problems and solutions. However, for mi-
nority classes, e.g., “feedback” (that is only 9.8%
among all the posts), the classifiers are not able to
learn well. We also observe that many “problem”
types are classified as “solutions”. This may be
explained by two reasons. First, problem descrip-
tion posts use very similar vocabulary as solution
posts. Second, there are more solution posts in the
training data.

4.2 Thread Level Classification Results

For thread level classification, we use a binary
setup, useful vs. not. We used leave-one-out-
cross-validation for the 200 labeled threads. In-
side each fold, first we use the training set to train
the post level classifiers. Then we relabel all the
posts in the training set as well as in the testing set
with the classifier just trained to obtain the post
category hypotheses. After the post level labeling,
we extract features for thread classification as de-

scribed in Section 3. We then train the maximum
entropy classifier and test it for the final classi-
fication of threads. We use the precision, recall,
and F1 score as the evaluation metrics. Results are
shown in Table 5. For a comparison, we also show
the performance when using reference post labels
for both training and test sets (last row). This is
expected to give an upper bound performance re-
garding the use of post level information for thread
classification.

Post Classifier Precision Recall F1
Baseline: rule 68.50 97.16 80.35
NB 78.21 86.52 82.15
HMM 79.14 91.32 84.97
Ref 84.62 93.62 88.89

Table 5: Thread level binary classification results
using features extracted from output of different
post level classifiers.

We can see from Table 5 that in general thread
classification results depend heavily on the perfor-
mance of post level classification. HMM achieves
the best performance. Using reference post cat-
egory shows the upper bound of how post level
classification could affect thread level classifica-
tion. We also evaluated using the reference post
tags in training the thread classifier, and automatic
tags for testing, but that did not perform as well as
using automatically obtained tags for both train-
ing and testing, suggesting a matched training and
testing condition is better. Overall we achieved
very good performance – an F1 score of about
85% using HMM for post categorization. This is a
promising result given that we have a quite small
data set for training.

5 Conclusion

In this paper, we described a two-step classifi-
cation approach to determine whether a thread
is helpful to users seeking solutions. We per-
form post level classification in the first step,
then use the generated post tag information with
other global features for thread level classification.
We showed in the experiment that our approach
worked well in a technical support online forum.
For future work, we plan to use joint optimization
for the two tasks. In addition, instead of using pre-
defined categories for posts, we may derive post
categories automatically from the corpus that suit
better for the thread classification task.

1416



References
Gao Cong, Long Wang, Chin-Yew Lin, Young-In Song,

and Yueheng Sun. 2008. Finding question-answer
pairs from online forums. In SIGIR, pages 467–474.

Shilin Ding, Gao Cong, Chin-Yew Lin, and Xiaoyan
Zhu. 2008. Using conditional random fields to ex-
tract contexts and answers of Questions from online
forums. In ACL.

Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: an update.
SIGKDD Explorations, 11(1):10–18.

Jizhou Huang, Ming Zhou, and Dan Yang. 2007. Ex-
tracting Chatbot Knowledge from Online Discussion
Forums. In SIGIR, pages 423–428.

1417


