

















































Learning with Partially Ordered Representations

Jane Chandlee
Tri-Co Department of Linguistics

Haverford College
jchandlee@haverford.edu

Rémi Eyraud
QARMA team, LIS

Aix-Marseille University
remi.eyraud@lis-lab.fr

Jeffrey Heinz
Department of Linguistics

Institute for Advanced Computational Science
Stony Brook University

jeffrey.heinz@stonybrook.edu

Adam Jardine
Department of Linguistics

Rutgers University
adam.jardine@rutgers.edu

Jonathan Rawski
Department of Linguistics

Institute for Advanced Computational Science
Stony Brook University

jonathan.rawski@stonybrook.edu

Abstract

This paper examines the characterization and
learning of grammars defined with enriched
representational models. Model-theoretic ap-
proaches to formal language theory tradition-
ally assume that each position in a string be-
longs to exactly one unary relation. We con-
sider unconventional string models where po-
sitions can have multiple, shared properties,
which are arguably useful in many applica-
tions. We show the structures given by these
models are partially ordered, and present a
learning algorithm that exploits this order-
ing relation to effectively prune the hypothe-
sis space. We prove this learning algorithm,
which takes positive examples as input, finds
the most general grammar which covers the
data.

1 Introduction

Foundational connections between formal lan-
guages, finite-state automata, and logic have been
known for decades (Büchi, 1960; Thomas, 1997).
Logical approaches are advantageous since they
flexibly admit different representations. In many
domains, such as biological sequencing or linguis-
tics, shared properties of symbols in sequences
provide information currently ignored by string-
based inference algorithms, which largely focus
on learning automata (de la Higuera, 2010). Here
we explore the idea that domain-specific knowl-
edge can be encoded representationally via model

theory (Libkin, 2004), and shows how these repre-
sentations can facilitate pattern learning.

This paper synthesizes results in grammatical
inference and model theory to present a novel al-
gorithm which learns classes of formal languages
using enriched representations of strings. In fact,
our model-theoretic approach immediately gen-
eralizes these results to arbitrary data structures.
Here we are concerned with the learning of those
formal languages which can be defined via a set
of structural constraints, such as the Strictly k-
Local and Strictly k-Piecewise languages (Rogers
and Pullum, 2011; Rogers et al., 2010). Models
of strings in the languages must not contain these
forbidden structures (Rogers et al., 2013). Specif-
ically, we define a learner whose hypothesis space
is structured as a partial order by the relational sig-
nature of the particular model theory. We show
how to traverse this space bottom-up from posi-
tive data to find a grammar which covers the data
with the most general constraints.

The paper is structured as follows: Section
2 provides mathematical preliminaries in model
theory. Section 3 characterizes ordering rela-
tions over these structures. Section 4 generalizes
the grammars employed in string extension and
lattice-based learning (Heinz, 2010; Heinz et al.,
2012) to show how these model theoretic struc-
tures can define classes of formal languages. Sec-
tion 5 discusses some entailments our learning al-



1

a

2

b

3

b

4

a
/ / /

1

a

2

b

3

b

4

a
< < <

<
<

<

Figure 1: Visualizations of the successor (left) and precedence (right) models of abba.

gorithm takes advantage of. Section 6 defines a
learning problem and criteria for selecting ade-
quate solutions. Section 7 presents a general-to-
specific, bottom-up algorithm which provably sat-
isfies the learning criteria. Section 8 concludes the
paper.

2 Preliminaries

2.1 Elements of Language Theory
The set of all possible finite strings of symbols
from a finite alphabet Σ and the set of strings of
length ≤ n are Σ∗ and Σ≤n, respectively. The
unique empty string is represented with λ . The
length of a string w is |w|, so |λ |= 0. If u and v
are two strings then we denote their concatenation
with uv. If w is a string and σ is the ith symbol in
w then wi = σ , so abcd2 = b.

The set of prefixes of w, Pref(w), is
{p ∈ Σ∗ | (∃s ∈ Σ∗)[w = ps]}, the set of suffixes
of w, Suff(w), is {s ∈ Σ∗ | (∃p ∈ Σ∗)[w = ps]},
the set of substrings, Substr(w), is
{u ∈ Σ∗ | (∃l,r ∈ Σ∗)[w = lur]}, and
the set of subsequences, Subseq(w) =
u1u2 · · ·un|∃v0 · v1 · · ·vn ∈ Σ∗[w = v0u1v1 · · ·unvn]

2.2 Elements of Finite Model Theory
Model theory, combined with logic, provides a
powerful way to study and understand mathemat-
ical objects with structures (Enderton, 2001). In
this paper we only consider finite relational mod-
els (Libkin, 2004) of strings in Σ∗.
Definition 1 (Models). A model signature is a tu-
ple S = 〈D;R1,R2, . . . ,Rm〉 where the domain D is
a finite set, and each Ri is a ni-ary relation over
the domain. A model for a set of objects Ω is
a total, one-to-one function from Ω to structures
whose type is given by a model signature.

For example, a conventional model for
strings in Σ∗ is given by the signature
Γ� de f= 〈D;�, [Rσ ]σ∈Σ〉 and the function M� :
Σ∗ → Γ� such that M�(w) de f= 〈Dw;�, [Rwσ ]σ∈Σ〉
where Dw

de f
= {1, . . . , |w|} is the domain,

�
de f
= {(i, i + 1) ∈ D × D} is the successor

relation which orders the elements of the domain,
and [Rwσ ]σ∈Σ is a set of |Σ| unary relations such
that for each σ ∈ Σ, Rwσ

de f
= {i ∈ Dw | wi = σ}. We

will usually omit the superscript w since it will be
clear from the context.

For example, with Σ = {a,b,c} and the
model above for strings, we have M�(abba) =
〈D = {1,2,3,4};� = {(1,2),(2,3),(3,4)},Ra =
{1,4},Rb = {2,3},Rc = /0〉 .

Figure 1 illustrates M�(abba) on the left.
Another conventional model is the precedence

model, with the signature Γ< de f= 〈D;<, [Rσ ]σ∈Σ〉.
It differs from the successor model only in that the
order relation is defined with general precedence

<
de f
= {(i, j) ∈ D×D | i < j} (Büchi, 1960; Mc-

Naughton and Papert, 1971; Rogers et al., 2013).
Under this signature, the string abba has the fol-
lowing model.

M<(abba) = 〈D = {1,2,3,4};<=
{(1,2),(1,3),(1,4),(2,3),(2,4),(3,4)},Ra =
{1,4},Rb = {2,3},Rc = /0〉.

Figure 1 illustrates M<(abba) on the right.
The model-theoretic framework is not unique to

strings. It extends to arbitrary data structures by
expanding parts of the model signature. For ex-
ample, Rogers (2003) describes a model-theoretic
characterization of trees of arbitrary dimension-
ality where the domain D is a Gorn tree domain
(Gorn, 1967). This is a hereditarily prefix closed
set D of node addresses, that is to say, for every
d ∈ D with d = αi, where i ∈ N, α ∈ N∗ it holds
that α ∈D, and for every d ∈D with d = αi 6= α0,
then α(i−1) ∈ D.

In this view, a string may be called a one-
dimensional or unary-branching tree, since it has
one axis along which its nodes are ordered. In
a standard tree mdoel signature, the set of nodes
is ordered by two binary relations, “dominance”
and “immediate left-of”. Suppose s is the mother
of two nodes t and u in some standard tree, and
also assume that t precedes u. Then we might
say that s dominates the string tu. Standard or



two-dimensional trees, then, relate nodes to one-
dimensional trees (strings) by immediate domi-
nance. A three-dimensional tree relates nodes to
two-dimensional, i.e. standard trees, correspond-
ing to Tree-Adjoining Grammar derivations. In
general, a d-dimensional tree is a set of nodes
ordered by d dominance relations such that the
n-th dominance relation relates nodes to (n− 1)-
dimensional trees (for d = 1, single nodes are
zero-dimensional trees).

While a Gorn tree domain as written encodes
these dominance and precedence relations implic-
itly, we may explicitly write them out model-
theoretically so that a signature for a Σ-labeled 2-
d tree is Γ�≺ = 〈D;�,≺, [Rσ ]σ∈Σ〉 where � is the
“immediate dominance” relation and≺ is the “im-
mediate left-of” relation. Model signatures that in-
clude transitive closure relations of each of these
have also been studied.

ε

0

00 01

010 011

1

10 11

110 111

1110

112

Figure 2: 2-dimensional tree model. Dominance and
precedence relations shown with solid/dashed and dot-
ted lines, respectively

2.3 Unconventional Word Models

Whereas Rogers (2003) generalized conventional
word models to trees, here we generalize word
models in a different way. Conventional string
models are the successor and precedence models
introduced previously. What makes these mod-
els conventional is the unary relations which es-
sentially label each domain element with a single,
mutually exclusive, property: the property of be-
ing some σ ∈ Σ.

In contrast, unconventional models for strings
recognize that distinct alphabetic symbols may
share properties, and expands the model signa-
ture by including these properties as unary re-
lations (Strother-Garcia et al., 2016; Vu et al.,
2018). For example, a conventional model of

Σ = {a, . . . ,z,A, . . . ,Z}would include 52 unary re-
lations, one for each lowercase and capital letter.
On the other hand, an unconventional model might
only include 27: 26 for the letters, and one unary
relation Capital. Then, letters A and a share the
‘A’ property and A additionally has the property of
being Capital.

In linguistics, speech sounds are commonly
decomposed into binary features based on their
phonetic properties. So the set of segments
{z,Z,d,b,g,. . .} all share the property +Voice,
meaning the vocal cords are activated, while
the segments {s,S,t,p,k,. . .} share the property
-Voice, meaning the vocal cords are not acti-
vated. Thus unconventional models may refer to
individual features in defining grammatical con-
straints, rather than each individual segment.

Different representations of strings and trees
provide a unified perspective on well-known sub-
classes of the regular languages from a model-
theoretic and logical perspective (Thomas, 1997;
Rogers et al., 2013). However, they also open up
new doors for grammatical inference by allowing
one to consider other models for strings (Strother-
Garcia et al., 2016; Vu et al., 2018).

3 Subfactors, Superfactors, Ideals and
Filters

We sometimes refer to the model of a string w as a
structure. However, structures are more general in
that they correspond to any mathematical structure
conforming to the model signature. As such, while
a model of a string w will always be a structure, a
structure will not always be a model of a string
w. The size of a structure S, denoted |S|, coincides
with the cardinality of its domain.

We next wish to introduce a partial ordering
over structures. To do so, we must define the terms
connected, restriction, and factor. For each struc-
ture S = 〈D;�,R1, . . .Rn〉 let the binary “connect-
edness” relation C be defined as follows.

C
de f
= {(x,y) ∈ D × D | ∃i ∈

{1 . . .n},∃(x1 . . .xm) ∈ Ri,∃s, t ∈ {1 . . .m},x =
xs,y = xt}

Informally, domain elements x and y belong to
C provided they belong to some non-unary rela-
tion. Let C∗ denote the symmetric transitive clo-
sure of C.

Definition 2 (Connected structure). A structure
S = 〈D;�,R1,R2, . . . ,Rn〉 is connected iff for all
x,y ∈ D, (x,y) ∈C∗.



For example, M�(abba) above is a connected
structure. However, the structure Sab, ba shown
below which is identical to M�(abba) except it
omits the pair (2,3) from the order relation is
not connected since none of (1,3),(1,4), (2,3) nor
(2,4) belong to C∗. Sab, ba = 〈D = {1,2,3,4};�=
{(1,2),(3,4)},Ra = {1,4},Rb = {2,3},Rc = /0〉

1

a

2

b

3

b

4

a
/ /

Note that no string in Σ∗ has structure Sab, ba as
its model.

Definition 3. A = 〈DA;�,RA1 , . . . ,RAn 〉 is a re-
striction of B = 〈DB;�,RB1 , . . . ,RBn 〉 iff DA ⊆ DB
and for each m-ary relation Ri, we have RAi =
{(x1 . . .xm) ∈ RBi | x1, . . . ,xm ∈ DA}.

Informally, one identifies a subset A of the do-
main of B and strips B of all elements and relations
which are not wholly within A. What is left is a re-
striction of B to A.

Definition 4. Structure A is a subfactor of struc-
ture B (Av B) if A is connected, there exists a re-
striction of B denoted B′, and there exists h : A→
B′ such that for all a1, . . .am ∈ A and for all Ri in
the model signature: if h(a1), . . .h(am) ∈ B′ and
Ri(a1, . . .am) holds in A then Ri(h(a1), . . .h(am))
holds in B′. If Av B we also say that B is a super-
factor of A.

In other words, properties that hold of the con-
nected structure A also hold in a related way within
B.

If A v B and |A|= k then we say A is a
k-subfactor of B. For all w ∈ Σ∗, and for
any model M of Σ∗, let the subfactors of
w be Subfact(M,w) = {A | A v M(w)} and
the k-subfactors of w be Subfactk(M,w) =
{A | A v M(w), |A|≤ k}. We also define
Subfact(M,Σ∗) to be

⋃
w∈Σ∗ Subfact(M,w) and

Subfactk(M,Σ∗) to be
⋃

w∈Σ∗ Subfactk(M,w).
When M is understood from context, we write
Subfact(w) instead of Subfact(M,w). We de-
fine the sets of superfactors Supfact(M,w) and
Supfact(M,Σ∗) similarly.

Observe that (Subfact(M,w),v) is a partially
ordered set (poset). The next definition and lemma
establishes that models of strings are principal el-
ements of ideals and filters.

Definition 5 (Ideals). A subset I of a poset is an
Ideal if

• I is non-empty

• for every x in I, y≤ x implies that y is in I

• for every x,y in I, there exists some element z
in I, such that x≤ z and y≤ z.

The dual of an ideal is a filter.

Definition 6 (Filters). A subset F of a poset is a
filter iff

• F is non-empty

• for every x in F, x≤ y implies that y is in F

• for every x,y in F, there exist some element z
in F, such that z≤ x and z≤ y.

Definition 7 (Principal Ideals, Filters and Ele-
ments). For any poset 〈X ,≤〉, the smallest filter
containing x ∈ X is a principal filter and x is the
principal element of this filter. Similarly, the small-
est ideal containing x ∈ X is a principal ideal and
x is the principal element of this ideal.

Remark 1. Given a model M of Σ∗ and
k > 0, Subfactk(M,w) is a principal ideal
in Subfact(M,Σ∗) whose principal element is
M(w). Supfactk(M,w) is a principal filter
in Supfact(M,Σ∗) whose principal element is
M(w). The empty structure 〈 /0; /0, . . . /0〉 is a sub-
factor of every structure in Subfact(M,Σ∗).

The next two propositions show how this rep-
resentational perspective unifies the treatment of
substrings and subsequences. They are subfactors
under the successor and precedence models, re-
spectively. A string x = x1 · · ·xn is a substring of y
iff there exists l,r such that y = lxr. String x is a
subsequence of y iff there exists v0,v1, . . .vn such
that w = v0x1v1 · · ·xnvn.
Proposition 1 (Substrings are subfactors under
M�). For all strings x,y ∈ Σ∗, x is a substring of y
iff M�(x)vM�(y).

Proof. Note that the result trivially holds for x =
λ : we restrict ourselves to the case x 6= λ . Let
M�(x) = 〈Dx;�, [Rxσ ]〉 and M�(y) = 〈Dy;�, [R

y
σ ]〉

(⇒). Suppose x is a substring of
y: it exists l,r such that y = lxr =
σ1 . . .σ|l|σ|l|+1 . . .σ|l|+|x|σ|l|+|x|+1 . . .σ|l|+|x|+|r|.
This implies that, for all i, 1 ≤ i ≤ |x|, d ∈ Ryσ|l|+i
iff d ∈ Rxσi . Thus, if we set the isomorphism φ
to be such that φ(i) = |l|+i for 1 ≤ i ≤ |x|, we
have φ(M�(x)) that is a restriction of M�(y), and
therefore M�(x)vM�(y) by definition.



(⇐). Let y be the sequence of letters σ1 . . .σ|y|
and suppose M�(x) v M�(y): there exists a iso-
morphism φ : {1, . . . , |x|} → {1, . . . , |y|} such that
φ(M�(x)) is a restriction of M�(y). This means
that φ(Dx) ⊆ Dy and for all σ : φ(Rxσ ) = {φ(i) ∈
Ryσ | φ(i) ∈ φ(Dx)} (Definition 3). This implies
that x = σφ(1) . . .σφ(|x|). Given that � = {(i, i +
1) ∈ D×D}, we have φ(i + 1) = φ(i) + 1 and
thus there exist l and r in Σ∗ such that y =
lσφ(1) . . .σφ(|x|)r = lxr.

Proposition 2 (Subsequences are subfactors under
M<). For all strings x,y ∈ Σ∗, x is a subsequence
of y iff M<(x)vM<(y).

Proof. We leave this proof to the Reader since it
is of similar nature to the previous one.

4 Grammars, Languages, and Language
Classes

Factors can define grammars, formal languages,
and classes of formal languages. Usually a model
signature provides the vocabulary for some logi-
cal language. Sentences in this logical language
define sets of strings as follows. The language of
a sentence φ is all and only those strings whose
models satisfy φ . Within the regular languages,
many well-known subregular classes can be char-
acterized logically in this way (McNaughton and
Papert, 1971; Rogers and Pullum, 2011; Rogers
et al., 2013; Thomas, 1997).

Intuitively, the grammars we are interested in
consist of a finite list of forbidden subfactors,
whose largest size is bounded by k. Strings in
the language of this grammar are those which do
not contain any forbidden subfactors. In this way
these grammars are like logical expressions which
are ”conjunctions of negative literals” (Rogers
et al., 2013) where the negative literals are played
by the the forbidden factors.

Each forbidden subfactor is a principal element
of a filter and the language is all strings whose
models are not in any of these filters. For each
k, there is a class of languages including all and
only those languages that can be defined in this
way. For example, the Strictly k-Local (SLk)
and Strictly k-Piecewise languages can be de-
fined in this way; they are languages which forbid
finitely many substrings or subsequences, respec-
tively (Garcia et al., 1990; Rogers et al., 2010).
Formally:

Definition 8. Let k be some positive integer, and
M a model of Σ∗ with signature Γ. A grammar
G is a subset of Subfactk(M,Σ∗). The language
of G is L(G) = {w ∈ Σ∗ | Subfactk(M,w)∩G =
/0}. The class of languages L (M,k) = {L | ∃G ⊆
Subfactk(M,Σ∗),L(G) = L}.

The elements of G are principal elements of fil-
ters, and are called forbidden subfactors.

As an example, let Σ = {a,b,c} and consider
G = {M�(aa),M�(bb),M�(c)}. L(G) includes
the strings (ab)+ and (ba)+ and no other strings,
because the substrings aa, bb, and c are all forbid-
den. This language belongs to L (M�,2).

Proposition 3. For each w ∈ L(G) and each g ∈
G, Subfact(M,w) has a zero intersection with
Supfact(g).

Proof. Suppose there exists A ∈ Subfactk(Σ∗)
such that A vM(w) and g v A. This implies that
g vM(w) and thus that Subfactk(M,w)∩G 6= /0
which contradicts Definition 8.

In other words, the principal ideal of M(w) is
disjoint from the principal filters of the elements
of G.

5 Grammatical Entailments

Given a grammar G, we call a subfactor s in
Subfact(Σ∗) ungrammatical if it belongs to a
principal filter of any element of G. Subfactors
that are not ungrammatical are called grammat-
ical. Lemma 14 ensures that grammaticality is
downward entailing, in the sense that if a model
of the word M(w) is not contained in the principal
filters of the elements of the grammar, then neither
are the subfactors of M(w). But it also ensures that
ungrammaticality is upward entailing: if a model
of the word M(w) belongs to the principal filters
of the elements of the grammar, then all of the su-
perfactors of M(w) in that filter are likewise con-
tained.

In this way, the ideals and filters within a a par-
ticular model noted above give rise to these en-
tailment properties of grammaticality with respect
to the hypothesis space. If the learner constructs
filters, then the grammar G will allow structures
such that language membership is downward en-
tailing with respect to the grammar G, and lan-
guage non-membership is upward entailing with
respect to the grammar G.



[]

[capital] [a] [b] . . .

[capital, a] [a][]

[capital, a][] [a][capital, a] [a][a]

. . . . . . . . .

XX

X

X

X

X

XX X

X X X

Figure 3: The Structure ideals(blue) and filters(red) for a capitalized letter model.

5.1 Example: Text Capitalization

As an example, consider capitalized letters as dis-
cussed above. In an unconventional word model,
each capital letter at some position x is rep-
resented as satisfying one of the relations R ∈
{a(x),b(x), . . . ,z(x)} as well as the unary rela-
tion capital(x). Thus the relation a(x) is true
of both lowercase a and uppercase A, but a(x)∧
capital(x) is only true of uppercase A. Note also
that in this model no position x of a structure can
satisfy both predicates a(x) and b(x). We return to
this point in §7.

Figure 2 showcases the relationship among
these structures under a model M. The struc-
ture for A, [capital,a], contains as subfactors
[capital], [a], [], and the empty structure (not
shown). The empty structure is a subfactor of
[], and [] in turn is a subfactor of [capital] and
[a]. The subfactor [a] contains the subfactor [],
the domain element with no relations, but has su-
perfactors [capital,a], which has one domain
element and two relations, and [a][], which has
two domain elements, and the first satisfying the
property a. Subfactors and superfactors are listed
above and below each other, respectively, with
lines between them. Members of one ideal are
noted with a blue checkmark, and members of a
filter are noted by a red asterisk.

Applying this to the example in Figure 3, if the
structure [capital,a] is grammatical, then all of
its subfactors, such as [capital] and [a], and
[] are grammatical. Since those are grammatical,
each of their subfactors is also grammatical, which
in this case is just [ /0], shown in blue in Figure 3.
Conversely, if the structure [a][] is known to be

ungrammatical, then any structure which has it as
a subfactor is also ungrammatical (in this exam-
ple, [capital,a][], shown in Red in Figure 3.
To see the importance, consider a string with only
lowercase letters. In a connected model, the gram-
mar would ban 26 forbidden factors (A,B,C,...),
but the “capital” model bans just one, [capital].

5.2 Example: Long Distance Linguistic
Dependencies

As another example, sequences of speech sounds
as mentioned earlier may be decomposed into
binary features based on their phonetic proper-
ties like anteriority (±ant — whether it oc-
curs in the anterior of the vocal tract), stri-
dency (±str — whether it produces a high-
intensity fricative noise), or voicing (±voi —
whether it activates the vocal chords), among oth-
ers (Hayes, 2009). Each sound at some posi-
tion x is represented as satisfying relations R ∈
{±voi(x),±str(x), . . . ,±ant(x)}. Thus the re-
lation +str(x) is true of both the sound s as in
the first sound of “sue” and S, as in “shoe”, but
+str(x)∧−ant(x) is only true of S.

Note also that in this model no position x of a
structure can satisfy both predicates +str(x) and
−str(x). We return to this point in §7 below. We
again use square brackets to delimit the domain el-
ements and write the unary features within them,
so a model representation like

[
+str
+ant

][
+str
-ant

]
has the following visual representation:

+str
+ant

+str
-ant

<



[]

. . . . . . . . .

. . .

[
+str
+ant

]
[+str]

. . .

[
+str
+ant

][
+str
+ant

] [
+str
+ant

][
+str
-ant

]
[

+voi
+str
+ant

][
+str
+ant

] [
+str
+ant

][+voi
+str
-ant

] [+voi
+str
+ant

][
+str
-ant
]

. . . . . . . . .

. . .

X X

X

X

X

X

X

X X

X X

X

Figure 4: Structure ideals(blue) and filters(red) for a phonological precedence model.

To ease the exposition, we will use square
brackets to delimit the domain elements and write
the unary relations within them instead of specify-
ing the model in mathematical detail. In an uncon-
ventional subsequence word model, then, one pos-
sible structure of the subsequence s...S is writ-
ten
[
+str
+ant

][
+str
-ant

]
.

In many languages, the presence of certain seg-
ments is dependent on the presence of another
segment. In Samala, subsequences like s...s
are allowed but s...S are not, so words like
hasxintilawas are allowed but words like
hasxintilawaS are not (Hansson, 2010). In
an unconventional model, banning structures of
the form [+str][+str] is insufficient, since
all these segments share that stridency property,
while a structure like

[
+str
+ant

][
+str
-ant

]
will dis-

tinguish them, since they disallow stridents which
disagree on the ±ant(x) relations. The struc-
ture [+ant][-ant] however, is insufficient,
since consonants like p,b,m have that feature,
and would incorrectly ban acceptable strings. To
see the importance, a conventional string model
must ban multiple sibilant factors sS,zS,sZ,zZ, while
an unconventional model must just ban one,[
+str
+ant

][
+str
-ant

]
Figure 4 showcases the relationship among

these structures under a precedence model M<.
The structure for

[
+str
+ant

]
[+str] contains as sub-

factors (among others) [+str] [+str], [+str] [],
[], and the empty structure (not shown). The
empty structure is a subfactor of [], and [] in turn
is a subfactor of [+ant] and [-str], and so on.

If the structure
[
+str
+ant

][
+str
+ant

]
is grammatical,

then all of its subfactors, are grammatical, and
so are their subfactors, in turn. Conversely, if
the structure

[
+str
+ant

][
+str
-ant

]
is known to be un-

grammatical, then any structure which has it as
a subfactor is also ungrammatical (for example,[
+voi
+str
+ant

][
+str
-ant

]
, where the first segment is also

voiced +voi), shown in Red in Figure 4.

The structure filters give the learner an advan-
tage when confronting hypothesis spaces under
a particular model. In particular, it allows the
learner to prune vast swathes of the hypothesis
space as it reaches for principal elements of fea-
tures. If a learner identifies one structure as being
grammatical, the learner may infer that all of its
subfactors are also grammatical and not have to
consider them. Alternatively, if the learner knows
a structure is ungrammatical, it may infer that the
ideals above it are also ungrammatical.

Generally, these reductions can be exponential:
an alphabet of size 2n can be represented with
n unary relations in the model signature. How-
ever, this exponential reduction does not neces-
sarily make learning any easier. The reason for
this is that the size of Subfactk(M,Σ∗) equals
∑ki=1(2n)i where n is the number of unary re-
lations. Since a grammar is defined as a sub-
set of Subfactk(M,Σ∗), the number of consid-
ered grammars is thus very large. Therefore, the
problem of how to search this space effectively is
paramount.



6 The Learning Problem

For some M,k, is L (M,k) learnable from posi-
tive data? The short answer is Yes (Heinz, 2010;
Heinz et al., 2012). The solution presented in these
papers can be thought of as using the function
Subfactk(M,w) to identify permissible k-factors
in words w in the positive data. The k-factors that
are not permissible are forbidden. With sufficient
positive data, such a learning algorithm will con-
verge to a grammar that generates any target lan-
guage in the class. While this solution is sound in
theory, when the space of k-factors is very large, it
is not practical. Here, we make clear the problem
the learning algorithm solves.

We state the learning problem not in terms of
converging to a correct grammar in the limit as
previously studied, but instead of returning an ‘ad-
equate’ grammar given a finite positive sample.
Determining what counts as an adequate grammar
is what (De Raedt, 2008) calls a Quality Criterion.
Definition 9 (The Learning Problem). Fix Σ,
model M, and positive integer k. For any language
L ∈ L (M,k) and for any finite D ⊆ L, return a
grammar G such that

1. G is consistent, that is, it covers the data: D⊆
L(G);

2. L(G) is a smallest language in L which cov-
ers the data: so for all L ∈L where D ⊆ L,
we have L(G)⊆ L; and

3. G includes structures S that are restrictions of
structures S′ included in other grammars G′

that also satisfy (1) and (2): for all G′ satisfy-
ing the first two criteria for all S′ ∈ G′, there
exists S ∈ G such that Sv S′.

The first criterion is self-explanatory. The sec-
ond criterion is motivated by Angluin’s (1980)
analysis of identification in the limit. The third cri-
terion requires that the grammar contain the most
“general” subfactors. An example will help illus-
trate this criterion.

Consider again the grammar G =
{M�(aa),M�(bb),M�(c)} with Σ = {a,b,c}.
L(G) is the same as L(H) where H =
{M�(aa),M�(bb),M�(ac),M�(bc),M�(cc),
M�(ca),M�(cb)}. In H all the forbidden subfac-
tors are of size 2, whereas G encapsulates all of
the 2-factors in H which include c with a single
1-factor M�(c). Both grammars G and H may
satisfy criteria (1) and (2) but H would not satisfy
criterion (3) because of G.

7 A Bottom-Up Learning Algorithm

(De Raedt, 2008) identifies two directions of in-
ference: specific-to-general (i.e., ‘top-down’) and
general-to-specific (i.e., ‘bottom-up’). Since we
are trying to find the most general subfactors, top-
down inference has the potential to consider expo-
nentially many more subfactors than bottom-up in-
ference. It makes mores sense to traverse bottom-
up, that is, from the most general subfactors possi-
ble to the most specific. Additionally, once a sub-
factor is identified as an element of the grammar,
none of its superfactors (elements of its principal
filter) need to be considered further.

A bottom-up learner is shown in Algorithm 1.
Its input is a positive data sample D and an integer
k that identifies the upper bound on the size of the
subfactors.

Data: positive sample D, empty structure s0,
max constraint size k
Result: G, a set of constraints
Q←{s0};
G← /0;
V ← /0;
while Q 6= /0 do

s← Q.dequeue();
V ←V ∪{s};
if ∃x ∈ D such that sv x then

S← NextSupFact(s);
S′←{s ∈ S | |s|≤ k∧ (¬∃g ∈ G)[gv s]∧ s 6∈

V};
Q.enqueue(S′);

end
else

G← G∪{s};
end

end
return G;

Algorithm 1: Bottom-up learning algorithm for
lattice-structured constraints

Figure 5: Pruning the hypothesis space

The algorithm makes use of a queue Q, which
is initialized to contain just the empty structure s0.
It also initializes two empty sets: G, the grammar
that will ultimately be returned, and V , the set of
‘visited subfactors’. The subfactors in Q are con-



sidered one at a time, in order, and as each subfac-
tor s is considered it is added to V . If s is not a
subfactor of the model of any word in the positive
sample D (i.e., not contained by any data point in
D), then it is added to the grammar G.

If s is a subfactor of the sample, it is sent
to the function NextSupFact, which returns a
set of least superfactors for s. For concreteness,
NextSupFact(s) may be defined formally as fol-
lows:
NextSupFact(s) = {S ∈ Subfactk(Σ∗) | s v

S,¬∃S′[sv S′ v S]}.
Practically NextSupFact will be defined con-

structively so that each subfactor in Subfactk(Σ∗)
is constructed only once as needed. Thus, not
only will it not be needed to store the whole set
Subfactk(Σ∗) in memory, but the set V may be
excluded from the algorithm as well.

This set of superfactors is then filtered by the
following criteria: they must be smaller than k+1,
they must contain no element of G as a subfactor,
and they must not have been previously considered
(i.e., they cannot be in V ). Those structures that
survive this filter are added to Q. This procedure
continues until there are no more structures left to
consider in Q.

Theorem 1. For any L ∈L (M,k), and any finite
set P ⊆ L provided as input to Algorithm 1, it re-
turns a grammar G satisfying Definition 9.

Proof. Consider any x ∈ D. Algorithm 1 only
adds elements to G that are not subfactors of x, so
x 6∈ Supfact(G). Thus x ∈ L(G) and D ⊆ L(G),
satisfying Condition (1).

Consider any L′ ∈ L with D ⊆ L′. To show
L = L(G) ⊆ L′, consider any w ∈ L. Then
Subfact(w) ⊆ Subfact(D) and Subfact(D) ⊆
Subfact(L′) since D ⊆ L. Then Subfact(w) ⊆
Subfact(L′). Hence, w ∈ L′, and so L⊆ L′, satis-
fying Condition (2).

For condition (3), we use the fact that elements
in the grammar G were in Q at some point. Sup-
pose s,s′ are subfactors such that s∈G, s′ v s, and
(¬∃x ∈ D)[s′ vM(x)]. Since s ∈ G, then at some
point s ∈ Q.

If s′ v s then s′ will be added to Q before s
is generated by NextSupFact. Because Q is a
queue, s′ will also be removed from Q before s
is generated by NextSupFact. Since s′ is not con-
tained by any M(x) with x ∈ D, it will be added to
G. When s is generated by NextSupFact, it will

not pass the filter because it fails the second crite-
rion since s′ v s and s′ ∈ G. Then s is never added
to Q, and therefore s /∈ G, contra our original as-
sumption. Thus Condition (3) is satisfied.

One aspect of the algorithm to highlight is that
when a subfactor g is added to G, it is not added to
Q. Consequently, NextSupFact(g) is never added
to Q. In this way, finding elements of G prunes
the remainder of the space to be searched (see fig-
ure 5). In general, it is not the case that every el-
ement in the principal filter of g will not be gen-
erated by NextSupFact since some of these ele-
ments may belong to NextSupFact(x) for other
subfactors x on the Q. We expect subfactors on
the ‘border’ of Supfact(g) to be generated in this
way (and then they are filtered out). This pruning,
especially when the subfactors are quite general,
can significantly reduce the remaining space to be
traversed.

In regard to efficiency, in the worst case, the el-
ements of G are all very specific subfactors and
are greatest elements of Subfactk(Σ∗). In this
case, every subfactor Subfactk(Σ∗) will be added
to Q and the time complexity is thus exponen-
tial. However, we are primarily interested in the
case when Subfactk(D) are a small proportion
of Subfactk(Σ∗). This constitutes an example
of data sparsity. In this case, we believe the ele-
ments of the target grammar will be much ‘lower’
in the partial order and thus will be found much
more quickly. Determining what conditions on
Subfactk(D) and Subfactk(Σ∗) result in a poly-
nomial time run in the size of D is a focus of cur-
rent research activity.

Another area of active research is developing
a recipe for the NextSupFact function for mod-
els with a successor or precedence order relation
and arbitary unary relations. The basic idea un-
derlying the bottom-up algorithm is to develop a
spanning tree for the poset Subfact(Σ∗) and to
traverse this tree in a breadth-first manner. The
function NextSupFact helps control this search.
Ideally, NextSupFact would only generate each
subfactor once, which obviates the need to store
visited subfactors in V . This can be accomplished
to some extent in different ways. For incompatible
unary relations, like a and b in our capitalization
example, NextSupFact can be defined to prevent
adding property a to a position that already satis-
fies property b.

For compatible unary relations, like a and



capital in our capitalization example, an or-
dering over the unary relations such as a < b <
capital can help eliminate generating the same
subfactor in different ways. For example, if
NextSupFact is defined to only add ‘lesser’ unary
relations to positions that already have them then
it would only output [capital,a] given the sub-
factor [a] as input. On the other hand, when given
as input the subfactor [capital], it could not add
any unary relation to this position.

8 Conclusion

In this paper, we considered the problem of learn-
ing formal languages defined as the complement
of the union of finitely many principal filters,
whose principal elements make up the grammar.
This is one way to characterize the Strictly k-Local
and Strictly k-Piecewise languages, but the gen-
eralization here lets us consider enriched repre-
sentations of strings where different elements in a
string can be said to share properties. it also lets us
learn the shortest forbidden substrings in SLk (Ron
et al., 1996) This is useful in many applications
where domain-specific knowledge is available and
should be taken advantage of. Such enriched rep-
resentations, however, have a drawback. The num-
ber of subfactors is large which makes identifying
the principal elements of the filters difficult. This
paper showed that the partial ordering of the sub-
factors motivates a bottom-up learning algorithm
which finds the least subfactors whose filters do
not include the positive data.

Acknowledgments

We would like to thank James Rogers for very
helpful discussion on the notion of subfac-
tor. This work was supported by NIH grant
#R01HD87133-01 to JH.

References
Dana Angluin. 1980. Inductive inference of formal

languages from positive data. Information Control,
45:117–135.

J. Richard Büchi. 1960. Weak second-order arithmetic
and finite automata. Mathematical Logic Quarterly,
6(1-6):66–92.

Luc De Raedt. 2008. Logical and Relational Learning.
Springer-Verlag Berlin Heidelberg.

Herbert B. Enderton. 2001. A Mathematical Introduc-
tion to Logic, 2nd edition. Academic Press.

Pedro Garcia, Enrique Vidal, and José Oncina. 1990.
Learning locally testable languages in the strict
sense. In Proceedings of the Workshop on Algorith-
mic Learning Theory, pages 325–338.

Saul Gorn. 1967. Explicit definitions and linguistic
dominoes. In Systems and Computer Science, pages
77–115, Toronto. University of Toronto Press.

Gunnar Hansson. 2010. Consonant Harmony: Long-
Distance Interaction in Phonology. Number 145 in
University of California Publications in Linguistics.
University of California Press, Berkeley, CA. Avail-
able on-line (free) at eScholarship.org.

Bruce Hayes. 2009. Introductory Phonology. Wiley-
Blackwell.

Jeffrey Heinz. 2010. String extension learning. In Pro-
ceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 897–
906, Uppsala, Sweden. Association for Computa-
tional Linguistics.

Jeffrey Heinz, Anna Kasprzik, and Timo Kötzing.
2012. Learning with lattice-structured hypothesis
spaces. Theoretical Computer Science, 457:111–
127.

Colin de la Higuera. 2010. Grammatical Inference:
Learning Automata and Grammars. Cambridge
University Press.

Leonid Libkin. 2004. Elements of Finite Model The-
ory. Springer.

Robert McNaughton and Seymour Papert. 1971.
Counter-Free Automata. MIT Press.

James Rogers. 2003. Syntactic structures as multi-
dimensional trees. Research on Language and Com-
putation, 1(3-4):265–305.

James Rogers, Jeffrey Heinz, Gil Bailey, Matt Edlef-
sen, Molly Visscher, David Wellcome, and Sean
Wibel. 2010. On languages piecewise testable in the
strict sense. In The Mathematics of Language, vol-
ume 6149 of Lecture Notes in Artifical Intelligence,
pages 255–265. Springer.

James Rogers, Jeffrey Heinz, Margaret Fero, Jeremy
Hurst, Dakotah Lambert, and Sean Wibel. 2013.
Cognitive and sub-regular complexity. In Formal
Grammar, volume 8036 of Lecture Notes in Com-
puter Science, pages 90–108. Springer.

James Rogers and Geoffrey K. Pullum. 2011. Aural
pattern recognition experiments and the subregular
hierarchy. Journal of Logic, Language and Infor-
mation, 20:329–342.

Dana Ron, Yoram Singer, and Naftali Tishby. 1996.
The power of amnesia: Learning probabilistic au-
tomata with variable memory length. Machine
Learning, 25(2-3):117–149.



Kristina Strother-Garcia, Jeffrey Heinz, and Hyun Jin
Hwangbo. 2016. Using model theory for grammati-
cal inference: a case study from phonology. In Pro-
ceedings of The 13th International Conference on
Grammatical Inference, volume 57 of JMLR: Work-
shop and Conference Proceedings, pages 66–78.

Wolfgang Thomas. 1997. Languages, automata, and
logic. In Handbook of Formal Languages, volume 3,
chapter 7. Springer.

Mai Ha Vu, Ashkan Zehfroosh, Kristina Strother-
Garcia1, Michael Sebok, Jeffrey Heinz, and Her-
bert G. Tanner. 2018. Statistical relational learn-
ing with unconventional string models. Frontiers in
Robotics and AI.

https://doi.org/doi:10.3389/frobt.2018.00076
https://doi.org/doi:10.3389/frobt.2018.00076

