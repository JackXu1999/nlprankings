



















































Agree or Disagree: Predicting Judgments on Nuanced Assertions


Proceedings of the 7th Joint Conference on Lexical and Computational Semantics (*SEM), pages 214–224
New Orleans, June 5-6, 2018. c©2018 Association for Computational Linguistics

Agree or Disagree: Predicting Judgments on Nuanced Assertions

Michael Wojatzki Torsten Zesch
Language Technology Lab

University of Duisburg-Essen, Germany
michael.wojatzki@uni-due.de

torsten.zeschi@uni-due.de

Saif M. Mohammad Svetlana Kiritchenko
National Research Council Canada

Ottawa, Canada
saif.mohammad@nrc-cnrc.gc.ca

svetlana.kiritchenko@nrc-cnrc.gc.ca

Abstract

Being able to predict whether people agree or
disagree with an assertion (i.e. an explicit, self-
contained statement) has several applications
ranging from predicting how many people will
like or dislike a social media post to classi-
fying posts based on whether they are in ac-
cordance with a particular point of view. We
formalize this as two NLP tasks: predicting
judgments of (i) individuals and (ii) groups
based on the text of the assertion and previ-
ous judgments. We evaluate a wide range of
approaches on a crowdsourced data set con-
taining over 100,000 judgments on over 2,000
assertions. We find that predicting individual
judgments is a hard task with our best results
only slightly exceeding a majority baseline.
Judgments of groups, however, can be more
reliably predicted using a Siamese neural net-
work, which outperforms all other approaches
by a wide margin.

1 Introduction

One of the most basic reactions when reading a
sentence is to agree or disagree with it.1 Mech-
anisms that allow us to express agreement (e.g.
thumb-up, like, up-vote, ♥) or disagreement (e.g.
thumb-down, dislike, down-vote) towards posts
of other users can be found in almost all social
networking sites. The judgments associated with
posts that discuss controversial political or social
issues, such as legalization of drug, immigration
policy, or gun rights, are a rich source of infor-
mation for those interested in the opinions of in-
dividuals or groups. For instance, public opinion
regarding an issue is often illustrated by the num-
ber of retweets, likes, or upvotes that a politician
or influential person receives.

1You are probably thinking about whether you agree with
that statement right now.

Hence, especially for controversial issues, be-
ing able to predict how people judge posts has sev-
eral applications: people at large could automati-
cally anticipate if politicians, companies or other
decision makers would agree or disagree with a
new perspective on a problem or how they would
evaluate a new possible solution. The method can
also be used by journalists to more accurately ana-
lyze the homogeneity of opinions or to detect filter
bubbles in social media. Decision makers them-
selves would be able to evaluate in advance how
citizens, customers, or employees react to a press
announcement, a new regulation, or tweet. Social
media users could be enabled to search, sort or fil-
ter posts based on whether they are in accordance
with or contrary to their personal world view. Such
predictions could also be used to augment chat ap-
plications by indicating to a user if her recipients
will agree or disagree with a message to be sent,
enabling to choose a more or less confrontational
discussion style.

In this paper, we describe how the outlined use
cases can be framed as two inference tasks: pre-
dicting individual judgments and predicting judg-
ments of whole groups. As a first step, we restrict
ourselves to judgments on textual utterances that
are explicit, relevant, and that do not contain mul-
tiple positions. We will refer to such utterances as
assertions. For solving the tasks, we define the de-
gree to which two assertions are judged similar as
judgment similarity. This similarity allows us to
predict a judgment based on other judgments that
have been made on similar, known assertions.

Across both tasks, we compare this strategy
against several baselines and reference approaches
on a newly crowdsourced data set containing over
100 000 judgments on assertions. We find that,
for predicting individual judgments, our best re-
sults only slightly exceed a majority baseline, but
that judgments of groups can be more reliably pre-

214



Figure 1: Overview on the two prediction tasks.

dicted using a Siamese neural network, which out-
performs all other approaches by a wide margin.

2 Predicting Judgments

In order to predict if someone will agree with
an assertion, we need knowledge about that per-
son. Ideally, we would have access to a large set
of other assertions which the person has already
judged. We could then measure the similarity be-
tween previous assertions and the new assertion
and hypothesize that the judgment on the new as-
sertion should be the same as for a highly similar
one. In Figure 1, we show this case for binary
(yes/no) predictions on individuals and argue that
this can be also generalized to probabilistic pre-
dictions on groups of people. Thus, we formulate
two prediction tasks:

In the first task, we want to predict judgments
of individuals on assertions based on other judg-
ments by the same person. Thus, the first task is
formulated as follows: given a set of assertions
a1, ..., an relevant to an issue and the judgments
of a person pi on a1, ..., an−1 an automatic system
has to predict pi’s judgment on the assertion an.

In the second task, we want to predict judg-
ments of groups on assertions based on averaged
judgments of other assertions. Hence, this task
can be formalized as follows: given a set of judg-
ments of a group of persons p1, ..., pk on the asser-
tions a1, ..., an−1, an automatic systems must pre-
dict the judgment on the assertion an for the same
group of persons. Judgments of groups can be
expressed by an aggregated agreement score be-
tween -1 and 1, where -1 means that every person
disagrees to an assertion and 1 that every person
agrees to the assertion.

For measuring the similarity between two asser-
tions, we propose to compare how a large group
of people judges them. We define the degree to
which two assertions are judged similarly by a
large group as the judgment similarity of the two
assertions. However, judgments of other persons

are not easily available – e.g. if we want to predict
a judgment on a new, unseen assertion. To over-
come this limitation, we propose to use methods
that consider the texts of the assertions to mimic
judgment similarity and have thus the ability to
generalize from existing data collections.

3 Related Work

Measuring the judgment similarity of two asser-
tions is related to several NLP tasks such as the
detection of semantic text similarity (STS) (Agirre
et al., 2012), paraphrase recognition (Bhagat and
Hovy, 2013), and textual entailment (Dagan et al.,
2009).

Unlike semantic text similarity, we do not use
a notation of similarity based on the intuition of
humans, but one that derives from the context of
judgments. Hence, we define that the judgment
similarity of two assertions is 1 if two assertions
are consistently judged the same and are thus in-
terchangeable in the context of our task.

There are several reasons why assertions are
judged similarly: their text may convey similar se-
mantics such as in the assertions ‘Marijuana alle-
viates the suffering of chronically ill patients’ and
‘Marijuana helps chronically ill persons’. This
type of similarity corresponds to what methods
of semantic text similarity capture. However, a
strong judgment similarity of two assertions can
also be due to semantically entailed relationships
between assertions. For instance, if people agree
with ‘Marijuana is a gateway drug for teenagers
and damages growing brains’ most of them also
agree to ‘Marijuana is dangerous for minors’, de-
spite the texts being different in content and hav-
ing thus low semantic text similarity. In addition,
two assertions can also have a strong judgment
similarity because of underlying socio-cultural,
political, or personal factors. For instance, the as-
sertions ‘Consuming Marijuana has no impact on
your success at work’ and ‘Marijuana is not ad-
dictive’ describe different arguments for legalizing
marijuana, but judgments made on these assertions
are often correlated.

Our work also relates to other attempts on pre-
dicting reactions to text, such as predicting the
number of retweets (Suh et al., 2010; Petrovic
et al., 2011), the number of likes on tweets (Tan
et al., 2014), the number of karma points of red-
dit posts (Wei et al., 2016), or sales from prod-
uct descriptions (Pryzant et al., 2017). What those

215



works have in common is that they measure some
kind of popularity, which differs significantly from
our task: even if one agrees with a text, one might
decide not to retweet or like it for any number of
reasons. There are also cases in which one may
retweet a post with which one disagrees in order
to flag someone or something from the opposing
community. Furthermore, there are effects such
as the author’s followers affecting the visibility
of posts and thereby the likelihood of a like or a
retweet (Suh et al., 2010).

In addition, we relate to works that aim at pre-
dicting whether two texts (Menini and Tonelli,
2016) or sequences of utterances (Wang and
Cardie, 2014; Celli et al., 2016) express agreement
or disagreement with each other. More broadly,
we also relate to works that analyze stance (Mo-
hammad et al., 2016; Xu et al., 2016; Taulé et al.,
2017), sentiment (Pang and Lee, 2008; Liu, 2012;
Mohammad, 2016), or arguments (Habernal and
Gurevych, 2016; Boltuzic and Šnajder, 2016; Bar-
Haim et al., 2017) that are expressed via text. In
contrast to these works, we do not examine what
judgment, sentiment, or claim is expressed by a
text, but whether we can infer agreement or dis-
agreement based on judgments which were made
on other assertions.

Finally, we relate to work on analyzing and pre-
dicting outcomes of congressional roll-call voting.
These works constantly find that votes of politi-
cians can be explained by a low number of under-
lying, ideological dimensions such as being left
or right (Heckman and Snyder, 1996; Poole and
Rosenthal, 1997, 2001). Our work is different
from these attempts, as we do not consider politi-
cians who might have incentives to vote in accor-
dance with the ideological views of their party, and
as we base our prediction on the text of assertions.

4 Data Collection

For exploring how well the two tasks can be solved
automatically, we use the dataset Nuanced Asser-
tions on Controversial Issues (NAoCI) created by
Wojatzki et al. (2018). The dataset contains as-
sertions judged on a wide range of controversial
issues.2 The NAoCI dataset mimics a common
situation in many social media sites, where peo-
ple e.g. up- or downvote social media posts. How-
ever, it does not have the experimental problems

2The dataset is accessible from https://sites.
google.com/view/you-on-issues/

of using social media data directly. These prob-
lems include legal reasons of scraping social me-
dia data and moderator variables such as the defi-
nition of issues, the influence of previous posts, or
the question of whether someone is not judging an
assertion because she does not want to judge it or
because she did not perceive it.

The data was collected using crowdsourcing
conducted on crowdflower.com in two steps.
First, participants were asked to generate a large
set of assertions relevant to controversial issues.
The set of assertions was created using crowd-
sourcing, as a manual creation of assertions would
be potentially incomplete and subject to personal
bias. We provided instructions to make sure that
the assertions are natural, self-contained state-
ments about an issue. Next, a large number of
people was asked to indicate whether they agree
or disagree with these assertions.

The process was reviewed and approved by the
institutional ethics board of the National Research
Council Canada.

Generating Assertions In order to obtain real-
istic assertions, 69 participants were asked to gen-
erate assertions for sixteen predefined issues (see
Table 1). For each issue, the subjects were given
definition of the issue and a few example asser-
tions. In addition, the instructions state that as-
sertions should be explicit, relevant to an issue,
self-contained, and only contain a single position.
Specifically, the use of co-reference or hedging in-
dicated by words such as perhaps, maybe, or pos-
sibly was not permitted. After a removal of dupli-
cates and instances that did not follow the rules,
this process resulted in about 150 unique asser-
tions per issue (2,243 in total).

Judging Assertions Next, 230 subjects were
asked to indicate whether they agree or disagree
with an assertion, resulting in over 100 000 judg-
ments (see Table 1). The participants were free
to judge as many assertions on as many issues as
they wanted. On average each assertion is judged
by about 45 persons and each participant judged
over 400 assertions. For each person, agreement is
encoded with 1, disagreement with −1, and miss-
ing values with 0 (as not all subjects judged all
assertions). Additionally, we can also compute
the aggregated agreement score for each assertion
by simply subtracting the percentage of partici-
pants that disagreed with the assertion from the

216



# of # of
Issue Assertions Judgments

Black Lives Matter 135 6 154
Climate Change 142 6 473
Creationism in School 129 5 747
Foreign Aid 150 6 866
Gender Equality 130 5 969
Gun Rights 145 6 423
Marijuana 138 6 200
Mandatory Vaccination 134 5 962
Media Bias 133 5 877
Obama Care 154 6 940
Same-sex Marriage 148 6 899
US Electoral System 175 7 695
US in the Middle East 138 6 280
US Immigration 130 5 950
Vegetarian & Vegan Lifestyle 128 5 806
War on Terrorism 134 5 892

Total 2 243 101 133

Table 1: Issues and number of crowdsourced assertions
and judgments.

Aggregated Agreement Score

N
um

be
r 

of
 A

ss
er

tio
ns

−1.0 −0.5 0.0 0.5 1.0

0
50

10
0

15
0

20
0

Figure 2: Distribution of aggregated agreement scores.

percentage of participants that agreed with the as-
sertion. Figure 2 shows the distribution of ag-
gregated agreement scores (grouped into bins of
size .05) across all issues. The mass of the dis-
tribution is concentrated in the positive range of
possible values, which indicates that the partic-
ipants more often agree with the assertions than
they disagree. Consequently, baselines accounting
for this imbalance perform strongly in predicting
judgments on assertions. However, the distribu-
tion corresponds to what we observe in many so-
cial network sites, where e.g. the ratio of likes to
dislikes is also clearly skewed towards likes.

All data, the used questionnaires along with the
directions and examples are publicly available on
the project website.2

5 Measuring Judgment Similarity
Between Assertions

As mentioned above, we want to predict judg-
ments on a previously unseen assertion based on
judgments of similar assertions. For that purpose,
we need to measure the similarity of assertions
sim(a1, a2) based on their text only. For measur-
ing the similarity of two assertions we rely on the
judgment matrix J , with jp,a as the judgment pro-
vided by participant p for assertion a, with ~jp as
the row vector of all ratings of participant p, and
~ja as the column vector of all ratings provided for
assertion a. We measure the gold similarity of two
assertions by comparing their judgment vectors in
the matrix. If the vectors are orthogonal, the asser-
tions are maximally dissimilar (i.e. persons who
agree to assertion a1 disagree with a2). If the vec-
tors are parallel, the assertions have a perfect sim-
ilarity. We compute the cosine similarity between
the judgment vectors of two assertions. We cal-
culate the gold similarity between all unique pairs
(e.g. we do not use both a1 with a2 and a2 with
a1) in our data and do not consider self-pairing.

5.1 Experimental Setup
As baselines for this task, we utilize well-
established semantic text similarity (STS) meth-
ods that calculate overlap between the surface
forms of assertions. We use the following methods
as implemented by DKPro Similarity (Bär et al.,
2013)3: (i) unigram overlap expressed by the Jac-
card coefficient (Lyon et al., 2001), (ii) greedy
string tiling (Wise, 1996), (iii) longest common
sub string (Gusfield, 1997). Additionally, we use
averaged word embeddings (Bojanowski et al.,
2017).

Beyond the baselines, we apply two machine
learning approaches: a conventional SVM-based
classifier and a neural network. The SVM clas-
sifier is implemented using LibSVM (Chang and
Lin, 2011) as provided by DKProTC (Daxen-
berger et al., 2014).4 We use a combination of var-
ious ngram features, sentiment features (derived
from the system by Kiritchenko et al. (2014)5),
embedding features (averaged embeddings by Bo-
janowski et al. (2017)) and negation features. We
used a linear kernel with C=100 and the nu-SVR

3version 2.2.0
4version 1.0
5The NRC-Canada system ranked first in the SemEval

2013 (Nakov et al., 2013) and 2014 (Rosenthal et al., 2014)
tasks on sentiment analysis.

217



regression model. Iterative experiments showed
that this configuration gave the most stable results
across the issues. For the neural approach, we
adapt Siamese neural networks (SNN), which con-
sist of two identical branches or sub-networks that
try to extract useful representations of the asser-
tions and a final layer that merges these branches.
SNNs have been successfully used to predict text
similarity (Mueller and Thyagarajan, 2016; Necu-
loiu et al., 2016) and match pairs of sentences (e.g.
a tweet to reply) (Hu et al., 2014). In our SNN, a
branch consists of a layer that translates the asser-
tions into sequences of word embeddings, which
is followed by a convolution layer with a filter
size of two, max pooling over time layer, and a
dense layer. To merge the branches, we calculate
the cosine similarity of the extracted vector repre-
sentations. The SNN was implemented using the
deep learning framework deepTC (Horsmann and
Zesch, 2018) in conjunction with Keras6 and Ten-
sorflow (Abadi et al., 2016). In order to ensure full
reproducibility of our results, the source code for
both approaches is publicly available.7 We eval-
uate all approaches using 10-fold cross validation
and calculate Pearson correlation between the pre-
diction and the gold similarity.

5.2 Results

Table 2 shows the correlation of all approaches av-
eraged over all sixteen issues.8 Overall, the STS
baselines result in very low correlation coefficients
between .02 and .07, while the trained models ob-
tain coefficients around .6. This shows that the
systems can learn useful representations that cap-
ture judgment similarity and that this representa-
tion is indeed different from semantic similarity.
Since both models are purely lexical and still yield
reliable performance, we suspect that the relation-
ship between a pair of assertions and their judg-
ment similarity also has a lexical nature.

While STS baselines obtain consistently low re-
sults, we observe largely differing results per is-
sues (ranging from .32 to .72) with SVM and SNN
behaving alike. Detailed results for each issue are
listed in Table 3.

In order to better understand the results, we ex-

6
https://keras.io/

7
https://github.com/muchafel/judgmentPrediction

8As Pearsons r is defined in a probabilistic space it can-
not be averaged directly. Therefore, we first z-transform the
scores, average them and then transform them back into the
original range of values.

Method r

SNN .61
SVM .58
Embedding distance .07
Jaccard .07
Greedy string tiling .06
Longest common sub string .05

Table 2: Pearson correlation (averaged over all issues)
of text-based approaches for approximating similarity
of assertion judgments.5

Issue SVM SNN

Climate Change .70 .72
Gender Equality .67 .73
Mandatory Vaccination .68 .74
Obama Care .66 .70
Black Lives Matter .66 .74
Media Bias .63 .63
US Electoral System .63 .59
Same-sex Marriage .59 .61
War on Terrorism .56 .59
Foreign Aid .54 .46
US in the Middle East .52 .55
US Immigration .52 .57
Gun Rights .51 .64
Creationism in school .48 .51
Vegetarian and Vegan Lifestyle .43 .40
Legalization of Marijuana .37 .32

Table 3: Correlation coefficients of the similarity pre-
diction by the SVM and the SNN, obtained in 10 fold
cross-validation.

amine the scatter-plots that visualize assignment
of gold to prediction (x–Axis: gold, y–Axis: pre-
diction) and investigate cases that deviate strongly
from an ideal correlation. Figure 3 shows the scat-
ter plot for the issue Climate Change for both clas-
sifiers. For the SVM we observe that there is a
group of pairs that is predicted inversely propor-
tional, i.e. their gold value is positive, but the re-
gression assigns a clearly negative value. We ob-
serve that these instances mainly correspond to
pairs in which both assertions have high negative
word scores. For instance the pair, ‘There is not
a real contribution of human activities in Climate
change’ and ‘Climate change was made up by the
government to keep people in fear’, have a com-
parable high similarity of .20. The SVM, how-
ever, assigns them a similarity score of −.38. We
suspect that this effect results from the distribu-
tion of similarity scores that is skewed to the posi-
tive range of possible scores. Therefore, the SVM
probably assigns too much weight to ngrams that
signal a negative score. Far less pronounced, for
the neural approach, we find instances whose gold

218



values are negative, but which are assigned a pos-
itive value. When inspecting these pairs we find
that many of them contain one assertion which
uses a negation (e.g. not, unsure, or unlikely). An
example for this is the pair, ‘There has been an in-
crease in tropical storms of greater intensity which
can be attributed to climate change’ and ‘Different
changes in weather does not mean global warm-
ing’, that have a low similarity in the gold data
(−0.19), but get assigned a rather high similarity
score (.20).

6 Predicting Judgments of Individuals

Now that we have means for quite reliably esti-
mating the judgment similarity of assertions, we
can try to predict judgments on individual asser-
tions. We compare the judgment similarity meth-
ods against several baselines and collaborative fil-
tering methods (that make use of judgments that
are made by other persons to calculate person and
assertion similarity).

Baselines (BL) The random baseline predicts
agree or disagree for each assertion. We also
define the all agree baseline, which always pre-
dicts agree. As the data contains substantially
more agree judgments than disagree judgments
(c.f. Figure 2), this is a strong baseline. As a
third baseline, we average all known judgments of
a person and predict agree if this value is positive
and predict disagree otherwise. We refer to this
baseline as tendency.

Judgment Similarity (JS) We use the above de-
fined judgment similarity methods to calculate the
similarity between each of the assertions previ-
ously judged by that person and the assertion for
which we want to make the prediction. Then we
simply transfer the judgment of the most similar
assertion to the assertion of interest.9 To prevent
leakage, the scores of the prediction are taken from
the models that have been trained in the cross val-
idation. This means, for predicting the score of
a pair of assertions we use the model which does
not include the pair in the training set. As the ma-
trix is missing one entry for each prediction (i.e.
the judgment on the assertion for which we want
to make the prediction), one could theoretically
form a new matrix for each prediction and then

9Note that the subjects have all rated different numbers of
assertions. Thus, for the sake of comparability, we restrict
ourselves to the most similar assertion (as opposed to averag-
ing a judgment over the n most similar assertions.)

re-calculate all cosines. However, we find that
the judgment similarity between assertions does
not change significantly when a single entry in
the vectors of the assertions is removed or added.
Hence, due to computational complexity, the gold
similarity was calculated over the entire matrix of
judgments.

There are several assertions that do not have tex-
tual overlap, which is why the STS methods often
return a zero similarity. In such a case, we fall
back on the all agree baseline. We refer to strate-
gies which are based on judgment similarity as
most similar assertion (method), where method
indicates how the similarity is computed.

All strategies use all available context. For in-
stance, if we want to predict the judgment of the
assertion an and a prediction strategy considers
other judgments, the strategy uses all the judg-
ments on the assertions a1, ..., an−1.

Collaborative Filtering (CF) Collaborative fil-
tering (Adomavicius and Tuzhilin, 2005; Schafer
et al., 2007; Su and Khoshgoftaar, 2009) uses pre-
viously made judgments and judgments made by
others to predict future judgments. Collaborative
filtering has been successfully used in applica-
tion areas such shopping recommendations (Lin-
den et al., 2003), or personalization of news (Das
et al., 2007). Note that collaborative filtering re-
quires knowledge of how others judged the asser-
tion for which the system tries to make a predic-
tion. Therefore, these strategies are not applicable
if we want to predict judgments on a previously
unseen assertion. Nevertheless, they represent an
upper bound for our text-based predictions.

As a simple collaborative filtering strategy, we
predict how the majority of other persons judged
an assertion. Therefore, we average the judgments
of all other users and predict agree if this value
is positive and disagree if the value is negative.
This strategy will be referred to as mean other. In
addition, we compute the similarity between pairs
of people by calculating the cosine similarity be-
tween the vector that corresponds to all judgments
a person has made. We use this person–person
similarity to determine the most similar person
and then transfer the judgment on an of the user
which is most similar to pi. We refer to this strat-
egy as most similar user. We also use the (gold)
judgment similarity between assertions to predict
agree or disagree based on how the assertion that
is most similar to an has been judged. We call this

219



−0.4 −0.2 0.0 0.2 0.4 0.6

−
0.

4
−

0.
2

0.
0

0.
2

0.
4

0.
6

gold judgment similarity

pr
ed

ic
te

d 
ju

dg
m

en
t s

im
ila

rit
y

(a) SVM

−0.4 −0.2 0.0 0.2 0.4 0.6

−
0.

4
−

0.
2

0.
0

0.
2

0.
4

0.
6

gold judgment similarity

pr
ed

ic
te

d 
ju

dg
m

en
t s

im
ila

rit
y

(b) SNN

Figure 3: Comparison of gold judgment similarity and judgment similarity as predicted by the SVM and the SNN
for the issue Climate Change.

Strategy Type Accuracy

most similar user CF .85
most similar assertion (gold) CF .76
tendency BL .75
mean other CF .74
most similar assertion (SNN) JS .73
most similar assertion (SVM) JS .72
all agree BL .71
most similar assertion (jaccard) JS .70
most similar assertion (embedding) JS .68
most similar assertion (gst) JS .69
most similar assertion (lcss) JS .67
random BL .50

Table 4: Accuracy of different approaches for predict-
ing judgments of individuals.

strategy most similar assertion (gold).

6.1 Results

Table 4 shows the accuracy of the strategies across
all issues obtained using leave-one-out cross val-
idation. We observe that all strategies are signif-
icantly better than the random baseline. On av-
erage, the all agree strategy is more than 20%
above the random baseline and thus represents a
highly competitive baseline. The tendency base-
line, which is a refinement of all agree, is even
4% higher. Only the collaborative filtering strate-
gies most similar assertion and most similar user
beat this baseline. With an accuracy of about 85%
the most similar user strategy performs best. The
methods that use the learned judgment similar-
ity beat the all agree but fall behind the tendency
baseline. The fact that methods based on judgment

similarity are already close to their upper-bound
(most similar assertion (gold)) shows that their
potential is limited, even if measuring judgment
similarity can be significantly improved. One pos-
sible explanation for comparably low performance
of most similar assertion is that the past assertions
are not sufficient to make a meaningful prediction.
For instance, if only a few assertions have been
judged in the past and none of them is similar to
a new assertion, then a prediction becomes guess-
ing. As expected from their poor performance of
approximating judgment similarity, the methods
relying on STS measures fall behind the all agree.

7 Predicting Judgments of Groups

We now turn to predicting judgments of groups,
i.e. the task of estimating what percentage of a
group of people are likely to agree to an asser-
tion. We illustrate the prediction task in the fol-
lowing example: From the assertion ‘Marijuana is
almost never addictive’ with an aggregated agree-
ment score of 0.9 we want to predict a compara-
tively lower value for the assertion ‘Marijuana is
sometimes addictive’.

Direct Prediction (DP) As a reference ap-
proach, we train different regression models that
predict the aggregated agreement score directly
from the text of the assertion. We train each model
over all issues in order to achieve the necessary
generalization.

Again, we compare more traditional models
based on feature engineering and neural models.

220



For the feature engineering approach we exper-
iment with the following feature sets: First, we
use a length feature which consists of the num-
ber of words per assertion. To capture stylis-
tic variations, we compute a feature vector con-
sisting of the number of exclamation and ques-
tion marks, the number of modal verbs, the av-
erage word length in an assertion, POS type ra-
tio, and type token ratio. We capture the word-
ing of assertions by different ngram features. For
capturing the semantics of words, we again de-
rive features from the pre-trained fastText word
vectors (Bojanowski et al., 2017). To capture the
emotional tone of an assertion, we extract fea-
tures from the output of the readily available sen-
timent tool NRC-Canada Sentiment Analysis Sys-
tem (Kiritchenko et al., 2014).

As the neural approach on directly predicting
aggregated judgments, we use a single branch of
the Siamese network. However, since we are try-
ing to solve a regression problem, here the net-
work ends in a single node equipped with a linear
activation function. Through iterative experiments
we found out that it is advantageous to add two ad-
ditional dense layers before the final node. As this
model resembles a convolutional neural network
(CNN), we label this approach as CNN.

Judgment Similarity (JS) In analogy to the
prediction of judgments of individuals, we first
calculate the judgment similarity of two assertions
using the SVM and SNN approaches that take pair
of assertions into account. We then take the n-
most similar assertions and return the average of
the resulting scores. As an upper bound, we also
compute the judgment similarity that results from
the gold data. Note, that this upper bound again
assumes knowledge about judgments on the asser-
tion for which we actually want to make a pre-
diction. We make the code for both approaches
publicly available.10

7.1 Results
Table 5 shows the performance of the different ap-
proaches for predicting judgments of groups. For
the prediction based on judgment similarity, we
observe large differences between the the SVM
and SNN predictions. This is especially interest-
ing because the performance of the similarity pre-
diction is comparable. We attribute this to the sys-
tematic error made by the SVM when trying to

10
https://github.com/muchafel/judgmentPrediction

Model Type r

gold (n = 7) JS .90
gold (n = 1) JS .84
SNN (n = 34) JS .74
SNN (n = 1) JS .45
SVM (n = 18) JS .42
CNN DP .40
sentiment + trigrams DP .36
trigrams DP .35
unigrams + embeddings DP .32
unigrams DP .32
SVM (n = 1) JS .32
sentiment + trigrams + style DP .27
sentiment DP .13
style DP .10
length DP .00

Table 5: Correlation coefficients for approaches on pre-
dicting judgments of groups.

predict the similarity of assertions that have a neg-
ative agreement score. While the SVM only out-
performs the plain regressions if the prediction is
based on several assertions, we observe a substan-
tially better performance for the judgment similar-
ity based on the SNN. For the best judgment sim-
ilarity model (SNN with n = 34), we obtain a
coefficient of r = .74 which is substantially better
than the direct prediction model (CNN, r = .40).

For the plain regression, we observe that the
CNN outperforms all models based on feature en-
gineering and that among the SVM models ngram
features yield the best performance. While the
sentiment feature alone has low performance, the
model that combines sentiment and ngrams shows
slight improvement over the trigrams alone. The
length feature and the style features alone have a
comparable low performance and models which
combine these feature with lexical features show a
lower performance than the lexical models alone.

Issue-wise analysis To better understand the
differences between the judgment similarity meth-
ods, we inspect their performance depending on
the number of given assertions. Figure 4 shows
this comparison both for individual issues and av-
eraged across all issues. The upper-bound reaches
a correlation of up to r = .89 (n = 8). The
strength of this correlation and the fact that even
our best estimate is still 15 points less shows
the potential of judgment similarity for predicting
judgments of groups.

For the SNN, the predictions follow a similar
pattern: resembling a learning curve, the perfor-
mance increases rapidly with increasing n, but

221



0 20 40

0

0.5

1

number of assertions

co
rr

el
at

io
n

SVM
SNN
Gold

(a) Averaged

0 20 40

0

0.5

1

number of assertions

(b) issue-wise (SVM)

0 20 40

0

0.5

1

number of assertions

(c) issue-wise (SNN)

Figure 4: Prediction quality based on the transfer of the n-most similar assertions (expressed by the strength of
correlation with the gold values). Sub-figure a) shows the scores averaged across all issues. We show the variance
obtained on individual issues by the SVM in Sub-Figure b) and by the SNN in Sub-Figure c).

then plateaus from a certain number of assertions.
However, the number of assertions for which we
observe a plateau varies significantly. For the
SVM we observe a similar pattern for most of the
approaches, but the plateau is often reached much
later. There are two issues (US Engagement in the
Middle East and US Immigration) where we do
not observe an increase in performance with in-
creasing n. We suspect that the systematic error of
the SVM is particularly strong here.

8 Conclusion & Future Work

In this paper, we examined whether an automati-
cally measured judgment similarity can be used to
predict the judgments of individuals or groups on
assertions. We compare these judgment similarity
approaches against several reference approaches
on a data set of over 100,000 judgments on over
2,000 assertions. For the prediction of individual
judgments reference approaches yield competitive
results. However, for the prediction of group judg-
ments the best approach using judgment similar-
ity as predicted by a SNN outperforms other ap-
proaches by a wide margin.

While the presented approaches represent a first
take on predicting judgments on assertions, the
proposed tasks also suggest several directions of
future research. These include more advanced al-
gorithmic solutions and experiments for obtain-
ing a deeper understanding of the relationship be-
tween text and judgments. For improving the au-
tomatic prediction, we want to explore how robust

the learned models are by examining whether they
can be transferred between issues. In addition, we
want to examine if knowledge bases, issue spe-
cific corpora, or issue specific word vectors can
improve the current approaches. To better under-
stand what textual properties of assertions cause
judgment similarity, we want to annotate and ex-
perimentally control typed relationships (e.g. para-
phrases, entailment) of pairs of assertions. Be-
ing able to predict the degree to which two asser-
tions are judged similarly might also be helpful for
NLP tasks in which one tries to predict opinions
or stance of the author of an text. Hence, we want
to examine if judgment similarity can be used to
boost the performance of systems in these tasks.

Acknowledgments

This work was supported by the Deutsche
Forschungsgemeinschaft (DFG) under grant No.
GRK 2167, Research Training Group “User-
Centred Social Media”. We would also like to
thank Tobias Horsmann for helpful discussions on
implementing the SNN.

References
Martı́n Abadi, Paul Barham, Jianmin Chen, Zhifeng

Chen, Andy Davis, Jeffrey Dean, Matthieu Devin,
Sanjay Ghemawat, Geoffrey Irving, Michael Isard,
Manjunath Kudlur, Josh Levenberg, Rajat Monga,
Sherry Moore, Derek G. Murray, Benoit Steiner,
Paul Tucker, Vijay Vasudevan, Pete Warden, Martin
Wicke, Yuan Yu, and Xiaoqiang Zheng. 2016. Ten-

222



sorFlow: A System for Large-scale Machine Learn-
ing. In Proceedings of the 12th USENIX Conference
on Operating Systems Design and Implementation,
OSDI’16, pages 265–283, Savannah, USA.

Gediminas Adomavicius and Alexander Tuzhilin.
2005. Toward the next generation of recommender
systems: A survey of the state-of-the-art and pos-
sible extensions. IEEE Transactions on Knowledge
and Data Engineering, 17(6):734–749.

Eneko Agirre, Mona Diab, Daniel Cer, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A pi-
lot on semantic textual similarity. In Proceedings of
the SemEval, pages 385–393, Montreal, Canada.

Daniel Bär, Torsten Zesch, and Iryna Gurevych. 2013.
DKPro Similarity: An Open Source Framework for
Text Similarity. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics: System Demonstrations, pages 121–126,
Sofia, Bulgaria.

Roy Bar-Haim, Indrajit Bhattacharya, Francesco Din-
uzzo, Amrita Saha, and Noam Slonim. 2017. Stance
Classification of Context-Dependent Claims. In
Proceedings of the 15th Conference of the European
Chapter of the Association for Computational Lin-
guistics (EACL, pages 251–261, Valencia, Spain.

Rahul Bhagat and Eduard Hovy. 2013. What is a para-
phrase? Computational Linguistics, 39(3):463–472.

Piotr Bojanowski, Edouard Grave, Armand Joulin, and
Tomas Mikolov. 2017. Enriching word vectors with
subword information. Transactions of the Associa-
tion for Computational Linguistics, 5:135–146.

Filip Boltuzic and Jan Šnajder. 2016. Fill the gap! an-
alyzing implicit premises between claims from on-
line debates. In Proceedings of the Third Workshop
on Argument Mining (ArgMining2016), pages 124–
133, Berlin, Germany.

Fabio Celli, Evgeny Stepanov, Massimo Poesio, and
Giuseppe Riccardi. 2016. Predicting Brexit: Clas-
sifying agreement is better than sentiment and poll-
sters. In Proceedings of the Workshop on Computa-
tional Modeling of People’s Opinions, Personality,
and Emotions in Social Media (PEOPLES), pages
110–118, Osaka, Japan.

Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technol-
ogy, 2:27:1–27:27. Software available at http://
www.csie.ntu.edu.tw/˜cjlin/libsvm.

Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan
Roth. 2009. Recognizing textual entailment: Ratio-
nal, evaluation and approaches. Natural Language
Engineering, 15(4):i–xvii.

Abhinandan S. Das, Mayur Datar, Ashutosh Garg, and
Shyam Rajaram. 2007. Google News Personaliza-
tion: Scalable Online Collaborative Filtering. In

Proceedings of the 16th International Conference on
World Wide Web (WWW ’07), pages 271–280, Banff,
Canada.

Johannes Daxenberger, Oliver Ferschke, Iryna
Gurevych, and Torsten Zesch. 2014. DKPro TC:
A Java-based Framework for Supervised Learning
Experiments on Textual Data. In Proceedings of
52nd Annual Meeting of the Association for Compu-
tational Linguistics: System Demonstrations, pages
61–66, Baltimore, Maryland.

Dan Gusfield. 1997. Algorithms on strings, trees and
sequences: computer science and computational bi-
ology. Cambridge university press.

Ivan Habernal and Iryna Gurevych. 2016. Which Ar-
gument is More Convincing? Analyzing and Pre-
dicting Convincingness of Web Arguments Using
Bidirectional LSTM. In Proceedings of the 54th An-
nual Meeting of the Association for Computational
Linguistics, pages 1589–1599, Berlin, Germany.

James J. Heckman and James M. Snyder. 1996. Lin-
ear Probability Models of the Demand for Attributes
with an Empirical Application to Estimating the
Preferences of Legislators. The RAND Journal of
Economics, 28:142–189.

Tobias Horsmann and Torsten Zesch. 2018. DeepTC
– An Extension of DKPro Text Classification for
Fostering Reproducibility of Deep Learning Ex-
periments. In Proceedings of the International
Conference on Language Resources and Evaluation
(LREC), Miyazaki, Japan.

Baotian Hu, Zhengdong Lu, Hang Li, and Qingcai
Chen. 2014. Convolutional Neural Network Ar-
chitectures for Matching Natural Language Sen-
tences. In Advances in Neural Information Process-
ing Systems (NIPS 27), pages 2042–2050, Montreal,
Canada.

Svetlana Kiritchenko, Xiaodan Zhu, and Saif M Mo-
hammad. 2014. Sentiment Analysis of Short In-
formal Texts. Journal of Artificial Intelligence Re-
search (JAIR), 50:723–762.

Greg Linden, Brent Smith, and Jeremy York. 2003.
Amazon.com recommendations: Item-to-item Col-
laborative Filtering. IEEE Internet computing,
7(1):76–80.

Bing Liu. 2012. Sentiment analysis and opinion min-
ing. Synthesis lectures on human language tech-
nologies, 5(1):1–167.

Caroline Lyon, James Malcolm, and Bob Dickerson.
2001. Detecting short passages of similar text in
large document collections. In Proceedings of Con-
ference on Empirical Methods in Natural Language
Processing, pages 118–125, Pittsburgh, USA.

Stefano Menini and Sara Tonelli. 2016. Agreement and
Disagreement: Comparison of Points of View in the

223



Political Domain. In Proceedings of the 26th Inter-
national Conference on Computational Linguistics
(COLING), pages 2461–2470, Osaka, Japan.

Saif M. Mohammad. 2016. Sentiment analysis: De-
tecting valence, emotions, and other affectual states
from text. Emotion Measurement, pages 201–238.

Saif M. Mohammad, Svetlana Kiritchenko, Parinaz
Sobhani, Xiaodan Zhu, and Colin Cherry. 2016.
SemEval-2016 Task 6: Detecting Stance in Tweets.
In Proceedings of the SemEval, pages 31–41, San
Diego, USA.

Jonas Mueller and Aditya Thyagarajan. 2016. Siamese
Recurrent Architectures for Learning Sentence Sim-
ilarity. In Proceedings of the Thirtieth AAAI Con-
ference on Artificial Intelligence, AAAI’16, pages
2786–2792, Phoenix, USA.

Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,
Veselin Stoyanov, Alan Ritter, and Theresa Wilson.
2013. SemEval-2013 Task 2: Sentiment Analysis in
Twitter. In Proceedings of the SemEval, pages 312–
320, Atlanta, USA.

Paul Neculoiu, Maarten Versteegh, and Mihai Rotaru.
2016. Learning text similarity with siamese recur-
rent networks. In Proceedings of the 1st Workshop
on Representation Learning for NLP, pages 148–
157.

Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends R© in In-
formation Retrieval, 2(1–2):1–135.

Sasa Petrovic, Miles Osborne, and Victor Lavrenko.
2011. RT to Win! Predicting Message Propaga-
tion in Twitter. Proceedings of the Fifth Interna-
tional AAAI Conference on Weblogs and Social Me-
dia, 11:586–589.

Keith T. Poole and Howard Rosenthal. 1997.
Congress: A Political-economic History of Roll Call
Voting. Oxford University Press.

Keith T. Poole and Howard Rosenthal. 2001. D-
Nominate after 10 Years: A Comparative Update
to Congress: A Political-Economic History of Roll-
Call Voting. Legislative Studies Quarterly, 26(1):5–
29.

Reid Pryzant, Young-joo Chung, and Dan Jurafsky.
2017. Predicting Sales from the Language of Prod-
uct Descriptions. In Proceedings of the SIGIR
2017 Workshop on eCommerce (ECOM 17), Tokyo,
Japan.

Sara Rosenthal, Alan Ritter, Preslav Nakov, and
Veselin Stoyanov. 2014. Semeval-2014 task 9: Sen-
timent analysis in twitter. In Proceedings of the Se-
mEval, pages 73–80, Dublin, Ireland.

J. Ben Schafer, Dan Frankowski, Jon Herlocker, and
Shilad Sen. 2007. Collaborative Filtering Recom-
mender Systems. The Adaptive Web, pages 291–
324.

Xiaoyuan Su and Taghi M. Khoshgoftaar. 2009. A
Survey of Collaborative Filtering Techniques. Ad-
vances in Artificial Intelligence, pages 4:2–4:2.

Bongwon Suh, Lichan Hong, Peter Pirolli, and Ed H.
Chi. 2010. Want to be Retweeted? Large Scale An-
alytics on Factors Impacting Retweet in Twitter Net-
work. In Proceedings of the Second IEEE Interna-
tional Conference on Social Computing, pages 177–
184, Washington, USA.

Chenhao Tan, Lillian Lee, and Bo Pang. 2014. The Ef-
fect of Wording on Message Propagation: Topic-and
Author-controlled Natural Experiments on Twitter.
In Proceedings of the ACL, pages 175–185, Balti-
more, USA.

Mariona Taulé, M Antonia Martı, Francisco Rangel,
Paolo Rosso, Cristina Bosco, and Viviana Patti.
2017. Overview of the task of stance and gender de-
tection in tweets on catalan independence at ibereval
2017. In Notebook Papers of 2nd SEPLN Work-
shop on Evaluation of Human Language Technolo-
gies for Iberian Languages (IBEREVAL), Murcia,
Spain, September, volume 19.

Lu Wang and Claire Cardie. 2014. Improving Agree-
ment and Disagreement Identification in Online Dis-
cussions with A Socially-Tuned Sentiment Lexicon.
In Proceedings of the 5th Workshop on Computa-
tional Approaches to Subjectivity, Sentiment and So-
cial Media Analysis (WASSA ’14), pages 97–106.

Zhongyu Wei, Yang Liu, and Yi Li. 2016. Is This Post
Persuasive? Ranking Argumentative Comments in
the Online Forum. In Proceedings of the ACL, pages
195–200, Berlin, Germany.

Michael J. Wise. 1996. Yap3: Improved detection
of similarities in computer program and other texts.
ACM SIGCSE Bulletin, 28(1):130–134.

Michael Wojatzki, Saif M. Mohammad, Torsten Zesch,
and Svetlana Kiritchenko. 2018. Quantifying Qual-
itative Data for Understanding Controversial Is-
sues. In Proceedings of the 11th Edition of the
Language Resources and Evaluation Conference
(LREC-2018), Miyazaki, Japan.

Ruifeng Xu, Yu Zhou, Dongyin Wu, Lin Gui, Jiachen
Du, and Yun Xue. 2016. Overview of NLPCC
Shared Task 4: Stance Detection in Chinese Mi-
croblogs. In International Conference on Computer
Processing of Oriental Languages, pages 907–916,
Kunming, China.

224


