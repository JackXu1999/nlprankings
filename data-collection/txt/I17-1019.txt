



















































Addressing Domain Adaptation for Chinese Word Segmentation with Global Recurrent Structure


Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 184–193,
Taipei, Taiwan, November 27 – December 1, 2017 c©2017 AFNLP

Addressing Domain Adaptation for Chinese Word Segmentation with
Global Recurrent Structure

Shen Huang and Xu Sun and Houfeng Wang∗
Key Laboratory of Computational Linguistics, Ministry of Education

School of Electronics Engineering and Computer Science, Peking University
Beijing, P.R.China, 100871

huangshenno1,xusun,wanghf@pku.edu.cn

Abstract

Boundary features are widely used in
traditional Chinese Word Segmentation
(CWS) methods as they can utilize un-
labeled data to help improve the Out-of-
Vocabulary (OOV) word recognition per-
formance. Although various neural net-
work methods for CWS have achieved per-
formance competitive with state-of-the-art
systems, these methods, constrained by
the domain and size of the training corpus,
do not work well in domain adaptation. In
this paper, we propose a novel BLSTM-
based neural network model which incor-
porates a global recurrent structure de-
signed for modeling boundary features dy-
namically. Experiments show that the
proposed structure can effectively boost
the performance of Chinese Word Seg-
mentation, especially OOV-Recall, which
brings benefits to domain adaptation. We
achieved state-of-the-art results on 6 do-
mains of CNKI articles, and competitive
results to the best reported on the 4 do-
mains of SIGHAN Bakeoff 2010 data.

1 Introduction

Since Chinese writing system does not have ex-
plicit word delimiters, word segmentation be-
comes an essential first step for further Chinese
language processing. In recent years, Chinese
Word Segmentation (CWS) has experienced great
advancement. One mainstream method is to re-
gard word segmentation task as a sequence label-
ing problem (Xue, 2003; Peng et al., 2004) where
each character is assigned a tag indicating its po-
sition in the word. This method has been proved

∗Corresponding author

effective as it turns word segmentation into a struc-
tured discriminative learning task which can be
handled by supervised learning algorithms such as
Maximum Entropy (ME) (Berger et al., 1996) and
Conditional Random Fields (CRF) (Lafferty et al.,
2001). Furthermore, rich features can be incorpo-
rated into these systems to improve their perfor-
mances and most state-of-the-art systems are still
based on feature-based models.

Recently, neural network models are drawing
increasing attention in Natural Language Process-
ing (NLP) tasks. They significantly reduced fea-
ture engineering effort and achieved competitive
or state-of-the-art results in many NLP tasks. Col-
lobert et al. (2011) developed a general neural net-
work architecture for sequence labeling tasks. Fol-
lowing this work, many neural network model-
s (Zheng et al., 2013; Pei et al., 2014; Chen et al.,
2015a,b) have been applied to CWS and some ap-
proached state-of-the-art performance.

However, these neural network models, as well
as other supervised methods, do not work well
in domain adaptation. In recent years, manual-
ly annotated training corpus mostly come from
the news domain. When it shifts to other do-
mains such as literature or medicine, where there
are many domain-related words that rarely ap-
pear in other domains, Out-of-Vocabulary (OOV)
word recognition becomes an important problem.
Moreover, different domains means different lan-
guage usages and contexts. Therefore, the In-
Vocabulary (IV) word segmentation performance
is also affected. As a result, CWS accuracies can
drop gravely on cross-domain corpora. For ex-
ample, consider a sentence “三聚氰胺(melamine)
/ 致(lead to) / 婴幼儿(baby) / 泌尿系(urinary
tract) / 结石(stones)”. Here the word “三聚氰
胺(melamine)” is a chemical that often appears in
medicine-related domains while seldom appears in
other domains. It is a four-Chinese-character word

184



where each character stands for ‘three’, ‘gather’,
‘cyanide’ and ‘amine’. The four characters are to-
tally irrelevant. A supervised CWS system trained
on news domain corpus would face great chal-
lenges on segmenting this word correctly

Several approaches have been proposed to ad-
dress the domain adaption problem for CWS. One
major family proposed to compose boundary fea-
tures by fitting the relevance of consecutive char-
acters using Accessor Variety (AV) (Feng et al.,
2004a,b), or Chi-square Statistics (Chi2) (Chang
and Han, 2010). Combining the boundary fea-
tures with other hand-crafted features, these meth-
ods were shown to achieve better performance on
OOV words.

Inspired by these models, we propose a nov-
el BLSTM-based neural network model which in-
corporates a global recurrent structure designed to
model boundary features dynamically. This struc-
ture can learn to utilize the target domain corpus
and extract the correlation or irrelevance between
characters, which is a reminiscence of the discrete
boundary features such as Accessor Variety (AV).

The contributions of this paper are two folds:

• First, we propose a global recurrent structure
and incorporate it in the BLSTM-based neu-
ral network model for CWS. The structure
can capture correlations between characters,
and thus is especially efficient for segmenting
OOV words and enhancing the performance
of CWS on non-news domains.

• We obtain competitive results comparing to
the best reported in the literature on the
SIGHAN Bakeoff 2010 data, which is a
benchmark dataset for cross-domain CWS.

2 BLSTM Architecture for Chinese
Word Segmentation

We regard Chinese word segmentation task as a
character-based sequence labeling problem, by la-
beling each character a tag from {S, B, E, M}.
These tags indicate the position of the character
in the segmented word. B, E, M represents Begin,
Middle, End of a multi-character segmentation re-
spectively, while S represents a single-character
segmentation.

Figure 1 illustrates the general BLSTM archi-
tecture for Chinese word segmentation.

Figure 1: General architecture for Chinese word
segmentation.

2.1 Embeddings

The outputs of the embedding layer is a concatena-
tion of three parts: character embeddings, bigram
embeddings and boundary feature embeddings.

We adopt the the local window approach which
assumes that the tag of a character largely depend-
s on its neighboring characters. For each charac-
ter ci in a given input sentence c[1:n], the context
characters c[i−w/2:i+w/2] and their corresponding
bigrams are chosen to be fed into the network-
s, where w is the context window size. As most
CWS methods do, we will set w = 5 in our exper-
iments.

Given a character set V of size |V |, each char-
acter c ∈ V will be mapped into a d-dimensional
embedding space as Embc(c) ∈ Rd by a lookup
table Mc ∈ Rd×|V |. Similarly, each bigram b ∈
{c1c2|c1 ∈ V, c2 ∈ V } will be mapped into a d-
dimensional embedding space as Embb(b) ∈ Rd
by a lookup table Mb ∈ Rd×|V |×|V |.

The boundary feature embeddings are hidden
vectors computed from the current bigrams and
the whole bigarm history, which will be explained
in detail in Section 3.

185



Three kinds of embeddings of the context char-
acters c[i−2:i+2] and their corresponding bigram-
s are then concatenated into a single vector xi ∈
RH1 , whereH1 = 5d+4d+4dbf . dbf is the num-
ber of hidden units output by the boundary feature
embeddings. Then, this vector xi is fed into the
BLSTM layer.

2.2 Bidirectional LSTM Network
Following the embedding layer is an one-layer
BLSTM network (Graves and Schmidhuber,
2005). By combining hidden states from two sep-
arate LSTM layers, it can incorporate long period-
s of contextual information from both directions.
The LSTM cell is implemented as follows (Graves
et al., 2013):

it = σ(Wxixt +Whiht−1 +Wcict−1 + bi)
ft = σ(Wxfxt +Whfht−1 +Wcfct−1 + bf )
ct = ftct−1 + ittanh(Wxcxt +Whcht−1 + bc)
ot = σ(Wxoxt +Whoht−1 +Wcoct + bo)
ht = ottanh(ct)

(1)

where σ is the logistic sigmoid function, and i,
f , o and c are the input gate, forget gate, output
gate and the cell respectively, all of which are the
same dimension as the hidden output h. The sub-
scripts of the weight matrix describe the meaning
as the name suggests. For instance, Wxi is the in-
put gate weight matrix for input x.

The outputs of the BLSTM layer are the con-
catenation of a forward hidden sequence

→
h and a

backward hidden sequence
←
h which will be fed to

the decoding layer that contains a linear transfor-
mation with no non-linear function:

f(ti|c[i−w/2:i+w/2]) = Wd(
→
hi ⊕

←
hi) + bd (2)

where Wd ∈ R|T |×H2 , bd ∈ R|T |. H2 is
the number of hidden units of the outputs for the
BLSTM layer. f(ti|c[i−w/2:i+w/2]) ∈ R|T | is the
score vector for each possible tag. Here in Chinese
word segmentation, we set T = {S,B,E,M}.
2.3 Tag Inference
To model the correlations between tags in neigh-
borhoods and jointly decode the best chain of tags
for a given sentence, a transition scoreAij is intro-
duced to measure the probability of jumping from

tag i ∈ T to tag j ∈ T (Collobert et al., 2011).
For an input sentence c[1:n] with a tag sequence
t[1:n], a sentence-level score can be formulated as
follows:

s(c[1:n], t[1:n], θ) =
n∑
i=1

(Ati−1ti+fθ(ti|c[i−2:i+2]))
(3)

where fθ(ti|c[i−2:i+2]) indicates the score out-
put for the ith tag computed by the neural network
described above with parameters θ.

3 Global Recurrent Structure

Chinese word segmentation is essentially a task of
resolving the relevance of consecutive characters.
Lacking knowledge of such relevance, recogniz-
ing out-of-domain words has been the bottleneck
of domain adaption in CWS. However, Bound-
ary features such as Accessor Variety (AV) (Feng
et al., 2004a,b), Mutual Information (Sun and X-
u, 2011) and Chi-square Statistics (Chi2) (Chang
and Han, 2010) are features designed to fit such
relevance. A significant advantage of boundary
features is that they can compute the correlation
of characters from a large scale corpora, annotated
or not, to boost the OOV word recognition perfor-
mance. As a result, they are especially effective
for cross-domain CWS.

In this paper, we propose 5 novel global recur-
rent structures to generate embeddings that mim-
ic the boundary features for further computing,
which needs minimal pre-processing and feature
engineering. The structures are designed to cap-
ture the intuition that nearby sentences in a single-
domain corpus often share certain words. Thus the
correlation of characters within or across certain
words can be learned, and those involving OOV
words notably enhance domain adaption for CWS.
GRS-1 The basic structure(GRS-1) is illustrat-
ed in Figure 2. It looks like LSTM-2 (Chen et al.,
2015b) when incorporated into the BLSTM mod-
el. However, the difference is that common re-
current networks will reset the hidden states every
time they process a new sentence in NLP problem-
s while the hidden states in our structure are never
reset.

hk+1,0, ck+1,0 = hk,nk , ck,nk (4)

where hk,i and ck,i are the hidden state and cell
vector of the kth sentence at the ith step, nk is the

186



Figure 2: Global recurrent structure.

length of the kth sentence. For simplicity, in the
following part we will ignore the subscript k and
always indicate the current sentence.

As a result of such warm start mechanism, our
structure can to some extent record the history in-
formation in recent sentences. And some informa-
tion may last long in the structure.

Here we choose the LSTM cell as it can learn
to keep relatively long term memory. We fol-
low the equations (1) to implement it and direct-
ly take hi as the boundary feature embeddings for
the bigram bi = cici+1 in the basic structure,
where the input is the concatenation of embed-
dings of a bigram and its corresponding characters
Embb(bi)⊕ Embc(ci)⊕ Embc(ci+1).

Embbf (bi) = hi (5)

where hi is the output of the recurrent network at
the ith step.

We also propose four more variants of the struc-
ture that are shown in Figure 3.
GRS-2 To better fit the boundary features, we
add a full-connection hidden layer following the
recurrent network. The boundary feature embed-
dings are calculated as follows:

Embbf (bi) = σ(Wbfhi + bbf ) (6)

where σ is the logistic sigmoid function.
GRS-3 Considering the hidden states are noisy
and contains much information of other words, we
want the hidden values more relevant to the current
bigram, so a gate is introduced to the structure.
The boundary feature embeddings are calculated

as follows:

Ei = Embc(ci)⊕ Embc(ci+1)⊕ Embb(bi)
g(bi) = σ(WgEi + bg)
Embbf (bi) = g(bi)hi

(7)

where ⊕ is the symbol for concatenation.
GRS-4 GRS-4 is a combination version of GRS-
2 and GRS-3 by adding a full-connection hidden
layer following the gated output.
GRS-5 GRS-5 is a more complicated version
which tries to mimic the Accessor Variety(AV)
criterion. AV criterion is a feature describing the
number of distinct characters that precede or suc-
ceed a certain string s. For simplicity, we only
focus on strings with length = 2, in other word-
s, bigrams. Therefore, we substitute the input of
GRS-4 with a bigarm and its preceding character
to fit its left AV and similarly with a bigram and
its succeeding character to fit its right AV. At last,
we simply concatenate the two embeddings as the
final boundary feature embeddings (Actually they
are trigram boundary feature embeddings):

ELi = Embc(ci−1)⊕ Embb(bi)
ERi = Embc(ci+1)⊕ Embb(bi−1)
gL(trii) = σ(WLg E

L
i + b

L
g )

gR(trii) = σ(WRg E
R
i + b

R
g )

EmbLbf (trii) = σ(W
L
bfg

L(trii)hLi + b
L
bf )

EmbRbf (trii) = σ(W
R
bfg

R(trii)hRi + b
R
bf )

Embbf (trii) = EmbLbf (trii)⊕ EmbRbf (trii)
(8)

where trii = ci−1cici+1 and other values have the
same meanings as above.

4 Training

Instead of using the Max-Margin criterion (Taskar
et al., 2005) adopted by previous neural network
models for CWS (Zheng et al., 2013; Pei et al.,
2014; Chen et al., 2015a,b), we try to directly
maximize the log-probability of the correct tag se-
quence following Lample et al. (2016):

log(p(y|X)) = s(X, y)− log(
∑
ỹ∈YX

es(X,ỹ))

= s(X, y)− logadd
ỹ∈YX

s(X, ỹ)
(9)

187



Figure 3: Four variants of the global recurrent structure.

where YX represents all possible tag sequences
for a sentence X . While decoding, we predict the
output sequence which obtains the maximum s-
core as follows:

y∗ = argmax
ỹ∈YX

s(X, ỹ) (10)

The optimal sequence can be computed using
dynamic programming. We use Adam (Kingma
and Ba, 2014) to maximize the objective function.

5 Experiments

5.1 Experimental Setup

Data. We use the PKU corpus drawn from news
domain for the source-domain training. The PKU
dataset is provided by SIGHAN Bakeoff 2005 (E-
merson, 2005). We regard the random 90% sen-
tences of the training data as training set and the
rest 10% sentences as development set. We also
use the test part of the PKU dataset to measure
the in-domain segmentation ability of our mod-
els. Following Liu et al. (2014)’s settings, our
domain adaption experiments are performed on
the four testing sets from the SIGHAN Bakeoff
2010 (Zhao and Liu, 2010) whose domains cov-
er finance, computer, medicine and literature. In
addition, we manually annotate six more corpora
from non-news domains as testing sets, including
finance, medicine, geology, agriculture, material
and weather domains, which are extracted from
abstracts of papers in CNKI1. These data are an-
notated following the guideline proposed by Yu et
al. (2001). The OOV rate of these data are relative-
ly high because they are more academic. Statistics

1http://www.cnki.net/

of the training and testing data are shown in the
Table 1.

All datasets are pre-processed by replacing the
Chinese idioms and the continuous English letters
and digits with a unique token.
Embeddings. We use word2vec2 to pre-train
character embeddings on the training corpus. The
bigram embeddings are initialized with the aver-
age of the corresponding two characters’ embed-
dings.
Discrete Boundary Features. The discrete
boundary features which will be used in Sec-
tion 5.3 are extracted from the datasets mentioned
above and the Chinese Gigaword corpus3, follow-
ing methods in Sun and Xu (2011)’s paper.
Hyper-parameters. The hyper-parameters are
tuned according to the experimental results. The
detailed values are shown in Table 2.

Character & bigram embedding size 100
Boundary feature embedding size 100
Hidden unit number(cell in GRS) 300
Hidden unit number(cell in BLSTM) 300
Batch size 10
Early stop 5
Initial learning rate 0.02
Dropout rate on input layer 0.2
Regularization 10−4

Table 2: Settings of the hyper-parameters.

5.2 Model Selection
We evaluate the baseline BLSTM model and our
five proposed structures with the parameter set-
tings in Table 2 on the PKU test data and six do-

2 http://word2vec.googlecode.com/
3https://catalog.ldc.upenn.edu/LDC2003T05

188



Dataset
Train Test Test-Bakeoff2010

PKU Finance Computer Medicine Literature
# of Sent. 19056 1945 561 1330 1309 671

# of Words 1109947 104372 33035 35319 31499 35735
OOV Rate 0.0575 0.0874 0.1522 0.1102 0.0694

Dataset
Test-CNKI

Finance Medicine Geology Agriculture Material Weather
# of Sent. 100 100 100 100 100 100

# of Words 27549 37803 29251 28780 26778 27228
OOV Rate 0.0437 0.2247 0.1910 0.1689 0.2224 0.1449

Table 1: Statistics of datasets used in this paper.

mains from the CNKI dataset. The results are
shown in Table 3. The BLSTM+GRS-4 model
with a gate and an additional full-connection hid-
den layer achieves the best performances among
all domains. Surprisingly, the most delicate struc-
ture GRS-5 seems to be of no help to the CWS
task.

To examine whether OOV recognition can ben-
efit from GRS, we also look into the IV and OOV
recalls of the PKU dataset respectively. Table 4
and Table 5 show that the proposed GRS can effec-
tively improve the segmentation performance on
OOV words, which empirically proves its domain
adaption ability. BSLTM-2, similar to LSTM-
2 (Chen et al., 2015b), is an architecture comprised
of two stacking bidirectional LSTM hidden layers.
GRS-4 is short for BLSTM+GRS-4 model.

Methods IV Recall OOV Recall
BLSTM 97.12 83.01

BLSTM-2 96.89 82.59
GRS-4 96.91 83.78

Table 4: IV and OOV recalls on the PKU develop-
ment data.

Methods IV Recall OOV Recall
BLSTM 96.35 82.67

BLSTM-2 96.11 82.01
GRS-4 96.25 83.96

Table 5: IV and OOV recalls on the PKU test data.

5.3 Final Results

In this section, We compare our BLSTM+GRS-4
model with previous state-of-the-art methods.

Experimental results on the four test domain-
s from SIGHAN Bakeoff 2010 are shown in Ta-

ble 6. We also attempt to integrate discrete bound-
ary features into the models. In our experiments,
we choose the Accessor Variety(AV) (Feng et al.,
2004a,b) which is a feature widely used in tradi-
tional Chinese word segmentation. Our F-scores
and OOV recalls are competitive to those report-
ed by Liu et al. (2014) and Jiang et al. (2013).
However, following Liu et al. (2014)’s setting,
we choose the PKU dataset as the training cor-
pus while Jiang et al. (2013)’s model is trained
on a different corpus. The results are not direct-
ly comparable. The results prove the incredible
effectiveness of the global recurrent structure on
OOV recognition and overall segmentation, com-
parable to the BLSTM model that directly incor-
porates discrete AV features. Adding discrete AV
features into our model seem not to be a notable
improvement, which also confirms that our model
already has certain domain adaption ability.

Models PKU MSRA
(Zheng et al., 2013) 92.8 93.9

(Pei et al., 2014) 95.2 97.2
(Chen et al., 2015a) 96.4 97.6
(Chen et al., 2015b) 96.5 97.4
(Chen et al., 2015a)* 94.5 95.4
(Chen et al., 2015b)* 94.8 95.6
(Cai and Zhao, 2016) 95.5 96.5
(Zhang et al., 2016) 95.7 97.7

BLSTM 95.9 97.0
This work 95.9 97.1

Table 7: Comparison of our model with previous
neural models on the PKU and MSRA datasets.
Results with * are from runs on their released im-
plementation (Cai and Zhao, 2016).

We compare the in-domain experimental result-
s on the PKU and MSRA datasets with previ-

189



Dataset Baseline(F%) GRS-1(F%) GRS-2(F%) GRS-3(F%) GRS-4(F%) GRS-5(F%)
PKU 95.91 95.17 95.90 95.35 95.92 94.81

Out-of-Domain
Finance 96.87 97.15 96.78 96.68 97.15 96.09

Medicine 85.01 86.24 85.98 85.83 87.13 85.97
Geology 87.59 88.52 88.90 88.38 89.22 86.90

Agriculture 89.51 90.54 91.16 90.90 91.18 90.08
Material 87.29 88.84 89.04 88.28 89.62 87.79
Weather 90.62 92.21 92.70 92.21 93.21 91.15

Table 3: Experimental results of the baseline BLSTM model and our proposed structures on the PKU
test data and six domains from the CNKI dataset.

Method
Finance Computer Medicine Literature

Avg-F Avg-Roov
F Roov F Roov F Roov F Roov

BLSTM 94.70 86.02 92.17 81.84 91.34 73.51 92.51 73.80 92.68 78.79
BLSTM+AV 95.77 90.91 93.57 82.82 92.50 83.12 93.79 84.60 93.91 85.36

GRS-4 95.81 91.21 93.99 83.81 92.26 83.27 94.33 81.30 94.10 84.90
GRS-4+AV 95.77 91.02 93.20 83.97 91.80 82.17 93.50 82.01 93.57 84.77

Liu2014 95.54 88.53 93.93 87.53 92.47 78.28 92.49 76.84 93.61 82.80
Jiang2013 93.16 91.19 93.34 93.53 92.80

Table 6: Experimental results of the baseline BLSTM model, best-performance BLSTM+GRS-4 model,
models with discrete AV features and models proposed by others on the SIGHAN Bakeoff2010 data.

ous neural models, which is shown in Table 7.
The baseline BLSTM model with no modification
or augmentation can achieve comparative result-
s while the GRS does little help to the in-domain
Chinese word segmentation task.

5.4 Error Analysis

We collect and analyze the errors on the Medicine
corpus from Sighan Bakeoff 2010 in light of the
fact that the results are the worst among the four
domains. We calculate accuracies of individual
OOV words, where accuracies are simply treat-
ed as 0 or 1 for further counting, and catego-
rize them according to their frequencies in the
testing corpus. Statistics are shown in Figure 4.
From the trendlines we can infer that in our pro-
posed GRS more occurrences yield higher accura-
cy while common BLSTM models can rarely ben-
efit from this. That conforms to the intuition of
our model that can utilize correlation information
of testing corpora. Our model thereupon performs
better with the increase of the size of testing cor-
pus as long as the OOV words appear more.

Although the trendline of our model is promis-
ing, there are some OOV words that occurs fre-
quently but are wrongly segmented. Some ex-
amples are listed in Table 8. Errors involving

“肾脏”(kidney) and “维生素C”(vitamin C) are
typical examples of the Combination Ambigui-
ty, where there are some words containing “肾
脏” such as “肾脏病学”(nephrology). Likewise,
“维生素”(vitamin) is a frequent word that con-
fuses our model. “甲型H1N1流感”(influenza
A(H1N1)) reveals another severe problem that
most CWS systems confront when processing the
mix of Chinese characters and digits, punctuation-
s or letters from other languages. The commonly
used methods by treating consecutive digits or let-
ters as one indeed boost the performance on cor-
pora where most characters are Chinese. However,
with the increase of characters other than Chinese,
it is becoming a problem that should be reconsid-
ered carefully.

OOV Word English Correct Total
肾脏 kidney 15 39

甲型H1N1流感 influenza A 0 30
(H1N1)

维生素C vitamin C 2 23

Table 8: Some examples of wrongly segmented
OOV words with high frequency.

190



Figure 4: OOV word recognition accuracies on the Medicine corpus.

6 Related Work

Word segmentation has been pursued with consid-
erable efforts in the Chinese NLP community. One
mainstream method is regarding word segmenta-
tion task as a sequence labeling problem (Xue,
2003; Peng et al., 2004). Recently, researcher-
s have tended to explore neural network based
approaches (Collobert et al., 2011; Zheng et al.,
2013; Qi et al., 2014) to reduce efforts of feature
engineering. Pei et al. (2014) used a neural ten-
sor model to capture the complicated interactions
between tags and context characters. Experiments
in his paper also show that bigram embeddings are
of great benefit. To incorporate complicated com-
binations and long-term dependency information
of the context characters, gated recursive mod-
el (Chen et al., 2015a) and LSTM model (Chen
et al., 2015b) were used respectively. Moreover,
Xu and Sun (2016) proposed a dependency-based
gated recursive model which merges the benefit-
s of the two models above. Coincidentally, Cai
and Zhao (2016) and Zhang et al. (2016) both ad-
dressed the problem of lacking word-based fea-
tures that previous neural CWS models have. Cai
and Zhao (2016) proposed a novel gated com-
bination neural network which thoroughly elim-
inates context windows and can utilize complete
segmentation history. Zhang et al. (2016) pro-
posed a transition-based neural model which re-
places manually designed discrete features with
neural features.

Domain adaption for Chinese word segmenta-
tion has been widely exploited before neural CWS
models are proposed. Jiang et al. (2013) utilized
the web text(160K Wikipedia) to improves seg-

mentation accuracies on several domains. Zhang
et al. (2014) studied type-supervised domain adap-
tation for Chinese segmentation by making use
of domain-specific tag dictionaries and only un-
labeled target domain data. Liu et al. (2014) pro-
posed a variant CRF model to leverage both fully
and partially annotated data transformed from dif-
ferent sources of free annotations consistently.

Some researches which focus on making use
of unlabeled data for word segmentation also do
help to domain adaption. Zhao and Kit (2008)
and Zhang et al. (2013a) improved segmentation
performance by mutual information between char-
acters, collected from large unlabeled data. Li
and Sun (2009) used punctuation information in
a large raw corpus to learn a segmentation mod-
el, and achieve better recognition of OOV word-
s. Sun and Xu (2011) explored several statisti-
cal features derived from both unlabeled data to
help improve character-based word segmentation.
Zhang et al. (2013b) proposed a semi-supervised
approach that dynamically extracts representation-
s of label distributions from both in-domain corpo-
ra and out-of-domain corpora.

7 Conclusion and Perspectives

In this paper, we propose a novel global recurren-
t structure to model dynamic boundary features
and incorporate it in the BLSTM-based neural net-
work model for Chinese Word Segmentation. The
structure can capture correlations between charac-
ters, and thus is especially effective for segment-
ing OOV words and enhancing the performance of
CWS on non-news domains.

The proposed global recurrent structure is not
limited to the Chinese word segmentation task. It

191



can be easily adapted to other sequence labeling
problems that may benefit from the history infor-
mation carried in the structure.

Although the structure is effective in this task,
it’s admittedly hard to train a stable model. As
our future work, we would like to try some pre-
training methods to handle this problem. And we
plan to apply our method to other natural language
processing tasks, such as Name Entity Recogni-
tion (NER). Also, the hybrid model is a great idea
to try and we will do it later.

Acknowledgments

Our work is supported by National Natural
Science Foundation of China (No.61370117 &
No.61433015). The corresponding author of this
paper is Houfeng Wang.

References
Adam L. Berger, Vincent J. Della Pietra, and Stephen

A. Della Pietra. 1996. A maximum entropy ap-
proach to natural language processing. Computa-
tional Linguistics 22(1):39–71.

Deng Cai and Hai Zhao. 2016. Neural word segmenta-
tion learning for chinese. CoRR abs/1606.04300.

Baobao Chang and Dongxu Han. 2010. Enhancing do-
main portability of chinese segmentation model us-
ing chi-square statistics and bootstrapping. In Pro-
ceedings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing. Association
for Computational Linguistics, pages 789–798.

Xinchi Chen, Xipeng Qiu, Chenxi Zhu, and Xuanjing
Huang. 2015a. Gated recursive neural network for
chinese word segmentation. In Proceedings of the
53rd Annual Meeting of the Association for Compu-
tational Linguistics and the 7th International Joint
Conference on Natural Language Processing (Vol-
ume 1: Long Papers). Association for Computation-
al Linguistics, Beijing, China, pages 1744–1753.

Xinchi Chen, Xipeng Qiu, Chenxi Zhu, Shiyu Wu, and
Xuanjing Huang. 2015b. Sentence modeling with
gated recursive neural network. In Proceedings of
the 2015 Conference on Empirical Methods in Natu-
ral Language Processing. Association for Computa-
tional Linguistics, Lisbon, Portugal, pages 793–798.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Research
999888:2493–2537.

Thomas Emerson. 2005. The second international chi-
nese word segmentation bakeoff. In Proceedings
of the Second SIGHAN Workshop on Chinese Lan-
guage Processing. pages 123–133.

Haodi Feng, Kang Chen, Xiaotie Deng, and Weimin
Zheng. 2004a. Accessor variety criteria for chinese
word extraction. Computational Linguistics, Vol-
ume 30, Number 1, March 2004 .

Haodi Feng, Kang Chen, Chunyu Kit, and Xiaotie
Deng. 2004b. Unsupervised segmentation of chi-
nese corpus using accessor variety. In Natural Lan-
guage Processing - IJCNLP 2004, First Internation-
al JointConference, Hainan Island, China, March
22-24, 2004, Revised Selected Papers. pages 694–
703.

Alex Graves, Abdel rahman Mohamed, and Geof-
frey E. Hinton. 2013. Speech recognition with deep
recurrent neural networks. CoRR abs/1303.5778.

Alex Graves and Jürgen Schmidhuber. 2005. Frame-
wise phoneme classification with bidirectional lstm
and other neural network architectures. Neural Net-
works 18:602–610.

Wenbin Jiang, Meng Sun, Yajuan Lü, Yating Yang, and
Qun Liu. 2013. Discriminative learning with natural
annotations: Word segmentation as a case study. In
ACL.

Diederik P. Kingma and Jimmy Ba. 2014. Adam:
A method for stochastic optimization. CoRR ab-
s/1412.6980.

John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth In-
ternational Conference on Machine Learning. ICM-
L ’01, pages 282–289.

Guillaume Lample, Miguel Ballesteros, Sandeep Sub-
ramanian, Kazuya Kawakami, and Chris Dyer. 2016.
Neural architectures for named entity recognition.
In HLT-NAACL.

Zhongguo Li and Maosong Sun. 2009. Punctuation as
implicit annotations for chinese word segmentation.
Computational Linguistics 35:505–512.

Yijia Liu, Yue Zhang, Wanxiang Che, Ting Liu, and
Fan Wu. 2014. Domain adaptation for crf-based chi-
nese word segmentation using free annotations. In
EMNLP.

Wenzhe Pei, Tao Ge, and Baobao Chang. 2014. Max-
margin tensor neural network for chinese word seg-
mentation. In Proceedings of the 52nd Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers). Association for Computa-
tional Linguistics, Baltimore, Maryland, pages 293–
303.

Fuchun Peng, Fangfang Feng, and Andrew Mccallum.
2004. Chinese Segmentation and New Word Detec-
tion using Conditional Random Fields. In Proceed-
ings of COLING. pages 562–571.

192



Yanjun Qi, Sujatha G. Das, Ronan Collobert, and Jason
Weston. 2014. Deep learning for character-based in-
formation extraction. In ECIR.

Weiwei Sun and Jia Xu. 2011. Enhancing chinese word
segmentation using unlabeled data. In EMNLP.

Ben Taskar, Vassil Chatalbashev, Daphne Koller, and
Carlos Guestrin. 2005. Learning structured predic-
tion models: a large margin approach. In ICML.

Jingjing Xu and Xu Sun. 2016. Dependency-based gat-
ed recursive neural network for chinese word seg-
mentation. In ACL.

Nianwen Xue. 2003. Chinese Word Segmentation
as Character Tagging. Computational Linguistics
8(1):29–48.

Shiwen Yu, Jianming Lu, Xuefeng Zhu, Huiming Du-
an, Shiyong Kang, Honglin Sun, Hui Wang, Qiang
Zhao, and Weidong Zhan. 2001. Processing norms
of modern chinese corpus. Technical report .

Longkai Zhang, Li Li, Zhengyan He, Houfeng Wang,
and Ni Sun. 2013a. Improving chinese word seg-
mentation on micro-blog using rich punctuations. In
ACL.

Longkai Zhang, Houfeng Wang, Xu Sun, and Mairgup
Mansur. 2013b. Exploring representations from un-
labeled data with co-training for chinese word seg-
mentation. In EMNLP.

Meishan Zhang, Yue Zhang, Wanxiang Che, and Ting
Liu. 2014. Type-supervised domain adaptation for
joint segmentation and pos-tagging. In EACL.

Meishan Zhang, Yue Zhang, and Guohong Fu. 2016.
Transition-based neural word segmentation. In A-
CL.

Hai Zhao and Chunyu Kit. 2008. An empirical com-
parison of goodness measures for unsupervised chi-
nese word segmentation with a unified framework.
In IJCNLP.

Hongmei Zhao and Qiu Liu. 2010. The cips-sighan
clp2010 chinese word segmentation backoff.

Xiaoqing Zheng, Hanyang Chen, and Tianyu Xu. 2013.
Deep learning for Chinese word segmentation and
POS tagging. In Proceedings of the 2013 Confer-
ence on Empirical Methods in Natural Language
Processing. Association for Computational Linguis-
tics, Seattle, Washington, USA, pages 647–657.

193


