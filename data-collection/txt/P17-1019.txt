



















































Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics


Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 199–208
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1019

Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 199–208
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1019

Generating Natural Answers by Incorporating
Copying and Retrieving Mechanisms in Sequence-to-Sequence Learning

Shizhu He1, Cao Liu1,2, Kang Liu1 and Jun Zhao1,2
1 National Laboratory of Pattern Recognition, Institute of Automation,

Chinese Academy of Sciences, Beijing, 100190, China
2 University of Chinese Academy of Sciences, Beijing, 100049, China

{shizhu.he, cao.liu, kliu, jzhao}@nlpr.ia.ac.cn

Abstract

Generating answer with natural language
sentence is very important in real-world
question answering systems, which need-
s to obtain a right answer as well as
a coherent natural response. In this
paper, we propose an end-to-end ques-
tion answering system called COREQA in
sequence-to-sequence learning, which in-
corporates copying and retrieving mech-
anisms to generate natural answers with-
in an encoder-decoder framework. Specif-
ically, in COREQA, the semantic units
(words, phrases and entities) in a natural
answer are dynamically predicted from the
vocabulary, copied from the given ques-
tion and/or retrieved from the correspond-
ing knowledge base jointly. Our em-
pirical study on both synthetic and real-
world datasets demonstrates the efficien-
cy of COREQA, which is able to generate
correct, coherent and natural answers for
knowledge inquired questions.

1 Introduction

Question answering (QA) systems devote to pro-
viding exact answers, often in the form of phrases
and entities for natural language questions (Wood-
s, 1977; Ferrucci et al., 2010; Lopez et al., 2011;
Yih et al., 2015), which mainly focus on analyzing
questions, retrieving related facts from text snip-
pets or knowledge bases (KBs), and finally pre-
dicting the answering semantic units-SU (word-
s, phrases and entities) through ranking (Yao and
Van Durme, 2014) and reasoning (Kwok et al.,
2001).

However, in real-world environments, most
people prefer the correct answer replied with a
more natural way. For example, most existing

<ÀîÁ¬½Ü£¬³öÉúµØµã£¬±±¾©>

<ÀîÁ¬½Ü£¬¹ú¼®£¬ÐÂ¼ÓÆÂ>

<ÀîÁ¬½Ü£¬³öÉúÄêÔÂ£¬1963Äê4ÔÂ26ÈÕ>

...

ÀîÁ¬½ÜÊÇÄÄÀïÈË£¿ ÀîÁ¬½Ü³öÉúÓÚ±±¾©£¬Ëû

<ÀîÁ¬½Ü£¬ÐÔ±ð£¬ÄÐ>

ÏÖÔÚÊÇÐÂ¼ÓÆÂ¹ú¼®¡£

Copy

Reasoning

From Question

From KB

Question Response

Jet Liwhere was Jet Li was born in Beijing. He is now a Singaporean citizen.

Copying and Retrieving

Predicting

Copying from Question

Retrieving from KB

Question Natural Answer

Do you know from ?

Knowledge
Base

Figure 1: Incorporating copying and retrieving
mechanisms in generating a natural answer.

commercial products such as Siri1 will reply a nat-
ural answer “Jet Li is 1.64m in height.” for the
question “How tall is Jet Li?”, rather than only
answering one entity “1.64m”. Basic on this ob-
servation, we define the “natural answer” as the
natural response in our daily communication for
replying factual questions, which is usually ex-
pressed in a complete/partial natural language sen-
tence rather than a single entity/phrase. In this
case, the system needs to not only parse question,
retrieve relevant facts from KB but also generate a
proper reply. To this end, most previous approach-
es employed message-response patterns. Figure 1
schematically illustrates the major steps and fea-
tures in this process. The system first needs to rec-
ognize the topic entity “Jet Li” in the question and
then extract multiple related facts <Jet Li, gender,
Male>, <Jet Li, birthplace, Beijing> and <Jet
Li, nationality, Singapore> from KB. Based on
the chosen facts and the commonly used message-
response patterns “where was %entity from?” -
“%entity was born in %birthplace, %pronoun is
%nationality citizen.”2, the system could finally
generate the natural answer (McTear et al., 2016).

In order to generate natural answers, typical

1http://www.apple.com/ios/siri/
2In this pattern, %entity indicates the placeholder of the

topic entity, %property indicates the property value of the
topic entity.

199

https://doi.org/10.18653/v1/P17-1019
https://doi.org/10.18653/v1/P17-1019


products need lots of Natural Language Process-
ing (NLP) tools and pattern engineering (McTear
et al., 2016), which not only suffers from high
costs of manual annotations for training data and
patterns, but also have low coverage that cannot
flexibly deal with variable linguistic phenomena
in different domains. Therefore, this paper de-
votes to develop an end-to-end paradigm that gen-
erates natural answers without any NLP tools (e.g.
POS tagging, parsing, etc.) and pattern engineer-
ing. This paradigm tries to consider question an-
swering in an end-to-end framework. In this way,
the complicated QA process, including analyz-
ing question, retrieving relevant facts from KB,
and generating correct, coherent, natural answer-
s, could be resolved jointly.

Nevertheless, generating natural answers in an
end-to-end manner is not an easy task. The key
challenge is that the words in a natural answer may
be generated by different ways, including: 1) the
common words usually are predicted using a (con-
ditional) language model (e.g. “born” in Figure 1);
2) the major entities/phrases are selected from the
source question (e.g. “Jet Li”); 3) the answering
entities/phrases are retrieved from the correspond-
ing KB (e.g. “Beijing”). In addition, some words
or phrases even need to be inferred from related
knowledge (e.g. “He” should be inferred from the
value of “gender”). And we even need to deal with
some morphological variants (e.g. “Singapore” in
KB but “Singaporean” in answer). Although ex-
isting end-to-end models for KB-based question
answering, such as GenQA (Yin et al., 2016), were
able to retrieve facts from KBs with neural mod-
els. Unfortunately, they cannot copy SUs from
the question in generating answers. Moreover,
they could not deal with complex questions which
need to utilize multiple facts. In addition, exist-
ing approaches for conversational (Dialogue) sys-
tems are able to generate natural utterances (Ser-
ban et al., 2016; Li et al., 2016) in sequence-to-
sequence learning (Seq2Seq). But they cannot in-
teract with KB and answer information-inquired
questions. For example, CopyNet (Gu et al., 2016)
is able to copy words from the original source in
generating the target through incorporating copy-
ing mechanism in conventional Seq2Seq learning,
but they cannot retrieve SUs from external memo-
ry (e.g. KBs, Texts, etc.).

Therefore, facing the above challenges, this pa-
per proposes a neural generative model called

COREQA with Seq2Seq learning, which is able to
reply an answer in a natural way for a given ques-
tion. Specifically, we incorporate COpying and
REtrieving mechanisms within Seq2Seq learning.
COREQA is able to analyze the question, retrieve
relevant facts and generate a sequence of SUs us-
ing a hybrid method with a completely end-to-end
learning framework. We conduct experiments on
both synthetic data sets and real-world datasets,
and the experimental results demonstrate the effi-
ciency of COREQA compared with existing end-
to-end QA/Dialogue methods.

In brief, our main contributions are as follows:

• We propose a new and practical question an-
swering task which devotes to generating nat-
ural answers for information inquired ques-
tions. It can be regarded as a fusion task of
QA and Dialogue.

• We propose a neural network based model,
named as COREQA, by incorporating copy-
ing and retrieving mechanism in Seq2Seq
learning. In our knowledge, it is the first
end-to-end model that could answer complex
questions in a natural way.

• We implement experiments on both synthet-
ic and real-world datasets. The experimental
results demonstrate that the proposed model
could be more effective for generating cor-
rect, coherent and natural answers for knowl-
edge inquired questions compared with exist-
ing approaches.

2 Background: Neural Models for
Sequence-to-Sequence Learning

2.1 RNN Encoder-Decoder

Recurrent Neural Network (RNN) based Encoder-
Decoder is the backbone of Seq2Seq learn-
ing (Cho et al., 2014). In the Encoder-Decoder
framework, an encoding RNN first transform a
source sequential object X = [x1, ..., xLX ] into
an encoded representation c. For example, we can
utilize the basic model: ht = f(xt,ht−1); c =
φ(h1, ...,hLX ), where {ht} are the RNN hidden
states, c is the context vector which could be as-
sumed as an abstract representation of X . In prac-
tice, gated RNN variants such as LSTM (Hochre-
iter and Schmidhuber, 1997) and GRU (Chung
et al., 2014) are commonly used for learning long-
term dependencies. And the another encoding

200



Do you know where was Jet_Li from ?

𝒉1 𝒉2 𝒉3 𝒉4 𝒉5 𝒉6 𝒉7 𝒉8
Subject Property Object

Jet_Li gender Male

Jet_Li birthplace Beijing

Jet_Li nationality Singapore

Jet_Li birthdate 26 April 1963

… … …

Attentive Read 
from Question

Attentive Read 
from KB Copying 

from Question
Retrieving 
from KB

𝒇1

𝒇2

𝒇3

𝒇4

𝒇…

𝒒

𝑠1 𝑠2 𝑠3 𝑠4 𝑠5

<eos> Jet_Li was born in

Jet_Li was born in Beijing

𝑠5

… … …
Softmax

𝑃 Beijing = 𝑃𝑝𝑟(Beijing) + 𝑃𝑐𝑜(Beijing) + 𝑃re(Beijing)

(a) Knowledge (facts) Retrieval

(c) Decoder: Natural Answer Generation

(d) Predicting, Copying  (from Question) 
and Retrieving (from KB)

DNN DNN

KB PositionQuestion PositionVocabulary

DNN
Question context

KB context

Question Copying 
History

KB Retrieving 
History

(e) State Update

“in” embedding

Copying “in” 
from Question

Retrieving “in” 
from KB

(b) Encoder: Question and KB Representation

Figure 2: The overall diagram of COREQA.

tricks is Bi-directional RNN, which connect two
hidden states of positive time direction and neg-
ative time direction. Once the source sequence
is encoded, another decoding RNN model is to
generate a target sequence Y = [y1, ..., yLY ],
through the following prediction model: st =
f(yt−1, st−1, c); p(yt|y<t, X) = g(yt−1, st, c),
where st is the RNN hidden state at time t, the
predicted target word yt at time t is typically per-
formed by a softmax classifier over a settled vo-
cabulary (e.g. 30,000 words) through function g.

2.2 The Attention Mechanism

The prediction model of classical decoders for
each target word yi share the same context vec-
tor c. However, a fixed vector is not enough to ob-
tain a better result on generating a long targets.The
attention mechanism in the decoding can dynam-
ically choose context ct at each time step (Bah-
danau et al., 2014), for example, representing ct
as the weighted sum of the source states {ht},

ct =
∑LX

i=1
αtihi; αti =

eρ(st−1,hi)∑
i′ e

ρ(st−1,h′i)
(1)

where the function ρ use to compute the atten-
tive strength with each source state, which usually
adopts a neural network such as multi-layer per-
ceptron (MLP).

2.3 The Copying Mechanism

Seq2Seq learning heavily rely on the “meaning”
for each word in source and target sequences, how-
ever, some words in sequences are “no-meaning”
symbols and it is improper to encode them in en-
coding and decoding processes. For example, gen-
erating the response “Of course, read” for reply-
ing the message “Can you read the word ‘read’?”
should not consider the meaning of the second
“read”. By incorporating the copying mechanism,
the decoder could directly copy the sub-sequences
of source into the target (Vinyals et al., 2015). The
basic approach is to jointly predict the indexes
of the target word in the fixed vocabulary and/or
matched positions in the source sequences (Gu
et al., 2016; Gulcehre et al., 2016).

3 COREQA

To generate natural answers for information in-
quired questions, we should first recognize key
topics in the question, then extract related facts
from KB, and finally fusion those instance-level
knowledge with some global-level “smooth” and
“glue” words to generate a coherent reply. In this
section, we present COREQA, a differentiable Se-
q2Seq model to generate natural answers, which is
able to analyze the question, retrieve relevant fact-
s and predict SUs in an end-to-end fashion, and
the predicted SUs may be predicted from the vo-

201



cabulary, copied from the given question, and/or
retrieved from the corresponding KB.

3.1 Model Overview

As illustrated in Figure 2, COREQA is an encoder-
decoder framework plugged with a KB engineer.
A knowledge retrieval module is firstly employed
to retrieve related facts from KB by question anal-
ysis (see Section 3.2). And then the input question
and the retrieved facts are transformed into the cor-
responding representations by Encoders (see Sec-
tion 3.3). Finally, the encoded representations are
feed to Decoder for generating the target natural
answer (see Section 3.4).

3.2 Knowledge (facts) Retrieval

We mainly focus on answering the information in-
quired questions (factual questions, and each ques-
tion usually contains one or more topic entities).
This paper utilizes the gold topic entities for sim-
plifying our design. Given the topic entities, we
retrieve the related facts from the corresponding
KB. KB consists of many relational data, which
usually are sets of inter-linked subject-property-
object (SPO) triple statements. Usually, question
contains the information used to match the subject
and property parts in a fact triple, and answer in-
corporates the object part information.

3.3 Encoder

The encoder transforms all discrete input symbol-
s (including words, entities, properties and prop-
erties’ values) and their structures into numerical
representations which are able to feed into neural
models (Weston et al., 2014).

3.3.1 Question Encoding

Following (Gu et al., 2016), a bi-directional RN-
N (Schuster and Paliwal, 1997) is used to trans-
form the question sequence into a sequence of
concatenated hidden states with two independent
RNNs. The forward and backward RNN respec-
tively obtain {−→h 1, ...,

−→
h LX} and {

←−
h LX , ...,

←−
h 1}.

The concatenated representation is considered to
be the short-term memory of question (MQ =
{ht},ht = [

−→
h t,
←−
h LX−t+1]). q = [

−→
h LX ,

←−
h 1] is

used to represent the entire question, which could
be used to compute the similarity between the
question and the retrieved facts.

3.3.2 Knowledge Base Encoding
We use s, p and o denote the subject, property and
object (value) of one fact f, and es, ep and eo to de-
note its corresponding embeddings. The fact rep-
resentation f is then defined as the concatenation
of es, ep and eo. The list of all related facts’ repre-
sentations, {f} = {f1, ..., fLF } (refer to MKB , LF
denotes the maximum of candidate facts), is con-
sidered to be a short-term memory of KB while
answering questions about the topic entities.

In addition, given the distributed representation
of question and candidate facts, we define the
matching scores function between question and
facts as S(q, fj) = DNN1(q, fj) = tanh(W2 ·
tanh(W1 ·[q, fj ]+b1)+b2), , whereDNN1 is the
matching function defined by a two-layer percep-
tron, [·, ·] denotes vector concatenation, and W1,
W2, b1 and b2 are the learning parameters. In fac-
t, we will make a slight change of the matching
function because it will also depend on the state of
decoding process at different times. The modified
function is S(q, st, fj) = DNN1(q, st, fj) where
st is the hidden state of decoder at time t.

3.4 Decoder
The decoder uses an RNN to generate a natural
answer based on the short-term memory of ques-
tion and retrieved facts which represented as MQ
and MKB , respectively. The decoding process
of COREQA have the following differences com-
pared with the conventional decoder:
Answer words prediction: COREQA predicts
SUs based on a mixed probabilistic model of three
modes, namely the predict-mode, the copy-mode
and the retrieve-mode, where the first mode pre-
dicts words with the vocabulary, and the two latter
modes pick SUs from the questions and matched
facts, respectively;
State update: the predicted word at step t − 1 is
used to update st, but COREQA uses not only its
word embedding but also its corresponding posi-
tional attention informations in MQ and MKB ;
Reading short-Memory MQ and MKB: MQ and
MKB are fed into COREQA with two ways, the
first one is the “meaning” with embeddings and
the second one is the positions of different words
(properties’ values).

3.4.1 Answer Words Prediction
The generated words (entities) may come from vo-
cabulary, source question and matched KB. Ac-
cordingly, our model use three correlative output

202



layer: shortlist prediction layer, question location
copying layer and candidate-facts location retriev-
ing layer, respectively. And we use the softmax
classifier of the above three cascaded output lay-
ers to pick SUs. We assume a vocabulary V =
{v1, ..., vN} ∪ {UNK}, where UNK indicates any
out-of-vocabulary (OOV) words. Therefore, we
have adopted another two set of SUs XQ and XKB
which cover words/entities in the source question
and the partial KB. That is, we have adopted the
instance-specific vocabulary V ∪ XQ ∪ XKB for
each question. It’s important to note that these
three vocabularies V , XQ and XKB may overlap.

At each time step t in the decoding process, giv-
en the RNN state st together with MQ and MKB ,
the probabilistic function for generating any target
SU yt is a “mixture” model as follow

p(yt|st, yt−1,MQ,MKB) =
ppr(yt|st, yt−1, ct) · pm(pr|st, yt−1)+

pco(yt|st, yt−1,MQ) · pm(co|st, yt−1)+
pre(yt|st, yt−1,MKB) · pm(re|st, yt−1)

(2)

where pr, co and re stand for the predict-mode,
the copy-mode and the retrieve-mode, respective-
ly, pm(·|·) indicates the probability model for
choosing different modes (we use a softmax
classifier with two-layer MLP). The probability of
the three modes are given by

ppr(yt|·) =
1

Z
eψpr(yt)

pco(yt|·) =
1

Z

∑
j:Qj=yt

eψco(yt)

pre(yt|·) =
1

Z

∑
j:KBj=yt

eψre(yt)

(3)

where ψpr(·), ψco(·) and ψre(·) are score func-
tions for choosing SUs in predict-mode (from V),
copy-mode (from XQ) and retrieve-mode (from
XKB), respectively. And Z is the normaliza-
tion term shared by the three modes, Z =
eψpr(v) +

∑
j:Qj=v

eψco(v) +
∑

j:KBj=v
eψre(v).

And the three modes could compete with each oth-
er through a softmax function in generating tar-
get SUs with the shared normalization term (as
shown in Figure 2. Specifically, the scoring func-
tions of each mode are defined as follows:
Predict-mode: Some generated words need rea-
soning (e.g. “He” in Figure 1) and morphological
transformation (e.g. “Singaporean” in Figure 1).
Therefore, we modify the function as ψpr(yt =
vi) = vTi Wpr[st, cqt , ckbt ] , where vi ∈ Rdo is the

word vector at the output layer (not the input word
embedding), Wpr ∈ R(dh+di+df )×do (di, dh and
df indicate the size of input word vector, RNN de-
coder hidden state and fact representation respec-
tively), and cqt and ckbt are the temporary mem-
ory of reading MQ and MKB at time t (see Sec-
tion 3.4.3).
Copy-mode: The score for “copying” the word
xj from question Q is calculated as ψco(yt =
xj) = DNN2(hj , st,histQ) , where DNN2 is
a neural network function with a two-layer MLP
and histQ ∈ RLX is an accumulated vector which
record the attentive history for each word in ques-
tion (similar with the coverage vector in (Tu et al.,
2016)).
Retrieve-mode: The score for “retrieving” the
entity word vj from retrieval facts (“Objec-
t” part) is calculated as ψre(yt = vj) =
DNN3(fj , st,histKB) , where DNN3 is also a
neural network function and histKB ∈ RLF is an
accumulated vector which record the attentive his-
tory for each fact in candidate facts.

3.4.2 State Update

In the generic decoding process, each RNN hid-
den state st is updated with the previous state
st−1, the word embedding of previous predict-
ed symbol yt−1, and an optional context vector
ct (with attention mechanism). However, yt−1
may not come from vocabulary V and not own-
s a word vector. Therefore, we modify the state
update process in COREQA. More specifically,
yt−1 will be represented as concatenated vector of
[e(yt−1), rqt−1 , rkbt−1 ], where e(yt−1) is the word
embedding associated with yt−1, rqt−1 and rkbt−1
are the weighted sum of hidden states in MQ and
MKB corresponding to yt−1 respectively.

rqt =
∑LX

j=1
ρtjhj , rkbt =

∑LF
j=1

δtjfj

ρtj =





1

K1
pco(xj |·), xj = yt

0 otherwise

δtj =





1

K2
pre(fj |·), object(fj) = yt

0 otherwise

(4)

where object(f) indicate the “object” part of fac-
t f (see Figure 2), and K1 and K2 are the nor-
malization terms which equal

∑
j′:x′j=yt

pco(x
′
j |·)

and
∑

j′:object(f ′j)=yt
pre(f

′
j |·), respectively, and it

203



could consider the multiple positions matching yt
in source question and KB.

3.4.3 Reading short-Memory MQ and MKB
COREQA employ the attention mechanism at de-
coding process. At each decoder time t, we se-
lective read the context vector cqt and ckbt from
the short-term memory of question MQ and re-
trieval facts MKB (alike to Formula 1). In addi-
tion, the accumulated attentive vectors histQ and
histKB are able to record the positional informa-
tion of SUs in the source question and retrieved
facts.

3.5 Training

Although some target SUs in answer are copied
and retrieved from the source question and the ex-
ternal KB respectively, COREQA is fully differen-
tial and can be optimized in an end-to-end manner
using back-propagation. Given the batches of the
source questions {X}M and target answers {Y }M
both expressed with natural language (symbolic
sequences), the objective function is to minimize
the negative log-likelihood:

L = − 1
N

M∑

k=1

LY∑

t=1

log[p(y
(k)
t |y

(k)
<t , X

(k)] (5)

where the superscript (k) indicates the index of
one question-answer (Q-A) pair. The network is
no need for any additional labels for training mod-
els, because the three modes sharing the same
softmax classifier for predicting target words,
they can learn to coordinate with each other by
maximizing the likelihood of observed Q-A pairs.

4 Experiments

In this section, we present our main experimental
results in two datasets. The first one is a small syn-
thetic dataset in a restricted domain (only involv-
ing four properties of persons) (Section 4.1). The
second one is a big dataset in open domain, where
the Q-A pairs are extracted from community QA
website and grounded against a KB with an Integer
Linear Programming (ILP) method (Section 4.2).
COREQA and all baseline models are trained on a
NVIDIA TITAN X GPU using TensorFlow3 tools,
where we used the Adam (Kingma and Ba, 2014)
learning rule to update gradients in all experimen-
tal configures. The sources codes and data will be

3https://www.tensorflow.org/

released at the personal homepage of the first au-
thor4.

4.1 Natural QA in Restricted Domain

Task: The QA systems need to answer question-
s involving 4 concrete properties of birthdate (in-
cluding year, month and day) and gender).
Through merely involving 4 properties, there are
plenty of QA patterns which focus on different as-
pects of birthdate, for example, “What year were
you born?” touches on “year”, but “When is your
birthday?” touches on “month and day”.
Dataset: Firstly, 108 different Q-A pattern-
s have been constructed by two annotators, one
in charge of raising question patterns and an-
other one is responsible for generating corre-
sponding suitable answer patterns, e.g. When
is %e birthday? → She was born in
%m %dth. where the variables %e, %y, %m,
%d and %g (deciding she or he) indicates the per-
son’s name, birth year, birth month, birth day and
gender, respectively. Then we randomly generate
a KB which contains 80,000 person entities, and
each entity including four facts. Given KB fact-
s, we can finally obtain specific Q-A pairs. And
the sampling KB, patterns, and the generated Q-
A pairs are shown in Table 1. In order to main-
tain the diversity, we randomly select 6 patterns
for each person. Finally, we totally obtain 239,934
sequences pairs (half patterns may be unmatched
because of “gender” property).

Q-A Patterns
Examples (e.g. KB facts

(e2,year,1987);(e2,month,6);
(e2,day,20);(e2,gender,male))

When is %e birthday? When is e2 birthday?
He was born in %m %dth. He was born in June 20th.
What year were %e born? What year were e2 born?
%e is born in %y year. e2 is born in 1987 year.

Table 1: Sample KB facts, patterns and their gen-
erated Q-A pairs.

Experimental Setting: The total 239,934 Q-A
pairs are split into training (90%) and testing set
(10%). The baseline includes 1) generic RNN
Encoder-Decoder (marked as RNN), 2) Seq2Seq
with attention (marked as RNN+atten), 3) Copy-
Net, and 4) GenQA. For a fair comparison, we use
bi-directional LSTM for encoder and another LST-
M for decoder for all Seq2Seq models, with hid-
den layer size = 600 and word embedding dimen-

4http://www.nlpr.ia.ac.cn/cip/shizhuhe/publications.html

204



sion = 200. We set LF as 5.
Metrics: We adopt (automatic evaluation (AE)
to test the effects of different models. AE consid-
ers the precisions of the entire predicted answer-
s and four specific properties, and the answer is
complete correct only when all predicted proper-
ties’ values is right. To measure the performance
of the proposed method, we select following met-
rics, including Pg5, Py, Pm and Pd which denote
the precisions for ‘gender’, ‘year’, ‘month’ and
‘day’ properties, respectively. And PA, RA and
F1A indicate the precision, recall and F1 in the
complete way.
Experimental Results: The AE experimental re-
sults are shown in Table 2. It is very clear from Ta-
ble 2 that COREQA significantly outperforms all
other compared methods. The reason of the Gen-
QA’s poor performance is that all synthetic ques-
tions need multiple facts, and GenQA will “safe-
ly” choose the most frequent property (“gender”)
for all questions. We also found the performances
on “year” and “day” have a little worse than other
properties such as “gender”, it may because there
have more ways to answer questions about “year”
and “day”.

Models Pg Py Pm Pd PA RA F1A
RNN 72.2 0 1.1 0.2 0 27.5 0

RNN+atten 55.8 1.1 11.3 9.5 1.7 34 3.2
CopyNet 75.2 8.7 28.3 5.8 3.7 32.5 6.7
GenQA 73.4 0 0 0 0 27.1 0

COREQA 100 84.8 93.4 81 87.4 94 90.6

Table 2: The AE results (%) on synthetic test data.

Discussion: Because of the feature of directly
“hard” copy and retrieve SUs from question and
KB, COREQA could answer questions about un-
seen entities.To evaluate the effects of answering
questions about unseen entities, we re-construct
2,000 new person entities and their correspond-
ing facts about four known properties, and obtain
6,081 Q-A pairs through matching the sampling
patterns mentioned above. The experimental re-
sults are shown in Table 3, it can be seen that the
performance did not fall too much.

Entities Pg Py Pm Pd PA RA F1A
Seen 100 84.8 93.4 81 87.4 94 90.6

Unseen 75.1 84.5 93.5 81.2 63.8 85.1 73.1

Table 3: The AE (%) for seen and unseen entities.

5The “gender” is right when the entity name (e.g. ‘e2’) or
the personal pronoun (e.g. ‘She’) in answer is correct.

4.2 Natural QA in Open Domain

Task: To test the performance of the proposed
approach in open domains, we modify the task of
GenQA (Yin et al., 2016) for supporting multi-
facts (a typical example is shown in Figure 1).
That is, a natural QA system should generate a
sequence of SUs as the natural answer for a giv-
en natural language question through interacting
with a KB.
Dataset: GenQA have released a corpus6, which
contains a crawling KB and a set of ground Q-
A pairs. However, the original Q-A pairs on-
ly matched with just one single fact. In fac-
t, we found that a lot of questions need more
than one fact (about 20% based on sampling in-
spection). Therefore, we crawl more Q-A pairs
from Chinese community QA website (Baidu Zhi-
dao7). Combined with the originally published
corpus, we create a lager and better-quality data
for natural question answering. Specifically, an
Integral Linear Programming (ILP) based method
is employed to automatically construct “ground-
ing” Q-A pairs with the facts in KB (inspired
by the work of adopting ILP to parse question-
s (Yahya et al., 2012)). In ILP, the main con-
straints and considered factors are listed below:
1) the “subject” entity and “object” enti-
ty of a triple have to match with question word-
s/phrases (marked as subject mention) and answer
words/phrases (marked as object mention) respec-
tively; 2) any two subject mentions or object men-
tions should not overlap; 3) a mention can match
at most one entity; 4) the edit distance be-
tween the Q-A pair and the matched candidate
fact (use a space to joint three parts) is smaller,
they are more relevant. Finally, we totally obtain
619,199 instances (an instance contains a ques-
tion, an answer, and multiple facts), and the num-
ber of instances that can match one and multiple
facts in KB are 499,809 and 119,390, respectively.
Through the evaluation of 200 sampling instances,
we estimate that approximate 81% matched facts
are helpful for the generating answers. However,
strictly speaking, only 44% instances are truly cor-
rect grounding. In fact, grounding the Q-A pairs
from community QA website is a very challenge
problem, we will leave it in the future work.
Experimental Setting: The dataset is split into
training (90%) and testing set (10%). The sen-

6https://github.com/jxfeb/Generative QA
7https://zhidao.baidu.com/

205



tences in Chinese are segmented into word se-
quences with Jieba8 tool. And we use the word-
s with the frequency larger than 3, which cover-
ing 98.4% of the word in the corpus. For a fair
comparison, we use bi-directional LSTM for the
encoder and another LSTM for decoder for al-
l Seq2Seq models, with hidden layer size = 1024
and word embedding dimension = 300. We selec-
t CopyNet (more advanced Seq2Seq model) and
GenQA for comparison. We set LF as 10.
Metrics: Besides adopting the AE as a met-
ric (same as GenQA (Yin et al., 2016)), we ad-
ditionally use manual evaluation (ME) as anoth-
er metric. ME considers three aspects about the
quality of the generated answer (refer to (Asghar
et al., 2016)): 1) correctness; 2) syntactical flu-
ency; 3) coherence with the question. We employ
two annotators to rate such three aspects of Copy-
Net, GenQA and COREQA. Specifically, we sam-
ple 100 questions, and conduct C23 = 3 pair-wise
comparisons for each question and count the win-
ning times of each model (comparisons may both
win or both lose).
Experimental Results: The AE and ME result-
s are shown in Table 4 and Table 5, respectively.
Meanwhile, we separately present the results ac-
cording to the number of the facts which a ques-
tion needs in KB, including just one single fac-
t (marked as Single), multiple facts (marked as
Multi) and all (marked as Mixed). In fact, we
train two separate models for Single and Multi
questions for the unbalanced data . From Table 4
and Table 5, we can clearly observe that CORE-
QA significantly outperforms all other baseline
models. And COREQA could generate a bet-
ter natural answer in three aspects: correctness,
fluency and coherence. CopyNet cannot interac-
t with KB which is important to generate correc-
t answers. For example, for “Who is the direc-
tor of The Little Chinese Seamstress?”, if without
the fact (The Little Chinese Seamstress, director,
Dai Siji), QA systems cannot generate a correct
answer.

Models Single Multi Mixed
CopyNet 9.7 0.8 8.7
GenQA 47.2 28.9 45.1

COREQA 58.4 42.7 56.6

Table 4: The AE accuracies (%) on real world test
data.

8https://github.com/fxsjy/jieba

Models Correctness Fluency Coherence
CopyNet 0 13.3 3.3
GenQA 26.7 33.3 20

COREQA 46.7 50 60

Table 5: The ME results (%) on sampled mixed
test data.

Case Study and Error Analysis: Table 6 gives
some examples of generated by COREQA and the
gold answers to the questions in test set. It is very
clearly seen that the parts of generating SUs are
predicted from the vocabulary, and other SUs are
copied from the given question (marked as bold)
and retrieved from the KB (marked as underline).
And we analyze sampled examples and believe
that there are several major causes of errors: 1)
did not match the right facts (ID 6); 2) the gener-
ated answers contain some repetition of meaning-
less words (ID 7); 3) the generated answers are not
coherence natural language sentences (ID 8).

5 Related Work

Seq2Seq learning is to maximize the likelihood
of predicting the target sequence Y conditioned
on the observed source sequence X (Sutskever
et al., 2014), which has been applied success-
fully to a large number of NLP tasks such as
Machine Translation (Wu et al., 2016) and Dia-
logue (Vinyals and Le, 2015). Our work is par-
tially inspired by the recent work of QA and
Dialogue which have adopted Seq2Seq learning.
CopyNet (Gu et al., 2016) and Pointer Network-
s (Vinyals et al., 2015; Gulcehre et al., 2016)
which could incorporate copying mechanism in
conventional Seq2Seq learning. Different from
our application which deals with knowledge in-
quired questions and generates natural answers,
CopyNet (Gu et al., 2016) and Pointer Network-
s (Gulcehre et al., 2016) can only copy words from
the original input sequence. In contrast, COREQA
is able to retrieve SUs from external memory. And
GenQA (Yin et al., 2016) can only deal with the
simple questions which could be answered by one
fact, and it also did not incorporate the copying
mechanism in Seq2Seq learning.

Moreover, our work is also inspired by Neural
Abstract Machine (Graves et al., 2016; Yin et al.,
2015; Liang et al., 2016) which could retrieve facts
from KBs with neural models. Unlike natural an-
swer, Neural Abstract Machine (Mou et al., 2016)
is concentrating on obtaining concrete answer en-

206



ID Question Gold Answer Generated Natural Answer

1
~;�MpÅd´=I<º =I<\<Ñ) \<<§=I<

Which country did Hargreaves of Bayern comes from? British! born in Canada Canadians, British

2
âyx´=x�º Çxy3´éÜIBÖ éÜIBÖ§´Çx

Which ethnic groups is Sha Zukang from? Han, now he is the DSG of the UN DSG of the UN, Han

3
ÛËA#Óù´Xº ´[!ÑW[ ÛÛÛËËËAAA###ÓÓÓùùù[óÆ[

Who is Robert Schumann? a writer, musician Robert Schuhmann is a writer and philosopher.

4
êd.ì´Xº ¦´¶v¥$Ä
 `D�v¥$Ä


Who is Mascherano? He is a football player An excellent football player

5
n�S�à¿��ü´Xº g# ���üüü´g#

Who is the director of The Little Chinese Seamstress? Dai Sijie Director Dai Sijie

6
>Kcn�´Xû�º M!�*À!Új¸ ¾�f

Who shot the movie The Iron Triangle? Tsui Hark, Johnny To, Ringo Lam Feng Xiaogang

7
X�R6Iù<�
[]�º å< ´´´

Who knows some details of Xi Murong? poetess yes, yes, yes

8
��,´=úimu�º AT´þ° þ°

Which company developed the game Crazy Arcade? should be the Shanda Group playing Shanda Group

Table 6: Examples of the generated natural answers by COREQA.

tities with neural network based reasoning.

6 Conclusion and Future Work

In this paper, we propose an end-to-end system
to generate natural answers through incorporating
copying and retrieving mechanisms in sequence-
to-sequence learning. Specifically, the sequences
of SUs in the generated answer may be predict-
ed from the vocabulary, copied from the given
question and retrieved from the corresponding K-
B. And the future work includes: a) lots of ques-
tions cannot be answered directly by facts in a KB
(e.g. “Who is Jet Li’s father-in-law?”), we plan to
learn QA system with latent knowledge (e.g. K-
B embedding (Bordes et al., 2013)); b) we plan to
adopt memory networks (Sukhbaatar et al., 2015)
to encode the temporary KB for each question.

Acknowledgments

The authors are grateful to anonymous review-
ers for their constructive comments. The work
was supported by the Natural Science Foundation
of China (No.61533018) and the National High
Technology Development 863 Program of China
(No.2015AA015405).

References

Nabiha Asghar, Pascal Poupart, Jiang Xin, and Hang
Li. 2016. Online sequence-to-sequence reinforce-
ment learning for open-domain conversational a-
gents. arXiv preprint arXiv:1612.03929 .

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
learning to align and translate. arXiv preprint arX-
iv:1409.0473 .

Antoine Bordes, Nicolas Usunier, Alberto Garcia-
Duran, Jason Weston, and Oksana Yakhnenko.
2013. Translating embeddings for modeling multi-
relational data. In Advances in neural information
processing systems. pages 2787–2795.

Kyunghyun Cho, Bart Van Merriënboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using rnn encoder-decoder
for statistical machine translation. arXiv preprint
arXiv:1406.1078 .

Junyoung Chung, Caglar Gulcehre, KyungHyun Cho,
and Yoshua Bengio. 2014. Empirical evaluation of
gated recurrent neural networks on sequence model-
ing. arXiv preprint arXiv:1412.3555 .

David Ferrucci, Eric Brown, Jennifer Chu-Carroll,
James Fan, David Gondek, Aditya A Kalyanpur,
Adam Lally, J William Murdock, Eric Nyberg, John
Prager, et al. 2010. Building watson: An overview
of the deepqa project. AI magazine 31(3):59–79.

Alex Graves, Greg Wayne, Malcolm Reynolds,
Tim Harley, Ivo Danihelka, Agnieszka Grabska-
Barwińska, Sergio Gómez Colmenarejo, Edward
Grefenstette, Tiago Ramalho, John Agapiou, et al.
2016. Hybrid computing using a neural net-
work with dynamic external memory. Nature
538(7626):471–476.

Jiatao Gu, Zhengdong Lu, Hang Li, and O.K. Vic-
tor Li. 2016. Incorporating copying mechanism
in sequence-to-sequence learning. In Proceedings
of the 54th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Paper-
s). Association for Computational Linguistics, pages
1631–1640.

Caglar Gulcehre, Sungjin Ahn, Ramesh Nallapati,
Bowen Zhou, and Yoshua Bengio. 2016. Pointing
the unknown words. In Proceedings of the 54th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers). Association
for Computational Linguistics, pages 140–149.

207



Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation
9(8):1735–1780.

Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980 .

Cody Kwok, Oren Etzioni, and Daniel S Weld. 2001.
Scaling question answering to the web. ACM Trans-
actions on Information Systems (TOIS) 19(3):242–
262.

Jiwei Li, Will Monroe, Alan Ritter, Dan Jurafsky,
Michel Galley, and Jianfeng Gao. 2016. Deep rein-
forcement learning for dialogue generation. In Pro-
ceedings of the 2016 Conference on Empirical Meth-
ods in Natural Language Processing. Association
for Computational Linguistics, pages 1192–1202.

Chen Liang, Jonathan Berant, Quoc Le, Kenneth D
Forbus, and Ni Lao. 2016. Neural symbolic ma-
chines: Learning semantic parsers on freebase with
weak supervision. arXiv preprint arXiv:1611.00020
.

Vanessa Lopez, Victoria Uren, Marta Sabou, and En-
rico Motta. 2011. Is question answering fit for the
semantic web?: a survey. Semantic Web 2(2):125–
155.

Michael McTear, Zoraida Callejas, and David Griol.
2016. The Conversational Interface: Talking to S-
mart Devices. Springer Publishing Company, Incor-
porated, 1st edition.

Lili Mou, Zhengdong Lu, Hang Li, and Zhi Jin.
2016. Coupling distributed and symbolic execution
for natural language queries. arXiv preprint arX-
iv:1612.02741 .

Mike Schuster and Kuldip K Paliwal. 1997. Bidirec-
tional recurrent neural networks. IEEE Transactions
on Signal Processing 45(11):2673–2681.

Iulian V Serban, Alessandro Sordoni, Yoshua Bengio,
Aaron Courville, and Joelle Pineau. 2016. Building
end-to-end dialogue systems using generative hier-
archical neural network models. In Proceedings of
the 30th AAAI Conference on Artificial Intelligence
(AAAI-16).

Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al.
2015. End-to-end memory networks. In Advances
in neural information processing systems. pages
2440–2448.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural network-
s. In Advances in neural information processing sys-
tems. pages 3104–3112.

Zhaopeng Tu, Zhengdong Lu, Yang Liu, Xiaohua Liu,
and Hang Li. 2016. Coverage-based neural machine
translation. arXiv preprint arXiv:1601.04811 .

Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly.
2015. Pointer networks. In Advances in Neural In-
formation Processing Systems. pages 2692–2700.

Oriol Vinyals and Quoc Le. 2015. A neural conversa-
tional model. arXiv preprint arXiv:1506.05869 .

Jason Weston, Sumit Chopra, and Antoine Bordes.
2014. Memory networks. CoRR abs/1410.3916.

William A Woods. 1977. Lunar rocks in natural en-
glish: Explorations in natural language question an-
swering. In Linguistic structures processing. pages
521–569.

Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V
Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus
Macherey, et al. 2016. Google’s neural machine
translation system: Bridging the gap between hu-
man and machine translation. arXiv preprint arX-
iv:1609.08144 .

Mohamed Yahya, Klaus Berberich, Shady Elbas-
suoni, Maya Ramanath, Volker Tresp, and Gerhard
Weikum. 2012. Natural language questions for the
web of data. In Proceedings of the 2012 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning. Association for Computational Linguis-
tics, pages 379–390.

Xuchen Yao and Benjamin Van Durme. 2014. Infor-
mation extraction over structured data: Question an-
swering with freebase. In Proceedings of the 52nd
Annual Meeting of the Association for Computation-
al Linguistics (Volume 1: Long Papers). Association
for Computational Linguistics, pages 956–966.

Wen-tau Yih, Ming-Wei Chang, Xiaodong He, and
Jianfeng Gao. 2015. Semantic parsing via staged
query graph generation: Question answering with
knowledge base. In Proceedings of the 53rd Annual
Meeting of the Association for Computational Lin-
guistics and the 7th International Joint Conference
on Natural Language Processing (Volume 1: Long
Papers). Association for Computational Linguistics,
pages 1321–1331.

Jun Yin, Xin Jiang, Zhengdong Lu, Lifeng Shang,
Hang Li, and Xiaoming Li. 2016. Neural generative
question answering. In Proceedings of the Interna-
tional Joint Conference on Artificial Intelligence (I-
JCAI).

Pengcheng Yin, Zhengdong Lu, Hang Li, and Ben Kao.
2015. Neural enquirer: Learning to query tables.
arXiv preprint arXiv:1512.00965 .

208


	Generating Natural Answers by Incorporating Copying and Retrieving Mechanisms in Sequence-to-Sequence Learning

