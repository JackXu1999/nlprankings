



















































GTR-LSTM: A Triple Encoder for Sentence Generation from RDF Data


Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1627–1637
Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics

1627

GTR-LSTM: A Triple Encoder for Sentence Generation from RDF Data

Bayu Distiawan Trisedya1, Jianzhong Qi1, Rui Zhang1∗, Wei Wang2
1 The University of Melbourne

2 University of New South Wales
btrisedya@student.unimelb.edu.au

{jianzhong.qi,rui.zhang}@unimelb.edu.au
weiw@cse.unsw.edu.au

Abstract

A knowledge base is a large repository of
facts that are mainly represented as RDF
triples, each of which consists of a sub-
ject, a predicate (relationship), and an ob-
ject. The RDF triple representation of-
fers a simple interface for applications to
access the facts. However, this represen-
tation is not in a natural language form,
which is difficult for humans to under-
stand. We address this problem by propos-
ing a system to translate a set of RDF
triples into natural sentences based on an
encoder-decoder framework. To preserve
as much information from RDF triples as
possible, we propose a novel graph-based
triple encoder. The proposed encoder en-
codes not only the elements of the triples
but also the relationships both within a
triple and between the triples. Experi-
mental results show that the proposed en-
coder achieves a consistent improvement
over the baseline models by up to 17.6%,
6.0%, and 16.4% in three common metrics
BLEU, METEOR, and TER, respectively.

1 Introduction

Knowledge bases (KBs) are becoming an en-
abling resource for many applications includ-
ing Q&A systems, recommender systems, and
summarization tools. KBs are designed based
on a W3C standard called the Resource De-
scription Framework (RDF)1. An RDF triple
consists of three elements in the form of
〈subject, predicate (relationship), object〉. It
describes a relationship between an entity (the
subject) and another entity or literal (the object)
∗Corresponding author
1https://www.w3.org/RDF/

RDF
triples

〈John Doe,birth place,London〉
〈John Doe,birth date,1967-01-10〉
〈London,capital of,England〉

Target
sentence

John Doe was born on
1967-01-10 in London,
the capital of England.

Table 1: RDF based sentence generation.

via the predicate. This representation allows easy
data share between KBs. However, usually the el-
ements of a triple are stored as Uniform Resource
Identifiers (URIs), and many predicates (words or
phrases) are not intuitive; this representation is dif-
ficult to comprehend by humans.

Translating RDF triples into natural sentences
helps humans to comprehend the knowledge
embedded in the triples, and building a natural
language based user interface is an important
task in user interaction studies (Damljanovic
et al., 2010). This task has many applications,
such as question answering (Bordes et al.,
2014; Fader et al., 2014), profile summariz-
ing (Lebret et al., 2016; Chisholm et al., 2017),
and automatic weather forecasting (Mei et al.,
2016). For example, the SPARQL inference of
a Q&A system (Unger et al., 2012) returns a
set of RDF triples which need to be translated
into natural sentences to provide a more read-
able answer for the users. Table 1 illustrates
such an example. Suppose a user is asking
a question about “John Doe”. By querying
a KB, a Q&A system retrieves three triples
“〈John Doe,birth place,London〉”,
“〈John Doe,birth date,1967-01-10〉”,
and “〈London,capital of,England〉.”
We aim to generate a natural sentence that
incorporates the information of the triples and
is easier to be understood by the user. In this
example, the generated sentence is “John Doe
was born on 1967-01-10 in London,



1628

the capital of England.”
Most existing studies for this task use domain

specific rules. Bontcheva and Wilks (2004) create
rules to generate sentences in the medical domain,
while Cimiano et al. (2013) create rules to gener-
ate step by step cooking instructions. The prob-
lem of rule-based methods is that they need a lot
of human efforts to create the rules, which mostly
cannot deal with complex or novel cases.

Recent studies propose neural language gener-
ation systems. Lebret et al. (2016) generate the
first sentence of a biography by a conditional neu-
ral language model. Mei et al. (2016) propose
an encoder-aligner-decoder architecture to gener-
ate weather forecasts. The model does not need
predefined rules and hence generalizes better to
open domain data.

A straightforward adaptation of neural language
generation system is to use the encoder-decoder
model by first concatenating the elements of the
RDF triples into a linear sequence and then feed-
ing the sequence as the model input to learn the
corresponding natural sentence. We implemented
such a model (detailed in Section 3.2) that ranked
top in the WebNLG Challenge 20172. This Chal-
lenge has a primary objective of generating syntac-
tically correct natural sentences from a set of RDF
triples. Our model achieves the highest global
scores on the automatic evaluation, outperforming
competitors that use rule-based methods, statisti-
cal machine translation, and neural machine trans-
lation (Gardent et al., 2017b).

While our previous model achieves a good re-
sult, simply concatenating the elements in the
RDF triples may lose the relationship between
entities that affects the semantics of the result-
ing sentence (cf. Table 3). To address this is-
sue, in this paper, we propose a novel graph-based
triple encoder model that maintain the structure of
RDF triples as a small knowledge graph named
the GTR-LSTM model. This model computes the
hidden state of each entity in a graph to pre-
serve the relationships between entities in a triple
(intra-triple relationships) and the relationships
between entities in related triples (inter-triple re-
lationships) that helps to achieve even more ac-
curate sentences. This leads to two problems of
preserving the relationships in a knowledge graph:
(1) how to deal with a cycle in a knowledge graph;
(2) how to deal with multiple non-predefined re-

2http://talc1.loria.fr/webnlg/stories/challenge.html

lationships between two entities in a knowledge
graph. The proposed model differs from existing
non-linear LSTM models such as Tree LSTM (Tai
et al., 2015) and Graph LSTM (Liang et al., 2016)
in addressing the mentioned problem. In particu-
lar, Tree LSTM does not allow cycles, while the
proposed model handles cycles by first using a
combination of topological sort and breadth-first
traversal over a graph, and then using an atten-
tion model to capture the global information of the
knowledge graph. Meanwhile, Graph LSTM only
allows a predefined set of relationships between
entities, while the proposed model allows any re-
lationships by treating them as part of the input for
the hidden state computation.

To further enhance the capability of our model
to handle unseen entities, we propose to use en-
tity masking, which maps the entities in the model
training pairs to their types, e.g., we map an en-
tity (literal) “1967-01-10” to a type symbol
“DATE” in the training pairs. This way, our model
can learn to handle any date entities rather than
just “1967-01-10”. This is particularly helpful
when there is a limited training dataset.

Our contributions are:

• We propose an end-to-end encoder-decoder
based framework for the problem of translat-
ing RDF triples into natural sentences.

• We further propose a graph-based triple en-
coder to optimize the amount of information
preserved in the input of the framework. The
proposed model can handle cycles to cap-
ture the global information of a knowledge
graph. The proposed model also handles non-
predefined relationships between entities.

• We evaluate the proposed framework and
model over two real datasets. The results
show that our model outperforms the state-
of-the-art models consistently.

The rest of this paper is organized as follows.
Section 2 summarizes previous studies on sen-
tence generation. Section 3 details the proposed
model. Section 4 presents the experimental re-
sults. Section 5 concludes the paper.

2 Related Work

The studied problem falls in the area of Natu-
ral Language Generation (NLG) (Reiter and Dale,
2000). Bontcheva and Wilks (2004) follow a



1629

traditional NLG approach to generate sentences
from RDF data in the medical domain. They
start with filtering repetitive RDF data (document
planning) and then group coherent triples (micro-
planning). After that, they aggregate the sentences
generated for coherent triples to produce the fi-
nal sentences (aggregation and realization). Cimi-
ano et al. (2013) generate cooking recipes from
semantic web data. They focus on using a large
corpus to extract lexicon in the cooking domain.
The lexicon is then used with a traditional NLG
approach to generate cooking recipes. Duma and
Klein (2013) learn a sentence template from a par-
allel RDF data and text corpora. They first align
entities in RDF triples with entities mentioned in
sentences. Then, they extract templates from the
aligned sentences by replacing the entity mention
with a unique token. This method works well on
RDF triples in a seen domain but fails on RDF
triples in a previously unseen domain.

Recently, several methods using neural net-
works are proposed. Lebret et al. (2016) gener-
ate the first sentence of a biography using a con-
ditional neural language model. This model is
trained to predict the next word of a sentence not
only based on previous words, but also by using
features captured from Wikipedia infoboxes. Mei
et al. (2016) propose an encoder-aligner-decoder
model to generate weather forecasts. The aligner
is used to filter the most relevant data to be used to
predict the weather forecast. Both studies experi-
ment on cross-domain datasets. The result shows
that the neural language generation approach is
more flexible to work in an open domain since it
is not limited to handcrafted rules. This motivates
us to use a neural network based framework.

The most similar system to ours is Neural
Wikipedian (Vougiouklis et al., 2017), which gen-
erates a summary from RDF triples. It uses feed-
forward neural networks to encode RDF triples
and concatenate them as the input of the decoder.
The decoder uses LSTM to predict a sequence of
words as a summary. There are differences from
our work. First, Neural Wikipedian only works
with a set of RDF triples with a single entity point
of view (i.e., the entity of interest must be in either
the subject or the object of every triple). Our sys-
tem does not have this constraint. Second, Neu-
ral Wikipedian uses standard feed-forward neural
networks in the encoder. We design new triple en-
coder models to accommodate specific features of

…

…

Encoder

Decoder

Target Text

De-lexicalizerSentence Normalizer

Target Text Pre-processor

RDF Triples 

Entity Type Mapper Masking Module

RDF Pre-processor

s1 p1 o1 on…

w1 w2 wm…

Figure 1: RDF sentence generation based on an
encoder-decoder architecture.
RDF triples. Experimental results show that our
framework outperforms Neural Wikipedian.

3 Proposed Model

We start with the problem definition. We con-
sider a set of RDF triples as the input, which is
denoted by T = [t1, t2, ..., tn] where a triple ti
consists of three elements (subject si, predicate pi,
and object oi), ti = 〈si, pi, oi〉. Every element can
contain multiple words. We aim to generate a set
of sentences that consist of a sequence of words
S = 〈w1, w2, ..., wm〉, such that the relationships
in the input triples are correctly represented in S
while the sentences have a high quality. We use
BLEU, METEOR, and TER to assess the quality
of the sentence (detailed in Section 4). Table 1 il-
lustrates our problem input and the target output.

This section is organized as follows. First
we describe the overall framework (Section 3.1).
Next, we describe three triple encoder models in-
cluding the adapted standard BLSTM model (Sec-
tion 3.2), the adapted standard triple encoder
model (Section 3.3), and the proposed GTR-LSTM
model (Section 3.4). The decoder which is used
for all encoder models is described in Section 3.5.
The entity masking is described in Section 3.6

3.1 Solution Framework
Our solution framework uses an encoder-decoder
architecture as illustrated in Fig. 1. The framework



1630

consists of three components including an RDF
pre-processor, a target text pre-processor, and an
encoder-decoder module.

The RDF pre-processor consists of an entity
type mapper and a masking module. The entity
type mapper maps the subjects and objects in the
triples to their types, such that the sentence pat-
terns learned are based on entity types rather than
entities. For example, the input entities in Table 1,
“John Doe”, “London”, “England”, and
“1967-01-10” can be mapped to “PERSON”,
“CITY”, “COUNTRY”, and “DATE”, respectively.
The mapping has been shown in our experiments
to be highly effective in improving the model
output quality. The masking module converts
each entity into an entity identifier (eid). The
target text pre-processor consists of a text nor-
malizer and a de-lexicalizer. The text normal-
izer converts abbreviations and dates into the
same format as the corresponding entities in the
triples. The de-lexicalizer replaces all entities
in the target sentences by their eids. The RDF
and target text pre-processors are detailed in Sec-
tion 3.6. The replaced target sentences are com-
bined with the original target sentences and the
English Wikipedia articles is used as a corpus to
learn the word embeddings of the vocabulary.

To accommodate the RDF data, in the encoder
side, we consider three triple encoder models: (1)
the adapted standard BLSTM encoder; (2) the
adapted standard triple encoder; and (3) the pro-
posed GTR-LSTM triple encoder. The adapted
standard BLSTM encoder concatenates the tokens
in RDF triples as an input sequence, while the
standard triple encoder first encodes each RDF
triple into a vector representation and then con-
catenates the vectors of different triples. The latter
model better captures intra-triple relationships but
suffers in capturing inter-triple relationships. Con-
sidering the native representation of RDF triples as
a small knowledge graph, our graph-based GTR-
LSTM triple encoder captures both intra-triple and
inter-triple entity relationships.

3.2 Adapted Standard BLSTM Encoder

The standard encoder-decoder model with a
BLSTM encoder is a sequence to sequence learn-
ing model (Cho et al., 2014). To adapt such a
model for our problem, we transform a set of RDF
triples input T into a sequence of elements (i.e.,
T = [w1,1, w1,2, ..., w1,j , ..., wn,j ]), where wn,j is

John → w1,1
Doe → w1,2
birth → w1,3
place → w1,4
London → w1,5
London → w2,1
capital → w2,2
of → w2,3
England → w2,4
<pad> → w2,5
… → wn,1
… → wn,2
… → wn,3
… → wn,4
… → wn,5

t1

word embedding
Input

representationLSTM 

t2

tn

hn,1

hn,2

hn,3

hn,4

hn,5

h1,5

h2,5

hn,5

wn,1

wn,2

wn,3

wn,4

wn,5

...

hT

Figure 2: LSTM-based standard triple encoder.

the word embedding of a word in the n-th triple.
For example, following the triples in Table 1, w1,1
is the word embedding of “John”, w1,2 is the word
embedding of “Doe”, etc. This sequence forms
an input for the encoder. We use zero padding
to ensure that each input has the same representa-
tion size. The rest of the model is the same as the
standard encoder-decoder model with an attention
mechanism (Bahdanau et al., 2015). We call this
model the adapted standard BLSTM encoder.

3.3 Adapted Standard Triple Encoder

The standard BLSTM encoder suffers in captur-
ing the element relationships as the elements are
simply concatenated together. Next, we adapt the
standard BLSTM encoder to aggregate the word
embeddings of the elements of the same triple to
retain the intra-triple relationship. We call this the
adapted standard triple encoder.

The adaptation is done by grouping the ele-
ments of each triple, so the input is represented
as T = [〈w1,1, ..., w1,j〉, ..., 〈wn,1, ...wn,j〉], where
wn,j is the word embedding of a word in the n-th
triple. We use zero padding to ensure that each
triple has the same representation size. An LSTM
network of the encoder computes a hidden state of
each triple and concatenates them together to be
the input for the decoder:

hT = [f(t1); f(t2); ...; f(tn)] (1)

where hT is the input vector representation for the
decoder and f is an LSTM network (cf. Fig. 2).

3.4 GTR-LSTM Triple Encoder

The adapted standard triple encoder has an ad-
vantage in preserving the intra-triple relationship.
However, it has not considered the structural rela-



1631

birth_place capital_of

Mary

EnglandLondonJohn

spouse

lead_by
Figure 3: A small knowledge graph formed by a
set of RDF triples.

tionships between the entities in different triples.
To overcome this limitation, we propose a graph-
based triple encoder. We call it the GTR-LSTM
triple encoder. This encoder takes the input triples
in the form of a graph, which preserves the natural
structure of the triples (cf. Fig. 3).

GTR-LSTM differs from existing Graph
LSTM (Liang et al., 2016) and Tree LSTM (Tai
et al., 2015) models in the following aspects.
Graph LSTM is proposed for image data. It con-
structs the graph based on the spatial relationships
among super-pixels of an image. Tree LSTM uses
the dependency tree as the structure of a sentence.
Both models have a predefined relationship
between the vertices (Graph LSTM uses spatial
relationships: top, bottom, left, or right between
super-pixels; Tree LSTM uses dependencies
between words in a sentence as the relationship).
In contrast, a KB has an open set of relationships
between the vertices (i.e., the predicate defines
the relationship between entities/vertices) which
make our problem more difficult to model.

Our GTR-LSTM triple encoder overcomes the
difficulty as follows. It receives a directed graph
G = 〈V,E〉 as the input, where V is a set of
vertices that represent entities or literals, and E
is a set of directed edges that represent predi-
cates. Since the graph can contain cycles, we use a
combination of topological sort and breadth-first
traversal algorithms to traverse the graph. The
traversal is used to create an ordering of feeding
the vertices into a GTR-LSTM unit to compute
their hidden states. We start with running a topo-
logical sort to establish an order of the vertices
until no further vertex has a zero in-degree. For
the remaining vertices, they must be in strongly
connected component(s). Then, we run a breadth-
first traversal over the remaining vertices with a
random starting vertex, since every vertex can be
reached from all vertices of a strongly connected
component. When a vertex vi is visited, the hid-
den states of all adjacent vertices of vi are com-
puted (or updated if the hidden state of the vertex

< >

John
null

hjohn

h0

Mary

hmary

England
capital_of

hengland

London

hlondon

John

lead_by

h’john

spouse birth_place

Attention model

Figure 4: GTR-LSTM triple encoder.

is already computed in the previous step).
Following the graph in Fig. 3, the order of hid-

den state computation is as follows. The pro-
cess starts with a vertex with zero in-degree. Be-
cause there is no such vertex, a vertex is ran-
domly selected as the starting vertex. Assume we
pick “John” as the starting vertex, then we com-
pute hjohn using h0 as the previous hidden state.
Next, following the breadth-first traversal algo-
rithm, we visit vertex “John” and compute hmary
and hlondon by passing hjohn as the previous hid-
den state. Next step, vertex “Mary” is visited, but
no hidden states are computed or updated since it
does not have any adjacent vertices. In the last
step, vertex “England” is visited and hjohn is
updated. Fig. 4 illustrates the overall process.

Different from the Graph LSTM, our GTR-
LSTM model computes a hidden state by taking
into account the processed entity and its edge (the
edge pointing to the current entity from the previ-
ous entity) to handle non-predefined relationships
(any relationships between entities in a knowledge
graph). Thus, our GTR-LSTM unit (cf. Fig. 4)
receives two inputs, i.e., the entity and its relation-
ship. We propose the following model to compute
the hidden state of each GTR-LSTM unit.

it = σ

(∑
e

(
U iexte +W

ieht−1
))

(2)

fte = σ
(
Ufxte +W

fht−1
)

(3)

ot = σ

(∑
e

(Uoexte +W
oeht−1)

)
(4)

gt = tanh

(∑
e

(Ugexte +W
geht−1)

)
(5)

ct =

(
ct−1 ∗

∑
e

fte

)
+ (gt ∗ it) (6)

ht = tanh(ct) ∗ ot (7)

Here, U and W are learned parameter matrices, σ
denotes the sigmoid function, ∗ denotes element-



1632

x1 x2 xn

Decoder

hT

Decoder previous hidden state hdt

Attention model

…

α =	{α1,	α2,	…,	αn	}

Figure 5: Attention model of GTR-LSTM.

wise multiplication, and x is the input at the cur-
rent time-step. The input gate i determines the
weight of the current input. The forget gate f de-
termines the weight of the previous state. The out-
put gate o determines the weight of the cell state
forwarded to the next time-step. The state g is the
candidate hidden state used to compute the inter-
nal memory unit c based on the current input and
the previous state. The subscript t is the time-
step. The subscript/superscript e is the input el-
ement (an entity or a predicate). Following Tree
LSTM (Tai et al., 2015) and Graph LSTM (Liang
et al., 2016), we also use a separate forget gate for
each input that allows the GTR-LSTM unit to in-
corporate information from each input selectively.

From Fig. 4, we can see that the traversal cre-
ates two branches, one ended in hmary and the
other ended in h′john. After the encoder computes
the hidden states of each vertex, h′john does not
include the information of hmary and vice versa.
Moreover, the graph can contain cycles that cause
difficulty in determining the starting and ending
vertices. Our traversal procedure ensures that the
hidden states of all vertices are updated based on
their adjacent vertices (local neighbors). To fur-
ther capture the global information of the graph,
we apply an attention model on the GTR-LSTM
triple encoder. The attention model takes the hid-
den states of all vertices computed by the encoder
and the previous hidden state of the decoder to
compute the final input vector of each decoder
time-step. Figure 5 illustrates the attention model
of GTR-LSTM. Inspired by Luong et al. (2015),
we adapt the following equation to compute the
weight of each vertex.

αn =
exp(hdt

T
Wxn)∑|X|

j=1 exp(h
d
t
T
Wxj)

(8)

Here, hdt is the previous hidden state of the
decoder, |X| is the total number of entities in
the triples, W is a learned parameter matrix, xn
and xj are hidden states of vertices, and α =
{α1, α2, ..., αn} is the weight vector of all ver-
tices. Then the input of the decoder for each time-
step can be computed as follows.

hT =

|X|∑
n=1

αnxn (9)

3.5 Decoder
The decoder of the proposed framework is a stan-
dard LSTM. It is trained to generate the output
sequence by predicting the next output word wt
conditioned on the hidden state hdt. The current
hidden state hdt is conditioned on the hidden state
of the previous time-step hdt−1, the output of the
previous time-step wt−1, and input vector repre-
sentation hT . The hidden state and the output of
the decoder at time-step t are computed as:

hdt = f(h
d
t−1, wt−1, hT ) (10)

wt = softmax(V ht) (11)

Here, f is a single LSTM unit, and V is the
hidden-to-output weight matrix. The encoder and
the decoder are trained to maximize the condi-
tional log-likelihood:

p(Sn | Tn) =
|Sn|∑
t=1

logwt (12)

Hence, the training objective is to minimize the
negative conditional log-likelihood:

J =
N∑

n=1

− log p(Sn | Tn) (13)

where (Sn, Tn) is a pair of output word sequence
and input RDF triple set given for the training.

3.6 Entity Masking
Entity masking makes our framework generalizes
better to unseen entities. This technique addresses
the problem of a limited training set which is faced
by many NLG problems.

Entity masking replaces entity mentions with
eids and entity types in both the input triples and
the target sentences. However, we do not want our
model to be overly generalized either. Thus, we
need to have general and specific entity types. For
example, the entity “John Doe” is replaced by
“ENT-1 PERSON GOVERNOR”. To add the en-
tity types, we use the DBpedia lookup API. The



1633

API returns several entity types. The general and
specific entity types are defined by the level of the
word in the WordNet (Fellbaum, 1998) hierarchy.

In the encoder side, each element of the
triple tn = 〈sn, pn, on〉 is transformed into
sn = 〈lsn , gsn , dsn〉 , pn = 〈lpn〉, and on =
〈lon , gon , don〉, where l is the label of an element,
g is the general entity type, and d is the specific
entity type. The labels of the subject and the ob-
ject are latter replaced by eids, while the label of
the predicate is preserved, since it indicates the re-
lationship between the subject and the object.

On the decoder side, the entities in the tar-
get text are also replaced by their corresponding
eids. Entity matching is beyond the scope of our
study. We simply use a combination of three string
matching methods to find entity mentions in the
sentence: exact matching, n-gram matching, and
parse tree matching. The exact matching is used
to find the exact mention; the n-gram matching is
used to handle partial matching with the same to-
ken length; and parse tree matching is used to find
a partial matching with different token length.

4 Experiments

We evaluate our framework on two datasets. The
first is the dataset from Gardent et al. (2017a).
We call it the WebNLG dataset. This dataset con-
tains 25,298 RDF triple set-text pairs, with 9,674
unique sets of RDF triples. The dataset con-
sists of a Train+Dev dataset and a Test Unseen
dataset. We split Train+Dev into a training set
(80%), a development set (10%), and a Seen test-
ing set (10%). The Train+Dev dataset contains
RDF triples in ten categories (topics, e.g., astro-
naut, monument, food, etc.), while the Test Un-
seen dataset has five other unseen categories. The
maximum number of triples in each RDF triple
set is seven. For the second dataset, we collected
data from Wikipedia pages regarding landmarks.
We call it the GKB dataset. We first extract RDF
triples from Wikipedia infoboxes and sentences
from the Wikipedia text that contain entities men-
tioned in the RDF triples. Human annotators then
filter out false matches to obtain 1,000 RDF triple
set-text pairs. This dataset is split into the train-
ing and development set (80%) and the testing set
(20%). Table 1 illustrates an example of the data
pairs of WebNLG and GKB dataset.

We implement the existing models, the adapted

model, and the proposed model using Keras3.
We use three common evaluation metrics in-
cluding BLEU (Papineni et al., 2002), ME-
TEOR (Denkowski and Lavie, 2011), and
TER (Snover et al., 2006). For the metric com-
putation and significance testing, we use MultE-
val (Clark et al., 2011).

4.1 Tested Models

We compare our proposed graph-based triple
encoder (GTR-LSTM, Section 3.4) with three
existing model including the adapted standard
BLSTM encoder (BLSTM, Section 3.2), Neural
Wikipedian (Vougiouklis et al., 2017) (TFF), and
statistical machine translation (Hoang and Koehn,
2008) (SMT) trained on a 6-gram language model.
We also compare with the adapted standard triple
encoder (TLSTM, Section 3.3).

4.2 Hyperparameters

We use grid search to find the best hyperparame-
ters for the neural networks. We use GloVe (Pen-
nington et al., 2014) trained on the GKB and
WebNLG training data and full English Wikipedia
data dump to get 300-dimension word embed-
dings. We use 512 hidden units for both en-
coder and decoder. We use a 0.5 dropout rate
for regularization on both encoder and decoder to
avoid overfitting. We train our model on NVIDIA
Tesla K40c. We find that using adaptive learn-
ing rates for the optimization is efficient and leads
the model to converge faster. Thus, we use
Adam (Kingma and Ba, 2015) with a learning rate
of 0.0002 instead of stochastic gradient descent.
The update of parameters in training is computed
using a mini batch of 64 instances. We further ap-
ply early stopping to detect the convergence.

4.3 Effect of Entity Masking

Table 2 shows the overall comparison of model
performance. It shows that entity masking gives a
consistent performance improvement for all mod-
els. Generalizing the input triples and target sen-
tences helps the models to learn the relationships
between entities from their types. This is partic-
ularly helpful when there is limited training data.
We use a combination of exact matching, n-gram
matching and parse tree matching to find the entity
mentions in the sentence. The entity masking ac-
curacy for WebNLG dataset is 87.15%, while for

3https://nmt-keras.readthedocs.io/en/latest/



1634

Model
Metric/Dataset BLEU↑ METEOR↑ TER↓

Seen Unseen GKB Seen Unseen GKB Seen Unseen GKB

Entity
Unmasking

Existing models
BLSTM 42.7 23.0 28.0 34.4 28.7 27.5 55.7 69.9 67.7
SMT 41.1 23.9 27.7 33.2 28.3 27.6 57.0 70.1 63.8
TFF 44.6 26.4 26.4 33.9 29.4 27.2 52.4 62.6 60.1

Adapted model TLSTM 45.9 28.1 29.4 34.9 30.1 28.5 50.5 62.7 59.0
Our proposed GTR-LSTM 54.0 29.2 37.1 37.3 27.8 30.6 45.3 59.8 55.1

Entity
Masking

Existing models
BLSTM 49.8 28.0 34.8 38.3 29.4 28.6 49.9 64.9 65.8
SMT 46.5 24.8 32.0 37.1 29.1 28.5 52.3 62.2 67.8
TFF 47.8 28.4 33.7 35.9 30.5 28.9 49.9 61.2 58.4

Adapted Model TLSTM 50.5 31.6 36.7 36.5 30.7 30.1 47.7 60.4 57.2
Our proposed GTR-LSTM 58.6 34.1 40.1 40.6 32.0 34.6 41.7 57.9 50.6

Table 2: Comparison of model performance.

RDF inputs 〈Elizabeth Tower, location, London〉, 〈Wembley Stadium, location, London〉,〈London, capital of, England〉, 〈Theresa May, prime minister, England〉

Reference london , england is home to wembley stadium and the elizabeth tower.
the name of the leader in england is theresa may.

BLSTM
england is lead by theresa may and is located in the city of london .
the elizabeth tower is located in the city of england and is located in
the wembley stadium.

SMT wembley stadium is located in london , elizabeth tower . theresa may
is the leader of england , england.

TFF the elizabeth tower is located in london , england , where wembleystadium is the leader and theresa may is the leader.

TLSTM the wembley stadium is located in london , england . the country is
the location of elizabeth tower . theresa may is the leader of london.

GTR-LSTM the wembley stadium and elizabeth tower are both located in london ,
england . theresa may is the leader of england.

Table 3: Sample output of the system. The error is highlighted in bold.

the GKB dataset is 82.45%.
Entity masking improves the BLEU score of

the proposed GTR-LSTM model by 8.5% (from
54.0 on the Entity Unmasking model to 58.6 on
the Entity Masking model), 16.7%, and 8.0%
on the WebNLG seen testing data (denoted by
“Seen”), WebNLG unseen testing data (denoted
by “Unseen”), and the GKB testing data (denoted
by “GKB”). Using the entity masking not only
improves the performance by recognizing the un-
known vocabulary via eid masking but also im-
proves the running time performance by requiring
a smaller training vocabulary.

4.4 Effect of Models

Table 2 also shows that the proposed GTR-LSTM
triple encoder achieves a consistent improvement
over the baseline models, and the improvement is
statistically significant, with p < 0.01 based on
the t-test of all metrics. We use MultEval to com-
pute the p value based on an approximate random-
ization (Clark et al., 2011). The improvement on
the BLEU score indicates that the model reduces
the errors in the generated sentence. Our manual
inspection confirms this result. The better (lower)
TER score suggests that the model generates a
more compact output (i.e., better aggregation).

Table 3 shows a sample output of all models.
From this table, we can see that all baseline
models produce sentences that contain wrong
relationships between entities (e.g., the BLSTM
output contains a wrong relationship “the
elizabeth tower is located in the
city of england”). Moreover, the base-
line models generate sentences with a weak
aggregation (e.g., “Elizabeth Tower” and
“Wembley Stadium” are in separate sentences
for TLSTM). The proposed GTR-LSTM model
successfully avoids these problems.

Model training time. GTR-LSM is slower in
training than the baseline models, which is ex-
pected as it needs to encode more information.
However, its training time is no more than twice as
that of any baseline models tested, and the train-
ing can complete within one day which seems
reasonable. Meanwhile, the number of parame-
ters trained for GTR-LSTM is up to 59% smaller
than those of the baseline models, which saves the
space cost for model storage.

4.5 Human Evaluation

To complement the automatic evaluation, we con-
duct human evaluations for all of the masked mod-
els. We ask five human annotators. Each of them



1635

Model
Dataset/Metric Seen Unseen GKB

Correctness Grammar Fluency Correctness Grammar Fluency Correctness Grammar Fluency

Existing Models
BLSTM 2.25 2.33 2.29 1.53 1.71 1.68 1.54 1.84 1.84
SMT 2.03 2.11 2.07 1.36 1.48 1.44 1.81 1.99 1.89
TFF 1.77 1.91 1.88 1.44 1.69 1.66 1.71 1.99 1.96

Adapted Model TLSTM 2.53 2.61 2.55 1.75 1.93 1.86 2.21 2.38 2.35
Our Proposed GTR-LSTM 2.64 2.66 2.57 1.96 2.04 1.99 2.29 2.42 2.41

Table 4: Human evaluation results.

has studied English for at least ten years and com-
pleted education in a full English environment for
at least two years. We provide a website4 that
shows them the RDF triples and the generated text.
The annotators are given training on the scoring
criteria. We also provide scoring examples. We
randomly selected 100 sets of triples along with
the output of each model. We only select sets of
triples that contain more than two triples. Follow-
ing (Gardent et al., 2017b), we use three evalua-
tion metrics including correctness, grammatical-
ity, and fluency. For each pair of triple set and
generated sentences, the annotators are asked to
give a score between one to three for each metric.

Correctness is used to measure the semantics of
the output sentence. A score of 3 is given to gen-
erated sentences that contain no errors in the rela-
tionships between entities; a score of 2 is given to
generated sentences that contain one error in the
relationship; and a score of 1 is given to gener-
ated sentences that contain more than one errors
in the relationships. Grammaticality is used to rate
the grammatical and spelling errors of the gener-
ated sentences. Similar to the correctness metric,
a score of 3 is given to generated sentences with
no grammatical and spelling errors; a score of 2 is
given to generated sentences with one error; and a
score of 1 for the others. The last metric, fluency,
is used to measure the fluency of the sentence out-
put. We ask the annotators to give a score based on
the aggregation of the sentences and the existence
of sentence repetition. Table 4 shows the results
of the human evaluations. The results confirm the
automatic evaluation in which our proposed model
achieves the best scores.

Error analysis. We further perform a manual
inspection of 100 randomly selected output
sentences of GTR-LSTM and BLSTM on the
Seen and Unseen test data. We find that 32%
of BLSTM output contains wrong relationships
between entities. In comparison, only 8%
of GTR-LSTM output contains such errors.
Besides, we find duplicate sub-sentences in

4http://bit.ly/gkb-mappings

the output of GTR-LSTM (15%). The fol-
lowing output is an example: “beef kway
teow is a dish from singapore,
where english language is spoken
and the leader is tony tan. the
leader of singapore is tony tan.”
While the duplicate sentence is not wrong, it
affects the reading experience. We conjecture that
the LSTM in the decoder caused such an issue.
We aim to solve this problem in future work.

5 Conclusions

We proposed a novel graph-based triple encoder
GTR-LSTM for sentence generation from RDF
data. The proposed model maintains the struc-
ture of input RDF triples as a small knowledge
graph to optimize the amount of information pre-
served in the input of the model. The proposed
model can handle cycles to capture the global in-
formation of a knowledge graph and also handle
non-predefined relationships between entities of a
knowledge graph.

Our experiments show that GTR-LSTM offers
a better performance than all the competitors. On
the WebNLG dataset, our model outperforms the
best existing model, the standard BLSTM model,
by up to 17.6%, 6.0%, and 16.4% in terms of
BLEU, METEOR, and TER scores, respectively.
On the GKB dataset, our model outperforms the
standard BLSTM model by up to 15.2%, 20.9%,
and 23.1% in these three metrics, respectively.

Acknowledgments

Bayu Distiawan Trisedya is supported by the In-
donesian Endowment Fund for Education (LPDP).
This work is supported by Australian Research
Council (ARC) Discovery Project DP180102050
and Future Fellowships Project FT120100832,
and Google Faculty Research Award. This work
is partly done while Jianzhong Qi is visiting the
University of New South Wales. Wei Wang
was partially supported by D2DCRC DC25002,
DC25003, ARC DP 170103710 and 180103411.



1636

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-

gio. 2015. Neural machine translation by jointly
learning to align and translate. In International
Conference on Learning Representations (ICLR).
http://arxiv.org/abs/1409.0473.

Kalina Bontcheva and Yorick Wilks. 2004. Auto-
matic Report Generation from Ontologies: The
MIAKT Approach, Springer, Berlin, Heidelberg,
pages 324–335. https://doi.org/10.1007/978-3-540-
27779-8 28.

Antoine Bordes, Sumit Chopra, and Jason We-
ston. 2014. Question answering with subgraph
embeddings. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Lan-
guage Processing (EMNLP). pages 615–620.
https://www.aclweb.org/anthology/D/D14/D14-
1067.pdf.

Andrew Chisholm, Will Radford, and Ben Hachey.
2017. Learning to generate one-sentence biogra-
phies from wikidata. In Proceedings of the 15th
Conference of the European Chapter of the Associ-
ation for Computational Linguistics (EACL). pages
633–642. http://aclweb.org/anthology/E17-1060.

Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using rnn encoder–decoder
for statistical machine translation. In Proceedings of
the 2014 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP). pages 1724–
1734. http://www.aclweb.org/anthology/D14-1179.

Philipp Cimiano, Janna Lüker, David Nagel, and
Christina Unger. 2013. Exploiting ontology lexica
for generating natural language texts from rdf data.
In Proceedings of the 14th European Workshop on
Natural Language Generation (ENLG). pages 10–
19. http://www.aclweb.org/anthology/W13-2102.

Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis test-
ing for statistical machine translation: Control-
ling for optimizer instability. In Proceedings
of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Lan-
guage Technologies (ACL-HLT). pages 176–181.
http://www.aclweb.org/anthology/P11-2031.

Danica Damljanovic, Milan Agatonovic, and Hamish
Cunningham. 2010. Natural language interfaces
to ontologies: combining syntactic analysis and
ontology-based lookup through the user interac-
tion. In Proceedings of the 7th International Confer-
ence on The Semantic Web (ISWC). pages 106–120.
https://doi.org/10.1007/978-3-642-13486-9 8.

Michael J. Denkowski and Alon Lavie. 2011. Me-
teor 1.3: Automatic metric for reliable optimiza-
tion and evaluation of machine translation systems.

In Proceedings of the Sixth Workshop on Statis-
tical Machine Translation (WMT). pages 85–91.
http://aclweb.org/anthology/W11-2107.

Daniel Duma and Ewan Klein. 2013. Gener-
ating natural language from linked data: Un-
supervised template extraction. In Proceed-
ings of the 10th International Conference on
Computational Semantics (IWCS). pages 83–94.
http://www.aclweb.org/anthology/W13-0108.

Anthony Fader, Luke Zettlemoyer, and Oren Et-
zioni. 2014. Open question answering over
curated and extracted knowledge bases. In
Proceedings of the 20th ACM SIGKDD In-
ternational Conference on Knowledge Discov-
ery and Data Mining (KDD). pages 1156–1165.
https://doi.org/10.1145/2623330.2623677.

Christiane Fellbaum. 1998. WordNet: An
Electronic Lexical Database. MIT Press.
http://aclweb.org/anthology/J99-2008.

Claire Gardent, Anastasia Shimorina, Shashi Narayan,
and Laura Perez-Beltrachini. 2017a. Creating train-
ing corpora for nlg micro-planners. In Proceedings
of the 55th Annual Meeting of the Association for
Computational Linguistics (ACL). pages 179–188.
http://aclweb.org/anthology/P17-1017.

Claire Gardent, Anastasia Shimorina, Shashi Narayan,
and Laura Perez-Beltrachini. 2017b. The webnlg
challenge: Generating text from rdf data. In Pro-
ceedings of the 10th International Conference on
Natural Language Generation (INLG). pages 124–
133. http://www.aclweb.org/anthology/W17-3518.

Hieu Hoang and Philipp Koehn. 2008. De-
sign of the moses decoder for statistical ma-
chine translation. In Software Engineering, Test-
ing, and Quality Assurance for Natural Lan-
guage Processing (SETQA-NLP). pages 58–65.
http://www.aclweb.org/anthology/W08-0510.

Diederik P. Kingma and Jimmy Lei Ba. 2015. Adam:
A method for stochastic optimization. In Inter-
national Conference on Learning Representations
(ICLR). https://arxiv.org/abs/1412.6980.

Rémi Lebret, David Grangier, and Michael Auli. 2016.
Neural text generation from structured data with ap-
plication to the biography domain. In Proceed-
ings of the 2016 Conference on Empirical Methods
in Natural Language Processing (EMNLP). pages
1203–1213. https://aclweb.org/anthology/D16-
1128.

Xiaodan Liang, Xiaohui Shen, Jiashi Feng, Liang Lin,
and Shuicheng Yan. 2016. Semantic object pars-
ing with graph lstm. In Proceedings of the 14th
European Conference on Computer Vision (ECCV).
pages 125–143. https://doi.org/10.1007/978-3-319-
46448-0 8.

http://arxiv.org/abs/1409.0473
http://arxiv.org/abs/1409.0473
http://arxiv.org/abs/1409.0473
https://doi.org/10.1007/978-3-540-27779-8_28
https://doi.org/10.1007/978-3-540-27779-8_28
https://www.aclweb.org/anthology/D/D14/D14-1067.pdf
https://www.aclweb.org/anthology/D/D14/D14-1067.pdf
https://www.aclweb.org/anthology/D/D14/D14-1067.pdf
https://www.aclweb.org/anthology/D/D14/D14-1067.pdf
http://aclweb.org/anthology/E17-1060
http://aclweb.org/anthology/E17-1060
http://aclweb.org/anthology/E17-1060
http://www.aclweb.org/anthology/D14-1179
http://www.aclweb.org/anthology/D14-1179
http://www.aclweb.org/anthology/D14-1179
http://www.aclweb.org/anthology/D14-1179
http://www.aclweb.org/anthology/W13-2102
http://www.aclweb.org/anthology/W13-2102
http://www.aclweb.org/anthology/W13-2102
http://www.aclweb.org/anthology/P11-2031
http://www.aclweb.org/anthology/P11-2031
http://www.aclweb.org/anthology/P11-2031
http://www.aclweb.org/anthology/P11-2031
https://doi.org/10.1007/978-3-642-13486-9_8
https://doi.org/10.1007/978-3-642-13486-9_8
https://doi.org/10.1007/978-3-642-13486-9_8
https://doi.org/10.1007/978-3-642-13486-9_8
https://doi.org/10.1007/978-3-642-13486-9_8
http://aclweb.org/anthology/W11-2107
http://aclweb.org/anthology/W11-2107
http://aclweb.org/anthology/W11-2107
http://aclweb.org/anthology/W11-2107
http://www.aclweb.org/anthology/W13-0108
http://www.aclweb.org/anthology/W13-0108
http://www.aclweb.org/anthology/W13-0108
http://www.aclweb.org/anthology/W13-0108
https://doi.org/10.1145/2623330.2623677
https://doi.org/10.1145/2623330.2623677
https://doi.org/10.1145/2623330.2623677
http://aclweb.org/anthology/J99-2008
http://aclweb.org/anthology/P17-1017
http://aclweb.org/anthology/P17-1017
http://aclweb.org/anthology/P17-1017
http://www.aclweb.org/anthology/W17-3518
http://www.aclweb.org/anthology/W17-3518
http://www.aclweb.org/anthology/W17-3518
http://www.aclweb.org/anthology/W08-0510
http://www.aclweb.org/anthology/W08-0510
http://www.aclweb.org/anthology/W08-0510
http://www.aclweb.org/anthology/W08-0510
https://arxiv.org/abs/1412.6980
https://arxiv.org/abs/1412.6980
https://arxiv.org/abs/1412.6980
https://aclweb.org/anthology/D16-1128
https://aclweb.org/anthology/D16-1128
https://aclweb.org/anthology/D16-1128
https://aclweb.org/anthology/D16-1128
https://doi.org/10.1007/978-3-319-46448-0_8
https://doi.org/10.1007/978-3-319-46448-0_8
https://doi.org/10.1007/978-3-319-46448-0_8
https://doi.org/10.1007/978-3-319-46448-0_8


1637

Thang Luong, Hieu Pham, and Christopher D. Man-
ning. 2015. Effective approaches to attention-based
neural machine translation. In Proceedings of the
2015 Conference on Empirical Methods in Natural
Language Processing (EMNLP). pages 1412–1421.
http://aclweb.org/anthology/D15-1166.

Hongyuan Mei, Mohit Bansal, and Matthew R. Wal-
ter. 2016. What to talk about and how? se-
lective generation using lstms with coarse-to-fine
alignment. In Proceedings of the 2016 Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies (NAACL-HLT). pages 720–730.
http://www.aclweb.org/anthology/N16-1086.

Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. Bleu: a method for auto-
matic evaluation of machine translation. In Proceed-
ings of 40th Annual Meeting of the Association for
Computational Linguistics (ACL). pages 311–318.
http://aclweb.org/anthology/P02-1040.pdf.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors
for word representation. In Proceedings of the
2014 Conference on Empirical Methods in Natural
Language Processing (EMNLP). pages 1532–1543.
https://aclweb.org/anthology/D14-1162.

Ehud Reiter and Robert Dale. 2000. Building natural
language generation systems. Cambridge Univer-
sity Press.

Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Trans-
lation in the Americas (AMTA). pages 223–231.
http://mt-archive.info/AMTA-2006-Snover.pdf.

Kai Sheng Tai, Richard Socher, and Christopher D.
Manning. 2015. Improved semantic representations
from tree-structured long short-term memory net-
works. In Proceedings of the 53rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL) and the 7th International Joint Conference
on Natural Language Processing (IJCNLP). pages
1556–1566. http://www.aclweb.org/anthology/P15-
1150.

Christina Unger, Lorenz Bhmann, Jens Lehmann,
Axel-Cyrille Ngonga Ngomo, Daniel Gerber,
and Philipp Cimiano. 2012. Template-based
question answering over rdf data. In Pro-
ceedings of the 21st international conference
on World Wide Web (WWW). pages 639–648.
https://doi.org/10.1145/2187836.2187923.

Pavlos Vougiouklis, Hady Elsahar, Lucie-Aime Kaffee,
Christoph Gravier, Frederique Laforest, Jonathon
Hare, and Elena Simperl. 2017. Neural wikipedian:
Generating textual summaries from knowledge
base triples. arXiv preprint arXiv:1711.00155
https://arxiv.org/pdf/1711.00155.pdf.

http://aclweb.org/anthology/D15-1166
http://aclweb.org/anthology/D15-1166
http://aclweb.org/anthology/D15-1166
http://www.aclweb.org/anthology/N16-1086
http://www.aclweb.org/anthology/N16-1086
http://www.aclweb.org/anthology/N16-1086
http://www.aclweb.org/anthology/N16-1086
http://aclweb.org/anthology/P02-1040.pdf
http://aclweb.org/anthology/P02-1040.pdf
http://aclweb.org/anthology/P02-1040.pdf
https://aclweb.org/anthology/D14-1162
https://aclweb.org/anthology/D14-1162
https://aclweb.org/anthology/D14-1162
http://mt-archive.info/AMTA-2006-Snover.pdf
http://mt-archive.info/AMTA-2006-Snover.pdf
http://mt-archive.info/AMTA-2006-Snover.pdf
http://www.aclweb.org/anthology/P15-1150
http://www.aclweb.org/anthology/P15-1150
http://www.aclweb.org/anthology/P15-1150
http://www.aclweb.org/anthology/P15-1150
http://www.aclweb.org/anthology/P15-1150
https://doi.org/10.1145/2187836.2187923
https://doi.org/10.1145/2187836.2187923
https://doi.org/10.1145/2187836.2187923
https://arxiv.org/pdf/1711.00155.pdf
https://arxiv.org/pdf/1711.00155.pdf
https://arxiv.org/pdf/1711.00155.pdf
https://arxiv.org/pdf/1711.00155.pdf

