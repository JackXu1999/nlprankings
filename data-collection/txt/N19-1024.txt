



















































Neural Finite-State Transducers: Beyond Rational Relations


Proceedings of NAACL-HLT 2019, pages 272–283
Minneapolis, Minnesota, June 2 - June 7, 2019. c©2019 Association for Computational Linguistics

272

Neural Finite State Transducers: Beyond Rational Relations
Chu-Cheng Lin

Computer Science Department
Johns Hopkins University

Baltimore, MD 21218, USA
kitsing@cs.jhu.edu

Hao Zhu
Dept. of Computer Science and Technology

Tsinghua University
Beijing, China

zhuhao15@mails.tsinghua.edu.cn

Matthew R. Gormley
Machine Learning Department
Carnegie Mellon University
Pittsburgh, PA 15213, USA
mgormley@cs.cmu.edu

Jason Eisner
Computer Science Department

Johns Hopkins University
Baltimore, MD 21218, USA
jason@cs.jhu.edu

Abstract

We introduce neural finite state transducers
(NFSTs), a family of string transduction models
defining joint and conditional probability distri-
butions over pairs of strings. The probability of
a string pair is obtained by marginalizing over
all its accepting paths in a finite state transducer.
In contrast to ordinary weighted FSTs, however,
each path is scored using an arbitrary function
such as a recurrent neural network, which breaks
the usual conditional independence assumption
(Markov property). NFSTs are more powerful
than previous finite-state models with neural fea-
tures (Rastogi et al., 2016). We present training
and inference algorithms for locally and globally
normalized variants of NFSTs. In experiments
on different transduction tasks, they compete
favorably against seq2seq models while offer-
ing interpretable paths that correspond to hard
monotonic alignments.

1 Introduction

Weighted finite state transducers (WFSTs) have been
used for decades to analyze, align, and transduce
strings in language and speech processing (Roche
and Schabes, 1997; Mohri et al., 2008). They form
a family of efficient, interpretable models with well-
studied theory. A WFST describes a function that
maps each string pair (x,y) to a weight—often a
real number representing p(x,y) or p(y | x). The
WFST is a labeled graph, in which each path a
represents a sequence of operations that describes
how some x and some y could be jointly generated,
or how x could be edited into y. Multiple paths for
the same (x,y) pair correspond to different analyses
(labeled alignments) of that pair.

However, WFSTs can only model certain func-
tions, known as the rational relations (Berstel and
Reutenauer, 1988).The weight of a path is simply
the product of the weights on its arcs. This means

s1
ε:εs0 s2<BOS>

ε:ε
<EOS>

a:æ
<i-a><V><o-æ>

b:b
<i-b><C><o-b>

a:!
<i-a>
ε:æ
<o-æ>

th:θ
<i-t><i-h><C><o-θ>

Figure 1: A marked finite-state transducer T . Each arc in
T is associated with input and output substrings, listed
above the arcs in the figure. The arcs are not labeled with
weights as in WFSTs. Rather, each arc is labeled with
a sequence of marks (shown in brown) that featurize
its qualities. The neural scoring model scores a path by
scoring each mark in the context of all marks on the
entire path. The example shown here is from the G2P
application of §4.1; for space, only a few arcs are shown.
ε represents the empty string.

that in a random path of the form a  b  c,
the two subpaths are conditionally independent
given their common state b: a Markov property.

In this paper, we propose neural finite state trans-
ducers (NFSTs), in which the weight of each path is
instead given by some sort of neural network, such
as an RNN. Thus, the weight of an arc can depend
on the context in which the arc is used. By aban-
doning the Markov property, we lose exact dynamic
programming algorithms, but we gain expressiv-
ity: the neural network can capture dependencies
among the operations along a path. For example,
the RNN might give higher weight to a path if it
is “internally consistent”: it might thus prefer to
transcribe a speaker’s utterance with a path that
maps similar sounds in similar contexts to similar
phonemes, thereby adapting to the speaker’s accent.



273

Consider a finite-state transducer T as in Figure 1
(see Appendix A for background). Using the com-
position operator ◦, we can obtain a new FST, x◦T ,
whose accepting paths correspond to the accepting
paths of T that have input string x. Similarly, the
accepting paths of T ◦ y correspond to the accept-
ing paths of T that have output string y. Finally,
x◦T ◦y extracts the paths that have both properties.
We define a joint probability distribution over (x,y)
pairs by marginalizing over those paths:

p(x,y) =
∑

a∈x◦T ◦y
p(a) =

1

Z(T )
∑

a∈x◦T ◦y
p̃(a) (1)

where p̃(a) is the weight of path a and Z(T ) =∑
a∈T p̃(a) is a normalization constant.
We define p̃(a) , expGθ(a) with Gθ(a) being

some parametric scoring function. In our experi-
ments, we will adopt a fairly simple left-to-right
RNN architecture (§2.2), but one could easily sub-
stitute fancier architectures. We will also consider
defining Gθ by a locally normalized RNN that
ensures Z(T ) = 1.
In short, we use the finite-state transducer T to

compactly define a set of possible paths a. The
number of paths may be exponential in the size of
T , or infinite if T is cyclic. However, in contrast to
WFSTs, we abandon this combinatorial structure in
favor of neural nets when defining the probability
distribution over a. In the resulting marginal distri-
bution p(x,y) given in equation (1), the path a that
aligns x and y is a latent variable. This is also true
of the resulting conditional distribution p(y | x).
We explore training and inference algorithms

for various classes of NFST models (§3). Classical
WFSTs (Mohri et al., 2008) and BiRNN-WFSTs
(Rastogi et al., 2016) use restricted scoring functions
and so admit exact dynamic programming algo-
rithms. For general NFSTs, however, we must resort
to approximate computation of the model’s training
gradient, marginal probabilities, and predictions.
In this paper, we will use sequential importance
sampling methods (Lin and Eisner, 2018), leaving
variational approximation methods to future work.

Defining models using FSTs has several benefits:

Output-sensitive encoding Currently popular
models of p(y | x) used in machine translation
and morphology include seq2seq (Sutskever
et al., 2014), seq2seq with attention (Bahdanau
et al., 2015; Luong et al., 2015), the Trans-
former (Vaswani et al., 2017). These models

first encode x as a vector or sequence of vec-
tors, and then condition the generation of y on
this encoding. The vector is determined from x
only. This is also the case in the BiRNN-WFST
(Rastogi et al., 2016), a previous finite-state
model to which we compare. By contrast, in
our NFST, the state of the RNN as it reads and
transduces the second half of x is influenced
by the first halves of both x and y and their
alignment.

Inductive bias Typically, a FST is constructed
with domain knowledge (possibly by compil-
ing a regular expression), so that its states
reflect interpretable properties such as syllable
boundaries or linguistic features. Indeed, we
will show below how to make these proper-
ties explicit by “marking” the FST arcs. The
NFST’s path scoring function then sees these
marks and can learn to take them into account.
The NFST also inherits any hard constraints
from the FST: if the FST omits all (x,y) paths
for some “illegal” x,y, then p(x,y) = 0 for
any parameter vector θ (a “structural zero”).

Interpretability Like a WFST, an NFST can
“explain” why it mapped x to y in terms of a
latent path a, which specifies a hard monotonic
labeled alignment. The posterior distribution
p(a | x,y) specifies which paths a are the best
explanations (e.g., Table 5).

We conduct experiments on three tasks:
grapheme-to-phoneme, phoneme-to-grapheme, and
action-to-command (Bastings et al., 2018). Our
results on these datasets show that our best models
can improve over neural seq2seq and previously
proposed hard alignment models.

2 Neuralizing Finite-State Transducers

2.1 Neuralized FSTs
An NFST is a pair (T , Gθ), where T is an un-
weighted FST with accepting paths A and Gθ :
A → R is a function that scores these paths. As ex-
plained earlier, we then refer to p̃(a) = expGθ(a)
as the weight of path a ∈ A. A weighted relation
between input and output strings is given by p̃(x,y),
which is defined to be the total weight of all paths
with input string x ∈ Σ∗ and output string y ∈ ∆∗,
where where Σ and ∆ are the input and output
alphabets of T . The real parameter vector θ can be
adjusted to obtain different weighted relations. We



274

Model Training Algorithms Long-Term Output-Output Dependency Left-to-Right Factorization

WFSTs Dynamic Programming 7 3
BiRNN-WFSTs Dynamic Programming 7 3
Local NFSTs Importance Sampling 3 3
Global NFSTs Importance Sampling 3 7

Table 1: Comparison between WFSTs, BiRNN-WFSTs (Rastogi et al., 2016), and NFSTs.

can normalize p̃ to get a probability distribution as
shown in equation (1).

2.2 A basic scoring architecture

Weighted FST. AWFST over the (+,×) semir-
ing can be regarded as the special case in which
Gθ(a) ,

∑|a|
t=1 gθ(at). This is a sum of scores

assigned to the arcs in a = a1a2 · · · .

Marked FST. Our innovation is to allow the arcs’
scores to depend on their context in the path. Now
θ no longer associates a fixed score with each
arc. Rather, we assume that each arc a in the FST
comes labeled with a sequence of marks from
a mark alphabet Ω, as illustrated in Figure 1.
The marks reflect the FST constructor’s domain
knowledge about what arc a does (see §4.2 be-
low). We now define Gθ(a) = Gθ(ω(a)), where
ω(a) = ω(a1)ω(a2) · · · ∈ Ω∗ is the concatenated
sequence of marks from the arcs along path a.
It is sometimes helpful to divide marks into dif-

ferent classes. An arc can be regarded as a possible
“edit” that aligns an input substring with an out-
put substring in the context of transitioning from
one FST state to another. The arc’s input marks
describe its input substring, its output marks de-
scribe its output substring, and the remaining marks
may describe other properties of the arc’s aligned
input-output pair or the states that it connects.
Recall that an FST encodes domain knowledge.

Its paths represent alignments between input and
output strings, where each alignment specifies a
segmentation of x and y into substrings labeled
with FST states. Decorating the arcs with marks
furnishes the path scoring model with domain-
specific information about the alignments.

RNN scoring. Ifθmerely associated a fixed score
with each mark, then the marked FST would be no
more powerful than the WFST. To obtain contextual
mark scores as desired, one simple architecture is a

recurrent neural network:

Gθ(ω) ,
|ω|∑
t=1

gθ(st−1, ωt) (2)

st = fθ(st−1, ωt), with s0 = 0 (3)

where st−1 ∈ Rd is the hidden state vector of the
network after reading ω1 · · ·ωt−1. The gθ function
defines the score of reading ωt in this left context,
and fθ defines how doing so updates the state.
In our experiments, we chose fθ to be the GRU

state update function (Cho et al., 2014). We defined
gθ(s, ωt) , (Ws + b)>emb(ωt). The parameter
vector θ specifies the GRU parameters,W,b, and
the mark embeddings emb(ω).
One could easily substitute much fancier archi-

tectures, such as a stacked BiLSTM with attention
(Tilk and Alumäe, 2016), or a Transformer (Vaswani
et al., 2017).

2.3 Partitioned hidden vectors
In hopes of improving the inductive bias of the
learner, we partitioned the hidden state vector into
three sub-vectors: st = [sat ; sxt ; s

y
t ]. The mark scor-

ing function fθ(st−1, ωt) was as before, but we
restricted the form of gθ, the state update function.
sat encodes all past marks and depends on the full
hidden state so far: sat = gaθ(st−1, ωt). However,
we make sxt encode only the sequence of past input
marks, ignoring all others. Thus, sxt = gxθ(s

x
t−1, ωt)

if ωt is an input mark, and sxt = sxt−1 otherwise.
Symmetrically, syt encodes only the sequence of
past output marks. This architecture is somewhat
like Dyer et al. (2016), which also uses different
sub-vectors to keep track of different aspects of the
history.

2.4 Local normalization
A difficulty with the general model form in equa-
tion (1) is that the normalizing constant Z(T ) =∑

a∈T p̃(a) must sum over a large set of paths—in
fact, an infinite set if T is cyclic. This sum may
diverge for some values of the parameter vector θ,
which complicates training of the model (Dreyer,



275

2011). Even if the sum is known to converge, it is
in general intractable to compute it exactly. Thus,
estimating the gradient of Z(T ) during training
involves approximate sampling from the typically
high-entropy distribution p(a). The resulting es-
timates are error-prone because the sample size
tends to be too small and the approximate sampler
is biased.
A standard solution in the WFST setting (e.g.

Cotterell et al., 2014) is to use a locally normalized
model, in which Z(T ) is guaranteed to be 1.1 The
big summation over all paths a is replaced by small
summations—which can be computed explicitly—
over just the outgoing edges from a given state.
Formally, we define the unnormalized score of

arc ai in the context of path a in the obvious way, by
summing over the contextual scores of its marks:

g̃θ(ai) ,
k∑

t=j+1

gθ(st−1, ωt) (4)

where j = |ω(a1) · · ·ω(ai−1)| and k =
|ω(a1) · · ·ω(ai)|. Its normalized score is then

gθ,T (ai) , log
(

exp g̃θ(ai)/
∑

a′
exp g̃θ(a

′)
)

where a′ ranges over all arcs in T (including ai
itself) that emerge from the same state as ai does.
We can now score the paths in T using

Gθ,T (a) =

|a|∑
i=1

gθ,T (ai) (5)

This gives rise to a proper probability distribution
p(a) , p̃(a) = expGθ,T (a) over the paths of
T . No global normalization constant is necessary.
However, note that the scoring function now requires
T as an extra subscript, because it is necessary when
scoring a to identify the competitors in T of each arc
ai. Thus,when p(x,y) is found as usual by summing
up the probabilities of all paths in x ◦ T ◦ y, each
path is still scored using its arcs’ competitors from
T . This means that each state in x ◦ T ◦ y must
record the state in T from which it was derived.

3 Sampling, Training, and Decoding
3.1 Sampling from conditioned distributions

with amortized inference
Many algorithms for working with probability
distributions—including our training and decoding

1Provided that every state in T is co-accessible, i.e., has a
path to a final state.

algorithms below—rely on conditional sampling.
In general, we would like to sample a path of T
given the knowledge that its input and output strings
fall into sets X and Y respectively.2 If X and Y
are regular languages, this is equivalent to defining
T ′ = X ◦ T ◦ Y and sampling from

p(a | T ′) , p̃(a)∑
a′∈T ′ p̃(a

′)
, (6)

Due to the nonlinearity of Gθ, the denominator
of equation (6) is generally intractable. If T ′ is
cyclic, it cannot even be computed by brute-force
enumeration. Thus, we fall back on normalized
importance sampling, directly adopting the ideas
of Lin and Eisner (2018) in our more general FST
setting. We employ a proposal distribution q:

p(a | T ′) = Ea∼q[
p(a | T ′)
q(a)

], (7)

≈
M∑

m=1

p̃(a(m))

q(a(m)) · Ẑ
· I(a = a(m))

= p̂(a | T ′),

where Ẑ =
∑M

m′=1
p̃(a(m

′))

q(a(m
′))
, and q is a locally

normalized distribution over paths a ∈ T ′. In this
paper we further parametrize q as

qφ(a; T ′) =
T∏
t=1

qt(at | a1...t−1;φ, T ′),

(8)
qt(a | a:t−1;φ, T ′) ∝ exp(g(st−1, at;θ, T ) + Cφ),

where Cφ , C(s′t, X, Y,φ) ∈ R, s′t ,
f(st−1,ω(a)) is a compatibility function that is
typically modeled using a neural network. In this
paper, one the following three cases are encountered:

• X = x, is a string, and Y = ∆∗:
in this case T ′ = x ◦ T . We let
Cφ = C

x(s′t,RNNx(x, i,φ);φ), where i
is the length of the input prefix in a1...t.a,
RNNx(x, i,φ) is the hidden state of the i-th
position after reading x (not a nor ω) back-
wards, and Cx(·, ·) is a feed-forward network
that takes the concatenated vector of all argu-
ments, and outputs a real scalar. We describe
the parametrization of Cx in Appendix C.1.

2WhenX or Y is larger than a single string, it is commonly
all of Σ∗ or ∆∗ respectively, in which case conditioning on it
gives no information.



276

• X = Σ∗, and Y = y is a string: in
this case T ′ = T ◦ y. We let Cφ =
Cy(s′t,RNNy(y, j,φ);φ), where j is the
length of the output prefix in a1...t.a, and
RNNy, Cy are similarly defined as in RNNx

and Cx.

• X and Y are both strings — X =
x, Y = y: in this case we let Cφ =
Cxy(s′t,RNNx(x, i,φ),RNNy(y, j,φ);φ).

Given a path prefix a:t−1, qt(a | a:t−1;φ, T ′)
is defined over arcs a such that a:t−1.a is a valid
path prefix in T ′. To optimize φ with regard to
qφ, we follow (Lin and Eisner, 2018) and seek
to find φ∗ = argminφKL[p̂||qφ], where p̂ is the
approximate distribution defined in equation (7),
which is equivalent tomaximizing the log-likelihood
of qφ(a) when a is distributed according to the
approximation p̂.

3.2 Training

In this paper, we consider joint training. The loss
function of our model is defined as the negative log
joint probability of string pair (x,y):

L(x,y) = − log p(x,y) = − log
∑

a∈x◦T ◦y
p(a).

(9)

Since p is an exponential family distribution, the
gradients of L can be written as (Bishop, 2006)

∇L(x,y) = −Ea∼p(·|x◦T ◦y)[∇ log p(a)], (10)

where p(· | x ◦ T ◦ y) is a conditioned distribution
over paths. Computing equation (10) requires sam-
pling from p(· | x ◦ T ◦ y), which, as we discuss in
§3.1, is often impractical. We therefore approximate
it with

∇θL(x,y) = −Ea∼p(·|x◦T ◦y)[∇θ log p(a)]
≈ −Ea∼p̂(·|x◦T ◦y)[∇θ log p(a)]

(11)

= −
M∑

m=1

w(m)∇θGθ(a(m)), (12)

where q is a proposal distribution parametrized as
in equation (8) (discussed in §3.1,) a(1) . . .a(M) ∼
q are i.i.d. samples of paths in x ◦ T ◦ y, and
w(m) is the importance weight of them-th sample
satisfying w(m) ∝ expGθ(a

(m))

q(a(m))
,
∑M

m=1w
(m) = 1.

Pseudocode for calculating equation (12) is listed
in Algorithm 1.

Algorithm 1 Compute approximate gradient for
updating Gθ
Require: Gθ : A → R is an NFST scoring func-

tion, q is a distribution over paths,M ∈ N is
the sample size

1: function Get-Gradient(Gθ,M , q)
2: for m in 1 . . .M do
3: a(m) ∼ q
4: w̃(m) ← expGθ(a

(m))
q(a)

5: end for
6: Ẑ ←

∑M
m=1 w̃

(m)

7: for m in 1 . . .M do
8: w(m) ← w̃(m)

Ẑ
9: end for
10: return −

∑M
m=1w

(m)∇θGθ(a(m))
11: end function

3.3 Decoding most probable strings
Besides finding good paths in a conditioned dis-
tribution as we discuss in §3.1, we are also often
interested in finding good output strings, which is
conventionally referred to as the decoding problem,
which we define to be finding the best output string
y∗ , argmaxy∈L(Y ) pY(y | T ′), where

pY(y | T ′) ,
∑

a∈T ′◦y p̃(a)∑
a′∈T ′ p̃(a

′)
. (13)

ŷ∗ , argmaxy P̂Y(y | T ′) is a consistent estima-
tor of y∗, which can directly be used to find the
best string. However, making this estimate accu-
rate might be expensive: it requires sampling many
paths in the machine T ′, which is usually cyclic,
and therefore has infinitely many more paths, than
T ′ ◦ yk, which has finitely many paths when A is
acyclic. On the other hand, for the task of finding
the best string among a pool candidates, we do not
need to compute (or approximate) the denominator
in equation (13), since

y∗ = argmax
y∈L(Y )

∑
a∈T ′◦y

p̃(a). (14)

As in the case for paths, the language L(Y ) is
usually infinitely large. However given an output
candidate yk ∈ L′ ⊆ L(Y ), we can approximate
the summation in equation (14) using importance
sampling:∑
a∈T ′◦yk

p̃(a) = Ea∼q(·|T ′◦yk)[
p̃(a)

q(a | T ′ ◦ yk)
],

(15)



277

Algorithm 2 Training procedure for Gθ. See Appendix C.2 for implementation details.
Require: (T , Gθ) is an NFST, D = {(x1,y1) . . . (x|D|,y|D|)} is the training dataset, LR : N→ R is a

learning rate scheduler, θ0 are the initial parameters of Gθ,M is a given sample size, maxEpoch ∈ N
is the number of epochs to train for

1: procedure Train(T , Gθ, D, LR, θ0,M , maxEpochs)
2: for epoch ∈ [1 . . .maxEpochs] do
3: for (xi,yi) ∈ shuffle(D) do
4: T ′ ← xi ◦ T ◦ yi
5: Construct distribution q(· | T ′) according to equation (8)
6: u← Get-Gradient(Gθ,M, q) (listed in Algorithm 1)
7: θ ← θ − LR(epoch)× u
8: (Optional) update the parameters of q(· | T ′).
9: end for
10: end for
11: end procedure

where q(· | T ′ ◦ yk) is a proposal distribution
over paths in T ′ ◦ yk. In this paper we parametrize
q(· | T ′◦yk) following the definition in equation (8).
When L′ is finitely large, we reduce the decoding
task into a reranking task.

To populate L′, one possibility is to marginalize
over paths in the approximate distribution p̂(a | T ′)
discussed in §3.1 to obtain an estimate p̂Y(y | T ′),
and use its support as L′. Note that it’s possible
to populate the candidate pool in other ways, each
with its advantages and drawbacks: for example,
one can use a top-k path set from a weighted
(Markovian) FST. This approach guarantees exact
computation, and the pool quality would no longer
depend on the qualities of the smoothing distribution
qφ. However it is also a considerably much weaker
model and may yield uninspiring candidates. In
the common case where the conditioned machine
T ′ = X ◦ T ◦ Y has X = x ∈ Σ∗ as the input
string, and Y is the universal acceptor that accepts
∆∗, one can obtain a candidate pool from seq2seq
models: seq2seq models can capture long distance
dependencies between input and output strings,
and are typically fast to train and decode from.
However they are not applicable in the case where
L(Y ) 6= ∆∗. Experimental details of decoding are
further discussed in §4.3.

4 Experiments

Our experiments mainly aim to: (1) show the effec-
tiveness of NFSTs on transduction tasks; (2) illus-
trate that howprior knowledge can be introduced into
NFSTs and improve the performance; (3) demon-
strate the interpretability of our model. Through-
out, we experiment on three tasks: (i) grapheme-

to-phoneme, (ii) phoneme-to-grapheme, and (iii)
actions-to-commands. We compare with compet-
itive string transduction baseline models in these
tasks.

4.1 Tasks and datasets

We carry out experiments on three string transduc-
tion tasks:

Grapheme-to-phoneme and phoneme-to-
grapheme (G2P/P2G) refer to the transduction
between words’ spelling and phonemic transcrip-
tion. English has a highly irregular orthography
(Venezky, 2011), which necessitates the use of
rich models for this task. We use a portion of the
standard CMUDict dataset: the Sphinx-compatible
version of CMUDict (Weide, 2005). As for metrics,
we choose widely used exact match accuracy and
edit distance.

Action-to-command (A2C) refers to the transduc-
tion between an action sequence and imperative
commands. We use NACS (Bastings et al., 2018) in
our experiment. As for metrics, we use exact match
accuracy (EM). Note that the in A2C setting, a
given input can yield different outputs, e.g. I_JUMP
I_WALK I_WALK corresponds to both “jump and
walk twice” and “walk twice after jump”. NACS is
a finite set of action-command pairs; we consider
a predicted command to be correct if it is in the
finite set and its corresponding actions is exactly the
input. We evaluate on the length setting proposed
by Bastings et al. (2018), where we train on shorter
sequences and evaluate on longer sequences.



278

4.2 FST designs
NFSTs require an unweighted FST T which defines
a scaffold for the relation it recognizes. In this paper
we experiment with two versions of T : the first is
a simple ‘general’ design T0, which contains only
three states s{0,1,2}, where the only arc between
q0 and q1 consumes the mark <BOS>; and the only
arc between q1 and q2 consumes the mark <EOS>.
T0 has exactly one accepting state, which is q2. To
ensure that T0 defines relation for all possible string
pairs (x,y) ∈ Σ∗×∆∗, we add all arcs of the form
a = (s1, s1,ω, σ, δ), ∀(σ, δ) ∈ Σ×∆ to T .
To recognize transduction rules defined in the

Wikipedia English IPA Help page, we define TIPA,
which has all states and arcs of T0, and additional
states and arcs to handle multi-grapheme and multi-
phoneme transductions defined in the IPA Help:3 for
example, the transduction th→ T is encoded as two
arcs (s1, s3,ω, t, T) and (s3, s1,ω, h, ε). Because
of the lack of good prior knowledge that can be
added to A2C experiments, we only use general
FSTs in those experiments for such experiments.
Nor do we encode special marks that we are going
to introduce below.4

4.2.1 Design of mark sequences
As with regular WFSTs, the arcs can often be hand-
engineered to incorporate prior knowledge. Recall
that aswe describe in §2.2,eacharc is associatedwith
a mark sequence. In this paper,we will always derive
the mark sequence on an arc a = (s′, s,ω′, σ, δ)
of the transducer T as ω = [σ,ω′, δ, s], where
ω′ ∈ Ω∗ can be engineered to reflect FST- and
application-specific properties of a path, such as
the IPA Help list we mentioned earlier. One way to
encode such knowledge into mark sequences is to
have special mark symbols in mark sequences for
particular transductions. In this paperwe experiment
with two schemes of marks:

• IPA Help (IPA). We define the IPA mark
ωIPA = {C | V}, where the symbol C indicates
that this arc is part of a transduction rule listed
in the consonant section of the Wikipedia
English IPA Help page. Similarly, the mark V
indicates that the transduction rule is listed in
the vowel section.

3https://en.wikipedia.org/wiki/Help:
IPA/English

4The NACS dataset was actually generated from a regular
transducer, which we could in principle use, but doing so would
make the transduction fully deterministic and probably not
interesting/hard enough.

• PhonemeClasses (Phone).Wedefine Phone
marks ωPhone = Φ(δ), where Φ is a lookup
function that returns the phoneme class of δ
defined by the CMUDict dataset.5

In this paper we experiment with the following
three FST and mark configurations for G2P/P2G
experiments:

• -IPA-Phone in which case ω′ = ∅ for all
arcs. T = T0.

• +IPA-Phone in which caseω′ = [ωIPA] when
the transduction rule is found in the IPA Help
list, otherwise ω′ = ∅. T = TIPA.

• +IPA+Phone in which case ω′ =
[ωIPAωPhone] when the transduction rule is
found in the IPA Help list, otherwise ω′ =
[ωPhone]. T = TIPA.

As we said earlier, we only use T = T0 with no
special marks for A2C experiments. Experimental
results on these different configurations are in §5.3.

4.3 Decoding methods
Weexperimentwith the followingmethods to decode
the most probable strings:

• Approximate Posterior (AP). We approx-
imate the posterior distribution over out-
put strings p̂Y(y | T ′), and pick ŷ∗ =
argmaxy p̂Y(y | T ′) as the output.

• Reranking AP.As we discuss in §3.3, improv-
ing ŷ∗ by taking more path samples in T ′ may
be expensive. The reranking method uses the
support of p̂Y as a candidate pool L′, and for
each yk ∈ L′ we estimate equation (15) using
path samples in T ′ ◦ yk.

• Reranking External. This decoding method
uses k-best lists from external models. In this
paper, we make use of sequence-to-sequence
baseline models as the candidate pool L′.

• Reranking AP + External. This decoding
method uses the union of the support of p̂Y
and k-best lists from the sequence-to-sequence
baseline models as the candidate pool L′.

In this paper,we take 128 path samples per candidate
for all Reranking methods.

5https://github.com/cmusphinx/cmudict/blob/
master/cmudict.phones

https://en.wikipedia.org/wiki/Help:IPA/English
https://en.wikipedia.org/wiki/Help:IPA/English
https://github.com/cmusphinx/cmudict/blob/master/cmudict.phones
https://github.com/cmusphinx/cmudict/blob/master/cmudict.phones


279

5 Results

5.1 Baselines

We compare NFSTs against the following baselines:

BiRNN-WFSTs proposed by Rastogi et al.
(2016),were weighted finite-state transducers whose
weights encode input string features by the use of re-
current neural networks. As we note in Table 1, they
can be seen as a special case of NFSTs, where the
Markov property is kept, but where exact inference
is still possible.

Seq2seq models are the standard toolkit for trans-
duction tasks. We make use of the attention mech-
anism proposed by Luong et al. (2015), which
accomplishes ‘soft alignments’ that do not enforce
a monotonic alignment constraint.

Neuralized IBMModel 1 is a character transduc-
tion model recently proposed by Wu et al. (2018),
which marginalizes over non-monotonic hard align-
ments between input and output strings. Like (Luong
et al., 2015), they did not enforce monotonic align-
ment constraints; but unlike them, they did not
make use of the input feeding mechanism,6 where
past alignment information is fed back into the
RNN decoder. This particular omission allows (Wu
et al., 2018) to do exact inference with a dynamic
programming algorithm.

All baseline systems are tuned on the validation
sets. The seq2seq models employ GRUs, with word
and RNN embedding size = 500 and a dropout rate
of 0.3. They are trained with the Adam optimizer
(Kingma and Ba, 2014) over 50 epochs. The Neu-
ralized IBMModel 1 models are tuned as described
in (Wu et al., 2018).

5.2 The effectiveness of NFSTs

5.2.1 Does losing the Markov property help?

Table 2 indicates that BiRNN-WFST models (Ras-
togi et al., 2016) perform worse than other models.
Their Markovian assumption helps enable dynamic
programming, but restricts their expressive power,
which greatly hampers the BiRNN-WFST’s per-
formance on the P2G/G2P task. The NACS task
also relies highly on output-output interactions, and
BiRNN-WFST performs very poorly there.

6We discuss this further in Appendix B.1.

G2P / P2G NACS

EM Accuracy Edit Distance EM Accuracy

Dev Test Dev Test Test

BiRNN-WFST 16.9 15.9 1.532 1.645 5.6
Seq2seq 30.7 28.9 1.373 1.426 9.0
Neuralized IBM Model 1 31.6 30.2 1.366 1.398 —

Local NFSTs 32.7 31.8 1.319 1.332 15.64

Table 2: Average exact match accuracy (%, higher the
better) and edit distance (lower the better) on G2P
and P2G as well as exact match accuracy on NACS.
Comparison between our models with baselines. For
NFST models, we make use of the Reranking AP
decoding method described in §4.2.

5.2.2 Effectiveness of proposed decoding
methods

Table 3 shows results from different decoding
methods on the G2P/P2G tasks, configuration
+IPA+Phone. AP performs significantly worse
than Reranking AP, suggesting that the estimate
ŷ∗ suffers from the variance problem. Interestingly,
of decoding methods that employ external models,
Reranking External performs better thanRerank-
ing AP + External, despite having a smaller candi-
date pool. We think there is some product-of-experts
effect in Reranking External since the external
model may not be biased in the same way as our
model is. But such benefits vanish when candidates
from AP are also in the pool — our learned approxi-
mation learns the bias in the model— and hence the
worse performance in Reranking AP + External.
This suggests an interesting regularization trick
in practice: populating the candidate pool using
external models to hide our model bias. However
when we compare our method against non-NFST
baseline methods we do not make use of such tricks,
to ensure a more fair comparison.

EM Accuracy Edit Distance

Dev Test Dev Test

AP 28.2 28.2 1.513 1.467
Reranking AP 32.7 31.8 1.319 1.332
Reranking External 33.3 32.7 1.297 1.298
Reranking AP + External 32.9 32.0 1.309 1.303

Table 3: Average exact match accuracy (%, higher the
better) and edit distance (lower the better) on G2P and
P2G. The effectiveness of different decoding methods.

5.3 Prior knowledge: does it help?
In Table 4 we see that combining both +IPA and
+Phone improves model generalizability over the
general FST (-IPA -Phone). We also note that using
only the IPA marks leads to degraded performance



280

EM Accuracy Edit Distance

Dev Test Dev Test

-IPA -Phone 31.8 29.3 1.38 1.373
+IPA -Phone 31.3 29.2 1.367 1.431
+IPA +Phone 32.7 31.8 1.319 1.332

Table 4: Average exact match accuracy (%, higher the
better) and edit distance (lower the better) on G2P and
P2G. The effectiveness of different FST designs.

compared to the general FST baseline. This is a
surprising result — one explanation is the IPA
marks are not defined on all paths that transduce
the intended input-output pairs: NFSTs are capable
of recognizing phoneme-grapheme alignments in
different paths,7 but only one such path is marked
by +IPA. But we leave a more thorough analysis to
future work.

6 Related Work

Recently, there has been work relating finite-state
methods and neural architectures. For example,
Schwartz et al. (2018) and Peng et al. (2018) have
shown the equivalence between some neural models
and WFSAs. The most important differences of
our work is that in addition to classifying strings,
NFSTs can also transduce strings. Moreover, NFSTs
also allow free topology of FST design, and breaks
the Markovian assumption. In addition to models
we compare against in §4, we note that (Aharoni
and Goldberg, 2017; Deng et al., 2018) are also
similar to our work; in that they also marginalize
over latent alignments, although they do not enforce
the monotonicity constraint. Work that discusses
globally normalized sequence models are relevant to
ourwork. In this paper,we discuss a training strategy
that bounds the partition function; other ways to
train a globally normalized model (not necessarily
probabilistic) include (Wiseman and Rush, 2016;
Andor et al., 2016). On the other hand, our locally
normalized FSTs bear resemblance to (Dyer et al.,
2016), which was also locally normalized, and also
employed importance sampling for training.

7 Conclusions and Future Work

Neural finite state transducers (NFSTs) are able
to model string pairs, considering their monotonic
alignment but also enjoying RNNs’ power to handle
non-finite-state phenomena. They compete favor-

7This is discussed further in Appendix B.2.

ably with state-of-the-art neural models on trans-
duction tasks. At the same time, it is easy to inject
domain knowledge into NFSTs for inductive bias,
and they offer interpretable paths.
In this paper, we have used rather simple archi-

tectures for our RNNs; one could experiment with
multiple layers and attention. One could also ex-
periment with associating marks differently with
arcs—the marks are able to convey useful domain
information to the RNNs. For example, in a P2G
or G2P task, all arcs that cross a syllable boundary
might update the RNN state using a syllable
mark. We envision using regular expressions to
build the NFSTs, and embedding marks in the regu-
lar expressions as a way of sending useful features
to the RNNs to help them evaluate paths.

In this paper,we have studiedNFSTs as standalone
systems. But as probabilistic models, they can be
readily embedded in a bigger picture: it should be
directly feasible to incorporate a globally/locally
normalized NFST in a larger probabilistic model
(Finkel and Manning, 2009; Chiang et al., 2010).

The path weights of NFSTs could be interpreted
simply as scores, rather than log-probabilities. One
would then decode by seeking the 1-best path with
input x, e.g., via beam search or Monte Carlo Tree
Search. In this setting, one might attempt to train the
NFST using methods similar to the max-violation
structured perceptron or the structured SVM.

Acknowledgments

This work has been generously supported by a
Google Faculty Research Award and by Grant No.
1718846 from the National Science Foundation,
both to the last author. Hao Zhu is supported by
Tsinghua University Initiative Scientific Research
Program. We thank Shijie Wu for providing us IBM
Neuralized Model 1 experiment results.

References
Roee Aharoni and Yoav Goldberg. 2017. Morphological

inflection generation with hard monotonic attention.
In ACL.

Daniel Andor, Chris Alberti, David Weiss, Aliaksei
Severyn, Alessandro Presta, Kuzman Ganchev, Slav
Petrov, and Michael Collins. 2016. Globally normal-
ized transition-based neural networks. In Association
for Computational Linguistics.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.
2015. Neural machine translation by jointly learning
to align and translate. In Proceedings of ICLR.

https://arxiv.org/abs/1603.06042
https://arxiv.org/abs/1603.06042


281

Joost Bastings, Marco Baroni, Jason Weston, Kyunghyun
Cho, and Douwe Kiela. 2018. Jump to better conclu-
sions: Scan both left and right. In Proceedings of the
2018 EMNLP Workshop BlackboxNLP: Analyzing
and Interpreting Neural Networks for NLP, pages
47–55.

Jean Berstel, Jr. and Christophe Reutenauer. 1988. Ra-
tional Series and Their Languages. Springer-Verlag,
Berlin, Heidelberg.

Christopher M Bishop. 2006. Pattern recognition and
machine learning.

David Chiang, Jonathan Graehl, Kevin Knight, Adam
Pauls, and Sujith Ravi. 2010. Bayesian inference for
finite-state transducers. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 447–455. Association for
Computational Linguistics.

Kyunghyun Cho, Bart van Merrienboer, Çaglar Gülçehre,
Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk,
and Yoshua Bengio. 2014. Learning phrase repre-
sentations using rnn encoder-decoder for statistical
machine translation. In EMNLP, pages 1724–1734.
ACL.

Ryan Cotterell, Nanyun Peng, and Jason Eisner. 2014.
Stochastic contextual edit distance and probabilistic
FSTs. In Proceedings of the 52nd Annual Meeting
of the Association for Computational Linguistics
(Volume 2: Short Papers), pages 625–630, Baltimore.

Yuntian Deng, Yoon Kim, Justin Chiu, Demi Guo,
and Alexander Rush. 2018. Latent alignment and
variational attention. In S. Bengio, H. Wallach,
H. Larochelle, K. Grauman, N. Cesa-Bianchi, and
R. Garnett, editors, Advances in Neural Information
Processing Systems 31, pages 9735–9747. Curran
Associates, Inc.

Markus Dreyer. 2011. A Non-Parametric Model for the
Discovery of Inflectional Paradigms from Plain Text
Using Graphical Models over Strings. Ph.D. thesis,
Johns Hopkins University, Baltimore, MD.

Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and
Noah A. Smith. 2016. Recurrent neural network
grammars. In HLT-NAACL.

Jenny Rose Finkel and Christopher D Manning. 2009.
Hierarchical bayesian domain adaptation. In Proceed-
ings of Human Language Technologies: The 2009
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
602–610. Association for Computational Linguistics.

Diederik P. Kingma and Jimmy Ba. 2014. Adam:
A method for stochastic optimization. CoRR,
abs/1412.6980.

Chu-Cheng Lin and Jason Eisner. 2018. Neural particle
smoothing for sampling from conditional sequence
models. In Proceedings of the 2018 Conference

of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies (NAACL-HLT), pages 929–941, New
Orleans.

Thang Luong, Hieu Pham, and Christopher D Manning.
2015. Effective approaches to attention-based neural
machine translation. In Proceedings of the 2015 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1412–1421.

Mehryar Mohri, Fernando Pereira, and Michael Riley.
2008. Speech recognition with weighted finite-state
transducers. In Springer Handbook of Speech Pro-
cessing, pages 559–584. Springer.

Hao Peng, Roy Schwartz, Sam Thomson, and Noah A.
Smith. 2018. Rational recurrences. In EMNLP.

Pushpendre Rastogi, Ryan Cotterell, and Jason Eisner.
2016. Weighting finite-state transductions with neural
context. In Proceedings of the 2016 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies (NAACL-HLT), pages 623–633, San
Diego. 11 pages. Supplementary material (1 page)
also available.

Emmanuel Roche and Yves Schabes, editors. 1997.
Finite-State Language Processing. MIT Press.

Roy Schwartz, Sam Thomson, and Noah A. Smith. 2018.
Sopa: Bridging cnns, rnns, and weighted finite-state
machines. CoRR, abs/1805.06061.

Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.
Sequence to sequence learning with neural networks.
In Proceedings of the 27th International Conference
on Neural Information Processing Systems - Volume
2, NIPS’14, pages 3104–3112, Cambridge, MA, USA.
MIT Press.

Ottokar Tilk and Tanel Alumäe. 2016. Bidirectional
recurrent neural network with attention mechanism
for punctuation restoration. In INTERSPEECH.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need.

Richard L Venezky. 2011. The structure of English
orthography, volume 82. Walter de Gruyter.

Robert Weide. 2005. The carnegie mellon pronouncing
dictionary [cmudict. 0.6].

Sam Wiseman and Alexander M. Rush. 2016. Sequence-
to-sequence learning as beam-search optimization. In
EMNLP.

Shijie Wu, Pamela Shapiro, and Ryan Cotterell. 2018.
Hard non-monotonic attention for character-level trans-
duction. In Proceedings of the 2018 Conference on
Empirical Methods in Natural Language Processing,
pages 4425–4438.

http://dblp.uni-trier.de/db/conf/emnlp/emnlp2014.html#ChoMGBBSB14
http://dblp.uni-trier.de/db/conf/emnlp/emnlp2014.html#ChoMGBBSB14
http://dblp.uni-trier.de/db/conf/emnlp/emnlp2014.html#ChoMGBBSB14
http://cs.jhu.edu/~jason/papers/#cotterell-peng-eisner-2014
http://cs.jhu.edu/~jason/papers/#cotterell-peng-eisner-2014
http://papers.nips.cc/paper/8179-latent-alignment-and-variational-attention.pdf
http://papers.nips.cc/paper/8179-latent-alignment-and-variational-attention.pdf
http://cs.jhu.edu/~jason/papers/#dreyer-2011
http://cs.jhu.edu/~jason/papers/#dreyer-2011
http://cs.jhu.edu/~jason/papers/#dreyer-2011
http://cs.jhu.edu/~jason/papers/#lin-eisner-2018-naacl
http://cs.jhu.edu/~jason/papers/#lin-eisner-2018-naacl
http://cs.jhu.edu/~jason/papers/#lin-eisner-2018-naacl
http://cs.jhu.edu/~jason/papers/#rastogi-cotterell-eisner-2016
http://cs.jhu.edu/~jason/papers/#rastogi-cotterell-eisner-2016
https://mitpress.mit.edu/books/finite-state-language-processing
http://dl.acm.org/citation.cfm?id=2969033.2969173
https://arxiv.org/pdf/1706.03762.pdf
https://arxiv.org/pdf/1706.03762.pdf


282

A Finite-state transducers

A.1 Rational Relations
A relation is a set of pairs—in this paper, a subset
of Σ∗ × ∆∗, so it relates strings over an “input”
alphabet Σ to strings over an “output” alphabet ∆.
A weighted relation is a function R that maps

any string pair (x,y) to a weight in R≥0.
We say that the relation R is rational if R can

be defined by some weighted finite-state transducer
(FST) T . As formalized in AppendixA.3, this means
thatR(x,y) is the total weight of all accepting paths
in T that are labeled with (x,y) (which is 0 if there
are no such accepting paths). The weight of each
accepting path in T is given by the product of its
arc weights, which fall in R>0.
The set of pairs support(R) , {(x,y) :

R(x,y) > 0} is then said to be a regular rela-
tion because it is recognized by the unweighted FST
obtained by dropping the weights from T . In this
paper, we are interested in defining non-rational
weighting functions R with this same regular sup-
port set.

A.2 Finite-state transducers
We briefly review finite-state transducers
(FSTs). Formally, an FST is a tuple T0 =
(Σ,∆, Q,A0, I, F ) where

• Σ is a finite input alphabet

• ∆ is a finite output alphabet

• Q is a finite set of states

• A0 ⊆ Q×Q× (Σ ∪ {�})× (∆ ∪ {�}) is the
set of weighted arcs

• I ⊆ Q is the set of initial states (conventionally
|I| = 1)

• F ⊆ Q is the set of final states

Let a = a1 . . . aT (for T ≥ 0) be an accepting path
in T0, that is, each ai = (qi−1, qi, σi, δi) ∈ A0 and
q0 ∈ I, qT ∈ F . We say that the input and output
strings of a are σ1 · · ·σT and δ1 · · · δT .

A.3 Real-valued weighted FSTs
Weighted FSTs (WFSTs) are defined very simi-
larly to FSTs. A WFST is formally defined as
a 6-tuple, just like an (unweighted) FST: T =
(Σ,∆, Q,A, I, F ), with arcs carrying weights:A ⊆
Q×Q× (Σ∪{�})× (∆∪{�})×R. Compared to

FST arcs in Appendix A.2, a WFST arc each ai =
(qi−1, qi, σi, δi, κi) ∈ A has weight κi. We also
define the weight of a to be w(a) ,

⊗T
i=1 κi ∈ R.

The weight of the entire WFST T is defined as
the total weight (under ⊕) of all accepting paths:

T [ ] ,
⊕
a

w(a) ∈ R (16)

More interestingly, the weight T [x,y] of a string
pair x ∈ Σ∗,y ∈ ∆∗ is given by similarly summing
w(a) over just the accepting paths a whose input
string is x and output string is y.

B More analysis on the effectiveness of
NFSTs

B.1 Does feeding alignments into the decoder
help?

In particular,we attribute ourmodels’ outperforming
Neuralized IBM Model 1 to the fact that a complete
history of past alignments is remembered in the
RNN state. (Wu et al., 2018) noted that in charac-
ter transduction tasks, past alignment information
seemed to barely affect decoding decisions made
afterwards. However, we empirically find that there
is performance gain by explicitly modeling past
alignments. This also shows up in our preliminary
experiments with non-input-feeding seq2seq mod-
els, which resulted in about 1% of lowered accuracy
and about 0.1 longer edit distance.

B.2 Interpretability of learned paths
The model is not required to learn transduction
rules that conform to our linguistic knowledge.
However, we expect that a well-performing one
would tend to pick up rules that resemble what we
know. To verify this, we obtain samples (listed in
Table 4) from p̂(a | x,y) using the importance
sampling algorithm described in §3.3. We find that
our NFST model has learned to align phonemes
and graphemes, generating them alternately. It has
no problem picking up obvious pairs in the English
orthography (e.g. (S, c h), and (N, n g)). We also
find evidence that the model has picked up how
context affects alignment: for example, the model
has learned that the bigram ‘gh’ is pronounced
differently in different contexts: in ‘onslaught,’
it is aligned with O in the sequence ‘augh;’ in
‘Willingham,’ it spans over two phonemes N h; and
in ‘ghezzi,’ it is aligned with the phoneme g. We
also find that our NFST has no problem learning
phoneme-grapheme alignments that span over two



283

Input / Output Paths P̂ (a | x,y)

/mAôS/
marche

�:m m:A a:ô r:S c:� h:� e:� 96.5%
�:m m:A a:ô r:� �:S c:� h:� e:� 2.5%
�:m m:A a:� �:ô r:S c:� h:� e:� 1.0%

/OnslOt/
onslaught

�:O o:n n:� �:s s:l l:O a:� u:� g:� h:t t:� 76.3%
�:O o:n n:s s:l l:O a:� u:� g:� h:t t:� 21.4%
�:O o:n n:� �:s s:l l:O a:� u:� g:� h:� �:t t:� 1.5%

/wIlINh@m/
Willingham

�:w W:I i:l l:� l:� �:I i:N n:� g:� �:h h:@ a:� �:m m:� 40.1%
�:w W:I i:l l:� l:I i:N n:� g:� �:h h:@ a:� �:m m:� 36.6%
�:w W:I i:l l:� l:I i:N n:� g:h h:@ a:� �:m m:� 7.4%

/gezI/ ghezzi �:g g:� h:e e:z z:� I:z i:� 98.8%
�:g g:e h:� e:z z:I z:� i:� 1.2%

Table 5: Most probable paths from x ◦ T ◦ y under the approximate posterior distribution.

arcs, which is beyond the capability of of ordinary
WFSTs.

C Implementation Details

C.1 Model parametrization details
As mentioned before, the type of RNN that we
use is GRU. The GRU parameterizing Gθ has
500 hidden states. The embedding sizes of tokens,
including the input symbol, output symbol and states,
and marks are all 500. During inference we make
use of proposal distributions qφ(a | T ′), where
T ′ ∈ {x ◦ T , T ◦ y,x ◦ T ◦ y}. All RNNs used
to parametrize qφ are also GRUs, with 125 hidden
states. qφ makes use of input/output embeddings
independent from Gθ, which also have size 125 in
this paper. The feed-forward networks Cx,y,xy are
parametrized by 3-layer networks, with ReLU as
the activation function of the first two layers. The
output dimension sizes of the first and second layers
are bD/2c and bD/4c, where D is the input vector
dimension size.

C.2 Training procedure details
We use stochastic gradient descent (SGD) to train
Gθ. For each example, we compute the gradient
using normalized importance sampling over an
ensemble of 512 particles (paths), the maximum
that we could compute in parallel. By using a
large ensemble, we reduce both the bias (from
normalized importance sampling) and the variance
of the gradient estimate; we found that smaller
ensembles did not work as well. Thus, we used only
one example per minibatch.
We train the ‘clamped’ proposal distribution

qφ(a | x ◦ T ◦ y) differently from the ‘free’ ones
qφ(a | x ◦ T ) and qφ(a | T ◦ y). The clamped

distribution is trained alternately with Gθ, as listed
in Algorithm 2. We evaluate on the development
dataset at the endof each epochusing theReranking
External method described in §4.3. When the EM
accuracy stops improving, we fix the parameters
of Gθ and start training qφ(x ◦ T ) and qφ(T ◦ y)
on the inclusive KL divergence objective function,
using methods described in (Lin and Eisner, 2018).
We then initialize the free distributions’ RNNs using
those of the clamped distributions. We train the free
proposal distributions for 30 epochs, and evaluate
on the development dataset at the end of each epoch.
Results from the best epochs are reported in this
paper.


