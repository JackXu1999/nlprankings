



















































Multi-Granular Sequence Encoding via Dilated Compositional Units for Reading Comprehension


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2141–2151
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

2141

Multi-Granular Sequence Encoding via Dilated Compositional Units for
Reading Comprehension

Yi Tay†, Luu Anh Tuanψ, Siu Cheung Huiφ
†ytay017@e.ntu.edu.sg

ψat.luu@i2r.a-star.edu.sg
φasschui@ntu.edu.sg

†,φSchool of Computer Science and Engineering, Nanyang Technological University
ψA*Star, Institute for Infocomm Research, Singapore

Abstract

Sequence encoders are crucial components in
many neural architectures for learning to read
and comprehend. This paper presents a new
compositional encoder for reading compre-
hension (RC). Our proposed encoder is not
only aimed at being fast but also expressive.
Specifically, the key novelty behind our en-
coder is that it explicitly models across mul-
tiple granularities using a new dilated com-
position mechanism. In our approach, gat-
ing functions are learned by modeling rela-
tionships and reasoning over multi-granular
sequence information, enabling compositional
learning that is aware of both long and short
term information. We conduct experiments
on three RC datasets, showing that our pro-
posed encoder demonstrates very promising
results both as a standalone encoder as well
as a complementary building block. Empiri-
cal results show that simple Bi-Attentive archi-
tectures augmented with our proposed encoder
not only achieves state-of-the-art / highly com-
petitive results but is also considerably faster
than other published works.

1 Introduction

Teaching machines to read, comprehend and rea-
son lives at the heart of reading comprehension
(RC) tasks (Rajpurkar et al., 2016; Lai et al., 2017;
Dunn et al., 2017; Kočiskỳ et al., 2017). In these
tasks, the goal is to answer questions based on a
given passage, effectively testing the learner’s ca-
pability to understand natural language. This has
been an extremely productive area of research in
the recent years, giving rise to many highly ad-
vanced neural network architectures (Xiong et al.,
2016; Weissenborn et al., 2017; Seo et al., 2016;
Hu et al., 2017; Shen et al., 2017; Wang and
Jiang, 2016b; Wang et al., 2018). A common de-
nominator in many of these models is the com-
positional encoder, i.e., usually a bidirectional

recurrent-based (LSTM (Hochreiter and Schmid-
huber, 1997) or GRU (Cho et al., 2014)) encoder
that sequentially parses the text sequence word-
by-word. This helps to model compositionality of
words, capturing rich and complex linguistic and
syntactic structure in language.

While the usage of recurrent encoder is often
regarded as indispensable in highly complex RC
tasks, there are still several challenges and prob-
lems pertaining to its usage in modern RC tasks.
Firstly, documents can be extremely long to the
point where running a BiRNN model across a long
document is computationally prohibitive1. This
is aggravated since RC tasks can be easily ex-
tended to reasoning over multiple long documents.
Secondly, recurrent encoders have limited access
to long term context since each word is sequen-
tially parsed. This restricts any form of multi-
sentence and intra-document reasoning from hap-
pening within compositional encoder layer.

To this end, we propose a new compositional
encoder that can either be used in place of stan-
dard RNN encoders or serve as a new module
that is complementary to existing neural archi-
tectures. Our proposed encoder leverages dilated
compositions to model relationships across mul-
tiple granularities. That is, for a given word in
the target sequence, our encoder exploits both
long-term (far) and short-term (near) information
to decide how much information to retain for
it. Intuitively, this can be interpreted as learn-
ing to compose based on modeling relationships
between word-level, phrase-level, sentence-level,
paragraph-level and so on. The output of the di-
lated composition mechanism acts as gating func-
tions, which are then used to learn compositional
representations of the input sequence.

1Many recent works tackle this issue (Yu et al., 2018;
Choi et al., 2017). However, this work presents a comple-
mentary/orthogonal approach to many of these works.



2142

A brief high-level overview to our proposed en-
coder is given as follows: Firstly, sequences are
chunked into blocks based on user-defined (hyper-
parameter) block sizes. Block sizes are often di-
lated in nature, i.e., 1, 2, 4, 10, 25, etc., in order
to capture more long-term information. Our en-
coder takes the neural bag-of-words representation
of each block size and compresses/folds all words
(that reside in each block) into a single summed
embedding. All blocks are then passed into
fully-connected layers and re-expanded/unfolded
to their original sequence lengths. For each word,
the gating vectors are then constructed by mod-
eling the relationships between all blocks that this
word resides in. As such, this can be interpreted as
a divide-and-conquer sequence encoding method.

This has several advantages. Firstly, we enable
a major speedup by avoiding either costly step-by-
step gate construction while still maintaining inter-
actions between neighboring words. As such, our
model belongs to a class of architectures which is
inspired by QRNNs (Bradbury et al., 2016) and
SRUs (Lei and Zhang, 2017). The key differ-
ence is that our gates are not constructed by con-
volution layers but explicit block-based matching
across multiple ranges, both long and short. Sec-
ondly, modeling at a long range (e.g., 25 or 50)
enables our model to look further ahead as op-
posed to only one step forward. As such, the
learned gates not only possess information about
nearby words but also a larger overview of the
context. This is in similar2 spirit to self-attention,
albeit occuring within the encoder. Thirdly, the
final gates are formed by modeling relationships
between multi-range projections (n-gram blocks),
allowing for fine-grained intra-document relation-
ships to be captured. The overall contributions of
our work are as follows:

• We propose DCU (Dilated Compositional
Units3), a new compositional encoder for
both fast and expressive sequence encoding.
We propose an overall architecture that uti-
lizes DCU within a Bi-Attentive framework
for both multiple choice and span prediction
RC tasks. DCU can be used as a standalone
(without RNNs) for fast reading and/or to-

2It is good to note that our approach explicitdly compares
across blocks of multi-granularities while self-attention com-
pares on a word-level basis.

3This model was originally known as MRU (Multi-Range
Reasoning Units) and was published on ArXiv on March
2018.

gether with RNN models (i.e., DCU-LSTM)
for more expressive reading.

• We conduct extensive experiments on three
large-scale and challenging RC datasets -
RACE (Lai et al., 2017), SearchQA (Dunn
et al., 2017) and NarrativeQA (Kočiskỳ et al.,
2017). Our model is lightweight, fast and
efficient, achieving state-of-the-art or highly
competitive performance on three datasets.

• Despite its simplicity, our model outperforms
highly complex models such as Dynamic Fu-
sion Networks (DFN) (Xu et al., 2017) on
RACE. While DFN takes approximately a
week to train, spending at least several hours
per epoch, our model converges in less than
12 hours with only 4 − 5 minutes per epoch.
Moreover, our model outperforms DFN by
2%− 6% on the RACE benchmark and other
strong baselines such as the Gated Attention
Reader by 10%. On RACE, we outperform
DFN without any recurrent and convolution
layers. Ablation studies show an improve-
ment of up to 6% when using DCU over a
LSTM/GRU encoder.

2 Dilated Compositional Units (DCU)

In this section, we describe our proposed DCU en-
coder.

2.1 Dilated Composition Mechanism
The inputs to the DCU encoder are (1) a se-
quence {w1, w2 · · ·w`}, and (2) list of ranges
{r1, r2 · · · rk} where k is the number of times the
fold/unfold operation is executed. The final output
of the encoder is a sequence of vectors which re-
tains the same dimensionality as its inputs. Figure
1 provides an illustration of the overall encoder ar-
chitecture.

2.1.1 Fold Operation
This section describes the operation for each rj .
For each rj and the input sequence, the fold oper-
ation performs the summation of every rj word.
This is essentially the NBOW (neural bag-of-
words) representation. This reduces the overall
document length to `/rj where each item in the se-
quence is the sum of every rj word. Given the new
sequence of `/rj tokens, we then pass each token
into a single layered feed-forward neural network:

w̄t = σr(Wa(wt)) + ba (1)



2143

DCU Encoder

Unfold Op

Output

Fold Op

Gating 

Dense Layer
Applied to All

Apply 
Gates

QueryPassage

Input
Encoder

Input
Encoder

DCU
Encoder

Bi-Attention

DCU
Encoder

DCU
Encoder

Start End

QueryPassage

Input
Encoder

Input
Encoder

DCU
Encoder

Bi-Attention

Choice

Input
Encoder

Bi-Attention

MLP

Score

Span Prediction Model Multiple Choice Model

Reasoning 
over
Multiple
Blocks

Simple or
Recurrent 
DCU

Figure 1: High-level overview of (1) our proposed DCU encoder (left), (2) Span Prediction Architecture (center) and (3)
Multiple Choice Architecture (right). In the DCU encoder, blocks are formed at multi-granular levels. A block embedding is
learned for each granularity. The composition gates for each word is constructed by modeling the relationships between all
NBOW (neural bag-of-words) blocks that it resides in.

where Wa ∈ Rd×d and ba ∈ Rd are the param-
eters of the fold layer. σr is the ReLU activation
function. wt is the t-th token in the sequence.

2.1.2 Unfold Operation

Given the transformed tokens w̄1, w̄2 · · · w̄`/rj ,
we then expand/unfold them into the original se-
quence length. Note that for each rj , the param-
eters Wa,ba are not shared between blocks. Fig-
ure 2 depicts the fold-unfold operation for a single
value of rj .

Proj Proj Proj Proj

Fold

Input

Transform
Unfold

Output

Figure 2: Overview of the Fold-Unfold operation for rj = 2.

Overall, the key intuition of each fold-unfold
operation is to learn representations of a block
of a single granularity (say, blocks of 2). The
main rationale for unfolding is to enable reason-
ing over multiple blocks (or granularities). This is
described in the next section.

2.1.3 Multi-Granular Reasoning
From k different calls of the Fold/Unfold opera-
tion at different block sizes, we pass the concate-
nated vector of all transformed tokens into a two
layered feed-forward neural network.

gt = F2(F1([w1t ;w
2
t ; · · ·wkt ])) (2)

where F1(.),F2(.) are feed-forward networks with
ReLU activations, i.e., σr(Wx + b). [; ] is the
concatenation operator. gt is interpreted as a gat-
ing vector learned from multiple granularities and
Equation (2) is learning the relationships between
a token’s representation at multiple hierarchies de-
pending on the values of rj . Notably, it is easy to
see that every n pairs of words will have the same
gating vector where n is the lowest value of rj . As
such, the value of the unigram, i.e., rj = 1 (projec-
tion of every single token) is critical as it prevents
identical gating vectors across the sequence.

2.2 Encoding Operation
To learn the DCU encoded representation of each
word, we consider two variations of DCU en-
coders.

2.2.1 Simple Encoding
In this variation, we use gt as a gating vector to
control the fine-grained balance between the pro-



2144

jection of each word wt in the original input doc-
ument and the original representation.

zt = tanh(Wp wt) + bp (3)
yt = σ(gt) ∗ wt + (1− σ(gt)) zt (4)

where {y1, y2, · · · y`} is the output document rep-
resentation. σ is the sigmoid function. Note that
this formulation is in similar spirit to highway net-
works (Srivastava et al., 2015). However, since
our gating function is learned via reasoning over
multi-granular sequence blocks, it captures more
compositionality and long range context. Note
that an optional and additional projection may be
applied to wt but we found that it did not yield
much empirical benefit.

2.2.2 Recurrent Encoding (DCU cell)
In the second variation, we consider a recurrent
(sequential) variant. This is in similar spirit to
QRNNs (Bradbury et al., 2016) and SRUs (Lei
and Zhang, 2017) which reduces computation cost
by pre-learning the gating vectors. The following
operations describe the operations of the recurrent
DCU cell for each time step t.

ct = gt � ct−1 + (1− gt)� zt (5)
ht = ot � ct (6)

where ct, ht are the cell and hidden states at time
step t. gt are the gates learned from the output of
the multi-range reasoning step. ot is an additional
output gate learned via applying an affine trans-
form on the input vector wt, i.e., ot = Wo(wt) +
bo. Similar to RNNs, the Recurrent DCU parses
the input sequence word-by-word. However, the
cost is significantly reduced because we do not
have expensive matrix operations that are executed
in an non-parallel fashion. Finally, the outputs of
the DCU encoder are a series of hidden vectors
{h1, h2 · · ·h`} for each word in the sequence.

3 Overall Model Architectures

This section describes the overall model architec-
tures that utilize DCU encoders. In our exper-
iments, we focus on both multiple-choice based
(RACE) and span prediction RC tasks (SearchQA,
NarrativeQA). Since the core focus of this paper is
our encoder, we briefly provide the high-level de-
tails4 of our vanilla Bi-Attentive model. The Bi-
Attentive models that are used in our experiments

4This is primarily due to the lack of space as the main
focus of this work is the DCU. Source code will be released at
hhttps://github.com/vanzytay/EMNLP18_DCU.

act as baselines, often being less complex than cur-
rent competitive models such as BiDAF (Seo et al.,
2016), AMANDA (Kundu and Ng, 2018) or DFN
(Xu et al., 2017). Figure 1 (center and right side
of the figure) provides a high-level illustration of
these architectures.

3.1 Multiple Choice Models

In the Multiple Choice (MCQ) model, there are
three types of input sequences, namely Passage
(P ), Question (Q) and Answers (Aj). The out-
put of the model (for each answer), is a score
s(P,Q,Aj) ∈ [0, 1] denoting the strength of Aj .

Input Encoding Each input sequence is passed
into first a projection layer. To enhance the input
word representations, we also include the standard
EM (exact match) binary feature to each word. In
this case, we use a three-way EM adaptation, i.e.,
EM(P,Q), EM(Q,A) andEM(P,A). The pro-
jected embeddings are then passed into a single-
layered highway network.

Compositional Encoder In our experiments,
we vary the encoder in this layer. Typical choices
of encoders in this layer are LSTMs or GRUs. We
vary this in our experiments in order to benchmark
the effectiveness of our proposed DCU encoder.
The output of this layer has the same dimensions
as its inputs (typically the hidden states of a RNN
model).

Bi-Attention Layer - This layer models the in-
teractions between P,Q and A. Let B(.) be a
standard bidirectional attention that utilizes mean-
pooling aggregation. The scoring function is the
bilinear product of the nonlinearly transformed in-
put, i.e., F (x)>i MF (y)i. We first apply B(P,Q)
to form bi-attentive P q, Qp representations. Sub-
sequently, we apply B(P q, Aj) to learn a vector
representation for each answer. A temporal sum
pooling is applied on the outputs of P qa, Apj and

concatenated to form afj ∈ R2d.

Answer Selection Let {a1, a2 · · · aNa} be the
inputs to this layer andNa is the number of answer
candidates. Motivated by the work in retrieval-
based QA (Severyn and Moschitti, 2015), we in-
clude word overlap features to each answer can-
didate. This word overlap feature is in similar
spirit to the EM feature. Each overlap operation
between two sequences returns four features. We
convert each answer vector aj into a scalar via

hhttps://github.com/vanzytay/EMNLP18_DCU


2145

afj = Softmax(W2(σr(W1([aj ]) + b1) + b2))
where W∗, b∗ and ∗ = {1, 2} are standard dense
layer parameters.

Optimization The MCQ-based model mini-
mizes the multi-class cross entropy where the
number of classes corresponds to the number of
choices.

3.2 Span Prediction Model

In the Span Prediction Model, the goal is to extract
(or predict a span s, e) where P [s : e] is the answer
to the query. As such, the key interaction in this
architecture is between P and Q. For most part,
the model architecture remains similar especially
for the input encoding layers and compositional
encoder layer. The key difference is that we reduce
the number of input sequence from three to two.

Input Encoding This follows the same design
as the MCQ-based model, albeit for two sequences
instead of three. Similarly, the two-way EM fea-
ture is added before passing into the highway
layer.

Compositional Encoder This layer remains
identical to the MCQ-based model.

Bi-Attention Layer We adopt a different bi-
attention function for span prediction. More
specifically, we use the ‘SubMultNN’ or the
‘Mult’ adaptation from (Wang and Jiang, 2016a)
(which is tuned) and compare aligned sequences
betweenP andQ to formP q, the query-dependent
passage representation.

Answer Pointer Layer In this layer, we pass
P q through a two layered compositional encoder
(which is varied similar to the compositional en-
coder layer and will be further elaborated in our
experiments.). The start pointer and end pointer
are determined by F (H1), F (H2) where H1, H2
are the hidden outputs from the first and sec-
ond encoders respectively. F (.) is a linear trans-
form, projecting each hidden state to a scalar. We
pass both of them into softmax functions to obtain
probability distributions.

Optimization Following (Seo et al., 2016;
Wang and Jiang, 2016b), we minimize the joint
cross entropy loss of the start and end probability
distributions. During inference, we follow (Wang
and Jiang, 2016b) to find the best answer span.

4 Empirical Evaluation

In this section, we report our experimental results
and comparisons against other published works.

4.1 Datasets

For our experiments, we use one challenging mul-
tiple choice MC dataset and two span-prediction
MC datasets.

RACE (Reading Comprehension from Exami-
nations) (Lai et al., 2017) is a recently proposed
dataset that is constructed from real world exam-
inations. Given a passage, there are several ques-
tions with four options each. The authors argue
that RACE is more challenging compared to pop-
ular benchmarks (e.g., SQuAD (Rajpurkar et al.,
2016)) as more multi-sentence and compositional
reasoning are required. There are two subsets
of RACE, namely RACE-M (Middle school) and
RACE-H (High school). The latter is considered
to be harder than the former.

SearchQA (Dunn et al., 2017) is a recent dataset
that emulates a real world QA system. It involves
extracting passages from search engine results and
requiring models to answer questions by reasoning
and reading these search snippets.

NarrativeQA (Kočiskỳ et al., 2017) is a recent
benchmark proposed for story-based reading com-
prehension. Different from many RC datasets, the
answers are handwritten by human annotators. We
focus on the summaries setting instead of reading
full stories since our model is targetted at standard
RC tasks.

MCQ datasets are evaluated using the standard
accuracy metric. For RACE, we train models on
the entire dataset, i.e., both RACE-M and RACE-
H, and evaluate them separately. For RACE, the
model selection is based on each subset’s respec-
tive development set. For SearchQA, we follow
(Kundu and Ng, 2018; Dunn et al., 2017) which
evaluates unigram exact match (EM) and n-gram
F1 scores. For NarrativeQA, since the answers are
human written and not constrained to a span that
can be found in the passage, the evaluation met-
rics are BLEU-1, BLEU-4, Meteor and Rouge-L
following (Kočiskỳ et al., 2017).

4.2 Competitor Methods

We describe the key competitors on each dataset.



2146

RACE The key competitors are the Stanford
Attention Reader (Stanford AR) (Chen et al.,
2016), Gated Attention Reader (GA) (Dhin-
gra et al., 2016), and Dynamic Fusion Net-
works (DFN) (Xu et al., 2017). GA incor-
porates a multi-hop attention mechanism that
helps to refine the answer representations. DFN
is an extremely complex model. It uses (1)
BiMPM’s matching functions (Wang et al., 2017c)
for extensive matching between Q,P and A,
(2) multi-hop reasoning powered by ReasoNet
(Shen et al., 2017) and (3) employs reinforce-
ment learning techniques for dynamic strategy
selection. A leaderboard for this dataset is
maintained at http://www.qizhexie.com/
data/RACE_leaderboard. Note that the cur-
rent state-of-the-art5 (Radford et al., 2018), is a
generative pre-training model trained on a large
external corpus.

SearchQA The main competitor baseline is the
AMANDA model proposed by (Kundu and Ng,
2018). AMANDA uses a multi-factor self-
attention module, along with a question focused
span prediction. AMANDA also uses BiLSTM
layers for input encoding and at the span predic-
tion layers. We also compare against the reported
ASR (Kadlec et al., 2016) baselines which were
reported in (Dunn et al., 2017).

NarrativeQA On this benchmark, we compare
with the reported baselines in (Kočiskỳ et al.,
2017). We compete on the summaries setting, in
which the baselines are a context-less sequence to
sequence (seq2seq) model, ASR (Kadlec et al.,
2016) and BiDAF (Seo et al., 2016). As per
reviewer request, we also benchmark a stronger
competitor, namely R-NET (Wang et al., 2017b)
on this benchmark. We use the open source
implementation6 at https://github.com/
HKUST-KnowComp/R-Net.

4.3 Our Methods

Across our experiments, we benchmark several
variants of our proposed DCU. The first is de-
noted as Sim-DCU which corresponds to the Sim-
ple DCU model described earlier. The model de-
noted by DCU (without any prefix) corresponds to

5This paper was not public at the time of EMNLP 2018
submission.

6Note that the authors of this repository state some dif-
ferences with the original R-NET. However, they get similar
scores on the SQuAD benchmark.

the recurrent DCU model. Finally, the final vari-
ant is the DCU-LSTM which places a DCU en-
coder layer on top of a BiLSTM layer. We report
the dimensions of the encoder as well as training
time (per epoch) for each variant. The encompass-
ing framework for DCU is the Bi-Attentive mod-
els described for MCQ-based problems and span
prediction problems. Unless stated otherwise, the
encoder in the pointer layer for span prediction
models also uses DCU. However, for the Hybrid
DCU-LSTM models, answer pointer layers use
BiLSTMs. For the RACE-dataset, we additionally
report scores of an ensemble of nine Sim-DCU
models. This is to facilitate comparison against
ensemble models of (Xu et al., 2017). We tune the
dimensionality of the DCU cell within a range of
100− 300 in denominations of 50. The results re-
ported are the best performing models on the held-
out set.

4.4 Implementation Details
We implement all models in TensorFlow (Abadi
et al., 2015). Word embeddings are initial-
ized with 300d GloVe (Pennington et al., 2014)
vectors and are not fine-tuned during training.
Dropout rate is tuned amongst {0.1, 0.2, 0.3}
on all layers including the embedding layer.
For our DCU model, we use range values of
{1, 2, 4, 10, 25}. DCU encoders are only ap-
plied on the passage and not the query. We
adopt the Adam optimizer (Kingma and Ba, 2014)
with a learning rate of 0.0003/0.001/0.001 for
RACE/SearchQA/NarrativeQA respectively. The
batch size is set to 64/256/32 accordingly. The
maximum sequence lengths are 500/200/1100 re-
spectively. For NarrativeQA, we use the Rouge-
L score to find the best approximate answer rel-
ative to the human written answer for training the
span model. All models are trained and all runtime
benchmarks are based on a TitanXP GPU.

4.5 Experimental Results on RACE
Table 1 reports our results on the RACE bench-
mark dataset. Our proposed DCU model achieves
the best result for both single models and ensem-
ble models. We outperform highly complex mod-
els such as DFN. We also pull ahead of other
recent baselines such as ElimiNet (Parikh et al.,
2018) and GA by at least 5%. The best single
model score from RACE-H and RACE-M alter-
nates between Sim-DCU and DCU. Overall, there
is a 6% improvement on the RACE-H dataset and

http://www.qizhexie.com/data/RACE_leaderboard
http://www.qizhexie.com/data/RACE_leaderboard
https://github.com/HKUST-KnowComp/R-Net
https://github.com/HKUST-KnowComp/R-Net


2147

Model RACE-M RACE-H RACE Time
Sliding Window (Lai et al., 2017) 37.3 30.4 32.2 N/A
Stanford AR (Chen et al., 2016) 44.2 43.0 43.3 N/A
GA (Dhingra et al., 2016) 43.7 44.2 44.1 N/A
ElimiNet (Parikh et al., 2018) N/A N/A 44.5 N/A
Dynamic Fusion Network (Xu et al., 2017) 51.5 45.7 47.4 ≈8 hours (1 week∗)
Bi-Attention (No Encoder) 50.6 44.0 44.9 3 min (9 hours)
Bi-Attention (250d GRU) 48.5 42.1 44.0 16 min (2 days)
Bi-Attention (250d LSTM) 50.3 40.9 43.6 18 min (2 days)
Bi-Attention (250d Sim-DCU) 57.7 47.4 50.4 4 min (12 hours)
Bi-Attention (250d DCU) 56.1 47.5 50.0 12 min (20 hours)
GA + ElimiNet (Parikh et al., 2018) N/A N/A 47.2 N/A
DFN Ensemble (x9) (Xu et al., 2017) 55.6 49.4 51.2 N/A
Bi-Attention (Sim-DCU) Ensemble (x9) 60.2 50.3 53.3 N/A

Table 1: Comparison against other published models on RACE dataset (Lai et al., 2017). Competitor result are reported
from (Lai et al., 2017; Xu et al., 2017). Best result for each category (single and ensemble) is in boldface. Last column reports
estimated training time per epoch and total time for convergence. ∗ is an estimated value that we obtain from asking the authors.

Dev Test
Model Acc F1 Acc F1 Time
TF-IDF max (Dunn et al., 2017) 13.0 N/A 12.7 N/A N/A
ASR (Kadlec et al., 2016) 43.9 24.2 41.3 22.8 N/A
AMANDA (Kundu and Ng, 2018) 48.6 57.7 46.8 56.6 ≈8∗ min
Bi-Attention† (No Encoder) 12.4 20.2 18.9 12.3 ≈17 sec
Bi-Attention† (150d BiLSTM) 40.0 51.3 38.6 49.0 ≈7 min
Bi-Attention† (300d LSTM) 40.3 48.7 38.2 46.4 ≈6 min
Bi-Attention† (300d Sim-DCU) 44.1 45.5 42.9 43.1 ≈25 sec
Bi-Attention† (300d DCU) 48.6 54.8 46.8 53.3 ≈2 min
Bi-Attention (200d Hybrid DCU-LSTM) 50.5 59.9 49.4 59.5 ≈7 min

Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
following (Kundu and Ng, 2018). All models with † use the same encoder in the answer pointer layer. ∗ is an estimate running
a replicated model with same batch size (b = 256) as our models.

1.8% improvement on the RACE-M dataset. Our
Sim-DCU model also runs at 4 minutes per itera-
tion, which is dramatically faster and simpler than
DFN or other recurrent models. We believe that
this finding highlights the importance of designing
strong and fast baselines for the task at hand.

In general, we also found that the usage of a
recurrent cell is not really crucial on this dataset
since (1) Sim-DCU and DCU can achieve com-
parable performance to each other, (2) GRU and
LSTM models do not have a competitive edge and
(3) using no encoder can achieve comparable7 per-
formance to DFN. Finally, an ensemble of Sim-
DCU models achieve state-of-the-art performance
on the RACE dataset, achieving an overall score

7Nevertheless, this suggests the importance of bench-
marking good and strong baselines since a well-tuned base-
line model can outperform DFN, a highly complicated model.

of 53.3%.

4.6 Experimental Results on SearchQA

Table 2 reports our results on the SearchQA
dataset. We draw the reader’s attention to the per-
formance of the 300d DCU encoder. We achieve
the same accuracy as AMANDA without using
any LSTM or GRU encoder. This model runs
at 2 minutes per epoch, making it 4 times more
efficient than AMANDA (estimated, with iden-
tical batch size). While AMANDA also uses
multi-factor self-attention, along with character
enhanced representations, our simple DCU en-
coder used within a mere baseline bi-attentive
framework comes close in performance. Finally,
the hybrid combination, DCU-LSTM significantly
outperforms AMANDA by 3%.

Contrary to MCQ-based datasets, we found that



2148

Model BLEU-1 BLEU-4 Meteor Rouge-L Time
Seq2Seq† 15.89 1.26 4.08 13.15 N/A
ASR† (Kadlec et al., 2016) 23.20 6.39 7.77 22.26 N/A
BiDAF† (Seo et al., 2016) 33.72 15.53 15.38 36.30 N/A
R-NETφ (Wang et al., 2017b) 34.90 20.30 18.00 36.70 N/A
Bi-Attention (300d LSTM) 31.18 15.34 14.42 32.95 ≈1 hour
Bi-Attention (150d BiLSTM) 34.22 18.22 16.19 38.32 ≈1 hour
Bi-Attention (300d Sim-DCU) 9.15 1.69 3.95 11.16 1 min
Bi-Attention (300d DCU) 33.28 16.15 15.84 36.65 18 mins
Bi-Attention (150d Hybrid DCU-LSTM) 36.55 19.79 17.87 41.44 ≈1 hour

Table 3: Experimental Results on the NarrativeQA reading comprehension challenge (Kočiskỳ et al., 2017) using summaries.
† are baselines reported by (Kočiskỳ et al., 2017). φ was obtained by running an open-source implementation of R-NET on the
benchmark.

Sim-DCU model could not achieve comparable re-
sults to the recurrent DCU. We hypothesize that
this is due to the need to predict spans. Nev-
ertheless, the 300d DCU outperforms an LSTM
encoder and remains competitive to a BiLSTM
of similar dimensionality8. We also observe
that LSTM and DCU are complementary. This
shows that stacking a DCU encoder over standard
LSTMs can give a performance boost relative to
using each encoder separately.

4.7 Experimental Results on NarrativeQA

Table 3 reports our results on the NarrativeQA
benchmark. First, we observe that 300d DCU
can achieve comparable performance with BiDAF
(Seo et al., 2016). When compared with a BiL-
STM of equal output dimensions (150d), we find
that our DCU model performs competitively, with
less than 1% deprovement across all metrics.
However, the time cost required is significantly
reduced. The performance of our model is sig-
nificantly better than 300d LSTM model while
also being significantly faster. Here, we note that
Sim-DCU does not produce reasonable results at
all, which seems to be in similar vein to results
on SearchQA, i.e., a recursive cell that processes
word-by-word is mandatory for span prediction.
However, our results show that it is not neces-
sary to construct gates in a word-by-word fashion.
Finally, DCU-LSTM significantly outperforms all
models in terms of ROUGE-L, including BiDAF
on this dataset. Performance improvement over
the vanilla BiLSTM model ranges from 1%− 3%
across all metrics, suggesting that DCU encoders

8In terms of representation and parameter size, we con-
sider a 150d BiLSTM to be equivalent to a 300d LSTM for
fair comparison.

are also effective as a complementary neural build-
ing block.

5 Related Work

A diverse collection of MC datasets such
as SQuAD (Rajpurkar et al., 2016) and
CNN/DailyMail (Hermann et al., 2015) are
readily available for benchmarking new deep
learning models. New datasets have been recently
released (Kočiskỳ et al., 2017; Joshi et al., 2017;
Lai et al., 2017; Welbl et al., 2017; Dhingra
et al., 2017; Trischler et al., 2016), claiming to
involve a greater need for going beyond simple
surface-level matching. As such, these datasets
often emphasize the extent of compositional and
multi-sentence reasoning required to tackle its
questions.

In recent years, a wide range of innovative neu-
ral solutions have also been proposed, mainly in-
volving bi-attention (Seo et al., 2016; Xiong et al.,
2016; Cui et al., 2016) and answer pointers (Wang
and Jiang, 2016b). Recent work also investigates
the notion of multi-hop reasoning (Dhingra et al.,
2016; Shen et al., 2017; Xu et al., 2017), rein-
forcement learning (Shen et al., 2017; Wang et al.,
2017a; Hu et al., 2017), pretraining on auxilliary
tasks (Radford et al., 2018; Peters et al., 2018;
McCann et al., 2017, 2018) and self-matching /
self-attention (Kundu and Ng, 2018; Wang et al.,
2017b; Yu et al., 2018). While many of these
works use BiLSTMs as standard building blocks,
(Yu et al., 2018) proposed a RNN-less model ar-
chitecture by utilizing components inspired by the
Transformer architecture (Vaswani et al., 2017).

Our work is mainly concerned with designing
an efficient encoder that is able to capture not only
compositional information but also long-range and



2149

short-range information. More specifically, our re-
current DCU encoder takes on a similar architec-
ture to Quasi-Recurrent Neural Networks (Brad-
bury et al., 2016) and Simple Recurrent Units (Lei
and Zhang, 2017). In these models, gates are
pre-learned and then applied. However, differ-
ent from existing models such as QRNNs that use
convolution layers as gates, we use block-based
fold-unfold layers for learning gates. Our model
also draws inspiration from dilation, in particu-
lar dilated RNNs (Chang et al., 2017) and dilated
convolutions (Kalchbrenner et al., 2016), that in-
tuitively help to model long-range dependencies.
Notably, our work is orthogonal to recent advances
that are targetted at speeding up the reading pro-
cess. Such works include residual dilated convo-
lutions (Wu et al., 2017), self-attention (Yu et al.,
2018) and coarse-to-fine grained paradigm (Choi
et al., 2017). However, while speed is one of the
clear benefits of this work, our work is the first to
introduce the idea of block-based multi-granular
reasoning. We believe that this new building block
is complementary/useful to the RC task in general.

6 Conclusion and Future Work

We proposed a novel neural architecture, the DCU
encoder and an overall bi-attentive model for both
MCQ-based and span prediction MC tasks. We
apply it to three MC datasets and achieve com-
petitive performance on all without the use of
recurrent and convolution layers. Our proposed
method outperforms DFN, an extremely complex
model, without using any LSTM/GRU layer. We
also remain competitive to AMANDA and BiDAF
without any LSTM/GRU. While our proposed en-
coder demonstrates promise on reasoning and un-
derstanding natural language, we believe that our
encoder is generalizable to other domains beyond
reading comprehension. However, we defer this
prospect to future work.

References
Martı́n Abadi, Ashish Agarwal, Paul Barham, Eugene

Brevdo, Zhifeng Chen, Craig Citro, Greg S. Cor-
rado, Andy Davis, Jeffrey Dean, Matthieu Devin,
Sanjay Ghemawat, Ian Goodfellow, Andrew Harp,
Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal
Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh
Levenberg, Dan Mané, Rajat Monga, Sherry Moore,
Derek Murray, Chris Olah, Mike Schuster, Jonathon
Shlens, Benoit Steiner, Ilya Sutskever, Kunal Tal-
war, Paul Tucker, Vincent Vanhoucke, Vijay Va-

sudevan, Fernanda Viégas, Oriol Vinyals, Pete
Warden, Martin Wattenberg, Martin Wicke, Yuan
Yu, and Xiaoqiang Zheng. 2015. TensorFlow:
Large-scale machine learning on heterogeneous sys-
tems. Software available from tensorflow.org.
http://tensorflow.org/.

James Bradbury, Stephen Merity, Caiming Xiong, and
Richard Socher. 2016. Quasi-recurrent neural net-
works. CoRR abs/1611.01576.

Shiyu Chang, Yang Zhang, Wei Han, Mo Yu, Xiaox-
iao Guo, Wei Tan, Xiaodong Cui, Michael Witbrock,
Mark A Hasegawa-Johnson, and Thomas S Huang.
2017. Dilated recurrent neural networks. In Ad-
vances in Neural Information Processing Systems.
pages 76–86.

Danqi Chen, Jason Bolton, and Christopher D Man-
ning. 2016. A thorough examination of the
cnn/daily mail reading comprehension task. arXiv
preprint arXiv:1606.02858 .

Kyunghyun Cho, Bart Van Merriënboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using rnn encoder-decoder
for statistical machine translation. arXiv preprint
arXiv:1406.1078 .

Eunsol Choi, Daniel Hewlett, Jakob Uszkoreit, Illia
Polosukhin, Alexandre Lacoste, and Jonathan Be-
rant. 2017. Coarse-to-fine question answering for
long documents. In Proceedings of the 55th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers). volume 1, pages
209–220.

Yiming Cui, Zhipeng Chen, Si Wei, Shijin Wang,
Ting Liu, and Guoping Hu. 2016. Attention-over-
attention neural networks for reading comprehen-
sion. arXiv preprint arXiv:1607.04423 .

Bhuwan Dhingra, Hanxiao Liu, Zhilin Yang,
William W Cohen, and Ruslan Salakhutdinov.
2016. Gated-attention readers for text comprehen-
sion. arXiv preprint arXiv:1606.01549 .

Bhuwan Dhingra, Kathryn Mazaitis, and William W
Cohen. 2017. Quasar: Datasets for question an-
swering by search and reading. arXiv preprint
arXiv:1707.03904 .

Matthew Dunn, Levent Sagun, Mike Higgins, Ugur
Guney, Volkan Cirik, and Kyunghyun Cho. 2017.
Searchqa: A new q&a dataset augmented with
context from a search engine. arXiv preprint
arXiv:1704.05179 .

Karl Moritz Hermann, Tomas Kocisky, Edward
Grefenstette, Lasse Espeholt, Will Kay, Mustafa Su-
leyman, and Phil Blunsom. 2015. Teaching ma-
chines to read and comprehend. In Advances in Neu-
ral Information Processing Systems. pages 1693–
1701.

http://tensorflow.org/
http://tensorflow.org/
http://tensorflow.org/
http://tensorflow.org/


2150

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation
9(8):1735–1780.

Minghao Hu, Yuxing Peng, and Xipeng Qiu. 2017.
Mnemonic reader for machine comprehension.
arXiv preprint arXiv:1705.02798 .

Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke
Zettlemoyer. 2017. Triviaqa: A large scale distantly
supervised challenge dataset for reading comprehen-
sion. arXiv preprint arXiv:1705.03551 .

Rudolf Kadlec, Martin Schmid, Ondrej Bajgar, and
Jan Kleindienst. 2016. Text understanding with
the attention sum reader network. arXiv preprint
arXiv:1603.01547 .

Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan,
Aaron van den Oord, Alex Graves, and Koray
Kavukcuoglu. 2016. Neural machine translation in
linear time. arXiv preprint arXiv:1610.10099 .

Diederik P. Kingma and Jimmy Ba. 2014. Adam:
A method for stochastic optimization. CoRR
abs/1412.6980.

Tomáš Kočiskỳ, Jonathan Schwarz, Phil Blunsom,
Chris Dyer, Karl Moritz Hermann, Gábor Melis,
and Edward Grefenstette. 2017. The narrativeqa
reading comprehension challenge. arXiv preprint
arXiv:1712.07040 .

Souvik Kundu and Hwee Tou Ng. 2018. A question-
focused multi-factor attention network for question
answering .

Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang,
and Eduard Hovy. 2017. Race: Large-scale reading
comprehension dataset from examinations. arXiv
preprint arXiv:1704.04683 .

Tao Lei and Yu Zhang. 2017. Training rnns as fast as
cnns. arXiv preprint arXiv:1709.02755 .

Bryan McCann, James Bradbury, Caiming Xiong, and
Richard Socher. 2017. Learned in translation: Con-
textualized word vectors. In Advances in Neural In-
formation Processing Systems. pages 6297–6308.

Bryan McCann, Nitish Shirish Keskar, Caiming Xiong,
and Richard Socher. 2018. The natural language de-
cathlon: Multitask learning as question answering.
arXiv preprint arXiv:1806.08730 .

Soham Parikh, Ananya Sai, Preksha Nema, and
Mitesh M Khapra. 2018. Eliminet: A model
for eliminating options for reading com-
prehension with multiple choice questions.
https://openreview.net/forum?id=B1bgpzZAZ.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. In Proceedings of the 2014
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP 2014, October 25-29,
2014, Doha, Qatar, A meeting of SIGDAT, a Special
Interest Group of the ACL. pages 1532–1543.

Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word rep-
resentations. arXiv preprint arXiv:1802.05365 .

Alec Radford, Karthik Narasimhan, Tim Salimans, and
Ilya Sutskever. 2018. Improving language under-
standing by generative pre-training .

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. Squad: 100,000+ questions
for machine comprehension of text. arXiv preprint
arXiv:1606.05250 .

Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and
Hannaneh Hajishirzi. 2016. Bidirectional attention
flow for machine comprehension. arXiv preprint
arXiv:1611.01603 .

Aliaksei Severyn and Alessandro Moschitti. 2015.
Learning to rank short text pairs with convolutional
deep neural networks. In Proceedings of the 38th
International ACM SIGIR Conference on Research
and Development in Information Retrieval, Santi-
ago, Chile, August 9-13, 2015. pages 373–382.

Yelong Shen, Po-Sen Huang, Jianfeng Gao, and
Weizhu Chen. 2017. Reasonet: Learning to stop
reading in machine comprehension. In Proceedings
of the 23rd ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining. ACM,
pages 1047–1055.

Rupesh Kumar Srivastava, Klaus Greff, and Jürgen
Schmidhuber. 2015. Highway networks. CoRR
abs/1505.00387.

Adam Trischler, Tong Wang, Xingdi Yuan, Justin Har-
ris, Alessandro Sordoni, Philip Bachman, and Ka-
heer Suleman. 2016. Newsqa: A machine compre-
hension dataset. arXiv preprint arXiv:1611.09830 .

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems. pages 6000–6010.

Shuohang Wang and Jing Jiang. 2016a. A compare-
aggregate model for matching text sequences. CoRR
abs/1611.01747.

Shuohang Wang and Jing Jiang. 2016b. Machine com-
prehension using match-lstm and answer pointer.
arXiv preprint arXiv:1608.07905 .

Shuohang Wang, Mo Yu, Shiyu Chang, and Jing
Jiang. 2018. A co-matching model for multi-
choice reading comprehension. arXiv preprint
arXiv:1806.04068 .

Shuohang Wang, Mo Yu, Xiaoxiao Guo, Zhiguo Wang,
Tim Klinger, Wei Zhang, Shiyu Chang, Gerald
Tesauro, Bowen Zhou, and Jing Jiang. 2017a. R3:
Reinforced reader-ranker for open-domain question
answering. arXiv preprint arXiv:1709.00023 .

https://openreview.net/forum?id=B1bgpzZAZ
https://openreview.net/forum?id=B1bgpzZAZ
https://openreview.net/forum?id=B1bgpzZAZ
https://openreview.net/forum?id=B1bgpzZAZ


2151

Wenhui Wang, Nan Yang, Furu Wei, Baobao Chang,
and Ming Zhou. 2017b. Gated self-matching net-
works for reading comprehension and question an-
swering. In Proceedings of the 55th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers). volume 1, pages 189–198.

Zhiguo Wang, Wael Hamza, and Radu Florian. 2017c.
Bilateral multi-perspective matching for natural lan-
guage sentences. In Proceedings of the Twenty-Sixth
International Joint Conference on Artificial Intelli-
gence, IJCAI 2017, Melbourne, Australia, August
19-25, 2017. pages 4144–4150.

Dirk Weissenborn, Georg Wiese, and Laura Seiffe.
2017. Making neural qa as simple as possible but
not simpler. arXiv preprint arXiv:1703.04816 .

Johannes Welbl, Pontus Stenetorp, and Sebastian
Riedel. 2017. Constructing datasets for multi-hop
reading comprehension across documents. arXiv
preprint arXiv:1710.06481 .

Felix Wu, Ni Lao, John Blitzer, Guandao Yang,
and Kilian Weinberger. 2017. Fast reading
comprehension with convnets. arXiv preprint
arXiv:1711.04352 .

Caiming Xiong, Victor Zhong, and Richard Socher.
2016. Dynamic coattention networks for question
answering. CoRR abs/1611.01604.

Yichong Xu, Jingjing Liu, Jianfeng Gao, Yelong Shen,
and Xiaodong Liu. 2017. Towards human-level ma-
chine reading comprehension: Reasoning and in-
ference with multiple strategies. arXiv preprint
arXiv:1711.04964 .

Adams Wei Yu, David Dohan, Quoc Le, Thang
Luong, Rui Zhao, and Kai Chen. 2018. Fast
and accurate reading comprehension by combin-
ing self-attention and convolution. In Interna-
tional Conference on Learning Representations.
https://openreview.net/forum?id=B14TlG-RW.

https://openreview.net/forum?id=B14TlG-RW
https://openreview.net/forum?id=B14TlG-RW
https://openreview.net/forum?id=B14TlG-RW
https://openreview.net/forum?id=B14TlG-RW

