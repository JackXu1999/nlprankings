



















































Hidden Softmax Sequence Model for Dialogue Structure Analysis


Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 2063–2072,
Berlin, Germany, August 7-12, 2016. c©2016 Association for Computational Linguistics

Hidden Softmax Sequence Model for Dialogue Structure Analysis

Zhiyang He1, Xien Liu2, Ping Lv2, Ji Wu1
1Department of Electronic Engineering, Tsinghua University, Beijing, China
2Tsinghua-iFlytek Joint Laboratory for Speech Technology, Beijing, China

{zyhe ts, xeliu, luping ts, wuji ee}@mail.tsinghua.edu.cn

Abstract

We propose a new unsupervised learning
model, hidden softmax sequence model
(HSSM), based on Boltzmann machine for
dialogue structure analysis. The model
employs three types of units in the hidden
layer to discovery dialogue latent struc-
tures: softmax units which represent latent
states of utterances; binary units which
represent latent topics specified by dia-
logues; and a binary unit that represents
the global general topic shared across the
whole dialogue corpus. In addition, the
model contains extra connections between
adjacent hidden softmax units to formu-
late the dependency between latent states.
Two different kinds of real world dialogue
corpora, Twitter-Post and AirTicketBook-
ing, are utilized for extensive comparing
experiments, and the results illustrate that
the proposed model outperforms sate-of-
the-art popular approaches.

1 Introduction

Dialogue structure analysis is an important and
fundamental task in the natural language process-
ing domain. The technology provides essential
clues for solving real-world problems, such as pro-
ducing dialogue summaries (Murray et al., 2006;
Liu et al., 2010), controlling conversational agents
(Wilks, 2006), and designing interactive dialogue
systems (Young, 2006; Allen et al., 2007) etc.
The study of modeling dialogues always assumes
that for each dialogue there exists an unique latent
structure (namely dialogue structure), which con-
sists of a series of latent states.1

1Also called dialogue acts or speech acts in some past
work. In this paper, for simplicity we will only use the term
“latent state” to describe the sequential dialogue structure.

Some past works mainly rely on supervised or
semi-supervised learning, which always involve
extensive human efforts to manually construct la-
tent state inventory and to label training samples.
Cohen et al. (2004) developed an inventory of la-
tent states specific to E-mail in an office domain
by inspecting a large corpus of e-mail. Jeong et
al. (2009) employed semi-supervised learning to
transfer latent states from labeled speech corpora
to the Internet media and e-mail. Involving exten-
sive human efforts constrains scaling the training
sample size (which is essential to supervised learn-
ing) and application domains.

In recent years, there has been some work
on modeling dialogues with unsupervised learn-
ing methods which operate only on unlabeled ob-
served data. Crook et al. (2009) employed Dirich-
let process mixture clustering models to recog-
nize latent states for each utterance in dialogues
from a travel-planning domain, but they do not
inspect dialogues’ sequential structure. Choti-
mongkol (2008) proposed a hidden Markov model
(HMM) based dialogue analysis model to study
structures of task-oriented conversations from in-
domain dialogue corpus. More recently, Ritter et
al. (2010) extended the HMM based conversa-
tion model by introducing additional word sources
for topic learning process. Zhai et al. (2014)
assumed words in an utterance are emitted from
topic models under HMM framework, and topics
were shared across all latent states. All these dia-
logue structure analysis models are directed gener-
ative models, in which the HMMs, language mod-
els and topic models are combined together.

In this study, we attempt to develop a Boltz-
mann machine based undirected generative
model for dialogue structure analysis. As for
the document modeling using undirected gener-
ative model, Hinton and Salakhutdinov (2009)
proposed a general framework, replicated soft-

2063



max model (RSM), for topic modeling based
on restricted Boltzmann machine (RBM). The
model focuses on the document-level topic anal-
ysis, it cannot be applied for the structure analy-
sis. We propose a hidden softmax sequence model
(HSSM) for the dialogue modeling and structure
analysis. HSSM is a two-layer special Boltzmann
machine. The visible layer contains softmax units
used to model words in a dialogue, which are the
same with the visible layer in RSM (Hinton and
Salakhutdinov, 2009). However, the hidden layer
has completely different design. There are three
kinds of hidden units: softmax hidden units, which
is utilized for representing latent states of dia-
logues; binary units used for representing dialogue
specific topics; and a special binary unit used for
representing the general topic of the dialogue cor-
pus. Moreover, unlike RSM whose hidden binary
units are conditionally independent when visible
units are given, HSSM has extra connections uti-
lized to formulate the dependency between adja-
cent softmax units in the hidden layer. The con-
nections are the latent states of two adjacent utter-
ances. Therefore, HSSM can be considered as a
special Boltzmann machine.

The remainder of this paper is organized as fol-
lows. Section 2 introduces two real world dia-
logue corpora utilized in our experiments. Section
3 describes the proposed hidden softmax sequence
model. Experimental results and discussions are
presented in Section 4. Finally, Section 5 presents
our conclusions.

2 Data Set

Two different datasets are utilized to test the ef-
fectiveness of our proposed model: a corpus of
post conversations drawn from Twitter (Twitter-
Post), and a corpus of task-oriented human-human
dialogues in the airline ticket booking domain
(AirTicketBooking).

2.1 Twitter-Post

Conversations in Twitter are carried out by re-
plying or responding to specific posts with short
140-character messages. The post length restric-
tion makes Twitter keep more chat-like interac-
tions than blog posts. The style of writing used
on Twitter is widely varied, highly ungrammatical,
and often with spelling errors. For example, the
terms “be4”, “b4”, and “bef4” are always appeared
in the Twitter posts to represent the word “before”.

Here, we totally collected about 900, 000 raw
Twitter dialogue sessions. The majority of conver-
sation sessions are very short; and the frequencies
of conversation session lengths follow a power law
relationship as described in (Ritter et al., 2010).
For simplicity , in the data preprocessing stage
non-English sentences were dropped; and non-
English characters, punctuation marks, and some
non-meaning tokens (such as “&”) were also fil-
tered from dialogues. We filtered short Twitter di-
alogue sessions and randomly sampled 5,000 di-
alogues (the numbers of utterances in dialogues
rang from 5 to 25) to build the Twitter-Post dataset.

2.2 AirTicketBooking

The AirTicketBooking corpus consists of a set of
task-oriented human-human mandarin dialogues
from an airline ticket booking service center. The
manual transcripts of the speech dialogues are uti-
lized in our experiments. In the dataset, there is
always a relative clear structure underlying each
dialogue. A dialogue often begins with a cus-
tomer’s request about airline ticket issues. And
the service agent always firstly checks the client’s
personal information, such as name, phone num-
ber and credit card numberm, etc. Then the agent
starts to deal with the client’s request. We totally
collected 1,890 text-based dialogue sessions ob-
taining about 40,000 conversation utterances with
length ranging from 15 to 100.

3 Dialogue Structure Analysis

3.1 Model Design

Figure 1: Hidden layer that consists of different
types of latent variables

We design an undirected generative model
based on Boltzmann machine. As we known, di-
alogue structure analysis models are always based
on an underlying assumption: each utterance in
the dialogues is generated from one latent state,
which has a causal effect on the words. For in-
stance, an utterance in AirTicketBooking dataset,
“Tomorrow afternoon, about 3 o’clock” corre-

2064



sponds to the latent state “Time Information”.
However, by carefully examining words in dia-
logues we can observe that not all words are gener-
ated from the latent states (Ritter et al., 2010; Zhai
and Williams, 2014). There are some words rele-
vant to a global or background topic shared across
dialogues. For example, “about” and “that” be-
long to a global (general English) topic. Some
other words in a dialogue may be strongly re-
lated to the dialogue specific topic. For exam-
ple, “cake”, “toast” and “pizza” may appear in a
Twitter dialogue with respect to a specific topic,
“food”. From the perspective of generative model,
we can also consider that words in a dialogue are
generated by the mixture model of latent states, a
global/background topic, and a dialogue specific
topic. Therefore, there are three kinds of units
in the hidden layer of our proposed model, which
are displayed in Figure 1. hφ is a softmax unit,
which indicates the latent state for a utterance. hψ
and hξ represent the general topic, and the dia-
logue specific topic, respectively. For the visible
layer, we utilize the softmax units to model words
in each utterance, which is the same with the ap-
proach in RSM (Hinton and Salakhutdinov, 2009).
In Section 3.2, We propose a basic model based on
Boltzmann machine to formulate each word in ut-
terances of dialogues.

A dialogue can be abstractly viewed as a se-
quence of latent states in a certain reasonable or-
der. Therefore, formulating the dependency be-
tween latent states is another import issue for dia-
logue structure analysis. In our model, we assume
that each utterance’s latent state is dependent on
its two neighbours. So there exist connections be-
tween each pair of adjacent hidden softmax units
in the hidden layer. The details of the model will
be presented in Section 3.3.

3.2 HSM: Hidden Softmax Model

Notation Explanation
K dictionary size
J number of latent states
V observed visibles representing words in dialogues
b bias term of V
hφ latent variables representing latent states
hψ latent variable representing corpus general topic
hξ latent variables representing dialogue specific topics
aφ bias terms of hφ

aψ bias term of hψ

aξ bias terms of hξ

Wφ weights connecting hφ to V
Wψ weights connecting hψ to V
Wξ weights connecting hξ to V
F, Fs, Fe weights between hidden softmax units

Table 1: Definition of notations.

Words of utterance 1

... ... ...

Words of utterance 2 Words of utterance 3

Utterance 1 Utterance 2 Utterance 3

Figure 2: Hidden Softmax Model. The bottom
layer are softmax visible units and the top layer
consists of three types of hidden units: softmax
hidden units used for representing latent states, a
binary stochastic hidden unit used for represent-
ing the dialogue specific topic, and a special bi-
nary stochastic hidden unit used for representing
corpus general topic. Upper: The model for a di-
alogue session containing three utterances. Con-
nection lines in the same color related to a latent
state represent the same weight matrix. Lower:
A different interpretation of the Hidden Softmax
Model, in which Dr visible softmax units in the
rth utterance are replaced by one single multino-
mial unit which is sampled Dr times.

Table 1 summarizes important notations utilized
in this paper. Before introducing the ultimate
learning model for dialogue structure analysis, we
firstly discuss a simplified version, Hidden Soft-
max Model (HSM), which is based on Boltzmann
machine and assumes that the latent variables are
independent given visible units. HSM has a two-
layer architecture as shown in Figure 2. The en-
ergy of the state {V,hφ,hψ,hξ} is defined as fol-
lows:

E(V, hφ, hψ, hξ) =Ēφ(V, hφ) + Ēψ(V, hψ)

+ Ēξ(V, hξ) + C(V),
(1)

where Ēφ(V,hφ), Ēψ(V,hψ) and Ēξ(V,hξ) are
sub-energy functions related to hidden variables
hφ, hψ, and hξ, respectively. C(V) is the shared
visible units bias term. Suppose K is the dictio-
nary size, Dr is the rth utterance size (i.e. the

2065



number of words in the rth utterance), and R is
the number of utterances in the a dialogue.

For each utterance vr(r = 1, .., R) in the dia-
logue session we have a hidden variable vector hφr
(with size of J ) as a latent state of the utterance,
the sub-energy function Ēφ(V,hφ) is defined by

Ēφ(V, hφ) =−
R∑
r=1

J∑
j=1

Dr∑
i=1

K∑
k=1

hφrjW
φ
rjikvrik

−
R∑
r=1

J∑
j=1

hφrja
φ
rj ,

(2)

where vrik = 1 means the ith visible unit vri in the
rth utterance takes on kth value, hφrj = 1 means the

rth softmax hidden units takes on jth value, and aφrj
is the corresponding bias. W φrjik is a symmetric
interaction term between visible unit vri that takes
on kth value and hidden variable hφr that takes on
jth value.

The sub-energy function Ēψ(V,hψ), related to
the global general topic of the corpus, is defined
by

Ēψ(V, hψ) = −
R∑
r=1

Dr∑
i=1

K∑
k=1

hψWψrikvrik − hψaψ. (3)

The sub-energy function Ēξ(V,hξ) corresponds to
the dialogue specific topic, and is defined by

Ēξ(V, hξ) = −
R∑
r=1

Dr∑
i=1

K∑
k=1

hξW ξrikvrik − hξaξ. (4)

Wψrik in Eq. (3) and W
ξ
rik in Eq. (4) are two sym-

metric interaction terms between visible units and
the corresponding hidden units, which are similar
to W φrjik in (2); a

ψ and aξ are the corresponding
biases. C(V) is defined by

C(V) = −
R∑
r=1

Dr∑
i=1

K∑
k=1

vrikbrik, (5)

where brik is the corresponding bias.
The probability that the model assigns to a vis-

ible binary matrix V = {v1, v2, ..., vD} (where
D =

∑R
r=1Dr is the dialogue session size) is

P (V) = 1Z
∑

hφ, hψ,hξ

exp(−E(V, hφ, hψ, hξ))

Z =
∑

V

∑
hφ, hψ,hξ

exp(−E(V, hφ, hψ, hξ),
(6)

where Z is known as the partition function or nor-
malizing constant.

In our proposed model, for each word in the
document we use a softmax unit to represent it.
For the sake of simplicity, assume that the order
of words in an utterance is ignored. Therefore, all
of these softmax units can share the same set of
weights that connect them to hidden units, thus the
visible bias term C(V) and the sub-energy func-
tions Ēφ(V,hφ), Ēψ(V,hψ) and Ēξ(V,hξ) in Eq.
(1) can be redefined as follows:

Ēφ(V, hφ) =−
R∑
r=1

J∑
j=1

K∑
k=1

hφrjW
φ
jkv̂rk

−
R∑
r=1

(Dr

J∑
j=1

hφrja
φ
j )

(7)

Ēψ(V, hψ) = −
K∑
k=1

hψWψk v̂k −Dhψaψ (8)

Ēξ(V, hξ) = −
K∑
k=1

hξW ξk v̂k −Dhξaξ (9)

C(V) = −
K∑
k=1

v̂kbk, (10)

where v̂rk =
∑Dr

i=1 vrik denotes the count for the
kth word in the rth utterance of the dialogue, v̂k =∑R

r=1 v̂rk is the count for the k
th word in whole

dialogue session. Dr and D (D =
∑R

r=1Dr)
are employed as the scaling parameters, which can
make hidden units behave sensibly when dealing
with dialogues of different lengths (Hinton and
Salakhutdinov, 2009).

The conditional distributions are given by soft-
max and logistic functions:

P (hφrj = 1|V) =
exp(

∑K
k=1 W

φ
jkv̂rk +Dra

φ
j )∑J

j′=1 exp(
∑K
k=1 W

φ
j′kv̂rk +Dra

φ
j′)
(11)

P (hψ = 1|V) = σ(
K∑
k=1

Wψk v̂k +Da
ψ) (12)

P (hξ = 1|V) = σ(
K∑
k=1

W ξk v̂k +Da
ξ) (13)

P (vrik = 1|hφ, hψ, hξ) =
exp(

∑J
j=1 h

φ
rjW

φ
jk + h

ψWψk + h
ξW ξk + bk)∑K

k′=1 exp(
∑J
j=1 h

φ
rjW

φ
jk′ + h

ψWψk′ + h
ξW ξk′ + bk′)

,

(14)

where σ(x) = 1/(1 + exp(−x)) is the logistic
function.

2066



3.3 HSSM: Hidden Softmax Sequence Model

In this section, we consider the dependency be-
tween the adjacent latent states of utterances,
and extend the HSM to hidden softmax sequence
model (HSSM), which is displayed in Figure 3.
We define the energy of the state {V,hφ,hψ,hξ}
in HSSM as follows:

E(V, hφ, hψ, hξ) =Ēφ(V, hφ) + Ēψ(V, hψ) + Ēξ(V, hξ)

+ C(V) + ĒΦ(hφ, hφ),
(15)

where C(V), Ēφ(V,hφ), Ēψ(V,hψ) and
Ēξ(V,hξ) are the same with that in HSM.
The last term ĒΦ(hφ,hφ) is utilized to formulate
the dependency between latent variables hφ,
which is defined as follows:

ĒΦ(hφ, hφ) =−
J∑
q=1

hφsF
s
q h

φ
1q −

J∑
q=1

hφRqF
e
q h

φ
e

−
R−1∑
r=1

J∑
j=1

J∑
q=1

hφrjFjqh
φ
r+1,q,

(16)

where hφs and h
φ
e are two constant scalar variables

(hφs ≡ 1, hφe ≡ 1), which represent the virtual
beginning state unit and ending state unit of a di-
alogue. F s is a vector with size J , and its ele-
ments measure the dependency between hφs and
the latent softmax units of the first utterance. F e

also contains J elements, and in contrast to F s,
F e represents the dependency measure between
hφe and the latent softmax units of the last utter-
ance. F is a symmetric matrix for formulating de-
pendency between each two adjacent hidden units
pair (hφr , h

φ
r+1), r = 1, ..., R− 1.

Utterance 1 Utterance 2 Utterance 3

Figure 3: Hidden softmax sequence model. A con-
nection between each pair of adjacent hidden soft-
max units is added to formulate the dependency
between the two corresponding latent states.

3.4 Parameter Learning
Exact maximum likelihood learning in the pro-
posed model is intractable. “Contrastive Diver-
gence” (Hinton, 2002) can be used for HSM’s
learning, however, it can not be utilized for HSSM,
because the hidden-to-hidden interaction term,
{F, F s, F e}, result in the intractability when ob-
taining exact samples from the conditional distri-
bution P (hφrj = 1|V), r = [1, R], j ∈ [1, J ].
We use the mean-field variational inference (Hin-
ton and Zemel, 1994; Neal and Hinton, 1998;
Jordan et al., 1999) and a stochastic approxima-
tion procedure (SAP) (Tieleman, 2008) to esti-
mate HSSM’s parameters. The variational learn-
ing is utilized to get the data-dependent expecta-
tions, and SAP is utilized to estimate the model’s
expectation. The log-likelihood of the HSSM has
the following variational lower bound:

logP (V; θ) ≥
∑

h

Q(h) logP (V, h; θ) +H(Q). (17)

Q(h) can be any distribution of h in theory.
θ = {W φ,Wψ,W ξ, F, F s, F e} (the bias terms
are omitted for clarity) are the model parameters.
h = {hφ,hψ,hξ} represent all the hidden vari-
ables. H(·) is the entropy functional. In varia-
tional learning, we try to find parameters that min-
imize the Kullback-Leibler divergences between
Q(h) and the true posterior P (h|V; θ). A naive
mean-field approach can be chosen to obtain a
fully factorized distribution for Q(h):

Q(h) =

[
R∏
r=1

q(hφ)

]
q(hψ) q(hξ), (18)

where q(hφrj = 1) = µ
φ
rj , q(h

ψ = 1) = µψ,
q(hξ = 1) = µξ. µ = {µφ, µψ, µξ} are the pa-
rameters of Q(h). Then the lower bound on the
log-probability logP (V; θ) has the form:

logP (V; θ) ≥− Ēφ(V, µφ)− Ēψ(V, µψ)− Ēξ(V, µξ)
− C(V)− ĒΦ(µφ, µφ)− logZ,

(19)

where Ēφ(V, µφ), Ēψ(V, µψ), Ēξ(V, µξ), and
ĒΦ(µφ, µφ) have the same forms, by replacing µ
with h, as Eqs. (7), (8), (9), and (16), respectively.

We can maximize this lower bound with respect
to parameters µ for fixed θ, and obtain the mean-
field fixed-point equations:

µφrj =

exp(
∑K
k=1 W

φ
jkv̂rk +Dra

φ
j +D

j
prev +D

j
next − 1)∑J

j′=1 exp(
∑K
k=1 W

φ
j′kv̂rk +Dra

φ
j′ +D

j′
prev +D

j′
next − 1)

,

(20)

2067



µψ = σ(

K∑
k=1

Wψk v̂k +Da
ψ) (21)

µξ = σ(

K∑
k=1

W ξk v̂k +Da
ξ), (22)

where Djprev and D
j
next are two terms relevant to

the derivative of the RHS of Eq. (19) with respect
to µφrj , defined by

Djprev =
{
F sj , r = 1∑J
q=1 µ

φ
r−1,qFqj , r > 1

Djnext =
{∑J

q=1 Fjqµ
φ
r+1,q, r < R.

F ej , r = R

The updating of µ can be carried out iteratively
until convergence. Then, (V,µ) can be considered
as a special “state” of HSSM, thus the SAP can be
applied to update the model’s parameters, θ, for
fixed (V,µ).

4 Experiments and Discussions

It’s not easy to evaluate the performance of a dia-
logue structure analysis model. In this study, we
examined our model via qualitative visualization
and quantitative analysis as done in (Ritter et al.,
2010; Zhai and Williams, 2014). We implemented
five conventional models to conduct an extensive
comparing study on the two corpora: Twitter-Post
and AirTicketBooking. Conventional models in-
clude: LMHMM (Chotimongkol, 2008), LMH-
MMS (Ritter et al., 2010), TMHMM, TMHMMS,
and TMHMMSS (Zhai and Williams, 2014). In
our experiments, for each corpus we randomly se-
lect 80% dialogues for training, and use the rest
20% for testing. We select three different num-
ber (10, 20 and 30) of latent states to evaluate all
the models. In TMHMM, TMHMMS and TMH-
MMSS, the number of “topics” in the latent states
and a dialogue is a hyper-parameter. We con-
ducted a series of experiments with varying num-
bers of topics, and the results illustrated that 20
is the best choice on the two corpora. So, for all
the following experimental results of TMHMM,
TMHMMS and TMHMMSS, the corresponding
topic configurations are set to 20.

The number of estimation iterations for all the
models on training sets is set to 10,000; and on
held-out test sets, the numver of iterations for in-
ference is set to 1000. In order to speed-up the

learning of HSSM, datasets are divided into mini-
batches, each has 15 dialogues. In addition, the
learning rate and momentum are set to 0.1 and 0.9,
respectively.

4.1 Qualitative Evaluation

Dialogues in Twitter-Post always begin with three
latent states: broadcasting what they (Twitter
users) are doing now (“Status”), broadcasting an
interesting link or quote to their followers (“Ref-
erence Broadcast”), or asking a question to their
followers (“Question to Followers”).2 We find that
structures discoverd by HSSM and LMHMMS
with 10 latent states are most reasonable to inter-
pret. For example, after the initiating state (“Sta-
tus”, “Reference Broadcast”, or “Question to Fol-
lowers”), it was often followed a “Reaction” to
“Reference Broadcast” (or “Status”), or a “Com-
ment” to “Status”, or a “Question” to “Status” (
“Reference Broadcast”, or “Question to Follow-
ers”’) etc. Compared with LMHMMS, besides ob-
taining similar latent states, HSSM exhibits pow-
erful ability in learning sequential dependency re-
lationship between latent states. Take the follow-
ing simple Twitter dialogue session as an example:
: rt i like katy perry lt lt we see tht lol

: lol gd morning

: lol gd morning how u

: i’m gr8 n urself

: i’m good gettin ready to head out

: oh ok well ur day n up its cold out here

...

LMHMMS labelled the second utterance (“lol gd
morning ”) and the third utterance (“lol good
morning how u ” ) into the same latent state, while
HSSM treats them as two different latent states
(Though they both have almost the same words).
The result is reasonable: the first “gd morning” is
a greeting, while the second “gd morning” is a re-
sponse.

For AirTicketBooking dataset, the state-
transition diagram generated with our model
under the setting of 10 latent states is presented
in Figure 4. And several utterance examples
corresponding to the latent staes are also showed
in Table 2. In general, conversations begin
with sever agent’s short greeting, such as “Hi,
very glad to be of service.”, and then transit to
checking the passenger’s identity information or

2For simplicity and readability in consistent, we follow
the same latent state names used in (Ritter et al., 2010)

2068



inquiring the passenger’s air ticket demand; or
it’s directly interrupted by the passenger with
booking demand which is always associated with
place information. After that, conversations are
carried out with other booking related issues, such
as checking ticket price or flight time.

The flowchart produced by HSSM can be rea-
sonably interpreted with knowledge of air ticket
booking domain, and it most consistent with the
agent’s real workflow of the Ticket Booking Cor-
poration3 compared with other models. We notice
that conventional models can not clearly distin-
guish some relevant latent states from each other.
For example, these baseline models always con-
found the latent state “Price Info” with the latent
state “Reservation”, due to certain words assigned
large weights in the two states, such as “打折 (dis-
count)”, and “信用卡 (credit card)” etc. Further-
more, Only HSSM and LMHMMS have dialogue
specific topics, and experimental results illustrate
that HSSM can learn much better than LMHMMS
which always mis-recognize corpus general words
as belonging to dialogue specific topic (An exam-
ple is presented in Table 3).

Please Waiting

Confirmation

Inquiry

Start

Place Info Price Info

Time InfoPassenger Info

End

Reservation

0.27

0.29

0.10

0.26

0.21
0.36

0.19

0.17

0.26 0.18

0.31

0.25

0.12

0.11

0.13

Figure 4: Transitions between latent states on
AirTicketBooking generated by our HSSM model
under the setting of J = 10 latent states. Transi-
tion probability cut-off is 0.10.

4.2 Quantitative Evaluation

For quantitative evaluation, we examine HSSM
and traditional models with log likelihood and an
ordering task on the held-out test set of Twitter-
Post and AirTicketBooking.

3We hide the corporation’s real name for privacy reasons.

Latent States Utterance Examples Utterance Examples
(Chinese) (English Translation)

Start 您好，很高兴为您服务。 Hello, very glad to be of service.
Inquiry 您想预定机票吗？ Do you want to make a flight

reservation?
Place Info 我想预定一张北京到上海的

机票。
I want to book an air ticket from
Beijing to Shanghai.

Time Info 明天上午10点左右。 Tomorrow morning, about 10
o’clock.

Price Info 成人机票1300元一张。 The adult ticket is 1300 Yuan.
Passenger Info 姓 名 李 东 ， 身 份 证

号12345。
My name is Li Dong, and my ID
number is 12345.

Confirmation 好的，可以。 Yes, that’s OK.
Please Waiting 请稍等，我帮您查询。 Please wait a moment, I’ll check

for you.
Reservation 请预定一张，我想用信用卡

支付。
Please make a reservation, I want
to use a credit card to pay.

End 欢迎下次来电，再见。 Welcome to call next time. Bye.

Table 2: Utterance examples of latent states dis-
covered by our model.

Model Top Words

HSSM 十点,李东,福州,厦门,上航, ...ten o’clock, Dong Li (name), Fuzhou (city), Xiamen
(city), Shanghai Airlines, ...

LMHMMS 有,十点,额,李东,预留, ...have, ten o’clock, er, Dong Li (name), reserve, ...

Table 3: One example of dialogue specific topic
learned on the same dialogue session with HSSM
and LMHMMS, respectively.

Log Likelihood The likelihood metric mea-
sures the probability of generating the test set us-
ing a specified model. The likelihood of LMHMM
and TMHMM can be directed computed with the
forward algorithm. However, since likelihoods of
LMHMMS, TMHMMS and TMHMMSS are in-
tractable to compute due to the local dependen-
cies with respect to certain latent variables, Chib-
style estimating algorithms (Wallach et al., 2009)
are employed in our experiments. For HSSM, the
partition function is a key problem for calculating
the likelihood, and it can be effectively estimated
by Annealed Importance Sampling (AIS) (Neal,
2001; Salakhutdinov and Murray, 2008).

Figure 5 presents the likelihood of different
models on the two held-out datasets. We can ob-
serve that HSSM achieves better performance on
likelihood than all the other models under different
number of latent states. On Twitter-Post dataset
our model slightly surpasses LMHMMS, and it
performs much better than all traditional models
on AirTicketBooking dataset.

Ordering Test Following previous work
(Barzilay and Lee, 2004; Ritter et al., 2010;
Zhai and Williams, 2014), we utilize Kendall’s
τ (Kendall, 1938) as evaluation metric, which
measures the similarity between any two se-
quential data and ranges from −1 (indicating a
reverse ordering) to +1 (indicating an identical

2069



J = 10 J = 20 J = 30

Figure 5: Negative log likelihood (smaller is better) on held-out datasets of Twitter-Post (upper) and
AirTicketBooking (lower) under different number of latent states J .

ordering). This is the basic idea: for each dialogue
session with n utterances in the test set, we firstly
generate all n! permutations of the utterances;
then evaluate the probability of each permutation,
and measure the similarity, i.e. Kendall’s τ ,
between the max-probability permutation and
the original order; finally, we average τ values
for all dialogue sessions as the model’s ordering
test score. As pointed out by Zhai et al. (2014),
it’s however infeasible to enumerate all possible
permutations of dialogue sessions when the
number of utterances in large. In experiments,
we employ the incrementally adding permutation
strategy, as used by Zhai et al. (2014), to build
up the permutation set. The results of ordering
test are presented in Figure 6. We can see that
HSSM exhibits better performance than all the
other models. For the conventional models, it
is interesting that LMHMMS, TMHMMS and
TMHMMSS achieve worse performances than
LMHMM and TMHMM. This is likely because
the latter two models allow words to be emitted
only from latent states (Zhai and Williams, 2014),
while the former three models allow words to
be generated from additional sources. This
also implies HSSM’s effectiveness of modeling
distinct information uderlying dialogues.

4.3 Discussion

The expermental results illustrate the effective-
ness of the proposed undirected dialogue struc-
ture analysis model based on Boltzmann machine.

The conducted experiments also demonstrate that
undirected models have three main merits for text
modeling, which are also demonstrated by Hinton
and Salakhutdinov (2009), Srivastava et al. (2013)
through other tasks. Boltzmann machine based
undirected models are able to generalize much bet-
ter than traditional directed generative model; and
model learning is more stable. Besides, an undi-
rected model is more suitable for describing com-
plex dependencies between different kinds of vari-
ables.

We also notice that all the models can, to some
degree, capture the sequential structure in the di-
alogues, however, each model has a special char-
acteristic which makes itself fit a certain kind of
dataset better. HSSM and LMHMMS are more
appropriate for modeling the open domain dataset,
such as Twitter-Post used in this paper, and the
task-oriented domain dataset with one relatively
concentrated topic in the corpus and special in-
formation for each dialogue, such as AirTicket-
Booking. As we known, dialogue specific top-
ics in HSSM or LMHMMS are used and trained
only within corresponding dialogues. They are
crucial for absorbing certain words that have im-
portant meaning but do not belongs to latent states.
In addition, for differet dataset, dialogue specific
topics may have different effect to the model-
ing. Take the Twitter-Post for an example, dia-
logue specific topics formulate actual themes of
dialogues, such as a pop song, a sport news. As for
the AirTicketBooking dataset, dialogue specific

2070



J = 10

0 20 40 60 80 100
0

0.2

0.4

0.6

0.8

1.0
av

er
ag

e 
 k

en
d
al

l’
s 

 τ

J = 20

0 20 40 60 80 100
0

0.2

0.4

0.6

0.8

1.0

J = 30

0 20 40 60 80 100
0

0.2

0.4

0.6

0.8

1.0

0 20 40 60 80 100
0

0.2

0.4

0.6

0.8

1.0

av
er

ag
e 

 k
en

d
al

l’
s 

 τ

0 20 40 60 80 100
0

0.2

0.4

0.6

0.8

1.0

# of random permutations

0 20 40 60 80 100
0

0.2

0.4

0.6

0.8

1.0

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9

HSSM LMHMM LMHMMS TMHMM TMHMMS TMHMMSS

Figure 6: Average Kendall’s τ measure (larger is better) on held-out datasets of Twitter-Post (upper) and
AirTicketBooking (lower) under different number of latent states J .

topics always represent some special information,
such as the personal information, including name,
phone number, birthday, etc. In summary, each di-
alogue specific topic reflects special information
which is different from other dialogues.

The three models, TMHMM, TMHMMS and
TMHMMSS, which do not include dialogue spe-
cific topics, should be utilized on the task-oriented
domain dataset, in which each dialogue has little
special or personnal information. For example, the
three models perform well on the the BusTime and
TechSupport datasets (Zhai and Williams, 2014),
in which name entities are all replaced by different
semantic types (e.g. phone numbers are replaced
by “<phone>”, E-mail addresses are replaced by
“<email>”, etc).

5 Conclusions

We develope an undirected generative model,
HSSM, for dialogue structure analysis, and exam-
ine the effectiveness of our model on two different
datasets, Twitter posts occurred in open-domain
and task-oriented dialogues from airline ticket
booking domain. Qualitative evaluations and
quantitative experimental results demonstrate that
the proposed model achieves better performance
than state-of-the-art approaches. Compared with
traditional models, the proposed HSSM has more
powerful ability of discovering structures of latent

states and modeling different word sources, in-
cluding latent states, dialogue specific topics and
global general topic.

According to recent study (Srivastava et al.,
2013), a deep network model exhibits much ben-
efits for latent variable learning. A dialogue may
actually have a hierarchy structure of latent states,
therefore the proposed model can be extended to a
deep model to capture more complex structures.
Another possible way to extend the model is to
consider modeling long distance dependency be-
tween latent states. This may further improve the
model’s performance.

Acknowledgments

We are grateful to anonymous reviewers for their
helpful comments and suggestions. We would like
to thank Alan Ritter for kindly providing the raw
Twitter dataset.

This work is supported in part by the Na-
tional Natural Science Funds of China under Grant
61170197 and 61571266, and in part by the Elec-
tronic Information Industry Development Fund
under project “The R&D and Industrialization
on Information Retrieval System Based on Man-
Machine Interaction with Natural Speech”.

2071



References
James Allen, Nathanael Chambers, George Ferguson,

Lucian Galescu, Hyuckchul Jung, Mary Swift, and
William Taysom. 2007. Plow: A collaborative task
learning agent. In Proceedings of the National Con-
ference on Artificial Intelligence, volume 22, page
1514. Menlo Park, CA; Cambridge, MA; London;
AAAI Press; MIT Press; 1999.

Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models with applications
to generation and summarization. In proceedings of
HLT-NAACL 2004, pages 113–120.

Ananlada Chotimongkol. 2008. Learning the structure
of task-oriented conversations from the corpus of in-
domain dialogs. Ph.D. thesis, SRI International.

William W Cohen, Vitor R Carvalho, and Tom M
Mitchell. 2004. Learning to classify email
into“speech acts”. In EMNLP, pages 309–316.

Nigel Crook, Ramon Granell, and Stephen Pulman.
2009. Unsupervised classification of dialogue acts
using a dirichlet process mixture model. In Proceed-
ings of the SIGDIAL 2009 Conference: The 10th An-
nual Meeting of the Special Interest Group on Dis-
course and Dialogue, pages 341–348. Association
for Computational Linguistics.

Geoffrey E Hinton and Ruslan R Salakhutdinov. 2009.
Replicated softmax: an undirected topic model. In
Advances in neural information processing systems,
pages 1607–1614.

Geoffrey E Hinton and Richard S Zemel. 1994.
Autoencoders, minimum description length, and
helmholtz free energy. Advances in neural informa-
tion processing systems, pages 3–3.

Geoffrey E Hinton. 2002. Training products of experts
by minimizing contrastive divergence. Neural com-
putation, 14(8):1771–1800.

Minwoo Jeong, Chin-Yew Lin, and Gary Geunbae Lee.
2009. Semi-supervised speech act recognition in
emails and forums. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing: Volume 3-Volume 3, pages 1250–1259.
Association for Computational Linguistics.

Michael I Jordan, Zoubin Ghahramani, Tommi S
Jaakkola, and Lawrence K Saul. 1999. An intro-
duction to variational methods for graphical models.
Machine learning, 37(2):183–233.

Maurice G Kendall. 1938. A new measure of rank
correlation. Biometrika, 30(1/2):81–93.

Jingjing Liu, Stephanie Seneff, and Victor Zue. 2010.
Dialogue-oriented review summary generation for
spoken dialogue recommendation systems. In Hu-
man Language Technologies: The 2010 Annual
Conference of the North American Chapter of the
Association for Computational Linguistics, pages
64–72. Association for Computational Linguistics.

Gabriel Murray, Steve Renals, Jean Carletta, and Jo-
hanna Moore. 2006. Incorporating speaker and
discourse features into speech summarization. In
Proceedings of the main conference on Human Lan-
guage Technology Conference of the North Amer-
ican Chapter of the Association of Computational
Linguistics, pages 367–374. Association for Com-
putational Linguistics.

Radford M Neal and Geoffrey E Hinton. 1998. A
view of the em algorithm that justifies incremental,
sparse, and other variants. In Learning in graphical
models, pages 355–368. Springer.

Radford M Neal. 2001. Annealed importance sam-
pling. Statistics and Computing, 11(2):125–139.

Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Un-
supervised modeling of twitter conversations.

Ruslan Salakhutdinov and Iain Murray. 2008. On the
quantitative analysis of deep belief networks. In
Proceedings of the 25th international conference on
Machine learning, pages 872–879. ACM.

Nitish Srivastava, Ruslan R Salakhutdinov, and Geof-
frey E Hinton. 2013. Modeling documents with
deep boltzmann machines. UAI.

Tijmen Tieleman. 2008. Training restricted boltz-
mann machines using approximations to the likeli-
hood gradient. In Proceedings of the 25th interna-
tional conference on Machine learning, pages 1064–
1071. ACM.

Hanna M Wallach, Iain Murray, Ruslan Salakhutdinov,
and David Mimno. 2009. Evaluation methods for
topic models. In Proceedings of the 26th Annual In-
ternational Conference on Machine Learning, pages
1105–1112. ACM.

Yorick Wilks. 2006. Artificial companions as a new
kind of interface to the future internet.

Steve J Young. 2006. Using pomdps for dialog man-
agement. In SLT, pages 8–13.

Ke Zhai and Jason D Williams. 2014. Discovering
latent structure in task-oriented dialogues. In ACL
(1), pages 36–46.

2072


