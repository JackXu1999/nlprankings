















































Applying Sentiment-oriented Sentence Filtering to Multilingual Review Classification


Proceedings of the Workshop on Sentiment Analysis where AI meets Psychology (SAAIP), IJCNLP 2011, pages 51–58,
Chiang Mai, Thailand, November 13, 2011.

Applying Sentiment-oriented Sentence Filtering to Multilingual Review
Classification

Takashi Inui and Mikio Yamamoto

Graduate School of Systems and Information Engineering

University of Tsukuba

1-1-1 Tenoudai, Tsukuba, Ibaraki 305-8573, JAPAN

{inui,myama}@cs.tsukuba.ac.jp

Abstract

A method for multilingual review classi-

fication is described. In this classifica-

tion task, machine translation techniques

are used to remove language gaps in the

dataset, but many translation errors occur

as a side-effect. These errors cause a de-

crease in the review classification perfor-

mance. To resolve this problem, we intro-

duce a sentiment-oriented sentence filter-

ing module to the process of multilingual

review classification. Experimental results

showed that the proposed method achieved

81.7% classification accuracy for the eval-

uation data.

1 Introduction

People can nowadays easily disseminate informa-

tion including their personal subjective opinions

on products and services on the Internet. The mas-

sive amounts of this type of information are ben-

eficial for both product companies and users who

are planning to purchase and use the products. The

information is mainly presented in a textual form,

so in the research field of natural language pro-

cessing, many researchers have focused on devel-

oping techniques for sentiment analysis (or opin-

ion mining) (Pang and Lee, 2008; Tang et al.,

2009).

One fundamental technique in sentiment anal-

ysis (opinion mining) is to classify review texts.

Unlike the conventional topic-based text classi-

fication task, classifiers for review classification

must discriminate between positive and negative

aspects of opinions in a review text. In the re-

view classification task, supervised machine learn-

ing methods such as Naive Bayes and Support

Vector Machines have been mostly applied (Pang

et al., 2002; Mullen and Collier, 2004; Whitelaw

et al., 2005). These supervised approaches have

achieved good performance, but they have a cru-

cial issue: they require a large amount of labeled

data, which involves the high cost of manual an-

notation.

Approaches to reduce or avoid the cost of

annotation have been proposed, such as semi-

supervised and substitutional data approaches.

Semi-supervised approaches (e.g., that by Aue

and Gamon (2005)) provide a simple solution

by combining labeled and unlabeled data. Sub-

stitutional data approaches provide substitutional

labeled data, available at low costs, instead of

pure labeled data. The tasks of domain adaption

(Blitzer et al., 2007) and multilingual text clas-

sification (Banea et al., 2008; Wan, 2009; Banea

et al., 2010) are special cases of substitutional ap-

proaches.

In this paper, we examine the effectiveness of

applying a sentence filtering module to multilin-

gual document classification, especially to mul-

tilingual review classification. In multilingual

review classification, machine translation tech-

niques are usually used to remove language gaps

in the dataset. But, even if one can use the state-

of-the-art machine translation techniques, many

translation errors occur as a side-effect. These er-

rors cause a decrease in the review classification

performance. In this study, to resolve this prob-

lem, we introduce a sentiment-oriented sentence

filtering module to the process of multilingual re-

view classification. we focus on the quality rather

than the quantity of the training data, and attempt

to filter out some worthless sentences from the

dataset.

The rest of this paper is organized as follows.

First, we provide an overview of multilingual re-

view classification in Section 2. In addition, an

issue essentially related to the task of multilingual

review classification is presented. In Section 3, we

explain our sentiment-oriented sentence filtering

method. In Section 4, we report on our experi-

51



���������������

��������������������

���������
������

����������

�����������

�������

�����������

����������

����������������������

����������

�����
�����
���

Figure 1: Monolingual review classification

ments investigating the effectiveness of applying

our filtering method to multilingual review classi-

fication.

2 Multilingual Review Classification

2.1 Overview

Figure 1 shows an ordinary processing flow of text

(review) classification with monolingual data. In a

monolingual setting, in both the training phase and

classification phase, text documents in the dataset

are described in the same language (language X

in Figure 1). Figure 2, in contrast, shows a mul-

tilingual setting for review classification. In this

setting, text documents in the classification phase

are described in a different language, Y, from X.

To remove the language gap between the train-

ing and test datasets, machine translation (MT)

techniques are used in the training phase. By

translating text documents in the dataset from the

source language X into the target language Y,

an MT system automatically generates a substi-

tutional dataset in which text documents are de-

scribed in the target language Y1.

2.2 The issue

Here, the MT system succeeds in removing the

language gap between X and Y. However, many

translation errors occur in the dataset as a side-

effect. In general, a text classifier uses information

1Note that even though a small amount of original labeled
documents is described (i.e., not translated) in language Y in
general cases, this is omitted in Figure 2 for simplicity.

���������������

��������������������

���������
������

����������

�����������

�������

����������

�����������

����������

�����
�����
���

������������

������������
������

�����������
������

�����������

������
�����������

Figure 2: Multilingual review classification

about word distributions over the dataset. When

there are erroneous translations in the dataset, a

situation is invoked in which the distributions of

each word in the dataset differ between the train-

ing data and the test data. As a result, these er-

rors cause increase of text classification errors in-

directly (dotted line in Figure 2).

3 Applying Sentiment-oriented Sentence

Filtering

In this section, our method for reducing the in-

fluences of translation errors is proposed. In

the proposed method, documents translated by an

MT system are then compressed by a sentiment-

oriented sentence filtering module. We begin with

discussion about our key idea of the proposed

method, and then explain our sentiment-oriented

sentence filtering.

3.1 Key idea

Consider the relationship between a labeled

dataset for training a text classifier and its clas-

sification accuracy. In a general case, the larger

the labeled training dataset, the better the perfor-

mance of the text classifier. However, in the case

of multilingual review classification, this relation-

ship does not hold due to the translation errors be-

52



���������
������

����������

�����������

�����������

������������

������������
������

�����������

�����������

���������
����������

���������

Figure 3: Multilingual review classification with

sentence filtering

cause if the labeled data increase, the translation

errors included in them may also increase. That

is, the number of translation errors may be propor-

tional to the number of labeled documents.

According to the above discussion, we focused

our attention on the quality rather than the quan-

tity of the labeled data. To achieve this change

of focus, we introduce a sentence filtering module

after the machine translation step. Figure 3 shows

the training phase of multilingual review classifi-

cation with the sentence filtering module. In the

sentence filtering module, a translated document

is compressed into a snippet consisting of impor-

tant parts of the translated document for the review

classification task. Since the generated snippet is

shorter than the input document, and recalling that

the number of translation errors may be propor-

tional to the quantity of the dataset, applying sen-

tence filtering should help to prevent errors being

incorporated into the dataset.

3.2 Sentiment-oriented sentence filtering

Our sentence filtering module aims to generate

text snippets by excluding translation errors from

the input translated documents. To do so, we

developed a sentiment-oriented sentence filtering

method.

We need to develop criteria by which sentences

should be extracted. The most direct approach

is that all sentences correctly translated are ex-

tracted and all remaining erroneous sentences are

excluded. This may work well, but it is infeasible

because detecting whether a sentence is correctly

translated is difficult.

Instead, we consider an alternative approach

based on sentiment information. Pang et al.

(2004) found that an important factor for a review

classification task is whether each sentence in a

document to be classified holds subjective aspects.

Generally, subjective sentences contribute to the

performance of review classification, while objec-

tive sentences do not. According to this finding,

we adopted the following sentence filtering crite-

ria: all sentences holding subjective aspects are

extracted and all remaining objective sentences

are excluded. We consider that objective sentences

with translation errors are not only unnecessary

but also harmful for the multilingual review clas-

sification.

In this study, we detect a sentence S Y as hold-

ing subjective aspects when all the following con-

ditions are fulfilled.

(1) S Y includes at least one polarity word,

(2) A sentence S X , which has a translation rela-

tion to S Y , also includes at least one polarity

word,

(3) All the polarity words in S X and S Y have the

same sentiment polarity.

Condition (1) is commonly used in the field of

sentiment analysis (Kim and Hovy, 2005). Condi-

tions (2) and (3), on the other hand, are originally

derived from the translation process in the mul-

tilingual review classification. By adding these

two conditions, we achieve more robust subjec-

tivity detection. Figure 4 shows an example of

the sentence filtering process. Sentences S Y2 and

S Y4 fulfill all the conditions and thus are extracted.

Sentences S Y1 and S Y3 are excluded. S Y1 violates

condition (1): it has no polarity words. S Y3 vi-

olates condition (3): although S Y3 has a negative

polarity word, S X3 has a positive polarity word.

In this example, one can see that the snippet gen-

erated keeps almost all the subjective information

and also that it succeeds in eliminating parts of er-

roneous translations.

53



�����������

������������

�����������

����
����
����
����

����
����
����
����

����
������

��

��

��

��

��

�

�����������
���������

�����������

��
��

�����
��������������

������������������

��

��

��

Figure 4: Example of sentence filtering process

4 Evaluation

We conducted experiments for investigating the

effectiveness of applying our sentiment-oriented

sentence filtering method to the multilingual re-

view classification.

4.1 Experimental settings

4.1.1 Multilingual review classification

methods

Review classification methods that enable han-

dling of multilingual data have been proposed. We

adopted those proposed by Banea et al. (2008) and

Wan (2009) in our experiments, since theirs are

well-known and standard methods.

Banea’s method (2008) has two classification

models that are dependent on the running position

of the MT system.

Training data Translation Model (TrTM) This

model is actually shown in Figure 2. A text

classifier is learned using a dataset described

in the target language. To do so, before the

text classifier is learned, documents (reviews)

in the training dataset that are described in

the source language are translated into the

same language as those in the test dataset.

We do not need to do anything with the test

dataset.

Test data Translation Model (TeTM) This is a

reverse version of TrTM. A text classifier

is learned using a dataset described in the

source language. In this model, documents

in the test dataset are translated into the same

language as that in the training dataset be-

fore the classification phase is run. We do not

need to do anything with the training dataset.

Wan’s method (2009) combines the above two

models through the multi-viewpoint style co-

training approach proposed by Blum and Mitchell

(1998). Here, the source language and the target

language are considered as each viewpoint. The

sets of features extracted from dataset described

in each language are simultaneously used in the

co-training framework. This method iteratively

runs TrTM and TeTM. For each iteration, two

sets of additional unlabeled review dataset, one

is described in the target language and another is

the same dataset but is translated into the source

language, are applied as input to TrTM/TeTM to

predict their (temporal) class label. Of all pre-

dicted review data, a subset confidently predicted

is added into the original labeled training dataset.

We call this method the Co-training Model in the

remainder of this paper.

The sentence filtering mentioned in the previous

section is a preprocessing stage of multilingual re-

view classification. Therefore, each classification

model (TrTM, TeTM, and Co-training) is able to

run without any modifications. We can directly

use the snippets as elements of the training/test

dataset.

4.1.2 Dataset

Works on sentiment analysis have usually been

carried out in English because there is a large

amount of English linguistic resources available

for sentiment analysis. Thus, in this study we set

English as a source language and Japanese as a

target language.

We collected reviews for use in our experiments

from one of the most popular global e-commerce

sites, Amazon. We accessed Amazon.com

(“http://www.amazon.com/”) for English reviews

and Amazon.co.jp (“http://www.amazon.co.jp/”)

for Japanese reviews.

54



Table 1: Number of English/Japanese polarity words

polarity words all positive negative

English 1,392 609 783

Japanese 724 340 384

Table 2: Number of documents/sentences including a polarity word

data type #documents #sentences

English 9,738/10,000 (97%) 51,661/82,310 (63%)

EtoJ 8,283/10,000 (83%) 26,424/82,310 (32%)

Japanese 955/ 1,000 (96%) 3,498/ 7,466 (47%)

JtoE 985/ 1,000 (99%) 5,017/ 7,466 (67%)

First, we prepared a common product list. This

is a list of products that can be purchased through

both Amazon.com and Amazon.co.jp. We used

in this study a list of MP3 audio players, such

as “iPod (Apple)” and “Walkman (Sony)”. Sec-

ond, we retrieved and crawled a set of reviews by

using the above list from Amazon.com and Ama-

zon.co.jp. All crawled reviews hold an up-to-five-

star user rating. We regarded reviews holding four

or five stars as positive reviews and those holding

one or two stars as negative reviews. As a result,

we obtained 1,000 Japanese reviews (500 positive

/ 500 negative reviews), and 10,000 English re-

views (5,000 positive / 5,000 negative reviews).

In our setting, the source language was English.

The volume of English reviews was 10 times that

of Japanese ones. All reviews were original, and

there were no duplicates.

4.1.3 Polarity dictionary

We need to prepare a set of polarity words to run

sentiment-oriented sentence filtering. We used a

polarity dictionary generated as follows.

1) We constructed initial polarity dictionaries

by using the methods by Takamura et al.

(2005b) and Takamura et al. (2005a) 2.

In these methods, the English polarity dic-

tionary is constructed based on WordNet

(1998) information, and the Japanese polarity

dictionary is constructed based on Iwanami

Japanese-language dictionary (1994), respec-

tively. Each method output a set of

2The essential part of the above both papers is the same.
The difference is only that language for the input. In the
(Takamura et al., 2005b) the authors introduced for the En-
glish polarity dictionary, and in the (Takamura et al., 2005a)
introduced for the Japanese polarity dictionary.

word/polarity pairs with a confidence level.

2) We manually corrected words with a high

confidence level, and we eliminated words

with a low confidence level from the initial

dictionary.

Table 1 shows the number of English/Japanese

polarity words in our dictionary.

Table 2 shows the number of docu-

ments/sentences including a polarity word in

the dataset. The abbreviation EtoJ means English

documents were translated to Japanese. The

abbreviation JtoE means translation in the oppo-

site direction. On the document level, excepting

the case of EtoJ (83%, slightly low percentage),

almost all documents (reviews) included at least

one polarity word. This means that the set of

polarity words used in the experiments has wide

coverage.

4.1.4 Other settings

We used as a machine translation system the Ex-

cite automatic translation service3. This site pro-

vides rule-based machine translation between En-

glish and Japanese (both EtoJ and JtoE).

For learning review classifiers, we used a lin-

ear kernel support vector machine (SVM) and the

software package Classias4 for training SVM clas-

sifiers. Unigram-based binary feature vectors were

constructed. As the tokenization process (recog-

nizing word separations) for Japanese reviews, we

used a well-known Japanese NLP programming

software package, MeCab5. All English words in

3http://www.excite.co.jp/world/
4http://www.chokkan.org/software/classias/

index.html.en
5http://mecab.sourceforge.net/

55



���

���

���

���

���

���

����� ����� ������������

��������� ��������
�������
��������

Figure 5: Effects of sentence extraction

the dataset were lower-cased.

We used ten-fold cross-validation for the evalu-

ation.

4.2 Experimental results

The experimental results are shown in Table 3 (see

also Figure 5). The value in each cell indicates

the classification accuracy. Each column shows

the multilingual review classification method, and

each row shows the sentence extraction method in

the sentence filtering step. PNWords is the sen-

tence extraction method described in Section 3,

i.e., our proposed method. The others are base-

line methods for comparison. WITHOUT means

that the sentence filtering step was skipped at the

training phase of text classifiers; all sentences in

the reviews in the training dataset were used in

the training phase. RANDOM means that snip-

pets were generated by randomly extracting K per-

cent of sentences from the original reviews in the

dataset. We set K=50 in the experiments. Un-

likeWITHOUT and PNWords, RANDOM had es-

sentially randomness. Therefore, we prepared five

sets of snippets by running RANDOM five times

and then measured five accuracy values. The aver-

age accuracy is shown in Table 3.

We also developed a system which was trained

on documents written in Japanese in order to see

what is the accuracy of the system when a MT is

not used. The accuracy of this system is 77.9%.

To investigate the performances of the three

multilingual classification methods, we first ig-

nored the effects of sentence filtering modules and

simply compared the accuracies of the first row,

i.e., the results obtained by WITHOUT. Table 3

Table 3: Effects of sentence extraction

TrTM TeTM Co-training

WITHOUT 73.6 73.7 78.4

RANDOM 73.0 69.0 77.5

PNWords 77.0 78.1 81.7

shows that the accuracy of Co-training is higher

than that of both TrTM and TeTM. Thus, the co-

training model is considered to have an advan-

tage over both TrTM and TeTM. This result cor-

responds with those reported by Wan (2009). We

confirmed that Wan’s co-training method outper-

forms TrTM and TeTM in a multilingual review

classification problem.

Next, we investigated the effectiveness of the

proposed sentence filtering method. In compar-

ing WITHOUT and RANDOM for each multilin-

gual review classification method, when the sen-

tence filtering step with the RANDOM method

was added to the training phase of text classifiers,

the classification accuracy worsened rather than

improved. One can see that extracting sentences

without thought (namely, at random) does not con-

tribute to improvement of the text classification

performance. Last, in comparing WITHOUT and

PNWords, one can see that PNWords outperforms

WITHOUT for all the multilingual review classi-

fication methods and that the combination of Co-

training and PNWords achieves the best perfor-

mance. From the results, we can conclude that our

sentiment-oriented sentence filtering method can

improve multilingual review classification.

56



5 Related Works

Several methods of monolingual document-level

sentiment classification have been proposed. In

the early works in this field, such as by Pang et

al. (2002), Mullen and Collier (2004), and Ga-

mon (2004), the interest was in simply applying

machine learning approaches. The latest works in

this field have discussed some specific features for

sentiment analysis. For example, Li et al. (2009)

and Dasgupta and Ng (2010) considered shifting

polarity and ambiguous polarity in documents.

The multilingual setting is also a recent topic.

As described in Section 4, Banea et al. (2008) pro-

posed a simple solution using machine translation.

Wan (2009) extended Banea’s work, and applied

for English/Chinese reviews. Denecke (2008) also

proposed a similar method for English/German re-

views. He used SentiWordNet6, which is an en-

hanced lexical resource for sentiment analysis and

opinion mining.

In the word-level multilingual sentiment classi-

fication area, Mihalcea et al. (2007) proposed two

methods for translating polarity words using bilin-

gual dictionaries and a parallel corpus. Scheible

(2010) proposed a graph-based approach to obtain

translation information of polarity words. He used

English/German dataset.

In the sentence-level multilingual sentiment

classification area, Banea et al. (2010) conducted

experiments with six languages (English, Arabic,

French, German, Romanian and Spanish) , and re-

ported that one can predict sentence-level subjec-

tivity in languages other than English, by leverag-

ing on a manually annotated English dataset, with

71.3% (for Arabic) to 73.66% (for Spanish).

6 Conclusion

We investigated the effectiveness of applying our

sentiment-oriented sentence filtering method to re-

duce the influence of translation errors in multi-

lingual document-level review classification. Our

filtering method can improve the performance of

multilingual review classification. Experimental

results showed that the proposed method achieved

81.7% classification accuracy.

The following issues will need to be addressed

to refine our method.

• In this study, we treated sentence-level lin-

guistic units to reduce the influence of trans-

6http://sentiwordnet.isti.cnr.it/

lation errors. In the future, we will also in-

vestigate performances when extracting fine-

grained linguistic units, such as words and

phrases. For example, Wei and Pal (2010)

attempted to apply structural correspondence

learning (Blitzer et al., 2006; Blitzer et al.,

2007) to find a low dimensional document

representation.

• We applied the proposed method only to En-

glish/Japanese dataset. Additional experi-

ments with other languages should be con-

ducted for further and more sophisticated

data analysis.

• Yang et al. (2009) handled heterogeneous

data in a framework of transfer learning (Pan

and Yang, 2010). The relationship between

our approach and transfer learning would be

interesting to examine.

References

Anthony Aue and Michael Gamon. 2005. Customiz-
ing sentiment classifiers to new domains: a case
study. In Proceedings of Recent Advances in Nat-
ural Language Processing.

Carmen Banea, Rada Mihalcea, Janyce Wiebe, and
Samer Hassan. 2008. Multilingual subjectivity
analysis using machine translation. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, pages 127–135.

Carmen Banea, Rada Mihalcea, and Janyce Wiebe.
2010. Multilingual subjectivity: Are more lan-
guages better? In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics,
pages 28–36.

John Blitzer, Ryan McDonald, and Rernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing, pages 120–128.

John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, bollywood, boom-boxes and
blenders: Domain adaptation for sentiment classi-
fication. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 440–447.

Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In Pro-
ceedings of the eleventh annual conference on Com-
putational learning theory, pages 92–100.

57



Sajib Dasgupta and Vincent Ng. 2010. Mine the easy
and classify the hard: Experiments with automatic
sentiment classification. In Proceedings of the 47th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 701–709.

Kerstin Denecke. 2008. Using SentiWordNet for mul-
tilingual sentiment analysis. In Proceedings of the
ICDE Workshop on Data Engineering for Blogs, So-
cial Media, and Web 2.0, pages 507–512.

C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. The MIT Press.

Michael Gamon. 2004. Sentiment classification on
customer feedback data: Noisy data, large feature
vectors, and the role of linguistic analysis. In Pro-
ceedings of the 18th International Conference on
Computational Linguistics.

Soo-Min Kim and Eduard Hovy. 2005. Automatic de-
tection of opinion bearing words and sentences. In
Proceedings of the 2nd International Joint Confer-
ence on Natural Language Processing, pages 61–66.

Shoushan Li, Sophia Yat Mei Lee, Ying Chen, Chu-
Ren Huang, and Guodong Zhou. 2009. Sentiment
classification and polarity shifting. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics.

Rada Mihalcea, Carmen Banea, and Janyce Wiebe.
2007. Learning multilingual subjective language
via cross-lingual projections. In Proceedings of the
45th Annual Meeting of the Association for Compu-
tational Linguistics.

Tony Mullen and Nigel Collier. 2004. Sentiment anal-
ysis using support vector machines with diverse in-
formation sources. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language
Processing, pages 412–418.

M. Nishio, E. Iwabuchi, and S. Mizutani. 1994.
Iwanami Japanese-language dictionary. Iwanami
Shoten.

Sinno Jialin Pan and Qiang Yang. 2010. A survey on
transfer learning. IEEE Transactions on Knowledge
and Data Engineering, 22(10):1345–1359.

Bo Pang and Lillian Lee. 2004. A sentimental edu-
cation: Sentiment analysis using subjectivity sum-
marization based on minimum cuts. In Proceedings
of the 42th Annual Meeting of the Association for
Computational Linguistics, pages 271–278.

Bo Pang and Lillian Lee. 2008. Opinion Mining and
Sentiment Analysis. Now Publishers Inc.

Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? sentiment classification using
machine learning techniques. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 76–86.

Christian Scheible. 2010. Sentiment translation
through lexicon induction. In Proceedings of the
ACL 2010 Student Research Workshop, pages 25–
30.

Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2005a. Extracting semantic orientation of words
using spin model. In IPSJ SIG Note (NL-168-22),
pages 141–148. (In Japanese).

Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2005b. Extracting semantic orientations of words
using spin model. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational
Linguistics, pages 133–140.

Huifeng Tang, Songbo Tan, and Xueqi Cheng. 2009.
A survey on sentiment detection of reviews. Expert
Systems with Applications, 36(7):10760–10773.

Xiaojun Wan. 2009. Co-training for cross-lingual sen-
timent classification. In Proceedings of the 47th An-
nual Meeting of the Association for Computational
Linguistics, pages 235–243.

Bin Wei and Christopher Pal. 2010. Cross lingual
adaptation: An experiment on sentiment classifica-
tions. In Proceedings of the 48th Annual Meeting
of the Association for Computational Linguistics,
pages 258–262.

Casey Whitelaw, Navendu Garg, and Shlomo Arga-
mon. 2005. Using appraisal groups for sentiment
analysis. In Proceedings of the ACM 14th Confer-
ence on Information and Knowledge Management.

Qiang Yang, Yuqiang Chen, Gui rong Xue, Wenyuan
Dai, and Yong Yu. 2009. Heterogeneous transfer
learning for image clustering via the social web. In
Proceedings of the ACL-IJCNLP, pages 1–9.

58


