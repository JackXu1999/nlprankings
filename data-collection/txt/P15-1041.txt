



















































Learning to Adapt Credible Knowledge in Cross-lingual Sentiment Analysis


Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 419–429,

Beijing, China, July 26-31, 2015. c©2015 Association for Computational Linguistics

Learning to Adapt Credible Knowledge in Cross-lingual Sentiment
Analysis

Qiang Chen∗,†, Wenjie Li†,?, Yu Lei†, Xule Liu∗, Yanxiang He∗,‡
∗School of Computer Science, Wuhan University, China

†Department of Computing, The Hong Kong Polytechnic University, Hong Kong
?Hong Kong Polytechnic University Shenzhen Research Institute, China
‡The State Key Lab of Software Engineering, Wuhan University, China

∗{qchen, xuleliu, yxhe}@whu.edu.cn
†{csqchen, cswjli, csylei}@comp.polyu.edu.hk

Abstract

Cross-lingual sentiment analysis is a task
of identifying sentiment polarities of texts
in a low-resource language by using sen-
timent knowledge in a resource-abundant
language. While most existing approaches
are driven by transfer learning, their
performance does not reach to a promising
level due to the transferred errors. In this
paper, we propose to integrate into knowl-
edge transfer a knowledge validation mod-
el, which aims to prevent the negative
influence from the wrong knowledge by
distinguishing highly credible knowledge.
Experiment results demonstrate the neces-
sity and effectiveness of the model.

1 Introduction

With the wide range of business value, sentiment
analysis has drawn increasing attention in the past
years. The extensive research and development
efforts produce a variety of reliable sentiment
resources for English, one of the most popular
language in the world. These available rich
resources become the treasure of knowledge to
help conduct or enhance sentiment analysis in
the other languages, which is a task known as
cross-lingual sentiment analysis (CLSA). In the
literature of CSLA, the language with abundant
reliable resources is called the source language
(e.g., English), while the low-resource language is
referred to as the target language (e.g., Chinese).
However, in this paper, the situation is a low
resource language scenario, where the source
language is English, and the target language is
Chinese.

The main idea of existing CLSA researches is
to first build up the connection between the source
and target languages to overcome the language
barrier, and then develop an appropriate knowl-
edge transfer approach to leverage the annotated

data from the source language to train a sentiment
classification model in the target language, either
supervised or semi-supervised. In particular, these
approaches exploit and convert the knowledge
learned from the source language to automatically
generate and expand the pseudo-training data for
the target language.

The machine translation (MT) service is one
of the most common ways used to build the
language connection (Wan, 2008; Banea et al.,
2008; Wan, 2009; Wei and Pal, 2010; Gui et
al., 2014). Although it is claimed in Duh et al.
(2011) that the MT service is ripe for CLSA,
the imperfect MT quality hinders existing MT-
based CLSA approaches from the further advance.
In our preliminary study, we find that even the
Google translator1 (i.e., one of the most widely
used online MT service (Shankland 2013)) may
unavoidably changes the sentiment polarity of
the translated text, as illustrated below, with a
percentage of around 10%.

[Original English Text]: I am at home on bed
rest and desperate for something good to read.
[Sentiment Label: Negative]
[Translated Chinese Text]: ·3[¹K>E
Úý"�ÀÜéÐw" {Meaning: I am in bed
to rest at home and feel that desperate things are
also good to read.}[Sentiment Label: Positive]

The noisy data generated by MT errors for sure
will weaken the contribution of the transferred
knowledge and even worse may create conflicting
knowledge. While it is a critical step in CLSA to
localize the sentiment knowledge learned from the
source language in the target language, to the best
of our knowledge, hardly any previous research
has focused on knowledge validation to filter out
the noisy knowledge having sentiment changes
caused by wrong translations during knowledge
transfer.

1http://translate.google.com

419



To reduce the noisy sentiment knowledge intro-
duced into the target language, we are motivated to
validate the knowledge transferred from the source
language by checking its linguistic distributions
and sentiment polarity consistency with the known
knowledge in the target language. Different
from previous co-training based approaches where
two language views recommend knowledge to
each other in the same manner, we consider
the source language as the “supervisor” and the
target language as the “learner”. The “supervisor”
boosts itself with its own accumulated labeled data
(called knowledge) and meanwhile recommends
its confident knowledge to the “learner”. The
“learner” tries to select trustworthy knowledge
based on the recommendation to update and
expand its training data. Adding a process to
efficiently filter out noisy knowledge and retain the
self-adaptive and interested new knowledge makes
the subsequent boosting process more credible.
This is why our approach can outperform state-of-
the-art CLSA approaches.

The rest of this paper is organized as follows.
Section 2 summarizes the related work. Section 3
explains the proposed model. Section 4 presents
experimental results. Finally, Section 5 concludes
the paper and suggests future work.

2 Related Work

2.1 Sentiment Analysis

Sentiment has been analyzed in different language
granularity, e.g., entity, aspect, sentence and
document. This paper focuses on sentiment
analysis of online product reviews in the document
level.

Existing approaches are generally categorized
into lexicon-based and machine learning based
approaches (Liu, 2012). Lexicon-based approach-
es highly depend on sentiment lexicons. Turney
(2002) derives the overall phrase and document
sentiment scores by averaging the sentiment
scores provided in a lexicon over the words
included. Similar idea is adopted in (Hiroshi et
al., 2004; Kennedy and Inkpen, 2006). Machine
learning based approaches, on the other hand,
apply classification models. The task-specific
features are designed to train sentiment polarity
classifiers. Pang et al. (2002) compare the
performance of NB, SVM and ME on movie
reviews. SVM is found more effective. Gamon
(2004) shows that SVM with deep linguistic

features can further improve the performance. A
variety of other machine learning approaches are
also proposed to sentiment classification (Mullen
and Collier, 2004; Read, 2005; Hassan and Radev,
2010; Socher et al., 2013).

Cross-domain sentiment classification (CDSC)
shares certain common characteristics with cross-
lingual sentiment classification (CLSC) (Tan et al.,
2007; Li et al., 2009; Pan and Yang, 2010; He et
al., 2011a; Glorot et al., 2011). Notice that the gap
between source domain and target domain is the
main difference between CDSC and CLSC. CLSC
copes with two different datasets in two different
languages. This difference makes CLSC a new
challenge, drawing specific attention to researcher
recently.

2.2 Cross-lingual Sentiment Analysis

There are two alternative solutions to cross-lingual
sentiment analysis. One is ensemble learning
that combines multiple classifiers. The other is
transfer learning that develops strategies to adapt
the knowledge from one language to the other.
Wan (2008) is among the pioneers to develop
the ensemble learning solutions, where multiple
classifiers learned from different training datasets
including those in original languages and trans-
lated languages are combined by voting. Most
researches, on the other hand, explore transfer
learning and focus on knowledge adaptation. For
example, Wan (2009) applies a supervised co-
training framework to iteratively adapt knowledge
learned from the two languages by transferring
translated texts to each other. Other similar work
includes (Wei and Pal, 2010) and (He, 2011b). All
these approaches rely on MT to build language
connection.

Meanwhile, the unlabeled parallel data is also
employed to fill the gap between two languages.
To solve the feature coverage problem with the
EM algorithm, Meng et al. (2012) leverage the
unlabeled parallel data to learn unseen sentiment
words. Similarly, Popat et al. (2013) use the
unlabeled parallel data to cluster features in order
to reduce the data sparsity problem. Meng et
al. (2012) and Popat et al. (2013) also use
the unlabeled parallel data to reduce the negative
influence of the noisy and incorrect sentiment
labels introduced by machine translation and
knowledge transfer. However, the parallel data is
also a scarce resource.

420



Some existing transfer learning based CLSA
methods have attempted to address the noisy
knowledge problem caused by wrong labels by
checking label consistency. For example, to
filter out the unconfident labels in Chinese, the
supervised learning method proposed by (Xu et
al., 2011) runs boosting in Chinese by checking
consistency between the labels manually annotat-
ed in English and predicted by Chinese classifiers
on translated Chinese. The work in (Gui et al.,
2014) follows the same line although it considers
knowledge transferring between two languages.
On the contrary, the main focus of our work is
to filter out the noisy knowledge having sentiment
changes by wrong translations. Actually, both
label consistency checking and linguistic distribu-
tion checking are important. Any one alone cannot
work well. In fact, both of them are considered as
the knowledge validation in our work, though the
later is our focus.

3 Credible Boosting Model

In this paper, we propose a knowledge validation
approach to improve the effectiveness of knowl-
edge transfer without directly using extra parallel
data. Our target is to filter out the noisy senti-
ment labels introduced by MT and the incorrect
sentiment labels generated by imperfect classifier
in the source language. Here, the knowledge is
referred to as a collection of distributed document
presentations with sentiment labels that have been
verified to be robust in sentiment classification (Le
and Mikolov, 2014). A novel credible boosting
model, namely CredBoost is proposed to apply
transfer-supervised learning with an added self-
validation mechanism to guarantee the knowledge
transferred highly credible and self-adaptive.

3.1 Problem Description

In a standard cross-lingual sentiment analysis
setting, the training data includes labeled English
reviews LEN = {(xleni , yi)}Mi=1 and unlabeled
Chinese reviews UCN = {xucnj }Nj=1, where xki
(k = len or ucn) represents review i and yi ∈
{−1, 1} is the sentiment label of review xli. The
test data is Chinese reviews TCN = {xtcns }Ss=1.

We now introduce the unlabeled data into
credBoost’s setting. LEN is divided into two
disjoint partsLTEN andL

B
EN , whereL

T
EN for basic

training and LBEN for self-boosting. We translate
LEN into Chinese to obtain extra labeled Chinese

pseudo-reviews LTrCN = {(xlcnTri , yi)}Mi=1 and
UCN into English to obtain extra unlabeled
English pseudo-reviews UTrEN = {xlenTrj }Nj=1.
Thereby, we obtain a pair of pseudo-parallel data
(UCN , UTrEN ).

The task is to use LEN and UCN to train a
Chinese classifier to predict sentiment polarity for
the test data TCN . It is a standard transfer learning
problem. We consider two language views, i.e.,
source language view DS and target language
viewDτ . DS boosts itself with the labeled English
data and recommend translated knowledge to Dτ ,
while Dt selects self-adaptive ones to boost itself.

3.2 Framework of CredBoost
The CredBoost model involves two synchronously
boosting views for two languages respectively.
During training, one view acts as a “supervisor”
that recommends and passes the knowledge to the
other view. The same knowledge is also added
into its own view for boosting by automatically
updating the weights of the labeled data. The
other view acts as a “learner” that receives the
recommended knowledge and selects the best-
suited new knowledge to learn.

As mentioned before, the knowledge trans-
ferred through MT is not reliable. The source
language view may also make wrong predictions
and thus transfer the wrong knowledge to the
target language even the translations are correct.
Whether or not the “learner” can benefit from
its “supervisor” and how much it benefits highly
depends on the credibility and adaptiveness of
the recommended knowledge accepted by the
“learner”. Knowledge validation is necessary to
ensure the quality of learning. The objective
of knowledge validation is to identify the new
and acquired knowledge from recommendations.
Both language views are iteratively trained until
learning converges or reaches the iteration upper
bound.

In the source language view, at iteration (t),
the CredBoost model first uses LT (t)EN to train a
basic classifier C(t)EN and then uses C(t)EN to predict
L
B(t)
EN and U

(t)
TrEN . Top m and top n instances are

sampled from LB(t)EN and U
(t)
TrEN respectively, by

Formula (1) :
O

(t)
EN = {(xLBi′ , ŷLBi′ )}meni′=1

TR
(t)
EN = {(xUTri , ŷUTri )}neni=1

(1)

where O(t)EN denotes the candidates to be added

421



into the training data, and TR(t)EN the knowledge
to be recommended to the target language view.
We use the source knowledge validation function
VS(O

(t)
EN ) to identify the acquired knowledge

K
(t)
′Ac learned in the previous learning process and

the new knowledge K(t)′Nw fresh to the current
knowledge system from O(t)EN . The importance of
each training instance is updated according to the
performance of prediction by Formula (2) :

ω
′Ac
i′ =

 e�(t) ·
√
ν

(t)

i′ · c(t)i′ if ŷ
′Ac
i′ 6= y

′Ac
i′√

ν
(t)

i′ · c(t)i′ otherwise;

ω
′Nw
j′ =

{
e�(t) · log (1 +√e · c(t)j′ ) if ŷ

′Ac
j′ 6= y

′Ac
j′

log (1 +
√
e · c(t)j′ ) otherwise.

(2)

where c(t)j′ is the confidence of an instance
given by C(t)EN , thus log (1 +

√
e · c(t)j′ ) > 1 is to

enhance the weight of new knowledge because of
the higher significance contributing to the later
learning. ν(t)i′ (< 1) is the adaptiveness score
given by the source knowledge validation function
VS(O

(t)
EN ). �(t)(> 1) is the error rate of C(t)EN ,

thus e�(t) > 1 is to reward the wrongly predicted
data in the next iteration. ŷ

′Ac
i′ is the label

given by C(t)EN and y
′Ac
i′ is the manually annotated

label. For the incorrectly predicted instance, the
weight is boosted inversely to the performance
of the current classifier. The instance identified
as the new knowledge which contributes more
to performance improvement is given a reward
parameter to enhance its significant in the next
training iteration. Data sets update by Formula (3).
The training starts with iteration (1), the training
data is initially set as LT (1)EN = L

T
EN .

L
T (t+1)
EN = L

T (t)
EN ∪K(t)′Ac ∪K(t)′Nw

L
B(t+1)
EN = L

B(t)
EN − (K(t)′Ac ∪K(t)′Nw)

(3)

In the target language view, at iteration (t),
the CredBoost model receives the recommended
knowledge TR(t)EN and projects it to O

(t)
CN from

the unlabeled Chinese data U (t)CN with the pseudo-
parallel data (U (t)CN , U

(t)
TrEN ). O

CN
(t) is validat-

ed by the target knowledge validation function
Vτ (O

(t)
CN ) to identify the acquired knowledge

K
(t)
Ac and the new knowledge K

(t)
Nw. K

(t)
Ac and

K
(t)
Nw are projected to K

(t)
∗Ac and K

(t)
∗Nw from

the unlabeled English pseudo-data U (t)TrEN . The
weight of an instance is updated by Formula (4),
and the parameter setting is similar to that in

the source language view. The confidence c(t)i
is directly transferred from Ds. We reward the
validated knowledge to raise their significance in
the training data considering they are originally
Chinese.

ωAci =

√
c
(t)
i · log(1 +

√
e · v(t)i )

ωNwj = e
log (1+

√
e·c(t)j ) = 1 +

√
e · c(t)j

(4)

We update the data setting by Formula (5). The
training data is initially set as UT (1)CN = U

T
CN . The

CredBoost model is illustrated in Algorithm 1.

L
(t+1)
TrCN = L

(t)
TrCN ∪K(t)Ac ∪K(t)Nw

U
(t+1)
CN = U

(t)
CN − (K(t)Ac ∪K(t)Nw)

U
(t+1)
TrEN = U

(t)
TrEN − (K(t)∗Ac ∪K(t)∗Nw)

(5)

Algorithm 1 CredBoost Model
Input: English labeled data LTEN and LBEN , translated
English unlabeled data UTrEN , translated Chinese data
LTrCN and unlabeled Chinese data UCN ;
Initialize: Weights W (1)EN = {1}M for LTEN and
W

(1)
TrCN = {1}M for LTrCN ;

For t = 1, · · · , T :
1. Use LT (t)EN to learn English classifier CEN(t);
2. Use C(t)EN to predict LB(t)EN and U (t)TrEN sample top

m and top n instances from LB(t)EN and U
(t)
TrEN , O

(t)
EN and

TR
(t)
EN ;
3. Validate O(t)EN by knowledge validation function

VS(O
(t)
EN ) to identify acquired knowledge K

(t)
′Ac and new

knowledge K(t)′Nw, generate the weights for them by
Formula (2), then recommend TR(t)EN to Dτ ;

4. Project TR(t)EN to O
(t)
CN with pseudo-parallel data

(U
(t)
CN , U

(t)
TrEN ), and use knowledge validation function

Vτ (O
(t)
CN ) to identify acquired knowledge K

(t)
Ac and new

knowledge K(t)Nw, then generate weights for them by
Formula (4);

5. UpdateDS by Formula (2) andDτ by Formula (5);
End For.
Output: Chinese classifier C(T )CN .

3.3 Knowledge Validation

Knowledge is familiarity, awareness or under-
standing of someone or something, such as
facts, information or skills, which is acquired
through experience or education by perceiving,
discovering or learning2. It can be implicit or
explicit.

In machine learning, natural language knowl-
edge is a continuously improving hypothesis that
consists of both semantic and significant domain

2Definition from Oxford Dictionary of English, avail-
able at: http://oxforddictionaries.com/view/
entry/m_en_us126.

422



characters. While language is the expression of
semantic, semantic is the carrier of sentiment.
Using another word, two texts with more smaller
semantic distance have higher probability to share
the same sentiment polarity. Choi and Cardie
(2008) assert that the sentiment polarity of natural
language can be better inferred by compositional
semantics. They also suggest that incorporating
compositional semantics into learning can im-
prove the performance of sentiment classifiers.
Saif et al. (2012) also demonstrate that the
addition of extra semantic features can further
improve performance.

In order to filter out noisy and incorrect senti-
ment labels, we propose a knowledge validation
approach to reduce these noisy data that hinder the
improvement of learning performance. Knowl-
edge validation is a way to identify the acquired
knowledge implied in current knowledge system
and also the new knowledge fresh to current
knowledge system. The knowledge can be repre-
sented in the semantic space. (Le and Mikolov,
2014) project documents into a low-dimension
semantic space with a deep learning approach,
known as document-to-vector (Doc2Vec3). Con-
sidering that Dov2Vec has been verified to be
efficient in many NLP tasks including sentiment
analysis, we follow previous research to represent
knowledge embedded in product reviews with the
vectors generated by Doc2Vec.

Suppose distributed representations (i.e., low-
dimensional vectors) of the all reviews including
{LTEN , LBEN , UTrEN} and {LTrCN , UCN}
are {V(LTEN ),V(LBEN ),V(UTrEN )} and
{V(LTrCN ),V(UCN )} respectively. At iteration
(t), V(LT (t)EN ) is the current knowledge system
of the English view and V(L(t)TrCN ) is that of
the Chinese. The knowledge validation runs
separately in the source and target views.

In the target language view, at iteration (t),
suppose the prediction confidence of the candidate
(xUi , ŷ

U
i ) ∈ O(t)CN is c(t)i . We define the

adaptiveness score as the average distance of top
ζ+ semantic distances between the instance xLBi
and the positive cluster of L(t)TrCN , denoted as

L
(t)+
TrCN , and top ζ

(t)
− = ζ+ ·

L(t)+
L(t)−

semantic distances

between xUi and the negative cluster, denoted as

3Doc2Vec is one of the models implemented in the free
python library Gensim which can be freely downloaded at:
https://pypi.python.org/pypi/gensim.

L
(t)−
TrCN , where L(t)+ and L(t)− are the numbers of

the elements in L(t)+TrCN and L
(t)−
TrCN respectively.

The validation parameters are defined by Formula
(6), ωr is the weight of training instance V(r), ν(t)i
is the adaptiveness score, and V label∗ ∈ {1,−1} is
the validated label which denotes the knowledge
belonging to the positive cluster L(t)+TrCN or the
negative cluster L(t)−TrCN . The validation process
is illustrated in Algorithm 2, where the acquired
knowledge is k(t)Ac, and the new knowledge is k

(t)
Nw.

D(V(xLBi ),V(r)) = V(x
LB
i )

T · V(r)
‖ V(xLBi ) ‖ · ‖ V(r) ‖

⇒


ν

(t)+
i =

1
ζ+

∑
r∈L(t)+

EN

ωr D(V(xLBi ),V(r))

ν
(t)−
i =

1

ζ
(t)
−

∑
r′∈L(t)−

EN

ωr′ D(V(xLBi ),V(r′))

⇒ ∆(ν(t)i ) = ν(t)+i − ν(t)−i
⇒ δ(t)i =

1

e1+∆(ν
(t)
i )

⇒ V label∗ =
{

1 if δ(t)i > 0.5,
−1 if δ(t)i ≤ 0.5.

⇒ ν(t)i =
{
ν

(t)+
i if V label∗ = 1,
ν

(t)−
i if V label∗ = −1.

(6)

where D(V(xLBi ),V(r)) is the Cosine distance
between the distributed representations of the two
reviews. ν(t)+i and ν

(t)−
i are the weighted averages

of the semantic distances. δ(t)i is the Sigmoid
function which computes the probability that the
data is distributed in the positive cluster L(t)+TrCN .

In the source language view, at iteration
(t), let’s suppose the prediction confidence of
candidate (xLBi′ , ŷ

LB
i′ ) ∈ O(t)EN to be c(t)i′ . The

definitions of validation parameters are similar
to those in the target language view. The
validation process is illustrated in Algorithm 3.
The validation is looser, because the training data
and candidates are both in English. This differs
from it in the target view.

4 Experiments

4.1 Experimental Setup

We evaluate the proposed CredBoost model on
an open cross-lingual sentiment analysis task in
NLP&CC 20134. The data set provided is a

4NLP&CC is an annual conference of Chinese infor-
mation technology professional committee organized by
Chinese computer Federation (CCF). It mainly focuses
on the study and application novelty of natural language
processing and Chinese computation. CLSA task is the
task 3 of NLP&CC 2013. For more details and open

423



Algorithm 2 Knowledge Validation Vτ (Dτ )
Input: Labeled Chinese training data L(t)TrCN , weights
of labeled data W (t)CN and semantics vectors of all
English data for iteration (t): {V(L(t)TrCN ),V(U (t)CN )};
Initialize: K(1)′Ac = φ, K

(1)
′Nw = φ;

For xUi in O
(t)
CN :

1. Use L(t)TrCN to train a classifier C(t)CN ,
then use C(t)CN predict xUi , giving la-
bel yCNi ;

2. Get validated label V label∗ , positive and
negative average distances ν(t)+i , ν

(t)−
i

of xUi by fomula (6);
3. If ν(t)+i < ψ and ν

(t)−
i < ψ:

If ŷLBi = V label∗ :
Then K(t)Nw ← K(t)Nw + xUi ;

Else:
If ŷLBi = V label∗ = yCNi :
Then K(t)Ac ← K(t)Ac + xUi ;

End For.
Output: K(t)Nw, K

(t)
Ac .

Algorithm 3 Knowledge Validation VS(DS)
Input: Weights of labeled data W (1)EN and semantics
vectors of all English data for iteration (t):
{V(LT (t)EN ),V(LB(t)EN ),V(U (t)TrEN )};
Initialize: K(1)′Ac = φ, K

(1)
′Nw = φ;

For xLBi′ in O
(t)
EN :

1. Get validated label V label′ , positive and
negative average distances ν(t)+i′ , ν

(t)−
i′

of xLBi′ by fomula (6);
2. If ν(t)+i′ < ψ and ν

(t)−
i′ < ψ:

If ŷLBi′ = V label′ :
Then K(t)′Nw ← K(t)′Nw + xLBi′ ;

Else:
If ŷLBi′ = V label′ :
Then K(t)′Ac ← K(t)′Ac + xLBi′ ;

End For.
Output: K(t)′Nw, K

(t)
′Ac.

collection of bilingual Amazon product reviews
in Books, DVD and Music domains. It contains
4,000 labeled English reviews, 4,000 Chinese test
reviews, and 17,814, 47,071, 29,677 unlabeled
Chinese reviews in three different domains. We
randomly select 2,000 unlabeled Chinese reviews
in each domain to train classifiers. Besides, the
pseudo-data sets described in CredBoost model
are translated with Google translator. The data set
is summarized in Table 1.

To better illustrate the significance of knowl-
edge validation during knowledge transfer, we
compare the proposed method with the following
baseline methods:

Lexicon-based (LB): The standard English
MPQA sentiment lexicons are translated into

resource, you can available at: http://tcci.ccf.org.
cn/conference/2013/index.html.

Domain English ChineseL U L U

Books Train 4,000 - - 2,000Test - - 4,000 -

DVD Train 4,000 - - 2,000Test - - 4,000 -

Music Train 4,000 - - 2,000Test - - 4,000 -

Table 1: Experimental data sets. All data sets
are balanced, L represents labeled data and U
represents unlabeled data.

Chinese and then utilized together with a small
number of Chinese turning words, negations and
intensifiers to predict the sentiment polarities of
the Chinese test reviews.

Basic SVM (BSVM-CN): The labeled English
reviews are translated into Chinese, which are then
used as the pseudo-training data to train a Chinese
SVM classifier.

Primarily boost transfer learning (BTL-1):
The labeled English reviews are used to train
the English classifier, which is applied to label
the English translations of the unlabeled Chinese
reviews. These labeled Chinese reviews obtained
via MT together with the Chinese translations of
the labeled English reviews are then used as the
pseudo-training data to train a Chinese sentiment
classifier.

Best result in NLP&CC 2013 (BR2013): This
is the best result reported in NLP&CC 2013.
Unfortunately, the specification of the method is
not available.

Self-boost (SB-CN) in Chinese: The labeled
English reviews are translated into Chinese, which
are used as the pseudo-training data to train a basic
Chinese classifier. This classifier is iteratively
refined by choosing the most confidently predicted
English reviews to add into the Chinese training
data until a predefined iteration number reaches. It
can be also considered as a self-adaptive boosting
approach.

Iteratively boost transfer learning (BTL-2):
This is an enhanced transfer learning method shar-
ing the same learning framework with CredBoost
but it ignores knowledge validation. It iteratively
transfers the knowledge from English to Chinese.
The learning in both languages iteratively boosts
themselves separately. The transfer size is 16,
comparable to that in CredBoost.

Basic co-training (CoTr): The co-training
method proposed in (Wan, 2009) is implemented.
It is bidirectional transfer learning. In each

424



iteration, 10 positive and 10 negative reviews are
transferred from one language to the other.

Doc2vec feature CredBoost (dCredB): This
method is similar to CredBoost except that
document-to-vector is used to generate features
when training basic classifiers. The vectors
are obtained from both original and translated
reviews. The dimension of doc2vec is 300, while
the other parameters are set as default.

The baseline methods described above are
categorized into three classes: the first four
which are preliminary methods, the middle three
which are several state-of-the-art models being
comparable to our proposed model, and the last
one which is a comparison to suggest that the
knowledge representation is not the answer to the
performance improvement. For all the methods
excluding LB and BR2013, we use support vector
machines (SVMs) as basic classifiers. We use
the Liblinear package (Fan et al., 2008) with the
linear kernel5. All methods use Unigram+Bigram
features to train the basic classifiers, except for
dCredB.

4.2 Experimental Result

In this work, there are two main parameters that
may significantly influence the performance of our
proposed model. They are the new knowledge
validation boundary ψ and the validation scale
ζ+ in the training data. We set the values of
parameters with the grid search strategy. We
first fix initial ζ+ = 14 to search the best
new knowledge validation boundary ψ from an
empirical value set {0.30, 0.35, 0.40, 0.45, 0.50}.
We then fix the best ψ = 0.40 to check the
suitable validation scale ζ+ from the initial value
set {6, 8, 9, 10, 11, 12, 14, 16} in which values are
comparable with the knowledge transfer scale
of CoTr in the training data. Besides, the
recommendation size m for English is set to 20
and the recommendation size n for Chinese is set
to 40. The final settings are listed in Table 2.
The performance is evaluated in terms of accuracy
(Ac) defined by Formula (7).

Ac(f) =
pf

P f
, Avg Ac =

1

3
·
∑
f
′∈F

Ac(f
′
) (7)

where pf is the number of correct predictions
and P f is the total number of the test data; F ∈
{Books,DV D,Music} is the domain set.

5The parameter setting used in this paper is ‘-s 7’.

Domain ψ ζ+ m n
Books 0.45 12 20 40
DVD 0.40 12 20 40
Music 0.40 9 20 40

Table 2: Parameter settings of three domains in
this paper.

Approaches Domain Avg AcBooks DVD Music
LB 0.7770 0.7832 0.7595 0.7709

BSVM-CN 0.7940 0.7995 0.7778 0.7904
BTL-1 0.8010 0.8058 0.7605 0.7891

BR2013 0.7850 0.7773 0.7513 0.7712
SB-CN 0.8400 0.8428 0.8012 0.8280
BTL-2 0.8105 0.8265 0.7980 0.8117
CoTr 0.8025 0.8508 0.7812 0.8115

dCredB 0.6485 0.6753 0.6700 0.6646
CredBoost 0.8465 0.8518 0.8093 0.8359

Table 3: Macro performance of all approaches
in three domains. All values are accuracies and
Avg-Ac represents the average accuracy in three
domains.

The performances are reported in Tables 3 and
4. As shown, CredBoost outperforms all the other
comparison methods. The first four baselines
have poor performances compared to others. This
suggests that the CLSA problem cannot be well
solved by directly learning from the labeled
translated data without any knowledge adaption or
knowledge validation. SB-CN, BTL-2 and CoTr
employ iterative boosting to adapt knowledge
from the source English to the target Chinese with-
out validating the transferred knowledge. They
inevitably mis-recommend the massive noisy data
into Chinese. CredBoost, in contrast, introduces
knowledge validation into transfer learning with
iterative boosting. It better adapts knowledge from
English to Chinese and thus ensures the credibility
of the accepted knowledge. Its best result justifies
our assumption.

Specifically, SB-CN leverages both the Chinese
training data translated from the labeled English
data and the unlabeled Chinese data used for
boosting. The boosting in Chinese iteratively
selects the trustworthy data with the labels as-
signed by the Chinese classifier. Our proposed
method, however, exploits two different languages
simultaneously with an additional boosting step,
i.e., it transfers knowledge from English to
Chinese during boosting. We then use knowledge
validation model to validate the unlabeled Chinese
data whose labels are assigned by the English

425



Model (Books) Positive Negative AcP R F1 P R F1
LB 0.7368 0.8400 0.7850 0.8140 0.7000 0.7527 0.7700

BSVM-CN 0.8249 0.7465 0.7837 0.7685 0.8415 0.8033 0.7940
BTL-1 0.8537 0.7265 0.7850 0.7620 0.8755 0.8148 0.8010

BR2013 - - - - - - 0.7850
SB-CN 0.8716 0.7975 0.8329 0.8134 0.8825 0.8465 0.8400
BTL-2 0.7105 0.8881 0.7894 0.9105 0.7588 0.8278 0.8105
CoTr 0.8339 0.7555 0.7928 0.7765 0.8495 0.8114 0.8025

dCredB 0.5310 0.6941 0.6017 0.7660 0.6202 0.6854 0.6485
CredBoost 0.8225 0.8640 0.8427 0.8705 0.8306 0.8501 0.8465

Model (DVD) Positive Negative AcP R F1 P R F1
LB 0.7648 0.8180 0.7905 0.8044 0.7485 0.7754 0.7832

BSVM-CN 0.7745 0.8450 0.8082 0.8295 0.7540 0.7900 0.7995
BTL-1 0.8282 0.7715 0.7988 0.7861 0.8400 0.8122 0.8058

BR2013 - - - - - - 0.7773
SB-CN 0.8853 0.7875 0.8335 0.8086 0.8980 0.8510 0.8428
BTL-2 0.8525 0.8104 0.8309 0.8005 0.8444 0.8219 0.8265
CoTr 0.8374 0.8705 0.8536 0.8652 0.8310 0.8478 0.8508

dCredB 0.6070 0.7030 0.6515 0.7435 0.6542 0.6960 0.6753
CredBoost 0.8440 0.8572 0.8508 0.8595 0.8465 0.8530 0.8518

Model (Music) Positive Negative AcP R F1 P R F1
LB 0.7387 0.8030 0.7695 0.7842 0.7160 0.7485 0.7595

BSVM-CN 0.8492 0.6755 0.7525 0.7306 0.8800 0.7984 0.7778
BTL-1 0.8437 0.6395 0.7275 0.7097 0.8815 0.7863 0.7605

BR2013 - - - - - - 0.7513
SB-CN 0.8787 0.6990 0.7786 0.7501 0.9035 0.8197 0.8012
BTL-2 0.7285 0.8461 0.7829 0.8675 0.7616 0.8111 0.7980
CoTr 0.8536 0.6790 0.7564 0.7335 0.8835 0.8015 0.7812

dCredB 0.5860 0.7043 0.6397 0.7540 0.6455 0.6955 0.6700
CredBoost 0.7258 0.8708 0.7917 0.8928 0.7653 0.8241 0.8093

Table 4: Micro performance of all approaches in three domains. P: Precision, R: Recall, F1: micro-F
measure, Ac: Accuracy, and - represents unknown. The model in BR2013 is unknown, thus its micro
performance is unavailable.

classifier. It is reasonable that a Chinese classifier
performs better on Chinese text than an English
classifier performs on the translated English text
due to the different language distributions and MT
errors. However, as shown in Tables 3 and 4,
the better performance of our proposed method
compared with that of the self-boosting method
further suggests the effectiveness of our proposed
knowledge validation model.

Figure 1 illustrates the continuous changes of
performances vs. the corresponding growth sizes
of the training data sets for SB-CN, BTL-2,
CoTr, and CredBoost. According to our common
sense, noisy data have negative influence on
performance improvement. Compared to the other
three methods, CredBoost accepts less number of
training instances during learning while it achieves
more improvement. This verifies the ability
of CredBoost that can filter out the noisy data
recommended by the English sentiment classifier.

In Figure 1(a), the curves of BTL-2 and CoTr

suggest that directly transferring the knowledge
recommended from English imports many noisy
data into Chinese. It is also obvious that the
performance curve of CredBoost implies a stable
improvement trend while the other three decrease
after certain iterations because of the accumulated
negative influence from the noisy data. Figure
1(b) shows CredBoost accepts decreased training
instances after certain iterations because the
number of “high-quality” instances decrease when
learning proceeds. This finding suggests that
knowledge validation would rather abandon “less-
credible” knowledge with higher probability than
easily accept it. Knowledge validation in the
proposed model guarantees highly-credible learn-
ing when transferring knowledge from English to
Chinese. The results also show that CredBoost
has great potential to achieve better performance
approaching to supervised approaches if more
unlabeled Chinese data are available.

Another interesting finding is also observed.

426



 

(a) Performances comparison in three domains 

(b) Growth sizes comparison in three domains 

0 20 40 60 80 100 120

0.65

0.7

0.75

0.8

0.85

Performance in Books domain.

Iteration Number

A
c
c
u
ra

c
y

 

 

selfBoost

CoTr

BTL-2

CredBoost

0 20 40 60 80 100 120
0.76

0.77

0.78

0.79

0.8

0.81

0.82

0.83

0.84

0.85

0.86

Performance in DVD domain.

Iteration Number

A
c
c
u
ra

c
y

 

 

selfBoost

CoTr

BTL-2

CredBoost

0 20 40 60 80 100 120

0.6

0.65

0.7

0.75

0.8

Performance in Music domain.

Iteration Number

A
c
c
u
ra

c
y

 

 

selfBoost

CoTr

BTL-2

CredBoost

0 20 40 60 80 100 120
0

5

10

15

20

25

30
Growth Sizes in Books domain.

Iteration Number

N
O

. 
o
f 

In
s
ta

n
c
e
s

 

 

selfBoost

CoTr

BTL-2

CredBoost

0 20 40 60 80 100 120
0

5

10

15

20

25

30
Growth Sizes in DVD domain.

Iteration Number

N
O

. 
o
f 

In
s
ta

n
c
e
s

 

 

selfBoost

CoTr

BTL-2

CredBoost

0 20 40 60 80 100 120
0

5

10

15

20

25
Growth Sizes in Music domain.

Iteration Number

N
O

. 
o
f 

In
s
ta

n
c
e
s

 

 

selfBoost

CoTr

BTL-2

CredBoost

Figure 1: Performances vs. Growth Sizes for SB-CN, CoTr, BTL-2, and CredBoost in three domains.
The similar performance curves of CoTr is also reported in (Gui et al., 2014).

Although document-to-vector represents content
semantic well, it cannot determine the sentiment
polarity of text well, even when the document-
to-vectors that are used to train basic classifiers
are learned on the mixture of the translated and
original reviews. The superior performance of
CredBoost to dCredB suggests that the semantic
representation is effective to identify highly-
credible acquired knowledge and new knowledge
but it alone may not be sufficient enough to model
the sentiment information.

We also conduct some other experiments to
study the sensitivity of the new knowledge valida-
tion boundary ψ and the validation scale ζ+ in the
training data. The experimental results show that
the performances with different parameter settings
fluctuate around the best result reported in Tables
3 and 4 in a small range. Our model is basically
quite stable.

5 Conclusion

In this paper, we propose a semi-supervised learn-
ing model, called CredBoost, to address cross-
lingual (English vs Chinese) sentiment analysis
without direct labeled Chinese data nor direct
parallel data. We propose to introduce knowledge
validation during transfer learning to reduce the

noisy data caused by machine translation errors or
inevitable mistakes made by the source language
sentiment classifier. The experimental result
demonstrates the effectiveness of the proposed
model. In the future, we will explore more suitable
knowledge representations and knowledge valida-
tion in the CredBoost framework.

Acknowledgements

We thank all the anonymous reviewers for their
detailed and insightful comments on this paper.
The work described in this paper was supported
by the Research Grants Council of Hong Kong
project (PolyU 5202/12E and PolyU 152094/14E)
and the grants from the National Natural Sci-
ence Foundation of China (61272291, 61472290,
61472291 and 61303115).

References
Carmen Banea and Rada Mihalcea, Janyce Wiebe,

Samer Hassan. 2008. Multilingual Subjectivity
Analysis Using Machine Translation. In Proceed-
ings of the 2008 Conference on Empirical Methods
in Natual Language Processing, pages 127-135,
Honolulu, October.

Carmen Banea, Yoonjung Choi, Lingjia Deng, Samer
Hassan, Michael Mohler, Bishan Yang, Claire

427



Cardie, Rada Mihalcea, Janyce Wiebe. 2013. CPN-
CORE: A Text Semantic Similarity System Infused
with Opinion Knowledge. In Proceedings of the
Main Conference and the SHared Task in *SEM
2013, pages 221-228, Atlanta, Georgia, June 13-14,
2013.

Yejin Choi and Claire Cardie. 2008. Learning with
Compositional Semantics as Structural Inference for
Subsentential Sentiment Analysis. In Proceedings
of the 2008 Conference on Empirical Methods
in Natural Language Processing, pages 792-801,
Honolulu, October 2008.

Kevin Duh and Akinori Fujino and Masaaki Nagata.
2011. Is Machine Translation Ripe for Cross-
lingual Sentiment Classification? In Proceedings
of the 49th Annual Meeting of the Association for
Computational Linguistics: shortpapers, pages 429-
433, Portland, Oregon, June 19-24, 2011.

Rong-En Fan, Kai-Wei Chang, Cho-Jui Ksieh, Xiang-
Rui Wang, Chih-Jen Lin. 2008. LIBLINEAR: A
Library for Large Linear Classification. In Journal
of Machine Learning Research, 9 (2008) 1871-1874.

Micheal Gamon. 2004. Sentiment Classification
on Customer Feedback Data: Noisy Data, Large
Feature Vectors and the Role of Linguistic Analysis.
In Proceedings of the 20th International Conference
on Computational Linguistics, pages 841-847, CH.

Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Domain Adaptation for Large-scale Senti-
ment Classification: A Deep Learning Approach.
In Proceedings of the 28th International Conference
on Machine Learning, pages 513-520, Bellevue,
Washington, USA.

Lin Gui, Ruifeng Xu, Qin Lu, Jun Xu, Jian Xu,
Bin Liu, Xiaolong Wang. 2014. Cross-lingual
Opinion Analysis via Negative Transfer Detection.
In Proceedings of the 52th Annual Meeting of the
Association for Computational Linguistics (short
paper), pages 860-865, Baltimore, Maryland, USA,
June 23-25 2014.

Ahmed Hassan and Dragomir Radev. 2010. I-
dentifying Text Polarity Using Random Walks.
In Proceedings of the 48th Annual Meeting on
Association for Computational Linguistics, pages
395-403, Uppsala, Sweden, 11-16 July 2010.

Yulan He, Chenghua Lin, Harith Alani. 2011a.
Automatically Extracting Polarity-bearing Topics
for Cross Domain Sentiment Classification. In
Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Huamn
Language Technologies, pages 123-131, Portland,
Oregon, USA.

Yulan He. 2011b. Latent Sentiment Model for Weakly-
Supervised Cross-Lingual Sentiment Classification.
In Proceedings of the 33th European Conference on
Information Retrieval(ECIR 2011), 18-21 Apr 2011,
Dublin, Ireland.

KANAYAMA Hiroshi, NASUKAWAA Tetsuya,
WATANABE Hideo. 2004. Deeper Sentiment
Analysis Using Machine Translation Technology.
In Proceedings of the 20th International Conference
on Computational Linguistics, pages 494-500.

Alistair Kennedy and Diana Inkpen. 2006. Senti-
ment Classification of Movie and Product Reviews
Using Contextual Valence Shifters. Computational
Intelligence,22(2):110-125.

Quoc Le, Tomas Mikolov. 2014. Distributed
Representations of Sentences and Documents. In
Proceedings of the 31th International Conference on
Machine Learning, Beijing, China, 2014. JMLR:
W&CP volume 32.

Tao Li, Vikas Sindhwani, Chris Ding, and Yi Zhang.
2009. Knowledge Transformation for Cross-
Domain Sentiment Classification. In Proceedings
of the 32th International ACM SIGIR Conference
on Research and Development in Information Re-
trieval, pages 716-717, Boston, MA, USA.

Bing Liu. May 2012. Sentiment Analysis and Opinion
Mining. Morgan & Claypool Publisher.

Xinfan Meng, Furu Wei, Xiaohua Liu, Ming Zhou, Ge
Xu, Houfeng Wang. 2012. Cross-Lingual Mixture
Model for Sentiment Classification. In Proceedings
of the 50th Annual Meeting of the Association for
Computational Linguistics, pages 572-581, Jeju,
Republic of Korea, 8-14 July 2012.

Tony Mullen and Nigel Collier. 2004. Sentiment
analysis using support vector machines with di-
verse inoformation sources. In Proceedings of
the Conference on Empirical Methods in Natural
Language Processing, pages 412-418, (July 2004)
poster paper.

Sinno Jialin Pan and Qiang Yang, Fellow, IEEE. 2010.
A Survey on Transfer Learning. In Journal of IEEE
Transactions on Knowledge and Data Engineering,
Vol.22, NO.10, October 2010.

Bo Pang and Lillian Lee, Shivakumar Vaithyanathan.
2002. Thumps Up? Sentiment Classification using
Machine Learning Techniques. In Proceedings
of the 2002 Conference on Empirical Methods
in Natural Language Processing, pages 79-86,
Philadelphia, July 2002.

Kashyap Popat, Balamurali A R, Pushpak Bhat-
tacharyya and Gholamreza Haffari. 2013. The
Haves and the Have-Nots: Leverage Unlabeled
Corpora for Sentiment Analysis. In Proceedings
of the 51th Annual Meeting of the Association for
Computational Linguistics, pages 412-422, Sofia,
Bulgaria, 4-9 August 2013.

Jonathon Read. 2005. Using Emotions to reduce
Dependency in Machine Learning Techniques for
Sentiment Classification. In Proceedings of the 43th
Annual Meeting on Association for Computational
Linguistics Student Research Workshop, pages 43-
48.

428



Hassan Saif, Yulan He and Harith Alani. 2012.
Semantic Sentiment Analysis of Twitter. In
Proceedings of the 11th International Semantics
Web Conference ISWC 2012, Boston, USA.

Stephen Shankland. 2013. Google Translate now
serves 200 millon people daily. In CNET. CBS
Interactive Inc. May 18, 2013.

Richard Socher, Alex Perelygin, Jean Y. Wu, Jason
Chuang, Chiristopher D. Manning, Andrew Y. Ng
and Christopher Potts. 2013. Recursive Deep
Models for Semantics Computationality Over a
Sentiment Treebank. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing.

Songbo Tan, Gaowei Wu, Huifeng Tang and Xueqi
Cheng. 2007. A Novel Scheme for Domain-transfer
Problem in the context of Sentiment Analysis. In
CIKM 2007, November 6-8, 2007, Lisboa, Portugal.

Peter D. Turney. 2002. Thumps Up or Thumps
Down? Semantic Orientation Applied to Unsuper-
vised Classification of Reviews. In Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 417-424, Philadelphia,
July 2002.

Xiaojun Wan. 2008. Using Bilingual Knowledge
and Ensemble Technics for Unsupervised Chi-
nese Sentiment Analysis. In Proceedings of the
2008 Conference on Empirical Methods in Natual
Language Processing, pages 553-561, Honolulu,
October 2008.

Xiaojun Wan. 2009. Co-Training for Cross-Lingual
Sentiment Classification. In Proceedings of the 47th
Annual Meeting of the ACL and the 4th IJCNLP of
the AFNLP, pages 235-243, Suntec, Singapore, 2-7
August 2009.

Bin Wei and Christopher Pal. 2010. Cross Lingual
Adaptation: An Experiment on Sentiment Classifi-
cations. In Proceedings of the 48 Annual Meeting of
the Association for Computational Linguistics (short
paper), pages 258-262, Uppsala, Sweden, 11-16
July 2010.

Ruifeng Xu, Jun Xu and Xiaolong Wang. 2011.
Instance Level Transfer Learning for Cross Lingual
Opinion Analysis. In Proceedings of the 2nd Work-
shop on Computational Approaches to Subjectivity
and Sentiment Analysis, ACL-HLT 2011, pages 182-
188, 24 June, 2011, Portland, Oregon, USA.

429


