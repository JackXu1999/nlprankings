



















































Extract and Edit: An Alternative to Back-Translation for Unsupervised Neural Machine Translation


Proceedings of NAACL-HLT 2019, pages 1173–1183
Minneapolis, Minnesota, June 2 - June 7, 2019. c©2019 Association for Computational Linguistics

1173

Extract and Edit: An Alternative to Back-Translation for
Unsupervised Neural Machine Translation

Jiawei Wu and Xin Wang and William Yang Wang
Department of Computer Science

University of California, Santa Barbara
Santa Barbara, CA 93106 USA

{jiawei wu,xwang,william}@cs.ucsb.edu

Abstract

The overreliance on large parallel corpora sig-
nificantly limits the applicability of machine
translation systems to the majority of lan-
guage pairs. Back-translation has been dom-
inantly used in previous approaches for un-
supervised neural machine translation, where
pseudo sentence pairs are generated to train
the models with a reconstruction loss. How-
ever, the pseudo sentences are usually of low
quality as translation errors accumulate dur-
ing training. To avoid this fundamental is-
sue, we propose an alternative but more effec-
tive approach, extract-edit, to extract and then
edit real sentences from the target monolingual
corpora. Furthermore, we introduce a com-
parative translation loss to evaluate the trans-
lated target sentences and thus train the un-
supervised translation systems. Experiments
show that the proposed approach consistently
outperforms the previous state-of-the-art un-
supervised machine translation systems across
two benchmarks (English-French and English-
German) and two low-resource language pairs
(English-Romanian and English-Russian) by
more than 2 (up to 3.63) BLEU points.

1 Introduction

Promising results have been achieved in Neu-
ral Machine Translation (NMT) by representation
learning (Cho et al., 2014b; Sutskever et al., 2014).
But recent studies (Koehn and Knowles, 2017; Is-
abelle et al., 2017; Sennrich, 2017) highlight the
overreliance of current NMT systems on large par-
allel corpora. In real-world cases, the majority of
language pairs have very little parallel data, so the
models need to leverage monolingual data to ad-
dress this challenge (Gulcehre et al., 2015; Zhang
and Zong, 2016; He et al., 2016; Yang et al., 2018).

While many studies have explored how to
use the monolingual data to improve transla-
tion performance with limited supervision, lat-

Source Language Space Target Language Space

s-t Translation

t-s Translation
Reconstruction 

Loss

Comparative 
Translation 

Loss

source 
sentences 

s-t Translation

Extract and Edit

(a) Extract-Edit

(b) Back-Translation

target 
sentences 

translated 
sentences 

by t-s

translated 
sentences 

by s-t

Figure 1: The comparison between two approaches of
unsupervised NMT, extract-edit and back-translation.
When training the source-to-target (s-t) translation
model, instead of using the t-s back-translated sen-
tences to train the model, we directly set the extracted-
edited sentences as pivotal points to guide the training.

est approaches (Lample et al., 2018a; Artetxe
et al., 2018; Lample et al., 2018b) focus on the
fully unsupervised scenario. Back-translation has
been dominantly used in these approaches, where
pseudo sentence pairs are generated to train the
translation systems with a reconstruction loss.
However, it is inefficient because the generated
pseudo sentence pairs are usually of low qual-
ity. During the dual learning of back-translation,
the errors could easily accumulate and thus the
learned target language distribution would gradu-
ally deviate from the real target distribution. This
critical drawback hinders the further development
of the unsupervised NMT systems.

An alternative solution is to extract real paral-
lel sentences from comparable monolingual cor-
pora, and then use them to train the NMT systems.
Recently, neural-based methods (Chu et al., 2016;
Grover and Mitra, 2017; Grégoire and Langlais,
2018) aim to select potential parallel sentences



1174

from monolingual corpora in the same domain.
However, these neural models need to be trained
on a large parallel dataset first, which is not appli-
cable to language pairs with limited supervision.

In this paper, we propose a radically different
approach for unsupervised NMT—extract-edit, a
powerful alternative to back-translation (see Fig-
ure 1). Specifically, to train the source-to-target
translation model, we first extract potential par-
allel sentence candidates in the target language
space given a source language sentence. Since
it cannot be guaranteed that there always exist
potential parallel sentence pairs in monolingual
corpora, we further propose a simple but effec-
tive editing mechanism to revise the extracted sen-
tences, making them aligned with the source lan-
guage sentence. Then a comparative translation
loss is introduced to evaluate the translated sen-
tence based on the extracted-and-edited ones and
train the translation model. Compared to back-
translation, extract-edit avoids the distribution de-
viation issue by extracting and editing real sen-
tences from the target language space. Those
extracted-and-edited sentences serve as pivotal
points in the target language space to guide the un-
supervised learning. Thus, the learned target lan-
guage distribution could be closer to the real one.
The extract-edit model and the translation model,
the two major parts of our method, can be jointly
trained in a fully unsupervised way.

Empirical results on popular benchmarks show
that exact-edit consistently outperforms the state-
of-the-art unsupervised NMT system (Lample
et al., 2018b) with back-translation across four dif-
ferent languages pairs. In summary, our main con-
tributions are three-fold1:

• We propose a more effective alternative
paradigm to back-translation, extract-edit, to
train the unsupervised NMT systems with po-
tentially real sentence pairs;

• We introduce a comparative translation loss
for unsupervised learning, which optimizes
the translated sentence by maximizing its
relative similarity with the source sentence
among the extracted-and-edited pairs;

• Our method advances the previous state-of-
the-art NMT systems across four different

1The source code can be found in this repos-
itory: https://github.com/jiaweiw/
Extract-Edit-Unsupervised-NMT

language pairs under monolingual corpora
only scenario.

2 Background

Without parallel sentence pairs as constraints on
mapping language spaces, training NMT systems
is an ill-posed problem because there are many
potential mapping solutions. Nevertheless, some
promising methods have been proposed in this
field (Lample et al., 2018a; Artetxe et al., 2018;
Lample et al., 2018b). The main technical pro-
tocol of these approaches can be summarized as
three steps: Initialization, Language Modeling,
and Back-Translation. In this section, we mainly
introduce the three steps and the crucial settings
that we have followed in our work.

In the remainder of the paper, we denote the
space of source and target languages by S and T ,
respectively. enc and dec refer to the encoder and
decoder models in the sequence-to-sequence sys-
tems. Vs→t stands for the composition of enc in
the source language and dec in the target language,
which can be viewed as the source-to-target trans-
lation system.

Initialization Given the ill-posed nature of the
unsupervised NMT task, a suitable initialization
method can help model the natural priors over
the mapping of two language spaces we expect to
reach. There are mainly two initialization meth-
ods: (1) bilingual dictionary inference (Conneau
et al., 2018; Artetxe et al., 2018; Lample et al.,
2018a) and (2) byte-pair encoding (BPE) (Sen-
nrich et al., 2016b; Lample et al., 2018b). As
shown in Lample et al. (2018b), the inferred bilin-
gual dictionary can provide a rough word-by-word
alignment of semantics, and the BPE can reduce
the vocabulary size and eliminate the presence of
unknown words in the output results.

In our extract-edit approach, to extract poten-
tial parallel sentence pairs, we need to compare the
semantic similarity of sentences between two lan-
guages first. A proper initialization can also help
align the semantic spaces and extract potential par-
allel pairs within them. Thus, following the previ-
ous methods, we use the inferred bilingual dictio-
nary as described in Conneau et al. (2018) for un-
related language pairs and the shared BPE in Lam-
ple et al. (2018b) as initialization for related ones.

Language Modeling After a proper initializa-
tion, given large amounts of monolingual data, we

https://github.com/jiaweiw/Extract-Edit-Unsupervised-NMT
https://github.com/jiaweiw/Extract-Edit-Unsupervised-NMT


1175

Target Language Space

Evaluation 
Network

Top-k Extracted Sentences

Translation System

Top-k Extracted-and-Edited Sentences

(a) Extract via Nearest Neighbor Search (b) Edit via Information Integration (c) Comparative Translation

Source 
Sentence

Encoder
Encoder

D
ec

od
er

D
ec

od
er

Translated 
Target Sentence

Maxpooling

Source 
Sentence

Encoder

Comparative  
Translation Loss

Language Modeling 
Loss

Encoder

Comparative  
Translation 

Loss

Source Language Space

Figure 2: The overview of our unsupervised NMT model based on the extract-edit approach. Given a source
sentence, (a) the top-k potential parallel sentences of the target language are extracted via nearest neighbor search.
(b) The extracted sentences are further edited with the source sentence. (c) The evaluation network evaluates the
translated sentence and the extracted-and-edited sentences based on their similarities with the source sentence.
Note that (1) all the encoders share the same parameters (same for decoders); (2) the decoding processes are non-
differentiable, so the language modeling loss and the comparative translation loss are used to train the learning
modules before and after the decoding processes, respectively.

can train language models on both source and tar-
get languages. These models express a data-driven
prior about the composition of sentences in each
language. In NMT, language modeling is accom-
plished via denosing autoencoding, by minimiz-
ing:

Llm(θenc, θdec) =Ex∼S [− log Vs→s(x|C(x))]+
Ey∼T [− log Vt→t(y|C(y))]

(1)

where C is a noise model with some words
dropped and swapped, θenc and θdec are the learn-
able parameters of enc and dec. Vs→s and Vt→t
are the encoder-decoder language models on the
source and target sides, respectively.

In our extract-edit approach, we follow simi-
lar settings and adopt the noise model proposed
by Lample et al. (2018a). Note that the param-
eters of all enc are shared (same for dec) in our
framework to ensure a strong alignment and map-
ping between two languages. This sharing opera-
tion is essential for both the translation model and
the extract-edit model. Thus, we use enc to repre-
sent encoders in source language modeling encS
and in target language modeling encT (same for
dec).

Back-Translation Back-translation (Sennrich
et al., 2016b) has been dominantly used in prior

work to train the unsupervised NMT system. It
couples the source-to-target translation model
with a backward target-to-source model and trains
the whole system with a reconstruction loss. This
can be viewed as converting the unsupervised
problem into a supervised scenario by generating
pseudo language pairs (He et al., 2016).

Despite the popularity of back-translation in the
previous methods (Lample et al., 2018a; Artetxe
et al., 2018; Lample et al., 2018b), we argue that
it suffers from the low-quality pseudo language
pairs. Thus, in this work, we propose a new
paradigm, extract-edit, to address this issue by
extracting and editing potential real parallel sen-
tences. Below we describe our approach in details.

3 Extract-Edit

The overview of our extract-edit approach is
shown in Figure 2. We first extract and edit real
sentences from the target language space accord-
ing to their similarities with the source sentence.
These extracted-and-edited sentences serve as piv-
otal points in the target language space, which lo-
cate a probable region where the real target sen-
tence could be. Then we introduce a comparative
translation loss to evaluate the translated sentence
and train the system. Basically, the comparative
translation loss encourages the translated sentence
to approximate the real sentence by maximizing its



1176

relative similarity with the source sentence com-
pared to the extracted-and-edited sentences. As a
result, we manage to minimize the deviation of the
learned target language distribution and the map-
ping noises between two language spaces.

3.1 Extract

Most existing methods in comparable corpora
mining introduce two encoders to represent sen-
tences of two languages separately, and then use
another network to measure the similarity (Chu
et al., 2016; Grover and Mitra, 2017; Grégoire
and Langlais, 2018). However, owing to the
shared encoders and decoders in language mod-
eling, the semantic spaces of two languages are
already strongly connected in our scenario.

Therefore, to avoid extra computation re-
sources, we directly use the enc in language mod-
eling to obtain sentence embeddings for two lan-
guages. As shown in Figure 2 (a), for a given
source sentence s, we use the nearest neighbor
search based on L2 distance to find top-k real
sentences from the target language space (k is a
hyper-parameter decided empirically). The sen-
tence embeddings used for searching are com-
puted based on the shared encoder enc. The rea-
son to choose top-k sentences rather than top-1 is
to keep a high recall rate and obtain more related
samples from the target language space. Finally,
given the source sentence s, we denote M as a set
of the k potential parallel target sentences:

M = {t| min
1,··· ,k

(||es − et||), t ∈ T }, (2)

where es and et are sentence embeddings encoded
by the shared encoder enc.

3.2 Edit

Even though the extracted sentences could serve
as pivotal points to guide NMT, there is no guar-
antee that there always exists a parallel sentence in
the target corpus. Thus, in order to make it closer
to the real paired sentence in the target language
space, we propose an editing mechanism to revise
the extracted target sentence t ∈ M based on the
semantics of the source sentence s. As described
in Figure 2 (b), we employ a maxpooling layer to
reserve the more significant features between the
source sentence embedding es and the extracted
sentence embedding et (t ∈ M ), and then decode

it into a new sentence t′:

M ′ = {t′|t′ = dec(maxpooling(es, et)), t ∈M},
(3)

where M ′ is the set of the extracted-and-edited
sentences. Based on the semantic information of
the source sentence s, we can further improve
the extracted results with this editing mechanism.
Unlike other studies using the editing to gener-
ate more structural sentences (Guu et al., 2018;
Hashimoto et al., 2018), here the revised sentences
are designed to serve as better pivotal points in the
target language space to guide the translation pro-
cedure. This can also be viewed as adding con-
straints when aligning the two language spaces.

3.3 Evaluate
Given a source sentence s, we can translate it as t∗

using the source-to-target translation model Ps→t.
Meanwhile, a set M ′ of k sentences can also be
generated by the extract-edit approach described
above. Although the M ′ may contain potential
parallel sentences t′ for s, we cannot directly use
(s, t′) as ground-truth sentence pairs to train the
translation model Vs→t because the NMT system
is sensitive to noises (Cho et al., 2014a; Cheng
et al., 2018). The rough operation like this will
result in sub-optimal translation performance.

Therefore, in order to assess the quality of the
translated sentence t∗ and train the translation
model Vs→t, we introduce an evaluation network
R for evaluating the relative similarities between
the source and target sentences among all sentence
pairs. The evaluation network R is a multilayer
perceptron; it takes the target sentence embedding
et and source sentence embedding es as inputs,
and converts them into the joint embedding space
as rt and rs. So the similarity

α(t|s) = cosine(rt, rs) =
rt · rs
||rt||||rs||

. (4)

Then, a softmax-like formulation is used to com-
pute the ranking score for the translated sentence
t∗ given the extracted-and-edited sentence set M ′:

P (t∗|s,M ′) = exp(λα(t
∗|s))∑

t′∈M ′∪{t∗} exp(λα(t
′|s))

, (5)

where the hyper-parameter λ is similar to the in-
verse temperature of the softmax function. Lower
λ encourages the model to treat all extracted-
edited sentences equally, while higher λ highlights
the importance of sentences with higher-score.



1177

3.4 Learning
Comparative Translation As introduced
above, the ranking score calculates the relative
similarity between the < s, t∗ > pair and all
the extracted-and-edited pairs < s, t′ >. As-
suming we have a good evaluation network R
with θR denoting its parameters, we further
introduce the comparative translation loss Lcom
for unsupervised machine translation:

Lcom(θenc|θR) = −E(logP (t∗ = Vs→t(s)|s,M ′)),
(6)

where θenc is the parameters of the shared encoder
enc. Basically, the translation model is trying to
minimize the relative distance of the translated
sentence t∗ to the source sentence s compared
to the top-k extracted-and-edited sentences in the
target language space. Intuitively, we view the
top-k extracted-and-edited sentences as the anchor
points to locate a probable region in the target lan-
guage space, and iteratively improve the source-
to-target mapping via the comparative learning
scheme.

Combined with the language modeling con-
straints as described in Equation 1, the final loss
function for training the the translation model
Vs→t is defined as:

Ls→t(θenc, θdec|θR) =ωlmLlm(θenc, θdec)+
ωcomLcom(θenc|θR),

(7)

where ωlm and ωcom are hyper-parameters weigh-
ing the importance of the language modeling and
the comparative learning.

Adversarial Objective Meanwhile, we need to
learn a good evaluation network R to transform
sentence embedding of the shared encoder into the
comparable space. The evaluation network R is
also shared by two languages to ensure a strong
connection between two language spaces. Inspired
by adversarial learning (Goodfellow et al., 2014),
we can view our translation system as a “genera-
tor” that learns to generate a good translation with
a higher similarity score than the extracted-and-
edited sentences, and the evaluation network R as
a “discriminator” that learns to rank the extracted-
and-edited sentences (real sentences in the target
language space) higher than the translated sen-
tences. Thus, we have the following objective
function for the evaluation network R:

LR(θR) = −Et′∈M ′(logP (t
′ |s,M ′)). (8)

Algorithm 1: The algorithm of our unsuper-
vised NMT system with extract-edit approach.

1 Given two monolingual corpora, source S and target T ;
2 Initialization as in Section 2;
3 Language Modeling as in Section 2 to obtain the

initialized translation model V (0)s→t = enc
(0) ◦ dec(0);

4 for n← 1 to N do
5 Given a source sentence s;
6 Extract the top-k target sentences as the set M ;
7 Edit the sentences in M to obtain the set M ′;
8 Update the evaluation network

R : θR ← argminLR;
9 Update the shared encoder and decoder R:

θenc, θdec ← argminLs→t;
10 Update the translation model:

V
(n+1)
s→t = enc

(n) ◦ dec(n);

11 return V (N+1)s→t = enc
(N) ◦ dec(N).

Based on Equation 7 and 8, the final adversarial
objective is defined as

min
θenc,θdec

max
θR
L(θenc, θdec, θR)

= −LR(θR) + Ls→t(θenc, θdec|θR),
(9)

where the translation model Vs→t and the eval-
uation network R play the two-player mini-max
game. We evenly alternately update between the
encoder-decoder translation model and the evalu-
ation network. The detailed training procedure is
described in Algorithm 1.

3.5 Model Selection
In the fully unsupervised setting, we do not have
access to parallel sentence pairs. Thus, we need
to find a criterion correlated with the translation
quality to select hyper-parameters. For a neural
translation model Vs→t, we propose the following
criterion Ds→t to tune the hyper-parameters:

Ds→t = Es∈S [E(logP (t∗|s,M ′))], (10)

where t∗ = Vs→t(s). Basically, we choose the
hyper-parameters with the maximum expectation
of the ranking scores of all translated sentences.

4 Experiments

4.1 Datasets
We consider four language pairs: English-French
(en-fr), English-German (en-de), English-Russian
(en-ru) and English-Romanian (en-ro) for evalu-
ation. For a fair comparison, we use the same
corpora as in Lample et al. (2018b) for these lan-
guages for fair comparison. For English, French,



1178

Model en→fr fr→en en→de de→en

LSTM Cell

Lample et al. (2018b) 24.28 (+0.00) 23.74 (+0.00) 14.71 (+0.00) 19.60 (+0.00)
Ours (Top-1 Extract) 24.43 (+0.15) 23.90 (+0.16) 14.54 (−0.17) 19.49 (−0.11)
Ours (Top-1 Extract + Edit) 24.54 (+0.26) 24.08 (+0.34) 14.63 (−0.08) 19.57 (−0.03)
Ours (Top-10 Extract) 26.12 (+1.84) 25.83 (+2.09) 17.01 (+2.30) 21.40 (+1.80)
Ours (Top-10 Extract + Edit) 26.97 (+2.69) 26.66 (+2.92) 17.48 (+2.77) 21.93 (+2.33)

Transformer Cell

Lample et al. (2018b) 25.14 (+0.00) 24.18 (+0.00) 17.16 (+0.00) 21.00(+0.00)
Ours (Top-1 Extract) 25.30 (+0.16) 24.23 (+0.05) 17.12 (−0.04) 21.06 (+0.06)
Ours (Top-1 Extract + Edit) 25.44 (+0.30) 24.36 (+0.18) 17.14 (−0.02) 21.10 (+0.10)
Ours (Top-10 Extract) 26.91 (+1.77) 25.64 (+1.46) 19.11 (+1.95) 22.84 (+1.84)
Ours (Top-10 Extract + Edit) 27.56 (+2.42) 26.90 (+2.72) 19.55 (+2.39) 23.29 (+2.29)

Model en→ro ro→en en→ru ru→en

LSTM Cell

Lample et al. (2018b) 19.65 (+0.00) 18.52 (+0.00) 6.24 (+0.00) 7.83 (+0.00)
Ours (Top-1 Extract) 19.73 (+0.08) 18.56 (+0.04) 6.32 (+0.08) 7.99 (+0.16)
Ours (Top-1 Extract + Edit) 19.81 (+0.16) 18.69 (+0.17) 6.44 (+0.20) 8.12 (+0.29)
Ours (Top-10 Extract) 21.57 (+1.92) 20.32 (+1.80) 8.87 (+2.63) 9.76 (+1.93)
Ours (Top-10 Extract + Edit) 22.08 (+2.43) 20.83 (+2.31) 9.35 (+3.11) 10.21 (+2.38)

Transformer Cell

Lample et al. (2018b) 21.18 (+0.00) 19.44 (+0.00) 7.98 (+0.00) 9.09 (+0.00)
Ours (Top-1 Extract) 21.15 (−0.03) 19.52 (+0.08) 8.03 (+0.05) 9.20 (+0.11)
Ours (Top-1 Extract + Edit) 21.23 (+0.05) 19.59 (+0.15) 8.16 (+0.18) 9.28 (+0.19)
Ours (Top-10 Extract) 23.04 (+1.86) 21.43 (+1.99) 10.24 (+2.26) 12.29 (+3.20)
Ours (Top-10 Extract + Edit) 23.31 (+2.13) 21.60 (+2.16) 11.07 (+3.09) 12.72 (+3.63)

Table 1: The experimental results on all four language pairs and directions. The results are evaluated with BLEU
metric on newstest 2014 for en↔frand newstest 2016 for en↔de, en↔ro and en↔ru. The (+) and (−) stand for
performance gains and loss separately compared with baseline models with the same NMT cells.

German and Russian, all the available sentences
are used from the WMT monolingual News Crawl
datasets from years 2007 through 2017. As for Ro-
manian, we combine the News Crawl dataset and
WMT’16 monolingual dataset. The translation re-
sults are evaluated on newstest 2014 for en-fr, and
newstest 2016 for en-de, en-ro and en-ru.

4.2 Implementation Details

We follow previous methods (Koehn et al., 2007;
Lample et al., 2018b) to initialize our models.

Initialization We use Moses scripts (Koehn
et al., 2007) for tokenization. While the system re-
quires cross-lingual BPE embeddings to initialize
the shared lookup table for related languages, we
set the number of BPE codes as 60, 000. Follow-
ing the previous preprocessing protocol (Lample
et al., 2018b), the embeddings are then generated
using fastText (Bojanowski et al., 2017) with an
embedding dimension of 512, a context window
of size 5 and 10 negative samples.

Model Structure In this work, the NMT mod-
els can be built upon long short-term memory
(LSTM) (Hochreiter and Schmidhuber, 1997) and
Transformer (Vaswani et al., 2017) cells. For
LSTM cells, both the encoder and decoder have
3 layers. As for Transformer, we use 4 lay-
ers both in the encoder and the decoder. As for
both LSTM and Transformer, all encoder param-
eters are shared across two languages. Similarly,
we share all decoder parameters across two lan-
guages. Both two model structure are optimized
using Adam (Kingma and Ba, 2014) with a batch
size of 32. The rate for LSTM cell is 0.0003 while
Transformer’s is set as 0.0001. The weights in
Equation 7 are ωlm = ωext = 1. The λ for cal-
culating ranking scores is 0.5. As for the evalu-
ation network R, we use a multilayer perceptron
with two hidden layers of size 512. For efficient
nearest neighbor search in the extracting step, we
use the open-source Faiss library (Johnson et al.,
2017)2. We calculate the similarity of sentences in

2https://github.com/facebookresearch/
faiss

https://github.com/facebookresearch/faiss
https://github.com/facebookresearch/faiss


1179

each episode instead of each batch for computa-
tional efficiency. At decoding time, sentences are
generated using greedy decoding.

4.3 Results and Analysis

In this study, we aim to validate the effective-
ness of extract-edit versus back-translation for un-
supervised neural machine translation (NMT), so
we set the unsupervised NMT method in Lam-
ple et al. (2018b) as the baseline because it cur-
rently achieves the state-of-the-art performance on
all language pairs.3 The overall translation re-
sults across four language pairs are shown in Ta-
ble 1. In most of the cases, our proposed extract-
edit approach can outperform the baseline mod-
els trained with back-translation. Our full mod-
els (LSTM/Transformer + Top-10 Extract + Edit)
achieve more than 2 BLEU points improvement
consistently across all the language pairs. Espe-
cially, on the ru → en translation with the Trans-
former cell, our full model surpasses the baseline
score by 3.63 BLEU points. These results validate
the effectiveness of our approach and indicate that
the proposed extract-edit learning framework can
learn a better mapping and alignment between lan-
guage spaces than back-translation.

However, if extracting only top-1 target sen-
tence in our approach, the performances are not al-
ways improved (e.g., en→ de, de→ en, and en→
ro). Besides, Top-10 Extract + Edit models con-
sistently outperforms Top-1 Extract + Edit. This is
because more extracted-and-edited sentences lead
to a higher recall, so more useful information will
be used to guarantee the translation quality. The
comparative translation loss can avoid the model
suffering from the noise while taking advantage of
more information. In other words, it is more likely
to project the source sentence into the probable re-
gion in the target language space with more sen-
tences serving as the anchor points, and the com-
parative learning scheme iteratively approximates
towards more accurate target points. This high-
lights the importance of the extraction number k,
which we further discuss next.

4.4 Ablation Study

3Note that for a fair comparison, we are not including the
results of the unsupervised phrase-based statistical machine
translation system (PBSMT). But theoretically, our extract-
edit learning framework can be generalized to other types of
machine translation systems such as PBSMT.

Figure 3: The effect of the number k of the extracted
sentences in our approach on en→fr translation.

The Effect of Extraction Number k As shown
in Table 1, the number k of the extracted-and-
edited sentences plays a vital role in our approach.
Thus for a more intuitive overview of its impact,
we further train and evaluate multiple models with
k = 1, 3, 5, 8, 10 on en → fr translation task.
The detailed results are shown in Figure 3. This
study shows that the translation performance of
our approach is indeed improving as k increases.
Because as we analyze above, larger k ensure a
higher recall and thus more critical semantic in-
formation can be utilized to assist the translation.
Besides, the diversity of extracted-and-edited sen-
tences can potentially provide a more accurate lo-
calization of the probable region where the target
sentence should be. Although we can infer that the
models would perform even better with k > 10
from Figure 3, more computational resources will
be required for that and we already observe a de-
celerated growth of BLEU scores from k = 8 to
k = 10. Therefore, in this paper, we set k = 10
for the full models.

The Quality of Extraction Model In this sec-
tion, we quantitatively evaluate the unsupervised
extraction part of our model and compare it with
the state-of-the-art supervised extraction model.
Following Grégoire and Langlais (2018), we train
a fully supervised parallel pair extraction model,
where two Bi-LSTMs are implemented to encode
sentences of two languages, and a feed-forward
network is followed to culminate in a sigmoid
output layer. The model is trained with around
500, 000 English-French parallel sentence pairs
sampled from Europarl corpus (Koehn, 2005). As
for our unsupervised extraction model, we directly
use the jointly trained extraction part in our frame-



1180

Noise Model Hits@1 Hits@3 Hits@5 Hits@8 Hits@10 Hits@15 Hits@20

0% Supervised (Upperbound) 67.3 80.7 89.9 94.5 97.1 98.7 99.3Unsupervised (Ours) 52.2 54.6 68.8 80.2 89.1 91.8 93.3

50% Supervised (Upperbound) 64.8 78.0 86.8 91.3 95.6 97.4 99.0Unsupervised (Ours) 46.9 49.7 62.1 73.4 83.2 87.6 89.2

90% Supervised (Upperbound) 63.7 76.4 84.2 89.1 93.8 96.5 98.1Unsupervised (Ours) 41.5 46.8 58.0 69.3 77.2 83.9 87.8

Table 2: The experimental results of parallel sentence mining on the newstest 2012 en→ fr translation dataset with
different levels of added sentence noises. Metric: The percentage of Hits@k.

Cell Learning BLEU

LSTM MLE Loss 12.40Comparative Loss 24.54

Transformer MLE Loss 14.15Comparative Loss 25.44

Table 3: The performance of the unsupervised NMT
systems with different learning objectives on en → fr
newstest 2014.

work to extract the potential parallel sentences
based on the scores computed by Equation 4. For
evaluation, we sample 1, 000 parallel sentences
from the newstest 2014 corpus and create three
test sets with a noise ratio 0%, 50%, and 90% to
simulate noisy real-world data. We report Hits@k
results, which shows the percentage of the golden
parallel sentences appear within the top-k place.

The detailed results are shown in Table 2. Al-
though our extraction model structure is different
from the supervised extraction model, it can still
give us a good insight into the upperbound and
gap of performance. We can observe a noticeable
gap between unsupervised and supervised meth-
ods, but the gap is narrowing as the rank increases.
Meanwhile, in our unsupervised method, the per-
formance grows quickly when k ≤ 10. From Ta-
ble 2 we also notice that k = 10 is a sweet point,
where the accuracy is high and the computational
cost is relatively acceptable.

The Effect of Comparative Translation Fi-
nally, we aim to roughly evaluate the effect of
the proposed comparable translation loss in our
model. Thus, we compare our model with a two-
staged NMT system, where we extract and edit the
parallel pairs and retrain the NMT system with the
standard maximum likelihood estimation (MLE)
loss in a supervised way (by taking the extracted-
and-edited sentences as the ground-truth targets).
We compare the performance on the en→ fr new-

stest 2014 dataset, and the results are shown in Ta-
ble 3. We can observe that with the MLE loss,
the translation performance will drop nearly 50%.
The results indirectly reflect that the NMT sys-
tems are sensitive to noises in the training datasets.
Meanwhile, it demonstrates by treating extracted-
edit sentences as pivotal points instead of ground
truth, our proposed comparative translation loss
can avoid the NMT model suffers from the noise.

4.5 Discussion

Although our extract-edit approach can achieve
better performance than the back-translation
mechanism, it is still worth mentioning that our
approach has more strict constraints on the do-
mains of the source and target corpus. The extract-
edit approach will work well when there is infor-
mation overlap in the two language spaces. When
there is little overlap in terms of domains, it will
be much harder to find a good cluster of initial
candidates, which may also complicate the edit-
ing process. As for the back-translation mecha-
nism, it requires less overlap in terms of the lan-
guage spaces because the language priors can be
learnt in any domains. However, the corpus with
matching domains can be easily obtained nowa-
days (e.g., Wikipedia and the news articles), which
makes our extract-edit approach still widely appli-
cable.

5 Related Work

Unsupervised NMT The current NMT sys-
tems (Sutskever et al., 2014; Cho et al., 2014a;
Bahdanau et al., 2015; Gehring et al., 2017;
Vaswani et al., 2017) are known to easily over-
fit and result in an inferior performance when
the training data is limited (Koehn and Knowles,
2017; Isabelle et al., 2017; Sennrich, 2017). Many
research efforts have been spent on how to uti-
lize the monolingual data to improve the NMT



1181

system when only limited supervision is avail-
able (Gulcehre et al., 2015; Sennrich et al., 2016a;
He et al., 2016; Zhang and Zong, 2016; Yang et al.,
2018). Recently, Lample et al. (2018a); Artetxe
et al. (2018); Lample et al. (2018b) make en-
couraging progress on unsupervised NMT struc-
ture mainly based on initialization, denoising lan-
guage modeling, and back-translation. However,
all these unsupervised models are based on the
back-translation learning framework to generate
pseudo language pairs for training. Our work
leverages the information from real target lan-
guage sentences.

Comparable Corpora Mining Comparable
corpora mining aims at extracting parallel sen-
tences from comparable monolingual corpora
such as news stories written on the same topic
in different languages. Most of the previous
methods align the documents based on metadata
and then extract parallel sentences using human-
defined features (Munteanu and Marcu, 2002,
2006; Hewavitharana and Vogel, 2011). Recent
neural-based methods (Chu et al., 2016; Grover
and Mitra, 2017; Grégoire and Langlais, 2018)
learn to identify parallel sentences in the semantic
spaces. However, these methods require large
amounts of parallel sentence pairs to train the
systems first and then test the performance on
raw comparable corpora, which does not apply
to languages with limited resources. Instead, we
explore the corpora mining in an unsupervised
fashion and propose a joint training framework
with machine translation.

Retrieval-Augmented Text Generation Our
work is also related to the recent work on apply-
ing retrieval mechanisms to augment text genera-
tion, such as image captioning (Kuznetsova et al.,
2013; Mason and Charniak, 2014), dialogue gen-
eration (Song et al., 2016; Yan et al., 2016; Wu
et al., 2018) and style transfer (Lin et al., 2017;
Li et al., 2018). Some editing-based models (Guu
et al., 2018; Hashimoto et al., 2018) are proposed
to further enhance the retrieved text. Recent work
in machine translation (Gu et al., 2018) augments
an NMT model with sentence pairs retrieved by an
off-the-shelf search engine. However, these meth-
ods are two-staged with supervised retrieval first.
In our work, the extracted-edited sentences are not
directly used as the ground truth to train the trans-
lation model. Instead, we view these sentences as

pivotal points in the target language space and fur-
ther we propose a comparative translation loss to
train the system in a fully unsupervised way.

6 Conclusion

In this paper, we propose an extract-edit approach,
an effective alternative to the widely-used back-
translation in unsupervised NMT. Instead of gen-
erating pseudo language pairs to train the systems
with the reconstruction loss, we design a com-
parative translation loss that leverages real sen-
tences in the target language space. Empirically,
our method advances the previous state-of-the-art
NMT systems across four language pairs using the
monolingual corpora only. Theoretically, we be-
lieve the extract-edit learning framework can be
generalized to other types of unsupervised ma-
chine translation systems and even some other un-
supervised learning tasks.

Acknowledgments

The authors would like to thank the anonymous re-
viewers for their thoughtful comments. The work
was supported by the Facebook Low Resource
Neural Machine Translation Research Award. The
authors are solely responsible for the contents of
the paper, and the opinions expressed in this publi-
cation do not reflect those of the funding agencies.

References

Mikel Artetxe, Gorka Labaka, Eneko Agirre, and
Kyunghyun Cho. 2018. Unsupervised neural ma-
chine translation. In Proceedings of the 6th Inter-
national Conference on Learning Representations
(ICLR).

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. Proceedings of the
4th International Conference for Learning Repre-
sentations (ICLR).

Piotr Bojanowski, Edouard Grave, Armand Joulin,
and Tomas Mikolov. 2017. Enriching word vec-
tors with subword information. Transactions of the
Association of Computational Linguistics (TACL),
5(1):135–146.

Yong Cheng, Zhaopeng Tu, Fandong Meng, Junjie
Zhai, and Yang Liu. 2018. Towards robust neural
machine translation. In Proceedings of the 56th An-
nual Meeting of the Association for Computational
Linguistics (ACL).



1182

Kyunghyun Cho, Bart van Merrienboer, Dzmitry Bah-
danau, and Yoshua Bengio. 2014a. On the proper-
ties of neural machine translation: Encoder–decoder
approaches. In Proceedings of SSST-8, Eighth Work-
shop on Syntax, Semantics and Structure in Statisti-
cal Translation, pages 103–111.

Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014b. Learning
phrase representations using rnn encoder–decoder
for statistical machine translation. In Proceedings of
the 2014 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1724–
1734.

Chenhui Chu, Raj Dabre, and Sadao Kurohashi. 2016.
Parallel sentence extraction from comparable cor-
pora with neural network features. In Proceedings
of 9th Language Resources and Evaluation Confer-
ence (LREC).

Alexis Conneau, Guillaume Lample, Marc’Aurelio
Ranzato, Ludovic Denoyer, and Hervé Jégou. 2018.
Word translation without parallel data. In Proceed-
ings of the 6th International Conference on Learning
Representations (ICLR).

Jonas Gehring, Michael Auli, David Grangier, De-
nis Yarats, and Yann N Dauphin. 2017. Convolu-
tional sequence to sequence learning. In Proceed-
ings of the 34th International Conference on Ma-
chine Learning (ICML), pages 1243–1252.

Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,
Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. 2014. Generative ad-
versarial nets. In Proceedings of 28th Conference on
neural information processing systems (NeurIPS),
pages 2672–2680.

Francis Grégoire and Philippe Langlais. 2018. Extract-
ing parallel sentences with bidirectional recurrent
neural networks to improve machine translation. In
Proceedings of the 27th International Conference on
Computational Linguistics (COLING).

Jeenu Grover and Pabitra Mitra. 2017. Bilingual word
embeddings with bucketed cnn for parallel sentence
extraction. In Proceedings of ACL 2017, Student Re-
search Workshop, pages 11–16.

Jiatao Gu, Yong Wang, Kyunghyun Cho, and Vic-
tor OK Li. 2018. Search engine guided non-
parametric neural machine translation. In Proceed-
ings of the 32th AAAI Conference on Artificial Intel-
ligence (AAAI).

Caglar Gulcehre, Orhan Firat, Kelvin Xu, Kyunghyun
Cho, Loic Barrault, Huei-Chi Lin, Fethi Bougares,
Holger Schwenk, and Yoshua Bengio. 2015. On us-
ing monolingual corpora in neural machine transla-
tion. In Proceedings of the 3rd International Con-
ference on Learning Representations (ICLR).

Kelvin Guu, Tatsunori B Hashimoto, Yonatan Oren,
and Percy Liang. 2018. Generating sentences by
editing prototypes. Transactions of the Association
of Computational Linguistics (TACL), 6:437–450.

Tatsunori B Hashimoto, Kelvin Guu, Yonatan Oren,
and Percy S Liang. 2018. A retrieve-and-edit frame-
work for predicting structured outputs. In Proceed-
ings of 32th Conference on Neural Information Pro-
cessing Systems (NeurIPS), pages 10073–10083.

Di He, Yingce Xia, Tao Qin, Liwei Wang, Nenghai Yu,
Tieyan Liu, and Wei-Ying Ma. 2016. Dual learn-
ing for machine translation. In Proceedings of Ad-
vances in Neural Information Processing Systems
(NeurIPS), pages 820–828.

Sanjika Hewavitharana and Stephan Vogel. 2011. Ex-
tracting parallel phrases from comparable data. In
Proceedings of the 4th Workshop on Building and
Using Comparable Corpora: Comparable Corpora
and the Web, pages 61–68.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Pierre Isabelle, Colin Cherry, and George Foster. 2017.
A challenge set approach to evaluating machine
translation. In Proceedings of the 2017 Conference
on Empirical Methods in Natural Language Pro-
cessing (EMNLP), pages 2486–2496.

Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2017.
Billion-scale similarity search with gpus. arXiv
preprint arXiv:1702.08734.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. In Proceedings
of the 3rd International Conference for Learning
Representations (ICLR).

Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT summit, vol-
ume 5, pages 79–86.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al. 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of the 45th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
177–180.

Philipp Koehn and Rebecca Knowles. 2017. Six chal-
lenges for neural machine translation. In Pro-
ceedings of the First Workshop on Neural Machine
Translation, pages 28–39.

Polina Kuznetsova, Vicente Ordonez, Alexander Berg,
Tamara Berg, and Yejin Choi. 2013. Generalizing
image captions for image-text parallel corpus. In
Proceedings of the 51st Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
790–796.



1183

Guillaume Lample, Alexis Conneau, Ludovic Denoyer,
and Marc’Aurelio Ranzato. 2018a. Unsupervised
machine translation using monolingual corpora only.
In Proceedings of the 6th International Conference
on Learning Representations (ICLR).

Guillaume Lample, Myle Ott, Alexis Conneau, Lu-
dovic Denoyer, and Marc’Aurelio Ranzato. 2018b.
Phrase-based & neural unsupervised machine trans-
lation. In Proceedings of the 2018 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 5039–5049.

Juncen Li, Robin Jia, He He, and Percy Liang. 2018.
Delete, retrieve, generate: a simple approach to sen-
timent and style transfer. In Proceedings of the
2018 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies (NAACL-HLT), pages
1865–1874.

Kevin Lin, Dianqi Li, Xiaodong He, Zhengyou Zhang,
and Ming-Ting Sun. 2017. Adversarial ranking for
language generation. In Proceedings of the 31th
Conference on Neural Information Processing Sys-
tems (NeurIP), pages 3155–3165.

Rebecca Mason and Eugene Charniak. 2014. Domain-
specific image captioning. In Proceedings of the
18th Conference on Computational Natural Lan-
guage Learning (CoNLL), pages 11–20.

Dragos Stefan Munteanu and Daniel Marcu. 2002.
Processing comparable corpora with bilingual suf-
fix trees. In Proceedings of the 2002 Conference on
Empirical Methods in Natural Language Processing
(EMNLP).

Dragos Stefan Munteanu and Daniel Marcu. 2006. Ex-
tracting parallel sub-sentential fragments from non-
parallel corpora. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Compu-
tational Linguistics (COLING-ACL), pages 81–88.

Rico Sennrich. 2017. How grammatical is character-
level neural machine translation? assessing mt qual-
ity with contrastive translation pairs. In Proceed-
ings of the 15th Conference of the European Chap-
ter of the Association for Computational Linguistics
(EACL), pages 376–382.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016a. Improving neural machine translation mod-
els with monolingual data. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 86–96.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016b. Neural machine translation of rare words
with subword units. In Proceedings of the 54th An-
nual Meeting of the Association for Computational
Linguistics (ACL), pages 1715–1725.

Yiping Song, Rui Yan, Xiang Li, Dongyan Zhao, and
Ming Zhang. 2016. Two are better than one: An en-
semble of retrieval-and generation-based dialog sys-
tems. arXiv preprint arXiv:1610.07149.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Proceedings of Advances in neural infor-
mation processing systems (NeurIPS), pages 3104–
3112.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Proceedings of 31st Conference on
Neural Information Processing Systems (NeurIPS),
pages 5998–6008.

Yu Wu, Furu Wei, Shaohan Huang, Zhoujun Li,
and Ming Zhou. 2018. Response generation by
context-aware prototype editing. arXiv preprint
arXiv:1806.07042.

Rui Yan, Yiping Song, and Hua Wu. 2016. Learning
to respond with deep neural networks for retrieval-
based human-computer conversation system. In
Proceedings of the 39th International ACM SIGIR
conference on Research and Development in Infor-
mation Retrieval (SIGIR), pages 55–64.

Zhen Yang, Wei Chen, Feng Wang, and Bo Xu.
2018. Unsupervised neural machine translation with
weight sharing. In Proceedings of the 56th Annual
Meeting of the Association for Computational Lin-
guistics (ACL), pages 46–55.

Jiajun Zhang and Chengqing Zong. 2016. Exploit-
ing source-side monolingual data in neural machine
translation. In Proceedings of the 2016 Conference
on Empirical Methods in Natural Language Pro-
cessing (EMNLP), pages 1535–1545.


