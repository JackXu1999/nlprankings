



















































Native Language Identification on Text and Speech


Proceedings of the 12th Workshop on Innovative Use of NLP for Building Educational Applications, pages 398–404
Copenhagen, Denmark, September 8, 2017. c©2017 Association for Computational Linguistics

Native Language Identification on Text and Speech

Marcos Zampieri1, Alina Maria Ciobanu2, Liviu P. Dinu2
1University of Wolverhampton, United Kingdom

2University of Bucharest, Romania
marcos.zampieri@uni-koeln.de

Abstract

This paper presents an ensemble system
combining the output of multiple SVM
classifiers to native language identification
(NLI). The system was submitted to the
NLI Shared Task 2017 fusion track which
featured students essays and spoken re-
sponses in form of audio transcriptions
and iVectors by non-native English speak-
ers of eleven native languages. Our sys-
tem competed in the challenge under the
team name ZCD and was based on an en-
semble of SVM classifiers trained on char-
acter n-grams achieving 83.58% accuracy
and ranking 3rd in the shared task.

1 Introduction

Native language identification (NLI) is the task of
automatically identifying non-native speakers’ na-
tive language based on their foreign language pro-
duction. As evidenced in Malmasi (2016) NLI is a
vibrant research area in NLP and is usually mod-
eled as single-label text classification.

NLI is based on the assumption that the mother
tongue influences second language acquisition
(SLA) and production. Corpora containing texts
and utterances by non-native speakers are used to
train systems that are able to recognize features
that are prominent in the production of speakers
of a particular native language. These features are
subsequently used to identify texts (or utterances)
that are likely to be written or spoken by speakers
of the same language.

There are two important reasons to study NLI.
Firstly, there is SLA. NLI methods can be ap-
plied to learner corpora to investigate the influence
of native language in second language acquisi-
tion and production complementing corpus-based
and corpus-driven studies. The second reason is a

practical one. NLI methods can be an important
part of several NLP systems including, for exam-
ple, author profiling systems developed for foren-
sic linguistics.

This paper presents the system submitted by the
ZCD team to the NLI Shared Task 2017 (Mal-
masi et al., 2017). The organizers of the chal-
lenge provided participants with a dataset contain-
ing essays and spoken responses in form of tran-
scriptions and acoustic features (iVectors) by non-
native English speakers of eleven native languages
taking a standardized assessment of English profi-
ciency for academic purposes. Native languages
included are: Arabic, Chinese, French, German,
Hindi, Italian, Japanese, Korean, Spanish, Telugu,
and Turkish. To discriminate between these eleven
native languages we apply an ensemble of multi-
ple linear SVM classifiers trained on character n-
grams. The main motivation behind the choice of
this approach is the success of linear SVMs and
SVM ensembles in NLI and in similar text classi-
fication tasks such as dialect, language variety, and
similar language identification as will be discussed
in Section 2.

2 Related Work

There have been several NLI studies published
in the past few years. Due to the availability
of suitable language resources for English (e.g.
learner corpora), the vast majority of these stud-
ies dealt with English (Brooke and Hirst, 2012;
Bykh and Meurers, 2014), however, a few NLI
studies have been published on other languages.
Examples of NLI applied to languages other than
English include Arabic (Ionescu, 2015), Chinese
(Wang et al., 2016), and Finnish (Malmasi and
Dras, 2014).

To the best of our knowledge, the NLI Shared
Task 2013 (Tetreault et al., 2013) was the first

398



Team Approach System Paper
Jarvis SVM trained on character n-grams (1-9), word n-grams (1-4),

and POS n-grams (1-4)
(Jarvis et al., 2013)

Oslo SVM trained on character n-grams (1-7) (Lynum, 2013)
Unibuc String Kernels and Local Rank Distance (LRD) (Popescu and Ionescu, 2013)
MITRE Bayes ensemble of multiple classifiers (Henderson et al., 2013)
Tuebingen SVM trained on word n-grams (1-2), and POS n-grams (1-5), and

syntactic features (dependencies)
(Bykh et al., 2013)

NRC Ensemble of SVM classifiers trained on character trigrams, word
n-grams (1-2), POS n-grams (2-4), and syntactic features (depen-
dencies)

(Goutte et al., 2013)

CMU-Haifa Maximum Entropy trained on word n-grams (1-4), POS n-grams
(1-4), and spelling features

(Tsvetkov et al., 2013)

Cologne-Nijmegen SVM classifier with TF-IDF weighting trained on character n-
grams (1-6), word n-grams (1-2), and POS n-grams (1-4)

(Gebre et al., 2013)

NAIST SVM trained on character n-grams (2-3), word n-grams (1-2),
and POS n-grams (2-3), and syntactic features (dependencies and
TSG)

(Mizumoto et al., 2013)

UTD SVM trained on word n-grams (1-2) (Wu et al., 2013)

Table 1: Top ten NLI Shared Task 2013 entries ordered by performance.

shared task to provide a benchmark for NLI focus-
ing on written texts by non-native English speak-
ers. A few years later, the 2016 Computational
Paralinguistics Challenge (Schuller et al., 2016)
included an NLI task on speech data. The NLI
Shared Task 2017 combines these two modalities
of non-native language production by including
essays and spoken responses of test takers in form
of transcriptions and iVectors.

The combination of text and speech has been
previously used in similar shared tasks such as the
dialect identification shared tasks organized at the
VarDial workshop series (Zampieri et al., 2017)
and described in more detail in Section 2.2.

In the next sections we present the most suc-
cessful entries submitted for the NLI Shared Task
2013 and their overlap with methods applied to di-
alect, language variety, and similar language iden-
tification.

2.1 NLI Shared Task 2013

The aforementioned NLI Shared Task 2013
(Tetreault et al., 2013) established the first bench-
mark for NLI on written texts. Organizers of
the first NLI task provided participants with the
TOEFL 11 (Blanchard et al., 2013) dataset which
contained essays written by students native speak-
ers of the same eleven languages included in the
NLI Shared Task 2017.

Twenty-nine teams participated in the competi-
tion, testing a wide range of computational meth-
ods for NLI. In Table 1 we list the top ten best

entries ranked by performance along with their re-
spective system description papers.

The best system by Jarvis et al. (2013) applied
a linear SVM classifier trained on character, word,
and POS n-grams. Seven out of the ten best en-
tries in the shared task used SVM classifiers. This
indicates that SMVs are a very good fit for NLI
and motivates us to test SVM classifiers in our
ensemble-based system described in this paper.

2.2 Overlap with Dialect Identification

In the last few years, we observed a significant
and important overlap between NLI approaches
and computational methods applied to dialect, lan-
guage variety, and similar language identification.
So far the overlap between the two tasks has not
been substantially explored in the literature.

Members of several teams that submitted sys-
tems to the NLI Shared Task 2013, some of them
presented in Table 1, also participated in the di-
alect identification shared tasks organized within
the scope of the VarDial workshop series held
from 2014 to 2017. The three related shared tasks
organized at the VarDial workshop thus far are the
Discriminating between Similar Languages (DSL)
task organized from 2014 to 2017, Arabic Di-
alect Identification (ADI) organized in 2016 and
2017, and German Dialect Identification (GDI) or-
ganized in 2017.

Next we list some of the teams that adapted sys-
tems from NLI to dialect identification in the past
few years.

399



• Variations of the string kernels method by the
Unibuc team (Popescu and Ionescu, 2013)
competed in the ADI task in 2016 (Ionescu
and Popescu, 2016) and in 2017 (Ionescu and
Butnaru, 2017) achieving the best results.

• Cologne-Nijmegen’s TF-IDF-based ap-
proach (Gebre et al., 2013) competed in
the DSL shared task 2015 (Zampieri et al.,
2015a) as team MMS ranking among the top
3 systems.

• A variation of NRC’s SVM approach (Goutte
et al., 2013) competed in the DSL 2014
(Goutte et al., 2014) achieving the best re-
sults.

• Bobicev applied Prediction for Partial Match-
ing (PPM) in the NLI shared task (Bobicev,
2013) with results that did not reach top
ten performance. A similar improved ap-
proached competed in the DSL 2015 (Bo-
bicev, 2015) ranking in the top half of the ta-
ble.

• A similar approach to the one by Jarvis
(Jarvis et al., 2013) that ranked 1st place in
the NLI task 2013 competed in the DSL 2017
(Bestgen, 2017), achieving the best perfor-
mance in the competition.

• Variations of MQ’s SVM ensemble approach
(Malmasi et al., 2013) have competed in
the DSL 2015 (Malmasi and Dras, 2015)
and the ADI 2016 (Malmasi and Zampieri,
2016), achieving the best performance in
both shared tasks.

This section evidenced an important overlap be-
tween NLI methods and dialect identification
methods both in terms of participation overlap in
the shared tasks and in terms of successful ap-
proaches. With the exception of Bobicev (2013),
most teams that were ranked among the top ten en-
tries in the NLI shared task were also successful at
the VarDial workshop shared tasks.

Detailed information about all approaches and
performance obtained in these competitions can
be found in the VarDial shared task reports
(Zampieri et al., 2014, 2015b; Malmasi et al.,
2016b; Zampieri et al., 2017) and in the evaluation
paper by Goutte et al. (2016).

3 Methods

In the next sections we describe the data provided
by the shared task organizers and the ensemble
SVM approach applied by the ZCD team.

3.1 Data

The organizers of the NLI Shared Task 2017
provided participants with data corresponding to
eleven native languages: Arabic, Chinese, French,
German, Hindi, Italian, Japanese, Korean, Span-
ish, Telugu and Turkish. The training dataset
consists of 11,000 essays, orthographic transcrip-
tions of 45-second English spoken responses, and
iVectors (1,000 instances for each of the eleven
native languages), while the development dataset
was stratified similarly, containing 100 instances
for each native language.

There were individual tracks in which only the
essays or only the responses could be used and
a fusion track in which both the essays and the
speech transcriptions (including iVectors) could be
used. The test dataset, containing 1,100 instances
with essays, speech transcriptions and iVectors,
was released at a later date.

The use of a dataset containing text and speech
is the main new aspect of the 2017 NLI task so we
decide to compete in the fusion track taking both
modalities into account. The approach used in our
submission is described next.

3.2 Approach

We built a classification system based on SVM en-
sembles, following the methodology proposed by
Malmasi and Dras (2015).

The idea behind classification ensembles is to
improve the overall performance by combining
the results of multiple classifiers. Such systems
have proved successful not only in NLI and di-
alect identification, as evidenced in the previous
sections, but also in numerous text classification
tasks, among which are complex word identifica-
tion (Malmasi et al., 2016a) and grammatical er-
ror diagnosis (Xiang et al., 2015). The classifiers
can differ in a wide range of aspects, such as algo-
rithms, training data, features or parameters.

In our system, the classifiers used different fea-
tures. We experimented with the following fea-
tures: character n-grams (with n in {1, ..., 10})
from essays and speech transcripts, word n-grams
(with n in {1, 2}) from essays and speech tran-
scripts, and iVectors. For the n-gram features we

400



System F1 (macro) Accuracy
Essays + Transcriptions + iVectors 0.8358 0.8355
Essays + Transcriptions 0.8191 0.8191
Official Baseline (with iVectors) 0.7901 0.7909
Official Baseline (without iVectors) 0.7786 0.7791
Random Baseline 0.0909 0.0909

Table 2: ZCD results and baselines for the fusion track.

used TF-IDF weighting applied on the tokenized
version of the essays and speech transcripts (pro-
vided by the organizers). As a pre-processing step,
we lowercased all words.

We first trained a classifier for each type of
feature using the essays as input data, and per-
formed cross-validation to determine the optimal
value for the SVM hyperparameter C, searching
in {10−5, ..., 105}. Further, for the n-gram fea-
tures we kept only those classifiers whose individ-
ual cross-validation performance was higher than
0.8. Thus, our first ensemble consisted of individ-
ual classifiers using character n-grams (with n in
{6, 7, 8}) from essays and speech transcripts.

For the second ensemble, we introduced an
additional classifier using the iVectors as fea-
tures. To combine the classifiers, we employed a
majority-based fusion method: the class label pre-
dicted by the ensemble is the one that was pre-
dicted by the majority of the classifiers. We used
the SVM implementation provided by Scikit-learn
(Pedregosa et al., 2011), based on the Liblinear li-
brary (Fan et al., 2008).

On the development dataset, the first ensemble
(essays + speech transcripts) obtained 0.83 accu-
racy, and the second ensemble (essays + speech
transcripts + iVectors) obtained 0.84 accuracy.

4 Results

We submitted two runs of our system. The first
run included the essays and the transcriptions of
responses, whereas the second run included also
the iVectors. We present the results obtained by
the two runs along with a random baseline and the
performance of the unigram-based official base-
line system in terms of F1 score and accuracy in
Table 2.

The best results were achieved by the second
run, reaching 83.55% accuracy and 83.58% F1
score. As can be seen in Table 2, the iVectors bring
a performance improvement of about 1.6 percent-
age points in terms of accuracy and F1 score.

Ten teams participated in the fusion track and
our best run was ranked 3rd by the shared task or-
ganizers. Ranks were calculated using McNemars
test for statistical significance, a common practice
in many NLI shared tasks (e.g. DSL 2016 (Mal-
masi et al., 2016b), and the shared tasks at WMT
(Bojar et al., 2016)).

The confusion matrix of our best submission is
presented in Table 3. We observed that the best
performance was obtained for Japanese and the
worst performance was obtained for Arabic. Not
surprisingly, most confusion occurred between
Hindi and Telugu. Our initial analysis indicates
that this confusion occurred because of geographic
proximity and not by intrinsic linguistic proper-
ties shared by these two languages, as Hindi and
Telugu do not belong to the same language family
- Hindi is a Hindustani language and Telugu is a
Dravidian language.

5 Most Informative Features

As briefly discussed in the introduction of this pa-
per, NLI methods can provide interesting informa-
tion about patterns in non-native language that can
be used to study second language acquisition and
L1 interference or language transfer. For this pur-
pose, in Table 4 we present the top ten most in-
formative character 8-grams for each of the eleven
languages in the dataset according to our classifier.

It is not surprising that named entities are very
informative for our system and highly discrimina-
tive for most native languages. For example, es-
says and responses from China often contain place
names like China, Taipei, Taiwan, and Beijing,
whereas those from Turkey contain Istanbul and,
of course, Turkey. These features are very frequent
in essays and responses by Chinese and Turkish
speakers due to topical bias and not because of any
intrinsic linguistic property of Chinese or Turkish.
However, in other languages, interesting linguis-
tic patterns can be identified by looking at these
features.

401



CHI JPN KOR HIN TEL FRE ITA SPA GER ARA TUR
CHI 91 3 2 0 0 0 2 0 0 1 1
JPN 2 93 2 0 1 1 0 0 1 0 0
KOR 4 14 77 0 0 1 1 1 0 1 1
HIN 1 0 1 80 18 0 0 0 0 0 0
TEL 0 0 1 18 78 0 0 1 0 2 0
FRE 2 0 0 2 1 87 5 0 2 1 0
ITA 0 0 0 1 0 6 85 3 3 2 0
SPA 1 1 2 2 1 4 7 77 2 2 1
GER 0 1 0 3 0 3 2 1 90 0 0
ARA 2 2 2 3 2 7 1 2 1 77 1
TUR 1 2 0 3 0 2 3 1 1 3 84

Table 3: Confusion matrix on the test set.

Language Most Informative Features
Arabic |alot of | alot of|y thing | statmen|statment|e alot o|tatment |ery thin|very thi|every th|
Chinese | i think| taiwan |i think |beijing | beijing| taipei | in chin|in china|n china | chinese|
French | indeed |. indeed| . indee|indeed ,|ndeed , | france |developp| french |to concl|o conclu|
German | , that | and um | germany|germany | berlin |, that t|have to | have to| um yeah|um yeah |
Hindi | towards|towards |as compa| as comp|various | various|s compar| enjoyme| mumbai | behind |
Italian |hink tha|nk that |ink that| in ital|n italy |in fact |in italy| in fact|i think | italian|
Japanese |in japan|n japan | in japa|apanese | japanes|japanese| japan ,|japan , |i disagr| japan .|
Korean | korean |in korea| in kore|n korea | however|however |korea , | korea ,| . howev|. howeve|
Spanish | mexico |oing to |going to| going t| diferen|le that | the cit|es that |diferent|ple that|
Telugu |ing the |hyderaba| hyderab|yderabad|derabad | subject| we can | i concl|i conclu|where as|
Turkish | turkey |istanbul|stanbul | istanbu| uh and |n turkey| in turk|in turke|s about | . becau|

Table 4: Top ten most informative character 8-grams for each language.

In the most informative features for French, for
example, we find developp from the French
développé which leads to a misspelling of the En-
glish word developed. In Arabic we observed a
number of features that indicate misspellings. The
Arabic alphabet is very different from the Latin
one, making spelling English words particularly
challenging for native speakers of Arabic. The top
ten most informative features for Arabic include
word boundary errors such as every thing for ev-
erything, and alot for a lot, as well as the omission
of vowels such as statment for statement.

6 Conclusion

To the best of our knowledge, the NLI Shared Task
2017 fusion track was the first shared task to pro-
vide both written and spoken data for NLI. It was
an interesting opportunity to evaluate the perfor-
mance of NLI methods beyond written texts.

In this paper we highlighted the overlap be-
tween NLI and dialect, language variety, and sim-
ilar language identification and used an approach

that achieved high results in both tasks. We ap-
plied an SVM ensemble approach trained char-
acter n-grams achieving competitive results of
83.55% accuracy ranking 3rd in the fusion track.

Even though the results obtained by our ap-
proach were not low, we believe that there is still
room for improvement. In previous shared tasks
(e.g. NLI 2013, DSL 2015, and ADI 2016) we ob-
served that SVM ensembles ranked higher in the
results tables than our method did in the NLI 2017.
We are investigating whether the combination of
features or the implementation itself can be opti-
mized for better performance.

Acknowledgements

We would like to thank the NLI Shared Task 2017
organizers for making the dataset available and for
replying promptly to all our inquiries. We further
thank the anonymous reviewers for their valuable
feedback.

Liviu P. Dinu is supported by UEFISCDI,
project number 53BG/2016.

402



References
Yves Bestgen. 2017. Improving the Character Ngram

Model for the DSL Task with BM25 Weighting and
Less Frequently Used Feature Sets. In Proceedings
of the Fourth Workshop on NLP for Similar Lan-
guages, Varieties and Dialects (VarDial). Valencia,
Spain, pages 115–123.

Daniel Blanchard, Joel Tetreault, Derrick Higgins,
Aoife Cahill, and Martin Chodorow. 2013. Toefl11:
A corpus of non-native english. Technical report,
Educational Testing Service.

Victoria Bobicev. 2013. Native language identification
with ppm. In Proceedings of the Eighth Workshop
on Innovative Use of NLP for Building Educational
Applications. Atlanta, Georgia, pages 180–187.

Victoria Bobicev. 2015. Discriminating between sim-
ilar languages using ppm. In Proceedings of
the Joint Workshop on Language Technology for
Closely Related Languages, Varieties and Dialects
(LT4VarDial). Hissar, Bulgaria, pages 59–65.

Ondrej Bojar, Rajen Chatterjee, Christian Federmann,
Yvette Graham, Barry Haddow, Matthias Huck, An-
tonio Jimeno Yepes, Philipp Koehn, Varvara Lo-
gacheva, Christof Monz, et al. 2016. Findings of the
2016 Conference on Machine Translation. In Pro-
ceedings of WMT .

Julian Brooke and Graeme Hirst. 2012. Measuring
Interlanguage: Native Language Identification with
L1-influence Metrics. In Proceedings of Language
Resources and Evaluation (LREC). pages 779–784.

Serhiy Bykh and Detmar Meurers. 2014. Exploring
Syntactic Features for Native Language Identifica-
tion: A Variationist Perspective on Feature Encod-
ing and Ensemble Optimization. In Proceedings
of COLING 2014, the 25th International Confer-
ence on Computational Linguistics: Technical Pa-
pers. Dublin, Ireland, pages 1962–1973.

Serhiy Bykh, Sowmya Vajjala, Julia Krivanek, and
Detmar Meurers. 2013. Combining shallow and
linguistically motivated features in native language
identification. In Proceedings of the Eighth Work-
shop on Innovative Use of NLP for Building Edu-
cational Applications. Atlanta, Georgia, pages 197–
206.

Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A Library for Large Linear Classification. Journal
of Machine Learning Research 9:1871–1874.

Binyam Gebrekidan Gebre, Marcos Zampieri, Peter
Wittenburg, and Tom Heskes. 2013. Improving na-
tive language identification with tf-idf weighting. In
Proceedings of the Eighth Workshop on Innovative
Use of NLP for Building Educational Applications.
Atlanta, Georgia, pages 216–223.

Cyril Goutte, Serge Léger, and Marine Carpuat. 2013.
Feature space selection and combination for na-
tive language identification. In Proceedings of the
Eighth Workshop on Innovative Use of NLP for
Building Educational Applications. Atlanta, Geor-
gia, pages 96–100.

Cyril Goutte, Serge Léger, and Marine Carpuat. 2014.
The nrc system for discriminating similar languages.
In Proceedings of the First Workshop on Applying
NLP Tools to Similar Languages, Varieties and Di-
alects (VarDial). Dublin, Ireland, pages 139–145.

Cyril Goutte, Serge Léger, Shervin Malmasi, and Mar-
cos Zampieri. 2016. Discriminating Similar Lan-
guages: Evaluations and Explorations. In Pro-
ceedings of Language Resources and Evaluation
(LREC).

John Henderson, Guido Zarrella, Craig Pfeifer, and
John D. Burger. 2013. Discriminating non-native
english with 350 words. In Proceedings of the
Eighth Workshop on Innovative Use of NLP for
Building Educational Applications. Atlanta, Geor-
gia, pages 101–110.

Radu Tudor Ionescu. 2015. A Fast Algorithm for Local
Rank Distance: Application to Arabic Native Lan-
guage Identification. In Proceedings of the Interna-
tional Conference on Neural Information Process-
ing. Springer, pages 390–400.

Radu Tudor Ionescu and Andrei Butnaru. 2017. Learn-
ing to identify arabic and german dialects using mul-
tiple kernels. In Proceedings of the Fourth Work-
shop on NLP for Similar Languages, Varieties and
Dialects (VarDial). Valencia, Spain, pages 200–209.

Radu Tudor Ionescu and Marius Popescu. 2016.
UnibucKernel: An Approach for Arabic Dialect
Identification Based on Multiple String Kernels. In
Proceedings of the Third Workshop on NLP for Sim-
ilar Languages, Varieties and Dialects (VarDial3).
Osaka, Japan, pages 135–144.

Scott Jarvis, Yves Bestgen, and Steve Pepper. 2013.
Maximizing classification accuracy in native lan-
guage identification. In Proceedings of the Eighth
Workshop on Innovative Use of NLP for Building
Educational Applications. Atlanta, Georgia, pages
111–118.

André Lynum. 2013. Native language identification us-
ing large scale lexical features. In Proceedings of
the Eighth Workshop on Innovative Use of NLP for
Building Educational Applications. Atlanta, Geor-
gia, pages 266–269.

Shervin Malmasi. 2016. Native Language Identifica-
tion: Explorations and Applications. Ph.D. thesis.

Shervin Malmasi and Mark Dras. 2014. Finnish Native
Language Identification. In Proceedings of the Aus-
tralasian Language Technology Association Work-
shop.

403



Shervin Malmasi and Mark Dras. 2015. Language
identification using classifier ensembles. In Pro-
ceedings of the VarDial Workshop.

Shervin Malmasi, Mark Dras, and Marcos Zampieri.
2016a. LTG at SemEval-2016 Task 11: Complex
Word Identification with Classifier Ensembles. In
Proceedings of SemEval.

Shervin Malmasi, Keelan Evanini, Aoife Cahill, Joel
Tetreault, Robert Pugh, Christopher Hamill, Diane
Napolitano, and Yao Qian. 2017. A Report on the
2017 Native Language Identification Shared Task.
In Proceedings of the 12th Workshop on Building
Educational Applications Using NLP. Copenhagen,
Denmark.

Shervin Malmasi, Sze-Meng Jojo Wong, and Mark
Dras. 2013. Nli shared task 2013: Mq submission.
In Proceedings of the Eighth Workshop on Innova-
tive Use of NLP for Building Educational Applica-
tions. Atlanta, Georgia, pages 124–133.

Shervin Malmasi and Marcos Zampieri. 2016. Arabic
Dialect Identification in Speech Transcripts. In Pro-
ceedings of the Third Workshop on NLP for Similar
Languages, Varieties and Dialects (VarDial3). Os-
aka, Japan, pages 106–113.

Shervin Malmasi, Marcos Zampieri, Nikola Ljubešić,
Preslav Nakov, Ahmed Ali, and Jörg Tiedemann.
2016b. Discriminating between similar languages
and arabic dialect identification: A report on the
third dsl shared task. In Proceedings of the 3rd
Workshop on Language Technology for Closely Re-
lated Languages, Varieties and Dialects (VarDial).
Osaka, Japan.

Tomoya Mizumoto, Yuta Hayashibe, Keisuke Sak-
aguchi, Mamoru Komachi, and Yuji Matsumoto.
2013. Naist at the nli 2013 shared task. In Pro-
ceedings of the Eighth Workshop on Innovative Use
of NLP for Building Educational Applications. At-
lanta, Georgia, pages 134–139.

F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine Learn-
ing in Python. Journal of Machine Learning Re-
search 12:2825–2830.

Marius Popescu and Radu Tudor Ionescu. 2013. The
story of the characters, the dna and the native lan-
guage. In Proceedings of the Eighth Workshop on
Innovative Use of NLP for Building Educational Ap-
plications. Atlanta, Georgia, pages 270–278.

Björn Schuller, Stefan Steidl, Anton Batliner, Julia
Hirschberg, Judee K. Burgoon, Alice Baird, Aaron
Elkins, Yue Zhang, Eduardo Coutinho, and Keelan
Evanini. 2016. The INTERSPEECH 2016 Compu-
tational Paralinguistics Challenge: Deception, Sin-
cerity & Native Language. In Proceedings of Inter-
speech. pages 2001–2005.

Joel Tetreault, Daniel Blanchard, and Aoife Cahill.
2013. A Report on the First Native Language Iden-
tification Shared Task. In Proceedings of the Eighth
Workshop on Building Educational Applications Us-
ing NLP. Atlanta, GA, USA.

Yulia Tsvetkov, Naama Twitto, Nathan Schneider,
Noam Ordan, Manaal Faruqui, Victor Chahuneau,
Shuly Wintner, and Chris Dyer. 2013. Identifying
the l1 of non-native writers: the cmu-haifa system.
In Proceedings of the Eighth Workshop on Innova-
tive Use of NLP for Building Educational Applica-
tions. Atlanta, Georgia, pages 279–287.

Lan Wang, Masahiro Tanaka, and Hayato Yamana.
2016. What is your Mother Tongue?: Improving
Chinese Native Language Identification by Cleaning
Noisy Data and Adopting BM25. In Proceedings of
the International Conference on Big Data Analysis
(ICBDA). IEEE, pages 1–6.

Ching-Yi Wu, Po-Hsiang Lai, Yang Liu, and Vin-
cent Ng. 2013. Simple yet powerful native lan-
guage identification on toefl11. In Proceedings of
the Eighth Workshop on Innovative Use of NLP for
Building Educational Applications. Atlanta, Geor-
gia, pages 152–156.

Yang Xiang, Xiaolong Wang, Wenying Han, and
Qinghua Hong. 2015. Chinese Grammatical Error
Diagnosis Using Ensemble Learning. In Proceed-
ings of the 2nd Workshop on Natural Language Pro-
cessing Techniques for Educational Applications.
pages 99–104.

Marcos Zampieri, Binyam Gebrekidan Gebre, Hernani
Costa, and Josef van Genabith. 2015a. Comparing
approaches to the identification of similar languages.
In Proceedings of the Joint Workshop on Language
Technology for Closely Related Languages, Vari-
eties and Dialects (LT4VarDial). Hissar, Bulgaria,
pages 66–72.

Marcos Zampieri, Shervin Malmasi, Nikola Ljubešić,
Preslav Nakov, Ahmed Ali, Jörg Tiedemann, Yves
Scherrer, and Noëmi Aepli. 2017. Findings of the
VarDial Evaluation Campaign 2017. In Proceedings
of the Fourth Workshop on NLP for Similar Lan-
guages, Varieties and Dialects (VarDial). Valencia,
Spain, pages 1–15.

Marcos Zampieri, Liling Tan, Nikola Ljubešić, and
Jörg Tiedemann. 2014. A Report on the DSL Shared
Task 2014. In Proceedings of the First Workshop on
Applying NLP Tools to Similar Languages, Varieties
and Dialects (VarDial). Dublin, Ireland, pages 58–
67.

Marcos Zampieri, Liling Tan, Nikola Ljubešić, Jörg
Tiedemann, and Preslav Nakov. 2015b. Overview
of the dsl shared task 2015. In Proceedings of
the Joint Workshop on Language Technology for
Closely Related Languages, Varieties and Dialects
(LT4VarDial). Hissar, Bulgaria, pages 1–9.

404


