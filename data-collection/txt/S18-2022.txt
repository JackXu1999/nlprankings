



















































Fine-grained Entity Typing through Increased Discourse Context and Adaptive Classification Thresholds


Proceedings of the 7th Joint Conference on Lexical and Computational Semantics (*SEM), pages 173–179
New Orleans, June 5-6, 2018. c©2018 Association for Computational Linguistics

Fine-grained Entity Typing through Increased Discourse Context
and Adaptive Classification Thresholds

Sheng Zhang
Johns Hopkins University
zsheng2@jhu.edu

Kevin Duh
Johns Hopkins University
kevinduh@cs.jhu.edu

Benjamin Van Durme
Johns Hopkins University
vandurme@cs.jhu.edu

Abstract
Fine-grained entity typing is the task of as-
signing fine-grained semantic types to entity
mentions. We propose a neural architecture
which learns a distributional semantic repre-
sentation that leverages a greater amount of se-
mantic context – both document and sentence
level information – than prior work. We find
that additional context improves performance,
with further improvements gained by utiliz-
ing adaptive classification thresholds. Experi-
ments show that our approach without reliance
on hand-crafted features achieves the state-of-
the-art results on three benchmark datasets.

1 Introduction

Named entity typing is the task of detecting the
type (e.g., person, location, or organization) of
a named entity in natural language text. Entity
type information has shown to be useful in natural
language tasks such as question answering (Lee
et al., 2006), knowledge-base population (Carl-
son et al., 2010; Mitchell et al., 2015), and co-
reference resolution (Recasens et al., 2013). Mo-
tivated by its application to downstream tasks, re-
cent work on entity typing has moved beyond stan-
dard coarse types towards finer-grained semantic
types with richer ontologies (Lee et al., 2006; Ling
and Weld, 2012; Yosef et al., 2012; Gillick et al.,
2014; Del Corro et al., 2015). Rather than as-
suming an entity can be uniquely categorized into
a single type, the task has been approached as a
multi-label classification problem: e.g., in “... be-
came a top seller ... Monopoly is played in 114
countries. ...” (Figure 1), “Monopoly” is consid-
ered both a game as well as a product.

The state-of-the-art approach (Shimaoka et al.,
2017) for fine-grained entity typing employs an
attentive neural architecture to learn representa-
tions of the entity mention as well as its con-
text. These representations are then combined

Entity
Encoder

Sentence-level 
Context Encoder

Document-level 
Context Encoder

sentence-level context
document-level context

Attention

Feature representation

Type embeddings
… Logistic Regression

Adaptive Thresholds

 [ /game, /product ]Output types:

Featurizer

“… became a top seller …                    is played in 114 countries. …”Monopoly

Figure 1: Neural architecture for predicting the types
of entity mention “Monopoly” in the text “... became a
top seller ... Monopoly is played in 114 countries. ...”.
Part of document-level context is omitted.

with hand-crafted features (e.g., lexical and syn-
tactic features), and fed into a linear classifier with
a fixed threshold. While this approach outper-
forms previous approaches which only use sparse
binary features (Ling and Weld, 2012; Gillick
et al., 2014) or distributed representations (Yo-
gatama et al., 2015), it has a few drawbacks: (1)
the representations of left and right contexts are
learnt independently, ignoring their mutual con-
nection; (2) the attention on context is computed
solely upon the context, considering no alignment
to the entity; (3) document-level contexts which
could be useful in classification are not exploited;
and (4) hand-crafted features heavily rely on sys-
tem or human annotations.

To overcome these drawbacks, we propose a
neural architecture (Figure 1) which learns more
context-aware representations by using a better at-
tention mechanism and taking advantage of se-
mantic discourse information available in both the
document as well as sentence-level contexts. Fur-

173



ther, we find that adaptive classification thresh-
olds leads to further improvements. Experiments
demonstrate that our approach, without any re-
liance on hand-crafted features, outperforms prior
work on three benchmark datasets.

2 Model

Fine-grained entity typing is considered a multi-
label classification problem: Each entity e in the
text x is assigned a set of types T ∗ drawn from the
fine-grained type set T . The goal of this task is to
predict, given entity e and its context x, the assign-
ment of types to the entity. This assignment can
be represented by a binary vector y ∈ {1, 0}|T |
where |T | is the size of T . yt = 1 iff the entity is
assigned type t ∈ T .

2.1 General Model

Given a type embedding vector wt and a featurizer
ϕ that takes entity e and its context x, we employ
the logistic regression (as shown in Figure 1) to
model the probability of e assigned t (i.e., yt = 1)

P (yt = 1) =
1

1 + exp (−wᵀt ϕ(e, x))
, (1)

and we seek to learn a type embedding matrix
W = [w1, . . . , w|T |] and a featurizer ϕ such that

T ∗ = argmax
T

∏

t∈T
P (yt = 1) ·

∏

t/∈T
P (yt = 0).

(2)
At inference, the predicted type set T̂ assigned

to entity e is carried out by

T̂ =
{
t ∈ T : P (yt = 1) ≥ rt

}
, (3)

with rt the threshold for predicting e has type t.

2.2 Featurizer

As shown in Figure 1, featurizer ϕ in our model
contains three encoders which encode entity e and
its context x into feature vectors, and we consider
both sentence-level context xs and document-level
context xd in contrast to prior work which only
takes sentence-level context (Gillick et al., 2014;
Shimaoka et al., 2017). 1

1Document-level context has also been exploited in
Yaghoobzadeh and Schütze (2015); Yang et al. (2016); Karn
et al. (2017); Gupta et al. (2017).

The output of featurizer ϕ is the concatenation
of these feature vectors:

ϕ(e, x) =




f(e)

gs(xs, e)

gd(xd)


 . (4)

We define the computation of these feature vectors
in the followings.
Entity Encoder: The entity encoder f computes
the average of all the embeddings of tokens in en-
tity e.
Sentence-level Context Encoder: The encoder gs
for sentence-level context xs employs a single bi-
directional RNN to encode xs. Formally, let the
tokens in xs be x1s, . . . , x

n
s . The hidden state hi

for token xis is a concatenation of a left-to-right
hidden state

−→
hi and a right-to-left hidden state

←−
hi ,

hi =

[−→
h i
←−
h i

]
=

[−→
f (xis,

−→
h i−1)

←−
f (xis,

←−
h i+1)

]
, (5)

where
−→
f and

←−
f are L-layer stacked LSTMs

units (Hochreiter and Schmidhuber, 1997). This
is different from Shimaoka et al. (2017) who use
two separate bi-directional RNNs for context on
each side of the entity mention.
Attention: The feature representation for xs is a
weighted sum of the hidden states: gs(xs, e) =∑n

i=1 aihi, where ai is the attention to hidden
state hi. We employ the dot-product attention (Lu-
ong et al., 2015). It computes attention based on
the alignment between the entity and its context:

ai =
exp (hᵀiWaf(e))∑n
j=1 exp (h

ᵀ
jWaf(e))

, (6)

where Wa is the weight matrix. The dot-product
attention differs from the self attention (Shimaoka
et al., 2017) which only considers the context.
Document-level Context Encoder: The encoder
gd for document-level context xd is a multi-layer
perceptron:

gd(xd) = relu(Wd1 tanh(Wd2DM(xd))), (7)

where DM is a pretrained distributed memory
model (Le and Mikolov, 2014) which converts the
document-level context into a distributed repre-
sentation. Wd1 and Wd2 are weight matrices.

174



2.3 Adaptive Thresholds

In prior work, a fixed threshold (rt = 0.5) is
used for classification of all types (Ling and Weld,
2012; Shimaoka et al., 2017). We instead assign
a different threshold to each type that is optimized
to maximize the overall strict F1 on the dev set.
We show the definition of strict F1 in Section 3.1.

3 Experiments

We conduct experiments on three publicly avail-
able datasets.2 Table 1 shows the statistics of these
datasets.
OntoNotes: Gillick et al. (2014) sampled sen-
tences from OntoNotes (Weischedel et al., 2011)
and annotated entities in these sentences using 89
types. We use the same train/dev/test splits in Shi-
maoka et al. (2017). Document-level contexts are
retrieved from the original OntoNotes corpus.
BBN: Weischedel and Brunstein (2005) annotated
entities in Wall Street Journal using 93 types. We
use the train/test splits in Ren et al. (2016b) and
randomly hold out 2,000 pairs for dev. Document
contexts are retrieved from the original corpus.
FIGER: Ling and Weld (2012) sampled sentences
from 780k Wikipedia articles and 434 news re-
ports to form the train and test data respectively,
and annotated entities using 113 types. The splits
we use are the same in Shimaoka et al. (2017).

Train Dev Test Types

OntoNotes 251,039 2,202 8,963 89
BBN 84,078 2,000 13,766 93
FIGER 2,000,000 10,000 563 113

Table 1: Statistics of the datasets.

3.1 Metrics

We adopt the metrics used in Ling and Weld
(2012) where results are evaluated via strict, loose
macro, loose micro F1 scores. For the i-th in-
stance, let the predicted type set be T̂i, and the
reference type set Ti. The precision (P ) and re-
call (R) for each metric are computed as follow.
Strict:

P = R =
1

N

N∑

i=1

δ(T̂i = Ti)

2 We made the source code and data publicly available at
https://github.com/sheng-z/figet.

Loose Macro:

P =
1

N

N∑

i=1

|T̂i ∩ Ti|
|T̂i|

R =
1

N

N∑

i=1

|T̂i ∩ Ti|
|Ti|

Loose Micro:

P =

∑N
i=1 |T̂i ∩ Ti|∑N

i=1 |T̂i|

R =

∑N
i=1 |T̂i ∩ Ti|∑N

i=1 |Ti|

3.2 Hyperparameters
We use open-source GloVe vectors (Pennington
et al., 2014) trained on Common Crawl 840B
with 300 dimensions to initialize word embed-
dings used in all encoders. All weight parame-
ters are sampled from U(−0.01, 0.01). The en-
coder for sentence-level context is a 2-layer bi-
directional RNN with 200 hidden units. The DM
output size is 50. Sizes of Wa, Wd1 and Wd2 are
200×300, 70×50, and 50×70 respectively. Adam
optimizer (Kingma and Ba, 2014) and mini-batch
gradient is used for optimization. Batch size is
200. Dropout (rate=0.5) is applied to three feature
functions. To avoid overfitting, we choose models
which yield the best strict F1 on dev sets.

3.3 Results
We compare experimental results of our approach
with previous approaches3, and study contribu-
tion of our base model architecture, document-
level contexts and adaptive thresholds via ablation.
To ensure our findings are reliable, we run each
experiment twice and report the average perfor-
mance.

Overall, our approach significantly increases
the state-of-the-art macro F1 on both OntoNotes
and BBN datasets.

On OntoNotes (Table 3), our approach im-
proves the state of the art across all three met-
rics. Note that (1) without adaptive thresholds or
document-level contexts, our approach still out-
performs other approaches on macro F1 and micro
F1; (2) adding hand-crafted features (Shimaoka
et al., 2017) does not improve the performance.

3For PLE (Ren et al., 2016b), we were unable to replicate
the performance benefits reported in their work, so we report
the results after running their codebase.

175



ID Sentence Gold Prediction

A
... Canada’s declining crude output, combined with ... will
help intensify U.S. reliance on oil from overseas. ...

/other
/other
/other/health
/other/health/treatment

B
Bozell joins Backer Spielvogel Bates and Ogilvy Group as

U.S. agencies with interests in Korean agencies.
/organization
/organization/company

/organization
/organization/company

Table 2: Examples showing the improvement brought by document-level contexts and dot-product attention.
Entities are shown in the green box. The gray boxes visualize attention weights (darkness) on context tokens.

Approach Strict Macro Micro

BINARY(Gillick et al., 2014) N/A N/A 70.01
KWSABIE(Yogatama et al., 2015) N/A N/A 72.98

PLE(Ren et al., 2016b) 51.61 67.39 62.38
Ma et al. (2016) 49.30 68.23 61.27
AFET(Ren et al., 2016a) 55.10 71.10 64.70
FNET(Abhishek et al., 2017) 52.20 68.50 63.30
NEURAL(Shimaoka et al., 2017) 51.74 70.98 64.91

w/o Hand-crafted features 47.15 65.53 58.25

OUR APPROACH 55.52 73.33 67.61
w/o Adaptive thresholds 53.49 73.11 66.78
w/o Document-level contexts 53.17 72.14 66.51
w/ Hand-crafted features 54.40 73.13 66.89

Table 3: Results on the OntoNotes dataset.

This indicates the benefits of our proposed model
architecture for learning fine-grained entity typ-
ing, which is discussed in detail in Section 3.4;
and (3) BINARY and KWASIBIE were trained on
a different dataset, so their results are not directly
comparable.

Approach Strict Macro Micro

PLE(Ren et al., 2016b) 49.44 68.75 64.54
Ma et al. (2016) 70.43 75.78 76.50
AFET(Ren et al., 2016a) 67.00 72.70 73.50
FNET(Abhishek et al., 2017) 60.40 74.10 75.70

OUR APPROACH 60.87 77.75 76.94
w/o Adaptive thresholds 58.47 75.84 75.03
w/o Document-level contexts 58.12 75.65 75.11

Table 4: Results on the BBN dataset.

On BBN (Table 4), while Ma et al. (2016)’s la-
bel embedding algorithm holds the best strict F1,
our approach notably improves both macro F1 and
micro F1.4 The performance drops to a competi-
tive level with other approaches if adaptive thresh-
olds or document-level contexts are removed.

On FIGER (Table 5) where no document-level
context is currently available, our proposed ap-

4 Integrating label embedding into our proposed approach
is an avenue for future work.

Approach Strict Macro Micro

KWSABIE(Yogatama et al., 2015) N/A N/A 72.25
Attentive(Shimaoka et al., 2016) 58.97 77.96 74.94
FNET(Abhishek et al., 2017) 65.80 81.20 77.40

Ling and Weld (2012) 52.30 69.90 69.30
PLE(Ren et al., 2016b) 49.44 68.75 64.54
Ma et al. (2016) 53.54 68.06 66.53
AFET(Ren et al., 2016a) 53.30 69.30 66.40
NEURAL(Shimaoka et al., 2017) 59.68 78.97 75.36

w/o Hand-crafted features 54.53 74.76 71.58

OUR APPROACH 60.23 78.67 75.52
w/o Adaptive thresholds 60.05 78.50 75.39
w/ Hand-crafted features 60.11 78.54 75.33

Table 5: Results on the FIGER dataset.

proach still achieves the state-of-the-art strict and
micro F1. If compared with the ablation vari-
ant of the NEURAL approach, i.e., w/o hand-crafted
features, our approach gains significant improve-
ment. We notice that removing adaptive thresh-
olds only causes a small performance drop; this
is likely because the train and test splits of FIGER
are from different sources, and adaptive thresholds
are not generalized well enough to the test data.
KWASIBIE, Attentive and FNET were trained on a
different dataset, so their results are not directly
comparable.

3.4 Analysis

Table 2 shows examples illustrating the benefits
brought by our proposed approach. Example A il-
lustrates that sentence-level context sometimes is
not informative enough, and attention, though al-
ready placed on the head verbs, can be misleading.
Including document-level context (i.e., “Canada’s
declining crude output” in this case) helps pre-
clude wrong predictions (i.e., /other/health and
/other/health/treatment). Example B shows that
the semantic patterns learnt by our attention mech-
anism help make the correct prediction. As we
observe in Table 3 and Table 5, adding hand-
crafted features to our approach does not im-

176



prove the results. One possible explanation is that
hand-crafted features are mostly about syntactic-
head or topic information, and such information
are already covered by our attention mechanism
and document-level contexts as shown in Table 2.
Compared to hand-crafted features that heavily
rely on system or human annotations, attention
mechanism requires significantly less supervision,
and document-level or paragraph-level contexts
are much easier to get.

Through experiments, we observe no improve-
ment by encoding type hierarchical informa-
tion (Shimaoka et al., 2017).5 To explain this, we
compute cosine similarity between each pair of
fine-grained types based on the type embeddings
learned by our model, i.e., wt in Eq. (1). Table 6
shows several types and their closest types: these
types do not always share coarse-grained types
with their closest types, but they often co-occur
in the same context.

Type Closest Types

/other/event/accident /location/transit/railway/location/transit/bridge

/person/artist/music /organization/music/person/artist/director

/other/product/mobile phone /location/transit/railway/other/product/computer

/other/event/sports event /location/transit/railway/other/event

/other/product/car /organization/transit/other/product

Table 6: Type similarity.

4 Conclusion

We propose a new approach for fine-grained en-
tity typing. The contributions are: (1) we pro-
pose a neural architecture which learns a distribu-
tional semantic representation that leverage both
document and sentence level information, (2) we
find that context increased with document-level in-
formation improves performance, and (3) we uti-
lize adaptive classification thresholds to further
boost the performance. Experiments show our
approach achieves new state-of-the-art results on
three benchmarks.

5The type embedding matrix W for the logistic regres-
sion is replaced by the product of a learnt weight matrix V
and the constant sparse binary matrix S which encodes type
hierarchical information.

Acknowledgments

This work was supported in part by the JHU Hu-
man Language Technology Center of Excellence
(HLTCOE), and DARPA LORELEI. The U.S.
Government is authorized to reproduce and dis-
tribute reprints for Governmental purposes. The
views and conclusions contained in this publica-
tion are those of the authors and should not be
interpreted as representing official policies or en-
dorsements of DARPA or the U.S. Government.

References
Abhishek Abhishek, Ashish Anand, and Amit Awekar.

2017. Fine-grained entity type classification by
jointly learning representations and label embed-
dings. In Proceedings of the 15th Conference of the
European Chapter of the Association for Computa-
tional Linguistics: Volume 1, Long Papers, pages
797–807, Valencia, Spain. Association for Compu-
tational Linguistics.

Andrew Carlson, Justin Betteridge, Richard C Wang,
Estevam R Hruschka Jr, and Tom M Mitchell. 2010.
Coupled semi-supervised learning for information
extraction. In Proceedings of the third ACM inter-
national conference on Web search and data mining,
pages 101–110. ACM.

Luciano Del Corro, Abdalghani Abujabal, Rainer
Gemulla, and Gerhard Weikum. 2015. Finet:
Context-aware fine-grained named entity typing. In
Proceedings of the 2015 Conference on Empirical
Methods in Natural Language Processing, pages
868–878, Lisbon, Portugal. Association for Compu-
tational Linguistics.

Dan Gillick, Nevena Lazic, Kuzman Ganchev, Jesse
Kirchner, and David Huynh. 2014. Context-
dependent fine-grained entity type tagging. arXiv
preprint arXiv:1412.1820.

Nitish Gupta, Sameer Singh, and Dan Roth. 2017. En-
tity linking via joint encoding of types, descriptions,
and context. In Proceedings of the 2017 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 2681–2690, Copenhagen, Denmark.
Association for Computational Linguistics.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Sanjeev Karn, Ulli Waltinger, and Hinrich Schütze.
2017. End-to-end trainable attentive decoder for hi-
erarchical entity classification. In Proceedings of the
15th Conference of the European Chapter of the As-
sociation for Computational Linguistics: Volume 2,
Short Papers, pages 752–758, Valencia, Spain. As-
sociation for Computational Linguistics.

177



Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Quoc Le and Tomas Mikolov. 2014. Distributed repre-
sentations of sentences and documents. In Proceed-
ings of the 31st International Conference on Ma-
chine Learning (ICML-14), pages 1188–1196.

Changki Lee, Yi-Gyu Hwang, Hyo-Jung Oh, Soojong
Lim, Jeong Heo, Chung-Hee Lee, Hyeon-Jin Kim,
Ji-Hyun Wang, and Myung-Gil Jang. 2006. Fine-
Grained Named Entity Recognition Using Con-
ditional Random Fields for Question Answering.
Springer Berlin Heidelberg, Berlin, Heidelberg.

Xiao Ling and Daniel S. Weld. 2012. Fine-grained
entity recognition. In Proceedings of the Twenty-
Sixth AAAI Conference on Artificial Intelligence,
AAAI’12, pages 94–100. AAAI Press.

Minh-Thang Luong, Hieu Pham, and Christopher D.
Manning. 2015. Effective approaches to attention-
based neural machine translation. In Proceedings of
the 2015 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1412–1421, Lis-
bon, Portugal. Association for Computational Lin-
guistics.

Yukun Ma, Erik Cambria, and SA GAO. 2016. La-
bel embedding for zero-shot fine-grained named en-
tity typing. In Proceedings of COLING 2016, the
26th International Conference on Computational
Linguistics: Technical Papers, pages 171–180. The
COLING 2016 Organizing Committee.

Tom M. Mitchell, William W. Cohen, Estevam R. Hr-
uschka Jr., Partha Pratim Talukdar, Justin Bet-
teridge, Andrew Carlson, Bhavana Dalvi Mishra,
Matthew Gardner, Bryan Kisiel, Jayant Krishna-
murthy, Ni Lao, Kathryn Mazaitis, Thahir Mo-
hamed, Ndapandula Nakashole, Emmanouil Anto-
nios Platanios, Alan Ritter, Mehdi Samadi, Burr Set-
tles, Richard C. Wang, Derry Tanti Wijaya, Abhi-
nav Gupta, Xinlei Chen, Abulhair Saparov, Malcolm
Greaves, and Joel Welling. 2015. Never-ending
learning. In Proceedings of the Twenty-Ninth AAAI
Conference on Artificial Intelligence, January 25-
30, 2015, Austin, Texas, USA., pages 2302–2310.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. In Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1532–
1543.

Marta Recasens, Marie-Catherine de Marneffe, and
Christopher Potts. 2013. The life and death of dis-
course entities: Identifying singleton mentions. In
Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 627–633, Atlanta, Georgia. Association for
Computational Linguistics.

Xiang Ren, Wenqi He, Meng Qu, Lifu Huang, Heng
Ji, and Jiawei Han. 2016a. Afet: Automatic fine-
grained entity typing by hierarchical partial-label
embedding. In Proceedings of the 2016 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 1369–1378, Austin, Texas. Associ-
ation for Computational Linguistics.

Xiang Ren, Wenqi He, Meng Qu, Clare R. Voss, Heng
Ji, and Jiawei Han. 2016b. Label noise reduction in
entity typing by heterogeneous partial-label embed-
ding. In Proceedings of the 22Nd ACM SIGKDD
International Conference on Knowledge Discovery
and Data Mining, KDD ’16, pages 1825–1834, New
York, NY, USA. ACM.

Sonse Shimaoka, Pontus Stenetorp, Kentaro Inui, and
Sebastian Riedel. 2016. An attentive neural ar-
chitecture for fine-grained entity type classification.
In Proceedings of the 5th Workshop on Automated
Knowledge Base Construction, pages 69–74, San
Diego, CA. Association for Computational Linguis-
tics.

Sonse Shimaoka, Pontus Stenetorp, Kentaro Inui, and
Sebastian Riedel. 2017. Neural architectures for
fine-grained entity type classification. In Proceed-
ings of the 15th Conference of the European Chap-
ter of the Association for Computational Linguistics:
Volume 1, Long Papers, pages 1271–1280, Valencia,
Spain. Association for Computational Linguistics.

Ralph Weischedel and Ada Brunstein. 2005. Bbn pro-
noun coreference and entity type corpus. Linguistic
Data Consortium, Philadelphia, 112.

Ralph Weischedel, Eduard Hovy, Mitchell Mar-
cus, Martha Palmer, Robert Belvin, Sameer Prad-
han, Lance Ramshaw, and Nianwen Xue. 2011.
Ontonotes: A large training corpus for enhanced
processing. Handbook of Natural Language Pro-
cessing and Machine Translation. Springer.

Yadollah Yaghoobzadeh and Hinrich Schütze. 2015.
Corpus-level fine-grained entity typing using con-
textual information. In Proceedings of the 2015
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 715–725, Lisbon, Portugal.
Association for Computational Linguistics.

Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He,
Alex Smola, and Eduard Hovy. 2016. Hierarchi-
cal attention networks for document classification.
In Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 1480–1489, San Diego, California. Associa-
tion for Computational Linguistics.

Dani Yogatama, Daniel Gillick, and Nevena Lazic.
2015. Embedding methods for fine grained entity
type classification. In Proceedings of the 53rd An-
nual Meeting of the Association for Computational
Linguistics and the 7th International Joint Confer-
ence on Natural Language Processing (Volume 2:

178



Short Papers), pages 291–296, Beijing, China. As-
sociation for Computational Linguistics.

Mohamed Amir Yosef, Sandro Bauer, Johannes Hof-
fart, Marc Spaniol, and Gerhard Weikum. 2012.
HYENA: Hierarchical type classification for entity
names. In Proceedings of COLING 2012: Posters,
pages 1361–1370, Mumbai, India. The COLING
2012 Organizing Committee.

179


