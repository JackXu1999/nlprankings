



















































Fast Query Expansion on an Accounting Corpus using Sub-Word Embeddings


Proceedings of the Second Workshop on Subword/Character LEvel Models, pages 61–65
New Orleans, Louisiana, June 6, 2018. c©2018 Association for Computational Linguistics

Fast Query Expansion for an Accounting Corpus using Sub-word
Embeddings

Hrishikesh V. Ganu
Intuit India Development Center

Bangalore, India
hrishikeshvganu@gmail.com

Viswa Datha P.
mail@vishwa.be

Abstract

We present early results from a system un-
der development which uses sub-word em-
beddings for query expansion in the pres-
ence of mis-spelled words and other aberra-
tions. We work for a company which cre-
ates accounting software and the end goal is
to improve customer experience when they
search for help on our “Customer Care” por-
tal. Our customers use colloquial language,
non-standard acronyms and sometimes mis-
spell words when they use our Search por-
tal or interact over other channels. How-
ever, our Knowledge Base has curated con-
tent which leverages technical terms and is
in language which is quite formal. This re-
sults in the answer not being retrieved even
though the answer might actually be present in
the documentation (as assessed by a human).
We address this problem by creating equiva-
lence classes of words with similar meanings
(with the additional property that the map-
pings to these equivalence classes are robust
to mis-spellings) using sub-word embeddings
and then use them to fine tune a Search index
to improve recall.

1 Introduction

Accounting and taxation is a complex domain-
especially for small businesses who might not
have the necessary accounting skills but yet need
to be compliant with regulations. Hence con-
sumers of accounting software frequently seek ad-
vice both about accounting as well as about the
product itself.

During an audit process when we started to an-
alyze customers’ queries with the help of internal
experts, we realised that for a significant number
of queries the answers were already available but
not retrieved by the search engine because it relies
only on keyword based search.

This is primarily due to the following reasons:

1. Self employed and small business owners use
colloquial language and terms like I didn’t re-
ceive the money customer XYZ owes to me in-
stead of I didn’t receive my receivables from
XYX

2. Even when customers use accounting terms
there are misspellings or structural variants
(Form1040 vs Form1040-ES)

1.1 Previous Work

While there have been earlier approaches which
deal with these problems through methods like
lemmatization, fuzzy matching etc. details of
which are given in (Gormley and Tong, 2015), we
wanted a method that could be fine-tuned to our
dataset without creating hand-crafted rules. Re-
cent progress in creating distributional representa-
tions of words has found applications in Informa-
tion Retrieval (IR) due to work by Zamani and
Croft (2017), Korpusik et al. (2017) and Cao and
Lu (2017) among others. Some of these models
provide the query as an input to a neural network at
prediction time while others rely on using the em-
beddings for query expansion. Our work is in-line
with some of these earlier efforts but with a goal
of creating an end-end working system that is ro-
bust to mis-spellings and other aberrations around
the composition of words. Thus in this paper we
focus on how we integrated sub-word embeddings
with Search systems to create a real-life retrieval
system which is currently under development. We
demonstrate how this can solve a very practical
problem around Information Retrieval from ac-
counting/taxation related corpora.

2 Approach

Our basic approach is to create a search index
to enable searching for either the exact word

61



or it’s synonyms if they are present in the cor-
pus/vocabulary. In addition our approach is able to
map mis-spelled (and hence OOV) words to their
“correct” versions on the fly at query time. It in-
volves the following steps:

1. Our expert Customer Care Agents (CCA)
create a hand-curated list of “important”
words including named entities, actions that
customers perform with our products etc.

2. We generate word and sub-word embeddings
from our own data and then use these embed-
dings to create nearest neighbors for the im-
portant words. The procedure is similar to
that used by Bojanowski et al. (2016) so we
don’t reproduce it here. Hyperparameter set-
tings that were varied by us are mentioned in
Section 3.3.

3. We use the “synonym contraction” mech-
anism of Elasticsearch to update the in-
dex with “synonyms”. Since this is a
standard procedure we refer the reader
to Gormley and Tong (2015) for details.
During search, the query is also “ana-
lyzed” using the same “analyzer”. This
can be seen as a kind of query re-
formulation where word is replaced by
OR(word, synonym1, synonym2...). Note
that for Out of Vocabulary (OOV) words at
query time we get “synonyms” by relying on
the embeddings of the sub-words, following
the procedure in Bojanowski et al. (2016).

3 Experiments

In this section we discuss the overall objective of
the experiment and then mention details about the
dataset, pre-processing, training and hyperparam-
eter tuning.

3.1 Objective

As mentioned in Section 2, we wish to under-
stand if an Elasticsearch “Synonyms” file based
on sub-word embeddings can lead to a higher re-
call during search compared to a “Synonyms” file
based purely on word embeddings (without sub-
word embeddings) especially in the presence of
mis-spellings and other perturbations. See Fig-
ure 1 for a high level overview of how the “Syn-
onyms” files are created.

Algorithm 1 Offline: Create Nearest Neighbours
Require: Set: RootWords . Hand-crafted set of

the core/important words
Require: Hashmap H: RootWord→ ListofWords
Require: Set: V . Vocabulary
Require: Integer K . # neighbours to retrieve
Require:

function NN(rootWord,V, K)
return {w|w ∈ K NN(rootWord) ∩ V }

end function
function POPULATENN(H,Rootwords,V,K)

for rootWord ∈ RootWords do
. Insert word and it’s neighbours into H
H[rootWord]= NN(rootWord, V, K)

. Remove assigned words from V
V← V \ Set(NN(rootWord,V,K) )

end for
end function

Ensure: H . Hashmap with root words as keys
and K-Nearest Neighbours as values

3.2 Dataset and Pre-processing

We realised that the dataset for validating the ben-
efit from this system had to have the following
properties:

1. For each query we had to know the matching
answer. This was to be used as ground truth.

2. We wanted the answer to be relevant to the
matching query but did not want the words in
the query to be a subset of words in the an-
swer. This is because had there been good
word overlap there wouldn’t be a need for
query expansion.

After failing to find a well-researched corpus
around accounting we decided to use (IRS, 2017),
which is a manual written by the Internal Rev-
enue Service (IRS), USA to provide information
about personal income tax to taxpayers. While
we have also performed experiments on our pro-
prietary knowledge base, for this paper we chose
(IRS, 2017) because it is a widely used document
and is also available in the public domain. This
will enable the larger NLP community to verify
and expand upon our findings. Figure 2 shows
the layout of a typical page in books like (IRS,
2017). Our basic idea was to use perturbed ver-
sions of “headings” (see Section 3.2.1) as queries
(perturbed to mimic mis-spellings/typos from real

62



Figure 1: Starting with raw data from (IRS, 2017),
a skipgram model (see Section 3.3) was trained, fol-
lowed by identifying and indexing 10 nearest neighbors
per word. This set of nearest neighbors was then pro-
vided to Elasticsearch (ES) as a ”Synonyms” file which
was internally incorporated into the ES index.

users) and to use content in the “body” as docu-
ments. Since several downstream models depend
on tokenization, we wish to state that for all ex-
periments in this paper we tokenized words using
whitespace and using punctuation symbols.

3.2.1 Mis-spelling synthesizer
Our objective here is to compare sub-word em-
beddings with word embeddings and understand
how the robustness to small character level pertur-
bations affects the final recall after search. Since
“headings” in books are well formatted and don’t
have mis-spellings we had to either synthetically
generate mis-spelled words or collect statistics
around how users mis-spell words in real life. We
describe both approaches below.

1. Synthetic word-level perturbations: We
assume that a word w is a sequence of
k characters indexed as ci so that w =
{c1, c2, .., ci, .., ck} and a sentence S is a se-
quence of n words, indexed as wj . We se-
lect 10% of the queries uniformly at ran-
dom and create perturbed versions of them.
Then for each such query we synthesize
perturbed/mis-spelled versions of words by
choosing one word uniformly at random (say
wj) to perturb from each sentence and then
substitute a random selection of d0.20|wj |e
adjacent characters by random alphanumeric

characters different from the original.

2. Mis-spelling statistics learned from user
data: Ideally we wanted to generate per-
turbed versions of all queries from real-world
user data. However because we had limited
access to human agents 1, we were able to
get only word-level perturbations as against
full query level-perturbations from human
editors. We followed a process where we
first picked the most frequent 200 words
out of 903 distinct words forming the vo-
cabulary for queries . Let’s call the set of
these words as set T . For each such word
wi ∈ T we asked 5 human annotators to
type in these words at a targeted pace of 33
words per minute 2 to simulate the pace at
which users type in the real world. This
allowed us to collect the set of typed ver-
sions Si of each word wi (which includes
mis-spelled versions as well as the correct
word itself) and thus compute a distribu-
tion Di = Pr(x|correct word is wi) where
x ∈ Si for each wi. Finally, we used these
distributions to perturb queries by sampling
from Di for each word in the query that oc-
curred in the top 200. Note that this method
does not create true query-level perturbations
since whether a word is mis-spelled or not
might depend on factors like length of the
query, presence of other words etc. Here
we look at word mis-spellings when they are
typed out in isolation.

3.3 Training skipgram model

A skipgram model was trained on the entire cor-
pus from (IRS, 2017) (using original, unperturbed
headings). While in a conventional ML setting this
procedure (of using the entire dataset for training
without holding out a test sample) might be inap-
propriate, in our case the main objective is to un-
derstand the retrieval systems’ robustness to mis-
spellings and other character level perturbations.
In our setup, such perturbations are not present
anywhere in the corpus which is clean and free
of all mis-spellings. This means that training the

1We performed this experiment based on suggestions
from reviewers and hence had less than a week to gather and
process input from human editors

2The average pace for typing is 33 words per minute based
on some studies. See https://en.wikipedia.org/
wiki/Typing

63



Figure 2: “A: Page from a book” illustrates how a page
in a book consists of a heading and some body of text
beneath it. This raw corpus was used for training sub-
word skipgram models.
“B: Generating mis-spelled headings”” illustrates how
we generate misspelled versions of words in ”head-
ings” by passing them through a ”Mis-spelling synthe-
sizer” (See Section 3.2.1). Mis-spelled headings were
used only during search and not for training the skip-
gram model.

skipgram model on the entire corpus does not pre-
vent us from drawing valid conclusions about the
generalization ability of the system as far as ro-
bustness to mis-spelled words is concerned. We
largely followed the process employed by (Bo-
janowski et al., 2016) and also leveraged their
open-source code 3. Additionally, because we
wanted to retrieve nearest neighbors, we experi-
mented with different settings for the number of
neighbors retrieved and sub-word sizes. We relied
on internal domain experts to help us determine
the configuration with the best synonyms. Details
of the experiments and the best configuration cho-
sen are given in Table 1

4 Results

To achieve the objective mentioned in Section 3.1
and in-line with details in Section 3.2, we sent
the same set of 805 queries (in our case “head-
ings” perturbed through Section 3.2.1 are queries)
to two different Elasticsearch instances: 1) an ES
instance having a “Synonyms” file derived from
word-level embeddings, called ESbase and 2) an

3https://github.com/facebookresearch/
fastText

Sub-
word
size

#
neigh-
bours

Comments

3-6 10 Good quality neighbours.
This is the best configura-
tion

2-8 10 Quality worsened
3-6 20 Irrelevant synonyms
2-4 10 Some non-sensical words

in synonyms

Table 1: Hyperparameter tuning for generating em-
beddings

instance having a “Synonym” file derived from
sub-word embeddings, called ESsub−word (see
Figure 3).

4.1 Metrics
We use following notation and metrics:

• correctESbase : number of questions for
which ESbase returned correct results. This
is denoted as “# correct baseline” as per the
legend in Figure 3

• correctESsub−word : number of questions for
which ESsub−word returned correct results.
This is denoted as “# correct sub-word” as per
the legend in Figure 3

• Recall Ratio (x 100)= 100 correctESsub−wordcorrectESbase
In Figure 3 we report results based on queries gen-
erated using the “Synthetic word-level perturba-
tions” method in Section 3.2.1. The primary re-
sult from this dataset and from some of our experi-
ments on internal proprietary corpora indicate that
the Recall Ratio (x 100) metric varies from 110
to 114 depending on how many top results are re-
trieved and other parameters. This means that we
get between a 10% to 14% lift 4 in the number of
questions for which correct answers are retrieved
when using sub-word embeddings vs when using
word embeddings.

Another insight from Figure 3 is that the in-
crease in recall does not come for free. For K =

4Following a suggestion from an anonymous reviewer
we also collected statistics on word-level errors by human
users and used them to generate perturbed queries (See ”Mis-
spellings statistics learned from user data” in Section 3.2.1).
The lift was 8% with this more realistic dataset but we could
perturb only the top 200 words due to limited availability of
editors. Note that this dataset also has some synthetic ele-
ments.

64



10, ESbase gives an empty response 30 times
compared with 6 times for ESsub−word. Note that
in Figure 3 the total of #correct baseline and #null
baseline does not equal total number of queries
because for some queries the result set was not
empty but contained all “wrong” answers. Similar
connotation holds for the corresponding sub-word
versions.

Figure 3: Comparison of the Elasticsearch index us-
ing sub-word embeddings, called ESsub−word with
the one using word level embeddings, called ESbase.
X-axis indicates how many results (say K) from Elas-
ticsearch are considered. E.g: if K = 10, we consider
only the top 10 results. Y-axis denotes for how many of
the 805 queries fired, we received the correct answer in
the top K. Because queries are just the perturbed head-
ings, note that for this dataset we assume there’s only 1
correct answer per query-the one that appears beneath
the heading in (IRS, 2017).

5 Discussion

To test the hypothesis that sub-word embeddings
are more robust to spelling error and other pertur-
bations we conducted an experiment with search
retrieval as the end goal.

• Through a comparison of sub-word and word
level embeddings on a dataset (IRS, 2017)
in the public domain, we have demonstrated
that using sub-word embeddings for creating
the “Synonyms” file for Elasticsearch indeed
leads to better recall.

• Since we work in industry we also wish to
emphasize that our solution has several fea-
tures which make it attractive from a prac-
tical perspective. In particular, because our
solution integrates well with IR systems like
ES or Solr, we have very low latency ( 50ms)

compared to query expansion systems utiliz-
ing ML model predictions at query time.

• One of the limitations of this study is that
while we have demonstrated a system which
increases recall through query expansion, we
need to complement it with a layer which can
re-rank the retrieved results in a more mean-
ingful way. The default ranking module used
by ES does not work well for our use case.
Adding such a layer will allow us to measure
and track ranking based metrics (like DCG)
once the solution goes live and we start re-
ceiving user feedback.

Acknowledgments

We thank the two anonymous reviewers for their
suggestions to improve the paper in terms of both
content and style. Their suggestions, led us to con-
duct some quick experiments with human editors
on how people mis-type words and we have mod-
ified the paper to reflect these new results.

References
Piotr Bojanowski, Edouard Grave, Armand Joulin,

and Tomas Mikolov. 2016. Enriching word vec-
tors with subword information. arXiv preprint
arXiv:1607.04606.

Shaosheng Cao and Wei Lu. 2017. Improving word
embeddings with convolutional feature learning and
subword information. In AAAI, pages 3144–3151.

Clinton Gormley and Zachary Tong. 2015. Elastic-
search: The Definitive Guide, 1st edition. O’Reilly
Media, Inc.

IRS. 2017. Your Federal Income Tax. Department of
Treasury, Internal Revenue Service, USA.

Mandy Korpusik, Zachary Collins, and James Glass.
2017. Character-based embedding models and
reranking strategies for understanding natural lan-
guage meal descriptions. Proc. Interspeech 2017,
pages 3320–3324.

Hamed Zamani and W. Bruce Croft. 2017. Relevance-
based word embedding. CoRR, abs/1705.03556.

A Supplemental Material

At the following url, we provide examples of the
nearest neighbors generated offline for the syn-
onyms file as well as neighbors generated for
OOV words at query time. https://goo.gl/
3oQ2xF

65


