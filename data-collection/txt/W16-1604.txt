



















































On the Compositionality and Semantic Interpretation of English Noun Compounds


Proceedings of the 1st Workshop on Representation Learning for NLP, pages 27–39,
Berlin, Germany, August 11th, 2016. c©2016 Association for Computational Linguistics

On the Compositionality and Semantic Interpretation of
English Noun Compounds

Corina Dima
Collaborative Research Center 833
University of Tübingen, Germany

corina.dima@uni-tuebingen.de

Abstract

In this paper we present a study cover-
ing the creation of compositional distri-
butional representations for English noun
compounds (e.g. computer science) using
two compositional models proposed in the
literature. The compositional representa-
tions are first evaluated based on their sim-
ilarity to the corresponding corpus-learned
representations and then on the task of au-
tomatic classification of semantic relations
for English noun compounds. Our experi-
ments show that compositional models are
able to build meaningful representations
for more than half of the test set com-
pounds. However, using pre-trained com-
positional models does not lead to the ex-
pected performance gains for the semantic
relation classification task. Models using
compositional representations have a sim-
ilar performance as a basic classification
model, despite the advantage of being pre-
trained on a large set of compounds.

1 Introduction

Creating word representations for multiword ex-
pressions is a challenging NLP task. The chal-
lenge comes from the fact that these constructions
have “idiosyncratic interpretations that cross word
boundaries (or spaces)” (Sag et al., 2002). A good
example of such challenging multiword expres-
sions are noun compounds (e.g. finger nail, health
care), where the meaning of a compound often in-
volves combining some aspect or aspects of the
meanings of its constituents.

Over the last few years distributed word repre-
sentations (Collobert et al., 2011b; Mikolov et al.,
2013; Pennington et al., 2014) have proven very
successful at representing single-token words, and
there have been several attempts at creating com-
positional distributional models of meaning for

multi-token expressions, in particular adjective-
word phrases (Baroni and Zamparelli, 2010), de-
terminer phrases (Dinu et al., 2013b) or verb
phrases (Yin and Schütze, 2014).

Studying the semantics of multiword units, and
in particular the semantic interpretation of noun
compounds has been an active area of research
in both theoretical and computational linguistics.
Here, one train of research has focused on under-
standing the mechanism of compounding by pro-
viding a label for the relation between the con-
stituents (e.g. in finger nail, the nail is PART OF the
finger) as in (Ó Séaghdha, 2008; Tratz and Hovy,
2010) or by identifying the preposition in the pre-
ferred paraphrase of the compound (e.g. nail of
the finger) as in (Lauer, 1995).

In this paper, we explore compositional distri-
butional models for English noun compounds, and
analyze the impact of such models on the task of
predicting the compound-internal semantic rela-
tion given a labeled dataset of compounds. At the
same time, we analyze the results of the compo-
sitional process through the lens of the semantic
relation annotation, in an attempt to uncover com-
pounding patterns that are particularly challenging
for the compositional distributional models.

2 Context and Compound Interpretation

There are two possible settings for compound
interpretation: out-of-context interpretation and
context-dependent interpretation.

Bauer (1983, pp. 45) describes a continuum of
types of complex words, arranged with respect
to their formation status and to how dependent
their interpretation is on the context: (i)“nonce for-
mations, coined by a speaker/writer on the spur
of the moment to cover some immediate need”,
where there is a large ambiguity with respect
to the meaning of the compound which cannot
be resolved without the immediate context (e.g.
Nakov’s (2013) example compound plate length,

27



for which a possible interpretation in a given con-
text could be what your hair is when it drags in
your food); (ii) institutionalized lexemes, whose
potential ambiguity is canceled by the frequency
of use and familiarity with the term, and whose
more established meaning could be inferred based
on the meanings of the constituents and prior
world experience, without the need for an imme-
diate context (e.g. orange juice); (iii) lexicalized
lexemes, where the meaning has become a con-
vention which cannot be inferred from the con-
stituents alone and can only be successfully inter-
preted if the term is familiar or if the context pro-
vides enough clues (e.g. couch potato1).

The available datasets we use (described in Sec-
tion 3) are very likely to contain some very low
frequency items of type (i), whose actual inter-
pretation would necessitate taking the immediate
context into account, as well some highly lexical-
ized compounds of type (iii), where the meaning
can only be deduced from context. Nevertheless,
because of a lack of annotated resources that pro-
vide the semantic interpretation of a compound to-
gether with its context, we will focus on the out-
of-context interpretation of compounds.

3 Datasets

3.1 English Compound Dataset for
Compositionality

The English compound dataset used for the com-
position tests was constructed from two existing
compound datasets and a selection of the nom-
inal compounds in the WordNet database. The
first existing compound dataset was described in
(Tratz, 2011) and contains 19158 compounds2.
The second existing compound dataset was pro-
posed in (Ó Séaghdha, 2008) and contains 1443
compounds3.

Additional compounds were collected from the
WordNet 3.1 database files 4, more specifically
from the noun database file data.noun. The
WordNet compound collection process involved
3 steps: (i) collecting all candidate compounds,

1a couch potato is not a potato, but a person who exercises
little and spends most of the time in front of a TV.

2The dataset is part of the semantically-enriched
parser described in (Tratz, 2011) which can be obtained
from http://www.isi.edu/publications/licensed-sw/
fanseparser/

3Available at http://www.cl.cam.ac.uk/˜do242/
Resources/1443_Compounds.tar.gz

4Available at http://wordnetcode.princeton.edu/
wn3.1.dict.tar.gz

i.e. words that contained an underscore or a dash
(e.g. abstract entity, self-service); (ii) filtering out
candidates that included numbers or dots, or had
more than 2 constituents; (iii) filtering out candi-
dates where either one of the constituents had a
part-of-speech tag that was different from noun
or verb. The part-of-speech tagging of the can-
didate compounds was performed using the spaCy
Python library for advanced natural language pro-
cessing5. The reason for allowing both noun
and verb as accepted part-of-speech tags was
that given the extremely limited context available
when PoS-tagging a compound the tagger would
frequently label as verb multi-sense words that
were actually nouns in the given context (e.g. eye
drop, where drop was tagged as a verb). The final
compound list extracted from WordNet 3.1 con-
tained 18775 compounds.

The compounds collected from all three re-
sources were combined into one list. The list
was deduplicated and filtered for capitalized com-
pounds (the Tratz (2011) dataset contained a small
amount of person names and titles). A final fil-
tering step removed all the compounds where ei-
ther of the two constituents or the compound itself
did not have a minimum frequency of 100 in the
support corpus (presented later, in Section 4.1).
The frequency filtering step was motivated by the
assumption that the compositional process can be
better modeled using “well-learned” word vectors
that are based on a minimum number of contexts.

The final dataset contains 27220 compounds,
formed through the combination of 5335 modifiers
and 4761 heads. The set of unique modifiers and
heads contains 7646 words, with 2450 words ap-
pearing both as modifiers and as heads. The dictio-
nary for the final dataset contains therefore 34866
unique words. The dataset was partitioned into
train, test and dev splits containing 19054,
5444 and 2722 compounds respectively.

3.2 English Compound Datasets for
Semantic Interpretation

The Tratz (2011) dataset and the Ó Séaghdha
(2008) dataset are both annotated with seman-
tic relations between the compound constituents.
The Tratz (2011) dataset has 37 semantic relations
and 19158 compounds. The Ó Séaghdha (2008)
dataset has 1443 compounds annotated with 6
coarse relation labels (ABOUT, ACTOR, BE, HAVE,

5https://spacy.io/

28



IN, INST). Appendix A lists the relations in the
two datasets together with some example anno-
tated compounds.

For both datasets a small fraction of the
constituents had to be recoded to the artificial
underscore-based form described in Section 4.1, in
order to maximize the coverage of the word repre-
sentations for the constituents (e.g. database was
recoded to data base).

4 Composition Models for English
Nominal Compounds

A common view of natural language regards it as
being inherently compositional. Words are com-
bined to obtain phrases, which in turn combine
to create sentences. The composition continues to
the paragraph, section and document levels. It is
this defining trait of human language, its compo-
sitionality, that allows us to produce and to under-
stand the potentially infinite number of utterances
in a human language.

Gottlob Frege (1848-1925) is credited with
phrasing this intuition into the form of a principle,
known as the Principle of Compositionality: “The
meaning of the whole is a function of the mean-
ing of the parts and their mode of combination”
(Dowty et al., 1981, p.8).

The adoption of distributional vectors as a
proxy for the meaning of individual words (in
other words, having a “meaning of the parts”) en-
couraged researchers to focus their attention on
finding composition models which could act as the
“mode of combination”.

When applied to vector space models of lan-
guage, the idea of looking for a “mode of
combination” translates to finding a composi-
tion function f which takes as input some n-
dimensional distributional representations for the
two constituents constructed using a support cor-
pus, ucorpus, vcorpus ∈ Rn and outputs another
n-dimensional representation for the compound
pcomposed ∈ Rn,

pcomposed = f(ucorpus, vcorpus)

Additionally, we consider pcorpus ∈ Rn, the
learned representation for the compound, to be the
“gold standard” for the composed representation
of the compound pcomposed. Therefore the com-
position function f should minimize JMSE , the
mean squared error between the composed and the

corpus-induced representations:

JMSE =
nc∑
i=1

1
n

n∑
j=1

(pcomposedij − pcorpusij )2

where nc is the number of compounds in our
dataset.

Previous studies like (Guevara, 2010; Baroni
and Zamparelli, 2010) evaluate their proposed
composition functions on training data created us-
ing the following procedure: first, they gather a
set of word pairs to model. Then, a large corpus
is used to construct distributional representations
both for the word pairs as well as for the individ-
ual words in each pair. In order to derive word pair
representations the corpus is first pre-processed
such that all the occurrences of the word pairs of
interest are linked with the underscore character
‘ ’. This tricks the tokenizer into considering each
pair a singe-unit word, thus making it possible to
record its co-occurrence statistics using the same
distributional methods one would use for a gen-
uine single-unit word.

The same methodology is applied here for creat-
ing a training dataset for compositional models us-
ing the list of compounds described in Section 3.1.
The process is detailed in Section 4.1.

Next, we selected two composition functions
(we also refer to them as composition models)
from the ones presented in the literature:

• the full additive model, introduced in Zan-
zotto et al. (2010) (in their work this model
is called the estimated additive model) and
popularized as part of the DISSECT toolkit
(Dinu et al., 2013a; Dinu et al., 2013b). In
this model the two constituent vectors u and
v ∈ Rn are composed by multiplying them
via two square matrices A,B ∈ Rn×n. A
and B are the same for every u and v, so dur-
ing training we only have to estimate the pa-
rameters in two n × n matrices, making the
model constant in the number of parameters.
The mathematical formulation of the full ad-
ditive model is presented in Eq. 1.

p = Au + Bv (1)

• the matrix model, introduced in Socher et al.
(2011). It is a non-linear composition model
where the constituent vectors u, v ∈ Rn
are first concatenated, resulting in a vector

29



[u; v] ∈ R2n and then multiplied with a ma-
trix W ∈ R2n×n. The result of the mul-
tiplication is an n-dimensional vector which
is passed as a final step through a non-linear
function g (in this case the element-wise hy-
perbolic tangent tanh). The parameter ma-
trix W which has to be estimated during the
training process is the same for all the pos-
sible input vectors u and v. Since this com-
position function is implemented via a neural
network, a bias term b ∈ Rn is added after the
multiplication of the matrix W with the con-
catenated vector [u; v]. The complete form of
this composition function is given in Eq. 2.

p = g(W[u; v] + b) (2)

The preference for these particular composition
models is justified by their constant number of pa-
rameters with respect to the vocabulary size. This
allows us to use this composition model for a sig-
nificantly larger number of constituents than the
one in the list of compounds it was trained on. In
particular, this allows us to predict a composition
vector even for the compounds that were not at-
tested in the corpus, if their constituents are fre-
quent enough to be part of our full vocabulary.

Both models were reimplemented using the
Torch7 library (Collobert et al., 2011a), whose nn-
graph module allows for an easy creation of archi-
tectures with multiple inputs and outputs. Reim-
plementing the composition models is also justi-
fied by the use of trained composition models as
a form of pre-training for the semantic interpreta-
tion models described in Section 5.

4.1 Compound-aware Word Representations
The support corpus for creating English word rep-
resentations for compositionality experiments (re-
ferred to in Section 3.1) was obtained by concate-
nating the raw text from the ENCOW14AX corpus
(Schäfer, 2015) and the pre-processed 2014 En-
glish Wikipedia dump described and made avail-
able in Müller and Schütze (2015). A preprocess-
ing step similar to the one described in Müller
and Schütze (2015) was applied to the concate-
nated corpus: the text was lowercased and the dig-
its were replaced with 0s. An additional prepro-
cessing step was necessary for creating compound
representations. A list of compounds (described
in Section 3.1) was used to recode the initial cor-
pus such that the two-part compounds in the list

would be considered a single token. The recod-
ing process involved replacing different spelling
variants of a compound - written as two sepa-
rate words, contiguously or with a dash (as in
dress code, dresscode or dress-code), as well as
their respective plural forms (dress codes, dress-
codes, dress-codes) with an artificial underscore-
based form (e.g. dress code). We did not, how-
ever, modify the plural first constituents (i.e. sav-
ings account), nor did we normalize the spelling
variation which is the result of different spelling
standards as in color scheme (American English)
and colour scheme (British English). The result
was a 9 billion words raw-text corpus with a cor-
responding vocabulary containing 424,014 words
(both simplex words and compounds) with mini-
mum frequency 100 (the full vocabulary had 16M
words).

The raw-text corpus was the basis for training
300 dimensional word representations using the
GloVe package (Pennington et al., 2014). The
GloVe model was trained for 15 iterations using
a 10-word symmetric context (20 words in total)
for constructing the co-occurence matrix. The
vector spaces were normalized to the L2-norm,
first across features and then across samples using
scikit-learn (Pedregosa et al., 2011).

4.2 Evaluation and Results

The parameters of the two composition models
described in Section 4 were estimated with the
help of the list of compounds in the train set
described in Section 3.1 and the word represen-
tations presented in Section 4.1. We evaluated
the performance of the composition models on the
test split of the dataset, using the rank evalu-
ation proposed by Baroni and Zamparelli (2010).
Using a trained model, we generate composed rep-
resentations for all the compounds in the test
set. The composed representation of each com-
pound is ranked with respect to all the 34866
unique words in the dictionary (the set of all
compounds and their respective constituents) us-
ing the cosine similarity. The best possible re-
sult is when the corpus-learned representation is
the nearest neighbor of the composed representa-
tion, and corresponds to assigning the rank 1 to
the composed vector. Rank 2 is assigned when the
corpus-learned representation is the second near-
est neighbor, and so on. The cut-off rank 1000
is assigned to all the representations with a rank

30



VA
RI

ET
Y&

GE
NU

S_
OF

TO
PI
C_

OF
_C

OG
NI

TI
ON

&E
M
OT

IO
N

JU
ST

IF
IC

AT
IO

N

PA
RT

IA
L_

AT
TR

IB
UT

E_
TR

AN
SF

ER

W
HO

LE
+
AT

TR
IB

UT
E&

FE
AT

UR
E&

QU
AL

IT
Y_

VA
LU

E_
IS

_C
HA

RA
CT

ER
IS

TI
C_

OF

EX
PE

RI
EN

CE
R-

OF
-E

XP
ER

IE
NC

E

TI
M
E-

OF
2

TO
PI
C_

OF
_E

XP
ER

T

M
IT

IG
AT

E&
OP

PO
SE

AD
J-L

IK
E_

NO
UN

OB
TA

IN
&A

CC
ES

S&
SE

EK

LE
XI

CA
LI
ZE

D

AM
OU

NT
-O

F

US
ER

_R
EC

IP
IE

NT

CO
NT

AI
N

CR
EA

TO
R-

PR
OV

ID
ER

-C
AU

SE
_O

F

M
EA

NS

OR
GA

NI
ZE

&S
UP

ER
VI

SE
&A

UT
HO

RI
TY

W
HO

LE
+
PA

RT
_O

R_
M
EM

BE
R_

OF

PA
RT

&M
EM

BE
R_

OF
_C

OL
LE

CT
IO

N&
CO

NF
IG

&S
ER

IE
S

PU
RP

OS
E

TI
M
E-

OF
1

OW
NE

R-
US

ER

SU
BS

TA
NC

E-
M
AT

ER
IA

L-
IN

GR
ED

IE
NT

EM
PL

OY
ER

SU
BJ

EC
T

M
EA

SU
RE

CR
EA

TE
-P

RO
VI

DE
-G

EN
ER

AT
E-

SE
LL

LO
CA

TI
ON

OT
HE

R

RE
LA

TI
ON

AL
-N

OU
N-

CO
M
PL

EM
EN

T

EQ
UA

TI
VE

TO
PI
C

PE
RF

OR
M
&E

NG
AG

E_
IN

OB
JE

CT
IV

E

0

5

10

15

20

25

m
e
d
ia

n
 r

a
n
ks

0

500

1000

1500

2000

2500

3000

co
m

p
o
u
n
d
 c

o
u
n
t

Figure 1: Semantic relations in the Tratz (2011) dataset: number of compounds labeled with a relation
(green triangle) vs. the median rank assigned to their composed representations by the full additive model
(blue circle).

≥ 1000. The first, second and third quartiles (Q1,
Q2/median, Q3) are then computed for the sorted
list of ranks of the composed representations of the
test set compounds. The result of our evaluation
are displayed in Table 1.

Model Ranks dev Ranks test
Q1 Q2 Q3 Max Q1 Q2 Q3 Max

matrix 2 5 28 1K 1 5 25 1K
full additive 1 5 28 1K 1 5 25 1K

Table 1: Composition models results: quartiles
for the ranks assigned to the dev and test com-
posed representations (lower is better).

Both composition models obtain good results
on the test dataset with respect to the Q1, Q2,
Q3 quartiles. Ranks in the 1-5 range, which were
assigned to half of the test set compounds cor-
respond to a well-built compound representation
which resides in the expected vectorial neighbor-
hood. For the next quarter of the data, the rank in
the 6-25 range points to a representation that might
still be considered reasonable depending on the
application. For the last segment of ranked com-
pounds the constructed representations are most
likely incorrect. As detailed in the next paragraph,
such high ranks usually suggest a difficulty in cre-
ating a compound representation based on the con-
stituent representations and indicate that the com-
pound belongs to a special class (e.g. lexicalized,
multi-sense etc). For both models the maximum
assigned rank is the cut-off rank 1000.

To put these results into perspective, the results

of compositional models were interpreted through
the lens of annotated semantic relations in publicly
available datasets. Figure 1 plots the median rank
assigned to the compounds with a particular se-
mantic relation against the number of compounds
labeled with that semantic relation in the subset
of the Tratz (2011) dataset included in the compo-
sitionality dataset described in Section 3.1. The
figure confirms the intuition that recovering the
meaning of lexicalized compounds like eye candy
and basket case is very difficult given only the
constituents: the LEXICALIZED relation, which
labels 131 compounds, has the median rank 27.
Another difficult semantic relation for the compo-
sition model is PARTIAL ATTRIBUTE TRANSFER,
which labels compounds such as hairline crack
and bullet train, which has a median rank of 12
for its 41 compounds. The high median rank sug-
gests that this type of attributive relation is difficult
to model using distributional representations of the
individual constituents, as it is based on a common
attribute which is not present in the surface form
of the compound (the width for the hairline and
the crack; the speed for the bullet and the train).

5 Automatic Semantic Relation
Classification for English Nominal
Compounds

The goal of the current section is to asses the im-
pact of composition models on the task of auto-
matic semantic relation classification for English
nominal compounds. The semantic relation classi-
fication task deals with predicting the correct label
for the relation between the constituents of a com-

31



pound, given a fixed set of possible labels (e.g. the
label of the relation linking iron to fence in iron
fence is MATERIAL). The two datasets described
in Section 3.2 are used as a testbed for the com-
parison of the composition models described in
Section 4. The state of the art results for these
datasets are 65.4% 5-fold cross-validation (CV)
accuracy for the Ó Séaghdha dataset, obtained in
Ó Séaghdha and Copestake (2013), 79.3% 10-
fold CV accuracy for an unpublished version of
the Tratz dataset, with 17509 noun pairs annotated
with 43 semantic relations (Tratz and Hovy, 2010)
and 77.70% 10-fold CV accuracy on a subset of
the Tratz (2011) dataset obtained in (Dima and
Hinrichs, 2015).

Our MLP architecture for semantic classifica-
tion consists of two modules: the composition
module which constructs the compound represen-
tation from the representations of its constituents
and the classification module which takes as in-
put the constructed compound representation and
uses it as a basis for classifying the compound with
respect to the semantic relations defined by each
dataset.6

In the experiments described next the architec-
ture of the composition module varies according
to the method used for creating compound rep-
resentations, while the classification module al-
ways follows the same architecture: a linear layer
Wrel ∈ Rnc×k where nc is the dimensionality of
the compound representation and k is the number
of semantic relations in the dataset, the nonlin-
earity tanh and a softmax layer that selects the
“winning” semantic relation from the k possible
relations. Another constant addition to the full ar-
chitecture is a 0.1 dropout layer for regularization
and a reLU nonlinearity between the composition
and the classification modules.

All the described models are trained using a
negative log-likelihood criterion, optimized with
mini-batch Adagrad (Duchi et al., 2011) with a
fixed initial learning rate (0.1, Tratz dataset; 5e−2,
Ó Séaghdha dataset), learning rate decay 1e − 5,
weight decay 1e− 5 and a batch size of 100 as hy-
perparameters for the optimization process. The
models are trained using early stopping with a pa-
tience of 100 epochs.

Our working hypothesis is that learning first
how to compose, and then doing the semantic re-

6The code for composing representations and for doing
automatic classification of semantic relations is available at
https://github.com/corinadima/gWordcomp

lation classification task should yield better results
than when the composition is learned based only
on the signal provided by the classification task.
We expect that pre-training the composition mod-
ule would make the semantic relation classifica-
tion task easier and that having a good compound
representation would aid its semantic interpreta-
tion.

We define as a basic composition module a
simple architecture that takes as input u and v,
the two n-dimensional constituent representations,
concatenates them, and multiplies the concate-
nated 2n-dimensional vector with a matrix W .
Depending on the output dimensions of the model
we want to compare it to, the dimensions of W
will range from ∈ R2n×n to ∈ R2n×4n.

Table 2 presents the results of the classification
models, grouped according to the number of pa-
rameters in the composition module. We used the
matrix and full additive composition models eval-
uated in Section 4.2 as pre-trained composition
modules.

The first two rows in Table 2 present the results
of doing semantic relation classification using the
composed compound representations as the only
input to the classifier. In these settings, which are
labeled compoM300×600 and compoFA300×600, the
input is the composed representation as computed
by the pretrained matrix and full additive compo-
sition models. The composed representations are
kept fixed during the classification process. This
configuration obtained the weakest results from all
the tested configurations. An explanation for this
result might be that the composition models per-
form well for only half of our test compounds,
meaning that a good portion of the compounds
have a potentially suboptimal representation.

In the next two rows the pre-trained
composition models are fine-tuned for
the semantic classification task (models
labeled pretrain matrix600×300 and pre-
train fullAdditive600×300). The input in this
case are the initial corpus-based vectors of the two
constituents.

Contrary to our hypothesis, the classification re-
sults of the basic600×300 model (the last model in
the first subsection) are on par or slightly better
than the previous results, where the classification
used the direct or fine-tuned output of a pre-trained
composition module.

This effect extends to the other settings that

32



Composition module Pre-trained? Fine-tuned? Tratz CV Ó Séaghdha CV
compoM300×600 yes no 74.22% 57.52%
compoFA300×600 yes no 73.70% 56.62%
pretrain matrix600×300 yes yes 78.05% 59.18%
pretrain fullAdditive600×300 yes yes 77.89% 59.18%
basic600×300 no no 78.57% 59.25%
pretrain matrix fullAdd600×600 yes yes 78.92% 59.39%
basic600×600 no no 78.88% 59.60%
c1c2 compoM900×900 yes no 79.06% 61.12%
c1c2 compoFA900×900 yes no 79.07% 59.60%
basic600×1200 no no 79.03% 59.60%
c1c2 compoMcompoFA1200×1200 yes no 79.16% 59.18%
basic600×2400 no no 79.36% 58.49%

Table 2: Semantic relation classification results on the Tratz and Ó Séaghdha datasets using accuracy
as a classification measure. Results obtained through 10-fold cross-validation on the Tratz dataset and
5-fold CV on the Ó Séaghdha dataset (with the original folds).

were investigated, where:

• both pre-trained composition models are
used for the composition module; the com-
pound representation is the concatenation
of the two composed representations (pre-
train matrix fullAdd600×600); even if the
combined classifier outperforms each of the
classifiers based on only one composition
model, its results are still on par with the ones
of the basic classifier with a similar num-
ber of parameters (basic600×600, see results
in Table 2, subsection 2).

• the initial vector representations of
the constituents as well as their
composed representation are used
as an input (c1c2 compoM900×900,
c1c2 compoFA900×900); the composition
is in this case not fine-tuned; the results
on the Tratz (2011) dataset are again
similar to the comparable basic model
(basic600×1200). The c1c2 compoM900×900
obtains the best overall result, 61.12%, on
the Ó Séaghdha (2008) dataset.

• the input consists of the initial vector rep-
resentations and both composed representa-
tions (c1c2 compoMcompoFA1200×1200); the
composed vectors are fixed; the results are
compared to the basic600×2400 model (again,
with a similar number of parameters). This
last section contains the best overall result for

the Tratz (2011) dataset, 79.36%, obtained by
the basic600×2400 model.

To understand this unexpected result we
analyzed the predictions made by the best
performing classification models, basic600×2400
and c1c2 compoMcompoFA1200×1200, on the
Tratz (2011) dataset. The analysis targeted the dis-
tribution of errors per semantic relation for each
of the two classifiers. As the distribution of com-
pounds labeled with a particular semantic relation
is rather skewed, we found it more informative
to look at the percentage of errors for each class
(shown in Figure 2) rather than at the absolute er-
ror values.

A first conclusion that can be drawn from this
figure is that the two models have roughly the
same distribution of errors: both struggle the
most with the semantic relations with a low com-
pound count (left side of the figure) and with the
class of lexicalized compounds. In addition, even
some of the relations with more than 500 labeled
examples (starting from SUBSTANCE-MATERIAL-
INGREDIENT) remain difficult to identify (in par-
ticular the heterogeneous OTHER relation, which
labels compounds whose relation is not covered
by the rest of the inventory, and the EQUATIVE re-
lation, which labels compounds based on subtype
or logical-and relations, i.e. mozzarella cheese, fe-
male owner).

An analysis of the classification errors revealed
that both classifiers actually struggle to generalize
above the lexical level. If a word has the majority

33



VA
RI

ET
Y&

GE
NU

S_
OF

JU
ST

IF
IC

AT
IO

N

W
HO

LE
+
AT

TR
IB

UT
E&

FE
AT

UR
E&

QU
AL

IT
Y_

VA
LU

E_
IS

_C
HA

RA
CT

ER
IS

TI
C_

OF

TO
PI
C_

OF
_C

OG
NI

TI
ON

&E
M
OT

IO
N

PA
RT

IA
L_

AT
TR

IB
UT

E_
TR

AN
SF

ER

TI
M
E-

OF
2

PE
RS

ON
AL

_N
AM

E

EX
PE

RI
EN

CE
R-

OF
-E

XP
ER

IE
NC

E

PE
RS

ON
AL

_T
IT

LE

TO
PI
C_

OF
_E

XP
ER

T

LE
XI

CA
LI
ZE

D

M
IT

IG
AT

E&
OP

PO
SE

OB
TA

IN
&A

CC
ES

S&
SE

EK

AM
OU

NT
-O

F

US
ER

_R
EC

IP
IE

NT

CO
NT

AI
N

AD
J-L

IK
E_

NO
UN

M
EA

NS

CR
EA

TO
R-

PR
OV

ID
ER

-C
AU

SE
_O

F

OR
GA

NI
ZE

&S
UP

ER
VI

SE
&A

UT
HO

RI
TY

W
HO

LE
+
PA

RT
_O

R_
M
EM

BE
R_

OF

PA
RT

&M
EM

BE
R_

OF
_C

OL
LE

CT
IO

N&
CO

NF
IG

&S
ER

IE
S

PU
RP

OS
E

OW
NE

R-
US

ER

TI
M
E-

OF
1

EM
PL

OY
ER

SU
BS

TA
NC

E-
M
AT

ER
IA

L-
IN

GR
ED

IE
NT

SU
BJ

EC
T

M
EA

SU
RE

CR
EA

TE
-P

RO
VI

DE
-G

EN
ER

AT
E-

SE
LL

LO
CA

TI
ON

OT
HE

R

EQ
UA

TI
VE

RE
LA

TI
ON

AL
-N

OU
N-

CO
M
PL

EM
EN

T

TO
PI
C

PE
RF

OR
M
&E

NG
AG

E_
IN

OB
JE

CT
IV

E

0

20

40

60

80

100

%
 c

la
ss

if
ic

a
ti

o
n
 e

rr
o
rs

basic600× 2400

c1c2_compoMcompoFA1200× 1200

Figure 2: Error analysis for semantic relation classification on the Tratz (2011) dataset: the percentage of
errors for each semantic class for the basic600×2400 (blue, left) and the c1c2 compoMcompoFA1200×1200
(green, right) models. The semantic relations are sorted by compound count (low count to the left, high
count to the right).

of compounds labeled with a relation (e.g. TOPIC
for compounds with guide: travel guide, fishing
guide), other compounds with the head guide will
be assigned the same relation (e.g. user guide
is labeled TOPIC although the correct relation is
USER RECIPIENT). This phenomenon where the
classifier memorizes lexical associations between
words in particular slots and classification labels
as opposed to learning relations between the words
in the two slots is referred to in Levy et al. (2015)
as lexical memorization. To get a sense of how this
phenomenon affects our classification task we plot
in Figure 3 two ratios for every semantic relation
in the Tratz (2011) dataset: the number of distinct
modifiers over the total number of compounds and
the number of distinct heads over the total number
of compounds. A small ratio indicates that a large
subset of the compounds labeled with a particular
semantic relation share a common constituent: for
example, the ADJ-LIKE NOUN subclass has only
7 distinct modifiers for 254 compounds, resulting
in a very low modifier ratio (0.03). Similarly the
AMOUNT OF subclass has 168 compounds with 15
heads (head ratio: 0.09).

Comparing Figure 3 to Figure 2, one can ob-
serve that the majority of the classes with ei-
ther a low head ratio or a low modifier ra-
tio also have the lowest percentage of er-
rors per class. This is the case for relations
like TIME OF2, TOPIC OF EXPERT, AMOUNT-OF,
ADJ-LIKE NOUN and MEASURE, all of which
have under 10% error rate. A notable excep-
tion is the PERSONAL NAME semantic relation for
which the classifiers manage to have a small error

rate even with very diverse modifiers and heads
(both modifier and head ratio is 0.96). A more re-
alistic estimate of the actual performance of the
classifiers are the semantic relations which have
both a larger number of compounds and a more
diverse set of constituents, like in the case of
USER RECIPIENT, CREATOR PROVIDER CAUSE-
OF, WHOLE+PART OR MEMBER OF or PURPOSE,
which have a 40-60% error rate.

As a concluding point, the best results in our
study are comparable to the respective state-of-
the-art counterparts (79.3%/77.70% accuracy vs.
79.36% accuracy on the Tratz data; 65.4% vs.
61.12% on the Ó Séaghdha data). However, it
must be taken into account that in this study
the only available information for the classifiers
comes from the word embeddings themselves, and
from the correlations learned in the composition
process. By contrast the classifiers used in (Tratz
and Hovy, 2010; Tratz, 2011) relied on an exten-
sive feature set which included information from
the WordNet (hypernyms, synonyms, gloss, part-
of-speech indicators; “lexicalized” indicator if the
compound had an WN entry as a single term), Ro-
get’s thesaurus, surface-level features and n-gram
features extracted from the Web 1T corpus. The
state-of-the-art of the Ó Séaghdha (2008) dataset
is based on both lexical features (for the individ-
ual constituents, constructed on the basis of de-
pendency relations) and relational features (for the
typical interactions of constituents, constructed on
the basis of contexts where the constituents appear
together as separate words). The distributional
representations we use as input are likely to cap-

34



VA
RI

ET
Y&

GE
NU

S_
OF

JU
ST

IF
IC

AT
IO

N

W
HO

LE
+
AT

TR
IB

UT
E&

FE
AT

UR
E&

QU
AL

IT
Y_

VA
LU

E_
IS

_C
HA

RA
CT

ER
IS

TI
C_

OF

TO
PI
C_

OF
_C

OG
NI

TI
ON

&E
M
OT

IO
N

PA
RT

IA
L_

AT
TR

IB
UT

E_
TR

AN
SF

ER

TI
M
E-

OF
2

PE
RS

ON
AL

_N
AM

E

EX
PE

RI
EN

CE
R-

OF
-E

XP
ER

IE
NC

E

PE
RS

ON
AL

_T
IT

LE

TO
PI
C_

OF
_E

XP
ER

T

LE
XI

CA
LI
ZE

D

M
IT

IG
AT

E&
OP

PO
SE

OB
TA

IN
&A

CC
ES

S&
SE

EK

AM
OU

NT
-O

F

US
ER

_R
EC

IP
IE

NT

CO
NT

AI
N

AD
J-L

IK
E_

NO
UN

M
EA

NS

CR
EA

TO
R-

PR
OV

ID
ER

-C
AU

SE
_O

F

OR
GA

NI
ZE

&S
UP

ER
VI

SE
&A

UT
HO

RI
TY

W
HO

LE
+
PA

RT
_O

R_
M
EM

BE
R_

OF

PA
RT

&M
EM

BE
R_

OF
_C

OL
LE

CT
IO

N&
CO

NF
IG

&S
ER

IE
S

PU
RP

OS
E

OW
NE

R-
US

ER

TI
M
E-

OF
1

EM
PL

OY
ER

SU
BS

TA
NC

E-
M
AT

ER
IA

L-
IN

GR
ED

IE
NT

SU
BJ

EC
T

M
EA

SU
RE

CR
EA

TE
-P

RO
VI

DE
-G

EN
ER

AT
E-

SE
LL

LO
CA

TI
ON

OT
HE

R

EQ
UA

TI
VE

RE
LA

TI
ON

AL
-N

OU
N-

CO
M
PL

EM
EN

T

TO
PI
C

PE
RF

OR
M
&E

NG
AG

E_
IN

OB
JE

CT
IV

E

0.0

0.2

0.4

0.6

0.8

1.0

ra
ti

o
 o

f 
d
is

ti
n
ct

 m
o
d
if
ie

rs
 o

r 
h
e
a
d
s 

to
 #

co
m

p
o
u
n
d
s 

(p
e
r 

re
la

ti
o
n
)

distinct modifier ratio

distinct head ratio

minimum ratio

Figure 3: Diversity of modifiers and heads per relation: a low ratio for either the modifier (blue circle)
or the head (green triangle) correlates with a small error rate for the classification task.

ture both lexical and relational aspects, but do not
explicitly model pairwise constituent interactions.

6 Conclusions

In this paper we have presented a study covering
the creation of compositional distributional rep-
resentations for English noun compounds. The
representations created by the compositional mod-
els were further evaluated on the task of auto-
matic semantic relation classification for English
noun compounds, using two preexisting annotated
datasets. The experiments are, to the best of
our knowledge, the first compositional investiga-
tions focusing on English noun compounds. The
composition models have a good performance and
manage to build meaningful composed vectors for
half of the test set compounds.

The investigation of semantically annotated
compound datasets revealed that composition
models cannot represent compounds with lexical-
ized meaning. This suggests that the represen-
tations of compounds where the meaning of the
whole is substantially different from the one of
the parts should be learned directly from corpus
co-occurence data. Another vocabulary-related
observation concerns the extensive pre-processing
necessary to create distributional representations
for compounds. Spelling variation (e.g. health
care, health-care, healthcare) artificially creates
separate forms with the same meaning. Such
forms should be identified and collapsed back to a
single meaning representation when creating vec-
tor space models of language.

The semantic relation classification experi-
ments showed that state-of-the-art composition
models must be further refined before they can
be of use for downstream semantic tasks. In our
experiments compositional models were unable to
improve upon a basic model for semantic relation
identification, despite being pretrained on a large
set of compounds. Their mediocre performance on
the semantic relation classification task is likely
caused by the use of individual word representa-
tions as the exclusive source of input, combined
with the expectation that mathematical composi-
tion functions can directly extract and model pat-
terns of interaction between pairs of words. We
hypothesize that composition models can be im-
proved by first modeling the semantic relations be-
tween words and then using the semantic relation
representations together with the word representa-
tions as inputs to the composition process.

Acknowledgments

The author is indebted to Melanie Bell for the
fruitful discussions and her comprehensive com-
ments on the initial draft of the paper. The author
would also like to thank Emanuel Dima and Er-
hard Hinrichs, as well as the anonymous reviewers
for their insightful comments and suggestions. Fi-
nancial support for the research reported in this pa-
per was provided by the German Research Foun-
dation (DFG) as part of the Collaborative Research
Center “The Construction of Meaning” (SFB 833),
project A3.

35



References
Marco Baroni and Roberto Zamparelli. 2010. Nouns

are vectors, adjectives are matrices: Represent-
ing adjective-noun constructions in semantic space.
In Proceedings of the Conference on Empirical
Methods in Natural Language Processing (EMNLP
2010), pages 1183–1193, Massachussetts, USA.

Laurie Bauer. 1983. English word-formation. Cam-
bridge University Press.

Ronan Collobert, Koray Kavukcuoglu, and Clément
Farabet. 2011a. Torch7: A Matlab-like environment
for machine learning. In BigLearn, NIPS Workshop.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011b. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493–2537.

Corina Dima and Erhard Hinrichs. 2015. Auto-
matic noun compound interpretation using deep neu-
ral networks and word embeddings. In Proceedings
of the 11th International Conference on Computa-
tional Semantics (IWCS 2015), pages 173–183, Lon-
don, UK.

Georgiana Dinu, The Pham Nghia, and Marco Baroni.
2013a. DISSECT - DIStributional SEmantics Com-
position Toolkit. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics (ACL 2013), pages 31–36, Sofia, Bulgaria.

Georgiana Dinu, The Pham Nghia, and Marco Baroni.
2013b. General estimation and evaluation of com-
positional distributional semantic models. In ACL
Workshop on Continuous Vector Space Models and
their Compositionality, Sofia, Bulgaria.

David R. Dowty, Robert Wall, and Stanley Peters.
1981. Introduction to Montague semantics, vol-
ume 11 of Synthese Language Library. Springer
Science & Business Media.

John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. The Journal of Ma-
chine Learning Research, 12:2121–2159.

Emiliano Guevara. 2010. A regression model of
adjective-noun compositionality in distributional se-
mantics. In Proceedings of the 2010 Workshop on
GEometrical Models of Natural Language Seman-
tics, pages 33–37. Association for Computational
Linguistics.

Mark Lauer. 1995. Designing statistical language
learners: Experiments on compound nouns. Ph.D.
thesis, Macquarie University.

Omer Levy, Steffen Remus, Chris Biemann, Ido Da-
gan, and Israel Ramat-Gan. 2015. Do Supervised

Distributional Methods Really Learn Lexical Infer-
ence Relations? In Proceedings of the 2015 Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics - Human Lan-
guage Technologies (NAACL HLT 2015), Denver,
CO, USA.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.

Thomas Müller and Hinrich Schütze. 2015. Robust
morphological tagging with word representations.
In Proceedings of the 2015 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics - Human Language Technologies
(NAACL HLT 2015), Denver, CO, USA.

Preslav Nakov. 2013. On the interpretation of noun
compounds: Syntax, semantics, and entailment.
Natural Language Engineering, 19(03):291–330.

Diarmuid Ó Séaghdha and Ann Copestake. 2013. In-
terpreting compound nouns with kernel methods.
Natural Language Engineering, 19(03):331–356.

Diarmuid Ó Séaghdha. 2008. Learning compound
noun semantics. Ph.D. thesis, Computer Laboratory,
University of Cambridge. Published as University
of Cambridge Computer Laboratory Technical Re-
port 735.

F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine learn-
ing in Python. Journal of Machine Learning Re-
search, 12:2825–2830.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. GloVe: Global vectors
for word representation. In Proceedings of the Em-
piricial Methods in Natural Language Processing
(EMNLP 2014), volume 12.

Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multiword
expressions: A pain in the neck for NLP. In Compu-
tational Linguistics and Intelligent Text Processing,
pages 1–15. Springer.

Roland Schäfer. 2015. Processing and querying large
web corpora with the COW14 architecture. In
Challenges in the Management of Large Corpora
(CMLC-3).

Richard Socher, Jeffrey Pennington, Eric H. Huang,
Andrew Y. Ng, and Christopher D. Manning. 2011.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2011), pages 151–161.
Association for Computational Linguistics.

36



Stephen Tratz and Eduard Hovy. 2010. A taxonomy,
dataset, and classifier for automatic noun compound
interpretation. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics (ACL-10), Uppsala, Sweden.

Stephen Tratz. 2011. Semantically-enriched parsing
for natural language understanding. Ph.D. thesis,
University of Southern California.

Wenpeng Yin and Hinrich Schütze. 2014. An explo-
ration of embeddings for generalized phrases. In
ACL 2014 Student Research Workshop, pages 41–
47, Baltimore, USA.

Fabio Massimo Zanzotto, Ioannis Korkontzelos,
Francesca Fallucchi, and Suresh Manandhar. 2010.
Estimating Linear Models for Compositional Distri-
butional Semantics. In Proceedings of the 23rd In-
ternational Conference on Computational Linguis-
tics, pages 1263–1271.

37



A Overview of the Semantic Relations in the Tratz (2011) and Ó Séaghdha (2008)
Datasets

Category name Dataset percentage Example

Objective
OBJECTIVE 17.1% leaf blower

Doer-Cause-Means
SUBJECT 3.5% police abuse
CREATOR-PROVIDER-CAUSE OF 1.5% ad revenue
JUSTIFICATION 0.3% murder arrest
MEANS 1.5% faith healer

Purpose/Activity Group
PERFORM&ENGAGE IN 11.5% cooking pot
CREATE-PROVIDE-GENERATE-SELL 4.8% nicotine patch
OBTAIN&ACCESS&SEEK 0.9% shrimp boat
MITIGATE&OPPOSE 0.8% flak jacket
ORGANIZE&SUPERVISE&AUTHORITY 1.6% ethics authority
PURPOSE 1.9% chicken spit

Ownership, Experience, Employment, Use
OWNER-USER 2.1% family estate
EXPERIENCER-OF-EXPERIENCE 0.5% family greed
EMPLOYER 2.3% team doctor
USER RECIPIENT 1.0% voter pamphlet

Temporal Group
TIME-OF1 2.2% night work
TIME-OF2 0.5% birth date

Location and Whole+Part/Member of
LOCATION 5.2% hillside home
WHOLE+PART OR MEMBER OF 1.7% robot arm

Composition and Containment Group
CONTAIN 1.2% shoe box
SUBSTANCE-MATERIAL-INGREDIENT 2.6% plastic bag
PART&MEMBER OF COLLECTION&CONFIG&SERIES 1.8% truck convoy
VARIETY&GENUS OF 0.1% plant species
AMOUNT-OF 0.9% traffic volume

Topic Group
TOPIC 7.0% travel story
TOPIC OF COGNITION&EMOTION 0.3% auto fanatic
TOPIC OF EXPERT 0.7% policy expert

Other Complements Group
RELATIONAL-NOUN-COMPLEMENT 5.6% eye shape
WHOLE+ATTRIBUTE&FEATURE 0.3% earth tone
&QUALITY VALUE IS CHARACTERISTIC OF

Attributive and Equative
EQUATIVE 5.4% fighter plane
ADJ-LIKE NOUN 1.3% core activity
PARTIAL ATTRIBUTE TRANSFER 0.3% skeleton crew
MEASURE 4.2% hour meeting

Other
LEXICALIZED 0.8% pig iron
OTHER 5.4% contact lense

Personal*
PERSONAL NAME 0.5% Ronald Reagan
PERSONAL TITLE 0.5% Gen. Eisenhower

Table 3: Semantic relations in the Tratz inventory - abbreviated version of Table 4.5 from Tratz (2011).

38



Relation Frequency Proportion Examples

BE 191 13.2% guide dog, rubber wheel, cat burglar
HAVE 199 13.8% family firm, coma victim, sentence structure, computer clock, star cluster
IN 308 21.3% pig pen, air disaster, evening edition, dawn attack
ACTOR 266 18.4% army coup, project organiser
INST 236 16.4% cereal cultivation, foot imprint
ABOUT 243 16.8% history book, waterways museum, embryo research, house price

Table 4: Semantic relations in the Ó Séaghdha inventory - Table 6.2 from Ó Séaghdha (2008), augmented
with examples from Table 3.1.

39


