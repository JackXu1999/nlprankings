











































Dynamic and Static Topic Model for Analyzing Time-Series Document Collections


Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 516–520
Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics

516

Dynamic and Static Topic Model
for Analyzing Time-Series Document Collections

Rem Hida† Naoya Takeishi‡† Takehisa Yairi† Koichi Hori†
†Department of Aeronautics and Astronautics, The University of Tokyo

{hida,yairi,hori}@ailab.t.u-tokyo.ac.jp
‡RIKEN Center for Advanced Intelligence Project, Tokyo, Japan

naoya.takeishi@riken.jp

Abstract

For extracting meaningful topics from
texts, their structures should be considered
properly. In this paper, we aim to analyze
structured time-series documents such as
a collection of news articles and a series
of scientific papers, wherein topics evolve
along time depending on multiple topics in
the past, and are also related to each other
at each time. To this end, we propose a
dynamic and static topic model, which si-
multaneously considers the dynamic struc-
tures of the temporal topic evolution and
the static structures of the topic hierarchy
at each time. We show the results of ex-
periments on collections of scientific pa-
pers, in which the proposed method out-
performed conventional models. More-
over, we show an example of extracted
topic structures, which we found helpful
for analyzing research activities.

1 Introduction

Probabilistic topic models such as latent Dirichlet
allocation (LDA) (Blei et al., 2003) have been uti-
lized for analyzing a wide variety of datasets such
as document collections, images, and genes. Al-
though vanilla LDA has been favored partly due to
its simplicity, one of its limitations is that the out-
put is not necessarily very understandable because
the priors on the topics are independent. Conse-
quently, there has been a lot of research aimed at
improving probabilistic topic models by utilizing
the inherent structures of datasets in their model-
ing (see, e.g., Blei and Lafferty (2006); Li and Mc-
Callum (2006); see Section 2 for other models).

In this work, we aimed to leverage the dynamic
and static structures of topics for improving the
modeling capability and the understandability of

topic models. These two types of structures, which
we instantiate below, are essential in many types of
datasets, and in fact, each of them has been con-
sidered separately in several previous studies. In
this paper, we propose a topic model that is aware
of both of these structures, namely dynamic and
static topic model (DSTM).

The underlying motivation of DSTM is twofold.
First, a collection of documents often has dynamic
structures; i.e., topics evolve along time influenc-
ing each other. For example, topics in papers are
related to topics in past papers. We may want
to extract such dynamic structures of topics from
collections of scientific papers for summarizing
research activities. Second, there are also static
structures of topics such as correlation and hierar-
chy. For instance, in a collection of news articles,
the “sports” topic must have the “baseball” topic
and the “football” topic as its subtopic. This kind
of static structure of topics helps us understand the
relationship among them.

The remainder of this paper is organized as fol-
lows. In Section 2, we briefly review related work.
In Section 3, the generative model and the infer-
ence/learning procedures of DSTM are presented.
In Section 4, the results of the experiments are
shown. This paper is concluded in Section 5.

2 Related Work

Researchers have proposed several variants of
topic models that consider the dynamic or static
structure. Approaches focusing on the dynamic
structure include dynamic topic model (DTM)
(Blei and Lafferty, 2006), topic over time (TOT)
(Wang and McCallum, 2006), multiscale dynamic
topic model (MDTM) (Iwata et al., 2010), de-
pendent Dirichlet processes mixture model (D-
DPMM) (Lin et al., 2010), and infinite dynamic
topic model (iDTM) (Ahmed and Xing, 2010).



517

Dt number of documents at epoch t
ntd number of words in the d-th doc. at epoch t
wtd,i the i-th word in the d-th doc. at epoch t
K total number of subtopics
S number of supertopics
ytd,i supertopic of wtd,i
ztd,i subtopic of wtd,i
1✓td multinomial distribution over supertopics for

the d-th doc. at epoch t
2✓td,s multinomial distribution over subtopics for the

d-th doc. in s-th supertopic at epoch t
�tk multinomial distribution over words for the k-

th subtopic at epoch t
2↵ts static structure weight (prior of 2✓td,s)
�t dynamic structure weight between topics at

time t� 1 and those at epoch t

Table 1: Notations in the proposed model.

These methods have been successfully applied to
a temporal collection of documents, but none of
them take temporal dependencies between multi-
ple topics into account; i.e., in these models, only
a single topic contributes to a topic in the future.

For the static structure, several models includ-
ing correlated topic model (CTM) (Lafferty and
Blei, 2006), pachinko allocation model (PAM) (Li
and McCallum, 2006), and segmented topic model
(STM) (Du et al., 2010) have been proposed. CTM
models the correlation between topics using the
normal distribution as the prior, PAM introduces
the hierarchical structure to topics, and STM uses
paragraphs or sentences as the hierarchical struc-
ture. These models can consider the static struc-
ture such as correlation and hierarchy between
topics. However, most of them lack the dynamic
structure in their model; i.e., they do not premise
temporal collections of documents.

One of the existing methods that is most re-
lated to the proposed model is the hierarchical
topic evolution model (HTEM) (Song et al., 2016).
HTEM captures the relation between evolving
topics using a nested distance-dependent Chinese
restaurant process. It has been successfully ap-
plied to a temporal collection of documents for ex-
tracting structure but does not take multiple topics
dependencies into account either.

In this work, we built a new model to overcome
the limitation of the existing models, i.e., to ex-
amine both the dynamic and static structures si-
multaneously. We expect that the proposed model
can be applied to various applications such as topic
trend analysis and text summarization.

Figure 1: Graphical model of the proposed model
for epochs t� 1 and t.

3 Dynamic and Static Topic Model

In this section, we state the generative model of
the proposed method, DSTM. Afterward, the pro-
cedure for inference and learning is presented. Our
notations are summarized in Table 1.

3.1 Generative Model
In the proposed model, DSTM, the dynamic and
static structures are modeled as follows.

Dynamic Structure We model the temporal
evolution of topic-word distribution by making it
proportional to a weighted sum of topic-word dis-
tributions at the previous time (epoch), i.e.,

�tk ⇠ Dirichlet
 

KX

k0=1

�tk,k0�
t�1
k0

!
, (1)

where �tk denotes the word distribution of the k-th
topic at the t-th time-epoch, and �tk,k0 is a weight
that determines the dependency between the k-th
topic at epoch t and the k0-th topic at epoch t� 1.

Static Structure We model the static structure
as a hierarchy of topics at each epoch. We uti-
lize the supertopic-subtopic structure as in PAM
(Li and McCallum, 2006), where the priors of top-
ics (subtopics) are determined by their supertopic.

Generative Process In summary, the generative
process at epoch t is as follows.
1. For each subtopic k = 1, ..,K ,

(a) Draw a topic-word distribution
�tk ⇠ Dirichlet(

P
k0 �

t
k,k0�

t�1
k0 ).

2. For each document d = 1, ..., Dt,
(a) Draw a supertopic distribution

1✓td ⇠ Dirichlet(1↵t).



518

(b) For each supertopic s = 1, ..., S,
i. Draw a subtopic distribution

2✓td,s ⇠ Dirichlet(2↵ts).
(c) For each word i = 1, ..., ntd,

i. Draw a supertopic-word assignment
ytd,i ⇠ Multinomial(1✓td).

ii. Draw a subtopic-word assignment
ztd,i ⇠ Multinomial(2✓td,ytd,i).

iii. Draw a word-observation
wtd,i ⇠ Multinomial(�tztd,i).

Note that the above process should be repeated for
every epoch t. The corresponding graphical model
is presented in Figure 1.

3.2 Inference and Learning
Since analytical inference for DSTM is in-
tractable, we resort to a stochastic EM algorithm
(Andrieu et al., 2003) with the collapsed Gibbs
sampling (Griffiths and Steyvers, 2004). How-
ever, such a strategy is still much costly due to the
temporal dependencies of �. Therefore, we intro-
duce a further approximation; we surrogate �t�1k0
in Eq. (1) by its expectation ˆ�t�1k0 = E[�

t�1
k0 ]. This

compromise enables us to run the EM algorithm
for each epoch in sequence from t = 1 to t = T
without any backward inference. In fact, such ap-
proximation technique is also utilized in the infer-
ence of MDTM (Iwata et al., 2010).

Note that the proposed model has a moderate
number of hyperparameters to be set manually,
and that they can be tuned according to the ex-
isting know-how of topic modeling. This feature
makes the proposed model appealing in terms of
inference and learning.

E-step In E-step, the supertopic/subtopic as-
signments are sampled. Given the current state of
all variables except ytd,i and z

t
d,i, new values for

them should be sampled according to

p(ytd,i = s, z
t
d,i = k | w

t, yt, zt,�t�1, 1↵t, 2↵t,�t)

/
ntd,s\i +

1↵ts

ntd\i +
PS

s=1
1↵ts

·
ntd,s,k\i +

2↵ts,k

ntd,s\i +
PK

k=1
2↵ts,k

·
ntk,v\i +

PK
k0=1 �

t
k,k0

ˆ�t�1k0,v

ntk\i +
PK

k0=1 �
t
k,k0

,

(2)

where ntk,v denotes the number of tokens assigned
to topic k for word v at epoch t, ntk=

P
v n

t
k,v,

and ntd,s and n
t
d,s,k denote the number of tokens in

document d assigned to supertopic s and subtopic

NIPS Drone

Date 1987–1999 2009–2016
# Documents 1,740 1,035
# Vocabulary 11,443 3,442

# Tokens 2,271,087 68,305

Table 2: Summary of the datasets.

k (via s), at epoch t respectively. Moreover, nt·\i
denotes the count yielded excluding the i-th token.

M-step In M-step, 2↵t and �t are updated using
the fixed-point iteration (Minka, 2000).

(2↵ts,k)
⇤ = 2↵ts,k

PDt
d=1 (n

t
d,s,k +

2↵ts,k)� (
2↵ts,k)

PDt
d=1 (n

t
d,s +

2↵ts)� (2↵ts)
, (3)

(�tk,k0 )
⇤ = �tk,k0

P
v �̂

t�1
k0,vB

t
k0,v

 (ntk +
P

k0 �
t
k,k0 )� (

P
k0 �

t
k,k0 )

. (4)

Here, is the digamma function, 2↵ts=
P

k
2↵ts,k,

and

Btk0,v =  
⇣
ntk,v +

X

k0

�tk,k0 ˆ�
t�1
k0,v

⌘
� 

⇣X

k0

�tk,k0 ˆ�
t�1
k0,v

⌘
.

Overall Procedure The EM algorithm is run for
each epoch in sequence; at epoch t, after running
the EM until convergence, ˆ�tk,v is computed by

ˆ�tk,v =
ntk,v +

P
k0 �

t
k,k0

ˆ�t�1k0,v
ntk +

P
k0 �

t
k,k0

,

and then this value is used for the EM at the next
epoch t + 1. Moreover, see Supplementary A for
the computation of the statistics of the other vari-
ables.

4 Experiments

4.1 Datasets

We used two datasets comprising technical pa-
pers: NIPS (Perrone et al., 2016) and Drone (Liew
et al., 2017). NIPS is a collection of the pa-
pers that appeared in NIPS conferences. Drone
is a collection of abstracts of papers on unmanned
aerial vehicles (UAVs) and was collected from re-
lated conferences and journals for surveying re-
cent developments in UAVs. The characteristics
of those datasets are summarized in Table 2. See
Supplementary B for the details of data prepro-
cessing.



519

NIPS Drone
static dynamic K30 (S15) K40 (S20) K50 (S25) K15 (S3) K20 (S3) K25 (S3)

LDA - - 1455.6 (16.7) 1407.3 (15.9) 1374.6 (16.8) 1624.3 (191.1) 1634.8 (189.1) 1644.7 (193.0)
PAM X - 1455.1 (18.2) 1407.0 (17.5) 1376.9 (16.7) 1587.4 (185.1) 1589.9 (191.4) 1590.8 (186.8)

DRTM - X 1380.7 (18.5) 1308.6 (17.5) 1253.9 (17.9) 1212.5 (153.2) 1206.1 (148.0) 1201.2 (143.5)
DSTM X X 1378.7 (16.5) 1301.0 (17.9) 1247.3 (17.2) 1194.2 (148.2) 1180.0 (147.0) 1171.6 (141.4)

Table 3: Means (and standard deviations) of PPLs averaged over all epochs for each dataset with different
values of K and S. The proposed method, DSTM, achieved the smallest PPL.

Figure 2: Part of the topic structure extracted from Drone dataset using the proposed method. The solid
arrows denote the temporal evolution of “planning” topics. The dotted arrows mean that “planning”
topics are related to “hardware”, “control”, and “mapping” topics via some supertopics (filled circles).

4.2 Evaluation by Perplexity
First, we evaluate the performance of the proposed
method quantitatively using perplexity (PPL):

PPL = exp

0

@�
PD

d=1

P
wtestd

log p(wd,i|M)
PD

d=1 n
test
d

1

A .

For each epoch, we used 90% of tokens in each
document for training and calculated the PPL us-
ing the remaining 10% of tokens. We randomly
created 10 train-test pairs and evaluated the means
of the PPLs over those random trials. We com-
pared the performance of DSTM to three base-
lines: LDA (Blei et al., 2003), PAM (Li and Mc-
Callum, 2006), and the proposed model without
the static structure, which we term DRTM. See
Supplementary C on their hyperparameter setting.

The means of the PPLs averaged over all epochs
for each dataset with different values K are shown
in Table 3. In both datasets with every setting
of K, the proposed model, DSTM, achieved the
smallest PPL, which implies its effectiveness for
modeling a collection of technical papers. For
clarity, we conducted paired t-tests between the
perplexities of the proposed method and those of
the baselines. On the differences between DSTM
and DRTM, the p-values were 4.2 ⇥ 10�2 (K =
30), 7.9 ⇥ 10�5 (K = 40), and 6.4 ⇥ 10�7
(K = 50) for the NIPS dataset, and 1.3 ⇥ 10�4

(K = 15), 8.8⇥ 10�5 (K = 20), and 4.9⇥ 10�6
(K = 25) for the Drone dataset, respectively. It
is also noteworthy that DRTM shows more sig-
nificant improvement relative to LDA than PAM
does. This suggests that the dynamic structure
with multiple-topic dependencies is essential for
datasets of this kind.

4.3 Analysis of Extracted Structure
We examined the topic structures extracted from
the Drone dataset using DSTM. In Figure 2, we
show a part of the extracted structure regarding
planning of the UAV’s path and/or movement. We
identified “planning” topics by looking for key-
words such as “trajectory” and “motion.” In Fig-
ure 2, each node is labeled with eight most prob-
able keywords. Moreover, solid arrows (dynamic
relations) are drawn if the corresponding �tk,k0 is
larger than 200, and dotted arrows (static relations)
are drawn between a supertopic and subtopics with
the two or three largest values of 2↵ts,k.

Looking at the dynamic structure, we may
see how research interest regarding planning has
changed. For example, word “online” first
emerges in the “planning” topic in 2016. This
is possibly due to the increasing interest in real-
time planning problems, which is becoming fea-
sible due to the recent development of on-board
computers. In regard to the static structures, for



520

example, the “planning” topic is related to the
“hardware” and “control” topics in 2013 and 2014,
whereas it is also related to the “mapping” topic in
2015 and 2016. Looking at these static structures,
we may anticipate how research areas are related
to each other in each year. In this case, we can
anticipate that planning problems are combined
with mapping problems well in recent years. Note
that we cannot obtain these results unless the dy-
namic and static structures are considered simul-
taneously.

5 Conclusion

In this work, we developed a topic model with
dynamic and static structures. We confirmed the
superiority of the proposed model to the conven-
tional topic models in terms of perplexity and ana-
lyzed the topic structures of a collection of papers.
Possible future directions of research include auto-
matic inference of the number of topics and appli-
cation to topic trend analysis in various domains.

Acknowledgments

We thank Chun Fui Liew for sharing his dataset
with us and advice.

References
Amr Ahmed and Eric P. Xing. 2010. Timeline: A dy-

namic hierarchical Dirichlet process model for re-
covering birth/death and evolution of topics in text
stream. In Proceedings of the 26th Conference on
Uncertainty in Artificial Intelligence, pages 20–29.

Christophe Andrieu, Nando de Freitas, Arnaud Doucet,
and Michael I. Jordan. 2003. An introduction to
MCMC for machine learning. Machine Learning,
50(1):5–43.

David M. Blei and John D. Lafferty. 2006. Dynamic
topic models. In Proceedings of the 23rd Interna-
tional Conference on Machine Learning, pages 113–
120.

David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet allocation. Journal of Ma-
chine Learning Research, 3(Jan):993–1022.

Lan Du, Wray Buntine, and Huidong Jin. 2010. A
segmented topic model based on the two-parameter
Poisson-Dirichlet process. Machine Learning,
81(1):5–19.

Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. In Proceedings of the National
Academy of Sciences of the United States of Amer-

ica, volume 101, pages 5228–5235.

Tomoharu Iwata, Takeshi Yamada, Yasushi Sakurai,
and Naonori Ueda. 2010. Online multiscale dy-
namic topic models. In Proceedings of the 16th
ACM SIGKDD International Conference on Knowl-

edge Discovery and Data Mining, pages 663–672.

John D. Lafferty and David M. Blei. 2006. Correlated
topic models. In Advances in Neural Information
Processing Systems, volume 18, pages 147–154.

Wei Li and Andrew McCallum. 2006. Pachinko allo-
cation: DAG-structured mixture models of topic cor-
relations. In Proceedings of the 23rd International
Conference on Machine Learning, pages 577–584.

Chun Fui Liew, Danielle DeLatte, Naoya Takeishi,
and Takehisa Yairi. 2017. Recent developments in
aerial robotics: A survey and prototypes overview.
arXiv:1711.10085.

Dahua Lin, Eric Grimson, and John W. Fisher. 2010.
Construction of dependent Dirichlet processes based
on Poisson processes. In Advances in Neural Infor-
mation Processing Systems, volume 23, pages 1396–
1404.

Thomas Minka. 2000. Estimating a Dirichlet distribu-
tion. Technical report, MIT.

Valerio Perrone, Paul A. Jenkins, Dario Spano, and
Yee Whye Teh. 2016. Poisson random fields for dy-
namic feature models. arXiv:1611.07460.

Jun Song, Yu Huang, Xiang Qi, Yuheng Li, Feng Li,
Kun Fu, and Tinglei Huang. 2016. Discovering hier-
archical topic evolution in time-stamped documents.
Journal of the Association for Information Science

and Technology, 67(4):915–927.

Xuerui Wang and Andrew McCallum. 2006. Topics
over time: A non-Markov continuous-time model
of topical trends. In Proceedings of the 12th ACM
SIGKDD International Conference on Knowledge

Discovery and Data Mining, pages 424–433.

https://event.cwi.nl/uai2010/papers/UAI2010_0296.pdf
https://event.cwi.nl/uai2010/papers/UAI2010_0296.pdf
https://event.cwi.nl/uai2010/papers/UAI2010_0296.pdf
https://event.cwi.nl/uai2010/papers/UAI2010_0296.pdf
https://doi.org/10.1023/A:1020281327116
https://doi.org/10.1023/A:1020281327116
http://doi.acm.org/10.1145/1143844.1143859
http://doi.acm.org/10.1145/1143844.1143859
http://dl.acm.org/citation.cfm?id=944919.944937
http://dx.doi.org/10.1007/s10994-010-5197-4
http://dx.doi.org/10.1007/s10994-010-5197-4
http://dx.doi.org/10.1007/s10994-010-5197-4
https://doi.org/10.1073/pnas.0307752101
https://doi.org/10.1073/pnas.0307752101
https://doi.org/10.1145/1835804.1835889
https://doi.org/10.1145/1835804.1835889
http://papers.nips.cc/paper/2906-correlated-topic-models.pdf
http://papers.nips.cc/paper/2906-correlated-topic-models.pdf
http://doi.acm.org/10.1145/1143844.1143917
http://doi.acm.org/10.1145/1143844.1143917
http://doi.acm.org/10.1145/1143844.1143917
http://arxiv.org/abs/arXiv:1711.10085
http://arxiv.org/abs/arXiv:1711.10085
http://papers.nips.cc/paper/4151-construction-of-dependent-dirichlet-processes-based-on-poisson-processes.pdf
http://papers.nips.cc/paper/4151-construction-of-dependent-dirichlet-processes-based-on-poisson-processes.pdf
http://arxiv.org/abs/arXiv:1611.07460
http://arxiv.org/abs/arXiv:1611.07460
https://doi.org/10.1002/asi.23439
https://doi.org/10.1002/asi.23439
http://doi.acm.org/10.1145/1150402.1150450
http://doi.acm.org/10.1145/1150402.1150450
http://doi.acm.org/10.1145/1150402.1150450

