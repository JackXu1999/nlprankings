



















































Hierarchical Multi-label Classification of Text with Capsule Networks


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, pages 323–330
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

323

Hierarchical Multi-label Classification of Text with Capsule Networks

Rami Aly, Steffen Remus, and Chris Biemann

Language Technology group
Universität Hamburg, Hamburg, Germany

{5aly,remus,biemann}@informatik.uni-hamburg.de

Abstract

Capsule networks have been shown to demon-
strate good performance on structured data in
the area of visual inference. In this paper
we apply and compare simple shallow cap-
sule networks for hierarchical multi-label text
classification and show that they can perform
superior to other neural networks, such as
CNNs and LSTMs, and non-neural network
architectures such as SVMs. For our experi-
ments, we use the established Web of Science
(WOS) dataset and introduce a new real-world
scenario dataset, the BlurbGenreCollection
(BGC). Our results confirm the hypothesis that
capsule networks are especially advantageous
for rare events and structurally diverse cate-
gories, which we attribute to their ability to
combine latent encoded information.

1 Introduction

In hierarchical multi-label classification (HMC),
samples are classified into one or multiple class
labels that are organized in a structured label hier-
archy (Silla and Freitas, 2011). HMC has been
thoroughly researched for traditional classifiers
(Sun and Lim, 2001; Silla and Freitas, 2011),
but with the increase of available data, the desire
for more specific and specialized hierarchies in-
creases. However, since traditional approaches fail
to generalize adequately, more sophisticated and
robust classification methods are receiving more
attention. Complex neural network classifiers on
the contrary are computationally expensive, diffi-
cult to analyze, and the amount of hyperparam-
eters is significantly higher as compared to other
classification approaches. This makes it difficult
to apply the local classifier approach (Silla and
Freitas, 2011), where multiple classifiers are em-
ployed to cover different parts of the hierarchy.
Therefore, in this paper we focus on the global ap-
proach – one classifier that is able to capture the
entire hierarchy at once. There are indications

that capsule networks (Hinton et al., 2011; Sabour
et al., 2017) are successful at finding, adapting,
and agreeing on latent structures in the underlying
data in the area of image recognition as well as
recently in the field of natural language process-
ing (Zhao et al., 2018). This insight motivates our
research question: To which extent can the capa-
bilities of capsule networks be transferred and ap-
plied to HMC in order to capture the categories’
underlying structures?

In our experiments1 we compare HMC-
adjusted capsule networks to several baseline neu-
ral as well as non-neural architectures on the
BlurbGenreCollection (BGC), a dataset which we
collected and that consists of so-called blurbs of
books and their hierarchically structured writing
genres. Additionally, we test our hypothesis on
the Web of Science (WOS) dataset (Kowsari et al.,
2017). The main benefit of capsules is their abil-
ity to encode information of each category sepa-
rately by associating each capsule with one cate-
gory. Combining encoded features independently
for each capsule, and thus category, enables cap-
sule networks to handle label combinations bet-
ter than previous approaches. This property is es-
pecially relevant for HMC since documents that
for instance only belong to a parent category, e.g.
Fiction, often share similar features such as the
most frequent words or n-grams with documents
that additionally classify into one of the parent’s
child labels, e.g. Mystery & Suspense or Fantasy.
This makes it difficult for traditional classifiers to
distinguish between parent and child labels cor-
rectly, especially if the specific combination of la-
bels was never observed during training. This pa-
per contributes in two ways: Firstly, we introduce
the new openly accessible BlurbGenreCollection
dataset for the English language. This dataset
is created and only minimally adjusted on basis

1Code for replicating results: https://github.
com/uhh-lt/BlurbGenreCollection-HMC

https://github.com/uhh-lt/BlurbGenreCollection-HMC
https://github.com/uhh-lt/BlurbGenreCollection-HMC


324

of a vertical search webpage for books and thus
presents a real-world scenario task. Secondly, we
thoroughly analyze the properties of capsule net-
works for HMC. To the best of our knowledge,
capsule networks have not yet been applied and
tested in the HMC domain.

2 Related Work

Neural networks for HMC: In hierarchical
multi-label classification (HMC) samples are as-
signed one or multiple class labels, which are
organized in a structured label hierarchy (Silla
and Freitas, 2011). For text classification (TC),
we treat a document as a sample and its cate-
gories as labels. Convolutional Neural Networks
(CNNs) and different types of Recurrent Neu-
ral Networks (RNNs) (Goodfellow et al., 2016;
Kim, 2014), most notably long short-term mem-
ory units (LSTMs, Hochreiter and Schmidhuber,
1997) have shown to be highly efficient in TC
tasks. For HMC, Cerri et al. (2014) use concate-
nated multi-layer perceptrons (MLP), where each
MLP is associated to one level of the class hierar-
chy. Kowsari et al. (2017) use multiple concate-
nated deep learning architectures (CNN, LSTM,
and MLP) to HMC on a dataset with a rather shal-
low hierarchy with only two levels. Similar to
Kiritchenko et al. (2005), Baker and Korhonen
(2017) treat the HMC task as a multi-label clas-
sification problem that considers every label in the
hierarchy, but they additionally leverage the co-
occurrence of labels within the hierarchy to initial-
ize the weights of their CNN’s final layer (Kurata
et al., 2016).

Capsule Networks: Capsule networks encap-
sulate features into groups of neurons, so-called
capsules (Hinton et al., 2011; Sabour et al., 2017).
Originally introduced for a handwritten digit im-
age classification task where each digit has been
associated with a capsule, capsules have shown to
learn more robust representations for each class as
they capture parent-child relationships more ac-
curately. They reached on-par performance with
more complex CNN architectures, even outper-
forming them in several classification tasks such
as the affNIST and MultiMNIST dataset (Sabour
et al., 2017). First attempts to use capsules for sen-
timent analysis were carried out by (Wang et al.,
2018) on the basis of an RNN, however, they
did not employ the routing algorithm, thus highly
limiting the capabilities of capsules. Zhao et al.

(2018) show that capsule networks can outperform
traditional neural networks for TC by a great mar-
gin when training on single-labeled and testing
on multi-labeled documents of the Reuters-21578
dataset since the routing of capsules behaves like a
parallel attention mechanism regarding the selec-
tion of categories. By connecting a BiLSTM to
a capsule network for relation extraction, Zhang
et al. (2018) show that capsule networks improve
at extracting n-ary relations, with n > 2, per sen-
tence and thus confirm the observation of (Zhao
et al., 2018) in a different context. For multi-task
learning, Xiao et al. (2018) use capsule networks
to improve the differentiation between tasks. They
encapsulate features in different capsules and use
the routing algorithm to cluster features for each
task. Further applications to NLP span aggression,
toxicity and emotion detection (Srivastava et al.,
2018; Rathnayaka et al., 2018), embedding cre-
ation for knowledge graph completion (Nguyen
et al., 2019), and knowledge transfer of user in-
tents (Xia et al., 2018). Despite the suitable prop-
erties of capsule networks to classify into hierar-
chical structured categories, they have not yet been
applied to HMC. This work aims to fill the gap
by applying and thoroughly analyzing capsules’
properties at HMC.

3 Capsule Network for HMC

For each category in the hierarchy, an associated
capsule outputs latent information of the category
in form of a vector as opposed to a single scalar
value used in traditional neural networks. The
vector is equivariant with its length defining the
pseudo-probability of its activation and its orien-
tation representing different cases of a category’s
existence. This distributional representation in the
form of a vector instead of a scalar makes cap-
sules exponentially more informative than tradi-
tional perceptrons (Sabour et al., 2017).

The input of capsules in the first capsule layer
of a capsule network is called primary capsules
and can be of arbitrary dimension, typically com-
ing from a convolutional layer or from the hid-
den state of a recurrent network. The output vec-
tor of a primary capsule represents latent informa-
tion such as local order and semantic representa-
tions of words (Zhao et al., 2018). Each capsule
j in the next layer, called classification capsules,
take as input the weighted sum sj =

∑
i cj|iûj|i

of the prediction vectors of all primary capsules



325

i. A capsule’s prediction vector ûj|i is generated
by multiplying the output uj|i by a weight matrix
Wij . Since the length of a vector of a classification
capsule should be interpreted as the probability of
the corresponding category, a squashing function
vj = squash(sj) is applied, which scales the out-
put of each classification capsule non-linearly be-
tween zero and one. The coupling coefficients cj|i
that determine the contribution of each primary
capsule’s output to a classification capsule are cal-
culated using a dynamic routing heuristic (Sabour
et al., 2017). It iteratively decides the routes of
capsules and thus how to cluster features for each
category. The pseudocode for the full routing al-
gorithm is written in Algorithm 1.

Routing algorithm
Result: vj
Initialization: ∀i ∈ Primary.∀j ∈
Classification : bj|i ← 0.

for r iterations do
∀i ∈ Primary : ci ← softmax(bi)
∀j ∈ Clas. : vj ← squash(

∑
i cj|iûj|i)

∀i ∈ Primary.∀j ∈ Clas. : bj|i ←
bj|i + ûj|i · vj

end
Algorithm 1: Routing algorithm as described in
(Sabour et al., 2017)

The coupling coefficients are generated by ap-
plying the softmax function to the log prior proba-
bilities that primary capsule i should be coupled to
classification capsule j. The probability is higher
when the primary capsule’s prediction vector is
more similar to the classification capsule’s output.
Therefore, primary capsules try to predict the out-
put of the capsule in the subsequent layer. Since vj
is partially determined by uj|i, their similarity in-
creases for the next iteration. Thus a convergence
is guaranteed.

This routing algorithm is superior regarding
its ability to combine and generalize information
compared to primitive routing algorithms such as
max-pooling layers, as the latter only stores the
most prominent features while the others are ig-
nored. This leads to CNNs having more difficulty
differentiating between classes with highly similar
features (Sabour et al., 2017), but since most label
combinations appear rarely and categories often
share features with their parents, it is a desirable
property to exploit for hierarchical classification.

Architecture: The HMC task is converted to a

300

Primary Capsules Classification
 Capsules

Flatten
Routing

Embedding
Convolutional

Reshape
h

d x q

s

1q

Cp Cc

 =d-dimension

l

l

Figure 1: Architecture of our capsule network with d
being the dimensionality of a capsule’s output.

multi-label classification task using the hierarchy
of labels: All explicitly labeled classes must also
include all ancestor labels of the hierarchy. The
architecture of our capsule network is shown in
Figure 1 and consists of four layers. We designed
a minimal capsule network, similar to CapsNet-
1 in (Xiao et al., 2018) in order to benefit from
capsules and dynamic routing while maintaining
high comparability to a similarly simple CNN. In
our network, the primary capsules take as input the
output created by a preceding convolutional layer.
For each classification capsule, the routing algo-
rithm is then used to cluster the outputs of all cp
primary capsules. The pseudo-probability ||vj || is
then assigned to the category associated with the
effective classification capsule. We follow Sabour
et al. (2017), and use their margin loss function.

Leveraging Label Co-occurrence: We fur-
ther follow the layer weight initialization in-
troduced by (Kurata et al., 2016) in order to
leverage label co-occurrences during the learn-
ing process of a neural network. Since label
co-occurrences such as {Fiction, Mystery & Sus-
pense} or {Fiction, Fantasy} naturally occur in
HMC because of parent-child relationships be-
tween categories, we aim to bias the learning pro-
cess of the capsule network in respect to the co-
occurrences in the dataset by initializing W with
label co-occurrences. Weights between a primary
capsule and the co-occurring classification cap-
sules are initialized using a uniform distribution
while all other values are set to zero.

Label Correction: A classifier may assign la-
bels to classes that do not conform with the under-
lying hierarchy of the categories as the activation
function as well as the routing algorithm look at
each category separately. For instance, if the cap-
sule network only assigns the label Fantasy then
the prediction is inconsistent with the hierarchy as
its parent Fiction has not been labeled. Inconsis-
tencies with respect to the hierarchical structure of
categories are corrected by a post-processing step.



326

We applied three different ways of label correc-
tion: Correction by extension, removal and thresh-
old. The former two systematically add parent or
remove parentless labels to make the prediction
consistent (Baker and Korhonen, 2017). There-
fore, the first method adds Fiction to the predic-
tions while the second one removes the prediction
Fantasy (and all its children) in its entirety. Cor-
rection by threshold calculates the average confi-
dence of all ancestors for an inconsistent predic-
tion and adds them if above the threshold (Kir-
itchenko et al., 2005).

4 Experiments

Datasets: We test our hypothesis on two differ-
ent datasets with fundamentally different proper-
ties, the BlurbGenreCollection2 (BGC), and the
WOS-11967 (Web of Science, Kowsari et al.,
2017).

The BGC dataset consists of book blurbs (short
advertising texts) and several book-related meta-
information such as author, date of publication,
number of pages, and so on. Each blurb is cate-
gorized into one or multiple categories in a hier-
archy. With their permission, we crawled the Pen-
guin Random House website and performed clean-
ing steps, such as: removing categories that do
not rely on content (e.g. audiobooks), and remov-
ing category combinations that appear less than
five times. The dataset follows the well-known
dataset properties as described in (Lewis et al.,
2004): Firstly, at least one writing-genre is as-
signed to each book and secondly, every ancestor
of a book’s label is assigned to it as well. It is
important to note that the most specific genre of a
book does not have to be a leaf. For instance, the
most specific category of a book could be Chil-
dren’s Books, although Children’s Books has fur-
ther sub-genres, such as Middle Grade books. Fur-
thermore, in this dataset, each child-label has ex-
actly one parent, forming all-together a hierarchy
in form of a forest. Nonetheless, the label distribu-
tion remains highly unbalanced and diverse, with a
total of 1, 342 different label co-occurrences from
a pool of 146 different labels arranged on 4 hierar-
chy levels.

The WOS dataset consists of abstracts of pub-
lished papers from the Web of Science. The hi-

2The dataset is available at https://www.inf.
uni-hamburg.de/en/inst/ab/lt/resources/
data/blurb-genre-collection.html

BGC WOS-11967
Number of texts 91,892 11,967
Average number of tokens 93.56 125.90
Total number of classes 146 40
Classes on level 1;2;3;4 7; 46; 77; 16 7; 33; -; -
Average number of labels 3.01 2
Total number of label co-occurrences 1342 33
Co-occurrence entropy (normalized) 0.7345 0.9973
Samples per category standard deviation 4374.19 529.43

Table 1: Quantitative characteristics of both datasets.
Normalized entropy is the quotient between entropy
and the log of co-occurrence cardinality.

erarchy of the WOS dataset is shallower, but sig-
nificantly broader, with fewer classes in total. In
addition to having only as many co-occurrences as
leaf nodes, measuring the entropy of label com-
binations shows that the dataset is unnaturally bal-
anced – a consequence of the dataset’s requirement
to assign exactly two labels to each example. Ta-
ble 1 shows further important quantitative charac-
teristics of both datasets.

Feature selection: Since CNNs and our cap-
sule network require a fixed input length, we limit
the texts to the first 100 tokens, which covers the
complete input for over 90% of the dataset. We
remove stop-words, most punctuation and low-
frequency words (< 2). For the BGC, we kept spe-
cial characters like exclamation marks as they can
be frequently found in blurbs that have a younger
target audience and hence could provide useful in-
formation. We are using pre-trained fastText em-
beddings3 provided by Bojanowski et al. (2017)
and adjust them during training.

Baselines: We employ a one-vs-rest classifi-
cation strategy using one SVM (Cortes and Vap-
nik, 1995) for each label with linear kernels and
tf-idf values in a bag-of-words fashion as feature
vectors. Also, we apply the CNN as described by
Kim (2014) and an LSTM with recurrent dropout
(Gal and Ghahramani, 2016).4 For all experiments
we use the initialization strategy as described in
(Baker and Korhonen, 2017), which takes label co-
occurrences for initializing the weights of the final
layer, and the label correction method by thresh-

3https://fasttext.cc/docs/en/
pretrained-vectors.html

4All neural networks use the Adam optimizer, a dropout
probability of 0.5 and a minibatch size of 32. LSTM and
CNN use the binary cross entropy loss. Further hyperparame-
ters for (BGC, WOS) – CNN: filters: (1500, 1000), windows:
{3,4,5}, l. rate: (0.0005, 0.001), l. decay: (0.9, 1), epochs:
(30, 20); LSTM: hidden units: (1500, 1000), l. rate: (0.005,
0.001), epochs: (15, 25); capsule network: num. capsules:
(55, 32), windows: (90, 50), primary/class. cap. dim.: 8/16,
l. rate: (0.001, 0.002), l. decay: (0.4, 0.95), epochs: 4

https://www.inf.uni-hamburg.de/en/inst/ab/lt/resources/data/blurb-genre-collection.html
https://www.inf.uni-hamburg.de/en/inst/ab/lt/resources/data/blurb-genre-collection.html
https://www.inf.uni-hamburg.de/en/inst/ab/lt/resources/data/blurb-genre-collection.html
https://fasttext.cc/docs/en/pretrained-vectors.html
https://fasttext.cc/docs/en/pretrained-vectors.html


327

BGC WOS-11967
Model Recall Precision F1 Subset Acc. Recall Precision F1 Subset Acc.
SVM 61.11 85.37 71.23 35.79 72.43 89.84 80.20 56.47
CNN 64.75± 0.41 83.87± 0.09 73.08± 0.27 37.26± 0.52 84.06± 0.93 91.68± 1.00 87.71± 0.58 75.16± 1.66

LSTM 69.12± 1.24 75.49± 3.54 72.16± 1.01 37.99± 1.52 83.78± 1.69 87.56± 1.04 85.63± 1.22 76.80± 2.15
Caps. Network 71.73± 0.63 77.21± 0.54 74.37± 0.35 37.70± 0.68 80.67± 1.27 82.75± 2.42 81.69± 0.70 64.97± 0.49

Table 2: All results with their corresponding 95% confidence intervals, measured across three runs.

old with a confidence value of 0.2.5 The dataset
is split into 64% train, 16% validation and 20%
test. For evaluation, we measure subset accuracy,
micro-averaged recall, precision, and F1 as defined
in (Sorower, 2010; Silla and Freitas, 2011).

5 Results

Results are shown in Table 2. Regarding the BGC
dataset, the capsule network yields the highest F1
and recall, the SVM the highest precision, while
the LSTM showed the best result in subset accu-
racy. On WOS, all neural network architectures
beat the baseline SVM model by a substantial mar-
gin. However, both, the SVM and the capsule net-
work, are substantially outperformed by the CNN
and LSTM. In Figure 2 we further observe a per-
formance decline for deeper levels of the hierar-
chy. On BGC, the capsule network performs best
on every level of the hierarchy with an increasing
margin for more specific labels.

5.1 Identification of label co-occurrences

We argue that the pronounced performance differ-
ence between the datasets is due to the ability of
capsules to handle label combinations better than
the CNN and LSTM. We observe, as shown in Fig-
ure 4, that capsule networks are beneficial for ex-
amples with many label assignments. While the
capsule network performs worse on BGC for a la-
bel set cardinality of 1 and 2, it starts to perform
better at a cardinality of 3 and almost doubles the
F1 of all baselines for 9 and 11. The number of ex-
amples decreases exponentially with the label set
cardinality, so that the ability of networks to com-
bine labels is becoming increasingly important.

In contrast, in the WOS dataset, exactly one
parent-child label combination is assigned to each
example, resulting in a label set cardinality of two
for the whole dataset. There are comparably few
label combinations, which occur with a high fre-
quency in the dataset (cf. Table 1). The benefit of
capsules can thus not apply here.

5These options consistently performed well in prelimi-
nary experiments.

(a) BGC

(b) WOS

Figure 2: Scores on different levels for the BGC (a) and
WOS (b). The lines are the cumulative scores.

To verify this hypothesis, we conduct a fur-
ther test exclusively on BGC examples with label
combinations that have not been observed during
training (5, 943 samples). As shown in Table 3,
the capsule network again achieves the highest
F1 score, outperforming the other networks, es-
pecially in terms of recall. In order to create hi-
erarchical inconsistencies in the WOS dataset, we
test two modifications on the training data while
the test data is kept the same: a) 50% of all child
labels are removed, and b) for each sample, either
the child or the parent label is kept. Results of this
study are shown in Table 4. Removing 50% of the
children labels results in the capsule network be-
ing more similar to the CNN and LSTM in terms
of subset accuracy. However, for the second modi-
fication, where label combinations are completely
omitted for training, the capsule network signifi-
cantly outperforms both networks. Figure 3 shows
that different primary capsules are routed to the
classification capsule representing the parent cat-



328

Fiction

Mystery & Suspense

Fantasy

Humor

0.58 0.25 0.69 0.21 0.18 0.57 0.58 0.24 0.71 0.58 0.54 0.66 0.14 0.45 0.15 0.62

0.23 0.58 0.16 0.6 0.67 0.27 0.25 0.57 0.15 0.29 0.26 0.13 0.74 0.37 0.69 0.22

0.1 0.09 0.09 0.1 0.08 0.09 0.1 0.12 0.09 0.07 0.13 0.14 0.07 0.1 0.07 0.1

0.08 0.08 0.06 0.09 0.07 0.07 0.07 0.06 0.06 0.06 0.07 0.07 0.05 0.09 0.09 0.07

1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16

Fiction

Mystery & Suspense

Fantasy

Humor

0.59 0.42 0.7 0.49 0.25 0.61 0.61 0.3 0.7 0.66 0.31 0.43 0.46 0.27 0.2 0.62

0.11 0.2 0.1 0.16 0.26 0.13 0.13 0.16 0.1 0.11 0.1 0.14 0.17 0.12 0.24 0.12

0.2 0.2 0.13 0.24 0.28 0.17 0.16 0.44 0.12 0.14 0.5 0.31 0.24 0.48 0.26 0.17

0.09 0.18 0.08 0.11 0.21 0.09 0.09 0.1 0.08 0.09 0.1 0.12 0.13 0.13 0.3 0.09

Figure 3: Connection strength between primary capsules (x-axis) and classification capsules (y-axis) for two BGC
samples: top belonging to {Fiction, Mystery & Suspense} and bottom to {Fiction, Fantasy} with Fiction being
their parent category. A reduced number of primary capsules and categories was used for visualization purposes.

1 2 3 4 5 6 7 8 9 11
0

10
20
30
40
50
60
70
80
90

100

1

10

100

1000

10000

CNN
LSTM
Capsule
Frequency

Label set cardinality

F
1-

S
co

re

F
re

qu
en

cy

Figure 4: Test F1-scores of classifiers for different label
cardinalities.

BGC, unobserved R P F1
CNN 46.21 68.95 55.34
LSTM 45.79 60.48 52.13
Capsule Net. 53.30 61.21 56.98

Table 3: Performance results on the test set with label
combinations not seen during training.

egory Fiction than to the children. Some primary
capsules learn features for specific children cate-
gories. For instance Primary Capsule 5 is not in-
clined to any category for the bottom sample be-
cause of missing features for Mystery & Suspense
in this sample. Some capsules distribute their con-
nection strength to the parent and child category
evenly, likely due to the categories’ similarities.
To combine encoded features for each category
separately while using the softmax to ensure that
primary capsules encapsulate features of specific
categories appears to be the main cause of these
significant performance differences. These obser-
vations also align with previous work, especially
see (Sabour et al., 2017; Zhao et al., 2018).

Modified WOS 50% Child
Labels

Either Parent
or Child

F1 Acc F1 Acc
CNN 75.15 36.28 41.93 16.36
LSTM 73.00 35.09 38.74 5.28
Capsule Net. 71.59 35.21 67.23 34.27

Table 4: Results on the modified WOS training data.
Firstly, by removing 50% of the children labels and sec-
ondly, by removing label combinations completely.

6 Conclusion

This first application of capsule networks to the
HMC task indicates that the beneficial properties
of capsules can be successfully utilized. By asso-
ciating each category in the hierarchy with a sep-
arate capsule, as well as using a routing algorithm
to combine in capsules encoded features, cap-
sule networks have shown to identify and combine
categories with similar features more accurately
than the baselines. The introduced dataset, the
BlurbGenreCollection (BGC), is compiled from a
real-world scenario and is indicative of the promis-
ing properties of capsule networks for HMC tasks,
since most hierarchically organized datasets con-
sist of substantial amounts of rare label combina-
tions, where algorithms are very likely to be con-
fronted with unseen label combinations.

This initial attempt shows the advantage of sim-
plistic capsule networks over traditional methods
for HMC. Future architectures could for example
employ a cascade of capsule layers with each cap-
sule in one layer being associated to a category of
one specific level in the hierarchy.



329

References
Simon Baker and Anna Korhonen. 2017. Initializ-

ing neural networks for hierarchical multi-label text
classification. In Proceedings of the 16th Biomedi-
cal Natural Language Processing Workshop, pages
307–315, Vancouver, Canada.

Piotr Bojanowski, Edouard Grave, Armand Joulin, and
Tomas Mikolov. 2017. Enriching word vectors with
subword information. Transactions of the Associa-
tion for Computational Linguistics, 5(1):135–146.

Ricardo Cerri, Rodrigo C. Barros, and André C.P.L.F.
de Carvalho. 2014. Hierarchical multi-label classifi-
cation using local neural networks. Journal of Com-
puter and System Sciences, 80(1):39 – 56.

Corinna Cortes and Vladimir Vapnik. 1995. Support-
vector networks. Machine learning, 20(3):273–297.

Yarin Gal and Zoubin Ghahramani. 2016. A theo-
retically grounded application of dropout in recur-
rent neural networks. In Advances in neural infor-
mation processing systems 2016, pages 1019–1027,
Barcelona, Spain.

Ian Goodfellow, Yoshua Bengio, and Aaron Courville.
2016. Deep Learning. MIT Press. http://www.
deeplearningbook.org.

Geoffrey E. Hinton, Alex Krizhevsky, and Sida D.
Wang. 2011. Transforming auto-encoders. In Inter-
national Conference on Artificial Neural Networks,
pages 44–51, Espoo, Finland.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Yoon Kim. 2014. Convolutional neural networks
for sentence classification. In Proceedings of the
2014 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 1746–1751,
Doha, Qatar.

Svetlana Kiritchenko, Stan Matwin, and A. Fazel
Famili. 2005. Functional annotation of genes using
hierarchical text categorization. In BioLINK SIG:
Linking Literature, Information and Knowledge for
Biology, Detroit, MI, USA. Workshop track.

Kamran Kowsari, Donald E. Brown, Mojtaba Hei-
darysafa, Kiana Jafari Meimandi, Matthew S. Ger-
ber, and Laura E. Barnes. 2017. HDLTex: Hierar-
chical deep learning for text classification. In IEEE
International Conference on Machine Learning and
Applications, pages 364–371, Cancún, Mexico.

Gakuto Kurata, Bing Xiang, and Bowen Zhou. 2016.
Improved neural network-based multi-label classifi-
cation with better initialization leveraging label co-
occurrence. In Proceedings of the 2016 Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, pages 521–526, New Orleans,
LA, USA.

David D. Lewis, Yiming Yang, Tony G. Rose, and Fan
Li. 2004. RCV1: A new benchmark collection for
text categorization research. Journal of Machine
Learning Research, 5(Apr):361–397.

Dai Quoc Nguyen, Thanh Vu, Tu Dinh Nguyen,
Dat Quoc Nguyen, and Dinh Q. Phung. 2019. A
capsule network-based embedding model for knowl-
edge graph completion and search personalization.
In Proceedings of the 2019 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Minneapolis, MN, USA.

Prabod Rathnayaka, Supun Abeysinghe, Chamod
Samarajeewa, Isura Manchanayake, and Malaka
Walpola. 2018. Sentylic at IEST 2018: Gated re-
current neural network and capsule network based
approach for implicit emotion detection. In Pro-
ceedings of the 9th Workshop on Computational Ap-
proaches to Subjectivity, Sentiment and Social Me-
dia Analysis, pages 254–259, Brussels, Belgium.

Sara Sabour, Nicholas Frosst, and Geoffrey E. Hinton.
2017. Dynamic routing between capsules. In Ad-
vances in Neural Information Processing Systems
30, pages 3856–3866, Long Beach, CA, USA.

Carlos N. Silla and Alex A. Freitas. 2011. A survey
of hierarchical classification across different appli-
cation domains. Data Mining and Knowledge Dis-
covery, 22(1-2):31–72.

Mohammad S. Sorower. 2010. A literature survey on
algorithms for multi-label learning. Oregon State
University, Corvallis, OR, USA.

Saurabh Srivastava, Prerna Khurana, and Vartika
Tewari. 2018. Identifying aggression and toxicity in
comments using capsule network. In Proceedings of
the First Workshop on Trolling, Aggression and Cy-
berbullying (TRAC-2018), pages 98–105, Santa Fe,
NM, USA.

Aixin Sun and Ee-Peng Lim. 2001. Hierarchical text
classification and evaluation. In Proceedings of the
2001 IEEE International Conference on Data Min-
ing, ICDM ’01, pages 521–528, San Jose, CA, USA.

Yequan Wang, Aixin Sun, Jialong Han, Ying Liu, and
Xiaoyan Zhu. 2018. Sentiment analysis by capsules.
In Proceedings of the 2018 World Wide Web Confer-
ence, pages 1165–1174, Lyon, France.

Congying Xia, Chenwei Zhang, Xiaohui Yan,
Yi Chang, and Philip Yu. 2018. Zero-shot user
intent detection via capsule neural networks. In
Proceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing, pages
3090–3099, Brussels, Belgium.

Liqiang Xiao, Honglun Zhang, Wenqing Chen,
Yongkun Wang, and Yaohui Jin. 2018. MCapsNet:
Capsule network for text with multi-task learning.
In Proceedings of the 2018 Conference on Empiri-
cal Methods in Natural Language Processing, page
4565–4574, Brussels, Belgium.

https://www.aclweb.org/anthology/W17-2339
https://www.aclweb.org/anthology/W17-2339
https://www.aclweb.org/anthology/W17-2339
https://aclweb.org/anthology/Q17-1010
https://aclweb.org/anthology/Q17-1010
http://www.sciencedirect.com/science/article/pii/S0022000013000718
http://www.sciencedirect.com/science/article/pii/S0022000013000718
https://papers.nips.cc/paper/6241-a-theoretically-grounded-application-of-dropout-in-recurrent-neural-networks.pdf
https://papers.nips.cc/paper/6241-a-theoretically-grounded-application-of-dropout-in-recurrent-neural-networks.pdf
https://papers.nips.cc/paper/6241-a-theoretically-grounded-application-of-dropout-in-recurrent-neural-networks.pdf
http://www.deeplearningbook.org
http://www.deeplearningbook.org
https://link.springer.com/chapter/10.1007/978-3-642-21735-7_6
https://www.aclweb.org/anthology/D14-1181
https://www.aclweb.org/anthology/D14-1181
http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.68.5824
http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.68.5824
https://ieeexplore.ieee.org/document/8260658
https://ieeexplore.ieee.org/document/8260658
https://www.aclweb.org/anthology/N16-1063
https://www.aclweb.org/anthology/N16-1063
https://www.aclweb.org/anthology/N16-1063
https://www.aclweb.org/anthology/W18-6237
https://www.aclweb.org/anthology/W18-6237
https://www.aclweb.org/anthology/W18-6237
https://papers.nips.cc/paper/6975-dynamic-routing-between-capsules.pdf
https://www.aclweb.org/anthology/W18-4412
https://www.aclweb.org/anthology/W18-4412
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.19.6575&rep=rep1&type=pdf
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.19.6575&rep=rep1&type=pdf
https://dl.acm.org/citation.cfm?id=3186015
https://www.aclweb.org/anthology/D18-1348
https://www.aclweb.org/anthology/D18-1348
https://www.aclweb.org/anthology/D18-1486
https://www.aclweb.org/anthology/D18-1486


330

Ningyu Zhang, Shumin Deng, Zhanlin Sun, Xi Chen,
Wei Zhang, and Huajun Chen. 2018. Attention-
based capsule networks with dynamic routing for re-
lation extraction. In Proceedings of the 2018 Con-
ference on Empirical Methods in Natural Language
Processing, page 986–992, Brussels, Belgium.

Wei Zhao, Jianbo Ye, Min Yang, Zeyang Lei, Suofei
Zhang, and Zhou Zhao. 2018. Investigating capsule
networks with dynamic routing for text classifica-
tion. In Proceedings of the 2018 Conference on Em-
pirical Methods in Natural Language, pages 3110 –
3119, Brussels, Belgium.

https://www.aclweb.org/anthology/D18-1120
https://www.aclweb.org/anthology/D18-1120
https://www.aclweb.org/anthology/D18-1120
https://www.aclweb.org/anthology/D18-1350
https://www.aclweb.org/anthology/D18-1350
https://www.aclweb.org/anthology/D18-1350

