



















































Dependency-Based Bracketing Transduction Grammar for Statistical Machine Translation


Coling 2010: Poster Volume, pages 1185–1193,
Beijing, August 2010

Dependency-Based Bracketing Transduction Grammar
for Statistical Machine Translation

Jinsong Su, Yang Liu, Haitao Mi, Hongmei Zhao, Yajuan Lü, Qun Liu
Key Laboratory of Intelligent Information Processing

Institute of Computing Technology
Chinese Academy of Sciences

{sujinsong,yliu,htmi,zhaohongmei,lvyajuan,liuqun}@ict.ac.cn
Abstract

In this paper, we propose a novel
dependency-based bracketing transduc-
tion grammar for statistical machine
translation, which converts a source sen-
tence into a target dependency tree. Dif-
ferent from conventional bracketing trans-
duction grammar models, we encode tar-
get dependency information into our lex-
ical rules directly, and then we employ
two different maximum entropy models
to determine the reordering and combi-
nation of partial dependency structures,
when we merge two neighboring blocks.
By incorporating dependency language
model further, large-scale experiments on
Chinese-English task show that our sys-
tem achieves significant improvements
over the baseline system on various test
sets even with fewer phrases.

1 Introduction

Bracketing transduction grammar (BTG) (Wu,
1995) is an important subclass of synchronous
context free grammar, which employs a special
synchronous rewriting mechanism to parse paral-
lel sentence of both languages.

Due to the prominent advantages such as the
simplicity of grammar and the good coverage of
syntactic diversities in different language pairs,
BTG has attracted increasing attention in statis-
tical machine translation (SMT). In flat reorder-
ing model (Wu, 1996; Zens et al., 2004), which
assigns constant reordering probabilities depend-
ing on the language pairs, BTG constraint proves
to be very effective for reducing the search space
of phrase reordering. To pursue a better method
to predict the order between two neighboring

blocks1, Xiong et al. (2006) present an enhanced
BTG with a maximum entropy (ME) based re-
ordering model. Along this line, source-side syn-
tactic knowledge is introduced into the reorder-
ing model to improve BTG-based translation (Se-
tiawan et al., 2007; Zhang et al., 2007; Xiong et
al., 2008; Zhang and Li, 2009). However, these
methods mainly focus on the utilization of source
syntactic knowledge, while ignoring the modeling
of the target-side syntax that directly influences
the translation quality. As a result, how to ob-
tain better translation by exploiting target syntac-
tic knowledge is somehow neglected. Thus, we
argue that it is important to model the target-side
syntax in BTG-based translation.

Recently, modeling syntactic information on
the target side has progressed significantly. De-
pending on the type of output, these models can
be divided into two categories: the constituent-
output systems (Galley et al., 2006; Zhang et
al., 2008; Liu et al., 2009) and dependency-
output systems (Eisner, 2003; Lin, 2004; Ding
and Palmer, 2005; Quirk et al., 2005; Shen et
al., 2008). Compared with the constituent-output
systems, the dependency-output systems provide a
simpler platform to capture the target-side syntac-
tic information, while also having the best inter-
lingual phrasal cohesion properties (Fox, 2002).
Typically, Shen et al. (2008) propose a string-to-
dependency model, which integrates the target-
side well-formed dependency structure into trans-
lation rules. With the dependency structure, this
system employs a dependency language model
(LM) to exploit long distance word relations, and
achieves a significant improvement over the hier-
archical phrase-based system (Chiang, 2007). So

1A block is a bilingual phrase without maximum length
limitation.

1185



we think it will be a promising way to integrate the
target-side dependency structure into BTG-based
translation.

In this paper, we propose a novel dependency-
based BTG (DepBTG) for SMT, which represents
translation in the form of dependency tree. Ex-
tended from BTG, our grammars operate on two
neighboring blocks with target dependency struc-
ture. We integrate target syntax into bilingual
phrases and restrict target phrases to the well-
formed structures inspired by (Shen et al., 2008).
Then, we adopt two ME models to predict how to
reorder and combine partial structures into a target
dependency tree, which gives us access to captur-
ing the target-side syntactic information. To the
best of our knowledge, this is the first effort to
combine the translation generation with the mod-
eling of target syntactic structure in BTG-based
translation.

The remainder of this paper is structured as fol-
lows: In Section 2, we give brief introductions to
the bases of our research: BTG and dependency
tree. In Section 3, we introduce DepBTG in detail.
In Section 4, we further illustrate how to create
two ME models to predict the reordering and de-
pendency combination between two neighboring
blocks. Section 5 describes the implementation
of our decoder. Section 6 shows our experiments
on Chinese-English task. Finally, we end with a
summary and future research in Section 7.

2 Background

2.1 BTG

BTG is a special case of synchronous context free
grammar. There are three rules utilized in BTG:

A → [A1, A2] (1)

A → 〈A1, A2〉 (2)

A → x/y (3)

where the reordering rules (1) and (2) are used
to merge two neighboring blocks A1 and A2 in
a straight or inverted order, respectively. The lex-
ical rule (3) is used to translate the source phrase
x into the target phrase y.

���� �������	
���	� ���	���	�	�� ��	�� ������������
Figure 1: The dependency tree for sentence The
UN will provide abundant financial aid to Haiti
next week.

2.2 Dependency Tree

In a given sentence, each word depends on a par-
ent word, except for the root word. The depen-
dency tree for a given sentence reflects the long
distance dependency and grammar relations be-
tween words. Figure 1 shows an example of a de-
pendency tree, where a black arrow points from a
child word to its parent word.

Compared with constituent tree, dependency
tree directly models semantic structure of a sen-
tence in a simpler form. Thus, it provides a desir-
able platform for us to utilize the target-side syn-
tactic knowledge.

3 Dependency-based BTG

3.1 Grammars

In this section, we extend the original BTG into
DepBTG. The rules of DepBTG, which derive
from that of BTG, merge blocks with target de-
pendency structure into a larger one. These rules
take the following forms:

Ad → [A1d, A2d]CC (4)

Ad → [A1d, A2d]LA (5)
Ad → [A1d, A2d]RA (6)
Ad → 〈A1d, A2d〉CC (7)
Ad → 〈A1d, A2d〉LA (8)
Ad → 〈A1d, A2d〉RA (9)

Ad → x/y (10)
where A1d and A

2
d represent two neighboring

blocks with target dependency structure. Rules
(4)∼(9) are used to determine the reordering and
combination of two dependency structures, when

1186



�������� � ���  !
� ��� ""#�$�$

�
%$���&$�'�$� �(#�$�$%$���&$�'�$�

%$���&$�'�$� �(#�$�$ �������� %$���&$�'�$� �(#�$�$
�������� %$���&$�'�$� �(#�$�$)*(+$�, �&� -! )*

(+$�,�������� %$���&$�'�$� �(#�$�$

�(

Figure 2: Dependency operations on the neigh-
boring dependency structures. CC = coordinate
concatenate, LA = left adjoining, and RA = right
adjoining.

we merge two neighboring blocks. Rule (10) is
applied to generate bilingual phrase (x , y) with
target dependency structure learned from train-
ing corpus. To distinguish the rules with differ-
ent functions, the rules (4)∼(9) and rule (10) are
named as merging rules and lexical rule, respec-
tively.

Specifically, we first merge the neighboring
blocks in the straight order using rules (4)∼(6) or
in the inverted order using rules (7)∼(9). Then,
according to different merging rules, we conduct
some operations to combine the corresponding de-
pendency structures in the target order: coordinate
concatenate (CC), left adjoining (LA) and right
adjoining (RA).

To clearly illustrate our operations, we show the
process of applying three dependency operations
to build larger structures in Figure 2. Adopting
rule (4), the dependency structures “( ( financial
) aid )”1 and “( to ( Haiti ) )” can be combined
into a larger one consisting of two sibling subtrees
(see Figure 2(a)). Adopting rule (5), we can adjoin
the left dependency structure “( abundant )” to the
leftmost sub-root of the right dependency struc-

1We use the lexicon dependency grammar (Hellwig,
2006) to express the projective dependency tree. Using this
grammar, the words in the brackets are defined as the child
words depending on the parent word outside the brackets.

ture “( ( financial ) aid ) ( to ( Haiti ) )” (see Figure
2(b)). Adopting rule (6), we can include the right
dependency structure “( ( abundant ) ( financial )
aid ) ( to ( Haiti ) )” as a child of the rightmost sub-
root of the left dependency structure “( provide )”
(see Figure 2(c)). In a similar way, rules (7)∼(9)
are applied to deal with two partial structures in
the inverted order.

3.2 Well-Formed Dependency Structures

As illustrated in the previous sub section, the
rules of DepBTG operate on the blocks with tar-
get dependency structure. Following (Shen et al.,
2008), we restrict the target phrases to the well-
formed dependency structures. The main differ-
ence is that we use more relaxed constraints to
extract more bilingual phrases with rational struc-
ture. Take a sentence S = w1w2 ...wn for exam-
ple, we denote the parent word ID of word wi with
di, and show the definitions of structures as fol-
lows.

Defination 1 A dependency structure di ...j is
fixed on head h, where h ∈ [i, j], if and only if
it meets the following conditions

• dh /∈ [i , j ]
• ∀k ∈ [i , j ] and k 6= h, dk ∈ [i , j ]
• ∀k ∈ [i , j ], dk = h or dk ∈ [i , j ]

Defination 2 A dependency structure di ...dj is
floating with children C, for a non-empty set
C ⊆ {i...j}, if and only if it meets the following
conditions

• ∃h /∈ [i , j ], s.t .∀k ∈ C , dk = h
• ∀k ∈ [i , j ] and k /∈ C , dk ∈ [i , j ]
• ∀k /∈ [i , j ], dk /∈ [i , j ] or dk = cl
or dk = cr

where cl and cr represent the IDs of the leftmost
and rightmost words in the set C, respectively.
Note that the underline indicates the difference
between our definition and that of (Shen et al.,
2008). In our model, we regard the floating struc-
ture, which is not complete on its boundary sub-
roots, as an useful structure, since it will become
a complete constituent by combining it with other
partial structures. For example, the dependency

1187



./0 .10.20345657468649 345657468649:;<64:4 5=>:?==@ :;<64:4 5=>:?==@
Figure 3: (a) A fixed structure and (b) (c) two
floating structures. Note that (c) is ill-formed in
(Shen et al., 2008).

structures shown in Figure 3 are all well-formed
structures. However, according to the definitions
of (Shen et al., 2008), 3(c) is ill-formed because
aid does not include its leftmost child word abun-
dant in the structure.

4 ME Models for Merging Rules

4.1 The Models

A simple way to estimate the probabilities of the
merging rules is to adopt maximum likelihood es-
timation to obtain the conditional probabilities.
However, this method is not applicable to merging
rules because the dependency structures become
larger and larger during decoding, which are very
sparse in the corpus.

Inspired by MEBTG translation (Xiong et al.,
2006), which considers phrase reordering as a
classification problem, we model the reordering
and combination of two neighboring dependency
structures based on the ME principle. Owing to
data sparseness and the complexity of multi-class
classification, we establish two ME models rather
than an unified ME model: one for the reorder-
ing between blocks, called reordering model; the
other for the dependency operations on the corre-
sponding dependency structures, called operation
model.

Thus, according to the ME scheme, we decom-
pose the probability Ω of each merging rule into

Ω = pθ1(o|A1d, A2d) · pθ2(d|A1d, A2d)

=
exp(Σiθ1ih1i(o,A

1
d, A

2
d))

Σo exp(Σiθ1ih1i(o,A1d, A
2
d))

·

exp(Σjθ2jh2j(d,A
1
d, A

2
d))

Σd exp(Σjθ2jh2j(d,A
1
d, A

2
d))

where the functions h1i ∈ {0, 1} are the fea-
tures of the ME-based reordering model,

θ1i are the corresponding weights, and o ∈
{straight, inverted}. Similarly, the func-
tions h2j ∈ {0, 1} and the weights θ2j are
trained for the ME-based operation model, and
d ∈ {CC,LA,RA}.

4.2 Example Extraction

To train the ME models, we extract examples
from a string-to-dependency word-aligned corpus
during the process of bilingual phrases extraction
(Koehn et al., 2005), and then collect various fea-
tures for the models.

For the reordering model, we adopt the method
of (Xiong et al., 2006) to extract reordering exam-
ples. Due to the limit of space, we skip the details
of this method.

For the operation model, given an operation
training example consisting of two neighboring
dependency structures: the left structure dl and the
right structure dr , we firstly classify it into differ-
ent categories by the dependency relation between
dl and dr:

• if dl and dr have the same parent, the cate-
gory of the example is CC;

• if dl depends on the leftmost sub-root of dr ,
the category of the example is LA;

• if dr depends on the rightmost sub-root of dl,
the category of the example is RA.

For instance, Figure 4 shows an operation exam-
ple with RA operation, where the sub-root word
week of dr depends on the rightmost sub-root
word provide of dl.

Then, we collect various features from the fol-
lowing nodes: the rightmost sub-root of dl, and
its rightmost child node; the leftmost sub-root of
dr, and its leftmost child node. Here, we specu-
late that these nodes may carry useful information
for the dependency combination of the two struc-
tures, since they locate nicely at the boundary sub-
trees of dl and dr. For simplicity, we refer to these
nodes as the feature nodes of the example. Let’s
revisit Figure 4, the feature nodes of the example
are marked with dashed ellipses. The rightmost
sub-root word of dl is provide, and its rightmost
child word is to; The leftmost sub-root word of dr
is week, and its leftmost child word is next.

1188



Type Name Description

Lexical Features

Wlh(dr) The leftmost sub-root word of dr
Wrh(dl) The rightmost sub-root word of dl
Wllc(dr) The leftmost child word of Wlh(dr)
Wrrc(dl) The rightmost child word of Wrh(dl)

POS Features

Plh(dr) The POS of Wlh(dr)
Prh(dl) The POS of Wrh(dl)
Pllc(dr) The POS of Wllc(dr)
Prrc(dl) The POS of Wrrc(dl)

Table 1: Feature categories in the ME-based operation model.

Type Features and Instances

Unigram Features
Wrh(dl) = provide Wrrc(dl) = to Wlh(dr) = week Wllc(dr) = next

Prh(dl)=VV Prrc(dl) = TO Plh(dr) = NN Pllc(dr) = ADJ

Bigram Features

Wrh(dl) Wlh(dr) = provide week Wrh(dl) Plh(dr) = provide NN
Prh(dl) Wlh(dr) = VV week Prh(dl) Plh(dr) = VV NN

Wrh(dl) Wllc(dr) = provide next Wrh(dl) Pllc(dr) = provide ADJ
Prh(dl) Wllc(dr) = VV next Prh(dl) Pllc(dr) = VV ADJ
Wrrc(dl) Wlh(dr) = to week Wrrc(dl) Plh(dr) = to NN
Prrc(dl) Wlh(dr) = TO week Prrc(dl) Plh(dr) = TO NN

Table 2: ME operation features and instances of the example shown in Figure 4.

abundant/ADJ

to/TOaid/NN

Haiti/NR

week/NN

next/ADJfinancial/ADJ

provide/VV

dA dB
Figure 4: An example with RA category consist-
ing of the neighboring dependency structures dl
and dr . The dashed ellipses denote the feature
nodes of the example, and each node consists of
one word and its corresponding POS tag.

In addition, to keep the number of operation ex-
amples acceptable, we follow (Xiong et al., 2006)
to only extract the smallest one from the examples
with the same feature nodes in each sentence.

4.3 Features

To capture reordering information, we use the
boundary words of bilingual blocks as features,
which are proved to be very effective in (Xiong

et al., 2006).
To capture dependency operation information,

we design two kinds of features on the feature
nodes: the Lexical features and Parts-of-speech
(POS) features. With the POS features, the op-
eration ME model will do exact predicating to the
best of its ability, and then can back off to approxi-
mately predicating if exact predicating fails. Table
1 shows these feature categories in detail.

Furthermore, we also use some bigram features,
since it is generally admitted that the combina-
tion of different features can lead to better per-
formance than unigram features. To better under-
stand our operation features, we continue with the
example shown in Figure 4, listing features and
instances in Table 2.

5 Implementation Details

5.1 Decoder

We develop a CKY-style decoder which uses the
following features: (1) Phrase translation proba-
bilities in two directions, (2) Lexical translation
probabilities in two directions, (3) N-gram LM

1189



score, (4) ME-based reordering model score, (5)
Number of phrases, (6) Number of target words,
(7) ME-based operation model score, (8) Depen-
dency LM scores at word level and POS level
separately, and (9) Discount on ill-formed depen-
dency structures. Here, the former six features are
also used in MEBTG translation.

5.2 Dependency Language Model

Following (Shen et al., 2008), we apply different
tri-gram dependency LMs at word level and POS
level separately to DepBTG translation.

Given a dependency structure, where wh
is the parent word, wL = wl1 ...wln and
wR = wr1 ...wrm are child word sequences
on the left side and right side respectively, the
probability of a tri-gram is computed as follows:

P (wL, wR|wh-as-head)
= P (wL|wh-as-head) · P (wR|wh-as-head)

Here P (wL|wh-as-head) can be decomposed into:

P (wL|wh-as-head)
= P (wl1 |wh-as-head) · P (wl2 |wl1 , wh-as-head)

... · P (wln |wln−1 , wln−2)

where ‘-as-head’ is used to distinguish the head
word from child word in the language model. In
like manner, P (wR|wh-as-head) has a similar cal-
culation method.

5.3 Ill-Formed Dependency Structure

To preserve the good coverage of bilingual
phrases, we keep some bilingual phrases with the
special ill-formed dependency structure. Differ-
ent from the well-formed structures, where all the
children of the sub-roots are complete, these ill-
formed structures are not complete on the chil-
dren of the boundary sub-roots, lacking a well-
formed sub structure on the boundary. We con-
sider them as useful structures with gaps, each of
which can be combined with some well-formed
structures into a larger well-formed one. To re-
duce the search space, we constrain the number of
gap to one on each boundary. During decoding,
we directly substitute the gap in a structure with
another well-formed structure which has the same
direction.

CDEFGHI JE KDIKKLMNHKNJ OGNKNPGKQKGH R HGSKSJID
dT dU

Figure 5: Dependency combination of the ill-
formed dependency structure dl with the right
well-formed dependency structure dr. G denotes
gap and the dotted line denotes the substitution of
the gap G with dr.

For example, there are two dependency struc-
tures in Figure 5: dl is an ill-formed structure with
a right gap, and dr is a well-formed one. Instead
of investigating three operations to combine these
structures, we fill the gap of dl with dr, and then
compute the corresponding score of the RA oper-
ation on the sub structures “( to )” and “( ( disaster
) area )” in the ME-based operation model.

6 Experiment

6.1 Setup

The training corpus1 comes from LDC with
1.54M bilingual sentences (41M Chinese words
and 48M English words). We run GIZA++ (Och
and Ney, 2000) to obtain word alignments with
the heuristic method “grow-diag-final-and”. Then
we parse the English sentences to generate a
string-to-dependency word-aligned corpus using
the parser (Huang et al., 2009). From this cor-
pus, we extract bilingual phrases with dependency
structure. Here, the maximum length of the source
phrase is set to 7. For the n-gram LM, we use
SRILM Toolkits (Stolcke, 2002) to train a 4-gram
LM on the Xinhua portion of the Gigaword cor-
pus. For the dependency LM, we train different
3-gram dependency LMs at word level and POS
level separately on the English side of the training
corpus.

During the process of bilingual phrase extrac-
tion, we collect the neighboring blocks without

1The training corpus consists of six LDC corpora:
LDC2002E18, LDC2003E07, LDC2003E14, Hansards part
of LDC2004T07, LDC2004T08 , LDC2005T06.

1190



any length limitation to obtain examples for two
ME models. For the reordering model, we obtain
about 22.6M examples with monotone order and
4.8M examples with inverted order. For the op-
eration model, we obtain about 5.9M examples
with CC operation, 14.8M examples with LA
operation, and 9.7M examples with RA opera-
tion. After collecting various features from the
examples, we use the ME training toolkit devel-
oped by Zhang (2004) to train ME models with
the following parameters: iteration number i=200
and Gaussian prior g=1.0.

The 2002 NIST MT Evaluation test set is used
as the development set. The 2003 and 2005 NIST
MT Evaluation test sets are our test sets. We per-
form the MERT training (Och, 2003) to tune the
optimal feature weights on the development set.
To run the decoder, we prune the phrase table with
b = 100, prune the chart with n = 50, α = 0.1. See
(Xiong et al., 2006) for the meanings of these pa-
rameters. The translation quality is evaluated by
case-insensitive BLEU-4 metric (Papineni et al.,
2002), as calculated by mteval-v11b.pl.

6.2 Results

Since (Xiong et al., 2006) has made a deep inves-
tigation on the ME-based reordering model, we
mainly focus on the study of the ME-based oper-
ation model. To explore the utility of the various
features in the operation model, we randomly se-
lect about 10K examples from all the operation
examples as held-out data, and use the rest exam-
ples as training data. Then, we train the operation
models on different feature sets and investigate the
performance of models on the held-out data.

Table 3 shows the accuracy rates of the ME op-
eration models using different feature sets. We
find that the bigram feature set provides the most
persuasive evidences and achieves best perfor-
mance than other feature sets.

To investigate the influences of various factors
on the system performance, we carried out exper-
iments on the NIST Chinese-English task with the
following systems:

• MEBTG + all: an MEBTG translation sys-
tem, which uses all bilingual phrases. It is
our baseline system;

Model Accuracy Rate

lexical features 87.614%
POS features 88.232%

unigram features 90.024%
bigram features 93.907%

all features 93.290%

Table 3: The accuracy rates of the ME-based oper-
ation models on the held-out data set using differ-
ent feature sets. Unigram features include lexical
features and POS features, and bigram features are
the combinations of different unigram features.

• MEBTG + filter1: a baseline system, which
uses the bilingual phrases consistent to the
well-formed dependency structures by (Shen
et al., 2008);

• MEBTG + filter2: a baseline system, which
uses the bilingual phrases consistent to our
well-formed dependency structures;

• MEBTG + filter3: a baseline system, which
uses the bilingual phrases consistent to our
well-formed dependency structures and the
special ill-formed dependency structures;

• DepBTG + unigram features: a DepBTG
system which only uses the unigram features
in the ME-based operation model;

• DepBTG + bigram features: a DepBTG sys-
tem which only uses the bigram features in
the ME-based operation model;

• DepBTG + all features: a DepBTG system
which uses all features in the ME-based op-
eration model;

• DepBTG + unigram features + dep LMs:
a DepBTG system with dependency LMs,
where only the unigram features are adopted
in the ME-based operation model;

• DepBTG + bigram features + dep LMs:
a DepBTG system with dependency LMs,
where only the bigram features are adopted
in the ME-based operation model;

• DepBTG + all features + dep LMs: a
DepBTG system with dependency LMs,
where all features are adopted in the ME-
based operation model.

1191



System Type #Bp MT03 MT05

MEBTG

all( baseline ) 81.4M 33.41 32.65
filter1 27.8M 32.17(↓ 1.24) 31.26(↓ 1.39)
filter2 33.7M 32.77(↓ 0.64) 31.93(↓ 0.72)
filter3 58.5M 33.29(↓ 0.12) 32.71(↑ 0.06)

DepBTG

unigram features 59.9M 33.46(↑ 0.05) 32.67(↑ 0.02)
bigram features 59.9M 33.57(↑ 0.16) 32.89(↑ 0.24)

all features 59.9M 33.59(↑ 0.18) 32.86(↑ 0.21)
unigram features + dep LMs 59.9M 33.90(↑ 0.49) 33.29(↑ 0.64)
bigram features + dep LMs 59.9M 34.18(↑ 0.77) 33.58(↑ 0.93)

all features + dep LMs 59.9M 34.10(↑ 0.69) 33.55(↑ 0.90)

Table 4: Experimental results on Chinese-English NIST Task.

Experiment results are summarized in Table 4.
Our baseline system extracts 81.4M bilingual
phrases and achieves the BLEU scores of 33.41
and 32.65 on two test sets separately. Adopt-
ing the constraint of the well-formed structures by
(Shen et al., 2008), we extract 27.8M bilingual
phrases, which lead to great drops in BLEU score:
1.24 points and 1.39 points on two test sets sep-
arately(see Row 3). Using the constraint of our
well-formed structures, the number of extracted
bilingual phrases is 33.7M . We observe the simi-
lar results that the performance drops 0.64 points
and 0.72 points over the baseline system on two
test sets, respectively (see Row 4). Furthermore,
we add some bilingual phrases with the special
ill-formed structure into our phrase table, and the
number of the bilingual phrases in use is 58.5M
accounting up 71.9% of the full phrases. For two
test sets, our system achieves the BLEU scores of
33.29 and 32.71 (see Row 5), which are very close
to the scores of baseline system. Those experi-
mental results demonstrate that phrase coverage
has a great effect on the system performance and
our definitions of the allowed dependency struc-
tures are useful to retain rational bilingual phrases.

Then, by employing the ME-based operation
model and two 3-gram dependency LMs, the
DepBTG system outperforms the MEBTG system
in almost all cases. The experimental results indi-
cate that the dependency LMs are more effective
than the ME-based operation model for DepBTG
system. Especially, using bigram features and de-
pendency LMs, the DepBTG system obtains ab-

solute improvements on two test sets: 0.77 BLEU
points on NIST03 test set and 0.93 BLEU points
on NIST05 test set (see Row 10), which are both
statistically significant at p < 0.05 using the sig-
nificance tester developed by Zhang et al. (2004).

7 Conclusion and Future Work

In this paper, we propose a novel dependency-
based BTG to directly model the syntactic struc-
ture of the translation. Using the bilingual phrases
with target dependency structure, our system em-
ploys two ME models to generate the transla-
tion in line with dependency structure. Based on
the target dependency structure, our system filters
26.4% bilingual phrases (from 81.4M to 59.9M ),
captures the target-side syntactic knowledge by
dependency language models, and achieves sig-
nificant improvements over the baseline system.

There is some work to be done in the future. To
better utilize the syntactic information, we will put
more effort on the study of the dependency LM
with deeper syntactic knowledge. Moreover, we
believe that modeling the syntax of both sides is a
promising method to further improve BTG-based
translation and this will become a study emphasis
in our future research. Finally, inspired by (Tu
et al., 2010), we will replace 1-best dependency
trees with dependency forests to further increase
the phrase coverage.

Acknowledgement

The authors were supported by National Nat-
ural Science Foundation of China, Contracts

1192



60736014 and 60873167, and 863 State Key
Project No.2006AA010108. We thank the anony-
mous reviewers for their insightful comments. We
are also grateful to Zhaopeng Tu, Shu Cai and Xi-
anhua Li for their helpful feedback.

References

Chiang, David. 2007. Hierarchical phrase-based
translation. Computational Linguistics.

Ding, Yuan and Martha Palmer. 2005. Machine trans-
lation using probabilistic synchronous dependency
insertion grammars. In Proc. of ACL.

Eisner, Jason. 2003. Learning non-isomorphic tree
mappings for machine translation. In Proc. of ACL.

Fox, Heidi J. 2002. Phrasal cohesion and statistical
machine translation. In Proc. of EMNLP.

Galley, Michel, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proc.
of ACL.

Hellwig, Peter. 2006. Parsing with dependency gram-
mars, volume ii. An International Handbook of
Contemporary Research.

Huang, Liang, Wenbin Jiang, and Qun Liu. 2009.
Bilingually-constrained (monolingual) shift-reduce
parsing. In Proc. of EMNLP.

Koehn, Philipp, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Statistical phrase-based trans-
lation. In Proceedings International Workshop on
Spoken Language Translation.

Lin, Dekang. 2004. A path-based transfer model for
machine translation. In Proc. of Coling.

Liu, Yang, Yajuan Lü, and Qun Liu. 2009. Improving
tree-to-tree translation with packed forests. In Proc.
of ACL.

Och, Franz Josef and Hermann Ney. 2000. Improved
statistical alignment models. In Proc. of ACL.

Och, Franz Josef. 2003. Minimum error rate training
in statistical machine translation. In Proc. of ACL.

Papineni, Kishore, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proc. of ACL.

Quirk, Christopher, Arul Menezes, and Colin Cherry.
2005. Dependency treelet translation: Syntactically
informed phrasal smt. In Proc. of ACL.

Setiawan, Hendra, Min-Yen Kan, and Haizhou Li.
2007. Ordering phrases with function words. In
Proc. of ACL.

Shen, Libin, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proc. of ACL.

Stolcke, Andreas. 2002. Srilm - an extensible lan-
guage modeling toolkit. In Proc. of ICSLP.

Tu, Zhaopeng, Yang Liu, Young-Sook Hwang, Qun
Liu, and Shouxun Lin. 2010. Dependency forest
for statistical machine translation. In Proc. of COL-
ING.

Wu, Dekai. 1995. Stochastic inversion transduction
grammars, with appliction to segmentation, backet-
ing, and alignment of parallel corpora. In Proc. of
IJCAI.

Wu, Dekai. 1996. A polynomial-time algorithm for
statistical machine translation. In Proc. of ACL.

Xiong, Deyi, Qun Liu, and Shouxun Lin. 2006. Maxi-
mum entropy based phrase reordering model for sta-
tistical machine translation. In Proc. of ACL.

Xiong, Deyi, Min Zhang, Aiti Aw, and Haizhou Li.
2008. Linguistically annotated BTG for statistical
machine translation. In Proc. of Coling.

Zens, Richard, Hermann Ney, Taro Watanabe, and Ei-
ichiro Sumita. 2004. A polynomial-time algorithm
for statistical machine translation. In Proc. of Col-
ing.

Zhang, Min and Haizhou Li. 2009. Tree kernel-
based svm with structured syntactic knowledgefor
btg-based phrase reordering. In Proc. of EMNLP.

Zhang, Ying, Stephan Vogel, and Alex Waibel. 2004.
Interpreting bleu/nist scores how much improve-
ment do we need to have a better system? In Proc.
of LREC.

Zhang, Dongdong, Mu Li, Chi-Ho Li, and Ming Zhou.
2007. Phrase reordering model integrating syntactic
knowledge for smt. In Proc. of EMNLP.

Zhang, Min, Hongfei Jiang, Aiti Aw, Haizhou Li,
Chew Lim Tan, and Sheng Li. 2008. A tree
sequence alignment-based tree-to-tree translation
model. In Proc. of ACL.

Zhang, Le. 2004. Maximum entropy modeling took-
lkit for python and c++.

1193


