



















































AMR Parsing with an Incremental Joint Model


Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 680–689,
Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics

AMR Parsing with an Incremental Joint Model

Junsheng Zhou†, Feiyu Xu‡, Hans Uszkoreit‡, Weiguang Qu†, Ran Li† and Yanhui Gu†
† Language Information Processing and Social Computing Lab

School of Computer Science and Technology, Nanjing Normal University, China
{zhoujs, wgqu, gu}@njnu.edu.cn, liran3277@sina.com

‡ Language Technology Lab, DFKI, Germany
{feiyu, uszkoreit}@dfki.de

Abstract

To alleviate the error propagation in the tra-
ditional pipelined models for Abstract Mean-
ing Representation (AMR) parsing, we for-
mulate AMR parsing as a joint task that per-
forms the two subtasks: concept identification
and relation identification simultaneously. To
this end, we first develop a novel component-
wise beam search algorithm for relation iden-
tification in an incremental fashion, and then
incorporate the decoder into a unified frame-
work based on multiple-beam search, which
allows for the bi-directional information flow
between the two subtasks in a single incre-
mental model. Experiments on the public
datasets demonstrate that our joint model sig-
nificantly outperforms the previous pipelined
counterparts, and also achieves better or com-
parable performance than other approaches to
AMR parsing, without utilizing external se-
mantic resources.

1 Introduction

Producing semantic representations of text is moti-
vated not only by theoretical considerations but also
by the hypothesis that semantics can be used to im-
prove many natural language tasks such as question
answering, textual entailment and machine transla-
tion. Banarescu et al. (2013) described a semantics
bank of English sentences paired with their logical
meanings, written in Abstract Meaning Representa-
tion (AMR), which is rapidly emerging as an impor-
tant practical form of structured sentence semantics.
Recently, some literatures reported some promising
applications of AMR. Pan et al. (2015) presented

an unsupervised entity linking system with AMR,
achieving the performance comparable to the super-
vised state-of-the-art. Liu et al. (2015) demonstrated
a novel abstractive summarization framework driven
by the AMR graph that shows promising results.
Garg et al. (2016) showed that AMR can signifi-
cantly improve the accuracy of a biomolecular in-
teraction extraction system compared to only using
surface- and syntax-based features. Mitra and Baral
(2016) presented a question-answering system by
exploiting the AMR representation, obtaining good
performance.

Automatic AMR parsing is still in a nascent
stage. Flanigan et al. (2014) built the first AMR
parser, JAMR, based on a pipelined approach, which
breaks down the whole task into two separate sub-
tasks: concept identification and relation identifica-
tion. Considering that node generation is an impor-
tant limiting factor in AMR parsing, Werling et al.
(2015) proposed an improved approach to the con-
cept identification subtask by using a simple clas-
sifier over actions which generate these subgraphs.
However, the overall architecture is still based on the
pipelined model.

As a common drawback of the staged architec-
ture, errors in upstream component are often com-
pounded and propagated to the downstream predic-
tion. The downstream components, however, can-
not impact earlier decision. For example, for the
verb “affect” in the example shown in Figure 1,
there exist two possible concepts: “affect-01” and
“affect-02”. Comparatively, the first concept has
more common use cases than the second one. But,
when the verb “affect” is followed by the noun “ac-

680



cent”, it should evoke the concept “affect-02”. Ob-
viously, the correct concept choice for the verb “af-
fect” should exploit a larger context, and even the
whole semantic structure of the sentence, which is
more probable to be unfolded at the downstream re-
lation identification stage. This example indicates
that it is necessary to allow for the interaction of in-
formation between the two stages.

try-01

He

affect-02

accent

:ARG0

:ARG1

:ARG1
:ARG0

country

name

British

:op1

:name
:mod

Figure 1: The AMR graph for the sentence “He tries to affect a
British accent.”

To address this problem, in this paper we refor-
mulate this task as a joint parsing problem by ex-
ploiting an incremental parsing model. The under-
lying learning algorithm has shown the effectiveness
on some other Natural Language Processing (NLP)
tasks, such as dependency parsing and extraction of
entity mentions and relations (Collins and Roark,
2004; Hatori et al., 2012; Li and Ji, 2014). However,
compared to these NLP tasks, the AMR parsing is
more challenging in that the AMR graph is more
complicated. In addition, the nodes in the graph are
latent.

One main challenge to search for concept frag-
ments and relations incrementally is how to com-
bine the two subtasks in a unified framework. To
this end, we first develop a novel Component-Wise
Beam Search (CWBS) algorithm for incremental re-
lation identification to examine the accuracy loss in
a fully incremental fashion compared to the global
fashion in which a sequence of concept fragments
derived from the whole sentence are required as in-
put, as the MSCG algorithm in JAMR. Secondly,
we adopt a segment-based decoder similar to the
multiple-beam algorithm (Zhang and Clark, 2008b)
for concept identification, and then incorporate the
CWBS algorithm for relation identification into this
framework, combining the two subtasks in a sin-
gle incremental model. For parameter estimation,
“violation-fixing” perceptron is adopted since it is

designed specifically for inexact search in structured
learning (Huang et al., 2012).

Experimental results show that the proposed joint
framework significantly outperforms the pipelined
counterparts, and also achieves better or comparable
performance than other AMR parsers, even without
employing external semantic resources.

2 Background

2.1 AMR Parsing Task

Nodes of an AMR graph are labeled with con-
cepts, and edges are labeled with relations. Con-
cepts can be English words (“He”), PropBank event
predicates (“try-01”, “affect-02”), or special key-
words (“British”). For example, “affect-02” rep-
resents a PropBank roleset that corresponds to the
first sense of “affect”. According to (Banarescu et
al., 2013), AMR uses approximately 100 relations.
The rolesets and core semantic relations (e.g., ARG0
to ARG5) are adopted from the PropBank annota-
tions in OntoNotes. Other semantic relations include
“mode”, “name”, “time”, “topic” and so on. The
AMR guidelines provide more detailed descriptions.

2.2 The Pipelined Models for AMR Parsing

The AMR parser JAMR is a two-stage algorithm that
first identifies concepts and then identifies the rela-
tions that obtain between these.

The concept identification stage maps spans of
words in the input sentence to a sequence of con-
cept graph fragments. Note that these graph frag-
ments, in some cases, are subgraphs with multi-
ple nodes and edges, not just one labeled concept
node. The relation identification stage adds edges
among the concept subgraph fragments identified in
the first stage. JAMR requires the output subgraph
G =< VG, EG > should respect the following con-
straints:

(1) Simple: For any two vertices u and v ∈ VG, EG
includes at most one edge between u and v.

(2) Connected: G must be weakly connected (every
vertex reachable from every other vertex, ignor-
ing the direction of edges).

(3) Deterministic: For each node u ∈ VG, and for
each label l ∈ {ARG0, . . . ,ARG5} , there is at

681



most one outgoing edge inEG from uwith label
l.

To find a maximum spanning AMR graph, JAMR
proposed a two-step approach1 . First, a graph that
ignores constraint (3) but respects the others was
created, by searching for the maximum spanning
connected subgraph from an edge-labeled, directed
graph representing all possible relations between the
identified concepts; Second, a Lagrangian relaxation
was adopted to iteratively adjust the edge scores so
as to enforce constraint (3).

In order to train the parser, JAMR built an auto-
matic aligner that uses a set of rules to greedily align
concepts to spans of words in the training data to
generate an alignment table.

3 Algorithms

Based on the hypothesis that concept identification
and relation identification are interrelated, we pro-
pose to jointly perform the two subtasks in a sin-
gle model. To this end, we present an incremental
model for AMR parsing. Evidence from psycholin-
guistic research also suggests that human language
comprehension is incremental. Comprehenders do
not wait until the end of the sentence before they
build a syntactic or semantic representation for the
sentence.

However, the challenges of successfully applying
the incremental joint model to this problem formu-
lation are: 1) how can we design an effective decod-
ing algorithm for identifying the relations between
the nodes in an incremental fashion, given a partial
sequence of spans, i.e., a partial sequence of gold-
standard concept fragments; 2) further, if given a
sentence, how can we design an incremental frame-
work to perform concept identification and relation
identification simultaneously. In the following sub-
sections we introduce our solutions to these chal-
lenges in detail.

3.1 An Incremental Decoding Algorithm for
Relation Identification

We define the relation identification problem as find-
ing the highest scoring graph y from all possible out-

1In this paper, we refer to this two-step approach for relation
identification as MSCG algorithm.

puts given a sequence of concept fragments c:

F (c) = argmax
Gen(c)

Score(y) (1)

where Gen(c) denotes the set of possible AMR
graph for the input c. The score of an output parse
y is defined to be decomposed by edges, and with a
linear model:

Score(y) =
∑

e∈Ey
wT · ϕ(e) (2)

where ϕ(e) is the feature vector over the edge e, and
w is weight vector of the model.

The AMR graph is a directed graph that respects
three constraints (see section 2.2) and has a node
marked as the focus node. Obviously, finding such a
maximum spanning graph in AMR parsing in fact
carries more complexities than that of maximum
spanning tree (MST) decoding for syntactic parsing.
Especially, performing the task incrementally is sub-
stantially harder than doing it non-incrementally. In
both cases, parsing is in general intractable and we
provide an approximate inference algorithm to make
these cases tractable.

Inspired by the graph-based dependency parser
under the framework of beam-search, which yields
a competitive performance compared to the exact-
search-based counterpart (Zhang and Clark, 2008a),
we develop a CWBS algorithm for the relation iden-
tification task.

Basically, the decoder works incrementally, build-
ing a state item (i.e. a partial AMR graph) fragment
by fragment. When each concept fragment is pro-
cessed, edges are added between the current con-
cept fragment and its predecessors. However, how
to treat its predecessors is a difficult problem. In
our experiments, we found that if we consider every
preceding concept fragment to the left of the cur-
rent fragment in a right-to-left order in the search
process, the decoder suffers from low efficiency
and poor performance. Unlike the beam-search for
dependency parsing, which can greatly reduce the
search space by exploiting the projectivity property
of the dependency tree (Covington, 2001; Zhang and
Clark, 2008a), this naive search process in this con-
text inevitably leads to huge search space, and fur-
thermore is difficult to guarantee the connectivity of

682



output graph. Instead, we propose a component-
wise beam search scheme, which can not only al-
leviate much noisy partial candidate, but also ensure
that the final output graph is connected.

Algorithm 1 shows the pseudocode for the com-
plete procedure of the decoder. In a nutshell, the
algorithm builds the AMR graph in one left-to-right
pass over the sequence of concept fragments. Beam
search is applied by keeping the B-best2 items in
the agenda at each processing stage, according to the
scores of partial graph up to the current concept frag-
ment. Lets take an illustrative diagram to demon-
strate the procedure (see Figure 2). When appending
the current concept fragment to the left partial graph
to extend it, we just need to consider the relations be-
tween current concept and each preceding connected
component. However, even at this single step, pick-
ing B-best extended partial graphs is still a difficult
task due to the large combination space. Here, we
adopt an effective nested beam search strategy at
this step. In other words, edges are added between
the current concept fragment and its preceding con-
nected components by iterating through these com-
ponents in a right-to-left order3 using an inner beam-
search. When examining the edges between the cur-
rent concept fragment and some preceding compo-
nent, four elementary actions are used:

(1) SHIFT (lines 12-14): Add only current concept
to the partial graph.

(2) LEFT-ARC (lines 16-19): Add current concept
and a highest-scoring edge from a node in the
current concept to a node in some preceding
connected component to the partial graph.

(3) RIGHT-ARC (lines 21-24): Add current con-
cept and a highest-scoring edge from a node in
some preceding connected component to a node
in current concept to the partial graph.

(4) LEFT & RIGHT-ARCS (lines 26-27): Add
current concept and highest-scoring left arc and
right arc to the partial graph.

The first three actions are similar in form to those
in the Arc-Standard algorithm for transition-based

2The constant B denotes the beam size.
3The right-to-left order reflects the principle of local priority.

Figure 2: An illustrative diagram for CWBS algorithm. Each
dotted box corresponds to a connected component in the par-

tial graph, each of which consists one or multiple concept frag-

ments. The rightmost subgraph corresponds to the current con-

cept fragment.

dependency parsing (Nivre, 2008; Zhang and Clark,
2008a). The last one is defined to cope with the
cases where there may be multiple parents for some
nodes in an AMR graph. Note that the “SHIFT”
action does not add any edges. This operation is
particularly necessary because the partial graphs are
not always connected during the search process. In
our experiments, we also found that the number of
connected components during search process is rel-
atively small, which is generally less than 6. It is im-
portant to note that, in order to guarantee the output
graph connected, when the last concept fragment is
encountered, the “SHIFT” action is skipped (see line
10 in Algorithm 1), and the other three ‘arc’ actions
will add edges to connect the last concept fragment
with all preceding connected components to yield a
connected graph.

For purpose of brevity, we introduce some
functional symbols in Algorithm 1. Function
CalEdgeScores(state, ci) calculates the scores of
all candidate edges between the nodes in current
concept fragment ci and the nodes in the partial
graph in state covering (c1, c2, . . . , ci−1). For com-
puting the scores of edges, we use the same fea-
tures as JAMR (refer to Flanigan et al. (2014) for
more details). Function FindComponents(state)
returns all connected components (p1, p2, . . . , pm)
in the partial graph in state, sorted by the max-
imum end position of spans including in every
component. The AddItem function adds the cur-
rent concept fragment and left/right arc to the
partial graph. Function AppendItem(buf, item)
inserts the partial graph item into buf by its
score. Functions GetMaxLeftEdge(ci, pj) and

683



Algorithm 1 The incremental decoding algorithm for
relation identification.
Input: A sequence of concept fragments (c1, c2, . . . , cn)
Output: Best AMR graph including (c1, c2, . . . , cn)

1: agenda← {Empty-graph}
2: for i← 1 . . . n do
3: for state in agenda do
4: CalEdgeScores(state, ci)
5: (p1, p2, . . . , pm)← FindComponents(state)
6: innerAgenda← state
7: for j ← m. . . 1 do
8: buf ← NULL
9: for item in innerAgenda do

10: if i < n then
11: //Add only ci to the item
12: newitem← item
13: AddItem(newitem, ci)
14: AppendAgenda(buf, newitem, i, n)
15: // Add a left arc from ci to pj to the item
16: newitem← item
17: le← GetMaxLeftEdge(ci, pj)
18: AddItem(newitem, ci, le)
19: AppendAgenda(buf, newitem, i, n)
20: //Add a right arc from pj to ci the item
21: newitem← item
22: re← GetMaxRightEdge(pj , ci)
23: AddItem(newitem, ci, le)
24: AppendAgenda(buf, newitem, i, n)
25: //Add both left and right arc to the item
26: AddItem(item, ci, le, re)
27: AppendAgenda(buf, item, i, n)
28: innerAgenda← B-best(buf)
29: agenda← innerAgenda
30: return agenda[0]
31: function AppendAgenda(buf, item, i, n)
32: //parameter n represents the terminal position
33: if i = n then
34: CalRootFeatures(item)
35: AppendItem(buf, item)

GetMaxRightEdge(pj , ci) pick the highest-scoring
left-arc and right-arc linking current fragment ci and
the connected component pj by the scores returned
from the CalEdgeScores function, respectively.

Finally, the function CalRootFeatures(g) first
computes the scores for all nodes in the output graph
g by treating them as the candidate root respectively,
and then pick the node with the highest score as
the focus node of the graph. When computing the
score for each candidate node, similar to JAMR, two

types of features were used: the concept of the node,
and the shortest dependency path from a word in the
span to the root of the dependency tree.

The time complexity of the above algorithm is
O(MB2n), where M is the maximum number of
connected components during search, B is beam
size and n is the number of concept fragments. It
is linear in the length of sequence of concept frag-
ments. However, the constant in the O is relatively
large. In practice, the search space contains a large
number of invalid partial candidates. Therefore,
we introduce three partial output pruning schemes
which are helpful in reducing search space as well
as making the input for parameter update less noisy.

Firstly, we limit the number of children and par-
ents of every node. By observing the training data,
we set the maximum numbers of children and par-
ents of every node as 7 and 4, respectively. Sec-
ondly, due to the fact that all frame arguments
ARG0-ARG5 are derived from the verb framesets,
the edges with label l ∈ {ARG0, . . . , ARG5} that
do not outgo from a verb node will be skipped.

Finally, consider the determinism constraint (as il-
lustrated in section 2.2) that should be satisfied by an
AMR representation. When one edge has the same
label l ∈ {ARG0, . . . , ARG5} as one of edges out-
going from the same parent node, this edge will also
be skipped. Obviously, this type of pruning can en-
force the determinism constraint for every decoding
output.

3.2 Joint Decoding for Concept Identifica-tion
and Relation Identification

In this section, we further consider the joint decod-
ing problem for a given sentence x, which maps the
sentence x to an output AMR graph y. The objective
function for the joint decoding is as follows:

ŷ = argmax
y′∈Gen(x)

(wT · φ(x, y′) + wT · f(y′)) (3)

where the first term is to calculate the score over
all concept fragments derived from the words in the
sentence x, and the second one is to calculate the
score over all edges linking the concept fragments.
Maximizing Equation (3) amounts to concurrently
maximizing the score over the concept fragments
and the score over the edges. Admittedly, the joint
decoding problem is more intricate and in general

684



intractable. Therefore, we use a beam-search-based
incremental decoder for approximate joint inference
during training and testing.

In order to combine the two subtasks in a uni-
fied framework, we first relax the exact-search for
concept identification in JAMR by beam search,
resulting in a segment-based decoder similar to
the multiple-beam algorithm in (Zhang and Clark,
2008b; Li and Ji, 2014), and then incorporate the
CWBS algorithm for relation identification (as de-
picted in section 3.1) into this framework, which
provides a natural formulation for combining the
two subtasks in a single incremental model.

Algorithm 2 shows the joint decoding algorithm.
In short, during performing joint decoding incre-
mentally for the input sentence, for each word index
i in the input sentence, it maintains a beam for the
partial graphs whose last segments end at the i-th
word, which is denoted as agendas[i] in the algo-
rithm. When the i-th word is processed, it either trig-
gers concepts starting from this word by looking up
the alignment table generated from the training data,
or evokes no concept (we refer to this type of words
as function words). If the current word triggers mul-
tiple concepts, we first append each candidate con-
cept to the partial graphs in the beam agendas[i−1],
by using a component-wise beam search way (see
section 3.1), and then pick B-best extended partial
graphs by exploiting the features from both the con-
cept level and relation level to compute the overall
scores.

In particular, judging whether a word is a func-
tion word is an important and difficult task. For
example, the word “make” corresponds to multiple
candidate concepts in the alignment table, such as
“make-01” and “make-02”. However, it can also
act as a functional word in some cases. To re-
solve the judgement problem, we view each word
as a function word and a non-function word at the
same time to allow them to compete against each
other by their scores. For instance, for the i-th
word, this is done by combining all partial graphs
in the beam agendas[i − 1] with those in the beam
agendas[i] to select B-best items and then record
them in agendas[i], which is represented as the
Union function in Algorithm 2.

After all words are processed, the highest-scoring
graph in the beam corresponding to the terminal po-

Algorithm 2 The joint decoding algorithm.
Input: Input sentence x = (w1, w2, . . . , wn)
Output: Best AMR graph derived from x

1: agendas[0]← ∅
2: last← Scan(x)
3: for i← 1 . . . n do
4: list← Lookup(x, i)
5: if list.size > 0 then
6: preAgenda← agendas[i− 1]
7: for cf ∈ list do
8: end← i+ cf .size− 1
9: if preAgenda.size = 0 then

10: g ← Graph.empty
11: CalConceptFeatures(g, cf )
12: AppConcept(agendas, end, g, cf, last)
13: else
14: for item ∈ preAgenda do
15: g ← item
16: CalConceptFeatures(g, cf )
17: AppConcept(agendas, end, g, cf, last)
18: Union(agendas, i, i− 1)
19: else
20: agendas[i]← agendas[i− 1]
21: bestGraph← agendas[last][0]
22: return bestGraph

sition of the sentence is selected as the output.
In algorithm 2, function Scan(x) is used to search

the terminal position corresponding to the last con-
cept fragment in the sentence x, which will be
passed as a parameter to the function AppConcept.
The Scan function can be efficiently implemented by
calling the function Lookup in a right-to-left order.
Function Lookup(x, i) maps a sequence of words
starting from the index i in sentence x to a set of can-
didate concept fragments, by looking up the align-
ment table that was generated from the training data.
The alignments are accomplished using an aligner
from JAMR. Motivated by Werling et al. (2015), we
also adopt two additional actions to generate the can-
didate concept fragments: LEMMA and VERB. The
action LEMMA is executed by using the lemma of
the source token as the generated node title, and the
action VERB is to find the most similar verb in Prop-
Bank based on Jaro-Winkler distance, and adopt its
most frequent sense.

Function CalConceptFeatures(g, cf ) calculates
the feature vector for the candidate concept frag-
ment cf and the partial graph g, using the features

685



defined in Table 1. Among them, features 1-4
are from JAMR. Additional features 5-16 aim to
capture the association between the current concept
and the context in which it appears. Function
AppConcept(agendas, end, g, cf, last) appends
the current concept cf to the partial graph g,
and then inserts the extended partial graph into
agendas[end]. Note that when the parameter end
equals to the parameter last, this function will call
the function CalRootFeatures to select the focus
node, as illustrated in Algorithm 1.

Name Description

1 Fragment given
words

Relative frequency estimates of
the probability of a concept
fragment given the span of
words.

2 Span length The length of the span.
3 NER 1 if the span corresponds to a

named entity, 0 otherwise.
4 Bias 1 for any concept fragment

from the alignment table, 0 oth-
erwise.

5 c

c represents the current con-
cept label, w represents the cur-
rent words, lem represents the
current lemmas, pos represents
the current POS tags. w−1 de-
notes the first word to the left of
current word, w+1 denotes the
first word to the right of current
word, and so on.

6 c+ w
7 c+ lem
8 c+ pos
9 c+ w−1
10 c+ w+1
11 c+ pos−1
12 c+ pos+1
13 c+ w−2
14 c+ w+2
15 c+ pos−2
16 c+ pos+2

Table 1: Features associated with the concept fragments.

3.3 Violation-Fixing Perceptron for Training
Online learning is an attractive method for the struc-
tured learning since it quickly converges within a
few iterations (Collins, 2002). Particularly, Huang
et al. (2012) establish a theoretical framework called
“violation-fixing perceptron” which is tailored for
structured learning with inexact search and has prov-
able convergence properties. Since our incremen-
tal decoding for AMR parsing is an approximate in-
ference, it is very natural to employ violation-fixing
perceptron here for AMR parsing training.

Specifically, we use an improved update method
“max-violation” which updates at the worst mistake,

and converges much faster than early update with
similar or better accuracy. We adopt this idea here
as follows: decode the whole sentence, and find
the word index i∗ where the difference between the
candidate partial graph and gold-standard one is the
biggest. Only part of the graph ending at the word
index i∗ is used to calculate the weight update, in
order to account for search errors.

To reduce overfitting, we used averaged parame-
ters after training to decode test instances in our ex-
periments. The resulting model is called averaged
perceptron (Collins, 2002).

Additionally, in our training algorithms, the im-
plementation of the oracle function is rela-tively
straightforward. Specifically, when the i-th span is
processed in the incremental parsing process, the
partial gold-standard AMR graph up to the i-th span
consists of the edges and nodes that appear before
the end position of the i-th span, over which the
gold-standard feature vectors are calculated.

4 Experiments

4.1 Dataset and Evaluation Metric

Following previous studies on AMR parsing, our ex-
periments were performed on the newswire sections
of LDC2013E117 and LDC2014T12, and we also
follow the official split for training, development and
evaluation. Finally, we also show our parsers perfor-
mance on the full LDC2014T12 dataset. We evalu-
ate the performance of our parser using Smatch v2.0
(Cai and Knight, 2013), which counts the precision,
recall and F1 of the concepts and relations together.

4.2 Development Results

Generally, larger beam size will increase the com-
putational cost while smaller beam size may reduce
the performance. As a tradeoff, we set the beam size
as 4 throughout our experiments. Figure 3 shows the
training curves of the averaged violation-fixing per-
ceptron with respect to the performance on the both
development sets. As we can see the curves con-
verge very quickly, at around iteration 3.

686



 

0.66

0.67

0.68

0.69

0.7

0.71

0.72

0 1 2 3 4 5 6 7 8 9 10

F
-m

ea
su

re

Number of training iterations

LDC2014T112
LDC2103E117

Figure 3: Learning curves on development sets.

Dataset System P R F1

LDC2013E117 MSCG .85 .77 .81CWBS .85 .78 .81

LDC2014T12 MSCG .84 .77 .80CWBS .84 .77 .80

Table 2: Results of two different relation identification algo-
rithms.

4.3 Incremental Relation Identification
Performance

Before performing joint decoding, we should first
verify the effectiveness of our incremental algorithm
CWBS. The first question about CWBS is whether
the component-wise search is a valid scheme for de-
riving the gold-standard AMR graph given the se-
quence of gold-standard concepts. Therefore, we
first implement an oracle function by performing the
incremental component-wise search for each frag-
ment sequence c to get a “pseudo-gold” graph G

′
c;

Then we compare with gold-standard AMR graph
Gc . On the training data of LDC2013E117 and
LDC2014T12, we respectively got an overall 99.6%
and 99.7% F-scores for all < G

′
c, Gc > pairs, which

indicates that our component-wise search is an ef-
fective incremental search scheme.

Further, we train a perceptron model using the
max-violation update to approximate the oracle
search procedure. As shown in Table 2, our in-
cremental algorithm CWBS achieves almost the
same performance as the non-incremental algorithm
MSCG in JAMR, using the same features as MSCG.
The results indicate that CWBS is a competitive al-
ternative to MSCG.

4.4 Joint Model vs. Pipelined Model
In this section, we compare the overall performance
of our joint model to the pipelined model, JAMR4.
To give a fair comparison, we first implemented sys-
tem 1 only using the same features (i.e., features 1-
4 in Table 1) as JAMR for concept fragments. Ta-
ble 3 gives the results on the two datasets. In terms
of F-measure, we gain a 6% absolute improvement,
and a 5% absolute improvement over the results of
JAMR on the two different experimental setups re-
spectively.

Next, we implemented system 2 by using more
lexical features to capture the association between
concept and the context (i.e., features 5-16 in Table
1). Intuitively, these lexical contextual features
should be helpful in identifying concepts in parsing
process. As expected, the results in Table 3 show
that we gain 3% improvement over the two different
datasets respectively, by adding only some addi-
tional lexical features.

Dataset System P R F1

LDC2013E117
JAMR(fixed) .67 .58 .62

System 1 .72 .65 .68
System 2 .73 .69 .71

LDC2014T12
JAMR(fixed) .68 .59 .63

System 1 .74 .63 .68
System 2 .73 .68 .71

Table 3: Comparison between our joint approaches and the
pipelined counterparts.

Dataset System P R F1

LDC2013E117
CAMR* .69 .67 .68
CAMR .71 .69 .70

Our approach .73 .69 .71

LDC2014T12
CAMR* .70 .66 .68
CAMR .72 .67 .70

CCG-based .67 .66 .66
Our approach .73 .68 .71

Table 4: Final results of various methods.

4.5 Comparison with State-of-the-art
We give a comparison between our approach and
other state-of-the-art AMR parsers, including CCG-
based parser (Artzi et al., 2015) and dependency-
based parser (Wang et al., 2015b). For comparison

4We use the latest, fixed version of JAMR, available at
https://tiny.cc/jamr.

687



purposes, we give two results from two different ver-
sions of dependency-based AMR parser5: CAMR*
and CAMR. Compared to the latter, the former de-
notes the system that does not use the extended fea-
tures generated from the semantic role labeling sys-
tem, word sense disambiguation system and so on,
which is directly comparable to our system.

From Table 4 we can see that our parser achieves
better performance than other approaches, even
without utilizing any external semantic resources.

We also evaluate our parser on the full
LDC2014T12 dataset. We use the train-
ing/development/test split recommended in the
release: 10,312 sentences for training, 1,368 sen-
tences for development and 1,371 sentences for
testing. For comparison, we include the results of
JAMR, CAMR*, CAMR and SMBT-based parser
(Pust et al., 2015), which are also trained on the
same dataset. The results in Table 5 show that
our approach outperforms CAMR*, and obtains
comparable performance with CAMR. However,
our approach achieves slightly lower performance,
compared to the SMBT-based parser, which adds
data and features drawn from various external
semantic resources.

Dataset System P R F1

LDC2014T12

JAMR(fixed) .64 .53 .58
CAMR* .68 .60 .64
CAMR .70 .62 .66

SMBT-based - - .67
Our approach .70 .62 .66

Table 5: Final results on the full LDC2014T12 dataset.

5 Related Work

Our work is motivated by JAMR (Flanigan et al.,
2014), which is based on a pipelined model, re-
sulting in a large drop in overall performance when
moving from gold concepts to system concepts.

Wang et al. (2015a) uses a two-stage approach;
dependency parses are modified by executing a se-
quence of actions to resolve dis-crepancies between
dependency tree and AMR structure. Goodman
et al. (2016) improves the transition-based parser
with the imitation learning algorithms, achieving al-
most the same performance as that of Wang et al.

5The code is available at https://github.com/
Juicechuan/AMRParsing

(2015b), which exploits the extended features from
additional trained analysers, including co-reference
and semantic role labelers. Artzi et al. (2015) in-
troduces a new CCG grammar induction algorithm
for AMR parsing, combined with a factor graph
to model non-compositional phenomena. Pust et
al. (2015) adapts the SBMT parsing framework to
AMR parsing by designing an AMR transformation,
and adding external semantic resources. More re-
cently, Damonte et al. (2016) also presents an incre-
mental AMR parser based on a simple transition sys-
tem for dependency parsing. However, compared to
our parser, their parser cannot parse non-projective
graphs, resulting in a limited coverage.

Our work is also inspired by a new computa-
tional task of incremental semantic role labeling, in
which semantic roles are assigned to incomplete in-
put (Konstas et al., 2014).

6 Conclusions and Future Work

In this paper, we present a new approach to AMR
parsing by using an incremental model for perform-
ing the concept identification and relation identifica-
tion jointly, which alleviates the error propagation in
the pipelined model.

In future work, we plan to improve the parsing
performance by exploring more features from the
coreference resolution, word sense disambiguation
system and other external semantic resources. In
addition, we are interested in further incorporating
the incremental semantic role labeling into our in-
cremental framework to allow bi-directional infor-
mation flow between the two closely related tasks.

Acknowledgments

This research is supported by projects 61472191,
61272221 under the National Natural Science
Foundation of China, projects 14KJB520022,
15KJA420001 under the Natural Science Research
of Jiangsu Higher Education Institutions of China,
and partially supported by the German Federal Min-
istry of Education and Research (BMBF) through
the project ALL SIDES (01IW14002) and BBDC
(contract 01IS14013E). We would also like to thank
the insightful comments from the three anonymous
reviewers.

688



References
Yoav Artzi, Kenton Lee, and Luke Zettlemoyer. 2015.

Broad-coverage CCG Semantic Parsing with AMR. In
Proc. of EMNLP, pages 1699–1710.

Laura Banarescu, Claire Bonial, Shu Cai, Madalina
Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin
Knight, Philipp Koehn, Martha Palmer, , and Nathan
Schneider. 2013. Abstract Meaning Representation
for Sembanking. In Proc. of the Linguistic Annotation
Workshop and Interoperability with Discourse.

Shu Cai and Kevin Knight. 2013. Smatch: an Evaluation
Metric for Semantic Feature Structures. In Proc. of
ACL, pages 748–752.

Michael Collins and Brian Roark. 2004. Incremental
Parsing with the Perceptron Algorithm. In Proc. of
ACL, pages 111–118.

Michael Collins. 2002. Discriminative Training Meth-
ods for Hidden Markov Models: Theory and Ex-
periments with Perceptron algorithms. In Proc. of
EMNLP, pages 1–8.

Michael A. Covington. 2001. A Fundamental Algorithm
for Dependency Parsing. In Proc. of ACM Southeast
Conference.

Marco Damonte, Shay B. Cohen, and Giorgio Satta.
2016. An Incremental Parser for Abstract Meaning
Representation. arXiv preprint at arXiv:1608.06111.

Jeffrey Flanigan, Sam Thomson, Jaime Carbonell, Chris
Dyer, and Noah A. Smith. 2014. A Discriminative
Graph-Based Parser for the Abstract Meaning Repre-
sentation. In Proc. of ACL, pages 1426–1436.

Sahil Garg, Aram Galstyan, Ulf Hermjakob, and Daniel
Marcu. 2016. Extracting Biomolecular Interactions
Using Semantic Parsing of Biomedical Text. In Proc.
of AAAI.

James Goodman, Andreas Vlachos, and Jason Na-
radowsky. 2016. Noise Reduction and Targeted Ex-
ploration in Imitation Learning for Abstract Meaning
Representation Parsing. In Proc. of ACL, pages 1–11.

Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, and Ju-
nichi Tsujii. 2012. Incremental Joint Approach to
Word Segmentation, POS Tagging, and Dependency
Parsing in Chinese. In Proc. of ACL, pages 1045–
1053.

Liang Huang, Suphan Fayong, and Yang Guo. 2012.
Structured Perceptron with Inexact Search. In Proc.
of HLT-NAACL, pages 142–151.

Ioannis Konstas, Frank Keller, Vera Demberg, and
Mirella Lapata. 2014. Incremental Semantic Role
Labeling with Tree Adjoining Grammar. In Proc. of
EMNLP, pages 301–312.

Qi Li and Heng Ji. 2014. Incremental Joint Extraction of
Entity Mentions and Relations. In Proc. of ACL, pages
402–412.

Fei Liu, Jeffrey Flanigan, Sam Thomson, Norman Sadeh,
and Noah A. Smith. 2015. Toward Abstractive Sum-
marization Using Semantic Representations. In Proc.
of NAACL, pages 1086–1077.

Arindam Mitra and Chitta Baral. 2016. Addressing a
Question Answering Challenge by Combining Statis-
tical Methods with Inductive Rule Learning and Rea-
soning. In Proc. of AAAI.

Joakim Nivre. 2008. Algorithms for Deterministic Incre-
mental Dependency Parsing. Computational Linguis-
tics, 34(4):513–553.

Xiaoman Pan, Taylor Cassidy, Ulf Hermjakob, Heng Ji,
and Kevin Knight. 2015. Unsupervised Entity Link-
ing with Abstract Meaning Representation. In Proc. of
NAACL, pages 1130–1139.

Michael Pust, Ulf Hermjakob, Kevin Knight, Daniel
Marcu, and Jonathan May. 2015. Parsing English
into Abstract Meaning Representation Using Syntax-
Based Machine Translation. In Proc. of EMNLP,
pages 1143–1154.

Chuan Wang, Nianwen Xue, and Sameer Pradhan.
2015a. A Transition-based Algorithm for AMR Pars-
ing. In Proc. of NAACL, pages 366–375.

Chuan Wang, Nianwen Xue, and Sameer Pradhan.
2015b. Boosting Transition-based AMR Parsing with
Re-fined Actions and Auxiliary Analyzers. In Proc. of
ACL, pages 857–862.

Keenon Werling, Gabor Angeli, and Christopher D. Man-
ning. 2015. Robust Subgraph Generation Improves
Abstract Meaning Representation Parsing. In Proc. of
ACL, pages 982–991.

Yue Zhang and Stephen Clark. 2008a. A Tale of
Two Parsers: Investigating and Combining Graph-
Based And transition-Based Dependency Parsing Us-
ing Beam-search. In Proc. of EMNLP, pages 562–571.

Yue Zhang and Stephen Clark. 2008b. Joint Word Seg-
mentation and POS Tagging Using a Single Percep-
tron. In Proc. of ACL, pages 888–896.

689


