



















































Mean Hierarchical Distance Augmenting Mean Dependency Distance


Proceedings of the Third International Conference on Dependency Linguistics (Depling 2015), pages 161–170,
Uppsala, Sweden, August 24–26 2015.

Mean Hierarchical Distance
Augmenting Mean Dependency Distance

Yingqi Jing
Department of Linguistics

Zhejiang University
Hangzhou, China

jakenevergivesup@gmail.com

Haitao Liu
Department of Linguistics

Zhejiang University
Hangzhou, China
htliu@163.com

Abstract

With a dependency grammar, this study provides

a unified method for calculating the syntactic

complexity in linear and hierarchical dimensions.

Two metrics, mean dependency distance (MDD)

and mean hierarchical distance (MHD), one

for each dimension, are adopted. Some results

from the Czech-English dependency treebank

are revealed: (1) Positive asymmetries in the

distributions of the two metrics are observed in

English and Czech, which indicates both languages

prefer the minimalization of structural complexity

in each dimension. (2) There are significantly

positive correlations between sentence length (SL),

MDD, and MHD. For longer sentences, English

prefers to increase the MDD, while Czech tends to

enhance the MHD. (3) A trade-off relationship of

syntactic complexity in two dimensions is shown

between the two languages. English tends to reduce

the complexity of production in the hierarchical

dimension, whereas Czech prefers to lessen the

processing load in the linear dimension. (4) The

threshold of the MDD2 and MHD2 in English and

Czech is 4.

1 Introduction

The syntactic structures of human languages are
generally described as two-dimensional, and many
structural linguists use tree diagrams to represent
them. For example, Tesnière (1959) employed
tree-like dependency diagrams called stemmas to
depict the structure of sentences. Tesnière also
distinguished between linear order and structural
order. In this study, we follow Tesnière’s clear-
cut separation of these two dimensions and inves-
tigate the relation between them by using an En-
glish and Czech dependency treebank, designing
different measures to quantify the complexity of
syntactic structure in each dimension.

The relationship between linear order and struc-
tural order is a crucial topic for all structural syn-
tax. For Tesnière (1959: 19), structural order (hi-
erarchical order) preceded linear order in the mind
of a speaker. Speaking a language involves trans-
forming structural order to linear order, whereas
understanding a language involves transforming
linear order to structural order. It is worth men-
tioning that Tesnière’s stemmas do not reflect ac-
tual word order, but rather they convey only hier-
archical order. This separation of the two ordering
dimensions has had great influence on the devel-
opment of dependency grammar and word-order
typology. The ability to separate the two dimen-
sions has been argued to be an advantage for de-
pendency grammar, since it is more capable than
constituency grammar of examining each dimen-
sion independently (Osborne, 2014).

The real connection between hierarchical or-
der and word order is evident when the princi-
ple of projectivity or continuity is defined in de-
pendency grammar (see, e.g., Lecerf, 1960; Hays,
1964: 519; Robinson, 1970: 260; Mel’čuk, 1988:
35; Nivre, 2006: 71). According to Hudson (1984:
98),

“if A depends on B, and some other ele-
ment C intervenes between them (in lin-
ear order of strings), then C depends di-
rectly on A or on B or on some other
intervening element.”

Projectivity is immediately visible in dependency
trees; a projective tree, as shown in Figure 1, has
no crossing lines. But it must be mentioned that
projectivity is not a property of the dependency
tree in itself, but only in relation to the linear string
of words (Nivre, 2003: 51), and some languages
with relatively free word order (e.g., German, Rus-
sian, and Czech) have more crossing lines than
languages with relatively rigid word order (Liu,
2010: 1576). Here, we also use the term “pro-

161



jection” in linear algebra as a means of trans-
forming a two-dimensional syntactic structure to
one-dimensionality. Thus, in a projective or non-
projective dependency tree, the string of words is
just an image projected by the structural sentence
onto the spoken chain, which extends successively
on a timeline.

Figure 1: A dependency tree of The small streams
make the big rivers.1

This study focuses on exploring the structural
rules of English and Czech using two metrics,
mean dependency distance (MDD), as first ex-
plored by Liu (2008), and mean hierarchical dis-
tance (MHD), as introduced and employed here
for the first time. These metrics help predict lan-
guage comprehension and production complexity
in each dimension. The metrics are mainly based
on the empirical findings in psycholinguistics and
cognitive science, and we tend to bind the two di-
mensions of syntactic structure together. To assess
the value of these metrics, we have explored the
syntactic complexity of English and Czech with
the help of the Prague Czech-English Dependency
Treebank 2.0 (PCEDT 2.0).

The rest of this manuscript introduces the
PCEDT 2.0 and data pre-processing in Section 2.
The theoretical background and previous empiri-
cal studies concerned with the two metrics (MDD
and MHD) are presented in Section 3, and our
methods for calculating them are also given in this
section. In Section 4, we present the results and
findings, which are summarized in the last section.

1The sentence The small streams make the big rivers is the
English translation of Tesnière’s (1959: 19) example, but lin-
ear order and projection lines have been added to the stemma.

2 Czech-English dependency treebank

The material used in this study is the PCEDT 2.0,
which is a manually parsed Czech-English parallel
corpus, sized at over 1.2 million running words in
almost 50,000 sentences for each language (Hajič
et al., 2012). The English part of the PCEDT
2.0 contains the entire Penn Treebank-Wall Street
Journal (WSJ) Section (Linguistic Data Consor-
tium, 1999). The Czech part consists of Czech
translations of all of the Penn Treebank-WSJ texts.
The corpus is 1:1 sentence-aligned. The par-
allel sentences of both languages are automati-
cally morphologically annotated and parsed into
surface-syntax dependency trees according to the
Prague Dependency Treebank 2.0 (PDT 2.0) an-
notation scheme. This scheme acknowledges an
analytical layer (a-layer, surface syntax) and a tec-
togrammatical layer (t-layer, deep syntax) of the
corpus (Hajič et al., 2012). Only the a-layer was
used for the current study. More information about
the treebank and its annotation scheme is available
on the PCEDT 2.0 website.2

Figure 2: A sample parallel sentence at the a-layer

PCEDT 2.0 is a strictly aligned corpus, which
is stored as *.treex format using the XML-based
Prague Markup Language (PML). It can be easily
visualized with the tree editor TrEd and displayed
as the sample parallel sentence (en. Mr. Nixon
was to leave China today. cs. Nixon měl z Čı́ny
odletět dnes. ) in Figure 2. The word alignment is
indicated by the dashed grey arrows pointing from

2http://ufal.mff.cuni.cz/pcedt2.0/

162



the English part to the Czech part.
We first extract data from the original Treex

documents with R 3.0.2, supported by the XML
package for parsing each node of the treebank,
and restore it into a Microsoft Access database.
The transformed corpus is much easier to access
and analyze (Liu, 2009: 113). Table 1 shows a
previous English sample sentence converted into
a new format, and the header contains sentence
number (sn), word number (wn), word (w), part-
of-speech (POS), governor number (gn), governor
(g) and dependency relations (dep). The root
verb is the only word that has no governor and
we indicate its lack of a governor and governor
number using 0.

sn wn w POS gn g dep
1770 1 Mr. NNP 2 Nixon Atr
1770 2 Nixon NNP 3 was Sb
1770 3 was VBD 0 0 Pred
1770 4 to TO 3 was AuxP
1770 5 leave VB 4 to Adv
1770 6 China NNP 5 leave Obj
1770 7 today NN 5 leave Obj
1770 8 . . 3 was AuxG

Table 1: A converted sample sentence in English

The a-layer of the corpus contains 1,173,766
English nodes and 1,172,626 Czech word tokens,
which are combined into 49,208 parallel sen-
tences. Sentences with less than three words (e.g.,
@, Virginia:, New Jersey:) or some special four-
element sentences (e.g., “Shocked.”, Právnı́ci
jistě ne.) were removed from each language (477
and 474 sentences). They are mainly specific
markers in the news or incomplete sentences.
Finally, the intersection of two language sets
constitutes the corpus used in our study according
to the sentence number. Table 2 presents an
overview of our corpus with 48,647 parallel sen-
tences (s), and the mean sentence length (msl) of
English and Czech is 24.1 and 23.63, respectively.
However, Czech has a much higher percentage of
non-projective (n.p.) dependencies than English.

name size s msl n.p.
en 1172244 48647 24.1 0.01%
cs 1149630 48647 23.63 3.11%

Table 2: General description of the corpus

3 Mean dependency distance and mean
hierarchical distance

Previous scholars have devoted a lot of effort to
building a well-suited metric for measuring and
predicting syntactic complexity of all human lan-
guages, for instance, Yngve’s (1960; 1996) Depth
Hypothesis3 and Hawkins’ (2003; 2009) princi-
ple of Domain Minimalization. The current psy-
cholinguistics and cognitive science have also pro-
vided evidence for this issue. Gibson (1998;
2000) conducted many reading experiments and
proposed a Dependency Locality Theory (DLT),
which associates the increasing structural integra-
tion cost with the distance of attachment. Fiebach
et al. (2002) and Phillips et al. (2005) observed a
sustained negativity in the ERP signal during sen-
tence regions with filler-gap dependencies, indi-
cating increased syntactic integration cost. These
studies have a common interest in connecting lin-
ear dependency distance with language processing
difficulty.

The concept of “dependency distance (DD)”
was first put forward by Heringer et al. (1980:
187) and defined by Hudson (1995: 16) as “the
distance between words and their parents, mea-
sured in terms of intervening words.” With the
previous theoretical and empirical evidence, Liu
(2008: 170) proposed the mean dependency dis-
tance (MDD) as a metric for language comprehen-
sion difficulty and gave the formula in (1) to cal-
culate it.

MDD =
1

n

n∑

i=1

|DDi| (1)

In this formula, n represents the total number of
dependency pairs in a sentence, and |DDi| is the
absolute value of the i-th dependency distance. It
must be noted that DD can be positive or negative,
denoting the relative position or dependency
direction between a dependent and its governor.
Thus, the MDD of a sentence is the average value
of all pairs of |DDi|.

The present study builds on this distance-based
notion of dependencies and extends the concept
into the hierarchical dimension. The act of
listening involves transforming a linear sentence

3Yngve took a constituency-based view and measured the
depth of a sentence by counting the maximum number of
symbols stored in the temporary memory when building a
syntactic tree. Yngve’s model and mertic are specifically de-
signed for sentence production.

163



into a two-dimensional syntactic tree; this bottom-
up process is concerned with integrating each
linguistic element with its governor and forms
a binary syntactic unit. Storage or processing
costs occur when a node has to be retained in
the listener’s working memory before it forms
a dependency with its governor (Gibson, 1998).
This theory has laid the fundations of many
comprehension-oriented metrics.

Conversely, the act of speaking involves
transforming a stratified tree to a horizontal line.
This top-down process is almost like a spreading
activation where the activation of a concept will
spread to neighboring nodes (Hudson, 2010:
74-79). Then each concept can be expressed
and pronounced sequentially on a timeline. The
complexity of this activation procedure is hypoth-
esized and measured by the conceptual distance
between the root of a sentence and some other
nodes.

The major evidence supporting our assumption
is the empirical findings of code-switching by
Eppler (2010; 2011), and Wang and Liu (2013).
They report that the MDD of mixed dependencies
(words from distinct languages) is larger than that
of monolingual ones, suggesting that increased
processing complexity can actually promote code-
switching. These conclusions are drawn from the
studies on German-English and Chinese-English
code-switching. However, Eppler, and Wang
and Liu have only concentrated on investigating
the phenomena from the listener’s perspective in
terms of MDD; they neglect the fact that one of
the major motivations for code-switching is to
lessen a speaker’s production load.4 For instance,
appropriate words or phrases are not instantly
accessible, so the speaker seeks some alternative
expressions in another language to guarantee
continuity in speech. This trade-off relation may
provide a starting point to measure the structural
complexity from the speaker’s perspective.

A stratified syntactic tree can be projected
horizontally, and we record the relative distance
between each node and the root, as shown in
Figure 3. Non-projective sentences can be repre-
sented in the same way. Here, we take the root of
a syntactic tree as a reference point and designate
its projection position as 0; it is the central node

4Some scholars may focus on the social motivations of
code-switching, such as accommodating oneself to a social
group, but the present study tends to emphasize its psycho-
logical property.

and provides critical information about syntactic
constituency (Boland et al., 1990; Trueswell
et al., 1993). The vertical distance between a
node and the root, or the path length traveling
from the root to a certain node along the depen-
dency edges, is defined as “hierarchical distance
(HD)”. For example, the HD of the word China
in Figure 3 is 3, which denotes the vertical dis-
tance or path length between the node and the root.

Figure 3: Projection of a dependency tree in two
dimensions

The average value of all HDs in a sentence is the
mean hierarchical distance (MHD). In this study
we hypothesize that the MHD is a metric for pred-
icating the structural complexity in the hierarchi-
cal dimension. It can be expressed with formula
(2).

MHD =
1

n

n∑

i=1

HDi (2)

According to the formulas (1) and (2), we can
calculate MDD and MHD of the sample sen-
tence in Figure 3. The MDD of this sen-
tence is (1+1+1+1+1+2)/6=1.17 and the MHD
is (2+1+1+2+3+3)/6=2. Note that punctuation
marks are rejected when measuring the MDD and
MHD.

Furthermore, these two metrics can be applied
to measure a text or treebank. To do this, one need
merely average the MDD and the MHD of all the
sentences in the text or treebank, and in so doing
the results represent the MDD and the MHD of
the language at hand. In the following parts, we
use MDD2 and MHD2 to represent the measures
at the textual level. For a text with a specific num-
ber of sentences (s), its MDD2 and MHD2 can be
calculated with (3) and (4), respectively.

164



MDD2 =
1

s

s∑

j=1

MDDj (3)

MHD2 =
1

s

s∑

j=1

MHDj (4)

To sum up, the syntactic structure of language
has two dimensions, which can be reduced to one
dimension by means of orthogonal projections.
Two statistical metrics (MDD and MHD), one for
each dimension, are proposed. These metrics mea-
sure syntactic complexity. To be more specific,
MDD is actually a comprehension-oriented metric
that measures the difficulty of transforming linear
sequences into layered trees, whereas MHD is a
production-oriented metric that measures the com-
plexity of transforming hierarchical structures to
strings of words. These metrics are applicable at
both the sentential and the textual levels. In the
next section, we further investigate the relations
and distributions of MDD and MHD in English
and Czech sentences.

4 Results

Section 3 defined the two metrics, MDD and
MHD, and gave their corresponding formulas for
calculation. In this section, we first calculate the
MDD and MHD of each sentence in English and
Czech, and describe their distributions in nature.
The correlations between sentence length (SL),
MDD, and MHD are then tested. Further, we ex-
tend the two metrics to the textual level, and com-
pare the MDD2 and MHD2 of English and Czech.
Finally, the threshold of the two metrics in both
languages is investigated.

4.1 Asymmetric distributions of MDD and
MHD

Hawkins (2003: 122; 2009: 54) proposed a
Performance-Grammar Correspondence Hypothe-
sis (PGCH),

“grammars have conventionalized syn-
tactic structures in proportion to their
degree of preference in performance, as
evidenced by patterns of selection in
corpora and by ease of processing in
psycholinguistic experiments”.

The PGCH predicts an underlying correlation be-
tween variation data in performance and the fixed

conventions of grammars. In other words, the
more preferred a structure X is, the more produc-
tively grammaticalized it will be, and the easier it
is to process due to the frequency effect (Harley,
1995: 146-148; Hudson, 2010: 193-197).

The patterns of syntactic variation can reflect
the underlying processing efficiency; hence we
first focus on describing the distributions of MDD
and MHD of each sentence in the treebank. Fig-
ure 4 exhibits two positively skewed distributions
of MDD and MHD when the SL (no punctuations)
of each English sentence equals 10. The Pearson’s
moment skewness coefficients (Sk) are 1.31 and
0.78.5 The coefficients indicate that most English
sentences with 10 words get MDD and MHD val-
ues below the mean.

Figure 4: Asymmetric distributions of MDD and
MHD for English sentences (SL=10)

Some other types of English and Czech sen-
tences of different lengths, the frequency of which
is more than 50 times in the treebank, are also
positively skewed in the distribution of MDD and
MHD, as shown in Figure 5. The skewness coef-
ficients of the two metrics of both languages are
all positive, fluctuating around 1, though there is
no significant correlation between SL and Sk. It
appears that the mass of both English and Czech
sentences, of whatever length, tend to have lower

5The Pearson’s moment coefficient of skewness is mea-
sured by the formula (Sk = µ3/µ

3
2
2 ) , where µ2 and µ3 are

the second and third central moments. For a symmetric dis-
tribution, if the data set looks the same to the left and right
of the center point, the skewness value is equal to zero. If
Sk > 0, it is a positive skewing indicating more than half of
the data below the mean, whereas if Sk < 0, it is negatively
skewed with more data above the mean.

165



Figure 5: Relationships between SL and Sk in MDD and MHD

MDD and MHD values. Why are lower MDD
and MHD preferred in both languages? If gram-
mars are assumed to be independent of processing
(Chomsky, 1969), no such consistent asymmetric
distributions of the two metrics in different lan-
guage types would be expected. One possibility
for accounting for the skewness is that syntactic
rules are direct responses to processing ease and
are grammaticalizations of efficiency principles
(Hawkins, 1994: 321). Hence, we can observe
these preferences in two dimensions, and both En-
glish and Czech tend to minimize the MDD and
MHD values. The minimalization of these two
metrics reflects the efficiency principle of human
language.

4.2 Correlations between SL, MDD, and
MHD

Another relevant issue concerning the MDD and
MHD is whether these metrics can predict the
structural complexity for varying sentence lengths
in different languages. Table 3 displays the pos-
itive correlations between SL, MDD and MHD
in English and Czech, and they are all signif-
icantly correlated (p<0.01). Correlation coeffi-
cients (Cor) between SL and MHD in English
and Czech are the highest (0.74 and 0.74, respec-
tively), which is followed by moderate correla-
tions (0.54 and 0.42) between SL and MDD in the
two languages. The MDD and MHD in both lan-
guages are the least correlated with each other, but
they are also significant.

More precisely, we build a linear regression
model to fit the data. The goodness of fit (R2) and

slope (k) can be used to evaluate the model and
predict the increase rate of the two languages. The
R2 between SL and MHD is acceptable at 0.54 and
0.54, while the other two pairs in each language
get pretty low values. The slope of the SL-MHD
fitting line in English (0.09) is slightly lower than
that in Czech (0.12), which suggests the increase
of SL will bring more gains of MHD in Czech than
in English.

We also visualize the relationships between
MDD and MHD of English and Czech sentences
with a scatter plot in Figure 6. Although a large
overlap is shown between MDD and MHD, we
can still observe different extensions in each lan-
guage. If the SL is taken as a moderator variable,
English sentences tend to increase the MDD for
longer sentences, whereas Czech sentences prefer
higher MHD as the SL is increasing. This varia-
tion of preference in different languages can also
be predicted by the above linear model. From the
perspective of language processing, English sen-
tences prefer to enhance the comprehension dif-
ficulty rather than the production cost as the sen-

Lang X-Y Cor p k R2

en
SL-MDD 0.54 <0.01 0.03 0.3
SL-MHD 0.74 <0.01 0.09 0.54
MDD-MHD 0.19 <0.01 0.41 0.04

cs
SL-MDD 0.42 <0.01 0.02 0.18
SL-MHD 0.74 <0.01 0.12 0.54
MDD-MHD 0.11 <0.01 0.36 0.01

Table 3: Correlations between SL, MDD, and
MHD

166



tences get longer; on the contrary, Czech sentences
prefer increasing the structural complexity in hier-
archical dimension, which is assumed to be con-
nected with the production load here.

Figure 6: Relationships between MDD and MHD
of English and Czech sentences

4.3 Trade-off relation between MDD2 and
MHD2

The two metrics can be expanded to measure the
MDD2 and MHD2 of certain languages as well,
and compare the values across different language
types. English and Czech are both mitigated lan-
guages with a subject-verb-object (SVO) word or-
der, but the word order of Czech is relatively un-
restricted, whereas English word order has been
claimed to become rigid due to the loss of case in-
flections (Tesnière, 1959: 33; Vennemann, 1974;
Steele, 1978; Liu, 2010). Due to this high de-
gree of word order variation, it is almost inevitable
for Czech to have more non-projective structures
than English. Will the high percentage of non-
projective dependency relations in Czech enlarge
its MDD2, or will the two metrics even differen-
tiate the syntactic complexity across the two lan-
guages?

Figure 7 represents the MDD2 and MHD2 of
English and Czech. The MDD2 of English is 2.31
and that of Czech is 2.18. These numbers are sim-
ilar to Liu’s (2008) results, which were arrived at
by investigating the MDD2 of twenty languages.
The MHD2 is 3.41 for English and 3.78 for Czech.
All values are below 4. English and Czech both
get a lower MDD2 than MHD2, but the MDD2 of
Czech is slightly lower than that of English, even

though Czech has a much higher percentage of
non-projectivity. Projectivity is of course widely
viewed as a constraint in natural language parsing,
but the number of projectivity violations that actu-
ally occur does not appear to have predictive value
for language processing difficulty in the linear di-
mension.

There seems to be a zero-sum property of the
two metrics in different languages. English gains
a relatively higher MDD2 than Czech but has a
lower MHD2. Conversely, even though the MDD2
of Czech is not as high as that of English, its
MHD2 is greater than that of English. This recip-
rocal relationship is given at the sentential level in
Figure 6, and is also shown at the textual level in
Figure 7. This trade-off relation between the struc-
tural complexity in the two dimensions partially
proves the dynamic balance of code-switching
from the listener’s and speaker’s perspectives.

This also reveals that the weights of the two
metrics are not equal in varying language types.
English tends to reduce the structural complexity
in the hierarchical dimension, while Czech prefers
to lessen the processing cost in the linear dimen-
sion.

Figure 7: MDD2 and MHD2 of English and Czech

4.4 Threshold of MDD2 and MHD2
The two metrics, MDD2 and MHD2, can differ-
entiate the syntactic complexity or difficulty be-
tween English and Czech in each dimension. But
can they reveal any common attribute between
varying languages? Cowan (2001) claimed that a
more precise capacity limit of short-term memory
should be about four chunks on the average, and
Liu (2008) also observed a threshold of MDD2

167



Figure 8: Cumulative average values of MDD2 and MHD2 in English and Czech

for twenty languages at about 4. Does there ex-
ist a universal boundary value in the hierarchical
dimension?

To answer these questions, we make a time-
series plot to characterize real-time variation of
MDD2 and MHD2 in English and Czech, as shown
in Figure 8. Due to a large quantity of sentences,
the horizontal axis of the plot is scaled logarith-
mically. A high degree of variation in MDD2 and
MHD2 is displayed at first, and when more sen-
tences (about 102 sentences) are added in, the cu-
mulative average values become stable in both lan-
guages. In this plot, we can also find that the max-
imum values of MDD2 and MHD2 in the two lan-
guages are below 4,6 though a small part of the
MHD2 value in Czech is above 4. This minor de-
viation is mainly caused by fewer sentences and
some extreme examples. It should be noted that
the corpus used in the present study has a relatively
long mean sentence length (around 24 words per
sentence), and some sentences with fewer words
are also removed, which will, to some extent, en-
large the MDD2 and MHD2 of the two languages.
But a threshold of the MDD2 and MHD2 below 4
is shown as well, and we believe that there do exist
boundary conditions for syntactic structure in the
two dimensions, and the threshold is largely due
to the capacity limits of short-term memory.

Thus, the capacity limit of working memory can
be described in the process of both language com-
prehension and production, and a similar bound-

6The MDD2 for English and Czech is even below 3, but
for another language in Liu’s (2008) study, i.e. Chinese, the
MDD2 was 3.66.

ary value of 4 reflects their internal coherence.

5 Conclusions

We have presented a systematic study of how to
measure the complexity of the syntactic structures
of human languages, extending previous distance-
based theories. Two statistical metrics (MDD
and MHD) have been proposed for predicting the
structural complexity of language, one for each di-
mension. The MDD is comprehension-oriented by
measuring the difficulty of speaking, whereas the
MHD is production-oriented, calculating the cost
of listening. The two metrics are applicable at both
the sentential and the textual levels.

Data from the Czech-English dependency tree-
bank have been used to test and justify our ap-
proach. Some major findings are summarized as
follows. (1) Positive asymmetries in the distribu-
tions of the MDD and MHD are observed in En-
glish and Czech. Both languages prefer to mini-
mize the processing ease in each dimension. (2)
There are significantly positive correlations be-
tween SL, MDD, and MHD. For longer sentences,
English prefers to increase the MDD, while Czech
tends to enhance the MHD. (3) A reciprocal re-
lationship of syntactic complexity in the two di-
mensions is shown between English and Czech,
which indicates an imbalance in weight of MDD2
and MHD2. English tends to reduce the syntactic
complexity in the hierarchical dimension, whereas
Czech prefers to lessen the processing load in the
linear dimension. (4) The threshold of MDD2 and
MHD2 in the two languages is 4 (even below 3 for
the MDD2), which suggests internal coherence for

168



the process of language comprehension and pro-
duction.

More quantitative work is needed for the two
metrics, especially concerning empirical validty in
the arena of psycholinguistics. Furthermore, typo-
logical studies are another potentially useful direc-
tion for exploration.

Acknowledgments

We would like to thank the anonymous review-
ers for their insightful suggestions and comments,
Timothy Osborne for his helpful discussions and
careful proofreading. This work is partly sup-
ported by the National Social Science Foundation
of China (Grant No. 11&ZD188).

References
Julie E. Boland, Michael K. Tanenhaus, and Susan M.

Garnsey. 1990. Evidence for the immediate use
of verb control information in sentence processing.
Journal of Memory and Language, 29(4): 413–432.

Noam Chomsky. 1969. Aspects of the Theory of Syn-
tax. MIT press.

Nelson Cowan. 2001. The magical number 4 in short-
term memory: A reconsideration of mental storage
capacity. Behavioral and Brain Sciences, 24(1): 87–
185.

Eva Eppler. 2010. Emigranto: the syntax of German-
English code-switching. Vienna: Braumüller.

Eva Eppler. 2011. The Dependency Distance Hy-
pothesis for bilingual code-switching. In Proceed-
ings of the International Conference on Dependency
Linguistics, pages 145–154. Barcelona, Spain, 5–7
September.

Christian J. Fiebach, Matthias Schlesewsky, and An-
gela D. Friederici. 2002. Separating syntactic
memory costs and syntactic integration costs during
parsing: The processing of German WH-questions.
Journal of Memory and Language, 47(2): 250–272.

Edward Gibson. 1998. Linguistic complexity: locality
of syntactic dependencies. Cognition, 68(1): 1–76.

Edward Gibson. 2000. The dependency locality the-
ory: a distance-based theory of linguistic complex-
ity. In Alec Marantz, Yasushi Miyashita, and Wayne
O’Neil (eds.), Image, language, brain: papers
from the First Mind Articulation Project Symposium,
pages 95–126. Cambridge, MA: MIT Press.

Jan Hajič, Eva Hajičová, Jarmila Panevová, Petr Sgall,
Silvie Cinková, Eva Fučı́ková, Marie Mikulová, Petr
Pajas, Jan Popelka, Jiřı́ Semecký, Jana Šindlerová,
Jan Štěpánek, Josef Toman, Zdeňka Urešová, and

Zdeněk Žabokrtský. 2012. Prague Czech-English
Dependency Treebank 2.0 LDC2012T08. DVD.
Philadelphia: Linguistic Data Consortium.

Jan Hajič, Eva Hajičová, Jarmila Panevová, Petr
Sgall, Ondřej Bojar, Silvie Cinková, Eva Fučı́ková,
Marie Mikulová, Petr Pajas, Jan Popelka, Jiřı́
Semecký, Jana Šindlerová, Jan Štěpánek, Josef
Toman, Zdeňka Urešová, and Zdeněk Žabokrtský.
2012. Announcing Prague Czech-English Depen-
dency Treebank 2.0. In Proceedings of the Eighth
International Conference on Language Resources
and Evaluation (LREC-2012), pages 3153–3160. Is-
tanbul, Turkey, 21–27, May. European Language
Resources Association (ELRA).

Linguistic Data Consortium. 1999. Penn Treebank 3.
LDC99T42.

Trevor Harley. 1995. The Psychology of Language.
Hove: Psychology Press.

John Hawkins. 1994. A Performance Theory of Order
and Constituency. Cambridge: Cambridge Univer-
sity Press.

John Hawkins. 2003. Efficiency and complexity
in grammars: three general principles. In John C.
Moore, and Maria Polinsky (eds.), The Nature of
Explanation in Linguistic Theory, pages 121–152.
Stanford, Calif: CSLI Publications.

John Hawkins. 2009. Language universals and the
performance-grammar correspondence hypothesis.
In Morten H. Christiansen, Chris Collins, and Shi-
mon Edelman (eds.), Language Universals, pages
54–78. Oxford: Oxford University Press.

David Hays. 1964. Dependency theory: a formalism
and some observations. Language, 40(4): 511–525.

Hans-Jürgen Heringer, Bruno Strecker, and Rainer
Wimmer. 1980. Syntax. Fragen - Lösungen - Al-
ternative. München: Fink.

Richard Hudson. 1984. Word Grammar. Oxford:
Blackwell.

Richard Hudson. 1995. Measuring syntac-
tic difficulty. Unpublished paper. URL
www.phon.ucl.ac.uk/home/dick/papers/difficulty.htm

Richard Hudson. 2010. An Introduction to Word
Grammar. Cambridge: Cambridge University
Press.

Yves Lecerf. 1960. Programme des Conflits, Modèle
des Conflits. Traduction Automatique, 1(4): 11–18;
1(5): 17–36.

Haitao Liu. 2008. Dependency distance as a metric of
language comprehension difficulty. Journal of Cog-
nitive Science, 9(2): 159–191.

Haitao Liu. 2009. Dependency grammar: from theory
to practice. Beijing: Science Press.

169



Haitao Liu. 2010. Dependency direction as a means
of word-order typology: A method based on de-
pendency treebanks. Lingua, 120(6): 1567–1578.

Igor Mel’čuk. 1988. Dependency Syntax: Theory and
Practice. Albany, NY: State University of New York
Press.

Joakim Nivre. 2003. An Efficient Algorithm for Pro-
jective Dependency Parsing. In Proceedings of the
8th International Workshop on Parsing Technologies
(IWPT 03), pages 149–160. Nancy, France, 23–25
April.

Joakim Nivre. 2006. Inductive Dependency Parsing.
Netherlands: Springer.

Timothy Osborne. 2014. Dependency grammar. In
Andrew Carnie, Yosuke Sato, and Daniel Siddiqi
(eds.), The Routledge Handbook of Syntax, pages
604–626. London: Routledge.

Colin Phillips, Nina Kazanina, and Shani H. Abada.
2005. ERP effects of the processing of syntactic
long-distance dependencies. Cognitive Brain Re-
search, 22(3), 407–428.

Jane Robinson. 1970. Dependency structures and
transformational rules. Language, 46(2): 259–285.

Susan Steele. 1978. Word order variation: A typolog-
ical study. In Joseph H. Greenberg, Charles A. Fer-
guson, and Edith A. Moravcsik (eds.), Universals
of human language, vol. 4: Syntax, pages 585–624.
Stanford: Stanford University Press.

Lucien Tesnière. 1959. Éléments de syntaxe struc-
turale. Paris: Klincksieck.

John C. Trueswell, Michael K., and Christopher Kello.
1993. Verb-specific constraints in sentence process-
ing: separating effects of lexical preference from
garden-paths. Journal of Experimental Psychology:
Learning, Memory, and Cognition, 19(3), 528–553.

Theo Vennemann. 1974. Topics, subjects and word or-
der: From SXV to SVX via TVX. In John M Ander-
son, and Charles Jones (eds.), Historical linguistics,
pages 339–376. Amsterdam: North-Holland Pub.
Co.

Lin Wang and Haitao Liu. 2013. Syntactic variations
in Chinese–English code-switching. Lingua, 123:
58-73.

Victor Yngve. 1960. A model and an hypothesis for
language structure. In Proceedings of the American
philosophical society, pages 444–466. 17, October.
Philadelphia: American Philosophical Society.

Victor Yngve. 1996. From grammar to science: New
foundations for general linguistics. Amsterdam &
Philadelphia: John Benjamins.

170


