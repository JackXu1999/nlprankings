



















































Predicting Power Relations between Participants in Written Dialog from a Single Thread


Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 339–344,
Baltimore, Maryland, USA, June 23-25 2014. c©2014 Association for Computational Linguistics

Predicting Power Relations between Participants
in Written Dialog from a Single Thread

Vinodkumar Prabhakaran
Dept. of Computer Science

Columbia University, New York, NY

vinod@cs.columbia.edu

Owen Rambow
Cntr. for Comp. Learning Systems

Columbia University, New York, NY

rambow@ccls.columbia.edu

Abstract

We introduce the problem of predicting
who has power over whom in pairs of peo-
ple based on a single written dialog. We
propose a new set of structural features.
We build a supervised learning system to
predict the direction of power; our new
features significantly improve the results
over using previously proposed features.

1 Introduction

Computationally analyzing the social context in
which language is used has gathered great interest
within the NLP community recently. One of the
areas that has generated substantial research is the
study of how social power relations between peo-
ple affect and/or are revealed in their interactions
with one another. Researchers have proposed sys-
tems to detect social power relations between par-
ticipants of organizational email threads (Bramsen
et al., 2011; Gilbert, 2012; Prabhakaran and Ram-
bow, 2013), online forums (Danescu-Niculescu-
Mizil et al., 2012; Biran et al., 2012; Danescu-
Niculescu-Mizil et al., 2013), chats (Strzalkowski
et al., 2012), and off-line interactions such as pres-
idential debates (Prabhakaran et al., 2013; Nguyen
et al., 2013). Automatically identifying power and
influence from interactions can have many prac-
tical applications ranging from law enforcement
and intelligence to online marketing.

A significant number of these studies are per-
formed in the domain of organizational email
where there is a well defined notion of power (or-
ganizational hierarchy). Bramsen et al. (2011) and
Gilbert (2012) predict hierarchical power relations
between people in the Enron email corpus using
lexical features extracted from all the messages
exchanged between them. However, their ap-
proaches primarily apply to situations where large
collections of messages exchanged between pairs

of people are available. In (Prabhakaran and Ram-
bow, 2013), we introduced the problem of detect-
ing whether a participant of an email thread has
power over someone else in the thread and estab-
lished the importance of dialog structure in that
task. However, in that work we did not detect over
whom that person has power.

In this paper, we introduce a new problem for-
mulation. We predict the hierarchical power rela-
tion between pairs of participants in an email in-
teraction thread based solely on features extracted
from that thread. As a second major contribution,
we introduce a new set of features to capture as-
pects of participant behavior such as responsive-
ness, and we show that these features are signifi-
cantly correlated with the direction of power. We
present a fully automatic system for this task ob-
taining an accuracy of 73.0%, an improvement of
6.9% over 68.3% by a system using only lexical
features. This best-performing system uses our
new feature set.

2 Motivation

Early NLP-based approaches such as Bramsen et
al. (2011) and Gilbert (2012) built systems to pre-
dict hierarchical power relations between people
in the Enron email corpus using lexical features
from all the messages exchanged between them.
One limitation of this approach is that it relies
solely on lexical cues and hence works best when
large collections of messages exchanged between
the pairs of people are available. For example,
Bramsen et al. (2011) excluded sender-recipient
pairs who exchanged fewer than 500 words from
their evaluation set, since they found smaller text
samples are harder to classify. By taking the mes-
sage out of the context of the interaction in which
it was exchanged, they fail to utilize cues from the
structure of interactions, which complements the
lexical cues in detecting power relations, as we
showed in (Prabhakaran and Rambow, 2013).

339



We modeled the problem of detecting power re-
lationships differently in (Prabhakaran and Ram-
bow, 2013): we predicted whether a participant
in an email thread has a certain type of power
or not. However, in that work we did not pre-
dict over whom he/she has that power. This
may result in noisy features; consider a thread in
which participant X has power over participant
Y , who has power over participant Z . By ag-
gregating features over all messages sent by Y ,
features salient to a subordinate-superior interac-
tion are incorrectly conflated with those salient to
superior-subordinate interaction. Another limita-
tion of (Prabhakaran and Rambow, 2013) is that
we used manual annotations for many of our fea-
tures such as dialog acts and overt displays of
power. Relying on manual annotations for features
limited our analysis to a small subset of the Enron
corpus, which has only 18 instances of hierarchi-
cal power. Consequently, our findings with respect
to hierarchical power were weak in terms of both
correlations of features and system performance.

In this paper, we introduce the problem of pre-
dicting who has power over whom in pairs of inter-
acting participants based on a single thread of in-
teractions. From (Bramsen et al., 2011) we retain
the idea that we want to predict the power relation
between pairs of people. But in contrast to their
formulation, we retain the goal from (Prabhakaran
and Rambow, 2013) that we want to study com-
munication in the context of an interaction, and
that we want to be able to make predictions us-
ing only the emails exchanged in a single thread.
Like (Prabhakaran and Rambow, 2013), we use
features to capture the dialog structure, but we use
automatic taggers to generate them and assume no
manual annotation at all at training or test time.
This allows us to use the entire Enron email cor-
pus for this study.

3 Data

In this work, we use the version of Enron email
corpus by Yeh and Harnly (2006) which captures
the thread structure of email exchanges. The cor-
pus contains 36,615 email threads. We excluded a
small subset of 419 threads that was used for pre-
vious manual annotation efforts, part of which was
also used to train the DA and ODP taggers (Sec-
tion 5) that generate features for our system. The
average number of email messages per thread was
around 3. We divided the remaining threads into

train (50%), dev (25%) and test (25%) sets by ran-
dom sampling. We then applied various basic NLP
preprocessing steps such as tokenization, POS tag-
ging and lemmatization to the body of email mes-
sages. We use the Enron gold organizational hier-
archy released by Agarwal et al. (2012) to model
hierarchical power. Their corpus was manually
built using information from Enron organizational
charts. It includes relations of 1,518 employees
and captures dominance relations between 13,724
pairs of them. Theirs is the largest such data set
available to the best of our knowledge.

4 Problem Formulation

Let t denote an email thread and Mt denote the
set of all messages in t . Also, let Pt be the set
of all participants in t , i.e., the union of senders
and recipients (To and CC) of all messages in
Mt . We are interested in detecting power rela-
tions between pairs of participants who interact
within a given email thread. Not every pair of par-
ticipants (p1 , p2 ) ∈ Pt × Pt interact with one an-
other within t . Let IMt(p1 , p2 ) denote the set of
Interaction Messages — non-empty messages in
t in which either p1 is the sender and p2 is one
of the recipients or vice versa. We call the set of
(p1 , p2 ) such that |IMt(p1 , p2 )| > 0 the interact-
ing participant pairs of t (IPPt ).

We focus on the manifestations of power in in-
teractions between people across different levels
of hierarchy. For every (p1 , p2 ) ∈ IPPt , we query
the set of dominance relations in the gold hierar-
chy to determine their hierarchical power relation
(HP(p1 , p2 )). We exclude pairs that do not exist
in the gold hierarchy from our analysis and denote
the remaining set of related interacting participant
pairs as RIPPt . We assign HP(p1 , p2 ) to be su-
perior if p1 dominates p2 , and subordinate if p2
dominates p1 . Table 1 shows the total number of
pairs in IPPt and RIPPt from all the threads in
our corpus and across train, dev and test sets.

Description Total Train Dev Test
# of threads 36,196 18,079 8,973 9,144∑

t |IPPt | 355,797 174,892 91,898 89,007∑
t |RIPPt | 15,048 7,510 3,578 3,960

Table 1: Data Statistics
Row 1 presents the total number of threads in different

subsets of the corpus. Row 2 and 3 present the number of
interacting participant pairs (IPP ) and related interacting

participant pairs (RIPP ) in those subsets.

340



Given a thread t and a pair of participants
(p1 , p2 ) ∈ RIPPt , we want to automatically de-
tect HP(p1 , p2 ). This problem formulation is
similar to the ones in (Bramsen et al., 2011) and
(Gilbert, 2012). However, the difference is that for
us an instance is a pair of participants in a single
thread of interaction (which may or may not in-
clude other people), whereas for them an instance
constitutes all messages exchanged between a pair
of people in the entire corpus. Our formula-
tion also differs from (Prabhakaran and Rambow,
2013) in that we detect power relations between
pairs of participants, instead of just whether a par-
ticipant had power over anyone in the thread.

5 Structural Analysis

In this section we analyze various features that
capture the structure of interaction between the
pairs of participants in a thread. Each feature f
is extracted with respect to a person p over a ref-
erence set of messages M (denoted f pM ). For a
pair (p1 , p2 ), we extract 4 versions of each fea-
ture f : f p1IMt (p1 ,p2 ), f

p2
IMt (p1 ,p2 )

, f p1Mt and f
p2
Mt

. The
first two capture behavior of the pair among them-
selves, while the third and fourth capture their
overall behavior in the entire thread. We group our
features into three categories — THRNew, THRPR

and DIAPR. THRNew is a set of new features we
propose, while THRPR and DIAPR incorporate fea-
tures we proposed in (Prabhakaran and Rambow,
2013). THRNew and THRPR capture the structure
of message exchanges without looking at the con-
tent of the emails (e.g., how many emails did a per-
son send), while DIAPR captures the pragmatics of
the dialog and requires an analysis of the content
of the emails (e.g., did they issue any requests).

THRNew: This is a new set of features we in-
troduce in this paper. It includes the average num-
ber of recipients (AvgRecipients) and To recipients
(AvgToRecipients) in emails sent by p, the per-
centage of emails p received in which he/she was
in the To list (InToList%), boolean features de-
noting whether p added or removed people when
responding to a message (AddPerson and Re-
movePerson), average number of replies received
per message sent by p (ReplyRate) and average
number of replies received from the other person
of the pair to messages where he/she was a To re-
cipient (ReplyRateWithinPair). ReplyRateWithin-
Pair applies only to IMt(p1 , p2 ).

THRPR: This feature set includes two meta-

data based feature sets — positional and verbosity.
Positional features include a boolean feature to de-
note whether p sent the first message (Initiate),
and relative positions of p’s first and last messages
(FirstMsgPos and LastMsgPos) in M . Verbosity
features include p’s message count (MsgCount),
message ratio (MsgRatio), token count (Token-
Count), token ratio (TokenRato) and tokens per
message (TokenPerMsg), all calculated over M .

DIAPR: In (Prabhakaran and Rambow, 2013),
we used dialog features derived from manual an-
notations — dialog acts (DA) and overt displays
of power (ODP) — to model the structure of inter-
actions within the message content. In this work,
we obtain DA and ODP tags on the entire cor-
pus using automatic taggers trained on those man-
ual annotations. The DA tagger (Omuya et al.,
2013) obtained an accuracy of 92%. The ODP
tagger (Prabhakaran et al., 2012) obtained an ac-
curacy of 96% and F-measure of 54%. The DA
tagger labels each sentence to be one of the 4
dialog acts: Request Action, Request Informa-
tion, Inform, and Conventional. The ODP Tag-
ger identifies sentences (mostly requests) that ex-
press additional constraints on its response, be-
yond those introduced by the dialog act. We use
5 features: ReqAction%, ReqInform%, Inform%,
Conventional%, and ODP% to capture the per-
centage of sentences in messages sent by p that has
each of these labels. We also use a feature to cap-
ture the number of p’s messages with a request that
did not get a reply, i.e., dangling requests (Dan-
glingReq%), over all messages sent by p.

We perform an unpaired two-sample two-tailed
Student’s t-Test comparing mean values of each
feature for subordinates vs. superiors. For our
analysis, a data point is a related interacting pair,
and not a message. Hence, a message with mul-
tiple recipients who have a superior/subordinate
relation with the sender will contribute to features
for multiple data points. We limit our analysis to
the related interacting pairs from only our train
set. Table 2 presents mean values of features for
subordinates and superiors at the interaction level.
Thread level versions of these features also ob-
tained similar results overall in terms of direction
of difference and significance. We denote three
significance levels — * (p < .05 ), ** (p < .01 ),
and *** (p < .001 ). To control false discovery
rates in multiple testing, we adjusted the p-values
(Benjamini and Hochberg, 1995). We summarize

341



Feature Name Mean(f subIMt ) Mean(f
sup
IMt

)

THRNew

AvgRecipients∗∗∗ 21.14 43.10
AvgToRecipients∗∗∗ 18.19 38.94
InToList% 0.82 0.80
ReplyRate∗∗∗ 0.86 1.23
ReplyRateWithinPair∗∗∗ 0.16 0.10
AddPerson 0.48 0.47
RemovePerson∗∗∗ 0.41 0.37

THRPR

Initiate∗∗∗ 0.45 0.56
FirstMsgPos 0.04 0.03
LastMsgPos∗∗∗ 0.15 0.11
MsgCount∗∗∗ 0.64 0.70
MsgRatio∗∗∗ 0.44 0.56
TokenCount 91.22 83.26
TokenRatio∗∗∗ 0.45 0.55
TokenPerMsg∗ 140.60 120.87

DIAPR

Conventional%∗∗∗ 0.15 0.17
Inform%∗∗∗ 0.78 0.72
ReqAction%∗∗∗ 0.02 0.04
ReqInform%∗∗∗ 0.05 0.06
DanglingReq%∗∗∗ 0.12 0.15
ODP%∗∗∗ 0.03 0.06

Table 2: Student’s t-Test Results of fpIMt .
THRNew: new meta-data features; THRPR, DIAPR: meta-data

and dialog-act features from previous studies;
* (p < .05 ); ** (p < .01 ); *** (p < .001 )

the main findings on the significant features below.

1. Superiors send messages addressed to more
people (AvgRecipients and AvgToRecipi-
ents). Consequently, they get more replies to
their messages (ReplyRate). However, con-
sidering messages where the other person of
the pair is addressed in the To list (ReplyRate-
WithinPair), subordinates get more replies.

2. Superiors issue more requests (ReqAction%
and ReqInform%) and overt displays of
power (ODP%). Subordinates issue more
informs (Inform%) and, surprisingly, have
fewer unanswered requests (DanglingReq%).

3. Superiors initiate the interactions more often
than subordinates (Initiate). They also leave
interactions earlier (LastMsgPos).

4. Superiors send shorter messages (Token-
PerMsg). They also send more messages
(MsgCount & MsgRatio) and even contribute
a higher ratio of tokens in the thread (Token-
Ratio) despite sending shorter messages.

Finding 1 goes in line with findings from stud-
ies analyzing social networks that superiors have
higher connectivity in the networks that they are
part of (Rowe et al., 2007). Intuitively, those who
have higher connectivity also send emails to larger
number of people, and hence our result. Since su-
periors address more people in their emails, they
also have a higher chance of getting replies. Find-
ing 2 also aligns with the general intuition about
how superiors and subordinates behave within in-
teractions (e.g., superiors exhibit more overt dis-
plays of power than subordinates).

Findings 3 & 4 are interesting since they re-
veal special characteristics of threads involving hi-
erarchically related participants. In (Prabhakaran
and Rambow, 2013), we had found that persons
with hierarchical power rarely initiated threads
and contributed less within the threads. But that
problem formulation was different — we were
identifying whether a person in a given thread had
hierarchical power over someone else or not. The
data points in that formulation included partici-
pants from threads that did not have any hierar-
chically related people, whereas our current for-
mulation do not. These findings suggest that if a
person starts an email thread, he’s likely not to be
the one who has power, but if a thread includes a
pair of people who are hierarchically related, then
it is likely to be initiated by the superior and he/she
tends to contribute more in such threads.

6 Predicting Direction of Power

We build an SVM-based supervised learning sys-
tem that can predict HP(p1 , p2 ) to be either su-
perior or subordinate based on the interaction
within a thread t for any pair of participants
(p1 , p2 ) ∈ RIPPt . We deterministically fix the
order of participants in (p1 , p2 ) such that p1 is the
sender of the first message in IMt(p1 , p2 ). We
use the ClearTK (Ogren et al., 2008) wrapper for
SVMLight (Joachims, 1999) in our experiments.
We use the related interacting participant pairs in
threads from the train set to train our models and
optimize our performance on those from the dev
set. We report results obtained on dev and test sets.

In our formulation, values of many features are
undefined for some instances (e.g., Inform% is un-
defined when MsgCount = 0). Handling of unde-
fined values for features in SVM is not straight-
forward. Most SVM implementations assume the
value of 0 by default in such cases, conflating them

342



Description Accuracy
Baseline (Always Superior) 52.54
Baseline (Word Unigrams + Bigrams) 68.56
THRNew 55.90
THRPR 54.30
DIAPR 54.05
THRPR + THRNew 61.49
DIAPR + THRPR + THRNew 62.47
LEX 70.74
LEX + DIAPR + THRPR 67.44
LEX + DIAPR + THRPR + THRNew 68.56
BEST (= LEX + THRNew) 73.03
BEST (Using p1 features only) 72.08
BEST (Using IMt features only) 72.11
BEST (Using Mt only) 71.27
BEST (No Indicator Variables) 72.44

Table 3: Accuracies on feature subsets (dev set).
THRNew: new meta-data features; THRPR, DIAPR: meta-data
and dialog-act features from previous studies; LEX: ngrams;

BEST: best subset; IMt stands for IMt(p1, p2)

with cases where Inform% is truly 0. In order to
mitigate this issue, we use an indicator feature for
each structural feature to denote whether or not it
is valid. Since we use a quadratic kernel, we ex-
pect the SVM to pick up the interaction between
each feature and its indicator feature.

Lexical features have already been shown to be
valuable in predicting power relations (Bramsen
et al., 2011; Gilbert, 2012). We use another fea-
ture set LEX to capture word ngrams, POS (part
of speech) ngrams and mixed ngrams. A mixed
ngram (Prabhakaran et al., 2012) is a special case
of word ngram where words belonging to open
classes are replaced with their POS tags. We found
the best setting to be using both unigrams and bi-
grams for all three types of ngrams, by tuning in
our dev set. We then performed experiments using
all subsets of {LEX, THRNew, THRPR, DIAPR }.

Table 3 presents the results obtained using var-
ious feature subsets. We use a majority class
baseline assigning HP(p1 , p2 ) to be always su-
perior, which obtains 52.5% accuracy. We also
use a stronger baseline using word unigrams and
bigrams as features, which obtained an accuracy
of 68.6%. The performance of the system using
each structural feature class on its own is very
low. Combining all three of them improves the
accuracy to 62.5%. The highest performance ob-
tained without using any message content is for
THRPR and THRNew (61.5%). LEX features by

itself obtain a very high accuracy of 70.7%, con-
firming the importance of lexical patterns in this
task. Perplexingly, adding all structural features to
LEX reduces the accuracy by around 2.2 percent-
age points. The best performing system (BEST)
uses LEX and THRNew features and obtains an
accuracy of 73.0%, a statistically significant im-
provement over the LEX-only system (McNemar).

We also performed an ablation study to under-
stand the importance of different slices of our fea-
ture sets. If we remove all feature versions with
respect to the second person, the accuracy drops
to 72.1%. This suggests that features about the
other person’s behavior also help the prediction
task. If we remove either the thread level versions
of features or interaction level versions of features,
the accuracy again drops, suggesting that both the
pair’s behavior among themselves, and their over-
all behavior in the thread add value to the predic-
tion task. Removing the indicator feature denot-
ing the structural features’ validity also reduces
the performance of the system.

We now discuss evaluation on our blind test set.
The majority baseline (Always Superior) for ac-
curacy is 55.0%. The word unigrams and bigrams
baseline obtains an accuracy of 68.3%. The LEX
system (using other forms of ngrams as well) ob-
tains a slightly lower accuracy of 68.1%. Our
BEST system using LEX and THRNew features
obtains an accuracy of 73.0% (coincidentally the
same as on the dev set), an improvement of 6.9%
over the system using only lexical features.

7 Conclusion

We introduced the problem of predicting who has
power over whom based on a single thread of writ-
ten interactions. We introduced a new set of fea-
tures which describe the structure of the dialog.
Using this feature set, we obtain an accuracy of
73.0% on a blind test. In future work, we will
tackle the problem of three-way classification of
pairs of participants, which will cover cases in
which they are not in a power relation at all.

Acknowledgments

This paper is based upon work supported by the
DARPA DEFT Program. The views expressed are
those of the authors and do not reflect the official
policy or position of the Department of Defense
or the U.S. Government. We also thank several
anonymous reviewers for their feedback.

343



References
Apoorv Agarwal, Adinoyi Omuya, Aaron Harnly, and

Owen Rambow. 2012. A Comprehensive Gold
Standard for the Enron Organizational Hierarchy. In
Proceedings of the 50th Annual Meeting of the ACL
(Short Papers), pages 161–165, Jeju Island, Korea,
July. Association for Computational Linguistics.

Yoav Benjamini and Yosef Hochberg. 1995. Control-
ling the false discovery rate: a practical and pow-
erful approach to multiple testing. Journal of the
Royal Statistical Society. Series B (Methodological),
pages 289–300.

Or Biran, Sara Rosenthal, Jacob Andreas, Kathleen
McKeown, and Owen Rambow. 2012. Detecting
influencers in written online conversations. In Pro-
ceedings of the Second Workshop on Language in
Social Media, pages 37–45, Montréal, Canada, June.
Association for Computational Linguistics.

Philip Bramsen, Martha Escobar-Molano, Ami Patel,
and Rafael Alonso. 2011. Extracting social power
relationships from natural language. In Proceedings
of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 773–782, Portland, Oregon, USA,
June. Association for Computational Linguistics.

Cristian Danescu-Niculescu-Mizil, Lillian Lee,
Bo Pang, and Jon Kleinberg. 2012. Echoes of
power: language effects and power differences in
social interaction. In Proceedings of the 21st in-
ternational conference on World Wide Web, WWW
’12, New York, NY, USA. ACM.

Cristian Danescu-Niculescu-Mizil, Moritz Sudhof,
Dan Jurafsky, Jure Leskovec, and Christopher Potts.
2013. A computational approach to politeness with
application to social factors. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
250–259, Sofia, Bulgaria, August. Association for
Computational Linguistics.

Eric Gilbert. 2012. Phrases that signal workplace hier-
archy. In Proceedings of the ACM 2012 conference
on Computer Supported Cooperative Work, CSCW
’12, pages 1037–1046, New York, NY, USA. ACM.

Thorsten Joachims. 1999. Making Large-Scale SVM
Learning Practical. In Bernhard Schölkopf, Christo-
pher J.C. Burges, and A. Smola, editors, Advances
in Kernel Methods - Support Vector Learning, Cam-
bridge, MA, USA. MIT Press.

Viet-An Nguyen, Jordan Boyd-Graber, Philip Resnik,
Deborah A. Cai, Jennifer E. Midberry, and Yuanxin
Wang. 2013. Modeling topic control to detect in-
fluence in conversations using nonparametric topic
models. Machine Learning, pages 1–41.

Philip V. Ogren, Philipp G. Wetzler, and Steven
Bethard. 2008. ClearTK: A UIMA toolkit for sta-
tistical natural language processing. In Towards

Enhanced Interoperability for Large HLT Systems:
UIMA for NLP workshop at Language Resources
and Evaluation Conference (LREC).

Adinoyi Omuya, Vinodkumar Prabhakaran, and Owen
Rambow. 2013. Improving the quality of minor-
ity class identification in dialog act tagging. In Pro-
ceedings of the 2013 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
802–807, Atlanta, Georgia, June. Association for
Computational Linguistics.

Vinodkumar Prabhakaran and Owen Rambow. 2013.
Written dialog and social power: Manifestations of
different types of power in dialog behavior. In Pro-
ceedings of the IJCNLP, pages 216–224, Nagoya,
Japan, October. Asian Federation of Natural Lan-
guage Processing.

Vinodkumar Prabhakaran, Owen Rambow, and Mona
Diab. 2012. Predicting Overt Display of Power in
Written Dialogs. In Human Language Technolo-
gies: The 2012 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, Montreal, Canada, June. Associ-
ation for Computational Linguistics.

Vinodkumar Prabhakaran, Ajita John, and Dorée D.
Seligmann. 2013. Who had the upper hand? rank-
ing participants of interactions based on their rela-
tive power. In Proceedings of the IJCNLP, pages
365–373, Nagoya, Japan, October. Asian Federation
of Natural Language Processing.

Ryan Rowe, German Creamer, Shlomo Hershkop, and
Salvatore J. Stolfo. 2007. Automated social hier-
archy detection through email network analysis. In
Proceedings of the 9th WebKDD and 1st SNA-KDD
2007 workshop on Web Mining and Social Network
Anal. ACM.

Tomek Strzalkowski, Samira Shaikh, Ting Liu,
George Aaron Broadwell, Jenny Stromer-Galley,
Sarah Taylor, Umit Boz, Veena Ravishankar, and
Xiaoai Ren. 2012. Modeling leadership and influ-
ence in multi-party online discourse. In Proceedings
of COLING, pages 2535–2552, Mumbai, India, De-
cember. The COLING 2012 Organizing Committee.

Jen-Yuan Yeh and Aaron Harnly. 2006. Email thread
reassembly using similarity matching. In CEAS
2006 - The Third Conference on Email and Anti-
Spam, July 27-28, 2006, Mountain View, California,
USA, Mountain View, California, USA, July.

344


