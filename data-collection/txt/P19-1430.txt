



















































Chinese Relation Extraction with Multi-Grained Information and External Linguistic Knowledge


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4377–4386
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

4377

Chinese Relation Extraction with Multi-Grained Information and
External Linguistic Knowledge

Ziran Li1,2∗ , Ning Ding1,2∗, Zhiyuan Liu2,3,4, Hai-Tao Zheng1,2† , Ying Shen5
1Tsinghua Shenzhen International Graduate School, Tsinghua University

2Department of Computer Science and Technology, Tsinghua University, Beijing, China
3Institute for Artificial Intelligence, Tsinghua University, Beijing, China

4State Key Lab on Intelligent Technology and Systems, Tsinghua University, Beijing, China
5School of Electronics and Computer Engineering, Peking University Shenzhen Graduate School

{lizr18,dingn18}@mails.tsinghua.edu.cn

Abstract

Chinese relation extraction is conducted using
neural networks with either character-based or
word-based inputs, and most existing meth-
ods typically suffer from segmentation errors
and ambiguity of polysemy. To address the is-
sues, we propose a multi-grained lattice frame-
work (MG lattice) for Chinese relation ex-
traction to take advantage of multi-grained
language information and external linguistic
knowledge. In this framework, (1) we incorpo-
rate word-level information into character se-
quence inputs so that segmentation errors can
be avoided. (2) We also model multiple senses
of polysemous words with the help of external
linguistic knowledge, so as to alleviate poly-
semy ambiguity. Experiments on three real-
world datasets in distinct domains show con-
sistent and significant superiority and robust-
ness of our model, as compared with other
baselines. The source code of this paper can
be obtained from https://github.com/
thunlp/Chinese_NRE.

1 Introduction

Relation extraction (RE) has a pivotal role in infor-
mation extraction (IE), aiming to extract seman-
tic relations between entity pairs in natural lan-
guage sentences. In downstream applications, this
technology is a key module for constructing large-
scale knowledge graphs. Recent developments in
deep learning have heightened the interest for neu-
ral relation extractions (NRE), which attempt to
use neural networks to automatically learn seman-
tic features (Liu et al., 2013; Zeng et al., 2014,
2015; Lin et al., 2016; Zhou et al., 2016; Jiang
et al., 2016).

∗ indicates equal contribution
† Corresponding author: Hai-Tao Zheng. ( E-mail:

zheng.haitao@sz.tsinghua.edu.cn )

Figure 1: An example of segmentation ambiguity and
polysemy ambiguity in Chinese RE.

Although it is not necessary for NRE to per-
form feature engineering, they ignore the fact
that different language granularity of input will
have a significant impact on the model, especially
for Chinese RE. Conventionally, according to the
difference in granularity, most existing methods
for Chinese RE can be divided into two types:
character-based RE and word-based RE.

For the character-based RE, it regards each in-
put sentence as a character sequence. The short-
coming of this kind of method is that it can-
not fully exploit word-level information, capturing
fewer features than the word-based methods. For
the word-based RE, word segmentation should be
first performed. Then, a word sequence is derived
and fed into the neural network model. However,
the performance of the word-based models could
be significantly impacted by the quality of seg-
mentation.

For example, as shown in Fig 1, the Chinese
sentence “达尔文研究所有杜鹃 (Darwin studies
all the cuckoos)” has two entities, which are “达
尔文 (Darwin)” and “杜鹃 (cuckoos)”, and the
relation between them is Study. In this case, the

https://github.com/thunlp/Chinese_NRE
https://github.com/thunlp/Chinese_NRE


4378

correct segmentation is “达尔文 (Darwin) /研究
(studies) /所有 (all the) /杜鹃 (cuckoos)” . Nev-
ertheless, semantics of the sentence could become
entirely different as the segmentation changes. If
the segmentation is “达尔文 (In Darwin) /研究所
(institute) / 有 (there are) / 杜鹃 (cuckoos)”, the
meaning of the sentence becomes ’there are cuck-
oos in Darwin institute’ and the relation between
“达尔文 (Darwin)” and “杜鹃 (cuckoos)” turns
into Ownership, which is wrong. Hence, neither
character-based methods nor word-based methods
can sufficiently exploit the semantic information
in data. Worse still, this problem becomes sev-
erer when datasets is finely annotated, which are
scarce in number. Obviously, to discover high-
level entity relationships from plain texts, we need
the assistance of comprehensive information with
various granularity.

Furthermore, the fact that there are many pol-
ysemous words in datasets is another point ne-
glected by existing RE models, which limits the
ability of the model to explore deep semantic fea-
tures. For instance, the word “杜鹃” has two
different senses, which are ’cuckoos’ and ’aza-
leas’. But it’s difficult to learn both senses infor-
mation from plain texts without the help of exter-
nal knowledge. Therefore, the introduction of ex-
ternal linguistic knowledge will be of great help to
NRE models.

In this paper, we proposed the multi-granularity
lattice framework (MG lattice), a unified model
comprehensively utilizes both internal informa-
tion and external knowledge, to conduct the Chi-
nese RE task. (1) The model uses a lattice-based
structure to dynamically integrate word-level fea-
tures into the character-based method. Thus, it
can leverage multi-granularity information of in-
puts without suffering from segmentation errors.
(2) Moreover, to alleviate the issue of polysemy
ambiguity, the model utilizes HowNet (Dong and
Dong, 2003), which is an external knowledge base
manually annotates polysemous Chinese words.
Then, the senses of words are automatically se-
lected during the training stage and consequently,
the model can fully exploit the semantic informa-
tion in data for better RE performance.

Sets of experiments has been conducted on
three manually labeled RE datasets. The results
indicate that our model significantly outperforms
multiple existing methods, achieving state-of-the-
art results on various datasets across different do-

mains.

2 Related Work

Recent years RE, especially NRE, has been widely
studied in the NLP field. As a pioneer, (Liu et al.,
2013) proposed a simple CNN RE model and it is
regarded as one seminal work that uses a neural
network to automatically learn features. On this
basis, (Zeng et al., 2014) developed a CNN model
with max-pooling, where positional embeddings
were first used to represent the position informa-
tion. Then the PCNNs model (Zeng et al., 2015)
designed the multi-instance learning paradigm for
RE. However, the PCNNs model suffers the issue
of the selection of sentences. To address the prob-
lem, Lin et al. (2016) applied the attention mech-
anism over all the instances in the bag. Further,
Jiang et al. (2016) proposed a model with multi-
instance and multi-label paradigms. Although PC-
NNs models are more efficient, they cannot exploit
contextual information like RNNs. Hence, LSTM
with attention mechanism was also applied to the
RE task (Zhang and Wang, 2015; Zhou et al.,
2016; Lee et al., 2019).

Existing methods for Chinese RE are mostly
character-based or word-based implementations of
mainstream NRE models (Chen and Hsu, 2016;
Rönnqvist et al., 2017; ZHANG et al., 2017; Xu
et al., 2017). In most cases, these methods only
focus on the improvement of the model itself, ig-
noring the fact that different granularity of input
will have a significant impact on the RE mod-
els. The character-based model can not utilize
the information of words, capturing fewer fea-
tures than the word-based model. On the other
side, the performance of the word-based model is
significantly impacted by the quality of segmen-
tation (Zhang and Yang, 2018). Although some
methods are used to combine character-level and
word-level information in other NLP tasks like
character-bigrams (Chen et al., 2015; Yang et al.,
2017) and soft words (Zhao and Kit, 2008; Chen
et al., 2014; Peng and Dredze, 2016), the informa-
tion utilization is still very limited.

Then, tree-structured RNNs was proposed to
address the shortcomings. Tai et al. (2015) pro-
posed a tree-like LSTM model to improve the
semantic representation. This type of structure
has been applied into various tasks, including hu-
man action recognition (Sun et al., 2017), NMT
encoders (Su et al., 2017), speech tokenization



4379

(Sperber et al., 2017) and NRE (Zhang and Yang,
2018). Although the lattice LSTM model can ex-
ploit word and word sequence information, it still
could be severely affected by the ambiguity of pol-
ysemy. In other words, these models cannot han-
dle the polysemy of words with the change of lan-
guage situation. Therefore, the introduction of
external linguistic knowledge is very necessary.
We utilize sense-level information with the help
of HowNet proposed by Dong and Dong (2003),
which is a concept knowledge base that annotates
Chinese with correlative word senses. In addition,
the open-sourced HowNet API (Qi et al., 2019) is
also used in our work.

3 Methodology

Given a Chinese sentence and two marked enti-
ties in it, the task of Chinese relation extraction
is to extract semantic relations between the two
entities. In this section, we present our MG lat-
tice model for Chinese relation extraction in detail.
As shown in Fig 2, the model could be introduced
from three aspects:

Figure 2: MG lattice framework. 1

Input Representation. Given a Chinese sen-
tence with two target entities as input, this part rep-
resents each word and character in the sentence.
Then the model can utilize both word-level and
character-level information.

MG Lattice Encoder. Incorporating external
knowledge into word sense disambiguation, this

1In order to keep the figure clear and concise, we do not
show gate cells and the backward direction.

part uses a lattice-structure LSTM network to con-
struct a distributed representation for each input
instance.

Relation Classifier. After the hidden states are
learned, a character-level mechanism is adapted to
merge features. Then the final sentence represen-
tations are fed into a softmax classifier to predict
relations.

We will introduce all the three parts in the fol-
lowing subsections in detail.

3.1 Input Representation

The input of our model is a Chinese sentence s
with two marked entities. In order to utilize multi-
granularity information, we represent both charac-
ters and words in the sentence.

3.1.1 Character-level Representation
Our model takes character-based sentences as di-
rect inputs, that is, regarding each input sentence
as a character sequence. Given a sentence s con-
sisting of M characters s = {c1, ..., cM}, we first
map each character ci to a vector of dc dimensions,
denoted as xcei ∈ Rd

c
, via the Skip-gram model

(Mikolov et al., 2013).
In addition, we leverage position embeddings to

specify entity pairs, which are defined as the rel-
ative distances from the current character to head
and tail entities (Zeng et al., 2014). Specifically,
the relative distances from the i-th character ci to
the two marked entities are denoted as p1i and p

2
i

respectively. We calculate p1i as below:

p1i =


i− b1 i < b1,
0 b1 ≤ i ≤ e1,
i− e1 i > e1,

(1)

where b1 and e1 are the start and end indices of
the head entity. The computation of p2i is similar
to Eq. 1. Then, p1i and p

2
i are transformed into two

corresponding vectors, denoted as xp1i ∈ Rd
p

and
xp2i ∈ Rd

p
, by looking up a position embedding

table.
Finally, the input representation for character ci,

denoted as xci ∈ Rd (d = dc+2×dp), is concate-
nated by character embedding xcei , position em-
beddings xp1i and x

p2
i :

xci = [x
ce
i ;x

p1
i ;x

p2
i ]. (2)

Then, the representation of characters xc =
{xc1, ...,xcM} will be directly fed into our model.



4380

3.1.2 Word-level Representation
Although our model takes character sequences as
direct inputs, in order to fully capture word-level
features, it also needs the information of all poten-
tial words in the input sentences. Here, a potential
word is any character subsequence that matches
a word in a lexicon D built over segmented large
raw text. Let wb,e be such a subsequence start-
ing from the b-th character to the e-th character.
To represent wb,e, we use the word2vec (Mikolov
et al., 2013) to convert it into a real-valued vector
xwb,e ∈ Rd

w
.

However, the word2vec method maps each
word to only one single embedding, ignoring the
fact that many words have multiple senses. To
tackle this problem, we incorporate HowNet as an
external knowledge base into our model to repre-
sent word senses rather than words.

Hence, given a word wb,e, we first obtain all
K senses of it by retrieving the HowNet. Using
Sense(wb,e) to denote the senses set of wb,e, we

then convert each sense sen(wb,e)k ∈ Sense(wb,e)
into a real-valued vector xsenb,e,k ∈ Rd

sen
through

the SAT model (Niu et al., 2017). The SAT model
is on the basis of the Skip-gram, which can jointly
learn word and sense representations. Finally, the
representation of wb,e is a vector set denoted as
xsenb,e = {xsenb,e,1, ...,xsenb,e,K}.

In the next section, we will introduce how our
model utilizes sense embeddings.

3.2 Encoder

The direct input of the encoder is a character se-
quence, together with all potential words in lexi-
con D. After training, the output of the encoder is
the hidden state vectorsh of an input sentence. We
introduce the encoder with two strategies, includ-
ing the basic lattice LSTM and the multi-graind
lattice (MG lattice) LSTM.

3.2.1 Basic Lattice LSTM Encoder
Generally, a classical LSTM (Hochreiter and
Schmidhuber, 1997) unit is composed of four ba-
sic gates structure: one input gate ij controls
which information enters into the unit; one output
gate oj controls which information would be out-
putted from the unit; one forget gate fj controls
which information would be removed in the unit.
All three gates are accompanied by weight matrix
W . Current cell state cj records all historical in-
formation flow up to the current time. Therefore,

the character-based LSTM functions are:
icj = σ(Wix

c
j + Uih

c
j−1 + bi),

ocj = σ(Wox
c
j + Uoh

c
j−1 + bo),

f cj = σ(Wfx
c
j + Ufh

c
j−1 + bf ),

c̃cj = tanh(Wcx
c
j + Uch

c
j−1 + bc),

(3)

ccj = f
c
j � ccj−1 + icj � c̃cj , (4)

hcj = o
c
j � tanh(ccj), (5)

where σ() means the sigmoid function. Hence, the
current cell state cj will be generated by calcu-
lating the weighted sum using both previous cell
state and current information generated by the cell
(Graves, 2013).

Given a word wb,e in the input sentence which
matches the external lexicon D, the representation
can be obtained as follows:

xwb,e = e
w(wb,e), (6)

where b and e denotes the start and the end of
the word, and ew is the lookup table . Under this
circumstance, the computation of ccj incorporates
word-level representation xwb,e to construct the ba-
sic lattice LSTM encoder. Further, a word cell cwb,e
is used to represent the memory cell state of xwb,e .
The computation of cwb,e is:

iwb,e = σ(Wix
w
b,e + Uih

c
b + bi),

fwb,e = σ(Wfx
w
b,e + Ufh

c
b + bf ),

c̃wb,e=tanh(Wcx
w
b,e + Uch

c
b + bc),

(7)

cwb,e = f
w
b,e � ccb + iwb,e � c̃wb,e, (8)

where iwb,e and f
w
b,e serve as a set of word-level in-

put and forget gates.
The cell state of the e-th character will be cal-

culated by incorporating the information of all the
words that end in index e, which is wb,e with
b ∈ {b′|wb′,e ∈ D}. To control the contribution
of each word, an extra gate icb,e is used:

icb,e = σ(Wx
c
e + Uc

w
b,e + b

l). (9)

Then the cell value of the e-th character is com-
puted by:

cce =
∑

b∈{b′|wb′,e∈D}

αcb,e � cwb,e +αce � c̃ce, (10)



4381

where αcb,e and α
c
e are normalization factors, set-

ting the sum to 1:

αcb,e=
exp(icb,e)

exp(ice)+
∑

b′∈{b′′|wb′′,e∈D}
exp(icb′,e)

, (11)

αce=
exp(ice)

exp(ice)+
∑

b′∈{b′′|wb′′,e∈D}
exp(icb′,e)

. (12)

Finally, we use Eq. 5 to compute the final hid-
den state vectors hcj for each character of the se-
quence. This structure is also used in Zhang and
Yang (2018).

3.2.2 MG Lattice LSTM Encoder
Although the basic lattice encoder can explicitly
leverages character and word information, it could
not fully consider the ambiguity of Chinese. For
instance, as shown in Figure 2, the word w2,3 (杜
鹃) has two senses: sen(w2,3)1 represents ’azalea’
and sen(w2,3)2 represents ’cuckoo’, but there is only
one representation for w2,3 in the basic lattice en-
coder, which is xw2,3.

To address this shortcoming, we improve the
model by adding sense-level paths as external
knowledge to the model. Hence, a more compre-
hensive lexicon would be constructed. As men-
tioned in 3.1, the representation of the k-th sense
of the word wb,e is xsenb,e,k.

For each word wb,e which matches the lexicon
D, we will take all its sense representations into
the calculation. The computation of the k-th sense
of word wb,e is:

isenb,e,k = σ(Wix
sen
b,e,k + Uih

c
b + bi),

f senb,e,k = σ(Wfx
sen
b,e,k + Ufh

c
b + bf ),

c̃senb,e,k=tanh(Wcx
sen
b,e,k+Uch

c
b+bc),

(13)

csenb,e,k = f
sen
b,e,k � ccb + isenb,e,k � c̃senb,e,k, (14)

where csenb,e,k represents the memory cell of the k-
th sense of the word wb,e. Then all the senses
are merged into a comprehensive representation to
compute the memory cell ofwb,e, which is denoted
as csenb,e :

csenb,e =
∑
k

αsenb,e,k � csenb,e,k, (15)

αsenb,e,k=
exp(isenb,e,k)

K∑
k′

exp(isenb,e,k′)

, (16)

where isenb,e,k is an extra gate to control the contri-
bution of the k-th sense, and is computed similar
as Eq. 9.

In this situation, all the sense-level cell states
will be incorporated into the word representation
csenb,e , which could better represent the polysemous
word. Then, similar to Eq. 9 - 12, all the recurrent
paths of words ending in index e will flow into the
current cell cce:

cce=
∑

b∈{b′|wd
b′,e∈D}

αsenb,e � csenb,e +αce � c̃ce. (17)

Finally, the hidden state h are still computed by
Eq. 5 and then sent to the relation classifier.

3.3 Relation Classifier
After the hidden state of an instance h ∈ Rdh×M
is learnt, we first adopt a character-level attention
mechanism to merge h into a sentence-level fea-
ture vector, denoted as h∗ ∈ Rdh . Here, dh indi-
cates the dimension of the hidden state and M is
the sequence length. Then, the final sentence rep-
resentation h∗ is fed into a softmax classifier to
compute the confidence of each relation.

The representation h∗ of the sentence is com-
puted as a weighted sum of all character feature
vectors in h:

H = tanh(h), (18)

α = softmax(wTH), (19)

h∗ = hαT , (20)

where w ∈ Rdh is a trained parameter and α ∈
RM is the weight vector of h.

To compute the conditional probability of each
relation, the feature vector h∗ of sentence S is fed
into a softmax classifier:

o =Wh∗ + b, (21)

p(y|S) = softmax(o), (22)

where W ∈ RY×dh is the transformation matrix
and b ∈ RY is a bias vector. Y indicates the total
number of relation types, and y is the estimated
probability for each type. This mechanism is also
applied to (Zhou et al., 2016).

Finally, given all (T ) training examples
(S(i), y(i)), we define the objective function using
cross-entropy as follows:

J(θ) =

T∑
i=1

log p(y(i)|S(i), θ), (23)



4382

where θ indicates all parameters of our model.
To avoid co-adaptation of hidden units, we ap-

ply dropout (Hinton et al., 2012) on the LSTM
layer by randomly removing feature detectors
from the network during forward propagation.

4 Experiments

In this section, we conduct a series of experiments
on three manually labeled datasets. Our models
show superiority and effectiveness compared with
other models. Furthermore, generalization is an-
other advantage of our models, because there are
five corpora used to construct the three datasets,
which are entirely different in topics and manners
of writing. The experiments will be organized as
follows:

(1) First, we study the ability of our model to
combine character-level and word-level informa-
tion by comparing it with char-based and word-
based models;

(2) Then we focus on the impact of sense rep-
resentation, carrying out experiments among three
different kinds of lattice-based models;

(3) Finally, we make comparisons with other
proposed models in relation extraction task.

4.1 Datasets and Experimental Settings
Datasets. We carry out our experiments on
three different datasets, including Chinese San-
Wen (Xu et al., 2017), ACE 2005 Chinese corpus
(LDC2006T06) and FinRE.

The Chinese SanWen dataset contains 9 types of
relations among 837 Chinese literature articles, in
which 695 articles for training, 84 for testing and
the rest 58 for validating. The ACE 2005 dataset
is collected from newswires, broadcasts, and we-
blogs, containing 8023 relation facts with 18 re-
lation subtypes. We randomly select 75% of it to
train the models and the remaining is used for eval-
uation.

For more diversity in test domains, we manu-
ally annotate the FinRE dataset from 2647 finan-
cial news in Sina Finance 2, with 13486, 3727 and
1489 relation instances for training, testing and
validation respectively. The FinRE contains 44
distinguished relationships including a special re-
lation NA, which indicates that there is no relation
between the marked entity pair.

Evaluation Metrics. Multiple standard evalu-
ation metrics are applied in the experiments, in-

2https://finance.sina.com.cn/

Hyper-parameter value
learning rate 0.0005
dropout probability 0.5
char embedding size 100
lattice embedding size 200
position embedding size 5
LSTM hidden 200
regularization 1e-8

Table 1: Hyper-parameters

cluding the precision-recall curve, F1-score, Pre-
cision at top N predictions (P@N) and area un-
der the curve (AUC). With comprehensive evalua-
tions, models can be estimated from multiple an-
gles.

Parameter Settings. We tune the parameters
of our models by grid searching on the validation
dataset. Grid search is utilized to select optimal
learning rate λ for Adam optimizer (Kingma and
Ba, 2014) among {0.0001, 0.0005, 0.001, 0.005, }
and position embedding dp in {5, 10, 15, 20}.
Table 1 shows the values of the best hyper-
parameters in our experiments. The best models
were selected by early stopping using the evalu-
ation results on the validation dataset. For other
parameters, we follow empirical settings because
they make little influence on the whole perfor-
mance of our models.

Models FinRE SanWen ACE

Word-
based

Word-baseline 41.23 54.26 64.43
+char CNN 41.60 56.62 68.86
+char LSTM 42.20 57.92 69.81

Char-
based

Character-baseline 40.50 60.34 71.52
+softword 41.42 60.69 69.81
+bichar 40.52 61.34 71.86
+softword + bichar 42.03 61.75 72.63

Ours
Basic Lattice 47.41 63.88 77.12
MG Lattice 49.26 65.61 78.17

Table 2: F1-scores of word-baselines, character base-
lines and lattice-based models on all datasets.

4.2 Effect of Lattice Encoder.
In this part, we mainly focus on the effect of
the encoder layer. As shown in Table 2, we
conducted experiments on char-based, word-based
and lattice-based models on all datasets. The
word-based and character-based baselines are im-
plemented by replacing the lattice encoder with
a bidirectional LSTM. In addition, character and
word features are added to these two baselines re-
spectively, so that they can use both character and
word information. For word baseline, we utilize



4383

Datasets ACE-2005 SanWen FinRE
P@N 100 200 300 Mean 100 200 300 Mean 100 200 300 Mean

Basic Lattice 99.01 94.03 94.68 95.91 96.04 90.05 89.04 91.71 97.03 92.04 90.70 93.26
Basic Lattice (SAT) 97.03 97.01 96.01 96.69 93.07 93.03 91.36 92.49 98.02 93.03 90.70 93.92

MG Lattice 98.02 97.51 96.01 97.18 94.06 93.03 90.70 92.60 100.0 92.54 89.70 94.08

Table 3: Precision@N of lattice-based models on all datasets.

an extra CNN/LSTM to learn hidden states for
characters of each word (char CNN/LSTM). For
char baseline, bichar and softword (word in which
the current character is located) are used as word-
level features to improve character representation.

The lattice-based approaches include two
lattice-based models, and both of them can explic-
itly leverage both character and word information.
The basic lattice uses the encoder mentioned in
3.2.1, which can dynamically incorporate word-
level information into character sequences. For
MG lattice, each sense embedding will be used to
construct an independent sense path. Hence, there
is not only word information, but also sense infor-
mation flowing into cell states.

Figure 3: Precision-recall curves for three lattice-based
models on ACE-2005.

Results of word-based model. With automatic
word segmentation, the baseline of the word-based
model yields 41.23%, 54.26% and 64.43% F1-
score on three datasets. The F1-scores are in-
creased to 41.6%, 56.62 and 68.86% by adding
character CNN to the baseline model. Compared
with the character CNN, character LSTM repre-
sentation gives slightly higher F1-scores, which
are 42.2%, 57.92%, and 69.81% respectively.
The results indicate that character information
will promote the performance of the word-based
model, but the increase in F1-score is not signifi-
cant.

Results of character-based model. For the
character baseline, it gives higher F1-scores com-
pared with the word-based methods. By adding
soft word feature, the F1-scores slightly increase
on FinRE and SanWen dataset. Similar results
are achieved by adding character-bigram. Ad-
ditionally, a combination of both word features
yields best F1-scores among character-based mod-
els, which are 42.03%, 61.75%, and 72.63%.

Results of lattice-based model. Although we
take multiple strategies to combine character and
word information in baselines, the lattice-based
models still significantly outperform them. The
basic lattice model improves the F1-scores of
three datasets from 42.2% to 47.35%, 61.75% to
63.88% and 72.63% to 77.12% respectively. The
results demonstrate the ability to exploit charac-
ter and word sequence information of the lattice-
based model. Comparisons and analysis of the
lattice-based models will be introduced in the next
subsection.

4.3 Effect of Word Sense Representations

In this section, we will study the effect of word
sense representations by utilizing sense-level in-
formation with different strategies. Hence, three
types of lattice-based models are used in our ex-
periments. First, the basic lattice model uses
word2vec (Mikolov et al., 2013) to train the word
embeddings, which considers no word sense infor-
mation. Then, we introduce the basic lattice (SAT)
model as a comparison, for which the pre-trained
word embeddings are improved by sense informa-
tion (Niu et al., 2017). Moreover, the MG lattice
model uses sense embeddings to build indepen-
dent paths and dynamically selects the appropriate
sense.

The results of P@N shown in Table 3 demon-
strate the effectiveness of word sense representa-
tions. The basic lattice (SAT) gives better perfor-
mance than the original basic lattice model thanks
to considering sense information into word em-
beddings. Although the basic lattice (SAT) model
reaches better overall results, the precision of the



4384

(a) Results on FinRE (b) Results on SanWen (c) Results on ACE-2005

Figure 4: Precision-recall curves of BLSTM, Att-BLSTM, CNN, PCNN, PCNN+ATT, Basic lattice and MG lattice
on all datasets. All models (except the Basic and MG lattice) are character-based.

top 100 instances is still lower than the lattice-
basic model. Compared with the other two mod-
els, MG lattice shows superiority in all indexes
of P@N, achieving the best results in the mean
scores.

To compare and analyze the effectiveness of
all lattice-based models more intuitively, we re-
port the precision-recall curve of the ACE-2005
dataset in Figure 3 as an example. Although the
basic lattice (SAT) model obtains better overall
performance than the original basic lattice model,
the precision is still lower when the recall is low,
which corresponds to the results in Table 3. This
situation indicates that considering multiple senses
only in the pre-trained stage would add noise to the
word representations. In other words, the word
representation tends to favor the commonly used
senses in the corpora, which will disturb the model
when the correct sense of the current word is not
the common one. Nevertheless, the MG lattice
model successfully avoids this problem, giving the
best performance in all parts of the curve. This re-
sult indicates that the MG lattice model is not sig-
nificantly impacted by the noisy information be-
cause it can dynamically select the sense paths in
different contexts. Although MG lattice model
shows effectiveness and robustness on the over-
all results, it is worth noting that the improvement
is limited. The situation indicates that the utiliza-
tion of multi-grained information could still be im-
proved. A more detailed discussion is in Section
5.

4.4 Final Results

In this section, we compare the performance of the
lattice-based model with various proposed meth-
ods. The proposed models we selected are as fol-

Models
FinRE SanWen ACE-2005

AUC F1 AUC F1 AUC F1
BLSTM 28.80 42.87 50.21 61.04 60.40 70.03

Att-BLSTM 27.81 41.48 50.42 59.48 61.85 70.69

CNN 27.12 41.47 47.81 59.42 64.49 72.41

PCNN 30.49 45.51 48.26 61.00 66.10 74.33

PCNN+Att 31.89 46.13 50.41 60.55 65.79 73.17

Basic Lattice 36.58 47.41 56.88 63.88 70.51 77.12

MG Lattice 38.74 49.26 57.33 65.61 72.28 78.17

Table 4: AUC and F1-scores of BLSTM, Att-BLSTM,
CNN, PCNN, PCNN +Att, Basic lattice and MG lattice
on all datasets. All models (except the Basic and MG
lattice) are character-based.

lows:
CNN (Zeng et al., 2014) proposes a CNN model

for relation extraction.
PCNN (Zeng et al., 2015) puts forward a piece-

wise CNN model with multi-instance learning.
BLSTM (Zhang and Wang, 2015) proposes a

bidrectional LSTM model for relation extraction.
Att-BLSTM (Zhou et al., 2016) is a bidrec-

tional LSTM model with word-level attention
mechanism. 3

PCNN+ATT (Lin et al., 2016) improves PCNN
model with selective attention mechanism.

We conduct experiments on both character-
based and word-based versions of the five mod-
els mentioned above. The results show that the
character-based versions perform better than the
word-based versions for all models on all datasets.
Consequently, we only use the character-based
version of the five selected models in the following
experiments.

3For the sake of fairness, we add position embeddings
to both BLSTM and Att-BLSTM, which are not used in the
original papers.



4385

For comprehensive comparison and analysis,
we report precision-recall curves in Figure 4 and
F1-scores and AUC in Table 4. From the re-
sults, we can observe that: (1) Lattice-based mod-
els significantly outperform other proposed mod-
els on datasets from different domains. Thanks to
the polysemy information, the MG lattice model
performs best among all models, showing supe-
riority and effectiveness on the Chinese RE task.
The results indicate that sense-level information
could enhance the ability to capturing deep se-
mantic information from text. (2) The gap be-
tween the basic lattice model and the MG lat-
tice model becomes narrow on the dataset FinRE.
The reason for this phenomenon is that FinRE is
constructed from financial report corpus, and the
words of financial reports are often rigorous and
unambiguous. (3) In comparison, the PCNN and
PCNN+ATT models perform worse in the SanWen
and ACE datasets. The reason is that there are po-
sitional overlaps between entity pairs in these two
datasets, making PCNN unable to take full advan-
tage of the piece-wise mechanism. The results in-
dicate that the PCNN-based methods have a high
dependence on the form of the dataset. In com-
parison, our models show robustness on all three
datasets.

5 Conclusion and Future Work

In this paper, we propose the MG lattice model
for Chinese relation extraction. The model in-
corporates word-level information into character
sequences to explore deep semantic features and
avoids the issue of polysemy ambiguity by intro-
ducing external linguistic knowledge, which is re-
garded as sense-level information. We compre-
hensively evaluate our model on various datasets.
The results show that our model significantly out-
performs other proposed methods, reaching the
state-of-the-art results on all datasets.

In the future, we will attempt to improve the
ability of the MG Lattice to utilize multi-grained
information. Although we have used word, sense
and character information in our work, more level
of information can be incorporated into the MG
Lattice. From coarse to fine, sememe-level in-
formation can be intuitively valuable. Here, se-
meme is the minimum semantic unit of word
sense, whose information may potentially assist
the model to explore deeper semantic features.
From fine to coarse, sentences and paragraphs

should be taken into account so that a border range
of contextual information can be captured.

6 Acknowledgement

This research is supported by the National
Natural Science Foundation of China (Grant
No. 61773229), the Basic Scientific Re-
search Program of Shenzhen City (Grant No.
JCYJ20160331184440545), and the Overseas Co-
operation Research Fund of Graduate School
at Shenzhen, Tsinghua University (Grant No.
HW2018002). Moreover, Zhiyuan Liu is sup-
ported by the National Key Research and Develop-
ment Program of China (No. 2018YFB1004503).
Finally, we would like to thank the anonymous
reviewers for their helpful feedback and sugges-
tions.

References
Xinchi Chen, Xipeng Qiu, Chenxi Zhu, Pengfei Liu,

and Xuanjing Huang. 2015. Long short-term mem-
ory neural networks for chinese word segmentation.
In Proceedings of the 2015 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1197–1206.

Yanping Chen, Qinghua Zheng, and Wei Zhang. 2014.
Omni-word feature and soft constraint for chinese
relation extraction. In Proceedings of the 52nd An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), volume 1,
pages 572–581.

Yu-Ju Chen and Jane Yung-jen Hsu. 2016. Chinese re-
lation extraction by multiple instance learning. In
Workshops at the Thirtieth AAAI Conference on Ar-
tificial Intelligence.

Zhendong Dong and Qiang Dong. 2003. Hownet-a hy-
brid language and knowledge resource. In Proceed-
ings of NLP-KE.

Alex Graves. 2013. Generating sequences with
recurrent neural networks. arXiv preprint
arXiv:1308.0850.

Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky,
Ilya Sutskever, and Ruslan R Salakhutdinov. 2012.
Improving neural networks by preventing co-
adaptation of feature detectors. arXiv preprint
arXiv:1207.0580.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Xiaotian Jiang, Quan Wang, Peng Li, and Bin Wang.
2016. Relation extraction with multi-instance multi-
label convolutional neural networks. In Proceedings



4386

of COLING 2016, the 26th International Confer-
ence on Computational Linguistics: Technical Pa-
pers, pages 1471–1480.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Joohong Lee, Sangwoo Seo, and Yong Suk Choi.
2019. Semantic relation classification via bidirec-
tional lstm networks with entity-aware attention us-
ing latent entity typing.

Yankai Lin, Shiqi Shen, Zhiyuan Liu, Huanbo Luan,
and Maosong Sun. 2016. Neural relation extraction
with selective attention over instances. In Proceed-
ings of the 54th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), volume 1, pages 2124–2133.

ChunYang Liu, WenBo Sun, WenHan Chao, and
Wanxiang Che. 2013. Convolution neural network
for relation extraction. In International Conference
on Advanced Data Mining and Applications, pages
231–242. Springer.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in neural information processing
systems, pages 3111–3119.

Yilin Niu, Ruobing Xie, Zhiyuan Liu, and Maosong
Sun. 2017. Improved word representation learning
with sememes. In Proceedings of the 55th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), volume 1, pages
2049–2058.

Nanyun Peng and Mark Dredze. 2016. Improving
named entity recognition for chinese social me-
dia with word segmentation representation learning.
arXiv preprint arXiv:1603.00786.

Fanchao Qi, Chenghao Yang, Zhiyuan Liu, Qiang
Dong, Maosong Sun, and Zhendong Dong. 2019.
Openhownet: An open sememe-based lexical
knowledge base. arXiv preprint arXiv:1901.09957.

Samuel Rönnqvist, Niko Schenk, and Christian Chiar-
cos. 2017. A recurrent neural model with attention
for the recognition of chinese implicit discourse re-
lations. arXiv preprint arXiv:1704.08092.

Matthias Sperber, Graham Neubig, Jan Niehues, and
Alex Waibel. 2017. Neural lattice-to-sequence
models for uncertain inputs. arXiv preprint
arXiv:1704.00559.

Jinsong Su, Zhixing Tan, Deyi Xiong, Rongrong Ji, Xi-
aodong Shi, and Yang Liu. 2017. Lattice-based re-
current neural network encoders for neural machine
translation. In Thirty-First AAAI Conference on Ar-
tificial Intelligence.

Lin Sun, Kui Jia, Kevin Chen, Dit-Yan Yeung,
Bertram E Shi, and Silvio Savarese. 2017. Lattice
long short-term memory for human action recogni-
tion. In Proceedings of the IEEE International Con-
ference on Computer Vision, pages 2147–2156.

Kai Sheng Tai, Richard Socher, and Christopher D
Manning. 2015. Improved semantic representations
from tree-structured long short-term memory net-
works. arXiv preprint arXiv:1503.00075.

Jingjing Xu, Ji Wen, Xu Sun, and Qi Su. 2017. A
discourse-level named entity recognition and rela-
tion extraction dataset for chinese literature text.
arXiv preprint arXiv:1711.07010.

Jie Yang, Yue Zhang, and Fei Dong. 2017. Neu-
ral word segmentation with rich pretraining. arXiv
preprint arXiv:1704.08960.

Daojian Zeng, Kang Liu, Yubo Chen, and Jun Zhao.
2015. Distant supervision for relation extraction via
piecewise convolutional neural networks. In Pro-
ceedings of the 2015 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1753–
1762.

Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou,
Jun Zhao, et al. 2014. Relation classification via
convolutional deep neural network.

Dongxu Zhang and Dong Wang. 2015. Relation classi-
fication via recurrent neural network. arXiv preprint
arXiv:1508.01006.

Qian-qian ZHANG, Meng-dong CHEN, and Lian-
zhong LIU. 2017. An effective gated recurrent
unit network model for chinese relation extraction.
DEStech Transactions on Computer Science and En-
gineering, (wcne).

Yue Zhang and Jie Yang. 2018. Chinese ner using lat-
tice lstm. arXiv preprint arXiv:1805.02023.

Hai Zhao and Chunyu Kit. 2008. Unsupervised seg-
mentation helps supervised learning of character
tagging for word segmentation and named entity
recognition. In Proceedings of the Sixth SIGHAN
Workshop on Chinese Language Processing.

Peng Zhou, Wei Shi, Jun Tian, Zhenyu Qi, Bingchen
Li, Hongwei Hao, and Bo Xu. 2016. Attention-
based bidirectional long short-term memory net-
works for relation classification. In Proceedings of
the 54th Annual Meeting of the Association for Com-
putational Linguistics (Volume 2: Short Papers),
volume 2, pages 207–212.


