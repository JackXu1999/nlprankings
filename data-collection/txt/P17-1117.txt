



















































Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics


Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1273–1283
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1117

Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1273–1283
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1117

Multi-Task Video Captioning with Video and Entailment Generation

Ramakanth Pasunuru and Mohit Bansal
UNC Chapel Hill

{ram, mbansal}@cs.unc.edu

Abstract

Video captioning, the task of describing
the content of a video, has seen some
promising improvements in recent years
with sequence-to-sequence models, but
accurately learning the temporal and log-
ical dynamics involved in the task still re-
mains a challenge, especially given the
lack of sufficient annotated data. We im-
prove video captioning by sharing knowl-
edge with two related directed-generation
tasks: a temporally-directed unsuper-
vised video prediction task to learn richer
context-aware video encoder representa-
tions, and a logically-directed language
entailment generation task to learn bet-
ter video-entailing caption decoder rep-
resentations. For this, we present a
many-to-many multi-task learning model
that shares parameters across the encoders
and decoders of the three tasks. We
achieve significant improvements and the
new state-of-the-art on several standard
video captioning datasets using diverse au-
tomatic and human evaluations. We also
show mutual multi-task improvements on
the entailment generation task.

1 Introduction

Video captioning is the task of automatically gen-
erating a natural language description of the con-
tent of a video, as shown in Fig. 1. It has various
applications such as assistance to a visually im-
paired person and improving the quality of online
video search or retrieval. This task has gained re-
cent momentum in the natural language process-
ing and computer vision communities, esp. with
the advent of powerful image processing features
as well as sequence-to-sequence LSTM models. It

Figure 1: A video captioning example from the
YouTube2Text dataset, with the ground truth captions
and our many-to-many multi-task model’s predicted caption.

is also a step forward from static image captioning,
because in addition to modeling the spatial visual
features, the model also needs to learn the tempo-
ral across-frame action dynamics and the logical
storyline language dynamics.

Previous work in video captioning (Venu-
gopalan et al., 2015a; Pan et al., 2016b) has shown
that recurrent neural networks (RNNs) are a good
choice for modeling the temporal information in
the video. A sequence-to-sequence model is then
used to ‘translate’ the video to a caption. Venu-
gopalan et al. (2016) showed linguistic improve-
ments over this by fusing the decoder with external
language models. Furthermore, an attention mech-
anism between the video frames and the caption
words captures some of the temporal matching re-
lations better (Yao et al., 2015; Pan et al., 2016a).
More recently, hierarchical two-level RNNs were
proposed to allow for longer inputs and to model
the full paragraph caption dynamics of long video
clips (Pan et al., 2016a; Yu et al., 2016).

Despite these recent improvements, video cap-
tioning models still suffer from the lack of suffi-
cient temporal and logical supervision to be able
to correctly capture the action sequence and story-
dynamic language in videos, esp. in the case of
short clips. Hence, they would benefit from incor-
porating such complementary directed knowledge,
both visual and textual. We address this by jointly
training the task of video captioning with two
related directed-generation tasks: a temporally-

1273

https://doi.org/10.18653/v1/P17-1117
https://doi.org/10.18653/v1/P17-1117


directed unsupervised video prediction task and a
logically-directed language entailment generation
task. We model this via many-to-many multi-task
learning based sequence-to-sequence models (Lu-
ong et al., 2016) that allow the sharing of param-
eters among the encoders and decoders across the
three different tasks, with additional shareable at-
tention mechanisms.

The unsupervised video prediction task, i.e.,
video-to-video generation (adapted from Srivas-
tava et al. (2015)), shares its encoder with the
video captioning task’s encoder, and helps it learn
richer video representations that can predict their
temporal context and action sequence. The entail-
ment generation task, i.e., premise-to-entailment
generation (based on the image caption domain
SNLI corpus (Bowman et al., 2015)), shares its de-
coder with the video captioning decoder, and helps
it learn better video-entailing caption representa-
tions, since the caption is essentially an entailment
of the video, i.e., it describes subsets of objects
and events that are logically implied by or follow
from the full video content). The overall many-to-
many multi-task model combines all three tasks.

Our three novel multi-task models show statis-
tically significant improvements over the state-of-
the-art, and achieve the best-reported results (and
rank) on multiple datasets, based on several au-
tomatic and human evaluations. We also demon-
strate that video captioning, in turn, gives mutual
improvements on the new multi-reference entail-
ment generation task.

2 Related Work

Early video captioning work (Guadarrama et al.,
2013; Thomason et al., 2014; Huang et al., 2013)
used a two-stage pipeline to first extract a subject,
verb, and object (S,V,O) triple and then generate a
sentence based on it. Venugopalan et al. (2015b)
fed mean-pooled static frame-level visual features
(from convolution neural networks pre-trained on
image recognition) of the video as input to the lan-
guage decoder. To harness the important frame
sequence temporal ordering, Venugopalan et al.
(2015a) proposed a sequence-to-sequence model
with video encoder and language decoder RNNs.

More recently, Venugopalan et al. (2016) ex-
plored linguistic improvements to the caption de-
coder by fusing it with external language models.
Moreover, an attention or alignment mechanism
was added between the encoder and the decoder

to learn the temporal relations (matching) between
the video frames and the caption words (Yao et al.,
2015; Pan et al., 2016a). In contrast to static visual
features, Yao et al. (2015) also considered tem-
poral video features from a 3D-CNN model pre-
trained on an action recognition task.

To explore long range temporal relations, Pan
et al. (2016a) proposed a two-level hierarchical
RNN encoder which limits the length of input in-
formation and allows temporal transitions between
segments. Yu et al. (2016)’s hierarchical RNN
generates sentences at the first level and the sec-
ond level captures inter-sentence dependencies in
a paragraph. Pan et al. (2016b) proposed to simul-
taneously learn the RNN word probabilities and
a visual-semantic joint embedding space that en-
forces the relationship between the semantics of
the entire sentence and the visual content. Despite
these useful recent improvements, video caption-
ing still suffers from limited supervision and gen-
eralization capabilities, esp. given the complex
action-based temporal and story-based logical dy-
namics that need to be captured from short video
clips. Our work addresses this issue by bringing in
complementary temporal and logical knowledge
from video prediction and textual entailment gen-
eration tasks (respectively), and training them to-
gether via many-to-many multi-task learning.

Multi-task learning is a useful learning
paradigm to improve the supervision and the
generalization performance of a task by jointly
training it with related tasks (Caruana, 1998;
Argyriou et al., 2007; Kumar and Daumé III,
2012). Recently, Luong et al. (2016) combined
multi-task learning with sequence-to-sequence
models, sharing parameters across the tasks’
encoders and decoders. They showed improve-
ments on machine translation using parsing and
image captioning. We additionally incorporate
an attention mechanism to this many-to-many
multi-task learning approach and improve the
multimodal, temporal-logical video captioning
task by sharing its video encoder with the encoder
of a video-to-video prediction task and by sharing
its caption decoder with the decoder of a linguistic
premise-to-entailment generation task.

Image representation learning has been success-
ful via supervision from very large object-labeled
datasets. However, similar amounts of supervi-
sion are lacking for video representation learning.
Srivastava et al. (2015) address this by propos-

1274



LSTM

LSTM

LSTM

LSTM

LSTM

LSTM

LSTM

LSTM

LSTM

LSTM

LSTM

LSTM

LSTM

LSTM

LSTM

LSTM

LSTM

LSTM

LSTM

LSTM

LSTM

LSTM

LSTM

LSTM

Figure 2: Baseline sequence-to-sequence model for video
captioning: standard encoder-decoder LSTM-RNN model.

ing unsupervised video representation learning via
sequence-to-sequence RNN models, where they
reconstruct the input video sequence or predict the
future sequence. We model video generation with
an attention-enhanced encoder-decoder and har-
ness it to improve video captioning.

The task of recognizing textual entailment
(RTE) is to classify whether the relationship be-
tween a premise and hypothesis sentence is that
of entailment (i.e., logically follows), contradic-
tion, or independence (neutral), which is help-
ful for several downstream NLP tasks. The re-
cent Stanford Natural Language Inference (SNLI)
corpus by Bowman et al. (2015) allowed training
end-to-end neural networks that outperform ear-
lier feature-based RTE models (Lai and Hocken-
maier, 2014; Jimenez et al., 2014). However, di-
rectly generating the entailed hypothesis sentences
given a premise sentence would be even more ben-
eficial than retrieving or reranking sentence pairs,
because most downstream generation tasks only
come with the source sentence and not pairs. Re-
cently, Kolesnyk et al. (2016) tried a sequence-
to-sequence model for this on the original SNLI
dataset, which is a single-reference setting and
hence restricts automatic evaluation. We modify
the SNLI corpus to a new multi-reference (and
a more challenging zero train-test premise over-
lap) setting, and present a novel multi-task training
setup with the related video captioning task (where
the caption also entails a video), showing mutual
improvements on both the tasks.

3 Models

We first discuss a simple encoder-decoder model
as a baseline reference for video captioning. Next,
we improve this via an attention mechanism. Fi-
nally, we present similar models for the unsuper-
vised video prediction and entailment generation
tasks, and then combine them with video caption-
ing via the many-to-many multi-task approach.

3.1 Baseline Sequence-to-Sequence Model
Our baseline model is similar to the stan-
dard machine translation encoder-decoder RNN

Figure 3: Attention-based sequence-to-sequence baseline
model for video captioning (similar models also used for
video prediction and entailment generation).

model (Sutskever et al., 2014) where the final state
of the encoder RNN is input as an initial state to
the decoder RNN, as shown in Fig. 2. The RNN
is based on Long Short Term Memory (LSTM)
units, which are good at memorizing long se-
quences due to forget-style gates (Hochreiter and
Schmidhuber, 1997). For video captioning, our
input to the encoder is the video frame features1

{f1, f2, ..., fn} of length n, and the caption word
sequence {w1, w2, ..., wm} of length m is gener-
ated during the decoding phase. The distribution
of the output sequence w.r.t. the input sequence is:

p(w1, ..., wm|f1, ..., fn) =
m∏

t=1

p(wt|hdt ) (1)

where hdt is the hidden state at the t
th time step of

the decoder RNN, obtained from hdt−1 and wt−1
via the standard LSTM-RNN equations. The dis-
tribution p(wt|hdt ) is given by softmax over all the
words in the vocabulary.

3.2 Attention-based Model
Our attention model architecture is similar to Bah-
danau et al. (2015), with a bidirectional LSTM-
RNN as the encoder and a unidirectional LSTM-
RNN as the decoder, see Fig. 3. At each time step
t, the decoder LSTM hidden state hdt is a non-
linear recurrent function of the previous decoder
hidden state hdt−1, the previous time-step’s gener-
ated word wt−1, and the context vector ct:

hdt = S(h
d
t−1, wt−1, ct) (2)

1We use several popular image features such as VGGNet,
GoogLeNet and Inception-v4. Details in Sec. 4.1.

1275



UNSUPERVISED

VIDEO PREDICTION
VIDEO CAPTIONING

ENTAILMENT

GENERATION

Video Encoder Language Encoder

Video Decoder Language Decoder

LSTM LSTM LSTM LSTM

LSTM LSTM LSTM LSTM

LSTM LSTM LSTM LSTM

LSTM LSTM LSTM LSTM

Figure 4: Our many-to-many multi-task learning model to share encoders and decoders of the video captioning, unsupervised
video prediction, and entailment generation tasks.

where ct is a weighted sum of encoder hidden
states {hei}:

ct =

n∑

i=1

αt,ih
e
i (3)

These attention weights {αt,i} act as an alignment
mechanism by giving higher weights to certain en-
coder hidden states which match that decoder time
step better, and are computed as:

αt,i =
exp(et,i)∑n
k=1 exp(et,k)

(4)

where the attention function et,i is defined as:

et,i = w
T tanh(W eah

e
i +W

d
ah

d
t−1 + ba) (5)

where w, W ea , W
d
a , and ba are learned parameters.

This attention-based sequence-to-sequence model
(Fig. 3) is our enhanced baseline for video caption-
ing. We next discuss similar models for the new
tasks of unsupervised video prediction and entail-
ment generation and then finally share them via
multi-task learning.

3.3 Unsupervised Video Prediction
We model unsupervised video representation by
predicting the sequence of future video frames
given the current frame sequence. Similar to
Sec. 3.2, a bidirectional LSTM-RNN encoder and
an LSTM-RNN decoder is used, along with at-
tention. If the frame level features of a video
of length n are {f1, f2, ..., fn}, these are di-
vided into two sets such that given the current
frames {f1, f2, .., fk} (in its encoder), the model
has to predict (decode) the rest of the frames
{fk+1, fk+2, .., fn}. The motivation is that this

helps the video encoder learn rich temporal rep-
resentations that are aware of their action-based
context and are also robust to missing frames and
varying frame lengths or motion speeds. The opti-
mization function is defined as:

minimize
φ

n−k∑

t=1

||fdt − ft+k||22 (6)

where φ are the model parameters, ft+k is the true
future frame feature at decoder time step t and fdt
is the decoder’s predicted future frame feature at
decoder time step t, defined as:

fdt = S(h
d
t−1, f

d
t−1, ct) (7)

similar to Eqn. 2, with hdt−1 and f
d
t−1 as the

previous time step’s hidden state and predicted
frame feature respectively, and ct as the attention-
weighted context vector.

3.4 Entailment Generation
Given a sentence (premise), the task of entail-
ment generation is to generate a sentence (hypoth-
esis) which is a logical deduction or implication
of the premise. Our entailment generation model
again uses a bidirectional LSTM-RNN encoder
and LSTM-RNN decoder with an attention mech-
anism (similar to Sec. 3.2). If the premise sp is
a sequence of words {wp1, wp2, ..., wpn} and the hy-
pothesis sh is {wh1 , wh2 , ..., whm}, the distribution
of the entailed hypothesis w.r.t. the premise is:

p(wh1 , ..., w
h
m|wp1, ..., wpn) =

m∏

t=1

p(wht |hdt ) (8)

where the distribution p(wht |hdt ) is again obtained
via softmax over all the words in the vocabulary
and the decoder state hdt is similar to Eqn. 2.

1276



3.5 Multi-Task Learning

Multi-task learning helps in sharing information
between different tasks and across domains. Our
primary aim is to improve the video captioning
model, where visual content translates to a tex-
tual form in a directed (entailed) generation way.
Hence, this presents an interesting opportunity to
share temporally and logically directed knowledge
with both visual and linguistic generation tasks.
Fig. 4 shows our overall many-to-many multi-task
model for jointly learning video captioning, unsu-
pervised video prediction, and textual entailment
generation. Here, the video captioning task shares
its video encoder (parameters) with the encoder of
the video prediction task (one-to-many setting) so
as to learn context-aware and temporally-directed
visual representations (see Sec. 3.3).

Moreover, the decoder of the video caption-
ing task is shared with the decoder of the textual
entailment generation task (many-to-one setting),
thus helping generate captions that can ‘entail’,
i.e., are logically implied by or follow from the
video content (see Sec. 3.4).2 In both the one-to-
many and the many-to-one settings, we also allow
the attention parameters to be shared or separated.
The overall many-to-many setting thus improves
both the visual and language representations of the
video captioning model.

We train the multi-task model by alternately op-
timizing each task in mini-batches based on a mix-
ing ratio. Let αv, αf , and αe be the number
of mini-batches optimized alternately from each
of these three tasks – video captioning, unsuper-
vised video future frames prediction, and entail-
ment generation, resp. Then the mixing ratio is de-
fined as αv(αv+αf+αe) :

αf
(αv+αf+αe)

: αe(αv+αf+αe) .

4 Experimental Setup

4.1 Datasets

Video Captioning Datasets We report results
on three popular video captioning datasets. First,
we use the YouTube2Text or MSVD (Chen and
Dolan, 2011) for our primary results, which con-

2Empirically, logical entailment helped captioning more
than simple fusion with language modeling (i.e., partial sen-
tence completion with no logical implication), because a cap-
tion also entails a video in a logically-directed sense and
hence the entailment generation task matches the video cap-
tioning task better than language modeling. Moreover, a
multi-task setup is more suitable to add directed information
such as entailment (as opposed to pretraining or fusion with
only the decoder). Details in Sec. 5.1.

tains 1970 YouTube videos in the wild with sev-
eral different reference captions per video (40 on
average). We also use MSR-VTT (Xu et al.,
2016) with 10, 000 diverse video clips (from a
video search engine) – it has 200, 000 video clip-
sentence pairs and around 20 captions per video;
and M-VAD (Torabi et al., 2015) with 49, 000
movie-based video clips but only 1 or 2 captions
per video, making most evaluation metrics (except
paraphrase-based METEOR) infeasible. We use
the standard splits for all three datasets. Further
details about all these datasets are provided in the
supplementary.

Video Prediction Dataset For our unsupervised
video representation learning task, we use the
UCF-101 action videos dataset (Soomro et al.,
2012), which contains 13, 320 video clips of 101
action categories, and suits our video captioning
task well because it also contains short video clips
of a single action or few actions. We use the stan-
dard splits – further details in supplementary.

Entailment Generation Dataset For the entail-
ment generation encoder-decoder model, we use
the Stanford Natural Language Inference (SNLI)
corpus (Bowman et al., 2015), which contains
human-annotated English sentence pairs with clas-
sification labels of entailment, contradiction and
neutral. It has a total of 570, 152 sentence pairs
out of which 190, 113 correspond to true entail-
ment pairs, and we use this subset in our multi-task
video captioning model. For improving video cap-
tioning, we use the same training/validation/test
splits as provided by Bowman et al. (2015), which
is 183, 416 training, 3, 329 validation, and 3, 368
testing pairs (for the entailment subset).

However, for the entailment generation multi-
task results (see results in Sec. 5.3), we modify
the splits so as to create a multi-reference setup
which can afford evaluation with automatic met-
rics. A given premise usually has multiple entailed
hypotheses but the original SNLI corpus is set
up as single-reference (for classification). Due to
this, the different entailed hypotheses of the same
premise land up in different splits of the dataset
(e.g., one in train and one in test/validation) in
many cases. Therefore, we regroup the premise-
entailment pairs and modify the split as follows:
among the 190, 113 premise-entailment pairs sub-
set of the SNLI corpus, there are 155, 898 unique
premises; out of which 145, 822 have only one hy-

1277



pothesis and we make this the training set, and
the rest of them (10, 076) have more than one hy-
pothesis, which we randomly shuffle and divide
equally into test and validation sets, so that each of
these two sets has approximately the same distri-
bution of the number of reference hypotheses per
premise.

These new validation and test sets hence con-
tain premises with multiple entailed hypotheses as
ground truth references, thus allowing for auto-
matic metric evaluation, where differing genera-
tions still get positive scores by matching one of
the multiple references. Also, this creates a more
challenging dataset for entailment generation be-
cause of zero premise overlap between the training
and val/test sets. We will make these split details
publicly available.

Pre-trained Visual Frame Features For the
three video captioning and UCF-101 datasets, we
fix our sampling rate to 3fps to bring unifor-
mity in the temporal representation of actions
across all videos. These sampled frames are
then converted into features using several state-
of-the-art pre-trained models on ImageNet (Deng
et al., 2009) – VGGNet (Simonyan and Zisserman,
2015), GoogLeNet (Szegedy et al., 2015; Ioffe and
Szegedy, 2015), and Inception-v4 (Szegedy et al.,
2016). Details of these feature dimensions and
layer positions are in the supplementary.

4.2 Evaluation (Automatic and Human)

For our video captioning as well as entailment
generation results, we use four diverse auto-
matic evaluation metrics that are popular for im-
age/video captioning and language generation in
general: METEOR (Denkowski and Lavie, 2014),
BLEU-4 (Papineni et al., 2002), CIDEr-D (Vedan-
tam et al., 2015), and ROUGE-L (Lin, 2004). Par-
ticularly, METEOR and CIDEr-D have been jus-
tified to be better for generation tasks, because
CIDEr-D uses consensus among the (large) num-
ber of references and METEOR uses soft match-
ing based on stemming, paraphrasing, and Word-
Net synonyms. We use the standard evaluation
code from the Microsoft COCO server (Chen
et al., 2015) to obtain these results and also to
compare the results with previous papers.3

We also present human evaluation results based

3We use avg. of these four metrics on validation set to
choose the best model, except for single-reference M-VAD
dataset where we only report and choose based on METEOR.

on relevance (i.e., how related is the generated
caption w.r.t. the video contents such as actions,
objects, and events; or is the generated hypothesis
entailed or implied by the premise) and coherence
(i.e., a score on the logic, readability, and fluency
of the generated sentence).

4.3 Training Details

We tune all hyperparameters on the dev splits:
LSTM-RNN hidden state size, learning rate,
weight initializations, and mini-batch mixing ra-
tios (tuning ranges in supplementary). We use
the following settings in all of our models (un-
less otherwise specified): we unroll video en-
coder/decoder RNNs to 50 time steps and lan-
guage encoder/decoder RNNs to 30 time steps.
We use a 1024-dimension RNN hidden state size
and 512-dim vectors to embed visual features and
word vectors. We use Adam optimizer (Kingma
and Ba, 2015). We apply a dropout of 0.5. See
subsections below and supp for full details.

5 Results and Analysis

5.1 Video Captioning on YouTube2Text

Table 1 presents our primary results on the
YouTube2Text (MSVD) dataset, reporting several
previous works, all our baselines and attention
model ablations, and our three multi-task models,
using the four automated evaluation metrics. For
each subsection below, we have reported the im-
portant training details inline, and refer to the sup-
plementary for full details (e.g., learning rates and
initialization).

Baseline Performance We first present all our
baseline model choices (ablations) in Table 1.
Our baselines represent the standard sequence-to-
sequence model with three different visual feature
types as well as those with attention mechanisms.
Each baseline model is trained with three random
seed initializations and the average is reported (for
stable results). The final baseline model ⊗ instead
uses an ensemble (E), which is a standard denois-
ing method (Sutskever et al., 2014) that performs
inference over ten randomly initialized models,
i.e., at each time step t of the decoder, we generate
a word based on the avg. of the likelihood prob-
abilities from the ten models. Moreover, we use
beam search with size 5 for all baseline models.
Overall, the final baseline model with Inception-
v4 features, attention, and 10-ensemble performs

1278



Models METEOR CIDEr-D ROUGE-L BLEU-4
PREVIOUS WORK

LSTM-YT (V) (Venugopalan et al., 2015b) 26.9 - - 31.2
S2VT (V + A) (Venugopalan et al., 2015a) 29.8 - - -
Temporal Attention (G + C) (Yao et al., 2015) 29.6 51.7 - 41.9
LSTM-E (V + C) (Pan et al., 2016b) 31.0 - - 45.3
Glove + DeepFusion (V) (E) (Venugopalan et al., 2016) 31.4 - - 42.1
p-RNN (V + C) (Yu et al., 2016) 32.6 65.8 - 49.9
HNRE + Attention (G + C) (Pan et al., 2016a) 33.9 - - 46.7

OUR BASELINES
Baseline (V) 31.4 63.9 68.0 43.6
Baseline (G) 31.7 64.8 68.6 44.1
Baseline (I) 33.3 75.6 69.7 46.3
Baseline + Attention (V) 32.6 72.2 69.0 47.5
Baseline + Attention (G) 33.0 69.4 68.3 44.9
Baseline + Attention (I) 33.8 77.2 70.3 49.9
Baseline + Attention (I) (E) ⊗ 35.0 84.4 71.5 52.6

OUR MULTI-TASK LEARNING MODELS
⊗ + Video Prediction (1-to-M) 35.6 88.1 72.9 54.1
⊗ + Entailment Generation (M-to-1) 35.9 88.0 72.7 54.4
⊗ + Video Prediction + Entailment Generation (M-to-M) 36.0 92.4 72.8 54.5

Table 1: Primary video captioning results on Youtube2Text (MSVD), showing previous works, our several strong baselines,
and our three multi-task models. Here, V, G, I, C, A are short for VGGNet, GoogLeNet, Inception-v4, C3D, and AlexNet visual
features; E = ensemble. The multi-task models are applied on top of our best video captioning baseline ⊗, with an ensemble.
All the multi-task models are statistically significant over the baseline (discussed inline in the corresponding results sections).

well (and is better than all previous state-of-the-
art), and so we next add all our novel multi-task
models on top of this final baseline.

Multi-Task with Video Prediction (1-to-M)
Here, the video captioning and unsupervised video
prediction tasks share their encoder LSTM-RNN
weights and image embeddings in a one-to-many
multi-task setting. Two important hyperparam-
eters tuned (on the validation set of caption-
ing datasets) are the ratio of encoder vs decoder
frames for video prediction on UCF-101 (where
we found that 80% of frames as input and 20% for
prediction performs best); and the mini-batch mix-
ing ratio between the captioning and video pre-
diction tasks (where we found 100 : 200 works
well). Table 1 shows a statistically significant im-
provement4 in all metrics in comparison to the best
baseline (non-multitask) model as well as w.r.t. all
previous works, demonstrating the effectiveness
of multi-task learning for video captioning with
video prediction, even with unsupervised signals.

Multi-Task with Entailment Generation (M-
to-1) Here, the video captioning and entail-
ment generation tasks share their language de-
coder LSTM-RNN weights and word embeddings
in a many-to-one multi-task setting. We observe

4Statistical significance of p < 0.01 for CIDEr-D and
ROUGE-L, p < 0.02 for BLEU-4, p < 0.03 for METEOR,
based on the bootstrap test (Noreen, 1989; Efron and Tibshi-
rani, 1994) with 100K samples.

that a mixing ratio of 100 : 50 alternating mini-
batches (between the captioning and entailment
tasks) works well here. Again, Table 1 shows
statistically significant improvements5 in all the
metrics in comparison to the best baseline model
(and all previous works) under this multi-task set-
ting. Note that in our initial experiments, our en-
tailment generation model helped the video cap-
tioning task significantly more than the alternative
approach of simply improving fluency by adding
(or deep-fusing) an external language model (or
pre-trained word embeddings) to the decoder (us-
ing both in-domain and out-of-domain language
models), again because a caption also ‘entails’ a
video in a logically-directed sense and hence this
matches our captioning task better (also see results
of Venugopalan et al. (2016) in Table 1).

Multi-Task with Video and Entailment Gener-
ation (M-to-M) Combining the above one-to-
many and many-to-one multi-task learning mod-
els, our full model is the 3-task, many-to-many
model (Fig. 4) where both the video encoder
and the language decoder of the video caption-
ing model are shared (and hence improved) with
that of the unsupervised video prediction and en-
tailment generation models, respectively.6 A mix-
ing ratio of 100 : 100 : 50 alternate mini-batches

5Statistical significance of p < 0.01 for all four metrics.
6We found the setting with unshared attention parameters

to work best, likely because video captioning and video pre-
diction prefer very different alignment distributions.

1279



Models M C R B
Venugopalan (2015b)? 23.4 - - 32.3
Yao et al. (2015)? 25.2 - - 35.2
Xu et al. (2016) 25.9 - - 36.6
Rank1: v2t navigator 28.2 44.8 60.9 40.8
Rank2: Aalto 26.9 45.7 59.8 39.8
Rank3: VideoLAB 27.7 44.1 60.6 39.1
Our Model (New Rank1) 28.8 47.1 60.2 40.8

Table 2: Results on MSR-VTT dataset on the 4 metrics.
?Results are reimplementations as per Xu et al. (2016).
We also report the top 3 leaderboard systems – our model
achieves the new rank 1 based on their ranking method.

Models METEOR
Yao et al. (2015) 5.7
Venugopalan et al. (2015a) 6.7
Pan et al. (2016a) 6.8
Our M-to-M Multi-Task Model 7.4

Table 3: Results on M-VAD dataset.

of video captioning, unsupervised video predic-
tion, and entailment generation, resp. works well.
Table 1 shows that our many-to-many multi-task
model again outperforms our strongest baseline
(with statistical significance of p < 0.01 on all
metrics), as well as all the previous state-of-the-
art results by large absolute margins on all met-
rics. It also achieves significant improvements on
some metrics over the one-to-many and many-to-
one models.7 Overall, we achieve the best results
to date on YouTube2Text (MSVD) on all metrics.

5.2 Video Captioning on MSR-VTT, M-VAD

In Table 2, we also train and evaluate our fi-
nal many-to-many multi-task model on two other
video captioning datasets (using their standard
splits; details in supplementary). First, we eval-
uate on the new MSR-VTT dataset (Xu et al.,
2016). Since this is a recent dataset, we list pre-
vious works’ results as reported by the MSR-VTT
dataset paper itself.8 We improve over all of these
significantly. Moreover, they maintain a leader-
board9 on this dataset and we also report the top 3
systems from it. Based on their ranking method,
our multi-task model achieves the new rank 1 on
this leaderboard. In Table 3, we further eval-
uate our model on the challenging movie-based
M-VAD dataset, and again achieve improvements
over all previous work (Venugopalan et al., 2015a;

7Many-to-many model’s improvements have a statistical
significance of p < 0.01 on all metrics w.r.t. baseline, and
p < 0.01 on CIDEr-D w.r.t. both one-to-many and many-to-
one models, and p < 0.04 on METEOR w.r.t. one-to-many.

8In their updated supplementary at https:
//www.microsoft.com/en-us/research/wp-content/
uploads/2016/10/cvpr16.supplementary.pdf

9
http://ms-multimedia-challenge.com/leaderboard

Models M C R B
Entailment Generation 28.0 108.4 59.7 36.6
+Video Caption (M-to-1) 28.7 114.5 60.8 38.9

Table 4: Entailment generation results with the four metrics.

Pan et al., 2016a; Yao et al., 2015).10

5.3 Entailment Generation Results
Above, we showed that the new entailment gener-
ation task helps improve video captioning. Next,
we show that the video captioning task also
inversely helps the entailment generation task.
Given a premise, the task of entailment generation
is to generate an entailed hypothesis. We use only
the entailment pairs subset of the SNLI corpus for
this, but with a multi-reference split setup to al-
low automatic metric evaluation and a zero train-
test premise overlap (see Sec. 4.1). All the hyper-
parameter details (again tuned on the validation
set) are presented in the supplementary. Table 4
presents the entailment generation results for the
baseline (sequence-to-sequence with attention, 3-
ensemble, beam search) and the multi-task model
which uses video captioning (shared decoder) on
top of the baseline. A mixing ratio of 100 : 20 al-
ternate mini-batches of entailment generation and
video captioning (resp.) works well.11 The multi-
task model achieves stat. significant (p < 0.01)
improvements over the baseline on all metrics,
thus demonstrating that video captioning and en-
tailment generation both mutually help each other.

5.4 Human Evaluation
In addition to the automated evaluation metrics,
we present pilot-scale human evaluations on the
YouTube2Text (Table 1) and entailment genera-
tion (Table 4) results. In each case, we compare
our strongest baseline with our final multi-task
model by taking a random sample of 200 gener-
ated captions (or entailed hypotheses) from the test
set and removing the model identity to anonymize
the two models, and ask the human evaluator to
choose the better model based on relevance and
coherence (described in Sec. 4.2). As shown in
Table 5, the multi-task models are always better
than the strongest baseline for both video caption-
ing and entailment generation, on both relevance

10Following previous work, we only use METEOR be-
cause M-VAD only has a single reference caption per video.

11Note that this many-to-one model prefers a different mix-
ing ratio and learning rate than the many-to-one model for
improving video captioning (Sec. 5.1), because these hyper-
parameters depend on the primary task being improved, as
also discussed in previous work (Luong et al., 2016).

1280



(a) (b) (c)

Figure 5: Examples of generated video captions on the YouTube2Text dataset: (a) complex examples where the multi-task
model performs better than the baseline; (b) ambiguous examples (i.e., ground truth itself confusing) where multi-task model
still correctly predicts one of the possible categories (c) complex examples where both models perform poorly.

YouTube2Text Entailment
Relev. Coher. Relev. Coher.

Not Distinguish. 65.0% 93.0% 73.5% 94.5%
Baseline Wins 14.0% 1.0% 12.5% 1.5%
Multi-Task Wins 21.0% 6.0% 15.0% 4.0%

Table 5: Human evaluation on captioning and entailment.

Given Premise Generated
Entailment

a man on stilts is playing a tuba for
money on the boardwalk

a man is playing
an instrument

a girl looking through a large tele-
scope on a school trip

a girl is looking
at something

several young people sit at a table
playing poker

people are play-
ing a game

the stop sign is folded up against the
side of the bus

the sign is not
moving

a blue and silver monster truck mak-
ing a huge jump over crushed cars

a truck is being
driven

Table 6: Examples of our multi-task model’s generated en-
tailment hypotheses given a premise.

and coherence, and with similar improvements (2-
7%) as the automatic metrics (shown in Table 1).

5.5 Analysis
Fig. 5 shows video captioning generation re-
sults on the YouTube2Text dataset where our fi-
nal M-to-M multi-task model is compared with
our strongest attention-based baseline model for
three categories of videos: (a) complex examples
where the multi-task model performs better than
the baseline; (b) ambiguous examples (i.e., ground
truth itself confusing) where multi-task model still
correctly predicts one of the possible categories
(c) complex examples where both models perform
poorly. Overall, we find that the multi-task model
generates captions that are better at both temporal
action prediction and logical entailment (i.e., cor-
rect subset of full video premise) w.r.t. the ground
truth captions. The supplementary also provides

ablation examples of improvements by the 1-to-M
video prediction based multi-task model alone, as
well as by the M-to-1 entailment based multi-task
model alone (over the baseline).

On analyzing the cases where the baseline is
better than the final M-to-M multi-task model, we
find that these are often scenarios where the multi-
task model’s caption is also correct but the base-
line caption is a bit more specific, e.g., “a man is
holding a gun” vs “a man is shooting a gun”.

Finally, Table 6 presents output examples of our
entailment generation multi-task model (Sec. 5.3),
showing how the model accurately learns to pro-
duce logically implied subsets of the premise.

6 Conclusion

We presented a multimodal, multi-task learning
approach to improve video captioning by incor-
porating temporally and logically directed knowl-
edge via video prediction and entailment genera-
tion tasks. We achieve the best reported results
(and rank) on three datasets, based on multiple au-
tomatic and human evaluations. We also show mu-
tual multi-task improvements on the new entail-
ment generation task. In future work, we are ap-
plying our entailment-based multi-task paradigm
to other directed language generation tasks such as
image captioning and document summarization.

Acknowledgments

We thank the anonymous reviewers for their help-
ful comments. This work was partially supported
by a Google Faculty Research Award, an IBM
Faculty Award, a Bloomberg Data Science Re-
search Grant, and NVidia GPU awards.

1281



References

Andreas Argyriou, Theodoros Evgeniou, and Massim-
iliano Pontil. 2007. Multi-task feature learning. In
NIPS.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In ICLR.

Samuel R Bowman, Gabor Angeli, Christopher Potts,
and Christopher D Manning. 2015. A large anno-
tated corpus for learning natural language inference.
In EMNLP.

Rich Caruana. 1998. Multitask learning. In Learning
to learn, Springer, pages 95–133.

David L Chen and William B Dolan. 2011. Collect-
ing highly parallel data for paraphrase evaluation. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies-Volume 1. Association for Com-
putational Linguistics, pages 190–200.

Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakr-
ishna Vedantam, Saurabh Gupta, Piotr Dollár, and
C Lawrence Zitnick. 2015. Microsoft COCO cap-
tions: Data collection and evaluation server. arXiv
preprint arXiv:1504.00325 .

Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai
Li, and Li Fei-Fei. 2009. ImageNet: A large-scale
hierarchical image database. In CVPR. IEEE, pages
248–255.

Michael Denkowski and Alon Lavie. 2014. Meteor
universal: Language specific translation evaluation
for any target language. In EACL.

Bradley Efron and Robert J Tibshirani. 1994. An intro-
duction to the bootstrap. CRC press.

Sergio Guadarrama, Niveda Krishnamoorthy, Girish
Malkarnenkar, Subhashini Venugopalan, Raymond
Mooney, Trevor Darrell, and Kate Saenko. 2013.
Youtube2text: Recognizing and describing arbitrary
activities using semantic hierarchies and zero-shot
recognition. In CVPR. pages 2712–2719.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation
9(8):1735–1780.

Haiqi Huang, Yueming Lu, Fangwei Zhang, and
Songlin Sun. 2013. A multi-modal clustering
method for web videos. In International Conference
on Trustworthy Computing and Services. pages 163–
169.

Sergey Ioffe and Christian Szegedy. 2015. Batch nor-
malization: Accelerating deep network training by
reducing internal covariate shift. In ICML.

Sergio Jimenez, George Duenas, Julia Baquero,
Alexander Gelbukh, Av Juan Dios Bátiz, and
Av Mendizábal. 2014. UNAL-NLP: Combining soft
cardinality features for semantic textual similarity,
relatedness and entailment. In In SemEval. pages
732–742.

Diederik Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. In ICLR.

Vladyslav Kolesnyk, Tim Rocktäschel, and Sebastian
Riedel. 2016. Generating natural language inference
chains. arXiv preprint arXiv:1606.01404 .

Abhishek Kumar and Hal Daumé III. 2012. Learning
task grouping and overlap in multi-task learning. In
ICML.

Alice Lai and Julia Hockenmaier. 2014. Illinois-LH: A
denotational and distributional approach to seman-
tics. Proc. SemEval 2:5.

Chin-Yew Lin. 2004. ROUGE: A package for auto-
matic evaluation of summaries. In Text Summa-
rization Branches Out: Proceedings of the ACL-04
workshop. volume 8.

Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol
Vinyals, and Lukasz Kaiser. 2016. Multi-task se-
quence to sequence learning. In ICLR.

Eric W Noreen. 1989. Computer-intensive methods for
testing hypotheses. Wiley New York.

Pingbo Pan, Zhongwen Xu, Yi Yang, Fei Wu, and Yuet-
ing Zhuang. 2016a. Hierarchical recurrent neural
encoder for video representation with application to
captioning. In CVPR. pages 1029–1038.

Yingwei Pan, Tao Mei, Ting Yao, Houqiang Li, and
Yong Rui. 2016b. Jointly modeling embedding and
translation to bridge video and language. In CVPR.
pages 4594–4602.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In ACL. pages
311–318.

Karen Simonyan and Andrew Zisserman. 2015. Very
deep convolutional networks for large-scale image
recognition. In ICLR.

Khurram Soomro, Amir Roshan Zamir, and Mubarak
Shah. 2012. UCF101: A dataset of 101 human ac-
tions classes from videos in the wild. arXiv preprint
arXiv:1212.0402 .

Nitish Srivastava, Elman Mansimov, and Ruslan
Salakhutdinov. 2015. Unsupervised learning of
video representations using lstms. In ICML. pages
843–852.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In NIPS. pages 3104–3112.

1282



Christian Szegedy, Sergey Ioffe, and Vincent Van-
houcke. 2016. Inception-v4, inception-resnet and
the impact of residual connections on learning. In
CoRR.

Christian Szegedy, Wei Liu, Yangqing Jia, Pierre
Sermanet, Scott Reed, Dragomir Anguelov, Du-
mitru Erhan, Vincent Vanhoucke, and Andrew Ra-
binovich. 2015. Going deeper with convolutions. In
CVPR. pages 1–9.

Jesse Thomason, Subhashini Venugopalan, Sergio
Guadarrama, Kate Saenko, and Raymond J Mooney.
2014. Integrating language and vision to generate
natural language descriptions of videos in the wild.
In COLING.

Atousa Torabi, Christopher Pal, Hugo Larochelle, and
Aaron Courville. 2015. Using descriptive video ser-
vices to create a large data source for video annota-
tion research. arXiv preprint arXiv:1503.01070 .

Ramakrishna Vedantam, C Lawrence Zitnick, and Devi
Parikh. 2015. CIDEr: Consensus-based image de-
scription evaluation. In CVPR. pages 4566–4575.

Subhashini Venugopalan, Lisa Anne Hendricks, Ray-
mond Mooney, and Kate Saenko. 2016. Improving
lstm-based video description with linguistic knowl-
edge mined from text. In EMNLP.

Subhashini Venugopalan, Marcus Rohrbach, Jeffrey
Donahue, Raymond Mooney, Trevor Darrell, and
Kate Saenko. 2015a. Sequence to sequence-video
to text. In CVPR. pages 4534–4542.

Subhashini Venugopalan, Huijuan Xu, Jeff Donahue,
Marcus Rohrbach, Raymond Mooney, and Kate
Saenko. 2015b. Translating videos to natural lan-
guage using deep recurrent neural networks. In
NAACL HLT .

Jun Xu, Tao Mei, Ting Yao, and Yong Rui. 2016. Msr-
vtt: A large video description dataset for bridging
video and language. In CVPR. pages 5288–5296.

Li Yao, Atousa Torabi, Kyunghyun Cho, Nicolas Bal-
las, Christopher Pal, Hugo Larochelle, and Aaron
Courville. 2015. Describing videos by exploiting
temporal structure. In CVPR. pages 4507–4515.

Haonan Yu, Jiang Wang, Zhiheng Huang, Yi Yang, and
Wei Xu. 2016. Video paragraph captioning using
hierarchical recurrent neural networks. In CVPR.

1283


	Multi-Task Video Captioning with Video and Entailment Generation

