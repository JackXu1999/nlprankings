



















































Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics


Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 118–124
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-2019

Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 118–124
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-2019

A Generative Parser with a Discriminative Recognition Algorithm

Jianpeng Cheng Adam Lopez and Mirella Lapata
School of Informatics, University of Edinburgh

jianpeng.cheng@ed.ac.uk, {alopez,mlap}@inf.ed.ac.uk

Abstract

Generative models defining joint distribu-
tions over parse trees and sentences are
useful for parsing and language modeling,
but impose restrictions on the scope of fea-
tures and are often outperformed by dis-
criminative models. We propose a frame-
work for parsing and language modeling
which marries a generative model with
a discriminative recognition model in an
encoder-decoder setting. We provide in-
terpretations of the framework based on
expectation maximization and variational
inference, and show that it enables parsing
and language modeling within a single im-
plementation. On the English Penn Treen-
bank, our framework obtains competi-
tive performance on constituency parsing
while matching the state-of-the-art single-
model language modeling score.1

1 Introduction

Generative models defining joint distributions
over parse trees and sentences are good theoretical
models for interpreting natural language data, and
appealing tools for tasks such as parsing, grammar
induction and language modeling (Collins, 1999;
Henderson, 2003; Titov and Henderson, 2007;
Petrov and Klein, 2007; Dyer et al., 2016). How-
ever, they often impose strong independence as-
sumptions which restrict the use of arbitrary fea-
tures for effective disambiguation. Moreover, gen-
erative parsers are typically trained by maximiz-
ing the joint probability of the parse tree and the
sentence—an objective that only indirectly relates
to the goal of parsing. At test time, these mod-
els require a relatively expensive recognition algo-

1Our code is available at https://github.com/
cheng6076/virnng.git.

rithm (Collins, 1999; Titov and Henderson, 2007)
to recover the parse tree, but the parsing per-
formance consistently lags behind their discrim-
inative competitors (Nivre et al., 2007; Huang,
2008; Goldberg and Elhadad, 2010), which are di-
rectly trained to maximize the conditional proba-
bility of the parse tree given the sentence, where
linear-time decoding algorithms exist (e.g., for
transition-based parsers).

In this work, we propose a parsing and lan-
guage modeling framework that marries a gener-
ative model with a discriminative recognition al-
gorithm in order to have the best of both worlds.
The idea of combining these two types of mod-
els is not new. For example, Collins and Koo
(2005) propose to use a generative model to gen-
erate candidate constituency trees and a discrimi-
native model to rank them. Sangati et al. (2009)
follow the opposite direction and employ a gener-
ative model to re-rank the dependency trees pro-
duced by a discriminative parser. However, pre-
vious work combines the two types of models in a
goal-oriented, pipeline fashion, which lacks model
interpretations and focuses solely on parsing.

In comparison, our framework unifies genera-
tive and discriminative parsers with a single objec-
tive, which connects to expectation maximization
and variational inference in grammar induction
settings. In a nutshell, we treat parse trees as latent
factors generating natural language sentences and
parsing as a posterior inference task. We showcase
the framework using Recurrent Neural Network
Grammars (RNNGs; Dyer et al. 2016), a recently
proposed probabilistic model of phrase-structure
trees based on neural transition systems. Differ-
ent from this work which introduces separately
trained discriminative and generative models, we
integrate the two in an auto-encoder which fits our
training objective. We show how the framework
enables grammar induction, parsing and language

118

https://doi.org/10.18653/v1/P17-2019
https://doi.org/10.18653/v1/P17-2019


modeling within a single implementation. On the
English Penn Treebank, we achieve competitive
performance on constituency parsing and state-of-
the-art single-model language modeling score.

2 Preliminaries

In this section we briefly describe Recurrent Neu-
ral Network Grammars (RNNGs; Dyer et al.
2016), a top-down transition-based algorithm for
parsing and generation. There are two versions of
RNNG, one discriminative, the other generative.
We follow the original paper in presenting the dis-
criminative variant first.

The discriminative RNNG follows a shift-
reduce parser that converts a sequence of words
into a parse tree. As in standard shift-reduce
parsers, the RNNG uses a buffer to store unpro-
cessed terminal symbols and a stack to store par-
tially completed syntactic constituents. At each
timestep, one of the following three operations2 is
performed:

• NT(X) introduces an open non-terminal X
onto the top of the stack, represented as an
open parenthesis followed by X, e.g., (NP.

• SHIFT fetches the terminal in the front of the
buffer and pushes it onto the top of the stack.

• REDUCE completes a subtree by repeatedly
popping the stack until an open non-terminal
is encountered. The non-terminal is popped
as well, after which a composite term repre-
senting the entire subtree is pushed back onto
the top of the stack, e.g., (NP the cat).

The above transition system can be adapted
with minor modifications to an algorithm that gen-
erates trees and sentences. In generator tran-
sitions, there is no input buffer of unprocessed
words but there is an output buffer for storing
words that have been generated. To reflect the
change, the previous SHIFT operation is modified
into a GEN operation defined as follows:

• GEN generates a terminal symbol and add it
to the stack and the output buffer.

2To be precise, the total number of operations under our
description is |X|+2 since the NT operation varies with the
non-terminal choice X.

3 Methodology

Our framework unifies generative and discrimi-
native parsers within a single training objective.
For illustration, we adopt the two RNNG variants
introduced above with our customized features.
Our starting point is the generative model (§ 3.1),
which allows us to make explicit claims about the
generative process of natural language sentences.
Since this model alone lacks a bottom-up recog-
nition mechanism, we introduce a discriminative
recognition model (§ 3.2) and connect it with the
generative model in an encoder-decoder setting.
To offer a clear interpretation of the training objec-
tive (§ 3.3), we first consider the parse tree as la-
tent and the sentence as observed. We then discuss
extensions that account for labeled parse trees. Fi-
nally, we present various inference techniques for
parsing and language modeling within the frame-
work (§ 3.4).

3.1 Decoder (Generative Model)
The decoder is a generative RNNG that models
the joint probability p(x, y) of a latent parse tree y
and an observed sentence x. Since the parse tree
is defined by a sequence of transition actions a,
we write p(x, y) as p(x, a).3 The joint distribu-
tion p(x, a) is factorized into a sequence of transi-
tion probabilities and terminal probabilities (when
actions are GEN), which are parametrized by a
transitional state embedding u:

p(x, a) = p(a)p(x|a)

=

|a|∏

t=1

p(at|ut)p(xt|ut)I(at=GEN) (1)

where I is an indicator function and ut represents
the state embedding at time step t. Specifically,
the conditional probability of the next action is:

p(at|ut) =
|a|∏

t=1

exp(atu
T
t + ba)∑

a′∈A exp(a
′uTt + ba′)

(2)

where at represents the action embedding at time
step t, A the action space and ba the bias. Simi-
larly, the next word probability (when GEN is in-
voked) is computed as:

p(wt|ut) =
|a|∏

t=1

exp(wtu
T
t + bw)∑

w′∈W exp(w
′uTt + bw′)

(3)

3We assume that the action probability does not take the
actual terminal choice into account.

119



whereW denotes all words in the vocabulary.
To satisfy the independence assumptions im-

posed by the generative model, ut uses only a
restricted set of features defined over the output
buffer and the stack — we consider p(a) as a con-
text insensitive prior distribution. Specifically, we
use the following features: 1) the stack embed-
ding dt which encodes the stack of the decoder
and is obtained with a stack-LSTM (Dyer et al.,
2015, 2016); 2) the output buffer embedding ot;
we use a standard LSTM to compose the output
buffer and ot is represented as the most recent state
of the LSTM; and 3) the parent non-terminal em-
bedding nt which is accessible in the generative
model because the RNNG employs a depth-first
generation order. Finally, ut is computed as:

ut = W2 tanh(W1[dt,ot,nt] + bd) (4)

where Ws are weight parameters and bd the bias.

3.2 Encoder (Recognition Model)
The encoder is a discriminative RNNG that com-
putes the conditional probability q(a|x) of the
transition action sequence a given an observed
sentence x. This conditional probability is factor-
ized over time steps as:

q(a|x) =
|a|∏

t=1

q(at|vt) (5)

where vt is the transitional state embedding of the
encoder at time step t.

The next action is predicted similarly to Equa-
tion (2), but conditioned on vt. Thanks to the dis-
criminative property, vt has access to any contex-
tual features defined over the entire sentence and
the stack — q(a|x) acts as a context sensitive pos-
terior approximation. Our features4 are: 1) the
stack embedding et obtained with a stack-LSTM
that encodes the stack of the encoder; 2) the in-
put buffer embedding it; we use a bidirectional
LSTM to compose the input buffer and repre-
sent each word as a concatenation of forward and
backward LSTM states; it is the representation
of the word on top of the buffer; 3) to incorpo-
rate more global features and a more sophisticated
look-ahead mechanism for the buffer, we also use
an adaptive buffer embedding īt; the latter is com-
puted by having the stack embedding et attend to

4Compared to Dyer et al. (2016), the new features we in-
troduce are 3) and 4), which we found empirically useful.

all remaining embeddings on the buffer with the
attention function in Vinyals et al. (2015); and
4) the parent non-terminal embedding nt. Finally,
vt is computed as follows:

vt = W4 tanh(W3[et, it, īt,nt] + be) (6)

where Ws are weight parameters and be the bias.

3.3 Training

Consider an auto-encoder whose encoder infers
the latent parse tree and the decoder generates the
observed sentence from the parse tree.5 The max-
imum likelihood estimate of the decoder parame-
ters is determined by the log marginal likelihood
of the sentence:

log p(x) = log
∑

a

p(x, a) (7)

We follow expectation-maximization and varia-
tional inference techniques to construct an ev-
idence lower bound of the above quantity (by
Jensen’s Inequality), denoted as follows:

log p(x) ≥ Eq(a|x) log
p(x, a)

q(a|x) = Lx (8)

where p(x, a) = p(x|a)p(a) comes from the de-
coder or the generative model, and q(a|x) comes
from the encoder or the recognition model. The
objective function6 in Equation (8), denoted by
Lx, is unsupervised and suited to a grammar in-
duction task. This objective can be optimized with
the methods shown in Miao and Blunsom (2016).

Next, consider the case when the parse tree
is observed. We can directly maximize the log
likelihood of the parse tree for the encoder out-
put log q(a|x) and the decoder output log p(a):

La = log q(a|x) + log p(a) (9)

This supervised objective leverages extra informa-
tion of labeled parse trees to regularize the distri-
bution q(a|x) and p(a), and the final objective is:

L = Lx + La (10)
where Lx and La can be balanced with the task
focus (e.g, language modeling or parsing).

5Here, GEN and SHIFT refer to the same action with dif-
ferent definitions for encoding and decoding.

6See § 4 and Appendix A for comparison between this
objective and the importance sampler of Dyer et al. (2016).

120



Learned word embedding dimensions 40
Pretrained word embedding dimensions 50
POS tag embedding dimensions 20
Encoder LSTM dimensions 128
Decoder LSTM dimensions 256
LSTM layer 2
Encoder dropout 0.2
Decoder dropout 0.3

Table 1: Hyperparameters.

3.4 Inference

We consider two inference tasks, namely parsing
and language modeling.

Parsing In parsing, we are interested in the
parse tree that maximizes the posterior p(a|x) (or
the joint p(a, x)). However, the decoder alone
does not have a bottom-up recognition mecha-
nism for computing the posterior. Thanks to the
encoder, we can compute an approximated pos-
terior q(a|x) in linear time and select the parse
tree that maximizes this approximation. An al-
ternative is to generate candidate trees by sam-
pling from q(a|x), re-rank them with respect to
the joint p(x, a) (which is proportional to the true
posterior), and select the sample that maximizes
the true posterior.

Language Modeling In language modeling, our
goal is to compute the marginal probability
p(x) =

∑
a p(x, a), which is typically intractable.

To approximate this quantity, we can use Equa-
tion (8) to compute a lower bound of the log like-
lihood log p(x) and then exponentiate it to get a
pessimistic approximation of p(x).7

Another way of computing p(x) (without lower
bounding) would be to use the variational approxi-
mation q(a|x) as the proposal distribution as in the
importance sampler of Dyer et al. (2016). How-
ever, this is beyond the scope of this work and we
leave detailed discussions to Appendix A.

4 Related Work

Our framework is related to a class of variational
autoencoders (Kingma and Welling, 2014), which
use neural networks for posterior approximation
in variational inference. This technique has been
previously used for topic modeling (Miao et al.,

7As a reminder, the language modeling objective is
exp(NLL/T ), where NLL denotes the total negative log
likelihood of the test data and T the token counts.

Discrimina-
tive parsers

Socher et al. (2013) 90.4
Zhu et al. (2013) 90.4
Dyer et al. (2016) 91.7
Cross and Huang (2016) 89.9
Vinyals et al. (2015) 92.8

Generative
parsers

Petrov and Klein (2007) 90.1
Shindo et al. (2012) 92.4
Dyer et al. (2016) 93.3

This work
argmaxa q(a|x) 89.3
argmaxa p(a, x) 90.1

Table 2: Parsing results (F1) on the PTB test set.

2016) and sentence compression (Miao and Blun-
som, 2016). Another interpretation of the pro-
posed framework is from the perspective of guided
policy search in reinforcement learning (Bachman
and Precup, 2015), where a generative parser is
trained to imitate the trace of a discriminative
parser. Further connections can be drawn with
the importance-sampling based inference of Dyer
et al. (2016). There, a generative RNNG and a
discriminative RNNG are trained separately; dur-
ing language modeling, the output of the discrim-
inative model serves as the proposal distribution
of an importance sampler p(x) = Eq(a|x)

p(x,a)
q(a|x) .

Compared to their work, we unify the generative
and discriminative RNNGs in a single framework,
and adopt a joint training objective.

5 Experiments

We performed experiments on the English Penn
Treebank dataset; we used sections 2–21 for train-
ing, 24 for validation, and 23 for testing. Follow-
ing Dyer et al. (2015), we represent each word
in three ways: as a learned vector, a pretrained
vector, and a POS tag vector. The encoder word
embedding is the concatenation of all three vec-
tors while the decoder uses only the first two since
we do not consider POS tags in generation. Ta-
ble 1 presents details on the hyper-parameters we
used. To find the MAP parse tree argmaxa p(a, x)
(where p(a, x) is used rank the output of q(a|x))
and to compute the language modeling perplex-
ity with the evidence lower bound (where a ∼
q(a|x)), we collect 100 samples from q(a|x),
same as Dyer et al. (2016).

Experimental results for constituency parsing
and language modeling are shown in Tables 2
and 3, respectively. As can be seen, the single
framework we propose obtains competitive pars-

121



KN-5 255.2
LSTM 113.4
Dyer et al. (2016) 102.4
This work: a ∼ q(a|x) 99.8

Table 3: Language modeling results (perplexity).

ing performance. Comparing the two inference
methods for parsing, ranking approximated MAP
trees from q(a|x) with respect to p(a, x) yields a
small improvement, as in Dyer et al. (2016). It is
worth noting that our parsing performance lags be-
hind Dyer et al. (2016). We believe this is due to
implementation disparities, such as the modeling
of the reduce operation. While Dyer et al. (2016)
use an LSTM as the syntactic composition func-
tion of each subtree, we adopt a rather simple com-
position function based on embedding averaging
for memory concern.

On language modeling, our framework achieves
lower perplexity compared to Dyer et al. (2016)
and baseline models. This gain possibly comes
from the joint optimization of both the genera-
tive and discriminative components towards a lan-
guage modeling objective. However, we acknowl-
edge a subtle difference between Dyer et al. (2016)
and our approach compared to baseline language
models: while the latter incrementally estimate
the next word probability, our approach (and Dyer
et al. 2016) directly assigns probability to the en-
tire sentence. Overall, the advantage of our frame-
work compared to Dyer et al. (2016) is that it
opens an avenue to unsupervised training.

6 Conclusions

We proposed a framework that integrates a gen-
erative parser with a discriminative recognition
model and showed how it can be instantiated with
RNNGs. We demonstrated that a unified frame-
work, which relates to expectation maximization
and variational inference, enables effective pars-
ing and language modeling algorithms. Evalua-
tion on the English Penn Treebank, revealed that
our framework obtains competitive performance
on constituency parsing and state-of-the-art results
on single-model language modeling. In the fu-
ture, we would like to perform grammar induc-
tion based on Equation (8), with gradient descent
and posterior regularization techniques (Ganchev
et al., 2010).

Acknowledgments We thank three anonymous
reviewers and members of the ILCC for valu-
able feedback, and Muhua Zhu and James Cross
for help with data preparation. The support of
the European Research Council under award num-
ber 681760 “Translating Multiple Modalities into
Text” is gratefully acknowledged.

References
Philip Bachman and Doina Precup. 2015. Data gener-

ation as sequential decision making. In Advances in
Neural Information Processing Systems, MIT Press,
pages 3249–3257.

Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania, Pennsylvania, Philadel-
phia.

Michael Collins and Terry Koo. 2005. Discriminative
reranking for natural language parsing. Computa-
tional Linguistics 31(1):25–70.

James Cross and Liang Huang. 2016. Incremental
parsing with minimal features using bi-directional
lstm. In Proceedings of the 54th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 2: Short Papers). Berlin, Germany, pages 32–
37.

Chris Dyer, Miguel Ballesteros, Wang Ling, Austin
Matthews, and Noah A. Smith. 2015. Transition-
based dependency parsing with stack long short-
term memory. In Proceedings of the 53rd Annual
Meeting of the Association for Computational Lin-
guistics and the 7th International Joint Conference
on Natural Language Processing (Volume 1: Long
Papers). Beijing, China, pages 334–343.

Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros,
and Noah A. Smith. 2016. Recurrent neural net-
work grammars. In Proceedings of the 2016 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies. San Diego, California, pages
199–209.

Kuzman Ganchev, Jennifer Gillenwater, Ben Taskar,
et al. 2010. Posterior regularization for structured
latent variable models. Journal of Machine Learn-
ing Research 11(Jul):2001–2049.

Zoubin Ghahramani and Matthew J. Beal. 2000. Vari-
ational inference for Bayesian mixtures of factor
analysers. In Advances in Neural Information Pro-
cessing Systems, MIT Press, pages 449–455.

Yoav Goldberg and Michael Elhadad. 2010. An effi-
cient algorithm for easy-first non-directional depen-
dency parsing. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics. Los Angeles, California, pages 742–750.

122



James Henderson. 2003. Inducing history representa-
tions for broad coverage statistical parsing. In Pro-
ceedings of the 2003 Human Language Technology
Conference of the North American Chapter of the
Association for Computational Linguistics. Edmon-
ton, Canada, pages 24–31.

Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
ACL-08: HLT . Columbus, Ohio, pages 586–594.

Diederik P Kingma, Shakir Mohamed, Danilo Jimenez
Rezende, and Max Welling. 2014. Semi-supervised
learning with deep generative models. In Advances
in Neural Information Processing Systems, MIT
Press, pages 3581–3589.

Diederik P Kingma and Max Welling. 2014. Auto-
encoding variational Bayes. In Proceedings of the
International Conference on Learning Representa-
tions. Banff, Canada.

Yishu Miao and Phil Blunsom. 2016. Language as a
latent variable: Discrete generative models for sen-
tence compression. In Proceedings of the 2016 Con-
ference on Empirical Methods in Natural Language
Processing. Austin, Texas, pages 319–328.

Yishu Miao, Lei Yu, and Phil Blunsom. 2016. Neu-
ral variational inference for text processing. In Pro-
ceedings of The 33rd International Conference on
Machine Learning. New York, New York, USA,
pages 1727–1736.

Andriy Mnih and Karol Gregor. 2014. Neural vari-
ational inference and learning in belief networks.
In Proceedings of the 31st International Conference
on Machine Learning. Beijing, China, pages 1791–
1799.

Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, Gülsen Eryigit, Sandra Kübler, Svetoslav
Marinov, and Erwin Marsi. 2007. Maltparser: A
language-independent system for data-driven de-
pendency parsing. Natural Language Engineering
13(2):95–135.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP). Doha, Qatar, pages 1532–
1543.

Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics; Proceedings of the Main Confer-
ence. Rochester, New York, pages 404–411.

Reuven Y Rubinstein and Dirk P Kroese. 2008. Simu-
lation and the Monte Carlo method. John Wiley &
Sons.

Federico Sangati, Willem Zuidema, and Rens Bod.
2009. A generative re-ranking model for depen-
dency parsing. In Proceedings of the 11th In-
ternational Conference on Parsing Technologies
(IWPT’09). Paris, France, pages 238–241.

Hiroyuki Shindo, Yusuke Miyao, Akinori Fujino, and
Masaaki Nagata. 2012. Bayesian symbol-refined
tree substitution grammars for syntactic parsing.
In Proceedings of the 50th Annual Meeting of the
Association for Computational Linguistics: Long
Papers-Volume 1. Jeju Island, Korea, pages 440–
448.

Richard Socher, John Bauer, Christopher D. Manning,
and Ng Andrew Y. 2013. Parsing with composi-
tional vector grammars. In Proceedings of the 51st
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers). Sofia,
Bulgaria, pages 455–465.

Ivan Titov and James Henderson. 2007. Constituent
parsing with incremental sigmoid belief networks.
In Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics. Prague,
Czech Republic, pages 632–639.

Oriol Vinyals, Łukasz Kaiser, Terry Koo, Slav Petrov,
Ilya Sutskever, and Geoffrey Hinton. 2015. Gram-
mar as a foreign language. In Advances in Neural
Information Processing Systems, MIT Press, pages
2773–2781.

Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang,
and Jingbo Zhu. 2013. Fast and accurate shift-
reduce constituent parsing. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers). Sofia,
Bulgaria, pages 434–443.

A Comparison to Importance Sampling
(Dyer et al., 2016)

In this appendix we highlight the connections be-
tween importance sampling and variational infer-
ence, thereby comparing our method with Dyer
et al. (2016).

Consider a simple directed graphical model
with discrete latent variables a (e.g., a is the tran-
sition action sequence) and observed variables x
(e.g., x is the sentence). The model evidence, or
the marginal likelihood p(x) =

∑
a p(x, a) is of-

ten intractable to compute. Importance sampling
transforms the above quantity into an expectation
over a distribution q(a), which is known and easy
to sample from:

p(x) =
∑

a

p(x, a)
q(a)

q(a)
= Eq(a)w(x, a) (11)

where q(a) is the proposal distribution
and w(x, a) = p(x,a)q(a) the importance weight.

123



The proposal distribution can potentially depend
on the observations x, i.e., q(a) , q(a|x).

A challenge with importance sampling lies in
choosing a proposal distribution which leads to
low variance. As shown in Rubinstein and Kroese
(2008), the optimal choice of the proposal distri-
bution is in fact the true posterior p(a|x), in which
case the importance weight p(a,x)p(a|x) = p(x) is con-
stant with respect to a. In Dyer et al. (2016), the
proposal distribution depends on x, i.e., q(a) ,
q(a|x), and is computed with a separately-trained,
discriminative model. This proposal choice is
close to optimal, since in a fully supervised setting
a is also observed and the discriminative model
can be trained to approximate the true posterior
well. We hypothesize that the performance of their
importance sampler is dependent on this specific
proposal distribution. Besides, their training strat-
egy does not generalize to an unsupervised setting.

In comparison, variational inference approach
approximates the log marginal likelihood log p(x)
with the evidence lower bound. It is a natural
choice when one aims to optimize Equation (11)
directly:

log p(x) = log
∑

a

p(x, a)
q(a)

q(a)

≥ Eq(a) log
p(x, a)

q(a)

(12)

where q(a) is the variational approximation of
the true posterior. Again, the variational approx-
imation can potentially depend on the observa-
tion x (i.e., q(a) , q(a|x)) and can be com-
puted with a discriminative model. Equation (12)
is a well-defined, unsupervised training objec-
tive which allows us to jointly optimize genera-
tive (i.e., p(x, a)) and discriminative (i.e., q(a|x))
models. To further support the observed vari-
able a, we augment this objective with supervised
terms shown in Equation (10), following Kingma
et al. (2014) and Miao and Blunsom (2016).

Equation (12) can be also used to approximate
the marginal likelihood p(x) (e.g., in language
modeling) with its lower bound. An alternative
choice without lower bounding is to use the vari-
ational approximation q(a|x) as the proposal dis-
tribution in importance sampling (Equation (11)).
Ghahramani and Beal (2000) show that this pro-
posal distribution leads to improved results of im-
portance samplers. However, a potential drawback
of importance sampling-based approach is that it

is prone to numerical underflow. In practice, we
observed similar language modeling performance
for both methods.

124


	A Generative Parser with a Discriminative Recognition Algorithm

