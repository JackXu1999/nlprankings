















































Automatic Transformation of the Thai Categorial Grammar Treebank to Dependency Trees


Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 1243–1250,
Chiang Mai, Thailand, November 8 – 13, 2011. c©2011 AFNLP

Automatic Transformation of the Thai Categorial Grammar
Treebank to Dependency Trees

Christian Rishøj
CST

University of Copenhagen
crjensen@hum.ku.dk

Taneth Ruangrajitpakorn
HLT Lab
NECTEC

taneth.rua@nectec.or.th

Prachya Boonkwan
HLT Lab
NECTEC

prachya.boo@nectec.or.th

Thepchai Supnithi
HLT Lab
NECTEC

thepchai.sup@nectec.or.th

Abstract
A method for deriving an approximately
labeled dependency treebank from the
Thai Categorial Grammar Treebank has
been implemented. The method involves
a lexical dictionary for assigning depen-
dency directions to the CG types associ-
ated with the grammatical entities in the
CG bank, falling back on a generic map-
ping of CG types in case of unknown words.
Currently, all but a handful of the trees
in the Thai CG bank can unambiguously
be transformed into directed dependency
trees. Dependency labels can optionally be
assigned with a learned classifier, which in
a preliminary evaluation with a very small
training set achieves 76.5% label accuracy.
In the process, a number of annotation er-
rors in the CG bank were identified and
corrected. Although rather limited in its
coverage, excluding e.g. long-distance de-
pendencies, topicalisations and longer sen-
tences, the resulting treebank is believed
to be sound in terms of structural annota-
tional consistency and a valuable comple-
ment to the scarce Thai language resources
in existence.

1 Introduction
Syntactic resources play an essential role for the
majority of NLP applications, but for the Thai lan-
guage, openly available syntactic resources are few
in number: So far, the only reported resources are
the CG treebank [Ruangrajitpakorn et al., 2009]
and the NAiST dependency bank [Wacharaman-
otham et al., 2007, Sudprasert, 2008]. Others are
either unpublished or minuscule in size. Rather
than relying exclusively on labor-intensive manual
annotation for further expanding the resources, it
would be economically sound to leverage existing
efforts and transform an existing treebank in one
formalism into another.

1.1 Categorial grammar
Categorial grammar (CG) is a lexicalised theory in
natural language syntax motivated by the principle

of constitutionality and organised according to the
syntactic elements [Steedman, 2000, Ajdukiewicz,
1935], and forms the theoretical basis for the Thai
CG treebank. The resource building effort has
been very fruitful, but there remains phenomena of
Thai language, including long-distance dependen-
cies and topicalisation [Warotamasikkhadit, 1997],
which are unhandled by the instantiation of CG
currently in use.

Additionally, although the Thai language
belongs to a fixed word order typology, Thai
spoken language exhibits some flexibility in word
order, due to the occasional preference of Thai
language users for correspondence in rhyme. As
an example, consider the following sentence1:

อังกฤษ คิดคน อยาง หนัก วัคซีน
ปองกันเชื้อ ไวรัส ไขหวัดนก
(Lit: England/NE invent/V “-ly”/ADVPFX
heavy/ADJ vaccine/N protect/V virus/N
avian_flu/NP)
“The British are strenuously developing an
Avian Flu vaccine.”

The adverbial compound formed by อยาง หนัก
(“strenuously”) conventionally occurs after the di-
rect object, but is in this sentence, it is realised
in the pre-direct object position2 in order for the
last syllable [ nakL] of อยาง หนัก to rhyme with
the first syllable [ wakH] of วัคซีน (“vaccine”). To
some degree, this phenomenon from spoken lan-
guage shines through in written language, espe-
cially in the domains of news and recent politics,
and is causing a challenge for the employment of
CG grammar.

1The Thai adverbialising prefix อยาง can be
likened to the English “-ly” suffix, which produces
an adverbial form from an adjective. Artificial word
boundaries are inserted for clarity.

2When language users exploit this flexibility in word
order to produce aesthetically pleasing sound patterns,
it results in a marked form, but the phenomenon
is nonetheless productive, and encountered frequently
enough to necessitate handling in NLP applications.

1243



NP

PP

DT NN IN DT JJ NN
the man in the funny hat

−− HD

MO

AC −− −− HD

NP

PP

NP

DT NN IN DT JJ NN
the man in the funny hat

−− HD

MO

AC −− −− HD

MO
PN

ATTR
DET

PP

DET

the man in the funny hat

Figure 1: Ambiguous representations in phrase structure space.

VP

VP

PRP VBP RB VBN PRP
I have not seen him

SBJ HD MO

−−

−− −−
OBJA

AUX

ADVSUBJ

I have not seen him

OBJA

AUX

ADV
SUBJ

I have not seen him

Figure 2: Ambiguous representations in dependency space.

can cause several constituents to be considered erroneous,
it will always cause exactly one wrong dependency. This
error measure can therefore be considered more intuitively
adequate.
Dependency representation also makes it easy to evalu-

ate the performance of a parser selectively. If edge labels
are used, common tasks such as judging the performance of
a parser for subject detection are solved by counting only
the corresponding edges – a trivial modification of the nor-
mal evaluation method.
One other advantage of representing language as depen-

dencies is that some gratuitous ambiguities are avoided en-
tirely. Many of these are constructions where it is unclear
whether an embedded level of phrase structure should be
assumed or not (see Figure 1). For example, there is no
consensus about whether particular languages actually do
or do not have verb phrases; in a dependency representa-
tion this issue simply does not arise.
To be sure, there are also instances of ambiguities that

arise only in dependency representation. Typically these
are constructions where more than two constituents would
form a larger phrase (see Figure 2). Lin proposes the auto-
matic normalization of dependency trees to eliminate such
inconsequential differences in an extra step.
There are also some constructions where several depen-

dency structures are possible but none is preferable. For in-
stance, in conjunction phrases there is no consensus about
which word is the phrase head, and in fact every solution
causes problems from a linguistic perspective (see Figure
3). In such cases one representation is usually chosen arbi-
trarily.

CNP

NNP CC NNP
Lyn and Mary

−− HD −− CJCJ

Lyn and Mary

CJ

KON

Lyn and Mary

Figure 3: Different dependency structures for conjunction
phrases.

3. The Tool DEPSY
Lin’s proposed algorithm, taken from (Magerman,

1994), proceeds essentially as follows:

1. determine the head child of the root phrase node
2. apply the algorithm to all constituents of the head
child, and obtain its head word, which will constitute
the root word of the dependency tree

3. apply the algorithm to all siblings of the head child,
and subordinate their head words under the current
head word.

The head child of a phrase node is found by consulting
a head table that associates each phrase category with a
direction and a list of other categories; e.g. the head of a
PP might be defined as the first preposition to the left, and
the head of a VP as the first finite verb to the left. The
resulting dependencies are simple pairs of words, with no
edge labels. Also, only connected trees (where only one
root node exists) can be handled.
The tool DEPSY (Dependency Synthesizer) implements

this fundamental algorithm together with some extensions
to overcome its limitations. Extending the principle of
table-driven operation, the exact behaviour is controlled by
several tables beyond the head table:

• a table of additional conversion functions is consulted
for each node in the phrase tree before the fundamental
algorithm is applied.

• a label table is used during the transformation to
choose a label for each dependency edge.

• a second table of conversion functions is consulted for
each word in the dependency tree after the fundamen-
tal algorithm has been applied.

As an example of a useful conversion of phrase trees,
assume that PP phrases are annotated without an embedded
NP in a corpus (cf. Figure 1), which means that they can
have only one lexical head. The basic algorithm alone can-
not derive the desired dependency structure from the left
phrase tree. (Eisner, 1996) proposed inserting an embed-
ded NP into each PP to solve this problem. By adding this
conversion to our table of preprocessing functions we can

Figure 1: Example of ambiguity arising only in dependency representation [Daum et al., 2004]

1.2 Dependency representation

In recent years, dependency grammar has seen a
dramatic increase in interest, likely due to a num-
ber of appealing properties of the representation.
In comparison to phrase structure grammar, de-
pendency structures provide a relatively direct en-
coding of predicate-argument structure, which is
relevant to subsequent analyses [Nivre, 2005]. De-
pendency representation is arguably better suited
for languages with flexible word order. Addition-
ally, having no non-terminal nodes, dependency
structures are often perceived as leaving room for
less ambiguity as well as being more computation-
ally manageable.

Certainly, dependency representation has draw-
backs of its own in terms of ambiguity, some of
which are specific to dependency representation.
In particular, Figure 1 shows a construction with
an unambiguous constituent structure, which in
dependency space is ambiguous with respect to the
attachment of the adverb [Daum et al., 2004].

Furthermore, dependency structure allows for a
number of ways to represent coordinated phrases,
some having the coordinating conjunction as head
(CCH) of the coordinate structure, and a special
dependency label CJT that does not describe the
grammatical function of the conjuncts (Figure 2a).
Another option is having the coordinating conjunc-
tion as dependent (CCD) of one of the conjuncts,
thus allowing one conjunct to occur with a de-
pendency label denoting its grammatical function
(Figure 2b). McDonald and Nivre [2007] offer a
thorough review of these and other candidate anal-
yses in use. Unfortunately, none of the conceivable
representations are unproblematic from a linguistic
perspective [Daum et al., 2004], or offer the same
transparency as the coordination rules of CCG do
[Boonkwan, 2009].

Nonetheless, given the availability of generally
applicable, trainable dependency parsers, and re-
ports of beneficial applications of dependency anal-
ysis in tasks such as word-alignment [Ma et al.,
2008] and reordering [Chang et al., 2009] for statis-
tical machine translation, a dependency treebank
of good quality is a highly desirable resource.

1.1. DEPENDENCY GRAMMAR 5

Figure 1.3: Two analyses of coordination in dependency grammar.

optional. The valency frame of the verb is normally taken to include argument dependents, but some
theories also allow obligatory non-arguments to be included. Returning to figure 1.1, the subject
and the object would normally be treated as valency-bound dependents of the verb had, while the
adjectival modifiers of the nouns news and markets would be considered valency-free. The preposi-
tional modification of the noun effect may or may not be treated as valency-bound, depending on
whether the entity undergoing the effect is supposed to be an argument of the noun effect or not.
Another term that is sometimes used in connection with valency constraints is arity, which primarily
refers to the number of arguments that a predicate takes (without distinguishing the types of these
arguments).

While most head-complement and head-modifier structures have a straightforward analysis
in dependency grammar, there are also constructions that have a more unclear status. This group
includes constructions that involve grammatical function words, such as articles, complementizers
and auxiliary verbs, but also structures involving prepositional phrases. For these constructions, there
is no general consensus in the tradition of dependency grammar as to whether they should be analyzed
as dependency relations at all and, if so, what should be regarded as the head and what should be
regarded as the dependent. For example, some theories regard auxiliary verbs as heads taking lexical
verbs as dependents; other theories make the opposite assumption; and yet other theories assume
that verb chains are connected by relations that are not dependencies in the usual sense.

Another kind of construction that is problematic for dependency grammar (as for most the-
oretical traditions) is coordination. According to the structuralist tradition, coordination is an en-
docentric construction, since it contains not only one but several heads that can replace the whole
construction syntactically. However, this raises the question of whether coordination can be analyzed
in terms of binary relations holding between a head and a dependent. Consider the following simple
examples:

They operate ships and banks.
She bought and ate an apple.

In the first example, it seems clear that the phrase ships and banks functions as a direct object of the
verb operate, but it is not immediately clear how this phrase can be given an internal analysis that is
compatible with the basic assumptions of dependency grammar, since the two nouns ships and banks
seem to be equally good candidates for being heads. Similarly, in the second example, the noun apple

(a) CCH1.1. DEPENDENCY GRAMMAR 5

Figure 1.3: Two analyses of coordination in dependency grammar.

optional. The valency frame of the verb is normally taken to include argument dependents, but some
theories also allow obligatory non-arguments to be included. Returning to figure 1.1, the subject
and the object would normally be treated as valency-bound dependents of the verb had, while the
adjectival modifiers of the nouns news and markets would be considered valency-free. The preposi-
tional modification of the noun effect may or may not be treated as valency-bound, depending on
whether the entity undergoing the effect is supposed to be an argument of the noun effect or not.
Another term that is sometimes used in connection with valency constraints is arity, which primarily
refers to the number of arguments that a predicate takes (without distinguishing the types of these
arguments).

While most head-complement and head-modifier structures have a straightforward analysis
in dependency grammar, there are also constructions that have a more unclear status. This group
includes constructions that involve grammatical function words, such as articles, complementizers
and auxiliary verbs, but also structures involving prepositional phrases. For these constructions, there
is no general consensus in the tradition of dependency grammar as to whether they should be analyzed
as dependency relations at all and, if so, what should be regarded as the head and what should be
regarded as the dependent. For example, some theories regard auxiliary verbs as heads taking lexical
verbs as dependents; other theories make the opposite assumption; and yet other theories assume
that verb chains are connected by relations that are not dependencies in the usual sense.

Another kind of construction that is problematic for dependency grammar (as for most the-
oretical traditions) is coordination. According to the structuralist tradition, coordination is an en-
docentric construction, since it contains not only one but several heads that can replace the whole
construction syntactically. However, this raises the question of whether coordination can be analyzed
in terms of binary relations holding between a head and a dependent. Consider the following simple
examples:

They operate ships and banks.
She bought and ate an apple.

In the first example, it seems clear that the phrase ships and banks functions as a direct object of the
verb operate, but it is not immediately clear how this phrase can be given an internal analysis that is
compatible with the basic assumptions of dependency grammar, since the two nouns ships and banks
seem to be equally good candidates for being heads. Similarly, in the second example, the noun apple

(b) CCD

Figure 2: Two possible analyses of a coordination
[Kübler et al., 2009]

1.3 Outline
The rest of the paper is organised as follows. In
Section 2 we briefly review other works dealing
with similar transformations, before presenting the
approach taken in this work in Section 3. Section 4
describes the experimental setting and results. A
discussion follows in Section 5, and conclusions in
Section 6 .

2 Related work
In preparation for the CoNLL-X shared task on
dependency parsing [Buchholz and Marsi, 2006],
a number dependency trees were derived from
a number of constituency-based phrase structure
treebanks, most of which have grammatical func-
tion (e.g. “subject” and “object”) as part of the
annotation. The conversion process for such tree-
banks would involve a head table with rules of the
form

• “the head child of a VP/clause is the child
with the HD/predicator/hd/Head function”
and

• “[the dependency label] for a token is the func-
tion of the biggest constituent of which this
token is the lexical head”.

The case of the Thai CG bank is different, as it
does not directly contain any grammatical func-
tions. On the other hand, identifying head tokens

1244



is relatively straight-forward when augmenting the
CG annotation with dependency directions.

Chanev et al. [2006] faced a similar situation in
their transformation of the BulTreeBank to depen-
dency representation. Heads were first identified
from explicitly stated rules in a head table. Lack-
ing explicit grammatical functions in the source
treebank, they explored a heuristic rule-based ap-
proach for the labeling with a dependency table,
containing rules based on parent constituents. Al-
though good results are achieved, they report of
errors like mistaken subjects and objects.

3 Methodology
The situation with the Thai CG bank is a lit-
tle different. Together with a set of combinatory
grammar rules, the CG type tags and bracketing
present in the treebank unambiguously specify the
constituent structure of the treebank sentences.
When the CG type tags are augmented with depen-
dency directions, a dependency tree can be derived
with relative ease from the CG-based constituents.
Grammatical functions, however, are not immedi-
ately evident from the CG trees.

We first describe a relatively straight-forward
method for deriving the dependency trees, and
next consider the more daunting task of assigning
functional labels to the dependency arcs. Figure
3 shows a schematical overview of the proposed
method.

3.1 Terminology
For any given CG type t, we use the arity (admit-
tedly a bit sloppily) to denote the ordered list of
arguments expected by a type. The arity of the
type s\np/ws/np is thus /np, /ws and \np — that
is,

• a noun phrase from the right, followed by

• a subordinate clause beginning with the Thai
word วา (“that”, subordinate clause marker),
and

• another noun phrase from the left.

Complementary, we define the yield of t as the set
of possible CG types which may result from func-
tional application of a CG rule. The transitive verb
type s\np/np, for example, yields

• s\np and
• s

after receiving a np to the right, and another np
to the left, respectively. This is simply the basic
combinatory CCG rules:

X/Y Y ⇒ X
Y X\Y ⇒ X

Algorithm 1 Pseudo-code for propagating depen-
dency directions to non-terminal nodes.

def propagateDirect ions ( node ) :
for ch i l d in node . ch i ld r en :

propagateDirect ions ( ch i l d )
i f node . hasDependencyDirection :

return
for ch i l d in node . ch i ld r en :

i f ch i l d . y i e l d s ( node . type ) :
node . depDirs = ch i ld . depDirs
break

3.2 Dependency directions
The first transformation step, which can be re-
garded as a preprocessing step before the actual
transformation, involves a lexical dictionary for as-
signing dependency directions to the CG types as-
sociated with the grammatical entities in the CG
bank, falling back on a generic CG to CDG map-
ping in case of unknown words.

Note that only terminal nodes will have as-
signed dependency directions assigned by this pro-
cedure. Dependency directions are propagated to
non-terminal nodes in a bottom-up fashion by the
procedure propagateDirections (Algorithm 1).
In identifying which child to adopt dependency
directions from, the parent node type is checked
against the yield of each child node.

3.3 Head finding
Dependency arcs are assigned by a procedure that
implements the CDG dependency derivation rules
introduced by Boonkwan and Steedman [2011]
(motivated by Collins, 1999). The idea is to trace
the derivation implied by the CG rules, registering
the dependency relations specified by the CDG-
augmentation along the way.

The derivation rules handled are as follows. Let
c : d signify a CDG type c and a dependency struc-
ture d, and the notion h(d1) → h(d2) represent a
dependency arc between the head of d1 and the
head of d2 (with h(d1) governing h(d2)) . Then
the derivation rules are:

X/<Y:d1 Y:d2 ⇒ h(d1)← h(d2)
X/>Y:d1 Y:d2 ⇒ h(d1)→ h(d2)
Y:d1 X\<Y:d2 ⇒ h(d1)← h(d2)
Y:d1 X\>Y:d2 ⇒ h(d1)→ h(d2)

In a simple example, Mary[np]
drinks[s\ < np/ > np] fresh[np/ < np] milk[np],
fresh would combine with milk, yielding an np and
a dependency arc specifying milk as head of fresh.

In addition to the standard combinatory rules
for forward and backward functional application
above, the Thai CG bank makes use of a CCG-
style serialisation rule to handle e.g. serial verb

1245



CG trees CDG treesAugment

Lexical 
dictionary

CG to CDG 
map

Unlabeled
dep.
trees

Derive 
dependencies

Gold CDG &
labeled dependency 

tree pairs

Train 
classifier

Labeling 
model

Label by 
classifier

Approximately 
labeled  

dep. trees

Figure 3: Transformation overview

constructions, which are used in Thai (and Chi-
nese) to express serial or consecutive events. As
Boonkwan [2009], we take the notion of a serial
verb construction to mean a series of verbs or verb
phrases without explicit connectives marked with
(or understood to have) the same grammatical cat-
egories, and sharing at least one common argu-
ment, typically a subject.

As an example, the verbs ตรวจ (“examine”)
and พบ (“find”) occur serially in the following sen-
tence3 from the CG bank, indicating a resultative
course of events [Thepkanjana and Uehara, 2009]:

นักวิชาการ ตรวจ พบ ไวรัส โคโรนา
ใน ชะมด
(Lit: scientist/N examine/V find/V virus/N
corona/N in/PP civet/N)
“The scientist examined the civet and found
coronavirus.”

We introduce a generalised derivation rule for se-
rial constructions which simply designates the head
of the first dependency structure as governing the
head of the following dependency structure:

X:d1 X:d2 ⇒ h(d1)→ h(d2)

The rule is generalised in the sense that it han-
dles serial noun constructions as well as serial verb
constructions.

Further CCG-style combinatory rules, such as
functional composition and type raising, are not
currently in use in the Thai CG bank, and therefore
not handled by the transformation.

An outline of the head finding procedure is given
as Algorithm 2. Intuitively, the algorithm proceeds
by, for each node in turn, beginning at the termi-
nal nodes, identifying sibling nodes which satisfy
the arity of the of the node CG type. For each
sibling node satisfying an argument, the depen-
dency derivation rule is applied and the sibling is
removed.

3Artificial word boundaries inserted for clarity.

Algorithm 2 Pseudo-code for the head-finding
procedure.

def assignHeads ( node ) :
for c in node . nonterminalChildren :

assignHeads ( c )
for c in node . terminalChi ldren :

assignHeads ( c )
for arg in node . type . arguments :

i f arg . s ide == ’ r i gh t ’ :
s i b l = node . r i g h t S i b l i n g s . f i r s t

else
s i b l = node . l e f t S i b l i n g s . l a s t

break un le s s arg . matches ( s i b l )
i f arg . s ide == ’ r i gh t ’ :

i f arg . dependencyDir == ’> ’ :
reg i s terHead ( s i b l , node )

else : # <
reg i s terHead ( node , s i b l )

node . r i g h t S i b l i n g s . s h i f t
else : # l e f t

i f arg . dependencyDir == ’> ’ :
reg i s terHead ( node , s i b l )

else : # <
reg i s terHead ( s i b l , node )

node . l e f t S i b l i n g s . pop

It is worth noting that while both terminal and
non-terminal nodes are involved in this process, we
are only interested in assigning dependency arcs
to terminal nodes, as non-terminals are absent in
the all-terminal dependency structure. This is en-
sured by an implementation detail of the proce-
dure registerHead (omitted from Algorithm 2),
in which non-terminal nodes act as proxies for their
terminal heads.

3.4 Dependency labeling
Although the CDG-augmentation of the CG tree-
bank implies a dependency structure for each
sentence, there are no immediate clues avail-
able about the specific grammatical functions of
dependency arcs. Obviously, there are some
clear-cut cases: When a token with CDG type
((s\ < np)\ > (s\ < np))\ < num modifies a token
with CDG type num, it must be an application of a
quantifier (with dependency type “quan”), as ex-
emplified in Figure 4.

Other cases are less obvious. Even when taking

1246



Figure 4: An unmistakable labeling case: Given the CDG types of หลาย (“diverse”) and ชนิด (“species”),
the only dependency label supported by the examined data is “quan” (quantifier).

dependency direction and argument position into
consideration, there are still cases with several pos-
sible dependency types, as shown in Figure 5.

While for many practical purposes an unlabeled
dependency structure is sufficient, having proper
dependency labels is nonetheless desirable. In lack
of an exact transformation, the approach explored
in this work relies instead on training a classifier
to predict the correct dependency label given local
features of the tokens involved, as they occur in
the dependency structure derived from the CDG
tree.

From a related work (manuscript in prepara-
tion), we have obtained a number of CDG trees
from labeled dependency trees. Such pairs of la-
beled dependency trees and CDG trees can serve
as training material for the label classifier.

We evaluate different feature sets for classifica-
tion task. The basic feature set contains:

• CDG type of token and its head

• # of left siblings

• # of right siblings

• Dependency direction (L/R)

• Concatenations of token/head CDG and # of
left siblings

Other feature sets extend the basic set including
additional information:

+forms Token and head word form.

+POS(L) Part-of-speech tag for the token, as as-
signed by a learned part-of-speech tagger4.

+POS(G) “Gold” part-of-speech tag as found in
the labeled dependency tree.

4 Experiments
The Thai CG bank made available for this re-
search contains 1,428 phrases with corresponding
CG trees. Trees are mostly medium to low in token
count, with 49 single-token trees, and the longest
phrase having 9 tokens. Median phrase length is 3

4SVMTool [Giménez and Marquez, 2004], trained
on the Orchid corpus [Sornlertlamvanich et al., 1997].

(a) Subject (subj)

(b) Direct object (dobj)

(c) adverbialiser (advm)

Figure 5: Different labeling of left-pointing depen-
dency arcs with s\ < np/ > np as head CDG type
and np as dependent CDG type. See Figure 6 for
depency type abbreviations.

tokens. With a total token count of 5,143 tokens,
the arithmetic mean length is 3.60.

As there is no explicit sentence boundary marker
in Thai [Satayamas and Kawtrakul, 2004], it is of-
ten unclear what constitutes a sentence. Thus,
rather than ordinary sentences, the treebank con-
tains phrases of different types, reflecting the par-
titioning of token sequences made by the treebank
annotators for the purpose of treebanking:

• 539 verb phrases or subject-omitted phrases
(s\np),

• 363 sentences (s),

• 372 noun phrases (np) and

• 4 prepositional phrases (pp).

The lexical dictionary used contains possible CDG
types for 38.250 word forms, with an average of 2
types listed per word form. For six of the word
forms, the dictionary lists several possible depen-
dency directions for a single CG type. These con-
fusable CDG types and the dictionary entries they
occur for are listed in Table 1.

An example sentence from the treebank affected
by the ambiguous mapping of the adverb ตอนนี้

1247



Word form(s) CG type Possible CDG equivalents Interpretation

นา (s\np)/(s\np) (s\ < np)/ > (s\ < np) Adverb “please”
(s\ < np)/ < (s\ < np) Auxiallary verb “should”

ตอนนั้น (“at that time”) s/ > s Conjunction
ตอนนี้ (“at present”) s/s
ขณะนั้น (“at that moment”) s/ < s Adverb of time
ขณะนี้ (“this moment”)
กับ np\np/np np\ > np/ > np Preposition “with”np\ < np/ > np Conjunction “and”
Table 1: Entries in the lexical dictionary which are ambiguous with respect to dependency direction

(“at present”) is 5:

ตอนนี้ เธอ กำลัง ยุง มาก
(Lit: this-moment/ADV she/PRON
“-ing”/AUX busy/ADJ very/ADV)
“At this moment she is being very busy.”

Examples of the latter two ambiguity classes of
the lexical dictionary (Table 1) were not encoun-
tered in the treebank — i.e. the affected word
forms do not occur with an ambiguous CG type.
In dealing with these ambiguous entries in the lex-
ical dictionary, we simply (and naively) choose the
first mapping option.

The generic CG to CDG mapping, used in addi-
tion to the lexical dictionary as fallback for word
forms not found in the lexical dictionary, also ex-
hibits some degree of ambiguity. The seven CG
types with multiple possible CDG equivalents are
listed in Table 2.

In evaluating the classification-based approach
to assigning dependency labels, a sample of 678
labeled dependency edges from the NAiST depen-
dency treebank [Wacharamanotham et al., 2007]
was used, along with corresponding CDG trees.
The feature sets suggested in section 3.4 on page 4
were evaluated with four different classifiers6 (see
Table 3).

5 Discussion
Transforming CDG-augmented CG trees to unla-
beled dependency trees was successful. In this
work, the issue of ambiguous CDG types affects
only a very small number of trees, but remains an
issue to be aware of.

5The Thai auxillary verb กำลัง indicates the
present participle, meaning ”in the act of”, similar to
the English suffix, ”-ing”. Articifial word boundaries
are inserted for clarity.

6Experiments with the learners were done using
leave-one-out cross-validation, with the exception of
LibSVM, which was run using the standard K-fold
cross-validation of the easy.py script [Chang and Lin,
2001].

CG type Possible CDG equivalents
np\np/np np\ > np/ > np

np\ < np/ > np
s/s s/ > s

s/ < s
s\s s\ > s

s\ < s
(s\np)/(s\np) (s\ < np)/ > (s\ < np)

(s\ < np)/ < (s\ < np)
s/(s\np) s/ > (s\ < np)

s/ < (s\ < np)
s\(s\np) s\ < (s\ < np)

s\ > (s\ < np)
s/(s\np)/np s/ > (s\np)/ > np

s/ < (s\np)/ > np

Table 2: Cases of CD to CDG mappings which are
ambigous with respect to dependency direction

For dependency labeling, only a small amount of
training data (in the form of sentences with both
CDG and labeled dependency analyses) was avail-
able for this prelimenary experiment. Using this,
dependency labels were not reliably recoverable
(76.5% label accuracy). However, it seems hope-
ful that better recovery of dependency labels can
be obtained with this approach once more training
material become available.

Head and dependent word forms, as well as part-
of-speech tags, were beneficial as added features
for the label classifier, resulting in a substantial
reduction in error rate — from 0.357 to 0.235 (≈
34%).

On the other hand, rather than the approx-
imated classfier-based approach to labeling, one
could consider settling for an exact but partial la-
beling by only assigning those dependency labels
which unambigously arise from tuples of head and
dependent CDG types. However, the dependency
labels obtainable with absolute certainty in this
way are often of the less interesting kind — e.g.
“conj” for a s\ < np governed by a s/ > (s\ < np)
— while more useful labels remain ambiguous.

1248



Basic +forms +POS(L) +POS(G) +POS(L) +POS(G)Classifier & +forms & +forms
Random Forest 61.9% 69.6% 66.1% 68.9% 72.9% 71.8%
LibSVM 64.3% 73.9% 66.1% 70.2% 74.5% 76.5%
Nearest Neighbors 60.5% 64.8% 62.4% 64.5% 70.5% 70.4%
Naive Bayes 60.6% 63.7% 61.7% 65.8% 63.7% 66.2%

Table 3: Accuracy of different classifiers and feature sets in recovering the correct dependency labels

Complements subject (subj) • clausal subject
(csubj) • direct object (dobj) • indirect object
(iobj) • prepositional object (pobj) • preposi-
tional complement (pcomp) • subject or ob-
ject predicative (pred) • clausal predicative
(cpred) • conjunction (conj) • subordinating
conjunction (sconj) • nominaliser (nom) •
adverbialiser (advm)

Adjuncts parenthetical modifier (modp) • re-
strictive modifier (modr) • tense modifier
(modt) • mood modifier (modm) • aspect
modifier (moda) • locative modifier (modl)
• parenthetical apposition (appa) • restric-
tive apposition (appr) • relative clause modi-
fication (rel) • determiner (det) • quantifier
(quan) • classifier (cl) • coordination (co-
ord) • negation (neg) • punctuation (punc)
• double preposition (dprep) • parallel serial
verb (svp) • sequence serial verb (svs)

Figure 6: Dependency types from the annotation
guidelines [Sudprasert, 2008] for the NAiST depen-
dency treebank.

6 Conclusion and future work
In the process, a considerable amount of syntacti-
cal and annotational errors in the Thai CG bank
were identified and corrected. The authors are of
the belief that this work has not only provided
a means for continual expansion of resources for
Thai natural language processing, but also helped
improve the quality the existing CG resource.

As a related work (manuscript in preparation)
progresses, more CDG trees for labeled NAiST de-
pendency trees will become available. With this
extra training material, the learned classifier used
for labeling in this work should become more reli-
able. Further improvement might also be achiev-
able from an expanded feature set, including for ex-
ample the argument position (in addition to side)
and one or two generations of grandparent nodes.

Further development of the Thai CDG formal-
ism is also expected, in particular for the analysis
of sentence-like noun phrases. This will likely need
special handling in the dependency representation
as well. Currently, the NAiST annotation guide-

lines do not specify a label for this phenomenon.

Acknowledgements
The authors wish to thank the NAiST unit
of Kasetsart University and the HLT Lab of
NECTEC for making their treebanks available for
experiments.

References
K. Ajdukiewicz. Die syntaktische konnexität. Stu-

dia Philosophica, 1(1):27, 1935.

P. Boonkwan. A memory-based approach to the
treatment of serial verb construction in combi-
natory categorial grammar. In Proceedings of
the 12th Conference of the European Chapter of
the Association for Computational Linguistics:
Student Research Workshop, page 10–18, 2009.

P. Boonkwan and M. Steedman. Grammar in-
duction from text using small syntactic proto-
types. In Proceedings of the 5th International
Joint Conference on Natural Language Process-
ing (to appear), Chiang Mai, 2011.

S. Buchholz and E. Marsi. CoNLL-X shared task
on multilingual dependency parsing. In Tenth
Conference on Computational Natural Language
Learning, page 149, 2006.

A. Chanev, K. Simov, P. Osenova, and S. Mari-
nov. Dependency conversion and parsing of the
BulTreeBank. In Proc. of the LREC-Workshop
Merging and Layering Linguistic Information,
2006.

C. Chang and C. Lin. LIBSVM: a library
for support vector machines, 2001. URL
http://www.csie.ntu.edu.tw/cjlin/libsvm.

P. C Chang, H. Tseng, D. Jurafsky, and C. D
Manning. Discriminative reordering with chinese
grammatical relations features. In Proceedings of
the Third Workshop on Syntax and Structure in
Statistical Translation, page 51–59, 2009.

M. Collins. Head-driven statistical models for nat-
ural language parsing. PhD thesis, University of
Pennsylvania, 1999.

1249



M. Daum, K. Foth, and W. Menzel. Automatic
transformation of phrase treebanks to depen-
dency trees. In Proceedings of LREC, 2004.

J. Giménez and L. Marquez. SVMTool: a general
POS tagger generator based on support vector
machines. In In Proceedings of the 4th Inter-
national Conference on Language Resources and
Evaluation, 2004.

S. Kübler, R. McDonald, and J. Nivre. Depen-
dency parsing. Synthesis Lectures on Human
Language Technologies, 2(1):1–127, 2009.

Y. Ma, S. Ozdowska, Y. Sun, and A. Way. Improv-
ing word alignment using syntactic dependen-
cies. In Proceedings of the ACL-08: HLT Second
Workshop on Syntax and Structure in Statistical
Translation (SSST-2), page 69–77, 2008.

R. McDonald and J. Nivre. Characterizing the er-
rors of data-driven dependency parsing models.
In Proc. of the Joint Conf. on Empirical Meth-
ods in Natural Language Processing and Compu-
tational Natural Language Learning (EMNLP-
CoNLL), 2007.

J. Nivre. Dependency grammar and dependency
parsing. MSI report, 5133, 2005.

T. Ruangrajitpakorn, K. Trakultaweekoon, and
T. Supnithi. A syntactic resource for thai: CG
treebank. In Proceedings of the 7th Workshop on
Asian Language Resources, page 96–101, 2009.

V. Satayamas and A. Kawtrakul. Wide-Coverage
grammar extraction from thai treebank. In Pro-
ceedings of Papillon 2004 Workshops on Multi-
lingual Lexical Databases, 2004.

V. Sornlertlamvanich, T. Charoenporn, and H. Isa-
hara. ORCHID: thai part-of-speech tagged cor-
pus. Technical Report TR-NECTEC-1997-001,
1997.

M. Steedman. The syntactic process, volume 131.
MIT Press, 2000.

S. Sudprasert. Dependency annota-
tion guideline for thai, version 1.4 (in
thai). Technical report, 2008. URL
http://naist.cpe.ku.ac.th/tred/.

K. Thepkanjana and S. Uehara. Resulta-
tive constructions with “implied-result”
and “entailed-result” verbs in thai and en-
glish: a contrastive study. Linguistics,
47(3):589–618, 2009. ISSN 0024-3949. URL
http://www.thefreelibrary.com/-a0204693751.

C. Wacharamanotham, M. Suktarachan, and
A. Kawtrakul. The development of web-
based annotation system for thai tree-
bank. page 305, Thailand, 2007. URL
http://naist.cpe.ku.ac.th/tred/.

U. Warotamasikkhadit. Fronting and backing top-
icalization in thai. Mon-Khmer Studies, 27:
303–6, 1997.

1250


