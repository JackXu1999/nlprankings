



















































A dataset and baselines for sequential open-domain question answering


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1077–1083
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

1077

A dataset and baselines for sequential open-domain question answering

Ahmed Elgohary∗, Chen Zhao∗, Jordan Boyd-Graber
Department of Computer Science

University of Maryland, College Park
{elgohary, chenz, jbg}@cs.umd.edu

Abstract
Previous work on question-answering systems
mainly focuses on answering individual ques-
tions, assuming they are independent and de-
void of context. Instead, we investigate se-
quential question answering, asking multiple
related questions. We present QBLink, a new
dataset of fully human-authored questions.
We extend existing strong question answer-
ing frameworks to include previous questions
to improve the overall question-answering ac-
curacy in open-domain question answering.
The dataset is publicly available at http://
sequential.qanta.org.

1 Introduction

The framework of combining information retrieval
and neural reading comprehension has been the ba-
sis of several systems for answering open-domain
questions over unstructured text (Chen et al., 2017;
Wang et al., 2018; Clark and Gardner, 2018; Htut
et al., 2018). Typically, such systems take one in-
put question at a time, retrieving and ranking mul-
tiple paragraphs that potentially contain the answer.
A reading comprehension model then produces a
ranked list of candidate answer spans from each
paragraph. The final answer is then selected from
the produced spans.

In information-seeking dialogs, e.g., personal
assistants, users interact with a question answering
system by asking a sequence of related questions,
where questions share the same predicate, entities,
or at least a topic. Answering each question in isola-
tion is sub-optimal as information from previously
asked questions and previously obtained answers
can help better answer the current question.

We study the task of sequential open-domain
question answering. We ask how a standard open-
domain question answering system can incorpo-
rate connections between question-answer pairs

∗The first two authors contributed equally.

Lead-in: Only twenty-one million units in this system
will ever be created. For 10 points each:
Question 1: Name this digital payment system whose
transactions are recorded on a “block chain”.
Answer: Bitcoin
Question 2: Bitcoin was invented by this person, who,
according to a dubious Newsweek cover story, is a 64-
year-old Japanese-American man who lives in California.
Answer: Satoshi Nakamoto
Question 3: This online drugs marketplace, Chris Bor-
glum’s one-time favorite, used bitcoins to conduct all of
its transactions. It was started in 2011 by Ross Ulbricht
using the pseudonym Dread Pirate Roberts.
Answer: Silk Road

Figure 1: An example sequence of questions from
QBLink. The lead-in and question 1 are asking about
the same object/answer. The subject of question 2 is
the same as the object of question 1. All questions are
about a narrow topic, Bitcoin.

in the same sequence. We introduce QBLink, a
new dataset of about 18,000 question sequences
(Figure 1), each sequence consists of three natu-
rally occurring human-authored questions (totaling
around 56,000 unique questions). The sequences
themselves are also naturally occurring (i.e., we
do not artificially combine individually-authored
questions to form sequences), which allows us to
focus more on the important connections between
questions that should be incorporated to improve
the end-to-end question answering accuracy.

We compare sequence-aware models to base-
lines that process each question separately. For
our sequence-aware models, we tweak the retrieval
component by incorporating previous questions
and their answers together with the current ques-
tion to better rank the retrieved paragraphs. For the
reader, we use the semantic relations between enti-
ties in previous questions (or their corresponding
answers) and entities mentioned in the paragraph

http://sequential.qanta.org
http://sequential.qanta.org


1078

being read (candidate answers) to better choose
the answer entity. Both the retrieval and reading
steps can be slightly improved by incorporating
sequence information.

Our contributions are two-fold: first, we present
a new dataset for sequential question answering.
Our dataset is composed of complex questions
about a variety of topics. We make the dataset pub-
licly available to encourage future research. Sec-
ond, we use our dataset to compare baselines in
the open-domain question answering setup with
the goal of showing that incorporating sequential
connections between questions is advantageous.

2 Sequential Question Answering Task

We define the task of open-domain sequential
question answering: given a document collection
D and questions grouped into disjoint sequences
{Si | i = 1 . . . n} where each Si is an ordered se-
quence of question, answer pairs, and a subset of
documents Si = ((q

j
i , a

j
i , D

j
i ) | j = 1 . . .m), the

task is to answer questions qĵi with document evi-

dences Dĵi given access to previously asked ques-
tions in the same sequence and their corresponding
answers {(qji , a

j
i ) | j < ĵ}.

Following Chen et al. (2017), we split the task
into two steps—a retrieval step and a reading step.
In the retrieval step the current question qĵi and
previous questions and answers {(qji , a

j
i ) | j < ĵ}

are used to retrieve a ranked list of paragraphs Dĵi
from D that are likely to contain the correct answer
to the current question qĵi . The retrieved paragraphs

Dĵi are the input to the reading step that selects a

span from Dĵi as the answer to q
ĵ
i . The reading

step has access to previous questions and answers
{(qji , a

j
i ) | j < ĵ} as well.

3 Dataset Construction

This section describes QBLink’s construction.
QBLink is based on the bonus questions of Quiz
Bowl tournaments. Unlike previous work that only
uses the starter (or tossup) questions (Boyd-Graber
et al., 2012), bonus questions are not interruptable
(players always hear the complete question) and
have greater variability in difficulty. Bonus ques-
tions start with a lead-in text, which sets the stage
for the rest of the question, followed by a sequence
of related questions. Figure 1 shows an example of
a sequence of three questions.

Num. Questions (Num. Sequences) × 3
Training 45,747 (15,249)
Developme 3,630 (1,210)
Testing 6,555 (2,185)

Num. Sequences per Domain
Current Events 240
Fine Arts 2,588
Geography 472
History 3,961
Literature 3,879
Mythology 758
Philosophy 692
Religion 746
Science 4,028
Social Science 827
Trash 453

Num. Questions Tokens
Training 32.6 ± 9.6
Developme 33.5 ± 9.9
Testing 32.1 ± 10.24

Num. TagMe Entities
Training 2.46 ± 1.68
Developme 2.49 ± 1.68
Testing 2.48 ± 1.77
Num. Unique Answers 43,597
Num. Unique Answer Pages 18,529

Table 1: Statistics about QBLink. Most questions are
fairly long and contain 2.5 entity mentions in average
which demonstrates the complexity of the questions.

Specifically, we collect bonus questions from
http://quizdb.org for the tournaments in
2008–2018. Each question is categorized by topic
as history, literature, science, geography, fine arts,
philosophy, religion, mythology, social sciences,
current events or trash. We filter out too short
questions (fewer than ten tokens), and only keep
questions with exactly three sub-questions.

We map the answers to unambiguous Wikipedia
pages using combination of rule based match-
ing and fuzzy string matching, then filter out the
questions whose answers are not mapped to any
Wikipedia page (12.5% of the questions).

To keep our development and test set intact and
and of a reasonable percentage of questions, we
use the questions in 2014 tournament (the year
with the largest number of questions) for devel-
opment and testing, and the rest of the questions
are used for training. Table 1 shows the number
of question sequences and questions per split as
well as tokens and linked Wikipedia mentions per
question. We use TagMe (Ferragina and Scaiella,
2010) for mention detection and linking question
text to Wikipedia.

http://quizdb.org


1079

4 Baselines

We build our baselines on the DrQA framework
of Chen et al. (2017) for open-domain question an-
swering over Wikipedia.1 The framework operates
in a retrieval phase followed by a reading phase.

The retrieval phase uses a simple tf-idf (Salton
and Buckley, 1987) ranking of Wikipedia articles
given a question as the query.

The reading phase is a multi-layer recurrent
neural network model that extracts an answer
span from the top d retrieved paragraphs. The
reader model computes a contextualized represen-
tation of each token ti by running the token se-
quence through a multi-layer bidirectional long
short-term memory network (BiLSTM) (Hochre-
iter and Schmidhuber, 1997) and taking the corre-
sponding hidden state to each token at the top layer.
The question itself is encoded in a vector q as a
weighted average of the hidden states of a BiLSTM
over the word embeddings of its individual tokens.
The model then computes an unnormalized proba-
bility score of ti as the start and end token of the
answer span,

Start(i) = exp(ti
TWstartq);

End(i) = exp(ti
TWendq). (1)

To find the answer in multiple paragraphs at test
time, we merge all paragraphs before feeding them
to the reader (Clark and Gardner, 2018).

4.1 Answering Question in Isolation
We experiment with three models that ignore the
sequential connections between questions and an-
swer each question in isolation. Our first model is a
simple information retrieval (IR) baseline that only
uses the retrieval component: the title of the top-1
Wikipedia article is predicted as the answer.

Our second baseline is the full DrQA base-
line whose reader is trained/tuned on the train-
ing/development questions of our dataset. To as-
sign paragraphs to each of the training questions,
we follow a similar distant-supervision approach
to Chen et al. (2017). We retrieve the top twenty
Wikipedia articles for each question, exclude the
paragraphs that do not contain the gold answer,
and then rank the remaining paragraphs using tf-
idf. Each of the top ten paragraphs is paired
with the question to form a data instance for train-
ing the reader.

1We use the Wikipedia dump of 2017–09–20.

Finally, we tweak the DrQA reader to limit the
candidate answer spans to entity mentions that are
linked to Wikipedia. We set the pre-normalization
start and end scores of spans that are not detected
mentions to zero.

4.2 Incorporating Context in Retrieval
To incorporate the sequential connections between
questions in the retrieval phase, we append the
previously asked questions to the current question.
We also compare appending the predicted answers
(top-1 span) to each of the previous questions as
well as the gold answers to the current question.

4.3 Incorporating Context in Reader
In addition to encoding which entities have ap-
peared in previous questions, we also want to
provide our models with relationship information.
However, pre-defined relationships from knowl-
edge bases tend to be brittle. Instead, we use a
continuous representation of relationships (Iyyer
et al., 2016). For example, suppose we want to
encode the relationships for an entity (answer can-
didate) that starts at i and ends at j. We summarize
that entities relationships from each of possible k
relation-spans. A relation-span is a sequence of to-
kens from Wikipedia that contains both the answer
candidate and an answer to a previous question
(For example, the correct answer in Figure 2 has
a relation-span “He is best known for defending
President Ronald Reagan during the assassination
attempt by John Hinckley Jr.” with the previous
answer “Ronald Reagan”). This is summarized
in a vector rij by merging all k relation-spans in
a single span that is then fed through a BiLSTM
whose hidden states are combined as a weighted
sum where the weights are computed with self-
attention (Lin et al., 2017).

The stronger the similarity between the relation
that the question is asking about and the relation-
spans, the higher the score of the candidate answer
should be. We estimate that similarity r by con-
catenating the elementwise absolute difference and
hadamard product between rij and the question
embedding q. Then, we use a trainable weight
vector wrel to combine the components of the
concatenation output and produce a single simi-
larity score as

r = wrel
T [|q− rij|;q ◦ rij].

This can then influence the final selection of
the answer span by adding the relation similarity



1080

Method EM
Baselines: Questions in Isolation

IR 15.6
DrQA 39.3
DrQA + limiting to entities 39.7

DrQA + Retrieval with context
Previous questions 36.4
Previous predicted answers 39.8
Previous gold answers 40.1

DrQA + Reading with context
Append relation descriptions

w/ predicted answers 40.2

Append relation descriptions
w/ gold answers 40.7

Explicit relation embedding
w/ predicted answers 38.3

Explicit relation embedding
w/ gold answers 39.5

IR w/ Previous gold answers +
Reading w/ Append relation
descriptions w/ gold answers

40.7

Table 2: Incorporating sequence information in the re-
trieval and the reading step slightly improves overall
accuracy compared to answering questions in isolation.

score r to the start and end scores of the candidate
answer (Equation 1) as

Start(i) = exp(ti
TWstartq+ r)

End(j) = exp(tj
TWendq+ r). (2)

The relation embedding module is trained jointly
with the reader.

5 Baseline Results

We use QBLink to compare the baselines’ ques-
tion answering accuracy. Incorporating previous
questions and answers slightly improves the accu-
racy (Table 2).

We set the maximum number of retrieved doc-
uments to ten, and each document is divided into
paragraphs each of 400 tokens. At test time, we
merge the top ten ranked such paragraphs and feed
them to the reader. We use the reader network
of Chen et al. (2017). We limit the number of re-
lation description spans for each entity pair to five.
We used an LSTM of one hidden layer and 128 hid-
den units for the paragraph, question, and relation
description encoders. Each reader was trained for
twenty epochs.

Table 2 summarizes the results of the baselines
(Section 4). Question-answering accuracy is exact-
match accuracy since we limit the answer spans
to entity mentions whose boundaries are fixed
for all models.

Question: This man attempted to impress Jodie Foster
by shooting Ronald Reagan, but he failed to kill the Pres-
ident. At trial, he was found not guilty by reason of
insanity.
Gold answer to previous question: Ronald Reagan
Predict without relation span: George H. W. Bush
Correct answer: John Hinckley Jr.
Relation span: He is best known for defending Presi-
dent Ronald Reagan during the assassination attempt by
John Hinckley Jr.

Figure 2: Modeling the relation between President
Ronald Reagan and John Hinckley Jr. expressed by re-
lation span helps the reader select the correct answer
entity.

Incorporating the previous answer in the retrieval
and the reading components slightly improves the
overall question answering accuracy (Table 2). The
accuracy drops by more than 3% when using the en-
tire text of previous questions in the retrieval phase.
Modeling relations reduces the accuracy slightly
compared to augmenting paragraphs with relation
spans. One possible explanation is that our rela-
tion embedding model ends up being under-trained
since we could not retrieve any relation-spans for
many questions. Replacing Wikipedia with a larger
corpus (e.g., ClueWeb) might help improve the
training of the relation embedding model. Unsur-
prisingly, the gold answers to previous questions
are more useful than the predicted answers, which
highlights a need for models that take into account
the uncertainty about previous answers when gold
previous answers are not available. However, pro-
viding answers to previous questions is consistent
for most Quiz Bowl tournament play.

Figure 2 gives an example of how explicit rela-
tion embedding helps reader get a correct predic-
tion. Without the relation span, the model predicts
George H. W. Bush (vice president at that time) as
correct answer. Including the direct relation span
between Reagan and John Hinckley Jr., the model
gets the correct answer.

6 Related Work and Discussion

We adopt the open-domain question answering
framework (Wang et al., 2018; Chen et al., 2017).
Previous work considers improving that base
framework itself (Clark and Gardner, 2018;
Swayamdipta et al., 2018, inter alia). But retains
the assumption of answering individual questions.

Aside from the open-domain setup, much of the



1081

recent work on question answering has focused on
the sub-problem of reading-comprehension, where
the gold answer to each question is assumed to
exist in a given single paragraph for the model to
read (Hermann et al., 2015; Rajpurkar et al., 2016;
Seo et al., 2017). Another line of work on ques-
tion answering is question answering over struc-
tured knowledge-bases (Berant et al., 2013; Be-
rant and Liang, 2014; Yao and Van Durme, 2014;
Gardner and Krishnamurthy, 2017). Although
we focus on the more general open-domain setup,
QBLink can be adapted to be usable in the reading-
comprehension setup as well as the question an-
swering over knowledge-bases setup.

Several question answering datasets have been
proposed (Berant et al., 2013; Joshi et al., 2017;
Trischler et al., 2017; Rajpurkar et al., 2018, inter
alia). However, all of them were limited to answer-
ing individual questions.

Saha et al. (2018) study the problem of sequen-
tial question answering, and introduce a dataset
for the task. However, we differ from them in two
aspects: 1) They consider question-answering over
structured knowledge-bases. 2) Their dataset con-
struction was overly synthetic: templates were col-
lected by human annotators given knowledge-base
predicates. Further, sequences were constructed
synthetically as well by grouping individual ques-
tions by predicate or subjects.

Both Iyyer et al. (2017) and Talmor and Berant
(2018) answer complex questions by decomposing
each into a sequence of simple questions. Iyyer
et al. (2017) adopt a semantic parsing approach
to answer questions over semi-structured tables.
They construct a dataset of around 6,000 question
sequences by asking humans to rewrite a set of
2,000 complex questions into simple sequences.
Talmor and Berant (2018) consider the setup of
open-domain question answering over unstructured
text, but their dataset is constructed synthetically
(with human paraphrasing) by combining simple
questions with a few rules.

In parallel to our work, Choi et al. (2018) and
Reddy et al. (2018) introduce sequential question
answering datasets (QuAC and CoQA) that focus
on the reading comprehension setup (i.e., a sin-
gle text snippet is pre-specified for answering the
given questions). QBLink is entirely naturally oc-
curring (all questions and answers were authored
independently from any knowledge sources) and is
primarily designed to challenge human players.

The idea of our baseline to improving the reading
step by incorporating additional relation descrip-
tion spans is similar as Weissenborn et al. (2017)
and Mihaylov and Frank (2018), who integrate
background commonsense knowledge into reading-
comprehension systems. Both rely on structured
knowledge bases to extract information about se-
mantic relations that hold between entities. On
the other hand, we extract text spans that mention
each pair of entities and encoded them into vector
representations of the relations between entities.

7 Conclusions and Future Work

We introduce QBLink, a dataset of 56,000 natu-
rally occurring sequential question, answer pairs.
The questions are designed primarily to challenge
human players in Quiz Bowl tournaments. We
use QBLink to evaluate baselines for sequential
open-domain question answering. We show that
incorporating sequential information helps slightly
improve question answering accuracy.

In the future, we would like to invest in building
better sequential question answering models that
push the accuracy beyond the presented baselines.
Specifically, we will look at how to better model
the interaction between the reader and the relation
embedding model and how to improve the relation
embedding model itself by adopting ideas from the
relation extraction literature (Miwa and Bansal,
2016; Peng et al., 2017; Ammar et al., 2017).

Acknowledgments

We thank the anonymous reviewers and members
of the UMD CLIP lab for their comments and sug-
gestions. Jordan Boyd-Graber is supported by NSF
Grant IIS-1652666. Ahmed Elgohary is supported
by an IBM PhD fellowship. Any opinions, find-
ings, conclusions, or recommendations expressed
here are those of the authors and do not necessarily
reflect the view of the sponsors.

References
Waleed Ammar, Matthew Peters, Chandra Bhagavat-

ula, and Russell Power. 2017. The AI2 system at
SemEval-2017 task 10 (ScienceIE): semi-supervised
end-to-end entity and relation extraction. In The In-
ternational Workshop on Semantic Evaluation.

Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic parsing on freebase from
question-answer pairs. In Empirical Methods in Nat-
ural Language Processing.



1082

Jonathan Berant and Percy Liang. 2014. Semantic pars-
ing via paraphrasing. In Association for Computa-
tional Linguistics.

Jordan Boyd-Graber, Brianna Satinoff, He He, and Hal
Daumé III. 2012. Besting the quiz master: Crowd-
sourcing incremental classification games. In Em-
pirical Methods in Natural Language Processing.

Danqi Chen, Adam Fisch, Jason Weston, and Antoine
Bordes. 2017. Reading Wikipedia to answer open-
domain questions. In Association for Computa-
tional Linguistics.

Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen
tau Yih, Yejin Choi, Percy Liang, and Luke Zettle-
moyer. 2018. Quac: Question answering in context.
In Empirical Methods in Natural Language Process-
ing.

Christopher Clark and Matt Gardner. 2018. Simple
and effective multi-paragraph reading comprehen-
sion. In Association for Computational Linguistics.

Paolo Ferragina and Ugo Scaiella. 2010. Tagme:
on-the-fly annotation of short text fragments (by
wikipedia entities). In ACM International Confer-
ence on Information and Knowledge Management.

Matt Gardner and Jayant Krishnamurthy. 2017. Open-
vocabulary semantic parsing with both distributional
statistics and formal knowledge. In AAAI Confer-
ence on Artificial Intelligence.

Karl Moritz Hermann, Tomas Kocisky, Edward Grefen-
stette, Lasse Espeholt, Will Kay, Mustafa Suleyman,
and Phil Blunsom. 2015. Teaching machines to read
and comprehend. In Advances in Neural Informa-
tion Processing Systems.

Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long
short-term memory. Neural computation 9(8):1735–
1780.

Phu Mon Htut, Samuel R Bowman, and Kyunghyun
Cho. 2018. Training a ranking function for open-
domain question answering. In North American
Chapter of the Association for Computational Lin-
guistics: Student Research Workshop.

Mohit Iyyer, Anupam Guha, Snigdha Chaturvedi, Jor-
dan Boyd-Graber, and Hal Daumé III. 2016. Feud-
ing families and former friends: Unsupervised learn-
ing for dynamic fictional relationships. In North
American Association for Computational Linguis-
tics.

Mohit Iyyer, Wen tau Yih, and Ming-Wei Chang. 2017.
Search-based neural structured learning for sequen-
tial question answering. In Association for Compu-
tational Linguistics.

Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke
Zettlemoyer. 2017. TriviaQA: A large scale dis-
tantly supervised challenge dataset for reading com-
prehension. In Association for Computational Lin-
guistics.

Zhouhan Lin, Minwei Feng, Cicero Nogueira dos San-
tos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua
Bengio. 2017. A structured self-attentive sentence
embedding. In International Conference on Learn-
ing Representations.

Todor Mihaylov and Anette Frank. 2018. Knowledge-
able reader: Enhancing cloze-style reading compre-
hension with external commonsense knowledge. In
Association for Computational Linguistics.

Makoto Miwa and Mohit Bansal. 2016. End-to-end re-
lation extraction using lstms on sequences and tree
structures. In Association for Computational Lin-
guistics.

Nanyun Peng, Hoifung Poon, Chris Quirk, Kristina
Toutanova, and Wen-tau Yih. 2017. Cross-sentence
n-ary relation extraction with graph LSTMs. Trans-
actions of the Association for Computational Lin-
guistics 5.

P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang. 2016.
SQuAD: 100,000+ questions for machine compre-
hension of text. In Empirical Methods in Natural
Language Processing.

Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018.
Know what you don’t know: Unanswerable ques-
tions for SQuAD. In Association for Computational
Linguistics.

Siva Reddy, Danqi Chen, and Christopher D Manning.
2018. CoQA: A conversational question answering
challenge. In Empirical Methods in Natural Lan-
guage Processing.

Amrita Saha, Vardaan Pahuja, Mitesh M Khapra,
Karthik Sankaranarayanan, and Sarath Chandar.
2018. Complex sequential question answering: To-
wards learning to converse over linked question an-
swer pairs with a knowledge graph. In AAAI Confer-
ence on Artificial Intelligence.

Gerard Salton and Chris Buckley. 1987. Term weight-
ing approaches in automatic text retrieval. Technical
report, Cornell University.

Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and
Hannaneh Hajishirzi. 2017. Bidirectional attention
flow for machine comprehension. In International
Conference on Learning Representations.

Swabha Swayamdipta, Ankur P Parikh, and Tom
Kwiatkowski. 2018. Multi-mention learning for
reading comprehension with neural cascades. In
International Conference on Learning Representa-
tions.

Alon Talmor and Jonathan Berant. 2018. The web as
a knowledge-base for answering complex questions.
In North American Association for Computational
Linguistics.



1083

Adam Trischler, Tong Wang, Xingdi Yuan, Justin Har-
ris, Alessandro Sordoni, Philip Bachman, and Ka-
heer Suleman. 2017. NewsQA: A machine compre-
hension dataset. In The 2nd Workshop on Represen-
tation Learning for NLP.

Shuohang Wang, Mo Yu, Jing Jiang, Wei Zhang, Xiaox-
iao Guo, Shiyu Chang, Zhiguo Wang, Tim Klinger,
Gerald Tesauro, and Murray Campbell. 2018. Ev-
idence aggregation for answer re-ranking in open-
domain question answering. In International Con-
ference on Learning Representations.

Dirk Weissenborn, Tom Koisk, and Chris Dyer.
2017. Dynamic integration of background knowl-
edge in neural NLU systems. arXiv preprint
arXiv:1706.02596 .

Xuchen Yao and Benjamin Van Durme. 2014. Infor-
mation extraction over structured data: Question an-
swering with freebase. In Association for Computa-
tional Linguistics.


