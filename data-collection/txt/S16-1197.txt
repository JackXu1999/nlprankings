



















































UTA DLNLP at SemEval-2016 Task 12: Deep Learning Based Natural Language Processing System for Clinical Information Identification from Clinical Notes and Pathology Reports


Proceedings of SemEval-2016, pages 1268–1273,
San Diego, California, June 16-17, 2016. c©2016 Association for Computational Linguistics

UTA DLNLP at SemEval-2016 Task 12: Deep Learning Based Natural
Language Processing System for Clinical Information Identification from

Clinical Notes and Pathology Reports

Peng Li
Computer Science and Engineering

University of Texas at Arlington
jerryli1981@gmail.com

Heng Huang∗
Computer Science and Engineering

University of Texas at Arlington
heng@uta.edu

Abstract

We propose a deep neural network based nat-
ural language processing system for clinical
information (such as time information, event
spans, and their attributes) extraction from raw
clinical notes and pathology reports. Our ap-
proach uses the context words and their part-
of-speech tags and shape information as fea-
tures. We utilize the temporal (1D) convolu-
tion neural network to learn the hidden fea-
ture representations. In prediction step, we use
the Multilayer Perceptron (MLP) to predict
event spans. The empirical evaluation demon-
strates that our approach significantly outper-
forms baseline methods.

1 Introduction

In past several years, there has been much interest in
applying neural network based deep learning tech-
niques to solve many natural language processing
(NLP) tasks. From low-level tasks such as language
modeling, POS tagging, named entity recognition,
and semantic role labeling (Collobert et al., 2011;
Mikolov et al., 2013), to high-level tasks such as
machine translation, information retrieval, semantic
analysis (Kalchbrenner and Blunsom, 2013; Socher
et al., 2011a; Tai et al., 2015) and sentence relation
modeling tasks such as paraphrase identification and
question answering (Socher et al., 2011b; Iyyer et
al., 2014; Yin and Schutze, 2015). Deep represen-
tation learning has demonstrated its importance for

∗To whom all correspondence should be addressed. This
work was partially supported by NSF-IIS 1117965, NSF-IIS
1302675, NSF-IIS 1344152, NSF-DBI 1356628, NIH R01
AG049371.

these tasks. All the tasks get performance improve-
ment via learning either word level representations
or sentence level representations.

In this work, we introduce the deep representa-
tion learning technologies to the electronic medi-
cal record research. Specifically, we focus on clin-
ical information extraction, using clinical notes and
pathology reports from the Mayo Clinic. Our system
is designed to identify event expressions consisting
of the following components:

• The spans (character offsets) of the expression
in the raw text

• Contextual Modality: ACTUAL, HYPOTHET-
ICAL, HEDGED or GENERIC

• Degree: MOST, LITTLE or N/A

• Polarity: POS or NEG

• Type: ASPECTUAL, EVIDENTIAL or N/A
The input of our system consists of raw clinical

notes or pathology reports. The following is an ex-
ample:

April 23, 2014: The patient did not have
any postoperative bleeding so we will resume
chemotherapy with a larger bolus on Friday
even if there is slight nausea.

The output annotations over the text capture the
key information such as event mentions and at-
tributes. Table 1 illustrates the output of clinical in-
formation extraction in details.

To solve this task, the major challenge is how
to precisely identify the spans (character offsets) of

1268



Clinical Note Event Mention Event Attribute
April 23, 2014:
The patient did not
have any
postoperative
bleeding so we will
resume
chemotherapy with
a larger bolus on
Friday even if there
is slight nausea.

bleeding
type=N/A polarity=NEG
degree=N/A modality=ACTUAL

resume
type=ASPECTUAL polarity=POS
degree: N/A modality=ACTUAL

chemotherapy
type=ASPECTUAL polarity=POS
degree=N/A modality=ACTUAL

bolus
type=ASPECTUAL polarity=POS
degree=N/A modality=ACTUAL

nausea
type=ASPECTUAL polarity=POS
degree=N/A modality=HYPOTHETICAL

Table 1: An example of information extraction from clinical note.

the event expressions from raw clinical notes. Tradi-
tional machine learning approaches usually build a
supervised classifier with features generated by the
Apache clinical Text Analysis and Knowledge Ex-
traction System (cTAKES) 1. For example, BluLab
system (Velupillai et al., 2015) extracted morpholog-
ical (lemma), lexical (token), and syntactic (part-of-
speech) features encoded from cTAKES. Although
using the domain specific information extraction
tools can improve the performance, learning how to
use this software well for clinical domain feature en-
gineering is still very time-consuming. In short, a
simple and effective method that only leverages ba-
sic NLP modules and achieves high extraction per-
formance is desired for regular users.

To address this challenge, we propose a deep neu-
ral networks based method, especially convolution
neural network (Collobert et al., 2011), to learn hid-
den feature representations directly from raw clin-
ical notes. More specifically, one method first ex-
tracts a window of surrounding words for the can-
didate word. Then, we attach each word with their
part-of-speech tag and shape information as extra
features. After that, our system deploys a temporal
convolution neural network to learn hidden feature
representations. Finally, our system uses Multilayer
Perceptron (MLP) to predict event spans. Note that
we use the same model to predict event attributes.

1Apache cTAKES is a natural language processing system
for extraction of information from electronic medical record
clinical free-text

2 Constructing High Quality Training
Dataset

The major advantage of our system is that we only
leverage NLTK 2 tokenization and a POS tagger to
preprocess our training dataset. When implementing
our neural network based clinical information ex-
traction system, we found it is not easy to construct
high quality training data due to the noisy format of
clinical notes. Choosing the proper tokenizer is quite
important for span identification. After conducting
experiments, we found that “RegexpTokenizer” can
match our needs. This tokenizer can generate spans
for each token via sophisticated regular expression
such as:

n l t k . t o k e n i z e . RegexpToken ize r
( ”\w+ |\ $ [\ d \ . ] + | \ S+” )

We then use “PerceptronTagger” as our part-of-
speech tagger due to its fast tagging speed. Note that
when extracting context words, please make sure
you deploy the same tokenization module instead of
just splitting strings by space.

3 Neural Network Classifier

Event span identification is the task of extracting
character offsets of the expression in raw clinical
notes. This subtask is quite important due to the fact
that the event span identification accuracy will affect
the accuracy of attribute identification. We first run
our neural network classifier to identify event spans.
Then, given each span, our system tries to identify
attribute values.

2http://www.nltk.org

1269



3.1 Temporal Convolutional Neural Network
The way we use temporal convolution neural net-
work for event span and attribute classification is
similar with the approach proposed by (Collobert et
al., 2011). Generally speaking, we consider a word
as represented byK discrete featuresw ∈ D1×· · ·×
DK , where DK is the dictionary for the kth feature.
In our scenario, we just use three features such as
token mention, pos tag, and word shape. Note that
word shape features are used to represent the ab-
stract letter pattern of the word by mapping lower-
case letters to “x”, upper-case to “X”, numbers to
“d”, and retaining punctuation. We associate to each
feature a lookup table. Given a word, a feature vec-
tor is then obtained by concatenating all lookup table
outputs. Then a clinical snippet is transformed into
a word embedding matrix. The matrix can be fed
to further 1-dimension convolutional neural network
and max pooling layers. Below we will briefly in-
troduce core concepts of Convolutional Neural Net-
work (CNN).

Temporal Convolution
Temporal Convolution applies one-dimensional

convolution over the input sequence. The one-
dimensional convolution is an operation between a
vector of weights m ∈ Rm and a vector of inputs
viewed as a sequence x ∈ Rn. The vector m is the
filter of the convolution. Concretely, we think of x
as the input sentence and xi ∈ R as a single feature
value associated with the i-th word in the sentence.
The idea behind the one-dimensional convolution is
to take the dot product of the vector m with each m-
gram in the sentence x to obtain another sequence
c:

cj = m
Txj−m+1:j . (1)

Usually, xi is not a single value, but a d-
dimensional word vector so that x ∈ Rd×n. There
exist two types of 1d convolution operations. One
was introduced by (Waibel et al., 1989) and also
known as Time Delay Neural Networks (TDNNs).
The other one was introduced by (Collobert et al.,
2011). In TDNN, weights m ∈ Rd×m form a matrix.
Each row of m is convolved with the corresponding
row of x. In (Collobert et al., 2011) architecture, a
sequence of length n is represented as:

x1:n = x1 ⊕ x2 · · · ⊕ xn , (2)

where ⊕ is the concatenation operation. In gen-
eral, let xi:i+j refer to the concatenation of words
xi,xi+1, . . . ,xi+j . A convolution operation in-
volves a filter w ∈ Rhk, which is applied to a win-
dow of h words to produce the new feature. For ex-
ample, a feature ci is generated from a window of
words xi:i+h−1 by:

ci = f(w · xi:i+h−1 + b) , (3)

where b ∈ R is a bias term and f is a non-linear
function such as the hyperbolic tangent. This filter is
applied to each possible window of words in the se-
quence {x1:h,x2:h+1, . . . ,xn−h+1:n} to produce the
feature map:

c = [c1, c2, . . . , cn−h+1] , (4)

where c ∈ Rn−h+1.
We also employ dropout on the penultimate layer

with a constraint on `2-norms of the weight vector.
Dropout prevents co-adaptation of hidden units by
randomly dropping out a proportion p of the hid-
den units during forward-backpropagation. That is,
given the penultimate layer z = [ĉ1, . . . , ĉm], in-
stead of using:

y = w · z+ b (5)

for output unit y in forward propagation, dropout
uses:

y = w · (z ◦ r) + b , (6)
where ◦ is the element-wise multiplication operator
and r ∈ Rm is a masking vector of Bernoulli random
variables with probability p of being 1. Gradients are
backpropagated only through the unmasked units.
At test step, the learned weight vectors are scaled by
p such that ŵ = pw, and ŵ is used to score unseen
sentences. We additionally constrain l2-norms of the
weight vectors by re-scaling w to have ||w||2 = s
whenever ||w||2 > s after a gradient descent step.

4 Experimental Results

4.1 Dataset

We use the Clinical TempEval corpus 3 as the eval-
uation dataset. This corpus was based on a set of

3http://alt.qcri.org/semeval2016/task12/index.php?id=data

1270



Category Train Dev Test
Documents 293 147 151

Events 38872 20973 18989

Table 2: Number of documents, event expressions in the train-
ing, development and testing portions of the THYME data

600 clinical notes and pathology reports from can-
cer patients at the Mayo Clinic. These notes were
manually de-identified by the Mayo Clinic to re-
place names, locations, etc. with generic placehold-
ers, but time expression were not altered. The notes
were then manually annotated with times, events,
and temporal relations in clinical notes. These an-
notations include time expression types, event at-
tributes, and an increased focus on temporal rela-
tions. The event, time, and temporal relation anno-
tations were distributed separately from the text us-
ing the Anafora standoff format. Table 2 shows the
number of documents, event expressions in the train-
ing, development and testing portions of the 2016
THYME data.

4.2 Evaluation Metrics
All of the tasks were evaluated using the standard
metrics of precision (P), recall (R), and F1:

P =
|S ∩H|
|S|

R =
|S ∩H|
|H|

F1 =
2 · P ·R
P +R

(7)

where S is the set of items predicted by the sys-
tem and H is the set of items manually annotated
by the humans. Applying these metrics of the tasks
only requires a definition of what is considered an
“item” for each task. For evaluating the spans of
event expressions, items were tuples of character
offsets. Thus, system only received credit for iden-
tifying events with exactly the same character off-
sets as the manually annotated ones. For evaluating
the attributes of event expression types, items were
tuples of (begin, end, value) where begin and end
are character offsets and value is the value that was
given to the relevant attribute. Thus, systems only re-
ceived credit for an event attribute if they both found

an event with correct character offsets and then as-
signed the correct value for that attribute (Bethard et
al., 2015).

4.3 Hyperparameters and Training Details

Objective Function

We want to maximize the likelihood of the correct
class. This is equivalent to minimizing the negative
log-likelihood (NLL). More specifically, the label ŷ
given the inputs xh is predicted by a softmax classi-
fier that takes the hidden state hj as input:

p̂θ(y|xh) = softmax(W · xh + b)
ŷ = argmax

y
p̂θ(y|xh) (8)

After that, the objective function is the negative
log-likelihood of the true class labels yk:

J(θ) = − 1
m

m∑

k=1

log p̂θ(y
k|xkh) +

λ

2
||θ||22 , (9)

where m is the number of training examples and the
superscript k indicates the k-th example.

Hyperparameters

We use Lasagne 4 deep learning framework. We
first initialize our word representations using pub-
licly available 300-dimensional Glove word vec-
tors 5. We deploy CNN model with kernel width
of 2, a filter size of 300, sequence length is 2 ∗
windows size+1, number filters is seqlen−kw+1,
stride is 1, pool size is seqlen − filter size + 1,
cnn activation function is tangent, MLP activation
function is sigmoid. MLP hidden dimension is 50.
We initialize CNN weights using a uniform dis-
tribution. Finally, by stacking a softmax function
on top, we can get normalized log-probabilities.
Training is done through stochastic gradient descent
over shuffled mini-batches with the AdaGrad update
rule (Duchi et al., 2011). The learning rate is set to
0.05. The mini-batch size is 100. The model param-
eters were regularized with a per-minibatch L2 reg-
ularization strength of 10−4.

4https://github.com/Lasagne/Lasagne
5http://nlp.stanford.edu/projects/glove/

1271



4.4 Results and Discussions
Table 3 shows results on the event expression tasks.
Our initial submits RUN 4 and 5 outperformed the
memorization baseline on every metric on every
task. The precision of event span identification is
close to the max report. However, our system got
lower recall. The reason of lower recall values is that
the word2vec may not cover more domain specific
words. Table 4 shows results on the phase 2 subtask.

DocTimeRel
Methods P R F1

Memorize 0.675
Ours RUN5 0.788 0.788 0.788
Ours RUN6 0.786 0.786 0.786

Median report 0.724
Max report 0.843

Table 4: Phase 2: DocTimeRel

5 Conclusions

In this paper, we introduced a new clinical infor-
mation extraction system that only leverages deep
neural networks to identify event spans and their
attributes from raw clinical notes. We trained deep
neural networks based classifiers to extract clinical
event spans. Our method attached each word to their
part-of-speech tag and shape information as extra
features. We then hire temporal convolution neural
network to learn hidden feature representations. The
entire experimental results demonstrate that our ap-
proach consistently outperforms the existing base-
line methods on standard evaluation datasets.

Our research proved that we can get competi-
tive results without the help of a domain specific
feature extraction toolkit, such as cTAKES. Also
we only leverage basic natural language process-
ing modules such as tokenization and part-of-speech
tagging. With the help of deep representation learn-
ing, we can dramatically reduce the cost of clinical
information extraction system development. Due to
the recall values are still low, we will consider use
domain specific tools to enhance feature engineer-
ing.

References
Steven Bethard, Leon Derczynski, Guergana Savova,

Guergana Savova, James Pustejovsky, and Marc Ver-

hagen. 2015. Semeval-2015 task 6: Clinical tempeval.
Proc. SemEval.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.
Natural language processing (almost) from scratch.
The Journal of Machine Learning Research, 12:2493–
2537.

John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning and
stochastic optimization. The Journal of Machine
Learning Research, 12:2121–2159.

Mohit Iyyer, Jordan Boyd-Graber, Leonardo Claudino,
Richard Socher, and Hal Daumé III. 2014. A neu-
ral network for factoid question answering over para-
graphs. In Empirical Methods in Natural Language
Processing.

Nal Kalchbrenner and Phil Blunsom. 2013. Recur-
rent continuous translation models. In Proceedings of
the 2013 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1700–1709, Seat-
tle, Washington, USA. Association for Computational
Linguistics.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representations
of words and phrases and their compositionality. In
Advances in Neural Information Processing Systems,
pages 3111–3119.

Richard Socher, Eric H Huang, Jeffrey Pennin, Christo-
pher D Manning, and Andrew Y Ng. 2011a. Dynamic
pooling and unfolding recursive autoencoders for para-
phrase detection. In Advances in Neural Information
Processing Systems, pages 801–809.

Richard Socher, Jeffrey Pennington, Eric H Huang, An-
drew Y Ng, and Christopher D Manning. 2011b.
Semi-supervised recursive autoencoders for predicting
sentiment distributions. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 151–161. Association for Compu-
tational Linguistics.

Kai Sheng Tai, Richard Socher, and Christopher D Man-
ning. 2015. Improved semantic representations
from tree-structured long short-term memory net-
works. arXiv preprint arXiv:1503.00075.

Sumithra Velupillai, Danielle L Mowery, Samir Abdel-
rahman, Lee Christensen, and Wendy W Chapman.
2015. Blulab: Temporal information extraction for
the 2015 clinical tempeval challenge. Association for
Computational Linguistics.

Alexander Waibel, Toshiyuki Hanazawa, Geoffrey Hin-
ton, Kiyohiro Shikano, and Kevin J Lang. 1989.
Phoneme recognition using time-delay neural net-
works. Acoustics, Speech and Signal Processing,
IEEE Transactions on, 37(3):328–339.

1272



span modality degree polarity type
Methods P R F1 P R F1 P R F1 P R F1 P R F1

Memorize 0.878 0.834 0.855 0.810 0.770 0.789 0.874 0.831 0.852 0.812 0.772 0.792 0.855 0.813 0.833
Ours RUN4 0.908 0.842 0.874 0.842 0.780 0.810 0.904 0.838 0.869 0.876 0.812 0.842 0.877 0.813 0.844
Ours RUN5 0.900 0.850 0.874 0.837 0.790 0.813 0.896 0.845 0.870 0.861 0.813 0.836 0.869 0.820 0.844

Median report 0.887 0.846 0.874 0.830 0.780 0.810 0.882 0.838 0.869 0.868 0.813 0.839 0.854 0.813 0.844
Max report 0.915 0.891 0.903 0.866 0.843 0.855 0.911 0.887 0.899 0.900 0.875 0.887 0.894 0.870 0.882

Table 3: System performance comparisons. Note that Run4 means the window size is 4, and Run5 means the window size is 5

Wenpeng Yin and Hinrich Schutze. 2015. Multigrancnn:
An architecture for general matching of text chunks
on multiple levels of granularity. In Proceedings of th
53rd Annual Meeting of the Association for Computa-
tional Linguistics, pages 63–73.

1273


