



















































Dependency Grammar Induction with Neural Lexicalization and Big Training Data


Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1683–1688
Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics

Dependency Grammar Induction with Neural Lexicalization and Big
Training Data∗

Wenjuan Han, Yong Jiang and Kewei Tu
{hanwj, jiangyong ,tukw}@shanghaitech.edu.cn

School of Information Science and Technology
ShanghaiTech University, Shanghai, China

Abstract

We study the impact of big models (in
terms of the degree of lexicalization) and
big data (in terms of the training cor-
pus size) on dependency grammar induc-
tion. We experimented with L-DMV, a
lexicalized version of Dependency Model
with Valence (Klein and Manning, 2004)
and L-NDMV, our lexicalized extension of
the Neural Dependency Model with Va-
lence (Jiang et al., 2016). We find that
L-DMV only benefits from very small de-
grees of lexicalization and moderate sizes
of training corpora. L-NDMV can bene-
fit from big training data and lexicaliza-
tion of greater degrees, especially when
enhanced with good model initialization,
and it achieves a result that is competitive
with the current state-of-the-art.

1 Introduction

Grammar induction is the task of learning a gram-
mar from a set of unannotated sentences. In the
most common setting, the grammar is unlexical-
ized with POS tags being the tokens, and the train-
ing data is the WSJ10 corpus (the Wall Street
Journal corpus with sentences no longer than 10
words) containing no more than 6,000 training
sentences (Cohen et al., 2008; Berg-Kirkpatrick
et al., 2010; Tu and Honavar, 2012).

Lexicalized grammar induction aims to incor-
porate lexical information into the learned gram-
mar to increase its representational power and im-
prove the learning accuracy. The most straight-
forward approach to encoding lexical informa-
tion is full lexicalization (Pate and Johnson, 2016;
Spitkovsky et al., 2013). A major problem with

∗This work was supported by the National Natural Sci-
ence Foundation of China (61503248).

full lexicalization is that the grammar becomes
much larger and thus learning is more data de-
manding. To mitigate this problem, Headden et al.
(2009) and Blunsom and Cohn (2010) used par-
tial lexicalization in which infrequent words are
replaced by special symbols or their POS tags.
Another straightforward way to mitigate the data
scarcity problem of lexicalization is to use training
corpora larger than the standard WSJ corpus. For
example, Pate and Johnson (2016) used two large
corpora containing more than 700k sentences;
Marecek and Straka (2013) utilized a very large
corpus based on Wikipedia in learning an unlexi-
calized dependency grammar. Finally, smoothing
techniques can be used to reduce the negative im-
pact of data scarcity. One example is Neural DMV
(NDMV) (Jiang et al., 2016) which incorporates
neural networks into DMV and can automatically
smooth correlated grammar rule probabilities.

Inspired by this background, we conduct a sys-
tematic study regarding the impact of the degree
of lexicalization and the training data size on the
accuracy of grammar induction approaches. We
experimented with a lexicalized version of Depen-
dency Model with Valence (L-DMV) (Klein and
Manning, 2004) and our lexicalized extension of
NDMV (L-NDMV). We find that L-DMV only
benefits from very small degrees of lexicalization
and moderate sizes of training corpora. In com-
parison, L-NDMV can benefit from big training
data and lexicalization of greater degrees, espe-
cially when it is enhanced with good model ini-
tialization. The performance of L-NDMV is com-
petitive with the current state-of-the-art.

2 Methods

2.1 Lexicalized DMV

We choose to lexicalize an extended version of
DMV (Gillenwater et al., 2010). We adopt a sim-

1683



Hidden Layer:

Inputs:

Softmax Layer:

Head POS TagValency Head Word

…
 CHILD Outputs DECISION Outputs

Wdec

Wdir

…
Wchdvdec = Wdecf

vchd = Wchdf
Fully Connected Layer:

f = ReLU(Wdir[vval; vword; vtag])

[vval; vword; vtag]

Softmax(vchd)
Softmax(vdec)

Figure 1: The structure of the neural networks in
the L-NDMV model. It predicts the probabilities
of the CHILD rules and DECISION rules.

ilar approach to that of Spitkovsky et al. (2013)
and Blunsom and Cohn (2010) and represent each
token as a word/POS pair. If a pair appears in-
frequently in the corpus, we simply ignore the
word and represent it only with the POS tag. We
control the degree of lexicalization by replacing
words that appear less than a cutoff number in
the WSJ10 corpus with their POS tags. With a
very large cutoff number, the grammar is virtu-
ally unlexicalized; but when the cutoff number be-
comes smaller, the grammar becomes closer to be
fully lexicalized. Note that our method is different
from previous practice that simply replaces rare
words with a special “unknown” symbol (Head-
den III et al., 2009). Using POS tags instead of the
“unknown” symbol to represent rare words can be
helpful in the neural approach introduced below
in that the learned word vectors are more informa-
tive.

2.2 Lexicalized NDMV
With a larger degree of lexicalization, the gram-
mar contains more tokens and hence more param-
eters (i.e., grammar rule probabilities), which re-
quire more data for accurate learning. Smoothing
is a useful technique to reduce the demand for data
in this case. Here we employ a neural approach to
smoothing. Specifically, we propose a lexicalized
extension of neural DMV (Jiang et al., 2016) and
we call the resulting approach L-NDMV.

Extended Model: The model structure of L-
NDMV is similar to that of NDMV except for
the representations of the head and the child of
the CHILD and DECISION rules. The net-
work structure for predicting the probabilities of
CHILD rules [pc1 , pc2 , ..., pcm ] (m is the vocab-
ulary size; ci is the i-th token) and DECISION

rules [pstop, pcontinue] given the head word, head
POS tag, direction and valence is shown in Fig-
ure 1. We denote the input continuous represen-
tations of the head word, head POS tag and va-
lence by vword, vtag and vval respectively. By
concatenating these vectors we get the input repre-
sentation to the neural network: [vval; vword; vtag].
We map the input representation to the hidden
layer f using the direction-specific weight matrix
Wdir and the ReLU activation function. We rep-
resent all the child tokens with matrix Wchd =
[Wword, Wtag] which contains two parts: child
word matrix Wword ∈ Rm×k and child POS tag
matrix Wtag ∈ Rm×k′ , where k and k′ are the pre-
specified dimensions of output word vectors and
tag vectors respectively. The i-th rows of Wword
and Wtag represent the output continuous repre-
sentations of the i-th word and its POS tag respec-
tively. Note that for two words with the same POS
tag, the corresponding POS tag representations are
the same. We take the product of f and the child
matrix Wchd and apply a softmax function to ob-
tain the CHILD rule probabilities. For DECISION
rules, we replace Wchd with the decision weight
matrix Wdec and follow the same procedure.

Extended Learning Algorithm: The original
NDMV learning method is based on hard-EM and
is very time-consuming when applied to L-NDMV
with a large training corpus. We propose two im-
provements to achieve significant speedup. First,
at each EM iteration we collect grammar rule
counts from a different batch of sentences instead
of from the whole training corpus and train the
neural network using only these counts. Second,
we train the same neural network across EM it-
erations without resetting. More details can be
found in the supplementary material. Our algo-
rithm can be seen as an extension of online EM
(Liang and Klein, 2009) to accommodate neural
network training.

2.3 Model Initialization

It was previously shown that the heuristic KM ini-
tialization method by Klein and Manning (2004)
does not work well for lexicalized grammar in-
duction (Headden III et al., 2009; Pate and John-
son, 2016) and it is very helpful to initialize learn-
ing with a model learned by a different grammar
induction method (Le and Zuidema, 2015; Jiang
et al., 2016). We tested both KM initialization and
the following initialization method: we first learn

1684



an unlexicalized DMV using the grammar induc-
tion method of Naseem et al. (2010) and use it
to parse the training corpus; then, from the parse
trees we run maximum likelihood estimation to
produce the initial lexicalized model.

3 Experimental Setup

For English, we used the BLLIP corpus1 in ad-
dition to the regular WSJ corpus in our experi-
ments. Note that the BLLIP corpus is collected
from the same news article source as the WSJ
corpus, so it is in-domain and is ideal for train-
ing grammars to be evaluated on the WSJ test set.
In order to solve the compatibility issue as well
as improve the POS tagging accuracy, we used
the Stanford tagger (Toutanova et al., 2003) to re-
tag the BLLIP corpus and selected the sentences
for which the new tags are consistent with the
original tags, which resulted in 182244 sentences
with length less than or equal to 10 after remov-
ing punctuations. We used this subset of BLLIP
and section 2-21 of WSJ10 for training, section
22 of WSJ for validation and section 23 of WSJ
for testing. We used training sets of four differ-
ent sizes: WSJ10 only (5779 sentences) and 20k,
50k, and all sentences from the BLLIP subset. For
Chinese, we obtained 4762 sentences for training
from Chinese Treebank 6.0 (CTB) after convert-
ing data to dependency structures via Penn2Malt
(Nivre, 2006) and then stripping off punctuations.
We used the recommended validation and test data
split described in the documentation.

We trained the models with different degrees of
lexicalization. We control the degree of lexicaliza-
tion by replacing words that appear less than a cut-
off number in the WSJ10 or CTB corpus with their
POS tags. For each degree of lexicalization, we
tuned the dimension of the hidden layer of the neu-
ral network on the validation dataset. For English,
we tested nine word cutoff numbers: 100000, 500,
200, 100, 80, 70, 60, 50, and 40, which resulted in
vocabulary sizes of 35, 63, 98, 166, 203, 226, 267,
306, and 390 respectively; for Chinese, the word
cutoff numbers are 100000, 100, 70, 50, 40, 30,
20, 12, and 10. Ideally, with higher degrees of lex-
icalization, the hidden layer dimension should be
larger in order to accommodate the increased num-
ber of tokens. For the neural network of L-NDMV,
we initialized the word and tag vectors in the neu-

1Brown Laboratory for Linguistic Information Processing
(BLLIP) 1987-89 WSJ Corpus Release 1

ral network by learning a CBOW model using the
Gensim package (Řehůřek and Sojka, 2010). We
set the dimension of input and output word vectors
to 100 and the dimension of input and output tag
vectors to 20. We trained the neural network with
learning rate 0.03, mini-batch size 200 and mo-
mentum 0.9. Because some of the neural network
weights are randomly initialized, the model con-
verges to a different local minimum in each run of
the learning algorithm. Therefore, for each setup
we ran our learning algorithm for three times and
reported the average accuracy. More detail of the
experimental setup can be found in the supplemen-
tary material.

4 Experimental Results

4.1 Results on English

Figure 2(a) shows the directed dependency accu-
racy (DDA) of the learned lexicalized DMV with
KM initialization. It can be seen that on the
smallest WSJ10 training corpus, lexicalization im-
proves learning only when the degree of lexical-
ization is small; with further lexicalization, the
learning accuracy significantly degrades. On the
three larger training corpora, the impact of lexi-
calization on the learning accuracy is still negative
but is less severe. Overall, lexicalization seems
to be very data demanding and even our largest
training corpora could not bring about the bene-
fit of lexicalization. Increasing the training cor-
pus size is helpful regardless of the degree of lex-
icalization, but the learning accuracies with the
50K dataset are almost identical to those with the
full dataset, suggesting diminishing return of more
data.

Figure 2(b) shows the results of L-NDMV with
KM initialization. The parsing accuracy is im-
proved under all the settings, showing the advan-
tage of NDMV. The range of lexicalization de-
grees that improve learning becomes larger, and
the degradation in accuracy with large degrees of
lexicalization becomes much less severe. Dimin-
ishing return of big data as seen in the first figure
can still be observed.

Figure 2(c) shows the results of L-NDMV with
the initialization method described in section 2.3.
It can be seen that lexicalization becomes less data
demanding and the learning accuracy does not de-
crease until the highest degrees of lexicalization.
Larger training corpora now lead to significantly
better learning accuracy and support lexicalization

1685



0.42

0.46

0.5

0.54

0.58

0.62

0.66

0 50 100 150 200 250 300 350 400 450

D
D

A

Vocabulary Size

WSJ10
20K Sentences
50K Sentences
All Sentences

0.54

0.56

0.58

0.6

0.62

0.64

0.66

0.68

0.7

0.72

0 50 100 150 200 250 300 350 400 450

D
D

A

Vocabulary Size

WSJ10
20K Sentences
50K Sentences
All Sentences

(a) L-DMV with KM initialization on English (b) L-NDMV with KM initialization on English

0.72

0.73

0.74

0.75

0.76

0.77

0.78

0.79

0 50 100 150 200 250 300 350 400 450

D
D

A

Vocabulary Size

WSJ10
20K Sentences
50K Sentences
All Sentences

0.2

0.25

0.3

0.35

0.4

0.45

0.5

0.55

0.6

0.65

0 50 100 150 200 250 300 350 400 450 500 550

D
D

A

Vocabulary Size

L-NDMV with Good Init
L-NDMV with KM Init
L-DMV with KM Init

(c) L-NDMV with good initialization on English (d) L-DMV and L-NDMV on Chinese

Figure 2: The impact of the training corpus size and the degree of lexicalization on L-DMV and L-
NDMV with different initialization methods on English and Chinese.

of greater degrees than smaller corpora. Dimin-
ishing return of big data is no longer observed,
which implies further increase in accuracy with
even more data.

Table 1 compares the result of L-NDMV (with
the largest corpus and the vocabulary size of 203
which was selected on the validation set) with pre-
vious approaches to dependency grammar induc-
tion. It can be seen that L-NDMV is competitive
with previous state-of-the-art approaches. We did
some further analysis of the learned word vectors
in L-NDMV in the supplementary material.

4.2 Results on Chinese

Figure 2(d) shows the results of the three ap-
proaches on the Chinese treebank. Because the
corpus is relatively small, we did not study the im-
pact of the corpus size. Similar to the case of En-
glish, the accuracy of lexicalized DMV degrades
with more lexicalization. However, the accuracy
with L-NDMV increases significantly with more
lexicalization even without good model initializa-
tion. Adding good initialization further boosts the
performance of L-NDMV, but the benefit of lexi-
calization is less significant (from 0.55 to 0.58).

Methods WSJ10 WSJ
Unlexicalized Approaches, with WSJ10

EVG (Headden III et al., 2009) 65.0 -
TSG-DMV (Blunsom and Cohn, 2010) 65.9 53.1
PR-S (Gillenwater et al., 2010) 64.3 53.3
HDP-DEP (Naseem et al., 2010) 73.8 -
UR-A E-DMV (Tu and Honavar, 2012) 71.4 57.0
Neural E-DMV(Jiang et al., 2016) 72.5 57.6

Systems Using Lexical Information and/or More Data
LexTSG-DMV (Blunsom and Cohn, 2010) 67.7 55.7
L-EVG (Headden III et al., 2009) 68.8 -
CS (Spitkovsky et al., 2013) 72.0 64.4
MaxEnc (Le and Zuidema, 2015) 73.2 65.8
L-NDMV + WSJ 75.1 59.5
L-NDMV + Large Corpus 77.2 63.2

Table 1: Comparison of recent grammar induction
systems.

5 Effect of Grammar Rule Probability
Initialization

We compare four initialization methods to L-
NDMV: uniform initialization, random initial-
ization, KM initialization (Klein and Manning,
2004), and good initialization as described in sec-
tion 2.3 in Figure 3. Here we trained the L-NDMV
model on the WSJ10 corpus with the same exper-
imental setup as in section 3.

1686



Init Type

Good
Initialization

KM
Initialization

Random
Initialization

Uniform
Initialization

.00

.10

.20

.30

.40

.50

.60

.70

.80
D
D
A

Figure 3: Comparison of four initialization meth-
ods to L-NDMV: uniform initialization, random
initialization, KM initialization and good initial-
ization.

Again, we find that good initialization leads
to better performance than KM initialization, and
both good initialization and KM initialization are
significantly better than random and uniform ini-
tialization. Note that our results are different from
those by Pate and Johnson (2016), who found that
uniform initialization leads to similar performance
to KM initialization. We speculate that it is be-
cause of the difference in the learning approaches
(we use neural networks which may be more sen-
sitive to initialization) and the training and test
corpora (we use news articles while they use tele-
phone scripts).

6 Conclusion and Future Work

We study the impact of the degree of lexicaliza-
tion and the training data size on the accuracy of
dependency grammar induction. We experimented
with lexicalized DMV (L-DMV) and our lexical-
ized extension of Neural DMV (L-NDMV). We
find that L-DMV only benefits from very small
degrees of lexicalization and moderate sizes of
training corpora. In contrast, L-NDMV can ben-
efit from big training data and lexicalization of
greater degrees, especially when enhanced with
good model initialization, and it achieves a result
that is competitive with the state-of-the-art.

In the future, we plan to study higher degrees of
lexicalization or full lexicalization, as well as even
larger training corpora (such as the Wikipedia cor-
pus). We would also like to experiment with other
grammar induction approaches with lexicalization
and big training data.

References
Taylor Berg-Kirkpatrick, Alexandre Bouchard-Côté,

John DeNero, and Dan Klein. 2010. Painless un-
supervised learning with features. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 582–590. As-
sociation for Computational Linguistics.

Phil Blunsom and Trevor Cohn. 2010. Unsupervised
induction of tree substitution grammars for depen-
dency parsing. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1204–1213. Association for Com-
putational Linguistics.

Shay B Cohen, Kevin Gimpel, and Noah A Smith.
2008. Logistic normal priors for unsupervised prob-
abilistic grammar induction. In Advances in Neural
Information Processing Systems, pages 321–328.

Jennifer Gillenwater, Kuzman Ganchev, Joao Graça,
Fernando Pereira, and Ben Taskar. 2010. Sparsity in
dependency grammar induction. In Proceedings of
the ACL 2010 Conference Short Papers, pages 194–
199. Association for Computational Linguistics.

William P Headden III, Mark Johnson, and David
McClosky. 2009. Improving unsupervised depen-
dency parsing with richer contexts and smoothing.
In Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 101–109. Association for Computa-
tional Linguistics.

Yong Jiang, Wenjuan Han, and Kewei Tu. 2016. Un-
supervised neural dependency parsing. In Proceed-
ings of the 2016 Conference on Empirical Methods
in Natural Language Processing, pages 763–771,
Austin, Texas. Association for Computational Lin-
guistics.

Dan Klein and Christopher D. Manning. 2004. Corpus-
based induction of syntactic structure: Models of de-
pendency and constituency. In Proceedings of the
42Nd Annual Meeting on Association for Computa-
tional Linguistics, ACL ’04, Stroudsburg, PA, USA.
Association for Computational Linguistics.

Phong Le and Willem Zuidema. 2015. Unsupervised
dependency parsing: Let’s use supervised parsers.
arXiv preprint arXiv:1504.04666.

Percy Liang and Dan Klein. 2009. Online em for un-
supervised models. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, NAACL ’09, pages
611–619, Stroudsburg, PA, USA. Association for
Computational Linguistics.

David Marecek and Milan Straka. 2013. Stop-
probability estimates computed on a large corpus
improve unsupervised dependency parsing. In ACL
(1), pages 281–290.

1687



Tahira Naseem, Harr Chen, Regina Barzilay, and Mark
Johnson. 2010. Using universal linguistic knowl-
edge to guide grammar induction. In Proceedings of
the 2010 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1234–1244. Asso-
ciation for Computational Linguistics.

Joakim Nivre. 2006. Inductive dependency parsing.
Springer.

John K Pate and Mark Johnson. 2016. Grammar induc-
tion from (lots of) words alone.

Radim Řehůřek and Petr Sojka. 2010. Software Frame-
work for Topic Modelling with Large Corpora. In
Proceedings of the LREC 2010 Workshop on New
Challenges for NLP Frameworks, pages 45–50, Val-
letta, Malta. ELRA. http://is.muni.cz/
publication/884893/en.

Valentin I Spitkovsky, Hiyan Alshawi, and Daniel Ju-
rafsky. 2013. Breaking out of local optima with
count transforms and model recombination: A study
in grammar induction. In EMNLP, pages 1983–
1995.

Kristina Toutanova, Dan Klein, Christopher D Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 173–180. Association for Compu-
tational Linguistics.

Kewei Tu and Vasant Honavar. 2012. Unambiguity
regularization for unsupervised learning of proba-
bilistic grammars. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 1324–1334. Association for
Computational Linguistics.

1688


