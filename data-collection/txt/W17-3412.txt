



















































Introducing Structure into Neural Network-Based Semantic Models


Proceedings of the 15th Meeting on the Mathematics of Language, page 127,
London, UK, July 13–14, 2017. c©2017 Association for Computational Linguistics

Introducing Structure into Neural Network-Based Semantic Models

Stephen Clark
University of Cambridge and DeepMind
stephen.clark@cl.cam.ac.uk

In this talk I will describe two attempts at in-
troducing syntactic structure into semantic models
using neural network architectures. The first study
focuses on a particular grammatical construction,
namely relative clauses, and centers around the
design of a new dataset for testing compositional
distributional models. The dataset is called REL-
PRON, and consists of pairs of terms and proper-
ties, such as

telescope : device that astronomer uses.

The idea is that a good compositional model will
produce a vector representation of the property
which is close to the vector for the term.

The second study focuses on deriving seman-
tic vectors for phrases and whole sentences, which
are then used in two tasks: sentence entailment,
and a dictionary definition-term matching task. A
feature of the proposed solution is that the syntac-
tic structure for a sentence is induced in an un-
supervised fashion, trained end-to-end in order to
optimize for the final task.

I will finish with some thoughts on whether and
how insights from traditional models of syntax and
semantics can contribute to semantic models based
on neural networks.

127


