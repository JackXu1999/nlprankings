



















































Neural User Simulation for Corpus-based Policy Optimisation of Spoken Dialogue Systems


Proceedings of the SIGDIAL 2018 Conference, pages 60–69,
Melbourne, Australia, 12-14 July 2018. c©2018 Association for Computational Linguistics

60

Neural User Simulation for Corpus-based Policy Optimisation for
Spoken Dialogue Systems

Florian L. Kreyssig, Iñigo Casanueva
Paweł Budzianowski and Milica Gašić

Cambridge University Engineering Department,
Trumpington Street, Cambridge, CB2 1PZ, UK
{flk24,ic340,pfb30,mg436}@cam.ac.uk

Abstract

User Simulators are one of the major tools
that enable offline training of task-oriented
dialogue systems. For this task the
Agenda-Based User Simulator (ABUS) is
often used. The ABUS is based on hand-
crafted rules and its output is in seman-
tic form. Issues arise from both properties
such as limited diversity and the inability
to interface a text-level belief tracker. This
paper introduces the Neural User Simu-
lator (NUS) whose behaviour is learned
from a corpus and which generates natu-
ral language, hence needing a less labelled
dataset than simulators generating a se-
mantic output. In comparison to much of
the past work on this topic, which evalu-
ates user simulators on corpus-based met-
rics, we use the NUS to train the policy of
a reinforcement learning based Spoken Di-
alogue System. The NUS is compared to
the ABUS by evaluating the policies that
were trained using the simulators. Cross-
model evaluation is performed i.e. training
on one simulator and testing on the other.
Furthermore, the trained policies are tested
on real users. In both evaluation tasks the
NUS outperformed the ABUS.

1 Introduction

Spoken Dialogue Systems (SDS) allow human-
computer interaction using natural speech. Task-
oriented dialogue systems, the focus of this work,
help users achieve goals such as finding restau-
rants or booking flights (Young et al., 2013).

Teaching a system how to respond appropriately
in a task-oriented setting is non-trivial. In state-of-
the-art systems this dialogue management task is
often formulated as a reinforcement learning (RL)

problem (Young et al., 2013; Roy et al., 2000;
Williams and Young, 2007; Gašić and Young,
2014). In this framework, the system learns by a
trial and error process governed by a reward func-
tion. User Simulators can be used to train the pol-
icy of a dialogue manager (DM) without real user
interactions. Furthermore, they allow an unlimited
number of dialogues to be created with each dia-
logue being faster than a dialogue with a human.

In this paper the Neural User Simulator (NUS)
is introduced which outputs natural language and
whose behaviour is learned from a corpus. The
main component, inspired by (El Asri et al.,
2016), consists of a feature extractor and a neu-
ral network based sequence-to-sequence model
(Sutskever et al., 2014). The sequence-to-
sequence model consists of a recurrent neural net-
work (RNN) encoder that encodes the dialogue
history and a decoder RNN which outputs natural
language. Furthermore, the NUS generates its own
goal and possibly changes it during a dialogue.
This allows the model to be deployed for training
more sophisticated DM policies. To achieve this, a
method is proposed that transforms the goal-labels
of the used dataset (DSTC2) into labels whose be-
haviour can be replicated during deployment.

The NUS is trained on dialogues between real
users and an SDS in a restaurant recommendation
domain. Compared to much of the related work on
user simulation, we use the trained NUS to train
the policy of a reinforcement learning based SDS.
In order to evaluate the NUS, an Agenda-Based
User-Simulator (ABUS) (Schatzmann et al., 2007)
is used to train another policy. The two policies
are compared against each other by using cross-
model evaluation (Schatztmann et al., 2005). This
means to train on one model and to test on the
other. Furthermore, both trained policies are tested
on real users. On both evaluation tasks the NUS
outperforms the ABUS, which is currently one of



61

the most popular off-line training tools for rein-
forcement learning based Spoken Dialogue Sys-
tems (Koo et al., 2015; Fatemi et al., 2016; Chen
et al., 2017; Chang et al., 2017; Casanueva et al.,
2018; Weisz et al., 2018; Shah et al., 2018).

The remainder of this paper is organised as fol-
lows. Section 2 briefly describes task-oriented di-
alogue. Section 3 describes the motivation for the
NUS and discusses related work. Section 4 ex-
plains the structure of the NUS, how it is trained
and how it is deployed for training a DM’s policy.
Sections 5 and 6 present the experimental setup
and results. Finally, Section 7 gives conclusions.

2 Task-Oriented Dialogue

A Task-Oriented SDS is typically designed ac-
cording to a structured ontology, which defines
what the system can talk about. In a system rec-
ommending restaurants the ontology defines those
attributes of a restaurant that the user can choose,
called informable slots (e.g. different food types,
areas and price ranges), the attributes that the user
can request, called requestable slots (e.g. phone
number or address) and the restaurants that it has
data about. An attribute is referred to as a slot and
has a corresponding value. Together these are re-
ferred to as a slot-value pair (e.g. area=north).

Using RL the DM is trained to act such that is
maximises the cumulative future reward. The pro-
cess by which the DM chooses its next action is
called its policy. A typical approach to defining
the reward function for a task-oriented SDS is to
apply a small per-turn penalty to encourage short
dialogues and to give a large positive reward at the
end of each successful interaction.

3 Motivation and Related Work

Ideally the DM’s policy would be trained by inter-
acting with real users. Although there are models
that support on-line learning (Gašić et al., 2011),
for the majority of RL algorithms, which require
a lot of interactions, this is impractical. Further-
more, a set of users needs to be recruited every
time a policy is trained. This makes common prac-
tices such as hyper-parameter optimization pro-
hibitively expensive. Thus, it is natural to try to
learn from a dataset which needs to be recorded
only once, but can be used over and over again.

A problem with learning directly from recorded
dialogue corpora is that the state space that was
visited during the collection of the data is limited;

the size of the recorded corpus usually falls short
of the requirements for training a statistical DM.
However, even if the size of the corpus is large
enough the optimal dialogue strategy is likely not
to be contained within it.

A solution is to transform the static corpus into
a dynamic tool: a user simulator. The user simu-
lator (US) is trained on a dialogue corpus to learn
what responses a real user would provide in a
given dialogue context. The US is trained using
supervised learning since the aim is for it to learn
typical user behaviour. For the DM, however, we
want optimal behaviour which is why supervised
learning cannot be used. By interacting with the
SDS, the trained US can be used to train the DM’s
policy. The DM’s policy is optimised using the
feedback given by either the user simulator or a
separate evaluator. Any number of dialogues can
be generated using the US and dialogue strategies
that are not in the recorded corpus can be explored.

Most user-simulators work on the level of user
semantics. These usually consist of a user di-
alogue act (e.g. inform, or request) and a cor-
responding slot-value pair. The first statistical
user simulator (Eckert et al., 1997) used a sim-
ple bi-gram model P (au |am) to predict the next
user act au given the last system act am. It has
the advantage of being purely probabilistic and
domain-independent. However, it does not take
the full dialogue history into account and is not
conditioned on a goal, leading to incoherent user
behaviour throughout a dialogue. Scheffler and
Young (2000, 2001) attempted to overcome goal
inconsistency by proposing a graph-based model.
However, developing the graph structure requires
extensive domain-specific knowledge. Pietquin
and Dutoit (2006) combined features from Shef-
fler and Young’s work with Eckert’s Model, by
conditioning a set of probabilities on an explicit
representation of the user goal and memory. A
Markov Model is also used by Georgila et al.
(2005). It uses a large feature vector to describe
the user’s current state, which helps to compensate
for the Markov assumption. However, the model
is not conditioned on any goal. Therefore, it is
not used to train a dialogue policy since it is im-
possible to determine whether the user goal was
fulfilled. A hidden Markov model was proposed
by Cuayáhuitl et al. (2005), which was also not
used to train a policy. Chandramohan et al. (2011)
cast user simulation as an inverse reinforcement



62

learning problem where the user is modelled as
a decision-making agent. The model did not in-
corporate a user goal and was hence not used to
train a policy. The most prominent user model
for policy optimisation is the Agenda-Based User
Simulator (Schatzmann et al., 2007), which repre-
sents the user state elegantly as a stack of neces-
sary user actions, called the agenda. The mecha-
nism that generates the user response and updates
the agenda does not require any data, though it can
be improved using data. The model is conditioned
on a goal for which it has update rules in case the
dialogue system expresses that it cannot fulfil the
goal. El Asri et al. (2016) modelled user simula-
tion as a sequence-to-sequence task. The model
can keep track of the dialogue history and user be-
haviour is learned entirely from data. However,
goal changes were not modelled, even though a
large proportion of dialogues within their dataset
(DSTC2) contains goal changes. Their model out-
performed the ABUS on statistical metrics, which
is not surprising given that it was trained by opti-
mising a statistical metric and the ABUS was not.

The aforementioned work focuses on user sim-
ulation at the semantic level. Multiple issues
arise from this approach. Firstly, annotating the
user-response with the correct semantics is costly.
More data could be collected, if the US were to
output natural language. Secondly, research sug-
gests that the two modules of an SDS perform-
ing Spoken Language Understanding (SLU) and
belief tracking should be jointly trained as a sin-
gle entity (Mrkšić et al., 2017; Sun et al., 2016,
2014; Zilka and Jurcicek, 2015; Ramadan et al.,
2018). In fact in the second Dialogue State Track-
ing Challenge (DSTC2) (Henderson et al., 2014),
the data of which this work uses, systems which
used no external SLU module outperformed all
systems that only used an external SLU Module1.
Training the policy of a DM in a simulated envi-
ronment, when also using a joint system for SLU
and belief tracking is not possible without a US
that produces natural language. Thirdly, a US is
sometimes augmented with an error model which
generates a set of competing hypotheses with as-
sociated confidence scores trying to replicate the
errors of the speech recogniser. When the error
model matches the characteristics of the speech
recogniser more accurately, the SDS performs bet-
ter (Williams, 2008). However, speech recogni-

1The best-performing models used both.

tion errors are badly modelled based on user se-
mantics since they arise (mostly) due to the pho-
netics of the spoken words and not their seman-
tics (Goldwater et al., 2010). Thus, an SDS that is
trained with a natural language based error model
is likely to outperform one trained with a semantic
error model when tested on real users. Sequence-
to-sequence learning for word-level user simula-
tion is performed in (Crook and Marin, 2017),
though the model is not conditioned on any goal
and hence not used for policy optimisation. A
word-level user simulator was also used in (Li
et al., 2017) where it was built by augmenting the
ABUS with a natural language generator.

4 Neural User Simulator

Feature 
History 

Feature  
Extractor 

Request-Vector 
Accepted Venue 

Original Request-Vector 

Sequence to Sequence
Model User Utterance 

Spoken Dialogue  
System 

Goal 
Generator Ontology 

Figure 1: General Architecture of the Neural User
Simulator. The System Output is passed to the
Feature Extractor. It generates a new feature vec-
tor that is appended to the Feature History, which
is passed to the sequence-to-sequence model to
produce the user utterance. At the start of the dia-
logue the Goal Generator generates a goal, which
might change during the course of the dialogue.

An overview of the NUS is given in Figure 1.
At the start of a dialogue a random goal G0 is gen-
erated by the Goal Generator. The possibilities
for G0 are defined by the ontology. In dialogue
turn T , the output of the SDS (daT ) is passed to
the NUS’s Feature Extractor, which generates a
feature vector vT based on daT , the current user
goal, GT , and parts of the dialogue history. This



63

vector is appended to the Feature History v1:T =
v1...vT . This sequence is passed to the sequence-
to-sequence model (Fig. 2), which will generate
the user’s length nT utterance uT = w0...wnT .
As in Figure 2, words in uT corresponding to a
slot are replaced by a slot token; a process called
delexicalisation. If the SDS expresses to the NUS
that there is no venue matching the NUS’s con-
straints, the goal will be altered by the Goal Gen-
erator.

4.1 Goal Generator

The Goal Generator generates a random goal
G0 = (C0, R) at the start of the dialogue.
It consists of a set of constraints, C0, which
specify the required venue e.g. (food=Spanish,
area=north) and a number of requests, R, that
specify the information that the NUS wants about
the final venue e.g. the address or the phone
number. The possibilities for Ct and R are de-
fined by the ontology. In DSTC2 Ct can consist
of a maximum of three constraints; food, area
and pricerange. Whether each of the three
is present is independently sampled with a prob-
ability of 0.66, 0.62 and 0.58 respectively. These
probabilities were estimated from the DSTC2 data
set. If no constraint is sampled then the goal is re-
sampled. For each slot in C0 a value (e.g. north
for area) is sampled uniformly from the ontol-
ogy. Similarly, the presence of a request is in-
dependently sampled, followed by re-sampling if
zero requests were chosen.

When training the sequence-to-sequence model,
the Goal Generator is not used, but instead the
goal labels from the DSTC2 dataset are used. In
DSTC2 one goal-label is given to the entire di-
alogue. This goal is always the final goal. If
the user’s goal at the start of the dialogue is
(food=eritrean, area=south), which is changed
to (food=spanish, area=south), due to the non-
existence of an Eritrean restaurant in the south, us-
ing only the final goal is insufficient to model the
dialogue. The final goal can only be used for the
requests as they are not altered during a dialogue.
DSTC2 also provides turn-specific labels. These
contain the constraints and requests expressed by
the user up until and including the current turn.
When training a policy with the NUS, such labels
would not be available as they “predict the future”,
i.e. when the turn-specific constraints change from
(area=south) to (food=eritrean, area=south) it

means that the user will inform the system about
her desire to eat Eritrean food in the current turn.

In related work on user-simulation for which the
DSTC2 dataset was used, the final goal was used
for the entire dialogue (El Asri et al., 2016; Serras
et al., 2017; Liu and Lane, 2017). As stated above,
we do not believe this to be sufficient. The follow-
ing describes how to update the turn-specific con-
straint labels such that their behaviour can be repli-
cated when training a DM’s policy, whilst allow-
ing goal changes to be modelled. The update strat-
egy is illustrated in Table 1 with an example. The
final turn keeps its constraints, from which we it-
erate backwards through the list of DSTC2’s turn-
specific constraints. The constraints of a turn will
be set to the updated constraints of the succeed-
ing turn, besides if the same slot is present with a
different value. In that case the value will be kept.
The behaviour of the updated turn-specific goal-
labels can be replicated when the NUS is used to
train a DM’s policy. In the example, the food type
changed due to the SDS expressing that there is
no restaurant serving Eritrean food in the south.
When deploying the NUS to train a policy, the goal
is updated when the SDS outputs the canthelp
dialogue act.

4.2 Feature Extractor

The Feature Extractor generates the feature vector
that is appended to the sequence of feature vec-
tors, here called Feature History, that is passed to
the sequence-to-sequence model. The input to the
Feature Extractor is the output of the DM and the
current goal Gt. Furthermore, as indicated in Fig-
ure 1, the Feature Extractor keeps track of the cur-
rently accepted venue as well as the current and
initial request-vector, which is explained below.

The feature vector vt = [at rt it ct] is made
up of four sub-vectors. The motivation behind the
way in which these four vectors were designed is
to provide an embedding for the system response
that preserves all necessary value-independent in-
formation.

The first vector, machine-act vector at, encodes
the dialogue acts of the system response and con-
sists of two parts; at =

[
a1t a

2
t

]
. a1t is a binary

representation of the system dialogue acts present
in the input. Its length is thus the number of possi-
ble system dialogue acts. It is binary and not one-
hot since in DSTC2 multiple dialogue acts can be
in the system’s response. a2t is a binary represen-



64

Ct Original Updated
C0 (food=eritrean) (area=south, food=eritrean, pricerange=cheap)
C1 (area=south, food=eritrean) (area=south, food=eritrean, pricerange=cheap)
C2 (area=south, food=spanish) (area=south, food=spanish, pricerange=cheap)
C3 (area=south, food=spanish, pricerange=cheap) (area=south, food=spanish, pricerange=cheap)

Table 1: An example of how DSTC2’s turn-specific constraint labels can be transformed such that their
behaviour can be replicated when training a dialogue manager.

tation of the slot if the dialogue act is request
or select and if it is inform or expl-conf
together with a correct slot-value pair for an in-
formable slot. The length is four times the num-
ber of informable slots. a2t is necessary due to
the dependence of the sentence structure on the ex-
act slot mentioned by the system. The utterances
of a user in response to request(food) and
request(area) are often very different.

The second vector, request-vector rt, is a bi-
nary representation of the requests that have not
yet been fulfilled. It’s length is thus the number of
requestable slots. In comparison to the other three
vectors the feature extractor needs to remember it
for the next turn. At the start of the dialogue the
indices corresponding to requests that are in R are
set to 1 and the rest to 0. Whenever the system in-
forms a certain request the corresponding index in
rt is set to 0. When a new venue is proposed rt is
reset to the original request vector, which is why
the Feature Extractor keeps track of it.

The third vector, inconsistency-vector it, repre-
sents the inconsistency between the system’s re-
sponse and Ct. Every time a slot is mentioned by
the system, when describing a venue (inform)
or confirming a slot-value pair (expl-conf or
impl-conf), the indices corresponding to the
slots that have been misunderstood are set to 1.
The length of it is the number of informable slots.
This vector is necessary in order for the NUS to
correct the system.

The fourth vector, ct, is a binary representa-
tion of the slots that are in the constraints Ct. It’s
length is thus the number of informable slots. This
vector is necessary in order for the NUS to be able
to inform about its preferred venue.

4.3 Sequence-To-Sequence Model

The sequence-to-sequence model (Figure 2) con-
sists of an RNN encoder, followed by a fully-
connect layer and an RNN decoder. An RNN can

be defined as:

(ht, st) = RNN (xt, st−1) (1)

At time-step t, an RNN uses an input xt and an
internal state st−1 to produce its output ht and
its new internal state st. A specific RNN-design
is usually defined using matrix multiplications,
element-wise additions and multiplications as well
as element-wise non-linear functions. There are a
plethora of different RNN architectures that could
be used and explored. Given that such exploration
is not the focus of this work a single layer LSTM
(Hochreiter and Schmidhuber, 1997) is used for
both the RNN encoder and decoder. The exact
LSTM version used in this work uses a forget gate
without bias and does not use peep-holes.

The first RNN (shown as white blocks in Fig. 2)
takes one feature vector vt at a time as its input
(xEt = vt). If the current dialogue turn is turn T
then the final output of the RNN encoder is given
by hET , which is passed through a fully-connected
layer (shown as the light-grey block) with linear
activation function:

pT = Wph
E
T + bp (2)

For a certain encoding pT the sequence-to-
sequence model should define a probability dis-
tribution over different sequences. By sampling
from this distribution the NUS can generate a di-
verse set of sentences corresponding to the same
dialogue context. The conditional probability dis-
tribution of a length L sequence is defined as:

P (u |p)=P (w0 |p)
L∏
t=1

P (wt |wt−1...w0,p) (3)

The decoder RNN (shown as dark blocks) will be
used to model P (wt |wt−1...w0,p). It’s input at
each time-step is the concatenation of an embed-
ding wt−1 (we used 1-hot) of the previous word
wt−1 (xDt = [wt−1 p]). For P (w0 |p) a start-
of-sentence (<SOS>) token is used as w−1. The



65

Figure 2: Sequence-To-Sequence model of the Neural User Simulator. Here, the NUS is generating
the user response to the third system output. The white, light-grey and dark blocks represent the RNN
encoder, a fully-connected layer and the RNN decoder respectively. The previous output of the decoder
is passed to its input for the next time-step. v3:1 are the first three feature vectors (see Sec. 4.2).

end of the utterance is modelled using an end-of-
sentence (<EOS>) token. When the decoder RNN
generates the end-of-sentence token, the decoding
process is terminated. The output of the decoder
RNN, hDt , is passed through an affine transform
followed by the softmax function, SM, to form
P (wt |wt−1...w0,p). A word wt can be obtained
by either taking the word with the highest proba-
bility or sampling from the distribution:

P (wt | wt−1...w0,p) = SM(WwhDt + bw) (4)
wt ∼ P (wt | wt−1...w0,p) (5)

During training the words are not sampled from
the output distribution, but instead the true words
from the dataset are used. This a common tech-
nique that is often referred to as teacher-forcing,
though it also directly follows from equation 3.

To generate a sequence using an RNN, beam-
search is often used. Using beam-search with n
beams, the words corresponding to the top n prob-
abilities of P (w0 |p) are the first n beams. For
each succeeding wt, the n words corresponding to
the top n probabilities of P (wt |wt−1...w0,p) are
taken for each of the n beams. This is followed
by reducing the number of beams from now n2

down to n, by taking the n beams with the high-
est probability P (wtwt−1...w0 |p). This is a de-
terministic process. However, for the NUS to al-
ways give the same response in the same context is
not realistic. Thus, the NUS cannot cover the full
breadth of user behaviour if beam-search is used.
To solve this issue while keeping the benefit of re-
jecting sequences with low probability, a type of
beam-search with sampling is used. The process
is identical to the above, but n words per beam
are sampled from the probability distribution. The

NUS is now non-deterministic resulting in a di-
verse US. Using 2 beams gave a good trade-off
between reasonable responses and diversity.

4.4 Training

The neural sequence-to-sequence model is trained
to maximize the log probability that it assigns to
the user utterances of the training data set:

L=
N∑
n=1

logP (w0 |p)
Ln∑
t=1

logP (wt |wt−1:0,p) (6)

The network was implemented in Tensorflow
(Abadi et al., 2015) and optimized using Ten-
sorflow’s default setup of the Adam optimizer
(Kingma and Ba, 2015). The LSTM layers and
the fully-connected layer had widths of 100 each
to give a reasonable number of overall parame-
ters. The width was not tuned. The learning rate
was optimised on a held out validation set and no
regularization methods used. The training set was
shuffled at the dialogue turn level.

The manual transcriptions of the DSTC2 train-
ing set (not the ASR output) were used to train the
sequence-to-sequence model. Since the transcrip-
tions were done manually they contained spelling
errors. These were manually corrected to ensure
proper delexicalization. Some dialogues were dis-
carded due to transcriptions errors being too large.
After cleaning the dataset the training set con-
sisted of 1609 dialogues with a total of 11638 dia-
logue turns. The validation set had 505 dialogues
with 3896 dialogue turns. The maximum sequence
length of the delexicalized turns was 22, including
the end of sentence character. The maximum dia-
logue length was 30 turns.



66

5 Experimental Setup

The evaluation of user simulators is an ongoing
area of research and a variety of techniques can
be found in the literature. Most papers published
on user simulation evaluate their US using direct
methods. These methods evaluate the US through
a statistical measure of similarity between the out-
puts of the US and a real user on a test set. Mul-
tiple models can outperform the ABUS on these
metrics. However, this is unsurprising since these
user simulators were trained on the same or sim-
ilar metrics. The ABUS was explicitly proposed
as a tool to train the policy of a dialogue manager
and it is still the dominant form of US used for this
task. Therefore, the only fair comparison between
a new US model and the ABUS is to use the in-
direct method of evaluating the policies that were
obtained by training with each US.

5.1 Training

All dialogue policies were trained with the PyDial
toolkit (Ultes et al., 2017), by interacting with ei-
ther the NUS or ABUS. The RL algorithm used is
GP-SARSA (Gašić and Young, 2014) with hyper-
parameters taken from (Casanueva et al., 2017).
The reward function used gives a reward of 20 to a
successfully completed dialogue and of -1 for each
dialogue turn. The maximum dialogue length was
25 turns. The presented metrics are success rate
(SR) and average reward over test dialogues. SR is
the percentage of dialogues for which the system
satisfied both the user’s constraints and requests.
The final goal, after possible goal changes, was
used for this evaluation. When policies are trained
using the NUS, its output is parsed using PyDial’s
regular expression based semantic decoder. The
policies were trained for 4000 dialogues.

5.2 Testing with a simulated user

In Schatzmann et. al (2005) cross-model evalua-
tion is proposed to compare user simulators. First,
the user simulators to be evaluated are used to train
N policy each. Then these policies are tested us-
ing the different user simulators and the results av-
eraged. Schatztmann et al. (2005) showed that a
strategy learned with a good user model still per-
forms well when tested on poor user models. If a
policy performs well on all user simulators and not
just on the one that it was trained on, it indicates
that the US with which it was trained is diverse
and realistic, and thus the policy is likely to per-

form better on real users. For each US five poli-
cies (N = 5), each using a different random seed
for initialisation, are trained. Results are reported
for both the best and the average performance on
1000 test dialogues. The ABUS is programmed to
always mention the new goal after a goal change.
In order to not let this affect our results we im-
plement the same for the NUS by re-sampling a
sentence if the new goal is not mentioned.

5.3 Testing with real users

Though the above test is already more indicative
of policy performance on real users than measur-
ing statistical metrics of user behaviour, a better
test is to test with human users. For the test on hu-
man users, two policies for each US that was used
for training are chosen from the five policies. The
first policy is the one that performed best when
tested on the NUS. The second is the one that per-
formed best when tested on the ABUS. This choice
of policies is motivated by a type of overfitting to
be seen in Sec. 6.1. The evaluation of the trained
dialogue policies in interaction with real users fol-
lows a similar set-up to (Jurčı́ček et al., 2011).
Users are recruited through the Amazon Mechan-
ical Turk (AMT) service. 1000 dialogues (250 per
policy) were gathered. The learnt policies were
incorporated into an SDS pipeline with a commer-
cial ASR system. The AMT users were asked to
find a restaurant that matches certain constraints
and find certain requests. Subjects were randomly
allocated to one of the four analysed systems. Af-
ter each dialogue the users were asked whether
they judged the dialogue to be successful or not
which was then translated to the reward measure.

6 Experimental Results

6.1 Cross-Model Evaluation

Table 2 shows the results of the cross-model eval-
uation after 4000 training dialogues. The policies
trained with the NUS achieved an average success
rate (SR) of 94.0% and of 96.6% when tested on
the ABUS and the NUS, respectively. By compar-
ison, the policies trained with the ABUS achieved
average SRs of 99.5% and 45.5% respectively.
Thus, training with the NUS leads to policies that
can perform well on both USs, which is not the
case for training with the ABUS. Furthermore, the
best SRs when tested on the ABUS are similar at
99.9% (ABUS) and 99.8% (NUS). When tested on
the NUS the best SRs were 71.5% (ABUS) and



67

Train. Sim. Eval. Sim.
NUS ABUS

Rew. Suc. Rew. Suc.
NUS-best 13.0 98.0N1 13.3 99.8
ABUS-best 1.53 71.5A1 13.8 99.9A2

NUS-avg 12.4 96.6 11.2 94.0
ABUS-avg -7.6 45.5 13.5 99.5

Table 2: Results for policies trained for 4000 di-
alogues on NUS and ABUS when tested on both
USs for 1000 dialogues. Five policies with differ-
ent initialisations were trained for each US. Both
average and best results are shown.

Train. Sim. Eval. Sim.
NUS ABUS

Rew. Suc. Rew. Suc.
NUS-best 12.2 95.9 13.9 99.9N2

ABUS-best -4.0 54.8 13.2 99.0
NUS-avg 12.0 95.4 12.2 97.3
ABUS-avg -9.48 42.3 12.8 98.4

Table 3: As Table 2 but trained for 1000 dialogues.

98.0% (NUS). This shows that the behaviour of
the Neural User Simulator is realistic and diverse
enough to train policies that can also perform very
well on the Agenda-Based User Simulator.

Of the five policies, for each US, the policy per-
forming best on the NUS was not the best perform-
ing policy on the ABUS. This could indicate that
the policy “overfits” to a particular user simulator.
Overfitting usually manifests itself in worse results
as the model is trained for longer. Five policies
trained on each US for only 1000 dialogues were
also evaluated, the results of which can be seen in
Table 3. After training for 1000 dialogues, the av-
erage SR of the policies trained on the NUS when
tested on the ABUS was 97.3% in comparison to
94.0% after 4000 dialogues. This behaviour was
observed for all five seeds, which indicates that the
policy indeed overfits to the NUS. For the policies
trained with the ABUS this was not observed. This
could indicate that the policy can learn to exploit
some of the shortcomings of the trained NUS.

6.2 Human Evaluation

The results of the human evaluation are shown in
Table 4 for 250 dialogues per policy. In Table 4
policies are marked using an ID (Uα) that trans-
lates to results in Tables 2 and 3. Both policies
trained with the NUS outperformed those trained

Training Simulator Human Evaluation
Rew. Suc.

NUS - N1 13.4 91.8
NUS - N2 13.8 93.4
ABUS - A1 13.3 90.0
ABUS - A2 13.1 88.5

Table 4: Real User Evaluation. Results over 250
dialogues with human users. N1 and A1 per-
formed best on the NUS. N2 and A2 performed
best on the ABUS. Rewards are not comparable to
Table 2 and 3 since all user goals were achievable.

on the ABUS in terms of both reward and suc-
cess rate. The best performing policy trained on
the NUS achieves a 93.4% success rate and 13.8
average rewards whilst the best performing policy
trained with the ABUS achieves only a 90.0% suc-
cess rate and 13.3 average reward. This shows that
the good performance of the NUS on the cross-
model evaluation transfers to real users. Further-
more, the overfitting to a particular US is also ob-
served in the real user evaluation. For not only the
policies trained on the NUS, but also those trained
on the ABUS, the best performing policy was the
policy that performed best on the other US.

7 Conclusion

We introduced the Neural User Simulator (NUS),
which uses the system’s response in its seman-
tic form as input and gives a natural language re-
sponse. It thus needs less labelling of the train-
ing data than User Simulators that generate a re-
sponse in semantic form. It was shown that the
NUS learns realistic user behaviour from a corpus
of recorded dialogues such that it can be used to
optimise the policy of the dialogue manager of a
spoken dialogue system. The NUS was compared
to the Agenda-Based User Simulator by evaluating
policies trained with these user simulators. The
trained policies were compared both by testing
them with simulated users and also with real users.
The NUS excelled on both evaluation tasks.

Acknowledgements

This research was partly funded by the EPSRC
grant EP/M018946/1 Open Domain Statistical
Spoken Dialogue Systems. Florian Kreyssig is
supported by the Studienstiftung des Deutschen
Volkes. Paweł Budzianowski is supported by the
EPSRC and Toshiba Research Europe Ltd.



68

References

Martı́n Abadi, Ashish Agarwal, Paul Barham, Eugene
Brevdo, Zhifeng Chen, Craig Citro, Greg S. Cor-
rado, Andy Davis, Jeffrey Dean, Matthieu Devin,
Sanjay Ghemawat, Ian Goodfellow, Andrew Harp,
Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal
Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh
Levenberg, Dandelion Mané, Rajat Monga, Sherry
Moore, Derek Murray, Chris Olah, Mike Schus-
ter, Jonathon Shlens, Benoit Steiner, Ilya Sutskever,
Kunal Talwar, Paul Tucker, Vincent Vanhoucke,
Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals,
Pete Warden, Martin Wattenberg, Martin Wicke,
Yuan Yu, and Xiaoqiang Zheng. 2015. TensorFlow:
Large-scale machine learning on heterogeneous sys-
tems. Software available from tensorflow.org.

Iñigo Casanueva, Paweł Budzianowski, Pei-Hao Su,
Nikola Mrkšić, Tsung-Hsien Wen, Stefan Ultes,
Lina Rojas-Barahona, Steve Young, and Milica
Gašić. 2017. A benchmarking environment for re-
inforcement learning based task oriented dialogue
management. In NIPS Deep Reinforcement Learn-
ing Symposium.

Iñigo Casanueva, Paweł Budzianowski, Pei-Hao Su,
Stefan Ultes, Lina Rojas-Barahona, Bo-Hsiang
Tseng, and Milica Gašić. 2018. Feudal reinforce-
ment learning for dialogue management in large do-
mains. In Proc. NAACL 2018.

Senthilkumar Chandramohan, Matthieu Geist, Fabrice
Lefevre, and Olivier Pietquin. 2011. User simula-
tion in dialogue systems using inverse reinforcement
learning. In Proceedings of the Twelfth Annual Con-
ference of the International Speech Communication
Association.

Cheng Chang, Runzhe Yang, Lu Chen, Xiang Zhou,
and Kai Yu. 2017. Affordable on-line dialogue pol-
icy learning. In Proceedings of the 2017 Confer-
ence on Empirical Methods in Natural Language
Processing, pages 2200–2209.

Lu Chen, Xiang Zhou, Cheng Chang, Runzhe Yang,
and Kai Yu. 2017. Agent-aware dropout dqn for
safe and efficient on-line dialogue policy learning.
In Proceedings of the 2017 Conference on Empiri-
cal Methods in Natural Language Processing, pages
2454–2464.

Paul Crook and Alex Marin. 2017. Sequence to se-
quence modeling for user simulation in dialog sys-
tems. In Proceedings of the 18th Annual Conference
of the International Speech Communication Associ-
ation.

Heriberto Cuayáhuitl, Steve Renals, Oliver Lemon, and
Hiroshi Shimodaira. 2005. Human-computer dia-
logue simulation using hidden markov models. In
Automatic Speech Recognition and Understanding,
2005 IEEE Workshop on, pages 290–295. IEEE.

Wieland Eckert, Esther Levin, and Roberto Pierac-
cini. 1997. User modeling for spoken dialogue sys-
tem evaluation. In Automatic Speech Recognition
and Understanding, 1997. Proceedings., 1997 IEEE
Workshop on, pages 80–87. IEEE.

Layla El Asri, Jing He, and Kaheer Suleman. 2016.
A sequence-to-sequence model for user simulation
in spoken dialogue systems. Proceedings of the
17th Annual Conference of the International Speech
Communication Association, pages 1151–1155.

Mehdi Fatemi, Layla El Asri, Hannes Schulz, Jing He,
and Kaheer Suleman. 2016. Policy networks with
two-stage training for dialogue systems. In Proceed-
ings of the 17th Annual Meeting of the Special Inter-
est Group on Discourse and Dialogue, pages 101–
110.

Milica Gašić and Steve Young. 2014. Gaussian pro-
cesses for pomdp-based dialogue manager optimiza-
tion. IEEE/ACM Transactions on Audio, Speech,
and Language Processing, 22(1):28–40.

M. Gašić, F. Jurčı́ček, B. Thomson, K. Yu, and
S. Young. 2011. On-line policy optimisation of spo-
ken dialogue systems via live interaction with hu-
man subjects. In Automatic Speech Recognition and
Understanding, 2011 IEEE Workshop on.

Kallirroi Georgila, James Henderson, and Oliver
Lemon. 2005. Learning user simulations for infor-
mation state update dialogue systems. In Ninth Eu-
ropean Conference on Speech Communication and
Technology.

Sharon Goldwater, Dan Jurafsky, and Christopher D
Manning. 2010. Which words are hard to rec-
ognize? prosodic, lexical, and disfluency factors
that increase speech recognition error rates. Speech
Communication, 52(3):181–200.

Matthew Henderson, Blaise Thomson, and Jason D
Williams. 2014. The second dialog state tracking
challenge. In Proceedings of the 15th Annual Meet-
ing of the Special Interest Group on Discourse and
Dialogue (SIGDIAL), pages 263–272.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Filip Jurčı́ček, Simon Keizer, Milica Gašić, Fran-
cois Mairesse, Blaise Thomson, Kai Yu, and Steve
Young. 2011. Real user evaluation of spoken dia-
logue systems using amazon mechanical turk. In
Proceedings of the Twelfth Annual Conference of the
International Speech Communication Association.

Diederik P Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization.

Sangjun Koo, Seonghan Ryu, and Gary Geunbae Lee.
2015. Implementation of generic positive-negative
tracker in extensible dialog system. In Automatic
Speech Recognition and Understanding (ASRU),
2015 IEEE Workshop on, pages 798–805. IEEE.

https://www.tensorflow.org/
https://www.tensorflow.org/
https://www.tensorflow.org/
http://aclweb.org/anthology/D17-1234
http://aclweb.org/anthology/D17-1234
http://aclweb.org/anthology/D17-1260
http://aclweb.org/anthology/D17-1260
http://www.aclweb.org/anthology/W16-3613
http://www.aclweb.org/anthology/W16-3613
http://www.aclweb.org/anthology/W14-4337
http://www.aclweb.org/anthology/W14-4337


69

Xiujun Li, Yun-Nung Chen, Lihong Li, Jianfeng Gao,
and Asli Celikyilmaz. 2017. End-to-end task-
completion neural dialogue systems. In Proceedings
of the Eighth International Joint Conference on Nat-
ural Language Processing (Volume 1: Long Papers),
pages 733–743, Taipei, Taiwan. Asian Federation of
Natural Language Processing.

Bing Liu and Ian Lane. 2017. Iterative policy learning
in end-to-end trainable task-oriented neural dialog
models. In 2017 IEEE Automatic Speech Recog-
nition and Understanding Workshop, ASRU, pages
482–489.

Nikola Mrkšić, Diarmuid Ó Séaghdha, Tsung-Hsien
Wen, Blaise Thomson, and Steve Young. 2017.
Neural belief tracker: Data-driven dialogue state
tracking. In Proceedings of the 55th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), volume 1, pages 1777–
1788.

Olivier Pietquin and Thierry Dutoit. 2006. A prob-
abilistic framework for dialog simulation and opti-
mal strategy learning. IEEE Transactions on Audio,
Speech, and Language Processing, 14(2):589–599.

Osman Ramadan, Paweł Budzianowski, and Milica
Gašić. 2018. Large-scale multi-domain belief track-
ing with knowledge sharing. In Proceedings of the
56th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 2: Short Papers). Asso-
ciation for Computational Linguistics.

Nicholas Roy, Joelle Pineau, and Sebastian Thrun.
2000. Spoken dialogue management using proba-
bilistic reasoning. In Proceedings of the 38th An-
nual Meeting on Association for Computational Lin-
guistics, pages 93–100. Association for Computa-
tional Linguistics.

Jost Schatzmann, Blaise Thomson, Karl Weilhammer,
Hui Ye, and Steve Young. 2007. Agenda-based user
simulation for bootstrapping a pomdp dialogue sys-
tem. In Human Language Technologies 2007: The
Conference of the North American Chapter of the
Association for Computational Linguistics; Com-
panion Volume, Short Papers, pages 149–152. As-
sociation for Computational Linguistics.

Jost Schatztmann, Matthew N Stuttle, Karl Weilham-
mer, and Steve Young. 2005. Effects of the user
model on simulation-based learning of dialogue
strategies. In Automatic Speech Recognition and
Understanding, 2005 IEEE Workshop on, pages
220–225. IEEE.

Konrad Scheffler and Steve Young. 2000. Probabilis-
tic simulation of human-machine dialogues. In
Acoustics, Speech, and Signal Processing, 2000.
ICASSP’00. Proceedings. 2000 IEEE International
Conference on, volume 2, pages II1217–II1220.
IEEE.

Konrad Scheffler and Steve Young. 2001. Corpus-
based dialogue simulation for automatic strategy
learning and evaluation. In Proc. NAACL Workshop
on Adaptation in Dialogue Systems, pages 64–70.

Manex Serras, Marı́a Inés Torres Torres, and Arantza
del Pozo. 2017. Regularized neural user model for
goal oriented spoken dialogue systems. In Interna-
tional Workshop on Spoken Dialogue Systems. As-
sociation for Computational Linguistics.

Pararth Shah, Dilek Hakkani-Tür, Gokhan Tür, Ab-
hinav Rastogi, Ankur Bapna, Neha Nayak, and
Larry Heck. 2018. Building a conversational agent
overnight with dialogue self-play. arXiv preprint
arXiv:1801.04871.

Kai Sun, Lu Chen, Su Zhu, and Kai Yu. 2014. The
sjtu system for dialog state tracking challenge 2.
In Proceedings of the 15th Annual Meeting of the
Special Interest Group on Discourse and Dialogue
(SIGDIAL), pages 318–326.

Kai Sun, Qizhe Xie, and Kai Yu. 2016. Recurrent poly-
nomial network for dialogue state tracking. Dia-
logue & Discourse, 7(3):65–88.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in neural information process-
ing systems, pages 3104–3112.

Stefan Ultes, Lina M. Rojas Barahona, Pei-Hao Su,
David Vandyke, Dongho Kim, Iñigo Casanueva,
Paweł Budzianowski, Nikola Mrkšić, Tsung-Hsien
Wen, Milica Gašić, and Steve Young. 2017. Py-
Dial: A Multi-domain Statistical Dialogue Sys-
tem Toolkit. In Proceedings of ACL 2017, System
Demonstrations, pages 73–78, Vancouver, Canada.
Association for Computational Linguistics.

Gellért Weisz, Paweł Budzianowski, Pei-Hao Su, and
Milica Gašić. 2018. Sample efficient deep reinforce-
ment learning for dialogue systems with large action
spaces. arXiv preprint arXiv:1802.03753.

Jason D Williams. 2008. Evaluating user simulations
with the cramér–von mises divergence. Speech com-
munication, 50(10):829–846.

Jason D Williams and Steve Young. 2007. Partially
observable markov decision processes for spoken
dialog systems. Computer Speech & Language,
21(2):393–422.

Steve Young, Milica Gašić, Blaise Thomson, and Ja-
son D Williams. 2013. Pomdp-based statistical spo-
ken dialog systems: A review. Proceedings of the
IEEE, 101(5):1160–1179.

Lukas Zilka and Filip Jurcicek. 2015. Incremental
lstm-based dialog state tracker. In Automatic Speech
Recognition and Understanding (ASRU), 2015 IEEE
Workshop on, pages 757–762. IEEE.

http://www.aclweb.org/anthology/I17-1074
http://www.aclweb.org/anthology/I17-1074
http://www.aclweb.org/anthology/P17-1163
http://www.aclweb.org/anthology/P17-1163
http://aclweb.org/anthology/P/P00/P00-1013.pdf
http://aclweb.org/anthology/P/P00/P00-1013.pdf
http://www.aclweb.org/anthology/N07-2038
http://www.aclweb.org/anthology/N07-2038
http://www.aclweb.org/anthology/N07-2038
http://www.aclweb.org/anthology/W/W14/W14-4343.pdf
http://www.aclweb.org/anthology/W/W14/W14-4343.pdf
http://aclweb.org/anthology/P17-4013
http://aclweb.org/anthology/P17-4013
http://aclweb.org/anthology/P17-4013

