



















































Interpreting Neural Networks with Nearest Neighbors


Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 136–144
Brussels, Belgium, November 1, 2018. c©2018 Association for Computational Linguistics

136

Interpreting Neural Networks with Nearest Neighbors

Eric Wallace∗, Shi Feng∗, Jordan Boyd-Graber
University of Maryland

{ewallac2,shifeng,jbg}@umiacs.umd.edu

Abstract

Local model interpretation methods explain
individual predictions by assigning an impor-
tance value to each input feature. This value is
often determined by measuring the change in
confidence when a feature is removed. How-
ever, the confidence of neural networks is not a
robust measure of model uncertainty. This is-
sue makes reliably judging the importance of
the input features difficult. We address this by
changing the test-time behavior of neural net-
works using Deep k-Nearest Neighbors. With-
out harming text classification accuracy, this
algorithm provides a more robust uncertainty
metric which we use to generate feature im-
portance values. The resulting interpretations
better align with human perception than base-
line methods. Finally, we use our interpreta-
tion method to analyze model predictions on
dataset annotation artifacts.

1 Introduction

The growing use of neural networks in sensitive
domains such as medicine, finance, and security
raises concerns about human trust in these ma-
chine learning systems. A central question is test-
time interpretability: how can humans understand
the reasoning behind model predictions?

A common way to interpret neural network
predictions is to identify the most important in-
put features. For instance, a visual saliency map
that highlights important pixels in an image (Sun-
dararajan et al., 2017) or words in a sentence (Li
et al., 2016). Given a model’s test prediction, the
importance of each input feature is the change in
model confidence when that feature is removed.

However, neural network confidence is not a
proper measure of model uncertainty (Guo et al.,
2017). This issue is emphasized when models
make highly confident predictions on inputs that

∗?Equal contribution

are completely void of information, for example,
images of pure noise (Goodfellow et al., 2015)
or meaningless text snippets (Feng et al., 2018).
Consequently, a model’s confidence may not prop-
erly reflect whether discriminative input features
are present. This issue makes it difficult to re-
liably judge the importance of each input fea-
ture using common confidence-based interpreta-
tion methods (Feng et al., 2018).

To address this, we apply Deep k-Nearest
Neighbors (DKNN) (Papernot and McDaniel,
2018) to neural models for text classification.
Concretely, predictions are no longer made with a
softmax classifier, but using the labels of the train-
ing examples whose representations are most sim-
ilar to the test example (Section 3). This provides
an alternative metric for model uncertainty, con-
formity, which measures how much support a test
prediction has by comparing its hidden represen-
tations to the training data. This representation-
based uncertainty measurement can be used in
combination with existing interpretation methods,
such as leave-one-out (Li et al., 2016), to better
identify important input features.

We combine DKNN with CNN and LSTM
models on six NLP text classification tasks, includ-
ing sentiment analysis and textual entailment, with
no loss in classification accuracy (Section 4). We
compare interpretations generated using DKNN
conformity to baseline interpretation methods,
finding DKNN interpretations rarely assign im-
portance to extraneous words that do not align
with human perception (Section 5). Finally, we
generate interpretations using DKNN conformity
for a dataset with known artifacts (SNLI), helping
to indicate whether a model has learned superficial
patterns. We open source the code for DKNN and
our results.1

1https://github.com/Eric-Wallace/deep-knn



137

2 Interpretation Through Feature
Attribution

Feature attribution methods explain a test predic-
tion by assigning an importance value to each in-
put feature (typically pixels or words).

In the case of text classification, we have an in-
put sequence of n words x = 〈w1, w2, . . . wn〉,
represented as one-hot vectors. The word se-
quence is then converted to a sequence of word
embeddings e = 〈v1,v2, . . .vn〉. A classifier
f outputs a probability distribution over classes.
The class with the highest probability is selected
as the prediction y, with its probability serving as
the model confidence. To create an interpretation,
each input word is assigned an importance value,
g(wi | x, y), which indicates the word’s contri-
bution to the prediction. A saliency map (or heat
map) visually highlights words in a sentence.

2.1 Leave-one-out Attribution
A simple way to define the importance g is via
leave-one-out (Li et al., 2016): individually re-
move a word from the input and see how the con-
fidence changes. The importance of word wi is the
decrease in confidence2 when word i is removed:

g(wi | x, y) = f(y | x)− f(y | x−i), (1)

where x−i is the input sequence with the ith word
removed and f(y | x) is the model confidence for
class y. This can be repeated for all words in the
input. Under this definition, the sign of the impor-
tance value is opposite the sign of the confidence
change: if a word’s removal causes a decrease in
the confidence, it gets a positive importance value.
We refer to this interpretation method as Confi-
dence leave-one-out in our experiments.

2.2 Gradient-Based Feature Attribution
In the case of neural networks, the model f(x) as
a function of word wi is a highly non-linear, dif-
ferentiable function. Rather than leaving one word
out at a time, we can simulate a word’s removal by
approximating f with a function that is linear in wi
through the first-order Taylor expansion. The im-
portance of wi is computed as the derivative of f
with respect to the one-hot vector:

∂f

∂wi
=

∂f

∂vi

∂vi
∂wi

=
∂f

∂vi
· vi (2)

2equivalently the change in class score or cross entropy
loss

Thus, a word’s importance is the dot product be-
tween the gradient of the class prediction with re-
spect to the embedding and the word embedding
itself. This gradient approximation simulates the
change in confidence when an input word is re-
moved and has been used in various interpreta-
tion methods for NLP (Arras et al., 2016; Ebrahimi
et al., 2017). We refer to this interpretation ap-
proach as Gradient in our experiments.

2.3 Interpretation Method Failures

Interpreting neural networks can have unexpected
negative results. Ghorbani et al. (2017) and Kin-
dermans et al. (2017) show how a lack of model
robustness and stability can cause egregious in-
terpretation failures in computer vision settings.
Feng et al. (2018) extend this to NLP and draw con-
nections between interpretation failures and adver-
sarial examples (Szegedy et al., 2014). To counter-
act this, new interpretation methods alone are not
enough—models must be improved. For instance,
Feng et al. (2018) argues that interpretation meth-
ods should not rely on prediction confidence as it
does not reflect a model’s uncertainty.

Following this, we improve interpretations by
replacing neural network confidence with a robust
uncertainty estimate using DKNN (Papernot and
McDaniel, 2018). This algorithm achieves compa-
rable accuracy on image classification tasks while
providing a better uncertainty metric capable of
defending against adversarial examples.

3 Deep k-Nearest Neighbors for
Sequential Inputs

This section describes Deep k-Nearest Neighbors,
its application to sequential inputs, and how we
use it to determine word importance values.

3.1 Deep k-Nearest Neighbors

Papernot and McDaniel (2018) propose Deep k-
Nearest Neighbors (DKNN), a modification to the
test-time behavior of neural networks.

After training completes, the DKNN algorithm
passes every training example through the model
and saves each of the layer’s representations. This
creates a new dataset, whose features are the rep-
resentations and whose labels are the model pre-
dictions. Test-time predictions are made by pass-
ing an example through the model and performing
k-nearest neighbors classification on the resulting
representations. This modification does not de-



138

grade the accuracy of image classifiers on several
standard datasets (Papernot and McDaniel, 2018).

For our purposes, the benefit of DKNN is
the algorithm’s uncertainty metric, the conformity
score. This score is the percentage of nearest
neighbors belonging to the predicted class. Con-
formity follows from the framework of conformal
prediction (Shafer and Vovk, 2008) and estimates
how much the training data supports a classifica-
tion decision.

The conformity score is based on the represen-
tations of every layer in the model, and there-
fore, a prediction only receives high conformity
if it largely agrees with neighboring examples at
all representation levels. This mechanism de-
fends against adversarial examples (Szegedy et al.,
2014), as it is difficult to construct a perturbation
which changes the neighbors at every layer. Con-
sequently, conformity is a better uncertainty met-
ric for both regular examples and adversarial ones,
making it suitable for generating interpretations.

3.2 Handling Sequences

The DKNN algorithm requires fixed-size vector
representations. To reach a fixed-size representa-
tion for text classification, we can take the final
hidden state of a recurrent neural network or use
a form of max pooling across time (Collobert and
Weston, 2008). We consider deep architectures of
these two forms, using each of the layers’ repre-
sentations as the features.

3.3 Conformity leave-one-out

Using conformity, we generate interpretations
through a modified version of leave-one-out (Li
et al., 2016). After removing a word, rather than
observing the drop in confidence, we instead mea-
sure the drop in conformity. Formally, we modify
classifier f in Equation 1 to output probabilities
based on conformity scores. We refer to this as
conformity leave-one-out in our experiments.

4 DKNN Maintains Classification
Accuracy

Interpretability should not come at the cost
of performance—before investigating how inter-
pretable DKNN is, we first evaluate its accuracy.
We experiment with six text classification tasks
and two models, verifying that DKNN achieves
accuracy comparable to regular classifiers.

4.1 Datasets and Models

We consider six common text classification tasks:
binary sentiment analysis using Stanford Senti-
ment Treebank (Socher et al., 2013, SST) and Cus-
tomer Reviews (Hu and Liu, 2004, CR), topic clas-
sification using TREC (Li and Roth, 2002), opin-
ion polarity (Wiebe et al., 2005, MPQA), and sub-
jectivity/objectivity (Pang and Lee, 2004, SUBJ).
Additionally, we consider natural language infer-
ence with SNLI (Bowman et al., 2015). We exper-
iment with BILSTM and CNN models.

CNN Our CNN architecture resembles Kim
(2014). We use convolutional filters of size three,
four, and five, with max-pooling over time (Col-
lobert and Weston, 2008). The filters are followed
by three fully-connected layers. We fine-tune
GLOVE embeddings (Pennington et al., 2014) of
each word. For DKNN, we use the activations
from the convolution layer and the three fully-
connected layers.

BILSTM Our architecture uses a bidirectional
LSTM (Graves and Schmidhuber, 2005), with the
final hidden state forming the fixed-size represen-
tation. We use three LSTM layers, followed by
two fully-connected layers. We fine-tune GLOVE
embeddings of each word. For DKNN, we use the
final activations of the three recurrent layers and
the two fully-connected layers.

SNLI Classifier Unlike other tasks with a single
input sentence, SNLI has two inputs, a premise and
hypothesis. Following Conneau et al. (2017), we
use the same model to encode the two inputs, gen-
erating representations u for the premise and v for
the hypothesis. We concatenate the two represen-
tations along with their dot-product and element-
wise absolute difference, arriving at a final repre-
sentation [u; v;u ∗ v; |u− v|]. This vector passes
through two fully-connected layers for classifica-
tion. For DKNN, we use the activations of the two
fully-connected layers.

Nearest Neighbor Search For accurate inter-
pretations, we trade efficiency for accuracy and
replace locally sensitive hashing (Gionis et al.,
1999) used by Papernot and McDaniel (2018) with
a k-d tree (Bentley, 1975). We use k = 75 nearest
neighbors at each layer. The empirical results are
robust to the choice of k.



139

4.2 Classification Results
DKNN achieves comparable accuracy on the five
classification tasks (Table 1). On SNLI, the BIL-
STM achieves an accuracy of 81.2% with a soft-
max classifier and 81.0% with DKNN.

5 DKNN is Interpretable

Following past work (Li et al., 2016; Murdoch
et al., 2018), we focus on the SST dataset for gen-
erating interpretations. Due to the lack of standard
interpretation evaluation metrics (Doshi-Velez and
Kim, 2017), we use qualitative interpretation eval-
uations (Smilkov et al., 2017; Sundararajan et al.,
2017; Li et al., 2016), performing quantitative ex-
periments where possible to examine the distinc-
tion between the interpretation methods.

5.1 Interpretation Analysis
We compare our method (Conformity leave-one-
out) against two baselines: leave-one-out using
regular confidence (Confidence leave-one-out, see
Section 2.1), and the gradient with respect to
the input (Gradient, see Section 2.2). To create
saliency maps, we normalize each word’s impor-
tance by dividing it by the total importance of the
words in the sentence. We display unknown words
in angle brackets <>. Table 2 shows SST interpre-
tation examples for the BILSTM model. Further
examples are on a supplementary website.3

Conformity leave-one-out assigns concentrated
importance values to a small number of input
words. In contrast, the baseline methods assign
non-zero importance values to numerous words,
many of which are irrelevant. For instance, in all
three examples of Table 2, both baselines highlight
almost half of the input, including words such as
“about” and “movie”. We suspect model confi-
dence is oversensitive to these unimportant input
changes, causing the baseline interpretations to
highlight unimportant words. On the other hand,
the conformity score better separates word impor-
tance, generating clearer interpretations.

The tendency for confidence-based approaches
to assign importance to many words holds for the
entire test set. We compute the average number
of highlighted words using a threshold of 0.05 (a
normalized importance value corresponding to a
light blue or light red highlight). Out of the av-
erage 20.23 words in SST test set, gradient high-

3https://sites.google.com/view/
language-dknn/

lights 5.32 words, confidence leave-one-out high-
lights 5.79 words, and conformity leave-one-out
highlights 3.65 words.

The second, and related, observation for
confidence-based approaches is a bias towards se-
lecting word importance based on the inherent
sentiment, rather than a word’s meaning in con-
text. For example, see “clash”, “terribly”, and “un-
faithful” in Table 2. The removal of these words
causes a small change in the model confidence.
When using DKNN, the conformity score indi-
cates that the model’s uncertainty has not risen
without these input words and leave-one-out does
not assign them any importance.

We characterize our interpretation method as
significantly higher precision, but slightly lower
recall than confidence-based methods. Confor-
mity leave-one-out rarely assigns high importance
to words that do not align with human perception
of sentiment. However, there are cases when our
method does not assign significant importance to
any word. This occurs when the input has a high
redundancy. For example, a positive movie re-
view that describes the sentiment in four distinct
ways. In these cases, leaving out a single senti-
ment word has little effect on the conformity as the
model’s representation remains supported by the
other redundant features. Confidence-based inter-
pretations, which interpret models using the linear
units that produce class scores, achieve higher re-
call by responding to every change in the input for
a certain direction but may have lower precision.

In the second example of Table 2, the word “ter-
ribly” is assigned a negative importance value, dis-
regarding its positive meaning in context. To ex-
amine if this is a stand-alone example or a more
general pattern of uninterpretable behavior, we
calculate the importance value of the word “ter-
ribly” in other positive examples. For each occur-
rence of the word “great” in positive validation ex-
amples, we paraphrase it to “awesome”, “wonder-
ful”, or “impressive”, and add the word “terribly”
in front of it. This process yields 66 examples.
For each of these examples, we compute the im-
portance value of each input word and rank them
from most negative to most positive (the most neg-
ative word has a rank of 1). We compare the av-
erage ranking of “terribly” from the three meth-
ods: 7.9 from conformity leave-one-out, 1.68 from
confidence leave-one-out, and 1.1 from gradient.
The baseline methods consistently rank “terribly”

https://sites.google.com/view/language-dknn/
https://sites.google.com/view/language-dknn/


140

SST CR TREC MPQA SUBJ

LSTM 86.7 82.7 91.5 88.9 94.8
LSTM DKNN 86.6 82.5 91.3 88.6 94.9
CNN 85.7 83.3 92.8 89.1 93.5
CNN DKNN 85.8 83.4 92.4 88.7 93.1

Table 1: Replacing a neural network’s softmax classifier with DKNN maintains classification accuracy
on standard text classification tasks.

Method Saliency Map

Conformity an intelligent fiction about learning through cultural clash.
Confidence an intelligent fiction about learning through cultural clash.
Gradient an intelligent fiction about learning through cultural clash.

Conformity <Schweiger> is talented and terribly charismatic.
Confidence <Schweiger> is talented and terribly charismatic.
Gradient <Schweiger> is talented and terribly charismatic.

Conformity Diane Lane shines in unfaithful.
Confidence Diane Lane shines in unfaithful.
Gradient Diane Lane shines in unfaithful.

Color Legend Positive Impact Negative Impact

Table 2: Comparison of interpretation approaches on SST test examples for the LSTM model. Blue
indicates positive impact and red indicates negative impact. Our method (Conformity leave-one-out) has
higher precision, rarely assigning importance to extraneous words such as “about” or “movie”.

as the most negative word, ignoring its meaning in
context. This echoes our suspicion: DKNN gener-
ates interpretations with higher precision because
conformity is robust to irrelevant changes.

5.2 Analyzing Dataset Annotation Artifacts

Through DKNN, we get a new uncertainty mea-
surement, conformity, that measures how a test ex-
ample’s representation is positioned relative to the
training data representations. In this section, we
use conformity leave-one-out to interpret a model
trained on SNLI. This dataset is known to con-
tain annotation artifacts and we demonstrate that
our interpretation method can help identify when
models exploit these dataset biases.

Recent studies (Gururangan et al., 2018; Poliak
et al., 2018) identified annotation artifacts in the
SNLI dataset. These works identified that super-
ficial patterns exist in the input which strongly
correlate with certain labels, making it possible
for models to “game” the task: obtain high ac-
curacy without true understanding. For instance,
the hypothesis of an entailment example is often

a more general paraphrase of the premise, using
words such as “outside” instead of “playing soccer
in a park”. Contradiction examples often contain
negation words or non-action verbs like “sleep-
ing”. Models trained solely on the hypothesis can
learn these patterns to achieve an accuracy consid-
erably higher than the majority baseline.

These studies indicate that the SNLI task can be
gamed. We look to confirm that some artifacts are
indeed exploited by normally trained models that
use full input pairs. We create saliency maps for
examples in the validation set using conformity
leave-one-out. Table 3 shows samples and more
can be found on the supplementary website.4 We
use the blue highlights to indicate words which
positively support the model’s predicted class, and
the color red to indicate words that support a dif-
ferent class. The first example is a randomly sam-
pled baseline, showing how the words “swims”
and “pool” support the model’s prediction of con-
tradiction. The other examples are selected be-

4https://sites.google.com/view/
language-dknn/

https://sites.google.com/view/language-dknn/
https://sites.google.com/view/language-dknn/


141

cause they contain terms identified as artifacts. In
the second example, conformity leave-one-out as-
signs extremely high word importance to “sleep-
ing”, disregarding other words necessary to pre-
dict Contradiction (i.e., the Neutral class is still
possible if “pets” is replaced with “people”). In
the final two hypothesis, the interpretation method
diagnoses the model failure, assigning high impor-
tance to “wearing”, rather than focusing positively
on the shirt color.

To explore this further, we compute the average
importance rank using conformity and confidence
leave-one-out for the top five artifacts in each SNLI
class identified by Gururangan et al. (2018). Ta-
ble 4 compares the average rank assigned by the
two methods, sorting the words by Pointwise Mu-
tual Information as provided by Gururangan et al.
(2018). The word “nobody” particularly stands
out: it is the most important input word every time
it appears in a contradiction example.

For most of the artifacts, conformity leave-one-
out assigns them a high importance, often rank-
ing the artifacts as the most important input word.
Confidence leave-one-out correlates less strongly
with the known artifacts, frequently assigning im-
portance values as low as fifth or sixth most im-
portant. Given the high correlation between con-
formity leave-one-out and the manually identified
artifacts, this interpretation method may serve as
a technique to identify undesirable biases a model
may have learned.

6 Discussion and Related Work

We connect the improvements made by confor-
mity leave-one-out to model confidence issues,
compare alternative interpretation improvements,
and discuss further features of DKNN.

6.1 Issues in Neural Network Confidence

Gradient and leave-one-out both interpret a model
by determining the importance value for each in-
put word. This effectively reduces the problem
of interpretation to one of determining model un-
certainty. Past work relies on model confidence
as a measure of uncertainty. However, a neu-
ral network’s confidence is unreasonably high: on
held-out examples, it far exceeds empirical error
rates (Guo et al., 2017). This is further exempli-
fied by the high confidence predictions produced
on inputs that are adversarial (Szegedy et al.,
2014) or contain solely noise (Goodfellow et al.,

2015). Most importantly for interpretation, the
change in confidence often will not properly re-
flect whether discriminative input features have
been removed (Feng et al., 2018).

6.2 Confidence Calibration is Insufficient
We attribute one interpretation failure to neural
network confidence issues. Guo et al. (2017) study
overconfidence and propose a calibration proce-
dure using Platt scaling. This adds a temperature
to the softmax function to align confidence with
accuracy. However, this is not input dependent.
The confidence is lower for both full-length exam-
ples and ones with words left out. Hence, selecting
influential words will remain difficult.

To verify this, we create an interpretation base-
line using temperature scaling. The results corrob-
orate the intuition: a calibrated leave-one-out does
not fix the interpretation issues. Qualitatively, the
calibrated interpretations are comparable to confi-
dence leave-one-out. Furthermore, calibrating the
DKNN conformity score followingPapernot and
McDaniel (2018) does not improve interpretability
compared to the uncalibrated conformity score.

6.3 Alternative Interpretation Improvements
Recent work improves interpretation methods
through other means. Smilkov et al. (2017) and
Sundararajan et al. (2017) both aggregate gradi-
ent values over multiple backpropagation passes to
eliminate local noise or satisfy interpretation ax-
ioms. This work does not address model confi-
dence and is orthogonal to our DKNN approach.

6.4 Interpretation Through Data Selection
Retrieval-Augmented Neural Networks (Zhao and
Cho, 2018) are similar to DKNN: they augment
model predictions with an information retrieval
system that searches over network activations
from the training data.

Retrieval-Augmented models and DKNN can
both select influential training examples for a test
prediction. In particular, the training data activa-
tions which are closest to the test point’s activa-
tions are influential according to the model. These
training examples can provide interpretations as
a form of analogy (Caruana et al., 1999), an in-
tuitive explanation for both machine learning ex-
perts and non-experts (Klein, 1989; Kim et al.,
2014; Koh and Liang, 2017; Wallace and Boyd-
Graber, 2018). However, unlike in computer vi-
sion where training data selection using DKNN



142

Prediction Input Saliency Map

Contradiction
Premise a young boy reaches for and touches the propeller of a vintage

aircraft.
Hypothesis a young boy swims in his pool.

Entailment
Premise a brown a dog and a black dog in the edge of the ocean with a

wave under them boats are on the water in the background.
Hypothesis the pets are sleeping on the grass..

Premise man in a blue shirt standing in front of a structure painted with
geometric designs.

Entailment Hypothesis a man is wearing a blue shirt.
Entailment Hypothesis a man is wearing a black shirt.

Color Legend Positive Impact Negative Impact

Table 3: Interpretations generated with conformity leave-one-out align with annotation biases identified
in SNLI. In the second example, the model puts emphasis on the word “sleeping”, disregarding other
words that could indicate the Neutral class. The final example diagnoses a model’s incorrect Entailment
prediction (shown in red). Green highlights indicate words that support the classification decision made
(shown in parenthesis), pink highlights indicate words that support a different class.

Label Artifact Conformity Confidence

Entailment

outdoors 2.93 3.26
least 2.22 4.41
instrument 3.57 4.47
outside 4.08 4.80
animal 2.00 4.73

Neutral

tall 1.09 2.61
first 2.14 2.99
competition 2.33 5.56
sad 1.39 1.79
favorite 1.69 3.89

Contradiction

nobody 1.00 1.00
sleeping 1.64 2.34
no 2.53 5.74
tv 1.92 3.74
cat 1.42 3.62

Table 4: The top SNLI artifacts identified by Guru-
rangan et al. (2018) are shown on the left. For each
word, we compute the average importance rank
over the validation set using either Conformity or
Confidence leave-one-out. A score of 1.0 indicates
that a word is always ranked as the most important
word in the input. Conformity leave-one-out as-
signs stronger importance to artifacts, suggesting
it better diagnoses model biases.

yielded interpretable examples (Papernot and Mc-
Daniel, 2018), our experiments did not find human
interpretable data points for SST or SNLI.

6.5 Trust in Model Predictions
Model confidence is important for real-world ap-
plications: it signals how much one should trust
a neural network’s predictions. Unfortunately,
users may be misled when a model outputs highly
confident predictions on rubbish examples (Good-
fellow et al., 2015; Nguyen et al., 2015) or ad-
versarial examples (Szegedy et al., 2014). Re-
cent work decides when to trust a neural network
model (Ribeiro et al., 2016; Doshi-Velez and Kim,
2017; Jiang et al., 2018). For instance, analyzing
local linear model approximations (Ribeiro et al.,
2016) or flagging rare network activations us-
ing kernel density estimation (Jiang et al., 2018).
The DKNN conformity score is a trust metric
that helps defend against image adversarial exam-
ples (Papernot and McDaniel, 2018). Future work
should study if this robustness extends to interpre-
tations.

7 Future Work and Conclusion

A robust model uncertainty estimate is critical to
determine feature importance. The DKNN confor-
mity score is one such uncertainty metric which
leads to higher precision interpretations. Al-
though DKNN is only a test-time improvement—



143

the model is still trained with maximum likeli-
hood. Combining nearest neighbor and maxi-
mum likelihood objectives during training may
further improve model accuracy and interpretabil-
ity. Moreover, other uncertainty estimators do
not require test-time modifications. For example,
modeling p(x) and p(y | x) using Bayesian Neu-
ral Networks (Gal et al., 2016).

Similar to other NLP interpretation meth-
ods (Sundararajan et al., 2017; Li et al., 2016),
conformity leave-one-out works when a model’s
representation is fixed-sized. For other NLP tasks,
such as structured prediction (e.g., translation and
parsing) or span prediction (e.g., extractive sum-
marization and reading comprehension), models
output a variable number of predictions and our in-
terpretation approach will not suffice. Developing
interpretation techniques for these types of models
is a necessary area for future work.

We apply DKNN to neural models for text
classification. This provides a better estimate of
model uncertainty—conformity—which we com-
bine with leave-one-out. This overcomes issues
stemming from neural network confidence, lead-
ing to higher precision interpretations. Most inter-
estingly, our interpretations are supported by the
training data, providing insights into the represen-
tations learned by a model.

Acknowledgments

Feng was supported under subcontract to
Raytheon BBN Technologies by DARPA award
HR0011-15-C-0113. JBG is supported by NSF
Grant IIS1652666. Any opinions, findings,
conclusions, or recommendations expressed here
are those of the authors and do not necessarily
reflect the view of the sponsor. The authors would
like to thank the members of the CLIP lab at
the University of Maryland and the anonymous
reviewers for their feedback.

References

Leila Arras, Franziska Horn, Grégoire Montavon,
Klaus-Robert Müller, and Wojciech Samek. 2016.
Explaining predictions of non-linear classifiers in
NLP. In Workshop on Representation Learning for
NLP.

Jon Louis Bentley. 1975. Multidimensional binary
search trees used for associative searching. Com-
munications of the ACM .

Samuel R. Bowman, Gabor Angeli, Christopher Potts,
and Christopher D. Manning. 2015. A large anno-
tated corpus for learning natural language inference.
In EMNLP.

Rich Caruana, Hooshang Kangarloo, John David N.
Dionisio, Usha S. Sinha, and David B. Johnson.
1999. Case-based explanation of non-case-based
learning methods. Proceedings of AMIA Symposium
.

Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In ICML.

Alexis Conneau, Douwe Kiela, Holger Schwenk, Loic
Barrault, and Antoine Bordes. 2017. Supervised
learning of universal sentence representations from
natural language inference data. In EMNLP.

Finale Doshi-Velez and Been Kim. 2017. Towards a
rigorous science of interpretable machine learning.
arXiv preprint arXiv: 1702.08608 .

Javid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing
Dou. 2017. HotFlip: White-box adversarial exam-
ples for text classification. In ACL.

Shi Feng, Eric Wallace, Alvin Grissom II, Mohit Iyyer,
Pedro Rodriguez, and Jordan Boyd-Graber. 2018.
Pathologies of neural models make interpretations
difficult. In EMNLP.

Yarin Gal, Yutian Chen, Roger Frigola, S. Gu, Alex
Kendall, Yingzhen Li, Rowan McAllister, Carl Ras-
mussen, Ilya Sutskever, Gabriel Synnaeve, Nilesh
Tripuraneni, Richard Turner, Oriol Vinyals, Adrian
Weller, Mark van der Wilk, and Yan Wu. 2016. Un-
certainty in Deep Learning. Ph.D. thesis, University
of Oxford.

Amirata Ghorbani, Abubakar Abid, and James Y. Zou.
2017. Interpretation of neural networks is fragile.
arXiv preprint arXiv: 1710.10547 .

Aristides Gionis, Piotr Indyk, and Rajeev Motwani.
1999. Similarity search in high dimensions via
hashing. In VLDB.

Ian J. Goodfellow, Jonathon Shlens, and Christian
Szegedy. 2015. Explaining and harnessing adver-
sarial examples. In ICLR.

Alex Graves and Jürgen Schmidhuber. 2005. Frame-
wise phoneme classification with bidirectional
LSTM and other neural network architectures. Neu-
ral networks : the official journal of the Interna-
tional Neural Network Society .

Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Wein-
berger. 2017. On calibration of modern neural net-
works. In ICML.

Suchin Gururangan, Swabha Swayamdipta, Omer
Levy, Roy Schwartz, Samuel R. Bowman, and
Noah A. Smith. 2018. Annotation artifacts in nat-
ural language inference data. In NAACL.



144

Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In KDD.

Heinrich Jiang, Been Kim, and Maya R. Gupta. 2018.
To trust or not to trust a classifier. arXiv preprint
arXiv: 1805.11783 .

Been Kim, Cynthia Rudin, and Julie A. Shah. 2014.
The bayesian case model: A generative approach for
casebased reasoning and prototype classification. In
NIPS.

Yoon Kim. 2014. Convolutional neural networks for
sentence classification. EMNLP .

Pieter-Jan Kindermans, Sara Hooker, Julius Ade-
bayo, Maximilian Alber, Kristof T. Schütt, Sven
Dähne, Dumitru Erhan, and Been Kim. 2017. The
(un)reliability of saliency methods. arXiv preprint
arXiv: 1711.00867 .

Gary A. Klein. 1989. Do decision biases explain too
much. In Proceedings of the Human Factors and
Ergonomics Society.

Pang Wei Koh and Percy Liang. 2017. Understand-
ing black-box predictions via influence functions. In
ICML.

Jiwei Li, Will Monroe, and Daniel Jurafsky. 2016. Un-
derstanding neural networks through representation
erasure. arXiv preprint arXiv: 1612.08220 .

Xin Li and Dan Roth. 2002. Learning question classi-
fiers. In COLT .

W. James Murdoch, Peter J. Liu, and Bin Yu. 2018. Be-
yond word importance: Contextual decomposition
to extract interactions from lstms. In ICLR.

Anh Mai Nguyen, Jason Yosinski, and Jeff Clune.
2015. Deep neural networks are easily fooled: High
confidence predictions for unrecognizable images.
In CVPR.

Bo Pang and Lillian Lee. 2004. A sentimental educa-
tion: Sentiment analysis using subjectivity summa-
rization based on minimum cuts. In ACL.

Nicolas Papernot and Patrick D. McDaniel. 2018.
Deep k-nearest neighbors: Towards confident, inter-
pretable and robust deep learning. arXiv preprint
arXiv: 1803.04765 .

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. GloVe: Global vectors for
word representation. In EMNLP.

Adam Poliak, Jason Naradowsky, Aparajita Haldar,
Rachel Rudinger, and Benjamin Van Durme. 2018.
Hypothesis only baselines in natural language infer-
ence. In *SEM@NAACL-HLT .

Marco Túlio Ribeiro, Sameer Singh, and Carlos
Guestrin. 2016. Why should i trust you?”: Explain-
ing the predictions of any classifier. In KDD.

Glenn Shafer and Vladimir Vovk. 2008. A tutorial on
conformal prediction. JMLR .

Daniel Smilkov, Nikhil Thorat, Been Kim, Fernanda B.
Viégas, and Martin Wattenberg. 2017. SmoothGrad:
removing noise by adding noise. arXiv preprint
arXiv: 1706.03825 .

Richard Socher, A. V. Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. 2013. Recursive deep models
for semantic compositionality over a sentiment tree-
bank. In EMNLP.

Mukund Sundararajan, Ankur Taly, and Qiqi Yan.
2017. Axiomatic attribution for deep networks. In
ICML.

Christian Szegedy, Wojciech Zaremba, Ilya Sutskever,
Joan Bruna, Dumitru Erhan, Ian J. Goodfellow, and
Rob Fergus. 2014. Intriguing properties of neural
networks. In ICLR.

Eric Wallace and Jordan Boyd-Graber. 2018. Trick me
if you can: Adversarial writing of trivia challenge
questions. In Proceedings of ACL 2018 Student Re-
search Workshop.

Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and emo-
tions in language. In LREC.

Jake Zhao and Kyunghyun Cho. 2018. Retrieval-
augmented convolutional neural networks for im-
proved robustness against adversarial examples.
arXiv preprint arXiv: 1802.09502 .


