



















































YouMakeup: A Large-Scale Domain-Specific Multimodal Dataset for Fine-Grained Semantic Comprehension


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 5133–5143,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

5133

YouMakeup: A Large-Scale Domain-Specific Multimodal Dataset for
Fine-Grained Semantic Comprehension

Weiying Wang Yongcheng Wang Shizhe Chen Qin Jin ∗
School of Information, Renmin Unversity of China

{wy.wang, 2015202012, cszhe1, qjin}@ruc.edu.cn

Abstract

Multimodal semantic comprehension has at-
tracted increasing research interests in recent
years, such as visual question answering and
caption generation. However, due to the data
limitation, fine-grained semantic comprehen-
sion which requires to capture semantic de-
tails of multimodal contents has not been
well investigated. In this work, we intro-
duce “YouMakeup”, a large-scale multimodal
instructional video dataset to support fine-
grained semantic comprehension research in
specific domain. YouMakeup contains 2,800
videos from YouTube, spanning more than 420
hours in total. Each video is annotated with
a sequence of natural language descriptions
for instructional steps, grounded in temporal
video range and spatial facial areas. The anno-
tated steps in a video involve subtle difference
in actions, products and regions, which require
fine-grained understanding and reasoning both
temporally and spatially. In order to evaluate
models’ ability for fined-grained comprehen-
sion, we further propose two groups of tasks
including generation tasks and visual question
answering tasks from different aspects. We
also establish a baseline of step caption gen-
eration for future comparison. The dataset
will be publicly available at https://
github.com/AIM3-RUC/YouMakeup to
support research investigation in fine-grained
semantic comprehension.

1 Introduction

Videos which naturally contain rich multimodal
semantic information have been one of the main
sources for knowledge acquisition. In recent
years, video semantic comprehension has attracted
much research attention, with a number of datasets
and tasks being proposed such as activity recog-
nition (Xu et al., 2017), dense video captioning

∗ Corresponding author.

(Krishna et al., 2017a), visual question answering
(Lei et al., 2018, 2019) etc. However, most works
are limited to only capturing coarse semantic in-
formation such as action recognition in broad cat-
egories. Fine-grained comprehension instead has
not been fully explored, especially for discriminat-
ing actions with subtle difference or understanding
temporal relations of actions in a certain activity.

Instructional videos, which contain series of
steps to accomplish certain tasks, are suitable
sources to investigate fine-grained semantic com-
prehension and reasoning. As shown in Table 1,
different instructional video datasets have been
released. However, current datasets suffer from
small data scales or coarse annotations to support
fine-grained analysis. For example, datasets col-
lected in (Alayrac et al., 2016; Rohrbach et al.,
2012; Stein and McKenna, 2013; Kuehne et al.,
2014; Das et al., 2013) only contain hundreds
or fewer videos and actions. Although COIN
dataset (Tang et al., 2019) is in large scale, it
aims to cover wide range of action categories in-
stead of distinguishing actions with subtle differ-
ence, and also lacks fine-grained step annotations.
The YouCook2 dataset (Zhou et al., 2018) con-
tains fairly large number of videos and temporal
grounded sentences in the cooking domain. How-
ever, since different cooking steps contain appar-
ent visual variation in actions, food and kitchen
utilities, it might not require fine-grained reason-
ing over temporal and spatial dimensions to iden-
tify different steps.

In order to overcome previous limitations, we
collect a new instructional video dataset named
“YouMakeup” in specific makeup domain for fine-
grained multimodal semantic comprehension. The
advantages of opting for the makeup domain are
threefolds. Firstly, makeup instructional videos
are more fine-grained in nature because different
steps share the same facial background but con-

https://github.com/AIM3-RUC/YouMakeup
https://github.com/AIM3-RUC/YouMakeup


5134

Dataset # ofvideos
Total
len(h)

Avg
len(m)

Step Annotation Domain Source# of steps Type T.G S.G
“5task” (Alayrac et al., 2016) 150 5 2 - Sent. X - General YouTube
COIN (Tang et al., 2019) 11,827 476.5 2.6 46,354 Ctg.(778) X - General YouTube
MPII (Rohrbach et al., 2012) 44 8 10.9 5,609 Ctg.(65) X X Cooking recorded
YouCook (Das et al., 2013) 88 2.5 1.6 - - - - Cooking YouTube
50 Salads (Stein and McKenna, 2013) 50 4.5 6.4 966 Ctg.(17) X - Cooking recorded
Breakfast (Kuehne et al., 2014) 1,989 77 2.3 8,456 Ctg.(48) X - Cooking recorded
Ikea FA (Toyer et al., 2017) 101 4 2.3 1,911 Ctg.(-) X X Furniture recorded
YouCook2 (Zhou et al., 2018) 2,000 176 5.3 3,829 Sent. X - Cooking YouTube
EPIC-KITCHENs (Damen et al., 2018) 432 55 7.6 39,596 Sent. X X Cooking recorded
YouMakeup (ours) 2,800 421 9 30,626 Sent. X X Makeup YouTube

Table 1: Comparison of different instructional video datasets. Ctg. = pre-defined categories; Sent. = sentence;
T.G = temporal grounding; S.G = spatial grounding; ‘recorded’ means videos are self-recorded by collectors. The
YouMakeup dataset is unique in large-scale data size and fine-grained annotations.

tain at least one subtle but critical difference in ac-
tion, tool or facial area. Therefore, it requires fine-
grained discrimination within temporal and spa-
tial context. Secondly, there are abundant makeup
instructional videos on the Internet with manual
commentary or scripts, which makes it easy to
collect and annotate. Last but not least, makeup
video analysis is of great value which can facilitate
both editing and searching process for cosmetic
companies and users. The collected YouMakeup
dataset consists of 2,800 makeup videos crawled
from YouTube, which spans more than 420 hours.
As shown in Figure 1, we manually annotate a se-
quence of natural language sentences to describe
different instructional steps for each video and
each step is grounded both in temporal video seg-
ment and spatial face areas in fine-grained details.
There are totally 30,626 steps with 10.9 steps on
average for each video, indicating the complexity
of makeup activities.

For the purpose of comprehensively evaluat-
ing fine-grained analysis, we propose two groups
of potential semantic comprehension tasks on
YouMakeup: Generation and Question Answering
(QA) tasks. The Generation tasks include tempo-
ral step segmentation, step caption generation and
spatial area grounding, which reflects an overall
semantic comprehension performance. In order
to further measure video semantic reasoning abil-
ity, we design four QA tasks for detailed evalua-
tions from four aspects as illustrated in Figure 8.
The Facial Image Ordering task aims to track sub-
tle changes on facial appearance after each step,
which requires to reason influences of actions on
object states. The Step Ordering task is to sort
step descriptions according to their temporal order
in the video, requiring temporal action reasoning
and visual semantic matching. The Time Range

Selection task requires the precise temporal local-
ization of specific step in the video, forcing mod-
els to distinguish fine-grained difference between
makeup steps. The Theme Inference task aims to
select a best theme for the video, which demands
high-level summarization of video content.

The main contributions of this work are three-
folds: 1) We introduce a large-scale fine-
grained instructional video dataset “YouMakeup”
in makeup domain to support research on fine-
grained multimodal semantic comprehension. To
the best of our knowledge, it is the largest instruc-
tional video dataset in specific domain with fine-
grained temporal and spatial grounded annotation.
2) We propose two groups of tasks to evaluate fine-
grained video comprehension abilities, including
generation tasks and four QA tasks, which require
fine-grained semantic understanding and reason-
ing in different aspects and levels. 3) We propose
a baseline framework for the step caption genera-
tion task to demonstrate that the fine-grained anal-
ysis and long temporal dependencies are essential
for multimodal semantic comprehension.

2 Related Work

2.1 Instructional Video Datasets
Existing instructional video datasets can be di-
vided into two groups according to the domain
diversity as summarized in Table 1. The first
group aims to involve diverse activities from dif-
ferent domains. (Alayrac et al., 2016) contains
5 tasks such as “Making a coffee” and “Chang-
ing car tire”. COIN (Tang et al., 2019) is a large
scale dataset which contains videos of 180 dif-
ferent tasks in 12 domains related to our daily
life. These datasets are constructed to improve
model’s generalization ability rather than support
the fine-grained semantic comprehension. The



5135

Figure 1: An example video in YouMakeup dataset. We annotate a sequence of step descriptions grounded in
temporal video range and spatial face areas for each video. Best viewed in color.

other group focuses on specific domain, such as
furniture assembling and cooking. (Rohrbach
et al., 2012; Stein and McKenna, 2013; Kuehne
et al., 2014) contain videos about simple cooking
activities. YouCook (Das et al., 2013) consists of
88 videos with long text summarization. These
datasets are limited in both number of videos and
actions. YouCook2 (Zhou et al., 2018) is rela-
tively large with 2000 videos, spanning 176 hours.
Though cooking events are of rich semantics con-
taining various foods, kitchen utilities and actions
(Nishimura et al., 2019; Hahn et al., 2018), such
variety makes it hard to measure fine-grained com-
prehension ability of models. For example, ob-
jects in “Sprinkle salt and pepper to the taste”
and “Place the bacon at the top” are very differ-
ent so that it might not be necessary to understand
the whole details of actions to distinguish the two
steps.

The strengths of our YouMakeup dataset com-
pared with previous works are in two aspects: (1)
it is large in scale with 420 hours in total. To the
best of our knowledge, it is the largest instruc-
tional dataset in specific domain with rich fine-
grained annotations. (2) Facial makeup is suitable
for fine-grained comprehension in nature for all
activities occur on the local facial area with sub-
tle differences.

2.2 Video Comprehension Tasks
A wide range of tasks have been proposed for se-
mantic comprehension on videos, such as action
detection, dense video captioning and video ques-
tion answering etc. The general video captioning

task requires to generate a single sentence for the
whole video, which cannot describe video con-
tent in details especially for long videos. So in
the dense video captioning task, the model needs
to detect meaningful events in the video and gen-
erate sentence to describe each one. Comparing
to the dense captioning tasks which focus on ac-
tivities with very different actions such as Activ-
ityNet challenge (Krishna et al., 2017a), makeup
instructional videos are more fine-grained which
contain actions with subtle difference, providing
more challenges for semantic comprehension.

Question answering is another way to effec-
tively evaluate semantic understanding. Apart
from image based QA datasets such as (Mali-
nowski and Fritz, 2014; Antol et al., 2015; Ren
et al., 2015a; Johnson et al., 2017), several video
based datasets have been released to explore spa-
tial and temporal inference of the video content.
However, they mainly focus on comprehension
within short video clips which contain simple
activities and interactions, such as (Jang et al.,
2017; Kim et al., 2016; Tapaswi et al., 2016; Ma-
haraj et al., 2017), etc. The TVQA dataset (Lei
et al., 2018, 2019) is constructed from complex
TV shows, but each question is associated with a
short clip up to 90 seconds and more focused on
joint understanding of visual and speech content.
In comparison, our proposed four QA tasks on
YouMakeup dataset are used to evaluate video se-
mantic reasoning abilities from different aspects,
such as spatial and temporal understanding for
long videos, causality reasoning of actions and



5136

Figure 2: Annotation website

global semantic summarization.

3 YouMakeup Dataset

3.1 Data Collection
Our goal is to build a large-scale multimodal in-
structional video dataset in the makeup domain to
support the fine-grained semantic comprehension
research. We start from collecting a list of fa-
mous cosmetic brands, such as Chanel, Mac, etc.,
and beauty bloggers with more than tens of thou-
sands followers. These companies and bloggers
are authoritative and professional in the makeup
domain with many people learning makeup skills
from them. Videos in their official channels are of
high quality. Based on the list, we search their of-
ficial channels on the YouTube and crawl makeup
instructional videos together with available meta
data such as video id, duration, title, tags and
English subtitles generated by YouTube automati-
cally. We process the subtitles into complete sen-
tences aligned with video time stamps.

Since specific names of cosmetic brands are not
important to understand makeup procedures, we
filter out them in raw titles and subtitles of videos.
We first create an initial list of cosmetic brands
and then refine it gradually by checking words fre-
quency and finding similar words via word embed-
ding model (Tomas Mikolov, 2013). Finally, we
utilize the refined list to remove cosmetic brands
in raw texts. We also create lists for facial areas
and cosmetic products through the similar process.

3.1.1 Step Annotation
We build an annotation system for step annotation.
Figure 2 shows the interface of our annotation sys-
tem. We provide video and English subtitles in the
annotation page to assist the annotation. The prod-
ucts and facial areas in the subtitle are emphasized

in red color to help annotators focus on related in-
formation. Annotators are asked to segment the
video into a series of steps, which includes label-
ing the start and end time of each step, selecting
the related facial areas in the given facial area list
and creating the caption to describe the step ac-
cording to the video and subtitles. We recruit fe-
male college students with more than two years of
makeup experience as annotators. Before starting
the annotation, each annotator is asked to annotate
a test video to verify their capability for annota-
tion. During annotation, each video is annotated
by one person and reviewed by another to ensure
the annotation quality.

3.1.2 Facial Image Annotation
We extract two groups of facial images from the
video. The first group is used to understand the
effect of makeup activities on facial appearance,
which supports our proposed QA task described
in Section 4.2. We extract images at the begin-
ning and the end of each step to capture the fa-
cial appearance before and after each step. In
order to select images containing faces, we ex-
tract images in 40 frames around each time stamp
and filter them with a pretrained Multi-Task CNN
(MTCNN) (Zhang et al., 2016) for face detec-
tion. We then manually filter out unsuitable im-
ages such as side face images and pure product
images.

The second group is to localize all facial areas
referred in each step for spatial grounding. We
extract key frames within the annotated segment
of each step via comparing similarity of different
frames followed by manual filtering of redundant
frames. Then we automatically detect facial land-
marks in the facial image and align the image re-
gion to corresponding facial areas of each step an-
notated in Section 3.1.1. Finally, we ask annota-
tors to adjust the bounding box of these facial ar-
eas on the images and obtain the final grounded
facial areas for each step.

3.1.3 Theme Annotation
Inferring the theme of a video is a basic ability
to understand the video content. Therefore, we
annotate the theme for each makeup video. The
original title of the video usually summarizes the
content or highlights specific features in the video,
which can be treated as the theme of the video.
As mentioned above, the specific cosmetic brand
names are removed in the title for generalization.



5137

Figure 3: Different categories of the makeup instruc-
tional video

We further ask annotators to refine the title with
the help of related meta information to make the
theme more accurate.

3.2 Dataset Statistics

The final YouMakeup dataset contains 2800
videos, spanning 420 hours 50 minutes, and rich
annotations. Videos can be mainly divided into
three categories as shown in Figure 3: 1) Makeup
for special occasions, such as school days or wed-
ding days; 2) Makeup tips for specific facial area
or cosmetic products, such as eye makeup or wear-
ing red lipstick; 3) Makeup transformation, such
as celebrities transformation. The first and third
types usually create full looks, while the second
focuses on specific step or facial area. We split
the dataset into training, testing and validation set
by 70%: 20%:10% and set up all the tasks on this
division.

(a) Video duration. (b) Step number.

Figure 4: Statistics of video duration and step number.

3.2.1 Video Length
Different from previous datasets containing videos
with similar duration (Lei et al., 2018; Tang et al.,
2019), videos in YouMakeup dataset are of vari-
ous lengths, which reflects the complex situation
in the realistic world. As shown in Figure 4(a),
the length of videos varies from 15s to 1h with

9min on average. The large diversity of length re-
sults from the diverse video categories and styles.
For example, tutorials from companies are usually
short and come straight to the point, while those
from beauty bloggers are more complex for they
may show makeup skills or share their opinions
about products in details.

3.2.2 Makeup Steps
There are 30,626 annotated steps in total with av-
erage 10.9 steps per video in YouMakeup. Fig-
ure 4(b) shows the distribution of step number.
Compared with instructional video datasets in
general domains, YouMakeup is more complex
with more steps on average and therefore bringing
more challenge for semantic understanding.

Each step associates with at least one and up
to seven facial areas. The frequency of grounded
facial areas are presented in Figure 5. All these
areas are close to each other on the face and might
contain overlaps for some of them. For example,
brow and brow bone are closely adjacent to each
other and the under-eye area overlaps with cheeks.
Therefore, fine-grained understanding is required
to distinguish these areas.

Figure 5: Frequency of grounded facial areas in
YouMakeup dataset.

There are more than 1500 unique words oc-
curred in the step captions. Figure 6(a) shows
the Wordcloud for most frequent 100 words ex-
cluding stopwords. The most frequent words in-
clude actions, products, facial areas and tools as
summarized in Table 2. These four categories of
words can be combined in various ways, generat-
ing large number of different fine-grained makeup
activities. For example, Figure 7 illustrates the
fine-grained activities for the action “apply”. The
graph shows that the number of activities is large.
Though the activities are similar, they are distinct
in actions, products, facial areas or other aspects.
Therefore, fine-grained comprehension is required
for telling such subtle differences.



5138

(a) Step caption. (b) Theme.

Figure 6: Wordcloud for different text annotations.

Type Terms
Actions apply, use, blend, highlight, draw, curler, fix, clean, pat, shape, empha-

size, press, sweep, correct, bake, tap, contour
Products eyeshadow, concealer, powder, pencil, foundation, lipstick, eyeliner,

blush, primer, shadow, highlighter, contour, bronzer, gel, cream, lip-
gloss, false lashes, blam, spray

Areas face, lips, eyelid, eye, lashes, brow, cheek, nose, under-eye, cheekbone,
lash line, forehead, chin, mouth, blemish, temple, jaw, hairline, jawline,
t-zone, nosebone, browbone, eye corner, lash line, philtrum, eyeball

Tools brush, blender, sponge, finger, puff, tweezer

Table 2: Top words of four categories in step captions.

Figure 7: Fine-grained activity graph stemmed from
action ‘Apply’.

4 Semantic Comprehension Tasks

For the purpose of exploring fine-grained seman-
tic comprehension in different aspects and levels,
we propose two groups of tasks namely genera-
tion and question answering, which contain 5 de-
tailed tasks according to the instructional and mul-
timodal characteristics of YouMakeup.

4.1 Task I: Step Generation
The step generation task includes identifying the
temporal boundaries for steps, grounding the fa-
cial areas and generating natural language step de-
scription for each step as shown in Figure 1. It is
a classical task in instructional video analysis for
evaluating model’s ability to capture the tempo-
ral flow of steps and grasp the procedural knowl-
edge. Due to the similarity between makeup ac-
tivities such as the similar motion, appearance of
products and the adjacent location of facial ar-
eas, this task calls for a fine-grained comprehen-
sion of video content. The task is closely related

to dense video captioning (Krishna et al., 2017a),
however, it needs to integrate makeup procedural
knowledge to generate fine-grained step segmen-
tation and step description instead of independent
event descriptions.

4.2 Task II: Facial Image Ordering
The facial image ordering task is to sort a series of
shuffled facial images into the right order accord-
ing to the ordered step captions, as shown in Fig-
ure 8(a). Instructional videos present steps for ac-
complishing a certain task. Tracking the changes
of object is crucial for procedure comprehension.
The effect of makeup is the fine-grained changes
of facial appearances. Some steps bring appar-
ent changes, for example, “Apply red lipsticks on
the lips” turns the lip color from nude to bold red.
However, some changes can be subtle and difficult
to identify, such as “Apply foundation on the face
with brush”, which may result in subtle changes of
the skin tone. Under most circumstances, the re-
sult brought by each step is very complex, which
not only depends on the current step but also relies
on the prior state of the facial appearance. Track-
ing changes on the face is an effective way to eval-
uate models’ fine-grained comprehension ability.

We choose five facial images of different steps
from a video randomly to form a question. Then
we set their original order as the ground truth and
the other three random sorts as the candidate an-
swers. We finally generate 12,000 questions, in-
cluding 8,400 for training, 1,200 for validation,
and 2,400 for testing.

4.3 Task III: Step Ordering
The step ordering task evaluates the capability of
sorting a series of step captions into the right order
according to the video. For human beings, com-
prehension is the prerequisite of ordering. The
step ordering task aims at developing model’s
comprehension ability in a multimodal scenario,
which calls for a joint understanding across differ-
ent modalities. Models need to align natural lan-
guage step captions with video content to solve the
problem as shown in Figure 8(b), fine-grained un-
derstanding in both text and video content for tem-
poral action reasoning and visual semantic match-
ing is required.

We select videos with more than four steps to
generate questions. For each question, we pro-
vide five captions of different steps from the video
and four different order choices as the candidate



5139

Figure 8: Four VQA tasks on YouMakeup Dataset. Best viewed in color.

answers, including the original one as the ground
truth. We finally collect 12,000 questions for this
task, including 8,400 for training, 1,200 for vali-
dation, and 2,400 for testing.

4.4 Task IV: Time Range Selection

In the time range selection task, models are re-
quired to localize a specific step in the video accu-
rately. We set the task in QA form where both the
video, the step caption and four candidate answers
are provided. As the case shown in Figure 8(c),
given the caption, the model needs to select the
most accurate time range from four candidate an-
swers after reviewing the whole video. To accom-
plish this task, models need to jointly understand
the step caption and the video, distinguish the fine-
grained difference of the makeup activities in each
video clips to find the best answer.

To form the question, we first select one step
from the video, provide its step caption as ques-
tion and set its time range as ground truth. Then
we choose the other three steps which have some
overlap in facial area with the question step from
the same video, and set their time range as the can-
didate answers. We finally collect 12,000 ques-
tions for this task, including 8,400 for training,
1,200 for validation and 2,400 for testing.

4.5 Task V: Theme Inference

The theme inference task requires to inference the
theme of a video. People can compress compli-
cated content and extract key information. Several
tasks have been set up to help the model develop
such ability, such as the video summarization task
which summarizes the video in several sentences.
Theme inference is another way to evaluate this
kind of ability. Since a video can be summarized
with focus on multiple aspects, it is difficult to
treat theme inference as generation task. We set
it in QA form based on the annotation in Sec-
tion 3.1.3.

As shown in Figure 8(d), we provide four candi-
date answers including the ground-truth. To gen-
erate candidate answers, we set up the candidate
answer set with titles and tags of all videos. Then
we utilize the FastText (Bojanowski et al., 2016),
Doc2Vec (Le and Mikolov, 2014) and Word2Vec
(Tomas Mikolov, 2013) together for theme feature
representation to search the top 10 nearest themes
for each video. These themes are then selected
by the annotator to form the candidate answers.
Given the makeup videos, the model needs to se-
lect the most suitable theme from four choices.

Different from the video classification task
which divides videos into fixed pre-defined cate-
gories, the candidate answers in the theme infer-
ence task are natural language sentences different
from each other. As shown in Figure 6(b), the



5140

themes involve diverse aspects, such as “smokey”
and “natural” for makeup style, “night” and “hal-
loween” for occasions, and “pink” and “red” for
color tone. Thus, the theme inference task requires
a fine-grained comprehensive understanding of the
video content in various aspects.

5 Experiment

We build a step caption generation system to pro-
vide a baseline for our YouMakeup dataset and
demonstrate the necessity of fine-grained spatial
and temporal understanding to solve the task. The
system utilizes the groundtruth step segmentation
in order to evaluate the captioning ability alone,
and is evaluated by the standard captioning metrics
including BLEU (Papineni et al., 2002), METEOR
(Denkowski and Lavie, 2014), ROUGE (Flick,
2004), CIDEr (Vedantam et al., 2015) and SPICE
(Anderson et al., 2016).

Implementation Details Our step caption gen-
eration model is based on the encoder-decoder
captioning framework, which is widely used in
video caption generation (Vinyals et al., 2015;
Chen et al., 2019). The encoder converts the video
clip into a fixed-dimensional vector and then the
decoder generates word sequences conditioning
on the encoded vector. Since both spatial and
temporal reasoning are important for the task, we
propose two types of encoder as follows: 1) spa-
tial encoder: faster RCNN (Ren et al., 2015b)
pretrained on the VisualGenome dataset (Krishna
et al., 2017b) is used to extract object features in a
single frame. We select one frame for each video
clip and detect at most 36 objects in the frame.
Mean pooling is applied on the extracted object
features to generate the video-level representation.
2) temporal encoder: Resnet152 (He et al., 2016)
pretrained on the ImageNet dataset (Deng et al.,
2009) is used to extract features for each frame.
We extract global frame-level features for every
16 frames and apply mean pooling on the temporal
dimension to generate the global video-level rep-
resentation. We employ the LSTM as our decoder,
which contains 1 hidden layer with 512 hidden
units. Adam optimizer is used to train our model
with batch size of 128 and learning rate of 0.0001.
We train at most 100 epochs and select the best
model according to captioning performance on the
validation set.

Results. Table 3 presents the step captioning
performance with ground truth step segmentation

Table 3: Step captioning performance with groundtruth
step segmentation on the YouMakeup dataset.

B@4 MeteorRouge Cider Spice

spatial 13.1 19.7 46.3 94.6 27.1
temporal 16.9 21.9 48.8 130.8 34.3
spatial+temporal 17.9 22.4 50.3 134.7 34.5

on the testing set of YouMakeup. We can see
that the combination of spatial and temporal fea-
tures achieves the best performance, demonstrat-
ing spatial and temporal information is comple-
mentary to generate step captions. The captioning
model based on the spatial feature alone is inferior
to that based on the temporal feature because our
framework does not utilize all clip information.
The overall performance on Cider of the base-
line step caption generation model is relatively
higher than baselines on other video captioning
datasets (Krishna et al., 2017a) due to the fine-
grained characteristics of the proposed dataset. In
the YouMakeup dataset, the styles of step cap-
tions are similar which can make it easy for the
caption generation system to achieve high scores
since evaluation metrics are not aware of the as-
pect importance in the caption such as detailed
tools, actions etc.

Although the generation system achieves high
evaluation scores, we find it fails to capture the
fine-grained details in the makeup instructional
videos. Figure 9 shows the caption generation re-
sults of the baseline system using both temporal
and spatial features on a specific video for the first
6 steps. According to the captions of step 1 and 3,
the system traces out the procedure roughly, lack-
ing of details such as makeup tools and related fa-
cial area. The other three step captions indicate the
system’s weak ability on fine-grained video com-
prehension and confirm the fine-grained character-
istics of YouMakeup. In step 2, the system mis-
takes color correction palette for eyeshadow due
to their similar appearance. However, the eye-
shadow is applied around eyes while color correc-
tion palette is used on the face for color correct-
ing. From step 4 to 6, system shows confusion
about the procedure of applying foundation and
concealer because they are similar in both appear-
ance and usage. System needs to associate prod-
ucts with their usage methods and facial areas they
are applied in order to grasp the subtle difference
between different makeup activities for generating
accurate step description.



5141

Figure 9: Comparison of generated captions and ground-truth captions for different steps. Best viewed in color.

6 Conclusion

In this paper, we introduce a new large-scale
instructional video dataset named YouMakeup
for fine-grained semantic comprehension. The
YouMakeup dataset contains 2,800 makeup in-
structional videos spanning more than 420 hours
in total. Based on the characteristics of makeup
instructional videos and the rich annotations of
temporal boundaries, grounded facial areas and
natural language descriptions of steps, our col-
lected dataset is more suitable to support the fine-
grained video comprehension research than pre-
vious datasets. We further design a generation
task and four question answering tasks to thor-
oughly evaluate the fine-grained semantic compre-
hension ability from different aspects and levels.
A baseline system for step caption generation also
demonstrates the necessity of fine-grained spatial
and temporal information. In the future work, we
plan to make thorough exploration on these pro-
posed tasks. We will make the dataset publicly
accessible in order to support the research investi-
gation in fine-grained semantic comprehension.

Acknowledgments

This work was supported by National Natural Sci-
ence Foundation of China (No. 61772535), Bei-
jing Natural Science Foundation (No. 4192028),
and National Key Research and Development Plan
(No. 2016YFB1001202). We would like to thank
our group member Jingjun Liang for his help in
building the annotation website and all the anno-
tators for their careful annotations.

References
Jean-Baptiste Alayrac, Piotr Bojanowski, Nishant

Agrawal, Josef Sivic, Ivan Laptev, and Simon
Lacoste-Julien. 2016. Unsupervised learning from
narrated instruction videos. In Proceedings of the

IEEE Conference on Computer Vision and Pattern
Recognition, pages 4575–4583.

Peter Anderson, Basura Fernando, Mark Johnson, and
Stephen Gould. 2016. Spice: Semantic proposi-
tional image caption evaluation. Adaptive Behavior,
11(4):382–398.

Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-
garet Mitchell, Dhruv Batra, C Lawrence Zitnick,
and Devi Parikh. 2015. Vqa: Visual question an-
swering. In Proceedings of the IEEE international
conference on computer vision, pages 2425–2433.

Piotr Bojanowski, Edouard Grave, Armand Joulin,
and Tomas Mikolov. 2016. Enriching word vec-
tors with subword information. arXiv preprint
arXiv:1607.04606.

Shizhe Chen, Yuqing Song, Yida Zhao, Qin Jin,
Zhaoyang Zeng, Bei Liu, Jianlong Fu, and Alexan-
der Hauptmann. 2019. Activitynet 2019 task 3:
Exploring contexts for dense captioning events in
videos. arXiv preprint arXiv:1907.05092.

Dima Damen, Hazel Doughty, Giovanni
Maria Farinella, Sanja Fidler, Antonino Furnari,
Evangelos Kazakos, Davide Moltisanti, Jonathan
Munro, Toby Perrett, Will Price, et al. 2018. Scal-
ing egocentric vision: The epic-kitchens dataset.
In Proceedings of the European Conference on
Computer Vision (ECCV), pages 720–736.

Pradipto Das, Chenliang Xu, Richard F Doell, and Ja-
son J Corso. 2013. A thousand frames in just a few
words: Lingual description of videos through latent
topics and sparse object stitching. In Proceedings of
the IEEE conference on computer vision and pattern
recognition, pages 2634–2641.

Jia Deng, Wei Dong, Richard Socher, Li Jia Li, Kai
Li, and Fei Fei Li. 2009. Imagenet: A large-scale
hierarchical image database. In IEEE Conference
on Computer Vision Pattern Recognition.

Michael Denkowski and Alon Lavie. 2014. Meteor
universal: Language specific translation evaluation
for any target language. In Proceedings of the Ninth
Workshop on Statistical Machine Translation, pages
376–380. Association for Computational Linguis-
tics.

https://doi.org/10.3115/v1/W14-3348
https://doi.org/10.3115/v1/W14-3348
https://doi.org/10.3115/v1/W14-3348


5142

Carlos Flick. 2004. Rouge: A package for automatic
evaluation of summaries. In Workshop on Text Sum-
marization Branches Out.

Meera Hahn, Nataniel Ruiz, Jean-Baptiste Alayrac,
Ivan Laptev, and James M Rehg. 2018. Learning
to localize and align fine-grained actions to sparse
instructions. arXiv preprint arXiv:1809.08381.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Sun
Jian. 2016. Deep residual learning for image recog-
nition. In IEEE Conference on Computer Vision
Pattern Recognition.

Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim,
and Gunhee Kim. 2017. Tgif-qa: Toward spatio-
temporal reasoning in visual question answering. In
Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 2758–2766.

Justin Johnson, Bharath Hariharan, Laurens van der
Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross
Girshick. 2017. Clevr: A diagnostic dataset for
compositional language and elementary visual rea-
soning. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages
2901–2910.

K Kim, C Nan, MO Heo, SH Choi, and BT Zhang.
2016. Pororoqa: Cartoon video series dataset for
story understanding. In Proceedings of NIPS 2016
Workshop on Large Scale Computer Vision System,
volume 15.

Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei,
and Juan Carlos Niebles. 2017a. Dense-captioning
events in videos. In The IEEE International Confer-
ence on Computer Vision (ICCV).

Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-
son, Kenji Hata, Joshua Kravitz, Stephanie Chen,
Li Jia Li, Li Jia Li, and David A. Shamma. 2017b.
Visual genome: Connecting language and vision us-
ing crowdsourced dense image annotations. Inter-
national Journal of Computer Vision, 123(1):32–73.

Hilde Kuehne, Ali Arslan, and Thomas Serre. 2014.
The language of actions: Recovering the syntax and
semantics of goal-directed human activities. In The
IEEE Conference on Computer Vision and Pattern
Recognition (CVPR).

Quoc Le and Tomas Mikolov. 2014. Distributed repre-
sentations of sentences and documents. In Interna-
tional conference on machine learning, pages 1188–
1196.

Jie Lei, Licheng Yu, Mohit Bansal, and Tamara L Berg.
2018. Tvqa: Localized, compositional video ques-
tion answering. In EMNLP.

Jie Lei, Licheng Yu, Tamara L Berg, and Mohit
Bansal. 2019. Tvqa+: Spatio-temporal ground-
ing for video question answering. arXiv preprint
arXiv:1904.11574.

Tegan Maharaj, Nicolas Ballas, Anna Rohrbach, Aaron
Courville, and Christopher Pal. 2017. A dataset
and exploration of models for understanding video
data through fill-in-the-blank question-answering.
In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, pages 6884–
6893.

Mateusz Malinowski and Mario Fritz. 2014. A multi-
world approach to question answering about real-
world scenes based on uncertain input. In Ad-
vances in neural information processing systems,
pages 1682–1690.

Taichi Nishimura, Atsushi Hashimoto, Yoko Ya-
makata, and Shinsuke Mori. 2019. Frame selection
for producing recipe with pictures from an execution
video of a recipe. In Proceedings of the 11th Work-
shop on Multimedia for Cooking and Eating Activi-
ties, pages 9–16. ACM.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics (ACL).

Mengye Ren, Ryan Kiros, and Richard Zemel. 2015a.
Exploring models and data for image question an-
swering. In Advances in neural information pro-
cessing systems, pages 2953–2961.

Shaoqing Ren, Kaiming He, Ross Girshick, and Jian
Sun. 2015b. Faster r-cnn: towards real-time object
detection with region proposal networks.

Marcus Rohrbach, Sikandar Amin, Mykhaylo An-
driluka, and Bernt Schiele. 2012. A database for
fine grained activity detection of cooking activities.
In 2012 IEEE Conference on Computer Vision and
Pattern Recognition, pages 1194–1201. IEEE.

Sebastian Stein and Stephen J McKenna. 2013. Com-
bining embedded accelerometers with computer vi-
sion for recognizing food preparation activities. In
Proceedings of the 2013 ACM international joint
conference on Pervasive and ubiquitous computing,
pages 729–738. ACM.

Yansong Tang, Dajun Ding, Yongming Rao, Yu Zheng,
Danyang Zhang, Lili Zhao, Jiwen Lu, and Jie Zhou.
2019. Coin: A large-scale dataset for comprehen-
sive instructional video analysis. arXiv preprint
arXiv:1903.02874.

Makarand Tapaswi, Yukun Zhu, Rainer Stiefelhagen,
Antonio Torralba, Raquel Urtasun, and Sanja Fidler.
2016. Movieqa: Understanding stories in movies
through question-answering. In Proceedings of the
IEEE conference on computer vision and pattern
recognition, pages 4631–4640.

Kai Chen Greg Corrado Jeffrey Dean Tomas Mikolov,
Ilya Sutskever. 2013. Distributed representations of
words and phrases and their compositionality. arXiv
preprint arXiv:1310.4546.



5143

Sam Toyer, Anoop Cherian, Tengda Han, and Stephen
Gould. 2017. Human pose forecasting via deep
markov models. In 2017 International Conference
on Digital Image Computing: Techniques and Ap-
plications (DICTA), pages 1–8. IEEE.

Ramakrishna Vedantam, C. Lawrence Zitnick, and
Devi Parikh. 2015. Cider: Consensus-based image
description evaluation. In CVPR.

Oriol Vinyals, Alexander Toshev, Samy Bengio, and
Dumitru Erhan. 2015. Show and tell: A neural im-
age caption generator. In IEEE Conference on Com-
puter Vision Pattern Recognition.

Huijuan Xu, Abir Das, and Kate Saenko. 2017. R-c3d:
Region convolutional 3d network for temporal activ-
ity detection. In Proceedings of the IEEE interna-
tional conference on computer vision, pages 5783–
5792.

K. Zhang, Z. Zhang, Z. Li, and Y. Qiao. 2016. Joint
face detection and alignment using multitask cas-
caded convolutional networks. IEEE Signal Pro-
cessing Letters, 23(10):1499–1503.

Luowei Zhou, Chenliang Xu, and Jason J Corso. 2018.
Towards automatic learning of procedures from web
instructional videos. In AAAI Conference on Artifi-
cial Intelligence, pages 7590–7598.

https://doi.org/10.1109/LSP.2016.2603342
https://doi.org/10.1109/LSP.2016.2603342
https://doi.org/10.1109/LSP.2016.2603342
https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17344
https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17344

