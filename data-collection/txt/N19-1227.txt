











































Partial Or Complete, That's The Question


Proceedings of NAACL-HLT 2019, pages 2190–2200
Minneapolis, Minnesota, June 2 - June 7, 2019. c©2019 Association for Computational Linguistics

2190

Partial Or Complete, That Is The Question

Qiang Ning,1 Hangfeng He,2 Chuchu Fan,1 Dan Roth1,2
1Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign

2Department of Computer and Information Science, University of Pennsylvania
{qning2,cfan10}@illinois.edu, {hangfeng,danroth}@seas.upenn.edu

Abstract

For many structured learning tasks, the data
annotation process is complex and costly. Ex-
isting annotation schemes usually aim at ac-
quiring completely annotated structures, under
the common perception that partial structures
are of low quality and could hurt the learn-
ing process. This paper questions this common
perception, motivated by the fact that struc-
tures consist of interdependent sets of vari-
ables. Thus, given a fixed budget, partly an-
notating each structure may provide the same
level of supervision, while allowing for more
structures to be annotated. We provide an in-
formation theoretic formulation for this per-
spective and use it, in the context of three di-
verse structured learning tasks, to show that
learning from partial structures can sometimes
outperform learning from complete ones. Our
findings may provide important insights into
structured data annotation schemes and could
support progress in learning protocols for
structured tasks.

1 Introduction

Many machine learning tasks require structured
outputs, and the goal is to assign values to a set of
variables coherently. Specifically, the variables in
a structure need to satisfy some global properties
required by the task. An important implication is
that once some variables are determined, the val-
ues taken by other variables are constrained. For
instance, in the temporal relation extraction prob-
lem in Fig. 1a, if met happened before leaving and
leaving happened on Thursday, then we know that
met must either be before Thursday (“met (1)”) or
has to happen on Thursday, too (“met (2)”) (Ning
et al., 2018a). Similarly, in the semantic frame of
the predicate gave (Kingsbury and Palmer, 2002)
in Fig. 1b, if the boy is ARG0 (short for argu-
ment 0), then it rules out the possibility of a frog

met (2) leavingmet (1)

Thursday

Time

I met with him before leaving for Paris 
on Thursday.

(a)

The boy gave a frog to the girl.
Arg0 Arg0 Arg0

PREDICATE(b)

! FOREHEAD
! LEFT_EYE

! TORSO
!

(c)

NECK

Before

Be_Included

Figure 1: Due to the inherent structural constraints
of each task, individual instances therein put restric-
tions on others. (a) The temporal relation between
met and Thursday has to be BEFORE (“met (1)”) or
BE INCLUDED (“met (2)”). (b) The argument roles
of a frog and to the girl cannot be ARG0 anymore.
(c) Given the position of the cat’s FOREHEAD and
LEFT EYE, a rough estimate of its NECK can be the
red solid box rather than the blue dashed box.

or to the girl taking the same role. Figure 1c fur-
ther shows an example of part-labeling of images
(Choi et al., 2018); given the position of FORE-
HEAD and LEFT EYE of the cat in the picture,
we roughly know that its NECK should be some-
where in the red solid box, while the blue dashed
box is likely to be wrong.

Data annotation for these structured tasks is
complex and costly, thus requiring one to make
the most of a given budget. This issue has been



2191

investigated for decades from the perspective of
active learning for classification tasks (Angluin,
1988; Atlas et al., 1990; Lewis and Gale, 1994)
and for structured tasks (Roth and Small, 2006a,b,
2008; Hu et al., 2019). While active learning aims
at selecting the next structure to label, we try to
investigate, from a different perspective, whether
we should annotate each structure completely or
partially. Conventional annotation schemes typi-
cally require complete structures, under the com-
mon perception that partial annotation could ad-
versely affect the performance of the learning al-
gorithm. But note that partial annotations will
allow for more structures to be annotated (see
Fig. 2). Therefore, a fair comparison should be
done while maintaining a fixed annotation bud-
get, which was not done before. Moreover, even
if partial annotation leads to comparable learning
performance to conventional complete schemes, it
provides more flexibility in data annotation.

Another potential benefit of partial annotation is
that it imposes constraints on the remaining parts
of a structure. As illustrated by Fig. 1, with par-
tial annotations, we already have some knowledge
about the unannotated parts. Therefore, further an-
notations of these variables may use the available
budget less efficiently; this effect was first dis-
cussed in Ning et al. (2018c). Motivated by the
observations in Figs. 1-2, we think it is impor-
tant to study partialness systematically, before we
hastily assume that completeness should always be
favored in data collection.

To study whether the above benefits of par-
tialness can offset its weakness for learning, our
first contribution is the proposal of early stop-
ping partial annotation (ESPA) scheme, which
randomly picks up instances to label in the be-
ginning, and stops before a structure is completed.
We do not claim that ESPA should always be pre-
ferred; instead, it serves as an alternative to con-
ventional, complete annotation schemes that we
should keep in mind, because, as we show later,
it can be comparable to (and sometimes even bet-
ter than) complete annotation schemes. ESPA is
straightforward to implement even in crowdsourc-
ing; instances to annotate can be selected offline
and distributed to crowdsourcers; this can be con-
trasted with the difficulties of implementing active
learning protocols in these settings (Ambati et al.,
2010; Laws et al., 2011). We think that ESPA is a
good representative for a systematic study of par-

tialness.

(a) Complete (b) Partial

Figure 2: If we need training data for a graph labeling
task (assuming the gold values for the nodes are given)
and our annotation budget allows us to annotate, for
instance, 10 edges in total, we could (a) completely an-
notate one graph (and then we run out of budget), or (b)
partially annotate two graphs.

Our second contribution is the development of
an information theoretic formulation to explain the
benefit of ESPA (Sec. 2), which we further demon-
strate via three structured learning tasks in Sec. 4:
temporal relation (TempRel) extraction (UzZaman
et al., 2013), semantic role classification (SRC),1

and shallow parsing (Tjong Kim Sang and Buch-
holz, 2000). These tasks are chosen because they
each represent a wide spectrum of structures that
we will detail later. As a byproduct, we extend
constraint-driven learning (CoDL) (Chang et al.,
2007) to cope with partially annotated structures
(Sec. 3); we call the algorithm Structured Self-
learning with Partial ANnotations (SSPAN) to dis-
tinguish it from CoDL.2

We believe in the importance of work in this
direction. First, partialness is inevitable in prac-
tice, either by mistake or by choice, so our the-
oretical analysis can provide unique insight into
understanding partialness. Second, it opens up op-
portunities for new annotation schemes. Instead of
considering partial annotations as a compromise,
we can in fact annotate partial data intentionally,
allowing us to design favorable guidelines and
collect more important annotations at a cheaper
price. Many recent datasets that were collected via
crowdsourcing are already partial, and this paper
provides some theoretical foundations for them.
Furthermore, the setting described here addresses
natural scenarios where only partial, indirect su-
pervision is available, as in Incidental Supervision

1A subtask of semantic role labeling (SRL) (Palmer et al.,
2010) that only classifies the role of an argument.

2There has been many works on learning from partial an-
notations, which we review in Sec. 3. SSPAN is only an ex-
perimental choice in demonstrating ESPA. Whether SSPAN
is better than other algorithms is out of the scope here, and a
better algorithm for ESPA will only strengthen the claims in
this paper.



2192

(Roth, 2017), and this paper begins to provide the-
oretical understanding for this paradigm, too. Fur-
ther discussions can be found in Sec. 5.

It is important to clarify that we assume uniform
cost over individual annotations (that is, all edges
in Fig. 2 cost equally), often the default setting in
crowdsourcing. We realize that the annotation dif-
ficulty can vary a lot in practice, sometimes incur-
ring different costs. To address this issue, we ran-
domly select instances to label so that on average,
the cost is uniform. We agree that, even with this
randomness, there could still be situations where
the assumption does not hold, but we leave it for
future studies, possibly in the context of active
learning schemes.

2 ESPA: Early Stopping Partial
Annotation

In this section, we study whether the effect demon-
strated by the examples in Fig. 1 exists in general.
First, we formally define structure and annotation.

Definition 1. A structure of size d is a vector
of random variables (RV) Y = [Y1, . . . , Yd] 2
C(Ld), where L = {`1, . . . , `|L|} is the label set
for each variable and C(Ld) ✓ Ld represents the
constraints imposed by this type of structure.

It is necessary to model a structure as a set
of random variables because when it is not com-
pletely annotated, there is still uncertainty in the
annotation assignment. To study partial annota-
tions, we introduce the following:

Definition 2. A k-step annotation (0  k  d) is a
vector of RVs Ak = [Ak,1, . . . , Ak,d] 2 (L [ u)d

where u is a special character for null, such that

dX

i=1

1(Ak,i 6= u) = k, (1)

P (Y|Ak = ak) = P (Y|Yj = ak,j , j 2 J ) , (2)

where J is the set of indices that ak,j 6= u.

Eq. (1) means that, in total, k variables are al-
ready annotated at step k. Obviously, A0 means
that no variables are labeled, and Ad means that
all variables in Y are determined. Ak is what we
call a k-step ESPA, so hereafter we use k/d to rep-
resent annotation completeness. Eq. (2) assumes
no annotation mistakes, so if the i-th variable is
labeled, then Yi must be the same as Ak,i.

To measure the theoretical benefit of Ak, we
propose the following quantity

Ik = log |C(L
d)|� E [log f(ak)] (3)

for k = 0, . . . , d, where f(ak) = |{y 2 C(Ld) :
P (y|ak) > 0}| is the total number of structures in
C(Ld) that are still valid given Ak = ak. Since
we assume that the labeled variables in Ak are se-
lected uniformly randomly, E [·] is simply the av-
erage of log f(ak). When k = 0, f(ak) ⌘ C(Ld)
and I0 ⌘ 0; as k increases, Ik increases since the
structure has more and more variables labeled; fi-
nally, when k = d, the structure is fully deter-
mined and Id ⌘ log |C(Ld)|. The first-order finite
difference, Ik� Ik�1, is the benefit brought by an-
notating an additional variable at step k; if Ik is
concave (i.e., a decaying Ik � Ik�1), the benefit
from a new annotation attenuates, suggesting the
potential benefit of the ESPA strategy.

In an extreme case where the structure is so
strong that it requires all individual variables to
share the same label, then labeling any variable
is sufficient for determining the entire structure.
Intuitively, we do not need to annotate more than
one variable. Our Ik quantity can support this in-
tuition: The structural constraint, C(Ld), contains
only |L| elements: {[`i, `i, . . . , `i]}

|L|
i=1, so I0 = 0,

and I1 = · · · = Id = log |L|. Since Ik does not in-
crease at all when k >= 1, we should adopt first-
step annotation A1. Another extreme case is that
of a trivial structure that has no constraints (i.e.,
C(Yd) = Yd). The annotation of all variables are
independent and we gain no advantage from skip-
ping any variables. This intuition can be supported
by our Ik analysis as well: Since Ik = k log |L|,
8k = 0, 1, . . . , d, Ik is linear and all steps con-
tribute equally to improving Ik by log |L|; there-
fore ESPA is not necessary.

Real-world structures are often not as trivial as
the two extreme cases above, but Ik can still serve
as a guideline to help determine whether it is ben-
eficial to use ESPA. We next discuss three diverse
types of structures and how to obtain Ik for them.
Example 1. The ranking problem is an impor-
tant machine learning task and often depends on
pairwise comparisons, for which the label set is
L = {<,>}. For a ranking problem with n items,
there are d = n(n�1)/2 pairwise comparisons in
total. Its structure is a chain following the transi-
tivity constraints, i.e., if A < B and B < C, then
A < C.



2193

Figure 3: The mutual information between the chain
structure and its k-step ESPA, Ik, is concave, suggest-
ing possible benefit of using ESPA. In the simulation,
there are n = 10 items in the chain and thus d = 45
pairs, k of which are labeled. The values of Ik’s, as de-
fined by Eq. (3), were obtained through averaging 1000
experiments. We use base-2 logarithm and the unit on
y-axis is thus “bit”.

A k-step ESPA Ak for a chain means that only
k (out of d) pairs are compared and labeled, re-
sulting in a directed acyclic graph (DAG). In this
case, f(ak) is actually counting the number of lin-
ear extensions of the DAG, which is known to be
#P-complete (Brightwell and Winkler, 1991), so
we do not have a closed-form solution to Ik. In
practice, however, we can use the Kahn’s algo-
rithm and backtracking to simulate with a rela-
tively small n, as shown by Fig. 3, where n = 10
and Ik was obtained through averaging 1000 ran-
dom simulations. Ik is concave, as reflected by the
downward shape of Ik � Ik�1. Therefore, new an-
notations are less and less efficient for the chain
structure, suggesting the usage of ESPA.

Example 2. The general assignment problem re-
quires assigning d agents to d0 tasks such that the
agent nodes and the task nodes form a bipartite
graph (without loss of generality, assume d  d0).
That is, an agent can handle exactly one task, and
each task can only be handled by at most one
agent. Then from the agents’ point of view, the la-
bel set for each of them is L = {1, 2, . . . , d0}, de-
noting the task assigned to the agent.

A k-step ESPA Ak for this problem means

that k agents are already assigned with tasks,
and f(ak) is to count the valid assignments
of the remaining tasks to the remaining d � k
agents, to which we have closed-form solutions:
f(ak) =

(d0�k)!
(d0�d)! , 8ak. According to Eq. (3), Ik =

log d
0!

(d0�k)! regardless of d or the distribution of
Ak, and is concave (Fig. 4 shows an example of
it when d = 4, d0 = 10).
Example 3. Sequence tagging is an important
NLP problem, where the tags of tokens are inter-
dependent. Take chunking as an example. A basic
scheme is for each token to choose from three la-
bels, B(egin), I(nside), and O(utside), to represent
text chunks in a sentence. That is, L = {B, I,O}.
Obviously, O cannot be immediately followed by I.

Let d be the number of tokens in a sentence. A
k-step ESPA Ak for chunking means that k tokens
are already labeled by B/I/O, and f(ak) counts the
valid BIO sequences that do not violate those ex-
isting annotations. Again, as far as we know, there
is no closed-form solution to f(ak) and Ik, but
in practice, we can use dynamic programming to
obtain f(ak) and then Ik using Eq. (3). We set
d = 10 and show Ik � Ik�1 for this task in Fig. 4,
where we observe the same effect we see in previ-
ous examples: The benefit provided by labeling a
new token in the structure attenuates.

Interestingly, based on Fig. 4, we find that the
slope of Ik�Ik�1 may be a good measure of the
“tightness” or “strength” of a structure. When
there is no structure at all, the curve is flat (black).
The BIO structure is intuitively simple, and it in-
deed has the flattest slope among the three struc-
tured tasks (purple). When the structure is a chain,
the level of uncertainty goes down rapidly with ev-
ery single annotation (think of standard sorting al-
gorithms); the constraint is intuitively strong and
in Fig. 4, it indeed has a steep slope (blue).

Finally, we want to emphasize that the defi-
nition of Ik in Eq. (3) is in fact backed by in-
formation theory. When we do not have prior in-
formation about Y, we can assume that Y follows
a uniform distribution over C(Ld). Then, Ik is es-
sentially the mutual information between structure
Y and annotation Ak, I(Y;Ak):

I(Y;Ak) = H(Y)�H(Y|Ak)

= log |C(Ld)|� E [H(Y|Ak = ak)]

= log |C(Ld)|� E [log f(ak)] ,

where H(·) is the entropy function. This is an im-



2194

portant discovery, since it points out a new way to
view a structure and its annotations. It may be use-
ful for studying active learning methods for struc-
tured tasks, and other annotation phenomena such
as noisy annotations. The usage of mutual infor-
mation also aligns well with the information bot-
tleneck framework (Shamir et al., 2010; Shwartz-
Ziv and Tishby, 2017; Yu and Principe, 2018), al-
though a more recent paper challenges the inter-
pretation of information bottleneck (Saxe et al.,
2018).

Figure 4: The Ik � Ik�1 curves from several different
structures. The curves are shifted to almost the same
starting point for better visualization, so the Y-Axis
grid is not shown. The curve for “Chain” was obtained
via simulations, and the other curves all have closed-
form formulations.

3 Learning from Partial Structures

So far, we have been advocating the ESPA strat-
egy to maximize the information we can get from
a fixed budget. Since early stopping leads to par-
tial annotations, one missing component before
we can benefit from it is an approach to learn-
ing from partial structures. In this study, we as-
sume the existence of a relatively small but com-
plete dataset that can provide a good initializa-
tion for learning from a partial dataset, which is
very similar to semi-supervised learning (SSL).
SSL, in its most standard form, studies the com-
bined usage of a labeled set T = {(xi, yi)}i
and an unlabeled set U = {xj}j , where the x’s
are instances and y’s are the corresponding la-
bels. SSL gains information about p(x) through U ,
which may improve the estimation of p(y|x). Spe-
cific algorithms range from self-training (Scud-

der, 1965; Yarowsky, 1995), co-training (Blum
and Mitchell, 1998), generative models (Nigam
et al., 2000), to transductive SVM (Joachims,
1999) etc., among which one of the most ba-
sic algorithms is Expectation-Maximization (EM)
(Dempster et al., 1977). By treating them as hid-
den variables, EM “marginalizes” out the missing
labels of U via expectation (i.e., soft EM) or maxi-
mization (i.e., hard EM). For structured ML tasks,
soft and hard EMs turn into posterior regulariza-
tion (PR) (Ganchev et al., 2010) and constraint-
driven learning (CoDL) (Chang et al., 2007), re-
spectively.

Unlike unlabeled data, the partially annotated
structures caused by early stopping urge us to gain
information not only about p(x), but also from
their labeled parts. There have been many existing
work along this line (Tsuboi et al., 2008; Fernan-
des and Brefeld, 2011; Hovy and Hovy, 2012; Lou
and Hamprecht, 2012), but in this paper, we de-
cide to extend CoDL to cope with partial annota-
tions due to two reasons. First, CoDL, which itself
can be viewed as an extension of self-training to
structured learning, is a wrapper algorithm having
wide applications. Second, as its name suggests,
CoDL learns from U by guidance of constraints,
so partial annotations in U are technically easy to
be added as extra equality constraints.

Algorithm 1 describes our Structured Self-
learning with Partial ANnotations (SSPAN) al-
gorithm that learns a model H. The same as
CoDL, SSPAN is a wrapper algorithm requir-
ing two components: LEARN and INFERENCE.
LEARN attempts to estimate the local decision
function for each individual instance regardless
of the global constraints, while INFERENCE takes
those local decisions and performs a global infer-
ence. Lines 3-9 are the procedure of self-training,
which iteratively completes the missing annota-
tions in P and learns from both T and the com-
pleted version of P (i.e., P̃).3 Line 6 requires that
the inference follows the structural constraints in-
herently in the task, turning the algorithm into
CoDL; Line 7 enforces those partial annotations in
ai, further turning it into SSPAN. In practice, IN-
FERENCE can be realized by the Viterbi or beam
search algorithm in sequence tagging, or more
generally, by Integer Linear Programming (ILP)

3Line 9 can be interpreted in different ways, either as T [
P̃ (adopted in this work) or as a weighted combination of
LEARN(T ) and LEARN(P̃) (adopted by (Chang et al., 2007)).



2195

(Punyakanok et al., 2005); either way, the partial
constraints of Line 7 can be easily incorporated.

Algorithm 1: Structured Self-learning with
Partial Annotations (SSPAN)
Input: T = {(xi,yi)}Ni=1, P = {(xi,ai)}N+Mi=N+1

1 Initialize H = LEARN(T )
2 while convergence criteria not satisfied do
3 P̃ = ;
4 foreach (xi,ai) 2 P do
5 ŷi = INFERENCE(xi;H), such that
6 ⇧ ŷi 2 C(Yd)
7 ⇧ ŷi,j = ai,j , 8ai,j 6= u
8 P̃ = P̃ [ {(xi, ŷi)}

9 H = LEARN(T + P̃)

10 return H

4 Experiment

In Sec. 2, we argued from an information theoretic
view that ESPA is beneficial for structured tasks if
we have a fixed annotation resource. We then pro-
posed SSPAN in Sec. 3 to learn from the resulting
partial structures. However, on one hand, there is
still a gap between the Ik analysis and the actual
system performance; on the other hand, whether
the benefit can be realized in practice also depends
on how effective the algorithm exploits partial an-
notations. Therefore, it remains to be seen how
ESPA works in practice. Here we use three NLP
tasks: temporal relation (TempRel) extraction, se-
mantic role classification (SRC), and shallow pars-
ing, analogous to the chain, assignment, and BIO
structures, respectively.

For all tasks, we compare the following two
schemes in Fig. 5, where we use graph struc-
tures for demonstration. Initially, we have a rel-
atively small but complete dataset T0, an unan-
notated dataset U0, and some budget to annotate
U0. The conventional scheme I, also our baseline
here, is to annotate each structure completely be-
fore randomly picking up the next one. Due to
the limited budget, some U0 remain untouched
(denoted by U ). The proposed scheme II adopts
ESPA so that all structures at hand are annotated
but only partially. For fair comparisons, we use
CoDL to incorporate U into scheme I as well.
Finally, the systems trained on the dataset from
I/II via CoDL/SSPAN are evaluated on unseen but
complete testset Ttest. Note that because ESPA is

a new annotation scheme, there exists no dataset
collected this way. We use existing complete
datasets and randomly throw out some annotations
to mimic ESPA in the following. Due to the ran-
domness in selecting which structures/instances to
keep in scheme I/II, we repeat the whole process
multiple times and report the mean F1. The bud-
get, defined as the total number of individual in-
stances that can be annotated, ranges from 10% to
100% with a stepsize of 10%, where x% means
x% of all instances in U0 can be annotated.

(I) Complete (II) ESPA
!"

same budget same budget

!# $ % %

Training Phase

!&'(&

Testing Phase

CoDL: !", !# and $ SSPAN: !" and %

$" $"

Figure 5: The two annotation schemes we compare
in Sec. 4. T , P , and U denote complete, partial, and
empty structures, respectively. Both schemes start with
a complete and relatively small dataset and an unanno-
tated dataset (green). (I) Conventional complete anno-
tation scheme (blue). (II) The proposed ESPA scheme
(red). Finally, they are tested on an unseen and com-
plete dataset (black).

4.1 Temporal Relation Extraction
Temporal relations (TempRel) are a type of im-
portant relations representing the temporal order-
ing of events described by natural language text.
That is to answer questions like which event hap-
pens earlier or later in time (see Fig. 1a). Since
time is physically one-dimensional, if A is before
B and B is also before C, then A must be before
C. In practice, the label set for TempRels can be
more complex, e.g., with labels such as SIMULTA-
NEOUS and VAGUE, but the structure can still be
represented by transitivity constraints (see Table 1
of (Ning et al., 2018a)), which can be viewed as



2196

an analogy of the chain structure in Example 1.
To avoid missing relations, annotators are re-

quired to exhaustively label every pair of events
in a document (i.e., the complete annotation
scheme), so it is necessary to study ESPA in
this context. Here we adopt the MATRES dataset
(Ning et al., 2018b) for its better inter-annotator
agreement and relatively large size.

Specifically, we use 35 documents as T0 (the
TimeBank-Dense section,4 147 documents as U0
(the TimeBank section minus those documents in
T0), and the Platinum section (a benchmark testset
of 20 documents with 1K TempRels) as Ttest. Note
that both schemes I and II are mimicked by down-
sampling the original annotations in MATRES,
where the budget is defined as the total number
of TempRels that are kept. Following CogComp-
Time (Ning et al., 2018d), we choose the same fea-
tures and sparse-averaged perceptron algorithm as
the LEARN component and ILP as INFERENCE for
SSPAN.

4.2 Semantic Role Classification (SRC)
Semantic role labeling (SRL) is to represent the
semantic meanings of language and answer ques-
tions like Who did What to Whom and When,
Where, How (Palmer et al., 2010). Semantic Role
Classification (SRC) is a subtask of SRL, which
assumes gold predicates and argument chunks and
only classifies the semantic role of each argument.
We use the Verb SRL dataset provided by the
CoNLL-2005 shared task (Carreras and Màrquez,
2005), where the semantic roles include num-
bered arguments, e.g., ARG0 and ARG1, and argu-
ment modifiers, e.g., location (AM-LOC), tempo-
ral (AM-TMP), and manner (AM-MNR) (see Prop-
Bank (Kingsbury and Palmer, 2002)). The struc-
tural constraints for SRC is that each argument can
be assigned to exactly one semantic role, and the
same role cannot appear twice for a single verb, so
SRC is an assignment problem as in Example 2.

Specifically, we use the Wall Street Journal
(WSJ) part of Penn TreeBank III (Marcus et al.,
1993). We randomly select 700 sentences from the
Sec. 24 of WSJ, among which 100 sentences as T0
and 600 sentences as U0. Our Ttest is 5700 sen-
tences (about 40K arguments) from Secs. 00, 01,
23. The budget here is defined as the total num-

4The original TimeBank-Dense section contains 36 docu-
ments, but in collecting MATRES, one of the documents was
filtered out because it contained no TempRels between main-
axis events.

ber of the arguments. We adopt the SRL system
in CogCompNLP (Khashabi et al., 2018) and uses
the sparse averaged perceptron as LEARN and ILP
as INFERENCE.

4.3 Shallow Parsing

Shallow parsing, also referred as chunking, is a
fundamental NLP task to identify constituents in a
sentence, such as noun phrases (NP), verb phrases
(VP), and adjective phrases (ADJP), which can be
viewed as extending the standard BIO structure in
Example 3 with different chunk types: B-NP, I-NP,
B-VP, I-VP, B-ADJP, I-ADJP, . . . , O.

We use the chunking dataset provided by the
CoNLL-2000 shared task (Tjong Kim Sang and
Buchholz, 2000). Specifically, we use 2K tokens’
annotations as T0, 14K tokens as U0, and the
benchmark testset (25K tokens) as Ttest. The bud-
get here is defined as the total number of tokens’
BIO labels. The algorithm we use here is the chun-
ker provided in CogCompNLP, where the LEARN
component is the sparse averaged perceptron and
the INFERENCE is described in (Punyakanok and
Roth, 2001).

4.4 Results

We compare the F1 performances of all three tasks
in Fig. 6, averaged from 50 experiments with dif-
ferent randomizations. As the budget increases,
the system F1 increases for both schemes I and II
in all three tasks, which confirms the capability of
the proposed SSPAN framework to learn from par-
tial structures. When the budget is 100% (i.e., the
entire U0 is annotated), schemes I and II have neg-
ligible differences; when the budget is not large
enough to cover the entire U0, scheme II is consis-
tently better than I in all tasks, which follows our
expectations based on the Ik analysis. The strict
improvement for all budget ratios indicates that the
observation is definitely not by chance.

Figure 7 further compares the improvement
from I to II across tasks. When the budget goes
down from 100%, the advantage of ESPA is more
prominent; but when the budget is too low, the
quality of P̃ degrades and hurts the performance
of SSPAN, leading to roughly hill-shaped curves
in Fig. 7. We have also conjectured based on
Fig. 4 that the structure strength goes up from BIO
chunks, to bipartite graphs, and to chains; interest-
ingly, the improvement brought by ESPA is con-
sistent with this order.



2197

++ ++ ++ ++ ++ ++ ++ ++ +

(a) Temporal Relation Extraction

+ ++ ++ ++ ++ +++

(b) SRC

+ + ++ +

(c) Shallow Parsing

Figure 6: Comparison of the baseline, complete annotation scheme and the proposed ESPA scheme (See I & II
in Fig. 5) under three structured learning tasks (note the scale difference). Each F1 value is the average of 50
experiments, and each curve is based on corresponding F1 values smoothed by Savitzky-Golay filters. We can see
that scheme II is consistently better than scheme I. Per the Wilcoxon rank-sum test, the significance levels at each
given budget are shown on the x-axes, where + and ++ mean p < 5% and p < 1%, respectively.

Admittedly, the improvement, albeit statisti-
cally significant, is small, but it does not dimin-
ish the contribution of this paper: Our goal is
to remind people that the ESPA scheme (or more
generally, partialness) is, at the least, comparable
to (or sometimes even better than) complete an-
notation schemes. Also, the comparison here is in
fact unfair to the partial scheme II, because we as-
sume equal cost for both schemes, although it of-
ten costs less in a partial scheme as a large prob-
lem is decomposed into smaller parts. Therefore,
the results shown here implies that the informa-
tion theoretical benefit of partialness can possibly
offset its disadvantages for learning.

Figure 7: The improvement of F1 brought by ESPA for
each task in Fig. 6. Note that we conjectured earlier
in Fig. 4 that the BIO structure is the weakest among
the three, which is consistent with the fact that shallow
parsing benefits the least from ESPA.

5 Dicussion and Conclusion

In this paper, we investigate a less studied, yet
important question for structured learning: Given
a limited annotation budget (either in time or
money), which strategy is better, completely an-

notating each structure until the budget runs out,
or annotating more structures at the cost of leav-
ing some of them partially annotated? Neubig
and Mori (2010) investigated this issue specifi-
cally in annotating word boundaries and pronunci-
ations for Japanese. Instead of annotating full sen-
tences, they proposed to annotate only some words
in a sentence (i.e., partially) that can be chosen
heuristically (e.g., skip those that we have seen or
those low frequency words). Conceptually, Neu-
big and Mori (2010) is an active learning work,
with the understanding that if the order of annota-
tion is deliberately designed, better learning can be
achieved. The current paper addresses the problem
from a different angle: Even without active learn-
ing, can we still answer the question above?

The observation driving our questions is that
when annotating a particular structure, the labels
of the yet to be labeled variables may already
be constrained by previous annotations and carry
less information than those in a totally new struc-
ture. Therefore, we systematically study the ESPA
scheme – stop annotating a given structure before
it is completed and continue annotating another
new structure.

An important notion is annotation cost.
Throughout the paper we have an ideal assump-
tion that the cost is linear in the total number
of annotations, but in practice the case can be
more complicated. First, the actual cost of each
individual annotation may vary across different
instances. We try to eliminate this issue by en-
forcing random selection of annotation instances,
rather than allowing the annotators to select
arbitrarily by themselves. This strategy may be
useful in practice as well, to avoid people only



2198

annotating easy cases. Second, even if we only
require labeling partial structures, it is likely
that the annotator still needs to comprehend the
entire structure, incurring additional cost (usually
in terms of time). This issue, however, is not
addressed in this paper.

Using this definition of cost, we provide a the-
oretical analysis for ESPA based on the mutual
information between target structures and anno-
tation processes. We show that for structures like
chains, bipartite graphs, and BIO chunks, the in-
formation brought by an extra annotation atten-
uates as the annotation of the structure is more
complete, suggesting to stop early and move to
a new structure (although it still remains unclear
when it is optimal to stop). This analysis is further
supported by experiments on temporal relation ex-
traction, semantic role classification, and shallow
parsing, three tasks analogous to the three struc-
tures analyzed earlier, respectively. The ratio of
the attenuation curve as in Fig. 4 is also shown
to be an actionable metric to quantify the strength
of a type of structure, which can be useful in var-
ious analysis, including judging whether ESPA is
worthwhile for a particular task. For example, a
more detailed Ik-based analysis for SRC shows
that predicates with more arguments are stronger
structures than those with fewer arguments; we
have investigated ESPA on those with more than 6
arguments and indeed, observed much larger im-
provement in SRC. More details on this analysis
are put in the appendix.

We think that the findings in this paper are very
important. First, as far as we know, we are the first
to propose the mutual information analysis that
provides a unique view of structured annotation,
that of the reduction in the uncertainty of a target
of interest Y by another random variable/process.
From this perspective, signals that have non-zero
mutual information with Y can be viewed as “an-
notations”. These can be partially labeled struc-
tures (studied here), partial labels (restricting the
possible labels rather than determining a single
one as in e.g., Hu et al. (2019), noisy labels (e.g.,
generated by crowdsourcing or heuristic rules) or,
generally, other indirect supervision signals that
are correlated with Y. As we proposed, these can
be studied within our mutual information frame-
work as well. This paper thus provides a way to an-
alyze the benefit of general incidental supervision
signals (Roth, 2017)) and possibly even provides

guidance in selecting good incidental supervision
signals.

Second, the findings here open up opportunities
for new annotation schemes for structured learn-
ing. In the past, partially annotated training data
have been either a compromise when complete-
ness is infeasible (e.g., when ranking entries in gi-
gantic databases), or collected freely without hu-
man annotators (e.g., based on heuristic rules). If
we intentionally ask human annotators for partial
annotations, the annotation tasks can be more flex-
ible and potentially, cost even less. This is be-
cause annotating complex structures typically re-
quire certain expertise, and smaller tasks are of-
ten easier (Fernandes and Brefeld, 2011). It is
very likely that some complex annotation tasks re-
quire people to read dozens of pages of annota-
tion guidelines, but once decomposed into smaller
subtasks, even laymen can handle them. Annota-
tion schemes driven by crowdsourced question-
answering, known to provide only partial cover-
age are successful examples of this idea (He et al.,
2015; Michael et al., 2017). Therefore, this paper
is hopefully interesting to a broad audience.

Acknowledgements

This research is supported in part by a grant from
the Allen Institute for Artificial Intelligence (al-
lenai.org); the IBM-ILLINOIS Center for Cog-
nitive Computing Systems Research (C3SR) - a
research collaboration as part of the IBM AI
Horizons Network; Contract HR0011-15-2-0025
with the US Defense Advanced Research Projects
Agency (DARPA); and by the Army Research
Laboratory (ARL) and was accomplished under
Cooperative Agreement Number W911NF-09-2-
0053 (the ARL Network Science CTA). The views
and conclusions contained in this document are
those of the authors and should not be inter-
preted as representing the official policies, either
expressed or implied, of the Army Research Lab-
oratory or the U.S. Government. The U.S. Gov-
ernment is authorized to reproduce and distribute
reprints for Government purposes notwithstanding
any copyright notation here on.

References
Vamshi Ambati, Stephan Vogel, and Jaime G Car-

bonell. 2010. Active learning and crowd-sourcing
for machine translation. In Proc. of the International



2199

Conference on Language Resources and Evaluation
(LREC), volume 1, page 2. Citeseer.

Dana Angluin. 1988. Queries and concept learning.
Machine Learning, 2(4):319–342.

Les E Atlas, David A Cohn, and Richard E Ladner.
1990. Training connectionist networks with queries
and selective sampling. In The Conference on Ad-
vances in Neural Information Processing Systems
(NIPS), pages 566–573.

Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In Proc.
of the Annual ACM Workshop on Computational
Learning Theory (COLT).

Graham Brightwell and Peter Winkler. 1991. Counting
linear extensions is #p-complete. In Proceedings of
the Twenty-third Annual ACM Symposium on Theory
of Computing, pages 175–181.

Xavier Carreras and Lluis Màrquez. 2005. Introduction
to the CoNLL-2005 shared task: Semantic role la-
beling. In Proc. of the Annual Conference on Com-
putational Natural Language Learning (CoNLL),
pages 152–164.

Ming-Wei Chang, Lev Ratinov, and Dan Roth. 2007.
Guiding semi-supervision with constraint-driven
learning. In Proc. of the Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
280–287.

Jonghyun Choi, Jayant Krishnamurthy, Aniruddha
Kembhavi, and Ali Farhadi. 2018. Structured set
matching networks for one-shot part labeling. The
IEEE Conference on Computer Vision and Pattern
Recognition (CVPR).

Arthur P Dempster, Nan M Laird, and Donald B Ru-
bin. 1977. Maximum likelihood from incomplete
data via the em algorithm. Journal of the Royal Sta-
tistical Society, Series B, 39(1):1–38.

Eraldo R Fernandes and Ulf Brefeld. 2011. Learning
from partially annotated sequences. In Proc. of the
Joint European Conference on Machine Learning
and Principles and Practice of Knowledge Discov-
ery in Databases (ECML-PKDD).

Kuzman Ganchev, Jennifer Gillenwater, and Ben
Taskar. 2010. Posterior regularization for structured
latent variable models. Journal of Machine Learn-
ing Research.

Luheng He, Mike Lewis, and Luke Zettlemoyer. 2015.
Question-answer driven semantic role labeling: Us-
ing natural language to annotate natural language.
In Proc. of the Conference on Empirical Methods
for Natural Language Processing (EMNLP), pages
643–653.

Dirk Hovy and Eduard Hovy. 2012. Exploiting partial
annotations with em training. In Proc. of the Annual
Meeting of the North American Association of Com-
putational Linguistics (NAACL), pages 31–38.

Peiyun Hu, Zack Lipton, Anima Anandkumar, and
Deva Ramanan. 2019. Active learning with partial
feedback. In Proc. of the International Conference
on Learning Representations (ICLR).

Thorsten Joachims. 1999. Transductive inference for
text classification using support vector machines. In
Proc. of the International Conference on Machine
Learning (ICML).

Daniel Khashabi, Mark Sammons, Ben Zhou, Tom
Redman, Christos Christodoulopoulos, Vivek Sriku-
mar, Nicholas Rizzolo, Lev Ratinov, Guanheng Luo,
Quang Do, Chen-Tse Tsai, Subhro Roy, Stephen
Mayhew, Zhili Feng, John Wieting, Xiaodong Yu,
Yangqiu Song, Shashank Gupta, Shyam Upadhyay,
Naveen Arivazhagan, Qiang Ning, Shaoshi Ling,
and Dan Roth. 2018. Cogcompnlp: Your swiss army
knife for nlp. In 11th Language Resources and Eval-
uation Conference.

Paul Kingsbury and Martha Palmer. 2002. From Tree-
bank to PropBank. In Proceedings of LREC-2002.

Florian Laws, Christian Scheible, and Hinrich Schütze.
2011. Active learning with amazon mechanical turk.
In Proc. of the Conference on Empirical Methods
for Natural Language Processing (EMNLP), pages
1546–1556.

David D Lewis and William A Gale. 1994. A sequen-
tial algorithm for training text classifiers. In Proc. of
International Conference on Research and Develop-
ment in Information Retrieval, SIGIR, pages 3–12.

Xinghua Lou and Fred A Hamprecht. 2012. Struc-
tured learning from partial annotations. In Proc. of
the International Conference on Machine Learning
(ICML), pages 371–378.

Mitchell P Marcus, Mary Marcinkiewicz, and Beatrice
Santorini. 1993. Building a large annotated corpus
of English: The Penn Treebank. Computational Lin-
guistics, 19(2):313–330.

Julian Michael, Gabriel Stanovsky, Luheng He, Ido
Dagan, and Luke Zettlemoyer. 2017. Crowdsourc-
ing question-answer meaning representations. arXiv
preprint arXiv:1711.05885.

Graham Neubig and Shinsuke Mori. 2010. Word-based
partial annotation for efficient corpus construction.
In Proc. of the International Conference on Lan-
guage Resources and Evaluation (LREC). Citeseer.

Kamal Nigam, Andrew Kachites Mccallum, Sebastian
Thrun, and Tom Mitchell. 2000. Text classification
from labeled and unlabeled documents using EM.
Machine Learning, 39(2/3):103–134.

Qiang Ning, Zhili Feng, Hao Wu, and Dan Roth. 2018a.
Joint reasoning for temporal and causal relations.
In Proc. of the Annual Meeting of the Association
of Computational Linguistics (ACL), pages 2278–
2288.



2200

Qiang Ning, Hao Wu, and Dan Roth. 2018b. A multi-
axis annotation scheme for event temporal relations.
In Proc. of the Annual Meeting of the Association
of Computational Linguistics (ACL), pages 1318–
1328.

Qiang Ning, Zhongzhi Yu, Chuchu Fan, and Dan Roth.
2018c. Exploiting partially annotated data for tem-
poral relation extraction. In The Joint Conference
on Lexical and Computational Semantics (*Proc. of
the Joint Conference on Lexical and Computational
Sematics), pages 148–153.

Qiang Ning, Ben Zhou, Zhili Feng, Haoruo Peng, and
Dan Roth. 2018d. CogCompTime: A tool for under-
standing time in natural language. In Proc. of the
Conference on Empirical Methods for Natural Lan-
guage Processing (EMNLP).

Martha Palmer, Daniel Gildea, and Nianwen Xue.
2010. Semantic role labeling, volume 3.

Vasin Punyakanok and Dan Roth. 2001. The use of
classifiers in sequential inference. In Proc. of the
Conference on Neural Information Processing Sys-
tems (NIPS), pages 995–1001.

Vasin Punyakanok, Dan Roth, Wen tau Yih, and Dav
Zimak. 2005. Learning and inference over con-
strained output. In Proc. of the International Joint
Conference on Artificial Intelligence (IJCAI), pages
1124–1129.

Dan Roth. 2017. Incidental supervision: Moving be-
yond supervised learning. In Proc. of the Confer-
ence on Artificial Intelligence (AAAI).

Dan Roth and Kevin Small. 2006a. Active learning
with perceptron for structured output. In Proc. of
the International Conference on Machine Learning
(ICML).

Dan Roth and Kevin Small. 2006b. Margin-based ac-
tive learning for structured output spaces. In Proc.
of the European Conference on Machine Learning
(ECML).

Dan Roth and Kevin Small. 2008. Active learning for
pipeline models. In Proc. of the National Confer-
ence on Artificial Intelligence (AAAI).

Andrew M Saxe, Yamini Bansal, Joel Dapello, Madhu
Advani, Artemy Kolchinsky, Brendan D Tracey, and
David D Cox. 2018. On the information bottleneck
theory of deep learning. In Proc. of the International
Conference on Learning Representations (ICLR).

H Scudder. 1965. Probability of error of some adap-
tive pattern-recognition machines. IEEE Transac-
tions on Information Theory, 11(3):363–371.

Ohad Shamir, Sivan Sabato, and Naftali Tishby. 2010.
Learning and generalization with the information
bottleneck. Theoretical Computer Science, 411(29-
30):2696–2711.

Ravid Shwartz-Ziv and Naftali Tishby. 2017. Opening
the black box of deep neural networks via informa-
tion. arXiv preprint arXiv:1703.00810.

Erik F. Tjong Kim Sang and Sabine Buchholz.
2000. Introduction to the CoNLL-2000 shared task:
Chunking. In Proceedings of the CoNLL-2000 and
LLL-2000, pages 127–132.

Yuta Tsuboi, Hisashi Kashima, Hiroki Oda, Shinsuke
Mori, and Yuji Matsumoto. 2008. Training condi-
tional random fields using incomplete annotations.
In Proc. the International Conference on Computa-
tional Linguistics (COLING), pages 897–904.

Naushad UzZaman, Hector Llorens, James Allen, Leon
Derczynski, Marc Verhagen, and James Pustejovsky.
2013. SemEval-2013 Task 1: TEMPEVAL-3: Eval-
uating time expressions, events, and temporal rela-
tions. *SEM, 2:1–9.

David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervied methods. In Proc. of
the Annual Meeting of the Association of Computa-
tional Linguistics (ACL).

Shujian Yu and Jose C Principe. 2018. Understanding
autoencoders with information theoretic concepts.
arXiv preprint arXiv:1804.00057.


