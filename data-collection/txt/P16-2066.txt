



















































Phrase Table Pruning via Submodular Function Maximization


Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 406–411,
Berlin, Germany, August 7-12, 2016. c©2016 Association for Computational Linguistics

Phrase Table Pruning via Submodular Function Maximization

Masaaki Nishino and Jun Suzuki and Masaaki Nagata
NTT Communication Science Laboratories, NTT Corporation
2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0237, Japan

{nishino.masaaki,suzuki.jun,nagata.masaaki}@lab.ntt.co.jp

Abstract

Phrase table pruning is the act of re-
moving phrase pairs from a phrase table
to make it smaller, ideally removing the
least useful phrases first. We propose a
phrase table pruning method that formu-
lates the task as a submodular function
maximization problem, and solves it by
using a greedy heuristic algorithm. The
proposed method can scale with input size
and long phrases, and experiments show
that it achieves higher BLEU scores than
state-of-the-art pruning methods.

1 Introduction

A phrase table, a key component of phrase-based
statistical machine translation (PBMT) systems,
consists of a set of phrase pairs. A phrase pair is a
pair of source and target language phrases, and is
used as the atomic translation unit. Today’s PBMT
systems have to store and process large phrase ta-
bles that contain more than 100M phrase pairs,
and their sheer size prevents PBMT systems for
running in resource-limited environments such as
mobile phones. Even if a computer has enough
resources, the large phrase tables increase turn-
around time and prevent the rapid development of
MT systems.

Phrase table pruning is the technique of remov-
ing ineffective phrase pairs from a phrase table
to make it smaller while minimizing the perfor-
mance degradation. Existing phrase table pruning
methods use different metrics to rank the phrase
pairs contained in the table, and then remove low-
ranked pairs. Metrics used in previous work are
frequency, conditional probability, and Fisher’s
exact test score (Johnson et al., 2007). Zens et
al. (2012) evaluated many phrase table pruning
methods, and concluded that entropy-based prun-

ing method (Ling et al., 2012; Zens et al., 2012)
offers the best performance. The entropy-based
pruning method uses entropy to measure the re-
dundancy of a phrase pair, where we say a phrase
pair is redundant if it can be replaced by other
phrase pairs. The entropy-based pruning method
runs in time linear to the number of phrase-pairs.
Unfortunately, its running time is also exponential
to the length of phrases contained in the phrase
pairs, since it contains the problem of finding an
optimal phrase alignment, which is known to be
NP-hard (DeNero and Klein, 2008). Therefore,
the method can be impractical if the phrase pairs
consist of longer phrases.

In this paper, we introduce a novel phrase ta-
ble pruning method that formulates and solves
the phrase table pruning problem as a submodu-
lar function maximization problem. A submodular
function is a kind of set function that satisfies the
submodularity property. Generally, the submod-
ular function maximization problem is NP-hard,
however, it is known that (1 − 1/e) optimal solu-
tions can be obtained by using a simple greedy al-
gorithm (Nemhauser et al., 1978). Since a greedy
algorithm scales with large inputs, our method can
be applicable to large phrase tables.

One key factor of the proposed method is its
carefully designed objective function that evalu-
ates the quality of a given phrase table. In this pa-
per, we use a simple monotone submodular func-
tion that evaluates the quality of a given phrase
table by its coverage of a training corpus. Our
method is simple, parameter free, and does not
cause exponential explosion of the computation
time with longer phrases. We conduct experiments
with two different language pairs, and show that
the proposed method shows higher BLEU scores
than state-of-the-art pruning methods.

406



2 Submodular Function Maximization

Let Ω be a base set consisting of M elements, and
g : 2Ω 7→ R be a set function that upon the input of
X ⊆ Ω returns a real value. If g is a submodular
function, then it satisfies the condition

g(X ∪ {x})− g(X) ≥ g(Y ∪ {x})− g(Y ) ,

where X,Y ∈ 2Ω, X ⊆ Y , and x ∈ Ω \ Y . This
condition represents the diminishing return prop-
erty of a submodular function, i.e., the increase in
the value of the function due to the addition of
item x to Y is always smaller than that obtained
by adding x to any subset X ⊆ Y . We say a sub-
modular function is monotone if g(Y ) ≥ g(X)
for any X,Y ∈ 2Ω satisfying X ⊆ Y . Since a
submodular function has many useful properties,
it appears in a wide range of applications (Kempe
et al., 2003; Lin and Bilmes, 2010; Kirchhoff and
Bilmes, 2014).

The maximization problem of a monotone sub-
modular function under cardinality constraints is
formulated as

Maximize g(X)

Subject to X ∈ 2Ω and |X| ≤ K ,

where g(X) is a monotone submodular function
and K is the parameter that defines maximum car-
dinality. This problem is known to be NP-hard, but
a greedy algorithm can find an approximate solu-
tion whose score is certified to be (1 − 1/e) opti-
mal (Nemhauser et al., 1978). Algorithm 1 shows
a greedy approximation method the can solve the
submodular function maximization problem under
cardinality constraints. This algorithm first sets
X ← ∅, and adds item x∗ ∈ Ω \ X that maxi-
mizes g(X ∪ {x∗})− g(X) to X until |X| = K.

Assuming that the evaluation of g(X) can be
performed in constant time, the running time of
the greedy algorithm is O(MK) because we need
O(M) evaluations of g(X) for selecting x∗ that
maximizes g(X ∪ {x∗}) − g(X), and these eval-
uations are repeated K times. If we naively apply
the algorithm to situations where M is very large,
then the algorithm may not work in reasonable
running time. However, an accelerated greedy
algorithm can work with large inputs (Minoux,
1978; Leskovec et al., 2007), since it can dras-
tically reduce the number of function evaluations
from MK. We applied the accelerated greedy al-
gorithm in the following experiments, and found it

Algorithm 1 Greedy algorithm for maximizing a
submodular function
Input: Base set Ω, cardinality K
Output: X ∈ 2Ω satisfying |X| = K.

1: X ← ∅
2: while |X| < K do
3: x∗ ← arg max

x∈Ω\X
g(X ∪ {x})− g(X)

4: X ← X ∪ {x∗}
5: output X

could solve the problems in 24 hours. Moreover,
further enhancement can be achieved by apply-
ing distributed algorithms (Mirzasoleiman et al.,
2013) and stochastic greedy algorithms (Mirza-
soleiman et al., 2015).

3 Phrase Table Pruning

We first define some notations. Let Ω =
{x1, . . . , xM} be a phrase table that has M phrase
pairs. Each phrase pair, xi, consists of a source
language phrase, pi, and a target language phrase,
qi, and is written as xi = 〈pi, qi〉. Phrases pi and
qi are sequences of words pi = (pi1, . . . , pi|pi|)
and qi = (qi1, . . . , qi|qi|), where pij represents the
j-th word of pi and qij represents the j-th word
of qi. Let ti be the i-th translation pair contained
in the training corpus, namely ti = 〈fi, ei〉, where
fi and ei are source and target sentences, respec-
tively. Let N be the number of translation pairs
contained in the corpus. fi and ei are represented
as sequences of words fi = (fi1, . . . , fi|fi|) and
ei = (ei1, . . . , ei|ei|), where fij is the j-th word of
sentence fi and eij is the j-th word of sentence ei.

Definition 1. Let xj = 〈pj , qj〉 be a phrase pair
and ti = 〈fi, ei〉 be a translation pair. We say xj
appears in ti if pj is contained in fi as a subse-
quence and qj is contained in ei as a subsequence.
We say phrase pair xj covers word fik if xj ap-
pears in 〈fi, ei〉 and fik is contained in the subse-
quence that equals pj . Similarly, we say xj covers
eik if xj appears in 〈fi, ei〉 and eik is contained in
the subsequence that equals qj .

Using the above definitions, we describe here
our phrase-table pruning algorithm; it formulates
the task as a combinatorial optimization problem.
Since phrase table pruning is the problem of find-
ing a subset of Ω, we formulate the problem as a
submodular function maximization problem under
cardinality constraints, i.e., the problem is finding

407



X ⊆ Ω that maximizes objective function g(X)
while satisfying the condition |X| = K, where
K is the size of pruned phrase table. If g(X) is
a monotone submodular function, we can apply
Algorithm 1 to obtain an (1 − 1/e) approximate
solution. We use the following objective function.

g(X) =
N∑

i=1

|fi|∑
k=1

log [c(X, fik) + 1]

+
N∑

i=1

|ei|∑
k=1

log [c(X, eik) + 1] ,

where c(X, fik) is the number of phrase pairs con-
tained in X that cover fik, the k-th word of the i-
th source sentence fi. Similarly, c(X, eik) is the
number of phrase pairs that cover eik.

Example 1. Consider phrase table X holding
phrase pairs x1 = 〈(das Haus), (the house)〉,
x2 = 〈(Haus), (house)〉, and x3 =
〈(das Haus), (the building)〉. If a corpus
consists of a pair of sentences f1 = “das Haus ist
klein” and e1 = “this house is small”, then x1 and
x2 appear in 〈f1, e1〉 and word f12 = “Haus” is
covered by x1 and x2. Hence c(X, f12) = 2.

This objective function basically gives high
scores to X if it contains many words of the train-
ing corpus. However, since we take the logarithm
of cover counts c(X, fik) and c(X, eik), g(X) be-
comes high when X covers many different words.
This objective function prefers to select phrase
pairs that frequently appear in the training corpus
but with low redundantly. This objective function
prefers pruned phrase tableX that contains phrase
pairs that frequently appear in the training corpus,
with no redundant phrase pairs. We prove the sub-
modularity of the objective function below.

Proposition 1. g(X) is a monotone submodular
function.

Proof. Apparently, every c(X, fik) and c(X, eik)
is a monotone function ofX , and it satisfies the di-
minishing return property since c(X ∪{x}, fik)−
c(X, fik) = c(Y ∪ {x}, fik) − c(Y, fik) for any
X ⊆ Y and x 6∈ Y . If function h(X) is mono-
tone and submodular, then φ(h(X)) is also mono-
tone and submodular for any concave function
φ : R 7→ R. Since log(X) is concave, every
log[c(X, fik)+1] and log[c(X, eik)+1] is a mono-
tone submodular function. Finally, if h1, . . . , hn
are monotone and submodular, then

∑
i hi is also

monotone and submodular. Thus g(X) is mono-
tone and submodular.

Computation costs If we know all counts
c(X, fik) and c(X, eik) for all fik, eik, then g(X∪
{x}) can be evaluated in time linear with the num-
ber of words contained in the training corpus1.
Thus our algorithm does not cause exponential
explosion of the computation time with longer
phrases.

4 Evaluation

4.1 Settings

We conducted experiments on the Chinese-
English and Arabic-English datasets used in NIST
OpenMT 2012. In each experiment, English was
set as the target language. We used Moses (Koehn
et al., 2007) as the phrase-based machine transla-
tion system. We used the 5-gram Kneser-Ney lan-
guage model trained separately using the English
GigaWord V5 corpus (LDC2011T07), a monolin-
gual corpus distributed at WMT 2012, and Google
Web 1T 5-gram data (LDC2006T13). Word
alignments are obtained by running giza++ (Och
and Ney, 2003) included in the Moses sys-
tem. As the test data, we used 1378 segments
for the Arabic-English dataset and 2190 seg-
ments for the Chinese-English dataset, where all
test segments have 4 references (LDC2013T07,
LDC2013T03). The tuning set consists of about
5000 segments gathered from MT02 to MT06
evaluation sets (LDC2010T10, LDC2010T11,
LDC2010T12, LDC2010T14, LDC2010T17). We
set the maximum length of extracted phrases to 7.
Table 1 shows the sizes of phrase tables. Follow-
ing the settings used in (Zens et al., 2012), we
reduce the effects of other components by using
the same feature weights obtained by running the
MERT training algorithm (Och, 2003) on full size
phrase tables and tuning data to all pruned tables.
We run MERT for 10 times to obtain 10 differ-
ent feature weights. The BLEU scores reported
in the following experiments are the averages of
the results obtained by using these different fea-
ture weights.

We adopt the entropy-based pruning method
used in (Ling et al., 2012; Zens et al., 2012) as
the baseline method, since it shows best BLEU

1Running time can be further reduced if we compute the
set of words covered by each phrase pair xi before executing
the greedy algorithm.

408



Language Pair Number of phrase pairs
Arabic-English 234M

Chinese-English 169M

Table 1: Phrase table sizes.

scores as per (Zens et al., 2012). We used the pa-
rameter value of the entropy-based method sug-
gested in (Zens et al., 2012). We also compared
with the significance-based method (Johnson et
al., 2007), which uses Fisher’s exact test to calcu-
late significance scores of phrase pairs and prunes
less-significant phrase pairs.

4.2 Results

Figure 1 and Figure 2 show the BLEU scores of
pruned tables. The horizontal axis is the number of
phrase pairs contained in a table, and the vertical
axis is the BLEU score. The values in the figure
are difference of BLEU scores between the pro-
posed method and the baseline method that shows
higher score. In the experiment with the Arabic-
English dataset, both methods can remove 80% of
phrase pairs without losing 1 BLEU point, and the
proposed method shows better performance than
the baseline methods for all table sizes. The differ-
ence in BLEU scores becomes larger when table
sizes are small. In the experiment on the Chinese-
English dataset, both methods can remove 80% of
phrase pairs without losing 1 BLEU point, and the
proposed method also shows comparable or better
performance. The difference in BLEU scores also
becomes larger when table sizes are small.

Figure 3 shows phrase table sizes in the bina-
rized and compressed phrase table format used in
Moses (Junczys-Dowmunt, 2012). The horizon-
tal axis is the number of phrase pairs contained in
the table, and the vertical axis is phrase table size.
We can see that there is a linear relationship be-
tween phrase table sizes and the number of phrase
pairs. The original phrase table requires 2.8GB
memory. In contrast, the 90% pruned table only
requires 350MB of memory. This result shows the
effectiveness of phrase table pruning on reducing
resource requirements in practical situations.

5 Related Work

Previous phrase table pruning methods fall into
two groups. Self-contained methods only use
resources already used in the MT system, e.g.,
training corpus and phrase tables. Entropy-based

100 101 102 103

Phrase pairs [M]

32

34

36

38

40

B
LE

U
[%

]

0.87

0.87

0.49
0.19

0.02
0.12

0.05

0.00

Proposed
Entropy
Fisher

Figure 1: BLEU score as a function of the number
of phrase pairs (Arabic-English).

100 101 102 103

Phrase pairs [M]

18

20

22

24

26

28

B
LE

U
[%

]
0.40

0.39

0.32
0.09

0.10 -0.14
-0.03 0.00

Proposed
Entropy
Fisher

Figure 2: BLEU score as a function of the number
of phrase pairs (Chinese-English).

methods (Ling et al., 2012; Zens et al., 2012), a
significance-based method (Johnson et al., 2007),
and our method are self-contained methods. Non
self-contained methods exploit usage statistics for
phrase pairs (Eck et al., 2007) and additional bilin-
gual corpora (Chen et al., 2009). Since self con-
tained methods require additional resources, it is
easy to apply to existing MT systems.

Effectiveness of the submodular functions max-
imization formulation is confirmed in various NLP
applications including text summarization (Lin
and Bilmes, 2010; Lin and Bilmes, 2011)
and training data selection for machine transla-
tion (Kirchhoff and Bilmes, 2014). These methods
are used for selecting a subset that contains impor-
tant items but not redundant items. This paper can
be seen as applying the subset selection formula-
tion to the phrase table pruning problem.

6 Conclusion

We have introduced a method that solves the
phrase table pruning problem as a submodular
function maximization problem under cardinal-

409



100 101 102 103

Phrase pairs [M]

101

102

103

104

Ta
bl

e
si

ze
[M

B
]

Proposed
Entropy
Fisher

Figure 3: Moses compact phrase table size as a
function of the number of phrase pairs (Arabic-
English).

ity constraints. Finding an optimal solution of
the problem is NP-hard, so we apply a scalable
greedy heuristic to find (1 − 1/e) optimal solu-
tions. Experiments showed that our greedy al-
gorithm, which uses a relatively simple objec-
tive function, can achieve better performance than
state-of-the-art pruning methods.

Our proposed method can be easily extended by
using other types of submodular functions. The
objective function used in this paper is a simple
one, but it is easily enhanced by the addition of
metrics used in existing phrase table pruning tech-
niques, such as Fisher’s exact test scores and en-
tropy scores. Testing such kinds of objective func-
tion enhancements is an important future task.

References
Yu Chen, Martin Kay, and Andreas Eisele. 2009. In-

tersecting multilingual data for faster and better sta-
tistical translations. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics (NAACL-HLT), pages
128–136.

John DeNero and Dan Klein. 2008. The complex-
ity of phrase alignment problems. In Proceedings
of the 46th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies (ACL-HLT), pages 25–28.

Matthias Eck, Stephan Vogel, and Alex Waibel. 2007.
Translation model pruning via usage statistics for
statistical machine translation. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics (NAACL-HLT), pages 21–24.

Howard Johnson, Joel Martin, George Foster, and
Roland Kuhn. 2007. Improving translation qual-

ity by discarding most of the phrasetable. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 967–975.

Marcin Junczys-Dowmunt. 2012. Phrasal rank-
encoding: Exploiting phrase redundancy and trans-
lational relations for phrase table compression. The
Prague Bulletin of Mathematical Linguistics, 98:63–
74.

David Kempe, Jon Kleinberg, and Éva Tardos. 2003.
Maximizing the spread of influence through a social
network. In Proceedings of the 9th ACM SIGKDD
international conference on Knowledge discovery
and data mining (KDD), pages 137–146.

Katrin Kirchhoff and Jeff Bilmes. 2014. Submod-
ularity for data selection in machine translation.
In Proceedings of the 2014 Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 131–141.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics Com-
panion Volume Proceedings of the Demo and Poster
Sessions, pages 177–180.

Jure Leskovec, Andreas Krause, Carlos Guestrin,
Christos Faloutsos, Jeanne VanBriesen, and Natalie
Glance. 2007. Cost-effective outbreak detection in
networks. In Proceedings of the 13th ACM SIGKDD
International Conference on Knowledge Discovery
and Data Mining (KDD), pages 420–429.

Hui Lin and Jeff Bilmes. 2010. Multi-document sum-
marization via budgeted maximization of submod-
ular functions. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics (NAACL-HLT), pages 912–920.

Hui Lin and Jeff Bilmes. 2011. A class of submodu-
lar functions for document summarization. In Pro-
ceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies (ACL-HLT), pages 510–520.

Wang Ling, João Graça, Isabel Trancoso, and Alan
Black. 2012. Entropy-based pruning for phrase-
based machine translation. In Proceedings of
the 2012 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 962–971.

Michel Minoux. 1978. Accelerated greedy algorithms
for maximizing submodular set functions. In Pro-
ceedings of the 8th IFIP Conference on Optimization
Techniques, pages 234–243.

410



Baharan Mirzasoleiman, Amin Karbasi, Rik Sarkar,
and Andreas Krause. 2013. Distributed submodular
maximization: Identifying representative elements
in massive data. In Advances in Neural Information
Processing Systems (NIPS), pages 2049–2057.

Baharan Mirzasoleiman, Ashwinkumar Badanidiyuru,
Amin Karbasi, Jan Vondrák, and Andreas Krause.
2015. Lazier than lazy greedy. In Proceedings of
the 29th AAAI Conference on Artificial Intelligence
(AAAI), pages 1812–1818.

George L Nemhauser, Laurence A Wolsey, and Mar-
shall L Fisher. 1978. An analysis of approximations
for maximizing submodular set functionsi. Mathe-
matical Programming, 14(1):265–294.

Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19–51.

Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 160–167.

Richard Zens, Daisy Stanton, and Peng Xu. 2012. A
systematic comparison of phrase table pruning tech-
niques. In Proceedings of the 2012 Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL), pages 972–983.

411


