



















































Identifying Teacher Questions Using Automatic Speech Recognition in Classrooms


Proceedings of the SIGDIAL 2016 Conference, pages 191–201,
Los Angeles, USA, 13-15 September 2016. c©2016 Association for Computational Linguistics

Identifying Teacher Questions Using  
Automatic Speech Recognition in Classrooms 

 
Nathaniel Blanchard1, Patrick J. Donnelly1, Andrew M. Olney2, Borhan Samei2,  

Brooke Ward3, Xiaoyi Sun3, Sean Kelly4, Martin Nystrand3, & Sidney K. D’Mello1 
1University of Notre Dame; 2University of Memphis;  

3University of Wisconsin-Madison; 4University of Pittsburgh 
384 Fitzpatrick Hall 

Notre Dame, IN 46646, USA 
nblancha|sdmello@nd.edu 

  

Abstract 

We investigate automatic question detection 
from recordings of teacher speech collected 
in live classrooms. Our corpus contains audio 
recordings of 37 class sessions taught by 11 
teachers. We automatically segment teacher 
speech into utterances using an amplitude en-
velope thresholding approach followed by 
filtering non-speech via automatic speech 
recognition (ASR). We manually code the 
segmented utterances as containing a teacher 
question or not based on an empirically-vali-
dated scheme for coding classroom dis-
course. We compute domain-independent 
natural language processing (NLP) features 
from transcripts generated by three ASR en-
gines (AT&T, Bing Speech, and Azure 
Speech). Our teacher-independent supervised 
machine learning model detects questions 
with an overall weighted F1 score of 0.59, a 
51% improvement over chance. Furthermore, 
the proportion of automatically-detected 
questions per class session strongly correlates 
(Pearson’s r = 0.85) with human-coded ques-
tion rates. We consider our results to reflect a 
substantial (37%) improvement over the 
state-of-the-art in automatic question detec-
tion from naturalistic audio. We conclude by 
discussing applications of our work for teach-
ers, researchers, and other stakeholders. 

1 Introduction 
Questions are powerful tools that can inspire 
thought and inquiry at deeper levels of compre-
hension (Graesser and Person, 1994; Beck et al., 
1996). There is a large body of work supporting a 
positive relationship between the use of certain 
types of questions with increased student engage-
ment and achievement (Applebee et al., 2003; 
Kelly, 2007). But not all questions are the same. 
Questions that solicit surface-level facts (called 

test questions) are far less predictive of achieve-
ment compared to more open-ended (or dialogic) 
questions (Nystrand and Gamoran, 1991; 
Gamoran and Nystrand, 1991; Applebee et al., 
2003; Nystrand, 2006). 

Fortunately, providing teachers with training 
and feedback on their use of instructional prac-
tices (including question-asking) can help them 
adopt techniques known to be associated with stu-
dent achievement (Juzwik et al., 2013). However, 
automatic computational methods are required to 
analyze classroom instruction on a large scale. 
Although there are well-known coding schemes 
for manual coding of questions in classroom envi-
ronments (Nystrand et al., 2003; Stivers and En-
field, 2010) research on automatically identifying 
these questions in live classrooms is in its infancy 
and is the focus of this work. 

1.1 Related Work 
To keep scope manageable, we limit our review 
of previous work to question detection from auto-
matic speech recognition (ASR) since the use of 
ASR transcriptions, rather than human transcrip-
tions, is germane to the present problem. 

Boakye et al. (2009) trained models to detect 
questions in office meetings. The authors used the 
ICSI Meeting Recorder Dialog Act (MRDA) cor-
pus, a set of 75 hour-long meetings recorded with 
headset and lapel microphones. Their ASR system 
achieved a word error rate (WER), a measure of 
edit distance comparing the hypothesis to the orig-
inal transcript, of 0.38 on the corpus. They trained 
an AdaBoost classifier to detect questions from 
word, part-of-speech, and parse tree features de-
rived from the ASR transcriptions, achieving F1 
scores of 0.52, 0.35, and 0.50, respectively, and 
0.50 combined. Adding contextual and acoustic 
features slightly improved the F1 score to 0.54, 

191



suggesting the importance of linguistic (as op-
posed to contextual or acoustic) information for 
question detection. 

Stolcke et al. (2000) built a dialogue act tagger 
on the conversational Switchboard corpus using 
ASR transcripts (WER 0.41). A Bayesian model, 
trained on likelihoods of word trigrams from ASR 
transcriptions, detected 42 dialogue acts with an 
accuracy of 65% (chance level 35%; human 
agreement 84%). Dialogue acts such as state-
ments, questions, apologies, or agreement were 
among those tagged. Limiting the models to con-
sider only the highest-confidence transcription 
(the 1-best ASR transcript) resulted in a 62% ac-
curacy with the bigram discourse model. Addi-
tionally, the authors noted a 21% decrease in clas-
sification error when human transcripts were used 
instead.  

Stolcke et al. (2000) also attempted to leverage 
prosody to distinguish yes-no questions from 
statements, dialogue acts which may be ambigu-
ous based on transcripts alone. On a selected sub-
set of their corpus containing an equal proportion 
of questions and statements they achieved an ac-
curacy of 75% using transcripts (chance 50%). 
Adding prosodic features increased their accuracy 
to 80%. 

Orosanu and Jouvet (2015) investigated dis-
crimination between statements and questions 
from ASR transcriptions from three French-lan-
guage corpora. Their training set consisted of 
10,077 statements and 10,077 questions, and their 
testing set consisted of 7,005 statements and 831 
questions. Using human transcriptions, the mod-
els classified 73% of questions and 78% of state-
ments correctly. When the authors tested the same 
model against ASR transcriptions, they observed 
a 3% reduction in classification accuracy.  
The authors also compared their datasets based on 
differences in speaking styles.  One corpus con-
sisted of unscripted, spontaneous speech from 
news broadcasts (classification accuracy 70%; 
WER 22%), while the other contained scripted di-
alogue from radio and TV channels (classification 
accuracy 73%; WER 28%).  

All the aforementioned studies have used man-
ually-defined sentence boundaries. However, a 
fully-automatic system for question detection 
would need to detect sentence boundaries without 
manual input. Orosanu and Jouvet (2015) simu-
lated imperfect sentence boundary detection using 
a semi-automatic method. They substituted sen-
tence boundaries defined by human-annotated 
punctuation with boundaries based on silence in 
the audio. When punctuation aligned with silence, 

the boundaries were left unchanged from the man-
ually-defined boundaries. This semi-automatic 
approach to segmentation resulted in a 3% in-
crease in classification errors.   

Finally, in preliminary precursor to this work, 
we explored the potential for question detection in 
classrooms from automatically-segmented utter-
ances that were transcribed by humans (Blanchard 
et al., 2016). We used 1,000 random utterances 
from our current corpus which we manually tran-
scribed and coded as containing a question or not 
(see Section 2.3). Using leave-one-speaker-out 
cross-validation, we achieved an overall-weighted 
F1 score of 0.66, with an F1 of 0.53 for the ques-
tion class. That work showed that question detec-
tion was possible from noisy classroom audio, al-
beit with human transcriptions.  

1.2 Challenges, Contributions, and Novelty 
We describe a novel question detection scenario 
in which we automatically identify teacher ques-
tions using ASR transcriptions of teacher speech 
in a real-world classroom environment. We have 
previously identified numerous constraints that 
need to be satisfied in order to facilitate question 
detection at scale. Such a system must be afforda-
ble, cannot be disruptive to either the teacher or 
the students, and must maintain student privacy, 
which precludes recording or filming individual 
students. Therefore, we primarily rely on a low-
cost, wireless headset microphone for recording 
teachers as they move about the classroom freely. 
This approach accommodates various seating ar-
rangements, classroom sizes, and room layouts, 
and attempts to minimize ambient classroom 
noise, muffled speech, or classroom interruptions, 
all factors that reflect the reality of real-world en-
vironments.  

There are a number of challenges with this 
work. For one, teacher questions in a classroom 
differ from traditional question-asking scenarios 
(e.g., meetings, informal conversations) where the 
goal of a question is to elicit information and the 
questioner usually does not know the answer 
ahead of time. In contrast, rather than infor-
mation-seeking, the key goal of teacher questions 
is to assess knowledge and to prime thought and 
discussion (Nystrand et al., 2003), thereby intro-
ducing difficulties in coding questions them-
selves. 

We note that ASR on classroom speech is par-
ticularly challenging given the noisy environment 
that includes classroom disruptions, accidental 
microphone contact, and sounds from students, 
chairs, and desks. Previous work on this data 

192



yielded WERs ranging from 0.34 to 0.60 
(D’Mello et al., 2015), suggesting that we have to 
contend with rather inaccurate transcripts.  

In addition, previous work reviewed in Section 
1.1 has focused on human-segmented speech, 
which is untenable for a fully-automated system. 
Therefore, our approach uses an automated ap-
proach to segment speech, which itself is an im-
perfect process.  

This imperfect pipeline ranging from question 
coding to ASR to utterance segmentation accu-
rately illustrates the difficulties of detecting ques-
tions in real-world environments. Nevertheless, 
we make several novel contributions while ad-
dressing these challenges. First, we implement 
fully automated methods to process teacher audio 
into segmented utterances from which we obtain 
ASR transcriptions. Second, we combine tran-
scriptions from multiple ASR engines to offset the 
inevitable errors associated with automatically 
segmenting and transcribing teacher audio. Third, 
we restrict our feature set to domain-independent 
natural language features that are more likely to 
generalize across different school subjects.  Fi-
nally, we use leave-one-teacher-out cross-valida-
tion so that our models generalize across teachers 
rather than optimizing for individual teachers. 

The remainder of the paper is organized as fol-
lows.  First, we discuss our data collection meth-
ods, data pre-processing, feature extraction ap-
proach, and our classification models in Section 2.  
In Section 3, we present our experiments and re-
view key results. We next discuss the implications 
of our findings and conclude with our future re-
search directions in Section 4. 

2 Method 
2.1 Data Collection 
Data was collected at six rural Wisconsin middle 
schools during literature, language arts, and civics 
classes taught by 11 different teachers (three 
male; eight female). Class sessions lasted between 
30 and 90 minutes, depending on the school. A to-
tal of 37 classroom sessions were recorded and 
live-coded on 17 separate days over a period of a 
year, totaling 32:05 hours of audio. 

Each teacher wore a wireless microphone to 
capture their speech. Based on previous work 
(D’Mello et al., 2015), a Samson 77 Airline wire-
less microphone was chosen for its portability, 
noise-canceling properties, and low-cost. The 
teacher’s speech was captured and saved as a 16 
kHz, 16-bit single channel audio file.  

2.2 Teacher Utterance Extraction 
Teacher speech was segmented into utterances us-
ing a two-step voice activity detection (VAD) al-
gorithm (Blanchard et al., 2015). First, the ampli-
tude envelope of the teacher’s low-pass filtered 
speech was passed through a threshold function in 
20-millisecond increments. Where the amplitude 
envelope was above threshold, the teacher was con-
sidered to be speaking. Any time speech was de-
tected, that speech was considered part of a poten-
tial utterance, meaning there was no minimum 
threshold for how short a potential utterance could 
be. Potential utterances were coded as complete 
when no speech was detected for 1,000 millisec-
onds (1 second).  

The thresholds were set low to ensure capture 
of all speech, but this also caused a high rate of 
false alarms in the form of non-speech utterances. 
These false alarms were filtered from the set of 
potential utterances with the Bing ASR engine 
(Microsoft, 2014). If the ASR engine rejected a 
potential utterance then it was determined to not 
contain any speech. Additionally, any utterances 
less than 125 milliseconds was removed, as this 
speech was not considered meaningful. 

We empirically validated the effectiveness of 
this utterance detection approach by manual cod-
ing a random subset of 1,000 potential utterances 
as either containing speech or not. We achieved 
high levels of both precision (96.3%) and recall 
(98.6%) and an F1 score of 0.97. We applied this 
approach to the full corpus to extract 10,080 utter-
ances from the 37 classroom recordings.  

2.3 Question Coding 
One limitation of automatically segmented speech 
is that each utterance may contain multiple ques-
tions, or conversely, a question may be spread 
across multiple utterances (Komatani et al., 2015). 
This occurs partly because we use both a static 
amplitude envelope threshold and a constant 
pause length to segment utterances rather than 
learning specific thresholds for each teacher. 
However, the use of a single threshold increases 
generalizability to new teachers. Regardless of 
method, voice activity detection is not a fully-
solved problem and any method is expected to 
yield some errors. 

To address this, we manually coded the 10,080 
extracted utterances as “containing a question” or 
“not containing a question” rather than “question” 
or “statement.” The distinction, though subtle, in-
dicated that a question phrase that is embedded 

193



within a large utterance would be coded as “con-
taining a question.” Conversely, we also ensured 
that if a question spans adjacent utterances then 
each utterance would be coded as “containing a 
question.” We also do not distinguish among dif-
ferent questions types in this initial work.  

Our definition of “question” follows coding 
schemes that are uniquely designed to analyze 
questions  in classroom discourse (Nystrand et al., 
2003). Questions are utterances in which the 
teacher solicits information from a student either 
procedurally (e.g., “Is everyone ready?”), rhetori-
cally (e.g., “Oh good idea James why don’t we just 
have recess instead of class today”), or for 
knowledge assessment/information solicitation 
purposes (e.g., “What is the capital of Indiana, 
Michael?”).  Likewise, the teacher calling on a 
different student to answer the same question 
(e.g., “Nope. Shelby?”) would also be considered 
a question, although in some coding schemes, the 
previous example would be classified as “Turn 
Eliciting” (Allwood et al., 2007). We do not con-
sider certain cases questions, such as when the 
teacher calls on a student for other reasons (e.g., 
to discipline them) or when the teacher reads from 
a novel in which a character asked a question. 

The coders were seven research assistants and 
researchers whose native language was English. 
The coders first engaged in a training task by la-
beling a common evaluation set of 100 utterances. 
These 100 utterances were manually selected to 
exemplify difficult cases. Once coding of the eval-
uation set was completed, the expert coder, who 
had considerable expertise with classroom dis-
course and who initially selected and coded the 
evaluation set, reviewed the codes. Coders were 
required to achieve a minimal level of agreement 
with the expert coder (Cohen’s kappa, κ	= 0.80). 
If the agreement was lower than 0.80, then errors 
were discussed with the coders. 

After this training task was completed, the cod-
ers coded a subset of utterances from the complete 
dataset. Coders listened to the utterances in tem-
poral order and assigned a code (question or not) 
to each based on the words spoken by the teacher, 
the teachers’ tone (e.g., prosody, inflection), and 
the context of the previous utterance. Coders 
could also flag an utterance for review by a pri-
mary coder, although this was rare. In all, 36% of 
the 10,080 utterances were coded as containing 
questions. A random subset of 117 utterances 
from the full dataset were selected and coded by 
the expert coder. Overall the coders and the pri-
mary coder obtained an agreement of κ	= 0.85.  

2.4 Automatic Speech Recognition (ASR) 
We used the Bing and AT&T Watson ASR sys-
tems (Microsoft, 2014; Goffin et al., 2005), based 
on evaluation in previous work (Blanchard, 2015; 
D’Mello et al., 2015). For both of these systems, 
individual utterances were submitted to the engine 
for transcription. We also considered the Azure 
Speech API (Microsoft, 2016) which processes a 
full-length classroom recording to produce a set 
of time-stamped words, from which we recon-
structed the individual utterances. 

We evaluated the performance of the ASR en-
gines on a random subset of 1,000 utterances. We 
considered two metrics: word error rate (WER), 
which accounts for word order between ASR and 
human transcripts, and simple word overlap 
(SWO), a metric that does not consider word or-
der. WER was computed by summing the number 
of substitutions, deletions, and insertions required 
to transform the human transcript into the com-
puter transcript, divided by the number of words 
in the human transcript. SWO was computed by 
dividing the number of words that appear in both 
the human and computer transcripts by the num-
ber of words in the human transcript. Table 1 pre-
sents the WER and SWO for the three ASR sys-
tems, where we note moderate accuracy given the 
complexity of the task in that we are processing 
conversational speech recorded in a noisy natural-
istic environment.  

 
Table 1. ASR word error rate and simple word over-

lap averaged by teacher for 1,000 utterances, with stand-
ard deviations shown in parentheses. 

ASR WER  SWO 
Bing Speech 0.45 (0.10) 0.55 (0.06) 
AT&T Watson 0.63 (0.11) 0.42 (0.11) 
Azure Speech 0.49 (0.07) 0.64 (0.16) 
 

2.5 Model Building 
We trained supervised classification models to 
predict if utterances contained a question or not 
(as defined in Section 2.3).      

Feature extraction. In this work we focused 
on a small set of domain-general features rather 
than word specific models, such as n-grams or 
parse trees. Because we sampled many different 
teachers and classes, the topics covered vary sig-
nificantly between class sessions, and a content-
heavy approach would likely overfit to specific 
topics. This decision helps emphasize generaliza-
bility across topics as our models are intended to 

194



be applicable to class sessions that discuss topics 
not covered in the training set. 

Features (N = 37) were generated using the 
ASR transcripts for each utterance obtained from 
Bing Speech, AT&T Watson, and Azure Speech 
engines. Of these, 34 features were obtained by 
processing each utterance with the Brill Tagger 
(Brill, 1992) and analyzing each token (Olney et 
al., 2003). Features included the presence or ab-
sence of certain words (e.g., what, why, how), cat-
egories of words (e.g., definition, comparison), or 
part-of-speech tags (e.g., presence of nouns, pres-
ence of adjectives). These features were previ-
ously used to detect domain-independent question 
properties from human-transcribed questions 
(Samei et al., 2014). We supplemented these fea-
tures with three additional features: proper nouns 
(e.g., student names), pronouns associated with 
uptake (teacher questions that incorporate student 
responses), and pronouns not associated with up-
take, as recommended by a domain expert on 
teacher questions.  

We extracted all 37 NLP features for each ASR 
transcription, yielding three feature sets. We also 
created a fourth set of NLP features that combined 
the features from the individual ASRs. For this 
set, each feature value was taken as the proportion 
of each features’ appearances in the three ASR 
outputs. For example, if a feature was present in 
an utterance as transcribed by Bing and AT&T, 
but not Azure, then the feature’s value would be 
0.67.  

Oversampling. We supplemented our imbal-
anced training data with synthetic instances (for 
the minority question class) generated with the 
Synthetic Minority Over-sampling Technique 
(SMOTE) algorithm (Chawla et al., 2011). Class 
distributions in the testing set were preserved.  

Classification and validation. We considered 
the following classifiers: logistic regression, ran-
dom forest, J48 decision tree, J48 with Bagging, 
Bayesian network, k-nearest neighbor (k = 7, 9, 
and 11), and J48 decision tree, using implementa-
tions from the WEKA toolkit (Hall et al., 2009). 
For each classifier, we tested with and without 
wrapping the classifiers with MetaCost, a cost-
sensitive procedure for imbalanced datasets that 
assigned a higher penalty (weights of 2 or 4) to 
misclassification of the question class. 

Classification models were validated using a 
leave-one-teacher-out cross-validation technique, 
in which models were built on data from 10 teach-
ers (training set) and validated on the held-out 
teacher (testing set). The process was repeated un-
til each teacher was included in the testing set. 

This cross-validation technique tests the potential 
of our models to generalize to unseen teachers 
both in terms of acoustic variability and in terms 
of variability in question asking. 

3 Results 
3.1 Classification Accuracy 
In Table 2 we present the best performing classi-
fication model for each ASR and their combina-
tion based on the F1 score for the question class 
(target metric). Table 2 includes the F1 score for 
the question class, the F1 score for the non-ques-
tion class, and the overall weighted F1 score. The 
best-performing individual ASR models were 
each Bayesian networks. The combined model 
was built with J48 with Bagging and with Meta-
Cost (miss weight of 2). We show the confusion 
matrix for this model in Table 3.  
 

Table 2. Results of best models for question detection. 

Model F1 
Question 

F1 Not-
Question 

F1 
Overall 

AT&T 0.52 0.68 0.63 
Azure 0.53 0.67 0.63 
Bing 0.54 0.67 0.63 
Combined 0.59 0.74 0.69 

 
Table 3. Confusion matrix of combined ASR model 

for Question (Q) and Utterances (U). 

n Actual Predicted 
  Q U 

3586 Q 2273 1313 
6494 U 1946 4548 

 
Overall, these results show a general con-

sistency between the models using individual 
ASR transcriptions, which imply the relative suc-
cess of each despite the differences in WER. Fur-
thermore, we note that the combination of three 
ASR transcriptions resulted in improved perfor-
mance compared to models built using individual 
ASR transcriptions. Using the combined model, 
we achieved slightly higher recall (0.63) than pre-
cision (0.57) for identifying questions.  

We also compared our results to a chance 
model that assigned the question label at the same 
rate (42%) as our model, but did so randomly 
across 10,000 iterations. We consider this ap-
proach to computing chance to be more informa-
tive than a naïve minority baseline model (as the 
class of interest is the minority class) that would 

195



yield perfect recall but negligible precision. The 
chance model had a mean recall of 0.42 and pre-
cision of 0.36 for the question class. From these 
averages, we calculated the chance F1 score for 
questions (0.39). Our combined model achieved 
an F1 score of 0.59 for the question class, which 
represents a 51% improvement over chance.  

3.2 Feature Analysis 
We explored the utility of the individual features 
using forward stepwise feature selection (Draper 
et al., 1966). For each individual ASR engine we 
identified the features selected in all folds of the 
teacher-level cross-validation procedure. We 
found four of the features were used in all three of 
the ASR models: how, what, why, and wh- (any 
word that starts with “wh-“, including who and 
where). The selection of these features across the 
different ASR feature sets is perhaps unsurprising, 
but these results confirm that identifying question 
words are paramount for detecting questions re-
gardless of the specific ASR engine.  

3.3 Consistency Across Class-Sessions 
The models were trained using leave-one-teacher-
out cross-validation, but we perform additional 
post-hoc analyses exploring the model’s accuracy 
across the 37 individual class sessions. This anal-
ysis allows an investigation of the stability of our 
model for individual class sessions, which will be 
essential for generalizability to future class ses-
sions and topics.  

Question Rate Analysis. Some applications 
only require an overall indication of the rate of 
question asking rather than identifying individual 
questions. To analyze the use of our model to 
these applications, we compared the proportion of 
predicted to actual questions for each class session 
(see Figure 1). There was a mean absolute differ-
ence of 0.08 (SD = 0.06) in the predicted propor-
tion of questions compared to the true proportion 
(Pearson’s r = 0.85). This small difference and 
strong correlation indicates that even though there 
are misclassifications at the level of individual ut-
terances, the error rate is ameliorated at the ses-
sion level, indicating the model performs well at 
correctly predicting the proportion of questions in 
a class session.  

Performance Across Class-Sessions. Figure 2 
presents a histogram of F1 scores for the question 
class by class session. We note that model accu-
racy was evenly spread across class sessions ra-
ther than being concentrated on the tails (which 
would indicate a skewed distribution). In particu-
lar, 25% of the class sessions scored below 0.47 

and 25% of the sessions scored above 0.66, yield-
ing an interquartile range of 0.47 to 0.66. Encour-
agingly, the poorest performing class session still 
yielded an F1 score of 0.33 while the best class 
session had a score of 0.84.  

 

 
Figure 1. Proportion of predicted to actual questions 

in each class session. 

 
Figure 2. Histogram of F1 scores for the question 

class by class-session.  

 
 

Figure 3. Models (ASR vs. human) built on 1,000    
utterance subset. 

 

3.4 Effects of ASR Errors 
We explored how our models were affected by 

ASR errors. We built models on the subset of 
1,000 utterances that we manually transcribed to 
evaluate WER and SWO of the ASRs in Section 
2.4. Specifically, we retrained the J48 classifier 
reported in Section 3.1 on this data subset, using 
the combination of features from the three indi-
vidual ASRs, comparing it to the same model built 

0.0

0.5

1.0

0.0 0.5 1.0

Ac
tu

al
 P

ro
po

rti
on

Predicted Proportion

0

4

7

0.35 0.45 0.55 0.65 0.75 0.85

Fr
eq

ue
nc

y

0.0

0.5

1.0

Question Non-Question Overall

F 1
Sc

or
e

ASR Human

196



Table 4. Confusion matrix showing a comparison of the ASR and Human models. 

using features extracted from the human transcrip-
tions. The results of leave-one-teacher-out cross-
validation are shown in Figure 3. 

We must note that direct comparisons of mod-
els built on this subset of 1,000 instances with 
those built on the full data set (Section 3.1) are 
improper due to significantly fewer training in-
stances in the former. In general, the human model 
achieved a higher F1 for the question class com-
pared to the combined ASR model, while the ASR 
model has a higher F1 for the non-question class. 
We also note the tendency of the human model to 
over-predict questions, potentially resulting from 
the use of the MetaCost wrapper. 

We further compared the predictions of the hu-
man and ASR models and observed that both 
models agreed in classifying utterances, either 
correctly or incorrectly, as questions and non-
questions 65% of the time (see Table 4). They dif-
fered 35% of the time, disagreeing 25% of the 
time for non-questions and 10% of the time for 
questions. We note that, when the models disa-
greed, the human model was more likely to clas-
sify a non-question as a question (16%) compared 
to the ASR (9%), presumably due to its tendency 
to over-predict questions as noted above. 

3.5 Analysis of Classification Errors 
We selected a random sample of 100 incorrectly 
classified utterances using the human transcrip-
tion model (so as to eliminate ASR errors as a po-
tential explanation) to study possible causes of er-
rors. We identified 44 utterances with common er-
ror patterns, whereas the cause of the error could 
not be easy discerned for the remaining 56 incor-
rectly classified utterances. 

Out of the 44 errors, 24 were misses (questions 
predicted as non-questions). In 5 of these 24 
misses, the question was only one part of the ut-
terance (e.g., “If I could just get this thing to open 
I’d be fine. Can you do it?”). The remaining 19 
errors yielded examples of question types that 
may be problematic for our model. These include 
calling on individual students (e.g., “Sam?”), rhe-
torical questions (e.g., “musical practice, 

right?”), implicit questions requiring clues from 
previous context (e.g., “why did she say that?”), 
fill-in-the-blank questions (e.g., “Madagascar 
and _______?”), and students being directed to 
speak, rather than being asked a traditional ques-
tion (e.g., “tell us about it”).  

Additionally, there were 20 false alarms (non-
questions incorrectly classified as questions). 
Nine of these non-questions were offhand/casual 
statements made by teachers (“I don’t know if you 
guys should call him that or not” said jokingly) 
while interacting with student, indicative of the 
difficulty of classifying questions in contexts with 
informal dialogue. Five short utterances may have 
been classified incorrectly because of limited con-
text (e.g., “good.” vs. “good?”, “okay.” vs. 
“okay?”). Three misclassifications involved 
teachers reading directly from a book, (e.g., quot-
ing a passage from a novel in which a character 
asks a question). Additionally, there was one 
aborted statement and one aborted question, in 
which the teacher started to say something but 
changed course mid-sentence (e.g., “No wh- … 
put that away!”). Finally, in another case, the 
teacher paused midsentence, resulting in a very 
short utterance that left the full intent of the state-
ment to the next utterance (e.g., “Juliet reversed 
course, the nurse…”). This last example high-
lights the difficulties of classifying questions with 
imperfect sentence boundaries (see Section 2.3) as 
is the case with our data. In general, 15 of the 20 
false alarms were associated with changes in 
speaking style from traditional teacher speech in 
classrooms.  

4 General Discussion 
The importance of teacher questions in class-
rooms is widely acknowledged in both policy 
(e.g., Common Core State Standards for Speaking 
and Listening (2010)) and research (Nystrand and 
Gamoran, 1991; Applebee et al., 2003; Nystrand 
et al., 2003). Teacher questions play a central role 
in student engagement and achievement, suggest-
ing that automating the detection of questions 

Actual Predicted 
  Human Q Human Q Human NQ Human NQ 
Priors  ASR Q ASR NQ ASR Q ASR NQ 
0.30 Q 0.15 0.07 0.03 0.05 
0.70 NQ 0.18 0.16 0.09 0.28 
Note: Q indicates a question and NQ indicates a non-question. Bolded numbers indicate both models 
agreed while italicized numbers indicate disagreement.  

197



might have important consequences for both re-
search on effective instructional strategies and on 
teacher professional development. Thus, our cur-
rent work centers on a fully-automated process for 
predicting teacher questions in a noisy real-world 
classroom environment, using only a full-length 
audio recording of teacher speech. 

4.1 Main Findings 
We present encouraging results with our auto-
mated processes, consisting of VAD to automati-
cally segment teacher speech, ASR transcriptions, 
NLP features, and machine learning. In particular, 
our question detection models excel in aggrega-
tion of utterances: the detected proportion of ques-
tions per class strongly correlates with the propor-
tion of actual questions in the classroom (Pear-
son’s r = 0.85). In addition, our models provided 
promising results in the detection of individual 
questions, although further refinement is needed. 
Both types of analysis are useful in providing 
formative feedback to teachers, at coarse- and 
fine-grained levels, respectively. 

A key contribution of our work over previous 
research is that our models were trained and tested 
on automatically-, and thus imperfectly-, seg-
mented utterances. This extends the work of (Oro-
sanu and Jouvet, 2015) which artificially explored 
perturbations of a subset of utterance boundaries 
using the automatic detection of silence within hu-
man-segmented spoken sentences. To our 
knowledge, our work is the first to detect spoken 
questions using a fully automated process. Our 
best model achieved an overall F1 score of 0.69 
and an F1 score of 0.59 for the question class. This 
represents a substantial 37% improvement in 
question detection accuracy over a recent state-of-
the-art model (Boakye et al., 2009) that reported 
an overall F1 of 0.50; the authors do not report F1 
for the question class so the comparison is based 
on the overall F1.  

We validated our models using leave-one-
teacher-out cross-validation, demonstrating gen-
eralizability of our approach across teachers in 
this dataset. Furthermore, we analyzed model per-
formance by class session, finding our model was 
consistent across class sessions, an encouraging 
result supporting our goals of domain-independ-
ent question detection.  

We also explored the differences between mod-
els using ASR transcriptions and using human 
transcriptions. Overall, the results were quite 
comparable suggesting that imperfect ASR need 
not be a barrier against automated question detec-
tion in live classrooms. 

4.2 Limitations and Future Work 
This study is not without limitations. We designed 
our approach to avoid overfitting to specific clas-
ses, teachers, or schools. However, all of our re-
cordings were collected in Wisconsin, a state that 
uses the Common Core standard. It is possible that 
the Common Core may impose aspects of a par-
ticular style of teaching that our models may over-
fit. Similarly, although we used speaker-inde-
pendent ASR and teacher-independent validation 
techniques to improve generalizability to new 
teachers, our sample of teachers are from a single 
region with traditional Midwestern accents and 
dialects. Therefore, broader generalizability 
across the U.S. and beyond remains to be seen.  

We acknowledge that our method for teacher 
utterance segmentation may potentially be im-
proved using proposed techniques in related 
works. Komatani et al. (2015) has explored de-
tecting and merging utterances segmented mid-
sentence, allowing analysis to take place on a full 
sentence, rather than a fragment, which may im-
prove question detection by merging instances in 
which questions were split. An alternative ap-
proach would be to automatically detect sentence 
boundaries within utterances, and extract features 
from each detected sentence. 

Our analysis of errors in Section 3.5 suggests 
that acoustic and contextual features may be 
needed to capture difficulty to classify questions. 
Additionally, related work on question detection 
(see Section 1.1) suggested that acoustic, contex-
tual, and temporal features (Boakye et al., 2009) 
may aid in the detection of questions. We will ex-
plore this in future work to determine if features 
capturing these properties will help improve our 
models for this task. Likewise, we will also ex-
plore temporal models, such as conditional ran-
dom fields and bi-directional long-short-term neu-
ral networks, which might better capture ques-
tions in the larger context of the classroom dia-
logue. This temporal analysis may help find se-
quences of consecutive questions, such as those 
present in question-and-answer sessions or in 
classroom discussions. 

Further, Raghu et al. (2015) has explored using 
context to identify non-sentential utterances 
(NSUs), defined as utterances that are not full sen-
tences but convey complete meaning in context. 
The identification of NSUs may improve our 
model’s ability to differentiate between difficult 
cases (e.g., calling on students, saying a student’s 
name for discipline).  

198



In addition to addressing these limitations by 
collecting a more representative corpus and com-
puting additional features, there are several other 
directions for future work. Specifically, we will 
focus on classifying question properties defined 
by Nystrand and Gameron (2003).  While we have 
explored these properties in previous work (Samei 
et al., 2014; Samei et al., 2015), that work used 
perfectly-segmented and human-transcribed ques-
tion text. We will continue this work using our 
fully-automatic approach that employs automatic 
segmentation and ASR transcriptions. 

4.3 Concluding Remarks 
We took steps towards fully-automated detection 
of teacher questions in noisy real-world classroom 
environments. The present contribution is one 
component of a broader effort to automate the col-
lection and coding of classroom discourse. The 
automated system is intended to catalyze research 
in this area and to generate personalized formative 
feedback to teachers, which enables reflection and 
improvement of their pedagogy, ultimately lead-
ing to increased student engagement and achieve-
ment. 

5 Acknowledgements 
This research was supported by the Institute of 
Education Sciences (IES) (R305A130030). Any 
opinions, findings and conclusions, or recom-
mendations expressed in this paper are those of 
the author and do not represent the views of the 
IES.   
 
References 

Jens Allwood, Loredana Cerrato, Kristiina Jok-
inen, Costanza Navarretta, and Patrizia Paggio. 
2007. The MUMIN coding scheme for the anno-
tation of feedback, turn management and sequenc-
ing phenomena. Language Resources and Evalu-
ation, 41(3–4):273–287. 

Arthur N Applebee, Judith A Langer, Martin Nys-
trand, and Adam Gamoran. 2003. Discussion-
based approaches to developing understanding: 
Classroom instruction and student performance in 
middle and high school English. American Edu-
cational Research Journal, 40(3):685–730. 

Isabel L. Beck, Margaret G. McKeown, Cheryl 
Sandora, Linda Kucan, and Jo Worthy. 1996. 

Questioning the author: A yearlong classroom im-
plementation to engage students with text. The El-
ementary School Journal:385–414. 

Nathaniel Blanchard, Patrick J Donnelly, Andrew 
M Olney, Borhan Samei, Brooke Ward, Xiaoyi 
Sun, Sean Kelly, Martin Nystrand, and Sidney K. 
D’Mello. 2016. Automatic detection of teacher 
questions from audio in live classrooms. In Pro-
ceedings of the 9th International Conference on 
Educational Data Mining (EDM 2016), pages 
288–291. International Educational Data Mining 
Society. 

Nathaniel Blanchard, Michael Brady, Andrew 
Olney, Marci Glaus, Xiaoyi Sun, Martin Nys-
trand, Borhan Samei, Sean Kelly, and Sidney K. 
D'Mello. 2015. A Study of automatic speech 
recognition in noisy classroom environments for 
automated dialog analysis. In Proceedings of the 
8th International Conference on Educational 
Data Mining (EDM 2015), pages 23-33. Interna-
tional Educational Data Mining Society. 

 
Nathaniel Blanchard, Sidney D’Mello, Martin 
Nystrand, and Andrew M. Olney. 2015. Auto-
matic classification of question & answer dis-
course segments from teacher’s speech in class-
rooms. In Proceedings of the 8th International 
Conference on Educational Data Mining (EDM 
2015), pages 282–288. International Educational 
Data Mining Society. 

Kofi Boakye, Benoit Favre, and Dilek Hakkani-
Tür. 2009. Any questions? Automatic question 
detection in meetings. In Proceedings of the IEEE 
Workshop on Automatic Speech Recognition & 
Understanding, (ASRU), pages 485–489. IEEE. 

Eric Brill. 1992. A simple rule-based part of 
speech tagger. In Proceedings of the Workshop on 
Speech and Natural Language, pages 112–116. 
Association for Computational Linguistics. 

Nitesh V. Chawla, Kevin W. Bowyer, Lawrence 
O. Hall, and W. Philip Kegelmeyer. 2011. 
SMOTE: synthetic minority over-sampling tech-
nique. Journal of Artificial Intelligence Research, 
16:321–357. 

Sidney K D’Mello, Andrew M Olney, Nathan 
Blanchard, Borhan Samei, Xiaoyi Sun, Brooke 
Ward, and Sean Kelly. 2015. Multimodal capture 
of teacher-student interactions for automated dia-
logic analysis in live classrooms. In Proceedings 

199



of the 2015 International Conference on Multi-
modal Interaction, pages 557–566. ACM. 

Norman Richard Draper, Harry Smith, and Eliza-
beth Pownell. 1966. Applied regression analysis. 
Wiley New York. 

Adam Gamoran and Martin Nystrand. 1991. 
Background and instructional effects on achieve-
ment in eighth-grade English and social studies. 
Journal of Research on Adolescence, 1(3):277–
300. 

Vincent Goffin, Cyril Allauzen, Enrico Bocchieri, 
Dilek Hakkani-Tür, Andrej Ljolje, Sarangarajan 
Parthasarathy, Mazin G. Rahim, Giuseppe Ric-
cardi, and Murat Saraclar. 2005. The AT&T 
WATSON Speech Recognizer. In Proceedings of 
the International Conference on Acoustics, 
Speech, and Signal Processing (ICASSP), pages 
1033–1036. IEEE. 

Arthur C. Graesser and Natalie K. Person. 1994. 
Question asking during tutoring. American Edu-
cational Research Journal, 31(1):104–137. 

Mark Hall, Eibe Frank, Geoffrey Holmes, Bern-
hard Pfahringer, Peter Reutemann, and Ian H. 
Witten. 2009. The WEKA Data Mining Software: 
An Update. ACM SIGKDD Explorations Newslet-
ter, 11(1):10–18. 

Mary M Juzwik, Carlin Borsheim-Black, Saman-
tha Caughlan, and Anne Heintz. 2013. Inspiring 
dialogue: Talking to learn in the English class-
room. Teachers College Press. 

Sean Kelly. 2007. Classroom discourse and the 
distribution of student engagement. Social Psy-
chology of Education, 10(3):331–352. 

Kazunori Komatani, Naoki Hotta, Satoshi Sato, 
and Mikio Nakano. 2015. User adaptive restora-
tion for incorrectly segmented utterances in spo-
ken dialogue systems. In 16th Annual Meeting of 
the Special Interest Group on Discourse and Dia-
logue, page 393. 

Microsoft. 2014. The Bing Speech Recognition 
Control. Technical report. 

Microsoft. 2016. Azure Speech API. Technical re-
port. 

Martin Nystrand. 2006. Research on the role of 
classroom discourse as it affects reading compre-
hension. Research in the Teaching of Eng-
lish:392–412. 

Martin Nystrand and Adam Gamoran. 1991. In-
structional discourse, student engagement, and lit-
erature achievement. Research in the Teaching of 
English:261–290. 

Martin Nystrand, Lawrence L Wu, Adam 
Gamoran, Susie Zeiser, and Daniel A Long. 2003. 
Questions in time: Investigating the structure and 
dynamics of unfolding classroom discourse. Dis-
course Processes, 35(2):135–198. 

Andrew Olney, Max Louwerse, Eric Matthews, 
Johanna Marineau, Heather Hite-Mitchell, and 
Arthur Graesser. 2003. Utterance classification in 
AutoTutor. In Proceedings of the HLT-NAACL 03 
Workshop on Building Educational Applications 
using Natural Language Processing-Volume 2, 
pages 1–8. Association for Computational Lin-
guistics. 

Luiza Orosanu and Denis Jouvet. 2015. Detection 
of sentence modality on French automatic speech-
to-text transcriptions. In Proceedings of the Inter-
national Conference on Natural Language and 
Speech Processing. 

Dinesh Raghu, Sathish Indurthi, Jitendra Ajmera, 
and Sachindra Joshi. 2015. A statistical approach 
for non-sentential utterance resolution for interac-
tive QA system. In Proceedings of the 16th An-
nual Meeting of the Special Interest Group on 
Discourse and Dialogue, page 335. 

Mickael Rouvier, Grégor Dupuy, Paul Gay, Elie 
Khoury, Teva Merlin, and Sylvain Meignier. 
2013. An open-source state-of-the-art toolbox for 
broadcast news diarization. Technical report. 

Borhan Samei, Andrew Olney, Sean Kelly, Mar-
tin Nystrand, Sidney D’Mello, Nathan Blanchard, 
Xiaoyi Sun, Marci Glaus, and Art Graesser. 2014. 
Domain independent assessment of dialogic prop-
erties of classroom discourse. In Proceedings of 
the 7th International Conference on Educational 
Data Mining (EDM 2014) pages 233-236. Inter-
national Educational Data Mining Society. 

Borhan Samei, Andrew M Olney, Sean Kelly, 
Martin Nystrand, Sidney D’Mello, Nathan 
Blanchard, and Art Graesser. 2015. Modeling 

200



classroom discourse: Do models that predict dia-
logic instruction properties generalize across pop-
ulations? Proceedings of the 8th International 
Conference on Educational Data Mining (EDM 
2015), pages 444-447. International Educational 
Data Mining Society. 

Tanya Stivers and Nick J. Enfield. 2010. A coding 
scheme for question–response sequences in con-
versation. Journal of Pragmatics, 42(10):2620–
2626. 

Andreas Stolcke, Noah Coccaro, Rebecca Bates, 
Paul Taylor, Carol Van Ess-Dykema, Klaus Ries, 
Elizabeth Shriberg, Daniel Jurafsky, Rachel Mar-
tin, and Marie Meteer. 2000. Dialogue act model-
ing for automatic tagging and recognition of con-
versational speech. Computational Linguistics, 
26(3):339–373. 

201


