



















































Alibaba Submission to the WMT18 Parallel Corpus Filtering Task


Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 917–922
Belgium, Brussels, October 31 - Novermber 1, 2018. c©2018 Association for Computational Linguistics

https://doi.org/10.18653/v1/W18-64109

Alibaba Submission to the WMT18 Parallel Corpus Filtering Task

Jun Lu, Xiaoyu Lv, Yangbin Shi, Boxing Chen
Machine Intelligence Technology Lab, Alibaba Group

Hangzhou, China
{joelu.luj, anzhi.lxy, taiwu.syb, boxing.cbx}@alibaba-inc.com

Abstract

This paper describes the Alibaba Machine
Translation Group submissions to the WMT
2018 Shared Task on Parallel Corpus Filter-
ing. While evaluating the quality of the par-
allel corpus, the three characteristics of the
corpus are investigated, i.e. 1) the bilin-
gual/translation quality, 2) the monolingual
quality and 3) the corpus diversity. Both rule-
based and model-based methods are adapted to
score the parallel sentence pairs. The final par-
allel corpus filtering system is reliable, easy to
build and adapt to other language pairs.

1 Introduction

The parallel corpus is an essential resource for
machine translation and multilingual natural lan-
guage processing. Apart from the quantity and
domain, the quality of parallel corpus is also
very important in MT system training (Koehn and
Knowles, 2017; Khayrallah and Koehn, 2018).
The Internet contains a large number of multilin-
gual resources, including parallel and comparable
sentences (Resnik and Smith, 2003). Many suc-
cessful machine translation systems are built using
the corpus crawled from the web. But in practice,
this kind of parallel corpus may be very noisy. The
task of Parallel Corpus Filtering tackles the prob-
lem of cleaning noisy parallel corpus.

In this task, we can divide the corpus cleaning
task into three parts. Firstly, a high-quality paral-
lel sentence pair should have the property that its
target sentence precisely translates the source sen-
tence, and vice versa. In this task, we attempt to
quantify the translation quality (also called bilin-
gual score) and accuracy of the sentence pair. Sec-
ondly, the quality of the target and/or source sen-
tences of the parallel corpus should also be eval-
uated. In this work, the target side sentences
are concerned a lot for their importance in NMT.

Thirdly, as described by the Parallel Corpus Fil-
tering task, the participants should not pay atten-
tion to the domain-relatedness. We need to focus
on all the domains so that the resulting MT sys-
tem can be widely used. So the diversity should
be evaluated while subsampling the parallel cor-
pus. Finally, the three characteristics of the par-
allel corpus are combined to build the final clean
corpus.

The paper is structured as follows: Section 2 de-
scribes our methods which are used in parallel cor-
pus filtering. Section 3 specifies the experiments
and results. The dataset for building model-based
methods is also detailed in this section. Conclu-
sions are drawn in Section 4.

2 Parallel Sentence Pairs Scoring
Methods

In this section, three kinds of scoring/filtering
methods are detailed.

2.1 Bilingual Quality Evaluation

Here, we describe the noisy corpus filtering rules
and two kinds of translation quality evaluation
methods: (1) Word Alignment Based bilingual
scoring and (2) Bitoken CNN Classifier based
bilingual scoring(Chen et al., 2016).

Rule-based Filtering
A series of heuristic rules are applied to filter
bad sentence pairs. They are simple but efficient,
which are described below.

• The length ratio of source sentence to target
sentence. Sentence length is calculated as the
number of tokens/words. In our system, the
ratio is set between 0.4 and 2.5.

• The edit distance between the source token
sequence and the target token sequence. A

917

https://doi.org/10.18653/v1/W18-64109


small edit distance indicates that the source
and target sentences are very similar. This
kind of corpus harms the performance of the
NMT system a lot (Khayrallah and Koehn,
2018). Besides, the edit distance can be nor-
malized by the average length of source and
target sentence length, which represents the
edit distance ratio. Both edit distance and
edit distance ratio are used to filter sentence
pairs in which the source and target sentence
are similar. In our system, a sentences pair
will be dropped if its edit distance is less than
2 or edit distance ratio is less than 0.1.

• The consistency of special tokens (Taghipour
et al., 2010). For example, the high-quality
sentence pairs should contain the same email
address in both source and target sentences
(if exists). In this task, special tokens are an
email address, URL, and a big Arabic num-
ber.

Word Alignment-based Bilingual Scoring

The word alignment model can be used for evalu-
ating the translation quality of bilingual sentence
pairs (Khadivi and Ney, 2005; Taghipour et al.,
2010; Ambati, 2011). Inspired by the work of
(Khadivi and Ney, 2005), we simplify the origi-
nal algorithm, and the translation score of sentence
pairs is given below:

score(s, t) =
1

m

∑

si,tj∈as2t
log p(tj |si)

+
1

n

∑

si,tj∈at2s
log p(si|tj) (1)

In Equation (1), s and t represent the source and
target sentences respectively, p(w1|w2) indicates
the word translation probability, as2t indicates the
source words to target words alignment, m and n
are the lengths of source and target sentences.

In this task, the word alignment model is trained
on a clean parallel corpus provided by WMT18
New Translation Task. We use the fast align
toolkit (Dyer et al., 2013) to train the model, and
get the forward and reverse word translation prob-
ability tables.

This model is also called alignment scoring
model.

Figure 1: Bitoken sequence

Bitoken CNN Classifier-based Bilingual
Scoring

Following the work of (Chen et al., 2016), a bito-
ken CNN based scoring model is built for transla-
tion quality evaluation.

In this model, the bitokens are extracted from
aligned sentence pairs. Figure 1 shows how a bito-
ken sequence can be obtained from a word-aligned
sentence pair. Each bitoken in the sequence is
treated as a word, and each bitoken sequence is
treated as a normal sentence. Then these bitoken
sentences are fed to the CNN Classifier to build
the bilingual scoring model. For every candidate
sentence pair, this model will give two probabili-
ties: ppos and pneg, and the quality score is treated
as scorebitoken = ppos − pneg. For the train data
set, the bitoken sequences obtained from the high-
quality corpus are labeled as positive. As for the
negative train data, we manually construct some
noisy data based on the clean data.(Lample et al.,
2017) For example, shuffle the target side sen-
tences of the clean parallel corpus, or randomly
delete the source or target sentence’s words. So
the negative bitoken sequences could be obtained
from this unparallel corpus.

This scoring model can also be called bito-
ken CNN scoring model.

2.2 Monolingual Quality Evaluation

Rule based Filtering

A few rules are applied to filtering the sentence
pairs whose source or target side are not good.
These rules are:

• The length of the sentence which is too short
(≤ 2 words) or too long (> 80 words) will be
dropped.

918



• The ratio of valid tokens counts to the length
of the sentence. Here, valid tokens are the
tokens which contain the letters in the corre-
sponding language. For example, a valid to-
ken in English should contain English letters.
In our system, the sentence is filtered if its
valid-tokens ratio is less than 0.2.

• Language filtering. For German-English par-
allel corpus, the source and target sentences’
languages should be English and German.
We can detect the sentence’s language by us-
ing a language detection tool we developed1.
The sentences pair is filtered if the languages
of its source and target sides are not German
and English.

Language Model Scoring
We use the language model to evaluate the qual-
ity of sentences. The language model is success-
fully used to select domain-related corpus (Yasuda
et al., 2008; Moore and Lewis, 2010). Besides,
the language model can also be used to filter out
ungrammatical data (Denkowski et al., 2012; Al-
lauzen et al., 2011), which is suitable for this task.

In our corpus filtering system, we focus on the
quality of target sentences, i.e. English sentences,
as they are more important in NMT. Firstly, a large
language model is built on all available English
monolingual corpus provided by WMT18. The
training corpus is cleaned using some rules men-
tioned above. Then the normalized-length lan-
guage model score can be regarded as the mono-
lingual quality score. But in practice, this method
has a shortcoming: it gives lower scores for the
good sentences that contain rare words. The train-
ing corpus needs to be generalized to overcome
this shortness, for example, we can replace the
words that occur less than 10 times in LM train
corpus with their part of speech tag(Axelrod et al.,
2015). Finally, the language model is re-built on
the generalized corpus.

2.3 Corpus Diversity
Rule-based Filtering
We could use a simple rule to reduce the number
of similar sentence pairs. Firstly, source and tar-
get sentences should be generalized. In our ex-
periment, for the English sentence, the general-
ization is done by removing all the characters ex-

1This tool is similar to Google’s CLD2:
https://github.com/CLD2Owners/cld2

cept for English letters. Also, a similar operation
is done for generalizing German sentences. After
that, if some sentence pairs have the same gener-
alized source or target sentences, the sentence pair
that has the highest quality score will be selected.

N-gram based Diversity Scoring
In this method, we aim to sub-select a corpus
which contains a variety of N-grams. Such a cor-
pus is regarded as high diversity. We follow the
work of (Ambati, 2011; Biçici and Yuret, 2011),
with the motivation for introducing a feature decay
function for the n-gram weight. In our system, af-
ter selecting a subset Sj−11 , the next sentence sj’s
diversity score is given by:

f(sj |Sj−11 ) =
∑N
n=1

∑
ng∈NG(sj ,n) weight(ng, j − 1)

norm(sj)
(2)

weight(ng, j − 1) = Freq(ng, S) ∗ e−λ∗Freq(ng,S
j−1
1 ),

where Sj−11 represents the set of selected sen-
tences which contains 1st to (j − 1)th sentences,
and S is the whole sentences pool to be selected.
f(sj |Sj−11 ) is the diversity score of sentence sj

under the condition that corpus Sj−11 is selected.
NG(sj , n) is all n-grams of size n in sentence

sj . |NG(sj , n)| is the size of the NG(sj , n).
norm(sj) is the normalization factor for sen-

tence sj , and equals
∑N

n=1 |NG(sj , n)|.
Freq(ng, S) is the frequency of n-gram in se-

lection data S.
λ is the exponential decay hyper parameter, λ =

1 in our experiment.
The equation (2) indicates that the n-gram is

weighted by its frequency in the pool set S and
selected set Sj−11 . The higher the frequency of n-
grams in the selected set, the lower the weight;
the higher the frequency of n-gram in the pool
set, the higher the weight. In practice, firstly, the
sentences pairs in the pool S are sorted by their
quality scores(combined by bilingual and mono-
lingual score) in descending order. Then the se-
lection method described above is carried out on
the target side of the bilingual corpus.

Parallel Phrases Diversity Scoring
The N-gram based Diversity Scoring is commonly
used for selecting monolingual sentences with
high diversity. Here we aim to sub-select a bilin-
gual corpus which contains a variety of parallel

919



phrases. With this kind of corpus, the MT model
will learn more translation knowledge.

Firstly, we use the fast align toolkit to train a
word alignment model. And then the phrase table
of the corpus can be extracted by using the Moses
toolkit. Next, we can obtain the parallel phrases
pairs for each sentence pair from the phrase table
using the methods of maximum matching. Finally,
following the method described in section N-gram
based Diversity Scoring, the same selection pro-
cedure (in which, N-gram is replaced by phrase
pairs) is used for sentence pairs’ scoring. In our
system, it works best when the phrase length is
less than 7.

2.4 Methods Combination and corpus
sampling

In our corpus filtering system, all the methods are
combined into a pipeline.

First of all, we apply all the bilingual and mono-
lingual rules to filter very noisy sentence pairs.
Then, two bilingual scores and target side lan-
guage model score could be produced by the above
corresponding models. These three scores are
individually normalized and then linearly com-
bined to produce a single quality score. Here,
the weights of these scores are selected with gird
search method(Hsu et al., 2003). After that, we
sort the sentence pairs by their corresponding
quality scores in the descending order. The diver-
sity method is then used to re-score/re-order the
corpus. Finally, we select two sets of the top-
N sentence pairs that contain totally 10 million
words and 100 million words.

3 Experiments and Results

In this section, we specify the experimental set-
tings and results in corpus filtering task.

3.1 Corpora and Settings
The selection data pool2 is provided by WMT18
Corpus Filtering Task, which contains about 100
million sentences pairs. It is very noisy. The task’s
participants are asked to sub-select sentence pairs
that amount to (a) 100 million words and (b) 10
million words.3 The quality of the resulting sub-
sets is determined by the BLEU scores of a sta-
tistical machine translation (Moses, phrase-based)

2http://www.statmt.org/wmt18/parallel-corpus-filtering-
data/data.gz

3http://www.statmt.org/wmt18/parallel-corpus-
filtering.html

and neural machine translation system (Marian)
trained on this data. In our SMT and NMT exper-
iments, we used the SMT and NMT configuration
that are provided by the task organizer4, as well as
the development and test set.

While building the alignment scoring model,
after using the bilingual and monolingual filter-
ing rules, 4,337,154 sentence pairs are selected
from the corpora provided by the WMT18 English-
German news translation task. Next, the fast align
tool is used to build the word alignment model on
the clean corpus, and then we can obtain the for-
ward and reverse word translation probability ta-
bles.

When building the bitoken CNN scoring
model, 20,000 positive labeled bitoken sequences
and 20,000 negative labeled bitoken sequences
are constructed. The fast align toolkit is also used
here. Then, we use the CONTEXT5 toolkit to
train the CNN models. The bitokens’ embedding
vectors are trained by word2vec6, and the size of
each vector was set to 200.

For target sentences’ quality evaluation, we use
the KenLM(Heafield et al., 2013) toolkit to train
the normal and generalized LM. The clean train-
ing corpus contains 60 million English sentences,
which are sub-selected from the corpora provided
by WMT18 News Translation Task.

3.2 Experimental Results

Firstly, the whole corpus which contains about
100 million sentence pairs was evaluated by train-
ing the SMT and NMT system. The final BLEU
scores are 21.21 and 7.8 respectively. This experi-
ment shows that the whole corpus is really noisy.

Other experimental results are detailed in Ta-
ble 1. The randomly sub-selected corpus’ perfor-
mance is also very poor. The sys 1 system uses the
bilingual/monolingual rules and alignment scor-
ing, which performed much better. We replace
the alignment scoring method by bitoken CNN
method and then build the sys 2 system. We
find that the alignment scoring method and bito-
ken CNN method are very similar in sentences
pairs scoring. As a result, a lot of sentence pairs
(about 70% in the subset) are selected by both
methods. The two methods are combined in sys 3,
which has a little improvement. While combining,

4http://www.statmt.org/wmt18/parallel-corpus-filtering-
data/dev-tools.tgz

5http://riejohnson.com/cnn download.html
6https://code.google.com/archive/p/word2vec/

920



System
ID

Method
10M words subset 100M words subset

sentence
pairs count

(×106)
SMT NMT

sentence
pairs count

(×106)
SMT NMT

- Random subset 1.31 15.25 7.73 8.23 18.21 7.57

sys 1
bilingual & monolingual rules

+ Alignment scoring
1.29 20.57 23.23 7.56 25.15 30.02

sys 2
bilingual & monolingual rules

+ bitoken CNN scoring
1.09 21.02 23.69 6.45 25.19 30.33

sys 3
bilingual & monolingual rules

+ Alignment + bitoken CNN
0.46 21.93 24.14 5.05 25.13 30.43

sys 4 sys 3 + Language Model 0.76 23.53 25.01 5.41 25.77 31.44
sys 5 sys 4 + Diversity Evaluation 0.64 23.79 25.34 5.41 25.77 31.44

Table 1: Methods used in Corpus selection and their performance

the original scores are normalized to the interval
[0, 1], and then the linear model is used to produce
a new score. In sys 3 system, the weights of align-
ment score and bitoken CNN score are 0.4 and 0.6
respectively.

The sys 4 introduced language mode score
based on sys 3. The weights of the align-
ment score, bitoken CNN score, and the language
model score are 0.4, 0.6 and 0.8 respectively. It
shows that the language model is useful in select-
ing clean sentences pairs.

Finally, based on sys 4, the corpus diversity fil-
tering rules and scoring are introduced in sys 5.
We find that the diversity method (only Parallel
Phrases Diversity Scoring is used in sys 5 system)
works well in selecting the smaller subset corpus,
e.g. the 10 million words corpus. For large subset
corpus selection, it almost has no improvement.
We attribute this to the sufficiently high diversity
of larger subset corpus.

4 Conclusions

In this paper, we present our corpus filtering sys-
tem for the WMT 2018 Corpus Filtering Task. In
our system, sentence pairs are evaluated in three
aspects: (1) the bilingual translation quality, (2)
the monolingual quality of the source and target
sentences and (3) the diversity of the sub-selected
corpus. Our experiments show that all the meth-
ods are contributed to building a cleaner parallel
corpus.

References
Alexandre Allauzen, Hélene Bonneau-Maynard, Hai-

Son Le, Aurélien Max, Guillaume Wisniewski,
François Yvon, Gilles Adda, Josep M Crego, Adrien
Lardilleux, Thomas Lavergne, et al. 2011. Limsi@
wmt11. In Proceedings of the Sixth Workshop on
Statistical Machine Translation, pages 309–315. As-
sociation for Computational Linguistics.

Vamshi Ambati. 2011. Active learning and crowd-
sourcing for machine translation in low resource
scenarios. Ph.D. thesis, University of Southern Cal-
ifornia.

Amittai Axelrod, Philip Resnik, Xiaodong He, and
Mari Ostendorf. 2015. Data selection with fewer
words. In Proceedings of the Tenth Workshop on
Statistical Machine Translation, pages 58–65, Lis-
bon, Portugal. Association for Computational Lin-
guistics.

Ergun Biçici and Deniz Yuret. 2011. Instance selec-
tion for machine translation using feature decay al-
gorithms. In Proceedings of the Sixth Workshop on
Statistical Machine Translation, pages 272–283. As-
sociation for Computational Linguistics.

Boxing Chen, Roland Kuhn, George Foster, Colin
Cherry, and Fei Huang. 2016. Bilingual methods for
adaptive training data selection for machine transla-
tion. In Proc. of AMTA, pages 93–103.

Michael Denkowski, Greg Hanneman, and Alon Lavie.
2012. The cmu-avenue french-english translation
system. In Proceedings of the Seventh Workshop on
Statistical Machine Translation, pages 261–266. As-
sociation for Computational Linguistics.

Chris Dyer, Victor Chahuneau, and Noah A Smith.
2013. A simple, fast, and effective reparameteriza-
tion of ibm model 2. In Proceedings of the 2013
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 644–648.

921



Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013. Scalable modi-
fied Kneser-Ney language model estimation. In Pro-
ceedings of the 51st Annual Meeting of the Associa-
tion for Computational Linguistics, pages 690–696,
Sofia, Bulgaria.

Chih-Wei Hsu, Chih-Chung Chang, Chih-Jen Lin, et al.
2003. A practical guide to support vector classifica-
tion.

Shahram Khadivi and Hermann Ney. 2005. Automatic
filtering of bilingual corpora for statistical machine
translation. In International Conference on Appli-
cation of Natural Language to Information Systems,
pages 263–274. Springer.

Huda Khayrallah and Philipp Koehn. 2018. On the
impact of various types of noise on neural machine
translation. In Proceedings of the 2nd Workshop on
Neural Machine Translation and Generation, pages
1–10.

Philipp Koehn and Rebecca Knowles. 2017. Six
challenges for neural machine translation. arXiv
preprint arXiv:1706.03872.

Guillaume Lample, Ludovic Denoyer, and
Marc’Aurelio Ranzato. 2017. Unsupervised
machine translation using monolingual corpora
only. CoRR, abs/1711.00043.

Robert C Moore and William Lewis. 2010. Intelligent
selection of language model training data. In Pro-
ceedings of the ACL 2010 conference short papers,
pages 220–224. Association for Computational Lin-
guistics.

Philip Resnik and Noah A Smith. 2003. The web
as a parallel corpus. Computational Linguistics,
29(3):349–380.

Kaveh Taghipour, Nasim Afhami, Shahram Khadivi,
and Saeed Shiry. 2010. A discriminative approach
to filter out noisy sentence pairs from bilingual cor-
pora. In Telecommunications (IST), 2010 5th Inter-
national Symposium on, pages 537–541. IEEE.

Keiji Yasuda, Ruiqiang Zhang, Hirofumi Yamamoto,
and Eiichiro Sumita. 2008. Method of selecting
training data to build a compact and efficient trans-
lation model. In Proceedings of the Third Interna-
tional Joint Conference on Natural Language Pro-
cessing: Volume-II.

922


