



















































SCIR-QA at SemEval-2017 Task 3: CNN Model Based on Similar and Dissimilar Information between Keywords for Question Similarity


Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 305–309,
Vancouver, Canada, August 3 - 4, 2017. c©2017 Association for Computational Linguistics

SCIR-QA at SemEval-2017 Task 3: CNN Model Based on Similar and
Dissimilar Information between Keywords for Question Similarity

Le Qi, Yu Zhang, Ting Liu
Research Center for Social Computing

and Information Retrieval
Harbin Institute of Technology

lqi,zhangyu,tliu@ir.hit.edu.cn

Abstract

We describe a method of calculating the
similarity between questions in communi-
ty QA. Questions in cQA are usually very
long and there are a lot of useless infor-
mation about calculating the similarity be-
tween questions. Therefore, we imple-
ment a CNN model based on similar and
dissimilar information on questions key-
words. We extract the keywords of ques-
tions, and then model the similar and dis-
similar information between the keyword-
s, and use the CNN model to calculate the
similarity.

1 Introduction

We participate in SemEval-2017 Task 3 Subtask B
(Nakov et al., 2017) on Community Question An-
swering. In this task, we are given a question from
community forum (named original question) and
10 related questions. We need to re-rank the relat-
ed questions according to their similarity between
the origin question.

Both the original question and the related ques-
tion have question subject and question body. The
subject is short. The body is long and contains a
lot of useless information. In our system, we try to
use keywords to replace questions to locate more
important information on the question, so we use a
keyword extraction algorithm that combines syn-
tactic information to get more accurate keywords.
Then we use a CNN model based on similar and
dissimilar information between questions to cal-
culate the similarity of questions. The model can
make good use of similar information and dissimi-
lar information between questions to get better re-
sults.

The paper is organized as follows: Section 2 in-
troduces our system. Section 3 introduces the ex-

periment. And in section 4, there are the conclu-
sions.

2 Model

In this section we describe our system in detail.
In Section 2.1 we show how we extract keyword-
s from the subject and body, and then in Section
2.2 we describe how to construct the CNN mod-
el based on similar and dissimilar information on
question keywords.

2.1 Keyword extraction
First, we cut the question subject and question
body. Then, we extract keywords from each sub-
sentence. We combine all the extracted keywords
together as a result.

We use an unsupervised keyword extraction
method based on dependency analysis. The
method uses syntactic dependency relations be-
tween words as clues. For the given question, we
not only use the statistical information and word
vector information, but also construct the depen-
dency graph to calculate the correlation intensity
between words, and then construct the weighted
graph according to the dependency degree, and
use the TextRank algorithm (Mihalcea and Ta-
rau, 2004) to iterate to calculate the word im-
portance score. The main steps include prepro-
cessing, the construction of the non-directional
weighted graph, graph ranking, and the selection
of the t words with the highest score as keywords
of the question, as shown in Figure 1.

Preprocess: The preprocessing process in-
cludes word segmentation and removing the stop
words. We use the remaining words as the candi-
date words of the keywords.

Construct the undirected weighted graph:
After preprocessing, all candidate words are rep-
resented as vertices of the graph. If two words co-
occur in a sentence, there is an edge to the two

305



Figure 1: Keywords extraction

vertices. The weight of the edge is calculated by
the statistical information on words, the word vec-
tor information and the dependent syntax analysis
information.

The methods that can be used to calculate the
correlation between two words are: Pointwise Mu-
tual Information (PMI), Average Mutual Informa-
tion(AMI) (Terra and Clarke, 2004), etc. Howev-
er, these methods only consider the statistical in-
formation between words, and do not consider the
syntactic dependencies. The syntactic dependency
between words has a positive effect on measuring
the importance of words.

The result of the dependency syntax analy-
sis is analogous to the tree structure. If we re-
move its root node, and ignore the arc of the
point, we can get an undirected dependency dia-
gram G′ = (V ′, E′), V ′ = w1, w2, ..., wn, E′ =
e1, e2, ..., em, where wi denotes a word and ej
denotes an undirected relationship between two
words. The undirected dependency graph guar-
antees that there is a dependency path between
any two words in the question, and the length
of the dependency path reflects the intensity of
the dependency relationship. Therefore, we intro-
duce the concept of dependency degree accord-
ing to the length of the dependent path (Zhang
et al., 2012), as shown in Equation(1), where
dr path len(wi, wj) represents the dependency
path length between words wi and wj , b is the su-
perparameter.

Dep(wi, wj) =
1

bdr path len(wi,wj)
(1)

The degree of correlation between two words,
that is, the weight of the edge is multiplied by the
gravitational value of the two words by the length
of the dependent path, as shown in Equation(2).

weight(wi, wj) = Dep(wi, wj) ∗ f(wi, wj) (2)
Among them, the concept of gravitational val-

ues proposed by (Wang et al., 2015), inspired by

gravitation. The word frequency is regarded as the
object mass, and the distance between the words
is taken as the distance of the object. The gravi-
tational value f(wi, wj) of the two words is given
by the Equation(3).

f(wi, wj) =
freq(wi) ∗ freq(wj)

d2
(3)

Graph ranking: We use the weighted Tex-
tRank algorithm to sort the graph. In the undi-
rected graph G = (V,E), V is the set of ver-
tices, E is the set of edges, and C(vi) is the set
of vertices connected to the vertex vi. The score
of the vertex vi is calculated from the Equation(4),
where weight(wi, wj) is calculated from the E-
quation(3), d is the damping coefficient.

ws(vi) = (1−d)+d∗
∑

vj∈C(vi)

weight(vi, vj)∑
vk∈C(vj)weight(vj , vk)

(4)

Then we select the t words with the highest s-
core as the keywords.

2.2 CNN model based on similar and
dissimilar information

We use a CNN model based on similar parts and
dissimilar parts between two sentences to get sen-
tence similarity. This model is proposed by (Wang
et al., 2016), now we will introduce the model
briefly. Figure 2 shows the structure of the model.

Given a sentence pair, the model represents each
keyword as a vector, and calculates a semantic
matching vector for each keyword based on part of
keywords in the other sentence. Then each word
vector is decomposed into two components based
on the semantic matching vector: a similar com-
ponent and a dissimilar component. After this, we
use a two-channel CNN to compose the similar
and dissimilar components into a feature vector.
Finally, a fully connected neural network is used
to predict the sentence similarity through the com-
posed feature vector.

306



First, with word embedding pre-trained by S-
tanford using GloVe’s model (Pennington et al.,
2014), we transform keywords of question S and
T into matrix S = [s1, s2, ..., sm] and T =
[t1, t2, ..., tn], where si and tj are 300-dimention
vectors of corresponding keywords, and m and n
are the length of keywords of S and T. Second, for
judging the similarity between two sentences, we
check whether each keyword in one sentence can
be covered by the other sentence. For a sentence
pair S and T, we first calculate a similarity ma-
trix A(m×n), where each element a(i,j) ∈ A(m×n)
computes cosine similarity between words si and
tj as

a(i,j) =
sTi tj

||si|| · ||tj || ∀si ∈ S, ∀tj ∈ T (5)

We calculate a semantic matching vector ŝi for
each word si by composing part of word vectors
in the other sentence T. In this way, we can match
a keyword si to some keywords in T. Similarly, we
also calculate all semantic matching vectors t̂i in
T. We define a semantic matching functions over
A(m×n)

fmatch(si, T ) =

∑k+w
j=k−w ai,jtj∑k+w
j=k−w ai,j

(6)

where
k = argmaxjai,j

w indicates the size of the window to consider
centered at k (the most similar word position). So
the semantic matchisng vector is a weighted aver-
age vector from tk−w to tk+w.

Third, after semantic matching, we have the se-
mantic matching vectors of ŝi and t̂j . Take s as an
example. We interpret ŝi as a semantic coverage
of word si by the sentence T. However, there must
be some difference between si and ŝi. So based on
its semantic matching vector ŝi, our model further
decomposes word si into two components: simi-
lar component ŝi+ and dissimilar component ŝi−.
Then we choose a linear decomposition method.
The motivation for the linear decomposition is that
the more similar between si and ŝi, the higher
proportion of si should be assigned to the similar
component. First, we calculate the cosine similar-
ity between si and ŝi. Then, we decompose si
linearly based on α. Eq.(7) gives the correspond-
ing definition:

a(i,j) =
sTi ŝi

||si|| · ||ŝi||

t̂j
+ = αsi

ŝi
− = (1− α)si (7)

Finally, due to the dissimilar and similar com-
ponents have strong connections, we use a two-
channel CNN model (Kim, 2014) to compose
them together. In the CNN model, we have three
layers. The first is a convolution layer. We define
a list of filters wo. The shape of each filter is d
h, where d is the dimension of word vectors and
h is the window size. Each filter is applied to t-
wo patches (a window size h of vectors) from both
similar and dissimilar channels, and generates a
feature. Eq.(8) expresses this process:

co,i = f(wo ∗ S+[i:i+h] + wo ∗ S−[i:i+h] + bo) (8)

The second layer is a pooling layer. We choose
max-pooling method to deal with variable feature
size. And the last layer is a full-connected layer.
We use a sigmoid function to constrain the result
within the range [0,1].

3 Experiment

We experimented with the corpus provided by
SemEval-2017 task3. Training set has 267 ques-
tions, each question has 10 related questions, a to-
tal of 2670 question pairs. Development set has
50 questions, 500 question pairs. The test set has
88 questions, 880 question pairs. We do the ex-
periment without preprocessing. We use Stanfod
Parser (De Marneffe and Manning, 2008) to parse

307



sentences. And we use the keyword extraction al-
gorithm described in 2.1, for each sub-sentence we
extract 1/3 of the words as keywords and set b =
1.4, d = 0.8. In the CNN model, we set up the filter
shape is 3*300. The number of filters is 500. We
set the similarity threshold of 0.5, that is, a score
greater than 0.5 is considered a positive case. And
we set the learning rate as 0.001. After 20 rounds
of training, we got the result in devlopment set and
test set.

Team MAP AvgRec MRR
Baseline(IR) 41.85 77.59 46.42

Baseline(Random) 29.81 62.65 33.02
simbow 47.22 82.60 50.07

LearningToQuestion 46.93 81.29 53.01
SCIR-QA 42.72 78.24 46.65

Table 1: Test Result

User or Team Name MAP AvgRec MRR
Sagustian 79.6 94.3 86.0
BeiHang 76.9 91.2 83.5
naman 75.1 90.8 81.33

LS2NSEMEVAL 74.4 88.3 79.5
NLMNIH 73.7 88.2 79.33
IIT-UHH 73.6 89.0 79.33

Organizers 71.4 86.1 76.67
MIT-QCRI 71.4 86.1 76.67
SCIR-QA 70.8 87.5 77.25

preslav 55.9 73.2 62.23

Table 2: Develop Result

The results in test set are shown in Table 1, the
first two lines are the baseline, the next two lines
are the best results, the last line is our result. And
results in development set are shown in Table 2. In
test set, our results are better than the baseline, but
there is still some distance from the best results. In
development set, our result is all not so good.

We think that because we do the experiment
without preprocessing, there exists too many un-
known words in word embeddings, which results
in poor system performance. On the other hand,
because the training corpus is too small, the neu-
ral network can not be well trained and can not
find meaningful features. Therefore, in the future
work, we will add features of artificial extraction
into neural network to improve performance. And
we will add features of artificial extraction into

neural network to improve performance.

4 Result and Future Work

We implement a CNN model based on similar
and dissimilar information between questions key-
words, and experiment on SemEval-2017 corpus.
The experimental results show that our method is
better than baseline, we can extract the key infor-
mation from the long sentence to model the ques-
tion better, which helps us to calculate the simi-
larity of the question. We think that keyword ex-
traction is important in this task, and in the future
we will try other keyword extraction methods to
achieve better results.

Acknowledgments

This work was supported by the National High
Technology Development 863 Program of Chi-
na (No.2015AA015407), National Natural Sci-
ence Foundation of China (No.61472105 and
No.61472107). And I am grateful to Professor
Zhang Yu for his guidance to my work. And I also
appreciate the support of SCIR laboratory.

References
Marie-Catherine De Marneffe and Christopher D Man-

ning. 2008. Stanford typed dependencies manual.
Technical report, Technical report, Stanford Univer-
sity.

Yoon Kim. 2014. Convolutional neural networks for
sentence classification. In EMNLP.

Rada Mihalcea and Paul Tarau. 2004. Textrank: Bring-
ing order into texts. Unt Scholarly Works pages
404–411.

Preslav Nakov, Doris Hoogeveen, Lluı́s Màrquez,
Alessandro Moschitti, Hamdy Mubarak, Timothy
Baldwin, and Karin Verspoor. 2017. SemEval-2017
task 3: Community question answering. In Proceed-
ings of the 11th International Workshop on Semantic
Evaluation. Association for Computational Linguis-
tics, Vancouver, Canada, SemEval ’17.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. In Empirical Methods in Nat-
ural Language Processing (EMNLP). pages 1532–
1543. http://www.aclweb.org/anthology/D14-1162.

Egidio Terra and C. L. A Clarke. 2004. Frequency es-
timates for statistical word similarity measures. In
Naacl 03 Conference of the North American Chap-
ter of the Association for Co. pages 244–251.

308



Rui Wang, Wei Liu, and Chris McDonald. 2015.
Corpus-independent generic keyphrase extraction
using word embedding vectors.

Zhiguo Wang, Haitao Mi, and Abraham Ittycheriah.
2016. Sentence similarity learning by lexical de-
composition and composition. In COLING.

Weinan Zhang, Zhaoyan Ming, Yu Zhang, Liqiang Nie,
Ting Liu, and Tat-Seng Chua. 2012. The use of de-
pendency relation graph to enhance the term weight-
ing in question retrieval. In COLING. pages 3105–
3120.

309


