



















































Cross-language Projection of Dependency Trees with Constrained Partial Parsing for Tree-to-Tree Machine Translation


Proceedings of the First Conference on Machine Translation, Volume 1: Research Papers, pages 1–11,
Berlin, Germany, August 11-12, 2016. c©2016 Association for Computational Linguistics

Cross-language Projection of Dependency Trees with
Constrained Partial Parsing for Tree-to-Tree Machine Translation

Yu Shen1, Chenhui Chu2∗, Fabien Cromieres2 and Sadao Kurohashi1
1Graduate School of Informatics, Kyoto University

2Japan Science and Technology Agency
{shen-yu,kuro}@nlp.ist.i.kyoto-u.ac.jp {chu,fabien}@pa.jst.jp

Abstract

Tree-to-tree machine translation (MT) that
utilizes syntactic parse trees on both
source and target sides suffers from the
non-isomorphism of the parse trees due
to parsing errors and the difference of an-
notation criterion between the two lan-
guages. In this paper, we present a method
that projects dependency parse trees from
the language side that has a high quality
parser, to the side that has a low qual-
ity parser, to improve the isomorphism of
the parse trees. We first project a part of
the dependencies with high confidence to
make a partial parse tree, and then com-
plement the remaining dependencies with
partial parsing constrained by the already
projected dependencies. MT experiments
verify the effectiveness of our proposed
method.

1 Introduction

According to how syntactic parse trees are used
in machine translation (MT), there are 4 types of
MT approaches: string-to-string that does not use
parse trees (Chiang, 2005; Koehn et al., 2007),
string-to-tree that uses parse trees on the target
side (Galley et al., 2006; Shen et al., 2008), tree-
to-string that uses parse trees on the source side
(Quirk et al., 2005; Liu et al., 2006; Mi and Huang,
2008), and tree-to-tree that uses parse trees on both
sides (Zhang et al., 2008; Richardson et al., 2015).
Intuitively, the tree-to-tree approach seems to be
the most appropriate. The reason is that it could
preserve the structure information on both sides,
which leads to fluent and accurate translations.

In practice, however, good quality parsers on
both the source and target sides are difficult to ac-

∗Corresponding author.

quire. In many cases, the parsing quality of one
side is much higher than that of the other side,
because the higher quality side has a well anno-
tated treebank or is linguistically easier to parse.
For example, in the case of Japanese-Chinese MT
that we study in this paper, the head-final charac-
teristic of Japanese (Isozaki et al., 2010) makes
the dependency parsing for Japanese much eas-
ier than that of Chinese. Currently, the depen-
dency parsing accuracy of Japanese is over 90%
(Kawahara and Kurohashi, 2006), while the Chi-
nese parsing accuracy is less than 80% (Shen et
al., 2012). Another problem is the annotation cri-
terion difference of the treebanks in different lan-
guages, which are used for training the parsers.
For example, the dependency annotations of noun
phrases and coordination could be different among
different languages. For example, in Japanese,
noun phrases and coordination are annotated as
modifier-head dependencies (Kawahara and Kuro-
hashi, 2006), while in Chinese they are annotated
as sibling dependencies (Shen et al., 2012). These
two problems lead to the parse difference between
the source and target parse trees, which affects the
translation rule extraction in tree-to-tree MT that
requires the isomorphism of the parse trees. This
extremely limits the translation quality of tree-to-
tree MT.

In this paper, we present an approach that
projects dependency trees from a high quality
(HQ) parser to a low quality (LQ) parser using
alignment information. The projection could re-
duce the parsing errors on the LQ side, and ad-
dress the annotation criterion difference problem.
This can make the LQ trees isomorphic to the HQ
trees, which can benefit the translation rule extrac-
tion in tree-to-tree MT, and thus improve the MT
performance. The idea of cross-language projec-
tion of parse trees has been proposed previously,
e.g., (Ganchev et al., 2009; Jiang et al., 2010; Goto

1



!"#

$%#

&'#

()#

*#

+,#

!"#$%&'

!"$%&'()

*+,!

!"#$%&#'($)*+#,-&.)/#0#1#%.!

($%#$%&'

"#$%&'())

*+,-./,01234!

!"#

()#

*#

-.##

/!0#

56789!

'(!

)!

:;!

&!

1!

234! $%!

'(!

)!

*+,-./,0!

&!

1!

234!

"#!

()#

+,#

*+,-./,0!

1!

<=4!

12#

&'#

>?!

$%!

$%#

34#

"#!

@A!

BBBCCCCCCCCCCCCCCCCCDDDDDDBBB!

Fig. 1: An example of the KyotoEBMT system on
Chinese-to-Japanese translation.

et al., 2015). However, few studies have been con-
ducted in the context of dependency based tree-to-
tree MT, which is the setting of this paper.

In addition, we propose a novel constrained par-
tial parsing method to address the word alignment
problems such as unaligned words and alignment
errors in projection. In detail, we first apply a par-
tial projection step to project a part of the depen-
dencies with high confidence judged by the align-
ment information and a projectivity criterion. We
thus obtain a projected “partial tree.” We then find
the missing dependencies from this partial tree by
applying a “partial parsing” method: we apply a
parser to find the missing dependencies subject to
respecting the projected dependencies, so that we
obtain a full dependency tree. Initially, the LQ
parser is used for the partial parsing process. Once
the entire projection process has been finished, we
select a part of the projected trees based on the de-
pendency projection ratio of the partial projection
step, and re-train a parser for the LQ side. This
re-trained parser tends to be more isomorphic to
the HQ parser, and thus we again apply it for the
partial parsing process.

We conduct experiments with an open source
dependency based tree-to-tree MT system Ky-
otoEBMT1 (Richardson et al., 2015) on the
Japanese-Chinese language pair. Because of the
improvement of the isomorphism of the source
and target parse trees by our proposed method,
we achieve significant MT performance improve-
ments on both Japanese-to-Chinese and Chinese-
to-Japanese translation directions.

2 The Difficulties of Tree-to-Tree MT
2.1 Overview of the KyotoEBMT System
This study is conducted on the KyotoEBMT sys-
tem (Richardson et al., 2015), which is a represen-

1 http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?KyotoEBMT

tative dependency based tree-to-tree MT system.
Figure 1 shows an overview of the KyotoEBMT
system on Chinese-to-Japanese translation. The
translation example database is automatically con-
structed from a parallel training corpus by means
of a discriminative alignment model (Riesa et al.,
2011). It contains “examples” that form the hy-
potheses to be combined during decoding. Note
that both source and target sides of all the exam-
ples are stored in dependency trees. An input sen-
tence is also parsed and transformed into a depen-
dency tree. For all the subtrees in the input de-
pendency tree, matching hypotheses are searched
in the example database. This step is the most
time consuming part, and a fast subtree retrieval
method (Cromieres and Kurohashi, 2011) is used.
There are many available hypotheses for one sub-
tree, and also, there are many possible hypothesis
combinations. The best combination is detected
by a lattice-based decoder, which optimizes a log-
linear model (Cromieres and Kurohashi, 2014). In
the example in Figure 1, four hypotheses are used.
They are combined and produce an output depen-
dency tree, which is the final translation. For more
details of the system, please refer to (Richardson
et al., 2015).

2.2 The Translation Example Extraction
Problem

One advantage of the KyotoEBMT system is that
it can handle examples that are discontinuous as
a word sequence but continuous structurally, be-
cause of the usage of both source and target parse
trees. In Figure 2, for example, the translation ex-
ample of “26-31:類似することを示唆する/4:表明
14:类似 (show the similarity)” and “0-2:このこと
は 30-35:示唆するものと思われる/0-4:认为这一
现象表明 (I think that this phenomenon shows)”
can be extracted by the KyotoEBMT system, be-
cause they are continuous in the parse trees. How-
ever, in phrase based MT (Koehn et al., 2007),
both of these two translation examples could not
be extracted. The reason for this is that “4:表明
(show)” and “14:类似 (similarity)” are discontin-
uous in the Chinese sentence; similarly, “0-2:この
ことは (this phenomenon)” and “30-35:示唆する
ものと思われる (I think that shows)” are discon-
tinuous in the Japanese sentence.

On the other hand, it also adds the constraint
that a translation example has to share the same
structure on the parse trees to guarantee the quality

2



!"#$%&'%()'**+*!

(,#-!
.,'%+&'%+%!

(,
#%
/
!

(,
#-
!

.
,
'
%
+
&
'
%
+
%
!

-,
+
0
!

-1.!

2-!

2-!
13-+*.4+%!
5',16#+*!

!"#$#"%&'(!

+%"7!

#%8"93'!
-(1%31*3!
-1&."'!
"#:9#3!

13-+*.4+%!
5',16#+*!

-#&#"1*!

-,+0!

(,#%/!

-1
.
!

2-
!

2-
!

1
3
-+
*.
4
+
%
!

5
'
,
1
6
#+
*!

#
)
*"#

+
#
'(
!

-#&
#"1
*!

1
%
3
!

+
%
"7
!

#%
8"9

3
'
!

2-
!

-(1
%
3
1
*3
!

-1
&
.
"'
!

"#:
9
#3
!

2 -
!

1
3
-+
*.
4
+
%
!

5
'
,
1
6
#+
*!

2-!

;!
<!

;!
<!

;
!

<
!

<
!
;
!

=1.1%'-'!

>
,
#%
'
-'
!

?1*-#%$)'**+*!

(,#%$!

Fig. 2: A motivated example that shows a word aligned Japanese-Chinese parse tree pair, where the solid
black boxes show the word alignments.

of the extracted examples. This could be a prob-
lem because of two reasons. The main reason is
parsing error. In Figure 2, for example, because
of the parsing errors in the Chinese parse tree, the
translation examples of “3-8:樹液中のＫ＋の/6-
10:树液中Ｋ＋的 (sap’s K+),” and “13-17:Ｋ＋
のみを含む/16-19:只含Ｋ＋ (only include K+)”
could not be extracted. The other reason is the an-
notation criterion difference. In Figure 2, for ex-
ample, the translation example of “18:標準 19:試
料/21:标准 22:试样(standard sample)” could not
be extracted, though both of the parses are correct.
In Japanese this kind of noun phrase structure is
annotated as the modifier-head, while in Chinese
it is annotated as siblings depending on the last
word.

One possible solution to address the above
problem is to loosen the constraint for translation
example extraction. For example, to extract the
“18:標準 19:試料/21:标准 22:试样(standard sam-
ple)” example caused by the annotation criterion
difference, we might allow the extraction of exam-
ples that are modifier-head and sibling subtrees on
the source and target sides, respectively. However,

!"#$%#&

'#"()()*&

+"'"!

!"#,"-&

.#/0%1,/)!

!"#,"-&

."#$()*!

23&'#%%&45"6!

73&'#%%&4896!

!#/0%1'%+&&

'#%%!

!"#,"--:&&

.#/0%1'%+&'#%%!

;!

<!

=! >!

;!
=!

<!

>!

;!
=!

<!

>!

;!
=!

<!

>!

Fig. 3: An overview of our constrained partial
parsing based projection method.

firstly, even the loosening in this degree could also
lead to other noisy translation examples; secondly,
what kind of loosening is required for the parse
error case is unclear, because the types of parse er-
rors are diverse. Therefore, instead of loosening
the constraint, we choose the cross-lingual projec-
tion approach to address the problem.

3 Projection of Dependency Trees with
Constrained Partial Parsing

Figure 3 is an overview of our proposed con-
strained partial parsing method. Firstly, we apply
a partial projection process to project a part of the

3



dependencies from the HQ tree using the HQ tree,
word alignment information and a projectivity cri-
terion. Note that the word alignment information
is omitted in Figure 3 for simplification. In Fig-
ure 3, the circled part in the HQ tree is projected.
Next, we apply partial parsing to complement the
other dependencies in the partially projected tree
using the LQ parser. In Figure 3, as the LQ parser
could parse the circled part in the original LQ tree
correctly, it also complements the dependencies
for the partially projected tree correctly. Once we
obtained the projected trees, we select a part of the
highly confident projected trees as training data to
re-train the LQ parser. Finally, we apply the re-
trained LQ parser for the partial parsing process,
which further improves the quality of projection.

In the remaining of this section, we describe the
details of partial projection, partial parsing, and re-
training of the LQ parser in Section 3.1, 3.2, and
3.3 respectively.

3.1 Partial Projection
3.1.1 Direct Mapping for Dependency Tree

Projection
We first present a direct mapping method for de-
pendency tree projection using word alignment,
which can be formalized as below.

Given a parallel sentence pair (S, T ), where
S = s1...si...sn, and T = t1...tj ...tm are sen-
tences of the HQ and LQ sides, respectively; si
and tj denote the word index (which also de-
notes the node index in the dependency tree) in
the corresponding sentences. We have a depen-
dency tree for S denoted as TreeS = {(si, sk)...}
that is composed of a set of dependencies, where
(si, sk) means that the word si is dependent on
the word sk. We also have an alignment set A =
{a(si, tj)...} from S to T , where a(si, tj) means
that the HQ word si is aligned to the LQ word tj .
The new LQ parse tree TreenewT is projected from
TreeS . We first perform the following preprocess-
ing for the unaligned HQ words.

• unaligned words (HQ side): If si is an un-
aligned word, link the dependencies around
si. More specifically, if si is unaligned, and
(sh, si) ∈ TreeS , (si, sk) ∈ TreeS , we
add (sh, sk) to TreeS , and discard (sh, si)
and (si, sk) from TreeS . This preprocess
can make two distinct words separated by
unaligned words be a modifier-head pair.
For example, in Figure 2, because “32:もの

(thing)” is an unaligned word, we add (30:示
唆 (show), 33:と (and)) to TreeS .

We then process each source node si in TreeS
in a top-down manner (from the root node to the
leaf node) by applying the following rules divided
by the alignment types.

• one to one alignment: If si aligns to a unique
tj , sk aligns to a unique tl, and (si, sk) ∈
TreeS , add (tj , tl) to TreenewT . For example,
in Figure 2, the Japanese dependency (0:この
(this), 1:こと (phenomenon)) is projected to
the Chinese side as (1:这2:一 (this)) by ap-
plying this rule.

• many to one alignment: If (si, sk, ...) aligns
to tj , we take the head sr (e.g., sk) from
(si, sk, ...) as the representative, and then
perform the same process as in the one to
one alignment case. For example, in Fig-
ure 2, a(33:と 34:思わ 35:れる (think), 0:认
为(think)) is a many to one alignment, and
we select the head “34:思わ” as the represen-
tative.

• one to many alignment: If si aligns to sev-
eral words (tj , tl, ...), similar to the many to
one alignment case, we take the head tr (e.g.,
tj) from (tj , tl, ...) based on the original LQ
tree as the representative, and then perform
the same process as in the one to one align-
ment case for si and tr.

• many to many alignment: Reduce this to one-
to-many and many-to-one cases, i.e., select
the representatives for both sides, and then
perform the same process as in the one to one
alignment case.

3.1.2 Partial Projection with Direct Mapping
There are several cases that the direct mapping
method could not deal with:

1. the other nodes in the one to many alignment
case: For the nodes (e.g., tl) (in (tj , tl, ...)
that align to one word si) other than the rep-
resentative tr, there are no clues to determine
their dependencies during the projection.

2. unaligned words (LQ side): If tj is an un-
aligned word, there are also no clues for the
projection. For example, in Figure 2, because
the word Chinese “3:现象 (phenomenon)”,

4



“15:与 (and)” and “20:的 (’s)” are unaligned
words, we cannot determine their dependen-
cies by projection.

3. alignment errors: Because the direct map-
ping method highly depends on word align-
ments, erroneous word alignments would
lead to wrong projected dependency results.
For example, in Figure 2, the Japanese
word “12:むしろ (preferably)” is incorrectly
aligned to the Chinese word “13:及其 (ex-
tremely)”; this erroneous alignment would
project the Japanese dependency (12:むしろ
(preferably), 14:+) to the Chinese side, lead-
ing to a projected dependency of (13:及其
(extremely), 19:+), which is obviously in-
correct. Alignment errors could happen due
to many factors, one of which is translation
shift. The erroneous alignment in Figure 2 is
caused by this.

Because of the existence of the above cases, we
only apply the direct mapping method for partial
projection. For the (1) and (2) cases, we leave the
dependencies for these words as null. For the (3)
case, we propose a projectivity criterion to detect
the alignment error, and again leave the dependen-
cies as null. Note that all of these three cases are
processed during the top-down projection process.

3.1.3 Adding a Projectivity Criterion to the
Projection Process

Projectivity is a property of dependency pars-
ing, which informally means that there should
not be crossing arcs in a dependency tree (Kubler
et al., 2009). For example, TreenewT =
{(0, 2)(1, 3)(2, 3)(3, −1)} (-1 denotes the root) is
not projective, because the arc of modifier-head
pair (0,2) and that of modifier-head pair (1,3) is
crossed. We use the projectivity property to de-
tect alignment errors during the top-down projec-
tion process. Suppose that by processing the HQ
tree from the root, we already have a partially pro-
jected LQ subtree. Next, we want to project a
new dependency in the HQ tree to the LQ side. If
adding this newly projected dependency to the par-
tially projected subtree leads to non-projectivity,2

we give up this projection and leave the depen-
dency as null.

Many alignment errors can be detected by the
property of projectivity. For example, in Figure

2 Note that not all non-projectivites are caused by align-
ment errors; a few of them are also due to translation shift.

2, if we use the erroneous alignment a(12:むし
ろ (preferably), 13:及其 (extremely)) to project
the Japanese dependency (12:むしろ (preferably),
14:+) to the Chinese side, we obtain the depen-
dency of (13:及其 (extremely), 19:+). Before the
projection for the node “12:むしろ (preferably)”,
because the node “24: 挙動 (behavior)” is an an-
cestor of this node in the Japanese tree, it has been
projected. The dependency (24: 挙動 (behavior),
26:類似 (similar)) has been projected to the Chi-
nese side, leading the dependency of (27:作用 (be-
havior), 14:类似 (similar)). (13:及其 (extremely),
19:+) and (27:作用 (behavior), 14:类似 (similar))
lead to non-projectivity. Therefore, we leave the
dependency for “13:及其 (extremely)” as null.

3.2 Partial Parsing
After the partial projection step, we obtain partial
projected trees, with null dependencies discussed
in Section 3.1.2. We then perform partial parsing
to complement these null dependencies. Before
the description of the partial parsing method, we
first review the formalism of dependency parsing
used in many previous studies such as (Kubler et
al., 2009; Shen et al., 2012):

Y ∗ = argmaxY ∈Φ(X)score(Y, X) (1)

where X = x1...xi...xn is the input sentence, Y
is a candidate tree, Φ(X) is a set of all possible
dependency trees over X . Y can be denoted as
Y = {(m, h) : 0 ≤ m ≤ n, 0 ≤ h ≤ n}, where
(m,h) is a dependency from the modifier xm to
the head xh. The problem of dependency parsing
is to search the best tree from Φ(X) that maxi-
mizes the score function score(Y,X). The score
function can be factorized as the summation of the
scores of its factors (subtrees):

score(Y, X) =
∑

F∈Y
score(F, X) (2)

The score function for each factor is denoted as the
inner product of a feature and a weight vector:

score(F, X) = w · f(F, X) (3)

The weight vector can be learnt by e.g., the av-
eraged structured perceptron algorithm (Collins,
2002) on an annotated treebank. During parsing,
the parser would utilize the learnt weight vector to
determine the best parse tree.

In our partial parsing method, we aim to keep
the dependencies in partial projected trees, while

5



complement the null dependencies to construct a
projective tree. To realize this, we set extremely
high scores to the projected dependencies to max-
imize the score(F, X) for these dependencies,
while for the null dependencies we set relatively
small scores. Doing so, the parser would search
the best tree that respects the partial projected
dependencies. In our experiments, we used the
projective second order graph based dependency
parser (Shen et al., 2012). We set the initial de-
pendency scores for the projected dependencies to
1e12, and 0 to the null dependencies.

3.3 Re-train a New Low Quality Side Parser

Re-training a new LQ parser on the projected trees
is necessary for two reasons. Initially, we use the
original LQ parser for the partial parsing process,
because we do not have a better choice; due to the
low accuracy and the annotation criterion differ-
ence problem of the LQ parser, we have the risk
that it will produce unsatisfying parsing results,
especially for the trees with a low ratio of depen-
dencies being projected. Secondly, if we perform
the LQ-to-HQ direction MT, we should make the
parsed trees of the input sentences isomorphic to
the projected trees. Re-training a new LQ parser
on the projected trees could address both of these
two problems. As the re-trained parser tend to
be more isomorphic to the HQ parser, it could be
more effective for the partial parsing process, and
could be applied for parsing the input sentences
for the LQ-to-HQ direction MT task.

Therefore, after the entire projection process,
we select a part of the projected trees, and re-train
a parser for the LQ side. How to select the pro-
jected trees for training the new LQ parser is an
open question. The main question is how to take
the balance of the quality and quantity of the pro-
jected trees. Currently, the selection criterion is
empirical based on the ratio of dependencies pro-
jected by the partial projection process in a tree,
defined by

ratio =
#projected dependencies

#all dependencies
(4)

The motivation behind this is that the more de-
pendencies projected by the partial projection in
a tree, the more isomorphic would the projected
tree be as the HQ tree, and the less affect would
be introduced by the original LQ parser during the
partial parsing process. We set a threshold, and

use the trees with the ratio higher than the thresh-
old for training the parser. We tried several thresh-
olds in our preliminary experiments, and selected
the best threshold of 0.78 (170k trees) based on the
MT performance.3

4 Experiments

We conducted Japanese-Chinese MT experiments
to verify the effectiveness of our constrained par-
tial parsing based projection method.

4.1 Settings

We conducted experiments on the scientific do-
main MT task on the Japanese-Chinese paper ex-
cerpt corpus (ASPEC-JC),4 which is one sub-
task of the workshop on Asian translation (WAT)5

(Nakazawa et al., 2015). The ASPEC-JC task
uses 672,315, 2,090, and 2,107 sentences for train-
ing, development, and testing, respectively. We
used the tree-to-tree MT system KyotoEBMT6

(Richardson et al., 2015) for all of our MT exper-
iments. For Chinese, we used the Chinese ana-
lyzing tool KyotoMorph7 proposed by Shen et al.
(2014) for segmentation and part-of-speech (POS)
tagging, and the SKP parser8 (Shen et al., 2012)
for parsing. As the baseline Chinese parser, we
trained SKP with the Penn Chinese treebank ver-
sion 5 (CTB5)9 containing 18k sentences in news
domain, and an in-house scientific domain tree-
bank of 10k sentences. For Japanese, we used JU-
MAN10 (Kurohashi et al., 1994) for morpholog-
ical analyzing, and the KNP parser for parsing11

(Kawahara and Kurohashi, 2006). We trained
two 5-gram language models for Chinese and
Japanese, respectively, on the training data of the
ASPEC corpus using the KenLM toolkit12 with
interpolated Kneser-Ney discounting, and used
them for all the experiments. In all of our ex-
periments, we used the discriminative alignment
model Nile13 (Riesa et al., 2011) for word align-
ment; tuning was performed by the k-best batch

3 The average partial projection ratio was 0.70.
4 http://lotus.kuee.kyoto-u.ac.jp/ASPEC/
5 http://orchid.kuee.kyoto-u.ac.jp/WAT/
6 http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?KyotoEBMT)
7 https://bitbucket.org/msmoshen/kyotomorph-beta
8 https://bitbucket.org/msmoshen/skp-beta
9 https://catalog.ldc.upenn.edu/LDC2005T01

10 http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?JUMAN
11 http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?KNP
12 https://github.com/kpu/kenlm/
13 https://github.com/neubig/nile

6



MIRA (Cherry and Foster, 2012) with 10 itera-
tions, and it was re-run for every experiment.

Note that, in our task, Japanese is the HQ parser
side, and Chinese is the LQ parser side, because of
the parsing accuracy difference (90% v.s. 80%).
Therefore, in our experiments, we projected the
Japanese parse trees to Chinese. We compared
the MT performance of our proposed projection
method with the baseline Chinese parser. For
Japanese-to-Chinese MT experiments, we com-
pared the MT results of the Chinese training data
parsed by the baseline parsed, to those of the pro-
jected trees. For Chinese-to-Japanese MT, we also
re-parsed the development and test Chinese sen-
tences using the SKP model trained on the pro-
jected Chinese trees, for the comparison.

4.2 MT Results

Table 1 shows the results, where KyotoEBMT is
the baseline system that used the Chinese parser
trained on CTB5; Baseline partial parsing de-
notes the projection systems that used the Chi-
nese parser trained on CTB5 for the partial pars-
ing process; Re-trained partial parsing denotes the
systems that used the Chinese parser re-trained
on the projected trees for the partial parsing pro-
cess. For reference, we also show the MT per-
formance of the phrase based, string-to-tree, and
tree-to-string systems, which are based on the
open-source GIZA++/Moses pipeline (Koehn et
al., 2007). Note that in all of the Moses, string-to-
tree, and tree-to-string settings, Japanese is always
in the string format, and Chinese is parsed by the
Berkeley parser14 (Petrov and Klein, 2007).15 The
significance tests were performed using the boot-
strap resampling method (Koehn, 2004).

We can see that, the Baseline KyotoEBMT sys-
tem outperforms the Moses, string-to-tree, and
tree-to-string systems, which verifies the effective-
ness of the tree-to-tree approach. The performance
difference of KyotoEBMT against the other three
MT approaches on the Ja-to-Zh direction is much
larger than those of the Zh-to-Ja direction. The
reason for this is that KyotoEBMT is much more
sensitive to the parsing accuracy on the source
side, because the source tree is utilized in the or-
dering of the final translation. Therefore using
Chinese as the source side limits the effectiveness

14 https://github.com/slavpetrov/berkeleyparser
15 We show the MT performance of Moses that only parsed

the Chinese data, because these were the baseline systems of
WAT.

System Ja-to-Zh Zh-to-Ja
Moses phrase based 27.25 33.94
Moses string-to-tree 26.20 N/A
Moses tree-to-string N/A 33.49
Baseline KyotoEBMT 29.33 34.73
Baseline partial parsing 30.12† 35.84†
Re-trained partial parsing 30.28† 36.18†‡

Tab. 1: BLEU scores for ASPEC Ja-to-Zh and
Zh-to-Ja (“†,” and “‡” indicate that the re-
sult is significantly better than “Baseline
KyotoEBMT” and “Baseline partial pars-
ing” at p < 0.01, respectively).

System Ja-to-Zh Zh-to-Ja
Baseline KyotoEBMT 13.13M 8.43M
Baseline partial parsing 15.69M 9.88M
Re-trained partial parsing 15.69M 9.90M

Tab. 2: Number of hypotheses for the test sen-
tences.

of the KyotoEBMT system. Baseline partial pars-
ing performs significantly better than the Baseline
KyotoEBMT, and Re-trained partial parsing fur-
ther improves the performance significantly. We
also observe slightly more improvement on the
Zh-to-Ja direction than the Ja-to-Zh direction. The
reason is similar to the one above that in Zh-to-Ja
task, we not only improve the translation example
extraction, but also the quality of the input trees.

To further understand the reason for the MT
improvement, we investigated the number of hy-
potheses for the test sentences. The number of hy-
potheses for a test sentence is the number all the
matching hypotheses in the example database for
all the subtrees in the input dependency structure
of the test sentence (refer to Section 2.1). The en-
tire number of hypotheses for all the test sentences
of different systems are shown in table 2. We can
see that the number of hypotheses for the partial
parsing systems is greatly larger than the baseline
KyotoEBMT system. The reason for this is that
our projection method significantly increased the
isomorphism of the source and target trees in the
training corpus, making more translation exam-
ples being extractable. More hypotheses are po-
tentially to improve the final MT performance.

In addition, we investigated the translation re-
sults of the Baseline KyotoEBMT and Re-trained
partial parsing systems. We found that there are

7



!"#$%&'$()*+,+-!./! 0$1,2"&'$3(4"25"%(4"2#&'6((

7'48,! 98,48,! 7'48,! 98,48,!

Fig. 4: An improved example of Zh-to-Ja translation (The subtrees in corresponding IDs/colors in the
input and output dependency trees show the translation examples being used during translation).

three reasons that lead to the improvement. We
explain these reasons through an improved exam-
ple of Zh-to-Ja translation shown in Figure 4. The
first reason is the improvement of the input parse
tree. There is a crucial parsing error in the input
tree of the Baseline KyotoEBMT system. The Ky-
otoMorph incorrectly assigned a wrong POS tag
“VV (verb)” for the word “15:抑制 (inhibition)”,
which should be “NN (noun)” in fact. This leads to
this word be the head of the whole following noun
phrase. Using this erroneous input parse tree, this
word is also translated into the head of the entire
noun phrase. Our Re-trained partial parsing cor-
rectly parsed the word “15:抑制 (inhibition)” as
a part of the noun phrase “15-18:抑制氧消耗实
验(inhibition of oxygen consumption test)”, lead-
ing to the correct translation. Although the Re-
trained partial parsing could not correct the wrong
POS tag of the word, because we also used this
kind of data to train the parser, it successfully
parsed this sentence. The second reason is the in-
crease of translation hypotheses. The number of
hypotheses for the Baseline KyotoEBMT system
is 2,447, while the number of hypotheses of the
Re-trained partial parsing system is 3,311. The
number of hypotheses for “0:针对...7:进行 8:了
(about...performed)” increased from 52 to 176 by
the Re-trained partial parsing system, which im-
proved the translation. The third reason is the iso-
morphism of the input and output target depen-
dency trees. Note that the noun phrases “15-18:
抑制氧消耗实验(inhibition of oxygen consump-
tion test)” and “20-23:大型蚤急性毒性实验(large-

scale flea acute toxicity test)” are parsed as sib-
lings in the Baseline KyotoEBMT system, while
in our Re-trained partial parsing model they are
parsed as modifier-head dependencies, which are
isomorphic to the Japanese parse tree. One unsat-
isfying point is that “21:蚤急性 (flea acute)” is an
unknown word, which is a difficult technical term
that could not be translated by both of the two sys-
tems.

5 Related Work

There are many previous studies that propose
many methods to address the difficulties in pro-
jecting the parse trees from a resource rich lan-
guage (e.g., English) to a low resource language,
to improve the parsing accuracy of the low re-
source language. The difficulties in projection
can be mainly divided into two categories: word
alignment errors and annotation criterion differ-
ence (Ganchev et al., 2009).

To address the word alignment error prob-
lem, several studies have proposed to train a tar-
get parser on high confidence partially projected
trees. Ganchev et al. (2009) presented a par-
tial projection method with constraints such as
language-specific annotation rules. They then
trained a target parser using the partially pro-
jected trees. Spreyer and Kuhn (2009) proposed
a similar method that trains both graph-based and
transition-based dependency parsers on the par-
tially projected trees. Rasooli and Collins (2015)
proposed a method to train a target parser on

8



“dense” projected trees. The “dense” projected
trees might only contain a part of dependencies
over a threshold. Our proposed method differs
from the previous studies in several aspects: we
propose the use of the projectivity criterion for
partial projection; we utilize the original target
parser and propose a constrained partial parsing
algorithm; we re-train a target parser on the full
trees generated by the partial parsing.

To address the annotation criterion difference
problem in projection, Hwa et al. (2005) firstly
projected the dependency parse trees, and then
applied post projection transformations based on
manually created rules. Jiang et al. (2011) pre-
sented a method that tolerates the syntactic non-
isomorphism between languages. This allows the
projected parse trees do not have to follow the an-
notation criterion of the source parse trees. Our
proposed method does not adjust the annotation
criterion difference between the source and the
projected trees, because in our tree-to-tree MT
task, we prefer isomorphic trees.

Only a few studies have been conducted to
improve MT performance via projection. For
string-to-string MT (Koehn et al., 2007), Goto et
al. (2015) proposed a pre-ordering method that
projects target side constituency trees to the source
side, and then generates pre-ordering rules based
on the projected trees. For tree-to-string MT,
Jiang et al. (2010) combined projection and su-
pervised constituency parsing by guiding the pars-
ing procedure of the supervised parser with the
projected parser. They showed that the guided
parser achieved comparable MT results on a tree-
to-string system (Liu et al., 2006), compared to
a normal supervised parser trained on thousands
of CTB trees. For tree-to-tree MT (Richardson et
al., 2015), Shen et al. (2015) proposed a naive
projection method. They complemented the re-
maining dependencies for a partially projected tree
with a backtracking method. Namely, they reused
the dependencies in the original target tree for the
complement without considering the partially pro-
jected dependencies. In contrast, in this paper
we propose partial parsing for the complement, in
which we search for the best parse tree by taking
account of the partially projected dependencies.

6 Conclusion

In this paper, we proposed a constrained partial
parsing method for projection to address the non-

isomorphic parse tree problem in a dependency
based tree-to-tree MT system. Experiments ver-
ified the effectiveness of our proposed method. As
future work, firstly, we plan to design a better way
for selecting the projected trees for re-training the
LQ parser. Secondly, we plan to perform the par-
tial parsing in several iterations. Finally, we plan
to conduct experiments on more language pairs
to show the language-dependence of our proposed
method.

Acknowledgments

We especially thank to Dr. Mo Shen for the in-
sightful discussion of the constrained partial pars-
ing method with us.

References
Colin Cherry and George Foster. 2012. Batch tuning

strategies for statistical machine translation. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
427–436, Montréal, Canada, June. Association for
Computational Linguistics.

David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL’05), pages
263–270, Ann Arbor, Michigan, June. Association
for Computational Linguistics.

Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings
of the 2002 Conference on Empirical Methods in
Natural Language Processing, pages 1–8. Associ-
ation for Computational Linguistics, July.

Fabien Cromieres and Sadao Kurohashi. 2011. Ef-
ficient retrieval of tree translation examples for
syntax-based machine translation. In Proceedings
of the 2011 Conference on Empirical Methods in
Natural Language Processing, pages 508–518, Ed-
inburgh, Scotland, UK., July. Association for Com-
putational Linguistics.

Fabien Cromieres and Sadao Kurohashi. 2014. Trans-
lation rules with right-hand side lattices. In Pro-
ceedings of the 2014 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP),
pages 577–588, Doha, Qatar, October. Association
for Computational Linguistics.

Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Pro-
ceedings of the 21st International Conference on

9



Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 961–968, Sydney, Australia, July. Asso-
ciation for Computational Linguistics.

Kuzman Ganchev, Jennifer Gillenwater, and Ben
Taskar. 2009. Dependency grammar induction via
bitext projection constraints. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP, pages
369–377, Suntec, Singapore, August. Association
for Computational Linguistics.

Isao Goto, Masao Utiyama, Eiichiro Sumita, and Sadao
Kurohashi. 2015. Preordering using a target-
language parser via cross-language syntactic projec-
tion for statistical machine translation. ACM Trans.
Asian Low-Resour. Lang. Inf. Process., 14(3):13:1–
13:23, June.

Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Natural language engineering, 11(03):311–325.

Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada, and
Kevin Duh. 2010. Head finalization: A simple
reordering rule for sov languages. In Proceedings
of the Joint Fifth Workshop on Statistical Machine
Translation and MetricsMATR, pages 244–251, Up-
psala, Sweden, July. Association for Computational
Linguistics.

Wenbin Jiang, Yajuan Lv, Yang Liu, and Qun Liu.
2010. Effective constituent projection across lan-
guages. In Coling 2010: Posters, pages 516–524,
Beijing, China, August. Coling 2010 Organizing
Committee.

Wenbin Jiang, Qun Liu, and Yajuan Lv. 2011. Re-
laxed cross-lingual projection of constituent syntax.
In Proceedings of the 2011 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1192–1201, Edinburgh, Scotland, UK., July. Asso-
ciation for Computational Linguistics.

Daisuke Kawahara and Sadao Kurohashi. 2006. A
fully-lexicalized probabilistic model for japanese
syntactic and case structure analysis. In Proceedings
of the Human Language Technology Conference of
the NAACL, Main Conference, pages 176–183, New
York City, USA, June. Association for Computa-
tional Linguistics.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics Com-
panion Volume Proceedings of the Demo and Poster
Sessions, pages 177–180, Prague, Czech Republic,
June. Association for Computational Linguistics.

Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Dekang Lin and
Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 388–395, Barcelona, Spain, July. Association
for Computational Linguistics.

Sandra Kubler, Ryan McDonald, Joakim Nivre, and
Graeme Hirst. 2009. Dependency Parsing. Mor-
gan and Claypool Publishers.

Sadao Kurohashi, Toshihisa Nakamura, Yuji Mat-
sumoto, and Makoto Nagao. 1994. Improve-
ments of Japanese morphological analyzer JUMAN.
In Proceedings of the International Workshop on
Sharable Natural Language, pages 22–28.

Yang (1) Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Compu-
tational Linguistics, pages 609–616, Sydney, Aus-
tralia, July. Association for Computational Linguis-
tics.

Haitao Mi and Liang Huang. 2008. Forest-based
translation rule extraction. In Proceedings of the
2008 Conference on Empirical Methods in Nat-
ural Language Processing, pages 206–214, Hon-
olulu, Hawaii, October. Association for Computa-
tional Linguistics.

Toshiaki Nakazawa, Hideya Mino, Isao Goto, Gra-
ham Neubig, Sadao Kurohashi, and Eiichiro Sumita.
2015. Overview of the 2nd Workshop on Asian
Translation. In Proceedings of the 2nd Workshop on
Asian Translation (WAT2015), pages 1–28, Kyoto,
Japan, October.

Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics; Proceedings of the Main Confer-
ence, pages 404–411, Rochester, New York, April.
Association for Computational Linguistics.

Chris Quirk, Arul Menezes, and Colin Cherry. 2005.
Dependency treelet translation: Syntactically in-
formed phrasal SMT. In Proceedings of the 43rd
Annual Meeting of the Association for Computa-
tional Linguistics (ACL’05), pages 271–279, Ann
Arbor, Michigan, June. Association for Computa-
tional Linguistics.

Mohammad Sadegh Rasooli and Michael Collins.
2015. Density-driven cross-lingual transfer of de-
pendency parsers. In Proceedings of the 2015
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 328–338, Lisbon, Portugal,
September. Association for Computational Linguis-
tics.

John Richardson, Raj Dabre, Chenhui Chu, Fabien
Cromières, Toshiaki Nakazawa, and Sadao Kuro-
hashi. 2015. KyotoEBMT System Description for

10



the 2nd Workshop on Asian Translation. In Pro-
ceedings of the 2nd Workshop on Asian Translation
(WAT2015), pages 54–60, Kyoto, Japan, October.

Jason Riesa, Ann Irvine, and Daniel Marcu. 2011.
Feature-rich language-independent syntax-based
alignment for statistical machine translation. In
Proceedings of the 2011 Conference on Empir-
ical Methods in Natural Language Processing,
pages 497–507, Edinburgh, Scotland, UK., July.
Association for Computational Linguistics.

Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008.
A new string-to-dependency machine translation al-
gorithm with a target dependency language model.
In Proceedings of ACL-08: HLT, pages 577–585,
Columbus, Ohio, June. Association for Computa-
tional Linguistics.

Mo Shen, Daisuke Kawahara, and Sadao Kurohashi.
2012. A reranking approach for dependency pars-
ing with variable-sized subtree features. In Proceed-
ings of the 26th Pacific Asia Conference on Lan-
guage, Information, and Computation, pages 308–
317, Bali,Indonesia, November. Faculty of Com-
puter Science, Universitas Indonesia.

Mo Shen, Hongxiao Liu, Daisuke Kawahara, and
Sadao Kurohashi. 2014. Chinese morphological
analysis with character-level pos tagging. In Pro-
ceedings of ACL, pages 253–258.

Yu Shen, Chenhui Chu, Fabien Cromieres, and Sadao
Kurohashi. 2015. Cross-language projection of
dependency trees for tree-to-tree machine transla-
tion. In Proceedings of the 29th Pacific Asia Con-
ference on Language, Information and Computing
(PACLIC2015), pages 80–88, Shanghai, China, 10.

Kathrin Spreyer and Jonas Kuhn. 2009. Data-driven
dependency parsing of new languages using incom-
plete and noisy training data. In Proceedings of
the Thirteenth Conference on Computational Nat-
ural Language Learning (CoNLL-2009), pages 12–
20, Boulder, Colorado, June. Association for Com-
putational Linguistics.

Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li,
Chew Lim Tan, and Sheng Li. 2008. A tree
sequence alignment-based tree-to-tree translation
model. In Proceedings of ACL-08: HLT, pages 559–
567, Columbus, Ohio, June. Association for Compu-
tational Linguistics.

11


