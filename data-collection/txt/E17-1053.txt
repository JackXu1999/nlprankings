



















































Unsupervised AMR-Dependency Parse Alignment


Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 558–567,
Valencia, Spain, April 3-7, 2017. c©2017 Association for Computational Linguistics

Unsupervised AMR-Dependency Parse Alignment

Wei-Te Chen
Department of Computer Science
University of Colorado Boulder
weite.chen@colorado.edu

Martha Palmer
Department of Linguistics

University of Colorado Boulder
martha.palmer@colorado.edu

Abstract

In this paper, we introduce an Abstract
Meaning Representation (AMR) to De-
pendency Parse aligner. Alignment is a
preliminary step for AMR parsing, and
our aligner improves current AMR parser
performance. Our aligner involves sev-
eral different features, including named
entity tags and semantic role labels, and
uses Expectation-Maximization training.
Results show that our aligner reaches an
87.1% F-Score score with the experimen-
tal data, and enhances AMR parsing.

1 Introduction

Abstract Meaning Representation (AMR) (Ba-
narescu et al., 2013) is a semantic representation
that expresses the logical meaning of English sen-
tences with rooted, directed, acylic graphs. AMR
associates semantic concepts with the nodes on
a graph, while the relations are the label edges
between concept nodes. Meanwhile, AMR re-
lies heavily on predicate-argument relations from
PropBank (Palmer et al., 2005), which share
several edge labels. The representation also en-
codes rich information, like semantic roles (all
the “ARGN” tags from PropBank), named enti-
ties (NE) (“person”, “location”, etc., concepts),
wiki-links (“:wiki” tags), and co-reference (reuse
of variables, e.g., p). An example AMR in PEN-
MAN format (Matthiessen and Bateman, 1991) is
shown in Figure 1.

The design of an AMR to English sentence
aligner is the first step for implementation of an
AMR parser, since AMR annotation does not
contain links between each AMR concept and
the original span of words. The basic alignment
strategy is to link the AMR tokens (either con-
cepts or edge labels) with their corresponding

(j / join-01
:ARG0 (p / person :wiki -

:name (p2 / name
:op1 "Pierre" :op2 "Vinken")

:age (t / temporal-quantity :quant 61
:unit (y / year)))

:ARG1 (b / board
:ARG1-of (h / have-org-role-91

:ARG0 p
:ARG2 (d2 / director

:mod (e / executive :polarity -))))
:time (d / date-entity :month 11 :day 29))

Figure 1: The AMR annotation of sentence “Pierre
Vinken, 61 years old, will join the board as a
nonexecutive director Nov. 29.” in PENMAN for-
mat

span of words. Another strategy is to find the
alignment from an AMR concept to a word node
in a dependency parse tree, the goal of this paper.
A dependency parse tree is a good structure for
attaching more information, e.g. named entity
tags, lemma, and semantic role labels, etc., and
provides richer syntactic information than the
span of words. An alignment between an AMR
concept and a dependency node represents a cor-
respondence between the meaning of this concept
and its child concepts and the phrase governed by
the dependency node (i.e., head word). An exam-
ple alignment is shown in Figure 2. For example,
the word node “Vinken” on the dependency parse
side in Figure 2 links to the lexical concept of
“Vinken” and, furthermore, links to the “p2/name”
and the “p/person” concepts since “Vinken” is
the head of the named entity “Pierre Vinken”
and the head of the whole noun phrase
“Pierre Vinken, 61 years old.”. In our work, we
use Expectation-Maximization(EM) (Dempster et
al., 1977) to train different feature probabilities,
including rule-based features, lexical forms,
relation labels, named entity tags, semantic role

558



Pierre Vinken , 61 years old , will join

nn punct

amod

advmodnum

punct

nsubj

aux

root

“Pierre” “Vinken” 61 year

name temporal-quantity

person

join-01

op1 op2 quant unit

name age

Arg0 Arg1

Arg0

Figure 2: The alignment between a subgraph of an
AMR (top) and a dependency parse (bottom) for
the “Pierre Vinken” sentence. Dashed lines link
dependency parse nodes and corresponding con-
cepts.

labels, and global features, etc. Then EM process-
ing incorporates all the individual probabilities
and estimates the final alignments.

We will describe AMR-English sentence align-
ment in general, and review related work, in
Section 2. Then the descriptions of our AMR-
dependency parse features and alignment model
are in Section 3. Our beam-search docoder is de-
scribed in Section 4. Our experimental results are
presented in Section 5, followed by our conclusion
and discussion of future work (Section 6).

2 AMR-English Sentence Aligner

A preliminary step for an AMR parser is aligning
AMR concepts and the original spans of words.
JAMR (Flanigan et al., 2014) includes a heuris-
tic alignment algorithm between AMR concepts
and words or phrases from the original sentence.
They use a set of alignment rules, like named en-
tity, fuzzy named entity, data entity, etc., with a
greedy strategy to match the alignments. This
aligner achieves a 90% F1 score on hand aligned
AMR-sentence pairs. On the other hand, the ISI
Aligner (Pourdamghani et al., 2014) presents a

generative model to align AMR graphs to sen-
tence strings. They propose a string-to-string
alignment model which transfers the AMR ex-
pression to a linearized string representation as
the initial step. Their training method is based
on the IBM word alignment model (Brown et
al., 1993) but they modify the objective function
of the alignment model. IBM Model-4 with a
symmetric method reaches the highest F1 score,
83.1%. When separating the alignments into roles
(edge labels) and non-roles (concepts), F1 scores
are 49.3% and 89.8%, respectively. In Werling’s
AMR parser (Werling et al., 2015), they con-
ceive of the alignment task as a linear program-
ming relaxation of a boolean problem. The ob-
jective function is to maximize the sum of ac-
tion reliability. Each concept is constrained to
align to exactly one token in a sentence. This
ensures that only adjacent nodes or nodes that
share the same title refer to the same token. They
hand-annotate 100 AMR parses, and their aligner
achieves an accuracy of 83.2%. By providing al-
ternative alignments to their graph-based AMR
parser, their aligner achieves a better Smatch score
than JAMR’s aligner.

However, two transition-based parsers which
parse dependency parse tree structures into AMRs,
e.g., the CAMR system (Wang et al., 2015; Wang
et al., 2016) and the RIGA system (Barzdins and
Gosko, 2016), tie for the best results in SemEval-
2016 task 8 (May, 2016). It is important to
note that the JAMR aligner was not designed to
align between a dependency word node and an
AMR concept where its alignment F1 score is only
69.8% (see Section 5.2). In order to deal with
this problem, (Chen, 2015) proposed a preliminary
aligner which estimates alignments by learning the
feature probabilities of lexical (surface) forms, re-
lations, named entities and semantic roles jointly.
Besides the objective to obtain alignment between
AMR concepts and original word spans, the esti-
mation of these feature probabilities is also useful
for further development of the AMR parser with
these initial models. In our paper, we extend their
previous work by adding rule-based and global
features, and adding a beam-search algorithm at
decoding time.

3 AMR-Dependency Parse Aligner

Our approach is an AMR-to-Dependency parse
aligner, which represents one AMR as a list of

559



Concepts C = 〈c1, c2, . . . , c|C|〉, and the corre-
sponding dependency parse as a list of dependency
word nodes D = 〈d1, d2, . . . , d|D|〉. An align-
ment function a is designed to produce exactly one
alignment to a dependency node dcj for each con-
cept cj , within a single sentence. Alternatively,
we can view a as a mapping function that accepts
one input variable concept cj and outputs a de-
pendency node dcj with which cj is aligned. A
is the alignment set that contains all different al
that cover possible alignments within C and D.
Our model adopts an asymmetric alignment direc-
tion, where one concept maps to exactly one de-
pendency parse node, and each dependency parse
node can be aligned by zero to multiple concepts.
We denote dependency node dc linked by concept
c as dc = a(c). cp is the parent concept of concept
c, while cs1 , cs2 , ..., csk are the k child concepts of
concept c.

3.1 Features

3.1.1 Basic Features
Several of the AMR concepts use the word form
directly. For example, the concept “join-01” in
Figure 1 would align to the dependency node
“join” naturally. Similarly, the leaf concepts usu-
ally align to identical terms in the dependency
parse. In Figure 1, the names “Pierre” and
”Vinken” are aligned to their word forms on the
dependency parse leaves. Therefore, we design
a straightforward rule-based probability, Prule,
which catches the appearance of the surface form.
Prule(c, dc) is defined as the probability that the
matching type for a given concept c and depen-
dency node dc are linked. The different types of
rules, e.g., word, lemma, numbers, and date, etc.,
and their proportional applicability to both AMR
concepts and leaves are listed in Table 1. For ex-
ample, the rule “Date” type aligns concept “11”
with word node “November” in Figure 1, while
“Numbers” aligns concept “5” with word node
“five”. Prule decides which match type to apply
by following a greedy matching strategy.

3.1.2 External Features
To capture alignments for concepts which do not
match any of the above basic rules, we design the
following four external feature probabilities:

PLemma(c, dc) = P (c|Word(dc))
Lemma Probability represents the likelihood that
a concept c aligns to a dependency word di. For

Match Type at Concept at Leaf
(1) Word 45.2% 73.4%

(2)
Word

- 0.9%
(case insensitive)

(3)
Lemma 10.8% 0.3%
(case insensitive)

(4)
Partial match 6.1% 8.2%
with word

(5)
Partial match 0.2% 0.3%
with lemma

(6) Numbers - 3.1%
(7) Ordinal Numbers - 2.8%
(8) Date - 4.3%
(9) Others 37.7% 6.5%

Table 1: The rules and distribution of basic match
types

example, in Figure 3a, the concept c =“temporal-
quantity” is highly likely to align to the word node
dc = “old” since “old” is usually the head word
of a phrase expressing age (“61 years old” here).
Also, have-org-role-91 can align to the word node
“director” since “director” appears quite often
with have-org-role-91 (defined as roles in orga-
nizations). Besides, some special leaf concepts,
like “:polarity -” (negative), and “:mode expres-
sive” (which is used to mark exclamational word),
also rely on this feature rather than the basic rules.

Prel(c, dc, dcp) = P (AMRLabel(c)|Path(dc, dcp))
Relation Probability is the conditional probabil-
ity of the AMR relation label of c, given the parse
tree path between dc and dcp , where dc and dcp
represent the dependency nodes that are aligned
by c and cp, respectively. Parse tree path is the
concatenation of all dependency tree and direc-
tion labels through the tree path between dc and
dcp . For example, the relation probability of c =
61, dc = 61, and dcp = old in Figure 3b is
P (quant|advmod ↓ num ↓). A parse tree path
is a useful feature for extracting relations between
any two tree nodes, e.g., Semantic Role Labeling
(SRL) (Gildea and Jurafsky, 2002) and relation ex-
traction (Bunescu and Mooney, 2005; Kambhatla,
2004; Xu et al., 2015), so we add relation proba-
bility to our model.

PNE(c, dc) = P (c|NamedEntity(dc))
Named Entity Probability is the probability of
the concept c conditioned on different named en-
tity types (e.g., PERSON, DATE, ORGANIZA-

560



61 years old

advmodnum

temporal-
quantity

61 year

quant unit

(a) Lemma Feature Probability
Plemma(c = temporal-quantity, dc = old)

61 years old

advmodnum

temporal-
quantity

61 year

quant unit

(b) Relation Feature Probability
Prel(c = 61, dc = 61, dcp = old)

Pierre Vinken , 61 years old

nn punct

amod

advmodnum

nsubj

PERSON

person

name
temporal-
quantity

name age

(c) Named Entity Probability
PNE(c = person, dc = V inken)

Pierre Vinken , 61 years old , will join
nsubj

Arg0

root

join-01

person board

ARG0 ARG1

(d) Semantic Roles Probability
PSR(c = person, dc = V inken, dcp = join)

Figure 3: The sample of feature probabilities that are used in our aligner. Dashed lines link AMR
concepts (top) and corresponding dependency parse nodes (bottom), while dense dashed lines link the
parent AMR concepts and its corresponding dependency parse nodes.

TION, etc.). NamedEntity(d) indicates the
named entity type of the phrase with d as the head
word. For example, after named entity recogni-
tion (NER) tagging, the label assigned to “PER-
SON” is the dependency parse tree node “Vinken”.
So the named entity probability of PNE(c =
person, dc = V inken) in Figure 3c is P (person
| PERSON). Since AMR contains a large amount
of named entity information, we assume that a fea-
ture based on an external named entity module
should improve the alignment accuracy.

PSR(c, dc, dcp)
= P (AMRLabel(c)|SemanticRole(dcp , dc))
Semantic Role Probability is the conditional
probability of the AMR relation label of c, given
the semantic role dc if dcp is a predicate and dc
is dcp’s argument. If a predicate-argument struc-
ture does not exist between dcp and dc, the se-
mantic role probability is omitted. For exam-
ple, in Figure 3d, the semantic role probability of
PSR(c = person, dc = V inken, dcp = join) is
equal to P (ARG0|Arg0). Since AMR depends
heavily on predicate-argument relations, external

predicate-argument information from an external
SRL system should enhance the overall alignment
accuracy.

The above four feature probabilities are learned
by the EM algorithm (Section 3.2).

3.1.3 Global Feature

The above basic and external features capture lo-
cal alignment information. However, to make sure
that a concept is aligned to the correct phrase head
word which represents the same sub-meaning, we
need a global feature to calculate coverage. The
design of our concept coverage feature is as fol-
lows:

RCC(c) Overlapping Ratio of the child concept
aligned phrases to their parent concept aligned
phrases plus the non-covered penalty. This ratio
is defined as:

561



61 years old

advmodnum

temporal-
quantity

61 year

quant unit

Figure 4: A sample of incorrect alignment. We use
this sample to calculate its overlapping ratio (Rcc)
let c = temporal-quantity
W (c) = {61, year}
Wchild(c) = {61} ∪ {61, year, old}
= {61, year, old}
Wchild(c) ∩W (c) = {61, year}
pen(c) = exp(−|{old}|) = 0.37
Rcc(c) = (22)× pen(c) = 0.37

RCC(c) =
|Wchild(c) ∩W (c)|

|W (c)| × pen(c)

W (c) = dc

Wchild(c) =
⋃

csi∈child(c)
dcsi

pen(c) = exp(−|Wchild(c) \ (Wchild(c) ∩W (c))|)

where W refers to the set of words that the
aligned dependency word node contains. The first
term of RCC ensures the child concepts contain
the largest possible subspans of the parent con-
cept span. The non-covered penalty term (pen)
is to prevent a child concept from aligning to a
word node that contains a larger word span than
the child’s parent concept. The pen term will in-
crease exponentially if child concepts align to a
larger word span. The back slash term “\” refers to
set subtraction. We take Figure 4 as an example of
an incorrect alignment example where the concept
“temporal-quantity” aligns to “year” and the con-
cept “year” aligns to “old”, the overlapping ratio
of this alignment is 0.37 since it suffers a penalty.
As we compare it with the correct alignment in
Figure 3b, the overlapping ratio of this alignment
is 0.67, which is much higher than the incorrect
one.

3.2 Training with EM Algorithm
The objective function of our AMR-to-
Dependency Parse aligner is listed as follows:
Since our long term goal is to design a de-
pendency parse to AMR parser, we define the
objective function Lθ as the probability that
dependency parses transfer to AMR graphs for
the AMR-to-Dependency Parse aligner:

θ = argmaxLθ(AMR|DEP) (1)

Lθ(AMR|DEP) =
∏

(C,D,A)
∈S

P (C|D)

=
∏

(C,D,A)
∈S

∑
a∈A

P (C, a|D) (2)

P (C, a|D) =
|C|∏
j=1

P (cj |dcj = a(cj), dcpj = a(c
p
j ))

(3)
where θ = (Plemma, Prel, PNE , PSR) is the set of
feature probabilities (parameters) we want to es-
timate, alignment set A is the latent variable we
want to observe, and S is the training sample that
contains a set of tuples (C,D,A), where C and
D are a 〈AMR, dependency parse〉 pair and A is
their alignment combination set. In equation (3),
the probability that dependency tree D translates
to AMR C with an alignment combination a is
equal to the product of all probabilities that con-
cept cj in C aligns to dependency node dcj and c

p
j

aligns to dependency node dcpj .

3.2.1 Expectation-Step
The E-Step estimates all the different alignment
probabilities of an input AMR and dependency
parse pair by giving the product of feature prob-
abilities. The alignment probability can be calcu-
lated using:

P (a|C,D) =
|C|∏
j=1

P (cj |dcj , dcpj )∑|D|
l=1

∑|D|
i=1 P (cj |di, dl)

(4)

P (cj |di, dl) = Prule(cj , di)× Plemma(cj , di)
× Prel(cj , di, dl)× PNE(cj , di)× PSR(cj , di, dl)

(5)

The alignment probability is equal to the prod-
uct of all tuple (c, dc, dcp)’s aligning probabilities.

562



Prule is obtained by a simple calculation from the
development set, while Plemma, Prel, PNE , and
PSR are initialized uniformly before the first round
of E-step. And these feature probabilities will be
updated during the M-step.

3.2.2 Maximization-Step
In the M-Step, feature probabilities are re-
estimated by collecting the count of all AMR-
dependency parse pairs. The count of lemma
(cntlemma), relation (cntrel), named entity
(cntNE), and semantic role (cntSR) features
are the normalized counts that are collected
from the accumulating probability of all pos-
sible alignments from the E-step. Here we
take the derivation of cntlemma as an example.
cntrel, cntNE , and cntSR can be obtained with
similar equations:

cntlemma(c|Word(dc);C,D) =∑
a∈A

P (c|dc, dcp)∑|D|
i=0

∑|D|
l=0 P (c|di, dl)

After we collect all counts for differ-
ent features, the four feature probabilities,
Plemma, Prel, PNE , and PSR, are updated with
their feature counts. Here we show the update
of Plemma as an example. The rest of feature
probability updates can be derived in the same
way:

P lemma(c, d)

←
∑

C∈AMR,
D∈DEP

cntlemma(c|Word(d);C,D)∑|C|
j=1 cntlemma(cj |Word(d);C,D)

After this, we apply the newer feature probabilities
to recalculate alignment probabilities in the E-step
again. EM iterates the E and M-steps until conver-
gence or certain criteria are met.

4 Decoding

At decoding time, we want to find the most likely
alignment a for the given 〈C,D〉. By applying
Equations (4) and (5), we define the search for
alignments as follows:

argmax
a

P (a|C,D) = argmax
a

|C|∏
j=1

RCC(cj)

∗ P (cj |di = a(cj), dl = a(cpj ))
This decoding problem finds the alignment a that
maximizes the likelihood, which we define in

Sent. Token # of
NE

# of
Arg.

G
ol

d train 8,276 176,422 3,750 58,520
dev. 409 8,695 415 2,574
test 415 8,786 401 3,107

A
ll

train 39,260 649,219 43,715 260,979
dev. 409 8,695 580 2,574
test 415 8,786 401 3,107

Table 2: The data split of the LDC DEFT AMR
corpus. Gold refers to the sentences also appear-
ing in OntoNotes 5.0 with gold annotations, while
All refers to all sentences in DEFT AMR corpus
with dependency parses, named entities, and se-
mantic roles generated by ClearNLP. Number of
tokens, named entities, and arguments(Arg.) in
each data set are also presented

Equation (5). The overlapping ratio(RCC) is in-
troduced to the likelihood function to ensure that
a parent concept covers a wider word span range
than its child concepts. A beam search algorithm
is designed to extract the target alignment without
exhaustively searching all of the candidate align-
ments (which has a complexity of O(|D||C|).)
The beam search starts from leaf concepts and
then walks through parent concepts after their
child concepts have been traversed. When we go
through concept cj , we need to consider all the
following likelihoods: 1) the accumulated likeli-
hood for aligning to any dependency word node
dcj from all the child concepts of cj , and 2) the
product of Plemma, PNE , Prel, PSR, and RCC
for cj . Instead of using during training, RCC is
only applied during decoding time. The probabili-
ties are obtained simply from the product of all the
above likelihoods. We keep the top-|b| alignment
probabilities and their aligned dependency node
dcj for each cj until we reach the root concept,
where |b| is the beam size. Finally we can trace
back and find the most likely alignment.

The running time for the beam search algorithm
is O(|b| ∗ |C| ∗ |D|2).

5 Experiments and Results

5.1 Experimental Data

The LDC DEFT Phase 2 AMR Annotation Re-
lease 2.01 consists of AMRs with English sentence

1LDC DEFT Phase 2 AMR Annotation Release 2.0, Re-
lease date: March 10th, 2016. https://catalog.ldc.upenn.edu
/LDC2016E25

563



pairs. Annotated selections from various gen-
res (including newswire, discussion forum, other
web logs, and television transcripts) are avail-
able, for a total of 39,260 sentences. This release
uses the PropBank Unification frame files (Bo-
nial et al., 2014; Bonial et al., 2016). To gen-
erate automatic dependency parses for all DEFT
AMR Release data, we use ClearNLP (Choi and
Mccallum, 2013) to produce dependency parses.
ClearNLP also labels semantic roles and named
entity tags automatically on the generated de-
pendency parses. This data set is named “All”.
To compare the effect of applying automatic de-
pendency parses to our aligner with gold depen-
dency parses, we select the sentences which ap-
pear in the OntoNotes 5.02 release as well. The
OntoNotes data contains TreeBanking, PropBank-
ing, and Named Entity annotations. OntoNotes
5.0 also uses PropBank Unification frame files
for PropBanking. This data set, containing a to-
tal of 8,276 of selected AMRs and their depen-
dency parses from OntoNotes, is named “Gold”.
To generate the development and test set, we man-
ually align the AMR concepts and dependency
word nodes. Since the manual alignment is time-
consuming, “Gold” and “All” data share the same
development/test set. Table 2 presents the statis-
tics for the experimental data.3

5.2 Experiment Results

We run EM for 50 iterations and ensure the EM
model converges. Afterwards, we use our decod-
ing algorithm to find the alignments that maximize
the likelihood. The test set data is used to evaluate
performance.

We first evaluate the performance of our system
with the external features added incrementally. Ta-
ble 3 indicates the results. By running with the
“Gold”‘ data, the only feature that improves sig-
nificantly over the baseline (rule-based and lexi-
con features only) is the semantic role feature. The
named entity feature actually hurts performance.
On the other hand, all the features contribute to
the F-Score incrementally for “All”. Again, the se-
mantic role feature still has the most positive im-
pact against other features, and a significant im-
provement over the baseline.

As we compare the F1 score on training with

2LDC OntoNotes Release 5.0, Release date: October
16th, 2013 https://catalog.ldc.upenn.edu/LDC2013T19

3The manually aligned data and our aligner will be avail-
able after this paper gets accepted

Data Feature P R F-Score

Gold

L 84.0 85.0 84.5
L + S 85.2 86.3 85.7
L + S + R 82.8 83.8 83.3
L + S + R + N 80.9 81.9 81.4

All

L 84.9 85.4 85.1
L + S 85.7 87.4 86.5
L + S + R 85.8 87.7 86.7
L + S + R + N 86.3 88.0 87.1

Table 3: Incremental Feature Contributions for
different features: L: lemma; R: relation; N : NE;
S: semantic role.

“All” and “Gold” data set, training with “All” out-
performs training with “Gold” data in all differ-
ent feature combinations. We believe there are
two reasons for this. First, the “All” data contains
richer information than the “Gold” data. “All” has
double the sentence size of “Gold”, and propor-
tionally more named entity labels. Second, the
automatic dependency parses do not hurt the per-
formance of our aligner very much. We believe
that our unsupervised alignment model works bet-
ter with more data, even without access to gold
standard dependency parses.

We then compare our aligner with three other
aligners: JAMR, another version of unsupervised
alignment (Chen, 2015), and ISI. To make them fit
our test data, we design a heuristic method to force
every unaligned concept (e.g., named entity and
“‘:polarity -”’ concepts) to align to a dependency
word node according to rule-based and global fea-
tures (see Section 3.1). The alignment is counted
as a correct match when the concept aligns to ei-
ther the head word or the partial word span of a
phrase. The alignments from concept relation to
word span (apply in ISI) are discarded in our task.
The results of the experiment are shown in Table 4.
Our aligner achieves the best F1 score in both the
“All” and “Gold” data sets, as it should, since it is
designed to align AMRs to dependency parses, as
was the Chen aligner. Our aligner performs better
than the Chen aligner by around 28% in F1 score.
We can conclude that the addition of rule-based
feature, global features, and beam-search in de-
coding time helps the alignment task substantially.

5.3 Apply to AMR Parsing

To evaluate how alignment can enhance AMR
parsing, we compare the parsing performance of

564



Data Aligner P R F-Score

Gold

Chen 2015 61.1 53.4 57.0
JAMR 78.5 62.8 69.8
ISI 78.6 71.4 74.9
Ours 85.2 86.3 85.7

All

Chen 2015 62.4 55.5 58.7
JAMR 80.2 65.9 72.4
ISI 80.4 74.9 77.6
Ours 86.3 88.0 87.1

Table 4: Results of different alignment models

the CAMR parser with different alignments pro-
duced by JAMR, ISI, and our aligner. To make
the alignments fit the CAMR parser, we convert
both ISI and our alignments to the original JAMR
alignment format, word span to AMR concept. We
get rid of the “:wiki” tag, which links the named
entity to its Wikipedia page, to simplify the pars-
ing task since we think the Wikify task (Mihalcea
and Csomai, 2007) is basically different from the
AMR parsing task. Smatch v2.0.2 is used to eval-
uate AMR parsing performance (Cai and Knight,
2012). The evaluation script is obtained from the
SemEval 2016 Task 8 website4.

A comparison of parsing results is given in Ta-
ble 5. We first train the parser with “Gold” Stan-
dard dependency parses and alignments from the
different aligners. Results show that our aligner
improves by a 2% F1 score over the two other
aligners. Then we train the AMR Parser system
with the “All” data set. The dependency parses at-
tached with semantic roles and named entities gen-
erated by ClearNLP are also provided to CAMR
as training data. CAMR use dependency parsing
results from Stanford dependency parser (Klein
and Manning, 2003) by default. Our aligner still
achieves slightly better performance than the other
two. Modifying the AMR parser to take advantage
of parse node-concept alignments could poten-
tially result in greater improvement, since CAMR
takes the input alignments as word span to AMR
concept.

5.4 Error Analysis

To further understand the advantages and the dis-
advantages of our model, we go through all incor-
rect alignments and manually categorize 40% of
them into different error types, with their propor-

4http://alt.qcri.org/semeval2016/task8/index.php?id=data-
and-tools

Data Aligner P R F-Score

Gold
JAMR 62.2 61.0 61.1
ISI 65.3 63.9 64.5
Ours 68.6 64.2 66.4

All
JAMR 64.2 63.0 63.1
ISI 66.1 65.1 65.6
Ours 68.1 64.7 66.7

Table 5: Comparison of using different alignments
with CAMR Parser.

tion:

Automatic Parsing Errors - 3.8%: ClearNLP has
a 92.96% unlabeled attachment score on the Penn
English Treebank evaluation set (Marcus et al.,
1993), Section 23, for dependency parsing. There-
fore, when training our aligner on the “All” data
set with dependency parses, named entities, and
semantic roles generated by ClearNLP, incorrect
parses occasionally show up. Since NE and se-
mantic roles are attached to dependency parses,
incorrect dependency parses cause additional NE
and semantic roles alignment errors, on top of the
dependency parse alignment errors.

Long Distance Dependencies - 14.2%: Long sen-
tences with long distance dependencies always
bring difficulty to NLP parsing tasks. Experimen-
tal results show that our model runs into troubles
when nearby concepts align to dependency nodes
which are far from each other. Co-reference is an
example that is highly likely to align to long dis-
tance dependencies, and our model can not deal
with it well.

Duplicate Words - 17.4%: When two identical
concepts align to different word nodes, our model
is confused by duplicate words. In Figure 5, there
are two “first”s in the sentence. One refers to
“first 6 rounds”, and the other refers to “first po-
sition”. However, our model faultily aligns both
ordinal-entity concepts to the same “first” word
node. Our model did not distinguish these two
ordinal-entities since the lexicon and named entity
tags of the two “first”s are identical.

Meaning Coverage Errors - 40.4%: We define a
good alignment as a concept that aligns to the cor-
rect phrase head word which represents the same
sub-meaning. So instead of aligning to a concept’s
word lexicon, sometimes a concept aligns to its
parent node (head word). However, the lexicon
features dominate the alignment probability in our

565



(a / and
:op1 (o / occupy-01

:ARG0 (p3 / person
:name (n / name

:op1 "Mingxia" :op2 "Fu"))
:ARG1 (p4 / position
:ord (o3 / ordinal-entity :value 1))

:op2 (o2 / occupy-01
:ARG0 (p / person
:name (n2 / name

:op1 "Bin" :op2 "Chi"))
:ARG1 (p2 / position
:ord (o4 / ordinal-entity :value 3))

:mod (r / respective)
:time (r3 / round-05 :quant 6

:ARG1 (c / compete-01)
:ord (o5 / ordinal-entity :value 1)))

Figure 5: The AMR annotation of sentence “In
the first 6 rounds of competition, Mingxia Fu and
Bin Chi are occupying the first and third positions
respectively”

E-M calculation. That causes our model to tend to
align a concept with its word form instead of its
head word. For example, English light verb con-
structions (LVCs), e.g., take a bath, are thought to
consist of a semantically general verb and a noun
that denotes an event or state. AMR representation
always drops light verb and uses eventive noun as
concept. Our model sometimes aligns this even-
tive noun concept to its nominal word node, which
is incorrect since the light verb on dependency
parse covers the same sub-meaning and should be
aligned.

6 Conclusion and Future Work

In this paper, we present an AMR-Dependency
Parse aligner, which estimates the feature proba-
bilities by running the EM algorithm. It can be
used directly by AMR parser. Results show that
our aligner performs better than other aligners, and
improves AMR parser performance. The latent
probabilities that we obtain during training, i.e.,
all the external feature sets, could also potentially
benefit a parser. We plan to develop our own AMR
parser, which will apply these external feature sets
as the basic model. We also plan to continue to
perfect our aligner via tuning the feature weights
and learning techniques, and adding new features,
like word embeddings and WordNet features.

Acknowledgments

We gratefully acknowledge the support of the Na-
tional Science Foundation Grants 0910992 IIS:RI:
Richer Representations for Machine Translation,
and NSF IIA-0530118 PIRE (a subcontract from
Johns Hopkins) for the 2014 Frederick Jelinek
Memorial Workshop for Meaning Representations
in Language and Speech Processing, and funding
under 2016 Summer Graduate School Fellowship
of University of Colorado at Boulder. Any opin-
ions, findings, and conclusions or recommenda-
tions expressed in this material are those of the
authors and do not necessarily reflect the views of
the National Science Foundation.

References
Laura Banarescu, Claire Bonial, Shu Cai, Madalina

Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin
Knight, Philipp Koehn, Martha Palmer, and Nathan
Schneider. 2013. Abstract meaning representation
for sembanking.

Guntis Barzdins and Didzis Gosko. 2016. RIGA at
semeval-2016 task 8: Impact of smatch extensions
and character-level neural translation on AMR pars-
ing accuracy. CoRR, abs/1604.01278.

Claire Bonial, Julia Bonn, Kathryn Conger, Jena D.
Hwang, and Martha Palmer. 2014. Propbank: Se-
mantics of new predicate types. In Nicoletta Cal-
zolari (Conference Chair), Khalid Choukri, Thierry
Declerck, Hrafn Loftsson, Bente Maegaard, Joseph
Mariani, Asuncion Moreno, Jan Odijk, and Stelios
Piperidis, editors, Proceedings of the Ninth Interna-
tional Conference on Language Resources and Eval-
uation (LREC’14), Reykjavik, Iceland, may. Euro-
pean Language Resources Association (ELRA).

Claire Bonial, Kathryn Conger, Jena D. Hwang, Aous
Mansouri, Yahya Aseri, Julia Bonn, Timothy OGor-
man, and Martha Palmer. 2016. Current directions
in english and arabic propbank. In Nancy Ide and
James Pustejovsky, editors, The Handbook of Lin-
guistic Annotation. Springer, Berlin.

Peter F. Brown, Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: Pa-
rameter estimation. Comput. Linguist., 19(2):263–
311, June.

Razvan C. Bunescu and Raymond J. Mooney. 2005.
A shortest path dependency kernel for relation ex-
traction. In Proceedings of the Conference on Hu-
man Language Technology and Empirical Methods
in Natural Language Processing, HLT ’05, pages
724–731, Stroudsburg, PA, USA. Association for
Computational Linguistics.

566



Shu Cai and Kevin Knight. 2012. Smatch: an evalu-
ation metric for semantic feature structures. submit-
ted.

Wei-Te Chen. 2015. Learning to map dependency
parses to abstract meaning representations. In Pro-
ceedings of the ACL-IJCNLP 2015 Student Research
Workshop, pages 41–46, Beijing, China, July. Asso-
ciation for Computational Linguistics.

Jinho D. Choi and Andrew Mccallum. 2013.
Transition-based dependency parsing with selec-
tional branching. In In Proceedings of the 51st An-
nual Meeting of the Association for Computational
Linguistics.

A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the
em algorithm. JOURNAL OF THE ROYAL STATIS-
TICAL SOCIETY, SERIES B, 39(1):1–38.

J. Flanigan, S. Thomson, J. Carbonell, C. Dyer, and
N. A. Smith. 2014. A discriminative graph-based
parser for the abstract meaning representation. In
Proc. of ACL, Baltimore, Maryland, June. Associa-
tion for Computational Linguistics.

Daniel Gildea and Daniel Jurafsky. 2002. Auto-
matic labeling of semantic roles. Comput. Linguist.,
28(3):245–288, September.

Nanda Kambhatla. 2004. Combining lexical, syntac-
tic, and semantic features with maximum entropy
models for extracting relations. In Proceedings of
the ACL 2004 on Interactive Poster and Demon-
stration Sessions, ACLdemo ’04, Stroudsburg, PA,
USA. Association for Computational Linguistics.

Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics - Volume 1, ACL ’03, pages 423–
430, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.

Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of english: The penn treebank. Com-
put. Linguist., 19(2):313–330, June.

C. Matthiessen and J.A. Bateman. 1991. Text gen-
eration and systemic-functional linguistics: experi-
ences from English and Japanese. Communication
in artificial intelligence. Pinter.

Jonathan May. 2016. Semeval-2016 task 8: Mean-
ing representation parsing. In Proceedings of the
10th International Workshop on Semantic Evalua-
tion (SemEval-2016), pages 1063–1073, San Diego,
California, June. Association for Computational
Linguistics.

Rada Mihalcea and Andras Csomai. 2007. Wikify!:
Linking documents to encyclopedic knowledge. In
Proceedings of the Sixteenth ACM Conference on

Conference on Information and Knowledge Man-
agement, CIKM ’07, pages 233–242, New York,
NY, USA. ACM.

Martha Palmer, Dan Guildea, and Paul Kingsbury.
2005. The Proposition Bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71–105, March.

Nima Pourdamghani, Yang Gao, Ulf Hermjakob, and
Kevin Knight. 2014. Aligning english strings with
abstract meaning representation graphs. In Proceed-
ings of the 2014 Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages
425–429. Association for Computational Linguis-
tics.

Chuan Wang, Xue Nianwen, and Pradhan Sameer.
2015. A transition-based algorithm for amr parsing.
In Proceedings of the 2015 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies.
Association for Computational Linguistics.

Chuan Wang, Nianwen Xue, Sameer Pradhan Prad-
han, Xiaoman Pan, and Heng Ji. 2016. Camr at
semeval-2016 task 8: An extended transition-based
amr parser. In Proceedings of the 10th International
Workshop on Semantic Evaluation (SemEval 2016).
Association for Computational Linguistics, June.

Keenon Werling, Gabor Angeli, and Christopher D.
Manning. 2015. Robust subgraph generation
improves abstract meaning representation parsing.
CoRR, abs/1506.03139.

Yan Xu, Lili Mou, Ge Li, Yunchuan Chen, Hao Peng,
and Zhi Jin. 2015. Classifying relations via long
short term memory networks along shortest depen-
dency paths. In In Proceedings of Conference on
Empirical Methods in Natural Language Process-
ing.

567


