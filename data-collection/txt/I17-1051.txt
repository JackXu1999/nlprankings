



















































Cross-Lingual Sentiment Analysis Without (Good) Translation


Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 506–515,
Taipei, Taiwan, November 27 – December 1, 2017 c©2017 AFNLP

Cross-Lingual Sentiment Analysis Without (Good) Translation

Mohamed Abdalla and Graeme Hirst
msa@ and gh@cs.toronto.edu

Department of Computer Science, University of Toronto
Toronto, Canada

Abstract

Current approaches to cross-lingual senti-
ment analysis try to leverage the wealth of
labeled English data using bilingual lexi-
cons, bilingual vector space embeddings,
or machine translation systems. Here
we show that it is possible to use a sin-
gle linear transformation, with as few as
2000 word pairs, to capture fine-grained
sentiment relationships between words in
a cross-lingual setting. We apply these
cross-lingual sentiment models to a di-
verse set of tasks to demonstrate their
functionality in a non-English context. By
effectively leveraging English sentiment
knowledge without the need for accurate
translation, we can analyze and extract
features from other languages with scarce
data at a very low cost, thus making sen-
timent and related analyses for many lan-
guages inexpensive.

1 Introduction

Methods for sentiment analysis and classification
have largely been limited to English, making use
of large amounts of labeled data to produce senti-
ment classification. As a consequence, many de-
veloped approaches cannot be readily applied to
other languages, which usually do not have the
wealth of labeled data that is exclusive to English.
Therefore many approaches which deal with other
languages often: i) experiment with small datasets
that are limited in domain or size of training and
testing sets (Lee and Renganathan, 2011; Tan and
Zhang, 2008), or ii) attempt to elucidate sentiment
lexicons for their respective languages (Moham-
mad et al., 2016).

A growing number of publications attempt to
leverage labeled English data to compensate for

the relative lack of training material in the other
languages. This is usually done through the use of
either bilingual lexicons (Balamurali et al., 2012),
machine translation (MT) systems (Salameh et al.,
2015; Zhou et al., 2016), or more recently, through
the use of bilingual vector space embeddings
(Chen et al., 2016).

Unfortunately, in some cases such data is still
expensive to obtain. Many languages do not
have good, or sometimes any, MT systems, and
the cost of producing word alignments or sen-
tence alignments for training bilingual word em-
beddings (BWE) (Zou et al., 2013; Bengio and
Corrado, 2015) or similar techniques (Jain and Ba-
tra, 2015) is prohibitive for data-poor languages.

Here we introduce a high-performance, low-
cost approach to cross-lingual sentiment classifi-
cation, which can be used to benchmark more ex-
pensive methods. We demonstrate the utility of
this approach by highlighting how very limited
training data suffices for effective cross-lingual
sentiment analysis in various contexts (both at the
word and sentence/document level). Our approach
relies on the simple vector space translation matrix
method (Mikolov et al., 2013a), which computes
a matrix to convert from the vector space of one
language to that of another. It hinges on the obser-
vation that sentiment is highly “preserved” even
in the face of poor translation accuracy. We ob-
served that a sentiment classifier trained only with
word vectors from English (hereafter referred to
as the target language) performs well on unseen
words from other unseen languages (referred to
as the source languages) that are translated into
the English vector space through the simple matrix
method, even with very poor translation scores.1

1This work involves transfers in both directions between
English and other languages. We will take the perspective of
the translation matrix (section 3.2) and refer to English as the
target language and the others as source languages.

506



We quantitatively evaluate our methods by
training on English and testing on words from
Spanish and Chinese. We chose these languages
in order to show: i) the robustness of the tech-
nique irrespective of the grammar of the source
and target languages and, ii) the ability to validate
the technique (i.e., as baselines for experiments).
We emphasize that, regardless of the source lan-
guages used, we treated them as if they had no
MT systems. We made use of only 2000, 4500, or
8500 word pairs – a task easily accomplished by
a human translator on a data-poor language. We
experiment with differing amounts of data during
training to show the robustness of our observation.
We then apply the fine-grained sentiment regres-
sor to the task of review classification as done by
Chen et al. (2016), and show that our naı̈ve algo-
rithm achieves results similar to their benchmark
but at a much lower cost.

2 Previous Work

2.1 Cross-Lingual Word Embeddings

Monolingual word embedding algorithms use
large unlabeled datasets to learn useful features
about the given language (Pennington et al., 2014;
Mikolov et al., 2013b). These algorithms learn
vector representations for the words of the lan-
guage — an encoding that has proven utility in
a variety of NLP tasks including sentiment anal-
ysis (Maas et al., 2011) and machine translation
(Mikolov et al., 2013a).

When working with more than one language,
we seek to satisfy two objectives: i) monolin-
gually, similar words of the same language have
similar embeddings; and ii) cross-lingually, simi-
lar words across languages also have similar em-
beddings. Satisfying these two criteria would al-
low us to use algorithms trained for the embed-
dings of a single language (such as English, with a
wealth of labeled data) for other languages as well.
Below we discuss algorithms to achieve the cross-
lingual objective, their costs, performance, and the
rationale underlying our algorithm design.

2.1.1 Offline Alignment
The simplest approach to achieving the cross-
lingual objective is to train each monolingual ob-
jective separately (create a model for each lan-
guage), and then learn a transformation to enforce
the second objective. This approach uses a dic-
tionary of paired words in order to learn a trans-

formation or ‘alignment’ from the vector space of
one language to that of another.

First introduced by Mikolov et al. (2013a), and
later extended by Faruqui and Dyer (2014), this
offline alignment is fast and low cost, but does not
achieve a high translation accuracy. A big draw-
back of these approaches is that using a dictionary
ignores the polysemic nature of languages. It is
also not clear or proven that a single transforma-
tion would be able to capture the relationship be-
tween all the words in a cross-lingual setting.

We opt to use offline alignment to show that
such a low-cost approach does, in fact, capture a
significant part of the relationship between words
of different languages when it comes to sentiment.
That is, a single transformation (linear in the case
of our work) is sufficient to learn a projection
which allows one to use labeled English data to
aid in sentiment analysis.

2.1.2 Parallel-Only
An alternative approach to offline alignment is the
parallel-only approach. Approaches which fall
into this group, such as BiCVM (Hermann and
Blunsom, 2013) and bilingual auto-encoder (BAE)
(Sarath Chandar et al., 2014), rely exclusively on
sentence-aligned parallel data to train a model
with similar representations. Such approaches can
be effective, but require extremely expensive data.
Another drawback is that these approaches can be
affected by the writing style of the parallel text
(Bengio and Corrado, 2015).

2.1.3 Jointly-Trained Model
Combining the offline alignment and parallel-only
algorithms is a third class of jointly-trained ap-
proaches. These approaches jointly optimize the
monolingual objective at the same time as the
cross-lingual objective, making use of both mono-
lingual and parallel data. Approaches like those
of Klementiev et al. (2012) and Zou et al. (2013)
use word-aligned data in order to learn the fine-
grained cross-lingual features and tend to be quite
slow. Other approaches (including that of Ben-
gio and Corrado (2015)) rely on sentence-aligned
data and are faster than those using word-aligned
data. While these models are more cost-efficient
than parallel-only approaches, it remains expen-
sive and sometimes prohibitive to obtain sentence
alignments for many languages (a problem that we
seek to avoid).

A lower-cost alternative to these expensive

507



jointly trained models was proposed by Duong
et al. (2016) and later used to project multiple lan-
guages in the same vector space (Duong et al.,
2017). The model involved creating and mak-
ing use of translations produced using a bilingual
dictionary during training. Using expectation-
maximization–inspired training, sentence transla-
tions were produced by selecting translations of
words based on context to deal with polysemy, and
this approach demonstrated improvements on the
simple linear transformation method. However,
even this model uses a significantly larger amount
of data than the methods used in this work, with
its smallest dictionary being composed of 35,000
word pairs compared to our approach, which can
use as little as 2000 words for both translation and
sentiment regression.

2.2 Cross-Lingual Sentiment Analysis

Previous approaches to cross-lingual sentiment
analysis can be classified into two main cate-
gories: i) those that rely on parallel corpora to
train BWE’s (i.e., they use pre-trained embed-
dings) (Chen et al., 2016; Sarath Chandar et al.,
2014; Tang and Wan, 2014), and ii) those that use
translation systems (Zhou et al., 2015, 2016) in
order to obtain aligned inputs to learn to extract
features which work on both languages. Both ap-
proaches allow the sentiment portion of training
and testing data to be in the same vector space.
However, many languages have no MT system,
and it is extremely expensive to create one on a
language-by-language basis.

Our proposed approach is simpler in that it re-
quires only a small word-list to learn both the em-
bedding and the sentiment classification, the du-
ality of which cannot be claimed by previous ap-
proaches.

3 Methods and Data

3.1 Data

3.1.1 Vector Space Data
English Vector Space Model For English we used
a model pre-trained on part of the Google News
dataset (which is composed of approximately 100
billion words).2 The words are represented by
300-dimensional vectors.
Spanish Vector Space Model For the Span-
ish word embeddings we opted to use a model

2https://code.google.com/archive/p/word2vec/

pre-trained on the Spanish Billion Word Corpus
(Cardellino, 2016). It consists of just under 1.5
billion words compiled from a variety of Spanish
resources. As with the English model, the words
are represented by 300-dimensional vectors.
Chinese Vector Space Model For Chinese word
embeddings we learned our own vector represen-
tations using a Wikimedia dump3 composed of
around 150 million words from 250,000 articles
in both simplified and traditional Chinese. We
used OpenCC4 to translate the articles in simpli-
fied Chinese to traditional. To segment the text
into tokens we used Jieba5. Finally, to create
the actual word embedding model, we used Gen-
sim (Řehůřek and Sojka, 2011) with the minimum
count set to 1, using continuous bag of words
(CBOW), a window of 8, and vector dimension
set to 300.

3.1.2 Word Lists
Translation Word List For the process of learn-
ing a translation matrix from one language to the
other, a lexicon of approximately 10,000 English
words was obtained online by scraping the most
commonly used words as determined by n-gram
frequency analysis in Google’s “Trillion Word
Corpus”6. The lexicon was then translated using
Google Translate7 in order to obtain correspond-
ing words in Spanish and Chinese. For alignment
lists of smaller sizes during experimentation, a
random subset of the larger list was selected. Dur-
ing the randomized selection, we discarded any
words which were not in the target language vector
space and whose translation was not in the source
language(s) vector space(s).
Binary Sentiment Word List For the task of bi-
nary sentiment classification we used a list8 cu-
rated by Hu and Liu (2004) containing both posi-
tive and negative English opinion words (or senti-
ment words). Google Translate was used to trans-
late the list into the other languages to obtain
cross-lingual word pairs. During training and test-
ing we made sure to balance the dataset and to dis-
card words that were not in the vector space of the
target language or whose translation was not in the
vector space of the source language(s).

3https://dumps.wikimedia.org/zhwiki/latest/
4https://github.com/BYVoid/OpenCC
5https://github.com/fxsjy/Jieba
6https://github.com/first20hours/google-10000-english
7https://translate.google.com/
8 https://github.com/williamgunn/SciSentiment

508



Low Stimulus High Stimulus
Arousal relaxed (2.39) infatuation (7.02)

Dominance victim (2.69) confident (7.68)
Valence death (1.61) beauty (7.82)

Table 1: Examples of words on each end of
the spectrum for each of the three dimensions
of ANEW. Numeric stimulus value is shown in
parentheses.

3.1.3 Fine-Grained Data
For fine-grained sentiment regression, we used
Affective Norms for English Words (ANEW)
(Bradley and Lang, 1999). The creators of ANEW
sought to provide emotional ratings for a large
number of words in the English language.

ANEW proposes that all human emotion can be
organized in a vector space with three basic un-
derlying dimensions (or axes). The first dimen-
sion, valence, ranges from pleasant to unpleasant;
the second dimension, arousal, ranges from calm
to excited; and the third dimension, dominance,
ranges from in-control to out-of-control. Exam-
ples are shown in Table 1.

Bradley and Lang (1999) used a nonverbal pic-
tographic measure, the Self-Assessment Manikin
(SAM) (Bradley and Lang, 1994), to measure
stimuli across these three dimensions. The fig-
ures in the SAM consist of bipolar scales depicting
different values along each of the three emotional
dimensions. For example, when considering va-
lence, SAM ranges from a frowning unhappy fig-
ure to a smiling happy figure. Similar ranges are
extended across the two other dimensions. Using
this test, Bradley and Lang were able to arrive at a
numerical value representing a word’s stimulus for
each dimension ranging from 1 to 9; where 1 is the
low value (unpleasant, calm, in-control) and 9 is
the high value (pleasant, excited, out-of-control).

3.1.4 Review Data
In this subsection, we discuss the data used to
replicate the review classification task done by
Chen et al. (2016) as a means of validating the
utility of our model. This experiment was done
using only English and Chinese because there was
no Spanish data for this task and the Arabic review
data-set that Chen et al. (2016) used was not freely
available.
Labeled English Reviews Following Chen et al.
(2016), we obtained a balanced dataset of 700,000
reviews of businesses on Yelp from Zhang et al.

(2015) with their sentiment ratings as labels rang-
ing from 1 for very negative to 5 for very positive.
Labeled Chinese Reviews Here we use a dataset
from Lin et al. (2015). Their work provides ho-
tel reviews, with labels ranging from 1 for very
negative to 5 for very positive. In order to fairly
compare our work with that of Chen et al. (2016),
we use 10,000 reviews for model selection, and
another unseen 10,000 as our test set.

3.2 Translation Matrix Technique
As described by Mikolov et al. (2013a), the trans-
lation matrix technique assumes that we are given
a set of word pairs and their associated vector
space representations. More specifically, we are
given j word pairs, {xi,zi} ji=1 where xi ∈ Rn is
a vector from the source language of word i and
zi ∈ Rm is the vector representation of the corre-
sponding translated word in the target language.

We then want to find a transformation matrix W
such that Wxi approximates zi. We learn this by
solving the following optimization problem:

min
W

j

∑
i=1
||Wxi− zi||2

Instead of solving with stochastic gradient de-
scent, we instead opt to use the closed-form so-
lution.

To translate a word from source language to tar-
get language, we can map it using z = Wx, and
then find the closest word in that language space
using cosine similarity as the measure of distance.
The method of testing was Monte Carlo cross-
validation run 10 times with a split of 90% training
data and 10% test data.

3.3 Models
3.3.1 Binary Sentiment Analysis Model
The sentiment analysis model, Figure 1(a), is a
simple linear support vector machine (SVM) clas-
sifier. Implemented using Sci-kit Learn’s SGD-
Classifier function (Pedregosa et al., 2011), this
model takes a word represented as a vector with
dimension of 300 and outputs a prediction of ei-
ther −1 or +1 (for negative and positive respec-
tively). The classifier itself performs stochastic
gradient descent (SGD) with l2 regularization to
arrive at the best classification.

The training procedure of this model is quite
simple and involves only the target language. The
model is trained only on word embeddings from

509



Figure 1: The three models used in the experi-
ments.

the target language but tested on embeddings re-
turned by the translation of words originally from
the source language(s). We made sure that the En-
glish translation of the test words had not been
seen before in training. The training/testing split
was changed to 80% and 20% from the previous
90%/10% to account for the smaller number of ex-
amples in the dataset (and the fact that we wanted
to test on a representative sample of the data).

3.3.2 Fine-Grained Sentiment Analysis
Model

The fine-grained sentiment analysis model in Fig-
ure 1(b) is a regression model to predict the
ANEW values for each of the three dimensions.
For this task, we built a regressor for each of the
three dimensions whose input is a 300-dimension
vector and whose output is a real number from 1
to 9. The regressor used was a Bayesian Ridge re-
gressor, which estimates a probabilistic model of
the regression problem. The prior for the parame-
ter w is given by a spherical Gaussian:

p(w|λ ) = N (w|0,λ−1Ip).
The model is similar to that of the Ridge regres-
sion. The model was implemented using Sci-kit
Learn’s with hyperparameters alpha 1 and alpha 2
set to 1.

The training procedure of this model is quite
simple and similar to that of the previous model.

1000 WORDS 4500 WORDS 8500 WORDS
Translation P@1 P@5 P@1 P@5 P@1 P@5
EN→ ES 20.3 34.6 33.42 46.13 34.79 47.79
EN→ CN 2.4 11.6 7.60 20.29 8.87 23.01

Table 2: Accuracy of the word translation method.
P@1 and P@5 represent Top-1 and Top-5 accu-
racy respectively.

Again, it only trains on the vector of the target
language, and is tested purely on words from the
source language whose translation into the target
language was not seen in the training of the re-
gressor (so that there may be no chance of skew-
ing the results). The training/testing split here was
75%/25%.

3.3.3 Review Classification Model
For the task of review classification, another
model Figure 1(c), was built to make use of the
previously described fine-grained sentiment anal-
ysis model. The classifier used for this task is a lo-
gistic regression classifier. For a given review ri in
the target language, composed of n words, we con-
struct a 1×Max length sentiment vector (where
Max length is the number of words of the longest
target review). We construct this vector by pass-
ing in the word embedding for every word into the
sentiment analysis model and placing the resulting
values (of 1 to 9) into the constructed array. The
array is then padded with 0’s in order to make it of
length Max length.

For reviews in the source language, the pro-
cess is similar, with the only change being that the
words are first translated from their original vec-
tor space to that of the target language before be-
ing passed into the sentiment classifier. Once the
review vector has been constructed, it is passed to
the classifier to produce a classification of 1 to 5.

As with the previous classifier, the training pro-
cedure is quite simple and involves only the tar-
get language (i.e., the classifier is trained only on
reviews which are originally from the target lan-
guage, but it is tested on reviews only from the
source language).

4 Results

4.1 Translation Accuracy
We first measure the accuracy of the matrix trans-
lation method using the same test described by
Mikolov et al. (2013a). The purpose of these tests
is two-fold: i) verifying that the data used and im-
plementation of the technique reproduces what is

510



Figure 2: Top-1 and Top-5 accuracy for Span-
ish and Chinese using various amounts of training
data.

expected, and ii) quantifying how much sentiment
is preserved with low translation accuracy (by ex-
plicitly noting the poor accuracy of the transla-
tion). The data used is described in section 2.1.
Table 2 shows the effect of training size (number
of words) on the accuracy of the translation matrix
method. As expected (and previously shown by
Mikolov et al. (2013a)), the translation accuracy
increases with more training examples, as shown
in Figure 2. The data and methods used in this
work are further validated as the translation accu-
racy results closely approximate those of Mikolov
et al. (2013a), with English-Spanish translational
accuracy achieving 35% and 48% accuracy for
P@1 and P@5 respectively when compared to the
original 33% and 51%. This concordance is fur-
ther validation of the method. It is also interest-
ing to note that Chinese, which is less like En-
glish than Spanish is, also suffers a lower trans-
lation score across both categories. However, we
see that this large drop is not represented signifi-
cantly in later portions.

4.2 Binary Word Sentiment Classification

The second experiment tested the binary cross-
lingual sentiment classification capabilities of the
matrix translation method, i.e., how well can we
differentiate between positive and negative words
of a language we have not seen before using a
model trained only on English words? Here we

Figure 3: F-measure of binary sentiment classi-
fier with varying amounts of training data for both
Spanish and Chinese.

used the binary sentiment word list described in
section 2.1.2 in order to assess whether or not the
translation matrix would preserve sentiment even
with poor translation accuracy scores. Given that
the classifier is trained only on the target language
vectors, we used the translation matrices produced
previously to translate a word from source to target
language embedding space.

As we can see in Table 3 and Figure 3, even
with low translation accuracy, such as the 1000-
word Chinese translation matrix, we are able to
achieve good binary sentiment classification. We
also noted that a significant drop in translation ac-
curacy results in only a relatively small drop in
sentiment classification performance.

4.3 Fine-Grained Sentiment Analysis

In the third experiment, we tested the accuracy
of cross-lingual regression when it comes to pre-
dicting a word’s value in any of the three ANEW
dimensions of valence, arousal, and dominance.
We attempted to predict the valence, arousal, and
dominance of words in source language, having
only trained on target language. As we saw in
the last experiment, massive drop-off in transla-
tion accuracy need not result in a massive drop-off
in sentiment analysis. As this is a regression prob-
lem, Table 4 presents both the r2 and the mean
squared error (MSE) as measurements of model
performance. The MSE per dimension is visual-

511



(a) Arousal (b) Dominance (c) Valence

Figure 4: Mean squared error for each regression problem with varying amounts of training data for all
three of the ANEW dimensions. (Lower is better).

Spanish Chinese
1000 Words
Precision 0.77 0.76
Recall 0.79 0.74
F-measure 0.78 0.75

4500 Words
Precision 0.82 0.79
Recall 0.82 0.77
F-measure 0.82 0.78

8500 Words
Precision 0.83 0.80
Recall 0.83 0.77
F-measure 0.83 0.78

Table 3: Results of the binary sentiment classifi-
cation task for each language with each translation
matrix.

ized in Figure 4. Given the data’s scale from 1 to
9 with an average standard deviation among par-
ticipants for each word of 2.02, an average mean
squared error of approximately 1 shows that our
model has high predictive power.

4.4 Sentiment Classification of Reviews

In the fourth experiment, we sought to show that
the regressor developed in section 3.3.2 could be
used as a feature extractor in performing other
tasks. To this end, we replicated the experiment
by Chen et al. (2016), who predicted hotel ratings
from Chinese reviews using a model trained only
on English restaurant reviews.

Chen et al. (2016) had two baseline models
which they beat with their new model: i) a lo-

Spanish Chinese
1000 Words
Arousal 0.24 (0.84) 0.24 (0.84)
Dominance 0.31 (0.73) 0.23 (0.83)
Valence 0.48 (2.08) 0.32 (2.78)

4500 Words
Arousal 0.33 (0.76) 0.28 (0.83)
Dominance 0.39 (0.63) 0.28 (0.74)
Valence 0.54 (1.81) 0.44 (2.27)

8500 Words
Arousal 0.33 (0.77) 0.29 (0.81)
Dominance 0.38 (0.65) 0.31 (0.69)
Valence 0.54 (1.78) 0.43 (2.26)

Table 4: Results of the fine-grained sentiment re-
gression task for each language with each transla-
tion matrix, in the form of r2(MSE).

gistic regression classifier (line 1 in Table 5), and
ii) a non-adversarial variation of adversarial deep
averaging network (ADAN) (line 2), referred to
as DAN (deep averaging network), which is one
of the state-of-the-art neural models for sentiment
classification. These were the only two models
which did not make use of either labeled Chi-
nese examples or an MT system, and therefore
were chosen to serve as a fair comparison to our
method. Both models use bilingual word em-
beddings as an input representation to map words
from both languages into the same vector space.
Our own model (line 3 in Table 5) uses logistic re-
gression on sentence arrays created by predicting
their ANEW values for each dimension to predict
review scores.

512



Approach Accuracy
Logistic regression (BWE) 30.58%
Deep averaging network (DAN) 29.11%
Logistic regression (ANEW) 28.05%

Table 5: Model performance on sentiment ex-
tracted vectors versus previous approaches. Lo-
gistic regression on predicted sentiment (ANEW)
values preformed similariy to both regression and
DAN on BWEs.

Table 5 shows that we were able to closely
match the accuracy of the baseline systems imple-
mented by Chen et al. (2016) for Chinese reviews.
These results demonstrate that our sentiment re-
gressors encoded enough information into the sen-
tence vectors to achieve similar results to the base-
line models which took bilingual word embedding
as input, and that the fine-grained sentiment model
can be used to extract sentiment-based features for
other tasks in languages where aligned data might
be expensive to obtain.

5 Discussion and Future Work

We have shown that the matrix translation method
can be used to infer and predict cross-lingual senti-
ment. More notably we observed that: i) sentiment
is preserved accurately even with sub-par trans-
lations, and ii) this low-cost approach also main-
tained fine-grained sentiment information between
languages.

We further cemented these observations through
a variety of experiments. The first experiment pre-
formed was testing the translation accuracy of the
method presented to verify validity of the algo-
rithm. The second experiment, was binary word
classification into either positive and negative for
words in Chinese or Spanish, given a model that
was trained only with English. With a translation
P@5 as low as 11%, our linear classifier was still
able to predict with 75% precision and recall the
polarity of a word’s sentiment. The third experi-
ment, in which we were able to predict a word’s
position on the three-dimensional ANEW 9-point
scale with an error margin of 1 (per dimension),
lent further credibility to the validity and data-
efficiency of our approach. Our fine-grained senti-
ment analysis with the ANEW scale is notable as it
demonstrates how the algorithm works at the word
level — useful in building sentiment lexicons in an
automated fashion at a very low cost and with lit-
tle manual effort. Our last experiment, Chinese

review classification, further highlighted the ro-
bustness of our model, by showing that vectors
created using the regressor encoded enough senti-
ment information to match the baseline methods of
passing in bilingual word embeddings to a trained
model. We showed how competitive the proposed
approach is when compared to much more expen-
sive methods and that it can directly be applied
to sentiment classification tasks for data-poor lan-
guages.

Throughout the experiments, we saw the gen-
eral trend of reduced error and increased accuracy
with more training data. However the increase
in accuracy starts to diminish with around 8500
words. The root of this leveling of accuracy could
be the inherent limitation of either the translation
technique used or the classification or regression
algorithms used or both.

A surprising finding was how well sentence vec-
tors composed of ANEW values for each word
performed when compared to the baselines of
Chen et al. (2016), Table 5. Achieving similar re-
sults to the previous baseline using the same clas-
sification algorithm (logistic regression) means
that the sentence vectors composed of ANEW val-
ues encode enough (and as much) information as
the BWE, or that the classification algorithm used
on the BWEs isn’t strong enough to extract more
meaningful relationships between the values. The
truth is probably a mixture of both reasons.

By choosing the number and type of languages
selected we have shown that this observation holds
across languages with different roots and different
grammar systems, which is further re-affirmed by
the fact that we train our models only on English
data, but test purely on Chinese and Spanish.

We explore the effect of poor translation on fine-
grained sentiment analysis by taking a look at a
few concrete examples of poor translations (Table
6). Table 6 presents four different Chinese words
with predicted valence of varying accuracy. We
can see that even with poor translation (i.e., the
closest five words are all completely unrelated, as
is the case with hungry), sentiment is still accu-
rately predicted. On the other hand we also show
that it is possible to have related words but have
poor prediction, as is the case with misery, because
the base sentiment predictor itself is not perfectly
accurate. This suggests that the sentiment vector
space, a hypothetical space produced by finding
the sentiment value of each point in the original

513



Chinese Word
(English)

Computed
English Translation

True Valence
(Predicted Valence)

(Hungry) Mugabe misrule 3.58 (2.85)
Boo hoo
Quagmire
Chikwanine
Sylvain Angerlotte

(Incentive) Circumstances dictate 7.00 (6.17)
Selfless sacrifices
Humilty
President Obama
EquityMarketReport

(Kindness) Really hateful lemmings 7.81 (6.84)
Loving
Dad
Flowering orchids
Dear robin

(Misery) Violence begets violence 1.93 (3.91)
Indignations
Sufferings
Sincerely
Unconfessed sin

Table 6: Four poorly translated Chinese words,
their accompanying true English translation, the
five nearest English words to the translated vector,
and the true (English) valence with the accompa-
nying predicted valence of the translated vector.

embedding space, which we term topological sen-
timent map, has two properties: i) it is maintained
through linear transformation, and ii) it is “flat”
enough that highly accurate mapping (read: trans-
lation) between languages is not required to ar-
rive at usable sentiment classification. This second
property allows for the sentiment analysis of lan-
guages where a large amount of labeled material
is not available at an extremely low cost, and can
also be used to aid in many cross-lingual sentiment
related tasks.

In the future we hope to extend both the analysis
and experiments discussed here to other languages
and applications. For example the cross-lingual
fine-grained sentiment analysis techniques could
possibly be used to study the change in sentiment
of words in a single language over time, leading
to new insights or re-affirming old ones. Future
analysis could compare different transformations
and their effect on sentiment analysis. The ability
to produce a “stable” topographic sentiment map
could also be used to evaluate algorithms which
create the vector spaces as well.

Lastly, future work will focus on developing
other low-cost approaches, possibly by imple-
menting Duong et al. (2016)’s technique to im-
prove the accuracy and precision of sentiment re-
gression. This would serve to demonstrate the (ex-
pected) limits of the linear transformation model’s

in handling polysemy and subsequently its impact
on the topological mapping of sentiment in vector
space.

Acknowledgements

We would like to thank Moustafa Abdalla for his
help and discussions. The work was financially
supported by the Natural Sciences and Engineer-
ing Research Council of Canada.

References
AR Balamurali, Aditya Joshi, and Pushpak Bhat-

tacharyya. 2012. Cross-lingual sentiment analysis
for Indian languages using linked WordNets. In Pro-
ceedings of the International Conference on Compu-
tational Linguistics (COLING), pages 73–82. Asso-
ciation for Computational Linguistics.

Yoshua Bengio and Greg Corrado. 2015. BilBOWA:
Fast bilingual distributed representations without
word alignments. arXiv:1410.2455.

Margaret M Bradley and Peter J Lang. 1994. Measur-
ing emotion: The self-assessment manikin and the
semantic differential. Journal of Behavior Therapy
and Experimental Psychiatry, 25(1):49–59.

Margaret M Bradley and Peter J Lang. 1999. Affec-
tive norms for English words (ANEW): Instruction
manual and affective ratings. Technical report, Cen-
ter for Research in Psychophysiology, University of
Florida.

Cristian Cardellino. 2016. Spanish Billion Words Cor-
pus and Embeddings.

Xilun Chen, Ben Athiwaratkun, Yu Sun, Kilian Q
Weinberger, and Claire Cardie. 2016. Adversarial
deep averaging networks for cross-lingual sentiment
classification. arXiv:1606.01614.

Long Duong, Hiroshi Kanayama, Tengfei Ma, Steven
Bird, and Trevor Cohn. 2016. Learning crosslin-
gual word embeddings without bilingual corpora.
In Proceedings of the 2016 Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 1285–1295.

Long Duong, Hiroshi Kanayama, Tengfei Ma, Steven
Bird, and Trevor Cohn. 2017. Multilingual training
of crosslingual word embeddings. In Proceedings of
the 15th Conference of the European Chapter of the
Association for Computational Linguistics (EACL),
pages 893–903.

Manaal Faruqui and Chris Dyer. 2014. Improving vec-
tor space word representations using multilingual
correlation. In Proceedings of the 14th Conference
of the European Chapter of the Association for Com-
putational Linguistics (EACL), pages 462–471.

514



Karl Moritz Hermann and Phil Blunsom. 2013. Mul-
tilingual distributed representations without word
alignment. arXiv:1312.6173.

Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the ACM
SIGKDD International Conference on Knowledge
Discovery and Data Mining (KDD-2004), pages
168–177. ACM.

Sarthak Jain and Shashank Batra. 2015. Cross-lingual
sentiment analysis using modified BRAE. In Pro-
ceedings of the 2015 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP),
pages 159–168.

Alexandre Klementiev, Ivan Titov, and Binod Bhat-
tarai. 2012. Inducing crosslingual distributed repre-
sentations of words. In Proceedings of the 24th In-
ternational Conference on Computational Linguis-
tics (COLING), pages 1459–1474.

Huey Yee Lee and Hemnaath Renganathan. 2011. Chi-
nese sentiment analysis using maximum entropy. In
Proceedings of the Workshop on Sentiment Analysis
where AI meets Psychology (SAAIP), page 89.

Yiou Lin, Hang Lei, Jia Wu, and Xiaoyu Li.
2015. An empirical study on sentiment classifi-
cation of Chinese review using word embedding.
arXiv:1511.01665.

Andrew L Maas, Raymond E Daly, Peter T Pham, Dan
Huang, Andrew Y Ng, and Christopher Potts. 2011.
Learning word vectors for sentiment analysis. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies (ACL-HLT), pages 142–150.

Tomas Mikolov, Quoc V Le, and Ilya Stuskever. 2013a.
Exploiting similarities among languages for ma-
chine translation. arXiv:1309.4168.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems (NIPS), pages 3111–3119.

Saif M Mohammad, Mohammad Salameh, and Svet-
lana Kiritchenko. 2016. Sentiment lexicons for Ara-
bic social media. In Proceedings of the Interna-
tional Conference on Language Resources and Eval-
uation (LREC), pages 33–37.

F Pedregosa, G Varoquaux, A Gramfort, V Michel,
B Thirion, O Grisel, M Blondel, P Prettenhofer,
R Weiss, V Dubourg, J Vanderplas, A Passos,
D Cournapeau, M Brucher, M Perrot, and E Duch-
esnay. 2011. Scikit-learn: Machine learning in
Python. Journal of Machine Learning Research,
12:2825–2830.

Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. GloVe: Global vectors for word

representation. In Proceedings of the 2014 Confer-
ence on Empirical Methods on Natural Language
Processing (EMNLP), volume 14, pages 1532–
1543.

R Řehůřek and P Sojka. 2011. Gensim - Python frame-
work for vector space modelling. Technical report,
NLP Centre, Faculty of Informatics, Masaryk Uni-
versity, Brno, Czech Republic.

Mohammad Salameh, Saif M Mohammad, and Svet-
lana Kiritchenko. 2015. Sentiment after transla-
tion: A case-study on Arabic social media posts.
In Proceedings of the 2015 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies
(NAACL—HLT).

AP Sarath Chandar, Stanislas Lauly, Hugo Larochelle,
Mitesh Khapra, Balaraman Ravindran, Vikas C
Raykar, and Amrita Saha. 2014. An autoencoder
approach to learning bilingual word representations.
In Advances in Neural Information Processing Sys-
tems (NIPS).

Songbo Tan and Jin Zhang. 2008. An empirical study
of sentiment analysis for Chinese documents. Ex-
pert Systems with Applications, 34(4):2622–2629.

Xuewei Tang and Xiaojun Wan. 2014. Learning bilin-
gual embedding model for cross-language senti-
ment classification. In Proceedings of the 2014
IEEE/WIC/ACM International Joint Conferences on
Web Intelligence (WI) and Intelligent Agent Tech-
nologies (IAT)-Volume 02, pages 134–141. IEEE
Computer Society.

Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015.
Character-level convolutional networks for text clas-
sification. In Advances in Neural Information Pro-
cessing Systems (NIPS), pages 649–657.

Huiwei Zhou, Long Chen, Fulin Shi, and Degen
Huang. 2015. Learning bilingual sentiment word
embeddings for cross-language sentiment classifi-
cation. In Proceedings of the 53rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 430–440.

Xinjie Zhou, Xianjun Wan, and Jianguo Xiao. 2016.
Cross-lingual sentiment classification with bilingual
document representation learning. In Proceed-
ings of the 54th Annual Meeting of the Association
for Computational Linguistics (ACL), pages 1403–
1412.

Will Y. Zou, Richard Socher, Daniel Cer, and Christo-
pher D. Manning. 2013. Bilingual word embeddings
for phrase-based machine translation. In Proceed-
ings of the 2013 Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages
1393–1398.

515


