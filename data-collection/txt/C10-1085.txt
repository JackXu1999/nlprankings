Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 752–760,

Beijing, August 2010

752

Automatic analysis of semantic similarity in comparable text

through syntactic tree matching

Erwin Marsi

TiCC, Tilburg University
e.c.marsi@uvt.nl

Emiel Krahmer

TiCC, Tilburg University

e.j .krahmer@uvt.nl

Abstract

We propose to analyse semantic similar-
ity in comparable text by matching syn-
tactic trees and labeling the alignments
according to one of ﬁve semantic simi-
larity relations. We present a Memory-
based Graph Matcher (MBGM) that per-
forms both tasks simultaneously as a com-
bination of exhaustive pairwise classiﬁca-
tion using a memory-based learner, fol-
lowed by global optimization of the align-
ments using a combinatorial optimization
algorithm. The method is evaluated on a
monolingual treebank consisting of com-
parable Dutch news texts. Results show
that it performs substantially above the
baseline and close to the human reference.

Introduction

1
Natural languages allow us to express essentially
the same underlying meaning as many alterna-
tive surface forms. In other words, there are of-
ten many similar ways to say the same thing.
This characteristic poses a problem for many nat-
ural language processing applications. Automatic
summarizers, for example,
typically rank sen-
tences according to their informativity and then
extract the top n sentences, depending on the re-
quired compression rate. Although the sentences
are essentially treated as independent of each
other, they typically are not. Extracted sentences
may have substantial semantic overlap, result-
ing in unintended redundancy in the summaries.
This is particularly problematic in the case of
multi-document summarization, where sentences
extracted from related documents are very likely

to express similar information in different ways
(Radev and McKeown, 1998). Therefore, if se-
mantic similarity between sentences could be de-
tected automatically, this would certainly help to
avoid redundancy in summaries.

Similar arguments can be made for many other
NLP applications. Automatic duplicate and pla-
giarism detection beyond obvious string overlap
requires recognition of semantic similarity. Au-
tomatic question-answering systems may beneﬁt
from clustering semantically similar candidate an-
swers.
Intelligent document merging software,
which supports a minimal but lossless merge of
several revisions of the same text, must handle
cases of paraphrasing, restructuring, compression,
etc. Recognizing textual entailments (Dagan et
al., 2005) could arguably be seen as a speciﬁc in-
stance of detecting semantic similarity.

In addition to merely detecting semantic simi-
larity, we can ask to what extent two expressions
share meaning. For instance, the meaning of one
sentence can be fully contained in that of another,
the meaning of one sentence can overlap only
partly with that of another, etc. This requires an
analysis of the semantic similarity between a pair
of expressions. Like detection, automatic analy-
sis of semantic similarity can play an important
role in NLP applications. To return to the case
of multi-document summarization, analysing the
semantic similarity between sentences extracted
from different documents provides the basis for
sentence fusion, a process where a new sentence
is generated that conveys all common information
from both sentences without introducing redun-
dancy (Barzilay and McKeown, 2005; Marsi and
Krahmer, 2005b).

753

Analysis of semantic similarity can be ap-
proached from different angles. A basic approach
is to use string similarity measures such as the
Levenshtein distance or the Jaccard similarity co-
efﬁcient. Although cheap and fast, this fails to
account for less obvious cases such as synonyms
or syntactic paraphrasing. At the other extreme,
we can perform a deep semantic analysis of two
expressions and rely on formal reasoning to de-
rive a logical relation between them. This ap-
proach suffers from issues with coverage and ro-
bustness commonly associated with deep linguis-
tic processing. We therefore think that the middle
ground between these two extremes offers the best
option.
In this paper we present a new method
for analysing semantic similarity in comparable
text.
It relies on a combination of morphologi-
cal and syntactic analysis, lexical resources such
as word nets, and machine learning from exam-
ples. We propose to analyse semantic similarity
between sentences by aligning their syntax trees,
where each node is matched to the most similar
node in the other tree (if any).
In addition, we
label these alignments according to the type of
similarity relation that holds between the aligned
phrases. The labeling supports further processing.
For instance, Marsi & Krahmer (2005b; 2008) de-
scribe how to generate different types of sentence
fusions on the basis of this relation labeling.

In the next Section we provide a more formal
deﬁnition of the task of matching syntactic trees
and labeling alignments, followed by a discusion
of related work in Section 3. Section 4 describes a
parallel, monolingual treebank used for develop-
ing and testing our approach. In Section 5 we pro-
pose a new algorithm for simultaneous node align-
ment and relation labeling. The results of several
evaluation experiments are presented in Section 6.
We ﬁnish with a conclusion.

2 Problem statement
Aligning a pair of similar syntactic trees is the pro-
cess of pairing those nodes that are most similar.
More formally:
let v be a node in the syntactic
tree T of sentence S and v0 a node in the syntactic
tree T 0 of sentence S0. A labeled node alignment
is a tuple < v, v0, r > where r is a label from a set
of relations. A labeled tree alignment is a set of

labeled node alignments. A labeled tree matching
is a tree alignment in which each node is aligned
to at most one other node.

For each node v, its terminal yield STR(v) is de-
ﬁned as the sequence of all terminal nodes reach-
able from v (i.e., a substring of sentence S).
Aligning node v to v0 with label r indicates that
relation r holds between their yields STR(v) and
STR(v0). We label alignments according to a small
set of semantic similarity relations. As an exam-
ple, consider the following Dutch sentences:

risico
risk

op
on

(1) a. Dagelijks

Daily
Alzheimer
Alzheimer

kofﬁe
coffee
en
and
koppen
cups
op
on

vermindert
diminishes
Dementie.
Dementia.
kofﬁe
coffee
Parkinson
Parkinson

per
a
en
and

b. Drie
Three
kans
chance

reduceert
dag
day
reduces
Dementie.
Dementia.

The corresponding syntax trees and their (partial)
alignment is shown in Figure 1. We distinguish
the following ﬁve mutually exclusive similarity
relations:

1. v equals v0

iff lower-cased STR(v) and
lower-cased STR(v0) are identical – example:
Dementia equals Dementia;

2. v restates v0 iff STR(v) is a proper para-
phrase of STR(v0) – example: diminishes re-
states reduces;

3. v generalizes v0 iff STR(v) is more general
than STR(v0) – example: daily coffee gener-
alizes three cups of coffee a day;

4. v speciﬁes v0 iff STR(v) is more speciﬁc than
STR(v0) – example: three cups of coffee a day
speciﬁes dailly coffee;

5. v intersects v0 iff STR(v) and STR(v0) share
meaning, but each also contains unique infor-
mation not expressed in the other – example:
Alzheimer and Dementia intersects Parkin-
son and Dementia.

Our interpretation of these relations is one of
common sense rather than strict logic, akin to
the deﬁnition of entailment employed in the RTE
challenge (Dagan et al., 2005). Note also that re-
lations are prioritized: equals takes precedence

754

Figure 1: Example of two aligned and labeled syntactic trees. For expository reasons the alignment is
not exhaustive.

over restates, etc. Furthermore, equals, restates
and intersects are symmetrical, whereas general-
izes is the inverse of speciﬁes. Finally, nodes con-
taining unique information, such as Alzheimer and
Parkinson, remain unaligned.

3 Related work
Many syntax-based approaches to machine trans-
lation rely on bilingual treebanks to extract trans-
fer rules or train statistical translation models. In
order to build bilingual treebanks a number of
methods for automatic tree alignment have been
developed, e.g., (Gildea, 2003; Groves et al.,
2004; Tinsley et al., 2007; Lavie et al., 2008).
Most related to our approach is the work on dis-
criminative tree alignment by Tiedemann & Kotz´e
(2009). However, these algorithms assume that
source and target sentences express the same in-
formation (i.e.
parallel text) and cannot cope
with comparable text where parts may remain un-
aligned. See (MacCartney et al., 2008) for further
arguments and empirical evidence that MT align-
ment algorithms are not suitable for aligning par-
allel monolingual text.

MacCartney, Galley, and Manning (2008) de-
scribe a system for monolingual phrase alignment
based on supervised learning which also exploits
external resources for knowledge of semantic re-
latedness.
In contrast to our work, they do not
use syntactic trees or similarity relation labels.
Partly similar semantic relations are used in (Mac-
Cartney and Manning, 2008) for modeling seman-
tic containment and exclusion in natural language
inference. Marsi & Krahmer (2005a) is closely

related to our work, but follows a more com-
plicated method: ﬁrst a dynamic programming-
based tree alignment algorithm is applied, fol-
lowed by a classiﬁcation of similarity relations us-
ing a supervised-classiﬁer. Other differences are
that their data set is much smaller and consists
of parallel rather than comparable text. A major
drawback of this algorithmic approach it that it
cannot cope with crossing alignments. We are not
aware of other work that combines alignment with
semantic relation labeling, or algorithms which
perform both tasks simultaneously.

4 Data collection

For developing our alignment algorithm we use
the DAESO corpus1. This is a Dutch parallel
monolingual treebank of 1 million words, half
of which were manually annotated. The corpus
consists of pairs of sentences with different lev-
els of semantic overlap, ranging from high (dif-
ferent Dutch translations of books from Darwin,
Montaigne and Saint-Exup´ery) to low (different
press releases from the two main news agencies
in The Netherlands, ANP and NOVUM). For this
paper, we concentrate on the latter part of the
DAESO corpus, where the proportion of Equals
and Restates is relatively low. This corpus seg-
ment consists of 8,248 pairs of sentences, contain-
ing 162,361 tokens (ignoring punctuation). All
sentences were tokenized and tagged, and subse-
quently parsed by the Alpino dependency parser
for Dutch (Bouma et al., 2001). Two annota-

1http://daeso.uvt.nl

smain

Generalizes
vermindert

np

np

Restates

smain

np

reduceert

np

Dagelijks

koffie

risico

pp

Drie

koppen

op

en

conj

Dementie

Altzheimer

pp

koffie
Intersects

per

dag

Parkinson

kans

pp

op
Equals
en

conj

Dementie

755

Alignment:

Eq:
95.48
2.69
95.83
2.27

Re:
58.50
7.63
71.38
3.77

Spec:
65.81
13.05
60.21
7.63

95.38
2.16
88.31
1.15

Words:

Full trees:

F:
SD:
F:
SD:

Labeling:
Gen:
65.00
11.25
66.71
8.17

25.85
18.74
62.67
6.14

Int: Macro: Micro:
88.72

62.11

71.36

81.92

Table 1: Average F-scores (in percentages, with Standard Deviations) for the six human annotators on
alignment and semantic relation labeling, for words and for full syntactic trees.

tors determined which sentences in the compa-
rable news reports contained semantic overlap.
Six other annotators produced manual alignments
of words and phrases in matched sentence pairs,
which resulted in 86,227 aligned pairs of nodes.

A small sample of 10 similar press releases
comprising a total of 48 sentence pairs was inde-
pendently annotated by all six annotators to deter-
mine inter-annotator agreement. We used preci-
sion, recall and F-score on alignment. To calcu-
late these scores for relation labeling, we simply
restrict the set of alignments to those labeled with
a particular relation, ignoring all others. Likewise,
we restrict these sets to terminal node alignments
in order to get scores on word alignment.

Given the six annotations A1, . . . , A6, we re-
peatedly took one as the T rue annotation against
which the ﬁve other annotations were evaluated.
We then computed the average scores over these
6 ∗ 5 = 30 scores (note that with this proce-
dure, precision, recall and F score end up being
equal). Table 1 summarizes the results, both for
word alignments and for full syntactic tree align-
ment. It can be seen that for alignment of words an
average F-score of over 95 % was obtained, while
alignment for full syntactic trees results in an F-
score of 88%. For relation labeling, the scores dif-
fered per relation, as is to be expected: the average
F-score for Equals was over 95% for both word
and full tree alignment2, and for the other rela-
tions average F-scores between 0.6 and 0.7 were

2At ﬁrst sight, it may seem that labeling Equals is a trivial
and deterministic task, for which the F-score should always
be close to 100%. However, the same word may occur multi-
ple times in the source or target sentences, which introduces
ambiguity. This frequently occurs with function words such
as determiners and prepositions. Moreover, choosing among
several equivalent Equals alignments may sometimes involve
a somewhat arbitrary decision. This situation arises, for in-
stance, when a proper noun is mentioned just once in the
source sentence but twice in the target sentence.

obtained. The exception to note is Intersects on
word level, which only occurred a few times ac-
cording to a few of the annotators. The macro
and micro (weighted) F-score averages on labeled
alignment are 62.11% and 88.72% for words, and
71.36% and 81.92% for full syntactic trees.

5 Memory-based Graph Matcher

In order to automatically perform the alignment
and labeling tasks described in Section 2, we cast
these tasks simultaneously as a combination of ex-
haustive pairwise classiﬁcation using a supervised
machine learning algorithm, followed by global
optimization of the alignments using a combina-
torial optimization algorithm.
Input to the tree
matching algorithm is a pair of syntactic trees con-
sisting of a source tree Ts and a target tree Tt.

Step 1: Feature extraction For each possible
pairing of a source node ns in tree Ts and a target
node nt in tree Tt, create an instance consisting of
feature values extracted from the input trees. Fea-
tures can represent properties of individual nodes,
e.g. the category of the source node is NP, or rela-
tions between nodes, e.g. source and target node
share the same part-of-speech.

Step 2: Classiﬁcation A generic supervised
classiﬁer is used to predict a class label for each
instance. The class is either one of the seman-
tic similarity relations or the special class none,
which is interpreted as no alignment. Our im-
plementation employs the memory-based learner
TiMBL (Daelemans et al., 2009), a freely avail-
able, efﬁcient and enhanced implementation of k-
nearest neighbour classiﬁcation. The classiﬁer is
trained on instances derived according to Step 1
from a parallel treebank of aligned and labeled
syntactic trees.

756

Step 3: Weighting Associate a cost with each
prediction so that high costs indicate low conﬁ-
dence in the predicted class and vice versa. We
use the normalized entropy of the class labels in
the set of nearest neighbours (H) deﬁned as

Data

word develop
word test
tree develop
tree test

Graph
pairs
2 664
547
2 664
547

Node Tokens
pairs
13 027
2 858
22 741
4 894

45 149
10 005
45 149
10 005

Aligned
nodes (%)
15.71
14.96
47.20
47.05

Table 2: Properties of develop and test data sets

(1)

H = −Pc∈C p(c) log2 p(c)

log2|C|

where C is the set of class labels encountered in
the set of nearest neighbours (i.e., a subset of the
ﬁve relations plus none), and p(c) is the probabil-
ity of class c, which is simply the proportion of
instances with class label c in the set of nearest
neighbours.
Intuitively this means that the cost
is zero if all nearest neighbours are of the same
class, whereas the cost goes to 1 if the nearest
neighbours are equally distributed over all possi-
ble classes.

Step 4: Matching The classiﬁcation step will
usually give rise to one-to-many alignment of
nodes. In order to reduce this to just one-to-one
alignments, we search for a node matching which
minimizes the sum of costs over all alignments.
This is a well-known problem in combinato-
rial optimization known as the Assignment Prob-
lem. The equivalent in graph-theoretical terms
is a minimum weighted bipartite graph match-
ing. This problem can be solved in polynomial
time (O(n3)) using e.g., the Hungarian algorithm
(Kuhn, 1955). The output of the algorithm is the
labeled tree matching obtained by removing all
node alignments labeled with the special none re-
lation.

6 Experiments
6.1 Experimental setup
Word alignment and full tree alignments are con-
ceptually different tasks, which require partly dif-
ferent features and may have different practical
applications. These are therefore addressed in
separate experiments.

Table 2 summarizes the respective sizes of de-
velopment and the held-out test set in terms of
number of aligned graph pairs, number of aligned
node pairs and number of tokens. The percentage
of aligned nodes over all graphs is calculated rela-
tive to the number of nodes over all graphs. Since

Data
word develop
word test
tree develop
tree test

Eq
84.92
85.62
56.61
58.40

Re
6.15
6.09
6.57
7.11

Spec Gen
1.77
2.10
1.99
2.17
6.38
7.52
7.40
6.38

Int
5.07
4.13
22.91
20.72

Table 3: Distribution of semantic similarity rela-
tions for word alignment and for full tree align-
ments in both develop and test data sets

alignments involving non-terminal nodes are ig-
nored in the task of word alignment, the number of
aligned node pairs and the percentage of aligned
nodes is lower in the word develop and word test
sets. Table 3 gives the distribution of semantic re-
lations in the development and test set, for word
and tree alignment.
It can be observed that the
distribution if fairly skewed with Equals being the
majority class, even more so for word alignments.
Another thing to notice is that Intersects are much
more frequent at the level of non-terminal align-
ments.

Development was carried out using 10-fold
cross validation on the development data and con-
sequently reported scores on the development data
are averages over 10 folds. Only two parameters
were coarsely optimized on the development set.
First, the amount of downsampling of the none
class varied between 0.1 or 0.5. Second, the pa-
rameter k of the memory-based classiﬁer – the
number of nearest neighbours taken into account
during classiﬁcation – ranged from 1 to 15. Opti-
mal settings were ﬁnally applied when testing on
the held-out data.

A simple greedy alignment procedure served as
baseline. For word alignment, identical words are
aligned as Equals and identical roots as Restates.
For full tree alignment, this is extended to the level
of phrases so that phrases with identical words are
aligned as Equals and phrases with identical roots
as Restates. The baseline does not predict Spec-

757

iﬁes, Generalizes or Intersects relations, as that
would require a more involved, knowledge-based
approach.

All features used are described in Table 4.
The word-based features rely on pure string pro-
cessing and require no linguistic preprocessing.
The morphology-based features exploit the lim-
ited amount of morphological analysis provided
by the Alpino parser (Bouma et al., 2001). For
instance, it provides word roots and decomposes
compound words. Likewise the part-of-speech-
based features use the coarse-grained part-of-
speech tags assigned by the Alpino parser. The
lexical-semantic features rely on the Cornetto
database (Vossen et al., 2008), a recent exten-
sion to the Dutch WordNet, to look-up synonym
and hypernym relations among source and tar-
get lemmas. Unfortunately there is no word
sense disambiguation module to identify the cor-
rect senses.
In addition, a background corpus
of over 500M words of (mainly) news text pro-
vides the word counts required to calculate the
Lin similarity measure (Lin, 1998). The syntax-
based features use the syntactic structure, which
is a mix of phrase-based and dependency-based
analysis. The phrasal features express similar-
ity between the terminal yields of source and tar-
get nodes. With the exception of same-parent-lc-
phrase, these features are only used for full tree
alignment, not for word alignment.

6.2 Results on word alignment
We evaluate our alignment model in two steps:
ﬁrst focussing on word alignment and then on full
tree alignment. Table 5 summarizes the results for
MBGM on word alignment (50% downsampling
and k = 3), which we compare statistically to the
baseline performance, and informally with the hu-
man scores reported in Table 1 in Section 4 (note
that the human scores are only for a subset of the
data used for automatic evaluation).

The ﬁrst thing to observe is that the MBGM
scores on the development and tests sets are
very similar throughout.
For predicting word
alignments, the MBGM system performs signif-
icantly better than the baseline system (t(18) =
17.72, p < .0001). On the test set, MBGM ob-
tains an F-score of nearly 89%, which is almost

exactly halfway between the scores of the base-
line system and the human scores.
In a similar
vein, the performance of the MBGM system on
relation labeling is considerably better than that
of the baseline system. For all semantic rela-
tions, MBGM performs signiﬁcantly better than
the baseline (t(18) > 9.4138, p < .0001 for each
relation, trivially so for the Speciﬁes, Generalizes
and Intersects relations, which the baseline system
never predicts).

The macro scores are plain averages over the 5
scores on each relation, whereas the micro scores
are weighted averages. As the Equals is the major-
ity class and at the same time easiest to predict, the
micro scores are higher. The macro scores, how-
ever, better reﬂect performance on the real chal-
lenge, that is, correctly predicting the relations
other than Equals. The MBGM macro average
is 27.37% higher than the baseline (but still some
10% below the human top line), while the micro
average is 5.83% higher and only 0.75% below
the human top line. Macro scores on the test set
are overall lower than those on the develop set,
presumably because of tuning on the development
data.

6.3 Results on tree alignment
Table 6 contains the results of full tree alignment
(50% downsampling and k = 5); here both termi-
nal and non-terminal nodes are aligned and clas-
siﬁed in one pass. Again scores on the develop-
ment and test set are very similar, the latter being
slightly better. For full tree alignment, MBGM
once again performs signiﬁcantly better than the
baseline, t(18) = 25.68, p < .0001. With an F-
score on the test set of 86.65, MBGM scores al-
most 20 percent higher than the baseline system.
This F-score is less than 2% lower than the aver-
age F-score obtained by our human annotators on
full tree alignment, albeit not on exactly the same
sample. The picture that emerges for semantic re-
lation labeling is closely related to the one we saw
for word alignments. MBGM signiﬁcantly out-
performs the baseline, for each semantic relation
(t(18) > 12.6636, p < .0001). MBGM scores a
macro average F-score of 52.24% (an increase of
30.05% over the baseline) and a micro average of
80.03% (12.68% above the base score). It is inter-

758

Feature

Type

Description

Word

word-subsumption

string

shared-pre-/in-/sufﬁx-len
source/target-stop-word
source/target-word-len
word-len-diff
source/target-word-uniq
same-words-lhs/rhs

int
bool
int
int
bool
int

indicate if source word equals, has as preﬁx, is a preﬁx of, has a sufﬁx, is a
sufﬁx of, has as inﬁx or is an inﬁx of target word
length of shared preﬁx/inﬁx/sufﬁx in characters
test if source/target word is in a stop word list of frequent function words
length of source/target word in characters
word length difference in characters
test if source/target word is unique in source/target sentence
no. of identical preceding/following words in source and target word contexts

root-subsumption

string

roots-share-pre-/in-/sufﬁx

bool

indicate if source root equals, has as preﬁx, is a preﬁx of, has a sufﬁx, is a sufﬁx
of, has as inﬁx or is an inﬁx of target root
source and target root share a preﬁx/inﬁx/sufﬁx

Morphology

source/target-pos
same-pos
source/target-content-word
both-content-word

string
bool
bool
bool

Part-of-speech
source/target part-of-speech
test if source and target have same part-of-speech
test if source/target word is a content word
test if both source and target word are content words

Lexical-semantic using Cornetto

cornet-restates

cornet-speciﬁes
cornet-generalizes
cornet-intersects

source/target-cat
same-cat
source/target-parent-cat
source/target-deprel
same-deprel
same-dephead-root

word-prec/rec
same-lc-phrase
same-parent-lc-phrase
source/target-phrase-len
phrase-len-diff

ﬂoat

ﬂoat
ﬂoat
ﬂoat

string
bool
string
string
bool
bool

ﬂoat
bool
bool
int
int

1.0 if source and target words are synonyms and 0.5 if they are near-synonyms,
zero otherwise
Lin similarity score if source word is a hyponym of target word, zero otherwise
Lin similarity score if source word is a hypernym of target word, zero otherwise
Lin similarity score if source word share a common hypernym, zero otherwise

Syntax

source/target syntactic category
test if source and target have same syntactic category
source/target syntactic category of parent node
source/target dependency relation
test if source and target have same dependency relation
test if the dependency heads of the source and target have same root

Phrasal

precision/recall on the yields of source and target nodes
test if lower-cased yields of source and target nodes are identical
test if lower-cased yields of parents of source and target nodes are identical
length of source/target phrase in words
phrase length difference in words

Table 4: Features (where slashes indicate multiple versions of the same feature, e.g. source/target-pos
represents the two features source-pos and target-pos)

esting to observe that MBGM obtains higher F-
scores on Equals and on Intersects (the two most
frequent relations) than the human annotators ob-
tained. As a result of this, the micro F-score of
the automatic full tree alignment is less than 2%
lower than the human reference score.

Tree alignment can also be implemented as a
two-step procedure, where in the ﬁrst step align-
ments and semantic relation classiﬁcations at the
word level are produced, while in the second step
these are used to predict alignments and seman-
tic relations for non-terminals. We experimented

with such a two-step procedure as well, in one ver-
sion using the actual word alignments and in the
other the predicted word alignments. The scores
of the two-step prediction are only marginally dif-
ferent from those of one step prediction, both for
alignment and for relation classiﬁcation, giving
improvements in the order of about 1% for both
subtasks. As is to be expected, the scores with
true word alignments are much better than those
with predicted word alignments. They are inter-
esting though, because they suggest that a fairly
good full tree alignment can be automatically ob-

759

Prec:
Develop baseline: Rec:

Develop MBGM:

Test baseline:

Test MBGM:

Alignment:

Eq:
81.84
93.10
87.11
94.54
95.91
95.02
83.83
93.87
88.57
94.20
95.41
94.80

Re:
46.26
34.71
39.66
61.26
46.19
52.67
43.12
27.01
33.22
53.33
40.21
45.85

Spec:
0.00
0.00
0.00
74.60
40.87
52.81
0.00
0.00
0.00
59.87
32.75
42.34

80.59
81.58
81.08
91.72
87.82
89.73
82.45
82.19
82.32
90.92
87.09
88.96

Labeling:
Gen:
0.00
0.00
0.00
67.82
43.22
52.80
0.00
0.00
0.00
54.21
43.28
48.17

Int: Macro: Micro:
80.22
0.00
82.20
0.00
0.00
80.70
90.82
45.80
86.96
27.27
88.85
34.19
0.00
82.17
82.02
0.00
82.14
0.00
89.90
42.47
86.11
20.31
27.48
87.97

25.61
25.56
25.35
68.80
50.61
57.50
25.39
24.18
24.36
60.84
46.39
51.73

Table 5: Scores (in percentages) on word alignment and semantic relation labeling

Prec:
Develop baseline: Rec:

Develop MBGM:

Test baseline:

Test MBGM:

Alignment:

Eq:
83.76
93.66
88.43
96.15
94.03
95.08
85.68
94.44
89.85
96.67
94.54
95.60

Re:
46.72
20.01
28.02
55.90
26.64
36.08
42.24
14.08
21.12
60.25
27.87
38.11

Spec:
0.00
0.00
0.00
54.40
21.71
31.03
0.00
0.00
0.00
46.92
19.55
27.60

82.50
54.54
65.67
92.23
81.04
86.27
84.23
56.21
67.43
92.27
81.67
86.65

Labeling:
Gen:
0.00
0.00
0.00
56.15
29.34
38.54
0.00
0.00
0.00
56.85
30.94
40.07

Int: Macro: Micro:
82.18
0.00
0.00
54.34
65.42
0.00
84.99
70.33
74.68
70.27
70.30
79.50
84.14
0.00
56.15
0.00
67.35
0.00
85.23
68.64
71.01
75.44
80.03
69.80

26.10
22.74
23.29
66.59
48.40
54.21
25.58
21.70
22.19
65.87
48.87
54.24

F:
Prec:
Rec:
F:
Prec:
Rec:
F:
Prec:
Rec:
F:

F:
Prec:
Rec:
F:
Prec:
Rec:
F:
Prec:
Rec:
F:

Table 6: Scores (in percentages) on full tree alignment and semantic relation labeling

tained given a manually checked word alignment.

7 Conclusions

We have proposed to analyse semantic similarity
between comparable sentences by aligning their
syntax trees, matching each node to the most sim-
ilar node in the other tree (if any).
In addi-
tion, alignments are labeled with a semantic sim-
ilarity relation. We have presented a Memory-
based Graph Matcher (MBGM) that performs
both tasks simultaneously as a combination of ex-
haustive pairwise classiﬁcation using a memory-
based learning algorithm, and global optimization
of alignments using a combinatorial optimization
algorithm. It relies on a combination of morpho-
logical/syntactic analysis, lexical resources such
as word nets, and machine learning using a par-

allel monolingual treebank. Results on aligning
comparable news texts from a monolingual paral-
lel treebank for Dutch show that MBGM consis-
tently and signiﬁcantly outperforms the baseline,
both for alignment and labeling. This holds both
for word alignment and tree alignment.

In future research we will test MBGM on other
data, as the DAESO corpus contains sub-corpora
with various degrees of semantic overlap. In addi-
tion, we intend to explore alternative features from
word space models. Finally, we plan to evaluate
MBGM in the context of NLP applications such
as multi-document summarization. This includes
work on how to deﬁne similarity at the sentence
level in terms of the proportion of aligned con-
stituents. Both MBGM and the annotated data set
will be publicly released.2

760

MacCartney, B. and C.D. Manning. 2008. Modeling seman-
tic containment and exclusion in natural language infer-
ence. In Proceedings of the 22nd International Confer-
ence on Computational Linguistics-Volume 1, pages 521–
528.

MacCartney, Bill, Michel Galley, and Christopher D. Man-
ning. 2008. A phrase-based alignment model for natural
language inference. In Proceedings of the 2008 Confer-
ence on Empirical Methods in Natural Language Process-
ing, pages 802–811, Honolulu, Hawaii, October.

Marsi, Erwin and Emiel Krahmer. 2005a. Classiﬁcation of
semantic relations by humans and machines. In Proceed-
ings of the ACL 2005 workshop on Empirical Modeling
of Semantic Equivalence and Entailment, pages 1–6, Ann
Arbor, Michigan.

Marsi, Erwin and Emiel Krahmer. 2005b. Explorations in
sentence fusion.
In Proceedings of the 10th European
Workshop on Natural Language Generation, Aberdeen,
GB.

Radev, D.R. and K.R. McKeown. 1998. Generating natural
language summaries from multiple on-line sources. Com-
putational Linguistics, 24(3):469–500.

Tiedemann, J. and G. Kotz´e.

2009. Building a Large
Machine-Aligned Parallel Treebank.
In Eighth Interna-
tional Workshop on Treebanks and Linguistic Theories,
page 197.

Tinsley, J., V. Zhechev, M. Hearne, and A. Way. 2007. Ro-
bust language-pair independent sub-tree alignment. Ma-
chine Translation Summit XI, pages 467–474.

Vossen, P., I. Maks, R. Segers, and H. van der Vliet. 2008.
Integrating lexical units, synsets and ontology in the Cor-
netto Database.
In Proceedings of LREC 2008, Mar-
rakech, Morocco.

Acknowledgments

This work was conducted within the DAESO
project funded by the Stevin program (De Ned-
erlandse Taalunie).

References
Barzilay, Regina and Kathleen R. McKeown. 2005. Sen-
tence fusion for multidocument news summarization.
Computational Linguistics, 31(3):297–328.

Bouma, Gosse, Gertjan van Noord, and Robert Malouf.
2001. Alpino: Wide-coverage computational analysis of
Dutch. In Daelemans, Walter, Khalil Sima’an, Jorn Veen-
stra, and Jakub Zavre, editors, Computational Linguistics
in the Netherlands 2000., pages 45–59. Rodopi, Amster-
dam, New York.

Daelemans, W.,

J. Zavrel, K. Van der Sloot,

and
A. Van den Bosch.
2009. TiMBL: Tilburg Memory
Based Learner, version 6.2, reference manual. Technical
Report ILK 09-01, Induction of Linguistic Knowledge,
Tilburg University.

Dagan, I., O. Glickman, and B. Magnini. 2005. The PAS-
CAL Recognising Textual Entailment Challenge. In Pro-
ceedings of the PASCAL Challenges Workshop on Recog-
nising Textual Entailment, Southampton, U.K.

Gildea, Daniel. 2003. Loosely tree-based alignment for
machine translation.
In Proceedings of the 41st Annual
Meeting on Association for Computational Linguistics,
pages 80–87, Sapporo, Japan.

Groves, D., M. Hearne, and A. Way. 2004. Robust sub-
sentential alignment of phrase-structure trees.
In Pro-
ceedings of the 20th International Conference on Com-
putational Linguistics (CoLing ‘04), pages 1072–1078.

Krahmer, Emiel, Erwin Marsi, and Paul van Pelt. 2008.
Query-based sentence fusion is better deﬁned and leads
to more preferred results than generic sentence fusion. In
Moore, J., S. Teufel, J. Allan, and S. Furui, editors, Pro-
ceedings of the 46th Annual Meeting of the Association
for Computational Linguistics: Human Language Tech-
nologies, pages 193–196, Columbus, Ohio, USA.

Kuhn, Harold W. 1955. The Hungarian Method for the as-
signment problem. Naval Research Logistics Quarterly,
2:83–97.

Lavie, A., A. Parlikar, and V. Ambati.

2008. Syntax-
driven learning of sub-sentential translation equivalents
and translation rules from parsed parallel corpora. In Pro-
ceedings of the Second Workshop on Syntax and Structure
in Statistical Translation, pages 87–95.

Lin, D. 1998. An information-theoretic deﬁnition of similar-
ity. In Proceedings of the 15th International Conference
on Machine Learning, pages 296–304.

