



















































Taking into account Inter-sentence Similarity for Update Summarization


Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 204–209,
Taipei, Taiwan, November 27 – December 1, 2017 c©2017 AFNLP

Taking into account Inter-sentence Similarity for Update Summarization

Maâli Mnasri
CEA, LIST,

Univ. Paris-Sud,
Université Paris-Saclay.

maali.mnasri@cea.fr

Gaël de Chalendar
CEA, LIST,

Gif-sur-Yvette,
F-91191 France.

gael.de-chalendar@cea.fr

Olivier Ferret
CEA, LIST,

Gif-sur-Yvette,
F-91191 France.

olivier.ferret@cea.fr

Abstract
Following Gillick and Favre (2009), a lot
of work about extractive summarization
has modeled this task by associating two
contrary constraints: one aims at maxi-
mizing the coverage of the summary with
respect to its information content while
the other represents its size limit. In this
context, the notion of redundancy is only
implicitly taken into account. In this ar-
ticle, we extend the framework defined
by Gillick and Favre (2009) by examin-
ing how and to what extent integrating se-
mantic sentence similarity into an update
summarization system can improve its re-
sults. We show more precisely the impact
of this strategy through evaluations per-
formed on DUC 2007 and TAC 2008 and
2009 datasets.

1 Introduction

As recently shown by Hirao et al. (2017) from a
theoretical viewpoint, there is still room for im-
provements in the extractive approach of Auto-
matic Summarization (AS), which is the frame-
work in which our work takes place. In this con-
text, many methods have been developed for se-
lecting sentences according to various features and
aggregating the results of this selection for build-
ing summaries. All these methods aim at select-
ing the most informative sentences and minimiz-
ing their redundancy while not exceeding a maxi-
mum length.

In this article, we focus on Multi-Document
Summarization (MDS) and more particularly on

its update variant. Having two or more sets of doc-
uments ordered chronologically, the update sum-
marization task consists in summarizing the newer
documents under the assumption that a user has al-
ready read the older documents. Hence, the work
done for tackling this task has mainly extended the
work done for MDS by taking into account the
notion of novelty through different means. Wan
(2012) integrates this notion in the framework of
graph-based methods for computing the salience
of sentences while Delort and Alfonseca (2012)
achieve this integration in the context of hierarchi-
cal topic models. Li et al. (2012) go one step fur-
ther in hierarchical Bayesian models by applying
the paradigm of Hierarchical Dirichlet Processes
to the update task.

Another interesting way to consider the prob-
lem of AS is to formalize it as a constraint op-
timization problem based on Integer Linear Pro-
gramming (ILP). ILP-based approaches are very
flexible as they allow to jointly optimize several
constraints and were found very successful for
MDS, as illustrated by the ICSISumm system of
Gillick and Favre (2009). They have also been de-
veloped for the update summarization task, with
work such as (Li et al., 2015) about the weighting
of concepts.

However, the most obvious weakness of such
methods, particularly the one proposed by Gillick
and Favre (2009), is their implicit way of model-
ing information redundancy. This prevents them
from exploiting work about textual entailment or
paraphrase detection, which could be especially
relevant in the context of MDS. In this article, we
aim at extending the update summarization frame-

204



work of Gillick and Favre (2009) by integrating
non-redundancy features in a more explicit way
through sentences semantic similarities.

2 Summarization framework

For performing MDS with update, the approach
we propose includes two main steps. First, we
perform a semantic clustering over the input doc-
uments sentences, including the old and the new
document sets. Then, we select a subset of sen-
tences for the summary while considering the se-
mantic information resulting from the clustering
step. We detail each of these two steps hereafter.

2.1 Semantic clustering
As in basic MDS, update MDS aims to opti-
mize the content relevance, the information redun-
dancy within a document set and the final sum-
mary length. The additional constraint is to detect
information novelty in the new documents in order
to avoid repeating what has already been read.

Since emphasizing information novelty in the
update summary is equivalent to penalizing old in-
formation in the new document set, we should start
by identifying sentences from the new set that are
equivalent to sentences from the old set. One way
to achieve such identification is to perform seman-
tic clustering over all the sentences, whatever their
source. The aim of semantic clustering here is to
group sentences conveying the same information,
even when they are expressed in different ways.
In addition to detecting redundancy over time, this
filtering step allows to decrease the sentence se-
lection cost by reducing the number of possible
combinations of sentences. Furthermore, consid-
ering similarity at the sentence level rather than the
sub-sentence level is more efficient since the num-
ber of sentences is much lower, which ensures less
calculations of pairwise similarities.

Clustering method For performing the seman-
tic clustering of sentences, we need a clustering
algorithm that uses semantic similarity as a major
feature with a low computing time. Partitioning
algorithms like k-means require the number of
clusters to be set in advance, which is inconsistent
with our main need. Moreover, setting up a
similarity threshold is less dependent on the size
of the considered data than setting up the number
of clusters. While the latter depends on the test
data size and the content diversity, the former
depends on the similarity measure itself and could

be set up on large annotated corpora. Among the
clustering methods relying on a similarity matrix
as input, we chose the Markov Cluster Algorithm
(MCL), a network-based clustering algorithm
simulating the flow in graphs, known to be fast
and scalable (van Dongen, 2000). It assumes that
”a random walk on a network that visits a dense
cluster will likely not leave it until many of its
vertices have been visited”. In our case, it turns
the adjacency matrix of sentences into a transition
matrix. Since our goal is to build small and tight
clusters, we removed pairwise similarities under a
given threshold. Finally, as MCL performs hard
clustering, each sentence belongs to one cluster
only.

Semantic similarity measure Sentence seman-
tic similarity has gained a lot of interest recently,
especially in the context of SemEval evaluations
(Agirre et al., 2016). However, in practice, most
proposed similarity measures for AS are subject to
a time efficiency problem which tends to increase
with the quality of the similarity measure. This is
the case of the lexical word alignement based simi-
larity that won the SemEval 2014 sentence similar-
ity task (Sultan et al., 2014). We found it unusable
in our set-up due to its computational complexity
as we calculate about 5 million sentence pair simi-
larities for some datasets while the SemEval 2014
corpus, for instance, gathers only 3,750 sentence
pairs. Given this constraint, we chose a similarity
measure relying on low dimensional word vectors
from word embeddings. In fact, simply averag-
ing word embeddings of all words in a sentence
has proven to produce a sentence vector encoding
its meaning and has shown a good performance
in multiple tasks and particularly in text similarity
tasks (Das et al., 2016; White et al., 2015; Ger-
shman and Tenenbaum, 2015; Hill et al., 2016).
We adopted this method to represent sentences and
used only the embeddings of unigrams since bi-
grams and phrases are generally not well covered
by the existing pre-trained embeddings1. Before
building the sentence vectors, we did not perform
any normalization of the words in documents (un-
igrams) as words in pre-trained embeddings are
not normalized. Finally, we classically defined the
similarity of two sentences as the cosine similarity
of their vectors.

1Only 0.08% of TAC 2008 dataset bigrams are covered by
the Glove840B embeddings.

205



2.2 Sentence selection
Extracting one sentence per cluster to generate
summaries as in (Zopf et al., 2016) leads to poor
results for update MDS. We have rather added the
information brought by our semantic clustering to
an ILP model for selecting sentences. This model
is the ICSISumm model proposed by Gillick and
Favre (2009). It is a maximum coverage model
operating at the concept level where concepts are
word bigrams. The score of a summary is the sum
of its bigram weights. Each bigram is credited
only once in the summary score to favor diver-
sity at the lexical level. Thus, redundancy is glob-
ally and implicitly minimized. To address the up-
date task, the value of concepts appearing in first
sentences are up-weighted by a factor of 3 as in
(Gillick and Favre, 2009). The ILP problem is for-
malized as follows:

Maximize :
∑
i

wi.ci

Subject to :
∑
j

sj .lj ≤ L (1)

sj .Occij ≤ ci,∀i, j (2)∑
j

sj .Occij ≥ ci, ∀i (3)

ci ∈ {0, 1} ∀i and sj ∈ {0, 1} ∀j

where ci is a variable indicating the presence
of the concept i in the summary; wi refers to the
weight of the concept i, which is its document fre-
quency; lj represents the length of the sentence j
while L is a constant representing the summary
maximum length; sj is the variable that indicates
the presence of the sentence j in the summary and
finally, Occij is a constant parameter indicating
the presence of concept i in sentence j. While the
constraint (1) prevents the whole summary from
exceeding the maximum length limit, constraints
(2) and (3) ensure its consistency.

To take into account the semantic clustering of
sentences in the ILP model, we focused on the
weighting of bigrams since in such models, the
concept weighting method is a key factor in the
performance of the system (Li et al., 2015). As our
aim is to reduce redundancy with the old informa-
tion, we chose to penalize the weights of bigrams
occurring in both old and new documents. If a bi-
gram appears in a cluster including sentences from

both old and new document sets, its weight is pe-
nalized by an α parameter as follows: w′i =

wi
α .

The value of α is determined on a development
set. As in (Gillick and Favre, 2009), bigrams
whose weights are lower than a fixed threshold are
pruned before solving the ILP problem for com-
putational efficiency. However, since this pruning
can have a negative impact results if it is too re-
strictive (Boudin et al., 2015), we carried out the
penalization process after the bigram pruning step.

3 Experiments

3.1 Evaluation Setup

For our experiments, we used the DUC 2007 up-
date corpus and TAC 2008 and 2009 update cor-
pora. The 3 datasets are composed respectively
of 10, 48 and 44 topics. They gather respec-
tively about 25, 20 and 20 news articles per topic.
The articles are ordered chronologically and parti-
tioned into 3 sets, A to C, for DUC 2007 and two
sets, A to B, for both TAC 2008 and 2009. We
only considered sets A and B for all the datasets.

To evaluate our approach, we classically
adopted the ROUGE2 framework (Lin, 2004),
which estimates a summary score by its n-gram
overlap with several reference summaries (Rn).
Although our method is unsupervised, we had to
tune two parameters: the similarity threshold in
the clustering step (for sparcifying the input sim-
ilarity matrix) and the penalization factor α in
the sentence selection. As training data, we used
for each dataset the two other datasets. To set
up these parameters, we followed a greedy se-
quential approach for optimizing ROUGE on each
training set. We maximized the ROUGE-2 recall
score (bigrams overlap) particularly since it has
shown the best agreement with manual evaluations
(Hong et al., 2014). Yet, we report in what fol-
lows three variants of ROUGE: ROUGE-1, which
computes the overlap with reference summaries
in terms of unigrams, ROUGE-2, described pre-
viously and ROUGE-SU4, which computes the
overlap of skip-grams with a skip distance of 4
words at most. Again following (Hong et al.,
2014), we only report the recall values of the
ROUGE metrics because their precision and f-
measure values are very close to them.

2ROUGE parameters: -n 2 -2 4 -m -l 100 -u -c 95 -p 0.5
-f A -r 1000

206



System/dataset DUC 2007 TAC 2008 TAC 2009

R1 R2 RSU4 R1 R2 RSU4 R1 R2 R4

Baseline systems

ICSISumm 2009 33.73 7.59 11.23 38.28 11.19 14.46 37.40 10.37 13.86
ICSISumm-BG-DOWN-1 34.46 7.91 11.74 36.99 10.15 13.66 37.39 10.25 13.87
ICSISumm-BG-DOWN-2 33.71 7.55 11.22 38.02 11.05 14.18 37.27 9.91 13.62

State-of-the-art systems

Supervised ILP - - - - 9.99 13.61 - 9.61 13.77
Topic Modeling - - - 36,73 10.41 13.79 36.42 9.58 13.53
CorrRank - - - 36.71 9.70 13.19 36.87 9.73 13.59

Proposed systems

MCL-W2V-ICSISumm 34.99 8.14 11.79 38.52 11.49 14.68 37.50 10.48 13.98
MCL-GLOVE-ICSISumm 36.08 9.46 12.96 38.62 11.57 14.75 37.53 10.60 14.08
MCL-ConceptNet-ICSISumm 35.23 8.30 11.98 38.28 11.21 14.49 37.53 10.38 13.91

Table 1: Average ROUGE recall scores on DUC 2007, TAC 2008 and TAC 2009 datasets

3.2 Results
We compare our system to three baselines and
three high-level state-of-the-art references.

Baseline systems

• ICSISumm 2009. This is the system de-
scribed in Section 2.2, on which we built our
contribution. We report here the version with
no sentence compression. It is worth noting
that ICSISumm was still found the best per-
forming system in (Hong et al., 2014).

• ICSISumm-BG-DOWN-1. This baseline is
an adaptation of the ICSISumm 2009 system
in which we down-weight the bigrams occur-
ring in the chronologically first set of docu-
ments (A).

• ICSISumm-BG-DOWN-2. In this modified
version of the ICSISumm 2009 system, we
down-weight the bigrams whose frequency in
the chronologically first set of documents (A)
is greater than their frequency in the more re-
cent document set (B).

The last two baselines, which do not include our
semantic clustering of sentences, are tested to
check how effective is this clustering and to what
extent it is needed.

State-of-the-art systems

• Topic Modeling. This system uses topic
probability distributions for salience determi-
nation and a dynamic modeling approach for
redundancy control (Wang and Zhou, 2012).

• CorrRank. This algorithm selects sentences
using a topic evolution pattern by filtering
sentences from particular topic evolution cat-
egories (Lei and Yanxiang, 2010).

• Supervised ILP. This system predicts the bi-
grams weights by means of a supervised
model using salience and novelty features at
the sentence and bigram level. Sentence se-
lection is done by an ILP model and a regres-
sion model for sentence reranking (Li et al.,
2015).

Proposed systems We present the results of our
system with different pre-trained word embed-
dings for evaluating sentence similarities. All
the considered training sets showed that the op-
timal performance is reached when the penaliza-
tion factor α is set to 3. No similarity threshold is
set lower than 0.95, which guarantees a precision
level for the similarity measure at least equal to the
precisions reported in Table 2.

• MCL-W2V-ICSISumm. This version re-
lies on 3 million vectors (300 dimensions)
trained with the CBOW model of Word2Vec
(Mikolov et al., 2013) on 100 billion words
from a Google News dataset.

• MCL-GLOVE-ICSISumm. In this run, we
used 2.2 million word vectors (300 dimen-
sions) trained with GloVe (Pennington et al.,
2014) on the 840 billion tokens from the
Common Crawl repository.

• MCL-ConceptNet-ICSISumm. This version
computes similarities with the ConceptNet

207



Dataset Precision Recall

MSRpara 91.44 17.69
SemEvaL STS 2014 88.00 14.17
SemEvaL STS 2015 90.60 11.46
SemEvaL STS 2016 88.28 25.98

Table 2: Results of our sentence semantic similar-
ity with the minimum threshold value of 0.95

Vector Ensemble embeddings (Speer and
Chin, 2016), which is a combination of
GloVe and Word2Vec embeddings enriched
with knowledge from the ConceptNet seman-
tic network and PPDB.

Table 1 shows that for all the different word
embeddings, our system outperforms all our ref-
erences. The improvement is observed for the
three ROUGE measures we used. The improve-
ment over ICSISumm 2009, which has the same
settings as our system, confirms the interest of
handling redundancy explicitly in the update sum-
marization task. However, the improvement over
ICSISumm-BG-DOWN-1&2 also shows that ba-
sic methods for performing this handling are not
efficient in ILP models, contrary to our sentence
semantic clustering approach. Our second ver-
sion using Glove pre-trained vectors reports higher
results than those using Word2Vec or Concept-
Net Ensemble word vectors. This could be ex-
plained by the size of the training sets for building
the word vectors as the Common Crawl dataset is
much larger than the Google News dataset. More-
over, the impact of the quality of the vectors on
our results indirectly confirms the interest of our
proposal.

Since the semantic similarity of sentences is
central in our approach, we have tried to charac-
terize a posteriori the similarity corresponding to
the value of our similarity threshold as it was opti-
mized on our development sets. We have applied
our semantic similarity measure to reference eval-
uation datasets for sentence similarity3: the MSR
Paraphrase Corpus (Dolan et al., 2004) and the Se-
mEval STS datasets (Agirre et al., 2016). In or-
der to calculate precision and recall scores on the
SemEval datasets, we consider a result as a true
positive if our similarity is higher than 0.95 and

3To our knowledge, the only dataset specifically dedicated
to the evaluation of sentence clustering in the context of MDS
is described in (Geiss, 2009) but it is not publicly available.

the gold standard similarity is higher than 34. We
present in Table 2, the evaluation of our similarity
measure using the Google’s pre-trained word vec-
tors. On all datasets, our similarity shows a high
precision but a weak recall. This trend is partic-
ularly noticeable on the MSR Paraphrase Corpus:
when our system regroups two sentences, they are
paraphrases in 91.44% of the cases, which fits our
initial hypotheses and illustrates their validity.

4 Conclusion and Perspectives

For concluding, we showed that taking into ac-
count the semantic similarity of sentences for dis-
carding redundancy in a maximal bigram cover-
age problem improves the update summarization
performance and can be done by modifying the
weights of bigrams in an ILP model according to
the results of the semantic clustering of sentences.

The most direct perspective we will follow for
extending this work is to improve the recall of the
semantic similarity measure to increase the abil-
ity of our system to detect redundancy. In a more
global extension, we will associate this criterion
about redundancy with criteria more focused on
information salience based on the discourse struc-
ture of documents.

References
Eneko Agirre, Carmen Banea, Daniel Cer, Mona Diab,

Aitor Gonzalez-Agirre, Rada Mihalcea, German
Rigau, and Janyce Wiebe. 2016. SemEval-2016
Task 1: Semantic Textual Similarity, Monolingual
and Cross-Lingual Evaluation. In Proceedings of
the 10th International Workshop on Semantic Eval-
uation (SemEval-2016), pages 497–511, San Diego,
California.

Florian Boudin, Hugo Mougard, and Benot Favre.
2015. Concept-based Summarization using Inte-
ger Linear Programming: From Concept Pruning to
Multiple Optimal Solutions. In 2015 Conference on
Empirical Methods in Natural Language Processing
(EMNLP 2015), pages 1914–1918, Lisbon, Portu-
gal.

Arpita Das, Harish Yenala, Manoj Chinnakotla, and
Manish Shrivastava. 2016. Together we stand:
Siamese Networks for Similar Question Retrieval.
In Proceedings of the 54th Annual Meeting of the
Association for Computational Linguistics (ACL
2016), pages 378–387, Berlin, Germany.

Jean-Yves Delort and Enrique Alfonseca. 2012. Du-
alSum: a Topic-Model based approach for update
4Starting from 3, two sentences are considered as equiva-

lent in the SemEval gold standard.

208



summarization. In 13th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics (EACL 2012), pages 214–223, Avignon,
France.

Bill Dolan, Chris Quirk, and Chris Brockett. 2004.
Unsupervised construction of large paraphrase cor-
pora: exploiting massively parallel news sources. In
Proceedings of the 20th international conference on
Computational Linguistics (COLING 2004), pages
350–356.

Stijn van Dongen. 2000. Graph Clustering by Flow
Simulation. Ph.D. thesis, University of Utrecht.

Johanna Geiss. 2009. Creating a Gold Standard for
Sentence Clustering in Multi-Document Summa-
rization. In 47th Annual Meeting of the Associa-
tion for Computational Linguistics and 4th Interna-
tional Joint Conference on Natural Language Pro-
cessing of the AFNLP (ACL-IJCNLP 2009), Student
Research Workshop, pages 96–104, Singapore.

S.J. Gershman and J.B. Tenenbaum. 2015. Phrase sim-
ilarity in humans and machines. In Proceedings of
the 37th Annual Conference of the Cognitive Science
Society.

Dan Gillick and Benoit Favre. 2009. A Scalable Global
Model for Summarization. In Workshop on Inte-
ger Linear Programming for Natural Language Pro-
cessing (ILP’09), pages 10–18, Boulder, Colorado.

Felix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.
Learning Distributed Representations of Sentences
from Unlabelled Data. In 2016 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies (HLT-NAACL 2016), pages 1367–1377,
San Diego, California.

Tsutomu Hirao, Masaaki Nishino, Jun Suzuki, and
Masaaki Nagata. 2017. Enumeration of Extractive
Oracle Summaries. In 15th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics (EACL 2017), pages 386–396, Valencia,
Spain.

Kai Hong, John Conroy, Benoit Favre, Alex Kulesza,
Hui Lin, and Ani Nenkova. 2014. A Reposi-
tory of State of the Art and Competitive Baseline
Summaries for Generic News Summarization. In
Ninth International Conference on Language Re-
sources and Evaluation (LREC’14), pages 1608–
1616, Reykjavik, Iceland. ELRA.

Huang Lei and He Yanxiang. 2010. CorrRank: Update
Summarization Based on Topic Correlation Analy-
sis. In Advanced Intelligent Computing Theories
and Applications. With Aspects of Artificial Intelli-
gence. 6th International Conference on Intelligent
Computing (ICIC 2010), pages 641–648, Changsha,
China.

Chen Li, Yang Liu, and Lin Zhao. 2015. Improving
Update Summarization via Supervised ILP and Sen-
tence Reranking. In Proceedings of the 2015 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 1317–1322, Denver, Col-
orado.

Jiwei Li, Sujian Li, Xun Wang, Ye Tian, and Baobao
Chang. 2012. Update Summarization using a Multi-
level Hierarchical Dirichlet Process Model. In COL-
ING 2012, pages 1603–1618, Mumbai, India.

Chin-Yew Lin. 2004. ROUGE: A Package for Auto-
matic Evaluation of Summaries. In ACL-04 Work-
shop Text Summarization Branches Out, pages 74–
81, Barcelona, Spain.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word repre-
sentations in vector space. In International Con-
ference on Learning Representations 2013 (ICLR
2013), workshop track.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. GloVe: Global Vectors for
Word Representation. In 2014 Empirical Methods
in Natural Language Processing (EMNLP 2014),
pages 1532–1543.

Robert Speer and Joshua Chin. 2016. An Ensem-
ble Method to Produce High-Quality Word Embed-
dings. CoRR, abs/1604.01692.

Md Arafat Sultan, Steven Bethard, and Tamara Sum-
ner. 2014. DLS@CU: Sentence Similarity from
Word Alignment. In 8th International Workshop on
Semantic Evaluation (SemEval 2014), pages 241–
246, Dublin, Ireland.

Xiaojun Wan. 2012. Update Summarization Based on
Co-Ranking with Constraints. In COLING 2012,
pages 1291–1300, Mumbai, India.

Hongling Wang and Guodong Zhou. 2012. Toward a
Unified Framework for Standard and Update Multi-
Document Summarization. ACM Transactions on
Asian Language Information Processing (TALIP),
11(2):1–18.

Lyndon White, Roberto Togneri, Wei Liu, and Mo-
hammed Bennamoun. 2015. How Well Sentence
Embeddings Capture Meaning. In Proceedings of
the 20th Australasian Document Computing Sympo-
sium (ADCS’15), pages 1–8, New York, NY, USA.
ACM.

Markus Zopf, Eneldo Loza Mencı́a, and Johannes
Fürnkranz. 2016. Sequential Clustering and Con-
textual Importance Measures for Incremental Up-
date Summarization. In 26th International Confer-
ence on Computational Linguistics (COLING 2016),
pages 1071–1082.

209


