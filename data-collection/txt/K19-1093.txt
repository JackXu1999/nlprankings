











































A Personalized Sentiment Model with Textual and Contextual Information


Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 992–1001
Hong Kong, China, November 3-4, 2019. c©2019 Association for Computational Linguistics

992

A Personalized Sentiment Model with Textual and Contextual
Information

Siwen Guo Sviatlana Höhn

ILIAS Research Lab, CSC,
University of Luxembourg

{siwen.guo,sviatlana.hoehn,christoph.schommer}@uni.lu

Christoph Schommer

Abstract

In this paper, we look beyond the traditional
population-level sentiment modeling and con-
sider the individuality in a person’s expres-
sions by discovering both textual and contex-
tual information. In particular, we construct
a hierarchical neural network that leverages
valuable information from a person’s past ex-
pressions, and offer a better understanding of
the sentiment from the expresser’s perspective.
Additionally, we investigate how a person’s
sentiment changes over time so that recent in-
cidents or opinions may have more effect on
the person’s current sentiment than the old
ones. Psychological studies have also shown
that individual variation exists in how easily
people change their sentiments. In order to
model such traits, we develop a modified at-
tention mechanism with Hawkes process ap-
plied on top of a recurrent network for a user-
specific design. Implemented with automati-
cally labeled Twitter data, the proposed model
has shown positive results employing different
input formulations for representing the con-
cerned information.

1 Introduction

Sentiment is one of the key factors affecting hu-
man behavior. Studying the way in which sen-
timent is perceived, evolved and expressed is an
essential part in artificial intelligence. To an-
alyze sentiment in text, researchers have made
different assumptions on linguistic behaviors that
are leveraged with approaches developed based
on the nature of the text, the representation of
the related information and the objectives. How-
ever, majority of the studies are conducted at the
population-level which assumes that people fol-
low a common understanding with regard to the
use of language. Such approaches can be inaccu-
rate in the cases where people use the same lexical
choices to convey different messages or vice versa.

Harris (2006) stated that ‘no two alike’ showing
the inherent difference in human that motivates
the research of personalized sentiment analysis.
Grounded in the psychological works, we argue
that it is significant to study the effect of individu-
ality in the expressions and to investigate the pos-
sibility of providing a deeper understanding of the
expressions from the writers’ own perspectives. In
this work, we concern the use of preferred lexi-
cal choices when expressing sentiment (Reiter and
Sripada, 2002) and the level of consistency in re-
taining a sentiment (Janis and Field, 1956).

Besides the targeted text message itself, we ex-
ploit two types of contextual information for the
purpose of realizing the psychological aspects: a
person’s expressions in the past and the time when
the expressions were made. With the goal of dis-
covering the effect of the contextual information,
distinct formulation methods are proposed to in-
tegrate the information in the personalized senti-
ment model. The backboned model is a hierarchi-
cal neural network which follows a conventional
embedding – recurrent – attention structure with
each part rectified for the task. The embedding
block is used to generate representations for the
used information; the recurrent network fulfills the
task of relating to the information from the past;
the attention model is shaped with Hawkes pro-
cess (Laub et al., 2015) in order to model the infor-
mation decay for each expresser. Generally, recur-
rent networks consider the order of the elements
in a sequence but omit the different gaps between
them. Hawkes process is utilized to compensate
this issue. Furthermore, a novel approach with a
user – factor transformation is employed to merge
the Hawkes process within the attention model and
to construct user-specific processes. For evalua-
tion, we take the data from a number of frequent
users on social platforms where Twitter is used as
an example. The data is domain-independent, and



993

it is possible to obtain self-labeled texts that aligns
with our goal of understanding the expressers’ per-
spectives. Significant improvements are seen with
certain input formulations, and different Hawkes
processes applied for the users are visualized. In
the end, we conclude that it is effective to intro-
duce the contextual information to the model.

2 Related Work

Individualities are mostly considered in senti-
ment analysis when analyzing product-review
texts (Gong et al., 2016; Chen et al., 2016b; Wu
et al., 2018). A common issue that challenges the
research of this area is data sparsity. It is infea-
sible to build or train an effective model for each
user. Gong et al. (2016) address this issue by relat-
ing to a global model that captures ‘social norms’,
and individualities are included by adapting from
the global model via a series of linear transfor-
mations. While in the works that apply neural
networks, the user information is embedded sep-
arately (Chen et al., 2016b) or added at the atten-
tion layer (Chen et al., 2016a; Wu et al., 2018)
in order to make user-specific predictions at the
output. In other works, the aspect of personal-
ization is relaxed to provide user-group based pre-
dictions (Gong et al., 2017). The aforementioned
approaches are modeled with domain-dependent
text, while our concentration on the text associated
with various topics makes the task more challeng-
ing. Wu and Huang (2016) focus on microblog
posts as well and apply the same concept of us-
ing a global model and an individual model via
multi-task learning as in Gong et al. (2016). In ad-
dition, users’ social relations are leveraged to en-
hance the individual models. Similarly, followers’
and followees’ information is also used in Song
et al. (2015) while a variant of latent factor model
is utilized. Most of the studies leverage earlier
posts from users in order to better understand the
individuality; however the evolvement of the sen-
timent is largely neglected — the preferences of
users are considered constant. In this work, we
take the user dynamics into consideration, and in-
corporate user information both in the input and in
the Hawkes process to deal with the data sparsity
and to offer personalized analysis.

Technically, there are very few works that inves-
tigate the different gaps between the input nodes in
a recurrent neural network. Neil et al. (2016) have
proposed a phased LSTM that utilizes an addi-

tional time gate to control the passing of the infor-
mation. However, the time gate is triggered by pe-
riodic oscillations while modeling sensory events,
which makes such a design less flexible when the
time gaps are highly various. As an alternative, we
explicitly add the time gaps in the Hawkes process
to offer a time-sensitive modeling.

In our previous works, we have evaluated the
effectiveness of considering individual differences
in sentiment analysis by employing a concept-
based representation and the static or universal
Hawkes process (Guo et al., 2018, 2019a). In
this paper, we advance the development by adopt-
ing five input formulations with different combi-
nations of granular levels, and propose a refined
model with user-specific Hawkes process to con-
stitute a step forward in capturing the nuances of
user dynamics and providing insights in the per-
sonalized modeling.

3 Personalized Sentiment Model

Motivated by the diversity in individuality, we in-
troduce a model that considers both textual and
contextual information and applies hierarchical
neural networks to facilitate the aspect of person-
alization in sentiment analysis. Particularly, we fo-
cus on analyzing the effect of contextual informa-
tion and discover ways to embed such information
in the prediction process.

3.1 Model Structure
The personalized sentiment model follows a con-
ventional embedding – recurrent – attention struc-
ture as shown in Figure 1 with modifications ap-
plied at each block. First, a formulation method
is applied in order to represent the current and a
number of earlier posts of a user. The embedding

Figure 1: The structure of the sentiment model.



994

layer takes the formulated inputs and produces a
vector for each time step at the recurrent layer.
After that, the outputs of the recurrent layer to-
gether with the auxiliary time differences are fed
to the attention block. Encoded user index is used
in the Hawkes-attention layer when different set-
tings of Hawkes process are considered for differ-
ent users. A fully connected layer is applied af-
terwards to regularize the output of the Hawkes-
attention layer. Finally, the output of the model yt
is generated which is the predicted sentiment label
of the target text.

3.2 Textual and Contextual Information
In linguistic studies, the notion of ‘context’ varies
from theory to theory that some in monologism
see it as ‘secondary complications’, whereas in di-
alogist theory, it considers the reflexive relation
between an expression and its setting or occasion
essential (Linell, 2009). In this work, we target
social text and regard context as an important fac-
tor in the setting of social platforms. Based on the
characteristics of such text and the applicability in
modeling, we categorize the information used in
the sentiment model into two genres: Textual In-
formation — the information that can be extracted
directly from the target text, and Contextual In-
formation — the information that is not present
in the target text but is associated with the text ac-
cording to the user index.

Textual Information
Text is the central part in sentiment analysis. Re-
searchers in this area have proposed various ap-
proaches aiming to provide a deep text under-
standing given the complex nature of how peo-
ple express their sentiments in text. Moreover,
methods designed at the population-level for many
other natural language processing tasks can also
be used for sentiment analysis. Generally, such
methods start at a pre-defined granular level and
generate a representation for the text by capturing
related information in each granule and the ones
surrounding it. In the end, the text is represented
explicitly (e.g., concepts as in Poria et al., 2014)
and / or implicitly (e.g., embeddings as in Pen-
nington et al., 2014 and Peters et al., 2018).

Contextual Information
Besides the target text itself, other types of infor-
mation have been used to support the understand-
ing of the sentiment as well.

Earlier Posts correspond to the texts produced
by the same user in the past. The use of earlier
posts leverages the assumption that a person may
have similar lexical choices when expressing opin-
ions about related topics while different individu-
als share different preferences in this regard. By
analyzing the lexical choices and the topics or en-
tities associated with them, the tendency of repeat-
ing such patterns in a user’s text in the future can
be beneficial to the prediction.

Timestamp corresponds to the creation time of
the text. It has been shown that there exists a cer-
tain level of consistency in an individual’s opin-
ions and such consistency varies from one individ-
ual to another (Janis and Field, 1956). We study
such a trait by taking the timestamp of each earlier
post and applying Hawkes process to observe how
the effect of the information on a user’s behaviors
or opinions decays over time. Note that here, we
do not distinguish the inconsistency between the
time when the expression was made and the time
when the sentiment was felt.

3.3 Input Formulations
We employ different formulations for the input se-
quence based on the representation method. For
all the formulations, timestamps are apart from
other information and are used as an auxiliary in-
put directly at the attention layer with Hawkes pro-
cess. Additionally, user index is used as a feature
in the input in order to handle the issue of data
sparsity, and by doing that, the model is able to
analyze textual and contextual relations targeting
a specific user. The encoded user index is also
used at the Hawkes-attention layer when consid-
ering individual differences in information decay.

Atomic Representation (AR)
In this formulation, four types of components are
extracted: concepts, entities, negations and user
index. Concepts are extracted based on Cambria
et al. (2018) which contain conceptual and affec-
tive information, and can be seen as the ‘signal
terms’ regarding lexical choices. Entities are ex-
tracted based on grammatical rules as the ‘targets’
of a user’s lexical choice. Negations are extracted
based on the lexicon by Reitan et al. (2015) for
their ability to invert the orientation of a sentiment.
User index is extracted for its role in personaliza-
tion. After extracting the components, an embed-
ding layer is applied to generate a representation
vector for the text.



995

Representation with Pre-trained Word
Embeddings (WE)
Pre-trained word vectors such as GloVe (Penning-
ton et al., 2014) and Word2Vec (Mikolov et al.,
2013), generate embeddings according to the co-
occurrences of the words. The word embeddings
are aggregated dimension-wise to produce a vector
for each post. The user index is encoded by itself
and then combined with the representation of the
post at each time point of an input sequence.

Representation with Concepts and Words
(CW)
Since the pre-trained word vectors do not consider
the contexts of the target words, we combine the
representations of both words and concepts in this
formulation. The word embeddings are taken as
the same as the one in the WE formulation. The
concepts appeared in the text are encoded together
with the associated user index so that the relation
between the user and the use of concepts can be
learned. Afterwards, the two types of representa-
tion of the same post are concatenated to generate
an input sequence for the recurrent layer.

Representation with Deep Contextualization
(DC)
Peters et al. (2018) proposed a deep contextualized
representation (ELMo) that takes a finer granular
level (characters) to generate embeddings for the
text by leveraging a deep bidirectional language
model. Prominent results are shown across a num-
ber of linguistic tasks using this representation.
We apply ELMo for each post, and then combine
the representation of the post and the encoded user
index at each time point.

Representation with Combined Granular
Levels (Combi)
This formulation combines three granular lev-
els, namely character-level (DC), word-level and
concept-level (CW). The representations are em-
bedded separately and are concatenated afterwards
as mentioned in Peters et al. (2018).

3.4 Recurrent Neural Network with Input
Selection from Post History

We apply a deep recurrent neural network with
long short-term memory (LSTM, Hochreiter and
Schmidhuber, 1997) on the input sequences con-
structed with one of the input formulations. Each

input sequence Xi consists of an entry of the cur-
rent post at the end of the sequence (which con-
tains textual information and the encoded user in-
dex) and a number of earlier posts by the same user
(contextual information), i.e.,

Xi = [Hi�n, ..., Hi�2, Hi�1, Fa(xi)] (1)

where

Hj =

(
Fa(xj) if u(xj) = u(xi) ,
0 else

n is the number of earlier posts considered, Fa is
the formulation chosen beforehand, and u is the
user index of the post.

Additionally, a selection procedure fol-
lowed Guo et al. (2019b), is performed for
choosing the earlier post xj of a target text xi
from user u. The use of this procedure is moti-
vated by the large difference in user frequency
(the number of posts of a user in a given corpus),
as well as the observation of the case where the
recent posts are unrelated to the current one while
the related posts have appeared long before. The
selection is done by calculating the similarity
between the topics of each earlier post and the
target text. Given a fixed number of time steps T
in a recurrent network and a similarity threshold
�, the recent T earlier posts that have a similarity
score larger or equal to � are chosen. For the case
where the number of chosen posts is smaller than
T , other earlier posts are added in the sequence
as complements prioritizing on the recent ones.
After the selection, the posts in each sequence are
ordered by time.

3.5 Hawkes-Attention Layer
A modified attention mechanism is applied on top
of the recurrent neural network. Attention model
has the ability to provide more flexibilities at the
output layer (Bahdanau et al., 2015). The network
can ‘attend’ to different histories based on the im-
mediate situation. As in Yang et al. (2016), the
model is defined as follows:

ui = tanh (Wthi + bt) (2)

↵i =
exp(u>i ut)P
i exp(u

>
i ut)

(3)

�i = ↵ihi (4)

v =
X

i

�i (5)



996

where hi is the i-th output of the recurrent net-
work, ui is the hidden representation of hi, and ut
is the ‘context vector’. Here, we randomly initial-
ize the context vector which is later jointly learned
with other weights during the training phase. �i
is the representation of the information learned at
time step i. Lastly, v summarizes all the informa-
tion of the posts from the corresponding sequence.
However, conventional recurrent networks and at-
tention models do not differentiate the relations
between time steps regarding various time inter-
vals. To model this difference, we shape the rep-
resentation of the post �i with Hawkes process
before summarizing them at the last step (Equa-
tion 5) in the attention mechanism.

Universal Hawkes Process
Hawkes process is known for modeling the exci-
tation and decay of information over time. When
using exponential decay as the excitation function,
the Hawkes conditional intensity is (Laub et al.,
2015):

�⇤(t) = �+
X

ti<t

↵e��(t�ti) (6)

where � describes the positive background inten-
sity, t is the current time and ti is the time when the
past event happened. ↵ and � are the most impor-
tant factors in the Hawkes process that the former
corresponds to the amount of excitement the past
event brought to the system while the latter corre-
sponds to the decay rate of the excitement. Taking
the same concept as in Guo et al. (2019a), we see a
post in the past as an ‘event’ that can influence the
decision in the future and such influence decays
over time. Instead of treating all the past events
equally, we use �i in Equation 4 as the background
intensity and ↵ = ✏�0i as the amount of excitement
the post at time step i contributes to the current
decision. Note that �0i = max(�i, 0) for we do
not consider negative effect from the past. With
this modification, the information decay of a past
event can also depend on the relativeness between
the past and the current events, and ✏ can be seen
as a scaler to balance the importance of adding the
process. As a result, Equation 5 is replaced with
the following:

v0 =
X

i:�ti>0
(�i + ✏�

0
ie

���ti) (7)

where �ti indicates the time gap between the
earlier post at time step i and the current post.

The current post is included in the summariza-
tion when �ti = 0. ✏ and � are learned jointly
with other learnable parameters during the train-
ing phase.

User-specific Hawkes Process
In order to build user-specific Hawkes process, we
compute the values of ✏ and � in Equation 7 for
each user by applying learned transformation vec-
tors on the encoded user index. In this way, dif-
ferent behaviors concerning the information decay
can be analyzed. That is, ✏ and � are calculated as

✏ = a>✏ E(u) (8)

� = a>�E(u) (9)

where E is the user-index encoder. The trans-
formation vectors a✏ and a� are learned during
the training process, and other settings remain the
same with the universal Hawkes process.

Similarly, Cao et al. (2017) also integrate a
Hawkes process in a neural-based system. To
avoid pre-defining a time decay function, the time
range in an observation is split into a number of
disjoint intervals, whereas user information is em-
bedded in the input. Although this non-parametric
method can be applied in our model, the selection
of the number of intervals undermines the flexibil-
ity of the process. However, as in their work, a
fully connected layer is applied afterwards which
takes the exited (decayed) information representa-
tion v0 as input and outputs the final prediction of
the sentiment yt.

4 Experiments

We investigate the effect of textual and contex-
tual information in personalization and evaluate
the performance of the model employing different
input formulations.

4.1 Dataset
The Sentiment1401 corpus is chosen in the exper-
iments for complying the requirements that

1. there are sufficient frequent users,

2. the text is domain-independent,

3. the desired textual and contextual informa-
tion is present,

1
http://help.sentiment140.com/

for-students, last seen on September 24, 2019

http://help.sentiment140.com/for-students
http://help.sentiment140.com/for-students


997

4. the corpus is annotated from the writers’ per-
spectives.

The corpus is labeled automatically by emoticons
as described in Go et al. (2009) and reflects a user-
specific view in contrast to the corpus labeled by
others such as the SemEval2 corpus. However,
the automatic labeling may also contain a certain
level of noise caused by the variation in emoti-
con usage and the unreliability at the user end.
The experimented dataset is created by taking the
messages from the users who have posted at least
20 times before a pre-set timestamp. This results
in 2369 users with overall 122,000 messages in
which 79,009 are positive and 42,991 are nega-
tive. Furthermore, the dataset is split into a train-
ing set, a development set and a test set according
to two pre-set time points to ensure that the pre-
diction is only made based on the messages in the
past. Other details of the dataset can be found in
the appendix.

4.2 Experimental Settings
The experiments are conducted using Keras3 with
TensorFlow4 backend. The concepts used in the
AR and CW formulations are based on SenticNet
55. In WE and CW, 100 dimensional Twitter word
vectors are taken from GloVe6. The ELMo word
representations in the DC input formulation are
supported by TensorFlow Hub7, which are later
re-trained with other weights in the model. The
inputs with AR, WE and DC are encoded into dif-
ferent lengths based on the number of elements
in each formulation, however they are suppressed
at the embedding layer that generates a vector of
length 100 at each time step in order to make fair
comparisons. In CW and Combi, the input vec-
tors fed to the recurrent layer are longer (164 and
264 respectively) because of the concatenation of
representations. The dimension of user embed-
dings is set to 32. There are three recurrent layers
at the recurrent block that each contains 100 units,

2
http://alt.qcri.org/semeval2017/

task4/, last seen on September 24, 2019
3
https://keras.io/, last seen on September 24,

2019
4
https://www.tensorflow.org/, last seen on

September 24, 2019
5
https://sentic.net/downloads/, last seen on

September 24, 2019
6
https://nlp.stanford.edu/projects/

glove/, last seen on September 24, 2019
7
https://tfhub.dev/google/elmo/2, last seen

on September 24, 2019

and the number of time steps T is set to 20. For
the selection procedure, the same setting is used as
in Guo et al. (2019b), where Manhattan distance
is used as the ground measurement for calculating
topic similarities and the similarity threshold � is
set empirically at 0.8. At the attention block, the
time unit is hour, and the values of ✏ and � are ini-
tialized at 0.01 and 0.001 respectively when using
the universal Hawkes process; the initial values
for the vectors a✏ and a� when using user-specific
processes are also vectors of 0.01 and 0.001 re-
spectively, and the length of the vectors has to be
the same with the dimension of user embeddings,
which is 32. The dimension of the fully connected
layer applied before the output is set to the same
as the number of units in the recurrent layer. We
report the overall accuracy of the model as well as
the F1 scores for the positive and negative classes.
Detailed settings of the model, sample codes for
the Hawkes process, and trained models with the
Combi formulation can be found in the supple-
mentary material.

4.3 Results
Table 1 shows the performance of the model in
different settings. The best result is given by
the Combi formulation with user-specific Hawkes
process. Comparing to the result we have reported
previously in Guo et al. (2019a) with an accuracy
of 76.13, the best performance with this model has
reached 80.38 using the same test set.

Results with Different Input Formulations
Across different input formulations, improve-
ments can be seen comparing the models using
the universal - and the user-specific Hawkes pro-
cess. Although the increase when applying the AR
formulation is not significant, the improvement
of other formulations are significant (t test with
p < 0.05). The lack of improvements with the AR
formulation when learning user-specific behaviors
can be caused by the sparser representation com-
pared to the other formulations. A matching from
each post to a list of concepts and negations is per-
formed which omits information that is not present
in the given list. The list of concepts provided
by SenticNet 5 is more restricted than the word
vectors by GloVe, and is far less flexible than the
character-based representation. In addition, due to
the highly unstructured nature of social texts, the
preprocessing of the posts plays a significant role
in the AR formulation, which also affects the per-

http://alt.qcri.org/semeval2017/task4/
http://alt.qcri.org/semeval2017/task4/
https://keras.io/
https://www.tensorflow.org/
https://sentic.net/downloads/
https://nlp.stanford.edu/projects/glove/
https://nlp.stanford.edu/projects/glove/
https://tfhub.dev/google/elmo/2


998

Input Universal Hawkes Process User-specific Hawkes Process
Formulation Pos. F1 Neg. F1 Accuracy Pos. F1 Neg. F1 Accuracy

AR 75.12 76.87 76.04 74.94 77.48 76.28
WE 76.06 77.06 76.58 76.61 78.41 77.55
CW 76.43 77.71 77.10 76.90 79.10 78.06
DC 76.60 79.71 78.27 77.10 80.36 78.86

Combi 77.78 80.67 79.34 78.06 82.25 80.38

Table 1: Performance of the sentiment model when applying the universal - and the user-specific Hawkes process
with different input formulations.

formance substantially.
We can also observe improvements when us-

ing finer granular levels which are more sensi-
tive and representative towards user variations.
Note that using the character-based DC formula-
tion alone offers better performance than using the
combination of word - and concept-level represen-
tations; however the ELMo representation has a
more complex structure, a higher dimensional out-
put, and it takes longer time to re-train the weights
in the network. In conclusion, the best solution
for constructing representation for the inputs is to
leverage the combined granular levels from char-
acter to word, and to concept (Combi). With
such a representation, the system is able to ana-
lyze user-specific behaviors regarding the lexical
usage and the consistency of sentiment.

Results for Various Lengths of History
Figure 2 shows the performance of the models
while using the CW formulation. The models are
tested for T from 1 where no earlier posts are con-
sidered, to 20 after which no significant improve-
ment can be observed due to the number of related
posts a user normally publishes.

Figure 2: Comparison of the universal - and the user-
specific Hawkes process-based models while using dif-
ferent time steps.

For the case when the user history is not incor-
porated in the model (T = 1,�t = 0), we can
deduce that v0 = �+ ✏�0, which leads to an accu-
racy of 73.87 with the universal ✏ and 74.46 with
the user-specific ✏ (Equation 8).

We can observe an increase in both models
when rising the number of time steps T . The in-
crease indicates that the personalization is effec-
tive and earlier posts are valid contextual informa-
tion. By using the selection procedure, the mod-
els with a smaller number of T (except for when
T = 1) take into account more related posts in the
past. The increase grows slightly faster towards
smaller numbers of T , which is also caused by the
limitation of user frequencies in the experimented
corpus. We believe that given a sufficient number
of frequent users, the performance of the proposed
models can be further improved.

Results for Various User Frequencies

The performance of the models when applying for
users with different frequencies can be seen in Fig-
ure 3. The x-axis corresponds to the lower bound
of the user frequency. We take the lower bound for
the illustration because there are different numbers
of users for each frequency, and many frequencies
have no users to assign to. With both models, sig-
nificant growths for each input formulation can be
observed while increasing the lower bound of the
frequency. Note that although the Combi formu-
lation gives the overall best performance, we can
see from the figure that it does not give the best
results in all the cases. For instance, when the user
frequency is around 80, the WE formulation has
the best accuracy in both models. However, such
an observation is also restricted by the number of
frequent users in general — with only 372 posts
in the test set when the user frequency is at least
100, the performance is highly dependent on the
remaining 3 users. Another observation is that the



999

Figure 3: Performance of the universal - and the user-specific Hawkes process-based models for users with different
frequencies when applying different formulations.

WE formulation performs better than the CW for-
mulation in higher user frequencies, but it has a
lower overall performance because there are more
users who have published less than 30 posts than
the ones who have more.

4.4 Visualization of User-specific
Hawkes-Attention

In order to examine the user-specific Hawkes pro-
cess, we visualize the intermediate calculations for
the values of ✏ and � for 10 random users (Fig-
ure 4). Each cell in the figure corresponds to the
value of a✏i ⇤ Ei(u) in Equation 8 (top figure) or
a� i ⇤ Ei(u) in Equation 9 (bottom figure) at di-
mension i for the respective user.

Figure 4: The vectors of ✏ and � in the user-specific
Hawkes process of the random 10 users.

The effect of the learned transformation vectors
on the 10 users is illustrated. It can be seen that

the last user in the figure (the one at the bottom
line) has the greatest values for ✏ and �, which
means that the decay factor has a great impact on
the prediction for this user than the others but the
influence from the past decays comparably fast —
the user is affected a lot by recent events. In con-
trast, among the 10 users, the second last user is
the least influenced by the past which is visual-
ized in darker colors. From this figure, we can see
that the different decaying processes are indeed
learned for different users with the vector trans-
formation. One may argue that the behavior of the
Hawkes process also depends on the time period
of the experimented dataset; however, if an earlier
post (outside of the training period) is highly rel-
evant to the current one, the large value of �i can
still prevail regardless the value of ✏.

5 Conclusion

This paper presents a personalized sentiment
model that captures the individualities in express-
ing sentiment and analyzes the evolvement of sen-
timent over time. Particularly, we categorize the
information used for the modeling into textual and
contextual information, and evaluate the effective-
ness of using the contextual information to boost
the performance of the model. A novel attention
mechanism with user-specific Hawkes process is
employed for this purpose. Technically, it also
provides an alternative for studying various time
gaps in temporal sequences with neural networks.
Different input formulations are applied in which
the combined granular representation performs the
best. Based on our findings, we can conclude that
the individual variation indeed affects the analy-



1000

sis, and the contextual information, as an essential
part in human interactions, positively contributes
to the performance.

Because the informal text we have used devi-
ates from the language standard, the representa-
tion of input text plays a significant role in improv-
ing the performance. In the future work, we will
exploit phonetic representation which can provide
another source of information for such text. The
posts can be transcribed into phonetic sequences,
for instance, by using the International Phonetic
Alphabet, in order to handle certain misspellings
and to study the trend of using letters with sim-
ilar pronunciations as substitutions. Moreover,
other types of contextual information should be
explored as well to enhance the understanding of
individual behaviors on social platforms. As an
example, social relations can be used to identify
abnormalities in the change of sentiment, espe-
cially in the case that a user is exceptionally stimu-
lated by other users or special events which causes
untypical behaviors. The personalized model can
also be helpful in other scenarios, such as to offer
deep understanding for user-tailored conversations
or companionship.

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-

gio. 2015. Neural machine translation by jointly
learning to align and translate. In 3rd Inter-
national Conference on Learning Representations,
ICLR 2015, San Diego, CA, USA, May 7-9, 2015,
Conference Track Proceedings.

Erik Cambria, Soujanya Poria, Devamanyu Hazarika,
and Kenneth Kwok. 2018. SenticNet 5: Discover-
ing conceptual primitives for sentiment analysis by
means of context embeddings. In Proceedings of
AAAI.

Qi Cao, Huawei Shen, Keting Cen, Wentao Ouyang,
and Xueqi Cheng. 2017. DeepHawkes: bridging the
gap between prediction and understanding of infor-
mation cascades. In Proceedings of the 2017 ACM
on Conference on Information and Knowledge Man-
agement, pages 1149–1158. ACM.

Huimin Chen, Maosong Sun, Cunchao Tu, Yankai Lin,
and Zhiyuan Liu. 2016a. Neural sentiment classifi-
cation with user and product attention. In Proceed-
ings of the 2016 Conference on Empirical Methods
in Natural Language Processing, pages 1650–1659.

Tao Chen, Ruifeng Xu, Yulan He, Yunqing Xia, and
Xuan Wang. 2016b. Learning user and product
distributed representations using a sequence model

for sentiment analysis. IEEE Computational Intelli-
gence Magazine, 11(3):34–44.

Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter sentiment classification using distant supervision.
CS224N Project Report, Stanford, 1(12).

Lin Gong, Mohammad Al Boni, and Hongning Wang.
2016. Modeling social norms evolution for person-
alized sentiment classification. In Proceedings of
the 54th Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers),
volume 1, pages 855–865.

Lin Gong, Benjamin Haines, and Hongning Wang.
2017. Clustered model adaption for personalized
sentiment analysis. In Proceedings of the 26th In-
ternational Conference on World Wide Web, pages
937–946. International World Wide Web Confer-
ences Steering Committee.

Siwen Guo, Sviatlana Höhn, and Christoph Schommer.
2019a. Looking into the past: evaluating the effect
of time gaps in a personalized sentiment model. In
Proceedings of the 34th ACM/SIGAPP Symposium
on Applied Computing, pages 1057–1060. ACM.

Siwen Guo, Sviatlana Höhn, and Christoph Schommer.
2019b. Topic-based historical information selection
for personalized sentiment analysis. In Proceedings
of the 27th European Symposium on Artificial Neu-
ral Networks, Computational Intelligence and Ma-
chine Learning (ESANN), pages 379–384.

Siwen Guo, Sviatlana Höhn, Feiyu Xu, and Christoph
Schommer. 2018. Personalized sentiment analysis
and a framework with attention-based Hawkes pro-
cess model. In International Conference on Agents
and Artificial Intelligence, pages 202–222. Springer.

Judith Rich Harris. 2006. No two alike: Human nature
and human individuality. WW Norton & Company.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Irving L Janis and Peter B Field. 1956. A behavioral
assessment of persuasibility: Consistency of indi-
vidual differences. Sociometry, 19(4):241–259.

Patrick J Laub, Thomas Taimre, and Philip K Pol-
lett. 2015. Hawkes processes. arXiv preprint
arXiv:1507.02822.

Per Linell. 2009. Rethinking language, mind, and
world dialogically, chapter 3. IAP.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in neural information processing
systems, pages 3111–3119.



1001

Daniel Neil, Michael Pfeiffer, and Shih-Chii Liu.
2016. Phased LSTM: Accelerating recurrent net-
work training for long or event-based sequences.
In Advances in Neural Information Processing Sys-
tems, pages 3882–3890.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. GloVe: Global vectors for word
representation. In Proceedings of the 2014 confer-
ence on empirical methods in natural language pro-
cessing (EMNLP), pages 1532–1543.

Matthew Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word rep-
resentations. In Proceedings of the 2018 Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, Volume 1 (Long Papers), pages
2227–2237.

Soujanya Poria, Erik Cambria, Grégoire Winterstein,
and Guang-Bin Huang. 2014. Sentic patterns:
Dependency-based rules for concept-level sentiment
analysis. Knowledge-Based Systems, 69:45–63.

Johan Reitan, Jørgen Faret, Björn Gambäck, and Lars
Bungum. 2015. Negation scope detection for Twit-
ter sentiment analysis. In Proceedings of the 6th
Workshop on Computational Approaches to Subjec-
tivity, Sentiment and Social Media Analysis, pages
99–108.

Ehud Reiter and Somayajulu Sripada. 2002. Human
variation and lexical choice. Computational Lin-
guistics, 28(4):545–553.

Kaisong Song, Shi Feng, Wei Gao, Daling Wang,
Ge Yu, and Kam-Fai Wong. 2015. Personalized
sentiment classification based on latent individuality
of microblog users. In Twenty-Fourth International
Joint Conference on Artificial Intelligence.

Fangzhao Wu and Yongfeng Huang. 2016. Person-
alized microblog sentiment classification via multi-
task learning. In Thirtieth AAAI Conference on Ar-
tificial Intelligence.

Zhen Wu, Xin-Yu Dai, Cunyan Yin, Shujian Huang,
and Jiajun Chen. 2018. Improving review repre-
sentations with user attention and product attention
for sentiment classification. In Thirty-Second AAAI
Conference on Artificial Intelligence.

Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He,
Alex Smola, and Eduard Hovy. 2016. Hierarchi-
cal attention networks for document classification.
In Proceedings of the 2016 Conference of the North
American Chapter of the Associpation for Computa-
tional Linguistics: Human Language Technologies,
pages 1480–1489.


