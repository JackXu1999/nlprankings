



















































DocChat: An Information Retrieval Approach for Chatbot Engines Using Unstructured Documents


Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 516–525,
Berlin, Germany, August 7-12, 2016. c©2016 Association for Computational Linguistics

DocChat: An Information Retrieval Approach for Chatbot Engines
Using Unstructured Documents

Zhao Yan† ∗, Nan Duan‡ , Junwei Bao+ , Peng Chen§ , Ming Zhou‡ ,
Zhoujun Li† , Jianshe Zhou¶

†State Key Laboratory of Software Development Environment, Beihang University
‡Microsoft Research +Harbin Institute of Technology

§Microsoft Search Technology Center ¶BAICIT, Capital Normal University
†{yanzhao, lizj}@buaa.edu.cn +baojunwei001@gmail.com

‡§{nanduan, peche, mingzhou}@microsoft.com ¶zhoujs@cnu.edu.cn

Abstract

Most current chatbot engines are designed
to reply to user utterances based on exist-
ing utterance-response (or Q-R)1 pairs. In
this paper, we present DocChat, a novel
information retrieval approach for chat-
bot engines that can leverage unstructured
documents, instead of Q-R pairs, to re-
spond to utterances. A learning to rank
model with features designed at different
levels of granularity is proposed to mea-
sure the relevance between utterances and
responses directly. We evaluate our pro-
posed approach in both English and Chi-
nese: (i) For English, we evaluate Doc-
Chat on WikiQA and QASent, two answer
sentence selection tasks, and compare it
with state-of-the-art methods. Reasonable
improvements and good adaptability are
observed. (ii) For Chinese, we compare
DocChat with XiaoIce2, a famous chitchat
engine in China, and side-by-side evalua-
tion shows that DocChat is a perfect com-
plement for chatbot engines using Q-R
pairs as main source of responses.

1 Introduction

Building chatbot engines that can interact with hu-
mans with natural language is one of the most
challenging problems in artificial intelligence.
Along with the explosive growth of social media,
like community question answering (CQA) web-
sites (e.g., Yahoo Answers and WikiAnswers) and
social media websites (e.g., Twitter and Weibo),

∗Contribution during internship at Microsoft Research.
1For convenience sake, we denote all utterance-response

pairs (either QA pairs or conversational exchanges from so-
cial media websites like Twitter) as Q-R pairs in this paper.

2http://www.msxiaoice.com

the amount of utterance-response (or Q-R) pairs
has experienced massive growth in recent years,
and such a corpus greatly promotes the emergence
of various data-driven chatbot approaches.

Instead of multiple rounds of conversation, we
only consider a much simplified task, short text
conversation (STC) in which the response R is a
short text and only depends on the last user utter-
anceQ. Previous methods for the STC task mostly
rely on Q-R pairs and fall into two categories:
Retrieval-based methods (e.g., Ji et al., 2014).
This type of methods first retrieve the most pos-
sible 〈Q̂, R̂〉 pair from a set of existing Q-R pairs,
which best matches current utterance Q based on
semantic matching models, then take R̂ as the re-
sponse R. One disadvantage of such a method is
that, for many specific domains, collecting such Q-
R pairs is intractable. Generation based methods
(e.g., Shang et al., 2015). This type of meth-
ods usually uses an encoder-decoder framework
which first encode Q as a vector representation,
then feed this representation to decoder to gener-
ate response R. Similar to retrieval-based meth-
ods, such approaches also depend on existing Q-R
pairs as training data. Like other language genera-
tion tasks, such as machine translation and para-
phrasing, the fluency and naturality of machine
generated text is another drawback.

To overcome the issues mentioned above,
we present a novel response retrieval approach,
DocChat, to find responses based on unstruc-
tured documents. For each user utterance, instead
of looking for the best Q-R pair or generating a
word sequence based on language generation tech-
niques, our method selects a sentence from given
documents directly, by ranking all possible sen-
tences based on features designed at different lev-
els of granularity. On one hand, using documents
rather than Q-R pairs greatly improve the adapt-

516



ability of chatbot engines on different chatting top-
ics. On the other hand, all responses come from
existing documents, which guarantees their fluen-
cy and naturality. We also show promising results
in experiments, on both QA and chatbot scenarios.

2 Task Description

Formally, given an utteranceQ and a document set
D, the document-based chatbot engine retrieves
responseR based on the following three steps:
• response retrieval, which retrieves response

candidates C from D based on Q:

C = Retrieve(Q,D)

Each S ∈ C is a sentence existing in D.
• response ranking, which ranks all response

candidates in C and selects the most possible
response candidate as Ŝ:

Ŝ = arg max
S∈C

Rank(S,Q)

• response triggering, which decides whether
it is confident enough to responseQ using Ŝ:

I = Trigger(Ŝ,Q)

where I is a binary value. When I equals to
true, let the response R = Ŝ and output R;
otherwise, output nothing.

In the following three sections, we will describe
solutions of these three components one by one.

3 Response Retrieval

Given a user utterance Q, the goal of response re-
trieval is to efficiently find a small number of sen-
tences fromD, which have high possibility to con-
tain suitable sentences as Q’s response. Although
it is not necessarily true that a good response
always shares more words with a given utterance,
this measurement is still helpful in finding possi-
ble response candidates (Ji et al., 2014).

In this paper, the BM25 term weighting formu-
las (Jones et al., 2000) is used to retrieve response
candidates from documents. Given each docu-
ment Dk ∈ D, we collect a set of sentence triples
〈Sprev,S,Snext〉 fromDk, where S denotes a sen-
tence in Dk, Sprev and Snext denote S’s previous
sentence and next sentence respectively. Two spe-
cial tags, 〈BOD〉 and 〈EOD〉, are added at the

beginning and end of each passage, to make sure
that such sentence triples can be extracted for ev-
ery sentence in the document. The reason for in-
dexing each sentence together with its context sen-
tences is intuitive: If a sentence within a document
can respond to an utterance, then its context should
be revelent to the utterance as well.

4 Response Ranking

Given a user utteranceQ and a response candidate
S, the ranking function Rank(S,Q) is designed
as an ensemble of individual matching features:

Rank(S,Q) =
∑
k

λk · hk(S,Q)

where hk(·) denotes the k-th feature function, λk
denotes hk(·)’s corresponding weight.

We design features at different levels of gran-
ularity to measure the relevance between S and
Q, including word-level, phrase-level, sentence-
level, document-level, relation-level, type-level
and topic-level, which will be introduced below.

4.1 Word-level Feature

We define three word-level features in this work:
(1) hWM (S,Q) denotes a word matching feature
that counts the number (weighted by the IDF val-
ue of each word in S) of non-stopwords shared
by S and Q. (2) hW2W (S,Q) denotes a word-to-
word translation-based feature that calculates the
IBM model 1 score (Brown et al., 1993) of S and
Q based on word alignments trained on ‘question-
related question’ pairs using GIZA++ (Och and
Ney, 2003). (3) hW2V (S,Q) denotes a word
embedding-based feature that calculates the aver-
age cosine distance between word embeddings of
all non-stopword pairs 〈vSj , vQi〉. vSj represent
the word vector of jth word in S and vQj repre-
sent the word vector of ith word in Q.

4.2 Phrase-level Feature

4.2.1 Paraphrase
We first describe how to extract phrase-level para-
phrases from an existing SMT (statistical machine
translation) phrase table.
PT = {〈si, ti, p(ti|si), p(si|ti)〉}3 is a phrase

table, which is extracted from a bilingual cor-
pus, where si (or ti) denotes a phrase, in source

3We omit lexical weights that are commonly used in
phrase tables, as they are not useful in paraphrase extraction.

517



(or target) language, p(ti|si) (or p(si|ti)) de-
notes the translation probability from si (or ti)
to ti (or si). We follow Bannard and Callison-
Burch (2005) to extract a paraphrase table PP =
{〈si, sj , score(sj ; si)〉}. si and sj denote two
phrases in source language, score(sj ; si) denotes
a confidence score that si can be paraphrased to
sj , which is computed based on PT :

score(sj ; si) =
∑
t

{p(t|si) · p(sj |t)}

The underlying idea of this approach is that, two
source phrases that are aligned to the same target
phrase trend to be paraphrased.

We then define a paraphrase-based feature as:

hPP (S,Q) =
∑N
n=1

∑|S|−n
j=0

CountPP (Sj+n−1j ,Q)
|S|−n+1
N

where Sj+n−1j denotes the consecutive word se-
quence (or phrase) in S , which starts from Sj
and ends with Sj+n−1, N denotes the maximum
n-gram order (here is 3). CountPP (Sj+n−1j ,Q)
is computed based on the following rules:

• If Sj+n−1j ∈ Q, then CountPP (Sj+n−1j ,Q) = 1;

• Else, if 〈Sj+n−1j , s, score(s;Sj+n−1j )〉 ∈ PP
and Sj+n−1j ’s paraphrase s occurs in Q, then
CountPP (Sj+n−1j ,Q) = score(s;Sj+n−1j )

• Else, CountPP (Sj+n−1j ,Q) = 0.

4.2.2 Phrase-to-Phrase Translation
Similar to hPP (S,Q), a phrase translation-based
feature based on a phrase table PT is defined as:

hPT (S,Q) =
∑N
n=1

∑|S|−n
j=0

CountPT (Sj+n−1j ,Q)
|S|−n+1
N

where CountPT (Sj+n−1j ,Q) is computed based
on the following rules:

• If Sj+n−1j ∈ Q, then CountPT (Sj+n−1j ,Q) = 1;

• Else, if 〈Sj+n−1j , s, p(Sj+n−1j |s), p(s|Sj+n−1j )〉 ∈
PT and Sj+n−1j ’s translation s ∈ Q,
then CountPT (Sj+n−1j ,Q) = p(Sj+n−1j |s) ·
p(s|Sj+n−1j )

• Else, CountPT (Sj+n−1j ,Q) = 0

We train a phrase table based on ‘question-answer’
pairs crawled from community QA websites.

4.3 Sentence-level Feature
We first present an attention-based sentence em-
bedding method based on a convolution neural
network (CNN), whose input is a sentence pair
and output is a sentence embedding pair. Two fea-
tures will be introduced in Section 4.3.1 and 4.3.2,
which are designed based on two sentence embed-
ding models trained using different types of data.

In the input layer, given a sentence pair
〈SX ,SY 〉, an attention matrix A ∈ R|SX |×|SY | is
generated based on pre-trained word embeddings
of SX and SY , where each element Ai,j ∈ A is
computed as:

Ai,j = cosine(vSXi , vSYj )

where vSXi (or v
SY
j ) denotes the embedding vector

of the ith (or jth) word in SX (or SY ).
Then, column-wise and row-wise max-pooling

are applied to A to generate two attention vectors
V SX ∈ R|SX | and V SY ∈ R|SY |, where the kth
elements of V SX and V SY are computed as:

V SXk = max
1<l<|SY |

{Ak,l} and V SYk = max
1<l<|SX |

{Al,k}

V SXk (or V
SY
k ) can be interpreted as the attention

score of the kth word in SX (or SY ) with regard to
all words in SY (or SX ).

Next, two attention distributions DSX ∈ R|SX |
and DSY ∈ R|SY | are generated for SX and SY
based on V SX and V SY respectively, where the
kth elements of DSX and DSY are computed as:

DSXk =
eV

SX
k∑|SX |

l=1
eV

SX
l

and D
SY
k =

eV
SY
k∑|SY |

l=1
eV

SY
l

DSXk (or D
SY
k ) can be interpreted as the normal-

ized attention score of the kth word in SX (or SY )
with regard to all words in SY (or SX ).

Last, we update each pre-trained word embed-
ding vSXk (or v

SY
k ) to v̂

SX
k (or v̂

SY
k ), by multiplying

every value in vSXk (or v
SY
k ) with D

SX
k (or D

SY
k ).

The underlying intuition of updating pre-trained
word embeddings is to re-weight the importance
of each word in SX (or SY ) based on SY (or SX ),
instead of treating them in an equal manner.

In the convolution layer, we first derive an in-
put matrix ZSX = {l1, ..., l|SX |}, where lt is the
concatenation of a sequence ofm = 2d−14 updat-
ed word embeddings [v̂SXt−d, ..., v̂

SX
t , ..., v̂

SX
t+d], cen-

tralized in the tth word in SX . Then, the convo-
4In this paper, m is set to 3.

518



lution layer performs sliding window-based fea-
ture extraction to project each vector representa-
tion lt ∈ ZSX to a contextual feature vector hSXt :

hSXt = tanh(Wc · lt)

where Wc is the convolution matrix, tanh(x) =
1−e−2x
1+e−2x is the activation function. The same oper-
ation is performed to SY as well.

In the pooling layer, we aggregate local fea-
tures extracted by the convolution layer from SX ,
and form a sentence-level global feature vector
with a fixed size independent of the length of the
input sentence. Here, max-pooling is used to force
the network to retain the most useful local features
by lSXp = [v

SX
1 , ..., v

SX
K ], where:

vSXi = max
t=1,...,|SX |

{hSXt (i)}

hSXt (i) denotes the ith value in the vector h
SX
t .

The same operation are performed to SY as well.
In the output layer, one more non-linear trans-

formation is applied to lSXp :

y(SX) = tanh(Ws · lSXp )

Ws is the semantic projection matrix, y(SX) is the
final sentence embedding of SX . The same opera-
tion is performed to SY to obtain y(SY ).

We train model parameters Wc and Ws by min-
imizing the following ranking loss function:

L = max{0,M − cosine(y(SX), y(SY ))
+cosine(y(SX), y(S−Y ))}

where M is a constant, S−Y is a negative instance.

4.3.1 Causality Relationship Modeling

We train the first attention-based sentence embed-
ding model based on a set of ‘question-answer’
pairs as input sentence pairs, and then design a
causality relationship-based feature as:

hSCR(S,Q) = cosine(ySCR(S), ySCR(Q))

ySCR(S) and ySCR(Q) denote the sentence em-
beddings of S and Q respectively. We expect
this feature captures the causality relationship be-
tween questions and their corresponding answers,
and works on question-like utterances.

4.3.2 Discourse Relationship Modeling
We train the second attention-based sentence em-
bedding model based on a set of ‘sentence-next
sentence’ pairs as input sentence pairs, and then
design a discourse relationship-based feature as:

hSDR(S,Q) = cosine(ySDR(S), ySDR(Q))
ySDR(S) and ySDR(Q) denote the sentence em-
beddings of S and Q respectively. We expect this
feature learns and captures the discourse relation-
ship between sentences and their next sentences,
and works on statement-like utterances. Here, a
large number of ‘sentence-next sentence’ pairs can
be easily obtained from documents.

4.4 Document-level Feature
We take document-level information into consid-
eration to measure the semantic similarity between
Q and S, and define two context features as:
hDM (S∗,Q) = cosine(ySCR(S∗), ySCR(Q))

where S∗ can be Sprev and Snext that denote
previous and next sentences of S in the original
document. The sentence embedding model trained
based on ‘question-answer’ pairs (in Section
4.3.1) is directly used to generate context embed-
dings for hDM (Sprev,Q) and hDM (Snext,Q). So
no further training data is needed for this feature.

4.5 Relation-level Feature
Given a structured knowledge base, such as Free-
base5, a single relation question Q (in natural
language) with its answer can be first parsed
into a fact formatted as 〈esbj , rel, eobj〉, where
esbj denotes a subject entity detected from the
question, rel denotes the relationship expressed
by the question, eobj denotes an object entity
found from the knowledge base based on esbj
and rel. Then we can get 〈Q, rel〉 pairs. This
rel can help for modeling semantic relationships
between Q and R. For example, the Q-A
pair 〈What does Jimmy Neutron do? − inventor〉
can be parsed into 〈Jimmy Neutron, fiction-
al character occupation, inventor〉 where the rel
is fictional character occupation.

Similar to Yih et al. (2014), We use 〈Q, rel〉
pairs as training data, and learn a rel-CNN mod-
el, which can encode each question Q (or each re-
lation rel) into a relation embedding. For a giv-
en question Q, the corresponding relation rel+ is

5http://www.freebase.com/

519



treated as a positive example, and randomly select-
ed other relations are used as negative examples
rel−. The posterior probability of rel+ givenQ is
computed as:

P (rel+|Q) = e
cosine(y(rel+),y(Q))∑

rel− e
cosine(y(rel−),y(Q))

y(rel) and y(Q) denote relation embeddings of
rel and Q based on rel-CNN. rel-CNN is trained
by maximizing the log-posterior.

We then define a relation-based feature as:

hRE(S,Q) = cosine(yRE(Q), yRE(S))
yRE(S) and yRE(Q) denote relation embeddings
of S and Q respectively, coming from rel-CNN.
4.6 Type-level Feature
We extend each 〈Q, esbj , rel, eobj〉 in the Sim-
pleQuestions data set to 〈Q, esbj , rel, eobj , type〉,
where type denotes the type name of eobj based
on Freebase. Thus, we obtain 〈Q, type〉 pairs.

Similar to rel-CNN, we use 〈Q, type〉 pairs to
train another CNN model, denoted as type-CNN.
Based on which, we define a type-based feature as:

hTE(S,Q) = cosine(yTE(Q), yTE(S))
yTE(S) and yTE(Q) denote type embeddings of
S and Q respectively, coming from type-CNN.
4.7 Topic-level Feature
4.7.1 Unsupervised Topic Model
As the assumption that Q-R pair should share
similar topic distribution, We define an unsuper-
vised topic model-based feature hUTM as the av-
erage cosine distance between topic vectors of
all non-stopword pairs 〈vSj , vQi〉, where vw =
[p(t1|w), ..., p(tN |w)]T denotes the topic vector of
a given word w. Given a corpus, various topic
modeling methods, such as pLSI (probabilistic la-
tent semantic indexing) and LDA (latent Dirichlet
allocation), can be used to estimate p(ti|w), which
denotes the probability thatw belongs to a topic ti.

4.7.2 Supervised Topic Model
One shortcoming of the unsupervised topic model
is that, the topic size is pre-defined, which might
not reflect the truth on a specific corpus. In this pa-
per, we explore a supervised topic model approach
as well, based on ‘sentence-topic’ pairs.

We crawl a large number of 〈S, topic〉 pairs
from Wikipedia documents, where S denotes a

sentence, topic denotes the content name of the
section that S extracted from. Such content names
are labeled by Wikipedia article editors, and can
be found in the Contents fields.

Similar to rel-CNN and type-CNN, we use the
〈S, topic〉 pairs to train another CNN model, de-
noted as topic-CNN. Based on which, we define a
supervised topic model-based feature as:

hSTM (S,Q) = cosine(ySTM (S), ySTM (Q))

ySTM (S) and ySTM (Q) denote topic embeddings
of S andQ respectively, coming from topic-CNN.
4.8 Learning to Ranking Model
We employ a regression-based learning to rank
method (Nallapati, 2004) to train response rank-
ing model, based on a set of labeled 〈Q, C〉 pairs,
Feature weights in the ranking model are trained
by SGD based on the training data that consists
of a set of 〈Q, C〉 pairs, where Q denotes a user
utterance and C denotes a set of response candi-
dates. Each candidate S in C is labeled by + or−,
which indicates whether S is a suitable response
of Q (+), or not (−).

As manually labeled data, such as WikiQA
(Yang et al., 2015), needs expensive human an-
notation effort, we propose an automatic way to
collect training data. First, ‘question-answer’ (or
Q-A) pairs {Qi, Ai}Mi=1 are crawled from commu-
nity QA websites. Qi denotes a question. Ai de-
notes Qi’s answer, which includes one or more
sentences Ai = {s1, ..., sK}. Then, we index an-
swer sentences of all questions. Next, for each
question Qi, we run response retrieval to obtain
answer sentence candidates Ci = {s′1, ..., s

′
N}.

Last, if we know the correct answer sentences of
each questionQi, we can then label each candidate
in Ci as + or −. In experiments, manually labeled
data (WikiQA) is used in open domain question
answering scenario, and automatically generated
data is used in chatbot scenario.

5 Response Triggering

There are two types of utterances, chit-chat ut-
terances and informative utterances. The for-
mer should be handled by chit-chat engines, and
the latter is more suitable to our work, as docu-
ments usually contain formal and informative con-
tents. Thus, we have to respond to informative ut-
terances only. Response retrieval cannot always
guarantee to return a candidate set that contains

520



at least one suitable response, but response rank-
ing will output the best possible candidate all the
time. So, we have to decide which responses are
confident enough to be output, and which are not.

In this paper, we define response triggering as a
function that decides whether a response candidate
S has enough confidence to be output:
I = Trigger(S,Q)

= IU (Q) ∧ IRank(S,Q) ∧ IR(S)
where Trigger(Q,S) returns true, if and only if
all its three sub-functions return true.
IU (Q) returns true, if Q is an informative

query. We collect and label chit-chat queries based
on conversational exchanges from social media
websites to train the classifier.
IRank(S,Q) returns true, if the score s(S,Q)

exceeds an empirical threshold τ :

s(S,Q) = 1
1 + e−α·Rank(S,Q)

where α is the scaling factor that controls the dis-
tribution of s(·) smooth or sharp. Both α and τ are
selected based on a separated development set.
IR(S) returns true, if (i) the length of S is less

than a pre-defined threshold, and (ii) S does not
start with a phrase that expresses a progressive re-
lation, such as but also, besides, moreover and
etc., as the contents of sentences starting with such
phrases usually depend on their context sentences,
and they are not suitable for responses.

6 Related Work

For modeling dialogue. Previous works mainly
focused on rule-based or learning-based approach-
es (Litman et al., 2000; Schatzmann et al., 2006;
Williams and Young, 2007). These methods re-
quire efforts on designing rules or labeling data for
training, which suffer the coverage issue.

For short text conversation. With the fast de-
velopment of social media, such as microblog and
CQA services, large scale conversation data and
data-driven approaches become possible. Ritter et
al. (2011) proposed an SMT based method, which
treats response generation as a machine transla-
tion task. Shang et al. (2015) presented an RNN
based method, which is trained based on a large
number of single round conversation data. Gram-
matical and fluency problems are the biggest issue
for such generation-based approaches. Retrieval-
based methods selects the most suitable response

to the current utterance from the large number of
Q-R pairs. Ji et al. (2014) built a conversation sys-
tem using learning to rank and semantic matching
techniques. However, collecting enough Q-R pairs
to build chatbots is often intractable for many do-
mains. Compared to previous methods, DocChat
learns internal relationships between utterances
and responses based on statistical models at differ-
ent levels of granularity, and relax the dependen-
cy on Q-R pairs as response sources. These make
DocChat as a general response generation solution
to chatbots, with high adaptation capability.

For answer sentence selection. Prior work in
measuring the relevance between question and an-
swer is mainly in word-level and syntactic-level
(Wang and Manning, 2010; Heilman and Smith,
2010; Yih et al., 2013). Learning representation
by neural network architecture (Yu et al., 2014;
Wang and Nyberg, 2015; Severyn and Moschit-
ti, 2015) has become a hot research topic to go
beyond word-level or phrase-level methods. Com-
pared to previous works we find that, (i) Large s-
cale existing resources with noise have more ad-
vantages as training data. (ii) Knowledge-based
semantic models can play important roles.

7 Experiments

7.1 Evaluation on QA (English)
Take into account response ranking task and an-
swer selection task are similar, we first evaluate
DocChat in a QA scenario as a simulation. Here,
response ranking is treated as the answer selection
task, and response triggering is treated as the an-
swer triggering task.

7.1.1 Experiment Setup
We select WikiQA6 as the evaluation data, as it is
precisely constructed based on natural language
questions and Wikipedia documents, which con-
tains 2,118 ‘question-document’ pairs in the train-
ing set, 296 ‘question-document’ pairs in devel-
opment set, and 633 ‘question-document’ pairs in
testing set. Each sentence in the document of a
given question is labeled as 1 or 0, where 1 de-
notes the current sentence is a correct answer sen-
tence, and 0 denotes the opposite meaning. Given
a question, the task of WikiQA is to select answer
sentences from all sentences in a question’s corre-
sponding document. The training data settings of
response ranking features are described below.

6http://aka.ms/WikiQA

521



Fw denotes 3 word-level features, hWM , hW2W
and hW2V . For hW2W , GIZA++ is used to
train word alignments on 11.6M ‘question-related
question’ pairs (Fader et al., 2013) crawled from
WikiAnswers.7. For hW2V , Word2Vec (Mikolov
et al., 2013) is used to train word embedding on
sentences from Wikipedia in English.
Fp denotes 2 phrase-level features, hPP and

hPT . For hPP , bilingual data8 is used to extrac-
t a phrase-based translation table (Koehn et al.,
2003), from which paraphrases are extracted (Sec-
tion 4.2.1). For hPT , GIZA++ trains word align-
ments on 4M ‘question-answer’ pairs9 crawled
from Yahoo Answers10, and then a phrase ta-
ble is extracted from word alignments using the
intersect-diag-grow refinement.
Fs denotes 2 sentence-level features, hSCR and

hSDR. For hSCR, 4M ‘question-answer’ pairs (the
same to hPT ) is used to train the CNN model. For
hSDR, we randomly select 0.5M ‘sentence-next
sentence’ pairs from English Wikipedia.
Fd denotes document-level feature hDM . Here,

we didn’t train a new model. Instead, we just re-
use the CNN model used in hSCR.
Fr and Fty denote relation-level feature hRE

and type-level feature hTE . Bordes et al. (2015)
released the SimpleQuestions data set11, which
consists of 108,442 English questions. Each ques-
tion (e.g., What does Jimmy Neutron do?) is
written by human annotators based on a triple in
Freebase which formatted as 〈esbj , rel, eobj〉 (e.g.,
〈Jimmy Neutron, fictional character occupation,
inventor〉) Here, as described in Section 4.5 and
4.6, ‘question-relation’ pairs and ‘question-type’
pairs based upon SimpleQuestions data set are
used to train hRE and hTE .
Fto denotes 2 topic-level features, hUTM and

hSTM . For hUTM , we run LightLDA (Yuan et
al., 2015) on sentences from English Wikipedi-
a, where the topic is set to 1,000. For hSTM ,
4M ‘sentence-topic’ pairs are extracted from En-
glish Wikipedia (Section 4.7.2), where the most
frequent 25,000 content names are used as topics.

7http://wiki.answers.com
8We use 0.5M Chinese-English bilingual sentences in

phrase table extraction, i.e., LDC2003E07, LDC2003E14,
LDC2005T06, LDC2005T10, LDC2005E83, LDC2006E26,
LDC2006E34, LDC2006E85 and LDC2006E92.

9For each question, we only select the first sentence in its
answer to construct a ‘question-answer’ pair, as it contains
more causality information than sentences in other positions.

10https://answers.yahoo.com
11https://research.facebook.com/research/-babi/

Features MAP MRR
Fw 60.25% 61.70%
Fp 61.31% 62.61%
Fs 61.99% 64.32%
Fd 59.15% 61.17%
Fr 46.95% 45.89%
Fty 45.67% 43.37%
Fto 58.34% 59.96%

Table 1: Impacts of features at different levels.

# Methods MAP MRR
(1) Yih et al. (2013) 59.93% 60.68%
(2) Yang et al. (2015) 65.20% 66.52%
(3) Miao et al. (2015) 68.86% 70.69%
(4) Yin et al. (2015) 69.21% 71.08%
(5) DocChat 68.25% 70.73%
(6) DocChat+(2) 70.08% 72.22%

Table 2: Evaluation of AS task on WikiQA.

7.1.2 Results on Answer Selection (AS)

The performance of answer selection is evaluat-
ed by Mean Average Precision (MAP) and Mean
Reciprocal Rank (MRR). Among all ‘question-
document’ pairs in WikiQA, only one-third of
documents contain answer sentences to their cor-
responding questions. Similar to previous work,
questions without correct answers in the candidate
sentences are not taken into account.

We first evaluate the impact of features at each
level, and show results in Table 1. Fw, Fp, and
Fs perform best among all features, which makes
sense, as they can capture lexical features. Fr and
Fty perform not very good, but make sense, as the
training data (i.e. SimpleQuestions) are based on
Freebase instead of Wikipedia. Interestingly, we
find that Fto and Fd can achieve comparable re-
sults as well. We think the reason is that, their
training data come from Wikipedia, which fit the
WikiQA task very well.

We evaluate the quality of DocChat on Wik-
iQA, and show results in Table 2. The first four
rows in Table 2 represent four baseline methods,
including: (1) Yih et al. (2013), which makes use
of rich lexical semantic features; (2) Yang et al.
(2015), which uses a bi-gram CNN model with av-
erage pooling; (3) Miao et al. (2015), which uses
an enriched LSTM with a latent stochastic atten-
tion mechanism to model similarity between Q-R
pairs; and (4) Yin et al. (2015), which adds the at-
tention mechanism to the CNN architecture.

Table 2 shows that, without using WikiQA’s
training set (only development set for ranking
weights), DocChat can achieve comparable per-

522



Methods MAP MRR
CNNWikiQA 0.6575 0.7534
CNNQASent 0.6951 0.7633

DocChat 0.6896 0.7688

Table 3: Evaluation of AS on QASent.

formance with state-of-the-art baselines. Further-
more, by combining the CNN model proposed by
Yang et al. (2015) and trained on WikiQA training
set, we achieve the best result on both metrics.

Compared to previous methods, we think Doc-
Chat has the following two advantages: First, our
feature models depending on existing resources
are readily available (such as Q-Q pairs, Q-A
pairs, ‘sentence-next sentence’ pairs, and etc.), in-
stead of requiring manually annotated data (such
as WikiQA and QASent). Training of the response
ranking model does need labeled data, but the size
demanded is acceptable. Second, as the training
data used in our approach come from open domain
resources, we can expect a high adaptation capa-
bility and comparable results on other WikiQA-
like tasks, as our models are task-independent.

To verify the second advantage, we evaluate
DocChat on another answer selection data set,
QASent (Wang et al., 2007), and list results in Ta-
ble 3. CNNWikiQA and CNNQASent refer to the re-
sults of Yang et al. (2015)’s method, where the
CNN models are trained on WikiQA’s training
set and QASent’s training set respectively. All
these three methods train feature weights using
QASent’s development set. Table 3 tells, DocChat
outperforms CNNWikiQA in terms of MAP and
MRR, and achieves comparable results compared
to CNNQASent. The comparisons results show a
good adaptation capability of DocChat.

Table 4 evaluates the contributions of features
at different levels of granularity. To highlight the
differences, we report the percent deviation by re-
moving different features at the same level from
DocChat. From Table 4 we can see that, 1) Each
feature group is indispensable to DocChat; 2) Fea-
tures at sentence-level are most important than
other feature groups; 3) Compared to results in
Table 1, combining all features can significantly
promote the performance.

7.1.3 Evaluation of Answer Triggering (AT)
In both QA and chatbot, response triggering is im-
portant. Similar to Yang et al. (2015), we also
evaluate answer triggering using Precision, Recall,
and F1 score as metrics. We use the WikiQA de-

Models MAP Change MRR Change
DocChat 68.25% 70.73%

DocChat - Fw 66.06% -2.19 67.99% -2.74
DocChat - Fp 66.80% -1.45 68.66% -2.07
DocChat - Fs 65.49% -2.76 67.27% -3.46
DocChat - Fd 68.02% -0.23 69.79% -0.94
DocChat - Fr 67.00% -1.25 69.07% -1.66
DocChat - Fty 67.09% -1.16 69.28% -1.45
DocChat - Fto 66.85% -1.40 68.96% -1.77

Table 4: Impacts of different feature groups.

Methods Precision Recall F1
Yang et al. (2015) 28.34 35.80 31.64

DocChat 28.95 44.44 35.06

Table 5: Evaluation of AT on WikiQA.

velopment set to tune the scaling factor α and trig-
ger threshold τ that are described in Section 5,
where α is set to 0.9 and τ is set to 0.5.

Table 5 shows the evaluation results compare to
Yang et al. (2015). We think the improvements
come from the fact that our response ranking mod-
el are more discriminative, as more semantic-level
features are leveraged.

7.2 Evaluation on Chatbot (Chinese)

XiaoIce is a famous Chinese chatbot engine,
which can be found in many platforms including
WeChat official accounts (like business pages on
Facebook Messenger). The documents that each
official account maintains and post to their follow-
ers can be easily obtained from the Web. Mean-
while, a WeChat official account can choose to au-
thorize XiaoIce to respond to its followers’ utter-
ances. We design an interesting evaluation below
to compare DocChat with XiaoIce, based on the
publicly available documents.

7.2.1 Experiment Setup
For ranking features, 17M ‘question-related ques-
tions’ pairs crawled from Baidu Zhidao are used
to train word alignments for hW2W ; sentences
from Chinese Wikipedia are used to train word
embeddings for hW2V and a topic model for
hUTM ; the same bilingual phrase table described
in last experiment is also used to extract a Chinese
paraphrase table for hPP which use Chinese as
the source language; 5M ‘question-answer’ pairs
crawled from Baidu Zhidao are used for hPT ,
hSCR and hDM ; 0.5M ‘sentence-next sentence’
pairs from Chinese Wikipedia are used for hSDR;
1.3M ‘sentence-topic pairs’ crawled from Chi-
nese Wikipedia are used to train topic−CNN for

523



Utterance Response

\��®�{¤oº

(Do you know the history
of Beijing?)

[XiaoIce Response]:·�{¤Æ�Ø�Ð"
(I am not good at history class)
[DocChat Response]:�®{¤aÈ§±J
�3000cc"
(Beijing is a historical city that can be
traced back to 3,000 years ago.)

Table 6: XiaoIce response is more colloquial, as it
comes from Q-R pairs; while DocChat response is
more formal, as it comes from documents.

hSTM . As there is no knowledge base based la-
beled data for Chinese, we ignore relation-level
feature hRE and type-level feature hTE .

For ranking weights, we generate 90,321 〈Q, C〉
pairs based on Baidu Zhidao Q-A pairs by the au-
tomatic method described in Section 4.8. This da-
ta set is used to train the learning to rank model
feature weights {λk} by SGD.

For documents, we randomly select 10 WeChat
official accounts, and index their documents sepa-
rately. The average number of documents is 600.

Human annotators are asked to freely issue 100
queries to each official account to get XiaoIce re-
sponse. Thus, we obtain 100 〈query, XiaoIce
response〉 pairs for each official account. We al-
so send the same 100 queries of each official ac-
count to DocChat based on official account’s cor-
responding document index, and obtain anoth-
er 100 〈query, DocChat response〉 pairs. Given
these 1,000 〈query, XiaoIce response, DocChat
response〉 triples, we let human annotators do a
side-by-side evaluation, by asking them which re-
sponse is better for each query. Note that, the
source of each response is masked during evalu-
ation procedure. Table 6 gives an example.

7.2.2 DocChat v.s. XiaoIce
Table 7 shows the results. Better (or Worse) de-
notes a DocChat response is better (or worse) than
a XiaoIce response, Tie denotes a DocChat re-
sponse and a XiaoIce response are equally good or
bad. From Table 7 we observe that: (1) 156 Doc-
Chat responses (58+47+51) out of 1,000 queries
are triggered. The trigger rate of DocChat is
15.6%. We check un-triggered queries, and find
most of them are chitchat, such as ”hi”, ”hello”,
”who are you?”. (2) Better cases are more than
worse cases. Most queries in better cases are non-
chitchat ones, and their contents are highly relat-
ed to the domain of their corresponding WeChat
official accounts. (3) Our proposed method is a
perfect complement for chitchat engines on in-

Better Worse Tie
Compare to XiaoIce 58 47 51

Table 7: Chatbot side-by-side evaluation.

formative utterances. The reasons for bad cas-
es are two-fold: First, a DocChat response over-
laps with a query, but cannot actually response
it. For this issue, we need to refine the capa-
bility of our response ranking model on measur-
ing causality relationships. Second, we wrong-
ly send a chitchat query to DocChat, as current-
ly, we only use a white list of chitchat queries for
chitchat/non-chitchat classification (Section 5).

8 Conclusion

This paper presents a response retrieval method for
chatbot engines based on unstructured documents.
We evaluate our method on both question answer-
ing and chatbot scenarios, and obtain promising
results. We leave better triggering component and
multiple rounds of conversation handling to be ad-
dressed in our future work.

Acknowledgments

This paper is supported by Beijing Advanced
Innovation Center for Imaging Technology
(No.BAICIT-2016001), the National Natu-
ral Science Foundation of China (Grant Nos.
61170189, 61370126), National High Technology
Research and Development Program of China
under grant (No.2015AA016004), the Fund of the
State Key Laboratory of Software Development
Environment (No.SKLSDE-2015ZX-16).

References
Colin Bannard and Chris Callison-Burch. 2005. Para-

phrasing with bilingual parallel corpora. In Pro-
ceedings of Annual Meeting of the Association for
Computational Linguistics (ACL), pages 597–604.

Antoine Bordes, Nicolas Usunier, Sumit Chopra, and
Jason Weston. 2015. Large-scale simple question
answering with memory networks. arXiv preprint
arXiv:1506.02075.

Peter F Brown, Vincent J Della Pietra, Stephen A Della
Pietra, and Robert L Mercer. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
mation. Computational linguistics, 19(2):263–311.

Anthony Fader, Luke Zettlemoyer, and Oren Etzioni.
2013. Paraphrase-driven learning for open question
answering. In Proceedings of Annual Meeting of the
Association for Computational Linguistics (ACL).

524



Michael Heilman and Noah A Smith. 2010. Tree ed-
it models for recognizing textual entailments, para-
phrases, and answers to questions. In Proceedings
of Annual Conference of the North American Chap-
ter of the Association for Computational Linguistic-
s: Human Language Technologies (NAACL-HLT),
pages 1011–1019.

Zongcheng Ji, Zhengdong Lu, and Hang Li. 2014. An
information retrieval approach to short text conver-
sation. arXiv preprint arXiv:1408.6988.

K Sparck Jones, Steve Walker, and Stephen E. Robert-
son. 2000. A probabilistic model of information re-
trieval: development and comparative experiments:
Part 2. Information Processing & Management,
36(6):809–840.

Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. Proceed-
ings of Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies (NAACL-
HLT), 1:48–54.

Diane Litman, Satinder Singh, Michael Kearns, and
Marilyn Walker. 2000. Njfun: a reinforcemen-
t learning spoken dialogue system. In Proceedings
of the 2000 ANLP/NAACL Workshop on Conversa-
tional systems-Volume 3, pages 17–20.

Yishu Miao, Lei Yu, and Phil Blunsom. 2015. Neu-
ral variational inference for text processing. arXiv
preprint arXiv:1511.06038.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in neural information processing
systems (NIPS), pages 3111–3119.

Ramesh Nallapati. 2004. Discriminative models for
information retrieval. In Proceedings of the inter-
national ACM SIGIR conference on Research and
development in information retrieval, pages 64–71.

Franz Josef Och and Hermann Ney. 2003. A systemat-
ic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19–51.

Alan Ritter, Colin Cherry, and William B Dolan. 2011.
Data-driven response generation in social media. In
Proceedings of the conference on Empirical Meth-
ods in Natural Language Processing (EMNLP),
pages 583–593.

Jost Schatzmann, Karl Weilhammer, Matt Stuttle, and
Steve Young. 2006. A survey of statistical user sim-
ulation techniques for reinforcement-learning of di-
alogue management strategies. The knowledge en-
gineering review, 21(02):97–126.

Aliaksei Severyn and Alessandro Moschitti. 2015.
Learning to rank short text pairs with convolution-
al deep neural networks. In Proceedings of ACM
SIGIR Conference on Research and Development in
Information Retrieval, pages 373–382.

Lifeng Shang, Zhengdong Lu, and Hang Li. 2015.
Neural responding machine for short-text conversa-
tion. Proceedings of Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
1577–1586.

Mengqiu Wang and Christopher D Manning. 2010.
Probabilistic tree-edit models with structured laten-
t variables for textual entailment and question an-
swering. In Proceedings of the International Con-
ference on Computational Linguistics (COLING),
pages 1164–1172.

Di Wang and Eric Nyberg. 2015. A long short-
term memory model for answer sentence selection
in question answering. In Proceedings of Annual
Meeting of the Association for Computational Lin-
guistics (ACL), pages 707–712.

Mengqiu Wang, Noah A Smith, and Teruko Mitamu-
ra. 2007. What is the jeopardy model? a quasi-
synchronous grammar for qa. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), volume 7, pages 22–
32.

Jason D Williams and Steve Young. 2007. Partial-
ly observable markov decision processes for spo-
ken dialog systems. Computer Speech & Language,
21(2):393–422.

Yi Yang, Wen-tau Yih, and Christopher Meek. 2015.
Wikiqa: A challenge dataset for open-domain ques-
tion answering. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 2013–2018.

Wen-tau Yih, Ming-Wei Chang, Christopher Meek, and
Andrzej Pastusiak. 2013. Question answering using
enhanced lexical semantic models. In Proceedings
of Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 1744–1753.

Wen-tau Yih, Xiaodong He, and Christopher Meek.
2014. Semantic parsing for single-relation ques-
tion answering. In Proceedings of Annual Meeting
of the Association for Computational Linguistics (A-
CL), pages 643–648.

Wenpeng Yin, Hinrich Schütze, Bing Xiang, and
Bowen Zhou. 2015. Abcnn: Attention-based con-
volutional neural network for modeling sentence
pairs. arXiv preprint arXiv:1512.05193.

Lei Yu, Karl Moritz Hermann, Phil Blunsom, and
Stephen Pulman. 2014. Deep learning for answer
sentence selection. NIPS Deep Learning and Repre-
sentation Learning Workshop.

Jinhui Yuan, Fei Gao, Qirong Ho, Wei Dai, Jinliang
Wei, Xun Zheng, Eric Po Xing, Tie-Yan Liu, and
Wei-Ying Ma. 2015. Lightlda: Big topic model-
s on modest computer clusters. In Proceedings of
the Annual International Conference on World Wide
Web (WWW), pages 1351–1361.

525


