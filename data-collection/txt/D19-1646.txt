
























































Gazetteer-Enhanced Attentive Neural Networks for Named Entity Recognition


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 6232–6237,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

6232

Gazetteer-Enhanced Attentive Neural Networks for
Named Entity Recognition

Hongyu Lin1,3, Yaojie Lu1,3, Xianpei Han1,2,∗, Le Sun1,2, Bin Dong4, Shanshan Jiang4
1Chinese Information Processing Laboratory 2State Key Laboratory of Computer Science

Institute of Software, Chinese Academy of Sciences, Beijing, China
3University of Chinese Academy of Sciences, Beijing, China

4Ricoh Software Research Center (Beijing) Co.,Ltd.
{hongyu2016,yaojie2017,xianpei,sunle}@iscas.ac.cn

{bin.dong,shanshan.jiang}@srcb.ricoh.com

Abstract

Current region-based NER models only rely
on fully-annotated training data to learn ef-
fective region encoder, which often face the
training data bottleneck. To alleviate this prob-
lem, this paper proposes Gazetteer-Enhanced
Attentive Neural Networks, which can en-
hance region-based NER by learning name
knowledge of entity mentions from easily-
obtainable gazetteers, rather than only from
fully-annotated data. Specially, we first pro-
pose an attentive neural network (ANN), which
explicitly models the mention-context associa-
tion and therefore is convenient for integrating
externally-learned knowledge. Then we de-
sign an auxiliary gazetteer network, which can
effectively encode name regularity of mention-
s only using gazetteers. Finally, the learned
gazetteer network is incorporated into ANN
for better NER. Experiments show that our
ANN can achieve the state-of-the-art perfor-
mance on ACE2005 named entity recognition
benchmark. Besides, incorporating gazetteer
network can further improve the performance
and significantly reduce the requirement of
training data.

1 Introduction

Named entity recognition (NER), aiming to iden-
tify text mentions of specific entity types, is a
fundamental NLP task. Recently, region-based
NER approaches (Finkel and Manning, 2009; Xu
et al., 2017; Sohrab and Miwa, 2018) have at-
tracted significant attention, which first encode all
candidate regions (commonly all subsequences of
a sentence) using a region encoder, then identify
whether each subsequence is an entity mention of
target types using a classifier. For example, in
Figure 1 all subsequences of the sentence, such
as “George Washington”, will first be encoded,
∗Xianpei Han is the corresponding author.

a

SentenceCandidate Region

Inner-Region 
Encoder

Attentive Context 
Encoder

Region 
Representation

Utterance 
Encoder

Utterance

Utterance 
Representation

…

PER

…

PER ORG

Easily 
Available 
Gazetteer

[ George Washington ] was an leader. George Washington

(a) Attentive Neural Network (b) Gazetteer Network

Handcraft 
Training 

Data

Figure 1: The overall architecture of GEANN. The can-
didate region is “George Washington”, which literally
could be a person or an organization (university).

and then be classified into entity types. Com-
pared to sequential labeling models, region-based
models can naturally detect nested or overlapping
mentions by considering all subsequences, and
therefore are of great value to NER.

Generally, an effective region encoder should
capture two kinds of knowledge for NER. One
is name knowledge, which encodes the inner
compositional regularity of entity mentions, i.e.,
how likely a subsequence itself will be an entity
mention. For example, a region encoder should
know “George Washington” is a valid PER name
because “George” is a common first name and
“Washington” is a common last name. Another
is context knowledge, which identifies whether the
subsequence in the context indeed refers to an
entity. For example, a region encoder should know
“X said” is a suitable context for PER mention and
“study at X” is a suitable context for Org mention.

Currently, most region-based NER models learn
these two kinds of knowledge only from expen-
sive, fully-annotated training data, and therefore
often face the training data bottleneck, i.e., the
lack of training data will significantly undermine



6233

the performance. To address this problem, we
find that name knowledge can be effectively cap-
tured by leveraging easily-obtainable gazetteer re-
sources. For example, it is easy to learn the com-
pany name patterns “the...company” and “...Inc.”
from a company name gazetteer containing “the
Walt Disney company” and “Apple Inc.”. By cap-
turing mention regularity entailing gazetteers, the
region-based models can be enhanced with more
accurate name knowledge, and thereby the need
of fully-annotated training data can be reduced.

To this end, this paper proposes Gazetteer-
Enhanced Attentive Neural Networks (GEANN),
whose architecture is shown in Figure 1. Specifi-
cally, to better decouple name and context knowl-
edge for incorporating gazetteer information, we
first design a new region-based attentive neural
network (ANN), which introduces attention mech-
anism (Bahdanau et al., 2014; Vaswani et al.,
2017; Su et al., 2018) to explicitly model the
association between mentions and contexts. Start-
ing from ANN, we further introduce an auxiliary
gazetteer network, which can effectively learn
name knowledge only using gazetteers, i.e., it can
encode each utterance in a context-free way, and
identifies whether it matches regular patterns of
mentions. Finally, the learned gazetteer network
is incorporated into ANN to capture better name
and context knowledge. Experiments show that
ANN can achieve the state-of-the-art NER perfor-
mance, and incorporating name knowledge from
gazetteers can significantly reduce the training
data requirement. To the best of our knowl-
edge, this is the first work trying to explicitly
exploit mention-context association with attention
mechanism in region-based NER, as well as the
first work which enhances NER model with name
knowledge captured from gazetteers using neural
networks.

2 Attentive Neural Network for NER

This section describes our attentive neural net-
work, which directly classifies over all subse-
quences of a sentence to recognize whether each
subsequence corresponds to an entity mention.
Figure 1 (a) shows the architecture of ANN.

Given a sentence, ANN first maps all words in-
to word representations {x1,x2, ...,xn} follow-
ing Lample et al. (2016). Then a BiLSTM layer is
used to obtain context-aware word representation
hAt . After that, for each candidate region sij , we

follow Sohrab and Miwa (2018) to use an inner-
region encoder to obtain its representation sij ,
which captures name knowledge considering both
its boundary and inside information:

sij = MLP([h
A
i ;

1

j − i+ 1

j∑
k=i

hAk ;h
A
j ]), (1)

where MLP is a multi-layer perceptron. To explic-
itly model the association between a region and its
context, we design an attentive context encoder,
which outputs a contextual vector cij entailing the
context knowledge of sij by:

cij =

i−1∑
k=1

αijkh
A
k +

n∑
k=j+1

αijkh
A
k

αijk =
exp(eijk)∑i−1

t=1 exp(eijt) +
∑n

t=j+1 exp(eijt)

(2)

where

eijk = tanh(sij
TΛhAk +Wh

A
k + b) (3)

is an attentive model which scores how important
the word xk is for recognizing the entity type of
sij . After obtaining cij , we concatenate it with
sij , and feed it into a MLP classifier to obtain the
probabilities of sij corresponding to an entity type
(or NIL if sij is not a mention). Similar to previous
methods, ANN is learned by minimizing the cross-
entropy loss on the fully-annotated training data.

By decoupling and explicitly modeling the
mention-context association, ANN not only can
better identify entity mentions, but also is very
easy to incorporate external name knowledge.
This enables convenient integration of gazetteer
knowledge, as we will illustrate next.

3 Gazetteer-Enhanced ANN

One main drawback of ANN and other region-
based models is that they only rely on fully-
annotated data to learn both name knowledge and
context knowledge. Unfortunately, such training
data is very expensive to construct, which limits
the applications of these model to more entity
types. To tackle this problem, we proposes to learn
name knowledge from easily-obtainable, large-
scale gazetteers. In this way, the name knowledge
can be captured more accurately, and the require-
ment of fully-annotated data can be reduced.

To this end, we propose to incorporate an auxil-
iary gazetteer network into ANN, which can learn
and exploit name knowledge only using gazetteer-
s. Given an utterance, gazetteer network predicts



6234

whether it should be included in the gazetteer of
specific entity types, i.e., whether it is a valid
entity name. Gazetteer network is context-free,
which only considers whether the input follows
the compositional regularity of mentions, and
therefore can be trained only using gazetteers.

Formally, given an input utterance u =
{ui, ..., uj}, an utterance encoder first learns its
representation u, which has similar structure with
the inner-region encoder of ANN. After that, u
is used to compute the probabilities of it being a
valid name for each type:

OGu = s(Wu+ b). (4)

where s is the sigmoid function and kth dimension
of OGu indicates the probability of u being a valid
mention of type yk. As an utterance can possibly
be a valid mention of various types, we use a
multi-label, multi-class cross-entropy loss to train
our gazetteer network:

LG(θ) =
∑
u∈G

[g′u logO
G
u + (1− g′u) log(1−OGu )] (5)

where G is the gazetteers, g′u is an one-hot vector
whose kth will be set to 1 if utterance u is in the
gazetteer of type yk, elsewise 0.

In this way, a well-trained gazetteer network
can learn an effective utterance representation u,
which is used to identify whether the utterance is
a valid mention. This means u should capture
enough name knowledge of specific entity types.
To incorporate such knowledge into ANN, we
simply concatenate the representation learned by
gazetteer network and the representation learned
by the original inner-region encoder. Then this
new representation is fed to the following modules
of ANN. In this way, name knowledge learned
from gazetteers is incorporated to enhance the
region encoder, and the requirement of fully-
annotated data can be reduced.

4 Experiments

4.1 Experimental Settings
Data Preparation. We conducted experiments
on ACE2005 named entity recognition task1. We
used the same dataset splits as Wang and Lu
(2018); Katiyar and Cardie (2018). For each entity
type in ACE2005, we collect a gazetteer from

1Conventional NER datasets, such as CoNLL2003, re-
moved nested entity mentions and therefore are not suitable
for evaluating region-based NER models.

P R F1
LSTM-CRF 73.2 61.3 66.7
Neural Transition 75.2 65.5 70.0
Segmental Hypergraph 75.7 68.3 71.8
Exhaustive 81.2 66.9 73.4
ANN 78.9 69.8 74.1
GEANN 77.1 73.3 75.2

Table 1: Experiment results on ACE2005 named entity
mention recognition.

Wikipedia anchor texts, i.e., anchor texts linking to
an entity whose type are the same will be included
in a gazetteer. The same as previous studies,
models are evaluated using micro-F1. To balance
time complexity and recall, we follow Wang and
Lu (2018) to restrict mention length up to 6, which
covers more than 93% mentions.
Baselines. Following methods were compared:

1) LSTM-CRF (Lample et al., 2016), which is
the most widely used NER baseline, but it cannot
handle nested or overlapping mentions.

2) Neural Transition (Wang et al., 2018), a
transition model which achieved very competitive
performance on ACE2005.

3) Segmental Hypergraph (Wang and Lu,
2018), a hypergraph-based model which intro-
duces a new tagging schema and achieved the
state-of-the-art performance on ACE2005.

4) Exhaustive Model (Sohrab and Miwa,
2018), a region-based model using a region en-
coder to capture both inner and boundary features
of a candidate region, which is similar to ANN
without attentive contextual encoder.

4.2 Overall Results

Table 1 shows the overall results of our methods
compared with baselines. We can see that:

1) By explicitly modeling both name and con-
text knowledge, the proposed attentive neural
network is an effective region-based NER mod-
el, and achieves the state-of-the-art performance.
Compared with other baselines, ANN achieves
significant F1-score improvements.

2) Incorporating name knowledge from
gazetteers can significantly improve the
performance. GEANN further achieves 1.1
F1-score improvement over ANN. This indicates
that name knowledge learned from gazetteers
can significant enhance the region encoder, and
therefore improves the NER performance.



6235

65.6

70.2

73.0
74.5

75.2

46

51

56

61

66

71

76

0.1 0.25 0.5 0.75 1

GEANN
ANN
Exhaustive Model
Segmental Hypergraph
Neural Transition

Training Data Usage

Figure 2: F1-scores on ACE2005 when training data
size varies.

3) Our attentive context encoder provides an
effective way to exploit context knowledge for
NER. Compared with Exhaustive baseline, AN-
N achieves significant improvement by explicitly
modeling the association between entity mentions
and their contexts.

4.3 Effects of Gazetteer Network
To further investigate the effect of introducing
gazetteers, Table 2 shows the results when training
data size varies. We can see that:

1) For Transition and SH, their performance sig-
nificantly decreased when reducing training data.
We believe this because these approaches need
to model complex label structure, so large scale
training data are critical, and reducing training
data will have huge impact on them.

2) Region-based models are less sensitive to
the reduction of training data. We believe this
is because their output structure is simple, and
therefore can be trained using less training data.

3) GEANN can achieve significant improve-
ments over ANN regardless of training data size.
By leveraging name knowledge from gazetteers,
GEANN with only 50% training data can achieve
comparable performance with ANN trained with
the entire dataset.

4.4 GEANN with BERT
Pretrained context-aware representation, such as
ELMOs (Peters et al., 2018) and BERT (Devlin
et al., 2018), have shown significant progress in
many NLP tasks, especially in low-resource cases.
To verify the adaptivity of the proposed GEANN,
we further introduce BERT into models by replac-
ing the word embedding with BERT representa-
tions. Figure 3 shows the results. We can see

69.8

74.9

78.1
79.2

80.1

60

63

66

69

72

75

78

81

0.1 0.25 0.5 0.75 1

GEANN+BERT

ANN+BERT

Exhaustive Model+BERT

GEANN

ANN

Exhaustive Model

Training Data Usage

Figure 3: F1-scores of models with BERT on ACE2005
when training data size varies.

that even BERT can enhance NER, GEANN can
still further achieve significant improvement over
BERT regardless of the training data size. This
verified GEANN can further capture task-specific
name knowledge, which complement well with
universal pretrained language knowledge.

5 Related Work

Sequential labeling approaches (Zhou and Su,
2002; Chieu and Ng, 2002; Bender et al., 2003;
Settles, 2004; Lample et al., 2016) are widely
used in NER. But this paradigm cannot handle
nested mentions without specially designed tag-
ging schema (Lu and Roth, 2015; Katiyar and
Cardie, 2018; Wang and Lu, 2018; Lin et al.,
2019). Recently, region-based models provide a
natural solution for this issue. Finkel and Manning
(2009) first proposed to classify over regions cor-
responding to parsing tree nodes. Xu et al. (2017)
proposed to directly classify over all subsequences
with a neural network model. Sohrab and Miwa
(2018) extended their method by introducing a
new region encoder. Generally, these methods
have achieved promising results but heavily rely
on fully-annotated data.

Gazetteers or dictionaries have long been re-
garded as a useful and easily-obtainable resource
for NER. Previous methods commonly incorporat-
ed gazetteers by either using them to as handcraft
features (Bender et al., 2003; Tsuruoka and Tsujii,
2003; Ciaramita and Altun, 2005; Minkov et al.,
2005; Ritter et al., 2011; Seyler et al., 2018; Yu
et al., 2018), or using them to generate data by
distant supervision (Cohen, 2005; Ren et al., 2015;
Giannakopoulos et al., 2017; Shang et al., 2018).
However, the first kind of methods can not fully



6236

leverage the inner mention structure knowledge
entailing in gazetteers, while the second approach-
es will result in remarkable noise.

6 Conclusions

This paper first proposes attentive neural network-
s, an effective region-based model which explic-
itly models mention-context association. Then
we propose to incorporate an auxiliary gazetteer
network to enhance ANN. The gazetteer network
can effectively learn name knowledge only us-
ing easily-available gazetteers, and therefore can
significantly improve model performance and re-
duce data requirement. Experiments show that
GEANN achieves the state-of-the-art performance
on ACE2005 with much lower data requirement.

Acknowledgments

We sincerely thank the reviewers for their insight-
ful comments and valuable suggestions. More-
over, this work is supported by the National Nat-
ural Science Foundation of China under Grants
no. 61433015, 61572477 and 61772505; and the
Young Elite Scientists Sponsorship Program no.
YESS20160177.

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-

gio. 2014. Neural machine translation by jointly
learning to align and translate. arXiv preprint
arXiv:1409.0473.

Oliver Bender, Franz Josef Och, and Hermann Ney.
2003. Maximum entropy models for named en-
tity recognition. In Proceedings of the seventh
conference on Natural language learning at HLT-
NAACL 2003-Volume 4, pages 148–151. Association
for Computational Linguistics.

Hai Leong Chieu and Hwee Tou Ng. 2002. Named en-
tity recognition: a maximum entropy approach using
global information. In Proceedings of the 19th inter-
national conference on Computational linguistics-
Volume 1, pages 1–7. Association for Computational
Linguistics.

Massimiliano Ciaramita and Yasemin Altun. 2005.
Named-entity recognition in novel domains with
external lexical knowledge. In Proceedings of the
NIPS Workshop on Advances in Structured Learning
for Text and Speech Processing, volume 2005.

Aaron M Cohen. 2005. Unsupervised gene/protein
named entity normalization using automatically ex-
tracted dictionaries. In Proceedings of the acl-ismb
workshop on linking biological literature, ontologies

and databases: Mining biological semantics, pages
17–24. Association for Computational Linguistics.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. arXiv preprint arXiv:1810.04805.

Jenny Rose Finkel and Christopher D Manning. 2009.
Nested named entity recognition. In Proceedings
of the 2009 Conference on Empirical Methods in
Natural Language Processing: Volume 1-Volume
1, pages 141–150. Association for Computational
Linguistics.

Athanasios Giannakopoulos, Claudiu Musat, Andreea
Hossmann, and Michael Baeriswyl. 2017. Unsu-
pervised aspect term extraction with b-lstm & crf
using automatically labelled datasets. arXiv preprint
arXiv:1709.05094.

Arzoo Katiyar and Claire Cardie. 2018. Nested named
entity recognition revisited. In Proceedings of the
2018 Conference of the North American Chapter
of the Association for Computational Linguistics:
Human Language Technologies, Volume 1 (Long
Papers), pages 861–871. Association for Computa-
tional Linguistics.

Guillaume Lample, Miguel Ballesteros, Sandeep Sub-
ramanian, Kazuya Kawakami, and Chris Dyer. 2016.
Neural architectures for named entity recognition.
arXiv preprint arXiv:1603.01360.

Hongyu Lin, Yaojie Lu, Xianpei Han, and Le Sun.
2019. Sequence-to-nuggets: Nested entity men-
tion detection via anchor-region networks. arXiv
preprint arXiv:1906.03783.

Wei Lu and Dan Roth. 2015. Joint mention extraction
and classification with mention hypergraphs. In
Proceedings of the 2015 Conference on Empirical
Methods in Natural Language Processing, pages
857–867.

Einat Minkov, Richard C Wang, and William W Co-
hen. 2005. Extracting personal names from email:
Applying named entity recognition to informal tex-
t. In Proceedings of human language technology
conference and conference on empirical methods in
natural language processing.

Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word rep-
resentations. arXiv preprint arXiv:1802.05365.

Xiang Ren, Ahmed El-Kishky, Chi Wang, Fangbo Tao,
Clare R Voss, and Jiawei Han. 2015. Clustype:
Effective entity recognition and typing by relation
phrase-based clustering. In Proceedings of the 21th
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, pages 995–1004.
ACM.



6237

Alan Ritter, Sam Clark, Oren Etzioni, et al. 2011.
Named entity recognition in tweets: an experimen-
tal study. In Proceedings of the conference on
empirical methods in natural language processing,
pages 1524–1534. Association for Computational
Linguistics.

Burr Settles. 2004. Biomedical named entity recogni-
tion using conditional random fields and rich feature
sets. In Proceedings of the international joint work-
shop on natural language processing in biomedicine
and its applications, pages 104–107. Association for
Computational Linguistics.

Dominic Seyler, Tatiana Dembelova, Luciano Del Cor-
ro, Johannes Hoffart, and Gerhard Weikum. 2018. A
study of the importance of external knowledge in the
named entity recognition task. In Proceedings of the
56th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 2: Short Papers), pages
241–246.

Jingbo Shang, Liyuan Liu, Xiang Ren, Xiaotao Gu,
Teng Ren, and Jiawei Han. 2018. Learning named
entity tagger using domain-specific dictionary. arX-
iv preprint arXiv:1809.03599.

Mohammad Golam Sohrab and Makoto Miwa. 2018.
Deep exhaustive model for nested named entity
recognition. In Proceedings of the 2018 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 2843–2849. Association for Compu-
tational Linguistics.

Jinsong Su, Jiali Zeng, Deyi Xiong, Yang Liu, Mingx-
uan Wang, and Jun Xie. 2018. A hierarchy-
to-sequence attentional neural machine translation
model. IEEE/ACM Transactions on Audio, Speech
and Language Processing (TASLP), 26(3):623–632.

Yoshimasa Tsuruoka and Jun’ichi Tsujii. 2003. Boost-
ing precision and recall of dictionary-based protein
name recognition. In Proceedings of the ACL
2003 workshop on Natural language processing in
biomedicine-Volume 13, pages 41–48. Association
for Computational Linguistics.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is
all you need. In Advances in neural information
processing systems, pages 5998–6008.

Bailin Wang and Wei Lu. 2018. Neural segmental
hypergraphs for overlapping mention recognition.
In Proceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing, pages
204–214. Association for Computational Linguistic-
s.

Bailin Wang, Wei Lu, Yu Wang, and Hongxia Jin.
2018. A neural transition-based model for nested
mention recognition. In Proceedings of the 2018
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1011–1017. Association
for Computational Linguistics.

Mingbin Xu, Hui Jiang, and Sedtawut Watcharawit-
tayakul. 2017. A local detection approach for
named entity recognition and mention detection.
In Proceedings of the 55th Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers), pages 1237–1247. Association for
Computational Linguistics.

Xiaodong Yu, Stephen Mayhew, Mark Sammons, and
Dan Roth. 2018. On the strength of character lan-
guage models for multilingual named entity recog-
nition. arXiv preprint arXiv:1809.05157.

GuoDong Zhou and Jian Su. 2002. Named entity
recognition using an hmm-based chunk tagger. In
proceedings of the 40th Annual Meeting on Associa-
tion for Computational Linguistics, pages 473–480.
Association for Computational Linguistics.


