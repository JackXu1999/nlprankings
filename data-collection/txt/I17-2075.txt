



















































An Exploration of Data Augmentation and RNN Architectures for Question Ranking in Community Question Answering


Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 442–447,
Taipei, Taiwan, November 27 – December 1, 2017 c©2017 AFNLP

An Exploration of Data Augmentation and RNN Architectures for
Question Ranking in Community Question Answering

Charles Chen
School of EECS
Ohio University

Athens, OH 45701
lc971015@ohio.edu

Razvan Bunescu
School of EECS
Ohio University

Athens, OH 45701
bunescu@ohio.edu

Abstract

The automation of tasks in community
question answering (cQA) is dominated
by machine learning approaches, whose
performance is often limited by the num-
ber of training examples. Starting from
a neural sequence learning approach with
attention, we explore the impact of two
data augmentation techniques on question
ranking performance: a method that swaps
reference questions with their paraphrases,
and training on examples automatically se-
lected from external datasets. Both meth-
ods are shown to lead to substantial gains
in accuracy over a strong baseline. Fur-
ther improvements are obtained by chang-
ing the model architecture to mirror the
structure seen in the data.

1 Introduction

Community question answering (cQA) is an in-
formation seeking paradigm in which users ask
questions and contribute answers on a dedicated
website that facilitates quality-based ranking and
retrieval of contributed content. The questions
posted on a QA website range from very general
(e.g. Yahoo! Answers), to topic-specific, such as
programming languages (e.g. Stack Overflow) or
relevant for a geographical area (e.g. Qatar Liv-
ing). An important task in cQA is that of ques-
tion retrieval, wherein questions that have already
been answered on the website are ranked with re-
spect to how well their answers match the infor-
mation need expressed in a new question. Nu-
merous approaches to question retrieval, question
ranking, or question-question similarity have been
proposed over the last decade, of which (Xue et al.,
2008; Bernhard and Gurevych, 2008; Duan et al.,
2008; Cao et al., 2009; Wang et al., 2009; Bunescu

and Huang, 2010; Zhou et al., 2011) are just a
few. Very recently, question-question similarity
has received renewed interest as a subtask in the
SemEval cQA evaluation exercise (Nakov et al.,
2016). In this paper, we approach question rank-
ing in a context where the input is restricted to
the question text and describe data augmentation
methods and RNN architectures that are empiri-
cally shown to improve ranking performance. We
expect these ideas to also benefit more comprehen-
sive approaches, such as the SemEval cQA exer-
cise, which exploit answers and comments associ-
ated with previously answered questions.

2 Ranking Model with Attention

Following the notation of Bunescu and Huang
(2010), we use

〈
Qi � Qj |Qr

〉
to denote that

the answer to question Qi is expected to be more
useful than the answer to Qj in terms of satis-
fying the information need expressed in Qr. If〈
Qi � Qj |Qr

〉
, then the question ranking system

is expected to rank Qi higher than Qj , through a
scoring function s(Qr, Q) that is trained to capture
how relevant Q is to Qr. Training and evaluating
the scoring function requires a dataset of ranking
triples

〈
Qi � Qj |Qr

〉
. Ranking triples are usually

introduced implicitly by annotating questions into
3 major categories: paraphrases (P), useful (U),
and neutral (N ). A paraphrasing questionQp ∈ P
is semantically equivalent with or very close to
the reference question. A question Qu ∈ U is
deemed useful or relevant if its answer is expected
to overlap in information content with the answer
of the reference question, whereas the answer of
a neutral or irrelevant question Qn ∈ N should
be irrelevant for the reference question. Corre-
spondingly, the following relations are assumed
to hold: 〈Qp � Qu|Qr〉, i.e. a paraphrasing
question is more useful than a useful question;

442



Figure 1: Neural sequence learning with attention (SLA) model for question ranking.

〈Qu � Qn|Qr〉, i.e. a useful question is more
useful than a neutral question; and by transitiv-
ity 〈Qp � Qn|Qr〉. The resulting triples can be
used for training and evaluating the scoring func-
tion s(Qr, Q) using a ranking objective, which is
the approach taken in this paper. An alternative is
to use a binary classification objective by consid-
ering only two categories of questions, e.g. rele-
vant (P ∪ U) and irrelevant (N ), as was done in
SemEval. However, by ignoring the difference in
utility between paraphrases and useful questions
during training, a binary classification approach is
likely to underperform a ranking approach that is
trained on all the ranking triples implied by the
original 3 categories of questions.

To compute the ranking function s(Qr, Q), we
use neural sequence learning with attention (SLA),
as illustrated in Figure 1. Neural networks with
attention have been successful used in a wide va-
riety of tasks, ranging from image classification
and dynamic visual control (Mnih et al., 2014), to
machine translation (Bahdanau et al., 2015) and
image caption generation (Xu et al., 2015). Very
recently, the SLA approach was used for semantic
entailment (Rocktschel et al., 2015) and cQA tasks
(Mohtarami et al., 2016), although still with an ob-
jective (e.g., cross entropy) aimed at classification.
The questions Qr and Q are processed sequen-
tially, using for each a separate RNN with gated
recurrent units (GRU) (Cho et al., 2014). Follow-
ing the notation of Bahdanau et al. (2015), the
states st corresponding to positions t in question
Q are computed recursively as follows:

st = (1− zt) ◦ st−1 + zt ◦ ŝt (1)
ŝt = tanh(Wxt + U(rt ◦ st−1) + [Cct])
rt = σ(Wrxt + Urst−1 + [Crct])
zt = σ(Wzxt + Uzst−1 + [Czct])

The states ht for the reference question Qr are
computed using the same equations, but with dif-
ferent parameters and without the attention terms
shown between brackets. The initial state h0 = 0,
whereas s0 = tanh(Wshm) is computed as a nor-
malized linear transformation of the last state hm.
Words are mapped to their word2vec embeddings
xt, pre-trained on Google News (Mikolov et al.,
2013). States st require a context vector ct, to be
computed with the attention model below:

ct =

m∑
j=1

αtj ∗ hj , where αtj = exp(etj)∑m
k=1 exp(etk)

(2)

etj = a(st−1, hj) = v
T
a tanh(Wast−1 + Uahj)

The score s(Qr, Q) = vT sn is computed as a
linear combination of the RNN state correspond-
ing to the last word in Q. Given a set of train-
ing triples

〈
Qi, Qj |Qr

〉
, the model parameters are

trained to optimize the margin-based ranking cri-
terion shown in Equation 3.

J(θ) =
∑

Qi>Qj |Qr
max {0, γ − s(Qr, Qi) + s(Qr, Qj)} (3)

3 Data Augmentation

Supervised ML approaches are often limited by
the number of available training examples. Us-
ing the SLA approach described in Section 2, we
explore the impact of two data augmentation tech-
niques for question ranking: a novel method that
swaps reference questions with their paraphrases,
and training on examples from external datasets.

3.1 Question Swapping

Since paraphrases are semantically equivalent
with or very close to the reference questions, dur-
ing training we swap each paraphrase question

443



with the reference question, and generate addi-
tional ranking triples of the type 〈Qr � Qu|Qp〉,
〈Qu � Qn|Qp〉, and 〈Qr � Qn|Qp〉. We empha-
size that question swapping is done only for the
groups of questions used for training; the develop-
ment and test triples are kept the same. Paraphrase
questions are seldom entirely equivalent with the
reference question. Consequently, when question
swapping is used to augment the training exam-
ples, it will inevitably introduce some noise.

3.2 External Datasets

Another approach to increasing the size of the
training set is by adding examples from other
datasets. Table 1 shows the datasets used in the
experiments in this paper, together with statis-
tics such as the number of questions groups and
the total number of questions in each category.
The DRLM dataset was introduced by Zhang et
al. (2016) and, like Complex, contains questions
posted on Yahoo! Answers. However it does not
contain paraphrases and thus cannot benefit from
question swapping. The SemEval dataset (Nakov
et al., 2016) was created from questions posted on
the Qatar Living forum and has a different distri-
bution and structure. In particular, a question has
two fields: a body containing the actual question
and a subject. The body field often contains multi-
ple sentences. In the experiments reported in Sec-

Dataset Groups P U N Triples
Complex 60 89 730 714 9979
Simple 60 134 778 621 10436
SemEval 387 372 1148 2333 7247

– Train 267 232 841 1581 4984
– Devel 50 59 155 285 1002
– Test 70 81 152 467 1261

DRLM 1478 0 6434 7747 27111

Table 1: Datasets & Statistics.

tion 4, DRLM is used as an external dataset for
training Complex and SemEval question ranking
models.

3.2.1 Weighted External Data
External triples can be very different from target
triples in terms of vocabulary, syntactic structure,
or length. As such, considering external triples
as being equally important as target triples dur-
ing training can be detrimental to the target per-
formance. To alleviate this effect, we introduce
a tunable weight hyperparameter α ∈ [0, 1] such
that target triples get a weight of α in the objective

function, whereas external triples are assigned a
weight of 1 − α, both normalized by the number
of training triples in the target (T ) and external (E)
datasets, respectively.

J(θ) =
α

|T |JT (θ) +
(1− α)
|E| JE(θ) (4)

The overall objective function is shown in Equa-
tion 4, where JT and JE are defined using the
margin-based ranking criterion from Equation 3
on the corresponding dataset.

3.2.2 Selection with Language Models
To further alleviate the potential detrimental ef-
fects due to possibly significant lexical and syntac-
tic differences between external and target triples,
we train a character-aware neural language model
(LM) (Kim et al., 2016) on the set of questions
from the target question groups used for training
and rank all external questions in ascending or-
der, based on the perplexity computed by the tar-
get LM. We introduce a tunable proportion hy-
perparameter γ and select to add only the γ|T |
triples that can be obtained from the top ranked
external questions. This procedure enables the se-
lection of triples with external questions that are
most LM-similar with the target training ques-
tions, akin to the approach proposed by Moore and
Lewis (2010) for selecting external text segments
for training language models. LM-based data aug-
mentation was also shown to benefit domain adap-
tation for tasks such as temporal expression recog-
nition (Kolomiyets et al., 2011) and semantic role
labeling (Ngoc Do et al., 2015).

4 Experimental Evaluation

We evaluate the baseline SLA approach on
the Simple and Complex datasets introduced in
(Bunescu and Huang, 2010) and compare against
their SVM approach which uses a number of man-
ually engineered features, such as similarities be-
tween focus words (tagged by another SVM),
similarities between main verbs, and matchings
between dependency graphs anchored at focus
words. The 60 groups of questions in each dataset
are partitioned into 12 folds and at each cross-
validation iteration 10 folds are used for training,
1 for development and 1 for testing. This is re-
peated 12 times such that each fold gets to be
used for testing, and the results are pooled over all
folds. The SLA model is trained with AdaDelta

444



using minibatches of size 256, and regularized us-
ing early stopping on the validation fold. Table 2
shows the triple-level accuracies i.e. the percent-
age of ranking triples

〈
Qi � Qj |Qr

〉
for which

s(Qr, Qi) > s(Qr, Qj). The results show that the

Complex Simple
SVM SLA SVM SLA
82.5 85.6 82.1 85.8

Table 2: SLA baseline accuracy vs. SVM.

SLA model is a strong baseline, as it outperforms
the SVM approach of Bunescu and Huang (2010)
that uses explicit syntactic and focus information.

Dataset Triples Accuracy
Complex 9979 85.6

+ swaps 23296 86.5
SemEval 4984 87.6

+ swaps 8606 89.1

Table 3: Accuracy, w/ or w/o swaps in training.

Table 3 shows the impact of question swapping
on the Complex and SemEval datasets, following
the official training vs. test split for SemEval. Ta-
ble 4 shows the impact of adding the entire exter-
nal dataset DRLM to the Complex and SemEval
training examples, with and without swaps. Due
to the time consuming nature of cross-validation,
for the Complex dataset we chose to test only on
1 fold, using 10 folds as training and 1 fold as
validation. Since 10 training folds amount to 50
groups of questions, we call it Complex50. We
also evaluated the impact of adding examples from
Simple when training on Complex. The test and
validation datasets are never augmented with ex-
amples generated from swaps or external datasets.
External examples helped substantially on Com-
plex, which benefited from DRLM more than from
Simple, likely because DRLM’s question groups
are many and diverse, whereas Simple contains
the same groups as Complex, but with different
questions selected as reference. Combining the
two augmentation methods resulted in further im-
provements for Complex.

However, using all DRLM examples hurt Se-
mEval performance, which was not surprising
given the substantial difference between SemEval
and DRLM questions. Consequently, we ran an
additional evaluation in which we combined the
weighted scheme from Section 3.2.1 with the LM-
based selection from Section 3.2.2. To tune the

Dataset Triples Accuracy
Complex50 8387 80.9

+ Simple 17084 86.8
+ DRLM 35498 88.9
+ swaps 19278 84.6
+ Simple + swaps 43891 87.3
+ DRLM + swaps 46389 92.1

SemEval 4984 87.6
+ DRLM 32137 86.2
+ swaps 8606 89.1

Table 4: Accuracy on Complex and SemEval, w/
and w/o training on external examples or swaps.

weight α and the proportion γ we used grid search
on the development data, where α was selected
from {0.50, 0.70, 0.85, 1.0} and γ was selected
from consecutive powers of 2 starting from 0.5
until the proportion exhausted all external triples.
Table 5 shows the result of using weighted and
LM-selected external triples, on both small (10
groups for Complex, 50 groups for SemEval) and
big (all 50 groups for Complex, all 267 groups for
SemEval) target datasets. The results now show

Dataset Triples Accuracy
Complex10 1562 73.1

+ DRLM (α = 0.85, γ = 16) 26554 87.8
Complex50 8387 80.9

+ DRLM (α = 0.70, γ = 2) 25161 85.7
SemEval50 845 80.2

+ DRLM (α = 0.50, γ = 32) 27885 85.6
SemEval267 4984 87.6

+ DRLM (α = 0.85, γ = 0.5) 7490 88.0

Table 5: Results w/ and w/o training on weighted
external triples using LM-based selection.

consistent improvements from using external data
on both Complex and SemEval, with more marked
improvements when the target dataset is small.

4.1 Multiple Sequence Structures
So far, the SemEval experiments used only the
question body (Body). To also use the subject,
one could simply concatenate the subject and the
body (Body + Subj) and apply the same SLA ar-
chitecture from Figure 1. However, as shown in
Table 6, this actually hurt performance, likely be-
cause the system did not know where the question
body started in each input sequence. To capture
the SemEval question structure, we experimented
with the architecture shown in Figure 2, in which
different RNNs are used for the subject and the
body (Body & Subj). Given that subjects are sup-
posed to be short, we implemented attention only

445



Figure 2: Subject-Body RNN architecture.

for the body sequence. In a second version, the
output from the reference subject is concatenated
to the output from the reference body, and used to
initialize the RNN for the body of the second ques-
tion (Body || Subj). The results in Table 6 show
that the new architecture improves accuracy sub-
stantially, especially the second version with con-
catenated outputs.

Body Body + Subj Body & Subj Body || Subj
87.6 87.3 91.8 92.7

Table 6: SemEval accuracy, using Body & Subj.

5 Conclusion and Future Work

We explored data augmentation methods and RNN
architectures that were shown to improve question
ranking performance. We expect these ideas to
benefit more comprehensive approaches that also
exploit answers and comments associated with
previously answered questions, as was done in the
SemEval cQA evaluation exercise (Nakov et al.,
2016). The number and breadth of some experi-
ments were limited by the available computational
power, which we hope to address in future work.

Acknowledgments

We would like to thank the anonymous reviewers
for their helpful comments. This work was sup-
ported by an allocation of computing time from
the Ohio Supercomputer Center.

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-

gio. 2015. Neural machine translation by jointly
learning to align and translate. In International Con-
ference on Learning Representations (ICLR), pages
1–15.

Delphine Bernhard and Iryna Gurevych. 2008. An-
swering learners’ questions by retrieving question

paraphrases from social Q&A sites. In EANL ’08:
Proceedings of the Third Workshop on Innovative
Use of NLP for Building Educational Applications,
pages 44–52, Morristown, NJ, USA. Association for
Computational Linguistics.

Razvan Bunescu and Yunfeng Huang. 2010. Learn-
ing the relative usefulness of questions in commu-
nity QA. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Process-
ing, pages 97–107. Association for Computational
Linguistics.

Xin Cao, Gao Cong, Bin Cui, Christian Søndergaard
Jensen, and Ce Zhang. 2009. The use of categoriza-
tion information in language models for question re-
trieval. In Proceedings of the 18th ACM Conference
on Information and Knowledge Management, pages
265–274, New York, NY, USA. ACM.

Kyunghyun Cho, Bart van Merriënboer, Çalar
Gülçehre, Dzmitry Bahdanau, Fethi Bougares, Hol-
ger Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using RNN encoder–decoder
for statistical machine translation. In Proceedings of
the 2014 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1724–
1734, Doha, Qatar. Association for Computational
Linguistics.

Huizhong Duan, Yunbo Cao, Chin-Yew Lin, and Yong
Yu. 2008. Searching questions by identifying ques-
tion topic and question focus. In Proceedings of
ACL-08: HLT, pages 156–164, Columbus, Ohio.

Yoon Kim, Yacine Jernite, David Sontag, and Alexan-
der M. Rush. 2016. Character-aware neural lan-
guage models. In Proceedings of the Thirtieth AAAI
Conference on Artificial Intelligence, AAAI’16,
pages 2741–2749. AAAI Press.

Oleksandr Kolomiyets, Steven Bethard, and Marie-
Francine Moens. 2011. Model-portability experi-
ments for textual temporal analysis. In Proceed-
ings of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language
Technologies: Short Papers - Volume 2, HLT ’11,
pages 271–276, Stroudsburg, PA, USA. Association
for Computational Linguistics.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Proceedings of NIPS 26, pages 3111–3119.

Volodymyr Mnih, Nicolas Heess, Alex Graves, and Ko-
ray Kavukcuoglu. 2014. Recurrent models of visual
attention. In Advances in Neural Information Pro-
cessing Systems 27, pages 2204–2212.

Mitra Mohtarami, Yonatan Belinkov, Wei-Ning Hsu,
Kfir Bar Yu Zhang Tao Lei, Scott Cyphers, and
James Glass. 2016. SLS at SemEval-2016 Task 3:
Neural-based approaches for ranking in community
question answering. In Proceedings of SemEval,
pages 828–835.

446



Robert C. Moore and William Lewis. 2010. Intelli-
gent selection of language model training data. In
Proceedings of the ACL 2010 Conference Short Pa-
pers, ACLShort ’10, pages 220–224, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.

Preslav Nakov, Lluı́s Màrquez, Alessandro Moschitti,
Walid Magdy, Hamdy Mubarak, abed Alhakim Frei-
hat, Jim Glass, and Bilal Randeree. 2016. SemEval-
2016 Task 3: Community question answering. Pro-
ceedings of SemEval, pages 525–545.

Quynh Thi Ngoc Do, Steven Bethard, and Marie-
Francine Moens. 2015. Domain adaptation in se-
mantic role labeling using a neural language model
and linguistic resources. IEEE/ACM Transac-
tions on Audio, Speech and Language Processing,
23(11):1812–1823.

Tim Rocktschel, Edward Grefenstette, Karl Moritz
Hermann, Tom Koisk, and Phil Blunsom. 2015.
Reasoning about entailment with neural attention.
In International Conference on Learning Represen-
tations (ICLR), pages 1–15.

Kai Wang, Zhaoyan Ming, and Tat-Seng Chua. 2009.
A syntactic tree matching approach to finding sim-
ilar questions in community-based QA services. In
Proceedings of the 32nd International ACM SIGIR
Conference on Research and Development in Infor-
mation Retrieval, pages 187–194, New York, NY,
USA. ACM.

Kelvin Xu, Jimmy Lei Ba, Ryan Kiros, Kyunghyun
Cho, Aaron Courville, Ruslan Salakhutdinov,
Richard S. Zemel, and Yoshua Bengio. 2015. Show,
attend and tell: Neural image caption generation
with visual attention. In 32nd International Con-
ference on Machine Learning, ICML 2015, pages
2048–2057.

Xiaobing Xue, Jiwoon Jeon, and W. Bruce Croft. 2008.
Retrieval models for question and answer archives.
In Proceedings of the 31st Annual International
ACM SIGIR Conference on Research and Develop-
ment in Information Retrieval, pages 475–482, New
York, NY, USA. ACM.

Kai Zhang, Wei Wu, Fang Wang, Ming Zhou, and
Zhoujun Li. 2016. Learning distributed representa-
tions of data in community question answering for
question retrieval. In Proceedings of the 9th ACM
International Conference on Web Search and Data
Mining, pages 533–542. ACM.

Guangyou Zhou, Li Cai, Jun Zhao, and Kang Liu.
2011. Phrase-based translation model for question
retrieval in community question answer archives.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies - Volume 1, pages 653–662,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.

447


