



















































Towards Incremental Learning of Word Embeddings Using Context Informativeness


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, pages 162–168
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

162

Towards incremental learning of word embeddings using context
informativeness

Alexandre Kabbach
Dept. of Linguistics
University of Geneva

Center for Mind/Brain Sciences
University of Trento

Kristina Gulordava
Dept. of Translation

and Language Sciences
Universitat Pompeu Fabra

{firstname.lastname}@{unige.ch;upf.edu;unitn.it}

Aurélie Herbelot
Center for Mind/Brain Sciences,

Dept. of Information Engineering
and Computer Science
University of Trento

Abstract

In this paper, we investigate the task of learn-
ing word embeddings from very sparse data in
an incremental, cognitively-plausible way. We
focus on the notion of informativeness, that is,
the idea that some content is more valuable
to the learning process than other. We fur-
ther highlight the challenges of online learn-
ing and argue that previous systems fall short
of implementing incrementality. Concretely,
we incorporate informativeness in a previously
proposed model of nonce learning, using it for
context selection and learning rate modulation.
We test our system on the task of learning new
words from definitions, as well as on the task
of learning new words from potentially unin-
formative contexts. We demonstrate that infor-
mativeness is crucial to obtaining state-of-the-
art performance in a truly incremental setup.

1 Introduction

Distributional semantics models such as word em-
beddings (Bengio et al., 2003; Collobert et al.,
2011; Huang et al., 2012; Mikolov et al., 2013b)
notoriously require exposure to a large amount of
contextual data in order to generate high quality
vector representations of words. This poses prac-
tical challenges when the available training data
is scarce, or when distributional models are in-
tended to mimic humans’ word learning abilities
by constructing reasonable word representations
from limited observations (Lazaridou et al., 2017).
In recent work, various approaches have been pro-
posed to tackle these problems, ranging from task-
specific auto-encoders generating word embed-
dings from dictionary definitions only (Bosc and
Vincent, 2017, 2018), to Bayesian models used for
acquiring definitional properties of words via one-
shot learning (Wang et al., 2017), or recursive neu-
ral network models making use of morphological
structure (Luong et al., 2013).

Arguing that the ideal model should rely on
an all-purpose architecture able to learn from
any amount of data, Herbelot and Baroni (2017)
proposed a model called Nonce2Vec (N2V), de-
signed as a modification of Word2Vec (W2V;
Mikolov et al., 2013b), refactored to allow in-
cremental learning. The model was tested on
two datasets: a) the newly introduced defini-
tional dataset, where the task is to learn a nonce
word from its Wikipedia definition; and b) the
chimera dataset of Lazaridou et al. (2017), where
the task is to reproduce human similarity judge-
ments related to a novel word observed in 2-6 ran-
domly extracted sentences. The N2V model per-
formed much better than W2V on both datasets
but failed to outperform a basic additive model on
the chimera dataset, leading the authors to hypoth-
esise that their system would need to perform con-
tent selection to deal with the potentially uninfor-
mative chimera sentences.

There are two motivations to the present work.
The first is to provide a formal definition of the
notion of informativeness applied to both senten-
tial context (as a whole) and context words (taken
individually). To do so, we rely on the intuition
that an informative context is a context that is more
specific to a given target, and that this notion of
context specificity can be quantified by computing
the entropy of the probability distribution gener-
ated by a language model over a set of vocabulary
words, given the context.

The secondary motivation of this work lays in
considerations over incrementality. We show that
N2V itself did not fully implement its ideal of
‘online’ concept learning. We also point out that
architectures that have outperformed N2V since
its inception actually move even further from this
ideal. In contrast, we attempt to make our architec-
ture as close as possible to a realistic belief update
system, and we demonstrate that informativeness



163

is an essential part of retaining acceptable perfor-
mance in such a challenging setting.

2 Related work

The original Nonce2Vec (N2V) model is designed
to simulate new word acquisition by an adult
speaker who already masters a substantial vocab-
ulary. The system uses some ‘background’ lexical
knowledge in the shape of a distributional space
acquired over a large text corpus. A novel word is
then learnt by using information present in its con-
text sentence. To achieve its goal, N2V proposes
some modifications to the original Word2Vec ar-
chitecture to make it suitable for novel word learn-
ing. Three main changes are suggested: a) the
learning rate should be greatly heightened to allow
for learning via backpropagation using only the
limited amount of data; b) random subsampling
should be reduced to a minimum so that all avail-
able data is used; and c) embeddings in the back-
ground should be ‘frozen’ so that the high learn-
ing rate is prevented from ‘unlearning’ old lexical
knowledge in favour of the scarce, and potentially
uninformative, new context information.

Recent work outperforms N2V on the
chimera (Khodak et al., 2018; Schick and
Schütze, 2019a) and the definitional (Schick and
Schütze, 2018) datasets (see §1 for a description
of the datasets). However, they both deviate from
the original motivation of N2V which is to learn
word representations incrementally from any
amount of data. Instead, those state-of-the-art
models rely—at least in part—on a learned linear
regression matrix which is fixed for a given corpus
and thus does not lend itself well to incremental
learning.

Our work follows the philosophy of N2V but
pushes the notion of incrementality further by
identifying aspects of the original system that in
fact do not play well with true online learning. For
a start, the original model is not fully incremental
as it adopts a one-shot evaluation setup where each
test instance is considered individually and where
the background model is reloaded from scratch at
each test iteration. This does not test how the sys-
tem would react to learning multiple nonces one
after the other (as humans do in the course of
their lives). Related to this, whilst ‘freezing’ back-
ground vectors makes sense as a safety net when
using very high learning rates, it similarly goes
against the notion of incrementality. In any re-

alistic setup, indeed, we would like newly learnt
words to inform our background lexical knowl-
edge and become part of that background them-
selves, being refined over time and contributing
to acquiring the next nonce. Following this phi-
losophy, the system we present in this paper does
away with freezing previously learnt embeddings
and does not reload the background model at each
test iteration.

We should further note that our work on in-
formativeness echoes recent research on the use
of attention mechanisms. Vaswani et al. (2017),
followed by Devlin et al. (2018) and Schick and
Schütze (2019b), have shown that such mecha-
nisms can provide very powerful tools to build
sentence and contextualised word embeddings
which are amenable to transfer learning tasks.
However, we note that from our point of view,
these systems suffer from the same problem as the
previously mentioned architectures: the underly-
ing model consists of a large set of parameters
which can be used to learn a task-specific regres-
sion. It is not designed to be updated with each
new encountered experience.

3 Model

Let us consider context to be defined as a win-
dow of ±n words around a given target. We de-
fine two specific functions: context informative-
ness (CI) which characterises how informative an
entire context is with respect to its corresponding
target; and context word informativeness (CWI)
which characterises how informative a particular
context item is with respect to the target. For in-
stance, if target chases is seen in context c = {the,
cat, the, mouse}, the context informativeness is the
informativeness of c, and the context word infor-
mativeness can be computed for each element in c,
with the expectation, in this case, that the might be
less informative than cat or mouse. The CWI mea-
sure is dependent on CI, as we proceed to show.

3.1 Context informativeness
Let us consider a sequence c of n context items
c = c1 . . . cn. We define the context informative-
ness of a context sequence c as:

CI(c) = 1 +
1

ln(|V|)
∑
w∈V

p(w|c) ln p(w|c) (1)

CI is a slight modification of the Shannon en-
tropy H = −

∑
w∈V p(w|c) ln p(w|c), normalised



164

over the cardinality of the vocabulary |V| to out-
put values in [0, 1]. In this work, we use a CBOW
model (Mikolov et al., 2013a) to obtain the proba-
bility distribution p(w|c). We use CBOW because
it is the simplest word-generation model which
takes the relation between context words into ac-
count, i.e., in contrast to skipgram.

A context will be considered maximally infor-
mative (CI = 1) if only a single vocabulary item
is predicted to occur in context c with a non-null
probability. Conversely, a context will be consid-
ered minimally informative (CI = 0) if all vocab-
ulary items are predicted to occur in context cwith
equal probability. CI should therefore quantify
how specific a given context is regarding a given
target.

3.2 Context word informativeness
Let us consider c6=i = c1 . . . ci−1, ci+1 . . . cn to
be the sequence of context items taken from c to
which the ith item has been removed. We define
the context word informativeness of a context item
ci in a context sequence c as:

CWI(ci) = CI(c)− CI(c6=i) (2)

CWI outputs values in [−1, 1]: a context word
ci will be considered maximally informative
(CWI = 1) if removing it from a maximally in-
formative context leads to a minimally informative
one. Conversely, a context word ci will be con-
sidered minimally informative (CWI = −1) if
removing it from a minimally informative context
leads to a maximally informative one.

3.3 CWI-augmented Nonce2Vec
As explained in §2, N2V introduces several high-
level changes to the W2V architecture to achieve
learning from very sparse data. In practice, this
translates into the following design choices: a)
nonces are initialised by summing context word
embeddings (after subsampling); and b) nonces
are trained with an adapted skipgram function in-
corporating decaying window size, sampling and
learning rates at each iteration, while all other vec-
tors remain frozen. The learning rate is computed
via α = α0e−λt with a high α0.

The modifications we propose are as follows:
i) we incorporate informativeness into the initial-
isation phase by summing over the set of context
words with positive CWI only; ii) we train on the
entire context without subsampling and window

decay; and iii) we remove freezing and compute
the learning rate as a function of CWI for each
context item ci via:

α(ci) = αmax
etanh(β∗CWI(ci))+1 − 1

e2 − 1
(3)

The purpose of equation 3 is to modulate the learn-
ing rate depending on the context word informa-
tiveness for a context–target pair: α should be
maximal (α = αmax, where αmax is a hyper-
parameter) when context is maximally informa-
tive (CWI = 1) and minimal (α = 0) when con-
text is minimally informative (CWI = −1). The
function x 7→ ex+1−1

e2−1 is therefore designed as a lo-
gistic “S-shape” function with domain [−1, 1] →
[0, 1]. In practice, CWI values are highly depen-
dant on the language model used and may end up
all being close to 0 (±0.01 with our CBOW model
for instance). The tanh function and the β param-
eter are therefore added to compensate for this ef-
fect that would otherwise produce identical learn-
ing rates for all target-context pairs, regardless of
CWI values.

4 Experimental setup and evaluation

To test the robustness of the results of Herbelot
and Baroni (2017), we retrain a skipgram back-
ground model with the same hyperparameters but
from the more recent Wikipedia snapshot of Jan-
uary 2019, and obtain a similar correlation ratio
on the MEN similarity dataset (Bruni et al., 2014):
ρ = 0.74 vs ρ = 0.75 for Herbelot and Baroni
(2017). Probability distributions used for com-
puting CI and CWI are generated with a CBOW
model trained with gensim (Řehůřek and Sojka,
2010) on the same Wikipedia snapshot as our skip-
gram background model, and with the same hy-
perparameters. For the CWI-based learning rate
computation, we set αmax = 1, chosen according
to α0 in the original N2V for fair comparison; and
β = 1000, chosen given min and max CWI val-
ues output by CBOW to produce tanh(β ∗ x) val-
ues distributed across [−1, 1] and apply a learning
rate αmax = 1 to maximally informative context
words.

We report results on the definitional and the
chimera datasets (see §1). The definitional dataset
contains first sentences from Wikipedia for 1000
words: e.g. Insulin is a peptide hormone pro-
duced by beta cells of the pancreatic islets, where



165

the task is to learn the nonce insulin. Evalua-
tion is performed on 300 test instances in terms
of Median Rank (MR) and Mean Reciprocal
Rank (MRR). That is, for each instance, the Recip-
rocal Rank of the gold vector (the one that would
be obtained by training standard W2V over the
entire corpus) is computed over the sorted list of
neighbours of the predicted representation.

The chimera dataset simulates a nonce situation
where speaker encounters words for the first time
in naturally-occurring (and not necessarily infor-
mative) sentences. Each nonce instance in the data
is associated with 2 (L2), 4 (L4) or 6 (L6) sen-
tences showing the nonce in context, and a set of
six word probes human-annotated for similarity
to the nonce. For instance, the nonce VALTUOR
is shown in Canned sardines and VALTUOR be-
tween two slices of wholemeal bread and thinly
spread Flora Original [...], and its similarity as-
sessed with respect to rhubarb, onion, pear, straw-
berry, limousine and cushion. Evaluation is per-
formed on 110 test instances by computing the
Spearman correlation between the similarities out-
put by the system for each nonce-probe pair and
the similarities from the human subjects.

We evaluate both datasets using a one-shot
setup, as per the original N2V paper: each nonce
word in considered individually and the back-
ground model is reloaded at each test iteration. We
further propose an incremental evaluation setup
where the background model is loaded only once
at the beginning of testing, keeping its word vec-
tors modifiable during subsequent learning, and
where each newly learned nonce representation is
added to the background model. As performance
in the incremental setup proved to be dependent
on the order of the test items, we report average
and standard deviation scores computed from 10
test runs where the test set is shuffled each time.

5 Results

5.1 Improving additive models
Herbelot and Baroni (2017) show that a simple ad-
ditive model provides an extremely strong base-
line for nonce learning. So we first measure the
contribution of our notion of informativeness to
the context filtering module of a sum model. Com-
parison takes place across four settings: a) no
filter, where all words are retained; b) random,
which applies standard subsampling with a sam-
ple rate of 10,000, following the original N2V ap-

proach; c) self, where all items found in training
with a frequency above a given threshold are dis-
carded;1 and d) CWI, which only retains context
items with a positive CWI value.

Our results on the definitional dataset, displayed
in Table 1, show a consistent hierarchy of filters
with the SUM CWI model outperforming all other
SUM models, in both one-shot and incremental
evaluation setups. Results on the chimera dataset,
displayed in Table 2, are not as clear-cut, although
they do exhibit a similar trend on both L4 and
L6 test sets, with the notable result of achieving
state-of-the-art performances with our SUM CWI
model on the L4 and L6 test sets in incremental
setup, and near state-of-the-art performance on the
L6 test set in one-shot setup. This confirms once
again that additive models can provide very robust
baselines.

Qualitatively, the contribution of each filter on
the definitional dataset can be exemplified on
the following sentence, with nonce word Honey-
well: “Honeywell International Inc is an Amer-
ican multinational conglomerate company that
produces [...] aerospace systems for a wide va-
riety of customers from private consumers to ma-
jor corporations and governments.”. The no-filter
additive model outputs a rank of 383 (the gold vec-
tor for honeywell is found to be the 383th clos-
est neighbour of the predicted vector). The ran-
dom model randomly removes most (but not all)
high frequency words before summing, outputting
a rank of 192 (filtered-out words include also con-
tent words like international or company). The
self -information model reduces the size of the
context words set even further by removing all
high-frequency words left over by the random pro-
cess (rank 170). Finally, the CWI model outputs
the best rank at 85, removing all function words
while keeping some useful high-frequency words
such as international or company.

5.2 Improving neural models in one-shot
settings

Our results for neural models are also displayed
in Table 1 and Table 2: as-is refers to the orig-
inal N2V system; CWI init is N2V as-is with
CWI-based context filtering instead of subsam-
pling; and CWI alpha is a model with a CWI-based

1We take the log of the sample int values computed by
gensim for each word during training, keeping only items
with log values above 22, which gave us the best perfor-
mances overall.



166

one-shot incremental
Model MR MRR MR MRR

SOTA 49 .1754 – –

SUM no-filter 5,969 .0087 6,461± 225 .0014±.0002
SUM random 3,047 .0221 3,113± 179 .0071±.0012
SUM self 1,769 .0242 2,095±125 .0121±.0008
SUM CWI 935 .0374 961±24 .0322±.0011

N2V as-is 955 .0477 81,705±14,076 .0096±.0038
N2V CWI init 540 .0493 70,992±17,312 .0079±.0025
N2V CWI alpha 763 .0404 983±175 .0341±.0021

Table 1: Performance of various additive (SUM) and neural (N2V) models on the definitional dataset, measured
in terms of Median Rank (MR) and Mean Reciprocal Rank (MRR). SOTA in one-shot evaluation setup is reported
by the Form-Context model of Schick and Schütze (2018).

sum initialisation (as in SUM CWI), and a CWI-
based learning rate computed on unfiltered context
words, as detailed in §3.3.

When informativeness is incorporated to N2V
in the original one-shot evaluation setup, we also
observe near-systematic improvements. On the
definitional dataset in Table 1, CWI init improves
over the standard N2V as-is model (MR 540 vs
955; MRR .0493 vs .0477) or over the SUM CWI
baseline (MR 540 vs 935; MRR .0493 vs .0374).
In comparison to CWI init, our CWI alpha model
provides robust performances across evaluation
setups and datasets, often reaching similar if not
better results than our best baseline model (SUM
CWI) showing that a neural model fully based on
informativeness is a more robust alternative than
its counterparts. See for example Table 1 on the
definitional dataset where the N2V CWI alpha
model performs better than the SUM CWI model
in one-shot setup (MR 763 vs. 935; MRR .0404
vs .0374) or Table 2 on the chimera dataset where
it also performs better than the SUM CWI model
on both the L2 (ρ .3129 vs .3074) and the L4 (ρ
.3928 vs .3739) test sets and achieves state-of-the-
art performance on the L4 test set.

5.3 Improving incremental learning
As stated in §2, recent approaches to nonce learn-
ing have deviated from the original philosophy of
N2V and in fact, N2V itself did not fully imple-
ment an incremental setting. We now show that
the original N2V performance decreases signifi-
cantly on both datasets in an incremental evalu-
ation setup, without freezing of background vec-

tors. Compare the results of the N2V as-is model
in both one-shot and incremental evaluation setups
on the definitional dataset in Table 1: MR 955 vs
81,705±14,076 and MRR .0477 vs .0096±.0038;
and on the chimera dataset in Table 2: ρ .3412 vs
.1650±.0384 on L2; ρ .3514 vs .1144±.0620 on
L4 and ρ .4077 vs .1391±.0694 on L6. We find
this drastic decrease in performance to be related
to two distinct phenomena: 1) a sum effect which
leads vector representations for nonces to be close
to each other due to the sum initialisation creat-
ing very similar vectors in a ‘special’ portion of
the vector space; and 2) a snowball effect related
to the ‘unfreezing’ of the background space which
leads background vectors to be updated by back-
propagation at a very high learning rate at every
test iteration, moving their original meaning to-
wards the semantics of the new context they are
encountered in. This includes vectors for very fre-
quent words, which are encountered again in their
now shifted version when a new nonce is presented
to the system. This snowball effect ends up sig-
nificantly altering the quality of the background
model and its generated representations.

The sum effect is best illustrated by the decrease
in performance of SUM models between one-shot
and incremental setups on the definitional dataset
in Table 1, as this effect has the property of specif-
ically changing the nature and order of the nearest
neighbours of the predicted nonce vectors, which
is directly reflected in the MR and MRR evalua-
tion metrics on the definitional dataset given the
evaluation task. On the chimera dataset in Ta-
ble 2 however, this effect does not appear to neg-



167

one-shot incremental
Model L2 L4 L6 L2 L4 L6

SOTA .3634 .3844 .4360 – – –

SUM no-filter .3047 .3288 .3063 .3047±.0000 .3288±.0000 .3063±.0000
SUM random .3358 .3717 .3584 .3358±.0002 .3717±.0004 .3584±.0003
SUM self .3455 .3638 .3651 .3455±.0000 .3638±.0000 .3651±.0000
SUM CWI .3074 .3739 .4243 .3074±.0000 .3739±.0000 .4243±.0000

N2V as-is .3412 .3514 .4077 .1650±.0384 .1144±.0620 .1391±.0694
N2V CWI init .3002 .3482 .4218 .1451±.0265 .1522±.0396 .1225±.0544
N2V CWI alpha .3129 .3928 .4181 .2970±.0262 .3000±.0268 .2678±.0408

Table 2: Performance of various additive (SUM) and neural (N2V) models on the chimera dataset, measured in
terms of Spearman correlation. SOTA in one-shot evaluation setup on the L2 and L4 test sets are reported by the A
la carte model of Khodak et al. (2018), while SOTA on the L6 test set is reported by the attentive mimicking model
of Schick and Schütze (2019a).

atively impact performance given that evaluation
compares correlations between gold and predicted
similarity rankings of nonces with a prefixed set
of probes. The snowball effect however is visible
on both datasets in Table 1 and Table 2 when com-
paring performances of N2V models between one-
shot and incremental setups. It proves particularly
salient for neural models which do not make use of
informativeness-based adaptative learning rate (all
N2V models but N2V CWI alpha).

Our notion of informativeness proves even more
useful in the context of incremental nonce learn-
ing: on the definitional dataset in Table 1, our
informativeness-based models, be it SUM CWI or
N2V CWI alpha, achieve best (and comparable)
performances (MR 961±24 vs 983±175; MRR
.0322±.0011 vs .0341±.0021). Moreover, we ob-
serve that those models are able to mitigate the un-
desirable effects mentioned above, almost totally
for the sum effect (compare the performance of
the SUM CWI model in Table 1 between the incre-
mental and one-shot setups, versus the other SUM
models), and partially for the snowballing interfer-
ence of the high learning rate (compare the perfor-
mance of the N2V CWI alpha model in Table 1 be-
tween the incremental and one-shot setups, versus
the other N2V models). Performances of our SUM
CWI and N2V CWI alpha models in incremental
setup approach those of the one-shot setting. On
the chimera dataset in Table 2, which proves only
sensitive to the snowball effect, we also observe
that our N2V CWI alpha model is able to mitigate
this effect, although performances of the model in

incremental setup remain below those of the one-
shot setup, as well as those of the additive models
in incremental setup.

6 Conclusion

We have proposed an improvement of the orig-
inal N2V model which incorporates a notion of
informativeness in the nonce learning process.
We showed that our informativeness function was
very beneficial to a vector addition baseline, and
could be usefully integrated into the original N2V
approach, both at initialisation stage and during
learning, achieving state-of-the-art results on the
chimera dataset and on the definitional dataset in
an incremental evaluation setup. Although our
proposed notion of informativeness proved to be
mostly beneficial to incremental learning, noth-
ing prevents it from being incorporated to other
non-incremental models of nonce learning, pro-
vided that those models make use of contextual
information. On top of the performance improve-
ments observed, our proposed definition of infor-
mativeness benefits from being intuitive, debug-
gable at each step of the learning process, and
of relying on no external resource. We make
our code freely available at https://github.
com/minimalparts/nonce2vec.

Acknowledgements

We are grateful to our two anonymous reviewers
for their helpful comments.

https://github.com/minimalparts/nonce2vec
https://github.com/minimalparts/nonce2vec


168

References
Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and

Christian Janvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3:1137–1155.

Tom Bosc and Pascal Vincent. 2017. Learning word
embeddings from dictionary definitions only. In
Proceedings of the NIPS 2017 Workshop on Meta-
Learning.

Tom Bosc and Pascal Vincent. 2018. Auto-encoding
dictionary definitions into consistent word embed-
dings. In Proceedings of the 2018 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1522–1532, Brussels, Belgium. Associ-
ation for Computational Linguistics.

Elia Bruni, Nam-Khanh Tran, and Marco Baroni. 2014.
Multimodal distributional semantics. Journal of Ar-
tificial Intelligence Research, 49:1–47.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research,
12:2493–2537.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. BERT: pre-training of
deep bidirectional transformers for language under-
standing. CoRR, abs/1810.04805.

Aurélie Herbelot and Marco Baroni. 2017. High-risk
learning: acquiring new word vectors from tiny data.
In Proceedings of the 2017 Conference on Empiri-
cal Methods in Natural Language Processing, pages
304–309, Copenhagen, Denmark. Association for
Computational Linguistics.

Eric Huang, Richard Socher, Christopher Manning,
and Andrew Ng. 2012. Improving word represen-
tations via global context and multiple word proto-
types. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 873–882, Jeju Island,
Korea. Association for Computational Linguistics.

Mikhail Khodak, Nikunj Saunshi, Yingyu Liang,
Tengyu Ma, Brandon Stewart, and Sanjeev Arora.
2018. A la carte embedding: Cheap but effective
induction of semantic feature vectors. In Proceed-
ings of the 56th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 12–22, Melbourne, Australia. Associa-
tion for Computational Linguistics.

Angeliki Lazaridou, Marco Marelli, and Marco Baroni.
2017. Multimodal word meaning induction from
minimal exposure to natural text. Cognitive Science,
41(S4):677–705.

Thang Luong, Richard Socher, and Christopher Man-
ning. 2013. Better word representations with recur-
sive neural networks for morphology. In Proceed-
ings of the Seventeenth Conference on Computa-
tional Natural Language Learning, pages 104–113,
Sofia, Bulgaria. Association for Computational Lin-
guistics.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space. CoRR, abs/1301.3781.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeffrey Dean. 2013b. Distributed repre-
sentations of words and phrases and their composi-
tionality. In Proceedings of the 26th International
Conference on Neural Information Processing Sys-
tems - Volume 2, NIPS’13, pages 3111–3119, USA.
Curran Associates Inc.

Radim Řehůřek and Petr Sojka. 2010. Software Frame-
work for Topic Modelling with Large Corpora. In
Proceedings of the LREC 2010 Workshop on New
Challenges for NLP Frameworks, pages 45–50, Val-
letta, Malta. ELRA.

Timo Schick and Hinrich Schütze. 2018. Learning se-
mantic representations for novel words: Leveraging
both form and context. CoRR, abs/1811.03866.

Timo Schick and Hinrich Schütze. 2019a. Attentive
mimicking: Better word embeddings by attending
to informative contexts. CoRR, abs/1904.01617.

Timo Schick and Hinrich Schütze. 2019b. Rare words:
A major problem for contextualized embeddings
and how to fix it by attentive mimicking. CoRR,
abs/1904.06707.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in neural information pro-
cessing systems, pages 5998–6008.

Su Wang, Stephen Roller, and Katrin Erk. 2017. Distri-
butional Modeling on a Diet: One-shot Word Learn-
ing from Text Only. In Proceedings of the Eighth In-
ternational Joint Conference on Natural Language
Processing (Volume 1: Long Papers), pages 204–
213. Asian Federation of Natural Language Process-
ing.

http://dl.acm.org/citation.cfm?id=944919.944966
http://dl.acm.org/citation.cfm?id=944919.944966
http://metalearning.ml/2017/papers/metalearn17_bosc.pdf
http://metalearning.ml/2017/papers/metalearn17_bosc.pdf
https://www.aclweb.org/anthology/D18-1181
https://www.aclweb.org/anthology/D18-1181
https://www.aclweb.org/anthology/D18-1181
https://www.jair.org/index.php/jair/article/view/10857
http://dl.acm.org/citation.cfm?id=1953048.2078186
http://dl.acm.org/citation.cfm?id=1953048.2078186
http://arxiv.org/abs/1810.04805
http://arxiv.org/abs/1810.04805
http://arxiv.org/abs/1810.04805
https://www.aclweb.org/anthology/D17-1030
https://www.aclweb.org/anthology/D17-1030
http://www.aclweb.org/anthology/P12-1092
http://www.aclweb.org/anthology/P12-1092
http://www.aclweb.org/anthology/P12-1092
https://www.aclweb.org/anthology/P18-1002
https://www.aclweb.org/anthology/P18-1002
https://onlinelibrary.wiley.com/doi/full/10.1111/cogs.12481
https://onlinelibrary.wiley.com/doi/full/10.1111/cogs.12481
http://www.aclweb.org/anthology/W13-3512
http://www.aclweb.org/anthology/W13-3512
http://arxiv.org/abs/1301.3781
http://arxiv.org/abs/1301.3781
http://dl.acm.org/citation.cfm?id=2999792.2999959
http://dl.acm.org/citation.cfm?id=2999792.2999959
http://dl.acm.org/citation.cfm?id=2999792.2999959
http://is.muni.cz/publication/884893/en
http://is.muni.cz/publication/884893/en
http://arxiv.org/abs/1811.03866
http://arxiv.org/abs/1811.03866
http://arxiv.org/abs/1811.03866
http://arxiv.org/abs/1904.01617
http://arxiv.org/abs/1904.01617
http://arxiv.org/abs/1904.01617
http://arxiv.org/abs/1904.06707
http://arxiv.org/abs/1904.06707
http://arxiv.org/abs/1904.06707
https://arxiv.org/abs/1706.03762
https://arxiv.org/abs/1706.03762
http://aclweb.org/anthology/I17-1021
http://aclweb.org/anthology/I17-1021
http://aclweb.org/anthology/I17-1021

