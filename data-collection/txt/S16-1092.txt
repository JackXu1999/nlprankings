



















































USFD at SemEval-2016 Task 1: Putting different State-of-the-Arts into a Box


Proceedings of SemEval-2016, pages 609–613,
San Diego, California, June 16-17, 2016. c©2016 Association for Computational Linguistics

USFD at SemEval-2016 Task 1: Putting different
State-of-the-Arts into a Box

Ahmet Aker, Frederic Blain, Andres Duque∗, Marina Fomicheva+,
Jurica Seva, Kashif Shah and Daniel Beck

University of Sheffield, UK
University of Madrid, Spain∗

Universitat Pompeu Fabra, Spain+

ahmet.aker@ f.blain@ j.seva@ kashif.shah@
sheffield.ac.uk

andres.duq.fer@gmail.com, marina.fomicheva@upf.edu
debeck1@sheffield.ac.uk

Abstract

In this paper we describe our participa-
tion in the STS Core subtask which is the
determination of the monolingual seman-
tic similarity between pair of sentences.
In our participation we adapted state-of-
the-art approaches from related work ap-
plied on previous STS Core subtasks and
run them on the 2016 data. We inves-
tigated the performance of single meth-
ods but also the combination of them.
Our results show that Convolutional Neu-
ral Networks (CNN) are superior to both
the Monolingual Word Alignment and the
Word2Vec approaches. The combination
of all the three methods performs slightly
better than using CNN only. Our results
also show that the performance of our
systems varies between the datasets.

1 Introduction
Semantic Textual Similarity (STS) is a metric
which aims to determine the likeness between
two short textual entities. Therefore, STS is
widely used in many research areas such as Nat-
ural Language Processing, for a large amount
of tasks like Information Retrieval (IR), Natural
Language Understanding (NLU) or even Ma-
chine Translation (MT) evaluation, for which

STS allows capturing more information than
traditional metrics based on n-grams match like
BLEU (Papineni et al., 2002).

Part of the SemEval campaign, STS compe-
tition benefits from a growing interest over the
year (Agirre et al., 2012; Agirre et al., 2013;
Agirre et al., 2014; Agirrea et al., 2015). In
2016, participants had the possibility to com-
pete in two tasks: (i) STS split into ’STS Core’
and ’Cross-lingual STS’ which are respectively
an English monolingual and English/Spanish
bilingual subtasks; (ii) iSTS which focuses on
the interpretable aspect of STS assessment. In-
troduced for the first time in 2015, iSTS became
a standalone task this year.

This paper describes our participation in the
STS Core subtask and is organised as follows:
we first describe in Section 2 methods we have
adapted to tackle the task. Next, we present and
discuss our results (Section 3). Finally, Sec-
tion 4 will conclude this system description pa-
per with some remarks.

2 System description

For adressing monolingual semantic similarity
between two sentences different methods have
been proposed in related work. In our mono-
lingual STS participation we have focused on
some methods that were reported as state-of-

609



the-art systems. Given that those methods have
been reported separately a natural question that
arises from this is what the performance is when
those methods are combined. This is exactly
what we did and what we propose in this pa-
per. We adapted methods from related work that
have been applied on the monolingual STS task
and used their combined version on the new
2016 monolingual task. Methods adapted and
the strategy we used to combine them are de-
scribed in the following sections.

2.1 Monolingual Alignment
We use an alignment-based approach, which
was among the top-perfoming submissions in
the past year’s task (Sultan et al., 2015). Sen-
tence similarity score is computed in two sepa-
rate steps. First, an alignment between related
words in the input sentences is established us-
ing Monolingual Word Aligner (MWA) (Sultan
et al., 2014). Next, sentence similarity is calcu-
lated based on the proportion of aligned content
words:

sim(S1, S2) =
nac(S

1) + nac(S
2)

nc(S1) + nc(S2)
(1)

where nc(Si) and nac(S
i) are the number of

content words and the number of aligned con-
tent words in sentence i, respectively. MWA
makes alignment decisions based on lexical
similarity and contextual evidence. Lexical
similarity component identifies word pairs that
are possible candidates for alignment. Context
words are considered as evidence for alignment
if they are lexically similar and have the same or
equivalent syntactic relations with the words to
be aligned. Word pairs are aligned in decreas-
ing order of a weighted sum of their lexical and
contextual similarity.

2.2 Convolutional Neural Network Score
Another method adapted in our system is a Con-
volutional Neural Network (CNN), which gen-
erates a similarity score for each pair of sen-

tences. More specifically, we replicated the sys-
tem presented in (He et al., 2015), using previ-
ous SemEval data for training the network and
generating similarity values between 0 and 5,
for each of the test sentences given by the orga-
nizers. In the following subsections, we briefly
summarize the CNN system, although specific
details can be found in the cited paper.

Word Representation
Words in the sentences to be compared are

first transformed into vectors using the GloVe
word embeddings (Pennington et al., 2014).
These embeddings are trained on 840 bil-
lion tokens, and the resulting vectors are 300-
dimensional. Hence, each sentence sent with
n words will be transformed into a matrix
Msent ∈ Rn×300. Hence, senti:j denotes the
word embeddings of words i to j inside the
sentence, sent[k]i denotes the k-th dimension of
word embedding i and sent[k]i:j the k-th dimen-
sion of words i to j inside the sentence.

Sentence modelling
The technique makes use of two different

types of filters for extracting features from the
sentences: holistic and per-dimension. Holis-
tic filters generate a vector representing a ”tem-
poral” convolution, this is, complete regions of
the word sequence. On the other hand, per-
dimension filters perform spatial convolutions,
limited to a predefined dimension k. After the
application of those filters, the last step of the
convolutional layer is to perform pooling op-
erations over the vectors generated by the fil-
ters, in order to convert those vectors into scalar
values. The system defines three types of pool-
ing: max, min and mean. Finally, the system
also defines a group as a specific convolution
layer (with either holistic or per-dimension fil-
ters) with width ws and a specific type of pool-
ing, which operates over a sentence. A set of
groups composed by convolution layers with
the same width which explore different pooling
functions is called a block.

610



Similarity Measurement Layer
Once that the sentence representation is done

through the use of filters and pooling functions,
a way to compare two sentences has to be de-
fined. The comparison of two different sen-
tences is made over local regions of the sentence
representation. For this purpose, the system
consider the output of the convolutional layers
in order to perform both “horizontal” and “ver-
tical” comparisons. Given two vectors, each of
them representing the same region of an input
sentence, a new output vector is created by mea-
suring cosine distance, L2 Euclidean distance
and element-wise absolute difference. This out-
put vector is added to an accumulated vector
which stores the outputs of all the compared
regions of the input sentences. The final out-
put of the system generates an output similarity
score through a final log-softmax layer, which
receives the accumulated vector. System pa-
rameters (window size, number of filters, learn-
ing rate, regularization parameter, hidden units)
are maintained with respect to the original work
in which the system is presented.

2.3 Word2Vec
Word embeddings using Word2vec (Mikolov et
al., 2013) have been extensively used to mea-
sure the semantic similarity between words.
Our word embeddings comprise the vectors
published by Baroni et al. (2014). To measure
the similarity between a pair of sentences we
first remove from each sentence stop-words as
well as punctuations, query for each word its
vector representation and create a averaged sum
of the word vectors. The number of remaining
words in each sentence is used to average that
sentence. Finally, we use the resulting averaged
sum vectors and determine their similarity us-
ing cosine.

2.4 Model Combination
In this section, we present the experiments to
combine all methods described in previous sec-
tions. We formulated the problem as a regres-

sion task where we are given multiple features
capturing different attributes of inputs along
with gold labels and we predict the output for
unseen examples. Support Vector Regression
(SVR) (Chang and Lin, 2001) is the most com-
monly used algorithm for such tasks. We used
the 3 features described in previous sections and
trained SVR models to estimate a continuous
score within [0,5]. We evaluated different set-
tings of these models including various avail-
able kernels. Based on optimum performance,
we used a radial basis function (RBF) kernel,
which has been shown to perform very well in
quality estimation tasks (Callison-Burch et al.,
2012). Kernel parameters are optimised using
grid search with 5-fold cross-validation. The
correlation scores using SVR as learning algo-
rithm are reported in Table 3. It can be seen that
adding all features together improves the results
in as compared to the individual scores for each
of the methods.

Application on SemEval Data
The described system has been trained with

data from previous Semantic Textual Similarity
tasks of SemEval, more specifically data from
2012, 2013, 2014 and 2015. The final training
dataset is composed by 13,560 sentence pairs.

3 Results

In SemEval STS Core Task the performance
of each method (or system) is evaluated us-
ing Pearson Correlation that measures the lin-
ear correlation between the system’s outputs
and gold-standard data. A score of 1 indicates
100% correlation and 0 no correlation at all. Ta-
ble 3 shows the individual results for methods
adapted in this work as well as the result when
all methods are combined together as a single
system.

We can first observe that the best single
performing method is CNN, with an overral
achievement of 0.727 correlation score (see last
column). The other two approaches achieve

611



Method answer-
answer

headlines plagiarism postediting question-
question

OverALL

CNN 0.510 0.818 0.834 0.792 0.685 0.727
MWA 0.436 0.704 0.749 0.587 0.579 0.611
Word2vec 0.276 0.642 0.787 0.750 0.688 0.622
Combination 0.508 0.820 0.838 0.794 0.689 0.728

Table 1: Pearson correlation between the prediction and gold standard data.

substantially lower correlation figures with a
drop of about 0.10. Monolingual Word Align-
ment is the less performing method achieving
0.611, just behind Word2Vec with 0.622. From
our results we can also remark that the com-
bined version of all the three methods lead to
0.728 which is similar to the CNN method only.

Secondly, if we look at the results for each
dataset individually, we can also remark that the
performance of our system drastically change
from one to another. While all approaches per-
form poorly on the answer-answer data, they
achieve high correlation scores on the head-
lines, post-editing and plagiarism data sets. The
latter being the data on which our approaches
are the most efficient with a minimum of 0.749
correlation.

4 Conclusions and further studies

In this paper we described our participation to
the STS Core Task for SemEval 2016. We
adapted state-of-the-art methods from previ-
ous studies applied on the monolingual seman-
tic similarity task and run them on the 2016
data. We investigated the performance of sin-
gle methods individually but also combined al-
together.

Overall, our results show that the CNN-based
approach was the most effective compared to
the others individual approaches. The combi-
nation of those methods achieves slightly better
results than CNN only. We can also observe that
results vary between the different dataset: they
are highly satisfactory on both the headlines and
plagiarism dataset, but perform poorly on the
answer-answer data, especially Word2Vec with

only 0.276 Pearson Correllation compared to
gold-standard.

For the future work we aim to conduct a
deeper analysis about the performance of our
different systems. This will also include the un-
derstanding concerning their effectiveness over
on diverse datasets. Finally, we would like to
investigate the QuEst framework (Specia et al.,
2013) as additional method to describe the se-
mantic similarity between sentences:

QuEst framework

For our future work we aim to use the QuEst
framework (Specia et al., 2013) and extract
features to capture the semantic similarity be-
tween monolingual sentences. These features
are used and have shown to perform well in the
WMT shared tasks on QE. They include simple
counts, e.g. number of tokens in the segments,
language model probabilities and perplexities,
number of punctuation marks in source and tar-
get segments, among other features reflecting
the complexity and fluency of the given seg-
ments. Though these features are originally de-
signed to estimate the quality of machine trans-
lation, we aim to adapt and explore their poten-
tial in addition to the methods discussed in pre-
vious sections.

Acknowledgements

The research leading to these results has re-
ceived funding from the EU - Seventh Frame-
work Program (FP7/2007-2013) under grant
agreement n610916 SENSEI.

612



References
Eneko Agirre, Mona Diab, Daniel Cer, and Aitor

Gonzalez-Agirre. 2012. Semeval-2012 task 6:
A pilot on semantic textual similarity. In Pro-
ceedings of the First Joint Conference on Lexi-
cal and Computational Semantics-Volume 1: Pro-
ceedings of the main conference and the shared
task, and Volume 2: Proceedings of the Sixth
International Workshop on Semantic Evaluation,
pages 385–393. Association for Computational
Linguistics.

Eneko Agirre, Daniel Cer, Mona Diab, Aitor
Gonzalez-Agirre, and Weiwei Guo. 2013. sem
2013 shared task: Semantic textual similarity, in-
cluding a pilot on typed-similarity. In In* SEM
2013: The Second Joint Conference on Lexical
and Computational Semantics. Association for
Computational Linguistics. Citeseer.

Eneko Agirre, Carmen Banea, Claire Cardie, Daniel
Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei
Guo, Rada Mihalcea, German Rigau, and Janyce
Wiebe. 2014. Semeval-2014 task 10: Multi-
lingual semantic textual similarity. In Proceed-
ings of the 8th international workshop on seman-
tic evaluation (SemEval 2014), pages 81–91.

Eneko Agirrea, Carmen Baneab, Claire Cardiec,
Daniel Cerd, Mona Diabe, Aitor Gonzalez-
Agirrea, Weiwei Guof, Inigo Lopez-Gazpioa,
Montse Maritxalara, Rada Mihalceab, et al.
2015. Semeval-2015 task 2: Semantic textual
similarity, english, spanish and pilot on inter-
pretability. In Proceedings of the 9th inter-
national workshop on semantic evaluation (Se-
mEval 2015), pages 252–263.

Marco Baroni, Georgiana Dinu, and Germán
Kruszewski. 2014. Don’t count, predict! a
systematic comparison of context-counting vs.
context-predicting semantic vectors. In ACL (1),
pages 238–247.

Chris Callison-Burch, Philipp Koehn, Christof
Monz, Matt Post, Radu Soricut, and Lucia Specia,
editors. 2012. Proceedings of the Seventh Work-
shop on Statistical Machine Translation. Asso-
ciation for Computational Linguistics, Montréal,
Canada, June.

Chih-Chung Chang and Chuan-bi Lin. 2001. Train-

ing v-support vector classifiers: theory and algo-
rithms. Neural computation, 13(9):2119–2147.

Hua He, Kevin Gimpel, and Jimmy Lin. 2015.
Multi-perspective sentence similarity modeling
with convolutional neural networks. In Pro-
ceedings of the 2015 Conference on Empirical
Methods in Natural Language Processing, pages
1576–1586.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S
Corrado, and Jeff Dean. 2013. Distributed rep-
resentations of words and phrases and their com-
positionality. In Advances in neural information
processing systems, pages 3111–3119.

Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. BLEU: a method for auto-
matic evaluation of machine translation. In Pro-
ceedings of the 40th Annual Meeting on Associ-
ation for Computational Linguistics, pages 311–
318, Juillet.

Jeffrey Pennington, Richard Socher, and Christo-
pher D Manning. 2014. Glove: Global vectors
for word representation. In EMNLP, volume 14,
pages 1532–1543.

Lucia Specia, Kashif Shah, Jose G. C. De Souza,
Trevor Cohn, and Fondazione Bruno Kessler.
2013. Quest - a translation quality estimation
framework. In In Proceedings of the 51th Con-
ference of the Association for Computational Lin-
guistics (ACL), Demo Session.

Md Arafat Sultan, Steven Bethard, and Tamara Sum-
ner. 2014. Back to basics for monolingual align-
ment: Exploiting word similarity and contextual
evidence. Transactions of the Association for
Computational Linguistics, 2:219–230.

Md Arafat Sultan, Steven Bethard, and Tamara Sum-
ner. 2015. Dls@ cu: Sentence similarity from
word alignment and semantic vector composition.
In Proceedings of the 9th International Workshop
on Semantic Evaluation, pages 148–153.

613


