



















































At the Lower End of Language—Exploring the Vulgar and Obscene Side of German


Proceedings of the Third Workshop on Abusive Language Online, pages 119–128
Florence, Italy, August 1, 2019. c©2019 Association for Computational Linguistics

119

At the Lower End of Language—
Exploring the Vulgar and Obscene Side of German

Elisabeth Eder Ulrike Krieg-Holz
Institut für Germanistik

Alpen-Adria-Universität Klagenfurt, Klagenfurt, Austria
{elisabeth.eder | ulrike.krieg-holz}@aau.at

Udo Hahn
Jena University Language & Information Engineering (JULIE) Lab

Friedrich-Schiller-Universität Jena, Jena, Germany
udo.hahn@uni-jena.de

Abstract

In this paper, we describe a workflow for the
data-driven acquisition and semantic scaling
of a lexicon that covers lexical items from the
lower end of the German language register—
terms typically considered as rough, vulgar or
obscene. Since the fine semantic represen-
tation of grades of obscenity can only inad-
equately be captured at the categorical level
(e.g., obscene vs. non-obscene, or rough vs.
vulgar), our main contribution lies in applying
best-worst scaling, a rating methodology that
has already been shown to be useful for emo-
tional language, to capture the relative strength
of obscenity of lexical items. We describe the
empirical foundations for bootstrapping such a
low-end lexicon for German by starting from
manually supplied lexicographic categoriza-
tions of a small seed set of rough and vul-
gar lexical items and automatically enlarging
this set by means of distributional semantics.
We then determine the degrees of obscenity
for the full set of all acquired lexical items
by letting crowdworkers comparatively assess
their pejorative grade using best-worst scaling.
This semi-automatically enriched lexicon al-
ready comprises 3,300 lexical items and incor-
porates 33,000 vulgarity ratings. Using it as a
seed lexicon for fully automatic lexical acqui-
sition, we were able to raise its coverage up to
slightly more than 11,000 entries.

1 Introduction

With the rapid diffusion of social media in our
daily lives, we currently experience (and many of
us foster) a fundamental change of social commu-
nication habits. A main feature of this new era is
an unprecedented degree of public exposure and
visibility of individuals via (very) large and inten-
tionally open networks of “friends” or “followers.”
Blogs, chat rooms and online fora constitute even

looser connected social networks with lots of per-
sonally weakly acquainted or even unknown in-
terlocutors engaged in digital discourses. Unfor-
tunately, the chance for malicious interactions is
promoted by the sheer mass of players in these
networks and easy ways of hiding real individ-
ual identities via nick names or technically slightly
more advanced means of camouflage, such as fake
Web identities, including non-benevolent software
agents and chatbots (McIntire et al., 2010).

These promiscuous communication groups face
a high risk of anti-social behavior by aggres-
sive, ruthless or entirely hostile actors (Dad-
var et al., 2014; Wester et al., 2016; Li et al.,
2017b; Talukder and Carbunar, 2018). The phe-
nomena encountered range from (political, reli-
gious, ethnic, sexual) harassment, flaming, cy-
bertrolling, and cyberbullying to extremely eval-
uative (derogatory, hurtful, rough, rude, offensive,
abusive, vulgar, taboo, obscene) language use (for
a typological clarification attempt, cf. Waseem
et al. (2017)).

NLP research has recently directed its attention
towards these unwarranted effects of social me-
dia activities and targets the automatic recognition
of toxic language for the purpose of alerting and
warning (Huang et al., 2018), filtering and block-
ing (Yoon et al., 2010; Ghauth and Sukhur, 2015;
Chernyak, 2017; Wu et al., 2018), or reformulating
suspicious contents of this type by non-obtrusive
paraphrases (Su et al., 2017; Nogueira dos Santos
et al., 2018).

Yet, how can we distinguish sloppy colloquial
language we all use here and there from explicitly
abusive and inacceptable wording, the topic we
focus on in this paper, i.e., the kind of linguistic
behavior typically socially banned from civilized
discourse?



120

The standard way to deal with this challenge
is to define category systems (binary ones, such
as obscene vs. non-obscene, or staged ones, as il-
lustrated by pejorative vs. rough vs. vulgar) and
letting people decide on the assignment of lexi-
cal items to these discrete categories. Once such
categorical features are available, these lexical re-
sources can be exploited for analytic purposes.
Traditionally, these decisions were made by few
lexicographers but this approach suffers from sub-
jectivity and lack of flexibility, since this lexicon
of improper words is rapidly growing due to the
productiveness of language and thus changing al-
most every day.

Alternatively, a larger number of crowdworkers
can be hired to provide such category assignments
which increases the level of objectivity (on the ba-
sis of inter-worker consensus) and currency (cam-
paigns can be run without delay, on demand, with
low budgets). Yet, crowdsourced assessments, as
with lexicographers’ judgments, inherently suffer
from the problems of permeable and soft category
boundaries—what is rough for one person may be
vulgar for another and vice versa.

We challenge the established view that the rep-
resentation of obscenity of language is a dis-
crete categorical classification problem—no mat-
ter which category system is chosen—but rather
assume that it is a matter of differential degree.
Accordingly, we describe the empirical founda-
tions for bootstrapping and scaling such a lexicon
from the low end of stylistic conventions on de-
grees of obscenity. We start from expert-level lex-
icographic categorizations of a small set of pejora-
tive/rough/vulgar lexical items, enlarge this set by
distributional semantics methods and, then, deter-
mine the degree of obscenity of the items assem-
bled this way by letting crowdworkers make indi-
vidual assessments relative to the semantic poles
“neutral” and “vulgar” using a best-worst scal-
ing approach (Kiritchenko and Mohammad, 2016,
2017).

The resulting lexicon targeting that lower end
of German language comprises already 3,300 lex-
ical items, incorporates 33,000 human ratings, and
serves as a seed lexicon for fully automatically
acquiring and scoring new lexical items from the
same register. After several iterations, we finally
come up with VULGER, a lexicon of VULgar
GERman, totalling slightly more than 11,000 en-
tries.

2 Related Work

Lexicons covering offensive language are almost
only available for the English language. Perhaps
the earliest collection of such lexical items (in-
cluding phrases and multi-word expressions) is
due to Razavi et al. (2010) who manually assem-
bled approximately 2,700 dictionary entries. More
recent work on an alternative verb-centered lex-
icon (size is not specified) with a focus on hate
speech is reported by Gitari et al. (2015). The cur-
rently largest and most up to date English lexicon
of abusive words is provided by Wiegand et al.
(2018a) who manually and automatically collected
around 8,500 lexical items.1

Languages other than English are incorporated
in HURTLEX2 (Bassignana et al., 2018) which
forms a multilingual lexical resource of words that
hurt for 53 languages, among them Italian, Span-
ish, English and German. This lexicon grew out
of a manual selection of roughly 1,000 Italian hate
words originally organized around 17 categories,
with particular focus on derogatory words. It was
further semi-automatically extended with com-
plementary borrowings from the Italian MULTI-
WORDNET3 and BABELNET.4 HURTLEX also
excels with additional linguistic information (parts
of speech, lexicographic definitions) for its lem-
mas. The lexicon integration step yields roughly
1,160 multilingual lexical items (with the help of
the BABELNET API).

Manual curation (for the Italian portion) in-
cluded a categorization step for each lemma sense
into one of three categories: ‘Not Offensive’–
‘Neutral’–‘Offensive’. In a subsequent step, the
‘Neutral’ category was split into ‘Not Literally Pe-
jorative’ (insult by means of a semantic shift, e.g.
metaphorically) and ‘Negative Connotation’ (not
necessarily a direct derogatory use but used in a
derogatory way). 2-expert agreements plunged
from 87.6% for the 3-category decisions to 61%
for the extended 5-category decisions. Clearly, an
indicator that such categorical decisions are hard
to make even for competent native speakers.

As far as canonical German lexical resources
are concerned, their coverage at the low end of lan-

1https://github.com/uds-lsv/
lexicon-of-abusive-words

2http://hatespeech.di.unito.it/
resources.html

3http://multiwordnet.fbk.eu/english/
home.php

4https://babelnet.org/

https://github.com/uds-lsv/lexicon-of-abusive-words
https://github.com/uds-lsv/lexicon-of-abusive-words
http://hatespeech.di.unito.it/resources.html
http://hatespeech.di.unito.it/resources.html
http://multiwordnet.fbk.eu/english/home.php
http://multiwordnet.fbk.eu/english/home.php
https://babelnet.org/


121

guage is, not surprisingly, more than incomplete.
In effect, GERMANET V13.0,5 for instance, cov-
ers only 1,774 lexical items from our seed lexi-
con (3,300 lexical items, in total). Yet, this ratio is
even higher than for other lexical resources such
as HATEBASE,6 a repository which covers 95 lan-
guages (with 2,691 hate terms), yet only enumer-
ates 95 manually provided German hate speech
entries at all.

In conclusion, the compilation of lexicons for
offensive, abusive or hate language typically con-
sists of two steps. First, already available lex-
ical resources covering such pejorative lexical
items are identified and bundled in a seed lexi-
con. Next, this seed is incrementally enlarged—
using additional lexical resources (such as WORD-
NETs, WIKTIONARY, or BABELNET), or employ-
ing some sort of machine learning process (Wie-
gand et al., 2018a). Yet, the semantic core of such
lexicons are (manual or automatic) categorical as-
signments of either bi-polar (e.g., ‘Offensive’ vs.
‘Non-Offensive’) or multi-polar categories (e.g.,
‘Colloquial’ vs. ‘Rough’ vs. ‘Obscene’).

As an alternative to this scheme, our work fo-
cuses on substituting discrete categorical deci-
sions by continuous grading of the above distinc-
tions based on Best-Worst Scaling (Louviere et al.,
2015). We thus target a research desideratum al-
ready described by Schmidt and Wiegand (2017,
p.3-4) in the following way: “Despite their gen-
eral effectiveness, relatively little is known about
the creation process and the theoretical concepts
that underlie the lexical resources that have been
specially compiled for hate speech detection.”

3 (Tentatively) Characterizing Vulgar
Language

In our study, we not only consider hate speech and
abusive terms, but take a much broader perspective
on the topic of offensive language and its lexical-
izations. Still, this goal is very hard to characterize
by distinctive criteria since many lexical-semantic
dimensions seem to be involved and strongly in-
teract.

Vulgar language, as we conceive it, is predom-
inantly signalled by an overly lowered language
register, the taboo layer, with disgusting and ob-
scene lexicalizations generally banned from any

5http://www.sfs.uni-tuebingen.de/
GermaNet/

6https://hatebase.org

type of civilized discourse. Primarily (yet not
only), it addresses the lexical fields of sexual-
ity (sexual organs and activities, in particular), as
well as body orifices or other specific body parts
(e.g., “Fresse” (“puss”) as a negative denotation
for “Gesicht/Mund” (“face/mouth”)) and scato-
logic expressions. One often also observes mean-
ing transfers from animals with culture-dependent
negative connotations to humans (e.g., “Ratte”
(“rat”)). Pejorative words with marked negative
connotation also play a significant role here (e.g.,
“abkratzen” (“croak”)). Especially religious, eth-
nic and political orientations, the primary targets
of hate speech, gain a strong vulgar status when
they are combined with (animal-related) swear-
words such as “Schwein” (“pig”).

We are aware of the preliminary status of this
characterization of vulgar language, but consider
our work as a starting point for clarifying its nature
and systematicity in more depth.

4 Lexicon Acquisition Method

Since a broad-coverage lexicon of obscene Ger-
man (ranging on an interval from neutral to vulgar)
is missing, we decided on a weakly supervised ap-
proach to lexicon acquisition based on bootstrap-
ping. It consists of the following steps (the over-all
workflow is fundamentally inspired by the work of
Wiegand et al. (2018a), yet complements it by a
hitherto unexplored methodology to scale the de-
gree of obscenity of lexical items based on best-
worst scaling):

1. Language Resources: Select a seed lexicon
(possibly combining numerous relevant re-
sources) which contains a collection of lex-
ical items already tagged as rough and vul-
gar. Typically, this step reuses manually pre-
categorized lexical items (work typically due
to experienced lexicographers). Further, this
lexical collection can be enhanced by exploit-
ing large-scale corpora—these can either be
already annotated for (some degree of) vul-
garity or lack any annotation of this kind
at all—or representational derivatives there-
from, such as (word) embeddings.

2. Human Assessment: Refine the seed lex-
icon by complementary human assessments
of obscenity/vulgarity on the basis of crowd-
sourcing using differential best-worst scal-
ing.

http://www.sfs.uni-tuebingen.de/GermaNet/
http://www.sfs.uni-tuebingen.de/GermaNet/
https://hatebase.org


122

Figure 1: Generic language-independent workflow for lexicon acquisition (in blue) and its instantiation for German
(in green); solid blue arrows indicate control flow, data flow is represented by dashed blue arrows, green arrows
and ‘+’ stand for lexical data harvesting (with RegEx-style expressions for matching search terms), thin blue lines
link particular choices to realizations (implementations) of the blue main components of VULGER’s acquisition
system

3. Machine Learning: Use the resulting lex-
icon scored on a continuous neutrality-
vulgarity scale as training data for automat-
ically identifying and scoring new, themati-
cally relevant lexical items, ideally from cor-
pora containing a high amount of words re-
garding the property of interest (rough and
vulgar wording).

The first step of this workflow (illustrated in
Fig. 1), consisting of the assembly of relevant lex-
ical material from scratch, will be described in
Section 5. The second one, adding human assess-
ments for that lexical material, is dealt with in Sec-
tion 6, while the third step, automatic lexicon en-
hancement, is described in Section 7.

5 Building the Seed Lexicon

From the German slice of WIKTIONARY,7 we ex-
tracted all words marked as vulgar, rough and pe-
jorative.8 Additionally, we gathered entries tagged
with corresponding categories9 from the German

7https://de.wiktionary.org
8The exact terms and corresponding abbreviations are:

‘vulgär’, ‘vulg.’, ‘vul.’, ‘derb’ and ‘abwertend’, ‘abw.’.
9‘vulg.’, ‘derb’, ‘abwertend’

OPENTHESAURUS.10 As the focus of our corpus
lies on single words11 from a vulgar vocabulary,
we excluded phrases from this processing step.

The list resulting from this first step con-
tained some entries used as affixes in morphologi-
cally productive word formation, such as “*geil”,
“scheiß*”, “Scheiß*” and “Drecks*”, the latter
ones denoting variants of “shit” and “dirt”. We
cancelled these rudimentary entries from the list
because there is no way to get meaningful judg-
ments for them in isolation due to the many possi-
ble combinations yielding highly diverse degrees
of vulgarity.

In order to account for these terms in a
reasonable way we extended our list by har-
vesting rough and vulgar word forms concate-
nated with the affixes mentioned above from the
CODE ALLTAGS+d email corpus12 (Krieg-Holz
et al., 2016), the DORTMUNDER CHAT KOR-
PUS (Beißwenger, 2013)12 and from entries in

10https://www.openthesaurus.de
11The vast potential of the German language for produc-

tive noun composition within single compounds makes this
decision less restrictive than it seems; cf., e.g., the English
phrase form “son of a bitch” and its German single com-
pound equivalent “Hurensohn”.

12We only took words with a minimum frequency of 3.

https://de.wiktionary.org
https://www.openthesaurus.de


123

FASTTEXT (Grave et al., 2018) word embeddings,
the latter being based on COMMON CRAWL and
WIKIPEDIA.

Yet, we not only incorporated plain text corpora
or computationally derived lexical items (exploit-
ing the FASTTEXT embeddings) into our study, but
also included word embeddings as a representa-
tion format based on the distributional semantics
hypothesis and computationally derived from cor-
pora (see also Tulkens et al. (2016); Wiegand et al.
(2018a). Utilizing the word embeddings from the
corpora mentioned above and the GENSIM mod-
ule (Řehůřek and Sojka, 2010) we further gen-
erated, using the lexical seeds from the previous
round, similar words, i.e., close semantic neigh-
bors of these seed words by iteratively minimiz-
ing the threshold for similarity, until we found too
much noise was returned (a common procedure,
cf. also Tulkens et al. (2016)). We manually edited
the resulting list in regard to inflected forms, mis-
spellings and case sensitivity, but we intentionally
kept the ‘lexical noise’, i.e., presumably neutral
words. Since we planned to annotate the lexical
items identified this way by crowdsourcing in a
later phase, these neutral words also help coun-
terbalance the impact of rough and vulgar expres-
sions during assessments. In total, based on this
procedure we gathered a seed lexicon with 3,300
entries.

6 Enriching the Seed Lexicon: Scaling
Degrees of Vulgarity

We chose to annotate our seed words with Best-
Worst-Scaling (BWS), because it delivers high-
quality annotations with only a relatively small
number of annotation steps. BWS is an extension
of the method of paired comparison to multiple
choices, originally developed by Louviere et al.
(2015) and introduced into NLP for emotion scal-
ing by Kiritchenko and Mohammad (2016, 2017).
For BWS, annotators are presented with n items at
a time (an n-tuple, where n ¿ 1, and typically n =
4). They then have to decide which item from the
n-tuple under scrutiny is the best (highest in terms
of the property of interest) and which is the worst
(lowest in terms of the property of interest).

In our case, judges had to select the most neu-
tral and the most vulgar terms per given n-tuple.
We used the BWS tool13 from Kiritchenko and

13http://www.saifmohammad.com/WebPages/
BestWorst.html

Mohammad (2016, 2017) to generate 2N decision
alternatives (N denotes the size of our seed lexi-
con) and thus came up with 6,600 4-tuples to be
assessed. Tuples were produced randomly under
the premise that each term has to occur only once
in eight different tuples and each tuple is unique.

For the annotation process proper, we used the
crowdsourcing platforms FIGURE EIGHT14 and
CLICKWORKER,15 where each n-tuple was as-
sessed by five annotators. In order to get real-
valued scores from the BWS annotations we ap-
plied COUNTS ANALYSIS (Orme, 2009)16 and
thus got scores between +1 (most neutral) and−1
(most vulgar). Scores were calculated by subtract-
ing the percentage of times the term was chosen as
worst from the percentage of times the term was
chosen as best. We computed the split-half reli-
ability16 like Kiritchenko and Mohammad (2017)
by randomly splitting the annotations of a tuple
into two halves, calculating scores independently
for these halves and measuring the correlation be-
tween the resulting two sets of scores. We got
an average Pearson correlation of 0.9102 (+/ −
0.0022) over 100 trials.

7 Automatic Lexicon Extension

7.1 Regression Models
In order to further extend the lexicon in a purely
automatic way and also inspired by studies on au-
tomatic word emotion induction (especially by Li
et al. (2017a) and Buechel and Hahn (2018)) we
employed regression models to predict scores for
input words. The seed words served as training
and testing data for a linear regression and a ridge
regression model (linear regression with L2 regu-
larization during training).17 As features for the
words we used their respective word embeddings
(this, obviously, excludes lexical items from fur-
ther consideration for which no embeddings exist).

We experimented with different word embed-
dings. We built 100-dimensional word em-
beddings from CODE ALLTAGXL (Krieg-Holz
et al., 2016) using WORD2VEC (Mikolov et al.,
2013) for all words occurring at least 3 times in
CODE ALLTAGXL. Furthermore, we employed
WORD2VEC word embeddings from Reimers

14https://www.figure-eight.com
15https://www.clickworker.de
16Again we used the scripts from Kiritchenko and Moham-

mad (2016, 2017).
17For both we used the scikit-learn.org implemen-

tation using the default parameters.

http://www.saifmohammad.com/WebPages/BestWorst.html
http://www.saifmohammad.com/WebPages/BestWorst.html
https://www.figure-eight.com
https://www.clickworker.de


124

et al. (2014) with a minimum word frequency of
5 and 100 dimensions (UKP), 300-dimensional
FASTTEXT word embeddings from SPINNING-
BYTES (Cieliebak et al., 2017) trained on German
tweets (TWITTER) and, finally, FASTTEXT word
embeddings (Grave et al., 2018) based on COM-
MON CRAWL and WIKIPEDIA (FASTTEXT). We
also tried to utilize embeddings generated from the
German TWITTER HATESPEECH corpora from
Ross et al. (2016) and Wiegand et al. (2018b) un-
der the assumption that they might contain a large
number of rough and vulgar words. But due to
their small size and their nevertheless high propor-
tion of out-of-vocabulary words we had to exclude
both of these resources from further consideration.

Table 1 shows that the ridge regression model
performs equally or slightly better compared to the
linear regression model. Regarding the input fea-
tures the FASTTEXT token embeddings performed
best (see Table 2).

Embeddings LinReg RidgeReg p
CODE ALLTAGXL 0.574 0.575 0.004
UKP 0.682 0.682 0.121
TWITTER 0.735 0.735 0.073
FASTTEXT 0.766 0.779 0.001

Table 1: Averaged Pearson correlation (10-fold cross
validation) and p-value (two-sided t-test) for Linear
Regression (LinReg) and Ridge Regression (Ridge-
Reg)

Embeddings Pearson r p
CODE ALLTAGXL 0.575 < 0.001
UKP 0.682 < 0.001
TWITTER 0.735 < 0.001
FASTTEXT 0.779 —

Table 2: Averaged Pearson correlation (10-fold cross
validation) for different embeddings with Ridge Re-
gression, with significance difference to best perform-
ing embeddings (p-value from two-sided t-test)

7.2 Applying Regression Models to Enhance
the Lexicon

We used the best method (ridge regression and
FASTTEXT embeddings) to extend our lexicon
with three German swearword lists.18 There is an

18These lists were retrieved from http://www.
hyperhero.com/de/insults.htm, http:
//www.insult.wiki/wiki/Schimpfwort-Liste
and https://www.schimpfwoerter.de

overlap between swearwords and vulgar lexical-
izations, but not every swearword has strong vul-
gar status,19 e.g., “Schwein” (“pig”), a subtle dis-
tinction which our scaling approach accounts for
(cf. also the remarks made in Section 3).

We trained a ridge regression model on the seed
words (cf. Sections 5 and 6), i.e., the respective
word embeddings and the scores. This model was
then applied to the input swearwords (from the
three sources mentioned above), which do not oc-
cur in the seed lexicon already, and predicted the
neutrality/vulgarity scores of the remaining entries
on the basis of their word embeddings provided
that an embedding for the respective word was
found in the FASTTEXT embeddings.20 We ex-
cluded out-of-vocabulary words in order to avoid
getting too much noise in terms of wrongly scored
lexical items in our lexicon. Further we thus
dropped really rare words. With the words al-
ready contained in our seed lexicon and words not
present as embeddings removed, we assembled
2,046 additional entries following this approach.

Assuming that corpora for hate speech detec-
tion include a higher amount of vulgar and rough
words, we also made use of such datasets. There
exist two publicly available German-language text
corpora annotated for hate speech from which
we extracted lexical material. The first of them,
IWG HATESPEECH, originating from Ross et al.
(2016), contains about 500 tweets which were an-
notated by two judges using a binary categoriza-
tion scheme (“hate speech”: Yes or No) and a 6-
point Likert scale ranging from “not offensive” to
“very offensive”.21 The second corpus collected
by Wiegand et al. (2018b) contains more than
8,500 tweets and was compiled for GERMEVAL
2018, a challenge task addressing the recognition
and classification of offensive German language.22

The latter corpus was coarsely annotated with bi-
nary ‘Offense’ and ‘Other’ categories, but it also
comes with a 4-way classification schema where
besides the non-offensive ‘Other’ class ‘Offense’
was subdivided in three ways: ‘Profanity’ (no in-
tent to insult someone, yet the lexical choice is
negatively marked, with swearwords such as sca-

19Also not every vulgar word is a swearword.
20We also checked for different spellings regarding case

sensitivity.
21The corpus is available at https://github.com/

UCSM-DUE/IWG_hatespeech_public
22The corpus is available at https://projects.

cai.fbi.h-da.de/iggsa/

http://www.hyperhero.com/de/insults.htm
http://www.hyperhero.com/de/insults.htm
http://www.insult.wiki/wiki/Schimpfwort-Liste
http://www.insult.wiki/wiki/Schimpfwort-Liste
https://www.schimpfwoerter.de
https://github.com/UCSM-DUE/IWG_hatespeech_public
https://github.com/UCSM-DUE/IWG_hatespeech_public
https://projects.cai.fbi.h-da.de/iggsa/
https://projects.cai.fbi.h-da.de/iggsa/


125

tologic “Scheiße” (shit)), ‘Insult’ (clear intent to
offend someone) and ‘Abuse’ (an even stronger
form of ‘Insult’, i.e., an abusive utterance that de-
grades a target person/group by ascribing a social
identity to a person/group that is judged negatively
by a (perceived) majority of society).

From these two corpora we extracted words
from all tweets marked as ‘Offense’ = ‘YES’ by
one of the annotators and further removed stop
words, hashtags and words with non-alphabetic
characters excluding hyphens or a word length
smaller than 4. We also tried to lemmatize the
words23 and normalize spellings in regard to case
sensitivity, but admittedly inserted some noise into
our input words, i.e., some inflected forms and
other forms of semantic duplication could not
be normalized. After excluding words already
present in the seed lexicon or in the German swear-
words lists we applied the same procedure as used
for the swearwords and obtained another 5,700
new scored lexical entries.

Due to the lack of better resources we tried to
measure the reliability of the resulting scores in
a preliminary way by calculating the correlation
between the probability of a word being in an of-
fensive post and its score. We got a Pearson cor-
relation coefficient of only −0.35, probably also
caused by many words occurring just once, but
the correlation may also be inherently weak. In
future work, we plan to evaluate the automati-
cally determined extension of our seed word lexi-
con by feeding the lexical items back into another
crowdsourcing round and determining the correla-
tion between the human assessment and the auto-
matically derived scoring values.

The final version of VULGER, a lexicon with
VULgarity ratings of GERman words, enhanced
with swearwords and words from the two hate
speech corpora in the end comprises 11,046 en-
tries (see Table 3).

Resource # Lexical Items
Seed Words 3,300
German Swearwords 2,046
Twitter Hate Speech Corpora 5,700
Total 11,046

Table 3: Decomposition of contributions from various
language resources for VULGER, the current version of
the lexicon of VULgar GERman

23We used SPACY: https://spacy.io/

8 Conclusion

In this paper, we are concerned with the lexi-
cal segment at the lower stylistic end of each
natural language often referred to as rough, vul-
gar and obscene. This register typically cov-
ers very explicit and rude linguistic expressions
(taboo words). Standard lexical repositories have
mostly neglected lots of these expressions on pur-
pose although a pressing need can now be derived
for such an extension, e.g., for the purpose of iden-
tifying and neutralizing or blocking offensive and
humiliating utterances in social media.

Our workflow for building such a lower-end lex-
icon is based on three steps: assembling already
existing lexicons (or fragments therefrom) for this
stylistic subvariety of language, assigning degrees
of vulgarity for each lexical item included, and us-
ing this seed for continuous automatic enhance-
ment by weakly supervised machine learning pro-
cedures.

As far as the representation of the semantics
of these lexical items are concerned, we pro-
pose a continuous grading system to substitute
overly simplistic discrete categorical schemata
which have been prevailing so far. Still, the claim
that such a fine-grained representation is helpful
at all must also be demonstrated by experiments
in the future. In any case, we plan to use and it-
eratively extend our newly developed lexicon on
text corpora with similar biases into pejorative lan-
guages (including scores for obscenity). How-
ever, merely (automatically) extending a special-
ized lexicon might not necessarily prove beneficial
as evidenced by the results of Tulkens et al. (2016)
that showed no performance boost for a system us-
ing such an extended dictionary, at least for detect-
ing Dutch racist language.

In order to by-pass the sparse data problem,
methods like transfer learning might also be ap-
propriate here (Sahlgren et al., 2018). Still, the
validity of these new items and their scores have
to be experimentally validated, e.g., by feeding
newly found lexical material back to annotators
and compare their judgments with automatically
predicted ones.

We are also aware of the fact that purely lexi-
cally driven approaches to account for obscene, of-
fensive or vulgar language may not be sufficient to
solve the recognition problem completely and that
a broader discourse context has to be taken into
account, as well as the linguistic conventions in

https://spacy.io/


126

different communities (Owsley Sood et al., 2012).
Still, a lexicon of significant size and quality might
form the backbone for machines sensitive to rude
and vulgar language.

References

Elisa Bassignana, Valerio Basile, and Viviana Patti.
2018. HURTLEX: a multilingual lexicon of words
to hurt. In CLiC-it 2018 — Proceedings of the
5th Italian Conference on Computational Linguis-
tics. Torino, Italy, December 10-12, 2018, number
2253 in CEUR Workshop Proceedings, page #49.

Michael Beißwenger. 2013. Das Dortmunder Chat-
Korpus. Zeitschrift für germanistische Linguistik,
41(1):161–164.

Sven Buechel and Udo Hahn. 2018. Word emotion in-
duction for multiple languages as a deep multi-task
learning problem. In NAACL-HLT 2018 — Proceed-
ings of the 2018 Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics: Human Language Technologies. New
Orleans, Louisiana, USA, June 1-6, 2018, volume
1: Long Papers, pages 1907–1918, Stroudsburg/PA.
Association for Computational Linguistics (ACL).

Ekaterina Chernyak. 2017. Comparison of string
similarity measures for obscenity filtering. In
BSNLP 2017 — Proceedings of the 6th Workshop
on Balto-Slavic Natural Language Processing @
EACL 2017. Valencia, Spain, April 4, 2017, pages
97–101, Stroudsburg/PA. Association for Computa-
tional Linguistics (ACL).

Mark Cieliebak, Jan Deriu, Dominic Egger, and Fatih
Uzdilli. 2017. A Twitter corpus and benchmark
resources for German sentiment analysis. In So-
cialNLP 2017 — Proceedings of the 5th Interna-
tional Workshop on Natural Language Processing
for Social Media of the AFNLP SIG SocialNLP @
EACL 2017. Valencia, Spain, April 3, 2017, pages
45–51, Stroudsburg/PA. Association for Computa-
tional Linguistics (ACL).

Maral Dadvar, Dolf Trieschnigg, and Franciska M. G.
De Jong. 2014. Experts and machines against bul-
lies: a hybrid approach to detect cyberbullies. In Ad-
vances in Artificial Intelligence. Canadian AI 2014
— Proceedings of the 27th Canadian Conference on
Artificial Intelligence. Montréal, Québec, Canada,
May 6-9, 2014, number 8436 in Lecture Notes
in Articificial Intelligence (LNAI), pages 275–281,
Cham, Switzerland. Springer International Publish-
ing.

Khairil Imran Ghauth and Muhammad Shurazi Sukhur.
2015. Text censoring system for filtering mali-
cious content using approximate string matching and
Bayesian filtering. In Computational Intelligence in

Information Systems. INNS-CIIS 2014 — Proceed-
ings of the 4th INNS Symposia Series on Compu-
tational Intelligence in Information Systems. Ban-
dar Seri Begawan, Brunei, November 2014, number
331 in Advances in Intelligent Systems and Com-
puting Book Series (AISC), pages 149–158, Cham,
Switzerland. Springer International Publishing.

Njagi Dennis Gitari, Zhang Zuping, Hanyurwimfura
Damien, and Jun Long. 2015. A lexicon-based
approach for hate speech detection. International
Journal of Multimedia and Ubiquitous Engineering,
10(4):215–230.

Edouard Grave, Piotr Bojanowski, Prakhar Gupta, Ar-
mand Joulin, and Tomáš Mikolov. 2018. Learn-
ing word vectors for 157 languages. In LREC
2018 — Proceedings of the 11th International Con-
ference on Language Resources and Evaluation.
Miyazaki, Japan, May 7-12, 2018, pages 3483–
3487, Paris. European Language Resources Associ-
ation (ELRA).

Qianjia Huang, Jianhong Zhang, Diana Z. Inkpen, and
David Van Bruwaene. 2018. Cyberbullying inter-
vention interface based on convolutional neural net-
works. In TRAC 2018 — Proceedings of the 1st
Workshop on Trolling, Aggression and Cyberbully-
ing @ COLING 2018. Santa Fe, New Mexico, USA,
25 August, 2018, pages 42–51, Stroudsburg/PA. As-
sociation for Computational Linguistics (ACL).

Svetlana Kiritchenko and Saif M. Mohammad. 2016.
Capturing reliable fine-grained sentiment associa-
tions by crowdsourcing and best-worst scaling. In
NAACL-HLT 2016 — Proceedings of the 2016 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies. San Diego, California, USA,
June 12-17, 2016, pages 811–817, Stroudsburg/PA.
Association for Computational Linguistics (ACL).

Svetlana Kiritchenko and Saif M. Mohammad. 2017.
Best-worst scaling more reliable than rating scales:
a case study on sentiment intensity annotation. In
ACL 2017 — Proceedings of the 55th Annual Meet-
ing of the Association for Computational Linguis-
tics. Vancouver, British Columbia, Canada, July 30
- August 4, 2017, volume 2: Short Papers, pages
465–470, Stroudsburg/PA. Association for Compu-
tational Linguistics (ACL).

Ulrike Krieg-Holz, Christian Schuschnig, Franz
Matthies, Benjamin Redling, and Udo Hahn. 2016.
CODE ALLTAG: A German-language e-mail corpus.
In LREC 2016 — Proceedings of the 10th Interna-
tional Conference on Language Resources and Eval-
uation. Portorož, Slovenia, 23-28 May 2016, pages
2543–2550, Paris. European Language Resources
Association (ELRA-ELDA).

Minglei Li, Qin Lu, Yunfei Long, and Lin Gui. 2017a.
Inferring affective meanings of words from word
embedding. IEEE Transactions on Affective Com-
puting, 8(4):443–456.



127

Tai Ching Li, Joobin Gharibshah, Evangelos E.
Papalexakis, and Michalis Faloutsos. 2017b.
TROLLSPOT: detecting misbehavior in commenting
platforms. In ASONAM 2017 — Proceedings of
the 2017 IEEE/ACM International Conference on
Advances in Social Networks Analysis and Mining
2017. Sydney, Australia, July 31 - August 03, 2017,
pages 171–175, New York/NY. Association for
Computing Machinery (ACM).

Jordan J. Louviere, Terry N. Flynn, and A. A. J. Mar-
ley. 2015. Best-Worst Scaling: Theory, Methods and
Applications. Cambridge University Press, Cam-
bridge, U.K.

John P. McIntire, Lindsey K. McIntire, and Paul R.
Havig. 2010. Methods for chatbot detection in dis-
tributed text-based communications. In CTS 2010
— Proceedings of the 2010 International Sympo-
sium on Collaborative Technologies and Systems.
Chicago, Illinois, USA, 17-21 May 2010, pages 463–
472. IEEE.

Tomáš Mikolov, Ilya Sutskever, Kai Chen, Gregory S.
Corrado, and Jeffrey Dean. 2013. Distributed rep-
resentations of words and phrases and their com-
positionality. In Advances in Neural Information
Processing Systems 26 — NIPS 2013. Proceedings
of the 27th Annual Conference on Neural Infor-
mation Processing Systems. Lake Tahoe, Nevada,
USA, December 5-10, 2013, pages 3111–3119, Red
Hook/NY. Curran Associates, Inc.

Bryan Orme. 2009. Maxdiff analysis: simple counting,
individual-level logit, and HB. Sawtooth Software,
Inc.

Sara Owsley Sood, Judd Antin, and Elizabeth F.
Churchill. 2012. Profanity use in online communi-
ties. In CHI 2012 — Proceedings of the 30th ACM
SIGCHI Conference on Human Factors in Comput-
ing Systems. Austin, Texas, USA, May 5-10, 2012,
pages 1481–1490, New York/NY. Association for
Computing Machinery (ACM).

Amir Hossein Razavi, Diana Z. Inkpen, Sasha Uritsky,
and Stan Matwin. 2010. Offensive language detec-
tion using multi-level classification. In Advances
in Artificial Intelligence. Canadian AI 2010 — Pro-
ceedings of the 23rd Canadian Conference on Ar-
tificial Intelligence. Ottawa, Ontario, Canada, May
31 - June 2, 2010, number 6085 in Lecture Notes
in Computer Science (LNCS), pages 16–27, Berlin,
Heidelberg. Springer-Verlag.

Radim Řehůřek and Petr Sojka. 2010. Software frame-
work for topic modelling with large corpora. In Pro-
ceedings of the Workshop on New Challenges for
NLP Frameworks @ LREC 2010. La Valletta, Malta,
May 22, 2010, pages 45–50, Paris. European Lan-
guage Resources Association (ELRA).

Nils Reimers, Judith Eckle-Kohler, Carsten Schnober,
Jungi Kim, and Iryna Gurevych. 2014. GER-
MEVAL2014: nested named entity recognition

with neural networks. In KONVENS 2014 —
Proceedings of the Workshops of the 12th Edi-
tion of the KONVENS Conference: GermEval.
Hildesheim, Germany, October 8-10, 2014, pages
117–120, Hildesheim, Germany. Universitätsverlag
Hildesheim.

Björn Ross, Michael Rist, Guillermo Carbonell, Ben-
jamin Cabrera, Nils Kurowsky, and Michael Wo-
jatzki. 2016. Measuring the reliability of hate
speech annotations: the case of the European
refugee crisis. In NLP4CMC III — Proceedings
of the 3rd Workshop on Natural Language Pro-
cessing for Computer-Mediated Communication.
Bochum, Germany, 22 September 2016, number 17
in Bochumer Linguistische Arbeitsberichte (BLA),
pages 6–9.

Magnus Sahlgren, Tim Isbister, and Fredrik Olsson.
2018. Learning representations for detecting abu-
sive language. In ALW 2 — Proceedings of the 2nd
Workshop on Abusive Language Online @ EMNLP
2018. Brussels, Belgium, October 31, 2018, pages
115—123, Stroudsburg/PA. Association for Compu-
tational Linguistics (ACL).

Cı́cero Nogueira dos Santos, Igor Melnyk, and Inkit
Padhi. 2018. Fighting offensive language on social
media with unsupervised text style transfer. In ACL
2018 — Proceedings of the 56th Annual Meeting
of the Association for Computational Linguistics.
Melbourne, Victoria, Australia, July 15-20, 2018,
volume 2: Short Papers, pages 189–194, Strouds-
burg/PA. Association for Computational Linguistics
(ACL).

Anna Schmidt and Michael Wiegand. 2017. A survey
on hate speech detection using natural language pro-
cessing. In SocialNLP 2017 — Proceedings of the
5th International Workshop on Natural Language
Processing for Social Media of the AFNLP SIG So-
cialNLP @ EACL 2017. Valencia, Spain, April 3,
2017, pages 1–10. Association for Computational
Linguistics (ACL).

Hui-Po Su, Zhen-Jie Huang, Hao-Tsung Chang, and
Chuan-Jie Lin. 2017. Rephrasing profanity in Chi-
nese text. In ALW 1 — Proceedings of the 1st Work-
shop on Abusive Language Online @ ACL 2017.
Vancouver, British Columbia, Canada, August 4,
2017, pages 18–24, Stroudsburg/PA. Association for
Computational Linguistics (ACL).

Sajedul Talukder and Bogdan Carbunar. 2018. ABUS-
NIFF : automatic detection and defenses against abu-
sive FACEBOOK friends. In ICWSM 2018 — Pro-
ceedings of the 12th International AAAI Confer-
ence on Web and Social Media. Stanford, Califor-
nia, USA, June 25-28, 2018, pages 385–394, Palo
Alto/CA. AAAI Press.

Stéphan Tulkens, Lisa Hilte, Elise Lodewyckx, Ben
Verhoeven, and Walter Daelemans. 2016. A
dictionary-based approach to racism detection in



128

Dutch social media. In TA-COS 2016 — Proceed-
ings of the Workshop on Text Analytics for Cyberse-
curity and Online Safety @ LREC 2016. Portorož,
Slovenia, 23 May 2016, pages 11–17.

Zeerak Waseem, Thomas Davidson, Dana Warmsley,
and Ingmar Weber. 2017. Understanding abuse:
a typology of abusive language detection subtasks.
In ALW 1 — Proceedings of the 1st Workshop on
Abusive Language Online @ ACL 2017. Vancouver,
British Columbia, Canada, August 4, 2017, pages
78–84, Stroudsburg/PA. Association for Computa-
tional Linguistics (ACL).

Aksel Wester, Lilja Øvrelid, Erik Velldal, and
Hugo Lewi Hammer. 2016. Threat detection in on-
line discussions. In WASSA 2016 — Proceedings
of the 7th Workshop on Computational Approaches
to Subjectivity, Sentiment and Social Media Analysis
@ NAACL-HLT 2016. San Diego, California, USA,
June 16, 2016, pages 66–71, Stroudsburg/PA. Asso-
ciation for Computational Linguistics (ACL).

Michael Wiegand, Josef Ruppenhofer, Anna Schmidt,
and Clayton Greenberg. 2018a. Inducing a lexi-
con of abusive words: a feature-based approach.
In NAACL-HLT 2018 — Proceedings of the 2018
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies. New Orleans, Louisiana,
USA, June 1-6, 2018, volume 1: Long Papers, pages
1046–1056, Stroudsburg/PA. Association for Com-
putational Linguistics (ACL).

Michael Wiegand, Melanie Siegel, and Josef Rup-
penhofer. 2018b. Overview of the GERMEVAL
2018 Shared Task on the Identification of Offen-
sive Language. In Proceedings of the GERMEVAL
2018 Workshop @ KONVENS 2018. Vienna, Aus-
tria, September 21, 2018, pages 1–10.

Zhelun Wu, Nishant Kambhatla, and Anoop Sarkar.
2018. Decipherment for adversarial offensive lan-
guage detection. In ALW 2 — Proceedings of
the 2nd Workshop on Abusive Language Online
@ EMNLP 2018. Brussels, Belgium, October 31,
2018, pages 149–159, Stroudsburg/PA. Association
for Computational Linguistics (ACL).

Taijin Yoon, Sun-Young Park, and Hwan-Gue Cho.
2010. A smart filtering system for newly coined
profanities by using approximate string alignment.
In CIT 2010 — Proceedings of the IEEE 10th Inter-
national Conference on Computer and Information
Technology. Bradford, UK, 29 June - 1 July 2010,
pages 643–650. IEEE.


