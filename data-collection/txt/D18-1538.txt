











































Towards Semi-Supervised Learning for Deep Semantic Role Labeling


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4958–4963
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

4958

Towards Semi-Supervised Learning for Deep Semantic Role Labeling

Sanket Vaibhav Mehta⇤ Jay Yoon Lee⇤

School of Computer Science,
Carnegie Mellon University,

Pittsburgh, PA
{svmehta,jaylee,jgc}@cs.cmu.edu

Jaime Carbonell

Abstract
Neural models have shown several state-of-
the-art performances on Semantic Role Label-
ing (SRL). However, the neural models re-
quire an immense amount of semantic-role
corpora and are thus not well suited for low-
resource languages or domains. The paper
proposes a semi-supervised semantic role la-
beling method that outperforms the state-of-
the-art in limited SRL training corpora. The
method is based on explicitly enforcing syn-
tactic constraints by augmenting the train-
ing objective with a syntactic-inconsistency
loss component and uses SRL-unlabeled in-
stances to train a joint-objective LSTM. On
CoNLL-2012 English section, the proposed
semi-supervised training with 1%, 10% SRL-
labeled data and varying amounts of SRL-
unlabeled data achieves +1.58, +0.78 F1, re-
spectively, over the pre-trained models that
were trained on SOTA architecture with ELMo
on the same SRL-labeled data. Additionally,
by using the syntactic-inconsistency loss on
inference time, the proposed model achieves
+3.67, +2.1 F1 over pre-trained model on 1%,
10% SRL-labeled data, respectively.

1 Introduction

Semantic role labeling (SRL), a.k.a shallow se-
mantic parsing, identifies the arguments corre-
sponding to each clause or proposition, i.e. its se-
mantic roles, based on lexical and positional in-
formation. SRL labels non-overlapping text spans
corresponding to typical semantic roles such as
Agent, Patient, Instrument, Beneficiary, etc. This
task finds its use in many downstream applica-
tions such as question-answering (Shen and La-
pata, 2007), information extraction (Bastianelli
et al., 2013), machine translation, etc.

Several SRL systems relying on large annotated
corpora have been proposed (Peters et al., 2018;

⇤ Equal contribution, name order decided by coin flip.

He et al., 2017), and perform relatively well. A
more challenging task is to design an SRL method
for low resource scenarios (e.g. rare languages or
domains) where we have limited annotated data
but where we may leverage annotated data from
related tasks. Therefore, in this paper, we focus
on building effective systems for low resource sce-
narios and illustrate our system’s performance by
simulating low resource scenarios for English.

SRL systems for English are built using large
annotated corpora of verb predicates and their ar-
guments provided as part of the PropBank and
OntoNotes v5.0 projects (Kingsbury and Palmer,
2002; Pradhan et al., 2013). These corpora
are built by adding semantic role annotations to
the constituents of previously-annotated syntactic
parse trees in the Penn Treebank (Marcus et al.,
1993). Traditionally, SRL relies heavily on using
syntactic parse trees either from shallow syntac-
tic parsers (chunkers) or full syntactic parsers and
Punyakanok et al. shows significant improvements
by using syntactic parse trees.

Recent breakthroughs motivated by end-to-end
deep learning techniques (Zhou and Xu, 2015;
He et al., 2017) achieve state-of-the-art perfor-
mance without leveraging any syntactic signals,
relying instead on ample role-label annotations.
We hypothesize that by leveraging syntactic struc-
ture while training neural SRL models, we may
achieve robust performance, especially for low
resource scenarios. Specifically, we propose to
leverage syntactic parse trees as hard constraints
for the SRL task i.e., we explicitly enforce that
the predicted argument spans of the SRL network
must agree with the spans implied by the syn-
tactic parse of the sentence via scoring function
in the training objective. Moreover, we present
a semi-supervised learning (SSL) based formula-
tion, wherein we leverage syntactic parse trees for
SRL-unlabeled data to build effective SRL for low



4959

resource scenarios.
We build upon the state-of-the-art SRL system

by (Peters et al., 2018; He et al., 2017), where we
formulate SRL as a BIO tagging problem and use
multi-layer highway bi-directional LSTMs. How-
ever, we differ in terms of our training objective.
In addition to the log-likelihood objective, we also
include syntactic inconsistency loss (defined in
Section 2.3) which quantifies the hard constraint
(spans implied by syntactic parse) violations in our
training objective. In other words, while training
our model, we enforce the outputs of our system to
agree with the spans implied by the syntactic parse
of the sentence as much as possible. In summary,
our contributions to low-resource SRL are:

1. A novel formulation which leverages syntac-
tic parse trees for SRL by introducing them
as hard constraints while training the model.

2. Experiments with varying amounts of SRL-
unlabeled data that point towards semi-
supervised learning for low-resource SRL by
leveraging the fact that syntactic inconsis-
tency loss does not require labels.

2 Proposed Approach
We build upon an existing deep-learning approach
to SRL (He et al., 2017). First we revisit defini-
tions introduced by (He et al., 2017) and then dis-
cuss about our formulation.

2.1 Task definition
Given a sentence-predicate pair (x, v), SRL task is
defined as predicting a sequence of tags y, where
each yi belongs to a set of BIO tags (⌦). So,
for an argument span with semantic role ARGi,
B-ARGi tag indicates that the corresponding to-
ken marks the beginning of the argument span and
I-ARGi tag indicates that the corresponding token
is inside of the argument span and O tag indicates
that token is outside of all argument spans. Let
n = |x| = |y| be the length of the sentence. Fur-
ther, let srl-spans(y) denote the set of all argument
spans in the SRL tagging y. Similarly, parse-
spans(x) denotes the set of all unlabeled parse
constituents for the given sentence x. Lastly, SRL-
labeled/unlabeled data refers to sentence-predicate
pairs with/without gold SRL tags.

2.2 State-of-the-Art (SOTA) Model
He et al. proposed a deep bi-directional LSTM
to learn a locally decomposed scoring func-

tion conditioned on the entire input sentence-Pn
i=1 log p(yi|x). To learn the parameters of a

network, the conditional negative log-likelihood
L(w) of a sample of training data T =
{x(t),y(t)}8t is minimized, where L(w) is

L(w) = �
X

(x,y)2T

|y|X

i=1

log p(yi|x;w). (1)

Since Eq.(1) does not model any dependencies
between the output tags, the predicted output tags
tend to be structurally inconsistent. To alleviate
this problem, (He et al., 2017) searches for the best
ˆ

y over the space of all possibilities (⌦n) using the
scoring function f(x, y), which incorporates log
probability and structural penalty terms. The de-
tails of scoring function is on Appendix Eq.(7).

ˆ

y = argmax

y

02⌦n
f(x,y

0
) (2)

2.3 Structural Constraints
There are different types of structural constraints:
BIO, SRL and syntactic constraints. BIO con-
straints define valid BIO transitions for sequence
tagging. For example, B-ARG0 cannot be fol-
lowed by I-ARG1. SRL constraints define rules
on the role level and has three particular con-
straints: unique core roles (U), continuation roles
(C) and reference roles (R) (Punyakanok et al.,
2008). Lastly, syntactic constraints state that srl-
spans(y) have to be subset of parse-spans(x).

(He et al., 2017) use BIO and syntactic con-
straints at decoding time by solving Eq.(2) where
f(x, y) incorporates those constraints and report
that SRL constraints do not show significant im-
provements over the ensemble model. In partic-
ular, by using syntactic constraints, (He et al.,
2017) achieves up to +2 F1 score on CoNLL-2005
dataset via A* decoding.

Improvements of SRL system via use of syntac-
tic constraints is consistent with other observations
(Punyakanok et al., 2008). However, all previous
works enforce syntactic consistency only during
decoding step. We propose that enforcing syntac-
tic consistency during training time would also be
beneficial and show the efficacy experimentally on
Section 3.3.

Enforcing Syntactic Consistency To quantify
syntactic inconsistency, we define disagreeing-
spans(x,y) = {spani 2 srl-spans(y) | spani /2
parse-spans(x)}. Further, we define disagreement



4960

rate, d(x,y) 2 [0, 1], and syntactic inconsistency
score, s(x,y) 2 [�1, 1], as follows:

d(x,y) =

|disagreeing-spans(x,y)|
|srl-spans(y)| (3)

s(x,y) = 2⇥ d(x,y)� 1 (4)

Syntactic Inconsistency Loss (SI-Loss) For a
given (x, v), let us consider ˆy(t) to be the best
possible tag sequence (according to Eq.(2) during
epoch t of model training. Ideally, if our model
is syntax-aware, we would have d(x, ˆy(t)) = 0 or
r(x,

ˆ

y

(t)
) = 1. We define a loss component due to

syntactic inconsistency (SI-Loss) as follows:

SI-Loss = s(x, ˆy(t))
|ŷ(t)|X

i=1

log p(ŷ

(t)
i |x;w

(t)
) (5)

During training, we want to minimize SI-Loss.

2.4 Training with Joint Objective
Based on Eq.(1), a supervised loss, and Eq.(5),
the SI-Loss, we propose a joint training objective.
For a given sentence-predicate pair (x, v) and SRL
tags y, our joint training objective (at epoch t) is
defined as:

Joint loss = �↵1
|y|X

i=1

log p(yi|x;w)

+↵2 r(x, ˆy
(t)
)

|ŷ(t)|X

i=1

log p(ŷ

(t)
i |x;w

(t)
) (6)

Here, ↵1 and ↵2 are weights (hyper-parameters)
for different loss components and are tuned using
a development set. During training, we minimize
joint loss - i.e., negative log-likelihood (or cross-
entropy loss) and syntactic inconsistency loss.

2.5 Semi-supervised learning formulation
In low resource scenarios, we have limited labeled
data and larger amounts of unlabeled data. The
obvious question is how to leverage large amounts
of unlabeled data for training accurate models.
In context of SRL, we propose to leverage SRL-
unlabeled data in terms of parse trees.

Observing Eq.(5), one can notice that our
formulation of SI-Loss is only dependent upon
model’s predicted tag sequence ˆy(t) at a particular
time point t during training and the given sentence
and it does not depend upon gold SRL tags. We
leverage this fact in our SSL formulation to com-
pute SI-loss from SRL-unlabeled sentences.

Model/ Test F1 AverageLegend disagreement rate (%)
B100 84.40 14.69
B10 78.56 17.01
B1 67.28 21.17

J100 84.75 (+0.35) 14.48 (�1.43%)
J10 79.09 (+0.53) 16.25 (�4.47%)
J1 68.02 (+0.74) 20.49 (�3.21%)

Table 1: Comparison of baseline models (B) with the
models trained with joint objective (J).

Let sup-s be a batch of SRL-labeled sentences
and usup-s be a batch SRL-unlabeled sentences
only with parse information. In SSL setup, we
propose to train our model with joint objective
where sup-s only contributes to supervised loss
Eq.(1) and unsup-s contributes in terms of syn-
tactic inconsistency objective Eq.(5) and combine
them according to Eq.(6) to train them with joint
loss.

3 Experiments

3.1 Dataset
We evaluate our model’s performance on span-
based SRL dataset from CoNLL-2012 shared
task (Pradhan et al., 2013). This dataset con-
tains gold predicates as part of the input sen-
tence and also gold parse information correspond-
ing to each sentence which we use for defin-
ing hard constraints for SRL task. We use
standard train/development/test split containing
278K/38.3K/25.6K sentences. Further, there is ap-
prox. 10% disagreement between gold SRL-spans
and gold parse-spans (we term these as noisy syn-
tactic constraints). During training, we do not
preprocess data to handle these noisy constraints
but for the analysis related to enforcing syntactic
constraints during inference, we study both cases:
with and without noisy constraints. 1

3.2 Model configurations
For the SOTA system proposed in (Peters et al.,
2018), we use code from Allen AI2 to implement
our approach. We follow their initialization and
training configurations. Let BX , JX denote
model trained with X% of the SRL-labeled data
with cross-entropy and joint training objective, re-

1We preprocessed data for inference by simply deleting
syntactic parse trees for the sentences where we have dis-
agreement and perform standard Viterbi decoding for those
sentences and note that this preprocessing scheme was never
used during training.

2https://github.com/allenai/allennlp



4961

Base Model/ Test F1 AverageLegend disagreement rate (%)
B10 78.56 17.01

B10-SI1x 78.84 (+0.28) 16.17 (�4.94%)
B10-SI5x 78.67 (+0.11) 16.47 (�3.17%)

B10-SI10x 78.76 (+0.20) 16.09 (�5.4%)
B1 67.28 21.17

B1-SI1x 67.67 (+0.39) 20.14 (�4.87%)
B1-SI5x 67.74 (+0.46) 19.93 (�5.86%)
B1-SI10x 67.71 (+0.43) 20.16 (�4.77%)

Table 2: Training with SI-loss for varying sizes of
SRL-unlabeled data on top of the pre-trained baseline
models (B1, B10 on Table 1).

spectively. BX-SIUx and BX-JUx denote model
trained with SI-loss and Joint loss, respectively, on
the pre-trained BX model where X⇥U amount of
SRL-unlabeled data were used for further training.
To satisfy BIO constraints, we run Viterbi decod-
ing by default for inference.

3.3 Results
We are interested in answering following ques-
tions. (Q1) how well does the baseline model
produce syntactically consistent outputs, (Q2)
does our approach actually enforce syntactic con-
straints, (Q3) does our approach enforce syntactic
constraints without compromising the quality of
the system, (Q4) how well does our SSL formu-
lation perform, especially in low-resource scenar-
ios, and lastly (Q5) what is the difference in using
the syntactic constraints in training time compared
to using it at decoding time. To answer (Q1-2) fa-
vorably we report average disagreement rate com-
puted over test split. To answer (Q3-5), we report
overall F1-scores on CoNLL-2012 test set (using
standard evaluation script). For experiments us-
ing SRL-unlabeled data, we report average results
after running multiple experiments with different
random samples of it.

Does training with joint objective help? We
trained 3 models with random 1%, 10% and
whole 100% of the training set with joint objec-
tive (↵1 = ↵2 = 0.5). For comparison, we trained
3 SOTA models with the same training sets. All
models were trained for max 150 epochs and with
a patience of 20 epochs. Table 1 reports the results
of this experiment. We see that models trained
with joint objective (JX) improve over baseline
models (BX), both in terms of F1 and average
disagreement rate. These improvements provide
evidence for answering (Q1-3) favorably. Further,
gains are more in low resource scenarios because

Base Model/ Test F1 AverageLegend disagreement rate (%)
B10 78.56 17.01

B10-further 78.86 (+0.3) 16.25 (�2.06%)
B10-J1x 79.23 (+0.67) 16.03 (�5.76%)
B10-J5x 79.25 (+0.69) 16.01 (�5.88%)

B10-J10x 79.34 (+0.78) 15.88 (-6.64%)
B1 67.28 21.17

B1-further 67.76 (+0.48) 20.75 (�1.98%)
B1-J1x 68.45 (+1.17) 19.57 (�7.56%)
B1-J5x 68.86 (+1.58) 19.38 (-8.46%)
B1-J10x 68.61 (+1.33) 19.29 (�8.88%)

Table 3: Training with joint objective (J) on top of the
baseline models (B1, B10 from Table 1), with the same
SRL-labeled data used to train the baseline models and
with varying sizes of SRL-unlabeled data.

Decoding B10 B10-J10x B1 B1-J5x
Viterbi 78.56 79.34 67.28 68.86

Noisy syntactic constraints

A* 72.95 73.57 63.77 64.73(-5.61) (-5.77) (-3.51) (-4.13)
Gradient- 79.7 80.21 69.85 70.95

based (+1.41) (+0.87) (+2.57) (+2.1)
Noise-free syntactic constraints

A* 78.87 79.51 67.97 68.97(+0.31) (+0.17) (+0.69) (+0.11)
Gradient- 80.18 80.66 69.97 70.94

based (+1.62) (+1.32) (+2.69) (+2.08)

Table 4: Comparison of different decoding techniques:
Viterbi, A* (He et al., 2017) and gradient based infer-
ence (Lee et al., 2017) with noisy and noise-free syn-
tactic constraints1. Note that the (+/-) F1 are reported
w.r.t Viterbi decoding on the same column.

by training models jointly to satisfy syntactic con-
straints helps in better generalization when trained
with limited SRL corpora.

Does SSL based training work for low-resource
scenarios? To enforce syntactic constraints via
SI-loss on SRL-unlabeled data, we further train
pre-trained model with two objectives in SSL set
up: (a) SI-loss (Table 2) and (b) joint objective
(Table 3)

For experiment (a), we use square loss, �kW �
Wpre-traink2 regularizer to keep the model W close
to the pre-trained model Wpre-train to avoid catas-
trophic forgetting (� set to 0.005). We optimize
with SGD with learning rate of 0.01, ↵2 = 1.0,
patience of 10 epochs. We see that with SI-loss
improvements are significant in terms of average
disagreement rate as compared to F1.

For experiment (b), we train B1 and B10 with
joint objective in SSL set-up (as discussed in Sec-
tion 2.5). We use SGD with learning rate of 0.05,
↵1 = ↵2 = 1.0 and patience of 10 epochs. We
report +1.58 F1 and +0.78 F1 improvement over



4962

B1 and B10, trained with 5% and 100% SRL-
unlabeled data, respectively. Note that we can-
not achieve these improvements with simply fine-
tunning BX with supervised loss, as seen with
BX-further on Table 3. This provides evidence
to answer (Q4) favorably. In general, the per-
formance gains increase as the size of the SRL-
unlabeled data increases.

Is it better to enforce syntactic consistency on
decoding or on training time? To answer (Q5),
we conducted three experiments: using syntactic
constraints on (a) inference only, i.e. structured
prediction, (b) training only, and (c) both training
and inference steps. For the structured prediction,
we consider A* decoding, as used in (He et al.,
2017) and gradient-based inference (Lee et al.,
2017), which optimizes loss function similar to SI-
loss on Eq.(5) per example basis. If neither A*
decoding nor gradient-based inference is used, we
use Viterbi algorithm to enforce BIO constraints.
The performance is the best (bold on Table 4)
when syntactic consistency is enforced both on
training and inference steps, +3.67, +2.1 F1 score
improvement over B1 and B10 respectively, and
we conclude that the effort of enforcing syntactic
consistency on inference time is complementary
to the same effort on training time. However, note
that the overall performance increases as the bene-
fit from enforcing syntactic consistency with SSL
is far greater compared to marginal decrement on
structured prediction.
While syntactic constraints help both train and in-
ference, injecting constraints on train time is far
more robust compared to enforcing them on de-
coding time. The performance of the structured
prediction drops rapidly when the noise in the
parse information is introduced (x column of Ta-
ble 4). On the other hand, SSL was trained on
CoNLL2012 data where about 10% of the gold
SRL-spans do not match with gold parse-spans
and even when we increase noise level to 20% the
performance drop was only around 0.1 F1 score.

4 Related Work

The traditional approaches for SRL (Pradhan
et al., 2005; Koomen et al., 2005) constituted of
cascaded system with four subtasks: pruning, ar-
gument identification, argument labeling, and in-
ference. Recent approaches (Zhou and Xu,
2015; He et al., 2017) proposed end-to-end system
for SRL using deep recurrent or bi-LSTM-based

architecture with no syntactic inputs and have
achieved SOTA results on English SRL. Lastly,
(Peters et al., 2018) proposed ELMo, a deep con-
textualized word representations, and improved
the SOTA English SRL by 3.2 F1-points.

Even on the end-to-end learning, inference still
remains as a separate subtask and would be for-
malized as a constrained optimization problem. To
solve this problem ILP (Punyakanok et al., 2008),
A* algorithm (He et al., 2017) and gradient-based
inference (Lee et al., 2017) were employed. Fur-
ther, all of these works leveraged syntactic parse
during inference and was never used during train-
ing unless used as a cascaded system.

To the best of our knowledge, this work is the
first attempt towards SSL span-based SRL model.
Nonetheless, there were few efforts in SSL in
dependency-based SRL systems (Fürstenau and
Lapata, 2009; Deschacht and Moens, 2009; Croce
et al., 2010). (Fürstenau and Lapata, 2009) pro-
posed to augment the dataset by finding similar
unlabeled sentences to already labeled set and an-
notate accordingly. While interesting, the similar
augmentation technique is harder to apply to span-
based SRL as one requires to annotate the whole
span. (Deschacht and Moens, 2009; Croce et al.,
2010) proposed to leverage the relation between
words by learning latent word distribution over the
context, i.e. language model. Our paper also in-
corporates this idea by using ELMo as it is trained
via language model objective.

5 Conclusion and Future Work

We presented a SI-loss to enforce SRL systems
to produce syntactically consistent outputs. Fur-
ther, leveraging the fact that SI-loss does not re-
quire labeled data, we proposed a SSL formulation
with joint objective constituting of SI-loss and su-
pervised loss together. We show the efficacy of
the proposed approach on low resource settings,
+1.58, +0.78 F1 on 1%, 10% SRL-labeled data
respectively, via further training on top of pre-
trained SOTA model. We further show the struc-
tured prediction can be used as a complimentary
tool and show performance gain of +3.67, +2.1 F1
over pre-trained model on 1%, 10% SRL-labeled
data, respectively. Semi-supervised training from
the scratch and examination of semi-supervised
setting on large dataset remains as part of the fu-
ture work.



4963

References
Emanuele Bastianelli, Giuseppe Castellucci, Danilo

Croce, and Roberto Basili. 2013. Textual inference
and meaning representation in human robot interac-
tion. In Proceedings of the Joint Symposium on Se-
mantic Processing. Textual Inference and Structures

in Corpora, pages 65–69.

Danilo Croce, Cristina Giannone, Paolo Annesi, and
Roberto Basili. 2010. Towards open-domain seman-
tic role labeling. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-

guistics, ACL ’10, pages 237–246, Stroudsburg, PA,
USA. Association for Computational Linguistics.

Koen Deschacht and Marie-Francine Moens. 2009.
Semi-supervised semantic role labeling using the la-
tent words language model. In Proceedings of the
2009 Conference on Empirical Methods in Natu-

ral Language Processing: Volume 1 - Volume 1,
EMNLP ’09, pages 21–29, Stroudsburg, PA, USA.
Association for Computational Linguistics.

Hagen Fürstenau and Mirella Lapata. 2009. Semi-
supervised semantic role labeling. In Proceedings of
the 12th Conference of the European Chapter of the

Association for Computational Linguistics, EACL
’09, pages 220–228, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.

Luheng He, Kenton Lee, Mike Lewis, and Luke Zettle-
moyer. 2017. Deep semantic role labeling: What
works and whats next. In Proceedings of the 55th
Annual Meeting of the Association for Computa-

tional Linguistics (Volume 1: Long Papers), vol-
ume 1, pages 473–483.

Paul Kingsbury and Martha Palmer. 2002. From tree-
bank to propbank. In Proceedings of the Third
International Conference on Language Resources

and Evaluation (LREC’02). European Language Re-
sources Association (ELRA).

Peter Koomen, Vasin Punyakanok, Dan Roth, and
Wen-tau Yih. 2005. Generalized inference with
multiple semantic role labeling systems. In Pro-
ceedings of the Ninth Conference on Computational

Natural Language Learning, pages 181–184. Asso-
ciation for Computational Linguistics.

Jay Yoon Lee, Michael Wick, Sanket Vaibhav Mehta,
Jean-Baptiste Tristan, and Jaime Carbonell. 2017.
Gradient-based inference for networks with output
constraints. arXiv preprint arXiv:1707.08608v2.

Mitchell P Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large annotated
corpus of english: The penn treebank. Computa-
tional linguistics, 19(2):313–330.

Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word rep-
resentations. In Proc. of NAACL.

Sameer Pradhan, Kadri Hacioglu, Wayne Ward,
James H Martin, and Daniel Jurafsky. 2005. Seman-
tic role chunking combining complementary syn-
tactic views. In Proceedings of the Ninth Confer-
ence on Computational Natural Language Learning,
pages 217–220. Association for Computational Lin-
guistics.

Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Hwee Tou Ng, Anders Björkelund, Olga Uryupina,
Yuchen Zhang, and Zhi Zhong. 2013. Towards ro-
bust linguistic analysis using ontonotes. In Proceed-
ings of the Seventeenth Conference on Computa-

tional Natural Language Learning, pages 143–152.

Vasin Punyakanok, Dan Roth, and Wen-tau Yih. 2008.
The importance of syntactic parsing and inference in
semantic role labeling. Computational Linguistics,
34(2):257–287.

Dan Shen and Mirella Lapata. 2007. Using semantic
roles to improve question answering. In Proceed-
ings of the 2007 joint conference on empirical meth-

ods in natural language processing and computa-

tional natural language learning (EMNLP-CoNLL).

Jie Zhou and Wei Xu. 2015. End-to-end learning of
semantic role labeling using recurrent neural net-
works. In Proceedings of the 53rd Annual Meet-
ing of the Association for Computational Linguistics

and the 7th International Joint Conference on Natu-

ral Language Processing (Volume 1: Long Papers),
volume 1, pages 1127–1137.


