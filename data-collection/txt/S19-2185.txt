



















































The Sally Smedley Hyperpartisan News Detector at SemEval-2019 Task 4


Proceedings of the 13th International Workshop on Semantic Evaluation (SemEval-2019), pages 1057–1061
Minneapolis, Minnesota, USA, June 6–7, 2019. ©2019 Association for Computational Linguistics

1057

The Sally Smedley Hyperpartisan News Detector at SemEval-2019 Task 4:
Learning Classifiers with Feature Combinations and Ensembling

Kazuaki Hanawa*1,2, Shota Sasaki*1,2, Hiroki Ouchi1,2, Jun Suzuki2,1, Kentaro Inui2,1
(* equal contribution)

1RIKEN AIP, 2Tohoku University
{hanawa, sasaki.shota, hiroki.ouchi, jun.suzuki, inui}

@ecei.tohoku.ac.jp

Abstract

This paper describes our system submitted to
the formal run of SemEval-2019 Task 4: Hy-
perpartisan news detection. Our system is
based on a linear classifier using several fea-
tures, i.e., 1) embedding features based on
the pre-trained BERT embeddings, 2) arti-
cle length features, and 3) embedding fea-
tures of informative phrases extracted from
the by-publisher dataset. Our system
achieved 80.9% accuracy on the test set for
the formal run and got the 3rd place out of 42
teams.

1 Introduction

Hyperpartisan news detection (Kiesel et al., 2019;
Potthast et al., 2018) is a binary classification task,
in which given a news article text, systems have
to decide whether or not it follows a hyperparti-
san argumentation, i.e., “whether it exhibits blind,
prejudiced, or unreasoning allegiance to one party,
faction, cause, or person” (2019). As resources
for building such a system, the by-publisher
and by-article datasets are provided by the
organizer. The by-publisher dataset is a col-
lection of news articles labeled with the over-
all bias of the publisher as provided by Buz-
zFeed journalists or MediaBiasFactCheck.com.
The by-article dataset is a collection labeled
through crowdsourcing on an article basis. This
data contains only the articles whose labels are
agreed by all the crowd-workers. The performance
measure is accuracy on a balanced set of articles.

Our system is based on a linear classifier
using several types of features mainly consist-
ing of 1) embedding features based on the pre-
trained BERT embeddings (Devlin et al., 2018)
and 2) article length features and 3) embedding
features of informative phrases extracted from
by-publisher dataset. Our system achieved

80.9% accuracy on the test set for the formal run
and got 3rd place out of 42 teams in the formal
run.

2 System Description

This section first presents an overview of our sys-
tem and then elaborate on the feature set.

2.1 Overview of Our System
Our system is based on a linear classifier that mod-
els the conditional probability distribution over the
two labels (positive or negative) given features.
Let f be a feature vector. W denotes a trainable
weight matrix, and b is a trainable bias vector,
where f ∈ RD, W ∈ RD×2 and b ∈ R2, respec-
tively. Then, we compute the conditional proba-
bility as follows:

y = softmax(W>f + b). (1)

where, softmax(·) represent the softmax function
that receives an N -dimensional vector x and re-
turns another N dimensional vector, namely:

softmax(x) =
exp(x)∑
i exp(xi)

, (2)

and x = (x1, . . . , xN )>. After the softmax com-
putation, we obtain the two-dimensional vector
y ∈ R2. We assume that the first dimension of
this vector represents the probability of the pos-
itive label, and the second one represents that of
the negative label.

To boost the performance, we concatenate three
types of features, f1, f2, and f3, into the single fea-
ture vector f , where f1 ∈ RD1 , f2 ∈ RD2 and
f3 ∈ RD3 and D = D1 +D2 +D3.

As f1, f2 and f3, we design the following fea-
tures.

• f1: BERT feature (Section 2.2)



1058

• f2: Article length feature (Section 2.3)

• f3: Informative phrase feature (Section 2.4)

For training our classifiers, we used only
the by-article dataset but not the
by-publisher dataset. This is because
the labels of the by-publisher dataset turned
out rather noisy. In our preliminary experiments,
we found that the performance drops when
training the classifiers on the by-publisher
dataset.

Furthermore, we apply the following three tech-
niques.

1. Word dropout: We adopted word dropout
(Iyyer et al., 2015) for regularization. The
dropout rate was set to 0.3.

2. Over sampling: As mentioned above, the
gold label distribution of the training set is
unbalanced while that of the test set is bal-
anced. We deal with this imbalance problem
by sampling 169 (407− 238) extra examples
from hyperpartisan data.

3. Ensemble: We trained 100 models with dif-
ferent random seeds. In addition, we trained
models for 40, 50, 60 and 70 epochs for each
seed. Consequently, we finally average the
output of these 400 models.

2.2 BERT Feature
Our system uses BERT (Devlin et al., 2018). As a
strategy for applying BERT to downstream tasks,
Devlin et al. (2018) recommends a fine-tuning ap-
proach, which fine-tunes the parameters of BERT
on a target task. In contrast, we adopt a feature-
based approach, which uses the hidden states of
the pre-trained BERT in a task-specific model as
input representation. A main advantage of this
approach is computational efficiency. We do not
have to update any parameters of BERT. Once we
calculate a fixed feature vector for an article, we
can reuse it across all the models for ensemble.

In our system, we used BERT to compute a fea-
ture vector f1 for an input article. Specifically, we
first fed an article to the pre-trained BERT model
as input and extracted the representations of all the
words from the top four hidden layers. Then, to
summarize these representations into a single fea-
ture vector f1, we tried the following three meth-
ods.

1. Average: Averaging the representations of all
the words in an article.

2. BiLSTM: Using the representations as input
to BiLSTM. This is the same method as the
best performing one reported by Devlin et al.
(2018).

3. CNN: Using the representations as input to
CNN in the same way as Kim (2014).

As we describe in Section 3.2, we finally adopted
the averaged BERT vectors as f1.

2.3 Article Length Feature

As f2, we design a feature vector representing the
length of an input article. In our preliminary ex-
periments, we found a length bias in hyperparti-
san articles and non-hyperpartisan articles. Thus
a vector representing the length bias is expected
to be useful for discriminating these two types of
articles.

Specifically, we define a one hot feature vec-
tor f2 representing distribution of the lengths of
articles (the number of words in an article). To
represent the length of an article as a vector, we
make use of histogram bins. Consider the 100-
ranged histogram bins. The first bin represents
the length 1 to 100, and the second one repre-
sents the length 101 to 200. If the length of an
article is 255, the value of the third bin (201 to
300) takes 1. In the same way, the third element
of the length vector takes 1 and the others 0, i.e.,
f2 = [0, 0, 1, 0, 0, · · · ]. In our system, we set the
dimension of the vector as D2 = 11, whose last
(11-th) element corresponds to the length longer
than 1000.

2.4 Informative Phrase Feature

In the development set, we found some phrases in-
formative and useful for discriminating whether or
not a given article is hyperpartisan. We extracted
such informative phrases and mapped them to a
feature vector f3. In this section, we first explain
the procedure of extracting informative phrases,
and then we describe how to map them to a fea-
ture vector.

2.4.1 Phrase Set Creation
To create an informative phrase set, we exploit the
by-publisher articles. Basically, we take ad-
vantage of chi-squared statistics ofN -grams (N =
1, 2, 3).

Creation of Sh First, we calculate each chi-
squared value χxi of N -gram xi appearing in the



1059

by-publisher articles as follows:

χxi =
(O − E)2

E
. (3)

O and E are defined as follows:

O = ffalse(xi), (4)

E =
Tfalse × ftrue(xi)

Ttrue
, (5)

where ftrue(·) and ffalse(·) are functions that cal-
culate the frequency of xi in hyperpartisan articles
and non-hyperpartisan articles, respectively. Ttrue
and Tfalse are the summation of the frequency of
all N -grams in hyperpartisan articles and non-
hyperpartisan articles, respectively.

Then, based on the chi-squared values χxi , we
sort and select top-M N -grams. We can obtain a
typical N -gram set (hereinafter, referred to as Sh)
that is informative for judging whether an article
is hyperpartisan or not.1 In our system, we use
M = 200, 000.
Sh can mostly catch the characteristics of hy-

perpartisan articles. However, Sh may include
some N -grams that are typical of a certain pub-
lisher. This is because the by-publisher
dataset is labeled by the overall bias of the pub-
lisher as provided by BuzzFeed journalists or Me-
diaBiasFactCheck.com.

Creation of Sp To remedy this problem, we cre-
ate another phrase set Sp consisting of N -grams
that are typical of a publisher, and exclude them
from Sh.

Here we consider a certain publisher pl. First,
we calculate each chi-squared value of N -gram xi
in the same way as Eq 3,4,5 for Sh, but here we
consider true and false as appearing in articles
of publisher pl and articles of other publishers, re-
spectively, instead of appearing in hyperpartisan
articles and non-hyperpartisan articles. Then, we
pick up only N -gram xi where ftrue(xi) is less
than ffalse(xi) and sort all of them by the chi-
squared values. This is because we aim to ex-
clude only N -grams that are typical of a certain
publisher.

Next, we try four types of ways to create Spl
from sorted χx value statistics. Here j denotes the
index of the N -gram list sorted by χx values, i.e.,
χx1 is the highest value in all calculated χx values.

1Actually, we cut off N -gram x if ftrue(x) is more than
100,000 for Sh.

1. Top-To: The first setting is to select top-To
N -grams. Concretely, Spl is defined as fol-
lows:

Spl = {xj |j ≤ To}. (6)

2. χ-based: The second setting is to select N -
grams based on χ values. Concretely, Spl is
defined as follows:

Spl = {xj |χxj > Tc}. (7)

3. ftrue-based: The third setting is to select N -
grams based on ftrue(xj) values. Concretely,
Spl is defined as follows:

Spl = {xj |ftrue(xj) > Tf , j ≤ Tm}. (8)

4. ftrue-ffalse ratio-based: The fourth setting is
to select N -grams based on ratios between
ftrue and ffalse. Concretely, Spl is defined as
follows:

Spl =

{
xj

∣∣∣∣ ftrue(xj)ffalse(xj) > Tr, j ≤ Tm
}
. (9)

To, Tc, Tf , Tr and Tm are hyper-parameters2.
Next, we obtain Sp defined as follows:

Sp =
⋃
l

Spl . (10)

At last, we obtain an filteredN -gram set S defined
as follows:

S = Sh \ Sp. (11)

2.4.2 Phrase Embedding
We map each of the obtained N -gram phrase set
S to a feature vector f3. We exploited GloVe vec-
tors (Pennington et al., 2014) instead of one-hot
vectors in order to facilitate generalization.

First, we enumerate N -grams included in an ar-
ticle and compute each N -gram vector. Each vec-
tor is the average of GloVe vectors of included
words. For example, the vector for the phrase
“news article” is computed as follows:

GloVe(news) + GloVe(article)

2
.

Here, GloVe(w) denotes the GloVe
(glove.840B3) vector of the word w. Then, we
compute f3 as the average of all N -gram vectors
included in the article.

2In our experiments, we fix Tm to 200,000.
3https://nlp.stanford.edu/projects/

glove/

https://nlp.stanford.edu/projects/glove/
https://nlp.stanford.edu/projects/glove/


1060

Method Accuracy
Average 0.760
BiLSTM 0.712
CNN 0.758

Table 1: Accuracy of hyperpartisan classification for
each operation on BERT vectors.

3 Experiments

3.1 Settings
We trained linear classifiers on the by-article
dataset (not on the by-publisher dataset). In
order to estimate the performance in the test set
with each setting, we conducted 5-fold cross val-
idation on by-article dataset. For optimiza-
tion of the classifiers, we use Adam with learning
rate of 0.001, β1 = 0.9, β2 = 0.999. We set the
minibatch size to 32. Note that we did not take en-
semble approach in the experiments we report in
Section 3.2 and Section 3.3 for efficiency.

3.2 Operation on BERT Vectors
We conducted experiments on each of the
three methods for BERT vectors (BERT-Base,
Uncased4) mentioned in Section 2.2. In this ex-
periment, we only used f1 as a feature vector f ,
i.e., without using f2 and f3.

Table 1 shows the performance in each setting.
The averaging method was the best performance
this time. We therefore decided to adopt average
BERT vectors as f1 for the evaluation of the for-
mal run. In addition, we also used averaged BERT
vectors as f1 in the following experiments.

3.3 Method to Create N -gram Set
As mentioned in Section 2.4, we examined which
method is the best to create an informative N -
gram set S (and f3 derived from them). In this
experiment, we also used f1 and f2 with f3 as a
feature.

Table 2 shows the performance in each setting.
The performance was the best when we adopted
Top-To (To = 100) for Sp creation. We therefore
used f3 created in this setting.

3.4 Ablation
To verify the contribution of each three types of
features, we conducted feature ablation experi-
ments. In addition, we investigated to what extent

4https://github.com/google-research/
bert

Method Accuracy
Top-To (To = 100) 0.777
Top-To (To = 1000) 0.771
χ-based (Tc = 20000) 0.752
ftrue-based (Tf = 50) 0.754
ftrue-based (Tf = 150) 0.764
ftrue-ffalse ratio-based (Tr = 0.5) 0.754
ftrue-ffalse ratio-based (Tr = 0.8) 0.771
ftrue-ffalse ratio-based (Tr = 1.0) 0.756

Table 2: Accuracy of hyperpartisan classification for
each method to create N -gram set.

Features Ensemble Accuracy
f1, f2, f3 true 0.788
f1, f2, f3 false 0.777
f1, f2 false 0.769
f1 false 0.760

Table 3: Result of ablation.

ensemble approach improve the performance. In
this experiments, we use only 10 (not 100) differ-
ent random seeds for ensemble due to time con-
straints.

Table 3 shows the performance in each setting.
We found that f2 and f3 improved the accuracy by
about 0.01, respectively. Additionally, by using
the ensemble method, the accuracy increased by
about 0.01.

4 Conclusion

We described our system submitted to the formal
run of SemEval-2019 Task 4: Hyperpartisan news
detection. We trained a linear classifier using sev-
eral features mainly consisting of 1) BERT embed-
ding features, 2) article length features indicating
the distribution of lengths of articles and 3) em-
bedding features derived from filtered N -grams
that are typically found in hyperpartisan articles.
Our system achieved 80.9% accuracy on the test
set for the formal run and got 3rd place out of 42
teams.

Acknowledgments

This work was supported by JST CREST Grant
Number JPMJCR1301, Japan.

https://github.com/google-research/bert
https://github.com/google-research/bert


1061

References
2019. Pan @ SemEval 2019 - Hyperpartisan

News Detection. https://pan.webis.de/
semeval19/semeval19-web/index.html.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. arXiv preprint arXiv:1810.04805.

Mohit Iyyer, Varun Manjunatha, Jordan Boyd-Graber,
and Hal Daumé III. 2015. Deep unordered compo-
sition rivals syntactic methods for text classification.
In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the
7th International Joint Conference on Natural Lan-
guage Processing, volume 1, pages 1681–1691.

Johannes Kiesel, Maria Mestre, Rishabh Shukla, Em-
manuel Vincent, Payam Adineh, David Corney,
Benno Stein, and Martin Potthast. 2019. Semeval-
2019 task 4: Hyperpartisan news detection. In Pro-
ceedings of The 13th International Workshop on Se-
mantic Evaluation (SemEval).

Yoon Kim. 2014. Convolutional neural networks for
sentence classification. In Proceedings of the 2014
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 1746–1751.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. GloVe: Global vectors for
word representation. In Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1532–
1543.

Martin Potthast, Johannes Kiesel, Kevin Reinartz,
Janek Bevendorff, and Benno Stein. 2018. A sty-
lometric inquiry into hyperpartisan and fake news.
In Proceedings of 56th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
231–240.

https://pan.webis.de/semeval19/semeval19-web/index.html
https://pan.webis.de/semeval19/semeval19-web/index.html

