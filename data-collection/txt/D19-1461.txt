











































Build it Break it Fix it for Dialogue Safety: Robustness from Adversarial Human Attack


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 4537–4546,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

4537

Build it Break it Fix it for Dialogue Safety:

Robustness from Adversarial Human Attack

Emily Dinan

Facebook AI Research
edinan@fb.com

Samuel Humeau

Facebook AI Research
samuel.humeau@gmail.com

Bharath Chintagunta

Virginia Tech
jaic4@vt.edu

Jason Weston

Facebook AI Research
jase@fb.com

Abstract

The detection of offensive language in the con-
text of a dialogue has become an increasingly
important application of natural language pro-
cessing. The detection of trolls in public fo-
rums (Galán-Garcı́a et al., 2016), and the de-
ployment of chatbots in the public domain
(Wolf et al., 2017) are two examples that show
the necessity of guarding against adversarially
offensive behavior on the part of humans. In
this work, we develop a training scheme for a
model to become robust to such human attacks
by an iterative build it, break it, fix it strategy
with humans and models in the loop. In de-
tailed experiments we show this approach is
considerably more robust than previous sys-
tems. Further, we show that offensive lan-
guage used within a conversation critically de-
pends on the dialogue context, and cannot be
viewed as a single sentence offensive detec-
tion task as in most previous work. Our newly
collected tasks and methods are all made open
source and publicly available.

1 Introduction

The detection of offensive language has become
an important topic as the online community has
grown, as so too have the number of bad actors
(Cheng et al., 2017). Such behavior includes, but
is not limited to, trolling in public discussion fo-
rums (Herring et al., 2002) and via social media
(Silva et al., 2016; Davidson et al., 2017), employ-
ing hate speech that expresses prejudice against
a particular group, or offensive language specif-
ically targeting an individual. Such actions can
be motivated to cause harm from which the bad
actor derives enjoyment, despite negative conse-
quences to others (Bishop, 2014). As such, some
bad actors go to great lengths to both avoid detec-
tion and to achieve their goals (Shachaf and Hara,
2010). In that context, any attempt to automat-
ically detect this behavior can be expected to be

adversarially attacked by looking for weaknesses
in the detection system, which currently can eas-
ily be exploited as shown in (Hosseini et al., 2017;
Gröndahl et al., 2018). A further example, rele-
vant to the natural langauge processing commu-
nity, is the exploitation of weaknesses in machine
learning models that generate text, to force them
to emit offensive language. Adversarial attacks
on the Tay chatbot led to the developers shutting
down the system (Wolf et al., 2017).

In this work, we study the detection of offen-
sive language in dialogue with models that are ro-
bust to adversarial attack. We develop an auto-
matic approach to the “Build it Break it Fix it”
strategy originally adopted for writing secure pro-
grams (Ruef et al., 2016), and the “Build it Break
it” approach consequently adapting it for NLP (Et-
tinger et al., 2017). In the latter work, two teams of
researchers, “builders” and “breakers” were used
to first create sentiment and semantic role-labeling
systems and then construct examples that find their
faults. In this work we instead fully automate such
an approach using crowdworkers as the humans-
in-the-loop, and also apply a fixing stage where
models are retrained to improve them. Finally, we
repeat the whole build, break, and fix sequence
over a number of iterations.

We show that such an approach provides more
and more robust systems over the fixing iterations.
Analysis of the type of data collected in the itera-
tions of the break it phase shows clear distribution
changes, moving away from simple use of profan-
ity and other obvious offensive words to utterances
that require understanding of world knowledge,
figurative language, and use of negation to detect
if they are offensive or not. Further, data collected
in the context of a dialogue rather than a sentence
without context provides more sophisticated at-
tacks. We show that model architectures that use
the dialogue context efficiently perform much bet-



4538

ter than systems that do not, where the latter has
been the main focus of existing research (Wulczyn
et al., 2017; Davidson et al., 2017; Zampieri et al.,
2019).

Code for our entire build it, break it, fix it algo-
rithm is made open source, complete with model
training code and crowdsourcing interface for hu-
mans. Our data and trained models will also be
made available for the community via ParlAI1.

2 Related Work

The task of detecting offensive language has been
studied across a variety of content classes. Perhaps
the most commonly studied class is hate speech,
but work has also covered bullying, aggression,
and toxic comments (Zampieri et al., 2019).

To this end, various datasets have been cre-
ated to benchmark progress in the field. In hate
speech detection, recently Davidson et al. (2017)
compiled and released a dataset of over 24,000
tweets labeled as containing hate speech, offen-
sive language, or neither. In order to benchmark
toxic comment detection, The Wikipedia Toxic
Comments dataset (which we study in this work)
was collected and extracted from Wikipedia Talk
pages and featured in a Kaggle competition (Wul-
czyn et al., 2017; Google, 2018). Each of these
benchmarks examine only single-turn utterances,
outside of the context in which the language ap-
peared. In this work we recommend that future
systems should move beyond classification of sin-
gular utterances and use contextual information to
help identify offensive language.

Many approaches have been taken to solve these
tasks – from linear regression and SVMs to deep
learning (Noever, 2018). The best performing sys-
tems in each of the competitions mentioned above
(for aggression and toxic comment classification)
used deep learning approaches such as LSTMs and
CNNs (Kumar et al., 2018; Google, 2018). In this
work we consider a large-pretrained transformer
model which has been shown to perform well on
many downstream NLP tasks (Devlin et al., 2018).

The broad class of adversarial training is cur-
rently a hot topic in machine learning (Goodfel-
low et al., 2014). Use cases include training im-
age generators (Brock et al., 2018) as well as im-
age classifiers to be robust to adversarial examples
(Liu et al., 2019). These methods find the break-

1https://parl.ai/projects/dialogue_
safety/

ing examples algorithmically, rather than by us-
ing humans breakers as we do. Applying the same
approaches to NLP tends to be more challenging
because, unlike for images, even small changes to
a sentence can cause a large change in the mean-
ing of that sentence, which a human can detect but
a lower quality model cannot. Nevertheless algo-
rithmic approaches have been attempted, for ex-
ample in text classification (Ebrahimi et al., 2018),
machine translation (Belinkov and Bisk, 2018), di-
alogue generation tasks (Li et al., 2017) and read-
ing comprehension (Jia and Liang, 2017). The lat-
ter was particularly effective at proposing a more
difficult version of the popular SQuAD dataset.

As mentioned in the introduction, our approach
takes inspiration from “Build it Break it” ap-
proaches which have been successfully tried in
other domains (Ruef et al., 2016; Ettinger et al.,
2017). Those approaches advocate finding faults
in systems by having humans look for insecurities
(in software) or prediction failures (in models), but
do not advocate an automated approach as we do
here. Our work is also closely connected to the
“Mechanical Turker Descent” algorithm detailed
in (Yang et al., 2018) where language to action
pairs were collected from crowdworkers by incen-
tivizing them with a game-with-a-purpose tech-
nique: a crowdworker receives a bonus if their
contribution results in better models than another
crowdworker. We did not gamify our approach
in this way, but still our approach has common-
alities in the round-based improvement of models
through crowdworker interaction.

3 Baselines: Wikipedia Toxic Comments

In this section we describe the publicly available
data that we have used to bootstrap our build it
break it fix it approach. We also compare our
model choices with existing work and clarify the
metrics chosen to report our results.

Wikipedia Toxic Comments The Wikipedia
Toxic Comments dataset (WTC) has been col-
lected in a common effort from the Wikimedia
Foundation and Jigsaw (Wulczyn et al., 2017) to
identify personal attacks online. The data has
been extracted from the Wikipedia Talk pages, dis-
cussion pages where editors can discuss improve-
ments to articles or other Wikipedia pages. We
considered the version of the dataset that corre-
sponds to the Kaggle competition: “Toxic Com-
ment Classification Challenge” (Google, 2018)

https://parl.ai/projects/dialogue_safety/
https://parl.ai/projects/dialogue_safety/


4539

which features 7 classes of toxicity: toxic, se-
vere toxic, obscene, threat, insult, identity hate and
non-toxic. In the same way as in (Khatri et al.,
2018), every label except non-toxic is grouped into
a class OFFENSIVE while the non-toxic class is
kept as the SAFE class. In order to compare our
results to (Khatri et al., 2018), we similarly split
this dataset to dedicate 10% as a test set. 80% are
dedicated to train set while the remaining 10% is
used for validation. Statistics on the dataset are
shown in Table 1.

Models We establish baselines using two mod-
els. The first one is a binary classifier built on top
of a large pre-trained transformer model. We use
the same architecture as in BERT (Devlin et al.,
2018). We add a linear layer to the output of the
first token ([CLS]) to produce a final binary classi-
fication. We initialize the model using the weights
provided by (Devlin et al., 2018) corresponding
to “BERT-base”. The transformer is composed of
12 layers with hidden size of 768 and 12 atten-
tion heads. We fine-tune the whole network on the
classification task. We also compare it the fastText
classifier (Joulin et al., 2017) for which a given
sentence is encoded as the average of individual
word vectors that are pre-trained on a large cor-
pus issued from Wikipedia. A linear layer is then
applied on top to yield a binary classification.

Experiments We compare the two aforemen-
tioned models with (Khatri et al., 2018) who con-
ducted their experiments with a BiLSTM with
GloVe pre-trained word vectors (Pennington et al.,
2014). Results are listed in Table 2 and we com-
pare them using the weighted-F1, i.e. the sum
of F1 score of each class weighted by their fre-
quency in the dataset. We also report the F1 of
the OFFENSIVE-class which is the metric we fa-
vor within this work, although we report both.
(Note that throughout the paper, the notation F1
is always referring to OFFENSIVE-class F1.) In-
deed, in the case of an imbalanced dataset such
as Wikipedia Toxic Comments where most sam-
ples are SAFE, the weighted-F1 is closer to the F1
score of the SAFE class while we focus on detect-
ing OFFENSIVE content. Our BERT-based model
outperforms the method from Khatri et al. (2018);
throughout the rest of the paper, we use the BERT-
based architecture in our experiments. In particu-
lar, we used this baseline trained on WTC to boot-
strap our approach, to be described subsequently.

Train Valid Test

SAFE 89.8% 89.7% 90.1%
OFFENSIVE 10.2% 10.3% 9.1%
Total 114656 15958 15957

Table 1: Dataset statistics for our splits of Wikipedia
Toxic Comments.

OFFENSIVE F1 Weighted F1

fastText 71.4% 94.8%
BERT-based 83.4% 96.7%
(Khatri et al., 2018) - 95.4%

Table 2: Comparison between our models based on
fastText and BERT with the BiLSTM used by (Khatri
et al., 2018) on Wikipedia Toxic Comments.

4 Build it Break it Fix it Method

In order to train models that are robust to adver-
sarial behavior, we posit that it is crucial collect
and train on data that was collected in an adversar-
ial manner. We propose the following automated
build it, break it, fix it algorithm:

1. Build it: Build a model capable of detect-
ing OFFENSIVE messages. This is our best-
performing BERT-based model trained on
the Wikipedia Toxic Comments dataset de-
scribed in the previous section. We refer to
this model throughout as A0.

2. Break it: Ask crowdworkers to try to “beat
the system” by submitting messages that our
system (A0) marks as SAFE but that the
worker considers to be OFFENSIVE.

3. Fix it: Train a new model on these collected
examples in order to be more robust to these
adversarial attacks.

4. Repeat: Repeat, deploying the newly trained
model in the break it phase, then fix it again.

See Figure 1 for a visualization of this process.

4.1 Break it Details

Definition of OFFENSIVE Throughout data col-
lection, we characterize OFFENSIVE messages for
users as messages that would not be “ok to send
in a friendly conversation with someone you just
met online.” We use this specific language in an
attempt to capture various classes of content that
would be considered unacceptable in a friendly



4540

A0
SAFE

OFFENSIVE

Build it 

Existing data

training

Not broken: try again!

Adversarial data

Broken: add to new dataset

training

prediction

deploy

deploy

Offensive Message

breaker

A0

A1

A0 + A1

Break it 
(ROUND 2) 

Fix it 

prediction

training

Offensive Message

breaker

Break it 
(ROUND 1) (ROUND 1) 

Figure 1: The build it, break it, fix it algorithm we use
to iteratively train better models A0, . . . , AN . In exper-
iments we perform N = 3 iterations of the break it, fix
it loop for the single-turn utterance detection task, and
a further iteration for the multi-turn task in a dialogue
context setting.

conversation, without imposing our own defini-
tions of what that means. The phrase “with some-
one you just met online” was meant to mimic the
setting of a public forum.

Crowderworker Task We ask crowdworkers to
try to “beat the system” by submitting messages
that our system marks as SAFE but that the worker
considers to be OFFENSIVE. For a given round,
workers earn a “game” point each time they are
able to “beat the system,” or in other words, trick
the model by submitting OFFENSIVE messages
that the model marks as SAFE. Workers earn up
to 5 points each round, and have two tries for each
point: we allow multiple attempts per point so that
workers can get feedback from the models and bet-
ter understand their weaknesses. The points serve
to indicate success to the crowdworker and mo-
tivate to achieve high scores, but have no other
meaning (e.g. no monetary value as in (Yang et al.,
2018)). More details regarding the user interface
and instructions can be found in Appendix B.

Models to Break During round 1, workers try to
break the baseline model A0, trained on Wikipedia
Toxic Comments. For rounds i, i > 1, workers
must break both the baseline model and the model
from the previous “fix it” round, which we refer
to as Ai�1. In that case, the worker must submit
messages that both A0 and Ai�1 mark as SAFE but
which the worker considers to be OFFENSIVE.

4.2 Fix it Details

During the “fix it” round, we update the models
with the newly collected adversarial data from the
“break it” round. The training data consists of all
previous rounds of data, so that model Ai is trained
on all rounds n for n  i, as well as the Wikipedia
Toxic Comments data. We split each round of data
into train, validation, and test partitions. The val-
idation set is used for hyperparameter selection.
The test sets are used to measure how robust we
are to new adversarial attacks. With increasing
round i, Ai should become more robust to increas-
ingly complex human adversarial attacks.

5 Single-Turn Task

We first consider a single-turn set-up, i.e. detec-
tion of offensive language in one utterance, with
no dialogue context or conversational history.

5.1 Data Collection

Adversarial Collection We collected three
rounds of data with the build it, break it, fix it
algorithm described in the previous section. Each
round of data consisted of 1000 examples, leading
to 3000 single-turn adversarial examples in total.
For the remainder of the paper, we refer to this
method of data collection as the adversarial
method.

Standard Collection In addition to the adver-
sarial method, we also collected data in a non-
adversarial manner in order to directly compare
the two set-ups. In this method – which we refer
to as the standard method, we simply ask crowd-
workers to submit messages that they consider to
be OFFENSIVE. There is no model to break. In-
structions are otherwise the same.

In this set-up, there is no real notion of
“rounds”, but for the sake of comparison we re-
fer to each subsequent 1000 examples collected in
this manner as a “round”. We collect 3000 exam-
ples – or three rounds of data. We refer to a model
trained on rounds n  i of the standard data as Si.



4541

Single-Turn Adversarial (Round 1) and Standard Task OFFENSIVE Examples

contains non-profane contains contains requires contains
profanity offending words negation figurative language world knowledge sarcasm

Standard 13% 12% 12% 11% 8% 3%
Adversarial 0% 5% 23% 19% 14% 6%

Table 3: Language analysis of the single-turn standard and adversarial (round 1) tasks by human annotation of
various language properties. Standard collection examples contain more words found in an offensive words list,
while adversarial examples require more sophisticated language understanding.

5.1.1 Task Formulation Details

Since all of the collected examples are labeled
as OFFENSIVE, to make this task a binary clas-
sification problem, we will also add SAFE exam-
ples to it. The “safe data” is comprised of ut-
terances from the ConvAI2 chit-chat task (Dinan
et al., 2019; Zhang et al., 2018) which consists
of pairs of humans getting to know each other by
discussing their interests. Each utterance we used
was reviewed by two independent crowdworkers
and labeled as SAFE, with the same characteriza-
tion of SAFE as described before. For each par-
tition (train, validation, test), the final task has a
ratio of 9:1 SAFE to OFFENSIVE examples, mim-
icking the division of the Wikipedia Toxic Com-
ments dataset used for training our baseline mod-
els. Dataset statistics for the final task can be
found in Table 5. We refer to these tasks – with
both SAFE and OFFENSIVE examples – as the ad-
versarial and standard tasks.

5.1.2 Model Training Details

Using the BERT-based model architecture de-
scribed in Section 3, we trained models on each
round of the standard and adversarial tasks,
multi-tasking with the Wikipedia Toxic Comments
task. We weight the multi-tasking with a mixing
parameter which is also tuned on the validation
set. Finally, after training weights with the cross
entropy loss, we adjust the final bias also using the
validation set. We optimize for the sensitive class
(i.e. OFFENSIVE-class) F1 metric on the standard
and adversarial validation sets respectively.

For each task (standard and adversarial), on
round i, we train on data from all rounds n for
n  i and optimize for performance on the valida-
tion sets n  i.

5.2 Experimental Results

We conduct experiments comparing the adversar-
ial and standard methods. We break down the re-
sults into “break it” results comparing the data col-

% with % with avg. # avg. #
profanity “not” chars tokens

Std. (Rnds 1-3) 18.2 2.8 48.6 9.4
Adv. Rnd 1 2.6 5.8 53.7 10.3
Adv. Rnd 2 1.5 5.5 44.5 9
Adv. Rnd 3 1.2 9.8 45.7 9.3
Multi-turn Adv. 1.6 4.9 36.6 7.8

Table 4: Percent of OFFENSIVE examples in each task
containing profanity, the token “not”, as well as the av-
erage number of characters and tokens in each exam-
ple. Rows 1-4 are the single-turn task, and the last row
is the multi-turn task. Later rounds have less profan-
ity and more use of negation as human breakers have
to find more sophisticated language to adversarially at-
tack our models.

Rounds {1, 2 and 3} Train Valid Test
SAFE Examples 21,600 2700 2700
OFFENSIVE Examples 2400 300 300
Total Examples 24,000 3,000 3,000

Table 5: Dataset statistics for the single-turn rounds of
the adversarial task data collection. There are three
rounds in total all of identical size, hence the numbers
above can be divided for individual statistics. The stan-
dard task is an additional dataset of exactly the same
size as above.

lected and “fix it” results comparing the models
obtained.

5.2.1 Break it Phase

Examples obtained from both the adversarial and
standard collection methods were found to be
clearly offensive, but we note several differences
in the distribution of examples from each task,
shown in Table 4. First, examples from the stan-
dard task tend to contain more profanity. Using a
list of common English obscenities and otherwise
bad words2, in Table 4 we calculate the percentage
of examples in each task containing such obscen-
ities, and see that the standard examples contain

2https://github.com/LDNOOBW/List-of-Dirty-Naughty-
Obscene-and-Otherwise-Bad-Words



4542

WTC Baseline Standard models Adversarial models

Task Type Task Round A0 S1 S2 S3 A1 A2 A3

WTC - 83.3 80.6 81.1 82.1 81.3 78.9 78.0

Standard Task All (1-3) 68.1 83.3 85.8 88.0 83.0 85.3 83.7

Adversarial Task

1 0.0 51.7 69.3 68.6 71.8 79.0 78.2
2 0.0 10.8 26.4 31.8 0.0 64.4 62.1
3 0.0 12.3 17.1 13.7 32.1 0.0 59.9
All (1-3) 0.0 27.4 41.7 41.8 40.6 55.5 67.6

Table 6: Test performance of best standard models trained on standard task rounds (models Si for each round
i) and best adversarial models trained on adversarial task rounds (models Ai). All models are evaluated using
OFFENSIVE-class F1 on each round of both the standard task and adversarial task. A0 is the baseline model
trained on the existing Wiki Toxic Comments (WTC) dataset. Adversarial models prove to be more robust than
standard ones against attack (Adversarial Task 1-3), while still performing reasonably on Standard and WTC tasks.

at least seven times as many as each round of the
adversarial task. Additionally, in previous works,
authors have observed that classifiers struggle with
negations (Hosseini et al., 2017). This is borne
out by our data: examples from the single-turn ad-
versarial task more often contain the token “not”
than examples from the standard task, indicating
that users are easily able to fool the classifier with
negations.

We also anecdotally see figurative language
such as “snakes hiding in the grass” in the ad-
versarial data, which contain no individually of-
fensive words, the offensive nature is captured by
reading the entire sentence. Other examples re-
quire sophisticated world knowledge such as that
many cultures consider eating cats to be offen-
sive. To quantify these differences, we performed
a blind human annotation of a sample of the data,
100 examples of standard and 100 examples of ad-
versarial round 1. Results are shown in Table 3.
Adversarial data was indeed found to contain less
profanity, fewer non-profane but offending words
(such as “idiot”), more figurative language, and to
require more world knowledge.

We note that, as anticipated, the task becomes
more challenging for the crowdworkers with each
round, indicated by the decreasing average scores
in Table 7. In round 1, workers are able to get past
A0 most of the time – earning an average score
of 4.56 out of 5 points per round – showcasing
how susceptible this baseline is to adversarial at-
tack despite its relatively strong performance on
the Wikipedia Toxic Comments task. By round
3, however, workers struggle to trick the system,
earning an average score of only 1.6 out of 5. A
finer-grained assessment of the worker scores can

Single-Turn Multi

Round 1 2 3 (“4”)

Avg. score (0-5) 4.56 2.56 1.6 2.89

Table 7: Adversarial data collection worker scores.
Workers received a score out of 5 indicating how often
(out of 5 rounds) they were able to get past our clas-
sifiers within two tries. In later single-turn rounds it is
harder to defeat our models, but switching to multi-turn
makes this easier again as new attacks can be found by
using the dialogue context.

be found in Table 11 in the appendix.

5.2.2 Fix it Phase

Results comparing the performance of models
trained on the adversarial (Ai) and standard (Si)
tasks are summarized in Table 6, with further re-
sults in Table 13 in Appendix A.2. The adversar-
ially trained models Ai prove to be more robust
to adversarial attack: on each round of adversarial
testing they outperform standard models Si.

Further, note that the adversarial task becomes
harder with each subsequent round. In particu-
lar, the performance of the standard models Si
rapidly deteriorates between round 1 and round
2 of the adversarial task. This is a clear indi-
cation that models need to train on adversarially-
collected data to be robust to adversarial behavior.

Standard models (Si), trained on the standard
data, tend to perform similarly to the adversarial
models (Ai) as measured on the standard test sets,
with the exception of training round 3, in which
A3 fails to improve on this task, likely due to be-
ing too optimized for adversarial tasks. The stan-
dard models Si, on the other hand, are improving



4543

with subsequent rounds as they have more training
data of the same distribution as the evaluation set.
Similarly, our baseline model performs best on its
own test set, but other models are not far behind.

Finally, we remark that all scores of 0 in Table
6 are by design, as for round i of the adversarial
task, both A0 and Ai�1 classified each example as
SAFE during the ‘break it’ data collection phase.

6 Multi-Turn Task

In most real-world applications, we find that ad-
versarial behavior occurs in context – whether it
is in the context of a one-on-one conversation, a
comment thread, or even an image. In this work
we focus on offensive utterances within the con-
text of two-person dialogues. For dialogue safety
we posit it is important to move beyond classify-
ing single utterances, as it may be the case that
an utterance is entirely innocuous on its own but
extremely offensive in the context of the previous
dialogue history. For instance, “Yes, you should
definitely do it!” is a rather inoffensive message
by itself, but most would agree that it is a hurtful
response to the question “Should I hurt myself?”

6.1 Task Implementation

To this end, we collect data by asking crowdwork-
ers to try to “beat” our best single-turn classifier
(using the model that performed best on rounds
1-3 of the adversarial task, i.e., A3), in addition
to our baseline classifier A0. The workers are
shown truncated pieces of a conversation from the
ConvAI2 chit-chat task, and asked to continue the
conversation with OFFENSIVE responses that our
classifier marks as SAFE. The resulting conversa-
tions – including the newly provided OFFENSIVE
responses – are between 3 and 6 turns long. As be-
fore, workers have two attempts per conversation
to try to get past the classifier and are shown five
conversations per round. They are given a score
(out of five) at the end of each round indicating
the number of times they successfully fooled the
classifier.

We collected 3000 offensive examples in this
manner. As in the single-turn set up, we com-
bine this data with SAFE examples with a ratio
of 9:1 SAFE to OFFENSIVE for classifier training.
The safe examples are dialogue examples from
ConvAI2 for which the responses were reviewed
by two independent crowdworkers and labeled as
SAFE, as in the s single-turn task set-up. We refer

to this overall task as the multi-turn adversarial
task. Dataset statistics are given in Table 9.

6.2 Models

To measure the impact of the context, we train
models on this dataset with and without the given
context. We use the fastText and the BERT-based
model described in Section 3. In addition, we
build a BERT-based model variant that splits the
last utterance (to be classified) and the rest of the
history into two dialogue segments. Each segment
is assigned an embedding and the input provided
to the transformer is the sum of word embedding
and segment embedding, replicating the setup of
the Next Sentence Prediction that is used in the
training of BERT (Devlin et al., 2018).

6.3 Experimental Results

6.3.1 Break it Phase

During data collection, we observed that workers
had an easier time bypassing the classifiers than in
the single-turn set-up. See Table 7. In the single-
turn set-up, the task at hand gets harder with each
round – the average score of the crowdworkers de-
creases from 4.56 in round 1 to 1.6 in round 3. De-
spite the fact that we are using our best single-turn
classifier in the multi-turn set-up (A3), the task be-
comes easier: the average score per round is 2.89.
This is because the workers are often able to use
contextual information to suggest something of-
fensive rather than say something offensive out-
right. See examples of submitted messages in Ta-
ble 8. Having context also allows one to express
something offensive more efficiently: the mes-
sages supplied by workers in the multi-turn setting
were significantly shorter on average, see Table 4.

6.3.2 Fix it Phase

During training, we multi-tasked the multi-turn
adversarial task with the Wikipedia Toxic Com-
ments task as well as the single-turn adversarial
and standard tasks. We average the results of our
best models from five different training runs. The
results of these experiments are given in Table 10.

As we observed during the training of our base-
lines in Section 3, the fastText model architec-
ture is ill-equipped for this task relative to our
BERT-based architectures. The fastText model
performs worse given the dialogue context (an
average of 23.56 OFFENSIVE-class F1 relative
to 37.1) than without, likely because its bag-of-



4544

Multi-Turn Adversarial Task Examples

Context [P1:] greetings and salutations my friend
[P2:] hi how are you doing?
[P1:] i’m well. enjoying some r r from the navy station
[P2:] i love to cook. do you cook?
[P1:] i do on occasion. i love to read more though.

OFFENSIVE Response [P2:] reading the menu doesnt count

Context [P1:] hey how are you
[P2:] good but i keep thinking about the time i almost played for nba
[P1:] aww why did not you play
[P2:] lost my nerve i guess. what do you do?
[P1:] oh that’s to bad. i work at a gas station

OFFENSIVE Response [P2:] i need to follow my dream to not work in a gas station

Table 8: Examples from the multi-turn adversarial task. Responses can be offensive only in context.

Multi-Turn Adversarial Task Train Valid Test

SAFE Examples 21,600 2,700 2,700
OFFENSIVE Examples 2,400 300 300
Total Examples 24,000 3,000 3,000

Table 9: Multi-turn adversarial task data statistics.

Multi-Turn Adversarial Task Results

F1 Weighted-F1

fastText

with context 23.6 ± 1.9 85.9 ± 0.5
without context 37.1 ± 2.6 88.8 ± 0.6

BERT-based

(no segments)
with context 60.5 ± 1.3 92.2 ± 0.3
without context 56.8 ± 1.6 90.6 ± 0.7

BERT-based

(dialogue segments)
with context 66.4 ± 2.2 93.2 ± 0.4
without context 59.0 ± 2.5 91.2 ± 0.8

Table 10: Results of experiments on the multi-turn ad-
versarial task. We denote the average and one stan-
dard deviation from the results of five runs. Models that
use the context as input (“with context”) perform bet-
ter. Encoding this in the architecture as well (via BERT
dialogue segment features) gives us the best results.

embeddings representation is too simple to take
the context into account.

We see the opposite with our BERT-based mod-
els, indicating that more complex models are able
to effectively use the contextual information to
detect whether the response is SAFE or OFFEN-
SIVE. With the simple BERT-based architecture
(that does not split the context and the utterance
into separate segments), we observe an average of
a 3.7 point increase in OFFENSIVE-class F1 with
the addition of context. When we use segments

to separate the context from the utterance we are
trying to classify, we observe an average of a 7.4
point increase in OFFENSIVE-class F1. Thus, it
appears that the use of contextual information to
identify OFFENSIVE language is critical to making
these systems robust, and improving the model ar-
chitecture to take account of this has large impact.

7 Conclusion

We have presented an approach to build more ro-
bust offensive language detection systems in the
context of a dialogue. We proposed a build it,
break it, fix it, and then repeat strategy, whereby
humans attempt to break the models we built, and
we use the broken examples to fix the models. We
show this results in far more nuanced language
than in existing datasets. The adversarial data in-
cludes less profanity, which existing classifiers can
pick up on, and is instead offensive due to figu-
rative language, negation, and by requiring more
world knowledge, which all make current classi-
fiers fail. Similarly, offensive language in the con-
text of a dialogue is also more nuanced than stand-
alone offensive utterances. We show that classi-
fiers that learn from these more complex examples
are indeed more robust to attack, and that using
the dialogue context gives improved performance
if the model architecture takes it into account.

In this work we considered a binary problem
(offensive or safe). Future work could consider
classes of offensive language separately (Zampieri
et al., 2019), or explore other dialogue tasks, e.g.
from social media or forums. Another interesting
direction is to explore how our build it, break it, fix
it strategy would similarly apply to make neural
generative models safe (Henderson et al., 2018).



4545

Acknowledgments

Thank you to Stephen Roller for providing in-
sights and ideas for this project. The emoji image
in Figure 1 is by Twemoji3, and is licensed under
CC BY-4.0.

References

2018. 6th International Conference on Learning Rep-
resentations, ICLR 2018, Vancouver, BC, Canada,
April 30 - May 3, 2018, Conference Track Proceed-
ings. OpenReview.net.

Yonatan Belinkov and Yonatan Bisk. 2018. Synthetic
and natural noise both break neural machine transla-
tion. In (DBL, 2018).

Jonathan Bishop. 2014. Representations of trolls in
mass media communication: a review of media-
texts and moral panics relating to internet trolling.
International Journal of Web Based Communities,
10(1):7–24.

Andrew Brock, Jeff Donahue, and Karen Simonyan.
2018. Large scale gan training for high fi-
delity natural image synthesis. arXiv preprint
arXiv:1809.11096.

Justin Cheng, Cristian Danescu-Niculescu-Mizil, Jure
Leskovec, and Michael Bernstein. 2017. Anyone
can become a troll: Causes of trolling behavior in
online discussions. American Scientist, 105(3):152.

Thomas Davidson, Dana Warmsley, Michael Macy,
and Ingmar Weber. 2017. Automated hate speech
detection and the problem of offensive language.
In Eleventh International AAAI Conference on Web
and Social Media.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. CoRR, abs/1810.04805.

Emily Dinan, Varvara Logacheva, Valentin Malykh,
Alexander Miller, Kurt Shuster, Jack Urbanek,
Douwe Kiela, Arthur Szlam, Iulian Serban, Ryan
Lowe, et al. 2019. The second conversational
intelligence challenge (convai2). arXiv preprint
arXiv:1902.00098.

Javid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing
Dou. 2018. Hotflip: White-box adversarial exam-
ples for text classification. In Proceedings of the
56th Annual Meeting of the Association for Com-
putational Linguistics, ACL 2018, Melbourne, Aus-
tralia, July 15-20, 2018, Volume 2: Short Papers,
pages 31–36. Association for Computational Lin-
guistics.

3https://github.com/twitter/twemoji

Allyson Ettinger, Sudha Rao, Hal Daumé III, and
Emily M Bender. 2017. Towards linguistically gen-
eralizable nlp systems: A workshop and shared task.
arXiv preprint arXiv:1711.01505.

Patxi Galán-Garcı́a, José Gaviria de la Puerta, Car-
los Laorden Gómez, Igor Santos, and Pablo Garcı́a
Bringas. 2016. Supervised machine learning for the
detection of troll profiles in twitter social network:
Application to a real case of cyberbullying. Logic
Journal of the IGPL, 24(1):42–53.

Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,
Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. 2014. Generative ad-
versarial nets. In Advances in neural information
processing systems, pages 2672–2680.

Google. 2018. Toxic comment classification challenge.

Tommi Gröndahl, Luca Pajola, Mika Juuti, Mauro
Conti, and N Asokan. 2018. All you need is” love”:
Evading hate-speech detection. arXiv preprint
arXiv:1808.09115.

Peter Henderson, Koustuv Sinha, Nicolas Angelard-
Gontier, Nan Rosemary Ke, Genevieve Fried, Ryan
Lowe, and Joelle Pineau. 2018. Ethical challenges
in data-driven dialogue systems. In Proceedings of
the 2018 AAAI/ACM Conference on AI, Ethics, and
Society, AIES ’18, pages 123–129, New York, NY,
USA. ACM.

Susan Herring, Kirk Job-Sluder, Rebecca Scheckler,
and Sasha Barab. 2002. Searching for safety online:
Managing” trolling” in a feminist forum. The infor-
mation society, 18(5):371–384.

Hossein Hosseini, Sreeram Kannan, Baosen Zhang,
and Radha Poovendran. 2017. Deceiving google’s
perspective api built for detecting toxic comments.
arXiv preprint arXiv:1702.08138.

Robin Jia and Percy Liang. 2017. Adversarial exam-
ples for evaluating reading comprehension systems.
In (Palmer et al., 2017), pages 2021–2031.

Armand Joulin, Edouard Grave, Piotr Bojanowski, and
Tomas Mikolov. 2017. Bag of tricks for efficient
text classification. In Proceedings of the 15th Con-
ference of the European Chapter of the Association
for Computational Linguistics: Volume 2, Short Pa-
pers, pages 427–431. Association for Computational
Linguistics.

Chandra Khatri, Behnam Hedayatnia, Rahul Goel,
Anushree Venkatesh, Raefer Gabriel, and Arindam
Mandal. 2018. Detecting offensive content in
open-domain conversations using two stage semi-
supervision. CoRR, abs/1811.12900.

Ritesh Kumar, Atul Kr. Ojha, Shervin Malmasi, and
Marcos Zampieri. 2018. Benchmarking aggression
identification in social media. In Proceedings of the
First Workshop on Trolling, Aggression and Cyber-
bullying (TRAC-2018), pages 1–11, Santa Fe, New

https://openreview.net/group?id=ICLR.cc/2018/Conference
https://openreview.net/group?id=ICLR.cc/2018/Conference
https://openreview.net/group?id=ICLR.cc/2018/Conference
https://openreview.net/group?id=ICLR.cc/2018/Conference
https://openreview.net/forum?id=BJ8vJebC-
https://openreview.net/forum?id=BJ8vJebC-
https://openreview.net/forum?id=BJ8vJebC-
https://aclanthology.info/papers/P18-2006/p18-2006
https://aclanthology.info/papers/P18-2006/p18-2006
https://github.com/twitter/twemoji
https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge
https://doi.org/10.1145/3278721.3278777
https://doi.org/10.1145/3278721.3278777
https://aclanthology.info/papers/D17-1215/d17-1215
https://aclanthology.info/papers/D17-1215/d17-1215
http://arxiv.org/abs/1811.12900
http://arxiv.org/abs/1811.12900
http://arxiv.org/abs/1811.12900
https://www.aclweb.org/anthology/W18-4401
https://www.aclweb.org/anthology/W18-4401


4546

Mexico, USA. Association for Computational Lin-
guistics.

Jiwei Li, Will Monroe, Tianlin Shi, Sébastien Jean,
Alan Ritter, and Dan Jurafsky. 2017. Adversarial
learning for neural dialogue generation. In (Palmer
et al., 2017), pages 2157–2169.

Aishan Liu, Xianglong Liu, Jiaxin Fan, Yuqing Ma,
Anlan Zhang, Huiyuan Xie, and Dacheng Tao. 2019.
Perceptual-sensitive gan for generating adversarial
patches.

David Noever. 2018. Machine learning suites
for online toxicity detection. arXiv preprint
arXiv:1810.01869.

Martha Palmer, Rebecca Hwa, and Sebastian Riedel,
editors. 2017. Proceedings of the 2017 Confer-
ence on Empirical Methods in Natural Language
Processing, EMNLP 2017, Copenhagen, Denmark,
September 9-11, 2017. Association for Computa-
tional Linguistics.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. In Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1532–
1543.

Andrew Ruef, Michael Hicks, James Parker, Dave
Levin, Michelle L Mazurek, and Piotr Mardziel.
2016. Build it, break it, fix it: Contesting secure
development. In Proceedings of the 2016 ACM
SIGSAC Conference on Computer and Communica-
tions Security, pages 690–703. ACM.

Pnina Shachaf and Noriko Hara. 2010. Beyond vandal-
ism: Wikipedia trolls. Journal of Information Sci-
ence, 36(3):357–370.

Leandro Silva, Mainack Mondal, Denzil Correa,
Fabrı́cio Benevenuto, and Ingmar Weber. 2016. An-
alyzing the targets of hate in online social media.
In Tenth International AAAI Conference on Web and
Social Media.

Marty J Wolf, K Miller, and Frances S Grodzinsky.
2017. Why we should have seen that coming: com-
ments on microsoft’s tay experiment, and wider im-
plications. ACM SIGCAS Computers and Society,
47(3):54–64.

Ellery Wulczyn, Nithum Thain, and Lucas Dixon.
2017. Ex machina: Personal attacks seen at scale.
In Proceedings of the 26th International Conference
on World Wide Web, WWW 2017, Perth, Australia,
April 3-7, 2017, pages 1391–1399. ACM.

Zhilin Yang, Saizheng Zhang, Jack Urbanek, Will
Feng, Alexander H. Miller, Arthur Szlam, Douwe
Kiela, and Jason Weston. 2018. Mastering the dun-
geon: Grounded language learning by mechanical
turker descent. In (DBL, 2018).

Marcos Zampieri, Shervin Malmasi, Preslav Nakov,
Sara Rosenthal, Noura Farra, and Ritesh Kumar.
2019. Semeval-2019 task 6: Identifying and cate-
gorizing offensive language in social media (offen-
seval). arXiv preprint arXiv:1903.08983.

Saizheng Zhang, Emily Dinan, Jack Urbanek, Arthur
Szlam, Douwe Kiela, and Jason Weston. 2018. Per-
sonalizing dialogue agents: I have a dog, do you
have pets too? In Proceedings of the 56th Annual
Meeting of the Association for Computational Lin-
guistics, ACL 2018, Melbourne, Australia, July 15-
20, 2018, Volume 1: Long Papers, pages 2204–2213.
Association for Computational Linguistics.

https://aclanthology.info/papers/D17-1230/d17-1230
https://aclanthology.info/papers/D17-1230/d17-1230
https://aclanthology.info/volumes/proceedings-of-the-2017-conference-on-empirical-methods-in-natural-language-processing
https://aclanthology.info/volumes/proceedings-of-the-2017-conference-on-empirical-methods-in-natural-language-processing
https://aclanthology.info/volumes/proceedings-of-the-2017-conference-on-empirical-methods-in-natural-language-processing
https://aclanthology.info/volumes/proceedings-of-the-2017-conference-on-empirical-methods-in-natural-language-processing
http://www.aclweb.org/anthology/D14-1162
http://www.aclweb.org/anthology/D14-1162
https://doi.org/10.1145/3038912.3052591
https://openreview.net/forum?id=SJ-C6JbRW
https://openreview.net/forum?id=SJ-C6JbRW
https://openreview.net/forum?id=SJ-C6JbRW
https://aclanthology.info/papers/P18-1205/p18-1205
https://aclanthology.info/papers/P18-1205/p18-1205
https://aclanthology.info/papers/P18-1205/p18-1205

