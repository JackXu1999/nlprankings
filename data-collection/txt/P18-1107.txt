



















































Prefix Lexicalization of Synchronous CFGs using Synchronous TAG


Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1160–1170
Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics

1160

Prefix Lexicalization of Synchronous CFGs using Synchronous TAG

Logan Born and Anoop Sarkar
Simon Fraser University

School of Computing Science
{loborn,anoop}@sfu.ca

Abstract

We show that an ε-free, chain-free syn-
chronous context-free grammar (SCFG)
can be converted into a weakly equiva-
lent synchronous tree-adjoining grammar
(STAG) which is prefix lexicalized. This
transformation at most doubles the gram-
mar’s rank and cubes its size, but we show
that in practice the size increase is only
quadratic. Our results extend Greibach
normal form from CFGs to SCFGs and
prove new formal properties about SCFG,
a formalism with many applications in nat-
ural language processing.

1 Introduction

Greibach normal form (GNF; Greibach, 1965)
is an important construction in formal language
theory which allows every context-free grammar
(CFG) to be rewritten so that the first charac-
ter of each rule is a terminal symbol. A gram-
mar in GNF is said to be prefix lexicalized, be-
cause the prefix of every production is a lexi-
cal item. GNF has a variety of theoretical and
practical applications, including for example the
proofs of the famous theorems due to Shamir and
Chomsky-Schützenberger (Shamir, 1967; Chom-
sky and Schützenberger, 1963; Autebert et al.,
1997). Other applications of prefix lexicaliza-
tion include proving coverage of parsing algo-
rithms (Gray and Harrison, 1972) and decidability
of equivalence problems (Christensen et al., 1995).

By using prefix lexicalized synchronous
context-free grammars (SCFGs), Watanabe et al.
(2006) and Siahbani et al. (2013) obtain asymp-
totic and empirical speed improvements on a
machine translation task. Using a prefix lexical-
ized grammar ensures that target sentences can be
generated from left to right, which allows the use
of beam search to constrain their decoder’s search
space as it performs a left-to-right traversal of
translation hypotheses. To achieve these results,

new grammars had to be heuristically constrained
to include only prefix lexicalized productions, as
there is at present no way to automatically convert
an existing SCFG to a prefix lexicalized form.

This work investigates the formal properties of
prefix lexicalized synchronous grammars as em-
ployed by Watanabe et al. (2006) and Siahbani
et al. (2013), which have received little theoreti-
cal attention compared to non-synchronous prefix
lexicalized grammars. To this end, we first prove
that SCFG is not closed under prefix lexicaliza-
tion. Our main result is that there is a method
for prefix lexicalizing an SCFG by converting it
to an equivalent grammar in a different formal-
ism, namely synchronous tree-adjoining grammar
(STAG) in regular form. Like the GNF transfor-
mation for CFGs, our method at most cubes the
grammar size, but we show empirically that the
size increase is only quadratic for grammars used
in existing NLP tasks. The rank is at most dou-
bled, and we maintain O(n3k) parsing complex-
ity for grammars of rank k. We conclude that al-
though SCFG does not have a prefix lexicalized
normal form like GNF, our conversion to prefix
lexicalized STAG offers a practical alternative.

2 Background

2.1 SCFG
An SCFG is a tuple G = (N,Σ, P, S) where N is
a finite nonterminal alphabet, Σ is a finite termi-
nal alphabet, S ∈ N is a distinguished nontermi-
nal called the start symbol, and P is a finite set of
synchronous rules of the form

(1) 〈A1 → α1, A2 → α2〉

for some A1, A2 ∈ N and strings α1, α2 ∈ (N ∪
Σ)∗.1 Every nonterminal which appears in α1

1A variant formalism exists which requires thatA1 = A2;
this is called syntax-directed transduction grammar (Lewis
and Stearns, 1968) or syntax-directed translation schemata
(Aho and Ullman, 1969). This variant is weakly equivalent
to SCFG, but SCFG has greater strong generative capacity
(Crescenzi et al., 2015).



1161

A 1

A↓ 2a

B 2

B↓ 1b

A

cA∗

B

d
〈 〉 〈 〉

〈 A
A

a A ↓ 1

c ,

B 1

b B

d

〉
Figure 1: An example of synchronous rewriting in an STAG (left) and the resulting tree pair (right).

must be linked to exactly one nonterminal in α2,
and vice versa. We write these links using numer-
ical annotations, as in (2).

(2) 〈A→ A 1 B 2 , B → B 2 A 1 〉

An SCFG has rank k if no rule in the grammar
contains more than k pairs of linked nodes.

In every step of an SCFG derivation, we rewrite
one pair of linked nonterminals with a rule from
P , in essentially the same way we would rewrite
a single nonterminal in a non-synchronous CFG.
For example, (3) shows linked A and B nodes be-
ing rewritten using (2):
(3)
〈X 1 A 2 , B 2 Y 1 〉 ⇒ 〈X 1 A 2 B 3 , B 3 A 2 Y 1 〉

Note how the 1 and 2 in rule (2) are renumbered
to 2 and 3 during rewriting, to avoid an ambigu-
ity with the 1 already present in the derivation.

An SCFG derivation is complete when it con-
tains no more nonterminals to rewrite. A com-
pleted derivation represents a string pair generated
by the grammar.

2.2 STAG
An STAG (Shieber, 1994) is a tuple G =
(N,Σ, T, S) where N is a finite nonterminal al-
phabet, Σ is a finite terminal alphabet, S ∈ N is a
distinguished nonterminal called the start symbol,
and T is a finite set of synchronous tree pairs of
the form

(4) 〈t1, t2〉

where t1 and t2 are elementary trees as defined in
Joshi et al. (1975). A substitution site is a leaf
node marked by ↓ which may be rewritten by an-
other tree; a foot node is a leaf marked by ∗ that
may be used to rewrite a tree-internal node. Ev-
ery substitution site in t1 must be linked to ex-
actly one nonterminal in t2, and vice versa. As
in SCFG, we write these links using numbered an-
notations; rank is defined for STAG the same way
as for SCFG.

In every step of an STAG derivation, we rewrite
one pair of linked nonterminals with a tree pair
from T , using the same substitution and adjunc-
tion operations defined for non-synchronous TAG.
For example, Figure 1 shows linked A and B
nodes being rewritten and the tree pair resulting
from this operation. See Joshi et al. (1975) for de-
tails about the underlying TAG formalism.

2.3 Terminology

We use synchronous production as a cover term
for either a synchronous rule in an SCFG or a syn-
chronous tree pair in an STAG.

Following Siahbani et al. (2013), we refer to
the left half of a synchronous production as the
source side, and the right half as the target side;
this terminology captures the intuition that syn-
chronous grammars model translational equiva-
lence between a source phrase and its translation
into a target language. Other authors refer to
the two halves as the left and right components
(Crescenzi et al., 2015) or, viewing the grammar
as a transducer, the input and the output (Engel-
friet et al., 2017).

We call a grammar ε-free if it contains no pro-
ductions whose source or target side produces only
the empty string ε.

2.4 Synchronous Prefix Lexicalization

Previous work (Watanabe et al., 2006; Siahbani
et al., 2013) has shown that it is useful for the tar-
get side of a synchronous grammar to start with a
terminal symbol. For this reason, we define a syn-
chronous grammar to be prefix lexicalized when
the leftmost character of the target side2 of every
synchronous production in that grammar is a ter-
minal symbol.

Formally, this means that every synchronous
rule in a prefix lexicalized SCFG (PL-SCFG) is

2All of the proofs in this work admit a symmetrical variant
which prefix lexicalizes the source side instead of the target.
We are not aware of any applications in NLP where source-
side prefix lexicalization is useful, so we do not address this
case.



1162

of the form

(5) 〈A1 → α1, A2 → aα2〉

whereA1, A2 ∈ N , α1, α2 ∈ (N∪Σ)∗ and a ∈ Σ.
Every synchronous tree pair in a prefix lexical-

ized STAG (PL-STAG) is of the form

(6)

〈
A1

α1

,
A2

aα2

〉
whereA1, A2 ∈ N , α1, α2 ∈ (N∪Σ)∗ and a ∈ Σ.

3 Closure under Prefix Lexicalization

We now prove that the class SCFG is not closed
under prefix lexicalization.

Theorem 1. There exists an SCFG which cannot
be converted to an equivalent PL-SCFG.

Proof. The SCFG in (7) generates the language
L = {〈aibjci, bjai〉| i ≥ 0, j ≥ 1}, but this lan-
guage cannot be generated by any PL-SCFG:

(7)

〈S → A 1 , S → A 1 〉
〈A→ aA 1 c, A→ A 1 a〉
〈A→ bB 1 , A→ bB 1 〉
〈A→ b, A→ b〉
〈B → bB 1 , B → bB 1 〉
〈B → b, B → b〉

Suppose, for the purpose of contradiction, that
some PL-SCFG does generate L; call this gram-
mar G. Then the following derivations must all be
possible in G for some nontermials U, V,X, Y :

i) 〈U 1 , V 1 〉 ⇒∗ 〈bkU 1 bm, bnV 1 bp〉,
where k +m = n+ p and n ≥ 1

ii) 〈X 1 , Y 1 〉 ⇒∗ 〈aqX 1 cq, arY 1 as〉,
where q = r + s and r ≥ 1

iii) 〈S 1 , S 1 〉 ⇒∗ 〈α1X 1 α2, bα3Y 1 α4〉,
where α1, ..., α4 ∈ (N ∪ Σ)∗

iv) 〈X 1 , Y 1 〉 ⇒∗ 〈α5U 1 α6, α7V 1 α8〉,
where α5, α6, α8 ∈ (N ∪ Σ)∗, α7 ∈ Σ(N ∪
Σ)∗

i and ii follow from the same arguments used
in the pumping lemma for (non-synchronous)
context free languages (Bar-Hillel et al., 1961):
strings in L can contain arbitrarily many as, bs,
and cs, so there must exist some pumpable cycles

which generate these characters. In i, k + m =
n + p because the final derived strings must con-
tain an equal number of bs, and n ≥ 1 because
G is prefix lexicalized; in ii the constraints on q, r
and s follow likewise from L. iii follows from the
fact that, in order to pump on the cycle in ii, this
cycle must be reachable from the start symbol. iv
follows from the fact that a context-free produc-
tion cannot generate a discontinuous span. Once
the cycle in i has generated a b, it is impossible
for ii to generate an a on one side of the b and a c
on the other. Therefore i must always be derived
strictly later than ii, as shown in iv.

Now we obtain a contradiction. Given that G
can derive all of i through iv, the following deriva-
tion is also possible:
(8)

〈S 1 , S 1 〉
⇒∗ 〈α1X 1 α2, bα3Y 1 α4〉
⇒∗ 〈α1aqX 1 cqα2, bα3arY 1 asα4〉
⇒∗ 〈α1aqα5U 1 α6cqα2, bα3arα7V 1 α8asα4〉
⇒∗ 〈α1aqα5bkU 1 bmα6cqα2,

bα3a
rα7b

nV 1 bpα8a
sα4〉

But since n, r ≥ 1, the target string derived this
way contains an a before a b and does not belong
to L.

This is a contradiction: if G is a PL-SCFG then
it must generate i through iv, but if so then it also
generates strings which do not belong to L. Thus
no PL-SCFG can generate L, and SCFG must not
be closed under prefix lexicalization. �

There also exist grammars which cannot be pre-
fix lexicalized because they contain cyclic chain
rules. If an SCFG can derive something of the
form 〈X 1 , Y 1 〉 ⇒∗ 〈xX 1 , Y 1 〉, then it can
generate arbitrarily many symbols in the source
string without adding anything to the target string.
Prefix lexicalizing the grammar would force it to
generate some terminal symbol in the target string
at each step of the derivation, making it unable
to generate the original language where a source
string may be unboundedly longer than its corre-
sponding target. We call an SCFG chain-free if it
does not contain a cycle of chain rules of this form.
The remainder of this paper focuses on chain-free
grammars, like (7), which cannot be converted to
PL-SCFG despite containing no such cycles.

4 Prefix Lexicalization using STAG

We now present a method for prefix lexicalizing
an SCFG by converting it to an STAG.



1163

〈X 1 , A 1 〉 ⇒ 〈α1Y1 1 β1, B1 1 γ1〉 ⇒ 〈α1α2Y2 1 β2β1, B2 1 γ2γ1〉

⇒∗ 〈α1 · · ·αtYt 1 βt · · ·β1, Bt 1 γt · · · γ1〉 ⇒ 〈α1 · · ·αtαt+1βt · · ·β1, aγt+1γt · · · γ1〉

Figure 2: A target-side terminal leftmost derivation. a ∈ Σ; X,A, Yi, Bi ∈ N ; and αi, βi, γi ∈ (N ∪ Σ)∗.

〈
SXA

α1

,
SXA

aα2

〉
(a) 〈X → α1, A→ aα2〉

〈 SXA
YXA 1

α1

,
SXA

aα2BXA ↓ 1

〉
(b) 〈Y → α1, B → aα2〉〈 ZXA

YXA 1

α1 ZXA∗ β1

,
CXA

α2 BXA ↓ 1

〉
(c) 〈Y → α1Z 1 β1, B → C 1 α2〉

〈
YXA

α1 YXA∗ β1

,
CXA

α2

〉
(d) 〈X → α1Y 1 β1, A→ C 1 α2〉

Figure 3: Tree-pairs in GXA and the rules in G from which they derive.

Theorem 2. Given a rank-k SCFG G which is ε-
free and chain-free, an STAGH exists such thatH
is prefix lexicalized and L(G) = L(H). The rank
of H is at most 2k, and |H| = O(|G|3).

Proof. Let G = (N,Σ, P, S) be an ε-free, chain-
free SCFG. We provide a constructive method for
prefix lexicalizing the target side of G.

We begin by constructing an intermediate
grammar GXA for each pair of nonterminals
X,A ∈ N \ {S}. For each pair X,A ∈ N \ {S},
GXA will be constructed to generate the language
of sentential forms derivable from 〈X 1 , A 1 〉
via a target-side terminal leftmost derivation
(TTLD). A TTLD is a derivation of the form
in Figure 2, where the leftmost nontermi-
nal in the target string is expanded until it
produces a terminal symbol as the first char-
acter. We write 〈X 1 , A 1 〉 ⇒∗TTLD 〈u, v〉
to mean that 〈X 1 , A 1 〉 derives 〈u, v〉 by
way of a TTLD; in this notation, LXA =
{〈u, v〉|〈X 1 , A 1 〉 ⇒∗TTLD 〈u, v〉} is the
language of sentential forms derivable from
〈X 1 , A 1 〉 via a TTLD.

Given X,A ∈ N \ {S} we formally de-
fine GXA as an STAG over the terminal alpha-
bet ΣXA = N ∪ Σ and nonterminal alphabet
NXA = {YXA|Y ∈ N}, with start symbol SXA.
NXA contains nonterminals indexed by XA to
ensure that two intermediate grammars GXA and
GY B do not interact as long as 〈X,A〉 6= 〈Y,B〉.

GXA contains four kinds of tree pairs: 3

• For each rule in G of the form
〈X → α1, A→ aα2〉, a ∈ Σ, αi ∈ (N∪Σ)∗,
we add a tree pair of the form in Figure 3(a).

• For each rule in G of the form
〈Y → α1, B → aα2〉, a ∈ Σ, αi ∈ (N∪Σ)∗,
Y,B ∈ N \ {S}, we add a tree pair of the
form in Figure 3(b).

• For each rule in G of the form
〈Y → α1Z 1 β1, B → C 1 α2〉, Y, Z,B,C ∈
N \ {S}, αi, βi ∈ (N ∪ Σ)∗, we add a tree
pair of the form in Figure 3(c).

As a special case, if Y = Z we collapse the
root node and adjunction site to produce a
tree pair of the following form:

(9)

〈
ZXA 1

α1ZXA ∗ β1

,
CXA

α2BXA ↓ 1

〉
• For each rule in G of the form
〈X → α1Y 1 β1, A→ C 1 α2〉, Y,C ∈ N ,
αi, βi ∈ (N ∪ Σ)∗, we add a tree pair of the
form in Figure 3(d).

3In all cases, we assume that symbols inN (notNXA) re-
tain any links they bore in the original grammar, even though
they belong to the terminal alphabet in GXA and therefore
do not participate in rewriting operations. In the final con-
structed grammar, these symbols will belong to the nonter-
minal alphabet again, and the links will function normally.



1164

〈A→ B 2 cA 1 , A→ A 1 cB 2 〉〈
AAA 1

B ↓ 2 c AAA∗

,
AAA

c B ↓ 2 AAA ↓ 1

〉
Figure 4: An SCFG rule and a tree pair based off that rule,
taken from an intermediate grammar GAA. The tree pair is
formed according to the pattern illustrated in Figure 3(c). Ob-
serve that theB nodes retain the link they bore in the original
rule. This link is not functional in the intermediate grammar
(that is, it cannot be used for synchronous rewriting) because
B /∈ NAA, but it will be functional when this tree pair is
added to the final grammar H .

Figure 4 gives a concrete example of construct-
ing an intermediate grammar tree pair on the basis
of an SCFG rule.

Lemma 1. GXA generates the language LXA.

Proof. This can be shown by induction over
derivations of increasing length. The proof is
straightforward but very long, so we provide only
a sketch; the complete proof is provided in the sup-
plementary material.

As a base case, observe that a tree of the shape
in Figure 3(a) corresponds straightforwardly to the
derivation

(10) 〈X 1 , A 1 〉 ⇒ 〈α1, aα2〉

which is a TTLD starting from 〈X,A〉. By con-
struction, therefore, every TTLD of the shape in
(10) corresponds to some tree in GXA of shape
3(a); likewise every derivation inGXA comprising
a single tree of shape 3(a) corresponds to a TTLD
of the shape in (10).

As a second base case, note that a tree of the
shape in Figure 3(b) corresponds to the last step of
a TTLD like (11):

(11) 〈X 1 , A 1 〉 ⇒∗TTLD 〈uY 1 v,B 1 w〉
⇒ 〈uα1v, aα2w〉

In the other direction, the last step of any TTLD
of the shape in (11) will involve some rule of the
shape 〈Y → α1, B → aα2〉; by construction
GXA must contain a corresponding tree pair of
shape 3(b).

Together, these base cases establish a one-to-
one correspondence between single-tree deriva-
tions in GXA and the last step of a TTLD starting
from 〈X,A〉.

Now, assume that the last n steps of every
TTLD starting from 〈X,A〉 correspond to some
derivation over n trees in GXA, and vice versa.
Then the last n + 1 steps of that TTLD will also
correspond to some n+ 1 tree derivation in GXA,
and vice versa.

To see this, consider the step n+ 1 steps before
the end of the TTLD. This step may be in the mid-
dle of the derivation, or it may be the first step of
the derivation. If it is in the middle, then this step
must involve a rule of the shape

(12) 〈Y → α1Z 1 β1, B → C 1 α2〉

The existence of such a rule in G implies the exis-
tence of a corresponding tree in GXA of the shape
in Figure 3(c). Adding this tree to the existing
n-tree derivation yields a new n + 1 tree deriva-
tion corresponding to the last n + 1 steps of the
TTLD.4 In the other direction, if the n+ 1th tree5

of a derivation in GXA is of the shape in Figure
3(c), then this implies the existence of a produc-
tion in G of the shape in (12). By assumption the
first n trees of the derivation in GXA correspond
to some TTLD in G; by prepending the rule from
(12) to this TTLD we obtain a new TTLD of length
n + 1 which corresponds to the entire n + 1 tree
derivation in GXA.

Finally, consider the case where the TTLD is
only n + 1 steps long. The first step must involve
a rule of the form

(13) 〈X → α1Y 1 β1, A→ C 1 α2〉

The existence of such a rule implies the existence
of a corresponding tree in GXA of the shape in
Figure 3(d). Adding this tree to the derivation
which corresponds to the last n steps of the TTLD
yields a new n+1 tree derivation corresponding to
the entire n+ 1 step TTLD. In the other direction,
if the last tree of an n + 1 tree derivation in GA
is of the shape in Figure 3(d), then this implies the

4It is easy to verify by inspection of Figure 3 that when-
ever one rule from G can be applied to the output of another
rule, then the tree pairs in GXA which correspond to these
rules can compose with one another. Thus we can add the
new tree to the existing derivation and be assured that it will
compose with one of the trees that is already present.

5Although trees in GXA may contain symbols from the
nonterminal alphabet of G, these symbols belong to the ter-
minal alphabet in GXA. Only nonterminals in NXA will
be involved in this derivation, and by construction there is
at most one such nonterminal per tree. Thus a well-formed
derivation structure in GXA will never branch, and we can
refer to the n+ 1th tree pair as the one which is at depth n in
the derivation structure.



1165

existence of a production inG of the shape in (13).
By assumption the first n trees of the derivation in
GXA correspond to some TTLD inG; by prepend-
ing the rule from (13) to this TTLD we obtain a
new TTLD of length n + 1 which corresponds to
the entire n+ 1 tree derivation in GXA.

Taken together, these cases establish a one-to-
one correspondence between derivations in GXA
and TTLDs which start from 〈X,A〉; in turn they
confirm that GXA generates the desired language
LXA.

Once we have constructed an intermediate
grammar GXA for each X,A ∈ N \ {S}, we ob-
tain the final STAG H as follows:

1. Convert the input SCFG G to an equivalent
STAG. For each rule 〈A1 → α1, A2 → α2〉,
where Ai ∈ N , αi ∈ (N ∪ Σ)∗, create a tree
pair of the form

(14)

〈
A1

α1

,
A2

α2

〉
where each pair of linked nonterminals in the
original rule become a pair of linked substi-
tution sites in the tree pair. The terminal and
nonterminal alphabets and start symbol are
unchanged. Call the resulting STAG H .

2. For all X,A ∈ N \ {S}, add all of the tree
pairs from the intermediate grammar GXA to
the new grammar H . Expand N to include
the new nonterminal symbols in NXA.

3. For every X,A ∈ N , in all tree pairs where
the target tree’s leftmost leaf is labeled with
A and this node is linked to anX , replace this
occurrence of A with SXA. Also replace the
linked node in the source tree.

4. For every X,A ∈ N , let RXA be the set of
all tree pairs rooted in SXA, and let TXA be
the set of all tree pairs whose target tree’s
leftmost leaf is labeled with SXA. For ev-
ery 〈s, t〉 ∈ TXA and every 〈s′, t′〉 ∈ RXA,
substitute or adjoin s′ and t′ into the linked
SXA nodes in s and t, respectively. Add the
derived trees to H .

5. For all X,A ∈ N , let TXA be defined as
above. Remove all tree pairs in TXA from
H .

6. For all X,A ∈ N , let RXA be defined as
above. Remove all tree pairs in RXA from
H .

We now claim that H generates the same lan-
guage as the original grammar G, and all of the
target trees in H are prefix lexicalized.

The first claim follows directly from the con-
struction. Step 1 merely rewrites the grammar in
a new formalism. From Lemma 1 it is clear that
steps 2–3 do not change the generated language:
the set of string pairs generable from a pair of SXA
nodes is identical to the set generable from 〈X,A〉
in the original grammar. Step 4 replaces some
nonterminals by all possible alternatives; steps 5–
6 then remove the trees which were used in step 4,
but since all possible combinations of these trees
have already been added to the grammar, remov-
ing them will not alter the language.

The second claim follows from inspection of the
tree pairs generated in Figure 3. Observe that, by
construction, for all X,A ∈ N every target tree
rooted in SXA is prefix lexicalized. Thus the trees
created in step 4 are all prefix lexicalized variants
of non-lexicalized tree pairs; steps 5–6 then re-
move the non-lexicalized trees from the grammar.

�

Figure 5 gives an example of this transformation
applied to a small grammar. Note how A nodes at
the left edge of the target trees end up rewritten as
SAA nodes, as per step 4 of the transformation.

5 Complexity & Formal Properties
Our conversion generates a subset of the class of
prefix lexicalized STAGs in regular form, which
we abbreviate to PL-RSTAG (regular form for
TAG is defined in Rogers 1994). This section dis-
cusses some formal properties of PL-RSTAG.

Generative Capacity PL-RSTAG is weakly
equivalent to the class of ε-free, chain-free
SCFGs: this follows immediately from the proof
that our transformation does not change the lan-
guage generated by the input SCFG. Note that ev-
ery TAG in regular form generates a context-free
language (Rogers, 1994).

Alignments and Reordering PL-RSTAG gen-
erates the same set of reorderings (alignments) as
SCFG. Observe that our transformation does not
cause nonterminals which were linked in the orig-
inal grammar to become unlinked, as noted for ex-
ample in Figure 4. Thus subtrees which are gener-



1166

〈S → B 2 cA 1 , S → A 1 cB 2 〉
〈A→ B 2 cA 1 , A→ A 1 cB 2 〉
〈A→ a, A→ a〉
〈B → b, B → b〉

〈 S
B ↓ 1 c SAA

a

,

S

SAA

a

c B ↓ 1

〉 〈 A
B ↓ 1 c SAA

AAA 2

a

,

A

SAA

a AAA ↓ 2

c B ↓ 1

〉
〈 S

B ↓ 1 c SAA

AAA 2

a

,

S

SAA

a AAA ↓ 2

c B ↓ 1

〉 〈 A
B ↓ 1 c SAA

a

,

A

SAA

a

c B ↓ 1

〉 〈
AAA 1

B ↓ 2 c AAA∗

,
AAA

c B ↓ 2 AAA ↓ 1

〉
〈

AAA

B ↓ 1 c AAA∗

,
AAA

c B ↓ 1

〉 〈
B

b

,
B

b

〉 〈
A

a

,
A

a

〉
Figure 5: An SCFG and the STAG which prefix lexicalizes it. Non-productive trees have been omitted.

Grammar |G| |H| % of G prefix lexicalized log|G|(|H|)
Siahbani and Sarkar (2014a) (Zh-En) 18.5M 23.6T 63% 1.84
Example (7) 6 14 66% 1.47
ITG (10000 translation pairs) 10,003 170,000 99.97% 1.31

Table 1: Grammar sizes before and after prefix lexicalization, showing O(n2) size increase instead of the worst case O(n3).
|G| and |H| give the grammar size before and after prefix lexicalization; log|G| |H| is the increase as a power of the initial size.
We also show the percentage of productions which are already prefix lexicalized in G.

ated by linked nonterminals in the original gram-
mar will still be generated by linked nonterminals
in the final grammar, so no reordering information
is lost or added.6 This result holds despite the fact
that our transformation is only applicable to chain-
free grammars: chain rules cannot introduce any
reorderings, since by definition they involve only
a single pair of linked nonterminals.

Grammar Rank If the input SCFG G has rank
k, then the STAG H produced by our transforma-
tion has rank at most 2k. To see this, observe that
the construction of the intermediate grammars in-
creases the rank by at most 1 (see Figure 3(b)).
When a prefix lexicalized tree is substituted at the
left edge of a non-lexicalized tree, the link on the
substitution site will be consumed, but up to k+ 1
new links will be introduced by the substituting
tree, so that the final tree will have rank at most
2k.

In the general case, rank-k STAG is more pow-
erful than rank-k SCFG; for example, a rank-4
SCFG is required to generate the reordering in
〈S → A 1 B 2 C 3 D 4 , S → C 3 A 1 D 4 B 2 〉
(Wu, 1997), but this reordering is captured by the

6Although we consume one link whenever we substitute a
prefix lexicalized tree at the left edge of an unlexicalized tree,
that link can still be remembered and used to reconstruct the
reorderings which occurred between the two sentences.

following rank-3 STAG:〈 S
X

A ↓ 1 X 2

C ↓ 3

,
S

C ↓ 3 A ↓ 1 X ↓ 2

〉
〈

X

B ↓ 1 X∗ D ↓ 2

,
X

D ↓ 2 B ↓ 1

〉
For this reason, we speculate that it is possible to
further transform the grammars produced by our
lexicalization in order to reduce their rank, but
the details of this transformation remain as future
work.

This potentially poses a solution to an issue
raised by Siahbani and Sarkar (2014b). On a
Chinese-English translation task, they find that
sentences like (15) involve reorderings which can-
not be captured by a rank-2 prefix lexicalized
SCFG:
(15)
Tā bǔchōng shuō , liánhé zhèngfǔ mùqián zhuàngkuàng wěndı̀ng ...

He added that the coalition government is now in stable condition ...

If rank-k PL-RSTAG is more powerful than rank-k



1167

SCFG, using a PL-RSTAG here would permit cap-
turing more reorderings without using grammars
of higher rank.

Parse Complexity Because the grammar pro-
duced is in regular form, each side can be parsed
in time O(n3) (Rogers, 1994), for an overall parse
complexity of O(n3k), where n is sentence length
and k is the grammar rank.

Grammar Size and Experiments If H is the
PL-RSTAG produced by applying our transforma-
tion to an SCFG G, then H contains O(|G|3) el-
ementary tree pairs, where |G| is the number of
synchronous productions in G. When the set of
nonterminalsN is small compared to |G|, a tighter
bound is given by O(|G|2|N |2).

Table 1 shows the actual size increase on a vari-
ety of grammars: here |G| is the size of the ini-
tial grammar, |H| is the size after applying our
transformation, and the increase is expressed as a
power of the original grammar size. We apply our
transformation to the grammar from Siahbani and
Sarkar (2014a), which was created for a Chinese-
English translation task known to involve complex
reorderings that cannot be captured by PL-SCFG
(Siahbani and Sarkar, 2014b). We also consider
the grammar in (7) and an ITG (Wu, 1997) con-
taining 10,000 translation pairs, which is a gram-
mar of the sort that has previously been used for
word alignment tasks (cf Zhang and Gildea 2005).
We always observe an increase within O(|G|2)
rather than the worst-case O(|G|3), because |N |
is small relative to |G| in most grammars used for
NLP tasks.

We also investigated how the proportion of pre-
fix lexicalized rules in the original grammar affects
the overall size increase. We sampled grammars
with varying proportions of prefix lexicalized rules
from the grammar in Siahbani and Sarkar (2014a);
Table 2 shows the result of lexicalizing these sam-
ples. We find that the worst case size increase
occurs when 50% of the original grammar is al-
ready prefix lexicalized. This is because the size
increase depends on both the number of prefix lex-
icalized trees in the intermediate grammars (which
grows with the proportion of lexicalized rules) and
the number of productions which need to be lexi-
calized (which shrinks as the proportion of prefix
lexicalized rules increases). At 50%, both factors
contribute appreciably to the grammar size, anal-
ogous to how the function f(x) = x(1− x) takes

its maximum at x = 0.5.

|G| |H| % of G prefix lexicalized log|G|(|H|)
15k 42.4M 10% 1.83
15k 74.9M 20% 1.89
15k 97.8M 30% 1.91
15k 112M 40% 1.93
15k 118M 50% 1.93
15k 114M 60% 1.93
15k 102M 70% 1.92
15k 78.2M 80% 1.89
15k 43.6M 90% 1.83

Table 2: Effect of prefix lexicalized rules in G on final gram-
mar size.

6 Applications

The LR decoding algorithm from Watanabe et al.
(2006) relies on prefix lexicalized rules to gener-
ate a prefix of the target sentence during machine
translation. At each step, a translation hypoth-
esis is expanded by rewriting the leftmost non-
terminal in its target string using some grammar
rule; the prefix of this rule is appended to the ex-
isting translation and the remainder of the rule is
pushed onto a stack, in reverse order, to be pro-
cessed later. Translation hypotheses are stored in
stacks according to the length of their translated
prefix, and beam search is used to traverse these
hypotheses and find a complete translation. Dur-
ing decoding, the source side is processed by an
Earley-style parser, with the dot moving around to
process nonterminals in the order they appear on
the target side.

Since the trees on the target side of our trans-
formed grammar are all of depth 1, and none of
these trees can compose via the adjunction oper-
ation, they can be treated like context-free rules
and used as-is in this decoding algorithm. The
only change required to adapt LR decoding to use
a PL-RSTAG is to make the source side use a TAG
parser instead of a CFG parser; an Earley-style
parser for TAG already exists (Joshi and Schabes,
1997), so this is a minor adjustment.

Combined with the transformation in Section
4, this suggests a method for using LR decoding
without sacrificing translation quality. Previously,
LR decoding required the use of heuristically gen-
erated PL-SCFGs, which cannot model some re-
orderings (Siahbani and Sarkar, 2014a). Now, an
SCFG tailored for a translation task can be trans-
formed directly to PL-RSTAG and used for decod-



1168

ing; unlike a heuristically induced PL-SCFG, the
transformed PL-RSTAG will generate the same
language as the original SCFG which is known to
handle more reorderings.

Note that, since applying our transformation
may double the rank of a grammar, this method
may prove prohibitively slow. This highlights
the need for future work to examine the genera-
tive power of rank-k PL-RSTAG relative to rank-
k SCFG in the interest of reducing the rank of the
transformed grammar.

7 Related Work

Our work continues the study of TAGs and lexical-
ization (e.g. Joshi et al. 1975; Schabes and Waters
1993). Schabes and Waters (1995) show that TAG
can strongly lexicalize CFG, whereas CFG only
weakly lexicalizes itself; we show a similar re-
sult for SCFGs. Kuhlmann and Satta (2012) show
that TAG is not closed under strong lexicalization,
and Maletti and Engelfriet (2012) show how to
strongly lexicalize TAG using simple context-free
tree grammars (CFTGs).

Other extensions of GNF to new grammar for-
malisms include Dymetman (1992) for definite
clause grammars, Fernau and Stiebe (2002) for CF
valence grammars, and Engelfriet et al. (2017) for
multiple CFTGs. Although multiple CFTG sub-
sumes SCFG (and STAG), Engelfriet et al.’s re-
sult appears to guarantee only that some side of
every synchronous production will be lexicalized,
whereas our result guarantees that it is always the
target side that will be prefix lexicalized.

Lexicalization of synchronous grammars was
addressed by Zhang and Gildea (2005), but they
consider lexicalization rather than prefix lexical-
ization, and they only consider SCFGs of rank 2.
They motivate their results using a word alignment
task, which may be another possible application
for our lexicalization.

Analogous to our closure result, Aho and Ull-
man (1969) prove that SCFG does not admit a nor-
mal form with bounded rank like Chomsky normal
form.

Blum and Koch (1999) use intermediate gram-
mars like our GXAs to transform a CFG to GNF.
Another GNF transformation (Rosenkrantz, 1967)
is used by Schabes and Waters (1995) to define
Tree Insertion Grammars (which are also weakly
equivalent to CFG).

We rely on Rogers (1994) for the claim that

our transformed grammars generate context-free
languages despite allowing wrapping adjunction;
an alternative proof could employ the results of
Swanson et al. (2013), who develop their own
context-free TAG variant known as osTAG.

Kaeshammer (2013) introduces the class of syn-
chronous linear context-free rewriting systems to
model reorderings which cannot be captured by a
rank-2 SCFG. In the event that rank-k PL-RSTAG
is more powerful than rank-k SCFG, our work can
be seen as an alternative approach to the same
problem.

Finally, Nesson et al. (2008) present an algo-
rithm for reducing the rank of an STAG on-the-fly
during parsing; this presents a promising avenue
for proving a smaller upper bound on the rank in-
crease caused by our transformation.

8 Conclusion and Future Work

We have demonstrated a method for prefix lexi-
calizing an SCFG by converting it to an equiv-
alent STAG. This process is applicable to any
SCFG which is ε- and chain-free. Like the original
GNF transformation for CFGs our construction at
most cubes the grammar size, though when ap-
plied to the kinds of synchronous grammars used
in machine translation the size is merely squared.
Our transformation preserves all of the alignments
generated by SCFG, and retains properties such as
O(n3k) parsing complexity for grammars of rank
k. We plan to verify whether rank-k PL-RSTAG is
more powerful than rank-k SCFG in future work,
and to reduce the rank of the transformed grammar
if possible. We further plan to empirically evalu-
ate our lexicalization on an alignment task and to
offer a comparison against the lexicalization due
to Zhang and Gildea (2005).

Acknowledgements

The authors wish to thank the anonymous re-
viewers for their helpful comments. The re-
search was also partially supported by the Nat-
ural Sciences and Engineering Research Coun-
cil of Canada (NSERC RGPIN-2018-06437 and
RGPAS-2018-522574) to the second author. We
dedicate this paper to the memory of Prof. Ar-
avind Joshi; a short hallway conversation with him
at ACL 2014 was the seed for this paper.



1169

References
Alfred V. Aho and Jeffrey D. Ullman. 1969. Syntax

directed translations and the pushdown assembler.
Journal of Computer and System Sciences 3(1):37–
56.

Jean-Michel Autebert, Jean Berstel, and Luc Boas-
son. 1997. Context-free languages and pushdown
automata. In Grzegorz Rozenberg and Arto Salo-
maa, editors, Handbook of Formal Languages, Vol.
1, Springer-Verlag New York, Inc., New York, NY,
USA, pages 111–174. http://dl.acm.org/
citation.cfm?id=267846.267849.

Yehoshua Bar-Hillel, M. Perles, and Eliahu Shamir.
1961. On formal properties of simple phrase struc-
ture grammars. Zeitschrift für Phonetik, Sprachwis-
senschaft und Kommunikationsforschung 14:143–
172. Reprinted in Y. Bar-Hillel. (1964). Language
and Information: Selected Essays on their Theory
and Application, Addison-Wesley 1964, 116–150.

Norbert Blum and Robert Koch. 1999. Greibach nor-
mal form transformation revisited. Information and
Computation 150(1):112–118. https://doi.
org/10.1006/inco.1998.2772.

Noam Chomsky and Marcel-Paul Schützenberger.
1963. The algebraic theory of context-free lan-
guages. In P. Braffort and D. Hirschberg, editors,
Computer Programming and Formal Systems, Else-
vier, volume 35 of Studies in Logic and the Founda-
tions of Mathematics, pages 118–161.

Søren Christensen, Hans Hüttel, and Colin Stirling.
1995. Bisimulation equivalence is decidable for all
context-free processes. Information and Computa-
tion 121(2):143–148.

Pierluigi Crescenzi, Daniel Gildea, Andrea Marino,
Gianluca Rossi, and Giorgio Satta. 2015. Syn-
chronous context-free grammars and optimal linear
parsing strategies. Journal of Computer and System
Sciences 81(7):1333–1356. https://doi.org/
10.1016/j.jcss.2015.04.003.

Marc Dymetman. 1992. A generalized greibach nor-
mal form for definite clause grammars. In Pro-
ceedings of the 14th Conference on Computational
Linguistics - Volume 1. Association for Computa-
tional Linguistics, Stroudsburg, PA, USA, COLING
’92, pages 366–372. https://doi.org/10.
3115/992066.992126.

Joost Engelfriet, Andreas Maletti, and Sebastian
Maneth. 2017. Multiple context-free tree grammars:
Lexicalization and characterization. arXiv preprint.
http://arxiv.org/abs/1707.03457.

Henning Fernau and Ralf Stiebe. 2002. Sequen-
tial grammars and automata with valences.
Theoretical Computer Science 276(1):377–
405. https://doi.org/10.1016/
S0304-3975(01)00282-1.

James N. Gray and Michael A. Harrison. 1972. On
the covering and reduction problems for context-
free grammars. Journal of the ACM 19(4):675–
698. https://doi.org/10.1145/321724.
321732.

Sheila A. Greibach. 1965. A new normal-form theorem
for context-free phrase structure grammars. Journal
of the ACM 12(1):42–52. https://doi.org/
10.1145/321250.321254.

Aravind Joshi, Leon Levy, and Masako Taka-
hashi. 1975. Tree adjunct grammars. Journal
of Computer and System Sciences 10(1):136–
163. https://doi.org/10.1016/
S0022-0000(75)80019-5.

Aravind Joshi and Yves Schabes. 1997. Tree-adjoining
grammars. In G. Rozenberg and A. Salomaa, edi-
tors, Handbook of Formal Languages, Vol. 3: Be-
yond Words, Springer-Verlag New York, Inc., New
York, NY, USA, chapter 2, pages 69–124.

Miriam Kaeshammer. 2013. Synchronous linear
context-free rewriting systems for machine transla-
tion. In M. Carpuat, L. Specia, and D. Wu, edi-
tors, Proceedings of the Seventh Workshop on Syn-
tax, Semantics and Structure in Statistical Transla-
tion, SSST@NAACL-HLT 2013, Atlanta, GA, USA,
13 June 2013. Association for Computational Lin-
guistics, pages 68–77. http://aclweb.org/
anthology/W/W13/W13-0808.pdf.

Marco Kuhlmann and Giorgio Satta. 2012. Tree-
adjoining grammars are not closed under strong lex-
icalization. Computational Linguistics 38(3):617–
629. https://doi.org/10.1162/COLI_a_
00090.

Philip M. Lewis and Richard E. Stearns. 1968.
Syntax-directed transduction. Journal of the
ACM 15(3):465–488. https://doi.org/10.
1145/321466.321477.

Andreas Maletti and Joost Engelfriet. 2012. Strong
lexicalization of tree adjoining grammars. In The
50th Annual Meeting of the Association for Com-
putational Linguistics, Proceedings of the Confer-
ence, July 8-14, 2012, Jeju Island, Korea - Vol-
ume 1: Long Papers. The Association for Computa-
tional Linguistics, pages 506–515. http://www.
aclweb.org/anthology/P12-1053.

Rebecca Nesson, Giorgio Satta, and Stuart M.
Shieber. 2008. Optimal k-arization of synchronous
tree-adjoining grammar. In Proceedings of
ACL-08: HLT . Association for Computational
Linguistics, Columbus, Ohio, pages 604–612.
http://www.aclweb.org/anthology/P/
P08/P08-1069.pdf.

James Rogers. 1994. Capturing CFLs with tree ad-
joining grammars. In Proceedings of the 32nd An-
nual Meeting of the Association for Computational

http://dl.acm.org/citation.cfm?id=267846.267849
http://dl.acm.org/citation.cfm?id=267846.267849
http://dl.acm.org/citation.cfm?id=267846.267849
http://dl.acm.org/citation.cfm?id=267846.267849
https://doi.org/10.1006/inco.1998.2772
https://doi.org/10.1006/inco.1998.2772
https://doi.org/10.1006/inco.1998.2772
https://doi.org/10.1006/inco.1998.2772
https://doi.org/10.1016/j.jcss.2015.04.003
https://doi.org/10.1016/j.jcss.2015.04.003
https://doi.org/10.1016/j.jcss.2015.04.003
https://doi.org/10.1016/j.jcss.2015.04.003
https://doi.org/10.1016/j.jcss.2015.04.003
https://doi.org/10.3115/992066.992126
https://doi.org/10.3115/992066.992126
https://doi.org/10.3115/992066.992126
https://doi.org/10.3115/992066.992126
http://arxiv.org/abs/1707.03457
http://arxiv.org/abs/1707.03457
http://arxiv.org/abs/1707.03457
https://doi.org/10.1016/S0304-3975(01)00282-1
https://doi.org/10.1016/S0304-3975(01)00282-1
https://doi.org/10.1016/S0304-3975(01)00282-1
https://doi.org/10.1016/S0304-3975(01)00282-1
https://doi.org/10.1145/321724.321732
https://doi.org/10.1145/321724.321732
https://doi.org/10.1145/321724.321732
https://doi.org/10.1145/321724.321732
https://doi.org/10.1145/321724.321732
https://doi.org/10.1145/321250.321254
https://doi.org/10.1145/321250.321254
https://doi.org/10.1145/321250.321254
https://doi.org/10.1145/321250.321254
https://doi.org/10.1016/S0022-0000(75)80019-5
https://doi.org/10.1016/S0022-0000(75)80019-5
https://doi.org/10.1016/S0022-0000(75)80019-5
http://aclweb.org/anthology/W/W13/W13-0808.pdf
http://aclweb.org/anthology/W/W13/W13-0808.pdf
http://aclweb.org/anthology/W/W13/W13-0808.pdf
http://aclweb.org/anthology/W/W13/W13-0808.pdf
http://aclweb.org/anthology/W/W13/W13-0808.pdf
https://doi.org/10.1162/COLI_a_00090
https://doi.org/10.1162/COLI_a_00090
https://doi.org/10.1162/COLI_a_00090
https://doi.org/10.1162/COLI_a_00090
https://doi.org/10.1162/COLI_a_00090
https://doi.org/10.1145/321466.321477
https://doi.org/10.1145/321466.321477
https://doi.org/10.1145/321466.321477
http://www.aclweb.org/anthology/P12-1053
http://www.aclweb.org/anthology/P12-1053
http://www.aclweb.org/anthology/P12-1053
http://www.aclweb.org/anthology/P12-1053
http://www.aclweb.org/anthology/P/P08/P08-1069.pdf
http://www.aclweb.org/anthology/P/P08/P08-1069.pdf
http://www.aclweb.org/anthology/P/P08/P08-1069.pdf
http://www.aclweb.org/anthology/P/P08/P08-1069.pdf
https://doi.org/10.3115/981732.981754
https://doi.org/10.3115/981732.981754


1170

Linguistics. Association for Computational Linguis-
tics, Stroudsburg, PA, USA, ACL ’94, pages 155–
162. https://doi.org/10.3115/981732.
981754.

Daniel J. Rosenkrantz. 1967. Matrix equations and
normal forms for context-free grammars. Jour-
nal of the Association for Computing Machinery
14(3):501–507.

Yves Schabes and Richard C. Waters. 1993. Lexical-
ized context-free grammars. In L. Schubert, ed-
itor, 31st Annual Meeting of the Association for
Computational Linguistics, 22-26 June 1993, Ohio
State University, Columbus, Ohio, USA, Proceed-
ings.. ACL, pages 121–129. http://aclweb.
org/anthology/P/P93/P93-1017.pdf.

Yves Schabes and Richard C. Waters. 1995. Tree
insertion grammar: Cubic-time, parsable formal-
ism that lexicalizes context-free grammar without
changing the trees produced. Computational Lin-
guistics 21(4):479–513.

Eliahu Shamir. 1967. A representation theorem
for algebraic and context-free power series in
noncommuting variables. Information and Con-
trol 11(1/2):239–254. https://doi.org/10.
1016/S0019-9958(67)90529-3.

Stuart M. Shieber. 1994. Restricting the weak-
generative capacity of synchronous tree-adjoining
grammars. Computational Intelligence 10:371–
385. https://doi.org/10.1111/j.
1467-8640.1994.tb00003.x.

Maryam Siahbani, Baskaran Sankaran, and Anoop
Sarkar. 2013. Efficient left-to-right hierarchical
phrase-based translation with improved reorder-
ing. In Proceedings of the 2013 Conference
on Empirical Methods in Natural Language Pro-
cessing. Association for Computational Linguistics,
pages 1089–1099. http://www.aclweb.org/
anthology/D13-1110.

Maryam Siahbani and Anoop Sarkar. 2014a. Ex-
pressive hierarchical rule extraction for left-to-right
translation. In Proceedings of the 11th Biennial
Conference of the Association for Machine Trans-
lation in the Americas (AMTA-2014)., Vancouver,
Canada.

Maryam Siahbani and Anoop Sarkar. 2014b. Two im-
provements to left-to-right decoding for hierarchical
phrase-based machine translation. In A. Moschitti,
B. Pang, and W. Daelemans, editors, Proceedings of
the 2014 Conference on Empirical Methods in Nat-
ural Language Processing, EMNLP 2014, October
25-29, 2014, Doha, Qatar, A meeting of SIGDAT, a
Special Interest Group of the ACL. ACL, pages 221–
226. http://aclweb.org/anthology/D/
D14/D14-1028.pdf.

Ben Swanson, Elif Yamangil, Eugene Charniak, and
Stuart M. Shieber. 2013. A context free TAG vari-
ant. In Proceedings of the 51st Annual Meeting of

the Association for Computational Linguistics, ACL
2013, 4-9 August 2013, Sofia, Bulgaria, Volume 1:
Long Papers. The Association for Computational
Linguistics, pages 302–310. http://aclweb.
org/anthology/P/P13/P13-1030.pdf.

Taro Watanabe, Hajime Tsukada, and Hideki Isozaki.
2006. Left-to-right target generation for hierarchical
phrase-based translation. In N. Calzolari, C. Cardie,
and P. Isabelle, editors, ACL 2006, 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Com-
putational Linguistics, Proceedings of the Confer-
ence, Sydney, Australia, 17-21 July 2006. The As-
sociation for Computational Linguistics. http:
//aclweb.org/anthology/P06-1098.

Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics 23(3):377–403.

Hao Zhang and Daniel Gildea. 2005. Stochastic lex-
icalized inversion transduction grammar for align-
ment. In K. Knight, H. T. Ng, and K. Oflazer, ed-
itors, ACL 2005, 43rd Annual Meeting of the Asso-
ciation for Computational Linguistics, Proceedings
of the Conference, 25-30 June 2005, University of
Michigan, USA. The Association for Computational
Linguistics, pages 475–482. http://aclweb.
org/anthology/P/P05/P05-1059.pdf.

https://doi.org/10.3115/981732.981754
https://doi.org/10.3115/981732.981754
http://aclweb.org/anthology/P/P93/P93-1017.pdf
http://aclweb.org/anthology/P/P93/P93-1017.pdf
http://aclweb.org/anthology/P/P93/P93-1017.pdf
http://aclweb.org/anthology/P/P93/P93-1017.pdf
https://doi.org/10.1016/S0019-9958(67)90529-3
https://doi.org/10.1016/S0019-9958(67)90529-3
https://doi.org/10.1016/S0019-9958(67)90529-3
https://doi.org/10.1016/S0019-9958(67)90529-3
https://doi.org/10.1016/S0019-9958(67)90529-3
https://doi.org/10.1111/j.1467-8640.1994.tb00003.x
https://doi.org/10.1111/j.1467-8640.1994.tb00003.x
https://doi.org/10.1111/j.1467-8640.1994.tb00003.x
https://doi.org/10.1111/j.1467-8640.1994.tb00003.x
https://doi.org/10.1111/j.1467-8640.1994.tb00003.x
http://www.aclweb.org/anthology/D13-1110
http://www.aclweb.org/anthology/D13-1110
http://www.aclweb.org/anthology/D13-1110
http://www.aclweb.org/anthology/D13-1110
http://www.aclweb.org/anthology/D13-1110
http://aclweb.org/anthology/D/D14/D14-1028.pdf
http://aclweb.org/anthology/D/D14/D14-1028.pdf
http://aclweb.org/anthology/D/D14/D14-1028.pdf
http://aclweb.org/anthology/D/D14/D14-1028.pdf
http://aclweb.org/anthology/D/D14/D14-1028.pdf
http://aclweb.org/anthology/P/P13/P13-1030.pdf
http://aclweb.org/anthology/P/P13/P13-1030.pdf
http://aclweb.org/anthology/P/P13/P13-1030.pdf
http://aclweb.org/anthology/P/P13/P13-1030.pdf
http://aclweb.org/anthology/P06-1098
http://aclweb.org/anthology/P06-1098
http://aclweb.org/anthology/P06-1098
http://aclweb.org/anthology/P06-1098
http://aclweb.org/anthology/P/P05/P05-1059.pdf
http://aclweb.org/anthology/P/P05/P05-1059.pdf
http://aclweb.org/anthology/P/P05/P05-1059.pdf
http://aclweb.org/anthology/P/P05/P05-1059.pdf
http://aclweb.org/anthology/P/P05/P05-1059.pdf

