



















































FCICU at SemEval-2017 Task 1: Sense-Based Language Independent Semantic Textual Similarity Approach


Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 125–129,
Vancouver, Canada, August 3 - 4, 2017. c©2017 Association for Computational Linguistics

 
 
 

 

  

  FCICU at SemEval-2017 Task 1: Sense-Based Language Independent 

Semantic Textual Similarity Approach 

 

Basma Hassan1     Samir AbdelRahman2      Reem Bahgat2   Ibrahim Farag2 

 
1Faculty of Computers and Information 

Fayoum University, Fayoum, Egypt 

bhassan@fayoum.edu.eg 

2 Faculty of Computers and Information 

Cairo University, Giza, Egypt 

{s.abdelrahman, r.bahgat, i.farag}@fci-cu.edu.eg 

 

 

Abstract 

This paper describes FCICU team systems 

that participated in SemEval-2017 Seman-

tic Textual Similarity task (Task1) for 

monolingual and cross-lingual sentence 

pairs. A sense-based language independent 

textual similarity approach is presented, in 

which a proposed alignment similarity 

method coupled with new usage of a se-

mantic network (BabelNet) is used. Addi-

tionally, a previously proposed integration 

between sense-based and surface-based 

semantic textual similarity approach is ap-

plied together with our proposed approach. 

For all the tracks in Task1, Run1 is a string 

kernel with alignments metric and Run2 is 

a sense-based alignment similarity meth-

od. The first run is ranked 10th, and the 

second is ranked 12th in the primary track, 

with correlation 0.619 and 0.617 respec-

tively.  

1 Introduction 

Semantic Textual Similarity (STS) is the task of 

measuring the similarity between two short texts 

semantically. STS is very important because a 

wide range of Natural Language Processing 

(NLP) applications rely heavily on such task. 

This paper describes our participation in the 

STS task (Task1) at SemEval 2017 in all the six 

monolingual and cross-lingual tracks (Cer et al., 

2017). The STS task seeks to calculate a graded 

similarity score from 0 to 5 between two sentenc-

es according to their meaning, i.e. semantically. 

The monolingual tracks are Arabic, English, and 

Spanish sentence-pairs (track1, track3, and track5 

respectively), while the cross-lingual tracks are 

Arabic, Spanish, and Turkish sentences paired 

with English sentences (track2, track4a-4b, and 

track6 respectively). An additional Primary track 

is provided that presents the mean score of the re-

sults of all the other tracks. 

The similarity between two natural language 

sentences can be inferred from the quanti-

ty/quality of aligned constituents in both sentenc-

es. Such alignments provide valuable information 

regarding how and to what extent the two sen-

tences are related or semantically similar, where 

semantically equivalent text pairs are likely to 

have a successful alignment between their words. 

Our proposed sense-based approach employs this 

aspect to calculate the similarity between sen-

tence-pairs regardless of their language. This is 

achieved through a proposed word-sense aligner 

that relies mainly on a new usage of the semantic 

network BabelNet. BabelNet utilization compen-

sates the need of a machine translation module 

that is most commonly used to transfer cross-

lingual STS to monolingual. Besides, the pro-

posed sense-based similarity score is combined 

with a surface-based similarity score.  

The paper is organized as follows. Section 2 

explains our main multilingual sense-based align-

er. Section 3 describes our system that participated 

in all tracks. Section 4 shows the experiments 

conducted and analyzes the results achieved. Sec-

tion 5 concludes the paper and mentions some fu-

ture directions. 

2 Multilingual Sense-Based Aligner 

Highly semantically similar sentences should also 

have a high degree of conceptual alignment be-

tween their semantic units: words, tokens, 

 

 

 

 

 

 

 

 

 

 

125



 
 
 

 

phrases, etc. Several STS methods that use align-

ments in their calculations have been proposed in 

literature. Many of those methods were very suc-

cessful and were among the top performing meth-

ods during the last years of SemEval 2013-2016 

(Han et al., 2013; Han et al., 2015; Hänig et al., 

2015; Sultan et al., 2014a; Sultan et al., 2014b; 

Sultan et al., 2015). 

From this point, we present a sense-based STS 

approach that produces a similarity score between 

texts by means of a multilingual word-sense 

aligner. The following subsections describe in de-

tail the main resource utilized in our STS ap-

proach, namely BabelNet (details in subsection 

2.1), and our proposed word-sense aligner that our 

sense-based similarity method relies on (subsec-

tion 2.2). 

2.1 BabelNet 

BabelNet1 is a rich semantic knowledge resource 

that covers a wide range of concepts and named 

entities connected with large numbers of semantic 

relations (Navigli and Ponzetto, 2010). Concepts 

and relations are gathered from different lexical 

resources such as: WordNet, Wikipedia, Wikidata, 

Wiktionary, FrameNet, ImageNet, and others.  

BabelNet is made up of about 14 million en-

tries called Babel synsets. Each Babel synset is a 

set of multilingual lexicalizations (each being a 

Babel Sense) that represents a given meaning, ei-

ther concept or named entity, and contains all the 

synonyms which express that meaning in a range 

of different languages. For example, the concept 

‘A motor vehicle with four wheels’ is represented 

by the synset {caren, autoen, automobileen, automo-

bilefr, voiturefr, autofr, automóviles, autoes, cochees, 

otomobiltr, arabatr, سيارةar , مركبةar , عربةar}2, this syn-

set contains synonyms in English (EN), French 

(FR), Spanish (ES), Turkish (TR), and Arabic 

(AR) languages. 

BabelNet semantic knowledge is encoded as a 

labeled directed graph, where vertices are Babel 

synset (concepts or named entities), and edges 

connect pairs of synsets with a label indicating the 

type of the semantic relation between them.  

2.2 Word-Sense Aligner 

Alignment is the task of discovering and aligning 

similar semantic units in a pair of sentences ex-

pressed in a natural language. 
                                                      
1http://babelnet.org/ 
2 Each word is a Babel sense in the subscripted language.  

 

Figure 1: Token alignments using our aligner 

between monolingual English - English sen-

tence pair example. 

 

Figure 2: Token alignments using our aligner 

between cross-lingual English - Arabic sen-

tence pair from SemEval 2017-Track2 dataset. 

Our proposed multilingual aligner aligns tokens 

across two sentences based on the similarity of 

their corresponding Babel synsets. A token can be 

in the form of a single word or a multi-words to-

ken. When alignment of a single word token fails, 

its multi-words synonyms are retrieved from 

BabelNet. The proposed aligner aligns only a to-

ken that is neither a stop word nor a punctuation 

mark. 

Figure 1 shows an example of alignments be-

tween English monolingual sentence-pairs using 

our aligner. In this figure the idiom “kicked the 

bucket” is considered as a single token of multiple 

words, and it was successfully aligned with the 

token “died” in the other sentence because both 

tokens are synonyms to each other in BabelNet. 

Figure 2 illustrates an example of direct token 

alignments between English-Arabic cross-lingual 

sentence pairs without using any machine transla-

tion module for translating one sentence language 

to the other.   

Token-pairs are aligned one-to-one in decreas-

ing order of their Babel synsets similarity score (s) 

using Equation (1). The most commonly used Ba-

bel synset of each token is selected.  

AlS1,S2 = {(t, t’, s) : t  T1, t’  T2, and  s >  }; 

where Ti is a set of tokens of sentence i, and  is a 

threshold parameter for alignment score ( = 0.5)3 

2.3 Synset Similarity Measure 

Finding similarity between synsets is a fundamen-

tal part of our aligner. Hence, we proposed a syn-

set similarity measure based on the hypothesis 
                                                      
3 According to experimental results conducted, we found 

that the best value for this threshold is 0.5. 

126



 
 
 

 

that highly semantically similar concepts have 

high degree of common neighbor synsets. From 

this standpoint, this measure calculates the simi-

larity between Babel synset pairs (bsi, bsj) based 

on the overlap between their directly connected 

synsets. The overlap-coefficient is used, which is 

defined as the size of the intersection divided by 

the smaller of the size of the two sets. That is:  

|)||,min(|
),(

ji

ji

jisynset
NSNS

NSNS
bsbssim


          (1) 

where NSi and NSj are the sets of all neighbor 

Babel synsets having a connected edge with bsi 

and bsj in the BabelNet network respectively. 

Since synonyms are belong to the same synset, 

their similarity score is equal to 1. 

3 System Description 

Our systems are based on the past successful inte-

grated architecture of sense-based and surface-

based similarity functions presented in SemEval-

2015 system (Hassan et al., 2015). We use the in-

tegration in the latter system unchanged (Equation 

2),  where the current results are provided by tak-

ing the arithmetic mean of: 1) simproposed : a pro-

posed sentence-pair semantic similarity score (dif-

fers in each Run, details in subsection 3.2), and 2) 

simSC : the surface-based similarity function pro-

posed by Jimenez et al. (2012). Hence, 

 

2

),(),(
),(

2121

21

SSsimSSsim
SSsim

SCproposed 
    (2) 

The approach presented in (Jimenez et al., 

2012) represents sentence words as sets of q-

grams and measures semantic similarity based on 

soft cardinality computed from sentence q-grams 

similarity. Our system employs this approach, 

with the following parameters setup: p=2, bias=0, 

and =0.5.  

In this section, the text preprocessing details is 

firstly explained in subsection 3.1, and then each 

submitted Run is described in subsection 3.2.  

3.1 Text Preprocessing 

The given multilingual input sentences are pre-

processed beforehand to map the raw natural lan-

guage text into structured representation that can 

be processed. This process is including only four 

different tasks: (1) tokenization, (2) stopwords 

removal, (3) lemmatization, and (4) sense tagging.  

Tokenization: is carried out using Stanford 

CoreNLP4 (Manning et al., 2014), in which the 

input raw sentence text, in any language, is broken 

down into a set of tokens.  

Stopwords removal: is the task of removing 

all tokens that are either a stop word or a punctua-

tion mark.  

Lemmatization: is a language-dependent task, 

in which each token is annotated with its lemma. 

English tokens are lemmatized using Stanford 

CoreNLP (Manning et al., 2014). Spanish tokens 

are lemmatized using a freely available lemma-

token pairs dataset5. Arabic tokens are lemmatized 

using Madamira6 (Pasha et al., 2014). For Turkish 

tokens, lemmatization is not carried out. 

Sense tagging: is the task of attaching the Ba-

bel synsets (bs) to each sentence token (t). It is 

achieved by retrieving all the Babel synsets of to-

ken’s lemma. 

On completion of the text preprocessing phase, 

each sentence is represented by a set of tokens (T), 

in which each token (t) is annotated by its original 

word (tword), lemma (tlemma), and a set of Babel 

synsets (bst). This structured representation is then 

used as an input to our proposed aligner (subsec-

tion 2.3), and from which a set of aligned tokens 

across two sentences S1 and S2 is formed (AlS1,S2).  

3.2 Submitted Runs 

We made two system submissions to participate in 

all the provided monolingual and cross-lingual 

tracks, named Run1 and Run2. Each run proposes 

a new different sense-based similarity method be-

tween sentence-pairs. The proposed similarity 

score is then applied in Equation (2), simproposed, 

resulting in the final similarity score between two 

sentences in each run. In the following, each of 

the two runs is described. 

Run1: String Kernel with Alignments 

A kernel can be interpreted as a similarity measure 

between two sentences, it is a simple way of com-

puting the inner product of two data points in a 

feature space directly as a function of their origi-

nal space variables (Liang et al., 2011). At 

SemEval 2015, a string kernel was presented, 

which relied on the hypothesis that the greater the 

                                                      
4http://nlp.stanford.edu/software/corenl

p.shtml 
5http://www.lexiconista.com/datasets/lem

matization/ 
6http://camel.abudhabi.nyu.edu/madamira/ 

127



 
 
 

 

similarity of word senses between two texts, the 

higher their semantic equivalence will be (Hassan 

et al., 2015). Accordingly, this run employs the 

string kernel presented in (Hassan et al., 2015) in 

which the alignments obtained from our proposed 

aligner is used in mapping a sentence to feature 

space. The changed kernel mapping function is 

given by: 

 ),(max)(
1

i
ni

t ttsimS


               (3) 

where sim(t, ti) is the alignment score s of the 

two tokens if (t, ti, s)  AlS1,S2 , and is equal to 0 

otherwise, and n is the number of tokens con-

tained in sentence S, i.e. | T  |. 

The normalized string kernel between two sen-

tences S1 and S2 is calculated as follows (Shawe-

Taylor and Cristianini, 2004): 

),(),(

),(
),(

2211

21

21
SSSS

SS
SS

SS

S

NS



          (4) 





Tt

ttS SSSSSS )()()(),(),( 212121   

where T is the set of all tokens in both S1 and S2. 

Given two sentences, S1 and S2, our similarity 

score between S1 and S2 proposed by this run is 

the value of the normalized string kernel function 

between the two sentences (Equation 4). That is: 

 ),(),( 2121 SSSSsim NSproposed                (5) 

Run2: Alignment-Based Similarity Metric 

Alignment-based semantic similarity approaches 

presented in (Sultan et al., 2014a; Sultan et al., 

2014b; Sultan et al., 2015) relied only on the pro-

portions of the aligned content words on the two 

sentences. We hypothesized that alignments are 

not of the same importance, an alignment of syn-

onym tokens with alignment score 1 is not the 

same as an alignment of two semantically related 

tokens with score 0.5. Hence, the proposed simi-

larity score between S1 and S2 proposed for this 

run is based on the alignment scores as well as 

their proportion to the number of tokens in both 

sentences. It is given by:   

 
||||

.*2

),(
21

21

2,1

TT

sal

SSsim
SSAlal

proposed







             (6) 

where Ti is a set of tokens in sentence i, and al.s is 

the score calculated for the alignment al. 

Track Run1 Run2 Baseline 
Best 

Score 

1 : AR-AR .7158 .7158 .6045 .7543 

2 : AR-EN .6782 .6781  .7493 

3 : SP-SP .8484 .8489 .7117 .8559 

4a: SP-EN  .6926 .6854  .8302 

4b: SP-EN  .0254 .0214  .3407 

5 : EN-EN .8272 .8280 .7278 .8547 

6 : TR-EN .5452 .5390  .7706 

Primary .6190 .6166  .7316 

Table 1: System performance on SemEval-2107 da-

tasets. 

4 Experimental Results 

The main evaluation measure selected by the task 

organizers was the Pearson correlation between 

the system scores and the gold standard scores. 

Table 1 presents the official results of our submis-

sions in SemEval2017-Task1 for both Run1 and 

Run2 in the six tracks as well as the primary track. 

The best performing score obtained in each track 

is included as well alongside with the baseline 

system results announced by the task organizers. 

Our best system (Run1) achieved 0.619 correla-

tion and ranked the 10th run and the 5th team out of 

84 runs and 31 teams respectively.  

Although the performance of the two Runs dif-

fers slightly, it is noticeable from the table that 

Run1 (Kernel) performs better with cross-lingual 

sentence-pairs, while Run2 (Alignments) per-

forms better with monolingual sentence-pairs. 

Hence, relying on aligned tokens only in cross-

lingual sentences is insufficient.  

5 Conclusions and Future work 

Experimental results proved that, in spite of the 

fact that our proposed simple unsupervised ap-

proach relies only on BabelNet and token align-

ments, it is capable of assessing the semantic simi-

larity between two sentences in different lan-

guages with good performance, 10th run rank and 

5th team rank. Also, the proposed approach 

demonstrates the effectiveness and usefulness of 

using the BabelNet semantic network in solving 

the STS task. Some potential future work includes 

enhancing our proposed synset similarity method, 

and exploiting the extraction of promising content 

words in the given sentences. 

128



 
 
 

 

References 

Daniel Cer, Mona Diab, Eneko Agirre, Iñigo Lopez-

Gazpio, and Lucia Specia. 2017. SemEval-2017 

Task 1: Semantic Textual Similarity Multilingual 

and Crosslingual Focused Evaluation. In Proceed-

ings of the 11th International Workshop on Seman-

tic Evaluation (SemEval 2017), pages 1–14, Van-

couver, Canada. 

Lushan Han, Abhay Kashyap, Tim Finin, James May-

field, and Jonathan Weese. 2013. UMBC EBIQUI-

TY-CORE: Semantic textual similarity systems. In 

Proceedings of the Second Joint Conference on 

Lexical and Computational Semantics, pages. 44-

52, Atlanta, Georgia, USA 

Lushan Han, Justin Martineau, Doreen Cheng, and 

Christopher Thomas. 2015. Samsung: Align-and-

differentiate approach to semantic textual similari-

ty. In Proceedings of the 9th International Work-

shop on Semantic Evaluation (SemEval 2015), 

pages 172–177, Denver, Colorado, USA 

Christian Hänig, Robert Remus, and Xose De La 

Puente. 2015. ExB Themis: Extensive feature ex-

traction from word alignments for semantic textual 

similarity. In Proceedings of the 9th International 

Workshop on Semantic Evaluation (SemEval 

2015), pages 264–268, Denver, Colorado, USA 

Basma Hassan, Samir AbdelRahman, and Reem 

Bahgat. (2015). FCICU: The Integration between 

Sense-Based Kernel and Surface-Based Methods to 

Measure Semantic Textual Similarity. In Proceed-

ings of the 9th International Workshop on Semantic 

Evaluation (SemEval 2015), pages 154–158, Den-

ver, Colorado, USA. 

Sergio Jimenez, Claudia Becerra, and Alexander Gel-

bukh.  2012. Soft Cardinality: A Parameterized 

Similarity Function for Text Comparison. In Pro-

ceedings of the First Joint Conference on Lexical 

and Computational Semantics (*SEM), pages 449–

453, Montreal, Canada. 

Yizeng Liang, Qing-Song Xu, Hong-Dong Li, and 

Dong-Sheng Cao.  2011. Support Vector Machines 

and Their Application in Chemistry and Biotech-

nology. CRC Press. 

Christopher D. Manning, Mihai Surdeanu, John Bau-

er, Jenny Finkel, Steven J. Bethard, and David 

McClosky.  2014. The Stanford CoreNLP Natural 

Language Processing Toolkit. In Proceedings of 

the 52nd Annual Meeting of the Association for 

Computational Linguistics: System Demonstra-

tions, pages 55–60, Baltimore, Maryland. 

Roberto Navigli and Simone Paolo Ponzetto.  2010. 

BabelNet: Building a Very Large Multilingual Se-

mantic Network. In Proceedings of the 48th Annual 

Meeting of the Association for Computational Lin-

guistics, pages 216–225, Uppsala, Sweden. 

Arfath Pasha, Mohamed Al-Badrashiny, Mona Diab, 

Ahmed El Kholy, Ramy Eskander, Nizar Habash, 

Manoj Pooleery, Owen Rambow, and Ryan Roth. 

2014. MADAMIRA: A Fast, Comprehensive Tool 

for Morphological Analysis and Disambiguation of 

Arabic. In Proceedings of the 9th International 

Conference on Language Resources and Evalua-

tion (LREC’14), pages 1094–1101, Reykjavik, Ice-

land. 

Md Arafat Sultan, Steven Bethard, and Tamara 

Sumner. 2014a. Back to Basics for Monolingual 

Alignment: Exploiting Word Similarity and Con-

textual Evidence. Transactions of the Association 

for Computational Linguistics, 2:219–230. 

Md Arafat Sultan, Steven Bethard, and Tamara 

Sumner. 2014b. DLS@CU: Sentence Similarity 

from Word Alignment. In Proceedings of the 8th 

International Workshop on Semantic Evaluation 

(SemEval 2014), pages 241–246, Dublin, Ireland. 

Md Arafat Sultan, Steven Bethard, and Tamara 

Sumner. 2015. DLS@CU: Sentence Similarity 

from Word Alignment and Semantic Vector Com-

position. In Proceedings of the 9th International 

Workshop on Semantic Evaluation (SemEval 

2015), pages 148–153, Denver, Colorado, USA. 

John Shawe-Taylor and Nello Cristianini.  2004. Ker-

nel Methods for Pattern Analysis. Cambridge Uni-

versity Press. 

 

 

 

129


