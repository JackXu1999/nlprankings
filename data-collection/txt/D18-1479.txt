



















































Co-Stack Residual Affinity Networks with Multi-level Attention Refinement for Matching Text Sequences


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4492–4502
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

4492

Co-Stack Residual Affinity Networks with Multi-level Attention
Refinement for Matching Text Sequences

Yi Tay†∗, Luu Anh Tuanψ∗, Siu Cheung Huiφ
†φNanyang Technological University, Singapore

ψInstitute for Infocomm Research, A*Star Singapore
ytay017@e.ntu.edu.sg†, at.luu@i2r.a-star.edu.sgψ

asschui@ntu.edu.sgφ

Abstract

Learning a matching function between two
text sequences is a long standing problem in
NLP research. This task enables many po-
tential applications such as question answering
and paraphrase identification. This paper pro-
poses Co-Stack Residual Affinity Networks
(CSRAN), a new and universal neural archi-
tecture for this problem. CSRAN is a deep
architecture, involving stacked (multi-layered)
recurrent encoders. Stacked/Deep architec-
tures are traditionally difficult to train, due
to the inherent weaknesses such as difficulty
with feature propagation and vanishing gradi-
ents. CSRAN incorporates two novel compo-
nents to take advantage of the stacked archi-
tecture. Firstly, it introduces a new bidirec-
tional alignment mechanism that learns affin-
ity weights by fusing sequence pairs across
stacked hierarchies. Secondly, it leverages
a multi-level attention refinement component
between stacked recurrent layers. The key
intuition is that, by leveraging information
across all network hierarchies, we can not only
improve gradient flow but also improve over-
all performance. We conduct extensive ex-
periments on six well-studied text sequence
matching datasets, achieving state-of-the-art
performance on all.

1 Introduction

Determining the semantic affinity between two
text sequences is a long standing research prob-
lem in natural language processing research. This
is understandable, given that technical innovations
in this domain would naturally bring benefits to a
diverse plethora of applications ranging from para-
phrase detection to standard document retrieval.
This work focuses on short textual sequences, fo-
cusing on a myriad of applications such as natu-
ral language inference, question answering, reply

∗Denotes equal contribution.

prediction and paraphrase detection. This paper
presents a new deep matching model for universal
text matching.

Neural networks are dominant state-of-the-art
approaches for many of these matching problems
(Gong et al., 2017; Shen et al., 2017; Wang et al.,
2017; Chen et al., 2017). Fundamentally, neu-
ral networks operate via a concept of feature hi-
erarchy, in which hierarchical representations are
constructed as sequences propagate across the net-
work. In the context of matching, representations
are often (1) encoded, (2) matched, and then (3)
aggregated for prediction. Each key step often
comprises several layers, which consequently adds
to the overall depth of the network.

Unfortunately, it is a well established fact that
deep networks are difficult to train. This is at-
tributed to not only vanishing/exploding gradi-
ents but also an instrinsic difficulty pertaining
to feature propagation. To this end, commonly
adopted solutions include Residual connections
(He et al., 2016) and/or Highway layers (Srivas-
tava et al., 2015). The key idea in these approaches
is to introduce additional (skip/residual) connec-
tions, propagating shallower layers to deeper lay-
ers via shortcuts. To the best of our knowledge,
these techniques are generally applied to single se-
quences and therefore the notion of pairwise resid-
ual connections have not been explored.

This paper presents Co-Stack Residual Affin-
ity Networks (CSRAN), a stacked multi-layered
recurrent architecture for general purpose text
matching. Our model proposes a new co-stacking
mechanism that computes bidirectional affinity
scores by leveraging all feature hierarchies be-
tween text sequence pairs. More concretely, word-
by-word affinity scores are not computed just from
the final encoded representations but across all the
entire feature hierarchy.

There are several benefits to our co-stacking



4493

mechanism. Firstly, co-stacking acts as a form
of residual connector, alleviating the instrinsic is-
sues with network depth. Secondly, there are
more extensive matching interfaces between text
sequences as the affinity matrix is not computed
by just one representation but multiple represen-
tations instead. Naturally, increasing the oppor-
tunities for interactions between sequences is an
intuitive method for improving performance.

Additionally, our model incorporates a Multi-
level Attention Refinement (MAR) architecture in
order to fully leverage the stacked recurrent archi-
tecture. The MAR architecture is a multi-layered
adaptation and extension of the CAFE model (Tay
et al., 2017c), in which attention is computed,
compressed and then re-fed into the input se-
quence. In our approach, we use CAFE blocks to
repeatedly refine representations at each level of
the stacked recurrent encoder.

The overall outcome of the above-mentioned ar-
chitectural synergies is a highly competitive model
that establishes state-of-the-art performance on six
well-known text matching datasets such as SNLI
and TrecQA. The overall contributions of this
work are summarized as follows:

• We propose a new deep stacked recurrent ar-
chitecture for matching text sequences. Our
model is based on a new co-stacking mech-
anism which learns to align by exploiting
matching across feature hierarchies. This
can be interpreted as a new way to incorpo-
rate shortcut connections within neural mod-
els for sequence matching. Additionally, we
also propose a multi-level attention refine-
ment scheme to leverage our stacked recur-
rent model.

• While stacked architectures can potentially
lead to considerable improvements in per-
formance, our experiments show that in the
absence of our proposed CSRA (Co-stack
Residual Affinity) mechanism, stacking may
conversely lead to performance degradation.
As such, this demonstrates that our proposed
techniques are essential for harnessing the
potential of deep architectures.

• We conduct extensive experiments on four
text matching tasks across six well-studied
datasets, i.e., Natural Language Inference
(SNLI (Bowman et al., 2015), SciTail (Khot
et al., 2018)), Paraphrase Identification

(Quora, TwitterURL (Lan et al., 2017)), An-
swer Sentence Selection (Wang et al., 2007)
and Utterance-Response Matching (Ubuntu
(Lowe et al., 2015)). Our model achieves
state-of-the-art performance on all datasets.

2 Co-Stack Residual Affinity Networks

In this section, we introduce our proposed model
architecture for general/universal text matching.
The key idea of this architecture is to leverage
deep stacked layers, while mitigating the inherent
weaknesses of going deep. As such, our network is
in similar spirit to highway networks, residual net-
works and DenseNets, albeit tailored specifically
for pairwise architectures. Figure 1 illustrates a
high-level overview of our proposed model archi-
tecture.

2.1 Input Encoder

The inputs to our model are standard sequences of
words A and B which represent sequence a and
sequence b respectively. In the context of different
applications, a and b take different roles such as
premise/hypothesis or question/answer. Both se-
quences are converted into word representations
(via pretrained word embeddings) and character-
based representations. Character embeddings are
trainable parameters and a final character-based
word representation of d dimensions is learned by
passing all characters into a Bidirectional LSTM
encoder. This is standard, following many works
such as (Wang et al., 2017). Word embeddings
and character-based word representations are then
concatenated to form the final word representa-
tion. Next, the word representation is passed
through a (optional and tuned as a hyperparame-
ter) 2-layered highway network of d dimensions.

2.2 Stacked Recurrent Encoders

Next, word representations are passed into a
stacked recurrent encoder layer. Specifically, we
use Bidirectional LSTM encoders at this layer. Let
k be the number of layers of the stacked recurrent
encoder layer.

hit = BiLSTMi(ht−1) ∀t ∈ [1, 2 · · · `] (1)

where BiLSTMi represents the i-th BiLSTM layer
and hit represents the t-th hidden state of the i-th
BiLSTM layer. ` is the sequence length. Note that
the parameters are shared for both a and b.



4494

v v

Co-Stack 
Residual
Affinity
(CSRA)Max 

Pool

Sequence A Sequence B

Affinity 
Matrix

CAFE CAFE

CAFE CAFE

CAFE CAFE

Input Encoder Input Encoder

Bidirectional Alignment

Prediction Layer

Multi-level 
Attention
Refinement 
(MAR)

Aggregation 
Layer

Prediction 
Layer

BiLSTM

BiLSTM

BiLSTM

BiLSTM

BiLSTM

BiLSTM

BiLSTM

Figure 1: Illustration of the proposed Co-Stack Residual Affinity Network (CSRAN) architecture. Each color
coded matrix represents the interactions between two layers of sequence A and sequence B. (Best viewed in color)

2.3 Multi-level Attention Refinement (MAR)

Inspired by CAFE (Tay et al., 2017c) (Compare-
Align-Factorized Encoders), a top performing
model on the SNLI benchmark, we utilize CAFE
blocks between the BiLSTM layers. Each CAFE
block returns six features, which are generated by
a factorization operation using factorization ma-
chines (FM). While the authors in (Tay et al.,
2017c) simply use this operation in a single layer,
we utilize this in a multi-layered fashion which we
found to have worked well. This constitutes our
multi-level attention refinement mechanism. More
concretely, we apply the CAFE operation to the
outputs of each BiLSTM layer, allowing the next
BiLSTM layer to process the ‘augmented’ repre-
sentations. The next layer retains its dimension-
ality by projecting the augmented representation
back to its original size using the BiLSTM en-
coder. This can be interpreted as repeatedly refin-
ing representations via attention. As such, adding
CAFE blocks is a very natural fit to the stacked
recurrent architecture.

CAFE Blocks This section describes the oper-
ation of each CAFE block. The key idea behind

CAFE blocks is to align a and b, and compress
alignment vectors such as b′ − a (subtraction),
b′ � a (element-wise multiplication) and [b′; a]
(concatenation) into scalar features. These scalar
features are concatenated to the original input em-
bedding, which can be pased into another BiL-
STM layer for refining representations. Firstly,
a, b are modeled aligned via Eij = F (a)>F (b)
and then aligned via:

A′ = E>B and B′ = AE> (2)

Given aligned pairs (A′, B) and (B′, A), we gen-
erate three matching vectors for the concatenation
([a′i; bi]), element-wise multiplication (a

′
i�bi) and

subtraction vectors (a′i − bi) of each pair. After
which, we apply a factorization machine (Rendle,
2010) M(x) on each matching vector.

M(x) = w0 +
n∑
i=1

wi xi +
n∑
i=1

n∑
j=i+1

〈vi, vj〉 xi xj

(3)

where v ∈ Rd×k, w0 ∈ R and wi ∈ Rd. The
output M(x) is a scalar. Intuitively, this layer tries



4495

to learn pairwise interactions between every xi and
xj using factorized (vector) parameters v.

Factorization machines model low-rank struc-
ture within the matching vector, producing a scalar
feature. This enables efficient propagation of these
matching features to the next layer. The output of
each CAFE block is the original input to the CAFE
module, augmented with the output of the factor-
ization machines. As such, if the input sequence
is of d dimensions, then the output is d + 3 di-
mensions. Additionally, intra-attention is applied
in similar fashion as above to generate three more
features for each sequence. As a result, the output
dimensions for each word becomes d + 6.

2.4 Co-Stack Residual Affinity (CSRA)
This layer is the cornerstone of our proposed ap-
proach and is represented as the middle segment
of Figure 1 (the colorful matrices).

Co-Stacking Co-stacking refers to the fusion of
a and b across multiple hierarchies. Recall that the
affinity score between two words is typically com-
puted by sij = a>b. We extend this to a residual
formulation. More concretely, the affinity score
between both words is now computed as the max-
imum influence it has over all layers.

sij = max
∑
p

∑
q

a>pi bqj (4)

where api is the i-th word for the p-th stacked layer
for a and bqj is the j-th word for the q-th stacked
layer for b. The choice of the maximum oper-
ator is intuitive and is strongly motivated by the
fact that we would like to give a high affinity for
each word pair that shows a strong match at any
of different hierarchical stages of learning repre-
sentations. Note that this layer can be interpreted
as constructing a matching tensor based on multi-
hierarchical information and selecting the most in-
formative match across all representation hierar-
chies.

Bidirectional Alignment In order to learn (bidi-
rectionally) attentive representations, we first con-
catenate all stacked outputs to form a ` × kd vec-
tor. Next, we apply the following operations to
A ∈ R`a×kd and B ∈ R`b×kd.

Ā = S>B and B̄ = AS> (5)

where Ā ∈ R`b×kd, B̄ ∈ R`a×kd are the attentive
(aligned) representations.

2.4.1 Matching and Aggregation Layer
Next, we match the attentive (aligned) represen-
tations using the subtraction, element-wise multi-
plication and concatenation of each aligned word.
Subsequently, we pass this matching vector into a
k layered BiLSTM layer.

a′i = BiLSTMk([b̄i − ai, b̄i � ai, b̄i, ai]) (6)
b′i = BiLSTMk([āi − bi, āi � bi, āi, bi]) (7)

The final feature representation is learned via the
summation across the temporal dimension as fol-
lows:

z = [

`a∑
i=1

a′i ;

`b∑
i=1

b′i] (8)

where [.; .] is the concatenation operator.

2.5 Output and Prediction Layer
Our model predicts using the feature vector z for
every given sequence pair. At this layer, we utilize
standard fully connected layers. The number of
output layers is typically 2-3 and is a tuned hy-
perparameter. Softmax is applied onto the final
layer. The final layer is application specific, e.g.,
k classes for classification tasks and a two-class
softmax for pointwise ranking. For all datasets,
we optimize the cross entropy loss.

3 Experimental Evaluation

In this section, we introduce our experimental
setup, baselines and results.

3.1 Datasets and Competitor Baselines
We use six public benchmark datasets for evaluat-
ing our proposed approach. This section briefly in-
troduces each dataset, along with several state-of-
the-art approaches that we compare against. Table
1 provides a summary of the datasets used in our
experiments.

Stanford Natural Language Inference (SNLI)
(Bowman et al., 2015) is a well-known dataset for
entailment classification (or natural language in-
ference). The task is to determine if two sequences
entail/contradict or are neutral to each other. This
task is a three-way classification problem. On this
dataset, we compare with several state-of-the-art
models such as BiMPM (Wang et al., 2017), ESIM
(Chen et al., 2017), DIIN (Gong et al., 2017), DR-
BiLSTM (Ghaeini et al., 2018) and CAFE (Tay
et al., 2017c).



4496

Dataset Task |C| Pairs
SNLI Premise-Hypothesis 3 570K
Scitail Premise-Hypothesis 2 27K
Quora Question-Question 2 400K
Twitter Tweet-Tweet 2 51K
TrecQA Question-Answer R 56K
Ubuntu Utterance-Response R 1M

Table 1: Statistics of datasets used in our experiment.
|C| denotes the number of classes and R denotes a rank-
ing formulation. Twitter stands for the TwitterURL
dataset.

Science Entailment (SciTail) (Khot et al.,
2018) is a new entailment classification dataset
that was constructed from science questions and
answers. This dataset involves two-way classifi-
cation (entail or non-entail). We compare with
DecompAtt (Parikh et al., 2016), ESIM, DGEM
(Khot et al., 2018) and CAFE.

Quora Duplicate Detection is a well-studied
paraphrase identification dataset1. We use the
splits provided by (Wang et al., 2017). The task
is to determine if two questions are paraphrases
of each other. This task is formulated as a binary
classication problem. We compare with L.D.C
(Wang et al., 2016b), BiMPM, the DecompAtt im-
plementation by (Tomar et al., 2017) (word and
char level) and DIIN.

TwitterURL (Lan et al., 2017) is another dataset
for paraphrase identification. It was constructed
using Tweets referring to news articles. This task
is also a binary classification problem. We com-
pare with (1) MultiP (Xu et al., 2014), a strong
baseline, (2) the implementation of (He and Lin,
2016) by (Lan et al., 2017) and (3) the Subword +
LM model from (Lan and Xu, 2018).

TrecQA (Wang et al., 2007) is a well-studied
dataset for answer sentence selection task (or
question-answer matching). The goal is to rank
answers given a question. This task is formu-
lated as a pointwise learning-to-rank problem.
Baselines include HyperQA (Tay et al., 2017a),
Ranking-based Multi-Perspective CNN (He et al.,
2015) implementation by (Rao et al., 2016),
BiMPM, the compare-aggregate (Wang and
Jiang, 2016a) model extension by (Bian et al.,
2017) (we denote this model as CA), IWAN

1https://data.quora.com/
First-Quora-Dataset-Release-Question-Pairs

(Shen et al., 2017) and the recent MCAN model,
i.e., Multi-Cast Attention Networks (Tay et al.,
2018c). A leaderboard is maintained at https:
//aclweb.org/aclwiki/Question_
Answering_(State_of_the_art).

Ubuntu (Lowe et al., 2015) is a dataset for
Utterance-Response Matching and comprises 1-
million utterance-response pairs. This dataset is
based on the Ubuntu dialogue corpus. The goal
is to predict the response to a message. We use
the same setup as (Wu et al., 2016). Baselines in-
clude CNTN (Qiu and Huang, 2015), APLSTM
(dos Santos et al., 2016), MV-LSTM (Wan et al.,
2016a) and KEHNN (Wu et al., 2016). Results are
reported from (Wu et al., 2016).

Metrics For all datasets, we follow the evalua-
tion procedure from all the original papers. The
metric for SNLI, SciTail and Quora is the accuracy
metric. The metric for the TwitterURL dataset is
the F1 score. The metric for TrecQA is the Mean
Average Precision (MAP) and Mean Reciprocal
Rank (MRR) metric. The metric for Ubuntu is the
Recall@K for k = 1, 2, 5 (given 9 negative sam-
ples) and the binary classification accuracy score.

3.2 Experimental Setup
All baselines are reported from the respective pa-
pers. All models are trained with the Adam opti-
mizer (Kingma and Ba, 2014) with learning rates
tuned amongst {0.001, 0.0003, 0.0004}. Batch
size is tuned amongst {32, 64, 128, 256}. The
dimensions of the BiLSTM encoders are tuned
amongst {64, 100, 200, 300} and the number of
hidden dimensions of the prediction layers are
tuned amongst {100, 200, 300, 600}. The num-
ber of stacked recurrent layers is tuned from [2, 5]
and the number of aggregation BiLSTM layers
is tuned amongst {1, 2}. The number of predic-
tion layers is tuned from [1,3]. Parameters are
initialized using glorot uniform (Glorot and Ben-
gio, 2010). All unspecified activation functions
are ReLU activations. Word embeddings are ini-
tialized with GloVe (Pennington et al., 2014) and
fixed during training. We implement our model
in Tensorflow (Abadi et al., 2015) and use the
CUDNN implementation for all BiLSTM layers.

3.3 Experimental Results
Overall, our proposed CSRAN architecture
achieves state-of-the-art performance on all six
well-established datasets.

https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs
https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs
https://aclweb.org/aclwiki/Question_Answering_(State_of_the_art)
https://aclweb.org/aclwiki/Question_Answering_(State_of_the_art)
https://aclweb.org/aclwiki/Question_Answering_(State_of_the_art)


4497

On SNLI (Table 2), CSRAN achieves the best2

single model performance to date on the well-
established dataset. This demonstrates the effec-
tiveness of CSRAN, taking into consideration of
the inherent competitiveness of this well-known
benchmark. On SciTail (Table 3), CSRAN simi-
larly achieves the best performance to date on this
dataset, outperforming the existing CAFE model
by +3.4% absolute accuracy.

On Quora (Table 4), CSRAN also achieves
the best single model score, outperforming strong
baselines such as BiMPM (+1.1%) and DIIN
(+0.2%). Moreover, there is also considerable
performance improvement on the TwitterURL
dataset (Table 5) as CSRAN outperforms the exist-
ing state-of-the-art Subword + LM model (+8%)
and Deep Pairwise Word (+9.1%).

On TrecQA (Table 6), CSRAN achieves the
best performance on this dataset. CSRAN outper-
forms the existing state-of-the-art model, IWAN
(+3.2%/ + 4.6%). CSRAN also outperforms
strong competitive baselines such as BiMPM
(+5.2%/+3.6%) and MPCNN (+5.3%/+5.8%).
Finally, on Ubuntu (Table 7), CSRAN also outper-
forms many competitive models such as CNTN,
APLSTM and KEHNN. Performance improve-
ment over all metrics are ≈ 9% − 10% compared
to the existing state-of-the-art.

Overall, CSRAN achieves state-of-the-art per-
formance on six well-studied datasets. On sev-
eral datasets, our achieved performance is not only
the highest reported score but also outperforms the
existing state-of-the-art models by a considerable
margin.

Model Acc
BiMPM (Wang et al., 2017) 87.5
ESIM (Chen et al., 2017) 88.0
DIIN (Gong et al., 2017) 88.0
DR-BiLSTM (Ghaeini et al., 2018) 88.5
CAFE (Tay et al., 2017c) 88.5
CSRAN 88.7

Table 2: Experimental results on single model SNLI
dataset.

2For fair comparison, we do not compare with (1) mod-
els that use external contextualized word embeddings, e.g.,
CoVe (McCann et al., 2017) / ELMo (Peters et al., 2018) /
generative pretraining (Radford et al.) and (2) ensemble sys-
tems. As either (1) and/or (2) would also intuitively boost the
performance of the base CSRAN model.

Model Acc
DecompAtt (Parikh et al., 2016) 72.3
ESIM (Chen et al., 2017) 70.6
DGEM (Khot et al., 2018) 77.3
CAFE (Tay et al., 2017c) 83.3
CSRAN 86.7

Table 3: Experimental results on SciTail dataset.

Model Acc
L.D.C (Wang et al., 2016b) 87.5
Word DecompAtt (Tomar et al., 2017) 87.5
BiMPM (Wang et al., 2017) 88.1
Char DecompAtt (Tomar et al., 2017) 88.4
DIIN (Gong et al., 2017) 89.0
CSRAN 89.2

Table 4: Experimental results on Quora Duplicate De-
tection dataset.

Model F1
MultiP (Xu et al., 2014) 0.536
DeepPairwiseWord (He and Lin, 2016) 0.749
Subword + LM (Lan and Xu, 2018) 0.760
CSRAN 0.840

Table 5: Experimental results on TwitterURL para-
phrase dataset.

Model MAP/MRR
HyperQA (Tay et al., 2017a) 0.784/0.865
MPCNN (Rao et al., 2016) 0.801/0.877
BiMPM (Wang et al., 2017) 0.802/0.899
CA (Bian et al., 2017) 0.821/0.899
IWAN (Shen et al., 2017) 0.822/0.889
MCAN (Tay et al., 2018c) 0.838/0.904
CSRAN 0.854/0.935

Table 6: Experimental results on TrecQA dataset.

Model Acc R@1 R@2 R@5
CNTN 0.743 0.349 0.512 0.797
LSTM 0.725 0.361 0.494 0.801
APLSTM 0.758 0.381 0.545 0.801
MV-LSTM 0.767 0.410 0.565 0.800
KEHNN 0.786 0.460 0.591 0.819
MCAN 0.834 0.551 0.684 0.875
CSRAN 0.839 0.556 0.692 0.880

Table 7: Experimental results on the Ubuntu dataset
for utterance-response matching. Baseline results are
reported from (Wu et al., 2016).



4498

3.3.1 Training Efficency
With many BiLSTM layers, it is natural to be
skeptical about the training efficiency of our
model. However, since we use the CUDNN im-
plementation of the BiLSTM model, the runtime
is actually very manageable. On SNLI, with a
batch size of 128, our model with 3 stacked re-
current layers and 2 aggregation BiLSTM layers
runs at ≈17 minutes per epoch and converges in
less than 20 epochs. On SciTail, our model runs
at ≈ 2 minutes per epoch with a batch size of 32.
This is benchmarked on a TitanXP GPU. While
our model is targetted at performance and not effi-
ciency, this section serves as a reassurance that our
model is not computationally prohibitive.

3.4 Ablation Study

In order to study the effectiveness of the key com-
ponents in our proposed architecture, we conduct
an extensive ablation study. Table 8 reports the
results on several ablation baselines. There are
three key ablation baselines as follows: (1) we re-
moved MAR from the stacked recurrent network,
(2) we removed CSRA from the network and fi-
nally (3) we removed both MAR and CSRA from
the network. All ablation baselines reported are
stacked with 3 layers. Firstly, we observe that both

Ablation SNLI SciTail TrecQA
Original 88.6 88.0 0.86/0.90
w/o MAR 88.4 82.5 0.79/0.85
w/o CSRA 88.1 86.2 0.84/0.89
w/o MAR/CSRA 88.0 83.0 0.79/0.84

Table 8: Ablation study (development score) of our key
model components on three datasets.

MAR and CSRA are critical components in our
model, i.e., removing any of them would result
in a drop in performance. Secondly, we observe
that the relative utility of CSRA and MAR de-
pends on the dataset. Removing MAR sigificantly
reduces performance on SciTail and TrecQA. On
the other hand, removing CSRA degrades the per-
formance more than MAR on SNLI. Finally, it
is good to note that, while performance degrada-
tion on SNLI development set may not seem sig-
nificant, the w/o MAR and CSRA ablation perfor-
mance baseline achieved only 87.7% accuracy on
the test set, compared to 88.7% of the original
model. This is equivalent to dropping from state-
of-the-art to the 5th ranked model. Overall, we are

able to conclude that the CSRA and MAR make
meaningful improvements to our model architec-
ture.

3.5 Effect of Stack Depth

In this section, our goals are twofold - (1) study-
ing the effect of stack depth on model performance
and (2) determining if the proposed CSRAN
model indeed helps with enabling deeper stack
depths. In order to do so, we compute the devel-
opment set performance of two models. The first
is the full CSRAN architecture and the second is
a baseline stacked model architecture. Note that
the bidirectional alignment layer and remainder of
the model architecture (highway layers, etc.) re-
main completely identical to CSRAN to make this
study as fair as possible.

1 2 3 4 5
LDyers

80

81

82

83

84

85

86

87

88

D
ev

 A
cc

C65A1
6tDcNed 0Rdel

Figure 2: Relative effect of stack depth on CSRAN and
the baseline Stacked Model on SciTail dataset.

Figure 2 illustrates the model performance with
varying stack depth. As expected, the performance
of the stacked model declines when increasing the
stack depth. On the other hand, the performance
of CSRAN improves by adding additional layers.
The largest gain is when jumping from 2 layers to
3 layers. The subsequent performance improve-
ment from 3-5 layers is marginal. From this study,
the takeaway is that standard stacked architectures
are insufficient. As such, our proposed CSRA
mechanism can aid in enabling deeper models
which can result in stronger model performance3.

Next, we study the general effect of stack depth
(number of layers) on model performance. Fig-
ure 3 reports the model performance (dev accu-
racy) of our CSRAN architecture on Quora and

3The best result on Scitail was obtained with 5 layers.
Moreover, the difference in test performance between stacked
and single-layered model was considerably high (+2.5%)
even though dev performance increased by +1%.



4499

SNLI datasets. We observe that a stacked archi-
tecture with 3 layers is significantly better than
a single-layered architecture. The optimal devel-
opment score is 3-4 layers for SNLI and 3 layers
for Quora. However, we observe the performance
of Quora declines after 3 layers (notably it is still
higher than an unstacked model). However, the
performance on SNLI remains relatively stable.

1 2 3 4 5
LDyers

87.8

88.0

88.2

88.4

88.6

88.8

D
e
v
 6

co
re

4uorD
61LI

Figure 3: Effect of stack depth on CSRAN perfor-
mance on Quora and SNLI datasets.

4 Related Work

Learning to matching text sequences is a core and
fundamental research problem in NLP and Infor-
mation Retrieval. A wide range of NLP appli-
cations fall under this paradigm such as natural
language inference (Bowman et al., 2015; Khot
et al., 2018), paraphrase identification (Lan and
Xu, 2018), question answering (Severyn and Mos-
chitti, 2015), document search (Shen et al., 2014;
Hui et al., 2017), social media search (Rao et al.,
2018) and entity linking (Phan et al., 2017). As
such, universal text matching algorithms are gen-
erally very attractive, in lieu of the prospects of
potentially benefitting an entire suite of NLP ap-
plications.

Neural networks have been the prominent
choice for text matching. Earlier works are mainly
concerned with learning a matching function be-
tween RNN/CNN encoded representations (Sev-
eryn and Moschitti, 2015; Yu et al., 2014; Qiu
and Huang, 2015; Tay et al., 2017b, 2018b).
Models such as Recursive Neural Networks have
also been explored (Wan et al., 2016b). Sub-
sequently, attention-based models were adopted
(Rocktäschel et al., 2015; Wang et al., 2016a;
Parikh et al., 2016), demonstrating superior per-
formance relative to their non-attentive counter-
parts.

Today, the dominant state-of-the-art approaches
for text matching are mostly based on neural mod-

els configured with bidirectional attention layers
(Shen et al., 2017; Tay et al., 2017c). Bidirec-
tional attention comes in various flavours which
can be known as soft alignment (Shen et al., 2017;
Chen et al., 2017), decomposable attention (Parikh
et al., 2016), attentive pooling (dos Santos et al.,
2016) and even complex-valued attention (Tay
et al., 2018a). The key idea is to jointly soft align
text sequences such that they can be compared at
the index level. To this end, various comparison
functions have been utilized, ranging from feed-
forward neural networks (Parikh et al., 2016) to
factorization machines (Tay et al., 2017c). No-
tably, these attention (and bi-attention) mecha-
nisms are also widely adopted (or originated) from
many related sub-fields of NLP such as machine
translation (Bahdanau et al., 2014) and reading
comprehension (Xiong et al., 2016; Seo et al.,
2016; Wang and Jiang, 2016b).

Many text matching neural models are heav-
ily grounded in the compare-aggregate architec-
ture (Wang and Jiang, 2016a). In these models,
matching and comparisons occur between text se-
quences, aggregating features for making the final
prediction. Recent state-of-the-art models such
as BiMPM (Wang et al., 2017) and DIIN (Gong
et al., 2017) are representative of such architec-
tural paradigm, utilizing an attention-based match-
ing scheme and then a CNN or LSTM-based fea-
ture aggregator. Earlier works (Wan et al., 2016a;
He et al., 2015; He and Lin, 2016) exploit a similar
paradigm, albeit without the usage of attention.

Across many NLP and machine learning appli-
cations, utilizing stacked architectures is a com-
mon way to enhance representation capability of
the encoder (Sutskever et al., 2014; Graves et al.,
2013; Zhang et al., 2016; Nie and Bansal, 2017),
leading to performance improvement. Deep net-
works suffer from inherent difficulty in feature
propagation and/or vanishing/exploding gradients.
As a result, residual strategies have often been em-
ployed (He et al., 2016; Srivastava et al., 2015;
Huang et al., 2017). However, to the best of our
knowledge, this work presents a new way of resid-
ual connections, leveraging on the fact that pair-
wise formulation of the text matching task.

5 Conclusion

We proposed a deep stacked recurrent architec-
ture for general-purpose text sequence match-
ing. We proposed a new co-stack residual affin-



4500

ity mechanism for matching sequence pairs, lever-
aging multi-hierarchical information for learn-
ing bidirectional alignments. Our proposed
CSRAN model achieves state-of-the-art perfor-
mance across six well-studied benchmark datasets
and four different problem domains.

References
Martı́n Abadi, Ashish Agarwal, Paul Barham, Eugene

Brevdo, Zhifeng Chen, Craig Citro, Greg S. Cor-
rado, Andy Davis, Jeffrey Dean, Matthieu Devin,
Sanjay Ghemawat, Ian Goodfellow, Andrew Harp,
Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal
Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh
Levenberg, Dan Mané, Rajat Monga, Sherry Moore,
Derek Murray, Chris Olah, Mike Schuster, Jonathon
Shlens, Benoit Steiner, Ilya Sutskever, Kunal Tal-
war, Paul Tucker, Vincent Vanhoucke, Vijay Vasude-
van, Fernanda Viégas, Oriol Vinyals, Pete Warden,
Martin Wattenberg, Martin Wicke, Yuan Yu, and Xi-
aoqiang Zheng. 2015. TensorFlow: Large-scale ma-
chine learning on heterogeneous systems. Software
available from tensorflow.org.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
learning to align and translate. arXiv preprint
arXiv:1409.0473.

Weijie Bian, Si Li, Zhao Yang, Guang Chen, and
Zhiqing Lin. 2017. A compare-aggregate model
with dynamic-clip attention for answer selection. In
Proceedings of the 2017 ACM on Conference on In-
formation and Knowledge Management, CIKM ’17,
pages 1987–1990, New York, NY, USA. ACM.

Samuel R. Bowman, Gabor Angeli, Christopher Potts,
and Christopher D. Manning. 2015. A large an-
notated corpus for learning natural language infer-
ence. In Proceedings of the 2015 Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP 2015, Lisbon, Portugal, September 17-
21, 2015, pages 632–642.

Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei, Hui
Jiang, and Diana Inkpen. 2017. Enhanced LSTM for
natural language inference. In Proceedings of the
55th Annual Meeting of the Association for Compu-
tational Linguistics, ACL 2017, Vancouver, Canada,
July 30 - August 4, Volume 1: Long Papers, pages
1657–1668.

Reza Ghaeini, Sadid A Hasan, Vivek Datla, Joey
Liu, Kathy Lee, Ashequl Qadir, Yuan Ling, Aa-
ditya Prakash, Xiaoli Z Fern, and Oladimeji Farri.
2018. Dr-bilstm: Dependent reading bidirectional
lstm for natural language inference. arXiv preprint
arXiv:1802.05577.

Xavier Glorot and Yoshua Bengio. 2010. Understand-
ing the difficulty of training deep feedforward neu-
ral networks. In Proceedings of the thirteenth in-

ternational conference on artificial intelligence and
statistics, pages 249–256.

Yichen Gong, Heng Luo, and Jian Zhang. 2017.
Natural language inference over interaction space.
CoRR, abs/1709.04348.

Alex Graves, Abdel-rahman Mohamed, and Geoffrey
Hinton. 2013. Speech recognition with deep recur-
rent neural networks. In Acoustics, speech and sig-
nal processing (icassp), 2013 ieee international con-
ference on, pages 6645–6649. IEEE.

Hua He, Kevin Gimpel, and Jimmy J. Lin. 2015. Multi-
perspective sentence similarity modeling with con-
volutional neural networks. In Proceedings of the
2015 Conference on Empirical Methods in Natural
Language Processing, EMNLP 2015, Lisbon, Portu-
gal, September 17-21, 2015, pages 1576–1586.

Hua He and Jimmy J. Lin. 2016. Pairwise word in-
teraction modeling with deep neural networks for
semantic similarity measurement. In NAACL HLT
2016, The 2016 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, San Diego
California, USA, June 12-17, 2016, pages 937–948.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2016. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pages 770–
778.

Gao Huang, Zhuang Liu, Laurens van der Maaten, and
Kilian Q. Weinberger. 2017. Densely connected
convolutional networks. In 2017 IEEE Conference
on Computer Vision and Pattern Recognition, CVPR
2017, Honolulu, HI, USA, July 21-26, 2017, pages
2261–2269.

Kai Hui, Andrew Yates, Klaus Berberich, and Gerard
de Melo. 2017. Pacrr: A position-aware neural ir
model for relevance matching. In Proceedings of the
2017 Conference on Empirical Methods in Natural
Language Processing, pages 1049–1058.

Tushar Khot, Ashish Sabharwal, and Peter Clark. 2018.
Scitail: A textual entailment dataset from science
question answering. In AAAI.

Diederik P. Kingma and Jimmy Ba. 2014. Adam:
A method for stochastic optimization. CoRR,
abs/1412.6980.

Wuwei Lan, Siyu Qiu, Hua He, and Wei Xu. 2017.
A continuously growing dataset of sentential para-
phrases. arXiv preprint arXiv:1708.00391.

Wuwei Lan and Wei Xu. 2018. The importance of
subword embeddings in sentence pair modeling. In
Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies
(NAACL-HLT).



4501

Ryan Lowe, Nissan Pow, Iulian Serban, and Joelle
Pineau. 2015. The ubuntu dialogue corpus: A large
dataset for research in unstructured multi-turn dia-
logue systems. arXiv preprint arXiv:1506.08909.

Bryan McCann, James Bradbury, Caiming Xiong, and
Richard Socher. 2017. Learned in translation: Con-
textualized word vectors. In Advances in Neural In-
formation Processing Systems, pages 6297–6308.

Yixin Nie and Mohit Bansal. 2017. Shortcut-
stacked sentence encoders for multi-domain infer-
ence. In Proceedings of the 2nd Workshop on
Evaluating Vector Space Representations for NLP,
RepEval@EMNLP 2017, Copenhagen, Denmark,
September 8, 2017, pages 41–45.

Ankur P. Parikh, Oscar Täckström, Dipanjan Das, and
Jakob Uszkoreit. 2016. A decomposable attention
model for natural language inference. In Proceed-
ings of the 2016 Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP 2016,
Austin, Texas, USA, November 1-4, 2016, pages
2249–2255.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. In Proceedings of the 2014
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP 2014, October 25-29,
2014, Doha, Qatar, A meeting of SIGDAT, a Special
Interest Group of the ACL, pages 1532–1543.

Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word rep-
resentations. arXiv preprint arXiv:1802.05365.

Minh C Phan, Aixin Sun, Yi Tay, Jialong Han, and
Chenliang Li. 2017. Neupl: Attention-based seman-
tic matching and pair-linking for entity disambigua-
tion. In Proceedings of the 2017 ACM on Confer-
ence on Information and Knowledge Management,
pages 1667–1676. ACM.

Xipeng Qiu and Xuanjing Huang. 2015. Convolutional
neural tensor network architecture for community-
based question answering. In Proceedings of the
Twenty-Fourth International Joint Conference on
Artificial Intelligence, IJCAI 2015, Buenos Aires,
Argentina, July 25-31, 2015, pages 1305–1311.

Alec Radford, Karthik Narasimhan, Tim Salimans, and
Ilya Sutskever. Improving language understanding
by generative pre-training.

Jinfeng Rao, Hua He, and Jimmy J. Lin. 2016. Noise-
contrastive estimation for answer selection with
deep neural networks. In Proceedings of the 25th
ACM International on Conference on Information
and Knowledge Management, CIKM 2016, Indi-
anapolis, IN, USA, October 24-28, 2016, pages
1913–1916.

Jinfeng Rao, Wei Yang, Yuhao Zhang, Ferhan Ture,
and Jimmy Lin. 2018. Multi-perspective relevance
matching with hierarchical convnets for social me-
dia search. arXiv preprint arXiv:1805.08159.

Steffen Rendle. 2010. Factorization machines. In Data
Mining (ICDM), 2010 IEEE 10th International Con-
ference on, pages 995–1000. IEEE.

Tim Rocktäschel, Edward Grefenstette, Karl Moritz
Hermann, Tomáš Kočiskỳ, and Phil Blunsom. 2015.
Reasoning about entailment with neural attention.
arXiv preprint arXiv:1509.06664.

Cı́cero Nogueira dos Santos, Ming Tan, Bing Xiang,
and Bowen Zhou. 2016. Attentive pooling net-
works. CoRR, abs/1602.03609.

Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and
Hannaneh Hajishirzi. 2016. Bidirectional attention
flow for machine comprehension. arXiv preprint
arXiv:1611.01603.

Aliaksei Severyn and Alessandro Moschitti. 2015.
Learning to rank short text pairs with convolutional
deep neural networks. In Proceedings of the 38th
International ACM SIGIR Conference on Research
and Development in Information Retrieval, Santi-
ago, Chile, August 9-13, 2015, pages 373–382.

Gehui Shen, Yunlun Yang, and Zhi-Hong Deng. 2017.
Inter-weighted alignment network for sentence pair
modeling. In Proceedings of the 2017 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 1179–1189.

Yelong Shen, Xiaodong He, Jianfeng Gao, Li Deng,
and Grégoire Mesnil. 2014. A latent semantic model
with convolutional-pooling structure for informa-
tion retrieval. In Proceedings of the 23rd ACM In-
ternational Conference on Conference on Informa-
tion and Knowledge Management, pages 101–110.
ACM.

Rupesh Kumar Srivastava, Klaus Greff, and Jürgen
Schmidhuber. 2015. Highway networks. CoRR,
abs/1505.00387.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in neural information process-
ing systems, pages 3104–3112.

Yi Tay, Anh Tuan Luu, and Siu Cheung Hui. 2017a.
Enabling efficient question answer retrieval via hy-
perbolic neural networks. CoRR, abs/1707.07847.

Yi Tay, Anh Tuan Luu, and Siu Cheung Hui. 2018a.
Hermitian co-attention networks for text matching in
asymmetrical domains. In IJCAI, pages 4425–4431.

Yi Tay, Minh C. Phan, Anh Tuan Luu, and Siu Che-
ung Hui. 2017b. Learning to rank question answer
pairs with holographic dual LSTM architecture. In
Proceedings of the 40th International ACM SIGIR



4502

Conference on Research and Development in Infor-
mation Retrieval, Shinjuku, Tokyo, Japan, August 7-
11, 2017, pages 695–704.

Yi Tay, Luu Anh Tuan, and Siu Cheung Hui. 2017c.
A compare-propagate architecture with alignment
factorization for natural language inference. arXiv
preprint arXiv:1801.00102.

Yi Tay, Luu Anh Tuan, and Siu Cheung Hui. 2018b.
Cross temporal recurrent networks for ranking ques-
tion answer pairs. In Proceedings of AAAI 2018.

Yi Tay, Luu Anh Tuan, and Siu Cheung Hui. 2018c.
Multi-cast attention networks. In Proceedings of
KDD 2018, KDD ’18, pages 2299–2308, New York,
NY, USA. ACM.

Gaurav Singh Tomar, Thyago Duque, Oscar
Täckström, Jakob Uszkoreit, and Dipanjan
Das. 2017. Neural paraphrase identification of
questions with noisy pretraining. arXiv preprint
arXiv:1704.04565.

Shengxian Wan, Yanyan Lan, Jiafeng Guo, Jun Xu,
Liang Pang, and Xueqi Cheng. 2016a. A deep ar-
chitecture for semantic matching with multiple po-
sitional sentence representations. In Proceedings of
the Thirtieth AAAI Conference on Artificial Intel-
ligence, February 12-17, 2016, Phoenix, Arizona,
USA., pages 2835–2841.

Shengxian Wan, Yanyan Lan, Jun Xu, Jiafeng Guo,
Liang Pang, and Xueqi Cheng. 2016b. Match-srnn:
Modeling the recursive matching structure with spa-
tial rnn. arXiv preprint arXiv:1604.04378.

Bingning Wang, Kang Liu, and Jun Zhao. 2016a. In-
ner attention based recurrent neural networks for an-
swer selection. In Proceedings of the 54th Annual
Meeting of the Association for Computational Lin-
guistics, ACL 2016, August 7-12, 2016, Berlin, Ger-
many, Volume 1: Long Papers.

Mengqiu Wang, Noah A. Smith, and Teruko Mita-
mura. 2007. What is the jeopardy model? A quasi-
synchronous grammar for QA. In EMNLP-CoNLL
2007, Proceedings of the 2007 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
June 28-30, 2007, Prague, Czech Republic, pages
22–32.

Shuohang Wang and Jing Jiang. 2016a. A compare-
aggregate model for matching text sequences.
CoRR, abs/1611.01747.

Shuohang Wang and Jing Jiang. 2016b. Machine com-
prehension using match-lstm and answer pointer.
arXiv preprint arXiv:1608.07905.

Zhiguo Wang, Wael Hamza, and Radu Florian. 2017.
Bilateral multi-perspective matching for natural lan-
guage sentences.

Zhiguo Wang, Haitao Mi, and Abraham Ittycheriah.
2016b. Sentence similarity learning by lexical
decomposition and composition. arXiv preprint
arXiv:1602.07019.

Yu Wu, Wei Wu, Zhoujun Li, and Ming Zhou. 2016.
Knowledge enhanced hybrid neural network for text
matching. arXiv preprint arXiv:1611.04684.

Caiming Xiong, Victor Zhong, and Richard Socher.
2016. Dynamic coattention networks for question
answering. CoRR, abs/1611.01604.

Wei Xu, Alan Ritter, Chris Callison-Burch, William B
Dolan, and Yangfeng Ji. 2014. Extracting lexically
divergent paraphrases from twitter. Transactions
of the Association for Computational Linguistics,
2:435–448.

Lei Yu, Karl Moritz Hermann, Phil Blunsom, and
Stephen Pulman. 2014. Deep learning for answer
sentence selection. CoRR, abs/1412.1632.

Yu Zhang, Guoguo Chen, Dong Yu, Kaisheng Yaco,
Sanjeev Khudanpur, and James Glass. 2016. High-
way long short-term memory rnns for distant speech
recognition. In Acoustics, Speech and Signal Pro-
cessing (ICASSP), 2016 IEEE International Confer-
ence on, pages 5755–5759. IEEE.


