



















































Neural Semantic Parsing with Type Constraints for Semi-Structured Tables


Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1516–1526
Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics

Neural Semantic Parsing with Type Constraints
for Semi-Structured Tables

Jayant Krishnamurthy,1 Pradeep Dasigi,2 and Matt Gardner1
1Allen Institute for Artificial Intelligence

2Carnegie Mellon University
{jayantk, mattg}@allenai.org, pdasigi@cs.cmu.edu

Abstract

We present a new semantic parsing model
for answering compositional questions on
semi-structured Wikipedia tables. Our
parser is an encoder-decoder neural net-
work with two key technical innovations:
(1) a grammar for the decoder that only
generates well-typed logical forms; and
(2) an entity embedding and linking mod-
ule that identifies entity mentions while
generalizing across tables. We also in-
troduce a novel method for training our
neural model with question-answer super-
vision. On the WIKITABLEQUESTIONS
data set, our parser achieves a state-of-the-
art accuracy of 43.3% for a single model
and 45.9% for a 5-model ensemble, im-
proving on the best prior score of 38.7%
set by a 15-model ensemble. These re-
sults suggest that type constraints and en-
tity linking are valuable components to in-
corporate in neural semantic parsers.

1 Introduction

Semantic parsing is the problem of translating hu-
man language into computer language, and there-
fore is at the heart of natural language understand-
ing. A typical semantic parsing task is question
answering against a database, which is accom-
plished by translating questions into executable
logical forms (i.e., programs) that output their an-
swers. Recent work has shown that recurrent neu-
ral networks can be used for semantic parsing by
encoding the question then predicting each token
of the logical form in sequence (Jia and Liang,
2016; Dong and Lapata, 2016). These approaches,
while effective, have two major limitations. First,
they treat the logical form as an unstructured se-
quence, thereby ignoring type constraints on well-

formed programs. Second, they do not address en-
tity linking, which is a critical subproblem of se-
mantic parsing (Yih et al., 2015).

This paper introduces a novel neural semantic
parsing model that addresses these limitations of
prior work. Our parser uses an encoder-decoder
architecture with two key innovations. First, the
decoder generates from a grammar that guarantees
that generated logical forms are well-typed. This
grammar is automatically induced from typed log-
ical forms, and does not require any manual engi-
neering to produce. Second, the encoder incorpo-
rates an entity linking and embedding module that
enables it to learn to identify which question spans
should be linked to entities. Finally, we also intro-
duce a new approach for training neural semantic
parsers from question-answer supervision.

We evaluate our parser on WIKITABLEQUES-
TIONS, a challenging data set for question answer-
ing against semi-structured Wikipedia tables (Pa-
supat and Liang, 2015). This data set has a broad
variety of entities and relations across different ta-
bles, along with complex questions that necessi-
tate long logical forms. On this data set, our parser
achieves a question answering accuracy of 43.3%
and an ensemble of 5 parsers achieves 45.9%, both
of which outperform the previous state-of-the-art
of 38.7% set by an ensemble of 15 models (Haug
et al., 2017). We further perform several ablation
studies that demonstrate the importance of both
type constraints and entity linking to achieving
high accuracy on this task.

2 Related Work

Semantic parsers vary along a few important di-
mensions:

Formalism Early work on semantic parsing
used lexicalized grammar formalisms such as
Combinatory Categorial Grammar (Zettlemoyer

1516



and Collins, 2005, 2007; Kwiatkowski et al., 2011,
2013; Krishnamurthy and Mitchell, 2012; Artzi
and Zettlemoyer, 2013) and others (Liang et al.,
2011; Berant et al., 2013; Zhao and Huang, 2015;
Wong and Mooney, 2006, 2007). These for-
malisms have the advantage of only generating
well-typed logical forms, but the disadvantage of
introducing latent syntactic variables that make
learning difficult. Another approach is to treat se-
mantic parsing as a machine translation problem,
where the logical form is linearized then predicted
as an unstructured sequence of tokens (Andreas
et al., 2013). This approach is taken by recent neu-
ral semantic parsers (Jia and Liang, 2016; Dong
and Lapata, 2016; Locascio et al., 2016; Ling
et al., 2016). This approach has the advantage
of predicting the logical form directly from the
question without latent variables, which simpli-
fies learning, but the disadvantage of ignoring type
constraints on logical forms. Our type-constrained
neural semantic parser inherits the advantages of
both approaches: it only generates well-typed log-
ical forms and has no syntactic latent variables as
every logical form has a unique derivation. Recent
work has explored similar ideas to ours in the con-
text of Python code generation (Yin and Neubig,
2017; Rabinovich et al., 2017).

Entity Linking Identifying the entities men-
tioned in a question is a critical subproblem of se-
mantic parsing in broad domains and proper entity
linking can lead to large accuracy improvements
(Yih et al., 2015). However, semantic parsers have
typically ignored this problem by assuming that
entity linking is done beforehand (as the neural
parsers above do) or using a simple parameteri-
zation for the entity linking portion (as the lexical-
ized parsers do). Our parser explicitly includes an
entity linking module that enables it to model the
highly ambiguous and implicit entity mentions in
WIKITABLEQUESTIONS.

Supervision Semantic parsers can be trained
from labeled logical forms (Zelle and Mooney,
1996; Zettlemoyer and Collins, 2005) or question-
answer pairs (Liang et al., 2011; Berant et al.,
2013). Question-answer pairs were considered
easier to obtain than labeled logical forms, though
recent work has demonstrated that logical forms
can be collected efficiently and are more effec-
tive (Yih et al., 2016). However, a key advantage
of question-answer pairs is that they are agnos-
tic to the domain representation and logical form

language (e.g., lambda calculus or λ-DCS). This
property is important for problems such as semi-
structured tables where the proper domain repre-
sentation is unclear.

Data Sets We use WIKITABLEQUESTIONS to
evaluate our parser as this data set exhibits both
a broad domain and complex questions. Early
data sets, such as GEOQUERY (Zelle and Mooney,
1996) and ATIS (Dahl et al., 1994), have small
domains with only a handful of different predi-
cates. More recent data sets for question answer-
ing against Freebase have a much broader domain,
but simple questions (Berant et al., 2013; Cai and
Yates, 2013).

3 Model

This section describes our semantic parsing model
and training procedure. For clarity, we describe
the model on WIKITABLEQUESTIONS, though it
is also applicable to other problems. The input
to our model is a natural language question and a
context in which it is to be answered, which in our
task is a table. The model predicts the answer to
the question by semantically parsing it to a logical
form then executing it against the table.

Our model follows an encoder-decoder archi-
tecture using recurrent neural networks with Long
Short Term Memory (LSTM) cells (Hochreiter
and Schmidhuber, 1997). The input question and
table entities are first encoded as vectors that are
then decoded into a logical form (Figure 1). We
make two significant additions to the encoder-
decoder architecture. First, the encoder includes a
special entity embedding and linking module that
produces a link embedding for each question token
that represents the table entities it links to (Section
3.2). Second, the action space of the decoder is de-
fined by a type-constrained grammar which guar-
antees that generated logical forms satisfy type
constraints (Section 3.3).

We train the parser using question-answer pairs
as supervision using an approximation of marginal
loglikelihood based on enumerating logical forms
via dynamic programming on denotations (Pasu-
pat and Liang, 2016) (Section 3.4). This approx-
imation makes it possible to train neural models
with question-answer supervision, which is other-
wise difficult for efficiency and gradient variance
reasons.

1517



to decoder

to decoder

Predicted 
Grammar 

Ruleso1 onon

Encoder

Entity 
Linking

Which athlete was from 
South Korea after the year 2010?

((reverse athlete) (and (nation south_korea)
(year (num2cell (>= 2010)))))

g0  which             athlete         ...         2010

LSTM

Table Knowledge Graph

nation

from encoder

great 
britain

south 
korea

LSTM

Entity 
Linking

...

Entity 
Linking

LSTM

Attention

c

g1

LSTM

Attention

c → <r,c>

...

Question Logical
Form

Decoder

Knowledge graph 
Embedding

LSTM

athlete

karl 
schafer

kim 
yu-na

year

2002

2014

Figure 1: Overview of our semantic parsing model. The encoder performs entity embedding and linking
before encoding the question with a bidirectional LSTM. The decoder predicts a sequence of grammar
rules that generate a well-typed logical form.

3.1 Preliminaries

We follow (Pasupat and Liang, 2015) in using
the same table structure representation and λ-DCS
language for expressing logical forms. In this
representation, tables are expressed as knowledge
graphs over 6 types of entities: cells, cell parts,
rows, columns, numbers and dates. Each entity
also has a name, which is typically a string value
in the table. Our parser uses both the entity names
and the knowledge graph structure to construct
embeddings for each entity.

The logical form language consists of a collec-
tion of named sets and entities, along with oper-
ators on them. The named sets are used to se-
lect table cells, e.g., united states is the set
of cells that contain the text “united states”. The
operators include functions from sets to sets, e.g.,
the next operator maps a row to the next row.
Columns are treated as functions from cells to
their rows, e.g., (country united states)
generates the rows whose country column con-
tains “united states”. Other operators include re-
versing relations (e.g., in order to map rows to
cells in a certain column), relations that interpret
cells as numbers and dates, and set and arithmetic
operations. The language also includes aggrega-
tion and quantification operations such as count
and argmax, along with λ abstractions that can
be used to join binary relations.

Our parser also assigns a type to every λ-DCS
expression, which is used to enforce type con-
straints on generated logical forms. The base
types are cells c, parts p, rows r, numbers i,
and dates d. Columns such as country have
the functional type 〈c, r〉, representing functions

from cells c to rows r. Other operations have
more complex functional types, e.g., reverse
has type 〈〈c, r〉, 〈r, c〉〉, which enables us to write
(reverse country).1 The parser assigns ev-
ery λ-DCS constant a type, then applies standard
programming language type inference algorithms
(Pierce, 2002) to automatically assign types to
larger expressions.

3.2 Encoder

The encoder is a bidirectional LSTM augmented
with an entity embedding and linking module.

Notation. Throughout this section, we denote
entities as e, and their corresponding types as τ(e).
The set of all entities is denoted as E, and the en-
tities with type τ as Eτ . E includes the cells, cell
parts, and columns from the table in addition to
numeric entities detected in the question by NER.
The question is denoted as a sequence of tokens
[q1, ..., qn]. We use vw to denote a learned vec-
tor representation (embedding) of wordw, e.g., vqi
denotes the vector representation of the ith ques-
tion token.

Entity Embedding. The encoder first constructs
an embedding for each entity in the knowledge
graph given its type and position in the graph. Let
W (e) denote the set of words in the name of en-

1Technically, reverse has the parametric polymorphic
type 〈〈α, β〉, 〈β, α〉〉, where α and β are type variables that
can be any type. This type allows reverse to reverse any
function. However, this is a detail that can largely be ig-
nored. We only use parametric polymorphism when typing
logical forms to generate the type-constrained grammar; the
grammar itself does not have type variables, but rather a fixed
number of concrete instances – such as 〈〈c, r〉, 〈r, c〉〉 – of
the above polymorphic type.

1518



tity e and N(e) the neighbors of entity e in the
knowledge graph. Specifically, the neighbors of a
column are the cells it contains, and the neighbors
of a cell are the columns it belongs to. Each en-
tity’s embedding re is a nonlinear projection of a
type vector vτ(e) and a neighbor vector vN(e):

vN(e) =
1

|N(e)|
∑

e′∈N(e)

1
|W (e′)|

∑
w∈W (e′)

vw

re = tanh
(
Pτvτ(e) + PNvN(e)

)
The type vector vτ(e) is a one-hot vector for τ(e),
with dimension equal to the number of entity types
in the grammar. The neighbor vector vN(e) is sim-
ply an average of the word vectors in the names of
e’s neighbors. Pτ and PN are learned parameter
matrices for combining these two vectors.

Entity Linking. This module generates a link
embedding li for each question token representing
the entities it links to. The first part of this module
generates an entity linking score s(e, i) for each
entity e and token index i:

s(e, i) = max
w∈W (e)

vᵀwvqi + ψ
ᵀφ(e, i)

This score has two terms. The first represents
similarity in word embedding space between the
token and entity name, computed as function of
the embeddings of words in e’s name, W (e), and
the word embedding of the ith token, vqi . The
max-pooling architecture allows each question to-
ken to pick its best match in the entity’s name. The
second term represents a linear classifier with pa-
rameters ψ on features φ(e, i). The feature func-
tion φ includes only a few features: indicators for
exact token and lemma match, edit distance, an
NER tag indicator, and a bias feature. It also in-
cludes “related column” versions of the token and
lemma features that are active when e is a col-
umn and the original feature matches a cell entity
in column e. We found that features were an ef-
fective way to address sparsity in the entity name
tokens, many of which appear too infrequently to
learn embeddings for. We produce an independent
score for each entity and token index even though
we expect entities to link to multi-token spans in
order to avoid the quadratic computational com-
plexity of scoring each span.

Finally, the entity embeddings and linking
scores are combined to produce a link embedding
for each token. The scores s(e, i) are then fed into
a softmax layer over all entities e of the same type,

and the link embedding li is an average of entity
vectors re weighted by the resulting distribution,
then summed over all types. We include a null en-
tity, ∅, in each softmax layer to permit the model
to identify tokens that do not refer to an entity. The
null entity’s embedding is the all-zero vector and
its score s(∅, ·) = 0. Note that the null entity may
still be assigned high probability as the other entity
scores of may be negative. The link embedding `i
for the ith question token is computed as:

p(e|i, τ) = exp s(e, i)∑
e′∈Eτ∪{∅} exp s(e

′, i)

li =
∑
τ

∑
e∈Eτ

rep(e|i, τ)

For WIKITABLEQUESTIONS, we ran the entity
embedding and linking module over every entity.
However, this approach may be prohibitively ex-
pensive in applications with a very large number of
entities. In these cases, our method can be applied
by adding a preliminary filtering step to identify
a subset of entities that may be mentioned in the
question. This filter need not have high precision,
and therefore could rely on simple text overlap or
similarity heuristics. This reduced set of entities
can then be fed into the entity embedding and link-
ing module, which will learn to further prune this
set of candidates.

Bidirectional LSTM. We concatenate the link
embedding li and the word embedding vqi of each
token in the question, and feed them into a bidi-
rectional LSTM:

xi =
[
li
vqi

]
(ofi , fi) = LSTM(fi−1, xi)

(obi , bi) = LSTM(bi+1, xi)

oi =
[
ofi
obi

]
This process produces an encoded vector repre-

sentation of each token oi. The final LSTM hid-
den states fn+1 and b0 are concatenated and used
to initialize the decoder.

3.3 Decoder

The decoder is an LSTM with attention that selects
parsing actions from a grammar over well-typed
logical forms.

1519



Type-Constrained Grammar. The parser
maintains a state at each step of decoding that
consists of a logical form with nonterminals
standing for portions that are yet to be generated.
Each nonterminal is a tuple [τ,Γ] of a type τ and
a scope Γ that contains typed variable bindings,
(x : α) ∈ Γ, where x is a variable name and α is
a type. The scope is used to store and generate the
arguments of lambda expressions. The grammar
consists of a collection of four kinds of production
rules on nonterminals:

1. Application [τ,Γ]→([〈β, τ〉,Γ] [β,Γ])
rewrites a nonterminal of type τ by applying
a function from β to τ to an argument of type
β. We also permit applications with more
than one argument.

2. Constant [τ,Γ]→const where constant
const has type τ . This rule generates
both table-independent operations such as
argmax and table-specific entities such as
united states.

3. Lambda [〈α, τ〉,Γ]→ λx. [τ,Γ ∪ {(x : α)}]
generates a lambda expression where the ar-
gument has type α. x represents a fresh vari-
able name. The right hand side of this rule
extends the scope Γ with a binding for x then
generates an expression of type τ .

4. Variable [τ,Γ]→ x where (x : τ) ∈ Γ.
This rule generates a variable bound in a
previously-generated lambda expression that
is currently in scope.

We instantiate each of the four rules above by
replacing the type variables τ, α, β with concrete
types, producing, e.g., [c,Γ] → ([〈r, c〉,Γ] [r,Γ])
from the application rule. The set of instanti-
ated rules is automatically derived from a cor-
pus of logical forms, which we in turn produce
by running dynamic programming on denotations
(see Section 3.4). Every logical form can be de-
rived in exactly one way using the four kinds of
rules above; this derivation is combined with the
(automatically-assigned) type of each of the log-
ical form’s subexpressions to instantiate the type
variables in each rule. Finally, as a postprocessing
step, we filter out table-dependent rules, such as
those that generate table cells, to produce a table-
independent grammar. The table-dependent rules
are handled specially by the decoder in order to

guarantee that they are only generated when an-
swering questions against the appropriate table.
The table-independent grammar generates well-
typed expressions that include functions such as
next and quantifiers such as argmax; however,
it cannot generate cells, columns, or other table
entities.

The first action of the parser is to predict a
root type for the logical form, and then decoding
proceeds according to the production rules above.
Each time step of decoding fills the leftmost non-
terminal in the logical form, and decoding ter-
minates when no nonterminals remain. Figure 2
shows the sequence of decoder actions used to
generate an example logical form.

Network Architecture. The decoder is an
LSTM that outputs a distribution over grammar
actions using an attention mechanism over the en-
coded question tokens. The decoder also uses a
copy-like mechanism on the entity linking scores
to generate entities. Say that, during the jth time
step, the current nonterminal has type τ . The de-
coder generates a score for each grammar action
whose left-hand side is τ using the following equa-
tions:

(yj , hj) = LSTM(hj−1,
[
gj−1
oj−1

]
) (1)

aj = softmax(OW ayj) (2)

oj = (aj)TO (3)

sj = W 2τ relu(W
1

[
yj
oj

]
+ b1) + b2τ (4)

sj(ek) =
∑
i

s(ek, i)aji (5)

pj = softmax(


sj

sj(e1)
sj(e2)
...

) (6)
The input to the LSTM gj−1 is a grammar ac-

tion embedding for the action chosen in previ-
ous time step. g0 is a learned parameter vec-
tor, and h0 is the concatenated final hidden states
of the encoder LSTMs. The matrix O contains
the encoded token vectors o1, ..., , on from the en-
coder. The first three lines above perform a soft-
max attention over O using a learned parameter
matrix W a. The fourth line generates scores sj
for the table-independent grammar rules applica-
ble to type τ using a multilayer perceptron with

1520



Figure 2: The derivation of a logical form using
the type-constrained grammar. The nonterminals
in the left column have empty scope, and those in
the right column have scope Γ = {(x : r)}

weights W 1,b1,W 2τ ,b
2
τ . The fifth line generates a

score for each entity e with type τ by averaging
the entity linking scores with the current attention
aj . Finally, the table-independent and -dependent
scores are concatenated and softmaxed to produce
a probability distribution pj over grammar actions.
If a table-independent action is chosen, gj is a
learned parameter vector for that action. Other-
wise gj = gτ , which is a learned parameter repre-
senting the selection of an entity with type τ .

3.4 DPD Training

Our parser is trained from question-answer pairs,
treating logical forms as a latent variable. We use
an approximate marginal loglikelihood objective
function that first automatically enumerates a set
of correct logical forms for each example, then
trains on these logical forms. This objective sim-
plifies the search problem during training and is
well-suited to training our neural model.

The training data consists of a collection of
n question-answer-table triples, {(qi, ai, T i)}ni=1.
We first run dynamic programming on denotations
(Pasupat and Liang, 2016) on each table T i and
answer ai to generate a set of logical forms ` ∈ Li
that execute to the correct answer. Dynamic pro-
gramming on denotations (DPD) is an automatic
procedure for enumerating logical forms that exe-
cute to produce a particular value; it leverages the
observation that there are fewer denotations than
logical forms to enumerate this set relatively effi-

ciently. However, many of these logical forms are
spurious, in the sense that they do not represent the
question’s meaning. Therefore, the objective must
marginalize over the many logical forms generated
in this fashion:

O(θ) =
n∑
i=1

log
∑
`∈Li

P (`|qi, T i; θ)

We optimize this objective function using
stochastic gradient descent. If |Li| is small, e.g.,
5-10, the gradient of the ith example can be com-
puted exactly by simply replicating the parser’s
network architecture |Li| times, once per logical
form. However, |Li| often contains many thou-
sands of logical forms, which makes the above
computation infeasible. We address this problem
by truncating Li to the m = 100 shortest logical
forms, then using a beam search with a beam of
k = 5 to approximate the sum. Section 4.5 con-
siders the effect of varying the number of logical
forms m in this objective function.

We briefly contrast this approach with two other
commonly-used approaches. The first is a sim-
ilar marginal loglikelihood objective commonly
used in prior semantic parsing work with loglin-
ear models (Liang et al., 2011; Pasupat and Liang,
2015). However, this approach does not precom-
pute correct logical forms. Therefore, computing
its gradient requires running a wide beam search,
generating, e.g., 300 logical forms, executing each
one to identify which are correct, then backprop-
agating through a term for each. The wide beam
is required to find correct logical forms; however,
such a wide beam is prohibitively expensive with
a neural model due to the cost of each backpropa-
gation pass. Another approach is to train the net-
work with REINFORCE (Williams, 1992), which
essentially samples a logical form instead of using
beam search. This approach is known to be diffi-
cult to apply when the space of outputs is large and
the reward signal is sparse, and recent work has
found that maximizing marginal loglikelihood is
more effective in these circumstances (Guu et al.,
2017). Our approach makes it tractable to maxi-
mize marginal loglikelihood with a neural model
by using DPD to enumerate correct logical forms
beforehand. This up-front enumeration, combined
with the local normalization of the neural model,
makes it possible to restrict the beam search to
correct logical forms in the gradient computation,
which enables training with a small beam size.

1521



4 Evaluation

We evaluate our parser on the WIKITABLEQUES-
TIONS data set by comparing it to prior work and
ablating several components to understand their
contributions.

4.1 Experimental Setup

We used the standard train/test splits of WIK-
ITABLEQUESTIONS. The training set consists of
14,152 examples and the test set consists of 4,344
examples. The training set comes divided into 5
cross-validation folds for development using an
80/20 split. All data sets are constructed so that
the development and test tables are not present
in the training set. We report question answer-
ing accuracy measured using the official evalua-
tion script, which performs some simple normal-
ization of numbers, dates, and strings before com-
paring predictions and answers. When generating
answers from a model’s predictions, we skip logi-
cal forms that do not execute (which may occur for
some baseline models) or answer with the empty
string (which is never correct). All reported ac-
curacy numbers are an average of 5 parsers, each
trained on one training fold, using the respective
development set to perform early stopping.

We trained our parser with 20 epochs of
stochastic gradient descent. We used 200-
dimensional word embeddings for the question
and entity tokens, mapping all tokens that oc-
curred < 3 times in the training questions to UNK.
(We tried using a larger vocabulary that included
frequent tokens in tables, but this caused the
parser to seriously overfit.) The hidden and out-
put dimensions of the forward/backward encoder
LSTMs were set to 100, such that the concatenated
representations were also 200-dimensional. The
decoder LSTM uses 100-dimensional action em-
beddings and has a 200-dimensional hidden state
and output. The action selection MLP has a hidden
layer dimension of 100. We used a dropout proba-
bility of 0.5 on the output of both the encoder and
decoder LSTMs, as well as on the hidden layer
of the action selection MLP. All parameters are
initialized using Glorot initialization (Glorot and
Bengio, 2010). The learning rate for SGD is ini-
tialized to 0.1 with a decay of 0.01. At test time,
we decode with a beam size of 10.

Our model is implemented as a probabilis-
tic neural program (Murray and Krishnamurthy,
2016). This Scala library combines ideas from dy-

namic neural network frameworks (Neubig et al.,
2017) and probabilistic programming (Goodman
and Stuhlmüller, 2014) to simplify the imple-
mentation of complex neural structured prediction
models. This library enables a user to specify the
structure of the model in terms of discrete nonde-
terministic choices – as in probabilistic program-
ming – where a neural network is used to score
each choice. We implement our parser by defining
P (`|q, T ; θ), from which the library automatically
implements both inference and training. In partic-
ular, the beam search and the corresponding back-
propagation bookkeeping to implement the objec-
tive in Section 3.4 are both automatically handled
by the library. Code and supplementary material
for this paper are available at:
http://allenai.org/paper-appendix/emnlp2017-wt/

4.2 Results
Table 1 compares the accuracy of our semantic
parser to prior work on WIKITABLEQUESTIONS.
We distinguish between single models and ensem-
bles, as we expect ensembling to improve accu-
racy, but not all prior work has used it. Prior
work on this data set includes a loglinear semantic
parser (Pasupat and Liang, 2015), that same parser
with a neural, paraphrase-based reranker (Haug
et al., 2017), and a neural programmer that an-
swers questions by predicting a sequence of table
operations (Neelakantan et al., 2017). We find that
our parser outperforms the best prior result on this
data set by 4.6%, despite that prior result using
a 15-model ensemble. An ensemble of 5 parsers
improves accuracy by an additional 2.6% for a
total improvement of 7.2%. This ensemble was
constructed by averaging the logical form proba-
bilities of parsers trained on each of the 5 cross-
validation folds. Note that this ensemble is trained
on the entire training set – the development data
from one fold is training data for the others – so
we therefore cannot report its development accu-
racy. We investigate the sources of this accuracy
improvement in the remainder of this section via
ablation experiments.

4.3 Type Constraints
Our second experiment measures the importance
of type constraints on the decoder by comparing it
to sequence-to-sequence (seq2seq) and sequence-
to-tree (seq2tree) models. The seq2seq model gen-
erates the logical form a token at a time, e.g.,
[(, (,reverse, ...], and has been used in several

1522



Ensemble
Model Size Dev. Test

Neelakantan et al. (2017) 1 34.1 34.2
Haug et al. (2017) 1 - 34.8
Pasupat and Liang (2015) 1 37.0 37.1
Neelakantan et al. (2017) 15 37.5 37.7
Haug et al. (2017) 15 - 38.7
Our Parser 1 42.7 43.3
Our Parser 5 - 45.9

Table 1: Development and test set accuracy of our
semantic parser compared to prior work on WIK-
ITABLEQUESTIONS.

recent neural semantic parsers (Jia and Liang,
2016; Dong and Lapata, 2016). The seq2tree
model improves on the seq2seq model by includ-
ing an action for generating matched parenthe-
ses, then recursively generating the subtree within
(Dong and Lapata, 2016). These baseline models
use the same network architecture (including en-
tity embedding and linking) and training regime
as our parser, but assign every constant the same
type and have a different grammar in the decoder.
These models were implemented by preprocessing
logical forms and applying a different type system.

Table 2 compares the accuracy of our parser to
both the seq2seq and seq2tree baselines. Both of
these models perform considerably worse than our
parser, demonstrating the importance of type con-
straints during decoding. Interestingly, we found
that both baselines typically generate well-formed
logical forms: only 7.4% of seq2seq and 6.6% of
seq2tree’s predicted logical forms failed to exe-
cute. Type constraints prevent these errors from
occurring in our parser, though the relatively small
number of such errors does not does not seem to
fully explain the 9% accuracy improvement. We
hypothesize that the additional improvement oc-
curs because type constraints also increase the ef-
fective capacity of the model, as both the seq2seq
and seq2tree models must use some of their capac-
ity to learn the type constraints on logical forms.

4.4 Entity Embedding and Linking

Our next experiment measures the contribution of
the entity embedding and linking module. We
trained several ablated versions of our parser, re-
moving both the embedding similarity and featur-
ized classifier from the entity linking module. Ta-
ble 3 shows the accuracy of the resulting models.

Model Dev. Accuracy

seq2seq 31.3
seq2tree 31.6
Our Parser 42.7

Table 2: Development accuracy of our seman-
tic parser compared to sequence-to-sequence and
sequence-to-tree models.

Model Dev. Accuracy

Full model 42.7
token features, no similarity 28.1
all features, no similarity 37.8
similarity only, no features 27.5

Table 3: Development accuracy of ablated parser
variants trained without parts of the entity linking
module.

The results demonstrate that the entity linking fea-
tures are important, particularly the more complex
features beyond simple token matching. In our ex-
perience, the “related column” features are espe-
cially important for this data set, as columns that
appear in the logical form are often not mentioned
in the text, but rather implied by a mention of a
cell from the column. Embedding similarity alone
is not very effective, but it does improve accuracy
when combined with the featurized classifier. We
found that word embeddings enabled the parser to
overfit, which may be due to the relatively small
size of the training set, or because we did not use
pretrained embeddings. Incorporating pretrained
embeddings is an area for future work.

We also examined the effect of the entity em-
beddings computed using each entity’s knowledge
graph context by replacing them with one-hot vec-
tors for the entity’s type. The accuracy of this
parser dropped from 42.7% to 41.8%, demonstrat-
ing that the knowledge graph embeddings help.

4.5 DPD Training

Our final experiment examines the impact on ac-
curacy of varying the number of logical forms m
used when training with dynamic programming on
denotations. Table 4 shows the development ac-
curacy of several parsers trained with varying m.
These results demonstrate that using more logical
forms generally leads to higher accuracy.

1523



# of logical forms 1 5 10 50 100
Dev. Accuracy 39.7 41.9 41.6 43.1 42.7

Table 4: Development accuracy of our semantic
parser when trained with varying numbers of log-
ical forms produced by dynamic programming on
denotations.

4.6 Error Analysis

To better understand the mistakes made by our
system, we analyzed a randomly selected set of
100 questions that were answered incorrectly. We
identified three major classes of error:

Parser errors (41%): These are examples
where a correct logical form is available, but the
parser does not select it. A large number of these
errors (15%) occur on questions that require se-
lecting an answer from a given list of options, as
in Who had more silvers, Colombia or The Ba-
hamas? In such cases, the type of the predicted
answer is often wrong. Another common subclass
is entity linking errors due to missing background
knowledge (13%), e.g., understanding that largest
implicitly refers to the Area column.

Representation failures (25%): The knowl-
edge graph representation makes certain assump-
tions about the table structure and cell values
which are sometimes wrong. One common prob-
lem is that the graph lacks some cell parts neces-
sary to answer the question (15%). For example,
answering a question asking for a state may re-
quire splitting cell values in the Location column
into city and state names. Another common prob-
lem is unusual table structures (10%), such as a
table listing the number of Olympic medals won
by each country that has a final row for the to-
tals. These structures often cause quantifiers such
as argmax to select the wrong row.

Unsupported operations (11%): These are ex-
amples where the logical form language lacks a
necessary function. Examples of missing func-
tions are finding consecutive sets of values, com-
puting percentages and performing string opera-
tions on cell values.

5 Conclusion

We present a new semantic parsing model for
answering compositional questions against semi-
structured Wikipedia tables. Our semantic parser

extends recent neural semantic parsers by enforc-
ing type constraints during logical form genera-
tion, and by including an explicit entity embed-
ding and linking module that enables it to iden-
tify entity mentions while generalizing across ta-
bles. An evaluation on WIKITABLEQUESTIONS
demonstrates that our parser achieves state-of-the-
art results, and furthermore that both type con-
straints and entity linking make significant contri-
butions to accuracy. Analyzing the errors made by
our parser suggests that improving entity linking
and using the table structure are two directions for
future work.

References

Jacob Andreas, Andreas Vlachos, and Stephen Clark.
2013. Semantic parsing as machine translation. In
Proceedings of the 51st Annual Meeting of the Asso-
ciation for Computational Linguistics.

Yoav Artzi and Luke Zettlemoyer. 2013. Weakly su-
pervised learning of semantic parsers for mapping
instructions to actions. Transactions of the Associa-
tion for Computational Linguistics 1(1):49–62.

Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic parsing on Freebase from
question-answer pairs. In Proceedings of the 2013
Conference on Empirical Methods in Natural Lan-
guage Processing.

Qingqing Cai and Alexander Yates. 2013. Large-scale
semantic parsing via schema matching and lexicon
extension. In In Proceedings of the Annual Meeting
of the Association for Computational Linguistics.

Deborah A. Dahl, Madeleine Bates, Michael Brown,
William Fisher, Kate Hunicke-Smith, David Pallett,
Christine Pao, Alexander Rudnicky, and Elizabeth
Shriberg. 1994. Expanding the scope of the ATIS
task: The ATIS-3 corpus. In Proceedings of the
Workshop on Human Language Technology.

Li Dong and Mirella Lapata. 2016. Language to logi-
cal form with neural attention. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers).

Xavier Glorot and Yoshua Bengio. 2010. Understand-
ing the difficulty of training deep feedforward neu-
ral networks. In Proceedings of the Thirteenth In-
ternational Conference on Artificial Intelligence and
Statistics. pages 249–256.

Noah D Goodman and Andreas Stuhlmüller. 2014. The
Design and Implementation of Probabilistic Pro-
gramming Languages. http://dippl.org. Ac-
cessed: 2017-4-13.

1524



Kelvin Guu, Panupong Pasupat, Evan Zheran Liu,
and Percy Liang. 2017. From language to pro-
grams: Bridging reinforcement learning and maxi-
mum marginal likelihood. In Association for Com-
putational Linguistics (ACL).

Till Haug, Octavian-Eugen Ganea, and Paulina
Grnarova. 2017. Neural multi-step reasoning
for question answering on semi-structured tables
http://arxiv.org/abs/1702.06589.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation
9(8):1735–1780.

Robin Jia and Percy Liang. 2016. Data recombination
for neural semantic parsing. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers).

Jayant Krishnamurthy and Tom M. Mitchell. 2012.
Weakly supervised training of semantic parsers. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning.

Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and Luke
Zettlemoyer. 2013. Scaling semantic parsers with
on-the-fly ontology matching. In Proceedings of the
2013 Conference on Empirical Methods in Natural
Language Processing.

Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-
ter, and Mark Steedman. 2011. Lexical generaliza-
tion in CCG grammar induction for semantic pars-
ing. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing.

Percy Liang, Michael I Jordan, and Dan Klein. 2011.
Learning dependency-based compositional seman-
tics. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies-Volume 1.

Wang Ling, Phil Blunsom, Edward Grefenstette,
Karl Moritz Hermann, Tomáš Kočiský, Fumin
Wang, and Andrew Senior. 2016. Latent predictor
networks for code generation. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers).

Nicholas Locascio, Karthik Narasimhan, Eduardo
De Leon, Nate Kushman, and Regina Barzilay.
2016. Neural generation of regular expressions from
natural language with minimal domain knowledge.
In Proceedings of the 2016 Conference on Empiri-
cal Methods in Natural Language Processing.

Kenton W. Murray and Jayant Krishna-
murthy. 2016. Probabilistic neural programs
http://arxiv.org/abs/1612.00712.

Arvind Neelakantan, Quoc V. Le, Martı́n Abadi, An-
drew McCallum, and Dario Amodei. 2017. Learn-
ing a natural language interface with neural pro-
grammer. In ICLR.

Graham Neubig, Chris Dyer, Yoav Goldberg, Austin
Matthews, Waleed Ammar, Antonios Anasta-
sopoulos, Miguel Ballesteros, David Chiang,
Daniel Clothiaux, Trevor Cohn, et al. 2017.
DyNet: The dynamic neural network toolkit
https://arxiv.org/abs/1701.03980.

Panupong Pasupat and Percy Liang. 2015. Compo-
sitional semantic parsing on semi-structured tables.
In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the
7th International Joint Conference on Natural Lan-
guage Processing (Volume 1: Long Papers).

Panupong Pasupat and Percy Liang. 2016. Inferring
logical forms from denotations. In Proceedings of
the 54th Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers).

Benjamin C. Pierce. 2002. Types and Programming
Languages. The MIT Press, 1st edition.

Maxim Rabinovich, Mitchell Stern, and Dan Klein.
2017. Abstract syntax networks for code genera-
tion and semantic parsing. In The 55th Annual Meet-
ing of the Association for Computational Linguistics
(ACL).

Ronald J. Williams. 1992. Simple statistical gradient-
following algorithms for connectionist reinforce-
ment learning. Machine Learning 8(3):229–256.

Yuk Wah Wong and Raymond J. Mooney. 2006. Learn-
ing for semantic parsing with statistical machine
translation. In Proceedings of the Human Language
Technology Conference of the NAACL.

Yuk Wah Wong and Raymond J. Mooney. 2007. Learn-
ing synchronous grammars for semantic parsing
with lambda calculus. In Proceedings of the 45th
Annual Meeting of the Association for Computa-
tional Linguistics.

Scott Wen-tau Yih, Ming-Wei Chang, Xiaodong He,
and Jianfeng Gao. 2015. Semantic parsing via
staged query graph generation: Question answering
with knowledge base. In Proceedings of the Joint
Conference of the 53rd Annual Meeting of the ACL
and the 7th International Joint Conference on Natu-
ral Language Processing of the AFNLP.

Wen-tau Yih, Matthew Richardson, Christopher Meek,
Ming-Wei Chang, and Jina Suh. 2016. The value
of semantic parse labeling for knowledge base ques-
tion answering. In Proceedings of the 54th Annual
Meeting of the Association for Computational Lin-
guistics.

Pengcheng Yin and Graham Neubig. 2017. A syntactic
neural model for general-purpose code generation.
In The 55th Annual Meeting of the Association for
Computational Linguistics (ACL).

John M. Zelle and Raymond J. Mooney. 1996. Learn-
ing to parse database queries using inductive logic
programming. In Proceedings of the Thirteenth Na-
tional Conference on Artificial Intelligence.

1525



Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: struc-
tured classification with probabilistic categorial
grammars. In UAI ’05, Proceedings of the 21st Con-
ference in Uncertainty in Artificial Intelligence.

Luke S Zettlemoyer and Michael Collins. 2007. On-
line learning of relaxed CCG grammars for parsing
to logical form. In EMNLP-CoNLL.

Kai Zhao and Liang Huang. 2015. Type-driven in-
cremental semantic parsing with polymorphism. In
HLT-NAACL.

1526


