



















































Do Neural Dialog Systems Use the Conversation History Effectively? An Empirical Study


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 32–37
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

32

Do Neural Dialog Systems Use the Conversation History Effectively?
An Empirical Study

Chinnadhurai Sankar1,2,4∗ Sandeep Subramanian1,2,5

Christopher Pal1,3,5 Sarath Chandar1,2,4 Yoshua Bengio 1,2

1Mila 2Université de Montréal 3École Polytechnique de Montréal
4Google Research, Brain Team 5Element AI, Montréal

Abstract

Neural generative models have been become
increasingly popular when building conversa-
tional agents. They offer flexibility, can be eas-
ily adapted to new domains, and require min-
imal domain engineering. A common criti-
cism of these systems is that they seldom un-
derstand or use the available dialog history ef-
fectively. In this paper, we take an empiri-
cal approach to understanding how these mod-
els use the available dialog history by study-
ing the sensitivity of the models to artificially
introduced unnatural changes or perturbations
to their context at test time. We experiment
with 10 different types of perturbations on 4
multi-turn dialog datasets and find that com-
monly used neural dialog architectures like re-
current and transformer-based seq2seq models
are rarely sensitive to most perturbations such
as missing or reordering utterances, shuffling
words, etc. Also, by open-sourcing our code,
we believe that it will serve as a useful diag-
nostic tool for evaluating dialog systems in the
future 1.

1 Introduction

With recent advancements in generative models of
text (Wu et al., 2016; Vaswani et al., 2017; Rad-
ford et al., 2018), neural approaches to building
chit-chat and goal-oriented conversational agents
(Sordoni et al., 2015; Vinyals and Le, 2015; Ser-
ban et al., 2016; Bordes and Weston, 2016; Serban
et al., 2017b) has gained popularity with the hope
that advancements in tasks like machine transla-
tion (Bahdanau et al., 2015), abstractive summa-
rization (See et al., 2017) should translate to dialog
systems as well. While these models have demon-
strated the ability to generate fluent responses,

∗Corresponding author: chinnadhurai@gmail.com
1Code is available at https://github.com/

chinnadhurai/ParlAI/

they still lack the ability to “understand” and pro-
cess the dialog history to produce coherent and
interesting responses. They often produce bor-
ing and repetitive responses like “Thank you.” (Li
et al., 2015; Serban et al., 2017a) or meander away
from the topic of conversation. This has been often
attributed to the manner and extent to which these
models use the dialog history when generating re-
sponses. However, there has been little empirical
investigation to validate these speculations.

In this work, we take a step in that direction and
confirm some of these speculations, showing that
models do not make use of a lot of the informa-
tion available to it, by subjecting the dialog his-
tory to a variety of synthetic perturbations. We
then empirically observe how recurrent (Sutskever
et al., 2014) and transformer-based (Vaswani et al.,
2017) sequence-to-sequence (seq2seq) models re-
spond to these changes. The central premise of
this work is that models make minimal use of cer-
tain types of information if they are insensitive to
perturbations that destroy them. Worryingly, we
find that 1) both recurrent and transformer-based
seq2seq models are insensitive to most kinds of
perturbations considered in this work 2) both are
particularly insensitive even to extreme pertur-
bations such as randomly shuffling or reversing
words within every utterance in the conversation
history (see Table 1) and 3) recurrent models are
more sensitive to the ordering of utterances within
the dialog history, suggesting that they could be
modeling conversation dynamics better than trans-
formers.

2 Related Work

Since this work aims at investigating and gain-
ing an understanding of the kinds of information
a generative neural response model learns to use,
the most relevant pieces of work are where sim-

https://github.com/chinnadhurai/ParlAI/
https://github.com/chinnadhurai/ParlAI/


33

No Perturbations Token shuffling
1 Good afternoon ! Can I help you ? I afternoon help you Good ? ! Can
2 Could you show me where the Chinesc-style clothing is

located ? I want to buy a silk coat
the located Chinesc-style where is show a . buy you ? I
clothing want coat silk me Could to

3 This way , please . Here they are . They’re all handmade . are handmade . way please This all Here they . , They’re .
4 Model Response: How much is it ? Model Response: How much is it ?

Table 1: An example of an LSTM seq2seq model with attention’s insensitivity to shuffling of words in the dialog
history on the DailyDialog dataset.

ilar analyses have been carried out to understand
the behavior of neural models in other settings.
An investigation into how LSTM based uncondi-
tional language models use available context was
carried out by Khandelwal et al. (2018). They
empirically demonstrate that models are sensitive
to perturbations only in the nearby context and
typically use only about 150 words of context.
On the other hand, in conditional language mod-
eling tasks like machine translation, models are
adversely affected by both synthetic and natural
noise introduced anywhere in the input (Belinkov
and Bisk, 2017). Understanding what information
is learned or contained in the representations of
neural networks has also been studied by “prob-
ing” them with linear or deep models (Adi et al.,
2016; Subramanian et al., 2018; Conneau et al.,
2018).

Several works have recently pointed out the
presence of annotation artifacts in common text
and multi-modal benchmarks. For example, Guru-
rangan et al. (2018) demonstrate that hypothesis-
only baselines for natural language inference ob-
tain results significantly better than random guess-
ing. Kaushik and Lipton (2018) report that reading
comprehension systems can often ignore the entire
question or use only the last sentence of a doc-
ument to answer questions. Anand et al. (2018)
show that an agent that does not navigate or even
see the world around it can answer questions about
it as well as one that does. These pieces of work
suggest that while neural methods have the poten-
tial to learn the task specified, its design could lead
them to do so in a manner that doesn’t use all of
the available information within the task.

Recent work has also investigated the induc-
tive biases that different sequence models learn.
For example, Tran et al. (2018) find that recurrent
models are better at modeling hierarchical struc-
ture while Tang et al. (2018) find that feedfor-
ward architectures like the transformer and con-
volutional models are not better than RNNs at
modeling long-distance agreement. Transformers

however excel at word-sense disambiguation. We
analyze whether the choice of architecture and the
use of an attention mechanism affect the way in
which dialog systems use information available to
them.

3 Experimental Setup

Following the recent line of work on generative
dialog systems, we treat the problem of generat-
ing an appropriate response given a conversation
history as a conditional language modeling prob-
lem. Specifically we want to learn a conditional
probability distribution Pθ(y|x) where y is a rea-
sonable response given the conversation history x.
The conversation history is typically represented
as a sequence of utterances x1,x2, . . .xn, where
each utterance xi itself is comprised of a sequence
of words xi1 , xi2 . . . xik . The response y is a single
utterance also comprised of a sequence of words
y1, y2 . . . ym. The overall conditional probability
is factorized autoregressively as

Pθ(y|x) =
n∏
i=1

Pθ(yi|y<i,x1 . . .xn)

Pθ, in this work, is parameterized by a recurrent
or transformer-based seq2seq model. The crux of
this work is to study how the learned probability
distribution behaves as we artificially perturb the
conversation history x1, . . .xn. We measure be-
havior by looking at how much the per-token per-
plexity increases under these changes. For exam-
ple, one could think of shuffling the order in which
x1 . . .xn is presented to the model and observe
how much the perplexity of y under the model in-
creases. If the increase is only minimal, we can
conclude that the ordering of x1 . . .xn isn’t infor-
mative to the model. For a complete list of per-
turbations considered in this work, please refer to
Section 3.2. All models are trained without any
perturbations and sensitivity is studied only at test
time.



34

Figure 1: The increase in perplexity for different models when only presented with the k most recent utterances
from the dialog history for Dailydialog (left) and bAbI dialog (right) datasets. Recurrent models with attention
fare better than transformers, since they use more of the conversation history.

3.1 Datasets
We experiment with four multi-turn dialog
datasets.

bAbI dialog is a synthetic goal-oriented multi-
turn dataset (Bordes and Weston, 2016) consisting
of 5 different tasks for restaurant booking with in-
creasing levels of complexity. We consider Task 5
in our experiments since it is the hardest and is a
union of all four tasks. It contains 1k dialogs with
an average of 13 user utterances per dialog.

Persona Chat is an open domain dataset (Zhang
et al., 2018) with multi-turn chit-chat conversa-
tions between turkers who are each assigned a
“persona” at random. It comprises of 10.9k di-
alogs with an average of 14.8 turns per dialog.

Dailydialog is an open domain dataset (Li et al.,
2017) which consists of dialogs that resemble day-
to-day conversations across multiple topics. It
comprises of 13k dialogs with an average of 7.9
turns per dialog.

MutualFriends is a multi-turn goal-oriented
dataset (He et al., 2017) where two agents must
discover which friend of theirs is mutual based on
the friends’ attributes. It contains 11k dialogs with
an average of 11.41 utterances per dialog.

3.2 Types of Perturbations
We experimented with several types of perturba-
tion operations at the utterance and word (token)
levels. All perturbations are applied in isolation.

Utterance-level perturbations We consider the
following operations 1) Shuf that shuffles the se-
quence of utterances in the dialog history, 2) Rev
that reverses the order of utterances in the history

(but maintains word order within each utterance)
3) Drop that completely drops certain utterances
and 4) Truncate that truncates the dialog history
to contain only the k most recent utterances where
k ≤ n, where n is the length of dialog history.

Word-level perturbations We consider similar
operations but at the word level within every ut-
terance 1) word-shuffle that randomly shuffles the
words within an utterance 2) reverse that reverses
the ordering of words, 3) word-drop that drops
30% of the words uniformly 4) noun-drop that
drops all nouns, 5) verb-drop that drops all verbs.

3.3 Models

We experimented with two different classes
of models - recurrent and transformer-based
sequence-to-sequence generative models. All data
loading, model implementations and evaluations
were done using the ParlAI framework. We used
the default hyper-parameters for all the models as
specified in ParlAI.

Recurrent Models We trained a seq2seq
(seq2seq lstm) model where the encoder and
decoder are parameterized as LSTMs (Hochreiter
and Schmidhuber, 1997). We also experiment
with using decoders that use an attention mecha-
nism (seq2seq lstm att) (Bahdanau et al., 2015).
The encoder and decoder LSTMs have 2 layers
with 128 dimensional hidden states with a dropout
rate of 0.1.

Transformer Our transformer (Vaswani et al.,
2017) model uses 300 dimensional embeddings
and hidden states, 2 layers and 2 attention heads
with no dropout. This model is significantly
smaller than the ones typically used in machine



35

Models Test PPL Only
Last

Shuf Rev Drop
First

Drop
Last

Word
Drop

Verb
Drop

Noun
Drop

Word
Shuf

Word
Rev

Utterance level perturbations ( ∆ PPL[σ] ) Word level perturbations ( ∆ PPL[σ] )
DailyDialog

seq2seq lstm 32.90[1.40] 1.70[0.41] 3.35[0.38] 4.04[0.28] 0.13[0.04] 5.08[0.79] 1.58[0.15] 0.87[0.08] 1.06[0.28] 3.37[0.33] 3.10[0.45]
seq2seq lstm att 29.65[1.10] 4.76[0.39] 2.54[0.24] 3.31[0.49] 0.32[0.03] 4.84[0.42] 2.03[0.25] 1.37[0.29] 2.22[0.22] 2.82[0.31] 3.29[0.25]
transformer 28.73[1.30] 3.28[1.37] 0.82[0.40] 1.25[0.62] 0.27[0.19] 2.43[0.83] 1.20[0.69] 0.63[0.17] 2.60[0.98] 0.15[0.08] 0.26[0.18]

Persona Chat
seq2seq lstm 43.24[0.99] 3.27[0.13] 6.29[0.48] 13.11[1.22] 0.47[0.21] 6.10[0.46] 1.81[0.25] 0.68[0.19] 0.75[0.15] 1.29[0.17] 1.95[0.20]
seq2seq lstm att 42.90[1.76] 4.44[0.81] 6.70[0.67] 11.61[0.75] 2.99[2.24] 5.58[0.45] 2.47[0.67] 1.11[0.27] 1.20[0.23] 2.03[0.46] 2.39[0.31]
transformer 40.78[0.31] 1.90[0.08] 1.22[0.22] 1.41[0.54] −0.1[0.07] 1.59[0.39] 0.54[0.08] 0.40[0.00] 0.32[0.18] 0.01[0.01] 0.00[0.06]

MutualFriends
seq2seq lstm 14.17[0.29] 1.44[0.86] 1.42[0.25] 1.24[0.34] 0.00[0.00] 0.76[0.10] 0.28[0.11] 0.00[0.03] 0.61[0.39] 0.31[0.25] 0.56[0.39]
seq2seq lstm att 10.60[0.21] 32.13[4.08] 1.24[0.19] 1.06[0.24] 0.08[0.03] 1.35[0.15] 1.56[0.20] 0.15[0.07] 3.28[0.38] 2.35[0.22] 4.59[0.46]
transformer 10.63[0.03] 20.11[0.67] 1.06[0.16] 1.62[0.44] 0.12[0.03] 0.81[0.09] 0.75[0.05] 0.16[0.02] 1.50[0.12] 0.07[0.01] 0.13[0.04]

bAbi dailog: Task5
seq2seq lstm 1.28[0.02] 1.31[0.50] 43.61[15.9] 40.99[9.38] 0.00[0.00] 4.28[1.90] 0.38[0.11] 0.01[0.00] 0.10[0.06] 0.09[0.02] 0.42[0.38]
seq2seq lstm att 1.06[0.02] 9.14[1.28] 41.21[8.03] 34.32[10.7] 0.00[0.00] 6.75[1.86] 0.64[0.07] 0.03[0.03] 0.22[0.04] 0.25[0.01] 1.10[0.80]
transformer 1.07[0.00] 4.06[0.33] 0.38[0.02] 0.62[0.02] 0.00[0.00] 0.21[0.02] 0.36[0.02] 0.25[0.06] 0.37[0.06] 0.00[0.00] 0.00[0.00]

Table 2: Model performance across multiple datasets and sensitivity to different perturbations. Columns 1 & 2
report the test set perplexity (without perturbations) of different models. Columns 3-12 report the increase in
perplexity when models are subjected to different perturbations. The mean (µ) and standard deviation [σ] across
5 runs are reported. The Only Last column presents models with only the last utterance from the dialog history.
The model that exhibits the highest sensitivity (higher the better) to a particular perturbation on a dataset is in bold.
seq2seq lstm att are the most sensitive models 24/40 times, while transformers are the least with 6/40 times.

translation since we found that the model that re-
sembled Vaswani et al. (2017) significantly overfit
on all our datasets.

While the models considered in this work might
not be state-of-the-art on the datasets considered,
we believe these models are still competitive and
used commonly enough at least as baselines, that
the community will benefit by understanding their
behavior. In this paper, we use early stopping with
a patience of 10 on the validation set to save our
best model. All models achieve close to the per-
plexity numbers reported for generative seq2seq
models in their respective papers.

4 Results & Discussion

Our results are presented in Table 2 and Figure 1.
Table 2 reports the perplexities of different mod-
els on test set in the second column, followed by
the increase in perplexity when the dialog history
is perturbed using the method specified in the col-
umn header. Rows correspond to models trained
on different datasets. Figure 1 presents the change
in perplexity for models when presented only with
the k most recent utterances from the dialog his-
tory.

We make the following observations:

1. Models tend to show only tiny changes in
perplexity in most cases, even under extreme
changes to the dialog history, suggesting that
they use far from all the information that is
available to them.

2. Transformers are insensitive to word-
reordering, indicating that they could be
learning bag-of-words like representations.

3. The use of an attention mechanism in
seq2seq lstm att and transformers makes
these models use more information from ear-
lier parts of the conversation than vanilla
seq2seq models as seen from increases in per-
plexity when using only the last utterance.

4. While transformers converge faster and to
lower test perplexities, they don’t seem to
capture the conversational dynamics across
utterances in the dialog history and are less
sensitive to perturbations that scramble this
structure than recurrent models.

5 Conclusion

This work studies the behaviour of generative neu-
ral dialog systems in the presence of synthetically
introduced perturbations to the dialog history, that
it conditions on. We find that both recurrent and
transformer-based seq2seq models are not signifi-
cantly affected even by drastic and unnatural mod-
ifications to the dialog history. We also find sub-
tle differences between the way in which recurrent
and transformer-based models use available con-
text. By open-sourcing our code, we believe this
paradigm of studying model behavior by intro-
ducing perturbations that destroys different kinds
of structure present within the dialog history can



36

be a useful diagnostic tool. We also foresee this
paradigm being useful when building new dialog
datasets to understand the kinds of information
models use to solve them.

Acknowledgements

We would like to acknowledge NVIDIA for do-
nating GPUs and a DGX-1 computer used in this
work. We would also like to thank the anonymous
reviewers for their constructive feedback. Our
code is available at https://github.com/
chinnadhurai/ParlAI/.

References
Yossi Adi, Einat Kermany, Yonatan Belinkov, Ofer

Lavi, and Yoav Goldberg. 2016. Fine-grained anal-
ysis of sentence embeddings using auxiliary predic-
tion tasks. arXiv preprint arXiv:1608.04207.

Ankesh Anand, Eugene Belilovsky, Kyle Kastner,
Hugo Larochelle, and Aaron Courville. 2018.
Blindfold baselines for embodied qa. arXiv preprint
arXiv:1811.05013.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In Proceedings
Of The International Conference on Representation
Learning (ICLR 2015).

Yonatan Belinkov and Yonatan Bisk. 2017. Synthetic
and natural noise both break neural machine transla-
tion. arXiv preprint arXiv:1711.02173.

Antoine Bordes and Jason Weston. 2016. Learn-
ing end-to-end goal-oriented dialog. CoRR,
abs/1605.07683.

Alexis Conneau, German Kruszewski, Guillaume
Lample, Loı̈c Barrault, and Marco Baroni. 2018.
What you can cram into a single vector: Probing
sentence embeddings for linguistic properties. arXiv
preprint arXiv:1805.01070.

Suchin Gururangan, Swabha Swayamdipta, Omer
Levy, Roy Schwartz, Samuel R Bowman, and
Noah A Smith. 2018. Annotation artifacts in
natural language inference data. arXiv preprint
arXiv:1803.02324.

H. He, A. Balakrishnan, M. Eric, and P. Liang.
2017. Learning Symmetric Collaborative Dialogue
Agents with Dynamic Knowledge Graph Embed-
dings. arXiv e-prints.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural Computation,
9(8):1735–1780.

Divyansh Kaushik and Zachary C Lipton. 2018. How
much reading does reading comprehension require?
a critical investigation of popular benchmarks.
arXiv preprint arXiv:1808.04926.

Urvashi Khandelwal, He He, Peng Qi, and Dan Ju-
rafsky. 2018. Sharp nearby, fuzzy far away: How
neural language models use context. arXiv preprint
arXiv:1805.04623.

J. Li, M. Galley, C. Brockett, J. Gao, and B. Dolan.
2015. A Diversity-Promoting Objective Function
for Neural Conversation Models. ArXiv e-prints.

Yanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang
Cao, and Shuzi Niu. 2017. Dailydialog: A manually
labelled multi-turn dialogue dataset. arXiv preprint
arXiv:1710.03957.

Alec Radford, Karthik Narasimhan, Tim Salimans, and
Ilya Sutskever. 2018. Improving language under-
standing by generative pre-training. URL https://s3-
us-west-2. amazonaws. com/openai-assets/research-
covers/languageunsupervised/language under-
standing paper. pdf.

Abigail See, Peter J Liu, and Christopher D Man-
ning. 2017. Get to the point: Summarization
with pointer-generator networks. arXiv preprint
arXiv:1704.04368.

I. V. Serban, A. Sordoni, R. Lowe, L. Charlin,
J. Pineau, A. Courville, and Y. Bengio. 2017a. A
hierarchical latent variable encoder-decoder model
for generating dialogues. In Thirty-First AAAI Con-
ference (AAAI).

Iulian V Serban, Chinnadhurai Sankar, Mathieu Ger-
main, Saizheng Zhang, Zhouhan Lin, Sandeep Sub-
ramanian, Taesup Kim, Michael Pieper, Sarath
Chandar, Nan Rosemary Ke, et al. 2017b. A
deep reinforcement learning chatbot. arXiv preprint
arXiv:1709.02349.

Iulian Vlad Serban, Alessandro Sordoni, Yoshua Ben-
gio, Aaron C. Courville, and Joelle Pineau. 2016.
Building end-to-end dialogue systems using gener-
ative hierarchical neural network models. In Pro-
ceedings of AAAI.

Alessandro Sordoni, Michel Galley, Michael Auli,
Chris Brockett, Yangfeng Ji, Margaret Mitchell,
Jian-Yun Nie, Jianfeng Gao, and Bill Dolan. 2015.
A neural network approach to context-sensitive gen-
eration of conversational responses. arXiv preprint
arXiv:1506.06714.

Sandeep Subramanian, Adam Trischler, Yoshua Ben-
gio, and Christopher J Pal. 2018. Learning gen-
eral purpose distributed sentence representations via
large scale multi-task learning. arXiv preprint
arXiv:1804.00079.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in neural information process-
ing systems, pages 3104–3112.

https://github.com/chinnadhurai/ParlAI/
https://github.com/chinnadhurai/ParlAI/
http://arxiv.org/abs/1605.07683
http://arxiv.org/abs/1605.07683
http://arxiv.org/abs/1704.07130
http://arxiv.org/abs/1704.07130
http://arxiv.org/abs/1704.07130
http://arxiv.org/abs/1510.03055
http://arxiv.org/abs/1510.03055


37

Gongbo Tang, Mathias Müller, Annette Rios, and Rico
Sennrich. 2018. Why self-attention? a targeted eval-
uation of neural machine translation architectures.
arXiv preprint arXiv:1808.08946.

Ke Tran, Arianna Bisazza, and Christof Monz.
2018. The importance of being recurrent for
modeling hierarchical structure. arXiv preprint
arXiv:1803.03585.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems, pages 5998–6008.

Oriol Vinyals and Quoc Le. 2015. A neural conversa-
tional model. arXiv preprint arXiv:1506.05869.

Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V
Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus
Macherey, et al. 2016. Google’s neural ma-
chine translation system: Bridging the gap between
human and machine translation. arXiv preprint
arXiv:1609.08144.

Saizheng Zhang, Emily Dinan, Jack Urbanek, Arthur
Szlam, Douwe Kiela, and Jason Weston. 2018. Per-
sonalizing dialogue agents: I have a dog, do you
have pets too? arXiv preprint arXiv:1801.07243.


