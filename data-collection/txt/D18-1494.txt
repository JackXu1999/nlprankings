



















































Siamese Network-Based Supervised Topic Modeling


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4652–4662
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

4652

Siamese Network-Based Supervised Topic Modeling
Minghui Huang1,∗, Yanghui Rao1,†, Yuwei Liu1, Haoran Xie2, Fu Lee Wang3

1School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China
2Department of Mathematics and Information Technology,

The Education University of Hong Kong, Tai Po, Hong Kong
3School of Science and Technology,

The Open University of Hong Kong, Ho Man Tin, Kowloon, Hong Kong
huangmh25@mail2.sysu.edu.cn, raoyangh@mail.sysu.edu.cn,

liuyw23@mail2.sysu.edu.cn, hrxie2@gmail.com,
pwang@ouhk.edu.hk

Abstract

Label-specific topics can be widely used for
supporting personality psychology, aspect-
level sentiment analysis, and cross-domain
sentiment classification. To generate label-
specific topics, several supervised topic mod-
els which adopt likelihood-driven objective
functions have been proposed. However, it is
hard for them to get a precise estimation on
both topic discovery and supervised learning.
In this study, we propose a supervised topic
model based on the Siamese network, which
can trade off label-specific word distributions
with document-specific label distributions in
a uniform framework. Experiments on real-
world datasets validate that our model per-
forms competitive in topic discovery quantita-
tively and qualitatively. Furthermore, the pro-
posed model can effectively predict categor-
ical or real-valued labels for new documents
by generating word embeddings from a label-
specific topical space.

1 Introduction

As one of the most widely used text mining tech-
niques, topic modeling can extract meaningful de-
scriptions (i.e., topics) from a corpus (Blei, 2012).
Most previous topic models, such as probabilis-
tic Latent Semantic Analysis (pLSA) (Hofmann,
1999) and Latent Dirichlet Allocation (LDA) (Blei
et al., 2001) are unsupervised. In unsupervised
topic models, each document is defined as a mix-
ture distribution over topics and each topic is
represented as a mixture distribution over words.
Unsupervised topic models only exploit words
in documents and do not incorporate the guid-
ance of labels into learning processes. There-
fore, these models fail to discover label-specific
topics, which are important to support personality

∗ This work was finished when the first author was an
undergraduate student of her final year.

†The corresponding author.

psychology (Weiner and Graham, 1990), aspect-
level sentiment analysis (Liu, 2012), and cross-
domain sentiment classification (He et al., 2011).
For example, label-specific topics generated from
sentimental texts can help to find attributions and
causes for different sentiments by associating sen-
timents with real-world topics/events.

In light of this consideration, several super-
vised topic models are proposed to generate label-
specific topics. One of the most representative
models is the supervised Latent Dirichlet Alloca-
tion (sLDA) (Blei and McAuliffe, 2007), which re-
stricts a document being associated with one real-
valued response variable. To deal with categori-
cal labels, multi-class sLDA (sLDAc) (Wang et al.,
2009) and Labeled Latent Dirichlet Allocation (L-
LDA) (Ramage et al., 2009) are proposed, but they
are only applicable to classification. Recently,
a supervised Neural Topic Model (sNTM) (Cao
et al., 2015) is developed to tackle supervised tasks
of both classification and regression. As a hybrid
method, sNTM is in essence a neural network by
following the document-topic distribution in topic
models. Unfortunately, the label information has a
little effect on topic generation since sNTM mod-
els documents and labels separately.

The above limitation motivates us to develop a
supervised topic model which can jointly model
documents and labels. Particularly, we propose a
Siamese Labeled Topic Model (SLTM) to exploit
the information of documents and labels based
on the Siamese network (Bromley et al., 1993;
Hu et al., 2014; Wang and Zhang, 2017), where
weight matrices in SLTM represent conditional
distributions. Therefore, by constraining weight
matrices during the learning procedure, SLTM can
follow probabilistic characteristics of topic models
strictly. Compared to previous supervised topic
models, the main advantages of our SLTM are
summarized as follows. First, SLTM can gener-



4653

ate more coherent label-specific topics than oth-
ers. This is because the supervision of labels is
incorporated into topic modeling for SLTM. On
the other hand, the mapping of topics to labels is
unconstrained for most existing supervised topic
models, which renders many coherent topics be-
ing generated outside labels. Second, strengths
of neural networks are incorporated into SLTM to
bootstrap its inference power on label prediction.
Third, each word can be mapped to a topical em-
bedding space and represented by a word embed-
ding after generating label-specific topics.

To validate the effectiveness of the proposed
model, we evaluate it on two real-world datasets
in text mining. Experimental results indicate that
our method is able to discover more coherent and
label-specific topics than baseline models. More-
over, word embeddings learned by the proposed
model can be used to predict labels for new docu-
ments effectively.

The remainder of this paper is organized as fol-
lows. We summarize related studies on supervised
topic modeling in Section 2. For convenience of
describing our model, we present the neural net-
work view of topic models in Section 3. Then, we
detail the proposed SLTM in Section 4. Experi-
mental design and analysis of results are shown in
Section 5. Finally, we present conclusions and fu-
ture work in Section 6.

2 Related Work

Topic models, which focus on discovering unob-
served class variables named “topics” statistically,
have been widely used in text mining. One of
the early topic models is pLSA (Hofmann, 1999).
In pLSA, a document’s word vector was decom-
posed into a mixture of topics, and a topic was rep-
resented as a probability distribution over words.
LDA (Blei et al., 2001) extended pLSA by adding
Dirichlet priors for a document’s multinomial dis-
tribution over topics and a topic’s multinomial dis-
tribution over words, which makes it suitable to
generate topics for unseen documents.

The aforementioned models are unsupervised,
which may be computationally costly to do some
task-specific transformation when there is extra
labeling information (Cao et al., 2015). To ad-
dress this issue, several supervised topic models
have been proposed to introduce the label guid-
ance in learning processes. One of the most widely
used supervised topic models is sLDA (Blei and

McAuliffe, 2007). In sLDA, each document was
paired with a response variable which obeys the
Gaussian distribution. By extending the sLDA,
BP-sLDA (Chen et al., 2015) applied back propa-
gation over a deep architecture in conjunction with
stochastic gradient/mirror descent for model pa-
rameter estimation, leading to scalable and end-to-
end discriminative learning characteristics. Based
on sLDA, multi-class sLDA (sLDAc) (Wang et al.,
2009) was proposed to model documents with
categorical labels by adding a softmax classifier
rather than a linear regression in sLDA to a stan-
dard LDA. Another method of tackling corpora
with discrete labels is L-LDA (Ramage et al.,
2009), which associated each label with only one
topic. To improve the performance of L-LDA
in the classification task, Dependency-LDA (Dep-
LDA) (Rubin et al., 2012) incorporated an extra
topic model to capture the dependencies between
labels and took the label dependencies into con-
sideration when estimating topic distributions. Re-
cently, a nonparametric supervised topic model (Li
et al., 2018) was proposed to predict the response
of interest (e.g., product ratings and sales). The
limitation of above models is that they are only
applicable to either discrete or continuous data.

In this paper, we propose a Siamese network-
based supervised topic model named SLTM. The
most relevant work to SLTM is the supervised
Neural Topic Model (sNTM) for both classifica-
tion and regression tasks (Cao et al., 2015), which
constructed two hidden layers to generate the n-
gram topic and document-topic representations.
However, different from our SLTM using bag-
of-words methods, sNTM adopted fixed embed-
dings trained on external resources (Mikolov et al.,
2013). Thus, sNTM can not learn data-specific
topics. Furthermore, sNTM is hard to follow prob-
abilistic characteristics of the topic-word distribu-
tion in topic models, because a topic generated by
sNTM is composed of an infinite number of n-
grams. Finally, sNTM modeled documents and la-
bels separately rather than uniformly in our SLTM.

3 Preliminaries

For convenience of describing the proposed
model, we use hollow uppercase letters (e.g., D) to
represent collections, bold uppercase letters (e.g.,
W1) to represent matrices, bold lowercase let-
ters (e.g., yi) to represent vectors, regular upper-
case letters (e.g., M ) to represent scalar constants,



4654

Table 1: Frequently used notations.

Notation Description

M Number of documents
K Number of topics
N Size of the vocabulary
L Size of the label set
D Document collection

di ∈ D The i-th document
V Vocabulary

vj ∈ V The j-th word
Z Topic collection

zk ∈ Z The k-th topic
Y Label collection

yi ∈ RL Labels for document di
p(vj |di) The probability of vj given di

and regular lowercase letters (e.g., vj) to represent
scalar variables. Based on the above convention,
frequently used notations are shown in Table 1.
Given a document di with labels yi, our goal is
to discover topics with a neural network frame-
work. Therefore, we first describe the neural net-
work view of topic models briefly.

Topic modeling is a popular latent variable in-
ference method for co-occurrence data which as-
sociates unobserved classes with observations di
and vj , where vj is a word in di. The conditional
probability p(vj |di) is defined as:

p(vj |di) =
K∑
k=1

p(vj |zk)p(zk|di). (1)

Let φ(vj) = [p(vj |z1), . . . , p(vj |zK)] and
θ(di) = [p(z1|di), . . . , p(zK |di)], then p(vj |di)
in Equation 1 can be represented as the following
vector form:

p(vj |di) = φ(vj) · θ(di). (2)

We represent horizontal stack by commas
and vertical stack by semicolons, thus W1 =
[θ(d1)

T , . . . , θ(dM )
T ] ∈ RK×M and W2 =

[φ(v1); . . . ;φ(vN ] ∈ RN×K , which are con-
strained by: W1[k,m] ≥ 0, W2[n, k] ≥ 0,∑K

k=1W1[k,m] = 1, and
∑N

j=1W2[j, k] = 1,
where k ∈ [1,K], m ∈ [1,M ], and n ∈ [1, N ].
Then, the vector form in Equation 2 can be ex-

𝐲𝑖

𝑝(Z|𝑑𝑖)𝑑𝑖
𝐖1 𝐖2

𝑝(V|𝑑𝑖)

𝐖3

-

𝑝(Z|𝑑
𝑖

(𝑣𝑗−))𝑑
𝑖

(𝑣𝑗−)
𝐖1 𝐖2

𝑝(V|𝑑
𝑖

(𝑣𝑗−))

𝐲
𝑖

(𝑣𝑗−)

𝐖3

𝑝(𝑣𝑗|𝑑𝑖 )

 𝑝(𝑣𝑗|𝑑𝒊 )

 𝑝(𝑣𝑗|𝑑𝑖
(𝑣𝑗−))

𝐯𝑗

𝐯𝑗

Figure 1: SLTM’s word generation framework.

Document 

set

Predicted label

Predicted conditional 

probability

Input layer

.

.

.

Topic distribution

Hidden layer Output layer

Figure 2: SLTM’s architecture from the perspective of
neural networks.

tended to:

p(V|D) =

 p(v1|d1) · · · p(v1|dM )... . . . ...
p(vN |d1) · · · p(vN |dM )


=

 (φ(v1) · θ(d1)) · · · (φ(v1) · θ(dM ))... . . . ...
(φ(vN ) · θ(d1)) · · · (φ(vN ) · θ(dM ))


= W2W1. (3)

With Equation 3, topic models can be viewed
as neural networks, where D and V are input sets,
p(V|D) is the output set, and W1 and W2 are pa-
rameter matrices of the neural network.

4 Siamese Labeled Topic Model

Similar to generative models such as pLSA, we
propose a Siamese Labeled Topic Model (SLTM)
based on the aforementioned neural network per-
spective of topic models. Figure 1 illustrates the
framework of generating each word in SLTM, and
the process is as follows. For a document di in D,
the topic distribution p(Z|di) is estimated by:

p(Z|di) = W1di, (4)



4655

where di is the indicator vector (Yang et al., 2013)
of di, which means that the i-th entry of di is 1 and
other entries are 0. Labels of di are yi, which are
generated from the topic distribution of di as: yi =
W3p(Z|di). The above equation is constrained by
W3[l, k] ≥ 0, where l ∈ [1, L], k ∈ [1,K], and∑L

l=1W3[l, k] = 1 if L > 1. A topic zk in Z has
its word distribution p(V|zk), which is computed
by:

p(V|zk) = W2zk, (5)

where zk is the indicator vector of zk. Therefore,
words can be generated from di as:

p(V|di) = W2W1di. (6)

The architecture of SLTM from the perspec-
tive of neural networks is shown in Figure 2.
With respect to the model optimization, we adopt
the contrastive objective function used in previ-
ous works (Socher et al., 2014; Cui et al., 2014;
Cao et al., 2015; He et al., 2017). For document
di and every word vj in di, we randomly sam-
ple a document from the document set D which
does not contain vj , as a negative sample doc-
ument. The negative sample document is repre-
sented as d(vj−)i and has labels y

(vj−)
i . As shown

in Figure 1, the lower sub-network, which takes
d
(vj−)
i as input, has the same architecture as the

the upper sub-network, which takes di as input.
Because the document-topic distribution and the
topic-word distribution of a corpus are fixed, W1,
W2 and W3 are shared among two sub-networks
of our model. These two sub-networks are twin
networks and thus the proposed model is essen-
tially the Siamese network. Our objective is to
make word vj be learned by topics in document
di, while not be learned by topics in the nega-
tive sampled document d(vj−)i . Therefore, we only
take word vj in V into consideration during the
learning procedure, which can be implemented as
dot-multiplying p(V|di) with the indicator vector
of vj (i.e., vj) as: p̂(vj |di) = p(V|di) · vj . Par-
ticularly, the objective is to make the predicted
conditional probability p̂(vj |di) approach the ob-
served conditional probability p(vj |di) (i.e., term
frequency of word vj in document di), while make
the conditional probability p̂(vj |d

(vj−)
i ) approach

zero. Thus, the loss function of predicted condi-
tional probabilities and the observed conditional

Algorithm 1 Training Algorithm for SLTM
Input: S = {D,Y};

1: repeat
2: for all (di,yi) ∈ S do
3: for each word vj in document di do
4: Sample a document d(vj−)i which

does not contain vj ;
5: Calculate loss(SLTM);
6: if loss(SLTM) is reducing then
7: Update W1, W2 and W3;
8: end if
9: end for

10: end for
11: until convergence

probability can be defined as:

loss(di, d
(vj−)
i )

= |p(vj |di)− p̂(vj |di) + p̂(vj |d
(vj−)
i )|. (7)

We use another loss function loss(yi,y
(vj−)
i )

to capture labels of di and d
(vj−)
i , where

loss(yi,y
(vj−)
i ) = loss(yi)+loss(y

(vj−)
i ). In the

above, equations of loss(yi) and loss(y
(vj−)
i ) de-

pend on the property of labels. For categorical and
real-valued labels, the cross-entropy (Tang et al.,
2014) and the mean absolute error (Willmott and
Matsuura, 2005) are adopted, respectively.

The maximization of the weighted sum of con-
ditional likelihoods is equivalent to minimize the
losses of the weighted sum of loss functions, and
these two loss functions are weighted by a hyper-
parameter α as in (Tang et al., 2014). Thus, the
loss function of SLTM is:

loss(SLTM) = α× loss(di, d
(vj−)
i )

+ (1− α)× loss(yi,y
(vj−)
i ). (8)

The effect of α on predicting labels and discov-
ering topics will be investigated in Section 5.6.
Based on loss(SLTM), three kinds of weights,
i.e., W1, W2, and W3 can be updated together by
a vanilla back propagation (BP) algorithm with the
early stopping criteria (Bengio, 2012). The train-
ing algorithm is shown in Algorithm 1.

After training, we obtain both document-topic
and topic-word distributions. Then, each word can
be mapped to a topic-level embedding space and
represented as a word embedding. For instance,



4656

the word embedding of vj is generated from the
topic-word distribution W2 as:

e(vj) = W2[j, :]. (9)

The generated word embeddings can be used for
specific applications, such as label prediction. Par-
ticularly, we firstly represent a new document dn
by its document embeddings e(dn), where e(dn)
is the sum of word embeddings of all words in dn.
Then, the predicted labels ŷn of document dn can
be estimated by:

ŷn = f(W4e(dn)), (10)

where W4 denotes weights of each topic con-
tributing to labels, and f(.) is the activation func-
tion which depends on the type of labels. For cat-
egorical and normalized real-valued labels, we re-
spectively adopt softmax and sigmoid as activa-
tion functions. Note that we do not predict labels
for new documents based on W3 directly, because
topic distributions of these documents can only
be learned without the supervision of labels, i.e.,
new documents’ topic distributions may be incon-
sistent to W3. Finally, we update W4 and word
embeddings by RMSprop (Tieleman and Hinton,
2012) for label prediction.

5 Experiments

In this section, we firstly describe datasets and the
setting of experiments. Secondly, we investigate
the quality of generated topics by the topic coher-
ence score and qualitative analysis. Thirdly, the
quality of generated word embeddings is evaluated
by label prediction and word similarity. Finally,
the effect of the hyper-parameter α is evaluated on
coherence of topics and label prediction.

5.1 Datasets and Setting

To evaluate the effectiveness of our method com-
prehensively, we conduct experiments on two real-
world datasets with categorical and real-valued la-
bels, respectively. The first corpus named ISEAR
contains a collection of 7,666 sentences and each
item is manually tagged with a categorical label
over 7 emotions (Scherer and Wallbott, 1994). The
second dataset YouTube1 is often used for sen-
timent strength detection, which contains 3,407
comments on videos and each item is labeled

1http://sentistrength.wlv.ac.uk/

with a real value between 0.1 (i.e., very nega-
tive sentiment) and 0.9 (i.e., very positive senti-
ment). These two datasets are selected for their
similar word numbers in average. After remov-
ing stop words, the mean numbers of words in
each document are 8.53 and 8.56 for ISEAR and
YouTube. Besides, it is appropriate to evaluate the
model performance on predicting emotions and
sentiment strengths, because topics play an impor-
tant role in understanding sentences or user com-
ments (Liu, 2012). Since the proposed SLTM
is suitable to both topic discovery and classifi-
cation/regression tasks, we employ five kinds of
baselines for comparison.

The first kind are the support vector machine
(SVM), an efficient deep learning model for clas-
sification (i.e., fastText) (Grave et al., 2017), and
the following supervised topic models which are
confined to categorical labels:

• sLDAc (Wang et al., 2009): it models doc-
uments with categorical labels by adding a
softmax classifier to a standard LDA.

• L-LDA (Ramage et al., 2009): it is a super-
vised model which associates labels with top-
ics by one-to-one correspondence. Accord-
ingly, the number of topics in L-LDA must
equal the size of the label set.

• Dep-LDA (Rubin et al., 2012): it extends L-
LDA by introducing a multinomial distribu-
tion over labels and capturing the dependen-
cies between labels. Then, the label depen-
dencies are used to sample topic distributions
in supervised learning.

The second kind are the support vector re-
gression (SVR), a state-of-the-art deep learn-
ing model for sentiment strength detection (i.e.,
HCNN) (Chen et al., 2017), and the following
supervised topic models which are developed for
predicting real-valued labels only:

• sLDA (Blei and McAuliffe, 2007): it is a
classical supervised topic model, in which,
each document is paired with a response vari-
able, and the variable is defined as a Gaussian
distribution with a mean value that is com-
puted by a linear regression of topics.

• BP-sLDA (Chen et al., 2015): it applies back
propagation over a deep architecture together
with stochastic gradient/mirror descent for



4657

Table 2: Topic coherence scores on ISEAR using dif-
ferent numbers of top words T .

5 10 15

pLSA 0.0051 0.0024 -0.0013
LDA 0.0954 0.0492 0.0014

sLDAc 0.0014 0.0031 -0.0035
sNTM -0.9267 -0.9508 -0.9667
SLTM 0.1142 0.0680 0.0025

Table 3: Topic coherence scores on YouTube using dif-
ferent numbers of top words T .

5 10 15

pLSA -0.0535 -0.2435 -0.3829
LDA -0.0154 -0.2142 -0.3618
sLDA -0.0627 -0.2502 -0.3962

BP-sLDA -0.7021 -0.7670 -0.7900
sNTM -0.9138 -0.9253 -0.9376
SLTM -0.0993 -0.1967 -0.3268

parameter estimation of sLDA. The number
of hidden layers is set to 3.

The third kind is a supervised n-gram model
named sNTM, which is applicable to predict both
categorical and real-valued labels for new docu-
ments (Cao et al., 2015). In sNTM, each n-gram is
represented by a 300-dimensional embedding vec-
tor using the available tool word2vec2. By follow-
ing (Cao et al., 2015), a large-scale Google News
dataset with around 100 billion words is adopted
for training. For topic discovery, two unsuper-
vised topic models, pLSA (Hofmann, 1999) and
LDA (Blei et al., 2001), are used as the fourth kind
of baselines. Finally, we adopt two hybrid meth-
ods by combining LDA and supervised learning
algorithms as baselines. In particular, a softmax
classifier and a liner regression (LR) are used to
predict categorical and real-valued labels for doc-
uments, respectively. Unless otherwise specified,
we set α to 0.5 and adopt the stochastic gradient
descent with batch size of 100 for training SLTM.

5.2 Coherence Score of Topics
To investigate the quality of topics discovered by
SLTM quantitatively, we use the topic coherence
score based on the normalised pointwise mutual
information (Lau et al., 2014) as the evaluation
metric. Intuitively, a topic coherence score that

2https://code.google.com/p/word2vec/

Table 4: Each label’s top 5 words on ISEAR.
Labels Models Top 5 words of label-specific topics

fear
sLDAc home night car afraid fear

L-LDA/Dep-LDA night afraid car home fear felt
SLTM night afraid fear car home dark

joy
sLDAc year passed heard exam university

L-LDA/Dep-LDA friend got time passed felt
SLTM happy joy passed got university

guilt
sLDAc did didn’t asked guilty said

L-LDA/Dep-LDA felt guilty friend did mother
SLTM guilty felt mother did friend

disgust
sLDAc saw man disgusted disgust woman

L-LDA/Dep-LDA disgusted saw felt people friend
SLTM disgusted saw people man disgust

shame
sLDAc know ashamed teacher happened lot

L-LDA/Dep-LDA ashamed felt friend time did
SLTM ashamed felt shame class teacher

anger
sLDAc angry called new anger expected

L-LDA/Dep-LDA friend angry did time told
SLTM angry friend anger brother told

sadness
sLDAc father close died away years

L-LDA/Dep-LDA died friend sad felt time
SLTM died sad death away friend

is larger indicates that the quality of topics is bet-
ter. All unsupervised topic models (i.e., pLSA and
LDA) and supervised methods which associate
one label with multiple topics (i.e., sLDAc, sLDA,
BP-sLDA, and sNTM) are adopted for compari-
son. Although L-LDA and Dep-LDA can iden-
tify label-specific topics on ISEAR, these models’
one-to-one mapping of labels and topics makes
them unsuitable in this evaluation. Particularly, L-
LDA and Dep-LDA constraint each topic to words
in certain documents with the same label, which
renders their coherence scores being estimated by
a subset of the corpus only. On the other hand, the
quality of topics is evaluated on the whole corpus
for SLTM and other baseline models.

The average coherence scores of topics gener-
ated by different models on ISEAR and YouTube
are respectively shown in Table 2 and Table 3,
where the number of topics is 20, the number of
top words T is set to 5, 10, and 15, and the best
scores are highlighted in boldface. The results in-
dicate that SLTM can discover more coherent top-
ics than both unsupervised topic models and su-
pervised methods, except for T = 5 on YouTube.
It is also interesting to observe that supervised
baseline models (i.e., sLDAc, sLDA, BP-sLDA,
and sNTM) perform worse than pLSA and LDA
for most cases, which validates that it is challeng-
ing to trade off label-specific word distributions
with document-specific label distributions (Ram-
age et al., 2009).

5.3 Qualitative Analysis on Topics

In this part, we conduct qualitative inspection of
20 topics generated by SLTM. The ISEAR dataset



4658

0.00 0.25 0.50 0.75 1.00
0.0

0.2

0.4

0.6

0.8

1.0

joy
fear

anger
shame

sadness
disgust

guilt

Figure 3: Scatter plot of topics identified by SLTM on
ISEAR, where each point indicates a topic.

which contains multiple labels is used for illus-
tration, since it is inappropriate to present the re-
sults on YouTube with a single real-valued label.
For each model that is applicable to ISEAR, we
show top 5 words of the generated label-specific
topics in Table 4. It is worth to note that L-LDA
and Dep-LDA achieve the same top words, since
their difference only exists in the process of label
prediction. The results indicate that although all
models can learn meaningful topics, SLTM per-
forms better than baseline models in label-specific
topic discovery. For example, two words “happy”
and “joy” which are strongly related to the label of
“joy” are identified by SLTM with large probabil-
ities. Similar results can be observed in other la-
bels, thus topics discovered by our model are more
convenient to be understood than others. Such a
kind of performance enhancement is valuable to
many real-world applications, e.g., personality ed-
ucation and psychotherapy, by producing human
interpretable topics/events that evoke users’ par-
ticular emotions.

For completeness, we also examine all topics
generated by the baseline of sNTM. As mentioned
earlier, sNTM is based on n-grams, instead of
single words for SLTM and other baseline mod-
els. In the practical implementation, only uni-
grams and bigrams are considered since the em-
bedding representation becomes less precise as n
increases (Cao et al., 2015). The results indicate
that sNTM can generate some topic bigrams such
as “smelled disgusting” and “graduation exams”,
which are more appropriate to expressing a topic.
However, only three topics are manually examined

Table 5: Classification performance on ISEAR.

Accuracy Cohen’s kappa

SVM 0.5063 0.4240
fastText 0.5104 0.4298

LDA+softmax 0.1506 0.0089
sLDAc 0.1875 0.0540
L-LDA 0.4650 0.3758

Dep-LDA 0.4888 0.4036
sNTM 0.2478 0.1212
SLTM 0.5213 0.4415

to be correlated with the seven emotions. This val-
idates that sNTM is hard to introduce the guidance
of labels in topic generation, because it models
documents and labels separately.

To further evaluate the interpretability of topics
extracted from SLTM, we firstly get topic embed-
dings by: emb(zk) = W1[k, :]. Then, we map
emb(zk) to a two-dimensional space via Princi-
pal Component Analysis (PCA). Figure 3 presents
distributions of topics generated by SLTM over the
ISEAR dataset. The scatter plot indicates that top-
ics corresponding to the same label are closer than
those of different labels. Furthermore, the distance
between topics on correlated labels such as “fear”
and “anger”, is closer than that of topics on “joy”
and other labels.

5.4 Evaluation on Label Prediction
We here evaluate the quality of word embeddings
generated by SLTM on predicting categorical and
real-valued labels based on ISEAR and YouTube,
respectively. Since there are varied parameters for
different models, we randomly select 60% of in-
stances as the training set, 20% as the validation
set, and the remaining 20% as the testing set. The
values of parameters (e.g., the number of topics)
for each model are all determined by the valida-
tion set. In label prediction, the main difference
between SLTM and other supervised topic mod-
els is as follows. On one hand, a label-specific
word embedding is introduced for predicting la-
bels in SLTM according to Equation 10. On the
other hand, other supervised topic models for both
categorical and real-valued label prediction tasks
infer labels for unlabeled documents by topic dis-
tributions directly, in which, topic distributions of
unlabeled documents are learned without the su-
pervision of labels.

For the task of categorical label prediction, the



4659

Table 6: Regression performance on YouTube.

MAE pR2

SVR 0.1424 -0.0591
HCNN 0.1112 0.3462

LDA+LR 0.1408 -0.0069
sLDA 0.1583 -0.2836

BP-sLDA 0.1394 -0.0208
sNTM 0.1342 0.0807
SLTM 0.1005 0.4112

accuracy and the Cohen’s kappa score (Artstein
and Poesio, 2008) are used as the evaluation met-
rics. Table 5 shows the classification performance
of different models on ISEAR, where the best re-
sults are highlighted in boldface. For the predic-
tion of real-valued labels on YouTube, we com-
pare different models’ regression performance by
the mean absolute error (MAE) and the predictive
R2 (pR2) (Blei and McAuliffe, 2007), as shown in
Table 6. From the above results we can observe
that SLTM achieves substantial performance im-
provement over baselines in predicting both cate-
gorical and real-valued labels, which indicates that
word embeddings generated from labeled docu-
ments are more suitable for label prediction tasks
than topic distributions generated from unlabeled
documents without the guidance of labels.

5.5 Similarity of Word Embeddings

Word embeddings can reflect relations between
words, and most methods of generating word em-
beddings are based on the local context informa-
tion. This is because words with similar contexts
may have similar semantics. However, a large-
scale corpus is required to learn high quality word
embeddings from the local context. Different from
the previous word embedding generation meth-
ods, SLTM generates word embeddings based on
the global label-specific topic information (i.e.,
the topical embedding space). Therefore, we
further compare the quality of word embeddings
learned by SLTM and three widely used meth-
ods: Word2Vec (W2V) (Mikolov et al., 2013),
subword information Word2Vec (siW2V) (Joulin
et al., 2017), and SSPMI (Levy and Goldberg,
2014). Among these baseline word embedding
models, W2V and siW2V use the neural network
framework, and SSPMI implicitly factorizes the
pointwise mutual information (PMI) matrix of the
local word co-occurrence patterns.

Table 7: Word similarity results on ISEAR.

MEN SimLex Rare

W2V 0.002 -0.008 -0.119
siW2V 0.002 0.017 0.062
SSPMI 0.023 0.028 -0.004
SLTM 0.169 0.037 0.089

Table 8: Word similarity results on YouTube.

MEN SimLex Rare

W2V -0.018 0.004 -0.036
siW2V -0.002 0.019 -0.051
SSPMI -0.031 0.038 -0.026
SLTM 0.048 0.040 0.068

As our evaluation metric, the word similar-
ity is estimated as follows. Firstly, we calcu-
late cosine similarity scores for word pairs which
occur in both the training set and the testing
set. Secondly, word pairs are ranked accord-
ing to their cosine similarities in the embedding
space and human-assigned similarity scores, re-
spectively. Finally, rankings of word similarity
scores are evaluated by measuring the Spearman’s
rank correlation with rankings of human-assigned
similarity scores. A higher correlation value in-
dicates that it is more consistent to human judge-
ments in word similarity. The following standard
corpora which contain word pairs associated with
human-assigned similarity scores are used for this
evaluation: MEN (Bruni et al., 2014), SimLex-
999 (SimLex) (Hill et al., 2015), and Rare (Luong
et al., 2013).

We train W2V, siW2V, and SSPMI over each
corpus by setting the number of context window
size to 5. Furthermore, the dimension of word
embeddings generated from all models is set to
50 according to (Lai et al., 2016). The values of
word similarity on ISEAR and YouTube are re-
spectively shown in Table 7 and Table 8, where the
best results are highlighted in boldface. We can
observe that SLTM outperforms baselines for all
cases. The results indicate that word embeddings
learned from the global label-specific topic infor-
mation are better than those from the local context
information without any external corpora.



4660

0.2 0.4 0.6 0.8 1.0

0.10

0.05

0.00

0.05

0.10

T=5 T=10 T=15

Figure 4: Topic coherence scores on ISEAR using dif-
ferent α values.

5.6 Effect of the Hyper-parameter

After validating the effectiveness of SLTM on
discovering topics and learning word embed-
dings, we now investigate the effect of the hyper-
parameter in SLTM on these two aspects. Accord-
ing to Equation 8, the hyper-parameter α is used
to weight two kinds of loss functions. Since W2
can be updated subject to α > 0, we evaluate the
performance of SLTM by varying α from 0.1 to 1
over the ISEAR dataset, as follows.

First, we evaluate the influence of hyper-
parameter α on topic discovery by the coherence
score of topics. To clearly illustrate the perfor-
mance trend with different values of α, we set
the number of top words T to 5, 10, and 15, and
present topic coherence scores in Figure 4. The
results indicate that SLTM performs stably un-
der these α values on topic discovery, except for
α = 1 which ignores the label information totally.
This validates the importance of label information
in generating coherent topics.

Second, we use the learned word embeddings
to predict document labels under different val-
ues of α. As shown in Figure 5, we can ob-
serve that when α = 0.5, i.e., loss(di, d

(vj−)
i )

and loss(yi,y
(vj−)
i ) are weighted equally, SLTM

achieves the best performance in label prediction.
The results indicate that the co-occurrence of doc-
uments and words as well as the label information
are both important to generate good word embed-
dings. Furthermore, the label prediction perfor-
mance of SLTM using any of these α values is
better than that of most baselines (ref. Table 5).
This validates the robustness of SLTM with differ-
ent hyper-parameter values in supervised learning.

0.2 0.4 0.6 0.8 1.0

0.44

0.46

0.48

0.50

0.52

Accuracy Cohen s kappa

Figure 5: Label prediction performance on ISEAR us-
ing different α values.

We also conduct experiments on YouTube using
varied α values, which indicates that the hyper-
parameter has a similar effect on both datasets.

6 Conclusion

In this paper, we proposed a supervised topic
model named SLTM to discover label-specific
topics by jointly modeling documents and la-
bels. For the SLTM, weight matrices which repre-
sent document-topic and topic-word distributions
can strictly follow probabilistic characteristics of
topic models. Experiments were conducted on
datasets with both categorical and real-valued la-
bels, which validated that SLTM can not only dis-
cover more coherent topics, but also boost the per-
formance of supervised learning tasks by learning
high quality word embeddings. For future work,
we plan to speed-up the training process of SLTM
by GPUs and distributed algorithms. With the de-
velopment of deep learning techniques, we also
plan to de-emphasize irrelevant words with an at-
tention mechanism.

Acknowledgments

We are grateful to the anonymous reviewers for
their valuable comments on this manuscript. The
research has been supported by the National Nat-
ural Science Foundation of China (61502545,
U1711262, U1611264), a grant from the Research
Grants Council of the Hong Kong Special Admin-
istrative Region, China (UGC/FDS11/E03/16),
and the Innovation and Technology Fund (Project
No. GHP/022/17GD) from the Innovation and
Technology Commission of the Government of the
Hong Kong Special Administrative Region.



4661

References
Ron Artstein and Massimo Poesio. 2008. Inter-coder

agreement for computational linguistics. Computa-
tional Linguistics, 34(4):555–596.

Yoshua Bengio. 2012. Practical recommendations for
gradient-based training of deep architectures. In
Neural Networks: Tricks of the Trade - Second Edi-
tion, pages 437–478. Springer-Verlag.

David M. Blei. 2012. Probabilistic topic models. Com-
munications of the ACM, 55(4):77–84.

David M. Blei and Jon D. McAuliffe. 2007. Super-
vised topic models. In Proceedings of the Annual
Conference on Neural Information Processing Sys-
tems, pages 121–128.

David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2001. Latent dirichlet allocation. In Proceedings of
the Annual Conference on Neural Information Pro-
cessing Systems, pages 601–608.

Jane Bromley, Isabelle Guyon, Yann LeCun, Eduard
Säckinger, and Roopak Shah. 1993. Signature veri-
fication using a siamese time delay neural network.
In Proceedings of the Annual Conference on Neural
Information Processing Systems, pages 737–744.

Elia Bruni, Nam-Khanh Tran, and Marco Baroni. 2014.
Multimodal distributional semantics. Artificial In-
telligence Research, 49:1–47.

Ziqiang Cao, Sujian Li, Yang Liu, Wenjie Li, and Heng
Ji. 2015. A novel neural topic model and its super-
vised extension. In Proceedings of the 29th AAAI
Conference on Artificial Intelligence, pages 2210–
2216.

Huijun Chen, Xin Li, Yanghui Rao, Haoran Xie,
Fu Lee Wang, and Tak-Lam Wong. 2017. Senti-
ment strength prediction using auxiliary features. In
Proceedings of the 26th International Conference on
World Wide Web Companion, pages 5–14.

Jianshu Chen, Ji He, Yelong Shen, Lin Xiao, Xiaodong
He, Jianfeng Gao, Xinying Song, and Li Deng.
2015. End-to-end learning of LDA by mirror-
descent back propagation over a deep architecture.
In Proceedings of the Annual Conference on Neural
Information Processing Systems, pages 1765–1773.

Lei Cui, Dongdong Zhang, Shujie Liu, Qiming Chen,
Mu Li, Ming Zhou, and Muyun Yang. 2014. Learn-
ing topic representation for SMT with neural net-
works. In Proceedings of the 52nd Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 133–143.

Edouard Grave, Tomas Mikolov, Armand Joulin, and
Piotr Bojanowski. 2017. Bag of tricks for efficient
text classification. In Proceedings of the 15th Con-
ference of the European Chapter of the Association
for Computational Linguistics, pages 427–431.

Ruidan He, Wee Sun Lee, Hwee Tou Ng, and Daniel
Dahlmeier. 2017. An unsupervised neural attention
model for aspect extraction. In Proceedings of the
55th Annual Meeting of the Association for Compu-
tational Linguistics, pages 388–397.

Yulan He, Chenghua Lin, and Harith Alani. 2011.
Automatically extracting polarity-bearing topics for
cross-domain sentiment classification. In Proceed-
ings of the 49th Annual Meeting of the Association
for Computational Linguistics, pages 123–131.

Felix Hill, Roi Reichart, and Anna Korhonen. 2015.
Simlex-999: Evaluating semantic models with (gen-
uine) similarity estimation. Computational Linguis-
tics, 41(4):665–695.

Thomas Hofmann. 1999. Probabilistic latent seman-
tic analysis. In Proceedings of the 15th Conference
on Uncertainty in Artificial Intelligence, pages 289–
296.

Baotian Hu, Zhengdong Lu, Hang Li, and Qingcai
Chen. 2014. Convolutional neural network archi-
tectures for matching natural language sentences. In
Proceedings of the Annual Conference on Neural In-
formation Processing Systems, pages 2042–2050.

Armand Joulin, Edouard Grave, Piotr Bojanowski, and
Tomas Mikolov. 2017. Bag of tricks for efficient
text classification. In Proceedings of the 15th Con-
ference of the European Chapter of the Association
for Computational Linguistics, pages 427–431.

Siwei Lai, Kang Liu, Shizhu He, and Jun Zhao. 2016.
How to generate a good word embedding. IEEE In-
telligent Systems, 31(6):5–14.

Jey Han Lau, David Newman, and Timothy Baldwin.
2014. Machine reading tea leaves: Automatically
evaluating topic coherence and topic model quality.
In Proceedings of the 14th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics, pages 530–539.

Omer Levy and Yoav Goldberg. 2014. Neural word
embedding as implicit matrix factorization. In Pro-
ceedings of the Annual Conference on Neural Infor-
mation Processing Systems, pages 2177–2185.

Weifeng Li, Junming Yin, and Hsinchun Chen. 2018.
Supervised topic modeling using hierarchical dirich-
let process-based inverse regression: Experiments
on e-commerce applications. IEEE Transactions
on Knowledge and Data Engineering, 30(6):1192–
1205.

Bing Liu. 2012. Sentiment Analysis and Opinion Min-
ing. Synthesis Lectures on Human Language Tech-
nologies. Morgan & Claypool Publishers.

Thang Luong, Richard Socher, and Christopher D.
Manning. 2013. Better word representations with
recursive neural networks for morphology. In Pro-
ceedings of the 17th Conference on Computational
Natural Language Learning, pages 104–113.



4662

Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.
Corrado, and Jeffrey Dean. 2013. Distributed repre-
sentations of words and phrases and their composi-
tionality. In Proceedings of the Annual Conference
on Neural Information Processing Systems, pages
3111–3119.

Daniel Ramage, David Leo Wright Hall, Ramesh Nal-
lapati, and Christopher D. Manning. 2009. Labeled
LDA: A supervised topic model for credit attribu-
tion in multi-labeled corpora. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing, pages 248–256.

Timothy N. Rubin, America Chambers, Padhraic
Smyth, and Mark Steyvers. 2012. Statistical topic
models for multi-label document classification. Ma-
chine Learning, 88(1-2):157–208.

Klaus R. Scherer and Harald G. Wallbott. 1994. Evi-
dence for universality and cultural variation of dif-
ferential emotion response patterning. Personality
and Social Psychology, 66:310–328.

Richard Socher, Andrej Karpathy, Quoc V. Le, Christo-
pher D. Manning, and Andrew Y. Ng. 2014.
Grounded compositional semantics for finding and
describing images with sentences. Transactions
of the Association for Computational Linguistics,
2:207–218.

Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, Ting
Liu, and Bing Qin. 2014. Learning sentiment-
specific word embedding for twitter sentiment clas-
sification. In Proceedings of the 52nd Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 1555–1565.

Tijmen Tieleman and Geoffrey Hinton. 2012. Lecture
6.5-rmsprop: Divide the gradient by a running av-
erage of its recent magnitude. COURSERA: Neural
Networks for Machine Learning, 4(2):26–31.

Chong Wang, David M. Blei, and Fei-Fei Li. 2009. Si-
multaneous image classification and annotation. In
Proceedings of the 2009 IEEE Conference on Com-
puter Vision and Pattern Recognition, pages 1903–
1910.

Zhongqing Wang and Yue Zhang. 2017. A neural
model for joint event detection and summarization.
In Proceedings of the 26th International Joint Con-
ference on Artificial Intelligence, pages 4158–4164.

Bernard Weiner and Sandra Graham. 1990. Attribution
in personality psychology. Handbook of personal-
ity: Theory and research, pages 465–485.

Cort J Willmott and Kenji Matsuura. 2005. Advantages
of the mean absolute error (mae) over the root mean
square error (rmse) in assessing average model per-
formance. Climate Research, 30(1):79–82.

Chuan Yang, Lihe Zhang, Huchuan Lu, Xiang Ruan,
and Ming-Hsuan Yang. 2013. Saliency detection
via graph-based manifold ranking. In Proceedings

of the 2013 IEEE Conference on Computer Vision
and Pattern Recognition, pages 3166–3173.


