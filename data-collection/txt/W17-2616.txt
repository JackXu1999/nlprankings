



















































Representing Compositionality based on Multiple Timescales Gated Recurrent Neural Networks with Adaptive Temporal Hierarchy for Character-Level Language Models


Proceedings of the 2nd Workshop on Representation Learning for NLP, pages 131–138,
Vancouver, Canada, August 3, 2017. c©2017 Association for Computational Linguistics

Representing Compositionality based on Multiple Timescales Gated
Recurrent Neural Networks with Adaptive Temporal Hierarchy for

Character-Level Language Models

Moirangthem Dennis Singh, Jegyung Son, Minho Lee
School of Electronics Engineering
Kyungpook National University

Daegu, South Korea
{mdennissingh,wprud4,mholee}@gmail.com

Abstract

A novel character-level neural language
model is proposed in this paper. The pro-
posed model incorporates a biologically
inspired temporal hierarchy in the archi-
tecture for representing multiple composi-
tions of language in order to handle longer
sequences for the character-level language
model. The temporal hierarchy is intro-
duced in the language model by utilizing
a Gated Recurrent Neural Network with
multiple timescales. The proposed model
incorporates a timescale adaptation mech-
anism for enhancing the performance of
the language model. We evaluate our pro-
posed model using the popular Penn Tree-
bank and Text8 corpora. The experiments
show that the use of multiple timescales
in a Neural Language Model (NLM) en-
ables improved performance despite hav-
ing fewer parameters and with no addi-
tional computation requirements. Our ex-
periments also demonstrate the ability of
the adaptive temporal hierarchies to rep-
resent multiple compositonality without
the help of complex hierarchical architec-
tures and shows that better representation
of the longer sequences lead to enhanced
performance of the probabilistic language
model.

1 Introduction

Language Modeling is a fundamental task central
to Natural Language Processing (NLP) and lan-
guage understanding. A character-level language
model (CLM) can be interpreted as a probability
estimation method for the next character given a
sequence of characters as input. From the per-
spective of sequence generation, predicting one

character at a time has higher importance since
it allows the network to invent novel words and
strings. CLMs are commonly used for modeling
new words and there have been successful tech-
niques that use generative language models (LMs)
based on characters or phonemes (Sutskever et al.,
2011).

Recurrent neural networks have been applied to
CLMs (Sutskever et al., 2011; Graves, 2013). Re-
cently Kim et al. (2016b) introduced a LM with
explicit hierarchical architecture to work at char-
acter levels and word levels. Cooijmans et al.
(2017) introduced recurrent batch normalization
into CLMs which significantly improved the per-
formance. However, since the population statis-
tics are estimated separately for each time step, the
model is computationally intensive particularly for
a CLM where the number of steps are more than
conventional word level LMs. Similarly, Krueger
and Memisevic (2016) introduced regularization
in CLMs using a norm-stabilizer and reported an
increased training time for higher levels of regu-
larization.

In spite of the recent successes, CLMs still have
inferior performance compared to its equivalent
word-level models (Mikolov et al., 2012) since
these LMs need to consider longer history of to-
kens to properly predict the next one. In order
to improve the performance of the CLMs, there
is a need for better representation of the additional
levels of compositionality and the richer discourse
structure found in CLMs.

Heinrich et al. (2012) used multiple timescale
RNNs to learn the linguistic hierarchy for speech
related tasks and Ding et al. (2016) demonstrated
that, during listening to connected speech, cor-
tical activity of different timescales concurrently
tracked the time course of abstract linguistic com-
positionality at different hierarchical levels, such
as words, phrases and sentences. In this work,

131



we propose a character-level recurrent neural net-
work (RNN) LM that employs an adaptive mul-
tiple timescales approach to incorporate tempo-
ral hierarchies in the architecture to enhance the
representation of multiple compositionalities. Our
proposed model includes a novel timescale update
mechanism which enhances the adaptation of the
temporal hierarchy during the learning process.
We build the temporal hierarchical structure us-
ing fast and slow context units to imitate different
timescales. This temporal hierarchy concept is im-
plemented based on the multiple timescales gated
recurrent unit (MTGRU) (Kim et al., 2016a) that
incorporates multiple timescales at different layers
of the RNN.

Our model, inspired by the concept of tempo-
ral hierarchy found in the human brain (Botvinick,
2007; Meunier et al., 2010), demonstrates the abil-
ity to capture multiple compositionalities similar
to the findings of Ding et al. (2016). This bet-
ter representation learning capability enhances the
ability of our model to handle longer sequences
for the CLM. The resulting LM is a much sim-
pler model that does not incorporate explicit hi-
erarchical structures or normalization techniques.
We show that our CLM with the biologically in-
spired temporal hierarchy is able to achieve per-
formance comparable to the existing state-of-the-
art CLMs evaluated over the Penn Treebank (PTB)
and Text8 corpora.

2 Related Works

Recent advances in distributed representation
learning have demonstrated promising results in
language modeling. Distributed representation
learning approaches are a group of methods in
which real-valued vectors are trained to capture
the underlying meaning in the input. Bengio
et al. (2003) demonstrated that the probabilistic
language models can achieve much better gener-
alization.

Out-of-vocabulary words have always been a
problem in language tasks. To address the rare
word problem in language generation, Alexan-
drescu and Kirchhoff (2006) represented a word
as a set of shared factor embeddings. In an-
other approach, Sutskever et al. (2011) introduced
NLMs that incorporated a character-level model
with both input and output as characters. CLMs
are also capable of generating novel words and are
suitable for addressing the rare word problem.

Training a CLM has been a difficult task and
its performance has been lower than the word-
level LMs. Handling longer sequences is criti-
cal to improve the performance of CLMs and it
remains a challenge. Mikolov et al. (2012) pro-
posed an alternative approach, where subword-
level models are trained to benefit from the word-
level LMs. Pachitariu and Sahani (2013) proposed
impulse-response LMs for improving RNN LMs.
Recently, several studies on RNN based CLMs
have been proposed, mostly using the Long short-
term memory (LSTM) (Graves, 2013; Cooijmans
et al., 2017; Zhang et al., 2016), where numer-
ous regularization and normalization techniques
have been applied to enhance the performance of
CLMs. Additionally, weight generation networks
(Ha et al., 2017), hierarchical architectures (Chung
et al., 2017) in conjunction with the normalization
techniques have been proposed to achieve state-of-
the-art performance.

We propose a different approach for our CLM
by using a simpler biologically inspired tem-
poral hierarchy model. Recently, Ding et al.
(2016) demonstrated strong evidence for a neu-
ral tracking of hierarchical linguistic structures in
the brain. The study performed experiments to
determine whether neural representation of lan-
guage (speech) tracks hierarchical linguistic struc-
tures, rather than prosodic and statistical transi-
tional probability cues. In simpler terms, the brain
tracks and represents linguistic structures hierar-
chically. Similar findings have been shown in Me-
unier et al. (2010) and Botvinick (2007), but Ding
et al. (2016) was the first to confirm the same
in the domain of language. The authors hypoth-
esized that concurrent neural tracking of hierar-
chical linguistic structures provides a mechanism
for temporally integrating smaller linguistic units
into larger structures. Therefore, our knowledge
of the hierarchical nature of linguistic structures
and the theory of linguistic compositionality have
been shown to be biologically plausible. Previ-
ous works have applied this hierarchical structure
to RNNs in movement tracking (Paine and Tani,
2004), sensorimotor control systems (Yamashita
and Tani, 2008) and speech recognition (Heinrich
et al., 2012). Based on the above conclusions, we
adopt the multiple timescales concept to imple-
ment the temporal hierarchy architecture for rep-
resenting multiple compositionalities which will
help in handling longer sequences for our CLM.

132



Moreover, Yamashita and Tani (2008) had
tested their model performance over different val-
ues of the timescale τ to investigate the impact
of multiple timescales in RNNs. They showed
that different setting of the timescales significantly
affects the performance and a higher τ -ratio(τ -
slow/τ -fast) improved the performance. In this
spirit, we implement an adaptive timescale up-
date method for better performance compared to
a model with static timescales.

3 Proposed Character-Level Neural
Language Model

A character-level language model (CLM) esti-
mates a probability distribution over wt+1 given
a sequence w1:t = [w1, . . . , wt]. We propose a re-
current neural network based CLM with temporal
hierarchies using a multilayer gated recurrent neu-
ral network. Gated RNNs such as LSTM (Hochre-
iter and Schmidhuber, 1997) and Gated Recurrent
Unit (GRU) (Cho et al., 2014) address the prob-
lem of learning long range dependencies. GRU
and LSTM have been shown to yield comparable
performance (Chung et al., 2014). These gated
recurrent architectures are known to address the
vanishing gradient problem efficiently and multi-
layer architectures are known to be able to learn
expressive and complex features. However, the
phenomenon responsible for these algorithms to
approach human performance on speech and lan-
guage tasks cannot be ascertained owing to our
lack of understanding or insight into the actual
representations that are being learned. However,
due to our lack of understanding or insight into
the actual representations being learned, it is dif-
ficult to ascertain the phenomenon responsible for
these algorithms to approach human performance
on speech and language tasks. Therefore, concur-
rent to the studies of temporal hierarchy in neu-
roscience (Ding et al., 2016), we formulated a hy-
pothesis that multilayer gated recurrent neural net-
works can represent compositional hierarchies in
the learning process that involves a monotonically
increasing timescale hierarchy. We argue that hier-
archical temporal representations capture the lin-
guistic hierarchy of the input and are primarily
responsible for better performance of multilayer
gated recurrent architectures.

We propose a Multiple Timescales Gated Re-
current Unit (MTGRU) with adaptive timescales
for our language model. The MTGRU, which

x1 x2 x3

y1 y2 y3

Input Chars  h  e  l 

 e  l  l Target Chars

Input Layer

Output Layer

Fast Layer

Slow Layer

)(h 11
)(h 12

)(h 13

)(h 21
)(h 22

)(h 23

x4

y4

 l 

 o 

H
id

d
e
n
 S

ta
te

s

)(h 14

)(h 24

Figure 1: Proposed character-level neural lan-
guage model.

ht-1 ut

ht

th
~

1/τ

z

r

Figure 2: A Multiple Timescales Gated Recurrent
Unit.

has a temporal hierarchical architecture, is imple-
mented in the framework of a language model
as shown in Figure 1. We find that the multiple
timescales model is primarily responsible for ex-
plicitly guiding each layer of the neural network
to facilitate in learning of features operating over
increasingly slower timescales, corresponding to
subsequent levels in the compositional hierarchy.
The temporal hierarchy in the network is imple-
mented by applying a timescale constant at the end
of a conventional GRU unit, essentially adding an-
other constant gating unit that modulates the mix-
ture of past and current hidden states. In an MT-
GRU, each step takes as input xt,ht−1 and pro-
duces the hidden ht. The time constant τ added
to the activation ht of the MTGRU is shown in
Eq. (1). τ is used to control the timescale of each
GRU cell. Larger τ results in slower cell outputs
but it makes the cell focus on the slow features of
a dynamic sequence input. The MTGRU model is
illustrated in Figure 2.

133



rt = σ(Wxrxt +Whrht−1)
zt = σ(Wxzxt +Whzht−1)
ut = tanh(Wxuxt +Whu(rt � ht−1))
h̃t = ztht−1 + (1− zt)ut
ht = h̃t

1
τ

+ (1− 1
τ
)ht−1

(1)

where σ(·) and tanh(·) are the sigmoid and tan-
gent hyperbolic activation functions, � denotes
the element-wise multiplication operator, and rt,
zt are referred to as reset, update gates respec-
tively. ut and h̃t are the candidate activation and
candidate hidden state of the MTGRU.

We build the multilayer MTGRU-CLM with a
different timescale τ for each layer. Based on
our hypothesis that later layers should learn fea-
tures that operate over slower timescales, we set
larger τ as we go up the layers. We use the bits-
per-character (BPC) as the evaluation metric. The
timescale τ is initialized for each layers at the start
of the training.

We implement the proposed timescale update
mechanism by adaptively increasing the τ dur-
ing the training process as it is known that just
higher τ -ratio, without timescale adaptation, leads
to improved performance (Yamashita and Tani,
2008). As we proceed with the training epochs,
whenever the validation negative log-likelihood
(NLL) (shown in Eq.(2)) stopped decreasing, the
timescale τ is updated. A growth factor is used
to determine the growth rate of the timescales.
We update the timescales only after training has
completed for a particular number of epochs
(max epoch). The timescale update mechanism
is presented in Algorithm 1. In order to prevent
deteriorated performance over large increases in
the timescales, smaller growth factors are set for
the experiments.

− 1
N

N∑
n=1

T∑
t=1

logP
(
wnt | wn1 . . . wnt−1; θ

)
(2)

where N is the number of training sequences, T is
the length of the nth sequence, θ is the model pa-
rameter, and wnt is the token at time t of sequence
n and so on.

4 Experiments and Results

We evaluate our CLM on the Penn Treebank
(PTB) corpus (Marcus et al., 1993) and on the

Input: Current Timescale τ
Output: Updated Timescale τ
if current epoch > max epoch then

read growth factor;
if the validation NLL did not decrease
then
τ = τ * growth factor;
return τ ;

else
return τ ;

end
end

Algorithm 1: Timescale adaptation in MTGRU

τ {1.2, 1.3, 1.35, 1.4}
growth factor {1.01, 1.05, 1.1, 1.15}

learning rate {1e-2, 1e-3, 1e-4, 2e-3}
minibatch size {32, 64, 128}

Table 1: Grid of hyperparameters explored in the
experiments.

much larger Text8 corpus (Mahoney, 2009). We
use orthogonal initialization for all the weight ma-
trices and use stochastic gradient descent with gra-
dient clipping at 1.0 and step rule determined by
Adam (Kingma and Ba, 2014). We report the hy-
perparameter values that were explored in our ex-
periments in Table 1. The timescale for the fast
layer is initialized to 1 in all the experiments as
τ = 1 defines the default or the input timescale.

We also conduct an additional experiment for
the comparison of computational efficiency of our
model with normalization based techniques. The
details of all the experiments are described in the
sections below.

4.1 Penn Treebank (PTB) Corpus

The PTB corpus is divided to train, valid, and test
sets following Mikolov and Zweig (2012). For
this experiment we use 600 units in each layer
of the 2 layer MTGRU network. We train on
non-overlapping sequences of length 100 with a
mini-batch size of 64 and a learning rate of 0.002.
We initialize the timescales τ as {1, 1.3} for the
fast and the slow layers respectively. We set the
growth factor to 1.05 with a max epoch of 25.
The size of our model is 3.7M parameters.

We also implemented the batch normalized
gated recurrent unit (BN-GRU) following Cooij-
mans et al. (2017) as a baseline to compare with

134



Model BPC Size
Zhang et al. (2016) 1.49 -

Mikolov et al. (2012) 1.41 -
Krueger and Memisevic (2016) 1.39 4.25M∗

BN-GRU 1.39 4.1M
Mikolov et al. (2012) 1.37 -

Cooijmans et al. (2017) 1.32 4.25M∗

Ha et al. (2017) 1.31 4.25M
MTGRU-CLM 1.27 3.7M
Ha et al. (2017) 1.27 4.91M

Chung et al. (2017) 1.24 5.35M∗

MTGRU-CLM-Adaptive 1.24 3.7M
Ha et al. (2017) 1.22 14.41M

Table 2: Bits-Per-Character on PTB test and size
of the models. MTGRU-CLM and MTGRU-
CLM-Adaptive correspond to our CLMs with a
constant timescale and an adaptive timescales re-
spectively. ∗These are estimated model sizes as
the actual number of parameters is not available in
the literature.

our model. We use early-stopping on validation-
set performance and the resulting model is evalu-
ated over the test set and the results are summa-
rized in Table 2. Our model performed compa-
rable to the current state-of-the-art models with a
test BPC of 1.24. We also illustrate the perfor-
mance of our model in Table 4 under the different
settings of τ and growth factor given in Table 1.

For further analysis, we graph the hidden state
change rate of each layer by measuring the L2 dis-
tance between the past and current hidden states of
the MTGRU-CLM-Adaptive and the GRU CLM,
over the input time steps as shown in Figure 3.
The models are trained on the PTB set with the
same set of parameters. In this graph, “spikes” can
be interpreted as events where the input represen-
tation at each layer is varying significantly. The
first(fast) layer, where the layer of the recurrent
unit is exposed directly to the input (as illustrated
in Fig. 1), the spiking corresponds to each char-
acter of the input. Looking at the second(slow)
layer graph of MTGRU, we can observe the larger
spikes correspond to the end of each word, while
the same is not true in the case of GRU. It is
evident that MTGRU learns better representation
of each word by utilizing the temporal hierarchy
without any explicit architectural hierarchy. These
findings are consistent with the previous studies
on speech where in speech sounds, syllable-level

 

0

0.002

0.004

0.006

0.008

0.01

0.012

0.014

0.016

0.018

r e c u r r e n t n e u r a l n e two r k s a r e v e r y powe r f u l .

H
ID

D
EN

 S
TA

TE
 C

H
A

N
G

E 
R

A
TE

INPUTS

Analysis of GRU Hidden States 

Layer 1 Layer 2

0

0.002

0.004

0.006

0.008

0.01

0.012

0.014

r e c u r r e n t n e u r a l n e t wo r k s a r e v e r y p owe r f u l .

H
ID

D
EN

 S
TA

TE
 C

H
A

N
G

E 
R

A
TE

INPUTS

Analysis of MTGRU Hidden States 

Layer 1 Layer 2

Figure 3: Hidden state representation of GRU-
CLM and MTGRU-CLM-Adaptive.

information on short time scale is integrated into
word-level information over a longer time scale
(Yamashita and Tani, 2008; Ding et al., 2016).

4.2 Text8 Corpus

The Text8 corpus consists of 100M characters ex-
tracted from the Wikipedia corpus. We follow
Mikolov and Zweig (2012) and divide the train,
valid, and test sets accordingly in order to compare
with previous works. Since Text8 contains only al-
phabets and spaces, the total number of symbols is
just 27. We use 1200 units in each layer of the 2
layer MTGRU network and it is trained over non-
overlapping sequences of length 180. The mini-
batch size is set to 128 and the learning rate is
0.001. The timescales τ is initialized as {1, 1.3}
for the fast and the slow layers respectively with
a growth factor of 1.05. The max epoch pa-
rameter is set to be 25. The size of this model is
14.5M parameters. The performance of the result-
ing model with early-stopping on validation-set is
shown in Table 3. We also report the performance
of the BN-GRU baseline model on the Text8 cor-
pus. The MTGRU-CLM-Adaptive model obtains
a test BPC of 1.29 which is comparable to the

135



Model BPC Size
Mikolov et al. (2012) 1.54 -
Zhang et al. (2016) 1.49 -

Pachitariu and Sahani (2013) 1.48 -
BN-GRU 1.39 16.1M

Cooijmans et al. (2017) 1.36 16.22M∗

MTGRU-CLM 1.34 14.5M
Chung et al. (2017) 1.32 21M∗

Chung et al. (2017) 1.29 21M∗

MTGRU-CLM-Adaptive 1.29 14.5M

Table 3: Bits-Per-Character on Text8 test and size
of the Models. MTGRU-CLM and MTGRU-
CLM-Adaptive correspond to our CLMs with a
constant timescale and an adaptive timescales re-
spectively. ∗These are estimated parameter sizes
as the actual value is not available in the literature.

Corpus BPC
PTB Corpus 1.26±0.0158
Text8 Corpus 1.31±0.0187

Table 4: Performance of the proposed model under
different timescale updates. These are the perfor-
mance of the model under different settings of τ
and growth factors as shown in Table 1.

current state-of-the-art. The performance of this
model under different τ and growth factor set-
tings given in Table 1 is shown in Table 4.

4.3 Comparison of Computing Efficiency

In order to compare the computation efficiency
of MTGRU with the baselines, we replicated
the Sequential MNIST experiment from Cooi-
jmans et al. (2017) using our implementation
of LSTM, GRU, Batch Normalized (BN)-LSTM,
BN-GRU, MTGRU, and MTGRU-Adaptive fol-
lowing the same experimental conditions. De-
spite the faster convergence of BN-LSTM, BN-
GRU, MTGRU, and MTGRU-Adaptive as illus-

Model Accuracy Train Time
LSTM 98.89% 14 hours
GRU 98.56% 15 hours

BN-LSTM 99.01% 41 hours
BN-GRU 98.97% 43 hours
MTGRU 99.26% 17 hours

MTGRU-Adaptive 99.37% 17 hours

Table 5: Sequential MNIST classification results
with training duration of 100K steps.

0

0.5

1

1.5

2

2.5

3

0
4

2
0

0
8

4
0

0
1

2
6

0
0

1
6

8
0

0
2

1
0

0
0

2
5

2
0

0
2

9
4

0
0

3
3

6
0

0
3

7
8

0
0

4
2

0
0

0
4

6
2

0
0

5
0

4
0

0
5

4
6

0
0

5
8

8
0

0
6

3
0

0
0

6
7

2
0

0
7

1
4

0
0

7
5

6
0

0
7

9
8

0
0

8
4

0
0

0
8

8
2

0
0

9
2

4
0

0
9

6
6

0
0

C
ro

ss
 E

n
tr

o
p

y 
Lo

ss

Number of Steps

Comparison of Convergence Speed

BN-LSTM

GRU

LSTM

BN-GRU

MTGRU

MTGRU-Adaptive

Figure 4: Comparison of convergence speed of the
various models for Sequential MNIST classifica-
tion task.

trated in Figure 4, the total time taken to train
100K steps of BN-LSTM, BN-GRU significantly
increased compared to LSTM and GRU while the
training time of MTGRU and MTGRU-Adaptive
remained comparable to GRU as shown in Table 5.
The experiments were performed on one machine
with an Nvidia Titan-X GPU. Moreover, Kim et al.
(2016a) already demonstrated much faster conver-
gence in the case of MTGRU when compared to
GRU in sequence-to-sequence tasks.

5 Discussion

The proposed method based on a biologically in-
spired hierarchical structure can represent multiple
compositions of language by virtue of the adap-
tive multiple timescales in each layer. Sensitiv-
ity of the human brain to the compositional struc-
ture of language was recently confirmed by Ding
et al. (2016). By recording the activity of listeners
brains using magnetoencephalography (MEG), it
was found that the brain activates or spikes when
it is presented with individual words, phrases,
or a whole sentence. We successfully replicated
this property in our model and it achieves signif-
icant performance gains despite having a simpler
structure and lesser number of parameters. Our
model’s ability to represent compositions of lan-
guage at the word level is illustrated in Figure 3.
This illustration validates our hypothesis that mul-
tilayer gated recurrent neural networks can repre-
sent compositional hierarchies similar to the hu-
man brain.

136



The enhanced performance of the proposed
CLM illustrates that our approach can give bet-
ter generalization performance without the help
of complex hierarchical architectures. The re-
sults indicate that the temporal hierarchies with
the adaptive timescale approach can represent the
compositonality better and increases the capabil-
ity of the model to handle longer sequences for the
CLM. The results also demonstrate that our mul-
tilayer MTGRU model with adaptive timescale
performs comparable to the current state-of-the-
art models despite having fewer parameters and
a simpler architecture. The temporal hierarchy
approach eliminates the need for complex struc-
tures and normalization techniques (Cooijmans
et al., 2017; Krueger and Memisevic, 2016; Chung
et al., 2017; Ha et al., 2017) for the LM task,
thereby increasing the computational efficiency of
our model.

6 Conclusion

Our approach incorporates temporal hierarchies
in a character-level NLM to improve the perfor-
mance of the language model without introduc-
ing additional parameters. The proposed approach
takes into account the need for a biologically plau-
sible structure and a model to implement sim-
pler hierarchies for handling different level of lan-
guage compositions in order to tackle the longer
sequences problem in CLMs. Our approach with
adaptive timescales enables a simpler model with
a better representation of language to achieve sig-
nificant performance gains over existing models
with larger complexities and also alleviates the
need of additional computations.

Acknowledgment

This research was supported by ICT R&D pro-
gram of MSIP/IITP [R7124-16-0004, Develop-
ment of Intelligent Interaction Technology Based
on Context Awareness and Human Intention Un-
derstanding] (70%) and by the National Re-
search Foundation of Korea (NRF) grant funded
by the Korea government (MSIP) (No. NRF-
2016M3C1B6929647) (30%).

References
Andrei Alexandrescu and Katrin Kirchhoff. 2006. Fac-

tored neural language models. In Proceedings of
the Human Language Technology Conference of the

NAACL, Companion Volume: Short Papers. Associ-
ation for Computational Linguistics, pages 1–4.

Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. journal of machine learning research
3(Feb):1137–1155.

Matthew M Botvinick. 2007. Multilevel structure in
behaviour and in the brain: a model of fuster’s hi-
erarchy. Philosophical Transactions of the Royal
Society B: Biological Sciences 362(1485):1615–26.
https://doi.org/10.1098/rstb.2007.2056.

Kyunghyun Cho, Bart van Merrienboer, Çaglar
Gülçehre, Fethi Bougares, Holger Schwenk, and
Yoshua Bengio. 2014. Learning phrase repre-
sentations using RNN encoder-decoder for statis-
tical machine translation. CoRR abs/1406.1078.
http://arxiv.org/abs/1406.1078.

Junyoung Chung, Sungjin Ahn, and Yoshua Bengio.
2017. Hierarchical multiscale recurrent neural net-
works. In Proceeding of the International Confer-
ence on Learning Representations.

Junyoung Chung, Çaglar Gülçehre, KyungHyun
Cho, and Yoshua Bengio. 2014. Empirical
evaluation of gated recurrent neural networks
on sequence modeling. CoRR abs/1412.3555.
http://arxiv.org/abs/1412.3555.

Tim Cooijmans, Nicolas Ballas, César Laurent, Çağlar
Gülçehre, and Aaron Courville. 2017. Recurrent
batch normalization. In Proceeding of the Interna-
tional Conference on Learning Representations.

Nai Ding, Lucia Melloni, Hang Zhang, Xing Tian, and
David Poeppel. 2016. Cortical tracking of hierarchi-
cal linguistic structures in connected speech. Nature
neuroscience 19(1):158–164.

Alex Graves. 2013. Generating sequences with
recurrent neural networks. arXiv preprint
arXiv:1308.0850 .

David Ha, Andrew Dai, and Quoc V Le. 2017. Hyper-
networks. In Proceeding of the International Con-
ference on Learning Representations.

Stefan Heinrich, Cornelius Weber, and Stefan Wermter.
2012. Adaptive learning of linguistic hierarchy in
a multiple timescale recurrent neural network. In
International Conference on Artificial Neural Net-
works. Springer, pages 555–562.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation
9(8):1735–1780.

Minsoo Kim, Moirangthem Dennis Singh, and Minho
Lee. 2016a. Towards abstraction from extraction:
Multiple timescale gated recurrent unit for summa-
rization. ACL 2016 pages 70–77.

137



Yoon Kim, Yacine Jernite, David Sontag, and Alexan-
der M Rush. 2016b. Character-aware neural lan-
guage models. In Proceedings of the Thirtieth AAAI
Conference on Artificial Intelligence. AAAI Press,
pages 2741–2749.

Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980 .

David Krueger and Roland Memisevic. 2016. Regular-
izing rnns by stabilizing activations. In Proceeding
of the International Conference on Learning Repre-
sentations.

Matt Mahoney. 2009. Large text compression bench-
mark. URL: http://www. mattmahoney. net/text/text.
html .

Mitchell P Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large annotated
corpus of english: The penn treebank. Computa-
tional linguistics 19(2):313–330.

D. Meunier, R. Lambiotte, A. Fornito, K. D. Ersche,
and E. T. Bullmore. 2010. Hierarchical modularity
in human brain functional networks. ArXiv e-prints
.

Tomáš Mikolov, Ilya Sutskever, Anoop Deoras, Hai-
Son Le, Stefan Kombrink, and Jan Cernocky.
2012. Subword language modeling with neu-
ral networks. preprint (http://www. fit. vutbr.
cz/imikolov/rnnlm/char. pdf) .

Tomas Mikolov and Geoffrey Zweig. 2012. Context
dependent recurrent neural network language model.
In SLT . pages 234–239.

Marius Pachitariu and Maneesh Sahani. 2013. Reg-
ularization and nonlinearities for neural language
models: when are they needed? arXiv preprint
arXiv:1301.5650 .

Rainer W. Paine and Jun Tani. 2004. Motor primi-
tive and sequence self-organization in a hierarchi-
cal recurrent neural network. Neural Networks
17(89):1291 – 1309. New Developments in Self-
Organizing Systems.

Ilya Sutskever, James Martens, and Geoffrey E Hin-
ton. 2011. Generating text with recurrent neural
networks. In Proceedings of the 28th International
Conference on Machine Learning (ICML-11). pages
1017–1024.

Yuichi Yamashita and Jun Tani. 2008. Emer-
gence of functional hierarchy in a multiple
timescale neural network model: A humanoid
robot experiment. PLoS Comput Biol 4(11):1–18.
https://doi.org/10.1371/journal.pcbi.1000220.

Saizheng Zhang, Yuhuai Wu, Tong Che, Zhouhan Lin,
Roland Memisevic, Ruslan R Salakhutdinov, and
Yoshua Bengio. 2016. Architectural complexity
measures of recurrent neural networks. In Advances

in Neural Information Processing Systems. pages
1822–1830.

138


