



















































End-to-End Neural Relation Extraction with Global Optimization


Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1730–1740
Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics

End-to-End Neural Relation Extraction with Global Optimization

Meishan Zhang1 and Yue Zhang2 and Guohong Fu1
1. School of Computer Science and Technology, Heilongjiang University, China

2. Singapore University of Technology and Design
mason.zms@gmail.com,
yue zhang@sutd.edu.sg,

ghfu@hotmail.com

Abstract

Neural networks have shown promising
results for relation extraction. State-of-
the-art models cast the task as an end-to-
end problem, solved incrementally using
a local classifier. Yet previous work us-
ing statistical models have demonstrated
that global optimization can achieve better
performances compared to local classifica-
tion. We build a globally optimized neural
model for end-to-end relation extraction,
proposing novel LSTM features in order
to better learn context representations. In
addition, we present a novel method to in-
tegrate syntactic information to facilitate
global learning, yet requiring little back-
ground on syntactic grammars thus being
easy to extend. Experimental results show
that our proposed model is highly effec-
tive, achieving the best performances on
two standard benchmarks.

1 Introduction

Extracting entities (Florian et al., 2006, 2010) and
relations (Zhao and Grishman, 2005; Jiang and
Zhai, 2007; Sun et al., 2011; Plank and Mos-
chitti, 2013) from unstructured texts have been
two central tasks in information extraction (Grish-
man, 1997; Doddington et al., 2004). Traditional
approaches to relation extraction take entity recog-
nition as a predecessor step in a pipeline (Zelenko
et al., 2003; Chan and Roth, 2011), predicting re-
lations between given entities.

In recent years, there has been a surge of inter-
est in performing end-to-end relation extraction,
jointly recognizing entities and relations given free
text inputs (Li and Ji, 2014; Miwa and Sasaki,
2014; Miwa and Bansal, 2016; Gupta et al., 2016).
End-to-end learning prevents error propagation in

the pipeline approach, and allows cross-task de-
pendencies to be modeled explicitly for entity
recognition. As a result, it gives better relation ex-
traction accuracies compared to pipelines.

Miwa and Bansal (2016) were among the first to
use neural networks for end-to-end relation extrac-
tion, showing highly promising results. In partic-
ular, they used bidirectional LSTM (Graves et al.,
2013) to learn hidden word representations under
a sentential context, and further leveraged tree-
structured LSTM (Tai et al., 2015) to encode syn-
tactic information, given the output of a parser.
The resulting representations are then used for
making local decisions for entity and relation ex-
traction incrementally, leading to much improved
results compared with the best statistical model (Li
and Ji, 2014). This demonstrates the strength of
neural representation learning for end-to-end rela-
tion extraction.

On the other hand, Miwa and Bansal (2016)’s
model is trained locally, without considering struc-
tural correspondences between incremental deci-
sions. This is unlike existing statistical methods,
which utilize well-studied structured prediction
methods to address the problem (Li and Ji, 2014;
Miwa and Sasaki, 2014). As has been commonly
understood, learning local decisions for structured
prediction can lead to label bias (Lafferty et al.,
2001), which prevents globally optimal structures
from receiving optimal scores by the model. We
address this potential issue by building a struc-
tural neural model for end-to-end relation extrac-
tion, following a recent line of efforts on globally
optimized models for neural structured prediction
(Zhou et al., 2015; Watanabe and Sumita, 2015;
Andor et al., 2016; Wiseman and Rush, 2016).

In particular, we follow Miwa and Sasaki
(2014), casting the task as an end-to-end table-
filling problem. This is different from the action-
based method of Li and Ji (2014), yet has shown to

1730



be more flexible and accurate (Miwa and Sasaki,
2014). We take a different approach to representa-
tion learning, addressing two potential limitations
of Miwa and Bansal (2016).

First, Miwa and Bansal (2016) rely on exter-
nal syntactic parsers for obtaining syntactic in-
formation, which is crucial for relation extraction
(Culotta and Sorensen, 2004; Zhou et al., 2005;
Bunescu and Mooney, 2005; Qian et al., 2008).
However, parsing errors can lead to encoding in-
accuracies of tree-LSTMs, thereby hurting rela-
tion extraction potentially. We take an alternative
approach to integrating syntactic information, by
taking the hidden LSTM layers of a bi-affine at-
tention parser (Dozat and Manning, 2016) to aug-
ment input representations. Pretrained for parsing,
such hidden layers contain rich syntactic informa-
tion on each word, yet do not explicitly represent
parsing decisions, thereby avoiding potential is-
sues caused by incorrect parses.

Our method is also free from a particular syn-
tactic formalism, such as dependency grammar,
constituent grammar or combinatory categorial
grammar, requiring only hidden representations on
word that contain syntactic information. In con-
trast, the method of Miwa and Bansal (2016) must
consider tree LSTM formulations that are specific
to grammar formalisms, which can be structurally
different (Tai et al., 2015).

Second, Miwa and Bansal (2016) did not ex-
plicitly learn the representation of segments when
predicting entity boundaries or making relation
classification decisions, which can be intuitively
highly useful, and has been investigated in sev-
eral studies (Wang and Chang, 2016; Zhang et al.,
2016). We take the LSTM-Minus method of Wang
and Chang (2016), modelling a segment as the dif-
ference between its last and first LSTM hidden
vectors. This method is highly efficient, yet gives
as accurate results as compared to more complex
neural network structures to model a span of words
(Cross and Huang, 2016).

Evaluation on two benchmark datasets shows
that our method outperforms previous methods of
Miwa and Bansal (2016), Li and Ji (2014) and
Miwa and Sasaki (2014), giving the best reported
results on both benchmarks. Detailed analysis
shows that our integration of syntactic features is
as effective as traditional approaches based on dis-
crete parser outputs. We make our code publicly

Associated Press writer Patrick McDowell in Kuwait City
ORG PER PER GPE

ORG-AFF PHYS

Figure 1: Relation extraction. The example is
chosen from the ACE05 dataset, where ORG,
PER and GPE denote organization, person and
geo-political entities, respectively; ORG-AFF and
PHYS denote organization affiliation and physical
relations, respectively.

available under Apache License 2.0.1

2 Model

2.1 Task Definition
As shown in Figure 1, the goal of relation extrac-
tion is to mine relations from raw texts. It consists
of two sub-tasks, namely entity detection, which
recognizes valid entities, and relation classifica-
tion, which determines the relation categories over
entity pairs. We follow recent studies and recog-
nize entities and relations as one single task.

2.2 Method
We follow Miwa and Sasaki (2014) and Gupta
et al. (2016), treating relation extraction as a table-
filling problem, performing entity detection and
relation classification using a single incremental
model, which is similar in spirit to Miwa and
Bansal (2016) by performing the task end-to-end.

Formally, given a sentence w1w2 · · ·wn, we
maintain a table Tn×n, where T (i, j) denotes the
relation between wi and wj . When i = j, T (i, j)
denotes an entity boundary label. We map entity
words into labels under the BILOU (Begin, In-
side, Last, Outside, Unit) scheme, assuming that
there are no overlapping entities in one sentence
(Li and Ji, 2014; Miwa and Sasaki, 2014; Miwa
and Bansal, 2016). Only the upper triangular table
is necessary for indicating the relations.

We adopt the close-first left-to-right order
(Miwa and Sasaki, 2014) to map the two-
dimensional table into a sequence, in order to
fill the table incrementally. As shown in Fig-
ure 2, first {T (i, i)} are filled by growing i, and
then the sequence {T (i, i + 1)} is filled, and then
{T (i, i + 2)}, · · · , {T (i, i + n)} are filled incre-
mentally, until the table is fully annotated.

During the table-filling process, we take two la-
bel sets for entity detection (i = j) and relation

1https://github.com/zhangmeishan/NNRelationExtraction

1731



Associated Press writer Patrick McDowell in Kuwait City

Associated
Press
writer
Patrick

McDowell

in
Kuwait

City

1 B-ORG 9 ⊥ 16 ⊥ 22 ⊥ 27 ⊥ 31 ⊥ 34 ⊥ 36 ⊥
2 L-ORG 10←−−−−−ORG-AFF 17 ⊥ 23 ⊥ 28 ⊥ 32 ⊥ 35 ⊥

3 U-PER 11 ⊥ 18 ⊥ 24 ⊥ 29 ⊥ 33 ⊥
4 B-PER 12 ⊥ 19 ⊥ 25 ⊥ 30 ⊥

5 L-PER 13 ⊥ 20 ⊥ 26−−−→PHYS
6 O 14 ⊥ 21 ⊥

7 B-GPE 15 ⊥
8 L-GPE

Figure 2: Table-filling example, where numbers indicate the filling order.

classification (i < j), respectively. The labels for
entity detection include {B-*, I-*, L-*, O, U-* },
where * denotes the entity type, and the labels for
relation classification are {−→∗ ,←−∗ ,⊥}, where * de-
notes the relation category and⊥ denotes a NULL
relation.2

At each step, given a partially-filled table T , we
determine the most suitable label l for the next step
using a scoring function:

score(T, l) = WlhT , (1)

where Wl is a model parameter and hT is the vec-
tor representation of T . Based on the function,
we aim to find the best label sequence l1 · · · lm,
where m = n(n+1)2 , and the resulting sequence of
partially-filled tables is T0T1 · · ·Tm, where Ti =
FILL(Ti−1, li), and T0 is an empty table. Differ-
ent from previous work, we investigate a structural
model that is optimized for the label sequence
l1 · · · lm globally, rather than for each li locally.
2.3 Representation Learning

At the ith step, we determine the label li of the
next table slot based on the current hypothesis
Ti−1. Following Miwa and Bansal (2016), we use
a neural network to learn the vector representation
of Ti−1, and then use Equation 1 to rank candidate
next labels. There are two types of input features,
including the word sequence w1w2 · · ·wn, and the
readily filled label sequence l1l2 · · · li−1. We build
a neural network to represent Ti−1.

2.3.1 Word Representation
Shown in Figure 3, we represent each word wi
by a vector hwi using its word form, POS tag and
characters. Two different forms of embeddings are
used based on the word form, one being obtained
by using a randomly initialized look-up table Ew,

2We remove the illegal table-filling labels during decod-
ing for training and testing. For example, T (i, j) must be ⊥
if T (i, i) or T (j, j) equals O.

hw

⊕
ewe′w et hchar

⊕ ⊕
CNN

character sequence

Figure 3: Word representations.

tuned during training and represented by ew, and
the other being a pre-trained external word embed-
ding from E′w, which is fixed and represented by
e′w.3 For a POS tag t, its embedding et is obtained
from a look-up table Et similar to Ew.

The above two components have also been used
by Miwa and Bansal (2016). We further enhance
the word representation by using its character se-
quence (Ballesteros et al., 2015; Lample et al.,
2016), taking a convolution neural network (CNN)
to derive a character-based word representation
hchar, which has been demonstrated effective for
several NLP tasks (dos Santos and Gatti, 2014).
We obtain the final hwi based on a non-linear feed-
forward layer on e′w ⊕ ew ⊕ et ⊕ hchar, where ⊕
denotes concatenation.

2.3.2 Label Representation
In addition to the word sequence, the history la-
bel sequence l1l2 · · · li−1, and especially the la-
bels representing detected entities, are also use-
ful disambiguation. For example, the previous en-
tity boundary label can be helpful to deciding the
boundary label of the current word. During re-
lation classification, the types of the entities in-
volved can indicate the relation category between
them. We exploit the diagonal label sequence of
partial table T , which denotes entity boundaries,
to enhance the representation learning. A word’s
entity boundary label embedding el is obtained by

3We use the set of pre-trained glove word embeddings
available at http://nlp.stanford.edu/data/glove.6B.zip as ex-
ternal word embeddings.

1732



hi−1...... hi ...... hj ......

hseg = hj − hi−1

Figure 4: Segment representation.

using a randomly initialized looking-up table El.

2.3.3 LSTM Features
We follow Miwa and Bansal (2016), learn-
ing global context representations using LSTMs.
Three basic LSTM structures are used: a left-
to-right word LSTM (

−−−−→
LSTMw), a right-to-left

word LSTM (
←−−−−
LSTMw) and a left-to-right entity

boundary label LSTM (
−−−−→
LSTMe). Each LSTM

derives a sequence of hidden vectors for inputs.
For example, for w1w2 · · ·wn, −−−−→LSTMw gives
hw,→1 h

w,→
2 · · ·hw,→n .

Different from Miwa and Bansal (2016), who
use the output hidden vectors {hi} of LSTMs to
represent words, we exploit segment representa-
tions as well. In particular, for a segment of
text [i, j], the representation is computed by using
LSTM-Minus (Wang and Chang, 2016), shown by
Figure 4, where hj − hi−1 in a left-to-right LSTM
and hi − hj+1 in a right-to-left LSTM are used
to represent the segment [i, j]. The segment rep-
resentations can reflect entities in a sentence, and
thus can be potentially useful for both entity de-
tection and relation extraction.

2.3.4 Feature Representation
We use separate feature representations for entity
detection and relation classification, both of which
are extracted from the above three LSTM struc-
tures. In particular, we first extract a set of base
neural features, and then concatenate them and
feed them into a non-linear neural layer for entity
detection and relation classification, respectively.
Figure 5 shows the overall representation.

[Entity Detection] Figure 5(a) shows the fea-
ture representation for the entity detection. First,
we extract six feature vectors from the three basic
LSTMs, three of which are word features, namely
hw,→i , h

w,←
i and h

e,→
i−1 , and the remaining are seg-

ment features, namely hw,→[j,i−1], h
w,←
[j,i−1] and h

e,→
[j,i−1],

where j denotes the start position of the previ-
ous entity.4 The segment features are computed
dynamically from the partial outputs of entity de-
tection, according to the boundaries of the lastly-

4The non-entity word is treated as a special unit entity to
extract segmental features.

...... ...... ......

...... ...... ......

...... ......

hT

−−−−→
LSTMw

←−−−−
LSTMw

−−−−→
LSTMe

...... j − 1 j ...... i− 1 i i+ 1 ......

concatenate

feed-forward

(a) entity detection

...... ...... ...... ...... ......

...... ...... ...... ...... ......

...... ...... ...... ...... ......

hT

−−−−→
LSTMw

←−−−−
LSTMw

−−−−→
LSTMe

left entityi middle entityj right

concatenate

feed-forward

(b) relation classification

Figure 5: Feature representation.

formed entity during the decoding. The six vectors
are concatenated and then fed into a non-linear
layer for entity detection.

[Relation Classification] Figure 5(b) shows the
feature representation for relation classification.
Similar to entity detection, we extract a set of fea-
tures from the basic LSTMs (

−−−−→
LSTMw,

←−−−−
LSTMw

and
−−−−→
LSTMe), and then concatenate them for a

non-linear classification layer. The differences be-
tween relation classification with entity detection
lie in the range of hidden layers from LSTMs. For
relation classification between i and j, we split
each LSTM into five segments according to the
two entities ended with i and j. Formally, let
[s(i), i] and [s(j), j] denote the two entities above,
where s(·) denotes the start position of an entity,
the resulted segments are [0, s(i)− 1] (i.e., left, in
Figure 5(b)), [s(i), i] (i.e., entityi), [i+1, s(j)−1]
(i.e., middle), [s(j), j] (i.e., entityj) and [j + 1, n]
(i.e., right), respectively. For the word LSTMs,
we extract all five segment features, while the en-

1733



Models Encoder LAS
S-LSTM (2015) 1-Layer LSTM 90.9

K&G (2016) 2-Layer Bi-LSTM 91.9
D&M (2016) 4-Layer Bi-LSTM 93.8

Table 1: Encoder structures and performances of
three state-of-the-art dependency parsers, where
S-LSTM (2015) refers to Dyer et al. (2015), K&G
(2016) refers to the best parser of Kiperwasser and
Goldberg (2016), D&M (2016) refers to Dozat
and Manning (2016), and LAS (labeled attach-
ment score) is the major evaluation metric.

tity label LSTM, we only use the segment features
of entityi and entityj .

2.3.5 Syntactic Features

Previous work has shown that syntactic features
are useful for relation extraction (Zhou et al.,
2005). For example, the shortest dependency
path has been used by several relation extraction
models (Bunescu and Mooney, 2005; Miwa and
Bansal, 2016). Here we propose a novel method
to integrate syntax, without need for prior knowl-
edge on concrete syntactic structures.

In particular, we take state-of-the-art syntactic
parsers that use encoder-decoder neural models
(Buys and Blunsom, 2015; Kiperwasser and Gold-
berg, 2016), where the encoder represents the syn-
tactic features of the input sentences. For exam-
ple, LSTM hidden states over the input word/tag
sequences has been used frequently as syntac-
tic features (Kiperwasser and Goldberg, 2016).
Such features represent input words with syntac-
tic information. The parser decoder also leverages
partially-parsed results, such as features from par-
tial syntactic trees, although we do not use explicit
output features. Table 1 shows the encoder struc-
tures of three state-of-the-art dependency parsers.

Our method is to leverage trained syntactic
parsers, dumping the encoder feature represen-
tations given our inputs, using them directly as
part of input embeddings in our proposed model.
Denoting the dumped syntactic features on each
word as hsyn1 h

syn
2 · · ·hsynn , we feed them into a non-

linear neural layer, and then generate two LSTMs
(bi-directional) based on the outputs, namely−−−−→
LSTMsyn and

←−−−−
LSTMsyn, respectively, augment-

ing the original three LSTMs into five LSTMs.
Features are extracted from the two new LSTMs
in the same way as from the basic bi-directional

word LSTMs.
In this paper, we exploit the parser of Dozat and

Manning (2016), since it achieves the current best
performance for dependency parsing. Our method
can be easily generalized to other parsers, which
are potentially useful for our task as well. For ex-
ample, we can use a constituent parser in the same
way by dumping the implicit encoder features.

Our exploration of syntactic features has two
main advantages over the method of Miwa and
Bansal (2016), where dependency path LSTMs are
used for relation classification. On the one hand,
incorrect dependency paths between entity pairs
can propagate to relation classification in Miwa
and Bansal (2016), because these paths rely on ex-
plicit discrete outputs from a syntactic parser. Our
method can avoid the problem since we do not
compute parser outputs. On the other hand, the
computation complexity is largely reduced by us-
ing our method since sequential LSTMs are based
on inputs only, while the dependency path LSTMs
should be computed based on the dynamic en-
tity detection outputs. When beam search is ex-
ploited during decoding, increasing number of de-
pendency paths can be used by a surge of entity
pairs from beam outputs.

Our method can be extended into neural stack-
ing Wang et al. (2017), by doing back-propagation
training of the parser parameters during model
training, which are leave for future work.

2.4 Training and Search
2.4.1 Local Optimization
Previous work (Miwa and Bansal, 2016; Gupta
et al., 2016) trains model parameters by model-
ing each step for labeling one input sentence sepa-
rately. Given a partial table T , its neural represen-
tation hT is first obtained, and then compute the
next label scores {l1, l2, · · · , ls} using Equation 1.
The output scores are regularized into a probabil-
ity distribution {pl1 , pl2 , · · · , pls} by using a soft-
max layer. The training objective is to minimize
the cross-entropy loss between this output distri-
bution with the gold-standard distribution:

loss(T, lgi , Θ) = − log plgi , (2)

where lgi is the gold-standard next label for T , and
Θ is the set of all model parameters. We refer this
training method as local optimization, because it
maximizes the score of the gold-standard label at
each step locally.

1734



Algorithm 1 Beam-search.
agenda← { (empty table, score=0.0) }
for i in 1 · · ·max-step

next scored tables← { }
for scored table in agenda

labels← NEXTLABELS(scored table)
for next label in labels

new← FILL(scored table, next label)
ADDITEM(next scored tables, new)

agenda← TOP-B(next scored tables, B)

During the decoding phase, the greedy search
strategy is applied in consistence with the train-
ing. At each step, we find the highest-scored label
based on the current partial table, before going on
to the next step.

2.4.2 Global Optimization
We exploit the global optimization strategy of
Zhou et al. (2015) and Andor et al. (2016), max-
imizing the cumulative score of the gold-standard
label sequence for one sentence as a unit. Global
optimization has achieved success for several NLP
tasks under the neural setting (Zhou et al., 2015;
Watanabe and Sumita, 2015). For relation extrac-
tion, global learning gives the best performances
under the discrete setting (Li and Ji, 2014; Miwa
and Sasaki, 2014). We study such models here for
neural network models.

Given a label sequence of l1l2 · · · li, the score of
Ti is defined as follows:

score(Ti) =
i∑

j=0

score(Tj−1, lj)

= score(Ti−1) + score(Ti−1, li),

(3)

where score(T0) = 0 and score(Ti−1, li) is com-
puted by Equation 1. By this definition, we maxi-
mize the scores of all gold-standard partial tables.

Again cross-entropy loss is used to perform
model updates. At each step i, the objective func-
tion is defined by:

loss(x, T gi , Θ) = − log pT gi
= − log score(T

g
i )∑

T ′i
score(T ′i )

,
(4)

where x denotes the input sentence, T gi denotes
the gold-standard state at step i, and T ′i are all par-
tial tables that can be reached at step i.

The major challenge is to compute pT gi , be-
cause we cannot traverse all partial tables that are

valid at step i, since their count increases expo-
nentially by the step number. We follow Andor
et al. (2016), approximating the probability by us-
ing beam search and early-update.

Shown in Algorithm 1, we use standard
beam search, maintaining the B highest-scored
partially-filled tables in an agenda at each step.
When each action of table filling is taken, all hy-
potheses in the agenda are expanded by enumer-
ating the next labels, and the B highest-scored re-
sulting tables are used to replace the agenda for the
next step. Search begins with the agenda contain-
ing an empty table, and finishes when all cells of
the tables in the agenda have been filled. When the
beam size is 1, the algorithm is the same as greedy
decoding. When the beam size is larger than 1,
however, error propagation is alleviated. For train-
ing, the same beam search algorithm is applied to
training examples, and early-update (Collins and
Roark, 2004) is used to fix search errors.

3 Experiments

3.1 Data and Evaluation

We evaluate the proposed model on two datasets,
namely the ACE05 data and the corpus of Roth
and Yih (2004) (CONLL04), respectively. The
ACE05 dataset defines seven coarse-grained entity
types and six coarse-grained relation categories,
while the CONLL04 dataset defines four entity
types and five relation categories.

For the ACE05 dataset, we follow Li and Ji
(2014) and Miwa and Bansal (2016), splitting and
preprocessing the dataset into training, develop-
ment and test sets.5 For the CONLL04 dataset,
we follow Miwa and Sasaki (2014) to split the data
into training and test corpora, and then divide 10%
of the training corpus for development.

We use the micro F1-measure as the major met-
ric to evaluate model performances, treating an en-
tity as correct when its head region and type are
both correct,6 and regard a relation as correct when
the argument entities and the relation category are
all correct. We exploit pairwise t-test for measur-
ing significance values.

5https://github.com/tticoin/LSTM-ER/.
6For the ACE05 dataset, the head region is defined by the

corpus, and for the CONLL04 dataset, the head region covers
the entire scope of an entity.

1735



Network Structure Size
Word Embedding 200
Tag Embedding 50
Char Embedding 50

Entity Label Embedding 50
Input/Output of Word LSTMs 250

Input/Output of Entity Label LSTMs 100
Table Representation 300

Table 2: Dimension sizes.

Model Entity F1 Relation F1
baseline 81.5 50.9

-character 80.9 50.2
-segment 80.2 49.8(entity detection)

Table 3: Feature ablation tests.

3.2 Parameter Tuning
We update all model parameters by back propa-
gation using Adam (Kingma and Ba, 2014) with
a learning rate 10−3, using gradient clipping by
a max norm 10 and l2-regularization by a pa-
rameter 10−5. The dimension sizes of various
vectors in neural network structure are shown in
Table 2. All the hyper-parameters are tuned by
development experiments. All experiments are
conducted using gcc version 4.9.4 (Ubuntu 4.9.4-
2ubuntu1 14.04.1), on an Intel(R) Xeon(R) CPU
E5-2670 @ 2.60GHz.

Online training is used to learn parameters,
traversing over the entire training examples by 300
iterations. We select the best iteration number ac-
cording to the development results. In particu-
lar, we exploit pre-training techniques (Wiseman
and Rush, 2016) to learn better model parameters.
For the local model, we follow Miwa and Bansal
(2016), training parameters only for entity detec-
tion during the first 20 iterations. For the global
model, we pretrain our model using local opti-
mization for 40 iterations, before conducting beam
global optimization.

3.3 Development Experiments
We conduct several development experiments on
the ACE05 development dataset.

3.3.1 Feature Ablation Tests
We consider the baseline system with no syntac-
tic features using local training. Compared with
Miwa and Bansal (2016), we introduce character-
level features, and in addition exploit segmental

Model Beam Relation F1 Speed
Local 1 50.9 95.6

Local(+SS) 1 51.2 95.1

Global
1 51.4 95.3
3 51.8 52.0
5 52.6 36.9

Table 4: Comparisons between local and global
models, where SS denotes scheduled sampling,
and speed is measured by the number of sentences
per second.

features for entity detection. Feature ablation ex-
periments are conducted for the two types of fea-
tures. Table 3 shows the experimental results,
which demonstrate that the character-level fea-
tures and the segment features we use are both use-
ful for relation extraction.

3.3.2 Local v.s. Global Training
We study the influence of training strategies for re-
lation extraction without using syntactic features.
For the local model, we apply scheduled sampling
(Bengio et al., 2015), which has been shown to
improve the performance of relation extraction by
Miwa and Bansal (2016).

Table 4 shows the results. Scheduled sampling
achieves improved F-measure scores for the local
model. With the same greedy search strategy, the
globally normalized model gives slightly better re-
sults than the local model with scheduled sam-
pling. The performance of the global model in-
creases with a larger beam size. When beam size
5 is exploited, we obtain a further gain of 1.2%
on the relation F-measure, which is significantly
better than our baseline local model with sched-
uled sampling (p ≈ 10−4). However, the decoding
speed becomes intolerably slow when the beam
size increases beyond 5. Thus we exploit a beam
size of 5 for global training considering both per-
formance and efficiency.

3.3.3 Syntactic Features
We examine the effectiveness of the proposed im-
plicit syntactic features. Table 5 shows the devel-
opment results using both local and global opti-
mization. The proposed features improve the rela-
tion performances significantly under both settings
(p < 10−4), demonstrating that our use of syntac-
tic features is highly effective.

We also compare our feature integration method
with the traditional methods based on syntactic

1736



Model Features Entity F1 Relation F1

Local all 81.6 53.0-syn 81.5 50.9

Global all 81.9 54.2-syn 81.6 52.6

Table 5: The influence of syntactic features.

model ACE05 CONLL04Entity Relation Entity Relation
Our Model 83.6 57.5 85.6 67.8

M&B (2016) 83.4 55.6 — —
L&J (2014) 80.8 49.5 — —
M&S (2014) — — 80.7 61.0

Table 6: Final results on the test datasets.

outputs which Miwa and Bansal (2016) and all
previous methods use. We use the same parser
of Dozat and Manning (2016), building features
on its dependency outputs. We exploit the bi-
directional tree LSTM of Teng and Zhang (2016)
to extract neural features, and then exploit a non-
linear feed-forward neural network to combine the
two features. Similarly, we extract segment fea-
tures but by using max pooling instead over the
sequential outputs of the feed-forward layer, since
the vector minus is nonsense here. The final rela-
tion results are 53.1% and 53.9% for the local and
global models, respectively, which have no signif-
icantly differences compared with our models. On
the other hand, our method is relatively more effi-
cient, and flexible to the grammar formalism.

3.4 Final Results
Table 6 shows the final results on the test datasets
of ACE05 and CONLL04. We show several top-
performing systems in the table as well, where
M&B (2016) refers to Miwa and Bansal (2016),
who exploit end-to-end LSTM neural networks
with local optimization, and L&J (2014) and M&S
(2014) refer to Li and Ji (2014) and Miwa and
Sasaki (2014), respectively, which are both glob-
ally optimized models using discrete features, giv-
ing the top F-scores among statistical models.7

Overall, neural models give better performances
7Gupta et al. (2016) proposed a locally optimized model

but used a different test dataset from CONLL04 and a differ-
ent evaluation method, reporting entity and relation F-scores
of 93.6% and 72.1%, respectively. Their results are not di-
rectly comparable to the results in Table 6. In particular, they
regard an entity as correct if at least one token is tagged cor-
rectly, which influences the results significantly since multi-
word entities accounts for over 50% of all entities.

5 10 15 20 25 ≥30
20

40

60

80

100

A
cc

ur
ac

y(
%

)

global local

Figure 6: Sentence-level accuracies with respect
to sentence length.

1 2 3 4 5 ≥6
30

40

50

60

70

F-
sc

or
e(

%
)

+syn -syn

Figure 7: F-scores with respect to the distance be-
tween entity pairs.

than statistical models, and global optimization
can give improved performances as well. Our fi-
nal model achieves the best performances on both
datasets. Compared with the best reported re-
sults, our model gives improvements of 1.9% on
ACE05, and 6.8% on CONLL04.

3.5 Analysis
We conduct analysis on the ACE05 test dataset in
order to better understand our models, on its two
major contributions, first examining the influences
of global optimization, and then studying the gains
by using the proposed syntactic features.

Intuitively global optimization should give bet-
ter accuracies at the sentence level. We verify
this by examining the sentence-level accuracies,
where one sentence is regarded as correct when
all the labels in the resulted table are correct. Fig-
ure 6 shows the result, which is consistent with
our intuition. The sentence-level accuracies of the
globally normalized model are consistently better
than the local model. In addition, the accuracy de-
creases sharply as the sentence length increases,
with the local model suffering more severely from
larger sentences.

To understand the effectiveness of the proposed
syntactic features, we examine the relation F-
scores with respect to entity distances. Miwa
and Bansal (2016) exploit the shortest dependency
path, which can make the distance between two
entities closer compared with their sequential dis-

1737



tance, thus facilitating relation extraction. We ver-
ify whether the proposed syntactic features can
benefit our model similarly. As shown in Fig-
ure 7, the F-scores of entity-pairs with large dis-
tances see apparent improvements, demonstrating
that our use of syntactic features has a similar ef-
fect compared to the shortest dependency path.

4 Related Work

Entity recognition (Florian et al., 2004, 2006;
Ratinov and Roth, 2009; Florian et al., 2010; Kuru
et al., 2016) and relation extraction (Zhao and Gr-
ishman, 2005; Jiang and Zhai, 2007; Zhou et al.,
2007; Qian and Zhou, 2010; Chan and Roth, 2010;
Sun et al., 2011; Plank and Moschitti, 2013; Verga
et al., 2016) have received much attention in the
NLP community. The dominant methods treat the
two tasks separately, where relation extraction is
performed assuming that entity boundaries have
been given (Zelenko et al., 2003; Miwa et al.,
2009; Chan and Roth, 2011; Lin et al., 2016).

Several studies find that extracting entities and
relations jointly can benefit both tasks. Early work
conducts joint inference for separate models (Ji
and Grishman, 2005; Roth and Yih, 2004, 2007).
Recent work shows that joint learning and decod-
ing with a single model brings more benefits for
the two tasks (Li and Ji, 2014; Miwa and Sasaki,
2014; Miwa and Bansal, 2016; Gupta et al., 2016),
and we follow this line of work in the study.

LSTM features have been extensively exploited
for NLP tasks, including tagging (Huang et al.,
2015; Lample et al., 2016), parsing (Kiperwasser
and Goldberg, 2016; Dozat and Manning, 2016),
relation classification (Xu et al., 2015; Vu et al.,
2016; Miwa and Bansal, 2016) and sentiment
analysis (Li et al., 2015; Teng et al., 2016). Based
on the output of LSTM structures, Wang and
Chang (2016) introduce segment features, and ap-
ply it to dependency parsing. The same method is
applied to constituent parsing by Cross and Huang
(2016). We exploit this segmental representation
for relation extraction.

Global optimization and normalization has been
successfully applied on many NLP tasks that in-
volve structural prediction (Lafferty et al., 2001;
Collins, 2002; McDonald et al., 2010; Zhang and
Clark, 2011), using traditional discrete features.
For neural models, it has recently received increas-
ing interests (Zhou et al., 2015; Andor et al., 2016;
Xu, 2016; Wiseman and Rush, 2016), and im-

proved performances can be achieved with global
optimization accompanied by beam search. Our
work is in line with these efforts. To our knowl-
edge, we are the first to apply globally optimized
neural models for end-to-end relation extraction,
achieving the best results on standard benchmarks.

5 Conclusion

We investigated a globally normalized end-to-end
relation extraction model using neural network,
based on the table-filling framework proposed by
Miwa and Sasaki (2014). Feature representations
are learned from several LSTM structures over the
inputs, and a novel simple method is used to in-
tegrate syntactic information. Experiments show
the effectiveness of both global normalization and
syntactic features. Our final model achieved the
best performances on two benchmark datasets.

Acknowledgments

We thank the anonymous reviewers for their con-
structive comments, which help to improve the
paper, and Zhiyang Teng for dumping intermedi-
ate outputs from the bi-affine parser. This work
is supported by National Natural Science Foun-
dation of China (NSFC) grants 61602160 and
61672211, Natural Science Foundation of Hei-
longjiang Province (China) grant F2016036, Spe-
cial business expenses in Heilongjiang Province
(China) grant 2016-KYYWF-0183, the Singapore
Ministry of Education (MOE) AcRF Tier 2 grant
T2MOE201301 and SRG ISTD 2012 038 from
Singapore University of Technology and Design.
Yue Zhang is the corresponding author.

References
Daniel Andor, Chris Alberti, David Weiss, Aliaksei

Severyn, Alessandro Presta, Kuzman Ganchev, Slav
Petrov, and Michael Collins. 2016. Globally nor-
malized transition-based neural networks. In ACL,
pages 2442–2452.

Miguel Ballesteros, Chris Dyer, and Noah A. Smith.
2015. Improved transition-based parsing by model-
ing characters instead of words with lstms. In Pro-
ceedings of the EMNLP, pages 349–359.

Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and
Noam Shazeer. 2015. Scheduled sampling for se-
quence prediction with recurrent neural networks.
In NIPS, pages 1171–1179.

Razvan C Bunescu and Raymond J Mooney. 2005. A
shortest path dependency kernel for relation extrac-

1738



tion. In EMNLP, pages 724–731. Association for
Computational Linguistics.

Jan Buys and Phil Blunsom. 2015. Generative incre-
mental dependency parsing with neural networks. In
Proceedings of the 53rd ACL, pages 863–869.

Yee Seng Chan and Dan Roth. 2010. Exploiting back-
ground knowledge for relation extraction. In COL-
ING, pages 152–160.

Yee Seng Chan and Dan Roth. 2011. Exploiting
syntactico-semantic structures for relation extrac-
tion. In ACL, pages 551–560.

Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and ex-
periments with perceptron algorithms. In EMNLP,
pages 1–8.

Michael Collins and Brian Roark. 2004. Incremen-
tal parsing with the perceptron algorithm. In ACL,
pages 111–118.

James Cross and Liang Huang. 2016. Span-based
constituency parsing with a structure-label system
and provably optimal dynamic oracles. In EMNLP,
pages 1–11.

Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In ACL, pages
423–429.

George R Doddington, Alexis Mitchell, Mark A Przy-
bocki, Lance A Ramshaw, Stephanie Strassel, and
Ralph M Weischedel. 2004. The automatic content
extraction (ace) program-tasks, data, and evaluation.
In LREC, volume 2, page 1.

Timothy Dozat and Christopher D Manning. 2016.
Deep biaffine attention for neural dependency pars-
ing. arXiv preprint arXiv:1611.01734.

Chris Dyer, Miguel Ballesteros, Wang Ling, Austin
Matthews, and Noah A. Smith. 2015. Transition-
based dependency parsing with stack long short-
term memory. In ACL, pages 334–343.

R Florian, H Hassan, A Ittycheriah, H Jing, N Kamb-
hatla, X Luo, N Nicolov, and S Roukos. 2004. A sta-
tistical model for multilingual entity detection and
tracking. In NAACL, pages 1–8.

Radu Florian, Hongyan Jing, Nanda Kambhatla, and
Imed Zitouni. 2006. Factorizing complex models: A
case study in mention detection. In COLING/ACL,
pages 473–480.

Radu Florian, John Pitrelli, Salim Roukos, and Imed
Zitouni. 2010. Improving mention detection robust-
ness to noisy input. In EMNLP, pages 335–345.

Alex Graves, Abdel-rahman Mohamed, and Geoffrey
Hinton. 2013. Speech recognition with deep recur-
rent neural networks. In ICASSP, pages 6645–6649.
IEEE.

Ralph Grishman. 1997. Information extraction: Tech-
niques and challenges. In Information extraction a
multidisciplinary approach to an emerging informa-
tion technology, pages 10–27. Springer.

Pankaj Gupta, Hinrich Schütze, and Bernt Andrassy.
2016. Table filling multi-task recurrent neural net-
work for joint entity and relation extraction. In Pro-
ceedings of COLING 2016, pages 2537–2547.

Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidirec-
tional lstm-crf models for sequence tagging. arXiv
preprint arXiv:1508.01991.

Heng Ji and Ralph Grishman. 2005. Improving name
tagging by reference resolution and relation detec-
tion. In ACL, pages 411–418.

Jing Jiang and ChengXiang Zhai. 2007. A systematic
exploration of the feature space for relation extrac-
tion. In NAACL, pages 113–120.

Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Eliyahu Kiperwasser and Yoav Goldberg. 2016. Sim-
ple and accurate dependency parsing using bidirec-
tional lstm feature representations. TACL, 4:313–
327.

Onur Kuru, Ozan Arkan Can, and Deniz Yuret. 2016.
Charner: Character-level named entity recognition.
In Proceedings of COLING 2016, pages 911–921.

John Lafferty, Andrew McCallum, Fernando Pereira,
et al. 2001. Conditional random fields: Probabilistic
models for segmenting and labeling sequence data.
In ICML, volume 1, pages 282–289.

Guillaume Lample, Miguel Ballesteros, Sandeep Sub-
ramanian, Kazuya Kawakami, and Chris Dyer. 2016.
Neural architectures for named entity recognition.
In NAACL, pages 260–270.

Jiwei Li, Thang Luong, Dan Jurafsky, and Eduard
Hovy. 2015. When are tree structures necessary for
deep learning of representations? In Proceedings of
the EMNLP, pages 2304–2314.

Qi Li and Heng Ji. 2014. Incremental joint extraction
of entity mentions and relations. In Proceedings of
the Association for Computational Linguistics.

Yankai Lin, Shiqi Shen, Zhiyuan Liu, Huanbo Luan,
and Maosong Sun. 2016. Neural relation extrac-
tion with selective attention over instances. In ACL,
pages 2124–2133.

Ryan McDonald, Keith Hall, and Gideon Mann. 2010.
Distributed training strategies for the structured per-
ceptron. In NAACL, pages 456–464.

Makoto Miwa and Mohit Bansal. 2016. End-to-end re-
lation extraction using lstms on sequences and tree
structures. In ACL, pages 1105–1116.

1739



Makoto Miwa, Rune Sætre, Yusuke Miyao, and
Jun’ichi Tsujii. 2009. A rich feature vector for
protein-protein interaction extraction from multiple
corpora. In EMNLP, pages 121–130.

Makoto Miwa and Yutaka Sasaki. 2014. Modeling
joint entity and relation extraction with table repre-
sentation. In EMNLP, pages 1858–1869.

Barbara Plank and Alessandro Moschitti. 2013. Em-
bedding semantic similarity in tree kernels for do-
main adaptation of relation extraction. In ACL,
pages 1498–1507.

Longhua Qian and Guodong Zhou. 2010. Clustering-
based stratified seed sampling for semi-supervised
relation classification. In EMNLP, pages 346–355.

Longhua Qian, Guodong Zhou, Fang Kong, Qiaoming
Zhu, and Peide Qian. 2008. Exploiting constituent
dependencies for tree kernel-based semantic relation
extraction. In Coling 2008, pages 697–704.

Lev Ratinov and Dan Roth. 2009. Design chal-
lenges and misconceptions in named entity recog-
nition. In Proceedings of the Thirteenth Confer-
ence on Computational Natural Language Learning
(CoNLL-2009), pages 147–155.

Dan Roth and Wen-tau Yih. 2004. A linear program-
ming formulation for global inference in natural lan-
guage tasks. In CoNLL, pages 1–8.

Dan Roth and Wen-tau Yih. 2007. Global inference
for entity and relation identification via a linear pro-
gramming formulation. Introduction to statistical
relational learning, pages 553–580.

Cicero dos Santos and Maira Gatti. 2014. Deep con-
volutional neural networks for sentiment analysis of
short texts. In Proceedings of COLING 2014, pages
69–78.

Ang Sun, Ralph Grishman, and Satoshi Sekine. 2011.
Semi-supervised relation extraction with large-scale
word clustering. In ACL, pages 521–529.

Kai Sheng Tai, Richard Socher, and Christopher D.
Manning. 2015. Improved semantic representations
from tree-structured long short-term memory net-
works. In ACL, pages 1556–1566.

Zhiyang Teng, Duy Tin Vo, and Yue Zhang. 2016.
Context-sensitive lexicon features for neural senti-
ment analysis. In Proceedings of the EMNLP, pages
1629–1638.

Zhiyang Teng and Yue Zhang. 2016. Bidirectional
tree-structured lstm with head lexicalization. arXiv
preprint arXiv:1611.06788.

Patrick Verga, David Belanger, Emma Strubell, Ben-
jamin Roth, and Andrew McCallum. 2016. Multi-
lingual relation extraction using compositional uni-
versal schema. In Proceedings of the 2016 NAACL,
pages 886–896.

Ngoc Thang Vu, Heike Adel, Pankaj Gupta, and Hin-
rich Schütze. 2016. Combining recurrent and convo-
lutional neural networks for relation classification.
In Proceedings of the NAACL, pages 534–539.

Hongmin Wang, Yue Zhang, GuangYong Leonard
Chan, Jie Yang, and Hai Leong Chieu. 2017. Uni-
versal dependencies parsing for colloquial singa-
porean english. CoRR, abs/1705.06463.

Wenhui Wang and Baobao Chang. 2016. Graph-based
dependency parsing with bidirectional lstm. In ACL,
pages 2306–2315.

Taro Watanabe and Eiichiro Sumita. 2015. Transition-
based neural constituent parsing. In ACL, pages
1169–1179.

Sam Wiseman and Alexander M. Rush. 2016.
Sequence-to-sequence learning as beam-search op-
timization. In EMNLP, pages 1296–1306.

Wenduan Xu. 2016. Lstm shift-reduce ccg parsing. In
EMNLP, pages 1754–1764.

Yan Xu, Lili Mou, Ge Li, Yunchuan Chen, Hao Peng,
and Zhi Jin. 2015. Classifying relations via long
short term memory networks along shortest depen-
dency paths. In Proceedings of the 2015 EMNLP,
pages 1785–1794.

Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2003. Kernel methods for relation ex-
traction. Journal of machine learning research,
3(Feb):1083–1106.

Meishan Zhang, Yue Zhang, and Guohong Fu. 2016.
Transition-based neural word segmentation. In Pro-
ceedings of ACL, pages 421–431, Berlin, Germany.

Yue Zhang and Stephen Clark. 2011. Syntactic pro-
cessing using the generalized perceptron and beam
search. Computational Linguistics, 37(1):105–151.

Shubin Zhao and Ralph Grishman. 2005. Extracting
relations with integrated information using kernel
methods. In ACL, pages 419–426.

GuoDong Zhou, Jian Su, Jie Zhang, and Min Zhang.
2005. Exploring various knowledge in relation ex-
traction. In ACL, pages 427–434.

GuoDong Zhou, Min Zhang, DongHong Ji, and
QiaoMing Zhu. 2007. Tree kernel-based relation ex-
traction with context-sensitive structured parse tree
information. In EMNLP-CoNLL, pages 728–736.

Hao Zhou, Yue Zhang, Shujian Huang, and Jiajun
Chen. 2015. A neural probabilistic structured-
prediction model for transition-based dependency
parsing. In Proceedings of the 53rd ACL, pages
1213–1222.

1740


