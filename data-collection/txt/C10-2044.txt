383

Coling 2010: Poster Volume, pages 383–390,

Beijing, August 2010

Learning Phrase Boundaries

for Hierarchical Phrase-based Translation

Zhongjun HE Yao MENG Hao YU

Fujitsu R&D Center CO., LTD.

{hezhongjun, mengyao, yu}@cn.fujitsu.com
Abstract

Hierarchical phrase-based models pro-
vide a powerful mechanism to capture
non-local phrase reorderings for statis-
tical machine translation (SMT). How-
ever, many phrase reorderings are arbi-
trary because the models are weak on de-
termining phrase boundaries for pattern-
matching. This paper presents a novel
approach to learn phrase boundaries di-
rectly from word-aligned corpus without
using any syntactical information. We use
phrase boundaries, which indicate the be-
ginning/ending of phrase reordering, as
soft constraints for decoding. Experi-
mental results and analysis show that the
approach yields signiﬁcant improvements
over the baseline on large-scale Chinese-
to-English translation.

1 Introduction

The hierarchial phrase-based (HPB) model (Chi-
ang, 2005) outperformed previous phrase-based
models (Koehn et al., 2003; Och and Ney, 2004)
by utilizing hierarchical phrases consisting of both
words and variables. Thus the HPB model has
generalization ability: a translation rule learned
from a phrase pair can be used for other phrase
pairs with the same pattern, e.g. reordering infor-
mation of a short span can be applied for a large
span during decoding. Therefore, the model cap-
tures both short and long distance phrase reorder-
ings.

However, one shortcoming of the HPB model is
that it is difﬁcult to determine phrase boundaries
for pattern-matching. Therefore, during decod-
ing, a rule may be applied for all possible source
phrases with the same pattern. However, incorrect
pattern-matching will cause wrong translation.

Consider the following rule that is used to trans-
late the Chinese sentence in Figure 1 into English:

X → hXL de XR, XR in XLi

(1)

The rule translates the Chinese word “de” into
English word “in”, and swaps the left sub-phrase
covered by XL and the right sub-phrase covered
by XR on the target side. However, XL may
pattern-match 5 spans on the left side of “de” and
XR may pattern-match 3 spans on the right side.
Therefore, the rule produces 15 different deriva-
tions. However, 14 of them are incorrect.

The correct derivation Sc is shown in Figure 2,
while one of the wrong derivations Si is shown in
Figure 3. We observe that the basic difference be-
tween Sc and Si is the phrase boundary matched
by “XR”. In Sc, XR matches the span [7, 9] and
moves it as a whole unit. While in Si, XR matches
the span [7, 8] and left the last word [9, 9] be trans-
lated separately. Similarly, other incorrect deriva-
tions are caused by inadequate pattern-matching
of XL and/or XR.

Previous research showed that phrases should
be constrained to some extent for improving trans-
lation quality. Most of the existing approaches uti-
lized syntactic information to constrain phrases to
respect syntactic boundaries. Chiang (2005) in-
troduced a constituent feature to reward phrases
that match a syntactic tree but did not yield signif-
icant improvement. Marton and Resnik (2008) re-
vised this method by distinguishing different con-
stituent syntactic types, and deﬁned features for
each type to count whether a phrase matches or
crosses the syntactic boundary. This led to a sub-
stantial improvements. Gimpel and Smith (2008)
presented rich contextual features on the source
side including constituent syntactical features for
phrase-based translation. Cherry (2008) utilized
a dependency tree as a soft constraint to detect
syntactic cohesion violations for a phrase-based

384

Figure 1: An example of Chinese-English translation. The rule X → hXL de XR, XR in XLi

pattern-matches 5 and 3 spans on the left and right of the Chinese word “de”, respectively.

Sc ⇒ h她 将 成为 X, She will become Xi

⇒ h她 将 成为 X[4,5] 的 X[7,9], She will become X[7,9] in X[4,5]i
⇒ h她 将 成为 k 印度 有史以来 k 的 k 首位 女 总统,

She will become the ﬁrst female president in India’s historyi

Figure 2: The correct derivation with adequate pattern-matching of XR.

Si ⇒ h她 将 成为 X 总统, She will become X presidenti

⇒ h她 将 成为 X[4,5] 的 X[7,8] 总统, She will become X[7,8] in X[4,5] presidenti
⇒ h她 将 成为 k 印度 有史以来 k 的 k 首位 女 k 总统,

She will become the ﬁrst female in India’s history presidenti

Figure 3: A wrong derivation with inadequate pattern-matching of XR.

system. Xiong et al. (2009) presented a syntax-
driven bracketing model to predict whether two
phrases are translated together or not, using syn-
tactic features learned from training corpus. Al-
though these approaches differ from each other,
the main basic idea is the utilization of syntactic
information.

In this paper, we present a novel approach to
learn phrase boundaries for hierarchical phrase-
based translation. A phrase boundary indicates the
beginning or ending of a phrase reordering. Moti-
vated by Ng and Low (2004) that built a classiﬁer
to predict word boundaries for word segmenta-
tion, we build a classiﬁer to predict phrase bound-
aries. We classify each source word into one of the
4 boundary tags: “b” indicates the beginning of a
phrase, “m” indicates a word appears in the mid-

dle of a phrase, “e” indicates the end of a phrase,
“s” indicates a single-word phrase.

We use phrase boundaries as soft constraints for
decoding. To do this, we incorporate our classiﬁer
as a feature into the HPB model and propose an
efﬁcient decoding algorithm.

Compared to the previous work, out approach

has the following advantages:

• Our approach maintains the strength of the
phrase-based models since it does not re-
quire any syntactical information. There-
fore, phrases do not need to respect syntactic
boundaries.

• The training instances are directly learned
from a word-aligned bilingual corpus, rather
than from manually annotated corpus.

她1
ta

将2
jiang

成为3

chengwei

印度4
yindu

有史以来5
youshiyilai

X[5,5]

X[7,7]

的6
de

首位7
shouwei

女8
nü

总统9

zongtong

She1

will2

become3

the4 ﬁrst5 female6 president7

in8

India’s9 history10

385

• The decoder outputs phrase segmentation in-
in addition to

formation as a byproduct,
translation result.

Chiang (2007) used the standard log-linear
framework (Och and Ney, 2002) to combine var-
ious features:

We evaluate our approach on large-scale
Chinese-to-English translation. Experimental re-
sults and analysis show that using phrase bound-
aries as soft constraints achieves signiﬁcant im-
provements over the baseline system.

2 Previous Work

2.1 Learning Word Boundaries
In some languages, such as Chinese, words are not
demarcated. Therefore, it is a preliminary task to
determine word boundaries for a sentence, which
is the so-called word segmentation.

Ng and Low (2004) regarded word segmen-
tation as a classiﬁcation problem. They labelled
each Chinese character with one of 4 possible
boundary tags: “b”, “m”, “e” respectively indi-
cates the begin, the middle and the end of a word,
and “s” indicates a single-character word. Their
segmenter was built within a maximum entropy
framework and trained on manually segmented
sentences.

Learning phrase boundaries is analogous to
word boundaries. The basic difference is that
the unit for learning word boundaries is charac-
ter while the unit for learning phrase boundaries
is word.
In this paper, we adopt the boundary
tags presented by Ng and Low (2004) and build a
classiﬁer to predict phrase boundaries within max-
imum entropy framework. We train it directly on a
word-aligned bilingual corpus, without any man-
ually annotation and syntactical information.

2.2 The Hierarchical Phrase-based Model
We built a hierarchical phrase-based MT system
(Chiang, 2007) based on weighted SCFG. The
translation knowledge is represented by rewriting
rules:

X → hα, γ,∼i

(2)

where X is a non-terminal, α and γ are source and
target strings, respectively. Both of them contain
words and possibly co-indexed non-terminals. ∼
describes a one-to-one correspondence between
non-terminals in α and γ.

P r(e|f ) ∝Xi

λihi(α, γ)

(3)

where hi(α, γ) is a feature function and λi is
the weight of hi. Analogous to the previous
phrase-based model, Chiang deﬁned the follow-
ing features: translation probabilities p(γ|α) and
p(α|γ),
lexical weights pw(γ|α) and pw(α|γ),
word penalty, rule penalty, and a target n-gram
language model.

In this paper, we integrate a phrase boundary
classiﬁer as an additional feature into the log-
linear model to provide soft constraint for pattern-
matching during decoding. The feature weights
are optimized by MERT algorithm (Och, 2003).

3 Learning Phrase Boundaries

We build a phrase boundary classiﬁer (PBC)
within a maximum entropy framework. The PBC
predicts a boundary tag for each source word, con-
sidering contextual features:

Ptag(t|fj, F J

1 ) =

exp(Pi λihi(t, fj, F J
Pt exp(Pi λihi(t, fj, F J

1 ))
1 )

(4)

where, t ∈ {b, m, e, s}, fj is the jth word in
source sentence F J
1 , hi is a feature function and
λi is the weight of hi.

To build PBC, we ﬁrst present a method to rec-
ognize phrase boundaries and extract training ex-
amples from word-aligned bilingual corpus, then
we deﬁne contextual feature functions.

3.1 Phrase Boundary
During decoding,
intuitively, words within a
phrase should be translated or moved together.
Therefore, a phrase boundary should indicate re-
ordering information. We assign one of the
boundary tags (b, m, e, s) to each word in source
sentences. Thus the word with tag b, e or s is a
phrase boundary. One question is that how to as-
sign boundary tag to a word? In this paper, we
recognize the largest source span which has the
monotone translation. Then we assign boundary

386

because there is a cross link between the upper
bound of “短暂” and the lower bound of “访问”
on the target side. However, it is a PM span ac-
cording to the deﬁnition. Note that in some cases,
a source word may not be contained in any phrase
pair, therefore we consider a single word span as
a PM span, speciﬁcly.

An interesting feature of PM span is that if two
PM spans are consecutive on both source side and
their corresponding target side, the two PM spans
can be combined as a larger PM span. Formally,

Figure 4: Illustration for monotone span (a) and
PM span (b).

tags to each word in the source span, according to
their position.

1 , EI

To do this, we ﬁrst introduce some notations.
Given a bilingual sentence (F J
1 ) together with
word alignment matrix A, we use L(Aj) and
H(Aj) to represent the lowest and highest tar-
get word position which links to the source word
fj, respectively. Since the word alignment for fj
maybe “one-to-many”, all the corresponding tar-
get words will appear in the span [L(Aj), H(Aj)].
we deﬁne a source span [j1, j2] (1 ≤ j1 ≤ j2 ≤

J) a monotone span, iff:
1. ∀(j, i) ∈ A, j1 ≤ j ≤ j2 ↔ L(Aj1) ≤ i ≤

H(Aj2)

2. ∀k1, k2 ∈ [j1, j2], k1 ≤ k2 → H(Ak1) ≤

L(Ak2)

indicates

condition

The
, E

ﬁrst
that
H(Aj2 )
(F j2
L(Aj1 ) ) is a phrase pair as described
j1
previously in phrase-based SMT models. While
the second condition indicates that
the lower
target bound linked to a source word cannot be
lower than any target word position linked to the
previous source word. Therefore, a monotone
span does not contain crossed links or internal
reorderings.

Considering that word alignments could be
very noisy and complex in real-world data, we de-
ﬁne pseudo-monotone (PM) span by loosening the
second condition:

∀k1, k2 ∈ [j1, j2], k1 ≤ k2 → L(Ak1) ≤ L(Ak2)
(5)
This condition allows crossed links to some ex-
tent by loosening the bound of Ak1 from upper
to lower. Figure 4 (a) shows an example of
monotone span, in which the translation is mono-
tone. While Figure 4 (b) is not a monotone span

(F j
j1

, Ei

j+1, Ei2

i+1) = (F j2
j1

, Ei2
i1

)

(6)

i1)M(F j2

where [j1, j] and [j + 1, j2] are PM spans, [i1, i]
and [i + 1, i2] are the target spans corresponding
to [j1, j] and [j +1, j2], respectively. For example,
Figure 4 (a) shows a PM phrase pair that consists
of two small PM pairs “联合, jointly” and “举办
的, held by”.

In this paper, we are interested in phrase re-
ordering boundaries for a source sentence. We de-
ﬁne translation span (TS) the largest possible PM
span. A TS may consist of one or more PM spans.
According to our deﬁnition, cross links may ap-
pear within PM spans but do not appear between
PM spans within a TS. Therefore, TS is the largest
possible span that will be translated as a unit and
phrase reorderings may occur between TSs during
decoding.

To obtain phrase boundary examples from
word-aligned bilingual sentences, we ﬁrst ﬁnd all
possible TSs and then assign boundary tags to
each word. For a TS [j1, j2] (j1 < j2) that contain
more than two words, we assign “b” to the ﬁrst
word fj1 and “e” to the last word fj2, and “m” to
the middle words fj (j1 < j < j2). For a single
word span TS [j, j], we assign “s” to the word fj.
Figure 5 shows an example of labelling source
words with boundary tags. The source sentence is
segmented into 4 TSs. Using the phrase boundary
information to guide decoding, the decoder will
produce the correct derivation and translation as
shown in Figure 2.

(a)

(b)

387

她
0.78
b
m 6.4e-8
2.1e-8
e
s
0.22

将 成为
1.2e-5
0.10
0.75
5.4e-5
0.87
0.11
0.04
0.13

Table 1: The TPM for a source sentence. The
highest probability of each word is in bold.

4 Phrase Boundary Constrained

Decoding

Give a source sentence, we can assign boundary
tags to each word by running the PBC. During
decoding, a rule is prohibited to pattern-match
across phrase boundaries. By doing this, the PBC
is integrated as a hard constraint. However, this
method will invalidate a large number of rules and
the decoder suffers from a risk that there are not
enough rules to cover the source sentence.

Alternatively, inspired by previous approaches,
we integrate the phrase boundary classiﬁer as a
soft constraint by incorporating it as a feature into
the HPB model:

hpbc(F J

1 ) = log(

JYj=1

Ptag(t|fj, F J
1 ))

(7)

To perform translation, for each word fj in
a source sentence F J
1 , we ﬁrst compute all tag
probabilities Ptag(t|fj), where t ∈ (b, m, e, s),
j ∈ [1, J], according to Equation 4. Therefore, we
build a 4 × J tag-word probability matrix (TPM).
T P M [i, j] indicates the probability of the word
fj labelled with the tag ti. Table 1 shows the
TPM for a source text “她 将 成为”.

Then we select rule options from the rule ta-
ble that can be used for translating the source text.

as a bilingual sentence with word alignments, thus

Since each rule option (ef ,ee, a) 1 can be regarded
we ﬁnd all TS in ef and assign an initial tag (IT)

for each source word. This procedure is analogous
to label phrase boundary tags for a word-aligned
bilingual sentence. For example, the following
rules are used for translating the Chinese sentence
in Table 1:

1We keep word alignments of a rule when it is extracted

from bilingual sentence.

Figure 5:
Illustration for labelling the source
words with boundary tags.
The solid boxes
present word alignments. The bordered boxes are
TSs.

3.2 Feature Deﬁnition

The features we used for the PS model are anal-
ogous to (Ng and Low, 2004). For a word W0,
we deﬁne the following contextual features with a
window of “n”:

• The word feature Wn, which denotes the left

(right) n words of the current word W0;

• The part-of-speech (POS) feature Pn, which

denotes the POS tag of the word Wn.

For example, the tag of the word “成为 (be-
come)” in Figure 5 is “e”, indicating that it is
the end of a phrase. If we set the context window
n = 2, the features of the word “成为 (become)”
are:

• W−2=她 W−1=将 W0=成 为 W1=印 度

W2=有史以来

• P−2=r P−1=d P0=v P1=ns P2=l

We collect TSs from bilingual sentences to-
gether with the contextual features and used a
MaxEnt toolkit (Zhang, 2004) to train a PBC.

有
史
以
来 的

成
为

印
度

她 将

TAG b m e b e s b m e

She
will
become
the ﬁrst
female
president
in
India’s
history

388

X → h她bX∗1 , She X1i
X1 → h将b 成为e, will becomei

(8)
(9)

Since both the source sides of these two rules
are PM spans according to the word alignments,
the IT sequences for rule (8) and (9) are “b *”2
and “b e”, respectively. According to Table 1,
the initial hpbc score for these two rules can be
computed as follows:

h(7)
pbc = log(Ptag(b|她)) = log(T P M [1, 1]) (10)
h(8)
pbc = log(Ptag(b|将)) + log(Ptag(e|成为))

= log(T P M [1, 2]) + log(T P M [3, 3]) (11)

Note that to keep the tag sequence valid, e.g.
“m” follows “b” rather than “s”, the ITs maybe
updated during decoding.
The tag-updating
should be consistent with the deﬁnition of TS as
described in Section 3.1. Speciﬁcally, when the
non-terminal symbol X is derived from its cov-
ered span f (X), the boundary tags should be up-
dated.

When a tag of word fj is updated from tk1 to
tk2, the PBC score should also be updated accord-
ing to TPM:

∆P BC = log(T P M [k2, j]) − log(T P M [k1, j])
(12)
The following is a derivation of the source sen-

tence in Table 1:
S ⇒ h她bX∗1 , She X1i

⇒ h她b将b→m 成为e, She will becomei
When X1 is derived, the tag of its left boundary
word “将” is updated from “b” to “m”. The reason
is that after derivation, the combined span forms
a larger PM span and the left boundary of f (X1)
should be updated.

As a result, the hpbc score is recomputed:

hpbc(F 3

1 ) = h(7)

pbc + h(8)

pbc + ∆P BC

(13)

where,

∆P BC = log(T P M [2, 2]) − log(T P M [1, 2])
(14)

2We use “*” as a tag of the non-terminal symbol “X1”

since it has not been derived.

The decoding algorithm is efﬁcient since the
computing of the PBC score is a procedure of
table-lookup.

5 Experiments

5.1 Experimental Setup
Our experiments were on Chinese-to-English
translation. The training corpus (77M+81M) we
used are from LDC 3. The evaluation metric is
BLEU (Papineni et al., 2002), as calculated by
mteval-v11b.pl with case-insensitive matching of
n-grams, where n = 4.

To obtain word alignments, we ﬁrst

ran
GIZA++ (Och and Ney, 2002) in both translation
directions and then reﬁned it by “grow-diag-ﬁnal”
method (Koehn et al., 2003).

For the language model, we used the SRI Lan-
guage Modeling Toolkit (Stolcke, 2002) to train
two 4-gram models on xinhua portion of Giga-
Word corpus and the English side of the training
corpus.

The NIST MT03 test set is used to tune the fea-
ture weights of the log-linear model by MERT
(Och, 2003). We tested our system on the NIST
MT06 and MT08 test sets.

5.2 Results
The results are shown in Table 2. We tested vari-
ous settings of the context window. It is observed
that the small values of n (n = 1, 2) drop the
BLEU score, suggesting that perhaps there are not
enough contextual information. With more con-
textual information is used, the BLEU scores are
improved over all test sets. When n = 3, the most
signiﬁcant improvements are obtained on MT06G
and MT08. The improvements over the baseline
are statistically signiﬁcant at p < 0.01 by using
the signiﬁcant test method described in (Koehn,
2004). While for MT06N, the optimized context
window size is n = 4 but the improvement is
not statistically signiﬁcant.
In most cases, with
n larger than 3, we do not obtain further improve-
ments because of the data sparseness for training

3LDC2002E18,

LDC2002L27,

LDC2002T01,
LDC2003E07, LDC2003E14, LDC2004T07, LDC2005E83,
LDC2005T06, LDC2005T10, LDC2005T34, LDC2006E24,
LDC2006E26, LDC2006E34, LDC2006E86, LDC2006E92,
LDC2006E93, LDC2004T08(HK News, HK Hansards).

389

System
baseline

+PBC (n=1)
+PBC (n=2)
+PBC (n=3)
+PBC (n=4)

MT06G MT06N MT08
26.29
14.66
24.58
13.78
25.87
14.34
15.19*
27.25*
26.70
14.76

34.42
33.20
34.21
34.63
34.73

Table 2: Results on the test sets with different con-
text window (n) of the phrase boundary classiﬁer.
The largest BLEU score on each test set is in bold.
MT06G: MT06 GALE set. MT06N: MT06 NIST
set. *: signiﬁcantly better than the baseline at
p < 0.01.

the classiﬁer.

6 Discussion

The experimental results show that the phrase
boundary constrained method improves the BLEU
score over the baseline system. Furthermore, we
are interested in how the PBC affects the transla-
tion results? We compared the outputs generated
by the baseline and “+PBC (n = 3)” system and
found some interesting translations. For example,
the translations of a source sentence of NIST08
are as follows 4:

• Src: 美b

与m

7 汇率e

1 财长m
8 k 是b

2 抵m
9 关切m

3 中国m

4 访问e

5 k 环保b

6

10 重点e
11

• Ref: US1 Treasury-Secretary2 Arrives-in3
China4 for-a-Visit-with5 Environment6 and7
Exchange-Rate8 as9 Focus10,11

• HPB: US1 Treasury2

in-environmental-
protection6 and7 visit5 China4 is9 key11
to-the-concern-of10 the-exchange-rate8

• +PBC: US1 Treasury2 arrived-in3 China4
for-a-visit5 environmental-protection6 and7
exchange-rate8 is9 concerned-about10 the-
key11

In the example, both “环保” and “汇率” in the
source sentence are the concern of the “visit”.
Therefore, the source span [6, 8] indicates a co-
hesive phrase, which should be translated as a

4The co-indexes of the words on the source and target

sentence indicate word alignments.

whole unit. However, the baseline translates the
spans [6, 7] and [8, 8] separately. It moves [6, 7]
before “visit China” and [8, 8] after “concern”.
This makes an mistake on phrase reordering. We
observe that the “+PBC” system produces a bet-
ter translation. After incorporating the PBC as
a soft constraint, the system assigns a boundary
tag to each source word and segments the source
sentence into three TSs. According to our deﬁ-
nition, TSs are encouraged as pseudo-monotone
translation unit during decoding. As a result, the
“+PBC” system discourages some arbitrary re-
ordering rules and produces more ﬂuent transla-
tion.

7 Conclusion and Future Work

This paper presented a phrase boundary con-
strained method for hierarchical phrase-based
translation. A phrase boundary indicates begin
or end of a phrase reordering. We built a phrase
boundary classiﬁer within a maximum entropy
framework and learned phrase boundary exam-
ples directly from word-aligned bilingual corpus.
We proposed an efﬁcient decoding method to in-
tegrate the PBC into the decoder as a soft con-
straint. Experiments and analysis show that the
phrase boundary constrained method achieves sig-
niﬁcant improvements over the baseline system.

The most advantage of the PBC is that it han-
dles both syntactic and non-syntactic phrases. In
the future, We would like to try different meth-
ods to determine more informative phrase bound-
aries, e.g. Xiong et al. (2010) proposed a method
to learn translation boundaries from a hierarchical
tree that decomposed from word alignments using
a shift-reduce algorithm. In addition, we will try
more features as described in (Chiang et al., 2008;
Chiang et al., 2009), e.g. the length of the phrases
that covered by non-terminals.

References
Cherry, Colin. 2008. Cohesive phrase-based decoding
for statistical machine translation.
In Proceedings
of the 46rd Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies, page 72–80.

Chiang, David, Yuval Marton, and Philip Resnik.

390

2008. Online large-margin training of syntactic and
structural translation features. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, page 224–233.

Och, Franz Josef. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting of the Association for Com-
putational Linguistics, pages 160–167.

Papineni, K., S. Roukos, T. Ward, and W.-J. Zhu.
2002. Bleu: a method for automatic evaluation of
machine translation. In Proceedings of the 40th An-
nual Meeting of the Association for Computational
Linguistics, pages 311–318.

Stolcke, Andreas. 2002. SRILM – An extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken language Process-
ing, volume 2, pages 901–904.

Xiong, Deyi, Min Zhang, Aiti Aw, and Haizhou Li.
2009. A syntax-driven bracketing model for phrase-
based translation.
In ACL-IJCNLP 2009, page
315–323.

Xiong, Deyi, Min Zhang, and Haizhou Li.

2010.
Learning translation boundaries for phrase-based
decoding. In Human Language Technologies: The
2010 Annual Conference of the North American
Chapter of the ACL, page 136–144.

Zhang, Le.

2004. Maximum entropy model-
for python and c++. available at
ing toolkit
http://homepages.inf.ed.ac.uk/s0450736/maxent too-
lkit.html.

Chiang, David, Wei Wang, and Kevin Knight. 2009.
11,001 new features for statistical machine trans-
lation.
In Proceedings of Human Language Tech-
nologies: the 2009 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, page 218–226.

Chiang, David. 2005. A hierarchical phrase-based
model for statistical machine translation.
In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics, pages 263–270.

Chiang, David.

translation.
33(2):201–228.

2007. Hierarchical phrase-based
Computational Linguistics, pages

Gimpel, Kevin and Noah A. Smith.

2008. Rich
source-side context for statistical machine transla-
tion. In In Proceedings of the ACL-2008 Workshop
on Statistical Machine Translation (WMT-2008),
pages 9–17.

Koehn, Philipp, Franz J. Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of HLT-NAACL 2003, pages 127–133.

Koehn, Philipp. 2004. Statistical signiﬁcance tests for
machine translation evaluation.
In Proceedings of
the 2004 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 388–
395.

Marton, Yuval and Philip Resnik. 2008. Soft syntac-
tic constraints for hierarchical phrased-based trans-
lation. In Proceedings of the 46rd Annual Meeting
of the Association for Computational Linguistics:
Human Language Technologies, pages 1003–1011.

Ng, Hweetou and Jinkiat Low. 2004. Chinese part-
of-speech tagging: One-at-a-time or all-at-once?
word-based or character-based? In Proceedings of
the 2004 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP 2004), pages
277–284.

Och, Franz Josef and Hermann Ney.

2002. Dis-
criminative training and maximum entropy models
for statistical machine translation.
In Proceedings
of the 40th Annual Meeting of the Association for
Computational Linguistics, pages 295–302.

Och, Franz Josef and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. 30:417–449.

