



















































Uncover the Ground-Truth Relations in Distant Supervision: A Neural Expectation-Maximization Framework


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 326–336,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

326

Uncover the Ground-Truth Relations in Distant Supervision: A Neural
Expectation-Maximization Framework

Junfan Chen
BDBC and SKLSDE

Beihang University, China
chenjf@act.buaa.edu.cn

Richong Zhang∗
BDBC and SKLSDE

Beihang University, China
zhangrc@act.buaa.edu.cn

Yongyi Mao
School of EECS

University of Ottawa, Canada
ymao@uottawa.ca

Hongyu Guo
Digital Technologies Research Center

National Research Council Canada, Canada
hongyu.guo@nrc-cnrc.gc.ca

Jie Xu
School of Computing

University of Leeds, United Kingdom
j.xu@leeds.ac.uk

Abstract

Distant supervision for relation extraction en-
ables one to effectively acquire structured re-
lations out of very large text corpora with less
human efforts. Nevertheless, most of the prior-
art models for such tasks assume that the given
text can be noisy, but their corresponding la-
bels are clean. Such unrealistic assumption is
contradictory with the fact that the given la-
bels are often noisy as well, thus leading to
significant performance degradation of those
models on real-world data. To cope with this
challenge, we propose a novel label-denoising
framework that combines neural network with
probabilistic modelling, which naturally takes
into account the noisy labels during learning.
We empirically demonstrate that our approach
significantly improves the current art in uncov-
ering the ground-truth relation labels.

1 Introduction

Relation extraction aims at automatically extract-
ing semantic relationships from a piece of text.
Consider the sentence “Larry Page, the chief ex-
ecutive officer of Alphabet, Google’s parent com-
pany, was born in East Lansing, Michigan.”.
The knowledge triple (Larry Page, employed by,
Google) can be extracted. Despite various ef-
forts in building relation extraction models (Ze-
lenko et al., 2002; Zhou et al., 2005; Bunescu
and Mooney, 2005; Zeng et al., 2014; dos Santos
et al., 2015; Yang et al., 2016; Liu et al., 2015; Xu
et al., 2015; Miwa and Bansal, 2016; Gábor et al.,
2018), the difficulty of obtaining abundant training
data with labelled relations remains a challenge,
and thus motivates the development of Distant Su-
pervision relation extraction (Mintz et al., 2009;
Riedel et al., 2010; Zeng et al., 2015; Lin et al.,
2016; Ji et al., 2017; Zeng et al., 2018; Feng et al.,

∗Corresponding author

2018; Wang et al., 2018b,a; Hoffmann et al., 2011;
Surdeanu et al., 2012; Jiang et al., 2016; Ye et al.,
2017; Su et al., 2018; Qu et al., 2018). Distant su-
pervision (DS) relation extract methods collect a
large dataset with “distant” supervision signal and
learn a relation predictor from such data. In detail,
each example in the dataset contains a collection,
or bag, of sentences all involving the same pair
of entities extracted from some corpus (e.g., news
reports). Although such a dataset is expected to
be very noisy, one hopes that when the dataset is
large enough, useful correspondence between the
semantics of a sentence and the relation label it
implies still reinforces and manifests itself. De-
spite their capability of learning from large scale
data, we show in this paper that these DS relation
extraction strategies fail to adequately model the
characteristic of the noise in the data. Specifically,
most of the works fail to recognize that the labels
can be noisy in a bag and directly use bag labels
as training targets.

This aforementioned observation has inspired
us to study a more realistic setting for DS rela-
tion extraction. That is, we treat the bag labels as
the noisy observations of the ground-truth labels.
To that end, we develop a novel framework that
jointly models the semantics representation of the
bag, the latent ground truth labels, and the noisy
observed labels. The framework, probabilistic in
nature, allows any neural network, that encodes
the bag semantics, to nest within. We show that
the well-known Expectation-Maximization (EM)
algorithm can be applied to the end-to-end learn-
ing of the models built with this framework. As
such, we term the framework Neural Expectation-
Maximization, or the nEM.

Since our approach deviates from the conven-
tional models and regards bag labels not as the
ground truth, bags with the real ground-truth la-



327

(a) CSCL (b) NSCL

(c) CSNL (d) NSNL

Figure 1: Assumptions on the cleanness of DS dataset.
In each matrix, rows correspond to the sentences in a
bag, and columns correspond to the labels assigned to
the bag. A check mark on (Si, Lj) indicates that the
label Lj is supported by sentence Si.

bels are required for evaluating our model. To
that end, we manually re-annotate a fraction of
the testing bags in a standard DS dataset with
their real ground-truth labels. We then perform
extensive experiments and demonstrate that the
proposed nEM framework improves the current
state-of-the-art models in uncovering the ground
truth relations. To the best of our knowledge, this
work is the first that combines a neural network
model with EM training under the “noisy-sentence
noisy-label” assumption. The re-annotated testing
dataset 1, containing the ground-truth relation la-
bels, would also benefit the research community.

2 Problem Statement

Let set R contain all relation labels of interest.
Specifically, each label r in R corresponds to a
candidate relation in which any considered pair
(e, e′) of entities may participate. Since R can-
not contain all relations that are implied in a cor-
pus, we include in R an additional relation label
“NA”, which refers to any relation that cannot be
regarded as the other relations inR.

Any subset of R will be written as a {0, 1}-
valued vector of length |R|, for example, z, where
each element of the vector z corresponds to a label
r ∈ R. Specifically, if and only if label r is con-
tained in the subset, its corresponding element z[r]
of z equals 1. If two entities e and e′ participate in
a relation r, we say that the triple (e, r, e′) is fac-

1Will be released upon the acceptance of the paper

tual. Let B be a finite set, in which each b ∈ B is a
pair (e, e′) of entities. Each b = (e, e′) ∈ B serves
as the index for a bag xb of sentences.

The objective of DS is to use a large but noisy
training set to learn a predictor that predicts the re-
lations involving two arbitrary (possibly unseen)
entities; the predictor takes as input a bag of sen-
tences each containing the two entities of inter-
est, and hopefully outputs the set of all relations
in which the two entities participate.

3 Prior Art and Related Works

Relation extraction is an important task in natural
language processing. Many approaches with su-
pervised methods have been proposed to complete
this task. These works, such as (Zelenko et al.,
2002; Zhou et al., 2005; Bunescu and Mooney,
2005), although achieving good performance, rely
on carefully selected features and well labelled
dataset. Recently, neural network models, have
been used in (Zeng et al., 2014; dos Santos et al.,
2015; Yang et al., 2016; Liu et al., 2015; Xu et al.,
2015; Miwa and Bansal, 2016) for supervised re-
lation extraction. These models avoid feature en-
gineering and are shown to improve upon previous
models. But having a large number of parameters
to estimate, these models rely heavily on costly
human-labeled data.

Distant supervision was proposed in (Mintz
et al., 2009) to automatically generate large dataset
through aligning the given knowledge base to text
corpus. However, such dataset can be quite noisy.
To articulate the nature of noise in DS dataset, a
sentence is said to be noisy if it supports no rela-
tion labels of the bag, and a label of the bag is said
to be noisy if it is not supported by any sentence in
the bag. A sentence or label that is not noisy will
be called clean. The cleanness of a training exam-
ple may obey the following four assumptions, for
each of which an example is given In Figure 1.

• Clean-Sentence Clean-Label (CSCL): All
sentences and all labels are clean (Figure 1(a)).
• Noisy-Sentence Clean-Label (NSCL): Some

sentences may be noisy but all labels are clean
(Figure 1(b)). Note that CSCL is a special case
of NSCL.
• Clean-Sentence Noisy-Label (CSNL): All

sentences are clean but some labels may be
noisy(Figure 1(c)). Note that CSNL includes
CSCL as a special case.



328

• Noisy-Sentence Noisy-Label (NSNL): Some
sentences may be noisy and some labels may
also be noisy (Figure 1(d)).

Obviously, CSCL, NSCL, CSNL are all special
cases of NSNL. Thus NSNL is the most general
among all these assumptions.

The author of (Mintz et al., 2009) creates a
model under the CSCL assumption, which is how-
ever pointed out a too strong assumption (Riedel
et al., 2010). To alleviate this issue, many stud-
ies adopt NSCL assumption. Some of them, in-
cluding (Riedel et al., 2010; Zeng et al., 2015;
Lin et al., 2016; Ji et al., 2017; Zeng et al., 2018;
Feng et al., 2018; Wang et al., 2018b,a), formu-
late the task as a multi-instance learning prob-
lem where only one label is allowed for each
bag. These works take sentence denoising through
selecting the max-scored sentence (Riedel et al.,
2010; Zeng et al., 2015, 2018), applying sentence
selection with soft attention (Lin et al., 2016; Ji
et al., 2017), performing sentence level prediction
as well as filtering noisy bags (Feng et al., 2018)
and redistributing the noisy sentences into nega-
tive bags (Wang et al., 2018b,a). Other studies
complete this task with multi-instance multi-label
learning (Hoffmann et al., 2011; Surdeanu et al.,
2012; Jiang et al., 2016; Ye et al., 2017; Su et al.,
2018), which allow overlapping relations in a bag.
Despite the demonstrated successes, these mod-
els ignore the fact that relation labels can be noisy
and “noisy” sentences that indeed point to factual
relations may also be ignored. Two recent ap-
proaches (Liu et al., 2017; Luo et al., 2017) using
the NSNL assumption have also been introduced,
but these methods are evaluated based on the as-
sumption that the evaluation labels are clean.

We note that this paper is not the first work
that combines neural networks with EM train-
ing. Very recently, a model also known as Neural
Expectation-Maximization or NEM (Greff et al.,
2017) has been presented to learn latent represen-
tations for clusters of objects (e.g., images) un-
der a complete unsupervised setting. The NEM
model is not directly applicable to our problem
setting which deals with noisy supervision sig-
nals from categorical relation labels. Nonethe-
less, given the existence of the acronym NEM,
we choose to abbreviate our Neural Expecation-
Maximization model as the nEM model.

4 The nEM Framework

We first introduce the nEM architecture and its
learning strategy, and then present approaches to
encode a bag of sentences (i.e., the Bag Encoding
Models) needed by the framework.

4.1 The nEM Architecture

Let random variable X denote a random bag and
random variable Z denote the label set assigned to
X . Under the NSNL assumption, Z, or some la-
bels within, may not be clean for X . We introduce
another latent random variable Y , taking values as
a subset of R, indicating the set of ground-truth
(namely, clean) labels for X . We will write Y
again as an |R|-dimensional {0, 1}-valued vector.
From here on, we will adopt the convention that a
random variable will be written using a capitalized
letter, and the value it takes will be written using
the corresponding lower-cased letter.

A key modelling assumption in nEM is that ran-
dom variables X , Y and Z form a Markov chain
X → Y → Z. Specifically, the dependency of
noisy labels Z on the bag X is modelled as

pZ|X(z|x) :=
∑

y∈{0,1}|R|
pY |X(y|x)pZ|Y (z|y) (1)

The conditional distribution pZ|Y is modelled as

pZ|Y (z|y) :=
∏
r∈R

pZ[r]|Y [r] (z[r]|y[r]) (2)

That is, for each ground-truth label r ∈ R, Z[r]
depends only on Y [r]. Furthermore, we assume
that for each r, there are two parameters φ0r and
φ1r governing the dependency of Z[r] on Y [r] via

pZ[r]|Y [r](z[r]|y[r]):=


φ0r y[r]=0,z[r]=1
1−φ0r y[r]=0,z[r]=0
φ1r y[r]=1,z[r]=0
1−φ1r y[r]=1,z[r]=1

(3)

We will denote by {φ0r , φ1r : r ∈ R} collectively
by φ. .

On the other hand, we model pY |X by

pY |X(y|x) :=
∏
r∈R

pY [r]|X (y[r]|x) (4)

where x is the encoding of bag x, implemented
using any suitable neural network. Postponing
explaining the details of bag encoding to a later



329

section (namely Section 4.3), we here specify the
form of pY [r]|X (y[r]|x) for each r:

pY [r]|X (1|x) = σ
(
rTx+ br

)
(5)

where σ(·) is the sigmoid function, r is a |R|-
dimensional vector and br is bias. That is, pY [r]|X
is essentially a logistic regression (binary) classi-
fier based on the encoding x of bag x. We will
denote by θ the set of all parameters {r : r ∈ R}
and the parameters for generating encoding x.

At this end, we have defined the overall struc-
ture of the nEM framework (summarized in Figure
2). Next, we will discuss the learning of it.

Figure 2: The probabilistic framework of nEM

4.2 Learning with the EM Algorithm
Let `(z|x;φ, θ) be the log-likelihood of observing
the label set z given the bag x, that is,

`(z|x;φ, θ) := log pZ|X(z|x). (6)

The structure of the framework readily enables a
principled learning algorithm based on the EM al-
gorithm (Dempster et al., 1977).

Let `total(φ, θ) be the log-likelihood defined as

`total(φ, θ) :=
∑
b∈B

`(zb|xb;φ, θ). (7)

The learning problem can then be formulated as
maximizing this objective function over its param-
eters (φ, θ), or solving

(φ̂, θ̂) := arg max
φ,θ

`total(φ, θ). (8)

Let Q be an arbitrary distribution over {0, 1}|R|.
Then it is possible to show

`(z|x;φ, θ)

=log
∑

y∈{0,1}|R|
Q(y)

∏
r∈R

pY [r]|X(y[r]|x)pZ[r]|Y [r](z[r]|y[r])

Q(y)

>
∑

y∈{0,1}|R|
Q(y) log

∏
r∈R

pY [r]|X(y[r]|x)pZ[r]|Y [r](z[r]|y[r])

Q(y)

= L(z|x;φ, θ,Q)
(9)

where the lower bound L(z|x;φ, θ,Q), often re-
ferred to as the variational lower bound. Now we
define Ltotal as

Ltotal(φ,θ,{Qb :b∈B}):=
∑
b∈B
L (zb|xb;φ,θ,Qb) (10)

where we have introduced aQb for each b ∈ B. In-
stead of solving the original optimization problem
(8), we can turn to solving a different optimization
problem by maximizing Ltotal

(φ̂, θ̂, {Q̂b}) :=arg max
φ,θ,{Qb}

Ltotal(φ, θ, {Qb}) (11)

The EM algorithm for solving the optimization
problem (11) is essentially the coordinate ascent
algorithm on objective function Ltotal, where we
iterate over two steps, the E-Step and the M-Step.
In the E-Step, we maximizes Ltotal over {Qb} for
the current setting of (φ, θ) and in the M-Step, we
maximize Ltotal (φ, θ) for the current setting of
{Qb}. We now describe the two steps in detail,
where we will use superscript t to denote the iter-
ation number.
E-step: In this step, we hold (φ, θ) fixed and up-
date {Qb : b ∈ B} to maximize the lower bound
Ltotal(φ, θ, {Qb}). This boils down to update each
factor Qb,r of Qb according to:

Qt+1b,r (y[r]) :=pY [r]|X,Z(yb[r]|xb, zb[r]; θ
t, φtr)

=
pZ|Y (zb[r]|yb[r];φtr)pY |X(yb[r]|xb; θt)

pZ|X(zb[r]|xb; θt, φtr)

=
pZ|Y (zb[r]|yb[r];φtr)pY |X(yb[r]|xb; θt)∑

y[r]∈{0,1}
pZ|Y (zb[r]|y[r];φtr)pY |X(y[r]|xb; θt)

(12)

M-step: In this step, we hold {Qb} fixed and
update (φ, θ), to maximize the lower bound
Ltotal(φ, θ, {Qb}). Let

M =
∏
r∈R

pY [r]|X(yb[r]|xb)pZ[r]|Y [r](zb[r]|yb[r]), (13)

then (φ, θ) is updated according to:

(θt+1, φt+1) := argmax
θ,φ
Ltotal(θ, φ, {Qt+1b })

=argmax
θ,φ

∑
b∈B

∑
yb∈{0,1}|R|

Qt+1b (y)log
M

Qt+1b (y)

=argmax
θ,φ

∑
b∈B

∑
yb∈{0,1}|R|

Qt+1b (y)logM−
∑
b∈B

∑
yb∈{0,1}|R|

Qtb(y)logQ
t+1
b (y)

=argmax
θ,φ

∑
b∈B

∑
r∈R

∑
yb[r]∈{0,1}

Qt+1b,r (y[r])
(
log pZ|Y (zb[r]|yb[r];φr)

+ log pY |X(yb[r]|xb; θ)
)

(14)



330

Overall the EM algorithm starts with initializing
(φ, θ, {Qb}), and then iterates over the two steps
until convergence or after some prescribed number
of iterations is reached. There are however several
caveats on which caution is needed. First, the opti-
mization problem in the M-Step cannot be solved
in closed form. As such we take a stochastic gra-
dient descent (SGD) approach2.

In each M-Step, we perform ∆ updates, where
∆ is a hyper-parameter. Such an approach is
sometimes referred to as “Generalized EM” (Wu,
1983; Jojic and Frey, 2001; Greff et al., 2017).
Note that since the parameters θ are parameters
for the neural network performing bag encoding,
the objective function Ltotal is highly non-convex
with respect to θ. This makes it desirable to choose
an appropriate ∆. Too small ∆ results in little
change in θ and hence provides insufficient sig-
nal to update {Qb}; too large ∆, particularly at
the early iterations when {Qb} has not been suf-
ficiently optimized, tends to make the optimiza-
tion stuck in undesired local optimum. In practice,
one can try several values of ∆ by inspecting their
achieved value of Ltotal and select the ∆ giving
rise to the highest Ltotal. Note that such a tuning
of ∆ requires no access of the testing data. The
second issue is that the EM algorithm is known to
be sensitive to initialization. In our implementa-
tion, in order to provide a good initial parameter
setting, we set each Q0b to zb. Despite the fact that
z contains noise, this is a much better approxima-
tion of the posterior of true labels than any random
initialization.

Figure 3: The structure of Bag Encoding component.

The nEM framework needs the encoding of a
bag of x as discussed in Equation 4. Any suit-
able neural network can be deployed to achieve
this goal. Next, we present the widely used meth-
ods for DS relation extraction strategies: the Bag
Encoding Models.

2In fact, the approach should be called “stochastic gradi-
ent ascent”, since we are maximizing the objective function
not minimizing it.

4.3 Bag Encoding Models

As illustrated in Figure 3, the Bag Encoding Mod-
els include three components: Word-Position Em-
bedding, Sentence Encoding, and Sentence Selec-
tors.

4.3.1 Word-Position Embedding
For the jth word in a sentence, Word-Position Em-
bedding generates a vector representation wj as

concatenated three components
[
wwj ,w

p1
j ,w

p2
j

]
.

Specifically, wwj is the word embedding of the
word and wp1j and w

p2
j are two position embed-

dings. Here wp1j (resp. w
p2
j ) are the embedding

of the relative location of the word with respect to
the first (resp. second) entity in the sentence. The
dimensions of word and position embeddings are
denoted by dw and dp respectively.

4.3.2 Sentence Encoding
Sentence encoding uses Piecewise Convolutional
Neural Networks (PCNN) (Zeng et al., 2015;
Lin et al., 2016; Ye et al., 2017; Ji et al.,
2017), which consists of convolution followed
by Piecewise Max-pooling. The convolution op-
eration uses a list of matrix kernels such as
{K1,K2, · · · ,Klker} to extract n-gram features
through sliding windows of length lwin. Here
Ki ∈ Rlwin×de and lker is the number of ker-
nels. Let wj−lwin+1:j ∈ Rlwin×de be the concate-
nated vector of token embeddings in the jth win-
dow. The output of convolution operation is a ma-
trix U ∈ Rlker×(m+lwin−1) where each element is
computed by

Uij = Ki �wj−lwin+1:j + bi (15)

where � denotes inner product. In Piecewise
Max-Pooling, Ui is segmented to three parts{
U1i ,U

2
i ,U

3
i

}
depending on whether an element

in Ui is on the left or right of the two entities, or
between the two entities. Then max-pooling is ap-
plied to each segment, giving rise to

gip = max(Upi ), 1 6 i 6 lker, 1 6 p 6 3 (16)

Let g =
[
g1,g2,g3

]
. Then sentence encoding

outputs
v = ReLU(g) ◦ h, (17)

where ◦ is element-wise multiplication and h is a
vector of Bernoulli random variables, representing
dropouts.



331

4.3.3 Sentence Selectors
Let n be the number of sentences in a bag. We
denote a matrix V ∈ Rn×(lker×3)consisting of
each sentence vector vT . Vk: and V:j are used to
index the kth row vector and jth column vector
of V respectively. Three kinds of sentence-
selectors are used to construct the bag encoding.
Mean-selector (Lin et al., 2016; Ye et al., 2017):
The bag encoding is computed as

x =
1

n

n∑
k=1

Vk: (18)

Max-selector (Jiang et al., 2016): The jth element
of bag encoding x is computed as

xj = max(V:j) (19)

Attention-selector Attention mechanism is exten-
sively used for sentence selection in relation ex-
traction by weighted summing of the sentence vec-
tors, such as in (Lin et al., 2016; Ye et al., 2017; Su
et al., 2018). However, all these works assume that
the labels are correct and only use the golden label
embeddings to select sentences at training stage.
We instead selecting sentences using all label em-
beddings r and construct a bag encoding for each
label r ∈ R. The output is then a list of vectors
{xr}, in which the rth vector is calculated through
attention mechanism as

el=V
T
l:Ar, αk=

exp(ek)∑n
l=1exp(el)

, xr=
n∑
k=1

αkVk: (20)

where A ∈ Rdr×dr is a diagonal matrix and dr is
the dimension of relation embedding.

5 Experimental Study

We first conduct experiments on the widely used
benchmark data set Riedel (Riedel et al., 2010),
and then on the TARCED (Zhang et al., 2017) data
set. The latter allows us to control the noise level
in the labels to observe the behavior and working
mechanism of our proposed method. The code for
our model is found on the Github page 3.

5.1 Evaluation on the Riedel Dataset
The Riedel dataset2 is a widely used DS dataset for
relation extraction. It was developed in (Riedel
et al., 2010) through aligning entity pairs from

3https://github.com/AlbertChen1991/nEM
2http://iesl.cs.umass.edu/riedel/ecml/

Freebase with the New York Times (NYT) cor-
pus. There are 53 relations in the Riedel dataset,
including “NA”. The bags collected from the 2005-
2006 corpus are used as the training set, and the
bags from the 2007 corpus are used as the test set.
The training data contains 281, 270 entity pairs
and 522, 611 sentences; the testing data contains
96, 678 entity pairs and 172, 448 sentences.

5.1.1 Ground-Truth Annotation
The Riedel dataset contains no ground-truth la-
bels. The held-out evaluation (Mintz et al., 2009)
is highly unreliable in measuring a model’s perfor-
mance against the ground truth. Since this study
is concerned with discovering the ground-truth la-
bels, the conventional held-out evaluation is no
longer appropriate. For this reason, we annotate a
subset of the original test data for evaluation pur-
pose. Specifically, we annotate a bag xb by its all
correct labels, and if no such labels exist, we label
the bag NA.

In total 3762 bags are annotated, which in-
clude all bags originally labelled as non-NA (”Part
1”) and 2000 bags (”Part 2”) selected from the
bags originally labelled as NA but have rela-
tively low score for NA label under a pre-trained
PCNN+ATT (Lin et al., 2016) model. The ratio-
nale here is that we are interested in inspecting
the model’s performance in detecting the labels of
known relations rather than NA relation.

Table 1: The statistics of ground-truth annotation. ori-
gin denotes the total number of originally assigned la-
bels of these bags. correct and wrong denote the total
number of correctly assigned and wrongly assigned la-
bels. added denote the number of missing labels we
added into these bags.

bags origin correct wrong added
Part 1 1762 1953 1245 708 804
Part 2 2000 2000 938 1062 1211

The statistics of annotation is shown in Table
1. Through the annotation, we notice that, about
36% of the original labels in original non-NA bags
and 53% of labels in original NA bags are wrongly
assigned. Similar statistics has been reported in
previous works (Riedel et al., 2010; Feng et al.,
2018).

5.1.2 Evaluation Metrics and Baselines
For the Riedel dataset, we train the models on the
noisy training set and test the models on the man-
ually labeled test set. The precision-recall (PR)

https://github.com/AlbertChen1991/nEM
http://iesl.cs.umass.edu/riedel/ecml/


332

curve is reported to compare performance of mod-
els. Three baselines are considered:

• PCNN+MEAN (Lin et al., 2016; Ye et al.,
2017): A model using PCNN to encode sen-
tences and a mean-selector to generate bag en-
coding.
• PCNN+MAX (Jiang et al., 2016): A model

using PCNN to encode sentences and a max-
selector to generate bag encoding.
• PCNN+ATT (Lin et al., 2016; Ye et al., 2017;

Su et al., 2018): A model using PCNN to en-
code sentences and an attention-selector to gen-
erate bag encoding.

We compare the three baselines with their nEM
versions (namely using them as the Bag Encod-
ing component in nEM), which are denoted with a
“+nEM” suffix.

5.1.3 Implementation Detail
Following previous work, we tune our models us-
ing three-fold validation on the training set. As
the best configurations, we set dw = 50, dp = 5
and dr = 230. For PCNN, we set lwin = 3,
lker = 230 and set the probability of dropout to
0.5. We use Adadelta (Zeiler, 2012) with default
setting (ρ = 0.95, ε = 1e−6) to optimize the mod-
els and the initial learning rate is set as 1. The
batch size is fixed to 160 and the max length of a
sentence is set as 120.

For the noise model pZ|Y , we set φ0NA =
0.3, φ1NA = 0 for the NA label and φ

0
r = 0, φ

1
r =

0.9 for other labels r 6= NA. In addition, the num-
ber ∆ of SGD updates in M-step is set to 2000.

5.1.4 Predictive performance
The evaluation results on manually labeled test set
are shown in Figure 4. From which, we observe
that the PR-curves of the nEM models are above
their corresponding baselines by significant mar-
gins, especially in the low-recall regime. This
observation demonstrates the effectiveness of the
nEM model on improving the extracting perfor-
mance upon the baseline models. We also observe
that models with attention-selector achieve better
performance than models with mean-selector and
max-selector, demonstrating the superiority of the
attention mechanism on sentence selection.

We then analyze the predicting probability
of PCNN+ATT and PCNN+ATT+nEM on the
ground-truth labels in the test set. We divide
the predicting probability values into 5 bins and

0 0.1 0.2 0.3 0.4 0.5
0.6

0.65

0.7

0.75

0.8

0.85

0.9

0.95

1

Recall

Pr
ec
is
io
n

PCNN+ATT
PCNN+ATT+nEM
PCNN+MAX
PCNN+MAX+nEM
PCNN+MEAN
PCNN+MEAN+nEM

Figure 4: PR curves on DS test set.

count the number of label within each bin. The
result is shown in Figure 5(a). We observe
that the count for PCNN+ATT in bins 0.0 − 0.2,
0.2 − 0.4, 0.4 − 0.6 and 0.6 − 0.8 are all greater
than PCNN+ATT+nEM. But in bin 0.8 − 1.0, the
count for PCNN+ATT+nEM is about 55% larger
than PCNN+ATT. This observation indicates that
nEM can promote the overall predicting scores of
ground-truth labels.

Figure 5(b) compares PCNN+ATT and
PCNN+ATT+nEM in their predictive probabilities
on the frequent relations. The result shows that
PCNN+ATT+nEM achieves higher average pre-
dicting probability on all these relations, except
for the NA relation, on which PCNN+ATT+nEM
nearly levels with PCNN+ATT. This phenomenon
demonstrates that nEM tends to be more confident
in predicting the correct labels. In this case, rais-
ing the predictive probability on the correct label
does increase the models ability to make the cor-
rect decision, thereby improving performance.

0−0.2 0.2−0.4 0.4−0.6 0.6−0.8 0.8−1.0
0

200

400

600

800

1000

1200

1400

1600

1800

Score Range

C
ou

nt
s

PCNN+ATT
PCNN+ATT+nEM

NA Con Nat Com PoL PoD PoB
0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

Relation

A
ve

ra
ge

 S
co

re

PCNN+ATT
PCNN+ATT+nEM

(a) Score distribution (b) Scores on main relations

Figure 5: Predictive probabilities on the test set. CON,
Nat, Com, PoL, PoD and PoB represent ’contains’, ’na-
tionality’, ’company’, ’place lived’, ’place of death’
and ’place of birth’ relations respectively.

5.2 Evaluation on the TACRED Dataset
TACRED is a large supervised relation extraction
dataset collected in (Zhang et al., 2017). The text
corpus is generated from the TAC KBP evalua-
tion of years 2009 − 2015. Corpus from years



333

2009 − 2012, year 2013 and year 2014 are used
as training, validation and testing sets respectively.
In all annotated sentences, 68, 124 are used for
training, 22, 631 for validation and 15, 509 for
testing. There are in total 42, 394 distinct entity
pairs, where 33, 079 of these entity pairs appear in
exactly one sentence. Since these 33, 079 entity
pairs dominate the training set, we simply treat
each sentence as a bag for training and testing.
Another advantage of constructing single-sentence
bags is that it allows us to pinpoint the correspon-
dence between the correct label and its supporting
sentence. The number of relations in TACRED
dataset is 42, including a special relation “no re-
lation”, which is treated as NA in this study.

5.2.1 Constructing Semi-synthetic Dataset
To obtain insight into the working of nEM, we cre-
ate a simulated DS dataset by inserting noisy la-
bels into the training set of a supervised dataset,
TACRED. Since the training set of TACRED was
manually annotated, the labels therein may be re-
garded as the ground-truth labels. Training using
this semi-synthetic dataset allows us to easily ob-
serve models’ behaviour with respect to the noisy
labels and the true labels.

We inject artificial noise into the TACRED
training data through the following precedure. For
each bag xb in the training set, we generate a noisy
label vector z̃b from the observed ground-truth la-
bel vector yb. Specifically, z̃b is generated by flip-
ping each element yb[r] from 0 to 1 or 1 to 0
with a probability pf . This precedure simulates a
DS dataset through introducing wrong labels into
training bags, thus corrupts the training dataset.

5.2.2 Experimental Settings
Following (Zhang et al., 2017), the common rela-
tion classification metrics Precision, Recall and F1
are used for evaluation. The PCNN model is used
to generate the bag encoding since sentence selec-
tion is not needed in this setting. The same hyper-
parameter settings as in Section 5.1.3 are used in
this experiment. For the noise model pZ[r]|Y [r] of
PCNN +nEM, we set φ0r = 0.1, φ

1
r = 0.1 for each

label r ∈ R. The number ∆ of SGD updates in
each M-step is set to 1600.

5.2.3 Test Results
From Table 2, we see that PCNN+nEM achieves
better recall and F1 score than the PCNN model
under various noise levels. Additionally, the recall

and F1 margins between PCNN and PCNN+nEM
increase with noise levels. This suggests that
nEM keeps better performance than the corre-
sponding baseline model under various level of
training noise. We also observe that the precision
of nEM is consistently lower than that of PCNN
when noise is injected to TACRED. This is a nec-
essary trade-off present in nEM. The training of
nEM regards the training labels with less confi-
dence based on noisy-label assumption. This ef-
fectively lowers the probability of seen training la-
bels and considers the unseen labels also having
some probability to occur. When trained this way,
nEM, at the prediction time, tends to predict more
labels than its baseline (PCNN) does. Note that
in TACRED, each instance contains only a single
ground truth label. Thus the tendency of nEM to
predict more labels leads to the reduced precision.
However, despite this tendency, nEM, comparing
with PCNN, has a stronger capability in detecting
the correct label and gives the better recall of nEM.
The gain in recall dominates the loss in precision.

Table 2: Test performance on the TACRED dataset.

pf
PCNN PCNN+nEM

P R F1 P R F1
0.02 65.2 30.0 41.1 61.8 34.8 44.5
0.04 63.5 28.9 39.7 60.2 33.8 43.3
0.06 70.5 25.2 37.1 59.9 31.1 41.0
0.08 63.5 23.6 34.4 58.8 29.8 39.6
0.10 66.1 22.7 33.9 56.5 28.9 38.3

5.2.4 Training Label Probabilities
The predicting probabilities for the noise labels
and the original true labels are also evaluated un-
der the trained models. The results are shown
in Figure 6 (left) which reveals that with in-
creasing noise, the average probability for noisy
label sets of PCNN and PCNN+nEM both in-
crease and average scores for original label sets of
PCNN and PCNN+nEM both decrease. The per-
formance degradation of PCNN and PCNN+nEM
under noise appears different. The average proba-
bility for noisy label sets rises with a higher slope
in PCNN than in PCNN+nEM. Additionally, the
average probability for the original label sets of
PCNN+nEM is higher than or equal to PCNN at
all noise levels. These observations confirms that
the denoising capability of nEM is learned from
effectively denoising the training set.



334

0.62

0.64

0.66

O
rig

in
al

 la
be

l a
ve

ra
ge

 sc
or

e

0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1
0

0.1

0.2

N
oi

sy
 la

be
l a

ve
ra

ge
 sc

or
e

pf

PCNN Original
PCNN+nEM Original
PCNN Noisy
PCNN+nEM Noisy

0 2 4 6 8 10
0.03

0.035

0.04

0.045

0.05

0.055

0.06

0.065

0.07

0.075

EM Iterations

A
ve

ra
ge

 o
f Q

b,
r(y

[r]
)

Figure 6: Average probabilities for noisy and original
labels (left) and average Qb,r(y[r]) over EM iterations
(right)

5.2.5 Effectiveness of EM Iterations
For each training bag xb and each artificial noisy
label r, we track the probability Qb,r(y[r]) = 1
over EM iterations. This probability, measuring
the likelihood of the noise label r being correct, is
then averaged over r and over all bags xb. It can
be seen in Figure 6 (right) that the average value
of Qb,r(y[r]) decreases as the training progresses,
leading the model to gradually ignore noisy labels.
This demonstrates the effectiveness of EM itera-
tions and validates the proposed EM-based frame-
work.

6 Concluding Remarks

We proposed a nEM framework to deal with the
noisy-label problem in distance supervision rela-
tion extraction. We empirically demonstrated the
effectiveness of the nEM framework, and provided
insights on its working and behaviours through
data with controllable noise levels. Our frame-
work is a combination of latent variable models
in probabilistic modelling with contemporary deep
neural networks. Consequently, it naturally sup-
ports a training algorithm which elegantly nests
the SGD training of any appropriate neural net-
work inside an EM algorithm. We hope that
our approach and the annotated clean testing data
would inspire further research along this direc-
tion.

Acknowledgment

This work is supported partly by China 973 pro-
gram (No. 2015CB358700), by the National Nat-
ural Science Foundation of China (No. 61772059,
61421003), by the Beijing Advanced Innova-
tion Center for Big Data and Brain Computing
(BDBC), by State Key Laboratory of Software De-
velopment Environment (No. SKLSDE-2018ZX-
17) and by the Fundamental Research Funds for
the Central Universities.

References

Razvan C. Bunescu and Raymond J. Mooney. 2005.
Subsequence kernels for relation extraction. In
Advances in Neural Information Processing Sys-
tems 18 [Neural Information Processing Systems,
NIPS 2005, December 5-8, 2005, Vancouver, British
Columbia, Canada], pages 171–178.

Arthur P Dempster, Nan M Laird, and Donald B Rubin.
1977. Maximum likelihood from incomplete data
via the em algorithm. Journal of the royal statistical
society. Series B (methodological), pages 1–38.

Jun Feng, Minlie Huang, Li Zhao, Yang Yang, and Xi-
aoyan Zhu. 2018. Reinforcement learning for re-
lation classification from noisy data. In Proceed-
ings of the Thirty-Second AAAI Conference on Ar-
tificial Intelligence, (AAAI-18), the 30th innovative
Applications of Artificial Intelligence (IAAI-18), and
the 8th AAAI Symposium on Educational Advances
in Artificial Intelligence (EAAI-18), New Orleans,
Louisiana, USA, February 2-7, 2018, pages 5779–
5786.

Kata Gábor, Davide Buscaldi, Anne-Kathrin Schu-
mann, Behrang QasemiZadeh, Haı̈fa Zargayouna,
and Thierry Charnois. 2018. Semeval-2018 task
7: Semantic relation extraction and classification in
scientific papers. In SemEval@NAACL-HLT, pages
679–688. Association for Computational Linguis-
tics.

Klaus Greff, Sjoerd van Steenkiste, and Jürgen
Schmidhuber. 2017. Neural expectation maximiza-
tion. In Advances in Neural Information Processing
Systems 30: Annual Conference on Neural Informa-
tion Processing Systems 2017, 4-9 December 2017,
Long Beach, CA, USA, pages 6694–6704.

Raphael Hoffmann, Congle Zhang, Xiao Ling,
Luke S. Zettlemoyer, and Daniel S. Weld. 2011.
Knowledge-based weak supervision for information
extraction of overlapping relations. In The 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, Pro-
ceedings of the Conference, 19-24 June, 2011, Port-
land, Oregon, USA, pages 541–550.

Guoliang Ji, Kang Liu, Shizhu He, and Jun Zhao.
2017. Distant supervision for relation extraction
with sentence-level attention and entity descriptions.
In Proceedings of the Thirty-First AAAI Conference
on Artificial Intelligence, February 4-9, 2017, San
Francisco, California, USA., pages 3060–3066.

Xiaotian Jiang, Quan Wang, Peng Li, and Bin Wang.
2016. Relation extraction with multi-instance multi-
label convolutional neural networks. In COLING
2016, 26th International Conference on Computa-
tional Linguistics, Proceedings of the Conference:
Technical Papers, December 11-16, 2016, Osaka,
Japan, pages 1471–1480.



335

Nebojsa Jojic and Brendan J. Frey. 2001. Learning
flexible sprites in video layers. In 2001 IEEE Com-
puter Society Conference on Computer Vision and
Pattern Recognition (CVPR 2001), with CD-ROM,
8-14 December 2001, Kauai, HI, USA, pages 199–
206.

Yankai Lin, Shiqi Shen, Zhiyuan Liu, Huanbo Luan,
and Maosong Sun. 2016. Neural relation extraction
with selective attention over instances. In Proceed-
ings of the 54th Annual Meeting of the Association
for Computational Linguistics, ACL 2016, August 7-
12, 2016, Berlin, Germany, Volume 1: Long Papers.

Tianyu Liu, Kexiang Wang, Baobao Chang, and Zhi-
fang Sui. 2017. A soft-label method for noise-
tolerant distantly supervised relation extraction. In
Proceedings of the 2017 Conference on Empirical
Methods in Natural Language Processing, EMNLP
2017, Copenhagen, Denmark, September 9-11,
2017, pages 1790–1795.

Yang Liu, Furu Wei, Sujian Li, Heng Ji, Ming Zhou,
and Houfeng Wang. 2015. A dependency-based
neural network for relation classification. In Pro-
ceedings of the 53rd Annual Meeting of the Associ-
ation for Computational Linguistics and the 7th In-
ternational Joint Conference on Natural Language
Processing of the Asian Federation of Natural Lan-
guage Processing, ACL 2015, July 26-31, 2015, Bei-
jing, China, Volume 2: Short Papers, pages 285–
290.

Bingfeng Luo, Yansong Feng, Zheng Wang, Zhanxing
Zhu, Songfang Huang, Rui Yan, and Dongyan Zhao.
2017. Learning with noise: Enhance distantly su-
pervised relation extraction with dynamic transition
matrix. In Proceedings of the 55th Annual Meet-
ing of the Association for Computational Linguis-
tics, ACL 2017, Vancouver, Canada, July 30 - August
4, Volume 1: Long Papers, pages 430–439.

Mike Mintz, Steven Bills, Rion Snow, and Daniel Ju-
rafsky. 2009. Distant supervision for relation extrac-
tion without labeled data. In ACL 2009, Proceedings
of the 47th Annual Meeting of the Association for
Computational Linguistics and the 4th International
Joint Conference on Natural Language Processing
of the AFNLP, 2-7 August 2009, Singapore, pages
1003–1011.

Makoto Miwa and Mohit Bansal. 2016. End-to-end re-
lation extraction using lstms on sequences and tree
structures. In Proceedings of the 54th Annual Meet-
ing of the Association for Computational Linguis-
tics, ACL 2016, August 7-12, 2016, Berlin, Ger-
many, Volume 1: Long Papers.

Meng Qu, Xiang Ren, Yu Zhang, and Jiawei Han.
2018. Weakly-supervised relation extraction by
pattern-enhanced embedding learning. In Proceed-
ings of the 2018 World Wide Web Conference on
World Wide Web, WWW 2018, Lyon, France, April
23-27, 2018, pages 1257–1266.

Sebastian Riedel, Limin Yao, and Andrew McCal-
lum. 2010. Modeling relations and their men-
tions without labeled text. In Machine Learning
and Knowledge Discovery in Databases, European
Conference, ECML PKDD 2010, Barcelona, Spain,
September 20-24, 2010, Proceedings, Part III, pages
148–163.

Cı́cero Nogueira dos Santos, Bing Xiang, and Bowen
Zhou. 2015. Classifying relations by ranking with
convolutional neural networks. In Proceedings of
the 53rd Annual Meeting of the Association for
Computational Linguistics and the 7th International
Joint Conference on Natural Language Process-
ing of the Asian Federation of Natural Language
Processing, ACL 2015, July 26-31, 2015, Beijing,
China, Volume 1: Long Papers, pages 626–634.

Sen Su, Ningning Jia, Xiang Cheng, Shuguang Zhu,
and Ruiping Li. 2018. Exploring encoder-decoder
model for distant supervised relation extraction.
In Proceedings of the Twenty-Seventh International
Joint Conference on Artificial Intelligence, IJCAI
2018, July 13-19, 2018, Stockholm, Sweden., pages
4389–4395.

Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,
and Christopher D. Manning. 2012. Multi-instance
multi-label learning for relation extraction. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, EMNLP-
CoNLL 2012, July 12-14, 2012, Jeju Island, Korea,
pages 455–465.

William Yang Wang, Weiran Xu, and Pengda Qin.
2018a. DSGAN: generative adversarial training for
distant supervision relation extraction. In Proceed-
ings of the 56th Annual Meeting of the Associa-
tion for Computational Linguistics, ACL 2018, Mel-
bourne, Australia, July 15-20, 2018, Volume 1: Long
Papers, pages 496–505.

William Yang Wang, Weiran Xu, and Pengda Qin.
2018b. Robust distant supervision relation extrac-
tion via deep reinforcement learning. In Proceed-
ings of the 56th Annual Meeting of the Associa-
tion for Computational Linguistics, ACL 2018, Mel-
bourne, Australia, July 15-20, 2018, Volume 1: Long
Papers, pages 2137–2147.

CF Jeff Wu. 1983. On the convergence properties of
the em algorithm. The Annals of statistics, pages
95–103.

Yan Xu, Lili Mou, Ge Li, Yunchuan Chen, Hao Peng,
and Zhi Jin. 2015. Classifying relations via long
short term memory networks along shortest depen-
dency paths. In Proceedings of the 2015 Conference
on Empirical Methods in Natural Language Pro-
cessing, EMNLP 2015, Lisbon, Portugal, September
17-21, 2015, pages 1785–1794.

Yunlun Yang, Yunhai Tong, Shulei Ma, and Zhi-Hong
Deng. 2016. A position encoding convolutional



336

neural network based on dependency tree for re-
lation classification. In Proceedings of the 2016
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP 2016, Austin, Texas,
USA, November 1-4, 2016, pages 65–74.

Hai Ye, Wenhan Chao, Zhunchen Luo, and Zhoujun
Li. 2017. Jointly extracting relations with class ties
via effective deep ranking. In Proceedings of the
55th Annual Meeting of the Association for Compu-
tational Linguistics, ACL 2017, Vancouver, Canada,
July 30 - August 4, Volume 1: Long Papers, pages
1810–1820.

Matthew D. Zeiler. 2012. ADADELTA: an adaptive
learning rate method. CoRR, abs/1212.5701.

Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2002. Kernel methods for relation ex-
traction. In Proceedings of the 2002 Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP 2002, Philadelphia, PA, USA, July 6-7,
2002.

Daojian Zeng, Kang Liu, Yubo Chen, and Jun Zhao.
2015. Distant supervision for relation extraction via
piecewise convolutional neural networks. In Pro-
ceedings of the 2015 Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP 2015,
Lisbon, Portugal, September 17-21, 2015, pages
1753–1762.

Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou,
and Jun Zhao. 2014. Relation classification via
convolutional deep neural network. In COLING
2014, 25th International Conference on Computa-
tional Linguistics, Proceedings of the Conference:
Technical Papers, August 23-29, 2014, Dublin, Ire-
land, pages 2335–2344.

Xiangrong Zeng, Shizhu He, Kang Liu, and Jun Zhao.
2018. Large scaled relation extraction with rein-
forcement learning. In Proceedings of the Thirty-
Second AAAI Conference on Artificial Intelligence,
(AAAI-18), the 30th innovative Applications of Arti-
ficial Intelligence (IAAI-18), and the 8th AAAI Sym-
posium on Educational Advances in Artificial Intel-
ligence (EAAI-18), New Orleans, Louisiana, USA,
February 2-7, 2018, pages 5658–5665.

Yuhao Zhang, Victor Zhong, Danqi Chen, Gabor An-
geli, and Christopher D. Manning. 2017. Position-
aware attention and supervised data improve slot fill-
ing. In Proceedings of the 2017 Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP 2017, Copenhagen, Denmark, September
9-11, 2017, pages 35–45.

Guodong Zhou, Jian Su, Jie Zhang, and Min Zhang.
2005. Exploring various knowledge in relation ex-
traction. In ACL 2005, 43rd Annual Meeting of
the Association for Computational Linguistics, Pro-
ceedings of the Conference, 25-30 June 2005, Uni-
versity of Michigan, USA, pages 427–434.

http://arxiv.org/abs/1212.5701
http://arxiv.org/abs/1212.5701

