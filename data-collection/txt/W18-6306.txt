



















































Coreference and Coherence in Neural Machine Translation: A Study Using Oracle Experiments


Proceedings of the Third Conference on Machine Translation (WMT), Volume 1: Research Papers, pages 49–60
Belgium, Brussels, October 31 - Novermber 1, 2018. c©2018 Association for Computational Linguistics

https://doi.org/10.18653/v1/W18-64006

Coreference and Coherence in Neural Machine Translation:
A Study Using Oracle Experiments

Dario Stojanovski Alexander Fraser
Center for Information and Language Processing

LMU Munich
{stojanovski,fraser}@cis.lmu.de

Abstract

Cross-sentence context can provide valuable
information in Machine Translation and is crit-
ical for translation of anaphoric pronouns and
for providing consistent translations. In this
paper, we devise simple oracle experiments
targeting coreference and coherence. Oracles
are an easy way to evaluate the effect of dif-
ferent discourse-level phenomena in NMT us-
ing BLEU and eliminate the necessity to man-
ually define challenge sets for this purpose.
We propose two context-aware NMT mod-
els and compare them against models work-
ing on a concatenation of consecutive sen-
tences. Concatenation models perform better,
but are computationally expensive. We show
that NMT models taking advantage of context
oracle signals can achieve considerable gains
in BLEU, of up to 7.02 BLEU for corefer-
ence and 1.89 BLEU for coherence on subti-
tles translation. Access to strong signals al-
lows us to make clear comparisons between
context-aware models.

1 Introduction

Neural Machine Translation (NMT) (Bahdanau
et al., 2015) is a state-of-the-art approach to MT.
Standard NMT models translate an input language
sentence to an output language sentence, and do
not take into account discourse-level phenomena.
Cross-sentence context has already proven useful
for language modeling (Ji et al., 2015; Wang and
Cho, 2016) and dialogue systems (Serban et al.,
2016). It has also been of interest in Statistical
Machine Translation (SMT) research (Hardmeier,
2012; Hardmeier et al., 2013; Carpuat and Simard,
2012), and NMT research (Wang et al., 2017; Jean
et al., 2017; Tiedemann and Scherrer, 2017; Baw-
den et al., 2018; Tu et al., 2017; Voita et al., 2018).

Two important discourse phenomena for MT
are coreference and coherence. Pronominal coref-
erence relates to the issue of translating anaphoric

pronouns and is tackled in several works (Guillou,
2016; Hardmeier and Federico, 2010; Le Nagard
and Koehn, 2010) and is the central motivation for
the DiscoMT shared task on cross-lingual pronoun
prediction (Loáiciga et al., 2017). Coherence on
the other hand, is important for producing consis-
tent and coherent translations throughout a docu-
ment, especially for domain-specific terminology
(Carpuat, 2009; Ture et al., 2012; Gonzales et al.,
2017) and it is helpful to properly disambiguate
polysemous words. Modeling discourse-level phe-
nomena for MT is a challenging endeavor because
of difficulties in acquiring relevant linguistic sig-
nals. Measuring the effect of discourse-level phe-
nomena with automatic metrics such as BLEU is
also difficult as pointed out by Hardmeier (2012).

In this paper, we address these issues by propos-
ing several oracle experimental setups for eval-
uating the effect of coreference resolution (CR)
and coherence in MT. Oracle experiments provide
strong linguistic signals that enable strongly vis-
ible effects on BLEU scores, thus alleviating the
difficulty of using BLEU to evaluate discourse-
level phenomena in MT. Oracles highlight the ca-
pability of NMT systems to use context (which
we call context-aware NMT) and to handle dif-
ferent discourse-level phenomena. They provide
a variety of scenarios that can easily be set up
for any domain, dataset or language pair, unlike
discourse-specific challenge sets (Bawden et al.,
2018) which must be manually created. Further-
more, strong linguistic signals from oracles enable
us to easily study how the models use context.

Our primary task is translating subtitles from
English to German. Subtitles provide for a reason-
able diversity of topics necessary for testing coher-
ence. They also contain a large amount of short,
informal and conversational text, where anaphoric
pronouns are very important. We study corefer-
ence by aiding pronoun translation and coherence

49

https://doi.org/10.18653/v1/W18-64006


by providing disambiguation signals for transla-
tion of polysemous words. The oracles are auto-
matically created and targeted for each discourse
phenomenon. We additionally include a previous
target sentence oracle, where the context consists
of the previous target sentence, as a more generic
way of including context. This is an interesting
oracle, but this scenario is actually also beneficial
for online post-editing, because the gold standard
previous target sentence is available there.

We propose a simple, yet effective exten-
sion to standard RNN models for NMT (which
we refer to as NMT(RNN)) which models con-
text by employing attention over word embed-
dings only. We compare it against a standard
NMT(RNN) model working on a concatenation of
consecutive sentences (Tiedemann and Scherrer,
2017). Additionally, we evaluate the Transformer
(Vaswani et al., 2017) and propose a context-aware
NMT(Transformer) extension. Our oracles al-
low us to compare the context-aware NMT mod-
els with the baselines and make strong conclu-
sions. Moreover, we study how comparable ora-
cles are with the challenge sets proposed by Baw-
den et al. (2018) by analyzing the performance
of our context-aware model with both approaches.
Finally, we conduct a qualitative study and show
the inner workings of context-aware models under
different oracle settings.

Contributions: (i) We modify the data us-
ing an oracle experimental setup in order to ac-
commodate evaluating coreference and coherence
in NMT. (ii) Our evaluation is independent of
carefully constructed challenge sets, and can eas-
ily be transferred across language pairs and do-
mains. (iii) Results clearly show context-aware
NMT(RNN) and NMT(Transformer) can improve
performance over NMT models without access to
context. (iv) We empirically analyze the pros and
cons of the major approaches to context-aware
NMT and explain how different modeling deci-
sions interact with different discourse phenomena.
(v) We present the trade-offs in modeling power
versus speed that are important when considering
multiple sentences of context.

2 Oracle Signals for Coreference and
Coherence

Acquiring clean and strong context signals is a dif-
ficult challenge and previous work has not pro-
posed a way to do this on a larger scale. In our

work, we use oracles, where the context signals
are strong and allow us to carry out clear analysis.
We define three oracles which differ based on the
context supplied to the model.

First, we define the previous target sentence or-
acle where the context is the gold standard previ-
ous target sentence. Second, we define the coref-
erence or pronoun oracle where we simulate per-
fect knowledge of gender and number for pronoun
translation. Finally, we define the coherence or
more specifically, the repeated words oracle where
we help in identifying polysemous words and pro-
viding the correct signal for disambiguation.

Each of these oracles is accompanied by a fair
and a noisy oracle experimental setup. For the fair
setup, we obtain the linguistic signals in a realis-
tic way without having access to any target side
knowledge. In the noisy oracle setups, we add ad-
ditional target side information to the oracle sig-
nals. This additional information is not necessarily
relevant to the specific problem at hand (corefer-
ence or coherence) and it is used to test the robust-
ness of the models to identify the proper signals.

The oracle datasets are created in an automatic
way. We only need to manually define the list of
pronouns that will be taken into consideration in
the coreference oracle.

Oracle Table 1 shows samples from our ora-
cle setup. For each example we show the context,
original source sentence, our modified oracle sen-
tence and the target sentence. The first two exam-
ples show coreference (pronoun) oracle samples,
while the third one a coherence (repeated words)
oracle sample. The text in brackets shows which
is the counterpart repeated target word or the gen-
der of the noun the pronoun is referencing. It is not
explicitly provided to the models. The text preced-
ing the special token !@#$ in the oracle examples
is the input to the context part of the architecture.

For coreference, we aid the model with pronoun
translation as can be seen in example (c). In this
case, it refers to Roman (meaning novel), which is
apparent in the previous sentence (a). Without this
information the model will have difficulties gener-
ating the proper translation er (the German mas-
culine pronoun agreeing with Roman).

When creating the pronoun oracle setup, we do
not utilize the context sentence. Instead, we just
consider the current source and corresponding tar-
get sentence. If both sentences contain at least one
pronoun in their respective languages, we mark

50



the source pronouns with XPRONOUN and insert
the target pronouns in the context of the main sen-
tence, as in example (c).

The example shows that the context provides
access to perfect knowledge of the coreferent,
which in turn tells us the number and gender.
However, the models still need to learn to use the
correct pronouns. As we can see in example (g),
there may be multiple pronouns in the context.
Since (g) is an imperative sentence, Sie does not
have a pronoun counterpart in the source and it is
used in conjunction with the German verb for use.

Example (k) shows how we model the coher-
ence phenomenon by using repeated words. Given
the English word source in a sentence without
helpful context, it would be impossible to disam-
biguate between two possible translations of the
word: Quelle (a source of a fountain or figuratively
the source of information) or Ursprung (origin,
where something originates from). However, we
see that the previous sentence (i) contains the rele-
vant information to select the correct translation of
the English source. The word source is present in
the previous and current source sentence and Ur-
sprung is present in the previous and current target
sentence. When we find at least one repeated word
on both the source and target side, we mark the
source word with a special token XREP and the
repeated target word is used as context to the main
source sentence. The intuition here follows previ-
ous work (Tu et al., 2017) where past translation
decisions are used for disambiguation. This ora-
cle is admittedly weaker than the coreference one
since it relies on the assumption that a polysemous
word has already been seen in the text. However,
if a word occurs in two consecutive sentences, it is
likely that it will have the same translation.

For the previous target sentence oracle, we use
the gold standard previous target sentence as con-
text and don’t modify the main source sentence.
We also setup experiments with 2 and 3 previous
target sentences as context.

Fair For the fair coreference setup, we attempt
to acquire gender and number knowledge by using
a coreference resolution tool, namely CorefAnno-
tator from Stanford CoreNLP1 (Clark and Man-
ning, 2016a,b). We run the model on entire doc-
uments. We only modified sentences that contain
a pronoun which has an antecedent in the previ-
ous source sentence. Consequently, the pronoun is

1https://stanfordnlp.github.io/CoreNLP

context sentence
(a) Let me summarize the novel[masculine] for you.
source sentence
(b) It presents a problem
pronoun oracle sample
(c) er[masculine] !@#$ XPRONOUN It presents a problem.
target sentence
(d) Er präsentiert ein Problem.
context sentence
(e) But you have a charm[masculine] everyone else here seems
to respond to.
source sentence
(f) Use it. OK, sport?
multiple pronoun oracle sample
(g) Sie ihn[masculine] !@#$ Use XPRONOUN it. OK, sport?
target sentence
(h) Setzen Sie ihn ein.
context sentence
(i) When dealing with a crisis everyone knows you go right
to the source[Ursprung].
source sentence
(j) God the source is pretty.
repeated words oracle sample
(k) Ursprung !@#$ God the XREP source is pretty.
target sentence
(l) Mann, so ein hübscher Ursprung.

Table 1: Coreference and coherence oracle samples. For
detailed explanation of the examples, refer to Section 2.

marked and the antecedent is inserted into the con-
text of the given sentence. In this way, we don’t
utilize any target side knowledge.

For the fair coherence experiment, we don’t
have access to target side information and we just
put special emphasis on words that are polyse-
mous candidates. As a result, we only use repeated
source words. A repeated word is marked in the
main sentence and it is used as context.

For the fair previous sentence experimental
setup, we use the same models trained on the pre-
vious target sentence oracle setup, but evaluate
them by translating the previous source sentence
with a baseline model and using this translation as
context. Additionally, we train models where the
previous sentence is from the source side.

Noisy oracles In order to test the robustness
of context-aware models, we define noisy coref-
erence oracles. We use the same approach as in
the oracle, but the previous gold standard target
sentence is added at the beginning of the context
(which already contains the target side pronouns).

We also define noisy oracles for coherence. In
this case, this is achieved by marking repeated
source words and marking repeated target words
in the previous target sentence and using the mod-
ified previous target sentence as context.

51



3 Related Work

Bawden et al. (2018) is a recent work with simi-
larities to ours. They look at the scores computed
by context-aware models using challenge sets, by
comparing model scores on two perfect target lan-
guage sentences differing only on a single choice
of, e.g., gender for a pronoun, and providing two
different contexts to try to obtain, e.g., masculine
in the first case and feminine in the second case.

Like Bawden et al. (2018), we provide a focused
evaluation on coherence and coreference, but un-
like their work, we do not depend on manually cre-
ated datasets. Our simple oracles are a strong al-
ternative to manually constructed challenge sets,
as we can easily have a more diverse experimen-
tal setup (our oracles can be defined for different
languages, domains and datasets with little effort).

Several approaches have been proposed for
context-aware NMT that utilize a separate mecha-
nism to handle extra-sentential information. Wang
et al. (2017) integrate cross-sentence context using
gates in the decoder, which control information
flow between the cross-sentence context and the
current decoder state. However, the context repre-
sentation is fixed at each decoding time step, while
the model needs to focus on different parts of the
context. Tu et al. (2017) propose a caching mecha-
nism that stores previous translation decisions. As
a result, this approach fails to take into account
CR as stored translation decisions can’t be used to
address this phenomenon. Jean et al. (2017) and
Bawden et al. (2018) propose methods using a sep-
arate RNN-based context encoder. Tiedemann and
Scherrer (2017), propose concatenating the pre-
ceding sentence, both on source and target side
and then using a standard NMT model. These ap-
proaches are computationally expensive. They ei-
ther have an extra RNN-based encoder (Jean et al.,
2017; Bawden et al., 2018) or work on very long
sentences (Tiedemann and Scherrer, 2017).

A recent work by Voita et al. (2018) proposed
a context-aware Transformer model and provided
an analysis of anaphora resolution in MT. Their
proposed model is conceptually similar to our
NMT(Transformer) model, differing in that the
context is integrated in the encoder unlike our
model which does it in the decoder.

We propose a simple NMT(RNN) model that
only uses attention to encode the context and in-
tegrates it with a gating mechanism (Wang et al.,
2017). It provides for a better computational ef-

ficiency compared to models employing an extra
RNN-based encoder. We also propose a context-
aware Transformer model. In the experiments,
we compare our models against a concatena-
tion NMT(RNN) and NMT(Transformer) model
(Tiedemann and Scherrer, 2017).

4 Context-Aware Models

4.1 Lightweight context-aware NMT(RNN)
model

In this paper, we introduce a new lightweight
context-aware model based on the attention
encoder-decoder model proposed by Bahdanau
et al. (2015). We introduce this context-aware
model to compare against the proposed model by
Tiedemann and Scherrer (2017) as an alternative
approach to handling context.

The encoder part of the model, takes the source
sentence X = (x1, x2, . . . , xTx) and generates a
set of annotation vectors {h1, h2, . . . , hTx} where
hi =

[−→
h i;
←−
h i

]
.
−→
h i and

←−
h i are the i-th hid-

den states from the forward and backward recur-
rent networks respectively. The decoder generates
one target symbol yi at a time by computing the
conditional probability p(yi|y1, y2, . . . , yi−1, x) =
f(yi−1, si, ci) where ci represents the attention
weighted sum of annotation vectors and is com-
puted as in (Bahdanau et al., 2015). Unlike previ-
ous approaches that model context by employing
an RNN-based encoder (Jean et al., 2017; Bawden
et al., 2018), we propose to utilize the capability
of the attention mechanism only. This provides
for better computational efficiency, thus allowing
the model to exploit larger context at a lower com-
putational cost.

The context sentence is given as a sequence of
Xc = (xc1, x

c
2, . . . , x

c
T cx
). We map the tokens to

the corresponding word embeddings wci . We share
all embeddings across the model, including the
context ones. The attention on the cross-sentence
context is conditioned on the previously generated
token yi−1 current candidate decoder state si−1
and attention weighted main sentence representa-
tion ci. Formally, the context sentence represen-
tation is computed as cci =

∑T cx
j=1 βijwj where

β ∝ exp(f catt(yi−1, si−1, wj , ci)).
We integrate the context representation using

a gating mechanism (Wang et al., 2017) which
controls the flow of information between the cur-
rent decoder state and the context representation.
which is computed as g = fg(yi−1, si−1, ci, cci ).

52



The final decoder representation is computed as
si = fc(yi−1, si−1, ci, g ⊗ cci ).

4.2 Transformer context-aware model

The Transformer (Vaswani et al., 2017) is an
encoder-decoder architecture which fully relies on
attention. The encoder layers have two main
components, a multi-head self-attention and a
position-wise fully-connected feed-forward net-
work. Each of these components is followed by a
residual connection. In the self-attention sublayer,
each word from the input sentence acts as a query,
key and value when computing the attention. Each
attention head uses the queries and keys to com-
pute a dot product to which a softmax is applied
in order to get the attention weights to score the
values. Consequently, the representation of each
word depends on all the others. The final repre-
sentation is generated by concatenating the out-
put of the separate attention heads and inputting
it to the feed-forward network. The decoder on
the other hand, has three sublayers. It starts by
applying masked self-attention which is then used
to compute multi-head attention over the encoder
representation. This is then used as input to a feed-
forward network as in the encoder.

The proposed context-aware model in this pa-
per is built as an extension to the standard Trans-
former. All embeddings including the context em-
beddings are shared across the model. We mod-
ify the encoder by sharing the parameters for the
multi-head self-attention for the main and con-
text sentence. However, we don’t share the feed-
forward network after the self-attention.

The standard decoder computes a multi-head at-
tention ci over the main encoder representation us-
ing the output from the masked self-attention cmi .
We add an additional multi-head attention over the
context representation cci as well. Before comput-
ing the context attention, the output of the masked
self-attention is projected using a feed-forward
network. The main and context multi-head self-
attention representations are merged using a gat-
ing mechanism as si = gi ⊗ ci + (1 − gi) ⊗ cci
where gi = σ(Weci +Wccci +Wmc

m
i ).

5 Experiments

We train our models on OpenSubtitles2016 En-De
with ≈ 13.9M parallel sentences. The develop-
ment and test set consist of 6 and 7 documents ran-
domly sampled from the dataset, containing 3172

and 4627 sentences respectively. In the corefer-
ence oracle setup ≈ 7.8M training samples were
modified and added the appropriate context, while
in the coherence setup only ≈ 0.8M. The remain-
ing samples are unchanged and have no context.

We apply tokenization, truecasing and BPE
splitting computed jointly on both languages with
59500 operations. All sentences with length above
60 tokens are discarded. Batch size is 80. All
embeddings are tied (Press and Wolf, 2017) in-
cluding the ones in the context part of the archi-
tecture. Dropout (Gal and Ghahramani, 2016) of
0.2 is applied and 0.1 on the embeddings. We ap-
ply layer (Ba et al., 2016) and weight normaliza-
tion (Salimans and Kingma, 2016). The models
are trained with early-stopping based on the de-
velopment set’s cost. We report BLEU score on
detokenized text.

Our RNN-based model is implemented as an
extension to Nematus2 (Sennrich et al., 2017). We
used the Sockeye3 (Hieber et al., 2017) implemen-
tation of the Transformer. For the Transformer
we use hyper-parameters as similar as possible to
the ones in the Nematus models. We additionally
use label smoothing of value 0.1. Both, the base-
line and context-aware model have 4 layers. We
didn’t do any special hyper-parameter tuning for
the context-aware models, so further performance
improvements are possible. The datasets and the
source code for our context-aware models are pub-
licly available4.

6 Experimental Results

6.1 Previous target sentence oracle

In this section, we discuss the effect of using
context in context-aware NMT. In Table 2 we
show the results for the three different oracle
setups. Experiment (1a) shows that a baseline
NMT(RNN) model obtains 28.57 BLEU on the
test set. The NMT(Transformer) baseline (1b)
on the other hand, achieves 29.53 BLEU. Us-
ing the gold standard previous target sentence as
context, provides for 1.32 BLEU improvement
on the test for our context-aware NMT(RNN)
model (2a) and 1.78 BLEU for the concatenation
NMT(RNN) model (3a). Our proposed context-

2https://github.com/EdinburghNLP/
nematus

3https://github.com/awslabs/sockeye
4http://www.cis.uni-muenchen.de/

˜dario/projects/oracles

53



aware NMT(Transformer) model (2b) also im-
proves upon the baseline, but only by 0.6 BLEU,
and the concatenation model (3b) closely follows
the RNN model, adding 1.49 BLEU.

We also evaluate the usefulness of larger con-
text. Using the previous 2 (6a) and 3 (7a) sen-
tences consistently adds ≈ 0.6 BLEU with the
concatenation NMT(RNN) model. The context-
aware NMT(RNN) model, does not improve when
using 2 sentences (4a), but has large gains when
extending to 3 (5a). In our context-aware mod-
els, the larger context is handled by concatenat-
ing all previous sentences. The context-aware
NMT(Transformer) (4b), (5b) was actually hurt by
the larger context. On the other hand, for the con-
catenation model (6b), (7b) we observed some im-
provements, but they were not as consistent as the
gains for the NMT(RNN) model.

The results in (2ab), (3ab), (4ab), (5ab) (6ab),
(7ab) are obtained with models trained and eval-
uated with the gold standard previous target sen-
tences as context. In the fair experiments (8ab),
(9ab) we train with the gold standard previous tar-
get sentence as context, but then evaluate with
translations of the previous source sentences ob-
tained with the baseline model. This lowers the
performance of both NMT(RNN) models (8a),
(9a), but they still improve over the baseline.
Our context-aware NMT(Transformer) model (8b)
slightly lowers performance compared to the base-
line, unlike the concatenation model (9b).

Additionally, we train context-aware mod-
els where the previous sentence is obtained
from the source side (10ab), (11ab). Even in
such a scenario, context-aware and concatena-
tion NMT(RNN) models obtain improvements
over the baseline. Again, the concatenation
NMT(Transformer) shows improvements over the
baseline. The context-aware NMT(Transformer)
was not able to make use of the source side infor-
mation. Given that the encoder representations are
shared this is to some extent surprising and sug-
gests that additional encoder components are nec-
essary to model the contextual representation.

6.2 Coreference

Results for coreference are also shown in Table 2.
Experiments (12a) and (12b) show the results we
obtained with the pronoun oracle setup. It is clear
that NMT can benefit from strong coreference sig-
nals. We observed a large difference between the

(a) RNN (b) TF
(1) baseline 28.57 29.53
(2) context - gold prev. target 29.89 30.13
(3) concat - gold prev. target 30.35 31.02
(4) context - gold prev. 2 target 29.96 29.57
(5) context - gold prev. 3 target 30.95 29.98
(6) concat - gold prev. 2 target 30.96 31.69
(7) concat - gold prev. 3 target 31.56 31.26
(8) context - baseline prev. target 29.10 29.25
(9) concat - baseline prev. target 29.28 29.89
(10) context - prev. source 29.48 28.80
(11) concat - prev. source 29.56 30.25
Coreference
(12) context - pronoun oracle 34.35 34.60
(13) context - fair 29.05 28.76
(14) context - noisy pronoun oracle 33.61 34.62
(15) concat - noisy pronoun oracle 35.59 35.18
Coherence
(16) context - repeated target words 29.83 29.35
(17) context - repeated source words 29.27 29.04
(18) context - noisy rep. target words 30.07 29.85
(19) concat - noisy rep. target words 30.46 31.25

Table 2: BLEU scores from all of the oracle experimental
setups on the test set. Results in the first column correspond
to the NMT(RNN) context-aware and concatenation models
while the second column to the NMT(Transformer) ones. The
number in brackets in each line is used to indicate the corre-
sponding experiment throughout the text.

improvements on the development and the test set,
probably because this phenomenon is not equally
prominent in the datasets. In the absence of perfect
CR, this setup is a reasonable proxy for obtaining
coreference signals and gender information, and
the context-aware models achieve large improve-
ments over their respective baselines.

Experiments (13a) and (13b) show the results
for the fair coreference setup. Using a CR tool, we
identified the appropriate antecedents (to current
sentence pronouns) in the previous source sen-
tence and used them as context. The results show
small improvements on the test set. This signal
is significantly weaker. Moreover, only ≈ 0.3M
samples had a non-empty context, meaning a pro-
noun was referring to a coreferent as identified by
the CR tool. These results show that while weak,
the context-aware NMT(RNN) model is able to
utilize this signal. The NMT(Transformer) model
on the other hand, was significantly hurt by this
setup. We attribute this to the model not being able
to handle scenarios where the majority of the sam-
ples are without context information.

In the noisy pronoun oracle setup, the context
consists of the previous gold standard target sen-
tence to which we append the target side pronouns
as in the previously outlined pronoun oracle setup.
The results are shown in Table 2. We can ob-

54



serve that the context-aware NMT(RNN) model
(14a) is actually hurt by the extra information in
the form of previous target sentence. We attribute
the decrease to the model learning to strongly at-
tend to all pronouns in the context. As such, in
some cases, it chooses to attend to a pronoun from
the previous sentence which ends up acting as
noise in these models. Using oracles allowed us to
easily find this important weakness in our model
design. The context-aware NMT(Transformer)
model (14b) is more robust to noise and had no
problems identifying the appropriate information.

Using the same setting for the concatenation
NMT(RNN) model (15a), achieves best perfor-
mance with an absolute gain of 7.02 BLEU. Based
on the obtained results in (3a), we conclude that
the effects in (15a) are a compound of the capabil-
ity of concatenation models to make use of the pre-
vious sentence and target side pronouns. The same
effects can be observed for the NMT(Transformer)
concatenation model as well (15b). However, de-
spite the concatenation Transformer being able to
obtain better results for the previous target sen-
tence and pronoun oracle than the RNN model, the
compound effect is not as strong.

6.3 Coherence

Table 2 shows the results we obtained for the co-
herence experimental setup. For the oracle setup,
we identify repeated source and target words in
the previous and current sentence, mark the source
words and insert the target words in the context.
For the fair setup, we insert repeated source words
in the context. The aim with this scenario is to em-
phasize which words are potentially important for
disambiguation. Moreover, in the oracle setup, we
provide the presumably gold standard translation
of the repeated word in the appropriate context.

Both scenarios (16a), (17a) obtain improve-
ments over the baseline with the NMT(RNN)
model, although not as strong as the gains with
the pronoun oracle. One reason is that the num-
ber of samples with context is significantly smaller
than the pronoun oracle. Another potential reason
is that coherence is already modeled well by the
baseline. The results indicate that obtaining coher-
ence and disambiguating signals from past trans-
lation decisions, whether from an oracle such as
in our work or from the model itself (Tu et al.,
2017) is difficult. Nevertheless, the noticeable
gains in BLEU we observed in our experiments

confirm that further improvements can be made.
The context-aware NMT(Transformer) is hurt by
these oracle setups as shown in experiments (16b)
and (17b) because of the lack of sufficient context.

Table 2 presents the results for the noisy co-
herence oracle. The context-aware NMT(RNN)
model (18a) obtains improvement over the base-
line of 1.5 BLEU and the concatenation model
(19a) of 1.89 BLEU. This is likely a compound ef-
fect of having access to the entire previous target
sentence as in (2a) and (3a) and the weak signals
in the form of pointers to where disambiguation is
necessary. This is to some extent matched by the
Transformer experiments (18b), (19b).

6.4 Comparison with challenge sets

In order to assess the quality of our oracles, we
also set them up on OpenSubtitles2016 En-Fr and
compare them against the challenge sets proposed
in Bawden et al. (2018). This allows us to compare
the two methods and show whether we can draw
similar conclusions about a model when evaluat-
ing it with both the oracles and challenge sets. For
simplicity, we only evaluate our proposed context-
aware NMT(RNN) model. We randomly sampled
documents from the En-Fr dataset to create a de-
velopment and test set. The challenge sets are used
as provided by Bawden et al. (2018). We set up the
oracles in the same way as for En-De. However, in
French the pronouns le, la and les can also be used
as definite articles. Therefore, we used MarMoT
(Mueller et al., 2013) to filter out these instances.

We compare the methods by measuring the im-
provements a context-aware model achieves over
a baseline, on our oracles and on the challenge
sets. Since our oracles use target side knowledge,
we use the version of the challenge sets where the
previous sentence is from the target side. This
provides for a fairer comparison. We train our
context-aware model on the pronoun and repeated
words oracle. In order to evaluate the model on the
challenge sets, we train the model with the gold
standard previous target sentence as context.

The baseline model obtains a score of 27.73
BLEU on the test and by design, it achieves 50%
accuracy on the coreference and 50% accuracy
on the coherence challenge set. Our proposed
context-aware model trained on the pronoun ora-
cle achieved 30.72 BLEU on the test set. On the
repeated words oracle, it scored 28.25 BLEU. As
in the En-De experimental results, our model ob-

55



pronoun oracle meine er !@#$ XPRONOUN My reading of the prophecy is that XPRONOUN it will come in 2012
reference Meine Textstudien ergeben, daß er 2012 kommen wird
baseline Mein Lesen der Prophezeiung lautet, dass es 2012 kommen wird
context Meine Lesung der Prophezeiung ist, dass er 2012 kommen wird
repeated words
oracle

Abneigung Romulaner !@#$ If you had seen them kill your parents, you would XREP understand it
is always the XREP time for those XREP feelings.

reference Höatten Sie mit angesehen, wie Ihre Eltern getötet werden... Meine Abneigung gegen die Romulaner
ist universell.

baseline Wenn du gesehen hättest, wie sie deine Eltern töten würden, würdest du verstehen, dass es immer die
Zeit für diese Gefühle ist.

context Wenn du gesehen hättest, wie sie deine Eltern getötet haben, würdest du verstehen, dass es immer die
Zeit für diese Abneigung ist.

prev. sent. oracle Er dachte, die Geschichte handelte von einem Fisch. !@#$ It isn’t?
reference Tut sie nicht?
baseline Ist es nicht?
context Ist es nicht?

Table 3: Samples from the qualitative analysis.

tains small gains for coherence and larger ones
for coreference. The context-aware model we
trained with the previous target sentence as con-
text, scored 63.0% and 54.0%, on the corefer-
ence and coherence challenge set, respectively.
From these results we also can conclude that our
model is reasonably powerful to handle corefer-
ence and marginally improves coherence. These
results show that challenge sets and oracles pro-
vide comparable results when evaluating discourse
in MT. However, our oracle setups are easier to de-
fine and control.

6.5 Qualitative study

In this section, we show examples from our ora-
cle setups and provide visualizations of the extra-
sentential attention for our context-aware and
the concatenation NMT(RNN) model (Tiedemann
and Scherrer, 2017). We also show the activations
of the decoder gates which control the context in-
formation flow. This can help us understand how
the models make decisions at each time step.

In Table 3 we show the pronoun, repeated words
and previous target sentence oracles and com-
pare the output from a baseline and our proposed
context-aware model against the reference transla-
tion. For simplicity, in the visualizations for the
concatenation model, we only present the atten-
tion over the previous sentence and the sentence
separating token SEP.

The first row in Table 3 shows a pronoun oracle
sample. In this case, it refers to comet. It is ob-
vious that there is not sufficient information in the
main sentence alone to properly translate it and the
baseline model falls back to the data-driven prior,
which is to generate es.

me
ine

er </s
>

meine

Les@@

ung

der

Prophezeiung

ist

,

dass

er

2012

kommen

wird

...

</s>

(a) Pronoun

Ab
@@

ne
igu

ng

Ro
mu

lan
er

</s
>

wenn
du

gesehen
hättest

,
wie
sie

deine
Eltern

getötet
haben

,
würdest

du
verstehen

,
dass

es
immer

die
Zeit
für

diese
Ab@@

neigung
ist

.
</s>

(b) Repeated words

Figure 1: Context attention for the pronoun and repeated
words oracles.

From the visualization in Figure 1a we see that
our context-aware model pays attention to the ap-
propriate pronoun (meine, er). From Figure 3 we
see that for this example, the noisy oracle shows
the same behavior and correctly ignores the noise.
Furthermore, Figure 2a and Figure 2b show that
the gate activations follow the intuitive assump-
tion that they should be high when generating pro-
nouns. Our model in the noisy pronoun oracle
produced a correct translation, but it still weakly
paid attention to irrelevant parts of the sentence.
From Figure 4 we see that concatenation model
on the other hand, makes a clean distinction be-
tween what is relevant and what is not, and only
has strong attention over the pronouns.

56



meine
Les@@

ung
der

Prophezeiung
ist

,
dass

er
2012

kommen
wird

...
</s>

(a)

meine

Werte

der

Prophezeiung

sind

,

dass

er

2012

kommen

wird

...

</s>

(b)

wenn
du

gesehen
hättest

,
wie
sie

deine
Eltern

getötet
haben

,
würdest

du
verstehen

,
dass

es
immer

die
Zeit
für

diese
Ab@@

neigung
ist

.
</s>

(c)

hätten
Sie

gesehen
,

wie
sie

Ihre
Eltern
töten

,
würden

Sie
verstehen

,
dass

es
immer

die
Zeit
für

diese
Gefühle

ist
.

</s>

(d)

Figure 2: Gate activations for pronoun and repeated words
oracles. (a) pronoun oracle, (b) - noisy pronoun oracle, (c) -
repeated words oracle, (d) - noisy repeated words oracle.

ei
n

Pr
in

z
m

it
wa

ll@
@

en
d

go
l@

@
de

n@
@

em Ha
ar

da
s

ist ei
ne

Me
ta

ph
er

fü
r

ei
ne

n
Ko

m
e@

@
te

n
. m

ei
ne

er </
s>

meine
Werte

der
Prophezeiung

sind
,

dass
er

2012
kommen

wird
...

</s>

Figure 3: Context attention of our proposed model on the
noisy pronoun oracle.

ei
n

Pr
in

z
m

it
wa

ll@
@

en
d

go
l@

@
de

n@
@

em Ha
ar

da
s

ist ei
ne

Me
ta

ph
er

fü
r

ei
ne

n
Ko

m
e@

@
te

n
. m

ei
ne

er SE
P

meine
Werte

der
Prophezeiung

sind
,

dass
er

2012
...

</s>

Figure 4: Attention over the previous sentence of the con-
catenation model on the noisy pronoun oracle.

The second sample is selected from the repeated
words oracle setup. Because the reference transla-
tion does not exactly match the source sentence,
there is a small mismatch between the repeated
words on the source and target side. However,
we see that without the contextual signal that feel-
ings in this case refers to adverse feelings (as in-
dicated by Abneigung) the baseline falls back to
the more common translation Gefühle. We also
looked at the previous sentence which did not have

any context information and both the baseline and
the context-aware model generated Gefühle.

Figure 1b shows that the context-aware model
has no problem attending to the disambiguating
signal (Abneigung) and it also uses this signal
when generating the determiner dieses which is
dependent on the noun. However, we also can ob-
serve that given the incorrect indication to look at
the context when translating time, it also has at-
tention activation over the context as well. This is
closely followed by the gate activations in Figure
2c. The same doesn’t happen when translating the
marked source token understand. This is probably
because the model is confident that it doesn’t need
context when translating understand.

ich ka
nn

Ih
re

XR
EP

Ab
@

@
ne

ig
un

g
ge

ge
n

di
e

XR
EP

Ro
m

ul
an

er
be

gr
ei

fe
n

, ab
er

Si
e

so
llt

en
ih

r
ni

ch
t

fo
lg

en
. </

s>

hätten
Sie

gesehen
,

wie
sie

Ihre
Eltern
töten

,
würden

Sie
verstehen

,
dass

es
immer

die
Zeit
für

diese
Gefühle

ist
.

</s>

Figure 5: Context attention of our proposed model on the
noisy repeated words oracle.

From Figure 5 and Figure 2d we see that the
context-aware model in a noisy repeated words
oracle setting has difficulties identifying the co-
herence information and when to use it. It tends
to pay attention to certain words throughout the
whole sequence generation. This is likely a side
effect of having access to the previous target sen-
tence which in other cases provides useful infor-
mation. Although it pays attention to the appro-
priate repeated word (Abneigung), it still fails to
generate it. Since the concatenation model uses an
RNN over the context, it has no problem identify-
ing the disambiguating signal, marked with XREP
and generates it accordingly (Figure 6).

We also did an analysis of the previous target
sentence oracle as well as the models that use the
previous source sentence as context. We looked
at examples where there is an anaphoric pronoun
it. When the context is from the source side, our

57



ich ka
nn

Ih
re

XR
EP

Ab
@

@
ne

ig
un

g
ge

ge
n

di
e

XR
EP

Ro
m

ul
an

er
be

gr
ei

fe
n

, ab
er

Si
e

so
llt

en
ih

r
ni

ch
t

fo
lg

en
. SE

P

wenn
Sie
sie

gesehen
hätten

,
würden

Sie
verstehen

,
dass

es
immer

die
Ab@@

neigung
gegen

die
Romulaner

ist
.

</s>

Figure 6: Attention over the previous sentence of the con-
catenation model on the noisy repeated words oracle.

context-aware model tends to pay attention to a
single noun, while in the previous target sentence
oracle, it looks at more explicit gender informa-
tion, such as pronouns, articles etc. This is illus-
trated in the last example in Table 3 and Figure 7
and 8. In this case, it refers to die Geschichte or
story. When translating it both models paid atten-
tion to the appropriate place in the previous sen-
tence, but failed to generate the correct pronoun
sie. For this particular example, the concatenation
model paid no attention to the previous sentence.

er da
cht

e
, die Ge

sch
ich

te

ha
nd

elt
e

vo
n

ein
em

Fis
ch

. </s
>

ist
es

nicht
?

</s>

Figure 7: Context attention of our proposed model on the
previous target sentence.

he tho
ug

ht

it wa
s

a sto
ry

ab
ou

t
a fish . </s

>

ist
es

nicht
?

</s>

Figure 8: Context attention of our proposed model on the
previous source sentence.

6.6 Model inference speed
Although the concatenation model performs better
than our context-aware model, an important con-
sideration when working with context-aware NMT
is computational efficiency. We compared infer-
ence times for the RNN models on the develop-

ment set. We report times with context size of 1, 2
and 3 previous sentences.

The context model took 1233 seconds to de-
code the development set, while the concatenation
model 2063 seconds. The concatenation model
took additional ≈ 900 seconds for each additional
context sentence. Because our context-aware im-
plementation is not tightly dependent on context
length, there are no considerable drops in speed.
This is a disadvantage of the concatenation ap-
proach. If one is to use large context, or even
entire documents, the problem quickly becomes
very computationally expensive. This highlights
the necessity of specialized context-aware mod-
els. Since the Transformer can be more easily
parallelized, there is still room for improving the
computational performance of our context-aware
Transformer. As a result, we leave such a compar-
ison for future work.

7 Conclusion and Future Work

We used simple oracles to look at discourse-level
phenomena in MT. We compared context-aware
NMT models and show that these approaches pro-
vide large gains in BLEU for coreference and
coherence given clear oracle signals. We also
showed that even when using fair signals, such as
the previous source sentence or a system transla-
tion of the previous target sentence, NMT mod-
els benefit and make use of the extra informa-
tion. Some future work in context-aware NMT
can focus on using the standard NMT architecture,
which performs well. However, if one requires ac-
cess to larger context, vanilla NMT will have diffi-
culties scaling in terms of speed and perhaps even
in modeling ability. For this reason, a promising
way forward is studying different ways of model-
ing and integrating context that support fast infer-
ence. Oracle experiments will allow us to quickly
test interesting modeling differences.

Acknowledgments

We would like to thank the anonymous review-
ers for their valuable input and Daniel Ledda for
his help with examples. This project has re-
ceived funding from the European Research Coun-
cil (ERC) under the European Union’s Horizon
2020 research and innovation programme (grant
agreement№ 640550).

58



References
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-

ton. 2016. Layer normalization. arXiv preprint
arXiv:1607.06450.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In Proceedings of
the 3rd International Conference on Learning Rep-
resentations, ICLR ’15. ArXiv: 1409.0473.

Rachel Bawden, Rico Sennrich, Alexandra Birch, and
Barry Haddow. 2018. Evaluating Discourse Phe-
nomena in Neural Machine Translation. In NAACL
2018, New Orleans, USA.

Marine Carpuat. 2009. One translation per discourse.
In Proceedings of the Workshop on Semantic Evalu-
ations: Recent Achievements and Future Directions,
pages 19–27.

Marine Carpuat and Michel Simard. 2012. The trouble
with smt consistency. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, pages
442–449.

Kevin Clark and Christopher D. Manning. 2016a.
Deep reinforcement learning for mention-ranking
coreference models. In Proceedings of the 2016
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 2256–2262.

Kevin Clark and Christopher D. Manning. 2016b. Im-
proving coreference resolution by learning entity-
level distributed representations. In Proceedings of
the 54th Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers),
pages 643–653.

Yarin Gal and Zoubin Ghahramani. 2016. A theoret-
ically grounded application of dropout in recurrent
neural networks. In Advances in neural information
processing systems, pages 1019–1027.

Annette Rios Gonzales, Laura Mascarell, and Rico
Sennrich. 2017. Improving word sense disambigua-
tion in neural machine translation with sense embed-
dings. In Proceedings of the Second Conference on
Machine Translation, pages 11–19.

Liane Kirsten Guillou. 2016. Incorporating pronoun
function into statistical machine translation. Ph.D.
thesis, The University of Edinburgh, UK.

Christian Hardmeier. 2012. Discourse in statistical ma-
chine translation. a survey and a case study. Dis-
cours. Revue de linguistique, psycholinguistique et
informatique. A journal of linguistics, psycholin-
guistics and computational linguistics, (11).

Christian Hardmeier and Marcello Federico. 2010.
Modelling pronominal anaphora in statistical ma-
chine translation. In IWSLT (International Work-
shop on Spoken Language Translation); Paris,
France; December 2nd and 3rd, 2010., pages 283–
289.

Christian Hardmeier, Sara Stymne, Jörg Tiedemann,
and Joakim Nivre. 2013. Docent: A document-level
decoder for phrase-based statistical machine trans-
lation. In ACL 2013 (51st Annual Meeting of the
Association for Computational Linguistics); 4-9 Au-
gust 2013; Sofia, Bulgaria, pages 193–198.

Felix Hieber, Tobias Domhan, Michael Denkowski,
David Vilar, Artem Sokolov, Ann Clifton, and Matt
Post. 2017. Sockeye: A Toolkit for Neural Machine
Translation. ArXiv e-prints.

Sebastien Jean, Stanislas Lauly, Orhan Firat, and
Kyunghyun Cho. 2017. Does neural machine trans-
lation benefit from larger context? arXiv preprint
arXiv:1704.05135.

Yangfeng Ji, Trevor Cohn, Lingpeng Kong, Chris Dyer,
and Jacob Eisenstein. 2015. Document context lan-
guage models. arXiv preprint arXiv:1511.03962.

Ronan Le Nagard and Philipp Koehn. 2010. Aiding
pronoun translation with co-reference resolution. In
Proceedings of the Joint Fifth Workshop on Statis-
tical Machine Translation and MetricsMATR, pages
252–261.

Sharid Loáiciga, Sara Stymne, Preslav Nakov, Chris-
tian Hardmeier, Jörg Tiedemann, Mauro Cettolo,
and Yannick Versley. 2017. Findings of the 2017
discomt shared task on cross-lingual pronoun pre-
diction. In The Third Workshop on Discourse in Ma-
chine Translation.

Thomas Mueller, Helmut Schmid, and Hinrich
Schütze. 2013. Efficient higher-order CRFs for mor-
phological tagging. In Proceedings of the 2013
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 322–332, Seattle, Wash-
ington, USA.

Ofir Press and Lior Wolf. 2017. Using the output em-
bedding to improve language models. In Proceed-
ings of the 15th Conference of the European Chap-
ter of the Association for Computational Linguistics:
Volume 2, Short Papers, pages 157–163.

Tim Salimans and Diederik P Kingma. 2016. Weight
normalization: A simple reparameterization to ac-
celerate training of deep neural networks. In Ad-
vances in Neural Information Processing Systems,
pages 901–909.

Rico Sennrich, Orhan Firat, Kyunghyun Cho, Alexan-
dra Birch, Barry Haddow, Julian Hitschler, Marcin
Junczys-Dowmunt, Samuel Läubli, Antonio Valerio
Miceli Barone, Jozef Mokry, and Maria Nadejde.
2017. Nematus: a toolkit for neural machine trans-
lation. In Proceedings of the Software Demonstra-
tions of the 15th Conference of the European Chap-
ter of the Association for Computational Linguistics,
pages 65–68.

Iulian Vlad Serban, Alessandro Sordoni, Yoshua Ben-
gio, Aaron C Courville, and Joelle Pineau. 2016.

59



Building end-to-end dialogue systems using gener-
ative hierarchical neural network models. In AAAI,
pages 3776–3784.

Jörg Tiedemann and Yves Scherrer. 2017. Neural ma-
chine translation with extended context. In Proceed-
ings of the Third Workshop on Discourse in Machine
Translation, pages 82–92.

Zhaopeng Tu, Yang Liu, Shuming Shi, and Tong
Zhang. 2017. Learning to remember translation
history with a continuous cache. arXiv preprint
arXiv:1711.09367.

Ferhan Ture, Douglas W Oard, and Philip Resnik.
2012. Encouraging consistent translation choices.
In Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 417–426.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems, pages 6000–6010.

Elena Voita, Pavel Serdyukov, Rico Sennrich, and Ivan
Titov. 2018. Context-Aware Neural Machine Trans-
lation Learns Anaphora Resolution. In Proceed-
ings of the 56th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 1264–1274, Melbourne, Australia.

Longyue Wang, Zhaopeng Tu, Andy Way, and Qun
Liu. 2017. Exploiting cross-sentence context for
neural machine translation. In Proceedings of the
2017 Conference on Empirical Methods in Natural
Language Processing, pages 2826–2831.

Tian Wang and Kyunghyun Cho. 2016. Larger-context
language modelling with recurrent neural network.
In 54th Annual Meeting of the Association for Com-
putational Linguistics, ACL 2016 - Long Papers,
volume 3, pages 1319–1329.

60


