



















































Key-value Attention Mechanism for Neural Machine Translation


Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 290–295,
Taipei, Taiwan, November 27 – December 1, 2017 c©2017 AFNLP

Key-value Attention Mechanism for Neural Machine Translation

Hideya Mino 1,2∗ Masao Utiyama 3 Eiichiro Sumita 3 Takenobu Tokunaga 2
1NHK Science & Technology Research Laboratories

2Tokyo Institute of Technology
3National Institute of Information and Communication Technology

mino.h-gq@nhk.or.jp {mutiyama, eiichiro.sumita}@nict.go.jp
take@c.titech.ac.jp

Abstract

In this paper, we propose a neural machine
translation (NMT) with a key-value at-
tention mechanism on the source-side en-
coder. The key-value attention mechanism
separates the source-side content vector
into two types of memory known as the
key and the value. The key is used for
calculating the attention distribution, and
the value is used for encoding the context
representation. Experiments on three dif-
ferent tasks indicate that our model out-
performs an NMT model with a conven-
tional attention mechanism. Furthermore,
we perform experiments with a conven-
tional NMT framework, in which a part
of the initial value of a weight matrix is
set to zero so that the matrix is at the
same initial-state as the key-value atten-
tion mechanism. As a result, we obtain
comparable results with the key-value at-
tention mechanism without changing the
network structure.

1 Introduction

Recently, neural machine translation (NMT)
(Sutskever et al., 2014; Cho et al., 2014) has
achieved impressive results owing to its capac-
ity to model the translation process end-to-end
within a single probabilistic model. The unique
features of the most popular approaches to NMT
comprise a encoder-decoder architecture compris-
ing recurrent neural networks (RNNs) and an at-
tention mechanism, whereby the decoder can at-
tend directly to localized information from source
sequence tokens for generating a target sequence

∗ This work was performed while the first author was
affiliated with National Institute of Information and Commu-
nication Technology, Kyoto, Japan.

(Bahdanau et al., 2015; Luong et al., 2015). The
encoder-decoder architecture predicts the target
word with a target hidden-state and a context vec-
tor. This context vector is calculated as a weighted
average over all source hidden-states. The weight
of a source hidden-state is calculated as the inner
product of the source hidden-state and the target
word hidden-state. Note that the source hidden-
state acts as the key to weight itself. It also acts
as the value to predict the target word through the
context vector. Daniluk et al. (2017) suppose that
the dual use of a single vector makes training the
model difficult and propose the use of a key-value
paired structure, which is a generalized way of
storing content in the vector.

In this paper, we propose splitting the matrix
of the source hidden-states into two parts, an
approach suggested by Daniluk et al. (2017) and
Miller et al. (2016). The first part refers to the
key used to calculate the attention distribution or
weights. The second part refers to the value for
the source-side context representation.

We empirically demonstrate that the separation
of the source-side context vector into the key and
value significantly improves the performance of
an NMT using three different English-to-Japanese
translation tasks.

2 Related Work

A significant amount of research has been per-
formed on the use of memory within neural net-
works. For instance, an RNN features an implicit
memory in the form of recurring hidden states.
However, a vanilla RNN is known to have diffi-
culties in storing information for long time-spans
(Bengio et al., 1994). To overcome this prob-
lem, LSTM (Hochreiter and Schmidhuber, 1997),
or GRU (Cho et al., 2014), which contain memory
cells with a recurrently self-connected linear unit

290



have been proposed.
Attention-based neural networks with soft or

hard attention over an input have shown successful
results in a wide range of tasks including machine
translation (Bahdanau et al., 2015), sentence sum-
marization (Rush et al., 2015), and image caption-
ing (Xu et al., 2015). These attention-based net-
works use an encoded memory for both as the key
and value as described in the Introduction to cal-
culate the output.

In contrast to the dual use of a single mem-
ory vector, Miller et al. (2016) have proposed key-
value memory networks with key- and value-
memory vectors to solve question-answering
tasks, which use a generalized approach to store
content in the memory. The key-memory vec-
tors are used to calculate the attention weights,
which address relevant memories with respect to
the question, whereas the value-memory vectors
are used to calculate the contextual representation
to predict the answer. Daniluk et al. (2017) intro-
duce a key-value attention model for neural lan-
guage modeling that separates output vectors into
keys to calculate the attention distribution and val-
ues for encoding the next-word distribution and
context representation. We also focus on the key-
value attention model. Our approach differs from
the approach of Daniluk et al. (2017) in that they
use it for the language model only; in contrast we
use the key-value attention to encode the source-
side context and predict the target-side word for
translation.

3 Method

3.1 NMT with Attention

Our work is based on an attention-based NMT
(Luong et al., 2015), which generates a target sen-
tence y = (y1, ..., yN ) ∈ RVt×N from the source
sentence x = (x1, ..., xM ) ∈ RVs×M . Vs and Vt
denote the vocabulary size of the source and tar-
get side, respectively. The attention-based model
comprises two components, an encoder and a de-
coder. The encoder embeds the source sentence
x into vectors through an embedding matrix and
produces the hidden states using a bidirectional
RNN, which represents a forward and a backward
sequence. Thus, we have

−→
h i = enc1(Wsxi,

−→
h i−1), (1)←−

h i = enc2(Wsxi,
←−
h i+1). (2)

𝑑"#$ 𝑑"

𝛼$," 𝛼',"
𝑐"

・・・

・・・

・・・

𝑦"#$

𝑒" 𝑦"

+,

+,

ℎ'

+.
←

+0
←

+0
→

+.
→

ℎ2ℎ$

Figure 1: Encoder-decoder NMT architecture

Ws ∈ RK×Vs is an embedding matrix where K
is the word embedding size, and enc1 and enc2
are nonlinear functions as in LSTM. Then, as il-
lustrated in Figure 1, the forward and backward
hidden states

−→
h and

←−
h are concatenated into the

hidden states h = (h1, ..., hM ) ∈ RK×M as

hi = We[
−→
h ⊤i ;
←−
h ⊤i ]

⊤, (3)

where We ∈ RK×2K is a matrix for the affine
transform. Each hidden state, represented as a sin-
gle vector, can be seen a memory vector that in-
cludes not only the lexical information at its source
position, but also information about the left and
right contexts. Then, the decoder predicts the tar-
get sentence y using a conditional probability cal-
culated as bellow:

p(yj |y1,j−1, x) = softmax(Woej + bo), (4)
where Wo ∈ RVt×K and bo ∈ RVt are imple-
mented as a matrix and a bias of a feedforward
neural network with a softmax output layer. ej ∈
RK is calculated by concatenating a hidden state
with a context vector, and performing an affine
transform with tanh function as

ej = tanh(Wd[dj ; cj ]⊤), (5)

where Wd ∈ RK×2K is a matrix for the affine
transform; dj ∈ RK is the hidden state of the de-
coder RNN; and cj ∈ RK is the context vector
derived from the source sentence. dj is a fixed-
length continuous vector computed by

dj = dec(dj−1, yj−1). (6)

291



𝑑"#$ 𝑑"

𝛼$," 𝛼',"
𝑐"

・・・

・・・

𝑦"#$

𝑒" 𝑦"

+,

+,

ℎ'

.,

.,

+/
→

+/
←

ℎ2

./
→

./
←

+3
→

+3
←

ℎ$

.3
→

.3
←

Figure 2: Encoder-decoder NMT architecture with
key-value attention

Here dec is a nonlinear function analogous to enc1
or enc2; d1 is set to a matrix of an affine transfor-
mation of the last hidden state hM . The context
vector cj is computed as a convex sum of the hid-
den states hi of Equation (3):

cj =
M∑
i=1

αi,jhi, (7)

where αi,j , known as the attention weight, is a
scalar weight computed by

αi,j =
exp{score(dj−1, hi)}∑M
l=1 exp{score(dj−1, hl)}

, (8)

where the score function is referred as a content-
based function and can be an arbitrary similar-
ity function. We use the dot product, following
Luong et al. (2015).

3.2 NMT with Key-value Attention
Attention-based NMT encodes an arbitrary se-
quence of source-side words into fixed-length
dense vectors as in h in Eq. (3), which are used
to calculate the attention weights and the context
vectors as in Equations 8 and (7). However, the
requirement to compress all necessary information
into a single memory vector in each memory slot
is likely to cause performance deficiencies. There-
fore, to alleviate this problem, Miller et al. (2016)

and Daniluk et al. (2017) propose the use of a sep-
arate vector depending on the purpose. Inspired by
them, we introduce a key-value attention mecha-
nism into NMT to calculate the context vector with
explicit separate vectors as shown in Figure 2. The
encoder embeds the source sentence x and pro-
duces hidden states

−→
hi and

←−
hi as in Equations (1)

and (2). Then, the two hidden states are decom-
posed into two respective parts, which are a key
and a value, as

−→
hi =

[ −→
k i−→v i

]
,
←−
hi =

[ ←−
k i←−v i

]
, (9)

where the number of dimensions of the keys
−→
k i

and
←−
k j and the values −→v i and ←−v i is K/2. The

forward and backward hidden-states
−→
k
←−
k −→v and←−v are concatenated into the hidden states h as

hi =
[

ki
vi

]
=


Wf

[ −→
k i←−
k i

]

Wg

[ −→v i←−v i
]

 , (10)

where Wf ∈ RK/2×K and Wg ∈ RK/2×K are
matrices for the affine transform. The hidden
states k and v indicate the key- and value-memory
vector, respectively. Then, the decoder predicts
the target sentence y using a conditional probabil-
ity calculated as in Equations (4), (5), and (6). The
context vector cj in Eq. (5) is computed as a con-
vex sum of the value memory v in Equation (10):

cj =
M∑
i=1

αi,jvi (11)

where αi,j is calculated with the key memory ki
as

αi,j =
exp{score(dj−1, ki)}∑M
l=1 exp{score(dj−1, kl)}

. (12)

We also use the dot product for the score.

3.3 NMT Modifying Initial Weight
Another approach to alleviating this problem is
modifying the score function in Eq. (8) and the
initial value of the weight Wd in Eq. (5) of Sec-
tion 3.1. The benefit of this approach is that the
modification to the source code is minimal and the
Wd may be tuned for better values. We suppose

292



Training Development Test
Corpus Sents. Word types Avg. length Sents. Word types Sents. Word types

en ja en ja en ja en ja
IWSLT’07 40K 9K 10K 9.3 12.7 0.5K 1.2K 1.3K 0.5K 0.8K 0.9K
NTCIR-10 717K 105K 79K 23.3 27.7 2.0K 5.0K 4.4K 0.5K 2.4K 2.1K
ASPEC 843K 288K 143K 22.1 23.9 1.8K 7.1K 6.3K 1.8K 7.0K 6.4K

Table 1: Datasets

that the upper half of the hidden states h in Eq. (3)
produced by the encoder is used to calculate the
alignment weight and the lower half of h is used
to encode the source-side context vector. Then,
we present two modifications. Firstly, since the
score function in Eq. (8) calculates the alignment
weight, we modify Eq. (8) to be zero for the lower
half of the output of the score function as

αi,j =
exp{score(dj−1, hi ⊙ u)}∑M
l=1 exp{score(dj−1, hl ⊙ u)}

, (13)

where u ∈ RK is a vector for masking of which
the upper half is one and the lower half is zero, and
⊙ denotes the element-wise multiplication opera-
tion of the two vectors. Secondly, ej in Eq. (5) is
calculated with the context vector cj , of which the
upper half should not be used hereafter. Therefore,
we set the initial weight of Wd to

w1,1 . . . w1,k 0 . . . 0 w1,3K/2 . . . w1,2K
...

wK/2,1 . . . . . . . . . . . . . . . . . . . . . wK/2,2K
...

wK,1 . . . . . . . . . . . . . . . . . . . . . wK,2K

 .

The particular concern is that, unlike Section 3.2,
the upper and lower halves of h in the model are
not completely independent though the upper and
lower halves of the initial state are independently
used to train the model.

The objective of the three methods in this sec-
tion is to jointly maximize the conditional proba-
bility for each generated target word as

θ∗ = arg max
θ

T∑
t=1

Lt∑
j=1

log p(ytj |yt1,j−1,xt, θ), (14)

where (xt,yt) is the t-th training pair of sen-
tences, and Lt is the length of the t-th target sen-
tence yt.

4 Experiments

We evaluate the proposed method using three dif-
ferent English-to-Japanese translation tasks.

4.1 Data and Model Parameters

The corpora used were IWSLT’07 (Fordyce,
2007), NTCIR-10 (Goto et al., 2013), and AS-
PEC (Nakazawa et al., 2016) shown in Table 1.
We constrained training sentences to a maximum
length of 40 words to speed up the training. Each
test sentence had a single reference translation.

4.2 Settings

The inputs and outputs of our model are se-
quences of one-hot vectors with dimensionality
corresponding to the sizes of the source and tar-
get vocabularies. For NTCIR-10 and ASPEC,
we replaced words with frequencies less than 3
with the [UNK] symbol and excluded them from
the vocabularies. Each source and target word
was projected into a 540-dimensional continu-
ous Euclidean space to reduce the dimensionality.
The depth of the stacking LSTMs was 2 and the
hidden-layer size was set to 540. Each model was
optimized using Adam (Kingma and Ba, 2014)
with the following parameters: α = 1e − 3, β1 =
0.9, β2 = 0.999, and ϵ = 1e− 8. To prevent over-
fitting we used dropout (Srivastava et al., 2014)
with a drop rate of r = 0.5 to the last layer of
each stacking LSTM. All weight matrices of each
model were initialized by sampling from a normal
distribution of 0 mean and 0.05 standard deviation.
The gradient at each update was calculated using a
minibatch of at most 64 sentence pairs which was
run for a maximum of 20 iterations for the entire
training data. Training was early-stopped to maxi-
mize the performance on the development set mea-
sured by BLEU. We used a single Tesla K80 GPU
with 12 GB memory for training. For decoding,
we used a beam search with a beam size of 10.
The beam search was terminated when an end-of-
sentence [EOS] symbol was generated. We used
Chainer 1.21.0 (Tokui et al., 2015) to implement
all the models.

293



System
source This makes it difficult to reproduce by a thin film multi-reproducing head .
reference このため薄膜マルチ再生ヘッド (a thin film multi-reproducing head)による

再生が困難となる。
attn これ に よ り 、 薄 い (a thin) 薄膜 磁気 ヘッド (thin film reproducing head) に

よる再生が困難になる。
key-value これにより、薄膜磁気ヘッド (a thin film reproducing head)による再生は

困難である。
modifying-IW よって、薄膜磁気ヘッド (a thin film reproducing head)による再生が困難

である。

Figure 3: Examples of the outputs

System IWSLT’07 NTCIR-10 ASPEC
attn 49.1 31.1 29.6
key-value 49.3 33.8 † 30.7 †
modifying-IW 49.6 32.6 † 30.0

Table 2: BLEU scores for the attention-based
NMT (attn), NMT with the key-value attention
(key-value), and NMT modifying initial weight
(modifying-IW) (†: significantly better than attn
(p < 0.05).

5 Results

Table 2 summarizes the results for all the three
tasks. NMT with the key-value attention achieved
statistically significant results for the experiments
with NTCIR-10 and ASPEC, though the experi-
ments with IWSLT07 showed no such statistically
significant results. The reason for our model’s
small difference in BLEU for IWSLT07 is likely
due to the low number of word types used. The
number of word types used in IWSLT07 was much
lower than in the others, as presented in Table 1.
Our model can be considered to be more effective
for tasks with a vast vocabulary size. The results
with NMT modifying initial weight are almost
comparable to NMT with the key-value attention.
Figure 3 shows the example of the outputs with
each model. Though the attention-based NMT
translates the same part of the sentence (“thin”)
twice, the NMT with the key-value attention and
the NMT modifying initial weight translate cor-
rectly. These results show that the use of the sepa-
rate memories for every different purpose improve
the NMT translation quality and the initial weights
of the hidden layers are likely to be able to con-
trol a single memory to keep dealing with the key
and the value as separate as possible. For the train-
ing and translation speed, we did not observe large

difference between these three models, since the
three models have almost same number of param-
eters.

6 Conclusion

We propose a new method with the key-value at-
tention mechanism in order to make the atten-
tion mechanism simpler. Our empirical evalua-
tion shows that the proposed method is effective
in achieving substantial improvements in terms of
translation quality consistently across three differ-
ent tasks.

Acknowledgments

This work was partially supported by the program
“Promotion of Global Communications Plan: Re-
search, Development, and Social Demonstration
of Multilingual Speech Translation Technology”
of MIC, Japan.

References

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In ICLR.

Yoshua Bengio, Patrice Simard, and Paolo Frasconi.
1994. Learning long-term dependencies with gradi-
ent descent is difficult. IEEE Transactions on Neu-
ral Networks 5(2):157–166.

Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using rnn encoder–decoder
for statistical machine translation. In Proceedings of
the 2014 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP). Association
for Computational Linguistics, Doha, Qatar, pages
1724–1734.

294



Michal Daniluk, Tim Rocktäschel, Johannes Welbl,
and Sebastian Riedel. 2017. Frustratingly short at-
tention spans in neural language modeling. CoRR
abs/1702.04521.

Cameron Shaw Fordyce. 2007. Overview of the 4th
international workshop on spoken language transla-
tion iwslt 2007 evaluation campaign. In In Proceed-
ings of IWSLT 2007. Trento, Italy, pages 1–12.

Isao Goto, Ka-Po Chow, Bin Lu, Eiichiro Sumita, and
Benjamin K. Tsou. 2013. Overview of the patent
machine translation task at the NTCIR-10 work-
shop. In Proceedings of the 10th NTCIR Confer-
ence on Evaluation of Information Access Technolo-
gies, NTCIR-10, National Center of Sciences, Tokyo,
Japan, June 18-21, 2013.

Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long
short-term memory. Neural Comput. 9(8):1735–
1780.

Diederik P. Kingma and Jimmy Ba. 2014. Adam:
A method for stochastic optimization. CoRR
abs/1412.6980.

Thang Luong, Hieu Pham, and Christopher D. Man-
ning. 2015. Effective approaches to attention-based
neural machine translation. In Proceedings of the
2015 Conference on Empirical Methods in Natu-
ral Language Processing. Association for Compu-
tational Linguistics, Lisbon, Portugal, pages 1412–
1421.

Alexander H. Miller, Adam Fisch, Jesse Dodge, Amir-
Hossein Karimi, Antoine Bordes, and Jason We-
ston. 2016. Key-value memory networks for di-
rectly reading documents. In Proceedings of the
2016 Conference on Empirical Methods in Natural
Language Processing, EMNLP 2016, Austin, Texas,
USA, November 1-4, 2016. pages 1400–1409.

Toshiaki Nakazawa, Manabu Yaguchi, Kiyotaka Uchi-
moto, Masao Utiyama, Eiichiro Sumita, Sadao
Kurohashi, and Hitoshi Isahara. 2016. Aspec:
Asian scientific paper excerpt corpus. In Nico-
letta Calzolari (Conference Chair), Khalid Choukri,
Thierry Declerck, Marko Grobelnik, Bente Mae-
gaard, Joseph Mariani, Asuncion Moreno, Jan
Odijk, and Stelios Piperidis, editors, Proceedings
of the Ninth International Conference on Language
Resources and Evaluation (LREC 2016). European
Language Resources Association (ELRA), Portoro,
Slovenia, pages 2204–2208.

Alexander M. Rush, Sumit Chopra, and Jason Weston.
2015. A neural attention model for abstractive sen-
tence summarization. In Proceedings of the 2015
Conference on Empirical Methods in Natural Lan-
guage Processing. Association for Computational
Linguistics, Lisbon, Portugal, pages 379–389.

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: A simple way to prevent neural networks

from overfitting. Journal of Machine Learning Re-
search 15:1929–1958.

Ilya Sutskever, Oriol Vinyals, and Quoc V. V Le.
2014. Sequence to sequence learning with neural
networks. In Z. Ghahramani, M. Welling, C. Cortes,
N.D. Lawrence, and K.Q. Weinberger, editors, Ad-
vances in Neural Information Processing Systems
27, Curran Associates, Inc., pages 3104–3112.

Seiya Tokui, Kenta Oono, Shohei Hido, and Justin
Clayton. 2015. Chainer: a next-generation open
source framework for deep learning. In Proceedings
of Workshop on Machine Learning Systems (Learn-
ingSys) in The Twenty-ninth Annual Conference on
Neural Information Processing Systems (NIPS).

Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun
Cho, Aaron C. Courville, Ruslan Salakhutdinov,
Richard S. Zemel, and Yoshua Bengio. 2015. Show,
attend and tell: Neural image caption generation
with visual attention. In Proceedings of the 32nd In-
ternational Conference on Machine Learning, ICML
2015, Lille, France, 6-11 July 2015. pages 2048–
2057.

295


