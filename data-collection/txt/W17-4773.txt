



















































Multi-source Neural Automatic Post-Editing: FBKa•Žs participation in the WMT 2017 APE shared task


Proceedings of the Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 630–638
Copenhagen, Denmark, September 711, 2017. c©2017 Association for Computational Linguistics

Multi-source Neural Automatic Post-Editing:
FBK’s participation in the WMT 2017 APE shared task

Rajen Chatterjee1,2, Amin Farajian1,2, Matteo Negri2, Marco Turchi2,
Ankit Srivastava3, Santanu Pal4

1University of Trento
2Fondazione Bruno Kessler

3 German Research Center for Artificial Intelligence
4Saarland University

Abstract

Previous phrase-based approaches to Au-
tomatic Post-editing (APE) have shown
that the dependency of MT errors from the
source sentence can be exploited by jointly
learning from source and target informa-
tion. By integrating this notion in a neu-
ral approach to the problem, we present
the multi-source neural machine transla-
tion (NMT) system submitted by FBK to
the WMT 2017 APE shared task. Our
system implements multi-source NMT in
a weighted ensemble of 8 models. The
n-best hypotheses produced by this en-
semble are further re-ranked using fea-
tures based on the edit distance between
the original MT output and each APE hy-
pothesis, as well as other statistical models
(n-gram language model and operation se-
quence model). This solution resulted in
the best system submission for this round
of the APE shared task for both en-de and
de-en language directions. For the for-
mer language direction, our primary sub-
mission improves over the MT baseline up
to -4.9 TER and +7.6 BLEU points. For
the latter, where the higher quality of the
original MT output reduces the room for
improvement, the gains are lower but still
significant (-0.25 TER and +0.3 BLEU).

1 Introduction

Automatic post-editing (APE) aims to correct sys-
tematic machine translation (MT) errors, thereby
reducing translators workload and eventually in-
creasing translation productivity. The task, well
motivated in (Bojar et al., 2015) and (Bojar et al.,
2016), becomes necessary when working in a
“black-box” condition where the MT engine used

to translate is not directly accessible for retrain-
ing or for more radical internal modifications. As
pointed out in (Bojar et al., 2015), from the appli-
cation point of view an APE system can help to: i)
improve MT output by exploiting information un-
available to the decoder, or by performing deeper
text analysis that is too expensive at the decoding
stage; ii) provide professional translators with im-
proved MT output quality to reduce (human) post-
editing effort and iii) adapt the output of a general-
purpose MT system to the lexicon/style requested
in a specific application domain.

Different APE paradigms based on statistical
methods (Simard et al., 2007; Dugast et al., 2007;
Isabelle et al., 2007; Lagarda et al., 2009; Potet
et al., 2012; Rosa et al., 2013; Lagarda et al.,
2015; Chatterjee et al., 2017) have been proposed
in the past showing the effectiveness of APE sys-
tems. In the previous round of the APE shared task
(WMT16), neural (Junczys-Dowmunt and Grund-
kiewicz, 2016), hybrid (Chatterjee et al., 2016),
and phrase-based (Pal et al., 2016b) solutions were
all able to significantly improve MT output qual-
ity in domain-specific settings, with neural sys-
tem being the best in 2016. Some of the previ-
ous approaches, both phrase-based (Béchara et al.,
2011; Chatterjee et al., 2015b) and neural (Li-
bovický et al., 2016) also suggested the impor-
tance of jointly learning both from the source sen-
tences and from the corresponding translations in
order to take advantage of the strict dependency
between translation errors and the original source
sentences.

Learning from these lessons, this year the FBK
participation in the APE task is based on a multi-
source neural sequence-to-sequence architecture.
We extend the existing NMT implementation in
the Nematus toolkit (Sennrich et al., 2016a) to fa-
cilitate multi-source training and decoding. This
year we participated in both translation directions

630



(en-de and de-en) with similar system architec-
tures consisting of an ensemble of 8 neural models
followed by a re-ranker. On both tasks, our pri-
mary submissions achieved the best results, with
significant improvements over the baseline (-4.9
TER and +7.6 BLEU for en-de and -0.25 TER and
+0.3 BLEU for de-en).

2 Neural Machine Translation

As normally done in APE, we cast the problem as
a “monolingual translation” task in which a system
is trained on (src, mt, pe) triplets to “translate” (i.e.
correct) rough MT output (mt) into fluent and ad-
equate translations by learning from human post-
edits (pe). Following the recent success of neural
approaches (to MT in general and APE in partic-
ular), we develop our neural APE systems around
the sequence-to-sequence encoder-decoder archi-
tecture proposed in (Bahdanau et al., 2014) and
further developed by Sennrich et al. (2016a) in the
Nematus toolkit (Sennrich et al., 2017).

Neural machine translation aims to optimize
the parameters of the model to maximize the
log-likelihood of the training data. The ultimate
goal is to estimate a conditional probability model
pΘ(y|x), where Θ is the parameter set of the
model (the weights and biases of the network), y is
a target sentence and x is a source sentence. Thus,
the objective function is:

argmax
Θ

1

N

N∑

n=1

log(pΘ(yn|xn)); (1)

where N is the total number of sentence pairs in
the training corpus. The conditional probability is
computed as:

pΘ(y|x) =
Ty∏

t=1

pΘ(yt|y<t, x) (2)

where Ty is the number of words in the target sen-
tence. Given all the previous target words y<t and
the source x, the probability of target word yt, is
modelled by the decoder network as follows:

pΘ(yt|y<t, x) = g(ẏt−1, st, ct) (3)

where ẏt−1 is the word embedding of the previous
target word, st is the hidden state of the decoder,
and ct the source context vector (encoding of the
source sentence x) at time t. The decoder state st
is computed by a gated recurrent unit (GRU) (Cho

et al., 2014) in two steps. First, the previous hid-
den state and the previous target word embedding
are used to compute an intermediate hidden state
by a GRU unit:

s′t = f
′(st−1, ẏt−1) (4)

Then, the intermediate hidden state and the source
context vector are passed to another GRU to com-
pute the final hidden state of the decoder. In short:

st = f(s
′
t, ct) (5)

The source context vector is a weighted sum of all
the hidden states of a bi-directional encoder (Bah-
danau et al., 2014).1

ct =

Tx∑

j=1

atjhj (6)

where atj is the attention weight given to the j-th
encoder hidden state at decoding time t, and Tx is
the number of words in the source sentence. The
attention weight represents the importance of the
j-th hidden state of the encoder in generating the
target word of time t. It is drawn from a probabil-
ity distribution over all the hidden states of the en-
coder, which is computed by applying a softmax
operator over all the scores of the hidden units of
the encoder:

atj =
exp(etj)∑Tx
k=1 exp(etk)

(7)

where etj and etk are the score of the j-th and k-th
hidden units of the encoder at time step t, which
is a function of the intermediate hidden state of
the decoder (as mentioned in Equation 4) and the
hidden state of the encoder, as shown below:

etj = a(s
′
t, hj) (8)

The hidden state hj of the j-th source word is a
concatenation of the hidden states of the forward
and backward encoders:

hj = [
−→
hj ;
←−
hj ] (9)

where
−→
hj and

←−
hj are respectively the hidden state

of the forward and backward encoders. These hid-
den states are computed by the GRU unit that takes

1In rest of the paper, by encoder we mean bi-directional
encoder

631



previous/next hidden state and the word embed-
ding of the j-th source word (ẋj).

−→
h j = f(ẋj ,

−→
h j−1) (10)

←−
h j = f(ẋj ,

←−
h j+1) (11)

3 Multi-source implementation

The strict connection between MT errors and the
input source sentences suggests to develop APE
systems that leverage information both from the
source (src) and it’s corresponding translation (mt)
instead of looking at the machine-translated sen-
tence in isolation. Exploiting source information
as an additional input can in fact help the sys-
tem to disambiguate corrections applied at each
time step. For example, the German phrase “mein
Haus” (EN: my house) looks correct but if the
source phrase was “my home” then the correct
translation would be “mein Zuhaus”. In this case,
an APE system ignoring the source would have
left the sub-optimal MT output untouched.

Jointly learning from both source and transla-
tion has been previously proved to be effective in
(Béchara et al., 2011; Chatterjee et al., 2015b).
Such works, however, exploit the idea of a “joint
representation” of the input mainly in the statis-
tical phrase-based APE framework while, within
the neural paradigm, recent prior work mostly fo-
cuses on single-source systems (Pal et al., 2016a;
Junczys-Dowmunt and Grundkiewicz, 2016; Pal
et al., 2017). The only exception, to the best of
our knowledge, is the approach of Libovický et al.
(2016), who developed a multi-source neural APE
system. According to the authors, however, the
resulting network seems to be inadequate to learn
how to perform the minimum edits required to cor-
rect the MT segment. Rather, it learns to para-
phrase the input, which results in a high chance of
performing unnecessary corrections that would be
penalized by reference-based evaluations against
human post-edits. Therefore, to mitigate this prob-
lem, they represented the target as a minimum-
length sequence of edit operation needed to turn
the machine-translated sentence into the reference
post-edit.

Our multi-source APE implementation, which
is built on top of the network architecture dis-
cussed in §2, is similar to (Libovický et al., 2016)
but extends it with a context dropout, and consid-
ers the target as a sequence of words rather than

minimum-length sequence. We extend the archi-
tecture to have two encoders, one for src and an-
other for mt. Each encoder has its own attention
layer that is used to compute the weighted context
(Equation 6). The src and the mt weighted con-
texts (csrct and c

mt
t respectively) are then passed

to a merger layer to obtain the final context
(ct−merge). The merger layer concatenates both
contexts and applies a linear transformation, thus
the final context captures information from both
the inputs:

ct−merge = [csrct ; c
mt
t ] ∗Wct + bct (12)

where, Wct, bct are respectively the weight and
the bias of the merger layer. The final context
(ct−merge) is used by the decoder to compute tar-
get word probabilities (similar to Equation 3)

pΘ(yt|y<t, x) = g(ẏt−1, st, ct−merge) (13)

Context Dropout: Dropout was proposed by
Hinton et al. (2012) as a regularization technique
for deep networks to avoid over-fitting. The key
idea is to randomly drop some units (along with its
incoming and outgoing connections) from the neu-
ral network to prevent co-adaption on the training
data. It has been shown to be very effective on a
wide range of supervised learning tasks in vision,
speech recognition, document classification and
computational biology (Srivastava et al., 2014).
When applying dropout with a recurrent neural
network, Gal and Ghahramani (2016) showed that
using same dropout mask at each timestep is bet-
ter than ad hoc techniques where different dropout
are sampled at each time step. This strategy is also
retained in the Nematus toolkit with the exception
of using dropout at the token level instead of type
level. Since our multi-source architecture is im-
plemented on top of this toolkit, we also follow the
same dropout strategy. We use context dropout at
different layers of the network:

• To compute the attention score in Equation 8
we apply a shared dropout to the hidden state
of both encoders;

• To compute the final hidden state of the de-
coder in equation 5, we apply a dropout to the
merged context of the encoders (ct−merge).

We have observed that the use of context dropout
helps the model to avoid overfitting and allows
more stable performance on the validation set
when the model converges.

632



4 Experiments and Development Results

In this section we summarize how our systems
have been trained, tuned and combined to pro-
duce the FBK submissions to the WMT 2017 APE
shared task.

4.1 Data
EN-DE: We use ∼4M artificially-created train-
ing data from (Junczys-Dowmunt and Grund-
kiewicz, 2016) to train generic models that are
later fine-tuned with ∼500K artificial2 and 23K
(replicated 20 times) real post-edited training data
collected from previous year and this year shared
task (Bojar et al., 2016).3 The development set
released in the previous year shared task is used
to evaluate and compare different models’ perfor-
mance. All the data is segmented using the byte
pair encoding technique to obtain sub-word units
following (Sennrich et al., 2016b) in order to avoid
the problem of out-of-vocabulary words.

DE-EN: We create artificial post-editing train-
ing data by a round-trip translation using the sub-
set of parallel data released in the medical task at
WMT’14 (Bojar et al., 2014). The parallel data is
used to build a phrase-based MT system (PBMT)
for both en-de and de-en language directions. The
monolingual English data (considered as pe) is
first translated into German (considered as source)
using the en-de PBMT system, and then back-
translated into English (considered as mt) using
the de-en PBMT system. The parallel and mono-
lingual data each consists of ∼2M segments. To
train the APE systems we concatenate the round-
trip translated data, the parallel data where we con-
sider the reference as the MT output itself, and the
shared task training data (25K triplets) replicated
160 times to avoid possible biases towards the ar-
tificial data. All the data is segmented in sub-word
units (similar to the en-de direction), and the sys-
tems are evaluated on the development set released
for this years’ shared task.

4.2 Evaluation Metric
We run case-sensitive evaluation with TER, which
is based on edit distance, and BLEU (Papineni
et al., 2002), which is based on modified n-gram
precision. In addition to the standard evaluation

2https://github.com/amunmt/amunmt/
wiki/AmuNMT-for-Automatic-Post-Editing

3http://www.statmt.org/wmt17/ape-task.
html

metrics, we also measure the precision of our
APE system using sentence level TER score as
defined in (Chatterjee et al., 2015a):

Precision =
Number of Improved Sentences
Number of Modified Sentences

where “Number of Improved Sentences” is
the count of APE outputs that have lower TER
than the corresponding MT output, and “Number
of Modified Sentences” is the count of APE
outputs that have TER scores different from the
TER of the corresponding MT output.

4.3 Hyper parameters

The hyper parameters of all the systems in both
language directions are the same. The vocabu-
lary is created by selecting 50K most frequent sub-
words. Word embedding and GRU hidden state
size is set to 1024. Network parameters are op-
timized with Adagrad (Duchi et al., 2011) with a
learning rate of 0.01 following the work by Fara-
jian et al. (2016), which empirically showed that
Adagrad has a faster convergence rate and better
performance than Adadelta (Zeiler, 2012). Source
and target dropout is set to 10%, whereas, encoder
and decoder hidden states, weighted source con-
text, and embedding dropout is set to 20% (Sen-
nrich et al., 2016a). After each epoch, the train-
ing data is shuffled and the batches are created af-
ter sorting 2000 samples in order to speed-up the
training. The batch size is set to 100 samples, with
a maximum sentence length of 50 sub-words.

4.4 Models

For both language directions, we trained four
types of networks to capture different information
that can be leveraged together via ensemble tech-
niques. The results of the single best model for
en-de and de-en from each network type are re-
spectively reported in Tables 1 and 2. The perfor-
mance trends among different networks are similar
for both language directions. However, the varia-
tion are less visible in the case of de-en given the
fact that the room of improvement is much lower
due to higher MT quality (15.58 TER and 79.46
BLEU scores). Therefore, we base our discus-
sion for each model below on the results achieved
on the development data for the en-de direction,
where the performance variations among different
networks are much more visible.

633



SRC PE This system is similar to a NMT sys-
tem used for bilingual translation from a source
language to a target language. The parallel cor-
pus consist of source text and post-edits of MT
segments. We notice that the performance of this
system is below the MT Baseline indicating that
learning only from the source text is not enough to
improve the translation quality. Most likely, this
system generates (alternative, potentially correct)
translations that diverge from the MT output and
are thus penalized by automatic evaluation met-
rics that use human post-edits as references. This
can be confirmed from the fact that when we used
the reference test set for evaluation,4 the APE sys-
tem outperformed the MT Baseline by +4.2 BLEU
points (47.97 vs 43.79 BLEU scores).

MT PE This is a single-source neural APE sys-
tem similar to the previous one. However, in this
case the objective is “monolingual” translation as
opposed to bilingual in the previous case. Both
source and target languages are the same, and the
goal is to translate rough MT segments into their
corrected version. The results in Table 1 show
that learning from machine-translated text is bet-
ter than learning from the corresponding source
sentences (-3.2 TER and +6.0 BLEU points over
the MT Baseline). Though quite large, the perfor-
mance gain does not indicate if all the MT seg-
ments are improved. To better understand this as-
pect, we use the precision metric (as defined in
Section 4.2). A precision of 72% for this system
indicates that the majority of the MT segments that
are modified results in a better translation qual-
ity. The remaining 28% of deteriorated sentences
gives evidence of the “over-correction” problem
discussed in last years’ APE task overview (Bojar
et al., 2016).

MT+SRC PE One limitation of the “monolin-
gual translation” approach is that the APE system
is only trained on data in the target language, dis-
regarding information about the source language:
mappings learned from (mt, pe) pairs lose the con-
nection between the translated words (or phrases)
and the corresponding source terms (src). This im-
plies that information lost or distorted in the trans-
lation process is out of the reach of the APE com-
ponent, and the resulting errors are impossible to
recover. To overcome this limitation and to lever-
age both source and MT output, we introduced the

4http://hdl.handle.net/11234/1-2334

Systems TER BLEU Prec.
(%)

MT Baseline 24.81 62.92 -
SRC PE 26.66 61.91 49.07
MT PE 21.57 69.09 72.01
MT+SRC PE 19.77 70.72 78.22
MT+SRC PE TSL 20.07 70.52 78.77
Ens8 19.26 71.63 78.50
Ens8+Re-rank-A 19.22† 71.89† 78.84
Ens8+Re-rank-AB 19.35 70.94 78.07

Table 1: Performance of the APE systems on dev.
2016 (en-de) (“†” indicates statistically significant
differences wrt. MT Baseline with p<0.05).

Systems TER BLEU Precision
(%)

MT Baseline 15.58 79.46 -
SRC PE 28.50 58.17 20.22
MT PE 15.97 78.43 36.29
MT+SRC PE 15.61 78.59 44.67
MT+SRC PE TSL 15.89 78.48 42.58
Ens8 15.14 79.41 54.18
Ens8+Re-rank-A 15.04† 80.00† 68.86

Table 2: Performance of the APE systems on dev.
2017 (de-en) (“†” indicates statistically significant
differences wrt. MT Baseline with p<0.05).

multi-source neural sequence-to-sequence model
described in §3. Our multi-source neural APE
model clearly outperforms the strong monolingual
single-source model (-1.8 TER and +1.6 BLEU).
The improvement is also visible in terms of preci-
sion (+8.2%), which indicates that the source seg-
ment might be useful to disambiguate if the MT
word should be corrected or kept untouched, thus
helping to mitigate the over-correction problem.

MT+SRC PE TSL The low TER score of the
MT baseline (24.8 and 15.5 respectively for en-
de and de-en) indicates that the majority of the
MT words are correct. In order to induce a con-
servative approach (in other words, to induce the
APE system to preserve the correct MT words) we
use a task-specific loss (TSL) function that takes
into consideration the attention score of the MT
words before computing the target word probabil-
ities. The attention scores can act as a reward to
the target words that are present in the MT seg-
ment. To this aim, first we add the attention scores

634



from the mt encoder (Equation 8) to the respec-
tive target words in the softmax layer. Then, we
apply softmax to obtain the target word probabili-
ties. More formally:

pΘ(yt|y<t, Xsrc, Xmt) =
eytdec +

∑
eytenc∑

y′(e
y′
dec +

∑
ey

′
enc)

(14)
where, eydec and e

y
enc are respectively the scores of

the target word computed by the decoder layer and
the attention layer of the mt encoder (eyenc = 0 if
y /∈ MT ). Since a target word can occur multiple
times in the MT segment, we sum the scores of all
occurrences. In case a target word is not present in
the MT segment the score is 0.

Ensemble (Ens8) In order to leverage all the
network architectures discussed above, we ensem-
ble the two best models for each of them. Since
the networks are very diverse in terms of informa-
tion learned from the input representation we ob-
served that weighing all the models equally does
not improve over the single system. Therefore,
we generate 50-best hypothesis from the ensem-
ble system and then tune the model weights with
Batch-MIRA (Cherry and Foster, 2012) on the de-
velopment set to maximize the BLEU score. We
observe that, after 3 cycles of decoding and tuning,
the performance converges. The weighted ensem-
ble of 8 models further improves the translation
quality (-0.8 TER and +1.1 BLEU) over the best
single multi-source model (MT+SRC PE).

Re-ranking Following the improvements ob-
tained by re-ranking n-best hypotheses as shown
in (Pal et al., 2017), we use a re-ranker in our
submissions with two different sets of features:

Edit Distance (Re-rank-A) The first set con-
sists of shallow features that can be easily
extracted on-the-fly. It captures different types of
edit operations performed by an APE system over
the MT output. These features include number
of insertions, deletions, substitutions, shifts,
and length ratio between the MT segment and
each APE hypothesis, computed using TER. In
addition, we compute precision and recall of the
APE hypotheses in order to avoid over-correction
by rewarding the hypotheses that are closer to the
MT segment. Precision is the percentage of words
generated by the APE system that are present
in the MT segment, and recall is the percentage
of words in the MT segment that are generated

by the APE system. The feature weights are
optimized with Batch-MIRA on the development
set to maximize the BLEU score. Re-ranking
with these features gave further improvements
over the ensemble system. Since this is the best
configuration (as seen from Table 1 and 2), we
evaluate this system on the 2016 APE test set. The
results of this evaluation are reported in Table 3.
We observe that this system achieves significant
improvement over the MT baseline (-5.4 TER and
8.7 BLEU points) also on the 2016 test set.

Systems TER BLEU
MT Baseline 24.76 62.11
APE Baseline 24.64 63.47
Ens8+Re-rank-A 19.32† 70.88†

Table 3: Performance of the APE systems on
the 2016 test set (en-de) (“†” indicates statisti-
cally significant differences wrt. MT Baseline
with p<0.05).

Statistical (Re-rank-AB) This re-ranker is similar
to the one used in (Pal et al., 2017). The feature
set consists of the log probability given by the
neural models itself, the statistical n-gram lan-
guage model probability as well as the perplexity
normalized by sentence length, and features from
operation sequence model. In addition to this,
we also integrate all the features used by the
previous re-ranker, following the same procedure
to optimize their weights. The result of this
system is reported in Table 1 (Ens8+Re-rank-AB).
We observe that this re-ranker does not yield
performance improvements, probably due to
over-fitting. We leave further investigations on
this aspect for future work.

5 Results on Test Data

The shared task evaluation has been carried out
on 2,000 unseen samples consisting of src and mt
pairs from the same domain of the training data.
Our primary submission is Ens8+Re-rank-A (in
Table 1 and 2) that is a weighted ensemble of 8
neural APE models (2 best models from SRC PE,
MT PE, MT+SRC PE, and MT+SRC PE TSL).
As a contrastive submission, we wanted to eval-
uate the performance of a simpler system with a
higher throughput. Therefore, we select a sin-
gle best multi-source model (MT+SRC PE) with
a re-ranker that is based only on edit-distance fea-

635



tures (labelled as Contrastive-A in Table 4). For
en-de we also submitted (Ens8+Re-rank-AB) an-
other contrastive system that is based on ensem-
ble system plus the whole set of re-ranking fea-
tures (labelled as Contrastive-B in Table 4). Ac-
cording to the shared task results, as reported
in Table 4, our primary and contrastive submis-
sions achieve significant improvement over the
MT baseline for both language directions. It is
interesting to note that our contrastive-A submis-
sion, which is a much simpler version of the full-
fledged system, performs almost similar to our
primary submission for de-en and slightly worse
(+0.7 TER points) for en-de.

Systems en-de de-enTER BLEU TER BLEU
MT Baseline 24.48 62.49 15.55 79.54
APE Baseline 24.69 62.97 15.74 79.28
Primary 19.6 70.07 15.29 79.82
Contrastive-A 20.3 69.11 15.31 79.64
Contrastive-B 21.55 67.28 - -

Table 4: Official results on 2017 test set.

6 Conclusion

Based on the lessons learned from previous work
on APE, which suggest that the dependency of
MT errors from the source sentence can be ex-
ploited by jointly learning from source and target
information, we developed a multi-source NMT
system. Our implementation extends the existing
NMT toolkit (Nematus) to train multi-source APE
systems that learn from source and MT text to-
gether in order to increase robustness and preci-
sion. We trained several networks with different
input representation (single-source/multi-source)
to finally built an ensemble of 8 neural models.
The n-best hypotheses generated by this ensem-
ble were further re-ranked using features based on
the edit distance between the original MT output
and each APE hypothesis, as well as other statis-
tical models (n-gram language model and opera-
tion sequence model). On the en-de and de-en test
data released for the WMT 2017 APE shared task,
our primary submissions achieved significant im-
provements over the task baselines, which we out-
performed by a large margin (+7.6 and +0.3 BLEU
points on en-de and de-en) ranking first on both
language directions.

Acknowledgments

This work has been partially supported by the EC-
funded H2020 project QT21 (grant agreement no.
645452).

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-

gio. 2014. Neural machine translation by jointly
learning to align and translate. arXiv preprint
arXiv:1409.0473 .

Hanna Béchara, Yanjun Ma, and Josef van Genabith.
2011. Statistical Post-Editing for a Statistical MT
System. In Proceedings of the 13th Machine Trans-
lation Summit. Xiamen, China, pages 308–315.

Ondrej Bojar, Christian Buck, Christian Federmann,
Barry Haddow, Philipp Koehn, Johannes Leveling,
Christof Monz, Pavel Pecina, Matt Post, Herve
Saint-Amand, Radu Soricut, Lucia Specia, and Aleš
Tamchyna. 2014. Findings of the 2014 workshop
on statistical machine translation. In Proceedings of
the Ninth Workshop on Statistical Machine Trans-
lation. Association for Computational Linguistics,
Baltimore, Maryland, USA, pages 12–58.

Ondřej Bojar, Rajen Chatterjee, Christian Federmann,
Yvette Graham, Barry Haddow, Matthias Huck,
Antonio Jimeno Yepes, Philipp Koehn, Varvara
Logacheva, Christof Monz, Matteo Negri, Aure-
lie Neveol, Mariana Neves, Martin Popel, Matt
Post, Raphael Rubino, Carolina Scarton, Lucia Spe-
cia, Marco Turchi, Karin Verspoor, and Marcos
Zampieri. 2016. Findings of the 2016 conference
on machine translation. In Proceedings of the First
Conference on Machine Translation. Association for
Computational Linguistics, Berlin, Germany, pages
131–198.

Ondřej Bojar, Rajen Chatterjee, Christian Federmann,
Barry Haddow, Matthias Huck, Chris Hokamp,
Philipp Koehn, Varvara Logacheva, Christof Monz,
Matteo Negri, Matt Post, Carolina Scarton, Lucia
Specia, and Marco Turchi. 2015. Findings of the
2015 workshop on statistical machine translation.
In Proceedings of the Tenth Workshop on Statisti-
cal Machine Translation. Association for Computa-
tional Linguistics, Lisbon, Portugal, pages 1–46.

Rajen Chatterjee, José G. C. de Souza, Matteo Ne-
gri, and Marco Turchi. 2016. The fbk participa-
tion in the wmt 2016 automatic post-editing shared
task. In Proceedings of the First Conference on Ma-
chine Translation. Association for Computational
Linguistics, Berlin, Germany, pages 745–750.

Rajen Chatterjee, Gebremedhen Gebremelak, Matteo
Negri, and Marco Turchi. 2017. Online automatic
post-editing for mt in a multi-domain translation en-
vironment. In Proceedings of the 15th Conference

636



of the European Chapter of the Association for Com-
putational Linguistics: Volume 1, Long Papers. As-
sociation for Computational Linguistics, Valencia,
Spain, pages 525–535.

Rajen Chatterjee, Marco Turchi, and Matteo Negri.
2015a. The fbk participation in the wmt15 auto-
matic post-editing shared task. In Proceedings of the
Tenth Workshop on Statistical Machine Translation.
pages 210–215.

Rajen Chatterjee, Marion Weller, Matteo Negri, and
Marco Turchi. 2015b. Exploring the Planet of
the APEs: a Comparative Study of State-of-the-art
Methods for MT Automatic Post-Editing. In Pro-
ceedings of the 53rd Annual Meeting of the Associa-
tion for Computational Linguistics). Beijing, China.

Colin Cherry and George Foster. 2012. Batch tuning
strategies for statistical machine translation. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies. Asso-
ciation for Computational Linguistics, Stroudsburg,
PA, USA, NAACL HLT ’12, pages 427–436.

Kyunghyun Cho, Bart Van Merriënboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using rnn encoder-decoder
for statistical machine translation. arXiv preprint
arXiv:1406.1078 .

John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive Subgradient Methods for Online Learning
and Stochastic Optimization. Journal of Machine
Learning Research .

Loı̈c Dugast, Jean Senellart, and Philipp Koehn. 2007.
Statistical post-editing on systran’s rule-based trans-
lation system. In Proceedings of the Second Work-
shop on Statistical Machine Translation. Associa-
tion for Computational Linguistics, pages 220–223.

M Amin Farajian, Rajen Chatterjee, Costanza Con-
forti, Shahab Jalalvand, Vevake Balaraman, Mat-
tia A Di Gangi, Duygu Ataman, Marco Turchi, Mat-
teo Negri, and Marcello Federico. 2016. Fbks neu-
ral machine translation systems for iwslt 2016. In
Proceedings of the ninth International Workshop on
Spoken Language Translation, USA.

Yarin Gal and Zoubin Ghahramani. 2016. A theoret-
ically grounded application of dropout in recurrent
neural networks. In Advances in Neural Information
Processing Systems. pages 1019–1027.

Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky,
Ilya Sutskever, and Ruslan R Salakhutdinov. 2012.
Improving neural networks by preventing co-
adaptation of feature detectors. arXiv preprint
arXiv:1207.0580 .

Pierre Isabelle, Cyril Goutte, and Michel Simard. 2007.
Domain adaptation of mt systems through automatic
post-editing .

Marcin Junczys-Dowmunt and Roman Grundkiewicz.
2016. Log-linear combinations of monolingual and
bilingual neural machine translation models for au-
tomatic post-editing. In Proceedings of the First
Conference on Machine Translation. Association for
Computational Linguistics, Berlin, Germany, pages
751–758.

A-L Lagarda, Vicente Alabau, Francisco Casacu-
berta, Roberto Silva, and Enrique Diaz-de Liano.
2009. Statistical post-editing of a rule-based ma-
chine translation system. In Proceedings of Human
Language Technologies: The 2009 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, Companion Vol-
ume: Short Papers. Association for Computational
Linguistics, pages 217–220.

Antonio L Lagarda, Daniel Ortiz-Martı́nez, Vicent Al-
abau, and Francisco Casacuberta. 2015. Translat-
ing without in-domain corpus: Machine translation
post-editing with online learning techniques. Com-
puter Speech & Language 32(1):109–134.

Jindřich Libovický, Jindřich Helcl, Marek Tlustý,
Ondřej Bojar, and Pavel Pecina. 2016. Cuni system
for wmt16 automatic post-editing and multimodal
translation tasks. In Proceedings of the First Confer-
ence on Machine Translation. Association for Com-
putational Linguistics, Berlin, Germany, pages 646–
654.

Santanu Pal, Sudip Kumar Naskar, Mihaela Vela, Qun
Liu, and Josef van Genabith. 2017. Neural auto-
matic post-editing using prior alignment and rerank-
ing. In Proceedings of the 15th Conference of the
European Chapter of the Association for Computa-
tional Linguistics: Volume 2, Short Papers. Associa-
tion for Computational Linguistics, Valencia, Spain,
pages 349–355.

Santanu Pal, Sudip Kumar Naskar, Mihaela Vela, and
Josef van Genabith. 2016a. A neural network based
approach to automatic post-editing. In Proceed-
ings of the 54th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 2: Short
Papers). Association for Computational Linguistics,
Berlin, Germany, pages 281–286.

Santanu Pal, Marcos Zampieri, and Josef van Genabith.
2016b. USAAR: An Operation Sequential Model
for Automatic Statistical Post-Editing. In Proceed-
ings of the 11th Workshop on Statistical Machine
Translation (WMT).

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of
the 40th annual meeting on association for compu-
tational linguistics. pages 311–318.

Marion Potet, Laurent Besacier, Hervé Blanchon, and
Marwen Azouzi. 2012. Towards a better under-
standing of statistical post-edition usefulness. In
IWSLT . pages 284–291.

637



Rudolf Rosa, David Marecek, and Ales Tamchyna.
2013. Deepfix: Statistical post-editing of statisti-
cal machine translation using deep syntactic anal-
ysis. In ACL (Student Research Workshop). pages
172–179.

Rico Sennrich, Orhan Firat, Kyunghyun Cho, Alexan-
dra Birch, Barry Haddow, Julian Hitschler, Marcin
Junczys-Dowmunt, Samuel Läubli, Antonio Valerio
Miceli Barone, Jozef Mokry, and Maria Nadejde.
2017. Nematus: a toolkit for neural machine transla-
tion. In Proceedings of the Software Demonstrations
of the 15th Conference of the European Chapter of
the Association for Computational Linguistics. As-
sociation for Computational Linguistics, Valencia,
Spain, pages 65–68.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016a. Edinburgh Neural Machine Translation Sys-
tems for WMT 16. In Proceedings of the First
Conference on Machine Translation. Association for
Computational Linguistics, Berlin, Germany, pages
371–376.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016b. Neural machine translation of rare words
with subword units. In Proceedings of the 54th
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers). Asso-
ciation for Computational Linguistics, Berlin, Ger-
many, pages 1715–1725.

Michel Simard, Cyril Goutte, and Pierre Isabelle.
2007. Statistical phrase-based post-editing. In Hu-
man Language Technology Conference of the North
American Chapter of the Association of Computa-
tional Linguistics, Proceedings, April 22-27, 2007,
Rochester, New York, USA. pages 508–515.

Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: a simple way to prevent neural networks
from overfitting. Journal of Machine Learning Re-
search 15(1):1929–1958.

Matthew D Zeiler. 2012. ADADELTA: an adap-
tive learning rate method. arXiv preprint
arXiv:1212.5701 .

638


