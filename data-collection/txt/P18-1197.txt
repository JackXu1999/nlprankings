



















































To Attend or not to Attend: A Case Study on Syntactic Structures for Semantic Relatedness


Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 2116–2125
Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics

2116

To Attend or not to Attend:
A Case Study on Syntactic Structures for Semantic Relatedness

Amulya Gupta
Iowa State University

guptaam@iastate.edu

Zhu Zhang
Iowa State University

zhuzhang@iastate.edu

Abstract

With the recent success of Recurrent Neu-
ral Networks (RNNs) in Machine Trans-
lation (MT), attention mechanisms have
become increasingly popular. The pur-
pose of this paper is two-fold; firstly,
we propose a novel attention model on
Tree Long Short-Term Memory Networks
(Tree-LSTMs), a tree-structured general-
ization of standard LSTM. Secondly, we
study the interaction between attention
and syntactic structures, by experimenting
with three LSTM variants: bidirectional-
LSTMs, Constituency Tree-LSTMs, and
Dependency Tree-LSTMs. Our models
are evaluated on two semantic relatedness
tasks: semantic relatedness scoring for
sentence pairs (SemEval 2012, Task 6 and
SemEval 2014, Task 1) and paraphrase de-
tection for question pairs (Quora, 2017).1

1 Introduction

Recurrent Neural Networks (RNNs), in par-
ticular Long Short-Term Memory Networks
(LSTMs) (Hochreiter and Schmidhuber, 1997),
have demonstrated remarkable accomplishments
in Natural Language Processing (NLP) in recent
years. Several tasks such as information extrac-
tion, question answering, and machine transla-
tion have benefited from them. However, in their
vanilla forms, these networks are constrained by
the sequential order of tokens in a sentence. To
mitigate this limitation, structural (dependency or
constituency) information in a sentence was ex-
ploited and witnessed partial success in various
tasks (Goller and Kuchler, 1996; Yamada and

1Our code for experiments on the SICK dataset is pub-
licly available at https://github.com/amulyahwr/
acl2018

Knight, 2001; Quirk et al., 2005; Socher et al.,
2011; Tai et al., 2015).

On the other hand, alignment techniques
(Brown et al., 1993) and attention mechanisms
(Bahdanau et al., 2014) act as a catalyst to aug-
ment the performance of classical Statistical Ma-
chine Translation (SMT) and Neural Machine
Translation (NMT) models, respectively. In short,
both approaches focus on sub-strings of source
sentence which are significant for predicting target
words while translating. Currently, the combina-
tion of linear RNNs/LSTMs and attention mecha-
nisms has become a de facto standard architecture
for many NLP tasks.

At the intersection of sentence encoding and
attention models, some interesting questions
emerge: Can attention mechanisms be employed
on tree structures, such as Tree-LSTMs (Tai et al.,
2015)? If yes, what are the possible tree-based at-
tention models? Do different tree structures (in
particular constituency vs. dependency) have dif-
ferent behaviors in such models? With these ques-
tions in mind, we present our investigation and
findings in the context of semantic relatedness
tasks.

2 Background

2.1 Long Short-Term Memory Networks
(LSTMs)

Concisely, an LSTM network (Hochreiter and
Schmidhuber, 1997) (Figure 1) includes a memory
cell at each time step which controls the amount
of information being penetrated into the cell, ne-
glected, and yielded by the cell. Various LSTM
networks (Greff et al., 2017) have been explored
till now; we focus on one representative form. To
be more precise, we consider a LSTM memory
cell involving: an input gate it, a forget gate ft,
and an output gate ot at time step t. Apart from

https://github.com/amulyahwr/acl2018
https://github.com/amulyahwr/acl2018


2117

w0 w1

h0
c0

h1
c1

y0 y1

h2
c2 ...

Figure 1: A linear LSTM network. wt is the word
embedding, ht is the hidden state vector, ct is the
memory cell vector and yt is the final processed
output at time step t.

the hidden state ht−1 and input embedding wt of
the current word, the recursive function in LSTM
also takes the previous time’s memory cell state,
ct−1, into account, which is not the case in sim-
ple RNN. The following equations summarize a
LSTM memory cell at time step t:

it = σ(wtW
i + ht−1R

i + bi) (1)

ft = σ(wtW
f + ht−1R

f + bf ) (2)

ot = σ(wtW
o + ht−1R

o + bo) (3)

ut = tanh(wtW
u + ht−1R

u + bu) (4)

ct = it � ut + ft � ct−1 (5)

ht = ot � tanh(ct) (6)

where:

• (W i, W f , W o, Wu) ∈ RD x d represent in-
put weight matrices, where d is the dimension
of the hidden state vector and D is the dimen-
sion of the input word embedding, wt .

• (Ri, Rf , Ro, Ru) ∈ Rd x d represent recur-
rent weight matrices and (bi, bf , bo, bu) ∈
Rd represent biases.

• ct ∈Rd is the new memory cell vector at time
step t.

As can be seen in Eq. 5, the input gate it lim-
its the new information, ut, by employing the el-
ement wise multiplication operator �. Moreover,
the forget gate ft regulates the amount of infor-
mation from the previous state ct−1. Therefore,
the current memory state ct includes both new and
previous time step’s information but partially.

John ate an apple

nsubj

dobj

det

Figure 2: a. Left: A constituency tree; b. Right:
A dependency tree

A natural extension of LSTM network is a bidi-
rectional LSTM (bi-LSTM), which lets the se-
quence pass through the architecture in both direc-
tions and aggregate the information at each time
step. Again, it strictly preserves the sequential na-
ture of LSTMs.

2.2 Linguistically Motivated Sentence
Structures

Most computational linguists have developed a
natural inclination towards hierarchical structures
of natural language, which follow guidelines col-
lectively referred to as syntax. Typically, such
structures manifest themselves in parse trees. We
investigate two popular forms: Constituency and
Dependency trees.

2.2.1 Constituency structure
Briefly, constituency trees (Figure 2:a) indicate a
hierarchy of syntactic units and encapsulate phrase
grammar rules. Moreover, these trees explic-
itly demonstrate groups of phrases (e.g., Noun
Phrases) in a sentence. Additionally, they discrim-
inate between terminal (lexical) and non-terminal
nodes (non-lexical) tokens.

2.2.2 Dependency structure
In short, dependency trees (Figure 2:b) describe
the syntactic structure of a sentence in terms of the
words (lemmas) and associated grammatical rela-
tions among the words. Typically, these depen-
dency relations are explicitly typed, which makes
the trees valuable for practical applications such as
information extraction, paraphrase detection and
semantic relatedness.

2.3 Tree Long Short-Term Memory Network
(Tree-LSTM)

Child-Sum Tree-LSTM (Tai et al., 2015) is an epit-
ome of structure-based neural network which ex-
plicitly capture the structural information in a sen-
tence. Tai et al. demonstrated that information at



2118

HC0

IP OP UP

HC1

wP

CC0 CC1
fC1fC0

CP

HP

IP UP

OP

Parent	node

Child	nodeChild	node

wP : word embedding 
of parent node

HP ,HC0, HC1: hidden 
state vectors of parent, 
first child and second 
child respectively

CP ,CC0, CC1: memory 
cell state vectors of 
parent, first child and 
second child 
respectively

IP, OP: Input and Output 
gate vectors for parent 
node respectively

fC0, fC1 : Forget gate 
vectors for first and 
second child 
respectively

Figure 3: A compositional view of parent node in
Tree-LSTM network.

a parent node can be consolidated selectively from
each of its child node. Architecturally, each gated
vector and memory state update of the head node
is dependent on the hidden states of its children in
the Tree-LSTM. Assuming a good tree structure
of a sentence, each node j of the structure incorpo-
rates the following equations.:

h̃j =
∑

k∈C(j)

hk (7)

ij = σ(wjW
i + h̃jR

i + bi) (8)

fjk = σ(wjW
f + hkR

f + bf ) (9)

oj = σ(wjW
o + h̃jR

o + bo) (10)

uj = tanh(wjW
u + h̃jR

u + bu) (11)

cj = ij � uj +
∑

k∈C(j)

fjk � ck (12)

hj = oj � tanh(cj) (13)

where:

• wj ∈ RD represents word embedding of all
nodes in Dependency structure and only ter-
minal nodes in Constituency structure. 2

• (W i, W f , W o, Wu) ∈ RD x d represent in-
put weight matrices.

• (Ri, Rf , Ro, Ru) ∈ Rd x d represent recur-
rent weight matrices, and (bi, bf , bo, bu) ∈
Rd represent biases.

2wj is ignored for non-terminal nodes in a Constituency
structure by removing the wW terms in Equations 8-11.

w0 w1

h0

c0

h1

c1

w’0 w’1

h’1

c’1

a1 (Global align
weights)

c1 (context 
vector)

ĥ’1Attention layer

Figure 4: Global attention model

• cj ∈ Rd is the new memory state vector of
node j.

• C(j) is the set of children of node j.

• fjk ∈ Rd is the forget gate vector for child k
of node j.

Referring to Equation 12, the new memory cell
state, cj of node j, receives new information, uj ,
partially. More importantly, it includes the partial
information from each of its direct children, set
C(j), by employing the corresponding forget gate,
fjk.

When the Child-Sum Tree model is deployed
on a dependency tree, it is referred to as Depen-
dency Tree-LSTM, whereas a constituency-tree-
based instantiation is referred to as Constituency
Tree-LSTM.

2.4 Attention Mechanisms
Alignment models were first introduced in sta-
tistical machine translation (SMT) (Brown et al.,
1993), which connect sub-strings in the source
sentence to sub-strings in the target sentence.

Recently, attention techniques (which are ef-
fectively soft alignment models) in neural ma-
chine translation (NMT) (Bahdanau et al., 2014)
came into prominence, where attention scores are
calculated by considering words of source sen-
tence while decoding words in target language.
Although effective attention mechanisms (Luong
et al., 2015) such as Global Attention Model
(GAM) (Figure 4) and Local Attention Model
(LAM) have been developed, such techniques
have not been explored over Tree-LSTMs.

3 Inter-Sentence Attention on
Tree-LSTMs

We present two types of tree-based attention mod-
els in this section. With trivial adaptation, they can



2119

be deployed in the sequence setting (degenerated
trees).

3.1 Modified Decomposable Attention
(MDA)

Parikh et al. (2016)’s original decomposable inter-
sentence attention model only used word embed-
dings to construct the attention matrix, without
any structural encoding of sentences. Essentially,
the model incorporated three components:

Attend: Input representations (without se-
quence or structural encoding) of both sentences,
L and R, are soft-aligned.

Compare: A set of vectors is produced by sep-
arately comparing each sub-phrase of L to sub-
phrases in R. Vector representation of each sub-
phrase in L is a non-linear combination of rep-
resentation of word in sentence L and its aligned
sub-phrase in sentence R. The same holds true for
the set of vectors for sentence R.

Aggregate: Both sets of sub-phrases vectors are
summed up separately to form final sentence rep-
resentation of sentence L and sentence R.

We decide to augment the original decompos-
able inter-sentence attention model and general-
ize it into the tree (and sequence) setting. To be
more specific, we consider two input sequences:
L = (l1, l2....llenL), R = (r1, r2....rlenR) and
their corresponding input representations: L̄ = (l̄1,
l̄2....l̄lenL), R̄ = (r̄1, r̄2....r̄lenR); where lenL and
lenR represents number of words in L and R, re-
spectively.

3.1.1 MDA on dependency structure
Let’s assume sequences L andR have dependency
tree structures DL and DR. In this case, lenL and
lenR represents number of nodes in DL and DR,
respectively. After using a Tree-LSTM to encode
tree representations, which results in: D

′
L = (l̄

′
1,

l̄
′
2....l̄

′
lenL

), D
′
R = (r̄

′
1, r̄

′
2....r̄

′
lenR

), we gather un-
normalized attention weights, eij and normalize
them as follows:

eij = l̄
′
i(r̄

′
j)
T (14)

βi =

lenR∑
j=1

exp(eij)∑lenR
k=1 exp(eik)

∗ r̄′j (15)

αj =

lenL∑
i=1

exp(eij)∑lenL
k=1 exp(ekj)

∗ l̄′i (16)

From the equations above, we can infer that
the attention matrix will have a dimension lenL

x lenR. In contrast to the original model, we com-
pute the final representations of the each sentence
by concatenating the LSTM-encoded representa-
tion of root with the attention-weighted represen-
tation of the root 3:

h
′′
L = G([l̄

′
rootL

;βrootL ]) (17)

h
′′
R = G([r̄

′
rootR

;αrootR ]) (18)

where G is a feed-forward neural network. h
′′
L

and h
′′
R are final vector representations of input se-

quences L and R, respectively.

3.1.2 MDA on constituency structure
Let’s assume sequences L and R have con-
stituency tree structures CL and CR. Moreover,
assume CL and CR have total number of nodes
as NL (> lenL) and NR (> lenR), respectively.
As in 3.1.1, the attention mechanism is employed
after encoding the trees CL and CR. While en-
coding trees, terminal and non-terminal nodes are
handled in the same way as in the original Tree-
LSTM model (see 2.3).

It should be noted that we collect hidden states
of all the nodes (NL and NR) individually in CL
and CR during the encoding process. Hence, hid-
den states matrix will have dimension NL x d for
tree CL whereas for tree CR, it will have dimen-
sion NR x d; where d is dimension of each hidden
state. Therefore, attention matrix will have a di-
mension NL x NR. Finally, we employ Equations
14-18 to compute the final representations of se-
quences L and R.

3.2 Progressive Attention (PA)

In this section, we propose a novel attention mech-
anism on Tree-LSTM, inspired by (Quirk et al.,
2005) and (Yamada and Knight, 2001).

3.2.1 PA on dependency structure
Let’s assume a dependency tree structure of sen-
tence L = (l1, l2....llenL) is available as DL; where
lenL represents number of nodes in DL. Simi-
larly, tree DR corresponds to the sentence R =
(r1, r2....rlenR); where lenR represents number of
nodes in DR.

In PA, the objective is to produce the final vec-
tor representation of tree DR conditional on the
hidden state vectors of all nodes of DL. Similar to

3In the sequence setting, we compute the corresponding
representations for the last word in the sentence.



2120

the encoding process in NMT, we encode R by at-
tending each node of DR to all nodes in DL. Let’s
name this process Phase1. Next, Phase2 is per-
formed where L is encoded in the similar way to
get the final vector representation of DL.

Referring to Figure 5 and assuming Phase1 is
being executed, a hidden state matrix, HL, is ob-
tained by concatenating the hidden state vector of
every node in tree DL, where the number of nodes
inDL = 3. Next, treeDR is processed by calculat-
ing the hidden state vector at every node. Assume
that the current node being processed is nR2 of
DR, which has a hidden state vector, hR2. Before
further processing, normalized weights are calcu-
lated based on hR2 and HL. Formally,

Hpj = stack[hpj ] (19)

conpj = concat[Hpj , Hq] (20)

apj = softmax(tanh(conpjWc+b)∗Wa) (21)

where:

• p, q ∈ {L,R} and q 6= p

• Hq ∈ Rx x d represents a matrix obtained by
concatenating hidden state vectors of nodes
in tree Dq; x is lenq of sentence q.

• Hpj ∈Rx x d represents a matrix obtained by
stacking hidden state, hpj , vertically x times.

• conpj ∈ Rx x 2d represents the concatenated
matrix.

• apj ∈ Rx represents the normalized atten-
tion weights at node j of tree Dp; where Dp
is the dependency structure of sentence p.

• Wc ∈R2d x d and Wa ∈Rd represent learned
weight matrices.

The normalized attention weights in above
equations provide an opportunity to align the sub-
tree at the current node, nR2, in DR to sub-trees
available at all nodes in DL. Next, a gated mecha-
nism is employed to compute the final vector rep-
resentation at node nR2.

Formally,

h
′
pj =

(x−1)∑
0

((1− apj) ∗Hq + (apj) ∗Hpj) (22)

where:

• h′pj ∈ Rd represents the final vector repre-
sentation of node j in tree Dp

•
∑(x−1)

0 represents column-wise sum

Assuming the final vector representation of tree
DR is h

′
R, the exact same steps are followed for

Phase2 with the exception that the entire process is
now conditional on tree DR. As a result, the final
vector representation of tree DL, h

′
L, is computed.

Lastly, the following equations are applied to
vectors h

′
L and h

′
R, before calculating the angle

and distance similarity (see Section 4).

h
′′
L = tanh(h

′
L + hL) (23)

h
′′
R = tanh(h

′
R + hR) (24)

where:

• hL ∈ Rd represents the vector representation
of tree DL without attention.

• hR ∈Rd represents the vector representation
of tree DR without attention.

3.2.2 PA on constituency structure
Let CL and CR represent constituency trees of L
and R, respectively; where CL and CR have total
number of nodes NL (> lenL) and NR (> lenR).
Additionally, let’s assume that trees CL and CR
have the same configuration of nodes as in Sec-
tion 3.1.2, and the encoding of terminal and non-
terminal nodes follow the same process as in Sec-
tion 3.1.2. Assuming we have already encoded all
NL nodes of tree CL using Tree-LSTM, we will
have the hidden state matrix, HL, with dimension
NL x d. Next, while encoding any node of CR, we
consider HL which results in an attention vector
having shape NL. Using Equations 19-22 4, we
retrieve the final hidden state of the current node.
Finally, we compute the representation of sentence
R based on attention to sentence L. We perform
Phase2 with the same process, except that we now
condition on sentence R.

In summary, the progressive attention mecha-
nism refers to all nodes in the other tree while en-
coding a node in the current tree, instead of wait-
ing till the end of the structural encoding to estab-
lish cross-sentence attention, as was done in the
decomposable attention model.

4At this point, we will consider Cq and Cp instead of
Dq and Dp, respectively, in Equations 19-22. Additionally,
x will be equal to total number of nodes in the constituency
tree.



2121

nL1nL0

nL2

hL2

hL0 hL1

hL0
hL2
hL1

3 x 150

HL

Sentence L

nR1nR0

nR2

hR2

hR0 hR1

Sentence R

HL

wtL0

wtL1
wtL2

3 x 1

aR2
(normalized)

Phase 1 Phase 2

h+ hx

output

Start

1-wtL0

1-wtL1
1-wtL2

h’R2

nR0 nR1

nR2

hR2

hR1hR0

hR1
hR2
hR0

3 x 150

HR

Sentence R

nL0 nL1

nL2

hL2

hL1hL0

Sentence L

HR

wtR1

wtR0
wtR2

3 x 1

aL2
(normalized)

1-wtR1

1-wtR0
1-wtR2

h’L2

h’’L
h’’R

Start

hL hR

h’R
h’L

Figure 5: Progressive Attn-Tree-LSTM model

4 Evaluation Tasks

We evaluate our models on two tasks: (1) seman-
tic relatedness scoring for sentence pairs (SemEval
2012, Task 6 and SemEval 2014, Task 1) and
(2) paraphrase detection for question pairs (Quora,
2017).

4.1 Semantic Relatedness for Sentence Pairs

In SemEval 2012, Task 6 and SemEval 2014, Task
1, every sentence pair has a real-valued score that
depicts the extent to which the two sentences are
semantically related to each other. Higher score
implies higher semantic similarity between the
two sentences. Vector representations h

′′
L and h

′′
R

are produced by using our Modified Decomp-Attn
or Progressive-Attn models. Next, a similarity
score, ŷ between h

′′
L and h

′′
R is computed using the

same neural network (see below), for the sake of
fair comparison between our models and the orig-
inal Tree-LSTM (Tai et al., 2015).

hx = h
′′
L � h

′′
R (25)

h+ = |h
′′
L − h

′′
R| (26)

hs = σ(hxW
x + h+W

+ + bh) (27)

p̂θ = softmax(hsW
p + bp) (28)

ŷ = rT p̂θ (29)

where:

• rT = [1, 2..S]

• hx ∈Rd measures the sign similarity between
h

′′
L and h

′′
R

• h+ ∈ Rd measures the absolute distance be-
tween h

′′
L and h

′′
R

Following (Tai et al., 2015), we convert the re-
gression problem into a soft classification. We also
use the same sparse distribution, p, which was de-
fined in the original Tree-LSTM to transform the
gold rating for a sentence pair, such that y = rT p
and ŷ = rT p̂θ ≈ y. The loss function is the KL-
divergence between p and p̂:

J(θ) =

∑m
k=1KL(p

k||p̂kθ)
m

+
λ||θ||22

2
(30)

• m is the number of sentence pairs in the
dataset.

• λ represents the regularization penalty.

4.2 Paraphrase Detection for Question Pairs
In this task, each question pair is labeled as either
paraphrase or not, hence the task is binary clas-
sification. We use Eqs. 25 - 28 to compute the



2122

predicted distribution p̂θ. The predicted label, ŷ,
will be:

ŷ = arg maxyp̂θ (31)

The loss function is the negative log-likelihood:

J(θ) = −
∑m

k=1 y
k log ŷk

m
+
λ||θ||22

2
(32)

5 Experiments

5.1 Semantic Relatedness for Sentence Pairs
We utilized two different datasets:

• The Sentences Involving Compositional
Knowledge (SICK) dataset (Marelli et al.
(2014)), which contains a total of 9,927
sentence pairs. Specifically, the dataset has
a split of 4500/500/4927 among training,
dev, and test. Each sentence pair has a score
S ∈ [1,5], which represents an average of
10 different human judgments collected by
crowd-sourcing techniques.

• The MSRpar dataset (Agirre et al., 2012),
which consists of 1,500 sentence pairs. In
this dataset, each pair is annotated with a
score S ∈ [0,5] and has a split of 750/750 be-
tween training and test.

We used the Stanford Parsers (Chen and Man-
ning, 2014; Bauer) to produce dependency and
constituency parses of sentences. Moreover,
we initialized the word embeddings with 300-
dimensional Glove vectors (Pennington et al.,
2014); the word embeddings were held fixed dur-
ing training. We experimented with different op-
timizers, among which AdaGrad performed the
best. We incorporated a learning rate of 0.025 and
regularization penalty of 10−4 without dropout.

5.2 Paraphrase Detection for Question Pairs
For this task, we utilized the Quora dataset (Iyer;
Kaggle, 2017). Given a pair of questions, the
objective is to identify whether they are seman-
tic duplicates. It is a binary classification prob-
lem where a duplicate question pair is labeled
as 1 otherwise as 0. The training set contains
about 400,000 labeled question pairs, whereas the
test set consists of 2.3 million unlabeled question
pairs. Moreover, the training dataset has only 37%
positive samples; average length of a question is
10 words. Due to hardware and time constraints,
we extracted 50,000 pairs from the original train-
ing while maintaining the same positive/negative

ratio. A stratified 80/20 split was performed on
this subset to produce the training/test set. Finally,
5% of the training set was used as a validation set
in our experiments.

We used an identical training configuration as
for the semantic relatedness task since the essence
of both the tasks is practically the same. We also
performed pre-processing to clean the data and
then parsed the sentences using Stanford Parsers.

6 Results

6.1 Semantic Relatedness for Sentence Pairs

Table 1 summarizes our results. According to
(Marelli et al., 2014), we compute three evalua-
tion metrics: Pearson’s r, Spearman’s ρ and Mean
Squared Error (MSE). We compare our attention
models against the original Tree-LSTM (Tai et al.,
2015), instantiated on both constituency trees and
dependency trees. We also compare earlier base-
lines with our models, and the best results are
in bold. Since Tree-LSTM is a generalization of
Linear LSTM, we also implemented our atten-
tion models on Linear Bidirectional LSTM (Bi-
LSTM). All results are average of 5 runs. It is wit-
nessed that the Progressive-Attn mechanism com-
bined with Constituency Tree-LSTM is overall the
strongest contender, but PA failed to yield any per-
formance gain on Dependency Tree-LSTM in ei-
ther dataset.

6.2 Paraphrase Detection for Question Pairs

Table 2 summarizes our results where best results
are highlighted in bold within each category. It
should be noted that Quora is a new dataset and
we have done our analysis on only 50,000 sam-
ples. Therefore, to the best of our knowledge,
there is no published baseline result yet. For this
task, we considered four standard evaluation met-
rics: Accuracy, F1-score, Precision and Recall.
The Progressive-Attn + Constituency Tree-LSTM
model still exhibits the best performance by a
small margin, but the Progressive-Attn mechanism
works surprisingly well on the linear bi-LSTM.

6.3 Effect of the Progressive Attention Model

Table 3 illustrates how various models operate on
two sentence pairs from SICK test dataset. As
we can infer from the table, the first pair demon-
strates an instance of the active-passive voice phe-
nomenon. In this case, the linear LSTM and
vanilla Tree-LSTMs really struggle to perform.



2123

Table 1: Results on test dataset for SICK and MSRpar semantic relatedness task. Mean scores are
presented based on 5 runs (standard deviation in parenthesis). Categories of results: (1) Previous models
(2) Dependency structure (3) Constituency structure (4) Linear structure

Dataset Model Pearson’s r Spearman’s ρ MSE

SICK

Illinois-LH (2014) 0.7993 0.7538 0.3692
UNAL-NLP (2014) 0.8070 0.7489 0.3550

Meaning factory (2014) 0.8268 0.7721 0.3224
ECNU (2014) 0.8414 - -

Dependency Tree-LSTM (2015) 0.8676 (0.0030) 0.8083 (0.0042) 0.2532 (0.0052)
Decomp-Attn (Dependency) 0.8239 (0.0120) 0.7614 (0.0103) 0.3326 (0.0223)

Progressive-Attn (Dependency) 0.8424 (0.0042) 0.7733 (0.0066) 0.2963 (0.0077)
Constituency Tree-LSTM (2015) 0.8582 (0.0038) 0.7966 (0.0053) 0.2734 (0.0108)

Decomp-Attn (Constituency) 0.7790 (0.0076) 0.7074 (0.0091) 0.4044 (0.0152)
Progressive-Attn (Constituency) 0.8625 (0.0032) 0.7997 (0.0035) 0.2610 (0.0057)

Linear Bi-LSTM 0.8398 (0.0020) 0.7782 (0.0041) 0.3024 (0.0044)
Decomp-Attn (Linear) 0.7899 (0.0055) 0.7173 (0.0097) 0.3897 (0.0115)

Progressive-Attn (Linear) 0.8550 (0.0017) 0.7873 (0.0020) 0.2761 (0.0038)

MSRpar

ParagramPhrase (2015) 0.426 - -
Projection (2015) 0.437 - -

GloVe (2015) 0.477 - -
PSL (2015) 0.416 - -

ParagramPhrase-XXL (2015) 0.448 - -
Dependency Tree-LSTM 0.4921 (0.0112) 0.4519 (0.0128) 0.6611 (0.0219)

Decomp-Attn (Dependency) 0.4016 (0.0124) 0.3310 (0.0118) 0.7243 (0.0099)
Progressive-Attn (Dependency) 0.4727 (0.0112) 0.4216 (0.0092) 0.6823 (0.0159)

Constituency Tree-LSTM 0.3981 (0.0176) 0.3150 (0.0204) 0.7407 (0.0170)
Decomp-Attn (Constituency) 0.3991 (0.0147) 0.3237 (0.0355) 0.7220 (0.0185)

Progressive-Attn (Constituency) 0.5104 (0.0191) 0.4764 (0.0112) 0.6436 (0.0346)
Linear Bi-LSTM 0.3270 (0.0303) 0.2205 (0.0111) 0.8098 (0.0579)

Decomp-Attn (Linear) 0.3763 (0.0332) 0.3025 (0.0587) 0.7290 (0.0206)
Progressive-Attn (Linear) 0.4773 (0.0206) 0.4453 (0.0250) 0.6758 (0.0260)

Table 2: Results on test dataset for Quora paraphrase detection task. Mean scores are presented based
on 5 runs (standard deviation in parenthesis). Categories of results: (1) Dependency structure (2) Con-
stituency structure (3) Linear structure

Model Accuracy F-1 score Precision Recall
(class=1) (class=1) (class=1)

Dependency Tree-LSTM 0.7897 (0.0009) 0.7060 (0.0050) 0.7298 (0.0055) 0.6840 (0.0139)
Decomp-Attn (Dependency) 0.7803 (0.0026) 0.6977 (0.0074) 0.7095 (0.0083) 0.6866 (0.0199)

Progressive-Attn (Dependency) 0.7896 (0.0025) 0.7113 (0.0087) 0.7214 (0.0117) 0.7025 (0.0266)
Constituency Tree-LSTM 0.7881 (0.0042) 0.7065 (0.0034) 0.7192 (0.0216) 0.6846 (0.0380)

Decomp-Attn (Constituency) 0.7776 (0.0004) 0.6942 (0.0050) 0.7055 (0.0069) 0.6836 (0.0164)
Progressive-Attn (Constituency) 0.7956 (0.0020) 0.7192 (0.0024) 0.7300 (0.0079) 0.7089 (0.0104)

Linear Bi-LSTM 0.7859 (0.0024) 0.7097 (0.0047) 0.7112 (0.0129) 0.7089 (0.0219)
Decomp-Attn (Linear) 0.7861 (0.0034) 0.7074 (0.0109) 0.7151 (0.0135) 0.7010 (0.0315)

Progressive-Attn (Linear) 0.7949 (0.0031) 0.7182 (0.0162) 0.7298 (0.0115) 0.7092 (0.0469)

However, when our progressive attention mech-
anism is integrated into syntactic structures (de-
pendency or constituency), we witness a boost in
the semantic relatedness score. Such desirable be-
havior is consistently observed in multiple active-
passive voice pairs. The second pair points to a
possible issue in data annotation. Despite the pres-
ence of strong negation, the gold-standard score
is 4 out of 5 (indicating high relatedness). Inter-
estingly, the Progressive-Attn + Dependency Tree-

LSTM model favors the negation facet and outputs
a low relatedness score.

7 Discussion

In this section, let’s revisit our research questions
in light of the experimental results.

First, can attention mechanisms be built for
Tree-LSTMs? Does it work? The answer is
yes. Our novel progressive-attention Tree-LSTM
model, when instantiated on constituency trees,



2124

Table 3: Effect of the progressive attention model
Test Pair Gold BiLSTM Const. Tree Dep. Tree

ID (no attn) (PA) (no attn) (PA) (no attn) (PA)

1 S1: The badger is burrowing a hole.S2: A hole is being burrowed by the badger. 4.9 2.60 3.02 3.52 4.34 3.41 4.63

2 S1: There is no man screaming.S2: A man is screaming. 4 3.44 3.20 3.65 3.50 3.51 2.15

significantly outperforms its counterpart without
attention. The same model can also be deployed
on sequences (degenerated trees) and achieve
quite impressive results.

Second, the performance gap between the two
attention models is quite striking, in the sense that
the progressive model completely dominate its de-
composable counterpart. The difference between
the two models is the pacing of attention, i.e.,
when to refer to nodes in the other tree while en-
coding a node in the current tree. The progres-
sive attention model garners it’s empirical superi-
ority by attending while encoding, instead of wait-
ing till the end of the structural encoding to es-
tablish cross-sentence attention. In retrospect, this
may justify why the original decomposable atten-
tion model in (Parikh et al., 2016) achieved com-
petitive results without any LSTM-type encoding.
Effectively, they implemented a naive version of
our progressive attention model.

Third, do structures matter/help? The overall
trend in our results is quite clear: the tree-based
models exhibit convincing empirical strength; lin-
guistically motivated structures are valuable. Ad-
mittedly though, on the relatively large Quora
dataset, we observe some diminishing returns of
incorporating structural information. It is not
counter-intuitive that the sheer size of data can
possibly allow structural patterns to emerge, hence
lessen the need to explicitly model syntactic struc-
tures in neural architectures.

Last but not least, in trying to assess the im-
pact of attention mechanisms (in particular the
progressive attention model), we notice that the
extra mileage gained on different structural en-
codings is different. Specifically, performance lift
on Linear Bi-LSTM > performance lift on Con-
stituency Tree-LSTM, and PA struggles to see per-
formance lift on dependency Tree-LSTM. Inter-
estingly enough, this observation is echoed by an
earlier study (Gildea, 2004), which showed that
tree-based alignment models work better on con-

stituency trees than on dependency trees.
In summary, our results and findings lead to sev-

eral intriguing questions and conjectures, which
call for investigation beyond the scope of our
study:

• Is it reasonable to conceptualize attention
mechanisms as an implicit form of structure,
which complements the representation power
of explicit syntactic structures?

• If yes, does there exist some trade-off be-
tween the modeling efforts invested into syn-
tactic and attention structures respectively,
which seemingly reveals itself in our empiri-
cal results?

• The marginal impact of attention on depen-
dency Tree-LSTMs suggests some form of
saturation effect. Does that indicate a closer
affinity between dependency structures (rela-
tive to constituency structures) and composi-
tional semantics (Liang et al., 2013)?

• If yes, why is dependency structure a better
stepping stone for compositional semantics?
Is it due to the strongly lexicalized nature of
the grammar? Or is it because the depen-
dency relations (grammatical functions) em-
body more semantic information?

8 Conclusion

In conclusion, we proposed a novel progressive at-
tention model on syntactic structures, and demon-
strated its superior performance in semantic relat-
edness tasks. Our work also provides empirical
ingredients for potentially profound questions and
debates on syntactic structures in linguistics.



2125

References
Eneko Agirre, Mona Diab, Daniel Cer, and Aitor

Gonzalez-Agirre. 2012. Semeval-2012 task 6: A pi-
lot on semantic textual similarity. In Proceedings of
the First Joint Conference on Lexical and Computa-
tional Semantics-Volume 1: Proceedings of the main
conference and the shared task, and Volume 2: Pro-
ceedings of the Sixth International Workshop on Se-
mantic Evaluation, pages 385–393. Association for
Computational Linguistics.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
learning to align and translate. ICLR’2015.

John Bauer. Shift-reduce constituency parser.

Johannes Bjerva, Johan Bos, Rob Van der Goot, and
Malvina Nissim. 2014. The meaning factory: For-
mal semantics for recognizing textual entailment
and determining semantic similarity. In Proceedings
of the 8th International Workshop on Semantic Eval-
uation (SemEval 2014), pages 642–646.

Peter F Brown, Vincent J Della Pietra, Stephen A Della
Pietra, and Robert L Mercer. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
mation. Computational linguistics, 19(2):263–311.

Danqi Chen and Christopher Manning. 2014. A fast
and accurate dependency parser using neural net-
works. In Proceedings of the 2014 conference on
empirical methods in natural language processing
(EMNLP), pages 740–750.

Daniel Gildea. 2004. Dependencies vs. constituents for
tree-based alignment. In Proceedings of the 2004
Conference on Empirical Methods in Natural Lan-
guage Processing.

Christoph Goller and Andreas Kuchler. 1996. Learning
task-dependent distributed representations by back-
propagation through structure. In Neural Networks,
1996., IEEE International Conference on, volume 1,
pages 347–352. IEEE.

Klaus Greff, Rupesh K Srivastava, Jan Koutnı́k, Bas R
Steunebrink, and Jürgen Schmidhuber. 2017. Lstm:
A search space odyssey. IEEE transactions on neu-
ral networks and learning systems.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Csernai Iyer, Dandekar. First quora dataset release:
Question pairs.

Sergio Jimenez, George Duenas, Julia Baquero, and
Alexander Gelbukh. 2014. Unal-nlp: Combining
soft cardinality features for semantic textual simi-
larity, relatedness and entailment. In Proceedings of
the 8th International Workshop on Semantic Evalu-
ation (SemEval 2014), pages 732–742.

Kaggle. 2017. Quora question pairs.

Alice Lai and Julia Hockenmaier. 2014. Illinois-lh: A
denotational and distributional approach to seman-
tics. In Proceedings of the 8th International Work-
shop on Semantic Evaluation (SemEval 2014), pages
329–334.

Percy Liang, Michael I. Jordan, and Dan Klein. 2013.
Learning dependency-based compositional seman-
tics. Comput. Linguist., 39(2):389–446.

Minh-Thang Luong, Hieu Pham, and Christopher D
Manning. 2015. Effective approaches to attention-
based neural machine translation. arXiv preprint
arXiv:1508.04025.

Marco Marelli, Stefano Menini, Marco Baroni, Luisa
Bentivogli, Raffaella Bernardi, Roberto Zamparelli,
et al. 2014. A sick cure for the evaluation of com-
positional distributional semantic models. In LREC,
pages 216–223.

Ankur P Parikh, Oscar Täckström, Dipanjan Das, and
Jakob Uszkoreit. 2016. A decomposable attention
model for natural language inference. arXiv preprint
arXiv:1606.01933.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 confer-
ence on empirical methods in natural language pro-
cessing (EMNLP), pages 1532–1543.

Chris Quirk, Arul Menezes, and Colin Cherry. 2005.
Dependency treelet translation: Syntactically in-
formed phrasal smt. In Proceedings of the 43rd An-
nual Meeting on Association for Computational Lin-
guistics, pages 271–279. Association for Computa-
tional Linguistics.

Richard Socher, Eric H Huang, Jeffrey Pennington,
Andrew Y Ng, and Christopher D Manning. 2011.
Dynamic pooling and unfolding recursive autoen-
coders for paraphrase detection. In NIPS, vol-
ume 24, pages 801–809.

Kai Sheng Tai, Richard Socher, and Christopher D
Manning. 2015. Improved semantic representations
from tree-structured long short-term memory net-
works. arXiv preprint arXiv:1503.00075.

John Wieting, Mohit Bansal, Kevin Gimpel, and
Karen Livescu. 2015. Towards universal para-
phrastic sentence embeddings. arXiv preprint
arXiv:1511.08198.

Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical translation model. In Proceedings
of the 39th Annual Meeting on Association for Com-
putational Linguistics, pages 523–530. Association
for Computational Linguistics.

Jiang Zhao, Tiantian Zhu, and Man Lan. 2014. Ecnu:
One stone two birds: Ensemble of heterogenous
measures for semantic relatedness and textual entail-
ment. In Proceedings of the 8th International Work-
shop on Semantic Evaluation (SemEval 2014), pages
271–277.

https://nlp.stanford.edu/software/srparser.html
https://www.kaggle.com/c/quora-question-pairs
https://doi.org/10.1162/COLI_a_00127
https://doi.org/10.1162/COLI_a_00127

