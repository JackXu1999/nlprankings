



































coi00196.dvi


Improved Estimation of Entropy for
Evaluation of Word Sense Induction

Linlin Li∗
Microsoft Development Center Norway

Ivan Titov∗∗
University of Amsterdam

Caroline Sporleder†
Trier University

Information-theoretic measures are among the most standard techniques for evaluation of
clustering methods including word sense induction (WSI) systems. Such measures rely on
sample-based estimates of the entropy. However, the standard maximum likelihood estimates
of the entropy are heavily biased with the bias dependent on, among other things, the number of
clusters and the sample size. This makes the measures unreliable and unfair when the number
of clusters produced by different systems vary and the sample size is not exceedingly large. This
corresponds exactly to the setting of WSI evaluation where a ground-truth cluster sense number
arguably does not exist and the standard evaluation scenarios use a small number of instances of
each word to compute the score. We describe more accurate entropy estimators and analyze their
performance both in simulations and on evaluation of WSI systems.

1. Introduction

The task of word sense induction (WSI) has grown in popularity recently. WSI has the
advantage of not assuming a predefined inventory of senses. Rather, senses are induced
in an unsupervised fashion on the basis of corpus evidence (Schütze 1998; Purandare
and Pedersen 2004). WSI systems can therefore better adapt to different target domains
that may require sense inventories of different granularities. However, the fact that WSI
systems do not rely on fixed inventories also makes it notoriously difficult to evaluate
and compare their performance. WSI evaluation is a type of cluster evaluation problem.
Although cluster evaluation has received much attention (see, e.g., Dom 2001; Strehl
and Gosh 2002; Meila 2007), it is still not a solved problem. Finding a good way to
score partially incorrect clusters is particularly difficult. Several solutions have been

∗ Microsoft Development Center Norway. E-mail: linlin@coli.uni-saarland.de.
∗∗ Institute for Logic, Language and Computation. E-mail: titov@uva.nl.
† Computational Linguistics and Digital Humanities, Trier University, 54286 Trier, Germany.

E-mail: sporledc@uni-trier.de.

Submission received: 14 March 2013; revised version received: 13 September 2013; accepted for publication:
20 November 2013

doi:10.1162/COLI a 00196

© 2014 Association for Computational Linguistics



Computational Linguistics Volume 40, Number 3

proposed but information theoretic measures have been among the most successful
and widely used techniques. One example is the normalized mutual information, also
known as V-measure (Strehl and Gosh 2002; Rosenberg and Hirschberg 2007), which
has, for example, been adopted in the SemEval 2010 WSI task (Manandhar et al. 2010).

All information theoretic measures of cluster quality essentially rely on sample-
based estimates of entropy. For instance, the mutual information I(c, k) between a gold
standard class c and an output cluster k can be written H(c) + H(k) − H(k, c), where
H(c) and H(k) are the marginal entropies of c and k, respectively, and H(k, c) is their joint
entropy. The most standard estimator is the maximum-likelihood (ML) estimator, which
substitutes the probability of each event (cluster, classes, or cluster-class pair occurrence)
with its normalized empirical frequency.

Entropy estimators, even though consistent, are biased. This means that the expected
estimate of the entropy on a finite sample set is different from the true value. It is
also different from an expected estimate on a larger test set generated from the same
distribution, as the bias depends on the size of the sample. This discrepancy negatively
affects entropy-based evaluation measures, such as the V-measure. This is different from
supervised classification evaluation, where the classification accuracy on a finite test set
is expected to be equal to the error rate (for the independent and identically distributed,
i.i.d.) case, though it can be different due to variance (due to choice of the test set). As
long as the number of samples is large with respect to the number of classes and clusters,
the estimate is sufficiently close to the true entropy. Otherwise, the quality of entropy
estimators matters and the bias of the estimator can be large. This problem is especially
prominent for the ML estimator (Miller 1955).

In WSI, we are faced with exactly those conditions that negatively affect the entropy
estimators. In a typical setting, the number of examples per word is small—for example,
less than 100 on average for the SemEval 2010 WSI task. The number of clusters, on
the other hand, can be fairly high, with some systems outputting more than 10 sense
clusters per word on average. Because the bias of an entropy estimator is dependent
on, among other things, the number of clusters, the ranking of different WSI systems is
partly affected by the number of clusters they produce. Even worse, the ranking is also
affected by the size of the test set. The problem is exacerbated when computing the joint
entropy between clusters and classes, H(k, c), because this requires estimating the joint
probability of cluster-class pairs for which the statistics are even more sparse.

The bias problem of entropy estimators has long been known in the information
theory community and many studies have addressed this issue (e.g., Miller 1955; Batu
et al. 2002; Grasberger and Schürmann 1996). In this article, we compare different
estimators and their influence on the computed evaluation scores. We run simulations
using a Zipfian distribution where we know the true entropy. We also compare different
estimators against the SemEval 2010 WSI benchmark. Our results strongly suggest that
there are estimators, namely, the best-upper-bound (BUB) estimator (Paninski 2003)
and jackknifed (Tukey 1958; Quenouille 1956) estimators, which are clearly preferable
to the commonly used ML estimators.

2. Clustering Evaluation

2.1 Information-Theoretic Measures

The main challenge in evaluating clustering methods is that successful measures should
be able to compare solutions found at different levels of granularity. In other words,
one cannot assume that there exists one-to-one mapping between the predicted clusters

672



Li, Titov, and Sporleder Improved Estimation of Entropy for Word Sense Induction

and the gold-standard classes. A natural approach would be to consider arbitrary
statistical dependencies between the cluster assignment k and the class assignment c.
The standard measure of statistical dependence of two random variables is the Shannon
mutual information I(k, c) (MI). MI is 0 if two variables are independent, and it is equal
to the entropy of a variable if another variable is deterministically dependent on it.
Clearly, such measure would favor clusterings with higher entropy, and, consequently,
normalized versions of MI are normally used to evaluate clusterings. One instance of
normalized MI actively used in the context of WSI evaluation is the V-measure, or
symmetric uncertainty (Witte and Frank 2005; Rosenberg and Hirschberg 2007):

V(k, c) =
2I(k, c)

H(k) + H(c)
=

2(H(k) + H(c) − H(k, c))
H(k) + H(c)

though other forms of MI normalization have also been explored (Strehl and Gosh 2002).
Because the true marginal and the joint entropies are not known, the standard

maximum likelihood estimators (also called plug-in estimators of entropy) are normally
used instead. The ML estimates Ĥ have the analytical form of an entropy with the
normalized empirical frequency substituted instead of the unknown true membership
probabilities, for example:

Ĥ(c) =
m∑

i=1

−niN log
ni
N (1)

where ni is the number of times cluster i appears in the set, m is the number of clusters,
and N is the size of the set (i.e., the sample size).

The ML estimators of entropy are consistent but heavily negatively biased (see
Section 3 for details). In other words, the expectation of Ĥ is lower than the true entropy,
and this discrepancy increases with the number of clusters m and decreases with the
sample size N. When m is comparable to N, the ML estimator is known to be very
inaccurate (Paninski 2004).

Note that for V-measure estimation the main source of the estimation error is the
joint entropy H(k, c),1 as the number of possible pairs (c, k) for most systems would be
large whereas the total number of occurrences will remain the same as for the estimation
of H(c) and H(k). Therefore, the absolute value of the bias for Ĥ(c, k) will exceed the
aggregate bias of the estimators of marginal entropy, Ĥ(c) and Ĥ(k). As a result, the
V-measure will be positively biased, and this bias would be especially high for systems
predicting a large number of clusters.

This phenomenon has been previously noticed (Manandhar et al. 2010) but no satis-
factory explanation has been given. The shortcomings of the ML estimator are especially
easy to see on the example of a baseline system that assigns every instance in the testing
set to an individual cluster. This baseline, when averaged over the 100 target words, out-
performs all the participants’ systems of the SemEval-2010 task on the standard testing
set (Manandhar and Klapaftis 2009). Though we cannot compute the true bias for any
real system, the computation is trivial for this baseline. The true V-measure is equal to 0,

1 V-measure can be expressed via entropies in a number of different ways, although, for ML estimation
they are all equivalent. For some more complex estimators, including some of the ones considered here,
the resulting estimates will be somewhat different depending on the decomposition. We will focus on the
symmetric form presented here.

673



Computational Linguistics Volume 40, Number 3

as the baseline can be regarded as a limiting case of a stochastic system that picks up one
of the m clusters under the uniform distribution with m → ∞; the mutual information
between any class labels and clustering produced by such model equals 0 for every m.
However, the ML estimate for the V-measure is V̂(k, c) = 2Ĥ(c)/(log N + Ĥ(c)). For the
testing set of SemEval 2010, this estimate, averaged over all the words, yields 31.7%,
which by far exceeds the best result of any system (16.2%). On an infinite (or sufficiently
large) set, however, its performance would change to the worst. This is a problem not
only for the baseline but for any system which outputs a large number of classes: The
error measures computed on the small test set are far from their expectations on the
new data. We will see in our quantitative analyses (Section 5) that using more accurate
estimators will have the most significant effect on both the V-measure and on the ranks
of systems which output richer clustering, agreeing with this argument.

Though in this analysis we focused on the V-measure, other information theoretic
measures have also been proposed. Examples of such measures include the variation
of information measure (Meila 2007) VI(c, k) = H(c|k) + H(k|c) and Q0 measure (Dom
2001) H(c|k). This argument applies to these evaluation measures as well, and they can
all be potentially improved by using more accurate estimators.

2.2 Alternative Measures

Not only information-theoretic measures have been proposed for clustering evaluation.
An alternative evaluation strategy is to attempt to find the best possible mapping
between the predicted clusters and the gold-standard classes and then apply standard
measures like precision, recall, and F-score. However, if the best mapping is selected on
the test set the result can be overoptimistic, especially for rich clusterings. Consequently,
such methods constrain the set of permissible mappings to a restricted family. For exam-
ple, for the F-score, one considers only mappings from each class to a single predicted
cluster (Zhao and Karypis 2005; Agirre and Soroa 2007). This restriction is generally too
strong for many clustering problems (Meila 2007; Rosenberg and Hirschberg 2007), and
especially inappropriate for the WSI evaluation setting, as it penalizes sense induction
systems that induce more fine-grained senses than the ones present in the gold-standard
sense set.

The Paired F-score (Manandhar et al. 2010) is somewhat less restrictive than the
F-score measures in that it defines precision and recall in terms of pairs of instances (i.e.,
effectively evaluating systems based on the proportion of correct links). However, the
Paired F-score has the undesirable property that it ranks those systems highest which
put all instances in one cluster, thereby obtaining perfect recall.

As an alternative, the supervised evaluation measure has been proposed (Agirre
et al. 2006). This approach in addition to the testing set uses an auxiliary mapping set.
First the mapping is induced on the mapping set, then the quality of the mapping is
evaluated on the testing set. One problem with this evaluation scenario is that the size
of the mapping set has an effect on the results and there is no obvious criterion for
selecting the right size of the mapping set. For the WSI task, the importance of the set
size was empirically confirmed when the evaluation set was split in proportions 80:20
(80% for the mapping sets, and 20% for testing) instead of the original 60:40 split: The
scores of all top 10 systems improved and the ranking changed as well (Manandhar
et al. 2010) (see also Table 1 later in this article).

Further cluster evaluation measures have been proposed for other language pro-
cessing tasks, such as B3 (Bagga and Baldwin 1998) or CEAF (Luo 2005) for coreference
resolution evaluation. In this article, we are concerned with entropy-based measures.

674



Li, Titov, and Sporleder Improved Estimation of Entropy for Word Sense Induction

For a more general assessment of measures for clustering evaluation see Amigo et al.
(2009) and Klapaftis and Manandhar (2013).

3. Entropy Estimation

Given the influence that information theory has had on many fields, including signal
processing, neurophysiology, and psychology, to name a few, it is not surprising that the
topic of entropy estimation has received considerable attention over the last 50 years.2

However, much of the work has focused on settings where the number of classes is
significantly lower than the size of the sample. More recently a set-up where the sample
size N is comparable to the number of classes m has started to receive attention (Paninski
2003, 2004).

In this section, we start by discussing the intuition for why the ML estimator is
heavily biased in this setting. Though unbiased estimators of entropy do not exist,3

various techniques have been proposed to reduce the bias while controlling the vari-
ance (Grasberger and Schürmann 1996; Batu et al. 2002). We will discuss widely used
bias-corrected estimators, the ML estimator with Miller-Madow bias correction (Miller
1955) and the jackknifed estimator (Strong et al. 1998). Then we turn to the more
recent technique proposed specifically for the N ∼ m setting, the best-upper-bound
(BUB) estimator (Paninski 2003). We will conclude this section by explaining how these
estimators can be computed using stochastic (weighted) output of WSI systems.

3.1 Standard Estimators of Entropy

As we discussed earlier, the ML estimator (1) is negatively biased. For a fixed distri-
bution p, a little algebra can be used to show that the bias of the maximum likelihood
estimator can be written as

H − Ep(Ĥ) = Ep(D(p̂ ‖ p))

where Ep denotes an expectation under p, D is the Kullback-Leibler (KL) divergence,
and p̂ is the empirical distribution in the sample of N elements drawn i.i.d. from p.
Because the KL-divergence is always non-negative, it follows that the bias is always
non-positive. It also follows that the expected divergence is larger if the size of the
sample is small. In fact, this expression can be used to obtain the asymptotic bias rate
(N → ∞) (Miller 1955). The bias rate derived in this way would suggest a form of
correction to the ML estimator, called Miller-Madow bias correction ĤMM = Ĥ + m̂−1N ,
where m̂ is an estimate of m, as the true size of support m may not be known. In our
experiments, we use a basic estimator m̂ which is just the number of different clusters
(classes or cluster-class pairs depending on the considered entropy) appearing in the
sample. We will call the estimator ĤMM the Miller-Madow (MM) estimator. As the MM
estimator is motivated by the asymptotic behavior of the bias, it is not very appropriate
for N ∼ m.

2 For a relatively recent overview of progress in entropy estimation research see, for example, the
proceedings of the NIPS 2003 workshop on entropy estimation.

3 The expectation of any estimate from i.i.d. samples is a polynomial function of class probabilities.
The entropy is non-polynomial and therefore unbiased estimators do not exist.

675



Computational Linguistics Volume 40, Number 3

The bias of the ML estimator decreases with the size of the sample. Intuitively, an
estimate of the discrepancy in estimates produced from samples of different sizes can
be used to correct the ML estimator: If an estimate based on N − 1 samples significantly
exceeds the estimate from N samples, then the bias of the estimator is still large.
Roughly, this intuition is encoded in the jackknifed (JK) estimator (Strong et al. 1998):

ĤJK = NĤ − N − 1N
N∑

j=1

Ĥ−j

where Ĥ−j is the ML estimator based on the original sample excluding the example j.

3.2 BUB Estimator

We can observe that all the previous estimators can be expressed in the form of a linear
function of the ordered histogram statistics

Ĥ(aaa) =
N∑

j=0

aj,Nhj (2)

where hj is the number of classes which appear j times in the sample:

hj =
m∑

i=1

[[ni = j]] (3)

where [[ ]] denotes the indicator function. The coefficients aj,N for the ML, MM, and JK
estimators are equal to:

aML,j,N = − jN log
j

N

aMM,j,N = − jN log
j

N +
1 − jN

N

aJK,j,N = NaML,j,N − N − 1N ((N − j)aML,j,N−1 + jaML,j−1,N−1)

This observation suggests that it makes sense to study an estimator of the general
form Ĥ(aaa) as defined in Equation (2). Upper bounds on the bias and variance of such
estimators4 have been stated in Paninski (2003). These bounds imply an upper bound
on the standard measure of estimator performance, mean squared error (MSE, the
sum of the variance and squared bias). The worst-case estimator is then obtained
by selecting aaa to minimize the upper bound on MSE, and, therefore, it is called the
best-upper-bound estimator. This optimization problem5 corresponds to a regularized

4 We argued that variance is not particularly important for the ML estimator with N ∼ m. However,
for an arbitrary estimator of the form of Equation (2) this may not be true, as the coefficients aj,N
may be oscillating, resulting in an estimator with a large variance (Antos and Kontoyiannis 2001).

5 More formally, its modification where the L2 norm is optimized instead of the original L∞
optimization set-up.

676



Li, Titov, and Sporleder Improved Estimation of Entropy for Word Sense Induction

least-squares problem and can be solved analytically (see Appendix A and Paninski
[2003] for technical details).

This technique is fairly general, and can potentially be used to minimize the bound
for a particular type of distribution. This direction can be promising, as the types of
distributions observed in WSI are normally fairly skewed (arguably Zipfian) and tighter
bounds on MSE may be possible. In this work, we use the universal worst-case bounds
advocated in Paninski (2003).

3.3 Estimation with Stochastic Predictions

As many WSI systems maintain a distribution over predicted clusters, in SemEval 2010
the participants were encouraged to provide a weighted prediction (i.e., a distribution
over potential clusters for each example) instead of predicting just a single most-likely
cluster.

We interpret the weighted output of a system on an example l as a categorical
trial with the probabilities of outcomes p̃(l)i provided by the model where i is an index
of the cluster. Therefore a stochastic output of a system on the test set represents
a distribution over samples generated from these trials; the estimator can be com-
puted as an expectation under this distribution. For estimators of the form of Equa-
tion (2), we can exploit the linearity of expectations and write the expected value of the
estimator as

Ep̃
[
Ĥ(aaa)

]
=

N∑

j=0

aj,NEp̃
[
hj
]

where Ep̃
[
hj
]

is the expected number of classes with j counts. We can rewrite it using the
linearity property again, this time for expression (3):

Ep̃
[
Ĥ(aaa)

]
=

N∑

j=0

aj,N
m∑

i=1

P̃(ni = j, N) (4)

where P̃i(ni = j, N) is the distribution over the number of counts for non-identical
Bernoulli trials p̃(l)i and 1 − p̃(l)i (l = 1, . . . , N), known as the Poisson binomial distribu-
tion, a generalization of the standard binomial distribution. The probabilities can be
efficiently computed using one of alternative recursive formulas (Wang 1993). One
of the simplest schemes, with good numerical stability properties, is the recursive
computation:

P̃i(ni = j, t) = P̃i(ni = j − 1, t − 1)p̃(j)i + P̃i(ni = j, t − 1)(1 − p̃(j)i )

where j = 1, . . . , N.

4. Simulations

Because the true entropy (and V-measure) is not known on the WSI task, we start with
simulations where we generated samples from a known distribution and can compare

677



Computational Linguistics Volume 40, Number 3

0 5 10 15 20 25 30 35 40 45 50
0

0.5

1

1.5

2

2.5

Sample Size (N)

E
nt

ro
py

 

 

H
BUB
JK
MM
ML

Figure 1
The estimated and true entropy for uniform distribution.

the estimates (and their biases) with the true entropy. In all our experiments, we set the
number of clusters m to 10 and varied the sample size N (Figure 1). Each point on the
graph is the result of averaging over 1,000 sampling experiments.6

The distribution of senses for a given word is normally skewed: For most words
the vast majority of occurrences correspond to one or two most common senses even
though the total number of senses can be quite large (Kilgarriff 2004). This type of long-
tail distribution can be modeled with Zipf’s law. Consequently, most of our experiments
consider Zipfian distributions. For Zipf’s law, the probability of choosing an element
with rank k is proportional to 1ks , where s is a shape parameter. Small values of the
parameter s correspond to flatter distributions; the distributions with a larger s are
increasingly more skewed. The estimators’ prediction for Zipfian distributions with
a different s are shown in Figure 2. For s = 4, over 90% of the probability mass is
concentrated on a single class. For every distribution we plot the true entropy (H)
and the estimated values; compare results with a uniform distribution as seen in
Figure 1.

In all figures, we observe that over the entire range of sample sizes, the bias
for the bias-corrected estimates is indeed reduced substantially with respect to that
of the ML estimator. This difference is particularly large for smaller N—the realistic
setting for the computation of H(c, k) for the WSI task. For the uniform distribution
and flatter Zipf distributions (s = 1 and 2), the JK estimator seems preferable for all
but the smallest sample sizes (N > 3). The BUB estimator outperforms the JK estimator
with very skewed distributions (s = 3 and s = 4) and in most cases provides the least
biased estimates with very small N. However, these results with very small sample sizes
(N ≤ 2) may not have much practical relevance as any estimator is highly inaccurate in
this mode. The MM bias correction, as expected, is not sufficient for small N. Although
it outperforms the ML estimates, its error is consistently larger than those of other bias-
correction strategies.

Overall, the simulations suggest that the ML estimators are not very appropriate for
entropy estimation with the types of distributions which are likely to be observed in the

6 In this way we study only the bias of estimators.

678



Li, Titov, and Sporleder Improved Estimation of Entropy for Word Sense Induction

0 5 10 15 20 25 30 35 40 45 50
0

0.5

1

1.5

2

2.5

Sample Size (N)

E
nt

ro
py

 

 

H
BUB
JK
MM
ML

0 5 10 15 20 25 30 35 40 45 50
0

0.2

0.4

0.6

0.8

1

1.2

1.4

Sample Size (N)

E
nt

ro
py

 

 

H
BUB
JK
MM
ML

(a) s = 1 (b) s = 2

0 5 10 15 20 25 30 35 40 45 50
0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

Sample Size (N)

E
nt

ro
py

 

 

H
BUB
JK
MM
ML

0 5 10 15 20 25 30 35 40 45 50
0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

Sample Size (N)

E
nt

ro
py

 

 

H
BUB
JK
MM
ML

(c) s = 3 (d) s = 4

Figure 2
The estimated and true entropy of Zipf’s law.

WSI tasks. Both the JK and BUB estimators are considerably less biased alternatives to
the ML estimations.

5. Effects on WSI Evaluation

To gauge the effect of the bias problem on WSI evaluation, we computed how the
ranking of the SemEval 2010 systems (Manandhar et al. 2010) were affected by different
estimators. The SemEval 2010 organizers supplied a test set containing 8,915 manually
annotated examples covering 100 polysemous lemmas.

The average number of gold standard senses per lemma was 3.79. Overall,
27 systems participated and were ranked according to their performance on the test
set, applying the V-measure evaluation as well as paired F-score and a supervised
evaluation scheme. The systems were also compared against three baselines. For the
Most Frequent Sense (MFS) baseline all test instances of a given target lemma are
grouped into one cluster, that is, there is exactly one cluster per lemma. The second
baseline, Random, assigns each instance randomly to one of four clusters. The last
baseline, proposed in Manandhar and Klapaftis (2009), 1-cluster-per-instance (1ClI),
produces as many clusters as there are instances in the test set.

Table 1 gives an overview of the different systems and the three baselines (shown in
italics). The systems are presented in the order in which they were given in the official

679



Computational Linguistics Volume 40, Number 3

Table 1
V-measure computed with different estimators. Supervised recall is shown for comparison
(80:20 and 60:40 splits for mapping/evaluation, numbers as provided by Manandhar et al. 2010).
The corresponding ranks are shown in parentheses.

System C# ML MM JK BUB Supervised Recall
80:20 60:40

1ClI 89.1 31.6 (1) 29.5 (1) 27.4 (1) –3.6 (29) – –
Hermit 10.8 16.2 (2) 13.1 (4) 10.7 (4) 11.0 (2) 58.3 (17) 57.3 (18)
UoY 11.5 15.7 (4) 14.3 (2) 13.1 (2) 11.4 (1) 62.4 (1) 62.0 (1)
KSU KDD 17.5 15.7 (3) 13.2 (3) 11.0 (3) 7.6 (3) 52.2 (24) 50.4 (25)
Duluth-WSI 4.1 9.0 (5) 6.9 (5) 5.7 (5) 5.6 (5) 60.5 (2) 59.5 (5)
Duluth-WSI-SVD 4.1 9.0 (6) 6.9 (6) 5.7 (6) 5.6 (6) 60.5 (3) 59.5 (4)
Duluth-R-110 9.7 8.6 (7) 4.7 (16) 1.9 (20) 3 (17) 54.8 (23) 53.6 (23)
Duluth-WSI-Co 2.5 7.9 (8) 6.4 (7) 5.7 (7) 5.7 (4) 60.8 (4) 60.1 (2)
KCDC-PCGD 2.9 7.8 (9) 6.3 (8) 5.5 (8) 5.2 (7) 59.5 (9) 59.1 (7)
KCDC-PC 2.9 7.5 (10) 6.2 (9) 5.4 (9) 5.0 (8) 59.7 (8) 58.9 (9)
KCDC-PC-2 2.9 7.1 (11) 5.7 (12) 4.9 (12) 4.5 (13) 59.8 (7) 58.9 (8)
Duluth-Mix-Narrow-Gap 2.4 6.9 (15) 5.5 (14) 4.8 (14) 4.8 (9) 56.6 (21) 56.2 (21)
KCDC-GD-2 2.8 6.9 (14) 5.7 (11) 4.9 (11) 4.6 (12) 58.7 (13) 57.9 (15)
KCDC-GD 2.8 6.9 (12) 5.8 (10) 5.0 (10) 4.6 (11) 59.0 (11) 58.3 (11)
Duluth-Mix-Narrow-PK2 2.7 6.8 (16) 5.4 (15) 4.6 (15) 4.6 (10) 56.1 (22) 55.7 (22)
Duluth-MIX-PK2 2.7 5.6 (17) 4.3 (17) 3.5 (17) 3.5 (16) 51.6 (25) 50.5 (24)
Duluth-R-15 5.0 5.3 (18) 2.4 (20) 0.7 (24) 1.3 (22) 56.8 (20) 56.5 (19)
Duluth-WSI-Co-Gap 1.6 4.8 (19) 4.1 (18) 3.8 (16) 4.1 (15) 60.3 (5) 59.5 (3)
Random 4.0 4.4 (20) 1.9 (22) 0.5 (25) 0.8 (24) 57.3 (19) 56.5 (20)
Duluth-R-13 3.0 3.6 (21) 1.5 (25) 0.5 (26) 0.7 (25) 58.0 (18) 57.6 (17)
Duluth-WSI-Gap 1.4 3.1 (22) 2.6 (19) 2.5 (18) 2.7 (18) 59.8 (6) 59.3 (6)
Duluth-Mix-Gap 1.6 3.0 (23) 2.3 (21) 1.9 (19) 2.0 (19) 50.6 (26) 49.8 (26)
Duluth-Mix-Uni-PK2 2.0 2.4 (24) 1.8 (23) 1.5 (21) 1.4 (21) 19.3 (27) 19.1 (27)
Duluth-R-12 2.0 2.3 (25) 0.8 (27) 0.2 (27) 0.3 (28) 58.5 (16) 57.7 (16)
KCDC-PT 1.5 1.9 (26) 1.6 (24) 1.4 (22) 1.5 (20) 58.9 (12) 58.3 (13)
Duluth-Mix-Uni-Gap 1.4 1.4 (27) 1.0 (26) 0.8 (23) 1.0 (23) 18.7 (28) 18.9 (28)
KCDC-GDC 2.8 6.9 (13) 5.7 (13) 4.8 (13) 4.5 (14) 59.1 (10) 58.3 (10)
MFS 1.0 0 (29) 0.0 (29) 0.0 (28) 0.5 (27) 58.7 (15) 58.3 (12)
Duluth-WSI-SVD-Gap 1.0 0.0 (28) 0.0 (28) 0.0 (29) 0.5 (26) 58.7 (14) 58.2 (14)

KCDC-PC-2* 2.9 5.7 7.2 2.3 2.2 – –
UoY* 11.5 25.1 22.8 17.8 5.0 – –

SemEval 2010 results table (Table 4 in Manandhar et al. (2010), p. 66). Table 1 shows
the average number of clusters per word (C#), the V-measure computed with different
estimators (ML, MM, JK, and BUB), and the rankings it produces (in brackets).7 For
comparison, the results of a supervised evaluation are also shown. The bottom two rows
(KCDC-PC-2∗ and UoY∗) show the scores computed from the stochastic (weighted)
output (Section 3.3) for systems KCDC-PC-2 and UoY, respectively. Other systems did
not produce weighted output.

7 The ranking produced by the ML estimator should mirror that of the official results. In some cases it
does not—for example, system UoY was placed before KSU in the official results, whereas the ML
estimator would predict the reverse order. As the difference in V-measure is small, we attribute this
discrepancy to rounding errors. The system KCDC-GDC seems to be misplaced in the official results
list; according to V-measure it should be ranked higher. Our ranking was computed before rounding,
and there were no ties.

680



Li, Titov, and Sporleder Improved Estimation of Entropy for Word Sense Induction

0 2 4 6 8 10 12 14 16 18
0

0.01

0.02

0.03

0.04

0.05

0.06

0.07

Average Cluster Number

M
L−

JK

0 2 4 6 8 10 12 14 16 18
−0.01

0

0.01

0.02

0.03

0.04

0.05

0.06

0.07

0.08

0.09

Average Cluster Number

M
L−

B
U

B

(a) Ĥ - ĤJK (b) Ĥ - ĤBUB

Figure 3
Discrepancy in estimates as a function of the predicted number of classes.

The 27 systems vary widely in the number of average clusters they output
per lemma, ranging from 1.02 (Duluth-WSI-SVD) to 17.5 (KSU-KDD). To assess the
influence of the cluster granularity on the entropy estimates, we compared the estimates
given by the ML estimator against those given by JK and BUB for different numbers of
clusters. Figure 3 plots the cluster numbers output by the systems against the estimate
difference for ML vs. JK (Figure 3a) and ML vs. BUB (Figure 3b). If two estimators
agree perfectly (i.e., produce the same estimate), their difference should always be
zero, independent of the number of clusters. As can be seen, this is not the case. As
expected, the difference is larger for systems with larger numbers of clusters, such as
KSU-KDD. This trend will result in unfair preference towards systems producing richer
clusterings.

Figure 4 shows the effect that the discrepancy in estimates has on the rankings
produced by using either of the three estimators. Figure 4a plots the ranking of the
ML estimator against JK, and Figure 4b plots the ranking of ML against BUB. Dots that
lie on the diagonal line indicate systems whose rank has not changed. It can be seen that
this only applies to a minority of the systems. In general, there are significant differences
between the rankings produced by ML and those by JK or BUB. We have seen that the
ML estimator can lead to counterintuitive and undesirable results, such as ranking the

0 5 10 15 20 25 30
0

5

10

15

20

25

30

ML Ranking (SemEval10)

JK
 R

an
ki

ng

0 5 10 15 20 25 30
0

5

10

15

20

25

30

ML Ranking (SemEval10)

B
U

B
 R

an
ki

ng

(a) Ĥ vs. ĤJK (b) Ĥ vs. ĤBUB

Figure 4
Discrepancy in rankings as a function of the predicted number of classes.

681



Computational Linguistics Volume 40, Number 3

1-cluster-per-instance baseline highest. The BUB estimator corrects this and assigns the
last rank to this baseline.8

The estimate for the V-measure is based on the estimates of the marginal and joint
entropies. To confirm our intuition that joint entropies are more significantly corrected,
we looked into the differences between estimates of each entropy for five systems with
the largest number of clusters (excluding the 1C1I baseline). The average differences
in estimation of H(k) and H(k, c) between JK and ML estimators are 0.08 and 0.16,
respectively, confirming our hypothesis. Analogous discrepancies for the pair BUB vs.
ML are 0.15 and 0.06, respectively. The differences in the entropy of the gold standard
clustering, H(c), is less significant (< 0.02 for both methods) as the gold standard is less
fine-grained than the clusters proposed by these five systems.

For the stochastic version evaluation, we can observe that the score for the KCDC-
PC-2 system is mostly decreased with respect to the “deterministic” evaluation (except
for the MM estimator). Conversely, the score of UoY is mostly improved except to
the prediction of the BUB estimator. These differences are somewhat surprising: The
stochastic version resulted in significantly larger disagreement between the estimators
than the deterministic version. We do not yet have a satisfactory explanation for this
phenomenon.

It is important to notice that for the vast majority of the systems there is agreement
between the scores of the JK and BUB estimator, wheres the ML estimator significantly
overestimates the V-measure for most of the systems. This observation, coupled with
the observed behavior of the JK and BUB estimators in the simulations, suggest that
their predictions are considerably more reliable than predictions of the plug-in ML
estimator.

Comparing the V-measure (BUB) rankings to those obtained by supervised evalu-
ation (last two columns in Table 1) shows noticeable differences. Several systems that
rank highly according to the V-measure occupy the lower end of the scale when evalu-
ated according to supervised recall (Hermit, KSU KDD, Duluth-Mix-Narrow-Gap).

6. Conclusions

In this work, we analyzed the shortcomings of information-theoretic measures in the
context of WSI evaluation and argued that main drawbacks of these approaches, such
as the preference for the systems predicting richer clusterings or assigning the top score
to the 1-cluster-per-instance baseline, are caused by the bias of the underlying sample-
based estimates of entropy. We studied alternative estimators, including one specifically
designed to deal with cases where the number of examples is comparable with the num-
ber of clusters. Two of the considered estimators, the jackknifed estimator and the best-
upper-bound estimator, achieve consistently and significantly less biased results than
the standard ML estimator when evaluated in simulations with Zipfian distributions.
The corresponding estimates in the WSI evaluation context can result in significant
changes in scores and relative rankings, with systems producing richer clusterings more
severely affected. We believe that these results strongly suggest that more accurate
estimates of entropy should be used in future evaluations of sense induction systems.
Other unsupervised tasks in natural language processing, such as word clustering or

8 Note that the V-measure is actually negative here. Though this is not possible for the true V-measure,
the estimated V-measure expresses a difference between the estimated joint entropy and the marginal
entropies and can be negative.

682



Li, Titov, and Sporleder Improved Estimation of Entropy for Word Sense Induction

named entity disambiguation, may also benefit from using information-theoretic scores
based on more accurate estimators.

Appendix A: Derivation for the BUB Estimator

We provide a brief derivation for the BUB estimator and refer the reader to Paninski
(2003) for details and discussion. The BUB estimator is obtained by minimizing an upper
bound on MSE for estimators H(aaa) (see Equation (2)). First, MSE is bounded from above
by maximizing the variance and the bias independently:

max
p

Bp(Ĥ(aaa))2 + Vp(Ĥ(aaa)) ≤ maxp Bp(Ĥ(aaa))
2 + max

p
Vp(Ĥ(aaa)) (A.1)

where p = (p1, . . . , pm) is an underlying discrete measure; Bp and Vp are the bias and the
variance of the estimator given p. Then individual bounds both for the squared bias and
the variance can be constructed.

We start by deriving a bound for the bias. Using linearity of expectation, the expec-
tation of Ĥ(aaa) can be written as

Ep(Ĥ(aaa)) =
N∑

j=0

aj,NEp(hj) =
N∑

j=0

aj,N
m∑

i=1

Bj,N(pi)

where Bj,N(x) is the binomial polynomial
(N

j
)
xj(1 − x)N−j. Then, with simple algebra,

we have

Bp(Ĥ(aaa)) =
m∑

i=1

⎛

⎝
N∑

j=0

aj,NBj,N(pi) − H(pi)
⎞

⎠

where H(x) = −x log x, the entropy function. A uniform upper bound can be obtained
by bounding each term in the sum:

|Bp(Ĥ(aaa))| ≤ m sup
x

|
N∑

j=0

aj,NBj,N(x) − H(x)|

However, this bound is not too tight as it would overemphasize importance of the
approximation quality for components i with pi close to 1. Intuitively, the behavior near
0 is more important, as there can be more components pi close to 0. Paninski (2003)
generalizes this bound by considering a weighted version

|Bp(Ĥ(aaa))| ≤ 2 sup
x

f (x)|
N∑

j=0

aj,NBj,N(x) − H(x)| (A.2)

with the function f chosen to emphasize smaller components

f (x) =

{
m, x < 1m
1/x, x ≥ 1m

683



Computational Linguistics Volume 40, Number 3

As shown in Antos and Kontoyiannis (2001) and in Paninski (2003), bounds on
the variance of the estimator Ĥ(aaa) can be derived using either McDiarmid or Steele
bounds (Steele 1986). For the Steele bound, it has the form

Vp(Ĥ(aaa)) < N max
0≤j<N

(aj+1 − aj)2 (A.3)

Finally, MSE can be bounded by substituting Equations (A.2) and (A.3) into the
inequality (A.1). For computational reasons, instead of choosing aaa to minimize the
bound, the L2 relaxation of the L∞ loss is used, resulting in a regularized least-squares
problem.

Acknowledgments
The research was carried out when the
authors were at Saarland University. It was
funded by the German Research Foundation
DFG (Cluster of Excellence on Multimodal
Computing and Interaction, Saarland
University, Germany). We would like to
thank the anonymous reviewers for their
valuable feedback.

References
Agirre, Eneko, David Martı́nez, Oier López

de Lacalle, and Aitor Soroa. 2006. Evaluating
and optimizing the parameters of an
unsupervised graph-based WSD algorithm.
In Workshop on TextGraphs, at HLT-NAACL
2006, pages 89–96, New York, NY.

Agirre, Eneko and Aitor Soroa. 2007.
Semeval-2007 task 02: Evaluating word
sense induction and discrimination
systems. In Proceedings of the 4th
International Workshop on Semantic
Evaluation, pages 7–12, Prague.

Amigo, Enrique, Julio Gonzalo, Javier
Artiles, and Felisa Verdejo. 2009. A
comparison of extrinsic clustering
evaluation metrics based on formal
constraints. Information Retrieval,
12(4):461–486.

Antos, A. and I. Kontoyiannis. 2001.
Convergence properties of functional
estimates of discrete distributions. Random
Structures and Algorithms, 19:163–193.

Bagga, Amit and Breck Baldwin. 1998.
Algorithms for scoring coreference chains.
In The First International Conference on
Language Resources and Evaluation Workshop
on Linguistics Coreference, pages 563–566,
Granada.

Batu, Tugkan, Sanjoy Dasgupta, Ravi Kumar,
and Ronitt Rubinfeld. 2002. The complexity
of approximating entropy. In Symposium
on the Theory of Computing (STOC),
pages 678–687, Montreal.

Dom, Byron E. 2001. An information-
theoretic external cluster-validity measure.
Technical Report No. RJ10219, IBM.

Grasberger, P. and T. Schürmann. 1996.
Entropy estimation of symbol sequences.
CHAOS, 6(3):414–427.

Kilgarriff, Adam. 2004. How dominant
is the commonest sense of a word? In
Sojka, Kopecek, and Pala, editors, Text,
Speech, Dialogue, volume 3206 of Lecture
Notes in Artificial Intelligence. Springer,
pages 103–112.

Klapaftis, Ioannis P. and Suresh Manandhar.
2013. Evaluating word sense induction
and disambiguation methods. Language
Resources and Evaluation, 47(3):1–27.

Luo, Xiaoqiang. 2005. On coreference
resolution performance metrics. In
Proceedings of the Conference on Human
Language Technology and Empirical Methods
in Natural Language Processing (HLT-05),
pages 25–32, Vancouver.

Manandhar, Suresh and Ioannis Klapaftis.
2009. Semeval-2010 task 14: Evaluation
setting for word sense induction &
disambiguation systems. In Proceedings of
the Workshop on Semantic Evaluations:
Recent Achievements and Future Directions
(SEW-2009), pages 117–122, Boulder, CO.

Manandhar, Suresh, Ioannis Klapaftis,
Dmitriy Dligach, and Sameer Pradhan.
2010. Semeval-2010 task 14: Word sense
induction & disambiguation. In Proceedings
of the 5th International Workshop on Semantic
Evaluation, pages 63–68, Uppsala.

Meila, Marina. 2007. Comparing
clusterings—an information based
distance. Journal of Multivariate Analysis,
98:873–895.

Miller, G. 1955. Note on the bias of
information estimates. Information Theory
in Psychology II-B, pages 95–100.

Paninski, Liam. 2003. Estimation of entropy
and mutual information. Neural
Computation, 15:1,191–1,253.

684

http://www.mitpressjournals.org/action/showLinks?system=10.1162%2F089976603321780272
http://www.mitpressjournals.org/action/showLinks?system=10.1162%2F089976603321780272
http://www.mitpressjournals.org/action/showLinks?crossref=10.1007%2Fs10791-008-9066-8
http://www.mitpressjournals.org/action/showLinks?crossref=10.1007%2Fs10579-012-9205-0
http://www.mitpressjournals.org/action/showLinks?crossref=10.1007%2Fs10579-012-9205-0
http://www.mitpressjournals.org/action/showLinks?crossref=10.1016%2Fj.jmva.2006.11.013
http://www.mitpressjournals.org/action/showLinks?crossref=10.1002%2Frsa.10019
http://www.mitpressjournals.org/action/showLinks?crossref=10.1002%2Frsa.10019
http://www.mitpressjournals.org/action/showLinks?crossref=10.1063%2F1.166191


Li, Titov, and Sporleder Improved Estimation of Entropy for Word Sense Induction

Paninski, Liam. 2004. Estimating entropy
of m bins given fewer than m samples.
IEEE Transactions on Information Theory,
50(9):2,200–2,203.

Purandare, Amruta and Ted Pedersen. 2004.
Word sense discrimination by clustering
contexts in vector and similarity spaces.
In Proceedings of the CoNLL, pages 41–48,
Boston, MA.

Quenouille, M. 1956. Notes on bias and
estimation. Biometrika, 43:353–360.

Rosenberg, Andrew and Julia Hirschberg.
2007. V-measure: A conditional entropy-
based external cluster evaluation
measure. In Proceedings of the 2007
EMNLP-CoNll Joint Conference,
pages 410–420, Prague.

Schütze, Hinrich. 1998. Automatic word
sense discrimination. Computational
Linguistics, 24(1):97–123.

Steele, J. Michael. 1986. An Efron-Stein
inequality for nonsymmetric statistics.
Annals of Statistics, 14:753–758.

Strehl, Alexander and Joydeep Gosh. 2002.
Cluster ensembles: A knowledge reuse
framework for combining multiple
partitions. Journal of Machine Learning
Research, 3:583–617.

Strong, S., R. Koberle, S. R. van de Ruyter,
and W. Bialek. 1998. Entropy and
information in neural spike trains.
Physical Review Letters, 80:197–202.

Tukey, J. 1958. Bias and confidence in not
quite large samples. Annals of Mathematical
Statistics, 29:614.

Wang, Y. H. 1993. On the number of
successes in independent trials. Statistica
Sinica 3, 2:295–312.

Witte, Ian and Eibe Frank. 2005. Data
Mining: Practical Machine Learning Tools
and Techniques. Morgan Kaufmann,
Amsterdam.

Zhao, Y. and G. Karypis. 2005. Hierarchical
clustering algorithms for document
datasets. Data Mining and Knowledge
Discovery, 10(2):141–168.

685

http://www.mitpressjournals.org/action/showLinks?crossref=10.1093%2Fbiomet%2F43.3-4.353
http://www.mitpressjournals.org/action/showLinks?crossref=10.1103%2FPhysRevLett.80.197
http://www.mitpressjournals.org/action/showLinks?crossref=10.1007%2Fs10618-005-0361-3
http://www.mitpressjournals.org/action/showLinks?crossref=10.1007%2Fs10618-005-0361-3
http://www.mitpressjournals.org/action/showLinks?crossref=10.1109%2FTIT.2004.833360
http://www.mitpressjournals.org/action/showLinks?crossref=10.1214%2Faoms%2F1177706647
http://www.mitpressjournals.org/action/showLinks?crossref=10.1214%2Faoms%2F1177706647
http://www.mitpressjournals.org/action/showLinks?crossref=10.1214%2Faos%2F1176349952

















<<
  /ASCII85EncodePages false
  /AllowTransparency false
  /AutoPositionEPSFiles true
  /AutoRotatePages /None
  /Binding /Left
  /CalGrayProfile ()
  /CalRGBProfile (Adobe RGB \0501998\051)
  /CalCMYKProfile (None)
  /sRGBProfile (sRGB IEC61966-2.1)
  /CannotEmbedFontPolicy /Error
  /CompatibilityLevel 1.3
  /CompressObjects /Off
  /CompressPages true
  /ConvertImagesToIndexed true
  /PassThroughJPEGImages true
  /CreateJDFFile false
  /CreateJobTicket false
  /DefaultRenderingIntent /Default
  /DetectBlends true
  /DetectCurves 0.1000
  /ColorConversionStrategy /LeaveColorUnchanged
  /DoThumbnails false
  /EmbedAllFonts true
  /EmbedOpenType false
  /ParseICCProfilesInComments true
  /EmbedJobOptions true
  /DSCReportingLevel 0
  /EmitDSCWarnings false
  /EndPage -1
  /ImageMemory 1048576
  /LockDistillerParams true
  /MaxSubsetPct 100
  /Optimize false
  /OPM 1
  /ParseDSCComments true
  /ParseDSCCommentsForDocInfo true
  /PreserveCopyPage false
  /PreserveDICMYKValues true
  /PreserveEPSInfo true
  /PreserveFlatness true
  /PreserveHalftoneInfo false
  /PreserveOPIComments false
  /PreserveOverprintSettings true
  /StartPage 1
  /SubsetFonts false
  /TransferFunctionInfo /Apply
  /UCRandBGInfo /Remove
  /UsePrologue false
  /ColorSettingsFile (None)
  /AlwaysEmbed [ true
  ]
  /NeverEmbed [ true
  ]
  /AntiAliasColorImages false
  /CropColorImages true
  /ColorImageMinResolution 266
  /ColorImageMinResolutionPolicy /Warning
  /DownsampleColorImages false
  /ColorImageDownsampleType /Average
  /ColorImageResolution 300
  /ColorImageDepth 8
  /ColorImageMinDownsampleDepth 1
  /ColorImageDownsampleThreshold 1.50000
  /EncodeColorImages true
  /ColorImageFilter /FlateEncode
  /AutoFilterColorImages false
  /ColorImageAutoFilterStrategy /JPEG
  /ColorACSImageDict <<
    /QFactor 0.15
    /HSamples [1 1 1 1] /VSamples [1 1 1 1]
  >>
  /ColorImageDict <<
    /QFactor 0.15
    /HSamples [1 1 1 1] /VSamples [1 1 1 1]
  >>
  /JPEG2000ColorACSImageDict <<
    /TileWidth 256
    /TileHeight 256
    /Quality 30
  >>
  /JPEG2000ColorImageDict <<
    /TileWidth 256
    /TileHeight 256
    /Quality 30
  >>
  /AntiAliasGrayImages false
  /CropGrayImages true
  /GrayImageMinResolution 266
  /GrayImageMinResolutionPolicy /Warning
  /DownsampleGrayImages false
  /GrayImageDownsampleType /Average
  /GrayImageResolution 300
  /GrayImageDepth 8
  /GrayImageMinDownsampleDepth 2
  /GrayImageDownsampleThreshold 1.50000
  /EncodeGrayImages true
  /GrayImageFilter /FlateEncode
  /AutoFilterGrayImages false
  /GrayImageAutoFilterStrategy /JPEG
  /GrayACSImageDict <<
    /QFactor 0.15
    /HSamples [1 1 1 1] /VSamples [1 1 1 1]
  >>
  /GrayImageDict <<
    /QFactor 0.15
    /HSamples [1 1 1 1] /VSamples [1 1 1 1]
  >>
  /JPEG2000GrayACSImageDict <<
    /TileWidth 256
    /TileHeight 256
    /Quality 30
  >>
  /JPEG2000GrayImageDict <<
    /TileWidth 256
    /TileHeight 256
    /Quality 30
  >>
  /AntiAliasMonoImages false
  /CropMonoImages true
  /MonoImageMinResolution 900
  /MonoImageMinResolutionPolicy /Warning
  /DownsampleMonoImages false
  /MonoImageDownsampleType /Average
  /MonoImageResolution 1200
  /MonoImageDepth -1
  /MonoImageDownsampleThreshold 1.50000
  /EncodeMonoImages true
  /MonoImageFilter /CCITTFaxEncode
  /MonoImageDict <<
    /K -1
  >>
  /AllowPSXObjects false
  /CheckCompliance [
    /None
  ]
  /PDFX1aCheck true
  /PDFX3Check false
  /PDFXCompliantPDFOnly true
  /PDFXNoTrimBoxError true
  /PDFXTrimBoxToMediaBoxOffset [
    0.00000
    0.00000
    0.00000
    0.00000
  ]
  /PDFXSetBleedBoxToMediaBox true
  /PDFXBleedBoxToTrimBoxOffset [
    0.00000
    0.00000
    0.00000
    0.00000
  ]
  /PDFXOutputIntentProfile (None)
  /PDFXOutputConditionIdentifier ()
  /PDFXOutputCondition ()
  /PDFXRegistryName ()
  /PDFXTrapped /False

  /Description <<
    /CHS <FEFF4f7f75288fd94e9b8bbe5b9a521b5efa7684002000410064006f0062006500200050004400460020658768637b2654080020005000440046002f0058002d00310061003a0032003000300031002089c4830330028fd9662f4e004e2a4e1395e84e3a56fe5f6251855bb94ea46362800c52365b9a7684002000490053004f0020680751c6300251734e8e521b5efa7b2654080020005000440046002f0058002d00310061002089c483037684002000500044004600206587686376848be67ec64fe1606fff0c8bf753c29605300a004100630072006f00620061007400207528623763075357300b300260a853ef4ee54f7f75280020004100630072006f0062006100740020548c002000410064006f00620065002000520065006100640065007200200034002e003000204ee553ca66f49ad87248672c676562535f00521b5efa768400200050004400460020658768633002>
    /CHT <FEFF4f7f752890194e9b8a2d7f6e5efa7acb7684002000410064006f006200650020005000440046002065874ef67b2654080020005000440046002f0058002d00310061003a00320030003000310020898f7bc430025f8c8005662f70ba57165f6251675bb94ea463db800c5c08958052365b9a76846a196e96300295dc65bc5efa7acb7b2654080020005000440046002f0058002d003100610020898f7bc476840020005000440046002065874ef676848a737d308cc78a0aff0c8acb53c395b1201c004100630072006f00620061007400204f7f7528800563075357201d300260a853ef4ee54f7f75280020004100630072006f0062006100740020548c002000410064006f00620065002000520065006100640065007200200034002e003000204ee553ca66f49ad87248672c4f86958b555f5df25efa7acb76840020005000440046002065874ef63002>
    /DAN <FEFF004200720075006700200069006e0064007300740069006c006c0069006e006700650072006e0065002000740069006c0020006100740020006f007000720065007400740065002000410064006f006200650020005000440046002d0064006f006b0075006d0065006e007400650072002c00200064006500720020006600f800720073007400200073006b0061006c00200073006500730020006900670065006e006e0065006d00200065006c006c0065007200200073006b0061006c0020006f0076006500720068006f006c006400650020005000440046002f0058002d00310061003a0032003000300031002c00200065006e002000490053004f002d007300740061006e0064006100720064002000740069006c00200075006400760065006b0073006c0069006e00670020006100660020006700720061006600690073006b00200069006e00640068006f006c0064002e00200059006400650072006c006900670065007200650020006f0070006c00790073006e0069006e0067006500720020006f006d0020006f007000720065007400740065006c007300650020006100660020005000440046002f0058002d00310061002d006b006f006d00700061007400690062006c00650020005000440046002d0064006f006b0075006d0065006e007400650072002000660069006e006400650072002000640075002000690020006200720075006700650072006800e5006e00640062006f00670065006e002000740069006c0020004100630072006f006200610074002e0020004400650020006f007000720065007400740065006400650020005000440046002d0064006f006b0075006d0065006e0074006500720020006b0061006e002000e50062006e00650073002000690020004100630072006f00620061007400200065006c006c006500720020004100630072006f006200610074002000520065006100640065007200200034002e00300020006f00670020006e0079006500720065002e>
    /DEU <FEFF00560065007200770065006e00640065006e0020005300690065002000640069006500730065002000450069006e007300740065006c006c0075006e00670065006e0020007a0075006d002000450072007300740065006c006c0065006e00200076006f006e0020005000440046002f0058002d00310061003a0032003000300031002d006b006f006d00700061007400690062006c0065006e002000410064006f006200650020005000440046002d0044006f006b0075006d0065006e00740065006e002e0020005000440046002f0058002d003100610020006900730074002000650069006e0065002000490053004f002d004e006f0072006d0020006600fc0072002000640065006e002000410075007300740061007500730063006800200076006f006e0020006700720061006600690073006300680065006e00200049006e00680061006c00740065006e002e0020005700650069007400650072006500200049006e0066006f0072006d006100740069006f006e0065006e0020007a0075006d002000450072007300740065006c006c0065006e00200076006f006e0020005000440046002f0058002d00310061002d006b006f006d00700061007400690062006c0065006e0020005000440046002d0044006f006b0075006d0065006e00740065006e002000660069006e00640065006e002000530069006500200069006d0020004100630072006f006200610074002d00480061006e00640062007500630068002e002000450072007300740065006c006c007400650020005000440046002d0044006f006b0075006d0065006e007400650020006b00f6006e006e0065006e0020006d006900740020004100630072006f00620061007400200075006e0064002000410064006f00620065002000520065006100640065007200200034002e00300020006f0064006500720020006800f600680065007200200067006500f600660066006e00650074002000770065007200640065006e002e>
    /ESP <FEFF005500740069006c0069006300650020006500730074006100200063006f006e0066006900670075007200610063006900f3006e0020007000610072006100200063007200650061007200200064006f00630075006d0065006e0074006f00730020005000440046002000640065002000410064006f00620065002000710075006500200073006500200064006500620065006e00200063006f006d00700072006f0062006100720020006f002000710075006500200064006500620065006e002000630075006d0070006c006900720020006c00610020006e006f0072006d0061002000490053004f0020005000440046002f0058002d00310061003a00320030003000310020007000610072006100200069006e00740065007200630061006d00620069006f00200064006500200063006f006e00740065006e00690064006f00200067007200e1006600690063006f002e002000500061007200610020006f006200740065006e006500720020006d00e1007300200069006e0066006f0072006d00610063006900f3006e00200073006f0062007200650020006c0061002000630072006500610063006900f3006e00200064006500200064006f00630075006d0065006e0074006f0073002000500044004600200063006f006d00700061007400690062006c0065007300200063006f006e0020006c00610020006e006f0072006d00610020005000440046002f0058002d00310061002c00200063006f006e00730075006c007400650020006c006100200047007500ed0061002000640065006c0020007500730075006100720069006f0020006400650020004100630072006f006200610074002e002000530065002000700075006500640065006e00200061006200720069007200200064006f00630075006d0065006e0074006f00730020005000440046002000630072006500610064006f007300200063006f006e0020004100630072006f006200610074002c002000410064006f00620065002000520065006100640065007200200034002e003000200079002000760065007200730069006f006e0065007300200070006f00730074006500720069006f007200650073002e>
    /FRA <FEFF005500740069006c006900730065007a00200063006500730020006f007000740069006f006e00730020006100660069006e00200064006500200063007200e900650072002000640065007300200064006f00630075006d0065006e00740073002000410064006f006200650020005000440046002000710075006900200064006f006900760065006e0074002000ea0074007200650020007600e9007200690066006900e900730020006f0075002000ea00740072006500200063006f006e0066006f0072006d00650073002000e00020006c00610020006e006f0072006d00650020005000440046002f0058002d00310061003a0032003000300031002c00200075006e00650020006e006f0072006d0065002000490053004f00200064002700e9006300680061006e0067006500200064006500200063006f006e00740065006e00750020006700720061007000680069007100750065002e00200050006f0075007200200070006c007500730020006400650020006400e9007400610069006c007300200073007500720020006c006100200063007200e9006100740069006f006e00200064006500200064006f00630075006d0065006e00740073002000500044004600200063006f006e0066006f0072006d00650073002000e00020006c00610020006e006f0072006d00650020005000440046002f0058002d00310061002c00200076006f006900720020006c00650020004700750069006400650020006400650020006c0027007500740069006c0069007300610074006500750072002000640027004100630072006f006200610074002e0020004c0065007300200064006f00630075006d0065006e00740073002000500044004600200063007200e900e90073002000700065007500760065006e0074002000ea0074007200650020006f007500760065007200740073002000640061006e00730020004100630072006f006200610074002c002000610069006e00730069002000710075002700410064006f00620065002000520065006100640065007200200034002e0030002000650074002000760065007200730069006f006e007300200075006c007400e90072006900650075007200650073002e>
    /ITA (Utilizzare queste impostazioni per creare documenti Adobe PDF che devono essere conformi o verificati in base a PDF/X-1a:2001, uno standard ISO per lo scambio di contenuto grafico. Per ulteriori informazioni sulla creazione di documenti PDF compatibili con PDF/X-1a, consultare la Guida dell'utente di Acrobat. I documenti PDF creati possono essere aperti con Acrobat e Adobe Reader 4.0 e versioni successive.)
    /JPN <FEFF30b030e930d530a330c330af30b330f330c630f330c4306e590963db306b5bfe3059308b002000490053004f00206a196e96898f683c306e0020005000440046002f0058002d00310061003a00320030003000310020306b6e9662e03057305f002000410064006f0062006500200050004400460020658766f830924f5c62103059308b305f3081306b4f7f75283057307e30593002005000440046002f0058002d0031006100206e9662e0306e00200050004400460020658766f84f5c6210306b306430443066306f3001004100630072006f006200610074002030e630fc30b630ac30a430c9309253c2716730573066304f30603055304430023053306e8a2d5b9a30674f5c62103055308c305f0020005000440046002030d530a130a430eb306f3001004100630072006f0062006100740020304a30883073002000410064006f00620065002000520065006100640065007200200034002e003000204ee5964d3067958b304f30533068304c3067304d307e30593002>
    /KOR <FEFFc7740020c124c815c7440020c0acc6a9d558c5ec0020c791c131d558b294002000410064006f0062006500200050004400460020bb38c11cb2940020d655c778c7740020d544c694d558ba700020adf8b798d53d0020cee8d150d2b8b97c0020ad50d658d558b2940020bc29bc95c5d00020b300d55c002000490053004f0020d45cc900c7780020005000440046002f0058002d00310061003a0032003000300031c7580020addcaca9c5d00020b9dec544c57c0020d569b2c8b2e4002e0020005000440046002f0058002d003100610020d638d65800200050004400460020bb38c11c0020c791c131c5d00020b300d55c0020c790c138d55c0020c815bcf4b2940020004100630072006f0062006100740020c0acc6a90020c124ba85c11cb97c0020cc38c870d558c2edc2dcc624002e0020c774b807ac8c0020c791c131b41c00200050004400460020bb38c11cb2940020004100630072006f0062006100740020bc0f002000410064006f00620065002000520065006100640065007200200034002e00300020c774c0c1c5d0c11c0020c5f40020c2180020c788c2b5b2c8b2e4002e>
    /NLD (Gebruik deze instellingen om Adobe PDF-documenten te maken die moeten worden gecontroleerd of moeten voldoen aan PDF/X-1a:2001, een ISO-standaard voor het uitwisselen van grafische gegevens. Raadpleeg de gebruikershandleiding van Acrobat voor meer informatie over het maken van PDF-documenten die compatibel zijn met PDF/X-1a. De gemaakte PDF-documenten kunnen worden geopend met Acrobat en Adobe Reader 4.0 en hoger.)
    /NOR <FEFF004200720075006b00200064006900730073006500200069006e006e007300740069006c006c0069006e00670065006e0065002000740069006c002000e50020006f0070007000720065007400740065002000410064006f006200650020005000440046002d0064006f006b0075006d0065006e00740065007200200073006f006d00200073006b0061006c0020006b006f006e00740072006f006c006c0065007200650073002c00200065006c006c0065007200200073006f006d0020006d00e50020007600e6007200650020006b006f006d00700061007400690062006c00650020006d006500640020005000440046002f0058002d00310061003a0032003000300031002c00200065006e002000490053004f002d007300740061006e006400610072006400200066006f007200200075007400760065006b0073006c0069006e00670020006100760020006700720061006600690073006b00200069006e006e0068006f006c0064002e00200048007600690073002000640075002000760069006c0020006800610020006d0065007200200069006e0066006f0072006d00610073006a006f006e0020006f006d002000680076006f007200640061006e0020006400750020006f007000700072006500740074006500720020005000440046002f0058002d00310061002d006b006f006d00700061007400690062006c00650020005000440046002d0064006f006b0075006d0065006e007400650072002c0020007300650020006200720075006b00650072006800e5006e00640062006f006b0065006e00200066006f00720020004100630072006f006200610074002e0020005000440046002d0064006f006b0075006d0065006e00740065006e00650020006b0061006e002000e50070006e00650073002000690020004100630072006f00620061007400200065006c006c00650072002000410064006f00620065002000520065006100640065007200200034002e003000200065006c006c00650072002000730065006e006500720065002e>
    /PTB <FEFF005500740069006c0069007a006500200065007300730061007300200063006f006e00660069006700750072006100e700f50065007300200064006500200066006f0072006d00610020006100200063007200690061007200200064006f00630075006d0065006e0074006f0073002000410064006f00620065002000500044004600200063006100700061007a0065007300200064006500200073006500720065006d0020007600650072006900660069006300610064006f00730020006f0075002000710075006500200064006500760065006d00200065007300740061007200200065006d00200063006f006e0066006f0072006d0069006400610064006500200063006f006d0020006f0020005000440046002f0058002d00310061003a0032003000300031002c00200075006d0020007000610064007200e3006f002000640061002000490053004f002000700061007200610020006f00200069006e007400650072006300e2006d00620069006f00200064006500200063006f006e0074006500fa0064006f00200067007200e1006600690063006f002e002000500061007200610020006f00620074006500720020006d00610069007300200069006e0066006f0072006d006100e700f50065007300200073006f00620072006500200063006f006d006f00200063007200690061007200200064006f00630075006d0065006e0074006f0073002000500044004600200063006f006d00700061007400ed007600650069007300200063006f006d0020006f0020005000440046002f0058002d00310061002c00200063006f006e00730075006c007400650020006f0020004700750069006100200064006f002000750073007500e100720069006f00200064006f0020004100630072006f006200610074002e0020004f007300200064006f00630075006d0065006e0074006f00730020005000440046002000630072006900610064006f007300200070006f00640065006d0020007300650072002000610062006500720074006f007300200063006f006d0020006f0020004100630072006f006200610074002000650020006f002000410064006f00620065002000520065006100640065007200200034002e0030002000650020007600650072007300f50065007300200070006f00730074006500720069006f007200650073002e>
    /SUO <FEFF004b00e40079007400e40020006e00e40069007400e4002000610073006500740075006b007300690061002c0020006b0075006e0020006c0075006f0074002000410064006f0062006500200050004400460020002d0064006f006b0075006d0065006e007400740065006a0061002c0020006a006f0074006b00610020007400610072006b0069007300740065007400610061006e00200074006100690020006a006f006900640065006e0020007400e400790074007900790020006e006f00750064006100740074006100610020005000440046002f0058002d00310061003a0032003000300031003a007400e400200065006c0069002000490053004f002d007300740061006e006400610072006400690061002000670072006100610066006900730065006e002000730069007300e4006c006c00f6006e00200073006900690072007400e4006d00690073007400e4002000760061007200740065006e002e0020004c0069007300e40074006900650074006f006a00610020005000440046002f0058002d00310061002d00790068007400650065006e0073006f00700069007600690065006e0020005000440046002d0064006f006b0075006d0065006e0074007400690065006e0020006c0075006f006d0069007300650073007400610020006f006e0020004100630072006f0062006100740069006e0020006b00e400790074007400f6006f0070007000610061007300730061002e00200020004c0075006f0064007500740020005000440046002d0064006f006b0075006d0065006e00740069007400200076006f0069006400610061006e0020006100760061007400610020004100630072006f0062006100740069006c006c00610020006a0061002000410064006f00620065002000520065006100640065007200200034002e0030003a006c006c00610020006a006100200075007500640065006d006d0069006c006c0061002e>
    /SVE <FEFF0041006e007600e4006e00640020006400650020006800e4007200200069006e0073007400e4006c006c006e0069006e006700610072006e00610020006f006d002000640075002000760069006c006c00200073006b006100700061002000410064006f006200650020005000440046002d0064006f006b0075006d0065006e007400200073006f006d00200073006b00610020006b006f006e00740072006f006c006c006500720061007300200065006c006c0065007200200073006f006d0020006d00e50073007400650020006d006f0074007300760061007200610020005000440046002f0058002d00310061003a0032003000300031002c00200065006e002000490053004f002d007300740061006e00640061007200640020006600f6007200200075007400620079007400650020006100760020006700720061006600690073006b007400200069006e006e0065006800e5006c006c002e00200020004d0065007200200069006e0066006f0072006d006100740069006f006e0020006f006d00200068007500720020006d0061006e00200073006b00610070006100720020005000440046002f0058002d00310061002d006b006f006d00700061007400690062006c00610020005000440046002d0064006f006b0075006d0065006e0074002000660069006e006e00730020006900200061006e007600e4006e00640061007200680061006e00640062006f006b0065006e002000740069006c006c0020004100630072006f006200610074002e002000200053006b006100700061006400650020005000440046002d0064006f006b0075006d0065006e00740020006b0061006e002000f600700070006e00610073002000690020004100630072006f0062006100740020006f00630068002000410064006f00620065002000520065006100640065007200200034002e00300020006f00630068002000730065006e006100720065002e>
    /ENU (Use these settings to create Adobe PDF documents for submission to The Sheridan Press. Configured for Adobe Acrobat Distiller v8.0 02-28-07.)
  >>
  /Namespace [
    (Adobe)
    (Common)
    (1.0)
  ]
  /OtherNamespaces [
    <<
      /AsReaderSpreads false
      /CropImagesToFrames true
      /ErrorControl /WarnAndContinue
      /FlattenerIgnoreSpreadOverrides false
      /IncludeGuidesGrids false
      /IncludeNonPrinting false
      /IncludeSlug false
      /Namespace [
        (Adobe)
        (InDesign)
        (4.0)
      ]
      /OmitPlacedBitmaps false
      /OmitPlacedEPS false
      /OmitPlacedPDF false
      /SimulateOverprint /Legacy
    >>
    <<
      /AddBleedMarks false
      /AddColorBars false
      /AddCropMarks false
      /AddPageInfo false
      /AddRegMarks false
      /ConvertColors /ConvertToCMYK
      /DestinationProfileName ()
      /DestinationProfileSelector /DocumentCMYK
      /Downsample16BitImages true
      /FlattenerPreset <<
        /PresetSelector /HighResolution
      >>
      /FormElements false
      /GenerateStructure false
      /IncludeBookmarks false
      /IncludeHyperlinks false
      /IncludeInteractive false
      /IncludeLayers false
      /IncludeProfiles false
      /MultimediaHandling /UseObjectSettings
      /Namespace [
        (Adobe)
        (CreativeSuite)
        (2.0)
      ]
      /PDFXOutputIntentProfileSelector /DocumentCMYK
      /PreserveEditing true
      /UntaggedCMYKHandling /LeaveUntagged
      /UntaggedRGBHandling /UseDocumentProfile
      /UseDocumentBleed false
    >>
  ]
>> setdistillerparams
<<
  /HWResolution [2400 2400]
  /PageSize [612.000 792.000]
>> setpagedevice

