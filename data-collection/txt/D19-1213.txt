



















































Guiding the Flowing of Semantics: Interpretable Video Captioning via POS Tag


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 2068–2077,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

2068

Guiding the Flowing of Semantics: Interpretable Video Captioning via
POS Tag

Xinyu Xiao1,2, Lingfeng Wang1, Bin Fan1, Shiming Xiang1,2, Chunhong Pan1
1NLPR, Institute of Automation, Chinese Academy of Sciences

2School of Artificial Intelligence, University of Chinese Academy of Sciences
{xinyu.xiao, lfwang, bfan, smxiang, chpan}@nlpr.ia.ac.cn

Abstract

In the current video captioning models, the
video frames are collected in one network
and the semantics are mixed into one feature,
which not only increase the difficulty of the
caption decoding, but also decrease the inter-
pretability of the captioning models. To ad-
dress these problems, we propose an Adaptive
Semantic Guidance Network (ASGN), which
instantiates the whole video semantics to dif-
ferent POS-aware semantics with the supervi-
sion of part of speech (POS) tag. In the encod-
ing process, the POS tag activates the related
neurons and parses the whole semantic infor-
mation into corresponding encoded video rep-
resentations. Furthermore, the potential of the
model is stimulated by the POS-aware video
features. In the decoding process, the related
video features of noun and verb are used as
the supervision to construct a new adaptive at-
tention model which can decide whether to at-
tend to the video feature or not. With the ex-
plicit improving of the interpretability of the
network, the learning process is more transpar-
ent and the results are more predictable. Ex-
tensive experiments demonstrate the effective-
ness of our model when compared with state-
of-the-art models.

1 Introduction

Video captioning, which transforms the semantic
information in a video to a natural statement, has
received wide attention recently. The series of
scenes (both related and unrelated) in video frames
bring a huge challenge for the task of video cap-
tioning. Therefore, mastering the ability to pro-
cess the correlated and irrelevant semantic infor-
mation can improve the performance and inter-
pretability of the model of video captioning.

The classical deep learning based video cap-
tioning methods (Venugopalan et al., 2015a,b) in-
corporate both CNN and LSTM together as an

a man is lifting weights
DET NOUN AUX VERB NOUN

a girl is talking on the phone to another girl 

DET NOUN AUX VERB ADP DET NOUN ADP DET NOUN

(a)

(b)

Figure 1: Examples of video description and its POS
tags. The definition of POS tag can be seen in Section
3.1. From these examples, it can be seen that the words
that are belonging to the same POS tags undergo inflec-
tion for similar semantic properties and different POS
tags reflect different semantic properties distinctly.

encoder-decoder architecture which extracts all
the semantic information in one stream and fuses
them in a single feature. In the architecture, the
transformation of the semantics to words predic-
tion is uncertainty. Some other recent studies
(Dong et al., 2017; Wu et al., 2018) attempt to
improve the interpretability of the transformation
process. But the flowing of different semantic
information in the network streams and the acti-
vated neurons in network are still ambiguous. To
address these issues, the first is to take the dis-
crimination of different semantic information into
consideration. According to (Horoufchin et al.,
2018; Fargier and Laganaro, 2015), nouns and
verbs typically describe concrete objects and ac-
tions which can recruit the canonical neurons sys-
tem to activate corresponding representation pat-
terns in brain. This indicates that differential neu-
rons in brain are activated for lexical selection of
action and object words. Therefore, the part of
speech (POS) tag can be applied to guide the flow-
ing of different semantics into the corresponding
network streams.



2069

A POS tag is the category of words which has
similar grammatical properties. The words as-
signed to the same POS tag generally reflect sim-
ilar properties within the grammatical structure of
sentence. Fig.1 (a) shows a video and its cap-
tion “a man is lifting weights”. The nouns “man”,
“weights” and the verb “lifting” are belonging to
different POS tags and referring to different se-
mantic information in video. Moreover, in Fig. 1
(b), the caption of video is “a girl is talking on
the phone to another girl”. The words of “girl”,
“talking” and “phone” distinctly have their corre-
sponding visual signals in the video, but the oth-
ers are uncertain. This example indicates that the
nouns of objects and verbs of actions are generally
referred to visual words. Another property of the
POS tag is that when given a fixed sentence, the
POS tags of all the words in the sentence are fixed.
It ensures the reliability of the POS tag in helping
to extract and guide corresponding semantics.

According to Merity et al. (Merity et al., 2016),
the prediction of the next word not always need
to attend the visual feature. The gradients from
non-visual words could mislead and diminish the
effectiveness of the visual signal in guiding cap-
tion generation. To this end, Lu et al. (Lu et al.,
2016) proposed a “visual sentinel”, which applies
the hidden state in LSTM as supervision to adap-
tively decide if it is necessary to input visual fea-
ture to the language model when generating the
next word. However, the supervision information
from the hidden state isn’t credible which cannot
make sure it contains the corresponding visual de-
cision signals. According to the properties of the
POS tag, the nouns and verbs can be applied as
the supervision to distinguish whether the remain-
ing words in the sentence are visual words or not.

In this paper, to explicitly improve the inter-
pretability of video captioning model, we pro-
pose an Adaptive Semantic Guidance Network
(ASGN), which instantiates the whole video se-
mantics by part of speech (POS) tags to different
POS-aware semantics. At first, a POS-aware se-
mantic guider is proposed. It predicts the POS tags
of the words in descriptions, and guides differ-
ent POS-aware semantic information of video into
corresponding network streams by the predicted
POS tags. In this process, the specific CNN neu-
rons are activated under the supervision of POS
tags and the whole visual semantics are parsed into
POS tags related video features. Moreover, de-

pending on the POS-aware video features, a new
adaptive attention operation is introduced. The
video features related to noun and verb are used
as a supervision to get a sentinel gate, which de-
cides how much the attended feature can be im-
ported into the decoder LSTM when generating
the next word. A reinforcement learning (RL)
method is applied to optimize our model which
further demonstrates the validity of our method on
the video captioning task.

The main contributions of this paper are:

- Designing a POS-aware semantic guider to
predict the POS tags of words and guide dif-
ferent semantic information of video into cor-
responding network streams. Under the su-
pervision of POS tags, the CNN neurons are
selectively activated and aware of the related
POS tags, so that the whole video semantics
are parsed into the corresponding POS-aware
video features.

- Depending on the noun and verb instanti-
ated video features, a new adaptive attention
model is constructed to decide how much the
visual feature is imported into the decoder.

- Due to the guiding of POS tags, the flow-
ing of the type of semantic in which network
stream can be easily clarified. With the super-
vision of the noun and verb related features,
the judgment of the predicted word is visual
word or not is more reasonable. These make
the learning process more interpretable while
achieving state-of-the-art performance.

2 Related Works

Here, we first review the recent implements of
the POS tag in computer vision, then review the
most relevant works on video description task
like attention-based methods and interpretable im-
proved models.

The POS tag has been received attention in
some computer vision tasks, like visual question
answering (Wang et al., 2018b) and image cap-
tioning (A et al., 2019). He et al. (He et al., 2017)
utilized the POS tag of each word to determine
whether it is essential to input image representa-
tion into the word generator. Wang et al. (Wang
et al., 2018b) exploited the POS tag guided atten-
tion model in VQA to put more emphasis on the
important words such as nouns, verbs and adjec-
tives. All these methods realized the importance



2070

CNN

CNN

CNN

CNN

LSTMp LSTMp LSTMp LSTMp

LSTMw LSTMw LSTMw LSTMw

LSTMd LSTMd LSTMd LSTMd

NUM NOUN AUX EOS

BOS two men boat

two men are EOS
Description Generator Learning

POS Tag Learning

v1

v2

vi

vNs

●
●
●

●
●
●

●
●
●

●
●
●

2D CNN

●●●

●●●

●●●

●●●

●●●

3D CNN

v

V

gna gna gna gna

   
V V V V

●
●
●

●
●
●

●
●
●

V

Figure 2: The architecture of our Adaptive Semantic Guidance Network (ASGN) which can shunt the re-
quired semantic information into different video features when generating every word in caption. The features
{v1,v2, . . . ,vNs} are extracted by the CNN modules. � is the hadamard product module. gna is the new adaptive
attention model. A single-layer LSTM is set as the POS tag decoder and a two-layer stacked LSTM is set as the
language model. The BOS and EOS denote the begin-of-sentence and end-of-sentence, respectively.

of the POS tag in the linguistic computer vision.
However, they ignore the important property of
POS tag, which relates to different visual seman-
tics with different types of POS tag.

Attention-based methods have been widely
used in visual captioning models. Yao et al. (Yao
et al., 2015) considered the temporal structure of
video and proposed a temporal attention mecha-
nism to generate descriptions. Lu et al. (Lu et al.,
2016) proposed an adaptive attention model in im-
age captioning which can decide either to look at
the image or to rely on the context of sentence to
generate the next word.

Because of the highly nonlinearity and unclear
working mechanism of neural networks, the op-
erational processes of neural networks are always
treated as black-box processes. For the video
description task, some researchers (Dong et al.,
2017; Wu et al., 2018) attempted to improve the
interpretability of video description models. Dong
et al. (Dong et al., 2017) interpreted the learned
features of each neuron by a wide range of visual
concepts in the video description task. Wu et al.
(Wu et al., 2018) considered both the motion infor-
mation and the sentence semantic structure with
an attentive structured localization mechanism to
enhance the captioning model’s interpretability.

In this paper, we find the POS tag can be em-
ployed as a supervision to process irrelevant or
relevant semantic information in video descrip-
tion task. Under the supervision of POS tag, an
Adaptive Semantic Guidance Network (ASGN) is
proposed to guide different POS-aware semantic
information of video into corresponding network

streams. Moreover, the video features related to
noun and verb are used to get a sentinel gate which
can decide how much the attended feature can be
imported to the decoding process.

3 Our Method

In this section, we describe our ASGN in detail.
First, the POS-aware semantic guider with the
POS tag learning model is introduced. Next, an
adaptive attention model which constructs a new
sentinel gate will be presented. Finally, the de-
scription generator and its learning methods are
introduced.

3.1 POS-Aware Semantic Guider

The key of the proposed POS-aware semantic
guider is the POS tag, which is used to guide
different semantic information of video into cor-
responding network streams. Supposing that we
have a video described by a textual sentence S,
which consists of T words. The POS tag set of
each word in S is defined as P . To achieve this, a
POS tag learning model is designed to predict the
POS tags of the words in description.

We refer to (Al-Rfou et al., 2013) and anno-
tate the POS tags of captions in training set by
the polyglot toolkit1. Polyglot toolkit is a natu-
ral language pipeline that supports massive multi-
lingual applications, including the POS tag identi-
fication. According to the setting of the polyglot
toolkit, there are seventeen categories of POS tags:
noun (NOUN), verb (VERB), adjective (ADJ),

1http://polyglot.readthedocs.org

http://polyglot.readthedocs.org


2071

adposition (ADP), adverb (ADV), auxiliary verb
(AUX), coordinating conjunction (CONJ), de-
terminer (DET), interjection (INTJ), numeral
(NUM), particle (PART), pronoun (PRON), proper
noun (PROPN), punctuation (PUNCT), subordi-
nating conjunction (SCONJ), symbol (SYM) and
other unknown types (X). Different POS tags have
different descriptions or embellishments. Follow-
ing the universal set of this toolkit, the number of
POS tag categories is defined as Ns = 17.

Based on these POS tag categories, an encoder-
decoder framework is proposed to predict the spe-
cific POS tag of each word in sentences, which
can be seen in Fig. 2. Specifically, the video fea-
ture V is concatenated from the extractions of the
2D CNN and 3D CNN. Then, V is flowed into Ns
semantic guiding CNN modules. Each CNN mod-
ule relates to a corresponding POS tag category.
The outputs of these CNN modules are Ns video
features vi, where i = 1, . . . , Ns. The mean-
pooled feature v̄ = 1Ns

∑
i=1 vi and the sentence

S = {s0, s1, . . . , sT } are taken as the inputs to
the POS tag decoder LSTMp, where s0 is defined
as the begin-of-sentence (BOS). All the POS tags
of words in caption are sequentially generated by
LSTMp. The hidden state hpt is updated at time
step t ∈ {0, . . . , T} through:

hpt = LSTM
p(Wpest, v̄,h

p
t−1), (1)

where Wpe ∈ RNh×V is the word embedding ma-
trix,Nh is the length of hidden state and V denotes
the vocabulary size of the corresponding dataset’s
text library.

Given the ground-truth POS tags which are an-
notated to the corresponding sentence. Therefore,
the associated POS tags of the k-th video V are
Pk = {pk0, . . . , pkT }. We define the POS tag learn-
ing loss as:

Lp =−
1

N

N∑
k=1

T∑
t=0

log(p(pkt |pk0:t−1, v̄k)), (2)

where N is the number of training examples.
In the encoder-decoder framework, the POS tag
probability vector pt is predicted at time step
t. The mapping function is pt = f(h

p
t ) =

Softmax(FC(hpt )), where f : h
p
t → pt is a Soft-

max mixed function and FC is a full connection
layer. The predicted POS tag has the maximum
probability in pt. Similar to visual captioning, the
learned POS-aware semantic guider applies each

predicted word to predict the POS tag of the next
word in testing.

The learned pt is used to guide the flowing of
semantics from the Ns CNN modules at time step
t . The i-th CNN module is related to the i-th POS
tag category. The outputs of the CNN modules are
concatenated as Vr = [v1,v2, . . . ,vNs ], where
vi is generated by the specific neuron of the i-th
CNN module. The POS tag vector pt is applied
to activate the specific CNN module at the t-th
step. The operation is implemented by a hadamard
product module on the channel level:

Vrt = V
r � pt, (3)

where Vrt represents the video representation at
time step t. Then the video feature Vrt which con-
tains relevant visual semantic information will be
inputted into the language model at time step t.

Generally, the learned POS tag representations
can be used to activate the specific neurons of the
CNN encoder and parse the whole video represen-
tation to guide semantic information into corre-
sponding feature representations at each time step.
Compared with the normal CNN+LSTM model,
our POS-aware semantic guider constructs a cor-
relation between different types of POS-aware se-
mantic and the corresponding CNN modules.

3.2 Adaptive Attention Model

Although, Lu et al. (Lu et al., 2016) proposed a
sentinel to decide whether the predicted words are
visual words or not. Their model is learning from
the gradient of back propagation, which is still an
ambiguous process. The video features related to
different POS tags have different properties. For
example, the video features related to noun and
verb always have sufficient visual signals. Accord-
ing to the properties of POS tags, we propose a
more credible adaptive spatial attention model to
predict next word. Specifically, the related video
features of noun and verb are applied as the su-
pervision to get a sentinel gate. Through the sen-
tinel gate, when generating the next word, we can
distinguish its visual word or not and decide how
much the attended feature can be imported into the
decoder LSTM.

First, the video feature Vrt is reshaped to V
r
t =

[vrt1,v
r
t2, . . . ,v

r
tm], where m is the value of the

width times height of Vrt . Normal attention model
is defined as ga(Vrt ,h

d
t−1). h

d
t−1 is generated by

the description decoder LSTMd which is shown in



2072

Fig. 2. Formally, for the t-th time step, the atten-
tion part of the model ga is defined as follows:

zrt = tanh((W
rVrt + b

r)⊕Whrhdt−1),
αrt = softmax(W

αrzrt + b
αr), (4)

where Wr ∈ Rl×(Nc∗Ns), Whr ∈ Rl×Nh , Wαr ∈
Rl×1 are the transformation matrices that map the
CNN feature and hidden state to the same size; Nc
is the channel size of vi; br ∈ Rl and bαr ∈ R1
are the model biases. αrt is the attention weight
related to Vrt .

Second, to improve the structure of ga, our
model learns to use the related video features
of noun and verb as a supervision to distinguish
whether the words in a sentence are visual words
or not. All of the Ns video features are extracted
after the ReLU operation, and thus the values in
these feature maps are not less than zero. It indi-
cates that the value of the feature map can reflect
the response activation of the corresponding CNN
module. We assign the related feature maps of
noun and verb as the reference and obtain a value
R = mean[ 12m

∑m
j=1(vn+vv)], where vn and vv

are the related features of noun and verb from Vr.
Depending on the attended visual feature, a formu-
lae is introduced to ascertain whether the current t
word is a visual word or not:

cvt =

Ns∑
i=1

{pit if mean(
m∑
j=1

αrtjv
ri
j ) ≥ R}, (5)

where cvt is the weight of visual word at time step
t, pit is i-th value in the t-th POS tag representation
pt. Therefore, the weight of non-visual word is
cnvt = 1 − cvt . In our method, the sentinel gate is
defined as:

βt =
1

1 + exp(−log( c
v
t

cnvt
))
. (6)

The design of the sentinel gate can avoid the
gate value of βt being too small or too large. Based
on βt, the new adaptive attention model gna can
decide how much the attended feature can be im-
ported into the decoder LSTM as follows:

art = βtg
a = βt

m∑
j=1

αrtjv
r
tj . (7)

3.3 Description Generator

In the description generation stage, we adopt a
stacked two-layer LSTM to generate captions,
namely LSTMw and LSTMd. Following (Don-
ahue et al., 2015), the first LSTM layer LSTMw is
applied to encode the inputted sentence to enhance
the textual context information of each word vec-
tor hwt .

In the decoding stage, the encoded word vector
hwt and the processed video feature a

r
t are taken as

the inputs at t time step. The updating procedure
from 0 to T of LSTMd is written below:

hdt = LSTM
d(hwt ,a

r
t ,h

d
t−1), (8)

where hdt is the current output of LSTM
d at time

step t, the hd−1 can be set as a null vector.

3.3.1 Description Generator Learning
Maximum Likelihood Estimation: Given the k-
th video Vk and the associated sentence Wk =
{wk0 , . . . , wkT }, the generator loss can be formu-
lated as follows with the optimization of maxi-
mum likelihood estimation (MLE):

Ls =−
λ

N

N∑
k=1

T∑
t=0

log(p(wkt |wk0:t−1,Vk)), (9)

where p(wkt |wk0:t−1,Vk) is obtained from a Soft-
max mixed function; λ is the hyper-parameter.

Policy Gradient Optimization: For a fair com-
parison with recent works, the policy gradient
(PG) technique is adopted as the optimizer to
training our model. The objective in learning is
to minimize the negative expected reward of the
complete sampled sentenceWs = {ws0, . . . , wsT }:

Lθ = EWs∼pθ [r(W
s)], (10)

where r(Ws) is calculated by comparing sampled
caption with the reference caption in the speci-
fied evaluation metric. Following the implemen-
tation in (Rennie et al., 2017), we apply a single
Monte-Carlo sample to calculate the relative re-
ward ∆r(Ws), which is computed by a baseline
reward b. b is obtained by performing greedy de-
coding:

b = r(Ŵ), Ŵ = arg max p(wt|hdt ),
∇θLθ ≈− (r(Ws)− r(Ŵ))∇θ log[pθ(Ws)].

(11)



2073

Table 1: Ablation studies of our proposed video captioning model on Youtube2Text and MSR-VTT datasets. The
(RL) indicates this model is optimized by the reinforcement learning. The best results are in bold.

Model
Youtube2Text MSR-VTT

B-4 M R C B-4 M R C
ASGN 0.494 0.325 0.690 0.733 0.271 0.266 0.587 0.439
ASGN+L 0.501 0.329 0.699 0.762 0.391 0.268 0.595 0.449
ASGN+LA 0.514 0.331 0.696 0.758 0.384 0.269 0.602 0.448
ASGN+LNA 0.518 0.333 0.700 0.776 0.395 0.274 0.609 0.465
ASGN+LNA (RL) 0.521 0.333 0.703 0.803 0.405 0.278 0.615 0.490

4 Experiments

4.1 Dataset and Evaluation

We report the results of our method on the
Youtube2Text (Guadarrama et al., 2013) and
MSR-VTT (Xu et al., 2016) datasets. The
Youtube2Text dataset contains 1970 YouTube
video clips. According to the publicly provided
splits (Venugopalan et al., 2015b), 1200 videos are
used for training, 100 videos for validation and the
rest are used for testing. MSR-VTT is the largest
public dataset for video captioning up to now.
We follow the public splits (Venugopalan et al.,
2015a) and divide them into 6,513, 497 and 2,990
samples for training, validation and testing, re-
spectively. We reserve the words that appear in the
training set and yield two vocabularies which con-
tain 12,182 and 16,630 words for Youtube2Text
and MSR-VTT datasets, respectively.

In evaluation, we report the following metrics:
B-N (N=2,3,4) (Papineni et al., 2002), Meteor
(Banerjee and Lavie, 2005), Rouge-L (Lin, 2004)
and CIDEr (Vedantam et al., 2015). All the met-
rics are computed by the MS-COCO caption eval-
uation tool2.

4.2 Training Details

CNN Encoder: For the video representations, we
use a 2D CNN and a 3D CNN as the CNN encoder
collectively. The 3D CNN can operate all video
frames as a whole, which ensures the extracted
visual features contain all the semantic informa-
tion. The 2D CNN has more efficient learning and
representation capacity. The details of these two
CNNs can be seen as follows:

• 2D CNN We use ResNet-152 (He et al.,
2016) as the 2D CNN model. The feature
map is taken from the res5c layer (2,048-
dim).

2https://github.com/tylin/coco-caption

• 3D CNN The C3D (Tran et al., 2015) is ap-
plied as the 3D CNN model. The feature map
is extracted from the conv5b layer.

The equally-spaced 16 and 32 frames are sam-
pled from one video clip for Youtube2Text and
MSR-VTT, respectively. We perform a mean op-
eration among all the 2D CNN features. The rep-
resentation V of each video is composed by a con-
catenation of the 3D CNN feature and the 2D CNN
feature. Then, the feature map V is processed by
Ns semantic separating CNN modules. A resid-
ual block is adopted as the CNN module in our
method. The hidden state dimension of the LSTM
units is 1,024.

The Adam optimizer is adopted in training. We
first train the POS tag learning model in the POS-
aware semantic guider. In the later training, the pa-
rameters of the POS tag learning model are fixed.
The other parameters of our model is learned with
MLE. λ is set to be 1. The maximum number of
epochs of the MLE training is 30. The RL method
is applied to optimize the MLE trained model with
the CIDEr metric. At each epoch, the validation
set is used to evaluate the training model, and the
best CIDEr score model is selected for the final
testing. All of our experiments are implemented
with Pytorch (Paszke et al., 2017) and are con-
ducted on a Titan X GPU with 12G memory.

In caption testing, the beam search is adopted
for caption generation. The search size is set to be
5 in experiments.

4.3 Compared Approaches

Our method is compared with HRNE (Pan et al.,
2016), VideoLAB (Ramanishka et al., 2016), SA-
LSTM (Wang et al., 2018a) and attention-based
methods like MA (Hori et al., 2017), SCN (Gan
et al., 2016) and recently proposed PickNet (Chen
et al., 2018) and state-of-the-art RL optimized
methods Weakly (Shen et al., 2017), CIDEnt-RL

https://github.com/tylin/coco-caption


2074

Table 2: Performance compared with state-of-the-art
methods on Youtube2Text dataset. The (−) is an un-
known metric.

Model B-4 M R C

LSTM-I 0.446 0.297 − −
HRNE 0.438 0.331 − −
MA 0.504 0.318 − 0.699
SCN 0.511 0.335 − 0.777
TSA 0.517 0.340 − 0.749
SA-LSTM 0.523 0.341 0.698 0.803
PickNet 0.523 0.333 0.696 0.765

ASGN+LNA 0.547 0.342 0.717 0.813

Table 3: Performance compared with state-of-the-art
methods on MSR-VTT dataset.

Model B-4 M R C

VideoLAB 0.391 0.277 0.606 0.441
SA-LSTM 0.391 0.266 0.593 0.427
Weakly 0.414 0.283 0.611 0.489
CIDEnt-RL 0.405 0.284 0.614 0.517
HRL 0.413 0.287 0.617 0.480
PickNet 0.413 0.277 0.598 0.441

ASGN+LNA 0.420 0.282 0.621 0.505

(Pasunuru and Bansal, 2017), HRL (Wang et al.,
2017). Moreover, we compare our method with in-
terpretable improvement methods LSTM-I (Dong
et al., 2017) and TSA (Wu et al., 2018).

4.4 Ablation Study

We perform the ablation studies on the
Youtube2Text and MSR-VTT datasets for
our video captioning model. The results are
shown in Table 1. ASGN which imports video
feature into different network streams without
the POS tag guidance is adopted as the baseline
model. ASGN+L predicts the POS tag of each
word and applies the POS tag to guide the
semantic separation. “L” denotes the POS-aware
semantic guider. ASGN+LA adds an attention
model proposed by Lu et al. (Lu et al., 2016).
As a comparison, our proposed new adaptive
attention model is introduced to ASGN+LNA.

It can be seen that the ASGN+LNA achieves
the best performances in all metrics, which indi-
cates our proposed sentinel gate is more effective-
ness and reasonable to decide the quantity of at-
tended feature to the decoder LSTM. Compared
with the baseline model, the introduction of POS

tag in ASGN+L, which activates the specific neu-
rons and parses the whole visual representation of
video, can assign appropriate POS-aware seman-
tic information and achieve better performance.
Comparing with the results of MLE-based and
RL-based methods, the RL method can improve
the performance of MLE-based model by signifi-
cant margins across all metrics.

Table 4: Human evaluation between ASGN and
ASGN+L models.

Indifference ASGN+L Wins ASGN Wins

0.418 0.329 0.253

Table 5: The results under different supervisions of
POS tags on Youtube2Text dataset.

Model B-4 M R C

A+A 0.489 0.325 0.695 0.756
V 0.506 0.330 0.699 0.762
N 0.509 0.330 0.701 0.773
N+V 0.518 0.333 0.700 0.776

4.5 Quantitative Analysis

In Table 2 and Table 3, we compare our
ASGN+LNA model with the state-of-the-art mod-
els on the Youtube2Text and MSR-VTT datasets.
Following the operation of (Gan et al., 2016; Pa-
sunuru and Bansal, 2017), ASGN+LNA is the av-
erage ensemble of 5 ASGN+LNA (RL) models
trained with different initializations. From the re-
sults, our method achieves the competitive perfor-
mance on the two datasets. Compared with the
other interpretable improvement methods (Dong
et al., 2017; Wu et al., 2018), interpretability of
our neural network is explicitly improved, and the
performance of our model is more competitive.

0

0.2

0.4

0.6

0.8

1

Youtube2Text MSR-VTT
Acc B-2 B-3 B-4

Figure 3: The performance of the POS tag prediction.



2075

0.9999

0.7663

0.9899

0.5586

0.9992 0.998
0.8664

A man is sitting on a couch

Captions
0.9999

0.8519
1

0.8551
0.9855

0.8836
0.9959

A woman is lying on a bed

0.9998
0.8383

1 0.99 0.9964 0.9686

A woman is slicing a tomato

0.9963

0.4381

0.9995 0.9887

0.4191
0.5592

0.9144

0.2265 0.2608

A man is putting out in a big pole

NOUN

VERB

DET

AUX

ADP

ADJ

Figure 4: Visualizations of the probabilities of each word and corresponding POS tag in caption.

We introduce the human evaluation from (Pa-
sunuru and Bansal, 2017) for comparison between
ASGN and ASGN+L models. The relevance mea-
sures how related is the generated caption w.r.t,
to the video content is adopted as the metric.
In Table 4, the results of 150 samples from the
Youtube2Text test set are studied. It can be found
that the proposal of the semantic guider signifi-
cantly improves the semantic extraction ability of
network.

Moreover, to better verify the reliability of the
supervision of nouns and verbs, we add com-
parisons by adjusting the supervision to single
nouns (N), single verbs (V), adjectives and ad-
verbs (A+A), nouns and verbs (N+V), respec-
tively. The results on the Youtube2Text are pre-
sented in Table 5. It can be found that the model
under the supervision of V+N achieves the best
performance. Compared to verbs, the model under
the supervision of nouns is more reliable. Under
the supervision of A+A, the results of the model
indicate the words of adjectives or adverbs are not
always related to visual signals.

4.6 Visualized Analysis

To examine the reliability of the POS tag pre-
diction, the performance of the POS tag learning
model is measured using accuracy (Acc) and B-N
(N=2,3,4) over the test datasets of Youtube2Text
and MSR-VTT. The results are shown in Fig. 3.
The metric of Acc is to test the total predict per-
formance and the metric of B-N is to test the con-
tinuity of prediction. To better illustrate the ef-
fectiveness of the model, we set the beam search
size to be 1 which is the same with the training
process. These results indicate that the POS tag
learning model can provide reliable POS tag rep-

resentations to guide the semantics’ separation to
corresponding network streams.

Fig. 4 gives some results of generated cap-
tions and the corresponding probability of words
and POS tags. In Fig. 4, we can see that the
POS-aware semantic guider successfully guides
the POS-aware semantic to the generation of cap-
tions. The POS-aware neurons have higher prob-
ability are activated to extract corresponding se-
mantics to predict the relevant words at each time
step. In the first example, the POS tags of the
generated words with the highest probability have
high probability as well. It demonstrates the im-
provement of interpretability of our model.

To further present the interpretability of our
model, the neuron activations associated with the
POS tags, the weights of the sentinel gate, the gen-
erated video captions, and the real POS tags of the
captions are visualized in Fig. 5. Each element
of these samples is illustrated along with the word
prediction in sequence. Through these examples,
the process of the POS-aware semantic informa-
tion extraction and guidance in corresponding net-
work stream can be visualized explicitly. From the
illustration between the activated neuron and the
truth POS tags of the captions. It can be found that
the corresponding POS-aware neurons have high
activations through time. For example, in Fig. 5
(b), the “DET”, “NOUN” and “VERB” are pre-
cisely pointed to the POS tag of the words in “a
man is flying into the water”. It illustrates that the
corresponding neurons of the POS tags are suc-
cessfully activated at each time step. Moreover,
the illustrations between the weights of the sen-
tinel gate and the generated captions reveal that
our adaptive attention model can effectively cap-
ture both the visual words and non-visual words.



2076

Description:

A person is slicing a potato
POS tags:
DET NOUN VERB VERB DET NOUN

DET 

NOUN 

Description:

A man is flying into the water
POS tags:
DET NOUN AUX VERB ADP DET NOUN

NOUN

VERB

DET

Description:

A woman is putting a frying of hot of a pan
POS tags:
DET NOUN AUX VERB DET NOUN ADP ADJ ADP DET NOUN

NOUN

VERB

DET

Description:

The girls are dancing
POS tags:
DET NOUN VERB VERB

Description:

A dog is running on a field
POS tags:
DET NOUN AUX VERB ADP DET NOUN

Description:

Two girls are being in the other
POS tags:
NUM NOUN VERB VERB ADP DET ADJ

(a) (b) (c)

(d) (e) (f)

VERB 

0

1

Weights of sentinel gate

AUX

ADP
0

1

Weights of sentinel gate

AUX

ADP

ADJ0

2

Weights of sentinel gate

DET 

VERB 

0

1

Weights of sentinel gate
0

1

Weights of sentinel gate

DET

AUX

ADP

NOUN

VERB

NOUN

VERB

DET

NUM

ADP

ADJ
0

2

Weights of sentinel gate

NOUN 

Figure 5: Visualization of the sampled video frames, the neuron activations associated with the POS tags, the
weights of the sentinel gate, the generated captions, and the real POS tags of the captions.

From Fig. 5 (a), the words of “person”, “slicing”
and “potato” are obviously visual words which
have related visual signals in video, and our model
successfully extracts their visual information.

5 Conclusion

In this paper, we have proposed an Adaptive Se-
mantic Guidance Network (ASGN), which ex-
tracts as well as guides the POS-aware semantic
information into the corresponding encoded visual
representations under the supervision of POS tag.
Moreover, a new sentinel gate is introduced to
determine how much the attended feature can be
imported into the decoding process. It indicates
that interpretable improvement not only makes the
learning process more transparent, but also gives
model more space to explore. The promising per-
formance and interpretability improved merits of
our method demonstrate the effectiveness of the
POS tag. Furthermore, the proposed ASGN has
a good flexibility which can be employed to the
other language and vision fields, such as image
captioning, visual question answering, and so on.

6 Acknowledgment

We thank the anonymous reviewers for their help-
ful comments. This work was supported by the
National Natural Science Foundation of China un-
der Grants 91646207, 61773377, and 61573352,
the Young Elite Scientists Sponsorship Program

by CAST (2018QNRC001), and the Beijing Natu-
ral Science Foundation under Grant L172053.

References
Deshpande A, Aneja J, and Wang L. 2019. Diverse and

accurate image captioning guided by part-of-speech.
In CVPR, pages 10695–10704.

Rami Al-Rfou, Bryan Perozzi, and Steven Skiena.
2013. Polyglot: Distributed word representations
for multilingual NLP. In CoNLL, pages 183–192.

Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An
automatic metric for mt evaluation with improved
correlation with human judgments. In ACL Work-
shop, volume 29, pages 65–72.

Yangyu Chen, Shuhui Wang, Weigang Zhang, and
Qingming Huang. 2018. Less is more: Picking in-
formative frames for video captioning. In ECCV,
pages 367–384.

Jeff Donahue, Lisa Hendricks, Sergio Guadarrama,
Marcus Rohrbach, Subhashini Venugopalan, Trevor
Darrell, and Kate Saenko. 2015. Long-term recur-
rent convolutional networks for visual recognition
and description. In CVPR, pages 2625–2634.

Yinpeng Dong, Hang Su, Jun Zhu, and Bo Zhang.
2017. Improving interpretability of deep neural net-
works with semantic information. In CVPR, pages
975–983.

Raphaël Fargier and Marina Laganaro. 2015. Neu-
ral dynamics of object noun, action verb and action
noun production in picture naming. Brain and lan-
guage, 150:129–142.



2077

Zhe Gan, Chuang Gan, Xiaodong He, Yunchen Pu,
Kenneth Tran, Jianfeng Gao, Lawrence Carin, and
Li Deng. 2016. Semantic compositional networks
for visual captioning. Computing Research Reposi-
tory (CoRR), abs/1611.08002.

Sergio Guadarrama, Niveda Krishnamoorthy, Girish
Malkarnenkar, Subhashini Venugopalan, Raymond
Mooney, Trevor Darrell, and Kate Saenko. 2013.
Youtube2text: Recognizing and describing arbitrary
activities using semantic hierarchies and zero-shot
recognition. In ICCV, pages 2712–2719.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2016. Deep residual learning for image recog-
nition. In CVPR, pages 770–778.

Xinwei He, Baoguang Shi, Xiang Bai, Gui-Song Xia,
Zhaoxiang Zhang, and Weisheng Dong. 2017. Im-
age caption generation with part of speech guidance.
Pattern Recognition Letters.

Chiori Hori, Takaaki Hori, Teng-Yok Lee, Ziming
Zhang, Bret Harsham, John Hershey, Tim Marks,
and Kazuhiko Sumi. 2017. Attention-based multi-
modal fusion for video description. In ICCV, pages
4203–4212.

Houpand Horoufchin, Danilo Bzdok, Giovanni Buc-
cino, Anna Borghi, and Ferdinand Binkofski. 2018.
Action and object words are differentially anchored
in the sensory motor system - a perspective on cog-
nitive embodiment. 8.

Chin Lin. 2004. Rouge: A package for automatic eval-
uation of summaries. In ACL Workshop, volume 8.

Jiasen Lu, Caiming Xiong, Devi Parikh, and Richard
Socher. 2016. Knowing when to look: Adap-
tive attention via A visual sentinel for image cap-
tioning. Computing Research Repository (CoRR),
abs/1612.01887.

Stephen Merity, Caiming Xiong, James Bradbury, and
Richard Socher. 2016. Pointer sentinel mixture
models. Computing Research Repository (CoRR),
abs/1609.07843.

Pingbo Pan, Zhongwen Xu, Yi Yang, Fei Wu, and Yuet-
ing Zhuang. 2016. Hierarchical recurrent neural en-
coder for video representation with application to
captioning. In CVPR, pages 1029–1038.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In ACL, pages 311–
318.

Ramakanth Pasunuru and Mohit Bansal. 2017. Rein-
forced video captioning with entailment rewards. In
EMNLP, pages 979–985.

Adam Paszke, Sam Gross, Soumith Chintala, Gre-
gory Chanan, Edward Yang, Zachary DeVito, Zem-
ing Lin, Alban Desmaison, Luca Antiga, and Adam
Lerer. 2017. Automatic differentiation in pytorch.
In NeruIPS Workshop.

Vasili Ramanishka, Abir Das, Dong Park, Subhashini
Venugopalan, Lisa Hendricks, Marcus Rohrbach,
and Kate Saenko. 2016. Multimodal video descrip-
tion. In the ACM International Conference on Mul-
timedia, pages 1092–1096.

Steven Rennie, Etienne Marcheret, Youssef Mroueh,
Jarret Ross, and Vaibhava Goel. 2017. Self-critical
sequence training for image captioning. In CVPR,
pages 1179–1195.

Zhiqiang Shen, Jianguo Li, Zhou Su, Minjun Li,
Yurong Chen, Yu-Gang Jiang, and Xiangyang Xue.
2017. Weakly supervised dense video captioning.
In CVPR, pages 5159–5167.

Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo
Torresani, and Manohar Paluri. 2015. Learning
spatiotemporal features with 3d convolutional net-
works. In ICCV, pages 4489–4497.

Ramakrishna Vedantam, C. Zitnick, and Devi Parikh.
2015. Cider: Consensus-based image description
evaluation. In CVPR, pages 4566–4575.

Subhashini Venugopalan, Marcus Rohrbach, Jeffrey
Donahue, Raymond Mooney, Trevor Darrell, and
Kate Saenko. 2015a. Sequence to sequence - video
to text. In ICCV, pages 4534–4542.

Subhashini Venugopalan, Huijuan Xu, Jeff Donahue,
Marcus Rohrbach, Raymond Mooney, and Kate
Saenko. 2015b. Translating videos to natural lan-
guage using deep recurrent neural networks. In
NAACL, pages 1494–1504.

Bairui Wang, Lin Ma, Wei Zhang, and Wei Liu.
2018a. Reconstruction network for video cap-
tioning. Computing Research Repository (CoRR),
abs/1803.11438.

Xin Wang, Wenhu Chen, Jiawei Wu, Yuan-Fang Wang,
and William Yang Wang. 2017. Video captioning
via hierarchical reinforcement learning. Computing
Research Repository (CoRR), abs/1711.11135.

Zhe Wang, Xiaoyi Liu, Limin Wang, Yu Qiao, Xiao-
hui Xie, and Charless Fowlkes. 2018b. Structured
triplet learning with pos-tag guided attention for vi-
sual question answering. In WACV, pages 1888–
1896.

Xian Wu, Guanbin Li, Qingxing Cao, Qingge Ji, and
Liang Lin. 2018. Interpretable video captioning via
trajectory structured localization. In CVPR, pages
6829–6837.

Jun Xu, Tao Mei, Ting Yao, and Yong Rui. 2016.
MSR-VTT: A large video description dataset for
bridging video and language. In CVPR, pages 5288–
5296.

Li Yao, Atousa Torabi, Kyunghyun Cho, Nicolas Bal-
las, Christopher Pal, Hugo Larochelle, and Aaron
Courville. 2015. Describing videos by exploiting
temporal structure. In ICCV, pages 4507–4515.


