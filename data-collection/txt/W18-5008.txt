




































Introduction method for argumentative dialogue using paired question-answering interchange about personality


Proceedings of the SIGDIAL 2018 Conference, pages 70–79,
Melbourne, Australia, 12-14 July 2018. c©2018 Association for Computational Linguistics

70

Introduction method for argumentative dialogue using paired
question-answering interchange about personality

Kazuki Sakai1, Ryuichiro Higashinaka2, Yuichiro Yoshikawa1,
Hiroshi Ishiguro1, and Junji Tomita2

1Osaka University / JST ERATO
2NTT Media Intelligence Laboratories, NTT Corporation

1{sakai.kazuki,yoshikawa,ishiguro}@irl.sys.es.osaka-u.ac.jp
2{higashinaka.ryuichiro,tomita.junji}@lab.ntt.co.jp

Abstract

To provide a better discussion experience
in current argumentative dialogue sys-
tems, it is necessary for the user to feel
motivated to participate, even if the sys-
tem already responds appropriately. In
this paper, we propose a method that
can smoothly introduce argumentative di-
alogue by inserting an initial discourse,
consisting of question-answer pairs con-
cerning personality. The system can in-
duce interest of the users prior to agree-
ment or disagreement during the main dis-
course. By disclosing their interests, the
users will feel familiarity and motivation
to further engage in the argumentative di-
alogue and understand the system’s intent.
To verify the effectiveness of a question-
answer dialogue inserted before the argu-
ment, a subjective experiment was con-
ducted using a text chat interface. The
results suggest that inserting the question-
answer dialogue enhances familiarity and
naturalness. Notably, the results suggest
that women more than men regard the di-
alogue as more natural and the argument
as deepened, following an exchange con-
cerning personality.

1 Introduction

Argumentation is a process of reaching consensus
through premises and rebuttals, and it is an im-
portant skill required in daily life (Scheuer et al.,
2010). Through argumentation, we can not only
reach decisions, but also learn what others think.
Such decision-making and the interchange of
views are one of the most important and advanced
parts of human activities. If an artificial dialogue
system can argue on certain topics with us, this
can both help us to work efficiently and establish
a close relationship with the system.

Recently, there have been some studies con-
cerning argumentative dialogue systems. Hi-
gashinaka et al. developed an argumentative
dialogue system that can discuss certain top-
ics by using large-scale argumentation struc-
tures (Higashinaka et al., 2017). However, this
system could not provide all users with a satisfac-
tory discussion experience, even though it could
appropriately respond to their opinions. One pos-
sible reason for this is that some users are not nec-
essarily motivated to argue on the topics suggested
by the system.

We aim to improve an argumentative dia-
logue system by adding a function to motivate
a user to participate in an argumentative dia-
logue. To increase the user’s motivation to par-
ticipate in the argumentative dialogue, we focus
on small talk. Small talk can help participants
build certain relationships before they enter the
main dialogue (Zhao et al., 2014). In negotia-
tion and counseling, a close relationship between
two humans can improve the performance of cer-
tain tasks (Drolet and Morris, 2000; Kang et al.,
2012). Relationships between a user and system
are important for reaching a consensus though dia-
logue (Katagiri et al., 2013) Thus, it is considered
to be possible for a user to be naturally guided into
an argumentative dialogue by performing small
talk.

In practice, we adopted a question-answering
dialogue, where users are casually asked about
their personal experiences or ideas. This was
implemented by using what we call a personal
database (hereafter PDB), which involves pairs
consisting of a personal question and a corre-
sponding example answer, which are likely to ap-
pear in human-human conversation. When asked
about personal issues, users are expected to feel
interested in the system, and then be induced to
feel open and close to the system. Meanwhile, the
system provides its own answers to the questions



71

by using the PDB. From the answers of the user
and the system, users are expected to gain an idea
of what is common and different between them, a
requirement which has been suggested to be im-
portant for humans to be motivated to understand
one another (Uchida et al., 2016).

In this research, we extend the argumentative
dialogue system described in (Higashinaka et al.,
2017) to add a function that can smoothly in-
troduce argumentative dialogue by inserting a
question-answering dialogue using the PDB (here-
inafter referred to as PDB-QA dialogue). It is con-
sidered that users of the proposed system can be
expected to be motivated to partake in the argu-
mentative dialogue, and that they can then partake
in a deep discussion with the system. To verify
the effectiveness of this system, we conducted a
subjective experiment using a text chat interface.

The remainder of this paper is organized as fol-
lows. In Section 2, we describe related work. In
Section 3, we describe our proposed method, in-
cluding how to develop the question-answering di-
alogue and how to integrate this into an existing
argumentative dialogue system. In Section 4, we
describe an experiment we conducted, in which
human subjects expressed their impressions of the
dialogue through a text chat interface. We summa-
rize the paper and discuss future work in Section 5.

2 Related work

Although there is little work on an auto-
mated system that can perform discussion with
users, recently, there has been a great deal of
work aimed at automatically extracting premises
and conclusions from text; argumentation min-
ing has been applied to various data, includ-
ing legal text (Moens et al., 2007), newswire
text (Bal and Saint-Dizier, 2010), opinions in dis-
cussion forums (Rosenthal and McKeown, 2012),
and varied online text (Yanai et al., 2016).

There has been some research concerning the
introduction of a dialogue. Rogers et al. showed
that it became easier for two people to talk
during the first meeting by using an applica-
tion that can share their opinions on a dis-
play (Rogers and Brignull, 2002). Patricia et
al. reported that small talk in an initial dis-
course improved the interaction in a business sit-
uation (Pullin, 2010). Inaguma et al. analyzed
the prosodic features of shared laughter as an ice-
breaker in initial dialogues (Inaguma et al., 2016).

However, it is unclear how to develop an initial di-
alogue for smoothly introducing a discussion.

It is known that people interact with artifi-
cial constructions such as dialogue systems, vir-
tual agents, and robots in the same manner as
they interact with other humans (Reeves and Nass,
1996). Schegloff et al. showed that human conver-
sation usually interleaves the contents of a task-
oriented dialogue with social contents (Schegloff,
1968). Jiang et al. showed that 30% of all ut-
terances of Microsoft Cortana, a well-known task-
oriented dialogue system, consist of social con-
tents (Jiang et al., 2015). It is considered that per-
forming small talk can be natural in argumentative
dialogue systems.

There have been many studies on dialogue
systems that include small talk. Bechberger et
al. developed a dialogue system that conveys
news text and performs small talk related to the
news (Bechberger et al., 2016). Kobori et al.
showed that inserting small talk improved the im-
pressions of an interview system (Kobori et al.,
2016). Bickmore et al. showed that the task suc-
cess rate was improved by constructing a trust re-
lationship using small talk (Bickmore and Cassell,
2005). Tina et al. developed a dialogue sys-
tem that included the function of interacting using
small talk (Klüwer, 2015). We consider that ar-
gumentative dialogues may be performed deeply
since small talk can improve the trust relationship.

Related to the studies dealing with multiple dia-
logue strategies including argumentative and so-
cial dialogues, there are several works concern-
ing hybrid dialogue systems that integrate task-
oriented and chat-oriented dialogue systems. Pa-
paioannou et al. proposed a method to acquire dia-
logue strategies for hybrid systems in a robot using
reinforcement learning (Papaioannou and Lemon,
2017). Yu et al. showed that multiple di-
alogue systems can interact using appropriate
dialogue strategies learned through reinforce-
ment learning (Yu et al., 2017). Akasaka et al.
demonstrated a classification method for input
utterances to select what dialogue systems are
used (Akasaki and Kaji, 2017). However, in ini-
tial dialogue, it is unclear which dialogue strate-
gies can be employed to smoothly introduce an ar-
gumentative dialogue.



72

Figure 1: Flow of PDB-QA dialogue. Each part
contains two system utterances and two user utter-
ances. We used questions in an order based on the
similarity between the dialogue topic and question
text.

3 Proposed method

We propose a method for introducing an argu-
mentative dialogue using the PDB-QA dialogue,
which is a question-answering dialogue concern-
ing personality. We then describe some existing
argumentative dialogue systems. Next, we explain
how to develop an extended argumentative dia-
logue system using the PDB-QA dialogue.

3.1 PDB-QA dialogue by using
question-answering pair about
personality

The PDB consists of personal questions and exam-
ple answers and is used to ask the interlocutor for
detailed information (Tidwell and Walther, 2002).
Such questions may be asked even when the in-
terlocutor is a dialogue system (Nisimura et al.,
2011). In this study, we used the PDB described
in (Sugiyama et al., 2014). This PDB is a large-
scale database of pairs of questions and answers
related to personal information. Questions in-
cluded in the PDB involved various personal ques-
tions, question categories, answer examples, and
topics attached to each question. Based on the
degree of overlap of questions, question-answer
pairs frequently encountered during conversation
are extracted. The PDB includes personal ques-
tions such as “what dishes do you like?” and
“which places have you visited?”

We explain the procedure for generating a PDB-
QA dialogue using this PDB. As shown in Fig-
ure 1, the PDB-QA dialogue consists of several
parts. Each part consists of four utterances: the
system’s question using the PDB, the user’s re-
sponse, the system’s answer, and the user’s re-
sponse to this. To determine the order in which

Figure 2: Architecture of developed dialogue sys-
tem.

to ask multiple questions, we used the similar-
ity between the topic of argument and the ques-
tion text, calculated by Word2vec (Mikolov et al.,
2013). From parts 1 to N, we used questions in
an order starting from the highest similarity, i.e.,
part 1 uses a question with the N-th highest simi-
larity and part N uses another question that has the
highest similarity. This is because it is considered
that approaching the topic gradually is natural as a
dialogue structure. Through this process, we can
perform N parts of the PDB-QA dialogue.

3.2 Argumentative dialogue system

We used the argumentative dialogue system de-
scribed in (Higashinaka et al., 2017). This sys-
tem can generate appropriate argumentative dia-
logue text based on large-scale knowledge struc-
tures, called argumentation structures, which are
constructed manually. An argumentation structure
is represented by a graph structure, composed of
nodes that represent premises and edges represent-
ing support or nonsupport relationships, based on
an extended version of Walton’s model (Walton,
2013).

A user utterance is input into two modules: di-
alogue act estimation and proposition identifica-
tion. The dialogue act estimation module esti-
mates four dialogue-act types: assertion, question,
concession, and retraction. The proposition iden-
tification module determines the argumentation
node that contains the content closest to the input
user utterance. The discussion manager updates
the argumentation structure on the basis of the un-
derstanding result, which checks whether the cor-
responding node is already mentioned. Then the
dialogue manager retrieves premises that can be
used for support or rebuttal based on traversing
along with argumentation structures. The system
outputs a supportive or nonsupportive response to



73

Figure 3: Flow in the dialogue manager.

the user’s utterance.

3.3 Integration of argumentation dialogue
system and PDB-QA dialogue

Figure 2 illustrates the architecture of our argu-
mentative dialogue system. The user interacts
with the system through the text chat interface on
the browser. The natural language understand-
ing module has two modules related by the argu-
mentative dialogue system. Note that this mod-
ule is only used in the argument phase described
as follows. The dialogue manager manages two
dialogue states. One is the question-answering
phase, and the other is the argument phase. Fig-
ure 3 illustrates the flow of dialogue managed by
the dialogue manager. First, the dialogue manager
initiates the opening dialogue, such as by asking
the user her name. Then, it begins the question-
answering phase. In this phase, the PDB-QA di-
alogue is performed, as described in Section 3.1.
The PDB-QA dialogue is a predefined question-
answering dialogue, regardless of user utterances.
The answer of the system for a PDB question is
prepared by the experimenter in advance. The
natural language generation module changes the
system response such as adding conjunctions and
changing the end of the sentences using a dialogue
act. Later, the dialogue manager begins the argu-
ment phase. In the argument phase, the utterances
of the system are premises that can be used for
support or rebuttal, and they consist of the results
of the argumentative dialogue system.

Line Speaker Utterance
1 S Do you like trips?
2 U Yes, I do.
3 S I see. I like them too.

I want to go on a trip during the weekend.
4 U I also want to go during the weekend.
5 S Where do you want to go for a trip?
6 U I want to go abroad.
7 S Me too.

I want to go to France if I have the time.
8 U French dishes seem great.
9 S Which is the better place to travel

in Japan, Hokkaido or Okinawa?
10 U I want to go to Okinawa.
11 S You can enjoy unique dishes in Okinawa.
12 U I think goya-champuru (a traditional

Okinawa dish) is delicious.
13 S But Hokkaido also has delicious dishes,

such as Genghis Khan.
14 U I see. Hokkaido has more delicious food.

Figure 4: An example of the dialogue. The topic is
that of which place the better to travel to in Japan:
Hokkaido or Okinawa. Lines 1 ∼ 8 are part of the
PDB question dialogue, and lines 9 ∼ 14 are part
of the argumentative dialogue. Speaker S and U
represent the system and user, respectively.

Figure 4 shows an example of the dialogue we
performed. The topic is as follows: which is the
better place to travel to in Japan: Hokkaido or Ok-
inawa? Lines 1 ∼ 8 are a part of the PDB question
dialogue, and lines 9 ∼ 14 are a part of the argu-
mentative dialogue. Speakers S and U represent
the system and the user, respectively.

4 Experiment

In this section, we describe a subjective experi-
ment to verify the effect of inserting the PDB-QA
dialogue. We compared the subjects’ evaluations
and behavior for two types of dialogue: one with
PDB-QA and the other without it. The hypoth-
esis is that by inserting the PDB-QA dialogue in
advance, users are motivated to partake in the ar-
gumentative dialogue and can then discuss deeply
with the system. To verify this hypothesis, sub-
jects communicated with the argumentative dia-
logue system through a text chat interface on a
browser, and then recorded their impressions in a
questionnaire. We quantitatively evaluated the av-
erage number of words per utterance of the user in
the argument phase. It is expected that the num-
ber of words per user’s utterance in our argumen-
tative dialogue system should be relatively lower
than that in the previous system, because when a



74

Figure 5: Screenshot of the text chat interface.

user builds a relationship with the system, the user
expresses own ideas with fewer words.

4.1 Method
4.1.1 Subjects
Thirty-two Japanese adults (16 males and 16 fe-
males, with an average age of 20.3 years) partici-
pated as subjects. Half of the subjects participated
with the PDB condition, and the other half without
it. The ratio of males to females in each condition
was the same. One male with the PDB condition
and two males without were excluded because of
system failures, and the utterances of the remain-
ing 29 people were analyzed.

4.1.2 Apparatus
The experiment was conducted in a space sepa-
rated by curtains. A laptop PC was placed on the
table, and the PC displayed a web browser to show
the text chat interface, as shown in Figure 5. Note
that the dialogue in the experiment was performed
in Japanese. The dialogue text of the interaction
between the system and the subject was displayed
in the middle part of the browser, and a text box for
the subject to input his/her own utterances was dis-
played at the lower part of the browser. Note that
we call the sentence displayed in the interface an
“utterance.” In other words, sentences produced
by the system and input by the user with a key-
board are called the system’s and user’s utterances,
respectively.

4.1.3 Stimuli
In this experiment, we compared two conditions:
with and without the PDB. The condition with
PDB included two phases of dialogue: a question-
answering phase and an argument phase. The con-
dition without PDB included one phase of dia-
logue: the argument phase. In this experiment, the

subject and the system alternately provided utter-
ances. Each pair of such utterances is referred to
as one turn. Both conditions included two turns
of opening dialogue, such as asking the subject’s
name and a greeting. The question-answering
phase consisted of three parts, each of which in-
cluded two turns of dialogue. In total, six turns
of dialogue were performed. The argument phase
contained six turns of dialogue. We prepared five
discussion topics and assigned any of these to the
subject at random: (1) the pros and cons of driving
automobiles, (2) benefits of living in the country-
side vs. living in the city, (3) which is the better
place to travel to in Japan between Hokkaido and
Okinawa, (4) which is the better breakfast between
bread and rice, and (5) which is the better theme
park betweenTokyo Disney Resort and Universal
Studios Japan.

4.1.4 Procedure
This experiment was conducted according to the
following procedure. First, the experimenter gave
a subject the instructions for the experiment. The
contents of the instructions were that the subject
interacts with the system through the text chat in-
terface on the browser, interacts only once, and an-
swers the questionnaire after the dialogue. Next,
the experimenter asked the subject to read the
questionnaire in advance. After that, interaction
was started. After completing the dialogue, the
experimenter asked the subject to answer the ques-
tionnaire.

4.1.5 Measurement
The items of the questionnaire regarding impres-
sions were the same for both conditions, and there
were eleven items in total. These included ques-
tions related to the overall impression of the dia-
logue system, the argumentative dialogue, and the
user’s motivation for conversing with the dialogue
system. The items concerning the impression of
the dialogue consisted of the following five:

Q1 The utterances of the system are correct in
Japanese,

Q2 The dialogue with the system is easy to under-
stand,

Q3 The dialogue with the system is familiar,
Q4 The dialogue with the system has a lot of con-

tent, and

Q5 The dialogue with the system is natural.



75

Figure 6: Box plots of the results of the questionnaire.

Figure 7: Box plots of results for the average num-
bers of words and content words (nouns, verbs, ad-
jectives, conjunctions, and interjections) per utter-
ance in the argument phase. We used MeCab to
tokenize Japanese words.

The items concerning the impression of the argu-
ment dialogue were the following two:

Q6 You can deeply discuss the topic of X, and
Q7 You can smoothly enter the argumentative di-

alogue about X,

where X is the actual topic (e.g., which is the better
place to travel to in Japan between Hokkaido and
Okinawa). The items related to motivation for the
dialogue were the following four:

Q8 You want to convey your opinions,
Q9 You want to understand the system’s opinions,
Q10 You feel that the system wants to convey its

opinions, and
Q11 You feel that the system wants to understand

your opinions.

A Likert scale was used to elicit the sub-
jects’ impressions. We used a seven-point scale

that ranged from a value of 1, corresponding
to “strongly disagree,” to 7, corresponding to
“strongly agree.” The midpoint value of 4 corre-
sponded to “undecided.”

We also counted the average number of words
per user utterance and the average number of con-
tent words (nouns, verbs, adjectives, conjunctions,
and interjections) in the argument phase. We
used MeCab to tokenize the words and label the
Japanese parts of speech.

4.2 Result

Figure 6 presents the box plots of the answers to
the questionnaire. A Mann-Whitney U test was
used to compare the scores on the Likert scale. For
Q3, namely “the dialogue with the system is famil-
iar,” the median score for the condition with PDB
was found to be marginally significantly higher
than that for the condition without PDB (W = 143,
p < 0.1). For Q5, namely “the dialogue with
the system is natural,” the median score for the
condition with PDB was found to be significantly
higher than that for the condition without PDB
(W = 149.5, p < 0.05). For other questions, no
significant differences between the two conditions
were detected.

As shown in Figure 6, we did not directly con-
firm an improvement concerning the smooth intro-
duction to the argumentative dialogue by inserting
the PDB-QA dialogue. However, this figure sug-
gests that it is possible for the user to feel that
the dialogue is familiar and more natural when
the PDB-QA dialogue is inserted. This result may
be because the system performs in the manner in
which a human usually does, and a certain rela-



76

Figure 8: Box plots of results of the questionnaire
for male users.

Figure 9: Box plots of female results of the ques-
tionnaire for female users.

Figure 10: Box plots of results for male users for
the average number of words and content words per
utterance in the argumentation phase.

Figure 11: Box plots of results for female users for
the average number of words and content words per
utterance in the argumentation phase.

tionship is built between the user and the system.
Thus, it is considered that inserting the PDB-QA
dialogue improves the naturalness of the dialogue
and relationships.

Figure 7 presents the box plots of the average
numbers of words and content words per user ut-
terance. For the average number of words, the me-
dian score for the condition with PDB was found
to be significantly less than that for the condition
without PDB (W = 40, p < 0.01). Concerning
the average number of content words, the median
score for the condition with PDB was also found
to be significantly less than that for the condition
without PDB (W = 40, p < 0.01).

As shown in Figure 7, it was found that the av-
erage numbers of words and content words in the
condition with PDB were significantly less than
those in the condition without PDB. These re-
sults suggest that when the relationship between
the user and the system is not close, the users
may express their opinions using a larger number
of words, to correctly convey their own message;
on the other hand, when the relationship is close,
the users may express their opinions using fewer
words.

In general, it is known that there are some dif-
ferences in purposes of conversation owing to gen-
der differences (Tannen, 2001). In this study, we

suppose that the different purposes of conversation
resulting from gender differences may affect our
results. Therefore, we analyzed the effects of gen-
der. We divided the data by gender, and then plot-
ted each result. In the result for male users, shown
in Figure 8, no significant differences between the
two conditions were detected. On the other hand,
in the result for female users, shown in Figure 9,
we observe some significant differences between
the two conditions. According to this figure, for
Q5, namely “the dialogue with the system is natu-
ral,” the median score for the condition with PDB
was found to be marginally significantly higher
than that for the condition without PDB (W = 49,
p < 0.1). For Q6, namely “you can deeply dis-
cuss the topic,” the median score for the condition
with PDB was also found to be marginally signif-
icantly higher than that for the condition without
PDB (W = 49, p < 0.1). In addition, we compared
males’ and females’ data under the conditions with
and without PDB. As a result, for Q7, namely “you
can smoothly enter the argumentative dialogue,”
the median score with PDB for females was found
to be marginally significantly higher than that with
PDB for males (W = 13.5, p < 0.1). These re-
sults suggest that it is possible that females may
feel that the PDB-QA dialogue inserted before the
argumentative dialogue is more natural, and this



77

may lead to the result that females feel the argu-
mentative dialogue is deepened more. Thus, it is
suggested that inserting the PDB-QA dialogue in
our proposed method may be more effective for
females.

In addition, Figures 10 and 11 show the results
for male and female users for words and content
words, respectively. As shown in Figure 10, for
male users, the average number of content words
for the condition with PDB was found to be sig-
nificantly less than that without PDB (W = 7,
p < 0.05). This result may be because of their
degree of motivation, but the actual reason is un-
known. On the other hand, as shown in Figure 11,
for female users, the average numbers of words
and content words with PDB were found to be
marginally significantly less than those without
PDB (W = 14, p < 0.1, W = 15, p < 0.1, respec-
tively). These results suggest that females may use
fewer words when they feel familiarity with the in-
terlocutor.

5 Summary and future work

We proposed a PDB-QA dialogue method to
smoothly introduce an argumentative dialogue.
We conducted an evaluation experiment to ver-
ify the effectiveness of inserting the PDB-QA di-
alogue. The results suggest that the impressions
of the dialogue, such as familiarity and natural-
ness, may be improved by inserting the PDB-QA
dialogue. Specifically, we found that females may
perceive a PDB-QA dialogue inserted before an
argumentative dialogue as more natural, and this
may lead to the result that the argumentative dia-
logue can be deepened. We also found that when
the relationship between the user and the system
is not close, the users may express their opinions
using a larger number of words, whereas when the
relationship is close, the users may express their
opinions with fewer words.

We can improve the performance of the dia-
logue system by adjusting several parameters of
PDB dialogue, which were fixed in the experiment
for the sake of control. For example, we can adjust
how questions are chosen (the degree of similarity
of questions to be selected), the order of questions,
the number of questions, and the amount of infor-
mation to be presented in an answer to a question.
It may be possible to improve the performance if
we select better parameters depending on a user’s
preferences or the context of a conversation.

For further improvement, we can consider ani-
macy, which is another element that may be im-
portant. Animacy describes the characteristic of
being like a living being, in other words, the char-
acteristic of whether a human can relate to mind
and will in an object. We suppose that in a dia-
logue, it is important for the user to feel animacy
toward the interlocutor, because it is important for
the user to recognize the dialogue system as a spe-
cial target with which they can form a certain re-
lationship. As a preliminary experiment, we mea-
sured the psychological indicators for mind per-
ception (Gray et al., 2011). This scale can mea-
sure how much agency (capacity for self-control,
planning, and memory) and experience (capacity
for pleasure, fear, and hunger) the subject feels the
target has. Analyzing how impressions of agency
and experience might affect the answers to the
questionnaire or the behavior of users will be an
important aspect of future work.

In this paper, we compared the conditions with
and without PDB. Comparing the two conditions,
we surmise that at least three factors exist that af-
fect the results: whether utterances are in the form
of a question, whether they contain personal con-
tent, and whether they are related to the topic of
the argumentative dialogue. For the first factor,
we suppose that a question form can explicitly re-
veal common and differing sentiments in the an-
swer to the question. It is considered that this
makes it easy for the user to become interested.
For the second aspect, we suppose that asking a
question concerning personality can make it possi-
ble to construct a certain relationship more easily.
As regards the final point, we feel this prevents
a sudden change of topic. We suppose that this
makes it possible for the user to enter the argu-
mentative dialogue more smoothly. Investigating
the kinds of factors that affect a natural introduc-
tion into the argumentative dialogue will be a topic
of future work.

Acknowledgments

This work was supported by JST ERATO Grant
Number JPMJER1401, Japan.

References

S. Akasaki and N. Kaji. 2017. Chat detection in an
intelligent assistant: Combining task-oriented and
non-task-oriented spoken dialogue systems. In ACL.



78

B. K. Bal and P. Saint-Dizier. 2010. Towards building
annotated resources for analyzing opinions and ar-
gumentation in news editorials. In Proceedings of
the language resources and evaluation conference.

L. Bechberger, M. Schmidt, A. Waibel, and M. Fed-
erico. 2016. Personalized news event retrieval for
small talk in social dialog systems. In Proceedings
of Speech Communication; 12. ITG Symposium.

T. Bickmore and J. Cassell. 2005. Social Dialongue
with Embodied Conversational Agents. Advances in
Natural Multimodal Dialogue Systems.

A. L. Drolet and M. W. Morris. 2000. Rapport in
conflict resolution: Accounting for how face-to-face
contact fosters mutual cooperation in mixed-motive
conflicts. Journal of Experimental Social Psychol-
ogy, 36(1):26–50.

K Gray, AC Jenkins, AS Heberlein, and DM Weg-
ner. 2011. Distortions of mind perception in psy-
chopathology. In Proceedings of the National
Academy of Sciences of the United States of Amer-
ica, pages 477–479.

R. Higashinaka, K. Sakai, H. Sugiyama, H. Narimatsu,
T. Arimoto, T. Fukutomi, K. Matsui, Y. Ijima, H. Ito,
S. Araki, Y. Yoshikawa, H. Ishiguro, and Y. Matsuo.
2017. Argumentative dialogue system based on ar-
gumentation structures. In Proceedings of the 21st
Workshop on the Semantics and Pragmatics of Dia-
logue, pages 154–155.

H. Inaguma, K. Inoue, S. Nakamura, K. Takanashi, and
T. Kawahara. 2016. Prediction of ice-breaking be-
tween participants using prosodic features in the first
meeting dialogue. In Proceedings of the 2Nd Work-
shop on Advancements in Social Signal Processing
for Multimodal Interaction, pages 11–15.

J. Jiang, A. H. Awadallah, R. Jones, U. Ozertem, I. Zi-
touni, R. G. Kulkarni, and O. Z. Khan. 2015. Auto-
matic online evaluation of intelligent assistants. In
Proceedings of the 24th International Conference on
World Wide Web, pages 506–516.

S. Kang, J. Gratch, C. Sidner, R. Artstein, L. Huang,
and L. Morency. 2012. Towards building a virtual
counselor: Modeling nonverbal behavior during in-
timate self-disclosure. In Proceedings of 11th In-
ternational Conference on Autonomous Agents and
Multiagent Systems.

Y. Katagiri, K. Takanashi, M. Ishizaki, Y. Den, and
M. Enomoto. 2013. Concern alignment and trust in
consensus-building dialogues. In Proceedings of the
9th International Conference on Cognitive Science,
pages 422–428.

T. Klüwer. 2015. Social talk capabilities for dialogue
systems. universaar.

T. Kobori, M. Nakano, and T. Nakamura. 2016. Small
talk improves user impressions of interview dialogue
systems. In Proceedings of the 17th Annual Meeting

of the Special Interest Group on Discourse and Dia-
logue, pages 370–380.

T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and
J. Dean. 2013. Distributed representations of words
and phrases and their compositionality. In Ad-
vances in Neural Information Processing System,
pages 3111–3119.

M. Moens, E. Boiy, R. M. Palau, and C. Reed. 2007.
Automatic detection of arguments in legal texts. In
Proceedings of the 11th international conference on
Artificial intelligence and law, pages 225–230.

R. Nisimura, Y. Nishihara, R. Tsurumi, A. Lee,
H. Saruwatari, and K. Shikano. 2011. Takemaru-
kun: Speech-oriented information system for real
world research platform. In Proceedings of the 2011
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 583–593.

I. Papaioannou and O. Lemon. 2017. Combining
chat and task-based multimodal dialogue for more
engaging hri: A scalable method using reinforce-
ment learning. In Proceedings of the Companion
of the 2017 ACM/IEEE International Conference on
Human-Robot Interaction, pages 365–366.

P. Pullin. 2010. Small talk, rapport, and interna-
tional communicative competence: Lessons to learn
from belf. The Journal of Business Communication,
47(4):455–476.

B. Reeves and C. Nass. 1996. How people treat com-
puters, television, and new media like real people
and places. CSLI Publications and Cambridge uni-
versity press.

Y. Rogers and H. Brignull. 2002. Subtle ice-breaking:
encouraging socializing and interaction around a
large public display. In In Workshop on Public,
Community. and Situated Displays.

S. Rosenthal and K. McKeown. 2012. Detecting opin-
ionated claims in online discussions. In Proceedings
of the 6th IEEE International Conference on Seman-
tic Computing, pages 30–37.

E. A Schegloff. 1968. Sequencing in conversational
openings. American anthropologist, 70(6):1075–
1095.

O. Scheuer, F. Loll, N. Pinkwart, and B. M. McLaren.
2010. Computer–supported argumentation: A re-
view of the state of the art. International Jour-
nal of Computer-Supported Collaborative Learning,
5(1):43–102.

H. Sugiyama, T. Meguro, and R. Higashinaka. 2014.
Large-scale collection and analsis of personal
question-answer pairs for conversational agents. In
Proceedings of Intelligent Virtual Agents, pages
420–433.

D. Tannen. 2001. You junst don’t understand: women
and men in conversation. New York: HarperCollins.



79

L. C Tidwell and J. B. Walther. 2002. Computer-
mediated communication effects on disclosure, im-
pressions, and interpersonal evaluations. Human
Communication Research, 28(3):317–348.

T. Uchida, T. Minato, and H. Ishiguro. 2016. Does a
conversational robot need to have its own values? a
study of dialogue strategy to enhance people’s moti-
vation to use autonomous conversational robots. In
Proceedings of the 4th annual International Confer-
ence on Human-Agent Interaction, pages 187–192.

D. Walton. 2013. Methods of argumentation. Cam-
bridge University Press.

K. Yanai, Y. Kobayashi, M. Sato, T. Yanase, Miyoshi
T, Y. Niwa, and H. Ikeda. 2016. Debating artificial
intelligence. Hitachi Review, 65(6):151.

Z. Yu, A. W. Black, and A. I. Rudnicky. 2017. Learning
conversational systems that interleave task and non-
task content. In Proceedings of International Joint
Conference on Artificial Intelligence.

R. Zhao, A. Papangelis, and J. Cassell. 2014. Towards
a dyadic computational model of rapport manage-
ment for human-virtual agent interaction. In Pro-
ceedings of International Conference on Intelligent
Virtual Agents, pages 514–527.


