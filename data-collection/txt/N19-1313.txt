



















































Selective Attention for Context-aware Neural Machine Translation


Proceedings of NAACL-HLT 2019, pages 3092–3102
Minneapolis, Minnesota, June 2 - June 7, 2019. c©2019 Association for Computational Linguistics

3092

Selective Attention for Context-aware Neural Machine Translation

Sameen Maruf†∗ André F. T. Martins‡
†Faculty of Information Technology, Monash University, VIC, Australia

‡Unbabel & Instituto de Telecomunicações, Lisbon, Portugal
†{firstname.lastname}@monash.edu

‡andre.martins@unbabel.com

Gholamreza Haffari†

Abstract

Despite the progress made in sentence-level
NMT, current systems still fall short at achiev-
ing fluent, good quality translation for a full
document. Recent works in context-aware
NMT consider only a few previous sentences
as context and may not scale to entire docu-
ments. To this end, we propose a novel and
scalable top-down approach to hierarchical at-
tention for context-aware NMT which uses
sparse attention to selectively focus on relevant
sentences in the document context and then
attends to key words in those sentences. We
also propose single-level attention approaches
based on sentence or word-level information
in the context. The document-level context
representation, produced from these attention
modules, is integrated into the encoder or
decoder of the Transformer model depend-
ing on whether we use monolingual or bilin-
gual context. Our experiments and evaluation
on English-German datasets in different doc-
ument MT settings show that our selective at-
tention approach not only significantly outper-
forms context-agnostic baselines but also sur-
passes context-aware baselines in most cases.

1 Introduction

Neural machine translation has grown immensely
in the past few years, from the simplistic RNN-
based encoder-decoder models (Sutskever et al.,
2014; Bahdanau et al., 2015) to the state-of-the-art
Transformer architecture (Vaswani et al., 2017).
Most of these models rely on the attention mech-
anism as a major component, which involves fo-
cusing on different parts of a sequence to com-
pute new representations, and has proven to be
quite effective in improving the translation quality
(Vaswani et al., 2017). However, all of these mod-
els share the same inherent problem: the transla-
tion is still performed on a sentence-by-sentence

∗∗Work initiated during an internship at Unbabel.

basis, thus ignoring the long-range dependencies
which may be useful when it comes to translating
discourse phenomena.

More recently, context-aware NMT has been
gaining significant traction from the MT commu-
nity with majority of works coming out in the
past two years. Most of these focus on using
a few previous sentences as context (Jean et al.,
2017; Wang et al., 2017; Tu et al., 2018; Voita
et al., 2018; Zhang et al., 2018; Miculicich et al.,
2018) and neglect the rest of the document. Only
one existing work has endeavoured to consider the
full document context (Maruf and Haffari, 2018),
thus proposing a more generalised approach to
document-level NMT. However, the model is re-
strictive as the document-level attention computed
is sentence-based and static (computed only once
for the sentence being translated). A more recent
work (Miculicich et al., 2018) proposes to use a
hierarchical attention network (HAN) (Yang et al.,
2016) to model the contextual information in a
structured manner using word-level and sentence-
level abstractions; yet, it uses a limited number of
past source and target sentences as context and is
not scalable to entire document.

In this work, we propose a selective attention
approach to first selectively focus on relevant sen-
tences in the global document-context and then at-
tend to key words in those sentences, while ignor-
ing the rest.1 Towards this goal, we use sparse
attention, enabling an efficient and scalable use
of the context. The intuition behind this is the way
humans translate a sentence containing ambiguous
words. They may look for sentences in the whole
document which contain similar words and just fo-
cus on those for the translation. This attention,

1The term “selective attention” comes from cognitive sci-
ence and is defined as the act of focusing on a particular ob-
ject for a period of time while simultaneously ignoring irrel-
evant information that is also occurring (Dayan et al., 2000).



3093

which we call Hierarchical Attention, is computed
dynamically for each query word. Furthermore,
we propose a Flat Attention approach which is
based on either sentence or word-level information
in the context. We integrate the document-level
context representation, produced from these atten-
tion modules, into the encoder or decoder of the
Transformer model depending on whether we con-
sider monolingual (source-side) or bilingual (both
source and target-side) context.

Our contributions are as follows: (i) we propose
a novel and efficient top-down approach to hier-
archical attention for context-aware NMT, (ii) we
compare variants of selective attention with both
context-agnostic and context-aware baselines, and
(iii) we run experiments in both online (only past
context) and offline (both past and future context)
settings on three English-German datasets. Exper-
iments show that our approach improves upon the
Transformer by an overall +1.34, +2.06 and +1.18
BLEU for TED Talks, News-Commentary and Eu-
roparl, respectively. It also outperforms two recent
context-aware baselines (Zhang et al., 2018; Mi-
culicich et al., 2018) in majority of the cases.

2 Background

2.1 Neural Machine Translation

Generic NMT models are based on an encoder-
decoder architecture (Bahdanau et al., 2015;
Vaswani et al., 2017). The encoder reads the
source sentence denoted by x = (x1, x2, ..., xM )
and maps it to a continuous representation z = (z1,
z2, ..., zM ). Given z, an attentional decoder gen-
erates the target translation y = (y1, y2, ..., yN )
one word at a time in a left-to-right fashion. The
popular Transformer architecture (Vaswani et al.,
2017) follows the same structure by using stacked
self-attention and point-wise, fully connected lay-
ers for both the encoder and decoder.

Encoder The encoder stack is composed of L
identical layers, each containing two sub-layers.
The first, a multi-head self-attention sub-layer, al-
lows each position in the encoder to attend to
all positions in the previous layer of the encoder,
while the second, a feed-forward network, uses
two linear transformations with a ReLU activation.

Decoder The decoder stack is also composed of
L identical layers. In addition to the two sub-
layers, the decoder inserts a third sub-layer, which
performs multi-head attention over the output of

the encoder stack. Masking is used in the self-
attention sub-layer to prevent positions from at-
tending to subsequent positions thus avoiding left-
ward flow of information.

2.2 Document-level Machine Translation
In general, the probability of a document transla-
tion Y given the source document X is given by:

Pθ(Y |X) =
J∏

j=1

Pθ(y
j |xj ,D−j) (1)

where yj and xj denote the jth target and source
sentence, respectively, and D−j = {X−j ,Y−j} is
the collection of all other sentences in the source
and target document. Since generic NMT models
translate one word at a time, Eq. 1 becomes:

Pθ(Y |X) =
J∏

j=1

N∏
n=1

Pθ(y
j
n|y

j
<n,x

j ,D−j) (2)

where yjn is the nth word of the jth target sentence
and yj<n are the previously generated words.

Training The document-conditioned NMT
model Pθ(yj |xj ,D−j) is realised using a neural
architecture and usually trained via a two-step
procedure (Maruf and Haffari, 2018; Miculicich
et al., 2018). The first step involves pre-training
a standard sentence-level NMT model, and the
second step involves optimising the parameters
of the whole model, i.e., both the document-level
and the sentence-level parameters.

Decoding To generate the best translation for
a full document according to the document MT
model, the problem of maximizing Eq. 1 is
solved using a two-pass Iterative Decoding strat-
egy (Maruf and Haffari, 2018): first, the trans-
lation of each sentence is initialised using the
sentence-based NMT model; then, each trans-
lation is updated using the context-aware NMT
model fixing the other sentences’ translations.

3 Proposed Approach

The main goal of this paper is to have a document-
level NMT model which is memory-efficient, scal-
able, and capable of listening to the entire docu-
ment. To achieve this, we augment a sentence-
level NMT model (the Transformer (Vaswani
et al., 2017)) with an efficient hierarchical atten-
tion mechanism which has the ability to identify



3094

the key sentences in the document context and
then attend to the key words within those sen-
tences. As mentioned previously, we want to max-
imise Pθ(yj |xj ,D−j), where we take D−j to be
either the monolingual source or bilingual source
and target-side context in two settings: offline—
the context comes from both past and future, and
online—the context comes from only the past.

In this section, we show how to represent the
document-level context using our Context Layer,
how to regulate the information at the sentence
and document-level using context gating and fi-
nally we present our integrated model.

3.1 Document-level Context Layer

The context D−j is modeled via a single
Document-level Context Layer comprising of two
sub-layers: (i) a Multi-Head Context Attention
sub-layer, and (ii) a Feed-Forward sub-layer,
where the former consists of either a top-down
Hierarchical Attention module or a Flat Attention
module (explained shortly), and the latter is sim-
ilar to the Feed-Forward network in the original
Transformer architecture. Each sub-layer is fol-
lowed by a layer normalisation.2

Let us now describe the attention modules
which independently form the Multi-Head Con-
text Attention sub-layer.

3.1.1 Hierarchical Attention

Our hierachical attention module H-Attention(Qs,
Qw, Ks, Kw, Vw) (Figure 1) is a reformulation
of the Scaled Dot-Product Attention of Vaswani
et al. (2017). Here, we have five inputs consist-
ing of two types of keys and queries, one each for
the sentences and the words, while the values are
based only on words in the context. The Hierar-
chical Attention module has four operations:

1. Sentence-level Key Matching: This is per-
formed on a set of queries simultaneously,
packed together into a matrixQs. The sentence-
level keys are also packed into a matrix Ks. We
will describe in §3.3 how Qs and Ks are com-
puted. The attention weights are computed as:

αs = sparsemax(QsKsT /
√
dk) (3)

2We do not have residual connections after sub-layers in
our Document-level Context Layer as we found them to have
a deteriorating effect on the translation scores (also reported
by Zhang et al. (2018)).

Figure 1: Hierarchical Context Attention module.

where dk is the dimension of the keys, and αs
has dimensions equal to the total number of sen-
tences in the document. We propose to use
sparsemax (Martins and Astudillo, 2016), in-
stead of softmax, as this gives us the intended
selective attention behavior, that is identifying
the key sentences that may potentially be rele-
vant to the current sentence, hence making the
model more efficient in compressing its mem-
ory. A softmax attention, on the other hand, can
still assign low probability to sentences, form-
ing a long-tail and absorbing significant prob-
ability mass, and it cannot fully ignore those
sentences. An additive mask is used (before the
sparsemax operation) based on whether we train
for offline or online setting by masking out only
the current sentence or current and future sen-
tences, respectively.

2. Word-level Key Matching: Here the query
and key matrices, Qw and Kw, are word-level.
We perform a word-level key matching for each
sentence j in the document:

αjw = sparsemax(QwK
j
w
T
/
√
dk) (4)

where αjw is the word-level attention vector for
jth sentence.3 We can also use softmax, instead
of sparsemax, for a coarser key matching. We
explore the two variants in our experiments.

3. Re-scaling attention weights: The word-level
attention is further re-weighted by the cor-
3This can be done for only the sentences with non-zero

probabilities (obtained from the sentence-level key match-
ing), however, we found it to be computationally expensive,
as it required breaking down the batched matrices.



3095

responding sentence-level attention (Nallapati
et al., 2016) such that the probability of jth sen-
tence in a document is given by:

αjhier = αs(j)α
j
w (5)

where αs(j) is the attention weight for the
jth sentence obtained via Eq. 3 and αjw is
as in Eq. 4. The re-weighting, thus, pro-
duces a scaled attention vector αhier =
Concat(α1hier, ..., α

J
hier), each entry of which

corresponds to the attention weight for a spe-
cific word in the document.

4. Value Reading: The set of word-level values
is packed together into a matrix Vw and the ma-
trix of outputs is given by αhierVw. This mul-
tiplication, combined with sparsemax attention,
allows to prune the hierarchy.

We further extend the MULTIHEAD attention
function proposed by Vaswani et al. (2017) for our
Hierarchical Attention module as:

H-MULTIHEAD(Qs,Ks, Qw,Kw, Vw) =

Concat(head1, ..., headH)WO

where headh = H-Attention(QsW
Qs
h , QwW

Qw
h ,

KsW
Ks
h ,KwW

Kw
h , VwW

Vw
h ), W ’s are parameter

matrices and all (five) inputs are transformed using
separate linear layers.

3.1.2 Flat Attention

Another way to model the context D−j is via
single-level attention by re-using the Scaled Dot-
Product Attention in Vaswani et al. (2017),

Attention(Q,K, V ) = softmax(QKT /
√
dk)V

(6)
The attention4 here is of two types: (i) sentence-
level if K, V are computed for sentences in the
document, or (ii) word-level ifK, V are computed
for words in the document. The former module
is similar to the Memory Networks architecture of
Maruf and Haffari (2018) in that it uses sentence-
level information. However, there are two key dif-
ferences: (i) we use MultiHead attention as in the
Transformer architecture, and (ii) our context at-
tention is dynamic such that we have a separate
attention for each query word.

4We plan to investigate sparse flat attention in future work.

Figure 2: Encoder-side context integration.

3.2 Context Gating
As mentioned previously, the Multi-Head Context
Attention sub-layer is part of the Context Layer
(Figure 2), the output of which is fed into the
Transformer architecture through context gating
(Tu et al., 2018). For ith word in source or target:

γi = σ(Wrri +Wddi) (7)

r̃i = γi � ri + (1− γi)� di (8)

where W’s are parameter matrices, ri is the output
of encoder or decoder stack for ith word, di is the
output from the context layer for ith word and r̃i
is the final hidden representation for the same.

3.3 Integrated Model
The context can be integrated into the encoder or
decoder of the NMT model depending on if it is
monolingual or bilingual.5

Monolingual context integration in Encoder
We add the Document-level Context Layer along-
side the encoder stack as shown in Figure 2.
The Encoder Context Encoding block stores the
keys and values produced from the pre-trained
sentence-level NMT model. For word-level atten-
tion, the keys Kw and values Vw are composed
of vector representations (from last encoder layer)
of source words in the document, while for the
sentence-level attention, the keys Ks and values
Vs are composed of vector representations of sen-
tences in the document where the vector represen-
tation of each sentence is an average of the word

5We do not integrate context into both encoder and de-
coder as it would have redundant information from the source
(the context incorporated in the decoder is bilingual), in ad-
dition to increasing the complexity of the model.



3096

Figure 3: Decoder-side context integration.

representations in that sentence. The queries Qw,
Qs are linear transformations of the output of the
Lth encoder layer which are then matched with the
corresponding keys and values stored in the En-
coder Context Encoding block just described.

Bilingual context integration in Decoder We
again add the Document-level Context Layer
alongside the decoder stack as in Figure 3. How-
ever, instead of choosing the keys and values to be
monolingual as in the encoder, we follow Tu et al.
(2018) in choosing the key to match to the source-
side context, while designing the value to match
to the target-side context. Hence, the keys (in the
Decoder Context Encoding block) are composed
of context vectors from the Source Attention sub-
layer, while the values are composed of the hid-
den representations of the target words, both from
the last decoder layer. Again the keys Kw and
Ks are either for individual target words or tar-
get sentences, and same goes for Vw and Vs. The
queries Qw, Qs for the Context Layer come from
the Source Attention sub-layer in the Lth layer of
the decoder (Figure 3).

4 Experiments

4.1 Setup
Datasets We conduct experiments for
English→German on three different domains:
TED talks, News-Commentary and Europarl.
These datasets are chosen based on their variance

Domain #Sentences Document length
TED 0.21M/9K/2.3K 120.89/96.42/98.74
News 0.24M/2K/3K 38.93/26.78/19.35
Europarl 1.67M/3.6K/5.1K 14.14/14.95/14.06

Table 1: Training/development/test corpora statistics:
number of sentences (K stands for thousands and M for
millions), and average document length (in sentences).

in genre, style and level of formality:

• TED This corpus is from the IWSLT 2017 MT
track (Cettolo et al., 2012) and contains tran-
scripts of TED talks aligned at sentence level.
Each talk is considered to be a document. We
combine tst2016-2017 into the test set and the
rest are used for development.

• News-Commentary We obtain the sentence-
aligned document-delimited News Commentary
v11 corpus for training.6 The WMT’16 news-
test2015 and news-test2016 are used for devel-
opment and testing, respectively.

• Europarl This dataset is extracted from Eu-
roparl v7 (Koehn, 2005). The source and tar-
get sentences are aligned using the links pro-
vided by Tiedemann (2012). Following Maruf
and Haffari (2018), we use the SPEAKER tag as
the document delimiter. Documents longer than
5 sentences are kept and the resulting corpus is
randomly split into training, dev and test sets.

The corpora statistics are provided in Table 1.
All datasets are tokenised and truecased using the
Moses toolkit (Koehn et al., 2007), and split into
subword units using a joint BPE model with 30K
merge operations (Sennrich et al., 2016).

Models and Baselines For offline document
MT, we have two context-agnostic baselines: (i) a
modified version of RNNSearch (Bahdanau et al.,
2015), which incorporates dropout on the output
layer and improves the attention model by feeding
the previously generated word, and (ii) the state-
of-the-art Transformer architecture. For the online
case, we again have the Transformer as a context-
agnostic baseline and two context-aware baselines
(Zhang et al., 2018; Miculicich et al., 2018).

All models are implemented in C++ using
DyNet (Neubig et al., 2017). For RNNSearch, we
modify the sentence-based NMT implementation
in mantis (Cohn et al., 2016). The encoder is a sin-
gle layer bidirectional GRU (Cho et al., 2014) and

6
www.casmacat.eu/corpus/news-commentary.html

www.casmacat.eu/corpus/news-commentary.html


3097

Integration into Encoder Integration into Decoder
TED News Europarl TED News Europarl

Model BLEU Meteor BLEU Meteor BLEU Meteor BLEU Meteor BLEU Meteor BLEU Meteor
RNNSearch 19.24 40.81 16.51 36.79 26.26 44.14 19.24 40.81 16.51 36.79 26.26 44.14
Transformer 23.28 44.17 22.78 42.19 28.72 46.22 23.28 44.17 22.78 42.19 28.72 46.22
+Attention, sentence 24.47 45.25 24.78 43.90 29.60 46.98 24.38 44.82 24.67 43.82 29.67 47.04

word 24.55 44.89 24.55 43.75 29.63 46.94 24.27 44.95 24.23 43.44 29.68 46.93
+H-Attention, sparse-soft 24.23 44.81 24.76 44.10 29.72 47.03 24.19 44.94 24.67 43.86 29.69 46.97

sparse-sparse 24.27 45.07 24.66 44.18 29.64 47.04 24.14 45.32 24.49 43.49 29.59 47.02

Table 2: BLEU and Meteor scores for variants of our model and two context-agnostic baselines for offline docu-
ment MT. bold: Best performance. All reported results for our model are significantly better than both baselines.

Integration into Encoder Integration into Decoder
TED News Europarl TED News Europarl

Model BLEU Meteor BLEU Meteor BLEU Meteor BLEU Meteor BLEU Meteor BLEU Meteor
Zhang et al. (2018) 24.00 44.69 23.08 42.40 29.32 46.72 23.82 44.54 22.78 42.17 29.35 46.73
Miculicich et al. (2018) 24.58 45.48 25.03 44.02 28.60 46.09 24.39 45.23 24.38 43.58 29.58 46.91
Transformer 23.28 44.17 22.78 42.19 28.72 46.22 23.28 44.17 22.78 42.19 28.72 46.22
+Attention, sentence 24.38 45.01 24.46F 43.46F 29.59♣ 47.02♣ 24.29F 45.13F 24.75♣ 44.03♣ 29.56 46.84

word 24.22 45.05F 24.84F 44.27F 29.67♣ 47.04♣ 24.02 44.79 24.17F 43.53F 29.90♣ 47.11♣

+H-Attention, sparse-soft 24.34 45.05F 24.54F 43.66F 29.75♣ 47.22♣ 24.62F 45.32F 24.36F 43.67F 29.80F 47.11♣

sparse-sparse 24.42 45.38F 24.73F 44.06F 29.39♦ 46.78♦ 24.43F 45.10F 24.58F 43.75F 29.64F 46.94F

Table 3: BLEU and Meteor scores for variants of our model and three baselines for online document MT. bold:
Best performance. F, ♦, ♣: Statistically significantly better than our implementations of Zhang et al. (2018),
Miculicich et al. (2018), or both. All reported results for our model are significantly better than the Transformer.

the decoder is a 2-layer GRU with embeddings and
hidden dimensions set to 512. The dropout rate for
the output layer is set to 0.2. For the Transformer,
we use Transformer-DyNet7 implementation and
extend it for our context-aware NMT model.8 The
hidden dimensions and feed-forward layer size is
set to 512 and 2048 respectively. We use 4 layers9

each in the encoder and decoder with 8 attention
heads and employ label smoothing with a value of
0.1. We also employ all four types of dropouts as
in the original Transformer with a rate of 0.1 for
the sentence-based model and 0.2 for our context-
aware model.

For training all models, we use the default
Adam optimiser (Kingma and Ba, 2015) with an
initial learning rate of 0.0001 and employ early
stopping. For our context-aware NMT model, we
use a two-stage training strategy as described in
§2.2. For inference, we use Iterative Decoding
only when using the bilingual context. All exper-
iments are run on a single Nvidia P100 GPU with
16GBs of memory.10

7
https://github.com/duyvuleo/Transformer-DyNet

8The code is available at https://github.com/
sameenmaruf/selective-attn

9We found this configuration to be much more stable than
using 6 layers with almost no difference in performance as
reported by Xia et al. (2018).

10The experiments can also be run on GPUs with 10-
12GBs of memory by reducing the batch size at the expense

Evaluation Metrics For evaluation, we use
BLEU (Papineni et al., 2002) and Meteor (Lavie
and Agarwal, 2007) scores on tokenised text, and
measure statistical significance with respect to the
baselines, p < 0.05 (Clark et al., 2011).

4.2 Main Results

We divide our experiments into two parts: offline
and online document MT.

Offline Document MT From the scores of the
two context-agnostic baselines in Table 2, we can
see that the Transformer beats the RNNSearch
model in all cases by atleast +2.5 BLEU and +2.1
Meteor scores showing that our hyperparameter
choice for the Transformer is indeed effective.

For the Encoder Context integration, our Hier-
archical Attention models perform the (near) best
for News and Europarl datasets with +1.98 and +1
BLEU and +1.99 and +0.82 Meteor improvements
with respect to the Transformer. For TED talks,
however, we find the Flat Attention based mod-
els (sentence and word-level) to be the best with
+1.27 BLEU and +1.08 METEOR improvements.
For Decoder Context integration, we find the Hier-
archical Attention to be the best in majority of the
cases both in terms of BLEU and Meteor.

of increased computational cost.

https://github.com/duyvuleo/Transformer-DyNet
https://github.com/sameenmaruf/selective-attn
https://github.com/sameenmaruf/selective-attn


3098

Online Document MT From Table 3, all
our models significantly outperform the context-
agnostic baseline and are significantly better than
Zhang et al. (2018) in majority cases. For En-
coder Context integration, the HAN encoder (Mi-
culicich et al., 2018) is the best for TED and News
datasets, however, the results are statistically in-
significant with respect to our best model. For
Europarl, our Hierarchical Attention model per-
forms significantly better than Miculicich et al.
(2018) with a gain of +1.15 BLEU and +1.13 Me-
teor. For Decoder Context integration, our Hier-
achical Attention models are the winner in major-
ity cases and our best models beat Miculicich et al.
(2018) for all datasets based upon BLEU and Me-
teor. The main conclusion we draw from these re-
sults is that efficiently using the context informa-
tion at hand is crucial when it comes to improv-
ing the performance of context-aware NMT. Fur-
thermore, shorter pieces of text (e.g., the ones in
Europarl) benefit more from using global context
because their sentences may exhibit higher inter-
dependency than those in a longer piece of text.

Offline vs. Online Document MT Let us com-
pare the overall results for the offline and online
document MT settings. For all datasets and model
variants, we find the best BLEU and Meteor scores
in Tables 2 and 3 (highlighted in bold) to be quite
close to each other with those for the online set-
ting slightly better. This is quite self-explanatory,
because in essence, all of the datasets comprise of
talks, speeches or commentaries, which are in fact
produced in an online manner and hence we do not
see drastic improvements in terms of BLEU and
Meteor when conditioning on the future context.
This, in our opinion, does not mean that we should
never look into the future, but just that NMT mod-
els in general are highly subjective to data, and
whether context-aware models benefit from future
context is also dependent on that.

4.3 Analysis

Evaluation on Contrastive Pronoun Test Set
It has been argued that evaluation metrics which
quantify the overall translation quality are some-
what ill-equipped to assess how well models trans-
late inter-sentential phenomena such as pronouns.
Hence, we use a test suite of contrastive transla-
tions designed to measure accuracy of translating
the English pronoun it to its German counterparts
es, er and sie (Müller et al., 2018). We are inter-

Model antecedent distance
0 1 2 3 >3

Offline document MT
RNNSearch 0.415 0.310 0.424 0.440 0.647
Transformer 0.586 0.308 0.437 0.48 0.642
+Attention, sentence 0.677 0.314 0.439 0.478 0.697

word 0.686 0.347 0.464 0.511 0.679
+H-Attention, sparse-soft 0.676 0.308 0.440 0.480 0.686

sparse-sparse 0.652 0.303 0.435 0.471 0.701
Online document MT

Zhang et al. (2018) 0.622 0.321 0.450 0.485 0.658
Miculicich et al. (2018) 0.722 0.326 0.451 0.471 0.661
Transformer 0.586 0.308 0.437 0.48 0.642
+Attention, sentence 0.732 0.340 0.460 0.485 0.661

word 0.690 0.317 0.444 0.487 0.683
+H-Attention, sparse-soft 0.692 0.329 0.446 0.464 0.656

sparse-sparse 0.711 0.317 0.437 0.489 0.692

Table 4: Accuracy on contrastive test set with regard to
antecedent distance (in sentences) on TED Talks. An-
tecedent distance 0 means the pronoun occurs in the
same sentence as the antecedent.

ested to see if our global document-context mod-
els surpass the local context-aware baselines. Ta-
ble 4 shows that not only our global-context mod-
els are quite effective but our Hierarchical Atten-
tion model is most useful when the antecedent is
farther than three previous sentences. We also con-
clude that models for offline MT perform better
when antecedent distance is greater than two.

Subjective Evaluation We conduct a subjec-
tive evaluation to validate the benefit of exploit-
ing document-level context. Three native German
speakers were asked to choose the better (with ties
allowed) of two translations for each of 18 docu-
ments (randomly sampled from Europarl test set).
The two translations, one produced by the Trans-
former and the other by our Hierarchical Atten-
tion model, were evaluated in terms of: adequacy
(Which translation expresses the meaning of the
source text more adequately?) and fluency (Which
text has better German?) (Läubli et al., 2018). Let
a, b be number of ratings in favour of Transformer
or our model, respectively, and t be number of ties,
then number of successes x = b + 0.5t and trials
n = a+ b+ t. We test for statistically significant
preference of our model over the Transformer by
means of two-sided Sign Tests and find that our
model is better than the Transformer both in terms
of document-level adequacy (x = 39, n = 54, p =
0.0015) and fluency (x = 38, n = 54, p = 0.0038).

Model Complexity Model complexity is re-
ported in Table 5. Our context-aware models in-
troduce only 8% more parameters to the original



3099

Model #Params Speed (words/sec.)
Training Decoding

Zhang et al. (2018) 59.5M 3300 84.94
Miculicich et al. (2018) 54.8M 1650 76.90
Transformer 50M 5100 86.33
+Attention, sentence 53.7M 3750 83.84
+H-Attention, sparse-soft 54.2M 2600 74.11

Table 5: Model complexity for Encoder Context inte-
gration models (News-Commentary).

Transformer model. In comparison to the Trans-
former, our Hierarchical Attention model is slow
in training, dropping the speed by almost 50%11,
but it is still almost 40% faster than Miculicich
et al. (2018). At decoding time, our Hierarchical
Attention model is almost equivalent to Miculicich
et al. (2018) and only 13% slower than Zhang et al.
(2018). Hence, attending to the whole document
(instead of few previous sentences) does not add
to the time complexity of the model on average.

Qualitative Analysis To analyse the effect of
using sparse attention at both the sentence and
word-level, we looked at the attention weights
computed by sparsemax. Table 6 shows an ex-
ample where our model helped generate a correct
translation of the noun “thoughts” (highlighted in
bold). The context sentences shown in the bottom
box had the highest attention weights as assigned
by sparsemax. It seems that this particular atten-
tion head focuses more on phrases like “words of
sympathy”, “support’, “symbol of hope” which
are related to the query “thoughts”. Another ex-
ample in Table 7 shows how our model correctly
translates the pronoun “their”. Upon looking at the
words in the context sentences, it seems that this
particular attention head focuses on the words re-
lated to the antecedent “Croatia’s Serbian popula-
tion” with most of the weight concentrated around
neighbouring words in sentence sj−1. It is evi-
dent from both examples that word-level sparsity
is more prevalent in longer sentences in the con-
text. The same holds for sparsity at sentence-level.

5 Related Work

The body of work in document-level MT can be
broadly classified into two categories: conven-
tional MT and neural MT.

11DyNet implementation of sparsemax is CPU-based and
only operates on column vectors. We believe a GPU-based
matrix implementation would bring the speed much closer to
our Word Attention model (training: 3100, decoding: 81.38).

Src: my thoughts are also with the victims .
Ref: meine Gedanken sind auch bei den Opfern .
Transformer: ich denke auch an die Opfer .
Zhang et al. (2018): ich denke auch an die Opfer .
Miculicich et al. (2018): ich denke auch an die Opfer .
Our Model: meine Gedanken sind auch bei den Opfern .

Head 2: Attention to related words sympathy, support, hope
sj−2: ( FR ) Madam President , many things have already
been said , but I would like to echo all the words of
sympathy and support that have already been addressed
to the peoples of Tunisia and Egypt .
sj+4: it must implement a strong strategy towards
these countries .
sj−1: they are a symbol of hope for all those who
defend freedom .

Table 6: Example of noun disambiguation. Source
context sentences are ordered in decreasing probability
mass. The intensity of color corresponds to the atten-
tion given to a specific word before rescaling.

Src: Croatia is their homeland , too .
Ref: Kroatien ist auch ihre Heimat .
Transformer: Kroatien ist auch seine Heimat .
Our Model: Kroatien ist auch ihr Heimatland .

Head 8: Attention to words related to the antecedent.
sj−1: to name but a few , these include cooperation with
the Hague Tribunal , efforts made so far in prosecuting
corruption , restructuring the economy and finances and
greater commitment and sincerity in eliminating the
obstacles to the return of Croatia ’s Serbian population .
sj−4: by signing a border arbitration agreement with
its neighbour Slovenia , the new Croatian Government
has not only eliminated an obstacle to the negotiating
process , but has also paved the way for the resolution
of other issues .

Table 7: Example of pronoun disambiguation. Context
sentences are ordered in decreasing probability mass.

Conventional Document-level MT These can
further be classified into two main categories. The
first, which use cache-based memories (Tiede-
mann, 2010; Gong et al., 2011) and the second,
which focus on specific discourse phenomema like
anaphora (Hardmeier and Federico, 2010), lex-
ical cohesion (Xiong et al., 2013; Gong et al.,
2015; Mascarell, 2017) and coreference (Miculi-
cich Werlen and Popescu-Belis, 2017) to name
a few. Most of these approaches are, however,
restrictive as they mostly involve using hand-
crafted features similar to the conventional MT ap-
proaches.

Document-level Neural MT The works here
can again be divided into two categories: online—
use previous context only, and offline—use both
past and future contexts. Most works fall into the
former category, with those that use only a single



3100

previous sentence in the source (Jean et al., 2017;
Tiedemann and Scherrer, 2017; Voita et al., 2018);
one previous sentence both in source and target
(Bawden et al., 2018); more than one previous
source sentence (Wang et al., 2017; Zhang et al.,
2018); or a few previous source and target sen-
tences (Miculicich et al., 2018). Apart from fix-
ing the context length, there are few works which
use cache-based memories to store contextual in-
formation (Tu et al., 2018; Kuang et al., 2018) and
use that to improve the MT system performance.
A recent work (Maruf et al., 2018) reports promis-
ing results when using the complete history for
translating online conversations.

For the offline setting, however, there is only
one work that effectively uses the full document-
context on both source and target-side using mem-
ory networks (Maruf and Haffari, 2018). The
debate in document-level NMT today is mostly
about how much of the previous context to use and
there has been no comparison between the online
and offline setting except using only one previous
and following sentence (Voita et al., 2018).

Sparse Attention Sparse attention and its con-
strained variants have been used to address the
coverage problem in NMT (Malaviya et al., 2018)
by limiting the amount of attention that each
source word can receive. Apart from NMT, sparse
attention has been shown to yield promising re-
sults for NLP tasks of textual entailment (Martins
and Astudillo, 2016) and summarization (Niculae
and Blondel, 2017).

6 Conclusion

We have proposed a novel approach to hierar-
chical attention for context-aware NMT, based
on sparse attention, which is both scalable and
efficient. Experiments and evaluation on three
English→German datasets in offline and online
document MT settings show that our approach sur-
passes context-agnostic and two recent context-
aware baselines. The qualitative analysis shows
that the sparsity at sentence-level allows our model
to identify key sentences in the document context
and the sparsity at word-level allows it to focus on
key words in those sentences allowing for an effi-
cient compression of memory. In future work, we
plan to dig deeper on the benefits of sparse atten-
tion in terms of better interpretability of context-
aware NMT models.

Acknowledgments

The authors are grateful to the anonymous re-
viewers for their helpful comments and correc-
tions. SM would like to thank her colleagues
at Monash University: Veronika Kuchta, Har-
ald Bögeholz and Hagen Lauer, for their help in
the subjective evaluation. This work was sup-
ported by the Multi-modal Australian ScienceS
Imaging and Visualisation Environment (MAS-
SIVE) (www.massive.org.au), the Euro-
pean Research Council (ERC StG DeepSPIN
758969), the Fundação para a Ciência e Tec-
nologia through contracts UID/EEA/50008/2019
and CMUPERI/TIC/0046/2014 (GoLocal), and a
Google Faculty Research Award to GH.

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-

gio. 2015. Neural machine translation by jointly
learning to align and translate. In Proceedings of
the International Conference on Learning Represen-
tations.

Rachel Bawden, Rico Sennrich, Alexandra Birch, and
Barry Haddow. 2018. Evaluating discourse phe-
nomena in neural machine translation. In Proceed-
ings of the 2018 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, Volume 1
(Long Papers), pages 1304–1313. Association for
Computational Linguistics.

Mauro Cettolo, Christian Girardi, and Marcello Fed-
erico. 2012. Wit3: Web inventory of transcribed and
translated talks. In Proceedings of the 16th Con-
ference of the European Association for Machine
Translation (EAMT), pages 261–268, Trento, Italy.

Kyunghyun Cho, B van Merrienboer, Dzmitry Bah-
danau, and Yoshua Bengio. 2014. On the properties
of neural machine translation: Encoder-decoder ap-
proaches. In Eighth Workshop on Syntax, Semantics
and Structure in Statistical Translation (SSST-8).

Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis testing for
statistical machine translation: Controlling for op-
timizer instability. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies (Short
Papers), pages 176–181. Association for Computa-
tional Linguistics.

Trevor Cohn, Cong Duy Vu Hoang, Ekaterina Vy-
molova, Kaisheng Yao, Chris Dyer, and Gholamreza
Haffari. 2016. Incorporating structural alignment
biases into an attentional neural translation model.
In Proceedings of the North American Chapter of

www.massive.org.au
https://doi.org/10.18653/v1/N18-1118
https://doi.org/10.18653/v1/N18-1118
http://www.aclweb.org/anthology/P11-2031
http://www.aclweb.org/anthology/P11-2031
http://www.aclweb.org/anthology/P11-2031
http://www.aclweb.org/anthology/N16-1102
http://www.aclweb.org/anthology/N16-1102


3101

the Association for Computational Linguistics: Hu-
man Language Technologies, pages 876–885. Asso-
ciation for Computational Linguistics.

Peter Dayan, Sham Kakade, and P. Read Montague.
2000. Learning and selective attention. Nature Neu-
roscience, 3:1218–1223.

Zhengxian Gong, Min Zhang, and Guodong Zhou.
2011. Cache-based document-level statistical ma-
chine translation. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 909–919. Association for Computa-
tional Linguistics.

Zhengxian Gong, Min Zhang, and Guodong Zhou.
2015. Document-level machine translation evalua-
tion with gist consistency and text cohesion. In Pro-
ceedings of the Second Workshop on Discourse in
Machine Translation, pages 33–40, Lisbon, Portu-
gal. Association for Computational Linguistics.

Christian Hardmeier and Marcello Federico. 2010.
Modelling pronominal anaphora in statistical ma-
chine translation. In International Workshop on
Spoken Language Translation, pages 283–289.

Sebastien Jean, Stanislas Lauly, Orhan Firat, and
Kyunghyun Cho. 2017. Does neural machine
translation benefit from larger context? In
arXiv:1704.05135.

Diederik P. Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. In Proceedings
of the International Conference on Learning Repre-
sentations.

Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Conference Pro-
ceedings: the 10th Machine Translation Summit,
pages 79–86. AAMT.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondřej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
pages 177–180. Association for Computational Lin-
guistics.

Shaohui Kuang, Deyi Xiong, Weihua Luo, and
Guodong Zhou. 2018. Modeling coherence for
neural machine translation with dynamic and topic
caches. In Proceedings of the 27th International
Conference on Computational Linguistics, pages
596–606. Association for Computational Linguis-
tics.

Samuel Läubli, Rico Sennrich, and Martin Volk. 2018.
Has machine translation achieved human parity? A
case for document-level evaluation. In Proceedings
of the Conference on Empirical Methods in Natu-
ral Language Processing, pages 4791–4796. Asso-
ciation for Computational Linguistics.

Alon Lavie and Abhaya Agarwal. 2007. Meteor: An
automatic metric for mt evaluation with high levels
of correlation with human judgments. In Proceed-
ings of the Second Workshop on Statistical Machine
Translation, pages 228–231. Association for Com-
putational Linguistics.

Chaitanya Malaviya, Pedro Ferreira, and André F. T.
Martins. 2018. Sparse and constrained attention for
neural machine translation. In Proceedings of the
56th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 2: Short Papers), vol-
ume 2, pages 370–376.

André F. T. Martins and Ramón Astudillo. 2016. From
softmax to sparsemax: A sparse model of attention
and multi-label classification. In Proceedings of The
33rd International Conference on Machine Learn-
ing, volume 48, pages 1614–1623, New York, New
York, USA. PMLR.

Sameen Maruf and Gholamreza Haffari. 2018. Docu-
ment context neural machine translation with mem-
ory networks. In Proceedings of the 56th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), pages 1275–
1284. Association for Computational Linguistics.

Sameen Maruf, André F. T. Martins, and Gholamreza
Haffari. 2018. Contextual neural model for translat-
ing bilingual multi-speaker conversations. In Pro-
ceedings of the Third Conference on Machine Trans-
lation: Research Papers, pages 101–112, Brussels,
Belgium. Association for Computational Linguis-
tics.

Laura Mascarell. 2017. Lexical chains meet word
embeddings in document-level statistical machine
translation. In Proceedings of the Third Workshop
on Discourse in Machine Translation, pages 99–
109. Association for Computational Linguistics.

Lesly Miculicich, Dhananjay Ram, Nikolaos Pappas,
and James Henderson. 2018. Document-level neural
machine translation with hierarchical attention net-
works. In Proceedings of the 2018 Conference on
Empirical Methods in Natural Language Process-
ing, pages 2947–2954.

Lesly Miculicich Werlen and Andrei Popescu-Belis.
2017. Using coreference links to improve spanish-
to-english machine translation. In Proceedings of
the 2nd Workshop on Coreference Resolution Be-
yond OntoNotes (CORBON 2017), pages 30–40, Va-
lencia, Spain. Association for Computational Lin-
guistics.

Mathias Müller, Annette Rios, Elena Voita, and Rico
Sennrich. 2018. A large-scale test set for the eval-
uation of context-aware pronoun translation in neu-
ral machine translation. In Proceedings of the Third
Conference on Machine Translation: Research Pa-
pers, pages 61–72, Belgium, Brussels. Association
for Computational Linguistics.

https://doi.org/10.1038/81504
http://aclweb.org/anthology/D11-1084
http://aclweb.org/anthology/D11-1084
https://doi.org/10.18653/v1/W15-2504
https://doi.org/10.18653/v1/W15-2504
http://www.aclweb.org/anthology/P07-2045
http://www.aclweb.org/anthology/P07-2045
http://aclweb.org/anthology/C18-1050
http://aclweb.org/anthology/C18-1050
http://aclweb.org/anthology/C18-1050
http://aclweb.org/anthology/D18-1512
http://aclweb.org/anthology/D18-1512
http://aclweb.org/anthology/W07-0734
http://aclweb.org/anthology/W07-0734
http://aclweb.org/anthology/W07-0734
http://aclweb.org/anthology/P18-2059
http://aclweb.org/anthology/P18-2059
http://proceedings.mlr.press/v48/martins16.html
http://proceedings.mlr.press/v48/martins16.html
http://proceedings.mlr.press/v48/martins16.html
http://aclweb.org/anthology/P18-1118
http://aclweb.org/anthology/P18-1118
http://aclweb.org/anthology/P18-1118
http://www.aclweb.org/anthology/W18-6311
http://www.aclweb.org/anthology/W18-6311
https://doi.org/10.18653/v1/W17-4813
https://doi.org/10.18653/v1/W17-4813
https://doi.org/10.18653/v1/W17-4813
http://aclweb.org/anthology/D18-1325
http://aclweb.org/anthology/D18-1325
http://aclweb.org/anthology/D18-1325
https://doi.org/10.18653/v1/W17-1505
https://doi.org/10.18653/v1/W17-1505
http://www.aclweb.org/anthology/W18-6307
http://www.aclweb.org/anthology/W18-6307
http://www.aclweb.org/anthology/W18-6307


3102

Ramesh Nallapati, Bowen Zhou, Cı́cero Nogueira dos
Santos, Çaglar Gülçehre, and Bing Xiang. 2016.
Abstractive text summarization using sequence-to-
sequence RNNs and beyond. In Proceedings of
Conference on Natural Language Learning, pages
280–290. Association for Computational Linguis-
tics.

Graham Neubig, Chris Dyer, Yoav Goldberg, Austin
Matthews, Waleed Ammar, Antonios Anastasopou-
los, Miguel Ballesteros, David Chiang, Daniel
Clothiaux, Trevor Cohn, Kevin Duh, Manaal
Faruqui, Cynthia Gan, Dan Garrette, Yangfeng Ji,
Lingpeng Kong, Adhiguna Kuncoro, Gaurav Ku-
mar, Chaitanya Malaviya, Paul Michel, Yusuke
Oda, Matthew Richardson, Naomi Saphra, Swabha
Swayamdipta, and Pengcheng Yin. 2017. Dynet:
The dynamic neural network toolkit. arXiv preprint
arXiv:1701.03980.

Vlad Niculae and Mathieu Blondel. 2017. A regular-
ized framework for sparse and structured neural at-
tention. In I. Guyon, U. V. Luxburg, S. Bengio,
H. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-
nett, editors, Advances in Neural Information Pro-
cessing Systems 30, pages 3338–3348. Curran As-
sociates, Inc.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting on Association for Com-
putational Linguistics, pages 311–318. Association
for Computational Linguistics.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Neural machine translation of rare words with
subword units. In Proceedings of the 54th Annual
Meeting of the Association for Computational Lin-
guistics, pages 1715–1725.

Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.
Sequence to sequence learning with neural net-
works. In Proceedings of the 27th International
Conference on Neural Information Processing Sys-
tems, pages 3104–3112. MIT Press.

Jörg Tiedemann. 2010. Context adaptation in statis-
tical machine translation using models with expo-
nentially decaying cache. In Proceedings of the
2010 Workshop on Domain Adaptation for Natural
Language Processing, DANLP 2010, pages 8–15,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.

Jörg Tiedemann. 2012. Parallel data, tools and inter-
faces in OPUS. In Proceedings of the 8th Interna-
tional Conference on Language Resources and Eval-
uation (LREC’12), Istanbul, Turkey. European Lan-
guage Resources Association (ELRA).

Jörg Tiedemann and Yves Scherrer. 2017. Neural ma-
chine translation with extended context. In Proceed-
ings of the Third Workshop on Discourse in Machine
Translation, pages 82–92. Association for Computa-
tional Linguistics.

Zhaopeng Tu, Yang Liu, Shuming Shi, and Tong
Zhang. 2018. Learning to remember translation his-
tory with a continuous cache. Transactions of the
Association for Computational Linguistics, 6:407–
420.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems 30, pages 5998–6008. Curran Asso-
ciates, Inc.

Elena Voita, Pavel Serdyukov, Rico Sennrich, and Ivan
Titov. 2018. Context-aware neural machine trans-
lation learns anaphora resolution. In Proceedings
of the 56th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Pa-
pers), pages 1264–1274. Association for Computa-
tional Linguistics.

Longyue Wang, Zhaopeng Tu, Andy Way, and Qun
Liu. 2017. Exploiting cross-sentence context for
neural machine translation. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 2816–2821. Association
for Computational Linguistics.

Yingce Xia, Xu Tan, Fei Tian, Tao Qin, Nenghai Yu,
and Tie-Yan Liu. 2018. Model-level dual learning.
In Proceedings of the 35th International Conference
on Machine Learning, pages 5379–5388.

Deyi Xiong, Yang Ding, Min Zhang, and Chew Lim
Tan. 2013. Lexical chain based cohesion models
for document-level statistical machine translation.
In Proceedings of the 2013 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1563–1573. Association for Computational Linguis-
tics.

Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He,
Alex Smola, and Eduard Hovy. 2016. Hierarchi-
cal attention networks for document classification.
In Proceedings of the 2016 Conference of the North
American Chapter of the Association for Compu-
tational Linguistics: Human Language Technolo-
gies, pages 1480–1489. Association for Computa-
tional Linguistics.

Jiacheng Zhang, Huanbo Luan, Maosong Sun, Feifei
Zhai, Jingfang Xu, Min Zhang, and Yang Liu. 2018.
Improving the transformer translation model with
document-level context. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 533–542. Association for Com-
putational Linguistics.

http://www.aclweb.org/anthology/K16-1028
http://www.aclweb.org/anthology/K16-1028
http://papers.nips.cc/paper/6926-a-regularized-framework-for-sparse-and-structured-neural-attention.pdf
http://papers.nips.cc/paper/6926-a-regularized-framework-for-sparse-and-structured-neural-attention.pdf
http://papers.nips.cc/paper/6926-a-regularized-framework-for-sparse-and-structured-neural-attention.pdf
http://aclweb.org/anthology/P02-1040
http://aclweb.org/anthology/P02-1040
http://www.aclweb.org/anthology/P16-1162
http://www.aclweb.org/anthology/P16-1162
http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf
http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf
http://www.aclweb.org/anthology/W10-2602
http://www.aclweb.org/anthology/W10-2602
http://www.aclweb.org/anthology/W10-2602
https://doi.org/10.18653/v1/W17-4811
https://doi.org/10.18653/v1/W17-4811
http://aclweb.org/anthology/Q18-1029
http://aclweb.org/anthology/Q18-1029
http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf
http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf
http://aclweb.org/anthology/P18-1117
http://aclweb.org/anthology/P18-1117
http://aclweb.org/anthology/D17-1301
http://aclweb.org/anthology/D17-1301
http://proceedings.mlr.press/v80/xia18a.html
http://aclweb.org/anthology/D/D13/D13-1163.pdf
http://aclweb.org/anthology/D/D13/D13-1163.pdf
https://doi.org/10.18653/v1/N16-1174
https://doi.org/10.18653/v1/N16-1174
http://aclweb.org/anthology/D18-1049
http://aclweb.org/anthology/D18-1049

