



















































Learning to Flip the Sentiment of Reviews from Non-Parallel Corpora


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 6311–6316,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

6311

Learning to Flip the Sentiment of Reviews from Non-Parallel Corpora

Canasai Kruengkrai
Yahoo Japan Corporation

ckruengk@yahoo-corp.jp

Abstract

Flipping sentiment while preserving sentence
meaning is challenging because parallel sen-
tences with the same content but different sen-
timent polarities are not always available for
model learning. We introduce a method for
acquiring imperfectly aligned sentences from
non-parallel corpora and propose a model that
learns to minimize the sentiment and con-
tent losses in a fully end-to-end manner. Our
model is simple and offers well-balanced re-
sults across two domains: Yelp restaurant and
Amazon product reviews.1

1 Introduction

Text style transfer is the process of editing a sen-
tence to modify specific attributes (e.g., style, sen-
timent, and tense) while preserving its attribute-
independent content (Shen et al., 2017; Fu et al.,
2018; Li et al., 2018). In this work, we are
particularly interested in sentiment modification.
This task is challenging because parallel data (i.e.,
aligned sentences with the same content but dif-
ferent sentiment polarities) are not always avail-
able and they are costly to annotate. Major exist-
ing solutions that exploit non-parallel data include
training discriminators to guide sentence genera-
tion (Hu et al., 2017; Shen et al., 2017; Fu et al.,
2018), retrieving similar sentences from another
corpus and then editing them (Guu et al., 2018; Li
et al., 2018), and applying back-translation (Prab-
humoye et al., 2018; Zhang et al., 2018b; Lample
et al., 2019).

Achieving both accurate sentiment modification
and high content preservation can be difficult. Re-
cently, Li et al. (2018)’s framework has shown
promising results. They delete sentiment-specific
phrases (e.g., “works great”) from a given sen-

1The code for reproducibility will be available on the au-
thor’s website.

(a) the service was great too→ the service was n’t too great
(b) this one is by far the worst→ this one is the best by far

Figure 1: The retrieved sentences (right) from the op-
posite corpora given the query sentences (left).

tence, retrieve new phrases associated with the tar-
get sentiment (e.g, “barely used”), and recombine
them to generate an output. However, their frame-
work relies on separate modules and assumes
that clear boundaries exist between the sentiment-
specific phrases and the content, which is not al-
ways the case.

In this work, our objective is to develop a model
that can be trained end-to-end from sentence pairs
acquired from non-parallel corpora. Our approach
is based on the observation that shared tokens
between similar sentences in unaligned corpora
convey the content we wish to preserve, while
non-shared tokens more or less reflect sentiment
change. Figure 1 shows the examples of the re-
trieved sentences from the opposite corpora given
the query sentences. Although these sentence
pairs are not perfectly aligned, they provide a use-
ful signal of sentiment change and are easy to ob-
tain. By considering the retrieved sentences as the
noisy source sentences and the query sentences as
the target sentences, we can extend an encoder-
decoder model to learn sentiment modification us-
ing auxiliary loss functions.

Our contributions are as follows: We introduce
a search-and-corrupt method for creating training
data from non-parallel corpora (§3.1). We propose
a model that learns to minimize the sentiment and
content losses with an alignment constraint in a
fully end-to-end manner (§3.2). We empirically
show that our model produces more balanced re-
sults in terms of sentiment modification and con-
tent preservation than more complex models on
Yelp restaurant and Amazon product reviews (§4).



6312

2 Related wok

Following a seminal work by Hu et al. (2017),
researchers have developed a variety of meth-
ods for learning the structured/unstructured parts
(i.e., style/content) of latent representations. Hu
et al. (2017)’s method is based on the com-
bination of semi-supervised variational autoen-
coders (Kingma et al., 2014) and the wake-sleep
algorithm (Hinton et al., 1995). Shen et al. (2017)
utilized an adversarial training method that uses
a style discriminator to align the populations of
transferred and real target sentences. Other ap-
proaches related to disentangled representations
include learning multi-decoder/style-embedding
models (Fu et al., 2018) and using a language
model as the style discriminator (Yang et al.,
2018). Our model only has an objective compo-
nent to align sentence representations. We do not
force our model to learn the disentangled represen-
tations of style and content. Our work is closely
related to classical approaches for discovering par-
allel sentences in non-parallel corpora (Fung and
Cheung, 2004; Munteanu and Marcu, 2005) and
prototype-then-edit approaches (Guu et al., 2018).
Li et al. (2018) introduced the notion of attribute
markers, which are style-specific words/phrases
for disentangling style and content in a sentence
at the word level. There is also a line of work
that studies other aspects of words based on emo-
tional information (Xu et al., 2018; Zhang et al.,
2018a). Here, we make no assumption on phrase
boundaries between style and content. We sim-
ply exploit the local properties of two imperfectly
aligned sentences.

3 Proposed method

3.1 Data preprocessing

We derive Yelp restaurant and Amazon product re-
views from Li et al. (2018).2 The original training
and validation sets consist of unaligned corpora of
positive and negative sentences. Given a sentence
in one corpus, we retrieve a similar sentence from
another corpus. We use a nearest neighbor search
based on MinHash (Broder, 2000) and LSH For-
est (Bawa et al., 2005), which are implemented in
datasketch.3 Unlike Li et al. (2018), we do not use
pre-computed corpus statistics to delete sentiment-

2https://github.com/lijuncen/
Sentiment-and-Style-Transfer

3https://github.com/ekzhu/datasketch

0.0 0.2 0.4 0.6 0.8 1.0
Jaccard similarity

0

10000

20000

30000

40000

Fr
eq

ue
nc

y

Yelp
Amazon

Figure 2: Distribution of Jaccard similarities between
the query and retrieved sentences on the Yelp and Ama-
zon training sets.

specific words/phrases. We simply filter out punc-
tuation when doing indexing and querying. We
do sentence retrieval only in the training/validation
data preparation steps.

In the following, we use the sentence pair in
Figure 1(a) as a running example. The shared to-
kens between the two sentences include {“the”,
“service”, “was”, “great”, “too”}, while the to-
ken appearing only in the retrieved sentence is
“n’t”. We then consider the retrieved sentence as
our noisy source sentence and the query sentence
as our target sentence. Transforming the noisy
source sentence to the target sentence in this ex-
ample involves deleting “n’t” and reordering “too
great”. The sentiment flipping is obtained as a by-
product of this transformation.

A practical issue is that not all the retrieved sen-
tences are as clean as the running example. Fig-
ure 2 shows the distribution of Jaccard similari-
ties between the query and retrieved sentences on
the Yelp and Amazon training sets. Both domains
have a similar characteristic in that the majority of
Jaccard similarities is around 0.3. We observe that
the sentence pairs with Jaccard similarities below
0.3 typically have different sentence meanings, so
we filter them out. On Amazon, we also observe
that the sentence pairs with high Jaccard similari-
ties tend to have neutral sentiment. For example,
given the query sentence “my husband bought this
for me as a christmas present .” from the positive
corpus, we retrieve the sentence “i bought this as
a christmas present for my husband .” from the
negative corpus with Jaccard similarity 0.82. Both
sentences are neutral, which is not useful for learn-
ing sentiment flipping. Thus, on Amazon, we fur-
ther filter out the sentence pairs that have Jaccard
similarities exceeding 0.8.

https://github.com/lijuncen/Sentiment-and-Style-Transfer
https://github.com/lijuncen/Sentiment-and-Style-Transfer
https://github.com/ekzhu/datasketch


6313

Another issue is that not all the non-shared to-
kens reflect sentiment change. For example, given
the sentence “very pleasant atmosphere .”, we re-
trieve the sentence “waitress was n’t very pleas-
ant .” having Jaccard similarity 0.33. The non-
shared tokens in the retrieved sentence include
{“waitress”, “was”, “n’t”}. If we directly use
this retrieved sentence as the source sentence, our
model must learn to delete/add many tokens to
minimize the loss. Unfortunately, the tokens like
“waitress” and “was” convey the content that we
wish to preserve at test time, while only one to-
ken, “n’t”, is useful for sentiment flipping. To
deal with this undesired behavior, we randomly
replace 20% of the non-shared tokens in the re-
trieved sentences having Jaccard similarities less
than 0.5 with the 〈mask〉 token on both domains.
In the example, random replacement of the non-
shared tokens yields “〈mask〉 was n’t very pleas-
ant .”. The sentence corruption process should
alleviate overchanging the content words at test
time. We also apply this technique to learn a sen-
tence reconstructor (detailed in §3.2).

To this end, we can obtain imperfectly aligned
sentences with similar content but different sen-
timent polarities by using our search-and-corrupt
method. Assume we have the original training sets
X={x(1), . . . ,x(M)} and Y={y(1), . . . ,y(N)}.
Each training set belongs to a different senti-
ment polarity. Let y′ and x′ be retrieved, cor-
rupted versions of y and x, respectively. Our uni-
fied training set becomes D={(y′(i),x(i))}Mi=1 ∪
{(x′(i),y(i))}Ni=1.

3.2 Model

Having the training set D in the form of noisy
source and target sentences allows us to extend
the attention-based encoder-decoder model (Bah-
danau et al., 2015) for learning sentiment flipping.
Let enc and dec be an encoder and a decoder pa-
rameterized by θenc and θdec, respectively. Our
model first contains the sentiment loss:

Lsentiment(θenc,θdec) =
1

M

∑M
i=1 − log pdec(x

(i)|enc(y′(i)), sx)+
1

N

∑N
i=1 − log pdec(y

(i)|enc(x′(i)), sy)]. (1)

This loss is the combination of the standard neg-
ative log-likelihood losses. The only new compo-
nents here are the two sentiment embeddings sx
and sy. In practice, we represent them with two

searchx

!x y′

y
ℒcontent ℒsentiment

enc(y′)enc(!x)

dec(enc(!x), sx) dec(enc(y′), sx)

ℒalignment

Figure 3: Workflow of the proposed method. Red ar-
rows denote sentence corruption; blue arrows denote
encoding; green arrows denote decoding.

randomly initialized vectors. We add each of them
to the affine transformation of the hidden layer be-
fore applying the softmax in the decoder. These
sentiment embeddings should help in shifting the
probability distribution over the target tokens.

We then randomly replace 20% of the tokens
in the target sentence with the 〈mask〉 token and
use the corrupted target sentence as its own source
sentence. This process should guide the model to
learn to reconstruct itself from its corrupted ver-
sion (denoted by x̃ and ỹ) and hopefully to retain
the content. We design our content loss as follows:

Lcontent(θenc,θdec) =
1

M

∑M
i=1 − log pdec(x

(i)|enc(x̃(i)), sx)+
1

N

∑N
i=1 − log pdec(y

(i)|enc(ỹ(i)), sy)]. (2)

This loss is in the same form as Eq. (1) except
that here we change the encoder input from the re-
trieved, corrupted sentence to the corrupted target
sentence. Our idea is somewhat analogous to that
of denoising autoencoders (Vincent et al., 2010;
Bengio et al., 2013), in which a Gaussian noise is
added to a continuous-valued input and the model
learns to reconstruct a clean input.

To encourage the model to make more use of the
sentiment embeddings, we use the following loss
as a constraint:

Lalignment(θenc) =
1

M

∑M
i=1||enc(y

′(i))− enc(x̃(i))||22+
1

N

∑N
i=1||enc(x

′(i))− enc(ỹ(i))||22. (3)

This loss consists of the standard mean squared
error (MSE) losses. Here, we want the decoder to
use more information from the sentiment embed-
dings sx and sy to generate the target sentence,



6314

if the representations of the two noisy source sen-
tences produced by the same encoder are not con-
siderably different from each other. Our unified
objective becomes:

min
θenc,θdec

Lsentiment + Lcontent + Lalignment. (4)

Figure 3 shows the workflow of our method
given x as the target sentence. The procedure is
the same when considering y as the target sen-
tence. The sentence retrieval and corruption are
done once in preprocessing.

4 Experiments

4.1 Training details
We implement our model on top of OpenNMT-
py4 (Klein et al., 2017) based on Long Short-Term
Memory (LSTM) (Hochreiter and Schmidhuber,
1997). We use a single-layer bidirectional LSTM
as the encoder and a single-layer unidirectional
LSTM with attention (Bahdanau et al., 2015) and
input-feeding (Luong et al., 2015) as the decoder.
Our network configurations are identical to those
of Li et al. (2018). Specifically, we use 512 hidden
states and 128-dimensional word vectors.

We use Adam (Kingma and Ba, 2015) with the
batch size of 256, the learning rate of 0.001, and
the gradient clipping at 5. The learning rate de-
cays by halves if the validation perplexity does not
decrease. We initialize all model parameters and
word embeddings by sampling from U(−0.1, 0.1).
We train for 20 epochs or until the validation per-
plexity does not decrease for two epochs. At test
time, we use the beam size of 5. We use the
same network and parameter configurations for
all experiments. We conduct all experiments on
NVIDIA Tesla P40 GPUs.

4.2 Evaluation and baselines
Li et al. (2018) created 1000 human references
for each of the Yelp and Amazon test sets us-
ing crowdworkers, allowing us to perform auto-
matic evaluation. Following previous work (Shen
et al., 2017; Li et al., 2018), we use two quanti-
tative evaluation metrics: classification accuracy
and BLEU. Classification accuracy indicates the
percentage of system outputs correctly classified
as the target sentiment by a pre-trained sentiment
classifier. We train our sentiment classifier using
Vowpal Wabbit5 (Agarwal et al., 2014). We use

4https://github.com/OpenNMT/OpenNMT-py
5https://github.com/VowpalWabbit

Model Yelp Amazon
Acc BLEU Acc BLEU

Shen et al. (2017) 74.5 6.79 74.4 1.57
Fu et al. (2018) 46.8 11.24 70.3 7.87
Li et al. (2018) 88.3 12.61 53.4 27.12

This work 88.5 12.13 53.8 15.95
w/o Lsentiment 3.4 24.06 18.2 42.65
w/o Lcontent 86.4 10.08 53.9 14.77
w/o Lalignment 84.7 11.94 51.6 16.51
only Lsentiment 85.4 10.05 53.4 14.76

Table 1: Results on the Yelp and Amazon test sets.

bigram features and train for 20 epochs. BLEU
indicates the content similarity between system
outputs and human references. We compute
a BLEU score using the multi-bleu.perl
script shipped with OpenNMT-py.

We compare our method against three different
baselines. First, Shen et al. (2017)’s cross-aligned
autoencoder learns to directly align the popula-
tions of the transferred sentences from one sen-
timent with the actual sentences from the other.
Second, Fu et al. (2018)’s multi-decoder learns
the sentence representation containing only the
content information and generates the output us-
ing a sentiment-specific decoder. Both Shen
et al. (2017)’s and Fu et al. (2018)’s models are
based on adversarial training (Goodfellow et al.,
2014). Lastly, Li et al. (2018)’s delete-and-retrieve
method generates the output from the content and
the retrieved sentiment-specific phrases with a re-
current neural network.

4.3 Results

Table 1 shows the results of various models. Our
model based on the unified objective of Eq. (4) of-
fers better balanced results compared to its vari-
ants. When removing Lsentiment, our model de-
grades to an input copy-like method, resulting
in low classification accuracies but the highest
BLEU scores. When removing Lcontent, the
BLEU scores drop, indicating that the model can-
not maintain a sufficient number of content words.
Without Lalignment, we observe a reduction in
both accuracy and BLEU on Yelp. However, this
tendency is inconsistent on Amazon (i.e., -2.2
accuracy and +0.56 BLEU). When using only
Lsentiment, our model falls back to the vanilla
encoder-decoder model with a single loss, yield-
ing poorer results on both datasets.

https://github.com/OpenNMT/OpenNMT-py
https://github.com/VowpalWabbit


6315

Yelp Amazon

Source we sit down and we got some really slow and lazy service . this is the worst game i have come across in a long time .

Human the service was quick and responsive this is the best game i have come across in a long time .
Shen et al. (2017) we went down and we were a good , friendly food . this is the best thing i ve had for a few years .
Fu et al. (2018) we sit down and we got some really and fast food . this is the best knife i have no room with a long time .
Li et al. (2018) we got very nice place to sit down and we got some service . this is the best game i have come across in a long time .
This work we sat down and got some great service . this is the best item i have come across in a long time .

(a) From negative to positive

Yelp Amazon

Source my husband got a ruben sandwich , he loved it . i would definitely recommend this for a cute case .

Human my husband got a reuben sandwich, he hated it. i would definitely not recommend this for a cute case .
Shen et al. (2017) my husband got a appetizer sandwich , she was it wrong . i would not recommend this for a long time .
Fu et al. (2018) my husband got a beginning house with however i ignored . i would definitely recommend this for a bra does it .
Li et al. (2018) my husband got a ruben sandwich , it was too dry . i would not recommend this for a cute case .
This work my husband got a turkey sandwich and it was cold . i would not recommend this for a very long time .

(b) From positive to negative

Table 2: Example outputs on the Yelp and Amazon test sets. Sentiment-bearing words/phrases are colored.
Added/changed words are in italics.

4.4 Discussion

Li et al. (2018) have made the outputs of their
methods and those of Shen et al. (2017) and Fu
et al. (2018) publicly available, which allows us
to perform a comparison without much effort. On
Yelp, our unified objective model achieves the best
accuracy and a competitive BLEU score. On Ama-
zon, the adversarial training models of Shen et al.
(2017) and Fu et al. (2018) have high accuracies
but relatively low BLEU scores. These results
indicate that the adversarial training models can
modify the sentence to the target sentiment but fail
to preserve the content.

Compared with Li et al. (2018)’s delete-and-
retrieve method, which seems to reconcile both ac-
curacy and BLEU, our method yields reasonable
results given that it does not retrieve new sentences
at test time and does not explicitly delete/extract
sentiment-specific phrases. Li et al. (2018) also
use a separately trained neural language model to
select the best output (described in the last para-
graph of Sec. 4.4 in their paper), while we only
use a single-trained model. Table 2 shows the ex-
ample outputs of our model and others.

We further examine whether flipping between
sx and sy has any effect at test time. LSTM is
commonly known to be powerful and could gen-
erate the output sentence with the target sentiment
without using the information from the sentiment
embeddings sx and sy. If our model fails to learn
sx and sy, flipping them at test time should have

Source i have been there .
sx i have enjoyed eating there .
sy i have n’t been there .

Source i had experience .
sx i had great experience .
sy worst experience i ever had .

Table 3: Outputs of the proposed model given the same
source sentences with different sentiment embeddings
(sx = positive; sy = negative).

less or no effect. In other words, given the same
input sentence, changing sx to sy and vice versa
would result in nearly the same outputs. To test
this, we compose simple, neutral sentences and
feed them to our unified objective model trained
on Yelp with different sentiment embeddings. Ta-
ble 3 shows the generated outputs, confirming that
our model indeed makes use of sx and sy.

5 Conclusion

We have shown that our unified objective model
successfully learns from the noisy training sen-
tence pairs acquired from non-parallel corpora us-
ing our search-and-corrupt method. Even though
our model is conceptually simpler, it produces
competitive results against the strong baselines
across two domains.

Acknowledgments

We thank the anonymous reviewers of all versions
of this paper for their useful comments.



6316

References
Alekh Agarwal, Olivier Chapelle, Miroslav Dudı́k, and

John Langford. 2014. A reliable effective terascale
linear learning system. J. Mach. Learn. Res., 15(1).

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In Proceedings of
ICLR.

Mayank Bawa, Tyson Condie, and Prasanna Ganesan.
2005. LSH forest: Self-tuning indexes for similarity
search. In Proceedings of WWW.

Yoshua Bengio, Li Yao, Guillaume Alain, and Pas-
cal Vincent. 2013. Generalized denoising auto-
encoders as generative models. In Proceedings of
NIPS.

Andrei Z. Broder. 2000. Identifying and filtering near-
duplicate documents. In Proceedings of the 11th An-
nual Symposium on Combinatorial Pattern Match-
ing.

Zhenxin Fu, Xiaoye Tan, Nanyun Peng, Dongyan
Zhao, and Rui Yan. 2018. Style transfer in text: Ex-
ploration and evaluation. In Proceedings of AAAI.

Pascale Fung and Percy Cheung. 2004. Mining very-
non-parallel corpora: Parallel sentence and lexicon
extraction via bootstrapping and em. In Proceedings
of EMNLP.

Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,
Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. 2014. Generative ad-
versarial nets. In Proceedings of NIPS.

Kelvin Guu, Tatsunori B. Hashimoto, Yonatan Oren,
and Percy Liang. 2018. Generating sentences by
editing prototypes. Transactions of the Association
for Computational Linguistics (TACL).

Geoffrey E. Hinton, Peter Dayan, Brendan J. Frey, and
Radford M. Neal. 1995. The wake-sleep algorithm
for unsupervised neural networks. Science, 268.

Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long
short-term memory. Neural Comput., 9(8).

Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan
Salakhutdinov, and Eric P. Xing. 2017. Controllable
text generation. In Proceedings of ICML.

Diederik P. Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. In Proceedings
of ICLR.

Diederik P. Kingma, Danilo J. Rezende, Shakir Mo-
hamed, and Max Welling. 2014. Semi-supervised
learning with deep generative models. In Proceed-
ings of NIPS.

Guillaume Klein, Yoon Kim, Yuntian Deng, Jean
Senellart, and Alexander M. Rush. 2017. Open-
NMT: Open-source toolkit for neural machine trans-
lation. In Proceedings of ACL.

Guillaume Lample, Sandeep Subramanian,
Eric Michael Smith, Ludovic Denoyer,
Marc’Aurelio Ranzato, and Y-Lan Boureau. 2019.
Multiple-attribute text rewriting. In Proceedings of
ICLR.

Juncen Li, Robin Jia, He He, and Percy Liang. 2018.
Delete, retrieve, generate: A simple approach to sen-
timent and style transfer. In Proceedings of NAACL.

Thang Luong, Hieu Pham, and Christopher D. Man-
ning. 2015. Effective approaches to attention-
based neural machine translation. In Proceedings
of EMNLP.

Dragos Stefan Munteanu and Daniel Marcu. 2005. Im-
proving machine translation performance by exploit-
ing non-parallel corpora. Computational Linguis-
tics, 31(4).

Shrimai Prabhumoye, Yulia Tsvetkov, Ruslan
Salakhutdinov, and Alan W. Black. 2018. Style
transfer through back-translation. In Proceedings of
ACL.

Tianxiao Shen, Tao Lei, Regina Barzilay, and Tommi
Jaakkola. 2017. Style transfer from non-parallel text
by cross-alignment. In Proceedings of NIPS.

Pascal Vincent, Hugo Larochelle, Isabelle Lajoie,
Yoshua Bengio, and Pierre-Antoine Manzagol.
2010. Stacked denoising autoencoders: Learning
useful representations in a deep network with a lo-
cal denoising criterion. J. Mach. Learn. Res., 11.

Jingjing Xu, Xu Sun, Qi Zeng, Xuancheng Ren, Xi-
aodong Zhang, Houfeng Wang, and Wenjie Li. 2018.
Unpaired sentiment-to-sentiment translation: A cy-
cled reinforcement learning approach. In Proceed-
ings of ACL.

Zichao Yang, Zhiting Hu, Chris Dyer, Eric P Xing, and
Taylor Berg-Kirkpatrick. 2018. Unsupervised text
style transfer using language models as discrimina-
tors. In Proceedings of NIPS.

Yi Zhang, Jingjing Xu, Pengcheng Yang, and Xu Sun.
2018a. Learning sentiment memories for sentiment
modification without parallel data. In Proceedings
of EMNLP.

Zhirui Zhang, Shuo Ren, Shujie Liu, Jianyong Wang,
Peng Chen, Mu Li, Ming Zhou, and Enhong Chen.
2018b. Style transfer as unsupervised machine
translation. CoRR, abs/1808.07894.


