



















































Capsule Network with Interactive Attention for Aspect-Level Sentiment Classification


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 5489–5498,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

5489

Capsule Network with Interactive Attention for Aspect-Level
Sentiment Classification

12Chunning Du, 12Haifeng Sun, 12Jingyu Wang∗, 12Qi Qi, 12Jianxin Liao
12Tong Xu, 12Ming Liu

1State Key Laboratory of Networking and Switching Technology,
Beijing University of Posts and Telecommunications, Beijing 100876, China

2EBUPT Information Technology Co., Ltd., Beijing 100191, China
{duchunning,sunhaifeng 1,wangjingyu,qiqi}@ebupt.com

Abstract

Aspect-level sentiment classification is a cru-
cial task for sentiment analysis, which aims
to identify the sentiment polarities of specific
targets in their context. The main challenge
comes from multi-aspect sentences, which ex-
press multiple sentiment polarities towards
different targets, resulting in overlapped fea-
ture representation. However, most existing
neural models tend to utilize static pooling op-
eration or attention mechanism to identify sen-
timental words, which therefore insufficient
for dealing with overlapped features. To solve
this problem, we propose to utilize capsule
network to construct vector-based feature rep-
resentation and cluster features by an EM rout-
ing algorithm. Furthermore, interactive atten-
tion mechanism is introduced in the capsule
routing procedure to model the semantic re-
lationship between aspect terms and context.
The iterative routing also enables encoding
sentence from a global perspective. Experi-
mental results on three datasets show that our
proposed model achieves state-of-the-art per-
formance.

1 Introduction

Aspect-level sentiment classification is a fine-
grained task in the field of sentiment analysis
(Pang and Lee, 2008; Liu, 2012), which aims to
infer the sentiment polarity (e.g., positive, neutral,
negative) of a sentence with respect to the aspect.
It demands to differentiate sentiments towards dif-
ferent targets when there are multiple targets in
one sentence. For example, given the mentioned
aspect term {food, price, drinks}, and the sen-
tence is “The food was definitely good, but when
all was said and done, I just could not justify it
for the price including 2 drinks, $100/person.” For
aspect term food, the sentimental polarity is posi-

∗Corresponding author.

tive, but for aspect term price, the polarity is neg-
ative while for aspect term drinks, the polarity is
neutral. Recently, with the development of deep
learning techniques, various neural networks are
designed for this task and obtain promising results
(Wang et al., 2016a; Ma et al., 2017; Fan et al.,
2018).

The main challenge in aspect-level sentiment
classification is that one sentence expresses mul-
tiple sentiment polarities, resulting in overlapped
feature representation. Take the same example
above, the sentence simultaneously reviews on
‘food’, ‘price’, and ‘drinks’, and expresses three
different sentiment polarities. The highly over-
lapped features will confuse the classifier seri-
ously. However, most existing methods only keep
the most active feature by max-pooling operation
or utilize attention mechanism to find the senti-
mental words, which fails to distinguish the over-
lapped features.

Therefore, we propose a novel capsule net-
work and iterative EM routing method with in-
teractive attention (IACapsNet) to solve this prob-
lem. Capsule network (Hinton et al., 2011; Sabour
et al., 2017; Hinton et al., 2018) constructs vector-
based feature representation. Capsules in adjacent
layers are connected by dynamic routing, which
shows strengths in distinguishing overlapped fea-
tures by feature clustering (Sabour et al., 2017;
Zhang et al., 2019). In the aspect-level sentimen-
tal classification task, the vector-based overlapped
sentimental features towards different aspect terms
will be clustered by an Expectation-Maximization
(EM) routing algorithm, which makes the subse-
quent classification more clear. Furthermore, we
further devise an interactive attention-based rout-
ing mechanism in order to highlight the word-level
difference and model the semantic relationship be-
tween aspect terms and context.

Moreover, our iterative routing mechanism can



5490

be viewed as a top-down attention mechanism,
which is more efficient because of the global per-
spective compared to the standard attention mech-
anism. Standard attention mechanism in this task
only considers a part of the context information in
a sentence without considering the overall mean-
ing conveyed by the sentence, which may intro-
duce noise and downgrade the prediction accuracy,
especially for complex sentences. For example,
in an ironic statement for the aspect term “mac
os”: “Maybe the mac os improvement were not
the product they want to offer.”, the standard at-
tention mechanism will highlight the sentimental
word ‘improvement’ and confuse the classifier to
make the wrong prediction to be positive. Our
routing mechanism can tackle this by adjusting
the contribution of each low-level capsule based
on the high-level capsules (overall representation),
and the iterative update makes the overall repre-
sentation more accurate compared to other simi-
lar top-down attention (Liu et al., 2018; Zhao and
Zhang, 2018).

Our proposed model (IACapsNet) is evaluated
on three datasets: laptop, restaurant datasets from
the SemEval 2014 Task 4 and Twitter collection.
The experimental results show that our model
outperforms other baseline methods and achieves
state-of-the-art performance. Our contributions
are summarized as follows:

• We apply capsule network to aspect-level
sentiment classification to tackle the over-
lapped features by feature clustering. To the
best of our knowledge, there is no work that
investigates the performance of capsule net-
work in this task.

• An interactive attention mechanism is intro-
duced in the capsule routing to help model
the semantic relationship between aspect
term and context.

2 Related work

2.1 Aspect Level Sentiment Classification
Traditional approaches have designed rich features
about content and syntactic structures to capture
the sentiment polarity (Jiang et al., 2011; Pérez-
Rosas et al., 2012). However, These feature-
based methods are labor-intensive and the perfor-
mance highly depends on the quality of the fea-
tures. Recently, deep learning methods are be-
coming popular for aspect-level sentiment classi-

fication. Recurrent Neural Networks (RNNs) are
the most commonly used technique for this task
(Tang et al., 2016a). The attention mechanism
is further introduced to model the target-context
association (Wang et al., 2016b; Li et al., 2017;
Ma et al., 2017). Furthermore, Fan et al. (2018)
proposed MGAN to integrate fine-grained atten-
tion mechanisms, which is employed to character-
ize the word-level interactions between aspect and
context words. Very recently, CNN-based models
have shown the strengths in efficiency to tackle the
aspect-level sentiment classification (Xue and Li,
2018; Huang and Carley, 2018; Li et al., 2018).
However, all the previous methods utilize static
pooling operation or attention mechanism to lo-
cate the sentimental words, which fails to handle
the overlapped features. We introduces vector-
based feature representation and feature clustering
to address this.

2.2 Capsule Network

Capsule network was proposed to improve the rep-
resentational limitations of CNN and RNN by ex-
tracting features in the form of vectors. The tech-
nique was firstly proposed in (Hinton et al., 2011)
and improved in (Sabour et al., 2017; Hinton et al.,
2018), which is mainly devised for image process-
ing domain. Introducing capsules allows us to uti-
lize a routing mechanism instead of pooling op-
eration to generate high-level features which is a
more efficient way for features encoding. Routing-
by-agreement is able to cluster features in an itera-
tive way, which achieved impressive performance
recognizing highly overlapped digits.

Several types of capsule networks have been
proposed for natural language processing. Yang
et al. (2018) investigated capsule networks for
text classification. They also found that capsule
networks exhibit significant improvement when
transferring single-label to multi-label text classi-
fication. Similar property has also been observed
in the task of relation extraction (Zhang et al.,
2019, 2018). However, interactive word-level at-
tention is not considered in these typical capsule
routing methods.

3 Model

In this section, we describe the proposed capsule
network with interactive attention (IACapsNet) in
details. The aim of aspect-level sentiment clas-
sification is to predict the sentiment class y of a



5491

sentence over a specific aspect term, where y ∈
{positive, negative, neutral}. The overall archi-
tecture is shown in Figure 1. It consists of the
input embedding layer, bidirectional RNN layer,
primary capsule layer, and output layer.

3.1 Input Embedding Layer

The context’s input representations of IACapsNet
include word embeddingswn and position embed-
dings pn. The aspect term’s input representation
only consists of word embedding wan.

Word embedding is a distributed representa-
tion of a word, where words from the vocabulary
are mapped to vectors. Initializing words vectors
via pre-trained word vectors can improve the per-
formance due to their ability to capture syntac-
tic and semantic information of words from large
scale unlabeled text. In our model, we employ
the pre-trained word vector GloVe (Pennington
et al., 2014) to obtain the fixed word embedding
wn, w

a
n ∈ Rdw , where dw is the word vector di-

mension.
Considering that the context words with closer

distance to an aspect may have higher influence
on the sentiment analysis, we introduce position
embedding to encode the relative distance rn from
word wn to the aspect term. We define the po-
sition embedding matrix P ∈ Rdp×N , which is
randomly initialized and updated during the train-
ing process. Here, dp is the position embedding
dimension and N denotes the length of the sen-
tence. The corresponding word’s position embed-
dings pn can be obtained by looking up the posi-
tion embedding matrix P using rn.

The input representation for each context word
is the concatenation of word embeddings and po-
sition embeddings: xn = [wn; pn] ∈ Rdw+dp .

3.2 Bidirectional Recurrent Networks Layer

The recurrent neural networks can capture long-
distance dependencies within a sentence. A bidi-
rectional recurrent neural network is the first layer
of IACapsNet. The forward direction captures the
left context hl for a word and the backward direc-
tion captures the right context hr. We concatenate
the left context and the right context as the con-
textualized word representation hcn, h

a
n ∈ R2×dl ,

where dl is the dimension of hidden state, ha and
hc are the word representations for aspect term and
context, respectively.

Context

Bi-RNN

… … … …

… … … …

M a

Aspect

Bi-RNN

… …

… …

…

pool

…

pool

Interactive 
Attention

Primary 
capsules

Iterative Routing

Sentiment Classes

Ω"

Ω"#$

%$& %'&

%$ %( %) %'

*$ *( *) *'

ℎ$, ℎ(, ℎ), ℎ', ℎ$& ℎ'&

Figure 1: The architecture of IACapsNet

3.3 Primary Capsule Layer

The primary capsule is a group of neurons ob-
tained from the output of the convolutional oper-
ation performed on han and h

c
n. So, the output of

capsule is a vector representing different proper-
ties of the same objective. In aspect-level senti-
ment classification task, the properties may con-
tain the sentiment and aspect term features.

EM-based routing method (Hinton et al., 2018)
is implemented in our model, and except for the
high-dimensional output M , there is one more ac-
tivation probability a in our capsule, which is like
the activity in a standard neural net (shown in Fig-
ure 1).

3.4 Interactive Attention EM Routing

We have already decided on the outputs of all the
capsules ΩL in primary capsule layer and we now
want to decide which capsules ΩL+1 to active in
the layer above and how to assign each active low-
level capsule to one active higher-level capsule.

The vector-based features get clustered in the
high-level capsules by an EM based algorithm
where the outputs of high-level capsules play the
role of Gaussians and the output vectors of low-
level capsules play the role of the datapoints. The
means, variances, and activation probabilities of
the output capsules, as well as the assignment
probabilitiesR of the input capsules are iteratively
updated by alternating between an E-step and an
M-step. It can also be viewed as a parallel atten-



5492

tion mechanism in the opposite direction, which
can adjust the low-level word’s contribution based
on the sentence global representation. Moreover,
in order to model the semantic relationship be-
tween aspect term and context, we further devise
an interactive attention-based routing mechanism.

3.4.1 M-step
The M-step holds the assignment probabilities R
constant and adjusts each Gaussian (i.e. high-level
capsules) to maximize the sum of the weighted log
probabilities that the Gaussian would generate the
datapoints (i.e. low-level capsules) assigned to it.
This procedure aims to obtain the overall repre-
sentation among the low-level capsules for a given
iteration.

Firstly, every primary capsule i is transformed
by Wij to cast a vote Vij = MiWij for the output
of high-level capsule j. And we can get the mean
µj of the votes from the input capsules and the
variance σj about that mean for each dimension h:

µhj =

∑
iRijV

h
ij∑

iRij
, (1)

(σhj )
2 =

∑
iRij(V

h
ij − µhj )2∑
iRij

, (2)

where µhj is the h
th component of the capsule j’s

vectorized output Mj .
The activation probability of capsule j is calcu-

lated by

costhj = (βu + log(σ
h
j ))

∑
i

Rij , (3)

aj = sigmoid(λ(βα −
∑
h

costhj )), (4)

where βu and βα are trainable parameters denoting
fixed cost per input capsule when not activating it
and fixed cost for coding the mean and variance of
capsule j when activating it. The variance σ re-
flects the degree of agreement. An intuitive under-
standing of this activation probability is that if the
votes from low-level capsules are not agreed on
one high-level capsule, the activation of the high-
level capsule should be low. λ is an inverse tem-
perature parameter set 1e−3 with a fixed schedule.

3.4.2 E-step
The E-step adjusts the assignment probabilities R
for each datapoint (i.e. low-level capsule) to the

Gaussian (i.e. high-level capsules). This proce-
dure aims to adjust the contribution of each cap-
sule based on the high-level capsule (i.e. overall
representation) for a given iteration.

We firstly compute the negative log probability
density of the vectorized vote under the j’s Gaus-
sian distribution:

pj =
1√∏H

h 2π(σ
h
j )

2
exp(−

H∑
h

(V hij − µhj )2

2(σhj )
2

).

(5)
For capsule i in primary capsule layer, the assign-
ment probability is adjusted by:

Rij =
ajpj∑
j ajpj

. (6)

Alternating E-step and M-step will route the
output of capsule to a capsule in the layer above
that receives a cluster of high-dimensional fea-
tures.

Algorithm 1 Interactive attention EM routing.
Capsule i and j denote a low-level and high-level
capsule. ΩL and ΩL+1 denote the low-level and
high-level capsules set, respectively.

1: procedure INTERACTIVE ATTENTION EM
ROUTING(ai, Vij ,αa→c,αc→a)

2: ∀ capsules i and capsules j: Rij = 1|ΩL+1|
3: ∀ capsules i from context: ai = ai ∗αa→cn
4: ∀ capsules i from aspect: ai = ai ∗ αc→an
5: n is the token index from which the cap-

sule i comes.
6: for r iterations do
7: ∀j ∈ ΩL+1: M-STEP(ai, Rij ,Vij ,j)
8: ∀i ∈ ΩL: E-STEP(µj , σj ,aj ,Vij ,i)
9: end for

10: return aj
11: end procedure
12: procedure M-STEP(ai,Rij ,Vij)
13: ∀ capsule i: Rij = Rij ∗ ai
14: ∀ capsule j: compute µj and σj by Eq. 1

and 2
15: ∀ capsule j: compute aj by Eq. 4
16: end procedure
17: procedure E-STEP(µj ,σj ,aj ,Vij)
18: ∀ capsule j: compute pj and update Rij

by Eq. 5 and 6
19: end procedure



5493

3.4.3 Interactive Attention
The overlapped features in primary capsule layer
can be routed and clustered to high-level cap-
sules in an iterative way. However, purely alter-
nating the E-step and M-step will ignore the re-
lationship between the context and aspect term.
It has been demonstrated that target and context
can determine the representation of each other.
And the coordination of targets and their contexts
can remarkably enhance the performance of sen-
timent classification (Ma et al., 2017; Fan et al.,
2018). Word-level attention, which has already
been demonstrated to be essential is also ignored.
So, apart from the assignment probabilities, we
further introduce an interactive attention weight α
which is learned interactively between the context
and the aspect term.

Specifically, we implement scaled dot-product
attention which can be described as mapping a
query and a set of key-value pairs to a weight on
the word-level token. The queries are the aver-
aged representation of the context hc and the as-
pect term ha which are transformed to dimension
dk by trainable parameters:

qc =

∑N
n h

c
n

N
∗W qc ∈ Rdk , (7)

qa =

∑N
n h

a
n

N
∗W qa ∈ Rdk , (8)

The keys are the corresponding words’ represen-
tation from aspect term and context which are also
transformed to dimension dk:

kcn = h
c
n ∗W kc ∈ Rdk , (9)

kan = h
a
n ∗W ka ∈ Rdk , (10)

where W qc , W
q
a , W kc , W

k
a ∈ R2dl∗dk

The attention weights can be computed as fol-
lows:

sa→cn (qa, k
c
n) =

kcn ∗ qTa√
dk

, (11)

sc→an (qc, k
a
n) =

kan ∗ qTc√
dk

, (12)

αa→cn =
exp(sa→cn (qa, k

c
n))∑N

n=1 exp(s
a→c
n (qa, k

c
n))

, (13)

αc→an =
exp(sc→an (qc, k

a
n))∑N

n=1 exp(s
c→a
n (qc, k

a
n))

. (14)

In order to ensure a proper magnitude of sn
avoiding pushing the softmax function into re-
gions where it has extremely small gradients,
we introduce the scaling factor 1√

dk
following

(Vaswani et al., 2017). We can thus get the word-
level significance for each token in the context
and aspect term in an interactive way, which will
be adopted on each primary capsule’s activation
probability a. We detail the whole routing algo-
rithm in Algorithm 1.

3.5 Training Objective

In IACapsNet, each top-level capsule corresponds
to a sentiment category. The activation probability
a of each top-level capsule represents the proba-
bility that the input sentence belongs to the corre-
sponding category. We use a spread margin loss,
Lk for each top-level capsule k to directly maxi-
mize the gap between the activation of the target
class (at) and the activation of the other classes.
The total loss L is simply the sum of the losses of
all top-level capsules:

L =
∑
k 6=t

(max(0,m− (at − ak))2, (15)

where m is the margin which we set 0.9 with a
fixed schedule.

4 Experiments

In this section, we conduct extensive experiments
on three datasets to evaluate the proposed IACap-
sNet.

4.1 Experimental Setup

The experiments are implemented on three
datasets. The first two datasets are from the Se-
mEval 2014 Task 4 (Pontiki et al., 2014), which
contains reviews about laptops and restaurants, re-
spectively. The third one is a Twitter dataset col-
lected by (Dong et al., 2014). The statistics of
these datasets are listed in Table 2. Following
(Tang et al., 2016c), conflict category is removed
from the SemEval 2014 datasets to avoid datasets
getting unbalanced. Sentences are zero-padded to
the length of the longest sentence in respective
dataset. Results are measured by accuracy and
Macro-Averaged F1 score.

In our experiments, the pre-trained GloVe (Pen-
nington et al., 2014) is used to initialize the word
embeddings from context and aspect term. The



5494

Model Laptop Restaurant Twitter
ACC Macro-F1 ACC Macro-F1 ACC Macro-F1

ATAE-LSTM (Wang et al., 2016a) 69.27 - 78.50 - 69.88 -
TD-LSTM (Tang et al., 2016b) 68.83 68.43 78.00 66.73 66.62 64.01
MemNet (Tang et al., 2016c) 72.37 - 80.32 - 68.50 66.91
IAN (Ma et al., 2017) 72.10 - 78.60 - - -
RAM (Chen et al., 2017) 75.01 70.51 79.79 68.86 71.88 70.33
BILSTM-ATT-G 73.12 69.80 79.73 69.25 70.38 68.37
MGAN (Fan et al., 2018) 75.39 72.47 81.25 71.94 72.54 70.81
TNet (Li et al., 2018) 76.54 71.75 80.79 71.27 74.97 73.60
PBAN (Gu et al., 2018) 74.12 - 81.16 - - -
Cabasc (Liu et al., 2018) 75.07 - 80.89 - 71.53 -

IACapsNet 76.80 73.29 81.79 73.40 75.01 73.81

Table 1: Experimental results. The baseline results are retrieved from the original papers and (Li et al., 2018)

out-of-vocabulary words are initialized by sam-
pling from the uniform distribution U(−0.1, 0.1).
In the Bi-RNN layer, LSTM is utilized and the di-
mension of the hidden state is 300. The dimension
of the capsule’s output is 16. We use Adam opti-
mizer (Kingma and Ba, 2014) as our optimization
method with 5e−4 learning rate. L2 regularization
and dropout are adopted to avoid overfitting.

Dataset
Positive Neural Negative

Train Test Train Test Train Test
Restaurant 2164 728 637 196 807 196

Laptop 994 341 464 169 870 128
Twitter 1561 173 3127 346 1560 173

Table 2: Summary statistics of the datasets.

4.2 Model Comparisons

IACapsNet is compared with the following meth-
ods:

ATAE-LSTM(Wang et al., 2016a): An LSTM-
based model which learns attention embeddings
and combine them with the LSTM hidden states
to predict the polarity.

TD-LSTM (Tang et al., 2016b): It employs two
LSTMs to estimate the left context and the right
context, respectively. The concatenated context
representations perform the predictions.

IAN (Ma et al., 2017): An interactive attention
is implemented on the representation of context
and aspect learned by two LSTMs.

MemNet (Tang et al., 2016c): It applies atten-
tion mechanism over the word embeddings multi-

ple times and predicts sentiment based on the top-
most sentence representation.

RAM (Chen et al., 2017): Similar to Mem-
Net, RAM is a multi-layer architecture where each
layer consists of attention-based aggregation of
word features and a GRU cell to learn the sentence
representation.

BILSTM-ATT-G: (Zhang and Liu, 2017): It
models left and right contexts using two attention-
based LSTMs and introduces gates to measure the
importance of left context, right context, and the
entire sentence for the prediction.

MGAN (Fan et al., 2018): MGAN leverages the
fine-grained and coarse-grained attention, which
is further employed to characterize the word-level
interactions between aspect and context words.

PBAN (Gu et al., 2018): PBAN concentrates on
the position information of aspect terms and mu-
tually models the relation between aspect term and
sentence by employing bidirectional attention.

TNet (Li et al., 2018): It employs a CNN
layer to extract salient features from the trans-
formed word representations originated from a bi-
directional RNN layer.

Cabasc (Liu et al., 2018): Cabasc employs
sentence-level content attention mechanism to
capture the important information about given as-
pects from a global perspective.

4.3 Main Results

As shown in Table 1, IACapsNet achieves the best
performance on all the datasets. From Table 1, we
can have the following observations.

ATAE-LSTM performs better than TD-LSTM.



5495

One main reason may be the attention mechanism
in TD-LSTM that enables to notice the important
parts based on the aspect term. BILSTM-ATT-
G adopts a similar architecture with TD-LSTM
by modeling left context and right context us-
ing attention-based LSTM, which achieves better
results than ATAE-LSTM. IAN and MGAN in-
troduce the interactive attention in coarse-grained
and multi-grained ways respectively and bring re-
markable improvements. PBAN similarly utilize
a fine-grained bidirectional attention and performs
comparably with MGAN.

MemNet utilizes a more complex structure that
contains nine computational layers, which updates
the query vector at each hop. RAM also learns
multiple attended vectors on the memory, which
achieves superior results among the baseline mod-
els, especially on Laptop dataset.

Our proposed IACapsNet consistently performs
best on all the three datasets. The improvement is
mainly attributed to the feature clustering ability to
tackle the overlapped features and the iteratively
updating on coupling coefficients, which consid-
ers the overall meaning of the contexts. More-
over, compared with Cabasc, which also incorpo-
rates the overall representation to typical attention
mechanism in a static way, our iterative method
shows remarkable strengths.

4.4 Ablation Study

To analyze the effect of different components in-
cluding the routing mechanism and the introduced
interactive attention, we report the results of vari-
ants of IACapsNet. The results in Table 3 indi-
cate: (1) EM routing based IACapsNet outper-
forms IACapsNet-Cosine, which routes capsules
by cosine similarity (Sabour et al., 2017). One
main reason maybe cosine saturates at 1, which is
insensitive to the difference between a quite good
agreement and a very good agreement. (2) Inte-
grating interactive attention in routing mechanism
brings a remarkable improvement on both routing
mechanisms, which demonstrates the necessity to
consider the relationship between the aspect and
contexts during the routing procedure.

Moreover, EM routing also brings a boost on ef-
ficiency with fewer trainable parameters and faster
speed which is intuitively shown in Table 4 (IAN
is listed as baseline). From the table, it is easy to
conclude that IACapsNet is much more efficient
than IACapsNet-Cosine with about 10% and 36%

decrease in the number of trainable parameters and
running speed, respectively. Moreover, compared
to IAN, IACapsNet achieves a much better accu-
racy with fewer trainable parameters, meaning that
capsule network is more efficient in feature encod-
ing with fewer parameters. However, IACapsNet
costs more time compared to IAN, which is the
implicit deficiency of capsule network because of
the iterative calculation during routing.

4.5 Effects of Routing Iteration Number
As our proposed IACapsNet involves iterative pro-
cedure during routing. In this section, we investi-
gate the effects of different routing iteration num-
bers. Specifically, we conduct experiments on all
the three datasets and vary routing iteration num-
bers r from 1 to 4. The results are illustrated in
Figure 3.

The results show that IACapsNet achieves the
best performance at routing iteration number 2, 3
and 3 on the dataset RESTAURANT, LAPTOP,
and Twitter, respectively. When the number of
iteration is 1, our capsule network degrades to a
standard network, which obtains comparable re-
sults with IAN. While increasing r to 4, the per-
formance gets worse dramatically. Moreover, as
the number of iteration increases to 4, it brings
many difficulties to train IACapsNet. The model
becomes more sensitive, which fluctuates greatly
in loss and accuracy during training. Therefore, it
is appropriate to limit the routing iteration number
r and set it to be 2 or 3 depending on the perfor-
mance.

Model Accuracy Macro-F1
IACapsNet-cosine routing 81.12 72.05
-w/o interactive attention 80.64 71.76
IACapsNet-EM routing 81.79 73.40
-w/o interactive attention 80.89 71.84

Table 3: Ablation study on Restaurant dataset. Cosine
routing denotes routing measured by cosine similarity
(Sabour et al., 2017), and w/o means without.

4.6 Case Study
In order to assess the effect of our EM routing with
interactive attention mechanism, we visualize the
coupling coefficients. Our model is able to adjust
the contribution of each part based on the global
meaning of a sentence and shows superiority in
modeling complicated sentence. In this section,
we pick an example from RESTAURANT dataset



5496

the foo
d

wa
s

de
fin

ite
ly

go
od , bu

t
wh

en all wa
s

sai
d

an
d

do
ne , I jus

t
co

uld n'
t
jus

tfy it , for the
 
pri

ce
-lr

b-'

inc
lud

ing 2
dri

nk
s , $

10
0/p

ers
on

0.00

0.02

0.04

0.06  IAN (F)
 IACapsNet (T)

-0.02

-0.01

0.00

0.01
Target: prices (Negative)

0.00

0.02

0.04

0.06
 IAN (F)
 IACapsNet (T)

C
ou

pl
in

g 
co

ef
fic

ie
nt

s

Target: drinks (Neutral)

-0.02

-0.01

0.00

0.01

0.02
0.00

0.02

0.04

0.06  IAN (T)
 IACapsNet (T)

-0.010

-0.005

0.000

0.005

0.010Target: food (Positive)

Figure 2: Routing Visualization

Model Para. Train Infer.
IAN (Ma et al., 2017) 5.057 0.11 0.017
IACapsNet-cosine routing 4.614 0.33 0.044
IACapsNet 4.189 0.21 0.024

Table 4: Evaluation of efficiency. “Para.” denotes
the number of trainable parameters (M). “Train” and
“Infer.” denotes the training speed and inference
speed(second/step). Speed is measured on 1 NVIDIA
p100 GPU with batch size 64.

consisting of a long and complicated sentence and
3 aspect terms with 3 different sentimental polari-
ties. Figure 2 shows this example and the visual-
ization results of word-level coupling coefficients
which are the sum of one word’s coupling coeffi-
cients to the category. The attention weights from
IAN are also shown as a baseline, which is nor-
malized to the same scale with routing coupling
coefficients. The line in the chart reflects the dif-
ference ∆. ‘F’ in Figure 2 means false sentiment
classification, and ‘T’ means correct classification.

From the figure, we can observe that our rout-
ing methods can adjust the attended words accord-
ing to different aspect terms, which helps make all
the predictions correctly. Moreover, our routing
method can locate on the more important words
more efficiently. For example, in terms of the as-
pect ‘price’, the words “n’t” and “justify” is at-
tended, which are the corresponding essential sen-

Figure 3: Effects of Routing Iteration Number

timental words. However, they are ignored by
the ordinary attention mechanism, which leads a
wrong prediction. This shows that our routing
mechanism can capture important parts of a sen-
tence more accurately.

4.7 Conclusion and Future Work

We re-examine the deficiencies of existing models
with attention mechanism for aspect-level senti-
ment classification. And we propose to utilize cap-
sule network to handle the overlapped sentiment
features by features clustering, and iteratively ad-
just the attention weights from a global perspec-
tive. To the best of our knowledge, capsule net-
work is firstly applied in this task. Moreover, inter-
active attention is introduced to the dynamic rout-



5497

ing to model the semantic relationship between as-
pect term and sentence. The experimental results
verify that IACapsNet outperforms baseline mod-
els. The ablation and case studies show the effi-
cacy of different proposed modules.

In the future, our theory can be generalized to
other tasks that highly depends on the attention
mechanism. For example, reading comprehension
and machine translating.

Acknowledgements

This work was jointly supported by: (1) Na-
tional Natural Science Foundation of China
(No. 61771068, 61671079, 61471063, 61372120,
61421061); (2) Beijing Municipal Natural Sci-
ence Foundation (No.4182041, 4152039); (3)
the National Basic Research Program of China
(No. 2013CB329102); (4) Fundamental Research
Funds for the Central Universities under Grant
2018RC20; (5) BUPT Excellent Ph.D. Students
Foundation.

References

Peng Chen, Zhongqian Sun, Lidong Bing, and Wei
Yang. 2017. Recurrent attention network on mem-
ory for aspect sentiment analysis. In Proceedings of
the 2017 Conference on Empirical Methods in Nat-
ural Language Processing, EMNLP 2017, Copen-
hagen, Denmark, September 9-11, 2017.

Li Dong, Furu Wei, Chuanqi Tan, Duyu Tang, Ming
Zhou, and Ke Xu. 2014. Adaptive recursive neural
network for target-dependent twitter sentiment clas-
sification. In Proceedings of the 52nd Annual Meet-
ing of the Association for Computational Linguis-
tics, ACL 2014, June 22-27, 2014, Baltimore, MD,
USA, Volume 2: Short Papers.

Feifan Fan, Yansong Feng, and Dongyan Zhao. 2018.
Multi-grained attention network for aspect-level
sentiment classification. In Proceedings of the 2018
Conference on Empirical Methods in Natural Lan-
guage Processing, Brussels, Belgium, October 31 -
November 4, 2018.

Shuqin Gu, Lipeng Zhang, Yuexian Hou, and Yin
Song. 2018. A position-aware bidirectional atten-
tion network for aspect-level sentiment analysis. In
Proceedings of the 27th International Conference
on Computational Linguistics, COLING 2018, Santa
Fe, New Mexico, USA, August 20-26, 2018.

Geoffrey Hinton, Sara Sabour, and Nicholas Frosst.
2018. Matrix capsules with em routing. In ICLR
2018.

Geoffrey E. Hinton, Alex Krizhevsky, and Sida D.
Wang. 2011. Transforming auto-encoders. In Ar-
tificial Neural Networks and Machine Learning -
ICANN 2011 - 21st International Conference on Ar-
tificial Neural Networks, Espoo, Finland, June 14-
17, 2011, Proceedings, Part I.

Binxuan Huang and Kathleen M. Carley. 2018. Param-
eterized convolutional neural networks for aspect
level sentiment classification. In Proceedings of the
2018 Conference on Empirical Methods in Natural
Language Processing, Brussels, Belgium, October
31 - November 4, 2018.

Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and
Tiejun Zhao. 2011. Target-dependent twitter sen-
timent classification. In The 49th Annual Meeting
of the Association for Computational Linguistics:
Human Language Technologies, Proceedings of the
Conference, 19-24 June, 2011, Portland, Oregon,
USA.

Diederik P. Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. ICLR.

Cheng Li, Xiaoxiao Guo, and Qiaozhu Mei. 2017.
Deep memory networks for attitude identification.
In Proceedings of the Tenth ACM International Con-
ference on Web Search and Data Mining, WSDM
2017, Cambridge, United Kingdom, February 6-10,
2017.

Xin Li, Lidong Bing, Wai Lam, and Bei Shi. 2018.
Transformation networks for target-oriented senti-
ment classification. In Proceedings of the 56th An-
nual Meeting of the Association for Computational
Linguistics, ACL 2018, Melbourne, Australia, July
15-20, 2018, Volume 1: Long Papers.

Bing Liu. 2012. Sentiment Analysis and Opinion Min-
ing. Synthesis Lectures on Human Language Tech-
nologies. Morgan & Claypool Publishers.

Qiao Liu, Haibin Zhang, Yifu Zeng, Ziqi Huang, and
Zufeng Wu. 2018. Content attention model for as-
pect based sentiment analysis. In Proceedings of the
2018 World Wide Web Conference on World Wide
Web, WWW 2018, Lyon, France, April 23-27, 2018.

Dehong Ma, Sujian Li, Xiaodong Zhang, and Houfeng
Wang. 2017. Interactive attention networks for
aspect-level sentiment classification. In Proceed-
ings of the Twenty-Sixth International Joint Con-
ference on Artificial Intelligence, IJCAI 2017, Mel-
bourne, Australia, August 19-25, 2017.

Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in In-
formation Retrieval.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. In Proceedings of the 2014
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP 2014, October 25-29,
2014, Doha, Qatar, A meeting of SIGDAT, a Special
Interest Group of the ACL.



5498

Verónica Pérez-Rosas, Carmen Banea, and Rada Mi-
halcea. 2012. Learning sentiment lexicons in span-
ish. In Proceedings of the Eighth International
Conference on Language Resources and Evaluation,
LREC 2012, Istanbul, Turkey, May 23-25, 2012.

Maria Pontiki, Dimitris Galanis, John Pavlopoulos,
Harris Papageorgiou, Ion Androutsopoulos, and
Suresh Manandhar. 2014. Semeval-2014 task 4: As-
pect based sentiment analysis. In Proceedings of the
8th International Workshop on Semantic Evaluation,
SemEval@COLING 2014, Dublin, Ireland, August
23-24, 2014.

Sara Sabour, Nicholas Frosst, and Geoffrey E. Hinton.
2017. Dynamic routing between capsules. In Ad-
vances in Neural Information Processing Systems
30: Annual Conference on Neural Information Pro-
cessing Systems 2017, 4-9 December 2017, Long
Beach, CA, USA.

Duyu Tang, Bing Qin, Xiaocheng Feng, and Ting Liu.
2016a. Effective lstms for target-dependent senti-
ment classification. In COLING 2016, 26th Inter-
national Conference on Computational Linguistics,
Proceedings of the Conference: Technical Papers,
December 11-16, 2016, Osaka, Japan.

Duyu Tang, Bing Qin, Xiaocheng Feng, and Ting Liu.
2016b. Effective lstms for target-dependent senti-
ment classification. In COLING 2016, 26th Inter-
national Conference on Computational Linguistics,
Proceedings of the Conference: Technical Papers,
December 11-16, 2016, Osaka, Japan.

Duyu Tang, Bing Qin, and Ting Liu. 2016c. Aspect
level sentiment classification with deep memory net-
work. In Proceedings of the 2016 Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP 2016, Austin, Texas, USA, November
1-4, 2016.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems 30: Annual Conference on Neural
Information Processing Systems 2017, 4-9 Decem-
ber 2017, Long Beach, CA, USA.

Yequan Wang, Minlie Huang, Xiaoyan Zhu, and
Li Zhao. 2016a. Attention-based LSTM for aspect-
level sentiment classification. In Proceedings of the
2016 Conference on Empirical Methods in Natural
Language Processing, EMNLP 2016, Austin, Texas,
USA, November 1-4, 2016.

Yequan Wang, Minlie Huang, Xiaoyan Zhu, and
Li Zhao. 2016b. Attention-based LSTM for aspect-
level sentiment classification. In Proceedings of the
2016 Conference on Empirical Methods in Natural
Language Processing, EMNLP 2016, Austin, Texas,
USA, November 1-4, 2016.

Wei Xue and Tao Li. 2018. Aspect based sentiment
analysis with gated convolutional networks. In Pro-
ceedings of the 56th Annual Meeting of the Associa-
tion for Computational Linguistics, ACL 2018, Mel-
bourne, Australia, July 15-20, 2018, Volume 1: Long
Papers.

Min Yang, Wei Zhao, Jianbo Ye, Zeyang Lei, Zhou
Zhao, and Soufei Zhang. 2018. Investigating cap-
sule networks with dynamic routing for text classifi-
cation. In Proceedings of the 2018 Conference on
Empirical Methods in Natural Language Process-
ing, Brussels, Belgium, October 31 - November 4,
2018.

Ningyu Zhang, Shumin Deng, Zhanling Sun, Xi Chen,
Wei Zhang, and Huajun Chen. 2018. Attention-
based capsule network with dynamic routing for re-
lation extraction. In Proceedings of the 2018 Con-
ference on Empirical Methods in Natural Language
Processing, Brussels, Belgium, October 31 - Novem-
ber 4, 2018.

Xinsong Zhang, Pengshuai Li, Weijia Jia, and Hai
Zhao. 2019. Multi-labeled relation extraction with
attentive capsule network. AAAI.

Yue Zhang and Jiangming Liu. 2017. Attention mod-
eling for targeted sentiment. In Proceedings of the
15th Conference of the European Chapter of the
Association for Computational Linguistics, EACL
2017, Valencia, Spain, April 3-7, 2017, Volume 2:
Short Papers.

Shenjian Zhao and Zhihua Zhang. 2018. Attention-via-
attention neural machine translation. In Proceed-
ings of the Thirty-Second AAAI Conference on Ar-
tificial Intelligence, (AAAI-18), the 30th innovative
Applications of Artificial Intelligence (IAAI-18), and
the 8th AAAI Symposium on Educational Advances
in Artificial Intelligence (EAAI-18), New Orleans,
Louisiana, USA, February 2-7, 2018.


