



















































Unsupervised Learning of Prototypical Fillers for Implicit Semantic Role Labeling


Proceedings of NAACL-HLT 2016, pages 1473–1479,
San Diego, California, June 12-17, 2016. c©2016 Association for Computational Linguistics

Unsupervised Learning of Prototypical Fillers
for Implicit Semantic Role Labeling

Niko Schenk and Christian Chiarcos
Applied Computational Linguistics Lab

Goethe University Frankfurt am Main, Germany
{n.schenk,chiarcos}@em.uni-frankfurt.de

Abstract

Gold annotations for supervised implicit se-
mantic role labeling are extremely sparse and
costly. As a lightweight alternative, this paper
describes an approach based on unsupervised
parsing which can do without iSRL-specific
training data: We induce prototypical roles
from large amounts of explicit SRL annota-
tions paired with their distributed word repre-
sentations. An evaluation shows competitive
performance with supervised methods on the
SemEval 2010 data, and our method can eas-
ily be applied to predicates (or languages) for
which no training annotations are available.

1 Introduction

Semantic role labeling (SRL) (Gildea and Jurafsky,
2002) has become a well-established and highly im-
portant NLP component which directly benefits var-
ious downstream applications, such as text summa-
rization (Trandabăţ, 2011), recognizing textual en-
tailment (Sammons et al., 2012) or QA systems
(Shen and Lapata, 2007; Moreda et al., 2011). Its
goal is to detect verbal or nominal predicates, to-
gether with their associated arguments and semantic
roles, either by PropBank/Nombank (Palmer et al.,
2005; Meyers et al., 2004) or FrameNet (Baker et
al., 1998) analysis. In its traditional form, however,
SRL is restricted to the local syntactic context of the
predicate as in the following example from Ruppen-
hofer et al. (2010):

[GOAL/NIIn the centre of this room] there
was an upright beam, [THEMEwhich] had been
placed [TIMEat some period] as a support for

the old worm-eaten baulk of timber which
spanned the roof.

In a FrameNet-style analysis of the sentence,
the predicate place evokes the PLACING frame,
with two frame elements (roles) overtly expressed
(THEME and TIME) but with one role – GOAL – be-
yond the embedded relative clause and thus beyond
the scope of the SRL parser. Such implicit roles, or
null instantiations (NIs) (Fillmore, 1986; Ruppen-
hofer, 2005) are much harder to detect automatically,
as they require to broaden the analysis to the sur-
rounding discourse, commonly also to preceding (or
following) sentences.

State-of-the-art approaches to implicit SRL
(iSRL) are supervised and need a groundwork of
hand-annotated training data – which is costly, ex-
tremely sparse, limited to only a handful of predi-
cates, and requires careful feature engineering (Ger-
ber and Chai, 2012; Silberer and Frank, 2012; Li et
al., 2015). A first attempt has been made to combine
the scarce resources available (Feizabadi and Padó,
2015), but given the great diversity of predicate-
specific roles and enormous complexity of the task,
the main issues remain (Chen et al., 2010).

A promising exploratory effort recently made by
Gorinski et al. (2013) aims to overcome the anno-
tation bottleneck by using distributional methods to
infer evidence for elements filling null instantiated
roles. The authors do not rely on gold annotations
but instead learn distributional properties of fillers
induced from a large corpus.

Our Contribution: We propose an extension of the
distributional idea for unsupervised iSRL to loosen
the need for annotated training data. Specifically, we

1473



propose to induce predicate and role-specific proto-
typical fillers from large amounts of SRL annotated
texts in order to resolve null instantiations as (se-
mantically and syntactically) similar elements found
in the context. Parts of our approach have been suc-
cessfully applied in traditional SRL (Hermann et al.,
2014), but not yet to implicit roles. Our work dif-
fers from Gorinski et al. (2013) in that we extend
discrete context vectors to SRL-guided embeddings
and experiment with a variety of different configu-
rations. We intend not to set a new benchmark beat-
ing the current state-of-the-art for supervised iSRL,
but rather provide a simple and alternative strategy
which does not rely on manually annotated gold
data. Still, we demonstrate that our method is highly
competitive with supervised methods on one out of
two standard evaluation sets and that it can easily be
extended to other predicates for which no implicit
gold annotations are available.

2 Method

2.1 Prototypical Fillers

We use large amounts of explicit SRL annotations to
compute predicate-specific protofillers (prototypical
fillers) for each frame element (role) individually:

#»v protofiller =
1
N

N∑
i=0

E(wi) (1)

where N is the total number of tokens filling a par-
ticular role andE(·) is an embedding function which
maps a word wi to its distributed representation, i.e.,
a precomputed vector of d dimensions. Note that
only those words contribute to the protofiller of a
frame element which occur in this role.

2.2 Identifying Null Instantiations

Our approach generalizes over labeled filler in-
stances of the frame (PLACING in the example) as
found in corpus data, e.g., placed on the middle pic-
ture, planted on the top of the church, hung over
the river, laid on the table, etc. We exploit their
syntactic (here: prepositional) and semantic prop-
erties (inanimate, spacial NPs) in order to capture
a composed meaning and thus to approximate the
correct implicit role in the centre of this room. We
measure similarity between a trained protofiller #»v p

and a candidate constituent #»v c by cosine similar-
ity cos(θ) =

#»v p· #»v c
‖ #»v p‖‖ #»v c‖ and predict a candidate as

null instantiation which maximizes the inner prod-
uct with the protofiller. As candidate constituents
for an implicit argument we initially consider all ter-
minal and non-terminal nodes in a context window
of the predicate, ruling out those categories which
never occur as implicit arguments, which do contain
the target predicate and/or which are already overt
arguments. The result set comprises mainly nouns,
verbs and PPs. Candidate constituents in our evalua-
tion data are available from their respective (manual)
syntax annotation, but could easily be extracted us-
ing automated phrase-structure parsers. The candi-
date vectors for arbitrary length n-grams are derived
in the same way (by means of Equation 1).

2.3 Training Resources & Tools

In accordance with domain-specific evaluation data,
we chose to learn protofillers on two distinct cor-
pora: The Corpus of Late Modern English Texts,
CLMET (Smet, 2005) (≈35M tokens, 18th–20th
century novels) and a subset of the English Giga-
word corpus (Graff and Cieri, 2003) (≈500M to-
kens of Newswire texts). We label the first one with
SEMAFOR1 (Das et al., 2014), a FrameNet-style se-
mantic parser. We employ MATE2 (Björkelund et
al., 2009) to obtain a PropBank/NomBank analysis
for each sentence in Gigaword.

CLMET Gigaword

# explicit roles 21.9M 264.0M
# predicate instances 9.5M 122.5M
# roles per predicate 2.3 2.2
# predicates per sentence 7.6 4.2

Table 1: Statistics on the number of explicit fillers used for
training protofillers.

Table 1 highlights general statistics on the number
of predicates collected from both corpora. Two ob-
servations are worth noting: While on average the
number of explicitly realized roles/frame elements
per predicate/frame in both data sets is similar, we
find more predicate instances in CLMET than in Gi-
gaword. This is due to the FrameNet lexicon and
its more fine-grained modeling of lexical units, as

1http://www.cs.cmu.edu/˜ark/SEMAFOR/
2https://code.google.com/p/mate-tools/

1474



opposed to PropBank. Also note that FrameNet
currently specifies 9.7 frame elements per lexical
frame3 which – despite the fact that this number also
comprises non-core arguments – is much larger than
what can explicitly be labeled by the SRL systems.

Regarding the distributional component, we ex-
perimented with a variety of distributed word repre-
sentations: We chose out of the box vectors; Col-
lobert et al. (2011), dependency-based word embed-
dings (Levy and Goldberg, 2014) and the pre-trained
Google News vectors from word2vec4 (Mikolov et
al., 2013). Using the same tool, we also trained cus-
tom embeddings (bag-of-words and skip-gram) with
50 dimensions on our two corpora.

3 Evaluation

In order to assess the usefulness of our approach, a
quantitative evaluation has been conducted on two
iSRL test sets which have become a de facto stan-
dard in this domain: a collection of fiction novels
from the SemEval 2010 Shared Task with manual
annotations of null instantiations (Ruppenhofer et
al., 2010), and Gerber and Chai (2010)’s augmented
NomBank data set. Table 2 shows some general
statistics on the number of implicit roles and can-
didate phrases involved in our experiments. As to
have a comparison with the supervised approaches
referred to in this study, we also provide the size of
the training data.

SemEval NomBank

# predicate instances
in training set 1,370 816
in test set 1,703 437

# implicit arguments
in training set 245 650
in test set 259 246

# of candidate phrases
per predicate instance 27.6 52.2

proportion of single tokens 63.4% 47.9%
proportion of phrases 36.6% 52.1%

∅ length of candidate phrase
(in tokens) 5.8 7.1

Table 2: Statistics on implicit arguments and candidate phrases
from the test sections of the two evaluation sets.

3https://framenet.icsi.berkeley.edu/
fndrupal/current_status, accessed March 2016.

4https://code.google.com/p/word2vec/

3.1 SemEval Data
In Table 4, we report the classification scores for
the (NI-only) null instantiation linking task on the
SemEval data, given the parsed candidate phrases
and the gold information about the missing frame
element.5 For space reasons, we only include the re-
sults of our best-performing configuration, obtained
from protofillers trained on the late modern English
texts and Collobert et al. (2011) embeddings (C&W)
with the search space for candidate NIs limited to the
current and previous sentence. As a reference, we
compare our results to the two best models (M1 and
M1′) by Silberer and Frank (2012), the vector-based
resolver (VEC) by Gorinski et al. (2013) – which is
most similar to ours – and, finally, their ensemble
combination of four semantically informed resolvers
by majority vote (4X).

P R F1

Silberer and Frank (2012) M1 30.8 25.1 27.7
Silberer and Frank (2012) M1′ 35.6 20.1 25.7
Gorinski et al. (2013) VEC 21.0 18.0 19.0
Gorinski et al. (2013) 4X 26.0 24.0 25.0
This paper: C&W embeddings 27.2 25.7 26.4

Table 4: NI linking performance on the SemEval test data.

The figures in Table 4 suggest that our approach
clearly outperforms the vector-based method by
Gorinski et al. (2013) and is best in terms of over-
all recognition rate (recall) among all systems. One
potential reason for that might be that, in contrast
to the VEC resolver, we do not compute mere con-
text vectors but do rely on the valuable annotations
obtained from explicit SRL structures. Also, we do
not restrict our analysis to head words only, as we
have seen that syntactic information from function
words is crucial for the resolution of null instantiated
roles, too. Moreover, our distributional protofiller
method is highly competitive with state-of-the-art
performance by Silberer and Frank (2012), yet does
not yield better results in terms of F1 score. Note
however that, in contrast to their approach, ours is
largely unsupervised and does neither rely on gold
coreference chains, nor do we need to train on im-

5 This avoids error propagation from NI detection and allows
us to directly compare our results to previous approaches on the
same task. Note that Laparra and Rigau (2012) do only report
their accuracies for the full pipeline.

1475



B Gerber & Chai Laparra & Rigau Proto C&W Proto W2Vcbow

predicates: F1 P R F1 P R F1 P R F1 P R F1
sale 36.2 47.2 41.7 44.2 41.2 39.4 40.3 61.0 29.6 39.8 60.8 26.8 37.2
price 15.4 36.0 32.6 34.2 53.3 53.3 53.3 14.7 25.8 18.7 21.8 36.6 27.3
investor 9.8 36.8 40.0 38.4 43.0 39.5 41.2 22.5 48.3 30.7 24.1 57.2 33.9
bid 32.3 23.8 19.2 21.3 52.9 51.0 52.0 30.4 31.5 30.9 40.0 41.5 40.7
plan 38.5 78.6 55.0 64.7 40.7 40.7 40.7 41.1 43.2 42.1 44.3 51.0 47.4
cost 34.8 61.1 64.7 62.9 56.1 50.2 53.0 32.5 19.1 24.0 49.9 29.3 36.9
loss 52.6 83.3 83.3 83.3 68.4 63.5 65.8 54.8 73.1 62.6 54.7 63.8 58.9
loan 18.2 42.9 33.3 37.5 25.0 20.0 22.2 33.9 49.0 40.1 33.2 44.2 37.9
investment 0.0 40.0 25.0 30.8 47.6 35.7 40.8 29.1 21.8 24.9 39.2 34.3 36.6
fund 0.0 14.3 16.7 15.4 66.7 33.3 44.4 100.0 33.3 50.0 75.0 25.0 37.5

Overall 26.5 44.5 40.4 42.3 47.9 43.8 45.8 30.2 35.2 32.5 33.5 39.2 36.1

Table 3: Classification scores for implicit argument labeling on the NomBank test section. Baseline B from Gerber & Chai (2010):
uses previous occurrence of same predicate. Gerber & Chai (2010): supervised logistic regression classifier trained on implicit

fillers. Laparra & Rigau (2013): algorithm based on coherence relationship between predicates and fillers. Our best-performing

protofillers are obtained by Collobert et al. (2011) embeddings (Proto C&W) and custom trained vectors (Proto W2Vcbow) using
Gigaword SRL annotations.

plicit semantic roles in a supervised setting. An
error analysis of our method reveals that it is par-
ticularly effective for NIs encountered in the same
sentence as the target predicate (44.4% accuracy),
which seems plausible given the contextual setup in
which protofillers are derived.

3.2 NomBank Data

Compared to the SemEval data, Gerber and Chai
(2010)’s augmented NomBank resource covers only
ten nominal predicates, which allows us to nicely vi-
sualize the distributional profile based on their pro-
totypical fillers. For each predicate, we simply con-
catenate all per role computed protofillers and apply
multidimensional scaling to project the so obtained
vectors onto two dimensions (cf. Figure 1).

We observe that the predicate grouping is now
based on the prototypical fillers that they co-occur
with: In the Wall Street Journal texts, loss, loan and
investment are similar because their proto-agents
(A0 fillers) who lose, lend and invest resp. are se-
mantically shared (i.e. companies, banks). Simi-
larly, bid, cost and fund are related in that the tar-
gets or commodities (A2) are all money-financed.
Finally, the predicates sale and plan are to be ex-
pected as outliers as they are less homogeneous in
their prototypical argument structure.

We have empirically evaluated our protofiller

sale

price

investor

bid

plan

cost

loss

loan

investment
fund

−0.4

−0.2

0.0

0.2

−0.2 0.0 0.2
x

y

Figure 1: Clustered projection of the ten nominal predicates
from Gerber and Chai (2010) in protofiller space.

method also on this data set: Table 3 reports the
classification scores for implicit argument resolution
compared to the state-of-the-art (Laparra and Rigau,
2013). We restrict the search for implicit arguments
to certain predicate-specific parts-of-speech, since
some syntactic constituents (e.g., SBAR) never oc-
cur as implicit arguments. For choosing the final
implicit arguments for each individual predicate in-
stance, we follow the same deterministic strategy
as described in Gerber and Chai (2010), which in-
formally states that, if a certain role is not overtly
expressed (within a chain of mentions of the same
predicate in previous sentences), it is an implicit

1476



candidate. POS lists and cosine similarity thresholds
which trigger an actual prediction have been opti-
mized on the development set. The context window
for candidate NIs is optimal for the current and pre-
vious two sentences in our setting, which explains
why the the number of candidate constituents is ap-
proximately twice as large for the NomBank predi-
cates (cf. Table 2).

Our best-performing protofillers are again ob-
tained by Collobert et al. (2011) embeddings sub-
stituting explicit SRL annotations in the Gigaword
corpus, and with custom-trained embeddings using
the continuous bag-of-words model. Overall, our re-
sults significantly exceed the highly informed base-
line but cannot beat the state-of-the art on this test
set. For some predicates, the protofillers seem to
generalize better (higher recall), and in particular for
the low-frequency predicates (fund), precision can
be increased. Also, we found that the dependency-
based word embeddings do perform slightly worse
(not shown), compared to our optimal two con-
figurations. This might be due to the fact that
the inherent properties of dependency-based con-
texts mostly focus on relations between semanti-
cally valuable nouns, ignoring (“skipping”) func-
tional words and categories.6 The same pertains
to the pre-computed Google News vectors which
come with a frequency cutoff excluding stop words,
again a constraint which is harmful for the correct
identification of implicit roles. Furthermore, skip-
gram embeddings perform significantly worse than
the embeddings derived by the continuous bag-of-
words implementation (relative decrease in F1 by
more than 30%). Finally, we observed that infer-
ring implicit roles for nominal predicates is much
more challenging because our collected fillers ex-
hibit a much greater variation. For example, the
protoagents of loan can roughly be divided into two
categories, institutions and countries. This in turn
introduces noise and has a negative effect on the
quality of the singleton protofillers which by vector
average capture neither of the two groups perfectly.
Promising alternatives could operate on (topic-like)
protofiller clusters which we leave for future work.

6This is also nicely illustrated in Levy and Goldberg (2014).

4 Summary & Conclusion

We have described a lightweight approach for the
resolution of implicit semantic roles which does
not rely on manual gold annotations. For each
predicate-specific role, our method generalizes over
explicit SRL-guided annotations incorporating pre-
trained word embeddings. This allows us to capture
their idiosyncratic properties and use the so-inferred
protofillers to find null instantiated roles by means
of distributional similarity.

Our method has proven to be generally useful, in
particular on the SemEval data, where it is compet-
itive with supervised systems. Its greatest benefit
stems from its simplicity and from that fact that it
allows to induce null-instantiated roles for arbitrary
predicates. As it is applicable even if no iSRL train-
ing data is available, it represents a promising tech-
nique to address iSRL data scarcity issues.

In our experiment, we employed PropBank/Nom-
Bank-style (i)SRL annotations, and our general de-
sign clearly benefits from using small-scale inven-
tories of semantic roles. It should be noted though,
that our approach is not restricted to any particular
SRL tagset, but can be equally applied to other role
inventories with similar degrees of consistence and
size. Beyond SRL annotations in a strict sense, this
might even extend to syntactic dependency annota-
tions that are occasionally taken as a substitute for
semantic roles proper. In particular, we see poten-
tial in combining our experiments with on-going ef-
forts to cross-lingual projection, adaptation and har-
monization of syntax annotations along the lines of
Sukhareva and Chiarcos (2014, 2016) and related
approaches based on frameworks such as the Uni-
versal Dependencies (Nivre, 2015, UD).7 If success-
ful, an adaptation using grammatical relations rather
than semantic roles represents a promising possibil-
ity to create iSRL annotation and iSRL annotation
tools for other languages, as Universal Dependen-
cies are becoming increasingly available for major
and low-resourced languages and can be projected
to others.

The protofillers involved in this study
are available at: http://acoli.cs.
uni-frankfurt.de/resources.

7http://universaldependencies.github.io

1477



Acknowledgments

The authors would like to thank Joyce Chai and
Hendrik De Smet for providing us access to
their resources and corpora. We are grateful to
Egoitz Laparra for sending us their evaluation script
and also thank the anonymous reviewers for their
valuable feedback and insightful comments.

References

Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet Project. In Pro-
ceedings of the 36th Annual Meeting of the Asso-
ciation for Computational Linguistics and 17th In-
ternational Conference on Computational Linguistics
(ACL-COLIG 1998), pages 86–90, Montreal, Quebec,
Canada.

Anders Björkelund, Love Hafdell, and Pierre Nugues.
2009. Multilingual Semantic Role Labeling. In
Proceedings of the Thirteenth Conference on Compu-
tational Natural Language Learning (CoNLL-2009):
Shared Task, pages 43–48, Boulder, Colorado, June.

Desai Chen, Nathan Schneider, Dipanjan Das, and
Noah A. Smith. 2010. SEMAFOR: Frame Argument
Resolution with Log-linear Models. In Proceedings of
the 5th International Workshop on Semantic Evalua-
tion (SemEval-2010), pages 264–267, Los Angeles.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.
Natural Language Processing (Almost) from Scratch.
Journal of Machine Learning Research, 12:2493–
2537.

Dipanjan Das, Desai Chen, André F. T. Martins, Nathan
Schneider, and Noah A. Smith. 2014. Frame-semantic
Parsing. Computational Linguistics, 40(1):9–56.

Parvin S. Feizabadi and Sebastian Padó. 2015. Combin-
ing Seemingly Incompatible Corpora for Implicit Se-
mantic Role Labeling. In Proceedings of the 4th Joint
Conference on Lexical and Computational Semantics
(*SEM 2015), pages 40–50, Denver, CO.

Charles J. Fillmore. 1986. Pragmatically Controlled
Zero Anaphora. In Proceedings of Berkeley Linguis-
tics Society, pages 95–107, Berkeley, CA.

Matthew Gerber and Joyce Chai. 2010. Beyond Nom-
Bank: A Study of Implicit Arguments for Nominal
Predicates. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistics
(ACL-2010), pages 1583–1592, Uppsala, Sweden.

Matthew Gerber and Joyce Chai. 2012. Semantic Role
Labeling of Implicit Arguments for Nominal Predi-
cates. Computational Linguistics, 38(4):755–798.

Daniel Gildea and Daniel Jurafsky. 2002. Automatic La-
beling of Semantic Roles. Computational Linguistics,
28(3):245–288.

Philip Gorinski, Josef Ruppenhofer, and Caroline
Sporleder. 2013. Towards Weakly Supervised Resolu-
tion of Null Instantiations. In Proceedings of the 10th
International Conference on Computational Semantics
(IWCS 2013), pages 119–130, Potsdam, Germany.

David Graff and Christopher Cieri. 2003. English Gi-
gaword. Linguistic Data Consortium, Philadelphia,
LDC2003T05. Web Download.

Karl Moritz Hermann, Dipanjan Das, Jason Weston, and
Kuzman Ganchev. 2014. Semantic Frame Identifi-
cation with Distributed Word Representations. In Pro-
ceedings of the 52th Annual Meeting of the Association
for Computational Linguistics (ACL-2014), Baltimore,
Maryland.

Egoitz Laparra and German Rigau. 2012. Exploiting
Explicit Annotations and Semantic Types for Implicit
Argument Resolution. In Proceedings of the 6th Inter-
national Conference on Semantic Computing (ICSC-
2012), pages 75–78.

Egoitz Laparra and German Rigau. 2013. ImpAr: A De-
terministic Algorithm for Implicit Semantic Role La-
belling. In Proceedings of the 51st Annual Meeting of
the Association for Computational Linguistics (ACL-
2013), pages 1180–1189, Sofia, Bulgaria.

Omer Levy and Yoav Goldberg. 2014. Dependency-
Based Word Embeddings. In Proceedings of the 52nd
Annual Meeting of the Association for Computational
Linguistics (ACL-2014), pages 302–308, Baltimore,
MD.

Ru Li, Juan Wu, Zhiqiang Wang, and Qinghua Chai.
2015. Implicit Role Linking on Chinese Discourse:
Exploiting Explicit Roles and Frame-to-Frame Rela-
tions. In Proceedings of the 53rd Annual Meeting of
the Association for Computational Linguistics and the
7th International Joint Conference on Natural Lan-
guage Processing (ACL-IJCNLP 2015), pages 1263–
1271, Beijing, China.

Adam Meyers, Ruth Reeves, Catherine Macleod, Rachel
Szekely, Veronika Zielinska, Brian Young, and Ralph
Grishman. 2004. The NomBank Project: An In-
terim Report. In Proceedings of the HLT-NAACL 2004
Workshop on Frontiers in Corpus Annotation, pages
24–31, Boston, Massachusetts.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient Estimation of Word Represen-
tations in Vector Space. CoRR, abs/1301.3781.

Paloma Moreda, Hector Llorens, Estela Saquete, and
Manuel Palomar. 2011. Combining Semantic Infor-
mation in Question Answering Systems. Journal of
Information Processing and Management, 47(6):870–
885.

1478



Joakim Nivre. 2015. Towards a Universal Gram-
mar for Natural Language Processing. In Proceed-
ings of the 16th International Conference on Com-
putational Linguistics and Intelligent Text Processing
(CICLing-2015), pages 3–16, Cairo, Egypt. LNCS
9041, Springer.

Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An Annotated Cor-
pus of Semantic Roles. Computational Linguistics,
31(1):71–106.

Josef Ruppenhofer, Caroline Sporleder, Roser Morante,
Collin Baker, and Martha Palmer. 2010. SemEval-
2010 Task 10: Linking Events and Their Participants
in Discourse. In Proceedings of the 5th International
Workshop on Semantic Evaluation (SemEval-2010),
pages 45–50, Los Angeles, CA.

Josef Ruppenhofer. 2005. Regularities in Null Instantia-
tion. Ms, University of Colorado.

Mark Sammons, V.G.Vinod Vydiswaran, and Dan Roth.
2012. Recognizing Textual Entailment. In Daniel M.
Bikel and Imed Zitouni, editors, Multilingual Natu-
ral Language Applications: From Theory to Practice,
chapter 6, pages 209–258. IBM Press.

Dan Shen and Mirella Lapata. 2007. Using Seman-
tic Roles to Improve Question Answering. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Compu-
tational Natural Language Learning (EMNLP-CoNLL
2007), pages 12–21, Prague, Czech Republic.

Carina Silberer and Anette Frank. 2012. Casting Implicit
Role Linking as an Anaphora Resolution Task. In Pro-
ceedings of the First Joint Conference on Lexical and
Computational Semantics (*SEM-2012), pages 1–10,
Montreal, Quebec, Canada.

Hendrik De Smet. 2005. A Corpus of Late Modern En-
glish Texts. International Computer Archive of Mod-
ern and Medieval English (ICAME), 29:69–82.

Maria Sukhareva and Christian Chiarcos. 2014. Di-
achronic Proximity vs. Data Sparsity in Cross-lingual
Parser Projection. A Case Study on Germanic. In
COLING-2014 Workshop on Applying NLP Tools to
Similar Languages, Varieties and Dialects (VarDial-
2014), Dublin, Ireland.

Maria Sukhareva and Christian Chiarcos. 2016. Com-
bining Ontologies and Neural Networks for Analyzing
Historical Language Varieties. A Case Study in Mid-
dle Low German. In Proceedings of the 10th Interna-
tional Conference on Language Resources and Evalu-
ation (LREC-2016), Portorož, Slovenia.

Diana Trandabăţ. 2011. Using Semantic Roles to Im-
prove Summaries. In Proceedings of the 13th Eu-
ropean Workshop on Natural Language Generation
(ENLG-2011), pages 164–169, Nancy, France.

1479


