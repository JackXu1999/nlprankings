



















































Bridging Languages through Images with Deep Partial Canonical Correlation Analysis


Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 910–921
Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics

910

Bridging Languages through Images with Deep Partial Canonical
Correlation Analysis

Guy Rotman1, Ivan Vulić2 and Roi Reichart1
1 Faculty of Industrial Engineering and Management, Technion, IIT

2 Language Technology Lab, University of Cambridge
grotman@campus.technion.ac.il

iv250@cam.ac.uk roiri@technion.ac.il

Abstract

We present a deep neural network that
leverages images to improve bilingual text
embeddings. Relying on bilingual image
tags and descriptions, our approach con-
ditions text embedding induction on the
shared visual information for both lan-
guages, producing highly correlated bilin-
gual embeddings. In particular, we propose
a novel model based on Partial Canonical
Correlation Analysis (PCCA). While the
original PCCA finds linear projections of
two views in order to maximize their canon-
ical correlation conditioned on a shared
third variable, we introduce a non-linear
Deep PCCA (DPCCA) model, and de-
velop a new stochastic iterative algorithm
for its optimization. We evaluate PCCA
and DPCCA on multilingual word simi-
larity and cross-lingual image description
retrieval. Our models outperform a large
variety of previous methods, despite not
having access to any visual signal during
test time inference.1

1 Introduction

Research in multi-modal semantics deals with the
grounding problem (Harnad, 1990), motivated by
evidence that many semantic concepts, irrespec-
tive of the actual language, are grounded in the
perceptual system (Barsalou and Wiemer-Hastings,
2005). In particular, recent studies have shown
that performance on NLP tasks can be improved
by joint modeling of text and vision, with multi-
modal and perceptually enhanced representation
learning outperforming purely textual representa-

1Our code and data are available at: https://github.
com/rotmanguy/DPCCA.

tions (Feng and Lapata, 2010; Kiela and Bottou,
2014; Lazaridou et al., 2015).

These findings are not surprising, and can be
explained by the fact that humans understand lan-
guage not only by its words, but also by their vi-
sual/perceptual context. The ability to connect vi-
sion and language has also enabled new tasks which
require both visual and language understanding,
such as visual question answering (Antol et al.,
2015; Fukui et al., 2016; Xu and Saenko, 2016),
image-to-text retrieval and text-to-image retrieval
(Kiros et al., 2014; Mao et al., 2014), image caption
generation (Farhadi et al., 2010; Mao et al., 2015;
Vinyals et al., 2015; Xu et al., 2015), and visual
sense disambiguation (Gella et al., 2016).

While the main focus is still on monolingual set-
tings, the fact that visual data can serve as a natural
bridge between languages has sparked additional
interest towards multilingual multi-modal model-
ing. Such models induce bilingual multi-modal
spaces based on multi-view learning (Calixto et al.,
2017; Gella et al., 2017; Rajendran et al., 2016).

In this work, we propose a novel effective ap-
proach for learning bilingual text embeddings con-
ditioned on shared visual information. This addi-
tional perceptual modality bridges the gap between
languages and reveals latent connections between
concepts in the multilingual setup. The shared vi-
sual information in our work takes the form of
images with word-level tags or sentence-level de-
scriptions assigned in more than one language.

We propose a deep neural architecture termed
Deep Partial Canonical Correlation Analysis
(DPCCA) based on the Partial CCA (PCCA)
method (Rao, 1969). To the best of our knowledge,
PCCA has not been used in multilingual settings
before. In short, PCCA is a variant of CCA which
learns maximally correlated linear projections of
two views (e.g., two language-specific “text-based
views”) conditioned on a shared third view (e.g.,

https://github.com/rotmanguy/DPCCA
https://github.com/rotmanguy/DPCCA


911

the “visual view”). We discuss the PCCA and
DPCCA methods in §3 and show how they can be
applied without having access to the shared images
at test time inference.

PCCA inherits one disadvantageous property
from CCA: both methods compute estimates for
covariance matrices based on all training data. This
would prevent feasible training of their deep non-
linear variants, since deep neural nets (DNNs) are
predominantly optimized via stochastic optimiza-
tion algorithms. To resolve this major hindrance,
we propose an effective optimization algorithm
for DPCCA, inspired by the work of Wang et al.
(2015b) on Deep CCA (DCCA) optimization.

We evaluate our DPCCA architecture on two se-
mantic tasks: 1) multilingual word similarity and
2) cross-lingual image description retrieval. For the
former, we construct and provide to the commu-
nity a new Word-Image-Word (WIW) dataset con-
taining bilingual lexicons for three languages with
shared images for 5K+ concepts. WIW is used as
training data for word similarity experiments, while
evaluation is conducted on the standard multilin-
gual SimLex-999 dataset (Hill et al., 2015; Leviant
and Reichart, 2015).

The results reveal stable improvements over a
large space of non-deep and deep CCA-style base-
lines in both tasks. Most importantly, 1) PCCA
is overall better than other methods which do not
use the additional perceptual view; 2) DPCCA out-
performs PCCA, indicating the importance of non-
linear transformations modeled through DNNs; 3)
DPCCA outscores DCCA, again verifying the im-
portance of conditioning multilingual text embed-
ding induction on the shared visual view; and 4)
DPCCA outperforms two recent multi-modal bilin-
gual models which also leverage visual information
(Gella et al., 2017; Rajendran et al., 2016).

2 Related Work

This work is related to two research threads: 1)
multi-modal models that combine vision and lan-
guage, with a focus on multilingual settings; 2) cor-
relational multi-view models based on CCA which
learn a shared vector space for multiple views.

Multi-Modal Modeling in Multilingual Settings
Research in cognitive science suggests that human
meaning representations are grounded in our per-
ceptual system and sensori-motor experience (Har-
nad, 1990; Lakoff and Johnson, 1999; Louwerse,
2011). Visual context serves as a useful cross-

lingual grounding signal (Bruni et al., 2014; Glavaš
et al., 2017) due to its language invariance, even en-
abling the induction of word-level bilingual seman-
tic spaces solely through tagged images obtained
from the Web (Bergsma and Van Durme, 2011;
Kiela et al., 2015). Vulić et al. (2016) combine text
embeddings with visual features via simple tech-
niques of concatenation and averaging to obtain
bilingual multi-modal representations, with noted
improvements over text-only embeddings on word
similarity and bilingual lexicon extraction. How-
ever, similar to the monolingual model of Kiela and
Bottou (2014), their models lack the training phase,
and require the visual signal at test time.

Recent work from Gella et al. (2017) exploits vi-
sual content as a bridge between multiple languages
by optimizing a contrastive loss function. Further-
more, Rajendran et al. (2016) extend the work of
Chandar et al. (2016) and propose to use a pivot
representation in multimodal multilingual setups,
with English representations serving as the pivot.
While these works learn shared multimodal mul-
tilingual vector spaces, we demonstrate improved
performance with our models (see §7).

Finally, although not directly comparable, recent
work in neural machine translation has constructed
models that can translate image descriptions by
additionally relying on visual features of the im-
age provided (Calixto and Liu, 2017; Elliott et al.,
2015; Hitschler et al., 2016; Huang et al., 2016;
Nakayama and Nishida, 2017, inter alia).

Correlational Models CCA-based techniques
support multiple views on related data: e.g., when
coupled with a bilingual dictionary, input monolin-
gual word embeddings for two different languages
can be seen as two views of the same latent se-
mantic signal. Recently, CCA-based models for
bilingual text embedding induction were proposed.
These models rely on the basic CCA model (Chan-
dar et al., 2016; Faruqui and Dyer, 2014), its deep
variant (Lu et al., 2015), and a CCA extension
which supports more than two views (Funaki and
Nakayama, 2015; Rastogi et al., 2015). In this
work, we propose to use (D)PCCA, which organ-
ically supports our setup: it conditions the two
(textual) views on a shared (visual) view.

CCA-based methods (including PCCA) require
the estimation of covariance matrices over all train-
ing data (Kessy et al., 2017). This hinders the use
of DNNs with these models, as DNNs are typi-
cally trained via stochastic optimization over mini-



912

batches on very large training sets. To address
this limitation, various optimization methods for
Deep CCA were proposed. Andrew et al. (2013)
use L-BFGS (Byrd et al., 1995) over all training
samples, while Arora and Livescu (2013) and Yan
and Mikolajczyk (2015) train with large batches.
However, these methods suffer from high memory
complexity with unstable numerical computations.

Wang et al. (2015b) have recently proposed a
stochastic approach for CCA and DCCA which
copes well with small and large batch sizes while
preserving high model performance. They use or-
thogonal iterations to estimate a moving average of
the covariance matrices, which improves memory
consumption. Therefore, we base our novel opti-
mization algorithm for DPCCA on this approach.

3 Methodology: Deep Partial CCA

Given two image descriptions x and y in two lan-
guages and an image z that they refer to, the task is
to learn a shared bilingual space such that similar
descriptions obtain similar representations in the in-
duced space. The image z serves as a shared third
view on the textual data during training. The rep-
resentation model is then utilized in cross-lingual
and monolingual tasks. In this paper we focus on
the more realistic scenario where no relevant vi-
sual content is available at test time. For this goal
we propose a novel Deep Partial CCA (DPCCA)
framework.

In what follows, we first review the CCA model
and its deep variant: DCCA. We then introduce
our DPCCA architecture, and describe our new
stochastic optimization algorithm for DPCCA.

3.1 CCA and Deep CCA

DCCA (Andrew et al., 2013) extends CCA by
learning non-linear (instead of linear) transforma-
tions of features contained in the input matrices
X ∈ RDx×N and Y ∈ RDy×N , where Dx and
Dy are input vector dimensionalities, and N is the
number of input items. Since CCA is a special
case of the non-linear DCCA (see below), we here
briefly outline the more general DCCA model.

The DCCA architecture is illustrated in Fig-
ure 1a. Non-linear transformations are achieved
through two DNNs f : RDx×N → RD′x×N and
g : RDy×N → RD′y×N for X and Y . D′x and D′y
are the output dimensionalities. A final linear layer
is added to resemble the linear CCA projection.

The goal is to project the features of X and

Y into a shared L-dimensional (1 ≤ L ≤
min(D′x, D

′
y)) space such that the canonical corre-

lation of the final outputs F (X) = W Tf(X)
and G(Y ) = V T g(Y ) is maximized. W ∈
RD′x×L and V ∈ RD′y×L are projection matrices:
they project the final outputs of the DNNs to the
shared space. Wf and Vg (the parameters of f
and g) and the projection matrices are the model
parameters: WF = {Wf ,W }; VG = {Vg,V }.2
Formally, the DCCA objective can be written as:

max
WF ,VG

Tr(Σ̂FG)

so that Σ̂FF = Σ̂GG = I.
(1)

Σ̂FG ≡ 1N−1F (X)G(Y )
T is the estimation

of the cross-covariance matrix of the outputs,
and Σ̂FF ≡ 1N−1F (X)F (X)

T , Σ̂GG ≡
1

N−1G(Y )G(Y )
T are the estimations of the auto-

covariance matrices of the outputs.3 Further, fol-
lowing Wang et al. (2015b), the optimal solution
of Eq. (1) is equivalent to the optimal solution of
the following:

min
WF ,VG

1

N − 1‖F (X)−G(Y )‖
2
F

s.t. Σ̂FF = Σ̂GG = I.

(2)

The main disadvantage of DCCA is its inability to
support more than two views, and to learn condi-
tioned on an additional shared view, which is why
we introduce Deep Partial CCA.

3.2 New Model: Deep Partial CCA

Figure 1b illustrates the architecture of DPCCA.
The training data now consists of triplets
(xi,yi, zi)

N
1=1 from three views, forming the

columns of X , Y and Z, where xi ∈ RDx ,yi ∈
RDy , zi ∈ RDz for i = 1, . . . , N . The objective is
to maximize the canonical correlation of the first
two views X and Y conditioned on the shared
third variable Z. Following Rao (1969)’s work
on Partial CCA, we first consider two multivariate
linear multiple regression models:

F (X) = AZ + F (X|Z), (3)
G(Y ) = BZ +G(Y |Z). (4)

2For notational simplicity, we assume f(X) and g(Y )
to have zero-means, otherwise it is possible to centralize them
at the final layer of each network to the same effect.

3The CCA model can be seen as a special (linear) case
of the more general DCCA model. The basic CCA objective
can be recovered from the DCCA objective by simply setting
D′x = Dx, D′y = Dy and f(X) = idX , g(Y ) = idY ; id
is the identity mapping.



913

(a) (b)

Figure 1: DCCA and DPCCA architectures. (a): DCCA. X and Y (English and German image
descriptions) are fed through two identical deep feed-forward neural networks followed by a final linear
layer. The final nodes of the networks F (X) and G(Y ) are then maximally correlated via the CCA
objective. (b): DPCCA. In addition, a third (shared) variable Z (an image) is either optimized via an
identical architecture of the two main views (DPCCA Variant B, illustrated here) or kept fixed (DPCCA
Variant A). The final nodes of the networks F (X) and G(Y ) are maximally correlated conditioned on
the final node in the middle network H(Z) (or directly on the input node Z in DPCCA Variant A).

A,B ∈ RL×Dz are matrices of coefficients, and
F (X|Z),G(Y |Z) ∈ RL×N are normal random
error matrices: residuals. We then minimize the
mean-squared error regression criterion:

min
A

1

N − 1‖F (X)−AZ‖
2
F , (5)

min
B

1

N − 1‖G(Y )−BZ‖
2
F . (6)

After obtaining the optimal solutions for the coeffi-
cients, Â and B̂, the residuals are as follows:

F (X|Z) = F (X)− ÂZ

= F (X)− Σ̂FZΣ̂−1ZZZ. (7)

G(Y |Z) is computed in the analogous man-
ner, now relying on G(Y ) and B̂Z. Σ̂S′Z ≡

1
N−1SZ

T refers to the covariance matrix es-
timator of S′ and Z, where (S′, S) ∈
{(F ,F (X)), (G,G(Y )), (Z,Z)}.4

The canonical correlation between the residual
matrices F (X|Z) and G(Y |Z) is referred to as
the partial canonical correlation. The Deep PCCA
objective can be obtained by replacing F (X) and
G(Y ) with their residuals in Eq. (2):

min
WF ,VG

1

N − 1‖F (X|Z)−G(Y |Z)‖
2
F

s.t. Σ̂FF |Z = Σ̂GG|Z = I.

(8)

The computation of the conditional covariance ma-
trix Σ̂FF |Z can be formulated as follows:

Σ̂FF |Z ≡
1

N − 1F (X|Z)F (X|Z)
T

= Σ̂FF − Σ̂FZΣ̂−1ZZΣ̂
T
FZ . (9)

4A small value � > 0 is added to the main diagonal of the
covariance estimators for numerical stability.

The other conditional covariance matrix Σ̂GG|Z is
again computed in the analogous manner, replacing
F with G and X with Y .5

While the (D)PCCA objective is computed over
the residuals, after the network is trained (using
multilingual texts and corresponding images) we
can compute the representations of F (X) and
G(Y ) at test time without having access to im-
ages (see the network structure in Figure 1b). This
heuristic enables the use of DPCCA in a real-life
scenario in which images are unavailable at test
time, and its encouraging results are demonstrated
in §7.

Model Variants We consider two DPCCA vari-
ants : 1) in DPCCA Variant A, the shared view Z
is kept fixed; 2) DPCCA Variant B also optimizes
over Z, as illustrated in Figure 1b. Variant A may
be seen as a special case of Variant B.6

Variant B learns a non-linear function of the
shared variable, H(Z) = UTh(Z), during train-
ing, where h : RDz×N → RDz′×N is a DNN hav-
ing the same architecture as f and g. U ∈ RDz′×L
is the final linear layer of H , such that over-
all, the additional parameters of the model are
UH = {Uh,U}. Instead of assuming a linear
connection between F (X) and G(Y ) to Z, as
in Variant A, we now assume that the linear con-
nection takes place with H(Z). This assumption

5The original PCCA objective can be recovered by setting
D′x = Dx, D′y = Dy and f(X) = idX , g(Y ) = idY .

6For Variant A, in order for Z to be on the same range
of values as in F and G, we pass it through the activation
function of the network, Z = σ(Z). Due to space constraints
we discuss DPCCA Variant A in the supplementary material
only.



914

changes Eq. (3) and Eq. (4) to:7

F (X) = A′ ·H(Z) + F (X|H(Z)), (10)
G(Y ) = B′ ·H(Z) +G(Y |H(Z)). (11)

4 DPCCA: Optimization Algorithm

Training deep variants of CCA-style multi-view
models is non-trivial due to estimation on the en-
tire training set related to whitening constraints
(i.e., the orthogonality of covariance matrices). To
overcome this issue, Wang et al. (2015b) proposed
a stochastic optimization algorithm for DCCA via
non-linear orthogonal iterations (DCCA NOI). Re-
lying on the solution for DCCA (§4.1), we develop
a new optimization algorithm for DPCCA in §4.2.

4.1 Optimization of DCCA
The DCCA optimization from Wang et al. (2015b),
fully provided in Algorithm 1, relies on three key
steps. First, the estimation of the covariance matri-
ces in the form of Σ̂FF t at time t is calculated by
a moving average over the minibatches:

Σ̂FF t ←ρΣ̂FF t−1

+ (1− ρ)
( |bt|
N − 1

)−1
F (Xbt)F (Xbt)

T . (12)

bt is the minibatch at time t, Xbt is the current in-
put matrix at time t, and ρ ∈ [0, 1] controls the ratio
between the overall covariance estimation and the
covariance estimation of the current minibatch.8

This step eliminates the need of estimating the co-
variances over all training data, as well as the in-
herent bias when the estimate relies only on the
current minibatch.

Second, the DCCA NOI algorithm forces the
whitening constraints to hold by performing an
explicit matrix transformation in the form of:

˜F (Xbt) = Σ̂
− 1

2
FFt

F (Xbt). (13)

According to Horn et al. (1988), if ρ = 0:( |bt|
N − 1

)−1 ˜F (Xbt) ˜F (Xbt)T = I. (14)
Finally, in order to optimize the DCCA objective
(see Eq. (2)), the weights of the two DNNs are de-
coupled: i.e., the objective is disassembled into two
separate mean-squared error objectives. Instead of

7Note that the matrices of coefficients A′ , B′ ∈ RL×L.
8Setting ρ to a high value indicates slow updates of the

estimator; setting it low mostly erases the overall estimation
and relies more on the current minibatch estimation.

Algorithm 1 The non-linear orthogonal iterations (NOI)
algorithm for DCCA (DCCA NOI)

Input: Data matrices X ∈ RDx×N , Y ∈ RDy×N , time
constant ρ, learning rate η.

initialization: Initialize weights (WF , VG).
Randomly choose a minibatch (Xb0 , Yb0 ).
Initialize covariances:
Σ̂FF ← N−1|b0| F (Xb0)F (Xb0)

T

Σ̂GG ← N−1|b0| G(Yb0)G(Yb0)
T

for t = 1, 2, . . . , n do
Randomly choose a minibatch (Xbt , Ybt ).

Update covariances:
Σ̂FF ← ρΣ̂FF + (1− ρ)N−1|bt| F (Xbt)F (Xbt)

T

Σ̂GG ← ρΣ̂GG + (1− ρ)N−1|bt| G(Ybt)G(Ybt)
T

Fix G̃(Ybt) = Σ̂
− 1

2
GGG(Ybt), and compute ∇WF with

respect to:

min
WF

1
|bt|‖F (Xbt)− G̃(Ybt)‖

2
F

Update parameters:
WF ←WF − η∇WF
Fix ˜F (Xbt) = Σ̂

− 1
2

FF F (Xbt), and compute ∇VG with
respect to:

min
VG

1
|bt|‖G(Ybt)−

˜F (Xbt)‖2F

Update parameters:
VG ← VG − η∇VG
end for

Output: (WF ,VG)

trying to bring F (Xbt) and G(Ybt) closer in one
gradient descent step, two steps are performed: one
of the views is fixed, and a gradient step over the
other is performed, and so on, iteratively. The final
objective functions at each time step are:

min
WF

1

|bt|
‖F (Xbt)− G̃(Ybt)‖

2
F , (15)

min
VG

1

|bt|
‖G(Ybt)− ˜F (Xbt)‖

2
F . (16)

Wang et al. (2015b) show that the projection ma-
trices W and V converge to the exact solutions of
CCA as t→∞ when considering linear CCA.

4.2 Optimization of DPCCA
Our DPCCA optimization is based on the
DCCA NOI algorithm with several adjustments.
Besides the requirement to obtain the sample
covariances Σ̂FF and Σ̂GG, when calculating
the conditional variables F (X|Z), G(Y |Z),
Σ̂FF |Z and Σ̂GG|Z , we additionally have to
obtain the stochastic estimators Σ̂FZ , Σ̂GZ and
Σ̂ZZ . To this end, we use the moving average esti-
mation from Eq. (12). Next, we define the whiten-
ing transformation on the residuals:



915

˜F (Xbt |Zbt) = Σ̂
− 1

2
FFt|ZF (Xbt |Zbt), (17)

˜G(Ybt |Zbt) = Σ
− 1

2
GGt|ZG(Ybt |Zbt). (18)

As before, the whitening constraints hold when ρ
= 0. From here, we derive our two final objective
functions over the residuals at time t:

min
WF

1

|bt|
‖F (Xbt |Zbt)− ˜G(Ybt |Zbt)‖

2
F , (19)

min
VG

1

|bt|
‖G(Ybt |Zbt)− ˜F (Xbt |Zbt)‖

2
F . (20)

Equivalently to Eq. (15)-(16) that replace Eq. (2),
Eq. (19)-(20) replace Eq. (8) by performing
stochastic, decoupled and unconstrained steps. As
our algorithm performs CCA over the residuals, we
gain the same guarantees as Wang et al. (2015b),
now for the projection matrices of the residuals.

Algorithm 2 shows the full optimization pro-
cedure for the more complex DPCCA Variant B.
The full algorithm for Variant A is provided in the
supplementary material. The main difference is
that with Variant B we replace Z with H(Z) in
all equations where it appears, and we optimize
over UH along with WF and VG in Eq. (19) and
Eq. (20), respectively.

5 Tasks and Data

Cross-lingual Image Description Retrieval
The cross-lingual image description retrieval
task is formulated as follows: taking an image
description as a query in the source language, the
system has to retrieve a set of relevant descriptions
in the target language which describe the same
image. Our evaluation assumes a single-best
scenario, where only a single target description is
relevant for each query. In addition, in our setup,
images are not available during inference: retrieval
is performed based solely on text queries. This
enables a fair comparison between our model and
many baseline models that cannot represent images
and text in a shared space. Moreover, it allows
us to test our model in the realistic setup where
images are not available at test time. To avoid the
use of images at retrieval time with DPCCA, we
perform the retrieval on F (X) and G(Y ), rather
than on F (X|Z) and G(Y |Z) (see §3.2).

We use the Multi30K dataset (Elliott et al., 2016),
originated from Flickr30K (Young et al., 2014) that
is comprised of Flicker images described with 1-5
English descriptions per image. Multi30K adds

Algorithm 2 The non-linear orthogonal iterations (NOI)
algorithm for DPCCA Variant B

Input: Data matrices X ∈ RDx×N , Y ∈ RDy×N ,
Z ∈ RDz×N , time constant ρ, learning rate η.

initialization: Initialize weights (WF ,VG,UH ).
Randomly choose a minibatch (Xb0 ,Yb0 ,Zb0 ).
Initialize covariances:
Σ̂FF ← N−1|b0| F (Xb0)F (Xb0)

T

Σ̂GG ← N−1|b0| G(Yb0)G(Yb0)
T

Σ̂HH ← N−1|b0| H(Zb0)H(Zb0)
T

Σ̂FH ← N−1|b0| F (Xb0)H(Zb0)
T

Σ̂GH ← N−1|b0| G(Yb0)H(Zb0)
T

for t = 1, 2, . . . , n do
Randomly choose a minibatch (Xbt ,Ybt ,Zbt ).
Update covariances:
Σ̂FF ← ρΣ̂FF + (1− ρ)N−1|bt| F (Xbt)F (Xbt)

T

Σ̂GG ← ρΣ̂GG + (1− ρ)N−1|bt| G(Ybt)G(Ybt)
T

Σ̂HH ← ρΣ̂HH + (1− ρ)N−1|bt| H(Zbt)H(Zbt)
T

Σ̂FH ← ρΣ̂FH + (1− ρ)N−1|bt| F (Xbt)H(Zbt)
T

Σ̂GH ← ρΣ̂GH + (1− ρ)N−1|bt| G(Ybt)H(Zbt)
T

Update conditional variables:
F |H ← F (Xbt)− Σ̂FHΣ̂

−1
HHH(Zbt)

G|H ← G(Ybt)− Σ̂GHΣ̂
−1
HHH(Zbt)

Σ̂FF |H ← Σ̂FF − Σ̂FHΣ̂−1HHΣ̂
T
FH

Σ̂GG|H ← Σ̂GG − Σ̂GHΣ̂−1HHΣ̂
T
GH

Fix G̃|H = Σ̂−
1
2

GG|HG|H , and compute ∇WF , ∇UH
with respect to:
min

WF ,UH

1
|bt|‖F |H − G̃|H‖

2
F

Update parameters:
WF ←WF − η∇WF ,UH ← UH − η∇UH
Fix F̃ |H = Σ̂−

1
2

FF |HF |H , and compute ∇VG, ∇UH
with respect to:
min

VG,UH

1
|bt|‖G|H − F̃ |H‖

2
F

Update parameters:
VG ← VG − η∇VG,UH ← UH − η∇UH
end for

Output: (WF ,VG,UH )

German descriptions to a total of 30,014 images:
most were written independently of the English de-
scriptions, while some are direct translations. Each
image is associated with one English and one Ger-
man description. We rely on the original Multi30K
splits with 29,000, 1,014, and 1,000 triplets for
training, validation, and test, respectively.

Multilingual Word Similarity The word simi-
larity task tests the correlation between automatic
and human generated word similarity scores. We
evaluate with the Multilingual SimLex-999 dataset
(Leviant and Reichart, 2015): the 999 English (EN)



916

EN-DE EN-IT EN-RU

Nouns 4606 4735 4106
Adjectives 405 416 348
Verbs 392 400 227
Adverbs 167 161 142
Prepositions 12 12 9

Total 5598 5740 4838

Table 1: WIW statistics: the number of WIW en-
tries across POS classes in each language pair. The
numbers of words per POS class are not summed
to the total number of words as other (less frequent)
POS tags are also represented.

word pairs from SimLex-999 (Hill et al., 2015)
were translated to German (DE), Italian (IT), and
Russian (RU), and similarity scores were crowd-
sourced from native speakers.

We introduce a new dataset termed Word-Image-
Word (WIW), which we use to train word-level
models for the multilingual word similarity task.
WIW contains three bilingual lexicons (EN-DE,
EN-IT, EN-RU) with images shared between words
in a lexicon entry. Each WIW entry is a triplet: an
English word, its translation in DE/IT/RU, and a
set of images relevant to the pair.

English words were taken from the January
2017 Wikipedia dump. After removing stop words
and punctuation, we extract the 6,000 most fre-
quent words from the cleaned corpus not present
in SimLex. DE/IT/RU words were obtained semi-
automatically from the EN words using Google
Translate. The images are crawled from the Bing
search engine using MMFeat9 (Kiela, 2016) by
querying the EN words only. Following the sugges-
tions from the study of Kiela et al. (2016), we save
the top 20 images as relevant images.10

Table 1 provides a summary of the WIW dataset.
The dataset contains both concrete and abstract
words, and words of different POS tags.11 This
property has an influence on the image collection:
similar to Kiela et al. (2014), we have noticed
that images of more concrete concepts are less dis-
persed (see also examples from Figure 2).

6 Experimental Setup

Data Preprocessing and Embeddings For the
sentence-level task, all descriptions were lower-

9https://github.com/douwekiela/mmfeat.
10Offensive words and images are manually cleaned.
11POS tag information is taken from the NLTK toolkit for

the English words.

Figure 2: WIW examples from each of the three
bilingual lexicons. Note that the designated words
can be either abstract (true), express an action
(dance) or be more concrete (plant).

cased and tokenized. Each sentence is represented
with one vector: the average of its word embed-
dings. For English, we rely on 500-dimensional En-
glish skip-gram word embeddings (Mikolov et al.,
2013) trained on the January 2017 Wikipedia dump
with bag-of-words contexts (window size of 5). For
German we use the deWaC 1.7B corpus (Baroni
et al., 2009) to obtain 500-dimensional German em-
beddings using the same word embedding model.
For word similarity, to be directly comparable to
previous work, we rely on 300-dim word vectors
in EN, DE, IT, and RU from Mrkšić et al. (2017).

Visual features are extracted from the penul-
timate layer (FC7) of the VGG-19 network (Si-
monyan and Zisserman, 2015), and compressed to
the dimensionality of the textual inputs by a Princi-
pal Component Analysis (PCA) step. For the word
similarity task, we average the visual vectors across
all images of each word pair as done in, e.g., (Vulić
et al., 2016), before the PCA step.

Baseline Models We consider a wide variety
of multi-view CCA-based baselines. First, we
compare against the original (linear) CCA model
(Hotelling, 1936), and its deep non-linear exten-
sion DCCA (Andrew et al., 2013). For DCCA:
1) we rely on its improved optimization algorithm
from Wang et al. (2015a) which uses a stochas-
tic approach with large minibatches; 2) we com-
pare against the DCCA NOI variant (Wang et al.,
2015b) described by Algorithm 1, and another re-
cent DCCA variant with the optimization algorithm
based on a stochastic decorrelational loss (Chang
et al., 2017) (DCCA SDL); and 3) we also test
the DCCA Autoencoder model (DCCAE) (Wang
et al., 2015a), which offers a trade-off between
maximizing the canonical correlation of two sets of
variables and finding informative features for their
reconstruction.

Another baseline is Generalized CCA (GCCA)
(Funaki and Nakayama, 2015; Horst, 1961; Rastogi
et al., 2015): a linear model which extends CCA to

https://github.com/douwekiela/mmfeat


917

three or more views. Unlike PCCA, GCCA does
not condition two variables on the third shared one,
but rather seeks to maximize the canonical correla-
tions of all pairs of views. We also compare to Non-
parametric CCA (NCCA) (Michaeli et al., 2016),
and to a probabilistic variant of PCCA (PPCCA,
Mukuta and Harada (2014)).

Finally, we compare with the two recent models
which operate in the setup most similar to ours: 1)
Bridge Correlational Networks (BCN) (Rajendran
et al., 2016); and 2) Image Pivoting (IMG PIVOT)
from Gella et al. (2017). For both models, we re-
port results only with the strongest variant based on
the findings from the original papers, also verified
by additional experimentation in our work.12

Hyperparameter Tuning The hyperparameters
of the different models are tuned with a grid search
over the following values: {2,3,4,5} for number
of layers, {tanh, sigmoid, ReLU} as the activation
functions (we use the same activation function in all
the layers of the same network), {64,128,256} for
minibatch size, {0.001,0.0001} for learning rate,
and {128,256} for L (the size of the output vectors).
The dimensions of all mid-layers are set to the input
size. We use the Adam optimizer (Kingma and Ba,
2015), with the number of epochs set to 300.

For all participating models, we report test per-
formance of the best hyperparameter on the valida-
tion set. For word similarity, following a standard
practice (Levy et al., 2015; Vulić et al., 2017) we
tune all models on one half of the SimLex data
and evaluate on the other half, and vice versa. The
reported score is the average of the two halves.
Similarity scores for all tasks were computed using
the cosine similarity measure.

7 Results and Discussion

Cross-lingual Image Description Retrieval
We report two standard evaluation metrics: 1)
Recall at 1 (R@1) scores, and 2) the sentence-level
BLEU+1 metric (Lin and Och, 2004), a variant
of BLEU which smooths terms for higher-order
n-grams, making it more suitable for evaluating
short sentences. The scores for the retrieval task
with all models are summarized in Table 2.

12 More details about preprocessing and baselines (includ-
ing all links to their code), are in the the supplementary mate-
rial. We use original readily available implementations of all
baselines whenever this is possible, and our in-house imple-
mentations for baselines for which no code is provided by the
original authors.

R@1 BLEU+1
Model EN→DE DE→EN EN→DE DE→EN

DPCCA (Variant A) 0.795 0.779 0.836 0.827
DPCCA (Variant B) 0.809 0.794 0.848 0.839

DPCCA(B)+DCCA NOI (concat) 0.826 0.791 0.863 0.837
DCCA NOI (Wang et al., 2015b) 0.812 0.788 0.849 0.830
DCCA SDL (Chang et al., 2017) 0.507 0.487 0.552 0.533

DCCA (Wang et al., 2015a) 0.619 0.621 0.664 0.673
DCCAE (Wang et al., 2015a) 0.564 0.542 0.607 0.598

IMG PIVOT (Gella et al., 2017) 0.772 0.763 0.789 0.781
BCN (Rajendran et al., 2016) 0.579 0.570 0.628 0.629

PCCA (Rao, 1969) 0.785 0.737 0.825 0.787
CCA (Hotelling, 1936) 0.764 0.704 0.803 0.754

GCCA (Funaki and Nakayama, 2015) 0.699 0.690 0.742 0.743
NCCA (Michaeli et al., 2016) 0.157 0.165 0.205 0.213

PPCCA (Mukuta and Harada, 2014) 0.035 0.050 0.063 0.086

Table 2: Results on cross-lingual image description
retrieval. NN-based models are above the dashed
line. Best overall results are in bold. Best results
with non-deep models are underlined.

The results clearly demonstrate the superiority
of DPCCA (with a slight advantage to the more
complex Variant B) and of the concatenation of
their representation with that of the DCCA NOI
(strongest) baseline. Furthermore, the non-deep,
linear PCCA achieves strong results: it outscores
all non-deep models, as well as all deep models
except from DCCA NOI, IMG PIVOT in one case,
and its deep version: DPCCA. This emphasizes our
contribution in proposing PCCA for multilingual
processing with images as a cross-lingual bridge.

The results suggest that: 1) the inclusion of vi-
sual information in the training process helps the
retrieval task even without such information during
inference. DPCCA outscores all DCCA variants
(either alone or through a concatenation with the
DCCA NOI representation), and PCCA outscores
the original two-view CCA model; and 2) deep,
non-linear architectures are useful: our DPCCA
outperforms the linear PCCA model.

We also note clear improvements over the two re-
cent models which also rely on visual information:
IMG PIVOT and BCN. The gain over IMG PIVOT
is observed despite the fact that IMG PIVOT is
a more complex multi-modal model which relies
on RNNs, and is tailored to sentence-level tasks.
Finally, the scores from Table 2 suggest that im-
proved performance can be achieved by an en-
semble model, that is, a simple concatenation of
DPCCA (B) and DCCA NOI.

Multilingual Word Similarity The results, pre-
sented as standard Spearman’s rank correlation
scores, are summarized in Table 3: we present
fine-grained results over different POS classes for
EN and DE, and compare them to the results from



918

English-German

Model EN-Adj EN-Verbs EN-Nouns DE-Adj DE-Verbs DE-Nouns

DPCCA (Variant A) 0.640 0.311 0.369 0.430 0.321 0.404
DPCCA (Variant B) 0.626 0.316 0.382 0.462 0.319 0.399

DCCA NOI (Wang et al., 2015b) 0.611 0.308 0.361 0.441 0.297 0.398
DCCA (Wang et al., 2015a) 0.618 0.261 0.327 0.404 0.290 0.362

PCCA (Rao, 1969) 0.614 0.296 0.340 0.305 0.143 0.340
CCA (Hotelling, 1936) 0.557 0.297 0.321 0.284 0.157 0.346

GCCA (Funaki and Nakayama, 2015) 0.636 0.280 0.378 0.446 0.277 0.398

INIT EMB 0.582 0.160 0.306 0.407 0.164 0.285

Table 3: Results on EN and DE SimLex-999 (POS-based evaluation). All scores are Spearman’s rank
correlations. INIT EMB refers to initial pre-trained monolingual word embeddings (see §6).

EN-DE WIW EN-IT WIW EN-RU WIW

Model EN DE EN IT EN RU

DPCCA (A) 0.398 0.400 0.412 0.429 0.404 0.407
DPCCA (B) 0.405 0.400 0.413 0.427 0.413 0.402

PCCA 0.374 0.301 0.370 0.386 0.374 0.374

DCCA NOI 0.390 0.398 0.413 0.422 0.407 0.398
GCCA 0.395 0.386 0.414 0.407 0.412 0.396

INIT EMB 0.321 0.278 0.321 0.361 0.321 0.385

Table 4: Results (Spearman rank correlation) of our
models and the strongest baselines on Multilingual
SimLex-999 (all data).

a selection of strongest baselines. Further, Table 4
presents results on all SimLex word pairs. The
POS class result patterns for EN-IT and EN-RU
are very similar to the patterns in Table 3 and are
provided in the supplementary material. First, the
results over the initial monolingual embeddings
before training (INIT EMB) clearly indicate that
multilingual information is beneficial for the word
similarity task. We observe improvements with all
models (the only exception being extremely low-
scoring PPCCA and NCCA, not shown). More-
over, by additionally grounding concepts from two
languages in the visual modality it is possible to
further boost word similarity scores. This result
is in line with prior work in monolingual settings
(Chrupała et al., 2015; Kiela and Bottou, 2014;
Lazaridou et al., 2015), which have shown to profit
from multi-modal features.

The results on the POS classes represented in
SimLex-999 (nouns, verbs, adjectives, Table 3)
form our main finding: conditioning the multilin-
gual representations on a shared image leads to im-
provements in verb and adjective representations.
While for nouns one of the DPCCA variants is
the best performing model for both languages, the
gaps from the best performing baselines are much
smaller. This is interesting since, e.g., verbs are

more abstract than nouns (Hartmann and Søgaard,
2017; Hill et al., 2014). Considering the fact that
SimLex-999 consists of 666 noun pairs, 222 verb
pairs and 111 adjective pairs, this is the reason that
the gains of DPCCA over the strongest baselines
across the entire evaluation set are more modest
(Table 4). We note again that the same patterns
presented in Table 3 for EN-DE – more promi-
nent verb and adjective gains and a smaller gain on
nouns – also hold for EN-IT and EN-RU (see the
supplementary material).

8 Conclusion and Future Work

We addressed the problem of utilizing images as a
bridge between languages to learn improved bilin-
gual text representations. Our main contribution
is two-fold. First, we proposed to use the Partial
CCA (PCCA) method. In addition, we proposed
a stochastic optimization algorithm for the deep
version of PCCA that overcomes the challenges
posed by the covariance estimation required by the
method. Our experiments reveal the effectiveness
of these methods for both sentence-level and word-
level tasks. Crucially, our proposed solution does
not require access to images at inference/test time,
in line with the realistic scenario where images that
describe sentential queries are not readily available.

In future work we plan to improve our meth-
ods by exploiting the internal structure of images
and sentences as well as by effectively integrating
signals from more than two languages.

Acknowledgments

IV is supported by the ERC Consolidator Grant
LEXICAL: Lexical Acquisition Across Languages
(no 648909). GR and RR are supported by the
Infomedia Magnet Grant and by an AOL grant on
”connected experience technologies”.



919

References
Galen Andrew, Raman Arora, Jeff Bilmes, and Karen

Livescu. 2013. Deep canonical correlation analysis.
In Proceedings of ICML, pages 1247–1255.

Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-
garet Mitchell, Dhruv Batra, Lawrence C. Zitnick,
and Devi Parikh. 2015. VQA: Visual question an-
swering. In Proceedings of ICCV, pages 2425–
2433.

Raman Arora and Karen Livescu. 2013. Multi-view
CCA-based acoustic features for phonetic recogni-
tion across speakers and domains. In Proceedings
of ICASSP, pages 7135–7139.

Marco Baroni, Silvia Bernardini, Adriano Ferraresi,
and Eros Zanchetta. 2009. The WaCky Wide Web:
A collection of very large linguistically processed
web-crawled corpora. Language Resources and
Evaluation, 43(3):209–226.

Lawrence W. Barsalou and Katja Wiemer-Hastings.
2005. Situating abstract concepts. In D. Pecher and
R. Zwaan, editors, Grounding cognition: The role
of perception and action in memory, language, and
thought, pages 129–163.

Shane Bergsma and Benjamin Van Durme. 2011.
Learning bilingual lexicons using the visual similar-
ity of labeled web images. In Proceedings of IJCAI,
pages 1764–1769.

Elia Bruni, Nam Khanh Tram, and Marco Baroni. 2014.
Multimodal distributional semantics. Journal of Ar-
tificial Intelligence Research, 49:1–47.

Richard H Byrd, Peihuang Lu, Jorge Nocedal, and
Ciyou Zhu. 1995. A limited memory algorithm for
bound constrained optimization. SIAM Journal on
Scientific Computing, 16(5):1190–1208.

Iacer Calixto and Qun Liu. 2017. Incorporating global
visual features into attention-based neural machine
translation. In Proceedings of EMNLP, pages 992–
1003.

Iacer Calixto, Qun Liu, and Nick Campbell.
2017. Multilingual multi-modal embeddings
for natural language processing. arXiv preprint
arXiv:1702.01101.

Sarath Chandar, Mitesh M Khapra, Hugo Larochelle,
and Balaraman Ravindran. 2016. Correlational neu-
ral networks. Neural Computation, 28:257–285.

Xiaobin Chang, Tao Xiang, and Timothy M.
Hospedales. 2017. Deep multi-view learn-
ing with stochastic decorrelation loss. CoRR,
abs/1707.09669.

Grzegorz Chrupała, Ákos Kádár, and Afra Alishahi.
2015. Learning language through pictures. In Pro-
ceedings of ACL, pages 112–118.

Desmond Elliott, Stella Frank, and Eva Hasler. 2015.
Multilingual image description with neural sequence
models. arXiv preprint arXiv:1510.04709.

Desmond Elliott, Stella Frank, Khalil Sima’an, and Lu-
cia Specia. 2016. Multi30K: Multilingual English-
German image descriptions. In Proceedings of the
5th Workshop on Vision and Language, pages 70–
74.

Ali Farhadi, Mohsen Hejrati, Mohammad Amin
Sadeghi, Peter Young, Cyrus Rashtchian, Julia
Hockenmaier, and David Forsyth. 2010. Every pic-
ture tells a story: Generating sentences from images.
In Proceedings of ECCV, pages 15–29.

Manaal Faruqui and Chris Dyer. 2014. Improving vec-
tor space word representations using multilingual
correlation. In Proceedings of EACL, pages 462–
471.

Yansong Feng and Mirella Lapata. 2010. Visual infor-
mation in semantic representation. In Proceedings
of NAACL-HLT, pages 91–99.

Akira Fukui, Dong Huk Park, Daylen Yang, Anna
Rohrbach, Trevor Darrell, and Marcus Rohrbach.
2016. Multimodal compact bilinear pooling for vi-
sual question answering and visual grounding. In
Proceedings of EMNLP, pages 457–468.

Ruka Funaki and Hideki Nakayama. 2015. Image-
mediated learning for zero-shot cross-lingual docu-
ment retrieval. In Proceedings of EMNLP, pages
585–590.

Spandana Gella, Mirella Lapata, and Frank Keller.
2016. Unsupervised visual sense disambiguation for
verbs using multimodal embeddings. In Proceed-
ings of NAACL-HLT, pages 182–192.

Spandana Gella, Rico Sennrich, Frank Keller, and
Mirella Lapata. 2017. Image pivoting for learning
multilingual multimodal representations. In Pro-
ceedings of EMNLP, pages 2839–2845.

Goran Glavaš, Ivan Vulić, and Simone Paolo Ponzetto.
2017. If sentences could see: Investigating visual
information for semantic textual similarity. In Pro-
ceedings of IWCS.

Stevan Harnad. 1990. The symbol grounding problem.
Physica D: Nonlinear Phenomena, 42(1–3).

Mareike Hartmann and Anders Søgaard. 2017. Limi-
tations of cross-lingual learning from image search.
CoRR, abs/1709.05914.

Felix Hill, Roi Reichart, and Anna Korhonen. 2014.
Multi-modal models for concrete and abstract con-
cept meaning. Transactions of the ACL, 2:285–296.

Felix Hill, Roi Reichart, and Anna Korhonen. 2015.
Simlex-999: Evaluating semantic models with (gen-
uine) similarity estimation. Computational Linguis-
tics, 41(4):665–695.

http://proceedings.mlr.press/v28/andrew13.html
https://arxiv.org/abs/1505.00468
https://arxiv.org/abs/1505.00468
http://ieeexplore.ieee.org/document/6639047
http://ieeexplore.ieee.org/document/6639047
http://ieeexplore.ieee.org/document/6639047
https://doi.org/10.1007/s10579-009-9081-4
https://doi.org/10.1007/s10579-009-9081-4
https://doi.org/10.1007/s10579-009-9081-4
http://www.cogsci.ucsd.edu/$\sim $coulson/203/pecher-zwaan.pdf
https://dl.acm.org/citation.cfm?id=2283698
https://dl.acm.org/citation.cfm?id=2283698
https://www.jair.org/media/4135/live-4135-7609-jair.pdf
http://epubs.siam.org/doi/abs/10.1137/0916069
http://epubs.siam.org/doi/abs/10.1137/0916069
http://aclweb.org/anthology/D17-1105
http://aclweb.org/anthology/D17-1105
http://aclweb.org/anthology/D17-1105
https://arxiv.org/abs/1702.01101
https://arxiv.org/abs/1702.01101
https://arxiv.org/abs/1504.07225
https://arxiv.org/abs/1504.07225
http://arxiv.org/abs/1707.09669
http://arxiv.org/abs/1707.09669
http://www.anthology.aclweb.org/P/P15/P15-2019.pdf
https://arxiv.org/abs/1510.04709
https://arxiv.org/abs/1510.04709
http://www.aclweb.org/anthology/W16-3210
http://www.aclweb.org/anthology/W16-3210
https://dl.acm.org/citation.cfm?id=1888092
https://dl.acm.org/citation.cfm?id=1888092
http://www.aclweb.org/anthology/E14-1049
http://www.aclweb.org/anthology/E14-1049
http://www.aclweb.org/anthology/E14-1049
https://dl.acm.org/citation.cfm?id=1858010
https://dl.acm.org/citation.cfm?id=1858010
http://anthology.aclweb.org/D/D16/D16-1044.pdf
http://anthology.aclweb.org/D/D16/D16-1044.pdf
http://www.aclweb.org/anthology/D15-1070
http://www.aclweb.org/anthology/D15-1070
http://www.aclweb.org/anthology/D15-1070
http://anthology.aclweb.org/N/N16/N16-1022.pdf
http://anthology.aclweb.org/N/N16/N16-1022.pdf
http://www.aclweb.org/anthology/D17-1303
http://www.aclweb.org/anthology/D17-1303
http://aclweb.org/anthology/W/W17/W17-6809.pdf
http://aclweb.org/anthology/W/W17/W17-6809.pdf
https://doi.org/10.1016/0167-2789(90)90087-6
http://arxiv.org/abs/1709.05914
http://arxiv.org/abs/1709.05914
http://aclweb.org/anthology/Q/Q14/Q14-1023.pdf
http://aclweb.org/anthology/Q/Q14/Q14-1023.pdf
http://www.aclweb.org/anthology/J15-4004
http://www.aclweb.org/anthology/J15-4004


920

Julian Hitschler, Shigehiko Schamoni, and Stefan Rie-
zler. 2016. Multimodal pivots for image caption
translation. In Proceedings of ACL, pages 2399–
2409.

Berthold K.P. Horn, Hugh M. Hilden, and Shahriar Ne-
gahdaripour. 1988. Closed-form solution of abso-
lute orientation using orthonormal matrices. Journal
of Optical Society of America, 5(7):1127–1135.

Paul Horst. 1961. Generalized canonical correlations
and their applications to experimental data. Journal
of Clinical Psychology, 17(4):331–347.

Harold Hotelling. 1936. Relations between two sets of
variates. Biometrika, 28(3/4):321–377.

Po-Yao Huang, Frederick Liu, Sz-Rung Shiang, Jean
Oh, and Chris Dyer. 2016. Attention-based multi-
modal neural machine translation. In Proceedings
of WMT, pages 639–645.

Agnan Kessy, Alex Lewin, and Korbinian Strimmer.
2017. Optimal whitening and decorrelation. The
American Statistician.

Douwe Kiela. 2016. MMFeat: A toolkit for extracting
multi-modal features. In Proceedings of ACL Sys-
tem Demonstrations, pages 55–60.

Douwe Kiela and Léon Bottou. 2014. Learning image
embeddings using convolutional neural networks for
improved multi-modal semantics. In Proceedings of
EMNLP, pages 36–45.

Douwe Kiela, Felix Hill, Anna Korhonen, and Stephen
Clark. 2014. Improving multi-modal representa-
tions using image dispersion: Why less is sometimes
more. In Proceedings of ACL, pages 835–841.

Douwe Kiela, Anita Lilla Verő, and Stephen Clark.
2016. Comparing data sources and architectures for
deep visual representation learning in semantics. In
Proceedings of EMNLP, pages 447–456.

Douwe Kiela, Ivan Vulić, and Stephen Clark. 2015.
Visual bilingual lexicon induction with transferred
ConvNet features. In Proceedings of EMNLP, pages
148–158.

Diederik Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. In Proceedings
of ICLR (Conference Track).

Ryan Kiros, Ruslan Salakhutdinov, and Rich Zemel.
2014. Multimodal neural language models. In Pro-
ceedings of ICML, pages 595–603.

George Lakoff and Mark Johnson. 1999. Philosophy
in the flesh: The embodied mind and its challenge to
Western thought.

Angeliki Lazaridou, Nghia The Pham, and Marco Ba-
roni. 2015. Combining language and vision with
a multimodal skip-gram model. In Proceedings of
NAACL-HLT, pages 153–163.

Ira Leviant and Roi Reichart. 2015. Judgment lan-
guage matters: Multilingual vector space models for
judgment language aware lexical semantics. CoRR,
abs/1508.00106.

Omer Levy, Yoav Goldberg, and Ido Dagan. 2015. Im-
proving distributional similarity with lessons learned
from word embeddings. Transactions of the ACL,
3:211–225.

Chin-Yew Lin and Franz Josef Och. 2004. ORANGE:
A method for evaluating automatic evaluation met-
rics for machine translation. In Proceedings of COL-
ING, pages 501–507.

Max M. Louwerse. 2011. Symbol interdependency in
symbolic and embodied cognition. Topics in Cogni-
tive Science, 59(1):617–645.

Ang Lu, Weiran Wang, Mohit Bansal, Kevin Gimpel,
and Karen Livescu. 2015. Deep multilingual corre-
lation for improved word embeddings. In Proceed-
ings of NAACL-HLT, pages 250–256.

Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, Zhi-
heng Huang, and Alan Yuille. 2015. Deep caption-
ing with multimodal recurrent neural networks (m-
RNN). In Proceedings of ICLR (Conference Track).

Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, and
Alan L Yuille. 2014. Explain images with multi-
modal recurrent neural networks. arXiv preprint
arXiv:1410.1090.

Tomer Michaeli, Weiran Wang, and Karen Livescu.
2016. Nonparametric canonical correlation analysis.
In Proceedings of ICML, pages 1967–1976.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word representa-
tions in vector space. In Proceedings of ICLR (Con-
ference Track).

Nikola Mrkšić, Ivan Vulić, Diarmuid Ó Séaghdha, Ira
Leviant, Roi Reichart, Milica Gašić, Anna Korho-
nen, and Steve Young. 2017. Semantic specializa-
tion of distributional word vector spaces using mono-
lingual and cross-lingual constraints. Transactions
of the ACL, 5(1):309–324.

Yusuke Mukuta and Harada. 2014. Probabilistic par-
tial canonical correlation analysis. In Proceedings
of ICML, pages 1449–1457.

Hideki Nakayama and Noriki Nishida. 2017. Zero-
resource machine translation by multimodal
encoder–decoder network with multimedia pivot.
Machine Translation, 31(1-2):49–64.

Janarthanan Rajendran, Mitesh M. Khapra, Sarath
Chandar, and Balaraman Ravindran. 2016. Bridge
correlational neural networks for multilingual mul-
timodal representation learning. In Proceedings of
NAACL-HLT, pages 171–181.

http://www.aclweb.org/anthology/P16-1227
http://www.aclweb.org/anthology/P16-1227
https://www.osapublishing.org/josaa/abstract.cfm?\URI=josaa-4-4-629
https://www.osapublishing.org/josaa/abstract.cfm?\URI=josaa-4-4-629
https://doi.org/10.1002/1097-4679(196110)17:4<331::AID-JCLP2270170402>3.0.CO;2-D
https://doi.org/10.1002/1097-4679(196110)17:4<331::AID-JCLP2270170402>3.0.CO;2-D
http://www.jstor.org/stable/2333955?seq=1
http://www.jstor.org/stable/2333955?seq=1
http://www.aclweb.org/anthology/W16-2360
http://www.aclweb.org/anthology/W16-2360
https://arxiv.org/abs/1512.00809
http://www.aclweb.org/anthology/P16-4010
http://www.aclweb.org/anthology/P16-4010
http://www.aclweb.org/anthology/D14-1005
http://www.aclweb.org/anthology/D14-1005
http://www.aclweb.org/anthology/D14-1005
http://www.aclweb.org/anthology/P14-2135
http://www.aclweb.org/anthology/P14-2135
http://www.aclweb.org/anthology/P14-2135
http://www.aclweb.org/anthology/D16-1043
http://www.aclweb.org/anthology/D16-1043
http://aclweb.org/anthology/D15-1015
http://aclweb.org/anthology/D15-1015
https://arxiv.org/abs/1412.6980
https://arxiv.org/abs/1412.6980
http://proceedings.mlr.press/v32/kiros14.html
http://www.nytimes.com/books/first/l/lakoff-philosophy.html
http://www.nytimes.com/books/first/l/lakoff-philosophy.html
http://www.nytimes.com/books/first/l/lakoff-philosophy.html
http://www.aclweb.org/anthology/N15-1016
http://www.aclweb.org/anthology/N15-1016
https://arxiv.org/abs/1508.00106
https://arxiv.org/abs/1508.00106
https://arxiv.org/abs/1508.00106
http://anthology.aclweb.org/Q/Q15/Q15-1016.pdf
http://anthology.aclweb.org/Q/Q15/Q15-1016.pdf
http://anthology.aclweb.org/Q/Q15/Q15-1016.pdf
https://dl.acm.org/citation.cfm?id=1220427
https://dl.acm.org/citation.cfm?id=1220427
https://dl.acm.org/citation.cfm?id=1220427
https://doi.org/10.1111/j.1756-8765.2010.01106.x
https://doi.org/10.1111/j.1756-8765.2010.01106.x
http://www.aclweb.org/anthology/N15-1028
http://www.aclweb.org/anthology/N15-1028
https://arxiv.org/abs/1412.6632
https://arxiv.org/abs/1412.6632
https://arxiv.org/abs/1412.6632
https://arxiv.org/abs/1410.1090
https://arxiv.org/abs/1410.1090
http://proceedings.mlr.press/v48/michaeli16.html
https://arxiv.org/abs/1301.3781
https://arxiv.org/abs/1301.3781
http://www.aclweb.org/anthology/Q17-1022
http://www.aclweb.org/anthology/Q17-1022
http://www.aclweb.org/anthology/Q17-1022
http://proceedings.mlr.press/v32/mukuta14.html
http://proceedings.mlr.press/v32/mukuta14.html
https://link.springer.com/article/10.1007/s10590-017-9197-z
https://link.springer.com/article/10.1007/s10590-017-9197-z
https://link.springer.com/article/10.1007/s10590-017-9197-z
http://www.aclweb.org/anthology/N16-1021
http://www.aclweb.org/anthology/N16-1021
http://www.aclweb.org/anthology/N16-1021


921

B. Raja Rao. 1969. Partial canonical correlations. Tra-
bajos de estadistica y de investigación operativa,
20(2-3):211–219.

Pushpendre Rastogi, Benjamin Van Durme, and Raman
Arora. 2015. Multiview LSA: Representation learn-
ing via generalized CCA. In Proceedings of NAACL-
HLT, pages 556–566.

Karen Simonyan and Andrew Zisserman. 2015. Very
deep convolutional networks for large-scale image
recognition. In Proceedings of ICLR (Workshop
Track).

Oriol Vinyals, Alexander Toshev, Samy Bengio, and
Dumitru Erhan. 2015. Show and tell: A neural im-
age caption generator. In Proceedings of CVPR,
pages 3156–3164.

Ivan Vulić, Douwe Kiela, Stephen Clark, and Marie-
Francine Moens. 2016. Multi-modal representations
for improved bilingual lexicon learning. In Proceed-
ings of ACL, pages 188–194. ACL.

Ivan Vulić, Roy Schwartz, Ari Rappoport, Roi Reichart,
and Anna Korhonen. 2017. Automatic selection of
context configurations for improved class-specific
word representations. In Proceedings of CoNLL,
pages 112–122.

Weiran Wang, Raman Arora, Karen Livescu, and Jeff
Bilmes. 2015a. On deep multi-view representation
learning. In Proceedings of ICML, pages 1083–
1092.

Weiran Wang, Raman Arora, Karen Livescu, and
Nathan Srebro. 2015b. Stochastic optimization for
deep CCA via nonlinear orthogonal iterations. In
Proceedings of Communication, Control, and Com-
puting, pages 688–695.

Huijuan Xu and Kate Saenko. 2016. Ask, attend and
answer: Exploring question-guided spatial attention
for visual question answering. In Proceedings of
ECCV, pages 451–466.

Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho,
Aaron Courville, Ruslan Salakhudinov, Rich Zemel,
and Yoshua Bengio. 2015. Show, attend and tell:
Neural image caption generation with visual atten-
tion. In Proceedings of ICML, pages 2048–2057.

Fei Yan and Krystian Mikolajczyk. 2015. Deep corre-
lation for matching images and text. In Proceedings
of CVPR, pages 3441–3450.

Peter Young, Alice Lai, Micah Hodosh, and Julia Hock-
enmaier. 2014. From image descriptions to visual
denotations: New similarity metrics for semantic in-
ference over event descriptions. Transactions of the
ACL, 2:67–78.

http://link.springer.com/article/10.1007/BF03028532
http://www.aclweb.org/anthology/N15-1058
http://www.aclweb.org/anthology/N15-1058
https://arxiv.org/abs/1409.1556
https://arxiv.org/abs/1409.1556
https://arxiv.org/abs/1409.1556
http://ieeexplore.ieee.org/document/7298935/
http://ieeexplore.ieee.org/document/7298935/
http://anthology.aclweb.org/P/P16/P16-2.pdf
http://anthology.aclweb.org/P/P16/P16-2.pdf
http://aclweb.org/anthology/K17-1013
http://aclweb.org/anthology/K17-1013
http://aclweb.org/anthology/K17-1013
http://proceedings.mlr.press/v37/wangb15.html
http://proceedings.mlr.press/v37/wangb15.html
https://doi.org/10.1109/ALLERTON.2015.7447071
https://doi.org/10.1109/ALLERTON.2015.7447071
https://link.springer.com/chapter/10.1007/978-3-319-46478-7_28
https://link.springer.com/chapter/10.1007/978-3-319-46478-7_28
https://link.springer.com/chapter/10.1007/978-3-319-46478-7_28
https://arxiv.org/abs/1502.03044
https://arxiv.org/abs/1502.03044
https://arxiv.org/abs/1502.03044
http://ieeexplore.ieee.org/document/7298966/
http://ieeexplore.ieee.org/document/7298966/
http://aclweb.org/anthology/Q/Q14/Q14-1006.pdf
http://aclweb.org/anthology/Q/Q14/Q14-1006.pdf
http://aclweb.org/anthology/Q/Q14/Q14-1006.pdf

