



















































Trust, but Verify! Better Entity Linking through Automatic Verification


Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 828–838,
Valencia, Spain, April 3-7, 2017. c©2017 Association for Computational Linguistics

Trust, but Verify!
Better Entity Linking through Automatic Verification

Benjamin Heinzerling∗
AIPHES

Heidelberg Institute for
Theoretical Studies

benjamin.heinzerling@h-its.org

Michael Strube
Heidelberg Institute for

Theoretical Studies
michael.strube@h-its.org

Chin-Yew Lin
Microsoft Research
cyl@microsoft.com

Abstract

We introduce automatic verification as
a post-processing step for entity linking
(EL). The proposed method trusts EL sys-
tem results collectively, by assuming en-
tity mentions are mostly linked correctly,
in order to create a semantic profile of the
given text using geospatial and temporal
information, as well as fine-grained entity
types. This profile is then used to auto-
matically verify each linked mention indi-
vidually, i.e., to predict whether it has been
linked correctly or not. Verification allows
leveraging a rich set of global and pairwise
features that would be prohibitively expen-
sive for EL systems employing global in-
ference. Evaluation shows consistent im-
provements across datasets and systems.
In particular, when applied to state-of-the-
art systems, our method yields an abso-
lute improvement in linking performance
of up to 1.7 F1 on AIDA/CoNLL’03 and
up to 2.4 F1 on the English TAC KBP
2015 TEDL dataset.

1 Introduction

Entity linking (EL) is the task of automatically
linking mentions of entities such as persons, loca-
tions, or organizations to their corresponding entry
in a knowledge base (KB). The task is generally
approached by generating a set of candidate enti-
ties1 for a given mention and then ranking those
candidates. Approaches differ in whether they
rank a mention’s candidates independently of the
candidates of other mentions (“local inference”) or
∗The majority of this work was done during an internship

at Microsoft Research Asia.
1We use entity to refer to both real-word entities and to

their corresponding entries in the KB.

whether they rank all candidates of all mentions si-
multaneously by incorporating a global coherence
measure into the optimization goal (“global infer-
ence”).

While linguistically well-founded in the con-
cept of lexical cohesion (Halliday and Hasan,
1976), global inference approaches (Kulkarni et
al., 2009; Hoffart et al., 2011a) do not scale
well with number of mentions and number of
candidate entities. In contrast, local approaches
do not suffer from scalability issues, since they
only optimize the similarity between mention con-
text and candidate KB entry text (Bunescu and
Paşca, 2006; Cucerzan, 2007), usually also includ-
ing a popularity prior2 (Milne and Witten, 2008;
Spitkovsky and Chang, 2012). Recent local ap-
proaches achieve state-of-the-art results by using
convolutional neural networks to capture similar-
ity at multiple context sizes (Francis-Landau et al.,
2016), but, by definition, fail to take global coher-
ence into account.

To avoid the trade-off between the efficiency of
local inference on the one hand and the coherence
benefits of global inference on the other, we pro-
pose a two-stage approach: In the first stage, can-
didate entities are ranked by a fast, local inference-
based EL system. In the second stage these results
are used to create a semantic profile of the given
text, derived from rich data the KB contains about
the top-ranked candidates. Since the linking pre-
cision of current EL systems is relatively high, we
trust that this profile is reasonably accurate and
leverage it to measure the cohesive strength be-
tween a given candidate entity and the other linked
entities mentioned in the text. We then automati-
cally verify the first stage results by classifying en-
tity links as correct if they display high coherence,
and as wrong if there are only weak or no cohesive

2Also referred to as commonness prior by some authors.

828



ties to the semantic profile. Verification results can
be used in at least three ways:

1. To increase linking precision by filtering out
all entity links classified as wrong;

2. To rerank candidate entities by the class prob-
ability estimated by the verifier, i.e., prefer
candidates that were predicted as correct with
higher probability; or

3. To employ a more sophisticated EL system
to re-link all entity links classified as wrong,
using the entity links deemed correct as addi-
tional context.

In this work we investigate options 1. and 2., and
make the following contributions:

• We propose automatic verification as a post-
processing step for EL systems;

• We propose global coherence features based
on notions of entity type coherence, geo-
graphic coherence, and temporal coherence;

• We show how these novel features, as well as
features developed in prior work, can be used
to verify EL results; and

• We show that automatic verification consis-
tently improves linking performance in an
evaluation across two datasets and seven dif-
ferent EL systems.

2 Method

We cast entity linking verification as a supervised
classification task. Given EL system output on a
training set with gold standard linked entity an-
notations, we extract global, pairwise, and local
features and train a classifier to predict whether a
given mention has been linked correctly by the EL
system.

In the standard EL setting, global inference is
an NP-hard problem, since all combinations of
all candidate entities of all mentions are consid-
ered simultaneously. In our proposed automatic
verification setting, however, taking only the top
candidate entities into account allows us to em-
ploy knowledge-rich, global coherence features
that would be prohibitively expensive otherwise.

Figure 1: Example showing a geographical out-
lier: Breeder’ Stakes (red, in Canada) and contex-
tual entities located in Ireland and the UK (green).

2.1 Aspects of Global Coherence
Global coherence captures how well a candidate
entity fits into the overall semantic profile of a text.
Current global inference approaches optimize a
single coherence measure, most commonly a mea-
sure of general semantic relatedness such as the
Milne-Witten distance (Milne and Witten, 2008),
or keyphrase overlap relatedness (KORE) (Hoffart
et al., 2012).

In contrast, verification allows employing many
global coherence features, which we categorize
according to four aspects of coherence: geograph-
ical coherence and temporal coherence, which to
our knowledge have not been used before in EL,
as well as entity type coherence and the general
semantic relatedness mentioned above.

2.1.1 Geographic Coherence
Entities mentioned in a text tend to be geographi-
cally close or clustered around very few locations.
We use this observation to identify geographic out-
liers as potential entity linking mistakes.

For example, consider the mention Breed-
ers Stakes in the following excerpt (CoNLL
1112testa):

DUBLIN 1996-08-31 Result of the Tat-
tersalls Breeders Stakes , a race for two-
year-olds run over six furlongs at The
Curragh ...

DUBLIN, Tattersalls (a company doing business
in the UK and Ireland), and The Curragh (a

829



Predicate

:location.location.geolocation
:organization.organization.geographic_scope
:time.event.locations
:sports.sports_team.location
:organization.organization.headquarters

Table 1: Freebase predicates for querying geo-
coordinates of locations, geo-political entities, and
organizations.

Predicate

:people.person.date_of_birth
:organization.organization.date_founded
:sports.sports_team.founded
:location.dated_location.date_founded
:time.event.start_date
:film.film.initial_release_date
:music.album.release_date
:music.release.release_date
:architecture.structure.construction_started
:architecture.structure.opened

:people.deceased_person.date_of_death
:location.dated_location.date_dissolved
:time.event.end_date
:business.defunct_company.ceased_operations
:architecture.structure.closed

Table 2: Freebase predicates for querying the be-
gin (top) and end (bottom) of an entity’s temporal
range.

horse race track in Ireland) clearly situate the
text in Ireland (cf. Figure 1). However, some
current EL systems link Breeder Stakes to the
Wikipedia article about the Canadian horse race of
the same name, since the Irish race does not have
a Wikipedia article and other evidence3 suggests a
strong match.

We aim to identify these kinds of errors by first
querying locations (Table 1) of all linked mentions
in the document, and then performing geographic
outlier detection4. This yields a binary feature in-
dicating whether a candidate entity is a geographic
outlier or not.

Since outliers are rare and hence the resulting
features sparse, we also also add a feature for the
average geographic distance d̄(d, E) of a candi-
date entity e to all other entities in document D:

d̄(e, D) =

∑
e′∈D\e d(e, e

′)
|D| − 1

where d(e, e′) is the geographic distance between
entities e and e′, and |D| is the number of entities

3Specifically, high context-similarity due to the appositive
race, and an almost perfect string match between mention and
Wikipedia title.

4We use an ensemble of standard outlier detection algo-
rithms provided by the ELKI clustering toolkit (Achtert et
al., 2011).

mentioned in D. This feature is based on the in-
tuition that a candidate entity which is geograph-
ically closer to other entities is more likely to be
correct than a distant one.

Geographic scope varies across documents. For
example, entities mentioned in a text about world
politics will be geographically more distant than
entities in a text about a local business). As a
scale-invariant distance measure s(e, D), we di-
vide the average distance d̄(e, D) by the average
distance between all other entities:

s(e, D) = d̄(e, D)/

∑
e′,e′′∈D\e d(e

′, e′′)
|e′, e′′ ∈ D \ e|

2.1.2 Temporal Coherence
Applying the notion of coherence to the tempo-
ral dimension, we observe that entities mentioned
in a text tend to be temporally close or clustered
around a few points in time.

Entities are associated with temporal ranges
with a begin, i.e. the point in time at which the en-
tity comes into existence, and an end, i.e. the point
in time at which the entity ceases to exists. Using
the same approach as in geographical outlier de-
tection, we perform temporal outlier detection on
all begin and end times associated with linked en-
tities in the given text, and declare a candidate en-
tity as an temporal outlier if both its begin and end
were detected as outliers.

Since temporal outliers are rare, we also add a
feature aiming to capture temporal proximity and
distance in a softer fashion with higher coverage;
by calculating the total overlap T (e, D) between
the temporal range t(e) of a candidate entity e, and
the known temporal ranges of all other linked en-
tities in the document D:

T (e, D) =
∑

e′∈D\e

∣∣t(e) ∩ t(e′)∣∣
where |t(e) ∩ t(e′)| is the length of the overlap be-
tween the temporal ranges of entities e and e′.5

Analogously to the geographic distance feature,
we take temporal proximity, i.e. a large over-
lap with other temporal ranges, as evidence for
a correctly linked entity, and temporal distance,
i.e. only small or no overlap with other temporal

5We also extract this feature normalized by the number of
entity mentions in the document, but did not see any effect.
This is likely due to little variation in the number of entities
per document for which the KB contains temporal informa-
tion.

830



ranges, as evidence for a linking mistake. Tem-
poral ranges are queried from the KB using the
predicates shown in Table 2.

The final feature using temporal information
checks whether an entity’s temporal ranges con-
tains the document’s creation date. This feature is
based on the intuition that, especially in the news
genre, an existing entity is more likely to be men-
tioned than an entity that has already ceased to ex-
ist or did not exists at the time of writing. The
document creation date is either trivially obtained
if metadata is present, or heuristically by using the
first date found in the document text by the Heidel-
Time temporal tagger (Strötgen and Gertz, 2010).

2.1.3 Entity Type Coherence
Frequency statistics of the types of entities men-
tioned in a text are an indicator of what the text is
about. For example, looking at the entity type dis-
tribution shown in Table 3, we can tell that the cor-
responding text appears to be about rugby teams.
Unlike other methods for representing the “about-
ness” of a text, such as topic models, entity type
statistics are grounded in the KB, thus offering a
simple method of measuring the relatedness be-
tween entities in terms of their types via the simi-
larity of their type distributions.

Specifically, we model entity type coherence
between a given candidate entity e and all other
linked entities in document D as the cosine sim-
ilarity of the respective type distributions. Type
frequencies are TF-IDF weighted, in order to
discount frequent types (e.g. :base:tagit.
concept) and give more importance to salient
types occurring in the document (e.g. :base.
rugby.rugby_club):

cohtype(e, D) = sim(types(e), tfidf(types(D)))

where sim is the cosine similarity, types(e) a bi-
nary vector indicating the types of entity e, and
types(D) a vector whose entries are occurrence
counts of entity types in document D, which are
weighted by tfidf .

2.1.4 Semantic Relatedness
Measures of generic semantic relatedness are a
standard feature in global inference systems. We
add features for the average and maximum seman-
tic relatedness SemRel(e, D) of a candidate en-
tity e with respect to all other entities e′ mentioned
in document D, using two semantic relatedness
measures:

SemRelmax(e, D) = maxe′∈D\eSemDist(e, e′)

SemRelavg(e, D) = avge′∈D\eSemDist(e, e′)

where max and avg are the maximum and aver-
age operators. SemDist denotes either the Milne-
Witten Distance (Milne and Witten, 2008), which
defines relatedness of Wikipedia entries in terms
of shared incoming article links, or the Normal-
ized Freebase Distance (Godin et al., 2014), an
adaptation of the Milne-Witten Distance to Free-
base entities.

2.2 Pairwise Features

Semantic relation: Given a pair consisting of
a candidate entities and an entity mention in its
context, we add a feature encoding whether a (and
if yes which) semantic relation exists between the
two entities. We add different features depend-
ing on the type of context in which the entity pair
occurs: in the same sentence, within a fixed to-
ken window, and within the same noun phrase.
For example, in the noun phrase German Chan-
cellor Angela Merkel, we find a wasBornIn
and a isLeaderOf relation between YAGO en-
tities ANGELA MERKEL6 and GERMANY. We
expect this feature to be sparse, but strong evi-
dence for both arguments of the identified rela-
tion being linked correctly. We record the rela-
tion type, as some relations tend to be more infor-
mative than others, e.g., the playsFor relation,
which holds between players and sports teams,

6In this work, SMALL CAPS denote both real-world enti-
ties and their corresponding entries in the knowledge base.

TF-IDF Count Type

1115.67 2 :base.rugby.rugby_club
243.62 3 :organization.organization
231.76 2 :base.schemastaging.sports_team_extra
183.49 2 :sports.sports_team

56.34 2 :base.tagit.concept

Table 3: Entity type distribution in a document about rugby, sorted by type TF-IDF.

831



should provide stronger evidence than the less spe-
cific isCitizenOf relation, which holds be-
tween citizens and countries.
Person name consistency: Having observed that
some local inference systems tend to make the
mistake of linking a full name mention (e.g. “John
Smith”) to one entity, and a coreferent surname-
only mention (“Smith”) to a different one, we add
a binary feature that indicates whether a candidate
entity assigned to a partial person name mention
agrees with its unambiguous full name antecedent.

2.3 Local Features

Since the global and pairwise features do not have
high enough coverage to provide evidence for all
linked candidate entities, we employ local features
that are devised to capture similarity between a
candidate entity and its textual context. As these
features are commonly used in EL systems, we
only give brief descriptions for completeness.
Popularity prior: The prior probability of the
candidate entity given its mention, obtained from
the CrossWikis dictionary (Spitkovsky and Chang,
2012). This feature aims to cover unambiguous
and almost unambiguous mentions.
Entity type agreement: A binary feature indicat-
ing whether the candidate entity type, as found in
the KB agrees with the named entity type, as deter-
mined by the NER system during preprocessing.
Keyphrase match: Knowledge bases contain
various sources of key phrases, such as labels and
aliases of semantic types, or salient noun phrases
in description texts, e.g., noun phrases occurring
in the first, defining sentence of a Wikipedia arti-
cle. We add a binary feature indicating whether a
known keyphrase occurs in the context of a given
candidate entity.
Demonym match: This binary feature indicates
whether a mention is a demonym of its linked en-
tity, e.g., the mention text French is a demonym
match for the entity FRANCE.
Mention-entity string match: Finally, we ex-
tract features from the string similarity between
a mention and the known labels and aliases of
a candidate entity. The similarity measures in-
clude exact match, case-insensitive match, head
match, match with stop words filtered, fuzzy
string match, Levenshtein distance, and abbrevi-
ation pattern matches, as well as different combi-
nations of these.

Dataset CoNLL TAC15

Entity Type 99.2 98.5
Geographic 62.9 41.5
Temporal 87.6 79.4

Table 4: KB coverage of our proposed global co-
herence features. Shown are the percentages of
in-KB mentions in each dataset for which the KB
(YAGO or Freebase) contains the required infor-
mation for each coherence feature set.

3 Experiments

We evaluate our automatic verification method
by applying it to the entity linking results pro-
duced by seven systems on two standard datasets:
CoNLL, which consists of 1393 Reuters news ar-
ticles annotated with Wikipedia links by Hoffart
et al. (2011a) and TAC15, which comprises 315
news articles and discussion forum texts annotated
with Freebase links for the TAC KBP 2015 TEDL
shared task (Ji et al., 2015).

The KB coverage for each of our proposed
global coherence features on these two datasets is
shown in Table 4. YAGO and Freebase contain en-
tity type information for almost all in-KB entities
mentioned in the two datasets. Geographic data
is available for 62.9 percent on CoNLL, but only
for 41.5 percent of entities mentioned in TAC15.
This difference is likely due to the large fraction of
documents from the sports genre in CoNLL. These
documents include match result tables mentioning
a large number of sports teams, which can be eas-
ily located via their cities and stadiums. Temporal
information is present for most entities.

Our evaluation uses results of the following EL
systems:
AIDA (Hoffart et al., 2011a): This system glob-
ally optimizes a graph-based model incorporating
three factors: a popularity prior, the context sim-
ilarity of mention and candidate entity, and co-
herence modeled via general semantic relatedness
measures. We use the AIDA system output on
the CoNLL dataset as provided by the Wikilinks
project.7

SPOTL (Daiber et al., 2013): DBpedia Spotlight
is a local inference system. We use results ob-
tained from the Spotlight webservice.8

7https://github.com/wikilinks/conll03_
nel_eval

8https://github.com/dbpedia-spotlight/

832



FL (Francis-Landau et al., 2016): This local in-
ference system models mention and entity context
with a convolutional neural network (CNN). The
CNN captures semantic similarity of a given men-
tion’s context at different granularities (small con-
text window, paragraph, document) and the entity
context derived from the entity’s Wikipedia page.
PH (Pershina et al., 2015): This global in-
ference system applies Personal PageRank to a
graph whose nodes represent candidate entities
and whose edges indicate if a link between the cor-
responding Wikipedia articles exists. PH achieves
the best CoNLL performance among the systems
in our evaluation.
TAC-1 (Heinzerling and Strube, 2015): This sys-
tem uses local and pairwise inference in an easy-
first, incremental rule-based approach. Features
are based on popularity priors, contextual occur-
rence of keywords, entity type, and relational evi-
dence.
TAC-2 (Sil et al., 2015): This system employs a
global inference approach which partitions a doc-
ument into sets of mentions that appear near each
other. The partitioning is motivated by the in-
tuition that a given mention’s immediate context
provides the most salient information for disam-
biguation, and drastically reduces the search space
during global optimization.
TAC-3 (Dai et al., 2015): This local inference sys-
tem models mentions and entity context with a
CNN and word embeddings.

The systems were chosen for their popularity
(AIDA, SL), performance on CoNLL (FL, PH),
and performance on TAC15 (TAC systems). Un-
less stated otherwise, we use system output pro-
vided by authors for CoNLL systems, and pro-
vided by the workshop organizers for TAC15 sys-
tems.9 Our evaluation does not include (Glober-
son et al., 2016) and (Yamada et al., 2016), who
report better performance on CoNLL than PH, but
were unable to make system output available.

3.1 Setup and Implementation Details
Feature extraction is implemented as a UIMA
pipeline (Ferrucci and Lally, 2004); using the
Stanford CoreNLP (Manning et al., 2014) UIMA
components provided by DKPro (Eckart de
Castilho and Gurevych, 2014) for text segmen-
tation, POS tagging, and named entity recogni-

dbpedia-spotlight/wiki/Web-service
9http://www.nist.gov/tac/2015/KBP/

data.html

tion; DKPro WSD (Miller et al., 2013) for model-
ing entity mentions and links, and using Freebase
(Bollacker et al., 2008) and YAGO (Hoffart et al.,
2011a) as knowledge bases.

After feature extraction, we train a random for-
est classifier10 for each dataset, one using FL sys-
tem results for the CoNLL development set (216
documents) and one using TAC-1 results for the
TAC15 training set (168 documents).

For evaluation, we apply the verifier trained on
FL CoNLL development results to the test set re-
sults of the FL and AIDA systems, and a verifier
trained on PH training data to the PH test set re-
sults. For the test set output of TAC systems 1-3
we apply the verifier trained on the TAC15 train-
ing set output of TAC-1.

As metric we use strong link match as
implemented by the Wikilinks project for the
CoNLL dataset, and the official NIST scorer
(Hachey et al., 2014) for TAC15. This metric mea-
sures precision, recall, and F1 of matching entity
links and mention spans.

3.2 Results and Discussion

Evaluation results are shown in Table 5. Our
method improves the linking performance of all
evaluated EL systems. The impact is most no-
ticeable for the systems that only use local and
pairwise inference, namely FL (+1.9 F1), TAC-
1 (+2.4 F1), TAC-3 (+1.1 F1). The improved
TAC-1 result (68.1F1) is the best published link-
ing score on the TAC15 dataset.

Improvements are smaller for the global infer-
ence systems, AIDA, HP, and TAC-2. In contrast
to Ratinov et al. (2011), who report only a very
small increase in linking performance when in-
corporating global features into a local inference-
based system, our results indicate that global fea-
tures are useful and lead to considerable improve-
ments.

As expected, improvements are caused by in-
creased precision, due to filtering out likely link-
ing mistakes. The fact that this increase is not ac-
companied by a commensurate decrease in recall,
shows that our method predicts wrong linking de-
cisions with high accuracy.

On TAC15, we observe considerable improve-
ments in linking precision of up to 10.4 percent.

10Various other classifiers we tried, e.g. neural networks,
showed no better performance during cross-validation on de-
velopment sets.

833



Baseline After verification ∆
Dataset System Prec Rec F1 Prec Rec F1 Prec Rec F1

CoNLL

AIDA 83.2 83.6 83.4 86.0 82.3 84.1 +2.8 -1.3 +0.7
SPOTL 85.5 80.5 82.9 93.0 77.6 84.6 +7.5 -2.9 +1.7
FL 85.3 85.2 85.2 89.2 84.7 86.9 +4.0 -0.5 +1.7
PH 90.5 90.5 90.5 93.2 89.1 91.1 +2.7 -1.4 +0.6

TAC15
TAC-1 71.2 61.1 65.8 81.6 58.6 68.2 +10.4 -2.5 +2.4
TAC-2 71.4 57.9 63.9 81.2 53.3 64.4 +9.8 -4.6 +0.5
TAC-3 68.0 55.6 61.1 77.6 52.0 62.2 +9.6 -3.2 +1.1

Table 5: Results on CoNLL and TAC15 test sets. Baseline shows performance of the original systems,
After verification shows performance after application of our automatic verification method, and ∆ shows
the corresponding change. Bold font indicates best results for each metric and system.

On CoNLL, the precision increase is less pro-
nounced, arguably owing to the already higher
baseline precision, which leaves less room for im-
provement. Since EL is usually performed as part
of a larger task, such as knowledge base comple-
tion, search, or as part of a more comprehensive
entity analysis system (Durrett and Klein, 2014),
good precision is highly desirable in order to min-
imize error propagation to other system compo-
nents and downstream applications.

3.3 Candidate Reranking

We resort to the binary decision of either retain-
ing or removing an entity linked by an EL system
if no candidate entities and no meaningful confi-
dence scores are available. This is the case for the
output of many EL systems, such as the systems
participating in the TAC KBP TEDL 2015 chal-
lenge.

In case the EL system outputs not only the
top-ranked candidate entity, but also lower-ranked
ones, we can apply our verification method to
all candidates and rerank them according to their
probability of being correct. For example, if the
EL system linked a mention to candidate entity e1
over candidate e2, but verification assigns a higher
probability of being correct to e2, we rerank e2
over e1. Since we assume that the document’s
semantic profile derived from EL results is suffi-
ciently accurate, we do not recreate it after rerank-
ing a candidate.

Reranking the candidate entities produced by
the FL system on the CoNLL test set, this achieves
a similar increase in F1, but with a different
precision-recall trade-off (Table 6): We observe
highest precision at the cost of a lower recall for

System Prec Rec F1

FL baseline 85.3 85.2 85.2
FL filter 89.2 84.7 86.9
FL rerank 87.9 85.6 86.7

Table 6: Comparison of filtering and candidate en-
tity reranking performance on the CoNLL test set.

filtering, while reranking increases both precision
and recall.

3.4 Ablation Study

We conduct an ablation study to assess the impact
of the proposed global coherence features on pre-
diction performance. Applying backward elimina-
tion (John et al., 1994), we iteratively remove one
feature set and successively eliminate the feature
set with the largest impact (Figure 2).

Surprisingly, the string similarity features have
a large effect across all three systems. This sug-
gests that current systems do not optimally utilize
string similarity when selecting and ranking can-
didate entities for a given mention.

Our proposed global coherence features are
among the top features for all systems. This con-
tradicts prior findings by Ratinov et al. (2011) and
shows that global coherence has a considerable
impact on EL performance. We believe that this
is due to our proposed coherence features being
more informative than the generic semantic relat-
edness measures used in prior work. While abla-
tion indeed shows a relatively low importance of
semantic relatedness features (cf. SemRel in Fig-
ure 2), further research is required to test this hy-
pothesis.

834



84.0
84.5
85.0
85.5
86.0
86.5
87.0
87.5

FL
Strin

g sim

Tem
pora

l

Type
distr

ib.

Type
agre

em.

Geo
grap

hic

Sem
Rel

PER
nam

e

Key
phra

sePrio
r

62.0
63.0
64.0
65.0
66.0
67.0
68.0
69.0

TA
C

-1

Tem
pora

l
Prio

r

Geo
grap

hic

Type
distr

ib.

Strin
g sim

Type
agre

em.

PER
nam

e
Key

phra
se

Rela
tions

89.4
89.6
89.8
90.0
90.2
90.4
90.6
90.8
91.0
91.2

P
H

Strin
g sim

Geo
grap

hic

Tem
pora

l

Sem
Rel

Type
distr

ib.

Type
agre

em.

Key
phra

se
PER

nam
ePrio

r

Figure 2: Feature set ablations for the FL, TAC-1, and PH systems. The solid blue lines show the
performance impact in terms of strong link match F1 incurred from eliminating feature sets. The
red dashed line indicates baseline performance without verification.

3.5 Automatic Verification on Noisy Text

The TAC15 dataset consists of different text gen-
res: clean newswire articles, and noisy discussion
forum threads. Analysis of verification perfor-
mance on these two genres reveals that verifica-
tion has the biggest impact on noisy text (Table 7,
bottom), while the improvement is smaller for two
systems on clean text, and even slightly negative
for one system, namely the global inference sys-
tem TAC-2 (Table 7, top).

4 Related Work

Global coherence has been successfully employed
for EL in a number of seminal works (Kulkarni et
al., 2009; Hoffart et al., 2011b; Han et al., 2011),
and more recently by Moro et al. (2014), Pershina
et al. (2015), and Globerson et al. (2016), among
others. These approaches maximize global coher-
ence based on a general notion of semantic relat-
edness, while considering a fixed number of candi-
date entities for each mentions. Our approach dif-
fers from these in in two regards. Firstly, we intro-
duce specific aspects of coherence, namely entity
type coherence, geographic coherence, and tem-

poral coherence. While these aspects are limited
to certain entities, such as entities with a clearly
defined location and temporal range, our experi-
ments showed that features based on these notions
of coherence are useful on the types of texts found
in common datasets. Secondly, in our verification
setting, these rich coherence measures can be effi-
ciently incorporated since their computation is lin-
ear in the number of entities mentioned in a docu-
ment, while they would be prohibitively expensive
in the global inference EL setting.

Entity types have been used in prior work.
Cucerzan (2007) maximizes the agreement of
Wikipedia categories associated with candidate
entities. Due to intractability of the resulting
global optimization problem, the agreement of the
candidate entities for a given mention is maxi-
mized with respect to all categories of all candi-
date entites of all other mentions, and hence in-
cludes many wrong categories. Our approach is
more precise, since verification allows using only
the types of the top-ranked candidate entities. Sil
and Yates (2013) also employ entity types, but
only maximize type agreement of entity mentions
in a small context window. In contrast, our ap-

835



Baseline After verification ∆
Genre System Prec Rec F1 Prec Rec F1 Prec Rec F1

News
TAC-1 66.5 60.3 63.2 75.8 57.0 65.0 9.3 -3.3 1.8
TAC-2 69.7 59.9 64.4 79.3 53.9 64.2 9.6 -6.0 -0.2
TAC-3 63.0 59.1 61.0 71.3 54.3 61.7 8.3 -4.8 0.7

Forum
TAC-1 76.0 61.8 68.1 87.4 60.0 71.2 11.4 -1.8 3.1
TAC-2 73.1 56.1 63.5 83.0 52.8 64.6 9.9 -3.3 1.1
TAC-3 73.8 52.4 61.3 84.7 49.9 62.8 10.9 -2.5 1.5

Table 7: Verification on different text genres. See caption of Table 5 for details.

proach uses global context and hence allows cap-
turing long-distance relations.

Post-processing of EL system output has been
approached as an ensembling task (Rajani and
Mooney, 2016). In this setting, a meta-classifier
combines the output of different EL systems on
a given dataset, taking into account features such
as system confidence scores, past system perfor-
mance, and number of systems agreeing with a
given decision. Our approach differs from en-
sembling, since we post-process the output of a
single system, using rich semantic features. In
contrast, ensembling requires multiple system out-
puts and relies on meta-information about system
performance and decision confidence. Combining
these two post-processing methods is an interest-
ing problem for future work and could lead to fur-
ther improvements, since the two methods rely on
different types of information.

5 Conclusions and Future Work

We have introduced automatic verification as a
post-processing step for entity linking (EL). Our
method uses the output of an existing EL system
to create a semantic profile of the given text using
entity types, as well as geographic and temporal
information. Due to the high precision achieved
by state-of-the-art EL systems, this profile is a suf-
ficiently accurate representation of the text’s main
topic, and further situates the text temporally and
geographically This profile is then used to auto-
matically verify each linked mention individually,
i.e., to predict whether it has been linked cor-
rectly or not. Verification allows leveraging a rich
set of global and pairwise features that would be
prohibitively expensive for EL systems employing
global inference. Evaluation showed consistent
improvements when applying our method to seven
different EL systems on two different datasets.

Our main goal in future work is the better inte-
gration of our approach with existing EL systems.
Most notably, some EL systems produce meaning-
ful confidence scores, which we currently disre-
gard. We expect further improvements from in-
corporating various confidence measures into the
verification process. Automatic verification could
also be used in an easy-first setting to identify
likely correct decisions made by a fast and simple
EL system, and then perform the remaining de-
cisions with a more sophisticated system. Since
our features make use of coreference information
in the form of person name agreement, as well
as entity types, another line of future research is
expanding our proposed entity linking verifica-
tion method to entity analysis (Durrett and Klein,
2014), which models entity linking, coreference,
and entity typing as a joint task.

Acknowledgments

We thank Matthew Francis-Landau, Maria Per-
shina, as well as the TAC KBP 2015 organizers
for providing system output, and the anonymous
reviewers for providing helpful feedback. This
work has been supported by the German Research
Foundation as part of the Research Training Group
“Adaptive Preparation of Information from Het-
erogeneous Sources” (AIPHES) under grant No.
GRK 1994/1.

References
Elke Achtert, Ahmed Hettab, Hans-Peter Kriegel,

Erich Schubert, and Arthur Zimek. 2011. Spatial
outlier detection: Data, algorithms, visualizations.
In Advances in Spatial and Temporal Databases -
12th International Symposium, SSTD 2011, Min-
neapolis, MN, USA, August 24-26, 2011, Proceed-
ings, pages 512–516.

Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim

836



Sturge, and Jamie Taylor. 2008. Freebase: A col-
laboratively created graph database for structuring
human knowledge. In Proceedings of the 2008 ACM
SIGMOD International Conference on Management
of Data, Vancouver, B.C., Canada, 10–12 June 2008,
pages 1247–1250.

Razvan Bunescu and Marius Paşca. 2006. Using en-
cyclopedic knowledge for named entity disambigua-
tion. In Proceedings of the 11th Conference of the
European Chapter of the Association for Compu-
tational Linguistics, Trento, Italy, 3–7 April 2006,
pages 9–16.

Silviu Cucerzan. 2007. Large-scale named entity
disambiguation based on Wikipedia data. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Language Learning, Prague, Czech Re-
public, 28–30 June 2007, pages 708–716.

Hongliang Dai, Siliang Tang, Fei Wu, Zewu Ma, and
Yueting Zhuang. 2015. The ZJU-EDL system for
entity discovery and linking at TAC KBP 2015. In
Proceedings of the Eighth Text Analysis Conference,
Gaithersburg, MD, USA. National Institute of Stan-
dards and Technology.

Joachim Daiber, Max Jakob, Chris Hokamp, and
Pablo N. Mendes. 2013. Improving efficiency and
accuracy in multilingual entity extraction. In Pro-
ceedings of the 9th International Conference on Se-
mantic Systems (I-Semantics), pages 121–124, Graz,
Austria.

Greg Durrett and Dan Klein. 2014. A joint model
for entity analysis: Coreference, typing, and link-
ing. Transactions of the Association of Computa-
tional Linguistics, 2:477–490.

Richard Eckart de Castilho and Iryna Gurevych. 2014.
A broad-coverage collection of portable NLP com-
ponents for building shareable analysis pipelines. In
Proceedings of the Workshop on Open Infrastruc-
tures and Analysis Frameworks for HLT, pages 1–
11, Dublin, Ireland, August. Association for Com-
putational Linguistics and Dublin City University.

David A. Ferrucci and Adam Lally. 2004. UIMA: An
architectural approach to unstructured information
processing in the corporate research environment.
Natural Language Engineering, 10(3):327–348.

Matthew Francis-Landau, Greg Durrett, and Dan
Klein. 2016. Capturing semantic similarity for en-
tity linking with convolutional neural networks. In
Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 1256–1261, San Diego, California, June. As-
sociation for Computational Linguistics.

Amir Globerson, Nevena Lazic, Soumen Chakrabarti,
Amarnag Subramanya, Michael Ringaard, and Fer-
nando Pereira. 2016. Collective entity resolution

with multi-focal attention. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
621–631, Berlin, Germany, August. Association for
Computational Linguistics.

Fréderic Godin, Tom De Nies, Christian Beecks, Lau-
rens De Vocht, Wesley De Neve, Erik Mannens,
Thomas Seidl, and Rik Van de Walle. 2014. The
normalized freebase distance. In The Semantic
Web: ESWC 2014 Satellite Events, pages 218–221.
Springer, Anissaras, Crete, Greece.

Ben Hachey, Joel Nothman, and Will Radford. 2014.
Cheap and easy entity evaluation. In Proceedings
of the 52nd Annual Meeting of the Association for
Computational Linguistics (Volume 2: Short Pa-
pers), Baltimore, Md., 22–27 June 2014, pages 464–
469.

M. A. K. Halliday and Ruqaiya Hasan. 1976. Cohe-
sion in English. London, U.K.: Longman.

Xianpei Han, Le Sun, and Jun Zhao. 2011. Collective
entity linking in web text: A graph-based method. In
Proceedings of the 34th Annual International ACM
SIGIR Conference on Research and Development in
Information Retrieval, Beijing, China, 25–29 July
2011, pages 765–774.

Benjamin Heinzerling and Michael Strube. 2015. Vi-
sual error analysis for entity linking. In Proceedings
of the 53rd Annual Meeting of the Association for
Computational Linguistics: System Demonstrations,
Beijing, China, 26–31 July 2015, pages 37–42.

Johannes Hoffart, Fabian M. Suchanek, Klaus
Berberich, Edwin Lewis-Kelham, Gerard de Melo,
and Gerhard Weikum. 2011a. YAGO2: Explor-
ing and querying world knowledge in time, space,
context, and many languages. In Proceedings of the
20th World Wide Web Conference, Hyderabad, India,
28 March – 1 April, 2011, pages 229–232.

Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bor-
dino, Hagen Fürstenau, Manfred Pinkal, Marc Span-
iol, Bilyana Taneva, Stefan Thater, and Gerhard
Weikum. 2011b. Robust disambiguation of named
entities in text. In Proceedings of the 2011 Con-
ference on Empirical Methods in Natural Language
Processing, Edinburgh, Scotland, U.K., 27–29 July
2011, pages 782–792.

Johannes Hoffart, Stephan Seufert, Dat Ba Nguyen,
Martin Theobald, and Gerhard Weikum. 2012.
KORE: Keyphrase overlap relatedness for entity dis-
ambiguation. In Proceedings of the ACM 21st Con-
ference on Information and Knowledge Manage-
ment, Maui, Hawaii, USA, 29 October – 2 Novem-
ber 2010, pages 545–554.

Heng Ji, Joel Nothman, Ben Hachey, and Radu Florian.
2015. Overview of TAC-KBP 2015: Tri-lingual en-
tity discovery and linking. In Proceedings of the
Eighth Text Analysis Conference, Gaithersburg, MD,

837



USA. National Institute of Standards and Technol-
ogy.

George H. John, Ron Kohavi, and Karl Pfleger. 1994.
Irrelevant features and the subset selection problem.
In Proceedings of the 11th International Conference
on Machine Learning, pages 121–129.

Sayali Kulkarni, Amit Singh, Ganesh Ramakrishnan,
and Soumen Chakrabarti. 2009. Collective annota-
tion of Wikipedia entities in web text. In Proceed-
ings of the 15th ACM SIGKDD International Con-
ference on Knowledge Discovery and Data Mining,
Paris, France, 28 June – 1 July 2009, pages 457–466.

Christopher Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven Bethard, and David McClosky.
2014. The stanford corenlp natural language pro-
cessing toolkit. In Proceedings of 52nd Annual
Meeting of the Association for Computational Lin-
guistics: System Demonstrations, pages 55–60, Bal-
timore, Maryland, June. Association for Computa-
tional Linguistics.

Tristan Miller, Nicolai Erbs, Hans-Peter Zorn, Torsten
Zesch, and Iryna Gurevych. 2013. Dkpro wsd: A
generalized uima-based framework for word sense
disambiguation. In Proceedings of the 51st An-
nual Meeting of the Association for Computational
Linguistics: System Demonstrations, pages 37–42,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.

David Milne and Ian H. Witten. 2008. An effective,
low-cost measure of semantic relatedness obtained
from Wikipedia links. In Proceedings of the Work-
shop on Wikipedia and Artificial Intelligence: An
Evolving Synergy at AAAI-08, Chicago, Ill., 13 July
2008, pages 25–30.

Andrea Moro, Alessandro Raganato, and Roberto Nav-
igli. 2014. Entity linking meets word sense disam-
biguation: A unified approach. Transactions of the
Association for Computational Linguistics, 2:231–
244.

Maria Pershina, Yifan He, and Ralph Grishman. 2015.
Personalized page rank for named entity disam-
biguation. In Proceedings of the 2015 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 238–243, Denver, Colorado, May–
June. Association for Computational Linguistics.

Nazneen Fatema Rajani and Raymond Mooney. 2016.
Combining supervised and unsupervised enembles
for knowledge base population. In Proceedings of
the 2016 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1943–1948, Austin,
Texas, November. Association for Computational
Linguistics.

Lev Ratinov, Dan Roth, Doug Downey, and Mike An-
derson. 2011. Local and global algorithms for dis-
ambiguation to Wikipedia. In Proceedings of the

49th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), Port-
land, Oreg., 19–24 June 2011, pages 1375–1384.

Avirup Sil and Alexander Yates. 2013. Re-ranking for
joint named-entity recognition and linking. In Pro-
ceedings of the 22nd ACM international conference
on Conference on information & knowledge man-
agement, pages 2369–2374, Orlando, Florida, USA.
ACM.

Avirup Sil, Giorgiana Dinu, and Radu Florian. 2015.
The IBM systems for trilingual entity discovery and
linking at TAC 2015. In Proceedings of the Eighth
Text Analysis Conference, Gaithersburg, MD, USA.
National Institute of Standards and Technology.

Valentin I Spitkovsky and Angel X. Chang. 2012. A
cross-lingual dictionary for English Wikipedia con-
cepts. In Proceedings of the 8th International Con-
ference on Language Resources and Evaluation, Is-
tanbul, Turkey, 21–27 May 2012, pages 3168–3175.

Jannik Strötgen and Michael Gertz. 2010. Heideltime:
High quality rule-based extraction and normaliza-
tion of temporal expressions. In Proceedings of the
5th International Workshop on Semantic Evaluation,
pages 321–324, Uppsala, Sweden, July. Association
for Computational Linguistics.

Ikuya Yamada, Hiroyuki Shindo, Hideaki Takeda, and
Yoshiyasu Takefuji. 2016. Joint learning of the em-
bedding of words and entities for named entity dis-
ambiguation. In Proceedings of the 20th SIGNLL
Conference on Computational Natural Language
Learning (CoNLL), pages 250–259, Berlin, Ger-
many, August. Association for Computational Lin-
guistics.

838


