



















































NDMSCS: A Topic-Based Chinese Microblog Polarity Classification System


Proceedings of the Eighth SIGHAN Workshop on Chinese Language Processing (SIGHAN-8), pages 180–184,
Beijing, China, July 30-31, 2015. c©2015 Association for Computational Linguistics and Asian Federation of Natural Language Processing

NDMSCS: A Topic-Based Chinese Microblog Polarity Classification
System

Yang Wang, Yaqi Wang, Shi Feng, Daling Wang, Yifei Zhang
Northeastern University, Shenyang, China

{wangyangdm,wyqnumber1}@gmail.com,
{fengshi,wangdaling,zhangyifei}@ise.neu.edu.cn

Abstract

In this paper, we focus on topic-based mi-
croblog sentiment classification task that
classify the microblog’s sentiment polar-
ities toward a specific topic. Most of the
existing approaches for sentiment analy-
sis usually adopt the target-independent s-
trategy, which may assign irrelevant senti-
ments to the given topic. In this paper, we
leverage the non-negative matrix factor-
ization to get the relevant topic words and
then further incorporate target-dependent
features for topic-based microblog senti-
ment classification. According to the ex-
periment results, our system (NDMSCS)
has achieved a good performance in the
SIGHAN 8 Task 2.

1 Introduction

Nowadays, people are willing to express their feel-
ings and emotions via the microblog services, such
as Twitter and Weibo. Therefore, the microblog
has aggregated huge amount of sentences that con-
tain people’s rich sentiments. Extracting and ana-
lyzing the sentiments in microblogs has become a
hot research topic for both academic communities
and industrial companies.

The microblog usually has a length limitation
of 140 characters, which leads to extremely sparse
vectors for the learning algorithms. On the oth-
er hand, people are used to using a simple sen-
tence, or even a few words to express their attitude
or viewpoint toward a specific topic. Most of the
existing sentiment analysis methods could classify
the microblogs into positive, negative and neutral
categories. However, these methods usually adopt
the target-independent strategy, which may assign
irrelevant sentiments to the given topic.

In this paper we develop a machine learning
system for topic-based microblog polarity classi-

fication. Given a microblog and a topic, we in-
tend to classify whether the microblog is of pos-
itive, negative, or neutral sentiment towards the
given topic. For microblogs conveying both a
positive and negative sentiment towards the top-
ic, whichever is the stronger sentiment should be
chosen.

To tackle challenges, firstly we use non-
negative matrix factorization to find the topic rel-
evant words. And then we propose feature selec-
tion strategy and construct vectors to convert the
raw microblog text into the TFIDF feature val-
ues, combined with the linguistic features, which
we then use together with the labels to train our
sentiment classifier. Our approach includes an ex-
tensive usage of Python based NLP and machine
learning resources for conducting word segmenta-
tion, POS tagging and classifier implementation.

We evaluate our proposed system on the test set
of Topic-Based Chinese Message Polarity Classi-
fication Task in SIGHAN 8. Our system is ranked
3rd on the task test set for overall F1 value and al-
so achieves good performance in the positive and
negative F1 values. The experiment shows the ef-
fectiveness of our proposed system.

2 Non-negative Matrix Factorization

Topic based sentiment analysis task need to con-
sider the target that sentiment words described, so
we try to find the words related to the specific top-
ic. And the topics of test set are different from
the training set, so we want to use the wildcard to
replace the topic words to reduce the influence of
different topics. We consider using the topic mod-
eling to discovery the hidden topic information in
large collections of documents. People usually use
the probabilistic methods, such as Latent Dirich-
let allocation (LDA) (Blei et al., 2003), to build
the topic model. However, an effective alternative
is to use Non-negative Matrix Factorization (NM-
F) (Lee et al., 1999). NMF refers to an unsuper-

180



vised family of algorithms from linear algebra that
simultaneously performs dimension reduction and
clustering.

NMF takes non-negative matrix as an input, and
factorizes it into two smaller non-negative matri-
ces W and H, each having k dimensions. When
multiplied together, these factors approximate the
original matrix X. It finds a decomposition of sam-
ples X into two matrices W and H of non-negative
elements, by optimizing for the squared Frobenius
norm:

argmin
W,H
‖X −WH‖2 = Σi,jXi,j −WHi,j (1)

We can specify the parameter k to control the
number of topics that will be produced. The rows
of the matrix W provides weights for the input
documents relative to the k topics and these values
indicate the strength of association between docu-
ments and topics. The columns of the matrix H
provide weights for the terms relative to the topic-
s. By ordering the values in a given column and
selecting the top-ranked terms, we can produce a
description of the corresponding topic.

NMF implements the Nonnegative Double Sin-
gular Value Decomposition(NNDSVD) which is
proposed by Boutsidis et al. (2008). NNDSVD is
based on two SVD processes, one approximating
the data matrix, the other approximating positive
sections of the resulting partial SVD factors uti-
lizing an algebraic property of unit rank matrices.
The basic NNDSVD algorithm is better fit for s-
parse factorization.

Once the document-term matrix X has been
constructed, we apply NMF topic modeling as fol-
lows: First we initialize the value of k to 5 for
training data and 20 for test data. We generate ini-
tial factors using the NNDSVD. Then we apply
the NMF algorithm on the document-term matrix
X, using the initial parameters from first step, for a
fixed number of iterations (e.g. 1000) to produce
final factors (W, H). Each row of H is a distribu-
tion over all terms in a vocabulary, and easily in-
terpreted as the topics. In each topic we choose
top-ranked terms as the topic words.

The data preparation and topic modeling de-
scribed above can be implemented using the
Python Scikit-learn1 toolkit. We use TfidfVector-
izer to create document-term matrix of size (d, t),
and generate factor W of size (d, k) and factor H

1http://scikit-learn.org/

of size (k, t) by using NMF. Here d and t represent
the number of documents and terms, and k repre-
sents the number of topics. We get the topic words
in the training data as show in Table 1.

Topic ID Topic Words
Topic 1 ssix,n(, edge, galaxy,

mnine,ÊZ¬,\,´
Topic 2 F�,ê×X,¥I,É²,

i,IS,(X,9
Topic 3 üE,
1,ÄO|Ç,eN,

z©:,±,cÏ,±
Topic 4 hd,-3,Ih,Ôd,

û[,úi,NÞ,-h
Topic 5 ÒG,�·,º,e,

N�,Àª,Ó�á�·$,��

Table 1: The topic words extracted from the docu-
ment.

Because the documents carry a lot of noise and
the NMF algorithm doesn’t know anything about
the documents, terms, or topics it contains, we
manually inspect and remove the unrelated topic
words. The words were discarded for various rea-
sons: they were too generic, or irrelevant to the
primary topic. In order to convert the problem to
the topic independent emotion classification prob-
lem, we preprocess the microblog by replacing the
topic words with $TW$ and setting their POS tags
to noun.

3 System Overview

Figure 1 gives a brief overview of our system that
takes the microblogs and the corresponding labels
as inputs to learn sentiment classifiers. We build
a TFIDF-NMF pipeline to get the topic word-
s after preprocessing. We use three-way classi-
fication framework in which we incorporate rich
topic-dependent feature representations of the mi-
croblog text. The classifier is then used to predic-
t test microblog sentiment labels. The proposed
system basically include the module of prepro-
cessing, topic word expansion, feature extraction
and classification. In this section we discuss each
module in detail.

3.1 Preprocessing

Handle Traditional Chinese Text: Some of the
microblogs are written in traditional Chinese, so
we first convert the traditional Chinese to the sim-
plified Chinese based on the tool OpenCC2, which

2http://opencc.byvoid.com/

181



Training Data

Preprocessing
TFIDF-NMF 

Pipeline
Feature 

Extractor

Ensemble 
Classifier

Test Data

Sentiment
Dictionary

Bag of words features

POS features

Polarity features

Sentiment of 
Test Data

Figure 1: System Overview

is an open source project for character conversion
between Traditional and Simplified Chinese.

Replace URLs: The URLs can lead the reader
into the new webpages. These URLs do not car-
ry much information about the sentiment. But they
might help to identify whether the microblogs con-
tain sentiment information. Thus we use ‘http’ to
replace all the URLs in microblogs.

Remove Retweet Mentions: The retweet men-
tions in a microblog often start with ‘@’, and are
followed by people or organizations. This infor-
mation is also unhelpful for the sentiment classifi-
cation of the microblog. Hence they are removed.

Remove Unrelated Punctuations: Some punc-
tuation such as single comma and colon are re-
moved because they are unrelated to sentimen-
t analysis. Some punctuation such as the question
and exclamation mark could indicate people’s sen-
timents, so we preserve them for further steps.

Remove numbers: Numbers are usually with-
out any emotional information. Thus, numbers are
removed in order to refine the microblog content.
But there is a topic Samsung S6 in the training da-
ta, and we convert this topic to Samsung Ssix.

Text Segmentation: In the Chinese text analy-
sis task, we need to consider the word as a unit.
We use the Jieba3 Chinese text processing tool to
segment the Chinese microblogs into words. The
words in sentiment lexicons are added into Jie-
ba default dictionary, which could ensure a higher
segmentation accuracy.

Remove Stop Words: Stop words are extreme-
ly common words. And stop words do not carry
any sentiment information and thus are of no use.

Handle Unbalanced Data: In SIGHAN train-
ing dataset, the number of neutral microblogs is
about 4 times bigger than that of the microblogs
with emotions, which leads to serious unbalanced

3https://github.com/fxsjy/jieba/

data. To tackle this problem, we oversample the
microblogs with emotions to balance the dataset.

3.2 Baseline Model

SIGHAN provided two sentiment lexicon: NTUS-
D and DLUT Emotion Ontology. We combine
the two lexicons, remove the duplicate words, and
finally we get 14,828 positive words and 20,366
negative words in the new lexicon.

We first perform the preprocessing steps listed
in Section 3.1 and for each sentence we count the
number of positive and negative sentiment word-
s. Simple Sentiment Word-Count Method (SS-
WCM) (Yuan et al., 2013) is an intuitively basic
algorithm for sentiment classification. The polar-
ity of text is determined by the number of senti-
ment words. If the number of positive words is
larger than negative words, we will classify the
text as the positive polarity. If the number of pos-
itive words is less than negative words, we will
classify the text as the negative polarity. In other
cases, the text is classified as the neutral polarity.

3.3 Feature Extraction

The feature extraction process is a key component
for sentiment analysis. The feature vector consists
of bag of words features, POS features and polari-
ty features.

Bag of Words Features: We use unigram, bi-
grams and trigrams as features and the TFIDF as
the weighting scheme based on the bag-of-words
model. TFIDF is a term weighting scheme devel-
oped for information retrieval originally, that has
also achieved good performance in document clas-
sification and clustering tasks.

Part of Speech Features: We use Jieba Part
of Speech Tokenizer, which tags the POS of each
word after segmentation. The feature vector uses
POS tags to express of how many nouns, verbs,
adjectives, hashtags, emoticons, urls and special
punctuations like question marks and exclamation
marks a microblog consists. These elements are
normalized by the length of the microblog text.

Polarity Features: We leverage the given senti-
ment lexicons to increase the feature set and reflect
the sentiment words of the microblog in numerical
features. The feature vector consists of the follow-
ing features for each sentiment lexicon: number of
positive and negative sentiments words, sentiment
score (number of positive words minus number of
negative words), number of positive and negative

182



emoticons, number of positive and negative sen-
timents words around the topic words (context 5
words).

3.4 χ2 Feature Selection

The idea of χ2 feature selection is similar as mutu-
al information. For each feature and class, there is
also a score to measure if the feature and the class
are independent to each other. We can use χ2 test,
which is a statistic method to check if two events
are independent. It assumes the feature and class
are independent and calculates χ2 value. The large
score implies they are not independent. The larger
the score is, the higher dependency they have. So
we want keep features for each classes with high-
est χ2 scores. We use the Scikit library to select
features according to the k highest scores.

3.5 Classification

After pre-processing and feature extraction we
feed the features into a classifier. We tried vari-
ous classifiers using the Scikit library, including
Linear Support Vector Classification, Logistic Re-
gression and Random Forest.

Linear Support Vector Classification (Lin-
ear SVC) similar to SVM with parameter ker-
nel=‘linear’, but implemented in terms of liblinear
rather than libsvm, so it has more flexibility in the
choice of penalties and loss functions and should
scale better to large numbers of samples.

Logistic Regression is a linear model for clas-
sification rather than regression. In this model, the
probabilities describing the possible outcomes of
a single trial are modeled using a logistic function.

Random Forest fits a number of decision tree
classifiers on various sub-samples of the dataset
and use averaging to improve the predictive accu-
racy and control over-fitting.

These implementations fit a multiclass (one-vs-
rest) classification with L2 regularization. Af-
ter experimentation it was found that Linear SVC
gave the best performance. The parameters of the
model were computed using grid search. The pa-
rameter search uses a 5-fold cross validation to
find the maximum F-measure of different parame-
ter values.

We implement a simple ensemble classifier that
allows us to combine the different classifiers. It
simply takes the majority rule of the predictions by
the classifiers. The final classifier is the ensemble
of linear SVC, logistic regression, random forest.

4 Experiments and Results

4.1 SIGHAN Dataset

Microblogs labeled as positive, negative or neutral
were given by SIGHAN. The organizers provided
us with 4,905 microblogs which contain 5 topics
for training and 19,469 microblogs for the test data
which contain 20 topics.

4.2 Results

We present the score and rank obtained by the sys-
tem on the test dataset. There were 13 teams par-
ticipated the task 2 of SIGHAN8. We compare our
results with other participators using the F mea-
sure and the result is given in Table 2. The AVG
and MAX represent the average and max value of
the unrestricted result for all the participators. The
F1+ and F1- represent the F measure for the posi-
tive and negative class respectively.

Model F1+ F1- F1
Baseline 0.1451 0.3943 0.3587
POS + Polarity Features 0.1551 0.3607 0.6796
POS + Polarity Features
+ TFIDF Weighting 0.1625 0.3888 0.7483
MAX 0.6039 0.6938 0.8535
AVG 0.1915 0.3646 0.6978

Table 2: The comparison with other participators
for the classification task.

After combining POS features and polarity fea-
tures with the TFIDF weighting, the model add
features about the words, and the experiment re-
sult is improved.

5 Conclusion and Future Works

We present results for sentiment analysis on mi-
croblog by building a supervised system which
combines TFIDF weighting with linguistic fea-
tures which contain topic based features. We re-
port the overall F-measure for three-way classifi-
cation tasks: positive, negative and neutral.

At present, this system still has a lot of space
to promote. Later, we will consider the follow-
ing work to enhance the experiment result: Us-
ing the word vectors or neural network model for
sentiment analysis tasks. More in-depth study of
topics related features. For example, consider the
coreference resolution technology to deal with the
complicated situation refers to introducing syntax
analysis.

183



6 Acknowledgements

This work is supported by the National Basic Re-
search 973 Program of China under Grant No.
2011CB302200-G, the National Natural Science
Foundation of China under Grant No.61370074,
61402091.

References
Dalmia A, Gupta M, Varma V. 2015. SemEval 2015:

Twitter Sentiment Analysis The good, the bad and
the neutral! SemEval 2015

Jiang L, Yu M, Zhou M, et al. 2011. Target-dependent
twitter sentiment classification, volume 1. Associa-
tion for Computational Linguistics

Blei D M, Ng A Y, Jordan M I. 2003. Latent dirichlet
allocation the Journal of machine Learning research

Lee D D, Seung H S 1999. Learning the parts of ob-
jects by non-negative matrix factorization Nature

Boutsidis C, Gallopoulos E. 2008. SVD based initial-
ization: A head start for nonnegative matrix factor-
ization Pattern Recognition

Yuan B, Liu Y, Li H, et al. 2013. Sentiment Clas-
sification in Chinese Microblogs: Lexicon-based
and Learning-based Approaches International Pro-
ceedings of Economics Development and Research
(IPEDR)

Dong L, Wei F, Tan C, et al. 2014. Adaptive recur-
sive neural network for target-dependent twitter sen-
timent classification Association for Computational
Linguistics

Pang B, Lee L, Vaithyanathan S. 2002. Thumbs
up?: sentiment classification using machine learn-
ing techniques Association for Computational Lin-
guistics

Wang M, Liu M, Feng S, et al. 2014. A Novel Cal-
ibrated Label Ranking Based Method for Multiple
Emotions Detection in Chinese Microblogs Natural
Language Processing and Chinese Computing

Illecker M, Zangerle E. 2015. Real-time Twitter Senti-
ment Classification based on Apache Storm

Go A, Huang L, Bhayani R. 2009. Twitter sentiment
analysis Entropy

Wasi S B, Neyaz R, Bouamor H, et al. 2014. CMUQ@
Qatar: Using Rich Lexical Features for Sentiment
Analysis on Twitter SemEval 2014

184


