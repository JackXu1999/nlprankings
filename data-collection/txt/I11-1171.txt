















































Improving Chinese POS Tagging with Dependency Parsing


Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 1447–1451,
Chiang Mai, Thailand, November 8 – 13, 2011. c©2011 AFNLP

Improving Chinese POS Tagging with Dependency Parsing

Zhenghua Li, Wanxiang Che, Ting Liu∗

Research Center for Social Computing and Information Retrieval

MOE-Microsoft Key Laboratory of Natural Language Processing and Speech

School of Computer Science and Technology

Harbin Institute of Technology, China

{lzh,car,tliu}@ir.hit.edu.cn

Abstract

Recent research usually models POS tag-
ging as a sequential labeling problem, in
which only local context features can be
used. Due to the lack of morphological
inflections, many tagging ambiguities in
Chinese are difficult to handle unless con-
sulting larger contexts. In this paper, we
try to improve Chinese POS tagging by us-
ing long-distance dependencies produced
by a statistical dependency parser. Ex-
perimental results show that, despite er-
ror propagation, the syntactic features can
significantly improve the tagging accuracy
from 93.88% to 94.41% (p < 10−5).
Detailed analysis shows that these fea-
tures are helpful for ambiguous pairs like
{NN,VV} and {DEC,DEG}.1

1 Introduction

Part-of-speech (POS) tagging is a necessary and
important step for many natural language tasks,
for example, named entity recognition, parsing
and sentence boundary detection. In the cur-
rent literature, POS tagging is treated as a typ-
ical sequence labeling problem, to which many
models have been successfully applied, such as
maximum-entropy (Ratnaparkhi, 1996), condi-
tional random fields (CRF) (Lafferty et al., 2001)
and perceptron (Collins, 2002). To facilitate fast
decoding, these models make the Markovian inde-
pendence assumption that the current tag depends
on one or two previous tags. In addition, they can
merely consider local context features, e.g. two

∗Correspondence author: tliu@ir.hit.edu.cn
1DEG and DEC are the two POS tags for the frequently

used auxiliary word “�” (dē, of) in Chinese. The associative
“�” is tagged as DEG, such as “I/father � ú«/eyes
(eyes of the father)”; while the one in a relative clause is
tagged as DEC, such as “�/he��/made�?Ú/progress
(progress that he made)”.

words in both sides of the focus word. This works
quite well for English, because inflections are use-
ful and strong clues for POS tags. However, due
to the lack of morphological inflections, Chinese
POS tagging has proven to be much more chal-
lenging than English. With a typical sequential la-
beling model such as Conditional Random Fields
(CRF), the tagging accuracy is about 97% for En-
glish, while less than 94% for Chinese (Huang et
al., 2009).

NN-VV ambiguities are one of the most noto-
rious difficulties for Chinese POS tagging. Figure
1 gives two examples. We can see that the POS
tagger can effortlessly assign the right tags to both
“development” and “develop” in the English side.
However, it is very difficult in the Chinese side
since no word form inflection is available and the
context features may be too sparse or uninforma-
tive. However, the introduction of long-distance
dependencies can largely reduce this difficulty. In
the upper example of Figure 1, the coordinate re-
lation between “n´/NN” and “u�” is a strong
clue to “u�/NN”. In the lower example of Fig-
ure 1, it is also easy to tag “u�” as “VV” if its
coordination with “o/VV” is known.

维护 和 发展 两国
VV CC VV NN

Maintain and develop bilateral 

促进 贸易 和 发展
VV NN CC NN

Promote trade and development

关系
NN

relations

Figure 1: Examples of NN-VV ambiguities with
dependency structures. The focus word is “u�”.

As far as we know, there has been few research
that tries to improve POS tagging with dependency

1447



parsing . The reason for this may be three-fold.
Firstly, POS tags are indispensable features for de-
pendency parsing since pure lexical features lead
to severe sparseness problem. Therefore, POS tag-
ging is traditionally considered as a supporting
task for dependency parsing. Secondly, Chinese
dependency parsing performs not well. The accu-
racy is about 85% when gold-standard POS tags
are given, and quickly drops to about 79% when
using automatically assigned POS tags. Therefore,
error propagation may be an obstacle to research
on this idea. Thirdly, inefficiency of syntactic
parsing may be another concern. However, we be-
lieve that this problem can be relieved in the case
of dependency parsing, since efficient cubic-time
or even linear-time parsing models have been pro-
posed for dependency parsing (McDonald, 2006;
Nivre and Hall, 2005).

In this paper, we propose several kinds of syn-
tactic features based on the output of a statistical
dependency parser. And we use these features to
enhance a traditional POS tagging model so that
long-distance information can be explored. Ex-
perimental results show that this effort is reward-
ing, and the tagging accuracy is significantly im-
proved. Detailed error analysis confirms the use-
fulness of these syntactic features.

2 Baseline POS Taggers

Given an input sentence x = w1...wn, we denote
its POS tag sequence by t = t1...tn, where ti ∈
T , 1 ≤ i ≤ n, and T is the POS tag set. The
goal of POS tagging is to find the highest-scoring
sequence:

t̂ = arg max
t

µ(x, t)

We implement two baseline taggers, i.e., a
Perceptron-based tagger and a CRF-based tagger.
As a linear model, Perceptron defines the score of
a tag sequence to be

µ(x, t) = w · f(x, t)

where f(x, t) refers to the feature vector and w
is the corresponding weight vector. We use stan-
dard averaged perceptron to learn the weight vec-
tor (Collins, 2002).

As a probabilistic model, CRF defines the prob-
ability of a sequence to be

µ(x, t) = P (t|x) = e
w·f(x,t)

∑
t′ e

w·f(x,t′)

We adopt the exponentiated gradient algorithm to
learn the weight vector (Collins et al., 2008).

For POS tagging features f(x, t), we follow the
work of Zhang and Clark (2008a). Besides stan-
dard POS unigram (wi ti), bigram (ti−1 ti) and
trigram (ti−2 ti−1 ti) features, they explore many
features composed of Chinese characters, such as
ci,0 ti and ci,−1 ti, where ci,0 and ci,−1 denote the
start and end characters of wi. These character-
based features are very helpful for tagging accu-
racy. Due to space limitation, we refer to Zhang
and Clark (2008a) for the complete feature de-
scription. In order to distinguish these features
from our proposed syntactic features, we refer to
them as the basic features and denote them as
fb(x, t). Given w, we adopt the Viterbi algorithm
to get the optimal tagging sequence.

3 POS Tagging with Syntactic Features

The framework of our method is shown in Fig-
ure 2. Given an input sentence x, we first use the
CRF-based model to produce a tagging sequence
tC . Then, based on tC , we use a statistical de-
pendency parser to obtain the syntactic tree dA.
Finally, both tC and dA are used as additional
features in the enhanced Perceptron-based model.
We use Perceptron to build our model because it
is competitive to CRF in tagging accuracy but re-
quires much less training time. During training
phase, we adopt the 10-fold cross validation strat-
egy to produce both tC and dA for the training set.

Input sentence

CRF-based tagger

Dependency 

Parser

Perceptron-based tagger 

with syntactic features
d
A

t
C

Figure 2: Framework of our method.

Based on a guide POS sequence t′ (tC in this
paper) and a syntactic tree d, we propose three
kinds of features, as shown in Table 1. Our use of
guide POS Features fg(x, t′, t) is mainly inspired
by stacked learning, in which results of the first-
level predicator are used to guide the second (Co-
hen and de Carvalho, 2005; Nivre and McDonald,
2008; Martins et al., 2008).

Syntactic features fs(x,d, t) explore features
related with the head and children of the focus
word. Syntactic features with guide POS tags

1448



Guide POS Features: fg(x, t′, t) Syntactic Features: fs(x,d, t) Syntactic Features with Guide POS: fsg(x, t′,d, t)
t′i ti t

′
i−1 t

′
i ti #lc(i) ti wh(i) ti #lc(i) t

′
i ti #rc(i) t

′
i ti t

′
h(i) ti t

′
i d(i) ti

t′i−1 ti t
′
i t

′
i+1 ti wlc(i,k) ti d(i) ti t

′
lc(i,k) t

′
i ti t

′
rc(i,k) t

′
i ti t

′
h(i) t

′
i ti t

′
h(i) d(i) ti

t′i+1 ti t
′
i−1 t

′
i+1 ti #rc(i) ti wh(i) d(i) ti t

′
lc(i,k) ti t

′
rc(i,k) ti t

′
h(i) wi ti t

′
h(i) t

′
i d(i) ti

t′i ∼ fb(x) t′i−1 t′i t′i+1 ti wrc(i,k) ti wi d(i) ti t′h(i) wi d(i) ti

Table 1: Feature templates for our enhanced Perceptron-based tagger. t′ denotes a guide POS sequence,
which is tC in this paper. t′i ∼ fb(x) means that we concatenate t′i and each feature in fb(x, t) to obtain a
new one. h(i) denotes the index of the head of i in the syntactic tree d; while d(i) means the distance and
direction of the dependency h(i) → i. #lc(i) means the number of left-side children of i, and lc(i, k) is
the index of the kth left child of i. Analogously, #rc(i) and rc(i, k) considers right-side children of i.

fsg(x, t
′,d, t) further make use of the POS tags

of the head and children of the focus word. The
effectiveness of these features will be examined in
the experiments.

4 Experiments and Analysis

The Penn Chinese Treebank 5.1 (CTB5) is used as
the labeled data (Xue et al., 2005). We follow the
setup of Duan et al. (2007) and split CTB5 into
training (secs 001-815 and 1001-1136), develop-
ment (secs 886-931 and 1148-1151), and test (secs
816-885 and 1137-1147) sets. Head-finding rules
are used to turn the bracketed sentences into de-
pendency structures (Zhang and Clark, 2008b).

We adopt the second-order graph-based model
of McDonald and Pereira (2006) for our statisti-
cal dependency parser. Its time complexity for de-
coding is O(n3). On the test set, its parsing ac-
curacy is 85.01% when using gold-standard POS
tags, and is 78.82% when using automatic POS
tags produced by the baseline CRF tagger.

4.1 Main Results

Table 2 gives the final results. The first row con-
tains two baseline tagging models which only use
the basic features fb(x, t). We can see that the
Perceptron-based and CRF-based models achieve
comparable accuracies.

From the results in the second row, we can find
that using guide POS features only modestly (but
significantly) improve the accuracy. This model
can be regarded as the integrated model of both
Perceptron-based and CRF-based models.

In the third row, we explore syntactic features
based on gold-standard trees and aim to find out
the usefulness of syntactic features without error
propagation. Obviously, correct syntactic features
can greatly help resolve tagging ambiguities. Us-
ing all the features leads to the best accuracy.

Method Token Known Unknown

Perceptron with fb(x, t) 93.82 94.65 81.32
CRF with fb(x, t) 93.88 94.70 81.51

+fg(tC) 94.02 94.84 81.67

+fs(dG) 96.02 96.85 83.51
+fs(dG)+fsg(tC ,dG) 96.19 96.99 84.27
+fs(dG)+fsg(tC ,dG)+fg(tC) 96.26 97.05 84.37

+fs(dA) 94.06 94.91 81.44
+fs(dA)+fsg(tC ,dA) 94.41 95.26 81.67
+fs(dA)+fsg(tC ,dA)+fg(tC) 94.37 95.20 81.95

Table 2: Tagging accuracy on the test set (%). tC

denotes the tagging sequence of the baseline CRF
model. dG refers to the gold-standard tree; while
dA denotes the automatically parsed tree. Note
that we omit x and t in fs/g/sg(.) for brevity.

In the fourth row, we examine our method in the
realistic scenario. The syntactic tree is automati-
cally produced by the parser trained on the training
set. The accuracy improvement is modest but sig-
nificant when only adding pure syntactic features
fs(x,d

A, t) (p < 0.01).2 Using syntactic features
with guide POS tags, i.e., fsg(x, tC ,dA, t), can
boost the accuracy by a large margin. Compared
with the baseline models, the improvement is sig-
nificant (p < 10−5). Then, adding guide POS fea-
tures fg(x, tC , t) slightly decreases the accuracy,
but somehow improves the accuracy of unknown
words.

4.2 Error Analysis

To find out how the syntactic features help tagging,
we conduct detailed error analysis through com-
paring the results of different models, as shown in
Table 3. We choose the most frequent error pat-

2We adapt Dan Bikel’s randomized parsing evalua-
tion comparator to do significant test for POS tagging.
http://www.cis.upenn.edu/˜bikel/software.html

1449



terns made by the baseline CRF-based model, and
presents them in descending order of frequency.

error pattern CRF Gold Parse Auto Parse
VV → NN 456 -197 -15
NN → VV 341 -180 -30

DEC → DEG 227 -222 -66
NR → NN 224 +1 -5

DEG → DEC 191 -187 -57
JJ → NN 135 +10 0

NN → NR 84 -3 0
NN → JJ 63 0 +1

Table 3: The number of error patterns made by dif-
ferent models. An error pattern “X → Y” means
that the focus word, whose true tag is ‘X’, is as-
signed a tag ‘Y’. “CRF” refers to the baseline
CRF-based model. “Gold Parse” and “Auto Parse”
are two perceptron-based models augmented with
syntactic features, and correspond to the best mod-
els in the third and fourth rows of Table 2, respec-
tively. The signed numbers in the last two columns
present the change of error number.

From the column of “Gold Parse” we can see
that using correct syntactic features can greatly
reduce the errors for ambiguous pairs {NN,
VV} and {DEC, DEG}. Especially, nearly all
ambiguities of {DEC, DEG} are correctly re-
solved.However, syntactic features are not helpful
for ambiguities like {NN,NR} and {NN,JJ}. One
common characteristic of these two pairs are that
the two POS tags play similar roles from syntac-
tic view. In other words, their syntactic contexts
are usually similar, which naturally explains why
the gold-standard syntactic features fail to help. In
contrast, “NN” and “VV” (or “DEC” and “DEG”)
usually have completely different syntactic struc-
tures. This demonstrates that our proposed syntac-
tic features are very effective.

Using automatic syntactic features still help re-
solve {NN, VV} and {DEC, DEG}. However, the
error reduction is much less than that of using cor-
rect parse trees, which is obviously due to error
propagation. Likewise, the errors over {NN, NR}
and {NN,JJ} are not influenced.

5 Related Work

Recently, extensive research on Chinese POS tag-
ging has been done. Tseng et al. (2005) enhance
the tagging accuracy of unknown words by using
rich morphological features. Huang et al. (2009)
improve a bigram HMM POS tagger by latent an-

notation and self-training. Several methods are
proposed to handle joint word segmentation and
POS tagging of Chinese (Jiang et al., 2008; Zhang
and Clark, 2008a; Kruengkrai et al., 2009).

The most closely related work to our approach
is the one of Huang et al. (2007), which also ex-
plores syntactic features to boost the tagging accu-
racy. In stead of directly using syntactic features
in a discriminative POS tagger, they adopt the
RankBoost-based algorithm to rerank the N-best
output of a sophisticated HMM tagger (Collins
and Koo, 2005). As a discriminative model, the
reranker can make use of rich features includ-
ing morphological features, word/tag n-grams and
syntactic features. Another difference from our
work is that their syntactic tree is produced by
the constituent parser of Charniak (2000) which
jointly solves POS tagging and parsing. In this
way, they might obtain higher-quality syntactic
features since error propagation can be alleviated
to some extent. Their reranking approach lead to
an improvement of about 1% in tagging accuracy
over the HMM tagger. In this paper, we propose
another way to incorporate long-distance informa-
tion for POS tagging. In another perspective, our
approach may be more promising in real applica-
tions, since dependency parsing is simpler and po-
tentially more efficient than constituent parsing.

6 Conclusions

In this paper, we show that the accuracy of a
discriminative sequential POS tagger can be sub-
stantially improved by exploring syntactic fea-
tures. We also show that the syntactic features
can help resolve ambiguities like {NN,VV} and
{DEC,DEG}, which are difficult to handle when
only local contexts are considered. In the future,
we will investigate joint POS tagging and depen-
dency parsing models to further improve tagging
accuracy.

Acknowledgments

This work was supported by National Natural
Science Foundation of China (NSFC) via grant
60803093, 61133012, the Natural Scientific Re-
search Innovation Foundation in Harbin Insti-
tute of Technology (HIT.NSRIF.2009069) and the
Fundamental Research Funds for the Central Uni-
versities (HIT.KLOF.2010064).

1450



References

Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In ANLP’00, pages 132–139.

William W. Cohen and Vitor Rocha de Carvalho. 2005.
Stacked sequential learning. In IJCAI’05, pages
671–676.

Michael J. Collins and Terry Koo. 2005. Discrimina-
tive reranking for natural language parsing. Compu-
tational Linguistics, pages 25–70.

Michael Collins, Amir Globerson, Terry Koo, Xavier
Carreras, and Peter Bartlett. 2008. Exponentiated
gradient algorithms for conditional random fields
and max-margin markov networks. JMLR, 9:1775–
1822.

Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of EMNLP 2002.

Xiangyu Duan, Jun Zhao, , and Bo Xu. 2007. Proba-
bilistic models for action-based Chinese dependency
parsing. In Proceedings of ECML/ECPPKDD.

Zhongqiang Huang, Mary Harper, and Wen Wang.
2007. Mandarin part-of-speech tagging and dis-
criminative reranking. In EMNLP-CoNLL07, pages
1093–1102.

Zhongqiang Huang, Vladimir Eidelman, and Mary
Harper. 2009. Improving a simple bigram hmm
part-of-speech tagger by latent annotation and self-
training. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, Companion Volume: Short Pa-
pers, pages 213–216, Boulder, Colorado, June. As-
sociation for Computational Linguistics.

Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan Lü.
2008. A cascaded linear model for joint chinese
word segmentation and part-of-speech tagging. In
Proceedings of ACL-08: HLT, pages 897–904.

Canasai Kruengkrai, Kiyotaka Uchimoto, Jun’ichi
Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi
Isahara. 2009. An error-driven word-character hy-
brid model for joint chinese word segmentation and
pos tagging. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, pages 513–521.

John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of ICML 2001, pages
282–289.

AndrW F. T. Martins, Dipanjan Das, Noah A. Smith, and
Eric P. Xing. 2008. Stacking dependency parsers.
In EMNLP’08, pages 157–166.

Ryan McDonald and Fernando Pereira. 2006. Online
learning of approximate dependency parsing algo-
rithms. In EACL06.

Ryan McDonald. 2006. Discriminative Training and
Spanning Tree Algorithms for Dependency Parsing.
Ph.D. thesis, University of Pennsylvania.

Joakim Nivre and Johan Hall. 2005. Maltparser: A
language-independent system for data-driven depen-
dency parsing. In In Proc. of the Fourth Workshop
on Treebanks and Linguistic Theories, pages 13–95.

Joakim Nivre and Ryan McDonald. 2008. Integrat-
ing graph-based and transition-based dependency
parsers. In Proceedings of ACL 2008, pages 950–
958.

Adwait Ratnaparkhi. 1996. A maximum entropy
model for part-of-speech tagging. In Proceedings
of EMNLP 1996.

Huihsin Tseng, Daniel Jurafsky, and Christopher Man-
ning. 2005. Morphological features help pos tag-
ging of unknown words across language varieties. In
SIGHAN Workshop on Chinese Language Process-
ing.

Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The Penn Chinese Treebank: Phrase
structure annotation of a large corpus. In Natural
Language Engineering, volume 11, pages 207–238.

Yue Zhang and Stephen Clark. 2008a. Joint word seg-
mentation and POS tagging using a single percep-
tron. In ACL08, pages 888–896.

Yue Zhang and Stephen Clark. 2008b. A tale of
two parsers: Investigating and combining graph-
based and transition-based dependency parsing. In
EMNLP08, pages 562–571.

1451


