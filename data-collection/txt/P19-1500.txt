



















































Hierarchical Transformers for Multi-Document Summarization


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5070–5081
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

5070

Hierarchical Transformers for Multi-Document Summarization

Yang Liu and Mirella Lapata
Institute for Language, Cognition and Computation

School of Informatics, University of Edinburgh
yang.liu2@ed.ac.uk, mlap@inf.ed.ac.uk

Abstract

In this paper, we develop a neural summa-
rization model which can effectively process
multiple input documents and distill abstrac-
tive summaries. Our model augments a previ-
ously proposed Transformer architecture (Liu
et al., 2018) with the ability to encode docu-
ments in a hierarchical manner. We represent
cross-document relationships via an attention
mechanism which allows to share information
as opposed to simply concatenating text spans
and processing them as a flat sequence. Our
model learns latent dependencies among tex-
tual units, but can also take advantage of ex-
plicit graph representations focusing on simi-
larity or discourse relations. Empirical results
on the WikiSum dataset demonstrate that the
proposed architecture brings substantial im-
provements over several strong baselines.1

1 Introduction

Automatic summarization has enjoyed renewed
interest in recent years, thanks to the popular-
ity of neural network models and their ability to
learn continuous representations without recourse
to preprocessing tools or linguistic annotations.
The availability of large-scale datasets (Sandhaus,
2008; Hermann et al., 2015; Grusky et al., 2018)
containing hundreds of thousands of document-
summary pairs has driven the development of
neural architectures for summarizing single doc-
uments. Several approaches have shown promis-
ing results with sequence-to-sequence models that
encode a source document and then decode it into
an abstractive summary (See et al., 2017; Celiky-
ilmaz et al., 2018; Paulus et al., 2018; Gehrmann
et al., 2018).

Multi-document summarization — the task of
producing summaries from clusters of themati-

1Our code and data is available at https://github.
com/nlpyang/hiersumm.

cally related documents — has received signif-
icantly less attention, partly due to the paucity
of suitable data for the application of learning
methods. High-quality multi-document summa-
rization datasets (i.e., document clusters paired
with multiple reference summaries written by hu-
mans) have been produced for the Document Un-
derstanding and Text Analysis Conferences (DUC
and TAC), but are relatively small (in the range
of a few hundred examples) for training neu-
ral models. In an attempt to drive research fur-
ther, Liu et al. (2018) tap into the potential of
Wikipedia and propose a methodology for cre-
ating a large-scale dataset (WikiSum) for multi-
document summarization with hundreds of thou-
sands of instances. Wikipedia articles, specifically
lead sections, are viewed as summaries of various
topics indicated by their title, e.g.,“Florence” or
“Natural Language Processing”. Documents cited
in the Wikipedia articles or web pages returned
by Google (using the section titles as queries) are
seen as the source cluster which the lead section
purports to summarize.

Aside from the difficulties in obtaining train-
ing data, a major obstacle to the application of
end-to-end models to multi-document summariza-
tion is the sheer size and number of source doc-
uments which can be very large. As a result, it
is practically infeasible (given memory limitations
of current hardware) to train a model which en-
codes all of them into vectors and subsequently
generates a summary from them. Liu et al. (2018)
propose a two-stage architecture, where an extrac-
tive model first selects a subset of salient passages,
and subsequently an abstractive model generates
the summary while conditioning on the extracted
subset. The selected passages are concatenated
into a flat sequence and the Transformer (Vaswani
et al., 2017), an architecture well-suited to lan-
guage modeling over long sequences, is used to

https://github.com/nlpyang/hiersumm
https://github.com/nlpyang/hiersumm


5071

decode the summary.

Although the model of Liu et al. (2018) takes
an important first step towards abstractive multi-
document summarization, it still considers the
multiple input documents as a concatenated flat
sequence, being agnostic of the hierarchical struc-
tures and the relations that might exist among doc-
uments. For example, different web pages might
repeat the same content, include additional con-
tent, present contradictory information, or discuss
the same fact in a different light (Radev, 2000).
The realization that cross-document links are im-
portant in isolating salient information, elimi-
nating redundancy, and creating overall coherent
summaries, has led to the widespread adoption
of graph-based models for multi-document sum-
marization (Erkan and Radev, 2004; Christensen
et al., 2013; Wan, 2008; Parveen and Strube,
2014). Graphs conveniently capture the relation-
ships between textual units within a document col-
lection and can be easily constructed under the as-
sumption that text spans represent graph nodes and
edges are semantic links between them.

In this paper, we develop a neural summariza-
tion model which can effectively process multi-
ple input documents and distill abstractive sum-
maries. Our model augments the previously pro-
posed Transformer architecture with the ability to
encode multiple documents in a hierarchical man-
ner. We represent cross-document relationships
via an attention mechanism which allows to share
information across multiple documents as opposed
to simply concatenating text spans and feeding
them as a flat sequence to the model. In this
way, the model automatically learns richer struc-
tural dependencies among textual units, thus in-
corporating well-established insights from earlier
work. Advantageously, the proposed architecture
can easily benefit from information external to the
model, i.e., by replacing inter-document attention
with a graph-matrix computed based on the basis
of lexical similarity (Erkan and Radev, 2004) or
discourse relations (Christensen et al., 2013).

We evaluate our model on the WikiSum dataset
and show experimentally that the proposed archi-
tecture brings substantial improvements over sev-
eral strong baselines. We also find that the ad-
dition of a simple ranking module which scores
documents based on their usefulness for the target
summary can greatly boost the performance of a
multi-document summarization system.

2 Related Work

Most previous multi-document summarization
methods are extractive operating over graph-based
representations of sentences or passages. Ap-
proaches vary depending on how edge weights
are computed e.g., based on cosine similarity with
tf-idf weights for words (Erkan and Radev, 2004)
or on discourse relations (Christensen et al., 2013),
and the specific algorithm adopted for ranking text
units for inclusion in the final summary. Sev-
eral variants of the PageRank algorithm have been
adopted in the literature (Erkan and Radev, 2004)
in order to compute the importance or salience of
a passage recursively based on the entire graph.
More recently, Yasunaga et al. (2017) propose a
neural version of this framework, where salience
is estimated using features extracted from sen-
tence embeddings and graph convolutional net-
works (Kipf and Welling, 2017) applied over the
relation graph representing cross-document links.

Abstractive approaches have met with limited
success. A few systems generate summaries
based on sentence fusion, a technique which iden-
tifies fragments conveying common information
across documents and combines these into sen-
tences (Barzilay and McKeown, 2005; Filippova
and Strube, 2008; Bing et al., 2015). Although
neural abstractive models have achieved promis-
ing results on single-document summarization
(See et al., 2017; Paulus et al., 2018; Gehrmann
et al., 2018; Celikyilmaz et al., 2018), the ex-
tension of sequence-to-sequence architectures to
multi-document summarization is less straightfor-
ward. Apart from the lack of sufficient training
data, neural models also face the computational
challenge of processing multiple source docu-
ments. Previous solutions include model trans-
fer (Zhang et al., 2018; Lebanoff and Liu, 2018),
where a sequence-to-sequence model is pretrained
on single-document summarization data and fine-
tuned on DUC (multi-document) benchmarks, or
unsupervised models relying on reconstruction ob-
jectives (Ma et al., 2016; Chu and Liu, 2018).

Liu et al. (2018) propose a methodology for
constructing large-scale summarization datasets
and a two-stage model which first extracts salient
information from source documents and then uses
a decoder-only architecture (that can attend to very
long sequences) to generate the summary. We fol-
low their setup in viewing multi-document sum-
marization as a supervised machine learning prob-



5072

......
......

ranked 
paragraphs

source 
paragraphs

paragraph 
ranker

encoder
para 1

para L         

para L

decoder

abstractive 
summarizer

target 
summary

Figure 1: Pipeline of our multi-document summariza-
tion system. L source paragraphs are first ranked and
the L′-best ones serve as input to an encoder-decoder
model which generates the target summary.

lem and for this purpose assume access to large,
labeled datasets (i.e., source documents-summary
pairs). In contrast to their approach, we use a
learning-based ranker and our abstractive model
can hierarchically encode the input documents,
with the ability to learn latent relations across doc-
uments and additionally incorporate information
encoded in well-known graph representations.

3 Model Description

We follow Liu et al. (2018) in treating the gen-
eration of lead Wikipedia sections as a multi-
document summarization task. The input to a hy-
pothetical system is the title of a Wikipedia arti-
cle and a collection of source documents, while
the output is the Wikipedia article’s first section.
Source documents are webpages cited in the Ref-
erences section of the Wikipedia article and the
top 10 search results returned by Google (with
the title of the article as the query). Since source
documents could be relatively long, they are split
into multiple paragraphs by line-breaks. More
formally, given title T , and L input paragraphs
{P1, · · · , PL} (retrieved from Wikipedia citations
and a search engine), the task is to generate the
lead section D of the Wikipedia article.

Our summarization system is illustrated in Fig-
ure 1. Since the input paragraphs are numerous
and possibly lengthy, instead of directly applying
an abstractive system, we first rank them and sum-
marize the L′-best ones. Our summarizer follows
the very successful encoder-decoder architecture
(Bahdanau et al., 2015), where the encoder en-
codes the input text into hidden representations
and the decoder generates target summaries based
on these representations. In this paper, we focus
exclusively on the encoder part of the model, our
decoder follows the Transformer architecture in-

troduced in Vaswani et al. (2017); it generates a
summary token by token while attending to the
source input. We also use beam search and a
length penalty (Wu et al., 2016) in the decoding
process to generate more fluent and longer sum-
maries.

3.1 Paragraph Ranking
Unlike Liu et al. (2018) who rank paragraphs
based on their similarity with the title (using tf-idf-
based cosine similarity), we adopt a learning-
based approach. A logistic regression model is
applied to each paragraph to calculate a score in-
dicating whether it should be selected for summa-
rization. We use two recurrent neural networks
with Long-Short Term Memory units (LSTM;
Hochreiter and Schmidhuber 1997) to represent ti-
tle T and source paragraph P :

{ut1, · · · , utm} = lstmt({wt1, · · · , wtm}) (1)
{up1, · · · , upn} = lstmp({wp1, · · · , wpn}) (2)

where wti, wpj are word embeddings for tokens in
T and P , and uti, upj are the updated vectors for
each token after applying the LSTMs.

A max-pooling operation is then used over title
vectors to obtain a fixed-length representation ût:

ût = maxpool({ut1, · · · , utm}) (3)

We concatenate ût with the vector upi of each to-
ken in the paragraph and apply a non-linear trans-
formation to extract features for matching the title
and the paragraph. A second max-pooling opera-
tion yields the final paragraph vector p̂:

pi = tanh(W1([upi; ût])) (4)

p̂ = maxpool({p1, · · · , pn}) (5)

Finally, to estimate whether a paragraph should be
selected, we use a linear transformation and a sig-
moid function:

s = sigmoid(W2 ˆ(p)) (6)

where s is the score indicating whether para-
graph P should be used for summarization.

All input paragraphs {P1, · · · , PL} receive
scores {s1, · · · , sL}. The model is trained by
minimizing the cross entropy loss between si and
ground-truth scores yi denoting the relatedness of
a paragraph to the gold standard summary. We
adopt ROUGE-2 recall (of paragraph Pi against



5073

gold target text D) as yi. In testing, input para-
graphs are ranked based on the model predicted
scores and an ordering {R1, · · · , RL} is gener-
ated. The first L′ paragraphs {R1, · · · , RL′} are
selected as input to the second abstractive stage.

3.2 Paragraph Encoding

Instead of treating the selected paragraphs as
a very long sequence, we develop a hierarchi-
cal model based on the Transformer architecture
(Vaswani et al., 2017) to capture inter-paragraph
relations. The model is composed of several lo-
cal and global transformer layers which can be
stacked freely. Let tij denote the j-th token in the
i-th ranked paragraph Ri; the model takes vectors
x0ij (for all tokens) as input. For the l-th trans-
former layer, the input will be xl−1ij , and the output
is written as xlij .

3.2.1 Embeddings

Input tokens are first represented by word embed-
dings. Let wij ∈ Rd denote the embedding as-
signed to tij . Since the Transformer is a non-
recurrent model, we also assign a special posi-
tional embedding peij to tij , to indicate the po-
sition of the token within the input.

To calculate positional embeddings, we follow
Vaswani et al. (2017) and use sine and cosine func-
tions of different frequencies. The embedding ep
for the p-th element in a sequence is:

ep[i] = sin(p/10000
2i/d) (7)

ep[2i+ 1] = cos(p/10000
2i/d) (8)

where ep[i] indicates the i-th dimension of the em-
bedding vector. Because each dimension of the
positional encoding corresponds to a sinusoid, for
any fixed offset o, ep+o can be represented as a
linear function of ep, which enables the model to
distinguish relative positions of input elements.

In multi-document summarization, token tij has
two positions that need to be considered, namely i
(the rank of the paragraph) and j (the position
of the token within the paragraph). Positional
embedding peij ∈ Rd represents both positions
(via concatenation) and is added to word embed-
ding wij to obtain the final input vector x0ij :

peij = [ei; ej ] (9)

x0ij = wij + peij (10)

3.2.2 Local Transformer Layer

A local transformer layer is used to encode con-
textual information for tokens within each para-
graph. The local transformer layer is the same
as the vanilla transformer layer (Vaswani et al.,
2017), and composed of two sub-layers:

h = LayerNorm(xl−1 +MHAtt(xl−1)) (11)

xl = LayerNorm(h+ FFN(h)) (12)

where LayerNorm is layer normalization pro-
posed in Ba et al. (2016); MHAtt is the multi-
head attention mechanism introduced in Vaswani
et al. (2017) which allows each token to attend
to other tokens with different attention distribu-
tions; and FFN is a two-layer feed-forward net-
work with ReLU as hidden activation function.

3.2.3 Global Transformer Layer

A global transformer layer is used to exchange in-
formation across multiple paragraphs. As shown
in Figure 2, we first apply a multi-head pooling op-
eration to each paragraph. Different heads will en-
code paragraphs with different attention weights.
Then, for each head, an inter-paragraph attention
mechanism is applied, where each paragraph can
collect information from other paragraphs by self-
attention, generating a context vector to capture
contextual information from the whole input. Fi-
nally, context vectors are concatenated, linearly
transformed, added to the vector of each token,
and fed to a feed-forward layer, updating the rep-
resentation of each token with global information.

Multi-head Pooling To obtain fixed-length
paragraph representations, we apply a weighted-
pooling operation; instead of using only one rep-
resentation for each paragraph, we introduce a
multi-head pooling mechanism, where for each
paragraph, weight distributions over tokens are
calculated, allowing the model to flexibly encode
paragraphs in different representation subspaces
by attending to different words.

Let xl−1ij ∈ Rd denote the output vector of the
last transformer layer for token tij , which is used
as input for the current layer. For each paragraph
Ri, for head z ∈ {1, · · · , nhead}, we first trans-
form the input vectors into attention scores azij
and value vectors bzij . Then, for each head, we
calculate a probability distribution âzij over tokens



5074

within the paragraph based on attention scores:

azij =W
z
ax

l−1
ij (13)

bzij =W
z
b x

l−1
ij (14)

âzij = exp(a
z
ij)/

n∑
j=1

exp(azij) (15)

where W za ∈ R1∗d and W zb ∈ Rdhead∗d are
weights. dhead = d/nhead is the dimension of
each head. n is the number of tokens in Ri.

We next apply a weighted summation with an-
other linear transformation and layer normaliza-
tion to obtain vector headzi for the paragraph:

headzi = LayerNorm(W
z
c

n∑
j=1

azijb
z
ij) (16)

where W zc ∈ Rdhead∗dhead is the weight.
The model can flexibly incorporate multiple

heads, with each paragraph having multiple at-
tention distributions, thereby focusing on different
views of the input.

Inter-paragraph Attention We model the de-
pendencies across multiple paragraphs with an
inter-paragraph attention mechanism. Similar to
self-attention, inter-paragraph attention allows for
each paragraph to attend to other paragraphs by
calculating an attention distribution:

qzi =W
z
q head

z
i (17)

kzi =W
z
khead

z
i (18)

vzi =W
z
v head

z
i (19)

contextzi =

m∑
i=1

exp(qzi
Tkzi′)∑m

o=1 exp(q
z
i
Tkzo)

vzi′ (20)

where qzi , k
z
i , v

z
i ∈ Rdhead∗dhead are query,

key, and value vectors that are linearly trans-
formed from headzi as in Vaswani et al. (2017);
contextzi ∈ Rdhead represents the context vec-
tor generated by a self-attention operation over
all paragraphs. m is the number of input para-
graphs. Figure 2 provides a schematic view of
inter-paragraph attention.

Feed-forward Networks We next update token
representations with contextual information. We
first fuse information from all heads by concate-
nating all context vectors and applying a linear
transformation with weight Wc ∈ Rd∗d:

ci =Wc[context
1
i ; · · · ; context

nhead
i ] (21)

Multi-head Pooling Multi-head Pooling

head 1

head 2

head 3

head 1

head 2

head 3

context 
1

context 
2

context 
3

context 
1

context 
2

context 
3

Inter-paragraph 
Attention

Inter-paragraph 
Attention

Inter-paragraph 
Attention

context 

this is para one

Feed 
Forward

Feed 
Forward

Feed 
Forward

Feed 
Forward

context 

this is para two

Feed 
Forward

Feed 
Forward

Feed 
Forward

Feed 
Forward

this is para one this is para two

Figure 2: A global transformer layer. Different col-
ors indicate different heads in multi-head pooling and
inter-paragraph attention.

We then add ci to each input token vector xl−1ij ,
and feed it to a two-layer feed-forward network
with ReLU as the activation function and a high-
way layer normalization on top:

gij =Wo2ReLU(Wo1(x
l−1
ij + ci)) (22)

xlij = LayerNorm(gij + x
l−1
ij ) (23)

where Wo1 ∈ Rdff∗d and Wo2 ∈ Rd∗dff are the
weights, dff is the hidden size of the feed-forward
later. This way, each token within paragraph Ri
can collect information from other paragraphs in a
hierarchical and efficient manner.

3.2.4 Graph-informed Attention
The inter-paragraph attention mechanism can be
viewed as learning a latent graph representation
(self-attention weights) of the input paragraphs.
Although previous work has shown that simi-
lar latent representations are beneficial for down-
stream NLP tasks (Liu and Lapata, 2018; Kim
et al., 2017; Williams et al., 2018; Niculae et al.,
2018; Fernandes et al., 2019), much work in
multi-document summarization has taken advan-
tage of explicit graph representations, each focus-
ing on different facets of the summarization task



5075

(e.g., capturing redundant information or repre-
senting passages referring to the same event or
entity). One advantage of the hierarchical trans-
former is that we can easily incorporate graphs ex-
ternal to the model, to generate better summaries.

We experimented with two well-established
graph representations which we discuss briefly be-
low. However, there is nothing inherent in our
model that restricts us to these, any graph mod-
eling relationships across paragraphs could have
been used instead. Our first graph aims to capture
lexical relations; graph nodes correspond to para-
graphs and edge weights are cosine similarities
based on tf-idf representations of the paragraphs.
Our second graph aims to capture discourse re-
lations (Christensen et al., 2013); it builds an
Approximate Discourse Graph (ADG) (Yasunaga
et al., 2017) over paragraphs; edges between para-
graphs are drawn by counting (a) co-occurring en-
tities and (b) discourse markers (e.g., however,
nevertheless) connecting two adjacent paragraphs
(see the Appendix for details on how ADGs are
constructed).

We represent such graphs with a matrix G,
where Gii′ is the weight of the edge connecting
paragraphs i and i′. We can then inject this graph
into our hierarchical transformer by simply substi-
tuting one of its (learned) heads z′ with G. Equa-
tion (20) for calculating the context vector for this
head is modified as:

contextz
′

i =
m∑

i′=1

Gii′∑m
o=1Gio

vz
′

i′ (24)

4 Experimental Setup

WikiSum Dataset We used the scripts and urls
provided in Liu et al. (2018) to crawl Wikipedia
articles and source reference documents. We suc-
cessfully crawled 78.9% of the original documents
(some urls have become invalid and correspond-
ing documents could not be retrieved). We fur-
ther removed clone paragraphs (which are exact
copies of some parts of the Wikipedia articles);
these were paragraphs in the source documents
whose bigram recall against the target summary
was higher than 0.8. On average, each input
has 525 paragraphs, and each paragraph has 70.1
tokens. The average length of the target sum-
mary is 139.4 tokens. We split the dataset with
1, 579, 360 instances for training, 38, 144 for vali-
dation and 38, 205 for test.

Methods
ROUGE-L Recall

L′ = 5 L′ = 10 L′ = 20 L′ = 40

Similarity 24.86 32.43 40.87 49.49
Ranking 39.38 46.74 53.84 60.42

Table 1: ROUGE-L recall against target summary for
L′-best paragraphs obtained with tf-idf cosine similar-
ity and our ranking model.

For both ranking and summarization stages,
we encode source paragraphs and target sum-
maries using subword tokenization with Sentence-
Piece (Kudo and Richardson, 2018). Our vocabu-
lary consists of 32, 000 subwords and is shared for
both source and target.

Paragraph Ranking To train the regression
model, we calculated the ROUGE-2 recall (Lin,
2004) of each paragraph against the target sum-
mary and used this as the ground-truth score. The
hidden size of the two LSTMs was set to 256,
and dropout (with dropout probability of 0.2) was
used before all linear layers. Adagrad (Duchi
et al., 2011) with learning rate 0.15 is used for
optimization. We compare our ranking model
against the method proposed in Liu et al. (2018)
who use the tf-idf cosine similarity between each
paragraph and the article title to rank the input
paragraphs. We take the first L′ paragraphs from
the ordered paragraph set produced by our ranker
and the similarity-based method, respectively. We
concatenate these paragraphs and calculate their
ROUGE-L recall against the gold target text. The
results are shown in Table 1. We can see that our
ranker effectively extracts related paragraphs and
produces more informative input for the down-
stream summarization task.

Training Configuration In all abstractive mod-
els, we apply dropout (with probability of 0.1) be-
fore all linear layers; label smoothing (Szegedy
et al., 2016) with smoothing factor 0.1 is also used.
Training is in traditional sequence-to-sequence
manner with maximum likelihood estimation. The
optimizer was Adam (Kingma and Ba, 2014) with
learning rate of 2, β1 = 0.9, and β2 = 0.998;
we also applied learning rate warmup over the
first 8, 000 steps, and decay as in (Vaswani et al.,
2017). All transformer-based models had 256 hid-
den units; the feed-forward hidden size was 1, 024
for all layers. All models were trained on 4 GPUs
(NVIDIA TITAN Xp) for 500, 000 steps. We used



5076

Model ROUGE-1 ROUGE-2 ROUGE-L
Lead 38.22 16.85 26.89
LexRank 36.12 11.67 22.52
FT (600 tokens, no ranking) 35.46 20.26 30.65
FT (600 tokens) 40.46 25.26 34.65
FT (800 tokens) 40.56 25.35 34.73
FT (1,200 tokens) 39.55 24.63 33.99
T-DMCA (3000 tokens) 40.77 25.60 34.90
HT (1,600 tokens) 40.82 25.99 35.08
HT (1,600 tokens) + Similarity Graph 40.80 25.95 35.08
HT (1,600 tokens) + Discourse Graph 40.81 25.95 35.24
HT (train on 1,600 tokens/test on 3000 tokens) 41.53 26.52 35.76

Table 2: Test set results on the WikiSum dataset using ROUGE F1.

gradient accumulation to keep training time for all
models approximately consistent. We selected the
5 best checkpoints based on performance on the
validation set and report averaged results on the
test set.

During decoding we use beam search with beam
size 5 and length penalty with α = 0.4 (Wu et al.,
2016); we decode until an end-of-sequence token
is reached.

Comparison Systems We compared the pro-
posed hierarchical transformer against several
strong baselines:

Lead is a simple baseline that concatenates the ti-
tle and ranked paragraphs, and extracts the
first k tokens; we set k to the length of the
ground-truth target.

LexRank (Erkan and Radev, 2004) is a widely-
used graph-based extractive summarizer; we
build a graph with paragraphs as nodes and
edges weighted by tf-idf cosine similarity; we
run a PageRank-like algorithm on this graph
to rank and select paragraphs until the length
of the ground-truth summary is reached.

Flat Transformer (FT) is a baseline that applies
a Transformer-based encoder-decoder model
to a flat token sequence. We used a 6-layer
transformer. The title and ranked paragraphs
were concatenated and truncated to 600, 800,
and 1, 200 tokens.

T-DMCA is the best performing model of Liu
et al. (2018) and a shorthand for Transformer
Decoder with Memory Compressed Atten-
tion; they only used a Transformer decoder

and compressed the key and value in self-
attention with a convolutional layer. The
model has 5 layers as in Liu et al. (2018).
Its hidden size is 512 and its feed-forward
hidden size is 2, 048. The title and ranked
paragraphs were concatenated and truncated
to 3,000 tokens.

Hierarchical Transformer (HT) is the model
proposed in this paper. The model archi-
tecture is a 7-layer network (with 5 local-
attention layers at the bottom and 2 global at-
tention layers at the top). The model takes
the title and L′ = 24 paragraphs as input to
produce a target summary, which leads to ap-
proximately 1, 600 input tokens per instance.

5 Results

Automatic Evaluation We evaluated summa-
rization quality using ROUGE F1 (Lin, 2004). We
report unigram and bigram overlap (ROUGE-1
and ROUGE-2) as a means of assessing infor-
mativeness and the longest common subsequence
(ROUGE-L) as a means of assessing fluency.

Table 2 summarizes our results. The first
block in the table includes extractive systems
(Lead, LexRank), the second block includes sev-
eral variants of Flat Transformer-based models
(FT, T-DMCA), while the rest of the table presents
the results of our Hierarchical Transformer (HT).
As can be seen, abstractive models generally out-
perform extractive ones. The Flat Transformer,
achieves best results when the input length is set
to 800 tokens, while longer input (i.e., 1, 200 to-
kens) actually hurts performance. The Hierarchi-
cal Transformer with 1, 600 input tokens, outper-



5077

Model R1 R2 RL
HT 40.82 25.99 35.08
HT w/o PP 40.21 24.54 34.71
HT w/o MP 39.90 24.34 34.61
HT w/o GT 39.01 22.97 33.76

Table 3: Hierarchical Transformer and versions thereof
without (w/o) paragraph position (PP), multi-head
pooling (MP), and global transformer layer (GT).

forms FT, and even T-DMCA when the latter is
presented with 3, 000 tokens. Adding an external
graph also seems to help the summarization pro-
cess. The similarity graph does not have an ob-
vious influence on the results, while the discourse
graph boosts ROUGE-L by 0.16.

We also found that the performance of the Hi-
erarchical Transformer further improves when the
model is presented with longer input at test time.2

As shown in the last row of Table 2, when test-
ing on 3, 000 input tokens, summarization quality
improves across the board. This suggests that the
model can potentially generate better summaries
without increasing training time.

Table 3 summarizes ablation studies aiming to
assess the contribution of individual components.
Our experiments confirmed that encoding para-
graph position in addition to token position within
each paragraph is beneficial (see row w/o PP), as
well as multi-head pooling (w/o MP is a model
where the number of heads is set to 1), and the
global transformer layer (w/o GT is a model with
only 5 local transformer layers in the encoder).

Human Evaluation In addition to automatic
evaluation, we also assessed system performance
by eliciting human judgments on 20 randomly se-
lected test instances. Our first evaluation study
quantified the degree to which summarization
models retain key information from the documents
following a question-answering (QA) paradigm
(Clarke and Lapata, 2010; Narayan et al., 2018).
We created a set of questions based on the gold
summary under the assumption that it contains the
most important information from the input para-
graphs. We then examined whether participants
were able to answer these questions by reading
system summaries alone without access to the gold
summary. The more questions a system can an-
swer, the better it is at summarization. We cre-
ated 57 questions in total varying from two to

2This was not the case with the other Transformer models.

Model QA Rating
Lead 31.59 -0.383
FT 35.69 0.000
T-DMCA 43.14 0.147
HT 54.11 0.237

Table 4: System scores based on questions answered
by AMT participants and summary quality rating.

four questions per gold summary. Examples of
questions and their answers are given in Table 5.
We adopted the same scoring mechanism used
in Clarke and Lapata (2010), i.e., correct answers
are marked with 1, partially correct ones with 0.5,
and 0 otherwise. A system’s score is the average
of all question scores.

Our second evaluation study assessed the over-
all quality of the summaries by asking partici-
pants to rank them taking into account the fol-
lowing criteria: Informativeness (does the sum-
mary convey important facts about the topic in
question?), Fluency (is the summary fluent and
grammatical?), and Succinctness (does the sum-
mary avoid repetition?). We used Best-Worst Scal-
ing (Louviere et al., 2015), a less labor-intensive
alternative to paired comparisons that has been
shown to produce more reliable results than rating
scales (Kiritchenko and Mohammad, 2017). Par-
ticipants were presented with the gold summary
and summaries generated from 3 out of 4 systems
and were asked to decide which summary was the
best and which one was the worst in relation to
the gold standard, taking into account the criteria
mentioned above. The rating of each system was
computed as the percentage of times it was chosen
as best minus the times it was selected as worst.
Ratings range from −1 (worst) to 1 (best).

Both evaluations were conducted on the Ama-
zon Mechanical Turk platform with 5 responses
per hit. Participants evaluated summaries pro-
duced by the Lead baseline, the Flat Transformer,
T-DMCA, and our Hierarchical Transformer. All
evaluated systems were variants that achieved the
best performance in automatic evaluations. As
shown in Table 4, on both evaluations, participants
overwhelmingly prefer our model (HT). All pair-
wise comparisons among systems are statistically
significant (using a one-way ANOVA with post-
hoc Tukey HSD tests; p < 0.01). Examples of
system output are provided in Table 5.



5078

Pentagoet Archeological District

G
O

L
D

The Pentagoet Archeological District is a National Historic Landmark District located at the southern edge of
the Bagaduce Peninsula in Castine, Maine. It is the site of Fort Pentagoet, a 17th-century fortified trading post
established by fur traders of French Acadia. From 1635 to 1654 this site was a center of trade with the local
Abenaki, and marked the effective western border of Acadia with New England. From 1654 to 1670 the site
was under English control, after which it was returned to France by the Treaty of Breda. The fort was destroyed
in 1674 by Dutch raiders. The site was designated a National Historic Landmark in 1993. It is now a public
park.

Q
A

What is the Pentagoet Archeological District? [a National Historic Landmark District]
Where is it located? [Castine , Maine]
What did the Abenaki Indians use the site for? [trading center]

L
E

A
D

The Pentagoet Archeological District is a National Historic Landmark District located in Castine, Maine. This
district forms part of the traditional homeland of the Abenaki Indians, in particular the Penobscot tribe. In
the colonial period, Abenakis frequented the fortified trading post at this site, bartering moosehides, sealskins,
beaver and other furs in exchange for European commodities. ”Pentagoet Archeological district” is a National
Historic Landmark District located at the southern edge of the Bagaduce Peninsula in Treaty Of Breda.

F
T

the Pentagoet Archeological district is a National Historic Landmark District located at
the southern edge of the Bagaduce Peninsula in Treaty Of Breda. It was listed on the
national register of historic places in 1983.

T-
D

M
C

A The Pentagoet Archeological District is a national historic landmark district located in castine , maine . this
district forms part of the traditional homeland of the abenaki indians , in particular the Penobscot tribe. The
district was listed on the national register of historic places in 1982.

H
T

The Pentagoet Archeological district is a National Historic Landmark District located in Castine, Maine. This
district forms part of the traditional homeland of the Abenaki Indians, in particular the Penobscot tribe. In
the colonial period, Abenaki frequented the fortified trading post at this site, bartering moosehides, sealskins,
beaver and other furs in exchange for European commodities.

Melanesian Whistler

G
O

L
D The Melanesian whistler or Vanuatu whistler (Pachycephala chlorura) is a species of passerine bird in the

whistler family Pachycephalidae. It is found on the Loyalty Islands, Vanuatu, and Vanikoro in the far south-
eastern Solomons.

Q
A What is the Melanesian Whistler? [a species of passerine bird in the whistler family Pachycephalidae]

Where is it found? [Loyalty Islands , Vanuatu , and Vanikoro in the far south-eastern Solomons]

L
E

A
D The Australian golden whistler (Pachycephala pectoralis) is a species of bird found in forest, woodland, mallee,

mangrove and scrub in Australia (except the interior and most of the north) Most populations are resident, but
some in south-eastern Australia migrate north during the winter.

F
T The Melanesian whistler (P. Caledonica) is a species of bird in the family Muscicapidae. It is endemic to

Melanesia.

T-
D

M
C

A

The Australian golden whistler (Pachycephala chlorura) is a species of bird in the family Pachycephalidae,
which is endemic to Fiji.

H
T The Melanesian whistler (Pachycephala chlorura) is a species of bird in the family Pachycephalidae, which is

endemic to Fiji.

Table 5: GOLD human authored summaries, questions based on them (answers shown in square brackets) and
automatic summaries produced by the LEAD-3 baseline, the Flat Transformer (FT), T-DMCA (Liu et al., 2018)
and our Hierachical Transformer (HT).

6 Conclusions

In this paper we conceptualized abstractive multi-
document summarization as a machine learning
problem. We proposed a new model which is
able to encode multiple input documents hierar-
chically, learn latent relations across them, and ad-
ditionally incorporate structural information from
well-known graph representations. We have also
demonstrated the importance of a learning-based
approach for selecting which documents to sum-
marize. Experimental results show that our model
produces summaries which are both fluent and in-

formative outperforming competitive systems by a
wide margin. In the future we would like to apply
our hierarchical transformer to question answering
and related textual inference tasks.

Acknowledgments

We would like to thank Laura Perez-Beltrachini
for her help with preprocessing the dataset. This
research is supported by a Google PhD Fellow-
ship to the first author. The authors gratefully ac-
knowledge the financial support of the European
Research Council (award number 681760).



5079

References
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-

ton. 2016. Layer normalization. arXiv preprint
arXiv:1607.06450.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In In Proceedings of
the 3rd International Conference on Learning Rep-
resentations, San Diego, California.

Regina Barzilay and Kathleen R. McKeown. 2005.
Sentence fusion for multidocument news summa-
rization. Computational Linguistics, 31(3):297–
327.

Lidong Bing, Piji Li, Yi Liao, Wai Lam, Weiwei Guo,
and Rebecca Passonneau. 2015. Abstractive multi-
document summarization via phrase selection and
merging. In Proceedings of the 53rd Annual Meet-
ing of the Association for Computational Linguistics
and the 7th International Joint Conference on Natu-
ral Language Processing (Volume 1: Long Papers),
pages 1587–1597, Beijing, China.

Asli Celikyilmaz, Antoine Bosselut, Xiaodong He, and
Yejin Choi. 2018. Deep communicating agents for
abstractive summarization. In Proceedings of the
2018 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, Volume 1 (Long Pa-
pers), pages 1662–1675, New Orleans, Louisiana.

Janara Christensen, Mausam, Stephen Soderland, and
Oren Etzioni. 2013. Towards coherent multi-
document summarization. In Proceedings of the
2013 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 1163–1173, At-
lanta, Georgia. Association for Computational Lin-
guistics.

Eric Chu and Peter J Liu. 2018. Unsupervised neural
multi-document abstractive summarization. arXiv
preprint arXiv:1810.05739.

James Clarke and Mirella Lapata. 2010. Discourse
constraints for document compression. Computa-
tional Linguistics, 36(3):411–441.

John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. Journal of Machine
Learning Research, 12(Jul):2121–2159.

Günes Erkan and Dragomir R Radev. 2004. Lexrank:
Graph-based lexical centrality as salience in text
summarization. Journal of artificial intelligence re-
search, 22:457–479.

Patrick Fernandes, Miltiadis Allamanis, and Marc
Brockschmidt. 2019. Structured neural summariza-
tion. In Proceedings of the 7th International Con-
ference on Learning Representations, New Orleans,
Louisiana.

Katja Filippova and Michael Strube. 2008. Sentence
fusion via dependency graph compression. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing, pages 177–
185, Honolulu, Hawaii.

Sebastian Gehrmann, Yuntian Deng, and Alexander
Rush. 2018. Bottom-up abstractive summarization.
In Proceedings of the 2018 Conference on Empiri-
cal Methods in Natural Language Processing, pages
4098–4109, Brussels, Belgium.

Max Grusky, Mor Naaman, and Yoav Artzi. 2018.
Newsroom: A dataset of 1.3 million summaries with
diverse extractive strategies. In Proceedings of the
2018 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, Volume 1 (Long Pa-
pers), pages 708–719, New Orleans, Louisiana.

Karl Moritz Hermann, Tomas Kocisky, Edward
Grefenstette, Lasse Espeholt, Will Kay, Mustafa Su-
leyman, and Phil Blunsom. 2015. Teaching ma-
chines to read and comprehend. In Advances in Neu-
ral Information Processing Systems 28, pages 1693–
1701. Curran Associates, Inc.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Yoon Kim, Carl Denton, Luong Hoang, and Alexan-
der M Rush. 2017. Structured attention networks.
In Proceedings of the 5th International Conference
on Learning Representations, Toulon, France.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Thomas N Kipf and Max Welling. 2017. Semi-
supervised classification with graph convolutional
networks. In Proceedings of the 4th International
Conference on Learning Representations, San Juan,
Puerto Rico.

Svetlana Kiritchenko and Saif Mohammad. 2017.
Best-worst scaling more reliable than rating scales:
A case study on sentiment intensity annotation. In
Proceedings of the 55th Annual Meeting of the As-
sociation for Computational Linguistics, pages 465–
470, Vancouver, Canada.

Taku Kudo and John Richardson. 2018. Sentencepiece:
A simple and language independent subword tok-
enizer and detokenizer for neural text processing.
arXiv preprint arXiv:1808.06226.

Logan Lebanoff and Fei Liu. 2018. Automatic detec-
tion of vague words and sentences in privacy poli-
cies. In Proceedings of the 2018 Conference on Em-
pirical Methods in Natural Language Processing,
pages 3508–3517, Brussels, Belgium.



5080

Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. In Text Summarization
Branches Out: Proceedings of the ACL-04 Work-
shop, pages 74–81, Barcelona, Spain. Association
for Computational Linguistics.

Peter J Liu, Mohammad Saleh, Etienne Pot, Ben
Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam
Shazeer. 2018. Generating Wikipedia by summariz-
ing long sequences. In Proceedings of the 6th Inter-
national Conference on Learning Representations,
Vancouver, Canada.

Yang Liu and Mirella Lapata. 2018. Learning struc-
tured text representations. Transactions of the Asso-
ciation for Computational Linguistics, 6:63–75.

Jordan J Louviere, Terry N Flynn, and Anthony Al-
fred John Marley. 2015. Best-worst scaling: The-
ory, methods and applications. Cambridge Univer-
sity Press.

Shulei Ma, Zhi-Hong Deng, and Yunlun Yang. 2016.
An unsupervised multi-document summarization
framework based on neural document model. In
Proceedings of COLING 2016, the 26th Interna-
tional Conference on Computational Linguistics:
Technical Papers, pages 1514–1523, Osaka, Japan.

Shashi Narayan, Shay B. Cohen, and Mirella Lapata.
2018. Ranking sentences for extractive summariza-
tion with reinforcement learning. In Proceedings of
the 2018 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, Volume 1 (Long Pa-
pers), pages 1747–1759, New Orleans, Louisiana.

Vlad Niculae, André F. T. Martins, and Claire Cardie.
2018. Towards dynamic computation graphs via
sparse latent structure. In Proceedings of the 2018
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 905–911, Brussels, Bel-
gium.

Daraksha Parveen and Michael Strube. 2014. Multi-
document summarization using bipartite graphs. In
Proceedings of TextGraphs-9: the workshop on
Graph-based Methods for Natural Language Pro-
cessing, pages 15–24, Doha, Qatar.

Romain Paulus, Caiming Xiong, and Richard Socher.
2018. A deep reinforced model for abstractive sum-
marization. In Proceedings of the 6th International
Conference on Learning Representations, Vancou-
ver, Canada.

Dragomir Radev. 2000. A common theory of infor-
mation fusion from multiple text sources step one:
Cross-document structure. In 1st SIGdial Workshop
on Discourse and Dialogue, pages 74–83, Hong
Kong, China.

Evan Sandhaus. 2008. The New York Times Annotated
Corpus. Linguistic Data Consortium, Philadelphia,
6(12).

Abigail See, Peter J. Liu, and Christopher D. Manning.
2017. Get to the point: Summarization with pointer-
generator networks. In Proceedings of the 55th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1073–
1083, Vancouver, Canada.

Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe,
Jon Shlens, and Zbigniew Wojna. 2016. Rethinking
the inception architecture for computer vision. In
The IEEE Conference on Computer Vision and Pat-
tern Recognition (CVPR).

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems 30, pages 5998–6008. Curran Asso-
ciates, Inc.

Xiaojun Wan. 2008. An exploration of document
impact on graph-based multi-document summariza-
tion. In Proceedings of the 2008 Conference on Em-
pirical Methods in Natural Language Processing,
pages 755–762, Honolulu, Hawaii.

Adina Williams, Andrew Drozdov, and Samuel R.
Bowman. 2018. Do latent tree learning models iden-
tify meaningful structure in sentences? Transac-
tions of the Association for Computational Linguis-
tics, 6:253–267.

Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V
Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus
Macherey, et al. 2016. Google’s neural machine
translation system: Bridging the gap between hu-
man and machine translation. In arXiv preprint
arXiv:1609.08144.

Michihiro Yasunaga, Rui Zhang, Kshitijh Meelu,
Ayush Pareek, Krishnan Srinivasan, and Dragomir
Radev. 2017. Graph-based neural multi-document
summarization. In Proceedings of the 21st Confer-
ence on Computational Natural Language Learning
(CoNLL 2017), pages 452–462, Vancouver, Canada.

Jianmin Zhang, Jiwei Tan, and Xiaojun Wan. 2018.
Adapting neural single-document summarization
model for abstractive multi-document summariza-
tion: A pilot study. In Proceedings of the Interna-
tional Conference on Natural Language Generation.

A Appendix

We describe here how the similarity and discourse
graphs discussed in Section 3.2.4 were created.
These graphs were added to the hierarchical trans-
former model as a means to enhance summary
quality (see Section 5 for details).



5081

A.1 Similarity Graph

The similarity graph S is based on tf-idf cosine
similarity. The nodes of the graph are paragraphs.
We first represent each paragraph pi as a bag of
words. Then, we calculate the tf-idf value vik for
each token tik in a paragraph:

vik = Nw(tik)log(
Nd

Ndw(tik)
) (25)

where Nw(t) is the count of word t in the para-
graph, Nd is the total number of paragraphs,
and Ndw(t) is the total number of paragraphs con-
taining the word. We thus obtain a tf-idf vector
for each paragraph. Then, for all paragraph pairs
< pi, pi′ >, we calculate the cosine similarity of
their tf-idf vectors and use this as the weight Sii′
for the edge connecting the pair in the graph. We
remove edges with weights lower than 0.2.

A.2 Discourse Graphs

To build the Approximate Discourse Graph
(ADG)D, we follow Christensen et al. (2013) and
Yasunaga et al. (2017). The original ADG makes
use of several complex features. Here, we create
a simplified version with only two features (nodes
in this graph are again paragraphs).

Co-occurring Entities For each paragraph pi,
we extract a set of entities Ei in the paragraph
using the Spacy3 NER recognizer. We only use
entities with type {PERSON, NORP, FAC,
ORG, GPE, LOC, EVENT, WORK OF ART,
LAW}. For each paragraph pair < pi, pj >, we
count eij , the number of entities with exact match.

Discourse Markers We use the following 36 ex-
plicit discourse markers to identify edges between
two adjacent paragraphs in a source webpage:

again, also, another, comparatively, fur-
thermore, at the same time,however, im-
mediately, indeed, instead, to be sure,
likewise, meanwhile, moreover, never-
theless, nonetheless, notably, otherwise,
regardless, similarly, unlike, in addition,
even, in turn, in exchange, in this case,
in any event, finally, later, as well, espe-
cially, as a result, example, in fact, then,
the day before

3https://spacy.io/api/entityrecognizer

If two paragraphs < pi, pi′ > are adjacent in one
source webpage and they are connected with one
of the above 36 discourse markers, mii′ will be 1,
otherwise it will be 0.

The final edge weight Dii′ is the weighted sum
of eii′ and mii′

Dii′ = 0.2 ∗ eii′ +mii′ (26)


