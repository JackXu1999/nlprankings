



















































OPI: Semeval-2014 Task 3 System Description


Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 454–458,
Dublin, Ireland, August 23-24, 2014.

OPI: Semeval-2014 Task 3 System Description

Marek Kozlowski
National Information Processing Institute

mkozlowski@opi.org.pl

Abstract

In this paper, we describe the OPI system
participating in the Semeval-2014 task 3
Cross-Level Semantic Similarity. Our ap-
proach is knowledge-poor, there is no ex-
ploitation of any structured knowledge re-
sources as Wikipedia, WordNet or Babel-
Net. The method is also fully unsuper-
vised, the training set is only used in order
to tune the system. System measures the
semantic similarity of texts using corpus-
based measures of termsets similarity.

1 Introduction

The task Cross-Level Semantic Similarity of
SemEval-2014 aims at an evaluation for seman-
tic similarity across different sizes of text (lexi-
cal levels). Unlike prior SemEval tasks on textual
similarity that have focused on comparing similar-
sized texts, the mentioned task evaluates the case
where larger text must be compared to smaller
text, namely there are covered four semantic sim-
ilarity comparisons: paragraph to sentence, sen-
tence to phrase, phrase to word and word to sense.

We present the method for measuring the se-
mantic similarity of texts using a corpus-based
measure of termsets (set of words) similarity. We
start from preprocessing texts, identifying bound-
ary values, computing termsets similarities and de-
rive from them the final score, which is normal-
ized.

The input of the task consists of two text seg-
ments of different level. We want to determine
a score indicating their semantic similarity of the
smaller item to the larger item. Similarity is scored
from 0 to 4, when 0 means no semantic intersec-

This work is licensed under a Creative Commons Attribution
4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http:
//creativecommons.org/licenses/by/4.0/

tion, 4 means that two items have very similar
meanings.

2 Related Work

There are lots of papers about measuring the
similarity between documents and single words.
Document-level similarity works are based on
Vector Space Models (Salton and Lesk, 1971;
Salton and McGill, 1983). A significant effort has
also been put into measuring similarity at the word
level, namely by approaches that use distributional
semantics (Turney and Pantel, 2010).

Related work can be classified into four ma-
jor categories: vector-based document mod-
els methods, corpus-based methods, knowledge-
based methods and hybrid methods (Islam and
Inkpen, 2008).

Vector-based document models represent docu-
ment as a vector of words and the similarity eval-
uation is based on the number of words that oc-
cur in both texts. Lexical similarity methods have
problems with different words sharing common
sense. Next approaches, such as corpus-based and
knowledge-based methods, overcome the above
issues.

Corpus based methods apply scores provided by
Pointwise Mutual Information (PMI) and Latent
Semantic Analysis (LSA).

The Pointwise Mutual Information (PMI) (Tur-
ney, 2001) between two words wi and wj is:

PMI(wi, wj) = log2
p(wi, wj)

p(wi)p(wj)

The Latent Semantic Analysis (LSA) (Landauer
and Dumais, 1997; Landauer et al., 2007) is a
mathematical method for modelling of the mean-
ing of words and contexts by analysis of represen-
tative corpora. It models the meaning of words and
contexts by projecting them into a vector space of
reduced dimensionality, which is built up by ap-
plying singular value decomposition (SVD).

454



Knowledge based methods apply information
from semantic networks as WordNet. They ex-
ploit the structure of WordNet to compare con-
cepts. Leacock and Chodorow (1998) proposed
metric based on the length of the shortest path be-
tween two concepts. Lesk (1986) defined sim-
ilarity between concepts as the intersection be-
tween the corresponding glosses. Budanitsky and
Hirst (2006) conducted the research on various
WordNet-based measures. Standard thesaurus-
based measures of word pair similarity are based
only on a single path between concepts. By con-
trast Hughes and Ramage (2009) used a seman-
tic representation of texts from random walks on
WordNet.

Hybrid methods use both corpus-based mea-
sures and knowledge-based measures of word se-
mantic similarity to determine the text similarity
(Islam and Inkpen, 2008). Mihalcea and Corley
(2006) suggested a combined method by exploit-
ing corpus based measures and knowledge-based
measures of words semantic similarity. Another
hybrid method was proposed by Li et al. (2006)
that combines semantic and syntactic information.

The methods presented above are working at
fixed level of textual granularity (documents,
phrases, or words). Pilehvar et al. (2013) proposed
a unified approach to semantic similarity that oper-
ates at multiple levels. The method builds a com-
mon probabilistic representation over word senses
in order to compare different types of linguistic
data. Any lexical item is represented as a distri-
bution over a set of word senses (obtained from
WordNet), named as item’s semantic signature.

3 Our Approach

Our system is fully unsupervised and knowledge-
poor. It exploits Wikipedia as a raw corpus for
words co-occurrence estimation. The proposed
method is not using any kind of textual alignment
(e.g. exploiting PoS tagging or WordNet con-
cepts).

The method consists of four steps: prepro-
cessing, identifying boundary values, termset-to-
termset similarity computation, text-to-text sim-
ilarity phase, results normalization. The results
from the text-to-text similarity phase are very of-
ten beyond the range 0-4, therefore we must nor-
malize them. We evaluated two normalization
approaches: linear normalization and non-linear
one. The non-linear normalization is based on

built clusters (referring to integer values from 0 to
4), which are created using training data set. This
step will be described in details in the section 3.5.

3.1 Preprocessing
In the first step the compared texts are retrieved,
and then processed into the contexts. Context is
the preprocessed original text represented as a bag
of words. Texts are processed using a dictionary
of proper names, name entities recognizers, PoS-
taggers, providing as a result the required contexts.
Contexts contain nouns, adjectives, adverbs and
proper names. The output of this stage is a pair
of contexts passed to the next phase.

3.2 Identifying Boundary Values
This phase is introduced in order to fast detect
texts, which are unrelated (0 score) or very sim-
ilar (4 score). Unrelated ones are identified bas-
ing on the lack of any co-occurrences between
words from compared texts. It means that any
pair of words from compared contexts do not ap-
pear together in any Wikipedia paragraph. The
very similar texts are identified in two steps. At
first we check if all words from the shorter texts
are contained in the longer one. If the first check
is not fulfilled we compute: (c1,2) as the num-
ber of Wikipedia paragraphs that contain all of
words from both contexts in the nearest neigh-
borhood (20-words window), (c1) and (c2) as the
numbers of Wikipedia paragraphs that contain
contexts within 20-words window. If the ratio
c1,2/max(c1, c2) is higher than 50% then the ana-
lyzed pair of texts refers to the same concept (very
similar ones). Having two texts represented by
contexts we use the proximity Lucene1 query in
order to estimate the number of Wikipedia para-
graphs, which contain the words from contexts
within the 20-words window.

3.3 Termset-to-termset Similarity
Termset-to-termset similarity (t2tSim) is defined
by measure similar to PMI. Given a dictionary D
and two termsets (set of words) Wi ⊆ D and
Wj ⊆ D then the measure is expressed by the for-
mula:

t2tSim(Wi, Wj) =
c(Wi, Wj)

min(c(Wi), c(Wj))

Here, c(X1, .., Xn) is a number of Wikipedia para-
graphs that contain all terms covered by termsets

1http://lucene.apache.org/core/

455



X1, .., Xn. Two input termsets are semantically
close if the similarity measure t2tSim is higher
than the user-defined threshold (e.g. 10%). Com-
paring to the previous step we use the minimum
operator in the formula’s denominator in order to
take into account even one directed relevant asso-
ciation. It was proved experimentally that the pro-
posed measure leads to better results than the PMI
measure using NEAR query (co-occurrence within
a 10-words window). Specifically, the following
formula is used to collect the PMI value between
termsets using the Wikipedia as a background cor-
pus:

PMI(Wi, Wj) = log2
c(Wi, Wj) ∗WikiSize

c(Wi) ∗ c(Wj)
In the performed experiments we approximated
the value of WikiSize to 30 millions (number of
paragraphs of English articles in Wikipedia). In
table 1 we present results of Spearman correla-
tion reported by the System using different mea-
sures PMI and t2tSim. The second measure is
slightly better therefore it was chosen as the final
one. These correlations were computed after lin-
ear normalization of the output measures.

Level Measure Spearman
correlation

word2sense PMI 19
word2sense t2tSim 19
phrase2word PMI 29
phrase2word t2tSim 29
sentence2phrase PMI 45
sentence2phrase t2tSim 47
paragraph2sentence PMI 48
paragraph2sentence t2tSim 49

Table 1: Comparison of PMI and t2tSim mea-
sures in the semantic similarity task using Spear-
man correlation (percentages).

3.4 Text-to-text Similarity
Given two input texts we compute the termset-to-
termset similarities in order to derive the final se-
mantic score. We attempt to model the semantic
similarity of texts as a function of the semantic
similarities of the component termsets. We do this
by combining metrics of termset-to-termset simi-
larities and weights into a formula that is a poten-
tially good indicator of semantic similarity of the
two input texts. Weights (wm1 > wm2 > wm3)

are experimentally set with linear scalable values
wm1 = 4, wm2 = 2, wm3 = 1 respectively. The
pseudo-code of this phase is in Algorithm 1.

Algorithm 1 Text-to-text similarity
Input: cs, cl are contexts representing shorter and

longer texts respectively; wm1, wm2, wm3 as
weights for different scopes of similarity com-
parison;

Output: m as a similarity measure
m = 0
m = m + t2tSim(cs, cl) ∗ wm1
for term ti ∈ cl do

m = m + t2tSim(cs, {ti}) ∗ wm2
end for
for term tj ∈ cs do

m = m + t2tSim(cl, {tj}) ∗ wm2
end for
for term ti ∈ cs do

for term tj ∈ cl do
m = m + t2tSim({ti}, {tj}) ∗ wm3

end for
end for
return m

3.5 Results Normalization
The crucial part of the method is a process of nor-
malization obtained measures into the range (0,4).
The values 0 and 4 are covered by the step de-
scribed in the section 3.2. We need to normalize
values from the text-to-text similarity phase. This
step can be done in two ways: linear normaliza-
tion and non-linear one. The first one is a ca-
sual transformation defined as dividing elements
by theirs maximum and scaling to 4. The sec-
ond one is based on clustering training set. In
other words, using training set we induce rules
how reported text-to-text similarity values should
be transformed into the range (0,4). We imple-
mented hierarchical agglomerative clustering al-
gorithm (with average linkage)2 in order to clus-
ter similarity measures into five distinct groups.
Sorted centroids of the above created groups are
labeled with values 0 to 4 respectively. For each
new similarity measure (obtained in the testing
phase) we measure the distance to the closest clus-
ter’s centroids. The final value is derived linearly

2Hierarchical Agglomerative Clustering treats initially
each instance as a singleton cluster and then successively
agglomerate pairs of clusters using the average distance be-
tween cluster’s elements until the user defined number of
clusters persist.

456



from the distance to the centroids (i.e. if the value
is in the middle between centroids referring to 1
and 2, we assign as a final value 1.5). In the test-
ing step we use the non-linear normalization, the
evaluations on training set show that clustering
based approach provides marginal improvement
against linear normalization (about 1% according
to Spearman rank, 4-8% according to Pearson cor-
relation).

4 Results

In Task 3, systems were evaluated both within
one of four comparison types and also across all
comparison types. The system outputs and gold
standard ratings are compared in two ways, us-
ing Pearson correlation and Spearman’s rank cor-
relation (rho). Pearson correlation tests the degree
of similarity between the system’s similarity rat-
ings and the gold standard ratings. Spearman’s
rho tests the degree of similarity between the rank-
ings of the items according to similarity. Ranks
were computed by summing the correlation val-
ues across all four levels of comparisons. The sum
of the Pearson correlations is used for the official
rank of Task 3. However, the organizers provide
a second ranking using the sum of the Spearman
correlations.

Level System Pearson/
Spearman

word2sense OPI 15.2/13.1
word2sense SimCompass 35.6/34.4
word2sense Baseline 10.9/13.0
phrase2word OPI 21.3/18.8
phrase2word SimCompass 41.5/42.4
phrase2word Baseline 16.5/16.2
sentence2phrase OPI 43.3/42.4
sentence2phrase SimCompass 74.2/72.8
sentence2phrase Baseline 56.2/62.6

Table 2: Results for Pearson and Spearman corre-
lation (percentages) scored by OPI System, Sim-
Compass (the best performing one) and the Base-
line one.

We submitted only one run in three compari-
son types. We avoided the paragraph-to-sentence
comparison. Evaluations on training set show that
our method reports values below the baseline in
both types: paragraph-to-sentence and sentence-
to-phrase. In the testing phase we decided to per-
form only sentence-to-phrase comparison because

it reports better values than paragraph-to-sentence
according to Pearson correlation, which is used for
the official rank.

The best results our algorithm scores in the cat-
egory phrase-to-word. In this comparison type
it was ranked at 12th position among 21 partic-
ipating systems. In the word-to-sense it was at
14th position among 20 systems. The word-to-
sense comparison is converted into the task sim-
ilar to phrase-to-word by using glosses of target
senses. Each key of WordNet sense is replaced
with its gloss. It is the only situation when we
use the external knowledge resources, but it is
not a part of the algorithm. The last comparison
(sentence-to-phrase) was our worst, because we
did not beat the baseline, as we did in the previous
categories. In the sentence-to-phrase comparison
word alignment or syntax parsing seems to be very
important, in our case none of them was applied.
The main conclusion is that comparison of larger
text units can not be based on bag of words ap-
proaches, where order of words is not important.
Let us recall that our method is knowledge-poor,
what leads to difficulties in evaluating it against
knowledge-rich ones (using sense inventories e.g.
WordNet). Generally, we scored better results us-
ing Pearson correlation than Spearman’s one.

5 Conclusions

We presents our cross-level semantic similarity
method, which is knowledge-poor (not using any
kind of structured information from resources like
machine-readable dictionaries, thesaurus, or on-
tologies) and fully unsupervised (there is no learn-
ing phase leading to models enable to catego-
rize compared texts). The method exploits only
Wikipedia as a raw corpora in order to estimate
frequencies of co-occurrences. We were aimed
to verify how good results can be achieved us-
ing only corpus-based approach and not includ-
ing algorithms that have embedded deep language
knowledge. The system scores best in the phrase-
to-word (12th rank) and word-to-sense (14th rank)
types of comparison with regard to Pearson cor-
relation, while performing a little worse with the
Spearman’s correlation. The worst results were
reported in the sentence-to-phrase category, which
brings us the conclusion that larger text units de-
mand word alignment, syntax parsing and more
sophisticated text-to-text similarity models.

457



References
Alexander Budanitsky and Graeme Hirst. 2006. Eval-

uating WordNet-based measures of Lexical Seman-
tic Relatedness. Computational Linguistics, 32(1):
13–47.

Thomas Hughes and Daniel Ramage. 2007. Lexical
semantic relatedness with random graph walk. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 581–
589.

Aminul Islam and Diana Inkpen. 2008. Semantic Text
Similarity using Corpus-Based Word Similarity and
String Similarity. ACM Transactions on Knowledge
Discovery from Data, 2(2): 1–25.

Thomas Landauer and Susan Dumais. 1997. A solu-
tion to Platos problem: The latent semantic analysis
theory of acquisition, induction, and representation
of knowledge. Psychological Review, 104: 211–
240.

Thomas Landauer, Danielle McNamara, Simon Den-
nis and Walter Kintsch. 2007. Handbook of Latent
Semantic Analysis. Psychology Press.

Claudia Leacock and Martin Chodorow. 1998. Com-
bining local context and WordNet sense similarity
for word sense identification. In WordNet, An Elec-
tronic Lexical Database, pages 265–283.

Michael Lesk. 1986. Automatic sense disambiguation
using machine readable dictionaries: How to tell a
pine cone from an ice cream cone. In Proceedings
of the SIGDOC Conference, pages 24–26.

Yuhua Li, David McLean, Zuhair Bandar, James
O’Shea and Keeley Crockett. 2006. Sentence sim-
ilarity based on semantic nets and corpus statistics.
IEEE Transactions on Knowledge and Data Engi-
neering, 18(8): 1138-1149.

Rada Mihalcea, Courtney Corley and Carlo Strappa-
rava. 2006. Corpus-based and Knowledge-based
Measures of Text Semantic Similarity. In Proceed-
ings of the American Association for Artificial Intel-
ligence, pages 775–780.

Mohammad Pilehvar, David Jurgens and Roberto Nav-
igli. 2013. Align, Disambiguate and Walk: A Uni-
fied Approach for Measuring Semantic Similarity.
In Proceedings of the 51st Annual Meeting of the
Association for Computational Linguistics, pages
1341–1351.

Gerard Salton and Michael Lesk. 1971. Computer
evaluation of indexing and text processing. Prentice-
Hall, Englewood Cliffs, New Jersey.

Gerard Salton and Michael McGill. 1983. Alterna-
tion. Introduction to modern information retrieval.
McGraw-Hill.

Peter Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of seman-
tics. Journal of Articial Intelligence Research, 37:
141–188.

Peter Turney. 2001. Mining the web for synonyms:
PMI-IR versus LSA on TOEFL. In Proceedings
of the Twelfth European Conference on Machine
Learning, pages 491–502.

458


