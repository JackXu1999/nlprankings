



















































Retrieval-Based Neural Code Generation


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 925–930
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

925

Retrieval-Based Neural Code Generation

Shirley Anugrah Hayati Raphael Olivier Pravalika Avvaru
Pengcheng Yin Anthony Tomasic Graham Neubig

Language Technologies Institute, Carnegie Mellon University
{shayati,rolivier,pavvaru,pcyin,tomasic,gneubig}@cs.cmu.edu

Abstract

In models to generate program source code
from natural language, representing this code
in a tree structure has been a common ap-
proach. However, existing methods often fail
to generate complex code correctly due to a
lack of ability to memorize large and com-
plex structures. We introduce RECODE, a
method based on subtree retrieval that makes
it possible to explicitly reference existing code
examples within a neural code generation
model. First, we retrieve sentences that are
similar to input sentences using a dynamic-
programming-based sentence similarity scor-
ing method. Next, we extract n-grams of ac-
tion sequences that build the associated ab-
stract syntax tree. Finally, we increase the
probability of actions that cause the retrieved
n-gram action subtree to be in the predicted
code. We show that our approach improves the
performance on two code generation tasks by
up to +2.6 BLEU.1

1 Introduction

Natural language to code generation, a subtask
of semantic parsing, is the problem of converting
natural language (NL) descriptions to code (Ling
et al., 2016; Yin and Neubig, 2017; Rabinovich
et al., 2017). This task is challenging because it
has a well-defined structured output and the in-
put structure and output structure are in different
forms.

A number of neural network approaches have
been proposed to solve this task. Sequential ap-
proaches (Ling et al., 2016; Jia and Liang, 2016;
Locascio et al., 2016) convert the target code into
a sequence of symbols and apply a sequence-to-
sequence model, but this approach does not en-
sure that the output will be syntactically correct.

1Code available at https://github.com/
sweetpeach/ReCode

Tree-based approaches (Yin and Neubig, 2017;
Rabinovich et al., 2017) represent code as Ab-
stract Syntax Trees (ASTs), which has proven ef-
fective in improving accuracy as it enforces the
well-formedness of the output code. However,
representing code as a tree is not a trivial task, as
the number of nodes in the tree often greatly ex-
ceeds the length of the NL description. As a re-
sult, tree-based approaches are often incapable of
generating correct code for phrases in the corre-
sponding NL description that have low frequency
in the training data.

In machine translation (MT) problems (Zhang
et al., 2018; Gu et al., 2018; Amin Farajian et al.,
2017; Li et al., 2018), hybrid methods combin-
ing retrieval of salient examples and neural models
have proven successful in dealing with rare words.
Following the intuition of these models, we hy-
pothesize that our model can benefit from query-
ing pairs of NL descriptions and AST structures
from training data.

In this paper, we propose RECODE, and adap-
tation of Zhang et al. (2018)’s retrieval-based ap-
proach neural MT method to the code genera-
tion problem by expanding it to apply to gen-
eration of tree structures. Our main contribu-
tion is to introduce the use of retrieval methods
in neural code generation models. We also pro-
pose a dynamic programming-based sentence-to-
sentence alignment method that can be applied to
similar sentences to perform word substitution and
enable retrieval of imperfect matches. These con-
tributions allow us to improve on previous state-
of-the-art results.

2 Syntactic Code Generation

Given an NL description q, our purpose is to gen-
erate code (e.g. Python) represented as an AST a.
In this work, we start with the syntactic code gen-

https://github.com/sweetpeach/ReCode
https://github.com/sweetpeach/ReCode


926

eration model by Yin and Neubig (2017), which
uses sequences of actions to generate the AST be-
fore converting it to surface code. Formally, we
want to find the best generated AST â given by:

â = argmax
a

p(a|q)

p(a|q) =
T∏
t=1

p(yt|y<t, q)

where yt is the action taken at time step t and
y<t = y1...yt−1 and T is the number of total time
steps of the whole action sequence resulting in
AST a.

We have two types of actions to build an AST:
APPLYRULE and GENTOKEN. APPLYRULE(r)
expands the current node in the tree by applying
production rule r from the abstract syntax gram-
mar2 to the current node. GENTOKEN(v) pop-
ulates terminal nodes with the variable v which
can be generated from vocabulary or by COPYing
variable names or values from the NL description.
The generation process follows a preorder traver-
sal starting with the root node. Figure 1 shows
an action tree for the example code: the nodes cor-
respond to actions per time step in the construction
of the AST.

Interested readers can reference Yin and Neubig
(2017) for more detail of the neural model, which
consists of a bidirectional LSTM (Hochreiter and
Schmidhuber, 1997) encoder-decoder with action
embeddings, context vectors, parent feeding, and
a copy mechanism using pointer networks.

3 RECODE: Retrieval-Based Neural
Code Generation

We propose RECODE, a method for retrieval-
based neural syntactic code generation, using re-
trieved action subtrees. Following Zhang et al.
(2018)’s method for neural machine translation,
these retrieved subtrees act as templates that bias
the generation of output code. Our pipeline at test
time is as follows:
• retrieve from the training set NL descriptions

that are most similar with our input sentence
(§3.1),
• extract n-gram action subtrees from these

retrieved sentences’ corresponding target
ASTs (§3.2),

2https://docs.python.org/2/library/
ast.html

• alter the copying actions in these subtrees, by
substituting words of the retrieved sentence
with corresponding words in the input sen-
tence (§3.3), and
• at every decoding step, increase the probabil-

ity of actions that would lead to having these
subtrees in the produced tree (§3.4).

3.1 Retrieval of Training Instances
For every retrieved NL description qm from train-
ing set (or retrieved sentence for short), we com-
pute its similarity with input q, using a sentence
similarity formula (Gu et al., 2016; Zhang et al.,
2018):

sim(q, qm) = 1−
d(q, qm)

max(|q| ,|qm|)

where d is the edit distance. We retrieve only the
top M sentences according to this metric where
M is a hyperparameter. These scores will later be
used to increase action probabilities accordingly.

3.2 Extracting N -gram Action Subtrees
In Zhang et al. (2018), they collect n-grams from
the output side of the retrieved sentences and
encourage the model to generate these n-grams.
Word n-grams are obvious candidates when gen-
erating a sequence of words as output, as in NMT.
However, in syntax-based code generation, the
generation target is ASTs with no obvious linear
structure. To resolve this problem, we instead use
retrieved pieces of n-gram subtrees from the tar-
get code corresponding to the retrieved NL de-
scriptions. Though we could select successive
nodes in the AST as retrieved pieces, such as
[assign; expr*(targets); expr] from Figure
1, we would miss important structural information
from the rules that are used. Thus, we choose to
exploit actions in the generation model rather than
AST nodes themselves to be candidates for our re-
trieved pieces.

In the action tree (Figure 1), we consid-
ered only successive actions, such as sub-
trees where each node has one or no chil-
dren, to avoid overly rigid structures or com-
binatorial explosion of the number of retrieved
pieces the model has to consider. For ex-
ample, such an action subtree would be given
by [assign → expr*(targets), expr(value)
; expr(value) → List; List → epsilon].

As the node in the action tree holds structural
information about its children, we set the subtrees

https://docs.python.org/2/library/ast.html
https://docs.python.org/2/library/ast.html


927

t0

t1

t2

t3

t4

t5

t6

root ­> assign

assign ­> expr*(targets), expr(value) 

expr*(targets) ­> expr 

expr ­> Name 

Name ­> str 

GENTOKEN[params] 

GENTOKEN[/n] 

expr(value) ­> List 

List ­> epsilon 

Input       :  params is an empty list Target Code      : params = [ ]

Action Flow

Parent Feeding

Apply Rule

Generate Token

Generate Token 
 with Copy

t7

t8

t9

ti

ti

ti

Retrieved: List lst is an empty list Retrieved Code: lst = [ ]

GENTOKEN[lst] 

GENTOKEN[/n] 

Figure 1: The action sequence used to generate AST for the
target code given the input example. Dashed nodes repre-
sent terminals. Each node is labeled with time steps. AP-
PLYRULE action is represented as rule in this figure. Blue
dotted boxes denote 3-gram action subtrees. Italic words are
unedited words. Red bold words are different object names.

to have a fixed depth, linear in the size of the
tree. These can be considered “n-grams of ac-
tions”, emphasizing the comparison with machine
translation which uses n-grams of words. n is a
hyperparameter to be tuned.

3.3 Word Substitution in Copy Actions
Using the retrieved subtree without modification
is problematic if it contains at least one node cor-
responding to a COPY action because copied to-
kens from the retrieved sentence may be different
from those in the input. Figure 1 shows an ex-
ample when the input and retrieved sentence have
four common words, but the object names are dif-
ferent. The extracted action n-gram would contain
the rule that copies the second word (“lst”) of the
retrieved sentence while we want to copy the first
word (“params”) from the input.

By computing word-based edit distance be-
tween the input description and the retrieved sen-
tence, we implement a one-to-one sentence align-
ment method that infers correspondences between
uncommon words. For unaligned words, we alter
all COPY rules in the extracted n-grams to copy to-
kens by their aligned counterpart, such as replace
“params” with “lst”, and delete the n-gram sub-
tree, as it is not likely to be relevant in the pre-
dicted tree. Thus, in the example in Figure 1,
the GENTOKEN(LST) action in t5 will not be exe-
cuted.

3.4 Retrieval-Guided Code Generation
N -gram subtrees from all retrieved sentences are
assigned a score, based on the best similarity score

Dataset HS Django
Train 533 16,000
Dev 66 1,000
Test 66 1,805
Avg. tokens in description 39.1 14.3
Avg. number of nodes of AST 136.6 17.2

Table 1: Dataset statistics as reported Yin and Neubig (2017)

of all instances where they appeared. We normal-
ize the scores for each input sentence by subtract-
ing the average over the training dataset.

At decoding time, incorporate these retrieval-
derived scores into beam search: for a given time
step, all actions that would result in one of the
retrieved n-grams u to be in the prediction tree
has its log probability log(p(yt | yt−11 )) increased
by λ ∗ score(u) where λ is a hyperparameter, and
score(u) is the maximal sim(q, qm) from which u
is extracted. The probability distribution is then
renormalized.

4 Datasets and Evaluation Metrics
We evaluate RECODE with the Hearthstone (HS)
(Ling et al., 2016) and Django (Oda et al., 2015)
datasets, as preprocessed by Yin and Neubig
(2017). HS consists of Python classes that imple-
ment Hearthstone card descriptions while Django
contains pairs of Python source code and English
pseudo-code from Django web framework. Table
1 summarizes dataset statistics.

For evaluation metrics, we use accuracy of ex-
act match and the BLEU score following Yin and
Neubig (2017).

5 Experiments

For the neural code generation model, we use the
settings explained in Yin and Neubig (2017). For
the retrieval method, we tuned hyperparameters
and achieved best result when we set nmax = 4
and λ = 3 for both datasets3. For HS, we set
M = 3 and M = 10 for Django.

We compare our model with Yin and Neubig
(2017)’s model that we call YN17 for brevity,
and a sequence-to-sequence (SEQ2SEQ) model
that we implemented. SEQ2SEQ is an attention-
enabled encoder-decoder model (Bahdanau et al.,
2015). The encoder is a bidirectional LSTM and
the decoder is an LSTM.

5.1 Results
Table 2 shows that RECODE outperforms the base-
lines in both BLEU and accuracy, providing ev-

3n-gram subtrees are collected up to nmax-gram



928

idence for the effectiveness of incorporating re-
trieval methods into tree-based approaches.

HS Django
Acc BLEU Acc BLEU

SEQ2SEQ 0.0 55.0 13.9 67.3
YN17 16.2 75.8 71.6 84.5
ASN† 18.2 77.6 - -
ASN + SUPATT† 22.7 79.2 - -
RECODE 19.6 78.4 72.8 84.7

Table 2: Results compared to baselines. YN17 result is taken
from Yin and Neubig (2017). ASN result is taken from Rabi-
novich et al. (2017)

.

We ran statistical significance tests for RECODE
and YN17, using bootstrap resampling with N =
10,000. For the BLEU scores of both datasets, p <
0.001. For the exact match accuracy, p < 0.001
for Django dataset, but for Hearthstone, p > 0.3,
showing that the retrieval-based model is on par
with YN17. It is worth noting, though, that HS
consists of long and complex code, and that gener-
ating exact matches is very difficult, making exact
match accuracy a less reliable metric.

We also compare RECODE with Rabinovich
et al. (2017)’s Abstract Syntax Networks with
supervision (ASN+SUPATT) which is the
state-of-the-art system for HS. RECODE ex-
ceeds ASN without extra supervision though
ASN+SUPATT has a slightly better result. How-
ever, ASN+SUPATT is trained with supervised
attention extracted through heuristic exact word
matches while our attention is unsupervised.

5.2 Discussion and Analysis
From our observation and as mentioned in Rabi-
novich et al. (2017), HS contains classes with sim-
ilar structure, so the code generation task could
be simply matching the tree structure and filling
the terminal tokens with correct variables and val-
ues. However, when the code consists of complex
logic, partial implementation errors occur, lead-
ing to low exact match accuracy (Yin and Neubig,
2017). Analyzing our result, we find this intuition
to be true not only for HS but also for Django.

Examining the generated output for the Django
dataset in Table 3, we can see that in the first ex-
ample, our retrieval model can successfully gen-
erate the correct code when YN17 fails. This
difference suggests that our retrieval model ben-
efits from the action subtrees from the retrieved
sentences. In the second example, although our
generated code does not perfectly match the refer-
ence code, it has a higher BLEU score compared

Example 1
“if offset is lesser than integer 0, sign is set to ’-’, otherwise sign is ’+’ ” Input
sign = offset < 0 or ’-’ YN17
sign = ’-’ if offset < 0 else ’+’ RECODE
sign = ’-’ if offset < 0 else ’+’ Gold
Example 2
“evaluate the function timesince with d, now and reversed set Input
to boolean true as arguments, return the result.”
return reversed(d, reversed=now) YN17
return timesince(d, now, reversed=now) RECODE
return timesince(d, now, reversed=True) Gold
Example 3
“return an instance of SafeText , Input
created with an argument s converted into a string .”
return SafeText(bool(s)) YN17
return SafeText(s) RECODE
return SafeString(str(s)) Gold

Table 3: Django examples on correct code and predicted
code with retrieval (RECODE) and without retrieval (YN17).
NAME_BEGIN Earth Elemental NAME_END ATK_BEGIN 7 Input
ATK_END DEF_BEGIN 8 DEF_END COST_BEGIN 5
COST_END DUR_BEGIN -1 DUR_END TYPE_BEGIN Minion
TYPE_END PLAYER_CLS_BEGIN Shaman PLAYER_CLS_END
RACE_BEGIN NIL RACE_END RARITY_BEGIN Epic RARITY_END
DESC_BEGIN Taunt . Overload : ( 3 ) DESC_END.
class EarthElemental (MinionCard) : YN17

def __init__ (self) :
super ( ).__init__ ("Earth Elemental", 5,
CHARACTER_CLASS.SHAMAN, CARD_RARITY.EPIC,
buffs=[Buff(ManaChange(Count

(MinionSelector(None, BothPlayer())), -1))])
def create_minion (self, player) :

return Minion(7, 8, taunt=True)

class EarthElemental (MinionCard) : RECODE
def __init__ (self) :

super ( ).__init__ ("Earth Elemental", 5,
CHARACTER_CLASS.SHAMAN, CARD_RARITY.EPIC,
overload=3)

def create_minion (self, player) :
return Minion(7, 8, taunt=True)

class EarthElemental (MinionCard) : Gold
def __init__ (self) :

super ( ).__init__ ("Earth Elemental", 5,
CHARACTER_CLASS.SHAMAN, CARD_RARITY.EPIC,
overload=1)

def create_minion (self, player) :
return Minion(7, 8, taunt=True)

Table 4: HS examples on correct code and predicted code
with retrieval (RECODE) and without retrieval (YN17).

to the output of YN17 because our model can
predict part of the code (timesince(d, now,
reversed)) correctly. The third example shows
where our method fails to apply the correct action
as it cannot cast s to str type while YN17 can at
least cast s into a type (bool). Another common
type of error that we found RECODE’s generated
outputs is incorrect variable copying, similarly to
what is discussed in Yin and Neubig (2017) and
Rabinovich et al. (2017).

Table 4 presents a result on the HS dataset4. We
can see that our retrieval model can handle com-
plex code more effectively.

6 Related Work

Several works on code generation focus on do-
main specific languages (Raza et al., 2015; Kush-
man and Barzilay, 2013). For general purpose
code generation, some data-driven work has been

4More example of HS code is provided in the supplemen-
tary material.



929

done for predicting input parsers (Lei et al., 2013)
or a set of relevant methods (Raghothaman et al.,
2016). Some attempts using neural networks have
used sequence-to-sequence models (Ling et al.,
2016) or tree-based architectures (Dong and La-
pata, 2016; Alvarez-Melis and Jaakkola, 2017).
Ling et al. (2016); Jia and Liang (2016); Locas-
cio et al. (2016) treat semantic parsing as a se-
quence generation task by linearizing trees. The
closest work to ours are Yin and Neubig (2017)
and Rabinovich et al. (2017) which represent code
as an AST. Another close work is Dong and Lapata
(2018), which uses a two-staged structure-aware
neural architecture. They initially generate a low-
level sketch and then fill in the missing informa-
tion using the NL and the sketch.

Recent works on retrieval-guided neural ma-
chine translation have been presented by Gu et al.
(2018); Amin Farajian et al. (2017); Li et al.
(2018); Zhang et al. (2018). Gu et al. (2018) use
the retrieved sentence pairs as extra inputs to the
NMT model. Zhang et al. (2018) employ a sim-
pler and faster retrieval method to guide neural
MT where translation pieces are n-grams from re-
trieved target sentences. We modify Zhang et al.
(2018)’s method from textual n-grams to n-grams
over subtrees to exploit the code structural simi-
larity, and propose methods to deal with complex
statements and rare words.

In addition, some previous works have used
subtrees in structured prediction tasks. For ex-
ample, Galley et al. (2006) used them in syntax-
based translation models. In Galley et al. (2006),
subtrees of the input sentence’s parse tree are as-
sociated with corresponding words in the output
sentence.

7 Conclusion

We proposed an action subtree retrieval method at
test time on top of an AST-driven neural model for
generating general-purpose code. The predicted
surface code is syntactically correct, and the re-
trieval component improves the performance of a
previously state-of-the-art model. Our successful
result suggests that our idea of retrieval-based gen-
eration can be potentially applied to other tree-
structured prediction tasks.

Acknowledgements

We are grateful to Lucile Callebert for insight-
ful discussions, Aldrian Obaja Muis for helpful

input on early version writing, and anonymous
reviewers for useful feedback. This material is
based upon work supported by the National Sci-
ence Foundation under Grant No. 1815287.

References
D. Alvarez-Melis and T. Jaakkola. 2017. Tree struc-

tured decoding with doubly recurrent neural net-
works. In International Conference on Learning
Representations (ICLR).

M Amin Farajian, Marco Turchi, Matteo Negri, Mar-
cello Federico, and Fondazione Bruno Kessler.
2017. Multi-Domain Neural Machine Translation
through Unsupervised Adaptation. In Proceedings
of the Second Conference on Machine Translation
(WMT), Volume 1: Research Papers, pages 127–
137.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In International Con-
ference on Learning Representations (ICLR).

Li Dong and Mirella Lapata. 2016. Language to logi-
cal form with neural attention. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 33–34.

Li Dong and Mirella Lapata. 2018. Coarse-to-fine de-
coding for neural semantic parsing. In Proceedings
of the 56th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 731–742.

Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Pro-
ceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 961–968.

Jiatao Gu, Zhengdong Lu, Hang Li, and Victor O.K.
Li. 2016. Incorporating copying mechanism in
sequence-to-sequence learning. In Proceedings of
the 54th Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 1631–1640.

Jiatao Gu, Yong Wang, Kyunghyun Cho, and Victor
O. K. Li. 2018. Search engine guided neural ma-
chine translation. In Proceedings of the Thirty-
Second AAAI Conference on Artificial Intelligence
(AAAI).

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural Computation,
9(8):1735–1780.

R. Jia and P. Liang. 2016. Data recombination for
neural semantic parsing. In The 54th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 12–22.



930

Nate Kushman and Regina Barzilay. 2013. Using se-
mantic unification to generate regular expressions
from natural language. In Human Language Tech-
nologies: Conference of the North American Chap-
ter of the Association of Computational Linguistics,
Proceedings, pages 826–836.

Tao Lei, Fan Long, Regina Barzilay, and Martin C. Ri-
nard. 2013. From natural language specifications
to program input parsers. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 1294–1303.

Xiaoqing Li, Jiajun Zhang, and Chengqing Zong.
2018. One Sentence One Model for Neural Machine
Translation. In Proceedings of the Eleventh Interna-
tional Conference on Language Resources and Eval-
uation (LREC 2018).

Wang Ling, Phil Blunsom, Edward Grefenstette,
Karl Moritz Hermann, Tomáš Kočiský, Fumin
Wang, and Andrew Senior. 2016. Latent predictor
networks for code generation. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 599–609.

Nicholas Locascio, Karthik Narasimhan, Eduardo
De Leon, Nate Kushman, and Regina Barzilay.
2016. Neural generation of regular expressions
from natural language with minimal domain knowl-
edge. In Proceedings of the 2016 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 1918–1923.

Yusuke Oda, Hiroyuki Fudaba, Graham Neubig,
Hideaki Hata, Sakriani Sakti, Tomoki Toda, and
Satoshi Nakamura. 2015. Learning to generate
pseudo-code from source code using statistical ma-
chine translation (t). In Proceedings of the 2015
30th IEEE/ACM International Conference on Auto-
mated Software Engineering (ASE), pages 574–584.

Maxim Rabinovich, Mitchell Stern, and Dan Klein.
2017. Abstract syntax networks for code genera-
tion and semantic parsing. In Proceedings of the
55th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 1139–1149.

Mukund Raghothaman, Yi Wei, and Youssef Hamadi.
2016. Swim: Synthesizing what i mean: Code
search and idiomatic snippet synthesis. In Proceed-
ings of the 38th International Conference on Soft-
ware Engineering (ICSE), pages 357–367.

Mohammad Raza, Sumit Gulwani, and Natasa Milic-
Frayling. 2015. Compositional program synthesis
from natural language and examples. In Proceed-
ings of the 24th International Conference on Artifi-
cial Intelligence, pages 792–800.

Pengcheng Yin and Graham Neubig. 2017. A syntactic
neural model for general-purpose code generation.
In The 55th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 440–450.

Jingyi Zhang, Masao Utiyama, Eiichiro Sumita, Gra-
ham Neubig, and Satoshi Nakamura. 2018. Guiding
neural machine translation with retrieved translation
pieces. In Meeting of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL).


