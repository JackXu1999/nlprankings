



















































An Efficient Dynamic Oracle for Unrestricted Non-Projective Parsing


Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 256–261,

Beijing, China, July 26-31, 2015. c©2015 Association for Computational Linguistics

An Efficient Dynamic Oracle for Unrestricted Non-Projective Parsing

Carlos Gómez-Rodrı́guez
Departamento de Computación

Universidade da Coruña
Campus de Elviña, s/n
15071 A Coruña, Spain

carlos.gomez@udc.es

Daniel Fernández-González
Departamento de Informática

Universidade de Vigo
Campus As Lagoas, s/n
32004 Ourense, Spain
danifg@uvigo.es

Abstract

We define a dynamic oracle for the Cov-
ington non-projective dependency parser.
This is not only the first dynamic oracle
that supports arbitrary non-projectivity,
but also considerably more efficient
(O(n)) than the only existing oracle with
restricted non-projectivity support. Ex-
periments show that training with the dy-
namic oracle significantly improves pars-
ing accuracy over the static oracle baseline
on a wide range of treebanks.

1 Introduction

Greedy transition-based dependency parsers build
analyses for sentences incrementally by following
a sequence of transitions defined by an automaton,
using a scoring model to choose the best trans-
ition to take at each state (Nivre, 2008). While
this kind of parsers have become very popular,
as they achieve competitive accuracy with espe-
cially fast parsing times; their raw accuracy is still
behind that of slower alternatives like transition-
based parsers that use beam search (Zhang and
Nivre, 2011; Choi and McCallum, 2013). For this
reason, a current research challenge is to improve
the accuracy of greedy transition-based parsers as
much as possible without sacrificing efficiency.

A relevant recent advance in this direction is
the introduction of dynamic oracles (Goldberg and
Nivre, 2012), an improvement in the training pro-
cedure of greedy parsers that can boost their ac-
curacy without any impact on parsing speed. An
oracle is a training component that selects the best
transition(s) to take at a given configuration, us-
ing knowledge about the gold tree. Traditionally,
transition-based parsers were trained to follow a
so-called static oracle, which is only defined on
the configurations of a canonical computation that
generates the gold tree, returning the next trans-
ition in said computation. In contrast, dynamic

oracles are non-deterministic (not limited to one
sequence, but supporting all the possible computa-
tions leading to the gold tree), and complete (also
defined for configurations where the gold tree is
unreachable, choosing the transition(s) that lead to
a tree with minimum error). This extra robustness
in training provides higher parsing accuracy.

However, defining a usable dynamic oracle for
a given parser is non-trivial in general, due to
the need of calculating the loss of each configura-
tion, i.e., the minimum Hamming loss to the gold
tree from a tree reachable from that configuration.
While it is always easy to do this in exponential
time by simulating all possible computations in
the algorithm to obtain all reachable trees, it is
not always clear how to achieve this calculation
in polynomial time. At the moment, this prob-
lem has been solved for several projective pars-
ers exploiting either arc-decomposability (Gold-
berg and Nivre, 2013) or tabularization of compu-
tations (Goldberg et al., 2014). However, for pars-
ers that can handle crossing arcs, the only known
dynamic oracle (Gómez-Rodrı́guez et al., 2014)
has been defined for a variant of the parser by At-
tardi (2006) that supports a restricted set of non-
projective trees. To our knowledge, no dynamic
oracles are known for any transition-based parser
that can handle unrestricted non-projectivity.

In this paper, we define such an oracle for
the Covington non-projective parser (Covington,
2001; Nivre, 2008), which can handle arbitrary
non-projective dependency trees. As this al-
gorithm is not arc-decomposable and its tabular-
ization is NP-hard (Neuhaus and Bröker, 1997),
we do not use the existing techniques to define
dynamic oracles, but a reasoning specific to this
parser. It is worth noting that, apart from being the
first dynamic oracle supporting unrestricted non-
projectivity, our oracle is very efficient, solving the
loss calculation in O(n). In contrast, the restricted
non-projective oracle of Gómez-Rodrı́guez et al.

256



(2014) has O(n8) time complexity.
The rest of the paper is organized as follows:

after a quick outline of Covington’s parser in
Sect. 2, we present the oracle and prove its cor-
rectness in Sect. 3. Experiments are reported in
Sect. 4, and Sect. 5 contains concluding remarks.

2 Preliminaries

We will define a dynamic oracle for the non-
projective parser originally defined by Covington
(2001), and implemented by Nivre (2008) under
the transition-based parsing framework. For space
reasons, we only sketch the parser very briefly, and
refer to the above reference for more details.

Parser configurations are of the form c =
〈λ1, λ2, B,A〉, where λ1 and λ2 are lists of par-
tially processed words,B is another list (called the
buffer) with currently unprocessed words, andA is
the set of dependencies built so far. Suppose that
we parse a string w1 · · ·wn, whose word occur-
rences will be identified with their indices 1 · · ·n
for simplicity. Then, the parser starts at an initial
configuration cs(w1 . . . wn) = 〈[], [], [1 . . . n], ∅〉,
and executes transitions chosen from those in Fig-
ure 1 until a terminal configuration of the form
{〈λ1, λ2, [], A〉 ∈ C} is reached, and the sen-
tence’s parse tree is obtained from A.1

The transition semantics is very simple, mirror-
ing the double nested loop traversing word pairs in
the formulation by Covington (2001). When the
algorithm is in a configuration 〈λ1|i, λ2, j|B,A〉,
we will say that it is considering the focus words
i and j, located at the end of the first list and at the
beginning of the buffer. A decision is then made
about whether these two words should be linked
with a rightward arc i→ j (Right-Arc transition),
a leftward arc i ← j (Left-Arc transition) or not
linked (No-Arc transition). The first two choices
will be unavailable in configurations where the
newly-created arc would violate the single-head
constraint (a node cannot have more than one in-
coming arc) or the acyclicity constraint (cycles
are not allowed). In any of these three transitions,
i is then moved to the second list to make i−1 and
j the focus words for the next step. Alternatively,
we can choose to read a new word from the string
with a Shift transition, so that the focus words in

1The arcs in A form a forest, but we convert it to a tree by
linking any node without a head as a dependent of an artifi-
cial node at position 0 that acts as a dummy root. From now
on, when we refer to some dependency graph as a tree, we
assume that this transformation is being implicitly made.

the resulting configuration will be j and j + 1.
The result is a parser that can generate any pos-

sible dependency tree for the input, and runs in
quadratic worst-case time. Although in theory this
complexity can seem like a drawback compared to
linear-time transition-based parsers (e.g. (Nivre,
2003; Gómez-Rodrı́guez and Nivre, 2013)), it has
been shown by Volokh and Neumann (2012) to ac-
tually outperform linear algorithms in practice, as
it allows for relevant optimizations in feature ex-
traction that cannot be implemented in other pars-
ers. In fact, one of the fastest dependency parsers
to date uses this algorithm (Volokh, 2013).

3 The oracle

As sketched in Sect. 1, a dynamic oracle is a train-
ing component that, given a configuration c and
a gold tree tG, provides the set of transitions that
are applicable in c and lead to trees with minimum
Hamming loss with respect to tG. The Hamming
loss between a tree t and tG, written L(t, tG), is
the number of nodes that have a different head in t
than in tG. Following Goldberg and Nivre (2013),
we say that a set of arcs A is reachable from con-
figuration c, written c  A, if there is some (pos-
sibly empty) path of transitions from c to some
configuration c′ = 〈λ1, λ2, B,A′〉, with A ⊆ A′.
Then, we can define the loss of a configuration as

`(c) = min
t|c t

L(t, tG),

and the set of transitions that must be returned by
a correct dynamic oracle is then

od(c, tG) = {τ | `(c)− `(τ(c)) = 0},
i.e., the transitions that do not increase configur-
ation loss, and hence lead to the best parse (in
terms of loss) reachable from c. Therefore, imple-
menting a dynamic oracle reduces to computing
the loss `(c) for each configuration c.

Goldberg and Nivre (2013) show that the calcu-
lation of the loss is easy for parsers that are arc-
decomposable, i.e., those where for every config-
uration c and arc setA that is tree-compatible (i.e.
that can be a part of a well-formed parse2), c A
is entailed by c  (i → j) for every i → j ∈ A.
That is, if each arc in a tree-compatible set is indi-
vidually reachable from configuration c, then that

2In the cited paper, tree-compatibility required projectiv-
ity, as the authors were dealing with projective parsers. In
our case, since the parser is non-projective, tree-compatibility
only consists of the single-head and acyclicity constraints.

257



Shift: 〈λ1, λ2, j|B,A〉 ⇒ 〈λ1 · λ2|j, [], B,A〉
No-Arc: 〈λ1|i, λ2, B,A〉 ⇒ 〈λ1, i|λ2, B,A〉
Left-Arc: 〈λ1|i, λ2, j|B,A〉 ⇒ 〈λ1, i|λ2, j|B,A ∪ {j → i}〉

only if @k | k → i ∈ A (single-head) and i →∗ j 6∈ A (acyclicity).
Right-Arc: 〈λ1|i, λ2, j|B,A〉 ⇒ 〈λ1, i|λ2, j|B,A ∪ {i→ j}〉

only if @k | k → j ∈ A (single-head) and j →∗ i 6∈ A (acyclicity).

Figure 1: Transitions of the Covington non-projective dependency parser.

0 1 2 3 4

Figure 2: An example of non-arc-decomposability
of the Covington parser: graphical representation
of configuration c = 〈[1, 2], [], [3, 4], A = {1 →
2}〉. The solid arc corresponds to the arc set A,
and the circled indexes mark the focus words. The
dashed arcs represent the gold tree tG.

set of arcs is reachable from c. If this holds, then
computing the loss of a configuration c reduces to
determining and counting the gold arcs that are not
reachable from c, which is easy in most parsers.

Unfortunately, the Covington parser is not arc-
decomposable. This can be seen in the example of
Figure 2: while any of the gold arcs 2→3, 3→4,
4→1 can be reachable individually from the depic-
ted configuration, they are not jointly reachable as
they form a cycle with the already-built arc 1→2.
Thus, the configuration has only one individually
unreachable arc (0→2), but its loss is 2.

However, it is worth noting that non-arc-
decomposability in the parser is exclusively due
to cycles. If a set of individually reachable arcs do
not form a cycle together with already-built arcs,
then we can show that the set will be reachable.
This idea is the basis for an expression to compute
loss based on counting individually unreachable
arcs, and then correcting for the effect of cycles:

Theorem 1 Let c = 〈λ1, λ2, B,A〉 be a config-
uration of the Covington parser, and tG the set of
arcs of a gold tree. We call I(c, tG) = {x → y ∈
tG | c  (x → y)} the set of individually reach-
able arcs of tG; note that this set may overlap A.
Conversely, we call U(c, tG) = tG \ I(c, tG) the
set of individually unreachable arcs of tG from c.
Finally, let nc(G) denote the number of cycles in

a graph G.
Then `(c) = |U(c, tG)|+ nc(A ∪ I(c, tG)). 2
We now sketch the proof. To prove Theorem 1,

it is enough to show that (1) there is at least one
tree reachable from c with exactly that Hamming
loss to tG, and (2) there are no trees reachable from
cwith a smaller loss. To this end, we will use some
properties of the graphA∪I(c, tG). First, we note
that no node in this graph has in-degree greater
than 1. In particular, each node except for the
dummy root has exactly one head, either explicit
or (if no head has been assigned inA or in the gold
tree) the dummy root. No node has more than one
head: a node cannot have two heads in A because
the parser transitions enforce the single-head con-
straint, it cannot have two heads in I(c, tG) be-
cause tG must satisfy this constraint as well, and it
cannot have one head in A and another in I(c, tG)
because the corresponding arc in I(c, tG) would
be unreachable due to the single-head constraint.

This, in turn, implies that the graphA∪I(c, tG)
has no overlapping cycles, as overlapping cycles
can only appear in graphs with in-degree greater
than 1. This is the key property enabling us to
exactly calculate loss using the number of cycles.

To show (1), consider the graph A ∪ I(c, tG).
In each of its cycles, there is at least one arc
that belongs to I(c, tG), as A must satisfy the
acyclicity constraint. We arbitrarily choose one
such arc from each cycle, and remove it from
the graph. Note that this results in removing ex-
actly nc(A ∪ I(c, tG)) arcs, as we have shown
that the cycles in A ∪ I(c, tG) are disjoint. We
call the resulting graph B(c, tG). As it has max-
imum in-degree 1 and it is acyclic (because we
have broken all the cycles), B(c, tG) is a tree, mod-
ulo our standard assumption that headless nodes
are assumed to be linked to the dummy root.

This tree B(c, tG) is reachable from c and has
loss `(c) = |U(c, tG)|+nc(A∪I(c, tG)). Reach-
ability is shown by building a sequence of trans-

258



itions that will visit the pairs of words corres-
ponding to remaining arcs in order, and inter-
calating the corresponding Left-Arc or Right-Arc
transitions, which cannot violate the acyclicity or
single-head constraints. The term U(c, tG) in the
loss stems from the fact that A ∪ I(c, tG) can-
not contain arcs in U(c, tG), and the term nc(A ∪
I(c, tG)) from not including the nc(A ∪ I(c, tG))
arcs that we discarded to break cycles.

Finally, from these observations, it is easy to
see that B(c, tG) has the best loss among reach-
able trees, and thus prove (2): the arcs in U(c, tG)
are always unreachable by definition, and for each
cycle in nc(A ∪ I(c, tG)), the acyclicity con-
straint forces us to miss at least one arc. As
the cycles are disjoint, this means that we neces-
sarily miss at least nc(A ∪ I(c, tG)) arcs, hence
|U(c, tG)| + nc(A ∪ I(c, tG)) is indeed the min-
imum loss among reachable trees. �

Thus, to calculate the loss of a configuration c,
we only need to compute both of the terms in The-
orem 1. For the first term, note that if c has focus
words i and j (i.e., c = 〈λ1|i, λ2, j|B,A〉), then
an arc x→ y is in U(c, tG) if it is not in A, and at
least one of the following holds:
• j > max(x, y), as in this case we have read

too far in the string and will not be able to get
x and y as focus words,
• j = max(x, y) ∧ i < min(x, y), as in this

case we have max(x, y) as the right focus
word but the left focus word is to the left of
min(x, y), and we cannot move it back,
• there is some z 6= 0, z 6= x such that z → y ∈
A, as in this case the single-head constraint
prevents us from creating x→ y,
• x and y are on the same weakly connected

component of A, as in this case the acyclicity
constraint will not let us create x→ y.

All of these arcs can be trivially enumerated in
O(n) time (in fact, they can be updated in O(1)
if we start from the configuration that preceded c).
The second term of the loss, nc(A∪I(c, tG)), can
be computed by obtaining I(c, tG) as tG\U(c, tG)
to then apply a standard cycle-finding algorithm
(Tarjan, 1972) which, for a graph with maximum
in-degree 1, runs in O(n) time.

Algorithm 1 presents the resulting loss cal-
culation algorithm in pseudocode form, where
COUNTCYCLES is a function that counts the num-
ber of cycles in the given graph in linear time as
mentioned above. Note that the for loop runs in

Algorithm 1 Computation of the loss of a config-
uration.

1: function LOSS(c = 〈λ1|i, λ2, j|B,A〉, tG)
2: U ← ∅ . Variable U is for U(c, tG)
3: for each x→ y ∈ (tG \A) do
4: left ← min(x, y)
5: right ← max(x, y)
6: if j > right ∨
7: (j = right ∧ i < left)∨
8: (∃z > 0, z 6= x : z → y ∈ A)∨
9: WEAKLYCONNECTED(A, x, y) then

10: U ← u ∪ {x→ y}
11: I ← tG \U . Variable I is for I(c, tG)
12: return |U |+ COUNTCYCLES(A ∪ I )

linear time: the condition on line 8 can be com-
puted in constant time by recovering the head of
y. The call to WEAKLYCONNECTED in line 9
finds out whether the two given nodes are weakly
connected in A, and can also be resolved in
O(1), by querying the disjoint set data structure
that implementations of the Covington algorithm
commonly use for the parser’s acyclicity checks
(Nivre, 2008).

It is worth noting that the linear-time com-
plexity can also be achieved by a standalone im-
plementation of the loss calculation algorithm,
without recurse to the parser’s auxiliary data struc-
tures (although this is dubiously practical). To
do so, we can implement WEAKLYCONNECTED
so that the first call computes the connected com-
ponents of A in linear time (Hopcroft and Tarjan,
1973) and subsequent calls use this information to
find out if two nodes are weakly connected in con-
stant time.

On the other hand, a more efficient implementa-
tion than the one shown in Algorithm 1 (which we
chose for clarity) can be achieved by more tightly
coupling the oracle to the parser, as the relevant
sets of arcs associated with a configuration can be
obtained incrementally from those of the previous
configuration.

4 Experiments

To evaluate the performance of our approach, we
conduct experiments on both static and dynamic
Covington non-projective oracles. Concretely, we
train an averaged perceptron model for 15 itera-
tions on nine datasets from the CoNLL-X shared
task (Buchholz and Marsi, 2006) and all data-

259



Unigrams
L0w; L0p; L0wp; L0l; L0hw; L0hp; L0hl; L0l′w; L0l′p;
L0l′ l; L0r′w; L0r′p; L0r′ l; L0h2w; L0h2p; L0h2l; L0lw;
L0lp; L0ll; L0rw; L0rp; L0rl; L0wd; L0pd;
L0wvr; L0pvr; L0wvl; L0pvl; L0wsl; L0psl; L0wsr;
L0psr; L1w; L1p; L1wp; R0w; R0p; R0wp; R0l′w;
R0l′p; R0l′ l; R0lw; R0lp; R0ll; R0wd; R0pd; R0wvl;
R0pvl;R0wsl; R0psl; R1w; R1p; R1wp; R2w; R2p;
R2wp; CLw; CLp; CLwp; CRw; CRp; CRwp;
Pairs
L0wp+R0wp; L0wp+R0w; L0w+R0wp; L0wp+R0p;
L0p+R0wp; L0w+R0w; L0p+R0p;R0p+R1p;
L0w+R0wd; L0p+R0pd;
Triples
R0p+R1p+R2p; L0p+R0p+R1p; L0hp+L0p+R0p;
L0p+L0l′p+R0p; L0p+L0r′p+R0p; L0p+R0p+R0l′p;
L0p+L0l′p+L0lp; L0p+L0r′p+L0rp;
L0p+L0hp+L0h2p; R0p+R0l′p+R0lp;

Table 1: Feature templates. L0 and R0 denote
the left and right focus words; L1, L2, . . . are the
words to the left of L0 and R1, R2, . . . those to the
right of R0. Xih means the head of Xi, Xih2 the
grandparent, Xil and Xil′ the farthest and closest
left dependents, and Xir and Xir′ the farthest and
closest right dependents, respectively. CL and
CR are the first and last words between L0 andR0
whose head is not in the interval [L0, R0]. Finally,
w stands for word form; p for PoS tag; l for de-
pendency label; d is the distance between L0 and
R0; vl, vr are the left/right valencies (number of
left/right dependents); and sl, sr the left/right label
sets (dependency labels of left/right dependents).

sets from the CoNLL-XI shared task (Nivre et al.,
2007). We use the same feature templates for all
languages, which result from adapting the features
described by Zhang and Nivre (2011) to the data
structures of the Covington non-projective parser,
and are listed in detail in Table 1.

Table 2 reports the accuracy obtained by the
Covington non-projective parser with both or-
acles. As we can see, the dynamic oracle imple-
mented in the Covington algorithm improves over
the accuracy of the static version on all datasets
except Japanese and Swedish, and most improve-
ments are statistically significant at the .05 level.3

In addition, the Covington dynamic oracle
achieves a greater average improvement in ac-
curacy than the Attardi dynamic oracle (Gómez-
Rodrı́guez et al., 2014) over their respective static
versions. Concretely, the Attardi oracle accom-
plishes an average improvement of 0.52 percent-

3Note that the loss of accuracy in Japanese and Swedish
is not statistically significant.

s-Covington d-Covington
Language UAS LAS UAS LAS
Arabic 80.03 71.32 81.47∗ 72.77∗
Basque 75.76 69.70 76.49∗ 70.27∗
Catalan 88.66 83.92 89.28 84.26
Chinese 83.94 79.59 84.68∗ 80.16∗
Czech 77.38 71.21 78.58∗ 72.59∗
English 84.64 83.72 86.14∗ 84.96∗
Greek 79.33 72.65 80.52∗ 73.67∗
Hungarian 77.70 74.32 78.22 74.61
Italian 83.39 79.66 83.66 79.91
Turkish 82.14 76.00 82.38 76.15
Bulgarian 87.68 84.55 88.48∗ 85.32∗
Danish 84.07 79.99 84.98∗ 80.85∗
Dutch 80.28 77.55 81.17∗ 78.54∗
German 86.12 83.93 87.47∗ 85.15∗
Japanese 93.92 92.51 93.79 92.42
Portuguese 85.70 82.78 86.23 83.27
Slovene 75.31 68.97 76.76∗ 70.35∗
Spanish 78.82 75.84 79.87∗ 76.97∗
Swedish 86.78 81.29 86.66 81.21
Average 82.72 78.39 83.52 79.13

Table 2: Parsing accuracy (UAS and LAS, in-
cluding punctuation) of Covington non-projective
parser with static (s-Covington) and dynamic (d-
Covington) oracles on CoNLL-XI (first block) and
CoNLL-X (second block) datasets. For each lan-
guage, we run five experiments with the same
setup but different seeds and report the averaged
accuracy. Best results for each language are shown
in boldface. Statistically significant improvements
(α = .05) (Yeh, 2000) are marked with ∗.

age points in UAS and 0.71 in LAS, while our ap-
proach achieves 0.80 in UAS and 0.74 in LAS.

5 Conclusion

We have defined the first dynamic oracle for
a transition-based parser supporting unrestricted
non-projectivity. The oracle is very efficient, com-
puting loss in O(n), compared to O(n8) for the
only previously known dynamic oracle with sup-
port for a subset of non-projective trees (Gómez-
Rodrı́guez et al., 2014).

Experiments on the treebanks from the CoNLL-
X and CoNLL-XI shared tasks show that the dy-
namic oracle significantly improves accuracy on
many languages over a static oracle baseline.

Acknowledgments

Research partially funded by the Spanish Min-
istry of Economy and Competitiveness/ERDF
(grants FFI2014-51978-C2-1-R, FFI2014-51978-
C2-2-R), Ministry of Education (FPU grant pro-
gram) and Xunta de Galicia (grant R2014/034).

260



References
Giuseppe Attardi. 2006. Experiments with a multil-

anguage non-projective dependency parser. In Pro-
ceedings of the 10th Conference on Computational
Natural Language Learning (CoNLL-X), pages 166–
170, Morristown, NJ, USA. Association for Compu-
tational Linguistics.

Sabine Buchholz and Erwin Marsi. 2006. CoNLL-
X shared task on multilingual dependency parsing.
In Proceedings of the 10th Conference on Computa-
tional Natural Language Learning (CoNLL), pages
149–164.

Jinho D. Choi and Andrew McCallum. 2013.
Transition-based dependency parsing with selec-
tional branching. In Proceedings of the 51st An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1052–
1062, Sofia, Bulgaria.

Michael A. Covington. 2001. A fundamental al-
gorithm for dependency parsing. In Proceedings of
the 39th Annual ACM Southeast Conference, pages
95–102, New York, NY, USA. ACM.

Yoav Goldberg and Joakim Nivre. 2012. A dynamic
oracle for arc-eager dependency parsing. In Pro-
ceedings of COLING 2012, pages 959–976, Mum-
bai, India, December. Association for Computa-
tional Linguistics.

Yoav Goldberg and Joakim Nivre. 2013. Training
deterministic parsers with non-deterministic oracles.
Transactions of the Association for Computational
Linguistics, 1:403–414.

Yoav Goldberg, Francesco Sartorio, and Giorgio Satta.
2014. A tabular method for dynamic oracles in
transition-based parsing. Transactions of the Asso-
ciation for Computational Linguistics, 2:119–130.

Carlos Gómez-Rodrı́guez and Joakim Nivre. 2013.
Divisible transition systems and multiplanar de-
pendency parsing. Computational Linguistics,
39(4):799–845.

Carlos Gómez-Rodrı́guez, Francesco Sartorio, and
Giorgio Satta. 2014. A polynomial-time dy-
namic oracle for non-projective dependency pars-
ing. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 917–927. Association for Compu-
tational Linguistics.

John Hopcroft and Robert Endre Tarjan. 1973. Al-
gorithm 447: Efficient algorithms for graph manip-
ulation. Commun. ACM, 16(6):372–378, June.

Peter Neuhaus and Norbert Bröker. 1997. The com-
plexity of recognition of linguistically adequate de-
pendency grammars. In Proceedings of the 35th
Annual Meeting of the Association for Computa-
tional Linguistics (ACL) and the 8th Conference of
the European Chapter of the Association for Com-
putational Linguistics (EACL), pages 337–343.

Joakim Nivre, Johan Hall, Sandra Kübler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 shared task on de-
pendency parsing. In Proceedings of the CoNLL
Shared Task Session of EMNLP-CoNLL 2007, pages
915–932, June.

Joakim Nivre. 2003. An efficient algorithm for pro-
jective dependency parsing. In Proceedings of the
8th International Workshop on Parsing Technologies
(IWPT 03), pages 149–160. ACL/SIGPARSE.

Joakim Nivre. 2008. Algorithms for Deterministic In-
cremental Dependency Parsing. Computational Lin-
guistics, 34(4):513–553.

Robert Endre Tarjan. 1972. Depth-first search and lin-
ear graph algorithms. SIAM J. Comput., 1(2):146–
160.

Alexander Volokh and Günter Neumann. 2012. De-
pendency parsing with efficient feature extraction.
In Birte Glimm and Antonio Krüger, editors, KI,
volume 7526 of Lecture Notes in Computer Science,
pages 253–256. Springer.

Alexander Volokh. 2013. Performance-Oriented De-
pendency Parsing. Doctoral dissertation, Saarland
University, Saarbrücken, Germany.

Alexander Yeh. 2000. More accurate tests for the stat-
istical significance of result differences. In Proceed-
ings of the 18th International Conference on Com-
putational Linguistics (COLING), pages 947–953.

Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies: short papers - Volume 2, pages
188–193.

261


