




































Using Large Corpus N-gram Statistics to Improve Recurrent Neural Language Models


Proceedings of NAACL-HLT 2019, pages 3268–3273
Minneapolis, Minnesota, June 2 - June 7, 2019. c©2019 Association for Computational Linguistics

3268

Using Large Corpus N-gram Statistics to Improve Recurrent Neural
Language Models

Yiben Yang, Ji-Ping Wang
Department of Statistics
Northwestern University

Evanston, IL, 60208, USA
{yiben.yang,jzwang}@northwestern.edu

Doug Downey
Department of Electrical Engineering

& Computer Science
Northwestern University

Evanston, IL, 60208, USA
{d-downey}@northwestern.edu

Abstract

Recurrent neural network language models
(RNNLM) form a valuable foundation for
many NLP systems, but training the models
can be computationally expensive, and may
take days to train on a large corpus. We ex-
plore a technique that uses large corpus n-gram
statistics as a regularizer for training a neural
network LM on a smaller corpus. In experi-
ments with the Billion-Word and Wikitext cor-
pora, we show that the technique is effective,
and more time-efficient than simply training
on a larger sequential corpus. We also intro-
duce new strategies for selecting the most in-
formative n-grams, and show that these boost
efficiency.

1 Introduction

Recurrent neural network models of language
(RNNLMs) form a foundation for many natural
language processing systems. However, the net-
works can be expensive to train: training a single
model over several million tokens can take hours,
and searching through the large hyperparameter
space of RNNLMs often entails training and test-
ing hundreds of different models. This makes it
burdensome to experiment with new RNNLM ar-
chitectures on large corpora, or to train RNNLMs
for new textual domains.

RNNLMs are typically trained on sequential
text. In this paper, we investigate how to effi-
ciently augment the training of RNNLMs by reg-
ularizing the models to match n-gram statistics
taken from a much larger corpus. The motivation
is that large-corpus n-gram statistics may be in-
formative to an RNNLM trained on a smaller se-
quential corpus, but unlike RNNLM training, n-
gram statistics are inexpensive to compute even
over large corpora. Moreover, the statistics only
need to be computed once and can be re-used for
training many different smaller-corpus RNNLMs.

Naively, regularizing an RNNLM to match a

given set of n-gram statistics is non-trivial, be-
cause the marginal probabilities that n-gram statis-
tics represent are not parameters of the RNNLM.
In recent work, Noraset et al. (2018) showed that
it was possible to regularize an RNNLM to match
given n-gram statistics by training the network,
when started from a zero state, to match each n-
gram probability. However, the regularization ap-
proach in that work had tractability limitations—
the time cost of the regularization was sufficiently
high that using it was inferior to simply training
the RNNLM on more sequential text.

In this paper, we present an efficient n-gram reg-
ularization technique and show that the technique
can improve RNNLM training. Our method has
three distinctions from previous work that provide
efficiency. First, we prioritize regularizing only
the n-grams that are most likely to improve the
model, by focusing on cases where the RNNLM’s
sequential training corpus diverges significantly
from the n-gram statistics. Secondly, we regular-
ize the entire output softmax of the RNN to match
given conditional n-gram statistics, which means
we can impose a large number of statistical con-
straints using only one softmax evaluation. Fi-
nally, we use an ensemble of multiple loss func-
tions in our regularizer, which provides an addi-
tional boost. In experiments, we show how n-gram
regularization with these enhancements results in
better models using the same amount of training
time, compared to standard sequential training.
We also plan to release our code base and to the
research community.1

2 Methods

RNNLMs are trained to optimize a loss function
Ld, which is defined as the average negative log-
likelihood of a training corpus. We regularize the
RNNLM to match n-gram statistics by introduc-

1https://github.com/yangyiben/
Conditional-N-gram-Regularization



3269

ing another penalty term Lp to the loss function
that captures how well the model matches n-gram
statistics, giving a combined loss L:

L = Ld + αLp,

where α is a hyperparameter to control the regu-
larization strength. We use the term large corpus
to refer to the text utilized to compute the n-gram
statistics. We expect the large corpus to be multi-
ple orders of magnitude larger than the small cor-
pus utilized for computing the RNN’s sequential
loss Ld. In the rest of this section, we first de-
fine what we mean by conditional n-gram statis-
tics, and then present our regularization methods.

2.1 Conditional N-gram Statistics

An order-N n-gram is a sequence ofN words. For
a given corpus c, we denote the kth distinct order-
N n-gram wk1, wk2, ..., wkN as wNk , and denote
the corresponding order N − 1 n-gram formed by
the first N − 1 words as wN−1k . Here, in order to
eliminate ambiguity, the notation N − 1 is exclu-
sively used to represent the prefix. For instance,
w3k is the k-th trigram, while w

4−1
k is the trigram

prefix of the k-th 4-gram. We define conditional n-
gram statistics as the empirical conditional proba-
bilities of observing each next wordwkN given the
previous N − 1 gram wN−1k for all ks :

P̂ (wkN |wN−1k ) =
count(wNk )

count(wN−1k )
, ∀wNk ∈ Ω(c),

where wkN and wN−1k are the N th word and the
corresponding previous N − 1 gram of wNk re-
spectively, and Ω(c) is the set of all unique wNk s
contained in the corpus c. For a given RNNLM,
the model conditional probability for some n-gram
wNk is defined as:

Pθ(wkN |wN−1k ) = Eh(Pθ(wkN |w
N−1
k , h)),

where h is the model’s hidden state prior to en-
countering the N -gram. However, it is difficult
to express that expectation in terms of the model
parameters, so we adopt the approach from No-
raset et al. (2018), which has shown preliminary
evidence of forming an effective regularizer:

Eh(Pθ(wkN |wN−1k , h)) ≈ Pθ(wkN |w
N−1
k , h0),

where h0 is a zero hidden state.

2.2 Conditional N-gram Regularization

We propose three forms of regularization loss
functions that penalize the divergence of the
model’s conditional probabilities from the condi-
tional n-gram statistics.

2.2.1 Mean Squared Log Probability Ratio
We denote our first penalty as Lsqp , defined as:

Lsqp =
1

‖R‖
∑

wNk ∈R

(
log

P̂ (wkN |wN−1k )
Pθ(wkN |wN−1k )

)2
,

where R is a set of n-grams wNk to regularize,
and P̂ and Pθ are conditional n-gram statistics and
model conditional probabilities as defined in Sec-
tion 2.1. This penalty is similar to that of (No-
raset et al., 2018). However, instead of computing
the loss with multiple forward passes for differ-
ent wNk s that have the same w

N−1
k , we propose

to only perform one forward pass for each wN−1k ,
and regularize all subsequent N th words. This
makes our loss much more computationally effi-
cient. As this penalty only accounts differences
in point-wise probabilities for specific words wN ,
it can be used for partially specified distributions
where we only know the desired probabilities for
some N th words in a given context, but not the
entire distribution.

2.2.2 Mean Kullback–Leibler Divergence
We denote our second penalty as LKLp , defined as:

PwN−1k
= P̂ (w|wN−1k ), QwN−1k = Pθ(w|w

N−1
k )

LKLp =
1

‖R‖
∑

wN−1k ∈R

DKL(PwN−1k
||QwN−1k ),

where here R is a set of prefixes wN−1k to regu-
larize. This penalty regularizes all possible sub-
sequent words wN , thus it only works for fully-
specified reference distributions.

2.2.3 Combined Penalty
Because the above two penalty functions differ, we
hypothesize that they may be complementary and
propose a combined penalty Lcp = L

sq
p + LKLp .

2.3 N-gram Selection Strategy

Note that we do one forward pass for each unique
wN−1k , so only the number of distinct prefixes
wN−1k will significantly affect the computational



3270

cost of our regularization methods. Naively regu-
larizing all unique prefixes in a large corpus usu-
ally requires a large number of forward passes,
which could be expensive. We hypothesize that
some prefixes are more useful than others, so
we attempt to select the ones that will improve
the model the most. We propose to select pre-
fixes wN−1k that maximize the Expected Log-
likelihood Change (ELC), defined as:

ELC(wN−1k ) =
∑

P̂ (wNk )log
P̂ (wkN |wN−1k )
Pθ(wkN |wN−1k )

.

Ideally, Pθ would reflect the statistics of the
RNNLM, updated during training, but these are
expensive to obtain. Thus we propose to train a
inexpensive n-gram model (Chen and Goodman,
1999) on the sequential corpus to serve as Pθ, and
we use that to select a fixed set of n-grams to reg-
ularize.

3 Experiments

We now present our experiments measuring the ef-
fectiveness of conditional n-gram regularization.

3.1 Data and Settings

We experiment on a medium-size (2 layers
with 650 hidden states) LSTM language model
(Zaremba et al., 2014) over two corpora: Wiki-
text (Merity et al., 2016) and Google Billion-Word
(Chelba et al., 2013) (1B). We adopt weight ty-
ing (Inan et al., 2016) and variational dropout (Gal
and Ghahramani, 2016). All models are trained by
SGD for 30 epochs with batch size 64 and trun-
cated backpropagation (Mikolov et al., 2011) with
35 time steps. The learning rate starts at 20 and
then is reduced to 5 at epoch 20. For the 1B
corpus, we follow the same procedure in Yang
et al. (2017) to generate training, validation and
test sets, except that we use only the top 50K vo-
cabulary terms. For the Wikitext corpus, we adopt
the Wikitext-2 vocabulary. All of the RNNLM
sequential training sets are small subsets sampled
from the Wikitext-103 and 1B training corpora.

For each dataset, we use the whole training set
as the large corpus for building our reference con-
ditional n-gram statistics. In this study, we only
consider conditional trigram regularization for all
experiments. The regularization takes additional
time during training. To enable a fair experiment,
we equalize the training time between regularized

models and the baselines, by providing the base-
lines with more sequential training data than the
regularized models. The three proposed penalties
are almost equally fast, thus they can be compared
against the same baseline. Unlike RNNs, there
are no hyperparameter settings or decisions in-
volved in computing the n-gram counts, so this can
be done once and re-used across the many RNN
training runs that adequate hyperparameter search
for RNNs often entails. Moreover, counting n-
grams is fast. We approximate that it takes about
one minute to obtain n-gram statistics from the
Wikitext-103 training corpus, for example. Thus,
we set up our experiments to equalize neural net-
work training time, and we ignore the small one-
time cost of computing n-gram statistics.

We fix the number of bigrams per batch to be
500, and employ the proposed strategy to select
the top X most informative bigrams (X depends
on the number of batches in sequential data). Fi-
nally, instead of tuning the regularization strength
hyperparameter α for each setting, we fix α to be
1.0, 0.75 and 0.5 for the three sequential data sizes
based on the heuristic that larger sequential data
may need less regularization. More carefully tun-
ing the regularization strength might yield some-
what better results for our methods. Also, using
different regularization strengths in the combined
penalty might further improve results.

3.2 Results

In Table 1, we compare the performance of our
proposed methods against equal-time controlled
baselines under different token sizes for both the
Wikitext and 1B data sets. In the table, the num-
bers in the column headings indicate the token
count of the sequential corpus used to train the
regularized methods. The baselines train on larger
corpora, to ensure an equal-time setting as de-
scribed above. All regularized models outper-
form their baseline counterparts for all token sizes.
Among them, the models regularized by the com-
bined penalty consistently perform best. This il-
lustrates that conditional n-gram regularization is
effective at incorporating large-corpus statistics to
improve an RNNLM trained on a relatively small
corpus. Performing regularization using the com-
bined selection strategy yields more accurate mod-
els compared to simply training on a larger se-
quential corpus.

In Figure 1, we plot validation perplexities after



3271

each training epoch for models trained on the 5M-
token 1B corpus, against an equal-time baseline.
The plot shows that the relative performance of the
methods remains similar across training epochs.

Methods Wikitext Google 1B500K 1M 2M 2.5M 5M 10M
baseline 161 110 76 110 90 81
sq log 122 91 72 98 86 80
KL 137 100 77 100 86 78
combined 112 86 69 94 83 77

Table 1: Test perplexities of different methods and se-
quential training token sizes. sq log: mean squared log
probability ratio penalty. KL: mean Kullback-Leibler
divergence penalty. Models with the combined penalty
achieve the lowest perplexities.

Figure 1: Validation Perplexities on the 5M-token 1B
Dataset. The combined penalty performs best.

3.3 Analysis

In Table 2, we demonstrate how the number of reg-
ularized bigrams affects the performance. Here,
the regularized models always train on wikitext-
2 as a sequential corpus, and the baseline trains
on larger corpora as the fractions of bigrams in-
creases. The regularized model achieves 66 test
perplexity on Wikitext-2 corpus, which is about
2.7 points worse than a state of the art mixture of
softmax model (Yang et al., 2017) even though our
model has fewer parameters (25M vs. 35M). In-
cluding more bigrams helps lower the perplexity,
while it also demands extra computational time.
ELC performs best when using less than 20% of
the bigrams. Regularizing randomly selected n-
grams does not outperform the equal-time base-
line, indicating that not all n-grams are equally
useful. In order to be time efficient, it is impor-
tant to select informative n-grams, and ELC is an

% of total bigrams baseline random ELC
0% 86 86 86
5% 76 83 69
10% 71 77 67
20% 66 71 67
40% 61 68 66

Table 2: Test perplexities of RNNLMs trained on
Wikitext-2 regularized with different numbers of bi-
grams using the combined penalty. Baseline indicates
an equal-time controlled baseline, random is a regular-
ized model with bigrams selected randomly, and ELC
indicates our proposed strategy.

effective measure of informativeness.
In Table 3, we consider ensembling a standard

RNN with a KN-smoothed trigram model. This
achieves a ppl of 65, but requires 51M more pa-
rameters, whereas our regularization with the n-
grams achieves most of the perplexity improve-
ment at the cost of zero additional parameters.
Further, somewhat surprisingly we find that an en-
semble of our regularized RNNLM with the n-
gram model achieves much better perplexity of 59.

Models Test PPL
Unregularized RNNLM 86
Unregularized RNNLM + Trigram KN 65
Regularized RNNLM 67
Regularized RNNLM + Trigram KN 59

Table 3: Test perplexities of RNN-LMs trained with
and without regularization on the Wikitext-2 corpus,
ensembled with a KN-smoothed trigram model trained
on the Wikitext-103 corpus.

Another possible way of efficiently utilizing a
large corpus would involve training a Word2vec
model on the large corpus, and using the pre-
trained embeddings within a RNNLM trained on
a small corpus. This approach can utilize larger
corpora since training a Word2vec model is much
faster than training a RNNLM. However, in our
preliminary experiments with this approach, we
did not observe any improvement when using
word embeddings trained on a large corpus. Fur-
ther experiments with variants of this approach are
an item of future work.

4 Related Work

Chelba et al. (2017) trained large-order n-gram
models using a recurrent neural network trained
over limited context to produce the conditional
probability estimates. Our regularizer is trained
in a similar way, but by contrast we are focused
on how the regularizer can be used in concert with



3272

standard sequential RNNLM training to improve
the training procedure. We introduce n-gram se-
lection techniques and distinct loss functions that
increase the effectiveness of the combined train-
ing. Ganchev et al. (2010) presents a posterior
regularization method for restricting posterior dis-
tributions of probabilistic models with latent vari-
ables to obey predefined constraints using the EM
algorithm. This approach shares our goal of im-
posing constraints on probabilistic models, but we
focus on RNNLMs, which do not estimate dis-
tributions over latent state variables and are not
trained using EM. Finally, Mikolov et al. (2011),
Józefowicz et al. (2016) and Chelba et al. (2013)
trained ensembles of RNNLMs and KN-smoothed
n-gram models, and showed that one can obtain
a better model when ensembling RNNLMs with
n-gram models. Our experiments show that com-
pared to ensemble methods, conditional n-gram
regularization achieves similar results at the cost
of zero additional parameters, and can perform
even better when combined with ensembling.

5 Conclusion and Future Work

In this paper, we have proposed methods to utilize
using large corpus n-gram statistics to regularize
RNNLMs trained on a smaller corpus. Our ex-
periments demonstrate that the proposed regular-
ization penalties are effective in improving model
performance, and can be more time efficient than
training RNNLMs on a larger sequential corpus.
Selecting informative n-grams is shown to be im-
portant. In future work, we would like to obtain
a better theoretical understanding of why starting
the RNNLM from a zero state forms an effective
n-gram regularizer. We would also like to extend
our regularization approach to BiLSTMs (Peters
et al., 2017) and Transformers (Alec Radford and
Sutskever, 2018; Devlin et al., 2018).

Acknowledgments

This work was supported in part by NSF Grant
IIS-1351029 and the Allen Institute for Artificial
Intelligence.

References
Tim Salimans Alec Radford, Karthik Narasimhan and

Ilya Sutskever. 2018. Improving language under-
standing with unsupervised learning. Technical re-
port, OpenAI.

Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,
Thorsten Brants, and Phillipp Koehn. 2013. One bil-
lion word benchmark for measuring progress in sta-
tistical language modeling. CoRR, abs/1312.3005.

Ciprian Chelba, Mohammad Norouzi, and Samy
Bengio. 2017. N-gram language modeling us-
ing recurrent neural network estimation. CoRR,
abs/1703.10724.

Stanley F. Chen and Joshua Goodman. 1999. An
empirical study of smoothing techniques for lan-
guage modeling. Computer Speech & Language,
13(4):359–393.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. arXiv preprint arXiv:1810.04805.

Yarin Gal and Zoubin Ghahramani. 2016. A theoret-
ically grounded application of dropout in recurrent
neural networks. In Advances in Neural Informa-
tion Processing Systems 29: Annual Conference on
Neural Information Processing Systems 2016, De-
cember 5-10, 2016, Barcelona, Spain, pages 1019–
1027.

Kuzman Ganchev, João Graça, Jennifer Gillenwater,
and Ben Taskar. 2010. Posterior regularization for
structured latent variable models. Journal of Ma-
chine Learning Research, 11:2001–2049.

Hakan Inan, Khashayar Khosravi, and Richard Socher.
2016. Tying word vectors and word classifiers:
A loss framework for language modeling. CoRR,
abs/1611.01462.

Rafal Józefowicz, Oriol Vinyals, Mike Schuster, Noam
Shazeer, and Yonghui Wu. 2016. Exploring the lim-
its of language modeling. CoRR, abs/1602.02410.

Stephen Merity, Caiming Xiong, James Bradbury, and
Richard Socher. 2016. Pointer sentinel mixture
models. CoRR, abs/1609.07843.

Tomas Mikolov, Stefan Kombrink, Lukás Burget, Jan
Cernocký, and Sanjeev Khudanpur. 2011. Exten-
sions of recurrent neural network language model.
In Proceedings of the IEEE International Confer-
ence on Acoustics, Speech, and Signal Processing,
ICASSP 2011, May 22-27, 2011, Prague Congress
Center, Prague, Czech Republic, pages 5528–5531.

Thanapon Noraset, Doug Downey, and Lidong Bing.
2018. Estimating marginal probabilities of n-grams
for recurrent neural language models. In Proceed-
ings of the 2018 Conference on Empirical Methods
in Natural Language Processing, pages 2930–2935.
Association for Computational Linguistics.

Matthew E. Peters, Waleed Ammar, Chandra Bhaga-
vatula, and Russell Power. 2017. Semi-supervised
sequence tagging with bidirectional language mod-
els. CoRR, abs/1705.00108.



3273

Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and
William W. Cohen. 2017. Breaking the softmax bot-
tleneck: A high-rank RNN language model. CoRR,
abs/1711.03953.

Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals.
2014. Recurrent neural network regularization.
CoRR, abs/1409.2329.


