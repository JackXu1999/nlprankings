








































A Personalized Markov Clustering and Deep Learning Approach for
Arabic Text Categorization

Vasu Jindal
University of Texas at Dallas

Richardson, TX 75080
vasu.jindal@utdallas.edu

Abstract

Text categorization has become a key re-
search field in the NLP community. How-
ever, most works in this area are focused
on Western languages ignoring other
Semitic languages like Arabic. These lan-
guages are of immense political and so-
cial importance necessitating robust cate-
gorization techniques. In this paper, we
present a novel three-stage technique to
efficiently classify Arabic documents into
different categories based on the words
they contain. We leverage the significance
of root-words in Arabic and incorporate
a combination of Markov clustering and
Deep Belief Networks to classify Arabic
words into separate groups (clusters). Our
approach is tested on two public datasets
giving a F-Measure of 91.02%.

1 Introduction

In the emerging era of big data technology, there
has been a widespread increase in information ob-
tained from text documents. Furthermore, with
the rapid availability of machine-readable docu-
ments, text classification techniques have gained
tremendous interest during the recent years. Con-
sequently, automatic categorization of numerous
new documents to different categories has become
critical for political, social and for news research
purposes.

Text categorization techniques have been
widely investigated by researchers around the
world. However, most of the recent developments
in this field are focused on popular Western lan-
guages, ignoring Semitic and Middle-Eastern lan-
guages like Arabic, Hebrew, Urdu and Persian.
As discussed further in related works in Section
2.1, most classification algorithms utilized English

and Chinese to validate their methods while works
on Arabic are extremely rare. This is primarily
due to significant dialect differences between these
languages and their complex morphology. Ad-
ditionally, the presence of various inflections in
Arabic as opposed to English makes it difficult
for the NLP community to validate techniques on
the popular Middle Eastern languages. According
to the US Department of Cultural Affairs, Arabic
and Urdu are categorized as critical languages and
United Nations heavily emphasizes on the social
and political importance of these languages. Ara-
bic is listed as one of the six official languages of
the United Nations.

In this paper, we present a novel three stage
approach to classify Arabic text documents into
different categories combining Markov Cluster-
ing, Fuzzy-C-means and Deep Learning. To the
best of our knowledge, this is the first work that
leverages the heavy influence of root-words in
Arabic to extract features for both clustering and
deep learning to perform classification of Ara-
bic documents. First, we segment each document
and extract root-word information. Then we per-
form clustering with root-word based features us-
ing Fuzzy-C-Means and Markov clustering. This
allows us to separate documents into unsupervised
groups (clusters). We then train a deep belief
network (DBN) for each cluster using Restricted
Boltzmann Machines. This personalization which
is essentially training DBN for each cluster im-
proves the classification accuracy and features ex-
traction. Finally, we generate network graphs of
these clusters which can be used for similarity re-
latedness or summarization in future works. This
is the first work to use a modified combination of
Markov clustering and personalized deep learning
to classify documents into different categories.

The rest of the paper is organized as follows:
Section 2 discusses the literature review and Ara-



bic morphology. Section 3 focuses on methodol-
ogy for Markov clustering and deep learning. Sec-
tion 4 discusses our experimental results and fi-
nally, Section 5 summarizes the paper and presents
conclusions and future work.

2 Background

2.1 Related Works

As mentioned previously, even though numer-
ous works in text categorization have been pro-
posed for Western languages, works on categoriz-
ing Semitic languages like Arabic are very rare in
the NLP community.

Most previous works on Arabic text categoriza-
tion treats documents as a bag-of-words where the
text is represented as a vector of weighted fre-
quencies for each of the distinct words or tokens.
(Diederich et al. 2003; Sebastiani et al. 2002).
We use a similar approach to extract features from
documents based on root-word frequency. Early
efforts to categorize Arabic documents were per-
formed using Naive Bayes algorithm (El-Kourdi et
al., 2004), maximum entropy (Sawaf et al., 2001)
and support vector machines (Gharib et al., 2009).

N-gram frequency statistics technique was used
with Manhattan distance to categorize documents
by Khreisat (Khreisat, 2006) El-Halees described
a method based on association rules to classify
Arabic documents (El-Halees, 2007). Hmeidi I.
uses two machine learning methods for Arabic
text categorization: K-Nearest Neighbor (KNN)
and support vector machines (SVM) (Hmeidi et
al., 2008). An approach for feature selection in
Arabic text categorization was proposed using in-
formation gain and Chi-square (Yang and Peder-
sen, 1997). Abu-Errub A. proposed a new Arabic
text classification algorithm using Tf-Idf and Chi
square measurements (Abu-Errub, 2014). How-
ever, all these methods were not very efficient for
large datasets giving an accuracy of less than 70%
and were unable to classify documents with dif-
ferent diacritics. Diacritics are signs or accents
whose pronunciation or presence in a word can re-
sult in a different meaning.

Recently, a new technique using a combina-
tion of kNN and Rocchio classifier for text cat-
egorization was introduced (Mohammad et al.,
2016). This approach specifically solves the Word
Sense Disambiguation (WSD) problem in a su-
pervised approach using lexical samples of five
Arabic words. Although, this method achieved

higher accuracy than previous works, usage of
only five Arabic words limits usage for larger
datasets. Our proposed approach generates mul-
tiple roots of each Arabic words addressing the is-
sue of different diacritics in Arabic.

2.2 Arabic Morphology

Arabic belongs to the family of Semitic languages
and has significant morphological, syntactical and
semantical differences from other languages. It
consists of 28 letters and can be extended to 90 by
added shapes, marks, and vowels. Furthermore,
Arabic is written from right to left and letters have
different styles based on the position of its appear-
ance in the word. The base words of Arabic inflect
to express eight main features. Verbs inflect for
aspect, mood, person and voice. Nouns and ad-
jectives inflect for case and state. Verbs, nouns
and adjectives inflect for both gender and num-
ber. Arabic morphology consists from a bare root
verb form that is trilateral, quadrilateral, or pental-
iteral. The derivational morphology can be lexeme
= Root + Pattern or inflection morphology (word =
Lexeme + Features) where features are noun spe-
cific, verb specific or single letter conjunctions. In
contrast, in most European languages words are
formed by concatenating morphemes. For exam-
ple in German, ’Zeitgeit’(the spirit of the times) is
simply ’Zeit’(time) + ’geist’(spirit) i.e the root and
pattern are essentially interleaved.

Stem pattern are often difficult to parse in Ara-
bic as they interlock with root consonants (Abde-
lali, 2004). Arabic is also influenced by infixes
which may be consonants and vowels and can be
misinterpreted as root-words. One of the major
problem is usage of a consonant, hamza. Hamza
is not always pronounced and can be a vowel. This
creates a severe orthographic problem as words
may have differently positioned hamzas making
them different strings yet having similar meaning.

Furthermore, diacritics are critical in categoriz-
ing Arabic documents. For example, consider the
two words ”zhb” mean ”to go” and ”gold” differ-
ing by just one diacritic. The two words can only
be distinguished using diacritics. The word ”go”
may appear in a variety of text documents while
”gold” may likely appear in documents contain-
ing other finance related words. This is where our
personalized deep learning approach is extremely
efficient for Arabic as discussed in future sections.
For the purpose of clarity, we use the term ”root-



words” throughout this paper to represent the roots
of an Arabic word.

3 Methodology

Figure 1 presents an overview of our algorithm.
In summary, our approach consists of a pre-
processing stage, two stages of clustering and fi-
nally a learning stage. In the pre-processing stage
documents are tokenized and segmented into dif-
ferent words and the Tf-Idf weighted root-word
counts are extracted. Subsequently, we cluster the
documents by a combination of Fuzzy C-means
and Markov clustering in Step I and II. Finally, in
Step III, we use deep learning models on each ob-
tained cluster to personalize learning for each root
word cluster. Personalization essentially means to
train a separate deep belief network for each clus-
ter. Each of these stages is discussed in subsequent
sections.

3.1 Pre-Processing

In the pre-processing stage we first remove the
punctuation marks, auxiliary verbs, pronouns,
conjunctions etc. As we stated in section 2.2, it is
very important for processing Arabic to properly
use the semantic information provided by root-
words (Kanaan et al., 2009). Therefore, repre-
senting words presented in a document in the root
pattern increases efficiency of classification. Root
extraction or stemming for the Arabic dataset is
performed using a letter weight and order scheme.
For example, the root meaning ”write” has the
form k-t-b. More words can be formed using vow-
els and additional consonants in the root word.
Some of the words that can be formed using
k-t-b are kitab meaning ”book”, kutub meaning
”books”, katib meaning ”writer”, kuttab represent-
ing ”writers”, kataba meaning ”he wrote”, yaktubu
meaning ”he writes”, etc.

In our method we assign weights to letters of
Arabic and subsequently rank them based on their
frequency in the document. Root-words of the
Arabic word are selected by recurring patterns
with the maximum weight. Furthermore, we cal-
culate the standard Tf-Idf frequency of each root
word to use as features in clustering and deep
learning. Tf-Idf (term frequency-inverse docu-
ment frequency) is one of the widely used fea-
ture selection techniques in information retrieval
(Baeza-Yates et al., 1999). Tf measures the im-
portance of a term in a given document while Idf

signifies the relative importance of a term in a col-
lection of documents. In the next section, we will
discuss the clustering step of our approach.

3.2 Estimation of Initial Number of Clusters

The words frequencies (Tf-Idf) and root-word fre-
quencies are used from the pre-processing stage
and grouped into clusters. Once the words are
clustered, we consider each document and find
which cluster of words it may belong. This is done
by rank-matching based approach discussed later.
The clustering step is described below.

Consider each document to be P

P =

 p0,0 p0,1 · · · p0,W−1... ... · · · ...
pH−1,0 pH−1,2 · · · pH−1,W−1

 (1)
where pi,0 indicate extracted words tokenized
from the documents. pi,j are the similar root-
words of pi,0.

We first estimate the initial number of cate-
gories that may be present in a corpus of text doc-
uments. The estimation of initial number of clus-
ters is critical for text categorization as many Ara-
bic documents may contain synonyms, different
morphologies of Arabic and yet may have a close
meaning. We perform estimation by computing
the total number of modes found in all eigenvec-
tors of the data. Modes in each eigenvector of the
data are detected using kernel density estimation.
Then, significance test of the gradient and second
derivative of a kernel density estimation is com-
puted. A similar approach was used for bioinfor-
matics data (Pouyan et al., 2015). The number of
modes is approximated using the number of times,
the gradient changes from positive to negative for
each projection of data on the eigenvectors. K rep-
resents initial number of clusters approximated by
summation of all the modes in eigenvectors.

3.3 Initial Clustering Using Fuzzy-C-Mean

We leverage the heavy influence of root-words in
Arabic for clustering words (tf-idf) from the pre-
processing stage. As described in section 2.1, new
words in Arabic can be formed by filling vowels or
consonants. For example, k-t-b is a triliteral root
word as it contains a sequence of three consonants.
Accordingly, an improved version of Fuzzy-C-
Means clustering is developed to calculate the
membership probability of each words to its root



Estimation of 
Number of Distinct 

Root-words

Estimation of 
Number of Distinct 

Root-words

Modified 
Fuzzy C-
Means 

Clustering

Modified 
Fuzzy C-
Means 

Clustering

Modified
Markov 

Clustering

Modified
Markov 

Clustering

.

Step I Step II

Classification 
using Deep 

Belief 
Networks

Classification 
using Deep 

Belief 
Networks

TF-IDF of Arabic 
Documents

بسم هللا الرحمن الرحيم 9
مرحبا 5

Network 
Graphs of 

Arabic words

Step III

Figure 1: The general view of our proposed method

word. The clusters we obtain are intended to cor-
respond to the different root-words in the docu-
ment. Concisely, χ = {α1, ..., αj , ..., αK} will be
centers of K root-words C = {c1, ..., cj , ..., cK}
which represents potential similarities of root-
words. Words are assigned to different root word
clusters by minimizing the following optimization
model:

Jm =
N∑
i=1

K∑
j=1

umijDm(xi, αj) (2)

where word xi belongs to root word cj with the
membership probability of uij . The fuzzification
coefficient is selected as m = 2 similar to set by
James C. Bezdek. Dm(xi, αj) implies the Maha-
lanobis Distance between word xi and root word
cj . Since membership probability depends on the
dispersion of cluster cj , we use Mahalanobis Dis-
tance instead of Euclidean Distance as a distance
metric between xi and population cj . The initial
cluster set C will be available after applying the
revised Fuzzy-C-Means and used in future steps.

3.4 Merging and Ranking Clusters Using
Markov Clustering

Kernel density may result in duplicate clusters due
to the similar diacritics or morphologies in Ara-
bic. Furthermore, it is important to rank the clus-
ters based on the similar root words. We use
Markov clustering to address this issue, a fast,
divisive and scalable clustering algorithm based
on stochastic modelling of flow of networks (Van
Dongen, 2001). Markov clustering (MCL) has re-
cently emerged as a popular clustering technique
for determining cluster networks. The algorithm
computes the probability of random walks through
a graph by applying two main methods: expansion
and inflation. When MCL is applied on centers of
initial clusters, the centers corresponding to ini-
tial populations will be clustered in the same seg-
ments. We extract the final categories of the text
documents by merging these clusters. The redun-

dant clusters due to similar diacritics are merged
in this stage.

Once the words are clustered, we consider each
document and find the most similar cluster of
words it may belong to using a rank-matching
based approach. The ranking algorithm can be
explain by considering a graph of the Arabic root
words of interest. The frequency of a random walk
visiting a particular root word in sufficiently large
steps i.e. in a stationary distribution is the score
of the root-word. Let aij represent the fraction of
time root-word j has higher rank over root-word
i. Then weighted edges of graph Aij =

aij
aij+aji

.
Recall that we denoted the document with matrix
P in the previous step. Let pt(i) = P (Xt = i)
denote the distribution of the random walk at time
t. Then, the transition matrix can be formulated
as:

pTt+1 = p
T
t · P (3)

Iteratively, the random walk will converge to a
unique stationary distribution and the score can be
found using:

π(i) =
∑
j

π(j) · Aij +Aji∑
l(Ail +Ali)

(4)

We find the maximum match between the 40%
most frequent words in the document and every
cluster. Cluster of words consisting of the highest
number of words from top 40% frequent words in
the document is assigned to the document.

3.5 Deep Learning
We further use the state-of-the-art deep learning
for extracting features from the clusters and use
them for future clustering. We create separate
deep belief network classifier for each cluster,
which allows us to capture differences in between
dialects, topics etc. To reiterate, our novel contri-
bution is a personalized deep learning model for
each root word cluster. Personalized means train-
ing of each model for each specific root word, di-



h1
80 nodes

h2
80 nodes

h3
40 nodes

Input
Layer

Output
Layer

Root 
Words

Classes 
of 

Classified 
Root 
words

Pre-trained hidden layers using RBM

Figure 2: Deep Neural Network for C1

alects and diacritics. The classifiers get more fine-
grained by training one classifier for every cluster.

This personalization poses several advantages:
the features learned using deep learning are per-
sonalized for each root word. Consequently,
the technique is robust against different dialects,
scripts and way of writing. Secondly, personalized
deep learning models extract features from diacrit-
ics. As previously described in section 2.2, dia-
critics makes Arabic word classification very chal-
lenging. By extracting features from diacritics in
the context of appeared root-words, our personal-
ized deep learning approach efficiently solves this
problem.

We separately train deep belief networks (DBN)
on each cluster obtained from Stage II. For ex-
ample the deep network contains one input layer,
three hidden layers and one output layer for Clus-
ter 1 with hidden layers consisting of 80, 80 and
40 nodes respectively. The input of these classi-
fication models are words in each clusters. The
activation function of hidden units is the sigmoid
function which is traditionally used in nonlinear
neural networks. Furthermore, higher number of
parameters in neural networks generally makes pa-
rameter estimation much more difficult. There-
fore, it is inefficient to start training of deep neu-
ral networks from random initial weight and bias
values. We incorporate Restricted Boltzmann Ma-
chines (RBM) to pre-train the network and find
good initial weights for training deep belief net-
works (Le Roux and Bengio, 2008).

Let Di be a DBN model for cluster Ci. The
hidden layers of each Di are first trained as
RBMs using unlabeled inputs. We use Contrastive
Divergence-1 (CD-1) algorithm to obtain samples
after 1-step Gibbs sampling (Hinton, 2002). CD-
1 allows accurate estimation of gradient’s direc-
tion and minimize reconstruction error. Due to this

pre-training, Di learns an identity function with
same desired output as the original input. Further-
more, it enhances the robustness of Di by learning
feature representations of the Arabic words before
the final supervised learning stage.

Subsequently, the pre-trained DBN network is
fine-tuned by vanilla back-propagation with la-
beled segments as the input layer. Figure 2 gives
the abstract structure of final resulting D1 after
tuning and is also used for identification for clus-
terC1. If a new document is added for text catego-
rization then we find it’s nearest neighbor cluster
and use the deep learning model specific to that
cluster. Finally, we plot the network graphs of the
extracted cluster words for visualization of the as-
sociation of these words.

4 Experimental Results
We evaluate our three-stage technique using
two popular datasets previously used in Arabic
text categorization: 10,000 documents from Al-
Jazeera news website (http://www.aljazeera.net)
and 6,000 Saudi Press Agency (Al-Harbi et al.,
2008) documents. The results are reported using
10-fold cross validation. Our proposed method
achieves a precision of 91.2% and recall of 90.9%
giving F-measure of 91.02%.

Clustering is performed on a set consisting of
the total 12,000 documents, randomly sampled
from separately from Al-Jazeera and Saudi Press
Agency. We further ran deep learning on each
of these clusters and extracted network graphs.
Two example networks are presented in Figure 3
and 4. We compare our results with existing ap-
proaches in Table 1. We can see that our tech-
nique improves substantially on the previous pub-
lished works. Furthermore, it is capable to catego-
rize different diacritics by using clusters based on
root-words. Most misclassified cases in our algo-
rithm due to random outliers and/or mix categories
in a document. An example of a random outlier
are some recent words which are not influenced
by root-words. This can be further improved by
using a larger dataset and using new discrimina-
tive features for clustering and deep learning.

5 Conclusion and Future Work

This paper presents a novel three-stage technique
for Arabic text categorization using a combina-
tion of clustering and deep learning. We lever-
age the influence of root-words in Arabic to ex-
tract the features. Our technique is robust against



Table 1: Performance of our algorithm on Al-Jazeera Dataset

Technique Precision Recall Root-words based? Robust to Diacritcs?
Naive Bayes (El-Kourdi et al., 2004) 62.6% 57.4% No No

SVM (Hmeidi et al.,2008) 71.2% 66.7% No No
kNN (Mohammad AH et al., 2016) 83.0% 80.2% No No

Ours 91.2% 90.9% Yes Yes

Figure 3: Network of a document’s words from
Cluster 1

Figure 4: Network of a document’s words from
Cluster 2

different diacritics and complex morphology of
Semitic languages. Furthermore, this procedure
can be extended to Persian and Semitic languages
like Hebrew which heavily depend on root-words.
Future work includes extracting more discrimina-
tive features of root-words using deep learning
and improving training of our models using larger
datasets. We also plan to explore other Arabic
morphologies like lemmas used in Arabic depen-
dency parsing (Marton et al., 2003).

6 Acknowledgements

We acknowledge the support and guidance of
Quality of Life Technology (QoLT) laboratory in
the University of Texas at Dallas. We are thankful
to Dr. Maziyar Baran Pouyan and Dr. Mehrdad
Nourani for conceiving the original integration
techniques for bioinformatics data using Gaussian
Estimation and Fuzzy-c-means. We further like
to thank University of Texas at Dallas and ACL
Don and Betty Walker Scholarship program. We
are specially thankful to Dr. Christoph Teichmann
for his insightful comments as the mentor through
ACL Student Mentorship Program.

7 References

Ahmed Abdelali. 2004. Localization in modern
standard Arabic. Journal of the American Society
for Information Science and technology, 55(1):23-
28.

Aymen Abu-Errub. 2014. Arabic text classifi-
cation algorithm using tf-idf and chi square mea-
surements. International Journal of Computer Ap-
plications, 93(6).

S Al-Harbi, A Almuhareb, A Al-Thubaity, MS
Khorsheed, and A Al-Rajeh. 2008. Automatic
Arabic text classification. Journes internationals,
France, pp. 77–83.

Ricardo Baeza-Yates, Berthier Ribeiro-Neto, et
al. 1999. Modern information retrieval, volume
463. ACM press New York.

Joachim Diederich, Jorg Kindermann, Edda
Leopold,and Gerhard Paass. 2003. Authorship at-
tribution with support vector machines. Applied
intelligence, 19(1-2):109-123.

Alaa El-Halees. 2007. Arabic text classifica-
tion using maximum entropy. The Islamic Uni-
versity Journal(Series of Natural Studies and En-
gineering), 15(1):157-167.



Mohamed El Kourdi, Amine Bensaid, and
Tajje-eddine Rachidi. 2004. Automatic arabic
document categorization based on the Naive bayes
algorithm. In Proceedings of the Workshop on
Computational Approaches to Arabic Script-based
Languages, pages 51-58. Association for Compu-
tational Linguistics.

Tarek F Gharib, Mena B Habib, and Zaki T
Fayed. 2009. Arabic text classification using
support vector machines. International Journal of
Computer Applications, 16(4):192-199.

Geoffrey E Hinton. 2002. Training products
of experts by minimizing Contrastive Divergence.
Neural computation, 14(8):1771-1800.

Ismail Hmeidi, Bilal Hawashin, and Eyas El-
Qawasmeh. 2008. Performance of kNN and SVM
classifiers on full word arabic articles. Advanced
Engineering Informatics, 22(1):106–111.

Ghassan Kanaan, Riyad Al-Shalabi, Sameh Gh-
wanmeh, and Hamda Al-Maadeed. 2009. A com-
parison of text-classification techniques applied to
arabic text. Journal of the American society for
Information Science and Technology, 60(9):1836-
1844.

Laila Khreisat. 2006. Arabic text classification
using N-gram frequency statistics a comparative
study. DMIN, 2006:78-82.

Nicolas Le Roux and Yoshua Bengio. 2008.
Representational power of restricted boltzmann
machines and deep belief networks. Neural com-
putation, 20(6):1631-1649.

Yuval Marton, Nizar Habash, and Owen Ram-
bow. 2013. Dependency parsing of modern stan-
dard arabic with lexical and inflectional features.
Computational Linguistics, 39(1):161-194.

Adel Hamdan Mohammad, Omar Al-Momani,
and Tariq Alwadan. 2016. Arabic text catego-
rization using k-nearest neighbour, decision trees
(c4.5) and rocchio classifier: A comparative study.

M Baran Pouyan, V Jindal, J Birjandtalab,
and M Nourani. 2015. A two-stage cluster-
ing technique for automatic biaxial gating of flow
cytometry data. In Proceedings of 2015 IEEE

International Conference on Bioinformatics and
Biomedicine (BIBM), pages 511-516.

Hassan Sawaf, Jorg Zaplo, and Hermann Ney.
2001. Statistical classification methods for ara-
bic news articles. Natural Language Processing in
ACL’2001, Toulouse, France.

Fabrizio Sebastiani. 2002. Machine learning in
automated text categorization. ACM computing
surveys (CSUR), 34(1):1-47.

Stijn Marinus Van Dongen. 2001. Graph clus-
tering by flow simulation.

Yiming Yang and Jan O Pedersen. 1997. A
comparative study on feature selection in text cat-
egorization. In ICML, volume 97, pages 412-420.


