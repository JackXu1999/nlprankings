



















































Learning Global Features for Coreference Resolution


Proceedings of NAACL-HLT 2016, pages 994–1004,
San Diego, California, June 12-17, 2016. c©2016 Association for Computational Linguistics

Learning Global Features for Coreference Resolution

Sam Wiseman and Alexander M. Rush and Stuart M. Shieber
School of Engineering and Applied Sciences

Harvard University
Cambridge, MA, USA

{swiseman,srush,shieber}@seas.harvard.edu

Abstract

There is compelling evidence that corefer-
ence prediction would benefit from modeling
global information about entity-clusters. Yet,
state-of-the-art performance can be achieved
with systems treating each mention prediction
independently, which we attribute to the inher-
ent difficulty of crafting informative cluster-
level features. We instead propose to use re-
current neural networks (RNNs) to learn la-
tent, global representations of entity clusters
directly from their mentions. We show that
such representations are especially useful for
the prediction of pronominal mentions, and
can be incorporated into an end-to-end coref-
erence system that outperforms the state of the
art without requiring any additional search.

1 Introduction

While structured, non-local coreference models
would seem to hold promise for avoiding many com-
mon coreference errors (as discussed further in Sec-
tion 3), the results of employing such models in
practice are decidedly mixed, and state-of-the-art
results can be obtained using a completely local,
mention-ranking system.

In this work, we posit that global context is indeed
necessary for further improvements in coreference
resolution, but argue that informative cluster, rather
than mention, level features are very difficult to de-
vise, limiting their effectiveness. Accordingly, we
instead propose to learn representations of mention
clusters by embedding them sequentially using a re-
current neural network (shown in Section 4). Our
model has no manually defined cluster features, but

instead learns a global representation from the indi-
vidual mentions present in each cluster. We incor-
porate these representations into a mention-ranking
style coreference system.

The entire model, including the recurrent neu-
ral network and the mention-ranking sub-system, is
trained end-to-end on the coreference task. We train
the model as a local classifier with fixed context (that
is, as a history-based model). As such, unlike several
recent approaches, which may require complicated
inference during training, we are able to train our
model in much the same way as a vanilla mention-
ranking model.

Experiments compare the use of learned global
features to several strong baseline systems for coref-
erence resolution. We demonstrate that the learned
global representations capture important underlying
information that can help resolve difficult pronom-
inal mentions, which remain a persistent source of
errors for modern coreference systems (Durrett and
Klein, 2013; Kummerfeld and Klein, 2013; Wise-
man et al., 2015; Martschat and Strube, 2015). Our
final system improves over 0.8 points in CoNLL
score over the current state of the art, and the im-
provement is statistically significant on all three
CoNLL metrics.

2 Background and Notation

Coreference resolution is fundamentally a clustering
task. Given a sequence (xn)Nn=1 of (intra-document)
mentions – that is, syntactic units that can refer or be
referred to – coreference resolution involves parti-
tioning (xn) into a sequence of clusters (X(m))Mm=1
such that all the mentions in any particular cluster

994



X(m) refer to the same underlying entity. Since the
mentions within a particular cluster may be ordered
linearly by their appearance in the document,1 we
will use the notation X(m)j to refer to the j’th men-
tion in the m’th cluster.

A valid clustering places each mention in exactly
one cluster, and so we may represent a clustering
with a vector z ∈{1, . . . ,M}N , where zn =m iff
xn is a member of X(m). Coreference systems at-
tempt to find the best clustering z∗ ∈ Z under some
scoring function, with Z the set of valid clusterings.

One strategy to avoid the computational in-
tractability associated with predicting an entire clus-
tering z is to instead predict a single antecedent for
each mention xn; because xn may not be anaphoric
(and therefore have no antecedents), a “dummy” an-
tecedent � may also be predicted. The aforemen-
tioned strategy is adopted by “mention-ranking” sys-
tems (Denis and Baldridge, 2008; Rahman and Ng,
2009; Durrett and Klein, 2013), which, formally,
predict an antecedent ŷ ∈Y(xn) for each mention
xn, where Y(xn) = {1, . . . , n−1, �}. Through tran-
sitivity, these decisions induce a clustering over the
document.

Mention-ranking systems make their antecedent
predictions with a local scoring function f(xn, y)
defined for any mention xn and any antecedent
y ∈Y(xn). While such a scoring function clearly
ignores much structural information, the mention-
ranking approach has been attractive for at least two
reasons. First, inference is relatively simple and ef-
ficient, requiring only a left-to-right pass through a
document’s mentions during which a mention’s an-
tecedents (as well as �) are scored and the highest
scoring antecedent is predicted. Second, from a lin-
guistic modeling perspective, mention-ranking mod-
els learn a scoring function that requires a mention
xn to be compatible with only one of its coreferent
antecedents. This contrasts with mention-pair mod-
els (e.g., Bengtson and Roth (2008)), which score
all pairs of mentions in a cluster, as well as with cer-
tain cluster-based models (see discussion in Culotta
et al. (2007)). Modeling each mention as having
a single antecedent is particularly advantageous for
pronominal mentions, which we might like to model

1We assume nested mentions are ordered by their syntactic
heads.

as linking to a single nominal or proper antecedent,
for example, but not necessarily to all other corefer-
ent mentions.

Accordingly, in this paper we attempt to maintain
the inferential simplicity and modeling benefits of
mention ranking, while allowing the model to utilize
global, structural information relating to z in mak-
ing its predictions. We therefore investigate objec-
tive functions of the form

arg max
y1,...,yN

N∑
n=1

f(xn, yn) + g(xn, yn, z1:n−1) ,

where g is a global function that, in making pre-
dictions for xn, may examine (features of) the clus-
tering z1:n−1 induced by the antecedent predictions
made through yn−1.

3 The Role of Global Features

Here we motivate the use of global features for
coreference resolution by focusing on the issues that
may arise when resolving pronominal mentions in
a purely local way. See Clark and Manning (2015)
and Stoyanov and Eisner (2012) for more general
motivation for using global models.

3.1 Pronoun Problems
Recent empirical work has shown that the resolu-
tion of pronominal mentions accounts for a substan-
tial percentage of the total errors made by modern
mention-ranking systems. Wiseman et al. (2015)
show that on the CoNLL 2012 English development
set, almost 59% of mention-ranking precision errors
and almost 24% of recall errors involve pronominal
mentions. Martschat and Strube (2015) found a sim-
ilar pattern in their comparison of mention-ranking,
mention-pair, and latent-tree models.

To see why pronouns can be so problematic, con-
sider the following passage from the “Broadcast
Conversation” portion of the CoNLL development
set (bc/msnbc/0000/018); below, we enclose men-
tions in brackets and give the same subscript to co-
clustered mentions. (This example is also shown in
Figure 2.)

DA: um and [I]1 think that is what’s - Go
ahead [Linda]2.
LW: Well and uh thanks goes to [you]1 and to
[the media]3 to help [us]4...So [our]4 hat is off
to all of [you]5 as well.

995



This example is typical of Broadcast Conversation,
and it is difficult because local systems learn to my-
opically link pronouns such as [you]5 to other in-
stances of the same pronoun that are close by, such
as [you]1. While this is often a reasonable strategy,
in this case predicting [you]1 to be an antecedent of
[you]5 would result in the prediction of an incoher-
ent cluster, since [you]1 is coreferent with the singu-
lar [I]1, and [you]5, as part of the phrase “all of you,”
is evidently plural. Thus, while there is enough in-
formation in the text to correctly predict [you]5, do-
ing so crucially depends on having access to the his-
tory of predictions made so far, and it is precisely
this access to history that local models lack.

More empirically, there are non-local statistical
regularities involving pronouns we might hope mod-
els could exploit. For instance, in the CoNLL train-
ing data over 70% of pleonastic “it” instances and
over 74% of pleonastic “you” instances follow (re-
spectively) previous pleonastic “it” and “you” in-
stances. Similarly, over 78% of referential “I” in-
stances and over 68% of referential “he” instances
corefer with previous “I” and “he” instances, respec-
tively.

Accordingly, we might expect non-local models
with access to global features to perform signifi-
cantly better. However, models incorporating non-
local features have a rather mixed track record. For
instance, Björkelund and Kuhn (2014) found that
cluster-level features improved their results, whereas
Martschat and Strube (2015) found that they did not.
Clark and Manning (2015) found that incorporating
cluster-level features beyond those involving the pre-
computed mention-pair and mention-ranking prob-
abilities that form the basis of their agglomerative
clustering coreference system did not improve per-
formance. Furthermore, among recent, state-of-the-
art systems, mention-ranking systems (which are
completely local) perform at least as well as their
more structured counterparts (Durrett and Klein,
2014; Clark and Manning, 2015; Wiseman et al.,
2015; Peng et al., 2015).

3.2 Issues with Global Features

We believe a major reason for the relative inef-
fectiveness of global features in coreference prob-
lems is that, as noted by Clark and Manning (2015),
cluster-level features can be hard to define. Specif-

ically, it is difficult to define discrete, fixed-length
features on clusters, which can be of variable size
(or shape). As a result, global coreference features
tend to be either too coarse or too sparse. Thus, early
attempts at defining cluster-level features simply ap-
plied the coarse quantifier predicates all, none, most
to the mention-level features defined on the men-
tions (or pairs of mentions) in a cluster (Culotta et
al., 2007; Rahman and Ng, 2011). For example, a
cluster would have the feature ‘most-female=true’ if
more than half the mentions (or pairs of mentions)
in the cluster have a ‘female=true’ feature.

On the other extreme, Björkelund and Kuhn
(2014) define certain cluster-level features by con-
catenating the mention-level features of a cluster’s
constituent mentions in order of the mentions’ ap-
pearance in the document. For example, if a clus-
ter consists, in order, of the mentions (the president,
he, he), they would define a cluster-level “type” fea-
ture ‘C-P-P=true’, which indicates that the cluster is
composed, in order, of a common noun, a pronoun,
and a pronoun. While very expressive, these con-
catenated features are often quite sparse, since clus-
ters encountered during training can be of any size.

4 Learning Global Features

To circumvent the aforementioned issues with defin-
ing global features, we propose to learn cluster-level
feature representations implicitly, by identifying the
state of a (partial) cluster with the hidden state of
an RNN that has consumed the sequence of men-
tions composing the (partial) cluster. Before pro-
viding technical details, we provide some prelimi-
nary evidence that such learned representations cap-
ture important contextual information by display-
ing in Figure 1 the learned final states of all clus-
ters in the CoNLL development set, projected using
T-SNE (van der Maaten and Hinton, 2012). Each
point in the visualization represents the learned fea-
tures for an entity cluster and the head words of
mentions are shown for representative points. Note
that the model learns to roughly separate clusters by
simple distinctions such as predominant type (nom-
inal, proper, pronominal) and number (it, they, etc),
but also captures more subtle relationships such as
grouping geographic terms and long strings of pro-
nouns.

996



Figure 1: T-SNE visualization of learned entity repre-
sentations on the CoNLL development set. Each point
shows a gold cluster of size > 1. Yellow, red, and pur-
ple points represent predominantly common noun, proper
noun, and pronoun clusters, respectively. Captions show
head words of representative clusters’ mentions.

4.1 Recurrent Neural Networks

A recurrent neural network is a parameterized non-
linear function RNN that recursively maps an in-
put sequence of vectors to a sequence of hidden
states. Let (mj)Jj=1 be a sequence of J input vec-
tors mj ∈RD, and let h0 = 0. Applying an RNN to
any such sequence yields

hj ← RNN(mj ,hj−1;θ) ,

where θ is the set of parameters for the model, which
are shared over time.

There are several varieties of RNN, but by far
the most commonly used in natural-language pro-
cessing is the Long Short-Term Memory network
(LSTM) (Hochreiter and Schmidhuber, 1997), par-
ticularly for language modeling (e.g., Zaremba et al.
(2014)) and machine translation (e.g., Sutskever et
al. (2014)), and we use LSTMs in all experiments.

4.2 RNNs for Cluster Features

Our main contribution will be to utilize RNNs to
produce feature representations of entity clusters
which will provide the basis of the global term g.
Recall that we view a cluster X(m) as a sequence of
mentions (X(m)j )

J
j=1 (ordered in linear document or-

der). We therefore propose to embed the state(s) of
X(m) by running an RNN over the cluster in order.

In order to run an RNN over the mentions we need
an embedding function hc to map a mention to a real
vector. First, following Wiseman et al. (2015) define
φa(xn) : X → {0, 1}F as a standard set of local in-
dicator features on a mention, such as its head word,
its gender, and so on. (We elaborate on features be-
low.) We then use a non-linear feature embedding
hc to map a mention xn to a vector-space represen-
tation. In particular, we define

hc(xn) , tanh(W c φa(xn) + bc) ,

whereW c and bc are parameters of the embedding.
We will refer to the j’th hidden state of the RNN

corresponding to X(m) as h(m)j , and we obtain it ac-
cording to the following formula

h
(m)
j ← RNN(hc(X(m)j ),h(m)j−1;θ) ,

again assuming that h(m)0 = 0. Thus, we will ef-
fectively run an RNN over each (sequence of men-
tions corresponding to a) cluster X(m) in the docu-
ment, and thereby generate a hidden state h(m)j cor-
responding to each step of each cluster in the docu-
ment. Concretely, this can be implemented by main-
taining M RNNs – one for each cluster – that all
share the parameters θ. The process is illustrated in
the top portion of Figure 2.

5 Coreference with Global Features

We now describe how the RNN defined above is
used within an end-to-end coreference system.

5.1 Full Model and Training
Recall that our inference objective is to maximize
the score of both a local mention ranking term as
well as a global term based on the current clusters:

arg max
y1,...,yN

N∑
n=1

f(xn, yn) + g(xn, yn, z1:n−1)

We begin by defining the local model f(xn, y)
with the two layer neural network of Wiseman et
al. (2015), which has a specialization for the non-
anaphoric case, as follows:

f(xn, y) ,
{
uT
[
ha(xn)
hp(xn,y)

]
+ u0 if y 6= �

vTha(xn) + v0 if y = � .

997



DA: um and [I]1 think that is what’s - Go ahead [Linda]2.
LW: Well and thanks goes to [you]1 and to [the media]3 to help [us]4...So [our]4 hat is off to all of [you]5...

X(1)

h
(1)
1 h

(1)
2

[I] [you]

X(2)

h
(2)
1

[Linda]

X(3)

h
(3)
1

[the media]

X(4)

h
(4)
1 h

(4)
2

[us] [our]

[I], h(1)2 [Linda], h
(2)
1 [you], h

(1)
2 [the media], h

(3)
1 [us], h

(4)
2 [our], h

(4)
2 xn = [you] �, NA(xn)

Figure 2: Full RNN example for handling the mention xn = [you]. There are currently four entity clusters in scope
X(1), X(2), X(3), X(4) based on unseen previous decisions (y). Each cluster has a corresponding RNN state, two
of which (h(1) and h(4)) have processed multiple mentions (with X(1) notably including a singular mention [I]). At
the bottom, we show the complete mention-ranking process. Each previous mention is considered as an antecedent,
and the global term considers the antecedent clusters’ current hidden state. Selecting � is treated with a special case
NA(xn).

Above, u and v are the parameters of the model,
and ha and hp are learned feature embeddings of
the local mention context and the pairwise affinity
between a mention and an antecedent, respectively.
These feature embeddings are defined similarly to
hc, as

ha(xn) , tanh(W a φa(xn) + ba)
hp(xn, y) , tanh(W p φp(xn, y) + bp) ,

where φa (mentioned above) and φp are “raw” (that
is, unconjoined) features on the context of xn and
on the pairwise affinity between mentions xn and
antecedent y, respectively (Wiseman et al., 2015).
Note that ha and hc use the same raw features; only
their weights differ.

We now specify our global scoring function g
based on the history of previous decisions. Define
h

(m)
<n as the hidden state of cluster m before a de-

cision is made for xn – that is, h
(m)
<n is the state of

cluster m’s RNN after it has consumed all mentions
in the cluster preceding xn. We define g as

g(xn, y,z1:n−1) ,
{
hc(xn)Th

(zy)
<n if y 6= �

NA(xn) if y = � ,

where NA gives a score for assigning � based on
a non-linear function of all of the current hidden
states:

NA(xn) = qT tanh
(
W s

[
φa(xn)∑M
m=1 h

(m)
<n

]
+ bs

)
.

See Figure 2 for a diagram. The intuition behind
the first case in g is that in considering whether y
is a good antecedent for xn, we add a term to the
score that examines how well xn matches with the
mentions already inX(zy); this matching score is ex-
pressed via a dot-product.2 In the second case, when
predicting that xn is non-anaphoric, we add the NA
term to the score, which examines the (sum of) the
current states h(m)<n of all clusters. This information
is useful both because it allows the non-anaphoric
score to incorporate information about potential an-
tecedents, and because the occurrence of certain
singleton-clusters often predicts the occurrence of
future singleton-clusters, as noted in Section 3.

The whole system is trained end-to-end on coref-
erence using backpropagation. For a given training
document, let z(o) be the oracle mapping from men-
tion to cluster, which induces an oracle clustering.
While at training time we do have oracle clusters, we
do not have oracle antecedents (y)Nn=1, so following
past work we treat the oracle antecedent as latent (Yu
and Joachims, 2009; Fernandes et al., 2012; Chang
et al., 2013; Durrett and Klein, 2013). We train with
the following slack-rescaled, margin objective:

2We also experimented with other non-linear functions, but
dot-products performed best.

998



N∑
n=1

max
ŷ∈Y(xn)

∆(xn, ŷ)(1 + f(xn, ŷ) + g(xn, ŷ,z(o))

− f(xn, y`n)− g(xn, y`n, z(o))),

where the latent antecedent y`n is defined as

y`n , arg max
y∈Y(xn):z(o)y =z(o)n

f(xn, y) + g(xn, y, z(o))

if xn is anaphoric, and is � otherwise. The term
∆(xn, ŷ) gives different weight to different er-
ror types. We use a ∆ with 3 different weights
(α1, α2, α3) for “false link” (FL), “false new” (FN),
and “wrong link” (WL) mistakes (Durrett and Klein,
2013), which correspond to predicting an antecedent
when non-anaphoric, � when anaphoric, and the
wrong antecedent, respectively.

Note that in training we use the oracle clusters
z(o). Since these are known a priori, we can pre-
compute all the hidden states h(m)j in a document,
which makes training quite simple and efficient.
This approach contrasts in particular with the work
of Björkelund and Kuhn (2014) — who also incor-
porate global information in mention-ranking — in
that they train against latent trees, which are not an-
notated and must be searched for during training. On
the other hand, training on oracle clusters leads to a
mismatch between training and test, which can hurt
performance.

5.2 Search

When moving from a strictly local objective to one
with global features, the test-time search problem
becomes intractable. The local objective requires
O(n2) time, whereas the full clustering problem is
NP-Hard. Past work with global features has used
integer linear programming solvers for exact search
(Chang et al., 2013; Peng et al., 2015), or beam
search with (delayed) early update training for an
approximate solution (Björkelund and Kuhn, 2014).
In contrast, we simply use greedy search at test time,
which also requiresO(n2) time.3 The full algorithm

3While beam search is a natural way to decrease search er-
ror at test time, it may fail to help if training involves a local
margin objective (as in our case), since scores need not be cali-
brated across local decisions. We accordingly attempted to train
various locally normalized versions of our model, but found that

Algorithm 1 Greedy search with global RNNs
1: procedure GREEDYCLUSTER(x1, . . . , xN )
2: Initialize clusters X(1) . . . as empty lists, hidden states

h(0), . . . as 0 vectors in RD , z as map from mention to
cluster, and cluster counter M ← 0

3: for n = 2 . . . N do
4: y∗ ← arg max

y∈Y(xn)
f(xn, y) + g(xn, y, z1:n−1)

5: m← zy∗
6: if y∗ = � then
7: M ←M + 1
8: m←M
9: append xn to X(m)

10: zn ← m
11: h(m) ← RNN(hc(xn),h(m))
12: return X(1), . . . , X(M)

is shown in Algorithm 1. The greedy search algo-
rithm is identical to a simple mention-ranking sys-
tem, with the exception of line 11, which updates
the current RNN representation based on the previ-
ous decision that was made, and line 4, which then
uses this cluster representation as part of scoring.

6 Experiments

6.1 Methods
We run experiments on the CoNLL 2012 English
shared task (Pradhan et al., 2012). The task uses
the OntoNotes corpus (Hovy et al., 2006), consist-
ing of 3,493 documents in various domains and for-
mats. We use the experimental split provided in the
shared task. For all experiments, we use the Berke-
ley Coreference System (Durrett and Klein, 2013)
for mention extraction and to compute features φa
and φp.

Features We use the raw BASIC+ feature sets de-
scribed by Wiseman et al. (2015), with the following
modifications:

• We remove all features from φp that concate-
nate a feature of the antecedent with a feature of
the current mention, such as bi-head features.

• We add true-cased head features, a current
speaker indicator feature, and a 2-character

they underperformed. We also experimented with training ap-
proaches and model variants that expose the model to its own
predictions (Daumé III et al., 2009; Ross et al., 2011; Bengio et
al., 2015), but found that these yielded a negligible performance
improvement.

999



System MUC B
3 CEAFe

P R F1 P R F1 P R F1 CoNLL

B&K (2014) 74.30 67.46 70.72 62.71 54.96 58.58 59.40 52.27 55.61 61.63
M&S (2015) 76.72 68.13 72.17 66.12 54.22 59.58 59.47 52.33 55.67 62.47
C&M (2015) 76.12 69.38 72.59 65.64 56.01 60.44 59.44 52.98 56.02 63.02
Peng et al. (2015) - - 72.22 - - 60.50 - - 56.37 63.03
Wiseman et al. (2015) 76.23 69.31 72.60 66.07 55.83 60.52 59.41 54.88 57.05 63.39
This work 77.49 69.75 73.42 66.83 56.95 61.50 62.14 53.85 57.70 64.21

Table 1: Results on CoNLL 2012 English test set. We compare against recent state of the art systems, including (in
order) Bjorkelund and Kuhn (2014), Martschat and Strube (2015), Clark and Manning (2015), Peng et al. (2015), and
Wiseman et al. (2015). F1 gains are significant (p < 0.05 under the bootstrap resample test (Koehn, 2004)) compared
with Wiseman et al. (2015) for all metrics.

genre (out of {bc,bn,mz,nw,pt,tc,wb}) indica-
tor to φp and φa.

• We add features indicating if a mention has a
substring overlap with the current speaker (φp
and φa), and if an antecedent has a substring
overlap with a speaker distinct from the current
mention’s speaker (φp).

• We add a single centered, rescaled document
position feature to each mention when learning
hc. We calculate a mention xn’s rescaled doc-
ument position as 2n−N−1N−1 .

These modifications result in there being approx-
imately 14K distinct features in φa and approxi-
mately 28K distinct features in φp, which is far
fewer features than has been typical in past work.

For training, we use document-size minibatches,
which allows for efficient pre-computation of RNN
states, and we minimize the loss described in Sec-
tion 5 with AdaGrad (Duchi et al., 2011) (after
clipping LSTM gradients to lie (elementwise) in
(−10, 10)). We find that the initial learning rate cho-
sen for AdaGrad has a significant impact on results,
and we choose learning rates for each layer out of
{0.1, 0.02, 0.01, 0.002, 0.001}.

In experiments, we set ha(xn), hc(xn), and h(m)

to be ∈R200, and hp(xn, y)∈R700. We use a
single-layer LSTM (without “peep-hole” connec-
tions), as implemented in the element-rnn li-
brary (Léonard et al., 2015). For regularization,
we apply Dropout (Srivastava et al., 2014) with a
rate of 0.4 before applying the linear weights u,
and we also apply Dropout with a rate of 0.3 to the
LSTM states before forming the dot-product scores.

MUC B3 CEAFe CoNLL

MR 73.06 62.66 58.98 64.90
Avg, OH 73.30 63.06 58.85 65.07
RNN, GH 73.63 63.23 59.56 65.47
RNN, OH 74.26 63.89 59.54 65.90

Table 2: F1 scores of models described in text on CoNLL
2012 development set. Rows in grey highlight models
using oracle history.

Following Wiseman et al. (2015) we use the cost-
weights α = 〈0.5, 1.2, 1〉 in defining ∆, and we
use their pre-training scheme as well. For final re-
sults, we train on both training and development por-
tions of the CoNLL data. Scoring uses the official
CoNLL 2012 script (Pradhan et al., 2014; Luo et al.,
2014). Code for our system is available at https:
//github.com/swiseman/nn_coref. The
system makes use of a GPU for training, and trains
in about two hours.

6.2 Results

In Table 1 we present our main results on the CoNLL
English test set, and compare with other recent state-
of-the-art systems. We see a statistically significant
improvement of over 0.8 CoNLL points over the pre-
vious state of the art, and the highest F1 scores to
date on all three CoNLL metrics.

We now consider in more detail the impact of
global features and RNNs on performance. For these
experiments, we report MUC, B3, and CEAFe F1-
scores in Table 2 as well as errors broken down
by mention type and by whether the mention is
anaphoric or not in Table 3. Table 3 further parti-
tions errors into FL, FN, and WL categories, which

1000



Non-Anaphoric (FL)
Nom. HM Nom. No HM Pron.

MR 1061 1130 1075
Avg, OH 1983 1140 1011
RNN, GH 1914 1125 1893
RNN, OH 1913 1130 1842

# Mentions 9.0K 22.2K 3.1K

Anaphoric (FN + WL)
Model Nom. HM Nom. No HM Pron.

MR 665+326 666+56 533+796
Avg, OH 781+300 641+60 578+744
RNN, GH 767+303 648+57 664+727
RNN, OH 750+289 648+52 611+686

# Mentions 4.7K 1.0K 7.3K

Table 3: Number of “false link” (FL) errors on non-
anaphoric mentions (top) and number of “false new” (FN)
and “wrong link” (WL) errors on anaphoric mentions
(bottom) on CoNLL 2012 development set. Mentions
are categorized as nominal or proper with (previous) head
match (Nom. HM), nominal or proper with no head match
(Nom. No HM), and pronominal. Models are described
in the text, and rows in grey highlight models using oracle
history.

are defined in Section 5.1. We typically think of FL
and WL as representing precision errors, and FN as
representing recall errors.

Our experiments consider several different set-
tings. First, we consider an oracle setting
(“RNN, OH” in tables), in which the model receives
z

(o)
1:n−1, the oracle partial clustering of all mentions

preceding xn in the document, and is therefore not
forced to rely on its own past predictions when pre-
dicting xn. This provides us with an upper bound on
the performance achievable with our model. Next,
we consider the performance of the model under
a greedy inference strategy (RNN, GH), as in Al-
gorithm 1. Finally, for baselines we consider the
mention-ranking system (MR) of Wiseman et al.
(2015) using our updated feature-set, as well as a
non-local baseline with oracle history (Avg, OH),
which averages the representations hc(xj) for all
xj ∈X(m), rather than feed them through an RNN;
errors are still backpropagated through the hc repre-
sentations during learning.

In Table 3 we see that the RNN improves per-
formance overall, with the most dramatic improve-

Figure 3: Cluster predictions of greedy RNN model; co-
clustered mentions are of the same color, and intensity of
mention xj corresponds tohc(xn)Th

(i)
<k, where k= j+1,

i ∈ {1, 2}, and xn = “his.” See text for full description.

ments on non-anaphoric pronouns, though errors are
also decreased significantly for non-anaphoric nom-
inal and proper mentions that follow at least one
mention with the same head. While WL errors also
decrease for both these mention-categories under the
RNN model, FN errors increase. Importantly, the
RNN performance is significantly better than that
of the Avg baseline, which barely improves over
mention-ranking, even with oracle history. This sug-
gests that modeling the sequence of mentions in a
cluster is advantageous. We also note that while
RNN performance degrades in both precision and
recall when moving from the oracle history upper-
bound to a greedy setting, we are still able to recover
a significant portion of the possible performance im-
provement.

6.3 Qualitative Analysis

In this section we consider in detail the impact of the
g term in the RNN scoring function on the two error
categories that improve most under the RNN model
(as shown in Table 3), namely, pronominal WL errors
and pronominal FL errors. We consider an example
from the CoNLL development set in each category
on which the baseline MR model makes an error but
the greedy RNN model does not.

The example in Figure 3 involves the resolution
of the ambiguous pronoun “his,” which is brack-
eted and in bold in the figure. Whereas the baseline
MR model incorrectly predicts “his” to corefer with
the closest gender-consistent antecedent “Justin” —
thus making a WL error — the greedy RNN model

1001



Figure 4: Magnitudes of gradients of NA score applied
to bold “It’s” with respect to final mention in three pre-
ceding clusters. See text for full description.

correctly predicts “his” to corefer with “Mr. Kaye”
in the previous sentence. (Note that “the official”
also refers to Mr. Kaye). To get a sense of the greedy
RNN model’s decision-making on this example, we
color the mentions the greedy RNN model has pre-
dicted to corefer with “Mr. Kaye” in green, and the
mentions it has predicted to corefer with “Justin” in
blue. (Note that the model incorrectly predicts the
initial “I” mentions to corefer with “Justin.”) Let-
ting X(1) refer to the blue cluster, X(2) refer to the
green cluster, and xn refer to the ambiguous mention
“his,” we further shade each mention xj in X(1) so
that its intensity corresponds to hc(xn)Th

(1)
<k, where

k= j+ 1; mentions in X(2) are shaded analogously.
Thus, the shading shows how highly g scores the
compatibility between “his” and a cluster X(i) as
each of X(i)’s mentions is added. We see that when
the initial “Justin” mentions are added to X(1) the
g-score is relatively high. However, after “The com-
pany” is correctly predicted to corefer with “Justin,”
the score of X(1) drops, since companies are gener-
ally not coreferent with pronouns like “his.”

Figure 4 shows an example (consisting of a tele-
phone conversation between “A” and “B”) in which
the bracketed pronoun “It’s” is being used pleonas-
tically. Whereas the baseline MR model predicts
“It’s” to corefer with a previous “it” — thus mak-
ing a FL error — the greedy RNN model does not. In
Figure 4 the final mention in three preceding clusters
is shaded so its intensity corresponds to the magni-
tude of the gradient of the NA term in g with re-
spect to that mention. This visualization resembles
the “saliency” technique of Li et al. (2016), and it at-
tempts to gives a sense of the contribution of a (pre-
ceding) cluster in the calculation of the NA score.

We see that the potential antecedent “S-Bahn”
has a large gradient, but also that the initial, obvi-
ously pleonastic use of “it’s” has a large gradient,

which may suggest that earlier, easier predictions of
pleonasm can inform subsequent predictions.

7 Related Work

In addition to the related work noted throughout,
we add supplementary references here. Unstruc-
tured approaches to coreference typically divide into
mention-pair models, which classify (nearly) every
pair of mentions in a document as coreferent or
not (Soon et al., 2001; Ng and Cardie, 2002; Bengt-
son and Roth, 2008), and mention-ranking models,
which select a single antecedent for each anaphoric
mention (Denis and Baldridge, 2008; Rahman and
Ng, 2009; Durrett and Klein, 2013; Chang et al.,
2013; Wiseman et al., 2015). Structured approaches
typically divide between those that induce a clus-
tering of mentions (McCallum and Wellner, 2003;
Culotta et al., 2007; Poon and Domingos, 2008;
Haghighi and Klein, 2010; Stoyanov and Eisner,
2012; Cai and Strube, 2010), and, more recently,
those that learn a latent tree of mentions (Fernandes
et al., 2012; Björkelund and Kuhn, 2014; Martschat
and Strube, 2015).

There have also been structured approaches that
merge the mention-ranking and mention-pair ideas
in some way. For instance, Rahman and Ng (2011)
rank clusters rather than mentions; Clark and Man-
ning (2015) use the output of both mention-ranking
and mention pair systems to learn a clustering.

The application of RNNs to modeling (the trajec-
tory of) the state of a cluster is apparently novel,
though it bears some similarity to the recent work
of Dyer et al. (2015), who use LSTMs to embed the
state of a transition based parser’s stack.

8 Conclusion

We have presented a simple, state of the art approach
to incorporating global information in an end-to-end
coreference system, which obviates the need to de-
fine global features, and moreover allows for simple
(greedy) inference. Future work will examine im-
proving recall, and more sophisticated approaches
to global training.

Acknowledgments

We gratefully acknowledge the support of a Google
Research Award.

1002



References

Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam
Shazeer. 2015. Scheduled sampling for sequence pre-
diction with recurrent neural networks. In Advances in
Neural Information Processing Systems, pages 1171–
1179.

Eric Bengtson and Dan Roth. 2008. Understanding the
Value of Features for Coreference Resolution. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing, pages 294–303.
Association for Computational Linguistics.

Anders Björkelund and Jonas Kuhn. 2014. Learning
structured perceptrons for coreference Resolution with
Latent Antecedents and Non-local Features. ACL,
Baltimore, MD, USA, June.

Jie Cai and Michael Strube. 2010. End-to-end corefer-
ence resolution via hypergraph partitioning. In 23rd
International Conference on Computational Linguis-
tics (COLING), pages 143–151.

Kai-Wei Chang, Rajhans Samdani, and Dan Roth. 2013.
A Constrained Latent Variable Model for Coreference
Resolution. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Processing,
pages 601–612.

Kevin Clark and Christopher D. Manning. 2015. Entity-
centric coreference resolution with model stacking. In
Proceedings of the 53rd Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pages
1405–1415.

Aron Culotta, Michael Wick, Robert Hall, and Andrew
McCallum. 2007. First-order Probabilistic Models for
Coreference Resolution. In Human Language Tech-
nology Conference of the North American Chapter of
the Association of Computational Linguistics (NAACL
HLT).

Hal Daumé III, John Langford, and Daniel Marcu. 2009.
Search-based structured prediction. Machine Learn-
ing, 75(3):297–325.

Pascal Denis and Jason Baldridge. 2008. Special-
ized Models and Ranking for Coreference Resolution.
In Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, pages 660–
669. Association for Computational Linguistics.

John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive Subgradient Methods for Online Learning
and Stochastic Optimization. The Journal of Machine
Learning Research, 12:2121–2159.

Greg Durrett and Dan Klein. 2013. Easy Victories and
Uphill Battles in Coreference Resolution. In Proceed-
ings of the 2013 Conference on Empirical Methods in
Natural Language Processing, pages 1971–1982.

Greg Durrett and Dan Klein. 2014. A Joint Model
for Entity Analysis: Coreference, Typing, and Link-
ing. Transactions of the Association for Computa-
tional Linguistics, 2:477–490.

Chris Dyer, Miguel Ballesteros, Wang Ling, Austin
Matthews, and Noah A. Smith. 2015. Transition-
based dependency parsing with stack long short-term
memory. In Proceedings of the 53rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 334–343.

Eraldo Rezende Fernandes, Cı́cero Nogueira Dos Santos,
and Ruy Luiz Milidiú. 2012. Latent Structure Per-
ceptron with Feature Induction for Unrestricted Coref-
erence Resolution. In Joint Conference on EMNLP
and CoNLL-Shared Task, pages 41–48. Association
for Computational Linguistics.

Aria Haghighi and Dan Klein. 2010. Coreference Res-
olution in a Modular, Entity-centered Model. In The
2010 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 385–393. Association for Computational Lin-
guistics.

Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long
short-term memory. Neural Comput., 9:1735–1780.

Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
the 90% Solution. In Proceedings of the human lan-
guage technology conference of the NAACL, Compan-
ion Volume: Short Papers, pages 57–60. Association
for Computational Linguistics.

Philipp Koehn. 2004. Statistical Significance Tests for
Machine Translation Evaluation. In Proceedings of
the 2004 Conference on Empirical Methods in Natu-
ral Language Processing, pages 388–395. Citeseer.

Jonathan K. Kummerfeld and Dan Klein. 2013. Error-
driven Analysis of Challenges in Coreference Reso-
lution. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Processing,
Seattle, WA, USA, October.

Nicholas Léonard, Yand Waghmare, Sagar ad Wang, and
Jin-Hwa Kim. 2015. rnn: Recurrent Library for
Torch. arXiv preprint arXiv:1511.07889.

Jiwei Li, Xinlei Chen, Eduard Hovy, and Dan Jurafsky.
2016. Visualizing and understanding neural models in
nlp. In NAACL HLT.

Xiaoqiang Luo, Sameer Pradhan, Marta Recasens, and
Eduard Hovy. 2014. An Extension of BLANC to Sys-
tem Mentions. Proceedings of ACL, Baltimore, Mary-
land, June.

Sebastian Martschat and Michael Strube. 2015. Latent
structures for coreference resolution. TACL, 3:405–
418.

1003



Sebastian Martschat, Thierry Göckel, and Michael
Strube. 2015. Analyzing and visualizing coreference
resolution errors. In NAACL HLT, pages 6–10.

Andrew McCallum and Ben Wellner. 2003. Toward
Conditional Models of Identity Uncertainty with Ap-
plication to Proper Noun Coreference. Advances in
Neural Information Processing Systems 17.

Vincent Ng and Claire Cardie. 2002. Identifying
Anaphoric and Non-anaphoric Noun Phrases to Im-
prove Coreference Resolution. In Proceedings of
the 19th international conference on Computational
linguistics-Volume 1, pages 1–7. Association for Com-
putational Linguistics.

Haoruo Peng, Kai-Wei Chang, and Dan Roth. 2015. A
joint framework for coreference resolution and men-
tion head detection. In Proceedings of the 19th Con-
ference on Computational Natural Language Learning
(CoNLL), pages 12–21.

Hoifung Poon and Pedro M. Domingos. 2008. Joint un-
supervised coreference resolution with markov logic.
In 2008 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 650–659.

Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. Conll-
2012 Shared Task: Modeling Multilingual Unre-
stricted Coreference in OntoNotes. In Joint Confer-
ence on EMNLP and CoNLL-Shared Task, pages 1–40.
Association for Computational Linguistics.

Sameer Pradhan, Xiaoqiang Luo, Marta Recasens, Ed-
uard Hovy, Vincent Ng, and Michael Strube. 2014.
Scoring Coreference Partitions of Predicted Mentions:
A Reference Implementation. In Proceedings of the
Association for Computational Linguistics.

Altaf Rahman and Vincent Ng. 2009. Supervised Mod-
els for Coreference Resolution. In Proceedings of
the 2009 Conference on Empirical Methods in Natu-
ral Language Processing: Volume 2-Volume 2, pages
968–977. Association for Computational Linguistics.

Altaf Rahman and Vincent Ng. 2011. Narrowing the
modeling gap: A cluster-ranking approach to corefer-
ence resolution. J. Artif. Intell. Res. (JAIR), 40:469–
521.

Stéphane Ross, Geoffrey J. Gordon, and Drew Bagnell.
2011. A reduction of imitation learning and structured
prediction to no-regret online learning. In Proceedings
of the Fourteenth International Conference on Artifi-
cial Intelligence and Statistics, pages 627–635.

Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong
Lim. 2001. A Machine Learning Approach to Coref-
erence Resolution of Noun Phrases. Computational
Linguistics, 27(4):521–544.

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.

Dropout: A simple way to prevent neural networks
from overfitting. The Journal of Machine Learning
Research, 15(1):1929–1958.

Veselin Stoyanov and Jason Eisner. 2012. Easy-first
Coreference Resolution. In COLING, pages 2519–
2534. Citeseer.

Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. 2014.
Sequence to sequence learning with neural networks.
In Advances in Neural Information Processing Sys-
tems (NIPS), pages 3104–3112.

Laurens van der Maaten and Geoffrey E. Hinton. 2012.
Visualizing non-metric similarities in multiple maps.
Machine Learning, 87(1):33–55.

Sam Wiseman, Alexander M. Rush, Stuart M. Shieber,
and Jason Weston. 2015. Learning anaphoricity and
antecedent ranking features for coreference resolution.
In Proceedings of the 53rd Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pages
1416–1426.

Chun-Nam John Yu and Thorsten Joachims. 2009.
Learning Structural SVMs with Latent Variables. In
Proceedings of the 26th Annual International Confer-
ence on Machine Learning, pages 1169–1176. ACM.

Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals.
2014. Recurrent neural network regularization.
CoRR, abs/1409.2329.

1004


