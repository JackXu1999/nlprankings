



















































A Concrete Chinese NLP Pipeline


Proceedings of NAACL-HLT 2015, pages 86–90,
Denver, Colorado, May 31 – June 5, 2015. c©2015 Association for Computational Linguistics

A Concrete Chinese NLP Pipeline

Nanyun Peng, Francis Ferraro, Mo Yu, Nicholas Andrews,
Jay DeYoung, Max Thomas, Matthew R. Gormley, Travis Wolfe,

Craig Harman, Benjamin Van Durme, Mark Dredze
Human Language Technology Center of Excellence

Johns Hopkins University, Baltimore, Maryland USA

Abstract

Natural language processing research increas-
ingly relies on the output of a variety of syn-
tactic and semantic analytics. Yet integrating
output from multiple analytics into a single
framework can be time consuming and slow
research progress. We present a CONCRETE
Chinese NLP Pipeline: an NLP stack built
using a series of open source systems inte-
grated based on the CONCRETE data schema.
Our pipeline includes data ingest, word seg-
mentation, part of speech tagging, parsing,
named entity recognition, relation extraction
and cross document coreference resolution.
Additionally, we integrate a tool for visualiz-
ing these annotations as well as allowing for
the manual annotation of new data. We release
our pipeline to the research community to fa-
cilitate work on Chinese language tasks that
require rich linguistic annotations.

1 Introduction

Over the past few years, the natural language pro-
cessing community has shifted its attention towards
the Chinese language, with numerous papers cover-
ing a range of NLP tasks for Chinese. Last year’s
EMNLP and ACL alone featured two dozen papers
focused primarily on Chinese data1, not including
many others that considered Chinese language data
within a broader context. The large number of Chi-
nese speakers, coupled with the unique challenges
of Chinese compared to well studied Romance and

1Excluding the Chinese Restaurant Process.

Germanic languages, have driven these research ef-
forts. This focus has given rise to new NLP sys-
tems that enable the automated processing of Chi-
nese data. While some pipelines cover multiple
tasks, such as Stanford CoreNLP (Manning et al.,
2014), other tasks such as relation extraction are not
included.

Modern NLP research, including research fo-
cused on Chinese, often relies on automatically pro-
duced analytics, or annotations, from multiple stages
of linguistic analysis. Downstream systems, such as
sentiment analysis and question answering, assume
that data has been pre-processed by a variety of syn-
tactic and semantic analytics. Consider the task of
knowledge base population (KBP), in which infor-
mation is extracted from text corpora for inclusion
in a knowledge base. Associated information ex-
traction systems rely on various NLP analytics run
on the data of interest, such as relation extractors
that require the identification of named entities and
syntactically parsed text. Similarly, entity linking
typically assumes the presence of within document
coreference resolution, named entity identification
and relation extraction. These analytics themselves
rely on other core NLP systems, such as part of
speech tagging and syntactic parsing.

While each of these tasks have received exten-
sive attention and have associated research software
for producing annotations, the output of these com-
ponents must be integrated into a single cohesive
framework for use in a downstream task. This inte-
gration faces a wide variety of challenges resulting
from the simple fact that most research systems are
designed to produce good performance on an eval-

86



uation metric, but are not designed for integration
in a pipeline. Beyond the production of integrated
NLP pipelines, research groups often produce re-
sources of corpora annotated by multiple systems,
such as the Annotated Gigaword Corpus (Napoles
et al., 2012). Effective sharing of these corpora re-
quires a common standard.

These factors lead to the recent development of
CONCRETE, a data schema that represents numerous
types of linguistic annotations produced by a variety
of NLP systems (Ferraro et al., 2014). CONCRETE
enables interoperability between NLP systems, fa-
cilitates the development of large scale research sys-
tems, and aids sharing of richly annotated corpora.

This paper describes a Chinese NLP pipeline that
ingests Chinese text to produce richly annotated
data. The pipeline relies on existing Chinese NLP
systems that encompass a variety of syntactic and
semantic tasks. Our pipeline is built on the CON-
CRETE data schema to produce output in a struc-
tured, coherent and shareable format. To be clear,
our goal is not the development of new methods or
research systems. Rather, our focus is the integra-
tion of multiple tools into a single pipeline. The ad-
vantages of this newly integrated pipeline lie in the
fact that the components of the pipeline communi-
cate through a unified data schema: CONCRETE. By
doing so, we can

• easily switch each component of the pipeline to
any state-of-the-art model;

• keep several annotations of the same type gen-
erated by different tools; and

• easily share the annotated corpora.

Furthermore, we integrate a visualization tool for
viewing and editing the annotated corpora. We posit
all the above benefits as the contributions of this pa-
per and hope the efforts can facilitate ongoing Chi-
nese focused research and aid in the construction
and distribution of annotated corpora. Our code is
available at http://hltcoe.github.io.

2 The CONCRETE Data Schema

We use CONCRETE, a recently introduced data
schema designed to capture and layer many differ-

ent types of NLP output (Ferraro et al., 2014).2 A
primary purpose of CONCRETE is to ease analytic
pipelining. Based on Apache Thrift (Slee et al.,
2007), it captures NLP output via a number of inter-
working structs, which are translated automatically
into in-memory representations for many common
programming languages, including Java, C++ and
Python. In addition to being, in practice, language-
agnostic, CONCRETE and Thrift try to limit pro-
grammer error: Thrift generates I/O libraries, mak-
ing it easy for analytics to read and write CON-
CRETE files; with this common format and I/O li-
braries, developers can more easily share NLP out-
put. Unlike XML or JSON, Thrift’s automatic val-
idation of strongly typed annotations help ensure
legitimate annotations: developers cannot acciden-
tally populate a field with the wrong type of object,
nor must they manually cast values.

CONCRETE allows both within-document and
cross-document annotations. The former includes
standard tagging tasks (e.g., NER or POS), syn-
tactic parses, relation extraction and entity corefer-
ence, though Ferraro et al. (2014) show how CON-
CRETE can capture deeper semantics, such as frame
semantic parses and semantic roles. These within-
document annotations, such as entity coref, can form
the basis of cross-document annotations.

We chose CONCRETE as our data schema to sup-
port as many NLP analytics as possible. In the
future, we plan to add additional analytics to our
pipeline, and we expect other research groups to in-
tegrate their own tools. A flexible and well docu-
mented data schema is critical for these goals. Fur-
thermore, the release of multiple corpora in CON-
CRETE (Ferraro et al., 2014) support our goal of
facilitating the construction and distribution of new
Chinese corpora.

3 Analytic Pipeline

We describe each stage of our pipeline with a brief
description of the associated tool and relevant details
of its integration into the pipeline.

2CONCRETE, language interfaces, and utility libraries are
open-source projects (https://hltcoe.github.io/).

87



(a) The basic visualization of a Communication.
Each line is a tokenized sentence, with options to view
the part of speech, constituency and dependency parse,
and entity relation information.

(b) Multiple types of annotations can be viewed simul-
taneously. Here, entity information is laid atop a depen-
dency parse. A particular mention-of-interest is shown
in yellow, with all other mentions in pink.

Figure 1: CONCRETE Communication containing Chinese text displayed in Quicklime (section 3.7).

3.1 Data Ingest

The first stage of our pipeline requires in-
gesting existing Chinese text into CONCRETE
Communication objects, the core document rep-
resentation of CONCRETE. The existing CONCRETE
Java and Python utilities support ingesting raw text
files. Part of this process requires not only ingesting
the raw text, but identifying section (paragraph) and
sentence boundaries.

Not all corpora contain raw text, as many corpora
come with existing manual (or automatic) linguis-
tic annotations. We provide code to support two
data formats of existing Chinese corpora: the Chi-
nese ACE 2005 relation extraction dataset (Walker
et al., 2006) and the new Chinese Entities, Rela-
tions, and Events (ERE) dataset (Consortium, 2013).
Both data sets include annotations for entities and
a variety of relations (Aguilar et al., 2014). The
labeled entities and relations are represented by
CONCRETE EntityMentions and stored in a
EntityMentionSetList. Additional annota-
tions that are typically utilized by relation extraction
systems, such as syntactic parses, are provided auto-
matically by the pipeline.

3.2 Word Segmentation

Chinese text processing requires the identification of
word boundaries, which are not indicated in writ-
ten Chinese as they are in most other languages.
Our word segmentation is provided by the Stan-
ford CoreNLP3 (Manning et al., 2014) Chinese
word segmentation tool, which is a conditional ran-
dom field (CRF) model with character based fea-
tures and lexicon features according to Chang et al.
(2008). Word segmentations decisions are repre-
sented by CONCRETE Token objects and stored in
the TokenList. We follow the Chinese Penn Tree-
bank segmentation standard (Xue et al., 2005). Our
system tracks token offsets so that segmentation is
robust to unexpected spaces or line breaks within a
Chinese word.

3.3 Syntax

Part of speech tagging and syntactic parsing are also
provided by Stanford CoreNLP. The part of speech
tagger is based on Toutanova et al. (2003) adapted
for Chinese, which is a log-linear model under-
neath. Integration with CONCRETE was facilitated
by the concrete-stanford library 4, though support-
ing Chinese required significant modifications to the

3
http://nlp.stanford.edu/software/corenlp.shtml

4
https://github.com/hltcoe/concrete-stanford

88



library. Resulting tags are stored in a CONCRETE
TokenTaggingList.

Syntactic constituency parsing is based on the
model of Klein and Manning (2003) adapted
for Chinese. We obtained dependency parses
from the CoreNLP dependency converter. We
store the constituency parses as a CONCRETE
Parse, and the dependency analyses as CON-
CRETE DependencyParses.

3.4 Named Entity Recognition

We support the two most common named entity an-
notation standards: the CoNLL standard (four types:
person, organization, location and miscellaneous),
and the ACE standard, which includes the additional
types of geo-political entity, facility, weapon and ve-
hicle. The ACE standard also includes support for
nested entities. We used the Stanford CoreNLP NER
toolkit which is a CRF model based on the method
in Finkel et al. (2005), plus features based on Brown
clustering. For the CoNLL standard annotations, we
use one CRF model to label all the four types of en-
tities. For the ACE standard annotations, in order
to deal with the nested cases, we build one tagger
for each entity type. Each entity is stored in a CON-
CRETE EntityMention.

3.5 Relation Extraction

Relations are extracted for every pair of entity men-
tions. We use a log-linear model with both tra-
ditional hand-crafted features and word embedding
features. The hand-crafted features include all the
baseline features of Zhou et al. (2005) (excluding the
Country gazeteer and WordNet features), plus sev-
eral additional carefully-chosen features that have
been highly tuned for ACE-style relation extrac-
tion over years of research (Sun et al., 2011). The
embedding-based features are from Yu et al. (2014),
which represent each word as the outer product be-
tween its word embedding and a list of its asso-
ciated non-lexical features. The non-lexical fea-
tures indicate the word’s relative positions compar-
ing to the target entities (whether the word is the
head of any target entity, in-between the two enti-
ties, or on the dependency path between entities),
which improve the expressive strength of word em-
beddings. We store the extracted relations in CON-
CRETE SituationMentions. See Figure 2 for

Figure 2: ACE entity relations viewed through
Quicklime (Section 3.7).

an example visualization.

3.6 Cross Document Coreference Resolution

Cross document coreference resolution is performed
via the phylogenetic entity clustering model of
Andrews et al. (2014).5 Since the method is
fully unsupervised we did not require a Chinese
specific model. We use this system to cluster
EntityMentions and store the clustering in top
level CONCRETE Clustering objects.

3.7 Creating Manual Annotations

Quicklime6 is a browser-based tool for viewing and
editing NLP annotations stored in a CONCRETE
document. Quicklime supports a wide array of ana-
lytics, including parse trees, token taggings, entities,
mentions, and “situations” (e.g. relations.) Quick-
lime uses the visualization layer of BRAT (Stenetorp
et al., 2012) to display some annotations but does not
use the BRAT annotation editing layer. BRAT anno-
tations are stored in a standoff file format, whereas
Quicklime reads and writes CONCRETE objects us-
ing the Thrift JavaScript APIs. Figure 1 shows
Quicklime displaying annotations on Chinese data.
In particular, Quicklime can combine and overlay
multiple annotations, such as entity extraction and
dependency parses, as in Figure 1b. Figure 2 shows
entity relation annotations.

Acknowledgments

We would like to thank the reviewers for their help-
ful comments and perspectives. A National Science
Foundation Graduate Research Fellowship, under

5
https://bitbucket.org/noandrews/phyloinf

6
https://github.com/hltcoe/quicklime

89



Grant No. DGE-1232825, supported the second au-
thor. Any opinions expressed in this work are those
of the authors.

References
Jacqueline Aguilar, Charley Beller, Paul McNamee, and

Benjamin Van Durme. 2014. A comparison of
the events and relations across ace, ere, tac-kbp, and
framenet annotation standards. ACL 2014, page 45.

Nicholas Andrews, Jason Eisner, and Mark Dredze.
2014. Robust entity clustering via phylogenetic infer-
ence. In Association for Computational Linguistics.

Pi-Chuan Chang, Michel Galley, and Christopher D Man-
ning. 2008. Optimizing chinese word segmentation
for machine translation performance. In Third Work-
shop on Statistical Machine Translation.

Linguistic Data Consortium. 2013. DEFT ERE annota-
tion guidelines: Events v1.1.

Francis Ferraro, Max Thomas, Matthew R. Gormley,
Travis Wolfe, Craig Harman, and Benjamin Van
Durme. 2014. Concretely Annotated Corpora. In
AKBC Workshop at NIPS.

Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local information
into information extraction systems by gibbs sampling.
In ACL.

Dan Klein and Christopher D Manning. 2003. Accurate
unlexicalized parsing. In ACL.

Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David McClosky.
2014. The Stanford CoreNLP natural language pro-
cessing toolkit. In ACL: Demos.

Courtney Napoles, Matthew Gormley, and Benjamin Van
Durme. 2012. Annotated gigaword. In AKBC-
WEKEX Workshop at NAACL 2012.

Mark Slee, Aditya Agarwal, and Marc Kwiatkowski.
2007. Thrift: Scalable cross-language services imple-
mentation. Facebook White Paper.

Pontus Stenetorp, Sampo Pyysalo, Goran Topić, Sophia
Ananiadou, and Akiko Aizawa. 2012. Normalisation
with the brat rapid annotation tool. In International
Symposium on Semantic Mining in Biomedicine.

Ang Sun, Ralph Grishman, and Satoshi Sekine. 2011.
Semi-supervised relation extraction with large-scale
word clustering. In ACL.

Kristina Toutanova, Dan Klein, Christopher D Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In NAACL.

Christopher Walker, Stephanie Strassel, Julie Medero,
and Kazuaki Maeda. 2006. Ace 2005 multilingual
training corpus ldc2006t06. Linguistic Data Consor-
tium.

Naiwen Xue, Fei Xia, Fu-Dong Chiou, and Marta Palmer.
2005. The penn chinese treebank: Phrase structure an-
notation of a large corpus. Natural language engineer-
ing, 11(02):207–238.

Mo Yu, Matthew Gormley, and Mark Dredze. 2014.
Factor-based compositional embedding models. In
NIPS Workshop on Learning Semantics.

GuoDong Zhou, Jian Su, Jie Zhang, and Min Zhang.
2005. Exploring various knowledge in relation extrac-
tion. In ACL, pages 427–434.

90


