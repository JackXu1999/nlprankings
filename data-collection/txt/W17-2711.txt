



















































The Event StoryLine Corpus: A New Benchmark for Causal and Temporal Relation Extraction


Proceedings of the Events and Stories in the News Workshop, pages 77–86,
Vancouver, Canada, August 4, 2017. c©2017 Association for Computational Linguistics

The Event StoryLine Corpus: A New Benchmark for Causal and
Temporal Relation Extraction

Tommaso Caselli and Piek Vossen
Vrije Universiteit Amsterdam

De Boelelaan 1105 1081 HV Amsterdam (NL)
{t.caselli;p.t.j.m.vossen}@vu.nl

Abstract

This paper reports on the Event StoryLine
Corpus (ESC) v0.9, a new benchmark
dataset for the temporal and causal relation
detection. By developing this dataset, we
also introduce a new task, the StoryLine
Extraction from news data, which aims
at extracting and classifying events rele-
vant for stories, from across news docu-
ments spread in time and clustered around
a single seminal event or topic. In addi-
tion to describing the dataset, we also re-
port on three baselines systems whose re-
sults show the complexity of the task and
suggest directions for the development of
more robust systems.

1 Introduction

Humans have an appetite for information to ex-
plain the things they observe. Our minds con-
stantly mine the present for cues, merge this with
information from the past, and derive models for
reasoning and taking decisions. It is by means of
such explanatory patterns, and by extension of ex-
planatory relations among entities and events, that
we understand the changing world.

The current stream of information poses a big
challenge both to humans and systems to extract,
organize, and represent events and their relations.
News aggregation systems can easily monitor the
burst and the development of a topic, or news
story, but they fail in providing a content-based
analysis. Given a topic or trending story, people
still have to read the documents and reconstruct a
unitary and coherent report mentally. Current NLP
systems can identify complex information but they
lack a method to connect it in a unitary and co-
herent message. Steps in this direction have been
conducted but are very limited and do not cover

the full story that is told by these documents (e.g.
the textual entailment task, or script extraction).

Monitoring a news story from its beginning to
end is a challenging task, which requires systems
to be able to: 1) reconcile information from differ-
ent sources distributed in time; 2) resolve dedupli-
cation of information; and 3) extract informative
semantic structures.

It is surprising to observe how humans can per-
form these tasks with relative little effort. It has
been suggested that this capacity is partly based
on narrative strategies (Boyd, 2009; Gottschall,
2012). Such a structuring is possible thanks to
a key component of narratives, the plot struc-
ture (Bal, 1997), which provides a chronological
and logical ordering of events. This means that
events are not simply ordered in time but they
are selected and connected in such a way that
their relations are meaningful, i.e., they give rise
to a network of explanatory relations. Access-
ing and reconstructing plot structures for differ-
ent topics would be beneficial for lots of Natu-
ral Language Understanding applications (ques-
tion answering, summarization, co-reference res-
olution, event processing, and script extraction,
among others).

One of the necessary step for a StoryLine Ex-
traction task is to decide on a corpus to evaluate
performance of systems. This paper presents such
as resource: the Event StoryLine Corpus v0.9,
specifically designed for the evaluation of systems
aiming at reconstructing event-centric plot struc-
tures. The resource is still being extended with
new annotated texts, but in the remainder of the
paper we will refer to this first version. The cor-
pus has been developed by applying annotation
guidelines designed to mark-up the network of
explanatory relations which can be realized be-
tween pairs of events in a document belonging
to a specific topic. Furthermore, the guidelines

77



are compliant with other initiatives for event an-
notation: temporal processing (TimeML (Puste-
jovsky et al., 2003a) and Richer Event Descrip-
tion (RED) (O’Gorman et al., 2016)), event co-
reference (Event Coreference Bank+ (ECB+) (Cy-
bulska and Vossen, 2014b)), and causal re-
lations (Causal-TimeBank (Mirza and Tonelli,
2016), BECauSE (Dunietz et al., 2015), ROCSto-
ries (Mostafazadeh et al., 2016b) among others).

The remainder of the paper is structured as
follows: Section 2 will explain the annotation
scheme, describe the annotation layers of the
Event StoryLines Corpus (ESC) v0.9, and report
on agreement measures. Section 3 will describe
experiments related to the development of base-
lines for the StoryLine Extraction task. In Sec-
tion 4 a review of previous annotation initiatives
is given, showing differences and commonalities
between them and the ESC data. Finally, conclu-
sions and future work are reported in Section 5.
The annotated data, the evaluation scripts, and the
baselines models are publicly available. 1

2 The Event StoryLine Corpus v0.9

The primary goal of the ESC v0.9 dataset is to
provide an intrinsic evaluation benchmark for the
event-centric StoryLine Extraction task. The task
can be best described as a combination of three
basic subtasks:

• Event Detection and Classification Identify
and classify events in each document which
compose a topic, or a seminal event;

• Temporal Anchoring of Events Anchor
each event mention to the temporal expres-
sion expressing the time of its happening,
as well as to the Document Creation Time
(DCT);

• Explanatory Relation Identification and
Classification Select event pairs which are
temporally and logically connected, and then,
classify the storyline relation type.

A storyline relation can be best described as a
loose causal and temporal relation between a pair
of event mentions, where one event mention ex-
plains/justifies the occurrence of the other event
mention in the pair (more details are reported in

1https://github.com/cltl/
EventStoryLine.git

Section 2.3). Relations can be classified either as
rising action, or falling action.

An additional task is Event Co-reference Res-
olution, which aims at identifying co-referential
chains of events mentions both at within- and
cross-document levels. The availability of this
information allows us to deduplicate information
across event mentions by creating event instances,
i.e. formal semantic representation in RDF com-
pliant URIs that may integrate linguistic informa-
tion with external resources, and thus, allow rea-
soning (Fokkens et al., 2013).2 In the following
sections, we will illustrate the components of the
ESC Annotation Scheme and its annotation frame-
work.

2.1 Basic Components: Events and Temporal
Expressions

Events and temporal expressions are the basic
components of the annotation scheme for the ESC
v0.9 dataset.

The term “event” is used as a cover term to re-
fer to any situations that can happen, occur, or
hold. The use of the term event is a synonym to
“eventuality” introduced by Bach (1986), cover-
ing both dynamic and static situations (i.e. events
and states). The annotation of events in NLP is
a topic that got a lot of interest and on which yet
no consensus has been reached. In this work, we
adopted a definition of events that is provided in
the ECB+ Annotation Guidelines (Cybulska and
Vossen, 2014a), which is compatible with defini-
tions in ACE (Linguistic Data Consortium, 2005)
and TimeML. In particular, an event is any punc-
tual, durational, or stative situation which happens
or holds, and which results from a combination of
four components such as: 1) an action component
referring to what happens or holds; 2) a time slot
which is responsible for anchoring the action in
time ; 3) a location component which links the ac-
tion component to a place/location; and 4) a par-
ticipant component, which illustrates the “who”
or “what” is involved in the action component.

The annotation of the extent of events in ECB+
follows the solution adopted in TimeML. This
means that for each event mention, regardless of
its part-of-speech, only the lexical item which is
the bearer of the action meaning is annotated. This
normally corresponds to the head of the phrase

2http://groundedannotationframework.
org/files/2013/05/GAF_Poster.pdf

78



realizing the action component, i.e. the minimal
chunk, as illustrated in the following example3.
Annotated events are in bold.

1. This terrible war could have ended in a
month

However, exceptions to this rule apply. Adopt-
ing an event-centric annotation framework, ad-
herence to the text surface is not always main-
tained. For instance, cases of historically signif-
icant events which may be referred to with proper
nouns, such as World War II, the American Civil
War, are annotated with a unique action compo-
nent tag. Similarly, as the annotation is also pri-
marily focused towards event co-reference, pre-
modifiers of events can be included in the action
component tag any time they contribute to the
identification of a unique event instance:

2. 6.1-magnitude quake strikes Indonesia’s
Aceh.

Furthermore, ECB+ allows the annotation of
present- and past-participles in modifier position
as event mentions:

3. The earthquake [. . . ] left hundred trapped in
collapsed buildings.

Each action component is classified as be-
longing to one of seven possible classes.
Five of them, ACTION OCCURRENCE, AC-
TION ASPECTUAL, ACTION REPORTING, AC-
TION STATE, and ACTION PERCEPTION, mir-
ror TimeML classes. The two additional classes
, ACTION CAUSATIVE and ACTION GENERIC,
have been introduced to annotate events express-
ing casual relations, and events which are not an-
chored to a specific time and location expressing
generic actions (i.e. event mentions whose truth-
fulness is independent of the specific moment of
utterance).

Temporal expression mark-up is inherited from
TimeML following the TIMEX3 annotation guide-
lines. We modified the original ECB+ annota-
tion guidelines to be compatible with the TIMEX3
TimeML ones by: 1) using the TIMEX3 tag to an-
notate temporal expressions, 2) re-introducing the
type attribute as part of the temporal expression
tag; 3) re-introducing the attribute value for tem-
poral expressions’ normalization. We also allow

3All examples are taken from the ECB+ Annotation
Guidelines or the ECB+ annotated data

the creation of empty TIMEX3 tag, i.e. non-text
consuming temporal expression markables corre-
sponding to implicit, i.e. not realized in the text,
beginning and/or end points of temporal expres-
sions denoting a duration. In addition to this, tem-
poral expressions which have been included in ac-
tion tags as part of the action component descrip-
tion must be annotated also as independent tem-
poral expressions. This means that we allow mul-
tiple annotations on overlapping tokens over dif-
ferent text expressions. We made this choice be-
cause these temporal expressions in most cases
also function as temporal anchor of the event com-
ponent.

2.2 Temporal Anchoring of Events (TLINKs)

Temporal information plays an essential role for
StoryLine Extraction. At the same time, the anno-
tation of temporal relations is by no means a trivial
task.

Two types of temporal relations can be identi-
fied: 1) ordering relations, which involve elements
of the same ontological type, e.g. pairs of events
or temporal expressions; and 2) anchoring rela-
tions, which involve cross-type element relations,
e.g. pairs of event and related temporal expres-
sion. Although both types of temporal relations
are useful, they have different informational sta-
tus. Following Pustejovsky and Stubbs (2011), we
assume that the informational level of a temporal
relation can be expressed as a function of the infor-
mation contained in each temporal link and their
closure. Under this assumption, anchoring rela-
tions expressing when an event mention occurred
or its duration, are more informative than ordering
relations. The former allow us to put event men-
tions on a specific point (or interval) on an imagi-
nary timeline and, as a consequence, also gives us
the ordering relations between event mentions.

The ESC Annotation Scheme expresses tempo-
ral relations using the TimeML TLINK tag and re-
stricts them to anchoring relations. TLINKs be-
tween an event mention and a temporal expression
are systematically annotated when an anchoring
relation is instantiated. Anchoring relations may
hold between an event mention and a temporal
expression at intra- and inter-sentential levels In
addition to this, each event mention is also con-
nected to the Document Creation Time (DCT) of
each document.

Limiting the annotation to anchoring relations

79



is also a strategy to avoid the complexity of order-
ing relations between events. Most of the current
solutions are not optimal, as they give the annota-
tors too much freedom in the the selection of the
event pairs (e.g. TimeML), or force the annota-
tors to mark all possible relations (e.g. TimeBank-
Dense (Cassidy et al., 2014)), or limit the annota-
tions to the presence of explicit linguistic evidence
(e.g. RED).

The temporal values in ESC are derived from
the RED guidelines. We apply two sets of
TLINK values according to the type of anchor-
ing relation annotated: four values apply for rela-
tions between events and DCTs (namely before,
after, overlap, and contains), while only
one value (contains) applies to relations be-
tween events and temporal expressions. Annota-
tors are also instructed on the directionality of the
TLINK, which should always go from the temporal
expression, or DCT, to the target event.

2.3 Explanatory Relation Annotation
(PLOT LINKs)

The annotation of explanatory relations between
event pairs is encoded in the PLOT LINK tag, fol-
lowing a previous proposal described in Caselli
and Vossen (2016). PLOT LINKs are specifically
designed to capture the semantics of plot struc-
tures.

PLOT LINK annotation is conducted in two
steps: first, annotators have to identify all eligible
relations between event pairs, and then they have
to classify each relation as belonging to one of the
two classes: rising action, events which are
circumstantial to, cause or enable another event, or
falling action, which explicitly mark spec-
ulations and consequences, i.e. events which are
the (anticipated) outcome or the effect of another
event.

PLOT LINKs are related to causal and tempo-
ral relation annotation (Miltsakaki et al., 2004;
Bethard et al., 2008; Mirza and Tonelli, 2014;
Dunietz et al., 2015), but they differ in three ways:
1) they include the standard causal relations, i.e.
cause, enablement, and prevention, but also addi-
tional event-event relations such as contingency,
sub-event, entailment, and co-participation rela-
tions; 2) they are often not explicitly marked in the
text through a relational structure; and 3) they are
more specific than all events that stand in a tempo-
ral relation as they add explanatory information.

PLOT LINKs can be positioned in between
temporal and causal annotations by overcoming
current shortcomings, such as creation of uninfor-
mative pairs of events, in the former case, and an
extremely limited annotation in the latter, i.e. pres-
ence of an explicit causality trigger. Each pair of
events in a PLOT LINK relation is basically help-
ing the reader (and the machine) to connect events
in a meaningful way. In a nutshell, PLOT LINKs
aim at answering “why” something has happened.
Given their event-centric nature, the answer to
such a question must be another event mention ex-
plicitly stated in the document in analysis.

PLOT LINK relations are asymmetrical and
non-transitive. Non-transitivity is justified by con-
sidering the nature of this type of relations. They
apply at a local level of analysis between pairs of
events, and cannot be transferred to a global level,
i.e. inherited by the full chain of event mentions
which contribute to the identification of a story-
line. Although subjected to the chronological or-
der of events, this type of relations aims at making
explicit the coherence, or logical connections, of
the events in a (news) story.

When annotating PLOT LINKs, the (broad)
“causal” dimension of the relation is more promi-
nent than the temporal aspect. We are not filling-
up a timeline, where the axiom of the Inter-
nal Directionality of Time4 (Bonomi and Zuc-
chi, 2001) holds, but we are looking for ex-
planations of “why” events happened, accord-
ing to the information that we are given in the
document of analysis. Thus, in example 4,
the relation between the events “earthquake” and
“trapped” is obtained by answering the question
“why were people trapped?” and not by means
of transitive relation between the pairs earth-
quake rising action collapsed and collapsed
rising action trapped.

4. The earthquake killed 14 and left hundred
trapped in collapsed buildings.
earthquake rising action killed
earthquake rising action trapped
earthquake rising action collapsed
collapsed rising action trapped

Annotators are free to identify the pairs of
events which may stand in a PLOT LINK rela-

4Internal Directionality of Time: if it is true of my current
position in time, t, that the event e occurred in the past of t,
then it is true of any future position t′ that e is in the past of t

80



tion. We did not create a predefined set of pairs
of events which may stand in a plot link, as
in the TimeBank-Dense corpus, as this will re-
quire to create a really large graph between all
events occurring both in the same sentence and
across all sentences. However, we limited the an-
notation of PLOT LINKs to events which corre-
spond to one of the following three classes: AC-
TION OCCURRENCE, ACTION PERCEPTION,
ACTION STATE. We label those events as “se-
mantically full” or “semantically loaded” events.
Event mentions in these classes do have a con-
tent component describing a situation, rather than
expressing meta-level information on the events.5

The class of ACTION REPORTING is excluded
as well. In this case, the meaningful information
is represented by the “content” of a speech event
rather than by the lexical expression that intro-
duces it. This choice guarantees that only mean-
ingful events are part of a storyline.

Finally, PLOT LINKs also allow the annotation
of explicit causal relations between pairs of events.
Two binary attributes, cause and caused by,
must be selected in presence of explicit causal
relations. Explicit causal relations are intro-
duced either by ACTION CAUSATIVE events, or
causal signals such as conjunctions (e.g. because),
prepositions (e.g. by, from, for, among others),
and other connectives. An additional attribute,
signal, has been created to annotate the “mark-
ers” of the causal relation. At this stage of de-
velopment, the attribute is filled only when AC-
TION CAUSATIVE events are used to signal the
presence of a casual relation:

5. A massive quake struck off Aceh in 2004 ,
sparking a tsunami.
quake rising action tsunami
signal= sparking
cause = YES

2.4 Event Co-reference

Currently, the annotation of co-referential chains
among event mentions has been inherited from
ECB+ The ECB+ guidelines consider two event
mentions, either in the same document or across

5We consider event mentions contributing to the as-
sessment of the factuality profiling of an event mention,
including cognitive events, events belonging to the class
ACTION ASPECTUAL, which functions as lexical morpho-
syntactic markers of the the internal temporal structure of
a situation, and ACTION CAUSATIVE as meta-level event
mentions, and thus they are excluded.

documents, as co-referential when they refer to the
same event instance, i.e. if they describe the same
action component, and 1.) share the same partic-
ipants; 2) share the same temporal anchor; and 3)
share the same location.

2.5 Data

The ESC v0.9 dataset is currently composed by 22
topics from the ECB+ corpus concerning calamity
events, i.e. natural disasters, shootings, killings,
accidents, and trials, among others.

The corpus contains 258 documents, and a to-
tal of 7,275 event mentions (191 of which being
negated mentions).6 A total of 1,297 temporal ex-
pressions are present, 248 of them corresponds to
DCTs, of which 22 are realized by empty TIMEX3
tags. In the remainder of the cases, 10 articles, it
was not possible to recover a DCT, neither from
the articles, nor by searching the Web.

Following the extended anchoring relation ap-
proach for TLINKs, we annotated a total of 6,904
relations between events and DCT and events and
temporal expressions. The breakdown of the dis-
tribution of the values is reported in Table 1.

TLINK Value DCT TIMEX3
CONTAINS 522 2816
BEFORE 52 n.a.
AFTER 3283 n.a.
OVERLAP 160 n.a.

Table 1: TLINK value per DCT and temporal ex-
pression in the document.

As for the PLOT LINKs, a total of 2,265 ex-
planatory relations have been annotated, with an
average of 8.7 relations per document. 1,147 re-
lations have been classified as rising action,
while 1,118 as falling action. By extend-
ing the manually annotated relations with within-
document event co-reference chains, we reach a
total of 5,519 PLOT LINKs, almost three times
the average relation per document, i.e. 21.39.
This results in 2,653 rising action and 2,844
falling action relations, respectively. Fi-
nally, only 117 explicit causal relations have been
identified.7

6Event annotation is directly inherited from ECB+, where
only sentences containing relevant mentions of the topic were
annotated.

7Note that this can be extended further using the cross-
document event coreference chains of ECB+

81



The annotation of the ESC v0.9 corpus has
been conducted by 2 experts following a multi-
step process and using the web-based tool CAT
(Bartalesi Lenzi et al., 2012). In the first phase,
both annotators went through a training phase to
familiarize with the task, and were allowed to dis-
cuss and compare their annotations, especially for
the PLOT LINK task. This phase led to a revi-
sion of the annotation guidelines, by introducing
more specific rules to select event pairs. In the
second phase, the inter-annotator agreement was
calculated on a subset of the ESC v0.9 dataset.
In particular, given that the basic components, i.e.
event mentions, temporal expressions, and event
co-referential chains, are directly inherited from
the ECB+ corpus, the agreement was calculated
only for anchoring (i.e. TLINK tags) and ex-
planatory relations (i.e. PLOT LINK tags). Inter-
annotator agreement has been computed using the
Dice coefficient, both for relation detection and re-
lation classification. Two different subsets of the
ESC v0.9 corpus have been used for the two rela-
tions: one seminal event8 for TLINKs and 4 semi-
nal events9 for PLOT LINKs. We made this choice
because of the different nature of the two types of
relations. Results are reported in Table 2. The
scores for PLOT LINKs have been computed as
an average over the 4 seminal events.

Relation Type Identification Classification
TLINK 0.767 0.744
PLOT LINK 0.638 0.638

Table 2: Inter-annotator agreement: Dice coeffi-
cient at token level.

One of the most interesting observations on the
PLOT LINK analysis is that the agreement may
vary according to the type of seminal event. For
instance, the highest agreement has been observed
for T19: a shooting accident :Dice 0.723 for rela-
tion identification, and 0.728 for relation classifi-
cation. The lowest agreement was found for an es-
cape from prison (T3): Dice 0.48 for relation iden-
tification, and 0.471 for relation classification. The
results, although preliminary, suggest that differ-
ent types of seminal events may be narrated in dif-
ferent ways following different story patterns (e.g.
more or less linear stories).

8T37
9T3, T19, T37, T41

3 Experiments: Baselines

In this section, we describe the experimental re-
sults for a number of StoryLine Extraction base-
line systems on the ESC v0.9 dataset. The out-
comes of these experiments will be useful to com-
pare the performance of future (and more com-
plex) systems, as well as to have a preliminary as-
sessment of the complexity of the task.

The ESC v0.9 dataset has been divided into a
development set, consisting of 6 seminal events10

and a test set of 16 seminal events11. The test
subset contains a total of 4,027 PLOT LINKs
when extended with within-document event co-
reference chains. All experiments have been con-
ducted considering gold data for event mention ex-
tent, temporal expression extent and values, and
event co-reference.

Three baselines have been developed: 1) OP:
selection of event pairs in relations that mimic the
textual order of presentation; 2) PPMI1: selection
of event pairs using Positive Pointwise Mutual In-
formation (PPMI) obtained from a set of selected
seed pairs and the manually annotated pairs from
the development set; 3) PPMI-CONTAINS: selec-
tion of the event pairs using PPMI as in the PPMI1
model but restricting the sets of events to those
which share the same temporal anchors, i.e. have
a TLINK of type contains.

The seed pairs for the PPMI based models
have been extracted from the SemEval 2012
Task-2: Measuring Degrees of Relational Sim-
ilarity (Jurgens et al., 2012). In particular, we
extracted words pairs from the test set Phase-
1 Answers corresponding to class-8 (CAUSE-
PURPOSE), retaining only word pairs in the cat-
egories Cause:Effect, Cause:Compensatory Ac-
tion, Action/Activity, and Prevention, where both
words express events. This initial set of seed
elements has been further extended by looking
for “cause”, “enablement”, and “entails” relations
in SUMO (Niles and Pease, 2001, 2003) and in
WordNet (Miller, 1995). This resulted in a list
of 1,609 unique seed pairs. PPMI has been com-
puted using the DISSECT Toolkit (Dinu et al.,
2013), and pair frequencies have been extracted
from Google bigrams(Brants and Franz, 2006).
Rather than identifying a unique threshold for eli-
gible pairs, we looked for a range of PPMI values.

10T5, T7, T8, T32, T33, T35
11T1, T12, T13, T14, T16, T18, T19, T20, T22, T23, T24,

T3, T30, T37, T4, T41

82



Baseline Model
PLOT LINK Detection PLOT LINK Classification

P R F1 P R F1
OP 0.156 0.988 0.265 0.07 0.97 0.14
PPMI1 0.137 0.174 0.137 0.065 0.098 0.068
PPMI-CONTAINS 0.227 0.091 0.121 0.114 0.05 0.064

Table 3: Results of three baselines models on PLOT LINK identification and classification .

This has been identified by normalizing the PPMI
scores between 0 and 1, computing average and
standard deviation. This allowed us to identify a
minimum and a maximum normalized score12 for
PPMI, representing the boundaries of the range in-
side which event pairs in a PLOT LINK relation
can be identified and selected.

As for the extraction of the events in a
PLOT LINK relation from the test data,
co-occurrence frequencies were computed
per pairs of eligible event types (i.e. AC-
TION OCCURRENCE, ACTION PERCEPTION,
ACTION STATE) both at sentence and at doc-
ument level. PPMI values were obtained by
applying the same procedure used for the seed
pairs. In the PPMI1 model, all event pairs whose
score is within the range obtained from the seed
pairs were selected. On the other hand, in the
PPMI-CONTAINS model, the event pairs were
further filtered by applying the temporal anchor
constraints, i.e. they must both have a TLINK
of type contains with the same temporal
expression.

As for relation classification, i.e. the as-
signment of the values rising action or
falling action to an event pair, we decided
to always assign the rising action value, i.e.
the most frequent value from the manually anno-
tated data. In addition to this, we also aimed at
evaluating the impact of the order of presentation
of the information in a document on PLOT LINKs.

In Table 3, we report on the aggregated results,
i.e. average score over the test data, of the three
baselines. The relation detection subtask limits
the evaluation to the correctness/validity of the
event pairs identified by each model against the
extended gold data. On the other hand, in the clas-
sification subtask, both the event pair and the rela-
tion value must be correct. This means that if the
PLOT LINK value is wrong but the event pair is
correct, then the entire PLOT LINK is considered

12Average PPMI value=0.582; standard deviation=0.181;
minimum PPMI value=0.4; maximum PPMI value=0.763

incorrect. Standard Precision (P), Recall (R), and
F1-score (F1) apply for both subtasks.

The results, though preliminary, highlight the
complexity of the task. Not surprisingly the best
Recall value is obtained by the OP model. The cre-
ation of all possible pairs between eligible event
types clearly gives rise to a lot of False Posi-
tive pairs (P=0.156), showing that even when only
events in relevant sentences of specific topic are
selected, there is still information which is not
to be included in a storyline. For instance, there
could be references to events which occurred in
the past and which do not have any explanatory
relations with the event mentions referring to the
current topic, and presented to the reader for com-
parison or as additional background knowledge.

Different observations apply to the PPMI-based
models. In PPMI1, we can observe a big drop in
Recall (-0.841) and as well as in Precision, though
lower (-0.019). On the other hand, temporal con-
tainment seems to facilitate the aggregation of the
relevant pairs of a storyline, as shown by Precision
(P=0.227). At this stage of the implementation,
there is a lack of connection between events in dif-
ferent temporal anchors, thus limiting the connec-
tions between event pairs and having a negative
impact on the Recall.

By observing the results on the classification
task, it immediately appears that the textual or-
der of presentation of the information badly cor-
relates with PLOT LINK values. The low results
were in part expected given the distribution of the
rising action and falling action rela-
tions in the test data. To better understand the re-
sults, we run an additional evaluation on the base-
lines by taking into account only same sentence
pairs. In this case, we observed that all baselines
increase the Precision (P=0.123 for OP, P=0.095
for PPM1, and P=0.151 for PPMI-CONTAINS)
and downgrade the Recall scores. Given the eval-
uation framework for classification, this suggests
that, at least when in the same sentence, there is
a tendency to narrate the events following a logi-

83



cal order, not only a temporal one. However, this
does not hold anymore when cross-sentence rela-
tions are taken into account.

4 Related Work

Frameworks and models for understanding narra-
tives have mainly focused on fictional texts (Lehn-
ert, 1981; Goyal et al., 2010; Mani, 2012) Mod-
ern day news reports still reflect narrative struc-
tures but they have proven difficult for automatic
tools (Rospocher et al., 2016). To the best of
our knowledge, previous work on StoryLine Ex-
traction is limited, if we exclude the contribution
by Caselli and Vossen (2016). However, there
are several related works in NLP dealing with re-
lated tasks. The extraction of causal relations
is the nearest task. One of the most prominent
work is represented by the Penn Discourse Tree-
bank (PDTB) (Miltsakaki et al., 2004), where ex-
plicit and implicit causal relations are annotated
between discourse units.

The Causal-TimeBank (Mirza and Tonelli,
2016) has introduced a TimeML-based annotation
of causal relations between events on top of the
TempEval-3 TimeBank data. Casual relations are
annotated by means of a CLINK tag and only ex-
plicit causal relations are marked-up, i.e. the re-
lation must be signaled by a linguistic markers
(e.g. a preposition or a causal verb). This re-
sults in 318 CLINKs, 296 of which are in same-
sentence. The RED guidelines (O’Gorman et al.,
2016) combines event co-reference, temporal and
causal relations. In particular, causal relations
are expressed by means of precondition and
cause values, allowing both same sentence and
adjacent sentence relations, thus aiming at achiev-
ing a richer semantic representations of event re-
lations. The BECauSe Corpus 2.0 (Dunietz et al.,
2015) focuses on causal language, by represent-
ing what causal relationships are expressed in a
text/document, rather than taking into account real
world causality. Causal relations are annotated
only in presence of a causal connective (i.e. a lex-
ical item signaling the causal relation). The anno-
tation scheme is very rich as it allows the mark-up
of overlapping relations (e.g. temporal, correla-
tion, hypothetical, among others) as well.

Another relevant work is the CaTeRs annota-
tion scheme (Mostafazadeh et al., 2016b). In
CaTeRs, causal relations between events are anno-
tated from a “commonsense reasoning” perspec-

tive rather than starting from linguistic markers,
inspired by the mental model theory of causality.
The scheme identifies 9 classes of causal relations
as well as 4 classes of temporal relations. The
scheme has been applied over 320 stories from the
ROCStories Corpus (Mostafazadeh et al., 2016a),
which collects everyday stories (e.g. “got a phone
call”) composed by 5 sentences. The main goal of
the annotation is to focus on those causal and tem-
poral relations which may facilitate the learning of
stereotypical narrative structures.

In this work, we have extended the set of event-
event relations to be annotated using the notion of
explanatory relation. In our work both implicit and
explicit relations are annotated, allowing the an-
notation at both intra- and inter-sentential levels.
In addition to this, the availability of within- and
cross-document event co-reference chains allows
the extension of the annotated data across docu-
ments, providing access to a larger, “global” level
of analysis.

5 Conclusion and Future Works

This paper presents the Event StoryLine Corpus
v0.9, the first benchmark corpus for a StoryLine
Extraction task, i.e. temporally and logically con-
nected sequences of events related to a specific
topic from documents spread in time. We also
presented three baseline systems with their perfor-
mance on the data base. This task aims at mov-
ing away from current approaches on timeline and
causal relation extraction. With respect to the for-
mer task, storylines aim at the chronologically or-
dering only of events that are relevant to a story,
thus cleaning timeline structures. At the same
time, storylines extend causal relation extraction
by covering both explicit and implicit causal re-
lations between events, both at a intra- and inter-
sentential levels. This facilitates the learning of
narrative models, i.e. explanatory patterns in news
data, which can be used to identify both stereotyp-
ical and episodic narrations of seminal events, or
topics, in news. One the innovative aspects is the
connection with co-reference relations of events
across documents, thus making the annotated data
also useful for the development of cross-document
summarization systems.

The corpus will be extended in the future by
means of crowd-sourcing and by introducing an-
notations of climax events, i.e. the main events in
the story. In parallel, we aim at developing more

84



robust systems.

Acknowledgments

This work has been supported the NWO Spinoza
Prize project “Understanding Language by Ma-
chines” (sub-track 3).

References
Emmon Bach. 1986. The algebra of events. Linguistics

and philosophy 9(1):5–16.

Mieke Bal. 1997. Narratology: Introduction to the the-
ory of narrative. University of Toronto Press.

Valentina Bartalesi Lenzi, Giovanni Moretti, and
Rachele Sprugnoli. 2012. CAT: the CELCT Anno-
tation Tool. In In Proceedings of LREC 2012. pages
333–338.

Steven Bethard, William J Corvey, Sara Klingenstein,
and James H Martin. 2008. Building a corpus of
temporal-causal structure. In LREC.

Andrea Bonomi and Alessandro Zucchi. 2001. Tempo
e linguaggio: introduzione alla semantica del tempo
e dell’aspetto verbale. Pearson Italia Spa.

Brian Boyd. 2009. On the origin of stories. Harvard
University Press.

Thorsten Brants and Alex Franz. 2006. Web 1t 5-gram
corpus version 1.1. Google Inc .

Tommaso Caselli and Piek Vossen. 2016. The storyline
annotation and representation scheme (star): A pro-
posal. In Proceedings of the 2nd Workshop on Com-
puting News Storylines (CNS 2016). Association
for Computational Linguistics, Austin, Texas, pages
67–72. http://aclweb.org/anthology/W16-5708.

Taylor Cassidy, Bill McDowell, Nathanael Cham-
bers, and Steven Bethard. 2014. An annotation
framework for dense event ordering. In Pro-
ceedings of the 52nd Annual Meeting of the As-
sociation for Computational Linguistics (Volume
2: Short Papers). Association for Computational
Linguistics, Baltimore, Maryland, pages 501–506.
http://www.aclweb.org/anthology/P14-2082.

Agata Cybulska and Piek Vossen. 2014a. Guidelines
for ECB+ annotation of events and their coreference.
Technical Report NWR-2014-1, VU University Am-
sterdam.

Agata Cybulska and Piek Vossen. 2014b. Using a
sledgehammer to crack a nut? Lexical diversity and
event coreference resolution. In Proceedings of the
9th Language Resources and Evaluation Conference
(LREC2014). Reykjavik, Iceland.

Georgiana Dinu, Nghia The Pham, and Marco Baroni.
2013. Dissect - distributional semantics composi-
tion toolkit. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguis-
tics: System Demonstrations. Association for Com-
putational Linguistics, Sofia, Bulgaria, pages 31–36.
http://www.aclweb.org/anthology/P13-4006.

Jesse Dunietz, Lori Levin, and Jaime Carbonell.
2015. Annotating causal language using cor-
pus lexicography of constructions. In Pro-
ceedings of The 9th Linguistic Annotation Work-
shop. Association for Computational Linguis-
tics, Denver, Colorado, USA, pages 188–196.
http://www.aclweb.org/anthology/W15-1622.

Antske Fokkens, Marieke van Erp, Piek Vossen, Sara
Tonelli, Willem Robert van Hage, Luciano Serafini,
Rachele Sprugnoli, and Jesper Hoeksema. 2013.
Gaf: A grounded annotation framework for events.
In Workshop on Events: Definition, Detection,
Coreference, and Representation. Association for
Computational Linguistics, Atlanta, Georgia, pages
11–20. http://www.aclweb.org/anthology/W13-
1202.

Jonathan Gottschall. 2012. The storytelling animal:
How stories make us human. Houghton Mifflin Har-
court.

Amit Goyal, Ellen Riloff, and Hal Daume III.
2010. Automatically producing plot unit represen-
tations for narrative text. In Proceedings of the
2010 Conference on Empirical Methods in Natu-
ral Language Processing. Association for Compu-
tational Linguistics, Cambridge, MA, pages 77–86.
http://www.aclweb.org/anthology/D10-1008.

David Jurgens, Saif Mohammad, Peter Turney, and
Keith Holyoak. 2012. Semeval-2012 task 2: Mea-
suring degrees of relational similarity. In *SEM
2012: The First Joint Conference on Lexical
and Computational Semantics – Volume 1: Pro-
ceedings of the main conference and the shared
task, and Volume 2: Proceedings of the Sixth
International Workshop on Semantic Evaluation
(SemEval 2012). Association for Computational
Linguistics, Montréal, Canada, pages 356–364.
http://www.aclweb.org/anthology/S12-1047.

Wendy G Lehnert. 1981. Plot units and narrative sum-
marization. Cognitive Science 5(4):293–331.

Linguistic Data Consortium. 2005. ACE (Automatic
Content Extraction) English annotation guidelines
for entities.

Inderjeet Mani. 2012. Computational modeling of
narrative. Synthesis Lectures on Human Language
Technologies 5(3):1–142.

George A Miller. 1995. Wordnet: a lexical database for
english. Communications of the ACM 38(11):39–
41.

85



Eleni Miltsakaki, Rashmi Prasad, Aravind K Joshi, and
Bonnie L Webber. 2004. The penn discourse tree-
bank. In LREC.

Paramita Mirza and Sara Tonelli. 2014. An analy-
sis of causality between events and its relation to
temporal information. In Proceedings of COLING
2014, the 25th International Conference on Com-
putational Linguistics: Technical Papers. Dublin
City University and Association for Computational
Linguistics, Dublin, Ireland, pages 2097–2106.
http://www.aclweb.org/anthology/C14-1198.

Paramita Mirza and Sara Tonelli. 2016. Catena: Causal
and temporal relation extraction from natural lan-
guage texts. In Proceedings of COLING 2016,
the 26th International Conference on Computational
Linguistics: Technical Papers. The COLING 2016
Organizing Committee, Osaka, Japan, pages 64–75.
http://aclweb.org/anthology/C16-1007.

Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong
He, Devi Parikh, Dhruv Batra, Lucy Vanderwende,
Pushmeet Kohli, and James Allen. 2016a. A cor-
pus and evaluation framework for deeper under-
standing of commonsense stories. arXiv preprint
arXiv:1604.01696 .

Nasrin Mostafazadeh, Alyson Grealish, Nathanael
Chambers, James Allen, and Lucy Vanderwende.
2016b. Caters: Causal and temporal relation
scheme for semantic annotation of event struc-
tures. In Proceedings of the Fourth Workshop
on Events. Association for Computational Lin-
guistics, San Diego, California, pages 51–61.
http://www.aclweb.org/anthology/W16-1007.

Ian Niles and Adam Pease. 2001. Towards a stan-
dard upper ontology. In Proceedings of the interna-
tional conference on Formal Ontology in Informa-
tion Systems-Volume 2001. ACM, pages 2–9.

Ian Niles and Adam Pease. 2003. Mapping wordnet to
the sumo ontology. In Proceedings of the ieee inter-
national knowledge engineering conference. pages
23–26.

Tim O’Gorman, Kristin Wright-Bettner, and Martha
Palmer. 2016. Richer event description: Integrating
event coreference with temporal, causal and bridg-
ing annotation. In Proceedings of the 2nd Workshop
on Computing News Storylines (CNS 2016). Associ-
ation for Computational Linguistics, Austin, Texas,
pages 47–56. http://aclweb.org/anthology/W16-
5706.

James Pustejovsky, José Castao, Robert Ingria, Roser
Saurı̀, Robert Gaizauskas, Andrea Setzer, and Gra-
ham Katz. 2003a. TimeML: Robust Specification of
Event and Temporal Expressions in Text. In Fifth
International Workshop on Computational Seman-
tics (IWCS-5).

James Pustejovsky and Amber Stubbs. 2011. In-
creasing informativeness in temporal annotation.

In Proceedings of the 5th Linguistic Annotation
Workshop. Association for Computational Linguis-
tics, Portland, Oregon, USA, pages 152–160.
http://www.aclweb.org/anthology/W11-0419.

Marco Rospocher, Marieke van Erp, Piek Vossen,
Antske Fokkens, Itziar Aldabe, German Rigau,
Aitor Soroa, Thomas Ploeger, and Tessel Bogaard.
2016. Building event-centric knowledge graphs
from news. Web Semantics: Science, Services and
Agents on the World Wide Web 37:132–151.

86


