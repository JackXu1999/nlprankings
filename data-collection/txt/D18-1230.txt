



















































Learning Named Entity Tagger using Domain-Specific Dictionary


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2054–2064
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

2054

Learning Named Entity Tagger using Domain-Specific Dictionary

Jingbo Shang †∗ Liyuan Liu †∗ Xiaotao Gu † Xiang Ren ] Teng Ren ‡ Jiawei Han †
† University of Illinois at Urbana-Champaign, Urbana, IL, USA

] University of Southern California, Los Angeles, CA, USA
‡ CooTek Inc., Shanghai, China

†{shang7, ll2, xiaotao2, hanj}@illinois.edu ]xiangren@usc.edu ‡teng.ren@cootek.cn

Abstract

Recent advances in deep neural models allow
us to build reliable named entity recognition
(NER) systems without handcrafting features.
However, such methods require large amounts
of manually-labeled training data. There have
been efforts on replacing human annotations
with distant supervision (in conjunction with
external dictionaries), but the generated noisy
labels pose significant challenges on learning
effective neural models. Here we propose
two neural models to suit noisy distant super-
vision from the dictionary. First, under the
traditional sequence labeling framework, we
propose a revised fuzzy CRF layer to handle
tokens with multiple possible labels. After
identifying the nature of noisy labels in dis-
tant supervision, we go beyond the traditional
framework and propose a novel, more effec-
tive neural model AutoNER with a new Tie
or Break scheme. In addition, we dis-
cuss how to refine distant supervision for bet-
ter NER performance. Extensive experiments
on three benchmark datasets demonstrate that
AutoNER achieves the best performance when
only using dictionaries with no additional hu-
man effort, and delivers competitive results
with state-of-the-art supervised benchmarks.

1 Introduction

Recently, extensive efforts have been made on
building reliable named entity recognition (NER)
models without handcrafting features (Liu et al.,
2018; Ma and Hovy, 2016; Lample et al.,
2016). However, most existing methods require
large amounts of manually annotated sentences
for training supervised models (e.g., neural se-
quence models) (Liu et al., 2018; Ma and Hovy,
2016; Lample et al., 2016; Finkel et al., 2005).
This is particularly challenging in specific do-

∗Equal contribution.

mains, where domain-expert annotation is expen-
sive and/or slow to obtain.

To alleviate human effort, distant supervision
has been applied to automatically generate labeled
data, and has gained successes in various natural
language processing tasks, including phrase min-
ing (Shang et al., 2018), entity recognition (Ren
et al., 2015; Fries et al., 2017; He, 2017), aspect
term extraction (Giannakopoulos et al., 2017), and
relation extraction (Mintz et al., 2009). Mean-
while, open knowledge bases (or dictionaries) are
becoming increasingly popular, such as WikiData
and YAGO in the general domain, as well as
MeSH and CTD in the biomedical domain. The
existence of such dictionaries makes it possible
to generate training data for NER at a large scale
without additional human effort.

Existing distantly supervised NER models usu-
ally tackle the entity span detection problem by
heuristic matching rules, such as POS tag-based
regular expressions (Ren et al., 2015; Fries et al.,
2017) and exact string matching (Giannakopou-
los et al., 2017; He, 2017). In these models,
every unmatched token will be tagged as non-
entity. However, as most existing dictionaries have
limited coverage on entities, simply ignoring un-
matched tokens may introduce false-negative la-
bels (e.g., “prostaglandin synthesis” in Fig. 1).
Therefore, we propose to extract high-quality out-
of-dictionary phrases from the corpus, and mark
them as potential entities with a special “un-
known” type. Moreover, every entity span in a
sentence can be tagged with multiple types, since
two entities of different types may share the same
surface name in the dictionary. To address these
challenges, we propose and compare two neural
architectures with customized tagging schemes.

We start with adjusting models under the tra-
ditional sequence labeling framework. Typically,
NER models are built upon conditional random



2055

fields (CRF) with the IOB or IOBES tagging
scheme (Liu et al., 2018; Ma and Hovy, 2016;
Lample et al., 2016; Ratinov and Roth, 2009;
Finkel et al., 2005). However, such design can-
not deal with multi-label tokens. Therefore, we
customize the conventional CRF layer in LSTM-
CRF (Lample et al., 2016) into a Fuzzy CRF layer,
which allows each token to have multiple labels
without sacrificing computing efficiency.

To adapt to imperfect labels generated by dis-
tant supervision, we go beyond the traditional se-
quence labeling framework and propose a new
prediction model. Specifically, instead of predict-
ing the label of each single token, we propose to
predict whether two adjacent tokens are tied in the
same entity mention or not (i.e., broken). The key
motivation is that, even the boundaries of an en-
tity mention are mismatched by distant supervi-
sion, most of its inner ties are not affected, and
thus more robust to noise. Therefore, we design
a new Tie or Break tagging scheme to bet-
ter exploit the noisy distant supervision. Accord-
ingly, we design a novel neural architecture that
first forms all possible entity spans by detecting
such ties, then identifies the entity type for each
span. The new scheme and neural architecture
form our new model, AutoNER, which proves to
work better than the Fuzzy CRF model in our ex-
periments.

We summarize our major contributions as

• We propose AutoNER, a novel neural model
with the new Tie or Break scheme for the
distantly supervised NER task.
• We revise the traditional NER model to the

Fuzzy-LSTM-CRF model, which serves as a
strong distantly supervised baseline.
• We explore to refine distant supervision for bet-

ter NER performance, such as incorporating
high-quality phrases to reduce false-negative la-
bels, and conduct ablation experiments to verify
the effectiveness.
• Experiments on three benchmark datasets

demonstrate that AutoNER achieves the best
performance when only using dictionaries with
no additional human effort and is even compet-
itive with the supervised benchmarks.

We release all code and data for future studies1.
Related open tools can serve as the NER module

1 https://github.com/shangjingbo1226/
AutoNER

of various domain-specific systems in a plug-in-
and-play manner.

2 Overview

Our goal, in this paper, is to learn a named entity
tagger using, and only using dictionaries. Each
dictionary entry consists of 1) the surface names
of the entity, including a canonical name and a list
of synonyms; and 2) the entity type. Considering
the limited coverage of dictionaries, we extend ex-
isting dictionaries by adding high-quality phrases
as potential entities with unknown type. More de-
tails on refining distant supervision for better NER
performance will be presented in Sec. 4.

Given a raw corpus and a dictionary, we first
generate entity labels (including unknown la-
bels) by exact string matching, where conflicted
matches are resolved by maximizing the total
number of matched tokens (Etzioni et al., 2005;
Hanisch et al., 2005; Lin et al., 2012; He, 2017).

Based on the result of dictionary matching, each
token falls into one of three categories: 1) it be-
longs to an entity mention with one or more known
types; 2) it belongs to an entity mention with un-
known type; and 3) It is marked as non-entity.

Accordingly, we design and explore two neu-
ral models, Fuzzy-LSTM-CRF with the modified
IOBES scheme and AutoNER with the Tie or
Break scheme, to learn named entity taggers
based on such labels with unknown and multiple
types. We will discuss the details in Sec. 3.

3 Neural Models

In this section, we introduce two prediction mod-
els for the distantly supervised NER task, one un-
der the traditional sequence labeling framework
and another with a new labeling scheme.

3.1 Fuzzy-LSTM-CRF with Modified IOBES
State-of-the-art named entity taggers follow the
sequence labeling framework using IOB or
IOBES scheme (Ratinov and Roth, 2009), thus
requiring a conditional random field (CRF) layer
to capture the dependency between labels. How-
ever, both the original scheme and the conven-
tional CRF layer cannot handle multi-typed or
unknown-typed tokens. Therefore, we propose the
modified IOBES scheme and Fuzzy CRF layer ac-
cordingly, as illustrated in Figure 1.

Modified IOBES. We define the labels accord-
ing to the three token categories. 1) For a token

https://github.com/shangjingbo1226/AutoNER
https://github.com/shangjingbo1226/AutoNER


2056

B—
Diease

I—
Diease

S—
Chemical

O

…

B—
Diease

I—
Diease

S—
Chemical

O

…

<S> Thus , indomethacin

B—
Diease

I—
Diease

S—
Chemical

O

…

, by inhibition prostaglandin

B—
Diease

I—
Diease

S—
Chemical

O

…

B—
Diease

I—
Diease

S—
Chemical

O

…

B—
Diease

I—
Diease

S—
Chemical

O

…

B—
Diease

I—
Diease

S—
Chemical

O

…

of synthesis diminish

B—
Diease

I—
Diease

S—
Chemical

O

…

B—
Diease

I—
Diease

S—
Chemical

O

…

B—
Diease

I—
Diease

S—
Chemical

O

…

may

…

…

…

…
max log(

P
P )

Figure 1: The illustration of the Fuzzy CRF layer with modified IOBES tagging scheme. The named entity types
are {Chemical, Disease}. “indomethacin” is a matched Chemical entity and “prostaglandin synthesis” is an
unknown-typed high-quality phrase. Paths from Start to Endmarked as purple form all possible label sequences
given the distant supervision.

marked as one or more types, it is labeled with all
these types and one of {I, B, E, S} according to
its positions in the matched entity mention. 2) For
a token with unknown type, all five {I, O, B, E, S}
tags are possible. Meanwhile, all available types
are assigned. For example, when there are only
two available types (e.g., Chemical and Disease),
it has nine (i.e., 4× 2 + 1) possible labels in total.
3) For a token that is annotated as non-entity, it is
labeled as O.

As demonstrated in Fig. 1, based on the dic-
tionary matching results, “indomethacin” is a
singleton Chemical entity and “prostaglandin
synthesis” is an unknown-typed high-quality
phrase. Therefore, “indomethacin” is labeled
as S-Chemical, while both “prostaglandin”
and “synthesis” are labeled as O, B-Disease,
I-Disease, . . ., and S-Chemical because
the available entity types are {Chemical,
Disease}. The non-entity tokens, such as
“Thus” and “by”, are labeled as O.

Fuzzy-LSTM-CRF. We revise the LSTM-CRF
model (Lample et al., 2016) to the Fuzzy-LSTM-
CRF model to support the modified IOBES labels.

Given a word sequence (X1, X2, . . . , Xn),
it is first passed through a word-level BiL-
STM (Hochreiter and Schmidhuber, 1997) (i.e.,
forward and backward LSTMs). After concatenat-
ing the representations from both directions, the
model makes independent tagging decisions for
each output label. In this step, the model estimates
the score Pi,yj for the word Xi being the label yj .

We follow previous works (Liu et al., 2018; Ma
and Hovy, 2016; Lample et al., 2016) to define the
score of the predicted sequence, the score of the
predicted sequence (y1, y2, . . . , yn) is defined as:

s(X, y) =
n∑

i=0

Φyi,yi+1 +

n∑

i=1

Pi,yi (1)

where, Φyi,yi+1 is the transition probability from a
label yi to its next label yi+1. Φ is a (k + 2) ×
(k + 2) matrix, where k is the number of distinct
labels. Two additional labels start and end are
used (only used in the CRF layer) to represent the
beginning and end of a sequence, respectively.

The conventional CRF layer maximizes the
probability of the only valid label sequence. How-
ever, in the modified IOBES scheme, one sentence
may have multiple valid label sequences, as shown
in Fig. 1. Therefore, we extend the conventional
CRF layer to a fuzzy CRF model. Instead, it max-
imizes the total probability of all possible label se-
quences by enumerating both the IOBES tags and
all matched entity types. Mathematically, we de-
fine the optimization goal as Eq. 2.

p(y|X) =
∑

ỹ∈Ypossible e
s(X,ỹ)

∑
ỹ∈YX e

s(X,ỹ)
(2)

where YX means all the possible label sequences
for sequence X , and Ypossible contains all the pos-
sible label sequences given the labels of modified
IOBES scheme. Note that, when all labels and
types are known and unique, the fuzzy CRF model
is equivalent to the conventional CRF.

During the training process, we maximize the
log-likelihood function of Eq. 2. For inference, we
apply the Viterbi algorithm to maximize the score
of Eq. 1 for each input sequence.

3.2 AutoNER with “Tie or Break”

Identifying the nature of the distant supervision,
we go beyond the sequence labeling framework
and propose a new tagging scheme, Tie or
Break. It focuses on the ties between adjacent to-
kens, i.e., whether they are tied in the same entity
mentions or broken into two parts. Accordingly,
we design a novel neural model for this scheme.



2057

e r a m i c u n i o d ␣ a n d ␣ 8 G B ␣ R
c2,0 c2,1 c2,2 c2,3 c2,4 c2,5 c2,6 c2, c3,0 c3,1 c3,2 c3,3 c3,4 c3,5 c3,6 c3, c4,0 c4,2c4,1 c4, c5,0 c5,1 c5,2 c5, c6,0

ceramic unibody and␣

Tie Break Unknown

Entity Type: AspectTerm Entity Type: None

c ␣ b y

␣ RAM␣ 8GB ␣

c2,

Break

␣

␣

w i
c6,0c1,0 c3,6

t

with

h

Break

␣

␣

Entity Type: None

Unknown

c0, c1,2

Figure 2: The illustration of AutoNER with Tie or Break tagging scheme. The named entity type is
{AspectTerm}. “ceramic unibody” is a matched AspectTerm entity and “8GB RAM” is an unknown-typed
high-quality phrase. Unknown labels will be skipped during the model training.

“Tie or Break” Tagging Scheme. Specifically,
for every two adjacent tokens, the connection
between them is labeled as (1) Tie, when the
two tokens are matched to the same entity; (2)
Unknown, if at least one of the tokens belongs
to an unknown-typed high-quality phrase; (3)
Break, otherwise.

An example can be found in Fig. 2. The dis-
tant supervision shows that “ceramic unibody” is
a matched AspectTerm and “8GB RAM” is an
unknown-typed high-quality phrase. Therefore, a
Tie is labeled between “ceramic” and “unibody”,
while Unknown labels are put before “8GB”, be-
tween “8GB” and “RAM”, and after “RAM”.

Tokens between every two consecutive Break
form a token span. Each token span is associated
with all its matched types, the same as for the mod-
ified IOBES scheme. For those token spans with-
out any associated types, such as “with” in the ex-
ample, we assign them the additional type None.

We believe this new scheme can better exploit
the knowledge from dictionary according to the
following two observations. First, even though the
boundaries of an entity mention are mismatched
by distant supervision, most of its inner ties are
not affected. More interestingly, compared to
multi-word entity mentions, matched unigram en-
tity mentions are more likely to be false-positive
labels. However, such false-positive labels will
not introduce incorrect labels with the Tie or
Break scheme, since either the unigram is a true
entity mention or a false positive, it always brings
two Break labels around.

AutoNER. In the Tie or Break scheme, en-
tity spans and entity types are encoded into two
folds. Therefore, we separate the entity span de-
tection and entity type prediction into two steps.

For entity span detection, we build a binary
classifier to distinguish Break from Tie, while

Unknown positions will be skipped. Specifically,
as shown in Fig. 2, for the prediction between i-th
token and its previous token, we concatenate the
output of the BiLSTM as a new feature vector, ui.
ui is then fed into a sigmoid layer, which estimates
the probability that there is a Break as

p(yi = Break|ui) = σ(wTui)

where yi is the label between the i-th and its pre-
vious tokens, σ is the sigmoid function, and w is
the sigmoid layer’s parameter. The entity span de-
tection loss is then computed as follows.

Lspan =
∑

i|yi 6=Unknown

l
(
yi, p(yi = Break|ui)

)

Here, l(·, ·) is the logistic loss. Note that those
Unknown positions are skipped.

After obtaining candidate entity spans, we fur-
ther identify their entity types, including the None
type for non-entity spans. As shown in Fig. 2, the
output of the BiLSTM will be re-aligned to form
a new feature vector, which is referred as vi for
i-th span candidate. vi will be further fed into a
softmax layer, which estimates the entity type dis-
tribution as

p(tj |vi) =
et

T
j vi

∑
tk∈L e

tTk vi

where tj is an entity type and L is the set of all
entity types including None.

Since one span can be labeled as multiple types,
we mark the possible set of types for i-th entity
span candidate as Li. Accordingly, we modify the
cross-entropy loss as follows.

Ltype = H(p̂(·|vi, Li), p(·|vi))

Here, H(p, q) is the cross entropy between p and
q, and p̂(tj |vi, Li) is the soft supervision distribu-



2058

tion. Specifically, it is defined as:

p̂(tj |vi, Li) =
δ(tj ∈ Li) · et

T
j vi

∑
tk∈L δ(tk ∈ Li) · e

tTk vi

where δ(tj ∈ Li) is the boolean function of check-
ing whether the i-th span candidate is labeled as
the type tj in the distant supervision.

It’s worth mentioning that AutoNER has no
CRF layer and Viterbi decoding, thus being more
efficient than Fuzzy-LSTM-CRF for inference.

3.3 Remarks on “Unknown” Entities

“Unknown” entity mentions are not the entities of
other types, but the tokens that we are less confi-
dent about their boundaries and/or cannot identify
their types based on the distant supervision. For
example, in Figure 1, “prostaglandin synthesis” is
an “unknown” token span. The distant supervi-
sion cannot decide whether it is a Chemical, a
Disease, an entity of other types, two separate
single-token entities, or (partially) not an entity.
Therefore, in the FuzzyCRF model, we assign all
possible labels for these tokens.

In our AutoNER model, these “unknown” posi-
tions have undefined boundary and type losses, be-
cause (1) they make the boundary labels unclear;
and (2) they have no type labels. Therefore, they
are skipped.

4 Distant Supervision Refinement

In this section, we present two techniques to refine
the distant supervision for better named entity tag-
gers. Ablation experiments in Sec. 5.4 verify their
effectiveness empirically.

4.1 Corpus-Aware Dictionary Tailoring

In dictionary matching, blindly using the full dic-
tionary may introduce false-positive labels, as
there exist many entities beyond the scope of the
given corpus but their aliases can be matched. For
example, when the dictionary has a non-related
character name “Wednesday Addams”2 and its
alias “Wednesday”, many Wednesday’s will be
wrongly marked as persons. In an ideal case, the
dictionary should cover, and only cover entities
occurring in the given corpus to ensure a high pre-
cision while retaining a reasonable coverage.

2https://en.wikipedia.org/wiki/
Wednesday_Addams

As an approximation, we tailor the original dic-
tionary to a corpus-related subset by excluding en-
tities whose canonical names never appear in the
given corpus. The intuition behind is that to avoid
ambiguities, people will likely mention the canon-
ical name of the entity at least once. For example,
in the biomedical domain, this is true for 88.12%,
95.07% of entity mentions on the BC5CDR and
NCBI datasets respectively. We expect the NER
model trained on such tailored dictionary will have
a higher precision and a reasonable recall com-
pared to that trained on the original dictionary.

4.2 Unknown-Typed High-Quality Phrases

Another issue of the distant supervision is about
the false-negative labels. When a token span can-
not be matched to any entity surface names in the
dictionary, because of the limited coverage of dic-
tionaries, it is still difficult to claim it as non-entity
(i.e., negative labels) for sure. Specifically, some
high-quality phrases out of the dictionary may also
be potential entities.

We utilize the state-of-the-art distantly super-
vised phrase mining method, AutoPhrase (Shang
et al., 2018), with the corpus and dictionary in
the given domain as input. AutoPhrase only re-
quires unlabeled text and a dictionary of high-
quality phrases. We obtain quality multi-word and
single-word phrases by posing thresholds (e.g., 0.5
and 0.9 respectively). In practice, one can find
more unlabeled texts from the same domain (e.g.,
PubMed papers and Amazon laptop reviews) and
use the same domain-specific dictionary for the
NER task. In our experiments, for the biomedical
domain, we use the titles and abstracts of 686,568
PubMed papers (about 4%) uniformly sampled
from the whole PubTator database as the train-
ing corpus. For the laptop review domain, we
use the Amazon laptop review dataset3, which is
designed for the aspect-based sentiment analysis
(Wang et al., 2011).

We treat out-of-dictionary phrases as poten-
tial entities with “unknown” type and incorporate
them as new dictionary entries. After this, only to-
ken spans that cannot be matched in this extended
dictionary will be labeled as non-entity. Being
aware of these high-quality phrases, we expect the
trained NER tagger should be more accurate.

3http://times.cs.uiuc.edu/˜wang296/
Data/

https://en.wikipedia.org/wiki/Wednesday_Addams
https://en.wikipedia.org/wiki/Wednesday_Addams
http://times.cs.uiuc.edu/~wang296/Data/
http://times.cs.uiuc.edu/~wang296/Data/


2059

Table 1: Dataset Overview.
Dataset BC5CDR NCBI-Disease LaptopReview

Domain Biomedical Biomedical Technical Review

Entity Types Disease, Chemical Disease AspectTerm

Dictionary MeSH + CTD MeSH + CTD Computer Terms

Raw Sent. # 20,217 7,286 3,845

5 Experiments

We conduct experiments on three benchmark
datasets to evaluate and compare our proposed
Fuzzy-LSTM-CRF and AutoNER with many
other methods. We further investigate the effec-
tiveness of our proposed refinements for the dis-
tant supervision and the impact of the number of
distantly supervised sentences.

5.1 Experimental Settings

Datasets are briefly summarized in Table 1. More
details as as follows.

• BC5CDR is from the most recent BioCreative
V Chemical and Disease Mention Recognition
task. It has 1,500 articles containing 15,935
Chemical and 12,852 Disease mentions.
• NCBI-Disease focuses on Disease Name

Recognition. It contains 793 abstracts and 6,881
Disease mentions.
• LaptopReview is from the SemEval 2014 Chal-

lenge, Task 4 Subtask 1 (Pontiki et al., 2014) fo-
cusing on laptop aspect term (e.g., “disk drive”)
Recognition. It consists of 3,845 review sen-
tences and 3,012 AspectTerm mentions.

All datasets are publicly available. The first two
datasets are already partitioned into three subsets:
a training set, a development set, and a testing set.
For the LaptopReview dataset, we follow (Gian-
nakopoulos et al., 2017) and randomly select 20%
from the training set as the development set. Only
raw texts are provided as the input of distantly su-
pervised models, while the gold training set is used
for supervised models.

Domain-Specific Dictionary. For the biomedi-
cal datasets, the dictionary is a combination of
both the MeSH database4 and the CTD Chemical
and Disease vocabularies5. The dictionary con-
tains 322,882 Chemical and Disease entity
surfaces. For the laptop review dataset, the dic-
tionary has 13,457 computer terms crawled from a

4https://www.nlm.nih.gov/mesh/
download_mesh.html

5http://ctdbase.org/downloads/

public website6.

Metric. We use the micro-averaged F1 score as
the evaluation metric. Meanwhile, precision and
recall are presented. The reported scores are the
mean across five different runs.

Parameters and Model Training. Based on the
analysis conducted in the development set, we
conduct optimization with the stochastic gradient
descent with momentum. We set the batch size
and the momentum to 10 and 0.9. The learning
rate is initially set to 0.05 and will be shrunk by
40% if there is no better development F1 in the re-
cent 5 rounds. Dropout of a ratio 0.5 is applied in
our model. For a better stability, we use gradient
clipping of 5.0. Furthermore, we employ the early
stopping in the development set.

Pre-trained Word Embeddings. For the
biomedical datasets, we use the pre-trained 200-
dimension word vectors 7 from (Pyysalo et al.,
2013), which are trained on the whole PubMed
abstracts, all the full-text articles from PubMed
Central (PMC), and English Wikipedia. For
the laptop review dataset, we use the GloVe
100-dimension pre-trained word vectors8 instead,
which are trained on the Wikipedia and GigaWord.

5.2 Compared Methods

Dictionary Match is our proposed distant super-
vision generation method. Specifically, we apply
it to the testing set directly to obtain entity men-
tions with exactly the same surface name as in the
dictionary. The type is assigned through a major-
ity voting. By comparing with it, we can check
the improvements of neural models over the dis-
tant supervision itself.

SwellShark, in the biomedical domain, is ar-
guably the best distantly supervised model, es-
pecially on the BC5CDR and NCBI-Disease
datasets (Fries et al., 2017). It needs no human an-
notated data, however, it requires extra expert ef-
fort for entity span detection on building POS tag-
ger, designing effective regular expressions, and
hand-tuning for special cases.

Distant-LSTM-CRF achieved the best perfor-
mance on the LaptopReview dataset without an-
notated training data using a distantly supervised

6https://www.computerhope.com/jargon.
htm

7http://bio.nlplab.org/
8https://nlp.stanford.edu/projects/

glove/

https://www.nlm.nih.gov/mesh/download_mesh.html
https://www.nlm.nih.gov/mesh/download_mesh.html
http://ctdbase.org/downloads/
https://www.computerhope.com/jargon.htm
https://www.computerhope.com/jargon.htm
http://bio.nlplab.org/
https://nlp.stanford.edu/projects/glove/
https://nlp.stanford.edu/projects/glove/


2060

Table 2: [Biomedical Domain] NER Performance Comparison. The supervised benchmarks on the BC5CDR and
NCBI-Disease datasets are LM-LSTM-CRF and LSTM-CRF respectively (Wang et al., 2018). SwellShark has
no annotated data, but for entity span extraction, it requires pre-trained POS taggers and extra human efforts of
designing POS tag-based regular expressions and/or hand-tuning for special cases.

Method Human Effort BC5CDR NCBI-Disease

other than Dictionary Pre Rec F1 Pre Rec F1

Supervised Benchmark Gold Annotations 88.84 85.16 86.96 86.11 85.49 85.80

SwellShark Regex Design + Special Case Tuning 86.11 82.39 84.21 81.6 80.1 80.8

Regex Design 84.98 83.49 84.23 64.7 69.7 67.1

Dictionary Match
None

93.93 58.35 71.98 90.59 56.15 69.32

Fuzzy-LSTM-CRF 88.27 76.75 82.11 79.85 67.71 73.28

AutoNER 88.96 81.00 84.8 79.42 71.98 75.52

Table 3: [Technical Review Domain] NER Perfor-
mance Comparison. The supervised benchmark refers
to the challenge winner.

Method LaptopReview

Pre Rec F1

Supervised Benchmark 84.80 66.51 74.55

Distant-LSTM-CRF 74.03 31.59 53.93

Dictionary Match 90.68 44.65 59.84

Fuzzy-LSTM-CRF 85.08 47.09 60.63

AutoNER 72.27 59.79 65.44

LSTM-CRF model (Giannakopoulos et al., 2017).

Supervised benchmarks on each dataset are
listed to check whether AutoNER can deliver com-
petitive performance. On the BC5CDR and NCBI-
Disease datasets, LM-LSTM-CRF (Liu et al.,
2018) and LSTM-CRF (Lample et al., 2016)
achieve the state-of-the-art F1 scores without ex-
ternal resources, respectively (Wang et al., 2018).
On the LaptopReview dataset, we present the
scores of the Winner in the SemEval2014 Chal-
lenge Task 4 Subtask 1 (Pontiki et al., 2014).

5.3 NER Performance Comparison
We present F1, precision, and recall scores on all
datasets in Table 2 and Table 3. From both ta-
bles, one can find the AutoNER achieves the best
performance when there is no extra human effort.
Fuzzy-LSTM-CRF does have some improvements
over the Dictionary Match, but it is always worse
than AutoNER.

Even though SwellShark is designed for the
biomedical domain and utilizes much more ex-
pert effort, AutoNER outperforms it in almost all
cases. The only outlier happens on the NCBI-
disease dataset when the entity span matcher in

SwellShark is carefully tuned by experts for many
special cases.

It is worth mentioning that AutoNER beats
Distant-LSTM-CRF, which is the previous state-
of-the-art distantly supervised model on the Lap-
topReview dataset.

Moreover, AutoNER’s performance is compet-
itive to the supervised benchmarks. For exam-
ple, on the BC5CDR dataset, its F1 score is only
2.16% away from the supervised benchmark.

5.4 Distant Supervision Explorations

We investigate the effectiveness of the two tech-
niques that we proposed in Sec. 4 via ablation ex-
periments. As shown in Table 4, using the tailored
dictionary always achieves better F1 scores than
using the original dictionary. By using the tailored
dictionary, the precision of the AutoNER model
will be higher, while the recall will be retained
similarly. For example, on the NCBI-Disease
dataset, it significantly boosts the precision from
53.14% to 77.30% with an acceptable recall loss
from 63.54% to 58.54%. Moreover, incorporating
unknown-typed high-quality phrases in the dictio-
nary enhances every score of AutoNER models
significantly, especially the recall. These results
match our expectations well.

5.5 Test F1 Scores vs. Size of Raw Corpus

Furthermore, we explore the change of test F1
scores when we have different sizes of distantly
supervised texts. We sample sentences uniformly
random from the given raw corpus and then evalu-
ate AutoNER models trained on the selected sen-
tences. We also study what will happen when the
gold training set is available. The curves can be
found in Figure 3. The X-axis is the number of



2061

Table 4: Ablation Experiments for Dictionary Refinement. The dictionary for the LaptopReview dataset contains
no alias, so the corpus-aware dictionary tailoring is not applicable.

Method BC5CDR NCBI-Disease LaptopReview

Pre Rec F1 Pre Rec F1 Pre Rec F1

AutoNER w/ Original Dict 82.79 70.40 76.09 53.14 63.54 57.87 69.96 49.85 58.21

AutoNER w/ Tailored Dict 84.57 70.22 76.73 77.30 58.54 66.63 Not Applicable

AutoNER w/ Tailored Dict & Phrases 88.96 81.00 84.8 79.42 71.98 75.52 72.27 59.79 65.44

0 5000 10000 15000 20000
# of Distantly Labeled Training Sentences

0.70

0.75

0.80

0.85

Te
st

 F
1 

Sc
or

es

AutoNER-Gold+DistantSupervision
Supervised Benchmark
AutoNER-DistantSupervision

(a) BC5CDR

0 1000 2000 3000 4000 5000 6000 7000
# of Distantly Labeled Training Sentences

0.4

0.5

0.6

0.7

0.8

Te
st

 F
1 

Sc
or

es

AutoNER-Gold+DistantSupervision
Supervised Benchmark
AutoNER-DistantSupervision

(b) NCBI

0 1000 2000 3000 4000
# of Distantly Labeled Training Sentences

0.3

0.4

0.5

0.6

0.7

0.8

Te
st

 F
1 

Sc
or

es

AutoNER-Gold+DistantSupervision
Supervised Benchmark
AutoNER-DistantSupervision

(c) LaptopReview

Figure 3: AutoNER: Test F1 score vs. the number of distantly supervised sentences.

distantly supervised training sentences while the
Y-axis is the F1 score on the testing set.

When using distant supervision only, one can
observe a significant growing trend of testF1 score
in the beginning, but later the increasing rate slows
down when there are more and more raw texts.

When the gold training set is available, the dis-
tant supervision is still helpful to AutoNER. In the
beginning, AutoNER works worse than the super-
vised benchmarks. Later, with enough distantly
supervised sentences, AutoNER outperforms the
supervised benchmarks. We think there are two
possible reasons: (1) The distant supervision puts
emphasis on those matchable entity mentions; and
(2) The gold annotation may miss some good but
matchable entity mentions. These may guide the
training of AutoNER to a more generalized model,
and thus have a higher test F1 score.

5.6 Comparison with Gold Supervision

To demonstrate the effectiveness of distant super-
vision, we try to compare our method with gold
annotations provided by human experts.

Specifically, we conduct experiments on the
BC5CDR dataset by sampling different amounts
of annotated articles for model training. As shown
in Figure 4, we found that our method outper-
forms the supervised method by a large margin
when less training examples are available. For ex-
ample, when there are only 50 annotated articles
available, the test F1 score drops substantially to
74.29%. To achieve a similar test F1 score (e.g.,

0 100 200 300 400 500
# of Human Annotated Articles

0.50

0.55

0.60

0.65

0.70

0.75

0.80

0.85
Te

st
 F

1 
Sc

or
es

Supervised Benchmark
AutoNER-Distant Supervision

Figure 4: AutoNER: Test F1 score vs. the number of
human annotated articles.

83.91%) as our AutoNER models (i.e., 84.8%),
the supervised benchmark model requires at least
300 annotated articles. Such results indicate the
effectiveness and usefulness of AutoNER on the
scenario without sufficient human annotations.

Still, we observe that, when the supervised
benchmark is trained with all annotations, it
achieves the performance better than AutoNER.
We conjugate that this is because AutoNER lacks
more advanced techniques to handle distant super-
vision, and we leave further improvements of Au-
toNER to the future work.

6 Related Work

The task of supervised named entity recognition
(NER) is typically embodied as a sequence label-
ing problem. Conditional random fields (CRF)
models built upon human annotations and hand-
crafted features are the standard (Finkel et al.,
2005; Settles, 2004; Leaman and Gonzalez, 2008).
Recent advances in neural models have freed do-



2062

main experts from handcrafting features for NER
tasks. (Lample et al., 2016; Ma and Hovy, 2016;
Liu et al., 2018). Such neural models are in-
creasingly common in the domain-specific NER
tasks (Sahu and Anand, 2016; Dernoncourt et al.,
2017; Wang et al., 2018). Semi-supervised meth-
ods have been explored to further improve the ac-
curacy by either augmenting labeled datasets with
word embeddings or bootstrapping techniques in
tasks like gene name recognition (Kuksa and Qi,
2010; Tang et al., 2014; Vlachos and Gasperin,
2006). Unlike these existing approaches, our study
focuses on the distantly supervised setting without
any expert-curated training data.

Distant supervision has attracted many atten-
tions to alleviate human efforts. Originally, it was
proposed to leverage knowledge bases to super-
vise relation extraction tasks (Craven et al., 1999;
Mintz et al., 2009). AutoPhrase has demonstrated
powers in extracting high-quality phrases from
domain-specific corpora like scientific papers and
business reviews (Shang et al., 2018) but it cannot
categorize phrases into typed entities in a context-
aware manner. We incorporate the high-quality
phrases to enrich the domain-specific dictionary.

There are attempts on the distantly supervised
NER task recently (Ren et al., 2015; Fries et al.,
2017; He, 2017; Giannakopoulos et al., 2017). For
example, SwellShark (Fries et al., 2017), specif-
ically designed for biomedical NER, leverages a
generative model to unify and model noise across
different supervision sources for named entity typ-
ing. However, it leaves the named entity span
detection to a heuristic combination of dictionary
matching and part-of-speech tag-based regular ex-
pressions, which require extensive expert effort to
cover many special cases. Other methods (Ren
et al., 2015; He, 2017) also utilize similar ap-
proaches to extract entity span candidates before
entity typing. Distant-LSTM-CRF (Giannakopou-
los et al., 2017) has been proposed for the distantly
supervised aspect term extraction, which can be
viewed as an entity recognition task of a single
type for business reviews. As shown in our experi-
ments, our models can outperform Distant-LSTM-
CRF significantly on the laptop review dataset.

To the best of our knowledge, AutoNER is the
most effective model that can learn NER models
by using, and only using dictionaries without any
additional human effort.

7 Conclusion and Future Work

In this paper, we explore how to learn an effective
NER model by using, and only using dictionar-
ies. We design two neural architectures, Fuzzy-
LSTM-CRF model with a modified IOBES tag-
ging scheme and AutoNER with a new Tie or
Break scheme. In experiments on three bench-
mark datasets, AutoNER achieves the best F1
scores without additional human efforts. Its per-
formance is even competitive to the supervised
benchmarks with full human annotation. In ad-
dition, we discuss how to refine the distant super-
vision for better NER performance, including in-
corporating high-quality phrases mined from the
corpus as well as tailoring dictionary according to
the given corpus, and demonstrate their effective-
ness in ablation experiments.

In future, we plan to further investigate the
power and potentials of the AutoNER model with
Tie or Break scheme in different languages
and domains. Also, the proposed framework can
be further extended to other sequence labeling
tasks, such as noun phrase chunking. Moreover,
going beyond the classical NER setting in this
paper, it is interesting to further explore distant
supervised methods for the nested and multiple
typed entity recognitions in the future.

Acknowledgments

We would like to thank Yu Zhang from Univer-
sity of Illinois at Urbana-Champaign for provid-
ing results of supervised benchmark methods on
the BC5CDR and NCBI datasets.

Research was sponsored in part by U.S. Army
Research Lab. under Cooperative Agreement
No. W911NF-09-2-0053 (NSCTA), DARPA
under Agreement No. W911NF-17-C-0099,
National Science Foundation IIS 16-18481,
IIS 17-04532, and IIS-17-41317, DTRA HD-
TRA11810026, Google Ph.D. Fellowship and
grant 1U54GM114838 awarded by NIGMS
through funds provided by the trans-NIH
Big Data to Knowledge (BD2K) initiative
(www.bd2k.nih.gov). Any opinions, findings, and
conclusions or recommendations expressed in this
document are those of the author(s) and should
not be interpreted as the views of any U.S. Gov-
ernment. The U.S. Government is authorized to
reproduce and distribute reprints for Government
purposes notwithstanding any copyright notation
hereon.



2063

References
Mark Craven, Johan Kumlien, et al. 1999. Construct-

ing biological knowledge bases by extracting infor-
mation from text sources. In ISMB, volume 1999,
pages 77–86.

Franck Dernoncourt, Ji Young Lee, Ozlem Uzuner,
and Peter Szolovits. 2017. De-identification of pa-
tient notes with recurrent neural networks. Journal
of the American Medical Informatics Association,
24(3):596–606.

Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel S Weld, and Alexander Yates. 2005. Un-
supervised named-entity extraction from the web:
An experimental study. Artificial intelligence,
165(1):91–134.

Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In Proceedings of the 43rd annual meet-
ing on association for computational linguistics,
pages 363–370. Association for Computational Lin-
guistics.

Jason Fries, Sen Wu, Alex Ratner, and Christopher Ré.
2017. Swellshark: A generative model for biomed-
ical named entity recognition without labeled data.
arXiv preprint arXiv:1704.06360.

Athanasios Giannakopoulos, Claudiu Musat, Andreea
Hossmann, and Michael Baeriswyl. 2017. Unsuper-
vised aspect term extraction with b-lstm & crf using
automatically labelled datasets. In Proceedings of
the 8th Workshop on Computational Approaches to
Subjectivity, Sentiment and Social Media Analysis,
pages 180–188.

Daniel Hanisch, Katrin Fundel, Heinz-Theodor Mevis-
sen, Ralf Zimmer, and Juliane Fluck. 2005.
Prominer: rule-based protein and gene entity recog-
nition. BMC bioinformatics, 6(1):S14.

Wenqi He. 2017. Autoentity: automated entity de-
tection from massive text corpora. M.S. Thesis
for Computer Science of University of Illinois at
Urbana-Champaign.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Pavel P Kuksa and Yanjun Qi. 2010. Semi-supervised
bio-named entity recognition with word-codebook
learning. In Proceedings of the 2010 SIAM Inter-
national Conference on Data Mining, pages 25–36.
SIAM.

Guillaume Lample, Miguel Ballesteros, Sandeep Sub-
ramanian, Kazuya Kawakami, and Chris Dyer. 2016.
Neural architectures for named entity recognition.
In Proceedings of NAACL-HLT, pages 260–270.

Robert Leaman and Graciela Gonzalez. 2008. Ban-
ner: an executable survey of advances in biomedical
named entity recognition. In Biocomputing 2008,
pages 652–663. World Scientific.

Thomas Lin, Oren Etzioni, et al. 2012. Entity linking at
web scale. In Proceedings of the Joint Workshop on
Automatic Knowledge Base Construction and Web-
scale Knowledge Extraction, pages 84–88. Associa-
tion for Computational Linguistics.

Liyuan Liu, Jingbo Shang, Frank Xu, Xiang Ren, Huan
Gui, Jian Peng, and Jiawei Han. 2018. Empower
sequence labeling with task-aware neural language
model. AAAI.

Xuezhe Ma and Eduard Hovy. 2016. End-to-end se-
quence labeling via bi-directional lstm-cnns-crf. In
Proceedings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), volume 1, pages 1064–1074.

Mike Mintz, Steven Bills, Rion Snow, and Dan Juraf-
sky. 2009. Distant supervision for relation extrac-
tion without labeled data. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Vol-
ume 2-Volume 2, pages 1003–1011. Association for
Computational Linguistics.

Maria Pontiki, Dimitrios Galanis, John Pavlopou-
los, Haris Papageorgiou, Ion Androutsopoulos, and
Suresh Manandhar. 2014. Semeval-2014 task 4: As-
pect based sentiment analysis. In Proceedings of
Proceedings of the 8th International Workshop on
Semantic Evaluation (SemEval 2014), page 2735.

Sampo Pyysalo, Filip Ginter, Hans Moen, Tapio
Salakoski, and Sophia Ananiadou. 2013. Distribu-
tional semantics resources for biomedical text pro-
cessing. In Proceedings of the 5th International
Symposium on Languages in Biology and Medicine,
Tokyo, Japan, pages 39–43.

Lev Ratinov and Dan Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
CoNLL.

Xiang Ren, Ahmed El-Kishky, Chi Wang, Fangbo Tao,
Clare R Voss, and Jiawei Han. 2015. Clustype:
Effective entity recognition and typing by relation
phrase-based clustering. In Proceedings of the 21th
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, pages 995–1004.
ACM.

Sunil Sahu and Ashish Anand. 2016. Recurrent neu-
ral network models for disease name recognition us-
ing domain invariant features. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), vol-
ume 1, pages 2216–2225.



2064

Burr Settles. 2004. Biomedical named entity recogni-
tion using conditional random fields and rich feature
sets. In Proceedings of the international joint work-
shop on natural language processing in biomedicine
and its applications, pages 104–107. Association for
Computational Linguistics.

Jingbo Shang, Jialu Liu, Meng Jiang, Xiang Ren,
Clare R Voss, and Jiawei Han. 2018. Automated
phrase mining from massive text corpora. IEEE
Transactions on Knowledge and Data Engineering.

Buzhou Tang, Hongxin Cao, Xiaolong Wang, Qingcai
Chen, and Hua Xu. 2014. Evaluating word represen-
tation features in biomedical named entity recogni-
tion tasks. BioMed research international, 2014.

Andreas Vlachos and Caroline Gasperin. 2006. Boot-
strapping and evaluating named entity recognition in
the biomedical domain. In Proceedings of the HLT-
NAACL BioNLP Workshop on Linking Natural Lan-
guage and Biology, pages 138–145. Association for
Computational Linguistics.

Hongning Wang, Yue Lu, and ChengXiang Zhai. 2011.
Latent aspect rating analysis without aspect key-
word supervision. In Proceedings of the 17th ACM
SIGKDD international conference on Knowledge
discovery and data mining, pages 618–626. ACM.

Xuan Wang, Yu Zhang, Xiang Ren, Yuhao Zhang,
Marinka Zitnik, Jingbo Shang, Curtis Langlotz, and
Jiawei Han. 2018. Cross-type biomedical named en-
tity recognition with deep multi-task learning. arXiv
preprint arXiv:1801.09851.


