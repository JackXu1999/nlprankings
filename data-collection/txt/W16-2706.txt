Proceedings of the Sixth Named Entity Workshop, joint with 54th ACL, pages 41–46,
Berlin, Germany, August 12, 2016. c(cid:13)2016 Association for Computational Linguistics

41

Constructing a Japanese Basic Named Entity Corpus of Various Genres

Tomoya Iwakura1, Ryuichi Tachibana 2, and Kanako Komiya 3

1 Fujitsu Laboratories Ltd. 2 Commerce Link Inc. 3 Ibaraki University

Abstract

This paper introduces a Japanese Named
Entity (NE) corpus of various genres.
We annotated 136 documents in the Bal-
anced Corpus of Contemporary Written
Japanese (BCCWJ) with the eight types
of NE tags deﬁned by Information Re-
trieval and Extraction Exercise.
The
NE corpus consists of six types of gen-
res of documents such as blogs, maga-
zines, white papers, and so on, and the
corpus contains 2,464 NE tags in total.
The corpus can be reproduced with BC-
CWJ corpus and the tagging information
obtained from https://sites.google.com/
site/projectnextnlpne/en/ .

1 Introduction

Named Entity (NE) recognition is a process by
which the names of particular classes and nu-
meric expressions are recognized in text. NEs
include person names,
locations, organizations,
dates, times, and so on. NE recognition is one of
the basic technologies used in text processing, in-
cluding Information Extraction (IE), Question An-
swering (QA), and Information Retrieval (IR).

For the development of NE recognizers in early
stage, newspaper articles have been mainly used.
For example, the following data sets consist of
newspaper articles: eight types of basic Japanese
NE recognition data sets for Information Retrieval
and Extraction Exercise (IREX) (IREX Commit-
tee, 1999),
the CoNLL’03 shared task (Tjong
Kim Sang and De Meulder, 2003), an English
ﬁne-grained NE type that
includes 64 classes
(Weischedel and Brunstein, 2005), and Sekine’s
extended NE hierarchy that includes about 200
classes of NEs (Sekine et al., 2002).

As for Sekine’s extended NE hierarchy, NE
corpus have been created on various genres doc-
uments such as blogs, white papers and so on,
in BCCWJ (Maekawa et al., 2010).1 However,
compared with the corpus for Sekine’s extended
NE hierarchy, which covers several genres, cor-
pus for Japanese basic NEs have been created for
fewer genres of documents such as newspaper arti-
cles of IREX and leading sentences of Web pages
(Hangyo et al., 2012).

This paper introduces a Japanese Named En-
tity (NE) corpus of various genres called BCCWJ
Basic NE corpus. BCCWJ Basic NE corpus was
created for the sake of expanding genres of docu-
ments for Japanese basic NE researches. The cor-
pus includes 136 documents in the Balanced Cor-
pus of Contemporary Written Japanese (BCCWJ)
core data annotated with the eight types of NE tags
deﬁned by IREX. The corpus contains 2,464 NE
tags in total and the genres of the documents are
following: Yahoo! Chiebukuro (OC)2, White Pa-
per (OW), Yahoo! Blog (OY), Books (PB)，Mag-
azines (PM) and Newspapers (PN). This corpus in-
cludes genres of documents that have not been tar-
geted in existing NE corpus for IREX deﬁnition.
(IREX Committee, 1999; Hangyo et al., 2012).

2 IREX NE Deﬁnition

IREX committee deﬁned the eight NE types: AR-
TIFACT, LOCATION, ORGANIZATION, PER-
SON, DATE, MONEY, PERCENT and TIME. Ta-
ble 1 shows the eight NE types and their examples.
In addition to the eight NE types, OPTIONAL, for
ambiguous NEs, were deﬁned.

For example, from the following sentence, “Mr.
Miyazaki comes from Miyazaki.” in English, an

1The IREX deﬁnition is not a subset of the Sekine’s ex-

tended NE hierarchy.

on the Web.

2Yahoo! Chiebukuro consists documents from a QA site

42

Example
Nobel Prize
Japan

NE type
ARTIFACT
LOCATION
ORGANIZATION Foreign Ministry
PERSON
DATE
MONEY
PERCENT
TIME

Tom White
May, 5th
100 yen
100%
10:00 p.m.

Table 1: The eight NE types deﬁned by IREX and the examples.

NE recognizer should extract the ﬁrst Miyazaki as
PERSON and the second one as LOCATION be-
cause NE types are decided by context in the IREX
deﬁnition.
⟨P ER⟩ 宮崎 ⟨/P ER⟩
⟨LOC⟩ 宮崎 ⟨/LOC⟩

さん
(Mr.)
出身

(postposition)

(Miyazaki)

は

(Miyazaki)

(comes from)

PER and LOC in the above example indicate PER-
SON and LOCATION.

3 BCCWJ Basic NE corpus

We annotated 136 documents included in BC-
CWJ core data with IREX-deﬁned NE tags by
the following procedure.3 We choose the same
documents of a Japanese morphological analysis
corps.4

• Initial annotation: Six annotators, the authors
and three university students, annotated all
the documents with NEs. Each document
was annotated by only a member.

• Modiﬁcation: Four of the annotators checked
all the annotated documents again and mod-
iﬁed annotation errors. Annotation disagree-
ments are resolved based on discussion of an-
notators.
• Packaging： We prepared a package only in-
cluding annotated tags with the positions in
each documents. Users having BCCWJ can
reproduce the BCCWJ Basic NE corpus with
the package.

3We referred the annotation guideline careted by IREX
committee: https://nlp.cs.nyu.edu/irex/NE/df990214.txt .
This site is only Japanese.

4http://plata.ar.media.kyoto-u.ac.jp/mori/research/

NLR/JDC/ClassA-1.list

Table 2 shows the number of documents and
NE tags of each genre in BCCWJ Basic NE cor-
pus. For comparing purpose, the statistics of IREX
data. The number of documents of BCCWJ Ba-
sic NE corpus is more than the sum of the num-
ber of the IREX evaluation data: GENERAL data,
ARREST DATA. In addition, BCCWJ Basic NE
corpus includes documents other than newspapers
such as Yahoo! Chiebukuro and White Paper.

Table 3 shows the statistics of BCCWJ Basic
NE corpus. Table 4 shows the percentage of each
NE in a genre. We see from these statistics that
BCCWJ Basic NE corpus has different property
compared IREX. For example, we see that Yahoo!
Chiebukuro and White Paper include more AR-
TIFACT than newspapers and Magazine includes
more PERSON than the other genres.
4 Example Uses of BCCWJ Basic NE

corpus

This section describes some example uses of BC-
CWJ Basic NE corpus.

4.1 Evaluation of an NE recognizer
We evaluated KNP that extracts the eight types of
NEs listed in Table 1 based on the IREX deﬁni-
tion. KNP is one of the freely available state of
the art NE recognizers. We used Japanese mor-
phological analyzer JUMAN version 7.01 5 as a
morphological analyzer of KNP version 4.12 6.

Table 5 shows accuracy of KNP on BCCWJ Ba-
sic NE corpus. KNP was evaluated with Recall,
Precision and F-measure:

• Recall = NUM / (the number of correct NEs)
• Precision = NUM / (the number of words and

word chunks recognized as NEs by KNP)

5http://nlp.ist.i.kyoto-u.ac.jp/index.php?JUMAN
6http://nlp.ist.i.kyoto-u.ac.jp/index.php?KNP

43

Genre

the number of documents

the number of NEs

BCCWJ

Yahoo! Chiebukuro (OC)

White Paper (OW)
Yahoo! Blog (OY)

Books (PB)

Magazines (PM)
Newspapers (PN)

Total

Genre
CRL
DRY
NET
AT
AR
GE
Total

74
8
34
5
2
13
136

IREX

175
656
307
399
319
705

2,561 (2,464)

the number of documents

the number of NEs

1174
36
46
23
20
72

1,371

19,262

832
973
466
397
1,667

23,597 (22,822)

Table 2: The number of NEs in BCCWJ Basic NE corpus and the IREX data set. The documents
of NEs for IREX data is the number of news articles.
IREX data set consists of the following data
created from Mainichi Shimbun news articles: CRL NE DATA.idx (CRL)，DRYRUN03.idx (DRY)，
NEtraining981031.idx (NET)，ARREST TRAIN.idx (AT)，ARREST01.idx (AR)，GENERAL03.idx
(GE). The numbers between parentheses in Total columns indicate the number of NEs excluding OP-
TIONAL.

• F-measure = 2 × Recall × Precision / ( Re-

call + Precision )

where NUM is the number of correct NEs recog-
nized by KNP.

Compared with Newspapers, KNP showed
lower performance on Yahoo! Chiebukuro and
Yahoo! Blog. One of the reasons seems that
KNP was trained with IREX CRL data that con-
sists of news articles. Another reason is Yahoo!
Chiebukuro and Yahoo! Blog includes more ab-
breviations and colloquial expressions than news-
papers. Furthermore, KNP also showed lower per-
formance on White Paper even if White Paper doc-
uments were written language. One of the rea-
sons seems that White Paper includes more AR-
TIFACT NEs than Newspapers The accuracy of
KNP for ARTIFACT was lower than the other NEs
on Newspapers.

From this evaluation, we see that we can eval-
uate NE recognizers with different perspective by
using different genres of documents.

4.2 The Other Expected Use
We also expect that BCCWJ Basic NE corpus con-
tributes to the following research.

• NE recognition for colloquial expressions:
Yahoo! Blog contributes to NE recognition
researches for colloquial expressions because
Yahoo! Blog includes more colloquial ex-
pressions than Newspapers and White Paper
• Domain Adaptation: BCCWJ Basic NE cor-
pus includes six genres of documents, there-
fore, we expect BCCWJ Basic NE corpus is
useful for the research of domain adaptation
(Daum´e III, 2007).

• Revision learning for NE recognition: We
also have uploaded not only latest annotation
but also older versions of NE annotation re-
sults. Therefore, we can use the corpus as an
error detection research or revision learning
like Japanese morphological analysis (Naka-
gawa et al., 2002).

• Comparison of annotation performance on
different genres of documents: We can use

44

BCCWJ

Genre ART DATE LOC MON OPT ORG PERC PERS TIME
OC
OW
OY
PB
PM
PN
Total

54
163
25
29
13
24
308

19
129
60
50
42
165
465

57
140
52
87
32
188
557

19
128
61
26
17
118
369

6
15
79
169
203
78
550

9
9
7
0
5
59
89

0
33
11
6
1
38
89

3
0
3
8
2
22
37

8
39
9
24
4
13
97
IREX

Data
CRL
DRY
NET
AT
AR
GE
Total

ART DATE LOC MON OPT ORG PERC PERS TIME
747
42
67
11
13
49
929

5463
192
255
165
106
416
6597

3567
110
137
69
72
277
4232

3676
214
270
80
74
389
4703

3840
169
138
94
97
355
4693

390
33
32
19
8
15
497

585
42
47
7
8
86
775

492
6
19
3
0
21
541

502
24
8
18
19
59
630

Table 3: The number of NEs in BCCWJ Basic NE corpus. ART, LOC, MON, OPT, ORG, PERC
and PERS indicate ARTIFACT, LOCATION, MONEY, OPTIONAL, ORGANIZATION, PERCENT and
PERSON, respectively. The others are same as ones in Table 2.

this corpus for evaluating annotation perfor-
mance and annotation methods on different
genres of documents. One of the examples is
described in (Komiya et al., 2016). The pa-
per compared the following two methods to
annotate a corpus via non-expert annotators
for named entity (NE) recognition task. The
ﬁrst one is an annotation method by revising
the results of an existing NE recognizer. The
other is an annotation method by hand from
the beginning.

5 Conclusion

This paper introduced a Japanese Named Entity
(NE) corpus of various genres called BCCWJ Ba-
sic NE corpus. We annotated 136 documents in
the BCCWJ with the eight types of NE tags de-
ﬁned by IREX. Users having BCCWJ can repro-
duce use the corpus by using the annotation in-
formation of the corpus distributed at a web site.
Users having BCCWJ can reproduce use the cor-
pus by using the annotation information of the cor-
pus distributed at the following web site: https://
sites.google.com/site/projectnextnlpne/en/ .

Acknowledgments

We appreciate Ai Hirata, Maiko Yamazaki and
Masaaki Ichihara for their help of building this
corpus.

References
Hal Daum´e III. 2007. Frustratingly easy domain adap-

tation. In Proc. of ACL’07.

Masatsugu Hangyo, Daisuke Kawahara, and Sadao
Kurohashi.
Building a diverse docu-
ment leads corpus annotated with semantic relations.
pages 535–544.

2012.

IREX Committee. 1999. Proc. of the IREX workshop.

Kanako Komiya, Masaya Suzuki, Tomoya Iwakura,
Minoru Sasaki, and Hiroyuki Shinno. 2016. Com-
parison of annotating methods for named entity cor-
pora. In Proc. of LAW-X.

Kikuo Maekawa, Makoto Yamazaki, Takehiko
Maruyama, Masaya Yamaguchi, Hideki Ogura,
Wakako Kashino, Toshinobu Ogiso, Hanae Koiso,
and Yasuharu Den. 2010. Design, compilation,
and preliminary analyses of balanced corpus of
contemporary written japanese.
In Proceedings
of
the International Conference on Language
Resources and Evaluation, LREC 2010, 17-23 May
2010, Valletta, Malta.

45

DATE

LOC

MON OPT

ORG

BCCWJ

Genre ART
OC
OW
OY
PB
PM
PN

30.86% 10.86% 32.57% 5.14% 4.57% 10.86% 0%
24.85% 19.66% 21.34% 1.37% 5.95% 19.51% 5.03% 2.29% 0%
8.14% 19.54% 16.94% 2.28% 2.93% 19.87% 3.58% 25.74% 0.98%
7.27% 12.53% 21.80% 0%
6.02% 6.52% 1.50% 42.35% 2.01%
4.08% 13.17% 10.03% 1.57% 1.25% 5.33% 0.31% 63.63% 0.63%
3.40% 23.40% 26.68% 8.37% 1.84% 16.74% 5.39% 11.06% 3.12%

PERC PERS

TIME
3.43% 1.71%

IREX

LOC

DATE

MON OPT

Genre ART
CRL
DRY
NET
AT
AR
GE

TIME
3.88% 18.52% 28.36% 2.02% 3.04% 19.08% 2.55% 19.94% 2.61%
5.05% 13.22% 23.08% 3.97% 5.05% 25.72% 0.72% 20.31% 2.88%
6.89% 14.08% 26.21% 3.29% 4.83% 27.75% 1.95% 14.18% 0.82%
2.36% 14.81% 35.41% 4.08% 1.50% 17.17% 0.64% 20.17% 3.86%
3.27% 18.14% 26.69% 2.02% 2.02% 18.64% 0%
24.43% 4.79%
2.94% 16.62% 24.95% 0.90% 5.16% 23.33% 1.26% 21.30% 3.54%

PERC PERS

ORG

Table 4: The percentage of each NE in a genre. The meanings of items are same as ones in Table 2.

Tetsuji Nakagawa, Taku Kudo, and Yuji Matsumoto.
2002. Revision learning and its application to part-
of-speech tagging. In Proc. of ACL’02, pages 497–
504.

Satoshi Sekine, Kiyoshi Sudo, and Chikashi Nobata.
2002. Extended named entity hierarchy. In Proc. of
LREC’02.

Erik F. Tjong Kim Sang and Fien De Meulder.
2003.
Introduction to the conll-2003 shared task:
Language-independent named entity recognition. In
Proc. of CoNLL’03, pages 142–147.

R. Weischedel and A. Brunstein. 2005. Bbn pronoun
coreference and entity type corpus. linguistic data
consortium.

46

NE / Genre
ARTIFACT

DATE

LOCATION

MONEY

ORGANIZATION

Yahoo! Chiebukuro
12.70 (7.41, 44.44)
68.42 (68.42, 68.42)
82.69 (75.44, 91.49)

100.00 (100.00, 100.00)

33.33 (26.32, 45.45)

PERCENT
PERSON

Total

NE / Genre
ARTIFACT

DATE

LOCATION

ORGANIZATION

PERCENT
PERSON

TIME
Total

NE / Genre
ARTIFACT

DATE

LOCATION

MONEY

ORGANIZATION

PERCENT
PERSON

TIME
Total

0 (0, 0)

33.33 (50.00, 25.00)
56.20 (46.11, 71.96)

Yahoo! Blog

10.53 (8.00, 15.38)
71.58 (56.67, 97.14)
68.00 (65.38, 70.83)
50.00 (42.62, 60.47)
95.24 (90.91, 100.00)
68.75 (69.62, 67.90)
50.00 (33.33, 100.00)
63.06 (56.71, 71.01)

Magazines

72.73 (61.54, 88.89)
86.08 (80.95, 91.89)
28.07 (50.00, 19.51)

100.00 (100.00, 100.00)

66.67 (64.71, 68.75)

100.00 (100.00, 100.00)

62.82 (53.69, 75.69)
50.00 (50.00, 50.00)
60.56 (58.73, 62.50)

White Paper

45.69 (32.52, 76.81)
77.52 (77.52, 77.52)
86.47 (82.14, 91.27)
88.89 (88.89, 88.89)
70.83 (79.69, 63.75)
96.88 (93.94, 100.00)
59.57 (93.33, 43.75)
72.12 (68.56, 76.08)

Books

48.78 (34.48, 83.33)
51.69 (46.00, 58.97)
57.99 (56.32, 59.76)
39.13 (34.62, 45.00)
60.00 (50.00, 75.00)
72.67 (66.86, 79.58)
80.00 (75.00, 85.71)
62.56 (56.80, 69.61)

Newspapers

37.50 (37.50, 37.50)
86.24 (85.45, 87.04)
86.11 (82.01, 90.64)
94.12 (94.92, 93.33)
70.05 (64.41, 76.77)
93.15 (89.47, 97.14)
87.34 (88.46, 86.25)
72.73 (57.14, 100.00)
82.70 (79.77, 85.85)

Table 5: Accuracy of KNP on BCCWJ Basic NE corpus. The value indicates F-measure (Recall, Preci-
sion)．

