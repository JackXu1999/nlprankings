



















































UC Davis at SemEval-2019 Task 1: DAG Semantic Parsing with Attention-based Decoder


Proceedings of the 13th International Workshop on Semantic Evaluation (SemEval-2019), pages 119–124
Minneapolis, Minnesota, USA, June 6–7, 2019. ©2019 Association for Computational Linguistics

119

UC Davis at SemEval-2019 Task 1: DAG Semantic Parsing with
Attention-based Decoder

Dian Yu
University of California, Davis
dianyu@ucdavis.edu

Kenji Sagae
University of California, Davis
sagae@ucdavis.edu

Abstract

We present a simple and accurate model for
semantic parsing with UCCA as our submis-
sion for SemEval 2019 Task 1. We propose an
encoder-decoder model that maps strings to di-
rected acyclic graphs. Unlike many transition-
based approaches, our approach does not use
a state representation, and unlike graph-based
parsers, it does not score graphs directly.
Instead, we encode input sentences with a
bidirectional-LSTM, and decode with self-
attention to build a graph structure. Results
show that our parser is simple and effective for
semantic parsing with reentrancy and discon-
tinuous structures.

1 Introduction

Semantic parsing aims to capture structural re-
lationships between input strings and graph rep-
resentations of sentence meaning, going beyond
concerns of surface word order, phrases and re-
lationships. The focus on meaning rather than
surface relations often requires the use of reen-
trant nodes and discontinuous structures. Uni-
versal Conceptual Cognitive Annotation (UCCA)
(Abend and Rappoport, 2013) is designed to sup-
port semantic parsing with mappings between sen-
tences and their corresponding meanings in a
framework intended to be applicable across lan-
guages.

SemEval 2019 Task 1 (Hershcovich et al.,
2018b, 2019) focuses on semantic parsing of texts
into graphs consisting of terminal nodes that repre-
sent words, non-terminal nodes that represent in-
ternal structure, and labeled edges representing re-
lationships between nodes (e.g. participant, cen-
ter, linker, adverbial, elaborator), according to
the UCCA scheme. Annotated datasets are pro-
vided, and participants are evaluated in four set-
tings: English with domain-specific data, English

with out-of-domain data, German with domain-
specific data, and French with only development
and test data, but no training data. Additionally,
there are open and closed tracks, where the use of
additional resources is and is not allowed, respec-
tively. Our entry in the task is limited to the closed
track and the first setting, domain-specific English
using the Wiki corpus, where the relatively small
dataset (4113 sentences for training, 514 for devel-
opment, and 515 for testing) consists of annotated
sentences from English Wikipedia.

Our model follows the encoder-decoder archi-
tecture commonly used in state-of-the-art neural
parsing models (Kitaev and Klein, 2018; Kiper-
wasser and Goldberg, 2016b; Cross and Huang,
2016; Chen and Manning, 2014). However, we
propose a very simple decoder architecture that
relies only on a recursive attention mechanism
of the encoded latent representation. In other
words, the decoder does not require state encod-
ing and model-optimal inference whatsoever. Our
novel model achieved a macro-averaged F1-score
of 0.753 in labeled primary edges and 0.864 in
unlabeled primary edge prediction on the test set.
The results confirm the suitability of our proposed
model to the semantic parsing task.

2 Related work

Leveraging parallels between UCCA and known
approaches for syntactic parsing, Hershcovich
et al. (2017) proposed TUPA, a customized
transition-based parser with dense feature repre-
sentation. Based on this model, Hershcovich
et al. (2018a) used multitask learning effectively
by training a UCCA model along with similar
parsing tasks where more training data is avail-
able, such as Abstract Meaning Representation
(AMR) (Banarescu et al., 2013) and Universal De-
pendencies (UD) (Nivre et al., 2016). Due to



120

Figure 1: Illustration of the decoder for the beginning of a sentence, “Mariah Carey turned it down, and ...”.
Each vi represents the context embedding for each word i from the BiLSTM encoder. Words on edges represent
category labels between nodes, where A is participant and P is process. Circles represent nodes in the graph, each
with a pair in indices. Circles with 0 as the first index are terminal nodes, and circles with 1 as the first index
are non-terminal nodes. (1). Dashed green lines represent the attention mechanism for the word Carey, which
forms a continuous proper noun “Mariah Carey”. (2). Dashed red lines represent the attention mechanism for the
word down, which forms a discontinuous unit “turned ... down”. (3). Dotted blue lines represent the attention
mechanism for node1.4. The darker the color, the higher the attention score.

the requirements of reentrancy, discontinuity, and
non-terminals, other powerful parsers were shown
to be less suitable for parsing with UCCA (Hersh-
covich et al., 2017).

3 Parsing Model

BiLSTM models are capable of providing feature
representations with sequential data, and atten-
tion mechanisms (Vaswani et al., 2017) have been
applied successfully to parsing tasks (Kitaev and
Klein, 2018). Inspired by their success, our model
uses a BiLSTM encoder and a self-attention de-
coder. The encoder represents each node (terminal
and non-terminal) in the DAG without the need
to encode features and the current parser state.
The proposed decoder takes the encoded repre-
sentation as the configuration and uses attention
mechanism. Without any additional feature ex-
traction, it serves a similar role as an oracle and a
transition-system in transition-based parsers. We
jointly train a label prediction model and a discon-
tinuity prediction model. We predict remote edges
with a different encoder. An example of the pars-
ing model can be seen in Figure 1.

3.1 Terminal Nodes
To mitigate sparsity due to the small amount of
training data available, we concatenate part-of-
speech tags embeddings to word embeddings in
terminal nodes. In addition, because the connec-
tions between terminal nodes and non-terminal
nodes often require identification of named enti-

ties, we also added entity type and case informa-
tion as additional knowledge. Given a sentence
x = x1, ..., xn, the vector for each input token is
thus represented as ui = emb(xi) ◦ emb(posi) ◦
emb(entity typei)◦emb(casei), where casei is 1
if the first character of the word is capitalized and
0 otherwise. We use pretrained word embeddings
from fastText1 for emb(xi). POS tags and entity
types are predicted using external models2 and are
provided in the training corpus. Each word repre-
sentation from the encoder is vi = BiLSTM(ui).
We assign these contextual word embeddings as
vectors to terminal nodes.

3.2 Non-terminal Nodes

For non-terminal nodes with only one terminal
node as the child, the representation is the same as
its corresponding terminal node, i.e. a contextual
word embedding from the BiLSTM encoder. For
other non-terminal nodes that have more than one
terminal children or non-terminal children (i.e.
represent more than one word in the text), we use
a span representation. Following Cross and Huang
(2016), we represent the span between the words
xi, xj as vi,j = (fj−fi)◦(bi−bj) where f0, ..., fn
and b0, ..., bn are the output of the forward and
backward directions in the BiLSTM, respectively.
However, the linear subtractions from a nonlinear
recurrent neural network (RNN) as a span approx-
imation is not intuitive. Instead, we experimented

1https://fasttext.cc/
2https://spacy.io/

https://fasttext.cc/
https://spacy.io/


121

with an additional BiLSTM on the target span
xi, xi+1, ..., xj , similar to the recursive tree repre-
sentations in (Socher et al., 2013; Kiperwasser and
Goldberg, 2016a) but replaced the feed-forward
network with an LSTM. In our experiments with
the small dataset in the closed track of the English
domain-specific track, this method did not result
in improved performance.

3.3 Attention Mechanism For Decoding

Our basic decoding model is inspired by the global
attention mechanism used in machine translation.
The attention averages the encoded state in each
time step in the sequence with trainable weights
(Luong et al., 2015). We set a maximum sequence
length and calculate the attention weights (in prob-
ability) for the left boundary index of the span
given the node representation vi,j (i ≤ j):

hspan =MLP (vi,j) (1)

pleft boundary = softmax(hspan) (2)

where MLP is a multilayer perceptron and hspan
is of size (1, max sequence length). We choose
argmaxi pleft boundary as the index of the left
boundary of the predicted span. Let jl denote the
index of the left most child of the node j (for ex-
ample, in Figure 1, jl for node1.5 is 1 and jl for
node1.6 is 6)3. If i ≥ jl, then the node attends
to itself to indicate that a span cannot be created
yet (as is the case for node1.6 in Figure 1). Oth-
erwise, there is a span that forms a semantic unit
and we need to create a parent node. For example,
i = 1 for the node1.4, so we create a new node1.5
which connects the nodes within the span [1: 5],
i.e. node1.1, node1.3, and node1.4.

We do this recursively to attend to a previous in-
dex until the node attends to itself. Then we repeat
the procedure on the next word in the sequence.
The illustration is shown in Figure 1 with dotted
blue lines. The algorithm is presented in Algo-
rithm 1 below. primary parent indicates the par-
ent node to which the current node is not a remote
child (in the DAG setting, a child node may have
multiple parents). We set the maximum number of
recurrence to be 7 to prevent excessive node cre-
ation during inference.

Despite its simplicity, there are two limitations
to this method. One is the restriction of the maxi-
mum sequence length. The other is the distinction

3For simplicity, word indices start at 1 in the Figure.

Algorithm 1 Index-attention decoder
1: for recur num = 1 to max recur do
2: if i ≥ jl then
3: break
4: end if
5: hspan =MLP (vi,j)
6: iattn = argmax

i
softmax(hspan)

7: i = primary parent(viattn)l
8: end for

between the indices and the actual words in each
sentence. The model may cheat during training to
attend to specific indices regardless of the actual
words in these indices.

Motivated by the success of biaffine atten-
tion(Dozat and Manning, 2016) and self-attention
models (Vaswani et al., 2017), we replace the in-
dex attention decoder with a multiplication model
where we can leverage fast optimized matrix mul-
tiplication. Similar to the left most child, let jr
denote the index of the right most child of nodej .
vo = v[1 : jr] where v is the output from the
encoder of size (sequence length, batch size, hid-
den size). The scoring function is defined as:

hi = ReLU(W × vi + b) (3)

ho = ReLU(W × vo + b) (4)

mm = matrix multiplication(hi, h
T
o ) (5)

pleft boundary = softmax(mm) (6)

Compared to the index attention decoder above,
this decoder considers both the index and the span
representation and thus is more flexible and ro-
bust to new texts. The recurrence call remains the
same by replacing line 5 and 6 in Algorithm 1 with
equations 3− 6.

3.4 Label Prediction

Contextual information is important to label pre-
diction. For instance, in the sentence “It an-
nounced Carey returned to the studio to start ... ”,
the phrase “Carey returned to the studio” should
be labeled as a participant (A) instead of a scene
(H) according to the context. Ideally the encoder
will capture the information from the whole sen-
tence so that we only need the current span to pre-
dict its label (since the span has the context infor-
mation from both sides). However, as shown in



122

previous research with RNN models, the contex-
tual information is lost for a relatively long sen-
tence. Therefore, similar to the label prediction
problem with dependency parsers, we use a MLP
to predict the label of a span vi,j given its context
p = primary parent(vi,j).

h = ReLU(W 1l × (p ◦ vi,j) + b1l ) (7)

l = argmax
l

softmax(W 2l ∗ h+ b2l ) (8)

We also experimented with only using span repre-
sentation as seen in constituency parsing (Gaddy
et al., 2018) by replacing (p◦vi,j) with vi,j in equa-
tion 7. Surprisingly, this increased the F1 score on
the development set by 1.4 points. We conjecture
that this is due to the limited amount of training
data, which makes it more difficult to learn noisier
representations.

3.5 Discontinuous Unit

After finding the left boundary of the current span
unit as shown in section 3.3, we use two MLPs
for binary classification to check (1) if the span
forms a proper noun with which we need to com-
bine multiple terminal nodes to one non-terminal
node (as “Mariah Carey” in Figure 1) and (2) if the
span forms a discontinuous unit (such as “turn ...
down” in Figure 1).

probpropn =W
2
p ×ReLU(W 1p × vi,j + b1p) + b2p

(9)
probdiscont =W

2
d ×ReLU(W 1d × vi,j + b1d) + b2d

(10)
If the node span attends to a node in the left

and the model predicts a proper noun, we will cre-
ate a non-terminal node and links all the terminal
nodes i, i+1, ..., j as its terminal children (shown
as dashed green lines in Figure 1).

If the model predicts that the span is a discon-
tinuous unit, instead of connecting all the terminal
nodes as its children, the new created node only
connects nodei and nodej , and do the recurrence
checks afterwards as shown in Algorithm 1 (illus-
trated as dashed red lines in Figure 1).

3.6 Remote Edges

We predict remote edges the same way as the ma-
trix multiplication decoder for primary edges. We
use a different BiLSTM encoder to learn represen-
tations and avoid confusion between attention to
primary edges and remote edges.

unlabeled(F1) labeled(F1)
official 0.746 0.866
+ max recur = 7 0.747 0.867
+ child pred 0.760 0.87
+ β2 = 0.9 0.762 0.87
+ bug fix 0.769 0.873

Table 1: F1 score on primary edges evaluated on the
development set

4 Training and Inference

During training, nodei attends to the left most
child of its primary parent (nodep) recursively un-
til nodep is not the left most child of nodep’s par-
ent. Because a span representation contains infor-
mation from both left to right and right to left,
nodei with the highest attention score not only
contains the embedding of its terminal node, but
also the span between index i and j in the text. We
use cross entropy loss to jointly train for embed-
dings, the BiLSTM encoder, and the decoder.

For inference, we take the output of each token
in the text from the BiLSTM encoder as input and
create a non-terminal node for each terminal node.
We create a new node when the token embedding
attends to a different token outside of the current
span boundary. The recurrence algorithm for each
newly created non-terminal node shown in Algo-
rithm 1 is applied.

5 Experiments

For the encoder, we use a 2-layer, 500 dimensional
BiLSTM with 0.2 dropout. The word embedding
size is 300 with feature embedding size of 20 each
(pos tagging, entity type, and case information).
We use Adam optimizer (Kingma and Ba, 2014)
with β2 set to 0.9 as suggested by Dozat and Man-
ning (2016). Development set is used for early
stopping. Because of the small dataset (4113 train-
ing sentences), the model overfits after 4 epochs.

6 Results

Table 1 provides the results on the development set
and Table 2 shows the results on the test set. offi-
cial shows results of the model we submitted to
the competition with a maximum recursion num-
ber of 5 and a β2 = 0.99. We obtained higher
scores by increasing the recursion limit as in sec-
tion 3.3 (+ max revur = 7), using current span only



123

primary remote
unlabeled labeled unlabeled labeled

baseline 0.733 0.858 0.472 0.484
official 0.73 0.864 - -

final 0.753 0.864 0.447 0.447

Table 2: F1 score on primary and remote edges re-
ported on the test set

as explained in section 3.4 (+ child pred), chang-
ing β2 as shown in section 5 (+ β2 = 0.9) and fix-
ing minor bugs (+ bug fix) incrementally. baseline
shows the results of the baseline model (TUPA)
from Hershcovich et al. (2017). final shows the re-
sults of the model fine-tuned on the development
set mentioned in Table 1.

Since there are normally 0 or 1 remote edges in
each sentence in the training corpus, the remote
edge prediction model is not as effective. Still, the
model captures some remote relations. For exam-
ple, in the sentence “Additionally, Carey’s newly
slimmed Figure began to change, as she stopped
her exercise routines and gained weight”, the node
“gained weight” is predicted to point to “Carey”
where the target annotated remote child is “she”.
Discontinuous unit prediction also suffers from the
problem of insufficient training samples.

7 Conclusion

This paper describes the system that the UC Davis
team submitted to SemEval 2019 Task 1. We pro-
pose a recursive self-attention decoder with a sim-
ple architecture. Our model is effective in UCCA
semantic parsing, ranking third in the close track
in-domain task with modest fine-tuning, highlight-
ing the suitability of our approach.

Acknowledgments

This work was supported by the National Science
Foundation under Grant No. 1840191. Any opin-
ions, findings, and conclusions or recommenda-
tions expressed are those of the authors and do not
necessarily reflect the views of the NSF.

References
Omri Abend and Ari Rappoport. 2013. Universal con-

ceptual cognitive annotation (ucca). In ACL (1),
pages 228–238. The Association for Computer Lin-
guistics.

Laura Banarescu, Claire Bonial, Shu Cai, Madalina
Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin
Knight, Philipp Koehn, Martha Palmer, and Nathan
Schneider. 2013. Abstract meaning representation
for sembanking.

Danqi Chen and Christopher D. Manning. 2014. A fast
and accurate dependency parser using neural net-
works. In EMNLP, pages 740–750. ACL.

James Cross and Liang Huang. 2016. Span-based
constituency parsing with a structure-label system
and provably optimal dynamic oracles. CoRR,
abs/1612.06475.

Timothy Dozat and Christopher D. Manning. 2016.
Deep biaffine attention for neural dependency pars-
ing. CoRR, abs/1611.01734.

David Gaddy, Mitchell Stern, and Dan Klein. 2018.
What’s going on in neural constituency parsers? an
analysis. CoRR, abs/1804.07853.

Daniel Hershcovich, Omri Abend, and Ari Rappoport.
2017. A transition-based directed acyclic graph
parser for UCCA. In Proceedings of the 55th An-
nual Meeting of the Association for Computational
Linguistics, ACL 2017, Vancouver, Canada, July 30
- August 4, Volume 1: Long Papers, pages 1127–
1138.

Daniel Hershcovich, Omri Abend, and Ari Rappoport.
2018a. Multitask parsing across semantic represen-
tations. In Proceedings of the 56th Annual Meet-
ing of the Association for Computational Linguis-
tics, ACL 2018, Melbourne, Australia, July 15-20,
2018, Volume 1: Long Papers, pages 373–385.

Daniel Hershcovich, Zohar Aizenbud, Leshem
Choshen, Elior Sulem, Ari Rappoport, and Omri
Abend. 2019. Semeval 2019 task 1: Cross-
lingual semantic parsing with UCCA. CoRR,
abs/1903.02953.

Daniel Hershcovich, Leshem Choshen, Elior Sulem,
Zohar Aizenbud, Ari Rappoport, and Omri Abend.
2018b. Semeval 2019 shared task: Cross-lingual se-
mantic parsing with UCCA - call for participation.
CoRR, abs/1805.12386.

Diederik P. Kingma and Jimmy Ba. 2014. Adam:
A method for stochastic optimization. CoRR,
abs/1412.6980.

Eliyahu Kiperwasser and Yoav Goldberg. 2016a. Easy-
first dependency parsing with hierarchical tree lstms.
CoRR, abs/1603.00375.

Eliyahu Kiperwasser and Yoav Goldberg. 2016b. Sim-
ple and accurate dependency parsing using bidi-
rectional LSTM feature representations. CoRR,
abs/1603.04351.

Nikita Kitaev and Dan Klein. 2018. Constituency
parsing with a self-attentive encoder. CoRR,
abs/1805.01052.

http://dblp.uni-trier.de/db/conf/acl/acl2013-1.html#AbendR13
http://dblp.uni-trier.de/db/conf/acl/acl2013-1.html#AbendR13
http://dblp.uni-trier.de/db/conf/emnlp/emnlp2014.html#ChenM14
http://dblp.uni-trier.de/db/conf/emnlp/emnlp2014.html#ChenM14
http://dblp.uni-trier.de/db/conf/emnlp/emnlp2014.html#ChenM14
http://arxiv.org/abs/1612.06475
http://arxiv.org/abs/1612.06475
http://arxiv.org/abs/1612.06475
http://arxiv.org/abs/1611.01734
http://arxiv.org/abs/1611.01734
http://arxiv.org/abs/1804.07853
http://arxiv.org/abs/1804.07853
https://doi.org/10.18653/v1/P17-1104
https://doi.org/10.18653/v1/P17-1104
https://aclanthology.info/papers/P18-1035/p18-1035
https://aclanthology.info/papers/P18-1035/p18-1035
http://arxiv.org/abs/1903.02953
http://arxiv.org/abs/1903.02953
http://arxiv.org/abs/1805.12386
http://arxiv.org/abs/1805.12386
http://arxiv.org/abs/1412.6980
http://arxiv.org/abs/1412.6980
http://arxiv.org/abs/1603.00375
http://arxiv.org/abs/1603.00375
http://arxiv.org/abs/1603.04351
http://arxiv.org/abs/1603.04351
http://arxiv.org/abs/1603.04351
http://arxiv.org/abs/1805.01052
http://arxiv.org/abs/1805.01052


124

Minh-Thang Luong, Hieu Pham, and Christo-
pher D. Manning. 2015. Effective approaches to
attention-based neural machine translation. CoRR,
abs/1508.04025.

Joakim Nivre, Marie-Catherine de Marneffe, Filip
Ginter, Yoav Goldberg, Jan Hajic, Christopher D.
Manning, Ryan T. McDonald, Slav Petrov, Sampo
Pyysalo, Natalia Silveira, Reut Tsarfaty, and Daniel
Zeman. 2016. Universal dependencies v1: A mul-
tilingual treebank collection. In Proceedings of
the Tenth International Conference on Language
Resources and Evaluation LREC 2016, Portorož,
Slovenia, May 23-28, 2016.

R Socher, A Perelygin, J.Y. Wu, J Chuang, C.D. Man-
ning, A.Y. Ng, and C Potts. 2013. Recursive deep
models for semantic compositionality over a senti-
ment treebank. EMNLP, 1631:1631–1642.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. CoRR, abs/1706.03762.

http://arxiv.org/abs/1508.04025
http://arxiv.org/abs/1508.04025
http://www.lrec-conf.org/proceedings/lrec2016/summaries/348.html
http://www.lrec-conf.org/proceedings/lrec2016/summaries/348.html
http://arxiv.org/abs/1706.03762
http://arxiv.org/abs/1706.03762

