



















































Exploiting Contextual Information via Dynamic Memory Network for Event Detection


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1030–1035
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

1030

Exploiting Contextual Information via Dynamic Memory Network for
Event Detection

Shaobo Liu†§, Rui Cheng†§, Xiaoming Yu†, Xueqi Cheng†
†CAS Key Laboratory of Network Data Science and Technology,

Institute of Computing Technology, Chinese Academy of Science (CAS);
§School of Computer and Control Engineering, The University of CAS
{liushaobo, chengrui}@software.ict.ac.cn;

{yuxiaoming, cxq}@ict.ac.cn

Abstract

The task of event detection involves identify-
ing and categorizing event triggers. Contex-
tual information has been shown effective on
the task. However, existing methods which
utilize contextual information only process the
context once. We argue that the context can
be better exploited by processing the context
multiple times, allowing the model to perform
complex reasoning and to generate better con-
text representation, thus improving the over-
all performance. Meanwhile, dynamic mem-
ory network (DMN) has demonstrated promis-
ing capability in capturing contextual informa-
tion and has been applied successfully to var-
ious tasks. In light of the multi-hop mecha-
nism of the DMN to model the context, we
propose the trigger detection dynamic memory
network (TD-DMN) to tackle the event detec-
tion problem. We performed a five-fold cross-
validation on the ACE-2005 dataset and exper-
imental results show that the multi-hop mech-
anism does improve the performance and the
proposed model achieves best F1 score com-
pared to the state-of-the-art methods.

1 Introduction

According to ACE (Automatic Content Extrac-
tion) event extraction program, an event is iden-
tified by a word or a phrase called event trigger
which most represents that event. For example, in
the sentence “No major explosion we are aware
of”, an event trigger detection model is able to
identify the word “explosion” as the event trigger
word and further categorize it as an Attack event.
The ACE-2005 dataset also includes annotations
for event arguments, which are a set of words or
phrases that describe the event. However, in this
work, we do not tackle the event argument classi-
fication and focus on event trigger detection.

The difficulty of the event trigger detection task
lies in the complicated interaction between the

event trigger candidate and its context. For in-
stance, given a sentence at the end of a passage:

they are going over there to do a mission they
believe in and as we said, 250 left yesterday.

It’s hard to directly classify the trigger word
“left” as an “End-Position” event or a “Transport”
event because we are not certain about what the
number “250” and the pronoun “they” are refer-
ring to. But if we see the sentence:

we are seeing these soldiers head out.
which is several sentences away from the for-

mer one, we now know the “250” and “they” refer
to “the soldiers”, and from the clue “these soldiers
head out” we are more confident to classify the
trigger word “left” as the “Transport” event.

From the above, we can see that the event
trigger detection task involves complex reason-
ing across the given context. Exisiting methods
(Liu et al., 2017; Chen et al., 2015; Li et al.,
2013; Nguyen et al., 2016; Venugopal et al., 2014)
mainly exploited sentence-level features and (Liao
and Grishman, 2010; Zhao et al., 2018) proposed
document-level models to utilize the context.

The methods mentioned above either not di-
rectly utilize the context or only process the con-
text once while classifying an event trigger candi-
date. We argue that processing the context multi-
ple times with later steps re-evaluating the context
with information acquired from the previous steps
improves the model performance. Such a mech-
anism allows the model to perform complicated
reasoning across the context. As in the example,
we are more confident to classify “left” as a “Tran-
port” event if we know “250” and “they” refer to
“soldiers” in previous steps.

We utilize the dynamic memory network
(DMN) (Xiong et al., 2016; Kumar et al., 2016)
to capture the contextual information of the given
trigger word. It contains four modules: the input
module for encoding reference text where the an-



1031

Memory Module

Input Module

Answer Module

Question Module

Document Context Sentence as Question

Figure 1: Overview of the TD-DMN architecture

swer clues reside, the memory module for storing
knowledge acquired from previous steps, the ques-
tion module for encoding the questions, and the
answer module for generating answers given the
output from memory and question modules.

DMN is proposed for the question answering
task, however, the event trigger detection problem
does not have an explicit question. The original
DMN handles such case by initializing the ques-
tion vector produced by the question module with
a zero or a bias vector, while we argue that each
sentence in the document could be deemed as a
question. We propose the trigger detection dy-
namic memory network (TD-DMN) to incorporate
this intuition, the question module of TD-DMN
treats each sentence in the document as implicitly
asking a question “What are the event types for
the words in the sentence given the document con-
text”. The high-level architecture of the TD-DMN
model is illustrated in Figure 1.

We compared our results with two models: DM-
CNN(Chen et al., 2015) and DEEB-RNN(Zhao
et al., 2018) through 5-fold cross-validation on the
ACE-2005 dataset. Our model achieves best F1
score and experimental results further show that
processing the context multiple times and adding
implicit questions do improve the model perfor-
mance. The code of our model is available online.1

2 The Proposed Approach

We model the event trigger detection task as a
multi-class classification problem following exist-
ing work. In the rest of this section, we describe
four different modules of the TD-DMN separately
along with how data is propagated through these
modules. For simplicity, we discuss a single docu-
ment case. The detailed architecture of our model
is illustrated in Figure 2.

1https://github.com/AveryLiu/TD-DMN

2.1 Input Module
The input module further contains two layers:
the sentence encoder layer and the fusion layer.
The sentence encoder layer encodes each sentence
into a vector independently, while the fusion layer
gives these encoded vectors a chance to exchange
information between sentences.

Sentence encoder layer Given document d
with l sentences (s1, . . . , sl), let si denotes the i-th
sentence in dwith nwords (wi1, . . . , win). For the
j-th word wij in si, we concatenate its word em-
bedding wij with its entity type embedding2 eij
to form the vectorWij as the input to the sentence
encoder Bi-GRU(Cho et al., 2014) of size Hs. We
obtain the hidden state hij by merging the forward
and backward hidden states from the Bi-GRU:

hij =
−−−→
GRUs(Wij) +

←−−−
GRUs(Wij) (1)

where + denotes element-wise addition.
We feed hij into a two-layer perceptron to gen-

erate the unnormalized attention scalar uij :

uij = tanh(hij ·Ws1) ·Ws2 (2)

where Ws1 and Ws2 are weight parameters of the
perceptron and we omitted bias terms. uij is then
normalized to obtain scalar attention weight αij :

αij =
exp(uij)∑n
k=1 exp(uik)

(3)

The sentence representation si is obtained by:

si =

n∑
j=1

αijhij (4)

Fusion layer The fusion layer processes the en-
coded sentences and outputs fact vectors which
contain exchanged information among sentences.
Let si denotes the i-th sentence representation ob-
tained from the sentence encoder layer. We gen-
erate fact vector fi by merging the forward and
backward states from fusion GRU:

fi =
−−−−→
GRUf (si) +

←−−−
GRUf (si) (5)

Let Hf denotes the hidden size of the fusion
GRU, we concatenate fact vectors f1 to fl to ob-
tain the matrix F of size l by Hf , where the i-th
row in F stores the i-th fact vector fi.

2The ACE-2005 includes entity type (including type “NA”
for none-entity) annotations for each word, the entity type
embedding is a vector associated with each entity type.



1032

 Input 
Module

…

Wi1 Wij

…

Win

Bi-GRU Bi-GRU Bi-GRU
… …

……

Sentence 
Encoder Layer

hi1 hij hin… …

αi1 αij αin

si si+1 sl…s1 si-1…

Bi-GRU Bi-GRU Bi-GRU Bi-GRU Bi-GRU
…

Fusion
 Layer

…

…

…

fi… …f1 fi-1 fi+1 fl

mt

wi1 wij

eij

win

ein

win

ein

wij

eij

wi1

ei1
…

Wi1 Wij

…

Win

Bi-GRU Bi-GRU Bi-GRU
… …

……

Question 
Module

qi1 qij qin… …

Average

… …

q*
… …

qi1 qij qin

q* q*

Bi-GRU

ai1

Softmax

Dense layer

Trigger type

Answer Module

…
…

…

…

… …

…

…

…

Bi-GRU

aij

Softmax

Dense layer

Trigger type

Bi-GRU

ain

Softmax

Dense layer

Trigger type

…
…
…

ei1

mt mt

Gate Attention

Attn-GRU

Memory Module

Gate Attention

Attn-GRU Attn-GRU
…

…c
1 memory 

update m
1 Attn-GRU

…
ct memory update mt

m0 mt-1

Figure 2: The detailed architecture of the TD-DMN model. The figure depicts a simplified case where
a single document d with l sentences is the input to the input module and a sentence si of d with n
words is the input to the question module. The input module encodes document d into a fact matrix
{F |f1, . . . ,fl}. The question module encodes sentence si into the question vector q∗. The memory
module initializesm0 with q∗ and iteratively processes for t times, at each time k it produces a memory
vector mk using fact matrix F , question vector q∗ and previous memory state mk−1. The answer
module outputs the predicted trigger type for each word in si using the concatenated tensor of the hidden
states of the question module and the last memory statemt.

2.2 Question Module
The question module treats each sentence s in d as
implicitly asking a question: What are the event
types for each word in the sentence s given the
document d as context? For simplicity, we only
discuss the single sentence case. Iteratively pro-
cessing from s1 to sl will give us all encoded ques-
tions in document d.

Let Wij be the vector representation of j-th
word in si, the question GRU generates hidden
state qij by:

qij =
−−−→
GRUq(Wij) +

←−−−
GRUq(Wij) (6)

The question vector q∗ is obtained by averaging
all hidden states of the question GRU:

q∗ =
1

n

n∑
j=1

qij (7)

Let Hq denotes the hidden size of the question
GRU, q∗ is a vector of size Hq, the intuition here
is to obtain a vector that represents the question
sentence. q∗ will be used for the memory module.

2.3 Memory Module
The memory module has three components: the
attention gate, the attentional GRU(Xiong et al.,

2016) and the memory update gate. The attention
gate determines how much the memory module
should attend to each fact given the facts F , the
question q∗, and the acquired knowledge stored in
the memory vectormt−1 from the previous step.

The three inputs are transformed by:

u = [F ∗ q∗; |F − q∗|;F ∗mt−1; |F −mt−1|] (8)

where ; is concatenation. ∗, − and |.| are element-
wise product, subtraction and absolute value re-
spectively. F is a matrix of size (m,Hf ), while q∗

andmt−1 are vectors of size (1, Hq) and (1, Hm),
whereHm is the output size of the memory update
gate. To allow element-wise operation, Hf , Hq
andHm are set to a same valueH . Meanwhile, q∗

and mt−1 are broadcasted to the size of (m,H).
In equation 8, the first two terms measure the sim-
ilarity and difference between facts and the ques-
tion. The last two terms have the same functional-
ity for facts and the last memory state.

Let β of size l denotes the generated attention
vector. The i-th element in β is the attention
weight for fact fi. β is obtained by transforming
u using a two-layer perceptron:

β = softmax(tanh(u ·Wm1) ·Wm2) (9)



1033

Methods
Fold 1 Fold 2 Fold 3 Fold 4 Fold 5 Avg

P R F1 P R F1 P R F1 P R F1 P R F1 F1
DMCNN 67.6 60.5 63.9 62.6 63.1 62.9 68.9 62.1 65.3 68.9 65.0 66.9 66.0 65.5 65.8 64.9

DEEB-RNN 64.9 64.1 64.5 63.4 64.7 64.0 66.1 64.3 65.2 66.0 67.3 66.6 65.5 67.2 66.3 65.3
TD-DMN 1-hop 67.3 62.1 64.6 65.4 61.7 63.5 72.0 60.0 65.4 66.6 68.0 67.3 68.3 65.0 66.6 65.5
TD-DMN 2-hop 69.2 61.0 64.8 64.6 63.4 64.0 64.3 66.4 65.3 68.7 65.9 67.3 68.5 65.7 67.1 65.7
TD-DMN 3-hop 66.3 63.7 64.9 66.9 60.6 63.6 68.3 64.0 66.1 67.9 66.3 67.1 70.2 64.3 67.1 65.8
TD-DMN 4-hop 66.7 63.4 65.0 61.4 65.5 63.4 66.4 66.0 66.2 64.7 69.1 66.8 70.0 63.4 66.5 65.6

Table 1: 5-fold cross-validation results on the ACE-2005 dataset. The results are rounded to a single
digit. The F1 of the last column are calculated by averaging F1 scores of all folds.

where Wm1 and Wm2 are parameters of the per-
ceptron and we omitted bias terms.

The attentional GRU takes facts F , fact atten-
tion β as input and produces context vector c of
size Hc. At each time step t, the attentional GRU
picks the ft as input and uses βt as its update gate
weight. For space limitation, we refer reader to
(Xiong et al., 2016) for the detailed computation.

The memory update gate outputs the updated
memory mt using question q∗, previous memory
statemt−1 and context c:

mt = relu([q
∗;mt−1; c] ·Wu) (10)

where Wu is the parameter of the linear layer.
The memory module could be iterated several

times with a new β generated for each time. This
allows the model to attend to different parts of
the facts in different iterations, which enables the
model to perform complicated reasoning across
sentences. The memory module produces mt as
the output at the last iteration.

2.4 Answer Module
Answer module predicts the event type for each
word in a sentence. For each question GRU hidden
state qij , the answer module concatenates it with
the memory vector mt as the input to the answer
GRU with hidden size Ha. The answer GRU out-
puts aij by merging its forward and backward hid-
den states. The fully connected dense layer then
transforms aij to the size of the number of event
labels O and the softmax layer is applied to output
the probability vector pij . The k-th element in pij
is the probability for the word wij being the k-th
event type. Let yij be the true event type label for
word wij . Assuming all sentences are padded to
the same length n, the cross-entropy loss for the
single document d is applied as:

J(y,p) = −
l∑

i=1

n∑
j=1

O∑
k=1

I(yij = k)log p
(k)
ij

(11)

where I(·) is the indicator function.

3 Experiments

3.1 Dataset and Experimental Setup

Dateset
Different from prior work, we performed a 5-fold
cross-validation on the ACE 2005 dataset. We par-
titioned 599 files into 5 parts. The file names of
each fold can be found online3. We chose a differ-
ent fold each time as the testing set and used the
remaining four folds as the training set.

Baselines
We compared our model with two other mod-
els: DMCNN (Chen et al., 2015) and DEEB-
RNN (Zhao et al., 2018). DMCNN is a sentence-
level event detection model which enhances tradi-
tional convolutional networks with dynamic mul-
tiple pooling mechanism customized for the task.
The DEEB-RNN is a state-of-the-art document-
level event detection model which firstly generate
a document embedding and then use it to aid the
event detection task.

Evaluation
We report precision, recall and F1 score of each
fold along with the averaged F1 score of all folds.
We evaluated all the candidate trigger words in
each testing set. A candidate trigger word is cor-
rectly classified if its event subtype and offsets
match its human annotated label.

Implementation Details
To avoid overfitting, we fixed the word embedding
and added a 1 by 1 convolution after the embed-
ding layer to serve as a way of fine tuning but
with a much smaller number of parameters. We
removed punctuations, stop words and sentences
which have length less equal than 2. We used the
Stanford corenlp toolkit(Manning et al., 2014) to

3https://github.com/AveryLiu/TD-DMN/data/splits



1034

separate sentences. We down-sampled negative
samples to ease the unbalanced classes problem.

The setting of the hyperparameters is the same
for different hops of the TD-DMN model. We set
H , Hs, Hc, and Ha to 300, the entity type embed-
ding size to 50, Ws1 to 300 by 600, Ws2 to 600 by
1, Wm1 to 1200 by 600, Wm2 to 600 by 1, Wu to
900 by 300, the batch size4 to 10. We set the down-
sampling ratio to 9.5 and we used Adam optimizer
(Kingma and Ba, 2014) with weight decay set to
1e−5. We set the dropout(Srivastava et al., 2014)
rate before the answer GRU to 0.4 and we set all
other dropout rates to 0.2. We used the pre-trained
word embedding from (Le and Mikolov, 2014).

3.2 Results on the ACE 2005 Corpus

The performance of each model is listed in ta-
ble 1. The first observation is that models us-
ing document context drastically outperform the
model that only focuses on the sentence level fea-
ture, which indicates document context is helpful
in event detection task. The second observation
is that increasing number of hops improves model
performance, this further implies that processing
the context for multiple times does better exploit
the context. The model may have exhausted the
context and started to overfit, causing the perfor-
mance drop at the fourth hop.

The performance of reference models is much
lower than that reported in their original papers.
Possible reasons are that we partitioned the dataset
randomly, while the testing set of the original par-
tition mainly contains similar types of documents
and we performed a five-fold cross-validation.

3.3 The Impact of the Question Module

To reveal the effects of the question module, we
ran the model in two different settings. In the first
setting, we initialized the memory vector m0 and
question vector q∗ with a zero vector, while in the
second setting, we ran the model untouched. The
results are listed in the table 2. The two models
perform comparably under the 1-hop setting, this
implies that the model is unable to distinguish the
initialization values of the question vector well in
the 1-hop setting. For higher number of hops, the
untouched model outperforms the modified one.
This indicates that with a higher number of mem-
ory iterations, the question vector q∗ helps the
model to better exploit the context information.

4In each batch, there are 10 documents.

Methods F1 F ∗1
TD-DMN 1-hop 65.48 65.52
TD-DMN 2-hop 65.69 65.46
TD-DMN 3-hop 65.78 65.51
TD-DMN 4-hop 65.57 65.40

Table 2: The impact of the question module, F ∗1
indicates results with empty questions.

We still observe the increase and drop pattern of
the F1 for the untouched model. However, such a
pattern is not obvious with empty questions. This
implies that we are unable to have a steady gain
without the question module in this specific task.

4 Future Work

In this work, we explored the TD-DMN archi-
tecture to exploit document context. Extending
the model to include wider contexts across several
similar documents may also be of interest. The
detected event trigger information can be incor-
porated into question module when extending the
TD-DMN to the argument classification problem.
Other tasks with document context but without ex-
plicit questions may also benefit from this work.

5 Conclusion

In this paper, we proposed the TD-DMN model
which utilizes the multi-hop mechanism of the dy-
namic memory network to better capture the con-
textual information for the event trigger detec-
tion task. We cast the event trigger detection as
a question answering problem. We carried five-
fold cross-validation experiments on the ACE-
2005 dataset and results show that such multi-hop
mechanism does improve the model performance
and we achieved the best F1 score compared to the
state-of-the-art models.

Acknowledgments

We thank Prof. Huawei Shen for providing men-
torship in the rebuttal phase. We thank Jinhua Gao
for discussion on the paper presentation. We thank
Yixing Fan and Xiaomin Zhuang for providing ad-
vice regarding hyper-parameter tuning. We thank
Yue Zhao for the initial discussion on event ex-
traction. We thank Yaojie Lu for providing pre-
processing scripts and the results of DMCNN. We
thank anonymous reviewers for their advice. The
first author personally thank Wei Qi for being sup-
portive when he was about to give up.



1035

References
Yubo Chen, Liheng Xu, Kang Liu, Daojian Zeng,

and Jun Zhao. 2015. Event extraction via dy-
namic multi-pooling convolutional neural networks.
In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the
7th International Joint Conference on Natural Lan-
guage Processing (Volume 1: Long Papers), vol-
ume 1, pages 167–176.

Kyunghyun Cho, Bart van Merrienboer, Çaglar
Gülçehre, Fethi Bougares, Holger Schwenk, and
Yoshua Bengio. 2014. Learning phrase representa-
tions using RNN encoder-decoder for statistical ma-
chine translation. CoRR, abs/1406.1078.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit
Iyyer, James Bradbury, Ishaan Gulrajani, Victor
Zhong, Romain Paulus, and Richard Socher. 2016.
Ask me anything: Dynamic memory networks for
natural language processing. In International Con-
ference on Machine Learning, pages 1378–1387.

Quoc Le and Tomas Mikolov. 2014. Distributed rep-
resentations of sentences and documents. In Inter-
national Conference on Machine Learning, pages
1188–1196.

Qi Li, Heng Ji, and Liang Huang. 2013. Joint event
extraction via structured prediction with global fea-
tures. In Proceedings of the 51st Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), volume 1, pages 73–82.

Shasha Liao and Ralph Grishman. 2010. Using doc-
ument level cross-event inference to improve event
extraction. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 789–797. Association for Computational
Linguistics.

Shulin Liu, Yubo Chen, Kang Liu, and Jun Zhao. 2017.
Exploiting argument information to improve event
detection via supervised attention mechanisms. In
Proceedings of the 55th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), volume 1, pages 1789–1798.

Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David Mc-
Closky. 2014. The Stanford CoreNLP natural lan-
guage processing toolkit. In Association for Compu-
tational Linguistics (ACL) System Demonstrations,
pages 55–60.

Thien Huu Nguyen, Kyunghyun Cho, and Ralph Gr-
ishman. 2016. Joint event extraction via recurrent
neural networks. In Proceedings of the 2016 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 300–309.

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: a simple way to prevent neural networks
from overfitting. The Journal of Machine Learning
Research, 15(1):1929–1958.

Deepak Venugopal, Chen Chen, Vibhav Gogate, and
Vincent Ng. 2014. Relieving the computational bot-
tleneck: Joint inference for event extraction with
high-dimensional features. In Proceedings of the
2014 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 831–843.

Caiming Xiong, Stephen Merity, and Richard Socher.
2016. Dynamic memory networks for visual and
textual question answering. In International Con-
ference on Machine Learning, pages 2397–2406.

Yue Zhao, Xiaolong Jin, Yuanzhuo Wang, and Xueqi
Cheng. 2018. Document embedding enhanced event
detection with hierarchical and supervised attention.
In Proceedings of the 56th Annual Meeting of the
Association for Computational Linguistics (Volume
2: Short Papers), volume 2, pages 414–419.


