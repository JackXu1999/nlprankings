919

Coling 2010: Poster Volume, pages 919–927,

Beijing, August 2010

A Study on Position Information in Document Summarization 

You Ouyang       Wenjie Li       Qin Lu       Renxian Zhang 

Department of Computing, the Hong Kong Polytechnic University 

{csyouyang,cswjli,csluqin,csrzhang}@comp.polyu.edu.hk 

Abstract 

the 

very 

effective 

Position information has been proved to 
document 
be 
in 
summarization,  especially 
in  generic 
approaches 
summarization.  Existing 
mostly  consider 
information  of 
sentence positions in a document, based 
on  a  sentence  position  hypothesis  that 
the  importance  of  a  sentence  decreases 
with  its  distance  from  the  beginning  of 
the document. In this paper, we consider 
another kind of position information, i.e., 
the word position information, which is 
based  on  the  ordinal  positions  of  word 
appearances 
sentence 
positions.  An  extractive  summarization 
model 
to  provide  an 
evaluation  framework  for  the  position 
information.  The  resulting  systems  are 
evaluated  on  various  data  sets 
to 
demonstrate  the  effectiveness  of  the 
position 
different 
tasks.  Experimental 
summarization 
that  word  position 
results 
show 
is  more  effective  and 
information 
adaptive 
position 
information. 

is  proposed 

information 

sentence 

instead 

than 

of 

in 

It  springs 

Introduction 

1 
Position information has been frequently used in 
document  summarization. 
from 
human’s  tendency  of  writing  sentences  of 
greater topic centrality at particular positions in 
a  document.  For  example, 
in  newswire 
documents,  topic  sentences  are  usually  written 
earlier.  A  sentence  position  hypothesis  is  then 
given as: the first sentence in a document is the 
most important and the importance decreases as 

in 

the 
the  sentence  gets  further  away  from 
beginning.  Based  on  this  sentence  position 
hypothesis,  sentence  position 
features  are 
defined  by  the  ordinal  position  of  sentences. 
These position features have been proved to be 
very 
document 
summarization.  In  more  recent  summarization 
tasks,  such  as  query-focused  and  update 
summarization  tasks,  position  features  are  also 
widely used.  

effective 

generic 

Although in these tasks position features may 
be used in different ways, they are all based on 
the sentence  position hypothesis. So we regard 
them  as  providing 
the  sentence  position 
information. In this paper, we study a new kind 
of  position  information,  i.e.,  the  word  position 
information.  The  motivation  of  word  position 
information  comes  from  the  idea  of  assigning 
different importance to multiple appearances of 
one word in a document.  

As to many language models such as the bag-
of-words model, it is well acknowledged that a 
word which appears more frequently is usually 
more  important.  If  we  take  a  closer  look  at  all 
the appearances of one word, we can view this 
as a process that the different appearances of the 
same  word  raise  the  importance  of  each  other. 
Now let’s also take the order of the appearances 
into account. When reading a document, we can 
view  it  as  a  word  token  stream  from  the  first 
token to the last. When a new token is read, we 
attach more importance to previous tokens that 
have  the  same  lemma  because  they  are  just 
repeated by the new token. Inspired by this, we 
postulate  a  word  position  hypothesis  here:  for 
all  the  appearances  of  a  fixed  word,  the 
importance  of  each  appearance  depends  on  all 
its  following  appearances.  Therefore,  the  first 
appearance of a word is the most important and 
the 
the  ordinal 

importance  decreases  with 

920

positions of the appearances. Then, a novel kind 
of position features can be defined for the word 
appearances  based  on  their  ordinal  positions. 
We  believe  that  these  word  position  features 
have  some  advantages  when  compared 
to 
traditional sentence position features. According 
to  the  sentence  position  hypothesis,  sentence 
position 
features  generally  prefer  earlier 
sentences  in  a  document.  As  to  the  word 
position  features  that  attempt  to  differentiate 
word  appearances 
instead  of  sentences,  a 
sentence  which  is  not  the  first  one  in  the 
document may still not be penalized as long as 
its  words  do  not  appear  in  previous  sentences. 
Therefore,  word  position  features  are  able  to 
discover topic sentences in deep positions of the 
document. On the other hand, the assertion that 
the first sentence is always the most important is 
not true in actual data. It depends on the writing 
style  indeed.  For  example,  some  authors  may 
like to write some background sentences before 
topic  sentences.  In  conclusion,  we  can  expect 
word  position  features    to  be  more  adaptive  to 
documents with different structures.  

In the study of this paper, we define several 
word  position  features  based  on  the  ordinal 
positions of word appearances. We also develop 
a word-based summarization system to evaluate 
the effectiveness of the proposed word position 
features on a series of summarization data sets. 
The main contributions of our work are: 
(1) representation of word position information, 
which is a new kind of position information in 
document summarization area. 
(2)  empirical  results  on  various  data  sets  that 
demonstrate the impact of position information 
in different summarization tasks. 
2  Related Work 
The  use  of  position  information  in  document 
summarization has a long history. In the seminal 
work by (Luhn, 1958), position information was 
already  considered  as  a  good  indicator  of 
significant  sentences.  In  (Edmundson,  1969),  a 
location  method  was  proposed  that  assigns 
positive weights to the sentences to their ordinal 
positions in the document. Position information 
has  since  been  adopted  by  many  successful 
summarization  systems,  usually  in  the  form  of 
sentence position features. For example, Radev 
et  al.  (2004)  developed  a  feature-based  system 

is 

also 

MEAD based on word frequencies and sentence 
positions. The position feature was defined as a 
descending  function  of  the  sentence  position. 
The MEAD system performed very well in the 
generic  multi-document  summarization  task  of 
the  DUC  2004  competition.  Later,  position 
information 
to  more 
summarization  tasks.  For  example,  in  query-
focused  task,  sentence  position  features  are 
widely  used  in  learning-based  summarization 
systems as a component feature for calculating 
the  composite  sentence  score  (Ouyang  et  al, 
2007;  Toutanova  et  al,  2007).  However,  the 
effect of position features alone was not studied 
in these works.  

applied 

There  were  also  studies  aimed  at  analyzing 
and  explaining  the  effectiveness  of  position 
information. Lin and Hovy (1997) provided an 
empirical  validation  on  the  sentence  position 
hypothesis.  For  each  position,  the  sentence 
position yield was defined as the average value 
of  the  significance  of  the  sentences  with  the 
fixed position. It was observed that the average 
significance  at  earlier  positions  was  indeed 
larger.  Nenkova 
(2005)  did  a  conclusive 
overview  on  the  DUC  2001-2004  evaluation 
results. It was reported that position information 
is  very  effective  in  generic  summarization.  In 
generic single-document summarization, a lead-
based  baseline  that  simply  takes  the  leading 
sentences as the summary can outperform most 
submitted  summarization  system  in  DUC  2001 
and 2002. As in multi-document summarization, 
the 
is 
competitive  in  generating  short  summaries  but 
not 
summaries.  Schilder  and 
Kondadadi (2008) analyzed the effectiveness of 
the features that are used in their learning-based 
sentence  scoring  model 
for  query-focused 
summarization.  By  comparing  the  ROUGE-2 
results  of  each 
it  was 
reported  that  position-based  features  are  less 
effective 
than  frequency-based  features.  In 
(Gillick  et  al.,  2009),  the  effect  of  position 
information  in  the  update  summarization  task 
was  studied. By using ROUGE to  measure the 
density  of  valuable  words  at  each  sentence 
position, it was observed that the first sentence 
of newswire document was especially important 
for composing update summaries. They defined 
a binary sentence position feature based on the 

individual  feature, 

position-based 

baseline 

system 

longer 

in 

921

the 

Under 

bag-of-words  model, 

observation  and  the  feature  did  improve  the 
performance on the update summarization data. 
3  Methodology 
In the section, we first describe the word-based 
summarization  model.  The  word  position 
features  are  then  defined and  incorporated  into 
the summarization model. 
3.1  Basic Summarization Model 
To test the effectiveness of position information 
in document summarization, we first propose a 
word-based  summarization  model  for  applying 
the position information. The system follows a 
typical extractive style that constructs the target 
summary by selecting the most salient sentences.  
the 
probability of a word w in a document set D can 
be scaled by its frequency, i.e., p(w)=freq(w)/|D|, 
where freq(w) indicates the frequency of w in D 
and |D| indicates the total number of words in D. 
The probability of a sentence s={w1, …, wN} is 
then  calculated  as  the  product  of  the  word 
probabilities,  i.e.,  p(s)=Πi p(wi).  Moreover,  the 
probability  of  a  summary  consisting  a  set  of 
sentences,  denoted  as  S={s1,  …,  sM},  can  be 
calculated  by  the  product  of  the  sentence 
probabilities,  i.e.,  p(S)=Πj  p(sj).  To  obtain  the 
optimum summary, an intuitive idea is to select 
the sentences to maximize the overall summary 
probability  p(S),  equivalent 
to  maximizing 
log(p(S))  =  ΣjΣi log(p(wji))  =  ΣjΣi (logfreq(wji)- 
log|D|) = ΣjΣi log freq(wji) - |S|·log |D|,  
where  wji  indicates  the  ith  word  in  sj  and  |S| 
indicates the total number of words in S. As to 
practical  summarization 
tasks,  a  maximum 
summary  length  is  usually  postulated.  So  here 
we just assume that the length of the summary 
is fixed. Then, the above optimization target is 
logfreq(wji). 
equivalent 
From  the  view  of  information  theory,  the  sum 
can also be interpreted as a simple measure on 
the total information amount of the summary. In 
this  interpretation,  the  information  of  a  single 
word  wji  is  measured  by  log  freq(wji)  and  the 
summary  information  is  the  sum  of  the  word 
information. So the optimization target can also 
be interpreted as including the most informative 
words  to  form  the  most  informative  summary 
given the length limit.  

to  maximizing  ΣjΣi 

In  extractive  summarization,  summaries  are 
composed  by  sentence  selection.  As  to  the 
above optimization target, the sentence scoring 
function  for  ranking  the  sentences  should  be 
calculated as the average word information, i.e., 
score(s) = Σi log freq(wi) / |s|. 

After  ranking  the  sentences  by  their  ranking 
scores,  we  can  select  the  sentences  into  the 
summary by the descending order of their score 
until the length limit is reached. By this process, 
the  summary  with  the  largest    p(S)  can  be 
composed.  
3.2  Word Position Features 
With  the  above  model,  word  position  features 
are  defined  to  represent  the  word  position 
information  and  are  then  incorporated  into  the 
model. According to the motivation, the features 
are  defined  by  the  ordinal  positions  of  word 
appearances,  based  on  the  position  hypothesis 
that  earlier  appearances  of  a  word  are  more 
informative.  Formally,  for  the  ith  appearance 
among the total n appearances of a word w, four 
position  features  are  defined  based  on  i  and  n 
using different formulas as described below. 
(1)  Direct  proportion  (DP)  With  the  word 
position hypothesis, an intuitive idea is to regard 
the information degree of the first appearance as 
1 and the last one as 1/n, and then let the degree 
decrease  linearly  to  the  position  i.  So  we  can 
obtain  the  first  position  feature  defined  by  the 
direct proportion function, i.e., f(i)=(n-i+1)/n. 
(2) Inverse proportion (IP). Besides the linear 
function,  other  functions  can  also  be  used  to 
relationship  between 
characterize 
the 
position  and 
importance.  The  second 
position  feature  adopts  another  widely-used 
function,  the  inversed  proportion  function,  i.e., 
f(i)=1/i.  This  measure  is  similar  to  the  above 
one, but the information degree decreases by the 
inverse  proportional  function.  Therefore,  the 
degree  decreases  more  quickly  at  smaller 
positions,  which  implies  a  stronger  preference 
for leading sentences. 
(3)  Geometric  sequence  (GS).  For  the  third 
feature, we make an assumption that the degree 
of every appearance is the sum of the degree of 
all the following appearances, i.e., f(i) = f(i+1)+ 
f(i+2)+…+ f(n). It can be easily derived that the 
sequence also satisfies f(i) = 2·f(i-1). That is, the 
information  degree  of  each  new  appearance  is 

the 
the 

922

halved.  Then  the  feature  value  of  the  ith 
appearance can be calculated as f(i) = (1/2)i-1.  
(4) Binary function (BF). The final feature is a 
binary  position  feature  that  regards  the  first 
appearance as much more informative than the 
all  the  other  appearances,  i.e.,  f(i)=1,  if  i=1;  λ 
else, where λ is a small positive real number.  
3.3 
Incorporating the Position Features  
To  incorporate  the  position  features  into  the 
word-based summarization model, we use them 
to adjust the importance of the word appearance. 
For the ith appearance of a word w, its original 
importance is multiplied by the position feature 
value, i.e., log freq(w)·pos(w, i), where pos(w, i) 
is calculated by one of the four position features 
introduced above. By this, the position feature is 
also incorporated into the sentence scores, i.e., 
score’(s) = Σi [log freq(wi) · pos(wi)] / |s| 
3.4  Sentence Position Features 
In  our  study,  another  type  of  position  features, 
which  model  sentence  position  information,  is 
defined  for  comparison  with  the  word  position 
features. The sentence position features are also 
defined  by  the  above  four  formulas.  However, 
for each appearance, the definition of i and n in 
the formulas are changed to the ordinal position 
of  the  sentence  that  contains  this  appearance 
and  the  total  number  of  sentences  in  the 
document respectively. In fact, the effects of the 
features  defined  in  this  way  are  equivalent  to 
traditional  sentence  position  features.  Since  i 
and n are now defined by sentence positions, the 
feature  values  of  the  word  tokens  in  the  same 
sentence s are all equal. Denote it by pos(s), and 
the sentence score with the position feature can 
be written as  
score’(s) = ( Σw in slogfreq(w) · pos(s))/|s|  
= pos(s)·(Σ logw in s freq(w)/|s|), 
which can just be viewed as the product of the 
original score and a sentence position feature. 
3.5  Discussion 
By using the four functions to measure word or 
sentence position information, we can generate 
a  total  of  eight  position  features.  Among  the 
four  functions,  the  importance  drops  fastest 
under the binary function and the order is BF > 
GS > IP > DP. Therefore, the features based on 
the  binary  function  are  the  most  biased  to  the 

leading  sentences  in  the  document  and  the 
features based on the direct proportion function 
are the least. On the other hand, as mentioned in 
the  introduction,  sentence-based  features  have 
larger  preferences  for  leading  sentences  than 
word-based position features.  

An  example  is  given  below  to  illustrate  the 
difference  between  word  and  sentence  position 
features. This is a document from DUC 2001. 
1. GENERAL ACCIDENT, the leading British 
insurer,  said  yesterday  that  insurance  claims 
arising from Hurricane Andrew could 'cost it as 
much as Dollars 40m.'  
the  chairman  who  was 
2.  Lord  Airlie, 
extraordinary 
shareholders' 
addressing 
an 
meeting,  said: 
the  basis  of  emerging 
'On 
information,  General  Accident  advise  that  the 
losses  to  their  US  operations  arising  from 
Hurricane  Andrew,  which  struck  Florida  and 
Louisiana,  might  in  total  reach  the  level  at 
which  external  catastrophe  reinsurance  covers 
would become exposed'.  
3. What this means is that GA is able to pass on 
its  losses  to  external  reinsurers  once  a  certain 
claims threshold has been breached.  
4. It believes this threshold may be breached in 
respect of Hurricane Andrew claims.  
5.  However,  if  this  happens,  it  would  suffer  a 
post-tax loss of Dollars 40m (Pounds 20m).  
6.  Mr  Nelson  Robertson,  GA's  chief  general 
manager, explained later that the company has a  
1/2 per cent share of the Florida market.  
7. It has a branch in Orlando.  
8. The company's loss adjusters are in the area 
trying to estimate the losses.  
9.  Their  guess  is  that  losses  to  be  faced  by  all 
insurers may total more than Dollars 8bn.  
10.  Not  all  damaged  property  in  the  area  is 
insured and there have been estimates that the 
storm  caused  more 
than  Dollars  20bn  of 
damage.  
11. However, other insurers have estimated that 
losses  could  be  as  low  as  Dollars  1bn  in  total. 
12  Mr  Robertson  said:  'No  one  knows  at  this 
time what the exact loss is'. 

For  the  word  “threshold”  which  appears 
twice in the document, its original importance is 
log(2), for the appearance of “threshold” in the 
4th sentence, the modified score based on word 
position  feature  with  the  direct  proportion 
function  is  1/2·log(2).  In  contrast,  the  score 
based  on  sentence  position  feature  with  the 

923

same  function  is  9/12·log(2),  which  is  larger. 
For the appearance of the word “estimate” in the 
8th  sentence,  its  original  importance  is  log(3) 
(the three boldfaced tokens are regarded as one 
word  with  stemming).  The  word-based  and 
sentence-based scores are log(3) and 5/12·log(3) 
respectively.  So  its  importance  is  larger  under 
word  position  feature.  Therefore,  the  system 
with  word  position  features  may  prefer  the  8th 
sentence  that  is  in  deeper  positions  but  the 
system  with  sentence  position  feature  may 
prefer the 4th sentence. As for this document, the 
top  5  sentences  selected  by  sentence  position 
feature are {1, 4, 3, 5, 2} and the those selected 
by the word position features are {1, 8, 3, 6, 9}. 
This  clearly  demonstrates 
the  difference 
between the position features. 
4  Experimental Results 
4.1  Experiment Settings 
We  conduct  the  experiments  on  the  data  sets 
from  the  Document  Understanding  Conference 
(DUC)  run  by  NIST.  The  DUC  competition 
started  at  year  2001  and  has  successfully 
evaluated  various  summarization  tasks  up  to 
now.  In  the  experiments,  we  evaluate  the 
effectiveness of position information on several 
DUC 
various 
summarization  tasks.  One  of  the  evaluation 
criteria 
automatic 
summarization  evaluation  package  ROUGE,  is 
used 
the 
proposed  word  position  features  in  the  context 
of document  summarization1. The recall scores 
of  ROUGE-1  and  ROUGE-2,  which  are  based 
on  unigram  and  bigram  matching  between 
system summaries and reference summaries, are 
adopted as the evaluation criteria.  

the  effectiveness  of 

to  evaluate 

in  DUC, 

involve 

In the data sets used in the experiments, the 
original  documents  are  all  pre-processed  by 
sentence  segmentation,  stop-word  removal  and 
word  stemming.  Based  on  the  word-based 
summarization  model,  a  total  of  nine  systems 
are evaluated in the experiments, including the 
system with the original ranking model (denoted 
as None), four systems with each word position 
feature (denoted as WP) and four systems with 
each sentence position feature (denoted as SP). 
                                                 
1 We run ROUGE-1.5.5 with the parameters “-x -m -
n 2 -2 4 -u -c 95 -p 0.5 -t 0” 

used 

data 

sets 

that 

the 

set 

for 

For reference, the average ROUGE scores of all 
the  human  summarizers  and  all  the  submitted 
systems  from  the  official  results  of  NIST  are 
also  given  (denoted  as  Hum  and  NIST 
respectively).  
4.2  Redundancy Removal 
To  reduce  the  redundancy  in  the  generated 
summaries,  we  use  an  approach  similar  to  the 
maximum marginal relevance (MMR) approach 
in the sentence selection process (Carbonell and 
Goldstein, 1998). In each round of the sentence 
selection,  the  candidate  sentence  is  compared 
against 
the  already-selected  sentences.  The 
sentence  is  added  to  the  summary  only  if  it  is 
not significantly similar to any already-selected 
sentence, which is judged by the condition that 
the cosine similarity between the two sentences 
is less than 0.7. 
4.3  Generic Summarization 
In  the  first  experiment,  we  use  the  DUC  2001 
data 
single-document 
summarization  and  the  DUC  2004  data  set  for 
generic  multi-document  summarization.  The 
DUC  2001  data  set  contains  303  document-
summary pairs; the DUC 2004 data set contains 
45 document sets, with each set consisting of 10 
documents.  A  summary  is  required  for  each 
document  set.  Here  we  need  to  adjust  the 
ranking model for the multi-document task, i.e., 
the  importance  of  a  word  is  calculated  as  its 
total  frequency  in  the  whole  document  set 
instead of a single document. For both tasks, the 
summary length limit is 100 words. 

generic 

Table  1  and  2  below  provide  the  average 
ROUGE-1 and ROUGE-2 scores (denoted as R-
1  and  R-2)  of  all  the  systems.  Moreover,  we 
used  paired  two  sample  t-test  to  calculate  the 
significance of the differences between a pair of 
word and sentence position features. The bolded 
score  in  the  tables  indicates  that  that  score  is 
significantly  better 
the  corresponding 
paired one. For example, in Table 1, the bolded 
R-1  score  of  system  WP  DP  means  that  it  is 
significantly better than the R-1 score of system 
SP  DP.  Besides  the  ROUGE  scores,  two 
statistics,  the  number  of  “first  sentences 2 ” 
among  the  selected  sentences  (FS-N)  and  the 
                                                 
2 A “first sentence” is the sentence at the fist position 
of a document.  

than 

924

average  position  of  the  selected  sentences  (A-
SP), are also reported in the tables for analysis.  

 

R-1 

System 
R-2 
WP DP  0.4473  0.1942 
SP DP 
0.4396  0.1844 
0.4543  0.2023 
WP IP 
SP IP 
0.4502  0.1964 
WP GS  0.4544  0.2041 
SP GS 
0.4509  0.1974 
WP BF  0.4544  0.2036 
SP BF 
0.4239  0.1668 
None 
0.4193  0.1626 
NIST 
0.4445  0.1865 
Hum 
0.4568  0.1740 
Table 1. Results on the DUC 2001 data set  
 

FS-N 
301 
300 
290 
303 
278 
303 
253 
303 
265 
- 
- 

A-SP 
4.00 
3.69 
4.30 
3.08 
4.50 
2.93 
5.57 
9.64 
10.06

- 
- 

- 
- 

R-1 

A-SP 
4.16 
2.68 
3.77 
1.01 
3.67 
1.01 
3.14 
1 
10.98

FS-N 
89 
112 
108 
201 
110 
201 
127 
203 
36 
- 
- 

System 
R-2 
WP DP  0.3728  0.0911 
SP DP 
0.3724  0.0908 
0.3756  0.0912 
WP IP 
SP IP 
0.3690  0.0905 
WP GS  0.3751  0.0916 
SP GS 
0.3690  0.0905 
WP BF  0.3740  0.0926 
SP BF 
0.3685  0.0903 
None 
0.3550  0.0745 
NIST 
0.3340  0.0686 
Hum 
0.4002  0.0962 
Table 2. Results on the DUC 2004 data set 
 
From Table 1 and Table 2, it is observed that 
position information is indeed very effective in 
generic  summarization  so  that  all  the  systems 
with position features performed better than the 
system  None  which  does  not  use  any  position 
information.  Moreover,  it  is  also  clear  that  the 
proposed  word  position  features  consistently 
outperform the corresponding sentence position 
features. Though the gaps between the ROUGE 
scores are not large, the t-tests proved that word 
position  features  are  significantly  better  on  the 
DUC  2001  data  set.  On  the  other  hand,  the 
advantages  of  word  position  features  over 
sentence position features are less significant on 
the DUC 2004 data set. One reason may be that 
the  multiple  documents  have  provided  more 
candidate sentences for composing the summary. 
Thus it is possible to generate a good summary 
only 
the 

leading  sentences 

from 

the 

in 

documents. According to Table 2, the average-
sentence-position of system SP  BF is 1, which 
means  that  all  the  selected  sentences  are  “first 
sentences”.  Even  under  this  extreme  condition, 
the performance is not much worse. 

The  two  statistics  also  show  the  different 
preferences  of  the  features.  Compared  to  word 
position features, sentence position features are 
likely  to  select  more  “first  sentences”  and  also 
have  smaller  average-sentence-positions.  The 
abnormally  large  average-sentence-position  of 
SP  BF  in  DUC  2001  is  because  it  does  not 
differentiate  all  the  other  sentences  except  the 
first  one.  The  corresponding  word-position-
based  system  WP  BF  can  differentiate  the 
sentences since it is based on word positions, so 
its average-sentence-position is not that large. 
4.4  Query-focused Summarization 
Since  year  2005,  DUC  has  adopted  query-
focused  multi-document  summarization  tasks 
that  require  creating  a  summary  from  a  set  of 
documents to a given query. This task has been 
specified as the main evaluation task over three 
years  (2005-2007).  The  data  set  of  each  year 
contains about 50 DUC topics, with each topic 
including 25-50 documents and a query. In this 
experiment,  we  adjust  the  calculation  of  the 
word  importance  again  for  the  query-focused 
issue.  It  is  changed  to  the  total  number  of  the 
appearances that fall into the sentences  with at 
least one word in the query. Formally, given the 
query  which  is  viewed  as  a  set  of  words 
Q={w1, …, wT}, a sentence set SQ is defined as 
the set of sentences that contain at least one wi 
in  Q.  Then  the  importance  of  a  word  w  is 
calculated by its frequency in SQ. For the query-
focused  task,  the  summary  length  limit  is  250 
words. 

Table 3 below provides the average ROUGE-
1 and ROUGE-2 scores of all the systems on the 
DUC 2005-2007 data sets. The boldfaced terms 
in  the  tables  indicate  the  best  results  in  each 
column.  According  to  the  results,  on  query-
focused  summarization,  position  information 
seems  to  be  not  as  effective  as  on  generic 
summarization.  The  systems  with  position 
features can not outperform the system None. In 
fact,  this  is  reasonable  due  to  the  requirement 
specified  by  the  pre-defined  query.  Given  the 
query,  the  content  of  interest  may  be  in  any 

925

position of  the  document  and  thus  the  position 
information becomes less meaningful.  

On  the  other  hand,  we  find  that  though  the 
systems  with  word  position  features  cannot 
outperform 
it  does 
significantly  outperform 
the  systems  with 
sentence  position  features.  This  is  also  due  to 
the role of the query. Since it may refer to the 
specified  content 
the 

in  any  position  of 

system  None, 

the 

to 

fail 

in  discovering 

documents, sentence position features are more 
likely 
the  desired 
sentences  since  they  always  prefer  leading 
sentences.  In  contrast,  word  position  features 
are  less  sensitive  to  this  problem  and  thus 
perform  better.  Similarly,  we  can  see  that  the 
direct proportion (DP), which has the least bias 
for leading sentences, has the best performance 
among the four functions. 
2007 
2006 

2005 

R-1 

System
R-2 
WP DP  0.3791  0.0805
SP DP
0.3727  0.0776
WP IP
0.3772  0.0791
SP IP 
0.3618  0.0715
WP GS  0.3767  0.0794
SP GS
0.3616  0.0716
WP BF  0.3740  0.0741
SP BF
0.3647  0.0686
NONE
0.3788  0.0791
NIST 
0.3353 
0.0592
Hum 
0.4392  0.1022

2008 A 

R-2 

R-1 

System 
R-1 
WP DP  0.3687  0.0978  0.3758
SP DP 
0.3687  0.0971  0.3723
0.3709  0.1014  0.3741
WP IP 
SP IP 
0.3619  0.0975  0.3723
WP GS  0.3705  0.1004  0.3732
SP GS 
0.3625  0.0975  0.3723
WP BF  0.3661  0.0975  0.3678
SP BF 
0.3658  0.0965  0.3674
NONE 
0.3697  0.0978  0.3656
NIST 
0.3389  0.0799  0.3192
Hum 
0.4105  0.1156  0.3948

R-2 
0.0917
0.0869
0.0886
0.0739
0.0879
0.0739
0.0796
0.0742
0.0924
0.0741
0.1101

R-1 
0.3759
0.3763
0.3758
0.3693
0.3770
0.3693
0.3720
0.3683
0.3653
0.3468
0.4235

R-1 
0.4158
0.4118
0.4106
0.3909
0.4109
0.3909
0.3962
0.3852
0.4193
0.0962
0.4757

R-2 
0.1015
0.1031
0.1030
0.0994
0.1051
0.0994
0.1069
0.1043
0.0934
0.0890
0.1249

R-1 
0.3909
0.3832
0.3830
0.3590
0.3836
0.3590
0.3642
0.3547
0.3936
0.3707
0.4532

R-2 
0.1036
0.1011
0.1058
0.1037
0.1048
0.1037
0.0992
0.0980
0.0915
0.0676
0.1134

 

 

R-2 
0.1135 
0.1103 
0.1121 
0.1027 
0.1119 
0.1027 
0.1037 
0.1013 
0.1140 
0.3978 
0.1402 

2009 B 

R-1 
R-2 
0.3693  0.0922 
0.3704  0.0946 
0.3723  0.0906 
0.3690  0.0956 
0.3731  0.0917 
0.3690  0.0956 
0.3650  0.0936 
0.3654  0.0945 
0.3595  0.0834 
0.3315  0.0761 
0.3901  0.1059 

Table 3. Results on the DUC 2005 - 2007 data sets 

2008 B 

2009 A 

Table 4. Results on the TAC 2008 - 2009 data sets 

4.5  Update Summarization 
Since year 2008, the DUC summarization track 
has  become  a  part  of  the  Text  Analysis 
Conference (TAC). In the update summarization 
task,  each  document  set  is  divided  into  two 
ordered sets A and B. The summarization target 
on set A is the same as the query-focused task in 
DUC 2005-2007. As to the set B, the target is to 
write  an  update  summary  of  the  documents  in 
set B, under the assumption that the reader has 

already  read  the  documents  in  set  A.  The  data 
set  of  each  year  contains  about  50  topics,  and 
each topic includes 10 documents for set A, 10 
documents  for  set  B  and  an  additional  query. 
For  set  A,  we  follow  exactly  the  same  method 
used  in  section  4.4;  for  set  B,  we  make  an 
additional novelty check for the sentences in B 
with 
the  MMR  approach.  Each  candidate 
sentence for set B is now compared to both the 
selected  sentences  in  set  B  and  in  set  A  to 

926

ensure  its  novelty.  In  the  update  task,  the 
summary length limit is 100 words.  

Table 4 above provides the average ROUGE-
1 and ROUGE-2 scores of all the systems on the 
TAC 2008-2009 data sets. The results on set A 
and  set  B  are  shown  individually.  For  the  task 
on set A which is almost the same as the DUC 
2005-2007  tasks,  the  results  are  also  very 
similar.  A  small  difference  is  that  the  systems 
with  position  features  perform  slightly  better 
than  the  system  None  on  these  two  data  sets. 
Also,  the  difference  between  word  position 
features and sentence position features becomes 
smaller.  One  reason  may  be  that  the  shorter 
summary 
the  chance  of 
generating  good  summaries  only  from  the 
leading  sentences.  This  is  somewhat  similar  to 
the  results  reported  in  (Nenkova,  2005)  that 
position information is more effective for short 
summaries. 

increases 

length 

For  the  update  set  B,  the  results  show  that 
position information is indeed very effective. In 
the results, all the systems with position features 
significantly  outperform  the  system  None.  We 
attribute the reason to the fact that we are more 
concerned  with  novel 
information  when 
summarizing update set B. Therefore, the effect 
of the query is less on set B, which means that 
the effect of position information may be more 
pronounced  in  contrast.  On  the  other  hand, 
when  comparing  the  position  features,  we  can 
see  that  though  the  difference  of  the  position 
features  is  quite  small,  word  position  features 
are still better in most cases.  
4.6  Discussion 
Based on the experiments, we briefly conclude 
the  effectiveness  of  position  information  in 
document summarization. In different tasks, the 
effectiveness  varies 
indeed.  It  depends  on 
whether the given task has a preference for the 
sentences  at  particular  positions.  Generally,  in 
generic  summarization,  the  position  hypothesis 
works  well  and 
the  ordinal  position 
information  is  effective.  In  this  case,  those 
position features that are more distinctive, such 
as GS and BF, can achieve better performances. 
In contrast, in the query-focused task that relates 
to  specified  content  in  the  documents,  ordinal 
position information is not so useful. Therefore, 
the  more  distinctive  a  position  feature  is,  the 

thus 

the 

worse performance it leads to. However, in the 
update  summarization  task  that  also  involves 
queries, position information becomes effective 
again  since  the  role  of  the  query  is  less 
dominant on the update document set.   

On the other hand, by comparing the sentence 
position features and word position features on 
all  the  data  sets,  we  can  draw  an  overall 
conclusion  that  word  position  features  are 
consistently more appreciated. For both generic 
tasks in which position information is effective 
and  query-focused  tasks  in  which  it  is  not  so 
effective,  word  position  features  show  their 
advantages over sentence position features. This 
is  because  of  the  looser  position  hypothesis 
postulated  by  them.  By  avoiding  arbitrarily 
regarding 
leading  sentences  as  more 
important,  they  are  more  adaptive  to  different 
tasks and data sets. 
5  Conclusion and Future Work 
In this paper, we proposed a novel kind of word 
position features which consider the positions of 
word appearances instead of sentence positions. 
The  word  position  features  were  compared  to 
sentence  position  features  under  the  proposed 
sentence  ranking  model.  From  the  results  on  a 
series of DUC data sets, we drew the conclusion 
that 
the  word  position  features  are  more 
effective and adaptive than traditional sentence 
position  features.  Moreover,  we  also  discussed 
the  effectiveness  of  position  information  in 
different summarization tasks. 

In our future work, we’d like to conduct more 
detailed  analysis  on  position 
information. 
Besides  the  ordinal  positions,  more  kinds  of 
position information can be considered to better 
model the document structures. Moreover, since 
position hypothesis  is  not  always  correct  in  all 
documents,  we’d  also  like  to  consider  a  pre-
classification method, aiming at identifying the 
documents  for  which  position  information  is 
more suitable. 

 
Acknowledgement  The  work  described  in 
this  paper  was  supported  by  Hong  Kong  RGC 
Projects  (PolyU5217/07E).  We  are  grateful  to 
professor  Chu-Ren  Huang  for  his  insightful 
suggestions and discussions with us. 

927

References
Edmundson, H. P.. 1969. New methods in automatic 
Extracting. Journal of the ACM, volume 16, issue 
2, pp 264-285. 

Gillick, D., Favre, B., Hakkani-Tur, D., Bohnet, B., 
Liu,  Y.,  Xie,  S..  2009.  The 
ICSI/UTD 
Summarization System at TAC 2009. Proceedings 
of Text Analysis Conference 2009.  

Jaime  G.  Carbonell  and  Jade  Goldstein.  1998.  The 
use  of  MMR,  diversity-based  reranking 
for 
reordering documents and producing summaries. 
Proceedings of the 21st annual international ACM 
SIGIR  conference  on  Research  and  development 
in information retrieval, pp 335-336. 

Lin,  C.  and  Hovy,  E..  1997.  Identifying  Topics  by 
Position.  Proceedings  of  the  fifth  conference  on 
Applied  natural  language  processing  1997,  pp 
283-290. 

Luhn,  H.  P..  1958.  The  automatic  creation  of 
literature abstracts. IBM J. Res. Develop. 2, 2, pp 
159-165. 

Nenkova.  2005.  Automatic  text  summarization  of 
newswire:  lessons  learned  from  the  document 

 

understanding  conference.  Proceedings  of  the 
20th  National  Conference 
on  Artificial 
Intelligence, pp 1436-1441. 

Ouyang,  Y.,  Li,  S.,  Li,  W..  2007.  Developing 
learning strategies for topic-based summarization. 
Proceedings of the sixteenth ACM conference on 
Conference  on 
information  and  knowledge 
management, pp 79-86. 

Radev,  D.,  Jing,  H.,  Sty´s,  M.  and  Tam,  D..  2004. 
of  multiple 
and 

Centroid-based 
documents. 
Management, volume 40, pp 919–938. 

summarization 

Information 

Processing 

query-based 

Schilder, F., Kondadadi, R.. 2008. FastSum: fast and 
accurate 
multi-document 
summarization.  Proceedings  of  the  46th  Annual 
Meeting  of  the  Association  for  Computational 
Linguistics  on  Human  Language  Technologies, 
short paper session, pp 205-208. 

et 

Toutanova,  K. 

al.  2007.  The  PYTHY 
summarization  system:  Microsoft  research  at 
DUC 
of  Document 
Understanding Conference 2007.  

Proceedings 

2007. 

