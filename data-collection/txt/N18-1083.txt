



















































From Phonology to Syntax: Unsupervised Linguistic Typology at Different Levels with Language Embeddings


Proceedings of NAACL-HLT 2018, pages 907–916
New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics

From Phonology to Syntax: Unsupervised Linguistic Typology
at Different Levels with Language Embeddings

Johannes Bjerva
Department of Computer Science

University of Copenhagen
Denmark

bjerva@di.ku.dk

Isabelle Augenstein
Department of Computer Science

University of Copenhagen
Denmark

augenstein@di.ku.dk

Abstract
A core part of linguistic typology is the clas-
sification of languages according to linguistic
properties, such as those detailed in the World
Atlas of Language Structure (WALS). Doing
this manually is prohibitively time-consuming,
which is in part evidenced by the fact that only
100 out of over 7,000 languages spoken in the
world are fully covered in WALS.

We learn distributed language representations,
which can be used to predict typological prop-
erties on a massively multilingual scale. Ad-
ditionally, quantitative and qualitative analy-
ses of these language embeddings can tell us
how language similarities are encoded in NLP
models for tasks at different typological levels.
The representations are learned in an unsuper-
vised manner alongside tasks at three typolog-
ical levels: phonology (grapheme-to-phoneme
prediction, and phoneme reconstruction), mor-
phology (morphological inflection), and syn-
tax (part-of-speech tagging).

We consider more than 800 languages and find
significant differences in the language rep-
resentations encoded, depending on the tar-
get task. For instance, although Norwegian
Bokmål and Danish are typologically close to
one another, they are phonologically distant,
which is reflected in their language embed-
dings growing relatively distant in a phonolog-
ical task. We are also able to predict typolog-
ical features in WALS with high accuracies,
even for unseen language families.

1 Introduction

For more than two and a half centuries, linguistic
typologists have studied languages with respect to
their structural and functional properties (Haspel-
math, 2001; Velupillai, 2012). Although typol-
ogy has a long history (Herder, 1772; Gabelentz,
1891; Greenberg, 1960, 1974; Dahl, 1985; Com-
rie, 1989; Haspelmath, 2001; Croft, 2002), com-
putational approaches have only recently gained

popularity (Dunn et al., 2011; Wälchli, 2014;
Östling, 2015; Cotterell and Eisner, 2017; Asgari
and Schütze, 2017; Malaviya et al., 2017; Bjerva
and Augenstein, 2018). One part of traditional ty-
pological research can be seen as assigning sparse
explicit feature vectors to languages, for instance
manually encoded in databases such as the World
Atlas of Language Structures (WALS, Dryer and
Haspelmath, 2013). A recent development which
can be seen as analogous to this is the process
of learning distributed language representations in
the form of dense real-valued vectors, often re-
ferred to as language embeddings (Tsvetkov et al.,
2016; Östling and Tiedemann, 2017; Malaviya
et al., 2017). We hypothesise that these language
embeddings encode typological properties of lan-
guage, reminiscent of the features in WALS, or
even of parameters in Chomsky’s Principles and
Parameters framework (Chomsky, 1993).

In this paper, we model languages in deep neu-
ral networks using language embeddings, consid-
ering three typological levels: phonology, mor-
phology and syntax. We consider four NLP tasks
to be representative of these levels: grapheme-to-
phoneme prediction and phoneme reconstruction,
morphological inflection, and part-of-speech tag-
ging. We pose three research questions (RQs):

RQ 1 Which typological properties are encoded
in task-specific distributed language repre-
sentations, and can we predict phonologi-
cal, morphological and syntactic properties
of languages using such representations?

RQ 2 To what extent do the encoded properties
change as the representations are fine tuned
for tasks at different linguistic levels?

RQ 3 How are language similarities encoded in
fine-tuned language embeddings?

907



One of our key findings is that language represen-
tations differ considerably depending on the tar-
get task. For instance, for grapheme-to-phoneme
mapping, the differences between the representa-
tions for Norwegian Bokmål and Danish increase
rapidly during training. This is due to the fact that,
although the languages are typologically close to
one another, they are phonologically distant.

2 Related work

Computational linguistics approaches to typology
are now possible on a larger scale than ever before
due to advances in neural computational models.
Even so, recent work only deals with fragments of
typology compared to what we consider here.

Computational typology has to a large extent
focused on exploiting word or morpheme align-
ments on the massively parallel New Testament,
in approximately 1,000 languages, in order to in-
fer word order (Östling, 2015) or assign linguis-
tic categories (Asgari and Schütze, 2017). Wälchli
(2014) similarly extracts lexical and grammatical
markers using New Testament data. Other work
has taken a computational perspective on language
evolution (Dunn et al., 2011), and phonology (Cot-
terell and Eisner, 2017; Alishahi et al., 2017).

Language embeddings In this paper, we follow
an approach which has seen some attention in the
past year, namely the use of distributed language
representations, or language embeddings. Some
typological experiments are carried out by Östling
and Tiedemann (2017), who learn language em-
beddings via multilingual language modelling and
show that they can be used to reconstruct ge-
nealogical trees. Malaviya et al. (2017) learn lan-
guage embeddings via neural machine translation,
and predict syntactic, morphological, and phonetic
features.

Contributions Our work bears the most resem-
blance to Bjerva and Augenstein (2018), who fine-
tune language embeddings on the task of PoS tag-
ging, and investigate how a handful of typological
properties are coded in these for four Uralic lan-
guages. We expand on this and thus contribute
to previous work by: (i) introducing novel qual-
itative investigations of language embeddings, in
addition to thorough quantitative evaluations; (ii)
considering four tasks at three different typologi-
cal levels; (iii) considering a far larger sample of

several hundred languages; and (iv) grounding the
language representations in linguistic theory.

3 Background

3.1 Distributed language representations

There are several methods for obtaining dis-
tributed language representations by training a re-
current neural language model (Mikolov et al.,
2010) simultaneously for different languages
(Tsvetkov et al., 2016; Östling and Tiedemann,
2017). In these recurrent multilingual lan-
guage models with long short-term memory cells
(LSTM, Hochreiter and Schmidhuber, 1997), lan-
guages are embedded into an n-dimensional
space. In order for multilingual parameter shar-
ing to be successful in this setting, the neural net-
work is encouraged to use the language embed-
dings to encode features of language. In this pa-
per, we explore the embeddings trained by Östling
and Tiedemann (2017), both in their original state,
and by further tuning them for our four tasks.
These are trained by training a multilingual lan-
guage model with language representations on a
collection of texts from the New Testament, cov-
ering 975 languages. While other work has looked
at the types of representations encoded in different
layers of deep neural models (Kádár et al., 2017),
we choose to look at the representations only in
the bottom-most embedding layer. This is moti-
vated by the fact that we look at several tasks using
different neural architectures, and want to ensure
comparability between these.

3.1.1 Language embeddings as continuous
Chomskyan parameter vectors

We now turn to the theoretical motivation of the
language representations. The field of NLP is
littered with distributional word representations,
which are theoretically justified by distributional
semantics (Harris, 1954; Firth, 1957), summarised
in the catchy phrase You shall know a word by the
company it keeps (Firth, 1957). We argue that lan-
guage embeddings, or distributed representations
of language, can also be theoretically motivated by
Chomsky’s Principles and Parameters framework
(Chomsky and Lasnik, 1993; Chomsky, 1993,
2014). Language embeddings encode languages
as dense real-valued vectors, in which the dimen-
sions are reminiscent of the parameters found in
this framework. Briefly put, Chomsky argues that
languages can be described in terms of principles

908



(abstract rules) and parameters (switches) which
can be turned either on or off for a given lan-
guage (Chomsky and Lasnik, 1993). An example
of such a switch might represent the positioning
of the head of a clause (i.e. either head-initial or
head-final). For English, this switch would be set
to the ‘initial’ state, whereas for Japanese it would
be set to the ‘final’ state. Each dimension in an n-
dimensional language embedding might also de-
scribe such a switch, albeit in a more continuous
fashion. The number of dimensions used in our
language representations, 64, is a plausible num-
ber of parameter vector dimensions (Dunn et al.,
2011). If we were able to predict typological fea-
tures using such representations, this lends support
to the argument that languages, at the very least,
can be represented by theoretically motivated pa-
rameter vectors, with the given dimensionality.

3.2 Typological features in the World Atlas of
Language Structure

In the experiments for RQ1 and RQ2 we predict
typological features extracted from WALS (Dryer
and Haspelmath, 2013). We choose to investi-
gate three linguistic levels of language: phonol-
ogy, morphology, and syntax. This is motivated by
three factors: (i) these features are related to NLP
tasks for which data is available for a large lan-
guage sample; (ii) the levels cover a range from
basic phonological and morphological structure,
to syntactic structure, allowing us to approach our
research question from several angles; and (iii) the
features in these categories are coded in WALS for
a relatively large selection of languages. We ex-
tract the three feature sets which represent these
typological levels of language from WALS.1

Phonological features cover 20 features rang-
ing from descriptions of the consonant and vowel
inventories of a particular language to presence
of tone and stress markers. As an example, con-
sider WALS feature 13A (Tone).2 This feature
takes three feature values: (i) no tones, (ii) simple
tone system, and (iii) complex tone system. Most
Indo-European languages, such as English, Span-
ish, and Russian, do not have any tones (i). Nor-
wegian and Swedish are exceptions to this, as they
both have simple tone systems (ii) similar to that
in Japanese. Finally, complex tone systems (iii)

1These are defined in the chapter structure in WALS:
http://wals.info/chapter

2http://wals.info/feature/13A

are typically found in several African languages
as well as languages in South-East Asia.

Morphological features cover a total of 41 fea-
tures. We consider the features included in the
Morphology chapter as well as those included in
the Nominal Categories chapter to be morpholog-
ical in nature.3 This includes features such as the
number of genders, usage of definite and indefinite
articles and reduplication. As an example, con-
sider WALS feature 37A (Definite Articles).4 This
feature takes five values: (i) Definite word dis-
tinct from demonstrative, (ii) Demonstrative word
used as definite article, (iii) Definite affix, (iv) No
definite, but indefinite article, and (v) No definite
or indefinite article. Again, most Indo-European
languages fall into category (i), with Norwegian,
Swedish, and Danish as relative outliers in cate-
gory (iii).

Word-order features cover 56 features, encod-
ing properties such as the ordering of subjects, ob-
jects and verbs. As an example, consider WALS
feature 81A (Order of Subject, Object and Verb).5

This feature takes all possible combinations of the
three word classes as its feature values, with the
addition of a special class for No dominant order.
Most languages in WALS fall into the categories
SOV (41.0%) and SVO (35.4%).

4 Method

The general set-up of the experiments in this pa-
per is as follows. We aim at answering our three
research questions dealing with typological prop-
erties and similarities as encoded in language em-
beddings. In order to do this, we attempt to predict
typological features as they are encoded in WALS,
using language embeddings which have been fine-
tuned during training on tasks related to different
typological properties. The main interest in this
paper is therefore not on how well each model per-
forms on a given NLP task, but rather on what the
resulting language embeddings encode.

Concretely, we use language embeddings ~li
from a given training iteration of a given task as
input to a k-NN classifier, which outputs the ty-
pological class a language belongs to (as coded
in WALS). We train separate classifiers for each

3This choice was made as, e.g., feature 37A (Definite Ar-
ticles) includes as a feature value whether a definite affix is
used.

4http://wals.info/feature/37A
5http://wals.info/feature/81A

909



typological property and each target task. When
i = 0, this indicates the pre-trained language em-
beddings as obtained from Östling and Tiedemann
(2017). Increasing i indicates the number of it-
erations over which the system at hand has been
trained. In each experiment, for a given iteration
i, we consider each ~li ∈ L where L is the inter-
section Ltask ∩ Lpre, where Ltask is the set of
languages for a given task, and Lpre is the set of
languages for which we have pre-trained embed-
dings.

All results in the following sections are the
mean of three-fold cross-validation, and the mean
over the WALS features in each given category.6

We run the experiments in a total of three set-
tings: (i) evaluating on randomly selected lan-
guage/feature pairs from a task-related feature
set; (ii) evaluating on an unseen language fam-
ily from a task-related feature set; (iii) evaluating
on randomly selected language/feature pairs from
all WALS feature sets. This allows us to estab-
lish how well we can predict task-related features
given a random sample of languages (i), and a
sample from which a whole language family has
been omitted (ii). Finally, (iii) allows us to com-
pare the task-specific feature encoding with a gen-
eral one.

A baseline reference is also included, which
is defined as the most frequently occurring typo-
logical trait within each category.7 For instance,
in the morphological experiments, we only con-
sider the 41 WALS features associated with the
categories of morphology and nominal categories.
The overlap between languages for which we have
data for morphological inflection and languages
for which these WALS features are coded is rela-
tively small (fewer than 20 languages per feature).
This small dataset size is why we have opted for
a non-parametric k-Nearest Neighbours classifier
for the typological experiments. We use k = 1, as
several of the features take a large number of class
values, and might only have a single instance rep-
resented in the training set.

Table 1 shows the datasets we consider (detailed
in later sections), the typological class they are re-
lated to, the size of the language sample in the

6The mean accuracy score is a harsh metric, as some fea-
tures are very difficult to predict due to them, e.g., being very
language specific or taking a large number of different values.

7The languages represented in several of the tasks under
consideration have a high Indo-European bias. Hence, sev-
eral of the properties have a relatively skewed distribution,
providing us with a strong baseline.

task, and the size of the intersection Ltask ∩ Lpre.
The number of pre-trained language embeddings,
|Lpre|, is 975 in all cases. We focus the evaluation
for each task-specific language embedding set on
the typological property relevant to that dataset. In
addition, we also evaluate on a set of all typolog-
ical properties in WALS. Note that the evaluation
on all properties is only comparable to the evalu-
ation on each specific property, as the set of lan-
guages under consideration differs between tasks.

Dataset Class |Ltask| |Ltask ∩ Lpre|
G2P Phonology 311 102
ASJP Phonology 4,664 824
SIGMORPHON Morphology 52 29
UD Syntax 50 27

Table 1: Overview of tasks and datasets.

5 Phonology

5.1 Grapheme-to-phoneme

We use grapheme-to-phoneme (G2P) as a proxy
of a phonological task (Deri and Knight, 2016;
Peters et al., 2017). The dataset contains over
650,000 such training instances, for a total of 311
languages (Deri and Knight, 2016). The task is to
produce a phonological form of a word, given its
orthographic form and the language in which it is
written. Crucially, this mapping is highly different
depending on the language at hand. For instance,
take the word written variation, which exists in
both English and French:

(English, variation) -> ­vE@ri"eIS@n
(French, variation) -> ­vaKja"sjÕ

5.1.1 Experiments and Analysis
We train a sequence-to-sequence model with at-
tention for the task of grapheme-to-phoneme map-
ping.8 The model takes as input the characters of
each source form together with the language em-
bedding for the language at hand and outputs a
predicted phonological form. Input and output al-
phabets are shared across all languages. The sys-
tem is trained over 3,000 iterations.

Quantitative results Since we consider
Grapheme-to-Phoneme as a phonological task, we
focus the quantitative evaluation on phonological
features from WALS. We run experiments using
the language embeddings as features for a simple

8The system is described in detail in Section 8.

910



k-NN classifier. The results in Table 2 indicate
that G2P is a poor proxy for language phonology,
however, as typological properties pertaining to
phonology are not encoded. That is to say, the
k-NN results do not outperform the baseline, and
performance is on par even after fine tuning (no
significant difference, p > 0.05). In the unseen
setting, however, we find that pre-trained language
embeddings are significantly better (p < 0.05)
at predicting the phonological features than both
fine-tuned ones and the baseline.

System / features Random phon. Unseen phon. All feat.

Most Frequent Class *75.46% 65.57% 79.90%
k-NN (pre-trained) 71.45% *86.54% 80.39%
k-NN (fine-tuned) 71.66% 82.36% 79.17%

Table 2: Accuracies on prediction of WALS features
with language embeddings fine-tuned on Grapheme-to-
Phoneme mapping. Asterisks indicate results signifi-
cantly higher than both other conditions (p < 0.05).

Qualitative results We now turn to why this
task is not a good proxy of phonology. The task of
grapheme-to-phoneme is more related to the pro-
cesses in the diachronic development of the writ-
ing system of a language than it is to the actual
genealogy or phonology of the language. This is
evident when considering the Scandinavian lan-
guages Norwegian and Danish which are typolog-
ically closely related, and have almost exactly the
same orthography. In spite of this fact, the phonol-
ogy of the two languages differs drastically due to
changes in Danish phonology, which impacts the
mapping from graphemes to phonemes severely.
Hence, the written forms of the two languages
should be very similar, which makes the language
embeddings based on language modelling highly
similar to one another. However, when the em-
beddings are fine-tuned on a task taking orthogra-
phy as well as phonology into account, this is no
longer the case. Figure 1 shows that the language
embeddings of Norwegian Bokmål and Danish di-
verge from each other, which is especially strik-
ing when comparing to the converging with the
typologically much more distant languages Taga-
log and Finnish. However, the absolute differ-
ence between Norwegian Bokmål and both Taga-
log/Finnish is still greater than that of Norwegian
Bokmål and Danish even after 3,000 iterations.

Figure 1: Language similarities between Norwegian,
and Danish/Tagalog/Finnish, as G2P-based embed-
dings are fine-tuned.

5.2 Phonological reconstruction

As a second phonological task, we look at phono-
logical reconstruction using word lists from the
Automated Similarity Judgement Program (ASJP,
Wichmann et al. (2016)). This resource contains
word lists of at least 40 words per language for
more than 4,500 languages. The task we consider
is to reproduce a given source phonological form,
also given the language, for instance:

(English, wat3r) -> wat3r

The intuition behind these experiments is that lan-
guages with similar phonetic inventories will be
grouped together, as reflected in changes in the
language embeddings.

5.2.1 Experiments and Analysis
We train a sequence-to-sequence model with at-
tention, framed as an auto-encoding problem, us-
ing the same sequence-to-sequence architecture
and setup as for the grapheme-to-phoneme task.
The model takes as input the characters of each
source form together with the language embed-
ding for the language at hand and outputs the pre-
dicted target form which is identical to the source
form.

Quantitative results Since we also consider
phonological reconstruction to be a phonologi-
cal task, we focus the quantitative evaluation on
phonological features from WALS. As with the
G2P experiments, Table 3 shows that the fine-
tuned embeddings do not offer predictive power
above the most frequent class baseline (p > 0.05).
Observing the changes in the language embed-
dings reveals that the embeddings are updated to

911



a very small extent, indicating that these are not
used by the model to a large extent. This can be
explained by the fact that the task is highly similar
for each language, and that the model largely only
needs to learn to copy the input string.

We do, however find that evaluating on a set
with an unseen language family does yield results
significantly above baseline levels with the pre-
trained embeddings (p < 0.05), which together
with the G2P results indicate that the language
modelling objective does encode features to some
extent related to phonology.

System / features Random phon. Unseen phon. All feat.

Most Frequent Class *59.39% 63.71% *58.12%
k-NN (pre-trained) 53.02% *77.44% 51.6%
k-NN (fine-tuned) 53.09% *77.45% 51.9%

Table 3: Accuracies on prediction of WALS features
with language embeddings fine-tuned on Phonological
Reconstruction. Asterisks indicate results significantly
higher than non-bold conditions (p < 0.05).

6 Morphology

6.1 Morphological inflection

We use data from the Unimorph project, specif-
ically the data released for the SIGMORPHON-
2017 shared task (Cotterell et al., 2017).9 This
data covers 52 languages, thereby representing a
relatively large typological variety. Whereas the
shared task has two subtasks, namely inflection
and paradigm cell filling, we only train our sys-
tem using the inflection task. This was a choice
of convenience, as we are not interested in solv-
ing the task of morphological paradigm cell filling,
but rather observing the language embeddings as
they are fine-tuned. Furthermore we focus on the
high-resource setting in which we have access to
10,000 training examples per language. The in-
flection subtask is to generate a target inflected
form given a lemma with its part-of-speech as in
the following example:

(release, V;V.PTCP;PRS) -> releasing

6.1.1 Morphological experiments
We train a sequence-to-sequence model with
attention over 600 iterations, using the same
sequence-to-sequence architecture from the previ-
ous tasks.

9https://unimorph.github.io/

Quantitative results Since this is considered a
morphological task, Table 4 contains results using
the language embeddings to predict morphological
properties. The fine-tuned language embeddings
in this condition are able to predict morphological
properties in WALS significantly above baseline
levels and pre-trained embeddings (p < 0.05). We
further also obtain significantly better results in the
unseen setting (p < 0.05), in which the language
family evaluated on is not used in training. This
indicates that these properties are important to the
model when learning the task at hand.

System / Features Random morph. Unseen morph. All feat.

Most Frequent Class 77.98% 85.68% 84.12%
k-NN (pre-trained) 74.49% 88.83% 84.97%
k-NN (fine-tuned) *82.91% *91.92% 84.95%

Table 4: Accuracies on prediction of WALS features
with language embeddings fine-tuned on morpholog-
ical inflection. Asterisks indicate results significantly
higher than both other conditions (p < 0.05).

Qualitative results The performance of the
fine-tuned embeddings on prediction of morpho-
logical features is above baseline for most fea-
tures. For 18 out of the 35 features under consider-
ation both the baseline and k-NN performances are
at 100% from the outset, so these are not consid-
ered here.10 Figure 2 shows two of the remaining
17 features.11 We can observe two main patterns:
For some features such as 49A (Number of cases),
fine-tuning on morphological inflection increases
the degree to which the features are encoded in the
language embeddings. This can be explained by
the fact that the number of cases in a language is
central to how morphological inflection is treated
by the model. For instance, languages with the
same number of cases might benefit from sharing
certain parameters. On the other hand, the feature
38A (Indefinite Articles) mainly encodes whether
the indefinite word is distinct or not from the word
for one, and it is therefore not surprising that this
is not learned in morphological inflection.

7 Word order

7.1 Part-of-speech tagging
We use PoS annotations from version 2 of the
Universal Dependencies (Nivre et al., 2016). As

10This is partially explained by the fact that certain cate-
gories were completely uniform in the small sample as well
as by the Indo-European bias in the sample.

11The full feature set is included in the Supplements.

912



0 200 400 600
Iterations

0.0

0.2

0.4

0.6

0.8

1.0
A

cc
ur

ac
y

38A Indefinite Articles

k-NN
Most Frequent

0 200 400 600
Iterations

0.0

0.2

0.4

0.6

0.8

1.0

A
cc

ur
ac

y

49A Number of Cases

k-NN
Most Frequent

Figure 2: Prediction of two morphological features in
WALS with morphological language embeddings, one
data point per 50 iterations.

we are mainly interested in observing the lan-
guage embeddings, we down-sample all training
sets to 1,500 sentences (approximate number of
sentences of the smallest data sets) so as to min-
imise any size-related effects.

7.1.1 Word-order experiments
We approach the task of PoS tagging using a fairly
standard bi-directional LSTM architecture based
on Plank et al. (2016), detailed in Section 8.

Quantitative results Table 5 contains results on
WALS feature prediction using language embed-
dings fine-tuned on PoS tagging. We consider both
the set of word order features, which are relevant
for the dataset, and a set of all WALS features.
Using the fine-tuned embeddings is significantly
better than both the baseline and the pre-trained
embeddings (p < 0.05), in both the random and
the unseen conditions, indicating that the model
learns something about word order typology. This
can be expected, as word order features are highly
relevant when assigning a PoS tag to a word.

System / features Random W-Order Unseen W-Order All feat.

Most Frequent Class 76.81% 82.47% 82.93%
k-NN (pre-trained) 76.66% 92.76% 82.69%
k-NN (fine-tuned) *80.81% *94.48% 83.55%

Table 5: Accuracies on prediction of WALS features
with language embeddings fine-tuned on PoS tagging.
Asterisks indicate the result in bold significantly out-
performing both other conditions (p < 0.05).

Qualitative results We now turn to the syntac-
tic similarities between languages as encoded in
the fine-tuned language embeddings. We con-
sider a set of the North-Germanic languages Ice-
landic, Swedish, Norwegian Nynorsk, Danish,
Norwegian Bokmål, the West-Germanic language

English, and the Romance languages Spanish,
French, and Italian. We apply hierarchical cluster-
ing using UPGMA (Michener and Sokal, 1957) to
the pre-trained language embeddings of these lan-
guages.12 Striking here is that English is grouped
together with the Romance languages. This can
be explained by the fact that English has a large
amount of vocabulary stemming from Romance
loan words, which under the task of language
modelling results in a higher similarity with such
languages. We then cluster the embeddings fine-
tuned on PoS tagging in the same way. In this con-
dition, English has joined the rest of the Germanic
languages’ cluster. This can be explained by the
fact that, in terms of word ordering and morpho-
syntax, English is more similar to these languages
than it is to the Romance ones.

We can also observe that, whereas the ortho-
graphically highly similar Norwegian Bokmål and
Danish form the first sub-cluster in the pre-trained
condition, Norwegian Nynorsk replaces Danish in
this pairing when fine-tuning on PoS tagging. This
can be explained by the fact that morpho-syntactic
similarities between the two written varieties of
Norwegian are more similar to one another.

8 Implementation

8.1 Sequence-to-sequence modelling
The system architecture used in the sequence-to-
sequence tasks, i.e., G2P, phonological reconstruc-
tion, and morphological inflection is depicted in
Figure 3. The system is based on that devel-
oped by Östling and Bjerva (2017) and is imple-
mented using Chainer (Tokui et al., 2015). We
modify the architecture by concatenating a lan-
guage embedding, ~l, to the character embeddings
before encoding. In the grapheme-to-phoneme
and phonological reconstruction experiments, the
one-hot feature mapping before decoding is irrel-
evant and therefore omitted. The rest of the hyper-
parameters are the same as in Östling and Bjerva
(2017).

8.2 Sequence labelling
This system is based on Plank et al. (2016), and is
implemented using DyNet (Neubig et al., 2017).
We train using the Adam optimisation algorithm
(Kingma and Ba, 2014) over a maximum of 10
epochs using early stopping. We make two modifi-
cations to the bi-LSTM architecture of Plank et al.

12Included in the Supplements due to space restrictions.

913



h1 h2 h3 h4 hn

+

LSTM Encoder

Attention

LSTM Decoder h1 h2 h3 h4 hn

sapandınız

h1

h1

s

h2

h2

a

h3

h3

p

h4

h4

a

hn

hn

n

Embed

hn

hn

n

N;LGSPEC1;2P;SG;PST

One-hot

Concat.

2

100

101

102

103

104

105

106

107

108

109

110

111

112

113

114

115

116

117

118

119

120

121

122

123

124

125

126

127

128

129

130

131

132

133

134

135

136

137

138

139

140

141

142

143

144

145

146

147

148

149

150

151

152

153

154

155

156

157

158

159

160

161

162

163

164

165

166

167

168

169

170

171

172

173

174

175

176

177

178

179

180

181

182

183

184

185

186

187

188

189

190

191

192

193

194

195

196

197

198

199

NAACL-HLT 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

2010) simultaneously for different languages
(Tsvetkov et al., 2016; Östling and Tiedemann,
2017). In these recurrent multilingual lan-
guage models with long short-term memory cells
(LSTM, Hochreiter and Schmidhuber, 1997), lan-
guages are embedded into a n-dimensional space.
In order for multilingual parameter sharing to be
successful in this setting, the neural network is en-
couraged to use the language embeddings to en-
code features of language. Other work has ex-
plored learning language embeddings in the con-
text of neural machine translation (Malaviya et al.,
2017). In this work, we explore the embeddings
trained by Östling and Tiedemann (2017), both in
their original state, and by further tuning them for
PoS tagging.

2.3 Typological data

In the experiments for RQ3, we attempt to pre-
dict typological features. We extract the features
we aim to predict from WALS (Dryer and Haspel-
math, 2013). We consider features which are en-
coded for all four Uralic languages in our sample.

2.4 Language Embeddings as Chomskyan
Parameter Vectors

3 Morphology

3.1 Morphological inflection

Unimorph

3.2 Morphological Experiments

We train a sequence-to-sequence model based
on the system developed by Östling and Bjerva
(2017). The neural architecture is modified so
as to include an embedded language representa-
tion. During training, the errors are also back-
propagated into this embedding, meaning that the
encoded representation will be fine-tuned as the
task is learned. The system architecture is de-
picted in Figure ??.
~l

4 Phonology

4.1 Grapheme-to-phoneme

g2p data

4.2 Phonological Experiments

We train a sequence-to-sequence model identical
to the morphological system, using grapheme-to-
phoneme data.

5 Morphosyntax

5.1 Part-of-speech tagging

We use PoS annotations from version 2 of the Uni-
versal Dependencies (Nivre et al., 2016). We focus
on the four Uralic languages present in the UD,
namely Finnish (based on the Turku Dependency
Treebank, Pyysalo et al., 2015), Estonian (Muis-
chnek et al., 2016), Hungarian (based on the Hun-
garian Dependency Treebank, Vincze et al., 2010),
and North Sámi (Sheyanova and Tyers, 2017). As
we are mainly interested in observing the language
embeddings, we down-sample all training sets to
1500 sentences (approximate number of sentences
in the Hungarian data), so as to minimise any size-
based effects.

6 Method and experiments

We approach the task of PoS tagging using a fairly
standard bi-directional LSTM architecture, based
on Plank et al. (2016). The system is implemented
using DyNet (Neubig et al., 2017). We train
using the Adam optimisation algorithm (Kingma
and Ba, 2014) over a maximum of 10 epochs,
using early stopping. We make two modifica-
tions to the bi-LSTM architecture of Plank et al.
(2016). First of all, we do not use any atomic
embedded word representations, but rather use
only character-based word representations. This
choice was made so as to encourage the model
not to rely on language-specific vocabulary. Ad-
ditionally, we concatenate a pre-trained language
embedding to each word representation. That is
to say, in the original bi-LSTM formulation of
Plank et al. (2016), each word w is represented as
~w + LSTMc(w), where ~w is an embedded word
representation, and LSTMc(w) is the final states
of a character bi-LSTM running over the charac-
ters in a word. In our formulation, each word w
in language l is represented as LSTMc(w) + ~l,
where LSTMc(w) is defined as before, and ~l is
an embedded language representation. We use a
two-layer deep bi-LSTM, with 100 units in each
layer. The character embeddings used also have
100 dimensions. We update the language repre-
sentations, ~l, during training. The language repre-
sentations are 64-dimensional, and are initialised
using the language embeddings from Östling and
Tiedemann (2017). All PoS tagging results re-
ported are the average of five runs, each with dif-
ferent initialisation seeds, so as to minimise ran-

2

100

101

102

103

104

105

106

107

108

109

110

111

112

113

114

115

116

117

118

119

120

121

122

123

124

125

126

127

128

129

130

131

132

133

134

135

136

137

138

139

140

141

142

143

144

145

146

147

148

149

150

151

152

153

154

155

156

157

158

159

160

161

162

163

164

165

166

167

168

169

170

171

172

173

174

175

176

177

178

179

180

181

182

183

184

185

186

187

188

189

190

191

192

193

194

195

196

197

198

199

NAACL-HLT 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

2010) simultaneously for different languages
(Tsvetkov et al., 2016; Östling and Tiedemann,
2017). In these recurrent multilingual lan-
guage models with long short-term memory cells
(LSTM, Hochreiter and Schmidhuber, 1997), lan-
guages are embedded into a n-dimensional space.
In order for multilingual parameter sharing to be
successful in this setting, the neural network is en-
couraged to use the language embeddings to en-
code features of language. Other work has ex-
plored learning language embeddings in the con-
text of neural machine translation (Malaviya et al.,
2017). In this work, we explore the embeddings
trained by Östling and Tiedemann (2017), both in
their original state, and by further tuning them for
PoS tagging.

2.3 Typological data

In the experiments for RQ3, we attempt to pre-
dict typological features. We extract the features
we aim to predict from WALS (Dryer and Haspel-
math, 2013). We consider features which are en-
coded for all four Uralic languages in our sample.

2.4 Language Embeddings as Chomskyan
Parameter Vectors

3 Morphology

3.1 Morphological inflection

Unimorph

3.2 Morphological Experiments

We train a sequence-to-sequence model based
on the system developed by Östling and Bjerva
(2017). The neural architecture is modified so
as to include an embedded language representa-
tion. During training, the errors are also back-
propagated into this embedding, meaning that the
encoded representation will be fine-tuned as the
task is learned. The system architecture is de-
picted in Figure ??.
~l

4 Phonology

4.1 Grapheme-to-phoneme

g2p data

4.2 Phonological Experiments

We train a sequence-to-sequence model identical
to the morphological system, using grapheme-to-
phoneme data.

5 Morphosyntax

5.1 Part-of-speech tagging

We use PoS annotations from version 2 of the Uni-
versal Dependencies (Nivre et al., 2016). We focus
on the four Uralic languages present in the UD,
namely Finnish (based on the Turku Dependency
Treebank, Pyysalo et al., 2015), Estonian (Muis-
chnek et al., 2016), Hungarian (based on the Hun-
garian Dependency Treebank, Vincze et al., 2010),
and North Sámi (Sheyanova and Tyers, 2017). As
we are mainly interested in observing the language
embeddings, we down-sample all training sets to
1500 sentences (approximate number of sentences
in the Hungarian data), so as to minimise any size-
based effects.

6 Method and experiments

We approach the task of PoS tagging using a fairly
standard bi-directional LSTM architecture, based
on Plank et al. (2016). The system is implemented
using DyNet (Neubig et al., 2017). We train
using the Adam optimisation algorithm (Kingma
and Ba, 2014) over a maximum of 10 epochs,
using early stopping. We make two modifica-
tions to the bi-LSTM architecture of Plank et al.
(2016). First of all, we do not use any atomic
embedded word representations, but rather use
only character-based word representations. This
choice was made so as to encourage the model
not to rely on language-specific vocabulary. Ad-
ditionally, we concatenate a pre-trained language
embedding to each word representation. That is
to say, in the original bi-LSTM formulation of
Plank et al. (2016), each word w is represented as
~w + LSTMc(w), where ~w is an embedded word
representation, and LSTMc(w) is the final states
of a character bi-LSTM running over the charac-
ters in a word. In our formulation, each word w
in language l is represented as LSTMc(w) + ~l,
where LSTMc(w) is defined as before, and ~l is
an embedded language representation. We use a
two-layer deep bi-LSTM, with 100 units in each
layer. The character embeddings used also have
100 dimensions. We update the language repre-
sentations, ~l, during training. The language repre-
sentations are 64-dimensional, and are initialised
using the language embeddings from Östling and
Tiedemann (2017). All PoS tagging results re-
ported are the average of five runs, each with dif-
ferent initialisation seeds, so as to minimise ran-

Concat.

Figure 3: System architecture used in the seq-to-seq
tasks (morphological inflection, G2P, and phonological
reconstruction). Figure adapted with permission from
Östling and Bjerva (2017).

(2016). First of all, we do not use any atomic em-
bedded word representations, but rather use only
character-based word representations. This choice
was made so as to encourage the model not to rely
on language-specific vocabulary. Additionally, we
concatenate a pre-trained language embedding to
each word representation. In our formulation,
each word w is represented as LSTMc(w) + ~l,
where LSTMc(w) is the final states of a charac-
ter bi-LSTM running over the characters in a word
and~l is an embedded language representation. We
use a two-layer deep bi-LSTM with 100 units in
each layer, and 100-dimensional character embed-
dings. The rest of the hyper-parameters are the
same as in Plank et al. (2016).13

9 Discussion and Conclusions

The language embeddings obtained by fine-tuning
on linguistic tasks at various typological levels
were found to include typological information
somehow related to the task at hand. This lends
some support to the theoretical foundations of
such representations, in that it shows that it is pos-
sible to learn something akin to a vector of contin-
uous Chomskyan parameters (Chomsky, 1993).

9.1 RQ1: Encoding of typological features in
task-specific language embeddings

The features which are encoded depend to a large
degree on the task at hand. The language em-
beddings resulting from the phonological tasks did
not encode phonological properties in the sense of
WALS features, whereas the pre-trained ones did.

13Both modified systems are included in the Supplements,
and will be made publicly available.

La
ng

ua
ge

 M
od

el
lin

g
Po

S 
ta

gg
in

g

“Romance” “Germanic”

Figure 4: Language similarities changing during fine
tuning.

The morphological language embeddings were
found to encode morphological features, and the
PoS language embeddings were similarly found to
encode word order features.

A promising result is the fact that we were able
to predict typological features for unseen language
families. That is to say, without showing, e.g., a
single Austronesian training instance to the k-NN
classifier, typological features could still be pre-
dicted with high accuracies. This indicates that we
can predict typological features with language em-
beddings, even for languages for which we have
no prior typological knowledge.

Table 6 contains a comparison of the top five
and bottom five feature prediction accuracies for
the ASJP task.14 In the case of the phonologically
oriented ASJP task it is evident that the embed-
dings still encode something related to phonology,
as four out of five best features are phonological.

9.2 RQ2: Change in encoding of typological
features

The changes in the features encoded in language
embeddings are relatively monotonic. Features are
either learnt, forgotten, or remain static throughout
training. This indicates that the language represen-
tations converge towards a single optimum.

9.3 RQ3: Language similarities

Training language embeddings in the task of mul-
tilingual language modelling has been found to re-
produce trees which are relatively close matches

14See the Supplement for the remaining tasks.

914



to more traditional genealogical trees (Östling and
Tiedemann, 2017). We show a similar analysis
considering pre-trained and PoS fine-tuned em-
beddings, and it is noteworthy that fine-tuning on
PoS tagging in this case yielded a tree more faith-
ful to genealogical trees, such as those represented
on glottolog.org. Figure 4 shows an exam-
ple of this, in which a language modelling objec-
tive places English with Romance languages. This
makes sense, as the English lexicon contains a
large amount of Romance vocabulary. When fine-
tuning on PoS tagging, however, English is placed
among the Germanic languages, as it shares more
syntactic similarities with these.

Another striking result in terms of language
similarities in fine-tuned language embedding
spaces was found in the G2P task. We here found
that the phonological differences between some
otherwise similar languages, such as Norwegian
Bokmål and Danish, were accurately encoded.

Task Feature WALS Chapter Accuracy

ASJP

6A Phonology 89.45
18A Phonology 88.91
20A Morphology 82.74
19A Phonology 82.58

7A Phonology 80.97

144A Word Order 14.48
144L Word Order 10.07
62A Nominal Syntax 10.00
81B Word Order 9.52

133A Lexicon 8.18

Table 6: Top 5 and bottom 5 accuracies in feature pre-
diction using phonological language embeddings.

Acknowledgements

We would also like to thank Robert Östling for
giving us access to the pre-trained language em-
beddings. Isabelle Augenstein is supported by Eu-
rostars grant Number E10138. We further grate-
fully acknowledge the support of NVIDIA Corpo-
ration with the donation of the Titan Xp GPU used
for this research.

References

Afra Alishahi, Marie Barking, and Grzegorz Chrupała.
2017. Encoding of phonology in a recurrent neu-
ral model of grounded speech. In Proceedings
of the 21st Conference on Computational Natural
Language Learning (CoNLL 2017). Association for
Computational Linguistics, pages 368–378.

Ehsaneddin Asgari and Hinrich Schütze. 2017. Past,
present, future: A computational investigation of the
typology of tense in 1000 languages. In EMNLP.
Association for Computational Linguistics, pages
113–124.

Johannes Bjerva and Isabelle Augenstein. 2018. Track-
ing Typological Traits of Uralic Languages in Dis-
tributed Language Representations. In Proceedings
of the Fourth International Workshop on Computa-
tional Linguistics for Uralic Languages (IWCLUL).

Noam Chomsky. 1993. Lectures on government and
binding: The Pisa lectures. 9. Walter de Gruyter.

Noam Chomsky. 2014. The minimalist program. MIT
press.

Noam Chomsky and Howard Lasnik. 1993. The theory
of principles and parameters.

Bernard Comrie. 1989. Language universals and lin-
guistic typology: Syntax and morphology. Univer-
sity of Chicago press.

Ryan Cotterell and Jason Eisner. 2017. Probabilistic
typology: Deep generative models of vowel inven-
tories. In ACL. Association for Computational Lin-
guistics, pages 1182–1192.

Ryan Cotterell, Christo Kirov, John Sylak-Glassman,
Géraldine Walther, Ekaterina Vylomova, Patrick
Xia, Manaal Faruqui, Sandra Kübler, David
Yarowsky, Jason Eisner, et al. 2017. Conll-
sigmorphon 2017 shared task: Universal morpho-
logical reinflection in 52 languages. arXiv preprint
arXiv:1706.09031 .

William Croft. 2002. Typology and universals. Cam-
bridge University Press.

Östen Dahl. 1985. Tense and Aspect Systems. Basil
Blackwell Ltd., NewYork.

Aliya Deri and Kevin Knight. 2016. Grapheme-to-
phoneme models for (almost) any language. In ACL.
Association for Computational Linguistics, pages
399–408.

Matthew S. Dryer and Martin Haspelmath, editors.
2013. WALS Online. Max Planck Institute for Evo-
lutionary Anthropology, Leipzig. http://wals.
info/.

Michael Dunn, Simon J Greenhill, Stephen C Levin-
son, and Russell D Gray. 2011. Evolved structure
of language shows lineage-specific trends in word-
order universals. Nature 473(7345):79–82.

J. R. Firth. 1957. A synopsis of linguistic theory pages
1930–1955. 1952–1959:1–32.

Georg von der Gabelentz. 1891. Die Sprachwis-
senschaft, ihre Aufgaben, Methoden und bisherigen
Ergebnisse. Leipzig.

915



Joseph Greenberg. 1974. Language typology: A his-
torical and analytic overview, volume 184. Walter
de Gruyter.

Joseph H Greenberg. 1960. A quantitative approach
to the morphological typology of language. Inter-
national journal of American linguistics 26(3):178–
194.

Z. Harris. 1954. Distributional structure. Word
10:146–162.

Martin Haspelmath. 2001. Language typology and lan-
guage universals: An international handbook, vol-
ume 20. Walter de Gruyter.

J. Herder. 1772. Abhandlung über den Ursprung der
Sprache. Berlin: Christian Friedrich Voß.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation
9(8):1735–1780.

Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980 .

Ákos Kádár, Grzegorz Chrupała, and Afra Alishahi.
2017. Representation of linguistic form and func-
tion in recurrent neural network. Computational
Linguistics 43:761–780.

Chaitanya Malaviya, Graham Neubig, and Patrick Lit-
tell. 2017. Learning language representations for
typology prediction. In EMNLP. Association for
Computational Linguistics, pages 2519–2525.

Charles D Michener and Robert R Sokal. 1957. A
quantitative approach to a problem in classification.
Evolution 11(2):130–162.

Tomas Mikolov, Martin Karafiát, Lukas Burget, Jan
Cernockỳ, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In Inter-
speech. volume 2, page 3.

Graham Neubig, Chris Dyer, Yoav Goldberg, Austin
Matthews, Waleed Ammar, Antonios Anastasopou-
los, Miguel Ballesteros, David Chiang, Daniel
Clothiaux, Trevor Cohn, et al. 2017. Dynet: The
dynamic neural network toolkit. arXiv preprint
arXiv:1701.03980 .

Joakim Nivre, Marie-Catherine de Marneffe, Filip Gin-
ter, Yoav Goldberg, Jan Hajic, Christopher D Man-
ning, Ryan McDonald, Slav Petrov, Sampo Pyysalo,
Natalia Silveira, et al. 2016. Universal dependencies
v1: A multilingual treebank collection. In Proceed-
ings of the 10th International Conference on Lan-
guage Resources and Evaluation (LREC 2016).

Robert Östling. 2015. Word order typology through
multilingual word alignment. In The 53rd Annual
Meeting of the Association for Computational Lin-
guistics and the 7th International Joint Conference
on Natural Language Processing. pages 205–211.

Robert Östling and Johannes Bjerva. 2017. Su-rug at
the conll-sigmorphon 2017 shared task: Morpholog-
ical inflection with attentional sequence-to-sequence
models. In Proceedings of the CoNLL SIGMOR-
PHON 2017 Shared Task: Universal Morphologi-
cal Reinflection. Association for Computational Lin-
guistics, pages 110–113.

Robert Östling and Jörg Tiedemann. 2017. Continuous
multilinguality with language vectors. In EACL. As-
sociation for Computational Linguistics, pages 644–
649.

Ben Peters, Jon Dehdari, and Josef van Genabith.
2017. Massively multilingual neural grapheme-to-
phoneme conversion. In Proceedings of the First
Workshop on Building Linguistically Generalizable
NLP Systems. pages 19–26.

Barbara Plank, Anders Søgaard, and Yoav Goldberg.
2016. Multilingual part-of-speech tagging with
bidirectional long short-term memory models and
auxiliary loss. In ACL. Association for Computa-
tional Linguistics, pages 412–418.

Seiya Tokui, Kenta Oono, Shohei Hido, and Justin
Clayton. 2015. Chainer: a next-generation open
source framework for deep learning. In Proceedings
of Workshop on Machine Learning Systems (Learn-
ingSys) in The Twenty-ninth Annual Conference on
Neural Information Processing Systems (NIPS).

Yulia Tsvetkov, Sunayana Sitaram, Manaal Faruqui,
Guillaume Lample, Patrick Littell, David
Mortensen, Alan W Black, Lori Levin, and
Chris Dyer. 2016. Polyglot neural language models:
A case study in cross-lingual phonetic represen-
tation learning. In NAACL-HLT . Association for
Computational Linguistics, pages 1357–1366.

Viveka Velupillai. 2012. An introduction to linguistic
typology. John Benjamins Publishing.

Bernhard Wälchli. 2014. Algorithmic typology and
going from known to similar unknown categories
within and across languages. Aggregating Dialec-
tology, Typology, and Register Analysis: Linguistic
Variation in Text and Speech 28:355.

Søren Wichmann, Eric W. Holman, and Cecil H.
Brown (eds.). 2016. The ASJP Database (version
17).

916


