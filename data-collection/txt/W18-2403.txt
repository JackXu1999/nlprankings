



















































A Sequence Learning Method for Domain-Specific Entity Linking


Proceedings of the Seventh Named Entities Workshop, pages 14–21
Melbourne, Australia, July 20, 2018. c©2018 Association for Computational Linguistics

14

A Sequence Learning Method for Domain-Specific Entity Linking

Emrah Inan
Department of Computer Engineering

Ege University
emrah.inan@ege.edu.tr

Oguz Dikenelli
Department of Computer Engineering

Ege University
oguz.dikenelli@ege.edu.tr

Abstract

Recent collective Entity Linking studies
usually promote global coherence of all
the mapped entities in the same document
by using semantic embeddings and graph-
based approaches. Although graph-based
approaches are shown to achieve remark-
able results, they are computationally ex-
pensive for general datasets. Also, seman-
tic embeddings only indicate relatedness
between entity pairs without considering
sequences. In this paper, we address these
problems by introducing a two-fold neu-
ral model. First, we match easy mention-
entity pairs and using the domain informa-
tion of this pair to filter candidate entities
of closer mentions. Second, we resolve
more ambiguous pairs using bidirectional
Long Short-Term Memory and CRF mod-
els for the entity disambiguation. Our pro-
posed system outperforms state-of-the-art
systems on the generated domain-specific
evaluation dataset.

1 Introduction

Entity Linking is the task of matching ambiguous
mentions in a text to the corresponding entities in
the given knowledge base. The output of the en-
tity linking is a crucial step for many tasks, in-
cluding relation extraction (Weston et al., 2013),
link prediction (Nickel et al., 2015) and knowl-
edge graph completion (Minervini et al., 2016).
The main challenge is to disambiguate candidate
entities for the given mentions. For instance, it
requires to resolve the mention Wicker Park in
the following text ”Wicker Park is a 2004 Amer-
ican psychological drama mystery film directed by
Paul McGuigan and starring Josh Hartnett...” to

the referent entity Wicker Park (film)1 in DBpe-
dia. But the mention Wicker Park has three differ-
ent candidate entities as indicated in the Wikipedia
disambiguation page of this mention.

The key step for entity disambiguation is the
similarity computation between mention-entity
and entity-entity pairs. Early studies focused on
modeling the similarity between local context that
computes the similarity between mention con-
text and relevant candidate entities (Bunescu and
Paşca, 2006; Mihalcea and Csomai, 2007). Re-
cent state-of-the-art methods consider global co-
herence that is the relatedness between all can-
didate entities in the same document (Milne and
Witten, 2008; Kulkarni et al., 2009; Ratinov et al.,
2011). These methods depend on well-defined
link structures as seen in Wikipedia to compute
global coherence. After the emergence of word
embeddings (Mikolov et al., 2013), it facilitates
to produce more generalized coherence computa-
tions without using hand-crafted features. Hence,
the dependency of well-defined knowledge bases
has decreased and knowledge base agnostic ap-
proaches become revealed (Zwicklbauer et al.,
2016). Most recent deep learning approaches have
been presented as a way to support better general-
ization for the similarity measurement of context,
mention and entity (Sun et al., 2015). Also, men-
tions and entities are combined into the same con-
tinuous vector space for the entity disambiguation
(Yamada et al., 2016). From a different perspec-
tive, the entity disambiguation should be trans-
formed into as a sequence learning task to capture
more generalized semantics between candidate en-
tities and also mentions.

In this paper, we generate RDF embeddings
(Ristoski and Paulheim, 2016) as the input of
a sequence learning model using bidirectional

1http://dbpedia.org/resource/Wicker Park (film)



15

Long Short-Term Memory (LSTM) (Graves and
Schmidhuber, 2005). Then, we perform Con-
ditional Random Field (CRF) to match the best
mention-entity pairs. LSTM networks are not suit-
able for large entity vocabularies since English
DBpedia contains more than 5M entities. To re-
duce the size of these vocabularies, our study em-
ploys the two-fold method. First, we match easy
mention-entity pairs in which each mention con-
tains only one candidate entity. Similar to AIDA-
light study (Hoffart et al., 2013) we identify the
domain of the given text and the size of candi-
date entities are reduced to reasonable dimensions
for the detected domain. The contributions of our
study can be summarized as below:

• Our study proposes a novel algorithm that
first disambiguates easy mention-entity pairs
for a specific domain. Thereafter, it applies
CRF model to link more ambiguous entities.

• Our study provides a sequence learning
model like a translation task in which a se-
quence of mentions will be translated into a
sequence of referent entities in the domain-
specific knowledge base.

Our method employs one of prominent Named
Entity Recognition approaches (Lample et al.,
2016) to perform a domain-specific Entity Link-
ing. We aim to model the topical coherence of the
mention-entity pairs in terms of a sequence label-
ing task. We conduct the experimental setup us-
ing the well-known evaluation framework called
GERBIL (Usbeck et al., 2015) to compare our
study with the state-of-the-art Entity Linking sys-
tems. The rest of this paper is organized as fol-
lows: In Section 2, it gives an overview of re-
lated work. In Section 3, the sequence learning
method is proposed for a specific domain. Sec-
tion 4 presents the experiments are for the selected
approaches on the prepared evaluation dataset.
We conclude our study and highlight the research
questions in Section 5.

2 Related Work

Common trends in Entity Linking employs the
global coherence to identify entities. Traditional
studies mainly depend on Wikipedia link struc-
ture to disambiguate entities (Milne and Witten,
2008; Cucerzan, 2007). Also, TAGME (Ferragina
and Scaiella, 2010) exploits Wikipedia anchor link

texts for the mention detection and aims on-the-
fly annotation of short texts using agreement ap-
proach based on Wikipedia link structure. More-
over, these approaches focus on global coherence
approaches that emphasize the consistency of all
mention-entity pairs in the given text. AIDA-light
(Nguyen et al., 2014) considers global coherence
to disambiguate the entities and exploits YAGO2
(Hoffart et al., 2013) and Wikipedia domain hier-
archy. DBpedia Spotlight (Mendes et al., 2011),
Babelfy (Moro et al., 2014) and WAT (Piccinno
and Ferragina, 2014) have achieved remarkable re-
sults while using open domain knowledge bases.
However, these type of systems tends to work in-
herently worse in domain intensive environment.
These studies generally exploit hand-crafted fea-
tures to represent mentions and entities. Methods
based on word embeddings (Mikolov et al., 2013)
are recently popular including continuous word
vectors representations from large unstructured
texts. Doser (Zwicklbauer et al., 2016) leverages
word embeddings as the input of Personalized-
PageRank algorithm to disambiguate candidate
entities.

Most recently, neural models have been pre-
sented as a way to promote better generalization
without hand-crafted features. Sun et al. (2015)
presents a neural network approach using mention,
entity and context embeddings in a unified way.
They leverage a Convolutional Neural Network
model for context representation and consider po-
sitions of context words around mentions. They
identify entity disambiguation as a ranking task
that computes the similarity between mention-
context inputs and candidate entities. Yamada et
al. (2016) present a joint learning method com-
bining word and entity embeddings into the same
continuous vector space to disambiguate entities.
Similar to these two studies, Gupta et al. (2017)
extends the joint encoding of context, mention,
and entities with a fine-grained type information
defined for candidate entities. Similarly, we use
this entity type information as a domain indicator
to filter the candidate entities.

NeuPL (Phan et al., 2017) employs LSTM
and attention mechanism to disambiguate entities.
Also, it provides a fast Pair-Linking algorithm
which matches mention-entity pairs starting from
the easiest pair. NeuPL considers positional infor-
mation and word orderings. Therefore, two LSTM
networks are used to model the context of left and



16

right sides of each mention. Our study is simi-
lar to NeuPL in terms of resolving closer mention-
entity pairs rather than all pairs. Our disambigua-
tion method of closer mention-entity pairs is dif-
ferent from the pair linking method of NeuPL. Our
method leverages CRFs to disambiguate closer
neighbors as a named entity recognition task for
a specific domain.

3 Method

Our study implies Entity Linking as a sequence
learning task. For a given sequence mapping be-
tween mentions and entities, it consists a set of
mentions M = {m1,m2, ...,mN} and a set of
referent entities E = {e1, e2, ..., eN} in the En-
tity Linking task. In our work, the input size of
mentions are equal to the output size of entities
for each sequence mapping and N indicates the
size of sequence elements rather than defining the
size of the entire dictionary. The mention dic-
tionary may contain variations of a proper noun.
For instance, founder of the Republic of Turkey
is ”Mustafa Kemal Ataturk” and M may include
”Ataturk”, ”Mustafa Kemal”. Therefore, the size
of the entity dictionary is much less than the men-
tion dictionary. Also, sequence mappings contain
duplicates and the entity dictionary includes a lim-
ited number of unique entities for a specific do-
main.

In this study, we aim to map each mention to
a corresponding entity (Mi → Ei) in the given
knowledge base for specific domains. Similar
to recent studies (Zwicklbauer et al., 2016; Us-
beck et al., 2014) we assume that documents with
already detected mentions are the inputs of our
method. Also, we have another assumption in
which every mention contains one or more refer-
ent entities in the given knowledge base.

Figure 1 illustrates the general architecture of
our method including 3 layers

1. RDF2Vec Layer: RDF2Vec (Ristoski and
Paulheim, 2016) layer transforms each men-
tion into a numerical d-dimensional vectors.
We focus on entities and mentions, relations
are not taken into considerations in this study.

2. Bi-LSTM Layer: Bidirectional LSTM layer
will output hidden vector ht per time step t
and ht is computed for the given sequence
S = {r1, r2, ..., rN} where −→rt and ←−rt are

Figure 1: General structure of our method.

forward and backward pass vectors of RDF
elements.

3. CRF Layer: This layer is composed of full-
connected CRFs for ambiguous candidate en-
tities and maps the best mention-entity pairs
as a joint disambiguation task for specific do-
mains.

We obtain training data by extracting mentions
from Wikipedia and corresponding entities from
DBpedia for the movie domain. In the previ-
ous sample text about the mention Wicker Park,
three mentions are detected and these are inputs of
RDF2Vec layer. On the other hand, three referent
entities of these mentions are given as the output
of CRF layer.

To increase ambiguity of training data,
we extract Wikipedia disambiguation pages
for this type of mentions. For instance,
the mention Wicker Park has three candi-
date entities such as Wicker Park (film)2,
Wicker Park (Chicago park)3 and
Wicker Park (soundtrack)4. Also, another
mention Paul McGuigan includes two different
candidate entities. In the sample text, the last
mention Josh Hartnett has only one candidate
entity and it can be recognized as an easy tag and
is an indicator that this text might be related to the
movie domain.

3.1 Candidate Entity Generation

To generate candidate entities we select DBpedia
as the base knowledge base. We gather texts with

2http://dbpedia.org/resource/Wicker Park (film)
3http://dbpedia.org/resource/Wicker Park (Chicago park)
4http://dbpedia.org/resource/Wicker Park (soundtrack)



17

already detected mentions for these entities from
Wikipedia pages. For each mention, we query can-
didate entities and their domain information from
DBpedia. To do it, we check ”dct:subject” and
”rdf:type” properties of entities. Then, mentions
and candidate entities with their domain informa-
tion are recorded in a key-value store.

Wikipedia articles are separated into paragraphs
and each paragraph is retrieved in the key-value
store whether the paragraph includes any anno-
tated entity. If there is one or more entity in the
given paragraph and this paragraph does not ex-
ist in the annotated text list, the given paragraph is
loaded into a document store. At the same time,
Wikipedia disambiguation pages are searched for
each mention found in the paragraph. If there
exists any disambiguation page for any mention,
annotated texts with ambiguous entities are also
stored.

3.2 RDF2Vec
RDF2Vec model (Ristoski and Paulheim, 2016)
transforms word representations of Word2Vec
model into representations of RDF elements such
as classes, relations and instances. Instead of us-
ing words, dense RDF vectors are generated by
entities and relations from the given RDF model.
The overall structure of RDF2Vec model is listed
as follows:

1. Entity-relation sequences are constructed
from one of the strategies (Weistfeiler-
Lehman Subtree RDF Graph Kernels, graph
walks, etc.)

2. Neural language model is built by either
Skip-gram or CBOW algorithm from entity-
relation sequences

3. Entity relatedness is computed with Softmax
function from the neural language model

Before the neural language model is trained,
RDF model is transformed into the form of RDF
embeddings. Consequently, each embedding can
be represented as a numerical vector in Latent Fea-
ture Space. In this study, we do not use graph
walks to transform RDF model into entity-relation
sequences. Instead, we generate sequences of
mentions and their entities considering their posi-
tions in the Wikipedia pages. Then, we obtain two
different sequence documents for mention and en-
tity sequences like a neural translation model as

denoted in Figure 1. As sketched in this figure,
mentions of the sample text of Wicker Park and
corresponding entity sequences without relations
are the input of RDFVec model.

3.3 Bi-LSTM Layer

Recurrent Neural Networks (RNNs) employ se-
quential information to make predictions. It dif-
fers from classical neural networks by consider-
ing dependency between sequences of inputs and
outputs. RNNs operate recurrent tasks for every
element of the sequence and memorizes informa-
tion what has been computed so far. Bengio et al.
(Bengio et al., 1994) emphasizes that RNNs can
operate on long sequences in theory but in prac-
tice, they fail because they remember their most
recent inputs in the sequence. Long Short-term
Memory Networks(LSTMs) have been proposed
to overcome this problem by producing a memory-
cell to operate on long sequences (Hochreiter and
Schmidhuber, 1997).

To disambiguate entities, we first operate an
LSTM over input and output sequences. It will
output the hidden state ht at timestep t. Then the
entity disambiguation rule for e∗i

e∗i = argmaxj(logσ(Ohi+ b))j (1)

where logσ is the log softmax function of the
hidden state, and e∗i is the annotated entity which
has the highest score in this vector. The output
space of O is |E|x|E| dimensions in which E is
the length of candidate entities.

For the given input sequences of mentions, this
LSTM model computes a representation

−→
ht from

beginning to end in this sequence at every mention
t. But it may ignore the critical information from
the reverse order. To achieve it, a second LSTM
operates over the same sequence from end to be-
ginning. Then, this forward and backward LSTM
pair denotes as a bidirectional LSTM (Graves and
Schmidhuber, 2005). Bi-LSTM represents men-
tions with its left and right context and it is useful
to gather more comprehensive information from
the sequences.

3.4 Entity Disambiguation with CRFs

We model the entity disambiguation jointly us-
ing a Conditional Random Field (CRF) (Lafferty
et al., 2001). In this situation, we use CRF as a se-
quence model where Bi-LSTM provides features.



18

Hence, CRF computes a conditional probability as
a log-linear formulation

p(e|m) = exp(lp(m, e))∑
e′ exp(lp(m, e

′))
(2)

where m is the input sequence of mentions, e
is the output sequence of entities. Then, lp indi-
cates the log potential score of mention and en-
tity sequences. To generate a tractable function,
the potentials should be only included at local fea-
tures. Then we define Emission and Transition as
two types of potential scores in the Bi-LSTM CRF.
Then, the score is determined for these log poten-
tials such that

lp(e,m) =
∑
i

logθE(ei → mi)+logθT (ei−1 → ei)

(3)
where (logθE) is the emission potential score

for the mention at index i comes from the hidden
state of the Bi-LSTM at timestep i. The transition
potential scores (logθT ) are stored in a |E|x|E|
matrix P , where E is the entity dictionary and
consists of unique entities from short texts.

We use PyTorch5 to compute LSTM, Bi-LSTM
and CRF models. PyTorch is a dynamic neural
network tool in which we can define a computation
graph for each instance and can be executed on-
the-fly.

4 Experimental Setup

4.1 Dataset Generation

Manually annotated texts tend to be biased be-
cause people usually select familiar terms for the
entity annotation. Also, this annotation process
is sometimes noisy for unpopular terms. There-
fore, Wikipedia should be chosen because it is
curated by crowdsourcing and involves structured
annotation process. MSNBC (Cucerzan, 2007),
IITB (Kulkarni et al., 2009) and Wikilinks (Singh
et al., 2012) proposes experimental datasets for
general entity annotation tasks. Wikilinks pro-
vides a large-scale labeled corpus automatically
constructed via links to Wikipedia. Wikilinks
presents an automated method to identify a collec-
tion of massive amounts of entity mentions and is
based on crawling anchor links in Wikipedia pages
and exploiting anchor text as mentions. However,
Wikipedia can also be employed for the level of

5http://pytorch.org/tutorials/index.html

ambiguity adjustments in order to use disambigua-
tion pages and this is not directly indicated in Wik-
ilinks.

Ambiguity is the ratio between ambiguous and
unique entities and provides more realistic en-
vironment to entity annotators (Li et al., 2012).
To adjust ambiguity and generate annotated texts
for specific domains, we use a recent study
(Inan and Dikenelli, 2017) which extracts the lat-
est Wikipedia dump in English6 for specific do-
mains. To do it, they use Wikipedia category
pages and DBpedia ”dct:subject”7 property. Also,
they provide an ambiguous environment in which,
Wikipedia disambiguation pages are used for the
selected domains. As an example, the men-
tion Wicker Park has a Wikipedia disambiguation
page8 and it can be used to increase ambiguity in
the movie domain.

The movie evaluation dataset involves 123 an-
notated texts in English. For each text, the average
number of entities is 4.99 and there are 614 enti-
ties in total. Entities such as movies, directors, and
starring are extracted from infoboxes of Wikipedia
articles and mapped with referent entities by DB-
pedia. Disambiguation pages of these entities are
extracted in other domains such as music and loca-
tion to increase the ratio of ambiguity in the eval-
uation dataset for the movie domain. The ambigu-
ity ratio of the evaluation dataset is 48.79% com-
puted as the division of all ambiguous entities to
the total number of unique entities extracted for
the movie domain. Therefore, a more realistic am-
biguous dataset can be generated to evaluate Entity
Linking systems.

4.2 Results
We evaluate our method with several Entity
Linking approaches from GERBIL benchmarking
framework (Usbeck et al., 2015). We select Dis-
ambiguate to Knowledge Base (D2KB) task which
focuses on the disambiguation of detected men-
tions to the related entities in the knowledge base.
In this task, a given mention is guaranteed to map
to the corresponding entity.

AGDISTIS (Usbeck et al., 2014) chooses can-
didate entities for the detected mentions from sur-
face forms and generates a disambiguation graph
for these candidates. The generated disambigua-
tion graph is used in graph-based HITS algorithm

6https://dumps.wikimedia.org/enwiki/20170420/
7http://purl.org/dc/terms/subject
8https://en.wikipedia.org/wiki/Wicker Park



19

EL System Micro-F1 Micro-P Micro-R Macro-F1 Macro-P Macro-R
AGDISTIS 0.2063 0.2097 0.2031 0.3093 0.3098 0.309
AIDA 0.1485 0.1559 0.1417 0.1975 0.2035 0.1932
Babelfy 0.2101 0.2273 0.1953 0.284 0.2887 0.2812
Dbpedia Spotlight 0.1515 0.1515 0.1515 0.2044 0.2044 0.2044
Kea 0.1478 0.149 0.1466 0.1942 0.1976 0.1922
PBOH 0.2193 0.25 0.1953 0.282 0.282 0.282
WAT 0.2174 0.2451 0.1953 0.3124 0.3439 0.2967
LSTM 0.336 0.342 0.33 0.436 0.45 0.422
Bi-LSTM+CRF 0.446 0.488 0.41 0.546 0.564 0.53

Table 1: Evaluation scores of Entity Linking (EL) systems in GERBIL.

to match the best mention-entity pairs in the dis-
ambiguation step.

AIDA (Hoffart et al., 2011b) relies on a compu-
tation of global coherence between candidate en-
tities and dense subgraph algorithms executing on
the YAGO (Hoffart et al., 2011a) knowledge base.

Babelfy uses a graph-based disambiguation
algorithm and finds the densest subgraph sur-
rounded by candidate entities for the given men-
tion. Then, Babelfy leverages the densest sub-
graph to match the best mention and entity pair.

DBpedia Spotlight (Mendes et al., 2011) uses
a Vector Space Model (VSM) including DBpedia
entity occurrences where a multidimensional word
space has a representation per entity. Disambigua-
tion task of DBpedia spotlight transforms Inverse
Term Frequency (ITF) into an Inverse Candidate
Frequency (ICF) which depends on candidate en-
tities rather than terms and is an inverse propor-
tion of candidate entities associated with words in
VSM.

KEA (Waitelonis and Sack, 2016) proposes a
combination of dictionary and knowledge based
approaches. They analyze word co-occurrences of
Wikipedia pages and merge these co-occurences
with a graph analysis on the Wikipedia link struc-
ture and DBpedia.

PBOH (Ganea et al., 2016) is a collective
entity linking system that is based on light-
weight Wikipedia statistics. PBOH computes co-
occurrence of words and entities for a probabilistic
graphical model.

WAT (Piccinno and Ferragina, 2014) system
is a complex version of TagMe (Ferragina and
Scaiella, 2010). WAT depends on graph-based al-
gorithm and selection of the best mention-entity
pair from a vote-based algorithm.

Cornolti et al. (2013) expands general F1 mea-

sures to the Macro- and Micro- measures. While
Macro- measures are the average of the corre-
sponding measure over each document in all an-
notated documents, the Micro- measures consider
all tags together thus giving more importance to
documents having more tags. Table 1 illustrates
the overall scores for Entity Linking task is mea-
sured in the generated evaluation set with respect
to precision, recall, and F1-score. All scores
are low because of high ambiguity of the gen-
erated evaluation dataset. F1 scores show that
our study outperforms state-of-the-art studies us-
ing Bi-LSTM+CRF model on the generated eval-
uation dataset in the movie domain.

5 Conclusion

This study mainly presents a sequence learning
method for domain-specific entity linking using
sequence learning as a neural machine translation
task. We filter candidate entities leveraging do-
main information and eliminating easy matches
of mention-entity pairs. We employ a domain-
specific dataset to compare our work with exist-
ing studies in GERBIL. Our method outperforms
the state-of-the-art methods in the domain-specific
configuration.

In the future, we apply other decoder mod-
els using the attention mechanism to the current
model as a different joint disambiguation method
of candidate entities. Also, we will examine many
domain-specific datasets on this method.

References

Yoshua Bengio, Patrice Simard, and Paolo Frasconi.
1994. Learning long-term dependencies with gradi-
ent descent is difficult. IEEE transactions on neural
networks, 5(2):157–166.



20

Razvan Bunescu and Marius Paşca. 2006. Using en-
cyclopedic knowledge for named entity disambigua-
tion. In 11th conference of the European Chapter of
the Association for Computational Linguistics.

Marco Cornolti, Paolo Ferragina, and Massimiliano
Ciaramita. 2013. A framework for benchmarking
entity-annotation systems. In Proceedings of the
22nd international conference on World Wide Web,
pages 249–260. ACM.

Silviu Cucerzan. 2007. Large-scale named entity dis-
ambiguation based on wikipedia data. In Proceed-
ings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL).

Paolo Ferragina and Ugo Scaiella. 2010. Tagme:
On-the-fly annotation of short text fragments (by
wikipedia entities). In Proceedings of the 19th
ACM International Conference on Information and
Knowledge Management, CIKM ’10, pages 1625–
1628, New York, NY, USA. ACM.

Octavian-Eugen Ganea, Marina Ganea, Aurelien Luc-
chi, Carsten Eickhoff, and Thomas Hofmann. 2016.
Probabilistic bag-of-hyperlinks model for entity
linking. In Proceedings of the 25th International
Conference on World Wide Web, pages 927–938. In-
ternational World Wide Web Conferences Steering
Committee.

Alex Graves and Jürgen Schmidhuber. 2005. Frame-
wise phoneme classification with bidirectional lstm
and other neural network architectures. Neural Net-
works, 18(5-6):602–610.

Nitish Gupta, Sameer Singh, and Dan Roth. 2017. En-
tity linking via joint encoding of types, descriptions,
and context. In Proceedings of the 2017 Confer-
ence on Empirical Methods in Natural Language
Processing, pages 2681–2690.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Johannes Hoffart, Fabian M Suchanek, Klaus
Berberich, Edwin Lewis-Kelham, Gerard De Melo,
and Gerhard Weikum. 2011a. Yago2: exploring and
querying world knowledge in time, space, context,
and many languages. In Proceedings of the 20th
international conference companion on World wide
web, pages 229–232. ACM.

Johannes Hoffart, Fabian M. Suchanek, Klaus
Berberich, and Gerhard Weikum. 2013. Yago2: A
spatially and temporally enhanced knowledge base
from wikipedia. Artif. Intell., 194:28–61.

Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bor-
dino, Hagen Fürstenau, Manfred Pinkal, Marc Span-
iol, Bilyana Taneva, Stefan Thater, and Gerhard
Weikum. 2011b. Robust disambiguation of named
entities in text. In Proceedings of the Conference on

Empirical Methods in Natural Language Process-
ing, pages 782–792. Association for Computational
Linguistics.

Emrah Inan and Oguz Dikenelli. 2017. Wedgem:
A domain-specific evaluation dataset generator for
multilingual entity linking systems. In International
Conference on Web Information Systems Engineer-
ing, pages 221–228. Springer.

Sayali Kulkarni, Amit Singh, Ganesh Ramakrishnan,
and Soumen Chakrabarti. 2009. Collective annota-
tion of wikipedia entities in web text. In Proceed-
ings of the 15th ACM SIGKDD international con-
ference on Knowledge discovery and data mining,
pages 457–466. ACM.

John Lafferty, Andrew McCallum, and Fernando CN
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data.

Guillaume Lample, Miguel Ballesteros, Sandeep Sub-
ramanian, Kazuya Kawakami, and Chris Dyer. 2016.
Neural architectures for named entity recognition.
arXiv preprint arXiv:1603.01360.

Xuansong Li, Stephanie M Strassel, Heng Ji, Kira Grif-
fitt, and Joe Ellis. 2012. Linguistic resources for en-
tity linking evaluation: from monolingual to cross-
lingual. Annotation, 1:1.

Pablo N. Mendes, Max Jakob, Andrés Garcı́a-Silva,
and Christian Bizer. 2011. Dbpedia spotlight: Shed-
ding light on the web of documents. In Proceedings
of the 7th International Conference on Semantic Sys-
tems, I-Semantics ’11, pages 1–8, New York, NY,
USA. ACM.

Rada Mihalcea and Andras Csomai. 2007. Wikify!:
linking documents to encyclopedic knowledge. In
Proceedings of the sixteenth ACM conference on
Conference on information and knowledge manage-
ment, pages 233–242. ACM.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.

David Milne and Ian H. Witten. 2008. Learning to link
with wikipedia. In Proceedings of the 17th ACM
Conference on Information and Knowledge Man-
agement, CIKM ’08, pages 509–518, New York,
NY, USA. ACM.

Pasquale Minervini, Claudia d’Amato, Nicola Fanizzi,
and Floriana Esposito. 2016. Leveraging the schema
in latent factor models for knowledge graph comple-
tion. In Proceedings of the 31st Annual ACM Sym-
posium on Applied Computing, Pisa, Italy, April 4-8,
2016, pages 327–332.

Andrea Moro, Alessandro Raganato, and Roberto Nav-
igli. 2014. Entity Linking meets Word Sense Disam-
biguation: a Unified Approach. Transactions of the

https://doi.org/10.1145/1871437.1871689
https://doi.org/10.1145/1871437.1871689
https://doi.org/10.1145/1871437.1871689
https://doi.org/10.1016/j.artint.2012.06.001
https://doi.org/10.1016/j.artint.2012.06.001
https://doi.org/10.1016/j.artint.2012.06.001
https://doi.org/10.1145/2063518.2063519
https://doi.org/10.1145/2063518.2063519
https://doi.org/10.1145/1458082.1458150
https://doi.org/10.1145/1458082.1458150
https://doi.org/10.1145/2851613.2851841
https://doi.org/10.1145/2851613.2851841
https://doi.org/10.1145/2851613.2851841


21

Association for Computational Linguistics (TACL),
2:231–244.

Dat Ba Nguyen, Johannes Hoffart, Martin Theobald,
and Gerhard Weikum. 2014. Aida-light: High-
throughput named-entity disambiguation. In
LDOW, volume 1184 of CEUR Workshop Proceed-
ings. CEUR-WS.org.

Maximilian Nickel, Kevin Murphy, Volker Tresp, and
Evgeniy Gabrilovich. 2015. A review of rela-
tional machine learning for knowledge graphs: From
multi-relational link prediction to automated knowl-
edge graph construction. CoRR, abs/1503.00759.

Minh C Phan, Aixin Sun, Yi Tay, Jialong Han, and
Chenliang Li. 2017. Neupl: Attention-based seman-
tic matching and pair-linking for entity disambigua-
tion. In Proceedings of the 2017 ACM on Confer-
ence on Information and Knowledge Management,
pages 1667–1676. ACM.

Francesco Piccinno and Paolo Ferragina. 2014. From
tagme to WAT: a new entity annotator. In ERD’14,
Proceedings of the First ACM International Work-
shop on Entity Recognition & Disambiguation, July
11, 2014, Gold Coast, Queensland, Australia, pages
55–62.

Lev Ratinov, Dan Roth, Doug Downey, and Mike
Anderson. 2011. Local and global algorithms
for disambiguation to wikipedia. In Proceedings
of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language
Technologies-Volume 1, pages 1375–1384. Associ-
ation for Computational Linguistics.

Petar Ristoski and Heiko Paulheim. 2016. Rdf2vec:
RDF graph embeddings for data mining. In The Se-
mantic Web - ISWC 2016 - 15th International Se-
mantic Web Conference, Kobe, Japan, October 17-
21, 2016, Proceedings, Part I, pages 498–514.

Sameer Singh, Amarnag Subramanya, Fernando
Pereira, and Andrew McCallum. 2012. Wikilinks:
A large-scale cross-document coreference corpus la-
beled via links to wikipedia. University of Mas-
sachusetts, Amherst, Tech. Rep. UM-CS-2012-015.

Yaming Sun, Lei Lin, Duyu Tang, Nan Yang, Zhenzhou
Ji, and Xiaolong Wang. 2015. Modeling mention,
context and entity with neural networks for entity
disambiguation. In IJCAI, pages 1333–1339.

Ricardo Usbeck, Axel-Cyrille Ngonga Ngomo,
Michael Röder, Daniel Gerber, SandroAthaide
Coelho, Sören Auer, and Andreas Both. 2014.
AGDISTIS - Graph-Based Disambiguation of
Named Entities Using Linked Data. In Peter Mika,
Tania Tudorache, Abraham Bernstein, Chris Welty,
Craig Knoblock, Denny Vrandečić, Paul Groth,
Natasha Noy, Krzysztof Janowicz, and Carole
Goble, editors, The Semantic Web – ISWC 2014,
volume 8796 of Lecture Notes in Computer Science,
pages 457–471. Springer International Publishing.

Ricardo Usbeck, Michael Röder, Axel-Cyrille Ngonga
Ngomo, Ciro Baron, Andreas Both, Martin
Brümmer, Diego Ceccarelli, Marco Cornolti, Di-
dier Cherix, Bernd Eickmann, Paolo Ferragina,
Christiane Lemke, Andrea Moro, Roberto Navigli,
Francesco Piccinno, Giuseppe Rizzo, Harald Sack,
René Speck, Raphaël Troncy, Jörg Waitelonis, and
Lars Wesemann. 2015. GERBIL – general entity
annotation benchmark framework. In 24th WWW
conference.

Jörg Waitelonis and Harald Sack. 2016. Named entity
linking in# tweets with kea. In # Microposts, pages
61–63.

Jason Weston, Antoine Bordes, Oksana Yakhnenko,
and Nicolas Usunier. 2013. Connecting language
and knowledge bases with embedding models for re-
lation extraction. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP 2013, 18-21 October 2013,
Grand Hyatt Seattle, Seattle, Washington, USA, A
meeting of SIGDAT, a Special Interest Group of the
ACL, pages 1366–1371.

Ikuya Yamada, Hiroyuki Shindo, Hideaki Takeda, and
Yoshiyasu Takefuji. 2016. Joint learning of the em-
bedding of words and entities for named entity dis-
ambiguation. arXiv preprint arXiv:1601.01343.

Stefan Zwicklbauer, Christin Seifert, and Michael
Granitzer. 2016. DoSeR - A Knowledge-Base-
Agnostic Framework for Entity Disambiguation Us-
ing Semantic Embeddings. Springer International
Publishing, Cham.

http://dblp.uni-trier.de/db/conf/www/ldow2014.html#NguyenHTW14
http://dblp.uni-trier.de/db/conf/www/ldow2014.html#NguyenHTW14
http://arxiv.org/abs/1503.00759
http://arxiv.org/abs/1503.00759
http://arxiv.org/abs/1503.00759
http://arxiv.org/abs/1503.00759
https://doi.org/10.1145/2633211.2634350
https://doi.org/10.1145/2633211.2634350
https://doi.org/10.1007/978-3-319-46523-4_30
https://doi.org/10.1007/978-3-319-46523-4_30
https://doi.org/10.1007/978-3-319-11964-9_29
https://doi.org/10.1007/978-3-319-11964-9_29
http://svn.aksw.org/papers/2015/WWW_GERBIL/public.pdf
http://svn.aksw.org/papers/2015/WWW_GERBIL/public.pdf
http://aclweb.org/anthology/D/D13/D13-1136.pdf
http://aclweb.org/anthology/D/D13/D13-1136.pdf
http://aclweb.org/anthology/D/D13/D13-1136.pdf
https://doi.org/10.1007/978-3-319-34129-3_12
https://doi.org/10.1007/978-3-319-34129-3_12
https://doi.org/10.1007/978-3-319-34129-3_12

