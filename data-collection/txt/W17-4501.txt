



















































Video Highlights Detection and Summarization with Lag-Calibration based on Concept-Emotion Mapping of Crowdsourced Time-Sync Comments


Proceedings of the Workshop on New Frontiers in Summarization, pages 1–11
Copenhagen, Denmark, September 7, 2017. c©2017 Association for Computational Linguistics

 

Video Highlights Detection and Summarization with Lag-Calibration 
based on Concept-Emotion Mapping of Crowd-sourced Time-Sync 

Comments  

 
Qing Ping and Chaomei Chen 

College of Computing & Informatics 
Drexel University 

{qp27, cc345}@drexel.edu 

 
 
 
 

Abstract 

With the prevalence of video sharing, 
there are increasing demands for au-
tomatic video digestion such as high-
light detection. Recently, platforms 
with crowdsourced time-sync video 
comments have emerged worldwide, 
providing a good opportunity for high-
light detection. However, this task is 
non-trivial: (1) time-sync comments 
often lag behind their corresponding 
shot; (2) time-sync comments are se-
mantically sparse and noisy; (3) to de-
termine which shots are highlights is 
highly subjective. The present paper 
aims to tackle these challenges by pro-
posing a framework that (1) uses con-
cept-mapped lexical-chains for lag-
calibration; (2) models video high-
lights based on comment intensity and 
combination of emotion and concept 
concentration of each shot; (3) sum-
marize each detected highlight using 
improved SumBasic with emotion and 
concept mapping. Experiments on 
large real-world datasets show that our 
highlight detection method and sum-
marization method both outperform 
other benchmarks with considerable 
margins. 

1 Introduction 

Every day, people watch billions of hours of vid-
eos on YouTube, with half of the views on mo-
bile devices1. With the prevalence of video shar-
                                                        
1 https://www.youtube.com/yt/press/statistics.html 

ing, there is increasing demand for fast video di-
gestion. Imagine a scenario where a user wants 
to quickly grasp a long video, without dragging 
the progress bar repeatedly to skip shots unap-
pealing to the user. With automatically-generated 
highlights, users could digest the entire video in 
minutes, before deciding whether to watch the 
full video later. Moreover, automatic video high-
light detection and summarization could benefit 
video indexing, video search and video recom-
mendation.  

However, finding highlights from a video is not 
a trivial task. First, what is considered to be a 
“highlight” can be very subjective. Second, a 
highlight may not always be captured by analyz-
ing low-level features in image, audio and mo-
tions. Lack of abstract semantic information has 
become a bottleneck of highlight detection in tra-
ditional video processing.  

Recently, crowdsourced time-sync video com-
ments, or “bullet-screen comments” have 
emerged, where real-time generated comments 
will be flying over or besides the screen, synchro-
nized with the video frame by frame. It has gained 
popularity worldwide, such as niconico in Japan, 
Bilibili and Acfun in China, YouTube Live and 
Twitch Live in USA. The popularity of the time-
sync comments has suggested new opportunities 
for video highlight detection based on natural lan-
guage processing. 

Nevertheless, it is still a challenge to detect and 
label highlights using time-sync comments. First, 
there is almost inevitable lag for comments related 
to each shot. As in Figure 1, ongoing discussion 
about one shot may extend to next a few shots. 
Highlight detection and labeling without lag-
calibration may cause inaccurate results. Second, 

1



 

time-sync comments are sparse semantically, both 
in number of comments per shot and number of 
tokens per comment. Traditionally bag-of-words 
statistical model may work poorly on such data.  

Third, there is much uncertainty in highlight 
detection in an unsupervised setting without any 
prior knowledge. Characteristics of highlights 
must be explicitly defined, captured and modeled. 

To our best knowledge, little work has concen-
trated on highlight detection and labeling based on 
time-sync comments in unsupervised way. The 
most relevant work proposed to detect highlights 
based on topic concentration of semantic vectors 
of bullet-comments, and label each highlight with 
pre-trained classifier based on pre-defined tags 
(Lv, Xu, Chen, Liu, & Zheng, 2016). Neverthe-
less, we argue that emotion concentration is more 
important in highlight detection than general topic 
concentration. Another work proposed to extract 
highlights based on frame-by-frame similarity of 
emotion distribution (Xian, Li, Zhang, & Liao, 
2015). However, neither work proposed to tackle 
the issue of lag-calibration, emotion-topic concen-
tration balance and unsupervised highlight label-
ing simultaneously. 

To solve these problems, the present study pro-
poses the following: (1) word-to-concept and 
word-to-emotion mapping based on global word-
embedding, from which lexical-chains are con-
structed for bullet-comments lag-calibration; (2) 
highlight detection based on emotional and con-
ceptual concentration and intensity of lag-
calibrated bullet-comments; (3) highlight summa-
rization with modified Basic Sum algorithm that 
treats emotions and concepts as basic units in a 
bullet-comment.  

The main contribution of the present paper are 
as follows: (1) We propose an entirely unsuper-
vised framework for video highlight-detection and 
summarization based on time-sync comments; (2) 
We develop a lag-calibration technique based on 
concept-mapped lexical chains; (3) We construct 
large datasets for bullet-comment word-

embedding, bullet-comment emotion lexicon and 
ground-truth for highlight-detection and labeling 
evaluation based on bullet-comments. 

2 Related Work 

2.1 Highlight detection by video processing 
First, following the definition in previous work 
(M. Xu, Jin, Luo, & Duan, 2008), we define  high-
lights as the most memorable shots in a video with 
high emotion intensity. Note that highlight detec-
tion is different from video summarization, which 
focuses on condensed storyline representation of a 
video, rather than extracting affective contents 
(K.-S. Lin, Lee, Yang, Lee, & Chen, 2013).  

For highlight detection, some researchers pro-
pose to represent emotions in a video by a curve 
on the arousal-valence plane with low-level fea-
tures such as motion, vocal effects, shot length, 
and audio pitch (Hanjalic & Xu, 2005), color 
(Ngo, Ma, & Zhang, 2005), mid-level features 
such as laughing and subtitles (M. Xu, Luo, Jin, & 
Park, 2009). Nevertheless, due to the semantic gap 
between low-level features and high-level seman-
tics, accuracy of highlight detection based on vid-
eo processing is limited (K.-S. Lin et al., 2013). 

2.2 Temporal text summarization 
The work in temporal text summarization is rele-
vant to the present study, but also has differences. 
Some works formulate temporal text summariza-
tion as a constrained multi-objective optimization 
problem (Sipos, Swaminathan, Shivaswamy, & 
Joachims, 2012; Yan, Kong, et al., 2011; Yan, 
Wan, et al., 2011), as a graph optimization prob-
lem (C. Lin et al., 2012), as a supervised learning-
to-rank problem (Tran, Niederée, Kanhabua, 
Gadiraju, & Anand, 2015), and as online cluster-
ing problem (Shou, Wang, Chen, & Chen, 2013).  

The present study models the highlight detec-
tion as a simple two-objective optimization prob-
lem with constraints. However, the features cho-
sen to evaluate the “highlightness” of a shot are 
different from the above studies. Because a high-
light shot is observed to be correlated with high 
emotional intensity and topic concentration, cov-
erage and non-redundancy are not goals of opti-
mization any more, as in temporal text summari-
zation. Instead, we focus on modeling emotional 
and topic concentration in present study. 

 
Figure 1.Lag Effect of Time-Sync Com-

ments Shot by Shot. 
 

2



 

2.3 Crowdsourced time-sync comment min-
ing 

Several works focused on tagging videos shot-by-
shot with crowdsourced time-sync comments by 
manual labeling and supervised training (Ikeda, 
Kobayashi, Sakaji, & Masuyama, 2015), temporal 
and personalized topic modeling (Wu, Zhong, 
Tan, Horner, & Yang, 2014), or tagging video as a 
whole (Sakaji, Kohana, Kobayashi, & Sakai, 
2016). One work proposes to generate summariza-
tion of each shot by data reconstruction jointly on 
textual and topic level (L. Xu & Zhang, 2017). 

One work proposed a centroid-diffusion algo-
rithm to detect highlights (Xian et al., 2015). 
Shots are represented by latent topics by LDA. 
Another work proposed to use pre-trained seman-
tic vector of comments to cluster comments into 
topics, and find highlights based on topic concen-
tration (Lv et al., 2016). Moreover, they use pre-
defined labels to train a classifier for highlight la-
beling. The present study differs from these two 
studies in several aspects. First, before highlight 
detection, we perform lag-calibration to minimize 
inaccuracy due to comment lags. Second, we pro-
pose to represent each scene by the combination 
of topic and emotion concentration. Third, we per-
form both highlight detection and highlight label-
ing in unsupervised way. 

2.4 Lexical chain 
Lexical chains are a sequence of words in a cohe-
sive relationship spanning in a range of sentences. 
Early work constructs lexical chains based on syn-
tactic relations of words using the Roget’s Thesau-
rus without word sense disambiguation (Morris & 
Hirst, 1991). Later work expands lexical chains by 
WordNet relations with word sense disambigua-
tion (Barzilay & Elhadad, 1999; Hirst & St-Onge, 
1998). Lexical chains is also constructed based on 
word-embedded relations for disambiguation of 
multi-words (Ehren, 2017). The present study 
constructs lexical chains for proper lag-calibration 
based on global word-embedding. 

3 Problem Formulation  

The problem in the present paper can be formu-
lated as follows. The input is a set of time-sync 
comments, 𝐶 = {𝑐%, 𝑐', 𝑐(, … , 𝑐 * } with a set of 
timestamps 𝑇 = 	 {𝑡%, 𝑡', 𝑡(, … , 𝑡 * } of a video 𝒗, a 
compression ratio 𝜏123142315 for number of high-
lights to be generated, a compression ratio 

𝜏67889:; for number of comments in each high-
light summary. Our task is to (1) generate a set of 
highlight shots 𝑆(𝒗) = {𝑠%, 𝑠', 𝑠(, … , 𝑠@}, and (2) 
highlight summaries Α 𝒗 = 	 {𝐼%, 𝐼', 𝐼(, … , 𝐼@}  as 
close to ground truth as possible. Each highlight 
summary comprises a subset of all the comments 
in this shot: 𝐼2 	= {𝑐%, 𝑐', 𝑐(, … , 𝑐@C}. Number of 
highlight shots 𝑛 and number of comments in 
summary 𝑛2 are determined by 𝜏123142315 and 
𝜏67889:; respectively. 

4 Video Highlight Detection 

In this section, we introduce our framework for 
highlight detection. Two preliminary tasks are also 
described, namely construction of global time-
sync comment word embedding and emotion lexi-
con.   

4.1 Preliminaries 
Word-Embedding of Time-Sync Comments  

As pointed out earlier, one challenge in analyzing 
time-sync comments is the semantic sparseness, 
since number of comments and comment length 
are both very limited. Two semantically related 
words may not be related if they do not co-occur 
frequently in one video. To compensate, we con-
struct a global word-embedding on a large collec-
tion of time-sync comments.  

The word-embedding dictionary can be repre-
sented as: 𝐷{(𝑤%: 𝑣%), (𝑤': 𝑣'), … , (𝑤 I : 𝑣 I )，
where 𝑤2 is a word, 𝑣2 is the corresponding word-
vector, 𝑉 is the vocabulary of the corpus.  

Emotion Lexicon Construction 

As emphasized earlier, it is crucial to extract emo-
tions in time-sync comments for highlight detec-
tion. However, traditional emotion lexicons can-
not be used here, since there exist too many Inter-
net slangs that are specifically born on this type of 
platforms. For example, “23333” means “ha ha 
ha”, and “6666” means “really awsome”. There-
fore, we construct an emotion lexicon tailored for 
time-sync comments from the word-embedding 
dictionary trained from last step. First we manual-
ly label words of the five basic emotional catego-
ries (happy, anger, sad, fear and surprise) as seeds 
(Ekman, 1992), from the top frequent words in the 
corpus. Here the sixth emotion category “disgust” 
is omitted because it is relatively rare in the da-
taset, and could be readily incorporated for other 
datasets. Then we expand the emotion lexicon by 
searching the top 𝑁 neighbors of each seed word 

3



 

in the word-embedding space, and adding a 
neighbor to seeds if the neighbor meets at least 
percentage of overlap 𝛾MNO:49P with all the seeds 
with minimum similarity of 𝑠𝑖𝑚82@. The neigh-
bors are searched based on cosine similarity in the 
word-embedding space. 

4.2 Lag-Calibration 
In this section, we introduce our method for lag-
calibration following the steps of concept map-
ping, word-embedded lexical chain construction 
and lag-calibration. 

Concept Mapping 

To tackle the issue of semantic sparseness in time-
sync comments, and to construct lexical-chains of 
semantically related words, words of similar 
meanings should be mapped to same concept first. 
Given a set of comments 𝐶 of video 𝒗, we first 
propose a mapping ℱ from the vocabulary 𝑉T  of 
comments 𝐶 to a set of concepts 𝐾T , namely:  

ℱ: 𝑉T 	→ 	𝐾T			( 𝑉T ≥ 𝐾T ) 

More specifically, mapping ℱ maps each word 
𝑤X into a concept 𝑘 = ℱ(𝑤X): 

ℱ 𝑤X = ℱ 𝑤% = ℱ 𝑤' = ⋯ = ℱ 𝑤 5MP_@(\) =
𝑘, ∃𝑘 ∈ 𝐾T		𝑎𝑛𝑑	

{\|\∈5MP_@(\b)	∧	ℱ \ de}
5MP_@(\b)

≥ 𝜙MNO:49P
𝑤, 𝑜𝑡ℎ𝑒𝑟𝑤𝑖𝑠𝑒

  (1) 

and 𝑡𝑜𝑝_𝑛(𝑤X) returns the top 𝑛 neighbors of 
word 𝑤X based on cosine similarity. For every 
word 𝑤X in comment 𝐶, we check percentage of 
its neighbors already mapped to a concept 𝑘. If 
the percentage exceeds the threshold 𝜙MNO:49P, 
then word 𝑤X together with its neighbors will be 
mapped to 𝑘. Otherwise they will be mapped to a 
new concept 𝑤X. 
Lexical Chain Construction 

The next step is to construct all lexical chains in 
current time-sync comments of video 𝒗, so that 
lagged comments could be calibrated based on 
lexical chains. A lexical chain 𝑙2m comprises a set 
of triples 𝑙2m = 𝑤, 𝑡, 𝑐 , where 𝑤 is the actual 
mentioned word of concept 𝑘2 in comment 𝑐, 𝑡 is 
the timestamp of the comment 𝑐. A lexical chain 
dictionary 𝐷4On2o94	o192@ for time-sync comments 
𝐶 of video 𝒗: 𝐿4On2o94	o192@ = {𝑘%: 𝑙%%, 𝑙%', 𝑙%( … , 𝑘': 
𝑙'%, 𝑙'', 𝑙'( … , … , 𝑘 qr : (𝑙 qr %, 𝑙 qr ', 𝑙 qr ( … )},where 
𝑘2 ∈ 𝐾T  is a concept, and 𝑙2m is the 𝑗𝑡ℎ lexical 
chain of concept 𝑘2. The algorithm for lexical 
chain construction is described in Algorithm 1.  

Specifically, each comment in 𝐶 can be either 
appended to existing lexical chains, or added to 
new empty lexical chains, based on its temporal 
distance with existing chains controlled by Maxi-
mum silence 𝑙89n.  

Note that word senses in the lexical chains con-
structed here are not disambiguated as most tradi-
tional algorithms do. Nevertheless, we argue that 
lexical chains are still useful, since our concept 
mapping is constructed from time-sync comments 
in its natural order, a progressively semantic con-
tinuity that naturally reinforces similar word sens-
es for temporally close comments. This semantic 
continuity together with global word embedding 
ensures that our concept mapping is valid in most 
cases. 

Comment Lag-Calibration  

Now given constructed lexical chain dictionary 
𝐿4On2o94	o192@, we can calibrate the comments in 𝐶 
based on their lexical chains. From our observa-
tion, the first comment about one shot usually oc-
curs within the shot, while the rest may not be the 
case. Therefore, we calibrate the timestamp of 
each comment to the timestamp of first element of 
the lexical chain it belongs to. Among all the lexi-
cal chains (concepts) a comment belongs to, we 
pick the one with highest score 𝑠𝑐𝑜𝑟𝑒	e,o. 
𝑆𝑐𝑜𝑟𝑒	e,o is computed as the sum frequency of 
each word in the chain weighted by its logarithm 
global frequency log	(	𝐷 𝑤 . 𝑐𝑜𝑢𝑛𝑡). Therefore, 

Algorithm 1 Lexical Chain Construction 
Input time-sync comments 𝐶. Word-to-concept 
mapping ℱ. Maximum silence 𝑙89n.  
Output A dictionary of lexical chains 
𝐿4On2o94 	o192@.  
Initialize 𝐿4On2o94 	o192@ ← {} 
for each c in C do 

 𝑡o7::O@5 ← 𝑡o 
 for each word in 𝑐 do 
     𝑘 ← 	ℱ(𝑤𝑜𝑟𝑑)  
     if 𝑘 in 𝐿4On2o94 	o192@then 
          𝑐ℎ𝑎𝑖𝑛𝑠 ← 𝐿4On2o94 	o192@(𝑘) 
          𝑡P:ON2M76 ← 𝑡o192@6[4965] 
          if 𝒕𝒄𝒖𝒓𝒓𝒆𝒏𝒕 − 𝒕𝒑𝒓𝒆𝒗𝒊𝒐𝒖𝒔 ≤ 𝑙89n then 
                𝑐ℎ𝑎𝑖𝑛𝑠[𝑙𝑎𝑠𝑡] ← 𝑐ℎ𝑎𝑖𝑛𝑠[𝑙𝑎𝑠𝑡] ∪ 𝑐 
         	else 
               𝑐ℎ𝑎𝑖𝑛𝑠 ← 𝑐ℎ𝑎𝑖𝑛𝑠 ∪ {𝑐} 
          end if 
     else 
           𝐿4On2o94 	o192@(𝑘) ← {{𝑐}} 
     end if 
end for 

end for 
return 𝐿4On2o94 	o192@ 

Table 1. Lexical Chain Construction. 
 

4



 

each comment will be assigned to its most seman-
tically important lexical-chain (concept) for cali-
bration. The algorithm for the calibration is de-
scribed in Algorithm 2. 

Note that if there are multiple consecutive shots 

{𝑠%, 𝑠', … , 𝑠8} with comments of similar contents, 
our lag-calibration method may calibrate many 
comments in shots 𝑠', 𝑠(, … , 𝑠8 to the timestamp 
of the first shot  𝑠%, if these comments are con-
nected via lexical chains from shot  𝑠%. This is not 
necessarily a bad thing since we hope to avoid se-
lecting redundant consecutive highlight shots and 
leave opportunity for other candidate highlights, 
given a fixed compression ratio. 

Shot Importance Scoring 

In this section, we first segment comments by 
shots of equal temporal length 𝑙6oO@O, then we 
model shot importance. Then highlights could be 
detected based on shot importance. 

A shot’s importance is modeled to be impacted 
by two factors: comment concentration and com-
menting intensity. For comment concentration, as 
mentioned earlier, both concept and emotional 
concentration may contribute to highlight detec-
tion. For example, a group of concept-
concentrated comments like “the background mu-
sic/bgm/soundtrack of this shot is clas-
sic/inspiring/the best” may be an indicator of a 

highlight related to memorable background music. 
Meanwhile, comments such as “this plot is so 
funny/hilarious/lmao/lol/2333” may suggest a sin-
gle-emotion concentrated highlight. Therefore, we 
combine these two concentrations in our model. 
First, we define emotional concentration 𝒞O8M52M@ 
of shot 𝑠 based on time-sync comments 𝐶6 given 
emotional lexicon 𝐸 as follows:  

𝒞O8M52M@(𝐶6, 𝑠) = 	
%

 P∙	(P)
	                                (2) 

𝑝O =
{\|\∈T∧\∈(O)}

T
                                                  (3) 

Here we calculate the reverse of entropy of 
probabilities of five emotions within a shot as 
emotion concentration. Then we define topical 
concentration  𝒞5MP2o: 

𝒞5MP2o(𝐶6, 𝑠) = 	
%

 P∙	(P)
r


	                                  (4) 

𝑝e =
				@ 	(∈r	∧	ℱ  	 ∉ )

				@ 	(∈r	∧	ℱ  	 ∉ )∈(r)
	                    (5) 

where we calculate the reverse of entropy of all 
concepts within a shot as topic concentration. The 
probability of each concept 𝑘 is determined by 
sum frequencies of its mentioned words weighted 
by their global frequencies, and divided by those 
values of all words in the shot. 

Now the comment importance ℐoM88O@5	 𝐶6, 𝑠  
of shot 𝑠 can be defined as: 
ℐoM88O@5	 𝑠 = 𝜆 ∙ 𝒞O8M52M@ 𝐶6, 𝑠 + (1 − 𝜆) ∙
	𝒞5MP2o(𝐶6, 𝑠)                                                               (6) 

where 𝜆 is a hyper-parameter, controlling the bal-
ance between emotion and concept concentration. 

Finally, we define the overall importance of 
shot as: 

ℐ 𝐶6, 𝑠 = ℐoM88O@5	 𝐶6, 𝑠 	 ∙ 	log	( 𝐶6 )                        (7) 

Where 𝐶¡  is the length for all time-sync 
comments in shot 𝑠, which is a straightforward yet 
effective indicator of comment intensity per shot.  

Now the problem of highlight detection can be 
modeled as a maximization problem: 

𝑀𝑎𝑥𝑖𝑚𝑖𝑧𝑒					 ℐ 𝐶6, 𝑠 ∙ 𝑥¡6d%                                           (8) 

𝑆𝑢𝑏𝑗𝑒𝑐𝑡𝑖𝑣𝑒	𝑡𝑜							 𝑥¡


nd%
≤ 𝜏123142315 ∙ 𝑁

	𝑥6 ∈ 0,1 																										
 

5 Video Highlight Summarization 

Given a set of detected highlight shots 𝑆(𝒗) =
{𝑠%, 𝑠', 𝑠(, … , 𝑠@} of video 𝒗, each with all the lag-
calibrated comments 𝐶6 of that shot, we are at-

Algorithm 2 Lag-Calibration of Time-Sync Com-
ments 
Input time-sync comments 𝐶. Word-to-concept 
mapping ℱ. Lexical chain dictionary 
𝐿4On2o94 	o192@. Word-embedding dictionary 𝐷. 
Output Lag-calibrated time-sync comments 𝐶′.  
Initialize 𝐶′ ← 𝐶 
for each c in 𝐶′ do 
   𝑐ℎ𝑎𝑖𝑛¨O65 ,o ← {} 
   𝑠𝑐𝑜𝑟𝑒¨O65 ,o ← 0 
   for each word in 𝑐 do 
     𝑘 ← 	ℱ(𝑤𝑜𝑟𝑑)  
     𝑐ℎ𝑎𝑖𝑛	e,o ← 	 𝐿4On2o94 	o192@(𝑘)[𝑐] 
     𝑠𝑐𝑜𝑟𝑒	e,o ← 0 
     for (𝑤, 𝑡, 𝑐) in 𝑐ℎ𝑎𝑖𝑛	do 
            𝑁(𝑤) ← 	𝐷(𝑤). 𝑐𝑜𝑢𝑛𝑡 
       𝑠𝑐𝑜𝑟𝑒e,o ← 𝑠𝑐𝑜𝑟𝑒e,o + 1/log	(𝑁(𝑤)) 
     end for 
     if 𝑠𝑐𝑜𝑟𝑒e,o > 𝑠𝑐𝑜𝑟𝑒¨O65 then 
          𝑐ℎ𝑎𝑖𝑛¨O65 ,o ← 𝑐ℎ𝑎𝑖𝑛e,o 
     end if 
   end for 
   𝑡o ← 𝑡o192@«¬,­[®2:65] 
end for 
return 𝐶′ 
Table 2. Lag-Calibration of Time-Sync Com-
ments. 
 

5



 

tempting to generate summaries Α 𝒗 =
	{𝐼%, 𝐼', 𝐼(, … , 𝐼@}  so that 𝐼6 ⊂ 𝐶6 with compres-
sion ratio 𝜏67889:; and 𝐼6 is as close to ground 
truth as possible. 

We propose a simple but very effective summa-
rization model, an improvement over SumBasic 
(Nenkova & Vanderwende, 2005) with emotion 
and concept mapping and two-level updating 
mechanism.  

In the modified SumBasic, instead of only 
down-sampling the probabilities of words in a se-
lected sentence to prevent redundancy, we down-
sample the probabilities of both words and their 
mapped concepts for re-weighting each comment. 
This two-level updating mechanism could: (1) 
impose a penalty for sentences with semantically 
similar words to be selected; (2) still select a sen-
tence with word already in the summary if this 
word occurs much more frequently. In addition, 
we use a parameter emotion bias b±²³´µ to 
weight words and concepts when computing their 
probabilities, so that frequencies of emotional 
words and concepts will increase by b±²³´µ 
compared to non-emotional words and concepts. 

6 Experiment 

In this section, we conduct experiments on large 
real datasets for highlight detection and summari-
zation. We will describe the data collection pro-
cess, evaluation metrics, benchmarks and experi-
ment results. 

6.1 Data  
In this section, we describe the datasets collected 
and constructed in our experiments. All datasets 
and codes will be made publicly available on 
Github2. 

Crowdsourced Time-sync Comment Corpus 

To train the word-embedding described in 4.1.1, 
we have collected a large corpus of time-sync 
comment from Bilibli3, a content sharing website 
in China with time-sync comments. The corpus 
contains 2,108,746 comments, 15,179,132 tokens, 
91,745 unique tokens, from 6,368 long videos. 
Each comment has 7.20 tokens on average. 

Before training, each comment is first to-
kenized using Chinse word tokenization package 
Jieba4. Repeating characters in words such as 
                                                        
2 https://github.com/ChanningPing/VideoHighlightDetection 
3 https://www.bilibili.com/ 
4 https://github.com/fxsjy/jieba 

“233333”, “66666”, “哈哈哈哈” are replaced 
with two same characters.  

The word-embedding is trained using 
word2vec (Goldberg & Levy, 2014) with the skip-
gram model. Number of embedding dimensions is 
300, window size is 7, down-sampling rate is 1e-
3, words with frequency lower than 3 times are 
discarded.   

Emotion Lexicon Construction 

After the word-embedding is trained, we manually 
select emotional words belonging to the five basic 
categories from the 500 most-frequent words in 
the word-embedding. Then we expand the emo-
tion seeds iteratively using algorithm 1. After each 

expansion iteration, we also manually examine the 
expanded lexicon and remove inaccurate words to 
prevent the concept-drift effect, and use the fil-
tered expanded seeds for expansion in next round. 
The minimum overlap 𝛾MNO:49P is set to be 0.05, 
and minimum similarity 𝑠𝑖𝑚82@ is set to be 0.6. 
The selection of 𝛾MNO:49P and 𝑠𝑖𝑚82@ is selected 
based on grid search in the range of 0,1 . The 
number of words for each emotion initially and af-
ter final expansion are listed in Table 3.   

Video Highlights Data 

To evaluate our highlight-detection algorithm, we 
have constructed a ground-truth dataset. Our 
ground-truth dataset takes advantage of user-
uploaded mixed-clips about a specific video on 
Bilibli. Mixed-clips are a collage of video high-
lights by the user’s own preferences. Then we take 
the most-voted highlights as ground-truth for a 
video. 

The dataset contains 11 videos of 1333 minutes 
in length, with 75,653 time-sync comments in to-
tal. For each video, 3~4 video mix-clips about this 
video are collected from Bilibili. Shots that occur 
in at least 2 of all the mix-clips are considered as 
ground-truth highlights. All ground-truth high-
lights are mapped to the original video timeline, 
and the start and end time of the highlight are rec-
orded as ground-truth. The mix-clips are selected 
based on the following heuristics: (1) The mixed-
clips are searched on Bilibli using the keywords 

 Happy Sad Fear Anger Surprise 
Seeds 17 13 21 14 19 
All 157 235 258 284 226 

Table 3. Number of Initial and Expanded Emo-
tion Words. 

6



 

“video title + mixed clips”; (2) The mixed-clips 
are sorted by play times in descending order; (3) 
The mix-clip should be mainly about highlights of 
the video, not a plot-by-plot summary or gist; (4) 
The mix-clip should be under 10 minutes; (5) The 
mix-clip should contain a mix of several highlight 
shots instead of only one. 

On average, each video has 24.3 highlight 
shots. The mean shot length of highlights is 27.79 
seconds, while the mode is 8 and 10 seconds (fre-
quency=19).  

Highlights Summarization Data 

We also construct a highlight-summarization (la-
beling) dataset of the 11 videos. For each high-
light shot with its comments, we ask annotators to 
construct a summary of these comments by ex-
tracting as many comments as they see necessary. 
The rules of thumb are: (1) Comments of the same 
meaning will not be selected more than once; (2) 
The most representative comment for similar 
comments is selected; (3) If a comment stands out 
on its own, and is irrelevant to the current discus-
sion, it will be discarded. 

For 11 videos of 267 highlights, each highlight 
has on average 3.83 comments as its summary.  

6.2 Evaluation Metrics 
In this section, we introduce evaluation metrics 
for highlight-detection and summarization. 

Video Highlight Detection Evaluation 

For the evaluation of video highlight detection, we 
need to define what is a “hit” between a highlight 
candidate and reference. A rigid definition would 
be a perfect match of beginnings and ends be-
tween candidate and reference highlights. Howev-
er, this is too harsh for any models. A more toler-
ant definition would be whether there is an over-
lap between a candidate and reference highlight. 
However, this will still underestimate model per-
formance since users’ selection of beginning and 
end of a highlight can be quite arbitrary some 
times. Instead, we propose a “hit” with relaxation 
𝜀 between a candidate ℎ and the reference 𝐻	as 
follows: 

ℎ𝑖𝑡¸ ℎ, 𝐻 =
1,
0,
				∃1∈¹:(6º,Oº)∩(6º¸,Oº¼¸)∉∅

M51O:\26O   (9) 

Where 𝑠1, 𝑒1 is the start time and end time of 
highlight ℎ, and 𝜀 is the relaxation length of refer-
ence set 𝐻.  Further, the precision, recall and F-1 
measure can be defined as: 

𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 𝐻, 𝐻 = 125 1,¹
¿
º

¹
                                 (10) 

𝑅𝑒𝑐𝑎𝑙𝑙 𝐻, 𝐻 =
125 1,¹¿

º
¹

                                      (11) 

𝐹1(𝐻, 𝐻) = '∙Â:Oo262M@ ¹,¹ ∙ÃOo944 ¹,¹
Â:Oo262M@ ¹,¹ ¼ÃOo944 ¹,¹

                        (12) 

In present study, we set the relaxation length to 
be 5 seconds. Also, the length for a candidate 
highlight is set to be 15 seconds.  

Video Highlight Summarization Evaluation 

We use ROUGE-1 and ROUGE-2 (C.-Y. Lin, 
2004) as recall of candidate summary for evalua-
tion: 

ROUGE-n(C,R) =
TM7@5ÉÊ¬­º(n-gram)n-gram∈∈Ë

TM7@5(n-gram)n-gram∈∈Ë
             (13) 

We use BLEU-1 and BLEU-2 (Papineni, Rou-
kos, Ward, & Zhu, 2002) as precision. We choose 
BLEU for two reasons. First, a naïve precision 
metric will be biased for shorter comments, and 
BLEU can compensate this with the 𝐵𝑃 product 
factor: 

BLEU-n(C, 𝑅) = BP ∙
TM7@5ÏÐÉÊ¬­º(n-gram)n-gram∈∈Ñ

TM7@5(n-gram)n-gram∈∈r
 (14) 

𝐵𝑃 =
1, 𝑖𝑓	 𝐶 > 𝑅

𝑒(% Ã T ), 𝑖𝑓	 𝐶 ≤ 𝑅
 

Where 𝐶 is the candidate summary and 𝑅 is the 
reference summary. Second, while reference 
summary contains no redundancy, candidate 
summary could falsely select multiple comments 
that are very similar and match to the same key-
words in reference. In such case, the precision is 
extremely overestimated. BLEU will only count 
the match one-by-one, namely the number of 
match of a word will be the minimum frequencies 
in candidate and reference.   

Finally, the F-1 measure can be defined as: 

F1-n(C,R)= '	∙	BLEU-n(C, Ã) · ROUGE-n(C,R)
BLEU-n(C, Ã)  + ROUGE-n(C,R)

                          (15) 

6.3 Benchmark methods 
Benchmarks for Video Highlight Detection  

For highlight detection, we provide comparisons 
of different combinations of our model with three 
benchmarks: 

• Random-selection. We select highlight 
shots randomly from all shots of a video. 

• Uniform-selection. We select highlight 
shots at equal intervals. 

7



 

• Spike-selection. We select those highlight 
shots who have the most number of com-
ments within the shot. 

• Spike+E+T. This is our method taking into 
consideration of emotion and topic concen-
tration without the lag-calibration step. 

• Spike+L. This is our method with only the 
lag-calibration step without taking into con-
sideration of content concentration. 

• Spike+L+E+T. This is our full model. 
 

Benchmarks for Video Highlight Summariza-
tion  

For highlight summarization, we provide compar-
isons of our method with five benchmarks: 

• SumBasic. Summarization that exclusively 
exploits frequency for summary construc-
tion (Nenkova & Vanderwende, 2005) .  

• Latent Semantic Analysis (LSA). Summa-
rization of text based on singular value de-
composition (SVD) for latent topic discov-
ery (Steinberger & Jezek, 2004). 

• LexRank. Graph-based summarization that 
calculates sentence importance based on the 
concept of eigenvector centrality in a graph 
of sentences (Erkan & Radev, 2004). 

• KL-Divergence. Summarization based on 
minimization of KL-divergence between 
summary and source corpus using greedy 
search (Haghighi & Vanderwende, 2009). 

• Luhn method. Heuristic summarization 
that takes into consideration of both word 
frequency and sentence position in an arti-
cle (Luhn, 1958). 

6.4 Experiment Results 
In this section, we report experimental results for 
highlight detection and highlight summarization. 

Results of Highlight Detection 

In our highlight detection model, the threshold for 
cutting a lexical chain 𝑙89n is set to be 11 se-
conds, the threshold for concept mapping 
𝜙MNO:49P is set to be 0.5, threshold for concept 
mapping 𝑡𝑜𝑝_𝑛 is set to be 15, and the parameter 
𝜆 to control balance of emotion and concept con-
centration is set to be 0.9. A parameter analysis is 
provided in section 7. 

The comparisons of precision, recall and F1 
measures of different combinations of our method 
and the benchmarks are in Table 4. Our full model 

(Spike+L+E+T) outperforms all other benchmarks 
on all metrics. The precision and recall for Ran-
dom-selection and uniform selection are low since 
they do not incorporate any structural or content 
information. Spike-selection improves considera-
bly, since it takes advantage of the comment in-
tensity of a shot. However, not all comment-
intensive shots are highlights. For example, com-
ments at the beginning and end of a video are usu-
ally high-volume greetings and goodbyes as a 
courtesy. Also, spike-selection usually condenses 
highlights on consecutive shots with high-volume 
comments, while our method could jump and 
scatter to other less intensive but emotionally or 
conceptually concentrated shots. This can be ob-
served by the performance of Spike+E+T.  

We also observe that lag-calibration (Spike+L) 
alone improves the performance of Spike-
selection considerably, partially confirming our 
hypothesis that lag-calibration is important in 
time-sync comment related tasks.  

Results of Highlight Summarization 

In our highlight summarization model, the emo-
tional bias 𝑏O8M52M@ is set to be 0.3.  

The comparisons on 1-gram BLEU, ROUGE 
and F1 of our method and the benchmarks are in 
Table 5. Our method outperforms all other meth-
ods, especially on ROUGE-1. LSA has lowest 
BLEU, mainly because LSA favors long and mul-
ti-word sentences statistically, however these sen-
tences are not representative in time-sync com-

 BLEU-1 ROUGE-1 F1-1 
LSA 0.2382 0.4855 0.3196 

SumBasic 0.2854 0.3898 0.3295 
KL-divergence 0.3162 0.3848 0.3471 

Luhn 0.2770 0.4970 0.3557 
LexRank 0.3045 0.4325 0.3574 

Our method 0.3333 0.6006 0.4287 

Table 5. Comparison of Highlight Summariza-
tion Methods (1-Gram). 

 

 Precision Recall F-1 
Random-Selection 0.1578 0.1587 0.1567 
Uniform-Selection 0.1775 0.1830 0.1797 

Spike-Selection 0.2594 0.2167 0.2321 
Spike+E+T 0.2796 0.2357 0.2500 
Spike + L 0.3125 0.2690 0.2829 

Spike+L+E+T 0.3099 0.3071 0.3066 

Table 4. Comparison of Highlight Detection 
Methods. 

 

8



 

ments. The SumBasic method also performs rela-
tively poor since it considers semantically related 
words separately unlike our method that use con-
cepts instead of words.  

The comparisons on 2-gram BLUE, ROUGE 
and F1 of our method and the benchmarks are in 
Table 6. Our method also outperforms all other 
methods.  

From the results, we believe that it is crucial to 
perform lag-calibration as well as concept and 
emotion mapping before summarization of time-
sync comment texts. Lag-calibration shrinks pro-
longed comments to its original shots, preventing 
inaccurate highlight detection. Concept and emo-
tional mapping works because time-sync com-
ments are usually very short (7.2 tokens on aver-
age), the meaning of the comment is usually con-
centrated on one or two “central-words” in the 

comment. Emotion mapping and concept mapping 
could effectively prevent the redundancy in the 
generated summary.  

7 Influence of Parameters 

7.1 Influence of Shot Length 
We analyze the influence of shot length on 𝐹1 
score for highlight detection. First from the distri-
bution of highlight shot lengths in golden stand-
ards (Figure 2), we observe that most of the high-
light shot lengths lie in the range of [0,25] (se-
conds), with 10 seconds as the mode. Therefore, 
we plot the 𝐹1 score of all four models at different 
shot lengths ranging from 5 to 23 seconds (Figure 
3).  

From Figure 3 we observe that (1) our method 
(Spike+L+E+T) consistently outperforms the oth-
er benchmarks at varied shot lengths; (2) however, 
the advantage of our method over Spike method 
seems to be moderated as the shot length increas-
es. This is reasonable, because as the shot length 
becomes longer, the number of comments in each 

shot accumulates. After certain point, shot with 
significantly more comments will signify as high-
light, no matter of the emotions and topics it con-
tains. However, this may not always be the case. 
In reality, when there are too few comments, de-
tection totally relying on volume will fail; on the 
other hand, when there are overwhelming vol-
umes of comments evenly distributed among 
shots, spikes may not be a good indicator since 

every shot has equally large volumes of comments 
now. Moreover, most highlights in reality are be-
low 15 seconds, and Figure 3 shows that our 
method could detect highlights more accurately at 
such finer level.  

7.2 Parameters for Highlight Detection 
We analyze the influence of four parameters on 
recall for highlight detection: maximum silence 
for lexical chains l²ÕÖ, the threshold for concept 
mapping ϕØ±ÙÕÚ, the number of neighbors for 
concept mapping top_n, and the balance of emo-
tion and concept concentration λ (Figure 4).  

From Figure 4, we observe the following: (1) 
when it comes to lag-calibration, there seems to 
be an optimal Max Silence Length: 11 seconds as 
the longest blank continuance of a chain for our 
dataset. This value controls the compactness of a 
lexical chain. (2) In concept mapping, the Mini-
mum Overlap with Existing Concepts controls the 
threshold for concept-merge, the higher the 

 BLEU-2 ROUGE-2 F1-2 
SumBasic 0.1059 0.1771 0.1325 

LSA 0.0943 0.2915 0.1425 
LexRank 0.1238 0.2351 0.1622 

KL-divergence 0.1337 0.2362 0.1707 
Luhn 0.1227 0.3176 0.1770 

Our method 0.1508 0.3909 0.2176 

Table 6. Comparison of Highlight Summari-
zation Methods (2-Gram). 

 

 
Figure 2.Distribution of Shot Lengths in 

Highlight Golden Standards. 
 

 
Figure 3.Influence of Shot Length on F-1 

Scores of Highlight Detection. 
 

9



 

threshold the more similar the two merged con-
cepts are. The recall increases as overlap increase 
to a certain point (0.5 in our dataset), and will not 
improve further after such point. (3) In concept 
mapping, there seems to be an optimal Number of 
Neighbors for searching (15 in our dataset). (4) 
The balance between emotion and concept con-
centration (lambda) is more on the emotion side 
(0.9 in our dataset).  

7.3 Parameter for Highlight Summarization 
We also analyze the influence of emotion bias 
b±²³´µ on ROGUE-1 and ROGUE-2 for high-
light summarization. The results are depicted in 
Figure 5.  

From Figure 5, we observe that when it comes 
to highlight summarization, emotion plays a mod-
erate role (emotion bias = 0.3). This is less signifi-
cant than its role in the highlight detection task, 
where emotion concentration is much more im-
portant than concept concentration.  

 

8 Conclusion 

In this paper, we propose a novel unsupervised 
framework for video highlight detection and 
summarization based on crowdsourced time-sync 
comments. For highlight detection, we develop a 
lag-calibration technique that shrinks lagged 

comments back to their original scenes based on 
concept-mapped lexical-chains. Moreover, video 
highlights are detected by scoring of comment in-
tensity and concept-emotion concentration in each 
shot. For highlight summarization, we propose a 
two-level SumBasic that updates word and con-
cept probabilities at the same time in each itera-
tive sentence selection. In the future, we plan to 
integrate multiple sources of information for high-
light detection, such as video meta-data, audience 
profiles, as well as low-level features of multiple 
modalities through video-processing.  

References 
Barzilay, R., & Elhadad, M. (1999). Using lexical 

chains for text summarization. Advances in auto-
matic text summarization, 111-121.  

Ehren, R. (2017). Literal or idiomatic? Identifying the 
reading of single occurrences of German multi-
word expressions using word embeddings. Pro-
ceedings of the Student Research Workshop at the 
15th Conference of the European Chapter of the 
Association for Computational Linguistics, 103–
112.  

Ekman, P. (1992). An argument for basic emotions. 
Cognition & emotion, 6(3-4), 169-200.  

Erkan, G., & Radev, D. R. (2004). Lexrank: Graph-
based lexical centrality as salience in text summa-
rization. Journal of Artificial Intelligence Research, 
22, 457-479.  

Goldberg, Y., & Levy, O. (2014). word2vec ex-
plained: Deriving mikolov et al.'s negative-
sampling word-embedding method. arXiv preprint 
arXiv:1402.3722.  

Haghighi, A., & Vanderwende, L. (2009). Exploring 
content models for multi-document summarization. 
Paper presented at the Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference 
of the North American Chapter of the Association 
for Computational Linguistics. 

Hanjalic, A., & Xu, L.-Q. (2005). Affective video 
content representation and modeling. IEEE transac-
tions on multimedia, 7(1), 143-154.  

Hirst, G., & St-Onge, D. (1998). Lexical chains as 
representations of context for the detection and 
correction of malapropisms. WordNet: An electron-
ic lexical database, 305, 305-332.  

Ikeda, A., Kobayashi, A., Sakaji, H., & Masuyama, S. 
(2015). Classification of comments on nico nico 
douga for annotation based on referred contents. 
Paper presented at the Network-Based Information 
Systems (NBiS), 2015 18th International Confer-
ence on. 

 
Figure 4.Influence of Parameters for High-

light Detection. 

 
Figure 5.Influence of Parameter for Highlight 

Summarization. 
 
 

10



 

Lin, C., Lin, C., Li, J., Wang, D., Chen, Y., & Li, T. 
(2012). Generating event storylines from mi-
croblogs. Paper presented at the Proceedings of the 
21st ACM international conference on Information 
and knowledge management. 

Lin, C.-Y. (2004). Rouge: A package for automatic 
evaluation of summaries. Paper presented at the 
Text summarization branches out: Proceedings of 
the ACL-04 workshop. 

Lin, K.-S., Lee, A., Yang, Y.-H., Lee, C.-T., & Chen, 
H. H. (2013). Automatic highlights extraction for 
drama video using music emotion and human face 
features. Neurocomputing, 119, 111-117.  

Luhn, H. P. (1958). The automatic creation of litera-
ture abstracts. IBM Journal of research and devel-
opment, 2(2), 159-165.  

Lv, G., Xu, T., Chen, E., Liu, Q., & Zheng, Y. (2016). 
Reading the Videos: Temporal Labeling for 
Crowdsourced Time-Sync Videos Based on Se-
mantic Embedding. Paper presented at the AAAI. 

Morris, J., & Hirst, G. (1991). Lexical cohesion com-
puted by thesaural relations as an indicator of the 
structure of text. Computational linguistics, 17(1), 
21-48.  

Nenkova, A., & Vanderwende, L. (2005). The impact 
of frequency on summarization. Microsoft Re-
search, Redmond, Washington, Tech. Rep. MSR-
TR-2005, 101.  

Ngo, C.-W., Ma, Y.-F., & Zhang, H.-J. (2005). Video 
summarization and scene detection by graph mod-
eling. IEEE Transactions on Circuits and Systems 
for Video Technology, 15(2), 296-305.  

Papineni, K., Roukos, S., Ward, T., & Zhu, W.-J. 
(2002). BLEU: a method for automatic evaluation 
of machine translation. Paper presented at the Pro-
ceedings of the 40th annual meeting on association 
for computational linguistics. 

Sakaji, H., Kohana, M., Kobayashi, A., & Sakai, H. 
(2016). Estimation of Tags via Comments on Nico 
Nico Douga. Paper presented at the Network-
Based Information Systems (NBiS), 2016 19th In-
ternational Conference on. 

Shou, L., Wang, Z., Chen, K., & Chen, G. (2013). 
Sumblr: continuous summarization of evolving 
tweet streams. Paper presented at the Proceedings 
of the 36th international ACM SIGIR conference 
on Research and development in information re-
trieval. 

Sipos, R., Swaminathan, A., Shivaswamy, P., & Joa-
chims, T. (2012). Temporal corpus summarization 
using submodular word coverage. Paper presented 
at the Proceedings of the 21st ACM international 
conference on Information and knowledge man-
agement. 

Steinberger, J., & Jezek, K. (2004). Using latent se-
mantic analysis in text summarization and sum-
mary evaluation. Paper presented at the Proc. 
ISIM’04. 

Tran, T. A., Niederée, C., Kanhabua, N., Gadiraju, U., 
& Anand, A. (2015). Balancing novelty and sali-
ence: Adaptive learning to rank entities for timeline 
summarization of high-impact events. Paper pre-
sented at the Proceedings of the 24th ACM Interna-
tional on Conference on Information and 
Knowledge Management. 

Wu, B., Zhong, E., Tan, B., Horner, A., & Yang, Q. 
(2014). Crowdsourced time-sync video tagging us-
ing temporal and personalized topic modeling. Pa-
per presented at the Proceedings of the 20th ACM 
SIGKDD international conference on Knowledge 
discovery and data mining. 

Xian, Y., Li, J., Zhang, C., & Liao, Z. (2015). Video 
Highlight Shot Extraction with Time-Sync Com-
ment. Paper presented at the Proceedings of the 7th 
International Workshop on Hot Topics in Planet-
scale mObile computing and online Social neT-
working. 

Xu, L., & Zhang, C. (2017). Bridging Video Content 
and Comments: Synchronized Video Description 
with Temporal Summarization of Crowdsourced 
Time-Sync Comments. Paper presented at the Thir-
ty-First AAAI Conference on Artificial Intelli-
gence. 

Xu, M., Jin, J. S., Luo, S., & Duan, L. (2008). Hierar-
chical movie affective content analysis based on 
arousal and valence features. Paper presented at the 
Proceedings of the 16th ACM international confer-
ence on Multimedia. 

Xu, M., Luo, S., Jin, J. S., & Park, M. (2009). Affec-
tive content analysis by mid-level representation in 
multiple modalities. Paper presented at the Pro-
ceedings of the First International Conference on 
Internet Multimedia Computing and Service. 

Yan, R., Kong, L., Huang, C., Wan, X., Li, X., & 
Zhang, Y. (2011). Timeline generation through evo-
lutionary trans-temporal summarization. Paper pre-
sented at the Proceedings of the Conference on 
Empirical Methods in Natural Language Pro-
cessing. 

Yan, R., Wan, X., Otterbacher, J., Kong, L., Li, X., & 
Zhang, Y. (2011). Evolutionary timeline summari-
zation: a balanced optimization framework via iter-
ative substitution. Paper presented at the Proceed-
ings of the 34th international ACM SIGIR confer-
ence on Research and development in Information 
Retrieval. 

 

11


