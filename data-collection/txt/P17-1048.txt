



















































Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics


Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 518–529
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1048

Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 518–529
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1048

Joint CTC/attention decoding for end-to-end speech recognition

Takaaki Hori, Shinji Watanabe, John R. Hershey
Mitsubishi Electric Research Laboratories (MERL)
{thori,watanabe,hershey}@merl.com

Abstract

End-to-end automatic speech recognition
(ASR) has become a popular alterna-
tive to conventional DNN/HMM sys-
tems because it avoids the need for
linguistic resources such as pronuncia-
tion dictionary, tokenization, and context-
dependency trees, leading to a greatly
simplified model-building process. There
are two major types of end-to-end archi-
tectures for ASR: attention-based meth-
ods use an attention mechanism to per-
form alignment between acoustic frames
and recognized symbols, and connection-
ist temporal classification (CTC), uses
Markov assumptions to efficiently solve
sequential problems by dynamic program-
ming. This paper proposes a joint de-
coding algorithm for end-to-end ASR
with a hybrid CTC/attention architecture,
which effectively utilizes both advantages
in decoding. We have applied the pro-
posed method to two ASR benchmarks
(spontaneous Japanese and Mandarin Chi-
nese), and showing the comparable per-
formance to conventional state-of-the-art
DNN/HMM ASR systems without lin-
guistic resources.

1 Introduction

Automatic speech recognition (ASR) is currently
a mature set of technologies that have been widely
deployed, resulting in great success in interface
applications such as voice search. A typical ASR
system is factorized into several modules includ-
ing acoustic, lexicon, and language models based
on a probabilistic noisy channel model (Jelinek,
1976). Over the last decade, dramatic improve-
ments in acoustic and language models have been

driven by machine learning techniques known as
deep learning (Hinton et al., 2012).

However, current systems lean heavily on the
scaffolding of complicated legacy architectures
that grew up around traditional techniques. For
example, when we build an acoustic model from
scratch, we have to first build hidden Markov
model (HMM) and Gaussian mixture model
(GMM) followed by deep neural networks (DNN).
In addition, the factorization of acoustic, lexicon,
and language models is derived by conditional in-
dependence assumptions (especially Markov as-
sumptions), although the data do not necessarily
follow such assumptions leading to model mis-
specification. This factorization form also yields
a local optimum since the above modules are
optimized separately. Further, to well factorize
acoustic and language models, the system requires
linguistic knowledge based on a lexicon model,
which is usually based on a hand-crafted pronun-
ciation dictionary to map word to phoneme se-
quence. In addition to the pronunciation dictio-
nary issue, some languages, which do not ex-
plicitly have a word boundary, need language-
specific tokenization modules (Kudo et al., 2004;
Bird, 2006) for language modeling. Finally, in-
ference/decoding has to be performed by integrat-
ing all modules resulting in complex decoding.
Consequently, it is quite difficult for non-experts
to use/develop ASR systems for new applications,
especially for new languages.

End-to-end ASR has the goal of simplifying
the above module-based architecture into a single-
network architecture within a deep learning frame-
work, in order to address the above issues. There
are two major types of end-to-end architectures
for ASR: attention-based methods use an attention
mechanism to perform alignment between acous-
tic frames and recognized symbols, and connec-
tionist temporal classification (CTC), uses Markov

518

https://doi.org/10.18653/v1/P17-1048
https://doi.org/10.18653/v1/P17-1048


assumptions to efficiently solve sequential prob-
lems by dynamic programming (Chorowski et al.,
2014; Graves and Jaitly, 2014).

The attention-based end-to-end method solves
the ASR problem as a sequence mapping from
speech feature sequences to text by using encoder-
decoder architecture. The decoder network uses
an attention mechanism to find an alignment be-
tween each element of the output sequence and
the hidden states generated by the acoustic en-
coder network for each frame of acoustic input
(Chorowski et al., 2014, 2015; Chan et al., 2015;
Lu et al., 2016). This basic temporal attention
mechanism is too flexible in the sense that it allows
extremely non-sequential alignments. This may be
fine for applications such as machine translation
where input and output word order are different
(Bahdanau et al., 2014; Wu et al., 2016). How-
ever, in speech recognition, the feature inputs and
corresponding letter outputs generally proceed in
the same order. Another problem is that the input
and output sequences in ASR can have very dif-
ferent lengths, and these vary greatly from case to
case, depending on the speaking rate and writing
system, making it more difficult to track the align-
ment.

However, an advantage is that the attention
mechanism does not require any conditional in-
dependence assumptions, and could address all
the problems cited above. Although the align-
ment problems of attention-based mechanisms
have been partially addressed in (Chorowski et al.,
2014; Chorowski and Jaitly, 2016) using various
mechanisms, here we propose more rigorous con-
straints by using CTC-based alignment to guide
the decoding.

CTC permits an efficient computation of a
strictly monotonic alignment using dynamic pro-
gramming (Graves et al., 2006; Graves and Jaitly,
2014) although it requires language models and
graph-based decoding (Miao et al., 2015) except
in the case of huge training data (Amodei et al.,
2015; Soltau et al., 2016). We propose to take ad-
vantage of the constrained CTC alignment in a hy-
brid CTC/attention based system during decoding.
The proposed method adopts a CTC/attention hy-
brid architecture, which was originally designed
to regularize an attention-based encoder network
by additionally using a CTC during training (Kim
et al., 2017). The proposed method extends the ar-
chitecture to perform one-pass/rescoring joint de-

coding, where hypotheses of attention-based ASR
are boosted by scores obtained by using CTC out-
puts. This greatly reduces irregular alignments
without any heuristic search techniques.

The proposed method is applied to Japanese
and Mandarin ASR tasks, which require extra
linguistic resources including morphological an-
alyzer (Kudo et al., 2004) or word segmentation
(Xue et al., 2003) in addition to pronunciation dic-
tionary to provide accurate lexicon and language
models in conventional DNN/HMM ASR. Sur-
prisingly, the method achieved performance com-
parable to, and in some cases superior to, several
state-of-the-art DNN/HMM ASR systems, with-
out using the above linguistic resources.

2 From DNN/HMM to end-to-end ASR

This section briefly provides a formulation of con-
ventional DNN/HMM ASR and CTC or attention
based end-to-end ASR.

2.1 Conventional DNN/HMM ASR
ASR deals with a sequence mapping from T -
length speech feature sequence X = {xt ∈
RD|t = 1, · · · , T} to N -length word sequence
W = {wn ∈ V|n = 1, · · · , N}. xt is a D
dimensional speech feature vector (e.g., log Mel
filterbanks) at frame t and wn is a word at posi-
tion n in vocabulary V . ASR is mathematically
formulated with the Bayes decision theory, where
the most probable word sequence Ŵ is estimated
among all possible word sequences V∗ as follows:

Ŵ = arg max
W∈V∗

p(W |X). (1)

The posterior distribution p(W |X) is factorized
into the following three distributions by using the
Bayes theorem and introducing HMM state se-
quence S = {st ∈ {1, · · · , J}|t = 1, · · · , T}:

Eq. (1) ≈ arg max
W

∑

S

p(X|S)p(S|W )p(W ).

The three factors, p(X|S), p(S|W ), and p(W ),
are acoustic, lexicon, and language models, re-
spectively. These are further factorized by using
a probabilistic chain rule and conditional indepen-
dence assumption as follows:




p(X|S) ≈∏t
p(st|xt)
p(st)

,

p(S|W )≈∏t p(st|st−1,W ),
p(W ) ≈∏n p(wn|wn−1, . . . , wn−m−1),

519



where the acoustic model is replaced with
the product of framewise posterior distributions
p(st|xt) computed by powerful DNN classi-
fiers by using so-called pseudo likelihood trick
(Bourlard and Morgan, 1994). p(st|st−1,W ) is
represented by an HMM state transition given W ,
and the conversion from W to HMM states is de-
terministically performed by using a pronuncia-
tion dictionary through a phoneme representation.
p(wn|wn−1, . . . , wn−m−1) is obtained based on
an (m − 1)th-order Markov assumption as a m-
gram model.

These conditional independence assumptions
are often regarded as too strong assumption, lead-
ing to model mis-specification. Also, to train the
framewise posterior p(st|xt), we have to provide
a framewise state alignment st as a target, which
is often provided by a GMM/HMM system. Thus,
conventional DNN/HMM systems make the ASR
problem formulated with Eq. (1) feasible by us-
ing factorization and conditional independence as-
sumptions, at the cost of the problems discussed in
Section 1.

2.2 Connectionist Temporal Classification
(CTC)

The CTC formulation also follows from Bayes de-
cision theory (Eq. (1)). Note that the CTC formu-
lation uses L-length letter sequence C = {cl ∈
U|l = 1, · · · , L} with a set of distinct letters U .
Similarly to Section 2.1, by introducing frame-
wise letter sequence with an additional ”blank”
( < b >) symbol Z = {zt ∈ U ∪ < b >|t =
1, · · · , T}, and by using the probabilistic chain
rule and conditional independence assumption, the
posterior distribution p(C|X) is factorized as fol-
lows:

p(C|X) ≈
∑

Z

∏

t

p(zt|zt−1, C)p(zt|X)
︸ ︷︷ ︸

,pctc(C|X)

p(C)

p(Z)

(2)

As a result, CTC has three distribution com-
ponents similar to the DNN/HMM case, i.e.,
framewise posterior distribution p(zt|X), tran-
sition probability p(zt|zt−1, C)1, and prior dis-
tributions of letter and hidden-state sequences,

1Note that in the implementation, the transition value is
not normalized (i.e., not a probabilistic value) (Graves and
Jaitly, 2014; Miao et al., 2015), similar to the HMM state
transition implementation (Povey et al., 2011)

p(C) and p(Z), respectively. We also define
the CTC objective function pctc(C|X) used in
the later formulation. The framewise posterior
distribution p(zt|X) is conditioned on all in-
puts X , and it is quite natural to be modeled
by using bidirectional long short-term memory
(BLSTM): p(zt|X) = Softmax(Lin(ht)) and
ht = BLSTM(X). Softmax(·) is a sofmax activa-
tion function, and Lin(·) is a linear layer to convert
hidden vector ht to a (|U|+ 1) dimensional vector
(+1 means a blank symbol introduced in CTC).

Although Eq. (2) has to deal with a summa-
tion over all possible Z, it is efficiently computed
by using dynamic programming (Viterbi/forward-
backward algorithm) thanks to the Markov prop-
erty. In summary, although CTC and DNN/HMM
systems are similar to each other due to condi-
tional independence assumptions, CTC does not
require pronunciation dictionaries and omits an
GMM/HMM construction step.

2.3 Attention mechanism
Compared with hybrid and CTC approaches, the
attention-based approach does not make any con-
ditional independence assumptions, and directly
estimates the posterior p(C|X) based on a prob-
abilistic chain rule, as follows:

p(C|X) =
∏

l

p(cl|c1, · · · , cl−1, X)
︸ ︷︷ ︸

,patt(C|X)

, (3)

where patt(C|X) is an attention-based objective
function. p(cl|c1, · · · , cl−1, X) is obtained by

p(cl|c1, · · · , cl−1, X) = Decoder(rl,ql−1, cl−1)
ht = Encoder(X) (4)

alt = Attention({al−1}t,ql−1,ht) (5)
rl =

∑

t

altht. (6)

Eq. (4) converts input feature vectors X into a
framewise hidden vector ht in an encoder net-
work based on BLSTM, i.e., Encoder(X) ,
BLSTM(X). Attention(·) in Eq. (5) is based on
a content-based attention mechanism with convo-
lutional features, as described in (Chorowski et al.,
2015) (see Appendix A). alt is an attention weight,
and represents a soft alignment of hidden vector ht
for each output cl based on the weighted summa-
tion of hidden vectors to form letter-wise hidden
vector rl in Eq. (6). A decoder network is another

520



recurrent network conditioned on previous output
cl−1 and hidden vector ql−1, similar to RNNLM,
in addition to letter-wise hidden vector rl. We use
Decoder(·) , Softmax(Lin(LSTM(·))).

Attention-based ASR does not explicitly sep-
arate each module, and potentially handles the
all issues pointed out in Section 1. It implic-
itly combines acoustic models, lexicon, and lan-
guage models as encoder, attention, and decoder
networks, which can be jointly trained as a single
deep neural network.

Compared with DNN/HMM and CTC, which
are based on a transition form from t − 1 to t due
to the Markov assumption, the attention mecha-
nism does not maintain this constraint, and often
provides irregular alignments. A major focus of
this paper is to address this problem by using joint
CTC/attention decoding.

3 Joint CTC/attention decoding

This section explains a hybrid CTC/attention net-
work, which potentially utilizes both benefits of
CTC and attention in ASR.

3.1 Hybrid CTC/attention architecture
Kim et al. (2017) uses a CTC objective function as
an auxiliary task to train the attention model en-
coder within the multitask learning (MTL) frame-
work, and this paper also uses the same archi-
tecture. Figure 1 illustrates the overall architec-
ture of the framework, where the same BLSTM is
shared with CTC and attention encoder networks,
respectively). Unlike the sole attention model, the
forward-backward algorithm of CTC can enforce
monotonic alignment between speech and label
sequences during training. That is, rather than
solely depending on data-driven attention meth-
ods to estimate the desired alignments in long se-
quences, the forward-backward algorithm in CTC
helps to speed up the process of estimating the de-
sired alignment. The objective to be maximized is
a logarithmic linear combination of the CTC and
attention objectives, i.e., pctc(C|X) in Eq. (2) and
patt(C|X) in Eq. (3):
LMTL = λ log pctc(C|X) + (1− λ) log patt(C|X),

(7)

with a tunable parameter λ : 0 ≤ λ ≤ 1.

3.2 Decoding strategies
The inference step of our joint CTC/attention-
based end-to-end speech recognition is performed

ㅡ ㅡㅡz2 …

sos eosc1

q0

r0 r1 rL

H

h2 h4

q1 qL

hT

x1 x2 x3 x4 x5 x6 xT

Shared 
Encoder

CTC

Attention
Decoder

x7 x8

z4

h6 h8

…

c2

r2

q2

…c1 c2 …

c1 c2 …

Figure 1: Joint CTC/attention based end-to-end
framework: the shared encoder is trained by both
CTC and attention model objectives simultane-
ously. The shared encoder transforms our input
sequence {xt · · ·xT } into high level featuresH =
{ht · · ·hT }, and the attention decoder generates
the letter sequence {c1 · · · cL}.

by label synchronous decoding with a beam
search similar to conventional attention-based
ASR. However, we take the CTC probabilities into
account to find a hypothesis that is better aligned
to the input speech, as shown in Figure 1. Here-
after, we describe the general attention-based de-
coding and conventional techniques to mitigate the
alignment problem. Then, we propose joint de-
coding methods with a hybrid CTC/attention ar-
chitecture.

3.2.1 Attention-based decoding in general
End-to-end speech recognition inference is gener-
ally defined as a problem to find the most probable
letter sequence Ĉ given the speech input X , i.e.

Ĉ = arg max
C∈U∗

log p(C|X). (8)

In attention-based ASR, p(C|X) is computed by
Eq. (3), and Ĉ is found by a beam search tech-
nique.

Let Ωl be a set of partial hypotheses of the
length l. At the beginning of the beam search,
Ω0 contains only one hypothesis with the start-
ing symbol <sos> and the hypothesis score
α(<sos>, X) is set to 0. For l = 1 to Lmax, each
partial hypothesis in Ωl−1 is expanded by append-
ing possible single letters, and the new hypothe-
ses are stored in Ωl, where Lmax is the maximum

521



length of the hypotheses to be searched. The score
of each new hypothesis is computed in the log do-
main as

α(h,X) = α(g,X) + log p(c|g,X), (9)

where g is a partial hypothesis in Ωl−1, c is a letter
appended to g, and h is the new hypothesis such
that h = g · c. If c is a special symbol that repre-
sents the end of a sequence, <eos>, h is added to
Ω̂ but not Ωl, where Ω̂ denotes a set of complete
hypotheses. Finally, Ĉ is obtained by

Ĉ = arg max
h∈Ω̂

α(h,X). (10)

In the beam search process, Ωl is allowed to hold
only a limited number of hypotheses with higher
scores to improve the search efficiency.

Attention-based ASR, however, may be prone
to include deletion and insertion errors because
of its flexible alignment property, which can at-
tend to any portion of the encoder state sequence
to predict the next label, as discussed in Section
2.3. Since attention is generated by the decoder
network, it may prematurely predict the end-of-
sequence label, even when it has not attended to
all of the encoder frames, making the hypothesis
too short. On the other hand, it may predict the
next label with a high probability by attending to
the same portions as those attended to before. In
this case, the hypothesis becomes very long and
includes repetitions of the same letter sequence.

3.2.2 Conventional decoding techniques
To alleviate the alignment problem, a length
penalty term is commonly used to control the hy-
pothesis length to be selected (Chorowski et al.,
2015; Bahdanau et al., 2016). With the length
penalty, the decoding objective in Eq. (8) is
changed to

Ĉ = arg max
C∈U∗

{log p(C|X) + γ|C|} , (11)

where |C| is the length of the sequence C, and γ is
a tunable parameter. However, it is actually diffi-
cult to completely exclude hypotheses that are too
long or too short even if γ is carefully tuned. It
is also effective to control the hypothesis length
by the minimum and maximum lengths to some
extent, where the minimum and maximum are se-
lected as fixed ratios to the length of the input
speech. However, since there are exceptionally
long or short transcripts compared to the input

speech, it is difficult to balance saving such excep-
tional transcripts and preventing hypotheses with
irrelevant lengths.

Another approach is the coverage term re-
cently proposed in (Chorowski and Jaitly, 2016),
which is incorporated in the decoding objective in
Eq. (11) as

Ĉ = arg max
C∈U∗

{log p(C|X) + γ|C|

+η · coverage(C|X)} , (12)

where the coverage term is computed by

coverage(C|X) =
T∑

t=1

[
L∑

l=1

alt > τ

]
. (13)

η and τ are tunable parameters. The coverage term
represents the number of frames that have received
a cumulative attention greater than τ . Accord-
ingly, it increases when paying close attention to
some frames for the first time, but does not in-
crease when paying attention again to the same
frames. This property is effective for avoiding
looping of the same label sequence within a hy-
pothesis. However, it is still difficult to obtain a
common parameter setting for γ, η, τ , and the op-
tional min/max lengths so that they are appropriate
for any speech data from different tasks.

3.2.3 Joint decoding
Our joint CTC/attention approach combines the
CTC and attention-based sequence probabilities in
the inference step, as well as the training step.
Suppose pctc(C|X) in Eq. (2) and patt(C|X) in
Eq. (3) are the sequence probabilities given by
CTC and the attention model. The decoding ob-
jective is defined similarly to Eq. (7) as

Ĉ = arg max
C∈U∗

{λ log pctc(C|X)

+(1− λ) log patt(C|X)} . (14)

The CTC probability enforces a monotonic align-
ment that does not allow large jumps or looping
of the same frames. Accordingly, it is possible
to choose a hypothesis with a better alignment
and exclude irrelevant hypotheses without relying
on the coverage term, length penalty, or min/max
lengths.

In the beam search process, the decoder needs
to compute a score for each partial hypothesis us-
ing Eq. (9). However, it is nontrivial to combine
the CTC and attention-based scores in the beam

522



search, because the attention decoder performs it
output-label-synchronously while CTC performs
it frame-synchronously. To incorporate the CTC
probabilities in the hypothesis score, we propose
two methods.

Rescoring
The first method is a two-pass approach, in which
the first pass obtains a set of complete hypotheses
using the beam search, where only the attention-
based sequence probabilities are considered. The
second pass rescores the complete hypotheses us-
ing the CTC and attention probabilities, where the
CTC probabilities are obtained by the forward al-
gorithm for CTC (Graves et al., 2006). The rescor-
ing pass obtains the final result according to

Ĉ = arg max
h∈Ω̂
{λαctc(h,X) + (1− λ)αatt(h,X)} ,

(15)

where
{
αctc(h,X) , log pctc(h|X)
αatt(h,X) , log patt(h|X)

. (16)

One-pass decoding
The second method is one-pass decoding, in which
we compute the probability of each partial hypoth-
esis using CTC and an attention model. Here, we
utilize the CTC prefix probability (Graves, 2008)
defined as the cumulative probability of all label
sequences that have the partial hypothesis h as
their prefix:

pctc(h, . . . |X) =
∑

ν∈(U∪{<eos>})+
pctc(h · ν|X),

and we define the CTC score as

αctc(h,X) , log pctc(h, . . . |X), (17)

where ν represents all possible label sequences ex-
cept the empty string. The CTC score cannot be
obtained recursively as in Eq. (9), but it can be
computed efficiently by keeping the forward prob-
abilities over the input frames for each partial hy-
pothesis. Then it is combined with αatt(h,X).

The beam search algorithm for one-pass decod-
ing is shown in Algorithm 1. Ωl and Ω̂ are ini-
tialized in lines 2 and 3 of the algorithm, which
are implemented as queues that accept partial hy-
potheses of the length l and complete hypothe-
ses, respectively. In lines 4–25, each partial hy-
pothesis g in Ωl−1 is extended by each label c

Algorithm 1 Joint CTC/attention one-pass decod-
ing
1: procedure ONEPASSBEAMSEARCH(X ,Lmax)
2: Ω0 ← {<sos>}
3: Ω̂← ∅
4: for l = 1 . . . Lmax do
5: Ωl ← ∅
6: while Ωl−1 6= ∅ do
7: g ← HEAD(Ωl−1)
8: DEQUEUE(Ωl−1)
9: for each c ∈ U ∪ {<eos>} do

10: h← g · c
11: α(h,X)←λαctc(h,X)+(1−λ)αatt(h,X)
12: if c = <eos> then
13: ENQUEUE(Ω̂, h)
14: else
15: ENQUEUE(Ωl, h)
16: if |Ωl| > beamWidth then
17: REMOVEWORST(Ωl)
18: end if
19: end if
20: end for
21: end while
22: if ENDDETECT(Ω̂, l) = true then
23: break . exit for loop
24: end if
25: end for
26: return arg maxh∈Ω̂ α(h,X)
27: end procedure

in the label set U . Each extended hypothesis h
is scored in line 11, where CTC and attention-
based scores are obtained by αctc() and αatt(). Af-
ter that, if c = <eos>, the hypothesis h is as-
sumed to be complete and stored in Ω̂ in line 13.
If c 6= <eos>, h is stored in Ωl in line 15, where
the number of hypotheses in Ωl is checked in line
16. If the number exceeds the beam width, the hy-
pothesis with the worst score in Ωl is removed by
REMOVEWORST() in line 17.

In line 11, the CTC and attention model scores
are computed for each partial hypothesis. The at-
tention score is easily obtained in the same man-
ner as Eq. (9), whereas the CTC score requires
a modified forward algorithm that computes it
label-synchronously. The algorithm to compute
the CTC score is summarized in Appendix B. By
considering the attention and CTC scores during
the beam search, partial hypotheses with irregu-
lar alignments can be excluded, and the number of
search errors is reduced.

We can optionally apply an end detection tech-
nique to reduce the computation by stopping the
beam search before l reaches Lmax. Function
ENDDETECT(Ω̂, l) in line 22 returns true if
there is little chance of finding complete hypothe-
ses with higher scores as l increases in the future.

523



In our implementation, the function returns true
if

M−1∑

m=0

[
max

h∈Ω̂:|h|=l−m
α(h,X)−max

h′∈Ω̂
α(h′, X)<Dend

]
=M,

(18)
where Dend and M are predetermined thresholds.
This equation becomes true if complete hypothe-
ses with smaller scores are generated M times
consecutively. This technique is also available in
attention-based decoding and rescoring methods
described in Sections 3.2.1–3.2.3.

4 Experiments

We used Japanese and Mandarin Chinese ASR
benchmarks to show the effectiveness of the pro-
posed joint CTC/attention decoding approach.
The main reason for choosing these two languages
is that those ideogram languages have relatively
shorter lengths for letter sequences than those in
alphabet languages, which reduces computational
complexities greatly, and makes it easy to handle
context information in a decoder network. Our
preliminary investigation shows that Japanese and
Mandarin Chinese end-to-end ASR can be eas-
ily scaled up, and shows state-of-the-art perfor-
mance without using various tricks developed in
English tasks. Also, we would like to emphasize
that the system did not use language-specific pro-
cessing (e.g., morphological analyzer, Pinyin dic-
tionary), and simply used all appeared characters
in their transcriptions including Japanese syllable
and Kanji, Chinese, Arabic number, and alphabet
characters, as they are.

4.1 Corpus of Spontaneous Japanese (CSJ)

We demonstrated ASR experiments by using the
Corpus of Spontaneous Japanese (CSJ) (Maekawa
et al., 2000). CSJ is a standard Japanese ASR task
based on a collection of monologue speech data
including academic lectures and simulated presen-
tations. It has a total of 581 hours of training
data and three types of evaluation data, where each
evaluation task consists of 10 lectures (totally 5
hours). As input features, we used 40 mel-scale
filterbank coefficients, with their first and second
order temporal derivatives to obtain a total of 120-
dimensional feature vector per frame. The encoder
was a 4-layer BLSTM with 320 cells in each layer
and direction, and linear projection layer is fol-
lowed by each BLSTM layer. The 2nd and 3rd

bottom layers of the encoder read every second
hidden state in the network below, reducing the
utterance length by the factor of 4. We used the
content-based attention mechanism (Chorowski
et al., 2015), where the 10 centered convolution
filters of width 100 were used to extract the con-
volutional features. The decoder network was a
1-layer LSTM with 320 cells. The AdaDelta algo-
rithm (Zeiler, 2012) with gradient clipping (Pas-
canu et al., 2012) was used for the optimization.
Dend and M in Eq (18) were set as log 1e−10 and
3, respectively. The hybrid CTC/attention ASR
was implemented by using the Chainer deep learn-
ing toolkit (Tokui et al., 2015).

Table 1 first compares the character error rate
(CER) for conventional attention and MTL based
end-to-end ASR without the joint decoding. λ in
Eq. (7) was set to 0.1. When decoding, we man-
ually set the minimum and maximum lengths of
output sequences by 0.025 and 0.15 times input
sequence lengths, respectively. The length penalty
γ in Eq. (11) was set to 0.1. Multitask learning
(MTL) significantly outperformed attention-based
ASR in the all evaluation tasks, which confirms
the effectiveness of a hybrid CTC/attention archi-
tecture. Table 1 also shows that joint decoding,
described in Section 3.2, further improved the per-
formance without setting any search parameters
(maximum and minimum lengths, length penalty),
but only setting a weight parameter λ = 0.1 in
Eq. (15) similar to the MTL case. Figure 2 also
compares the dependency of λ on the CER for the
CSJ evaluation tasks, and showing that λ was not
so sensitive to the performance if we set λ around
the value we used at MTL (i.e., 0.1).

We also compare the performance of the
proposed MTL-large, which has a larger net-
work (5-layer encoder network), with the con-
ventional state-of-the-art techniques obtained by
using linguistic resources. The state-of-the-art
CERs of GMM discriminative training and DNN-
sMBR/HMM systems are obtained from the Kaldi
recipe (Moriya et al., 2015) and a system based on
syllable-based CTC with MAP decoding (Kanda
et al., 2016). The Kaldi recipe systems use aca-
demic lectures (236h) for AM training and all
training-data transcriptions for LM training. Un-
like the proposed method, these methods use lin-
guistic resources including a morphological an-
alyzer, pronunciation dictionary, and language
model. Note that since the amount of training

524



Table 1: Character error rate (CER) for conventional attention and hybrid CTC/attention end-to-end
ASR. Corpus of Spontaneous Japanese speech recognition (CSJ) task.

Model Hour Task1 Task2 Task3
Attention 581 11.4 7.9 9.0
MTL 581 10.5 7.6 8.3
MTL + joint decoding (rescoring) 581 10.1 7.1 7.8
MTL + joint decoding (one pass) 581 10.0 7.1 7.6
MTL-large + joint decoding (rescoring) 581 8.4 6.2 6.9
MTL-large + joint decoding (one pass) 581 8.4 6.1 6.9
GMM-discr. (Moriya et al., 2015) 236 for AM, 581 for LM 11.2 9.2 12.1
DNN/HMM (Moriya et al., 2015) 236 for AM, 581 for LM 9.0 7.2 9.6
CTC-syllable (Kanda et al., 2016) 581 9.4 7.3 7.5

6.0	  

7.0	  

8.0	  

9.0	  

10.0	  

11.0	  

12.0	  

13.0	  

0.0	   0.1	   0.2	   0.3	   0.4	   0.5	   0.6	   0.7	   0.8	   0.9	   1.0	  

Ch
ar
ac
te
r	  E

rr
or
	  R
at
e	  
(%

)	  

CTC	  weight	  

Task1	   Task2	   Task3	  

Figure 2: The effect of weight parameter λ in
Eq. (14) on the CSJ evaluation tasks (The CERs
were obtained by one-pass decoding).

data and experimental configurations of the pro-
posed and reference methods are different, it is
difficult to compare the performance listed in the
table directly. However, since the CERs of the
proposed method are superior to those of the best
reference results, we can state that the proposed
method achieves the state-of-the-art performance.

4.2 Mandarin telephone speech

We demonstrated ASR experiments on HKUST
Mandarin Chinese conversational telephone
speech recognition (MTS) (Liu et al., 2006).
It has 5 hours recording for evaluation, and
we extracted 5 hours from training data as a
development set, and used the rest (167 hours)
as a training set. All experimental conditions
were same as those in Section 4.1 except that
we used the λ = 0.5 in training and decoding
instead of 0.1 based on our preliminary investi-
gation and 80 mel-scale filterbank coefficients
with pitch features as suggested in (Miao et al.,
2016). In decoding, we also added a result of
the coverage-term based decoding (Chorowski
and Jaitly, 2016), as discussed in Section 3.2

(η = 1.5, τ = 0.5, γ = −0.6 for attention model
and η = 1.0, τ = 0.5, γ = −0.1 for MTL),
since it was difficult to eliminate the irregular
alignments during decoding by only tuning the
maximum and minimum lengths and length
penalty (we set the minimum and maximum
lengths of output sequences by 0.0 and 0.1 times
input sequence lengths, respectively and set
γ = 0.6 in Table 2).

Table 2 shows the effectiveness of MTL and
joint decoding over the attention-based approach,
especially showing the significant improvement of
the joint CTC/attention decoding. Similar to the
CSJ experiments in Section 4.1, we did not use
the length-penalty term or the coverage term in
joint decoding. This is an advantage of joint de-
coding over conventional approaches that require
many tuning parameters. We also generated more
training data by linearly scaling the audio lengths
by factors of 0.9 and 1.1 (speed perturb.). The fi-
nal model achieved 29.9% without using linguistic
resources, which defeats moderate state-of-the-art
systems including CTC-based methods2.

4.3 Decoding speed

We evaluated the speed of the joint decoding meth-
ods described in Section 3.2.3. ASR decoding was
performed with different beam widths of 1, 3, 5,
10, and 20, and the processing time and CER were
measured using a computer with Intel(R) Xeon(R)
processors, E5-2690 v3, 2.6 GHz. Although the
processors were multicore CPUs and the computer
had GPUs, we ran the decoding program as a

2 Although the proposed method did not reach the perfor-
mance obtained by a time delayed neural network (TDNN)
with lattice-free sequence discriminative training (Povey
et al., 2016), our recent work scored 28.0%, and outper-
formed the lattice-free MMI result with advanced network
architectures.

525



Table 2: Character error rate (CER) for conventional attention and hybrid CTC/attention end-to-end
ASR. HKUST Mandarin Chinese conversational telephone speech recognition (MTS) task.

Model dev eval
Attention 40.3 37.8
MTL 38.7 36.6
Attention + coverage 39.4 37.6
MTL + coverage 36.9 35.3
MTL + joint decoding (rescoring) 35.9 34.2
MTL + joint decoding (one pass) 35.5 33.9
MTL-large (speed perturb.) + joint decoding (rescoring) 31.1 30.1
MTL-large (speed perturb.) + joint decoding (one pass) 31.0 29.9
DNN/HMM – 35.9
LSTM/HMM (speed perturb.) – 33.5
CTC with language model (Miao et al., 2016) – 34.8
TDNN/HMM, lattice-free MMI (speed perturb.) (Povey et al., 2016) – 28.2

single-threaded process on a CPU to investigate its
basic computational cost.

Table 3: RTF versus CER for the one-pass and
rescoring methods.

Beam Rescoring One passTask
width RTF CER RTF CER

1 0.66 10.9 0.66 10.7
CSJ 3 1.11 10.3 1.02 10.1

Task1 5 1.50 10.2 1.31 10.0
10 2.46 10.1 2.07 10.0
20 5.02 10.1 3.76 10.0
1 0.68 37.1 0.65 35.9

HKUST 3 0.89 34.9 0.86 34.4
Eval set 5 1.04 34.6 1.03 34.2

10 1.55 34.4 1.50 34.0
20 2.66 34.2 2.55 33.9

Table 3 shows the relationships between the
real-time factor (RTF) and the CER for the CSJ
and HKUST tasks. We evaluated the rescoring and
one-pass decoding methods when using the end
detection in Eq. (18). In every beam width, we
can see that the one-pass method runs faster with
an equal or lower CER than the rescoring method.
This result demonstrates that the one-pass decod-
ing is effective for reducing search errors. Finally,
we achieved 1xRT with one-pass decoding when
using a beam width around 3 to 5, even though it
was a single-threaded process on a CPU. However,
the decoding process has not yet achieved real-
time ASR since CTC and the attention mechanism
need to access all of the frames of the input utter-
ance even when predicting the first label. This is
an essential problem of most end-to-end ASR ap-
proaches and will be solved in future work.

5 Summary and discussion

This paper proposes end-to-end ASR by us-
ing joint CTC/attention decoding, which outper-
formed ordinary attention-based end-to-end ASR
by solving the misalignment issues. The joint de-
coding methods actually reduced most of the ir-
regular alignments, which can be confirmed from
the examples of recognition errors and alignment
plots shown in Appendix C.

The proposed end-to-end ASR does not re-
quire linguistic resources, such as morphological
analyzer, pronunciation dictionary, and language
model, which are essential components of conven-
tional Japanese and Mandarin Chinese ASR sys-
tems. Nevertheless, the method achieved com-
parable/superior performance to the state-of-the-
art conventional systems for the CSJ and MTS
tasks. In addition, the proposed method does not
require GMM/HMM construction for initial align-
ments, DNN pre-training, lattice generation for se-
quence discriminative training, complex search in
decoding (e.g., FST decoder or lexical tree search
based decoder). Thus, the method greatly simpli-
fies the ASR building process, reducing code size
and complexity.

Future work will apply this technique to the
other languages including English, where we have
to solve an issue of long sequence lengths, which
requires heavy computation cost and makes it dif-
ficult to train a decoder network. Actually, neu-
ral machine translation handles this issue by us-
ing a sub word unit (concatenating several letters
to form a new sub word unit) (Wu et al., 2016),
which would be a promising direction for end-to-
end ASR.

526



References
Dario Amodei, Rishita Anubhai, Eric Battenberg, Carl

Case, Jared Casper, Bryan Catanzaro, Jingdong
Chen, Mike Chrzanowski, Adam Coates, Greg Di-
amos, et al. 2015. Deep speech 2: End-to-end
speech recognition in english and mandarin. arXiv
preprint arXiv:1512.02595 .

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
learning to align and translate. arXiv preprint
arXiv:1409.0473 .

Dzmitry Bahdanau, Jan Chorowski, Dmitriy Serdyuk,
Philemon Brakel, and Yoshua Bengio. 2016. End-
to-end attention-based large vocabulary speech
recognition. In IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP).
pages 4945–4949.

Steven Bird. 2006. NLTK: the natural language toolkit.
In Joint conference of the International Committee
on Computational Linguistics and the Association
for Computational Linguistics (COLING/ACL) on
Interactive presentation sessions. pages 69–72.

Hervé Bourlard and Nelson Morgan. 1994. Con-
nectionist speech recognition: A hybrid approach.
Kluwer Academic Publishers.

William Chan, Navdeep Jaitly, Quoc V Le, and Oriol
Vinyals. 2015. Listen, attend and spell. arXiv
preprint arXiv:1508.01211 .

Jan Chorowski, Dzmitry Bahdanau, Kyunghyun Cho,
and Yoshua Bengio. 2014. End-to-end continuous
speech recognition using attention-based recurrent
NN: First results. arXiv preprint arXiv:1412.1602
.

Jan Chorowski and Navdeep Jaitly. 2016. Towards
better decoding and language model integration
in sequence to sequence models. arXiv preprint
arXiv:1612.02695 .

Jan K Chorowski, Dzmitry Bahdanau, Dmitriy
Serdyuk, Kyunghyun Cho, and Yoshua Bengio.
2015. Attention-based models for speech recogni-
tion. In Advances in Neural Information Processing
Systems (NIPS). pages 577–585.

Alex Graves. 2008. Supervised sequence labelling
with recurrent neural networks. PhD thesis, Tech-
nische Universität München .

Alex Graves, Santiago Fernández, Faustino Gomez,
and Jürgen Schmidhuber. 2006. Connectionist
temporal classification: labelling unsegmented se-
quence data with recurrent neural networks. In
International Conference on Machine learning
(ICML). pages 369–376.

Alex Graves and Navdeep Jaitly. 2014. Towards end-
to-end speech recognition with recurrent neural net-
works. In International Conference on Machine
Learning (ICML). pages 1764–1772.

Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl,
Abdel-rahman Mohamed, Navdeep Jaitly, Andrew
Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N
Sainath, et al. 2012. Deep neural networks for
acoustic modeling in speech recognition: The shared
views of four research groups. IEEE Signal Process-
ing Magazine 29(6):82–97.

Frederick Jelinek. 1976. Continuous speech recogni-
tion by statistical methods. Proceedings of the IEEE
64(4):532–556.

Naoyuki Kanda, Xugang Lu, and Hisashi Kawai. 2016.
Maximum a posteriori based decoding for CTC
acoustic models. In Interspeech 2016. pages 1868–
1872.

Suyoun Kim, Takaaki Hori, and Shinji Watanabe.
2017. Joint CTC-attention based end-to-end speech
recognition using multi-task learning. In IEEE In-
ternational Conference on Acoustics, Speech and
Signal Processing (ICASSP). pages 4835–4839.

Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.
2004. Applying conditional random fields to
japanese morphological analysis. In Conference on
Empirical Methods on Natural Language Process-
ing (EMNLP). volume 4, pages 230–237.

Yi Liu, Pascale Fung, Yongsheng Yang, Christopher
Cieri, Shudong Huang, and David Graff. 2006.
HKUST/MTS: A very large scale mandarin tele-
phone speech corpus. In Chinese Spoken Language
Processing, Springer, pages 724–735.

Liang Lu, Xingxing Zhang, and Steve Renals. 2016.
On training the recurrent neural network encoder-
decoder for large vocabulary end-to-end speech
recognition. In IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP).
pages 5060–5064.

Kikuo Maekawa, Hanae Koiso, Sadaoki Furui, and Hi-
toshi Isahara. 2000. Spontaneous speech corpus of
japanese. In International Conference on Language
Resources and Evaluation (LREC). volume 2, pages
947–952.

Yajie Miao, Mohammad Gowayyed, and Florian
Metze. 2015. EESEN: End-to-end speech recogni-
tion using deep RNN models and WFST-based de-
coding. In IEEE Workshop on Automatic Speech
Recognition and Understanding (ASRU). pages
167–174.

Yajie Miao, Mohammad Gowayyed, Xingyu Na, Tom
Ko, Florian Metze, and Alexander Waibel. 2016.
An empirical exploration of ctc acoustic mod-
els. In IEEE International Conference on Acous-
tics, Speech and Signal Processing (ICASSP). pages
2623–2627.

Takafumi Moriya, Takahiro Shinozaki, and Shinji
Watanabe. 2015. Kaldi recipe for Japanese sponta-
neous speech recognition and its evaluation. In Au-
tumn Meeting of ASJ. 3-Q-7.

527



Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio.
2012. On the difficulty of training recurrent neural
networks. arXiv preprint arXiv:1211.5063 .

Daniel Povey, Arnab Ghoshal, Gilles Boulianne, Lukas
Burget, Ondrej Glembek, Nagendra Goel, Mirko
Hannemann, Petr Motlicek, Yanmin Qian, Petr
Schwarz, Jan Silovsky, Georg Stemmer, and Karel
Vesely. 2011. The kaldi speech recognition toolkit.
In IEEE Workshop on Automatic Speech Recogni-
tion and Understanding (ASRU).

Daniel Povey, Vijayaditya Peddinti, Daniel Galvez, Pe-
gah Ghahrmani, Vimal Manohar, Xingyu Na, Yim-
ing Wang, and Sanjeev Khudanpur. 2016. Purely
sequence-trained neural networks for asr based on
lattice-free MMI. In Interspeech. pages 2751–2755.

Hagen Soltau, Hank Liao, and Hasim Sak. 2016. Neu-
ral speech recognizer: Acoustic-to-word lstm model
for large vocabulary speech recognition. arXiv
preprint arXiv:1610.09975 .

Seiya Tokui, Kenta Oono, Shohei Hido, and Justin
Clayton. 2015. Chainer: a next-generation open
source framework for deep learning. In Proceedings
of Workshop on Machine Learning Systems (Learn-
ingSys) in NIPS.

Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V
Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus
Macherey, et al. 2016. Google’s neural ma-
chine translation system: Bridging the gap between
human and machine translation. arXiv preprint
arXiv:1609.08144 .

Nianwen Xue et al. 2003. Chinese word segmentation
as character tagging. Computational Linguistics and
Chinese Language Processing 8(1):29–48.

Matthew D Zeiler. 2012. Adadelta: an adaptive learn-
ing rate method. arXiv preprint arXiv:1212.5701 .

A Location-based attention mechanism

This section provides the equations of a location-
based attention mechanism Attention(·) in Eq. (5).

alt = Attention({al−1}t,ql−1,ht),

where {al−1}t = [al−1,1, · · · , al−1,T ]>. To obtain
alt, we use the following equations:

{ft}t = K ∗ al−1 (19)
elt = g

>tanh(Gqql−1 + Ghht + Gfft + b)
(20)

alt =
exp(etl)∑
t exp(etl)

(21)

K, Gq, Gh, Gf are matrix parameters. b and g are
vector parameters. ∗ denotes convolution along in-
put feature axis twith matrix K to produce feature
{ft}t.

Algorithm 2 CTC hypothesis score
1: function αCTC(h,X)
2: g, c← h . split h into the last label c and the rest g
3: if c = <eos> then
4: return log{γ(n)T (g) + γ

(b)
T (g)}

5: else
6: γ(n)1 (h)←

{
p(z1 = c|X) if g = <sos>
0 otherwise

7: γ(b)1 (h)← 0
8: Ψ← γ(n)1 (h)
9: for t = 2 . . . T do

10: Φ← γ(b)t−1(g) +
{

0 if last(g)=c
γ

(n)
t−1(g) otherwise

11: γ(n)t (h)←
(
γ

(n)
t−1(h) + Φ

)
p(zt = c|X)

12: γ(b)t (h) ←
(
γ

(b)
t−1(h) + γ

(n)
t−1(h)

)
p(zt =

<b>|X)
13: Ψ← Ψ + Φ · p(zt = c|X)
14: end for
15: return log(Ψ)
16: end if
17: end function

B CTC-based hypothesis score

The CTC score αctc(h,X) in Eq. (17) is computed
as shown in Algorithm 2. Let γ(n)t (h) and γ

(b)
t (h)

be the forward probabilities of the hypothesis h
over the time frames 1 . . . t, where the superscripts
(n) and (b) denote different cases in which all
CTC paths end with a nonblank or blank sym-
bol, respectively. Before starting the beam search,
γ

(n)
t () and γ

(b)
t () are initialized for t = 1, . . . , T

as

γ
(n)
t (<sos>) = 0, (22)

γ
(b)
t (<sos>)=

t∏

τ=1

γ
(b)
τ−1(<sos>)p(zτ =<b>|X),

(23)

where we assume that γ(b)0 (<sos>) = 1 and <b>
is a blank symbol. Note that the time index t and
input length T may differ from those of the input
utterance X owing to the subsampling technique
for the encoder (Povey et al., 2016; Chan et al.,
2015).

In Algorithm 2, the hypothesis h is first split
into the last label c and the rest g in line 2. If c
is <eos>, it returns the logarithm of the forward
probability assuming that h is a complete hypothe-
sis in line 4. The forward probability of h is given
by

pctc(h|X) = γ(n)T (g) + γ
(b)
T (g) (24)

according to the definition of γ(n)t () and γ
(b)
t (). If

c is not <eos>, it computes the forward proba-

528



bilities γ(n)t (h) and γ
(b)
t (h), and the prefix proba-

bility Ψ = pctc(h, . . . |X) assuming that h is not
a complete hypothesis. The initialization and re-
cursion steps for those probabilities are described
in lines 6–14. In this function, we assume that
whenever we compute the probabilities γ(n)t (h),
γ

(b)
t (h) and Ψ, the forward probabilities γ

(n)
t (g)

and γ(b)t (g) have already been obtained through
the beam search process because g is a prefix of
h such that |g| < |h|.

C Examples of irregular alignments

We list examples of irregular alignments caused by
attention-based ASR. Figure 3 shows an example
of repetitions of word chunks. The first chunk of
blue characters in attention-based ASR (MTL) is
appeared again, and the whole second chunk part
becomes insertion errors. Figure 4 shows an ex-
ample of deletion errors. The latter half of the
sentence in attention-based ASR (MTL) is bro-
ken, which causes deletion errors. The hybrid
CTC/attention with both multitask learning and
joint decoding avoids these issues. Figures 5 and 6
show alignment plots corresponding to Figs. 3 and
4, respectively, where X-axis shows time frames
and Y-axis shows the character sequence hypoth-
esis. These visual plots also demonstrate that the
proposed joint decoding approach can suppress ir-
regular alignments.

id: (20040717_152947_A010409_B010408-A-057045-057837) 
Reference 
但 是 如 果 你 想 想 如 果 回 到 了 过 去 你 如 果 带 着 这 个 现 在 的 记 
忆 是 不 是 很 痛 苦 啊 
MTL 
Scores: (#Correctness #Substitution #Deletion #Insertion) 28 2 3 45 
但 是 如 果 你 想 想 如 果 回 到 了 过 去 你 如 果 带 着 这 个 现 在 的 节 
如 果 你 想 想 如 果 回 到 了 过 去 你 如 果 带 着 这 个 现 在 的 节 如 果 
你 想 想 如 果 回 到 了 过 去 你 如 果 带 着 这 个 现 在 的 机 是 不 是 很 
・ ・ ・ 
Joint decoding 
Scores: (#Correctness #Substitution #Deletion #Insertion) 31 1 1 0 
HYP:  但 是 如 果 你 想 想 如 果 回 到 了 过 去 你 如 果 带 着 这 个 现 
在 的 ・ 机 是 不 是 很 痛 苦 啊 

Figure 3: Example of insertion errors appeared in
attention-based ASR with MTL and joint decod-
ing.

id: (A01F0001_0844951_0854386) 
Reference 
ま	 た	 え	 飛	 行	 時	 の	 エ	 コ	 ー	 ロ	 ケ	 ー	 シ	 ョ	 ン	 機	 能	 を	 よ	 り	 
詳	 細	 に	 解	 明	 す	 る	 為	 に	 超	 小	 型	 マ	 イ	 ク	 ロ	 ホ	 ン	 お	 よ	 び	 
生	 体	 ア	 ン	 プ	 を	 コ	 ウ	 モ	 リ	 に	 搭	 載	 す	 る	 こ	 と	 を	 考	 え	 て	 
お	 り	 ま	 す	 そ	 う	 す	 る	 こ	 と	 に	 よ	 っ	 て 

MTL 
Scores: (#Correctness #Substitution #Deletion #Insertion) 30 0 47 0 
ま	 た	 え	 飛	 行	 時	 の	 エ	 コ	 ー	 ロ	 ケ	 ー	 シ	 ョ	 ン	 機	 能	 を	 よ	 り	 
詳	 細	 に	 解	 明	 す	 る	 為	 ・	 ・	 ・	 ・	 ・	 ・	 ・	 ・	 ・	 ・	 ・	 ・	 ・	 
・	 ・	 ・	 ・	 ・	 ・	 ・	 ・	 ・	 ・	 ・	 ・	 ・	 ・	 ・	 ・	 ・	 ・	 ・	 ・	 ・	 
・	 ・	 ・	 ・	 ・	 ・	 ・	 ・	 ・	 ・	 に	 ・	 ・	 ・ 

Joint decoding 
Scores: (#Correctness #Substitution #Deletion #Insertion) 67 9 1 0 
ま	 た	 え	 飛	 行	 時	 の	 エ	 コ	 ー	 ロ	 ケ	 ー	 シ	 ョ	 ン	 機	 能	 を	 よ	 り	 
詳	 細	 に	 解	 明	 す	 る	 為	 に	 長	 国	 型	 マ	 イ	 ク	 ロ	 ホ	 ン	 お	 ・	 い	 
く	 声	 単	 位	 方	 を	 コ	 ウ	 モ	 リ	 に	 登	 載	 す	 る	 こ	 と	 を	 考	 え	 て	 
お	 り	 ま	 す	 そ	 う	 す	 る	 こ	 と	 に	 よ	 っ	 て	 

Figure 4: Example of deletion errors appeared in
attention-based ASR with MTL and joint decod-
ing.

(a) MTL (b) Joint decoding

Figure 5: Example of alignments includ-
ing insertion errors in attention-based ASR
with MTL and joint decoding (Utterance
id: 20040717 152947 A010409 B010408-A-
057045-057837).

(a) MTL (b) Joint decoding

Figure 6: Example of alignments includ-
ing deletion errors in attention-based ASR
with MTL and joint decoding (Utterance id:
A01F0001 0844951 0854386).

529


	Joint CTC/attention decoding for end-to-end speech recognition

