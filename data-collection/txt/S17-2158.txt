



















































FORGe at SemEval-2017 Task 9: Deep sentence generation based on a sequence of graph transducers


Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 920–923,
Vancouver, Canada, August 3 - 4, 2017. c©2017 Association for Computational Linguistics

FORGe at SemEval-2017 Task 9: Deep sentence generation based on a
sequence of graph transducers

Simon Mille 1, Roberto Carlini 1, Alicia Burga 1, Leo Wanner 1,2
1 Universitat Pompeu Fabra, Roc Boronat 138, 08018 Barcelona, Spain

2Institució Catalana de Recerca i Estudis Avançats (ICREA),
Lluis Companys 23, 08010 Barcelona, Spain
firstname.lastname@upf.edu

Abstract

We present the contribution of Universitat
Pompeu Fabra’s NLP group to the Sem-
Eval Task 9.2 (AMR-to-English Genera-
tion). The proposed generation pipeline
comprises: (i) a series of rule-based graph-
transducers for the syntacticization of the
input graphs and the resolution of mor-
phological agreements, and (ii) an off-the-
shelf statistical linearization component.

1 Setup of the system

The generator we presented for Task 9.2 of Sem-
Eval is a pipeline of graph transducers called
Fabra Open Rule-based Generator (FORGe).1 It
is built upon work presented, e.g., in (Bohnet,
2006; Wanner et al., 2010). It can be also con-
sidered an extended rule-based version of (Balles-
teros et al., 2015). The current generator has been
mainly developed on the dependency Penn Tree-
bank (Johansson and Nugues, 2007) automatically
converted to predicate-argument structures, and
adapted to the AMR inputs using SemEval’s train-
ing and evaluation sets.

1.1 Overview of the pipeline
The core of the generator is rule-based: graph
transduction grammars convert, in several steps,
abstract AMRs into syntactic structures that con-
tain all the morphological features needed to re-
trieve the final forms of all the words. The syntac-
tic structures are then linearized with an off-the-
shelf tool, and finally the final forms of the words
are retrieved. Our generator follows the theoreti-
cal model of the Meaning-Text Theory (Mel’čuk,
1988); the names of the intermediate layers used
in Table 1 come from the MTT terminology.

1A slightly updated version of the submitted system
can be found at https://www.upf.edu/web/taln/
resources

Step Layermtt #rul.
0 Conversion of AMRs format ConS N/A

into CoNLL’09 format
1 Mapping of AMRs onto SemS 190

predicate-argument graphs
2 Assignment of parts SemSpos 96

of speech
3 Derivation of deep syntactic DSyntS 267

structure
4 Introduction of function SSyntS 294

words
5 Resolution of agreements DMorphS 85
6 Linearization SMorphS N/A
7 Retrieval of surface forms Text 1
8 Post-processing Textfinal 4

Table 1: Overview of the AMR-to-text pipeline.

1.2 Input format conversion
Since our generator cannot read the provided
format, we converted the input AMRs to the
CoNLL’09 format (Hajič et al., 2009). We as-
sume that each sentence in the original file has
two components: a three-line comment with some
metadata (id, date, original sentence, etc.) and
the AMR tree. In order to map the AMR tree
to CoNLL, we assume that: (i) each node of the
tree will be defined as either (a) slash-separated
variable-value pair, (b) only the variable name, or
(c) only the value, and (ii) each branch will be de-
fined by a relation name preceded by a colon.

2 Generation from AMRs

There are 932 activated graph-transduction rules
in the pipeline; Steps 0 and 1 are AMR-specific,
while in the rest of the grammars, only one rule is
AMR-specific rule (at Step 3).

2.1 Mapping of AMRs onto
predicate-argument graphs

The mapping produces graphs that contain lin-
guistic information only, which includes mean-
ing bearing units and the following types of roles:

920



Arguments and coordinations (ARG0, ARG1, . . . ,
ARGn), relations coming from AMR non-core
and prepositional roles (NonCore), and underspe-
cified relations (Elaboration). Information that
does not need to be generated in the final sen-
tence is excluded: ontological information such
as types of entities (e.g., “person”, “date-entity”)
and relations (e.g., “has-rel-role”, “has-org-role”,
“month”); meta-information such as the origin of
a label (e.g., “Wiki”); etc. For this purpose, we re-
move the corresponding nodes and relations and
connect the remaining nodes. Furthermore, the
AMR core roles are maintained, while all other
roles (non-core, prepositional, coordinations) are
mapped to one single role or to a subgraph that
contains one node and two edges, as, e.g., in the
following example:

N1 N2 → N1 Npurpose N2
purpose

ARG1 ARG2

Since AMRs are provided in a tree format, there
are some reversed argumental relations (ARG0-of,
ARG1-of, etc.). For the sake of a consistent treat-
ment of all argumental relations, these relations
are inverted back in our setting and the “-of” ex-
tension is removed.2

2.2 Assignment of parts of speech

Before building the syntactic structures, we assign
parts of speech to each node of the structure. This
tagging is very rudimentary and slow: rules check
in a lexicon if a node label matches an entry and
retrieve the part of speech from that entry. Since
more than one part of speech is often possible for
one string, rules consult the context in the graph,
if necessary, in order to take a decision.

2.3 Derivation of the deep syntactic structure

This transduction performs a top-down recursive
syntacticization of the semantic graph. It looks for
the syntactic root of the sentence, and from there
for its syntactic dependent(s), for the dependent(s)
of the dependent(s), and so on.

The rules are organized in a cluster of three
grammars. The purpose of the first two grammars
is to identify the root of a syntactic tree in case
the original input structure does not contain one.3

2We are conscious that this transformation makes the task
more challenging. For instance, it sometimes creates cycles
in the graphs. Specific rules address this issue.

3Given that AMRs are provided in tree format, and the
root of this tree is the main node (the root) of the sentence,
this step can seem redundant. But since during Step 1 some

Obviously, verbs are the best candidates for being
the root; we pay special attention to the number
and complexity of dependents that a verb has (a
“heavier” verb is more likely to be a root), and to
the fact that a verb is not on the ARG2 side of a
node that comes from a non-core relation, since
this configuration makes syntacticization less nat-
ural in many cases. Consider the following AMR
graph:

I go have-condition you go
ARG0

ARG1
ARG2

ARG0

In this example, if the first “go” is chosen as the
root, the syntactic tree can unfold as “I go if you
go”. But if the second “go”, which is on the ARG2
side of the “have-condition” non-core node, is se-
lected as the root, the generator is currently unable
to build a sentence, since it should be able to invert
the “if” and has no such information.4

For this task, the main challenge is to produce
a well-formed tree that covers as much of the in-
put graph as possible, while avoiding the possible
dependency conflicts. One major issue to solve
is that one element can (legitimately) be the argu-
ment of several predicates, whereas (i) a syntac-
tic structure is a tree-like structure in which every
node can have only one governor, (ii) some pred-
icates need their argument(s) in syntax, and (iii)
some predicates cannot be realized with their ar-
gument(s). In other words, each argument has to
be assigned to the correct governor in syntax, and
duplicated when needed. The rules keep track of
whether a node has already been used and what it
has been used for.5

In the following example, “peek” is chosen as
the root, “dog” is its dependent, and the other pred-
icates of which “dog” was an argument become its
syntactic dependents. Furthermore, “dog” is du-
plicated in order to create a relative clause (Left:
predicate-argument; Right: Deep-Syntax):

he peek dog black bark he peek dog black bark dog

ARG0 ARG1 ARG1
ARG0

I II ATTR

ATTR
I

nodes are removed (see 2.1), it is possible that the original
root is missing, and the system needs to find a substitute.
More generally, we want our system to be able to generate
without any information about syntactic structure.

4Even for a human it is challenging to generate a natural
sentence; maybe something like “The fact that you go would
be the condition for me to go” would be acceptable.

5For more details on the deep-syntactic structures and
their relation with surface-syntactic structures, see (Balles-
teros et al., 2016).

921



2.4 Introduction of function words
The next step towards the realization of the sen-
tence is the introduction of all idiosyncratic words
and of a fine-grained (surface-)syntactic structure
that gives enough information for linearizing and
resolving agreements between the different words.
For this task, we use a valency (subcategoriza-
tion) lexicon built automatically from PropBank
(Kingsbury and Palmer, 2002) and NomBank
(Meyers et al., 2004); see (Mille and Wanner,
2015). For instance, the entry corresponding to
“peek” would contain the following information:

peek VB 01{pos=VB gp={II={prep=at, rel=IOBJ}}}
It indicates that, according to PropBank, the

second argument of “peek” needs the preposition
“at”. Hence, this preposition is introduced in the
surface-syntactic structure, as shown in the follow-
ing example:

he peek at dog the black bark that

SBJ IOBJ PMOD NMOD
NMOD

NMOD

SBJ

AMRs are underspecified in terms of tense, as-
pect, number, and definiteness. For the task, a past
progressive is equally correct as a simple present.
Our generator is able to introduce all types of aux-
iliaries and/or modals, but we set the default verbal
form to simple present. By default, nominal num-
ber is set to singular and definiteness to indefinite.
However, the corresponding rule can take different
decisions: in some cases, the presence of a def-
inite determiner is usually more adequate–for in-
stance, if the noun is modified by a relative clause
(cf. the above example). During this transduction,
anaphora are resolved, and personal pronouns are
introduced in the tree (this includes possessive,
relative and personal pronouns). Some syntac-
tic post-processing rules fix ill-formed structures
when possible.

2.5 Resolution of morpho-syntactic
agreements

Every word must be assigned all necessary mor-
phological information; some information comes
from the deeper strata of the pipeline (as, e.g., ver-
bal tense or finiteness), but some features come
from some other elements of the tree. In English,
for instance, verbs get their person and number
from their subject. In order to resolve these agree-
ments, the rules for this transduction check the
governor/dependent pairs, together with the syn-
tactic relation that links them together. During this
step, parts of speech are set to match the training

data used for the linearizer, and question and ex-
clamation marks are introduced.

2.6 Linearization
The surface-syntactic structures are linearized
with an off-the-shelf tool used in the first SRST
(Belz et al., 2011), a statistical tree linearizer
that orders bottom-up each head and its children
(Bohnet et al., 2011).

2.7 Retrieval of surface forms
With the morpho-syntactic information at
hand for each word, we just need to find the
corresponding surface form. We match the
triple <lemma><POS><morpho-syntactic
features> with an entry of a morphological
dictionary and simply replace the triple by
the surface form. In order to build such a
dictionary, we analyzed a large amount of
data6 and retrieved all possible combinations of
lemma, part of speech and morphological fea-
tures; e.g., for verbs: peek<VB><GER>=peeking;
peek<VB><FIN><PRES><3><SG>=peeks. If sev-
eral surface forms are found for a combination of
features, we keep the most frequent one. The final
sentence corresponding to the running example
would be He peeks at the black dog that barks.

2.8 Post-processing
A few post-processing rules make the output more
readable: the first letter of a sentence is converted
to upper case, punctuation signs are added, under-
scores are replaced by spaces, and spaces before
contracted elements (“’s” and “n’t”) are removed.

3 Results and discussion
Metric FORGe Average
Win pct 43.64 36.668
win+tie pct 57.43 54.19
trueskill 44.9 40.664
BLEU 4.74 11.362

Table 2: Results of the task

It takes in average about 2 seconds to generate
one sentence, 1.7 seconds of which is due to PoS
assignment. Table 2 shows the results that our sys-
tem obtained for the task, compared to the average
results of the other systems; the first three met-
rics are derived from the ranking obtained during
the human evaluation; BLEU is the result of an
n-gram based automatic evaluation. According to

6For this, we used the Open ANC corpus http://www.
anc.org/data/oanc/annotations/

922



the organizers of the task, trueskill (highlighted in
bold) is considered the “main” metric for this task.

Our system obtained results slightly above av-
erage for the human-based evaluation, while the
BLEU score is quite low, compared to that of the
other systems. We believe that this is due to the
fact that we prioritized the quality of the output
over coverage: the submitted generator produces
an output for 98.8% of the sentences, but the lat-
ter only contain 74.3% of the total of nodes con-
tained in the predicate-argument graphs (see Sec-
tion 2.1). Indeed, at two points of the pipeline, we
choose not to generate or to remove content from
the trees. First, during Step 3, when the generator
is not sure what to do with one predicate-argument
relation, it does not generate it. For instance, when
the root of a sentence is on the ARG2 side of a
node which originates from a non-core relation,
the whole subgraph of this node is omitted (see
Section 2.3). Second, during Step 4, if a subtree
that is likely to be faulty is identified (as, e.g., a
conjunction without a complement), it is removed.

Consider for example the following (partial)
gold output: Securing Reiss’ release has been a diplo-
matic priority for France, with President Nicolas Sarkozy

raising the case with other leaders. Our fallback whole-
coverage generator would output diplomacy France
prioritize secure like release Clotilde Reiss president Nico-

las Sarkozy raise case with onbehalfof person intervene...,
while our actual output is France prioritizes to secure
the release of Clotilde Reiss. Eventually, choosing the
production of readable sentences over their com-
pleteness allowed us to get better human ratings.
But this, combined to the fact that we always gen-
erate simple present tense and singular words, nat-
urally has a negative impact with an n-gram-based
metric (that includes a brevity penalty) such as
BLEU.7

Finally let us point out that most rules in this
system are language-independent. Experiments
adapting it to Spanish, Polish and German show
that with rich lexicons, the effort related to gram-
mars for obtaining well-formed texts is minimal.8

Acknowledgments
The work described in this paper has been par-
tially funded by the European Commission under

7Note that our fallback generator obtained a BLEU that is
only 18% inferior to the score of the submitted output.

8Currently, for indicative purposes, the approximate count
of non language-independent rules is the following: 2% in
Step 2, 14% in Step 3, 64% in Step 4, and 13% in Step 5.

the contract numbers FP7-ICT-610411, H2020-
645012-RIA, H2020-700024-RIA, and H2020-
DRS-01-2015.

References
Miguel Ballesteros, Bernd Bohnet, Simon Mille, and

Leo Wanner. 2015. Data-driven sentence genera-
tion with non-isomorphic trees. In Proceedings of
NAACL:HLT . ACL, Denver, CO, pages 387–397.

Miguel Ballesteros, Bernd Bohnet, Simon Mille, and
Leo Wanner. 2016. Data-driven deep-syntactic de-
pendency parsing. Natural Language Engineering
22(6):939–974.

Anja Belz, Mike White, Dominic Espinosa, Eric Kow,
Deirdre Hogan, and Amanda Stent. 2011. The first
Surface Realisation Shared Task: Overview and
evaluation results. In Proceedings of ENLG. Nancy,
France, pages 217–226.

Bernd Bohnet. 2006. Textgenerierung durch Transduk-
tion linguistischer Strukturen. Ph.D. thesis, Univer-
sity Stuttgart.

Bernd Bohnet, Simon Mille, Benoı̂t Favre, and Leo
Wanner. 2011. StuMaBa: From deep representation
to surface. In Proceedings of ENLG. Nancy, France,
pages 232–235.

J. Hajič, M. Ciaramita, and R. Johansson et al. 2009.
The CoNLL-2009 Shared Task: Syntactic and se-
mantic dependencies in multiple languages. In Pro-
ceedings of CoNLL. Boulder, CO, USA, pages 1–18.

Richard Johansson and Pierre Nugues. 2007. Extended
constituent-to-dependency conversion for English.
In Poceedings of NODALIDA. Tartu, Estonia, pages
105–112.

Paul Kingsbury and Martha Palmer. 2002. From Tree-
Bank to PropBank. In Proceedings of LREC. Las
Palmas, Canary Islands, Spain, pages 1989–1993.

Igor Mel’čuk. 1988. Dependency Syntax: Theory and
Practice. SUNY Press, Albany.

Adam Meyers, Ruth Reeves, Catherine Macleod,
Rachel Szekely, Veronika Zielinska, Brian Young,
and Ralph Grishman. 2004. The NomBank Project:
An interim report. In Proceedings of the Workshop
on Frontiers in Corpus Annotation, (HLT/NAACL).
Boston, MA, USA, pages 24–31.

Simon Mille and Leo Wanner. 2015. Towards large-
coverage detailed lexical resources for data-to-text
generation. In Proceedings of the First International
Workshop on D2T Generation. Edinburgh, Scotland.

Leo Wanner, Bernd Bohnet, Nadjet Bouayad-Agha,
François Lareau, and Daniel Nicklaß. 2010. MAR-
QUIS: Generation of user-tailored multilingual air
quality bulletins. Applied Artificial Intelligence
24(10):914–952.

923


