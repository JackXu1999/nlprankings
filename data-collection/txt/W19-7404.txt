

















































Automatic Detection and Classification of Argument Components using
Multi-task Deep Neural Network

Jean-Christophe Mensonides, Sébastien Harispe, Jacky Montmain
LGI2P, IMT Mines Ales,

Ales, France
firstname.lastname@mines-ales.fr

Véronique Thireau
CHROME, Université de Nı̂mes,

Nı̂mes, France
veronique.thireau@unimes.fr

Abstract

In this article we propose a novel method for
automatically extracting and classifying argu-
ment components from raw texts. We intro-
duce a multi-task deep learning framework ex-
ploiting weight parameters trained on auxil-
iary simple tasks, such as Part-Of-Speech tag-
ging or chunking, in order to solve more com-
plex tasks that require a fine-grained under-
standing of natural language. Interestingly,
our results show that the use of advanced deep
learning techniques framed in a multi-task set-
ting enables competing with state-of-the-art
systems that depend on handcrafted features.

1 Introduction

Argumentation consists in a set of methods aim-
ing at making an interlocutor adhere to an intro-
duced point of view or conclusion -or at least to
increase its adherence to the latter. In its sim-
plest form, argumentation is a reasoning process
selecting and structuring premises to attack or de-
fend a given conclusion. Although the study of
argumentation is well established in fields such as
Logic, Philosophy, or Linguistics, automatic ex-
traction and analysis of arguments from natural
language, commonly referred as Argument Min-
ing, is a relatively new research area. Advances
in Argument Mining are of major importance for
Natural Language Understanding and Processing,
in particular due to their several direct applications
and relationships with other tasks, e.g. fake news
detection, knowledge base enrichment and popu-
lation, source trustworthiness estimation. To date,
most advanced argument mining systems aims at
generating argument-based graphs identifying and
structuring premises, claims and conclusion from
raw texts (Cabrio and Villata, 2018). They can

usually be split into sequences of subtasks in-
cluding argument detection and argument linking
(Lippi and Torroni, 2016). This article focuses on
analyzing argumentative micro-structure, i.e., how
different argumentative components interact with
each other within a single text.

Three types of argument components are often
used in the annotation scheme considered in Argu-
ment mining: (i) major claims, reflecting the au-
thor’s standpoint on the debated topic, (ii) claims,
which are statements needing further justifications
to be accepted, and (iii) premises, which are jus-
tifications used in order to make a claim stand
(Stab and Gurevych, 2017). Those components
are further structured into a directly acyclic graph
in which nodes account for argumentative compo-
nents and edges account for oriented links between
them. Directed edges in the graph are labeled ei-
ther as a support or attack relationship, and are
only allowed a) from a premise to another premise,
b) from a premise to a (major) claim, and c) from
a claim to a (major) claim. Such an annotation
schema is adopted by (Stab and Gurevych, 2017)
in the Argument Annotated Essays corpus (version
2) composed of 402 manually annotated student
essays taken from essayforum.com.

A pipeline of treatments is generally applied to
automatically obtain the graph reflecting the un-
derlying argumentative structure of an essay. The
following intuitive decomposition involving four
subtasks is moreover often considered in prac-
tice (Stab and Gurevych, 2017): (1) argument
components identification, (2) argument compo-
nents classification, (3) assessment of the exis-
tence of directed edges between argument com-
ponents, and (4) tagging of the existing directed
edges either as a support or as an attack relation-



ship. This article will focus on solving subtasks
(1) and (2). It presents a novel multi-task ap-
proach to argument mining that does not require
handcrafted features as input. We are in particu-
lar interested by evaluating if recent deep learn-
ing techniques, such as recurrent neural network
mixed with multi-task learning, can compete with
traditional approaches based on handcrafted fea-
tures.

The paper is organized as follows. Section 2
introduces an overview of related work proposed
on tasks similar to (1) and (2). Section 3 intro-
duces our approach to solve those two tasks. Sec-
tion 4 describes the training details of the proposed
model. Section 5 describes our experiments and
results. Section 6 provides directions and perspec-
tives for future works.

2 Related work

Argument components detection consists in deter-
mining the boundaries separating the textual units
carrying arguments from the rest of the text. This
task is generally considered as a supervised text
segmentation problem at word level. Models ex-
ploiting the sequential aspect of texts, inherent
in the construction of a convincing argumenta-
tion, seem particularly adapted and are often used.
(Madnani et al., 2012) used a CRF (Conditional
Random Field) to identify non-argumentative seg-
ments within dissertations. (Levy et al., 2014)
identified the boundaries of textual units detail-
ing conclusions which were supporting or attack-
ing topics discussed in threads from Wikipedia.
(Ajjour et al., 2017) used LSTM (Long short-
term memory, recurrent neural network) to ex-
tract arguments from essays, editorials, and from
user-generated comments. (Goudas et al., 2014)
first identified sentences containing arguments and
then detected their boundaries within social me-
dia using a CRF. (Sardianos et al., 2015) deter-
mined argument components boundaries in news
articles using also using a CRF. Similarly, (Stab
and Gurevych, 2017) used a CRF to extract argu-
ment components in essays. (Eger et al., 2017)
leveraged deep learning techniques to extract ar-
guments from raw texts.

Determining the type of argument compo-
nents (premise, conclusion, etc.) has often been
treated as a supervised text classification prob-
lem. (Eckle-Kohler et al., 2015) distinguished
premises and conclusions in news articles using

Naive Bayes, Random Forest and SVM (Support
Vector Machine). (Park and Cardie, 2014) also
used a SVM to determine the extent to which
claims are justified in citizen’s comments related
to possible new legislation projects. (Stab and
Gurevych, 2017) classified argumentative compo-
nents into premises, claims and major claims in
essays using a SVM. (Persing and Ng, 2016) used
maximum entropy classification to determine the
type of argument components. (Potash et al.,
2016) used sequence-to-sequence recurrent neu-
ral networks to infer the type of argument com-
ponents.

Multi-tasks models are able to handle several
different problems by sharing a subset of shared
parameters. They have been subject to recent
interest within the Natural Language Processing
community (Hashimoto et al., 2016; Søgaard and
Goldberg, 2016; Eger et al., 2017; Yang et al.,
2016). This type of models is bio-inspired: human
beings are able to carry out a multitude of different
tasks and can exploit, when necessary, knowledge
related to different types of problems, making the
learning of new tasks faster and easier. (Ruder,
2017) states the reasons why this type of model is
effective from a machine learning point of view:
the use of several different corpora induces an im-
plicit increase in the number of examples available
during the training phase. In addition, the model
has to look for characteristics which may be useful
for all the tasks to be processed, which limits the
noise modeling and thus, leads to a better general-
ization.

(Søgaard and Goldberg, 2016) showed that in-
ducing a priori knowledge in a multi-task model,
by ordering the tasks to be learned, leads to better
performance. (Yang et al., 2016) have shown that
driving a multi-task and multi-language model can
improve performance on problems where data is
only partially annotated. (Hashimoto et al., 2016)
obtained competitive results on several different
tasks with a single model. However, we should
note that there is no guarantee on the benefits of
using multi-task models, and that their success de-
pends on the data distribution related to the various
problems treated (Mou et al., 2016; Alonso and
Plank, 2016; Bingel and Søgaard, 2017). (Schulz
et al., 2018) proposed a multi-task framework to
perform end-to-end argument mining. The result
they obtained are very promising. In this paper,
we are interested in leveraging auxiliary informa-



Figure 1: Overview of the model architecture (Layer-
wise). POS, ACD and ACC are respectively acronyms
for Part-Of-Speech, Argument Components Detection
and Argument Components Classification.

tions such as Part-Of-Speech and Chunking tags
in a multi-task learning setup, in order to perform
argument component detection and classification.

3 Proposed approach

We propose a model which aims at 1) determining
argument components boundaries within a set of
essays and 2) determining the type of each argu-
ment component within the latter essays. We draw
inspiration from the work of (Hashimoto et al.,
2016) and opt for a multi-task model without the
definition of handcrafted features. Specifically,
we use deep learning techniques and develop a
model that performs Part-Of-Speech (POS) tag-
ging, chunking, argument components boundaries
detection, and argument components classifica-
tion. An overview of the model architecture is
given in Figure 1. The different layers used in the
architecture are described below.

3.1 Word embeddings

We first use a word embedding layer, assigning
a vector representation et to each word wt given
in input to the system. We use GloVe (Penning-
ton et al., 2014) to obtain a set of pre-trained em-
beddings in an unsupervised fashion1. Note that,
word embeddings are continually optimized while
training the model on the different tasks described
below. Out-of-vocabulary words are mapped to a
special <UNK> token.

1We used pre-trained embeddings from https://nlp.
stanford.edu/projects/glove/.

3.2 POS tagging

The second layer of the model corresponds to
a POS tagging task, consisting in assigning a
POS tag (noun, verb, adjective, etc.) to each
word wt given in input to the system. We use a
bi-directional Gated Recurrent Unit (GRU) (Cho
et al., 2014) to encode input word sequences.

GRU is a recurrent neural network using a gat-
ing mechanism and avoiding the use of a separate
memory cell. At each time step t, GRU computes
the hidden state ht as follows:

ht = (1− zt) ∗ nt + zt ∗ h(t−1)

with

nt = tanh(Wnxt + bn + rt ∗ (Whnh(t−1) + bhn))

rt = σ(Wrxt + br +Whrh(t−1) + bhr)

zt = σ(Wzxt + bz +Whzh(t−1) + bhz)

where xt is the input at time step t, rt, zt and nt
are respectively the reset, update and new gates, σ
is the sigmoid function, and W and b are matrice
and vector parameters.

In order to exploit the past and future con-
texts of an element from a sequence of N in-
puts [x1, x1, ..., xN ], we construct a bi-directional
encoding by concatenating the hidden states ob-
tained with a forward encoding (e.g, at time step
t = 1, the input is x1, at time step t = 2, the input
is x2, etc.) and a backward encoding (e.g, at time
step t = 1, the input is xN , at time step t = 2, the
input is xN−1, etc.):

−→
ht =

−−−→
GRU(xt), t ∈ [1, N ]

←−
ht =

←−−−
GRU(xt), t ∈ [N, 1]

ht = [
−→
ht ;
←−
ht ]

We use word embeddings as input of the POS
tagging layer:

−−→
h
(1)
t =

−−−→
GRU(et)

←−−
h
(1)
t =

←−−−
GRU(et)

h
(1)
t = [

−−→
h
(1)
t ;
←−−
h
(1)
t ]

https://nlp.stanford.edu/projects/glove/
https://nlp.stanford.edu/projects/glove/


Then for each time step t we compute the prob-
ability of assigning the label k to the word wt as
follows:

p(y
(1)
t = k|h

(1)
t ) =

exp(Wsm(1)fc
(1)
t + bsm(1))∑

c1
exp(Wsm(1)fc

(1)
t + bsm(1))

(1)

fc
(1)
t = relu(Wfc(1)h

(1)
t + bfc(1)) (2)

where W and b are parameter matrices and vec-
tors, ReLU is the Rectified Linear Unit function
(Nair and Hinton, 2010), and c1 is the set of possi-
ble POS tags.

3.3 Chunking

Chunking is a task aiming at assigning a chunk
tag (noun phrase, verb phrase, etc.) to each word.
We compute hidden chunking states by exploiting
what the model has learned for the POS tagging
task:

−−→
h
(2)
t =

−−−→
GRU([et;h

(1)
t ; y

(POS)
t ])

←−−
h
(2)
t =

←−−−
GRU([et;h

(1)
t ; y

(POS)
t ])

h
(2)
t = [

−−→
h
(2)
t ;
←−−
h
(2)
t ]

where h(1)t is the hidden state obtained at time
step t on the POS tagging task and y(POS)t is
the weighted POS label embedding. Following
(Hashimoto et al., 2016), y(POS)t is defined as fol-
lows:

y
(POS)
t =

card(c1)∑
j=1

p(y
(1)
t = j|h

(1)
t )l(j) (3)

where l(j) is an embedding of the j-th POS tag.
POS tag embeddings are pre-trained using GloVe.

The probability to assign a chunk tag to each
word is then computed in a similar way to the one
for POS tags (eq. (1) and (2)), but with a set of
parameters specific to the chunking layer.

3.4 Argument Components Detection (ACD)

Argument components detection aims at delimit-
ing the boundaries of each argument component
within essays at the word level. We follow (Stab

and Gurevych, 2017) and treat this task as a super-
vised text segmentation problem, where labels fol-
low an IOB-tagset (Ramshaw and Marcus, 1999):
the first word of each argument component car-
ries an ”Arg-B” label, remaining words of said ar-
gument component bear an ”Arg-I” tag, and the
words not belonging to any argument component
bear an ”O” tag.

Each essay is considered as a single word se-
quence which we encode as follows:

−−→
h
(3)
t =

−−−→
GRU([et;h

(1)
t ; y

(POS)
t ;h

(2)
t ; y

(chunk)
t ])

←−−
h
(3)
t =

←−−−
GRU([et;h

(1)
t ; y

(POS)
t ;h

(2)
t ; y

(chunk)
t ])

h
(3)
t = [

−−→
h
(3)
t ;
←−−
h
(3)
t ]

where y(chunk)t is the weighted chunk label em-
bedding, computed in a similar way as the one for
POS labels (eq. (3)).

The probability to assign a chunk tag to a word
is then computed in a similar way as the one for
POS labels, but with a set of parameters specific
the ACD layer.

3.5 Argument Components Classification
(ACC)

Argument components classification aims at deter-
mining the type of each argument component be-
tween premise, claim and major claim. We treat
this task as a segment labeling problem. We con-
sider that a segment can be the sequence of words
belonging to a same argument component or can
be the sequence of words belonging to a same por-
tion of continuous text whose words do not belong
to an argument component. The notion of segment
is illustrated in Figure 2.

We encode each segment si, i ∈ [1, L] as fol-
lows:

−→
hit =

−−−→
GRU([eit;h

(1)
it ; y

(POS)
it ;h

(2)
it ; y

(chunk)
it ])

←−
hit =

←−−−
GRU([eit;h

(1)
it ; y

(POS)
it ;h

(2)
it ; y

(chunk)
it ])

hit = [
−→
hit;
←−
hit]

where it is the time step t for the segment si.



[S1] The greater our goal is, the more competi-
tion we need. [S2] Take Olympic games which is
a form of competition for instance, it is hard to
imagine how an athlete could win the game with-
out the training of his or her coach, and the help
of other professional staffs such as the people who
take care of his diet, and those who are in charge
of the medical care. [S3] The winner is the athlete
but the success belongs to the whole team. There-
fore [S4] without the cooperation, there would be
no victory of competition [S5] .
Consequently, no matter from the view of individ-
ual development or the relationship between com-
petition and cooperation we can receive the same
conclusion that [S6] a more cooperative attitudes
towards life is more profitable in one’s success.

Figure 2: Excerpt from an essay of the corpus illustrat-
ing the notion of segments. Text regions underlined by
a solid line are premises, those underlined by a dashed
line are claims, and the bold regions are major claims.
Segment numbers [S#] were added as indications. The
first segment is the region from the beginning of the text
to the first premise. The second segment corresponds
to the first premise. The third segment is the not un-
derlined region between the first premise and the first
claim, and so on.

In order to help the model focusing on the most
important markers (such as “I firmly believe that”
or “we can receive the same conclusion that”)
we use an attention mechanism (Bahdanau et al.,
2014), which in addition allows us to synthesize
the information carried by segments hidden states
into fixed size vectors:

uit = tanh(Watthit + batt)

αit =
exp(uᵀituatt)∑
t exp(u

ᵀ
ituatt)

shi =
∑
t

αithit

where Watt, batt and uatt are respectively param-
eter matrices, bias and vectors.

Then we encode each essay using the synthetic
segments hidden states shi:

−−→
h
(4)
j =

−−−→
GRU(shi), i ∈ [1, L]

←−−
h
(4)
j =

←−−−
GRU(shi), i ∈ [L, 1]

h
(4)
j = [

−−→
h
(4)
j ;
←−−
h
(4)
j ]

The probability of assigning a label to each seg-
ment is then computed similarly to that for POS
tags, but with a set of parameters specific to the
ACC layer.

4 Model Training

For each epoch of training, we optimized the
model’s parameters for each layer. That is, at each
epoch we trained the layers in the following or-
der: POS tagging, chunking, ACD and ACC. In
order to assess the relevance of implementing a
multi-task model, we trained two versions of the
model: a version where we trained every layer (re-
ferred as “w/ POS & chunking”), and a version for
which we voluntary omitted to train the POS tag-
ging and chunking layers (referred as “w/o POS &
chunking”). The training details of each layer are
described below.

4.1 POS tagging layer

Following (Hashimoto et al., 2016), we denote
θPOS = (WPOS , bPOS , θe) the set of parameters
involved in the POS tagging layer. WPOS repre-
sents the set of parameter matrices for the POS
tagging layer, bPOS the set of biases of the POS
tagging layer, and θe the set of parameters of the
words embedding layer. The cost function is de-
fined as:

J (1) = −
∑
s

∑
t

log p(y
(1)
t = k|h

(1)
t )

+λ ‖WPOS‖2 + δ
∥∥θe − θ′e∥∥2

where p(y(1)t = k|h
(1)
t ) is the probability of as-

signing the right label k to the wordwt of the word
sequence s, λ ‖WPOS‖2 is the L2 regularization
term and δ ‖θe − θ′e‖

2 is a secondary regulariza-
tion term. λ and δ are hyperparameters.

The secondary regularization term aims at stabi-
lizing the training by preventing θe from being too
specifically optimized to fit the POS tagging task.
Indeed, since θe is shared across all layers of the
model, excessive modifications of its parameters
would prevent the model from learning efficiently.
θ′e is the set of parameters involved in the word
embedding layer at last epoch.

4.2 Chunking layer

We denote θchunk = (Wchunk, bchunk, EPOS , θe)
the set of parameters involved in the chunking



layer. Wchunk et bchunk are respectively param-
eter matrices and bias of the chunking layer, in-
cluding those of θPOS . EPOS is the set of parame-
ters characterizing the POS label embeddings. The
cost function is defined as follows:

J (2) = −
∑
s

∑
t

log p(y
(2)
t = k|h

(2)
t )

+λ ‖Wchunking‖2 + δ
∥∥θPOS − θ′POS∥∥2

with p(y(2)t = k|h
(2)
t ) the probability of assign-

ing the right label k to the word wt of the word
sequence s. θ′POS is the set of parameters of the
POS tagging layer right before the training of the
chunking layer for the current epoch.

4.3 ACD layer
We denote θACD the set of parameters in-
volved in the ACD layer, with θACD =
(WACD, bACD, EPOS , Echunk, θe). WACD and
bACD are respectively parameter matrices and bias
of the ACD layer, including those of the chunk-
ing and POS tagging layers. Echunk is the set of
parameters characterizing the chunk label embed-
dings. The cost is defined as follows:

J (3) = −
∑
d

∑
t

log p(y
(3)
t = k|h

(3)
t )

+λ ‖WACD‖2 + δ
∥∥θchunk − θ′chunk∥∥2

with p(y(3)t = k|h
(3)
t ) the probability of assign-

ing the right label k to the word wt of the essay
d. θ′chunk is the set of parameters of the chunking
layer right before the training of the ACD layer for
the current epoch.

4.4 ACC layer
We denote θACC the set of parameters in-
volved in the ACC layer, with θACC =
(WACC , bACC , EPOS , Echunk, θe). WACC and
bACC are respectively parameter matrices and bias
of the ACC layer, including those of the chunking
and POS tagging layers. The cost function is de-
fined as follows:

J (4) = −
∑
d

∑
i

log p(y
(4)
i = k|sh

(4)
i )

+λ ‖WACC‖2 + δ
∥∥θchunk − θ′chunk∥∥2

with p(y(4)i = k|sh
(4)
i ) is the probability of assign-

ing the right label k to the segment si of the essay
d.

5 Experiments and discussion

5.1 Hyperparameters and training corpora

5.1.1 Optimization
We trained the model alternating, for each epoch,
the layers to be trained in the following order:
POS tagging, chunking, ACD and ACC. We used
Adam (Kingma and Ba, 2014) as learning algo-
rithm, with β1 = 0.9, β2 = 0.999 and � = 10−8.
The learning rate was shared across all layers and
was fixed to 10−3 at the beginning of the training,
and then multiplied by 0.75 every 10 epochs. In
order to limit the gradient exploding problem, we
used a gradient clipping strategy (Pascanu et al.,
2013). We followed (Hashimoto et al., 2016) and
used a clipping value of min(3.0, depth), where
depth stands for the number of bi-GRU involved
in the trained layer.

5.1.2 Parameters initialization
In order to smooth the backpropagation of the gra-
dient during the training phase, we used random
orthogonal matrices as initial weights for parame-
ter matrices of every GRU, as suggested by (Saxe
et al., 2013). The remaining parameter matrices
were initialized with values drawn from a gaussian

N (0,
√

2

nin
), with nin being the number of input

neurons in the layer, as proposed by (He et al.,
2015). Bias vectors were initialized as zero vec-
tors.

5.1.3 Vector dimensions used
We used 50 dimensional vectors for the words and
labels embeddings. We used 100 dimensional vec-
tors for the hidden states of every GRU in the
model.

5.1.4 Regularization
Following (Hashimoto et al., 2016), we used λ =
10−6 for the parameter matrices of every GRU and
λ = 10−5 for the remaining parameter matrices.
The secondary regularization term rate δ was set
to 10−2 for each layer. We also used Dropout (Sri-
vastava et al., 2014) on every layer, with a proba-
bility to affect neurons set to 0.2.

5.1.5 POS tagging and chunking corpora
We used the corpora from the shared task CoNLL-
2000 (Sang and Buchholz, 2000) and the associ-
ated labels to train the POS tagging and chunking
layers.



5.1.6 ACD and ACC corpora

We used the Argument Annotated Essays (version
2) corpora released by (Stab and Gurevych, 2017)
following the train/test split given to train the ACD
and ACC layers.

5.1.7 Training termination criteria

For a mono-task model, a common practice is to
stop the training right before overfitting. However,
it is not clear to determine when to stop the train-
ing when dealing with multi-task models since it
is possible that the model overfits only on a sub-
set of the target tasks. Thus, we decided to stop
the training when the model overfitted both on the
ACD and the ACC tasks on a held-out validation
set2. The reported results are the best we obtained
for each task on the test set, before overfitting on
the validation set. Note that hyperparameters can
be chosen so that the model overfits roughly at the
same time on the ACD and ACC tasks.

5.1.8 Simple ACC

We denote Simple ACC the ACC task with the fol-
lowing modification: every segment correspond-
ing to an argument component was treated as a sin-
gle special token<EMPTY>. We hypothesize that
this transformation will prevent the model from
focusing on words inside argument components,
but rather on its context, thus allowing a better
generalization process.

5.2 Results and discussion

We report the obtained results on the test data for
the tasks ACC, ACD and Simple ACC in Table 1.
The column ”w/o POS & chunking” refers to the
model version for which we omitted the training
of the POS tagging and chunking layers, while the
column ”w/ POS & chunking” refers to the model
version optimized for every tasks. As a baseline,
we use the human performances3 and the results
reported by (Stab and Gurevych, 2017), shown in
Table 2.

2We randomly sampled 10% essays from the training data
to build the validation set.

3Human performance corresponds the average perfor-
mances reached by human agents, as presented in (Stab and
Gurevych, 2017).

Table 1: Macros f1-scores obtained on the ACC, ACD
and Simple ACC tasks.

Task w/o POS &
chunking

w/ POS &
chunking

ACD 0.5922 0.8870
ACC 0.6950 0.7257

Simple
ACC

0.7670 0.7980

Table 2: F1-scores reported on the ACD and ACC tasks
by Stab and Gurevych (Stab and Gurevych, 2017) and
human agents.

Task F1-score from (Stab
and Gurevych, 2017)

Human
f1-score

ACD 0.867 0.886
ACC 0.826 0.868

5.2.1 General performance discussion
We obtained a macro f1-score of 0.8870 on ACD
with the ”w/ POS & chunking” model version,
reaching human performance. This result was
obtained without using handcrafted features, and
is comparable to the one reported in (Stab and
Gurevych, 2017). Regarding the ACC task, we
obtained a macro f1-score of 0.7980 with Simple
ACC for the ”w/ POS & chunking” version, rep-
resenting 96.6 % of the performance obtained in
(Stab and Gurevych, 2017) and 91.9% of the hu-
man performance.

5.2.2 Simple ACC assessment
We consider that the words composing argument
components are not really relevant to determine if
they are major claims, claims or premises. Hence,
we hypothesize that focusing on those words will
lead to model noise. Conversely, the context sur-
rounding argument components should be a good
indicator: sequences such as ”we can receive the
same conclusion that” seem to be strong indica-
tors that the upcoming argument component is not
a premise. This could explain the gap in perfor-
mance obtained between ACC and Simple ACC,
more particularly for the ”w/ POS & chunking”
version, with respective f1-scores of 0.7257 and
0.7980 (9.96% performance increase).

5.2.3 Multi-task framework assessment
Regarding the tasks ACD and Simple ACC, we
obtained f1-scores of 0.5922 and 0.7670 for the



”w/o POS & chunking” version and of 0.8870 and
0.7980 for the ”w/ POS & chunking”, represent-
ing respectively a 49.78% and a 4.1% performance
gain. Those results show the benefits of using a
multi-task framework and suggests that more sub-
tasks could be added to the model.

6 Upcoming work and perspectives

The results we got are encouraging and could
probably be improved, particularly by analyzing
optimal hyperparameters in a deeper way. The
performance difference between the ”w/ POS &
chunking” and ”w/o POS & chunking” models
tends to show that implementing more auxiliary
tasks could be beneficial. One exploration way
could be to insert a dependency parsing layer on
top of the chunking layer, as done in (Hashimoto
et al., 2016).

In order to implement a complete argument
mining system, as introduced by (Stab and
Gurevych, 2017), we plan to implement layers
which enable to automatically generate argument
graphs. Therefore, it is necessary to determine if
a directed edge exists between each ordered pair
of argument components, and also to label those
edges either as support or as attack relationships.

7 Conclusion

This article introduced the use of a novel model
based on a multi-task framework for automatically
extracting and classifying argument components
from raw texts. Interestingly, our results show that
the use of advanced deep learning techniques en-
ables competing with state-of-the-art systems that
depend on handcrafted features. The variation of
performance between the model exploiting aux-
iliary tasks (POS tagging and chunking) and a
version skipping those tasks clearly promotes the
added-value of a multi-task framework.

References
Yamen Ajjour, Wei-Fan Chen, Johannes Kiesel, Hen-

ning Wachsmuth, and Benno Stein. 2017. Unit seg-
mentation of argumentative texts. In Proceedings of
the 4th Workshop on Argument Mining, pages 118–
128.

Héctor Martı́nez Alonso and Barbara Plank. 2016.
When is multitask learning effective? semantic se-
quence prediction under varying data conditions.
arXiv preprint arXiv:1612.02251.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
learning to align and translate. arXiv preprint
arXiv:1409.0473.

Joachim Bingel and Anders Søgaard. 2017. Identi-
fying beneficial task relations for multi-task learn-
ing in deep neural networks. arXiv preprint
arXiv:1702.08303.

Elena Cabrio and Serena Villata. 2018. Five years of
argument mining: a data-driven analysis. In IJCAI,
pages 5427–5433.

Kyunghyun Cho, Bart Van Merriënboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using rnn encoder-decoder
for statistical machine translation. arXiv preprint
arXiv:1406.1078.

Judith Eckle-Kohler, Roland Kluge, and Iryna
Gurevych. 2015. On the role of discourse markers
for discriminating claims and premises in argumen-
tative discourse. In Proceedings of the 2015 Con-
ference on Empirical Methods in Natural Language
Processing, pages 2236–2242.

Steffen Eger, Johannes Daxenberger, and Iryna
Gurevych. 2017. Neural end-to-end learning
for computational argumentation mining. arXiv
preprint arXiv:1704.06104.

Theodosis Goudas, Christos Louizos, Georgios Peta-
sis, and Vangelis Karkaletsis. 2014. Argument ex-
traction from news, blogs, and social media. In
Hellenic Conference on Artificial Intelligence, pages
287–299. Springer.

Kazuma Hashimoto, Caiming Xiong, Yoshimasa Tsu-
ruoka, and Richard Socher. 2016. A joint many-task
model: Growing a neural network for multiple nlp
tasks. arXiv preprint arXiv:1611.01587.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2015. Delving deep into rectifiers: Surpass-
ing human-level performance on imagenet classifi-
cation. In Proceedings of the IEEE international
conference on computer vision, pages 1026–1034.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Ran Levy, Yonatan Bilu, Daniel Hershcovich, Ehud
Aharoni, and Noam Slonim. 2014. Context depen-
dent claim detection. In Proceedings of COLING
2014, the 25th International Conference on Compu-
tational Linguistics: Technical Papers, pages 1489–
1500.

Marco Lippi and Paolo Torroni. 2016. Argumenta-
tion mining: State of the art and emerging trends.
ACM Transactions on Internet Technology (TOIT),
16(2):10.



Nitin Madnani, Michael Heilman, Joel Tetreault, and
Martin Chodorow. 2012. Identifying high-level or-
ganizational elements in argumentative discourse.
In Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 20–28. Association for Computational Lin-
guistics.

Lili Mou, Zhao Meng, Rui Yan, Ge Li, Yan Xu,
Lu Zhang, and Zhi Jin. 2016. How transferable are
neural networks in nlp applications? arXiv preprint
arXiv:1603.06111.

Vinod Nair and Geoffrey E Hinton. 2010. Rectified
linear units improve restricted boltzmann machines.
In Proceedings of the 27th international conference
on machine learning (ICML-10), pages 807–814.

Joonsuk Park and Claire Cardie. 2014. Identifying
appropriate support for propositions in online user
comments. In Proceedings of the first workshop on
argumentation mining, pages 29–38.

Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio.
2013. On the difficulty of training recurrent neural
networks. In International conference on machine
learning, pages 1310–1318.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 confer-
ence on empirical methods in natural language pro-
cessing (EMNLP), pages 1532–1543.

Isaac Persing and Vincent Ng. 2016. End-to-end ar-
gumentation mining in student essays. In Proceed-
ings of the 2016 Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
1384–1394.

Peter Potash, Alexey Romanov, and Anna Rumshisky.
2016. Here’s my point: Joint pointer architecture for
argument mining. arXiv preprint arXiv:1612.08994.

Lance A Ramshaw and Mitchell P Marcus. 1999. Text
chunking using transformation-based learning. In
Natural language processing using very large cor-
pora, pages 157–176. Springer.

Sebastian Ruder. 2017. An overview of multi-task
learning in deep neural networks. arXiv preprint
arXiv:1706.05098.

Erik F Sang and Sabine Buchholz. 2000. Introduc-
tion to the conll-2000 shared task: Chunking. arXiv
preprint cs/0009008.

Christos Sardianos, Ioannis Manousos Katakis, Geor-
gios Petasis, and Vangelis Karkaletsis. 2015. Argu-
ment extraction from news. In Proceedings of the
2nd Workshop on Argumentation Mining, pages 56–
66.

Andrew M Saxe, James L McClelland, and Surya Gan-
guli. 2013. Exact solutions to the nonlinear dynam-
ics of learning in deep linear neural networks. arXiv
preprint arXiv:1312.6120.

Claudia Schulz, Steffen Eger, Johannes Daxenberger,
Tobias Kahse, and Iryna Gurevych. 2018. Multi-
task learning for argumentation mining in low-
resource settings. arXiv preprint arXiv:1804.04083.

Anders Søgaard and Yoav Goldberg. 2016. Deep
multi-task learning with low level tasks supervised
at lower layers. In Proceedings of the 54th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 2: Short Papers), pages 231–235.

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: a simple way to prevent neural networks
from overfitting. The journal of machine learning
research, 15(1):1929–1958.

Christian Stab and Iryna Gurevych. 2017. Parsing ar-
gumentation structures in persuasive essays. Com-
putational Linguistics, 43(3):619–659.

Zhilin Yang, Ruslan Salakhutdinov, and William
Cohen. 2016. Multi-task cross-lingual se-
quence tagging from scratch. arXiv preprint
arXiv:1603.06270.


