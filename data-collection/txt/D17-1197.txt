



















































Reference-Aware Language Models


Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1850–1859
Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics

Reference-Aware Language Models

Zichao Yang1∗, Phil Blunsom2,3, Chris Dyer1,2, and Wang Ling2
1Carnegie Mellon University, 2DeepMind, and 3University of Oxford

zichaoy@cs.cmu.edu, {pblunsom,cdyer,lingwang}@google.com

Abstract

We propose a general class of language
models that treat reference as discrete
stochastic latent variables. This decision
allows for the creation of entity mentions
by accessing external databases of refer-
ents (required by, e.g., dialogue genera-
tion) or past internal state (required to ex-
plicitly model coreferentiality). Beyond
simple copying, our coreference model
can additionally refer to a referent using
varied mention forms (e.g., a reference to
“Jane” can be realized as “she”), a charac-
teristic feature of reference in natural lan-
guages. Experiments on three representa-
tive applications show our model variants
outperform models based on deterministic
attention and standard language modeling
baselines.

1 Introduction

Referring expressions (REs) in natural language
are noun phrases (proper nouns, common nouns,
and pronouns) that identify objects, entities, and
events in an environment. REs occur frequently
and they play a key role in communicating infor-
mation efficiently. While REs are common in nat-
ural language, most previous work does not model
them explicitly, either treating REs as ordinary
words in the model or replacing them with special
tokens that are filled in with a post processing step
(Wen et al., 2015; Luong et al., 2015). Here we
propose a language modeling framework that ex-
plicitly incorporates reference decisions. In part,
this is based on the principle of pointer networks
in which copies are made from another source
(Gülçehre et al., 2016; Gu et al., 2016; Ling et al.,

∗Work completed at DeepMind.

dialogue

recipe

coref

M: the nirala is a 
nice restuarant

1) soy milk
2) leaves
3) banana

Blend soy milk and …

[I]1 think…Go ahead [Linda]2 … thanks goes to [you]1 …

a) reference to a list

b) reference to a table

c) reference to document context

the nirala moderate lebanese

ali baba moderate indian

name price food

Figure 1: Reference-aware language models.

2016; Vinyals et al., 2015; Ahn et al., 2016; Mer-
ity et al., 2016). However, in the full version of our
model, we go beyond simple copying and enable
coreferent mentions to have different forms, a key
characteristic of natural language reference.

Figure 1 depicts examples of REs in the con-
text of the three tasks that we consider in this
work. First, many models need to refer to a list
of items (Kiddon et al., 2016; Wen et al., 2015).
In the task of recipe generation from a list of
ingredients (Kiddon et al., 2016), the generation
of the recipe will frequently refer to these items.
As shown in Figure 1, in the recipe “Blend soy
milk and . . . ”, soy milk refers to the ingredi-
ent summaries. Second, reference to a database is
crucial in many applications. One example is in
task oriented dialogue where access to a database
is necessary to answer a user’s query (Young et al.,
2013; Li et al., 2016; Vinyals and Le, 2015; Wen
et al., 2015; Sordoni et al., 2015; Serban et al.,
2016; Bordes and Weston, 2016; Williams and
Zweig, 2016; Shang et al., 2015; Wen et al., 2016).
Here we consider the domain of restaurant rec-
ommendation where a system refers to restau-
rants (name) and their attributes (address, phone
number etc) in its responses. When the system

1850



says “the nirala is a nice restaurant”, it refers
to the restaurant name the nirala from the
database. Finally, we address references within
a document (Mikolov et al., 2010; Ji et al., 2015;
Wang and Cho, 2015), as the generation of words
will often refer to previously generated words. For
instance the same entity will often be referred to
throughout a document. In Figure 1, the entity
you refers to I in a previous utterance. In this
case, copying is insufficient– although the referent
is the same, the form of the mention is different.

In this work we develop a language model that
has a specific module for generating REs. A se-
ries of decisions (should I generate a RE? If yes,
which entity in the context should I refer to? How
should the RE be rendered?) augment a traditional
recurrent neural network language model and the
two components are combined as a mixture model.
Selecting an entity in context is similar to famil-
iar models of attention (Bahdanau et al., 2014),
but rather than being a soft decision that reweights
representations of elements in the context, it is
treated as a hard decision over contextual elements
which are stochastically selected and then copied
or, if the task warrants it, transformed (e.g., a pro-
noun rather than a proper name is produced as out-
put). In cases when the stochastic decision is not
available in training, we treat it as a latent vari-
able and marginalize it out. For each of the three
tasks, we pick one representative application and
demonstrate our reference aware model’s efficacy
in evaluations against models that do not explicitly
include a reference operation.

Our contributions are as follows:

• We propose a general framework to model
reference in language. We consider refer-
ence to entries in lists, tables, and document
context. We instantiate these tasks into three
specific applications: recipe generation, dia-
logue modeling, and coreference based lan-
guage models.

• We develop the first neural model of refer-
ence that goes being copying and can model
(conditional on context) how to form the
mention.

• We perform comprehensive evaluation of our
models on the three data sets and verify our
proposed models perform better than strong
baselines.

2 Reference-aware language models

Here we propose a general framework for
reference-aware language models.

We denote each document as a series of to-
kens x1, . . . , xL, where L is the number of tokens.
Our goal is to maximize p(xi | ci), the proba-
bility of each word xi given its previous context
ci = x1, . . . , xi−1. In contrast to traditional neu-
ral language models, we introduce a variable zi
at each position, which controls the decision on
which source xi is generated from. Then the con-
ditional probability is given by:

p(xi, zi | ci) = p(xi | zi, ci)p(zi | ci), (1)

where zi has different meanings in different con-
texts. If xi is from a reference list or a database,
then zi is one dimensional and zi = 1 denotes xi is
generated as a reference. zi can also model more
complex decisions. In coreference based language
model, zi denotes a series of sequential decisions,
such as whether xi is an entity, if yes, which entity
xi refers to. When zi is not observed, we will train
our model to maximize the marginal probability
over zi, i.e., p(xi|ci) =

∑
zi

p(xi|zi, ci)p(zi|ci).
2.1 Reference to lists
We begin to instantiate the framework by consid-
ering reference to a list of items. Referring to a
list of items has broad applications, such as gen-
erating documents based on summaries etc. Here
we specifically consider the application of recipe
generation conditioning on the ingredient lists. Ta-
ble. 1 illustrates the ingredient list and recipe for
Spinach and Banana Power Smoothie. We can
see that the ingredients soy milk, spinach
leaves, and banana occur in the recipe.

soy

decoder

list z
Yes No

encoder

Blend

soy

BOS

Blend

1)

2)

3)

Figure 2: Recipe pointer

Let the ingredients of a recipe be X = {xi}Ti=1
and each ingredient contains L tokens xi =

1851



Ingredients Recipe
1 cup plain soy milk Blend soy milk and spinach leaves

together in a blender until smooth. Add
banana and pulse until thoroughly blended.

3/4 cup packed fresh spinach leaves
1 large banana, sliced

Table 1: Ingredients and recipe for Spinach and Banana Power Smoothie.

{xij}Lj=1. The corresponding recipe is y =
{yv}Kv=1. We would like to model p(y|X) =
Πvp(yv|X, y<v).

We first use a LSTM (Hochreiter and Schmid-
huber, 1997) to encode each ingredient: hi,j =
LSTME(WExij , hi,j−1) ∀i. Then, we sum the
resulting final state of each ingredient to obtain the
starting LSTM state of the decoder. We use an at-
tention based decoder:

sv = LSTMD([WEyv−1, dv−1], sv−1),

pcopyv = ATTN({{hi,j}Ti=1}Lj=1, sv),
dv =

∑
ij

pv,i,jhi,j ,

p(zv|sv) = sigmoid(W [sv, dv]),
pvocabv = softmax(W [sv, dv]),

where ATTN(h, q) is the attention function that
returns the probability distribution over the set
of vectors h, conditioned on any input represen-
tation q. A full description of this operation is
described in (Bahdanau et al., 2014). The deci-
sion to copy from the ingredient list or generate
a new word from the softmax is performed us-
ing a switch, denoted as p(zv|sv). We can ob-
tain a probability distribution of copying each of
the words in the ingredients by computing pcopyv =
ATTN({{hi,j}Ti=1}Lj=1, sv) in the attention mech-
anism.
Objective: We can obtain the value of zv through
a string match of tokens in recipes with tokens
in ingredients. If a token appears in the ingre-
dients, we set zv = 1 and zv = 0 otherwise.
We can train the model in a fully supervised fash-
ion, i.e., we can obtain the probability of yv as
p(yv, zv|sv) = pcopyv (yv)p(1|sv) if zv = 1 and
pvocabv (yv)(1− p(1|si,v)) otherwise.

However, it may be not be accurate. In many
cases, the tokens that appear in the ingredients do
not specifically refer to ingredients tokens. For ex-
amples, the recipe may start with “Prepare a cup
of water”. The token “cup” does not refer to the
“cup” in the ingredient list “1 cup plain soy milk”.

To solve this problem, we treat zi as a latent vari-
able, we wish to maximize the marginal probabil-
ity of yv over all possible values of zv. In this way,
the model can automatically learn when to refer to
tokens in the ingredients. Thus, the probability of
generating token yv is defined as:

p(yv|sv) = pvocabv (yv)p(0|sv) + pcopyv (yv)p(1|sv)
= pvocabv (yv)(1− p(1|sv)) + pcopyv (yv)p(1|sv).

If no string match is found for yv, we simply set
p

copy
v (yv) = 0 in the above objective.

2.2 Reference to databases
We then consider the more complicated task of ref-
erence to database entries. Referring to databases
is quite common in question answering and di-
alogue systems, in which databases are external
knowledge and they are resorted to reply users’
query. In our paper, we consider the application
of task-oriented dialogue systems in the domain
of restaurant recommendations. Different from
lists that are one dimensional, databases are two-
dimensional and referring to table entries requires
sophisticated model design.

To better understand the model, we first make
a brief introduction of the data set. We use dia-
logues from the second Dialogue State Tracking
Challenge (DSTC2) (Henderson et al., 2014). Ta-
ble. 3 is one example dialogue from this dataset.

We can observe from this example, users get
recommendations of restaurants based on queries
that specify the area, price and food type of the
restaurant. We can support the system’s deci-
sions by incorporating a mechanism that allows
the model to query the database to find restau-
rants that satisfy the users’ queries. A sample of
our database (refer to data preparation part on how
we construct the database) is shown in Table 2.
We can observe that each restaurant contains 6 at-
tributes that are generally referred in the dialogue
dataset. As such, if the user requests a restau-
rant that serves “indian” food, we wish to train
a model that can search for entries whose “food”

1852



NAME PRICE RANGE FOOD AREA ADDRESS POST CODE PHONE
ali baba moderate lebanese centre 59 Hills Road City Centre CB 2, 1 NT 01462 432565
the nirala moderate indian north 7 Milton Road Chesterton CB 4, 1 UY 01223 360966

Table 2: Fragment of database for dialogue system.

M: Hello, welcome to the Cambridge
restaurant system? You can ask for
restaurants by area, price range
or food type . How may I help you?

U: i want a moderately priced
restaurant in the north town

M: the nirala is a nice restaurant in
the north of town in the moderate price
range

U: what is the address
M: Sure , the nirala is on 7 Milton

Road Chesterton

Table 3: Example dialogue, M stands for Machine
and U stands for User

M U M U

sentence encoder

turn encoder

decoder

attn

Figure 3: Hierarchical RNN Seq2Seq model. The
red box denotes attention mechanism over the ut-
terances in the previous turn.

column contains “indian”. Now, we describe how
we deploy a model that fulfills these requirements.
We first introduce the basic dialogue framework in
which we incorporates the table reference module.
Basic Dialogue Framework: We build a basic
dialogue model based on the hierarchical RNN
model described in (Serban et al., 2016), as in di-
alogues, the generation of the response is not only
dependent on the previous sentence, but on all sen-
tences leading to the response. We assume that
a dialogue is alternated between a machine and a
user. An illustration of the model is shown in Fig-
ure 3.

Consider a dialogue with T turns, the utter-
ances from a user and a machines are denoted
as X = {xi}Ti=1 and Y = {yi}Ti=1 respectively,
where i is the i-th utterance. We define xi =
{xij}|xi|j=1, yi = {yiv}|yi|v=1, where xij (yiv) denotes
the j-th (v-th) token in the i-th utterance from
the user (the machines). The dialogue sequence

starts with a machine utterance and is given by
{y1, x1, y2, x2, . . . , yT , xT }. We would like to
model the utterances from the machine

p(y1, y2, . . . , yT |x1, x2, . . . , xT ) =∏
i

p(yi|y<i, x<i) =
∏
i,v

p(yi,v|yi,<v, y<i, x<i).

We encode y<i and x<i into continuous space
in a hierarchical way with LSTM: Sentence En-
coder: For a given utterance xi, We encode it as
hxi,j = LSTME(WExi,j , h

x
i,j−1). The representa-

tion of xi is given by the hxi = h
x
i,|xi|. The same

process is applied to obtain the machine utterance
representation hyi = h

y
i,|yi|. Turn Encoder: We

further encode the sequence {hy1, hx1 , ..., hyi , hxi }
with another LSTM encoder. We shall refer the
last hidden state as ui, which can be seen as the
hierarchical encoding of the previous i utterances.
Decoder: We use ui−1 as the initial state of de-
coder LSTM and decode each token in yi. We can
express the decoder as:

syi,v = LSTMD(WEyi,v−1, si,v−1),

pyi,v = softmax(Ws
y
i,v).

We can also incoroprate the attetionn mecha-
nism in the decoder. As shown in Figure. 3, we
use the attention mechanism over the utterance in
the previous turn. Due to space limit, we don’t
present the attention based decoder mathmatically
and readers can refer to (Bahdanau et al., 2014) for
details.

2.2.1 Incorporating Table Reference
We now extend the decoder in order to allow the
model to condition the generation on a database.
Pointer Switch: We use zi,v ∈ {0, 1} to denote
the decision of whether to copy one cell from the
table. We compute this probability as follows:

p(zi,v|si,v) = sigmoid(Wsi,v).
Thus, if zi,v = 1, the next token yi,v is generated
from the database, whereas if zi,v = 0, then the
following token is generated from a softmax. We
now describe how we generate tokens from the
database.

1853



qq

attributes

table

z
Yes No

U

decoder Table Pointer

Step 1: attribute attn

Step 3: row attn

Step 5: column
 attn

papa

pcopypcopy pvocabpvocab

pcpc

prpr

Figure 4: Decoder with table pointer.

We denote a table with R rows and C columns
as {tr,c}, r ∈ [1, R], c ∈ [1, C], where tr,c is the
cell in row r and column c. The attribute of each
column is denoted as sc, where c is the c-th at-
tribute. tr,c and sc are one-hot vector.

Table Encoding: To encode the table, we first
build an attribute vector and then an encoding vec-
tor for each cell. The attribute vector is simply an
embedding lookup gc = WEsc. For the encod-
ing of each cell, we first concatenate embedding
lookup of the cell with the corresponding attribute
vector gc and then feed it through a one-layer MLP
as follows: then er,c = tanh(W [WEtr,c, gc]).

Table Pointer: The detailed process of calculating
the probability distribution over the table is shown
in Figure 4. The attention over cells in the table
is conditioned on a given vector q, similarly to the
attention model for sequences. However, rather
than a sequence of vectors, we now operate over a
table.
Step 1: Attention over the attributes to find
out the attributes that a user asks about, pa =
ATTN({gc}, q). Suppose a user says cheap, then
we should focus on the price attribute.
Step 2: Conditional row representation calcula-
tion, er =

∑
c p

a
cer,c ∀r. So that er contains the

price information of the restaurant in row r.
Step 3: Attention over er to find out the
restaurants that satisfy users’ query, pr =
ATTN({er}, q). Restaurants with cheap price
will be picked.
Step 4: Using the probabilities pr, we compute
the weighted average over the all rows ec =∑

r p
r
rer,c. {er} contains the information of

cheap restaurant.
Step 5: Attention over columns {er} to compute
the probabilities of copying each column pc =
ATTN({ec}, q).

Step 6: To get the probability matrix of copying
each cell, we simply compute the outer product
pcopy = pr ⊗ pc.
The overall process is as follows:

pa = ATTN({gc}, q),
er =

∑
c

pacer,c ∀r,

pr = ATTN({er}, q),
ec =

∑
r

prrer,c ∀c,

pc = ATTN({ec}, q),
pcopy = pr ⊗ pc.

If zi,v = 1, we embed the above attention pro-
cess in the decoder by replacing the conditioned
state q with the current decoder state syi,v.
Objective: As in previous task, we can train the
model in a fully supervised fashion, or we can
treat the decision as a latent variable. We can get
p(yi,v|si,v) in a similar way.
2.3 Reference to document context
Finally, we address the references that happen in a
document itself and build a language model that
uses coreference links to point to previous enti-
ties. Before generating a word, we first make the
decision on whether it is an entity mention. If
so, we decide which entity this mention belongs
to, then we generate the word based on that en-
tity. Denote the document as X = {xi}Li=1, and
the entities are E = {ei}Ni=1, each entity has Mi
mentions, ei = {mij}Mij=1, such that {xmij}Mij=1
refer to the same entity. We use a LSTM to
model the document, the hidden state of each to-
ken is hi = LSTM(WExi, hi−1). We use a set
he = {he0, he1, ..., heM} to keep track of the entity
states, where hej is the state of entity j.
Word generation: At each time step before gen-
erating the next word, we predict whether the word
is an entity mention:

pcoref(vi|hi−1, he) = ATTN(he, hi−1),
di =

∑
vi

p(vi)hevi ,

p(zi|hi−1) = sigmoid(W [hi−1, di]),

where zi denotes whether the next word is
an entity and if yes vi denotes which entity
the next word corefers to. If the next word is
an entity mention, then p(xi|vi, hi−1, he) =

1854



um and [I]1 think that is whats - Go ahead [Linda]2. Well and thanks goes to
[you]1 and to [the media]3 to help [us]4...So [our]4 hat is off to all of [you]5...

[I]1um

entity state
update process

I

[Linda]2

I

Linda

[You]1

You

Linda

update statepush state

empty
state

0 0
1

0

1

2

0

1

2

push stateattn
… …

attn

and

[I]1

of

[You]1

new
entity

entity
1

Figure 5: Coreference based language model, example taken from Wiseman et al. (2016).

softmax(W1 tanh(W2[hi−1, hevi ])) else
p(xi|hi−1) = softmax(W1hi−1). Hence,
p(xi|x<i) =

p(xi|hi−1)p(zi|hi−1, he) if zi = 0.
p(xi|vi, hi−1, he)×
pcoref(vi|hi−1, he)p(zi|hi−1, he) if zi = 1.

Entity state update: Since there are multiple
mentions for each entity and the mentions appear
dynamically, we need to keep track of the entity
state in order to use coreference information in en-
tity mention prediction. We update the entity state
he at each time step. In the beginning, he = {he0},
he0 denotes the state of an virtual empty entity and
is a learnable variable. If zi = 1 and vi = 0,
then it indicates the next word is a new entity men-
tion, then in the next step, we append hi to he, i.e.,
he = {he, hi}, if zi = 1 and vi > 0, then we
update the corresponding entity state with the new
hidden state, he[vi] = hi. Another way to update
the entity state is to use one LSTM to encode the
mention states and get the new entity state. Here
we use the latest entity mention state as the new
entity state for simplicity. The detailed update pro-
cess is shown in Figure 5.

Note that the stochastic decisions in this task are
more complicated than previous two tasks. We
need to make two sequential decisions: whether
the next word is an entity mention, and if yes,
which entity the mention corefers to. It is in-
tractable to marginalize these decisions, so we
train this model in a supervised fashion (refer to
data preparation part on how we get coreference
annotations).

3 Experiments

3.1 Data sets and preprocessing
Recipes: We crawled all recipes from www.
allrecipes.com. There are about 31, 000

recipes in total, and every recipe has an ingredi-
ent list and a corresponding recipe. We exclude
the recipes that have less than 10 tokens or more
than 500 tokens, those recipes take about 0.1% of
all data set. On average each recipe has 118 to-
kens and 9 ingredients. We random shuffle the
whole data set and take 80% as training and 10%
for validation and test. We use a vocabulary size
of 10,000 in the model.
Dialogue: We use the DSTC2 data set. We only
use the dialogue transcripts from the data set.
There are about 3,200 dialogues in total. The ta-
ble is not available from DSTC2. To reconstruct
the table, we crawled TripAdvisor for restaurants
in the Cambridge area, where the dialog dataset
was collected. Then, we remove restaurants that
do not appear in the data set and create a database
with 109 restaurants and their attributes (e.g. food
type). Since this is a small data set, we use 5-
fold cross validation and report the average re-
sult over the 5 partitions. There may be multi-
ple tokens in each table cell, for example in Ta-
ble. 2, the name, address, post code and phone
number have multiple tokens, we replace them
with one special token. For the name, address,
post code and phone number of the j-th row, we
replace the tokens in each cell with NAME j,
ADDR j, POSTCODE j, PHONE j. If a table

cell is empty, we replace it with an empty token
EMPTY. We do a string match in the transcript

and replace the corresponding tokens in transcripts
from the table with the special tokens. Each dia-
logue on average has 8 turns (16 sentences). We
use a vocabulary size of 900, including about 400
table tokens and 500 words.
Coref LM: We use the Xinhua News data set from
Gigaword Fifth Edition and sample 100,000 docu-
ments that has length in range from 100 to 500.
Each document has on average 234 tokens, so
there are 23 million tokens in total. We process

1855



the documents to get coreference annotations and
use the annotations, i.e., zi, vi, in training. We take
80% as training and 10% as validation and test re-
spectively. We ignore the entities that have only
one mention and for the mentions that have multi-
ple tokens, we take the token that is most frequent
in the all the mentions for this entity. After prepro-
cessing, tokens that are entity mentions take about
10% of all tokens. We use a vocabulary size of
50,000 in the model.

3.2 Baselines, model training and evaluation
We compare our model with baselines that do not
model reference explicitly. For recipe generation
and dialogue modeling, we compare our model
with basic seq2seq and attention model. We also
apply attention mechanism over the table for di-
alogue modeling as a baseline. For coreference
based language model, we compare our model
with simple RNN language model.

We train all models with simple stochastic gra-
dient descent with gradient clipping. We use a
one-layer LSTM for all RNN components. Hyper-
parameters are selected using grid search based on
the validation set.

Evaluation of our model is challenging since it
involves three rather different applications. We fo-
cus on evaluating the accuracy of predicting the
reference tokens, which is the goal of our model.
Specifically, we report the perplexity of all words,
words that can be generated from reference and
non-reference words. The perplexity is calcu-
lated by multiplying the probability of decision at
each step all together. Note that for non-reference
words, they also appear in the vocabulary. So it is a
fair comparison to models that do not model refer-
ence explicitly. For the recipe task, we also gener-
ate the recipes using beam size of 10 and evaluate
the generated recipes with BLEU. We didn’t use
BLEU for dialogue generation since the database
entries take only a very small part of all tokens in
utterances.

3.3 Results and analysis
The results for recipe generation, dialogue and
coref based language model are shown in Table 4,
5, and 6 respectively. The recipe results in Ta-
ble 4 verifies that modeling reference explicitly
improves performance. Latent and Pointer per-
form better than Seq2Seq and Attn model. The La-
tent model performs better than the Pointer model
since tokens in ingredients that match with recipes

do not necessarily come from the ingredients. Im-
posing a supervised signal gives wrong informa-
tion to the model and hence makes the result
worse. With latent decision, the model learns to
when to copy and when to generate it from the vo-
cabulary.

The findings for dialogue basically follow that
of recipe generation, as shown in Table 5. Con-
ditioning table performs better in predicting table
tokens in general. Table Pointer has the lowest
perplexity for tokens in the table. Since the table
tokens appear rarely in the dialogue transcripts,
the overall perplexity does not differ much and the
non-table token perplexity are similar. With atten-
tion mechanism over the table, the perplexity of
table token improves over basic Seq2Seq model,
but still not as good as directly pointing to cells
in the table, which shows the advantage of mod-
eling reference explicitly. As expected, using sen-
tence attention improves significantly over mod-
els without sentence attention. Surprisingly, Table
Latent performs much worse than Table Pointer.
We also measure the perplexity of table tokens that
appear only in test set. For models other than Ta-
ble Pointer, because the tokens never appear in the
training set, the perplexity is quite high, while Ta-
ble Pointer can predict these tokens much more
accurately. This verifies our conjecture that our
model can learn reasoning over databases.

The coref based LM results are shown in Ta-
ble 6. We find that coref based LM performs much
better on the entity perplexity, but is a little bit
worse for non-entity words. We found it was an
optimization problem and the model was stuck in a
local optimum. So we initialize the Pointer model
with the weights learned from LM, the Pointer
model performs better than LM both for entity per-
plexity and non-entity words perplexity.

In Appendix A, we also visualize the heat map
of table reference and list items reference. The
visualization shows that our model can correctly
predict when to refer to which entries according to
context.

4 Related Work

In terms of methodology, our work is closely re-
lated to previous works that incorporate copying
mechanism with neural models (Gülçehre et al.,
2016; Gu et al., 2016; Ling et al., 2016; Vinyals
et al., 2015). Our models are similar to models
proposed in (Ahn et al., 2016; Merity et al., 2016),

1856



Model
val test

PPL BLEU PPL BLEUAll Ing Word All Ing Word
Seq2Seq 5.60 11.26 5.00 14.07 5.52 11.26 4.91 14.39
Attn 5.25 6.86 5.03 14.84 5.19 6.92 4.95 15.15
Pointer 5.15 5.86 5.04 15.06 5.11 6.04 4.98 15.29
Latent 5.02 5.10 5.01 14.87 4.97 5.19 4.94 15.41

Table 4: Recipe results, evaluated in perplexity and BLEU score. All means all tokens, Ing denotes
tokens from recipes that appear in ingredients. Word means non-table tokens. Pointer and Latent differs
in that for Pointer, we provide supervised signal on when to generate a reference token, while in Latent
it is a latent decision.

Model All Table Table OOV Word

Seq2Seq 1.35±0.01 4.98±0.38 1.99E7±7.75E6 1.23±0.01
Table Attn 1.37±0.01 5.09±0.64 7.91E7±1.39E8 1.24±0.01
Table Pointer 1.33±0.01 3.99±0.36 1360 ± 2600 1.23±0.01
Table Latent 1.36±0.01 4.99±0.20 3.78E7±6.08E7 1.24±0.01
+ Sentence Attn
Seq2Seq 1.28±0.01 3.31±0.21 2.83E9 ± 4.69E9 1.19±0.01
Table Attn 1.28±0.01 3.17±0.21 1.67E7±9.5E6 1.20±0.01
Table Pointer 1.27±0.01 2.99±0.19 82.86±110 1.20±0.01
Table Latent 1.28±0.01 3.26±0.25 1.27E7±1.41E7 1.20±0.01

Table 5: Dialogue perplexity results. Table means tokens from table, Table OOV denotes table tokens
that do not appear in the training set. Sentence Attn denotes we use attention mechanism over tokens in
utterances from the previous turn.

Model val testAll Entity Word All Entity Word
LM 33.08 44.52 32.04 33.08 43.86 32.10
Pointer 32.57 32.07 32.62 32.62 32.07 32.69
Pointer
+ init

30.43 28.56 30.63 30.42 28.56 30.66

Table 6: Coreference based LM. Pointer + init
means we initialize the model with the LM
weights.

where the generation of each word can be condi-
tioned on a particular entry in knowledge lists and
previous words. In our work, we describe a model
with broader applications, allowing us to condi-
tion, on databases, lists and dynamic lists.

In terms of applications, our work is related to
chit-chat dialogue (Li et al., 2016; Vinyals and
Le, 2015; Sordoni et al., 2015; Serban et al.,
2016; Shang et al., 2015) and task oriented dia-
logue (Wen et al., 2015; Bordes and Weston, 2016;
Williams and Zweig, 2016; Wen et al., 2016).
Most of previous works on task oriented dialogues
embed the seq2seq model in traditional dialogue
systems, in which the table query part is not dif-
ferentiable, while our model queries the database
directly. Recipe generation was proposed in (Kid-
don et al., 2016). They use attention mechanism
over the checklists, whereas our work models ex-

plicit references to them. Context dependent lan-
guage models (Mikolov et al., 2010; Jozefowicz
et al., 2016; Mikolov et al., 2010; Ji et al., 2015;
Wang and Cho, 2015) are proposed to capture
long term dependency of text. There are also lots
of works on coreference resolution (Haghighi and
Klein, 2010; Wiseman et al., 2016). We are the
first to combine coreference with language model-
ing, to the best of our knowledge.

5 Conclusion

We introduce reference-aware language models
which explicitly model the decision of from where
to generate the token at each step. Our model
can also learns the decision by treating it as a la-
tent variable. We demonstrate on three applica-
tions, table based dialogue modeling, recipe gen-
eration and coref based LM, that our model per-
forms better than attention based model, which
does not incorporate this decision explicitly. There
are several directions to explore further based on
our framework. The current evaluation method is
based on perplexity and BLEU. In task oriented di-
alogues, we can also try human evaluation to see
if the model can reply users’ query accurately. It
is also interesting to use reinforcement learning to
learn the actions in each step in coref based LM.

1857



References
Sungjin Ahn, Heeyoul Choi, Tanel Pärnamaa, and

Yoshua Bengio. 2016. A neural knowledge lan-
guage model. CoRR, abs/1608.00318.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua
Bengio. 2014. Neural machine translation by
jointly learning to align and translate. CoRR,
abs/1409.0473.

Antoine Bordes and Jason Weston. 2016. Learn-
ing end-to-end goal-oriented dialog. arXiv preprint
arXiv:1605.07683.

Jiatao Gu, Zhengdong Lu, Hang Li, and Victor
O. K. Li. 2016. Incorporating copying mech-
anism in sequence-to-sequence learning. CoRR,
abs/1603.06393.

Çaglar Gülçehre, Sungjin Ahn, Ramesh Nallapati,
Bowen Zhou, and Yoshua Bengio. 2016. Pointing
the unknown words. CoRR, abs/1603.08148.

Aria Haghighi and Dan Klein. 2010. Coreference res-
olution in a modular, entity-centered model. In
Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the
Association for Computational Linguistics, pages
385–393. Association for Computational Linguis-
tics.

Matthew Henderson, Blaise Thomson, and Jason
Williams. 2014. Dialog state tracking challenge 2
& 3.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Yangfeng Ji, Trevor Cohn, Lingpeng Kong, Chris Dyer,
and Jacob Eisenstein. 2015. Document context lan-
guage models. arXiv preprint arXiv:1511.03962.

Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam
Shazeer, and Yonghui Wu. 2016. Exploring
the limits of language modeling. arXiv preprint
arXiv:1602.02410.

Chloé Kiddon, Luke Zettlemoyer, and Yejin Choi.
2016. Globally coherent text generation with neu-
ral checklist models. In Proc. EMNLP.

Jiwei Li, Will Monroe, Alan Ritter, Michel Galley,
Jianfeng Gao, and Dan Jurafsky. 2016. Deep rein-
forcement learning for dialogue generation. In Proc.
EMNLP.

Wang Ling, Edward Grefenstette, Karl Moritz Her-
mann, Tomáš Kočiský, Andrew Senior, Fumin
Wang, and Phil Blunsom. 2016. Latent predictor
networks for code generation. In Proc. ACL.

Thang Luong, Ilya Sutskever, Quoc V. Le, Oriol
Vinyals, and Wojciech Zaremba. 2015. Addressing
the rare word problem in neural machine translation.
In Proceedings of the 53rd Annual Meeting of the

Association for Computational Linguistics and the
7th International Joint Conference on Natural Lan-
guage Processing of the Asian Federation of Natural
Language Processing, ACL 2015, July 26-31, 2015,
Beijing, China, Volume 1: Long Papers, pages 11–
19.

Stephen Merity, Caiming Xiong, James Bradbury, and
Richard Socher. 2016. Pointer sentinel mixture
models. arXiv preprint arXiv:1609.07843.

Tomas Mikolov, Martin Karafiát, Lukas Burget, Jan
Cernockỳ, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In Inter-
speech, volume 2, page 3.

Iulian V Serban, Alessandro Sordoni, Yoshua Bengio,
Aaron Courville, and Joelle Pineau. 2016. Building
end-to-end dialogue systems using generative hier-
archical neural network models. In Proceedings of
the 30th AAAI Conference on Artificial Intelligence
(AAAI-16).

Lifeng Shang, Zhengdong Lu, and Hang Li. 2015.
Neural responding machine for short-text conversa-
tion. arXiv preprint arXiv:1503.02364.

Alessandro Sordoni, Michel Galley, Michael Auli,
Chris Brockett, Yangfeng Ji, Meg Mitchell, Jian-Yun
Nie, Jianfeng Gao, and Bill Dolan. 2015. A neural
network approach to context-sensitive generation of
conversational responses. In Proc. NAACL.

Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly.
2015. Pointer networks. In Proc. NIPS.

Oriol Vinyals and Quoc V. Le. 2015. A neural con-
versational model. In Proc. ICML Deep Learning
Workshop.

Tian Wang and Kyunghyun Cho. 2015. Larger-
context language modelling. arXiv preprint
arXiv:1511.03729.

Tsung-Hsien Wen, Milica Gasic, Nikola Mrksic,
Lina M Rojas-Barahona, Pei-Hao Su, Stefan Ultes,
David Vandyke, and Steve Young. 2016. A network-
based end-to-end trainable task-oriented dialogue
system. arXiv preprint arXiv:1604.04562.

Tsung-Hsien Wen, Milica Gasic, Nikola Mrksic, Pei-
hao Su, David Vandyke, and Steve J. Young. 2015.
Semantically conditioned LSTM-based natural lan-
guage generation for spoken dialogue systems. In
Proc. EMNLP.

Jason D Williams and Geoffrey Zweig. 2016. End-
to-end lstm-based dialog control optimized with su-
pervised and reinforcement learning. arXiv preprint
arXiv:1606.01269.

Sam Wiseman, Alexander M Rush, and Stuart M
Shieber. 2016. Learning global features for corefer-
ence resolution. arXiv preprint arXiv:1604.03035.

1858



Steve Young, Milica Gašić, Blaise Thomson, and Ja-
son D Williams. 2013. Pomdp-based statistical spo-
ken dialog systems: A review. Proceedings of the
IEEE, 101(5):1160–1179.

1859


