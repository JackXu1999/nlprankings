



















































Multi-View Domain Adapted Sentence Embeddings for Low-Resource Unsupervised Duplicate Question Detection


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 1630–1641,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

1630

Multi-View Domain Adapted Sentence Embeddings for
Low-Resource Unsupervised Duplicate Question Detection

Nina Poerner1,2 and Hinrich Schütze1
1Center for Information and Language Processing, LMU Munich, Germany

2Corporate Technology Machine Intelligence (MIC-DE), Siemens AG Munich, Germany
poerner@cis.uni-muenchen.de | inquiries@cislmu.org

Abstract

We address the problem of Duplicate Question
Detection (DQD) in low-resource domain-
specific Community Question Answering fo-
rums. Our multi-view framework MV-DASE
combines an ensemble of sentence encoders
via Generalized Canonical Correlation Anal-
ysis, using unlabeled data only. In our ex-
periments, the ensemble includes generic and
domain-specific averaged word embeddings,
domain-finetuned BERT and the Universal
Sentence Encoder. We evaluate MV-DASE
on the CQADupStack corpus and on addi-
tional low-resource Stack Exchange forums.
Combining the strengths of different encoders,
we significantly outperform BM25, all single-
view systems as well as a recent supervised
domain-adversarial DQD method.

1 Introduction

Duplicate Question Detection is the task of finding
questions in a database that are equivalent to an
incoming query. Many Community Question An-
swering (CQA) forums leave this task to the col-
lective memory of their users. This results in un-
necessary manual work for community members
as well as delayed access to answers (Hoogeveen
et al., 2015).

Automatic DQD is often approached as a super-
vised problem with community-generated training
labels. However, smaller CQA forums may suf-
fer from label sparsity: On Stack Exchange, 50%
of forums have fewer than 160 user-labeled dupli-
cates, and 25% have fewer than 50 (see Figure 1).1

To overcome this problem, two avenues have
been explored: The first is supervised domain-
adversarial training on a label-rich source fo-
rum (Shah et al., 2018), which works best when

1archive.org/details/stackexchange [data
dump: 2018-12-20]

100 101 102 103 104 105 106
#D
#Q

Figure 1: Distribution (log-scale box plot) of number of
questions (#Q) and number of labeled duplicates (#D)
on Stack Exchange. N = 165 forums.

source and target domains are related. The sec-
ond is unsupervised DQD via representation learn-
ing (Charlet and Damnati, 2017; Lau and Baldwin,
2016), which requires only unlabeled questions.
In this paper, we take the unsupervised avenue.

A major challenge in the context of domain-
specific CQA forums is that language usage may
differ from the “generic” domains of existing rep-
resentations. To illustrate this point, compare
the following Nearest Neighbor lists of the word
“tree”, based either on generic GloVe embeddings
(Pennington et al., 2014) or on FastText embed-
dings (Bojanowski et al., 2017) that were trained
on specific CQA forums:

generic (GloVe): trees, branches, leaf
chess: searches, prune, modify
outdoors: trees, trunk, trunks
gis: strtree, rtree, btree
wordpress: trees, hierachy, hierarchial
gaming: trees, treehouse, skills

Charlet and Damnati (2017) and Lau and Bald-
win (2016) report that representations trained on
in-domain data perform better on unsupervised
DQD than generic representations. But in a
low-resource setting, the amount of unlabeled in-
domain data is limited. This can result in low cov-
erage or quality, as illustrated by the in-domain
embedding neighbors of “tree” in the smallest fo-
rum from our dataset:

windowsphone: dreamspark, l535ds, generally

archive.org/details/stackexchange


1631

generic domain-specific

contextualized fG: GloVe
fD: FastText
(in-domain)

noncontextualized fU : USE
fB : BERT

(domain-finetuned)

Table 1: Ensemble used in our experiments.

It is therefore desirable to combine the over-
all quality and coverage of generic representations
with the domain-specificity of in-domain repre-
sentations via multi-view learning. There is a
large body of work on multi-view word embed-
dings (see Section 2.3), including domain adapted
word embeddings (Sarma et al., 2018).

Recent representation learning techniques go
beyond the word level and embed larger contexts
(e.g., sentences) jointly (Peters et al., 2018; De-
vlin et al., 2019; Cer et al., 2018). To reflect this
paradigm shift, we take multi-view representation
learning from the word to the sentence level and
propose MV-DASE (Multi-View Domain Adapted
Sentence Embeddings), a framework that com-
bines an ensemble of sentence encoders via Gen-
eralized Canonical Correlation Analysis (see Sec-
tion 3.1).

MV-DASE uses unlabeled in-domain data
only, making it applicable to the problem of un-
supervised DQD. As a framework, it is agnostic
to the internal specifics of its ensemble. In Sec-
tion 3.2, we describe an ensemble of different sen-
tence encoders: domain-specific and generic, con-
textualized and noncontextualized (see Table 1).
In Sections 4 and 5, we demonstrate that MV-
DASE is effective at duplicate retrieval on the
CQADupStack corpus (Hoogeveen et al., 2015)
and on additional low-resource Stack Exchange
forums. Significance tests show significant gains
over BM25, all single-view systems and domain-
adversarial supervised training as proposed by
Shah et al. (2018). In Sections 6 and 7, we suc-
cessfully evaluate MV-DASE on two additional
benchmarks: the SemEval-2017 DQD shared
task (Nakov et al., 2017) as well as the unsuper-
vised STS Benchmark (Cer et al., 2017).

2 Related Work

2.1 Duplicate Question Detection

Most prior work on DQD (e.g., Bogdanova et al.
(2015); Dos Santos et al. (2015); Baldwin et al.
(2016); Zhang et al. (2017); Rodrigues et al.

(2017); Hoogeveen et al. (2018)) focuses on su-
pervised architectures. As discussed, these ap-
proaches are not applicable to forums with few or
no labeled duplicates.

Shah et al. (2018) tackle label sparsity by
domain-adversarial training (ADA). More specif-
ically, they train a bidirectional Long-Short
Term Memory Network (LSTM) (Hochreiter and
Schmidhuber, 1997) on a label-rich source forum,
while minimizing the distance between source and
target domain representations. Their approach
beats BM25 and a simple transfer baseline in cases
where source and target domain are closely related
(e.g., AskUbuntu→SuperUser), but not on more
distant pairings. This is not ideal, as the existence
of a big related source forum is not guaranteed.

An alternative is unsupervised DQD via rep-
resentation learning, which does not require any
labels. Charlet and Damnati (2017) use a word
embedding-based soft cosine distance for dupli-
cate ranking. In a recent DQD shared task
(SemEval-2017 task 3B, Nakov et al. (2017)),
their best unsupervised system trails the best su-
pervised system by only 2% Mean Average Preci-
sion (MAP). This seems reasonable, given that the
implicit objective of many representation learning
methods (similar representations for similar ob-
jects) is closely related to the notion of a duplicate.

Charlet and Damnati (2017) report overall
better results when embeddings are trained on
domain-specific data rather than Wikipedia. How-
ever, they make no attempts to combine the two
domains. Lau and Baldwin (2016) evaluate two
representation learning techniques (doc2vec (Le
and Mikolov, 2014) and word2vec (Mikolov et al.,
2013a)) on CQADupStack. They also report bet-
ter results when representations are learned on
domain-specific rather than generic data.

2.2 Sentence embeddings and STS

Unsupervised DQD is related to the task of unsu-
pervised Semantic Textual Similarity (STS), i.e.,
sentence similarity scoring (Cer et al., 2017).
Arora et al. (2017) show that a weighted average
over pre-trained word embeddings, followed by
principal component removal, is a strong baseline
for STS. We use their weighting scheme, Smooth
Inverse Frequency (SIF), in Section 3.2.

Averaged word embeddings are insensitive to
word order. This stands in contrast to contextu-
alized encoders, such as LSTMs or Transform-



1632

ers (Vaswani et al., 2017). Contextualized en-
coders are typically trained as unsupervised lan-
guage models (Peters et al., 2018; Devlin et al.,
2019) or on supervised transfer tasks (Conneau
et al., 2017; Cer et al., 2018). At the time of writ-
ing, weighted averaged word embeddings achieve
better results than contextualized encoders on un-
supervised STS.2

2.3 Multi-view word embeddings
Multi-view representation learning is an umbrella
term for methods that transform different rep-
resentations of the same entities into a com-
mon space. In NLP, it has typically been ap-
plied to word embeddings. A famous example is
the cross-lingual projection of word embeddings
(Mikolov et al., 2013b; Faruqui and Dyer, 2014).
Monolingually, Rastogi et al. (2015) use Gener-
alized Canonical Correlation Analysis (GCCA) to
project different word representations into a com-
mon space. Yin and Schütze (2016) combine word
embeddings by concatenation, truncated Singular
Value Decomposition and linear projections; Bol-
legala and Bao (2018) use autoencoders. Sarma
et al. (2018) correlate generic and domain-specific
word embeddings by Canonical Correlation Anal-
ysis (CCA).

All of these methods are post-training, i.e., they
are applied to fully trained word embeddings.
MV-DASE falls into the same category, albeit at
the sentence level (see Section 3.1). Other meth-
ods, which we will call in-training, encourage
the alignment of embeddings during training (e.g.,
Bollegala et al. (2015); Yang et al. (2017)).

2.4 Multi-view sentence embeddings
Multi-view sentence embeddings are less fre-
quently explored than multi-view word embed-
dings. One exception is Tang and de Sa (2019),
who train a recurrent neural network and an av-
erage word embedding encoder jointly on an un-
labeled corpus. This method is in-training, i.e., it
cannot be used to combine pre-existing encoders.

Kiela et al. (2018) dynamically integrate an en-
semble of word embeddings into a task-specific
LSTM. They require labeled data and the result-
ing embeddings are task-specific.

Sarma et al. (2018) marry domain-adapted word
embeddings (see Section 2.3) with InferSent (Con-
neau et al., 2017), a bidirectional LSTM sentence

2http://ixa2.si.ehu.es/stswiki/index.
php/STSbenchmark

encoder trained on Stanford Natural Language In-
ference (SNLI) (Bowman et al., 2015). They ini-
tialize InferSent with the adapted embeddings and
then retrain it on SNLI. Note that this approach
is not feasible when the training regime of an en-
coder cannot be reproduced, e.g., when the origi-
nal training data is not publicly available.

3 Method

We now describe MV-DASE as a general frame-
work. For details on the ensemble used in this pa-
per, see Section 3.2.

3.1 Framework
GCCA basics. Given zero-mean random vec-
tors x1 ∈ Rd1 ,x2 ∈ Rd2 , Canonical Correla-
tion Analysis (CCA) finds linear transformations
θ1 ∈ Rd1 ,θ2 ∈ Rd2 such that θT1 x1 and θT2 x2
are maximally correlated. Bach and Jordan (2002)
show that CCA reduces to a generalized eigen-
value problem. A generalized eigenvalue problem
finds scalar-vector pairs (ρ,θ) that satisfy Aθ =
ρBθ for matrices A,B. Here, A,B are the fol-
lowing block matrices:[

0 Σ1,2
Σ2,1 0

]
θ = ρ

[
Σ1,1 0

0 Σ2,2

]
θ (1)

where Σ1,1,Σ2,2 are the covariance matrices of
x1,x2 and Σ1,2,Σ2,1 are their cross-covariance
matrices. We stack all d eigenvectors into an op-
erator Θ ∈ Rd×d1+d2 . Using this operator, multi-
view representations are projected as:

xmv = Θ

[
x1
x2

]
(2)

Generalized CCA (GCCA) generalizes CCA to
three or more random vectors x1 . . .xJ . There are
several variants of GCCA (Kettenring, 1971); we
follow Bach and Jordan (2002) and solve a multi-
view version of Equation 1: 0 Σ... Σ1,JΣ... 0 Σ...

ΣJ,1 Σ... 0

θ
= ρ

Σ1,1 0 00 Σ... 0
0 0 ΣJ,J

θ
(3)

For stability, we add τσjIj to every covariance
matrix Σj,j , where τ is a hyperparameter (here:
τ = 0.1), Ij is the identity matrix and σj is the

http://ixa2.si.ehu.es/stswiki/index.php/STSbenchmark
http://ixa2.si.ehu.es/stswiki/index.php/STSbenchmark


1633

average variance of xj . Like in the two-view case,
we stack all d eigenvectors into an operator: Θ ∈
Rd×

∑
j dj .

GCCA application. Assume that we have an
ensemble of J sentence encoders. The j’th en-
coder is denoted fj : S → Rdj , where S is the
set of all possible in-domain strings (here: in-
domain questions) and dj is determined by fj .
Assume also that we have a sample from S, i.e.,
a corpus of unlabeled in-domain strings, denoted
S = {s1, . . . , sN}. From this corpus, we create
one training matrix Xj per encoder:

Xj ∈ RN×dj =

 fj(s1)...
fj(sN )

 (4)
From Xj we estimate mean vector x̄j ∈ Rdj ,
covariance matrix Σj,j ∈ Rdj×dj and cross-
covariance matrices Σj,j′ ∈ Rdj×dj′ . We then
apply GCCA as described before, yielding Θ ∈
Rd×

∑
j dj . The multi-view embedding of a new

input q (e.g., a test query) is:

fmv(q) = Θ

f1(q)− x̄1...
fJ(q)− x̄J

 (5)
3.2 Ensemble
We use MV-DASE on the following ensemble:

• weighted averaged generic GloVe vectors
(Pennington et al., 2014)
• weighted averaged domain-specific FastText

vectors (Bojanowski et al., 2017)
• Universal Sentence Encoder (USE) (Cer

et al., 2018)
• domain-finetuned BERT (Devlin et al., 2019)

In this section, we describe the encoders in de-
tail. Note that the choice of encoders is orthog-
onal to the framework and other resources could
be used. Where possible, we base our selection
on the literature: We choose USE over InferSent
due to better performance on STS (Perone et al.,
2018), and BERT over ELMo (Peters et al., 2018)
due to better performance on linguistic probing
tasks (Liu et al., 2019a). The choice of GloVe for
generic word embeddings is based on Sarma et al.
(2018).
Weighted averaged word embeddings. We de-
note generic and domain-specific word embed-
dings of some word type i as wG,i ∈ RdG and

fG fD fB
(GloVe) (FastText) (BERT)

no SIF .089 .083 .134
wiki SIF .128 .100 .159
in-domain SIF .147 .104 .176

fB (BERT) ELMo

generic .138 .103
domain-finetuned .176 .155

Table 2: Mean Average Precision (MAP) averaged over
heldout forums. Top: MAP as a function of whether
and where SIF weights are estimated. Bottom: MAP of
generic vs. domain-finetuned BERT and ELMo. Eval-
uation setup is as described in Section 4, using four
heldout forums. Gray: best in column.

wD,i ∈ RdD . For wG,i, we use pre-trained 300-d
GloVe vectors.3 wD,i are trained using skipgram
FastText4 (100-d, default parameters) on the in-
domain corpus S. We SIF-weight all word em-
beddings by a · (a+p(i))−1, where p(i) is the uni-
gram probability of the word type and the smooth-
ing factor (here: a = 10−3) is taken from Arora
et al. (2017). We find that probabilities estimated
on S produce better results than the Wikipedia-
based probabilities provided by Arora et al. (2017)
(see Table 2, top), hence this is what we use below.
After weighting, we perform top-3 principal com-
ponent removal on the embedding matrices, which
is beneficial for word-level similarity tasks (Mu
et al., 2018). We denote the new embeddings of
word type i as ŵG,i, ŵD,i. The embedding of a
tokenized string s = (s1, . . . , sT ) is computed by
averaging:

fG(s) =
1

T

T∑
t=1

ŵG,st fD(s) =
1

T

T∑
t=1

ŵD,st

Contextualized encoders. USE and BERT are
downloaded as pre-trained models.5 6 USE is a
Transformer trained on SkipThought (Kiros et al.,
2015), conversation response prediction (Hender-
son et al., 2017) and SNLI. It outputs a single 512-
d sentence embedding, which we use as-is. Below,
USE is denoted fU .

3nlp.stanford.edu/data/glove.42B.300d.
zip

4github.com/facebookresearch/fastText
5tfhub.dev/google/

universal-sentence-encoder-large/3
6tfhub.dev/google/bert_uncased_L-12_

H-768_A-12/1

nlp.stanford.edu/data/glove.42B.300d.zip
nlp.stanford.edu/data/glove.42B.300d.zip
github.com/facebookresearch/fastText
tfhub.dev/google/universal-sentence-encoder-large/3
tfhub.dev/google/universal-sentence-encoder-large/3
tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1
tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1


1634

BERT is a Transformer that was pre-trained as
a masked language model with next sentence pre-
diction. We find that domain-finetuning BERT on
S results in improvements over generic BERT (see
Table 2, bottom). Note that domain-finetuning
refers to unsupervised training as a masked lan-
guage model, i.e., we only require unlabeled data
(Howard and Ruder, 2018). We use default param-
eters7 except for a reduced batch size of 8.

At test time, we take the following ap-
proach: BERT segments a token sequence s =
(s1, . . . , sT ) into a subword sequence s′ =
([CLS], s′1, . . . , s

′
T ′ , [SEP]), where [CLS] and

[SEP] are special tokens that were used during
pre-training, and T ′ ≥ T . BERT produces one
768-d vector vl,t per subword s′t and layer l ∈
[1, . . . , L], where L is the total number of layers
(here: 12). We SIF-weight all vectors according
to the probability of their subword (estimated on
S) and average over layers and subwords, exclud-
ing the special tokens:

fB(s) =
1

T ′ · L

T ′∑
t=1

L∑
l=1

a

a+ p(s′t)
vl,t

4 Evaluation on Stack Exchange

4.1 Data

Corpora. We evaluate MV-DASE on the
CQADupStack corpus (Hoogeveen et al., 2015),
which is based on a 2014 Stack Exchange dump.
CQADupStack contains 12 forums that have
enough duplicates for supervised training; as a
consequence, it may not be representative of low-
resource domains. We therefore supplement it
with 12 low-resource forums from the 2018-12-20
Stack Exchange dump.8 For our purposes, low-
resource means a forum with 100–200 duplicates,
which we consider sufficient for evaluation but in-
sufficient for supervised training. All duplicates
in the datasets were labeled by unpaid community
members. As a result, false negatives (i.e., un-
flagged duplicates) are common in the gold stan-
dard (Hoogeveen et al., 2016). While we do not
explicitly filter for language, the vast majority of
the data is in English.

7github.com/google-research/bert/blob/
master/run_pretraining.py

8Preprocessed low-resource data can be downloaded here:
github.com/npoe/lowresourcecqa

forum #Q #D #T

C
Q

A
D

up
St

ac
k

fo
ru

m
s

and android 23697 1579 2.4M
eng english 41791 3506 3.4M
gam gaming 46896 2207 4.0M
gis gis 38522 1099 4.6M
mat mathematica 17509 1271 2.6M
phy physics 39355 1769 6.1M
prg programmers 33052 1538 5.6M
sta stats 42921 890 7.2M
tex tex 71090 4939 7.4M
uni unix 48454 1648 5.5M
web webmasters 17911 1143 2.0M
wor wordpress 49146 719 5.6M

lo
w

-r
es

ou
rc

e
fo

ru
m

s

bud buddhism 5350 120 670K
che chess 4539 154 500K
cog cogsci 5687 126 800K
law law 11059 126 1.7M
net networkengineering 11386 154 1.5M
out outdoors 4651 124 580K
pro productivity 2508 127 380K
rev reverseengineering 15619 119 790K
sit sitecore 5605 130 680K
spo sports 4531 127 430K
sqa sqa 8360 166 950K
win windowsphone 3490 192 290K

Table 3: Forum statistics. #Q: total number of ques-
tions, #D: number of labeled duplicates, #T: number of
tokens in training set S. Gray: heldout forums.

Data split. We split every forum into a test and
training set, such that the test set contains all du-
plicates and the training set contains the remain-
ing unlabeled questions.9 The unlabeled training
set is used for FastText training, BERT domain-
finetuning, SIF weight estimation and GCCA.
Test queries are never seen during training, not
even in an unsupervised way. For hyperparam-
eter choices, we hold out two high-resource and
two low-resource forums (highlighted in Table 3).
They are not used for the final evaluation and sig-
nificance tests.
Preprocessing. Every question object consists of
a title (average length 9 words), a body (average
length 125 words), any number of answers or com-
ments, and metadata (e.g., upvotes, view counts).
We preprocess the data with the CQADupStack
package.10 To calculate question representations,
we use the concatenation of question title and
body. We always ignore answers, comments and
metadata, as this information is not usually avail-
able at the time a question is posted.

9We do not use the official CQADupStack train / test split,
as it is meant for supervised training and has comparatively
few duplicates per test set. Since MV-DASE is unsupervised,
we can afford a more robust evaluation on all labeled dupli-
cates.

10github.com/D1Doris/CQADupStack

github.com/google-research/bert/blob/master/run_pretraining.py
github.com/google-research/bert/blob/master/run_pretraining.py
github.com/npoe/lowresourcecqa
github.com/D1Doris/CQADupStack


1635

heldout test forums M
A

P

A
U

C
(.0

5)

N
D

C
G

P@
3

R
@

3

C
Q

A
D

up
St

ac
k

fo
ru

m
s

and eng gam gis mat prg phy sta tex uni web wor average over test forums

1 BM25 .175 .162 .310 .264 .132 .119 .216 .212 .116 .171 .103 .171 .181 .821 .314 .067 .196

2 fG (GloVe) .121 .093 .202 .148 .056 .084 .152 .153 .063 .120 .085 .093 .115 .755 .233 .042 .123
3 fD (FastText) .123 .083 .211 .169 .079 .091 .172 .175 .085 .136 .084 .107 .131 .817 .261 .047 .138
4 fU (USE) .183 .113 .347 .156 .081 .146 .195 .165 .071 .142 .110 .117 .153 .832 .285 .056 .163
5 fB (BERT) .141 .129 .262 .196 .103 .099 .190 .179 .090 .135 .109 .134 .150 .805 .276 .055 .159

6 MV-DASE .211 .177 .371 .274 .149 .181 .259 .236 .135 .206 .145 .183 .214 .904 .362 .080 .232

7 InferSent .069 .047 .145 .123 .041 .041 .105 .121 .042 .078 .053 .072 .082 .667 .182 .029 .085
8 doc2vec .102 .057 .141 .150 .064 .069 .138 .170 .067 .125 .083 .111 .112 .799 .234 .040 .116
9 ELMo .141 .116 .251 .179 .081 .097 .184 .182 .087 .147 .097 .117 .142 .835 .274 .051 .149

10 word-level CCA .149 .109 .253 .202 .101 .111 .190 .189 .096 .156 .103 .125 .153 .851 .290 .055 .161

11 upper bound 1.00 .351 .999

lo
w

-r
es

ou
rc

e
fo

ru
m

s

bud che cog law net out pro rev sit spo sqa win average over test forums

12 BM25 .276 .195 .269 .345 .167 .373 .196 .186 .430 .465 .275 .349 .306 .842 .461 .116 .345

13 fG (GloVe) .249 .125 .209 .312 .103 .260 .110 .134 .237 .363 .166 .239 .213 .781 .359 .079 .234
14 fD (FastText) .142 .064 .132 .255 .111 .168 .067 .101 .243 .239 .173 .136 .163 .767 .314 .060 .180
15 fU (USE) .332 .247 .384 .458 .152 .513 .214 .144 .282 .448 .221 .244 .306 .880 .470 .119 .352
16 fB (BERT) .261 .173 .221 .335 .137 .348 .171 .143 .324 .489 .194 .257 .262 .812 .411 .099 .294

17 MV-DASE .378 .259 .384 .447 .184 .495 .233 .241 .427 .523 .289 .352 .358 .924 .524 .137 .407

18 InferSent .154 .073 .117 .236 .079 .194 .089 .078 .192 .312 .123 .161 .158 .701 .281 .054 .162
19 doc2vec .133 .058 .117 .239 .146 .145 .057 .080 .192 .141 .140 .090 .135 .759 .279 .048 .143
20 ELMo .222 .140 .228 .332 .137 .248 .136 .092 .247 .433 .171 .278 .230 .837 .387 .084 .252
21 word-level CCA .260 .142 .237 .325 .146 .274 .111 .186 .312 .327 .218 .194 .233 .844 .391 .086 .254

22 ADA .229 .164 .161 .250 .132 .207 .117 .147 .225 .299 .193 .218 .195 .823 .347 .068 .201

23 upper bound 1.00 .341 .999

Table 4: Main results. Left: MAP on individual forums (heldout and test forums). Rightmost five columns: all
metrics averaged over test forums (excluding heldout forums). Gray: best in column.

4.2 Evaluation and Metrics

Given a test query q, we rank all candidates c 6= q
from the same forum by cos(f(q), f(c)), where f
is an encoder (e.g., MV-DASE). Our metrics are
MAP, AUC(.05), Normalized Discounted Cumu-
lative Gain (NDCG), Recall@3 (R@3) and Pre-
cision@3 (P@3). AUC(.05), the area under the
ROC curve up to a false positive rate of .05, is used
by Shah et al. (2018). Note that upper bounds on
P@3 and R@3 are not 1, since most duplicates
have only one original and a few have more than
three.

4.3 Baselines

Unsupervised. Our IR baseline is BM25 (Robert-
son et al., 1995) as implemented in Elasticsearch
6.5.4 (Gormley and Tong, 2015) with default pa-
rameters. We test against all single-view encoders
from our ensemble. The remaining unsupervised
baselines are:

• ELMo (Peters et al., 2018).11 We treat ELMo
like BERT (Section 3.2), i.e., we finetune12

the language model on the in-domain corpus
(3 epochs, batch size 8), SIF-weight all vec-
tors according to in-domain word probability
and then average over layers and tokens.

• Doc2vec (Le and Mikolov, 2014) trained on
the in-domain corpus, using the best DQD
hyperparameters reported in Lau and Bald-
win (2016).

• InferSent V.1.13 (Conneau et al., 2017)
• Our re-implementation of domain-adapted

CCA word embeddings (Sarma et al. (2018),
see Section 2.3). We use the same word
embeddings, SIF weights and component re-
moval described in Section 3.2. Denoted
“word-level CCA” below.

11tfhub.dev/google/elmo/2
12github.com/allenai/bilm-tf/blob/

master/bin/restart.py
13github.com/facebookresearch/InferSent

tfhub.dev/google/elmo/2
github.com/allenai/bilm-tf/blob/master/bin/restart.py
github.com/allenai/bilm-tf/blob/master/bin/restart.py
github.com/facebookresearch/InferSent


1636

MAP AUC(.05)

1 MV-DASE, ¬fG, ¬fD MV-DASE

2 ¬fB ,¬(fG, fD) ¬fB , ¬fD , ¬fG,
¬(fG, fD)

3 BM25, avg, concat,
ADA, word-level CCA,
ELMo, ¬fU , fD , fG,
fB , ¬(fB , fU ), fU

BM25, avg, concat,
doc2vec, ADA, word-
level CCA, ELMo,
fB , fD , fG, ¬fU , fU ,
¬(fB , fU )

4 InferSent, doc2vec InferSent

Table 5: Group rankings by transitive closure of paired
t-tests. ¬fj is MV-DASE without fj (see Table 6). No
particular order inside groups.

ADA. We evaluate the supervised domain-
adversarial method of Shah et al. (2018) (ADA)
on the low-resource forums. Recall that ADA
requires a related labeled source domain. To
achieve this, we pair every low-resource forum
(target) with the CQADupStack forum (source)
with which it has the highest word trigram over-
lap. See supplementary material for more details
and a table of all source-target mappings.14

4.4 Ablation studies

We perform a set of experiments where we omit
views from the ensemble. We also replace GCCA
with naive view concatenation or view averaging.
When averaging, we pad lower-dimensional vec-
tors (Coates and Bollegala, 2018).

4.5 Significance tests

We perform paired t-tests, using the 20 test set
forums as data points.15 We then find groups of
equivalent methods by transitive closure of a ∼
b ≡ p ≥ .05. Group A being ranked higher
than group B means that every method in A per-
forms significantly better than every method in B.
Two methods in the same group may differ signif-
icantly, but there exists a chain between them of
methods with insignificant differences.

5 Discussion

5.1 Comparison with baselines

BM25. BM25 is a tough baseline for DQD: In
terms of MAP, it is better than or comparable to

14We also experiment with Multinomial Adversarial Net-
works (MAN) (Chen and Cardie, 2018), a multi-source multi-
target framework that can be trained on all 24 forums jointly.
Initial results were not competitive with ADA, so we do not
include them here. See supplementary material for details.

15Ten forums for t-tests involving ADA.

M
A

P

A
U

C
(.0

5)

N
D

C
G

P@
3

R
@

3

C
Q

A
D

up
St

ac
k

1 ¬fG .002 .000 .000 .000 -.001
2 ¬fD -.002 -.008 -.004 -.001 -.002
3 ¬fU -.030 -.032 -.037 -.012 -.035
4 ¬fB -.010 -.006 -.011 -.003 -.008
5 ¬(fU , fB) -.056 -.042 -.066 -.022 -.065
6 ¬(fG, fD) -.012 -.016 -.017 -.005 -.015
7 concat -.042 -.071 -.058 -.017 -.047
8 avg -.046 -.075 -.064 -.018 -.051

lo
w

-r
es

ou
rc

e

9 ¬fG -.008 -.005 -.010 -.006 -.016
10 ¬fD .007 -.001 .012 .002 .006
11 ¬fU -.058 -.050 -.063 -.023 -.067
12 ¬fB -.017 -.006 -.014 -.010 -.028
13 ¬(fU , fB) -.120 -.069 -.130 -.054 -.160
14 ¬(fG, fD) -.010 -.005 -.007 -.005 -.014
15 concat -.058 -.078 -.070 -.023 -.069
16 avg -.067 -.081 -.080 -.028 -.082

Table 6: Ablation study. Deltas relative to MV-DASE.
Metrics were averaged over test forums before calcu-
lating deltas. concat/avg are naive view concatenation
and averaging. Gray: better than MV-DASE.

every single view (see Table 5). MV-DASE on the
other hand, which is built from the same views,
outperforms BM25 significantly and almost con-
sistently (19 out of 20 test forums), regardless of
the metric. This underlines the usefulness of our
multi-view approach.
Single views. MV-DASE outperforms the views
that make up its ensemble significantly and almost
consistently. There are two exceptions (out of 20
test forums): On law and outdoors, fU (USE) per-
forms slightly better on its own (Table 4, row 15).
Since these forums are less “technical” than most,
we hypothesize that they may be less in need of
domain adaptation.
Word-level CCA. The word-level CCA baseline
by Sarma et al. (2018) outperforms fG and fD on
their own (see Table 4, rows 10, 21), which vali-
dates the approach. The method is directly compa-
rable to MV-DASE¬(fU , fB), i.e., MV-DASE on
generic and domain-specific averaged word em-
beddings (see Table 6). The main differences
between them are (a) the order in which CCA
and averaging are performed and (b) whether the
CCA “vocabulary” is composed of word types or
sentences. Note that in contrast to MV-DASE,
word-level CCA is incompatible with contextu-
alized embeddings, since it requires a context-
independent one-to-one mapping between word
types and vectors.



1637

ADA. Supervised domain-adversarial ADA per-
forms significantly worse than unsupervised MV-
DASE (see Table 5). It is comparable to BM25 in
terms of AUC(.05) (the metric used by Shah et al.
(2018)), but not in terms of MAP.

Recall that we restricted the choice of source
domains to the 12 CQADupStack forums.
As a consequence, some target forums were
paired with non-ideal source forums (e.g., en-
glish→buddhism). It is possible that the baseline
would have performed better with a wider choice
of source domains. Nonetheless, this observa-
tion highlights a key advantage of our approach:
It does not depend on the availability of a label-
rich related source domain (or indeed, any labels
at all).
Other baselines. InferSent performs poorly on
the DQD task, which is surprising given its simi-
larity to USE. Recall that InferSent and USE are
pre-trained on sentence-level SNLI, but that the
training regime of USE also contains conversation
response prediction. So USE is expected to be
better equipped to handle (a) multi-sentence doc-
uments and (b) forum-style language.

Doc2vec is trained on the same data as fD, but
performs significantly worse. The difference be-
tween them may be due to the ability of FastText
to exploit orthography. Domain-finetuned ELMo
performs comparably to domain-finetuned BERT
on some forums but not consistently.

5.2 Ablation study

View ablation. On the low-resource forums,
omitting fD has a beneficial effect (Table 6, row
10). This suggests that the in-domain FastText em-
beddings have insufficient quality when learned on
the smallest forums and / or that domain-finetuned
BERT subsumes any positive effect. On the high-
resource CQADupStack forums, domain-specific
embeddings contribute positively, while generic
GloVe does not (rows 1,2). Table 5 shows that
omitting either fG or fD from the ensemble does
not lead to a significant drop in MAP, but omitting
both does.

USE has the biggest positive effect on MV-
DASE (Table 6, rows 3,11), also evidenced by the
fact that omitting it is significantly more harmful
than omitting any other single view (Table 5). Re-
call from Section 3.2 that USE is trained on super-
vised transfer tasks, while the remaining encoders
are fully unsupervised.

GCCA ablation. The naive concatenation or
averaging of views is significantly less effective
than view correlation by GCCA (Table 6, rows
7,8,15,16, and Table 5). This underlines that
multi-view learning is not just about which views
are combined, but also about how. Intuitively,
GCCA discovers which features from the differ-
ent encoders “mean the same thing” in the domain.
By contrast, concatenation treats views as orthog-
onal, while averaging mixes them in an unstruc-
tured way.

6 Evaluation on SemEval-2017 3B

In this section, we evaluate MV-DASE on
SemEval-2017 3B, a DQD shared task based on
the QatarLiving CQA forum. The benchmark pro-
vides manually labeled question pairs for train-
ing as well as additional unlabeled in-domain
data. Since MV-DASE is unsupervised, we dis-
card all training labels and concatenate training
and unlabeled data into a text corpus (≈ 1.5M to-
kens). This corpus is used for FastText training,
BERT domain-finetuning, SIF weight estimation
and GCCA, as described in Section 3.

The test set contains 88 queries q with ten candi-
dates c1 . . . c10 each. We preprocess all data with
the CQADupStack package and concatenate ques-
tion subjects and bodies, before encoding them.
We rank candidates by cos(f(q), f(c)) and evalu-
ate the result with the official shared task scorer.16

In keeping with the original leaderboard, we re-
port MAP and MRR (Mean Reciprocal Rank). We
compare against previous literature as well as all
individual views, view concatenation and averag-
ing. See Table 7 for results. Like we observed on
the Stack Exchange data, MV-DASE outperforms
its individual views, their concatenation and aver-
age. It beats the previous State of the Art (a super-
vised system) by a margin of 2.5% MAP.

7 Evaluation on unsupervised STS

While this paper focuses on Duplicate Question
Detection, MV-DASE is also applicable to other
unsupervised sentence-pair tasks. As proof of con-
cept, we test it on the unsupervised STS Bench-
mark (Cer et al., 2017). Here, the task is to predict
similarity scores y ∈ R for sentence pairs (s1, s2).

16alt.qcri.org/semeval2017/task3/data/
uploads/semeval2017_task3_submissions_
and_scores.zip

alt.qcri.org/semeval2017/task3/data/uploads/semeval2017_task3_submissions_and_scores.zip
alt.qcri.org/semeval2017/task3/data/uploads/semeval2017_task3_submissions_and_scores.zip
alt.qcri.org/semeval2017/task3/data/uploads/semeval2017_task3_submissions_and_scores.zip


1638

MAP MRR

1 fG (GloVe) 43.13 47.39
2 fD (FastText) 43.38 47.67
3 fU (USE) 48.22 52.73
4 fB (BERT) 43.51 48.52

5 MV-DASE 51.56 56.48
6 concat 44.66 49.84
7 avg 44.95 49.76

8 Filice et al. (2017)* 49.00 52.41
9 Charlet and Damnati (2017)* 47.87 50.97

10 Goyal (2017)* 47.20 53.22
11 Zhang and Wu (2018) 48.53 52.75
12 Yang et al. (2018) 48.97 -
13 Gonzalez et al. (2018) 48.56 52.41

14 IR baseline* 41.85 46.42
15 Random baseline* 29.81 33.02

Table 7: MAP and MRR (percentages) on SemEval-
2017 3B test set. *Shared task top teams (best run
out of three) and baselines as reported in Nakov et al.
(2017), Table 6. Gray: best in column.

We treat the benchmark training set as an un-
labeled corpus, i.e., we discard all labels and de-
stroy the original sentence pairings by shuffling.
The resulting corpus is used for BERT domain-
finetuning, SIF weight estimation and GCCA.
At test time, we measure Pearson’s r between
cos(f(s1), f(s2)) and y, where f is an encoder
(e.g., MV-DASE) and y is the ground truth sim-
ilarity of test set pair (s1, s2).

In this experiment, the ensemble contains USE
(fU ), domain-finetuned BERT (fB) and fG. For
fG, we either use SIF-weighted averaged GloVe
vectors (Section 3.2), or unweighted averaged
ParaNMT17 word and trigram vectors (Wieting
and Gimpel, 2018), which are the current State
of the Art on the unsupervised STS Benchmark
test set (Ethayarajh, 2018). The unlabeled training
set is very small (64K tokens); hence, we do not
include fD in the ensemble, and we finetune the
BERT language model for 10K rather than 100K
steps to avoid overfitting. Like on the DQD tasks,
MV-DASE beats its individual views as well as
naive view concatenation and averaging (see Ta-
ble 8). After adding ParaNMT to the ensemble,
we achieve competitive results.

8 Future Work

Non-Linear GCCA. In Section 3.1, we assumed
that relationships between representations are lin-
ear. This is probably reasonable for word embed-
dings (most cross-lingual word embeddings are

17github.com/jwieting/para-nmt-50m

fG = GloVe fG = ParaNMT

1 fG .731 / .647 .817 / .799
2 fU (USE) .793 / .762 .793 / .762
3 fB (BERT) .779 / .718 .779 / .718

4 MV-DASE .825 / .771 .842 / .804
5 concat .791 / .730 .826 / .772
6 avg .790 / .729 .823 / .771

Table 8: Pearson’s r (dev / test) on the unsupervised
STS Benchmark, using different embeddings for fG.
Gray: best in column. Underlined: current unsuper-
vised SoTA on test set (Wieting and Gimpel, 2018).

linear projections, e.g. Artetxe et al. (2018)), but
it is unclear whether it holds for sentence embed-
dings. Potential avenues for non-linear GCCA in-
clude Kernel GCCA (Tenenhaus et al., 2015) and
Deep GCCA (Benton et al., 2017).
More views. A major advantage of MV-DASE is
that it is agnostic to the number and specifics of
its views. We plan to investigate whether addi-
tional or different views (e.g., encoders learned on
related domains) can increase performance.

9 Conclusion

We have presented a multi-view approach to un-
supervised Duplicate Question Detection in low-
resource, domain-specific Community Question
Answering forums. MV-DASE is a multi-view
sentence embedding framework based on Gener-
alized Canonical Correlation Analysis. It com-
bines domain-specific and generic weighted av-
eraged word embeddings with domain-finetuned
BERT and the Universal Sentence Encoder, using
unlabeled in-domain data only.

Experiments on the CQADupStack corpus and
additional low-resource forums show significant
improvements over BM25 and all single-view
baselines. MV-DASE sets a new State of the Art
on a recent DQD shared task (SemEval-2017 3B),
with a 2.5% MAP improvement over the best su-
pervised system. Finally, an experiment on the
STS Benchmark suggests that MV-DASE has po-
tential on other unsupervised sentence-pair tasks.

Acknowledgements

We thank Bernt Andrassy and Pankaj Gupta at
Siemens MIC-DE, as well as our anonymous re-
viewers, for their helpful comments. This research
was funded by Siemens AG.

github.com/jwieting/para-nmt-50m


1639

References
Sanjeev Arora, Yingyu Liang, and Tengyu Ma. 2017.

A simple but tough-to-beat baseline for sentence em-
beddings. In ICLR, Toulon, France.

Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2018.
A robust self-learning method for fully unsupervised
cross-lingual mappings of word embeddings. In
ACL, pages 789–798, Melbourne, Australia.

Francis R Bach and Michael I Jordan. 2002. Kernel
independent component analysis. JMLR, 3:1–48.

Timothy Baldwin, Huizhi Liang, Bahar Salehi, Doris
Hoogeveen, Yitong Li, and Long Duong. 2016.
UniMelb at SemEval-2016 Task 3: Identifying sim-
ilar questions by combining a CNN with string sim-
ilarity measures. In International Workshop on
Semantic Evaluation, pages 851–856, San Diego,
USA.

Adrian Benton, Huda Khayrallah, Biman Gujral,
Dee Ann Reisinger, Sheng Zhang, and Raman
Arora. 2017. Deep generalized canonical correla-
tion analysis. arXiv preprint arXiv:1702.02519.

Dasha Bogdanova, Cicero dos Santos, Luciano Bar-
bosa, and Bianca Zadrozny. 2015. Detecting seman-
tically equivalent questions in online user forums. In
CoNLL, pages 123–131, Beijing, China.

Piotr Bojanowski, Edouard Grave, Armand Joulin, and
Tomas Mikolov. 2017. Enriching word vectors with
subword information. TACL, 5:135–146.

Danushka Bollegala and Cong Bao. 2018. Learning
word meta-embeddings by autoencoding. In COL-
ING, pages 1650–1661, Santa Fe, USA.

Danushka Bollegala, Takanori Maehara, and Ken-ichi
Kawarabayashi. 2015. Unsupervised cross-domain
word representation learning. In ACL, pages 730–
740, Beijing, China.

Samuel R Bowman, Gabor Angeli, Christopher Potts,
and Christopher D Manning. 2015. A large anno-
tated corpus for learning natural language inference.
In EMNLP, pages 632–642, Lisbon, Portugal.

Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-
Gazpio, and Lucia Specia. 2017. SemEval-2017
task 1: Semantic textual similarity multilingual
and crosslingual focused evaluation. In Interna-
tional Workshop on Semantic Evaluation, pages 1–
14, Vancouver, Canada.

Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua,
Nicole Limtiaco, Rhomni St John, Noah Constant,
Mario Guajardo-Cespedes, Steve Yuan, Chris Tar,
et al. 2018. Universal Sentence Encoder for English.
In EMNLP, pages 169–174, Brussels, Belgium.

Delphine Charlet and Geraldine Damnati. 2017. Sim-
Bow at SemEval-2017 Task 3: Soft-cosine se-
mantic similarity between questions for community

question answering. In International Workshop on
Semantic Evaluation, pages 315–319, Vancouver,
Canada.

Xilun Chen and Claire Cardie. 2018. Multinomial ad-
versarial networks for multi-domain text classifica-
tion. In NAACL-HLT, pages 1226–1240, New Or-
leans, USA.

Joshua Coates and Danushka Bollegala. 2018. Frus-
tratingly easy meta-embedding – computing meta-
embeddings by averaging source word embeddings.
In NAACL-HLT, pages 194–198, New Orleans,
USA.

Alexis Conneau, Douwe Kiela, Holger Schwenk, Loı̈c
Barrault, and Antoine Bordes. 2017. Supervised
learning of universal sentence representations from
natural language inference data. In EMNLP, pages
670–680, Copenhagen, Denmark.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In NAACL-HLT, pages 4171–4186, Min-
neapolis, USA.

Cicero Dos Santos, Luciano Barbosa, Dasha Bog-
danova, and Bianca Zadrozny. 2015. Learning hy-
brid representations to retrieve semantically equiv-
alent questions. In ACL, pages 694–699, Beijing,
China.

Kawin Ethayarajh. 2018. Unsupervised random walk
sentence embeddings: A strong but simple baseline.
In Workshop on Representation Learning for NLP,
pages 91–100, Melbourne, Australia.

Manaal Faruqui and Chris Dyer. 2014. Improving vec-
tor space word representations using multilingual
correlation. In EACL, pages 462–471, Gothenburg,
Sweden.

Simone Filice, Giovanni Da San Martino, Alessan-
dro Moschitti, and Roberto Basili. 2017. KeLP at
SemEval-2017 Task 3: Learning pairwise patterns
in community question answering. In International
Workshop on Semantic Evaluation, pages 326–333,
Vancouver, Canada.

Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan,
Pascal Germain, Hugo Larochelle, François Lavi-
olette, Mario Marchand, and Victor Lempitsky.
2016. Domain-adversarial training of neural net-
works. JMLR, 17:2096–2030.

Ana Gonzalez, Isabelle Augenstein, and Anders
Søgaard. 2018. A strong baseline for question rele-
vancy ranking. In EMNLP, pages 4810–4815, Brus-
sels, Belgium.

Clinton Gormley and Zachary Tong. 2015. Elastic-
search: The definitive guide: A distributed real-time
search and analytics engine. O’Reilly Media.

https://pdfs.semanticscholar.org/3fc9/7768dc0b36449ec377d6a4cad8827908d5b4.pdf
https://pdfs.semanticscholar.org/3fc9/7768dc0b36449ec377d6a4cad8827908d5b4.pdf
https://www.aclweb.org/anthology/P18-1073
https://www.aclweb.org/anthology/P18-1073
http://www.jmlr.org/papers/v3/bach02a
http://www.jmlr.org/papers/v3/bach02a
https://doi.org/10.18653/v1/S16-1027
https://doi.org/10.18653/v1/S16-1027
https://doi.org/10.18653/v1/S16-1027
https://arxiv.org/pdf/1702.02519
https://arxiv.org/pdf/1702.02519
https://doi.org/10.18653/v1/K15-1013
https://doi.org/10.18653/v1/K15-1013
https://doi.org/https://doi.org/10.1162/tacl_a_00051
https://doi.org/https://doi.org/10.1162/tacl_a_00051
https://www.aclweb.org/anthology/C18-1140
https://www.aclweb.org/anthology/C18-1140
https://doi.org/https://doi.org/10.3115/v1/p15-1071
https://doi.org/https://doi.org/10.3115/v1/p15-1071
https://doi.org/10.18653/v1/d15-1075
https://doi.org/10.18653/v1/d15-1075
https://doi.org/10.18653/v1/s17-2001
https://doi.org/10.18653/v1/s17-2001
https://doi.org/10.18653/v1/s17-2001
https://www.aclweb.org/anthology/D18-2029
https://doi.org/10.18653/v1/S17-2051
https://doi.org/10.18653/v1/S17-2051
https://doi.org/10.18653/v1/S17-2051
https://doi.org/10.18653/v1/S17-2051
https://doi.org/10.18653/v1/N18-1111
https://doi.org/10.18653/v1/N18-1111
https://doi.org/10.18653/v1/N18-1111
https://doi.org/10.18653/v1/n18-2031
https://doi.org/10.18653/v1/n18-2031
https://doi.org/10.18653/v1/n18-2031
https://doi.org/10.18653/v1/d17-1070
https://doi.org/10.18653/v1/d17-1070
https://doi.org/10.18653/v1/d17-1070
https://doi.org/10.18653/v1/N19-1423
https://doi.org/10.18653/v1/N19-1423
https://doi.org/10.18653/v1/N19-1423
https://doi.org/https://doi.org/10.3115/v1/p15-2114
https://doi.org/https://doi.org/10.3115/v1/p15-2114
https://doi.org/https://doi.org/10.3115/v1/p15-2114
https://www.aclweb.org/anthology/W18-3012
https://www.aclweb.org/anthology/W18-3012
https://doi.org/https://doi.org/10.3115/v1/e14-1049
https://doi.org/https://doi.org/10.3115/v1/e14-1049
https://doi.org/https://doi.org/10.3115/v1/e14-1049
https://doi.org/10.18653/v1/n16-1129
https://doi.org/10.18653/v1/n16-1129
https://doi.org/10.18653/v1/n16-1129
http://jmlr.org/papers/v17/15-239
http://jmlr.org/papers/v17/15-239
https://doi.org/10.18653/v1/d18-1515
https://doi.org/10.18653/v1/d18-1515


1640

Naman Goyal. 2017. LearningToQuestion at semeval
2017 Task 3: Ranking similar questions by learning
to rank using rich features. In International Work-
shop on Semantic Evaluation, pages 326–333, Van-
couver, Canada.

Matthew Henderson, Rami Al-Rfou, Brian Strope,
Yun-hsuan Sung, Laszlo Lukacs, Ruiqi Guo, San-
jiv Kumar, Balint Miklos, and Ray Kurzweil. 2017.
Efficient natural language response suggestion for
smart reply. arXiv preprint arXiv:1705.00652.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Doris Hoogeveen, Andrew Bennett, Yitong Li,
Karin M Verspoor, and Timothy Baldwin. 2018. De-
tecting misflagged duplicate questions in commu-
nity question-answering archives. In ICWSM, Stan-
ford, USA.

Doris Hoogeveen, Karin M Verspoor, and Timothy
Baldwin. 2015. CQADupStack: A benchmark data
set for community question-answering research.
In Australasian Document Computing Symposium,
Parramatta, Australia.

Doris Hoogeveen, Karin M Verspoor, and Timothy
Baldwin. 2016. CQADupStack: Gold or silver. In
Workshop on Web Question Answering Beyond Fac-
toids, volume 16, Pisa, Italy.

Jeremy Howard and Sebastian Ruder. 2018. Universal
language model fine-tuning for text classification. In
ACL, pages 328–339, Melbourne, Australia.

Jon R Kettenring. 1971. Canonical analysis of several
sets of variables. Biometrika, 58(3):433–451.

Douwe Kiela, Changhan Wang, and Kyunghyun Cho.
2018. Dynamic meta-embeddings for improved sen-
tence representations. In EMNLP, pages 1466–
1477, Brussels, Belgium.

Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,
Richard Zemel, Raquel Urtasun, Antonio Torralba,
and Sanja Fidler. 2015. Skip-thought vectors. In
NeurIPS, pages 3294–3302, Montreal, Canada.

Jey Hand Lau and Timothy Baldwin. 2016. An em-
pirical evaluation of doc2vec with practical insights
into document embedding generation. In Workshop
on Representation Learning for NLP, pages 78–86,
Berlin, Germany.

Quoc Le and Tomas Mikolov. 2014. Distributed rep-
resentations of sentences and documents. In ICML,
pages 1188–1196, Beijing, China.

Nelson F Liu, Matt Gardner, Yonatan Belinkov,
Matthew Peters, and Noah A Smith. 2019a. Lin-
guistic knowledge and transferability of contextual
representations. In NAACL-HLT, pages 1073–1094,
Minneapolis, USA.

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019b.
RoBERTa: A robustly optimized BERT pretraining
approach. arXiv preprint arXiv:1907.11692.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013a. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.

Tomas Mikolov, Quoc V Le, and Ilya Sutskever. 2013b.
Exploiting similarities among languages for ma-
chine translation. arXiv preprint arXiv:1309.4168.

Jiaqi Mu, Suma Bhat, and Pramod Viswanath. 2018.
All-but-the-top: Simple and effective postprocess-
ing for word representations. In ICLR, Vancouver,
Canada.

Preslav Nakov, Doris Hoogeveen, Lluıs Marquez,
Allessandro Moschitti, Hamdy Mubarak, Timothy
Baldwin, and Karin Verspoor. 2017. SemEval-2017
Task 3: Community question answering. In Inter-
national Workshop on Semantic Evaluation, pages
27–48, Vancouver, Canada.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. GloVe: Global vectors for word
representation. In EMNLP, pages 1532–1543,
Doha, Qatar.

Christian S Perone, Roberto Silveira, and Thomas S
Paula. 2018. Evaluation of sentence embeddings
in downstream and linguistic probing tasks. arXiv
preprint arXiv:1806.06259.

Matthew Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word repre-
sentations. In NAACL-HLT, pages 2227–2237, New
Orleans, USA.

Pushpendre Rastogi, Benjamin Van Durme, and Raman
Arora. 2015. Multiview LSA: Representation learn-
ing via generalized CCA. In NAACL-HLT, pages
556–566, Denver, USA.

Stephen E Robertson, Steve Walker, Susan Jones,
Micheline M Hancock-Beaulieu, Mike Gatford,
et al. 1995. Okapi at TREC-3. Text REtrieval Con-
ference (TREC), pages 109–126.

João António Rodrigues, Chakaveh Saedi, Vladislav
Maraev, Joao Silva, and António Branco. 2017.
Ways of asking and replying in duplicate question
detection. In Joint Conference on Lexical and Com-
putational Semantics, pages 262–270, Vancouver,
Canada.

Prathusha K Sarma, Yingyu Liang, and William A
Sethares. 2018. Domain adapted word embeddings
for improved sentiment classification. In Workshop
on Deep Learning Approaches for Low-Resource
NLP, pages 51–59, Melbourne, Australia.

https://doi.org/10.18653/v1/s17-2050
https://doi.org/10.18653/v1/s17-2050
https://doi.org/10.18653/v1/s17-2050
https://arxiv.org/abs/1705.00652
https://arxiv.org/abs/1705.00652
https://doi.org/https://doi.org/10.1162/neco.1997.9.8.1735
https://www.aaai.org/ocs/index.php/ICWSM/ICWSM18/paper/view/17841/17002
https://www.aaai.org/ocs/index.php/ICWSM/ICWSM18/paper/view/17841/17002
https://www.aaai.org/ocs/index.php/ICWSM/ICWSM18/paper/view/17841/17002
https://doi.org/https://doi.org/10.1145/2838931.2838934
https://doi.org/https://doi.org/10.1145/2838931.2838934
https://pdfs.semanticscholar.org/898c/de2f035a4728e629dd5592c13806140853b8.pdf
https://www.aclweb.org/anthology/P18-1031
https://www.aclweb.org/anthology/P18-1031
https://doi.org/https://doi.org/10.2307/2334380
https://doi.org/https://doi.org/10.2307/2334380
https://doi.org/10.18653/v1/d18-1176
https://doi.org/10.18653/v1/d18-1176
http://papers.nips.cc/paper/5950-skip-thought-vectors
https://doi.org/10.18653/v1/w16-1609
https://doi.org/10.18653/v1/w16-1609
https://doi.org/10.18653/v1/w16-1609
http://www.jmlr.org/proceedings/papers/v32/le14
http://www.jmlr.org/proceedings/papers/v32/le14
https://doi.org/10.18653/v1/N19-1112
https://doi.org/10.18653/v1/N19-1112
https://doi.org/10.18653/v1/N19-1112
https://arxiv.org/abs/1907.11692
https://arxiv.org/abs/1907.11692
https://arxiv.org/abs/1301.3781
https://arxiv.org/abs/1301.3781
https://arxiv.org/pdf/1309.4168
https://arxiv.org/pdf/1309.4168
https://arxiv.org/abs/1702.01417
https://arxiv.org/abs/1702.01417
https://doi.org/10.18653/v1/s16-1083
https://doi.org/10.18653/v1/s16-1083
https://doi.org/10.3115/v1/D14-1162
https://doi.org/10.3115/v1/D14-1162
https://arxiv.org/abs/1806.06259
https://arxiv.org/abs/1806.06259
https://doi.org/10.18653/v1/N18-1202
https://doi.org/10.18653/v1/N18-1202
https://doi.org/https://doi.org/10.3115/v1/n15-1058
https://doi.org/https://doi.org/10.3115/v1/n15-1058
https://doi.org/10.18653/v1/s17-1030
https://doi.org/10.18653/v1/s17-1030
https://www.aclweb.org/anthology/P18-2007
https://www.aclweb.org/anthology/P18-2007


1641

Darsh Shah, Tao Lei, Alessandro Moschitti, Salvatore
Romeo, and Preslav Nakov. 2018. Adversarial do-
main adaptation for duplicate question detection. In
EMNLP, pages 1056–1063, Brussels, Belgium.

Shuai Tang and Virginia R de Sa. 2019. Improving sen-
tence representations with multi-view frameworks.
In Interpretability and Robustness for Audio, Speech
and Language Workshop, Montreal, Canada.

Arthur Tenenhaus, Cathy Philippe, and Vincent Frouin.
2015. Kernel generalized canonical correlation
analysis. Computational Statistics & Data Analysis,
90:114–131.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is
all you need. In NeurIPS, pages 5998–6008, Long
Beach, USA.

John Wieting and Kevin Gimpel. 2018. ParaNMT-
50M: Pushing the limits of paraphrastic sentence
embeddings with millions of machine translations.
In ACL, pages 451–462, Melbourne, Australia.

Wei Yang, Wei Lu, and Vincent W Zheng. 2017. A
simple regularization-based algorithm for learning
cross-domain word embeddings. In EMNLP, pages
2898–2904, Copenhagen, Denmark.

Ziyi Yang, Chenguang Zhu, and Weizhu Chen. 2018.
Zero-training sentence embedding via orthogonal
basis. arXiv preprint arXiv:1810.00438.

Wenpeng Yin and Hinrich Schütze. 2016. Learning
word meta-embeddings. In ACL, pages 1351–1360,
Berlin, Germany.

Mingua Zhang and Yunfang Wu. 2018. An unsuper-
vised model with attention autoencoders for ques-
tion retrieval. In AAAI, pages 4978–4986, New Or-
leans, USA.

Wei Emma Zhang, Quan Z Sheng, Jey Han Lau, and
Ermyas Abebe. 2017. Detecting duplicate posts in
programming QA communities via latent semantics
and association rules. In WWW, pages 1221–1229.

https://doi.org/10.18653/v1/d18-1131
https://doi.org/10.18653/v1/d18-1131
https://arxiv.org/pdf/1805.07443
https://arxiv.org/pdf/1805.07443
https://doi.org/https://doi.org/10.1016/j.csda.2015.04.004
https://doi.org/https://doi.org/10.1016/j.csda.2015.04.004
https://papers.nips.cc/paper/7181-attention-is-all-you-need
https://papers.nips.cc/paper/7181-attention-is-all-you-need
https://www.aclweb.org/anthology/P18-1042
https://www.aclweb.org/anthology/P18-1042
https://www.aclweb.org/anthology/P18-1042
https://doi.org/10.18653/v1/d17-1312
https://doi.org/10.18653/v1/d17-1312
https://doi.org/10.18653/v1/d17-1312
https://arxiv.org/abs/1810.00438
https://arxiv.org/abs/1810.00438
https://doi.org/10.18653/v1/p16-1128
https://doi.org/10.18653/v1/p16-1128
https://doi.org/https://doi.org/10.1145/3038912.3052701
https://doi.org/https://doi.org/10.1145/3038912.3052701
https://doi.org/https://doi.org/10.1145/3038912.3052701

