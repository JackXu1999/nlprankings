



















































Team SWEEPer: Joint Sentence Extraction and Fact Checking with Pointer Networks


Proceedings of the First Workshop on Fact Extraction and VERification (FEVER), pages 150–155
Brussels, Belgium, November 1, 2018. c©2018 Association for Computational Linguistics

150

Team SWEEPer: Joint Sentence Extraction and Fact Checking with
Pointer Networks

Christopher Hidey∗
Department of Computer Science

Columbia University
New York, NY 10027

chidey@cs.columbia.edu

Mona Diab
Amazon AI Lab

diabmona@amazon.com

Abstract
Many tasks such as question answering and
reading comprehension rely on information
extracted from unreliable sources. These
systems would thus benefit from knowing
whether a statement from an unreliable source
is correct. We present experiments on the
FEVER (Fact Extraction and VERification)
task, a shared task that involves selecting sen-
tences from Wikipedia and predicting whether
a claim is supported by those sentences, re-
futed, or there is not enough information. Fact
checking is a task that benefits from not only
asserting or disputing the veracity of a claim
but also finding evidence for that position. As
these tasks are dependent on each other, an
ideal model would consider the veracity of the
claim when finding evidence and also find only
the evidence that is relevant. We thus jointly
model sentence extraction and verification on
the FEVER shared task. Among all partici-
pants, we ranked 5th on the blind test set (prior
to any additional human evaluation of the evi-
dence).

1 Introduction

Verifying claims using textual sources is a diffi-
cult problem, requiring natural language inference
as well as information retrieval if the sources are
not provided. The FEVER task (Thorne et al.,
2018) provides a large annotated resource for ex-
traction of sentences from Wikipedia and verifica-
tion of the extracted evidence against a claim. A
system that extracts and verifies statements in this
framework must consist of three components: 1)
Retrieving Wikipedia articles. 2) Identifying the
sentences from Wikipedia that support or refute
the claim. 3) Predicting supporting, refuting, or
not enough info.

We combine these components into two stages:
1) identifying relevant documents (Wikipedia ar-

∗Work completed while at Amazon AI Lab

ticles) for a claim and 2) jointly extracting sen-
tences from the top-ranked articles and predict-
ing a relation for whether the claim is supported,
refuted, or if there is not enough information
in Wikipedia. We first identify relevant docu-
ments by ranking Wikipedia articles according to
a model using lexical and syntactic features. Then,
we derive contextual sentence representations for
the claim paired with each evidence sentence in
the extracted documents. We use the ESIM mod-
ule (Chen et al., 2017b) to create embeddings for
each claim/evidence pair and use a pointer net-
work (Vinyals et al., 2015) to recurrently extract
only relevant evidence sentences while predicting
the relation using the entire set of evidence sen-
tences. Finally, given these components, which
are pre-trained using multi-task learning (Le et al.,
2016), we tune the parameters of the entailment
component given the extracted sentences.

Our experiments reveal that jointly training
the model provides a significant boost in perfor-
mance, suggesting that the contextual representa-
tions learn information about which sentences are
most important for relation prediction as well as
information about the type of relationship the evi-
dence has to the claim.

2 Document Retrieval

For the baseline system, the document retrieval
component from DrQA (Chen et al., 2017a) for
k = 5 documents only finds the supporting evi-
dence 55% of the time. This drops to 44% using
the same model for sentence retrieval at l = 5
sentences. In comparison, in the original work
of Chen et al. (2017a), they find a recall of 70-
86% for all tasks with k = 5. This is partly due
to the misleading information present in the false
claims, whereas for question answering, the ques-
tion is not designed adversarially to contain con-



151

tradicting information. Examining the supporting
and refuting claims in isolation, we find that doc-
ument retrieval at k = 5 is 59% and 49% respec-
tively and sentence selection at l = 5 is 49% and
39%. For example, the false claim ”Murda Beatz
was born on February 21, 1994.” (his birth date is
February 11) also retrieves documents for people
born on February 21. In the question answering
scenario, an example question might be ”Which
rap artist was born on February 11, 1994 in Fort
Erie, Ontario?” which allows an IR system to re-
turn documents using the disambiguating n-grams
about his location and place of birth.

This motivates the decision to focus on noun
phrases. Many of the claims contain the correct
topic of the Wikipedia article in the subject posi-
tion of both the claim and either the first or sec-
ond sentence. When the topic is not in the first
sentence, it is often because the title is ambiguous
and the first sentence is a redirect. For example,
for the article “Savages (2012 film)” the first two
sentences are “For the 2007 film, see The Savages.
Savages is a 2012 American crime thriller film di-
rected by Oliver Stone.”

We thus parse Wikipedia and all the claims us-
ing CoreNLP (Manning et al., 2014) and train a
classifier with the following lexical and syntactic
features from the claim and the first two sentences
in the Wikipedia article:

• TF/IDF from DrQA for full article

• Overlap between the subject/object/modifier
in the claim with the subject/object/modifier
in the first and second sentence of Wikipedia.
The topic of the article is often the subject of
the sentence in Wikipedia, but it is occasion-
ally a disambiguating modifier such as “also
known as”. The topic of the claim is often
in the subject position as well. We also add
overlap between named entities (in case the
parsing fails) and upper case words (in case
the parsing and NER fails). For each of sub-
ject/object/modifier/upper/entity, for both up-
per and lower case, we consider the coverage
of the claim from the first two Wikipedia sen-
tences, for 25 ∗ 2 ∗ 2 = 100 features.

• Average/max/minimum of GloVe (Penning-
ton et al., 2014) embeddings in the claim and
the first and second sentence of Wikipedia.
This feature captures the type of sentence. If
it is a person, it may have words like ’born’,

Model Recall at k = 5
Dev Test

Baseline (DrQA) 55.3 54.2
MLP without title features 82.7 79.7
MLP with title features 90.7 90.5

Figure 1: Document Retrieval Results

’known as’, etc. and if it is a disambiguation
sentence it should have words like ’refers’ or
’see’ or ’confused’. It also allows for the han-
dling of cases where the sentence tokeniza-
tion splits the first sentence.

• Features based on the title: whether the title
is completely contained in the claim and the
overlap between the claim and the title (with
and without metadata information in paren-
theses), for both upper and lower case.

2.1 Experiments
We train a Multi-Layer Perceptron (MLP) using
Pytorch (Paszke et al., 2017) on the top 1000 doc-
uments extracted using DrQA, which obtains a re-
call of 95.3%, with early stopping on the paper
development set. We used the Adam optimizer
(Kingma and Ba, 2015) with a learning rate of
0.001 and hyper-parameters of β1 = 0.9, β2 =
0.999, and � = 10−8. We used pre-trained 300-
dimensional GloVe embeddings. We clip gradi-
ents at 2 and use a batch size of 32.

2.2 Results
Document recall at k = 5 articles is presented in
Figure 1. We present results on the paper devel-
opment and test sets as the evidence for the shared
task blind test set was unavailable as of this writ-
ing. The model with lexical and syntactic features
obtains around 25 points absolute improvement
over the DrQA baseline and the title features when
added provide an additional 8 points improvement
on the development set and 10.8 points on test.

3 Joint Sentence Extraction and Relation
Prediction

Recurrent neural networks have been shown to be
effective for extractive summarization (Nallapati
et al., 2017; Chen and Bansal, 2018). However,
in this context we want to extract sentences that
also help predict whether the claim is supported or
refuted so we jointly model extraction and relation
prediction and train in a multi-task setting.



152

Figure 2: The contextual claim and evidence sentence
representations obtained with ESIM.

First, given the top k extracted documents from
our retrieval component, we select the top l sen-
tences using a weighted combination of Jaccard
similarity and cosine similarity of average word
embeddings. Hyper-parameters were tuned so that
the model would have minimal difference in re-
call from the document retrieval stage but still fit
in memory on a Tesla V100 with 16GB of mem-
ory. We selected k = 5 and l = 50, with Jaccard
similarity weighted at 0.3 and GloVe embeddings
weighted at 0.7. We found that recall on the devel-
opment set was 90.3.

We then store contextual representations of the
claim and each evidence sentence in a memory
network and use a pointer network to extract the
top 5 sentences sequentially. Simultaneously, we
use the entire memory of up to l sentences to pre-
dict the relation: supports, refutes, or not enough
info.

3.1 Sentence Representation
For each sentence in the evidence, we create con-
textual representations using the ESIM module
(Chen et al., 2017b), which first encodes the claim
and the evidence sentence using an LSTM, attends
to the claim and evidence, composes them using
an additional LSTM layer, and finally applies max
and average pooling over time (sentence length)
to obtain a paired sentence representation for the
claim and evidence. This representation is de-
picted in Figure 2. For more details, please refer
to (Chen et al., 2017b).

The representation for a claim c and extracted
evidence sentence ep for c is then:

mp = ESIM(c, ep) (1)

3.2 Sentence Extraction
Next, to select the top 5 evidence sentences, we
use a pointer network (Vinyals et al., 2015; Chen
and Bansal, 2018) over the evidence for claim c to

Figure 3: Our multi-task learning architecture with
contextual ESIM representations used for evidence
sentence extraction via a pointer network and relation
prediction with max/average pooling and dense layers.

extract sentences recurrently. The extraction prob-
ability1 for sentence ep at time t < 5 is then:

utp =

{
vTe tanh(W [mp;h

t,q]), ifpt 6= ps∀s < t.
− inf, otherwise.

(2)

P (pt|p0 · · · pt−1) = softmax(ut) (3)

with ht,q computed using the output of q hops
over the evidence (Vinyals et al., 2016; Sukhbaatar
et al., 2015):

αt,o = softmax(vTh tanh(Wg1mp +Wg2h
t,o−1))

(4)

ht,o =
∑
j

αt,oWg1mj (5)

At each timestep t, we update the hidden state zt of
the pointer network LSTM. Initially, ht,0 is set to
zt. We train and validate the pointer network using
the extracted top l sentences. For all training ex-
amples, we randomly replace evidence sentences
with gold evidence if no gold evidence is found.

3.3 Relation Prediction
In order to predict the support, refute, or not
enough info relation, we use a single-layer MLP
to obtain an abstract representation of the sentence
representation used for extraction, then apply av-
erage and max pooling over the contextual rep-
resentations of the claim and evidence sentences
to obtain a single representation m for the entire
memory. Finally, we use a 2-layer MLP to predict
this relation given m. The entire joint architecture
is presented in Figure 3.

1Set to −inf only while testing



153

3.4 Optimization

We train the model to minimize the negative log
likelihood of the extracted evidence sequence2 and
the cross-entropy loss (L(θrel)) of the relation pre-
diction. The pointer network is trained as in a
sequence-to-sequence model:

L(θptr) = −1/T
∑
t=0..T

logPθptr(pt|p0:t−1) (6)

and the overall loss is then:

L(θ) = λL(θptr) + L(θrel) (7)

Since the evidence selection and relation pre-
diction are scored independently for the FEVER
task, we select the parameters using early stopping
such that one set of parameters performs the best
in terms of evidence recall on the validation set
and another performs the best for accuracy.

Although the models are trained jointly to select
evidence sentences that help predict the relation to
the claim, we may obtain additional improvement
by tuning the parameters given the output of the
sentence extraction step. In order to do so, we first
select the top 5 sentences from the sentence extrac-
tor and predict the relation using only those sen-
tences rather than the entire memory as before. In
this scenario, we pre-train the model using multi-
task learning and tune the parameters for relation
prediction while keeping the sentence extraction
parameters fixed (and using separate representa-
tions for ESIM). We also experimented with a
reinforcement learning approach to tune the sen-
tence extractor as in (Chen and Bansal, 2018) but
found no additional improvement.

3.5 Experiments

We use Pytorch for our experiments. For the
multi-task learning and tuning, we use the Ada-
grad optimizer with learning rates of 0.01 and
0.001 and gradients clipped to 5 and 2, respec-
tively. For both experiments, we used a batch size
of 16. We used the paper development set for early
stopping. We initialized the word embeddings
with 300-dimensional GloVe vectors and fixed
them during training, using a 200-dimensional
projection. Out-of-vocabulary words were initial-
ized randomly during both training and evaluation.

2The evidence as given has no meaningful order but we
use the support/refute sequence as provided in the dataset as
it may contain annotator bias in terms of importance.

Dev Test
LA ER F LA ER F

Base 52.1 44.2 32.6 50.9 45.9 31.9
Gold 68.5 96.0 66.2 65.4 95.2 62.8
MLP 60.4 76.6 51.1 58.7 74.0 49.1
Sep. 56.8 74.9 45.3 53.8 72.7 42.3
MTL 64.0 79.6 55.3 60.5 77.7 52.1
Tune 64.5 79.6 55.8 61.9 77.7 53.2

Figure 4: Paper Development and Test Results (LA:
Label Accuracy, ER: Evidence Recall, F: FEVER
Score)

The second dimension of all other parameter ma-
trices was 200. For the pointer network, we used a
beam size of 5 and q = 3 hops. λ was set to 1.

4 Results

In Figure 4, we present the sentence extraction, re-
lation prediction, and overall FEVER score for the
paper development and test sets. We compare to
the baseline (Base) from the work of Thorne et al.
(2018). For comparison, we also provide the re-
sults when using gold document retrieval with a
perfect oracle to illustrate the upper bound for our
model (Gold). First, we illustrate the difference
in performance when we train a feedforward net-
work to score the sentences individually (MLP)
instead of recurrently with a pointer network. In
this setting, the sentence extraction in Figure 3 is
replaced by a 2-layer feedforward network that is
individually applied to every sentence in the mem-
ory. The output of the network is a score which is
then used to rank the top l = 5 sentences. The
model is still trained using multi-task learning but
with a binary cross-entropy loss in Equation 7 in-
stead of θptr. Furthermore, we show the results
of the sentence extraction and relation prediction
components when trained separately (Sep.). We
finally present the best results - when trained with
multi-task learning (MTL) and then tuned (Tune).

Our results demonstrate that jointly training a
pointer network and relation prediction classifier
improves over training separately. We also note
that the pointer network, which extracts sentences
recurrently by considering the previous sentence,
improves over selecting sentences independently
using an MLP. Although we obtained improve-
ment by tuning, the improvement is slight, which
suggests that the parameter space discovered by
multi-task learning is already learning most of the



154

LA EF1 F
Paper 62.2 31.6 53.5
Blind 59.7 29.7 49.9
Best 68.2 52.9 64.2

Figure 5: Blind Test Results (LA: Label Accuracy,
EF1: Evidence F1, F: FEVER Score)

examples where the model can both identify the
correct sentences and label. Finally, we notice
that improvements to document retrieval would
also improve our model. The gap between Gold
and Tune is around 20 points for evidence recall.
When using gold Wikipedia articles (1-2 docu-
ments), the number of sentences available (around
20 on average) is less than those selected for our
model (l = 50), which makes evidence retrieval
easier as the memory is smaller. As our models
for document retrieval are fairly simple, it is likely
that a more complex model could obtain better
performance with fewer documents.

Results on the shared task blind test set (prior to
any additional human evaluation of the evidence)
are presented in Figure 5. For comparison, we
show the results on the paper development and
test sets (Paper) when submitted for the shared
task as well as the results of the top system on the
leaderboard (Best). On the blind test set (Blind),
overall performance drops by 2-3 points for every
metric compared to the paper set. As our analy-
sis in Section 4.1 shows, the performance drops
significantly when 2 or more evidence sentences
are required. Thus, the performance decrease on
the blind test set may be caused by an increase in
the number of examples that require additional ev-
idence, although as of this writing the evidence for
the test set has not yet been released.

4.1 Analysis

We present an analysis of the performance of the
best model (Tune) when the model requires mul-
tiple pieces of evidence. We present results in Fig-
ure 6 for no evidence (for the “not enough info”
case) and 1, 2, and 3 or more sentences for label
accuracy, evidence recall, and document recall at
k = 5. When no evidence is required the model
only obtains 50% accuracy. We found that this in-
creases to 54% accuracy for the MTL model but
the accuracy on the other 2 classes decreases. This
suggests that using a larger memory improves per-
formance when there is not enough information

Num. LA ER DR
0 50 N/A N/A
1 74 86 93
2 65 17 26
3+ 73 3 14

Figure 6: Paper Development Results by Number of
Evidence Sentences Required (Num.: Sentences Re-
quired, LA: Label Accuracy, ER: Evidence Recall,
DR: Document Recall)

to make a prediction. Intuitively, reading all rel-
evant Wikipedia articles in their entirety would be
necessary for a human to determine this as well.
When only 1 sentence is required the model per-
forms well. However, the performance drops sig-
nificantly on recall when 2 or more sentences are
required. This is largely due to the performance of
the document retrieval component, which seems
to perform poorly when the evidence retrieval re-
quires 2 different Wikipedia articles. When ex-
actly 2 sentences are required, the pointer network
retrieves around 60% of the evidence if the re-
trieved documents are correct. When 3 or more
sentences are required, both components perform
poorly, suggesting there is significant room for im-
provement in this case (although there are very few
examples requiring this amount of evidence).

5 Conclusion

We presented the results of our system for the
FEVER shared task. We described our document
retrieval system using lexical and syntactic fea-
tures. We also described our joint sentence ex-
traction and relation prediction system with multi-
task learning. The results of our model suggest
that the largest gains in performance are likely to
come from improvements to detection of the “not
enough info” class and retrieval of Wikipedia arti-
cles (especially when more than one is required).

For future work, we plan to improve docu-
ment retrieval and experiment with different sen-
tence representations. Using title features im-
proved document retrieval but for non-Wikipedia
data titles would not be available. Furthermore, in
other datasets, the titles may not exactly match the
text of a claim very often and named entity dis-
ambiguation is sometimes needed. One avenue to
explore is neural topic modeling trained using ar-
ticle titles (Bhatia et al., 2016).



155

References
Shraey Bhatia, Jey Han Lau, and Timothy Baldwin.

2016. Automatic labelling of topics with neural
embeddings. In Proceedings of COLING 2016,
the 26th International Conference on Computational
Linguistics: Technical Papers, pages 953–963. The
COLING 2016 Organizing Committee.

Danqi Chen, Adam Fisch, Jason Weston, and Antoine
Bordes. 2017a. Reading wikipedia to answer open-
domain questions. In Proceedings of the 55th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1870–
1879. Association for Computational Linguistics.

Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei, Hui
Jiang, and Diana Inkpen. 2017b. Enhanced lstm for
natural language inference. In Proceedings of the
55th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
1657–1668. Association for Computational Linguis-
tics.

Yen-Chun Chen and Mohit Bansal. 2018. Fast abstrac-
tive summarization with reinforce-selected sentence
rewriting. In Proceedings of the 56th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), pages 675–686. Associa-
tion for Computational Linguistics.

Diederik Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. In International
Conference on Learning Represen- tations (ICLR).

Quoc Le, Ilya Sutskever, Oriol Vinyals, and Lukasz
Kaise. 2016. Multi-task sequence to sequence
learning. In International Conference on Learning
Represen- tations (ICLR).

Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David Mc-
Closky. 2014. The Stanford CoreNLP natural lan-
guage processing toolkit. In Association for Compu-
tational Linguistics (ACL) System Demonstrations,
pages 55–60.

Ramesh Nallapati, Feifei Zhai, and Bowen Zhou. 2017.
Summarunner: A recurrent neural network based se-
quence model for extractive summarization of doc-
uments. In Proceedings of the Thirty-First AAAI
Conference on Artificial Intelligence, February 4-9,
2017, San Francisco, California, USA., pages 3075–
3081.

Adam Paszke, Sam Gross, Soumith Chintala, Gre-
gory Chanan, Edward Yang, Zachary DeVito, Zem-
ing Lin, Alban Desmaison, Luca Antiga, and Adam
Lerer. 2017. Automatic differentiation in pytorch.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. In Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1532–
1543.

Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston,
and Rob Fergus. 2015. End-to-end memory net-
works. In C. Cortes, N. D. Lawrence, D. D. Lee,
M. Sugiyama, and R. Garnett, editors, Advances in
Neural Information Processing Systems 28, pages
2440–2448. Curran Associates, Inc.

James Thorne, Andreas Vlachos, Christos
Christodoulopoulos, and Arpit Mittal. 2018.
Fever: a large-scale dataset for fact extraction
and verification. In Proceedings of the 2018
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, Volume 1 (Long Papers),
pages 809–819. Association for Computational
Linguistics.

Oriol Vinyals, Samy Bengio, and Manjunath Kudlur.
2016. Order matters: Sequence to sequence for sets.
In International Conference on Learning Represen-
tations (ICLR).

Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly.
2015. Pointer networks. In C. Cortes, N. D.
Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett,
editors, Advances in Neural Information Processing
Systems 28, pages 2692–2700. Curran Associates,
Inc.


