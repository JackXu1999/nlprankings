



















































Nonparametric Spherical Topic Modeling with Word Embeddings


Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 537–542,
Berlin, Germany, August 7-12, 2016. c©2016 Association for Computational Linguistics

Nonparametric Spherical Topic Modeling with Word Embeddings

Nematollah Kayhan Batmanghelich∗
CSAIL, MIT

kayhan@mit.edu

Ardavan Saeedi*
CSAIL, MIT

ardavans@mit.edu

Karthik R. Narasimhan
CSAIL, MIT

karthikn@mit.edu

Samuel J. Gershman
Department of Psychology

Harvard University
gershman@fas.harvard.edu

Abstract

Traditional topic models do not account
for semantic regularities in language.
Recent distributional representations of
words exhibit semantic consistency over
directional metrics such as cosine simi-
larity. However, neither categorical nor
Gaussian observational distributions used
in existing topic models are appropriate to
leverage such correlations. In this paper,
we propose to use the von Mises-Fisher
distribution to model the density of words
over a unit sphere. Such a representation is
well-suited for directional data. We use a
Hierarchical Dirichlet Process for our base
topic model and propose an efficient infer-
ence algorithm based on Stochastic Vari-
ational Inference. This model enables us
to naturally exploit the semantic structures
of word embeddings while flexibly discov-
ering the number of topics. Experiments
demonstrate that our method outperforms
competitive approaches in terms of topic
coherence on two different text corpora
while offering efficient inference.1

1 Introduction

Prior work on topic modeling has mostly involved
the use of categorical likelihoods (Blei et al.,
2003; Blei and Lafferty, 2006; Rosen-Zvi et al.,
2004). Applications of topic models in the tex-
tual domain treat words as discrete observations,
ignoring the semantics of the language. Recent
developments in distributional representations of
words (Mikolov et al., 2013; Pennington et al.,

∗Authors contributed equally and listed alphabetically.
1Code is available at https://github.com/

Ardavans/sHDP.

2014) have succeeded in capturing certain seman-
tic regularities, but have not been explored exten-
sively in the context of topic modeling. In this pa-
per, we propose a probabilistic topic model with
a novel observational distribution that integrates
well with directional similarity metrics.

One way to employ semantic similarity is to
use the Euclidean distance between word vectors,
which reduces to a Gaussian observational distri-
bution for topic modeling (Das et al., 2015). The
cosine distance between word embeddings is an-
other popular choice and has been shown to be
a good measure of semantic relatedness (Mikolov
et al., 2013; Pennington et al., 2014). The von
Mises-Fisher (vMF) distribution is well-suited to
model such directional data (Dhillon and Sra,
2003; Banerjee et al., 2005) but has not been pre-
viously applied to topic models.

In this work, we use vMF as the observational
distribution. Each word can be viewed as a point
on a unit sphere with topics being canonical di-
rections. More specifically, we use a Hierarchi-
cal Dirichlet Process (HDP) (Teh et al., 2006), a
Bayesian nonparametric variant of Latent Dirich-
let Allocation (LDA), to automatically infer the
number of topics. We implement an efficient infer-
ence scheme based on Stochastic Variational Infer-
ence (SVI) (Hoffman et al., 2013).

We perform experiments on two different
English text corpora: 20 NEWSGROUPS and
NIPS and compare against two baselines - HDP
and Gaussian LDA. Our model, spherical HDP
(sHDP), outperforms all three systems on the mea-
sure of topic coherence. For instance, sHDP ob-
tains gains over Gaussian LDA of 97.5% on the
NIPS dataset and 65.5% on the 20 NEWSGROUPS
dataset. Qualitative inspection reveals consistent
topics produced by sHDP. We also empirically
demonstrate that employing SVI leads to efficient

537



topic inference.

2 Related Work

Topic modeling and word embeddings Das et
al. (2015) proposed a topic model which uses a
Gaussian distribution over word embeddings. By
performing inference over the vector representa-
tions of the words, their model is encouraged to
group words that are semantically similar, lead-
ing to more coherent topics. In contrast, we pro-
pose to utilize von Mises-Fisher (vMF) distribu-
tions which rely on the cosine similarity between
the word vectors instead of euclidean distance.

vMF in topic models The vMF distribution has
been used to model directional data by plac-
ing points on a unit sphere (Dhillon and Sra,
2003). Reisinger et al. (2010) propose an admix-
ture model that uses vMF to model documents rep-
resented as vector of normalized word frequen-
cies. This does not account for word level seman-
tic similarities. Unlike their method, we use vMF
over word embeddings. In addition, our model is
nonparametric.

Nonparametric topic models HDP and its vari-
ants have been successfully applied to topic mod-
eling (Paisley et al., 2015; Blei, 2012; He et al.,
2013); however, all these models assume a cate-
gorical likelihood in which the words are encoded
as one-hot representation.

3 Model

In this section, we describe the generative process
for documents. Rather than one-hot representa-
tion of words, we employ normalized word em-
beddings (Mikolov et al., 2013) to capture seman-
tic meanings of associated words. Word n from
document d is represented by a normalized M -
dimensional vector xdn and the similarity between
words is quantified by the cosine of angle between
the corresponding word vectors.

Our model is based on the Hierarchical Dirich-
let Process (HDP). The model assumes a collec-
tion of “topics” that are shared across documents
in the corpus. The topics are represented by the
topic centers µk ∈ RM . Since word vectors are
normalized, the µk can be viewed as a direction on
unit sphere. Von Mises−Fisher (vMF) is a distri-
bution that is commonly used to model directional
data. The likelihood of the topic k for word xdn

D

xdn

zdn

⇡d �

µk,k

1

�⇤

↵

'dn

 k,�k

�

✓d

Nd

(µ0, C0)
(m,�)

Figure 1: Graphical representation of our spheri-
cal HDP (sHDP) model. The symbol next to each
random variable denotes the parameter of its vari-
ational distribution. We assume D documents in
the corpus, each document contains Nd words and
there are countably infinite topics represented by
(µk, κk).

is:

f(xdn;µk;κk) = exp
(
κkµ

T
k xdn

)
CM (κk)

where κk is the concentration of the topic k, the
CM (κk) := κ

M/2−1
k /

(
(2π)M/2IM/2−1(κk)

)
is

the normalization constant, and Iν(·) is the mod-
ified Bessel function of the first kind at order ν.
Interestingly, the log-likelihood of the vMF is pro-
portional to µTk xdn (up to a constant), which is
equal to the cosine distance between two vectors.
This distance metric is also used in Mikolov et al.
(2013) to measure semantic proximity.

When sampling a new document, a subset of
topics determine the distribution over words. We
let zdn denote the topic selected for the word n of
document d. Hence, zdn is drawn from a categori-
cal distribution: zdn ∼ Mult(πd), where πd is the
proportion of topics for document d. We draw πd
from a Dirichlet Process which enables us to esti-
mate the the number of topics from the data. The
generative process for the generation of new doc-
ument is as follows:

β ∼ GEM(γ) πd ∼ DP(α, β)
κk ∼ log-Normal(m,σ2) µk ∼ vMF(µ0, C0)
zdn ∼ Mult(πd) xdn ∼ vMF(µk, κk)
where GEM(γ) is the stick-breaking distribution
with concentration parameter γ, DP(α, β) is a
Dirichlet process with concentration parameter α
and stick proportions β (Teh et al., 2012). We use

538



log-normal and vMF as hyper-prior distributions
for the concentrations (κk) and centers of the top-
ics (µk) respectively. Figure 1 provides a graphical
illustration of the model.

Stochastic variational inference In the rest of
the paper, we use bold symbols to denote the vari-
ables of the same kind (e.g., xd = {xdn}n,
z := {zdn}d,n). We employ stochastic variational
mean-field inference (SVI) (Hoffman et al., 2013)
to estimate the posterior distributions of the latent
variables. SVI enables us to sequentially process
batches of documents which makes it appropriate
in large-scale settings.

To approximate the posterior distribution of the
latent variables, the mean-field approach finds the
optimal parameters of the fully factorizable q (i.e.,
q(z, β,π,µ,κ) := q(z)q(β)q(π)q(µ)q(κ)) by
maximizing the Evidence Lower Bound (ELBO),

L(q) = Eq [log p(X, z, β,π,µ,κ)]− Eq [log q]
where Eq[·] is expectation with respect to q,
p(X, z, β,π,µ,κ) is the joint likelihood of the
model specified by the HDP model.

The variational distributions for z,π,µ have
the following parametric forms,

q(z) = Mult(z|ϕ)
q(π) = Dir(π|θ)
q(µ) = vMF(µ|ψ,λ),

where Dir denotes the Dirichlet distribution and
ϕ,θ,ψ and λ are the parameters we need to op-
timize the ELBO. Similar to (Bryant and Sud-
derth, 2012), we view β as a parameter; hence,
q(β) = δβ∗(β). The prior distribution κ does not
follow a conjugate distribution; hence, its poste-
rior does not have a closed-form. Since κ is only
one dimensional variable, we use importance sam-
pling to approximate its posterior. For a batch size
of one (i.e., processing one document at time), the
update equations for the parameters are:

ϕdwk ∝ exp{Eq[log vMF(xdw|ψk, λk)]
+ Eq[log πdk]}

θdk ← (1− ρ)θdk + ρ(αβk +D
W∑
n=1

ωwjϕdwk)

t← (1− ρ)t+ ρs(xd, ϕdk)
ψ ← t/‖t‖2, λ← ‖t‖2

where D, ωwj , W , ρ are the total number of docu-
ments, number of word w in document j, the total

number of words in the dictionary, and the step
size, respectively. t is a natural parameter for vMF
and s(xd, ϕdk) is a function computing the suffi-
cient statistics of vMF distribution of the topic k.
We use numerical gradient ascent to optimize for
β∗. For exact forms of Eq log[vMF(xdw|ψk, λk)]
and Eq[log πdk], see Appendix.

4 Experiments

Setup We perform experiments on two different
text corpora: 11266 documents from 20 NEWS-
GROUPS2 and 1566 documents from the NIPS cor-
pus3. We utilize 50-dimensional word embeddings
trained on text from Wikipedia using word2vec4.
The vectors are normalized to have unit `2-norm,
which has been shown to provide superior perfor-
mance (Levy et al., 2015)).

We evaluate our model using the measure of
topic coherence (Newman et al., 2010), which has
been shown to effectively correlate with human
judgement (Lau et al., 2014). For this, we com-
pute the Pointwise Mutual Information (PMI) us-
ing a reference corpus of 300k documents from
Wikipedia. The PMI is calculated using co-
occurence statistics over pairs of words (ui, uj)
in 20-word sliding windows:

PMI(ui, uj) = log
p(ui, uj)

p(ui) · p(uj)
Additionally, we also use the metric of normalized
PMI (NPMI) to evaluate the models in a similar
fashion:

NPMI(ui, uj) =
log p(ui,uj)p(ui)·p(uj)
− log p(ui, uj)

We compare our model with two baselines: HDP
and the Gaussian LDA model. We ran G-LDA
with various number of topics (k).

Results Table 2 details the topic coherence av-
eraged over all topics produced by each model.
We observe that our sHDP model outperforms G-
LDA by 0.08 points on 20 NEWSGROUPS and by
0.17 points in terms of PMI on the NIPS dataset.
The NPMI scores also show a similar trend with
sHDP obtaining the best scores on both datasets.
We can also see that the individual topics inferred

2http://qwone.com/˜jason/20Newsgroups/
3http://www.cs.nyu.edu/˜roweis/data.

html
4https://code.google.com/p/word2vec/

539



Gaussian LDA
vector shows network hidden performance net figure size
image feature learning term work references shown average

gaussian show model rule press introduction neurons present
equation motion neural word tion statistical point family

generalization action input means ing related large versus
images spike data words eq comparison neuron spread
gradient series function approximate performed source small median
theory final time derived em statistics fig physiology

dimensional robot set describe vol free cells children
1.16 0.4 0.35 0.29 0.25 0.25 0.21 0.2

Spherical HDP
neural function analysis press pattern problem noise algorithm
layer linear theory cambridge fig process gradient error

neurons functions computational journal temporal method propagation parameters
neuron vector statistical vol shape optimal signals computation

activation random field eds smooth solution frequency algorithms
brain probability simulations trans surface complexity feedback compute
cells parameter simulation springer horizontal estimation electrical binary
cell dimensional nonlinear volume vertical prediction filter mapping

synaptic equation dynamics review posterior solve detection optimization
1.87 1.73 1.51 1.44 1.41 1.19 1.12 1.03

Table 1: Examples of top words for the most coherent topics (column-wise) inferred on the NIPS dataset
by Gaussian LDA (k=40) and Spherical HDP. The last row for each model is the topic coherence (PMI)
computed using Wikipedia documents as reference.

Model
Topic Coherence

20 NEWS NIPS
pmi npmi pmi npmi

HDP 0.037 0.014 0.270 0.062
G-LDA (k=10) -0.061 -0.006 0.214 0.055
G-LDA (k=20) -0.017 0.001 0.215 0.052
G-LDA (k=40) 0.052 0.015 0.248 0.057
G-LDA (k=60) 0.082 0.021 0.137 0.034

sHDP 0.162 0.046 0.442 0.102

Table 2: Average topic coherence for various base-
lines (HDP, Gaussian LDA (G-LDA)) and sHDP.
k=number of topics. Best scores are shown in
bold.

by sHDP make sense qualitatively and have higher
coherence scores than G-LDA (Table 1). This sup-
ports our hypothesis that using the vMF likelihood
helps in producing more coherent topics. sHDP
produces 16 topics for the 20 NEWSGROUPS and
92 topics on the NIPS dataset.

Figure 2 shows a plot of normalized log-
likelihood against the runtime of sHDP and G-
LDA.5 We calculate the normalized value of log-
likelihood by subtracting the minimum value from
it and dividing it by the difference of maximum

5Our sHDP implementation is in Python and the G-LDA
code is in Java.

4 6 8 10 12 14 16
Seconds (log)

0

20

40

60

80

100

N
or

m
al

iz
ed

Lo
g-

Li
ke

lih
oo

d
(%

)

G-LDA
sHDP

Figure 2: Normalized log-likelihood (in percent-
age) over a training set of size 1566 documents
from the NIPS corpus. Since the log-likelihood
values are not comparable for the Gaussian LDA
and the sHDP, we normalize them to demon-
strate the convergence speed of the two inference
schemes for these models.

and minimum values. We can see that sHDP con-
verges faster than G-LDA, requiring only around
five iterations while G-LDA takes longer to con-
verge.

5 Conclusion

Classical topic models do not account for semantic
regularities in language. Recently, distributional

540



representations of words have emerged that exhibit
semantic consistency over directional metrics like
cosine similarity. Neither categorical nor Gaussian
observational distributions used in existing topic
models are appropriate to leverage such correla-
tions. In this work, we demonstrate the use of the
von Mises-Fisher distribution to model words as
points over a unit sphere. We use HDP as the base
topic model and propose an efficient algorithm
based on Stochastic Variational Inference. Our
model naturally exploits the semantic structures
of word embeddings while flexibly inferring the
number of topics. We show that our method out-
performs three competitive approaches in terms of
topic coherence on two different datasets.

Acknowledgments

Thanks to Rajarshi Das for helping with the Gaus-
sian LDA experiments and Matthew Johnson for
his help with the HDP code.

References
Arindam Banerjee, Inderjit S Dhillon, Joydeep Ghosh,

and Suvrit Sra. 2005. Clustering on the unit hyper-
sphere using von mises-fisher distributions. In Jour-
nal of Machine Learning Research, pages 1345–
1382.

David M Blei and John D Lafferty. 2006. Dynamic
topic models. In Proceedings of the 23rd interna-
tional conference on Machine learning, pages 113–
120. ACM.

David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet allocation. the Journal of ma-
chine Learning research, 3:993–1022.

David M Blei. 2012. Probabilistic topic models. Com-
munications of the ACM, 55(4):77–84.

Michael Bryant and Erik B Sudderth. 2012. Truly
nonparametric online variational inference for hier-
archical dirichlet processes. In Advances in Neural
Information Processing Systems, pages 2699–2707.

Rajarshi Das, Manzil Zaheer, and Chris Dyer. 2015.
Gaussian LDA for topic models with word embed-
dings. In Proceedings of the 53nd Annual Meeting
of the Association for Computational Linguistics.

Inderjit S Dhillon and Suvrit Sra. 2003. Modeling data
using directional distributions. Technical report,
Technical Report TR-03-06, Department of Com-
puter Sciences, The University of Texas at Austin.
URL ftp://ftp. cs. utexas. edu/pub/techreports/tr03-
06. ps. gz.

Siddarth Gopal and Yiming Yang. 2014. Von mises-
fisher clustering models.

Yulan He, Chenghua Lin, Wei Gao, and Kam-Fai
Wong. 2013. Dynamic joint sentiment-topic model.
ACM Transactions on Intelligent Systems and Tech-
nology (TIST), 5(1):6.

Matthew D Hoffman, David M Blei, Chong Wang, and
John Paisley. 2013. Stochastic variational infer-
ence. The Journal of Machine Learning Research,
14(1):1303–1347.

Matthew Johnson and Alan Willsky. 2014. Stochastic
variational inference for bayesian time series mod-
els. In Proceedings of the 31st International Confer-
ence on Machine Learning (ICML-14), pages 1854–
1862.

Jey Han Lau, David Newman, and Timothy Baldwin.
2014. Machine reading tea leaves: Automatically
evaluating topic coherence and topic model quality.
In EACL, pages 530–539.

Omer Levy, Yoav Goldberg, and Ido Dagan. 2015. Im-
proving distributional similarity with lessons learned
from word embeddings. Transactions of the Associ-
ation for Computational Linguistics, 3:211–225.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in neural information processing
systems, pages 3111–3119.

David Newman, Jey Han Lau, Karl Grieser, and Tim-
othy Baldwin. 2010. Automatic evaluation of
topic coherence. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 100–108. Association for Computa-
tional Linguistics.

John Paisley, Chingyue Wang, David M Blei, and
Michael I Jordan. 2015. Nested hierarchical dirich-
let processes. Pattern Analysis and Machine Intelli-
gence, IEEE Transactions on, 37(2):256–270.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. In Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1532–
1543.

Joseph Reisinger, Austin Waters, Bryan Silverthorn,
and Raymond J Mooney. 2010. Spherical topic
models. In Proceedings of the 27th International
Conference on Machine Learning (ICML-10), pages
903–910.

Michal Rosen-Zvi, Thomas Griffiths, Mark Steyvers,
and Padhraic Smyth. 2004. The author-topic model
for authors and documents. In Proceedings of the
20th conference on Uncertainty in artificial intelli-
gence, pages 487–494. AUAI Press.

Yee Whye Teh, Michael I Jordan, Matthew J Beal, and
David M Blei. 2006. Hierarchical dirichlet pro-
cesses. Journal of the American Statistical Associa-
tion, 101:1566–1581.

541



Yee Whye Teh, Michael I Jordan, Matthew J Beal, and
David M Blei. 2012. Hierarchical dirichlet pro-
cesses. Journal of the american statistical associ-
ation.

Appendinx

Mean field update equations

In this section, we provide the mean field update
equations. The SVI update equations can be de-
rived from the mean field update (Hoffman et al.,
2013).

The following term is computed for the update
equations:

Eq[log vMF(xdn|µk, κk)] = Eq[logCM (κk)]+
Eq[κk]xTdnEq[µk]

where CM (·) is explained in Section 3. The
difficulty here lies in computing Eq[κk] and
Eq[CM (κk)]. However, κ is a scalar value. Hence,
to compute Eq[κk], we divide a reasonable interval
of κk into grids and compute the weight for each
grid point as suggested by Gopal and Yang (2014):

p(κk|· · ·) ∝ exp (nk logCM (κk)+

κk

(
D∑
d=1

Nd∑
n=1

[ϕdn]k 〈xdn,Eq[µk]〉
))
×

logNormal(κk|m,σ2)

where nk =
∑D

d=1

∑Nd
d=1 [ϕdn]k and [a]k denotes

the k’th element of vector a. After computing
the normalized weights, we can compute Eq[κk]
or expectation of any other function of κk (e.g.,
Eq[CM (κk)]). The rest of the terms can be com-
puted as follows:

Eq[µk] = Eq

[
IM/2(κk)
IM/2−1(κk)

]
ψk,

ψk = Eq[κk]

(
D∑
d=1

Nd∑
n=1

[ϕdn]kxdn

)
+ C0µ0

ψk ← ψk‖ψk‖2 ,

[Eq[log(πd)]]k = Ψ([θd]k)−Ψ
(∑

k

[θd]k

)
,

[ϕdn]k ∝ exp (Eq[log vMF(xdn|µk, κk)] + Eq[log([πd]k)]) ,

[θd]k =α+
Nd∑
n=1

[ϕdn]k

Ψ(·) is the digamma function.

To find β∗, similar to Johnson and Willsky
(2014), we use the gradient expression of ELBO
with respect to β and take a truncated gradient step
on β ensuring β∗ ≥ 0.

542


