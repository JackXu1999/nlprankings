



















































Recognizing Salient Entities in Shopping Queries


Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 107–111,
Berlin, Germany, August 7-12, 2016. c©2016 Association for Computational Linguistics

Recognizing Salient Entities in Shopping Queries

Zornitsa Kozareva, Qi Li, Ke Zhai and Weiwei Guo
Yahoo!

701 First Avenue
Sunnyvale, CA 94089

zornitsa@kozareva.com
{lqi|kzhai}@yahoo-inc.com
weiwei@cs.columbia.edu

Abstract

Over the past decade, e-Commerce has
rapidly grown enabling customers to pur-
chase products with the click of a button.
But to be able to do so, one has to under-
stand the semantics of a user query and
identify that in digital lifestyle tv, digital
lifestyle is a brand and tv is a product.

In this paper, we develop a series of struc-
tured prediction algorithms for seman-
tic tagging of shopping queries with the
product, brand, model and product family
types. We model wide variety of features
and show an alternative way to capture
knowledge base information using embed-
dings. We conduct an extensive study over
37, 000 manually annotated queries and
report performance of 90.92 F1 indepen-
dent of the query length.

1 Introduction

Recent study shows that yearly e-Commerce sales
in the U.S. top 100 Billion (Fulgoni, 2014). This
leads to substantially increased interest in build-
ing semantic taggers that can accurately recognize
product, brand, model and product family types in
shopping queries to better understand and match
the needs of online shoppers.

Despite the necessity for semantic understand-
ing, yet most widely used approaches for prod-
uct retrieval categorize the query and the offer
(Kozareva, 2015) into a shopping taxonomy and
use the predicted category as a proxy for retrieving
the relevant products. Unfortunately, such proce-
dure falls short and leads to inaccurate product re-
trieval. Recent efforts (Manshadi and Li, 2009; Li,
2010) focused on building CRF taggers that recog-
nize basic entity types in shopping query such as
brands, types and models. (Li, 2010) conducted

a study over 4000 shopping queries and showed
promising results when huge knowledge bases are
present. (Paşca and Van Durme, 2008; Kozareva et
al., 2008; Kozareva and Hovy, 2010) focused on
using Hearst patterns (Hearst, 1992) to learn se-
mantic lexicons. While such methods are promis-
ing, they cannot be used to recognize all prod-
uct entities in a query. In parallel to the semantic
query understanding task, there have been seman-
tic tagging efforts on the product offer side. (Put-
thividhya and Hu, 2011) recognize brand, size and
color entities in eBay product offers, while (Kan-
nan et al., 2011) recognized similar fields in Bing
product catalogs.

Despite these efforts, to date there are three im-
portant questions, which have not been answered,
but we address in our work. (1) What is an alter-
native method when product knowledge bases are
not present? (2) Is the performance of the seman-
tic taggers agnostic to the query length? (3) Can
we minimize manual feature engineering for shop-
ping query log tagging using neural networks?

The main contributions of the paper are:

• Building semantic tagging framework for
shopping queries.

• Leveraging missing knowledge base entries
through word embeddings learned on large
amount of unlabeled query logs.

• Annotating 37, 000 shopping queries with
product, brand, model and product family en-
tity types.

• Conducting a comparative and efficiency
study of multiple structured prediction algo-
rithms and settings.

• Showing that long short-term memory net-
works reaches the best performance of 90.92
F1 and is agnostic to query length.

107



2 Problem Formulation and Modeling

2.1 Task Definition
We define our task as given a shopping query
identify and classify all segments that are prod-
uct, brand, product family and model, where:
-Product is generic term(s) for goods not specific
to a particular manufacturer (e.g. shirts).
-Brand is the actual name of the product manu-
facturer (e.g. Calvin Klein).
-Product Family is a brand-specific grouping of
products sharing the same product (e.g. Samsung
Galaxy).
-Model is used by manufacturer to distinguish
variations (e.g. for the brand Lexus has IS prod-
uct family, which has model 200t and 300 F
Sport).

For modeling, we denote with T =
{⊥, t1, t2, . . . , tK} the whole label space,
where ⊥ indicates a word that is not a part of an
entity and ti stands for an entity category. The
tagging models have to recognize the following
types product, brand, model, product family and⊥
(other) using the BIO schema (Tjong Kim Sang,
2002).

We denote as x = (x1, x2, . . . , xM ) a shopping
query of length M . The objective is to find the
best configuration ŷ such that:

ŷ = arg maxy p(y|x),

where y=(y1, y2, ..., yN ) (N ≤ M ) are the shop-
ping query segments labeled with their corre-
sponding entity category. Each segment yi cor-
responds to a triple 〈bi, ei, ti〉 indicating the start
index bi and end index ei of the sequence followed
by the entity category ti ∈ T . When ti = ⊥, the
segment contains only one word.

2.2 Structured Prediction Models

To tackle the shopping tagging problem of query
logs, we use Conditional Random Fields (Lafferty
et al., 2001, CRF)1, learning to search (Daumé III
et al., 2009, SEARN)2, structured percep-
tron (Collins, 2002, STRUCTPERCEPTRON) and
a long short-term memory networks extended by
CRF layer (Hochreiter and Schmidhuber, 1997;
Graves, 2012, LSTM-CRF).
CRF: is a popular algorithms for sequence tag-
ging tasks (Lafferty et al., 2001). The objective is

1taku910.github.io/crfpp/
2github.com/JohnLangford/vowpal_wabbit

to find the label sequence y = (y1, ..., yM ) that
maximizes

p(y|x) = 1Zλ(x) exp{λ · f(y,x)},
where Zλ(x) is the normalization factor, λ is the
weight vector and f(y,x) is the extracted feature
vector for the observed sequence x.
SEARN is a powerful structured prediction al-
gorithm, which formulates the sequence labeling
problem as a search process. The objective is to
find the label sequence y = (y1, ..., yM ) that max-imizes

p(y|x) ∝∑Mm=1 I[C(x,y1,...,ym−1)=ŷm],
whereC(•) is a cost sensitive multiclass classifier
and ŷ are the ground-truth labels.
STRUCTPERCEPTRON is an extension of the
standard perceptron. In our setting we model a
segment-based search algorithm, where each unit
is a segment of x (e.g., 〈bi, ei〉), rather than a sin-
gle word (e.g., xi). The objective is to find the
label sequence y = (y1, ..., yM ) that maximizes

p(y|x) ∝ w> · f(x,y),
where f(x,y) represents the feature vector for in-
stance x along with the configuration y and w is
updated as w← w + f(x, ŷ)− f(x,y).
LSTM-CRF The above algorithms heavily rely
on manually-crafted features to perform sequence
tagging. We decided to alleviate that by using
long short-term memory networks with a CRF
layer. Our model is similar to R-CRF (Mesnil et
al., 2015), but for the hidden recurrent layer we
use LSTM (Hochreiter and Schmidhuber, 1997;
Graves, 2012). We denote with hi the hidden vec-
tor produced by the LSTM cell at i-th token. Then
the conditional probability of y given a query x
becomes:

p(y|x) = 1Z(h) exp{
∑

i(W
h
yihi + W

t
yi,yi−1)},

where W hyi is the weight vector corresponding to
label yi, and W tyi,yi−1 is the transition score cor-
responding to yi and yi−1. During training, the
values of W h, W t, the LSTM layer and the input
word embeddings are updated through the stan-
dard back-propagation with AdaGrad algorithm.
We also concatenate pre-trained word embedding
and randomly initialized embedding (50-d) for the
knowledge-base types of each token and use this
information in the input layer. In our experiments,
we set the learning rate to 0.05 and take each query
as a mini-batch and run 5 epochs to finish training.

108



Features CRF SEARN STRUCTPERCEPTRONP (%) R (%) F1 P (%) R (%) F1 P (%) R (%) F1
POS 39.86 35.51 37.56 34.97 33.55 34.25 33.03 24.70 28.27
KB 51.64 41.08 45.76 41.96 37.26 39.47 35.70 35.97 35.84
WE 65.31 61.02 63.11 67.58 67.00 67.29 71.29 68.12 69.67

LEX+ORTO+PSTNL + POS + KB 86.49 83.84 85.15 84.19 84.30 84.24 88.88 86.87 87.87
LEX+ORTO+PSTNL + POS + WE 88.30 85.74 87.00 84.32 84.15 84.24 87.85 85.69 86.76

LEX+ORTO+PSTNL + POS + KB + WE 88.86 86.29 87.55 84.30 84.50 84.40 89.18 87.10 88.13

Table 1: Results from feature study.

2.3 Features

Lexical (LEX): are widely used N -gram features.
We use unigrams of the current w0, previous w−1
and next w+1 words, and bigrams w−1w0 and
w0w+1.
Orthographic (ORTO): are binary mutually non-
exclusive features that check if w0, w−1 and w+1
contain all-digits, any-digit, start-with-digit-end-
in-letter and start-with-letter-end-in-digit. They
are designed to capture model names like hero3
and m560.
Positional (PSTNL): are discrete features model-
ing the position of the words in the query. They
capture the way people tend to write products and
brands in the query.
Part-of-Speech (POS): capture nouns and proper
names to better recognize products and brands.
We use Stanford tagger (Toutanova et al., 2003).
Knowledgebase (KB): are powerful semantic fea-
tures (Tjong Kim Sang, 2002; Carreras et al.,
2002; Passos et al., 2014). We automatically
collected and manually validated 200K brands,
products, models and product families items ex-
tracted from Macy’s and Amazon websites.
WordEmbeddings (WE): While external knowl-
edge bases are great resource, they are expensive
to create and time-consuming to maintain. We use
word embeddings (Mikolov et al., 2013) 3 as a
cheap low-maintenance alternative for knowledge
base construction. We train the embeddings over
2.5M unlabeled shopping queries. For each token
in the query, we use as features the 200 dimen-
sional embeddings of the top 5 most similar terms
returned by cosine similarity.

3 Experiments and Results

Data Set To the best of our knowledge, there is
no publicly available shopping query data anno-
tated with product, brand, model, product family
and other categories. To conduct our experiments,
we collect 2.5M shopping queries through click

3https://code.google.com/p/word2vec/

logs (Hua et al., 2013). We randomly sampled
37, 000 unique queries from the head, torso and
tail of a commercial web search engine and asked
two independent annotators to tag the data. We
measured the Kappa agreement of the editors and
found .92 agreement, which is sufficient to warrant
the goodness of the annotations.

We randomly split the data into 80% for training
and 20% for testing. Table 2 shows the distribution
of the entity categories in the data.

Product Brand Model Prod. Family ⊥
Train 21,688 10,417 4,394 6,697 47,517
Test 5,413 2,659 1,099 1,716 11,780

Table 2: Entity category distribution.

We tune all parameters on the training set using
5-fold cross validation and report performance on
the test set. All results are calculated with the
CONLL evaluation script4.

Performance w.r.t. Features Table 1 shows the
performance of the different models and feature
combinations. We use the individual features as a
baseline. The obtained results show that these are
insufficient to solve such a complex task. We com-
pared the performance of the KB and WE features
when combined with (LEX+ORTO+PSTNL) infor-
mation. As we can see, both KB and WE reach
comparable performance. This study shows that
training embeddings on large in-domain data of
shopping queries is a reliable and cheap source
for knowledge base construction, when such in-
formation is not present. In our study the best
performance is reached when all features are com-
bined. Among all machine learning classifiers for
which we manually designed features, structured
perception reaches the best performance of 88.13
F1 score.

In addition to the feature combination and
model comparison, we also study in Figure 1 the
training time of each model in log scale against its
F1 score. SEARN is the fastest algorithm to train,

4cnts.ua.ac.be/conll2000/chunking/
conlleval.txt

109



Category CRF SEARN STRUCTPERCEPTRON LSTM-CRFP (%) R (%) F1 P (%) R (%) F1 P (%) R (%) F1 P (%) R (%) F1
brand 91.79 87.93 89.82 89.3 89.3 89.3 93.99 91.20 92.57 95.15 92.29 93.70
model 86.28 80.71 83.40 80.7 78.9 79.8 85.56 80.89 83.16 87.25 85.90 86.57

product 87.85 88.16 88.00 83.4 85.0 84.2 87.90 87.92 87.91 91.94 90.98 91.46
product family 89.27 81.41 85.16 81.4 79.0 80.2 88.12 82.17 85.04 87.98 87.47 87.73

Overall 88.86 86.29 87.55 84.3 84.5 84.4 89.18 87.10 88.13 91.61 90.24 90.92

Table 3: Per category performance.

●

●

●

82

84

86

88

100 1000 10000
training time (second)

f−
1 

sc
or

e

features

●

LEX+ORTO+PSTNL

LEX+ORTO+PSTNL+KB

LEX+ORTO+PSTNL+KB+WE

LEX+ORTO+PSTNL+POS

LEX+ORTO+PSTNL+POS+KB

LEX+ORTO+PSTNL+POS+KB+WE

LEX+ORTO+PSTNL+POS+WE

LEX+ORTO+PSTNL+WE

models

● ● ●CRF++ Searn StructPerceptron

Figure 1: Training time vs F1 performance.

while CRF takes the longest time to train. Among
all STRUCTPERCEPTRON offers the best balance
between efficiency and performance in a real time
setting.

Performance w.r.t. Entity Category Table 3
shows the performance of the algorithms with the
manually designed features against the automati-
cally induced ones with LSTM-CRF. We show
the performance of each individual product en-
tity category. Compared to all models and set-
tings, LSTM-CRF reaches the best performance
of 90.92 F1 score. The most challenging entity
types are product family and model, due to their
“wild” and irregular nature.

Performance w.r.t. Query Length Finally, we
also study the performance of our approach with
respect to the different query length. Figure 2
shows the F1 score of the two best performing al-
gorithms LSTM-CRF and STRUCTPERCEPTRON
against the different query length in the test set.
Around 83% of the queries have length between 2
to 5 words, the rest are either very short or very
long ones. As it can be seen in Figure 2, inde-
pendent of the query length, our models reach the
same performance for short and long queries. This
shows that the models are robust and agnostic to

the query length.

1 2 3 4 5 6 7 8 9 10
query length

0

500

1000

1500

2000

2500

n
u
m

b
e
r 

o
f 

q
u
e
ri

e
s

# of queries

0

20

40

60

80

100

F-
1
 s

co
re

Structured Percetron
LSTM_CRF

Figure 2: F1 performance with varying query length.

4 Conclusions and Future Work

In this work, we have defined the task of prod-
uct entity recognition in shopping queries. We
have studied the performance of multiple struc-
tured prediction algorithms to automatically rec-
ognize product, brand, model and product family
entities. Our comprehensive experimental study
and analysis showed that combining lexical, po-
sitional, orthographic, POS, knowledge base and
word embedding features leads to the best perfor-
mance. We showed that word embeddings trained
on large amount of unlabeled queries could sub-
stitute knowledge bases when they are missing
for specialized domains. Among all manually
designed feature classifiers STRUCTPERCEPTRON
reached the best performance. While among all
algorithms LSTM-CRF achieved the highest per-
formance of 90.92 F1 score. Our analysis showed
that our models reach robust performance inde-
pendent of the query length. In the future we plan
to tackle attribute identification to better under-
stand queries like “diamond shape emerald ring”,
where diamond shape is a cut and emerald is
a gemstone type. Such fine-grained information
could further enrich online shopping experience.

110



References
Xavier Carreras, Lluı́s Màrques, and Lluı́s Padró.

2002. Named entity extraction using adaboost. In
Proceedings of CoNLL-2002, pages 167–170.

Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proc. EMNLP,
pages 1–8.

Hal Daumé III, John Langford, and Daniel Marcu.
2009. Search-based structured prediction. Machine
Learning, 75(3):297–325.

Gian Fulgoni. 2014. State of the US retail economy in
q1 2014. In Comscore, Technical Report.

Alex Graves. 2012. Supervised Sequence Labelling
with Recurrent Neural Networks, volume 385 of
Studies in Computational Intelligence. Springer.

Marti Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proc. of the
14th conference on Computational linguistics, pages
539–545.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural Computation,
9(8):1735–1780.

Xian-Sheng Hua, Linjun Yang, Jingdong Wang, Jing
Wang, Ming Ye, Kuansan Wang, Yong Rui, and Jin
Li. 2013. Clickage: Towards bridging semantic and
intent gaps via mining click logs of search engines.
In Proceedings of the 21st ACM International Con-
ference on Multimedia, MM ’13, pages 243–252.

Anitha Kannan, Inmar E. Givoni, Rakesh Agrawal,
and Ariel Fuxman. 2011. Matching unstructured
product offers to structured product specifications.
In Proceedings of the 17th ACM SIGKDD Inter-
national Conference on Knowledge Discovery and
Data Mining, KDD ’11, pages 404–412.

Zornitsa Kozareva and Eduard Hovy. 2010. Learning
arguments and supertypes of semantic relations us-
ing recursive patterns. In Proceedings of the 48th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 1482–1491.

Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy.
2008. Semantic class learning from the web with
hyponym pattern linkage graphs. In Proceedings of
ACL-08: HLT, pages 1048–1056.

Zornitsa Kozareva. 2015. Everyone likes shopping!
multi-class product categorization for e-commerce.
In Proceedings of the 2015 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 1329–1333.

John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In icml, pages 282–289.

Xiao Li. 2010. Understanding the semantic struc-
ture of noun phrase queries. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 1337–1345.

Mehdi Manshadi and Xiao Li. 2009. Semantic tagging
of web search queries. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natu-
ral Language Processing of the AFNLP: Volume 2 -
Volume 2, pages 861–869.

Grégoire Mesnil, Yann Dauphin, Kaisheng Yao,
Yoshua Bengio, Li Deng, Dilek Z. Hakkani-Tür,
Xiaodong He, Larry P. Heck, Gökhan Tür, Dong
Yu, and Geoffrey Zweig. 2015. Using recurrent
neural networks for slot filling in spoken language
understanding. IEEE/ACM Transactions on Audio,
Speech & Language Processing, 23(3):530–539.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeffrey Dean. 2013. Distributed represen-
tations of words and phrases and their composition-
ality. volume abs/1310.4546, pages 3111–3119.

Marius Paşca and Benjamin Van Durme. 2008.
Weakly-supervised acquisition of open-domain
classes and class attributes from web documents and
query logs. In Proceedings of ACL-08: HLT, pages
19–27.

Alexandre Passos, Vineet Kumar, and Andrew McCal-
lum. 2014. Lexicon infused phrase embeddings for
named entity resolution. CoRR, abs/1404.5367.

Duangmanee (Pew) Putthividhya and Junling Hu.
2011. Bootstrapped named entity recognition for
product attribute extraction. In Proceedings of the
2011 Conference on Empirical Methods in Natural
Language Processing, pages 1557–1567.

Erik F. Tjong Kim Sang. 2002. Introduction to
the conll-2002 shared task: Language-independent
named entity recognition. In Proceedings of
CoNLL-2002, pages 155–158.

Kristina Toutanova, Dan Klein, Christopher D Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology
- Volume 1, pages 173–180.

111


