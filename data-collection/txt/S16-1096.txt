



















































WOLVESAAR at SemEval-2016 Task 1: Replicating the Success of Monolingual Word Alignment and Neural Embeddings for Semantic Textual Similarity


Proceedings of SemEval-2016, pages 634–639,
San Diego, California, June 16-17, 2016. c©2016 Association for Computational Linguistics

WOLVESAAR at SemEval-2016 Task 1:
Replicating the Success of Monolingual Word Alignment and

Neural Embeddings for Semantic Textual Similarity

Hanna Bechara, Rohit Gupta, Liling Tan,
Constantin Orăsan, Ruslan Mitkov, Josef van Genabith

University of Wolverhampton / UK,
Universität des Saarlandes / Germany

Deutsches Forschungszentrum für Künstliche Intelligenz / Germany
{hanna.bechara, r.gupta, corsasan, r.mitkov}@wlv.ac.uk,
liling.tan@uni-saarland.de, josef.van genabith@dfki.de

Abstract

This paper describes the WOLVESAAR sys-
tems that participated in the English Seman-
tic Textual Similarity (STS) task in SemEval-
2016. We replicated the top systems from
the last two editions of the STS task and ex-
tended the model using GloVe word embed-
dings and dense vector space LSTM based
sentence representations. We compared the
difference in performance of the replicated
system and the extended variants. Our vari-
ants to the replicated system show improved
correlation scores and all of our submissions
outperform the median scores from all partic-
ipating systems.

1 Introduction

Semantic Textual Similarity (STS) is the task of as-
signing a real number score to quantify the semantic
likeness of two text snippets. Similarity measures
play a crucial role in various areas of text processing
and translation technologies ranging from improv-
ing information retrieval rankings (Lin and Hovy,
2003; Corley and Mihalcea, 2005) and text summa-
rization to machine translation evaluation and en-
hancing matches in translation memory and termi-
nologies (Resnik and others, 1999; Ma et al., 2011;
Banchs et al., 2015; Vela and Tan, 2015).

The annual SemEval STS task (Agirre et al.,
2012; Agirre et al., 2013; Agirre et al., 2014; Agirre
et al., 2015) provides a platform where systems are
evaluated on the same data and evaluation criteria.

2 DLS System from STS 2014 and 2015

For the past two editions of the STS task, the top per-
forming submissions are from the DLS@CU team
(Sultan et al., 2014b; Sultan et al., 2015).

Their STS2014 submission is based on the pro-
portion of overlapping content words between the
two sentences treating semantic similarity as a
monotonically increasing function of the degree to
which two sentences contain semantically similar
units and these units occur in similar semantic con-
texts (Sultan et al., 2014b). Essentially, their seman-
tic metric is based on the proportion of aligned con-
tent words between two sentences, formally defined
as:

prop
(1)
Al =

|{i : [∃j : (i, j) ∈ Al] and w(1)i ∈ C}|
|{i : w(1)i ∈ C}|

(1)
where prop(1)Al is the monotonic proportion of the

semantic unit alignment from a set of alignments Al
that maps the positions of the words (i, j) between
sentences S(1) and S(2), given that the aligned units
belong to a set of content words, C. Since the pro-
portion is monotonic, the equation above only pro-
vides the proportion of semantic unit alignments for
S(1). The Al alignments pairs are automatically an-
notated by a monolingual word aligner (Sultan et al.,
2014a) that uses word similarity measures based on
contextual evidence from the Paraphrase Database
(PPDB) (Ganitkevitch et al., 2013) and syntactic de-
pendencies.

The same computation needs to be made for S(2).
An easier formulation of the equation without the
formal logic symbols is:

634



prop
(1)
Al =

sum(1 for wi, wj in Al
(1,2) if wi in C)

sum(1 for wi in S(1) if wi in C)
(2)

Since the semantic similarity between (S(1), S(2))
should be a single real number, Sultan et al. (2014b)
combined the proportions using harmonic mean:

sim(S(1), S(2)) =
2 ∗ prop(1)Al ∗ prop

(2)
Al

prop
(1)
Al + prop

(2)
Al

(3)

Instead of simply using the alignment propor-
tions, Sultan et al. (2015) extended their hypothe-
sis by leveraging pre-trained neural net embeddings
(Baroni et al., 2014). They posited that the seman-
tics of the sentence can be captured by the centroid
of its content words1 computed by the element-wise
sum of the content word embeddings normalized by
the number of content words in the sentence. To-
gether with the similarity scores from Equation 3
and the cosine similarity between two sentence em-
beddings, they trained a Bayesian ridge regressor to
learn the similarity scores between text snippets.

3 Our Replica of DLS for STS 2016

To replicate the success of Sultan et al. (2014b), we
use the monolingual word aligner from Sultan et
al. (2014a) to annotate the STS-2012 to STS-2015
datasets and computed the alignment proportions as
in Equation 1 and 2.

In duplicating Sultan et al. (2015) work, we first
have to tokenize and lemmatize text. The details of
pre-processing choices was undocumented in their
paper, thus we lemmatized the datasets with the
NLTK tokenizer (Bird et al., 2009) and PyWSD lem-
matizer (Tan, 2014). We use the lemmas to retrieve
the word embeddings from the COMPOSES vector
space (Baroni et al., 2014). Similar to Equation 2
(changing only the numerator), we sum the sentence
embedding’s centroid as follows:

v(S(1)) =
sum(v(wi) for wi in S

(1) if wi in C)

sum(1 for wi in S(1) if wi in C)
(4)

1In the implementation, they have used lemmas instead of
words to reduce sparsity when looking up the pre-trained em-
beddings (personal communication with Arafat Sultan).

where v(S(1)) refers to the dense vector space
representation of the sentence S(1) and v(wi) refers
to the word embedding of word i provided by the
COMPOSES vector space. The same computation
has to be done for S(2).

Intuitively, if either of the sentences contains
more or less content words than the other, we can
see the numerator changing but the denominator
changes with it. The difference between v(S(1)) and
v(S(2)) contributes to distributional semantic dis-
tance.

To calculate a real value similarity score between
the sentence vectors, we take the dot product be-
tween the vectors to compute the cosine similarity
between the sentence vectors:

sim(S(1), S(2)) =
v(S(1)) · v(S(2))
|v(S(1))| |v(S(2))| (5)

There was no clear indication of which vector
space Sultan et al. (2015) have chosen to compute
the similarity score from Equation 5. Thus we com-
pute two similarity scores using both COMPOSES
vector spaces trained with these configurations:

• 5-word context window, 10 negative samples,
subsampling, 400 dimensions

• 2-word context window, PMI weighting, no
compression, 300K dimensions

In this case, we extracted two similarity features
for every sentence pair. With the harmonic pro-
portion feature from Equation 3 and the similarity
scores from Equation 5, we trained a boosted tree
ensemble on the 3 features using the STS 2012 to
2015 datasets and submitted the outputs from this
model as our baseline submission in the English STS
Task in SemEval 2016.

3.1 Replacing COMPOSES with GloVe

Pennington et al. (2014) handles semantic regulari-
ties (Levy et al., 2014) explicitly by using a global
log-bilinear regression model which combines the
global matrix factorization and the local context vec-
tors when training word embeddings.

Instead of using the COMPOSES vector space,
we experimented with replacing the v(wi) com-

635



ponent in Equation 4 with the GloVe vectors,2

vglove(wi) such that:

simglove(S
(1), S(2)) =

vglove(S
(1)) · vglove(S(2))

|vglove(S(1))| |vglove(S(2))|
(6)

The novelty lies in the usage of the global matrix
to capture corpus wide phenomena that might not
be captured by the local context window. The model
leverages on both the non-zero elements in the word-
word co-occurence matrix (not a sparse bag-of-
words matrix) and the individual context window
vectors similar to the word2vec model (Mikolov et
al., 2013).

3.2 Similarity Using Tree LSTM

Recurrent Neural Nets (RNNs) allow arbitrarily
sized sentence lengths (Elman, 1990) but early work
on RNNs suffered from the vanishing/exploding
gradients problem (Bengio et al., 1994). Hochre-
iter and Schmidhuber (1997) introduced multiplica-
tive input and output gate units to solve the vanish-
ing gradients problem. While RNN and LSTM pro-
cess sentences in a sequential manner, Tree-LSTM
extends the LSTM architecture by processing the in-
put sentence through a syntactic structure of the sen-
tence. We use the ReVal metric (Gupta et al., 2015)
implementation of Tree-LSTM (Tai et al., 2015) to
generate the similarity score.

ReVal represents both sentences (h1, h2) using
Tree-LSTMs and predicts a similarity score ŷ based
on a neural network which considers both distance
and angle between h1 and h2:

h× = h1 � h2
h+ = |h1 − h2|
hs = σ

(
W (×)h× +W (+)h+ + b(h)

)

p̂θ = softmax
(
W (p)hs + b

(p)
)

ŷ = rT p̂θ

(7)

where, σ is a sigmoid function, p̂θ is the estimated
probability distribution vector and rT = [1 2...K].
The cost function J(θ) is defined over probability

2We use the 300 dimensions vectors from the GloVe model
trained on the Commoncrawl Corpus with 840B tokens, 2.2M
vocabulary.

distributions p and p̂θ using regularised Kullback-
Leibler (KL) divergence.

J(θ) =
1

n

n∑

i=1

KL
(
p(i)
∣∣∣
∣∣∣p̂(i)θ

)
+
λ

2
||θ||22 (8)

In Equation 8, i represents the index of each train-
ing pair, n is the number of training pairs and p is
the sparse target distribution such that y = rT p is
defined as follows:

pj =





y − byc, j = byc+ 1
byc − y + 1, j = byc
0 otherwise

for 1 ≤ j ≤ K, where, y ∈ [1,K] is the similarity
score of a training pair. This gives us a similarity
score between [1, K] which is mapped between [0,
1].3 Please refer to Gupta et al. (2015) for training
details.

4 Submission

We submitted three models based on the original
replication of the Sultan et al. (2014b) and Sultan
et al. (2015) system and our variants and extensions
of their approach.

Our baseline submission uses the similarity
score from Equations 3 and 5 as features to train
a linear ridge regression. Our baseline submis-
sion achieved an overall 0.69244 Pearson correlation
score on all domains.

Extending the baseline implementation, we
included the similarity score from Equations 6 and 8
to the feature set and trained a boosted tree ensemble
(Friedman, 2001) to produce our Boosted submis-
sion. Finally, we use the same feature set to train an
eXtreme Boosted tree ensemble (XGBoost) (Chen
and He, 2015; Chen and Guestrin, 2015) model.

We annotated the STS 2012 to 2015 datasets
with the similarity scores from Equations 2, 3,
5, 6, 8. The annotations and our open source
implementation of the system are available at
https://github.com/alvations/stasis
/blob/master/notebooks/STRIKE.ipynb

3score = (score-1)/K

636



answer-answer headlines plagiarism postediting question-question All
Baseline 0.48799 0.71043 0.80605 0.84601 0.61515 0.69244
Boosted 0.49415 0.71439 0.79655 0.83758 0.63509 0.69453
XGBoost 0.49947 0.72410 0.79076 0.84093 0.62055 0.69471
+Saarsheff 0.50628 0.77824 0.82501 0.84861 0.70424 0.73050
Median 0.48018 0.76439 0.78949 0.81241 0.57140 0.68923
Best 0.69235 0.82749 0.84138 0.8669 0.74705 0.77807

Table 1: Pearson Correlation Results for English STS Task at SemEval-2016

5 Results

Table 1 shows the results of our submission to the
English STS task in SemEval-2016; the median and
best scores are computed across all participating
teams in the task. Our baseline system performs
reasonably well, outperforming the median scores in
most domains.

Our extended variant of the baseline using
boosted tree ensemble performs better in the answer-
answer, headlines and postediting domains but per-
formed worse in others. Comparatively, it improves
the overall correlation score marginally by 0.002.

The system using XGBoost performs the best of
the 3 models but it underperforms in the headlines
and plagiarism domain when compared to the me-
dian scores.

Generally, we did not achieve the outstanding
scores in the task as compared to the top perform-
ing team DLS@CU in the English STS 2015. Our
XGBoost system performs far from the best scores
from the top systems. However, overall our correla-
tion scores are higher than the median scores across
all submissions for the task.

As a post-hoc test, we have evaluated our baseline
system by training on the STS 2012 to 2014 dataset
and testing on the STS 2015 dataset and we achieved
0.76141 weighted mean Pearson correlation score
on all domains. As compared to Sultan et al. (2015)
results of 0.8015 we are 0.04 points short of their
results which should technically rank our system at
20th out of 70+ submissions to the STS 2015 task4.

Machine Translation (MT) evaluation metrics
have shown competitive performance in previous

4Our replication attempt obtained better results compared to
our STS-2015 submission (MiniExperts) that used a Sup-
port Vector Machine regressor trained on a number of linguisti-
cally motivated features (Gupta et al., 2014); it achieved 0.7216
mean score (Béchara et al., 2015).

STS tasks (Barrón-Cedeño et al., 2013; Huang and
Chang, 2014; Bertero and Fung, 2015; Tan et al.,
2015). Tan et al. (2016) annotated the STS datasets
with MT metrics scores for every pair of sentence in
the training and evaluation data. We extend our XG-
Boost model with these MT metric annotations and
achieved a higher score for every domain leading
to an overall Pearson correlation score of 0.73050
(+Saarsheff in Table 1).

6 Conclusion

In this paper, we have presented our findings on
replicating the top system in the STS 2014 and 2015
task and evaluated our replica of the system in the
English STS task of SemEval-2016. We have in-
troduced variants and extensions to the replica sys-
tem by using various state-of-art word and sentence
embeddings. Our systems trained on (eXtreme)
Boosted Tree ensembles outperform the replica sys-
tem using linear regression. Although our replica
of the previous best system did not achieve stellar
scores, all our systems outperform the median scores
computed across all participating systems.

Acknowledgments

The research leading to these results has received
funding from the People Programme (Marie Curie
Actions) of the European Union’s Seventh Frame-
work Programme FP7/2007-2013/ under REA grant
agreement n ◦ 317471.

References
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor

Gonzalez-Agirre. 2012. SemEval-2012 Task 6: A
Pilot on Semantic Textual Similarity. In First Joint
Conference on Lexical and Computational Semantics
(*SEM): Proceedings of the Main Conference and the
Shared Task, pages 385–393, Montréal, Canada.

637



Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. SEM 2013 shared
task: Semantic Textual Similarity. In Second Joint
Conference on Lexical and Computational Semantics
(*SEM), Volume 1: Proceedings of the Main Confer-
ence and the Shared Task, pages 32–43, Atlanta, Geor-
gia.

Eneko Agirre, Carmen Banea, Claire Cardic, Daniel
Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo,
Rada Mihalcea, German Rigau, and Janyce Wiebe.
2014. SemEval-2014 Task 10: Multilingual Seman-
tic Textual Similarity. In Proceedings of the 8th Inter-
national Workshop on Semantic Evaluation (SemEval
2014), pages 81–91, Dublin, Ireland.

Eneko Agirre, Carmen Banea, Claire Cardie, Daniel
Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo,
Inigo Lopez-Gazpio, Montse Maritxalar, Rada Mihal-
cea, German Rigau, Larraitz Uria, and Janyce Wiebe.
2015. Semeval-2015 task 2: Semantic textual sim-
ilarity, english, spanish and pilot on interpretability.
In Proceedings of the 9th International Workshop on
Semantic Evaluation (SemEval 2015), pages 252–263,
Denver, Colorado.

Rafael E Banchs, Luis F D’Haro, and Haizhou Li. 2015.
Adequacy–fluency metrics: Evaluating mt in the con-
tinuous space model framework. Audio, Speech, and
Language Processing, IEEE/ACM Transactions on,
23(3):472–482.

Marco Baroni, Georgiana Dinu, and Germán Kruszewski.
2014. Don’t count, predict! a systematic compari-
son of context-counting vs. context-predicting seman-
tic vectors. In Proceedings of the 52nd Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), pages 238–247, Baltimore,
Maryland.

Alberto Barrón-Cedeño, Lluı́s Màrquez, Maria Fuentes,
Horacio Rodrı́guez, and Jordi Turmo. 2013. UPC-
CORE: What Can Machine Translation Evaluation
Metrics and Wikipedia Do for Estimating Semantic
Textual Similarity? In Second Joint Conference on
Lexical and Computational Semantics (*SEM), Vol-
ume 1: Proceedings of the Main Conference and the
Shared Task, pages 143–147, Atlanta, Georgia.

Hanna Béchara, Hernani Costa, Shiva Taslimipoora, Ro-
hit Guptaa, Constantin Orăsan, Gloria Corpas Pastorb,
and Ruslan Mitkova. 2015. Miniexperts: An svm ap-
proach for measuring semantic textual similarity. In
Proceedings of the 9th International Workshop on Se-
mantic Evaluation (SemEval 2015), pages 96–101.

Yoshua Bengio, Patrice Simard, and Paolo Frasconi.
1994. Learning long-term dependencies with gradient
descent is difficult. Neural Networks, IEEE Transac-
tions on, 5(2):157–166.

Dario Bertero and Pascale Fung. 2015. Hltc-hkust: A
neural network paraphrase classifier using translation
metrics, semantic roles and lexical similarity features.
In Proceedings of the 9th International Workshop on
Semantic Evaluation (SemEval 2015), pages 23–28,
Denver, Colorado.

Steven Bird, Ewan Klein, and Edward Loper. 2009. Nat-
ural language processing with Python. ” O’Reilly Me-
dia, Inc.”.

Tianqi Chen and Carlos Guestrin. 2015. Xgboost: Reli-
able large-scale tree boosting system.

Tianqi Chen and Tong He. 2015. xgboost: extreme gra-
dient boosting. R package version 0.4-2.

Courtney Corley and Rada Mihalcea. 2005. Measur-
ing the semantic similarity of texts. In Proceedings of
the ACL Workshop on Empirical Modeling of Semantic
Equivalence and Entailment, EMSEE ’05, pages 13–
18, Stroudsburg, PA, USA.

Jeffrey L Elman. 1990. Finding structure in time. Cog-
nitive science, 14(2):179–211.

Jerome H Friedman. 2001. Greedy function approxima-
tion: a gradient boosting machine. Annals of statistics,
pages 1189–1232.

Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2013. Ppdb: The paraphrase
database. In Proceedings of the 2013 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 758–764, Atlanta, Georgia.

Rohit Gupta, Hanna Béchara, Ismail El Maarouf, and
Constantin Orăsan. 2014. Uow: Nlp techniques
developed at the university of wolverhampton for
semantic similarity and textual entailment. In 8th
Int. Workshop on Semantic Evaluation (SemEval14),
pages 785–789.

Rohit Gupta, Constantin Orăsan, and Josef van Genabith.
2015. Reval: A simple and effective machine transla-
tion evaluation metric based on recurrent neural net-
works. In Proceedings of the 2015 Conference on
Empirical Methods in Natural Language Processing,
pages 1066–1072, Lisbon, Portugal.

Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long
short-term memory. Neural computation, 9(8):1735–
1780.

Pingping Huang and Baobao Chang. 2014. SSMT:A
Machine Translation Evaluation View To Paragraph-
to-Sentence Semantic Similarity. In Proceedings of
the 8th International Workshop on Semantic Evalua-
tion (SemEval 2014), pages 585–589, Dublin, Ireland.

Omer Levy, Yoav Goldberg, and Israel Ramat-Gan.
2014. Linguistic regularities in sparse and explicit
word representations. In CoNLL, pages 171–180.

638



Chin-Yew Lin and Eduard Hovy. 2003. Automatic
evaluation of summaries using n-gram co-occurrence
statistics. In Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology-Volume 1, pages 71–78.

Yanjun Ma, Yifan He, Andy Way, and Josef van Gen-
abith. 2011. Consistent translation using discrimina-
tive learning: a translation memory-inspired approach.
In Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies-Volume 1, pages 1239–1248.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositionality.
In Advances in neural information processing systems,
pages 3111–3119.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word rep-
resentation. In Proceedings of the 2014 Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP), pages 1532–1543, Doha, Qatar.

Philip Resnik et al. 1999. Semantic similarity in a taxon-
omy: An information-based measure and its applica-
tion to problems of ambiguity in natural language. J.
Artif. Intell. Res.(JAIR), 11:95–130.

Md Arafat Sultan, Steven Bethard, and Tamara Sum-
ner. 2014a. Back to basics for monolingual align-
ment: Exploiting word similarity and contextual evi-
dence. Transactions of the Association for Computa-
tional Linguistics, 2:219–230.

Md Arafat Sultan, Steven Bethard, and Tamara Sumner.
2014b. Dls@ cu: Sentence similarity from word align-
ment. In Proceedings of the 8th International Work-
shop on Semantic Evaluation (SemEval 2014), pages
241–246.

Md Arafat Sultan, Steven Bethard, and Tamara Sumner.
2015. Dls@ cu: Sentence similarity from word align-
ment and semantic vector composition. In Proceed-
ings of the 9th International Workshop on Semantic
Evaluation, pages 148–153.

Kai Sheng Tai, Richard Socher, and Christopher D. Man-
ning. 2015. Improved semantic representations from
tree-structured long short-term memory networks. In
Proceedings of the 53rd Annual Meeting of the Associ-
ation for Computational Linguistics and the 7th Inter-
national Joint Conference on Natural Language Pro-
cessing (Volume 1: Long Papers), pages 1556–1566,
Beijing, China.

Liling Tan, Carolina Scarton, Lucia Specia, and Josef
van Genabith. 2015. Usaar-sheffield: Semantic tex-
tual similarity with deep regression and machine trans-
lation evaluation metrics. In Proceedings of the 9th

International Workshop on Semantic Evaluation (Se-
mEval 2015), pages 85–89, Denver, Colorado.

Liling Tan, Carolina Scarton, Lucia Specia, and Josef van
Genabith. 2016. Saarsheff at semeval-2016 task 1:
Semantic textual similarity with machine translation
evaluation metrics and (extreme) boosted tree ensem-
bles. In Proceedings of the 10th International Work-
shop on Semantic Evaluation (SemEval 2016), San
Diego, California.

Liling Tan. 2014. Pywsd: Python implementations of
word sense disambiguation (wsd) technologies [soft-
ware]. https://github.com/alvations/pywsd.

Mihaela Vela and Liling Tan. 2015. Predicting machine
translation adequacy with document embeddings. In
Proceedings of the Tenth Workshop on Statistical Ma-
chine Translation, pages 402–410, Lisbon, Portugal.

639


