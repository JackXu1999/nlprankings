











































Document Similarity for Texts of Varying Lengths via Hidden Topics


Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 2341–2351
Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics

2341

Document Similarity for Texts of Varying Lengths via Hidden Topics

Hongyu Gong* Tarek Sakakini* Suma Bhat* Jinjun Xiong †
*University of Illinois at Urbana-Champaign, USA

†T. J. Watson Research Center, IBM
*{hgong6, sakakini, spbhat2}@illinois.edu †jinjun@us.ibm.com

Abstract

Measuring similarity between texts is an
important task for several applications.
Available approaches to measure docu-
ment similarity are inadequate for doc-
ument pairs that have non-comparable
lengths, such as a long document and its
summary. This is because of the lexi-
cal, contextual and the abstraction gaps be-
tween a long document of rich details and
its concise summary of abstract informa-
tion. In this paper, we present a document
matching approach to bridge this gap, by
comparing the texts in a common space
of hidden topics. We evaluate the match-
ing algorithm on two matching tasks and
find that it consistently and widely outper-
forms strong baselines. We also highlight
the benefits of the incorporation of domain
knowledge to text matching.

1 Introduction

Measuring the similarity between documents is
of key importance in several natural process-
ing applications including information retrieval
(Salton and Buckley, 1988), book recommenda-
tion (Gopalan et al., 2014), news categorization
(Ontrup and Ritter, 2002) and essay scoring (Lan-
dauer, 2003). A range of document similarity ap-
proaches have been proposed and effectively used
in recent applications including (Lai et al., 2015;
Bordes et al., 2015). Central to the tasks discussed
above is the assumption that the documents being
compared are of comparable lengths.

Advances in language processing approaches
to transform natural language understanding, such
as text summarization and recommendation, have
generated new requirements for comparing doc-
uments. For instance, summarization techniques

Table 1: A Sample Concept-Project Matching
Concept
Heredity: Inheritance and Variation of Traits
All cells contain genetic information in the form of DNA
molecules. Genes are regions in the DNA that contain
the instructions that code for the formation of proteins.
Project
Pedigree Analysis: A Family Tree of Traits
Do you have the same hair color or eye color as your
mother? When we look at members of a family it is easy
to see that some physical characteristics or traits are
shared. To start this project, you should draw a pedigree
showing the different members of your family. Ideally
you should include multiple people from at least three
generations.

(extractive and abstractive) are capable of auto-
matically generating textual summaries by con-
verting a long document of several hundred words
into a condensed text of only a few words while
preserving the core meaning of the original text
(Kedzie and McKeown, 2016). Conceivably, a re-
lated aspect of summarization is the task of bidi-
rectional matching of a summary and a document
or a set of documents, which is the focus of this
study. The document similarity considered in this
paper is between texts that have significant differ-
ences not only in length, but also in the abstraction
level (such as a definition of an abstract concept
versus a detailed instance of that abstract concept).

As an illustration, consider the task of match-
ing a Concept with a Project as shown in Table 1.
Here a Concept is a grade-level science curricu-
lum item and represents the summary. A Project,
listed in a collection of science projects, represents
the document. Projects typically are long texts
including an introduction, materials and proce-
dures, whereas science concepts are much shorter
in comparison having a title and a concise and ab-
stract description. The concepts and projects are
described in detail in Section 5.1. The matching



2342

task here is to automatically suggest a hands-on
project for a given concept in the curriculum, such
that the project can help reinforce a learner’s basic
understanding of the concept. Conversely, given a
science project, one may need to identify the con-
cept it covers by matching it to a listed concept in
the curriculum. This would be conceivable in the
context of an intelligent tutoring system.

Challenges to the matching task mentioned
above include: 1) The mismatch in the relative
lengths of the documents being compared – a long
piece of text (henceforth termed document) and a
short piece of text (termed summary) – gives rise
to the vocabulary mismatch problem, where the
document and the summary do not share a ma-
jority of terms. 2) The context mismatch prob-
lem arising because a document provides a reason-
able amount of text to infer the contextual mean-
ing of a term, but a summary only provides a lim-
ited context, which may or may not involve the
same terms considered in the document. These
challenges render existing approaches to compar-
ing documents–for instance, those that rely on
document representations (e.g., Doc2Vec (Le and
Mikolov, 2014))–inadequate, because the predom-
inance of non-topic words in the document intro-
duces noise to its representation while the sum-
mary is relatively noise-free, rendering Doc2Vec
inadequate for comparing them.

Our approach to the matching problem is to al-
low a multi-view generalization of the document,
where multiple hidden topics are used to estab-
lish a common ground to capture as much infor-
mation of the document and the summary as pos-
sible and use this to score the relevance of the
pair. We empirically validate our approach on two
tasks – that of project-concept matching in grade-
level science and that of scientific paper-summary
matching – using both custom-made and publicly
available datasets. The main contributions of this
paper are:
1. We propose an embedding-based hidden topic
model to extract topics and measure their impor-
tance in long documents.
2. We present a novel geometric approach to com-
pare documents with differing modality (a long
document to a short summary) and validate its per-
formance relative to strong baselines.
3. We explore the use of domain-specific word
embeddings for the matching task and show the
explicit benefit of incorporating domain knowl-

edge in the algorithm.
4. We make available the first dataset1 on project-
concept matching in the science domain to help
further research in this area.

2 Related Works

Document similarity approaches quantify the de-
gree of relatedness between two pieces of texts
of comparable lengths and thus enable matching
between documents. Traditionally, statistical ap-
proaches (e.g., (Metzler et al., 2007)) and vector-
space-based methods (including the robust Latent
Semantic Analysis (LSA) (Dumais, 2004)) have
been used for text similarity. More recently, neural
network-based methods have been used for doc-
ument representation and these include average
word embeddings (Mikolov et al., 2013), Doc2Vec
(Le and Mikolov, 2014), Skip-Thought vectors
(Kiros et al., 2015), recursive neural network-
based methods (Socher et al., 2014), LSTM archi-
tectures (Tai et al., 2015), and convolutional neural
networks (Blunsom et al., 2014).

Considering works that avoid using an ex-
plicit document representation for comparing
documents, the state-of-the-art method is Word
Mover’s Distance (WMD), which relies on pre-
trained word embeddings (Kusner et al., 2015).
Given these embeddings, the WMD defines the
distance between two documents as the best trans-
port cost of moving all words from one document
to another within the space of word embeddings.
The advantages of WMD are that it is hyper-
parameter free and achieves high retrieval accu-
racy on document classification tasks with docu-
ments of comparable lengths. However, it is com-
putationally expensive for long documents (Kus-
ner et al., 2015).

Clearly, what is lacking in prior literature is
a study of document similarity approaches that
match documents with widely different sizes. It is
this gap in literature that we expect to fill by way
of this study.
Latent Variable Models. Latent variable mod-
els including count-based and probabilistic models
have been studied in many previous works. Count-
based models such as Latent Semantic Indexing
(LSI) compare two documents based on their com-
bined vocabulary (Deerwester et al., 1990). When

1Our code and data are available at: https:
//github.com/HongyuGong/Document-
Similarity-via-Hidden-Topics.git

https://github.com/HongyuGong/Document-Similarity-via-Hidden-Topics.git
https://github.com/HongyuGong/Document-Similarity-via-Hidden-Topics.git
https://github.com/HongyuGong/Document-Similarity-via-Hidden-Topics.git


2343

(a) word geometry of general embedding (b) word geometry of science domain embeddings

Figure 1: Two key words “forces” and “matters” are shown in red and blue respectively. Red words
represent different senses of “forces”, and blue words carry senses of “matters”. “forces” mainly refers
to “army” and “matters” refers to “issues” in general embedding of (a), whereas “forces” shows its sense
of “gravity” and “matters” shows the sense of “solids” in science-domain embedding of (b)

documents have highly mismatched vocabularies
such as those that we study, relevant documents
might be classified as irrelevant. Our model is
built upon word-embeddings which is more robust
to such a vocabulary mismatch.

Probabilistic models such as Latent Dirichlet
Analysis (LDA) define topics as distributions over
words (Blei et al., 2003). In our model, topics are
low-dimensional real-valued vectors (more details
in Section 4.2).

3 Domain Knowledge

Domain information pertaining to specific areas of
knowledge is made available in texts by the use of
words with domain-specific meanings or senses.
Consequently, domain knowledge has been shown
to be critical in many NLP applications such as in-
formation extraction and multi-document summa-
rization (Cheung and Penn, 2013a), spoken lan-
guage understanding (Chen et al., 2015), aspect
extraction (Chen et al., 2013) and summarization
(Cheung and Penn, 2013b).

As will be described later, our distance metric
for comparing a document and a summary relies
on word embeddings. We show in this work, that
embeddings trained on a science-domain corpus
lead to better performance than embeddings on
the general corpus (WikiCorpus). Towards this,
we extract a science-domain sub-corpus from the
WikiCorpus, and the corpus extraction will be de-
tailed in Section 5.

To motivate the domain-specific behavior of
polysemous words, we will qualitatively explore
how domain-specific embeddings differ from the

general embeddings on two polysemous science
terms: forces and matters. Considering the fact
that the meaning of a word is dictated by its neigh-
bors, for each set of word embeddings, we plot the
neighbors of these two terms in Figure 1 on to 2 di-
mensions using Locally Linear Embedding (LLE),
which preserves word distances (Roweis and Saul,
2000). We then analyze the sense of the focus
terms–here, forces and matters.

From Figure 1(a), we see that for the word
forces, its general embedding is close to army,
soldiers, allies indicating that it is related with
violence and power in a general domain. Shift-
ing our attention to Figure 1(b), we see that for
the same term, its science embedding is closer
to torque, gravity, acceleration implying that its
science sense is more about physical interactions.
Likewise, for the word matters, its general embed-
ding is surrounded by affairs and issues, whereas,
its science embedding is closer to particles and
material, prompting that it represents substances.
Thus, we conclude that domain specific embed-
dings (here, science), is capable of incorporat-
ing domain knowledge into word representations.
We use this observation in our document-summary
matching system to which we turn next.

4 Model

Our model that performs the matching between
document and summary is depicted in Figure 2. It
is composed of three modules that perform prepro-
cessing, document topic generation, and relevance
measurement between a document and a summary.
Each of these modules is discussed below.



2344

Figure 2: The system for document-summary matching

4.1 Preprocessing
The preprocessing module tokenizes texts and re-
moves stop words and prepositions. This step al-
lows our system to focus on the content words
without impacting the meaning of original texts.

4.2 Topic Generation from Documents
We assume that a document (a long text) is a
structured collection of words, with the ‘structure’
brought about by the composition of topics. In
some sense, this ‘structure’ is represented as a set
of hidden topics. Thus, we assume that a docu-
ment is generated from certain hidden “topics”,
analogous to the modeling assumption in LDA.
However, unlike in LDA, the “topics” here are nei-
ther specific words nor the distribution over words,
but are are essentially a set of vectors. In turn, this
means that words (represented as vectors) con-
stituting the document structure can be generated
from the hidden topic vectors.

Introducing some notation, the word vectors in
a document are {w1, . . . ,wn}, and the hidden
topic vectors of the document are {h1, . . . ,hK},
where wi,hk 2 Rd, d = 300 in our experiments.

Linear operations using word embeddings have
been empirically shown to approximate their
compositional properties (e.g. the embedding
of a phrase is nearly the sum of the embed-
dings of its component words) (Mikolov et al.,
2013). This motivates the linear reconstruction
of the words from the document’s hidden topics
while minimizing the reconstruction error. We
stack the K topic vectors as a topic matrix
H = [h1, . . . ,hK ](K < d). We define the
reconstructed word vector ˜wi for the word wi as
the optimal linear approximation given by topic
vectors: ˜wi = H ˜↵i, where

˜↵i = argmin
↵i2RK

kwi �H↵ik22. (1)

The reconstruction error E for the whole docu-
ment is the sum of each word’s reconstruction er-

ror and is given by: E =
nP

i=1
kwi � ˜wik22. This

being a function of the topic vectors, our goal is to
find the optimal H⇤ so as to minimize the error E:

H⇤ = argmin
H2Rd⇥K

E(H)

= argmin

H2Rd⇥K

nX

i=1

min

↵i
kwi �H↵ik22, (2)

where k·k is the Frobenius norm of a matrix.
Without loss of generality, we require the topic

vectors {hi}Ki=1 to be orthonormal, i.e., hTi hj =
1(i=j). As we can see, the optimization problem
(2) describes an optimal linear space spanned by
the topic vectors, so the norm and the linear de-
pendency of the vectors do not matter. With the
orthonormal constraints, we simplify the form of
the reconstructed vector ˜wi as:

˜wi = HH
Twi. (3)

We stack word vectors in the document as a matrix
W = [w1, . . . ,wn]. The equivalent formulation
to problem (2) is:

min

H
kW �HHTWk22

s.t. HTH = I, (4)

where I is an identity matrix.
The problem can be solved by Singular Value

Decomposition (SVD), using which, the matrix
W can be decomposed as W = U⌃VT , where
UTU = I,VTV = I, and ⌃ is a diagonal ma-
trix where the diagonal elements are arranged in a
decreasing order of absolute values. We show in
the supplementary material that the first K vec-
tors in the matrix U are exactly the solution to
H⇤ = [h⇤1, . . . ,h

⇤
K ].

We find optimal topic vectors H⇤ =
[h⇤1, . . . ,h

⇤
K ] by solving problem (4). We

note that these topic vectors are not equally
important, and we say that one topic is more
important than another if it can reconstruct words



2345

with smaller error. Define Ek as the reconstruc-
tion error when we only use topic vector h⇤k to
reconstruct the document:

Ek = kW � h⇤kh⇤k
TWk22. (5)

Now define ik as the importance of topic h⇤k,
which measures the topic’s ability to reconstruct
the words in a document:

ik = kh⇤k
TWk22 (6)

We show in the supplementary material that the
higher the importance ik is, the smaller the recon-
struction error Ek is. Now we normalize ik as¯ik so
that the importance does not scale with the norm
of the word matrix W , and so that the importances
of the K topics sum to 1. Thus,

¯ik = ik/(
KX

j=1

ij). (7)

The number of topics K is a hyperparameter in our
model. A small K may not cover key ideas of the
document, whereas a large K may keep trivial and
noisy information. Empirically we find that K =
15 captures most important information from the
document.

4.3 Topic Mapping to Summaries
We have extracted K topic vectors {h⇤k}Kk=1 from
the document matrix W, whose importance is re-
flected by {¯ik}Kk=1. In this module, we measure
the relevance of a document-summary pair. To-
wards this, a summary that matches the document
should also be closely related with the “topics”
of that document. Suppose the vectors of the
words in a summary are stacked as a d ⇥ m ma-
trix S = [s1, . . . , sm], where sj is the vector of
the j-th word in a summary. Similar to the recon-
struction of the document, the summary can also
be reconstructed from the documents’ topic vec-
tors as shown in Eq. (3). Let ˜skj be the reconstruc-
tion of the summary word sj given by one topic
h⇤k: ˜s

k
j = h

⇤
kh

⇤
k
T sj .

Let r(h⇤k, sj) be the relevance between a topic
vector h⇤k and summary word sj . It is defined as
the cosine similarity between ˜skj and sj :

r(h⇤k, sj) = sj
T s̃kj /(ksjk2 · k˜skj k2). (8)

Furthermore, let r(h⇤k,S) be the relevance be-
tween a topic vector and the summary, defined to

be the average similarity between the topic vector
and the summary words:

r(h⇤k,S) =
1

m

mX

j=1

r(h⇤k, sj). (9)

The relevance between a topic vector and a sum-
mary is a real value between 0 and 1.

As we have shown, the topics extracted from
a document are not equally important. Natu-
rally, a summary relevant to more important top-
ics is more likely to better match the document.
Therefore, we define r(W,S) as the relevance be-
tween the document W and the summary S, and
r(W,S) is the sum of topic-summary relevance
weighted by the importance of the topic:

r(W,S) =
KX

k=1

¯ik · r(h⇤k,S), (10)

where ¯ik is the importance of topic h⇤k as defined
in (7). The higher r(W,S) is, the better the sum-
mary matches the document.

We provide a visual representation of the doc-
uments as shown in Figure 3 to illustrate the no-
tion of hidden topics. The two documents are
from science projects: a genetics project, Pedigree
Analysis: A Family Tree of Traits (ScienceBud-
dies, 2017a), and a weather project, How Do the
Seasons Change in Each Hemisphere (Science-
Buddies, 2017b). We project all embeddings to a
three-dimensional space for ease of visualization.

As seen in Figure 3, the hidden topics recon-
struct the words in their respective documents to
the extent possible. This means that the words of a
document lie roughly on the plane formed by their
corresponding topic vectors. We also notice that
the summary words (heredity and weather respec-
tively for the two projects under consideration) lie
very close to the plane formed by the hidden topics
of the relevant project while remaining away from
the plane of the irrelevant project. This shows that
the words in the summary (and hence the summary
itself) can also be reconstructed from the hidden
topics of documents that match the summary (and
are hence ‘relevant’ to the summary). Figure 3
visually explains the geometric relations between
the summaries, the hidden topics and the docu-
ments. It also validates the representation power
of the extracted hidden topic vectors.



2346

Figure 3: Words mode and genes from the doc-
ument on genetics and words storm and atmo-
spheric from document on weather are represented
by pink and blue points respectively. Linear space
of hidden topics in genetics form the pink plane,
where summary word heredity (the red point)
roughly lies. Topic vectors of the document on
weather form the blue plane, and the summary
word weather (the darkblue point) lies almost on
the same plane.

5 Experiments

In this section, we evaluate our document-
summary matching approach on two specific ap-
plications where texts of different sizes are com-
pared. One application is that of concept-project
matching useful in science education and the other
is that of summary-research paper matching.

Word Embeddings. Two sets of 300-
dimension word embeddings were used in our ex-
periments. They were trained by the Continu-
ous Bag-of-Words (CBOW) model in word2vec
(Mikolov et al., 2013) but on different corpora.
One training corpus is the full English WikiCor-
pus of size 9 GB (Al-Rfou et al., 2013). The
second consists of science articles extracted from
the WikiCorpus. To extract these science articles,
we manually selected the science categories in
Wikipedia and considered all subcategories within
a depth of 3 from these manually selected root
categories. We then extracted all articles in the
aforementioned science categories resulting in a
science corpus of size 2.4 GB. The word vectors
used for documents and summaries are both from
the pretrained word2vec embeddings.
Baselines We include two state-of-the-art methods
of measuring document similarity for comparison
using their implementations available in gensim
(Řehůřek and Sojka, 2010).

(1) Word movers’ distance (WMD) (Kusner
et al., 2015). WMD quantifies the distance be-
tween a pair of documents based on word em-
beddings as introduced previously (c.f. Related
Work). We take the negative of their distance as
a measure of document similarity (here between a
document and a summary).
(2) Doc2Vec (Le and Mikolov, 2014). Document
representations have been trained with neural net-
works. We used two versions of doc2vec: one
trained on the full English Wikicorpus and a sec-
ond trained on the science corpus, same as the cor-
pora used for word embedding training. We used
the cosine similarity between two text vectors to
measure their relevance.

For a given document-summary pair, we com-
pare the scores obtained using the above two meth-
ods with that obtained using our method.

5.1 Concept-Project matching

Science projects are valuable resources for learn-
ers to instigate knowledge creation via experimen-
tation and observation. The need for matching a
science concept with a science project arises when
learners intending to delve deeper into certain con-
cepts search for projects that match a given con-
cept. Additionally, they may want to identify the
concepts with which a set of projects are related.

We note that in this task, science concepts are
highly concise summaries of the core ideas in
projects, whereas projects are detailed instructions
of the experimental procedures, including an intro-
duction, materials and a description of the proce-
dure, as shown in Table 1. Our matching method
provides a way to bridge the gap between abstract
concepts and detailed projects. The format of the
concepts and the projects is discussed below.
Concepts. For the purpose of this study we use
the concepts available in the Next Generation Sci-
ence Standards (NGSS) (NGSS, 2017). Each con-
cept is accompanied by a short description. For
example, one concept in life science is Heredity:
Inheritance and Variation of Traits. Its descrip-
tion is All cells contain genetic information in the
form of DNA molecules. Genes are regions in the
DNA that contain the instructions that code for the
formation of proteins. Typical lengths of concepts
are around 50 words.
Projects. The website Science Buddies (Science-
Buddies, 2017c) provides a list of projects from a
variety of science and engineering disciplines such



2347

Table 2: Classification results for the Concept-Project Matching task. All performance differences were
statistically significant at p = 0.01.

method topic science topic wiki wmd science wmd wiki doc2vec science doc2vec wiki
precision 0.758± 0.012 0.750± 0.009 0.643± 0.070 0.568± 0.055 0.615± 0.055 0.661± 0.084

recall 0.885± 0.071 0.842± 0.010 0.735± 0.119 0.661± 0.119 0.843± 0.066 0.737± 0.149
fscore 0.818± 0.028 0.791± 0.007 0.679± 0.022 0.595± 0.020 0.695± 0.019 0.681± 0.032

as physical sciences, life sciences and social sci-
ences. A typical project consists of an abstract, an
introduction, a description of the experiment and
the associated procedures. A project typically has
more than 1000 words.
Dataset. We prepared a representative dataset 537
pairs of projects and concepts involving 53 unique
concepts from NGSS and 230 unique projects
from Science Buddies. Engineering undergrad-
uate students annotated each pair with the deci-
sion whether it was a good match or not and re-
ceived research credit. As a result, each concept-
project pair received at least three annotations,
and upon consolidation, we considered a concept-
project pair to be a good match when a majority
of the annotators agreed. Otherwise it was not
considered a good match. The ratio between good
matches and bad matches in the collected data was
44 : 56.
Classification Evaluation. Annotations from stu-
dents provided the ground truth labels for the clas-
sification task. We randomly split the dataset into
tuning and test instances with a ratio of 1 : 9. A
threshold score was tuned on the tuning data, and
concept-project pairs with scores higher than this
threshold were classified as a good matches dur-
ing testing. We performed 10-fold cross valida-
tion, and report the average precision, recall, F1
score and their standard deviation in Table 2.

Our topic-based metric is denoted as “topic”,
and the general-domain and science-domain em-
beddings are denoted as “wiki” and “science”
respectively. We show the performance of our
method against the two baselines while vary-
ing the underlying embeddings, thus resulting
in 6 different combinations. For example,
“topic science” refers to our method with science
embeddings. From the table (column 1) we notice
the following: 1) Our method significantly outper-
forms the two baselines by a wide margin (⇡10%)
in both the general domain setting as well as the
domain-specific setting. 2) Using science domain-
specific word embeddings instead of the general
word embeddings results in the best performance

across all algorithms. This performance was ob-
served despite the word embeddings being trained
on a significantly smaller corpus compared to the
general domain corpus.

Besides the classification metrics, we also eval-
uated the directed matching from concepts to
projects with ranking metrics.
Ranking Evaluation Our collected dataset re-
sulted in having a many-to-many matching be-
tween concepts and projects. This is because
the same concept was found to be a good match
for multiple projects and the same project was
found to match many concepts. The previously
described classification task evaluated the bidirec-
tional concept-project matching. Next we eval-
uated the directed matching from concepts to
projects, to see how relevant these top ranking
projects are to a given input concepts. Here we
use precision@k (Radlinski and Craswell, 2010)
as the evaluation metric, considering the percent-
age of relevant ones among top-ranking projects.

For this part, we only considered the methods
using science domain embeddings as they have
shown superior performance in the classificaiton
task. For each concept, we check the precision@k
of matched projects and place it in one of k+1 bins
accordingly. For example, for k=3, if only two of
the three top projects are a correct match, the con-
cept is placed in the bin corresponding to 2/3. In
Figure 4, we show the percentage of concepts that
fall into each bin for the three different algorithms
for k=1,3,6.

We observe that recommendations using the
hidden topic approach fall more in the high value
bin compared to others, performing consistently
better than two strong baselines. The advan-
tage becomes more obvious at precision@6. It is
worth mentioning that wmd science falls behind
doc2vec science in the classification task while it
outperforms in the ranking task.

5.2 Text Summarization
The task of matching summaries and documents
is commonly seen in real life. For example, we



2348

(a) Precision@1 (b) Precision@3 (c) Precision@6

Figure 4: Ranking Performance of All Methods

use an event summary “Google’s AlphaGo beats
Korean player Lee Sedol in Go” to search for rel-
evant news, or use the summary of a scientific pa-
per to look for related research publications. Such
matching constitutes an ideal task to evaluate our
matching method between texts of different sizes.
Dataset. We use a dataset from the CL-SciSumm
Shared Task (Jaidka et al., 2016). The dataset con-
sists of 730 ACL Computational Linguistics re-
search papers covering 50 categories in total. Each
category consists of a reference paper (RP) and
around 10 citing papers (CP) that contain citations
to the RP. A human-generated summary for the RP
is provided and we use the 10 CP as being relevant
to the summary. The matching task here is be-
tween the summary and all CPs in each category.
Evaluation. For each paper, we keep all of its
content except the sections of experiments and ac-
knowledgement (these sections were omitted be-
cause often their content is often less related to
the topic of the summary). The typical summary
length is about 100 words, while a paper has more
than 2000 words. For each topic, we rank all 730
papers in terms of their relevance generated by our
method and baselines using both sets of embed-
dings. For evaluation, we use the information re-
trieval measure of precision@k, which considers
the number of relevant matches in the top-k match-
ings (Manning et al., 2008). For each combination
of the text similarity approaches and embeddings,
we show precision@k for different k’s in Figure 5.
We observe that our method with science embed-
ding achieves the best performance compared to
the baselines, once again showing not only the
benefits of our method but also that of incorpo-
rating domain knowledge.

6 Discussion

Analysis of Results. From the results of the two
tasks we observe that our method outperforms two

Figure 5: Summary-Article Matching

strong baselines. The reason for WMD’s poor
performance could be that the many uninforma-
tive words (those unrelated to the central topic)
make WMD overestimate the distance between the
document-summary pair. As for doc2vec, its sin-
gle vector representation may not be able to cap-
ture all the key topics of a document. A project
could contain multifaceted information, e.g., a
project to study how climate change affects grain
production is related to both environmental sci-
ence and agricultural science.

Effect of Topic Number. The number of hid-
den topics K is a hyperparameter in our setting.
We empirically evaluate the effect of topic number
in the task of concept-project mapping. Figure 6
shows the F1 scores and the standard deviations
at different K. As we can see, optimal K is 18.
When K is too small, hidden topics are too few
to capture key information in projects. Thus we
can see that the increase of topic number from 3
to 6 brings a big improvement to the performance.
Topic numbers larger than the optimal value de-
grade the performance since more topics incorpo-
rate noisy information. We note that the perfor-



2349

Figure 6: F1 score on concept-project matching
with different topic numbers K

mance changes are mild when the number of top-
ics are in the range of [18, 31]. Since topics are
weighted by their importance, the effect of noisy
information from extra hidden topics is mitigated.
Interpretation of Hidden Topics. We consider
the summary-paper matching as an example with
around 10 papers per category. We extracted
the hidden topics from each paper, reconstructed
words with these topics as shown in Eq. (3), and
selected the words which had the smallest recon-
struction errors. These words are thus closely re-
lated to the hidden topics, and we call them topic
words to serve as an interpretation of the hid-
den topics. We visualize the cloud of such topic
words on the set of papers about word sense dis-
ambiguation as shown in Figure 7. We see that the
words selected based on the hidden topics cover
key ideas such as disambiguation, represent, clas-
sification and sentence. This qualitatively vali-
dates the representation power of hidden topics.
More examples are available in the supplementary
material.

We interpret this to mean that proposed idea of
multiple hidden topics captures the key informa-
tion of a document. The extracted “hidden top-
ics” represent the essence of documents, suggest-
ing the appropriateness of our relevance metric to
measure the similarity between texts of different
sizes. Even though our focus in this study was the
science domain we point out that the results are
more generally valid since we made no domain-
specific assumptions.
Varying Sensitivity to Domain. As shown in the
results, the science-domain embeddings improved
the classification of concept-project matching for

Figure 7: Topic words from papers on word sense
disambiguation

the topic-based method by 2% in F1-score, WMD
by 8% and doc2vec by 1%, thus underscoring the
importance of domain-specific word embeddings.

Doc2vec is less sensitive to the domain, because
it provides document-level representation. Even
if some words cannot be disambiguated due to
the lack of domain knowledge, other words in the
same document can provide complementary infor-
mation so that the document embedding does not
deviate too much from its true meaning.

Our method, also a word embedding method, is
not as sensitive to domain as WMD. It is robust
to the polysemous words with domain-sensitive
semantics, since hidden topics are extracted in
the document level. Broader contexts beyond
just words provide complementary information for
word sense disambiguation.

7 Conclusion

We propose a novel approach to matching docu-
ments and summaries. The challenge we address
is to bridge the gap between detailed long texts and
its abstraction with hidden topics. We incorpo-
rate domain knowledge into the matching system
to gain further performance improvement. Our
approach has beaten two strong baselines in two
downstream applications, concept-project match-
ing and summary-research paper matching.

Acknowledgments

This work is supported by IBM-ILLINOIS Cen-
ter for Cognitive Computing Systems Research
(C3SR) - a research collaboration as part of the
IBM AI Horizons Network. We thank ACL
anonymous reviewers for their constructive sug-
gestions. We thank Mathew Monfort for helping
deploy annotation tasks, and thank Jong Yoon Lee
for the dataset collection.



2350

References
Rami Al-Rfou, Bryan Perozzi, and Steven Skiena.

2013. Polyglot: Distributed word representations
for multilingual nlp. In Proceedings of the Seven-
teenth Conference on Computational Natural Lan-
guage Learning, pages 183–192, Sofia, Bulgaria.
Association for Computational Linguistics.

David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet allocation. Journal of ma-
chine Learning research, 3(Jan):993–1022.

Phil Blunsom, Edward Grefenstette, and Nal Kalch-
brenner. 2014. A convolutional neural network for
modelling sentences. In Proceedings of the 52nd
Annual Meeting of the Association for Computa-
tional Linguistics. Proceedings of the 52nd Annual
Meeting of the Association for Computational Lin-
guistics.

Antoine Bordes, Nicolas Usunier, Sumit Chopra, and
Jason Weston. 2015. Large-scale simple question
answering with memory networks. arXiv preprint
arXiv:1506.02075.

Yun-Nung Chen, William Yang Wang, Anatole Gersh-
man, and Alexander I Rudnicky. 2015. Matrix fac-
torization with knowledge graph propagation for un-
supervised spoken language understanding. In ACL
(1), pages 483–494.

Zhiyuan Chen, Arjun Mukherjee, Bing Liu, Meichun
Hsu, Malu Castellanos, and Riddhiman Ghosh.
2013. Exploiting domain knowledge in aspect ex-
traction. In EMNLP, pages 1655–1667.

Jackie Chi Kit Cheung and Gerald Penn. 2013a. Prob-
abilistic domain modelling with contextualized dis-
tributional semantic vectors. In ACL (1), pages 392–
401.

Jackie Chi Kit Cheung and Gerald Penn. 2013b. To-
wards robust abstractive multi-document summa-
rization: A caseframe analysis of centrality and do-
main. In ACL (1), pages 1233–1242.

Scott Deerwester, Susan T Dumais, George W Fur-
nas, Thomas K Landauer, and Richard Harshman.
1990. Indexing by latent semantic analysis. Jour-
nal of the American society for information science,
41(6):391.

Susan T Dumais. 2004. Latent semantic analysis. An-
nual review of information science and technology,
38(1):188–230.

Prem K Gopalan, Laurent Charlin, and David Blei.
2014. Content-based recommendations with pois-
son factorization. In Advances in Neural Informa-
tion Processing Systems, pages 3176–3184.

Kokil Jaidka, Muthu Kumar Chandrasekaran, Sajal
Rustagi, and Min-Yen Kan. 2016. Overview of
the cl-scisumm 2016 shared task. In In Proceed-
ings of Joint Workshop on Bibliometric-enhanced

Information Retrieval and NLP for Digital Libraries
(BIRNDL 2016).

Chris Kedzie and Kathleen McKeown. 2016. Ex-
tractive and abstractive event summarization over
streaming web text. In IJCAI, pages 4002–4003.

Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,
Richard Zemel, Raquel Urtasun, Antonio Torralba,
and Sanja Fidler. 2015. Skip-thought vectors. In
Advances in neural information processing systems,
pages 3294–3302.

Matt Kusner, Yu Sun, Nicholas Kolkin, and Kilian
Weinberger. 2015. From word embeddings to docu-
ment distances. In International Conference on Ma-
chine Learning, pages 957–966.

Siwei Lai, Liheng Xu, Kang Liu, and Jun Zhao. 2015.
Recurrent convolutional neural networks for text
classification. In AAAI, volume 333, pages 2267–
2273.

Thomas K Landauer. 2003. Automatic essay assess-
ment. Assessment in education: Principles, policy
& practice, 10(3):295–308.

Quoc Le and Tomas Mikolov. 2014. Distributed repre-
sentations of sentences and documents. In Proceed-
ings of the 31st International Conference on Ma-
chine Learning (ICML-14), pages 1188–1196.

Christopher D Manning, Prabhakar Raghavan, Hinrich
Schütze, et al. 2008. Introduction to information re-
trieval, volume 1. Cambridge university press Cam-
bridge.

Donald Metzler, Susan Dumais, and Christopher Meek.
2007. Similarity measures for short segments of
text. In European Conference on Information Re-
trieval, pages 16–27. Springer.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in neural information processing
systems, pages 3111–3119.

NGSS. 2017. Available at: https:
//www.nextgenscience.org. Accessed:
2017-06-30.

Jorg Ontrup and Helge Ritter. 2002. Hyperbolic self-
organizing maps for semantic navigation. In Ad-
vances in neural information processing systems,
pages 1417–1424.

Filip Radlinski and Nick Craswell. 2010. Comparing
the sensitivity of information retrieval metrics. In
Proceedings of the 33rd international ACM SIGIR
conference on Research and development in infor-
mation retrieval, pages 667–674. ACM.

Radim Řehůřek and Petr Sojka. 2010. Software Frame-
work for Topic Modelling with Large Corpora. In
Proceedings of the LREC 2010 Workshop on New

http://www.aclweb.org/anthology/W13-3520
http://www.aclweb.org/anthology/W13-3520
https://www.nextgenscience.org
https://www.nextgenscience.org


2351

Challenges for NLP Frameworks, pages 45–50,
Valletta, Malta. ELRA. http://is.muni.cz/
publication/884893/en.

Sam T Roweis and Lawrence K Saul. 2000. Nonlin-
ear dimensionality reduction by locally linear em-
bedding. science, 290(5500):2323–2326.

Gerard Salton and Christopher Buckley. 1988. Term-
weighting approaches in automatic text retrieval. In-
formation processing & management, 24(5):513–
523.

ScienceBuddies. 2017a. Available at: https:
//www.sciencebuddies.org/science-
fair-projects/project-ideas/
Genom p010/genetics-genomics/
pedigree-analysis-a-family-tree-
of-traits. Accessed: 2017-06-30.

ScienceBuddies. 2017b. Available at: https:
//www.sciencebuddies.org/science-
fair-projects/project-ideas/
Weather p006/weather-atmosphere/
how-do-the-seasons-change-in-each-
hemisphere. Accessed: 2017-06-30.

ScienceBuddies. 2017c. Available at: http://
www.sciencebuddies.org. Accessed: 2017-
06-30.

Richard Socher, Andrej Karpathy, Quoc V Le, Christo-
pher D Manning, and Andrew Y Ng. 2014.
Grounded compositional semantics for finding and
describing images with sentences. Transactions
of the Association for Computational Linguistics,
2:207–218.

Kai Sheng Tai, Richard Socher, and Christopher D
Manning. 2015. Improved semantic representations
from tree-structured long short-term memory net-
works. In Proceedings of the 53rd Annual Meet-
ing of the Association for Computational Linguistics
and the 7th International Joint Conference on Natu-
ral Language Processing (Volume 1: Long Papers),
volume 1, pages 1556–1566.

http://is.muni.cz/publication/884893/en
http://is.muni.cz/publication/884893/en
https://www.sciencebuddies.org/science-fair-projects/project-ideas/Genom_p010/genetics-genomics/pedigree-analysis-a-family-tree-of-traits
https://www.sciencebuddies.org/science-fair-projects/project-ideas/Genom_p010/genetics-genomics/pedigree-analysis-a-family-tree-of-traits
https://www.sciencebuddies.org/science-fair-projects/project-ideas/Genom_p010/genetics-genomics/pedigree-analysis-a-family-tree-of-traits
https://www.sciencebuddies.org/science-fair-projects/project-ideas/Genom_p010/genetics-genomics/pedigree-analysis-a-family-tree-of-traits
https://www.sciencebuddies.org/science-fair-projects/project-ideas/Genom_p010/genetics-genomics/pedigree-analysis-a-family-tree-of-traits
https://www.sciencebuddies.org/science-fair-projects/project-ideas/Genom_p010/genetics-genomics/pedigree-analysis-a-family-tree-of-traits
https://www.sciencebuddies.org/science-fair-projects/project-ideas/Weather_p006/weather-atmosphere/how-do-the-seasons-change-in-each-hemisphere
https://www.sciencebuddies.org/science-fair-projects/project-ideas/Weather_p006/weather-atmosphere/how-do-the-seasons-change-in-each-hemisphere
https://www.sciencebuddies.org/science-fair-projects/project-ideas/Weather_p006/weather-atmosphere/how-do-the-seasons-change-in-each-hemisphere
https://www.sciencebuddies.org/science-fair-projects/project-ideas/Weather_p006/weather-atmosphere/how-do-the-seasons-change-in-each-hemisphere
https://www.sciencebuddies.org/science-fair-projects/project-ideas/Weather_p006/weather-atmosphere/how-do-the-seasons-change-in-each-hemisphere
https://www.sciencebuddies.org/science-fair-projects/project-ideas/Weather_p006/weather-atmosphere/how-do-the-seasons-change-in-each-hemisphere
http://www.sciencebuddies.org
http://www.sciencebuddies.org

