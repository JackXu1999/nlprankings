



















































Proceedings of the...


D S Sharma, R Sangal and E Sherly. Proc. of the 12th Intl. Conference on Natural Language Processing, pages 219–228,
Trivandrum, India. December 2015. c©2015 NLP Association of India (NLPAI)

An Approach to Collective Entity Linking

Ashish Kulkarni Kanika Agarwal Pararth Shah Sunny Raj Rathod Ganesh Ramakrishnan
Department of Computer Science

Indian Institute of Technology Bombay
Mumbai, India

{kulashish, kanika1712, pararthshah717, sunnyrajrathod} @gmail.com
ganesh@cse.iitb.ac.in

Abstract

Entity linking is the task of disambiguat-
ing entities in unstructured text by link-
ing them to an entity in a catalog. Sev-
eral collective entity linking approaches
exist that attempt to collectively disam-
biguate all mentions in the text by leverag-
ing both local mention-entity context and
global entity-entity relatedness. However,
the complexity of these models makes
it unfeasible to employ exact inference
techniques and jointly train the local and
global feature weights. In this work we
present a collective disambiguation model,
that, under suitable assumptions makes ef-
ficient implementation of exact MAP in-
ference possible. We also present an effi-
cient approach to train the local and global
features of this model and implement it in
an interactive entity linking system. The
system receives human feedback on a doc-
ument collection and progressively trains
the underlying disambiguation model.

1 Introduction

Search systems proposed today (Chakrabarti et al.,
2006; Cheng et al., 2007; Kasneci et al., 2008; Li
et al., 2010) are greatly enriched by recognizing
and exploiting entities embedded in unstructured
pages. In a typical system architecture (Cucerzan,
2007; Dill et al., 2003; Kulkarni et al., 2009; Milne
and Witten, 2008) a spotter first identifies short to-
ken segments or “spots” as potential mentions of
entities from its catalog. For our purposes, a cata-
log consists of a directed graph of categories, to
which entity nodes are attached. Many entities
may qualify for a given text segment, e.g., both
Kernel trick and Linux Kernel might qualify for
the text segment “...Kernel...”. In the second stage,
a disambiguator assigns zero or more entities to

selected mentions, based on mention-entity coher-
ence, as well as entity-entity similarity.

Some of the recent work (Zhou et al., 2010; Lin
et al., 2012) shows that several mentions may have
no associated sense in the catalog. This is referred
to as the no-attachment (NA) problem (or NIL in
the TAC-KBP challenge (McNamee, 2009)). The
other, relatively lesser addressed challenge is that
of multiple attachments (Kulkarni et al., 2014),
where a mention might link to more than one enti-
ties from the catalog. This might often be a result
of insufficient context and has been acknowledged
by some of the recent entity disambiguation chal-
lenges1.

We present an approach to collective disam-
biguation of several mentions by combining vari-
ous mention-entity compatibility and entity-entity
relatedness features. Also, unlike most of the prior
work, we jointly learn the local and global feature
weights. Our Markov network-based model, along
with suitable assumptions, makes efficient learn-
ing possible. The model links mentions to zero or
more entities, thus offering a natural solution to
the problem of NAs and multiple attachments.

2 Prior Work

Earlier works (Dill et al., 2003; Bunescu and
Pasca, 2006; Mihalcea and Csomai, 2007) on
entity annotation focused on per-mention disam-
biguation. This involves selecting the best entity
to assign to a mention, independent of the assign-
ments to other mentions in the document. Wik-
ify! (Mihalcea and Csomai, 2007) for instance,
uses context overlap for disambiguation and com-
bines it with a classifier model that exploits lo-
cal and topical features. Cucerzan (Cucerzan,
2007) introduced the notion of agreement on cat-
egories of entities in addition to the local con-
text overlap, in which the entity context comprised

1http://web-ngram.research.microsoft.
com/ERD2014/219



out-links from and in-links to their corresponding
Wikipedia documents. Milne et al. (Milne and
Witten, 2008) formulated a “relatedness” measure
of similarity between two entities from Wikipedia,
based on their in-link overlap. Relatedness, in
conjunction with the prior probability of occur-
rence of an entity, was then used to train a clas-
sifier model. Han et al. (Han and Zhao, 2009)
leveraged the semantic information in Wikipedia
to build a large-scale semantic network and de-
veloped a similarity measure to be used for dis-
ambiguation. Kulkarni et al. (Kulkarni et al.,
2009) were the first to propose a general collec-
tive disambiguation approach, giving formulations
for trade-off between mention-entity compatibil-
ity and coherence between entities. Several graph-
based approaches (Hoffart et al., 2011; Fahrni and
Strube, 2012) followed that cast the disambigua-
tion problem as a problem of dense subgraph se-
lection from a graph of mentions and candidate en-
tities, making use of collective signals.

Most of these systems seem to prefer tagging
conservatively. Some of them (Cucerzan, 2007;
Hoffart et al., 2011) restrict their tagging to named
entities, while others use a subset of entities from
a background taxonomy such as TAP (Dill et al.,
2003) or Wikipedia (Milne and Witten, 2008; Mi-
halcea and Csomai, 2007). Others (Kataria et
al., 2011; Bhattacharya and Getoor, 2006) have
proposed LDA-based generative models but focus
only on person names. Some of the more recent
systems (Kulkarni et al., 2009; Han et al., 2011)
do perform aggressive spotting, aided by the an-
chor dictionary of Wikipedia entities and study the
recall-precision tradeoff.

(Kulkarni et al., 2014) propose a joint disam-
biguation model based on a Markov network of
entities as nodes and edges for their relatedness.
Disambiguation is achieved by performing a MAP
inference on this graph and it naturally handles the
NA and multiple attachment cases. Unlike other
approaches (Han and Sun, 2012; Ratinov et al.,
2011), their graph models candidate entities with
binary labels, instead of mentions with multiple
labels. A suitable assumption on cliques and their
potentials makes efficient computation of exact in-
ference possible. However, it is not clear as to how
the node and edge feature weights are set.

To the best of our knowledge, none of the graph-
based models above have attempted to jointly
learn the node and edge feature weights. While

there is prior work (Wellner et al., 2004; Wick
et al., 2009) that applied graphical models to the
problem of information extraction and coreference
resolution, exact inference and estimation is in-
tractable in these models. Similar approaches have
also been applied to the problem of entity disam-
biguation (Kulkarni et al., 2009; Han and Sun,
2012; Ratinov et al., 2011), but hardly anyone has
attempted to jointly learn the feature weights.

2.1 Our Contributions

We leverage the disambiguation model of (Kulka-
rni et al., 2014) and propose an efficient approach
to jointly learn the node and edge feature weights.
We also develop an interactive active learning
framework that progressively improves the model
as more training data becomes available. We im-
plemented our approach in an online annotation
system2 and used it to semi-automatically curate
labeled data3. Our trained model performs better
than several other systems including that of Kulka-
rni et al.

3 Preliminaries

We borrowed the features and the disambiguation
model from the work described in Kulkarni et al.
and present it in brief here. We first start with the
problem definition.

3.1 Problem Definition

The primary goal of document annotation is to link
entity mentions in an input document to entities
in a catalog. Mentions (or “spots”) are contigu-
ous token sequences in a text, e.g. Bush, that can
potentially link to an entity, e.g. George Bush in
the catalog. LetMd be the set of all mentions in
a document d and E be the set of all entities in
the catalog. Then, the entity linking problem is to
find for each mention m ∈ Md, the set of entities
Êm ⊂ E ∪ {NA} that it can link to.

As a first step, the input text d is processed by
a “spotter” to identify the set Md of mentions
and the set of candidates Em ⊂ E , ∀m ∈ Md.
em ∈ Em is called a candidate entity for spot m.
The setEd = ∪m∈Md Em forms the candidate en-
tities set for document d. This is then followed by
a “disambiguation” phase that obtains from Em,
the set Êm of entities that the mention m can ac-
tually link to. When none of the entities in Em

2http://tinyurl.com/entitydisamb-demo
3http://tinyurl.com/entitydisamb-data220



are valid, then Êm = {NA}. Alternatively, more
than one entities from Em might get promoted to
Êm. Assuming one sense per discourse (Gale et
al., 1992), an entity in the candidate set of more
than one mentions, links (or does not link) to all
those mentions.

3.2 Entity Catalog
A catalog is a structured knowledge base com-
prising categories with entities under them, along
with their attributes and relations. Wikipedia has
seen an extensive organic growth and covers enti-
ties spanning a vast set of domains. We report ex-
perimental results using the Wikipedia dump from
May 2011, with approximately 4.4 million enti-
ties. For evaluation on ERD, we used as catalog,
the snapshot of Freebase as provided in the chal-
lenge.

3.3 Features
We used three types of features - (1) Popularity-
based features of an entity: Prior Sense Probabil-
ity (Mihalcea and Csomai, 2007), In-Link Count,
Out-Link Count; (2) Mention-Entity compatibil-
ity features (Kulkarni et al., 2009); (3) Entity-
Entity relatedness features: Category-based Sim-
ilarity (Cucerzan, 2007), In-link based Similar-
ity (Milne and Witten, 2008), Out-link based Sim-
ilarity, Contextual Similarity.

3.4 The Disambiguation Model
Having identified the set of candidate entities for
each mention, a disambiguator attempts to link
each mention to zero or more entities. The label
of a candidate is a collective result of the interplay
of local mention-entity and global entity-entity re-
latedness signals.

A Markov Random Field (MRF) is an undi-
rected graphical model that captures local cor-
relations between random variables (Taskar and
Koller, 2001). A node is instantiated in the MRF

Figure 1: Candidate entities MRF model

graph for each possible entity mapping of each

mention instance in a document. Edges capture
entity-entity relatedness. Let xi be the node fea-
ture vector of candidate i and xij be the edge fea-
ture vector of the edge joining candidates i and j.
Each candidate corresponds to a random variable
that takes a binary label, yi ∈ {0, 1}, based on
whether or not it correctly disambiguates the un-
derlying mention. Let C be the set of all cliques
in the MRF and each clique c ∈ C be associated
with a clique potential φc(.). Cliques are restricted
to nodes and edges and their potentials are param-
eterized by log-linear functions of feature vectors;
i.e., log φc(.) = wc · xc, where, xc is the fea-
ture vector of a clique and wc, the correspond-
ing weight vector. The potentials are assumed to
be submodular (Taskar et al., 2004), that is, they
are associated with assignments where variables
in a clique have the same label. Let w0 and w1
be the node feature weights influencing node la-
bels 0 and 1 respectively and w00 and w11 be
the associative edge weights influencing the con-
nected nodes to take the same label. The proba-
bility of a complete graph labeling y is given by
P (y) = 1Z

∏
c∈C φc(yc), where Z is the partition

function. Disambiguation is achieved by doing
MAP inference on this graph.

L(y) = argmax
y∈Y

∑

i∈N
log φi(yi) +

∑

ij∈E
log φij(yij)

= argmin
y∈Y
−(
∑

i∈N
w0 · xi(1− yi) +w1 · xiyi

+
∑

ij∈E
w00 · xij(1− yij) +w11 · xijyij)

(1)

N is the set of nodes and E is the set of edges
in the MRF, yi ∈ {0, 1} and yij = yiyj . For
an MRF with binary labeled nodes and associa-
tive edge potentials, MAP inference can be com-
puted exactly in polynomial time, by finding the
min cut of an augmented flow graph (Boykov and
Kolmogorov, 2004). The MRF is augmented by
adding two special terminal nodes source, s and
sink, t that correspond to the two labels 0/1. For
each node i, we add terminal edges s → i with
weight 〈w0,xi〉 and i → t with weight 〈w1,xi〉.
For each neighborhood edge i → j, we assign
weight 〈(w00 + w11),xij〉. We also add weight
〈w00,xij〉 to the edge s → i and 〈w11,xij〉 to
the edge j → t. MAP inference in original MRF
corresponds to the s/t min cut on this augmented221



graph, with nodes on the s side of the cut getting
a label of 0, and the nodes on the t side being as-
signed a label of 1.

4 Learning Feature Weights

Algorithm 1: MRF Learning algorithm
Data: Training set {X , q̂}, MRF graph g, Slack penalty C, Iterations

T , Step size αt
Result: Weight vector w
w ← 0
t← 1
Nn ← number of nodes in g
fopt ←∞
wopt ← 0
while t ≤ T do

g ← construct flow network from g
q̃ ← s/t mincut of g
∇wξ(w)← 2w + C(q̂ − q̃)T X
w ← w − αt ∇wξ(w)
Project w onto the positive orthant
Compute function value f
if f < fopt then

fopt ← f
wopt ← w

end
t← t+ 1

end
return wopt

The submodularity restriction and binary labels,
make efficient implementation of learning possi-
ble. We jointly learn both node and edge feature
weights following the general max-margin frame-
work described in (Taskar et al., 2004; Vernaza et
al., 2008). Consider a graph with N nodes and
E edges constructed as described above. Follow-
ing Taskar et al., the learning problem can be for-
mulated in terms of the cut vector, such that, we
minimize the norm of the weight vector subject
to the constraint that the desired labeling scores
better than an arbitrary labeling by an amount that
scales with the Hamming distance between the de-
sired and incorrect labelings.

min
w≥0
‖w‖2 (2)

subject to

min
q∈ Q

∑

i,j∈E
w · xij(qij − q̂ij)− (Nn − q̂Tn · qn) ≥ 0

Here, Q is the set of all valid cuts and qij ∈ {0, 1}
indicates if edge i → j is cut (qij = 1). qn is
the cut vector for terminal edges with components
qsi and qit, where, qsi = 1 implies that i is la-
beled 1. q̂ is the cut vector corresponding to the
desired labeling. The first component of the con-
straint captures the difference in cost of the min
cut induced by the weights w and the desired la-
beling. The other component corresponds to the

number of labeling disagreements, Nn being the
number of nodes in the graph (excluding s and t).

By rearranging terms, we obtain

min
w≥0
‖w‖2 (3)

subject to min
q∈ Q

∑

i,j∈E
(wT · xij + q̂ij(δis + δjt))qij

≥ Nn +
∑

i,j∈E
(wT · xij)q̂ij

Here, δij is the Kronecker delta. It can be shown
that the left-hand-side and right-hand-side of the
inequality in the constraint are equivalent (Vernaza
et al., 2008). Moving the constraint to the objec-
tive, we get,

min
w≥0
‖w‖2 + C(Nn +

∑

i,j∈E
(wT · xij)q̂ij−

min
q∈ Q

∑

i,j∈E
(wT · xij + q̂ij(δis + δjt))qij)

Summing over all the documents in the training
set, we get the final objective,

min
w≥0
‖w‖2 +

∑

d∈D
(C (Nd +

∑

i,j∈Ed
(wT · xij)q̂ij −

min
q∈Q

∑

i,j∈Ed
(wT · xij + q̂ij(δis + δjt))qij))

(4)

Here, w = [wT0 w
T
1 w

T
00 w

T
11]

T , Nd is the num-
ber of nodes (excluding s and t) and Ed is the set
of edges in the candidate entity MRF graph for a
document d ∈ D, the set of all training documents,
s and t are special source and sink nodes, respec-
tively. The termNd− q̂ij(δis + δjt))qij gives the
number of misclassified nodes and

∑
i,j∈Ed w

T ·
xij q̂ij −wT · xijqij is the total capacity of incor-
rectly cut edges in the flow graph. C is the penalty
associated with the incorrect labeling. We solved
the formulation (4) using the subgradient descent
method as described in Algorithm 1.

4.1 Handling Unbalanced Training Data

The training data has many more entities labeled
0 as compared to those labeled 1. In our datasets,
we observed a skew of about 3 : 1. This results
in a bias towards the overrepresented class in the222



learning algorithm and the accuracy of the non-
dominant class suffers. We addressed this prob-
lem by assigning separate misclassification penal-
ties C0 and C1 for label 0 and 1 disagreements re-
spectively in equation 4, where, disagreements are
defined as below.

Definition 1. Let li ∈ {0, 1} and l̂i ∈ {0, 1} be the
predicted and actual labels of node i. We say that
a node i has label 0 disagreement if li 6= l̂i = 0.
Similarly it has label 1 disagreement if li 6= l̂i = 1.
Proposition 1. For an edge i → j with qij 6= q̂ij ,
exactly one of the nodes agrees on the label i.e.
li = l̂i (or lj = l̂j) and the other node disagrees
on the label i.e. lj 6= l̂j (li 6= l̂i).

Proof. Case 1: Let qij 6= q̂ij = 0. This implies
that the edge is not cut in the actual labeling and
therefore l̂i = l̂j . However, qij = 1 implies that
li 6= lj . It follows that either li = l̂i or lj = l̂j .
Case 2: Let qij 6= q̂ij = 1. Following a similar
argument as that for case 1 above, we have that
l̂i 6= l̂j and li = lj . Again, it follows that either
li = l̂i or lj = l̂j .

Definition 2. An edge i → j with qij 6= q̂ij , is
said to have a label 0 disagreement if either li 6=
l̂i = 0 or lj 6= l̂j = 0. It is said to have a label 1
disagreement if either li 6= l̂i = 1 or lj 6= l̂j = 1.

4.2 Active Learning

Our online annotation system presents an oppor-
tunity to continuously update the model as more
labeled data becomes available. The commonly
used passive learning approach involves manual
annotation of randomly and independently sam-
pled data. Due to the time and cost associated
with this process, often there is not enough train-
ing data to meet certain level of performance. Ac-
tive learning (Lewis and Catlett, 1994) aims to
minimize the labeling effort, by requesting labels
for the most informative samples, so as to achieve
a desired level of accuracy. While there are several
approaches to querying examples for labeling (Li
and Sethi, 2006), we follow a pragmatic approach,
that can be characterized as least certain querying
method. The method samples examples with the
smallest difference between two highest probabil-
ity classes. Our binary labeled MRF model labels
a node, based on the collective effect of the node
potential and the edge potentials on the edges con-
necting the node to its neighbors. We define cer-

tainty C(i) at a node ni as

C(i) =
∣∣∣
(
w0 · xi +

∑

(ij)∈E:j∈N(i)
w00 · xij

)
(5)

−
(
w1 · xi +

∑

(ij)∈E:j∈N(i)
w11 · xij

)∣∣∣

where N(i) is the set of all neighboring nodes of
the node i. The certainty score C(d) for a docu-
ment d is then computed as the average certainty
score across all nodes in that document.

C(d) =
1

|N |
∑

i∈N
C(i) (6)

The active learning algorithm then queries for a
document with the lowest C(d) and presents the
document for labeling. It might be possible to fur-
ther reduce the labeling effort by requesting labels
for only top k entities in the selected document,
where the entities are ordered in increasing values
of their C(i).

5 Experiments and Results

5.1 Data Sets

We use Wikicur (created by (Kulkarni et al.,
2014)) for training our model and present cross-
validation results. We also evaluate on sev-
eral other datasets from the entity linking litera-
ture. (Kulkarni et al., 2009) had created a dataset
(IITBpart) based on aggressive spotting but as-
suming single attachment. We, therefore, used our
annotation system to manually complete annota-
tions (to create IITBcur) for the documents in this
dataset.

5.2 Evaluation Measures

We follow the fuzzy evaluation measure (Cornolti
et al., 2013) that accounts for slight syntactic and
semantic variations in the match of a predicted and
true annotation, where an annotation a is defined
as the mention-entity pair 〈 m, e 〉. Using their
notion of weak annotation match Mw(a1, a2)4,
we use as performance metrics, Recall, Precision
and F1 micro-averaged over all documents in a
dataset. After factoring out spotter errors, we also
separately report the accuracy of our disambigua-
tion model alone (Referred to as “disambiguator
only”).

4which is true iff mentions m1 and m2 overlap in the in-
put text and entities e1 and e2 are synonyms223



Dataset Disambiguator
only

Weak annota-
tion match

P R F P R F
Wikicur .82 .67 .74 .82 .56 .67
IITBpart .82 .66 .73 .82 .50 .62

Table 1: Non-collective results (only node fea-
tures) on Wikicur set and IITBpart datasets

5.3 Experiments with only Node Features

5.3.1 Is there merit in data curation?

The data curation process presents an opportunity
for continuous training where our inference model
periodically evolves, as more and more data gets
curated. Optionally, in the absence of any curated
data to start with (at time t = t0 when our model
is yet untrained), one could use a Logistic Regres-
sion model, trained on a large uncurated dataset,
to warm-start the data curation process. As data
gets curated and our model is trained, we switch
to our trained model at time t = tk.

We trained binary label LR models using 10000
randomly sampled Wikipedia documents, replac-
ing an original Wikipedia document with its cu-
rated version from Wikicur, one at a time. Fig-
ure 2 plots the training accuracies of these models
for an increasing number of curated documents.
The improvement in accuracy could be explained
by the reduction in false negatives achieved by
virtue of aggressive tagging and multiple attach-
ments in the curated dataset. Based on this obser-
vation, we claim (and verify it in section 5.4.3) that
our MRF model too would benefit from data cura-
tion. At the same time, the use of an LR model for
warm-starting an online annotation system as ours
is strongly recommended.

5.3.2 How does our model perform?

Thereafter, we trained our candidate entity MRF
model on Wikicur dataset using the node features
alone. We report two-fold cross-validation results
on Wikicur and test results on IITBpart (Refer
table 1). These serve as a baseline for our collec-
tive approach.

5.4 Collective Disambiguation

Next, we trained our model using node features
and one or more edge features. Iterations T were
fixed at 600, C was tuned as described below, and
step size (at iteration t), αt = K/

√
t, where K

was empirically set to 0.01.

5.4.1 Effect of C on accuracy
The C parameter in equation 4 acts as a regularizer
and is indicative of the tolerance of disagreement
between predicted and true labels. It was tuned on
the training fold during two-fold cross-validation
onWikicur. Also, to account for the skew in label
0 and 1 instances in the dataset, we penalized label
0 and label 1 disagreements separately, using C0
and C1, respectively. A higher C1 for instance, im-
proves the label 1 recall while adversely impacting
the precision. It is this recall-precision tradeoff for
varying values of C0, C1, that we capture in Fig-
ure 3. We chose the best C0 and C1 from these for
all our experiments.

5.4.2 Effect of Edge Features

Table 2 shows the effect of different edge fea-
tures in a collective setting. The model seems to
benefit the most from the inlink and outlink relat-
edness features, while context overlap-based fea-
tures seem to be noisy. This is understandable
as context overlap-based signals are useful only
for topically coherent entities, which might not
hold true for an aggressively tagged corpus like
ours (Kulkarni et al., 2009).

Edge feature Disambiguator
only

Weak annotation
match

P R F P R F
Category .72 .74 .73 .72 .63 .67

Outlink (O) .84 .67 .74 .84 .57 .68
Inlink (I) .80 .73 .76 .80 .62 .70

Frequent (F) .84 .64 .73 .84 .54 .66
Synopsis .69 .61 .65 .69 .52 .59

Syn. V/Adj. .69 .67 .68 .69 .57 .62
Full text .85 .63 .73 .85 .54 .66

All features .44 .50 .47 .44 .42 .43
I+O .85 .67 .74 .85 .56 .68

I+O+F .79 .74 .76 .79 .63 .70

Table 2: Effect of edge features: two-fold cross
validation on Wikicur. Edge features that showed
improvement over node features are shown in
bold.

5.4.3 Does training help?

We sampled 50 documents from the Wikicur
dataset, 5 at a time and used them for training, ap-
plying both passive (PL) and active learning (AL).
The F1 measure evaluated on an independent test
set of 30 documents is shown in the plot (Refer to
figure 4). TheF1 on training set seems to fluctuate,
more so for Train-AL (Chen et al., 2006), but the
F1 on test set does show a steady improvement.224



Figure 2: Effect of data cura-
tion

Figure 3: Effect of varying C:
C0 and C1 Figure 4: Effect of training

IITBpart AQUAINT MSNBC
Annotator F P R F P R F P R
AIDA .07 .66 .04 .21 .35 .15 .47 .75 .35
Wikify! .37 .55 .28 .34 .29 .42 .41 .34 .51
TagMe .44 .45 .42 .51 .46 .57 .52 .48 .55
Wikipedia Miner .52 .57 .48 .47 .38 .63 .46 .55 .36
Illionis Wikifier .44 .58 .36 .34 .29 .42 .41 .34 .51
Our Model (Node+I) .67 .76 .60 .78 .81 .74 .67 .68 .66
Our Model
(Node+I+O+F)

.65 .69 .61 .79 .82 .75 .66 .63 .69

Table 3: Comparison with publicly available sys-
tems (as reported by (Cornolti et al., 2013)) on
three datasets

5.4.4 Comparison with collective approaches

We compared our system against several other
collective annotation approaches: AIDA (Hof-
fart et al., 2011), Wikify! (Mihalcea and Cso-
mai, 2007), TagMe (Ferragina and Scaiella, 2010),
Wikipedia Miner (Milne and Witten, 2008) and
Illionis Wikifier (Ratinov et al., 2011) on three
datasets viz. IITBpart, AQUAINT (Wikipedia
Miner) and MSNBC (Cucerzan, 2007). Our sys-
tem consistently beats all these systems on all the
three datasets (Table 3). Some of the other collec-
tive annotation systems like Cucerzan (F1 : .45),
CSAW (Kulkarni et al., 2009) (F1 : .69), (Han
et al., 2011) (F1 : .73), and (Han and Sun, 2012)
(F1 : .8) have used CSAW’s evaluation measure to
evaluate on IITBpart. We achieved an F1 of 0.6
using the same measure. The relatively lower F1
on this dataset could be attributed to inconsisten-
cies between the ground truth and our knowledge
base. During our manual annotation of IITBpart,
we came across over 8000 annotations that were
either added or removed5 to create the IITBcur
dataset.

We evaluated our system on the ERD dataset
and achieved R:.62, P:.66, F1 : .64. We be-
lieve that our system benefits from model training,
thereby performing better than that of (Kulkarni

5due to erroneous annotations or newer Wikipedia dump

et al., 2014) (F1 : .61). While some of the other
systems (Cornolti et al., 2014) at ERD performed
better, this could be attributed to their choice of
features. Our system offers an end-end annotation
framework that is interactive and jointly trains fea-
ture weights.

5.5 Results on IITBcur
Section 6 shows some examples of incomplete an-
notations in the IITBpart dataset. It is precisely
such cases that we tried to correct during data
preparation. Finally, we report the accuracy of
our model on the IITBcur dataset - P : 77.4%,
R : 54.3%, F1 : 63.8%.

5.6 Performance Evaluation

While our model allows for efficient inference and
learning, graph construction itself is an expen-
sive operation. For a document with |Ed| can-
didate entities, the graph construction complex-
ity is O

(
|Ed|2

)
. For documents in the Wikicur

set with 190 candidate entities on an average,
the average graph construction time was about 57
seconds. For the relatively larger documents in
the IITBcur dataset, the average graph construc-
tion time was around 1.5 minutes. The perfor-
mance could be improved by (a) pre-computing
the entity-entity features for all entities in the
knowledge base (b) dividing input document into
chunks and performing graph construction and in-
ference in parallel.

The running time for inference (Figure 5) shows
a slightly quadratic behavior in the number of can-
didate entities |Ed| of a document. Inference on
most documents runs in under 0.5 seconds.On the
relatively sparser Inlink+Outlink graphs (Refer to
Figure 6), training is much faster than the more
dense Category graphs. The faster training hap-
pens without trading off much on accuracy as can
be seen in Table 2. For our experiments, the model
was retrained at time t using all the available train-225



0 

500 

1000 

1500 

2000 

2500 

3000 

0 100 200 300 400 

Ti
m

e
 in

 m
ill

is
e

co
n

d
s 

#nodes 

(-,-,-) (-,-,1) (2,1,-) (3,2,-) (2,1,2) 

Figure 5: Running time for inference on Wikicur

ing data. While this might be acceptable for offline
training, online systems might benefit from faster
incremental training approaches.

0 

10 

20 

30 

40 

50 

60 

70 

80 

5 10 15 20 25 30 35 40 45 50 

Ti
m

e
 (

m
in

s.
) 

Size of training data 

Time for 100 iterations (C) Time for 100 iterations (I+O) 

Figure 6: Scalability of training

6 Challenges with data curation

Data curation is a tedious and challenging task. Its
inherent ambiguity often introduces annotator bias
leading to either incomplete or ambiguous annota-
tions in the curated data.

1. There might be cases when two or more en-
tities are correct as attachments for a men-
tion. E.g., mention ‘Barack Obama’ can
be tagged as Barack Obama or President of
United States, and both might seem correct
in the context that it appeared. A ‘one entity
per mention’ assumption makes it impossible
to honor such cases.

2. Human annotators often limit their attention
to the candidate entities retrieved by the spot-
ter and very rarely search the catalog for any
missed candidates. This results in a lot of

missed annotations and often many mentions
getting no attachments (NA).

3. Annotators also seem biased towards entity
names that match with the mention text.
However, this is often not true. E.g. a men-
tion of ‘cone snail’ disambiguates to Conidae
and Conus.

4. Wikipedia contains many disambiguation
pages that often show up in the candidate set
for a mention. Tagging a mention with a dis-
ambiguation page seems to beat the very pur-
pose of a disambiguation system. Ideally, the
mention should be annotated with one of the
entities on the disambiguation page or NA if
none of them is semantically right.

Table 4 shows some of these cases from the
IITBpart dataset. It is cases like these that we
attempted to correct in coming up with the curated
IITBcur dataset.

7 Conclusion

We presented an approach to jointly train the node
and edge features of a collective disambiguation
model for the purpose of entity linking. Our sys-
tem leverages active learning to bring down la-
beling effort. Experiments show that the model
benefits from training and improves with the avail-
ability of more labeled data. We consistently per-
formed better than many other systems on various
datasets. It also scales reasonably well and with
suggested tweaks can be used for large scale doc-
ument annotation.

References
Indrajit Bhattacharya and Lise Getoor. 2006. A la-

tent dirichlet model for unsupervised entity resolu-
tion. In SIAM INTERNATIONAL CONFERENCE
ON DATA MINING.

Y. Boykov and V. Kolmogorov. 2004. An experi-
mental comparison of min-cut/max-flow algorithms
for energy minimization in vision. Pattern Analy-
sis and Machine Intelligence, IEEE Transactions on,
26(9):1124–1137.

Razvan Bunescu and M Pasca. 2006. Using encyclo-
pedic knowledge for named entity disambiguation.
Proceedings of EACL, pages 9–16.

Soumen Chakrabarti, Kriti Puniyani, and Sujatha Das.
2006. Optimizing scoring functions and indexes
for proximity search in type-annotated corpora. In226



Ground Mention Ground Entity Predicted Entity Remarks
lifestyle Lifestyle NA Disambiguation page attachment

harsh reality NA Reality Incomplete data
effort NA Energy Incomplete data

self discipline Discipline self → Self, discipline → Discipline Overlapping mentions
god God (male deity) God Multiple correct entities

intellect Intelligence Intellect Multiple correct entities

Table 4: Examples of predictions on the IITBpart highlighting the challenges in data curation

Proceedings of the 15th international conference on
World Wide Web, WWW ’06, pages 717–726, New
York, NY, USA. ACM.

Jinying Chen, Andrew Schein, Lyle Ungar, and Martha
Palmer. 2006. An empirical study of the behav-
ior of active learning for word sense disambigua-
tion. In Proceedings of the Main Conference on Hu-
man Language Technology Conference of the North
American Chapter of the Association of Computa-
tional Linguistics, HLT-NAACL ’06, pages 120–
127, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.

Tao Cheng, Xifeng Yan, and Kevin C. Chang. 2007.
Entityrank: searching entities directly and holisti-
cally. In VLDB ’07: Proceedings of the 33rd inter-
national conference on Very large data bases, pages
387–398. VLDB Endowment.

Marco Cornolti, Paolo Ferragina, and Massimiliano
Ciaramita. 2013. A framework for benchmarking
entity-annotation systems. In Proceedings of the
22nd international conference on World Wide Web,
WWW ’13, pages 249–260, Republic and Canton of
Geneva, Switzerland. International World Wide Web
Conferences Steering Committee.

Marco Cornolti, Paolo Ferragina, Massimiliano Cia-
ramita, Hinrich Schütze, and Stefan Rüd. 2014. The
smaph system for query entity recognition and dis-
ambiguation. In Proceedings of the First Interna-
tional Workshop on Entity Recognition &#38; Dis-
ambiguation, ERD ’14, pages 25–30, New York,
NY, USA. ACM.

Silviu Cucerzan. 2007. Large-scale named entity dis-
ambiguation based on wikipedia data. In Proceed-
ings of EMNLP-CoNLL, volume 6, pages 708–716.

Stephen Dill, Nadav Eiron, David Gibson, Daniel
Gruhl, R. Guha, Anant Jhingran, Tapas Kanungo,
Sridhar Rajagopalan, Andrew Tomkins, John A.
Tomlin, and Jason Y. Zien. 2003. Semtag and
seeker: bootstrapping the semantic web via auto-
mated semantic annotation. In Proceedings of the
12th international conference on World Wide Web,
WWW ’03, pages 178–186, New York, NY, USA.
ACM.

Angela Fahrni and Michael Strube. 2012. Jointly
disambiguating and clustering concepts and entities
with markov logic. In COLING, pages 815–832.

Paolo Ferragina and Ugo Scaiella. 2010. Tagme:
on-the-fly annotation of short text fragments (by
wikipedia entities). In Proceedings of the 19th ACM
international conference on Information and knowl-
edge management, CIKM ’10, pages 1625–1628,
New York, NY, USA. ACM.

William A. Gale, Kenneth W. Church, and David
Yarowsky. 1992. One sense per discourse. In Pro-
ceedings of the workshop on Speech and Natural
Language, HLT ’91, pages 233–237, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.

Xianpei Han and Le Sun. 2012. An entity-topic model
for entity linking. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, EMNLP-CoNLL ’12, pages 105–
115, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.

Xianpei Han and Jun Zhao. 2009. Named entity
disambiguation by leveraging wikipedia semantic
knowledge. In Proceedings of the 18th ACM con-
ference on Information and knowledge management,
CIKM ’09, pages 215–224, New York, NY, USA.
ACM.

Xianpei Han, Le Sun, and Jun Zhao. 2011. Collective
entity linking in web text: a graph-based method. In
Proceedings of the 34th international ACM SIGIR
conference on Research and development in Infor-
mation Retrieval, pages 765–774. ACM.

Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bor-
dino, Hagen Fürstenau, Manfred Pinkal, Marc Span-
iol, Bilyana Taneva, Stefan Thater, and Gerhard
Weikum. 2011. Robust disambiguation of named
entities in text. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP ’11, pages 782–792, Stroudsburg, PA,
USA. Association for Computational Linguistics.

Gjergji Kasneci, Fabian M. Suchanek, Georgiana Ifrim,
Shady Elbassuoni, Maya Ramanath, and Gerhard
Weikum. 2008. Naga: harvesting, searching and
ranking knowledge. In Proceedings of the 2008
ACM SIGMOD international conference on Man-
agement of data, SIGMOD ’08, pages 1285–1288,
New York, NY, USA. ACM.

Saurabh S. Kataria, Krishnan S. Kumar, Rajeev R. Ras-
togi, Prithviraj Sen, and Srinivasan H. Sengamedu.
2011. Entity disambiguation with hierarchical topic227



models. In Proceedings of the 17th ACM SIGKDD
international conference on Knowledge discovery
and data mining, KDD ’11, pages 1037–1045, New
York, NY, USA. ACM.

Sayali Kulkarni, Amit Singh, Ganesh Ramakrishnan,
and Soumen Chakrabarti. 2009. Collective anno-
tation of Wikipedia entities in web text. Proceed-
ings of the 15th ACM SIGKDD international con-
ference on Knowledge discovery and data mining -
KDD ’09, page 457.

Ashish Kulkarni, Kanika Agarwal, Pararth Shah,
Sunny Raj Rathod, and Ganesh Ramakrishnan.
2014. System for collective entity disambiguation.
In Proceedings of the First International Workshop
on Entity Recognition &#38; Disambiguation, ERD
’14, pages 111–118, New York, NY, USA. ACM.

David D. Lewis and Jason Catlett. 1994. Heteroge-
neous uncertainty sampling for supervised learning.
In In Proceedings of the 11th International Confer-
ence on Machine Learning (ICML, pages 148–156.
Morgan Kaufmann.

Mingkun Li and Ishwar K. Sethi. 2006. Confidence-
based active learning. IEEE Trans. Pattern Anal.
Mach. Intell., 28(8):1251–1261, August.

Xiaonan Li, Chengkai Li, and Cong Yu. 2010. Enti-
tyengine: answering entity-relationship queries us-
ing shallow semantics. In Proceedings of the 19th
ACM international conference on Information and
knowledge management, CIKM ’10, pages 1925–
1926, New York, NY, USA. ACM.

Thomas Lin, Mausam, and Oren Etzioni. 2012. En-
tity linking at web scale. In Proceedings of the Joint
Workshop on Automatic Knowledge Base Construc-
tion and Web-scale Knowledge Extraction, AKBC-
WEKEX ’12, pages 84–88, Stroudsburg, PA, USA.
Association for Computational Linguistics.

Paul McNamee. 2009. Overview of the tac 2009
knowledge base population track.

Rada Mihalcea and Andras Csomai. 2007. Wikify!:
linking documents to encyclopedic knowledge. In
Proceedings of the sixteenth ACM conference on
Conference on information and knowledge manage-
ment, pages 233–242. ACM.

David Milne and Ian H Witten. 2008. Learning to link
with wikipedia. In Proceedings of the 17th ACM
conference on Information and knowledge manage-
ment, pages 509–518. ACM.

Lev Ratinov, Dan Roth, Doug Downey, and Mike
Anderson. 2011. Local and global algorithms
for disambiguation to wikipedia. In Proceedings
of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies - Volume 1, HLT ’11, pages 1375–1384,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.

Ben Taskar and Daphne Koller. 2001. Learning Asso-
ciative Markov Networks.

B. Taskar, V. Chatalbashev, and D. Koller. 2004.
Learning associative markov networks. In Proceed-
ings of the twenty-first international conference on
Machine learning, page 102. ACM.

P. Vernaza, B. Taskar, and D.D. Lee. 2008. Online,
self-supervised terrain classification via discrimina-
tively trained submodular markov random fields.
In Robotics and Automation, 2008. ICRA 2008.
IEEE International Conference on, pages 2750–
2757. IEEE.

Ben Wellner, Andrew McCallum, Fuchun Peng, and
Michael Hay. 2004. An integrated, conditional
model of information extraction and coreference
with application to citation matching. In Proceed-
ings of the 20th Conference on Uncertainty in Arti-
ficial Intelligence, UAI ’04, pages 593–601, Arling-
ton, Virginia, United States. AUAI Press.

Michael L. Wick, Aron Culotta, Khashayar Rohani-
manesh, and Andrew McCallum. 2009. An entity
based model for coreference resolution. In SDM,
pages 365–376.

Yiping Zhou, Lan Nie, Omid Rouhani-Kalleh, Fla-
vian Vasile, and Scott Gaffney. 2010. Resolving
surface forms to wikipedia topics. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics, COLING ’10, pages 1335–1343,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.

228


