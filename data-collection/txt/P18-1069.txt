
























































untitled


Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 743–753
Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics

743

Confidence Modeling for Neural Semantic Parsing

Li Dong†∗ and Chris Quirk‡ and Mirella Lapata†
† School of Informatics, University of Edinburgh

‡ Microsoft Research, Redmond
li.dong@ed.ac.uk chrisq@microsoft.com mlap@inf.ed.ac.uk

Abstract

In this work we focus on confidence mod-
eling for neural semantic parsers which
are built upon sequence-to-sequence mod-
els. We outline three major causes of un-
certainty, and design various metrics to
quantify these factors. These metrics are
then used to estimate confidence scores
that indicate whether model predictions
are likely to be correct. Beyond confi-
dence estimation, we identify which parts
of the input contribute to uncertain pre-
dictions allowing users to interpret their
model, and verify or refine its input. Ex-
perimental results show that our confi-
dence model significantly outperforms a
widely used method that relies on poste-
rior probability, and improves the quality
of interpretation compared to simply rely-
ing on attention scores.

1 Introduction

Semantic parsing aims to map natural language
text to a formal meaning representation (e.g., log-
ical forms or SQL queries). The neural sequence-
to-sequence architecture (Sutskever et al., 2014;
Bahdanau et al., 2015) has been widely adopted
in a variety of natural language processing tasks,
and semantic parsing is no exception. However,
despite achieving promising results (Dong and
Lapata, 2016; Jia and Liang, 2016; Ling et al.,
2016), neural semantic parsers remain difficult to
interpret, acting in most cases as a black box,
not providing any information about what made
them arrive at a particular decision. In this work,
we explore ways to estimate and interpret the

∗Work carried out during an internship at Microsoft Re-
search.

model’s confidence in its predictions, which we ar-
gue can provide users with immediate and mean-
ingful feedback regarding uncertain outputs.

An explicit framework for confidence modeling
would benefit the development cycle of neural se-
mantic parsers which, contrary to more traditional
methods, do not make use of lexicons or templates
and as a result the sources of errors and inconsis-
tencies are difficult to trace. Moreover, from the
perspective of application, semantic parsing is of-
ten used to build natural language interfaces, such
as dialogue systems. In this case it is important
to know whether the system understands the input
queries with high confidence in order to make de-
cisions more reliably. For example, knowing that
some of the predictions are uncertain would al-
low the system to generate clarification questions,
prompting users to verify the results before trig-
gering unwanted actions. In addition, the training
data used for semantic parsing can be small and
noisy, and as a result, models do indeed produce
uncertain outputs, which we would like our frame-
work to identify.

A widely-used confidence scoring method is
based on posterior probabilities p (y|x) where x
is the input and y the model’s prediction. For a
linear model, this method makes sense: as more
positive evidence is gathered, the score becomes
larger. Neural models, in contrast, learn a compli-
cated function that often overfits the training data.
Posterior probability is effective when making de-
cisions about model output, but is no longer a good
indicator of confidence due in part to the nonlin-
earity of neural networks (Johansen and Socher,
2017). This observation motivates us to develop
a confidence modeling framework for sequence-
to-sequence models. We categorize the causes of
uncertainty into three types, namely model uncer-
tainty, data uncertainty, and input uncertainty and
design different metrics to characterize them.



744

We compute these confidence metrics for a
given prediction and use them as features in a re-
gression model which is trained on held-out data
to fit prediction F1 scores. At test time, the re-
gression model’s outputs are used as confidence
scores. Our approach does not interfere with
the training of the model, and can be thus ap-
plied to various architectures, without sacrificing
test accuracy. Furthermore, we propose a method
based on backpropagation which allows to inter-
pret model behavior by identifying which parts of
the input contribute to uncertain predictions.

Experimental results on two semantic parsing
datasets (IFTTT, Quirk et al. 2015; and DJANGO,
Oda et al. 2015) show that our model is supe-
rior to a method based on posterior probability.
We also demonstrate that thresholding confidence
scores achieves a good trade-off between coverage
and accuracy. Moreover, the proposed uncertainty
backpropagation method yields results which are
qualitatively more interpretable compared to those
based on attention scores.

2 Related Work

Confidence Estimation Confidence estimation
has been studied in the context of a few NLP
tasks, such as statistical machine translation (Blatz
et al., 2004; Ueffing and Ney, 2005; Soricut and
Echihabi, 2010), and question answering (Gondek
et al., 2012). To the best of our knowledge, con-
fidence modeling for semantic parsing remains
largely unexplored. A common scheme for model-
ing uncertainty in neural networks is to place dis-
tributions over the network’s weights (Denker and
Lecun, 1991; MacKay, 1992; Neal, 1996; Blun-
dell et al., 2015; Gan et al., 2017). But the result-
ing models often contain more parameters, and the
training process has to be accordingly changed,
which makes these approaches difficult to work
with. Gal and Ghahramani (2016) develop a the-
oretical framework which shows that the use of
dropout in neural networks can be interpreted as
a Bayesian approximation of Gaussian Process.
We adapt their framework so as to represent un-
certainty in the encoder-decoder architectures, and
extend it by adding Gaussian noise to weights.

Semantic Parsing Various methods have been
developed to learn a semantic parser from natural
language descriptions paired with meaning repre-
sentations (Tang and Mooney, 2000; Zettlemoyer
and Collins, 2007; Lu et al., 2008; Kwiatkowski

et al., 2011; Andreas et al., 2013; Zhao and Huang,
2015). More recently, a few sequence-to-sequence
models have been proposed for semantic parsing
(Dong and Lapata, 2016; Jia and Liang, 2016;
Ling et al., 2016) and shown to perform compet-
itively whilst eschewing the use of templates or
manually designed features. There have been sev-
eral efforts to improve these models including the
use of a tree decoder (Dong and Lapata, 2016),
data augmentation (Jia and Liang, 2016; Kočiský
et al., 2016), the use of a grammar model (Xiao
et al., 2016; Rabinovich et al., 2017; Yin and Neu-
big, 2017; Krishnamurthy et al., 2017), coarse-to-
fine decoding (Dong and Lapata, 2018), network
sharing (Susanto and Lu, 2017; Herzig and Berant,
2017), user feedback (Iyer et al., 2017), and trans-
fer learning (Fan et al., 2017). Current semantic
parsers will by default generate some output for
a given input even if this is just a random guess.
System results can thus be somewhat unexpected
inadvertently affecting user experience. Our goal
is to mitigate these issues with a confidence scor-
ing model that can estimate how likely the predic-
tion is correct.

3 Neural Semantic Parsing Model

In the following section we describe the neural se-
mantic parsing model (Dong and Lapata, 2016; Jia
and Liang, 2016; Ling et al., 2016) we assume
throughout this paper. The model is built upon
the sequence-to-sequence architecture and is illus-
trated in Figure 1. An encoder is used to encode
natural language input q = q1 · · · q|q| into a vec-
tor representation, and a decoder learns to gen-
erate a logical form representation of its mean-
ing a = a1 · · · a|a| conditioned on the encoding
vectors. The encoder and decoder are two differ-
ent recurrent neural networks with long short-term
memory units (LSTMs; Hochreiter and Schmid-
huber 1997) which process tokens sequentially.
The probability of generating the whole sequence
p (a|q) is factorized as:

p (a|q) =
|a|∏

t=1

p (at|a<t, q) (1)

where a<t = a1 · · · at−1.
Let et ∈ Rn denote the hidden vector of

the encoder at time step t. It is computed via
et = fLSTM (et−1,qt), where fLSTM refers to the
LSTM unit, and qt ∈ Rn is the word embedding



745

… …

… <s> …

… …

i)

iii)

i)

ii)

iv)

Figure 1: We use dropout as approximate
Bayesian inference to obtain model uncertainty.
The dropout layers are applied to i) token vectors;
ii) the encoder’s output vectors; iii) bridge vectors;
and iv) decoding vectors.

of qt. Once the tokens of the input sequence are
encoded into vectors, e|q| is used to initialize the
hidden states of the first time step in the decoder.

Similarly, the hidden vector of the de-
coder at time step t is computed by dt =
fLSTM (dt−1,at−1), where at−1 ∈ Rn is the word
vector of the previously predicted token. Addi-
tionally, we use an attention mechanism (Luong
et al., 2015a) to utilize relevant encoder-side con-
text. For the current time step t of the decoder, we
compute its attention score with the k-th hidden
state in the encoder as:

rt,k ∝ exp{dt · ek} (2)

where
∑|q|

j=1 rt,j = 1. The probability of generat-
ing at is computed via:

ct =

|q|∑

k=1

rt,kek (3)

dattt = tanh (W1dt +W2ct) (4)

p (at|a<t, q) = softmaxat
(
Wod

att
t

)
(5)

where W1,W2 ∈ Rn×n and Wo ∈ R|Va|×n are
three parameter matrices.

The training objective is to maximize the like-
lihood of the generated meaning representation a
given input q, i.e., maximize

∑
(q,a)∈D log p (a|q),

where D represents training pairs. At test time,
the model’s prediction for input q is obtained via
â = argmaxa′ p (a

′|q), where a′ represents can-
didate outputs. Because p (a|q) is factorized as
shown in Equation (1), we can use beam search
to generate tokens one by one rather than iterating
over all possible results.

4 Confidence Estimation

Given input q and its predicted meaning rep-
resentation a, the confidence model estimates

Algorithm 1 Dropout Perturbation
Input: q, a: Input and its prediction

M: Model parameters
1: for i ← 1, · · · , F do
2: M̂i ← Apply dropout layers to M � Figure 1
3: Run forward pass and compute p̂(a|q;M̂i)
4: Compute variance of {p̂(a|q;M̂i)}Fi=1 � Equation (6)

score s (q, a) ∈ (0, 1). A large score indicates
the model is confident that its prediction is correct.
In order to gauge confidence, we need to estimate
“what we do not know”. To this end, we iden-
tify three causes of uncertainty, and design various
metrics characterizing each one of them. We then
feed these metrics into a regression model in order
to predict s (q, a).

4.1 Model Uncertainty
The model’s parameters or structures contain un-
certainty, which makes the model less confident
about the values of p (a|q). For example, noise in
the training data and the stochastic learning algo-
rithm itself can result in model uncertainty. We
describe metrics for capturing uncertainty below:

Dropout Perturbation Our first metric uses
dropout (Srivastava et al., 2014) as approxi-
mate Bayesian inference to estimate model un-
certainty (Gal and Ghahramani, 2016). Dropout
is a widely used regularization technique during
training, which relieves overfitting by randomly
masking some input neurons to zero according
to a Bernoulli distribution. In our work, we use
dropout at test time, instead. As shown in Algo-
rithm 1, we perform F forward passes through the
network, and collect the results {p̂(a|q;M̂i)}Fi=1
where M̂i represents the perturbed parameters.
Then, the uncertainty metric is computed by the
variance of results. We define the metric on the
sequence level as:

var{p̂(a|q;M̂i)}Fi=1. (6)

In addition, we compute uncertainty uat at the
token-level at via:

uat = var{p̂(at|a<t, q;M̂i)}Fi=1 (7)

where p̂(at|a<t, q;M̂i) is the probability of
generating token at (Equation (5)) using per-
turbed model M̂i. We operationalize token-
level uncertainty in two ways, as the aver-
age score avg{uat}|a|t=1 and the maximum score



746

max{uat}|a|t=1 (since the uncertainty of a sequence
is often determined by the most uncertain token).
As shown in Figure 1, we add dropout layers in
i) the word vectors of the encoder and decoder
qt,at; ii) the output vectors of the encoder et;
iii) bridge vectors e|q| used to initialize the hid-
den states of the first time step in the decoder; and
iv) decoding vectors dattt (Equation (4)).

Gaussian Noise Standard dropout can be
viewed as applying noise sampled from a
Bernoulli distribution to the network parameters.
We instead use Gaussian noise, and apply the
metrics in the same way discussed above. Let v
denote a vector perturbed by noise, and g a vector
sampled from the Gaussian distribution N (0, σ2).
We use v̂ = v + g and v̂ = v + v � g as two
noise injection methods. Intuitively, if the model
is more confident in an example, it should be more
robust to perturbations.

Posterior Probability Our last class of metrics
is based on posterior probability. We use the log
probability log p(a|q) as a sequence-level metric.
The token-level metric min{p(at|a<t, q)}|a|t=1 can
identify the most uncertain predicted token. The
perplexity per token − 1|a|

∑|a|
t=1 log p (at|a<t, q) is

also employed.

4.2 Data Uncertainty

The coverage of training data also affects the un-
certainty of predictions. If the input q does not
match the training distribution or contains un-
known words, it is difficult to predict p (a|q) re-
liably. We define two metrics:

Probability of Input We train a language model
on the training data, and use it to estimate the
probability of input p(q|D) where D represents the
training data.

Number of Unknown Tokens Tokens that do
not appear in the training data harm robustness,
and lead to uncertainty. So, we use the number of
unknown tokens in the input q as a metric.

4.3 Input Uncertainty

Even if the model can estimate p (a|q) reliably, the
input itself may be ambiguous. For instance, the
input the flight is at 9 o’clock can be interpreted as
either flight time(9am) or flight time(9pm). Se-
lecting between these predictions is difficult, es-
pecially if they are both highly likely. We use the

following metrics to measure uncertainty caused
by ambiguous inputs.

Variance of Top Candidates We use the vari-
ance of the probability of the top candidates to in-
dicate whether these are similar. The sequence-
level metric is computed by:

var{p(ai|q)}Ki=1
where a1 . . . aK are the K-best predictions ob-
tained by the beam search during inference (Sec-
tion 3).

Entropy of Decoding The sequence-level en-
tropy of the decoding process is computed via:

H[a|q] = −
∑

a′
p(a′|q) log p(a′|q)

which we approximate by Monte Carlo sampling
rather than iterating over all candidate predic-
tions. The token-level metrics of decoding en-
tropy are computed by avg{H[at|a<t, q]}|a|t=1 and
max{H[at|a<t, q]}|a|t=1.
4.4 Confidence Scoring
The sentence- and token-level confidence metrics
defined in Section 4 are fed into a gradient tree
boosting model (Chen and Guestrin, 2016) in or-
der to predict the overall confidence score s (q, a).
The model is wrapped with a logistic function so
that confidence scores are in the range of (0, 1).

Because the confidence score indicates whether
the prediction is likely to be correct, we can use the
prediction’s F1 (see Section 6.2) as target value.
The training loss is defined as:

∑

(q,a)∈D
ln(1+e−ŝ(q,a))yq,a+ ln(1+eŝ(q,a))(1−yq,a)

where D represents the data, yq,a is the target F1
score, and ŝ(q, a) the predicted confidence score.
We refer readers to Chen and Guestrin (2016)
for mathematical details of how the gradient tree
boosting model is trained. Notice that we learn
the confidence scoring model on the held-out set
(rather than on the training data of the semantic
parser) to avoid overfitting.

5 Uncertainty Interpretation

Confidence scores are useful in so far they can be
traced back to the inputs causing the uncertainty
in the first place. For semantic parsing, identifying



747

Backpropagation

: score of neuron 
: contribution ratio
(from to )

Figure 2: Uncertainty backpropagation at the neu-
ron level. Neuron m’s score um is collected from
child neurons c1 and c2 by um = vc1muc1 + v

c2
muc2 .

The score um is then redistributed to its parent
neurons p1 and p2, which satisfies vmp1 + v

m
p2 = 1.

which input words contribute to uncertainty would
be of value, e.g., these could be treated explicitly
as special cases or refined if they represent noise.

In this section, we introduce an algorithm
that backpropagates token-level uncertainty scores
(see Equation (7)) from predictions to input to-
kens, following the ideas of Bach et al. (2015) and
Zhang et al. (2016). Let um denote neuron m’s
uncertainty score, which indicates the degree to
which it contributes to uncertainty. As shown in
Figure 2, um is computed by the summation of the
scores backpropagated from its child neurons:

um =
∑

c∈Child(m)
vcmuc

where Child(m) is the set of m’s child neurons,
and the non-negative contribution ratio vcm indi-
cates how much we backpropagate uc to neu-
ron m. Intuitively, if neuron m contributes more
to c’s value, ratio vcm should be larger.

After obtaining score um, we redistribute it to
its parent neurons in the same way. Contribution
ratios from m to its parent neurons are normalized
to 1: ∑

p∈Parent(m)
vmp = 1

where Parent(m) is the set of m’s parent neurons.
Given the above constraints, we now define

different backpropagation rules for the operators
used in neural networks. We first describe the rules
used for fully-connected layers. Let x denote the
input. The output is computed by z = σ(Wx+b),
where σ is a nonlinear function, W ∈ R|z|∗|x| is
the weight matrix, b ∈ R|z| is the bias, and neu-
ron zi is computed via zi = σ(

∑|x|
j=1Wi,jxj +

bi). Neuron xk’s uncertainty score uxk is gath-

Algorithm 2 Uncertainty Interpretation
Input: q, a: Input and its prediction
Output: {ûqt}|q|t=1: Interpretation scores for input tokens
Function: TokenUnc: Get token-level uncertainty

1: � Get token-level uncertainty for predicted tokens
2: {uat}|a|t=1 ← TokenUnc(q, a)
3: � Initialize uncertainty scores for backpropagation
4: for t ← 1, · · · , |a| do
5: Decoder classifier’s output neuron ← uat
6: � Run backpropagation
7: for m ← neuron in backward topological order do
8: � Gather scores from child neurons
9: um ← ∑c∈Child(m) vcmuc

10: � Summarize scores for input words
11: for t ← 1, · · · , |q| do
12: uqt ←

∑
c∈qt uc

13: {ûqt}|q|t=1 ← normalize {uqt}|q|t=1

ered from the next layer:

uxk =

|z|∑

i=1

vzixkuzi =

|z|∑

i=1

|Wi,kxk|
∑|x|

j=1 |Wi,jxj |
uzi

ignoring the nonlinear function σ and the bias b.
The ratio vzixk is proportional to the contribution of
xk to the value of zi.

We define backpropagation rules for element-
wise vector operators. For z = x± y, these are:

uxk =
|xk|

|xk|+|yk|uzk uyk =
|yk|

|xk|+|yk|uzk

where the contribution ratios vzkxk and v
zk
yk

are de-
termined by |xk| and |yk|. For multiplication, the
contribution of two elements in 13 ∗3 should be the
same. So, the propagation rules for z = x�y are:

uxk=
| log |xk||

| log |xk||+| log |yk||uzk uyk=
| log |yk||

| log |xk||+| log |yk||uzk

where the contribution ratios are determined by
| log |xk|| and | log |yk||.

For scalar multiplication, z = λx where λ de-
notes a constant. We directly assign z’s uncer-
tainty scores to x and the backpropagation rule is
uxk = uzk .

As shown in Algorithm 2, we first initial-
ize uncertainty backpropagation in the decoder
(lines 1–5). For each predicted token at, we com-
pute its uncertainty score uat as in Equation (7).
Next, we find the dimension of at in the decoder’s
softmax classifier (Equation (5)), and initialize the
neuron with the uncertainty score uat . We then
backpropagate these uncertainty scores through



748

Dataset Example

IFTTT turn android phone to full volume at 7am monday to friday
date time−every day of the week at−((time of day (07)(:)(00)) (days of the week

(1)(2)(3)(4)(5))) THEN android device−set ringtone volume−(volume ({’
volume level’:1.0,’name’:’100%’}))

DJANGO for every key in sorted list of user settings
for key in sorted(user settings):

Table 1: Natural language descriptions and their meaning representations from IFTTT and DJANGO.

the network (lines 6–9), and finally into the neu-
rons of the input words. We summarize them and
compute the token-level scores for interpreting the
results (line 10–13). For input word vector qt, we
use the summation of its neuron-level scores as the
token-level score:

ûqt ∝
∑

c∈qt
uc

where c ∈ qt represents the neurons of word vec-
tor qt, and

∑|q|
t=1 ûqt = 1. We use the normalized

score ûqt to indicate token qt’s contribution to pre-
diction uncertainty.

6 Experiments

In this section we describe the datasets used in
our experiments and various details concerning
our models. We present our experimental re-
sults and analysis of model behavior. Our code is
publicly available at https://github.com/
donglixp/confidence.

6.1 Datasets
We trained the neural semantic parser introduced
in Section 3 on two datasets covering different do-
mains and meaning representations. Examples are
shown in Table 1.

IFTTT This dataset (Quirk et al., 2015) con-
tains a large number of if-this-then-that programs
crawled from the IFTTT website. The programs
are written for various applications, such as home
security (e.g., “email me if the window opens”),
and task automation (e.g., “save instagram pho-
tos to dropbox”). Whenever a program’s trigger is
satisfied, an action is performed. Triggers and ac-
tions represent functions with arguments; they are
selected from different channels (160 in total) rep-
resenting various services (e.g., Android). There
are 552 trigger functions and 229 action func-
tions. The original split contains 77, 495 training,
5, 171 development, and 4, 294 test instances. The

subset that removes non-English descriptions was
used in our experiments.

DJANGO This dataset (Oda et al., 2015) is built
upon the code of the Django web framework. Each
line of Python code has a manually annotated nat-
ural language description. Our goal is to map the
English pseudo-code to Python statements. This
dataset contains diverse use cases, such as itera-
tion, exception handling, and string manipulation.
The original split has 16, 000 training, 1, 000 de-
velopment, and 1, 805 test examples.

6.2 Settings
We followed the data preprocessing used in previ-
ous work (Dong and Lapata, 2016; Yin and Neu-
big, 2017). Input sentences were tokenized us-
ing NLTK (Bird et al., 2009) and lowercased.
We filtered words that appeared less than four
times in the training set. Numbers and URLs in
IFTTT and quoted strings in DJANGO were re-
placed with place holders. Hyperparameters of the
semantic parsers were validated on the develop-
ment set. The learning rate and the smoothing con-
stant of RMSProp (Tieleman and Hinton, 2012)
were 0.002 and 0.95, respectively. The dropout
rate was 0.25. A two-layer LSTM was used for
IFTTT, while a one-layer LSTM was employed
for DJANGO. Dimensions for the word embedding
and hidden vector were selected from {150, 250}.
The beam size during decoding was 5.

For IFTTT, we view the predicted trees as a set
of productions, and use balanced F1 as evaluation
metric (Quirk et al., 2015). We do not measure ac-
curacy because the dataset is very noisy and there
rarely is an exact match between the predicted out-
put and the gold standard. The F1 score of our
neural semantic parser is 50.1%, which is compa-
rable to Dong and Lapata (2016). For DJANGO,
we measure the fraction of exact matches, where
F1 score is equal to accuracy. Because there are
unseen variable names at test time, we use atten-
tion scores as alignments to replace unknown to-



749

Method IFTTT DJANGO

POSTERIOR 0.477 0.694

CONF 0.625 0.793
− MODEL 0.595 0.759
− DATA 0.610 0.787
− INPUT 0.608 0.785

Table 2: Spearman ρ correlation between confi-
dence scores and F1. Best results are shown in
bold. All correlations are significant at p < 0.01.

kens in the prediction with the input words they
align to (Luong et al., 2015b). The accuracy of
our parser is 53.7%, which is better than the re-
sult (45.1%) of the sequence-to-sequence model
reported in Yin and Neubig (2017).

To estimate model uncertainty, we set dropout
rate to 0.1, and performed 30 inference passes.
The standard deviation of Gaussian noise was
0.05. The language model was estimated using
KenLM (Heafield et al., 2013). For input un-
certainty, we computed variance for the 10-best
candidates. The confidence metrics were imple-
mented in batch mode, to take full advantage of
GPUs. Hyperparameters of the confidence scor-
ing model were cross-validated. The number of
boosted trees was selected from {20, 50}. The
maximum tree depth was selected from {3, 4, 5}.
We set the subsample ratio to 0.8. All other hyper-
parameters in XGBoost (Chen and Guestrin, 2016)
were left with their default values.

6.3 Results

Confidence Estimation We compare our ap-
proach (CONF) against confidence scores based
on posterior probability p(a|q) (POSTERIOR). We
also report the results of three ablation variants
(−MODEL, −DATA, −INPUT) by removing each
group of confidence metrics described in Sec-
tion 4. We measure the relationship between con-
fidence scores and F1 using Spearman’s ρ corre-
lation coefficient which varies between −1 and 1
(0 implies there is no correlation). High ρ indi-
cates that the confidence scores are high for cor-
rect predictions and low otherwise.

As shown in Table 2, our method CONF outper-
forms POSTERIOR by a large margin. The ablation
results indicate that model uncertainty plays the
most important role among the confidence met-
rics. In contrast, removing the metrics of data un-
certainty affects performance less, because most
examples in the datasets are in-domain. Improve-

F1 Dout Noise PR PPL LM #UNK Var

Dout 0.59
Noise 0.59 0.90
PR 0.52 0.84 0.82
PPL 0.48 0.78 0.78 0.89
LM 0.30 0.26 0.32 0.27 0.25
#UNK 0.27 0.31 0.33 0.29 0.25 0.32
Var 0.49 0.83 0.78 0.88 0.79 0.25 0.27
Ent 0.53 0.78 0.78 0.80 0.75 0.27 0.30 0.76

Table 3: Correlation matrix for F1 and individual
confidence metrics on the IFTTT dataset. All cor-
relations are significant at p < 0.01. Best predic-
tors are shown in bold. Dout is short for dropout,
PR for posterior probability, PPL for perplexity,
LM for probability based on a language model,
#UNK for number of unknown tokens, Var for
variance of top candidates, and Ent for Entropy.

F1 Dout Noise PR PPL LM #UNK Var

Dout 0.76
Noise 0.78 0.94
PR 0.73 0.89 0.90
PPL 0.64 0.80 0.81 0.84
LM 0.32 0.41 0.40 0.38 0.30
#UNK 0.27 0.28 0.28 0.26 0.19 0.35
Var 0.70 0.87 0.87 0.89 0.87 0.37 0.23
Ent 0.72 0.89 0.90 0.92 0.86 0.38 0.26 0.90

Table 4: Correlation matrix for F1 and individual
confidence metrics on the DJANGO dataset. All
correlations are significant at p < 0.01. Best pre-
dictors are shown in bold. Same shorthands apply
as in Table 3.

ments for each group of metrics are significant
with p < 0.05 according to bootstrap hypothesis
testing (Efron and Tibshirani, 1994).

Tables 3 and 4 show the correlation matrix for
F1 and individual confidence metrics on the IFTTT
and DJANGO datasets, respectively. As can be
seen, metrics representing model uncertainty and
input uncertainty are more correlated to each other
compared with metrics capturing data uncertainty.
Perhaps unsurprisingly metrics of the same group
are highly inter-correlated since they model the
same type of uncertainty. Table 5 shows the rel-
ative importance of individual metrics in the re-
gression model. As importance score we use the
average gain (i.e., loss reduction) brought by the
confidence metric once added as feature to the
branch of the decision tree (Chen and Guestrin,
2016). The results indicate that model uncer-
tainty (Noise/Dropout/Posterior/Perplexity) plays



750

Metric Dout Noise PR PPL LM #UNK Var Ent

IFTTT 0.39 1.00 0.89 0.27 0.26 0.46 0.43 0.34
DJANGO 1.00 0.59 0.22 0.58 0.49 0.14 0.24 0.25

Table 5: Importance scores of confidence metrics
(normalized by maximum value on each dataset).
Best results are shown in bold. Same shorthands
apply as in Table 3.

the most important role. On IFTTT, the number of
unknown tokens (#UNK) and the variance of top
candidates (var(K-best)) are also very helpful be-
cause this dataset is relatively noisy and contains
many ambiguous inputs.

Finally, in real-world applications, confidence
scores are often used as a threshold to trade-off
precision for coverage. Figure 3 shows how F1
score varies as we increase the confidence thresh-
old, i.e., reduce the proportion of examples that
we return answers for. F1 score improves mono-
tonically for POSTERIOR and our method, which,
however, achieves better performance when cov-
erage is the same.

Uncertainty Interpretation We next evaluate
how our backpropagation method (see Section 5)
allows us to identify input tokens contributing to
uncertainty. We compare against a method that in-
terprets uncertainty based on the attention mech-
anism (ATTENTION). As shown in Equation (2),
attention scores rt,k can be used as soft alignments
between the time step t of the decoder and the
k-th input token. We compute the normalized un-
certainty score ûqt for a token qt via:

ûqt ∝
|a|∑

t=1

rt,kuat (8)

where uat is the uncertainty score of the predicted
token at (Equation (7)), and

∑|q|
t=1 ûqt = 1.

Unfortunately, the evaluation of uncertainty in-
terpretation methods is problematic. For our se-
mantic parsing task, we do not a priori know which
tokens in the natural language input contribute to
uncertainty and these may vary depending on the
architecture used, model parameters, and so on.
We work around this problem by creating a proxy
gold standard. We inject noise to the vectors rep-
resenting tokens in the encoder (see Section 4.1)
and then estimate the uncertainty caused by each
token qt (Equation (6)) under the assumption that

100% 90% 80% 70% 60% 50% 40% 30%
Proportion of Examples

0.5

0.6

0.7

F
1
S
co
re

Posterior

Conf

(a) IFTTT

100% 90% 80% 70% 60% 50% 40% 30%
Proportion of Examples

0.5

0.6

0.7

0.8

0.9

1.0

A
cc
ur
ac
y

Posterior

Conf

(b) DJANGO

Figure 3: Confidence scores are used as thresh-
old to filter out uncertain test examples. As the
threshold increases, performance improves. The
horizontal axis shows the proportion of examples
beyond the threshold.

addition of noise should only affect genuinely un-
certain tokens. Notice that here we inject noise
to one token at a time1 instead of all parameters
(see Figure 1). Tokens identified as uncertain by
the above procedure are considered gold standard
and compared to those identified by our method.
We use Gaussian noise to perturb vectors in our
experiments (dropout obtained similar results).

We define an evaluation metric based on the
overlap (overlap@K) among tokens identified as
uncertain by the model and the gold standard.
Given an example, we first compute the interpre-
tation scores of the input tokens according to our
method, and obtain a list τ1 of K tokens with high-
est scores. We also obtain a list τ2 of K tokens
with highest ground-truth scores and measure the
degree of overlap between these two lists:

overlap@K =
|τ1 ∩ τ2|

K

1Noise injection as described above is used for evaluation
purposes only since we need to perform forward passes mul-
tiple times (see Section 4.1) for each token, and the running
time increases linearly with the input length.



751

Method IFTTT DJANGO

@2 @4 @2 @4

ATTENTION 0.525 0.737 0.637 0.684
BACKPROP 0.608 0.791 0.770 0.788

Table 6: Uncertainty interpretation against in-
ferred ground truth; we compute the overlap be-
tween tokens identified as contributing to uncer-
tainty by our method and those found in the gold
standard. Overlap is shown for top 2 and 4 tokens.
Best results are in bold.

google calendar−any event starts THEN facebook
−create a status message−(status message
({description}))

ATT post calendar event to facebook
BP post calendar event to facebook
feed−new feed item−(feed url(

url sports.espn.go.com)) THEN ...
ATT espn mlb headline to readability
BP espn mlb headline to readability
weather−tomorrow’s low drops below−((

temperature(0)) (degrees in(c))) THEN ...
ATT warn me when it’s going to be freezing tomorrow
BP warn me when it’s going to be freezing tomorrow
if str number[0] == ’ STR ’:

ATT if first element of str number equals a string STR .
BP if first element of str number equals a string STR .
start = 0

ATT start is an integer 0 .
BP start is an integer 0 .
if name.startswith(’ STR ’):

ATT if name starts with an string STR ,
BP if name starts with an string STR ,

Table 7: Uncertainty interpretation for ATTEN-
TION (ATT) and BACKPROP (BP) . The first line in
each group is the model prediction. Predicted to-
kens and input words with large scores are shown
in red and blue, respectively.

where K ∈ {2, 4} in our experiments. For ex-
ample, the overlap@4 metric of the lists τ1 =
[q7, q8, q2, q3] and τ2 = [q7, q8, q3, q4] is 3/4, be-
cause there are three overlapping tokens.

Table 6 reports results with overlap@2 and
overlap@4. Overall, BACKPROP achieves bet-
ter interpretation quality than the attention mech-
anism. On both datasets, about 80% of the
top-4 tokens identified as uncertain agree with the
ground truth. Table 7 shows examples where our
method has identified input tokens contributing to
the uncertainty of the output. We highlight to-
ken at if its uncertainty score uat is greater than
0.5 ∗ avg{uat′}

|a|
t′=1. The results illustrate that the

parser tends to be uncertain about tokens which are

function arguments (e.g., URLs, and message con-
tent), and ambiguous inputs. The examples show
that BACKPROP is qualitatively better compared to
ATTENTION; attention scores often produce inac-
curate alignments while BACKPROP can utilize in-
formation flowing through the LSTMs rather than
only relying on the attention mechanism.

7 Conclusions

In this paper we presented a confidence estimation
model and an uncertainty interpretation method
for neural semantic parsing. Experimental results
show that our method achieves better performance
than competitive baselines on two datasets. Direc-
tions for future work are many and varied. The
proposed framework could be applied to a variety
of tasks (Bahdanau et al., 2015; Schmaltz et al.,
2017) employing sequence-to-sequence architec-
tures. We could also utilize the confidence esti-
mation model within an active learning framework
for neural semantic parsing.

Acknowledgments

We would like to thank Pengcheng Yin for sharing
with us the preprocessed version of the DJANGO
dataset. We gratefully acknowledge the financial
support of the European Research Council (award
number 681760; Dong, Lapata) and the Adept-
Mind Scholar Fellowship program (Dong).

References
Jacob Andreas, Andreas Vlachos, and Stephen Clark.

2013. Semantic parsing as machine translation. In
Proceedings of the 51st Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 47–52,
Sofia, Bulgaria.

Sebastian Bach, Alexander Binder, Grgoire Montavon,
Frederick Klauschen, Klaus-Robert Mller, and Wo-
jciech Samek. 2015. On pixel-wise explanations
for non-linear classifier decisions by layer-wise rel-
evance propagation. PLOS ONE, 10(7):1–46.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In Proceedings of
the 3rd International Conference on Learning Rep-
resentations, San Diego, California.

Steven Bird, Ewan Klein, and Edward Loper.
2009. Natural Language Processing with Python.
O’Reilly Media.

John Blatz, Erin Fitzgerald, George Foster, Simona
Gandrabur, Cyril Goutte, Alex Kulesza, Alberto



752

Sanchis, and Nicola Ueffing. 2004. Confidence es-
timation for machine translation. In Proceedings of
the 20th International Conference on Computational
Linguistics, pages 315–321, Geneva, Switzerland.

Charles Blundell, Julien Cornebise, Koray
Kavukcuoglu, and Daan Wierstra. 2015. Weight
uncertainty in neural networks. In Proceedings
of the 32nd International Conference on Interna-
tional Conference on Machine Learning, pages
1613–1622, Lille, France.

Tianqi Chen and Carlos Guestrin. 2016. XGBoost: A
scalable tree boosting system. In Proceedings of
the 22nd ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining, pages
785–794, San Francisco, California.

John S Denker and Yann Lecun. 1991. Transforming
neural-net output levels to probability distributions.
In Advances in neural information processing sys-
tems, pages 853–859, Denver, Colorado.

Li Dong and Mirella Lapata. 2016. Language to logi-
cal form with neural attention. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics, pages 33–43, Berlin, Germany.

Li Dong and Mirella Lapata. 2018. Coarse-to-fine de-
coding for neural semantic parsing. In Proceedings
of the 56th Annual Meeting of the Association for
Computational Linguistics, Melbourne, Australia.

Bradley Efron and Robert J Tibshirani. 1994. An Intro-
duction to the Bootstrap. CRC press.

Xing Fan, Emilio Monti, Lambert Mathias, and Markus
Dreyer. 2017. Transfer learning for neural seman-
tic parsing. In Proceedings of the 2nd Workshop
on Representation Learning for NLP, pages 48–56,
Vancouver, Canada.

Yarin Gal and Zoubin Ghahramani. 2016. Dropout as
a bayesian approximation: Representing model un-
certainty in deep learning. In Proceedings of the
33rd International Conference on Machine Learn-
ing, pages 1050–1059, New York City, NY.

Zhe Gan, Chunyuan Li, Changyou Chen, Yunchen Pu,
Qinliang Su, and Lawrence Carin. 2017. Scalable
bayesian learning of recurrent neural networks for
language modeling. In Proceedings of the 55th An-
nual Meeting of the Association for Computational
Linguistics, pages 321–331, Vancouver, Canada.

D. C. Gondek, A. Lally, A. Kalyanpur, J. W. Mur-
dock, P. A. Duboue, L. Zhang, Y. Pan, Z. M. Qiu,
and C. Welty. 2012. A framework for merging and
ranking of answers in DeepQA. IBM Journal of Re-
search and Development, 56(3.4):14:1–14:12.

Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013. Scalable modi-
fied Kneser-Ney language model estimation. In Pro-
ceedings of the 51st Annual Meeting of the Associa-
tion for Computational Linguistics, pages 690–696,
Sofia, Bulgaria.

Jonathan Herzig and Jonathan Berant. 2017. Neural
semantic parsing over multiple knowledge-bases. In
Proceedings of the 55th Annual Meeting of the As-
sociation for Computational Linguistics, pages 623–
628, Vancouver, Canada.

Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long
short-term memory. Neural Computation, 9:1735–
1780.

Srinivasan Iyer, Ioannis Konstas, Alvin Cheung,
Jayant Krishnamurthy, and Luke Zettlemoyer. 2017.
Learning a neural semantic parser from user feed-
back. In Proceedings of the 55th Annual Meeting
of the Association for Computational Linguistics,
pages 963–973, Vancouver, Canada.

Robin Jia and Percy Liang. 2016. Data recombination
for neural semantic parsing. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics, pages 12–22, Berlin, Germany.

Alexander Johansen and Richard Socher. 2017. Learn-
ing when to skim and when to read. In Proceedings
of the 2nd Workshop on Representation Learning for
NLP, pages 257–264, Vancouver, Canada.

Tomáš Kočiský, Gábor Melis, Edward Grefenstette,
Chris Dyer, Wang Ling, Phil Blunsom, and
Karl Moritz Hermann. 2016. Semantic parsing with
semi-supervised sequential autoencoders. In Pro-
ceedings of the 2016 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1078–
1087, Austin, Texas.

Jayant Krishnamurthy, Pradeep Dasigi, and Matt Gard-
ner. 2017. Neural semantic parsing with type con-
straints for semi-structured tables. In Proceedings of
the 2017 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1517–1527, Copen-
hagen, Denmark.

Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-
ter, and Mark Steedman. 2011. Lexical generaliza-
tion in CCG grammar induction for semantic pars-
ing. In Proceedings of the 2011 Conference on Em-
pirical Methods in Natural Language Processing,
pages 1512–1523, Edinburgh, Scotland.

Wang Ling, Phil Blunsom, Edward Grefenstette,
Karl Moritz Hermann, Tomáš Kočiský, Fumin
Wang, and Andrew Senior. 2016. Latent predictor
networks for code generation. In Proceedings of the
54th Annual Meeting of the Association for Com-
putational Linguistics, pages 599–609, Berlin, Ger-
many.

Wei Lu, Hwee Tou Ng, Wee Sun Lee, and Luke Zettle-
moyer. 2008. A generative model for parsing natural
language to meaning representations. In Proceed-
ings of the 2008 Conference on Empirical Methods
in Natural Language Processing, pages 783–792,
Honolulu, Hawaii.



753

Thang Luong, Hieu Pham, and Christopher D. Man-
ning. 2015a. Effective approaches to attention-
based neural machine translation. In Proceedings of
the 2015 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1412–1421, Lisbon,
Portugal.

Thang Luong, Ilya Sutskever, Quoc Le, Oriol Vinyals,
and Wojciech Zaremba. 2015b. Addressing the rare
word problem in neural machine translation. In Pro-
ceedings of the 53rd Annual Meeting of the Associ-
ation for Computational Linguistics and the 7th In-
ternational Joint Conference on Natural Language
Processing, pages 11–19, Beijing, China.

David J. C. MacKay. 1992. A practical bayesian frame-
work for backpropagation networks. Neural Com-
putation, 4(3):448–472.

Radford M Neal. 1996. Bayesian learning for neural
networks, volume 118. Springer Science & Business
Media.

Yusuke Oda, Hiroyuki Fudaba, Graham Neubig,
Hideaki Hata, Sakriani Sakti, Tomoki Toda, and
Satoshi Nakamura. 2015. Learning to generate
pseudo-code from source code using statistical ma-
chine translation. In Proceedings of the 2015 30th
IEEE/ACM International Conference on Automated
Software Engineering, pages 574–584, Washington,
DC.

Chris Quirk, Raymond Mooney, and Michel Galley.
2015. Language to code: Learning semantic parsers
for if-this-then-that recipes. In Proceedings of the
53rd Annual Meeting of the Association for Compu-
tational Linguistics and the 7th International Joint
Conference on Natural Language Processing, pages
878–888, Beijing, China.

Maxim Rabinovich, Mitchell Stern, and Dan Klein.
2017. Abstract syntax networks for code genera-
tion and semantic parsing. In Proceedings of the
55th Annual Meeting of the Association for Compu-
tational Linguistics, pages 1139–1149, Vancouver,
Canada.

Allen Schmaltz, Yoon Kim, Alexander Rush, and Stu-
art Shieber. 2017. Adapting sequence models for
sentence correction. In Proceedings of the 2017
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 2797–2803, Copenhagen,
Denmark.

Radu Soricut and Abdessamad Echihabi. 2010.
Trustrank: Inducing trust in automatic translations
via ranking. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, pages 612–621, Uppsala, Sweden.

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: A simple way to prevent neural networks
from overfitting. Journal of Machine Learning Re-
search, 15:1929–1958.

Raymond Hendy Susanto and Wei Lu. 2017. Neural
architectures for multilingual semantic parsing. In
Proceedings of the 55th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 38–44,
Vancouver, Canada.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in Neural Information Process-
ing Systems, pages 3104–3112, Montreal, Canada.

Lappoon R. Tang and Raymond J. Mooney. 2000. Au-
tomated construction of database interfaces: Inter-
grating statistical and relational learning for seman-
tic parsing. In 2000 Joint SIGDAT Conference on
Empirical Methods in Natural Language Process-
ing and Very Large Corpora, pages 133–141, Hong
Kong, China.

T. Tieleman and G. Hinton. 2012. Lecture 6.5—
RMSProp: Divide the gradient by a running average
of its recent magnitude. Technical report.

Nicola Ueffing and Hermann Ney. 2005. Word-level
confidence estimation for machine translation us-
ing phrase-based translation models. In Proceedings
of the Conference on Human Language Technology
and Empirical Methods in Natural Language Pro-
cessing, pages 763–770, Vancouver, Canada.

Chunyang Xiao, Marc Dymetman, and Claire Gardent.
2016. Sequence-based structured prediction for se-
mantic parsing. In Proceedings of the 54th Annual
Meeting of the Association for Computational Lin-
guistics, pages 1341–1350, Berlin, Germany.

Pengcheng Yin and Graham Neubig. 2017. A syntactic
neural model for general-purpose code generation.
In Proceedings of the 55th Annual Meeting of the As-
sociation for Computational Linguistics, pages 440–
450, Vancouver, Canada.

Luke Zettlemoyer and Michael Collins. 2007. Online
learning of relaxed CCG grammars for parsing to
logical form. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 678–687, Prague, Czech Republic.

Jianming Zhang, Zhe Lin, Jonathan Brandt, Xiaohui
Shen, and Stan Sclaroff. 2016. Top-down neural at-
tention by excitation backprop. In European Con-
ference on Computer Vision, pages 543–559, Ams-
terdam, Netherlands.

Kai Zhao and Liang Huang. 2015. Type-driven in-
cremental semantic parsing with polymorphism. In
Proceedings of the 2015 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 1416–1421, Denver, Colorado.
















<<
  /ASCII85EncodePages false
  /AllowTransparency false
  /AutoPositionEPSFiles false
  /AutoRotatePages /None
  /Binding /Left
  /CalGrayProfile (Gray Gamma 2.2)
  /CalRGBProfile (sRGB IEC61966-2.1)
  /CalCMYKProfile (U.S. Web Coated \050SWOP\051 v2)
  /sRGBProfile (sRGB IEC61966-2.1)
  /CannotEmbedFontPolicy /Warning
  /CompatibilityLevel 1.7
  /CompressObjects /Off
  /CompressPages true
  /ConvertImagesToIndexed true
  /PassThroughJPEGImages true
  /CreateJobTicket false
  /DefaultRenderingIntent /Default
  /DetectBlends true
  /DetectCurves 0.0000
  /ColorConversionStrategy /LeaveColorUnchanged
  /DoThumbnails false
  /EmbedAllFonts true
  /EmbedOpenType false
  /ParseICCProfilesInComments true
  /EmbedJobOptions true
  /DSCReportingLevel 0
  /EmitDSCWarnings false
  /EndPage -1
  /ImageMemory 1048576
  /LockDistillerParams true
  /MaxSubsetPct 100
  /Optimize true
  /OPM 0
  /ParseDSCComments false
  /ParseDSCCommentsForDocInfo false
  /PreserveCopyPage true
  /PreserveDICMYKValues true
  /PreserveEPSInfo false
  /PreserveFlatness true
  /PreserveHalftoneInfo true
  /PreserveOPIComments false
  /PreserveOverprintSettings true
  /StartPage 1
  /SubsetFonts true
  /TransferFunctionInfo /Remove
  /UCRandBGInfo /Preserve
  /UsePrologue false
  /ColorSettingsFile ()
  /AlwaysEmbed [ true
  ]
  /NeverEmbed [ true
  ]
  /AntiAliasColorImages false
  /CropColorImages true
  /ColorImageMinResolution 200
  /ColorImageMinResolutionPolicy /OK
  /DownsampleColorImages true
  /ColorImageDownsampleType /Bicubic
  /ColorImageResolution 300
  /ColorImageDepth -1
  /ColorImageMinDownsampleDepth 1
  /ColorImageDownsampleThreshold 1.50000
  /EncodeColorImages true
  /ColorImageFilter /DCTEncode
  /AutoFilterColorImages false
  /ColorImageAutoFilterStrategy /JPEG
  /ColorACSImageDict <<
    /QFactor 0.76
    /HSamples [2 1 1 2] /VSamples [2 1 1 2]
  >>
  /ColorImageDict <<
    /QFactor 0.76
    /HSamples [2 1 1 2] /VSamples [2 1 1 2]
  >>
  /JPEG2000ColorACSImageDict <<
    /TileWidth 256
    /TileHeight 256
    /Quality 15
  >>
  /JPEG2000ColorImageDict <<
    /TileWidth 256
    /TileHeight 256
    /Quality 15
  >>
  /AntiAliasGrayImages false
  /CropGrayImages true
  /GrayImageMinResolution 200
  /GrayImageMinResolutionPolicy /OK
  /DownsampleGrayImages true
  /GrayImageDownsampleType /Bicubic
  /GrayImageResolution 300
  /GrayImageDepth -1
  /GrayImageMinDownsampleDepth 2
  /GrayImageDownsampleThreshold 1.50000
  /EncodeGrayImages true
  /GrayImageFilter /DCTEncode
  /AutoFilterGrayImages false
  /GrayImageAutoFilterStrategy /JPEG
  /GrayACSImageDict <<
    /QFactor 0.76
    /HSamples [2 1 1 2] /VSamples [2 1 1 2]
  >>
  /GrayImageDict <<
    /QFactor 0.76
    /HSamples [2 1 1 2] /VSamples [2 1 1 2]
  >>
  /JPEG2000GrayACSImageDict <<
    /TileWidth 256
    /TileHeight 256
    /Quality 15
  >>
  /JPEG2000GrayImageDict <<
    /TileWidth 256
    /TileHeight 256
    /Quality 15
  >>
  /AntiAliasMonoImages false
  /CropMonoImages true
  /MonoImageMinResolution 400
  /MonoImageMinResolutionPolicy /OK
  /DownsampleMonoImages true
  /MonoImageDownsampleType /Bicubic
  /MonoImageResolution 600
  /MonoImageDepth -1
  /MonoImageDownsampleThreshold 1.50000
  /EncodeMonoImages true
  /MonoImageFilter /CCITTFaxEncode
  /MonoImageDict <<
    /K -1
  >>
  /AllowPSXObjects false
  /CheckCompliance [
    /None
  ]
  /PDFX1aCheck false
  /PDFX3Check false
  /PDFXCompliantPDFOnly false
  /PDFXNoTrimBoxError true
  /PDFXTrimBoxToMediaBoxOffset [
    0.00000
    0.00000
    0.00000
    0.00000
  ]
  /PDFXSetBleedBoxToMediaBox true
  /PDFXBleedBoxToTrimBoxOffset [
    0.00000
    0.00000
    0.00000
    0.00000
  ]
  /PDFXOutputIntentProfile (None)
  /PDFXOutputConditionIdentifier ()
  /PDFXOutputCondition ()
  /PDFXRegistryName ()
  /PDFXTrapped /False

  /CreateJDFFile false
  /Description <<
    /CHT <FEFF4f7f752890194e9b8a2d7f6e5efa7acb7684002000410064006f006200650020005000440046002065874ef69069752865bc666e901a554652d965874ef6768467e5770b548c52175370300260a853ef4ee54f7f75280020004100630072006f0062006100740020548c002000410064006f00620065002000520065006100640065007200200035002e003000204ee553ca66f49ad87248672c4f86958b555f5df25efa7acb76840020005000440046002065874ef63002>
    /DAN <FEFF004200720075006700200069006e0064007300740069006c006c0069006e006700650072006e0065002000740069006c0020006100740020006f007000720065007400740065002000410064006f006200650020005000440046002d0064006f006b0075006d0065006e007400650072002c0020006400650072002000650067006e006500720020007300690067002000740069006c00200064006500740061006c006a006500720065007400200073006b00e60072006d007600690073006e0069006e00670020006f00670020007500640073006b007200690076006e0069006e006700200061006600200066006f0072007200650074006e0069006e006700730064006f006b0075006d0065006e007400650072002e0020004400650020006f007000720065007400740065006400650020005000440046002d0064006f006b0075006d0065006e0074006500720020006b0061006e002000e50062006e00650073002000690020004100630072006f00620061007400200065006c006c006500720020004100630072006f006200610074002000520065006100640065007200200035002e00300020006f00670020006e0079006500720065002e>
    /DEU <FEFF00560065007200770065006e00640065006e0020005300690065002000640069006500730065002000450069006e007300740065006c006c0075006e00670065006e0020007a0075006d002000450072007300740065006c006c0065006e00200076006f006e002000410064006f006200650020005000440046002d0044006f006b0075006d0065006e00740065006e002c00200075006d002000650069006e00650020007a0075007600650072006c00e40073007300690067006500200041006e007a006500690067006500200075006e00640020004100750073006700610062006500200076006f006e00200047006500730063006800e40066007400730064006f006b0075006d0065006e00740065006e0020007a0075002000650072007a00690065006c0065006e002e00200044006900650020005000440046002d0044006f006b0075006d0065006e007400650020006b00f6006e006e0065006e0020006d006900740020004100630072006f00620061007400200075006e0064002000520065006100640065007200200035002e003000200075006e00640020006800f600680065007200200067006500f600660066006e00650074002000770065007200640065006e002e>
    /ESP <FEFF005500740069006c0069006300650020006500730074006100200063006f006e0066006900670075007200610063006900f3006e0020007000610072006100200063007200650061007200200064006f00630075006d0065006e0074006f0073002000640065002000410064006f00620065002000500044004600200061006400650063007500610064006f007300200070006100720061002000760069007300750061006c0069007a00610063006900f3006e0020006500200069006d0070007200650073006900f3006e00200064006500200063006f006e006600690061006e007a006100200064006500200064006f00630075006d0065006e0074006f007300200063006f006d00650072006300690061006c00650073002e002000530065002000700075006500640065006e00200061006200720069007200200064006f00630075006d0065006e0074006f00730020005000440046002000630072006500610064006f007300200063006f006e0020004100630072006f006200610074002c002000410064006f00620065002000520065006100640065007200200035002e003000200079002000760065007200730069006f006e0065007300200070006f00730074006500720069006f007200650073002e>
    /FRA <FEFF005500740069006c006900730065007a00200063006500730020006f007000740069006f006e00730020006100660069006e00200064006500200063007200e900650072002000640065007300200064006f00630075006d0065006e00740073002000410064006f006200650020005000440046002000700072006f00660065007300730069006f006e006e0065006c007300200066006900610062006c0065007300200070006f007500720020006c0061002000760069007300750061006c00690073006100740069006f006e0020006500740020006c00270069006d007000720065007300730069006f006e002e0020004c0065007300200064006f00630075006d0065006e00740073002000500044004600200063007200e900e90073002000700065007500760065006e0074002000ea0074007200650020006f007500760065007200740073002000640061006e00730020004100630072006f006200610074002c002000610069006e00730069002000710075002700410064006f00620065002000520065006100640065007200200035002e0030002000650074002000760065007200730069006f006e007300200075006c007400e90072006900650075007200650073002e>
    /ITA (Utilizzare queste impostazioni per creare documenti Adobe PDF adatti per visualizzare e stampare documenti aziendali in modo affidabile. I documenti PDF creati possono essere aperti con Acrobat e Adobe Reader 5.0 e versioni successive.)
    /JPN <FEFF30d330b830cd30b9658766f8306e8868793a304a3088307353705237306b90693057305f002000410064006f0062006500200050004400460020658766f8306e4f5c6210306b4f7f75283057307e305930023053306e8a2d5b9a30674f5c62103055308c305f0020005000440046002030d530a130a430eb306f3001004100630072006f0062006100740020304a30883073002000410064006f00620065002000520065006100640065007200200035002e003000204ee5964d3067958b304f30533068304c3067304d307e305930023053306e8a2d5b9a3067306f30d530a930f330c8306e57cb30818fbc307f3092884c3044307e30593002>
    /KOR <FEFFc7740020c124c815c7440020c0acc6a9d558c5ec0020be44c988b2c8c2a40020bb38c11cb97c0020c548c815c801c73cb85c0020bcf4ace00020c778c1c4d558b2940020b3700020ac00c7a50020c801d569d55c002000410064006f0062006500200050004400460020bb38c11cb97c0020c791c131d569b2c8b2e4002e0020c774b807ac8c0020c791c131b41c00200050004400460020bb38c11cb2940020004100630072006f0062006100740020bc0f002000410064006f00620065002000520065006100640065007200200035002e00300020c774c0c1c5d0c11c0020c5f40020c2180020c788c2b5b2c8b2e4002e>
    /NLD (Gebruik deze instellingen om Adobe PDF-documenten te maken waarmee zakelijke documenten betrouwbaar kunnen worden weergegeven en afgedrukt. De gemaakte PDF-documenten kunnen worden geopend met Acrobat en Adobe Reader 5.0 en hoger.)
    /NOR <FEFF004200720075006b00200064006900730073006500200069006e006e007300740069006c006c0069006e00670065006e0065002000740069006c002000e50020006f0070007000720065007400740065002000410064006f006200650020005000440046002d0064006f006b0075006d0065006e00740065007200200073006f006d002000650072002000650067006e0065007400200066006f00720020007000e5006c006900740065006c006900670020007600690073006e0069006e00670020006f00670020007500740073006b007200690066007400200061007600200066006f0072007200650074006e0069006e006700730064006f006b0075006d0065006e007400650072002e0020005000440046002d0064006f006b0075006d0065006e00740065006e00650020006b0061006e002000e50070006e00650073002000690020004100630072006f00620061007400200065006c006c00650072002000410064006f00620065002000520065006100640065007200200035002e003000200065006c006c00650072002e>
    /PTB <FEFF005500740069006c0069007a006500200065007300730061007300200063006f006e00660069006700750072006100e700f50065007300200064006500200066006f0072006d00610020006100200063007200690061007200200064006f00630075006d0065006e0074006f0073002000410064006f00620065002000500044004600200061006400650071007500610064006f00730020007000610072006100200061002000760069007300750061006c0069007a006100e700e3006f002000650020006100200069006d0070007200650073007300e3006f00200063006f006e0066006900e1007600650069007300200064006500200064006f00630075006d0065006e0074006f007300200063006f006d0065007200630069006100690073002e0020004f007300200064006f00630075006d0065006e0074006f00730020005000440046002000630072006900610064006f007300200070006f00640065006d0020007300650072002000610062006500720074006f007300200063006f006d0020006f0020004100630072006f006200610074002000650020006f002000410064006f00620065002000520065006100640065007200200035002e0030002000650020007600650072007300f50065007300200070006f00730074006500720069006f007200650073002e>
    /SUO <FEFF004b00e40079007400e40020006e00e40069007400e4002000610073006500740075006b007300690061002c0020006b0075006e0020006c0075006f0074002000410064006f0062006500200050004400460020002d0064006f006b0075006d0065006e007400740065006a0061002c0020006a006f0074006b006100200073006f0070006900760061007400200079007200690074007900730061007300690061006b00690072006a006f006a0065006e0020006c0075006f00740065007400740061007600610061006e0020006e00e400790074007400e4006d0069007300650065006e0020006a0061002000740075006c006f007300740061006d0069007300650065006e002e0020004c0075006f0064007500740020005000440046002d0064006f006b0075006d0065006e00740069007400200076006f0069006400610061006e0020006100760061007400610020004100630072006f0062006100740069006c006c00610020006a0061002000410064006f00620065002000520065006100640065007200200035002e0030003a006c006c00610020006a006100200075007500640065006d006d0069006c006c0061002e>
    /SVE <FEFF0041006e007600e4006e00640020006400650020006800e4007200200069006e0073007400e4006c006c006e0069006e006700610072006e00610020006f006d002000640075002000760069006c006c00200073006b006100700061002000410064006f006200650020005000440046002d0064006f006b0075006d0065006e007400200073006f006d00200070006100730073006100720020006600f60072002000740069006c006c006600f60072006c00690074006c006900670020007600690073006e0069006e00670020006f006300680020007500740073006b007200690066007400650072002000610076002000610066006600e4007200730064006f006b0075006d0065006e0074002e002000200053006b006100700061006400650020005000440046002d0064006f006b0075006d0065006e00740020006b0061006e002000f600700070006e00610073002000690020004100630072006f0062006100740020006f00630068002000410064006f00620065002000520065006100640065007200200035002e00300020006f00630068002000730065006e006100720065002e>
    /ENU (Use these settings to create PDFs that match the "Required"  settings for PDF Specification 4.01)
    /CHS <FEFF4f7f75288fd94e9b8bbe5b9a521b5efa7684002000410064006f006200650020005000440046002065876863900275284e8e55464e1a65876863768467e5770b548c62535370300260a853ef4ee54f7f75280020004100630072006f0062006100740020548c002000410064006f00620065002000520065006100640065007200200035002e003000204ee553ca66f49ad87248672c676562535f00521b5efa768400200050004400460020658768633002>
  >>
>> setdistillerparams
<<
  /HWResolution [600 600]
  /PageSize [612.000 792.000]
>> setpagedevice

