



















































Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations


Proceedings of the 10th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 46–56
Minneapolis, June 6, 2019. c©2019 Association for Computational Linguistics

46

Using Structured Representation and Data: A Hybrid Model for Negation
and Sentiment in Customer Service Conversations

Amita Misra, Mansurul Bhuiyan, Jalal Mahmud, and Saurabh Tripathy
IBM-Research, Almaden

San Jose, CA, USA
amita.misra1|mansurul.bhuiyan|jumahmud|Saurabh.Tripathy2@ibm.com

Abstract

Twitter customer service interactions have re-
cently emerged as an effective platform to re-
spond and engage with customers. In this
work, we explore the role of negation in cus-
tomer service interactions, particularly applied
to sentiment analysis. We define rules to iden-
tify true negation cues and scope more suited
to conversational data than existing general re-
view data. Using semantic knowledge and
syntactic structure from constituency parse
trees, we propose an algorithm for scope de-
tection that performs comparable to state of the
art BiLSTM. We further investigate the results
of negation scope detection for the sentiment
prediction task on customer service conversa-
tion data using both a traditional SVM and a
Neural Network. We propose an antonym dic-
tionary based method for negation applied to a
CNN-LSTM combination model for sentiment
analysis. Experimental results show that the
antonym-based method outperforms the pre-
vious lexicon-based and neural network meth-
ods.

1 Introduction

Negation has been described as a polarity influ-
encer (Wilson et al., 2009) and therefore it has to
be taken into consideration while designing a sen-
timent prediction system, but how important it is
in twitter customer service conversations? For ex-
ample, both the customer service tweets in Table 1
have an explicit negation cue but the effect of cue
words on the polarity differ. The first tweet has
a negation cue [don’t] that changes the positive
polarity of the words in the scope [think you do
understand]. Additionally, tweet 1 has a hashtag
[Misleading] which could be a strong negative sig-
nal on its own. The second tweet has a cue word
[not] but it does not negate the words in that sen-
tence or change their polarity. The negation cue

[not] in the second tweet is not a true negation cue,
and hence it has no scope.

S.No Tweet Sentiment
1 @Username I don’t think you do un-

derstand. Buyers and Sellers deserve to
know facts,User actively prevents accu-
rate feedback. #Misleading.

Negative

2 @Username Sorry to hear this. Have
you had a chance to call/chat us? If not,
we can look into options:

Positive

Table 1: Customer Service Conversation.

Negation can be expressed in different ways in
natural language. It may be through the use of
explicit negation cues such as no, not and never
that have a morphologic indication of a negative
meaning. This also includes a group of broad or
semi negatives words (e.g. barely, hardly, and sel-
dom) that have a negative meaning but are without
any morphological negative. This has been also
referred to as clausal or syntactic negation (Quirk
et al., 1985; Givón, 1993). These cue words are of-
ten used to negate a statement or an assertion that
expresses a judgment or an opinion. However in
some contexts, these cue words function as excla-
mations, and not as true negation cues. These false
cues do not change the sentiment polarity of the
following expression, and hence do not have any
associated scope. We define rules to identify true
negation cues and their scopes more suited to con-
versational data than existing general review data.

The impact of negation has been studied in do-
mains such as biomedical, literary texts, and on-
line reviews (Szarvas et al., 2008; Morante et al.,
2008; Councill et al., 2010; Reitan et al., 2015;
Konstantinova et al., 2012); however, none of
the previous corpora are conversational in nature.
Scope definitions may depend on the domain. Re-
itan et al. (2015) showed that negation scope algo-



47

rithm trained on a twitter domain struggled when
tested on a medical domain. Majority of the pre-
vious work in scope detection has been dominated
by SVMs or Neural Networks, which require ex-
pensive annotated training data. Scope annotation
is costly and time-intensive as all the scope con-
flicts have to be resolved by mutual discussion
amongst expert annotators. Our main motivation
is to create a system that does not require a huge
amount of training data for scope detection, but
has comparable performance to machine learning
models that require annotated training data. The
proposed method uses constituency parse trees
and semantic knowledge to predict scope. The re-
sults in Table 7 show that the method is compara-
ble to state of the art BiLSTM model from (Fan-
cellu et al., 2016) on gold negation cues for scope
prediction. Since our method does not need expen-
sive training data, we could also use this method to
predict on other negation data sets. However, our
aim here was first to test if the predicted negation
scope improves sentiment in conversations.

For a real time sentiment prediction system, we
need both a cue prediction system to determine
the true negation cues, and scope detection. As
a first step, we use a data based approach to train
an SVM to predict true negation cues. It’s much
faster and simpler to get annotated data for cue
prediction, a binary task as compared to scope de-
tection, which is a sequence labeling task. This
is followed by a second step of constituency tree-
based negation scope detection for predicted cues.
The last step applies negation prediction coupled
with antonym dictionary to improve the senti-
ment performance for a combination CNN-LSTM
model.

The contributions of this paper are:

• Negation scope rules more suited to conver-
sational data.

• A constituency-tree based approach for scope
detection that uses both semantic and struc-
tural information, and does not require anno-
tated data for scope.

• An antonym based negation applied to a
combination CNN-LSTM model for senti-
ment prediction in conversations.

We begin with a discussion of related work in
Section 2, followed by negation corpus in Section

3, and negation cue and scope detection experi-
ments in Section 4. Next, we show the effect of in-
troducing negation detection for the sentiment task
in Section 5. We then compare and contrast the
twitter conversational sentiment data to previous
datasets in Section 6. Finally, Section 7 presents
the conclusions and future directions.

2 Related Work

Initial studies on negation scope detection were
performed in Biomedical domain including med-
ical reports, biological abstracts, and papers
(Szarvas et al., 2008). Morante and Daelemans
(2009) used a 2 step approach: first, a decision tree
to predict negation cues, followed by a CRF meta-
learner to predict negation scope. The model used
a combination of k-nearest neighbors, a support
vector machine, and a CRF. The research in this
field was further enhanced by a shared SemEval
2012 negation and scope resolution task (Morante
and Blanco, 2012). The organizers released a cue
and scope annotated corpus of Conan Doyle sto-
ries.

Read et al. (2012) described both, a rule-based
and a data driven approach for scope resolution.
Both the methods were driven by the hypothe-
sis that syntactic units correspond to scope an-
notations. The rule-based approach used heuris-
tic rules based on POS tags and constituent cat-
egory labels, while machine learning used SVM
based ranking of syntactic constituents. Limited
rule based system obtained similar results to the
data-driven system on a held-out set. This result
was particularly note-worthy since getting suffi-
cient scope annotation training data for every new
domain is quite expensive, and requires trained an-
notators. A comparison of these results motivated
us to further develop the rule-based system for the
conversational domain using both semantic infor-
mation and syntactic structure. (Councill et al.,
2010; Lapponi et al., 2012; Reitan et al., 2015)
used CRF-based sequence labeling using features
from dependency tree. Packard et al. (2014) used
hand-crafted heuristics to traverse Minimal Recur-
sion Semantics (MRS). However, if a reliable rep-
resentation for a sentence could not be created,
their system used a fall back mechanism based on
Read et al. (2012). Fancellu et al. (2016) showed
that a neural network based model using a BiL-
STM outperformed the previously developed clas-
sifiers on both scope token recognition and exact



48

scope matching for in domain testing but not on
a different domain. The authors noted that when
tested on a different test set from Wikipedia, White
(2012)’s model built on constituency-based fea-
tures performed better.

A survey on the role of negation in sentiment
analysis was done by (Wiegand et al., 2010) stat-
ing that negation expressions are ambiguous i.e.
in some contexts do not function as a negation
and, therefore, need to be disambiguated. Rules of
composition were defined by Moilanen and Pul-
man (2007) on the syntactic representation of a
sentence to account for negation and the model-
ing paradigm could be applied to determine the
sub-sentential polarity of the sentiment expressed.
(Councill et al., 2010) showed that a CRF based
negation enhanced classifier improved the F-score
of positive on-line reviews by 29.5% and 11.4%
for negative. Much recent progress in the field
has been in connection with the "The Interna-
tional Workshop on Semantic Evaluation" (Se-
mEval) (Nakov et al., 2013). Since 2013 the
workshop has included shared tasks on "Sentiment
Analysis in Twitter". Most of the top perform-
ing systems submitted used just a simple punctua-
tion model that assigns a negation cue scope over
all the terms to the next punctuation (Tang et al.,
2014; Miura et al., 2014; Mohammad et al., 2013).
(Kiritchenko et al., 2014a) reported an improve-
ment of up to 6.5 percentage points when handling
negated context on the SemEval-2013 test set. Us-
ing the simple punctuation model for scope detec-
tion, an improvement of upto 6% was reported by
(Reitan et al., 2015).

With the recent advances in deep learning and
use of embeddings, the CNN and LSTM based
models have shown to outperform traditional
SVM and lexicon based methods for sentiment in
twitter and review domain. Kim (2014) applied a
CNN based model to numerous document classi-
fication tasks, and improved the sentiment state of
the art using a CNN architecture with one layer of
convolution trained using word vectors obtained
from Mikolov et al. (2013) on 100 billion words
of Google News. Yin and Schütze (2015) com-
bined different word embeddings using multichan-
nel CNN. Wang et al. (2016) showed that a com-
bination CNN-LSTM outperformed CNN for sen-
timent task. Shin et al. (2017) integrated senti-
ment embeddings in a CNN to build simpler high-
performing models with much smaller word em-

beddings. However, none of the previous work
has explored negation coupled with antonyms to
get a better sentence representation for sentiment
prediction.

3 Conversational Negation Corpus

hardly lack lacking lacks neither
no nobody none nothing nowhere
cant arent dont doesnt didnt
havent isnt mightnt mustnt neednt
shouldnt wasnt werent wouldnt without
seldom scarcely wont never aint
barely nor not hadnt rather
hasnt shant

Table 2: Negation cue lexicon.

We selected conversations from the Twitter cus-
tomer service pages of different companies and
downloaded 89552 customer service tweets in to-
tal1. A lexicon of explicit cue words that may act
as indicators of negation was primarily adopted
from (Councill et al., 2010; Reitan et al., 2015).
It was further extended to include semi negative
words. The final set of cues used is shown in Ta-
ble 2. We then extracted 23243 tweets containing
explicit negation cues giving a frequency of 26%.
In contrast, the equivalent numbers for BioScope
corpus (Szarvas et al., 2008) and for Twitter cor-
pus (Reitan et al., 2015) are 13.8 % and 13.5% re-
spectively. Tottie (1991) presented a comprehen-
sive taxonomy of English negations and stated that
frequency of negation is 12.8% in written English.
In another statistical study on negation, (Biber,
1999) reported that negation is much more fre-
quent in conversation as compared to written dis-
course. Since we had a lot more negation cue
occurrences, we divided the tweets into 5 differ-
ent groups based on the number of negation cues
present in each tweet. A random sample was se-
lected from each group based on the number of
instances in each group giving a dataset of 2000
tweets. A separate set of 100 tweets was used as
a development set to help formulate the rules and
study negation patterns. Every tweet was anno-
tated by a pair of annotators. To test the robustness
of guidelines, we measured inter-annotator agree-
ments (IAA) for each pair of raters using the to-
ken level and full scope measures as used in previ-
ous work (Reitan et al., 2015). The token level is
the percentage of tokens annotators agreed upon.

1 Comapny names are anonymous for annotation



49

Since the average number of tokens in scope is
far less than the number outside the scope, this is
a skewed measure. For full scope, it is the per-
centage of scopes that have a complete and exact
match amongst annotators (PCS). After an initial
annotation phase of 1000 tweets, the average token
level agreement was 0.95 and full scope was 0.78.
All the scope conflicts were mutually resolved af-
ter discussion. Corpus statistics are shown in Ta-
ble 3. The average number of tokens per tweet is
22.3, per sentence is 13.6 and average scope length
is 2.9.

Total negation cues 2921
True negation cues 2674
False negation cues 247
Average scope length 2.9
Average sentence length 13.6
Average tweet length 22.3

Table 3: Cue and token distribution in the conversa-
tional negation corpus.

3.1 Annotation Guidelines
We define rules to identify true negation cues and
their scopes more suited to twitter customer ser-
vice conversational data than existing general re-
view data, which has its own characteristics such
as brevity and skewed distribution towards nega-
tive polarity (Sec. 6). The guidelines described
here were adapted from Councill et al. (2010)
but modified for customer service conversations.
Nouns and adjectives are key indicators of senti-
ment (Hu and Liu, 2004; Pang and Lee, 2008) and
hence we had a more restricted scope for noun and
adjectives as compared to verbs and adverbs. In
the following examples, the cue is underlined and
the scope is marked in bold.

• Annotating the negation cue.

– False Negation: Some negation cues can
be used in multiple senses and hence the
mere presence of an explicit cue in a
sentence does not imply that it functions
as a negator, (e.g., He could not help me
more). Reitan et al. (2015) reported that
in the twitter corpus the cue word no of-
ten occurs as an exclamation leading to
erroneous predictions. Such cues should
be marked as false negations.

– Negation cues are not part of the scope.

• Annotating the Scope

– Annotate the minimal span for scope.
– Scope is continuous.
– A noun or an adjective negated in a noun

phrase: If only the noun or adjective is
being negated then do not annotate the
entire clause. Consider each term sepa-
rately, (e.g., There are no details on the
return page).

– A verb or an adverb phrase: By and
large, the entire phrase is annotated,
(e.g., I do not want to update it any-
more ).

We used a different scheme for annotating
nouns and adverbs as compared to (Councill et al.,
2010). Our nouns have a more restricted scope
contrary to the previous work where typically the
entire phrase is negated in a noun phrase.

4 Negation Cue and Scope Detection
Experiments

We divided the dataset into train and test sets giv-
ing a training set of 2317 cues and test set of 604
cues to train both a cue detection and BiLSTM
scope prediction.

4.1 Negation Cue Detection

The task of cue detection system is to determine
if the potential cue word negates a concept in the
sentence. It is based on the state-of-the-art cue
classifier described by (Read et al., 2012; Velldal,
2011; Enger et al., 2017). A binary SVM classifier
is used to disambiguate the cue for only the known
cue words, considering the set of cue words as a
closed class. Our baseline system uses the features
and implementation as described in Enger et al.
(2017). The features used are the word form, POS
and lemma of the token, and lemmas for previous
and next position. Adding simple features such as
position of the cue word in the sentence, POS bi-
grams improves the F-score of false negation from
a 0.61 baseline to 0.68 on a test set containing 47
false and 557 actual negation cues. See Table 4.

F-Score

Baseline Proposed Support

False cues 0.61 0.68 47
Actual cues 0.97 0.98 557

Table 4: Cue classification on the test set.



50

4.2 Negation Scope Detection

Syntactic structure of the sentence has been of-
ten used to determine the scope of negation using
supervised classifiers (Morante and Daelemans,
2009; Councill et al., 2010; Reitan et al., 2015;
Read et al., 2012; Carrillo de Albornoz et al.,
2012). Our work is inspired by the previous
rule-based approach using constituency tree (Read
et al., 2012; Carrillo de Albornoz et al., 2012; Vell-
dal et al., 2012). We build on that work by adding
rules based on semantic information, the position
of the negation cue in the tree and the projection
of its parent based on phrase structure. The syntax
tree is obtained using Stanford CoreNLP (Man-
ning et al., 2014). It is possible that the nega-
tion marker may be present in the main clause
but semantically belong to the embedded clause.
(Gotti et al., 2008) mention that semantic content
of copula verbs is subsidiary to that of subject
complement, (e.g., A drunken worker does not be-
come rich, the negation marker "not" negates the
subject complement "rich" rather than the copula
verb "become"). Neg-raising is a linguistic phe-
nomenon where certain predicates such as think,
believe and seem occur in the main clause but may
be interpreted to negate the complement clause
(Fillmore, 1963; Horn, 1989). We move ahead in
a linear order on either finding a copula verb or
neg-raising predicates (NRPs). Table 5 contains
the list of such verbs used. At this point, the algo-
rithm branches based on POS tag of the token. We
traverse the tree in an upward direction until we
find a parent with the desired phrase tag as deter-
mined by the POS tag of the token. This method
differs from the previous work that finds the first
common ancestor enclosing the negation cue and
the word immediately after it, and assumes all de-
scendant leaf nodes to the right as its scope (Read
et al., 2012). Our detailed algorithm for finding
the scope is presented in Figure 1.

think believe seem appear feel
grow look prove remain smell
sound become might are am
been has were was is

Table 5: Neg-raising predicates (NRP) and copula
verb.

Though we use SBAR* tags from syntax tree to
determine the clause boundaries but it cannot de-
tect all boundaries. We therefore also used explicit

1. Traverse the tokens in linear order and stop on finding
any cue from the explicit cue lexicon.

2. Find the next first occurrence of noun, verb, adverb,
adjective.

3. If the verb is an instance of copula verb or neg-raising,
move to step2 else go to step4.

4. Branch depending upon POS tag of the token found in
step2.

(a) For nouns and adjectives:
• Traverse the tree in upward direction level

by level until you reach an ancestor with a
tag of NP, VP, ADJP, SBAR* or S* for ad-
jectives. For nouns, stop at NP, SBAR* or
S*.

• If a PP, VP, ADVP, SQ, SINV or SBAR* is a
right child of the ancestor, then remove that
child.

• Get all the descendant leaves as scope.
(b) For verbs and adverbs:

• Traverse the tree in upward direction level
by level until you reach an ancestor with a
tag of VP, SBAR* or S*.

• If there exists an SBAR*, SQ, or SINV tag
as a right child of the ancestor then remove
that child.

• Get all the descendant leaves as scope.
5. Apply post-processing rules to align the scopes.

Figure 1: Negation scope detection .

discourse connectives that signal a contrast rela-
tion, or a coordination to limit the scope. These
connectives act as a boundary for an idea ex-
pressed in one clause. For example, To be hon-
est I am not angry but upset, the scope of not as
per the rules given in Figure 1, would be angry
but upset. Once we find this scope, we use the
discourse connective ‘but’ to delimit the scope.
The list of connectives used is given in Table 6.
Morante and Blanco (2012) reported that for the
SemEval shared task on negation scope detection,
most of the systems were post processed to im-
prove their performance. Read et al. (2012) for-
mulated a set of slackening heuristics by remov-
ing certain constituents at the beginning or end of
a scope. This improved the alignment of scopes
from an initial 52.42% to 86.13%. Following a
similar approach, the post-processing rules were
designed and are given in Figure 2.

because while until however what
but though although nothing nowhere
whenever & and nonetheless whereas
whose why where wherever

Table 6: Prune-connective list



51

• If the scope contains a connective from the prune-
connective list then delimit the scope before the con-
nective.

• If the scope contains a punctuation then delimit the
scope before the punctuation marker.

• Remove the negation cue from the scope.
• Remove the scope words before the cue word, if any.
• If no scope is found after using these rules then predict

a default scope as all the tokens up to the first noun,
adjective or verb.

• Include the tokens after the negation cue, upto the be-

ginning of the predicted scope.

Figure 2: Post-processing heuristic rules.

4.3 Negation Scope Detection Evaluation

The algorithm is evaluated using two different
measures; token-level and scope-level. Every to-
ken can be either in-scope or out of scope. We
report the F-score for both in-scope and out-of-
scope tokens. Since the output is a sequence, F-
score metrics may be insufficient as it just consid-
ers individual tokens. We also report percentage
of correct scopes (PCS). Results are given in Ta-
ble 7. The out-of-scope token has a higher F-score

Punctuation BiLSTM Proposed

In-scope (F) 0.66 0.88 0.85
Out-scope (F) 0.87 0.97 0.97
PCS 0.52 0.72 0.72

Table 7: Negation classifier performance for scope de-
tection with gold cues and scope.

as compared to in-scope. This is expected since
scope tokens are restricted and less in number as
compared to out-of-scope (See Table 3). The in-
scope F-score is more important for the down-
stream task of sentiment as we apply negation on
predicted in-scope tokens for sentiment. The re-
sults show that our proposed model is comparable
to the BiLSTM model for sentences with gold cues
that have an annotated scope, but our model does
not require annotated data. For BiLSTM, we used
the implementation provided by the authors 2. We
also implemented a punctuation model that marks
as negated all terms from a negation cue to the next
punctuation. Fancellu et al. (2017) mentioned
punctuation alone as a strong predictor for nega-
tion scope detection task for a majority previous of
negation corpora. Notably, it performs poorly on
our data as our scope is more restricted. We next

2https://github.com/ffancellu/NegNN

show that having a restricted scope is beneficial to
the antonym based negation sentiment prediction.

5 Sentiment with Negation Detection
Pipeline

Here we show the integration of predicted nega-
tion scope in sentiment prediction pipeline. We
begin with an overview of the data preprocessing,
features and modeling, followed by our experi-
mental setup and results. Finally, a comparison
of the prediction performance of different systems
is presented.

5.1 Experimental Method
From our tweet collection, we discarded tweets
containing images and Non-English characters
and anonymized all user and company handles,
giving a dataset of 21746 tweets. A sentiment
annotation task was run on a data annotation
platform. Each tweet was initially annotated
by 5 annotators using a 4 point (0 to 3) Lik-
ert scale (Likert, 1932) indicating Not-At-All,
Slight, Moderate and Very about their percep-
tion on the sentiment for a given tweet. We used a
set of gold standard questions to filter out the bad
annotators, computed the average score for each
label, and assigned the maximum score. A tweet
is assigned a sentiment label if the maximum score
for that label is greater than 1 else it is discarded
from the study, giving a labeled dataset of 17779
tweets. To compute the inter-annotator agreement,
first we measured what percentage of the annota-
tors out of 5 contributed to the final sentiment la-
bel and then took the average over all the tweets
giving a 78.8% inter-annotator agreement.

5.1.1 Data Pre-processing
A cleaning module is incorporated to reduce spar-
sity when generating word-based features. We re-
placed all links/URL by a keyword URL, removed
# from the hashtags, replaced all @mentions, and
replaced emojis and emoticons with the word ex-
planation. An entity recognition module is run
to replace the identified entity using a keyword
"ENT".

5.1.2 Features
TFIDF-based unigram features.
Existence of consecutive question and exclama-
tion marks and capitalized words.
Emotion lexicon features: A count of the number
of words in each of the 8 emotion classes from

https://github.com/ffancellu/NegNN


52

the NRC emotion lexicon (Mohammad and Tur-
ney, 2010)
Sentiment lexicons used:
Bing Liu’s Opinion Lexicon (Ding et al., 2008);
The MPQA Subjectivity Lexicon (Wilson et al.,
2005); Sentiment140 Lexicon (Kiritchenko et al.,
2014b); NRC Hashtag Sentiment Lexicon (Kir-
itchenko et al., 2014b);
For a given tweet, we computed minimum, maxi-
mum, average and summation of positive and neg-
ative scores of the words in the tweet that lies
within a negation scope, and the average sentiment
score of the last word in the tweet.
Negation handling: Append "NOT_" to each word
in the scope.

5.1.3 SVM Evaluation
Libsvm (Chang and Lin, 2011) is used to imple-
ment the SVM classifier. The tweet annotated
dataset was divided into a train and test set (see
Table 8 for the distribution). The training set was
further split into a ratio of 85:15 for the valida-
tion set. The three parameters w1, w2 and C were
tuned using the validation set. The variables w1
and w2 are the penalty associated to a class and C
is the regularization. Table 8 shows the evaluation
metric using Precision, Recall and F measure for
each class in the test set.

We do not find a major difference for SVMs(
w/o negation). This is in spite of using the stan-
dard features such as prefixing the tokens in scope
with a keyword NOT_ and changing the polarity of
the sentiment-bearing words using sentiment lex-
icons as described in previous work. A possible
reason is that customer service domain is more
negative as compared to general review domain
See Section 6 for detailed analysis.

5.2 Neural Network Evaluation

Baseline 1: Our first baseline is a single layer
CNN as used in (Kim, 2014). The model consists
of a 1D convolution layer of window size 2 and 64
different filters. The convolution layer takes as in-
put the GloVe embeddings. Max pooling layer is
used to reduce the output dimensionality but keep
the most salient information.
Baseline 2: (Wang et al., 2016) presented a jointed
CNN and LSTM architecture. The features gener-
ated from convolution and pooling operation can
be viewed as local features similar to ngrams but
cannot handle long term dpendencies. LSTM can
handle CNN’s limitation by preserving historical

information for a long period of time. Using this
as a motivation, we included a convolutional layer
and max pooling layer before the input is fed into
an LSTM. A bidirectional LSTM layer is stacked
on the convolutions layer and the tweet represen-
tation is taken to the fully connected network.
Proposed Negation + Antonym CNN-LSTM :
We modified the sentence representation learned
by replacing a word in the negation scope with it’s
antonym. Using antonyms would reduce the Out-
of-Vocabulary words as compared to prefixing a
word with "NOT_" for learning word representa-
tions. Replacing all the words upto punctuation
with antonyms could entirely change the sentence
meaning and hence this required a more restricted
and accurate scope detection. We get the predicted
scopes from the scope detection model described
in Section 4. The antonym list is obtained from
AntNET (Rajana et al., 2017)

For the NN-based approaches, 20% data is used
for validation and we save the model weights only
if the validation accuracy improves. The outputs
of the LSTM are fed through a sigmoid layer for
binary classification. Regularization is performed
by using a drop-out rate of 0.2 in the drop-out
layer. The model is optimized using the (Kingma
and Ba, 2014) optimizer. The deep network was
implemented using the Keras package (Chollet
et al., 2015). Hyper-parameter optimization for
the neural network is performed using Hyperas,
a python package, based on hyperopt (Bergstra
et al., 2015). Results in Table 8 show that the
antonym based learned representations are more
useful for sentiment task as compared to prefixing
with NOT_. The proposed CNN-LSTM-Our-neg-
Ant improves upon the simple CNN-LSTM-w/o
neg. baseline with F1 scores improving from 0.72
to 0.78 for positive sentiment and from 0.83 to
0.87 for negative sentiment. Hence Negation cou-
pled with antonyms improves the sentiment pre-
diction for a customer service domain.

6 Discussion

In this section, we aim to show the particularities
of our dataset, suggesting the reasons why nega-
tion detection did not improve the performance of
the lexicon-based SVM when previous work had
seen huge performance gains, and intuitions on
how the antonym based method gives improve-
ment.

• Class Distribution



53

Positive Sentiment

Classifier Precision Recall Fscore
SVM-w/o neg. 0.57 0.72 0.64
SVM-Punct. neg. 0.58 0.70 0.63
SVM-our-neg. 0.58 0.73 0.65
CNN 0.63 0.83 0.72
CNN-LSTM 0.71 0.72 0.72
CNN-LSTM-Our-neg-Ant 0.78 0.77 0.78

Negative Sentiment

Precision Recall Fscore
SVM-w/o neg. 0.78 0.86 0.82
SVM-Punct. neg. 0.78 0.87 0.83
SVM-Our neg. 0.80 0.87 0.83
CNN 0.88 0.72 0.79
CNN-LSTM. 0.83 0.83 0.83
CNN-LSTM-our-neg-Ant 0.87 0.87 0.87

Train Test
Positive tweets 5121 1320
Negative tweets 9094 2244

Table 8: Sentiment classification evaluation, using dif-
ferent classifiers on the test set.

Our customer service dataset has a much
larger number of negative tweets while the
benchmark sentiment dataset used in most of
the previous systems has positive class as the
majority class (Kiritchenko et al., 2014b; Re-
itan et al., 2015; Nakov et al., 2013; Moham-
mad et al., 2013; Tang et al., 2014). Addi-
tionally, Reitan et al. (2015) reported that the
classifier struggles with negative class predic-
tion. A F-measure of 0.533 and 0.323 is re-
ported by Reitan et al. (2015) and Councill
et al. (2010) respectively, on negative class
prediction. In contrast, our baseline classifier
achieves a much higher F score of 0.82 on the
negative class.

• Cue Word Distribution.
The conversation negation corpus is anno-
tated for both actual negation cues and sen-
timent. To see if there exists some correla-
tion between the number of cues and senti-
ment, we calculated the percentage of posi-
tive and negative tweets with more than one
cue. 19% of positive tweets contain more
than one negation cue while for the negative
class it is 48%. Though we need more ev-
idence to support, but it is possible that the
number of negation cues in these conversa-
tions is a strong indicator of negative class,
hence the SVM based classifier had better

prediction on negative class detection.

• Sarcasm and Irony
Results in Table 8 show that the classifier
struggles with positive class precision. A
sentiment study on user-generated content
by (Sarmento et al., 2009; Carvalho et al.,
2009) has similar class distribution and re-
sults to ours. The sentences expressing nega-
tive opinions is almost the double of those ex-
pressing positive opinions and the precision
of identifying negative opinions (≈ 89%)
is significantly higher than the precision of
identifying positive opinions (≈60%). They
confirm the relevance of irony for sentiment
analysis by an error analysis of their present
classifier stating that a large proportion of
misclassifications (≈35%) derive from their
system’s inability to account for irony. We
then performed some manual error analysis
on the incorrect positive predictions for SVM
and observed that some of the incorrect pre-
dictions were actually sarcastic, see Table 9.
To get an insight on how our method im-
proves these types of predictions, consider
the example in Row2 in Table 9. It was
predicted as positive by SVM, CNN and
LSTM due to the positive word "Awesome".
Our method detects negation cue word "not"
with "able" in it’s scope. The antonym dic-
tionary then is used to replace "able" with
"incapable". Having a strong negative word
corrects the prediction to negative. These re-
sults indicate that there is room for improve-
ment for the positive class but negation han-
dling may not be enough. A combination of
negation and sarcasm may be a useful direc-
tion to explore in future for customer service
conversations.

S.No Tweet

1 Hi @username - Love u. I’d recommend not
displaying the early bird button in the app if it’s
broken and not working

2 looks like I won’t be able to vote because the
train is running late. Awesome

Table 9: Negative sarcastic examples.

7 Conclusion and Future Work

This paper presented an approach to negation cue
and scope detection in customer service inter-



54

actions on Twitter and the impact of using this
component for sentiment detection. We refined
the annotation guidelines for scope representa-
tion, gathering a dataset of 2000 labeled tweets.
Our rule based approach based on syntactic con-
stituents does not require annotated scope data
for training, but performs comparable to state of
the art BiLSTM. To evaluate the effectiveness
of negation modeling on sentiment detection, we
performed experiments using both an SVM and
CNN-LSTM Architecture. There was no signifi-
cant improvement between the two lexicon based
SVMs (with/without negation handling). The pro-
posed antonym based negation for CNN-LSTM
outperformed both a CNN and a combination
CNN-LSTM that did not handle negation. The
result and error analysis shows that customer ser-
vice interactions have higher frequency of nega-
tion cues, are more skewed towards negative class,
and are sometimes sarcastic. In future, we plan
to study other language phenomenon such as sar-
casm in combination with negation.

References
James Bergstra, Brent Komer, Chris Eliasmith, Dan

Yamins, and David D Cox. 2015. Hyperopt: a
python library for model selection and hyperparam-
eter optimization. Computational Science and Dis-
covery, 8(1):014008.

D. Biber. 1999. Longman Grammar of Spoken and
Written English. Grammar Reference Series. Long-
man.

Jorge Carrillo de Albornoz, Laura Plaza, Alberto Díaz,
and Miguel Ballesteros. 2012. UCM-I: A rule-based
syntactic approach for resolving the scope of nega-
tion. In Proceedings of the First Joint Conference on
Lexical and Computational Semantics, *SEM 2012,
June 7-8, 2012, Montréal, Canada., pages 282–287.
Association for Computational Linguistics.

Paula Carvalho, Luís Sarmento, Mário J. Silva, and Eu-
génio de Oliveira. 2009. Clues for detecting irony in
user-generated contents: Oh...!! it’s "so easy" ;-).
In Proceedings of the 1st International CIKM Work-
shop on Topic-sentiment Analysis for Mass Opinion,
TSA ’09. ACM.

Chih-Chung Chang and Chih-Jen Lin. 2011. Libsvm:
A library for support vector machines. ACM Trans.
Intell. Syst. Technol., 2(3):27:1–27:27.

François Chollet et al. 2015. Keras. https://keras.
io.

Isaac G. Councill, Ryan McDonald, and Leonid Ve-
likovich. 2010. What’s great and what’s not: Learn-
ing to classify the scope of negation for improved

sentiment analysis. In Proceedings of the Workshop
on Negation and Speculation in Natural Language
Processing, NeSp-NLP ’10, pages 51–59, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.

Xiaowen Ding, Bing Liu, and Philip S Yu. 2008. A
holistic lexicon-based approach to opinion mining.
In Proceedings of the 2008 international conference
on web search and data mining, pages 231–240.
ACM.

Martine Enger, Erik Velldal, and Lilja Øvrelid. 2017.
An open-source tool for negation detection: a
maximum-margin approach. In Proceedings of the
EACL workshop on Computational Semantics Be-
yond Events and Roles (SemBEaR), pages 64–69,
Valencia, Spain.

Federico Fancellu, Adam Lopez, Bonnie Webber, and
Hangfeng He. 2017. Detecting negation scope is
easy, except when it isn’t, pages 58–63. Association
for Computational Linguistics.

Federico Fancellu, Adam Lopez, and Bonnie L. Web-
ber. 2016. Neural networks for negation scope de-
tection. In Proceedings of the 54th Annual Meet-
ing of the Association for Computational Linguis-
tics, ACL 2016, August 7-12, 2016, Berlin, Ger-
many, Volume 1: Long Papers. The Association for
Computer Linguistics.

C.J. Fillmore. 1963. The position of embedding trans-
formations in a grammar. Word, 19.

T. Givón. 1993. English grammar: a function-based
introduction. Number v. 2 in English Grammar: A
Function-based Introduction. J. Benjamins Pub. Co.

M. Gotti, M. Dossena, and R. Dury. 2008. English
Historical Linguistics 2006: Selected papers from
the fourteenth International Conference on English
Historical Linguistics (ICEHL 14), Bergamo, Vol-
ume I: Syntax and Morphology. English Historical
Linguistics 2006. John Benjamins Publishing Com-
pany.

Laurence Horn. 1989. A Natural History of Negation.
University of Chicago Press.

Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the Tenth
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, KDD ’04. ACM.

Yoon Kim. 2014. Convolutional neural networks for
sentence classification. In Proceedings of the 2014
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP 2014, October 25-29,
2014, Doha, Qatar, A meeting of SIGDAT, a Special
Interest Group of the ACL, pages 1746–1751.

Diederik P. Kingma and Jimmy Ba. 2014. Adam:
A method for stochastic optimization. CoRR,
abs/1412.6980.

http://aclweb.org/anthology/S/S12/S12-1037.pdf
http://aclweb.org/anthology/S/S12/S12-1037.pdf
http://aclweb.org/anthology/S/S12/S12-1037.pdf
https://doi.org/10.1145/1961189.1961199
https://doi.org/10.1145/1961189.1961199
https://keras.io
https://keras.io
http://dl.acm.org/citation.cfm?id=1858959.1858969
http://dl.acm.org/citation.cfm?id=1858959.1858969
http://dl.acm.org/citation.cfm?id=1858959.1858969
http://aclweb.org/anthology/E17-2010
http://aclweb.org/anthology/E17-2010
http://aclweb.org/anthology/P/P16/P16-1047.pdf
http://aclweb.org/anthology/P/P16/P16-1047.pdf
https://books.google.com/books?id=1Zw5AAAAQBAJ
https://books.google.com/books?id=1Zw5AAAAQBAJ
https://books.google.com/books?id=1Zw5AAAAQBAJ
https://books.google.com/books?id=1Zw5AAAAQBAJ
https://books.google.com/books?id=1Zw5AAAAQBAJ
http://aclweb.org/anthology/D/D14/D14-1181.pdf
http://aclweb.org/anthology/D/D14/D14-1181.pdf
http://arxiv.org/abs/1412.6980
http://arxiv.org/abs/1412.6980


55

Svetlana Kiritchenko, Xiaodan Zhu, and Saif M. Mo-
hammad. 2014a. Sentiment analysis of short infor-
mal texts. J. Artif. Intell. Res., 50:723–762.

Svetlana Kiritchenko, Xiaodan Zhu, and Saif M Mo-
hammad. 2014b. Sentiment analysis of short in-
formal texts. Journal of Artificial Intelligence Re-
search, 50:723–762.

Natalia Konstantinova, Sheila CM De Sousa, Noa
P Cruz Díaz, Manuel J Mana López, Maite Taboada,
and Ruslan Mitkov. 2012. A review corpus anno-
tated for negation, speculation and their scope. In
LREC, pages 3190–3195.

Emanuele Lapponi, Erik Velldal, Lilja Ovrelid, and
Jonathon Read. 2012. Uio2: Sequence-labeling
negation using dependency features. In Proceedings
of the First Joint Conference on Lexical and Com-
putational Semantics - Volume 1: Proceedings of the
Main Conference and the Shared Task, and Volume
2: Proceedings of the Sixth International Workshop
on Semantic Evaluation, SemEval ’12. Association
for Computational Linguistics.

Rensis Likert. 1932. A technique for the measurement
of attitudes. Archives of psychology.

Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David Mc-
Closky. 2014. The Stanford CoreNLP natural lan-
guage processing toolkit. In Association for Compu-
tational Linguistics (ACL) System Demonstrations.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.
Corrado, and Jeffrey Dean. 2013. Distributed rep-
resentations of words and phrases and their com-
positionality. In Advances in Neural Information
Processing Systems 26: 27th Annual Conference on
Neural Information Processing Systems 2013. Pro-
ceedings of a meeting held December 5-8, 2013,
Lake Tahoe, Nevada, United States., pages 3111–
3119.

Yasuhide Miura, Shigeyuki Sakaki, Keigo Hattori, and
Tomoko Ohkuma. 2014. Teamx: A sentiment an-
alyzer with enhanced lexicon mapping and weight-
ing scheme for unbalanced data. In Proceedings of
the 8th International Workshop on Semantic Evalu-
ation (SemEval 2014), pages 628–632. Association
for Computational Linguistics.

Saif Mohammad, Svetlana Kiritchenko, and Xiaodan
Zhu. 2013. Nrc-canada: Building the state-of-the-
art in sentiment analysis of tweets. In Proceed-
ings of the 7th International Workshop on Semantic
Evaluation, SemEval@NAACL-HLT 2013, Atlanta,
Georgia, USA, June 14-15, 2013, pages 321–327.
The Association for Computer Linguistics.

Saif M Mohammad and Peter D Turney. 2010. Emo-
tions evoked by common words and phrases: Us-
ing mechanical turk to create an emotion lexicon. In
Proceedings of the NAACL HLT 2010 workshop on

computational approaches to analysis and genera-
tion of emotion in text, pages 26–34. Association for
Computational Linguistics.

Karo Moilanen and Stephen Pulman. 2007. Sentiment
composition. In Proceedings of Recent Advances in
Natural Language Processing (RANLP 2007), pages
378–382.

Roser Morante and Eduardo Blanco. 2012. *sem 2012
shared task: Resolving the scope and focus of nega-
tion. In Proceedings of the First Joint Conference on
Lexical and Computational Semantics - Volume 1:
Proceedings of the Main Conference and the Shared
Task, and Volume 2: Proceedings of the Sixth In-
ternational Workshop on Semantic Evaluation, Se-
mEval ’12, pages 265–274, Stroudsburg, PA, USA.
Association for Computational Linguistics.

Roser Morante and Walter Daelemans. 2009. A met-
alearning approach to processing the scope of nega-
tion. In Proceedings of the Thirteenth Confer-
ence on Computational Natural Language Learning,
CoNLL ’09, pages 21–29, Stroudsburg, PA, USA.
Association for Computational Linguistics.

Roser Morante, Anthony M. L. Liekens, and Walter
Daelemans. 2008. Learning the scope of negation in
biomedical texts. In 2008 Conference on Empirical
Methods in Natural Language Processing, EMNLP
2008, Proceedings of the Conference, 25-27 October
2008, Honolulu, Hawaii, USA, A meeting of SIG-
DAT, a Special Interest Group of the ACL, pages
715–724. ACL.

Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,
Veselin Stoyanov, Alan Ritter, and Theresa Wilson.
2013. Semeval-2013 task 2: Sentiment analysis
in twitter. In Second Joint Conference on Lexical
and Computational Semantics (* SEM), Volume 2:
Proceedings of the Seventh International Workshop
on Semantic Evaluation (SemEval 2013), volume 2,
pages 312–320.

Woodley Packard, Emily M. Bender, Jonathon Read,
Stephan Oepen, and Rebecca Dridan. 2014. Simple
negation scope resolution through deep parsing: A
semantic solution to a semantic problem. In Pro-
ceedings of the 52nd Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 69–78,
Baltimore, USA.

Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Found. Trends Inf. Retr.

Randolph Quirk, Sidney Greenbaum, Geoffrey Leech,
and Jan Svartvik. 1985. A Comprehensive Grammar
of the English Language. Longman, London.

Sneha Rajana, Chris Callison-Burch, Marianna Apidi-
anaki, and Vered Shwartz. 2017. Learning antonyms
with paraphrases and a morphology-aware neural
network. In Proceedings of the 6th Joint Conference
on Lexical and Computational Semantics (*SEM
2017), pages 12–21. Association for Computational
Linguistics.

https://doi.org/10.1613/jair.4272
https://doi.org/10.1613/jair.4272
https://doi.org/10.3115/v1/S14-2111
https://doi.org/10.3115/v1/S14-2111
https://doi.org/10.3115/v1/S14-2111
http://aclweb.org/anthology/S/S13/S13-2053.pdf
http://aclweb.org/anthology/S/S13/S13-2053.pdf
http://users.ox.ac.uk/~wolf2244/sentCompRANLP07Final.pdf
http://users.ox.ac.uk/~wolf2244/sentCompRANLP07Final.pdf
http://dl.acm.org/citation.cfm?id=2387636.2387679
http://dl.acm.org/citation.cfm?id=2387636.2387679
http://dl.acm.org/citation.cfm?id=2387636.2387679
http://dl.acm.org/citation.cfm?id=1596374.1596381
http://dl.acm.org/citation.cfm?id=1596374.1596381
http://dl.acm.org/citation.cfm?id=1596374.1596381
http://www.aclweb.org/anthology/D08-1075
http://www.aclweb.org/anthology/D08-1075
http://aclweb.org/anthology/S17-1002
http://aclweb.org/anthology/S17-1002
http://aclweb.org/anthology/S17-1002


56

Jonathon Read, Erik Velldal, Lilja Øvrelid, and
Stephan Oepen. 2012. Uio1: Constituent-based dis-
criminative ranking for negation resolution. In Pro-
ceedings of the First Joint Conference on Lexical
and Computational Semantics, *SEM 2012, June 7-
8, 2012, Montréal, Canada., pages 310–318. Asso-
ciation for Computational Linguistics.

Johan Reitan, Jorgen Faret, Bjorn Gamback, and Lars
Bungum. 2015. Negation scope detection for twitter
sentiment analysis. In WASSA@EMNLP.

Luís Sarmento, Paula Carvalho, Mário J. Silva, and Eu-
génio de Oliveira. 2009. Automatic creation of a ref-
erence corpus for political opinion mining in user-
generated content. In Proceedings of the 1st Inter-
national CIKM Workshop on Topic-sentiment Anal-
ysis for Mass Opinion, TSA ’09, pages 29–36, New
York, NY, USA. ACM.

Bonggun Shin, Timothy Lee, and Jinho D. Choi.
2017. Lexicon integrated CNN models with at-
tention for sentiment analysis. In Proceedings of
the 8th Workshop on Computational Approaches
to Subjectivity, Sentiment and Social Media Analy-
sis, WASSA@EMNLP 2017, Copenhagen, Denmark,
September 8, 2017, pages 149–158.

György Szarvas, Veronika Vincze, Richárd Farkas, and
János Csirik. 2008. The bioscope corpus: Anno-
tation for negation, uncertainty and their scope in
biomedical texts. In Proceedings of the Workshop
on Current Trends in Biomedical Natural Language
Processing, BioNLP ’08, pages 38–45, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.

Duyu Tang, Furu Wei, Bing Qin, Ting Liu, and Ming
Zhou. 2014. Coooolll: A deep learning system for
twitter sentiment classification. In Proceedings of
the 8th International Workshop on Semantic Eval-
uation, SemEval@COLING 2014, Dublin, Ireland,
August 23-24, 2014., pages 208–212.

Gunnel Tottie. 1991. Negation in English speech and
writing : a study in variation. San Diego : Aca-
demic Press. (Quantitative analyses of linguistic
structure series).

Erik Velldal. 2011. Predicting speculation: A sim-
ple disambiguation approach to hedge detection
in biomedical literature. Journal of Biomedi-
cal Semantics, 2(5). Supplement to the Fourth
International Symposium on Semantic Mining in
Biomedicine.

Erik Velldal, Lilja Øvrelid, Jonathon Read, and
Stephan Oepen. 2012. Speculation and negation:
Rules, rankers, and the role of syntax. Computa-
tional Linguistics, 38(2):369–410.

Xingyou Wang, Weijie Jiang, and Zhiyong Luo. 2016.
Combination of convolutional and recurrent neural
network for sentiment analysis of short texts. In
COLING 2016, 26th International Conference on

Computational Linguistics, Proceedings of the Con-
ference: Technical Papers, December 11-16, 2016,
Osaka, Japan, pages 2428–2437.

James Paul White. 2012. Uwashington: Negation res-
olution using machine learning methods. In *SEM
2012: The First Joint Conference on Lexical and
Computational Semantics – Volume 1: Proceedings
of the main conference and the shared task, and
Volume 2: Proceedings of the Sixth International
Workshop on Semantic Evaluation (SemEval 2012),
pages 335–339. Association for Computational Lin-
guistics.

Michael Wiegand, Alexandra Balahur, Benjamin Roth,
Dietrich Klakow, and Andrés Montoyo. 2010. A
survey on the role of negation in sentiment analysis.
In Proceedings of the Workshop on Negation and
Speculation in Natural Language Processing, NeSp-
NLP ’10, pages 60–68, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.

Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the Con-
ference on Human Language Technology and Em-
pirical Methods in Natural Language Processing,
HLT ’05, pages 347–354, Stroudsburg, PA, USA.
Association for Computational Linguistics.

Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2009. Recognizing contextual polarity: An explo-
ration of features for phrase-level sentiment analy-
sis. Comput. Linguist., 35(3):399–433.

Wenpeng Yin and Hinrich Schütze. 2015. Multichan-
nel variable-size convolution for sentence classifi-
cation. In Proceedings of the 19th Conference on
Computational Natural Language Learning, CoNLL
2015, Beijing, China, July 30-31, 2015, pages 204–
214.

http://aclweb.org/anthology/S/S12/S12-1041.pdf
http://aclweb.org/anthology/S/S12/S12-1041.pdf
https://doi.org/10.1145/1651461.1651468
https://doi.org/10.1145/1651461.1651468
https://doi.org/10.1145/1651461.1651468
https://aclanthology.info/papers/W17-5220/w17-5220
https://aclanthology.info/papers/W17-5220/w17-5220
http://dl.acm.org/citation.cfm?id=1572306.1572314
http://dl.acm.org/citation.cfm?id=1572306.1572314
http://dl.acm.org/citation.cfm?id=1572306.1572314
http://aclweb.org/anthology/S/S14/S14-2033.pdf
http://aclweb.org/anthology/S/S14/S14-2033.pdf
https://doi.org/10.1162/COLI_a_00126
https://doi.org/10.1162/COLI_a_00126
http://aclweb.org/anthology/C/C16/C16-1229.pdf
http://aclweb.org/anthology/C/C16/C16-1229.pdf
http://aclanthology.coli.uni-saarland.de/pdf/S/S12/S12-1044.pdf
http://aclanthology.coli.uni-saarland.de/pdf/S/S12/S12-1044.pdf
http://dl.acm.org/citation.cfm?id=1858959.1858970
http://dl.acm.org/citation.cfm?id=1858959.1858970
https://doi.org/10.3115/1220575.1220619
https://doi.org/10.3115/1220575.1220619
https://doi.org/10.1162/coli.08-012-R1-06-90
https://doi.org/10.1162/coli.08-012-R1-06-90
https://doi.org/10.1162/coli.08-012-R1-06-90
http://aclweb.org/anthology/K/K15/K15-1021.pdf
http://aclweb.org/anthology/K/K15/K15-1021.pdf
http://aclweb.org/anthology/K/K15/K15-1021.pdf

