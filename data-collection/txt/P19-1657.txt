



















































Show, Describe and Conclude: On Exploiting the Structure Information of Chest X-ray Reports


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6570–6580
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

6570

Show, Describe and Conclude:
On Exploiting the Structure Information of Chest X-Ray Reports

Baoyu Jing Zeya Wang Eric Xing
Petuum Inc., USA

{baoyu.jing, zeya.wang, eric.xing}@petuum.com

Abstract

Chest X-Ray (CXR) images are commonly
used for clinical screening and diagnosis. Au-
tomatically writing reports for these images
can considerably lighten the workload of ra-
diologists for summarizing descriptive find-
ings and conclusive impressions. The com-
plex structures between and within sections of
the reports pose a great challenge to the auto-
matic report generation. Specifically, the sec-
tion Impression is a diagnostic summarization
over the section Findings; and the appearance
of normality dominates each section over that
of abnormality. Existing studies rarely ex-
plore and consider this fundamental structure
information. In this work, we propose a novel
framework which exploits the structure infor-
mation between and within report sections for
generating CXR imaging reports. First, we
propose a two-stage strategy that explicitly
models the relationship between Findings and
Impression. Second, we design a novel co-
operative multi-agent system that implicitly
captures the imbalanced distribution between
abnormality and normality. Experiments on
two CXR report datasets show that our method
achieves state-of-the-art performance in terms
of various evaluation metrics. Our results ex-
pose that the proposed approach is able to gen-
erate high-quality medical reports through in-
tegrating the structure information.

1 Introduction

Chest X-Ray (CXR) image report generation aims
to automatically generate detailed findings and
diagnoses for given images, which has attracted
growing attention in recent years (Wang et al.,
2018a; Jing et al., 2018; Li et al., 2018). This tech-
nique can greatly reduce the workload of radiolo-
gists for interpreting CXR images and writing cor-
responding reports. In spite of the progress made
in this area, it is still challenging for computers

Findings:
The cardiac silhouette is 
enlarged and has a globular 
appearance. Mild bibasilar 
dependent atelectasis. No 
pneumothorax or large 
pleural effusion. No acute 
bone abnormality.

Impression:
Cardiomegaly with 
globular appearance of the 
cardiac silhouette. 
Considerations would 
include pericardial effusion 
or dilated cardiomyopathy.

Figure 1: An example of chest X-ray image along with
its report. In the report, the Findings section records de-
tailed descriptions for normal and abnormal findings;
the Impression section provides a diagnostic conclu-
sion. The underlined sentence is an abnormal finding.

to accurately write reports. Besides the difficul-
ties in detecting lesions from images, the complex
structure of textual reports can prevent the success
of automatic report generation. As shown in Fig-
ure 1, the report for a CXR image usually com-
prises two major sections: Findings and Impres-
sion. Findings section records detailed descrip-
tions about normal and abnormal findings, such as
lesions (e.g. increased lung marking). Impression
section concludes diseases (e.g. pneumonia) from
Findings and forms a diagnostic conclusion, con-
sisting of abnormal and normal conclusions.

Existing methods (Wang et al., 2018a; Jing
et al., 2018; Li et al., 2018) ignored the relation-
ship between Findings and Impression, as well as
the different distributions between normal and ab-
normal findings/conclusions. In addressing this
problem, we present a novel framework for au-
tomatic report generation by exploiting the struc-
ture of the reports. Firstly, considering the fact
that Impression is a summarization of Findings,
we propose a two-stage modeling strategy given
in Figure 3, where we borrow strength from im-



6571

age captioning task and text summarization task
for generating Impression. Secondly, we decom-
pose the generation process of both Findings and
Impression into the following recurrent sub-tasks:
1) examine an area in the image (or a sentence in
Findings) and decide if an abnormality appears; 2)
write detailed (normal or abnormal) descriptions
for the examined area.

In order to model the above generation pro-
cess, we propose a novel Co-operative Multi-
Agent System (CMAS), which consists of three
agents: Planner (PL), Abnormality Writer (AW)
and Normality Writer (NW). Given an image, the
system will run several loops until PL decides to
stop the process. Within each loop, the agents co-
operate with each other in the following fashion:
1) PL examines an area of the input image (or
a sentence of Findings), and decides whether the
examined area contains lesions. 2) Either AW or
NW will generate a sentence for the area based on
the order given by PL. To train the system, RE-
INFORCE algorithm (Williams, 1992) is applied
to optimize the reward (e.g. BLEU-4 (Papineni
et al., 2002)). To the best of our knowledge, our
work is the first effort to investigate the structure
of CXR reports.

The major contributions of our work are sum-
marized as follows. First, we propose a two-
stage framework by exploiting the structure of
the reports. Second, We propose a novel Co-
operative Multi-Agent System (CMAS) for mod-
eling the sentence generation process of each sec-
tion. Third, we perform extensive quantitative ex-
periments to evaluate the overall quality of the
generated reports, as well as the model’s ability
for detecting medical abnormality terms. Finally,
we perform substantial qualitative experiments to
further understand the quality and properties of the
generated reports.

2 Related Work

Visual Captioning The goal of visual caption-
ing is to generate a textual description for a
given image or video. For one-sentence cap-
tion generation, almost all deep learning meth-
ods (Mao et al., 2014; Vinyals et al., 2015; Don-
ahue et al., 2015; Karpathy and Fei-Fei, 2015)
were based on Convolutional Neural Network
(CNN) - Recurrent Neural Network (RNN) archi-
tecture. Inspired by the attention mechanism in
human brains, attention-based models, such as vi-

sual attention (Xu et al., 2015) and semantic at-
tention (You et al., 2016), were proposed for im-
proving the performances. Some other efforts
have been made for building variants of the hier-
archical Long-Short-Term-Memory (LSTM) net-
work (Hochreiter and Schmidhuber, 1997) to gen-
erate paragraphs (Krause et al., 2017; Yu et al.,
2016; Liang et al., 2017). Recently, deep rein-
forcement learning has attracted growing attention
in the field of visual captioning (Ren et al., 2017;
Rennie et al., 2017; Liu et al., 2017; Wang et al.,
2018b). Additionally, other tasks related to visual
captioning, (e.g., dense captioning (Johnson et al.,
2016), multi-task learning (Pasunuru and Bansal,
2017)) also attracted a lot of research attention.

Chest X-ray Image Report Generation Shin
et al. (2016) first proposed a variant of CNN-RNN
framework to predict tags (location and severity)
of chest X-ray images. Wang et al. (2018a) pro-
posed a joint framework for generating reference
reports and performing disease classification at
the same time. However, this method was based
on a single-sentence generation model (Xu et al.,
2015), and obtained low BLEU scores. Jing et al.
(2018) proposed a hierarchical language model
equipped with co-attention to better model the
paragraphs, but it tended to produce normal find-
ings. Despite Li et al. (2018) enhanced language
diversity and model’s ability in detecting abnor-
malities through a hybrid of template retrieval
module and text generation module, manually de-
signing templates is costly and they ignored the
template’s change over time.

Multi-Agent Reinforcement Learning The tar-
get of multi-agent reinforcement learning is to
solve complex problems by integrating multi-
ple agents that focus on different sub-tasks. In
general, there are two types of multi-agent sys-
tems: independent and cooperative systems (Tan,
1993). Powered by the development of deep learn-
ing, deep multi-agent reinforcement learning has
gained increasing popularity. Tampuu et al. (2017)
extended Deep Q-Network (DQN) (Mnih et al.,
2013) into a multi-agent DQN for Pong game;
Foerster et al. (2016); Sukhbaatar et al. (2016)
explored communication protocol among agents;
Zhang et al. (2018) further studied fully decentral-
ized multi-agent system. Despite these many at-
tempts, the multi-agent system for long paragraph
generation still remains unexplored.



6572

Figure 2: Overview of the proposed Cooperative Multi-Agent System (CMAS).

Figure 3: Show, Describe and Conclude.

3 Overall Framework

As shown in Figure 3, the proposed framework
is comprised of two modules: Findings and Im-
pression. Given a CXR image, the Findings mod-
ule will examine different areas of the image and
generate descriptions for them. When findings are
generated, the Impression module will give a con-
clusion based on findings and the input CXR im-
age. The proposed two-stage framework explic-
itly models the fact that Impression is a conclusive
summarization of Findings.

Within each module, we propose a Co-operative
Multi-Agent System (CMAS) (see Section 4) to
model the text generation process for each section.

4 Co-operative Multi-Agent System

4.1 Overview

The proposed Co-operative Multi-Agent System
(CMAS) consists of three agents: Planner (PL),
Normality Writer (NW) and Abnormality Writer
(AW). These agents work cooperatively to gener-
ate findings or impressions for given chest X-ray
images. PL is responsible for determining whether
an examined area contains abnormality, while NW
and AW are responsible for describing normality
or abnormality in detail (Figure 2).

The generation process consists of several
loops, and each loop contains a sequence of ac-

tions taken by the agents. In the n-th loop, the
writers first share their local states LSn−1,T =
{wn−1,t}Tt=1 (actions taken in the previous loop)
to form a shared global state GSn = (I, {si}n−1i=1 ),
where I is the input image, si is the i-th generated
sentence, and wi,t is the t-th word in the i-th sen-
tence of length T . Based on the global state GSn,
PL decides whether to stop the generation process
or to choose a writer (NW or AW) to produce the
next sentence sn . If a writer is selected, then it
will refresh its memory by GSn and generate a se-
quence of words {wn,t}Tt=1 based on the sequence
of local state LSn,t = {wn,1, · · · , wn,t−1}.

Once the generation process is terminated, the
reward module will compute a reward by compar-
ing the generated report with the ground-truth re-
port. Given the reward, the whole system is trained
via REINFORCE algorithm (Williams, 1992).

4.2 Policy Network

4.2.1 Global State Encoder
During the generation process, each agent will
make decisions based on the global state GSn.
Since GSn contains a list of sentences {si}n−1i=1 , a
common practice is to build a hierarchical LSTM
as Global State Encoder (GSE) for encoding it.
Equipping such an encoder with an excessive
number of parameters for each agent in CMAS
would be computation-consuming. We address
this problem in two steps. First, we tie weights
of GSE across the three agents. Second, in-
stead of encoding previous sentences from scratch,
GSE dynamically encodes GSn based on GSn−1.
Specifically, we propose a single layer LSTM with
soft-attention (Xu et al., 2015) as GSE. It takes a
multi-modal context vector ctxn ∈ RH as input,



6573

which is obtained by jointly embedding sentence
sn−1 and image I to a hidden space of dimension
H , and then generates the global hidden state vec-
tor gsn ∈ RH for the n-th loop by:

gsn = LSTM(gsn−1, ctxn) (1)

We adopt a visual attention module for produc-
ing context vector ctxn, given its capability of
capturing the correlation between languages and
images (Lu et al., 2017; Xu et al., 2015). The in-
puts to the attention module are visual feature vec-
tors {vp}Pp=1 ∈ RC and local state vector lsn−1
of sentence sn−1. Here, {vp}Pp=1 are extracted
from an intermediate layer of a CNN, C and p are
the number of channels and the position index of
vp. lsn−1 is the final hidden state of a writer (de-
fined in section 4.2.3). Formally, the context vec-
tor ctxn is computed by the following equations:

hp = tanh(Wh[lsn−1;gsn−1]) (2)

αp =
exp(Watthp)∑P
q=1 exp(Watthq)

(3)

vatt =
P∑
p=1

αpvp (4)

ctxn = tanh(Wctx[vatt; lsn−1]) (5)

where Wh, Watt and Wctx are parameter matri-
ces; {αp}Pp=1 are weights for visual features; and
[; ] denotes concatenation operation.

At the beginning of the generation process, the
global state is GS1 = (I). Let v̄ = 1P

∑P
i=1 vi,

the initial global state gs0 and cell state c0 are
computed by two single-layer neural networks:

gs0 = tanh(Wgsv̄) (6)

c0 = tanh(Wcv̄) (7)

where Wgs and Wc are parameter matrices.

4.2.2 Planner
After examining an area, Planner (PL) determines:
1) whether to terminate the generation process; 2)
which writer should generate the next sentence.
Specifically, besides the shared Global State En-
coder (GSE), the rest part of PL is modeled by a
two-layer feed-forward network:

hn = tanh(W2 tanh(W1gsn)) (8)

idxn = argmax(softmax(W3hn)) (9)

where W1, W2, and W3 are parameter matrices;
idxn ∈ {0, 1, 2} denotes the indicator, where 0
is for STOP, 1 for NW and 2 for AW. Namely, if
idxn = 0, the system will be terminated; else, NW
(idxn = 1) or AW (idxn = 2) will generate the
next sentence sn.

4.2.3 Writers
The number of normal sentences is usually 4-12
times to the number of abnormal sentences for
each report. With this highly unbalanced distribu-
tion, using only one decoder to model all of the
sentences would make the generation of normal
sentences dominant. To solve this problem, we de-
sign two writers, i.e., Normality Writer (NW) and
Abnormality Writer (AW), to model normal and
abnormal sentences. Practically, the architectures
of NW and AW can be different. In our practice,
we adopt a single-layer LSTM for both NW and
AW given the principle of parsimony.

Given a global state vector gsn, CMAS first
chooses a writer for generating a sentence based
on idxn. The chosen writer will re-initialize its
memory by taking gsn and a special token BOS
(Begin of Sentence) as its first two inputs. The
procedure for generating words is:

ht = LSTM(ht−1,Weywt−1) (10)

pt = softmax(Woutht) (11)

wt = argmax(pt) (12)

where ywt−1 is the one-hot encoding vector of
word wt−1; ht−1,ht ∈ RH are hidden states of
LSTM; We is the word embedding matrix and
Wout is a parameter matrix. pt gives the output
probability score over the vocabulary.

Upon the completion of the procedure (either
token EOS (End of Sentence) is produced or the
maximum time step T is reached), the last hidden
state of LSTM will be used as local state vector
lsn, which will be fed into GSE for generating
next global state vector GSn+1.

4.3 Reward Module

We use BLEU-4 (Papineni et al., 2002) to design
rewards for all agents in CMAS. A generated para-
graph is a collection (sab, snr) of normal sentences
snr = {snr1 , . . . , snrNnr} and abnormal sentences
sab = {sab1 , . . . , sabNab}, where Nab and Nnr are
the number of abnormal sentences and the number
of normal sentences, respectively. Similarly, the



6574

ground truth paragraph corresponding to the gen-
erated paragraph (sab, snr) is (s∗ab, s∗nr).

We compute BLEU-4 scores separately for ab-
normal and normal sentences. For the first n gen-
erated abnormal and normal sentences, we have:

f(sabn ) = BLEU({sab1 , · · · , sabn }, s∗ab) (13)
f(snrn ) = BLEU({snr1 , · · · , snrn }, s∗nr) (14)

Then, the immediate reward for sn (sabn or s
nr
n )

is r(sn) = f(sn) − f(sn−1). Finally, the dis-
counted reward for sn is defined as:

R(sn) =

∞∑
i=0

γir(sn+i) (15)

where γ ∈ [0, 1] denotes discounted factor, and
r(s1) = BLEU({s1}, s∗).

4.4 Learning
4.4.1 Reinforcement Learning
Given an input image I , three agents (PL, NW
and AW) in CMAS work simultaneously to gener-
ate a paragraph s = {s1, s2, . . . , sN} with the joint
goal of maximizing the discounted reward R(sn)
(Equation 15) for each sentence sn.

The loss of a paragraph s is negative expected
reward:

L(θ) = −En,sn∼πθ [R(sn)] (16)

where πθ denotes the entire policy network of
CMAS. Following the standard REINFORCE al-
gorithm (Williams, 1992), the gradient for the ex-
pectation En,sn∼πθ [R(sn)] in Equation 16 can be
written as:

∇θL(θ) = En,sn∼πθ [R(sn)∇θ−log πθ(sn, idxn)]
(17)

where − log πθ(sn, idxn) is joint negative log-
likelihood of sentence sn and its indicator idxn,
and it can be decomposed as:

− log πθ(sn, idxn)
=1{idxn=AW}LAW + 1{idxn=NW}LNW + LPL

=− 1{idxn=AW}
T∑
t=1

log pAW (wn,t)

− 1{idxn=NW}
T∑
t=1

log pNW (wn,t)

− log pPL(idxn)
(18)

where LAW , LNW and LPL are negative log-
likelihoods; pAW , pNW and pPL are probabilities
of taking an action; 1 denotes indicator function.

Therefore, Equation 17 can be re-written as:

∇θL(θ) = En,sn∼πθ [R(sn)(1{idxn=AW}∇LAW
+ 1{idxn=NW}∇LNW +∇LPL)]

(19)

4.4.2 Imitation Learning
It is very hard to train agents using reinforcement
learning from scratch, therefore a good initializa-
tion for policy network is usually required (Bah-
danau et al., 2016; Silver et al., 2016; Wang et al.,
2018b). We apply imitation learning with cross-
entropy loss to pre-train the policy network. For-
mally, the cross-entropy loss is defined as:

LCE(θ) = −λPL
N∑
n=1

{log pPL(idx∗n)}

−λNW
N∑
n=1

{1{idx∗n=NW}
T∑
t=1

log pNW (w
∗
n,t)}

−λAW
N∑
n=1

{1{idx∗n=AW}
T∑
t=1

log pAW (w
∗
n,t)}

(20)

where w∗ and idx∗ denote ground-truth word and
indicator respectively; λPL, λNW and λAW are
balancing coefficients among agents; N and T are
the number of sentences and the number of words
within a sentence, respectively.

4.5 CMAS for Impression

Different from the Findings module, the inputs
of the Impression module not only contain im-
ages I but also the generated findings f =
{f1, f2, . . . , fNf }, where Nf is the total number
of sentences. Thus, for the Impression module, the
n-th global state becomes GSn = (I, f , {si}n−1i=1 ).
The rest part of CMAS for the Impression mod-
ule is exactly the same as CMAS for the Findings
module. To encode f , we extend the definition of
multi-modal context vector ctxn (Equation 5) to:

ctxn = tanh(Wctx[vatt; fatt; lsn−1]) (21)

where fatt is the soft attention (Bahdanau et al.,
2014; Xu et al., 2015) vector, which is obtained
similar as vatt (Equation 3 and 4).



6575

5 Experiments

5.1 Datasets
IU-Xray Indiana University Chest X-Ray Col-
lection (Demner-Fushman et al., 2015) is a public
dataset containing 3,955 fully de-identified radi-
ology reports collected from the Indiana Network
for Patient Care, each of which is associated with a
frontal and/or lateral chest X-ray images, and there
are 7,470 chest X-ray images in total. Each re-
port is comprised of several sections: Impression,
Findings and Indication etc. We preprocess the re-
ports by tokenizing, converting tokens into lower-
cases and removing non-alpha tokens.

CX-CHR CX-CHR (Li et al., 2018) is a pro-
prietary internal dataset, which is a Chinese chest
X-ray report dataset collected from a professional
medical examination institution. This dataset con-
tains examination records for 35,500 unique pa-
tients, each of which consists of one or multiple
chest X-ray images as well as a textual report writ-
ten by professional radiologists. Each textual re-
port has sections such as Complain, Findings and
Impression. The textual reports are preprocessed
through tokenizing by “jieba”1, a Chinese text seg-
mentation tool, and filtering rare tokens.

5.2 Experimental Setup
Abnormality Term Extraction Human experts
helped manually design patterns for most frequent
medical abnormality terms in the datasets. These
patterns are used for labeling abnormality and nor-
mality of sentences, and also for evaluating mod-
els’ ability to detect abnormality terms. The ab-
normality terms in Findings and Impression are
different to some degree. This is because many ab-
normality terms in Findings are descriptions rather
than specific disease names. For examples, “low
lung volumes” and “thoracic degenerative” usu-
ally appear in Findings but not in Impression.

Evaluation Metrics We evaluate our proposed
method and baseline methods on: BLEU (Pa-
pineni et al., 2002), ROUGE (Lin, 2004) and
CIDEr (Vedantam et al., 2015). The results based
on these metrics are obtained by the standard im-
age captioning evaluation tool2. We also calculate
precision and average False Positive Rate (FPR)
for abnormality detection in generated textual re-
ports on both datasets.

1https://github.com/fxsjy/jieba.
2https://github.com/tylin/coco-caption

Implementation Details The dimensions of all
hidden states in Abnormality Writer, Normality
Writer, Planner and shared Global State Encoder
are set to 512. The dimension of word embedding
is also set as 512.

We adopt ResNet-50 (He et al., 2016) as im-
age encoder, and visual features are extracted from
its last convolutional layer, which yields a 7 ×
7 × 2048 feature map. The image encoder is pre-
trained on ImageNet (Deng et al., 2009)). For the
IU-Xray dataset, the image encoder is fine-tuned
on ChestX-ray14 dataset (Wang et al., 2017), since
the IU-Xray dataset is too small. For the CX-
CHR dataset, the image encoder is fine-tuned on
its training set. The weights of the image encoder
are then fixed for the rest of the training process.

During the imitation learning stage, the cross-
entropy loss (Equation 20) is adopted for all of the
agents, where λPL, λAW and λNW are set as 1.0.
We use Adam optimizer (Kingma and Ba, 2014)
with a learning rate of 5× 10−4 for both datasets.
During the reinforcement learning stage, the gra-
dients of weights are calculated based on Equa-
tion 19. We also adopt Adam optimizer for both
datasets and the learning rate is fixed as 10−6.

Comparison Methods For Findings section, we
compare our proposed method with state-of-the-
art methods for CXR imaging report generation:
CoAtt (Jing et al., 2018) and HGRG-Agent (Li
et al., 2018), as well as several state-of-the-art im-
age captioning models: CNN-RNN (Vinyals et al.,
2015), LRCN (Donahue et al., 2015), AdaAtt (Lu
et al., 2017), Att2in (Rennie et al., 2017). In ad-
dition, we implement several ablated versions of
the proposed CMAS to evaluate different compo-
nents in it: CMASW is a single agent system con-
taining only one writer, but it is trained on both
normal and abnormal findings. CMASNW,AW is a
simple concatenation of two single agent systems
CMASNW and CMASAW, which are respectively
trained on only normal findings and only abnormal
findings. Finally, we show CMAS’s performances
with imitation learning (CMAS-IL) and reinforce-
ment learning (CMAS-RL).

For Impression section, we compare our method
with Xu et al. (2015): SoftAttvision and SoftAtttext,
which are trained with visual input only (no find-
ings) and textual input only (no images). We also
report CMAS trained only on visual and textual
input: CMAStext and CMASvision. Finally, we also
compare CMAS-IL with CMAS-RL.



6576

Dataset Methods BLEU-1 BLEU-2 BLEU-3 BLEU-4 ROUGE CIDEr

CX-CHR

CNN-RNN (Vinyals et al., 2015) 0.590 0.506 0.450 0.411 0.577 1.580
LRCN (Donahue et al., 2015) 0.593 0.508 0.452 0.413 0.577 1.588
AdaAtt (Lu et al., 2017) 0.588 0.503 0.446 0.409 0.575 1.568
Att2in (Rennie et al., 2017) 0.587 0.503 0.446 0.408 0.576 1.566
CoAtt (Jing et al., 2018) 0.651 0.568 0.521 0.469 0.602 2.532
HGRG-Agent (Li et al., 2018) 0.673 0.587 0.530 0.486 0.612 2.895
CMASW 0.659 0.585 0.534 0.497 0.627 2.564
CMASNW,AW 0.657 0.579 0.522 0.479 0.585 1.532
CMAS-IL 0.663 0.592 0.543 0.507 0.628 2.475
CMAS-RL 0.693 0.626 0.580 0.545 0.661 2.900

IU-Xray

CNN-RNN (Vinyals et al., 2015) 0.216 0.124 0.087 0.066 0.306 0.294
LRCN (Donahue et al., 2015) 0.223 0.128 0.089 0.067 0.305 0.284
AdaAtt (Lu et al., 2017) 0.220 0.127 0.089 0.068 0.308 0.295
Att2in (Rennie et al., 2017) 0.224 0.129 0.089 0.068 0.308 0.297
CoAtt (Jing et al., 2018) 0.455 0.288 0.205 0.154 0.369 0.277
HGRG-Agent (Li et al., 2018) 0.438 0.298 0.208 0.151 0.322 0.343
CMASW 0.440 0.292 0.204 0.147 0.365 0.252
CMASNW,AW 0.451 0.286 0.199 0.146 0.366 0.269
CMAS-IL 0.454 0.283 0.195 0.143 0.353 0.266
CMAS-RL 0.464 0.301 0.210 0.154 0.362 0.275

Table 1: Main results for findings generation on the CX-CHR (upper) and IU-Xray (lower) datasets. BLEU-n
denotes the BLEU score that uses up to n-grams.

Dataset Methods BLEU-1 BLEU-2 BLEU-3 BLEU-4 ROUGE CIDEr

CX-CHR

SoftAtttext (Xu et al., 2015) 0.112 0.044 0.016 0.005 0.142 0.038
SoftAttvision (Xu et al., 2015) 0.408 0.300 0.247 0.208 0.466 0.932
CMAStext 0.182 0.141 0.127 0.119 0.356 2.162
CMASvision 0.415 0.357 0.323 0.296 0.511 3.124
CMAS-IL 0.426 0.360 0.322 0.290 0.504 3.080
CMAS-RL 0.428 0.361 0.323 0.290 0.504 2.968

IU-Xray

SoftAtttext (Xu et al., 2015) 0.179 0.047 0.006 0.000 0.161 0.032
SoftAttvision (Xu et al., 2015) 0.224 0.103 0.045 0.022 0.210 0.046
CMAStext 0.316 0.235 0.187 0.148 0.537 1.562
CMASvision 0.379 0.270 0.203 0.151 0.513 1.401
CMAS-IL 0.399 0.285 0.214 0.158 0.517 1.407
CMAS-RL 0.401 0.290 0.220 0.166 0.521 1.457

Table 2: Main results for impression generation on the CX-CHR (upper) and IU-Xray (lower) datasets. BLEU-n
denotes the BLEU score that uses up to n-grams.

5.3 Main Results

Comparison to State-of-the-art Table 1 shows
results on the automatic metrics for the Find-
ings module. On both datasets, CMAS outper-
forms all baseline methods on almost all metrics,
which indicates its overall efficacy for generat-
ing reports that resemble those written by human
experts. The methods can be divided into two
different groups: single sentence models (CNN-
RNN, LRCN, AdaAtt, Att2in) and hierarchical
models (CoAtt, HGRG-Agent, CMAS). Hierar-
chical models consistently outperform single sen-
tence models on both datasets, suggesting that the
hierarchical models are better for modeling para-
graphs. The leading performances of CMAS-IL
and CMAS-RL over the rest of hierarchical mod-
els demonstrate the validity of our practice in ex-
ploiting the structure information within sections.

Ablation Study CMASW has only one writer,
which is trained on both normal and abnormal
findings. Table 1 shows that CMASW can achieve
competitive performances to the state-of-the-art
methods. CMASNW, AW is a simple concatena-
tion of two single agent models CMASNW and
CMASAW, where CMASNW is trained only on
normal findings and CMASAW is trained only
on abnormal findings. At test time, the final
paragraph of CMASNW, AW is simply a concate-
nation of normal and abnormal findings gener-
ated by CMASNW and CMASAW respectively.
Surprisingly, CMASNW, AW performs worse than
CMASW on the CX-CHR dataset. We believe the
main reason is the missing communication proto-
col between the two agents, which could cause
conflicts when they take actions independently.
For example, for an image, NW might think “the



6577

Dataset CX-CHR IU-Xray
Methods Li et al. (2018) CMASNW,AW CMAS-IL CMAS-RL Li et al. (2018) CMASNW,AW CMAS-IL CMAS-RL
Precision 0.292 0.173 0.272 0.309 0.121 0.070 0.094 0.128

FPR 0.059 0.076 0.063 0.051 0.043 0.044 0.012 0.007

Table 3: Average precision and average False Positive Rate (FPR) for abnormality detection. (Findings)

Dataset CX-CHR IU-Xray
Methods CMAStext CMASvision CMAS-IL CMAS-RL CMAStext CMASvision CMAS-IL CMAS-RL
Precision 0.067 0.171 0.184 0.187 0.054 0.160 0.162 0.165

FPR 0.067 0.142 0.170 0.168 0.023 0.024 0.024 0.024

Table 4: Average precision and average False Positive Rate (FPR) for abnormality detection. (Impression)

heart size is normal”, while AW believes “the heart
is enlarged”. Such conflict would negatively af-
fect their joint performances. As evidently shown
in Table 1, CMAS-IL achieves higher scores than
CMASNW, AW, directly proving the importance of
communication between agents and thus the im-
portance of PL. Finally, it can be observed from
Table 1 that CMAS-RL consistently outperforms
CMAS-IL on all metrics, which demonstrates the
effectiveness of reinforcement learning.

Impression Module As shown in Table 2,
CMASvision and CMAStext have higher scores than
SoftAttvision and SoftAtttext, indicating the effec-
tiveness of CMAS. It can also be observed from
Table 2 that images provide better information
than text, since CMASvision and SoftAttvision ex-
ceed the scores of CMAStext and SoftAtttext to a
large margin on most of the metrics. However, fur-
ther comparison among CMAS-IL, CMAStext and
CMASvision shows that text information can help
improve the model’s performance to some degree.

5.4 Abnormality Detection

The automatic evaluation metrics (e.g. BLEU)
are based on n-gram similarity between the gener-
ated sentences and the ground-truth sentences. A
model can easily obtain high scores on these au-
tomatic evaluation metrics by generating normal
findings (Jing et al., 2018). To better understand
CMAS’s ability in detecting abnormalities, we re-
port its precision and average False Positive Rate
(FPR) for abnormality term detection in Table 3
and Table 4. Table 3 shows that CMAS-RL ob-
tains the highest precision and the lowest average
FPR on both datasets, indicating the advantage of
CMAS-RL for detecting abnormalities. Table 4
shows that CMAS-RL achieves the highest preci-
sion scores, but not the lowest FPR. However, FPR
can be lowered by simply generating normal sen-
tences, which is exactly the behavior of CMAStext.

5.5 Qualitative Analysis

In this section, we evaluate the overall quality of
generated reports through several examples. Fig-
ure 4 presents 5 reports generated by CMAS-RL
and CMASW, where the top 4 images contain
abnormalities and the bottom image is a normal
case. It can be observed from the top 4 examples
that the reports generated by CMAS-RL success-
fully detect the major abnormalities, such as “car-
diomegaly”, “low lung volumes” and “calcified
granulomas”. However, CMAS-RL might miss
secondary abnormalities sometimes. For instance,
in the third example, the “right lower lobe” is
wrongly-written as “right upper lobe” by CMAS-
RL. We find that both CMAS-RL and CMASW
are capable of producing accurate normal findings
since the generated reports highly resemble those
written by radiologists (as shown in the last exam-
ple in Figure 4). Additionally, CMASW tends to
produce normal findings, which results from the
overwhelming normal findings in the dataset.

5.6 Template Learning

Radiologists tend to use reference templates when
writing reports, especially for normal findings.
Manually designing a template database can be
costly and time-consuming. By comparing the
most frequently generated sentences by CMAS
with the most used template sentences in the
ground-truth reports, we show that the Normal-
ity Writer (NW) in the proposed CMAS is capable
of learning these templates automatically. Several
most frequently used template sentences (Li et al.,
2018) in the IU-Xray dataset are shown in Table 5.
The top 10 template sentences generated by NW
are presented in Table 6. In general, the templates
sentences generated by NW are similar to those
top templates in ground-truth reports.



6578

Figure 4: Examples of findings generated by CMAS-RL and CMASW on IU-Xray dataset, along with their corre-
sponding CXR images and ground-truth reports. Highlighted sentences are abnormal findings.

The lungs are clear.
Lungs are clear.
The lung are clear bilaterally.
No pneumothorax or pleural effusion.
No pleural effusion or pneumothorax.
There is no pleural effusion or pneumothorax.
No evidence of focal consolidation, pneumothorax, or pleural effusion.
No focal consolidation, pneumothorax or large pleural effusion.
No focal consolidation, pleural effusion, or pneumothorax identified..

Table 5: Most commonly used templates in IU-Xray.
Template sentences are clustered by their topics.

6 Conclusion

In this paper, we proposed a novel framework for
accurately generating chest X-ray imaging reports
by exploiting the structure information in the re-
ports. We explicitly modeled the between-section
structure by a two-stage framework, and implicitly
captured the within-section structure with a novel
Co-operative Multi-Agent System (CMAS) com-

The lungs are clear.
The heart is normal in size.
Heart size is normal.
There is no acute bony abnormality.
There is no pleural effusion or pneumothorax.
There is no pneumothorax.
No pleural effusion or pneumothorax.
There is no focal air space effusion to suggest a areas.
No focal consolidation.
Trachea no evidence of focal consolidation pneumothorax or pneumothorax.

Table 6: Top 10 sentences generated by CMAS. The
sentences are clustered by their topics.

prising three agents: Planner (PL), Abnormality
Writer (AW) and Normality Writer (NW). The en-
tire system was trained with REINFORCE algo-
rithm. Extensive quantitative and qualitative ex-
periments demonstrated that the proposed CMAS
not only could generate meaningful and fluent re-
ports, but also could accurately describe the de-
tected abnormalities.



6579

References
Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu,

Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron
Courville, and Yoshua Bengio. 2016. An actor-critic
algorithm for sequence prediction. ICLR.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
learning to align and translate. arXiv preprint
arXiv:1409.0473.

Dina Demner-Fushman, Marc D Kohli, Marc B Rosen-
man, Sonya E Shooshan, Laritza Rodriguez, Sameer
Antani, George R Thoma, and Clement J McDon-
ald. 2015. Preparing a collection of radiology ex-
aminations for distribution and retrieval. Journal
of the American Medical Informatics Association,
23(2):304–310.

Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. 2009. Imagenet: A large-scale hi-
erarchical image database. In Computer Vision and
Pattern Recognition, 2009. CVPR 2009. IEEE Con-
ference on, pages 248–255. Ieee.

Jeffrey Donahue, Lisa Anne Hendricks, Sergio Guadar-
rama, Marcus Rohrbach, Subhashini Venugopalan,
Kate Saenko, and Trevor Darrell. 2015. Long-term
recurrent convolutional networks for visual recogni-
tion and description. In Proceedings of the IEEE
conference on computer vision and pattern recogni-
tion, pages 2625–2634.

Jakob Foerster, Ioannis Alexandros Assael, Nando
de Freitas, and Shimon Whiteson. 2016. Learning to
communicate with deep multi-agent reinforcement
learning. In Advances in Neural Information Pro-
cessing Systems, pages 2137–2145.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2016. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pages 770–
778.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Baoyu Jing, Pengtao Xie, and Eric Xing. 2018. On the
automatic generation of medical imaging reports. In
56th Annual Meeting of Computational Linguistics
(ACL), pages 2577–2586.

Justin Johnson, Andrej Karpathy, and Li Fei-Fei. 2016.
Densecap: Fully convolutional localization net-
works for dense captioning. In Proceedings of the
IEEE Conference on Computer Vision and Pattern
Recognition, pages 4565–4574.

Andrej Karpathy and Li Fei-Fei. 2015. Deep visual-
semantic alignments for generating image descrip-
tions. In Proceedings of the IEEE conference
on computer vision and pattern recognition, pages
3128–3137.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Jonathan Krause, Justin Johnson, Ranjay Krishna, and
Li Fei-Fei. 2017. A hierarchical approach for gen-
erating descriptive image paragraphs. In Computer
Vision and Pattern Recognition (CVPR), 2017 IEEE
Conference on, pages 3337–3345. IEEE.

Christy Y Li, Xiaodan Liang, Zhiting Hu, and Eric P
Xing. 2018. Hybrid retrieval-generation reinforced
agent for medical image report generation. In Con-
ference on Neural Information Processing Systems
(NeurIPS).

Xiaodan Liang, Zhiting Hu, Hao Zhang, Chuang Gan,
and Eric P Xing. 2017. Recurrent topic-transition
gan for visual paragraph generation. arXiv preprint
arXiv:1703.07022.

Chin-Yew Lin. 2004. Rouge: A package for auto-
matic evaluation of summaries. Text Summarization
Branches Out.

Siqi Liu, Zhenhai Zhu, Ning Ye, Sergio Guadarrama,
and Kevin Murphy. 2017. Improved image caption-
ing via policy gradient optimization of spider. In
Proc. IEEE Int. Conf. Comp. Vis, volume 3, page 3.

Jiasen Lu, Caiming Xiong, Devi Parikh, and Richard
Socher. 2017. Knowing when to look: Adaptive at-
tention via a visual sentinel for image captioning. In
Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), volume 6,
page 2.

Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, Zhiheng
Huang, and Alan Yuille. 2014. Deep captioning
with multimodal recurrent neural networks (m-rnn).
arXiv preprint arXiv:1412.6632.

Volodymyr Mnih, Koray Kavukcuoglu, David Sil-
ver, Alex Graves, Ioannis Antonoglou, Daan Wier-
stra, and Martin Riedmiller. 2013. Playing atari
with deep reinforcement learning. arXiv preprint
arXiv:1312.5602.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of
the 40th annual meeting on association for compu-
tational linguistics, pages 311–318. Association for
Computational Linguistics.

Ramakanth Pasunuru and Mohit Bansal. 2017. Multi-
task video captioning with video and entailment
generation. In Proceedings of the 55th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), volume 1, pages 1273–
1283.

Zhou Ren, Xiaoyu Wang, Ning Zhang, Xutao Lv, and
Li-Jia Li. 2017. Deep reinforcement learning-based
image captioning with embedding reward. In Pro-
ceedings of the IEEE Conference on Computer Vi-
sion and Pattern Recognition, pages 290–298.



6580

Steven J Rennie, Etienne Marcheret, Youssef Mroueh,
Jarret Ross, and Vaibhava Goel. 2017. Self-critical
sequence training for image captioning. In CVPR,
volume 1, page 3.

Hoo-Chang Shin, Kirk Roberts, Le Lu, Dina Demner-
Fushman, Jianhua Yao, and Ronald M Summers.
2016. Learning to read chest x-rays: Recurrent neu-
ral cascade model for automated image annotation.
In Proceedings of the IEEE conference on computer
vision and pattern recognition, pages 2497–2506.

David Silver, Aja Huang, Chris J Maddison, Arthur
Guez, Laurent Sifre, George Van Den Driessche, Ju-
lian Schrittwieser, Ioannis Antonoglou, Veda Pan-
neershelvam, Marc Lanctot, et al. 2016. Mastering
the game of go with deep neural networks and tree
search. nature, 529(7587):484.

Sainbayar Sukhbaatar, Rob Fergus, et al. 2016. Learn-
ing multiagent communication with backpropaga-
tion. In Advances in Neural Information Processing
Systems, pages 2244–2252.

Ardi Tampuu, Tambet Matiisen, Dorian Kodelja, Ilya
Kuzovkin, Kristjan Korjus, Juhan Aru, Jaan Aru,
and Raul Vicente. 2017. Multiagent cooperation and
competition with deep reinforcement learning. PloS
one, 12(4):e0172395.

Ming Tan. 1993. Multi-agent reinforcement learning:
Independent vs. cooperative agents. In Proceedings
of the Tenth International Conference on Machine
Learning (ICML).

Ramakrishna Vedantam, C Lawrence Zitnick, and Devi
Parikh. 2015. Cider: Consensus-based image de-
scription evaluation. In Proceedings of the IEEE
conference on computer vision and pattern recog-
nition, pages 4566–4575.

Oriol Vinyals, Alexander Toshev, Samy Bengio, and
Dumitru Erhan. 2015. Show and tell: A neural im-
age caption generator. In Proceedings of the IEEE
conference on computer vision and pattern recogni-
tion, pages 3156–3164.

Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu,
Mohammadhadi Bagheri, and Ronald M Summers.
2017. Chestx-ray8: Hospital-scale chest x-ray
database and benchmarks on weakly-supervised
classification and localization of common thorax
diseases. In Computer Vision and Pattern Recog-
nition (CVPR), 2017 IEEE Conference on, pages
3462–3471. IEEE.

Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, and
Ronald M Summers. 2018a. Tienet: Text-image em-
bedding network for common thorax disease classi-
fication and reporting in chest x-rays. In Proceed-
ings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 9049–9058.

Xin Wang, Wenhu Chen, Jiawei Wu, Yuan-Fang Wang,
and William Yang Wang. 2018b. Video captioning

via hierarchical reinforcement learning. In Proceed-
ings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 4213–4222.

Ronald J Williams. 1992. Simple statistical gradient-
following algorithms for connectionist reinforce-
ment learning. Machine learning, 8(3-4):229–256.

Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho,
Aaron Courville, Ruslan Salakhudinov, Rich Zemel,
and Yoshua Bengio. 2015. Show, attend and tell:
Neural image caption generation with visual atten-
tion. In International conference on machine learn-
ing, pages 2048–2057.

Quanzeng You, Hailin Jin, Zhaowen Wang, Chen Fang,
and Jiebo Luo. 2016. Image captioning with seman-
tic attention. In Proceedings of the IEEE conference
on computer vision and pattern recognition, pages
4651–4659.

Haonan Yu, Jiang Wang, Zhiheng Huang, Yi Yang, and
Wei Xu. 2016. Video paragraph captioning using
hierarchical recurrent neural networks. In Proceed-
ings of the IEEE conference on computer vision and
pattern recognition, pages 4584–4593.

Kaiqing Zhang, Zhuoran Yang, Han Liu, Tong Zhang,
and Tamer Başar. 2018. Fully decentralized multi-
agent reinforcement learning with networked agents.
Proceedings of the Tenth International Conference
on Machine Learning (ICML).


