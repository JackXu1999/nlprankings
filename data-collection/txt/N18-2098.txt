



















































A Mixed Hierarchical Attention Based Encoder-Decoder Approach for Standard Table Summarization


Proceedings of NAACL-HLT 2018, pages 622–627
New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics

A Mixed Hierarchical Attention based Encoder-Decoder Approach for
Standard Table Summarization

Parag Jain† Anirban Laha†∗ Karthik Sankaranarayanan†
Preksha Nema∗ Mitesh M. Khapra∗‡ Shreyas Shetty∗

†IBM Research ∗IIT Madras, India
‡ Robert Bosch Center for Data Science and Artificial Intelligence, IIT Madras

{pajain34,anirlaha,kartsank}@in.ibm.com
{preksha,miteshk,shshett}@cse.iitm.ac.in

Abstract

Structured data summarization involves gen-
eration of natural language summaries from
structured input data. In this work, we con-
sider summarizing structured data occurring in
the form of tables as they are prevalent across
a wide variety of domains. We formulate the
standard table summarization problem, which
deals with tables conforming to a single pre-
defined schema. To this end, we propose a
mixed hierarchical attention based encoder-
decoder model which is able to leverage the
structure in addition to the content of the ta-
bles. Our experiments on the publicly avail-
able WEATHERGOV dataset show around 18
BLEU (∼ 30%) improvement over the current
state-of-the-art.

1 Introduction

Abstractive summarization techniques from struc-
tured data seek to exploit both structure and con-
tent of the input data. The type of structure on
the input side can be highly varied ranging from
key-value pairs (e.g. WIKIBIO (Lebret et al.,
2016)), source code (Iyer et al., 2016), ontolo-
gies (Androutsopoulos et al., 2014; Colin et al.,
2016), or tables (Wiseman et al., 2017), each of
which require significantly varying approaches.
In this paper, we focus on generating summaries
from tabular data. Now, in most practical ap-
plications such as finance, healthcare or weather,
data in a table are arranged in rows and columns
where the schema is known beforehand. How-
ever, change in the actual data values can neces-
sitate drastically different output summaries. Ex-
amples shown in the figure 1 have a predefined
schema obtained from the WEATHERGOV dataset
(Liang et al., 2009) and its corresponding weather
report summary. Therefore, the problem that we
seek to address in this paper is to generate ab-
stractive summaries of tables conforming to a pre-

defined fixed schema (as opposed to cases where
the schema is unknown). We refer to this set-
ting as standard table summarization problem.
Another problem that could be formulated is one
in which the output summary is generated from
multiple tables as proposed in a recent challenge
(Wiseman et al., 2017) (this setting is out of the
scope of this paper). Now, as the schema is fixed,
simple rule based techniques (Konstas and Lapata,
2013) or template based solutions could be em-
ployed. However, due to the vast space of selec-
tion (which attributes to use in the summary based
on the current value it takes) and generation (how
to express these selected attributes in natural lan-
guage) choices possible, such approaches are not
scalable in terms of the number of templates as
they demand hand-crafted rules for both selection
and generation.

We attempt to solve the problem of standard
table summarization by leveraging the hierarchi-
cal nature of fixed-schema tables. In other words,
rows consist of a fixed set of attributes and a ta-
ble is defined by a set of rows. We cast this prob-
lem into a mixed hierarchical attention model
following the encode-attend-decode (Cho et al.,
2015) paradigm. In this approach, there is static
attention on the attributes to compute the row
representation followed by dynamic attention on
the rows, which is subsequently fed to the de-
coder. This formulation is theoretically more effi-
cient than the fully dynamic hierarchical attention
framework followed by Nallapati et al. (2016).
Also, our model does not need sophisticated sam-
pling or sparsifying techniques like (Ling and
Rush, 2017; Deng et al., 2017), thus, retaining
differentiability. To demonstrate the efficacy of
our approach, we transform the publicly avail-
able WEATHERGOV dataset (Liang et al., 2009)
into fixed-schema tables, which is then used for
our experiments. Our proposed mixed hierarchi-

622



Figure 1: Standard Table Summarization with fixed schema tables as input.

cal attention model provides an improvement of
around 18 BLEU (around 30%) over the cur-
rent state-of-the-art result by Mei et al. (2016).

2 Tabular Data Summarization

A standard table consist of set of records (or
rows) R = (r1, r2, ...rT ) and each record r
has a fixed set of attributes (or columns) Ar =
(ar1, ar2, ...arM ). Tables in figure 1 have 7
columns (apart from ‘TYPE’) which correspond to
different attributes. Also U = (u1, u2, ...uT ) rep-
resents the type of each record where uk is one-
hot encoding for the record type for record rk.
Training data consists of instance pairs (Xi, Yi)
for i = 1, 2, ..n, where Xi = (Ri, Ui) represents
the input table and Yi = (y1, ..., yT ′) represents
the corresponding natural language summary. In
this paper, we propose an end-to-end model which
takes in a table instance X to produce the output
summary Y . This can be derived by solving in Y
the following conditional probability objective:

Y ∗ = arg max
Y

T ′∏

t=1

p(yt|y1, ..., yt−1, X) (1)

2.1 Mixed Hierarchical Attention Model
(MHAM)

Our model is based on the encode-attend-decode
paradigm as defined by Cho et al. (2015). It con-
sists of an encoder RNN which encodes a variable
length input sequence x = (x1, ..., xT ) into a rep-
resentation sequence c = (c1, ..., cT ). Another de-
coder RNN generates sequence of output symbols

Figure 2: Proposed Architecture

y = (y1, ..., yT ′), attending to different combina-
tions of ci while generating different yt.

As illustrated in figure 2, our encoder is not a
single RNN. The encoder has a hierarchical struc-
ture to leverage the structural aspect of a stan-
dard table: a table consists of a set of records
(or rows) and each record consists of values cor-
responding to a fixed set of attributes. We call it
a mixed hierarchical attention based encoder, as
it incorporates static attention and dynamic atten-
tion at two different levels of the encoder. At the
record level, the attention over record representa-
tions is dynamic as it changes with each decoder
time step. Whereas at the attribute level, since
the schema is fixed, a record representation can
be computed without the need of varying attention
over attributes - thus static attention is used. For

623



example, with respect to WEATHERGOV dataset, a
temperature record will always be defined by the
attributes like min, max and mean irrespective of
the decoder time step. So, attention over attributes
can be static. On the other hand, while generating
a word by the decoder, there can be a preference
of focusing on a record type say, temperature, over
some other type say, windSpeed. Thus, dynamic
attention is used across records.

[!ht]

Ārj = WjA
r
j , ∀j ∈ [1,M ] (2)

Irj = Ā
r
j � ūr , ūr = W0ur (3)

αrj = softmaxj(I
r) (4)

Br =
∑

j

αrjĀ
r
j (5)

cr = [hr;B
r] , hr = GRU(B

r) (6)

gr = σ(q
T tanh(Pcr)) (7)

βrt = v
T tanh(Wsst−1 +Wccr) (8)

wrt = softmaxr(βt) (9)

zt =
∑

r

γrt cr , γ
r
t =

grw
r
t∑

r
grwrt

(10)

st = GRU(zt, st−1) (11)

lt = W1st +W2zt + bl (12)

pt = softmax(lt) (13)

Capturing attribute semantics: We learn
record type embeddings and use them to calculate
attentions over attributes. For the trivial case
of all records being same type, it boils down to
having a single record type embedding. Given
attributes Ar for a record r, where each attribute
ari is encoded into a vector A

r
i based on the

attribute type (discussed further in section 3),
using equation 2 we embed each attribute where
Wj is the embedding matrix for jth attribute. We
embed record type one-hot vector ur through W0,
which is used to compute the importance score Irj
for attribute j in record r according to equation 3.

Static Attribute attention: Not all attribute val-
ues contribute equally to the record. Hence, we
introduce attention weights for attributes of each
record. These attention weights are static and does
not change with decoder time step. We calculate
the attention probability vector αr over attributes
using the attribute importance vector Ir. The at-
tention weights can then be used to calculate the

record representation Br for record r by using
equations 4 and 5.

Record Encoder: A GRU based RNN encoder
takes as input a sequence of attribute attended
records B1:N and returns a sequence of hidden
states h1:N , where hr is the encoded vector for
record Br. We obtain the final record encoding
cr (equation 6) by concatenating the GRU hidden
states with the embedded record encodings Br.

Static Record attention: In a table, a subset
of record types can always be more salient com-
pared to other record types. This is captured
by learning a static set of weights over all the
records. These weights regulate the dynamic at-
tention weights computed during decoding at each
time step. Equation 7 performs this step where gr
is the static record attention weight for rth record
and q and P are weights to be learnt. We do not
have any constraints on static attention vector.

Dynamic Record attention for Decoder: Our
decoder is a GRU based decoder with dynamic at-
tention mechanism similar to (Mei et al., 2016)
with modifications to modulate attention weights
at each time step using static record attentions. At
each time step t attention weights are calculated
using 8, 9, 10, where γrt is the aggregated atten-
tion weight of record r at time step t. We use the
soft attention over input encoder sequences cr to
calculate the weighted average, which is passed to
the GRU. GRU hidden state st is used to calculate
output probabilities pt by using a softmax as de-
scribed by equation 11, 12, 13, which is then used
to get output word yt.

Due to the static attention at attribute level, the
time complexity of a single pass is O(TM +
TT ′), where T is the number of records, M is
the number of attributes and T ′ is the number
of decoder steps. In case of dynamic attention
at both levels (as in Nallapati et al. (2016)), the
time complexity is much higher O(TMT ′). Thus,
mixed hierarchical attention model is faster than
fully dynamic hierarchical attention. For bet-
ter understanding of the contribution of hierar-
chical attention(MHAM), we propose a simpler
non-hierarchical (NHM) architecture with atten-
tion only at record level. In NHM, Br is cal-
culated by concatenating all the record attributes
along with corresponding record type.

624



Input Table Generated Output
TYPE TIME MIN MAX MEAN MODE MB100-4 MB20-2
temperature 17-30 27 36 29 - - -
windChill 17-30 14 26 18 - - -
windSpeed 17-30 14 20 16 - - 10-20
windDir 17-30 - - - SSW - -
gust 17-30 0 0 0 - - -
skyCover 17-30 - - - - 75-100 -
skyCover 17-21 - - - - 75-100 -
skyCover 17-26 - - - - 75-100 -
skyCover 21-30 - - - - 75-100 -
skyCover 26-30 - - - - 75-100 -
precipPotential 17-30 26 58 43 - - -
rainChance 17-21 - - - Lkly - -
snowChance 17-30 - - - Chc - -
snowChance 17-26 - - - Lkly - -

Reference: Periods of rain and possibly a thunderstorm . Some of the storms could produce heavy rain .
Temperature rising to near 51 by 10am , then falling to around 44 during the remainder of the day . Breezy ,
with a north northwest wind between 10 and 20 mph . Chance of precipitation is 90 % . New rainfall amounts
between one and two inches possible .

NHM:Periods of rain and possibly a thunderstorm . Some of the storms could produce heavy rain . Temperature
rising to near 51 by 8am , then falling to around 6 during the remainder of the day . Breezy , with a north
northwest wind 10 to 15 mph increasing to between 20 and 25 mph . Chance of precipitation is 90 % . New
rainfall amounts between one and two inches possible .

MHAM: Periods of rain and possibly a thunderstorm . Some of the storms could produce heavy rain . Temper-
ature rising to near 51 by 8am , then falling to around 44 during the remainder of the day . Breezy , with a north
northwest wind between 10 and 20 mph . Chance of precipitation is 90 % . New rainfall amounts between one
and two inches possible.

TYPE TIME MIN MAX MEAN MODE MB100-4 MB20-2
temperature 17-30 27 36 29 - - -
windChill 17-30 14 26 18 - - -
windSpeed 17-30 14 20 16 - - 10-20
windDir 17-30 - - - SSW - -
gust 17-30 0 0 0 - - -
skyCover 17-30 - - - - 75-100 -
skyCover 17-21 - - - - 75-100 -
skyCover 17-26 - - - - 75-100 -
skyCover 21-30 - - - - 75-100 -
skyCover 26-30 - - - - 75-100 -
precipPotential 17-30 26 58 43 - - -
rainChance 17-21 - - - Lkly - -
snowChance 17-30 - - - Chc - -
snowChance 17-26 - - - Lkly - -

Reference: A chance of rain and snow . Snow level 5500 feet . Mostly cloudy , with a low around 31 . Calm
wind becoming north northeast around 6 mph . Chance of precipitation is 40% .

NHM: A chance of rain and snow . Mostly cloudy , with a low around 31 . North northwest wind at 6 mph
becoming east southeast . Chance of precipitation is 40% .

MHAM: A chance of rain and snow . Snow level 5800 feet lowering to 5300 feet after midnight . Mostly cloudy
, with a low around 31 . North northwest wind at 6 mph becoming south southwest . Chance of precipitation is
40% .

TYPE TIME MIN MAX MEAN MODE MB100-4 MB20-2
temperature 17-30 27 36 29 - - -
windChill 17-30 14 26 18 - - -
windSpeed 17-30 14 20 16 - - 10-20
windDir 17-30 - - - SSW - -
gust 17-30 0 0 0 - - -
skyCover 17-30 - - - - 75-100 -
skyCover 17-21 - - - - 75-100 -
skyCover 17-26 - - - - 75-100 -
skyCover 21-30 - - - - 75-100 -
skyCover 26-30 - - - - 75-100 -
precipPotential 17-30 26 58 43 - - -
rainChance 17-21 - - - Lkly - -
snowChance 17-30 - - - Chc - -
snowChance 17-26 - - - Lkly - -

Reference: Rain and snow likely , becoming all snow after 8pm . Cloudy , with a low around 22 . South
southwest wind around 15 mph . Chance of precipitation is 60% . New snow accumulation of less than one
inch possible .

NHM : Rain or freezing rain likely before 8pm , then snow after 11pm , snow showers and sleet likely before
8pm , then a chance of rain or freezing rain after 3am . Mostly cloudy , with a low around 27 . South south-
east wind between 15 and 17 mph . Chance of precipitation is 80% . Little or no ice accumulation expected .
Little or no snow accumulation expected .

MHAM: Snow , and freezing rain , snow after 9pm . Cloudy , with a steady temperature around 23 . Breezy ,
with a south wind between 15 and 20 mph . Chance of precipitation is 60% . New snow accumulation of around
an inch possible .

Table 1: Anecdotal example. Records which contain all null attributes are not shown in the example. MB100-4
and MB20-2 correspond to mode-bucket-0-100-4 & mode-bucket-0-20-2 resp. in the dataset.

3 Experiments

Dataset and methodology: To evaluate our model
we have used WEATHERGOV dataset (Liang et al.,
2009) which is the standard benchmark dataset to
evaluate tabular data summarization techniques.
We compared the performance of our model
against the state-of-the-art work of MBW (Mei
et al., 2016), as well as two other baseline mod-
els KL (Konstas and Lapata, 2013) and ALK
(Angeli et al., 2010). Dataset consists of a to-
tal of 29,528 tables (25000:1000:3528 ratio for
train:validation:test splits) corresponding to sce-
narios created by collecting weather forecasts for
3,753 cities in the U.S.A over three days. There
are 12 record types consisting of both numeric
and categorical values. Each table contains 36
weather records (e.g., temperature, wind direction
etc.) along with a corresponding natural language
summary.

Input Encodings: Attributes were encoded
based on the attribute type. Numbers are encoded
in binary representation. Record type is encoded
as a one-hot vector. Mode attribute is encoded us-
ing specific ordinal encodings for example ‘Lkly’,

‘SChc’, ‘Chc’ are encoded as ‘00100000000000’,
‘00010000000000’ and ‘00001000000000’ re-
spectively. Similar works for directions, for
example ‘NW’, ‘NNE’ and ‘NE’ are encoded
as ‘00000100000000’, ‘00000011000000’ and
‘00000001000000’ resp. Time interval were also
encoded as ordinal encodings, for example ‘6-21’
is encoded as ‘111100’ and ‘6-13’ is encoded as
‘110000’, the six bits corresponding to six atomic
time intervals available in the dataset. Other at-
tributes and words were encoded as one-hot vec-
tors.

3.1 Training and hyperparameter tuning

We used TensorFlow (Abadi et al., 2015) for our
experiments. Encoder embeddings were initial-
ized by generating the values from a uniform dis-
tribution in the range [-1, 1). Other variables
were initialized using Glorot uniform initialization
(Glorot and Bengio, 2010). We tune each hyper-
parameter by choosing parameter from a ranges of
values, and selected the model with best sBLEU
score in validation set over 500 epochs. We did not
use any regularization while training the model.
For both the models, the hyperparameter tuning

625



was separately performed to give both models a
fair chance of performance. For both the models,
Adam optimizer (Kingma and Ba, 2014) was used
with learning rate set to 0.0001. We found embed-
ding size of 100, GRU size of 400, static record
attention sizeP of 150 to work best for MHAM
model. We also experimented using bi-directional
GRU in the encoder but there was no significant
boost observed in the BLEU scores.

Evaluation metrics: To evaluate our models
we employed BLEU and Rouge-L scores. In addi-
tion to the standard BLEU (sBleu) (Papineni et al.,
2002), a customized BLEU (cBleu) (Mei et al.,
2016) has also been reported. cBleu does not pe-
nalize numbers which differ by at most five; hence
20 and 18 will be considered same.

4 Results and Analyses

Table 2 describes the results of our proposed mod-
els (MHAM and NHM) along with the afore-
mentioned baseline models. We observe a sig-
nificant performance improvement of 16.6 cBleu
score (24%) and 18.3 sBleu score (30%) compared
to the current state-of-the-art model of MBW.
MHAM also shows an improvement over NHM
in all metrics demonstrating the importance of hi-
erarchical attention.

Model sBleu cBleu Rouge-L
KL 36.54 - -
ALK 38.40 51.50 -
MBW 61.0 70.4 -
NHM 76.2 85.0 86.4
MHAM 79.3 87.0 88.5

Table 2: Overall results

Attention analysis: Analysis of figure 3 re-
veals that the learnt attention weights are reason-
able. For example, as shown in figure 3(a), for the
phrase ‘with a high near 52’, the model had a high
attention on temperature before and while gener-
ating the number ‘52’. Similarly while generating
‘mostly cloudy’, the model had a high attention
on precipitation potential. Attribute attentions are
also learned as expected (in figure 3(b)). The tem-
perature, wind speed and gust records have high
weights on min/max/mean values which describe
these records.

Qualitative analysis: Table 1 contains exam-
ple table-summary pairs, with summary generated
by the proposed hierarchical and non-hierarchical

Figure 3: Heatmaps: (a) record level attention (top) and
(b) attribute level attention (bottom).

versions. We observe that our model is able to gen-
erate numbers more accurately by enabling hierar-
chical attention. Our model is also able to capture
weak signals like snow accumulation. Further, our
proposed model MHAM is able to avoid repetition
as compared to NHM.

5 Conclusion and Future Work

In this work, we have formulated the problem of
standard table summarization where all the ta-
bles come from a predefined schema. Towards
this, we proposed a novel mixed hierarchical at-
tention based encoder-decoder approach. Our ex-
periments on the publicly available WEATHERGOV
benchmark dataset have shown significant im-
provements over the current state-of-the-art work.
Moreover, this proposed method is theoretically
more efficient compared to the current fully dy-
namic hierarchical attention model. As future
work, we propose to tackle general tabular sum-
marization where the schema can vary across ta-
bles in the whole dataset.

6 Acknowledgements

We would like to acknowledge Abhijit Mishra for
his valuable feedback and comments.

References
Martı́n Abadi, Ashish Agarwal, Paul Barham, Eugene

Brevdo, Zhifeng Chen, Craig Citro, Greg S. Cor-
rado, Andy Davis, Jeffrey Dean, Matthieu Devin,

626



Sanjay Ghemawat, Ian Goodfellow, Andrew Harp,
Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal
Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh
Levenberg, Dan Mané, Rajat Monga, Sherry Moore,
Derek Murray, Chris Olah, Mike Schuster, Jonathon
Shlens, Benoit Steiner, Ilya Sutskever, Kunal Tal-
war, Paul Tucker, Vincent Vanhoucke, Vijay Vasude-
van, Fernanda Viégas, Oriol Vinyals, Pete Warden,
Martin Wattenberg, Martin Wicke, Yuan Yu, and Xi-
aoqiang Zheng. 2015. TensorFlow: Large-scale ma-
chine learning on heterogeneous systems. Software
available from tensorflow.org.

Ion Androutsopoulos, Gerasimos Lampouras, and
Dimitrios Galanis. 2014. Generating natural lan-
guage descriptions from OWL ontologies: the nat-
uralowl system. CoRR, abs/1405.6164.

Gabor Angeli, Percy Liang, and Dan Klein. 2010. A
simple domain-independent probabilistic approach
to generation. In Proceedings of the 2010 Confer-
ence on Empirical Methods in Natural Language
Processing, pages 502–512. Association for Com-
putational Linguistics.

KyungHyun Cho, Aaron C. Courville, and Yoshua
Bengio. 2015. Describing multimedia content using
attention-based encoder-decoder networks. CoRR,
abs/1507.01053.

Emilie Colin, Claire Gardent, Yassine Mrabet, Shashi
Narayan, and Laura Perez-Beltrachini. 2016. The
webnlg challenge: Generating text from dbpedia
data. In INLG 2016 - Proceedings of the Ninth Inter-
national Natural Language Generation Conference,
September 5-8, 2016, Edinburgh, UK, pages 163–
167.

Yuntian Deng, Anssi Kanervisto, Jeffrey Ling, and
Alexander M. Rush. 2017. Image-to-markup gener-
ation with coarse-to-fine attention. In Proceedings
of the 34th International Conference on Machine
Learning, ICML 2017, Sydney, NSW, Australia, 6-11
August 2017, pages 980–989.

Xavier Glorot and Yoshua Bengio. 2010. Understand-
ing the difficulty of training deep feedforward neu-
ral networks. In In Proceedings of the International
Conference on Artificial Intelligence and Statistics
(AISTATS10). Society for Artificial Intelligence and
Statistics.

Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and
Luke Zettlemoyer. 2016. Summarizing source code
using a neural attention model. In Proceedings of
the 54th Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers),
pages 2073–2083, Berlin, Germany. Association for
Computational Linguistics.

Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Ioannis Konstas and Mirella Lapata. 2013. Inducing
document plans for concept-to-text generation. In
EMNLP.

Rémi Lebret, David Grangier, and Michael Auli. 2016.
Neural text generation from structured data with ap-
plication to the biography domain. In Proceed-
ings of the 2016 Conference on Empirical Methods
in Natural Language Processing, pages 1203–1213,
Austin, Texas. Association for Computational Lin-
guistics.

Percy Liang, Michael I. Jordan, and Dan Klein. 2009.
Learning semantic correspondences with less super-
vision. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Pro-
cessing of the AFNLP: Volume 1 - Volume 1, ACL
’09, pages 91–99, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.

Jeffrey Ling and Alexander M. Rush. 2017. Coarse-to-
fine attention models for document summarization.
In Proceedings of the Workshop on New Frontiers in
Summarization, NFiS@EMNLP 2017, Copenhagen,
Denmark, September 7, 2017, pages 33–42.

Hongyuan Mei, Mohit Bansal, and Matthew R. Walter.
2016. What to talk about and how? selective gener-
ation using lstms with coarse-to-fine alignment. In
Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 720–730, San Diego, California. Association
for Computational Linguistics.

Ramesh Nallapati, Bing Xiang, and Bowen Zhou.
2016. Sequence-to-sequence rnns for text summa-
rization. CoRR, abs/1602.06023.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of
the 40th annual meeting on association for compu-
tational linguistics, pages 311–318. Association for
Computational Linguistics.

Sam Wiseman, Stuart M. Shieber, and Alexander M.
Rush. 2017. Challenges in data-to-document gener-
ation. In Proceedings of the 2017 Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP 2017, Copenhagen, Denmark, Septem-
ber 9-11, 2017, pages 2253–2263.

627


