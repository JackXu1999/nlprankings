



















































Graph based Neural Networks for Event Factuality Prediction using Syntactic and Semantic Structures


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4393–4399
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

4393

Graph based Neural Networks for Event Factuality Prediction
using Syntactic and Semantic Structures

Amir Pouran Ben Veyseh, Thien Huu Nguyen and Dejing Dou
Department of Computer and Information Science

University of Oregon
Eugene, OR 97403, USA

{apouranb, thien, dou}@cs.uoregon.edu

Abstract
Event factuality prediction (EFP) is the task of
assessing the degree to which an event men-
tioned in a sentence has happened. For this
task, both syntactic and semantic information
are crucial to identify the important context
words. The previous work for EFP has only
combined these information in a simple way
that cannot fully exploit their coordination. In
this work, we introduce a novel graph-based
neural network for EFP that can integrate the
semantic and syntactic information more ef-
fectively. Our experiments demonstrate the
advantage of the proposed model for EFP.

1 Introduction

Events are often presented in sentences via the
indication of anchor/trigger words (i.e., the main
words to evoke the events, called event mentions)
(Nguyen et al., 2016a). Event mentions can ap-
pear with varying degrees of uncertainty/factuality
to reflect the intent of the writers. In order for
the event mentions to be useful (i.e., for knowl-
edge extraction tasks), it is important to determine
their factual certainty so the actual event mentions
can be retrieved (i.e., the event factuality predic-
tion problem (EFP)). In this work, we focus on the
recent regression formulation of EFP that aims to
predict a real score in the range of [-3,+3] to quan-
tify the occurrence possibility of a given event
mention (Stanovsky et al., 2017; Rudinger et al.,
2018). This provides more meaningful informa-
tion for the downstream tasks than the classifica-
tion formulation of EFP (Lee et al., 2015). For
instance, the word “left” in the sentence “She left
yesterday.” would express an event that certainly
happened (i.e., corresponding to a score of +3 in
the benchmark datasets) while the event mention
associated with “leave” in the sentence “She for-
got to leave yesterday.” would certainly not hap-
pen (i.e., a score of -3).

go

I will seeing

after treatment

the others

of

need

when I care

medical

back

Figure 1: The dependency tree of the sentence “I will,
after seeing the treatment of others, go back when I
need medical care.”.

EFP is a challenging problem as different con-
text words might jointly participate to reveal the
factuality of the event mentions (i.e., the cue
words), possibly located at different parts of the
sentences and scattered far away from the anchor
words of the events. There are two major mech-
anisms that can help the models to identify the
cue words and link them to the anchor words, i.e.,
the syntactic trees (i.e., the dependency trees) and
the semantic information (Rudinger et al., 2018).
For the syntactic trees, they can connect the an-
chor words to the functional words (i.e., nega-
tion, modal auxiliaries) that are far away, but con-
vey important information to affect the factuality
of the event mentions. For instance, the depen-
dency tree of the sentence “I will, after seeing the
treatment of others, go back when I need medi-
cal care.” will be helpful to directly link the an-
chor word “go” to the modal auxiliary “will” to
successfully predict the non-factuality of the event
mention. Regarding the semantic information, the
meaning of the some important context words in
the sentences can contribute significantly to the



4394

factuality of an event mention. For example, in
the sentence “Knight lied when he said I went to
the ranch.”, the meaning represented by the cue
word “lied” is crucial to classify the event men-
tion associated with the anchor word “went” as
non-factual. The meaning of such cue words and
their interactions with the anchor words can be
captured via their distributed representations (i.e.,
with word embeddings and long-short term mem-
ory networks (LSTM)) (Rudinger et al., 2018).

The current state-of-the-art approach for EFP
has involved deep learning models (Rudinger
et al., 2018) that examine both syntactic and se-
mantic information in the modeling process. How-
ever, in these models, the syntactic and seman-
tic information are only employed separately in
the different deep learning architectures to gener-
ate syntactic and semantic representations. Such
representations are only concatenated in the final
stage to perform the factuality prediction. A ma-
jor problem with this approach occurs in the event
mentions when the syntactic and semantic infor-
mation cannot identify the important structures for
EFP individually (i.e., by itself). In such cases,
both the syntactic and semantic representations
from the separate deep learning models would be
noisy and/or insufficient, causing the poor qual-
ity of their simple combination for EFP. For in-
stance, consider the previous example with the an-
chor word “go”: “I will, after seeing the treatment
of others, go back when I need medical care.”.
On the one hand, while syntactic information (i.e.,
the dependency tree) can directly connect “will”
to “go”, it will also promote some noisy words
(i.e., “back”) at the same time due to the direct
links (see the dependency tree in Figure 1). On
the other hand, while deep learning models with
the sequential structure can help to downgrade the
noisy words (i.e., “back”) based on the semantic
importance and the close distance with “go”, these
models will struggle to capture “will” for the fac-
tuality of “go” due to their long distance.

From this example, we also see that the syn-
tactic and semantic information can complement
each other to both promote the important con-
text words and blur the irrelevant words. Conse-
quently, we argue that the syntactic and semantic
information should be allowed to interact earlier
in the modeling process to produce more effective
representations for EFP. In particular, we propose
a novel method to integrate syntactic and seman-

tic structures of the sentences based on the graph
convolutional neural networks (GCN) (Kipf and
Welling, 2016) for EFP. The modeling of GCNs
involves affinity matrices to quantify the connec-
tion strength between pairs of words, thus facili-
tating the integration of syntactic and semantic in-
formation. In the proposed model, the semantic
affinity matrices of the sentences are induced from
Long Short-Term Memory networks (LSTM) that
are then linearly integrated with the syntactic affin-
ity matrices of the dependency trees to produce
the enriched affinity matrices for GCNs in EFP.
The extensive experiments show that the proposed
model is very effective for EFP.

2 Related Work

EFP is one of the fundamental tasks in Informa-
tion Extraction. The early work on this problem
has employed the rule-based approaches (Nairn
et al., 2006; Saurı́, 2008; Lotan et al., 2013) or
the machine learning approaches (with manually
designed features) (Diab et al., 2009; Prabhakaran
et al., 2010; De Marneffe et al., 2012; Lee et al.,
2015), or the hybrid approaches of both (Saurı́ and
Pustejovsky, 2012; Qian et al., 2015). Recently,
deep learning has been applied to solve EFP. (Qian
et al., 2018) employ Generative Adversarial Net-
works (GANs) for EFP while (Rudinger et al.,
2018) utilize LSTMs for both sequential and de-
pendency representations of the input sentences.
Finally, deep learning has also been considered
for the related tasks of EFP, including event de-
tection (Nguyen and Grishman, 2015b; Nguyen
et al., 2016b; Lu and Nguyen, 2018; Nguyen and
Nguyen, 2019), event realis classification (Mita-
mura et al., 2015; Nguyen et al., 2016g), uncer-
tainty detection (Adel and Schütze, 2017), modal
sense classification (Marasovic and Frank, 2016)
and entity detection (Nguyen et al., 2016d).

3 Model

The formal definition of the EFP task is as fol-
lows. Let (x1, x2, . . . , xn) be a sentence that con-
tains some event mention of interest, where n is
the number of words/tokens and xi is the i-th to-
ken in the sentence. Also, let k be the position of
the anchor word in this sentence (i.e., token xk).
For EFP, the goal is to assign a real number in the
range of [-3, +3] to quantify the degree to which
the current event mention has happened. There
are three major components in the EFP model pro-



4395

posed in this work, i.e., (i) sentence encoding, (ii)
structure induction, and (iii) prediction.

3.1 Sentence Encoding
The first step is to convert each word in the sen-
tences into an embedding vector. In this work,
we employ the contextualized word representa-
tions BERT in (Devlin et al., 2018) for this pur-
pose. BERT is a pre-trained language representa-
tion model with multiple computation layers that
has been shown to improve many NLP tasks. In
particular, the sentence (x1, x2, ..., xn) would be
first fed into the pre-trained BERT model from
which the contextualized embeddings of the words
in the last layer are used for further computation.
We denote such word embeddings for the words in
(x1, x2, . . . , xn) as (e1, e2, . . . , en) respectively.

In the next step, we further abstract
(e1, e2, . . . , en) for EFP by feeding them into two
layers of bidirectional LSTMs (as in (Rudinger
et al., 2018)). This produces (h1, h2, . . . , hn) as
the hidden vector sequence in the last bidirectional
LSTM layer (i.e., the second one). We consider
(h1, h2, . . . , hn) as a rich representation of the
input sentence (x1, x2, . . . , xn) where each vector
hi encapsulates the context information of the
whole input sentence with a greater focus on the
current word xi.

3.2 Structure Induction
Given the hidden representation (h1, h2, . . . , hn),
it is possible to use the hidden vector correspond-
ing to the anchor word hk as the features to per-
form factuality prediction (as done in (Rudinger
et al., 2018)). However, despite the rich context
information over the whole sentence, the features
in hk are not directly designed to focus on the im-
port context words for factuality prediction. In
order to explicitly encode the information of the
cue words into the representations for the anchor
word, we propose to learn an importance matrix
A = (aij)i,j=1..n in which the value in the cell aij
quantifies the contribution of the context word xi
for the hidden representation at xj if the represen-
tation vector at xj is used to form features for EFP.
The importance matrix A would then be used as
the adjacent/weight matrix in the graph convolu-
tional neural networks (GCNs) (Kipf and Welling,
2016; Nguyen and Grishman, 2018) to accumu-
late the current hidden representations of the con-
text words into the new hidden representations for
each word in the sentence.

In order to learn the weight matrix A, as pre-
sented in the introduction, we propose to leverage
both semantic and syntactic structures of the input
sentence. In particular, for the semantic structure,
we use the representation vectors from LSTMs for
xi and xj (i.e., hi and hj) as the features to com-
pute the contribution score in the cell asemij of the
semantic weight matrix Asem = (asemij )i,j=1..n:

h′i = tanh(W
sem
1 hi)

asemij = sigmoid(W
sem
2 [h

′
i, h

′
j ])

Note that we omit the biases in the equations of
this paper for convenience. In the equations above,
[h′i, h

′
j ] is the concatenation of h

′
i and h

′
j . Essen-

tially, asemij is a scalar to determine the amount of
information that should be sent from the context
word xi to the representation at xj based on the
semantic relevance for EFP.

In the next step for the syntactic structure, we
employ the dependency tree for the input sentence
to generate the adjacent/weight matrix Asyn =
(asynij )i,j=1..n, where a

syn
ij is set to 1 if xi is con-

nected to xj in the tree, and 0 otherwise. Note that
we augment the dependency trees with the self-
connection and reverse edges to improve the cov-
erage of the weight matrix.

Finally, the weight matrix A for GCNs would
be the linear combination of the sematic structure
Asem and the syntactic structure Asyn with the
trade-off λ:

A = λAsem + (1− λ)Asyn

Given the weight matrix A, the GCNs (Kipf and
Welling, 2016) are applied to augment the repre-
sentations of the words in the input sentence with
the contextual representations for EFP. In particu-
lar, let H0 be the the matrix with (h1, h2, . . . , hn)
as the rows: H0 = [h1, h2, . . . , hn]. One layer
of GCNs would take an input matrix Hi (i ≥ 0)
and produce the output matrix Hi+1: Hi+1 =
g(AHiW

g
i ) where g is a non-linear function. In

this work, we employ two layers of GCNs (op-
timized on the development datasets) on the in-
put matrix H0, resulting in the semantically and
syntactically enriched matrix H2 with the rows of
(hg1, h

g
2, . . . , h

g
n) for EFP.

3.3 Prediction
This component predicts the factuality degree of
the input event mention based on the context-
aware representation vectors (hg1, h

g
2, . . . , h

g
n). In



4396

FactBank UW Meantime UDS-IH2
MAE r MAE r MAE r MAE r

(Lee et al., 2015)* - - 0.511 0.708 - - - -
(Stanovsky et al., 2017)* 0.590 0.710 0.420 0.660 0.340 0.470 - -
L-biLSTM(2)-S*† 0.427 0.826 0.508 0.719 0.427 0.335 0.960 0.768
L-biLSTM(2)-MultiBal**† 0.391 0.821 0.496 0.724 0.278 0.613 - -
L-biLSTM(1)-MultiFoc**† 0.314 0.846 0.502 0.710 0.305 0.377 - -
L-biLSTM(2)-MultiSimp w/UDS-IH2**† 0.377 0.828 0.508 0.722 0.367 0.469 0.965 0.771
H-biLSTM(1)-MultiSimp**† 0.313 0.857 0.528 0.704 0.314 0.545 - -
H-biLSTM(2)-MultiSimp w/UDS-IH2**† 0.393 0.820 0.481 0.749 0.374 0.495 0.969 0.760
L-biLSTM(2)-S+BERT* 0.381 0.85 0.475 0.752 0.389 0.394 0.895 0.804
L-biLSTM(2)-MultiSimp w/UDS-IH2+BERT** 0.343 0.855 0.476 0.749 0.358 0.499 0.841 0.841
H-biLSTM(1)-MultiSimp+BERT** 0.310 0.821 0.495 0.771 0.281 0.639 0.822 0.812
H-biLSTM(2)-MultiSimp w/UDS-IH2+BERT** 0.330 0.871 0.460 0.798 0.339 0.571 0.835 0.802
Graph-based (Ours)* 0.315 0.890 0.451 0.828 0.350 0.452 0.730 0.905
Ours with multiple datasets** 0.310 0.903 0.438 0.830 0.204 0.702 0.726 0.909

Table 1: Test set performance. * denotes the models trained on separate datasets while ** indicates those trained
on multiple datasets. †specifies the models in (Rudinger et al., 2018) that are significantly improved with BERT.

Dataset Train Dev Test Total
FactBank 6636 2462 663 9761
MEANTIME 967 210 218 1395
UW 9422 3358 864 13644
UDS-IH2 22108 2642 2539 27289

Table 2: The numbers of examples in each dataset

particular, as the anchor word is located at the k-th
position (i.e., the word xk), we first use the vector
hgk as the query to compute the attention weights
for each representation vector in (hg1, h

g
2, . . . , h

g
n).

These attention weights would then be employed
to obtain the weighted sum of (hg1, h

g
2, . . . , h

g
n) to

produce the feature vector V :

αi =W
a
1 h

g
k · (W

a
2 h

g
i )

>

α′1, α
′
2, . . . , α

′
n = softmax(α1, α2, . . . , αn)

V =
∑
i

α′iW
a
3 h

g
i

whereW a1 ,W
a
2 andW

a
3 are the model parameters.

The attention weights α′i would help to promote
the contribution of the important context words for
the feature vector V for EFP.

Finally, similar to (Rudinger et al., 2018), the
feature vector V is fed into a regression model
with two layers of feed-forward networks to pro-
duce the factuality score. Following (Rudinger
et al., 2018), we train the proposed model by op-
timizing the Huber loss with δ = 1 and the Adam
optimizer with learning rate = 1.0.

4 Experiments

4.1 Datasets, Resources and Parameters
Following the previous work (Stanovsky et al.,
2017; Rudinger et al., 2018), we evaluate the pro-

posed EFP model using four benchmark datasets:
FactBack (Saurı́ and Pustejovsky, 2009), UW (Lee
et al., 2015), Meantime (Minard et al., 2016) and
UDS-IH2 (Rudinger et al., 2018). The first three
datasets (i.e., FactBack, UW, and Meantime) are
the unified versions described in (Stanovsky et al.,
2017) where the original annotations for these
datasets are scaled to a number in [-3, +3]. For
the fourth dataset (i.e., UDS-IH2), we follow the
instructions in (Rudinger et al., 2018) to scale the
scores to the range of [-3, +3]. Each dataset comes
with its own training data, test data and develop-
ment data. Table 2 shows the numbers of examples
in all data splits for each dataset used in this paper.

We tune the parameters for the proposed model
on the development datasets. The best values we
find in the tuning process include: 300 for the
number of hidden units in the bidirectional LSTM
layers, 1024 for the dimension of the projected
vector h′i in the structure induction component,
300 for the number of feature maps for the GCN
layers, 600 for the dimention of the transformed
vectors for attention based on (W a1 ,W

a
2 ,W

a
3 ), and

300 for the number of hidden units in the two lay-
ers of the final regression model. For the trade-
off parameter λ between the semantic and syntac-
tic structures, the best value for the datasets Fact-
Back, UW and Meantime is λ = 0.6 while this
value for UDS-IH2 is λ = 0.8.

4.2 Comparing to the State of the Art

This section evaluates the effectiveness of the pro-
posed model for EFP on the benchmark datasets.
We compare the proposed model with the best re-
ported systems in the literature with linguistic fea-
tures (Lee et al., 2015; Stanovsky et al., 2017) and



4397

FactBank UW Meantime UDS-IH2
MAE r MAE r MAE r MAE r

The proposed model 0.310 0.903 0.438 0.830 0.204 0.702 0.726 0.909
- syntax structure (λ = 1) 0.314 0.867 0.442 0.801 0.251 0.658 0.753 0.893
- semantic structure (λ = 0) 0.337 0.832 0.449 0.782 0.288 0.604 0.798 0.862
- structure induction component 0.352 0.821 0.457 0.735 0.305 0.582 0.855 0.828
- BERT 0.342 0.831 0.462 0.751 0.315 0.570 0.896 0.817
- attention in prediction 0.312 0.890 0.441 0.821 0.221 0.695 0.737 0.899

Table 3: Correlation (r) and MAE for different model configurations. The model without BERT (i.e., - BERT)
uses Glove (Pennington et al., 2014) as in (Rudinger et al., 2018).

deep learning (Rudinger et al., 2018). Table 1
shows the performance. Importantly, to achieve
a fair comparison, we obtain the actual implemen-
tation of the current state-of-the-art EFP models
from (Rudinger et al., 2018), introduce the BERT
embeddings as the inputs for those models and
compare them with the proposed models (i.e., the
rows with “+BERT”). Following the prior work,
we use MAE (Mean Absolute Error), and r (Pear-
son Correlation) as the performance measures.

In the table, we distinguish two methods to
train the models investigated in the previous work:
(i) training and evaluating the models on sepa-
rate datasets (i.e., the rows associated with *),
and (ii) training the models on the union of Fact-
Bank, UW and Meantime, resulting in single mod-
els to be evaluated on the separate datasets (i.e.,
the rows with **). It is also possible to train the
models on the union of all the four datasets (i.e.,
FactBank, UW, Meantime and UDS-IH2) (corre-
sponding to the rows with w/UDS-IH2 in the ta-
ble). From the table, we can see that in the first
method to train the models the proposed model
is significantly better than all the previous mod-
els on FactBank, UW and UDS-IH2 (except for
the MAE measure on UW), and achieves compa-
rable performance with the best model (Stanovsky
et al., 2017) on Meantime. In fact, the proposed
model trained on the separate datasets also sig-
nificantly outperforms the current best models on
FactBank, UW and UDS-IH2 when these mod-
els are trained on the union of the datasets with
multi-task learning (except for MAE on Factbank
where the performance is comparable). Regard-
ing the second method with multiple datasets for
training, the proposed model (only trained on the
union of FactBank, UW and Meantime) is fur-
ther improved, achieving better performance than
all the other models in this setting for different
datasets and performance measures. Overall, the
proposed model yields the state-of-the-art perfor-

mance over all the datasets and measures (except
for MAE on UW with comparable performance),
clearly demonstrating the advantages of the model
in this work for EFP.

4.3 Ablation Study
Table 3 presents the performance of the proposed
model when different elements are excluded to
evaluate their contribution. We only analyze the
proposed model when it is trained with multi-
ple datasets (i.e., FactBank, UW and Meantime).
However, the same trends are observed for the
models trained with separate datasets. As we can
see from the table, both semantic and syntactic in-
formation are important for the proposed model
as eliminating any of them would hurt the perfor-
mance. Removing both elements (i.e., not using
the structure induction component) would signif-
icantly downgrade the performance. Finally, we
see that both the BERT embeddings and the at-
tention in the prediction are necessary for the pro-
posed model to achieve good performance.

5 Conclusion & Future Work

We present a graph-based deep learning model
for EFP that exploits both syntactic and semantic
structures of the sentences to effectively model the
important context words. We achieve the state-of-
the-art performance over several EFP datasets.

One potential issue with the current approach is
that it is dependent on the existence of the high-
quality dependency parser. Unfortunately, such
parser is not always available in different domains
and languages. Consequently, in the future work,
we plan to develop methods that can automatically
induce the sentence structures for EFP.

Acknowledgement
This research is partially supported by the NSF
grant CNS-1747798 to the IUCRC Center for Big
Learning.



4398

References
Heike Adel and Hinrich Schütze. 2017. Exploring dif-

ferent dimensions of attention for uncertainty detec-
tion. In Proceedings of the 15th Conference of the
European Chapter of the Association for Computa-
tional Linguistics: Volume 1, Long Papers, pages
22–34.

Marie-Catherine De Marneffe, Christopher D Man-
ning, and Christopher Potts. 2012. Did it happen?
the pragmatic complexity of veridicality assessment.
Computational linguistics, 38(2):301–333.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. arXiv preprint arXiv:1810.04805.

Mona T Diab, Lori Levin, Teruko Mitamura, Owen
Rambow, Vinodkumar Prabhakaran, and Weiwei
Guo. 2009. Committed belief annotation and tag-
ging. In Proceedings of the Third Linguistic Annota-
tion Workshop, pages 68–73. Association for Com-
putational Linguistics.

Thomas N Kipf and Max Welling. 2016. Semi-
supervised classification with graph convolutional
networks. arXiv preprint arXiv:1609.02907.

Kenton Lee, Yoav Artzi, Yejin Choi, and Luke Zettle-
moyer. 2015. Event detection and factuality assess-
ment with non-expert supervision. In Proceedings
of the 2015 Conference on Empirical Methods in
Natural Language Processing, pages 1643–1648.

Amnon Lotan, Asher Stern, and Ido Dagan. 2013.
Truthteller: Annotating predicate truth. In Proceed-
ings of the 2013 Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
752–757.

Weiyi Lu and Thien Huu Nguyen. 2018. Similar but
not the same: Word sense disambiguation improves
event detection via neural representation match-
ing. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP).

Ana Marasovic and Anette Frank. 2016. Multilin-
gual modal sense classification using a convolutional
neural network. Proceedings of the 1st Workshop on
Representation Learning for NLP, Rep4NLP@ACL
2016, Berlin, Germany, August 11, 2016, pages
111–120.

A-L Minard, Manuela Speranza, Ruben Urizar, Beg-
ona Altuna, MGJ van Erp, AM Schoen, CM van
Son, et al. 2016. Meantime, the newsreader mul-
tilingual event and time corpus. In Proceedings
of the Tenth International Conference on Language
Resources and Evaluation (LREC 2016). Portorož,
Slovenia.

Teruko Mitamura, Zhengzhong Liu, and Eduard Hovy.
2015. Overview of tac kbp 2015 event nugget track.
In Proceedings of Text Analysis Conference (TAC).

Rowan Nairn, Cleo Condoravdi, and Lauri Karttunen.
2006. Computing relative polarity for textual infer-
ence. In Proceedings of the fifth international work-
shop on inference in computational semantics (icos-
5).

Thien Huu Nguyen, Kyunghyun Cho, and Ralph Gr-
ishman. 2016a. Joint event extraction via recurrent
neural networks. In Proceedings of the Annual Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics (NAACL).

Thien Huu Nguyen, Lisheng Fu, Kyunghyun Cho, and
Ralph Grishman. 2016b. A two-stage approach for
extending event detection to new types via neural
networks. In Proceedings of the 1st ACL Workshop
on Representation Learning for NLP (RepL4NLP).

Thien Huu Nguyen and Ralph Grishman. 2015b. Event
detection and domain adaptation with convolutional
neural networks. In Proceedings of the Annual
Meeting of the Association for Computational Lin-
guistics (ACL).

Thien Huu Nguyen and Ralph Grishman. 2018. Graph
convolutional networks with argument-aware pool-
ing for event detection. In Proceedings of the Asso-
ciation for the Advancement of Artificial Intelligence
(AAAI).

Thien Huu Nguyen, Adam Meyers, and Ralph Grish-
man. 2016g. New york university 2016 system for
kbp event nugget: A deep learning approach. In
Proceedings of Text Analysis Conference (TAC).

Thien Huu Nguyen, Avirup Sil, Georgiana Dinu, and
Radu Florian. 2016d. Toward mention detection ro-
bustness with recurrent neural networks. In Pro-
ceedings of IJCAI Workshop on Deep Learning for
Artificial Intelligence (DLAI).

Trung Minh Nguyen and Thien Huu Nguyen. 2019.
One for all: Neural joint modeling of entities and
events. In Proceedings of the Association for the
Advancement of Artificial Intelligence (AAAI).

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 1532–1543.

Vinodkumar Prabhakaran, Owen Rambow, and Mona
Diab. 2010. Automatic committed belief tagging.
In Proceedings of the 23rd International Conference
on Computational Linguistics: Posters, pages 1014–
1022. Association for Computational Linguistics.

Zhong Qian, Peifeng Li, Yue Zhang, Guodong Zhou,
and Qiaoming Zhu. 2018. Event factuality identifi-
cation via generative adversarial networks with aux-
iliary classification. In Proceedings of the Twenty-
Seventh International Joint Conference on Artificial



4399

Intelligence, IJCAI 2018, July 13-19, 2018, Stock-
holm, Sweden., pages 4293–4300.

Zhong Qian, Peifeng Li, and Qiaoming Zhu. 2015. A
two-step approach for event factuality identification.
In Asian Language Processing (IALP), 2015 Inter-
national Conference on, pages 103–106. IEEE.

Rachel Rudinger, Aaron Steven White, and Benjamin
Van Durme. 2018. Neural models of factuality. In
Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Volume 1 (Long Papers), pages 731–744.

Roser Saurı́. 2008. A factuality profiler for eventual-
ities in text. Unveröffentlichte Dissertation, Bran-
deis University. Zugriff auf http://www. cs. brandeis.
edu/˜ roser/pubs/sauriDiss, 1.

Roser Saurı́ and James Pustejovsky. 2009. Factbank:
a corpus annotated with event factuality. Language
resources and evaluation, 43(3):227.

Roser Saurı́ and James Pustejovsky. 2012. Are you
sure that this happened? assessing the factuality de-
gree of events in text. Computational Linguistics,
38(2):261–299.

Gabriel Stanovsky, Judith Eckle-Kohler, Yevgeniy
Puzikov, Ido Dagan, and Iryna Gurevych. 2017. In-
tegrating deep linguistic features in factuality pre-
diction over unified datasets. In Proceedings of the
55th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 2: Short Papers), vol-
ume 2, pages 352–357.


