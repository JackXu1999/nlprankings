



















































Efficient Discontinuous Phrase-Structure Parsing via the Generalized Maximum Spanning Arborescence


Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1644–1654
Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics

Efficient Discontinuous Phrase-Structure Parsing via the Generalized
Maximum Spanning Arborescence

Caio Corro Joseph Le Roux Mathieu Lacroix
Laboratoire d’Informatique de Paris Nord,

Université Paris 13 – SPC, CNRS UMR 7030,
F-93430, Villetaneuse, France

{corro,leroux,lacroix}@lipn.fr

Abstract

We present a new method for the joint
task of tagging and non-projective depen-
dency parsing. We demonstrate its use-
fulness with an application to discontinu-
ous phrase-structure parsing where decod-
ing lexicalized spines and syntactic deriva-
tions is performed jointly. The main con-
tributions of this paper are (1) a reduction
from joint tagging and non-projective de-
pendency parsing to the Generalized Max-
imum Spanning Arborescence problem,
and (2) a novel decoding algorithm for
this problem through Lagrangian relax-
ation. We evaluate this model and obtain
state-of-the-art results despite strong inde-
pendence assumptions.

1 Introduction

Discontinuous phrase-structure parsing relies ei-
ther on formal grammars such as LCFRS, which
suffer from a high complexity, or on reductions to
non-projective dependency parsing with complex
labels to encode phrase combinations. We pro-
pose an alternative approach based on a variant of
spinal TAGs, which allows parses with disconti-
nuity while grounding this work on a lexicalized
phrase-structure grammar. Contrarily to previous
approaches, (Hall and Nivre, 2008; Versley, 2014;
Fernández-González and Martins, 2015), we do
not model supertagging nor spine interactions with
a complex label scheme. We follow Carreras et al.
(2008) but drop projectivity.

We first show that our discontinuous variant
of spinal TAG reduces to the Generalized Max-
imum Spanning Arborescence (GMSA) problem
(Myung et al., 1995). In a graph where vertices
are partitioned into clusters, GMSA consists in
finding the arborescence of maximum weight in-

cident to exactly one vertex per cluster. This prob-
lem is NP-complete even for arc-factored models.
In order to bypass complexity, we resort to La-
grangian relaxation and propose an efficient res-
olution based on dual decomposition which com-
bines a simple non-projective dependency parser
on a contracted graph and a local search on each
cluster to find a global consensus.

We evaluated our model on the discontinuous
PTB (Evang and Kallmeyer, 2011) and the Tiger
(Brants et al., 2004) corpora. Moreover, we show
that our algorithm is able to quickly parse the
whole test sets.

Section 2 presents the parsing problem. Sec-
tion 3 introduces GMSA from which we derive an
effective resolution method in Section 4. In Sec-
tion 5 we define a parameterization of the parser
which uses neural networks to model local prob-
abilities and present experimental results in Sec-
tion 6. We discuss related work in Section 7.

2 Joint Supertagging and Spine Parsing

In this section we introduce our problem and set
notation. The goal of phrase-structure parsing
is to produce a derived tree by means of a se-
quence of operations called a derivation. For in-
stance in context-free grammars the derived tree
is built from a sequence of substitutions of a non-
terminal symbol with a string of symbols, whereas
in tree adjoining grammars (TAGs) a derivation is
a sequence of substitutions and adjunctions over
elementary trees. We are especially interested
in building discontinuous phrase-structure trees
which may contain constituents with gaps.1

We follow Shen (2006) and build derived trees
from adjunctions performed on spines. Spines are
lexicalized unary trees where each level represents

1Although we will borrow concepts from TAGs, we do not
require derivations to be TAG compatible (i.e. well-nested
dependencies with a bounded number of gaps).

1644



What

WP

WHNP

, I

PRP

NP

said

VB

VP

S

, should

MD

I

PRP

NP

do

VB

VP

SQ

SBARQ

?

Figure 1: A derivation with spines and adjunc-
tions (dashed arrows). The induced dependency
tree is non-projective. Each color corresponds to a
spine. We omit punctuation to simplify figures.

a lexical projection of the anchor. Carreras et al.
(2008) showed how spine-based parsing could be
reduced to dependency parsing: since spines are
attached to words, equivalent derivations can be
represented as a dependency tree where arcs are la-
beled by spine operations, an adjunction together
with information about the adjunction site. How-
ever, we depart from previous approaches (Shen
and Joshi, 2008; Carreras et al., 2008) by relaxing
the projectivity constraint to represent all discon-
tinuous phrase-structure trees (see Figure 1).

We assume a finite set of spines S. A spine s
can be defined as a sequence of grammatical cat-
egories, beginning at root. For a sentence w =
(w0, w1, . . . , wn) where wk is the word at position
k and w0 is a dummy root symbol, a derivation
is a triplet (d, s, l) defined as follows. Adjunc-
tions are described by a dependency tree rooted at
0 written as a sequence of arcs d. If (h,m) ∈ d
with h ∈ {0, . . . , n} and m ∈ {1, . . . , n}, then
the derivation contains an adjunction of the root of
the spine at position m to a node from the spine
at position h. Supertagging, the assignment of a
spine to each word, is represented by a sequence
s = (s0, s1, . . . , sn) of n + 1 spines, each spine
sk being assigned to word wk. Finally, labeling
l = (l1, . . . , ln) is a sequence where lk is the label
of the kth arc (h,m) of d. The label consists of a
couple (op, i) where op is the type of adjunction,
here sister or regular2, and i is the index of the
adjunction node in sh.

Each derivation is assigned an arc-factored
score σ which is given by:

σ(d, s, l; w) =
∑

(h,m)∈d
Ω(h,m, sh, sm, lhm; w)

For instance, following score functions de-
2The distinction is not crucial for the exposition. We refer

readers to (Shen and Joshi, 2008; Carreras et al., 2008).

veloped in (Carreras et al., 2008), this func-
tion could read sh[i], sh[i + 1] and sm[0], where
s[i] denotes the i-th grammatical category of the
spine s. The score of the derivation in Fig-
ure 1 could then reflect that the spine WHNP-WP
associated with What is adjoined on the spine
SBARQ-SQ-VP-VB associated with do on a site
with the grammatical triple [VP WHNP VB].

We assume that Ω accounts for the contribution
of arcs, spines and labels to the score. The de-
tails of the contribution depend on the model. We
choose the following:

σ(d, s, l; w) =
∑

(h,m)∈d
(α(h,m; w)

+ν(sm;h,m,w)
+γ(lhm;h,m, sh,w))

where α is the score related to the dependency
tree, ν is the supertagging score and γ the label-
ing score. Note that functions α, ν and γ have
access to the entire input string w. Score func-
tion σ can be parameterized in many ways and
we discuss our implementation in Section 5. In
this setting, parsing a sentence w amounts to find-
ing the highest-scoring derivation (d∗, s∗, l∗) =
arg max(d,s,l) σ(d, s, l; w).

Recovering the derived tree from a derivation is
performed by recursively mapping each spine and
its dependencies to a possibly gappy constituent.
Given a spine sh and site index i, we look for
the leftmost sl and rightmost sr dependents at-
tached with regular adjunction. If any, we insert
a new node between sh[i] and sh[i + 1] with the
same grammatical category as the first one. This
new node fills the role of the foot node in TAGs.
Every dependent of sh[i] with anchor in interval
[l + 1, r − 1] is moved to the newly created node.
Remaining sister and regular adjunctions are sim-
ply attached to sh[i].

The complexity of the parsing problem depends
on the type of dependency trees. In the case of
projective trees, it has been shown (Eisner, 2000;
Carreras et al., 2008; Li et al., 2011) that this could
be performed in cubic worst-case time complex-
ity with dynamic programming, whether supertags
are fixed beforehand or not. However, the mod-
ification of the original Eisner algorithm requires
that chart cells must be indexed not only by spans,
or pairs of positions, but also by pairs of supertags.
In practice the problem is intractable unless heavy

1645



pruning is performed first in order to select a sub-
set of spines at each position.

In the case of non-projective dependency trees,
the problem has quadratic worst-case time com-
plexity when supertags are fixed, since the prob-
lem then amounts to non-projective parsing and
reduces to the Maximum Spanning Arborescence
problem (MSA) as in (McDonald et al., 2005).
Unfortunately, the efficient algorithm for MSA is
greedy and does not store potential substructure
candidates. Hence, when supertags are not fixed
beforehand, a new arborescence must be recom-
puted for each choice of supertags. This problem
can be seen as instance of the Generalized Max-
imum Spanning Arborescence problem, an NP-
complete problem, which we review in the next
section. Note that arc labels do not impact the
asymptotic complexity of an arc-factored model.
Indeed, only the labeled arc with maximum weight
between two vertices is considered when parsing.

3 The Generalized Maximum Spanning
Arborescence

In this section, we first define GMSA introduced
by Myung et al. (1995). We formulate this prob-
lem as an integer linear program. We then ex-
plain the reduction from the joint supertagging and
spine parsing task to this problem.3

3.1 Problem definition

Let D = (V,A) be a directed graph. Given a sub-
set T ⊆ A of arcs, V [T ] denotes the set of ver-
tices of V which are the tail or the head of at least
one arc of T . These vertices are said to be cov-
ered by T . A subset T ⊆ A of arcs is called an
arborescence if the graph (V [T ], T ) is connected,
acyclic and each vertex has at most one entering
arc. The vertex with no entering arc is called the
root of T . An arborescence covering all vertices is
called a spanning arborescence.

Let π = {V0, . . . , Vn}, n ∈ N be a partition
of V . Each element of π is called a cluster. An
arborescence T of D covering exactly one vertex
per cluster of π is called a generalized spanning
arborescence (GSA). Figure 2 gives an example
of a GSA. The partition of V is composed of a
cluster having one vertex and six clusters having
four vertices. Each cluster is depicted by a hatched
area. The GSA is depicted by the dashed arcs.

3A similar reduction can be obtained in the reverse direc-
tion, thus proving the NP-completeness of our problem.

Let W be a vertex subset of V . We denote
δ−(W ) (resp. δ+(W )) the set of arcs entering
(resp. leaving)W and δ(W ) = δ−(W )∪δ+(W ).4
Contracting W consists in replacing in D all ver-
tices in W by a new vertex w, replacing each
arc uv ∈ δ−(W ) by the arc uw and each arc
vu ∈ δ+(W ) by wu. Let Dπ be the graph ob-
tained by contracting each cluster of π in D. Note
that a GSA ofD and π induces a spanning arbores-
cence of Dπ.5 For instance, contracting each clus-
ter in the graph given by Figure 2 leads to a graph
Dπ having 7 vertices and the set of dashed arcs
corresponds to a spanning arborescence of Dπ.

Given arc weights φ ∈ RA, the weight of an ar-
borescence T is

∑
a∈T φa. Given (D,π, φ), the

Generalized Maximum Spanning Arborescence
problem (GMSA) consists in finding a GSA of D
and π of maximum weight whose root is in V0.

3.2 Integer linear program
Given a set S, z ∈ RS is a vector indexed by ele-
ments in S. For S′ ⊆ S, z(S′) = ∑s∈S′ zs.

A GSA T ⊆ A is represented by variables x ∈
{0, 1}V and y ∈ {0, 1}A such that xv (resp. ya) is
equal to 1 iff v ∈ V [T ] (resp. a ∈ T ).

Since a GSA of D and π induces a spanning
arborescence of Dπ, the arc-incidence vector y ∈
{0, 1}A of a GSA with root in V0 satisfies the fol-
lowing, adapted from MSA (Schrijver, 2003):

y(δ−(V0)) = 0 (1)
y(δ−(Vk)) = 1 ∀1 ≤ k ≤ n, (2)
y(δ−( ∪

Vk∈π′
Vk)) ≥ 1 ∀π′ ⊆ π \ {V0}. (3)

Let Y denote all the arc-incidence vectors on D
corresponding to a spanning arborescence in Dπ

whose root is the contraction of V0. Then,

Y = {y ∈ {0, 1}A|y satisfies (1)-(3)}.
GMSA can be formulated with the following in-

teger linear program:

max
x,y

φ · y (4)
s.t. y ∈ Y (5)

xv ≥ ya ∀v ∈ V, a ∈ δ(v), (6)
xv(Vk) = 1 ∀0 ≤ k ≤ n, (7)
xv ∈ {0, 1} ∀v ∈ V. (8)

4By an abuse of notation, we identify any singleton {v}
with its element v.

5The converse does not hold: an arc subset of A corre-
sponding to a spanning arborescence of Dπ may not be a
GSA of D and π since it may not induce a connected graph.

1646



Let W and T be the vertex and arc sets given by
xv = 1 and ya = 1 respectively. Since T is a
spanning arborescence of Dπ by (5), (V [T ], T ) is
an acyclic directed graph with n arcs such that V0
has no entering arc and Vi, i ∈ {1, . . . , n}, has
one entering arc. By constraints (7), W contains
one vertex per cluster of π. Moreover, by inequal-
ities (6), V [T ] ⊆ W . Since |W | = n + 1 and
|T | = n, W = V [T ] and (V [T ], T ) is connected,
so it is a GSA. Because its root is in V0 by (5), it
is an optimal solution for GMSA by (4).

3.3 Reduction from joint parsing to GMSA
Given an instance of the joint parsing problem, we
construct an instance of GMSA as follows. With
every spine s of every word wk different from w0,
we associate a vertex v. For k = 1, . . . , n, we
denote by Vk the set of vertices associated with
the spines of wk. We associate with w0 a set V0
containing only one vertex and V0 will now refer
both the cluster and the vertex it contains depend-
ing on the context. Let π = {V0, . . . , Vn} and
V = ∪nk=0Vk. For every couple u, v of vertices
such that u ∈ Vh and v ∈ Vm, h 6= m and m 6= 0,
we associate an arc uv corresponding to the best
adjunction of the root of spine sm associated with
v of Vm to spine sh associated with vertex u of Vh.
The weight of this arc is given by

φuv = α(h,m; w) + ν(sm;h,m,w)
+ max

lhm
γ(lhm;h,m, sh,w)

which is the score of the best adjunction of sm to
sh. This ends the construction of (D,π, φ).

There is a 1-to-1 correspondence between the
solutions to GMSA and those to the joint supertag-
ging and spine parsing task in which each adjunc-
tion is performed with the label maximizing the
score of the adjunction. Indeed, the vertices cov-
ered by a GSA T with root V0 correspond to the
spines on which the derivation is performed. By
definition of GSAs, one spine per word is chosen.
Each arc of T corresponds to an adjunction. The
score of the arborescence is the sum of the scores
of the selected spines plus the sum of the scores of
the best adjunctions with respect to T . Hence, one
can solve GMSA to perform joint parsing.

As an illustration, the GSA depicted in Figure 2
represents the derivation tree of Figure 1: the ver-
tices of V \ V0 covered by the GSA are those as-
sociated with the spines of Figure 1 and the arcs
represent the different adjunctions. For instance

V0
(ROOT)

V1
(What)

V2
(I)

V3
(said)

V4
(should)

V5
(I)

V6
(do)

Figure 2: The generalized spanning arborescence
inducing the derivation tree in Figure 1.

the arc from V3 to V2 represents the adjunction of
spine NP-PRP to spine S-VP-VB at index 0.

4 Efficient Decoding

Lagrangian relaxation has been successfully ap-
plied to various NLP tasks (Koo et al., 2010;
Le Roux et al., 2013; Almeida and Martins, 2013;
Das et al., 2012; Corro et al., 2016). Intuitively,
given an integer linear program, it consists in re-
laxing some linear constraints which make the
program difficult to solve and penalizing their vi-
olation in the objective function.

We propose a new decoding method for GMSA
based on dual decomposition, a special flavor of
Lagrangian relaxation where the problem is de-
composed in several independent subproblems.

4.1 Dual decomposition

To perform the dual decomposition, we first refor-
mulate the integer linear program (4)-(8) before
relaxing linear constraints. For this purpose, we
replace the variables y by three copies {yi} =
{y0, y1, y2}, yi ∈ {0, 1}A. We also consider vari-
ables z ∈ RA. Let φ0, φ1 and φ2 be arc weight
vectors such that

∑
i φ

i = φ.6 GMSA can then be
reformulated as:

max
x,{yi},z

∑
i

φi · yi (9)

s.t. y0 ∈ Y (10)
xv ≥ y1a ∀v ∈ V, a ∈ δ−(v), (11)
xv ≥ y2a ∀v ∈ V, a ∈ δ+(v), (12)
xv(Vk) = 1 ∀0 ≤ k ≤ n, (13)
xv ∈ {0, 1} ∀v ∈ V, (14)
z = yi ∀i. (15)

6In our implementation, we choose φ0 = φ1 = φ2 = 1
3
φ.

1647



Note that variables z only appear in equa-
tions (15). Their goal is to ensure equality be-
tween copies y0, y1 and y2. Variables z are usually
called witness variables (Komodakis et al., 2007).
Equality between y0, y1 and y2 implies that (10)-
(12) are equivalent to (5) and (6).

We now relax constraints (15) and build the dual
objective (Lemaréchal, 2001) L∗({λi}):

max
x,{yi},z

∑
i

φi · yi +
∑

i∈{0,1,2}
λi · (z − yi)

s.t. (10)− (14)

where {λi} = {λ0, λ1, λ2}, λi ∈ RA for i =
0, 1, 2, is the set of Lagrangian multipliers. The
dual problem is then:

min
{λi}
L∗({λi})

Note that, as there is no constraint on z, if
∑

i λ
i 6=

0 then L∗({λi}) = +∞. Therefore, we can re-
strict the domain of {λi} in the dual problem to
the set Λ = {{λi}|∑i λi = 0}. This implies that
z may be removed in the dual objective. This latter
can be rewritten as:

L∗({λi}) = max
x,{yi}

∑
i

φ̄i · yi

s.t. (10)− (14)

where φ̄i = φi − λi for i = 0, 1, 2.
4.2 Computing the dual objective
Given {λi} ∈ Λ, computing the dual objective
L∗({λi}) can be done by solving the two follow-
ing distinct subproblems:

P1(φ̄0) = max
y0

φ̄0 · y0

s.t. y0 ∈ Y
P2(φ̄1, φ̄2) = max

x,y1,y2
φ̄1 · y1 + φ̄2 · y2

s.t. (11)− (14)
yia ∈ {0, 1} ∀a ∈ A, i = 1, 2.

Subproblem P1 can be solved by simply running
the MSA algorithm on the contracted graph Dπ.

Subproblem P2 can be solved in a combinato-
rial way. Indeed, observe that each value of y1

and y2 is only constrained by a single value of x.
The problem amounts to selecting for each cluster

a vertex as well as all the arcs with positive weight
covering it. More precisely, for each vertex v ∈ V ,
compute the local weight cv defined by:∑

a∈δ−(v)
max{0, φ̄1}+

∑
a∈δ+(v)

max{0, φ̄2}.

Let V max be the set of vertices defined as fol-
lows. For k = 0, . . . , n, add in V max the ver-
tex v ∈ Vk with the maximum weight cv. Let A1
and A2 be the sets of arcs such that A1 (resp. A2)
contains all the arcs with positive weights entering
(resp. leaving) a vertex of V max. The vectors x, y1

and y2 corresponding respectively to the incidence
vectors of V max, A1 and A2 form an optimal so-
lution to P2.

Hence, both supbroblems can be be solved with
a O(|π|2) time complexity, that is quadratic w.r.t.
the length of the input sentence.7

4.3 Decoding algorithm
Our algorithm seeks for a solution to GMSA by
solving the dual problem since its solution is opti-
mal to GMSA whenever it is a GSA. If not, a so-
lution is constructed by returning the highest GSA
on the spines computed during the resolution of
the dual problem.

We solve the dual problem using a projected
subgradient descent which consists in iteratively
updating {λi} in order to reduce the distance to the
optimal assignment. Let {λi,t} denotes the value
of {λi} at iteration t. {λi,0} is initially set to 0. At
each iteration, the value of {λi,t+1} is computed
from the value of {λi,t} thanks to a subgradient of
the dual objective. More precisely, we have

{λi,t+1} = {λi,t} − ηt ×∇L∗({λi,t})}
where ∇L∗({λi,t}) is a subgradient of L∗({λi,t})
and ηt ∈ R is the stepsize at iteration t. We use
the projected subgradient from Komodakis et al.
(2007). Hence, at iteration t, we must solve repa-
rameterized subproblems P1 and P2 to obtain the
current solution (x̄t, ȳ0,t, ȳ1,t, ȳ2,t) of the dual ob-
jective. Then each multiplier is updated following

λi,t+1 = λi,t − ηt ×
ȳi,t − 2∑

j=0

ȳj,t

3

 .
Note that for any value of {λi}, L∗({λi}) gives

an upper bound for GMSA. So, whenever the
7In the general case, the time complexity is O(|V |2). But

in our problem, the number of vertices per cluster is bounded
by the grammar size: O(|V |2) = O(|Sπ|2) = O(|π|2).

1648



optimal solution x̄t, {ȳi,t} to the dual objective
L∗({λi,t}) at iteration t is a primal feasible solu-
tion, that is ȳ0,t = ȳ1,t = ȳ2,t, it is an optimal
solution to GMSA and the algorithm ends. Other-
wise, we construct a pipeline solution by perform-
ing a MSA on the vertices given by x̄t.

If after a fixed number of iterations we have not
found an optimal solution to GMSA, we return the
pipeline solution with maximum weight.

4.4 Lagrangian enhancement

The previsouly defined Lagrangian dual is valid
but may lead to slow convergence. Thus, we
propose three additional techniques which empir-
ically improve the decoding time and the conver-
gence rate: constraint tightening, arc reweighing
and problem reduction.

Constraint tightening: In subproblem P2, we
consider a vertex and all of its adjacent arcs of pos-
itive weight. However, we know that our optimal
solution must satisfy tree-shape constraints (5).
Thus, every cluster except the root must have ex-
actly one incoming arc and there is at most one arc
between two clusters. Both constraints are added
to P2 without hurting its time complexity.

Reweighing: By modifying weights such that
less incoming arcs have a positive weight, the so-
lution of P2 tends to be an arborescence. For each
cluster Vk ∈ π \ V0, let Âk be the set of incoming
arcs with the highest weight φ̂k. Then, let γk be
a value such that φa − γk is positive only for arcs
in Âk. Subtracting γk from the weight φa of each
arc of δ−(Vk) and adding γk to the objective score
does not modify the weight of the solution because
only one entering arc per cluster is selected.

Problem reduction: We use the pipeline solu-
tions computed at each iteration to set the value of
some variables. Let x̄, {ȳi} be the optimal solu-
tion of L∗({λi}) computed at any iteration of the
subgradient algorithm. For k = 1, . . . , n, let v̄
be the vertex of Vk such that x̄v̄ = 1. Using the
local weights (Section 4.2), for all v ∈ Vk \ {v̄},
L∗({λi})+cv−cv̄ is an upper bound on the weight
of any solution (x, y) to GMSA with xv = 1.
Hence, if it is lower than the weight of the best
pipeline solution found so far, we can guarantee
that xv = 0 in any optimal solution. We can check
the whole graph in linear time if we keep local
weights c in memory.

5 Neural Parameterization

We present a probabilistic model for our frame-
work. We implement our probability distributions
with neural networks, more specifically we build
a neural architecture on top of bidirectional recur-
rent networks that compute context sensitive rep-
resentations of words. At each step, the recurrent
architecture is given as input a concatenation of
word and part-of-speech embeddings. We refer the
reader to (Kiperwasser and Goldberg, 2016; Dozat
and Manning) for further explanations about bidi-
rectional LSTMs (Hochreiter and Schmidhuber,
1997). In the rest of this section, bm denotes the
context sensitive representation of word wm.

We now describe the neural network models
used to learn and assign weight functions α, ν and
γ under a probabilistic model. Given a sentence
w of length n, we assume a derivation (d, s, l)
is generated by three distinct tasks. By chain
rule, P (d, s, l|w) = Pα(d|w) × Pν(s|d,w) ×
Pγ(l|d, s,w). We follow a common approach in
dependency parsing and assign labels l in a post-
processing step, although our model is able to in-
corporate label scores directly. Thus, we are left
with jointly decoding a dependency structure and
assigning a sequence of spines. We note si the ith

spine:8

Pα(d|w)× Pν(s|d,w)
=

∏
(h,m)∈d

Pα(h|m,w)× Pν(sm|m,d,w)

=
∏

(h,m)∈d
Pα(h|m,w)× Pν(sm|m,h,w)

We suppose that adjunctions are generated by an
arc-factored model, and that a spine prediction de-
pends on both current position and head position.

Then parsing amounts to finding the most prob-
able derivation and can be realized in the log
space, which gives following weight functions:

α(h,m; w) = logPα(h|m,w)
ν(sm;h,m,w) = logPν(sm|m,h,w)

where α represents the arc contribution and ν the
spine contribution (cf. Section 2).

Word embeddings bk are first passed through
specific feed-forward networks depending on the

8We assume that the spine for the root w0 is unique.

1649



distribution and role. The result of the feed-
forward transformation parameterized by set of
parameters ρ of a word embedding bs is a vector
denoted b(ρ)s . We first define a biaffine attention
networks weighting dependency relations (Dozat
and Manning):

o
(α)
h,m = b

(α1)>
m W

(α)b
(α2)
h + V

(α)b
(α2)
h

where W (α) and V (α) are trainable parameters.
Moreover, we define a biaffine attention classifier
networks for class c as:

o
(τ)
c,h,m = b

(τ1)>
m W

(τc)b
(τ2)
h

+ V (τc)
(
b(τ1)m ⊕ b(τ2)h

)
+ u(τc)

where ⊕ is the concatenation. W (τc), V (τc) and
u(τc) are trainable parameters. Then, we define the
weight of assigning spine s to word at position m
with head h as o(ν)s,h,m.

Distributions Pα and Pν are parameterized by
these biaffine attention networks followed by a
softmax layer:

Pα(h|m,w) =
exp o(α)h,m∑
h′ exp o

(α)
h′,m

Pν(s|h,m,w) =
exp o(ν)s,h,m∑
s′ exp o

(ν)
s′,h,m

Now we move on to the post-processing step
predicting arc labels. For each adjunction of spine
s at position m to spine t at position h, instead
of predicting a site index i, we predict the non-
terminal nt at t[i] with a biaffine attention classi-
fier.9 The probability of the adjunction of spine s
at position m to a site labeled with nt on spine t at
position h with type a ∈ {regular, sister} is:

Pγ(nt, a|h,m) = Pγ′(nt|h,m,w)
× Pγ′′(a|h,mw)

Pγ and Pγ′′ are again defined as distributions
from the exponential family using biaffine atten-
tion classifiers:

Pγ′(nt|h,m, t) =
exp o(γ

′)
nt,h,m∑

nt′ exp o
(γ′)
nt,h,m

Pγ′′(a|h,m, t) =
exp o(γ

′′)
t,h,m∑

a′ exp o
(γ′′)
a′,h,m

9If a spine contains repeated non-terminal sequences, we
select the lowest match.

We use embeddings of size 100 for words and
size 50 for parts-of-speech tags. We stack two
bidirectional LSTMs with a hidden layer of size
300, resulting in a context sensitive embedding of
size 600. Embeddings are shared across distribu-
tions. All feed-forward networks have a unique
elu-activated hidden layer of size 100 (Clevert
et al., 2016). We regularize parameters with a
dropout ratio of 0.5 on LSTM input. We es-
timate parameters by maximizing the likelihood
of the training data through stochastic subgradi-
ent descent using Adam (Kingma and Ba, 2015).
Our implementation uses the Dynet library (Neu-
big et al., 2017) with default parameters.

6 Experiments

We ran a series of experiments on two corpora an-
notated with discontinuous constituents.

English We used an updated version of the Wall
Street Journal part of the Penn Treebank corpus
(Marcus et al., 1994) which introduces discontinu-
ity (Evang and Kallmeyer, 2011). Sections 2-21
are used for training, 22 for developpement and
23 for testing. We used gold and predicted POS
tags by the Stanford tagger,10 trained with 10-
jackknifing. Dependencies are extracted following
the head-percolation table of Collins (1997).

German We used the Tiger corpus (Brants
et al., 2004) with the split defined for the SPMRL
2014 shared task (Maier, 2015; Seddah et al.,
2013). Following Maier (2015) and Coavoux
and Crabbé (2017), we removed sentences num-
ber 46234 and 50224 as they contain anno-
tation errors. We only used the given gold
POS tags. Dependencies are extracted following
the head-percolation table distributed with Tulipa
(Kallmeyer et al., 2008).

We emphasize that long sentences are not fil-
tered out. Our derivation extraction algorithm
is similar to the one proposed in Carreras et al.
(2008). Regarding decoding, we use a beam
of size 10 for spines w.r.t. Pν(sm|m,w) =∑

h Pν(sm|h,m,w)× Pα(h|m,w) but allow ev-
ery possible adjunction. The maximum number of
iterations of the subgradient descent is set to 500
and the stepsize ηt is fixed following the rule of
Polyak (1987).

Parsing results and timing on short sentences
only (≤ 40 words) and full test set using the de-

10
http://nlp.stanford.edu/software/tagger.shtml

1650



fault discodop11 eval script are reported on Table 1
and Table 2.12 We report labeled recall (LR), pre-
cision (LP), F-measure (LF) and time measured
in minutes. We also report results published by
van Cranenburgh et al. (2016) for the discontin-
uous PTB and Coavoux and Crabbé (2017) for
Tiger. Moreover, dependency unlabeled attach-
ment scores (UAS) and tagging accuracies (Spine
acc.) are given on Table 3. We achieve signif-
icantly better results on the discontinuous PTB,
while being roughly 36 times faster together with a
low memory footprint.13 On the Tiger corpus, we
achieve on par results. Note however that Coavoux
and Crabbé (2017) rely on a greedy parser com-
bined with beam search.

Fast and efficient parsing of discontinuous con-
stituent is a challenging task. Our method can
quickly parse the whole test set, without any par-
allelization or GPU, obtaining an optimality cer-
tificate for more than 99% of the sentences in
less than 500 iterations of the subgradient descent.
When using a non exact decoding algorithm, such
as a greedy transition based method, we may not
be able to deduce the best opportunity for improv-
ing scores on benchmarks, such as the parameter-
ization method or the decoding algorithm. Here
the behavior may be easier to interpret and direc-
tions for future improvement easier to see. We
stress that our method is able to produce an op-
timality certificate on more than 99% of the test
examples thanks to the enhancement presented in
Section 4.4.

7 Related Work

Spine-based parsing has been investigated in
(Shen and Joshi, 2005) for Lexicalized TAGs with
a left-to-right shift-reduce parser which was sub-
sequently extended to a bidirectional version in
(Shen and Joshi, 2008). A graph-based algorithm
was proposed in (Carreras et al., 2008) for second-
order projective dependencies, and for a form of
non-projectivity occurring in machine translation
(i.e. projective parses of permutated input sen-
tences) in (Carreras and Collins, 2009).

Discontinuous phrase-structure parsing through
dependencies in contexts other that TAGs have

11
https://github.com/andreasvc/disco-dop/

12C2017 processing time is 137.338 seconds plus approxi-
matively 30 seconds for model and corpus loading (personnal
communication).

13Execution times are not directly comparable because we
report our experimental conditions and published results.

LR LP LF Time
Short sentences only

This work 90.63 91.01 90.82 ≈ 4
This work† 89.57 90.13 89.85 ≈ 4
VC2016† 87.00 ≈ 180

Full test set
This work 89.89 90.29 90.09 ≈ 6.5
This work† 88.90 89.45 89.17 ≈ 5.5

Table 1: Parsing results and processing time
on the english discontinuous PTB corpus. Re-
sults marked with † use predicted part-of-speech
tags. VC2016 indicates results of van Cranen-
burgh et al. (2016).

LR LP LF Time
Short sentences only

This work 82.69 84.68 83.67 ≈ 7.5
Full test set

This work 80.66 82.63 81.63 ≈ 11
C2017 81.60 ≈ 2.5

Table 2: Parsing results and processing time on
the german Tiger corpus. C2017 indicates results
of Coavoux and Crabbé (2017).

been explored in (Hall and Nivre, 2008; Versley,
2014; Fernández-González and Martins, 2015).
The first two encode spine information as arc la-
bels while the third one relaxes spine information
by keeping only the root and height of the adjunc-
tion, thus avoiding combinatorial explosion. La-
beling is performed as a post-processing step in
these approaches, since the number of labels can
be very high. Our model also performs labeling
after structure construction, but it could be per-
formed jointly without major issue. This is one
way our model could be improved.

GMSA has been studied mostly as a way to
solve the non directed version (i.e. with symet-
ric arc weights) (Myung et al., 1995), see (Pop,
2009; Feremans et al., 1999) for surveys on res-
olution methods. Myung et al. (1995) proposed
an exact decoding algorithm through branch-and-
bound using a dual ascent algorithm to compute
bounds. Pop (2002) also used Lagrangian relax-
ation – in the non directed case – where a single
subproblem is solved in polynomial time. How-
ever, the relaxed constraints are inequalities: if the
dual objective returns a valid primal solution, it is
not a sufficient condition in order to guarantee that

1651



UAS Spine acc.
English 93.70 97.32
English† 93.04 96.81
German 92.25 96.49

Table 3: Dependency parsing and tagging re-
sults. Results marked with † use predicted part-
of-speech tags.

it is the optimal solution (Beasley, 1993), and thus
the stopping criterion for the subgradient descent
is usually slow to obtain. To our knowledge, our
system is the first time that GMSA is used to solve
a NLP problem.

Dual decomposition has been used to derive ef-
ficient practical resolution methods in NLP, mostly
for machine translation and parsing, see (Rush
et al., 2010) for an overview and (Koo et al., 2010)
for an application to dependency parsing.

To accelerate the resolution, our method re-
lies heavily on problem reduction (Beasley, 1993),
which uses the primal/dual bounds to filter out
suboptimal assignments. Exact pruning based on
duality has already been studied in parsing, with
branch and bound (Corro et al., 2016) or column
generation (Riedel et al., 2012) and in machine
translation with beam search (Rush et al., 2013).

8 Conclusion

We presented a novel framework for the joint
task of supertagging and parsing by a reduction
to GMSA. Within this framework we developed a
model able to produce discontinuous constituents.
The scoring model can be decomposed into tag-
ging and dependency parsing and thus may rely
on advances in those active fields.

This work could benefit from several exten-
sions. Bigram scores on spines could be added
at the expense of a third subproblem in the dual
objective. High-order scores on arcs like grand-
parent or siblings can be handled in subproblem
P2 with the algorithms described in (Koo et al.,
2010). In this work, the parameters are learned as
separate models. Joint learning in the max-margin
framework (Komodakis, 2011; Komodakis et al.,
2015) may model interactions between vertex and
arc weights better and lead to improved accuracy.
Finally, we restricted our grammar to spinal trees
but it could be possible to allow full lexicalized
TAG-like trees, with substitution nodes and even
obligatory adjunction sites. Derivations compat-

ible with the TAG formalism (or more generally
LCFRS) could be recovered by the use of a con-
strained version of MSA (Corro et al., 2016).

Acknowledgements

We thank the anonymous reviewers for their in-
sightful comments. We thank Laura Kallmeyer
and Kilian Evang for providing us with the script
for the discontinuous PTB. First author is sup-
ported by a public grant overseen by the French
National Research Agency (ANR) as part of
the Investissements d’Avenir program (ANR-10-
LABX-0083). Second author, supported by a pub-
lic grant overseen by the French ANR (ANR-16-
CE33-0021), completed this work during a CNRS
research leave at LIMSI, CNRS / Université Paris
Saclay.

References
Miguel Almeida and Andre Martins. 2013. Fast and ro-

bust compressive summarization with dual decom-
position and multi-task learning. In Proceedings
of the 51st Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Pa-
pers), pages 196–206, Sofia, Bulgaria. Association
for Computational Linguistics.

John Beasley. 1993. Modern heuristic techniques for
combinatorial problems, chapter Lagrangian relax-
ation. John Wiley & Sons, Inc.

Sabine Brants, Stefanie Dipper, Peter Eisenberg, Sil-
via Hansen-Schirra, Esther König, Wolfgang Lezius,
Christian Rohrer, George Smith, and Hans Uszkor-
eit. 2004. Tiger: Linguistic interpretation of a ger-
man corpus. Research on language and computa-
tion, 2(4):597–620.

Xavier Carreras and Michael Collins. 2009. Non-
projective parsing for statistical machine translation.
In Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing, pages
200–209, Singapore. Association for Computational
Linguistics.

Xavier Carreras, Michael Collins, and Terry Koo. 2008.
TAG, dynamic programming, and the perceptron
for efficient, feature-rich parsing. In CoNLL 2008:
Proceedings of the Twelfth Conference on Com-
putational Natural Language Learning, pages 9–
16, Manchester, England. Coling 2008 Organizing
Committee.

Djork-Arné Clevert, Thomas Unterthiner, and Sepp
Hochreiter. 2016. Fast and accurate deep network
learning by exponential linear units (ELUs). In Pro-
ceedings of the 2016 International Conference on
Learning Representations.

1652



Maximin Coavoux and Benoit Crabbé. 2017. Incre-
mental discontinuous phrase structure parsing with
the gap transition. In Proceedings of the 15th Con-
ference of the European Chapter of the Association
for Computational Linguistics: Volume 1, Long Pa-
pers, pages 1259–1270. Association for Computa-
tional Linguistics.

Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of the
35th Annual Meeting of the Association for Com-
putational Linguistics, pages 16–23, Madrid, Spain.
Association for Computational Linguistics.

Caio Corro, Joseph Le Roux, Mathieu Lacroix, An-
toine Rozenknop, and Roberto Wolfler Calvo. 2016.
Dependency parsing with bounded block degree
and well-nestedness via lagrangian relaxation and
branch-and-bound. In Proceedings of the 54th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 355–
366, Berlin, Germany. Association for Computa-
tional Linguistics.

Andreas van Cranenburgh, Remko Scha, and Rens
Bod. 2016. Data-oriented parsing with discontinu-
ous constituents and function tags. Journal of Lan-
guage Modelling, 4(1):57–111.

Dipanjan Das, André F. T. Martins, and Noah A.
Smith. 2012. An exact dual decomposition algo-
rithm for shallow semantic parsing with constraints.
In The First Joint Conference on Lexical and Com-
putational Semantics, pages 209–217, Montréal,
Canada. Association for Computational Linguistics.

Timothy Dozat and Christopher D. Manning. Deep bi-
affine attention for neural dependency parsing. Pro-
ceedings of the 2017 International Conference on
Learning Representations.

Jason Eisner. 2000. Bilexical grammars and their
cubic-time parsing algorithms. In New Develop-
ments in Natural Language Parsing, pages 29–62.
Kluwer Academic Publishers.

Kilian Evang and Laura Kallmeyer. 2011. PLCFRS
parsing of english discontinuous constituents. In
Proceedings of the 12th International Conference on
Parsing Technologies, pages 104–116, Dublin, Ire-
land. Association for Computational Linguistics.

Corinne Feremans, Martine Labbé, and Gilbert La-
porte. 1999. The generalized minimum spanning
tree: Polyhedra and branch-and-cut. Electronic
Notes in Discrete Mathematics, 3:45–50.

Daniel Fernández-González and André F. T. Martins.
2015. Parsing as reduction. In Proceedings of the
53rd Annual Meeting of the Association for Compu-
tational Linguistics and the 7th International Joint
Conference on Natural Language Processing (Vol-
ume 1: Long Papers), pages 1523–1533, Beijing,
China. Association for Computational Linguistics.

Johan Hall and Joakim Nivre. 2008. Parsing discon-
tinuous phrase structure with grammatical functions.
In Advances in Natural Language Processing: 6th
International Conference, GoTAL 2008 Gothenburg,
Sweden, August 25-27, 2008 Proceedings, pages
169–180, Berlin, Heidelberg. Springer Berlin Hei-
delberg.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Laura Kallmeyer, Timm Lichte, Wolfgang Maier,
Yannick Parmentier, Johannes Dellert, and Kilian
Evang. 2008. Tulipa: Towards a multi-formalism
parsing environment for grammar engineering. In
Coling 2008: Proceedings of the workshop on
Grammar Engineering Across Frameworks, pages
1–8, Manchester, England. Coling 2008 Organizing
Committee.

Diederik Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. In Proceedings
of The International Conference on Learning Repre-
sentations (ICLR).

Eliyahu Kiperwasser and Yoav Goldberg. 2016. Sim-
ple and accurate dependency parsing using bidirec-
tional lstm feature representations. Transactions
of the Association for Computational Linguistics,
4:313–327.

Nikos Komodakis. 2011. Efficient training for pair-
wise or higher order crfs via dual decomposition. In
Computer Vision and Pattern Recognition (CVPR),
2011 IEEE Conference on, pages 1841–1848. IEEE.

Nikos Komodakis, Nikos Paragios, and Georgios Tzir-
itas. 2007. MRF optimization via dual decompo-
sition: Message-passing revisited. In 2007 IEEE
11th International Conference on Computer Vision,
pages 1–8. IEEE.

Nikos Komodakis, Bo Xiang, and Nikos Paragios.
2015. A framework for efficient structured max-
margin learning of high-order mrf models. IEEE
transactions on pattern analysis and machine intel-
ligence, 37(7):1425–1441.

Terry Koo, Alexander M. Rush, Michael Collins,
Tommi Jaakkola, and David Sontag. 2010. Dual
decomposition for parsing with non-projective head
automata. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1288–1298, Cambridge, MA. Associa-
tion for Computational Linguistics.

Joseph Le Roux, Antoine Rozenknop, and Jennifer
Foster. 2013. Combining PCFG-LA models with
dual decomposition: A case study with function la-
bels and binarization. In Proceedings of the 2013
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1158–1169, Seattle, Wash-
ington, USA. Association for Computational Lin-
guistics.

1653



Claude Lemaréchal. 2001. Lagrangian relaxation. In
Computational combinatorial optimization, pages
112–156. Springer.

Zhenghua Li, Min Zhang, Wanxiang Che, Ting Liu,
Wenliang Chen, and Haizhou Li. 2011. Joint mod-
els for chinese pos tagging and dependency parsing.
In Proceedings of the 2011 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1180–1191, Edinburgh, Scotland, UK. Association
for Computational Linguistics.

Wolfgang Maier. 2015. Discontinuous incremental
shift-reduce parsing. In Proceedings of the 53rd An-
nual Meeting of the Association for Computational
Linguistics and the 7th International Joint Confer-
ence on Natural Language Processing (Volume 1:
Long Papers), pages 1202–1212. Association for
Computational Linguistics.

Mitchell Marcus, Grace Kim, Mary Ann
Marcinkiewicz, Robert MacIntyre, Ann Bies,
Mark Ferguson, Karen Katz, and Britta Schas-
berger. 1994. The penn treebank: annotating
predicate argument structure. In HLT’94: Pro-
ceedings of the workshop on Human Language
Technology, pages 114–119, Morristown, NJ, USA.
Association for Computational Linguistics.

Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceed-
ings of Human Language Technology Conference
and Conference on Empirical Methods in Natural
Language Processing, pages 523–530, Vancouver,
British Columbia, Canada. Association for Compu-
tational Linguistics.

Young-Soo Myung, Chang-Ho Lee, and Dong-Wan
Tcha. 1995. On the generalized minimum spanning
tree problem. Networks, 26(4):231–241.

Graham Neubig, Chris Dyer, Yoav Goldberg, Austin
Matthews, Waleed Ammar, Antonios Anastasopou-
los, Miguel Ballesteros, David Chiang, Daniel
Clothiaux, Trevor Cohn, Kevin Duh, Manaal
Faruqui, Cynthia Gan, Dan Garrette, Yangfeng Ji,
Lingpeng Kong, Adhiguna Kuncoro, Gaurav Ku-
mar, Chaitanya Malaviya, Paul Michel, Yusuke
Oda, Matthew Richardson, Naomi Saphra, Swabha
Swayamdipta, and Pengcheng Yin. 2017. Dynet:
The dynamic neural network toolkit. arXiv preprint
arXiv:1701.03980.

Boris T Polyak. 1987. Introduction to optimization.
Optimization Software.

Petrica Claudiu Pop. 2002. The generalized minimum
spanning tree problem. Twente University Press.

Petrica Claudiu Pop. 2009. A survey of different in-
teger programming formulations of the generalized
minimum spanning tree problem. Carpathian Jour-
nal of Mathematics, 25(1):104–118.

Sebastian Riedel, David Smith, and Andrew McCal-
lum. 2012. Parse, price and cut—delayed column
and row generation for graph based parsers. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 732–
743, Jeju Island, Korea. Association for Computa-
tional Linguistics.

Alexander Rush, Yin-Wen Chang, and Michael
Collins. 2013. Optimal beam search for machine
translation. In Proceedings of the 2013 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 210–221, Seattle, Washington, USA.
Association for Computational Linguistics.

Alexander M Rush, David Sontag, Michael Collins,
and Tommi Jaakkola. 2010. On dual decomposition
and linear programming relaxations for natural lan-
guage processing. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1–11, Cambridge, MA. Associa-
tion for Computational Linguistics.

A. Schrijver. 2003. Combinatorial Optimization -
Polyhedra and Efficiency. Springer.

Djamé Seddah, Reut Tsarfaty, Sandra Kübler, Marie
Candito, D. Jinho Choi, Richárd Farkas, Jen-
nifer Foster, Iakes Goenaga, Koldo Gojenola Gal-
letebeitia, Yoav Goldberg, Spence Green, Nizar
Habash, Marco Kuhlmann, Wolfgang Maier, Joakim
Nivre, Adam Przepiórkowski, Ryan Roth, Wolf-
gang Seeker, Yannick Versley, Veronika Vincze,
Marcin Woliński, Alina Wróblewska, and Ville-
monte Eric de la Clergerie. 2013. Proceed-
ings of the Fourth Workshop on Statistical Pars-
ing of Morphologically-Rich Languages, chapter
Overview of the SPMRL 2013 Shared Task: A
Cross-Framework Evaluation of Parsing Morpho-
logically Rich Languages. Association for Compu-
tational Linguistics.

Libin Shen. 2006. Statistical LTAG Parsing. Ph.D. the-
sis, University of Pennsylvania.

Libin Shen and Aravind Joshi. 2005. Incremental
ltag parsing. In Proceedings of Human Language
Technology Conference and Conference on Empiri-
cal Methods in Natural Language Processing, pages
811–818, Vancouver, British Columbia, Canada.
Association for Computational Linguistics.

Libin Shen and Aravind Joshi. 2008. LTAG depen-
dency parsing with bidirectional incremental con-
struction. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Process-
ing, pages 495–504, Honolulu, Hawaii. Association
for Computational Linguistics.

Yannick Versley. 2014. Experiments with easy-first
nonprojective constituent parsing. In Proceedings
of the First Joint Workshop on Statistical Parsing
of Morphologically Rich Languages and Syntactic
Analysis of Non-Canonical Languages, pages 39–
53, Dublin, Ireland. Dublin City University.

1654


