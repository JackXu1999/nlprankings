Proceedings of Knowledge Resources for the Socio-Economic Sciences and Humanities associated with RANLP-17, pages 31–37,

31

Varna, Bulgaria, 7 Sep 2017.

https://doi.org/10.26615/978-954-452-040-3_005

Machine Learning Models

of Universal Grammar Parameter Dependencies

Dimitar Kazakov

Guido Cordoni

Dept. of Comp. Science

Dept. of Lang. and Ling. Sci.

University of York
dlk2@york.ac.uk

University of York
gc927@york.ac.uk

Andrea Ceolin

Dept. of Linguistics
U. of Pennsylvania

ceolin@sas.upenn.edu

Monica A. Irimia

Shin-Sook Kim

Dimitris Michelioudakis

Dept. of Comm. and Econ.

Dept. of Lang. and Ling. Sci.

Dept. of Lang. and Ling. Sci.

U. of Modena and Reggio Emilia

irimiamo@unimore.it

University of York
sk899@york.ac.uk

University of York
dm9540@york.ac.uk

Nina Radkevich

Dept. of Lang. and Ling. Sci.

Cristina Guardiano
Dip. Com. ed Econ.

Giuseppe Longobardi

Dept. of Lang. and Ling. Sci.

University of York
nr6920@york.ac.uk

UniMORE

cguardiano@unimore.it

University of York
gl6730@york.ac.uk

Abstract

The use of parameters in the descrip-
tion of natural language syntax has to
balance between the need to discrim-
inate among (sometimes subtly diﬀer-
ent) languages, which can be seen as
a cross-linguistic version of Chomsky’s
(1964) descriptive adequacy, and the
complexity of the acquisition task that
a large number of parameters would
imply, which is a problem for explana-
tory adequacy. Here we present a novel
approach in which a machine learning
algorithm is used to ﬁnd dependencies
in a table of parameters. The result is a
dependency graph in which some of the
parameters can be fully predicted from
others. These empirical ﬁndings can
be then subjected to linguistic analy-
sis, which may either refute them by
providing typological counter-examples
of languages not included in the origi-
nal dataset, dismiss them on theoret-
ical grounds, or uphold them as ten-
tative empirical laws worth of further
study.

1 Introduction

Parametric theories of generative grammar fo-
cus on the problem of a formal and principled
theory of grammatical diversity (Chomsky,
1981; Baker, 2001; Roberts, 2012). The basic
intuition of parametric approaches is that the

majority of observable syntactic diﬀerences
among languages are derived, usually through
complex deductive chains, from a smaller num-
ber of more abstract contrasts, drawn from
a universal list of discrete, and normally bi-
nary, options, called parameters. The relation
between observable patterns and the actual
syntactic parameters which vary across lan-
guages is quite indirect: syntactic parameters
are regarded as abstract diﬀerences often re-
sponsible for wider typological clusters of sur-
face co-variation, often through an intricate
deductive structure.
In this sense, the con-
cept of parametric data is not to be simplisti-
cally identiﬁed with that of syntactic pattern:
co-varying syntactic properties/patterns are in
fact the empirical manifestations of much more
abstract cognitive structures.

Syntactic parameters are conceived as deﬁn-
able by UG (i.e. universally comparable) and
set by each learner on the basis of her/his lin-
guistic environment. Open parameters, or any
set of more primitive concepts they can de-
rive from (Longobardi, 2005; Lightfoot, 2017),
deﬁne a variation space for biologically ac-
quirable grammars, set (a.k.a. closed) param-
eters specify each of these grammars. Thus,
the core grammar of every natural language
can in principle be represented by a string
of binary symbols (Clark and Roberts, 1993),
each coding the value of a parameter of UG.
The Parametric Comparison Method
(PCM, (Longobardi and Guardiano, 2009))
uses syntactic parameters to study historical

32

i.e.

relationships among languages. An impor-
tant aspect of parametric systems that is
particularly relevant to the present research
is that parameters form a pervasive network
of partial implications (Guardiano and Lon-
gobardi, 2005; Longobardi and Guardiano,
2009; Longobardi et al., 2013): one particular
value of some parameter A, but not the other,
often entails the irrelevance of parameter B,
whose consequences,
the corresponding
surface patterns, become predictable. Under
such conditions, B becomes redundant and
will not be set at all by the learner. The
PCM system makes such interdependencies
explicit:
in our notation, he symbols + and
− are used to represent the binary value
of each parameter; the symbol ‘0’,
instead,
encodes the neutralising eﬀect of implicational
cross-parametric dependencies,
cases in
which the content of a parameter is either
entirely predictable, or irrelevant altogether.
The conditions which must hold for each
parameter not to be neutralised are expressed
in a Boolean form, i.e., either as simple states
of another parameter (or negation thereof),
or as conjunctions or disjunctions of values of
other parameters.

i.e.

The PCM has shown that an important
eﬀect of the pervasiveness of parameter in-
terdependencies is a noticeable downsizing of
the space of grammatical variation: accord-
ing to some preliminary experiments (Borto-
lussi et al., 2011), the number of possible lan-
guages generated from a given set of indepen-
dent binary parameters is reduced from 1018
to 1011 when their interdependencies are taken
into account. This also crucially implies a no-
ticeable reduction of the space of possible lan-
guages that a learner has to navigate when
acquiring a language.

Here we adopt an empirical, data-driven ap-
proach to the task of identifying parameter de-
pendencies, which has been implemented on
a database of 71 languages described through
the values of 91 syntactic parameters (see Ap-
pendix A) expressing the internal syntax of
nominal structures. Our results show that
applying machine learning techniques to the
data reveals previously unknown dependencies
between parameters, which could potentially
lead to a further signiﬁcant reduction of the

if P1 = + and P2 = − then
P3 = +
else
P3 = −

Figure 1: Parameter dependency model exam-
ple

search space of possible languages.

This paper sets out to identify parameters
whose entire range of values can be fully pre-
dicted from the values of other parameters.
There is an important diﬀerence between pre-
viously published work on parameter depen-
dencies and this paper’s contribution, which
needs to be emphasised:
rather than state
that, for example, any language in which P1 =
+ will have a fully predictable value of P2 (a
fact which we encode as P2 = 0), we seek pa-
rameters whose value can be deduced in all
cases from the values of certain other param-
eters, e.g. as shown in the hypothetical ex-
ample in Figure 1. Should such a rule prove
to have universal validity, then parameter P3
would never oﬀer any advantage in separat-
ing any two languages, yet it could clearly still
play a useful role in describing them.

2 Learning Dependencies
We process our table of dimensions (#lang ×
#param) with the data mining package
WEKA (v.3.6.13) (Hall et al., 2009). More
speciﬁcally, we take the values of all parame-
ters but one for all languages (i.e. a dataset of
size (#lang × #param − 1), and learn a deci-
sion tree that predicts the value of the remain-
ing parameter from the values of the other pa-
rameters. (Typically, only a few are necessary
in each case.) This is repeated to produce a de-
cision tree for each of the parameters. The ma-
chine learning algorithm used was ID3 (Quin-
lan, 1986). The algorithm produces a deci-
sion tree, in which each leaf corresponds to the
value of the modelled parameter for the com-
bination of parameter values listed on the way
from the root to that leaf, e.g.: if F GN = −
and F GP = + then GCO = + (see Table 1).
Unlike some of the more sophisticated decision
tree learning algorithms, such as C4.5 (Quin-
lan, 1993), no postprocessing of the tree learnt

33

(such as pruning (Mitchell, 1997)) takes place,
and the tree remains an accurate, exact reﬂec-
tion of the training data. If the combination of
parameter values corresponding to one of the
leaves of the tree is not observed in the data,
the leaf contains the special label ‘null’ (see
the tree predicting GCO in Table 1).
In all
other cases, that is, whenever the leaf label is
‘+’, ‘-’ or ‘0’, this is supported by one or more
examples (languages) in the data.

Table 1: Examples of decision trees for param-
eters FGN and GCO
~~~~~~~~~~~~~~~~~
FGN:
if GCO = 0 then FGN = +
if GCO = + then FGN = -
if GCO = - then FGN = -
~~~~~~~~~~~~~~~~~~
GCO:
if FGN = 0 then GCO = null
if FGN = + then GCO = 0
if FGN = - then

;never occurs

if FGP = 0 then GCO = null;never occurs
if FGP = + then GCO = +
if FGP = - then GCO = -

~~~~~~~~~~~~~~~~~~

3 Results

The decision trees for all parameters were
used to produce a dependency graph in which
each vertex represents a parameter, and di-
rected edges link the parameters, whose val-
ues are needed to predict a given parameter,
with the node representing that parameter.
For instance, there are edges from both F GN
and F GP to GCO, as the decision tree for
GCO refers to the values of F GN and F GP .
Some of the decision trees are more complex,
making use of up to nine separate parame-
ters. The resulting graph is very complex (see
Fig. 2).Therefore, we also present a subset of
the graph (see Fig. 3), which only visualises
those trees predicting one parameter from the
value of one (as in the case of F GN) or two
other parameters (e.g. GCO). The fact that
some of the rules are missing from this graph
is not an issue: for each listed node, all of the
incoming edges are present, so that if we know
those parameters, we are guaranteed to know
the parameter they point to.

The interpretation of the graph is straight-
forward. For instance, looking at its top right

corner, one can deduce that for any language
in the dataset, it is enough to know the values
of parameters EZ3 and P LS in order to know
the value of EZ2, and therefore, of EZ1, too.
Knowing (the value of) F V P means one also
knows DM G and N SD;
if one knows both
F V P and DN N, the values of DN G, N SD,
DSN, DM P and DM G are fully predictable
for the given data set. In other words, 7 pa-
rameters (F V P , DN N, DN G, N SD, DSN,
DM P and DM G) can be reduced to just 2
without any loss of information.

Some of the rules identiﬁed by the algo-
rithm are not new, and are already contained
in the dataset, as encoded by the implicational
system described in Section 1. For instance,
the parameter RHM is encoded as 0 when
F GP = −, as the value of RHM is fully
predictable in those cases. When a decision
tree predicting FGP is learned, the result is as
follows: if RHM = 0 then F GP = − else
F GP = +.

Even the rest of the rules learned are still
just empirical ﬁndings that may change with
the addition of other examples of languages or
their validity may be questioned by linguists
on theoretical grounds.

Linguistic analysis of the results is ongoing,
and while no part of the results has been ac-
cepted as suﬃcient evidence to dispose of a
parameter, implication rules may be revised
on the basis of the decision trees learned, as in
the case of the parameter P LS. According to
its deﬁnition, the parameter “asks if in a lan-
guage without grammaticalized Number, a plu-
ral marker can also appear outside a nominal
phrase, marking a distributive relation between
the plural subject and the constituent bearing
it.” (E.g. P LS = + for Korean, but P LS = −
for Japanese.)

Prior to this research, there was an impli-
cation rule stating that P LS is neutralised
(that is, its value is predictable) for all combi-
nations of CGO and F GN values other than
CGO = − and F GN = −. This rule has
now been replaced with a new rule stating
that P LS is neutralised for all combinations
of values of F GM and F GN, except when
F GM = + and F GN = −, and the evidence
showing that the new rule is consistent with
the data came from the tree learned for P LS.

34

Figure 2: Full dependency graph

35

Figure 3: Partial dependency graph constructed from implications with up to two antecedents

36

R. Clark and I. Roberts. 1993. A computational
language learnability and language

model of
change. Linguistic Inquiry 24:299–345.

C. Guardiano and G. Longobardi. 2005. Para-
metric comparison and language taxonomy. In
M. Batllori, M. L. Hernanz, C. Picallo, and
F. Roca, editors, Grammaticalization and para-
metric variation, OUP, Oxford, pages 149–174.

M. Hall, E. Frank, G. Holmes, B. Pfahringer,
P. Reutemann, and I. H. Witten. 2009. The
WEKA data mining software. ACM SIGKDD
Explor. Newsl. 11:149–174.

D. W. Lightfoot. 2017. Discovering new variable
properties without parameters. Linguistic Anal-
ysis 41. Special edition: Parameters: What are
they? Where are they?

G. Longobardi. 2005.

A minimalist program
for parametric linguistics?
In H. Broekhuis,
N. Corver, M. Huybregts, U. Kleinhenz, and
J. Koster, editors, Organizing Grammar: Lin-
guistic Studies, Mouton de Gruyter, Berlin/New
York, pages 407–414.

G. Longobardi and C. Guardiano. 2009. Evidence
for syntax as a signal of historical relatedness.
Lingua 119(11).

G. Longobardi, C. Guardiano, G. Silvestri,
A. Boattini, and A. Ceolin. 2013. Toward a
syntactic phylogeny of modern Indo-European
languages.
Journal of Historical Linguistics
3(1):122–152.

T. Mitchell. 1997. Machine Learning. MIT.

R. Quinlan. 1986. Induction of decision trees. Ma-

chine Learning 1(1):81–106.

R. Quinlan. 1993. C4.5: Programs for Machine
Learning. Morgan Kaufmann Publ., San Mat-
teo, CA.

I.G. Roberts. 2012. On the nature of syntac-
tic parameters: a programme for research.
In
C. Galves, S. Cyrino, R. Lopes, F. Sandalo, and
J. Avelar, editors, Parameter Theory and Lan-
guage Change, Oxford University Press, Oxford,
pages 319–334.

4 Discussion
The results reported here show that applying
machine learning techniques to the data can
reveal previously unknown dependencies be-
tween parameters, leading to a potentially sig-
niﬁcant reduction in the search space of pos-
sible languages. The data contains more fea-
tures than data points, which can make for
the generation of spurious rules. The most
obvious way to counteract this, adding more
languages, comes at a very high cost, as it re-
quires well-trained linguists. One can also use
Occam’s Razor and limit the search space of
possible rules by limiting the number of an-
tecedents in the rule, e.g.
to two as we did
here. Yet another approach is to collect data
selectively for rules of interest, as only a small
number of parameters, e.g. 2–3 per language,
will be needed to test each rule.

This research could have important impli-
cations for the understanding of processes un-
derlying the faculty of language (potentially
strengthening the case for UG), with implica-
tions ranging from models of language acquisi-
tion to historical linguistics, where the syntac-
tic relatedness between two languages may be
more adequately measured. However, the ap-
proach requires a close collaboration between a
machine learning expert, discovering empirical
laws in the data, and a linguist who can test
their plausibility and theoretical consequences.
There is also an open theoretical computa-
tional learning challenge here presented by the
need to estimate the signiﬁcance of empirical
ﬁndings from a given number of examples (lan-
guages) with respect to the available range of
discriminative features in the dataset.

References
M. Baker. 2001. The Atoms of Language. Basic

Books, New York.

L. Bortolussi, G. Longobardi, C. Guardiano, and
A. Sgarro. 2011. How many possible languages
are there?
In G. Bel-Enguix, V. Dahl, and
M.D. Jim´enez-L´opez, editors, Biology, Compu-
tation and Liguistics, IOS, Amsterdam, pages
168–179.

N. Chomsky. 1964. Current issues in linguistic the-

ory. Mouton, The Hague.

N. Chomsky. 1981. Lectures on Government and

Binding. Foris, Dordrecht.

37

variable person

gramm. person

plurality spreading

feature spread to N

Appendix A: List of Parameters
FGP
FGM gramm. Case
FPC
gramm. perception
FGT gramm. temporality
FGN gramm. number
GCO gramm. collective number
PLS
FND number in D
NOD NP over D
FSN
FNN number on N
SGE
semantic gender
FGG gramm. gender
CGB unbounded sg N
DGR gramm. amount
DGP gramm. text anaphora
CGR strong amount
NSD strong person
FVP
DGD gramm. distality
DPQ free null partitive Q
DCN article-checking N
DNN null-N-licensing art
DIN
FGC gramm. classiﬁer
DBC strong classiﬁer
GSC
NOE N over ext. arg.
DMP def matching pronominal possessives
DMG def matching genitives
GCN Poss◦-checking N
GFN Gen-feature spread to Poss◦
GAL Dependent Case in NP
GUN uniform Gen
EZ1
EZ2
EZ3
GAD adpositional Gen
GFO GenO
PGO partial GenO
GFS GenS
GIT
GSI
ALP
GST
GEI
GNR non-referential head marking
HMP NP-heading modiﬁer

Genitive-licensing iterator
grammaticalised inalienability
alienable possession
grammaticalised Genitive
Genitive inversion

generalized linker
non-clausal linker
non-genitive linker

D-controlled inﬂ. on N

c-selection

structured APs
structured cardinals

AST
STC
GPC gender polarity cardinals
PMN personal marking on numerals
CQU cardinal quantiﬁers
PCA number spread through cardinal ad-

FFS
ADI
PSC

jectives
feature spread to structured APs
D-controlled inﬂ. on A
number spread from cardinal quan-
tiﬁers

RHM Head-marking on Rel
FRC verbal relative clauses
NRC nominalised relative clause
NOR NP over verbal relative clauses/

adpositional genitives

P over complement

AER relative extrap.
ARR free reduced rel
DOR def on relatives
NOP NP over non-genitive arguments
PNP
NPP N-raising with obl. pied-piping
NGO N over GenO
NOA N over As
NM2 N over M2 As
NM1 N over M1 As
EAF
fronted high As
NON N over numerals
FPO feature spread to genitive postposi-

tions

clitic poss.

ACM class MOD
DOA def on all +N
NEX gramm. expletive article
NCL
PDC article-checking poss.
ACL
APO adjectival poss.
WAP wackernagel adjectival poss.
AGE adjectival Gen
OPK obligatory possessive with kinship

enclitic poss. on As

nouns
split deictic demonstratives

TSP
TSD split demonstratives
TAD adjectival demonstratives
TDC article-checking demonstratives
TLC Loc-checking demonstratives
TNL NP over Loc
XCN conjugated nouns

