



















































Top-down Tree Structured Decoding with Syntactic Connections for Neural Machine Translation and Parsing


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 401–413
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

401

Top-down Tree Structured Decoding with Syntactic Connections for
Neural Machine Translation and Parsing

Jetic Gū, Hassan S. Shavarani, Anoop Sarkar
TASC 9404, 8888 University Drive,

Simon Fraser University,
Burnaby, BC V5A 1S6, Canada

{jeticg, sshavara, anoop}@sfu.ca

Abstract
The addition of syntax-aware decoding in
Neural Machine Translation (NMT) systems
requires an effective tree-structured neural net-
work, a syntax-aware attention model and a
language generation model that is sensitive to
sentence structure. We exploit a top-down
tree-structured model called DRNN (Doubly-
Recurrent Neural Networks) first proposed by
Alvarez-Melis and Jaakola (2017) to create
an NMT model called Seq2DRNN that com-
bines a sequential encoder with tree-structured
decoding augmented with a syntax-aware at-
tention model. Unlike previous approaches
to syntax-based NMT which use dependency
parsing models our method uses constituency
parsing which we argue provides useful in-
formation for translation. In addition, we
use the syntactic structure of the sentence to
add new connections to the tree-structured
decoder neural network (Seq2DRNN+SynC).
We compare our NMT model with sequential
and state of the art syntax-based NMT models
and show that our model produces more flu-
ent translations with better reordering. Since
our model is capable of doing translation and
constituency parsing at the same time we also
compare our parsing accuracy against other
neural parsing models.

1 Introduction
Neural machine translation (NMT) models were
initially proposed as extensions of sequential neu-
ral language models (Sutskever et al., 2014; Cho
et al., 2014; Bahdanau et al., 2015) or convolu-
tions over n-grams in the decoder (Kalchbren-
ner and Blunsom, 2013). Early methods for dis-
criminative training of machine translation models
showed that the loss functions for translation were
not sensitive to the production of certain impor-
tant words such as verbs, without which the out-
put sentence might be uninterpretable by humans.
A good solution was to penalise such bad outputs

using tree structures which get very low scores
if important words like verbs are missing (Chi-
ang, 2005; Zollmann and Venugopal, 2006; Gal-
ley et al., 2006). To this end, there has been a
push to incorporate some syntax into NMT mod-
els: Sennrich and Haddow (2016) incorporate
POS tags and dependency information from the
source side of a translation pair in NMT models.
Stahlberg et al. (2016) use source language syn-
tax to guide the decoder of an NMT system to
follow hierarchical structures (Hiero) rules (Chi-
ang, 2005). Eriguchi et al. (2016) and Bastings
et al. (2017) use tree-structured encoders to exploit
source language syntax. Aharoni and Goldberg
(2017) take the approach of serialising the parse
trees to use in a sequential decoder. Eriguchi et al.
(2017) propose an NMT+RNNG model, which ex-
plores the possibilities of using dependency syntax
trees from the target language using StackLSTMs
(Dyer et al., 2015, 2016) to aid a sequential de-
coder. These approaches showed promising im-
provements in translation quality but all the mod-
els in previous work, even the model in Eriguchi
et al. (2017) which uses RNNG, are bottom-up
tree structured decoders.

In contrast, we use a top-down tree-structured
model called DRNN (Doubly-Recurrent Neural
Networks) first proposed by Alvarez-Melis and
Jaakkola (2017) to model structural syntactic in-
formation for NMT. We call our novel NMT
model Seq2DRNN, using DRNNs as a tree-
structured decoder combined with a sequential en-
coder and a novel syntax-aware attention model.

All the previous work in syntax-aware NMT
mentioned above has focused on dependency pars-
ing as the syntactic model. In contrast, we wish to
pursue phrase structure (aka constituency) based
syntax-based NMT. We provide some analysis that
shows that constituency information can help re-
cover information in NMT decoding.



402

We perform extensive experiments comparing
our model against other state-of-the-art sequence
to sequence and syntax-aware NMT models and
show that our model can improve translation qual-
ity and reordering quality. The model performs
translation and constituency parsing simultane-
ously so we also compare our parsing accuracy to
other neural parsing models.

2 Model Description

In this paper, source sentence will be written
as f = x1, x2, ..., xn, target sentence as e =
y1, y2, ..., ym where yj is a word. Additionally,
pk represents a non-terminal symbol (constituent,
phrase) in the target sentence constituency tree.
[v;u] stands for concatenation of vectors v and u.
W(x) is the word embedding of word x.

The design of our NMT system follows the
encoder-decoder model (also known as a sequence
to sequence model) proposed by Cho et al. (2014)
and Sutskever et al. (2014). Our system uses
a standard bidirectional gated RNN (BiLSTM or
bidirectional Long Short-Term Memory) (Huang
et al., 2015) as the encoder and our proposed tree-
structured RNN as the decoder.

2.1 Sequence to Sequence NMT (Seq2Seq)

Neural machine translation models generally con-
sist of an encoder, a decoder and an attention
model (Luong et al., 2015; Cho et al., 2015). The
encoder is used to produce hidden representations
of the source sentence, which is fed into the de-
coder along with the attention information to pro-
duce the translation output sequence.

A common approach is to use bidirectional
LSTMs as encoder, produce forward hidden states−→
h i and backward hidden states

←−
h i, and the final

representation henci is the concatenation of both:

−→
h i =

−−→
RNNenc(

−→
h i−1,Wx(xi))←−

h i =
←−−
RNNenc(

←−
h i+1,Wx(xi))

henci = [
−→
h i;
←−
h i]

(1)

The decoder takes the output of the encoder
and generates a sequence in target language. The
attention mechanism provides additional context
vectors cj which is a weighted average contribu-
tion of each hi source side encoding.

hdecj = RNNdec(h
dec
j−1, [oj−1; cj ])

oj = softmax(Uhdecj + b)
(2)

Here, U is the readout matrix and b is the bias
vector. oj is the output word embedding.

2.2 NMT with a Tree-Structured Decoder
(Seq2DRNN)

The output translation from a translation system
should convey the same meaning as the input. This
includes the correct word choices but also the right
information structure. Sentence structure can be
viewed as starting with an action or state (de-
scribed via verbs or other predicates) and the en-
tities or propositions involved in that activity or
state (usually described via arguments to verbs).
Thus certain words in the output translation, like
verbs, are crucial to the understanding of the tar-
get language sentence but only provide marginal
value in n-gram matching evaluations like the
BLEU score. Tree representations, produced via
dependency parsing and constituency parsing, are
useful because they are sensitive to this informa-
tion structure. Our tree-structured decoder uses
a neural network to generate trees (described in
§2.2.1), which is incorporated into an NMT model
(our novel encoder-decoder model is in §2.2.2)
which translates and produces a parse tree. Our
new Syntactic Connection method (SynC) is de-
scribed in §2.2.4 which is combined with the
Seq2DRNN model (Seq2DRNN+SynC) and the
attention mechanism (§2.2.3).

2.2.1 Doubly-Recurrent Neural Network

The Doubly-Recurrent Neural Network model
(Alvarez-Melis and Jaakkola, 2017) takes a vec-
tor representation as input and generates a tree.
Alvarez-Melis and Jaakkola (2017) show that the
DRNN model can effectively reconstruct trees
but they do not use DRNNs within a full-scale
NMT system. We also use DRNNs for phrase-
structure (aka constituency) tree structures rather
than dependency trees as in previous work. DRNN
decoding proceeds top-down; the generation of
nodes at depth d depend solely on the state of
nodes at depth < d. Unlike previous work in
tree-structured decoding for NMT by Dyer et al.
(2016) and Eriguchi et al. (2017), the output sen-
tence generation is not done in sequence, where
the target word yj is generated after all y<j are
generated. DRNN first predicts the structure of
the sentence and then expands each component to
predict words. When generating yj , information
regarding the structure of words from 1 to j − 1
and j +1 to m can be used to aid prediction of yj .

A DRNN consists of two recurrent neural net-
work units, which separately process ancestral and



403

fraternal information about nodes in the tree. As-
suming a node is v, its immediate parent node is
P (v) and its closest sibling on the left side (ap-
pears in the target language sequence just before
v) is S(v). The label of node v is zv. Then the an-
cestral hidden representation ha and fraternal rep-
resentation hf of a node are calculated with Equa-
tion 3.

hav = RNN
a
dec(h

a
P (v), zP (v))

hfv = RNN
f
dec(h

f
S(v), zS(v))

(3)

hav and hfv are then combined to produce the hid-
den state of node v for prediction (predictive hid-
den state hv, Equation 4), which is used to predict
the labels of node v.

hv = tanh(Ufhfv + U
ahav) (4)

During the label prediction, DRNN first makes
topological decisions: whether (i) the current node
is a leaf node (node with no children, αv); then (ii)
whether the current node has siblings on its right-
hand side (γv). Both predictions are done using
sigmoid activations:

oav = σ(u
ahv)

αv = 1 if oav is activated.
(5)

ofv = σ(u
fhv)

γv = 1 if ofv is activated.
(6)

Then, label representation ov is predicted using
αv and γv, and the predictive hidden state hv:

ov = softmax(Uohv + αvua + γvuf ) (7)
At inference time each node at the same depth is

expanded independently, therefore the whole pro-
cess can be parallelised. This parallelism advan-
tage is not observed in any of the sequential de-
coders that generate output sequence strictly from
left to right nor from right to left (§4 has more dis-
cussion).

2.2.2 Parsing and Translating with DRNN

A DRNN is capable of producing a tree structure
with labels given an input vector representation. If
we train the DRNN to produce parse trees from
the output of an encoder RNN, this system will
be able to translate and parse at the same time.
(Alvarez-Melis and Jaakkola, 2017) in their paper
provided a proof-of-concept NMT experiment us-
ing dependency trees. Instead of an single RNN
unit to process fraternal information they had mul-
tiple for modelling the fraternal information on
syntactic tree structure because dependency parse

h^a

S

h^f

h^a

NP

h^a

VP

Andrei h^fLikes Cheese

h^a

h^a

Figure 1: Seq2DRNN on Constituency Tree

Encoder

Ich bin Doktor

Decoder

S

NP VP

I am NP

a doctor

hiddenRep

Context

Figure 2: Seq2DRNN Encoder-Decoder

trees differentiate between left and right children.
The model itself also disregards the sequential-
ity of natural language, and lack attention mech-
anisms to make it work in exchange for a strict
top-bottom decoding procedure.

We use constituency parse trees to represent
sentences in the target language (Figure 1) be-
cause constituency or phrase-structure trees are
more amenable to top-down derivation compared
to dependency trees. It is also easier to model for
DRNN, and presumably more capable at handling
unknown words which is common in NMT sys-
tems with limited vocabulary size.

Each node on the tree represents either a termi-
nal symbol (a word) or a non-terminal symbol (a
clause or phrase type). The sub-tree dominated by
a non-terminal node is the clause or phrase identi-
fied with this non-terminal node label.

A conventional bidirectional RNN (BiLSTM)
encoder (Cho et al., 2014; Sutskever et al., 2014)
is used to produce hidden states for the decoder
(see Figure 2).

We use breadth-first search to implement the
Seq2DRNN decoder. Two queues are used here:
current queue which is the queue containing all of
the nodes on the currently being processed depth,
and next queue with nodes on the next depth (Al-
gorithm 1 has all the details).

The decoding process starts from top to bottom,
from root to its children, then to its grandchildren,
and so on until the leaf nodes which are the output



404

words.
In our implementation, sentence clauses (S

nodes) are generated as the children of the root
node to generalise over sentence types and in case
there are multiple sentences in a single translation
pair.

Initially, the current queue will only have one
entry: the root node, which is initialised with the
hidden representation of the source sentence.

Each node in the current queue is expanded in
the following manner: first generate all of its sib-
lings and add them to the current queue, and if any
node happens to be non-terminal, generate its first
child and add it to the next queue. After the current
queue is empty, make next the new current queue
and start working on nodes at the next depth.

For training, we use back-propagation through
trees using the approach in Goller and Küchler
(1996). In the forward pass, the source sentence is
encoded into a hidden representation and fed into
the decoder. The decoder generates the tree, pre-
dicts the labels of every node from root to leaves.
Then in the backward pass, gradients are calcu-
lated and used to update the parameters. The loss
calculation includes losses in topological predic-
tions: oav and o

f
v (Equations 5 and 6) and label pre-

dictions: ov (Equation 7).

Loss(e) =
∑
v

Losslabel(ov, ôv)+

α
∑
v

Losstopo(oav, ô
a
v)+

α
∑
v

Losstopo(ofv ,
ˆ
ofv )

(8)

Here α is a hyper-parameter.

2.2.3 Attention Mechanism

Attention mechanisms usually work by adding an
additional context vector during label prediction.
We use a variation of an existing attention mech-
anism proposed by Luong et al. (2015). In our
attention model, we produce a context vector cv
for every node v by looking at all hidden states
produced by the encoder henci , then calculating the
weights and adding up the weighted hidden states.

weightv,i = Va tanh(Wahv + Uah
enc
i ) ∈ R (9)

cv =
n∑

i=1

ˆweightv,ih
enc
i (10)

After the calculation in Equation 9, the weights
are normalised with a softmax function before be-

Algorithm 1 Seq2DRNN Decoder
1: procedure DECODE(hiddenRep)
2: currentQueue← Node from hiddenRep
3: nextQueue← empty
4: loop:
5: if currentQueue is not empty then
6: node← currentQueue.pop()
7: Generate labels of node
8: if node has siblings then
9: currentQueue← sibling(node)

10: if node has children then
11: nextQueue← child(node)
12: goto loop

. all nodes at current depth are generated

. move on to the next depth
13: if nextQueue is not empty then
14: currentQueue← nextQueue
15: nextQueue← empty
16: goto loop.

. both queues should be empty now

ing used to calculate the context vector. The atten-
tion module allows the generation of labels to pay
more attention to specific token representations of
words in the input sentence.

2.2.4 SynC: Syntactic Connections for Lan-
guage Generation (Seq2DRNN+SynC)

A conventional Seq2Seq model uses an RNN lan-
guage model (Cho et al., 2014; Sutskever et al.,
2014) conditioned on the input representation pro-
duced by the encoder to generate the output one
word at a time (Equation 11). The prediction of a
word yj is directly conditioned on previously gen-
erated words where cj is the context vector.

P(e) =
∏
j

P(yj |y<j , cj) (11)

The problem with this word-level language
model is that it treats a sentence as a plain se-
quence of symbols regardless of its syntactic con-
struction. Sentences may contain multiple subor-
dinate clauses and their boundaries are not well-
modelled by sequential language models.

We propose a new method to connect the hid-
den units in the Seq2DRNN decoder that pays at-
tention to contextual tree relationships. The pre-
diction of the representation of a word or a con-
stituent zj (if a constituent then pj , if a word then
yj) is defined as follows:

P(zj | y<j , cj) =
P(zj | y<j , yk(∀k, zj ∈ pk∨
precedes(yk, zj)) | cj)

(12)

The generation of the representation of a
word/constituent zj , which is part of the clause



405

Ancestral (Phrases that contain current)

S VP

Fraternal (Previous phrases)

NP(Hungry...)<SOS>

current 
node

Word-level

...

Figure 3: SynC example: the three types of information
explicitly modelled when generating the current node/word
likes in Andrei when starving likes cheese.

that contain it (zj ∈ pk), with clauses before which
(precedes(pk, zj)), is conditioned on the follow-
ing information: i) Word-level: previously gener-
ated words y<j ; ii) Ancestral Clause: the clauses
that contain the current word pk, i.e. (∀kzj ∈ pk);
iii) Fraternal Clause: the clauses that precede the
current clause pk, i.e. (∀k precedes(pk, zj)).

In practice, the generation of a node looks at the
following representations:

1. Word-level: an RNN unit that produces the
representation of previous words as a se-
quence y<j ;

2. Ancestral: treating the ancestors of the cur-
rent node as a sequence (from root to the im-
mediate parent), the representation of that se-
quence: pk(∀k, zj ∈ pk);

3. Fraternal: treating the previous siblings of
the current node as well as the previous sib-
lings of its parent node and so on as a se-
quence, the representation of that sequence:
pk(∀k, precedes(pk, zj)).

SynC creates connections in the tree-structured
decoder that pays attention to the structural con-
text of generation of each terminal or non-terminal
symbol in the phrase structure tree. For example
in English, it is common for verb phrases to follow
a noun phrase. But that noun phrase could itself be
a subordinate clause with its own verb phrases. In
this case, our goal is to explicitly model the fact
that the previous phrase is a noun phrase instead
of just the entire sequence of words.

SynC can be easily incorporated in the proposed
Seq2DRNN model (Seq2DRNN+SynC). In addi-
tion to the fraternal RNN unit that focuses on pre-
ceding sibling nodes, and the ancestral DRNN unit
that focus on parent nodes, a node would also look
at its parent’s previous sibling state (the hidden

S

NP VP

Andrei when starving Likes Cheese

Figure 4: SynC in action; when generating the word likes
in Andrei when starving likes cheese, the prediction will be
made knowing that the preceding clause is a noun phrase.

vector representation of preceding clauses from
the very beginning of the sentence). When a non-
terminal symbol is expanded into a sub-tree, it’s
first child will not have a previous sibling to pro-
vide fraternal information (S(v) = Null, as in
Equ 3). In this case, SynC establishes connection
between its first child and its parent’s fraternal in-
formation provider for such fraternal RNN state
(S(v) = S(P (v))).

hfv =


S(v) 6= Null, RNNfdec(h

f
S(v), zS(v))

S(v) = Null, S(v) := S(P (v)),

RNNfdec(h
f
S(v), zS(v))

(13)
An example is shown in Figure 4. In this case,
a word-level language model will regard starving
as the previous word, which is less helpful for the
prediction of a verb phrase likes cheese.

3 Experiments

3.1 Model Training

Experiments in this paper utilise constituency
trees on the target side, these trees are obtained
by using the Stanford Lexical Parser (Klein and
Manning, 2003a) which we chose for its speed and
accuracy prior to training.

This procedure of pre-parsing data is not re-
quired at test time, our NMT system would take
a sentence as input and produces the translation in
target language along with its constituency tree as
output.

We use the German-English dataset from
IWSLT2017 1 for our experiments, and tst2010-
2015 as the test set (Table 1).

To compare with other decoders that utilise
target-side syntactic information, we also evaluate
on three more datasets from News Commentary v8
using newstest2016 as testset (Table 2).

We replace all rarely occurring words with

1The International Workshop on Spo-
ken Language Translation Evaluation 2017:
https://sites.google.com/site/iwsltevaluation2017/TED-tasks

https://sites.google.com/site/iwsltevaluation2017/TED-tasks


406

Train pairs 226,572
Test pairs 8,079
Unique source words 128,857
Unique target words 61,566
Average source sentence length 21
Average target sentence length 20

Table 1: IWSLT2017 Dataset information

Language pair DE-EN CS-EN RU-EN
Train pairs 166,313 134,453 131,492
Test pairs 2,999 2,999 2,998
Uniq. src lex 149,318 153,173 159,074
Uniq. tgt lex 68,415 59,909 64,220
Avg. src len 25 22 25
Avg. tgt len 25 25 26

Table 2: News Commentary v8 Dataset information

BLEU RIBES Perplx.
Seq2Seq 22.83 81.5 1.828
Seq2DRNN 23.53 80.4 1.644
Seq2DRNN+SynC 25.36 82.6 1.750

Table 3: IWSLT17 Experiment results

UNK (Unknown) tokens. Only the top 50,000
most frequent words are kept.

3.2 Modelling details

The implementation of all models in this paper
is done using DyNet (Neubig et al., 2017a) with
Autobatching (Neubig et al., 2017b). We use
Long Short-Term Memory (LSTM) (Hochreiter
and Schmidhuber, 1997) as RNN units. Each
LSTM unit has 2 layers, with input and hidden
dimension of 256. We use a minibatches of 64
samples. We use early stopping mechanism for
all experiments and Adam optimiser (Kingma and
Ba, 2015) as trainer.

Note that this configuration is significantly
smaller in both dimension and batch size than
those presented in IWSLT2017 due to hardware
limitations. All experiments are carried out on a
single GTX 1080 Ti GPU with 11GB of VRAM.

3.3 Results

Table 3 and Table 4 has the BLEU (Papineni
et al., 2002) and RIBES (Isozaki et al., 2010)
scores. In our IWSLT2017 tests, both Seq2DRNN
and Seq2DRNN+SynC produce better results than
the Seq2Seq baseline model in terms of BLEU
scores, while Seq2DRNN+SynC also produces
better RIBES scores indicating better reordering
of phrases in the output. The Seq2DRNN+SynC
model performs better than the Seq2DRNN
model. Both Seq2Seq and Seq2DRNN+SynC are
able to produce results with lower perplexities

than the baseline Seq2Seq model on the test data.
In our News Commentary v8 tests, the same

relative performance from Seq2DRNN(SynC) can
be observed. The Seq2DRNN+SynC model is
also able to out-perform the Str2Tree model
proposed by Aharoni and Goldberg (2017) and
NMT+RNNG by Eriguchi et al. (2017) in most
cases. Note that Eriguchi et al. (2017) used de-
pendency information instead of constituency in-
formation as presented in our work and Aharoni
and Goldberg (2017)’s work.

Table 5 shows an example translation from all
of the models we use in our experiments. Seq2Seq
is able to translate with the correct vocabulary, but
the sentences are often syntactically awkward. As
the sentence length increases the syntactic fluency
of Seq2Seq gets worse. Seq2DRNN is able to
produce more syntactically fluent sentences since
each lowest sub-clause contains typically ≤ 5
words. Seq2DRNN+SynC produces the best re-
sults in this example: produces more syntactically
fluent sentences, chooses the right words in the
right place more frequently.

We also took several examples from our
IWSLT17 experiment and blank out certain nouns
by replacing them with unknown tokens (Table 6).
Note that in our training set, most sentences do
not have unknown tokens, and those that do only
have at most 1. Our assumptions of the observed
patterns in this case are: i) the proposed models
are more capable at handling unknown tokens; ii)
while Seq2DRNN is more capable at retaining the
structure of the sentence, it cannot rely on a wider
context to predict certain common phrases with
noises in the source sentence; iii) the proposed
Seq2DRNN+SynC model is more capable at han-
dling unknown words both in the sense of being
better at retaining sentence structure and handling
noisy input.

3.4 Attention Module

We visualise the attention weights of our
Seq2DRNN+SynC model. Attention §2.2.3 com-
putes a context vector for each node in the tree (a
weighted sum of the source side vector represen-
tations). For the translation pair in Figure 2, we
show the attention weight of each pair of word and
node (Equation 9) in Figure 5.

The addition of syntax nodes in the output en-
ables the attention model to be used more effec-
tively and is also valuable for visual inspection of
syntactic nodes in the output mapping to the input.



407

Dataset DE-EN CS-EN RU-EN
BLEU RIBES BLEU RIBES BLEU RIBES

Seq2Seq 16.61 73.8 11.22 69.6 12.03 69.6
Str2Tree 16.13 — 11.65 — 11.94 —
NMT+RNNG 16.41 75.0 12.06 70.4 12.46 71.0
Seq2DRNN 16.90 75.1 11.84 67.3 12.04 69.7
Seq2DRNN+SynC 17.21 75.8 12.11 70.3 12.96 71.1

Table 4: News Commentary v8 Experiment results. Seq2Seq and NMT+RNNG results are taken from Eriguchi et al. (2017),
Str2Tree (string-to-linearised-tree) results (no RIBES scores) come from Aharoni and Goldberg (2017) All numbers reported
here are of non-ensemble models.

Source wir wiederholten diese übung mit denselben studenten. was glauben sie passiert nun? nun
verstanden sie den vorteil des prototyping. so wurde aus demselben, schlechten team eines
unter den besten. sie produzierten die höchste konstruktion in der geringsten zeit.

Literal we repeated this exercise with the same students. now what do you believe happened? now they
understand the value of prototyping. so the same terrible team became one of the very best. they
produced the tallest construction in the shortest time.

Gold we did the exercise again with the same students. what do you think happened then? so now
they understand the value of prototyping. so the same team went from being the very worst to
being among the very best. they produced the tallest structures in the least amount of time.

Seq2Seq we repeated this with the same students. what happened you think differently? now, you know,
the advantage of the design of the cycle. so, the same one of the team of the team among the
best. it produced songs in the slightest building.

Seq2DRNN well repeat these queries with the same students. what do you think of this? now it understood
the advantage of the interests. that’s been made of the same thing of one thing. they produced
the highest construction of the best time at the best time.

Seq2DRNN+SynC we repeated this practice with the same students. what do you think happened? now, they
understood the value of prototyping. it was being made of the same thing of one of the best
ones. they produced the highest construction in the best time.

Table 5: Translation Sample. Gold is the reference, and Literal is produced by a bilingual German-English speaker.

Figure 5: Attention Module in Seq2DRNN+SynC. “ich”
means “I”, “bin” is “am”, “doktor”’s literal translation is
“doctor”. Darker colour means higher weight (relevance
score) as calculated in Equation 9. The values of each col-
umn sum up to 1. The attention weights in this example per-
fectly align with the appropriate clauses. Additional example
is provided in the appendix.

3.5 Parsing Quality

To evaluate the parsing quality, we follow the
approach by Vinyals et al. (2015) and train a
DRNN(SynC) model to produce English to En-
glish(Tree) translation. We use the same data and
experiment settings that Vinyals et al. (2015) used:
the Wall Street Journal Penn Treebank English
corpus with golden constituency structure, 256 for
input/hidden dimension and 3 layers of RNN. We
evaluate on section 23 of the aforementioned WSJ
data using EVALB2. The results are presented in
Table 7.

Although falling short behind more specifically
2 https://nlp.cs.nyu.edu/evalb/

designed models like the RNNG by Dyer et al.
(2016), our model is able to produce better results
than the LSTM+AD model proposed by Vinyals
et al. (2015), which is more comparable to ours
since they are also using an NMT model to do con-
stituency parsing. Since our work is more focusing
on the translation aspect, optimising and designing
a dedicated parser is slightly off-topic here. Nev-
ertheless, it is worth noting that in 50.89% of the
cases, Seq2DRNN+SynC was able to produce out-
put that perfectly matches the reference. The same
number for sentences with less than 40 words is
52.16%, while the F-measure increases to 90.5.
This shows Seq2DRNN(SynC) when doing pars-
ing can produce outputs of similar quality when
handling longer sentences.

We also do evaluation on our translation results
from the IWSLT dataset. Since translation re-
sults do not come with reference parse trees, we
parse the output of our decoder using the same
parser we used in our other experiments: the Stan-
ford Parser. Constituency parsing evaluation is
done using Precision/Recall/F1-scores on the out-
put constituent spans (unlabelled) and spans and
labels (labelled). The results are presented in Ta-
ble 8 and Table 9. The parser we use gets F1 score
of 87.04 on Penn Treebank English constituency

https://nlp.cs.nyu.edu/evalb/


408

0 src es war zeit zum abendessen und wir hielten auss-
chau nach einem restaurant.

die gute nachricht ist , dass die person , die das
gesagt hat ann coulter war .

S2S and it was time for dinner , and we were looking
for a restaurant.

the good news is that the person who said that was
ann coulter .

S2D it was the time for dinner , and we were looking
for a restaurant.

the good news is that the person who said that was
ann coulter .

S2D+L it was time for dinner , and we were looking for a
restaurant.

the good news is that the person who said that was
ann coulter .

1 src es war zeit zum abendessen und wir hielten auss-
chau nach einem UNK.

die gute UNK ist , dass die person , die das gesagt
hat ann coulter war .

S2S and it was time for dinner , and we thought we
were looking for a window.

the good news is that the person who said that
the teacher was ann coulter .

S2D it was the time for dinner , and we were looking
for a UNK pilot.

the good motivator is that the person who said that
was ann coulter .

S2D+L it was time for dinner , and we were looking for a
UNK.

the good news is that the person who said that was
ann coulter .

2 src es war zeit zum UNK und wir hielten ausschau nach
einem UNK.

die gute UNK ist , dass die UNK , die das gesagt
hat ann coulter war .

S2S and it was time for the time time , and we thought
we looked at a window search.

the good news is that the UNK that
the UNK , which was ann coulter .

S2D it was the second time , and we would look for a
UNK pilot.

the good motivator is that the UNK group who said
that was ann coulter.

S2D+L it was a time for UNK , and we were looking for a
UNK.

the good news is that the people who said that was
ann coulter .

3 src es war UNK zum UNK und wir hielten ausschau
nach einem UNK.

die gute UNK ist , dass die UNK , die das gesagt
hat UNK war .

S2S UNK was a time time and UNK , looking for a
UNK look for a look at a war.

UNK is not a cop ’s needs to be ozzie good to good
good , which is that the UNK UNK that said that it
was said .

S2D it was time of time and guerrilla . the good motivator is that the UNK planner that said
this was a UNK video.

S2D+L it was time for the tone and night to watch the
connection .

the good news is that the people that said that was
mandated .

Table 6: Unknown noun experiment samples. Substituted and correct nouns are marked in Bold, while incorrect elements
are marked in underline. Examples shown are: no UNK; 1 UNK; 2 UNKs; 3 UNKs. When there are no unknown tokens,
all three compared models are able to produce reasonably good if not identical translations. When there is only one UNK
token, Seq2DRNN often does not use the context to predict an appropriate word or phrase. In contrast, both the Seq2Seq and
Seq2DRNN+SynC were able to correctly predict that die gute UNK ist could be translated to the good news is. When there are
2 UNK tokens in the source sentence, Seq2Seq produces more incorrect predictions, Seq2DRNN makes some mistakes, while
Seq2DRNN+SynC is able to get the most parts correct. Finally, when we replace 3 nouns, all models fail to some degree while
Seq2Seq’s output is the worst.

Model F-measure
Baseline (Vinyals et al., 2015) < 70
LSTM+AD (Vinyals et al., 2015) 88.3
Petrov (2010) 91.8
Dyer et al. (2016) 92.4
Seq2DRNN 89.4
Seq2DRNN+SynC 89.9

Table 7: Parser scores. Numbers from (Vinyals et al., 2015)
are of non-ensemble models.

Unlabelled
Prec. Rec. F1

Seq2DRNN 96.87 96.93 96.90
Seq2DRNN+SynC 96.43 95.89 96.16

Table 8: IWSLT Translation result constituency unlabelled
scores. Reference parse trees obtained using Stanford Parser.

parsing (Klein and Manning, 2003b).
The presence of SynC in the decoder influences

Labelled
Prec. Rec. F1

Seq2DRNN 91.63 91.69 91.66
Seq2DRNN+SynC 90.73 90.22 90.48

Table 9: IWSLT Translation result constituency labelled
scores. Reference parse trees obtained using Stanford Parser.

parse tree construction: the Seq2DRNN+SynC F1
score is comparable but lower than Seq2DRNN.

4 Related Work

Recent research shows that modelling syntax is
useful for various neural NLP tasks. Dyer et al.
(2015, 2016); Vinyals et al. (2015); Luong et al.
(2016) have works on language modelling and
parsing, Tai et al. (2015) on semantic analysis, and
Zhang et al. (2016) on sentence completion, etc.

Eriguchi et al. (2017) showed that NMT model
can benefit from neural syntactical parsing mod-



409

els. Choe and Charniak (2016) showed that a neu-
ral parsing problem shares similarity to neural lan-
guage modelling problem, which forms a build-
ing block of an NMT system. We can then make
the assumption that structural syntactic informa-
tion utilised in neural parsing models should be
able to aid NMT, which is shown to be true here.

Zhang et al. (2016) proposed TreeLSTM which
is another structured neural decoder. TreeL-
STM is not only structurally more complicated
but also uses external classifiers. Dong and
Lapata (2016) also proposed a sequence-to-tree
(Seq2Tree) model for question answering. Both
of these models are not designed for NMT and
lack a language model. While operate from top-to-
bottom like Seq2DRNN(+SynC), TreeLSTM and
Seq2Tree produce components that lack sequen-
tial continuity which we have shown to be non-
negligible for language generation.

Aharoni and Goldberg (2017), Wu et al. (2017),
and Eriguchi et al. (2017) experimented with NMT
models that utilise target side structural syntax.
Aharoni and Goldberg (2017) treated constituency
trees as sequential strings (linearised-tree) and
trained a Seq2Seq model to produce such se-
quences. Wu et al. (2017) proposed SD-NMT,
which models dependency syntax trees by adding
a shift-reduce neural parser to a standard RNN
decoder. Eriguchi et al. (2017) in addition to
Wu et al. (2017)’s work, proposed NMT+RNNG
which uses a modified RNNG generator (Dyer
et al., 2016) to process dependency instead of
constituency information as originally proposed
by Dyer et al. (2016), making it consequently
a StackLSTM sequential decoder with additional
RNN units so it is still a bottom-up tree-structured
decoder rather than a top-down decoder like ours.
Nevertheless, all of these research showed that
target side syntax could improve NMT systems.
We believe these models could also be augmented
with SynC connections (with NMT+RNNG one
has to instead use constituency information).

5 Conclusions
We propose an NMT model that utilises target
side constituency syntax with a strictly top-down
tree-structured decoder using Doubly-Recurrent
Neural Networks (DRNN) incorporated into an
encoder-decoder NMT model. We propose a new
way of modelling language generation by estab-
lishing additional clause-based syntactic connec-
tions called SynC. Our experiments show that

our proposed models can outperform a strong se-
quence to sequence NMT baseline and several ri-
val models and do parsing competitively.

In the future we hope to incorporate source side
syntax into the model. We plan to explore the
applications of SynC in NMT with more struc-
tured attention mechanisms, and potentially a hy-
brid phrase-based NMT systems with SynC, in
which the model can benefit from SynC to be more
extensible when handling larger lexicons.

Acknowledgement
We would like to thank the anonymous reviewers
for their helpful remarks. The research was also
partially supported by the Natural Sciences and
Engineering Research Council of Canada grants
NSERC RGPIN-2018-06437 and RGPAS-2018-
522574 and a Department of National Defence
(DND) and NSERC grant DGDND-2018-00025
to the third author.

References
Roee Aharoni and Yoav Goldberg. 2017. Towards

string-to-tree neural machine translation. In Pro-
ceedings of the 55th Annual Meeting on Association
for Computational Linguistics.

David Alvarez-Melis and Tommi S. Jaakkola. 2017.
Tree-structured decoding with doubly-recurrent
neural networks. In International Conference on
Learning Representations.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In International Con-
ference on Learning Representations.

Joost Bastings, Ivan Titov, Wilker Aziz, Diego
Marcheggiani, and Khalil Sima’an. 2017. Graph
convolutional encoders for syntax-aware neural ma-
chine translation. In Empirical Methods in Natural
Language Processing.

David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of the 43rd Annual Meeting on Association
for Computational Linguistics, pages 263–270. As-
sociation for Computational Linguistics.

KyungHyun Cho, Aaron C. Courville, and Yoshua
Bengio. 2015. Describing multimedia content using
attention-based encoder-decoder networks. In IEEE
Transactions on Multimedia Special Issue on Deep
Learning for Multimedia Computing.

Kyunghyun Cho, Bart van Merrienboer, Çaglar
Gülçehre, Fethi Bougares, Holger Schwenk, and



410

Yoshua Bengio. 2014. Learning phrase representa-
tions using RNN encoder-decoder for statistical ma-
chine translation. In Empirical Methods in Natural
Language Processing.

Do Kook Choe and Eugene Charniak. 2016. Parsing as
language modeling. In Empirical Methods in Natu-
ral Language Processing, pages 2331–2336.

Li Dong and Mirella Lapata. 2016. Language to log-
ical form with neural attention. In Proceedings of
the 54rd Annual Meeting on Association for Com-
putational Linguistics.

Chris Dyer, Miguel Ballesteros, Wang Ling, Austin
Matthews, and Noah A Smith. 2015. Transition-
based dependency parsing with stack long short-
term memory. In Proceedings of the 53rd Annual
Meeting on Association for Computational Linguis-
tics.

Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros,
and Noah A. Smith. 2016. Recurrent neural net-
work grammars. In Proceedings of North American
Chapter of the Association for Computational Lin-
guistics.

Akiko Eriguchi, Kazuma Hashimoto, and Yoshimasa
Tsuruoka. 2016. Tree-to-sequence attentional neu-
ral machine translation. In 54th Annual Meeting of
the Association for Computational Linguistics.

Akiko Eriguchi, Yoshimasa Tsuruoka, and Kyunghyun
Cho. 2017. Learning to parse and translate improves
neural machine translation. In 55th Annual Meeting
of the Association for Computational Linguistics.

Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training
of context-rich syntactic translation models. In
Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics, pages 961–968. Association for Computa-
tional Linguistics.

Christoph Goller and Andreas Küchler. 1996. Learning
task-dependent distributed representations by back-
propagation through structure. In In Proc. of the
ICNN-96, pages 347–352. IEEE.

Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long
short-term memory. Neural Comput., 9(8):1735–
1780.

Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidi-
rectional LSTM-CRF models for sequence tagging.
CoRR, abs/1508.01991.

Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito
Sudoh, and Hajime Tsukada. 2010. Automatic eval-
uation of translation quality for distant language
pairs. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP ’10, pages 944–952, Stroudsburg, PA,
USA. Association for Computational Linguistics.

Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
continuous translation models. In Empirical Meth-
ods in Natural Language Processing, pages 1700–
1709.

Diederik P Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. In International
Conference on Learning Representations.

Dan Klein and Christopher D. Manning. 2003a. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics - Volume 1, ACL ’03, pages 423–
430, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.

Dan Klein and Christopher D. Manning. 2003b. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics - Volume 1, ACL ’03, pages 423–
430, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.

Minh-Thang Luong, Quoc V Le, Ilya Sutskever, Oriol
Vinyals, and Lukasz Kaiser. 2016. Multi-task se-
quence to sequence learning. In International Con-
ference on Learning Representations.

Minh-Thang Luong, Hieu Pham, and Christopher D.
Manning. 2015. Effective approaches to attention-
based neural machine translation. In Empirical
Methods in Natural Language Processing.

Graham Neubig, Chris Dyer, Yoav Goldberg, Austin
Matthews, Waleed Ammar, Antonios Anastasopou-
los, Miguel Ballesteros, David Chiang, Daniel
Clothiaux, Trevor Cohn, Kevin Duh, Manaal
Faruqui, Cynthia Gan, Dan Garrette, Yangfeng Ji,
Lingpeng Kong, Adhiguna Kuncoro, Gaurav Ku-
mar, Chaitanya Malaviya, Paul Michel, Yusuke
Oda, Matthew Richardson, Naomi Saphra, Swabha
Swayamdipta, and Pengcheng Yin. 2017a. Dynet:
The dynamic neural network toolkit. In arXiv
preprint arXiv:1701.03980.

Graham Neubig, Yoav Goldberg, and Chris Dyer.
2017b. On-the-fly operation batching in dy-
namic computation graphs. In CoRR, volume
abs/1705.07860.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic eval-
uation of machine translation. In Proceedings of
the 40th Annual Meeting on Association for Com-
putational Linguistics, ACL ’02, pages 311–318,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.

Slav Petrov. 2010. Products of random latent vari-
able grammars. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, HLT ’10, pages 19–27, Stroudsburg, PA,
USA. Association for Computational Linguistics.



411

Rico Sennrich and Barry Haddow. 2016. Linguistic in-
put features improve neural machine translation. In
In Proceedings of the First Conference on Machine
Translation.

Felix Stahlberg, Eva Hasler, Aurelien Waite, and Bill
Byrne. 2016. Syntactically guided neural machine
translation. In Proceedings of Association for Com-
putational Linguistics.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in neural information process-
ing systems, pages 3104–3112.

Kai Sheng Tai, Richard Socher, and Christopher D
Manning. 2015. Improved semantic representations
from tree-structured long short-term memory net-
works. In Proceedings of the 53rd Annual Meeting
on Association for Computational Linguistics.

Oriol Vinyals, Lukasz Kaiser, Terry Koo, Slav Petrov,
Ilya Sutskever, and Geoffrey Hinton. 2015. Gram-
mar as a foreign language. In Proceedings of the
28th International Conference on Neural Informa-
tion Processing Systems - Volume 2, NIPS’15, pages
2773–2781, Cambridge, MA, USA. MIT Press.

Shuangzhi Wu, Dongdong Zhang, Nan Yang, Mu Li,
and Ming Zhou. 2017. Sequence-to-dependency
neural machine translation. In Proceedings of the
55th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
698–707. Association for Computational Linguis-
tics.

Xingxing Zhang, Liang Lu, and Mirella Lapata. 2016.
Tree recurrent neural networks with application to
language modeling. In North American Chapter of
the Association for Computational Linguistics.

Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart parsing.
In Proceedings of the Workshop on Statistical Ma-
chine Translation, pages 138–141. Association for
Computational Linguistics.



412

A Additional Translation Samples (IWSLT2017 German-English)
The samples here are from our IWSLT2017 German-English testset. We compared the performances of
all our proposed models as well as the baseline Seq2Seq model in Table 10.

We provide an additional example of our attention module visualisation in Figure 6 and for parser in
Figure 7.

Source 1 m ist das höchste, was ich gesehen habe.
Literal 1 m is the highest that i’ve seen.
Reference thirty-nine inches is the tallest structure i’ve seen.
Seq2Seq and the highest thing is i’ve seen.
Seq2DRNN one is the highest thing i’ve seen at that.
Seq2DRNN+SynC feet is the highest thing i’ve seen.
Source ich weiß nicht . sie wollten in die zeit zurck , bevor es autos gab oder twitter

oder amerika sucht den superstar.
Literal i dont know. they want to go back in time, before there were automobiles or

twitter or america looking for superstar.
Reference i dont know. they want to go back before there were automobiles or twitter or

american idol.
Seq2Seq i don’t know. they were in the days, when they were cars before cars or the

earnings, or america, and the country.
Seq2DRNN i don’t want to know before time, they wanted to go back before the cars be-

fore they were cars or americans.
Seq2DRNN+SynC i don’t know. they wanted to go back in time, they wanted to go back into

the before, before there had cars or twitter visitors.
Table 10: Translation Samples. Gold is the reference, and Literal is produced by a bilingual German-English speaker.
The reason we include the literal translation is that sometimes the reference translation from the corpus can have additional
components or be non-literal translations.

Figure 6: Attention visualisation (Seq2DRNN+SynC). Darker colour means higher attention weight as defined in Equ 9. The
sentence is randomly selected from our IWSLT experiment.



413

Figure 7: Parser attention visualisation (Seq2DRNN+SynC). Darker colour means higher attention weight as defined in Equ 9.
The sentence is randomly selected from our PennTreebank experiment.


