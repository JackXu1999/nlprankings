



















































Lexicon Guided Attentive Neural Network Model for Argument Mining


Proceedings of the 6th Workshop on Argument Mining, pages 67–73
Florence, Italy, August 1, 2019. c©2019 Association for Computational Linguistics

67

Lexicon Guided Attentive Neural Network Model for Argument Mining

Jian-Fu Lin,1 Kuo Yu Huang,1 Hen-Hsen Huang,2,3 Hsin-Hsi Chen1,3
1 Department of Computer Science and Information Engineering,

National Taiwan University, Taiwan
2 Department of Computer Science, National Chengchi University, Taiwan

3 MOST Joint Research Center for AI Technology and All Vista Healthcare, Taiwan
{cflin,kyhuang}@nlg.csie.ntu.edu.tw,

hhhuang@nccu.edu.tw, hhchen@ntu.edu.tw

Abstract

Identification of argumentative components is
an important stage of argument mining. Lex-
icon information is reported as one of the
most frequently used features in the argument
mining research. In this paper, we propose
a methodology to integrate lexicon informa-
tion into a neural network model by atten-
tion mechanism. We conduct experiments on
the UKP dataset, which is collected from het-
erogeneous sources and contains several text
types, e.g., microblog, Wikipedia, and news.
We explore lexicons from various application
scenarios such as sentiment analysis and emo-
tion detection. We also compare the experi-
mental results of leveraging different lexicons.

1 Introduction

Argument Mining (AM) is an emerging research
area that has drawn more and more attention since
around 2010. Recently, Project Debater from IBM
has shown such an AI machine supported by ar-
gument mining techniques can do well at argu-
ing. The task of AM can be divided into a few
stages: (1) Extracting argumentative components
from large texts, i.e., boundary detection or seg-
mentation; (2) Classifying the extracted compo-
nents into classes. In general, an argumenta-
tive component can be categorized into ”Claim”,
which usually contains conclusions and stance to-
ward the given topic, or ”Premise”, which con-
tains reasoning or evidence used to support or at-
tack a claim; (3) Predicting the relations between
the identified argumentative components, i.e., sup-
porting and attacking (Cabrio and Villata, 2018).
Some works also consider more complex rela-
tions such as recursively support/attack the rela-
tions themselves rather than merely build relations
between components (Peldszus and Stede, 2013).

Argument detection and classification can im-
prove legal reasoning (Moens et al., 2007), policy

formulation (Florou et al., 2013), and persuasive
writing (Stab and Gurevych, 2014). In this paper,
we focus on mining argumentative components
from a large collection of documents and further
classifying them into roles of support/opposition.
Our model is based on the recurrent neural net-
work (RNN) , which has been widely used in nat-
ural language processing tasks (Cho et al., 2014).
With the help of the attention mechanism (Bah-
danau et al., 2015), RNN can further attend on the
key information.

We propose a novel attention mechanism that
is guided by argumentative lexicon information.
Lexicon information is reported as one kind of the
most frequently used features in argument min-
ing (Cabrio and Villata, 2018). Previous works on
AM have tried to integrate lexical features into the
learning models (Levy et al., 2017; Nguyen and
Litman, 2015; Rinott et al., 2015). These lexi-
cons are mostly composed by human beings or de-
rived by hand-crafted rules, and result in domain-
specificity. That is, it may fail to be used for other
domains. In the contrast of scarcity of general
lexicon for AM, lexical resources are abundant in
other fields like sentiment analysis, opinion min-
ing, and emotion detection (Hu and Liu, 2004;
Mohammad and Turney, 2013; Kiritchenko and
Mohammad, 2016). As a more general domain,
AM may get the benefits of not only in-domain
lexicon, but also out-domain lexicons.

The contribution of this work is two-fold: (1)
We propose an attention mechanism to leverage
lexicon information. (2) In the face of the scarcity
of argument lexicon, we explore several differ-
ent types of lexicons to verify whether outside re-
sources are useful for AM tasks.

The rest of this paper is organized as follows.
Section 2 summarizes related works about AM.
The dataset and linguistic resources used for ex-
periments are shown in Section 3. We introduce



68

our model in Section 4 and show the experimen-
tal results in Section 5. We also look into the er-
rors made by our best model in Section 6. Sec-
tion 7 makes a discussion on experimental results
and concludes this work.

2 Related Works

Neural networks have been used in varieties of
AM tasks. To improve the vanilla LSTM model,
Stab et al. (2018a) use attention mechanism to fuse
topic and sentence information together. In the
work of Laha and Raykar (2016), they present sev-
eral bi-sequence classification models on different
datasets. However, rather than using some sophis-
ticated architecture such as attention, it considers
only different concatenation or condition method
on the output of LSTM. Eger et al. (2017) propose
an end-to-end training model to mining argument
structure, identifying argument components.

Besides syntactic and positional information,
lexical information is also reported as one of
the most used features in argument mining task
(Cabrio and Villata, 2018). In some similar re-
search fields such as sentiment analysis and emo-
tion mining, a number of works have been pro-
posed to combine lexical information with the NN
models. Teng et al. (2016) use lexical scores as the
weights and do the weighted sum over the outputs
of the LSTM model, in order to derive the sen-
tence scores. Zou et al. (2018) determines atten-
tion weights using lexicon labels, which lead the
model to focus on the lexicon words. Bar-Haim
et al. (2017) proposes an idea of expanding lexi-
cons to improve stance classifying task.

However, in AM, seldom works directly com-
bine lexicon with models. By using discourse fea-
ture, Levy et al. (2018) generates weak labels and
use weak supervision. Shnarch et al. (2018) also
present a methodology to blend such weak labeled
data with high quality but scarce labeled data for
AM. Al-Khatib et al. (2016) consider the distant
supervision method. Most of these works use the
end-to-end training paradigm with the outside re-
sources only for generating the weak label, which
may not fully leverage the information of the lexi-
cons.

3 Resources

In this section, we introduce the dataset used to
evaluate the performance of our proposed model.
Besides, we describe each lexicon in brief and

show how to perform the data preprocessing.

3.1 Data

We conduct the experiments on the dataset re-
leased by Stab et al. (2018b).1 The dataset in-
cludes 25,492 sentences over eight topics that are
randomly selected from an online list of controver-
sial topics.2 The selected topics, which are con-
sidered as queries, are used to retrieve documents
from heterogeneous sources via the Google search
engine. Among these sentences, 4,944 of them
are supporting arguments, 6,195 are opposing ar-
guments, and 14,353 are non-argument sentences.
This dataset is commonly used for sentential argu-
ment identification task. Levy et al. (2018) col-
lect a dataset with around 1.5 million sentences
over 150 topics from Wikipedia. However, only
2,500 of them are labeled. It may not be sufficient
for training a model, especially for neural network
models.

The definition of argumentative components
differs from dataset to dataset. In the dataset used
in this work, an argumentative component is a
span of text with reasoning or evidence, which is
able to either support or oppose a topic (Stab et al.,
2018b).

3.2 Lexicon resource

To improve the baseline model, we consider sev-
eral existing lexicons across different domains.
We first explore the claim lexicon that is built for
argument mining task (Levy et al., 2017). We also
include the lexicon resources often used in senti-
ment analysis (Hu and Liu, 2004) and emotion de-
tection (Mohammad and Turney, 2013). We pos-
tulate that the resources for these application sce-
narios may have the potential for argument min-
ing. We further develop a model based on the gen-
eral purpose lexicon, WordNet (Miller, 1995).

These resources are applied in different ways.
We use the claim lexicon (Levy et al., 2017), the
sentiment lexicon (Hu and Liu, 2004), and the
emotion lexicon (Mohammad and Turney, 2013)
to extract critical words from the i-th input sen-
tence Ci, forming a sentence Ai. In contrast,
we consult WordNet (Miller, 1995) to expand the
short topic Ti into the corresponding Ai.

1https://www.informatik.tu-darmstadt.
de/ukp/research_6/data

2https://www.procon.org/, https://www.
questia.com/library/controversial-topics

https://www.informatik.tu-darmstadt.de/ukp/research_6/data
https://www.informatik.tu-darmstadt.de/ukp/research_6/data
https://www.procon.org/
https://www.questia.com/library/controversial-topics
https://www.questia.com/library/controversial-topics


69

Claim Lexicon. The claim lexicon is a lexicon
containing words with argumentative characteris-
tics. Levy et al. (2017) use the appearance of the
term ”that” as a weak signal of sentences contain-
ing argumentative components. After collecting
nearly 1.86M sentences, they compute the prior
probability of the term ”that” P (that) occurring
in a sentence, and the probabilities P (that|wi),
where P (that|wi) denotes the probability of a
sentence having the term ”that” and the word wi
is in the suffix after the main concept (i.e. the tar-
get entity in a controversial topic), given the sen-
tence containing wi. Those words with a proba-
bility P (that|wi) > P (that) are included in their
proposed claim lexicon, resulting a lexicon with
around 600 claim words. The lexicon was then
used for designing sentence pattern rules called
claim sentence query (CSQ). They believe the
claim lexicon can help detect sentences containing
argument.

Sentiment Lexicon. Hu and Liu (2004) built
a sentiment lexicon that contains around 6,800
words. Each word is labeled as negative and/or
positive. We construct an additional sentenceA by
extracting the words that are in both sentiment lex-
icon and the input sentence C, regardless whether
they are positive or negative.

Emotion Lexicon. The emotion lexicon
built by Mohammad and Turney (2013) contains
around 14,200 words. Each word in the lexicon
is given eight emotion labels. An emotion in the
lexicon could be one of eight emotions, including
anger, anticipation, disgust, fear, joy, sadness, sur-
prise, and trust. The labels are defined as follows:

label(wi, ej) =

{
1, if wi associated with ej
0, otherwise

Wherewi is a word, and ej is one of the eight emo-
tions. In the experiment, we select the words that
have at least one emotion labeled as 1, resulting a
list of 6,468 words. We then use this list to create
an additional sentence A from the input sentence
C.

WordNet. To expand a topic T composed of
words wT1 , w

T
2 , ..., w

T
K , we expand each of the

words in it. For each word wTi , we use WordNet
(Miller, 1995) to find its corresponding synonyms.
We then put the found synonyms together, forming
an additional sentence A, an expanded version of
topic T .

Word Embedding

!"# !$# !%#… …

BiLSTM

Dense Dense Dense

… …

Softmax

… …

&

Softmax

'(

)"# )$# )%#

ℎ"# ℎ$# ℎ%#

ℎ%# +ℎ$#+ℎ"#
+

,

!"- !$- !.-

)"- )$- ).-

RNN
ℎ.-

… …

… …

/" /$ /%

Figure 1: The architecture of Lexicon Guided Attentive
Neural Network Model

4 Model

This section describes the development of the
baseline model and the proposed model. To iden-
tify sentence-level argumentative components, the
model is given a sentence C, which contains a se-
quence of wordswc1, w

c
2, ..., w

c
N and a topic T with

words wT1 , w
T
2 , ..., w

T
K . The input word sequence

is then encoded as a sequence of word embeddings
via the GloVe word vectors. The pre-trained word
vectors with the dimension of 100 released by Pen-
nington et al. (2014) are used. Based on the given
input, the model makes a prediction ŷ for the given
sentence, i.e., classifying it as supporting argu-
ment, opposing argument, or non-argument. For
comparison, we implement a baseline model with
the vanilla bidirectional LSTM (BiLSTM).

In order to exploit the linguistic knowledge,
Lei et al. (2018) highlight the sentiment words of
the input sentence, computing attention weight for
each word with them. By integrating the senti-
ment lexicon into the neural network model, the
work successfully improves the performance in
sentiment analysis. This work proposes a model
that integrates an outside lexicon resource into at-
tention mechanism (Vaswani et al., 2017). For
each input sentence, we compose an additional
sentenceA, which contains words wa1 , wa2 , ..., waM
based on the given lexicon. The additional sen-
tence A is then forwarded to the embedding layer
together with input sentence C. The output of



70

model F1 Parg+ Parg− Rarg+ Rarg−
BiLSTM .5337± .0123 .4521± .0391 .4832± .0393 .2911± .1139 .4816± .1276
ClaimLex∗ .5684± .0222 .4736± .0322 .5075± .0450 .3756± .1072 .5011± .0854
SentimentLex∗ .5718± .0165 .4937± .0365 .5125± .0414 .3590± .1043 .5240± .0889
EmotionLex∗ .5695± .0129 .4920± .0369 .5036± .0356 .3524± .0861 .5264± .1006
WordNet∗ .5788± .0142 .4846± .0292 .5191± .0376 .3724± .0818 .5235± .0772

Table 1: The results of the baseline model and the proposed model with different lexicon resources. The highest
score of each column is highlighted in bold font.

embedding layer is the sequences ec1, e
c
2, ..., e

c
N

and ea1, e
a
2, ..., e

a
M , representing the embedded sen-

tences C and A, respectively. Then, eci is fed into
BiLSTM and results in hci at the corresponding
time step. As for A, we add an RNN to collect its
information and take the output haM at the last time
step as its representation. Though Lei et al. (2018)
use an LSTM to encode the sentimental sentences,
we do not follow their approach. In our work, the
simple RNN outperforms the LSTM in the prelim-
inary experiments.

The attention weight of the i-th word (i.e. αi)
is determined by the concatenation of the output
of the BiLSTM hci and the output of the RNN (i.e.
haM ), which is given the additional sentence A as
the input:

αi =
exp(σ([hci ;h

a
M ]))∑N

i=1 exp(σ([h
c
i ;h

a
M ]))

(1)

where αi indicates the attention weight of i-th
word of the input sentence, and [hci ;h

a
M ] indicates

the concatenation of i-th hidden state and the RNN
output state. The scoring function σ(·) is designed
as:

σ([hci ;h
a
M ]) = tanh(Wc[h

c
i ;h

a
M ]) (2)

where Wc indicates trainable parameters.
All the weighted hidden states are then summed

up, and connected to a fully connected layer for
the final prediction:

o =

N∑
i=1

hci
′

(3)

hci
′
= αih

c
i (4)

Figure 1 illustrates the architecture of our model.

5 Experiments

Because most of the lengths of input sentences are
less than 60 and most of the lengths of additional

sentencesA are less than 20, we truncate them into
lengths of 60 and 20 respectively. The dataset has
25,492 sentences in total. We conduct 5-fold cross
validation for evaluating our model.

To evaluate our approaches, we report the
average macro F1 as ternary setting, precision
and recall of predicting supporting arguments
(Parg+, Rarg+), and precision and recall of pre-
dicting opposing arguments (Parg−, Rarg−). We
run paired t-test for each proposed model in com-
parison with the baseline model, and mark the
models having statistical significance (i.e. p-value
< 0.05) with a wildcard. As the result shown in
Table 1, we can observe that the proposed mod-
els benefit from the information from the adopted
lexicons, improving the performance of argumen-
tative components identification. The best model,
which uses WordNet to expand topic T , outper-
forms the baseline model by 4.5 percentage in F1.
The proposed model with the lowest F1 score (i.e.
ClaimLex) still outperforms the baseline by 3.4
percentage. Furthermore, the best performance re-
ported by Stab et al. (2018b) on the same dataset
is 0.4285 in macro F1, which is the result of only
incorporating topic information into their models.
This shows the impact of the lexicon information.

However, we can also observe that the result
of integrating claim lexicon (Levy et al., 2017)
is out of our expectation though it is a resource
for argument mining. Possible reasons are fig-
ured out as follows. Firstly, the lexicon is built
based on a strong assumption, i.e., the present of
the term ”that” indicates a high probability of the
occurrence of argumentative components. Sec-
ondly, the lexicon has only 586 words, indicating
a very small coverage with the whole vocabulary.
Thirdly, the lexicon built from the sentences across
100 different topics contains a number of domain-
specific words such as ”LGBTQ” and ”militarily”.
Highlighting of these domain-specific words may
cause noise when the topic is unrelated to them.



71

Topic Sentence Prediction Annotation

S1
death
penalty

Advocates of death penalty cite examples on how
imposing the death sentence or abolishing it have af-
fected crime rate.

attack support

S2
death
penalty

Pain of Death: Executing a person can be quick
and painless, or executing a person can be slow and
painful.

support non-arg

S3
gun
control

If you can get to the phone to call 911, if you are
strong enough to hold off your attacker until the po-
lice arrive, and if you can wait 15 minutes in the city
or 45 minutes in the country for law enforcement to
arrive while you struggle with the intruder, then you
might make it.

support attack

S4 cloning
Our of respect for human clones (human beings in
every respect), a ban on human cloning should be
opposed.

support attack

S5
gun
control

We should have more of it! non-arg support

Table 2: The examples that our best model fails to correctly predict. The sentences predicted/annotated as non-
argumentative ones are abbreviated to non-arg.

6 Error Analysis

To know better what kind of sentence would mis-
lead our model to make wrong predictions, we ran-
domly sample the sentences with error from our
best model, i.e., lexicon-guided attentive neural
network model with WordNet. After looking into
these errors, we find that the causes of a wrong
prediction can be briefly categorized into the fol-
lowing cases. Some illustrations of the errors are
listed in Table 2: (1) The sentences that have am-
biguous words or state an open question can eas-
ily lead our model to predict the sentences’ labels
from non-argumentative to argumentative, or pre-
dict the labels from one stance, i.e., supporting or
attacking, to the other. For example, both ”impos-
ing” and ”abolishing” are shown in S1, which may
cause the model to fails on correctly detecting the
stances. S2 states an open question on the influ-
ence of death penalty, but the model mistakes it
for an argumentative sentence; (2) When arguing
over an issue, people may use irony to attack the
opposite stance. Such statement may mislead the
model, as S3 has shown; (3) We also find that our
model may predict wrongly with the appearance of
double negation. The part of the sentence S4, ”a
ban on human cloning should be opposed”, con-
veys the supporting stance with a double negative
statement. With a limited amount of training data,
the model may not be able to comprehend rela-

tively complicated syntax.
On the other hand, some examples in the dataset

may have been wrongly annotated. According to
Stab et al. (2018b), arguments are defined as a
span of text having reasoning or evidence that can
be used to support or oppose a topic. S5 does ex-
plicitly declare its supporting stance, but neverthe-
less has no reasoning or evidence.

7 Conclusion

In this work, we propose a novel approach to lever-
age the lexicon from both in-domain and out-of-
domain sources for the task of argumentative com-
ponent mining. We explore several sources from
different application scenarios, from claim lexicon
(Levy et al., 2017) to other domain resources such
as sentiment analysis (Hu and Liu, 2004), emo-
tion detection (Mohammad and Turney, 2013),
and the general domain lexicon resource (Miller,
1995). Experimental results confirm the effective-
ness of the integration of lexicon information. The
scarcity of the resources in argument mining is
also highlighted in the discussion.

Acknowledgments

This research was partially supported by Ministry
of Science and Technology, Taiwan, under grants
MOST-106-2923-E-002-012-MY3, MOST-107-
2634-F-002-011-, MOST-108-2634-F-002-008-.



72

References
Khalid Al-Khatib, Henning Wachsmuth, Matthias Ha-

gen, Jonas Köhler, and Benno Stein. 2016. Cross-
domain mining of argumentative text through dis-
tant supervision. In Proceedings of the 2016 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 1395–1404, San Diego,
California. Association for Computational Linguis-
tics.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua
Bengio. 2015. Neural machine translation by
jointly learning to align and translate. CoRR,
abs/1409.0473.

Roy Bar-Haim, Lilach Edelstein, Charles Jochim, and
Noam Slonim. 2017. Improving claim stance classi-
fication with lexical knowledge expansion and con-
text utilization. In Proceedings of the 4th Workshop
on Argument Mining, pages 32–38, Copenhagen,
Denmark. Association for Computational Linguis-
tics.

Elena Cabrio and Serena Villata. 2018. Five years
of argument mining: a data-driven analysis. In
Proceedings of the Twenty-Seventh International
Joint Conference on Artificial Intelligence, IJCAI-
18, pages 5427–5433. International Joint Confer-
ences on Artificial Intelligence Organization.

Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using rnn encoder–decoder
for statistical machine translation. In Proceedings of
the 2014 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1724–
1734, Doha, Qatar. Association for Computational
Linguistics.

Steffen Eger, Johannes Daxenberger, and Iryna
Gurevych. 2017. Neural end-to-end learning for
computational argumentation mining. In Proceed-
ings of the 55th Annual Meeting of the Association
for Computational Linguistics (ACL 2017), volume
Volume 1: Long Papers, pages (11–22). Association
for Computational Linguistics.

Eirini Florou, Stasinos Konstantopoulos, Antonis
Koukourikos, and Pythagoras Karampiperis. 2013.
Argument extraction for supporting public policy
formulation. In Proceedings of the 7th Workshop on
Language Technology for Cultural Heritage, Social
Sciences, and Humanities, pages 49–54.

Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the Tenth
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, KDD ’04, pages
168–177, New York, NY, USA. ACM.

Svetlana Kiritchenko and Saif Mohammad. 2016. The
effect of negators, modals, and degree adverbs on
sentiment composition. In Proceedings of the 7th

Workshop on Computational Approaches to Subjec-
tivity, Sentiment and Social Media Analysis, pages
43–52, San Diego, California. Association for Com-
putational Linguistics.

Anirban Laha and Vikas Raykar. 2016. An empiri-
cal evaluation of various deep learning architectures
for bi-sequence classification tasks. In Proceedings
of COLING 2016, the 26th International Confer-
ence on Computational Linguistics: Technical Pa-
pers, pages 2762–2773, Osaka, Japan. The COLING
2016 Organizing Committee.

Zeyang Lei, Yujiu Yang, and Min Yang. 2018. Senti-
ment lexicon enhanced attention-based lstm for sen-
timent classification.

Ran Levy, Ben Bogin, Shai Gretz, Ranit Aharonov,
and Noam Slonim. 2018. Towards an argumentative
content search engine using weak supervision. In
Proceedings of the 27th International Conference on
Computational Linguistics, pages 2066–2081, Santa
Fe, New Mexico, USA. Association for Computa-
tional Linguistics.

Ran Levy, Shai Gretz, Benjamin Sznajder, Shay Hum-
mel, Ranit Aharonov, and Noam Slonim. 2017. Un-
supervised corpus–wide claim detection. In Pro-
ceedings of the 4th Workshop on Argument Mining,
pages 79–84, Copenhagen, Denmark. Association
for Computational Linguistics.

George A. Miller. 1995. Wordnet: A lexical database
for english. Commun. ACM, 38(11):39–41.

Marie-Francine Moens, Erik Boiy, Raquel Mochales
Palau, and Chris Reed. 2007. Automatic detection
of arguments in legal texts. In Proceedings of the
11th International Conference on Artificial Intelli-
gence and Law, ICAIL ’07, pages 225–230, New
York, NY, USA. ACM.

Saif M. Mohammad and Peter D. Turney. 2013.
Crowdsourcing a word-emotion association lexicon.
29(3):436–465.

Huy Nguyen and Diane Litman. 2015. Extracting
argument and domain words for identifying argu-
ment components in texts. In Proceedings of the
2nd Workshop on Argumentation Mining, pages 22–
28, Denver, CO. Association for Computational Lin-
guistics.

Andreas Peldszus and Manfred Stede. 2013. From ar-
gument diagrams to argumentation mining in texts:
A survey. Int. J. Cogn. Inform. Nat. Intell., 7(1):1–
31.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. In Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1532–
1543.

https://doi.org/10.18653/v1/N16-1165
https://doi.org/10.18653/v1/N16-1165
https://doi.org/10.18653/v1/N16-1165
https://doi.org/10.18653/v1/W17-5104
https://doi.org/10.18653/v1/W17-5104
https://doi.org/10.18653/v1/W17-5104
https://doi.org/10.24963/ijcai.2018/766
https://doi.org/10.24963/ijcai.2018/766
https://doi.org/10.3115/v1/D14-1179
https://doi.org/10.3115/v1/D14-1179
https://doi.org/10.3115/v1/D14-1179
https://doi.org/10.1145/1014052.1014073
https://doi.org/10.1145/1014052.1014073
https://doi.org/10.18653/v1/W16-0410
https://doi.org/10.18653/v1/W16-0410
https://doi.org/10.18653/v1/W16-0410
https://www.aclweb.org/anthology/C16-1260
https://www.aclweb.org/anthology/C16-1260
https://www.aclweb.org/anthology/C16-1260
https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16243
https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16243
https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16243
https://www.aclweb.org/anthology/C18-1176
https://www.aclweb.org/anthology/C18-1176
https://doi.org/10.18653/v1/W17-5110
https://doi.org/10.18653/v1/W17-5110
https://doi.org/10.1145/219717.219748
https://doi.org/10.1145/219717.219748
https://doi.org/10.1145/1276318.1276362
https://doi.org/10.1145/1276318.1276362
https://doi.org/10.3115/v1/W15-0503
https://doi.org/10.3115/v1/W15-0503
https://doi.org/10.3115/v1/W15-0503
https://doi.org/10.4018/jcini.2013010101
https://doi.org/10.4018/jcini.2013010101
https://doi.org/10.4018/jcini.2013010101
http://www.aclweb.org/anthology/D14-1162
http://www.aclweb.org/anthology/D14-1162


73

Ruty Rinott, Lena Dankin, Carlos Alzate Perez,
Mitesh M. Khapra, Ehud Aharoni, and Noam
Slonim. 2015. Show me your evidence - an auto-
matic method for context dependent evidence de-
tection. In Proceedings of the 2015 Conference on
Empirical Methods in Natural Language Process-
ing, pages 440–450, Lisbon, Portugal. Association
for Computational Linguistics.

Eyal Shnarch, Carlos Alzate, Lena Dankin, Mar-
tin Gleize, Yufang Hou, Leshem Choshen, Ranit
Aharonov, and Noam Slonim. 2018. Will it blend?
blending weak and strong labeled data in a neu-
ral network for argumentation mining. In Proceed-
ings of the 56th Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Pa-
pers), pages 599–605, Melbourne, Australia. Asso-
ciation for Computational Linguistics.

Christian Stab and Iryna Gurevych. 2014. Annotating
argument components and relations in persuasive es-
says. In Proceedings of COLING 2014, the 25th In-
ternational Conference on Computational Linguis-
tics: Technical Papers, pages 1501–1510.

Christian Stab, Tristan Miller, and Iryna Gurevych.
2018a. Cross-topic argument mining from hetero-
geneous sources using attention-based neural net-
works. arXiv preprint arXiv:1802.05758.

Christian Stab, Tristan Miller, Benjamin Schiller,
Pranav Rai, and Iryna Gurevych. 2018b. Cross-
topic argument mining from heterogeneous sources.
In Proceedings of the 2018 Conference on Em-
pirical Methods in Natural Language Processing,
pages 3664–3674, Brussels, Belgium. Association
for Computational Linguistics.

Zhiyang Teng, Duy Tin Vo, and Yue Zhang. 2016.
Context-sensitive lexicon features for neural senti-
ment analysis. In Proceedings of the 2016 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1629–1638, Austin, Texas. Asso-
ciation for Computational Linguistics.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In I. Guyon, U. V. Luxburg, S. Bengio,
H. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-
nett, editors, Advances in Neural Information Pro-
cessing Systems 30, pages 5998–6008. Curran As-
sociates, Inc.

Yicheng Zou, Tao Gui, Qi Zhang, and Xuanjing Huang.
2018. A lexicon-based supervised attention model
for neural sentiment analysis. In Proceedings of
the 27th International Conference on Computational
Linguistics, pages 868–877, Santa Fe, New Mexico,
USA. Association for Computational Linguistics.

https://doi.org/10.18653/v1/D15-1050
https://doi.org/10.18653/v1/D15-1050
https://doi.org/10.18653/v1/D15-1050
https://www.aclweb.org/anthology/P18-2095
https://www.aclweb.org/anthology/P18-2095
https://www.aclweb.org/anthology/P18-2095
https://www.aclweb.org/anthology/D18-1402
https://www.aclweb.org/anthology/D18-1402
https://doi.org/10.18653/v1/D16-1169
https://doi.org/10.18653/v1/D16-1169
http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf
http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf
https://www.aclweb.org/anthology/C18-1074
https://www.aclweb.org/anthology/C18-1074

