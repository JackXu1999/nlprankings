



















































Learning to Recognize Ancillary Information for Automatic Paraphrase Identification


Proceedings of NAACL-HLT 2016, pages 1109–1114,
San Diego, California, June 12-17, 2016. c©2016 Association for Computational Linguistics

Learning to Recognize Ancillary Information for
Automatic Paraphrase Identification

Simone Filice
DICII,

University of Roma, Tor Vergata
filice@info.uniroma2.it

Alessandro Moschitti∗
ALT, Qatar Computing Research Institute,

HBKU
amoschitti@qf.org.qa

Abstract

Previous work on Automatic Paraphrase Iden-
tification (PI) is mainly based on modeling
text similarity between two sentences. In con-
trast, we study methods for automatically de-
tecting whether a text fragment only appear-
ing in a sentence of the evaluated sentence pair
is important or ancillary information with re-
spect to the paraphrase identification task. En-
gineering features for this new task is rather
difficult, thus, we approach the problem by
representing text with syntactic structures and
applying tree kernels on them. The results
show that the accuracy of our automatic An-
cillary Text Classifier (ATC) is promising, i.e.,
68.6%, and its output can be used to improve
the state of the art in PI.

1 Introduction

Automatic PI is the task of detecting if two texts
convey the same meaning. For example, the fol-
lowing two sentences from the Microsoft Research
Paraphrase Corpus (MSRP) (Dolan et al., 2004):

S1a: Although it’s unclear whether Sobig was to
blame, The New York Times also asked employees at
its headquarters yesterday to shut down their com-
puters because of ”system difficulties.”

S1b: The New York Times asked employees at its
headquarters to shut down their computers yester-
day because of ”computing system difficulties.”

are paraphrases, while these other two are not:

∗ Professor at DISI, University of Trento.

S2a: Dr. Anthony Fauci, director of the National In-
stitute of Allergy and Infectious Diseases, agreed.

S2b: ”We have been somewhat lucky,” said Dr. An-
thony Fauci, director of the National Institute of Al-
lergy and Infectious Diseases.

Most previous work on automatic PI, e.g., (Madnani
et al., 2012; Socher et al., 2011), is based on a di-
rect comparison between the two texts, exploiting
different similarity scores into a machine learning
framework. However, these methods consider sen-
tences as monolithic units and can thus be misled by
ancillary information that does not modify the main
meaning expressed in the text.

For example, the additional text fragment (ATF),
“Although it’s unclear whether Sobig was to blame”,
from S1a expresses ancillary information, which
does not add much to the message of S1b, thus
the sentences are considered paraphrases. In con-
trast, S2b contains the ATF, “We have been some-
what lucky”, whose meaning is not linked to any
constituent of S1b. Since such text expresses rele-
vant information, the two sentences are not consid-
ered paraphrases.

In this paper, we study and design models for ex-
tracting ATFs from a sentence with respect to an-
other one and classifying if their meaning is ancil-
lary or important. For this purpose, we built a cor-
pus of sentence pairs using MSRP, where at least one
pair member always contains ATFs. We use SVMs
with tree kernels applied to syntactic representations
(Severyn and Moschitti, 2012) of ATFs for learning
automatic ATCs.

The results derived on MSRP show (i) a promis-
ing accuracy of our ATC and (ii) the output of ATC

1109



Figure 1: A pair of non-paraphrase sentences and its corresponding additional fragment.

can be used as a feature for improving the state-of-
the-art PI model.

2 Ancillary clauses in PI

Our main purpose in studying computational ap-
proaches to the detection of ancillary information is
its practical application to PI. Thus, given a pair of
sentences (in general two texts), we define ATFs as
ancillary information if their semantics:

(i) only appears in one of the two sentences and

(ii) does not change the main meaning of the sen-
tence, i.e., either the sentences are paraphrases
or, if they are not, such ATFs are not the reason
for their different meaning.

The definition above along with a syntactic repre-
sentation of the sentences can be applied to a para-
phrase corpus to build a dataset of ancillary vs. im-
portant ATFs. For example, Fig. 1 shows the shallow
tree representation1 we proposed in (Severyn and
Moschitti, 2012) of the sentences, S2a and S2b, re-
ported in the introduction, where the red label ρ in-
dicates that there is a link between the lemmas of the
two sentences (also shown by the dashed edges). ρ
can be propagated to the upper nodes to mark the re-
lated constituents. For example, the lemma National
is matched by the two sentences, thus both its father
node, NNP, and its grandfather constituent, NP, are
marked.

Such representation makes the extraction of ATFs
easier. For instance, Fig. 1 shows the text fragment

1The shallow trees are constituted by four levels: (i) word
lemmas as leaves, (ii) POS-tags as parent of lemmas,
(iii) phrases grouping POS-tag nodes and (iv) a final root S.

of S2b, “We have been somewhat lucky”, on the right.
This is an ATF since it is not aligned with any frag-
ments of S2a. Moreover, since it expresses a central
information to the sentence meaning, S2b cannot be
in paraphrase relation with S2a. Conversely, the ATF
of S1a, “Although it’s unclear whether Sobig was to
blame”, is ancillary to the main meaning of the sen-
tence, indeed, the annotators marked S1a and S1b as
a valid paraphrase.

3 Building the ATC corpus

The previous section has shown an approach to ex-
tract ATFs that can be potentially ancillary. This
uses an alignment approach based on lexical similar-
ity, which may fail to align some text constituents.
However, these mistakes only affect the precision
in extracting ATFs rather than the recall. In other
words, we can build a corpus that considers most
cases of additional information.

In particular, we design the following simple
heuristic: let Fi and Fj be the largest not aligned
(possibly discontinuous) word sequences appearing
in the sentence pair (Si, Sj), where Fi ∈ Si and
Fj ∈ Sj . We define ATF as the largest text between
Fi and Fj subject to d = |size(Fi)− size(Fj)| >
τ , where size(F ) is the number of words2 appear-
ing in F . If the condition is not satisfied no ATF is
extracted.

The condition over d is important because the sen-
tence aligner may fail to match some subsequences,
creating false ATFs. However, what is missed from
one sentence will be missed also in the other sen-

2Only verbs, nouns, adjectives, adverbs and numbers were
considered, assuming all the others as “not informative words”.

1110



Train Test
τ Ancillary Important Total Ancillary Important Total
1 971 687 1658 387 687 1074
2 426 364 790 166 151 317
3 166 169 335 62 79 141
4 59 73 132 21 36 57

Table 1: Number of additional clauses extracted from MSRP.

tence. Thus, in general, if we set a small d then
Fi and Fj misalignments may generate false ATFs.
In contrast, a large d would clearly prevent this
problem, although small ATFs (of size < d) may
be discarded. More precisely, smaller values of
τ may cause the selection of fragments that have
corresponding fragments in the other sentence, ex-
pressed with dissimilar words (i.e., the aligner failed
to match those constituents). Larger values of τ
make the heuristic more precise, but less effective
in retrieving smaller ATFs.
The ATF corpus. We applied the heuristic above
to extract an ATF (if exists) from each sentence pair
of MSRP. The number of the extracted ATFs de-
pends on τ as reported in Table 1. A manual in-
spection of the retrieved fragments revealed that:
(i) small values of τ , namely, 1 and 2, cause the
extraction of many fragments from one sentence
corresponding to fragments expressed with differ-
ent words in the other sentence: these are not ATFs.
(ii) With τ = 3, the heuristic is very precise and
captures most ATFs appearing in the sentence pairs.
(iii) Higher values of τ cause many valid fragments
to be missed.

Once ATFs are generated, we need to label them
as ancillary or important for PI such that this data
can be used for training and testing ATC. Interest-
ingly, the data can be automatically labeled exploit-
ing the MSRP annotation: given a sentence pair
from MSRP, we (i) extract the ATF from it and
(ii) automatically annotate it as ancillary if the pair
is a paraphrase and not ancillary otherwise. In other
words, an ATF is considered ancillary only if it is ex-
tracted from a paraphrase pair. To verify the correct-
ness of this approach, two experts manually labeled
the obtained data extracted with τ = 3 and found
that only 3.3% of the data was mislabeled with re-
spect to one annotator. The Cohen’s kappa agree-
ment between the annotators was 85%.

4 Experiments

In these experiments, we first evaluate state-of-the-
art PI models to create our baseline, then we experi-
ment with our ATC and finally, we combine them to
show that ATC can improve PI.

4.1 Deriving PI baselines

Dataset. We used MSRP, which consists of 4,076
sentence pairs in the training set and 1,725 sentence
pairs in test set. About 66% of the pairs are para-
phrases. The pairs were extracted from topically
similar Web news articles, applying some heuris-
tics that select potential paraphrases to be anno-
tated by human experts. We represent the sentence
pairs using shallow trees generated with the Stanford
parser3.

Models. We adopted our state-of-the-art PI ap-
proach we proposed in (Filice et al., 2015). This,
given two pairs of sentences, pa = 〈a1, a2〉 and
pb = 〈b1, b2〉, represents instances as shown Fig. 1,
and applies tree kernels to them. In particular, we
used our best kernel com derived in the work above:

SMK(pa, pb) = sf
(
SPTK(a1, b1)× SPTK(a2, b2),

SPTK(a1, b2)× SPTK(a2, b1)
)
,

where sf(x1, x2) = 1c log(e
cx1 + ecx2), and SPTK

is the Smoothed Partial Tree Kernel (Croce et al.,
2011). SMK considers the inherent symmetry of the
PI task and evaluates the best alignment between the
sentences in the input pairs. The sf is a softmax
operation used in place of the max function4, which
is not a valid kernel function. SPTK uses a simi-
larity function between words: we generated it with
the word2vec tool5 (Mikolov et al., 2013) using the

3nlp.stanford.edu/software/corenlp.shtml
4c=100 produces accurate approximation.
5https://code.google.com/p/word2vec

1111



Model Acc (%) P R F1
LK 75.9 78.4 88.1 82.9

SMK 76.4 76.6 92.9 83.9
SMK+LK 77.7 79.4 8.99 84.4

(Socher et al., 2011) 76.8 − − 83.6
(Madnani et al., 2012) 77.4 − − 84.1

Table 2: Results on Paraphrase Identification.

skip-gram model applied to the UkWaC corpus (Ba-
roni et al., 2009).

Results. As illustrated in Table 2, a binary Sup-
port Vector Machine equipped with SMK achieves a
very high accuracy. Moreover, SMK combined with
a linear kernel (LK) over similarity metrics6 attains
the state of the art in PI.

4.2 Experimental Evaluation on ATC

Dataset and models. We created an ATC dataset
with τ = 3 as described in Sec. 3. We make this
dataset available7. We learned ATC with the C-SVM
algorithm (Chang and Lin, 2011) inside KeLP8. The
examples are represented using the shallow tree like
the one on the right of Fig. 1. We used three dif-
ferent tree kernels: the Syntactic Tree Kernel (STK)
by Collins and Duffy (2001), the Partial Tree Ker-
nel (PTK) by Moschitti (2006) and SPTK using the
word2vec similarity defined before.

Given the small size of such dataset (Only 8% of
MSRP instances have additional fragments), we per-
formed a 5-fold cross validation. Table 3 illustrates
the Accuracy, Precision, Recall and F1 of our mod-
els. ATC based on SPTK provides the best accu-
racy, i.e., 68.6%, which is a promising result for this
research. The second most accurate classifier uses
PTK, which is more flexible than STK.

4.3 Using ATC in PI

We carried out error analysis on PI and observed
that the used classifier commits a systematic error:
when two sentences share a very similar large part
(identical in the extreme case) and one sentence has
an ATF, it almost always classifies the sentences as

6These include cosine similarities of lemmas, POS-tags, and
n-grams, longest common substring and longest common subse-
quence measures and Tree Kernel intra-pair similarities.

7http://alt.qcri.org/resources/ancillary
8https://github.com/SAG-KeLP

Kernel Acc (%) P R F1
STK 65.1 ± 6.5 65.4 ± 8.0 58.3 ± 5.6 61.5 ± 5.8
PTK 67.4 ± 8.2 69.7 ± 8.8 56.5 ± 7.7 62.4 ± 7.8

SPTK 68.6 ± 9.4 71.0 ± 9.0 57.9 ± 9.7 63.7 ± 9.3

Table 3: Results of Ancillary Text Classifiers

paraphrases, even if the ATF contains important in-
formation that invalidates the paraphrase relation.
This kind of mistakes can be corrected by ATC.

Thus, we created the following ensemble model:
given a pair to be classified, we apply our heuristic
for ATF extraction. If the heuristic does not find any
fragment in the pair, we only rely on the prediction
provided by PI. Otherwise, we combine the predic-
tion of ATC applied to ATF with the one of the PI
classifier using a stacking strategy (Wolpert, 1992),
i.e., the two predictions become the input features of
a third classifier that makes the final decision.

To train this meta-classifier, we need the predic-
tions from ATC and PI computed on a validation set.
Hence, we split the training set in two parts: one
part is used for training ATC and PI, while the other
is classified with the trained models to produce the
predictions for the meta-classifier. Then, the roles
of the two parts are inverted. The meta-classifier is
a linear SVM (Fan et al., 2008) implemented with
KeLP.

Note that: (i) since we use 5-fold cross-validation,
for each fold, we needed to apply the process de-
scribed above to each fold; and (ii) all the learning
algorithms and kernels adopt default parameters to
also facilitate the reproducibility of our results.

Results. Table 4 reports the comparison between
PI and PI combined with ATC (trained with SPTK).
The performance is derived only on sentence pairs
with ATFs.

The first column indicates the kernel used by the
PI classifier, while the second column reports ’+’ or
’-’ to indicate if PI is combined with ATC or not,
respectively. We note that ATC produces a great im-
provement, ranging from 8 absolute percent points
over LK to about 3 points over SMK+LK, i.e., the
state-of-the-art model. As expected, the more ac-
curate the baseline is, the lower the improvement is
produced.

It should be noted that only a relative small subset

1112



PI ATC Acc (%) P R F1
LK - 62.2± 5.4 57.8± 7.0 75.1± 7.4 65.3± 6.9
LK + 70.4± 5.5† 69.2± 6.8 68.2± 4.7 68.7± 5.7

SMK - 64.7± 6.0 59.0± 6.5 84.9± 4.6 69.5± 5.9
SMK + 69.1± 5.5 ‡ 66.9± 6.4 69.7± 5.8 68.2± 5.9

SMK+LK - 70.5± 4.0 66.3± 6.7 77.3± 6.1 71.2± 5.3
SMK+LK + 73.2± 5.2‡ 72.5± 5.4 71.3± 3.9 71.8± 3.9

Table 4: PI classifier performance using ATC. The test set is restricted to examples having additional fragments. † and ‡ mark
statistically significant differences in accuracy compared to the counterpart model not using ATC with confidence levels of 95%

and 90%, respectively (t-test).

PI ATC Acc (%) P R F1
LK - 75.5± 0.5 78.6± 0.9 87.6± 1.9 82.8± 0.4
LK + 76.2± 1.0† 79.5± 0.1 87.2± 2.2 83.1± 0.8

SMK - 75.6± 0.8 77.1± 0.4 90.7± 1.2 83.3± 0.7
SMK + 75.9± 0.9† 77.9± 1.3 89.7± 1.2 83.4± 0.6

SMK+LK - 78.1± 1.1 80.7± 0.6 88.6± 2.1 84.4± 0.9
SMK+LK + 78.3± 1.1‡ 81.1± 0.9 88.2± 1.8 84.5± 0.8

Table 5: PI classifier performance using ATC on the testset. † and ‡ mark statistically significant differences in accuracy compared
to the counterpart model not using ATC with confidence levels of 95% and 90%, respectively (t-test).

of MSRP contains additional fragments (about 8%
when τ = 3). Thus, the impact on the entire PI test-
set cannot be large. Tab. 5 reports the accuracy of the
previous models on the entire testset. An improve-
ment over all models, state-of-the-art included, can
be still observed, although it is less visible.

5 Conclusions

In this paper, we study and design models for learn-
ing to detect ancillary information in the context of
PI. We used a heuristic rule for selecting additional
fragments from paraphrase pairs, which, applied to
MSRP, generates our ATF dataset. We manually an-
notated the latter for training and testing our ATCs.

Our experiments using several kernel models
show that ATC can achieve a good accuracy (about
69%) and significantly impact the PI accuracy. Our
results suggest that:

(i) it is possible to recognize information humans
believe is ancillary; and

(ii) to go beyond the current results and technology
for high-level semantic tasks (e.g., PI), we can-
not just rely on shallow similarity features, but
we rather need to build components that ana-

lyze different aspects of text and then combine
the output of the different modules.

In the future, it would be interesting to use meth-
ods similar to those successfully used in question
answering research, e.g., matching entities in the
sentence trees using linked open data (Tymoshenko
et al., 2014; Tymoshenko and Moschitti, 2015) or
enriching trees with semantic information automat-
ically produced by classifiers, e.g., (Severyn et al.,
2013a; Severyn et al., 2013b).

Acknowledgements

This work has been partially supported by the EC
project CogNet, 671625 (H2020-ICT-2014-2, Re-
search and Innovation action) and by an IBM Fac-
ulty Award.

References
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and

Eros Zanchetta. 2009. The wacky wide web: a
collection of very large linguistically processed web-
crawled corpora. Language Resources and Evalua-
tion, 43(3):209–226.

Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM:
A library for support vector machines. ACM Transac-

1113



tions on Intelligent Systems and Technology, 2:27:1–
27:27.

Michael Collins and Nigel Duffy. 2001. Convolution
kernels for natural language. In Advances in Neural
Information Processing Systems 14, pages 625–632.
MIT Press.

Danilo Croce, Alessandro Moschitti, and Roberto Basili.
2011. Structured lexical similarity via convolution
kernels on dependency trees. In Proceedings EMNLP.

Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Un-
supervised construction of large paraphrase corpora:
Exploiting massively parallel news sources. In Proc.
of COLING ’04, Stroudsburg, PA, USA.

Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. Liblinear: A library
for large linear classification. J. Mach. Learn. Res.,
9:1871–1874, June.

Simone Filice, Giovanni Da San Martino, and Alessandro
Moschitti. 2015. Structural representations for learn-
ing relations between pairs of texts. In Proceedings of
the 53rd Annual Meeting of the Association for Com-
putational Linguistics and the 7th International Joint
Conference on Natural Language Processing (Volume
1: Long Papers), pages 1003–1013, Beijing, China,
July. Association for Computational Linguistics.

Nitin Madnani, Joel Tetreault, and Martin Chodorow.
2012. Re-examining machine translation metrics for
paraphrase identification. In Proceedings of NAACL
HLT ’12. ACL.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word representa-
tions in vector space. arXiv preprint arXiv:1301.3781.

Alessandro Moschitti. 2006. Efficient convolution ker-
nels for dependency and constituent syntactic trees. In
Proc. of ECML’06, pages 318–329.

Aliaksei Severyn and Alessandro Moschitti. 2012.
Structural relationships for large-scale learning of an-
swer re-ranking. In Proceedings of the 35th interna-
tional ACM SIGIR conference on Research and de-
velopment in information retrieval, pages 741–750.
ACM.

Aliaksei Severyn, Massimo Nicosia, and Alessandro
Moschitti. 2013a. Building structures from classifiers
for passage reranking. In CIKM.

Aliaksei Severyn, Massimo Nicosia, and Alessandro
Moschitti. 2013b. Learning adaptable patterns for
passage reranking. In CoNLL.

Richard Socher, Eric H. Huang, Jeffrey Pennington, An-
drew Y. Ng, and Christopher D. Manning. 2011. Dy-
namic pooling and unfolding recursive autoencoders
for paraphrase detection. In Advances in Neural Infor-
mation Processing Systems 24: 25th Annual Confer-
ence on Neural Information Processing Systems 2011.

Proceedings of a meeting held 12-14 December 2011,
Granada, Spain., pages 801–809.

Kateryna Tymoshenko and Alessandro Moschitti. 2015.
Assessing the impact of syntactic and semantic struc-
tures for answer passages reranking. In CIKM, pages
1451–1460. ACM.

Kateryna Tymoshenko, Alessandro Moschitti, and Aliak-
sei Severyn. 2014. Encoding semantic resources in
syntactic structures for passage reranking. In Proceed-
ings of EACL.

David H. Wolpert. 1992. Stacked generalization. Neural
Networks, 5:241–259.

1114


