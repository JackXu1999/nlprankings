



















































Identifying Eyewitness News-worthy Events on Twitter


Proceedings of The Fourth International Workshop on Natural Language Processing for Social Media, pages 7–13,
Austin, TX, November 1, 2016. c©2016 Association for Computational Linguistics

Identifying Eyewitness News-Worthy Events on Twitter

Erika Varis Doggett
Timeline Labs

Santa Monica, CA
erika.varis@timelinelabs.com ∗

Alejandro Cantarero
Timeline Labs

Santa Monica, CA
alejandro@timelinelabs.com †

Abstract

In this paper we present a filter for identifying
posts from eyewitnesses to various event types
on Twitter, including shootings, police activ-
ity, and protests. The filter combines sociolin-
guistic markers and targeted language con-
tent with straightforward keywords and regu-
lar expressions to yield good accuracy in the
returned tweets. Once a set of eyewitness
posts in a given semantic context has been pro-
duced by the filter, eyewitness events can sub-
sequently be identified by enriching the data
with additional geolocation information and
then applying a spatio-temporal clustering. By
applying these steps we can extract a complete
picture of the event as it occurs in real-time,
sourced entirely from social media.

1 Introduction

Current information has always been of paramount
interest to a variety of professionals, notably re-
porters and journalists, but also to crime and disaster
response teams (Diakopoulos et al., 2012; Vieweg
et al., 2010). With the explosion of the internet
and social media, more information is available–
and at a faster rate–on current events than ever be-
fore. A large proportion of non-professional people
are now in the position of news reporters, spreading
information through their social networks in many
cases faster than traditional news media (Beaumont,
2008; Beaumont, 2009; Ritholtz, 2013; Thielman,

∗ Current affiliation: The Walt Disney Studios
Erika.Varis.Doggett@disney.com

† Current affiliation: tronc, Inc.
acantarero@tronc.com

2013; Petrović et al., 2013), whether by sharing
and re-sharing the first news message of an event,
or through immediate personal eyewitness accounts.
This average eyewitness represents a valuable and
untapped information source for news professionals
and others. But given the wide variability of texts
and topics included in social media, the question re-
mains: how to sift the wheat from the chaff?

When your information source is an average user,
distinction of eyewitness posts from noise and off-
topic data is difficult. Everyday social media users
may be situated on the scene, but they do not always
frame their posts in the most informative or con-
sistent language. Their intended audience is a per-
sonal network of friends for whom a lot of contex-
tual information is already available, allowing their
utterances to be highly informative for that network,
but still ambiguous for strangers, or automated pro-
grams.

1.1 Related work

Previous studies have attempted to programmati-
cally identify eyewitnesses with limited success. Im-
ran (2013) achieved only a .57 precision accuracy
for their machine learning eyewitness classifier. Di-
akopoulos et al. (2012) reported a high accuracy for
a static eyewitness classifier at .89, but it is unclear
exactly how it was constructed, losing replicability
and verifiability. In addition, their classifier only an-
alyzed static datasets, whereas the speed of current
events reporting on social media calls for a tool for
online use.

In this paper we present a linguistic method for
identifying eyewitness social media messages from

7



a microblogging service such as Twitter, in a real-
time streaming environment. Our system identifies
messages on different eyewitness topics, including
shootings, police activity, and protests, and can eas-
ily be extended to further areas, such as celebrity
sightings and weather disasters. We further identify
events corresponding to groups of related messages
by enriching the data with geographical location and
then running a spatio-temporal clustering algorithm.

Our work provides the novel contributions of a
system that functions in a real-time streaming en-
vironment, combines information such as semantic
and spatio-temporal clustering, and utilizes simple
and fast computational tools over classifiers that re-
quire large training data and long setup time. In §2
we outline our process for finding eyewitness posts
and events. Section 3 presents results from this sys-
tem, and we provide some concluding remarks in §4.

2 Method

An eyewitness post on a social network is a text doc-
ument giving a first person account from a witness
to the event. As such, we looked to build filtering
rules based on language related to an event, exclud-
ing posts from official agencies (e.g. police, fire
departments), news outlets, and after-the-fact or re-
mote commentary. In this section, we describe how
filters can be constructed that are capable of doing
this in real-time on Twitter.

2.1 Datasets

We collected Twitter data from several real events
to find a set of eyewitness tweets to inform the cre-
ation of linguistic filters. Such a dataset can be
collected from the Twitter API (or any suitable 3rd
party vendor) by doing broad searches in a narrow
time window right after an event has happened. To
build a rule set for shootings, for example, we pulled
data from multiple mass shootings, including shoot-
ings in 2013-2014 at LAX in Los Angeles; Isla
Vista, CA; and the Las Vegas Wal-mart and Cici’s
Pizza. In these cases, searches around the local
place-names at the time of the shootings produced a
very broad set of documents, which were then man-
ually checked for true eyewitness texts, resulting in
an informative set of eyewitness tweets. By exam-
ining these eyewitness tweets, we discovered several

consistent language patterns particular to eyewitness
language.

2.2 Language patterns
One of the challenges of social media language is
that users exhibit a wide range of phrasing to indi-
cate an event has occurred. It is because of our world
knowledge that we are able, as human language
speakers, to understand that the person is discussing
a newsworthy event (Doyle and Frank, 2015).

With that in mind, we propose building filters that
consist of three parts. The first is a semantic context.
Examples here might be criminal shootings, unusual
police activity, or civil unrest. This semantic con-
text may be built using heuristic rules, or it may also
be derived from a machine learning approach. The
second part is the existence of salient linguistic fea-
tures that indicate an eyewitness speaker. Finally, we
look at similar linguistic features that indicate the
user is not an eyewitness, useful for blocking non-
eyewitness posts.

2.2.1 Eyewitness features
First person. First person pronouns are often

dropped on social media, but when present this is
a strong indicator that the event being described was
witnessed first-hand.

Immediate temporal markers. Words such as
“just”, “now”, “rn”1 indicate the event happened im-
mediately prior to the tweet or is ongoing.

Locative markers. Language may be used to
define a place in relation to the speaker, such as
“home”, “work”, “school”, or “here”.

Exclamative or emotive punctuation. Eyewit-
nesses to an exciting or emotional event express
their level of excitement in their messages. Com-
mon ways this may be achieved are through punc-
tuation (exclamation and question marks), emoti-
cons, emoji, or typing in all capital letters. These
are relatively common features used in social media
NLP (Thelwall et al., 2010; Agarwal et al., 2011;
Neviarouskaya et al., 2007).

Lexical exclamations and expletives. A normal
person is likely to use colorful language when wit-
nessing an event. Phrases such as “smh”2, “wtf”,
and expletives are often part of their posts.

1Twitter short-hand for “right now”.
2“Shake my head”

8



2.2.2 Non-eyewitness features
Non-eyewitness features are crucial as a post may

match the semantic context and have at least one of
the linguistic eyewitness markers above, but still not
be an eyewitness account of an event. The main
markers we found for non-eyewitness language fall
into a handful of categories, described below.

Jokes, memes, and incongruous emotion or
sentiment. The expected reaction to a violent crime
or disaster may include shock, sadness, anxiety,
confusion, and fear, among others (Shalev, 2002;
Armsworth and Holaday, 1993; North et al., 1994;
Norris, 2007)3. As such, it is reasonable to remove
posts with positive sentiment and emotion from eye-
witness documents related to a traumatic incident
(e.g. shootings).

Wrong part of speech, mood, or tense. The verb
“shoot” in the first person is unlikely to be used in a
criminal shooting context on a social network. The
conditional mood, for example in phrases such as
“what if, would’ve, wouldn’t”, indicates a hypothet-
ical situation rather than a real event. Similarly, fu-
ture tense does not indicate someone is witnessing
or has witnessed an event.

Popular culture references. Flagging and re-
moving posts with song lyrics or references to mu-
sic, bands, video games, movies, or television shows
can greatly improve results as it is not uncommon
for eyewitness features to be referencing a fictional
event the user saw in one of these mediums.

Temporal markers. Language such as “last
night, last week, weeks ago, months ago” and sim-
ilar phrases suggest an event happened an extended
period of time in the past, and is not a current eye-
witness.

2.3 Finding eyewitness events

Identifying an event from a set of eyewitness posts
can be done using a simple clustering approach.
Most common clustering methods on text data fo-

3While it is possible to have a psycho- or sociopathic wit-
ness who would not react with typical trauma emotions (Fowles,
1980; Ekman, 1985; Herpetz et al., 2001; Patrick, 1994), we
judged this situation to be sufficiently rare to be discounted. In
addition, while “gallows humor” occurs among first responders,
it is not generally widely spread outside their cohort (Moran,
2002), or is indicative of after-the-fact third party commen-
tary (Phillips, 2015; Davies, 2003).

cus on semantic similarity. However, the eyewitness
filters we created already enforced a level of seman-
tic similarity for their resulting documents, so such
clustering would not be effective for our use case.

Multiple separate events of a newsworthy nature
are unlikely to be occurring simultaneously in the
same location at the same time, or such instances
will be considered part of a single large event.
Therefore, we propose using a spatio-temporal clus-
tering algorithm to identify potential events. By
forcing the spatial proximity to be small (limited to
approximately a neighborhood in size) and the tem-
poral locality to be similarly tight, say less than 30
minutes, we can effectively group documents related
to events. A good summary of such methods is pro-
vided in Kisilevich et al. (2010).

2.4 Method summary

In this section, we describe a complete process for
finding eyewitness posts and events.

We start by enriching each document in the feed
with geolocation information of the Twitter user, for
use in the clustering step to identify events. Ge-
olocation information is central to our approach to
finding events, but less than 5% of tweets have lo-
cation data available. There are many approaches
that can be used to enrich social media posts with
a prediction of a user’s location. Good summaries
are available in Ajao et al. (2015) and Jurgens et
al. (2015). We implemented the method described
in Apreleva and Cantarero (2015) and were able to
add user location information to about 85% of users
in our datasets with an 8 km median error. This is
accurate enough to place users in their city or neigh-
borhood and enables us to find more posts related to
the same event.

After enriching the data, we apply the semantic
context topic filters, then the eyewitness linguistic
features, and then remove documents matching the
non-eyewitness features. This produces a set of eye-
witness documents. Specific examples of how to
construct these filter rules for criminal shootings,
police activity, and protests are available on github4.
Further event types could be easily constructed by
combining a relevant semantic context with the eye-
witness linguistic features presented here.

4https://github.com/erikavaris/topics

9



This set of eyewitness documents can then be
run through a spatio-temporal clustering approach
to find events. In our examples, the set of eyewit-
ness documents never had more than around 100-
200 documents in a 24-hour period. Since this set
is so small, we were able to use a simple approach
to clustering. We start by computing the complete
distance matrix for all points in the dataset using
the greater circle distance measure. The greater cir-
cle distance is the shortest distance between two
points on a sphere, a good approximation to dis-
tances on the surface of the Earth. We can then clus-
ter points using an algorithm such as DBSCAN (Es-
ter et al., 1996). DBSCAN clusters together points
based on density and will mark as outliers points
in low-density regions. It is commonly available
in many scientific computing and machine learning
packages in multiple programming languages, and
hence a good choice for our work.

For each cluster we then look at the max distance
between points in the cluster and check that it is less
than a distance τd. In our experiments we set the
threshold to be about 10 km. If the cluster is within
this threshold, we then sort the posts in the cluster
by time, and apply a windowing function over the
sorted list. If there are more than τs documents in
the windowed set, we label this set as an event. We
used τs = 1 for our experiments and time window-
ing functions, tw, in sizes between 20 and 40 min-
utes.

3 Experiments

Using the method described in the previous section,
we built filter rules for criminal shootings, unusual
police activity, and protests. We ran each filter rule
over the Twitter Firehose (unsampled data) on a 24/7
basis. We then sampled random days from each
filter, pulling data back for 24 hours, and applied
the spatio-temporal algorithm to identify potential
events.

Since the resulting sets of documents are rel-
atively small, we measured the accuracy of our
method by hand. Generally a method of this type
might report on the precision and recall of the solu-
tion, but it is not possible to truly measure the re-
call without reading all messages on Twitter in the
time period to make sure posts were not missed. In

our case, we simply conducted a search of news ar-
ticles after the fact to see if any major events were
not picked up by the filter rules on the days that were
sampled. For the days that we sampled, there were
no major news events that failed to show up in our
filters.

We optimized for precision than recall in this
study as the goal is to surface potential events oc-
curring on Twitter that may be newsworthy. It is
more useful to correctly identify a set of documents
as interesting with high accuracy than it is to have
found every newsworthy event on the network but
with many false positives.

Labeling the accuracy (precision) of a method sur-
rounding semantic topic goals is subjective, so we
had multiple people classify the resulting sets as
eyewitness, non-eyewitness, and off-topic, and then
averaged the results. We used the label “non-eye”
on posts that were referencing real events, but were
clearly second- or third-hand commentary and not
clearly embedded in the local community. Most of-
ten these posts represented a later stage of news dis-
semination where the author heard about the event
from afar and decided to comment on it.

While the authors of these tweets were not true
eyewitnesses to these events, they are potentially
interesting from a news commentary perspective,
and were accurate to the event topic. Thus, we
may consider the general event semantic accuracy as
the combined values of “eyewitness” and “non-eye”
tweets.

3.1 Results

3.2 Eyewitness posts

Accuracy results for different sets of eyewitness
posts on different dates are shown in Table 1.

What day data was pulled had an impact on the
accuracy measurement. Table 1 illustrates this dif-
ference particularly in the shooting results. For
02/02/2015, there was more Twitter traffic pertain-
ing to shootings than there was on 06/15/2015,
which likely influenced the higher eyewitness ac-
curacy of 72% vs. 46%. We have generally ob-
served on Twitter that when major events are occur-
ring the conversation becomes more focused and on
topic, and when nothing major is happening results
are lower volume and more noisy.

10



Table 1: Accuracy results for different eyewitness filters. Data were pulled for a 24 hour period on the random dates shown. Count
is the total number of documents in each set.

Topic Date Count Eyewitness Non-Eye Off-Topic Semantic Accuracy
Shooting 2/4/15 126 72% 21% 7% 93%
Shooting 6/15/15 41 46% 20% 34% 66%

Police 3/24/15 100 73% 14% 13% 87%
Police 6/17/15 293 71% 11% 18% 82%

Protests 5/23/15 196 56% 31% 13% 87%
Protests 6/23/15 89 52% 25% 24% 77%

Averages 62% 20% 18% 82%

In these data pulls, the combined eyewitness and
non-eyewitness general semantic accuracy was 93%
and 66%, respectively. We note that on average the
accuracy of our filters is 82% across the days and
filters measured.

3.2.1 Events
The approach outlined in 2.3 successfully sur-

faced on-topic events from the sets of eyewitness
tweets. We found its effectiveness to be low on
individual filter rules due to the low volume of
tweets. We were able to find more relevant clusters
by combining the criminal eyewitness topic streams
– shootings, police activity, and protests – that cor-
responded to events we could later find in the news
media.

In running these experiments, we found that it
was important to add an additional parameter to the
cluster that ensured there were tweets from differ-
ent users. It was common to find multiple tweets
that would cluster from the same user that was shar-
ing updates on a developing situation. Both of these
behaviors are of potential interest and the algorithm
may be adjusted to weight the importance of multi-
ple updates versus different user accounts.

4 Conclusion

This paper presents a novel combinatory method of
identifying eyewitness accounts of breaking news
events on Twitter, including simple yet extensive lin-
guistic filters together with grouping bursts of infor-
mation localized in time and space. Using primar-
ily linguistic filters based on sociolinguistic behav-
ior of users on Twitter, a variety of event types are
explored, with easily implemented extensions to fur-
ther event types.

The filters are particularly appealing in a busi-
ness application; with minimal training we were
able to teach users of our platform to construct new
rules to find eyewitness events in different topical
areas. These users had no knowledge of program-
ming, linguistics, statistics, or machine learning.We
found this to be a compelling way to build real-time
streams of relevant data when resources would not
allow placing a computational linguist, data scien-
tist, or similarly highly trained individual on these
tasks.

The system offers a straightforward technique
for eyewitness filtering compared with Diakopou-
los et al. (2012), easily implemented in a stream-
ing environment, requiring no large training datasets
such as with machine learning, and achieving higher
accuracies than comparable machine learning ap-
proaches (Imran et al., 2013). Together with spatio-
temporal clustering to identify eyewitness tweets
that are spatially and temporally proximate, our eye-
witness filter presents a valuable tool for surfacing
breaking news on social media.

For future research, a machine learning layer
could be added with broader linguistic filters, and
may help achieve higher recall while maintaining the
high accuracy achieved with our narrow linguistic
keywords.

References

Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Rambow,
and Rebecca Passonneau. 2011. Sentiment analysis of
twitter data. In Proceedings of the Workshop on Lan-
guage in Social Media, LSM 2011, Portland, Oregon,
June. Association for Computational Linguistics.

Oluwaseun Ajao, Jun Hong, and Weiru Liu. 2015. A sur-

11



vey of location inference techniques on twitter. Jour-
nal of Information Science, 41(6):855–864.

Sofia Apreleva and Alejandro Cantarero. 2015. Predict-
ing the location of users on twitter from low density
graphs. In Big Data (Big Data), 2015 IEEE Interna-
tional Conference on, pages 976–983, Oct.

Mary W. Armsworth and Margot Holaday. 1993. The ef-
fects of psychological trauma on children and adoles-
cents. Journal of counseling and Development : JCD,
72(1), September.

Claudine Beaumont. 2008. Mumbai attacks: Twitter
and flickr used to break news. The Daily Telegraph,
November.

Claudine Beaumont. 2009. New york plane crash: Twit-
ter breaks the news, again. The Daily Telegraph, Jan-
uary.

Christie Davies. 2003. Jokes that follow mass-mediated
disasters in a global electronic age. In Peter Narváez,
editor, Of Corpse: Death and Humor in Folklore and
Popular Culture. Utah State University Press, Logan,
Utah.

Nicholas Diakopoulos, Munmun De Choudhury, and Mor
Naaman. 2012. Finding and assessing social me-
dia information sources in the context of journalism.
In Proceedings of the SIGCHI Conference on Human
Factors in Computing Systems, CHI ’12, pages 2451–
2460, New York, NY, USA. ACM.

Gabriel Doyle and Michael C. Frank. 2015. Shared
common ground influences information density in mi-
croblog texts. In Human Language Technologies: The
2015 Annual Conference of the North American Chap-
ter of the ACL, Denver, Colorado, June. Association
for Computational Linguistics.

Paul Ekman. 1985. Telling lies: Clues to deceit in the
marketplace, marriage, and politics. New York: Nor-
ton.

Martin Ester, Hans-Peter Kriegel, Jörg Sander, and Xi-
aowei Xu. 1996. A density-based algorithm for dis-
covering clusters in large spatial databases with noise.
In Kdd, pages 226–231. AAAI Press.

Don C. Fowles. 1980. The three arousal model: Implica-
tions of gray’s two-factor learning theory for heart rate,
electrodermal activity, and psychopathy. Psychophys-
iology, 17(2):87–104.

Sabine C. Herpetz, Ulrike Werth, Gerald Lukas, Mutaz
Qunaibi, Annette Schuerkens, Hanns-Juergen Kunert,
Roland Freese, Martin Flesch, Ruediger Mueller-
Isberner, Michael Osterheider, and Henning Sass.
2001. Emotion in criminal offenders with psychopa-
thy and borderline personality disorder. Arch Gen Psy-
chiatry, 58(8).

Muhammad Imran, Shady Elbassuoni, Carlos Castillo,
Fernando Diaz, and Patrick Meier. 2013. Extracting

information nuggets from disaster-related messages in
social media. In T. Comes, F. Fiedrich, S. Fortier,
J. Geldermann, and L. Yang, editors, Proceedings from
the 10th International ISCRAM Conference, Baden-
Baden, Germany, May.

David Jurgens, Tyler Finethy, James McCorriston,
Yi Tian Xu, and Derek Ruths. 2015. Geolocation pre-
diction in twitter using social networks: a critical anal-
ysis and review of current practice. In Proceedings
of the 9th International AAAI Conference on Weblogs
and Social Media (ICWSM).

Slava Kisilevich, Florian Mansmann, Mirco Nanni, and
Salvatore Rinzivillo, 2010. Data Mining and Knowl-
edge Discovery Handbook, chapter Spatio-temporal
clustering, pages 855–874. Springer US, Boston, MA.

Carmen C. Moran. 2002. Humor as a moderator of com-
passion fatigue. In Charles R. Figley, editor, Treat-
ing Compassion Fatigue, Psychological Stress Series.
Routledge, June.

Alena Neviarouskaya, Helmut Prendinger, and Mitsuru
Ishizuka. 2007. Textual affect sensing for sociable
and expressive online communication. ASCII, 4738.

Fran H. Norris. 2007. Impact of mass shootings on sur-
vivors, families, and communities. PTSD Research
Quarterly, 18(1).

Carol S. North, Elizabeth M. Smith, and Edward L. Spitz-
nagel. 1994. Posttraumatic stress disorder in survivors
of a mass shooting. American Journal of Psychiatry,
151(1), January.

Christopher J. Patrick. 1994. Emotion and psychopathy:
Startling new insights. Psychophysiology, 31.

Sas̆a Petrović, Miles Osborne, Richard McCreadie, Craig
Macdonald, Iadh Ounis, and Luke Shrimpton. 2013.
Can twitter replace newswire for breaking news? In
International AAAI Conference on Web and Social Me-
dia, ICWSM, Boston, MA, July. Association for the
Advancement of Artificial Intelligence (AAAI).

Whitney Phillips. 2015. This is Why We Can’t Have
Nice Things: Mapping the Relationship Between On-
line Trolling and Mainstream Culture. MIT Press,
February.

Barry Ritholtz. 2013. Twitter is becoming the first and
quickest source of investment news. The Washington
Post, April.

Arieh Y. Shalev. 2002. Treating survivors in the acute
aftermath of traumatic events. In Rachel Yehuda, edi-
tor, Treating Trauma Survivors with PTSD. American
Psychiatric Publishing, Inc., Washington, D.C.

Mike Thelwall, Kevan Buckley, Georgios Paltoglou, and
Di Cai. 2010. Sentiment strength detection in short
informal text. Journal for the American Society for
Information Science and Technology, 61(12).

Sam Thielman. 2013. Twitter breaks news of the boston
marathon explosions. AdWeek, April.

12



Sarah Vieweg, Amanda L. Hughes, Kate Starbird, and
Leysia Palen. 2010. Microblogging during two natu-
ral hazards events: What twitter may contribute to situ-
ational awareness. In Proceedings of the SIGCHI Con-
ference on Human Factors in Computing Systems, CHI
’10, pages 1079–1088, New York, NY, USA. ACM.

13


