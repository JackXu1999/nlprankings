



















































Combining Character and Word Information in Neural Machine Translation Using a Multi-Level Attention


Proceedings of NAACL-HLT 2018, pages 1284–1293
New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics

Combining Character and Word Information in Neural Machine
Translation Using a Multi-Level Attention

Huadong Chen†, Shujian Huang†∗, David Chiang‡, Xinyu Dai†, Jiajun Chen†
†State Key Laboratory for Novel Software Technology, Nanjing University

{chenhd,huangsj,daixinyu,chenjj}@nlp.nju.edu.cn
‡Department of Computer Science and Engineering, University of Notre Dame

dchiang@nd.edu

Abstract

Natural language sentences, being hierarchi-
cal, can be represented at different levels of
granularity, like words, subwords, or charac-
ters. But most neural machine translation sys-
tems require the sentence to be represented as
a sequence at a single level of granularity. It
can be difficult to determine which granularity
is better for a particular translation task. In this
paper, we improve the model by incorporating
multiple levels of granularity. Specifically, we
propose (1) an encoder with character atten-
tion which augments the (sub)word-level rep-
resentation with character-level information;
(2) a decoder with multiple attentions that en-
able the representations from different levels
of granularity to control the translation cooper-
atively. Experiments on three translation tasks
demonstrate that our proposed models out-
perform the standard word-based model, the
subword-based model and a strong character-
based model.

1 Introduction

Neural machine translation (NMT) models (Britz
et al., 2017) learn to map from source lan-
guage sentences to target language sentences
via continuous-space intermediate representa-
tions. Since word is usually thought of as the ba-
sic unit of language communication (Jackendoff,
1992), early NMT systems built these represen-
tations starting from the word level (Sutskever
et al., 2014; Bahdanau et al., 2015; Cho et al.,
2014; Weng et al., 2017). Later systems tried us-
ing smaller units such as subwords to address the
problem of out-of-vocabulary (OOV) words (Sen-
nrich et al., 2016; Wu et al., 2016).

Although they obtain reasonable results, these
word or sub-word methods still have some poten-
tial weaknesses. First, the learned representations

∗ Corresponding author.

of (sub)words are based purely on their contexts,
but the potentially rich information inside the unit
itself is seldom explored. Taking the Chinese word
被打伤 (bei-da-shang) as an example, the three
characters in this word are a passive voice marker,
“hit” and “wound”, respectively. The meaning of
the whole word, “to be wounded”, is fairly com-
positional. But this compositionality is ignored if
the whole word is treated as a single unit.

Secondly, obtaining the word or sub-word
boundaries can be non-trivial. For languages like
Chinese and Japanese, a word segmentation step
is needed, which must usually be trained on la-
beled data. For languages like English and Ger-
man, word boundaries are easy to detect, but sub-
word boundaries need to be learned by methods
like BPE. In both cases, the segmentation model
is trained only in monolingual data, which may re-
sult in units that are not suitable for translation.

On the other hand, there have been multiple
efforts to build models operating purely at the
character level (Ling et al., 2015a; Yang et al.,
2016; Lee et al., 2017). But splitting this finely
can increase potential ambiguities. For example,
the Chinese word 红茶 (hong-cha) means “black
tea,” but the two characters means “red” and “tea,”
respectively. It shows that modeling the charac-
ter sequence alone may not be able to fully uti-
lize the information at the word or sub-word level,
which may also lead to an inaccurate representa-
tion. A further problem is that character sequences
are longer, making them more costly to process
with a recurrent neural network model (RNN).

While both word-level and character-level in-
formation can be helpful for generating better rep-
resentations, current research which tries to ex-
ploit both word-level and character-level informa-
tion only composed the word-level representation
by character embeddings with the word boundary
information (Ling et al., 2015b; Costa-jussà and

1284



Fonollosa, 2016) or replaces the word represen-
tation with its inside characters when encounter-
ing the out-of-vocabulary words (Luong and Man-
ning, 2016; Wu et al., 2016). In this paper, we pro-
pose a novel encoder-decoder model that makes
use of both character and word information. More
specifically, we augment the standard encoder to
attend to individual characters to generate better
source word representations (§3.1). We also aug-
ment the decoder with a second attention that at-
tends to the source-side characters to generate bet-
ter translations (§3.2).

To demonstrate the effectiveness of the pro-
posed model, we carry out experiments on
three translation tasks: Chinese-English, English-
Chinese and English-German. Our experiments
show that: (1) the encoder with character atten-
tion achieves significant improvements over the
standard word-based attention-based NMT system
and a strong character-based NMT system; (2) in-
corporating source character information into the
decoder by our multi-scale attention mechanism
yields a further improvement, and (3) our mod-
ifications also improve a subword-based NMT
model. To the best of our knowledge, this is the
first work that uses the source-side character in-
formation for all the (sub)words in the sentence to
enhance a (sub)word-based NMT model in both
the encoder and decoder.

2 Neural Machine Translation

Most NMT systems follow the encoder-decoder
framework with attention mechanism proposed by
Bahdanau et al. (2015). Given a source sentence
x = x1 · · · xl · · · xL and a target sentence y =
y1 · · · y j · · · yJ , we aim to directly model the trans-
lation probability:

P(y | x; θ) =
J∏

1

P(y j | y< j, x; θ),

where θ is a set of parameters and y< j is the
sequence of previously generated target words.
Here, we briefly describe the underlying frame-
work of the encoder-decoder NMT system.

2.1 Encoder

Following Bahdanau et al. (2015), we use a
bidirectional RNN with gated recurrent units
(GRUs) (Cho et al., 2014) to encode the source

sentence:
−→
hl = GRU(

−−→
hl−1, sl;

−→
θ )

←−
hl = GRU(

←−−
hl−1, sl;

←−
θ )

(1)

where sl is the l-th source word’s embedding,
GRU is a gated recurrent unit,

−→
θ and

←−
θ are the pa-

rameters of forward and backward GRU, respec-
tively; see Cho et al. (2014) for a definition.

The annotation of each source word xl is ob-
tained by concatenating the forward and backward
hidden states:

←→
hl =


−→
hl←−
hl

 .

The whole sequence of these annotations is used
by the decoder.

2.2 Decoder
The decoder is a forward RNN with GRUs pre-
dicting the translation y word by word. The prob-
ability of generating the j-th word y j is:

P(y j | y< j, x; θ) = softmax(

t j−1
d j
c j

)

where t j−1 is the word embedding of the ( j − 1)-th
target word, d j is the decoder’s hidden state of
time j, and c j is the context vector at time j. The
state d j is computed as

d j = GRU
(
d j−1,

[
t j−1
c j

]
; θd

)
.

The attention mechanism computes the context
vector ci as a weighted sum of the source annota-
tions,

c j =
I∑

i=1

α j,l
←→
hl (2)

where the attention weight α ji is

α ji =
exp (e ji)∑I

i′=1 exp (e ji′)
(3)

and

e jl = vTa tanh (Wad j−1 + Ua
←→
hl ) (4)

where va, Wa and Ua are the weight matrices of
the attention model, and e jl is an attention model

that scores how well d j−1 and
←→
hl match.

With this strategy, the decoder can attend to the
source annotations that are most relevant at a given
time.

1285



Word 
Embeddings

hl

hl

′ hl+1′

sl+1

Annotations

cl
I

cl
O

Attention 
Mechanism

hl-1

Op-1 OqOp
Character 

Embeddings

αl,p-2O αl,pI αl,qI

Oq+1

αl,q+1O

Op+1

αl,p+1I

Op-2

αl,p-1O

Oq+2

αl,q+2O

l

l l lllll

llllll

sl

Figure 1: Forward encoder with character attention at
time step l. The encoder alternates between reading
word embeddings and character context vectors. cl I and
clO denotes the inside and outside character-level con-
text vectors of the l-th word, respectively.

3 Character Enhanced Neural Machine
Translation

In this section, we present models which make use
of both character-level and word-level information
in the encoder-decoder framework.

3.1 Encoder with Character Attention
The encoder maps the source sentence to a se-
quence of representations, which is then used by
the attention mechanism. The standard encoder
operates purely on (sub)words or characters. How-
ever, we want to encode both, since both levels can
be linguistically significant (Xiong et al., 2017).

To incorporate multiple levels of granularity, we
extend the encoder with two character-level at-
tentions. For each source word, the characters of
the whole sentence can be divided into two parts,
those inside the word and those outside the word.
The inside characters contain information about
the internal structure of the word. The outside
characters may provide information about patterns
that cross word boundaries. In order to distinguish
the influence of the two, we use two separate atten-
tions, one for inside characters and one for outside
characters.

Note that we compute attention directly from
the character embedding sequence instead of using
an additional RNN layer. This helps to avoid the
vanishing gradient problem that would arise from
increasing the sequence length, and also keeps the
computation cost at a low level.

Figure 1 illustrates the forward encoder with
character attentions. We write the character em-
beddings as o = o1 · · · ok · · · oK . Let pl and ql be

the starting and ending character position, respec-
tively, of word xl. Then opl · · · oql are the inside
characters of word xl; o1 · · · opl−1 and oql+1 · · · oK
are the outside characters of word xl.

The encoder is an RNN that alternates between
reading (sub)word embeddings and character-
level information. At each time step, we first read
the word embedding:

−→
hl ′ = GRU(

−−→
hl−1, sl;

−→
θ′) (5)

Then we use the attention mechanisms to com-
pute character context vectors for the inside char-
acters:

cIl =
ql∑

m=pl

αIlmom

αIlm =
exp (elm)∑ql

m′=pl exp (elm′)

elm = vI · tanh (W I−→hl ′ + U Iom).
The outside character context vector clO is calcu-
lated in a similar way, using a different set of pa-
rameters, i.e. WO,UO, vO instead of W I ,U I , vI .

The inside and outside character context vectors
are combined by a feed-forward layer and fed into
the encoder RNN, forming the character-enhanced
word representation

−→
hl:

cCl = tanh(W
IcIl + W

OclO)
−→
hl = GRU(

−→
hl ′, cCl ;

−→
θ )

Note that this GRU does not share parameters with
the GRU in (5).

The backward hidden states are calculated in a
similar manner.

3.2 Decoder with Multi-Scale Attention
In order to fully exploit the character-level infor-
mation, we also make extensions to the decoder, so
that the character-level information can be taken
into account while generating the translation.

We propose a multi-scale attention mechanism
to get the relative information of current decod-
ing step from both word-level and character-level
representations. This attention mechanism is build
from the high-level to the low-level representa-
tion, in order to enhance high-level representation
with fine-grained internal structure and context.
The multi-scale attention mechanism is built (as
shown in Figure 2) from word-level to character-
level.

1286



d0 dj

yj

Decoder
Initial State dj-1

hl
hl

hL
hL

h1
h1

sls1 sL

Encoder

αj,1 αj,l αj,L

O1 OKOk
Character 

Embeddings

αj,1
c αj,k

c αj,K
c

Word 
Embeddings

cj
w

cj
c

Figure 2: Illustration of the decoder with our multi-
scale attention mechanism.

First, we get the word-level information. The
context vector cwj is calculated following the stan-
dard attention model (Eq. 2–4). And the hidden
state d̃ j is updated.

d̃ j = GRU
(
d j−1,

[
t j−1
cwj

]
; θ̃d

)
, (6)

Then we attend to the character-level represen-
tation, which provides more information about the
word’s internal structure. The context vector ccj
is calculated based on the updated hidden state
above,

ccj =
K∑

k=1

αcjkok

αcjk =
exp (e jk)∑K

k′=1 exp (e jk′)

e j,k = vc · tanh (Wcd̃ j + Ucok).
Finally, the word-level context vector cwj and

character-level context vector ccj are concatenated:

c j =

cwj
ccj

 .

And the final context vector c j is used to help pre-
dict the next target word.

P(y j | y< j, x; θ) = softmax(

t j−1
d j
c j

)

where d j is

d j = GRU(d̃ j, ccj; θd),

With this mechanism, both the (sub)word-level
and character-level representations could be used

to predict the next translation, which helps to en-
sure a more robust and reasonable choice. It may
also help to alleviate the under-translation prob-
lem because the character information could be a
complement to the word.

4 Experiments

We conduct experiments on three translation
tasks: Chinese-English (Zh-En), English-Chinese
(En-Zh) and English-German (En-De). We write
Zh↔En to refer to the Zh-En and En-Zh tasks to-
gether.

4.1 Datasets
For Zh↔En, the parallel training data consists
of 1.6M sentence pairs extracted from LDC cor-
pora, with 46.6M Chinese words and 52.5M En-
glish words, respectively.1 We use the NIST MT02
evaluation data as development data, and MT03,
MT04, MT05, and MT06 as test data. The Chinese
side of the corpora is word segmented using ICT-
CLAS.2 The English side of the corpora is lower-
cased and tokenized.

For En-De, we conduct our experiments on the
WMT17 corpus. We use the pre-processed par-
allel training data for the shared news transla-
tion task provided by the task organizers.3 The
dataset consitst of 5.6M sentence pairs. We use
newstest2016 as the development set and eval-
uate the models on newstest2017.

4.2 Baselines
We compare our proposed models with several
types of NMT systems:

• NMT: the standard attentional NMT model
with words as its input (Bahdanau et al.,
2015).

• RNN-Char: the standard attentional NMT
model with characters as its input.

• CNN-Char: a character-based NMT model,
which implements the convolutional neural
network (CNN) based encoder (Costa-jussà
and Fonollosa, 2016).

• Hybrid: the mixed word/character model
proposed by Wu et al. (2016).

1LDC2002E18, LDC2003E14, the Hansards portion of
LDC2004T08, and LDC2005T06.

2http://ictclas.nlpir.org
3http://data.statmt.org/wmt17/

translation-task/preprocessed/de-en/

1287



System MT02 MT03 MT04 MT05 MT06 Mean ∆

NMT 33.76 31.88 33.15 30.55 27.47 30.76
Word-att 34.28 32.26 33.82 31.02 27.93 31.26 +0.50
Char-att 34.85 33.71 34.91 32.08 28.66 32.34 +1.58

Table 1: Performance of the encoder with character attention and the encoder with word attention. Char-att and
Word-att denotes the encoder with character attention and the encoder with word attention, respectively.

• BPE: a subword level NMT model, which
processes the source side sentence by Byte
Pair Encoding (BPE) (Sennrich et al., 2016).

We used the dl4mt implementation of the atten-
tional model,4 reimplementing the above models.

4.3 Details

Training For Zh↔En, we filter out the sentence
pairs whose source or target side contain more
than 50 words. We use a shortlist of the 30,000
most frequent words in each language to train our
models, covering approximately 98.2% and 99.5%
of the Chinese and English tokens, respectively.
The word embedding dimension is 512. The hid-
den layer sizes of both forward and backward se-
quential encoder are 1024. For fair comparison,
we also set the character embedding size to 512,
except for the CNN-Char system. For CNN-Char,
we follow the standard setting of the original pa-
per (Costa-jussà and Fonollosa, 2016).

For En-De, we build the baseline system using
joint BPE segmentation (Sennrich et al., 2017).
The number of joint BPE operations is 90,000. We
use the total BPE vocabulary for each side.

We use Adadelta (Zeiler, 2012) for optimization
with a mini-batch size of 32 for Zh↔En and 50 for
En-De.

Decoding and evaluation We use beam search
with length-normalization to approximately find
the most likely translation. We set beam width to
5 for Zh↔En and 12 for En-De. The translations
are evaluated by BLEU (Papineni et al., 2002). We
use the multi-bleu script for Zh↔En,5 and the
multi-bleu-detok script for En-De.6

4https://github.com/nyu-dl/dl4mt-tutorial
5https://github.com/moses-smt/mosesdecoder/

blob/master/scripts/generic/multi-bleu.perl
6https://github.com/EdinburghNLP/nematus/

blob/master/data/multi-bleu-detok.perl

4.4 Results: Encoder with character
attention

This set of experiments evaluates the effectiveness
of our proposed character enhanced encoder. In
Table 1, we first compare the encoder with char-
acter attention (Char-att) with the baseline word-
based model. The result shows that our extension
of the encoder can obtain significantly better per-
formance (+1.58 BLEU).

Then, in order to investigate whether the im-
provement comes from the extra parameters in the
character layer, we compare our model to a word
embedding enhanced encoder. When the word em-
bedding enhanced encoder encodes a word, it at-
tends to the word’s embedding and other word
embedding in the sentence instead of attending
to the word’s inside and outside character em-
beddings. The results show that the word embed-
ding enhanced encoder (Word-att) only gets a 0.5
BLEU improvement than the baseline, while our
model is significantly better (+1.58 BLEU). This
shows that the benefit comes from the augmented
character-level information which help the word-
based encoder to learn a better source-side repre-
sentation.

Finally, we compare our character enhanced
model with several types of systems includ-
ing a strong character-based model proposed
by Costa-jussà and Fonollosa (2016) and a mixed
word/character model proposed by Wu et al.
(2016). In Table 2, rows 2 and 2′ confirm the find-
ing of Yang et al. (2016) that the traditional RNN
model performs less well when the input is a se-
quence of characters. Rows 4 and 4′ indicate that
Wu et al. (2016)’s scheme to combine of words
and characters is effective for machine translation.
Our model (row 5) outperforms other models on
the Zh-En task, but only outperforms the word-
based model on En-Zh. The results may suggest
that the CNN and RNN methods is also strong in
building the source representation.

1288



Task # System MT02 MT03 MT04 MT05 MT06 Mean ∆

Zh-En

1 NMT 33.76 31.88 33.15 30.55 27.47 30.76
2 RNN-Char 32.22 31.05 31.41 28.85 25.99 29.32 −1.44
3 CNN-Char 33.69 32.06 33.10 30.40 27.67 30.81 +0.05
4 Hybrid 34.33 33.10 33.41 30.96 28.00 31.37 +0.60
5 Char-att 34.85 33.71 34.91 32.08 28.66 32.34 +1.58
6 Multi-att 34.61 33.26 34.42 31.06 28.24 31.75 +0.98
7 5+6 35.42 33.9 35.23 32.62 29.36 32.68 +2.02

En-Zh

1′ NMT 31.58 22.20 23.47 22.50 21.47 22.41
2′ RNN-Char 28.78 21.03 21.70 19.81 20.98 20.88 −1.53
3′ CNN-Char 31.36 23.60 24.71 22.75 23.05 23.53 +1.12
4′ Hybrid 31.31 24.45 24.65 23.10 23.62 23.96 +1.55
5′ Char-att 30.93 23.63 24.42 21.92 23.6 23.39 +0.98
6′ Multi-att 30.17 22.09 24.09 22.29 23.8 23.07 +0.66
7′ 5′+6′ 32.91 25.02 25.69 24.03 25.20 24.99 +2.58

Table 2: Performance of different systems on the Chinese-English and English-Chinese translation tasks. Our
encoder with character attention (Char-att) improves over all other models on Zh-En and over the word-based
baseline on En-Zh. Adding our decoder with multi-scale attention (Multi-att) outperforms all other models.

Task # System MT02 MT03 MT04 MT05 MT06 Mean ∆

Zh-En
8 BPE 34.66 33.65 34.69 30.80 27.66 31.70 +0.94
9 Char-att 35.20 34.93 35.39 31.62 28.56 32.63 +1.86
10 9+Multi-att 36.68 35.39 35.93 32.08 29.74 33.29 +2.52

En-Zh
8′ BPE 30.17 22.09 24.09 22.29 23.80 23.07 +0.66
9′ Char-att 30.95 23.07 25.19 22.74 24.27 23.82 +1.41
10′ 9′+Multi-att 32.36 24.91 25.79 23.42 24.88 24.75 +2.34

Table 3: Comparison of our models on top of the BPE-based NMT model and the original BPE-based model on
the Chinese-English and English-Chinese translation tasks. Our models improve over the BPE baselines.

4.5 Results: Multi-scale attention

Rows 6 and 6′ in Table 2 verify that our multi-
scale attention mechanism can obtain better re-
sults than baseline systems. Rows 7 and 7′ in Ta-
ble 2 show that our proposed multi-scale attention
mechanism further improves the performance of
our encoder with character attention, yielding a
significant improvement over the standard word-
based model on both Zh-En (+2.02 vs. row 1) task
and En-Zh translation task (+2.58 vs. row 1′).

Compared to the CNN-Char model, our model
still gets +1.97 and +1.46 BLEU improvement
on Zh-En and En-Zh, respectively. Compared to
the mixed word/character model proposed by (Wu
et al., 2016), we find that our best model gives a
better result, demonstrating the benefits of exploit-
ing the character level information during decod-
ing.

System Dev Test ∆

BPE 28.41 23.05
Char-att 29.80 23.87 +0.82

+Multi-att 30.52 24.48 +1.43

Table 4: Case-sensitive BLEU on the English-German
translation tasks. Our systems improve over a baseline
BPE system.

4.6 Results: Subword-based models

Currently, subword-level NMT models are widely
used for achieving open-vocabulary translation.
Sennrich et al. (2016) introduced a subword-level
NMT model using subword-level segmentation
based on the byte pair encoding (BPE) algorithm.
In this section, we investigate the effectiveness of
our character enhanced model on top of the BPE
model. Table 3 shows the results on the Zh-En task

1289



(a) OOV words

Source 互联网业务仍将是中国通通通信信信业业业增长速度最快的业务 [...]

Reference
internet will remain the business with the fastest growth in china ’s telecommu-
nication industry [...]

NMT internet business will remain the fastest growing business in china . [...]

Hybrid
the internet will remain the fastest of china ’s communications growth speed
[...]

Ours
internet business will continue to be the fastest growing business of china ’s
telecommunications industry [...]

(b) Frequent words

Source [...]不是目前在巴勒斯坦被被被占占占领领领土土土所发生的事件的根源 [...]

Reference
[...] not the source of what happened on the palestinian territory occupied by
israel [...]

NMT
[...] such actions were not the source of the incidents of the current palestinian
occupation [...]

CNN-Char
[...] not the only source of events that took place in the palestinian occupation
[...]

Ours
[...] not the root of the current incidents that took place in the palestinian occu-
pied territories [...]

Source [...]将东东东西西西方方方冷冷冷战战战的象征柏林墙的三块墙体赠送到了联合国

Reference
[...] presented the united nations with three pieces of the berlin wall , a symbol
of the cold war between the east and the west .

NMT [...] sent the three pieces of UNK UNK to the un to the un

CNN-Char
[...] sent three pieces of UNK to the united nations, which was the cold war in
eastern china .

Ours
[...] presented the un on the 4th of the three wall UNK of the eastern and west-
ern cold war .

Table 5: Sample translations. For each example, we show the source, the reference and the translation from our
best model. “Ours” means our model with both Char-att and Multi-att.

and En-Zh translation task. Rows 8 and 8′ con-
firm that BPE slightly improves the performance
of the word-based model. But both our charac-
ter enhanced encoder and the multi-scale attention
yield better results. Our best model leads to im-
provements of up to 1.58 BLEU and 1.68 BLEU
on the Zh-En task and En-Zh translation task, re-
spectively.

We also conduct experiments on the En-De
translation task (as shown in Table 4). The result
is consistent with Zh-En task and En-Zh transla-
tion tasks. Our best model obtains 1.43 BLEU im-
provement over the BPE model.

System with OOV ∆ no OOV ∆

NMT 28.47 38.21
Hybrid 29.83 +1.36 37.79 −0.43
Ours 30.80 +2.33 39.39 +1.18

Table 6: Translation performance on source sentences
with and without OOV words. “Ours” means our model
with both Char-att and Multi-att.

4.7 Analysis

We have argued that the character information is
important not only for OOV words but also fre-
quent words. To test this claim, we divided the
MT03 test set into two parts according to whether

1290



the sentence contains OOV words, and evaluated
several systems on the two parts. Table 6 lists
the results. Although the hybrid model achieves
a better result on the sentences which contain
OOV words, it actually gives a worse result on the
sentences without OOV words. By contrast, our
model yields the best results on both parts of the
data. This shows that frequent words also benefit
from fine-grained character-level information.

Table 5 shows three translation examples. Ta-
ble 5(a) shows the translation of an OOV word
通信业 (tong-xin-ye, telecommunication indus-
try). The baseline NMT system can’t translate the
whole word because it is not in the word vocab-
ulary. The hybrid model translates the word to
“communication,” which is a valid translation of
the first two characters 通信. This mistranslation
also appears to affect other parts of the sentence
adversely. Our model translates the OOV word
correctly.

Table 5(b) shows two translation samples in-
volving frequent words. For the compound word
被占领土 (beizhanlingtu, occupied territory), the
baseline NMT system only partly translates the
word as “occupation” and ignores the main part
领土 (lingtu, territory). The CNN-Char model,
which builds up the word-level representation
from characters, also cannot capture领土 (lingtu).
However, our model correctly translates the word
as “occupied territories.” (The phrase “by Israel”
in the reference was inserted by the translator.)
The word东西方 (dongxifang, east and west) and
冷战 (lengzhan, cold war) are deleted by the base-
line model, and even the CNN-Char model trans-
lates 东西方 (dongxifang) incorrectly. By con-
trast, our model can make use of both words and
characters to translate the word 东西方 (dongxi-
fang) reasonably well as “eastern and western.”

5 Related Work

Many recent studies have focused on using
character-level information in neural machine
translation systems. These efforts could be roughly
divided into the following two categories.

The first line of research attempted to build neu-
ral machine translation models purely on char-
acters without explicit segmentation. Lee et al.
(2017) proposed to directly learn the segmentation
from characters by using convolution and pooling
layers. Yang et al. (2016) composed the high-level
representation by the character embedding and its

surrounding character-level context with a bidirec-
tional and concatenated row convolution network.
Different from their models, our model aims to use
characters to enhance words representation instead
of depending on characters solely; our model is
also much simpler.

The other line of research attempted to com-
bine character-level information with word-level
information in neural machine translation models,
which is more similar with our work. Ling et al.
(2015a) employed a bidirectional LSTM to com-
pose character embeddings to form the word-level
information with the help of word boundary in-
formation. Costa-jussà and Fonollosa (2016) re-
placed the word-lookup table with a convolutional
network followed by a highway network (Srivas-
tava et al., 2015), which learned the word-level
representation by its constituent characters. Zhao
and Zhang (2016) designed a decimator for their
encoder, which effectively uses a RNN to com-
pute a word representation from the characters of
the word. These approaches only consider word
boundary information and ignore the word-level
meaning information itself. In contrast, our model
can make use of both character-level and word-
level information.

Luong and Manning (2016) proposed a hybrid
scheme that consults character-level information
whenever the model encounters an OOV word.
Wu et al. (2016) converted the OOV words in
the word-based model into the sequence of its
constituent characters. These methods only focus
on dealing with OOV words by augmenting the
character-level information. In our work, we aug-
ment the character information to all the words.

6 Conclusion

In this paper, we have investigated the potential
of using character-level information in word-based
and subword-based NMT models by proposing
a novel character-aware encoder-decoder frame-
work. First, we extended the encoder with a
character attention mechanism for learning bet-
ter source-side representations. Then, we incor-
porated information about source-side characters
into the decoder with a multi-scale attention, so
that the character-level information can cooper-
ate with the word-level information to better con-
trol the translation. The experiments have demon-
strated the effectiveness of our models. Our anal-
ysis showed that both OOV words and frequent

1291



words benefit from the character-level informa-
tion.

Our current work only uses the character-level
information in the source-side. For future work, it
will be interesting to make use of finer-grained in-
formation on the target side as well.

Acknowledgments

The authors would like to thank the anonymous
reviewers for their valuable comments. This work
is supported by the National Science Foundation
of China (No. 61772261 and 61672277) and the
Jiangsu Provincial Research Foundation for Ba-
sic Research (No. BK20170074). Part of Huadong
Chen’s contribution was made while visiting Uni-
versity of Notre Dame. His visit was supported by
the joint PhD program of China Scholarship Coun-
cil.

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-

gio. 2015. Neural machine translation by jointly
learning to align and translate. In ICLR 2015.

Denny Britz, Quoc Le, and Reid Pryzant. 2017. Effec-
tive domain mixing for neural machine translation.

Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using RNN encoder-decoder
for statistical machine translation. In Proc. EMNLP,
pages 1724–1734.

Marta R. Costa-jussà and José A. R. Fonollosa. 2016.
Character-based neural machine translation. In Pro-
ceedings of the 54th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 2: Short
Papers), pages 357–361, Berlin, Germany. Associa-
tion for Computational Linguistics.

Ray S. Jackendoff. 1992. Semantic Structures. MIT
Press.

Jason Lee, Kyunghyun Cho, and Thomas Hofmann.
2017. Fully character-level neural machine trans-
lation without explicit segmentation. Transactions
of the Association for Computational Linguistics,
5:365–378.

Wang Ling, Isabel Trancoso, Chris Dyer, and Alan W.
Black. 2015a. Character-based neural machine
translation. CoRR, abs/1511.04586.

Wang Ling, Isabel Trancoso, Chris Dyer, and Alan W
Black. 2015b. Character-based neural machine
translation. arXiv preprint arXiv:1511.04586.

Minh-Thang Luong and D. Christopher Manning.
2016. Achieving open vocabulary neural machine
translation with hybrid word-character models. In
Proceedings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 1054–1063. Association for
Computational Linguistics.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proc. ACL, pages
311–318.

Rico Sennrich, Alexandra Birch, Anna Currey, Ulrich
Germann, Barry Haddow, Kenneth Heafield, An-
tonio Valerio Miceli Barone, and Philip Williams.
2017. The university of edinburgh’s neural mt sys-
tems for wmt17.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Neural machine translation of rare words
with subword units. In Proceedings of the 54th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1715–
1725, Berlin, Germany. Association for Computa-
tional Linguistics.

Rupesh Kumar Srivastava, Klaus Greff, and Jürgen
Schmidhuber. 2015. Training very deep networks.
In Proceedings of the 28th International Confer-
ence on Neural Information Processing Systems -
Volume 2, NIPS’15, pages 2377–2385, Cambridge,
MA, USA. MIT Press.

Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in Neural Information Process-
ing Systems 27, pages 3104–3112.

Rongxiang Weng, Shujian Huang, Zaixiang Zheng,
XIN-YU DAI, and Jiajun CHEN. 2017. Neural ma-
chine translation with word predictions. In Proceed-
ings of the 2017 Conference on Empirical Methods
in Natural Language Processing, pages 136–145,
Copenhagen, Denmark. Association for Computa-
tional Linguistics.

Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V.
Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus
Macherey, Jeff Klingner, Apurva Shah, Melvin
Johnson, Xiaobing Liu, Lukasz Kaiser, Stephan
Gouws, Yoshikiyo Kato, Taku Kudo, Hideto
Kazawa, Keith Stevens, George Kurian, Nishant
Patil, Wei Wang, Cliff Young, Jason Smith, Jason
Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado,
Macduff Hughes, and Jeffrey Dean. 2016. Google’s
neural machine translation system: Bridging the gap
between human and machine translation. CoRR,
abs/1609.08144.

H. Xiong, Z. He, X. Hu, and H. Wu. 2017. Multi-
channel Encoder for Neural Machine Translation.
arXiv:1712.02109.

1292



Zhen Yang, Wei Chen, Feng Wang, and Bo Xu. 2016.
A character-aware encoder for neural machine trans-
lation. In Proceedings of COLING 2016, the 26th
International Conference on Computational Lin-
guistics: Technical Papers, pages 3063–3070, Os-
aka, Japan. The COLING 2016 Organizing Commit-
tee.

Matthew D. Zeiler. 2012. ADADELTA: an adaptive
learning rate method. CoRR, abs/1212.5701.

Shenjian Zhao and Zhihua Zhang. 2016. An efficient
character-level neural machine translation. CoRR,
abs/1608.04738.

1293


