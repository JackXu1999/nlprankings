















































Box of Lies: Multimodal Deception Detection in Dialogues


Proceedings of NAACL-HLT 2019, pages 1768–1777
Minneapolis, Minnesota, June 2 - June 7, 2019. c©2019 Association for Computational Linguistics

1768

Box of Lies R©: Multimodal Deception Detection in Dialogues

Felix Soldner
Department of Security

and Crime Science
University College London

felix.soldner@ucl.ac.uk

Verónica Pérez-Rosas and Rada Mihalcea
Department of Computer Science

and Engineering
University of Michigan, Ann Arbor

{vrncapr,mihalcea}@umich.edu

Abstract

Deception often takes place during everyday
conversations, yet conversational dialogues re-
main largely unexplored by current work on
automatic deception detection. In this pa-
per, we address the task of detecting mul-
timodal deceptive cues during conversational
dialogues. We introduce a multimodal dataset
containing deceptive conversations between
participants playing The Tonight Show Star-
ring Jimmy Fallon R© Box of Lies game, in
which they try to guess whether an object de-
scription provided by their opponent is decep-
tive or not. We conduct annotations of mul-
timodal communication behaviors, including
facial and linguistic behaviors, and derive sev-
eral learning features based on these annota-
tions. Initial classification experiments show
promising results, performing well above both
a random and a human baseline, and reaching
up to 69% accuracy in distinguishing decep-
tive and truthful behaviors.

1 Introduction

Deception occurs often during dialogues, but un-
til now this setting has received little attention
from the research community (Tsunomori et al.,
2015). In this paper, we explore verbal, non-
verbal, and conversational dialog cues between
contestants playing the Box of Lies game in The
Tonight Show Starring Jimmy Fallon R© tv show. In
the game, participants try to guess whether an ob-
ject description provided by their opponent is de-
ceptive or not. The game scenario provides a rich
environment where we can explore several aspects
of deceptive behavior occurring during conversa-
tions. First, it allows us to study conversational
deception in the presence of multiple modalities
such as verbal and non-verbal behaviors. Second,
it provides observable assessments of participant’s
honesty, which is usually an important challenge

during deception research. Third, since partici-
pants experience the pressure to win the game in
front of a big audience, it presumably presents an
environment with high stakes.1

Recent work on multimodal deception detection
has already shown the importance of verbal and
non-verbal behaviors during the automatic iden-
tification of deceit (Abouelenien et al., 2017a).
Following this line of work, our main contribu-
tion consists of investigating whether such modal-
ities can also be leveraged to predict deception
in a conversational dialog as well as exploring
whether the dialogue setting adds meaningful in-
formation to the other modalities to potentially in-
crease the classification performance. Based on
earlier work (Perez-Rosas et al., 2014; Mihalcea
et al., 2013), we hypothesize that (1) including
dialogue features in addition to other multimodal
features (language and facial expressions) while
training a classifier increases the prediction perfor-
mance; (2) automatic classification of truthful and
deceptive behavior is better than random guess-
ing (50% for equal class sizes); and (3) automatic
classification of truthful and deceptive responses
is more accurate than human judgments (based on
performance of participants in the dataset).

To address these hypotheses, we first generate
a dataset containing verbal and non-verbal anno-
tations of deceptive and truthful interactions be-
tween the game participants. Next, we derive lin-
guistic, visual, and dialog cues based on our anno-
tations for the verbal and non-verbal components
of the dataset. The features are then used to con-
duct several learning experiments under different
scenarios that attempt to distinguish between de-
ceptive and truthful utterances either by focusing
on the statements generated by one participant at a
time (the game’s host or the guest), or by address-

1To our knowledge, these conversations are not scripted.



1769

ing them all together.
Our initial experiments show that language,

as well as behavioral and dialog features, carry
meaningful information. Moreover, the automatic
classification of deception can be performed with
an accuracy that is better than random guessing
and ourperforms human judgments.

2 Related Work

To tackle the problem of reliably detecting decep-
tion, researchers have applied various forms of
automated deception detection methods that rely
on machine learning approaches, which are able
to incorporate a variety of behavioral cues from
text, audiovisual or physiological data sources (Ott
et al., 2011; Fornaciari and Poesio, 2013; Mihalcea
and Strapparava, 2009; Abouelenien et al., 2016).

Many studies focused on text-based classifica-
tion, detecting false online reviews (Ott et al.,
2013) or deceptive transcribed statements from
court hearings (Fornaciari and Poesio, 2013).
Other studies utilized visual cues such as facial ex-
pressions or other body movements to detect de-
ception (Meservy et al., 2005). These methods
already show success in identifying deceptive be-
havior using individual modalities. In addition, re-
cent approaches, which combine multiple modali-
ties are able to further boost classification perfor-
mances (Abouelenien et al., 2017b; Perez-Rosas
et al., 2015).

However, multimodal approaches have not uti-
lized the dialogue dimension in combination with
other modalities as of yet. The dialogue dimension
captures the interaction between two individuals
and how they react to each other. One previous
study investigated such an interaction, in which
the researchers examined question types and their
behavioral effect on participants (Tsunomori et al.,
2015). Findings of the study showed that specific
questions led to more salient deceptive behavior
patterns in participants. This increase in feature
salience resulted in better deception detection per-
formances.

The interaction between two individuals in de-
ceptive conversations was also investigated by
Hancock et al. (2004), who examined deception
at the linguistic level. Participants, who were un-
aware of receiving a deceptive message, produced
more words, sense terms and asked more ques-
tions as compared to when they received a truthful
message. In a similar setting, in which two partici-

pants engaged in a question-response task, Levitan
et al. (2018) examined linguistic, gender and na-
tive language differences. They found significant
variations in these features for truthful and decep-
tive responses. The experimenters utilized these
variations in an automated classification task and
reached up to 72% accuracy.

These studies show that a focus on the linguis-
tic level and the interaction between individuals
can have a beneficial effect on detecting deceit.
Other studies examined non-verbal behavior. In
an experiment, Sen et al. (2018) video-recorded
conversations between participants in an interro-
gation game and examined participant’s facial ex-
pressions. The results showed that interrogators
exhibited different facial expressions when they
were lied to as opposed to when they were told
the truth. In a different approach, Yu et al. (2015)
observed head movements and facial expressions
between two individuals. The authors established
normalized non-verbal patterns, which enabled
them to capture interactional synchrony. This al-
lowed them to successfully discriminate between
truths and lies in the experiment.

Overall, this previous research demonstrates
that capturing verbal or non-verbal interactions
can convey meaningful information about deceit,
which can be leveraged for multimodal deception
detection.

3 Dataset of Deceptive Conversations

To explore the role played by conversation dynam-
ics in deceptive behaviors, we collected conversa-
tions where participants acted deceptively. Specif-
ically, we opted for identifying public sources
where the veracity or falsehood of conversation
participants is known.

In the game show Box of Lies, which is part of
the late-night talk show The Tonight Show Star-
ring Jimmy Fallon, these labels are known. The
host (Jimmy Fallon) and his guest play the game
Box of Lies, where participants take turns to play
the game. During the game, when is their turn, the
participants pick a box (from among nine avail-
able boxes) that contains an object they have to
describe to their opponent. The object is hidden
from the opponent through a separation wall be-
tween the two contestants. Participants sit oppo-
site to each other and see their upper body and face
through a cut hole in the separation wall. The op-
ponent must guess if the provided description is



1770

truthful or not. The participant with the best of
three guesses wins the game.

This setup allows us to observe verbal and non-
verbal behavior exhibited by the participants dur-
ing the dialogue interaction. In order to better
capture multimodal behavioral cues of deception
throughout the conversation, we decided to con-
duct annotations at utterance-level. We thus built
a rich multimodal dataset containing verbal and
non-verbal annotations for 1049 utterances, which
is used in the experiments reported in this paper.
The data collection and annotation process are de-
scribed below.

3.1 Data Collection

We search for publicly available Box of Lies
videos on the YouTube platform.2 We collected
25 videos that are currently available in the show
video-feed. The full set consists of 2 hours and 24
minutes of video. The average length of a video
is six minutes and contains around three rounds of
the game (this varies depending on the score and
on whether additional time was available for extra
rounds). Each video features a different guest and
Jimmy Fallon, resulting in 26 unique participants,
with 6 of them being males and 20 females.

3.2 Annotation of Multimodal
Communication Behaviors

To capture the non-verbal behavior of the partici-
pants, each video is initially segmented based on
the conversation turn-taking and annotated with
the help of the ELAN software (Wittenburg et al.,
2006). ELAN provides a multimodal annotation
platform on which audiovisual recordings are an-
notated in a multi-level tier structure. In our case,
we defined the following structure to annotate both
types of behavior: host verbal, host non-verbal,
guest verbal, and guest non-verbal.

3.2.1 Non-verbal Behaviors
To annotate facial and communication behaviors,
we use MUMIN, a multimodal coding scheme that
is used to study gestures and facial displays in
interpersonal communication with a focus on the
role played by multimodal expressions for feed-
back, turn management, and sequencing (Allwood

2The videos are originally produced by NBC and re-
trieved from Youtube. We consider that using YouTube
videos for research purposes falls under the ”fair use” clause,
which is stated on: https://www.youtube.com/
intl/en-GB/yt/about/copyright/fair-use/

et al., 2005). Given the nature of the video-
conversations being depicted in our dataset, which
show the face and upper bodies of the participants
and their interaction, we focus our annotations on
facial and conversational behavior. These choices
are motivated by previous research showing that
different expressions for truthful and deceptive be-
haviors are present (DePaulo et al., 2003) in the
eyes and mouth regions, as well as studies on the
role of conversational involvement in deceptive in-
teractions (Burgoon et al., 1999).
Facial behaviors. We annotate the categories for
visual cues and behaviors of eyebrows, eyes, gaze,
mouth-openness, mouth-lips, head, and the gen-
eral face. Each of the categories takes on one of
several mutually exclusive behavior values. Ta-
ble 1 shows the frequencies of all facial expres-
sions included in this set. In the table, we ob-
serve a slightly unequal representation of behav-
ioral categories (e.g., head movements are ob-
served more often than other facial expressions).
This is mainly attributed to camera angle changes
during the videos causing participant’s faces to be
only partly or not visible, thus restricting the be-
havioral coding. The annotated values reflect the
most dominant observed behavior in that time seg-
ment of the video.

Two annotators coded the videos, and after the
first three videos, the inter-annotator agreement
was measured by calculating the Kappa score, to
ensure accurate coding. If the agreement was be-
low Kappa (weighted) = 0.45 in any category, this
category was discussed to identify and reconcile
differences in the coding strategy. The annotators
re-coded the videos individually and compared
them again. This process was repeated until the
desired agreement was reached (above .40 for each
category). In most cases, we repeated the process
only twice, except for the “feedback receiving”
and “feedback eliciting” categories which were
discussed three times. Table 2 shows the final
Kappa score for each category.

3.2.2 Speaker’s Veracity
In the full video set, participants play 68 rounds
(29 truthful and 39 deceptive). Occasionally, de-
ceptive rounds also contain truthful statements,
in which contestants describe parts of the object
truthfully, but other parts deceptively, turning the
overall description into a lie. For example, a con-
testant might say: “I have before me, a green lob-
ster on a plate.” In truth, the object is a red lob-



1771

Label Count
General face

Smile 411
Neutral 342
Other 100
Laughter 83
Scowl 42

Eyebrows
Neutral/Normal 531
Raising 320
Frowning 76
Other 39

Mouth-Lips
Retracted 279
Neutral 267
Corners up 261
Other 102
Protruded 46
Corners down 21

Label Count
Head

Neutral/still 320
Waggle 292
Side-turn 242
Single Nod (Down) 165
Move Forward 153
Repeated Nods (Down) 122
Move Backward 117
Single Tilt (Sideways) 115
Single Jerk (Backwards Up) 78
Shake (repeated) 75
Repeated Tilts (Sideways) 18
Other 15
Single Slow Backwards Up 10
Repeated Jerks (Backwards Up) 7

Label Count
Mouth-Openness

Open mouth 763
Closed mouth 212
Other 3

Gaze
Towards interlocutor 674
Towards object 148
Down 37
Towards audience 36
Sideways 35
Other 34

Eyes
Neutral/Open 465
Closing-repeated 203
Closing-both 166
Other 121
Exaggerated Opening 8
Closing-one 4

Table 1: Frequency counts for participants’ face, head and mouth annotations.

Figure 1: Sample screenshots of truthful and deceptive behavior from the original videoclips; left-top (truth-
ful) : Eyebrows-raising, Eyes-open; left-bottom (truthful): Eyebrows-neutral, Eyes-open; right-top (deceptive):
Eyebrows-frowning, Eyes-closing (both); right-bottom (deceptive): Eyebrows-raising, Eyes-closing (both).

ster on a plate. The description contains truthful
and deceptive aspects, but it is considered to be
a deceptive round since the main purpose of the
statement is to deceive. This fine-grained distinc-
tion is captured during the annotation of behav-
iors, described below, which allows us to obtain
more precise veracity labels of the behavior. In
our example, the behavior associated with the de-
scription ”green” is labeled as deceptive, whereas
all the other behaviors are labeled as being truth-
ful.

To enable this annotation, we further process
our initial turn-by-turn segmentation to obtain
spoken segments by either of the participants. We
then code the veracity (i.e., truthful or deceptive)

for each verbal statement of the participants. Dur-
ing the veracity coding, we assume that the behav-
ior is always deceptive unless the verbal descrip-
tion indicates otherwise (i.e., accurate description
of the object), as the general goal of each partic-
ipant is to deceive their opponent. The final dis-
tribution of these annotations is 862 utterances la-
beled as deceptive, and 187 as truthful. Figure 1
shows examples of truthful and deceptive behav-
iors in the dataset.

3.3 Transcriptions

In order to include linguistic features in our analy-
ses, we first transcribe the participants’ conversa-
tions. To obtain transcriptions, we first extract the



1772

Kappa
Category Host Guest

General Face 0.75 0.70
Eyebrows 0.51 0.70
Eyes 0.56 0.92
Gaze 0.45 0.74
Mouth-Openness 0.64 0.47
Mouth-Lips 0.79 0.53
Head 0.60 0.55
Feedback receiving 0.47 0.72
Feedback eliciting 0.73 0.46
Average 0.61 0.64

Table 2: Inter-annotator agreement

Truthful Deceptive Total
Host 749 4211 4960
Guests 748 2496 3244
Total 1497 6707 8204

Table 3: Distribution of words for all transcriptions

audio of the corresponding video clip and slice it
based on the verbal annotation time-stamps. For
this task, we use Pympi (Lubbers and Torreira,
2013) and Ffmpy (Developers, 2016). We tran-
scribe the resulting audio clips using Amazon Me-
chanical Turk (AMT), a crowd-sourcing platform.
We notice that some of the clips include brief in-
terruptions among speakers, thus we ask the AMT
workers to transcribe only the speech of the main
speaker in the audio clip. After we collect all tran-
scriptions, we proofread them to avoid mistakes
such as double transcriptions and remove addi-
tional characters or descriptions (e.g. “person 1”,
clapping, [pause]). The final distribution of all the
words from the transcriptions is shown in Table
3. Example utterances of truthful and deceptive
statements are displayed in Table 4.

4 Methodology

Gathering data from different modalities creates
the need to combine them into a coherent fea-
ture set, which can be utilized by machine learn-
ing classifiers. The following subsections describe
how we generate features based on our annota-
tions for the verbal and non-verbal behavior com-
ponents of the dataset. These features are then
used to train and test the classifiers in our exper-
iments.

4.1 Linguistic Features
We derive various linguistic features from the tran-
scriptions of the participants’ speech, which in-
clude: unigrams, psycholinguistic features, part of
speech features, and word embedding features.

Unigrams. These features are created with bag-
of-words representations of all transcriptions from
the guests and the host. The unigrams are repre-
sented using their frequencies.
Psycholinguistic Features. These features are
created with the help of the Linguistic Inquiry and
Word count Lexicon (Version 2015) (Pennebaker
et al., 2007). They represent 80 different classes of
words, which can be attributed to different psycho-
logical dimensions. The features display the fre-
quencies of occurrences of classes, derived from
the occurrences of words, attributed to each class.
The lexicon has been successfully used in previous
work for automatic deception detection (Ott et al.,
2013; Mihalcea et al., 2013).
Part of Speech tags (PoS). These features are cre-
ated by obtaining PoS-tagging of transcripts. They
capture the grammatical and syntactical structure
of the utterances of the transcriptions (e.g., noun,
verb, adjective). Features display the distribution
of these categories in percentage for each utter-
ance.
Word Embeddings. These features are obtained
using Word2Vec by creating vector representa-
tions of the words in the transcriptions. By train-
ing word representations based on other words oc-
curring in the same context, these features capture
similarities of words next to each other and in con-
text. Together, all words are represented in a vec-
tor space in which similar words lay closer to each
other as compared to dissimilar words.

4.2 Non-verbal Behavioral Features

These features are generated from the non-verbal
behaviors described in Section 3.2 and represented
as percentages. Specifically, the different behav-
ioral values for a category (e.g., Head) in a verbal
utterance are counted and represented as percent-
ages. For example, a verbal utterance might last
for one minute and during that time head move-
ments might take several different values, such as
side-turn (20 sec.), shake (30 sec.), and single nod
(10 sec.). These times are transformed into per-
centages and the category head then consist of
33.33% side-turn, 50% shake, and 16.67% single
nod during the one-minute utterance. In this man-
ner, each facial area designates its percentage rep-
resentation of behavioral values, which add up to
100%. In case a behavior cannot be fully attributed
to one of the possible actions through the verbal
statement, left-over percentages are assigned to



1773

Participant Truthful Deceptive
Host ”In a, no. In a costume and also inside the box, a bunch

of Hershey kisses.”
”Ever heard of a boy band called Backstreet Boys?”

Guest ”Ok, it is, um, a rubiks cube inside jello.” ”Okey. Its a toaster oven. Ohhh ha ha ha ha”

Table 4: Examples of utterances from the transcriptions

none, representing the lack of occurrence of a be-
havioral action in its category. This transformation
is performed for all seven different facial areas we
have annotated, including General Facial Expres-
sions, Eyebrows, Eyes, Gaze, Mouth-Openness,
Mouth-Lips, and Head.

Our non-verbal behavior feature set thus con-
sists of all the facial expressions or head move-
ments expressed as the percentage of times they
occur during a speaker’s utterance. Possible at-
tributes for each of the seven categories can be
found in Table 1.

4.3 Dialogue Features

We derive dialogue-based features by exploring
verbal and non-verbal aspects of the interaction
between participants that are related to deceptive
behavior. The features attempt to capture decep-
tion cues the speaker’s exhibited during their con-
versation prior to the current utterance. These fea-
tures are obtained as follows:
Deception Changes. These features include the
count of truthful and deceptive utterances up to the
current utterance. We also aggregate the counts of
deceptive and truthful utterances to represent the
participation of the speaker during the conversa-
tion.
Non-verbal Changes. These features capture how
facial displays differ between consequent utter-
ances. We calculate these features by subtracting
the numeric vectors representing the non-verbal
behavior during the current utterance from the pre-
vious utterance.

4.4 Data Alignment

In order to attribute the corresponding non-verbal
behaviors to verbal utterances for later classifica-
tion tasks, each behavior receives a veracity label
(truthful or deceptive) individually. The veracity
label that overlaps with a behavior for more than
50% of its span, it is associated with that behavior.
The overlap is determined by comparing the time
stamp of the behavior and the veracity annotation,
which are obtained from the ELAN files. Table 5
displays the distribution of these feature sets.

Participant Lies Truths All
Guests 394 101 495
Host 468 85 553
All 862 187 1048

Table 5: Class distribution for host and guest features

5 Human Performance

In order to evaluate the automated methods and
compare them to human performance, we estab-
lish a human baseline, representing how well hu-
mans guess deceptive and truthful behavior cor-
rectly. Since the game show Box of Lies is already
set up in a way that participants have to guess if
their opponent is lying or telling the truth, their
performance serves as a baseline.

Thus, we use their assessments to obtain a con-
fusion matrix showing their correct and incorrect
guesses. We calculate their performance in terms
of accuracy, which reflects the proportion of cor-
rectly categorized descriptions of all object de-
scriptions; precision, which reflects the proportion
of correctly identified descriptions in one classi-
fied category; recall, which reflects the propor-
tion of correctly identified descriptions out of all
the object descriptions truly belonging to that cat-
egory; and f1-score, which reflects the weighted
average of precision and recall in that category.

Human performance is shown in Table 6. Since
participants tell 39 deceptive and 29 truthful de-
scriptions in total, the distribution is slightly un-
even, resulting in a baseline of 0.57 in detecting a
lie. Considering this, participants and the overall
accuracy is almost equal to the accuracy of ran-
dom guessing. This supports earlier findings that
humans are almost only as good as chance (Bond
and DePaulo, 2006).

Results for each class (detecting truthful or de-
ceptive descriptions) show that participants are
better at detecting truthful descriptions. This could
be based on the truth bias, which describes the
phenomenon according to which people generally
tend to believe others (Levine et al., 1999).



1774

Lie Truth
Acc. P R F P R F

Guests 55% 0.67 0.26 0.38 0.51 0.86 0.64
Host 58% 0.17 0.17 0.17 0.72 0.72 0.72
All 56% 0.47 0.24 0.32 0.58 0.79 0.67

Table 6: Human performances in the game ”Box of
Lies”; Acc. = Accuracy; P = Precision; R = Recall;
F = F1; Baseline (detecting a lie) = 0.57

6 Experiments

During our experiments, we use a Random Forest
classifier. We perform all the classification exper-
iments with the python package Scikit-learn (Pe-
dregosa et al., 2011) using the standard settings for
the model parameters. All classifiers are evaluated
using five-fold cross-validation. During our exper-
iments we focus on three scenarios:
(1) How well can we distinguish between truth-
ful and deceptive utterances in the dataset? In
this scenario, we explore whether the different fea-
tures we propose can capture differences between
truthful and deceptive behavior, regardless of the
speaker. Note that in this scenario, a significant
fraction of the data comes from the same speaker
(host).
(2) How well can we distinguish between truthful
and deceptive behaviors elicited by the guests? In
this experiment, we consider the subset of decep-
tive and truthful utterances produced by the guests
in our dataset. Again, we test our different feature
sets in the prediction of truthful and deceptive be-
havior, but this time we focus on learning decep-
tive patterns from several individuals, which might
exhibit different verbal and non-verbal behaviors.
(3) How well can we distinguish between truth-
ful and deceptive behaviors exhibited by the host?
In this scenario, we explore whether the availabil-
ity of data by the same individual can help to im-
prove the detection of deceptive behavior. In other
words, this experiment builds personalized decep-
tion models for the host using the different sets of
features representing verbal and non-verbal behav-
ior.

For each scenario, we test classifiers with fea-
tures derived from the different verbal and non-
verbal modalities as well as features that represent
the interaction between participants (described in
Sections 4.1, 4.2 and 4.3). We test the predictive
power of each feature set individually and we also
build joint models that combine all feature sets.
The classifiers performance is evaluated in terms

of accuracy, precision, recall, and f1-score.
An important challenge during the experiments

is that the nature of our dataset leads to a high
unbalance between the truthful and deceptive
classes, as shown in Table 5. During our ex-
periments, the imbalance of the data is tackled
by applying down-sampling to the deceptive class
(Oliphant, 2006). This ensures an equal distri-
bution of each label and results in a baseline of
0.50 in all scenarios. The results for verbal, non-
verbal, dialog features, and their combination for
each scenario are shown in Table 7.

Overall, our experiments show the benefit of
combining multiple sources of information on this
task, with accuracies well above the baseline and
a noticeable accuracy improvement when using all
feature sets.

7 Discussion

The different classification performances show
that adding information from several modalities
helps to increase the accuracy of the detection
system. Not surprisingly, the linguistic modality
shows the best performance among single modal-
ities (Scenarios 1 and 2). More interestingly, the
non-verbal modality is the second best indicator
of deception, despite a significant amount of facial
occlusions present in our dataset.3 Furthermore,
this finding is in line with other work on multi-
modal deception detection also showing that ges-
tures are a reliable indicator of deception (Perez-
Rosas et al., 2015).

In addition, we generate learning curves for
each modality in scenario 1 (figure 2). The curves
show that when training with 50 - 60% of the data,
the classifier starts to improve upon the (guessing)
baseline. The ascending trend does not seem to
level off, even with the entire dataset, indicating
that the classifier might benefit from more data.

Our experiment on exploring the classification
of deceptive behaviors from the host (scenario 3)
also lead to interesting insights. First, the lin-
guistic modality is the weakest since it obtained
the lowest performance in both classes. As the
difference in f-score values shows, the host does
not appear to use significantly different language
while telling lies or truths, at least not at the lexi-
cal and semantic level, as captured by our linguis-
tic features. Second, his non-verbal behavior does

3Facial occlusions are mainly attributed to changes in
camera angles occurring during the videos



1775

Lie Truth
Scenario Features Acc. P R F P R F

(1) General truths and lies
Linguistic 62% 0.61 0.67 0.64 0.63 0.57 0.6
Dialog 54% 0.54 0.56 0.55 0.54 0.52 0.53
Non-verbal 61% 0.64 0.54 0.58 0.6 0.69 0.64
All Features 65% 0.64 0.67 0.66 0.66 0.63 0.65

(2) Lies and truths by guests
Linguistic 66% 0.64 0.73 0.68 0.69 0.58 0.63
Dialog 57% 0.58 0.52 0.55 0.56 0.61 0.59
Non-verbal 61% 0.60 0.62 0.61 0.61 0.58 0.60
All Features 69% 0.66 0.76 0.71 0.72 0.61 0.69

(3) Lies and truths by host
Linguistic 55% 0.55 0.60 0.57 0.56 0.51 0.53
Dialog 58% 0.58 0.61 0.59 0.59 0.55 0.57
Non-verbal 57% 0.56 0.62 0.59 0.58 0.52 0.55
All Features 65% 0.67 0.61 0.64 0.64 0.69 0.67

Table 7: Automated performances for the three analyzed scenarios.The baseline for all scenarios (detecting a lie)
is 50%; Acc. = Accuracy; P = Precision; R = Recall; F = F1-score

Figure 2: Learning curves (averaged 5-fold cross vali-
dation) for each modality using random forest (general
truth/lie scenario)

seem to be different while telling lies and truths as
we observe noticeable improvement in the mod-
els build with non-verbal cues. Third, the perfor-
mance of the dialog features suggests that having
evidence of previous behavior (as captured by de-
ception changes and non-verbal behavior changes)
can be useful when modeling the deceptive behav-
ior of a single individual, further suggesting that
the non-verbal component of a lie seems to be
more individually shaped for each person as op-
posed to the linguistic component. However, the
differences in performance between scenarios 1
and 2 suggest that the current features might not
be enough to capture deceptive behavior by a sin-
gle individual since the developed classifiers still
find this task challenging.

The preliminary analyses of this new multi-
modal dataset show promising results by success-
fully classifying truthful and deceptive behaviors.
Currently, the dataset provides feature sets drawn
from three modalities (verbal and non-verbal, as

well as dialogue) but can be further analyzed to
extract additional features from other modalities
such as speech. Specifically, the dialogue has
the potential to add many more layers of infor-
mation by systematically analyzing verbal, non-
verbal, and speech patterns between the partici-
pants. These patterns can lead to detectable dif-
ferences between actions and reactions within a
dialogue (Tsunomori et al., 2015; Levitan et al.,
2016). We consider analyzing such patterns as a
future research venue to expand the analyses on
our dataset.

A challenge while using this data is the cur-
rent imbalance of truthful and deceptive feature
sets, which can have a detrimental effect on clas-
sification performance. However, there are sev-
eral other possible ways to address this issue other
than down-sampling as we did during our experi-
ments. For instance, other computational methods
could be explored, such as one-class classification
tasks. Such models train on a dataset from the
same distribution and classify new data as being
similar or different to that distribution. This way,
anomalies (i.e., behavior with a feature configura-
tion different from the training set) are detectable.
Since truthful behavior is underrepresented in our
dataset, the deceptive features could serve as the
training set and the goal is to detect truthful be-
havioral patterns. Expanding on other computa-
tional tasks also tackles future applicable prob-
lems of dealing with uneven datasets, as they are
often present when working with real-life datasets.
The issue of an underrepresented class is prevalent
in deception detection research.

Finally, the dataset could be expanded with
more behavioral data, mainly by augmenting the
number of truthful behaviors. Since all the contes-



1776

tants in our dataset are celebrities, it is likely that
other videos portraying them are available.

8 Conclusion

In this paper, we showed how we can success-
fully build a multimodal dialog dataset for decep-
tion detection, and presented exploratory decep-
tion classification tasks. We showed how we can
integrate multiple modalities and build feature sets
useful for automated processing. We were able to
achieve a classification performance that is better
than random guessing and exceeds human perfor-
mance. Furthermore, additional modalities sys-
tematically showed an improvement in classifica-
tion performance. The best performance of 69%
was obtained by combining multiple verbal, non-
verbal, and dialogue feature sets, which represents
a significant improvement over the human perfor-
mance of a maximum of 58% accuracy.

The dataset introduced in this paper represents
a first attempt to integrate the dialogue dimen-
sion with multiple other modalities in deception
detection research. It has the potential of trigger-
ing novel research on multimodal deception data,
specifically for speech and the dialogue dimen-
sion, which should be explored in the future.

All the data annotations described in this paper
are available upon request.

Acknowledgments

This material is based in part upon work sup-
ported by the Michigan Institute for Data Sci-
ence, by the National Science Foundation (grant
#1815291), and by the John Templeton Founda-
tion (grant #61156). Any opinions, findings, and
conclusions or recommendations expressed in this
material are those of the author and do not neces-
sarily reflect the views of the Michigan Institute
for Data Science, the National Science Founda-
tion, or the John Templeton Foundation.

References

Mohamed Abouelenien, Rada Mihalcea, and Mihai
Burzo. 2016. Analyzing Thermal and Visual Clues
of Deception for a Non-Contact Deception Detec-
tion Approach. pages 1–4. ACM Press.

Mohamed Abouelenien, Verónica Pérez-Rosas, Rada
Mihalcea, and Mihai Burzo. 2017a. Detecting de-
ceptive behavior via integration of discriminative

features from multiple modalities. IEEE Trans-
actions on Information Forensics and Security,
12(5):1042–1055.

Mohamed Abouelenien, Veronica Perez-Rosas, Rada
Mihalcea, and Mihai Burzo. 2017b. Detecting
Deceptive Behavior via Integration of Discrimina-
tive Features From Multiple Modalities. IEEE
Transactions on Information Forensics and Security,
12(5):1042–1055.

Jens Allwood, Loredana Cerrato, Laila Dybkjaer, Kris-
tiina Jokinen, Costanza Navarretta, and Patrizia Pag-
gio. 2005. The MUMIN multimodal coding scheme.
NorFA yearbook, 2005:129–157.

Charles F. Bond and Bella M. DePaulo. 2006. Accu-
racy of Deception Judgments. Personality and So-
cial Psychology Review, 10(3):214–234.

Judee K. Burgoon, David B. Buller, Cindy H. White,
Walid Afifi, and Aileen L. S. Buslig. 1999. The role
of conversational involvement in deceptive interper-
sonal interactions. Personality and Social Psychol-
ogy Bulletin, 25(6):669–686.

Bella M. DePaulo, James J. Lindsay, Brian E. Mal-
one, Laura Muhlenbruck, Kelly Charlton, and Har-
ris Cooper. 2003. Cues to deception. Psychological
Bulletin, 129(1):74–118.

FFmpeg Developers. 2016. Ffmpeg tool (Version
be1d324). http://ffmpeg.org/.

Tommaso Fornaciari and Massimo Poesio. 2013. Au-
tomatic deception detection in Italian court cases.
Artificial Intelligence and Law, 21(3):303–340.

Jeffrey T Hancock, Lauren E Curry, Saurabh Goorha,
and Michael T Woodworth. 2004. Lies in Conver-
sation: An Examination of Deception Using Auto-
mated Linguistic Analysis. page 6.

Timothy R. Levine, Hee Sun Park, and Steven A. Mc-
Cornack. 1999. Accuracy in detecting truths and
lies: Documenting the “veracity effect”. Commu-
nication Monographs, 66(2):125–144.

Sarah Ita Levitan, Yocheved Levitan, Guozhen An,
Michelle Levine, Rivka Levitan, Andrew Rosen-
berg, and Julia Hirschberg. 2016. Identifying indi-
vidual differences in gender, ethnicity, and person-
ality from dialogue for deception detection. In Pro-
ceedings of the Second Workshop on Computational
Approaches to Deception Detection, pages 40–44.

Sarah Ita Levitan, Angel Maredia, and Julia
Hirschberg. 2018. Linguistic Cues to Deception
and Perceived Deception in Interview Dialogues. In
Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Volume 1 (Long Papers), pages 1941–1950, New
Orleans, Louisiana. Association for Computational
Linguistics.



1777

Mart Lubbers and Francisco Torreira. 2013. Pympi-
ling: A Python module for processing ELANs EAF
and Praats TextGrid annotation files.

T.O. Meservy, M.L. Jensen, J. Kruse, D.P. Twitchell,
G. Tsechpenakis, J.K. Burgoon, D.N. Metaxas, and
J.F. Nunamaker. 2005. Deception Detection through
Automatic, Unobtrusive Analysis of Nonverbal Be-
havior. IEEE Intelligent Systems, 20(5):36–43.

Rada Mihalcea, Verónica Pérez-Rosas, and Mihai
Burzo. 2013. Automatic detection of deceit in ver-
bal communication. pages 131–134. ACM Press.

Rada Mihalcea and Carlo Strapparava. 2009. The lie
detector: Explorations in the automatic recognition
of deceptive language. In Proceedings of the ACL-
IJCNLP 2009 Conference Short Papers, pages 309–
312. Association for Computational Linguistics.

Travis E. Oliphant. 2006. A Guide to NumP. Trelgol
Publishing.

Myle Ott, Claire Cardie, and Jeffrey T. Hancock. 2013.
Negative deceptive opinion spam. In Proceedings of
the 2013 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, pages 497–501.

Myle Ott, Yejin Choi, Claire Cardie, and Jeffrey T.
Hancock. 2011. Finding deceptive opinion spam
by any stretch of the imagination. In Proceed-
ings of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language
Technologies-Volume 1, pages 309–319. Association
for Computational Linguistics.

Fabian Pedregosa, Gael Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, Jake Vanderplas, Alexan-
dre Passos, and David Cournapeau. 2011. Scikit-
learn: Machine Learning in Python. MACHINE
LEARNING IN PYTHON, 12:2825–2830.

James W Pennebaker, Cindy K Chung, Molly Ireland,
Amy Gonzales, and Roger J Booth. 2007. The De-
velopment and Psychometric Properties of LIWC.
page 22.

Veronica Perez-Rosas, Mohamed Abouelenien, Rada
Mihalcea, Y. Xiao, C.J. Linton, and Mihai Burzo.
2015. Verbal and Nonverbal Clues for Real-life De-
ception Detection. EMNLP, pages 2336–2346.

Veronica Perez-Rosas, Rada Mihalcea, Alexis Narvaez,
and Mihai Burzo. 2014. A Multimodal Dataset for
Deception Detection. LREC, pages 3118–3122.

Taylan Sen, Md Kamrul Hasan, Zach Teicher, and Mo-
hammed Ehsan Hoque. 2018. Automated Dyadic
Data Recorder (ADDR) Framework and Analysis of
Facial Cues in Deceptive Communication. Proceed-
ings of the ACM on Interactive, Mobile, Wearable
and Ubiquitous Technologies, 1(4):1–22.

Yuiko Tsunomori, Graham Neubig, Sakriani Sakti,
Tomoki Toda, and Satoshi Nakamura. 2015. An
analysis towards dialogue-based deception detec-
tion. In Natural Language Dialog Systems and In-
telligent Assistants, pages 177–187. Springer.

Peter Wittenburg, Hennie Brugman, Albert Russel,
Alex Klassmann, and Han Sloetjes. 2006. ELAN:
A Professional Framework for Multimodality Re-
search. page 4.

Xiang Yu, Shaoting Zhang, Zhennan Yan, Fei Yang,
Junzhou Huang, Norah E Dunbar, Matthew L
Jensen, Judee K Burgoon, and Dimitris N Metaxas.
2015. Is interactional dissynchrony a clue to decep-
tion? insights from automated analysis of nonver-
bal visual cues. IEEE transactions on cybernetics,
45(3):492–506.


