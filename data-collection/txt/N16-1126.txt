



















































Making Dependency Labeling Simple, Fast and Accurate


Proceedings of NAACL-HLT 2016, pages 1089–1094,
San Diego, California, June 12-17, 2016. c©2016 Association for Computational Linguistics

Making Dependency Labeling Simple, Fast and Accurate

Tianxiao Shen1 Tao Lei2 Regina Barzilay2
1The Institute for Theoretical Computer Science (ITCS)

Institute for Interdisciplinary Information Sciences
Tsinghua University

2MIT CSAIL
1shentianxiao0831@gmail.com

2{taolei, regina}@csail.mit.edu

Abstract

This work addresses the task of dependency
labeling—assigning labels to an (unlabeled)
dependency tree. We employ and extend a
feature representation learning approach, op-
timizing it for both high speed and accu-
racy. We apply our labeling model on top of
state-of-the-art parsers and evaluate its perfor-
mance on standard benchmarks including the
CoNLL-2009 and the English PTB datasets.
Our model processes over 1,700 English sen-
tences per second, which is 30 times faster
than the sparse-feature method. It improves
labeling accuracy over the outputs of top
parsers, achieving the best LAS on 5 out of
7 datasets1.

1 Introduction

Traditionally in dependency parsing, the tasks of
finding the tree structure and labeling the de-
pendency arcs are coupled in a joint achitecture.
While it has potential to eliminate errors propogated
through a separated procedure, joint decoding intro-
duces other sources of issues that can also lead to
non-optimal labeling assignments. One of the issues
arises from inexact algorithms adopted in order to
solve the hard joint search problem. For instance,
many parsers (Nivre et al., 2007; Titov and Hender-
son, 2007; Zhang et al., 2013; Dyer et al., 2015;
Weiss et al., 2015) adopt greedy decoding such as
beam search, which may prune away the correct la-
beling hypothesis in an early decoding stage. An-
other issue is caused by the absence of rich label

1Our code is available at https://github.com/
shentianxiao/RBGParser/tree/labeling.

features. Adding dependency labels to the combina-
torial space significantly slows down the search pro-
cedure. As a trade-off, many parsers such as MST-
Parser, TurboParser and RBGParser (McDonald et
al., 2005; Martins et al., 2010; Zhang et al., 2014)
incorporate only single-arc label features to reduce
the processing time. This restriction greatly limits
the labeling accuracy.

In this work, we explore an alternative approach
where the dependency labeling is applied as a sep-
arate procedure, alleviating the issues described
above. The potential of this approach has been ex-
plored in early work. For instance, McDonald et al.
(2006) applied a separate labeling step on top of the
first-order MSTParser. The benefit of such approach
is two-fold. First, finding the optimal labeling as-
signment (once the tree structure is produced) can
be solved via an exact dynamic programming algo-
rithm. Second, it becomes relatively cheap to add
rich label features given a fixed tree, and the ex-
act algorithm still applies when high-order label fea-
tures are included. However, due to performance is-
sues, such approach has not been adopted by the top
performing parsers. In this work, we show that the
labeling procedure, when optimized with recent ad-
vanced techniques in parsing, can achieve very high
speed and accuracy.

Specifically, our approach employs the recent
distributional representation learning technique for
parsing. We apply and extend the low-rank ten-
sor factorization method (Lei et al., 2014) to the
second-order case to learn a joint scoring function
over grand-head, head, modifier and their labels.
Unlike the prior work which additionally requires

1089



traditional sparse features to achieve state-of-the-art
performance, our extention alone delivers the same
level of accuracy, while being substantially faster.
As a consequence, the labeling model can be applied
either as a refinement (re-labeling) step on top of ex-
isting parsers with negligible cost of computation, or
as a part of a decoupled procedure to simplify and
speed up the dependency parsing decoding.

We evaluate on all datasets in the CoNLL-2009
shared task as well as the English Penn Treebank
dataset, applying our labeling model on top of state-
of-the-art dependency parsers. Our labeling model
processes over 1,700 English sentences per sec-
ond, which is 30 times faster than the sparse-feature
method. As a refinement (re-labeling) model, it
achieves the best LAS on 5 out of 7 datasets.

2 Method

2.1 Task Formulation
Given an unlabeled dependency parsing tree y of
sentence x, where y can be obtained using exist-
ing (non-labeling) parsers, we classify each head-
modifier dependency arc h → m ∈ y with a partic-
ular label lh→m. Let l =

⋃
h→m∈y{lh→m}, our goal

is to find the assignment with the highest score:

l∗ = arg max
l

S(x,y, l)

For simplicity, we omit x,y in the following dis-
cussion, which remain the same during the labeling
process. We assume that the score S(l) decomposes
into a sum of local scores of single arcs or pairs of
arcs (in the form of grand-head–head–modifier), i.e.

S(l) =
∑

h→m∈y
s1(h

q→ m) +
∑

g→h→m∈y
s2(g

p→ h q→ m)

where p = lg→h, q = lh→m.
Parameterizing the scoring function s1(h

q→ m)
and s2(g

p→ h q→ m) is a key challenge. We follow
Lei et al. (2014) to learn dense representations of
features, which have been shown to better generalize
the scoring function.

2.2 Scoring
The representation-based approach requires little
feature engineering. Concretely, let φg, φh, φm ∈
Rn be the atomic feature vector of the grand-
head, head and modifier word respectively, and

Unigram features:
form form-p form-n
lemma lemma-p lemma-n
POS POS-p POS-n
morph bias
Bigram features:
POS-p, POS POS, POS-n
form, POS lemma, POS
lemma, POS-p lemma, POS-n
Trigram features:
POS-p, POS, POS-n

Table 1: Word atomic features used by our model. POS, form,
lemma and morph stand for the POS tag, word form, word
lemma and morphology features respectively. The suffix -p
refers to the previous token, and -n refers to the next.

φg→h,p, φh→m,q ∈ Rd be the atomic feature vector
of the two dependency arcs respectively. It is easy
to define and compute these vectors. For instance,
φg (as well as φh and φm) can incorperate binary
features which indicate the word and POS tag of the
current token (and its local context), while φg→h,p
(and φh→m,q) can indicate the label, direction and
length of the arc between the two words.

The scores of the arcs are computed by (1)
projecting the atomic vectors into low-dimensional
spaces; and (2) summing up the element-wise prod-
ucts of the resulting dense vectors:

s1(h
q→ m) =

r1∑
i=1

[U1φh]i[V1φm]i[W1φh→m,q]i

where r1 is a hyper-parameter denoting the dimen-
sion after projection, and U1, V1 ∈ Rr1×n, W1 ∈
Rr1×d are projection matrices to be learned.

The above formulation can be shown equivalent
to factorizing a huge score table T1(·, ·, ·) into the
product of three matrices U1, V1 and W1, where T1
is a 3-way array (tensor) storing feature weights of
all possible features involving three components—
the head, modifier and the arc between the two. Ac-
cordingly, the formula to calculate s1(·) is equivalent
to summing up all feature weights (from T1) over the
structure h

q→ m.2
We depart from the prior work in the following

aspects. First, we naturally extend the factorization
approach to score second-order structures of grand-

2We refer readers to the original work (Lei et al., 2014) for
the derivation and more details.

1090



head, head and modifier,

s2(g
p→ h q→ m) =

r2∑
i=1

[U2φg]i[V2φh]i[W2φm]i

[X2φg→h,p]i[Y2φh→m,q]i

Here r2 is a hyper-parameter denoting the dimen-
sion, and U2, V2,W2 ∈ Rr2×n, X2, Y2 ∈ Rr2×d are
additional parameter matrices to be learned. Sec-
ond, in order to achieve state-of-the-art parsing ac-
curacy, prior work combines the single-arc score
s1(h

q→ m) with an extensive set of sparse features
which go beyond single-arc structures. However, we
find this combination is a huge impediment to de-
coding speed. Since our extention already captures
high-order structures, it readily delivers state-of-the-
art accuracy without the combination. This change
results in a speed-up of an order of magnitude (see
section 2.4 for a further discussion).

2.3 Viterbi Labeling

We use a dynamic programming algorithm to find
the labeling assignment with the highest score. Sup-
pose h is any node apart from the root, and g is h’s
parent. Let f(h, p) denote the highest score of sub-
tree hwith lg→h fixed to be p. Then we can compute
f(·, ·) using a bottom-up method, from leaves to the
root, by transition function

f(h, p) =
∑

h→m∈y
max

q

{
f(m, q) + s1(h

q→ m)

+ s2(g
p→ h q→ m)

}
And the highest score of the whole tree is

f(root) =
∑

root→m∈y
max

q
f(m, q) + s1(root

q→ m)

Once we get f(·, ·), we can determine the labels
backward, in a top-down manner. The time com-
plexity of our algorithm is O(NL2 · T ), where N
is the number of words in a sentence, L is the num-
ber of total labels, and T is the time of computing
features and scores.

2.4 Speed-up

In this section, we discuss two simple but effective
strategies to speed up the labeling procedure.

Pruning We prune unlikely labels by simply ex-
ploiting the part-of-speech (POS) tags of the head
and the modifier. Specifically, let 1(posh, posm, l)

denote whether there is an arc h l→ m in the train-
ing data such that h’s POS tag is posh and m’s POS
tag is posm. In the labeling process, we only con-
sider the possible labels that occur with the corre-
sponding POS tags. Let K be the average number
of possible labels per arc, then the time complexity
is dropped to O(NK2 · T ) approximately. In prac-
tice, K ≈ L/4. Hence this pruning step makes our
labeler 16 times faster.

Using Representation-based Scoring Only The
time to compute scores, i.e. T , consists of building
the features and fetching the corresponding feature
weights. For traditional methods, this requires enu-
merating feature templates, constructing feature ID
and searching the feature weight in a look-up table.
For representation-based scoring, the dense word
representations (e.g. U1φh) can be pre-computed,
and the scores are obtained by simple inner products
of small vectors. We choose to use representation-
based scoring only, therefore reducing the time to
O(NK2 · (r1 + r2) +NT ′). In practice, we find the
labeling process becomes about 30 times faster.

2.5 Learning
Let D = {(xi,yi, li)}Mi=1 be the collection of M
training samples. Our goal is to learn the values
of the set of parameters Θ = {U1, V1,W1, U2, V2,
W2, X2, Y2} based on D. Following standard prac-
tice, we optimize the parameter values in an online
maximum soft-margin framework, minimizing the
structural hinge loss:

loss(Θ) = max
l̂

{
S(̂l) + ‖li − l̂‖1

}
− S(li)

where ‖li − l̂‖1 is the number of different labels be-
tween li and l̂. We adjust parameters Θ by ∆Θ via
passive-aggressive update:

∆Θ = max
{
C,

loss(Θ)
‖δΘ‖2

}
· δΘ

where δΘ = dloss(Θ)dΘ denotes the derivatives and C
is a regularization hyper-parameter controlling the
maximum step size of each update.

To counteract over-fitting, we follow the common
practice of averaging parameters over all iterations.

1091



Model Catalan Chinese Czech English German Japanese SpanishUAS LAS UAS LAS UAS LAS UAS LAS UAS LAS UAS LAS UAS LAS
Best Shared Task - 87.86 - 79.17 - 80.38 - 89.88 - 87.48 - 92.57 - 87.64
Bohnet (2010) - 87.45 - 76.99 - 80.96 - 90.33 - 88.06 - 92.47 - 88.13
Zhang and McDonald (2014) 91.41 87.91 82.87 78.57 86.62 80.59 92.69 90.01 89.88 87.38 92.82 91.87 90.82 87.34
Alberti et al. (2015) 92.31 89.17 83.34 79.50 88.35 83.50 92.37 90.21 90.12 87.79 93.99 93.10 91.71 88.68
RBG 91.37 87.31 82.16 77.24 88.88 81.90 92.75 90.04 90.88 87.91 94.18 93.38 91.50 87.69+ our labeling 88.29 77.12 84.04 90.38 88.68 93.59 88.71

Table 2: Pipelined Results on CoNLL-2009.

3 Results

Experimental Setup We test our model on the
CoNLL-2009 shared task benchmark with 7 differ-
ent languages as well as the English Penn Treebank
dataset. Whenever available, we use the predicted
POS tags, word lemmas and morphological informa-
tion provided in the datasets as atomic features. Fol-
lowing standard practice, we use unlabeled attach-
ment scores (UAS) and labeled attachment scores
(LAS) as evaluation measure3. In order to compare
with previous reported numbers, we exclude punctu-
ations for PTB in the evaluation, and include punc-
tuations for CoNLL-2009 for consistency.

We use RBGParser4, a state-of-the-art graph-
based parser for predicting dependency trees, and
then apply our labeling model to obtain the depen-
dency label assignments. To demonstrate the effec-
tiveness of our model on other systems, we also ap-
ply it on two additional parsers – Stanford Neural
Shift-reduce Parser (Chen and Manning, 2014)5 and
TurboParser (Martins et al., 2010)6. In all reported
experiments, we use the default suggested settings
to run these parsers. The hyper-parameters of our
labeling model are set as follows: r1 = 50, r2 = 30,
C = 0.01.

Labeling Performance To test the performance of
our labeling method, we first train our model us-
ing the gold unlabeled dependency trees and eval-
uate the labeling accuray on CoNLL-2009. Table 3
presents the results. For comparison, we implement
a combined system which adds a rich set of tradi-
tional, sparse features into the scoring function and
jointly train the feature weights. As shown in the ta-
ble, using our representation-based method alone is

3We use the official evaluation script from CoNLL-X:
http://ilk.uvt.nl/conll/software.html

4https://github.com/taolei87/RBGParser
5http://nlp.stanford.edu/software/nndep.shtml
6http://www.cs.cmu.edu/˜ark/TurboParser/

Ours + Sparse Features Ours only
LAS Speed LAS Speed

Catalan 96.33 30 96.42 1070
Chinese 94.16 38 93.16 1304
Czech 95.54 71 95.60 2065
English 97.00 62 96.88 1751
German 96.93 113 96.89 1042
Japanese 98.92 305 98.95 2778
Spanish 96.53 43 96.68 1142

Table 3: LAS and parsing speed (sentence per second) based
on unlabeled golden trees.

UAS
Labeled Unlabeled

RBG 93.48 93.33

LAS

Before After
Stanford NN 89.37 89.55
Turbo 90.22 90.65
RBG 91.00 91.43

Runtime

Joint Two-step
Stanford NN 4.4 3.3
Turbo 182.1 119.4
RBG 365.4 305.7

Table 4: Joint vs. Separate analysis on PTB.

super fast, being 30 times faster than the implemen-
tation with traditional feature computation and able
to process over 1,700 English sentences per second.
It does not affect the LAS accuracy except for Chi-
nese.

PTB Results Table 4 shows the performance on
the English PTB dataset. We use RBGParser to pre-
dict both labeled and unlabeled trees, and there is
no significant difference between their UAS. This
finding lays the foundation for a separate procedure,
as the tree structure does not vary much comparing
to the joint procedure, and we can exploit rich la-
bel features and sophisticated algorithms to improve
the LAS. Our re-labeling model improves over the
predictions generated by the three different parsers,
ranging from 0.2% to 0.4% LAS gain. Moreover, the
labeling procedure runs in only 1.5 seconds on the
test set. If we use the existing parsers to only predict

1092



unlabeled trees, we also obtain speed improvement,
even for the highly speed-optimzed Stanford Neural
Parser.

CoNLL-2009 Results In Table 2, we compare our
model with the best systems7 of the CoNLL-2009
shared task, Bohnet (2010), Zhang and McDonald
(2014) as well as the most recent neural network
parser (Alberti et al., 2015). Despite the simplic-
ity of the decoupled parsing procedure, our labeling
model achieves LAS performance on par with the
state-of-the-art neural network parser. Specifically,
our model obtains the best LAS on 5 out of 7 lan-
guages, while the neural parser outperforms ours on
Catalan and Chinese.

4 Conclusion

The most common method for dependency parsing
couples the structure search and label search. We
demonstrate that decoupling these two steps yields
both computational gains and improvement in label-
ing accuracy. Specifically, we demonstrate that our
labeling model can be used as a post-processing step
to improve the accuracy of state-of-the-art parsers.
Moreover, by employing dense feature representa-
tions and a simple pruning strategy, we can signif-
icantly speed up the labeling procedure and reduce
the total decoding time of dependency parsing.

Acknowledgments

We thank Yuan Zhang for his help on the ex-
periments, and Danqi Chen for answering ques-
tions about their parser. We also thank the
MIT NLP group and the reviewers for their com-
ments. This work was supported in part by
the National Basic Research Program of China
Grant 2011CBA00300, 2011CBA00301, the Na-
tional Natural Science Foundation of China Grant
61361136003.

References

Chris Alberti, David Weiss, Greg Coppola, and Slav
Petrov. 2015. Improved transition-based parsing and
tagging with neural networks. In Proceedings of the

7Winners include Bohnet (2009), Che et al. (2009), Ges-
mundo et al. (2009) and Ren et al. (2009).

2015 Conference on Empirical Methods in Natural
Language Processing.

Bernd Bohnet. 2009. Efficient parsing of syntactic and
semantic dependency structures. In Proceedings of
the Thirteenth Conference on Computational Natural
Language Learning: Shared Task.

Bernd Bohnet. 2010. Very high accuracy and fast depen-
dency parsing is not a contradiction. In Proceedings of
the 23rd International Conference on Computational
Linguistics.

Wanxiang Che, Zhenghua Li, Yongqiang Li, Yuhang
Guo, Bing Qin, and Ting Liu. 2009. Multilingual
dependency-based syntactic and semantic parsing. In
Proceedings of the Thirteenth Conference on Compu-
tational Natural Language Learning: Shared Task.

Danqi Chen and Christopher D Manning. 2014. A
fast and accurate dependency parser using neural net-
works. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing.

Chris Dyer, Miguel Ballesteros, Wang Ling, Austin
Matthews, and Noah A Smith. 2015. Transition-based
dependency parsing with stack long short-term mem-
ory. arXiv preprint arXiv:1505.08075.

Andrea Gesmundo, James Henderson, Paola Merlo, and
Ivan Titov. 2009. A latent variable model of syn-
chronous syntactic-semantic parsing for multiple lan-
guages. In Proceedings of the Thirteenth Confer-
ence on Computational Natural Language Learning:
Shared Task.

Tao Lei, Yu Xin, Yuan Zhang, Regina Barzilay, and
Tommi Jaakkola. 2014. Low-rank tensors for scor-
ing dependency structures. In Proceedings of the 52th
Annual Meeting of the Association for Computational
Linguistics. Association for Computational Linguis-
tics.

André FT Martins, Noah A Smith, Eric P Xing, Pe-
dro MQ Aguiar, and Mário AT Figueiredo. 2010.
Turbo parsers: Dependency parsing by approximate
variational inference. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing. Association for Computational Linguis-
tics.

Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajič. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings of
the conference on Human Language Technology and
Empirical Methods in Natural Language Processing.
Association for Computational Linguistics.

Ryan McDonald, Kevin Lerman, and Fernando Pereira.
2006. Multilingual dependency analysis with a two-
stage discriminative parser. In Proceedings of the
Tenth Conference on Computational Natural Lan-
guage Learning. Association for Computational Lin-
guistics.

1093



Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,
Gülsen Eryigit, Sandra Kübler, Svetoslav Marinov,
and Erwin Marsi. 2007. MaltParser: A language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering.

Han Ren, Donghong Ji, Jing Wan, and Mingyao Zhang.
2009. Parsing syntactic and semantic dependencies
for multiple languages with a pipeline approach. In
Proceedings of the Thirteenth Conference on Compu-
tational Natural Language Learning: Shared Task.

Ivan Titov and James Henderson. 2007. A latent vari-
able model for generative dependency parsing. In Pro-
ceedings of 10th International Conference on Parsing
Technologies (IWPT).

David Weiss, Chris Alberti, Michael Collins, and Slav
Petrov. 2015. Structured training for neural network
transition-based parsing. In Proceedings of ACL 2015.

Hao Zhang and Ryan McDonald. 2014. Enforcing struc-
tural diversity in cube-pruned dependency parsing. In
Proceedings of the 52nd Annual Meeting of the Asso-
ciation for Computational Linguistics.

Hao Zhang, Liang Zhao, Kai Huang, and Ryan McDon-
ald. 2013. Online learning for inexact hypergraph
search. In Proceedings of EMNLP.

Yuan Zhang, Tao Lei, Regina Barzilay, and Tommi
Jaakkola. 2014. Greed is good if randomized: New
inference for dependency parsing. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP).

1094


