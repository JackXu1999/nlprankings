



















































Accurate Supervised and Semi-Supervised Machine Reading for Long Documents


Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2011–2020
Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics

Accurate Supervised and Semi-Supervised Machine Reading for Long
Documents

Izzeddin Gur∗
Department of Computer Science

University of California, Santa Barbara
izzeddingur@cs.ucsb.edu

Daniel Hewlett
Google Research

dhewlett@google.com

Alexandre Lacoste†
Element AI

alex.lacoste.shmu@gmail.com

Llion Jones
Google Research

llion@google.com

Abstract

We introduce a hierarchical architecture
for machine reading capable of extract-
ing precise information from long doc-
uments. The model divides the docu-
ment into small, overlapping windows and
encodes all windows in parallel with an
RNN. It then attends over these window
encodings, reducing them to a single en-
coding, which is decoded into an answer
using a sequence decoder. This hierarchi-
cal approach allows the model to scale to
longer documents without increasing the
number of sequential steps. In a super-
vised setting, our model achieves state of
the art accuracy of 76.8 on the WikiRead-
ing dataset. We also evaluate the model
in a semi-supervised setting by downsam-
pling the WikiReading training set to cre-
ate increasingly smaller amounts of su-
pervision, while leaving the full unlabeled
document corpus to train a sequence au-
toencoder on document windows. We
evaluate models that can reuse autoen-
coder states and outputs without fine-
tuning their weights, allowing for more ef-
ficient training and inference.

1 Introduction

Recently, deep neural networks (DNNs) have pro-
vided promising results for a variety of reading
comprehension and question answering tasks (We-
ston et al., 2014; Hermann et al., 2015; Rajpurkar
et al., 2016), which require extracting precise in-
formation from documents conditioned on a query.
While a basic sequence to sequence (seq2seq)
model (Sutskever et al., 2014) can perform these

∗Work completed while interning at Google Research.
†Work completed while at Google Research.

tasks by encoding a question and document se-
quence and decoding an answer sequence (Hewlett
et al., 2016), it has some disadvantages. The an-
swer may be encountered early in the text and
need to be stored across all the further recurrent
steps, leading to forgetting or corruption; Atten-
tion can be added to the decoder to solve this
problem (Hermann et al., 2015). Even with at-
tention, approaches based on Recurrent Neural
Networks (RNNs) require a number of sequential
steps proportional to the document length to en-
code each document position. Hierarchical read-
ing models address this problem by breaking the
document into sentences (Choi et al., 2017). In
this paper, we introduce a simpler hierarchical
model that achieves state-of-the-art performance
on our benchmark task without this linguistic
structure, and use it as framework to explore semi-
supervised learning for reading comprehension.

We first develop a hierarchical reader called
Sliding-Window Encoder Attentive Reader
(SWEAR) that circumvents the aforementioned
bottlenecks of existing readers. SWEAR, illus-
trated in Figure 1, first encodes each question into
a vector space representation. It then chunks each
document into overlapping, fixed-length windows
and, conditioned on the question representation,
encodes each window in parallel. Inspired by
recent attention mechanisms such as Hermann
et al. (2015), SWEAR attends over the window
representations and reduces them into a single
vector for each document. Finally, the answer is
decoded from this document vector. Our results
show that SWEAR outperforms the previous
state-of-the-art on the supervised WikiReading
task (Hewlett et al., 2016), improving Mean F1 to
76.8 from the previous 75.6 (Choi et al., 2017).

While WikiReading is a large dataset with mil-
lions of labeled examples, many applications of
machine reading have a much smaller number

2011



of labeled examples among a large set of unla-
beled documents. To model this situation, we con-
structed a semi-supervised version of WikiRead-
ing by downsampling the labeled corpus into a
variety of smaller subsets, while preserving the
full unlabeled corpus (i.e., Wikipedia). To take
advantage of the unlabeled data, we evaluated
multiple methods of reusing unsupervised recur-
rent autoencoders in semi-supervised versions of
SWEAR. Importantly, in these models we are able
to reuse all the autoencoder parameters without
fine-tuning, meaning the supervised phase only
has to learn to condition the answer on the doc-
ument and query. This allows for more efficient
training and online operation: Documents can be
encoded in a single pass offline and these en-
codings reused by all models, both during train-
ing and when answering queries. Our semi-
supervised learning models achieve significantly
better performance than supervised SWEAR on
several subsets with different characteristics. The
best-performing model reaches 66.5 with 1% of
the WikiReading dataset, compared to the 2016
state of the art of 71.8 (with 100% of the dataset).

2 Problem Description

Following the recent progress on end-to-end su-
pervised question answering (Hermann et al.,
2015; Rajpurkar et al., 2016), we consider the gen-
eral problem of predicting an answer A given a
query-document pair (Q,D). We do not make the
assumption that the answer should be present ver-
batim in the document.

2.1 Supervised Version

Given a document D = {d1, d2, · · · , dND} and
a query Q = {q1, q2, · · · , qNQ} as sequences of
words, our task is to generate a new sequence
of words that matches the correct answer A =
{a1, a2, · · · , aNA}. Because we do not assume
that A is a subsequence of D, the answer may
require blending information from multiple parts
of the document, or may be precisely copied from
a single location. Our proposed architecture sup-
ports both of these use cases.

The WikiReading dataset (Hewlett et al.,
2016), which includes a mix of categorization
and extraction tasks, is the largest dataset match-
ing this problem description. In WikiReading,
documents are Wikipedia articles, while queries
and answers are Wikidata properties and values,

respectively. Example Wikidata property-value
pairs are (place of birth, Paris),
(genre, Science Fiction). The dataset
contains 18.58M instances divided into training,
validation, and test with an 85/10/5 split. The
answer is present verbatim in the document only
47.1% of the time, severely limiting models that
label document spans, such as those developed
for the popular SQUAD dataset (Rajpurkar et al.,
2016).

2.2 Semi-Supervised Version

We also consider a semi-supervised version of the
task, where an additional corpus of documents
without labeled (Q,A) pairs is available. Tak-
ing advantage of the large size of the WikiReading
dataset, we created a series of increasingly chal-
lenging semi-supervised problems with the fol-
lowing structure:

• Unsupervised: The entire document corpus
(about 4M Wikipedia articles), with queries
and answers removed.

• Supervised: Five smaller training sets cre-
ated by sampling a random (1%, 0.5%, 0.1%)
of the WikiReading training set, and taking
(200, 100) random samples from each prop-
erty in the original training set.

3 Supervised Model Architecture

We now present our model, called Sliding-
Window Encoder Attentive Reader (SWEAR),
shown in Figure 1, and describe its operation in
a fully supervised setting. Given a (Q,D) pair,
the model encodes Q into a vector space represen-
tation with a Recurrent Neural Network (RNN).
The first layer of the model chunks the document
D into overlapping, fixed-length windows and en-
codes all windows in parallel with an RNN condi-
tioned on the question representation. The second
layer attends over the window representations, re-
ducing them into a single vector representing the
latent answer. Finally, the answer sequence A is
decoded from this vector using an RNN sequence
decoder.

3.1 Preliminaries and Notation

Each word w comes from a vocabulary V and is
associated with a vector ew which constitutes the
rows of an embedding matrixE. We denote by eD,
eQ, and eA the vector sequences corresponding to

2012



Figure 1: SWEAR model: Boxes are RNN cells, colors indicate parameter sharing.

the document, question, and answer sequences, re-
spectively. More specifically, we aim at obtaining
vector representations for documents and ques-
tions, then generating the words of the answer se-
quence.

Our model makes extensive use of RNN en-
coders to transform sequences into fixed length
vectors. For our purposes, an RNN encoder con-
sists of GRU units (Cho et al., 2014) defined as

ht = f(xt;ht−1; θ) (1)

where ht is hidden state at time t. f is a nonlinear
function operating on input vector xt and previ-
ous state, ht−1 with θ being its parameter vector.
Given an input sequence, the encoder runs over the
sequence of words producing the hidden vectors at
each step. We refer to the last hidden state of an
RNN encoder as the encoding of a sequence.

3.2 Sliding Window Recurrent Encoder

The core of the model is a sequence encoder that
operates over sliding windows in a manner analo-
gous to a traditional convolution. Before encod-
ing the document, we slide a window of length
l with a step size s over the document and pro-
duce n = bND−ls c document windows. This yields
a sequence of sub-documents (D1, D2, · · · , Dn),
where each Di contains a subsequence of l words
from the original document D. Intuitively, a pre-
cise answer may be present verbatim in one or

more windows, or many windows may contain ev-
idence suggestive of a more categorical answer.

Next, the model encodes each window condi-
tioned on a question encoding. We first encode the
question sequence once using a RNN (Enc) as

hq = Enc(eQ; θQ) (2)

where hq is the last hidden state and θQ represents
the parameters of the question encoder. Initialized
with this question encoding, we employ another
RNN to encode each document window as

hwi,0 = h
q

hwi = Enc(e
Di ; θW ) (3)

where hwi,0 is the initial hidden state, h
w
i is the last

hidden state, and θW represents the parameters of
the window encoder. θW is shared for every win-
dow and is decoupled from θQ. As the windows
are significantly smaller than the documents, en-
codings of windows will reflect the effect of ques-
tion encodings better, mitigating any long-distance
dependency problems.

3.3 Combining Window Encodings
SWEAR attends over the window encoder states
using the question encoding to produce a single
vector hd for the document, given by

pi ∝ exp(uTR tanh(WR[hwi , hq])) (4)
hd =

∑
i

pih
w
i (5)

2013



Model Mean F1
Placeholder seq2seq (HE16) 71.8

SoftAttend (CH17) 71.6
Reinforce (CH17) 74.5

Placeholder seq2seq (CH17) 75.6
SWEAR (w/ zeros) 76.4

SWEAR 76.8

Table 1: Results for SWEAR compared to top published re-
sults on the WikiReading test set.

HE16 Best SWEAR
Categorical 88.6 88.6

Relational 56.5 63.4
Date 73.8 82.5

Table 2: Mean F1 for SWEAR on each type of property com-
pared with the best results for each type reported in Hewlett
et al. (2016), which come from different models. Other pub-
lications did not report these sub-scores.

where [.] is vector concatenation, and pi is the
probability window i is relevant to answering the
question. WR and uR are parameters of the atten-
tion model.

3.4 Answer Decoding
Given the document encoding hd, an RNN de-
coder (Dec) generates the answer word sequence:

ha0 = h
d

hat = Dec(h
a
t−1;ωA) (6)

P (a∗t = wj) ∝ exp(eTj (WAhat + bA)) (7)
a∗t = argmaxj(P (a

∗
t = wj)) (8)

where ha0 is the initial hidden state and h
a
t is the

hidden vector at time t. A∗ = {a∗1, a∗2, · · · , a∗NA}
is the sequence of answer words generated. WA,
bA, and ωA are the parameters of the answer
decoder. The training objective is to minimize the
average cross-entropy error between the candidate
sequence A∗ and the correct answer sequence A.

3.5 Supervised Results
Before exploring unsupervised pre-training, we
present summary results for SWEAR in a fully su-
pervised setting, for comparison to previous work
on the WikiReading task, namely that of Hewlett
et al. (2016) and Choi et al. (2017), which we re-
fer to as HE16 and CH17 in tables. For further ex-
periments, results, and discussion see Section 5.2.
Table 1 shows that SWEAR outperforms the best

Doc length pct seq2seq SWEAR imp
[0, 200) 44.6 79.7 80.7 1.2

[200, 400) 19.5 76.7 77.8 1.5
[400, 600) 11.0 74.5 76.3 2.3
[600, 800) 6.6 72.8 74.3 2.1

[800, 1000) 4.3 71.5 72.8 1.8
[1000,max) 14.0 64.8 65.9 1.7

Table 3: Comparison of Mean F1 for SWEAR and a baseline
seq2seq model on the WikiReading test set across different
document length ranges. pct indicates the percentage of the
dataset falling in the given document length range. imp is the
percentage improvement of SWEAR over baseline.

results for various models reported in both pub-
lications, including the hierarchical models Sof-
tAttend and Reinforce presented by Choi et al.
(2017).1 Interestingly, SoftAttend computes an
attention over sentence encodings, analogous to
SWEAR’s attention over overlapping window en-
codings, but it does so on the basis of less powerful
encoders (BoW or convolution vs RNN), suggest-
ing that the extra computation spent by the RNN
provides a meaningful boost to performance.

To quantify the effect of initializing the window
encoder with the question state, we report results
for two variants of SWEAR: In SWEAR the win-
dow encoder is initialized with the question en-
coding, while in SWEAR w/ zeros, the window en-
coder is initialized with zeros. In both cases the
question encoding is used for attention over the
window encodings. For SWEAR w/ zeros it is ad-
ditionally concatenated with the document encod-
ing and passed through a 2-layer fully connected
neural network before the decoding step. Condi-
tioning on the question increases Mean F1 by 0.4.

Hewlett et al. (2016) grouped properties by an-
swer distribution: Categorical properties have a
small list of possible answers, such as countries,
Relational properties have an open set of answers,
such as spouses or places of birth, and Date prop-
erties (a subset of relational properties) have date
answers, such as date of birth. We reproduce this
grouping in Table 2 to show that SWEAR im-
proves performance for Relational and Date prop-
erties, demonstrating that it is better able to extract
precise information from documents.

Finally, we observe that SWEAR outperforms a
baseline seq2seq model on longer documents, as

1Document lengths differ between publications: We trun-
cate documents to the first 600 words, while Choi et al. trun-
cate to 1000 words or 35 sentences and Hewlett et al. truncate
to 300 words.

2014



shown in Table 3. The baseline model is roughly
equivalent to the best previously-published result,
Placeholder seq2seq (CH17) in Table 1, reach-
ing a Mean F1 of 75.5 on the WikiReading test-
set. SWEAR improves over the baseline in every
length category, but the differences are larger for
longer documents.

4 Semi-Supervised Model Architecture

We now describe semi-supervised versions of the
SWEAR model, to address the semi-supervised
problem setting described in Section 2.2. A wide
variety of approaches have been developed for
semi-supervised learning with Neural Networks,
with a typical scheme consisting of training an
unsupervised model first, and then reusing the
weights of that network as part of a supervised
model. We consider each of these problems in
turn, describing two types of unsupervised autoen-
coder models for sequences in Section 4.1 before
turning to a series of strategies for incorporating
the autoencoder weights into a final supervised
model in Section 4.3. All of these models reuse the
autoencoder weights without modification, mean-
ing a document can be encoded once by an of-
fline process, and the resulting encodings can be
used both during training and to answer multiple
queries online in a more efficient manner.

4.1 Recurrent Autoencoders for
Unsupervised Pre-training

Autoencoders are models that reconstruct their in-
put, typically by encoding it into a latent space and
then decoding it back again. Autoencoders have
recently proved useful for semi-supervised learn-
ing (Dai and Le, 2015; Fabius and van Amersfoort,
2014). We now describe two autoencoder mod-
els from the recent literature that we use for un-
supervised learning. The Recurrent Autoencoder
(RAE) is the natural application of the seq2seq
framework (Sutskever et al., 2014) to autoencod-
ing documents (Dai and Le, 2015): In seq2seq, an
encoder RNN already produces a latent represen-
tation hN , which is used to initialize a decoder
RNN. In RAE, the output sequence is replaced
with the input sequence, so learning minimizes the
cross-entropy between the reconstructed input se-
quence and the original input sequence. Encoder
and decoder cells share parameters θU .

4.1.1 Variational Recurrent Autoencoder
The Variational Recurrent Autoencoder (VRAE),
introduced by Fabius et al. (2014), is a RAE with
a variational Bayesian inference step where an un-
observed latent random variable generates the se-
quential data. The encoder and decoder are exactly
the same as RAE, but the latent state hN is not di-
rectly passed to the decoder. Instead, it is used to
estimate the parameters of a Gaussian distribution
with a diagonal covariance matrix: The mean is
given by µx = WµhN + bµ and the covariance by
Σx = WΣhN + bΣ, where Wµ, WΣ, bµ, and bΣ
are new variational step parameters. The decoder
is initialized with a single vector sampled from
this distribution, zx ∼ N (z|µx,Σx). For VRAE,
the Kullback-Leibler divergence between trained
Normal distribution and standard normal distribu-
tion, i.e., KL(N (µx,Σx)|N (0, I)), is added to
the loss.

4.1.2 Window Autoencoders
We take advantage of the SWEAR architecture
by training autoencoders for text windows, as
opposed to the standard document autoencoders.
These autoencoders operate on the same sliding
window subsequences as the supervised SWEAR
model, autoencoding all subsequences indepen-
dently and in parallel. This makes them easier
to train as they only have to compress short se-
quences of text into a fixed-length representation.
As the task of autoencoding is independent from
our supervised problem, we refer to the generated
encodings as global encodings.

4.2 Baseline: Initialization with Autoencoder
Embeddings

Our baseline approach to reusing an unsupervised
autoencoder in SWEAR is to initialize all em-
beddings with the pre-trained parameters and fix
them. We call this model SWEAR-SS (for semi-
supervised). The embedding matrix is fixed to the
autoencoder embeddings. All other parameters are
initialized randomly and trained as in the fully su-
pervised version. We found that initializing the
encoders and decoder with autoencoder weights
hurts performance.

4.3 Reviewer Models
Unfortunately, this baseline approach to semi-
supervised learning has significant disadvantages
in our problem setting. Pre-trained RNN param-
eters are not fully exploited since we observed

2015



Figure 2: Multi-layer reviewer model (SWEAR-MLR),
shown operating over a single window: Black boxes are RNN
cells with fixed weights copied from the autoencoder, dia-
monds indicate vector concatenation with adapter and FC lay-
ers. For simplicity, only the cells in dashed boxes are fully
illustrated (detailed in Figure 3), but the same structure is re-
peated for each cell.

catastrophic forgetting when initializing and fine-
tuning SWEAR with pre-trained weights. This
includes fixing window encoder parameters with
autoencoders and only fine-tuning question en-
coders. Second, conditioning the window en-
coders on the question eliminates the possibility
to train window representations offline and uti-
lize them later which causes a significant overhead
during testing.

Inspired by recent trends in deep learning mod-
els such as Progressive Neural Networks (Rusu
et al., 2016) and Reviewer Models (Yang et al.,
2016), we propose multiple solutions to these
problems. All of the proposed models process text
input first through a fixed autoencoder layer: fixed
pre-trained embeddings and fixed RNN encoder
parameters, both initialized from the autoencoder
weights. Above this autoencoder layer, we build
layers of abstraction that learn to adapt the pre-
trained models to the QA task.

4.3.1 Multi-Layer Reviewer (SWEAR-MLR)

The most straightforward extension to the baseline
model is to fix the pretrained autoencoder RNN
as the first layer and introduce a second, train-
able reviewer layer. To make this approach more
suitable for question answering, reviewer layers
utilize corresponding global encodings as well as
hidden states of the pre-trained autoencoders as
input (Figure 2). The aim is to review both pre-
trained question and window encodings to com-
pose a single vector representing the window con-
ditioned on the question.

Figure 3: Detailed illustration of the dashed box in SWEAR-
MLR question encoder. Black boxes are fixed parameters and
encodings. The window encoder is similar, except that the
output of the question reviewer layer is also added to the con-
catenated input (dashed line).

Encoding questions: The question is first
encoded by the autoencoder layer, h̃q =
Enc(eQ; θU ) where both word embeddings (E
and eQ) and encoder (θU ) are fixed and initialized
with pretrained parameters. A second, learnable
RNN layer then takes the output of the autoen-
coder layer and corresponding input embeddings
as input and produces the final question encoding,
hq = Enc(FC([eQ, h̃q, h̃t]); θQ) where FC is a
fully connected layer with ReLU activation func-
tion, and h̃t is the output of the autoencoder layer
at time step t. Figure 3 illustrates a single time-
step of the question encoder.

Encoding windows: Similarly, windows are
encoded first by the fixed autoencoder layer and
then by a reviewer layer, h̃wi = Enc(e

Di ; θU ) and
hwi = Enc(FC([e

Di , h̃wi , h̃
w
t , h

q]); θW ) where
h̃wt is the output of the autoencoder layer at time
step t. Unlike supervised SWEAR, in SWEAR-
MLR the window encoder is not initialized with
the question encoder state. Instead, the question
encoder state is an additional input to each unit in
the reviewer layer (illustrated as the dashed line
in Figure 3). Intuitively, the reviewer layer should
reuse the global window and question information
and encode only information relevant to the cur-
rent question.

4.3.2 Progressive Reviewer (SWEAR-PR)
Although the reviewer layer in SWEAR-MLR has
global window and question encodings as input, it
requires a number of sequential steps equal to the
window size, plus any additional reviewer steps.
The reviewer layer also has to re-encode windows
for each question, which is not ideal for online use.

2016



Figure 4: Progressive reviewer model (SWEAR-PR), shown
operating over a single window: Black boxes are RNN cells
with fixed weights copied from the autoencoder, diamonds
indicate vector concatenation with adapter and FC layers.
Dashed boxes contain reviewer layers. Cells within reviewer
layers are decoupled as indicated by different colors.

Figure 5: Illustration of attention cell: GRU state is used
to attend over attendable states, then final state is computed
by concatenating GRU state with context vector and passing
through a fully connected neural network.

To address these issues, we now present a Progres-
sive Reviewer model (SWEAR-PR) that reviews
the outputs of the encoders using a separate RNN
that is decoupled from the window size (Figure 4).

Encoding questions and windows: Similar
to SWEAR-MLR, SWEAR-PR first encodes the
questions and windows independently using au-
toencoder layers, h̃q = Enc(eQ; θU ) and h̃wi =
Enc(eDi ; θU ). To decouple the question and win-
dow encoders, however, SWEAR-PR does not
have a second layer as a reviewer.

Reviewing questions and windows: SWEAR-
PR employs two other RNNs to review the
question and window encodings and to compose
a single window representation conditioned on
the question. Question reviewer takes the same
pre-trained question encoding at each time step
and attends over the hidden states and input
embeddings of the pre-trained question encoder,
hq = AttnEnc(FC(h̃q);FC([h̃qt , e

Q]); θQ)
where AttnEnc is an RNN with an attention cell

which is illustrated in Figure 5. Outputs of the
fixed autoencoder layer and fixed word embed-
dings, [h̃qt , e

Q], are the attendable states. Window
reviewer on the other hand takes the pre-trained
window encoding and reviewed question encod-
ing at each time step and attends over the hidden
states of pre-trained window encoder, hwi =
AttnEnc(FC([h̃wi , h

q]);FC([h̃wt , e
Di ]); θW )

where outputs of the fixed autoencoder layer
and fixed word embeddings, [h̃wt , e

Di ], are the
attendable states. As length of the windows is
smaller than length of the reviewers, SWEAR-PR
has significantly smaller overhead compared to
other supervised and semi-supervised SWEAR
variants.

4.3.3 Shared Components
Reducing window encodings and decoding:

As in the supervised case described in 4, both re-
viewer models attend over the window encodings
using the question encoding and reduce them into
a single document encoding. Identical to answer
decoding described in 6, the answer is decoded
using another RNN taking the document state as
the initial state. The parameters of this answer de-
coder are initialized randomly.

Adapter layer: As the distribution and scale
of parameters may differ significantly between the
autoencoder layer and the reviewer layer, we use
an adapter layer similar to the adapters in Pro-
gressive Neural Networks (Rusu et al., 2016) to
normalize the pre-trained parameters:

Wout = a ∗ tanh(b ∗Win) (9)

where a and b are scalar variables to be learnt
and Win is a pre-trained input parameter. We put
adapter layers after every pre-trained parameter
connecting to a finetuned parameter such as on
the connections from pre-trained embeddings to
reviewer layer. We use dropout (Srivastava et al.,
2014) regularization on both inputs and outputs of
the reviewer cells.

5 Experimental Evaluation

As described in Section 2, we evaluate our mod-
els on the WikiReading task. In Section 3.5
we presented results for the supervised SWEAR
on the full WikiReading dataset, establishing it
as the highest-scoring method so far developed
for WikiReading. We now compare our semi-
supervised models SWEAR-MLR and SWEAR-

2017



Model 1% 0.5% 0.1%
SWEAR 63.5 57.6 39.5
SWEAR-SS (RAE) 64.7 62.8 55.3
SWEAR-SS (VRAE) 65.7 64.0 60.7

Table 4: Mean F1 results for SWEAR (fully supervised)
and SWEAR-SS (semi-supervised) trained on 1%, 0.5%, and
0.1% subsets, respectively. Variants of SWEAR-SS indicate
different sources of fixed encoder weights. 2

PR over various subsets of the WikiReading
dataset, using SWEAR as a baseline.

5.1 Experimental Setup

Following Hewlett et al. (2016), we use the Mean
F1 metric for WikiReading, which assigns partial
credit when there are multiple valid answers. We
ran hyperparameter tuning for all models and re-
port the result for the configuration with the high-
est Mean F1 on the validation set.

The supervised SWEAR model was trained on
both the full training (results reported in Sec-
tion 3.5) and on each subset of training data (re-
sults reported below). Unsupervised autoencoders
were trained on all documents in the WikiRead-
ing training set. We selected the autoencoder
with the lowest reconstruction error for use in
semi-supervised experiments. After initialization
with weights from the best autoencoder, learnable
parameters in the semi-supervised models were
trained exactly as in the supervised model.

Training Details
We implemented all models in a shared frame-

work in TensorFlow (Abadi et al., 2016). We used
the Adam optimizer (Kingma and Ba, 2014) for all
training, periodically halving the learning rate ac-
cording to a hyperparameter. Models were trained
for a maximum of 4 epochs.

Table 7 shows which hyperparameters were
tuned for each type of model, and the range of
values for each hyperparameter. The parameters
in the second group of the table are tuned for su-
pervised SWEAR and the best setting (shown in
bold) was used for other models where applicable.
We fixed the batch size to 8 for autoencoders and
64 for semi-supervised models. We used a trun-
cated normal distribution with a standard deviation
of 0.01 for VRAE.

5.2 Results and Discussion

2Initialization with Word2Vec (Mikolov et al., 2013) em-
beddings on 1% subset gives 64.0 Mean F1 score.

Model 100 200
SWEAR 25.0 33.0
SWEAR-SS (VRAE) 39.0 45.0

Table 5: Results for SWEAR and the best SWEAR-SS initial-
ization (VRAE) trained on 100- and 200- per-property sub-
sets, respectively.

Model Mean F1
SWEAR-PR 66.5

dropout on input only 65.4
no dropout 64.6
shared reviewer cells 63.8

SWEAR-MLR 63.0
w/o skip connections 60.0

Table 6: Results for semi-supervised reviewer models trained
on the 1% subset of WikiReading.

Table 4 and 5 show the results of SWEAR and
semi-supervised models with pretrained and fixed
embeddings. Results show that SWEAR-SS al-
ways improves over SWEAR at small data sizes,
with the difference become dramatic as the dataset
becomes very small. VRAE pretraining yields
the best performance. As training and test-
ing datasets have different distributions in per-
property subsets, Mean F1 for supervised and
semi-supervised models drops compared to uni-
form sampling. However, initialization with pre-
trained VRAE model leads to a substantial im-
provement on both subsamples. We further exper-
imented by initializing the decoder (vs. only the
encoder) with pretrained autoencoder weights but
this resulted in a lower Mean F1.

Table 6 shows the results of semi-supervised
reviewer models. When trained on 1% of the
training data, SWEAR-MLR and the supervised
SWEAR model perform similarly. Without us-
ing skip connections between embedding and hid-
den layers, the performance drops. The SWEAR-
PR model further improves Mean F1 and outper-
forms the strongest SWEAR-SS model, even with-
out fine-tuning the weights initialized from the au-
toencoder.

The success of SWEAR-PR rests on multiple
design elements working together, as shown by
the reduced performance caused by altering or
disabling them. Using dropout only on the in-
puts, or not using any dropout on reviewer cells,
causes a substantial decrease in Mean F1 score
(by 1.1 and 1.9, respectively). Configuring the
model with many more review steps (15) but with

2018



Parameter Space
All Models
Learning Rate [0.0001; 0.005]
Learning Rate Decay Steps {25k, 50k}
Gradient Clip [0.01; 1.0]
Supervised & Semi-Supervised
Embedding Size {128, 256, 512}
RNN State Size {256, 512}
Window Size {10, 20, 30}
Batch Size {64, 128}
Dropout {0.3, 0.5, 0.8}
Autoencoders Only
Embedding Sharing {Input, All}
KL-weight [0.0001;0.01]
Semi-Supervised Only
Finetune Pretrained Parameters {YES, NO}
Dropout {0.7, 0.8, 0.9}
Reviewer State Size {256, 512}
Question Reviewer Steps {0, 2, 3}
Window Reviewer Steps {2, 3, 5, 8}

Table 7: Hyperparameter search spaces for each model type.
We use {. . .} to denote a set of discrete values and [. . . ] to
denote a continuous range. Following Hewlett et al. (2016),
we ran a random search over the possible configurations.

a smaller hidden vector size (128) reduced Mean
F1 to 62.5. Increasing the number of review steps
for the question to 5 caused a decrease in Mean F1
of 2.1.

6 Related Work

Our model architecture is one of many hierarchi-
cal models for documents proposed in the litera-
ture. The most similar is proposed by Choi et al.
(2017), which uses a coarse-to-fine approach of
first encoding each sentence with a cheap BoW or
Conv model, then selecting the top k sentences to
form a mini-document which is then processed by
a standard seq2seq model. While they also evalu-
ate their approach on WikiReading, their emphasis
is on efficiency rather than model accuracy, with
the resulting model performing slightly worse than
the full seq2seq model but taking much less time
to execute. SWEAR also requires fewer sequential
steps than the document length but still computes
at least as many recurrent steps in parallel.

Our model can also be viewed as containing
a Memory Network (MemNet) built from a doc-
ument (Weston et al., 2014; Sukhbaatar et al.,
2015), where the memories are the window encod-
ings. The core MemNet operation consists of at-
tention over a set of vectors (memories) based on a
query encoding, and then reduction of a second set
of vectors by weighted sum based on the attention
weights. In particular, Miller et al. (2016) intro-

duce the Key-Value MemNet where the two sets
of memories are computed from the keys and val-
ues of a map, respectively: In their QA task, each
memory entry consists of a potential answer (the
value) and its context bag of words (the key).

Our reviewer approach is inspired by “Encode,
Review, Decode” approach introduced by Yang et
al. (2016), which showed the value of introducing
additional computation steps between the encoder
and decoder in a seq2seq model.

The basic recurrent autoencoder was first intro-
duced by Dai et al. (2015), a standard seq2seq
model with the same input and output. Fabius et
al. (2014) expanded this model into the Varia-
tional Recurrent Autoencoder (VRAE), which we
describe in Section 4.1.1. VRAE is an applica-
tion of the general idea of variational autoencod-
ing, which applies variational approximation to
the posterior to reconstruct the input (Kingma and
Welling, 2013). While we train window autoen-
coders, an alternative approach is hierarchical doc-
ument autoencoders (Li et al., 2015).

The semi-supervised approach of initializing
the weights of an RNN encoder with those of a
recurrent autoencoder was first studied by Dai et
al. (2015) in the context of document classifica-
tion and further studied by Ramachandran et al.
(2016) for traditional sequence-to-sequence tasks
such as machine translation. Our baseline semi-
supervised model can be viewed as an extension of
these approaches to a reading comprehension set-
ting. Dai et al. (2015) also explore initialization
from a language model, but find that the recurrent
autoencoder is superior, which is why we do not
consider language models in this work.

7 Conclusions

We have demonstrated the efficacy of the SWEAR
architecture, reaching state of the art performance
on supervised WikiReading. The model improves
the extraction of precise information from long
documents over the baseline seq2seq model. In
a semi-supervised setting, our method of reusing
(V)RAE encodings in a reading comprehension
framework is effective, with SWEAR-PR reaching
an accuracy of 66.5 on 1% of the dataset against
last year’s state of the art of 71.8 using the full
dataset. However, these methods require careful
configuration and tuning to succeed, and making
them more robust presents an excellent opportu-
nity for future work.

2019



References

Martı́n Abadi, Ashish Agarwal, Paul Barham, Eugene
Brevdo, Zhifeng Chen, Craig Citro, Gregory S. Cor-
rado, Andy Davis, Jeffrey Dean, Matthieu Devin,
Sanjay Ghemawat, Ian J. Goodfellow, Andrew Harp,
Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal
Józefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh
Levenberg, Dan Mané, Rajat Monga, Sherry Moore,
Derek Gordon Murray, Chris Olah, Mike Schuster,
Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Ku-
nal Talwar, Paul A. Tucker, Vincent Vanhoucke, Vi-
jay Vasudevan, Fernanda B. Viégas, Oriol Vinyals,
Pete Warden, Martin Wattenberg, Martin Wicke,
Yuan Yu, and Xiaoqiang Zheng. 2016. Tensorflow:
Large-scale machine learning on heterogeneous dis-
tributed systems. CoRR, abs/1603.04467.

KyungHyun Cho, Bart van Merrienboer, Dzmitry Bah-
danau, and Yoshua Bengio. 2014. On the properties
of neural machine translation: Encoder-decoder ap-
proaches. CoRR, abs/1409.1259.

Eunsol Choi, Daniel Hewlett, Alexandre Lacoste, Illia
Polosukhin, Jakob Uszkoreit, and Jonathan Berant.
2017. Coarse-to-fine question answering for long
document. In Association for Computational Lin-
guistics (ACL).

Andrew M Dai and Quoc V Le. 2015. Semi-supervised
sequence learning. In C. Cortes, N. D. Lawrence,
D. D. Lee, M. Sugiyama, and R. Garnett, editors,
Advances in Neural Information Processing Systems
28, pages 3079–3087. Curran Associates, Inc.

Otto Fabius and Joost R van Amersfoort. 2014. Varia-
tional recurrent auto-encoders. arxiv.

Karl Moritz Hermann, Tomáš Kočiský, Edward
Grefenstette, Lasse Espeholt, Will Kay, Mustafa Su-
leyman, and Phil Blunsom. 2015. Teaching ma-
chines to read and comprehend. In Advances in Neu-
ral Information Processing Systems (NIPS).

Daniel Hewlett, Alexandre Lacoste, Llion Jones, Illia
Polosukhin, Andrew Fandrianto, Jay Han, Matthew
Kelcey, and David Berthelot. 2016. Wikireading: A
novel large-scale language understanding task over
wikipedia. In Association for Computational Lin-
guistics (ACL).

Diederik P. Kingma and Jimmy Ba. 2014. Adam:
A method for stochastic optimization. CoRR,
abs/1412.6980.

Diederik P. Kingma and Max Welling. 2013. Auto-
encoding variational bayes. arxiv.

Jiwei Li, Minh-Thang Luong, and Dan Jurafsky. 2015.
A hierarchical neural autoencoder for paragraphs
and documents. In Association for Computational
Linguistics (ACL).

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in neural information processing
systems, pages 3111–3119.

Alexander Miller, Adam Fisch, Jesse Dodge, Amir-
Hossein Karimi, Antoine Bordes, and Jason We-
ston. 2016. Key-value memory networks for directly
reading documents. arxiv.

P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang. 2016.
Squad: 100,000+ questions for machine comprehen-
sion of text. In Empirical Methods in Natural Lan-
guage Processing (EMNLP).

Prajit Ramachandran, Peter J. Liu, and Quoc V. Le.
2016. Unsupervised pretraining for sequence to se-
quence learning. CoRR, abs/1611.02683.

A. A. Rusu, N. C. Rabinowitz, G. Desjardins, H. Soyer,
J. Kirkpatrick, K. Kavukcuoglu, R. Pascanu, and
R. Hadsell. 2016. Progressive Neural Networks.
arxiv.

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: A simple way to prevent neural networks
from overfitting. Journal of Machine Learning Re-
search, 15:1929–1958.

Sainbayar Sukhbaatar, arthur szlam, Jason Weston,
and Rob Fergus. 2015. End-to-end memory net-
works. In C. Cortes, N. D. Lawrence, D. D. Lee,
M. Sugiyama, and R. Garnett, editors, Advances in
Neural Information Processing Systems 28, pages
2440–2448. Curran Associates, Inc.

Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.
Sequence to sequence learning with neural net-
works. CoRR, abs/1409.3215.

Jason Weston, Sumit Chopra, and Antoine Bordes.
2014. Memory networks. CoRR, abs/1410.3916.

Zhilin Yang, Ye Yuan, Yuexin Wu, Ruslan Salakhutdi-
nov, and William W. Cohen. 2016. Encode, review,
and decode: Reviewer module for caption genera-
tion. arxiv.

2020


