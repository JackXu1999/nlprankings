



















































Word Sense Disambiguation: A Unified Evaluation Framework and Empirical Comparison


Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 99–110,
Valencia, Spain, April 3-7, 2017. c©2017 Association for Computational Linguistics

Word Sense Disambiguation:
A Unified Evaluation Framework and Empirical Comparison

Alessandro Raganato, Jose Camacho-Collados and Roberto Navigli
Department of Computer Science

Sapienza University of Rome
{raganato,collados,navigli}@di.uniroma1.it

Abstract

Word Sense Disambiguation is a long-
standing task in Natural Language Pro-
cessing, lying at the core of human lan-
guage understanding. However, the evalu-
ation of automatic systems has been prob-
lematic, mainly due to the lack of a re-
liable evaluation framework. In this pa-
per we develop a unified evaluation frame-
work and analyze the performance of
various Word Sense Disambiguation sys-
tems in a fair setup. The results show
that supervised systems clearly outper-
form knowledge-based models. Among
the supervised systems, a linear classi-
fier trained on conventional local features
still proves to be a hard baseline to beat.
Nonetheless, recent approaches exploit-
ing neural networks on unlabeled corpora
achieve promising results, surpassing this
hard baseline in most test sets.

1 Introduction

Word Sense Disambiguation (WSD) has been a
long-standing task in Natural Language Process-
ing (NLP). It lies at the core of language under-
standing and has already been studied from many
different angles (Navigli, 2009; Navigli, 2012).
However, the field seems to be slowing down
due to the lack of groundbreaking improvements
and the difficulty of integrating current WSD sys-
tems into downstream NLP applications (de La-
calle and Agirre, 2015). In general the field does
not have a clear path, partially owing to the fact
that identifying real improvements over existing
approaches becomes a hard task with current eval-
uation benchmarks. This is mainly due to the
lack of a unified framework, which prevents di-
rect and fair comparison among systems. Even

though many evaluation datasets have been con-
structed for the task (Edmonds and Cotton, 2001;
Snyder and Palmer, 2004; Navigli et al., 2007;
Pradhan et al., 2007; Agirre et al., 2010a; Nav-
igli et al., 2013; Moro and Navigli, 2015, in-
ter alia), they tend to differ in format, construc-
tion guidelines and underlying sense inventory. In
the case of the datasets annotated using WordNet
(Miller, 1995), the de facto sense inventory for
WSD, we encounter the additional barrier of hav-
ing text annotated with different versions. These
divergences are in the main solved individually by
using or constructing automatic mappings. The
quality check of such mapping, however, tends to
be impractical and this leads to mapping errors
which give rise to additional system inconsisten-
cies in the experimental setting. This issue is di-
rectly extensible to the training corpora used by
supervised systems. In fact, results obtained by
supervised or semi-supervised systems reported in
the literature are not completely reliable, because
the systems may not necessarily have been trained
on the same corpus, or the corpus was prepro-
cessed differently, or annotated with a sense inven-
tory different from the test data. Thus, together,
the foregoing issues prevent us from drawing reli-
able conclusions on different models, as in some
cases ostensible improvements may have been ob-
tained as a consequence of the nature of the train-
ing corpus, the preprocessing pipeline or the ver-
sion of the underlying sense inventory, rather than
of the model itself. Moreover, because of these
divergences, current systems tend to report results
on a few datasets only, making it hard to perform
a direct quantitative confrontation.

This paper offers two main contributions. First,
we provide a complete evaluation framework for
all-words Word Sense Disambiguation overcom-
ing all the aforementioned limitations by (1) stan-
dardizing the WSD datasets and training corpora

99



into a unified format, (2) semi-automatically con-
verting annotations from any dataset to WordNet
3.0, and (3) preprocessing the datasets by consis-
tently using the same pipeline. Second, we use
this evaluation framework to perform a fair quanti-
tative and qualitative empirical comparison of the
main techniques proposed in the WSD literature,
including the latest advances based on neural net-
works.

2 State of the Art

The task of Word Sense Disambiguation consists
of associating words in context with the most suit-
able entry in a pre-defined sense inventory. De-
pending on their nature, WSD systems are divided
into two main groups: supervised and knowledge-
based. In what follows we summarize the current
state of these two types of approach.

2.1 Supervised WSD

Supervised models train different features ex-
tracted from manually sense-annotated corpora.
These features have been mostly based on the in-
formation provided by the surroundings words of
the target word (Keok and Ng, 2002; Navigli,
2009) and its collocations. Recently, more com-
plex features based on word embeddings trained
on unlabeled corpora have also been explored
(Taghipour and Ng, 2015b; Rothe and Schütze,
2015; Iacobacci et al., 2016). These features are
generally taken as input to train a linear classifier
(Zhong and Ng, 2010; Shen et al., 2013). In ad-
dition to these conventional approaches, the latest
developments in neural language models have mo-
tivated some researchers to include them in their
WSD architectures (Kågebäck and Salomonsson,
2016; Melamud et al., 2016; Yuan et al., 2016).

Supervised models have traditionally been able
to outperform knowledge-based systems (Navigli,
2009). However, obtaining sense-annotated cor-
pora is highly expensive, and in many cases such
corpora are not available for specific domains.
This is the reason why some of these supervised
methods have started to rely on unlabeled corpora
as well. These approaches, which are often clas-
sified as semi-supervised, are targeted at overcom-
ing the knowledge acquisition bottleneck of con-
ventional supervised models (Pilehvar and Nav-
igli, 2014). In fact, there is a line of research
specifically aimed at automatically obtaining large
amounts of high-quality sense-annotated corpora

(Taghipour and Ng, 2015a; Raganato et al., 2016;
Camacho-Collados et al., 2016a).

In this work we compare supervised systems
and study the role of their underlying sense-
annotated training corpus. Since semi-supervised
models have been shown to outperform fully
supervised systems in some settings (Taghipour
and Ng, 2015b; Başkaya and Jurgens, 2016;
Iacobacci et al., 2016; Yuan et al., 2016),
we evaluate and compare models using both
manually-curated and automatically-constructed
sense-annotated corpora for training.

2.2 Knowledge-based WSD

In contrast to supervised systems, knowledge-
based WSD techniques do not require any sense-
annotated corpus. Instead, these approaches rely
on the structure or content of manually-curated
knowledge resources for disambiguation. One of
the first approaches of this kind was Lesk (1986),
which in its original version consisted of calcu-
lating the overlap between the context of the tar-
get word and its definitions as given by the sense
inventory. Based on the same principle, vari-
ous works have adapted the original algorithm by
also taking into account definitions from related
words (Banerjee and Pedersen, 2003), or by cal-
culating the distributional similarity between def-
initions and the context of the target word (Basile
et al., 2014; Chen et al., 2014). Distributional sim-
ilarity has also been exploited in different settings
in various works (Miller et al., 2012; Camacho-
Collados et al., 2015; Camacho-Collados et al.,
2016b). In addition to these approaches based on
distributional similarity, an important branch of
knowledge-based systems found their techniques
on the structural properties of semantic graphs
from lexical resources (Agirre and Soroa, 2009;
Guo and Diab, 2010; Ponzetto and Navigli, 2010;
Agirre et al., 2014; Moro et al., 2014; Weissenborn
et al., 2015; Tripodi and Pelillo, 2016). Gener-
ally, these graph-based WSD systems first create
a graph representation of the input text and then
exploit different graph-based algorithms over the
given representation (e.g., PageRank) to perform
WSD.

3 Standardization of WSD datasets

In this section we explain our pipeline for trans-
forming any given evaluation dataset or sense-
annotated corpus into a preprocessed unified for-

100



Figure 1: Pipeline for standardizing any given WSD dataset.

mat. In our pipeline we do not make any dis-
tinction between evaluation datasets and sense-
annotated training corpora, as the pipeline can be
applied equally to both types. For simplicity we
will refer to both evaluation datasets and training
corpora as WSD datasets.

Figure 1 summarizes our pipeline to standardize
a WSD dataset. The process consists of four steps:

1. Most WSD datasets in the literature use a
similar XML format, but they have some di-
vergences on how to encode the information.
For instance, the SemEval-15 dataset (Moro
and Navigli, 2015) was developed for both
WSD and Entity Linking and its format was
especially designed for this latter task. There-
fore, we decided to convert all datasets to a
unified format. As unified format we use the
XML scheme used for the SemEval-13 all-
words WSD task (Navigli et al., 2013), where
preprocessing information of a given corpus
is also encoded.

2. Once the dataset is converted to a unified for-
mat, we map the sense annotations from its
original WordNet version to 3.0, which is the
latest version of WordNet used in evaluation
datasets. This mapping is carried out semi-
automatically. First, we use automatically-
constructed WordNet mappings1 (Daude et
al., 2003). These mappings provide confi-
dence values which we use to initially map
senses whose mapping confidence is 100%.
Then, the annotations of the remaining senses
are manually checked, and re-annotated or re-
moved whenever necessary2. Additionally,
in this step we decided to remove all annota-
tions of auxiliary verbs, following the anno-
tation guidelines of the latest WSD datasets.

3. The third step consists of preprocessing
the given dataset. We used the Stanford

1http://nlp.lsi.upc.edu/tools/
download-map.php

2This manual correction involved less than 10% of all in-
stances for the datasets for which this step was performed.

CoreNLP toolkit (Manning et al., 2014) for
Part-of-Speech (PoS) tagging3 and lemmati-
zation. This step is performed in order to
ensure that all systems use the same prepro-
cessed data.

4. Finally, we developed a script to check that
the final dataset conforms to the aforemen-
tioned guidelines. In this final verification we
also ensured that the sense annotations match
the lemma and the PoS tag provided by Stan-
ford CoreNLP by automatically fixing all di-
vergences.

4 Data

In this section we summarize the WSD datasets
used in the evaluation framework. To all these
datasets we apply the standardization pipeline de-
scribed in Section 3. First, we enumerate all the
datasets used for the evaluation (Section 4.1). Sec-
ond, we describe the sense-annotated corpora used
for training (Section 4.2). Finally, we show some
relevant statistics extracted from these resources
(Section 4.3).

4.1 WSD evaluation datasets

For our evaluation framework we considered five
standard all-words fine-grained WSD datasets
from the Senseval and SemEval competitions:

• Senseval-2 (Edmonds and Cotton, 2001).
This dataset was originally annotated with
WordNet 1.7. After standardization, it con-
sists of 2282 sense annotations, including
nouns, verbs, adverbs and adjectives.

• Senseval-3 task 1 (Snyder and Palmer,
2004). The WordNet version of this dataset
was 1.7.1. It consists of three documents
from three different domains (editorial, news
story and fiction), totaling 1850 sense anno-
tations.

3In order to have a standard format which may be used by
languages other than English, we provide coarse-grained PoS
tags as given by the universal PoS tagset (Petrov et al., 2011).

101



#Docs #Sents #Tokens #Annotations #Sense types #Word types Ambiguity
Senseval-2 3 242 5,766 2,282 1,335 1,093 5.4
Senseval-3 3 352 5,541 1,850 1,167 977 6.8
SemEval-07 3 135 3,201 455 375 330 8.5
SemEval-13 13 306 8,391 1,644 827 751 4.9
SemEval-15 4 138 2,604 1,022 659 512 5.5
SemCor 352 37,176 802,443 226,036 33,362 22,436 6.8
OMSTI - 813,798 30,441,386 911,134 3,730 1,149 8.9

Table 1: Statistics of the WSD datasets used in the evaluation framework (after standardization).

• SemEval-07 task 17 (Pradhan et al., 2007).
This is the smallest among the five datasets,
containing 455 sense annotations for nouns
and verbs only. It was originally annotated
using WordNet 2.1 sense inventory.

• SemEval-13 task 12 (Navigli et al., 2013).
This dataset includes thirteen documents
from various domains. In this case the origi-
nal sense inventory was WordNet 3.0, which
is the same as the one that we use for all
datasets. The number of sense annotations is
1644, although only nouns are considered.

• SemEval-15 task 13 (Moro and Navigli,
2015). This is the most recent WSD dataset
available to date, annotated with WordNet
3.0. It consists of 1022 sense annotations
in four documents coming from three het-
erogeneous domains: biomedical, mathemat-
ics/computing and social issues.

4.2 Sense-annotated training corpora

We now describe the two WordNet sense-
annotated corpora used for training the supervised
systems in our evaluation framework:

• SemCor (Miller et al., 1994). SemCor4 is
a manually sense-annotated corpus divided
into 352 documents for a total of 226,040
sense annotations. It was originally tagged
with senses from the WordNet 1.4 sense
inventory. SemCor is, to our knowledge,
the largest corpus manually annotated with
WordNet senses, and is the main corpus used
in the literature to train supervised WSD sys-
tems (Agirre et al., 2010b; Zhong and Ng,
2010).

4We downloaded the SemCor 3.0 version at web.eecs.
umich.edu/˜mihalcea/downloads.html

• OMSTI (Taghipour and Ng, 2015a). OM-
STI (One Million Sense-Tagged Instances) is
a large corpus annotated with senses from
the WordNet 3.0 inventory. It was auto-
matically constructed by using an alignment-
based WSD approach (Chan and Ng, 2005)
on a large English-Chinese parallel corpus
(Eisele and Chen, 2010, MultiUN corpus).
OMSTI5 has already shown its potential as
a training corpus by improving the perfor-
mance of supervised systems which add it
to existing training data (Taghipour and Ng,
2015a; Iacobacci et al., 2016).

4.3 Statistics

Table 1 shows some statistics6 of the WSD
datasets and training corpora which we use in the
evaluation framework. The number of sense an-
notations varies across datasets, ranging from 455
annotations in the SemEval-07 dataset, to 2,282
annotations in the Senseval-2 dataset. As regards
sense-annotated corpora, OMSTI is made up of
almost 1M sense annotations, a considerable in-
crease over the number of sense annotations of
SemCor. However, SemCor is much more bal-
anced in terms of unique senses covered (3,730
covered by OMSTI in contrast to over 33K cov-
ered by SemCor). Additionally, while OMSTI
was constructed automatically, SemCor was man-
ually built and, hence, its quality is expected to be
higher.

Finally, we calculated the ambiguity level of
each dataset, computed as the total number of can-

5In this paper we refer to the portion of sense-annotated
data from the MultiUN corpus as OMSTI. Note that OMSTI
was released along with SemCor.

6Statistics included in Table 1: number of documents
(#Docs), sentences (#Sents), tokens (#Tokens), sense anno-
tations (#Annotations), sense types covered (#Sense types),
annotated lemma types covered (#Word types), and ambigu-
ity level (Ambiguity). There was no document information in
the OMSTI data released by Taghipour and Ng (2015a).

102



didate senses (i.e., senses sharing the surface form
of the target word) divided by the number of sense
annotations. The highest ambiguity is found on
OMSTI, which, despite being constructed auto-
matically, contains a high coverage of ambigu-
ous words. As far as the evaluation competition
datasets are concerned, the ambiguity may give a
hint as to how difficult a given dataset may be. In
this case, SemEval-07 displays the highest ambi-
guity level among all evaluation datasets.

5 Evaluation

The evaluation framework consists of the WSD
evaluation datasets described in Section 4.1. In
this section we use this framework to perform an
empirical comparison among a set of heteroge-
neous WSD systems. The systems used in the
evaluation are described in detail in Section 5.1,
the results are shown in Section 5.2 and a detailed
analysis is presented in Section 5.3.

5.1 Comparison systems

We include three supervised (Section 5.1.1) and
three knowledge-based (Section 5.1.2) all-words
WSD systems in our empirical comparison.

5.1.1 Supervised
To ensure a fair comparison, all supervised sys-
tems use the same corpus for training: SemCor
and Semcor+OMSTI7 (see Section 4.2). In the
following we describe the three supervised WSD
systems used in the evaluation:

• IMS (Zhong and Ng, 2010) uses a Support
Vector Machine (SVM) classifier over a set
of conventional WSD features. IMS8 is built
on a flexible framework which allows an easy
integration of different features. The default
implementation includes surrounding words,
PoS tags of surroundings words, and local
collocations as features.

• IMS+embeddings (Taghipour and Ng,
2015b; Rothe and Schütze, 2015; Iacobacci
et al., 2016). These approaches have shown
the potential of using word embeddings on
the WSD task. Iacobacci et al. (2016) carried

7As already noted by Taghipour and Ng (2015a), super-
vised systems trained on only OMSTI obtain lower results
than when trained along with SemCor, mainly due to OM-
STI’s lack of coverage in target word types.

8We used the original implementation available at http:
//www.comp.nus.edu.sg/˜nlp/software.html

out a comparison of different strategies for
integrating word embeddings as a feature in
WSD. In this paper we consider the two best
configurations in Iacobacci et al. (2016)9:
using all IMS default features including and
excluding surrounding words (IMS+emb
and IMS-s+emb, respectively). In both
cases word embeddings are integrated using
exponential decay (i.e., word weights drop
exponentially as the distance towards the
target word increases). Likewise, we use
Iacobacci et al.’s suggested learning strategy
and hyperparameters to train the word em-
beddings: Skip-gram model of Word2Vec10

(Mikolov et al., 2013) with 400 dimensions,
ten negative samples and a window size of
ten words. As unlabeled corpus to train the
word embeddings we use the English ukWaC
corpus11 (Baroni et al., 2009), which is made
up of two billion words from paragraphs
extracted from the web.

• Context2Vec (Melamud et al., 2016). Neural
language models have recently shown their
potential for the WSD task (Kågebäck and
Salomonsson, 2016; Yuan et al., 2016). In
this experiment we replicated the approach
of Melamud et al. (2016, Context2Vec), for
which the code12 is publicly available. This
approach is divided in three steps. First, a
bidirectional LSTM recurrent neural network
is trained on an unlabeled corpus (we con-
sidered the same ukWaC corpus used by the
previous comparison system). Then, a con-
text vector is learned for each sense annota-
tion in the training corpus. Finally, the sense
annotation whose context vector is closer to
the target word’s context vector is selected as
the intended sense.

Finally, as baseline we included the Most Fre-
quent Sense (MFS) heuristic, which for each tar-
get word selects the sense occurring the highest
number of times in the training corpus.

9We used the implementation available at https://
github.com/iiacobac/ims_wsd_emb

10code.google.com/archive/p/word2vec/
11http://wacky.sslmit.unibo.it/doku.

php?id=corpora
12https://github.com/orenmel/

context2vec

103



5.1.2 Knowledge-based
In this section we describe the three knowledge-
based WSD models used in our empirical compar-
ison:

• Lesk (Lesk, 1986) is a simple knowledge-
based WSD algorithm that bases its calcu-
lations on the overlap between the defini-
tions of a given sense and the context of the
target word. For our experiments we repli-
cated the extended version of the original al-
gorithm in which definitions of related senses
are also considered and the conventional
term frequency-inverse document frequency
(Jones, 1972, tf-idf ) is used for word weight-
ing (Banerjee and Pedersen, 2003, Leskext).
Additionally, we included the enhanced ver-
sion of Lesk in which word embeddings13 are
leveraged to compute the similarity between
definitions and the target context (Basile et
al., 2014, Leskext+emb)14.

• UKB (Agirre and Soroa, 2009; Agirre et al.,
2014) is a graph-based WSD system which
makes use of random walks over a seman-
tic network (WordNet graph in this case).
UKB15 applies the Personalized Page Rank
algorithm (Haveliwala, 2002) initialized us-
ing the context of the target word. Unlike
most WSD systems, UKB does not back-off
to the WordNet first sense heuristic and it
is self-contained (i.e., it does not make use
of any external resources/corpora). We used
both default configurations from UKB: us-
ing the full WordNet graph (UKB) and the
full graph including disambiguated glosses as
connections as well (UKB gloss).

• Babelfy (Moro et al., 2014) is a graph-based
disambiguation approach which exploits ran-
dom walks to determine connections between
synsets. Specifically, Babelfy16 uses ran-
dom walks with restart (Tong et al., 2006)
over BabelNet (Navigli and Ponzetto, 2012),
a large semantic network integrating Word-
Net among other resources such as Wikipedia

13We used the same word embeddings described in Section
5.1.1 for IMS+emb.

14We used the implementation from https://github.
com/pippokill/lesk-wsd-dsm. In this implementa-
tion additional definitions from BabelNet are considered.

15We used the last implementation available at
http://ixa2.si.ehu.es/ukb/

16We used the Java API from http://babelfy.org

or Wiktionary. Its algorithm is based on a
densest subgraph heuristic for selecting high-
coherence semantic interpretations of the in-
put text. The best configuration of Babelfy
takes into account not only the target sen-
tence in which the target word occurs, but
also the whole document.

As knowledge-based baseline we included the
WordNet first sense. This baseline simply selects
the candidate which is considered as first sense
in WordNet 3.0. Even though the sense order
was decided on the basis of semantically-tagged
text, we considered it as knowledge-based in this
experiment as this information is already avail-
able in WordNet. In fact, knowledge-based sys-
tems like Babelfy include this information in their
pipeline. Despite its simplicity, this baseline has
been shown to be hard to beat by automatic WSD
systems (Navigli, 2009; Agirre et al., 2014).

5.2 Results

Table 2 shows the F-Measure performance of all
comparison systems on the five all-words WSD
datasets. Since not all test word instances are
covered by the corresponding training corpora,
supervised systems have a maximum F-Score
(ceiling in the Table) they can achieve. Never-
theless, supervised systems consistently outper-
form knowledge-based systems across datasets,
confirming the results of Pilehvar and Navigli
(2014). A simple linear classifier over conven-
tional WSD features (i.e., IMS) proves to be ro-
bust across datasets, consistently outperforming
the MFS baseline. The recent integration of word
embeddings as an additional feature is beneficial,
especially as a replacement of the feature based
on the surface form of surrounding words (i.e.,
IMS-s+emb). Moreover, recent advances on neu-
ral language models (in the case of Context2Vec a
bi-directional LSTM) appear to be highly promis-
ing for the WSD task according to the results, as
Context2Vec outperforms IMS in most datasets.

On the other hand, it is also interesting to note
the performance inconsistencies of systems across
datasets, as in all cases there is a large performance
gap between the best and the worst performing
dataset. As explained in Section 4.3, the ambi-
guity level may give a hint as to how difficult the
corresponding dataset may be. In fact, WSD sys-
tems obtain relatively low results in SemEval-07,
which is the most ambiguous dataset (see Table 1).

104



Tr. Corpus System Senseval-2 Senseval-3 SemEval-07 SemEval-13 SemEval-15

Supervised

SemCor

IMS 70.9 69.3 61.3 65.3 69.5
IMS+emb 71.0 69.3 60.9 67.3 71.3

IMS-s+emb 72.2 70.4 62.6 65.9 71.5
Context2Vec 71.8 69.1 61.3 65.6 71.9

MFS 65.6 66.0 54.5 63.8 67.1
Ceiling 91.0 94.5 93.8 88.6 90.4

SemCor +
OMSTI

IMS 72.8 69.2 60.0 65.0 69.3
IMS+emb 70.8 68.9 58.5 66.3 69.7

IMS-s+emb 73.3 69.6 61.1 66.7 70.4
Context2Vec 72.3 68.2 61.5 67.2 71.7

MFS 66.5 60.4 52.3 62.6 64.2
Ceiling 91.5 94.9 94.7 89.6 91.1

Knowledge -

Leskext 50.6 44.5 32.0 53.6 51.0
Leskext+emb 63.0 63.7 56.7 66.2 64.6

UKB 56.0 51.7 39.0 53.6 55.2
UKB gloss 60.6 54.1 42.0 59.0 61.2

Babelfy 67.0 63.5 51.6 66.4 70.3
WN 1st sense 66.8 66.2 55.2 63.0 67.8

Table 2: F-Measure percentage of different models in five all-words WSD datasets.

Nouns Verbs Adj. Adv. All
#Instances 4,300 1,652 955 346 7,253
Ambiguity 4.8 10.4 3.8 3.1 5.8

Table 3: Number of instances and ambiguity level
of the concatenation of all five WSD datasets.

However, this is the dataset in which supervised
systems achieve a larger margin with respect to
the MFS baseline, which suggests that, in general,
the MFS heuristic does not perform accurately on
highly ambiguous words.

5.3 Analysis

To complement the results from the previous sec-
tion, we additionally carried out a detailed analysis
about the global performance of each system and
divided by PoS tag. To this end, we concatenated
all five datasets into a single dataset. This resulted
in a large evaluation dataset of 7,253 instances to
disambiguate (see Table 3). Table 4 shows the F-
Measure performance of all comparison systems
on the concatenation of all five WSD evaluation
datasets, divided by PoS tag. IMS-s+emb trained
on SemCor+OMSTI achieves the best overall re-
sults, slightly above Context2Vec trained on the
same corpus. In what follows we describe some of
the main findings extracted from our analysis.

Training corpus. In general, the results of
supervised systems trained on SemCor only
(manually-annotated) are lower than training

simultaneously on both SemCor and OMSTI
(automatically-annotated). This is a promising
finding, which confirms the results of previous
works (Raganato et al., 2016; Iacobacci et al.,
2016; Yuan et al., 2016) and encourages further
research on developing reliable automatic or semi-
automatic methods to obtain large amounts of
sense-annotated corpora in order to overcome the
knowledge-acquisition bottleneck. For instance,
Context2Vec improves 0.4 points overall when
adding the automatically sense-annotated OMSTI
as part of the training corpus, suggesting that more
data, even if not perfectly clean, may be beneficial
for neural language models.

Knowledge-based vs. Supervised. One of the
main conclusions that can be taken from the evalu-
ation is that supervised systems clearly outperform
knowledge-based models. This may be due to the
fact that in many cases the main disambiguation
clue is given by the immediate local context. This
is particularly problematic for knowledge-based
systems, as they take equally into account all the
words within a sentence (or document in the case
of Babelfy). For instance, in the following sen-
tence, both UKB and Babelfy fail to predict the
correct sense of state:

In sum, at both the federal and state government
levels at least part of the seemingly irrational
behavior voters display in the voting booth may
have an exceedingly rational explanation.

105



Tr. Corpus System Nouns Verbs Adjectives Adverbs All

Supervised

SemCor

IMS 70.4 56.1 75.6 82.9 68.4
IMS+emb 71.8 55.4 76.1 82.7 69.1

IMS-s+emb 71.9 56.9 75.9 84.7 69.6
Context2Vec 71.0 57.6 75.2 82.7 69.0

MFS 67.6 49.6 73.1 80.5 64.8
Ceiling 89.6 95.1 91.5 96.4 91.5

SemCor +
OMSTI

IMS 70.5 56.9 76.8 82.9 68.8
IMS+emb 71.0 53.3 77.1 82.7 68.3

IMS-s+emb 72.0 56.5 76.6 84.7 69.7
Context2Vec 71.7 55.8 77.2 82.7 69.4

MFS 65.8 45.9 72.7 80.5 62.9
Ceiling 90.4 95.8 91.8 96.4 92.1

Knowledge -

Leskext 54.1 27.9 54.6 60.3 48.7
Leskext+emb 69.8 51.2 51.7 80.6 63.7

UKB 56.7 39.3 63.9 44.0 53.2
UKB gloss 62.1 38.3 66.8 66.2 57.5

Babelfy 68.6 49.9 73.2 79.8 65.5
WN 1st sense 67.6 50.3 74.3 80.9 65.2

Table 4: F-Measure percentage of different models on the concatenation of all five WSD datasets.

In this sentence, state is annotated with its ad-
ministrative districts of a nation sense in the gold
standard. The main disambiguation clue seems
to be given by its previous and immediate subse-
quent words (federal and government), which tend
to co-occur with this particular sense. However,
knowledge-based WSD systems like UKB or Ba-
belfy give the same weight to all words in con-
text, underrating the importance of this local dis-
ambiguation clue in the example. For instance,
UKB disambiguates state with the sense defined
as the way something is with respect to its main at-
tributes, probably biased by words which are not
immediately next to the target word within the sen-
tence, e.g., irrational, behaviour, rational or ex-
planation.

Low overall performance on verbs. As can be
seen from Table 4, the F-Measure performance of
all systems on verbs is in all cases below 58%.
This can be explained by the high granularity of
verbs in WordNet. For instance, the verb keep con-
sists of 22 different meanings in WordNet 3.0, six
of them denoting “possession and transfer of pos-
session”17. In fact, the average ambiguity level of
all verbs in this evaluation framework is 10.4 (see

17https://wordnet.princeton.edu/man/
lexnames.5WN.html

Table 3), considerably greater than the ambiguity
on other PoS tags, e.g., 4.8 in nouns. Nonetheless,
supervised systems manage to comfortably out-
perform the MFS baseline, which does not seem
to be reliable for verbs given their high ambiguity.

Influence of preprocessing. As mentioned in
Section 3, our evaluation framework provides
a preprocessing of the corpora with Stanford
CoreNLP. This ensures a fair comparison among
all systems but may introduce some annotation in-
accuracies, such as erroneous PoS tags. However,
for English these errors are minimal18. For in-
stance, the global error rate of the Stanford PoS
tagger in all disambiguation instances is 3.9%,
which were fixed as explained in Section 3.

Bias towards the Most Frequent Sense. After
carrying out an analysis on the influence of MFS in
WSD systems19, we found that all supervised sys-
tems suffer a strong bias towards the MFS, with all
IMS-based systems disambiguating over 75% of
instances with their MFS. Context2Vec is slightly
less affected by this bias, with 71.5% (SemCor)
and 74.7% (SemCor+OMSTI) of answers corre-

18Even if preprocessing plays a minimal role for English,
it may be of higher importance for other languages, e.g., mor-
phologically richer languages (Eger et al., 2016).

19See Postma et al. (2016) for an interesting discussion on
the bias of current WSD systems towards the MFS.

106



sponding to the MFS. Interestingly, this MFS bias
is also present in graph knowledge-based systems.
In fact, Calvo and Gelbukh (2015) had already
shown how the MFS correlates strongly with the
number of connections in WordNet.

Knowledge-based systems. For knowledge-
based systems the WN first sense baseline proves
still to be extremely hard to beat. The only
knowledge-based system that overall manages
to beat this baseline is Babelfy, which, in fact,
uses information about the first sense in its
pipeline. Babelfy’s default pipeline includes a
confidence threshold in order to decide whether
to disambiguate or back-off to the first sense. In
total, Babelfy backs-off to WN first sense in 63%
of all instances. Nonetheless, it is interesting
to note the high performance of Babelfy and
Leskext+emb on noun instances (outperforming
the first sense baseline by 1.0 and 2.2 points,
respectively) in contrast to their relatively lower
performance on verbs, adjectives20 and adverbs.
We believe that this is due to the nature of the
lexical resource used by these two systems, i.e.,
BabelNet. BabelNet includes Wikipedia as one of
its main sources of information. However, while
Wikipedia provides a large amount of semantic
connections and definitions for nouns, this it not
the case for verbs, adjectives and adverbs, as they
are not included in Wikipedia and their source of
information mostly comes from WordNet only.

6 Conclusion and Future Work

In this paper we presented a unified evaluation
framework for all-words WSD. This framework is
based on evaluation datasets taken from Senseval
and SemEval competitions, as well as manually
and automatically sense-annotated corpora. In this
evaluation framework all datasets share a com-
mon format, sense inventory (i.e., WordNet 3.0)
and preprocessing pipeline, which eases the task
of researchers to evaluate their models and, more
importantly, ensures a fair comparison among all
systems. The whole evaluation framework21, in-
cluding guidelines for researchers to include their
own sense-annotated datasets and a script to vali-
date their conformity to the guidelines, is available
at http://lcl.uniroma1.it/wsdeval .

20The poor performance of Leskext+emb on adjective in-
stances is particularly noticeable.

21We have additionally set up a CodaLab competition
based on this evaluation framework.

We used this framework to perform an empirical
comparison among a set of heterogeneous WSD
systems, including both knowledge-based and su-
pervised ones. Supervised systems based on neu-
ral networks achieve the most promising results.
Given our analysis, we foresee two potential re-
search avenues focused on semi-supervised learn-
ing: (1) exploiting large amounts of unlabeled
corpora for learning word embeddings or train-
ing neural language models, and (2) automatically
constructing high-quality sense-annotated corpora
to be used by supervised WSD systems. As far as
knowledge-based systems are concerned, enrich-
ing knowledge resources with semantic connec-
tions for non-nominal mentions may be an impor-
tant step towards improving their performance.

For future work we plan to further extend
our unified framework to languages other than
English, including SemEval multilingual WSD
datasets, as well as to other sense inventories
such as Open Multilingual WordNet, BabelNet
and Wikipedia, which are available in different
languages.

Acknowledgments

The authors gratefully acknowledge
the support of the ERC Starting
Grant MultiJEDI No. 259234.

Jose Camacho-Collados is supported by a
Google PhD Fellowship in Natural Language Pro-
cessing.

References
Eneko Agirre and Aitor Soroa. 2009. Personalizing

PageRank for Word Sense Disambiguation. In Pro-
ceedings of EACL, pages 33–41.

Eneko Agirre, Oier Lopez De Lacalle, Christiane Fell-
baum, Andrea Marchetti, Antonio Toral, and Piek
Vossen. 2010a. Semeval-2010 task 17: All-words
word sense disambiguation on a specific domain. In
Proceedings of the Workshop on Semantic Evalua-
tions: Recent Achievements and Future Directions,
pages 123–128.

Eneko Agirre, Oier Lopez De Lacalle, Christiane Fell-
baum, Andrea Marchetti, Antonio Toral, and Piek
Vossen. 2010b. Semeval-2010 task 17: All-words
word sense disambiguation on a specific domain. In
Proceedings of the Workshop on Semantic Evalua-
tions: Recent Achievements and Future Directions,
pages 123–128.

Eneko Agirre, Oier Lopez de Lacalle, and Aitor Soroa.
2014. Random walks for knowledge-based word

107



sense disambiguation. Computational Linguistics,
40(1):57–84.

Satanjeev Banerjee and Ted Pedersen. 2003. Extended
gloss overlap as a measure of semantic relatedness.
In Proceedings of the 18th International Joint Con-
ference on Artificial Intelligence, pages 805–810,
Acapulco, Mexico.

Marco Baroni, Silvia Bernardini, Adriano Ferraresi,
and Eros Zanchetta. 2009. The wacky wide
web: a collection of very large linguistically pro-
cessed web-crawled corpora. Language resources
and evaluation, 43(3):209–226.

Pierpaolo Basile, Annalina Caputo, and Giovanni Se-
meraro. 2014. An Enhanced Lesk Word Sense Dis-
ambiguation Algorithm through a Distributional Se-
mantic Model. In Proceedings of COLING 2014,
the 25th International Conference on Computational
Linguistics: Technical Papers, pages 1591–1600,
Dublin, Ireland.

Osman Başkaya and David Jurgens. 2016. Semi-
supervised learning with induced word senses for
state of the art word sense disambiguation. Journal
of Artificial Intelligence Research, 55:1025–1058.

Hiram Calvo and Alexander Gelbukh. 2015. Is the
most frequent sense of a word better connected in a
semantic network? In International Conference on
Intelligent Computing, pages 491–499. Springer.

José Camacho-Collados, Mohammad Taher Pilehvar,
and Roberto Navigli. 2015. A Unified Multilingual
Semantic Representation of Concepts. In Proceed-
ings of ACL, pages 741–751.

José Camacho-Collados, Claudio Delli Bovi, Alessan-
dro Raganato, and Roberto Navigli. 2016a.
A Large-Scale Multilingual Disambiguation of
Glosses. In Proceedings of LREC, pages 1701–
1708, Portoroz, Slovenia.

José Camacho-Collados, Mohammad Taher Pilehvar,
and Roberto Navigli. 2016b. Nasari: Integrating
explicit knowledge and corpus statistics for a multi-
lingual representation of concepts and entities. Arti-
ficial Intelligence, 240:36–64.

Yee Seng Chan and Hwee Tou Ng. 2005. Scaling
up word sense disambiguation via parallel texts. In
AAAI, volume 5, pages 1037–1042.

Xinxiong Chen, Zhiyuan Liu, and Maosong Sun. 2014.
A unified model for word sense representation and
disambiguation. In Proceedings of EMNLP, pages
1025–1035, Doha, Qatar.

Jordi Daude, Lluis Padro, and German Rigau. 2003.
Validation and tuning of wordnet mapping tech-
niques. In Proceedings of RANLP.

Oier Lopez de Lacalle and Eneko Agirre. 2015. A
methodology for word sense disambiguation at 90%
based on large-scale crowdsourcing. Lexical and
Computational Semantics (* SEM 2015), page 61.

Philip Edmonds and Scott Cotton. 2001. Senseval-2:
Overview. In Proceedings of The Second Interna-
tional Workshop on Evaluating Word Sense Disam-
biguation Systems, pages 1–6, Toulouse, France.

Steffen Eger, Rüdiger Gleim, and Alexander Mehler.
2016. Lemmatization and morphological tagging in
german and latin: A comparison and a survey of the
state-of-the-art. In Proceedings of LREC 2016.

Andreas Eisele and Yu Chen. 2010. MultiUN: A Mul-
tilingual Corpus from United Nation Documents. In
Proceedings of the Seventh conference on Interna-
tional Language Resources and Evaluation, pages
2868–2872.

Weiwei Guo and Mona T. Diab. 2010. Combining
orthogonal monolingual and multilingual sources of
evidence for all words WSD. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 1542–1551, Upp-
sala, Sweden.

Taher H. Haveliwala. 2002. Topic-sensitive PageRank.
In Proceedings of the 11th International Conference
on World Wide Web, pages 517–526, Hawaii, USA.

Ignacio Iacobacci, Mohammad Taher Pilehvar, and
Roberto Navigli. 2016. Embeddings for word sense
disambiguation: An evaluation study. In Proceed-
ings of ACL, pages 897–907, Berlin, Germany.

Karen Spärck Jones. 1972. A statistical interpretation
of term specificity and its application in retrieval.
Journal of Documentation, 28:11–21.

Mikael Kågebäck and Hans Salomonsson. 2016. Word
sense disambiguation using a bidirectional lstm.
arXiv preprint arXiv:1606.03568.

L. Y. Keok and H. T. Ng. 2002. An empirical evalu-
ation of knowledge sources and learning algorithms
for word sense disambiguation. In Proceedings of
the 7th Conference on Empirical Methods in Nat-
ural Language Processing, pages 41–48, Philadel-
phia, USA.

Michael Lesk. 1986. Automatic sense disambiguation
using machine readable dictionaries: How to tell a
pine cone from an ice cream cone. In Proceedings
of the 5th Annual Conference on Systems Documen-
tation, Toronto, Ontario, Canada, pages 24–26.

Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David Mc-
Closky. 2014. The Stanford CoreNLP natural lan-
guage processing toolkit. In Association for Compu-
tational Linguistics (ACL) System Demonstrations,
pages 55–60.

Oren Melamud, Jacob Goldberger, and Ido Dagan.
2016. context2vec: Learning generic context em-
bedding with bidirectional lstm. In Proceedings of
CONLL.

108



Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. CoRR, abs/1301.3781.

George A Miller, Martin Chodorow, Shari Landes,
Claudia Leacock, and Robert G Thomas. 1994.
Using a semantic concordance for sense identifica-
tion. In Proceedings of the workshop on Human
Language Technology, pages 240–243. Association
for Computational Linguistics.

Tristan Miller, Chris Biemann, Torsten Zesch, and
Iryna Gurevych. 2012. Using distributional similar-
ity for lexical expansion in knowledge-based word
sense disambiguation. In COLING, pages 1781–
1796.

George A Miller. 1995. Wordnet: a lexical
database for english. Communications of the ACM,
38(11):39–41.

Andrea Moro and Roberto Navigli. 2015. Semeval-
2015 task 13: Multilingual all-words sense dis-
ambiguation and entity linking. Proceedings of
SemEval-2015.

Andrea Moro, Alessandro Raganato, and Roberto Nav-
igli. 2014. Entity Linking meets Word Sense Dis-
ambiguation: a Unified Approach. Transactions
of the Association for Computational Linguistics
(TACL), 2:231–244.

Roberto Navigli and Simone Paolo Ponzetto. 2012.
BabelNet: The automatic construction, evaluation
and application of a wide-coverage multilingual se-
mantic network. Artificial Intelligence, 193:217–
250.

Roberto Navigli, Kenneth C. Litkowski, and Orin Har-
graves. 2007. SemEval-2007 task 07: Coarse-
grained English all-words task. In Proceedings
of the Fourth International Workshop on Semantic
Evaluations (SemEval-2007), Prague, Czech Repub-
lic, pages 30–35.

Roberto Navigli, David Jurgens, and Daniele Vannella.
2013. SemEval-2013 Task 12: Multilingual Word
Sense Disambiguation. In Proceedings of SemEval
2013, pages 222–231.

Roberto Navigli. 2009. Word Sense Disambiguation:
A survey. ACM Computing Surveys, 41(2):1–69.

Roberto Navigli. 2012. A quick tour of word sense dis-
ambiguation, induction and related approaches. In
SOFSEM 2012: Theory and practice of computer
science, pages 115–129. Springer.

Slav Petrov, Dipanjan Das, and Ryan McDonald. 2011.
A universal part-of-speech tagset. arXiv preprint
arXiv:1104.2086.

Mohammad Taher Pilehvar and Roberto Navigli. 2014.
A large-scale pseudoword-based evaluation frame-
work for state-of-the-art Word Sense Disambigua-
tion. Computational Linguistics, 40(4).

Simone Paolo Ponzetto and Roberto Navigli. 2010.
Knowledge-rich Word Sense Disambiguation rival-
ing supervised system. In Proceedings of the 48th
Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 1522–1531, Upp-
sala, Sweden.

Marten Postma, Ruben Izquierdo, Eneko Agirre, Ger-
man Rigau, and Piek Vossen. 2016. Addressing
the MFS Bias in WSD systems. In Proceedings of
LREC, Portoroz, Slovenia.

Sameer Pradhan, Edward Loper, Dmitriy Dligach, and
Martha Palmer. 2007. SemEval-2007 task-17: En-
glish lexical sample, SRL and all words. In Pro-
ceedings of SemEval, pages 87–92.

Alessandro Raganato, Claudio Delli Bovi, and Roberto
Navigli. 2016. Automatic Construction and Evalua-
tion of a Large Semantically Enriched Wikipedia. In
Proceedings of IJCAI, pages 2894–2900, New York
City, NY, USA, July.

Sascha Rothe and Hinrich Schütze. 2015. Autoex-
tend: Extending word embeddings to embeddings
for synsets and lexemes. In Proceedings of ACL,
pages 1793–1803, Beijing, China.

Hui Shen, Razvan Bunescu, and Rada Mihalcea. 2013.
Coarse to fine grained sense disambiguation in
wikipedia. Proc. of *SEM, pages 22–31.

Benjamin Snyder and Martha Palmer. 2004. The En-
glish all-words task. In Proceedings of the 3rd In-
ternational Workshop on the Evaluation of Systems
for the Semantic Analysis of Text (SENSEVAL-3),
Barcelona, Spain, pages 41–43, Barcelona, Spain.

Kaveh Taghipour and Hwee Tou Ng. 2015a. One mil-
lion sense-tagged instances for word sense disam-
biguation and induction. CoNLL 2015, page 338.

Kaveh Taghipour and Hwee Tou Ng. 2015b. Semi-
supervised word sense disambiguation using word
embeddings in general and specific domains. Pro-
ceedings of NAACL HLT 2015, pages 314–323.

Hanghang Tong, Christos Faloutsos, and Jia-Yu Pan.
2006. Fast random walk with restart and its applica-
tions. In ICDM, pages 613–622.

Rocco Tripodi and Marcello Pelillo. 2016. A game-
theoretic approach to word sense disambiguation.
arXiv preprint arXiv:1606.07711.

Dirk Weissenborn, Leonhard Hennig, Feiyu Xu, and
Hans Uszkoreit. 2015. Multi-Objective Optimiza-
tion for the Joint Disambiguation of Nouns and
Named Entities. In Proceedings of ACL, pages 596–
605, Beijing, China.

Dayu Yuan, Julian Richardson, Ryan Doherty, Colin
Evans, and Eric Altendorf. 2016. Semi-supervised
word sense disambiguation with neural models. In
Proceedings of COLING, pages 1374–1385.

109



Zhi Zhong and Hwee Tou Ng. 2010. It Makes Sense:
A wide-coverage Word Sense Disambiguation sys-
tem for free text. In Proceedings of the ACL System
Demonstrations, pages 78–83.

110


