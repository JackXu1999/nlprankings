



















































Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics


Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1971–1982
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1180

Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1971–1982
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1180

Estimating Code-Switching on Twitter with a Novel Generalized
Word-Level Language Detection Technique

Shruti Rijhwani∗
Language Technologies Institute

Carnegie Mellon University
srijhwan@cs.cmu.edu

Royal Sequiera∗
University of Waterloo

Waterloo, Canada
rdsequie@uwaterloo.ca

Monojit Choudhury Kalika Bali Chandra Sekhar Maddila
Microsoft Research

Bangalore, India
{monojitc,kalikab,chmaddil}@microsoft.com

Abstract

Word-level language detection is neces-
sary for analyzing code-switched text,
where multiple languages could be mixed
within a sentence. Existing models are
restricted to code-switching between two
specific languages and fail in real-world
scenarios as text input rarely has a priori
information on the languages used. We
present a novel unsupervised word-level
language detection technique for code-
switched text for an arbitrarily large num-
ber of languages, which does not require
any manually annotated training data. Our
experiments with tweets in seven lan-
guages show a 74% relative error reduc-
tion in word-level labeling with respect to
competitive baselines. We then use this
system to conduct a large-scale quanti-
tative analysis of code-switching patterns
on Twitter, both global as well as region-
specific, with 58M tweets.

1 Introduction

In stable multilingual societies, communication
often features fluid alteration between two or
more languages – a phenomenon known as
code-switching1 (Gumperz, 1982; Myers-Scotton,
1993). It has been studied extensively in linguis-
tics, primarily as a speech phenomenon (Poplack,
1980; Gumperz, 1982; Myers-Scotton, 1993; Mil-
roy and Muysken, 1995; Auer, 2013). How-
ever, the growing popularity of computer mediated

∗* This work was done when the authors were affiliated
with Microsoft Research.

1This paper uses the terms ‘code-switching’ and ‘code-
mixing’ interchangeably.

communication, particularly social media, has re-
sulted in language data in the text form which
exhibits code-switching, among other speech-
like characteristics (Crystal, 2001; Herring, 2003;
Danet and Herring, 2007; Cardenas-Claros and
Isharyanti, 2009). With the large amount of online
content generated by multilingual users around the
globe, it becomes necessary to design techniques
to analyze mixed language, which can help not
only in developing end-user applications, but also
in conducting fundamental sociolinguistic studies.

Language detection (LD) is a prerequisite to
several NLP techniques. Most state-of-the-art LD
systems detect a single language for an entire doc-
ument or sentence. Such methods often fail to
detect code-switching, which can occur within a
sentence. In recent times, there has been some
effort to build word-level LD for code-switching
between a specific pair of languages (Nguyen and
Dogruöz, 2013; Elfardy et al., 2013; Solorio et al.,
2014; Barman et al., 2014). However, usually
user-generated text (e.g., on social media) has no
prior information of the languages being used.
Further, as several previous social-media based
studies on multilingualism have pointed out (Kim
et al., 2014; Manley, 2012), lack of general word-
level LD has been a bottleneck in studying code-
switching patterns in multilingual societies.

This paper proposes a novel technique for word-
level LD that generalizes to an arbitrarily large set
of languages. The method does not require a pri-
ori information on the specific languages (poten-
tially more than two) being mixed in an input text
as long as the languages are from a fixed (arbitrar-
ily large) set. Training is done without any man-
ually annotated data, while achieving accuracies
comparable to language-restricted systems trained

1971

https://doi.org/10.18653/v1/P17-1180
https://doi.org/10.18653/v1/P17-1180


with large amounts of labeled data. With a word-
level LD accuracy of 96.3% on seven languages,
this technique enabled us to analyze patterns of
code-switching on Twitter, which is the second
key contribution of this paper. To the best of our
knowledge, this is the first quantitative study of its
kind, particularly at such a large-scale.

2 Related Work

In this section, we will briefly survey the language
detection techniques (see Hughes et al. (2006) and
Garg et al. (2014) for comprehensive surveys),
and sociolinguistic studies on multilingualism (see
Nguyen et al. (2016) for a detailed survey) that
were enabled by these techniques.

Early work on LD (Cavnar and Trenkle, 1994;
Dunning, 1994) focused on detecting a single
language for an entire document. These ob-
tained high accuracies on well-formed text (e.g.,
news articles), which led to LD being consid-
ered solved (McNamee, 2005). However, there
has been renewed interest with the amount of
user-generated content on the web. Such text
poses unique challenges such as short length,
misspelling, idiomatic expressions and acronyms
(Carter et al., 2013; Goldszmidt et al., 2013). Xia
et al. (2009), Tromp and Pechenizkiy (2011) and
Lui and Baldwin (2012) created LD systems for
monolingual sentences, web pages and tweets.
Zhang et al. (2016) built an unsupervised model to
detect the majority language in a document. There
has also been document-level LD that assigns mul-
tiple language to each document (Prager, 1999;
Lui et al., 2014). However, documents were syn-
thetically generated, restricted to inter-sentential
language mixing. Also, these models do not frag-
ment the document based on language, making
language-specific analysis impossible.

Document-level or sentence-level LD does not
identify code-switching accurately, which can oc-
cur within a sentence. Word-level LD systems
attempt to remedy this problem. Most work has
been restricted to cases where two languages,
known a priori, is to be detected in the input i.e,
binary LD at the word-level. There has been
work on Dutch-Turkish (Nguyen and Dogruöz,
2013), English-Bengali (Das and Gambäck, 2014)
and Standard and dialectal Arabic (Elfardy et al.,
2013). King and Abney (2013) address word-
level LD for bilingual documents in 30 language
pairs, where the language pair is known a pri-

ori. The features for word-level LD proposed
by Al-Badrashiny and Diab (2016) are language-
independent, however, at any given time, the
model is only trained to tag a specific language
pair. There have also been two shared task se-
ries on word-level LD: FIRE (Roy et al., 2013;
Choudhury et al., 2014; Sequiera et al., 2015) fo-
cused on Indian languages and the EMNLP Code-
Switching Workshop (Solorio et al., 2014; Molina
et al., 2016). These pairwise LD methods vary
from dictionary-based to completely supervised
and semi-supervised. None tackle the imminent
lack of annotated data required for scaling to more
than one language pair.

There has been little research on word-level LD
that is not restricted to two languages. Ham-
marström (2007) proposed a model for multi-
lingual LD for short texts like queries. Gella
et al. (2014) designed an algorithm for word-
level LD across 28 languages. Jurgens et al.
(2017) use an encoder-decoder architecture for
word-level LD that supports dialectal variation
and code-switching. However, these studies ex-
periment with synthetically created multilingual
data, constrained either by the number of lan-
guage switches permitted or to phrase-level code-
switching, and are not equipped to handle the chal-
lenges posed by real-world code-switching.

Using tweet-level LD systems like the Com-
pactLanguageDetector2, there have been studies
on multilingualism in specific cities like Lon-
don (Manley, 2012) and Manchester (Bailey et al.,
2013). These studies, as well as Bergsma
et al. (2012), observe that existing LD systems fail
on code-switched text. Kim et al. (2014) studied
the linguistic behavior of bilingual Twitter users
from Qatar, Switzerland and Québec, and also ac-
knowledge that code-switching could not be stud-
ied due to the absence of appropriate LD tools.

Using word-level LD for English-Hindi (Gella
et al., 2013), Bali et al. 2014 observed that as
much as 17% of Indian Facebook posts had code-
switching, and Rudra et al. (2016) showed that the
native language is strongly preferred for express-
ing negative sentiment by English-Hindi bilin-
guals on Twitter. However, without accurate mul-
tilingual word-level LD, there have been no large-
scale studies on the extent and distribution of
code-switching across various communities.

2https://www.npmjs.com/package/cld

1972



3 Generalized Word-level LD

We present Generalized Word-Level Language
Detection, or GWLD, where:

• The number of supported languages can be
arbitrarily large

• Any number of the supported languages can
be mixed within a single input

• The languages in the input do not need to be
known a priori

• Any number of language switches are al-
lowed in the input.

• No manual annotation is required for training

Formalizing our model, let w = wi=1...n be a
natural language text consisting of a sequence of
words, w1 to wn. For our current work, we define
words to be whitespace-separated tokens (details
in Sec 5). Let L = {l1, l2, . . . , lk} be a set of k
natural languages. We assume that each wi can be
assigned to a unique language lj ∈ L.

We also define universal tokens like numbers,
emoticons, URLs, emails and punctuation, which
do not belong to any specific natural language.
Certain strings of alphabetic characters represent-
ing generic interjections or sounds, such as oh,
awww, zzz also fall in this category. For label-
ing these tokens, we use an auxiliary set of labels,
XL = {xl1, xl2, . . . , xlk}. Labeling each univer-
sal token with a specific language li (using xli) in-
stead of generically labeling all such tokens xl al-
lows preserving linguistic context when a memo-
ryless model like Hidden Markov Models (HMM)
are used for tagging. Further, various NLP tasks
on might require the input text, including these
universal tokens, to be split by language.

For input w, let the output from the LD system
be y = yi=1...n, a sequence of labels, where yi ∈
L ∪ XL. yi = lj if and only if, in the context of
w, wi is a word from lj . If wi is a universal token,
yi = xlj , when yi−1 = lj or yi−1 = xlj . If w1 is a
universal token, y1 = xlj , where lj is the label of
the first token ∈ L in the input.

Fig. 1 shows a few examples of labeled code-
switched tweets. Named entities (NE) are as-
signed labels according to the convention used by
King and Abney (2013).

4 Method

Word-level LD is essentially a sequence labeling
task. We use a Hidden Markov Model (HMM),

though any other sequence labeling technique,
e.g., CRFs, can be used as well.

The intuition behind the model architecture is
simple – a person who is familiar with k lan-
guages can easily recognize (and also understand)
the words when any of those languages are code-
switched, even if s/he has never seen any mixed
language text before. Analogously, is it possi-
ble that monolingual language models, when com-
bined, can identify code-switched text accurately?

Imagine we have k HMMs, where the ith HMM
has two states li and xli. Each state can label a
word. The HMMs are independent, but they are
tied to a common start state s and end state e,
forming a word-level LD model for monolingual
text in one of the k languages. Now, we make
transitions from li → lj possible, where i 6= j.
This HMM, shown in Fig. 2, is capable of gen-
erating and consequently, labeling code-switched
text between any of the k languages. The solid
and dotted lines show monolingual transitions and
the added code-switching transitions respectively.
Fig. 2 depicts three languages, however, the num-
ber of languages can be arbitrarily large.

Obtaining word-level annotated monolingual
and code-switched data is expensive and nearly in-
feasible for a large number of languages. Instead,
we automatically create weakly-labeled monolin-
gual text (setW) and use it to initialize the HMM
parameters. We then use Baum-Welch reestima-
tion on unlabeled data (set U) that has monolin-
gual and code-switched text in their natural distri-
bution. Sec. 5 discusses creation ofW and U .

4.1 Structure, Initialization and Learning

The structure of the HMM shown in Fig. 2 can be
formally described using:

• Set of states, S = s ∪ L ∪ XL ∪ e
• Set of observations, O
• Emission matrix (|S| × |O|)
• Transition matrix (|S| × |S|)

O consists of all seen events in the data, and
a special symbol unk for all unseen events. We
define an event as a token n-gram and we experi-
mented with n = 1 to 3. It is important to men-
tion that the n-grams do not spread over language
states. We also use special start and end symbols,
which are observed at states s and e respectively.
Elements of O are effectively what the states of
the HMM ‘emit’ or generate during decoding.

1973



Ex(1): no\l2 me\l2 lebante\l2 ahorita\l2 cuz\l1 I\l1 felt\l1 como\l2 si\l2 me\l2
kemara\l2 por\l2 dentro\l2 !\xl2 :o\xl2 Then\l1 I\l1 started\l1 getting\l1
all\l1 red\l1 ,\xl1 I\l1 think\l1 im\l1 allergic\l1 a\l2 algo\l2

Ex(2): @XXXXX\xl3 @XXXXX\xl3 :)\xl3 :)\xl3 :)\xl3 :)\xl3 hahahahah\xl3 alles\l3
is\l3 3D\xl3 voor\l3 mama\l4 hatta\l4 4D\xl4 :P\xl4 :P\xl4 :P\xl4 :P\xl4
Havva\l4 &\xl4 Yusuf\l4 olunca\l4 misafir\l4 fln\l4 dinlemez\l4 !!\xl4

Figure 1: Examples of code-switched tweets and the corresponding language labels. l1 = English, l2 =
Spanish, l3 = Dutch, l4 = Turkish. Usernames have been anonymized.

Figure 2: GWLD Hidden Markov Model. s→ xli
and li → e transitions omitted for clarity.

For any input, the HMM always starts in the
state s. The parameters to be learned are the tran-
sition and emission matrices.

We initialize these matrices using W . The tri-
gram, bigram and unigram word counts from the
data for each language in W are used to create
language models (LM) with modified Kneser-Ney
smoothing (Chen and Goodman, 1999). The emis-
sion values for state li are initialized with the re-
spective LM probabilities for all seen n-grams.
We also assign a small probability to unk. The
emissions for the xli state are initialized using the
counts of universal tokens for the language li in
W . These are identified using the preprocessing
techniques discussed in Sec. 5.1.

Possible transitions for each monolingual HMM
are li → li, li → xli and xli → li. We do not
have the xli → xli transition, because preprocess-
ing (Sec. 5.1) concatenates successive universal
tokens into a single token. This does not change
the output as the tokens can easily be separated af-
ter LD, but is a useful simplification for the model.
The transition values for li are initialized by the
probability of transitions between words and uni-
versal tokens in the text fromW .

As stated earlier, the model supports code-
switching by the addition of transitions li → lj ,
and xli → lj , for all i 6= j. For each state li, there
are 2k − 2 new transitions (Fig. 2). We initial-
ize these news edges with a small probability π,
before normalizing transitions for each state. π,
which we call the code-switch probability, is a hy-
perparameter tuned on a validation set.

Starting with the initialized matrices, we rees-
timate the transition and emission matrices using
the EM-like Baum-Welch algorithm (Welch, 2003)
over the large set of unlabeled text U .

4.2 Decoding

The input to the trained model is first preprocessed
as described in Sec. 5.1 (tokenization and identifi-
cation of universal tokens). The Viterbi algorithm
is then used with the HMM parameters to perform
word-level LD. When an unknown n-gram, is en-
countered, its emission probability is estimated by
recursively backing off to (n − 1)-gram, until we
find a known n-gram. If the unigram, i.e., the to-
ken, is also unknown, then the observation of the
symbol unk is used instead.

5 Dataset Creation

The data for both training and testing comes pri-
marily from Twitter because of its public API,
and studies have shown the presence of code-
switching in social media (Crystal, 2001; Herring,
2003; Danet and Herring, 2007; Cardenas-Claros
and Isharyanti, 2009; Bali et al., 2014).

Our experiments use monolingual and code-
switched tweets in seven languages – Dutch (nl),
English (en), French (fr), German (de), Portuguese
(pt), Spanish (es) and Turkish (tr). These form the
set L. The choice of languages is motivated by
several factors. First, LD is non-trivial as all these
languages use the Latin script. Second, a large
volume of tweets are generated in these languages.

1974



Third, there is annotated code-switched data avail-
able in nl-tr and en-es, which can be used for val-
idation and testing. Lastly, we know that certain
pairs of these languages are code-switched often.

5.1 Collection and Preprocessing
Using the Twitter API (Twitter, 2013), we col-
lected tweets over May-July 2015. We selected
tweets identified by Twitter LD API (Twitter,
2015) as one of the languages in L. We also re-
moved non-Latin script tweets.

As preprocessing, each tweet is first tokenized
using ark-twitter (Gimpel et al., 2011) and URLs,
hashtags and user mentions are identified using
regular expressions. We also identify emoticons,
punctuation, digits, special characters, and some
universal interjections and abbreviations (such as
RT, aww) as universal tokens. We use an existing
dictionary (Chittaranjan et al., 2014) for the latter.
Let the set of tweets after preprocessing be T .

5.2 SetsW and U
We use the COVERSET algorithm (Gella et al.,
2014) on each tweet in T . It obtains a confidence
score for a word wi belonging to a language lj us-
ing a Naive Bayes classifier trained on Wikipedia.
These scores are used to find the minimal set of
languages are required to label all the input words.
If COVERSET detects the tweet as monolingual
(i.e., one language can label all words) and the
identified language is the same as the Twitter LD
label, the tweet is added to the weakly-labeled set
W . These tweets are almost certainly monolin-
gual, as COVERSET has very high recall (and low
precision) for detecting code-switching. As these
are not manually labeled, we call them weakly-
labeled. W contains 100K tweets in each lan-
guage (700K in total).

From T , we randomly select 100K tweets in
each of the seven languages based on the Twitter
LD API labels. These tweets do not have word-
level language labels and may be code-switched
or have an incorrect Twitter language label. We
use these as unlabeled data, the set U .

5.3 Validation and Test Sets
We curate two word-level gold-standard datasets
for validation and testing. These sets contain
monolingual tweets in each of the seven languages
as well as code-switched tweets from certain lan-
guage pairs, based on the availability of real-world
data. However, it must be noted that GWLD can

L1-L2 Tweets L1 Tokens L2 Tokens
nl 100 (100) 965 (1099) –
fr 100 (102) 1085 (1045) –
pt 100 (100) 1080 (967) –
de 101 (100) 1078 (890) –
tr 100 (100) 939 (879) –
es 100 (100) 1067 (1119) –
en 100 (100) 1161 (1006) –
nl-en 65 (50) 498 (436) 243 (174)
fr-en 50 (48) 428 (370) 224 (227)
pt-en 53 (53) 463 (513) 278 (242)
de-en 49 (50) 417 (459) 293 (292)
tr-en 50 (50) 347 (336) 238 (209)
es-en 3013 (52) 8510 (355) 16356 (395)
nl-tr 735 (728) 5895 (8590) 5293 (8140)

Table 1: Test Set Statistics (Validation Set in
parentheses). Rows in gray show existing datasets.

detect code-switching between more than two lan-
guages. The language-wise distribution is shown
in Table 1. Including universal tokens, the valida-
tion and test set contain 33981 and 58221 tokens
respectively. The annotated tweets will be made
available for public use.

For es-en, we use the word-level annotated test
set from the code-switching shared task on lan-
guage detection (Solorio et al., 2014). We ignore
the tokens labeled NE, Ambiguous and Mixed dur-
ing our system evaluation (Sec. 6), as they do not
fall in the scope of this work. The words labeled
‘Other’ were marked as xli where li is en or es,
based on the context. We also use existing nl-
tr validation and test sets (Nguyen and Dogruöz,
2013), which contain posts from a web forum.

For the other language pairs, we created our
own validation and test sets, as none already ex-
ist. We randomly selected tweets for which CO-
VERSET identified code-switching with high con-
fidence. We gave 215 of these to six annotators
for word-level annotation. It is difficult to find
annotators who know all seven languages; elabo-
rate guidelines were provided on using online ma-
chine translation, dictionaries and search engines
for the task. Four out of the six annotators had
high inter-annotator agreement – the agreement on
L1 (language that the majority of the words in the
tweet belong to) was 0.93, L2 (the other language,
whenever present) was 0.8 and whether the tweet
is code-switched was 0.84. We did not find any in-
stances of code-switching between more than two

1975



Systems Acc L1L2Acc IsMix
Dictionary-based Baselines
MAXFREQ 0.824 0.752 0.600
MINCOVER 0.853 0.818 0.733
Existing Systems
LINGUINI NA 0.529 0.783
LANGID NA 0.830 0.783
POLYGLOT NA 0.521 0.692
GWLD: The Proposed Method
Initial 0.838 0.825 0.837
Reestimated 0.963 0.914 0.88

Table 2: Performance of LD Systems on Test Set

languages, which is rare in general. We distributed
3000 tweets between the four annotators (mono-
lingual and code-switched tweets from COVER-
SET). Disagreements were settled between the an-
notators and a linguist. A subset of the annotated
tweets form the validation and test sets (Table 1),
and were removed fromW and U .

6 Experiments and Results

We compare GWLD with three existing systems:
LINGUINI (Prager, 1999), LANGID (Lui and Bald-
win, 2012), and POLYGLOT (Lui et al., 2014).
None of these perform word-level LD, however,
LANGID and POLYGLOT return a list of languages
with confidence scores for the input. Since code-
switching with more than two languages is absent
in our dataset, we consider up to two language la-
bels. We define the tweet to be monolingual if the
difference between the confidence values for the
top two languages is greater than a parameter δ.
Otherwise, it is assumed to be code-switched with
the top two languages. δ is tuned independently
for the two LD systems on the validation set by
maximizing the metric L1L2 Accuracy (Sec. 6.2).
Inspired by Gella et al. (2013), we also compare
with dictionary-based word-level LD baselines.

6.1 Dictionary-based Baselines

For each language, we build a lexicon of all the
words and their frequencies found in W for that
language. Let the lexicon for language li ∈ L be
lexi. Let f(lexi, wj) be the frequency of wj in
lexi. We define the following baselines:

MAXFREQ: For each wj in w, MAXFREQ re-
turns lexi that has the maximum frequency for
that token. Therefore, the language label for wj is
yj = l[argmaxi f(lexi,wj)]. If the token is not found

in any lexicon, yj is assigned the value of yj−1.
MINCOVER: We find the smallest subset

mincov(w) ⊂ L, such that for all wj in input w,
we have at least one language li ∈ mincov(w)
with f(lexi, wj) > 0. If there is no such lan-
guage, then wj is not considered while comput-
ing mincov(w). Once mincov(w) is obtained,
labels yi are computed using the MAXFREQ strat-
egy, where the set of languages is restricted to
mincov(w) instead of L. Note that mincov(w)
need not be unique for w; in such cases, we choose
the mincov(w) which maximizes the sum of lex-
ical frequencies based on MAXFREQ labels.

6.2 Metrics
We define the Accuracy (Acc) of an LD system as
the fraction of words in the test set that are labeled
correctly. Since the existing LD systems do not
label languages at word-level, we also define:

IsMix is the fraction of tweets that are correctly
identified as either monolingual or code-mixed.
L1L2 Accuracy (L1L2Acc) is the mean accu-

racy of detecting language(s) at tweet-level. For
monolingual tweets, this accuracy is 1 if the gold
standard label is detected by the LD system, else
0. For code-switched tweets, the accuracy is 1 if
both languages are detected, 0.5 if one language is
detected, and 0 otherwise. L1L2Acc is the average
over all test set tweets.

6.3 Results
We use these metrics to assess performance on the
test set for the baselines, existing LD systems and
GWLD (Table 2). Initial refers to the HMM model
estimated from W and Reestimated refers to the
final model after Baum-Welch reestimation. The
parameter π is tuned on the validation set using
grid search. Reestimated GWLD has the best ac-
curacy of 0.963 and performs significantly better
than all the other systems for all metrics. Reesi-
matation improves the word-levelAcc for L1 from
0.89 to 0.97 and for L2 from 0.43 to 0.82. LIN-
GUINI and POLYGLOT likely have low L1L2Acc
because they are trained on synthetically-created
documents with no word-level code-switching.

Since our test set contains pre-existing anno-
tations for en-es (Solorio et al., 2014) and nl-tr
(Nguyen and Dogruöz, 2013), we compare with
state-of-the-art results on those datasets. On en-es
tokens, Al-Badrashiny and Diab (2016) reports an
F1-score of 0.964; GWLD obtains 0.978. Nguyen
and Dogruöz (2013) report 0.976 Acc on the nl-tr

1976



(a) (b)

Figure 3: Acc versus Dataset Parameters

Figure 4: Acc versus Number of Languages

test set. We obtain a less competitive 0.936. How-
ever, when errors between nl-en are ignored as
most of these are en words with nl gold-standard
labels (convention followed by the dataset cre-
ators), the revised Acc is 0.963. Notably, unlike
GWLD, both these models use large amounts of
annotated data for training and are restricted to de-
tecting only two languages.

Error Analysis: GWLD sometimes detects lan-
guages that are not present in the tweet, which ac-
count for a sizable fraction (39%) of all word-level
errors. Not detecting a language switch causes
8% of the errors. Most other errors are caused by
named entities, single-letter tokens, unseen words
and the nl-en annotation convention in the test set
from Nguyen and Dogruöz (2013).

6.4 Robustness of GWLD
We test the robustness of GWLD by varying
the size of the weakly-labeled set, the unlabeled
dataset and the number of languages the model is
trained to support.

6.4.1 Size ofW and U
The variation of Acc with the size ofW is shown
in Figure 3a. Even with 0.25% of the set (250

L1-L2 Acc IsCM GWLD-Acc
nl-en 0.979 0.943 0.967
fr-en 0.982 0.948 0.969
pt-en 0.977 0.952 0.964
de-en 0.984 0.956 0.975
tr-en 0.985 0.984 0.983
es-en 0.954 0.929 0.978
nl-tr 0.975 0.907 0.936

Table 3: Statistics for Pairwise (col. 2 and 3) and
GWLD Systems

tweets for each li ∈ L), the model has accuracy of
nearly 0.96. A slow rise in accuracy is observed
as the number of tweets in W is increased. We
also experiment with varying the size of U . In
Figure 3a, we see that with 0.25% of U (around
1,400 randomly sampled tweets), the accuracy on
the test set is lower than 0.91. This quickly in-
creases with 10% of U . Thus, GWLD achieves
Acc comparable to existing systems with very lit-
tle weakly-labeled data (just 250 tweets per lan-
guage, which are easily procurable for most lan-
guages) and around 50,000 unlabeled tweets.

6.4.2 Noise inW
Since a small, but pure, W gives high accuracy
(Sec. 6.4.1), we evaluate how artificially intro-
duced noise affectsAcc. The noise introduced into
the W of each language comes uniformly from
the other six languages. Figure 3b shows how in-
creasing fractions of noise slowly degrades accu-
racy, with a steep drop to 0.11 accuracy at 90%
noise, where the tweets from each incorrect lan-
guage outnumber the correct language tweets. We
test this with a pairwise model as well, as noise
from a single language might have greater effect.
The accuracy falls to 0.36 at 50% noise (Fig. 3b).
At this point, W has an equal number of tweets
from each language and is essentially useless.

6.4.3 Number of languages
Pairwise Models: Table 3 details two perfor-
mance metrics (defined in Sec. 5.2) for our model
trained on only two languages and the correspond-
ing 7-language GWLD Acc for that language pair.
Incremental Addition of Languages: We test
Acc while incrementally adding languages to the
model in a random order (nl-en-pt-fr-de-es-tr).
Figure 4 shows the variation in Acc for nl-en, pt-
en and fr-en as more languages are added to the

1977



Figure 5: Worldwide distribution of monolingual
and CS tweets (left and right charts respectively)

Figure 6: Worldwide CS point distribution

model. Although there is a slight degradation, in
absolute terms, the accuracy remains very high.

7 Code-Switching on Twitter

The high accuracy and fast processing speed (the
current multithreaded implementation labels 2.5M
tweets per hour) of GWLD enables us to conduct
large-scale and reliable studies of CS patterns on
Twitter for the 7 languages. In this paper, we con-
duct two such studies. The first study analyzes
50M tweets from across the world to understand
the extent and broad patterns of switching among
these languages. In the second study, we ana-
lyze 8M tweets from 24 cities to gain insights into
geography-specific CS patterns.

7.1 Worldwide Code-Switching Trends

We collected 50 million unique tweets that were
identified by the Twitter LD API as one of the
7 languages. We place this constraint to avoid
tweets from unsupported languages during anal-
ysis. Figure 5 shows the overall language distribu-
tion, including the CS language-pair distribution.
Approximately 96.5% of the tweets are monolin-
gual, a majority of which are en (74%).

Around 3.5% of all tweets are code-switched.
Globally, en-es, en-fr and en-pt are the three most

commonly mixed pairs accounting for 21.5%,
20.8% and 18.4% of all CS tweets in our data re-
spectively. Interestingly, 85.4% of the CS tweets
have en as one of the languages; fr is the next
most popularly mixed language, with fr-es (3.2%),
fr-pt (1.2%) and fr-nl (0.6%) as the top three ob-
served pairs. Although around 1% of CS tweets
were detected as containing more than two lan-
guages, these likely have low precision because of
language overdetection as discussed in Sec. 6.3.

Figure 6 shows the fraction of code-switch
points, i.e., how many times the language changes
in a CS tweet, for all the languages, as well as
for three language pairs with to highlight differ-
ent trends. Most CS tweets have one CS-point,
which implies that the tweet begins with one lan-
guage, and then ends with another. Such tweets
are very frequent for en-de where we observe that
usually the tweets state the same fact in both en
and de. This so-called translation function (Be-
gum et al., 2016) of CS is probably adopted for
reaching out to a wider and global audience. In
contrast, es-fr tweets have fewer tweets with sin-
gle and far more with two CS-point than average.
Tweets with two CS-points typically imply the in-
clusion of a short phrase or chunk from another
language. en-tr tweets have the highest number of
CS-points, implying rampant and fluid switching
between the two languages at all structural levels.

7.2 City-Specific Code-Switching Trends

Cosmopolitan cities are melting pots of cultures,
which make them excellent locations for studying
multilingualism and language interaction, includ-
ing CS (Bailey et al., 2013). We collected tweets
from 24 populous and highly cosmopolitan cities
from Europe, North America and South America,
where the primarily spoken language is one of the
7 languages detectable by GWLD. Around 8M
tweets were collected from these cities.

Table 4 shows the top and bottom 6 cities,
ranked by the fraction of CS tweets from that city.
The total number of tweets analyzed and the top
two CS pairs, along with their fractions (of CS
tweets from that city) are also reported. More de-
tails can be found in the supplementary material.
It is interesting to note that the 6 cities with lowest
CS tweet fractions have en as the major language,
whereas the 6 cities with highest CS fractions are
from non-English (Turkish, Spanish and French)
speaking geographies. In fact, the Pearson’s cor-

1978



Cities with highest fraction of CS tweet Cities with lowest fraction of CS tweets
City Tweets CS-fraction (CS pairs) City Tweets CS-fraction (CS pairs)
Istanbul 351K .12 (en-tr .53, nl-tr .13) Houston 588K .01 (en-es .22, en-fr .21)
Québec City 108K .08 (en-fr .45, es-fr .23) San Francisco 532K .02 (en-es .26, en-fr .19)
Paris 158K .07 (en-fr .43, fr-pt .21) NYC 690K .02 (en-es .21, en-fr .19)
Mexico City 332K .07 (en-es .54, es-fr .14) Miami 290K .02 (en-es .33, en-pt .20)
Brussels 100K .06 (en-fr .37, es-fr .15) London 492K .02 (en-fr .26, en-pt .17)
Madrid 147K .06 (en-es .43, es-fr .32) San Diego 432K .02 (en-es .29, en-fr .14)

Table 4: Top (left) and bottom (right) six cities according to the fraction of CS tweets.

Figure 7: en-es Run Length

relation between the fraction of monolingual En-
glish tweets and CS tweets for these 24 cities is
−0.85. Further, from Table 4 one can also ob-
serve that for non-English speaking geographies,
the majority language is most commonly mixed
with English, followed by French (Spanish, if
French is the majority language). Istanbul is an
exception, where Dutch is the second most com-
monly mixed language with Turkish, presumably
because of the large Turkish immigrant popula-
tion in Netherlands resulting in a sizeable Turkish-
Dutch bilingual diaspora (Doğruöz and Backus,
2009; Nguyen and Dogruöz, 2013).

Is there a difference in the way speakers mix a
pair of languages, say en and es, in en-speaking
goegraphies like San Diego, Miami, Houston and
New York City, and es-speaking geographies like
Madrid, Barcelona, Buenos Aires and Mexico
City? Indeed, as shown in Fig. 7, the distribu-
tion of the lengths of en and es runs (contigu-
ous sequence of words in a single language be-
ginning and ending with either a CS-point or be-
ginning/end of a tweet) in en-es CS tweets is sig-
nificantly different in en-speaking and es-speaking
geographies. en runs are longer in en-speaking
cities and vice versa, showing that the second lan-
guage is likely used in short phrases.

8 Conclusion and Future Work

We present GWLD, a system for word-level lan-
guage detection for an arbitrarily large set of lan-
guages that is completely unsupervised. Our re-

sults on monolingual and code-switched tweets in
seven Latin script languages show a high 0.963 ac-
curacy, significantly out-performing existing sys-
tems. Using GWLD, we conducted a large-scale
study of CS trends among these languages, both
globally and in specific cities.

One of the primary observations of this study
is that while code-switching on Twitter is com-
mon worldwide (3.5%), it is much more common
in non-English speaking cities like Istanbul (12%)
where 90% of the population speak Turkish. On
the other hand, while a third of the population
of Houston speaks Spanish and almost everybody
English, only 1% of the tweets from the city are
code-switched. All the trends indicate a global
dominance of English, which might be because
Twitter is primarily a medium for broadcast, and
English tweets have a wider audience. Bergsma
et al. (2012) show that “[On Twitter] bilinguals
bridge between monolinguals with English as a
hub, while monolinguals tend not to directly fol-
low each other.” Androutsopoulos (2006) argues
that due to linguistic non-homogenity of online
public spaces, languages like en, fr and de are typ-
ically preferred for communication, even though
in private spaces, ”bilingual talk” differs consider-
ably in terms of distribution and CS patterns.

As future directions, we plan to extend GWLD
to several other languages and conduct similar so-
ciolinguistic studies on CS patterns including not
only more languages and geographies, but also
other aspects like topic and sentiment.

Acknowledgments

We would like to thank Prof. Shambavi Pradeep
and her students from BMS College of Engineer-
ing for assisting with data annotation. We are also
grateful to Ashutosh Baheti and Silvana Hartmann
from Microsoft Research (Bangalore, India) for
help with data organization and error analysis.

1979



References
Mohamed Al-Badrashiny and Mona Diab. 2016. Lili:

A simple language independent approach for lan-
guage identification. In Proceedings of the 26th In-
ternational Conference on Computational Linguis-
tics (COLING). Osaka, Japan.

Jannis Androutsopoulos. 2006. Multilingualism, di-
aspora, and the internet: Codes and identities on
german-based diaspora websites. Journal of Soci-
olinguistics 10(4):520–547.

Peter Auer. 2013. Code-switching in conversation:
Language, interaction and identity. Routledge.

George Bailey, Joseph Goggins, and Thomas Ingham.
2013. What can Twitter tell us about the language
diversity of Greater Manchester? In Report by Mul-
tilingual Manchester. School of Languages, Lin-
guistics and Cultures at the University of Manch-
ester. http://bit.ly/2kG42Qf.

Kalika Bali, Yogarshi Vyas, Jatin Sharma, and Monojit
Choudhury. 2014. “I am borrowing ya mixing?” an
analysis of English-Hindi code mixing in Facebook.
In Proceedings of the First Workshop on Computa-
tional Approaches to Code Switching.

Utsab Barman, Amitava Das, Joachim Wagner, and
Jennifer Foster. 2014. Code mixing: A challenge for
language identification in the language of social me-
dia. In Proceedings of the First Workshop on Com-
putational Approaches to Code Switching.

Rafiya Begum, Kalika Bali, Monojit Choudhury, Kous-
tav Rudra, and Niloy Ganguly. 2016. Functions of
code-switching in tweets: An annotation framework
and some initial experiments. In Proceedings of
the Tenth International Conference on Language Re-
sources and Evaluation (LREC).

Shane Bergsma, Paul McNamee, Mossaab Bagdouri,
Clayton Fink, and Theresa Wilson. 2012. Language
identification for creating language-specific twitter
collections. In Proceedings of the second workshop
on language in social media. Association for Com-
putational Linguistics.

Mónica Stella Cardenas-Claros and Neny Isharyanti.
2009. Code-switching and code-mixing in internet
chatting: Between yes, ya, and si a case study. In
The JALT CALL Journal, 5.

Simon Carter, Wouter Weerkamp, and Manos
Tsagkias. 2013. Microblog language identification:
Overcoming the limitations of short, unedited and
idiomatic text. Language Resources and Evaluation
Journal 47:195–215.

William B Cavnar and John M Trenkle. 1994. N-gram-
based text categorization .

Stanley F Chen and Joshua Goodman. 1999. An
empirical study of smoothing techniques for lan-
guage modeling. Computer Speech & Language
13(4):359–393.

Gokul Chittaranjan, Yogrshi Vyas, Kalika Bali, and
Monojit Choudhury. 2014. Word-level language
identication using crf : Code-switching shared task
report of msr india system. In Proceedings of the
First Workshop on Computational Approaches to
Code Switching.

Monojit Choudhury, Gokul Chittaranjan, Parth Gupta,
and Amitava Das. 2014. Overview of FIRE 2014
track on transliterated search .

David Crystal. 2001. Language and the Internet. Cam-
bridge University Press.

Brenda Danet and Susan Herring. 2007. The Multilin-
gual Internet: Language, Culture, and Communica-
tion Online. Oxford University Press., New York.

Amitava Das and Bjorn Gambäck. 2014. Identifying
languages at the word level in code-mixed indian so-
cial media text. In Proceedings of the 11th Interna-
tional Conference on Natural Language Processing.
Goa, India, pages 169–178.

A Seza Doğruöz and Ad Backus. 2009. Innovative con-
structions in dutch turkish: An assessment of on-
going contact-induced change. Bilingualism: lan-
guage and cognition 12(01):41–63.

Ted Dunning. 1994. Statistical identification of lan-
guage. Computing Research Laboratory, New Mex-
ico State University.

Heba Elfardy, Mohamed Al-Badrashiny, and Mona
Diab. 2013. Code switch point detection in ara-
bic. In Natural Language Processing and Informa-
tion Systems, Springer, pages 412–416.

Archana Garg, Vishal Gupta, and Manish Jindal. 2014.
A survey of language identification techniques and
applications. Journal of Emerging Technologies in
Web Intelligence 6(4):388–400.

Spandana Gella, Kalika Bali, and Monojit Choudhury.
2014. “ye word kis lang ka hai bhai?” testing the
limits of word level language identification. In NL-
PAI.

Spandana Gella, Jatin Sharma, and Kalika Bali. 2013.
Query word labeling and back transliteration for in-
dian languages: Shared task system description .

Kevin Gimpel, Nathan Schneider, Brendan O’Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and A. Noah Smith. 2011. Part-of-speech tagging
for twitter: Annotation, features, and experiments.
In Proceedings of Annual Meeting of the Associa-
tion for Computational Linguistics (ACL).

Moises Goldszmidt, Marc Najork, and Stelios Papari-
zos. 2013. Boot-strapping language identifiers for
short colloquial postings. In Machine Learning and
Knowledge Discovery in Databases, volume 8189 of
Lecture Notes in Computer Science, pages 95–111.

1980



John. J. Gumperz. 1982. Discourse strategies. Cam-
bridge University Press, Cambridge.

Harald Hammarström. 2007. A fine-grained model for
language identification. In In Workshop of Improv-
ing Non English Web Searching. Proceedings of iN-
EWS 2007 Workshop at SIGIR.

Susan Herring, editor. 2003. Media and Language
Change. Special issue of Journal of Historical Prag-
matics 4:1.

Baden Hughes, Timothy Baldwin, SG Bird, Jeremy
Nicholson, and Andrew MacKinlay. 2006. Recon-
sidering language identification for written language
resources .

David Jurgens, Yulia Tsvetkov, and Dan Jurafsky.
2017. Incorporating dialectal variability for socially
equitable language identification. In Proceedings of
the 55th Annual Meeting of the Association for Com-
putational Linguistics (ACL). Vancouver, Canada.

Suin Kim, Ingmar Weber, Li Wei, and Alice Oh. 2014.
Sociolinguistic analysis of twitter in multilingual so-
cieties. In Proceedings of the 25th ACM conference
on Hypertext and social media.

Ben King and Steven Abney. 2013. Labeling the lan-
guages of words in mixed-language documents us-
ing weakly supervised methods. In Proceedings of
NAACL-HLT . pages 1110–1119.

Marco Lui and Timothy Baldwin. 2012. langid.py: An
off-the-shelf language identification tool. In In Pro-
ceedings of the ACL 2012 System Demonstrations.
pages 25–30.

Marco Lui, Jey Han Lau, and Timothy Baldwin. 2014.
Automatic detection and language identification of
multilingual documents. In Transactions of the As-
sociation for Computational Linguistics.

Ed Manley. 2012. Detecting languages in Londons
Twittersphere. In Blog post: Urban Movements.
http://bit.ly/2kBytHm.

P. McNamee. 2005. Language identification: A
solved problem suitable for undergraduate instruc-
tion. Journal of Computing Sciences in Colleges 20.

Lesley Milroy and Pieter Muysken. 1995. One speaker,
two languages: Cross-disciplinary perspectives on
code-switching. Cambridge University Press.

Giovanni Molina, Nicolas Rey-Villamizar, Thamar
Solorio, Fahad AlGhamdi, Mahmoud Ghoneim, Ab-
delati Hawwari, and Mona Diab. 2016. Overview
for the second shared task on language identification
in code-switched data. EMNLP 2016 page 40.

Carol Myers-Scotton. 1993. Dueling Languages:
Grammatical Structure in Code-Switching. Clare-
don, Oxford.

Dong Nguyen and A. Seza Dogruöz. 2013. Word level
language identification in online multilingual com-
munication. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing.

Dong Nguyen, A Seza Doğruöz, Carolyn P Rosé, and
Franciska de Jong. 2016. Computational sociolin-
guistics: A survey. Computational Linguistics .

Shana Poplack. 1980. Sometimes Ill start a sentence
in Spanish y termino en espaol. Linguistics 18:581–
618.

John M Prager. 1999. Language identification for mul-
tilingual documents. In Systems Sciences, 1999.
HICSS-32. Proceedings of the 32nd Annual Hawaii
International Conference.

Rishiraj Saha Roy, Monojit Choudhury, Prasenjit Ma-
jumder, and Komal Agarwal. 2013. Overview and
datasets of FIRE 2013 track on transliterated search.
In Working Notes of FIRE.

Koustav Rudra, Shruti Rijhwani, Rafiya Begum, Kalika
Bali, Monojit Choudhury, and Niloy Ganguly. 2016.
Understanding language preference for expression
of opinion and sentiment: What do Hindi-English
speakers do on Twitter? In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing.

Royal Sequiera, Monojit Choudhury, Parth Gupta,
Paolo Rosso, Shubham Kumar, Somnath Banerjee,
Sudip Kumar Naskar, Sivaji Bandyopadhyay, Gokul
Chittaranjan, Amitava Das, and Kunal Chakma.
2015. Overview of fire-2015 shared task on mixed
script information retrieval. In Working Notes of
FIRE.

Thamar Solorio, Elizabeth Blair, Suraj Mahar-
jan, Steven Bethard, Mona Diab, Mahmoud
Gohneim, Abdelati Hawwari, Fahad AlGhamdi, Ju-
lia Hirschberg, Alison Chang, et al. 2014. Overview
for the first shared task on language identification in
code-switched data. Proceedings of The First Work-
shop on Computational Approaches to Code Switch-
ing .

Erik Tromp and Mykola Pechenizkiy. 2011. Graph-
based n-gram language identification on short texts.
In In Proc. 20th Machine Learning conference of
Belgium and The Netherlands. pages 27–34.

Twitter. 2013. GET sta-
tuses/sample — Twitter Developers.
https://dev.twitter.com/docs/api/1/get/statuses/sample.

Twitter. 2015. GET help/languages
— Twitter Developers.
https://dev.twitter.com/rest/reference/get/help/languages.

Lloyd R Welch. 2003. Hidden markov models and the
baum-welch algorithm. IEEE Information Theory
Society Newsletter 53(4):10–13.

1981



Fei Xia, William D Lewis, and Hoifung Poon. 2009.
Language id in the context of harvesting language
data off the web. In In Proceedings of the 12th
EACL. pages 870–878.

Wei Zhang, Robert AJ Clark, Yongyuan Wang, and
Wen Li. 2016. Unsupervised language identifica-
tion based on latent dirichlet allocation. Computer
Speech & Language 39:47–66.

1982


	Estimating Code-Switching on Twitter with a Novel Generalized Word-Level Language Detection Technique

