



















































Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics


Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 2078–2088
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1190

Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 2078–2088
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1190

Revisiting Recurrent Networks for Paraphrastic Sentence Embeddings

John Wieting Kevin Gimpel
Toyota Technological Institute at Chicago, Chicago, IL, 60637, USA

{jwieting,kgimpel}@ttic.edu

Abstract

We consider the problem of learn-
ing general-purpose, paraphrastic sen-
tence embeddings, revisiting the setting
of Wieting et al. (2016b). While they
found LSTM recurrent networks to un-
derperform word averaging, we present
several developments that together pro-
duce the opposite conclusion. These in-
clude training on sentence pairs rather than
phrase pairs, averaging states to repre-
sent sequences, and regularizing aggres-
sively. These improve LSTMs in both
transfer learning and supervised settings.
We also introduce a new recurrent archi-
tecture, the GATED RECURRENT AVER-
AGING NETWORK, that is inspired by av-
eraging and LSTMs while outperforming
them both. We analyze our learned mod-
els, finding evidence of preferences for
particular parts of speech and dependency
relations. 1

1 Introduction

Modeling sentential compositionality is a funda-
mental aspect of natural language semantics. Re-
searchers have proposed a broad range of com-
positional functional architectures (Mitchell and
Lapata, 2008; Socher et al., 2011; Kalchbrenner
et al., 2014) and evaluated them on a large vari-
ety of applications. Our goal is to learn a general-
purpose sentence embedding function that can be
used unmodified for measuring semantic textual
similarity (STS) (Agirre et al., 2012) and can also
serve as a useful initialization for downstream
tasks. We wish to learn this embedding function

1Trained models and code are available at http://
ttic.uchicago.edu/˜wieting.

such that sentences with high semantic similar-
ity have high cosine similarity in the embedding
space. In particular, we focus on the setting of
Wieting et al. (2016b), in which models are trained
on noisy paraphrase pairs and evaluated on both
STS and supervised semantic tasks.

Surprisingly, Wieting et al. found that sim-
ple embedding functions—those based on aver-
aging word vectors—outperform more powerful
architectures based on long short-term memory
(LSTM) (Hochreiter and Schmidhuber, 1997). In
this paper, we revisit their experimental setting
and present several techniques that together im-
prove the performance of the LSTM to be superior
to word averaging.

We first change data sources: rather than
train on noisy phrase pairs from the Paraphrase
Database (PPDB; Ganitkevitch et al., 2013), we
use noisy sentence pairs obtained automatically
by aligning Simple English to standard English
Wikipedia (Coster and Kauchak, 2011). Even
though this data was intended for use by text sim-
plification systems, we find it to be efficient and ef-
fective for learning sentence embeddings, outper-
forming much larger sets of examples from PPDB.

We then show how we can modify and regular-
ize the LSTM to further improve its performance.
The main modification is to simply average the
hidden states instead of using the final one. For
regularization, we experiment with two kinds of
dropout and also with randomly scrambling the
words in each input sequence. We find that these
techniques help in the transfer learning setting and
on two supervised semantic similarity datasets as
well. Further gains are obtained on the super-
vised tasks by initializing with our models from
the transfer setting.

Inspired by the strong performance of both av-
eraging and LSTMs, we introduce a novel recur-
rent neural network architecture which we call

2078

https://doi.org/10.18653/v1/P17-1190
https://doi.org/10.18653/v1/P17-1190


the GATED RECURRENT AVERAGING NETWORK
(GRAN). The GRAN outperforms averaging and
the LSTM in both the transfer and supervised
learning settings, forming a promising new recur-
rent architecture for semantic modeling.

2 Related Work

Modeling sentential compositionality has received
a great deal of attention in recent years. A com-
prehensive survey is beyond the scope of this pa-
per, but we mention popular functional families:
neural bag-of-words models (Kalchbrenner et al.,
2014), deep averaging networks (DANs) (Iyyer
et al., 2015), recursive neural networks using syn-
tactic parses (Socher et al., 2011, 2012, 2013;
İrsoy and Cardie, 2014), convolutional neural net-
works (Kalchbrenner et al., 2014; Kim, 2014; Hu
et al., 2014), and recurrent neural networks using
long short-term memory (Tai et al., 2015; Ling
et al., 2015; Liu et al., 2015). Simple operations
based on vector addition and multiplication typi-
cally serve as strong baselines (Mitchell and Lap-
ata, 2008, 2010; Blacoe and Lapata, 2012).

Most work cited above uses a supervised learn-
ing framework, so the composition function is
learned discriminatively for a particular task. In
this paper, we are primarily interested in creating
general purpose, domain independent embeddings
for word sequences. Several others have pursued
this goal (Socher et al., 2011; Le and Mikolov,
2014; Pham et al., 2015; Kiros et al., 2015; Hill
et al., 2016; Arora et al., 2017; Pagliardini et al.,
2017), though usually with the intent to extract
useful features for supervised sentence tasks rather
than to capture semantic similarity.

An exception is the work of Wieting et al.
(2016b). We closely follow their experimental
setup and directly address some outstanding ques-
tions in their experimental results. Here we briefly
summarize their main findings and their attempts
at explaining them. They made the surprising dis-
covery that word averaging outperforms LSTMs
by a wide margin in the transfer learning setting.
They proposed several hypotheses for why this oc-
curs. They first considered that the LSTM was un-
able to adapt to the differences in sequence length
between phrases in training and sentences in test.
This was ruled out by showing that neither model
showed any strong correlation between sequence
length and performance on the test data.

They next examined whether the LSTM was

overfitting on the training data, but then showed
that both models achieve similar values of the
training objective and similar performance on in-
domain held-out test sets. Lastly, they considered
whether their hyperparameters were inadequately
tuned, but extensive hyperparameter tuning did not
change the story. Therefore, the reason for the per-
formance gap, and how to correct it, was left as an
open problem. This paper takes steps toward ad-
dressing that problem.

3 Models and Training

3.1 Models
Our goal is to embed a word sequence s into a
fixed-length vector. We focus on three composi-
tional models in this paper, all of which use words
as the smallest unit of compositionality. We de-
note the tth word in s as st, and we denote its word
embedding by xt.

Our first two models have been well-studied in
prior work, so we describe them briefly. The first,
which we call AVG, simply averages the embed-
dings xt of all words in s. The only parameters
learned in this model are those in the word em-
beddings themselves, which are stored in the word
embedding matrix Ww. This model was found by
Wieting et al. (2016b) to perform very strongly for
semantic similarity tasks.

Our second model uses a long short-term mem-
ory (LSTM) recurrent neural network (Hochreiter
and Schmidhuber, 1997) to embed s. We use the
LSTM variant from Gers et al. (2003) including its
“peephole” connections. We consider two ways to
obtain a sentence embedding from the LSTM. The
first uses the final hidden vector, which we denote
h−1. The second, denoted LSTMAVG, averages
all hidden vectors of the LSTM. In both variants,
the learnable parameters include both the LSTM
parameters Wc and the word embeddings Ww.

Inspired by the success of the two models
above, we propose a third model, which we call
the GATED RECURRENT AVERAGING NETWORK
(GRAN). The GATED RECURRENT AVERAGING
NETWORK combines the benefits of AVG and
LSTMs. In fact it reduces to AVG if the output
of the gate is all ones. We first use an LSTM to
generate a hidden vector, ht, for each word st in
s. Then we use ht to compute a gate that will
be elementwise-multiplied with xt, resulting in a
new, gated hidden vector at for each step t:

at = xt � σ(Wxxt +Whht + b) (1)

2079



where Wx and Wh are parameter matrices, b is a
parameter vector, and σ is the elementwise logis-
tic sigmoid function. After all at have been gener-
ated for a sentence, they are averaged to produce
the embedding for that sentence. This model in-
cludes as learnable parameters those of the LSTM,
the word embeddings, and the additional parame-
ters in Eq. (1). For both the LSTM and GRAN
models, we use Wc to denote the “compositional”
parameters, i.e., all parameters other than the word
embeddings.

The motivation for the GRAN is that we are
contextualizing the word embeddings prior to av-
eraging. The gate can be seen as an attention, at-
tending to the prior context of the sentence.2

We also experiment with four other variations of
this model, though they generally were more com-
plex and showed inferior performance. In the first,
GRAN-2, the gate is applied to ht (rather than xt)
to produce at, and then these at are averaged as
before.

GRAN-3 and GRAN-4 use two gates: one ap-
plied to xt and one applied to at−1. We tried
two different ways of computing these gates: for
each gate i, σ(Wxixt+Whiht+bi) (GRAN-3) or
σ(Wxixt +Whiht +Waiat−1 + bi) (GRAN-4).
The sum of these two terms comprised at. In this
model, the last average hidden state, a−1, was used
as the sentence embedding after dividing it by the
length of the sequence. In these models, we are
additionally keeping a running average of the em-
beddings that is being modified by the context at
every time step. In GRAN-4, this running average
is also considered when producing the contextual-
ized word embedding.

Lastly, we experimented with a fifth GRAN,
GRAN-5, in which we use two gates, calculated
by σ(Wxixt +Whiht + bi) for each gate i. The
first is applied to xt and the second is applied to ht.
The output of these gates is then summed. There-
fore GRAN-5 can be reduced to either word-
averaging or averaging LSTM states, depending
on the behavior of the gates. If the first gate
is all ones and the second all zeros throughout
the sequence, the model is equivalent to word-
averaging. Conversely, if the first gate is all ze-
ros and the second is all ones throughout the se-
quence, the model is equivalent to averaging the

2We tried a variant of this model without the gate. We ob-
tain at from f(Wxxt+Whht+b), where f is a nonlinearity,
tuned over tanh and ReLU. The performance of the model is
significantly worse than the GRAN in all experiments.

LSTM states. Further analysis of these models is
included in Section 4.

3.2 Training

We follow the training procedure of Wieting et al.
(2015) and Wieting et al. (2016b), described be-
low. The training data consists of a set S of phrase
or sentence pairs 〈s1, s2〉 from either the Para-
phrase Database (PPDB; Ganitkevitch et al., 2013)
or the aligned Wikipedia sentences (Coster and
Kauchak, 2011) where s1 and s2 are assumed to
be paraphrases. We optimize a margin-based loss:

min
Wc,Ww

1

|S|

( ∑

〈s1,s2〉∈S
max(0, δ − cos(g(s1), g(s2))

+ cos(g(s1), g(t1))) + max(0, δ − cos(g(s1), g(s2))

+ cos(g(s2), g(t2)))

)
+ λc ‖Wc‖2 + λw ‖Wwinitial −Ww‖2

(2)

where g is the model in use (e.g., AVG or LSTM),
δ is the margin, λc and λw are regularization
parameters, Wwinitial is the initial word embed-
ding matrix, and t1 and t2 are carefully-selected
negative examples taken from a mini-batch dur-
ing optimization. The intuition is that we want
the two phrases to be more similar to each other
(cos(g(s1), g(s2))) than either is to their respec-
tive negative examples t1 and t2, by a margin of at
least δ.

3.2.1 Selecting Negative Examples
To select t1 and t2 in Eq. (2), we simply choose the
most similar phrase in some set of phrases (other
than those in the given phrase pair). For simplicity
we use the mini-batch for this set, but it could be
a different set. That is, we choose t1 for a given
〈s1, s2〉 as follows:

t1 = argmax
t:〈t,·〉∈Sb\{〈s1,s2〉}

cos(g(s1), g(t))

where Sb ⊆ S is the current mini-batch. That is,
we want to choose a negative example ti that is
similar to si according to the current model. The
downside is that we may occasionally choose a
phrase ti that is actually a true paraphrase of si.

4 Experiments

Our experiments are designed to address the em-
pirical question posed by Wieting et al. (2016b):
why do LSTMs underperform AVG for transfer

2080



learning? In Sections 4.1.2-4.2, we make progress
on this question by presenting methods that bridge
the gap between the two models in the transfer set-
ting. We then apply these same techniques to im-
prove performance in the supervised setting, de-
scribed in Section 4.3. In both settings we also
evaluate our novel GRAN architecture, finding
it to consistently outperform both AVG and the
LSTM.

4.1 Transfer Learning
4.1.1 Datasets and Tasks
We train on large sets of noisy paraphrase pairs
and evaluate on a diverse set of 22 textual sim-
ilarity datasets, including all datasets from every
SemEval semantic textual similarity (STS) task
from 2012 to 2015. We also evaluate on the Sem-
Eval 2015 Twitter task (Xu et al., 2015) and the
SemEval 2014 SICK Semantic Relatedness task
(Marelli et al., 2014). Given two sentences, the
aim of the STS tasks is to predict their similar-
ity on a 0-5 scale, where 0 indicates the sentences
are on different topics and 5 indicates that they
are completely equivalent. We report the average
Pearson’s r over these 22 sentence similarity tasks.

Each STS task consists of 4-6 datasets covering
a wide variety of domains, including newswire,
tweets, glosses, machine translation outputs, web
forums, news headlines, image and video captions,
among others. Further details are provided in the
official task descriptions (Agirre et al., 2012, 2013,
2014, 2015).

4.1.2 Experiments with Data Sources
We first investigate how different sources of train-
ing data affect the results. We try two data
sources. The first is phrase pairs from the Para-
phrase Database (PPDB). PPDB comes in differ-
ent sizes (S, M, L, XL, XXL, and XXXL), where
each larger size subsumes all smaller ones. The
pairs in PPDB are sorted by a confidence mea-
sure and so the smaller sets contain higher preci-
sion paraphrases. PPDB is derived automatically
from naturally-occurring bilingual text, and ver-
sions of PPDB have been released for many lan-
guages without the need for any manual annota-
tion (Ganitkevitch and Callison-Burch, 2014).

The second source of data is a set of sen-
tence pairs automatically extracted from Simple
English Wikipedia and English Wikipedia arti-
cles by Coster and Kauchak (2011). This data
was extracted for developing text simplification

AVG LSTM LSTMAVG
PPDB 67.7 54.2 64.2
SimpWiki 68.4 59.3 67.5

Table 1: Test results on SemEval semantic textual
similarity datasets (Pearson’s r×100) when train-
ing on different sources of data: phrase pairs from
PPDB or simple-to-standard English Wikipedia
sentence pairs from Coster and Kauchak (2011).

systems, where each instance pairs a simple and
complex sentence representing approximately the
same information. Though the data was obtained
for simplification, we use it as a source of train-
ing data for learning paraphrastic sentence embed-
dings. The dataset, which we call SimpWiki, con-
sists of 167,689 sentence pairs.

To ensure a fair comparison, we select a sample
of pairs from PPDB XL such that the number of
tokens is approximately the same as the number
of tokens in the SimpWiki sentences.3

We use PARAGRAM-SL999 embed-
dings (Wieting et al., 2015) to initialize the word
embedding matrix (Ww) for all models. For all ex-
periments, we fix the mini-batch size to 100, and
λc to 0. We tune the margin δ over {0.4, 0.6, 0.8}
and λw over {10−4, 10−5, 10−6, 10−7, 10−8, 0}.
We train AVG for 7 epochs, and the LSTM for
3, since it converges much faster and does not
benefit from 7 epochs. For optimization we use
Adam (Kingma and Ba, 2015) with a learning rate
of 0.001. We use the 2016 STS tasks (Agirre et al.,
2016) for model selection, where we average the
Pearson’s r over its 5 datasets. We refer to this
type of model selection as test. For evaluation,
we report the average Pearson’s r over the 22
other sentence similarity tasks.

The results are shown in Table 1. We first note
that, when training on PPDB, we find the same
result as Wieting et al. (2016b): AVG outperforms
the LSTM by more than 13 points. However, when
training both on sentence pairs, the gap shrinks to
about 9 points. It appears that part of the inferior
performance for the LSTM in prior work was due
to training on phrase pairs rather than on sentence
pairs. The AVG model also benefits from train-
ing on sentences, but not nearly as much as the
LSTM.4

3The PPDB data consists of 1,341,188 phrase pairs and
contains 3 more tokens than the SimpWiki data.

4We experimented with adding EOS tags at the end of
training and test sentences, SOS tags at the start of train-

2081



Our hypothesis explaining this result is that in
PPDB, the phrase pairs are short fragments of text
which are not necessarily constituents or phrases
in any syntactic sense. Therefore, the sentences
in the STS test sets are quite different from the
fragments seen during training. We hypothesize
that while word-averaging is relatively unaffected
by this difference, the recurrent models are much
more sensitive to overall characteristics of the
word sequences, and the difference between train
and test matters much more.

These results also suggest that the SimpWiki
data, even though it was developed for text simpli-
fication, may be useful for other researchers work-
ing on semantic textual similarity tasks.

4.1.3 Experiments with LSTM Variations
We next compare LSTM and LSTMAVG. The lat-
ter consists of averaging the hidden vectors of the
LSTM rather than using the final hidden vector
as in prior work (Wieting et al., 2016b). We hy-
pothesize that the LSTM may put more empha-
sis on the words at the end of the sentence than
those at the beginning. By averaging the hidden
states, the impact of all words in the sequence is
better taken into account. Averaging also makes
the LSTM more like AVG, which we know to per-
form strongly in this setting.

The results on AVG and the LSTM models are
shown in Table 1. When training on PPDB, mov-
ing from LSTM to LSTMAVG improves perfor-
mance by 10 points, closing most of the gap with
AVG. We also find that LSTMAVG improves by
moving from PPDB to SimpWiki, though in both
cases it still lags behind AVG.

4.2 Experiments with Regularization

We next experiment with various forms of regu-
larization. Previous work (Wieting et al., 2016b,a)
only used L2 regularization. Wieting et al. (2016b)
also regularized the word embeddings back to
their initial values. Here we use L2 regularization

ing and test sentences, adding both, and adding neither. We
treated adding these tags as hyperparameters and tuned over
these four settings along with the other hyperparameters in
the original experiment. Interestingly, we found that adding
these tags, especially EOS, had a large effect on the LSTM
when training on SimpWiki, improving performance by 6
points. When training on PPDB, adding EOS tags only im-
proved performance by 1.6 points.

The addition of the tags had a smaller effect on LSTMAVG.
Adding EOS tags improved performance by 0.3 points on
SimpWiki and adding SOS tags on PPDB improved perfor-
mance by 0.9 points.

as well as several additional regularization meth-
ods we describe below.

We try two forms of dropout. The first is just
standard dropout (Srivastava et al., 2014) on the
word embeddings. The second is “word dropout”,
which drops out entire word embeddings with
some probability (Iyyer et al., 2015).

We also experiment with scrambling the inputs.
For a given mini-batch, we go through each sen-
tence pair and, with some probability, we shuf-
fle the words in each sentence in the pair. When
scrambling a sentence pair, we always shuffle both
sentences in the pair. We do this before selecting
negative examples for the mini-batch. The moti-
vation for scrambling is to make it more difficult
for the LSTM to memorize the sequences in the
training data, forcing it to focus more on the iden-
tities of the words and less on word order. Hence
it will be expected to behave more like the word
averaging model.5

We also experiment with combining scrambling
and dropout. In this setting, we tune over scram-
bling with either word dropout or dropout.

The settings for these experiments are largely
the same as those of the previous section with the
exception that we tune λw over a smaller set of
values: {10−5, 0}. When using L2 regulariza-
tion, we tune λc over {10−3, 10−4, 10−5, 10−6}.
When using dropout, we tune the dropout rate over
{0.2, 0.4, 0.6}. When using scrambling, we tune
the scrambling rate over {0.25, 0.5, 0.75}. We
also include a bidirectional model (“Bi”) for both
LSTMAVG and the GATED RECURRENT AVERAG-
ING NETWORK. We tune over two ways to com-
bine the forward and backward hidden states; the
first simply adds them together and the second
uses a single feedforward layer with a tanh ac-
tivation.

We try two approaches for model selection. The
first, test , is the same as was done in Section 4.1.2,
where we use the average Pearson’s r on the 5
2016 STS datasets. The second tunes based on
the average Pearson’s r of all 22 datasets in our
evaluation. We refer to this as oracle.

The results are shown in Table 2. They show
that dropping entire word embeddings and scram-

5We also tried some variations on scrambling that did not
yield significant improvements: scrambling after obtaining
the negative examples, partially scrambling by performing n
swaps where n comes from a Poisson distribution with a tun-
able λ, and scrambling individual sentences with some prob-
ability instead of always scrambling both in the pair.

2082



Model Regularization Oracle 2016 STS

AVG
none 68.5 68.4
dropout 68.4 68.3
word dropout 68.3 68.3

LSTM

none 60.6 59.3
L2 60.3 56.5
dropout 58.1 55.3
word dropout 66.2 65.3
scrambling 66.3 65.1
dropout, scrambling 68.4 68.4

LSTMAVG none 67.7 67.5dropout, scrambling 69.2 68.6
BiLSTMAVG dropout, scrambling 69.4 68.7

Table 2: Results on SemEval textual similarity
datasets (Pearson’s r × 100) when experimenting
with different regularization techniques.

Model Oracle STS 2016
GRAN (no reg.) 68.0 68.0
GRAN 69.5 68.9
GRAN-2 68.8 68.1
GRAN-3 69.0 67.2
GRAN-4 68.6 68.1
GRAN-5 66.1 64.8
BiGRAN 69.7 68.4

Table 3: Results on SemEval textual similarity
datasets (Pearson’s r × 100) for the GRAN ar-
chitectures. The first row, marked as (no reg.) is
the GRAN without any regularization. The other
rows show the result of the various GRAN models
using dropout and scrambling.

bling input sequences is very effective in improv-
ing the result of the LSTM, while neither type of
dropout improves AVG. Moreover, averaging the
hidden states of the LSTM is the most effective
modification to the LSTM in improving perfor-
mance. All of these modifications can be com-
bined to significantly improve the LSTM, finally
allowing it to overtake AVG.

In Table 3, we compare the various GRAN ar-
chitectures. We find that the GRAN provides a
small improvement over the best LSTM configu-
ration, possibly because of its similarity to AVG. It
also outperforms the other GRAN models, despite
being the simplest.

In Table 4, we show results on all individual
STS evaluation datasets after using STS 2016 for
model selection (unidirectional models only). The
LSTMAVG and GATED RECURRENT AVERAGING
NETWORK are more closely correlated in perfor-
mance, in terms of Spearman’s ρ and Pearson’r
r, than either is to AVG. But they do differ sig-
nificantly in some datasets, most notably in those
comparing machine translation output with its ref-

Dataset LSTMAVG AVG GRAN
MSRpar 49.0 45.9 47.7
MSRvid 84.3 85.1 85.2
SMT-eur 51.2 47.5 49.3
OnWN 71.5 71.2 71.5
SMT-news 68.0 58.2 58.7
STS 2012 Average 64.8 61.6 62.5
headline 77.3 76.9 76.1
OnWN 81.2 72.8 81.4
FNWN 53.2 50.2 55.6
SMT 40.7 38.0 40.3
STS 2013 Average 63.1 59.4 63.4
deft forum 56.6 55.6 55.7
deft news 78.0 78.5 77.1
headline 74.5 75.1 72.8
images 84.7 85.6 85.8
OnWN 84.9 81.4 85.1
tweet news 76.3 78.7 78.7
STS 2014 Average 75.8 75.8 75.9
answers-forums 71.8 70.6 73.1
answers-students 71.1 75.8 72.9
belief 75.3 76.8 78.0
headline 79.5 80.3 78.6
images 85.8 86.0 85.8
STS 2015 Average 76.7 77.9 77.7
2014 SICK 71.3 72.4 72.9
2015 Twitter 52.1 52.1 50.2

Table 4: Results on SemEval textual similarity
datasets (Pearson’s r × 100). The highest score in
each row is in boldface.

erence. Interestingly, both the LSTMAVG and
GATED RECURRENT AVERAGING NETWORK sig-
nificantly outperform AVG in the datasets focused
on comparing glosses like OnWN and FNWN.
Upon examination, we found that these datasets,
especially 2013 OnWN, contain examples of low
similarity with high word overlap. For exam-
ple, the pair 〈the act of preserving or protect-
ing something., the act of decreasing or reducing
something.〉 from 2013 OnWN has a gold similar-
ity score of 0.4. It appears that AVG was fooled
by the high amount of word overlap in such pairs,
while the other two models were better able to rec-
ognize the semantic differences.

4.3 Supervised Text Similarity

We also investigate if these techniques can im-
prove LSTM performance on supervised semantic
textual similarity tasks. We evaluate on two super-
vised datasets. For the first, we start with the 20
SemEval STS datasets from 2012-2015 and then
use 40% of each dataset for training, 10% for val-
idation, and the remaining 50% for testing. There
are 4,481 examples in training, 1,207 in validation,
and 6,060 in the test set. The second is the SICK
2014 dataset, using its standard training, valida-
tion, and test sets. There are 4,500 sentence pairs

2083



in the training set, 500 in the development set, and
4,927 in the test set. The SICK task is an eas-
ier learning problem since the training examples
are all drawn from the same distribution, and they
are mostly shorter and use simpler language. As
these are supervised tasks, the sentence pairs in the
training set contain manually-annotated semantic
similarity scores.

We minimize the loss function6 from Tai et al.
(2015). Given a score for a sentence pair in the
range [1,K], where K is an integer, with sentence
representations hL and hR, and model parameters
θ, they first compute:

h× = hL � hR, h+ = |hL − hR|,
hs = σ

(
W (×)h× +W (+)h+ + b(h)

)
,

p̂θ = softmax
(
W (p)hs + b

(p)
)
,

ŷ = rT p̂θ,

where rT = [1 2 . . . K]. They then define a
sparse target distribution p that satisfies y = rT p:

pi =





y − byc, i = byc+ 1
byc − y + 1, i = byc
0 otherwise

for 1 ≤ i ≤ K. Then they use the following loss,
the regularized KL-divergence between p and p̂θ:

J(θ) =
1

m

m∑

k=1

KL
(
p(k)

∥∥∥ p̂(k)θ
)
,

where m is the number of training pairs.
We experiment with the LSTM, LSTMAVG,

and AVG models with dropout, word dropout, and
scrambling tuning over the same hyperparameter
as in Section 4.2. We again regularize the word
embeddings back to their initial state, tuning λw
over {10−5, 0}. We used the validation set for
each respective dataset for model selection.

The results are shown in Table 5. The GATED
RECURRENT AVERAGING NETWORK has the best
performance on both datasets. Dropout helps the
word-averaging model in the STS task, unlike in
the transfer learning setting. The LSTM bene-
fits slightly from dropout, scrambling, and aver-
aging on their own individually with the excep-
tion of word dropout on both datasets and aver-
aging on the SICK dataset. However, when com-
bined, these modifications are able to significantly

6This objective function has been shown to perform very
strongly on text similarity tasks, significantly better than
squared or absolute error.

Model Regularization STS SICK Avg.

AVG

none 79.2 85.2 82.2
dropout 80.7 84.5 82.6
word dropout 79.3 81.8 80.6
none 68.4 80.9 74.7
dropout 69.6 81.3 75.5

LSTM word dropout 68.0 76.4 72.2
scrambling 74.2 84.4 79.3
dropout, scrambling 75.0 84.2 79.6

LSTMAVG

none 69.0 79.5 74.3
dropout 69.2 79.4 74.3
word dropout 65.6 76.1 70.9
scrambling 76.5 83.2 79.9
dropout, scrambling 76.5 84.0 80.3

GRAN

none 79.7 85.2 82.5
dropout 79.7 84.6 82.2
word dropout 77.3 83.0 80.2
scrambling 81.4 85.3 83.4
dropout, scrambling 81.6 85.1 83.4

Table 5: Results from supervised training on
the STS and SICK datasets (Pearson’s r × 100).
The last column is the average result on the two
datasets.

Model STS SICK Avg.
GRAN 81.6 85.3 83.5
GRAN-2 77.4 85.1 81.3
GRAN-3 81.3 85.4 83.4
GRAN-4 80.1 85.5 82.8
GRAN-5 70.9 83.0 77.0

Table 6: Results from supervised training on the
STS and SICK datasets (Pearson’s r × 100) for
the GRAN architectures. The last column is the
average result on the two datasets.

improve the performance of the LSTM, bringing
it much closer in performance to AVG. This ex-
periment indicates that these modifications when
training LSTMs are beneficial outside the trans-
fer learning setting, and can potentially be used to
improve performance for the broad range of prob-
lems that use LSTMs to model sentences.

In Table 6 we compare the various GRAN ar-
chitectures under the same settings as the previous
experiment. We find that the GRAN still has the
best overall performance.

We also experiment with initializing the super-
vised models using our pretrained sentence model
parameters, for the AVG model (no regularization),
LSTMAVG (dropout, scrambling), and GATED
RECURRENT AVERAGING NETWORK (dropout,
scrambling) models from Table 2 and Table 3. We
both initialize and then regularize back to these
initial values, referring to this setting as “univer-
sal”.7

7In these experiments, we tuned λw over
{10, 1, 10−1, 10−2, 10−3, 10−4, 10−5, 10−6, 10−7, 10−8, 0}

2084



# Sentence 1 Sentence 2 LAVG AVG Gold
1 the lamb is looking at the camera. a cat looking at the camera. 3.42 4.13 0.8
2 he also said shockey is “living the dream

life of a new york athlete.
“jeremy’s a good guy,” barber said, adding:“jeremy is
living the dream life of the new york athlete.

3.55 4.22 2.75

3 bloomberg chips in a billion bloomberg gives $1.1 b to university 3.99 3.04 4.0
4 in other regions, the sharia is imposed. in other areas, sharia law is being introduced by force. 4.44 3.72 4.75
5 three men in suits sitting at a table. two women in the kitchen looking at a object. 3.33 2.79 0.0
6 we never got out of it in the first place! where does the money come from in the first place? 4.00 3.33 0.8
7 two birds interacting in the grass. two dogs play with each other outdoors. 3.44 2.81 0.2

Table 7: Illustrative sentence pairs from the STS datasets showing errors made by LSTMAVG and
AVG. The last three columns show the gold similarity score, the similarity score of LSTMAVG, and the
similarity score of AVG. Boldface indicates smaller error compared to gold scores.

Model Regularization STS SICK

AVG
dropout 80.7 84.5
dropout, universal 82.9 85.6

LSTMAVG dropout, scrambling 76.5 84.0dropout, scrambling, universal 81.3 85.2

GRAN dropout, scrambling 81.6 85.1dropout, scrambling, universal 82.7 86.0

Table 8: Impact of initializing and regularizing
toward universal models (Pearson’s r×100) in su-
pervised training.

The results are shown in Table 8. Initializ-
ing and regularizing to the pretrained models sig-
nificantly improves the performance for all three
models, justifying our claim that these models
serve a dual purpose: they can be used a black box
semantic similarity function, and they possess rich
knowledge that can be used to improve the perfor-
mance of downstream tasks.

5 Analysis

5.1 Error Analysis

We analyze the predictions of AVG and the recur-
rent networks, represented by LSTMAVG, on the
20 STS datasets. We choose LSTMAVG as it cor-
relates slightly less strongly with AVG than the
GRAN on the results over all SemEval datasets
used for evaluation. We scale the models’ cosine
similarities to lie within [0, 5], then compare the
predicted similarities of LSTMAVG and AVG to the
gold similarities. We analyzed instances in which
each model would tend to overestimate or under-
estimate the gold similarity relative to the other.
These are illustrated in Table 7.

We find that AVG tends to overestimate the se-
mantic similarity of a sentence pair, relative to
LSTMAVG, when the two sentences have a lot of

and λc over {10, 1, 10−1, 10−2, 10−3, 10−4, 10−5, 10−6, 0}.

word or synonym overlap, but have either impor-
tant differences in key semantic roles or where one
sentence has significantly more content than the
other. These phenomena are shown in examples 1
and 2 in Table 7. Conversely, AVG tends to under-
estimate similarity when there are one-word-to-
multiword paraphrases between the two sentences
as shown in examples 3 and 4.

LSTMAVG tends to overestimate similarity
when the two inputs have similar sequences of
syntactic categories, but the meanings of the sen-
tences are different (examples 5, 6, and 7). In-
stances of LSTMAVG underestimating the similar-
ity relative to AVG are relatively rare, and those
that we found did not have any systematic patterns.

5.2 GRAN Gate Analysis

We also investigate what is learned by the gating
function of the GATED RECURRENT AVERAGING
NETWORK. We are interested to see whether its
estimates of importance correlate with those of tra-
ditional syntactic and (shallow) semantic analysis.

We use the oracle trained GATED RECURRENT
AVERAGING NETWORK from Table 3 and cal-
culate the L1 norm of the gate after embedding
10,000 sentences from English Wikipedia.8 We
also automatically tag and parse these sentences
using the Stanford dependency parser (Manning
et al., 2014). We then compute the average gate
L1 norms for particular part-of-speech tags, de-
pendency arc labels, and their conjunction.

Table 9 shows the highest/lowest average norm
tags and dependency labels. The network prefers
nouns, especially proper nouns, as well as cardinal
numbers, which is sensible as these are among the
most discriminative features of a sentence.

Analyzing the dependency relations, we find

8We selected only sentences of less than or equal to 15
tokens to ensure more accurate parsing.

2085



POS Dep. Label
top 10 bot. 10 top 10 bot. 10
NNP TO number possessive
NNPS WDT nn cop
CD POS num det
NNS DT acomp auxpass
VBG WP appos prep
NN IN pobj cc
JJ CC vmod mark
UH PRP dobj aux
VBN EX amod expl
JJS WRB conj neg

Table 9: POS tags and dependency labels with
highest and lowest average GATED RECURRENT
AVERAGING NETWORK gate L1 norms. The lists
are ordered from highest norm to lowest in the top
10 columns, and lowest to highest in the bottom
10 columns.

Dep. Label Weight
xcomp 170.6
acomp 167.1
root 157.4
amod 143.1
advmod 121.6

Table 10: Average L1 norms for adjectives (JJ)
with selected dependency labels.

that nouns in the object position tend to have
higher weight than nouns in the subject position.
This may relate to topic and focus; the object may
be more likely to be the “new” information related
by the sentence, which would then make it more
likely to be matched by the other sentence in the
paraphrase pair.

We find that the weights of adjectives depend
on their position in the sentence, as shown in Ta-
ble 10. The highest norms appear when an ad-
jective is an xcomp, acomp, or root; this typically
means it is residing in an object-like position in its
clause. Adjectives that modify a noun (amod) have

Dep. Label Weight
pcomp 190.0
amod 178.3
xcomp 176.8
vmod 170.6
root 161.8
auxpass 125.4
prep 121.2

Table 11: Average L1 norms for words with the
tag VBG with selected dependency labels.

medium weight, and those that modify another ad-
jective or verb (advmod) have low weight.

Lastly, we analyze words tagged as VBG, a
highly ambiguous tag that can serve many syn-
tactic roles in a sentence. As shown in Table 11,
we find that when they are used to modify a
noun (amod) or in the object position of a clause
(xcomp, pcomp) they have high weight. Medium
weight appears when used in verb phrases (root,
vmod) and low weight when used as prepositions
or auxiliary verbs (prep, auxpass).

6 Conclusion

We showed how to modify and regularize LSTMs
to improve their performance for learning para-
phrastic sentence embeddings in both transfer and
supervised settings. We also introduced a new re-
current network, the GATED RECURRENT AVER-
AGING NETWORK, that improves upon both AVG
and LSTMs for these tasks, and we release our
code and trained models.

Furthermore, we analyzed the different errors
produced by AVG and the recurrent methods and
found that the recurrent methods were learning
composition that wasn’t being captured by AVG.
We also investigated the GRAN in order to better
understand the compositional phenomena it was
learning by analyzing the L1 norm of its gate over
various inputs.

Future work will explore additional data
sources, including from aligning different trans-
lations of novels (Barzilay and McKeown, 2001),
aligning new articles of the same topic (Dolan
et al., 2004), or even possibly using machine trans-
lation systems to translate bilingual text into para-
phrastic sentence pairs. Our new techniques, com-
bined with the promise of new data sources, of-
fer a great deal of potential for improved universal
paraphrastic sentence embeddings.

Acknowledgments

We thank the anonymous reviewers for their valu-
able comments. This research used resources
of the Argonne Leadership Computing Facility,
which is a DOE Office of Science User Facility
supported under Contract DE-AC02-06CH11357.
We thank the developers of Theano (Theano De-
velopment Team, 2016) and NVIDIA Corporation
for donating GPUs used in this research.

2086



References
Eneko Agirre, Carmen Banea, Claire Cardie, Daniel

Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei
Guo, Inigo Lopez-Gazpio, Montse Maritxalar, Rada
Mihalcea, German Rigau, Larraitz Uria, and Janyce
Wiebe. 2015. SemEval-2015 task 2: Semantic tex-
tual similarity, English, Spanish and pilot on inter-
pretability. In Proceedings of the 9th International
Workshop on Semantic Evaluation (SemEval 2015).

Eneko Agirre, Carmen Banea, Claire Cardie, Daniel
Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei
Guo, Rada Mihalcea, German Rigau, and Janyce
Wiebe. 2014. SemEval-2014 task 10: Multilingual
semantic textual similarity. In Proceedings of the
8th International Workshop on Semantic Evaluation
(SemEval 2014).

Eneko Agirre, Carmen Banea, Daniel Cer, Mona Diab,
Aitor Gonzalez-Agirre, Rada Mihalcea, German
Rigau, and Janyce Wiebe. 2016. Semeval-2016
task 1: Semantic textual similarity, monolingual and
cross-lingual evaluation. Proceedings of SemEval
pages 497–511.

Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *SEM 2013 shared
task: Semantic textual similarity. In Second Joint
Conference on Lexical and Computational Seman-
tics (*SEM), Volume 1: Proceedings of the Main
Conference and the Shared Task: Semantic Textual
Similarity.

Eneko Agirre, Mona Diab, Daniel Cer, and Aitor
Gonzalez-Agirre. 2012. SemEval-2012 task 6: A
pilot on semantic textual similarity. In Proceedings
of the First Joint Conference on Lexical and Com-
putational Semantics-Volume 1: Proceedings of the
main conference and the shared task, and Volume
2: Proceedings of the Sixth International Workshop
on Semantic Evaluation. Association for Computa-
tional Linguistics.

Sanjeev Arora, Yingyu Liang, and Tengyu Ma. 2017.
A simple but tough-to-beat baseline for sentence em-
beddings. In Proceedings of the International Con-
ference on Learning Representations.

Regina Barzilay and Kathleen R McKeown. 2001. Ex-
tracting paraphrases from a parallel corpus. In Pro-
ceedings of the 39th annual meeting on Association
for Computational Linguistics.

William Blacoe and Mirella Lapata. 2012. A com-
parison of vector-based representations for seman-
tic composition. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning.

William Coster and David Kauchak. 2011. Simple en-
glish wikipedia: a new text simplification task. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies: short papers-Volume 2.

Bill Dolan, Chris Quirk, and Chris Brockett. 2004.
Unsupervised construction of large paraphrase cor-
pora: Exploiting massively parallel news sources. In
Proceedings of the 20th international conference on
Computational Linguistics.

Juri Ganitkevitch and Chris Callison-Burch. 2014. The
multilingual paraphrase database. In Proceedings of
the Ninth International Conference on Language Re-
sources and Evaluation (LREC-2014).

Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2013. PPDB: The Paraphrase
Database. In Proceedings of HLT-NAACL.

Felix A. Gers, Nicol N. Schraudolph, and Jürgen
Schmidhuber. 2003. Learning precise timing with
LSTM recurrent networks. The Journal of Machine
Learning Research 3.

Felix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.
Learning distributed representations of sentences
from unlabelled data. In Proceedings of the 2016
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies.

Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long
short-term memory. Neural computation 9(8).

Baotian Hu, Zhengdong Lu, Hang Li, and Qingcai
Chen. 2014. Convolutional neural network archi-
tectures for matching natural language sentences.
In Advances in Neural Information Processing Sys-
tems.

Ozan İrsoy and Claire Cardie. 2014. Deep recursive
neural networks for compositionality in language.
In Advances in Neural Information Processing Sys-
tems.

Mohit Iyyer, Varun Manjunatha, Jordan Boyd-Graber,
and Hal Daumé III. 2015. Deep unordered compo-
sition rivals syntactic methods for text classification.
In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the
7th International Joint Conference on Natural Lan-
guage Processing (Volume 1: Long Papers).

Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. 2014. A convolutional neural network for
modelling sentences. In Proceedings of the 52nd
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers).

Yoon Kim. 2014. Convolutional neural networks for
sentence classification. In Proceedings of the 2014
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP).

Diederik P. Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. In Proceedings
of International Conference on Learning Represen-
tations.

2087



Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,
Richard Zemel, Raquel Urtasun, Antonio Torralba,
and Sanja Fidler. 2015. Skip-thought vectors. In Ad-
vances in Neural Information Processing Systems.

Quoc V. Le and Tomas Mikolov. 2014. Distributed rep-
resentations of sentences and documents. In Pro-
ceedings of the 31st International Conference on
Machine Learning.

Wang Ling, Chris Dyer, Alan W Black, Isabel Tran-
coso, Ramon Fermandez, Silvio Amir, Luis Marujo,
and Tiago Luis. 2015. Finding function in form:
Compositional character models for open vocabu-
lary word representation. In Proceedings of the 2015
Conference on Empirical Methods in Natural Lan-
guage Processing.

Pengfei Liu, Xipeng Qiu, Xinchi Chen, Shiyu Wu, and
Xuanjing Huang. 2015. Multi-timescale long short-
term memory neural network for modelling sen-
tences and documents. In Proceedings of the 2015
Conference on Empirical Methods in Natural Lan-
guage Processing.

Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David Mc-
Closky. 2014. The Stanford CoreNLP natural lan-
guage processing toolkit. In Proceedings of 52nd
Annual Meeting of the Association for Computa-
tional Linguistics: System Demonstrations.

Marco Marelli, Luisa Bentivogli, Marco Baroni, Raf-
faella Bernardi, Stefano Menini, and Roberto Zam-
parelli. 2014. SemEval-2014 task 1: Evaluation of
compositional distributional semantic models on full
sentences through semantic relatedness and textual
entailment. In Proceedings of the 8th International
Workshop on Semantic Evaluation (SemEval 2014).

Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
the 46th Annual Meeting of the Association for Com-
putational Linguistics.

Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence 34(8).

Matteo Pagliardini, Prakhar Gupta, and Martin Jaggi.
2017. Unsupervised Learning of Sentence Embed-
dings using Compositional n-Gram Features. arXiv
preprint arXiv:1703.02507 .

Nghia The Pham, Germán Kruszewski, Angeliki
Lazaridou, and Marco Baroni. 2015. Jointly opti-
mizing word representations for lexical and senten-
tial tasks with the c-phrase model. In Proceedings
of the 53rd Annual Meeting of the Association for
Computational Linguistics and the 7th International
Joint Conference on Natural Language Processing
(Volume 1: Long Papers).

Richard Socher, Eric H. Huang, Jeffrey Pennington,
Andrew Y. Ng, and Christopher D. Manning. 2011.

Dynamic pooling and unfolding recursive autoen-
coders for paraphrase detection. In Advances in
Neural Information Processing Systems.

Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic composi-
tionality through recursive matrix-vector spaces. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning.

Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. 2013. Recursive deep models
for semantic compositionality over a sentiment tree-
bank. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Process-
ing.

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: A simple way to prevent neural networks
from overfitting. The Journal of Machine Learning
Research 15(1).

Kai Sheng Tai, Richard Socher, and Christopher D.
Manning. 2015. Improved semantic representations
from tree-structured long short-term memory net-
works. In Proceedings of the 53rd Annual Meet-
ing of the Association for Computational Linguistics
and the 7th International Joint Conference on Natu-
ral Language Processing (Volume 1: Long Papers).

Theano Development Team. 2016. Theano: A
Python framework for fast computation of mathe-
matical expressions. arXiv e-prints abs/1605.02688.
http://arxiv.org/abs/1605.02688.

John Wieting, Mohit Bansal, Kevin Gimpel, and Karen
Livescu. 2016a. Charagram: Embedding words and
sentences via character n-grams. In Proceedings of
the 2016 Conference on Empirical Methods in Nat-
ural Language Processing.

John Wieting, Mohit Bansal, Kevin Gimpel, and Karen
Livescu. 2016b. Towards universal paraphrastic
sentence embeddings. In Proceedings of the Inter-
national Conference on Learning Representations.

John Wieting, Mohit Bansal, Kevin Gimpel, Karen
Livescu, and Dan Roth. 2015. From paraphrase
database to compositional paraphrase model and
back. Transactions of the ACL (TACL) .

Wei Xu, Chris Callison-Burch, and William B Dolan.
2015. SemEval-2015 task 1: Paraphrase and seman-
tic similarity in Twitter (PIT). In Proceedings of the
9th International Workshop on Semantic Evaluation
(SemEval).

2088


	Revisiting Recurrent Networks for Paraphrastic Sentence Embeddings

