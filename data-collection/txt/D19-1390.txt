



























Improving Latent Alignment in Text Summarization by Generalizing the Pointer Generator


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 3762–3773,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

3762

Improving Latent Alignment in Text Summarization by Generalizing the
Pointer Generator

Xiaoyu Shen1,2∗, Yang Zhao3, Hui Su4 and Dietrich Klakow1
1Spoken Language Systems (LSV), Saarland University, Germany

2Max Planck Institute for Informatics, Saarland Informatics Campus, Germany
3The University of Tokyo, Japan

4Pattern Recognition Center, Wechat AI, Tencent Inc, China

Abstract

Pointer Generators have been the de facto
standard for modern summarization systems.
However, this architecture faces two major
drawbacks: Firstly, the pointer is limited to
copying the exact words while ignoring possi-
ble inflections or abstractions, which restricts
its power of capturing richer latent align-
ment. Secondly, the copy mechanism results
in a strong bias towards extractive generations,
where most sentences are produced by sim-
ply copying from the source text. In this pa-
per, we address these problems by allowing
the model to “edit” pointed tokens instead of
always hard copying them. The editing is per-
formed by transforming the pointed word vec-
tor into a target space with a learned relation
embedding. On three large-scale summariza-
tion dataset, we show the model is able to (1)
capture more latent alignment relations than
exact word matches, (2) improve word align-
ment accuracy, allowing for better model inter-
pretation and controlling, (3) generate higher-
quality summaries validated by both qualita-
tive and quantitative evaluations and (4) bring
more abstraction to the generated summaries.1

1 Introduction

Modern state-of-the-art (SOTA) summarization
models are built upon the pointer generator archi-
tecture (See et al., 2017). At each decoding step,
the model generates a sentinel to decide whether to
sample words based on the neural attention (gen-
eration mode), or directly copy from an aligned
source context (point mode) (Gu et al., 2016; Mer-
ity et al., 2017; Yang et al., 2017). Though outper-
forming the vanilla attention models, the pointer
generator only captures exact word matches. As
shown in Fig. 1, for abstractive summarization,

∗Correspondence to xshen@mpi-inf.mpg.de
1The source code is available at https://github.

com/chin-gyou/generalized-PG.

Figure 1: Alignment visualization of our model when
decoding “closes”. Posterior alignment is more ac-
curate for model interpretation. In contrast, the prior
alignment probability is spared to “announced” and
“closure”, which can be manually controlled to gener-
ate desired summaries. Decoded samples are shown
when aligned to “announced” and “closure” respec-
tively. Highlighted source words are those that can be
directly aligned to a target token in the gold summary.

there exists a large number of syntactic inflections
(escalated → escalates) or semantic transforma-
tions (military campaign → war), where the tar-
get word also has an explicit grounding in the
source context but changes its surface. In stan-
dard pointer generators, these words are not cov-
ered by the point mode. This largely restricts the
application of the pointer generator, especially on
highly abstractive dataset where only a few words
are exactly copied. Moreover, the hard copy oper-
ation biases the model towards extractive summa-
rization, which is undesirable for generating more
human-like summaries (Kryściński et al., 2018).

To solve this problem, we propose Generalized
Pointer Generator (GPG) which replaces the hard
copy component with a more general soft “edit-
ing” function. We do this by learning a relation
embedding to transform the pointed word into a



3763

target embedding. For example, when decoding
“closes” in Figure 1, the model should first point
to “closure” in the source, predict a relation to
be applied (noun → third person singular verb),
then transform “closure” into “closes” by applying
the relation transformation. The generalized point
mode is encouraged to capture such latent align-
ment which cannot be identified by the standard
pointer generator.

This improved alignment modelling is intrigu-
ing in that (a) people can better control the gener-
ation by manipulating the alignment trajectory, (b)
posterior alignment can be inferred by Bayes’ the-
orem (Deng et al., 2018; Shankar and Sarawagi,
2019) to provide a better tool for interpretation2

and finally (c) explicitly capturing the alignment
relation should improve generation performance.
(Figure 1 shows an example of how latent align-
ment can improve the controllability and interpre-
tation. Pointer generators fail to model such align-
ment relations that are not exact copies.) To elim-
inate the OOV problem, we utilize the byte-pair-
encoding (BPE) segmentation (Sennrich et al.,
2016) to split rare words into sub-units, which has
very few applications in summarization so far (Fan
et al., 2018; Kiyono et al., 2018), though being
a common technique in machine translation (Wu
et al., 2016; Vaswani et al., 2017; Gehring et al.,
2017).

Our experiments are conducted on three sum-
marization datasets: CNN/dailymail (Hermann
et al., 2015), English Gigaword (Rush et al., 2015)
and XSum (Narayan et al., 2018) (a newly col-
lected corpus for extreme summarization). We fur-
ther perform human evaluation and examine the
word alignment accuracy on the manually anno-
tated DUC 2004 dataset. Overall we find our
model provides the following benefits:

1. It can capture richer latent alignment and im-
prove the word alignment accuracy, enabling
better controllability and interpretation.

2. The generated summaries are more faithful
to the source context because of the explicit
alignment grounding.

2The induced alignment offers useful annotations for peo-
ple to identify the source correspondence for each target
word. News editors can post-edit machine-generated sum-
maries more efficiently with such annotation. For summary
readers, it also helps them track back the source context when
they are interested in some specific details. Derivation for in-
ducing the posterior alignment is in the appendix A.1.

3. It improves the abstraction of generations be-
cause our model allows editing the pointed
token instead of always copying exactly.

In the next section, we will first go over the
background, then introduce our model and finally
present the experiment results and conclusion.

2 Background

Let X,Y denote a source-target pair where
X corresponds to a sequence of words
x1, x2, . . . , xn and Y is its corresponding
summary y1, y2, . . . , ym. In this section, we
introduce two baseline models for automatically
generating Y from X:

2.1 Seq2seq with Attention

In a seq2seq model, each source token xi is en-
coded into a vector hi. At each decoding step t,
the decoder computes an attention distribution at
over the encoded vectors based on the current hid-
den state dt (Bahdanau et al., 2015):

at = softmax(f(hi, dt)) (1)

f is a score function to measure the similarity be-
tween hi and dt. The context vector ct and the
probability of next token are computed as below.

ct =
∑
i

hiat,i

y∗t = [dt ◦ ct]L
pvocab = softmax(y∗tW

T )

(2)

◦ means concatenation and L,W are trainable
parameters. We tie the parameters of W and
the word embedding matrix as in Press and Wolf
(2017); Inan et al. (2017). Namely, a target vector
y∗t is predicted, words having a higher inner prod-
uct with y∗t will have a higher probability.

2.2 Pointer Generator

The pointer generator extends the seq2seq model
to support copying source words (Vinyals et al.,
2015). At each time step t, the model first com-
putes a generation probability pgen ∈ [0, 1] by:

pgen = σ(MLPg([dt ◦ ct])) (3)

σ is a sigmoid function and MLPg is a learnable
multi-layer perceptron. pgen is the probability of
enabling the generation mode instead of the point



3764

mode. In the generation mode, the model com-
putes the probability over the whole vocabulary as
in Eq. 2. In the point mode, the model computes
which source word to copy based on the attention
distribution at from Eq.1. The final probability is
marginalized over at,i:

p(yt) = pgenpvocab(yt) + (1− pgen)
∑
i

at,iδ(yt|xi)

δ(yt|xi) =

{
1, if yt = xi.
0, otherwise.

(4)

If we know exactly from which mode each word
comes from, e.g., by assuming all co-occurred
words are copied, then the marginalization can be
omitted (Gulcehre et al., 2016; Wiseman et al.,
2017), but normally pgen is treated as a latent vari-
able (Gu et al., 2016; See et al., 2017).

3 Generalized Pointer Generator (GPG)

As seen in Eq .4, δ(yt|xi) is a 0-1 event that is
only turned on when yt is exactly the same word
as xi. This restricts the expressiveness of the point
mode, preventing it from paying attention to in-
flections, POS transitions or paraphrases. This
section explains how we generalize pointer net-
works to cover these conditions.

Redefine δ(yt|xi): We extend δ(yt|xi) by
defining it as a smooth probability distribution
over the whole vocabulary. It allows the pointer
to edit xi to a different word yt instead of simply
copying it. Following Eq. 2, we derive δ(yt|xi) by
first predicting a target embedding y∗t,i, then apply-
ing the softmax. The difference is that we derive
y∗t,i as the summation of the pointed word embed-
ding −→xi and a relation embedding r(dt, ht):

y∗t,i = r(dt, hi) +
−→xi

δ(yt|xi) = softmax(y∗t,iW T )
(5)

−→xi denotes the embedding of the word xi.
r(dt, ht) can be any function conditioning on dt
and ht, which we parameterize with a multi-
layer-perceptron in our experiments. The com-
putation of y∗t,i is similar to the classical TransE
model (Bordes et al., 2013) where an entity vec-
tor is added by a relation embedding to translate
into the target entity. The intuition is straight-
forward: After pointing to xt, humans usually
first decide which relation should be applied (in-
flection, hypernym, synonym, etc) based on the
context [dt, hi], then transform xi to the proper

target word yt. Using addition transformation is
backed by the observation that vector differences
often reflect meaningful word analogies (Mikolov
et al., 2013; Pennington et al., 2014) (“man” −
“king” ≈ “woman” − “queen”) and they are
effective at encoding a great amount of word
relations like hypernym, meronym and morpho-
logical changes (Vylomova et al., 2016; Hakami
et al., 2018; Allen and Hospedales, 2019). These
word relations reflect most alignment conditions
in text summarization. For example, humans of-
ten change the source word to its hypernym (boy
→ child), to make it more specific (person→man)
or apply morphological transformations (liked →
like). Therefore, we assume δ(yt|xi) can be well
modelled by first predicting a relation embedding
to be applied, then added to −→xi . If xi should
be exactly copied like in standard pointer genera-
tors, the relation embedding is a zero vector mean-
ing an identity transition. We also tried apply-
ing more complex transitions to −→xi like diagonal
mapping (Trouillon et al., 2016), but did not ob-
serve improvements. Another option is to estimate
δ(yt|xi) directly from (dt, hi) by an MLP regard-
less of −→xi . However, this leads to poor alignment
and performance drop because yt is not explic-
itly grounded on xi3. A comparison of different
choices can be found in the appendix A.4. In this
paper, we stick to Eq. 5 to compute δ(yt|xi).

Estimate Marginal Likelihood: Putting Eq. 5
back to Eq. 4, the exact marginal likelihood is
too expensive for training. The complexity grows
linearly with the source text length n and each
computation of δ(yt|xi) requires a separate soft-
max operation. One option is to approximate it
by sampling like in hard attention models (Xu
et al., 2015; Deng et al., 2017), but the training
becomes challenging due to the non-differentiable
sampling process. In our work, we take an alter-
native strategy of marginalizing only over k most
likely aligned source words. This top-k approx-
imation is widely adopted when the target distri-
bution is expected to be sparse and only a few
modes dominate (Britz et al., 2017; Ke et al., 2018;
Shankar et al., 2018). We believe this is a valid as-
sumption in text summarization since most source

3hi can contain context information from surrounding
words and thus not necessarily relates to word xi. It is part of
the reason that neural attention has a poor alignment (Koehn
and Knowles, 2017). Making it grounded on xi improves
alignment and performance is also observed in machine trans-
lation (Nguyen and Chiang, 2018; Kuang et al., 2018)



3765

Figure 2: Architecture of the generalized pointer. The same encoder is applied to encode the source and target.
When decoding “closes”, we first find top-k source positions with the most similar encoded state. For each position,
the decoding probability is computed by adding its word embedding and a predicted relation embedding.

tokens have a vanishingly small probability to be
transferred into a target word.

For each target word, how to determine the
k most likely aligned source words is cru-
cial. An ideal system should always include
the gold aligned source word in the top-k se-
lections. We tried several methods and find the
best performance is achieved when encoding each
source/target token into a vector, then choosing the
k source words that are closest to the target word
in the encoded vector space. The closeness is mea-
sured by the vector inner product4. The encoded
vector space serves like a contextualized word em-
bedding (McCann et al., 2017; Peters et al., 2018).
Intuitively if a target word can be aligned to a
source word, they should have similar semantic
meaning and surrounding context thus should have
similar contextualized word embeddings. The new
objective is then defined as in Eq. 6:

p(yt) = pgenpvocab(yt) + (1− pgen)ppoint(yt)

ppoint(yt) =
∑
i

at,iδ(yt|xi)

≈
∑

i;hTi e(yt)∈TopK

at,iδ(yt|xi)
(6)

e(yt) is the encoded vector for yt. The marginal-
ization is performed only over the k chosen source
words. Eq. 6 is a lower bound of the data likeli-
hood because it only marginalizes over a subset of
X . In general a larger k can tighten the bound to
get a more accurate estimation and we analyze the

4We compared several strategies for choosing the top-k
words and report it in Appendix A.5. Note that the top-k
approximation is only used for training, so we can spot the
whole target text to decide top-k candidates.

effect of k in Section 5.2. Note that the only extra
parameters introduced by our model are the multi-
layer-perceptron to compute the relation embed-
ding r(dt, hi). The marginalization in Eq. 6 can
also be efficiently parallelized. An illustration of
the generalized pointer is in Figure 2.

4 Related Work

Neural attention models (Bahdanau et al., 2015)
with the seq2seq architecture (Sutskever et al.,
2014) have achieved impressive results in text
summarization tasks. However, the attention vec-
tor comes from a weighted sum of source in-
formation and does not model the source-target
alignment in a probabilistic sense. This makes
it difficult to interpret or control model genera-
tions through the attention mechanism. In prac-
tice, people do find the attention vector is often
blurred and suffers from poor alignment (Koehn
and Knowles, 2017; Kiyono et al., 2018; Jain and
Wallace, 2019). Hard alignment models, on the
other hand, explicitly models the alignment rela-
tion between each source-target pair. Though the-
oretically sound, hard alignment models are hard
to train. Exact marginalization is only feasible for
data with limited length (Yu et al., 2016; Aharoni
and Goldberg, 2017; Deng et al., 2018; Backes
et al., 2018), or by assuming a simple copy gen-
eration process (Vinyals et al., 2015; Gu et al.,
2016; See et al., 2017). Our model can be viewed
as a combination of soft attention and hard align-
ment, where a simple top-k approximation is used
to train the alignment part (Shankar et al., 2018;
Shankar and Sarawagi, 2019). The hard align-
ment generation probability is designed as a re-
lation summation operation to better fit the sum-



3766

marization task. In this way, the generalized copy
mode acts as a hard alignment component to cap-
ture the direct word-to-word transitions. On the
contrary, the generation mode is a standard soft-
attention structure to only model words that are
purely functional, or need fusion, high-level in-
ference and can be hardly aligned to any specific
source context (Daumé III and Marcu, 2005).

5 Experiments and Results

In the experiment, we compare seq2seq with atten-
tion, standard pointer generators and the proposed
generalized pointer generator (GPG). To further
analyze the effect of the generalized pointer, we
implement a GPG model with only the point mode
(GPG-ptr) for comparison. We first introduce the
general setup, then report the evaluation results
and analysis.

5.1 General Setup

Dataset: We perform experiments on the
CNN/dailymail (Hermann et al., 2015), English
Gigaword (Rush et al., 2015) and XSum (Narayan
et al., 2018) dataset. We put statistics of the
datasets in Appendix A.2. CNN/DM contains
online news with multi-sentence summaries (We
use the non-anonymized version from See et al.
(2017)). English Gigaword paired the first sen-
tence of news articles with its headline. XSum cor-
pus provides a single-sentence summary for each
BBC long story. We pick these three dataset as
they have different properties for us to compare
models. CNN/DM strongly favors extractive sum-
marization (Kryściński et al., 2018). Gigaword
has more one-to-one word direct mapping (with
simple paraphrasing) (Napoles et al., 2012) while
XSum needs to perform more information fusion
and inference since the source is much longer than
the target (Narayan et al., 2018).

Model: We use single-layer bi-LSTM encoders
for all models. For comparison, hidden layer di-
mensions are set the same as in Zhou et al. (2017)
for Gigaword and See et al. (2017) for CNN/DM
and XSum. We train with batch size 256 for giga-
word and 32 for the other two. The vocabulary size
is set to 30k for all dataset. Word representations
are shared between the encoder and decoder. We
tokenize words with WordPiece segmentation (Wu
et al., 2016) to eliminate the OOV problem. More
details are in Appendix A.3

Inference: We decode text using beam

Figure 3: Test perplexity when increasing k

search (Graves, 2012) with beam size 10. We ap-
ply length normalization to rescale the score. Un-
like See et al. (2017); Gehrmann et al. (2018),
we do not explicitly impose coverage penalty
since it brings extra hyper-parameters. Instead,
for CNN/Dailymail, we use a simple tri-gram
penalty (Paulus et al., 2018) to prevent repeated
generations. GPG models use an exact marginal-
ization for testing and decoding, while for train-
ing and validation we use the top-k approximation
mentioned above. The decoder will first decode
sub-word ids then map them back to the normal
sentence. All scores are reported on the word level
and thus comparable with previous results. When
computing scores for multi-sentence summaries.
The generations are split into sentences with the
NLTK sentence tokenizer.

5.2 Results and Analysis

The results are presented in the following order:
We first study the effect of the hyperparameter
k, then evaluate model generations by automatic
metrics and look into the generation’s level of ab-
straction. Finally, we report the human evaluation
and word alignment accuracy.

Effect of K: k is the only hyperparameter in-
troduced by our model. Figure 3 visualizes the
effect of k on the test perplexity. As mentioned
in Sec 3, a larger k is expected to tighten the esti-
mation bound and improve the performance. The
figure shows the perplexity generally decreases as
increasing k. The effect on Gigaword and XSum
saturates at k = 6 and 10 respectively, so we fix
such k value for later experiments. For the longer
dataset CNN/Dailymail, the perplexity might still
decrease a bit afterwards, but the improvement is
marginal, so we set k = 14 for the memory limit.

Automatic Evaluation: The accuracy is evalu-
ated based on the standard metric ROUGE (Lin,



3767

Method R-1 R-2 R-L PPL

Point.Gen.+Cov* 39.53 17.28 36.38
Bottom Up† 41.22 18.68 38.34
seq2seq 39.79 17.37 36.34 17.49
Point.Gen. 40.03 17.52 36.77 12.36
GPG-ptr 40.54 18.05 37.19 10.23
GPG 40.95 18.01 37.46 9.37

Table 1: ROUGE score on CNN/Dailymail. * marks
results from See et al. (2017), and † from Gehrmann
et al. (2018). Underlined values are significantly better
than Point.Gen. with p = 0.05.

Method R-1 R-2 R-L PPL

seq2seq* 34.04 15.95 31.68
DRGD† 36.27 17.57 33.62
seq2seq 36.01 17.52 33.60 18.92
Point.Gen. 36.14 17.68 33.56 14.90
GPG-ptr 37.14 19.05 34.67 12.32
GPG 37.23 19.02 34.66 11.41

Table 2: ROUGE score on Gigaword. * marks results
from the word-based seq2seq implementation of Zhou
et al. (2017), and † from Li et al. (2017).

2004) and the word perplexity on the test data. We
report the ROUGE-1, ROUGE-2 and ROUGE-L
F-score measured by the official script. Table 1,
2 and 3 lists the results for CNN/Dailymail, Giga-
word and XSum respectively. Statistically signifi-
cant results are underlined5. On the top two rows
of each table, we include two results taken from
current state-of-the-art word-based models. They
are incomparable with our model because of the
different vocabulary, training and decoding pro-
cess, but we report them for completeness. Lower
rows are results from our implemented models.
Pointer generators bring only slight improvements
over the seq2seq baseline. This suggests that after
eliminating the OOV problem, the naive seq2seq
with attention model can already implicitly learn
most copy operations by itself. GPG models out-
perform seq2seq and pointer generators on all
dataset. The improvement is more significant for
more abstractive corpus Gigaword and XSum, in-
dicating our model is effective at identifying more
latent alignment relations.

Notably, even the pure pointer model (GPG-ptr)

5Results on the Gigaword test set is not significant due to
the smalle test size (1951 article-summary pairs).

Method R-1 R-2 R-L PPL

Point.Gen* 29.70 9.21 23.24
T-CONVS2S* 31.89 11.54 25.75
seq2seq 31.90 11.15 25.48 22.87
Point.Gen. 31.87 11.20 25.42 17.83
GPG-ptr 31.49 11.02 25.37 18.62
GPG 33.11 12.55 26.57 15.28

Table 3: ROUGE score on XSum. * marks results from
Narayan et al. (2018). Underlined values are signifi-
cantly better than Point.Gen. with p = 0.05.

without the generation mode outperforms stan-
dard pointer generators in CNN/DM and Giga-
word, implying most target tokens can be gen-
erated by aligning to a specific source word.
The finding is consistent with previous research
claiming CNN/DM summaries are largely extrac-
tive (Zhang et al., 2018; Kryściński et al., 2018).
Though the Gigaword headline dataset is more ab-
stractive, most words are simple paraphrases of
some specific source word, so pure pointer GPG-
ptr can work well. This is different from the XSum
story summarization dataset where many target
words require high-level abstraction or inference
and cannot be aligned to a single source word, so
combining the point and generation mode is nec-
essary for a good performance.

The word perplexity results are consistent over
all dataset (GPG < GPG-ptr < Point.Gen. <
seq2seq). The reduction of perplexity does not
necessarily indicate an increase for the ROUGE
score, especially for pointer generators. This
might attribute to the different probability compu-
tation of pointer generators, where the probabil-
ity of copied words are only normalized over the
source words. This brings it an inherent advantage
over other models where the normalization is over
the whole 30k vocabularies.

Level of Abstraction: In Tab. 4, we look into
how abstractive the generated summaries are by
calculating the proportion of novel unigram, bi-
gram and trigrams that do not exist in the corre-
sponding source text. On CNN/DM, as the genera-
tions contain multiple sentences, we further report
the proportion of novel sentences (obtained with
NLTK sent tokenize).

Tab. 4 reflects the clear difference in the level
of abstraction (seq2seq > GPG > GPG-ptr >
Point.Gen.). Though the seq2seq baseline gener-
ated most novel words, many of them are hallu-



3768

Models % of NNs in CNN/Dailymail % of NNs in Gigaword % of NNs in XSumNN-1 NN-2 NN-3 NN-S NN-1 NN-2 NN-3 NN-1 NN-2 NN-3
Seq2seq 0.38 3.56 7.98 54.97 16.15 52.84 73.76 27.05 76.54 92.07
Point.Gen. 0.04 1.51 4.29 35.82 13.99 47.79 68.53 19.45 66.68 84.59
GPG-ptr 0.17 2.05 5.08 41.64 14.05 48.09 70.70 20.03 69.54 87.14
GPG 0.35 2.91 5.66 49.24 15.14 52.07 72.73 24.16 71.93 87.94
Reference 9.08 46.71 67.99 97.78 48.26 84.53 94.43 32.24 84.12 95.92

Table 4: Proportion of novel n-grams (NN-1,2,3) and sentences (NN-S) on generated summaries. GPG generate
more novel words compared with standard pointer generators, though still slightly lower than seq2seq.

Figure 4: Pointing Ratio of the standard pointer gener-
ator and GPG (evaluated on the test data). GPG enables
the point mode more often, but quite a few pointed to-
kens are edited rather than simply copied.

cinated facts (see Fig 6), as has also been noted in
See et al. (2017). The abstraction of GPG model is
close to seq2seq and much higher than copy-based
pointer generators. We believe it comes from their
ability of “editing” pointed tokens rather than sim-
ple copying them.

To examine the pointing behavior of the GPG
model, we visualize the average pointing ratio
on three dataset in Fig. 4. The pointing ra-
tio can be considered as the chance that a word
is generated from the point mode instead of
the generation mode. We compute it as (1 −
pgen)

∑
i at,iδ(yt|xi)/p(yt), averaged over all tar-

get tokens in the test data. For the GPG model,
we further split it into the copy ratio (words that
are exactly copied) and the editing ratio (words
that are edited). We find the GPG model en-
ables the point mode more frequently than stan-
dord pointer generators, especially on the Giga-
word dataset (40% more). This also explains why
a pure pointer model is more effective for Giga-
word and CNN/DM. More than 60% target tokens
can be generated from the point mode, while for
XSum the ratio is less than 40%. Quite a few
pointing operation includes text rewriting (green

Article: (...) and bild reported that the (...)
Summary: (...) and bild report that the (...) [0.964]

Article: (...) thousands more death row prisoner (...)
Summary: (...) thousands more deaths row (...) [0.981]

Article: (...) surviving relatives of a woman who (...)
Summary: (...) family of woman who (...) [0.984]

Article: jordan ’s crown prince (...)
Summary: jordanian crown prince (...) [0.993]

Article: a middle-aged man and a young girl (...) [0.814]
Summary: a man and a child died (...)

Article: (...) , was abducted in 2012 (...)
Summary: (...) was kidnapped in (...) [0.924]

Figure 5: Examples of summaries produced by GPG.
Each two samples from CNN/DM, Gigaword and
XSum (up to down). bold denotes novel words and
their pointed source tokens. Bracketed numbers are the
pointing probability (1− pgen) during decoding.

bar in Fig. 4). This could explain why our model
is able to generate more novel words.

A few examples are displayed in Fig 5. We
find our model frequently changes the tense (re-
ported→ report), singular/plural (death→ deaths)
or POS tag (jordan→ jordanian) of words. Some-
times it also paraphrases (relatives → family) or
abstracts a noun to its hypernym (girl → child).
The word editing might be wrong though. For ex-
ample, “death row prisoner” is wrongly changed
to “deaths row prisoner” in the second example,
possibly because this phrase is rarely seen so that
the model made an error by mistaking “death” as
the main subject after “thousands more”6.

Human evaluation: We further perform a hu-
man evaluation to assess the model generations.
We focus on evaluating the fluency and faithful-
ness since the ROUGE score often fails to quan-

6It also reveals a limit of GPG model in that it only models
token-level alignment. For phrases like death row prisoner, it
cannot align it based on its compositional meaning.



3769

Fluency Faithfulness 0/1
seq2seq 0.83 0.61 0.53

Point.Gen. 0.78 0.65 0.55
GPG-ptr 0.79 0.78 0.67

GPG 0.82 0.74 0.69
Gold 0.96 0.92 0.96

Table 5: Human evaluation results on DUC 2004. 0/1
is the score for the 0/1 Turing test.

tify them (Schluter, 2017; Cao et al., 2018). 100
random source-target pairs are sampled from the
human-written DUC 2004 data for task 1&2 (Over
et al., 2007). Models trained on Gigaword are ap-
plied to generate corresponding summaries. The
gold targets, together with the model generations
are randomly shuffled then assigned to 10 hu-
man annotators. Each pair is evaluated by three
different people and the most agreed score is
adopted. Each pair is assigned a 0-1 score to in-
dicate (1) whether the target is fluent in gram-
mar, (2) whether the target faithfully conveys the
source information without hallucination and (3)
whether the target is considered human-generated
or machine-generated (like a 0/1 Turing test). The
averaged score is reported in Table 5. All models
generally achieve high scores in fluency, but gen-
erations from GPG models are more faithful to the
source information thereby have a larger chance
of fooling people into believe they’re human-
generated (over 0.1 higher score on the 0/1 Turing
test). This can be explained by GPG’s capability
at capturing more latent alignments. As shown in
Figure 4, GPG generates over half of the target
words by its point mode. Words are generated by
explicitly grounding on some source context in-
stead of fabricating freely.

Fig. 6 compares some generation snippets.
As can be observed, seq2seq models tend to
freely synthesize wrong facts not grounded on the
source text, especially on the more difficult XSum
dataset. In the last example, seq2seq only cap-
ture the subject “odom” and some keywords “po-
lice”, “basketball” then start to freely fabricate
random facts. Pointer generators are slightly better
as it is trained to directly copy keyword from the
source. However, once it starts to enter the gen-
eration mode (“of british” in example 2 and “has
been arrested” in example 3), the generation also
loses control. GPG largely alleviates the problems
because it can point to an aligned source word,

seq2seq Point.Gen. GPG

Prec 0.361 0.435 (0.512) 0.533 (0.628)

Table 6: Word Alignment Precision on DUC 2004.
Number in bracket is the posterior alignment precision.

then transform it by a learned relation embedding.
The explicit alignment modelling encourages the
model to stay close to the source information.

Alignment Accuracy: We also manually anno-
tate the word alignment on the same 100 DUC
2004 pairs. Following Daumé III and Marcu
(2005), words are allowed to be aligned with a
specific source word, phrase or a “null” anchor
meaning that it cannot be aligned with any source
word. The accuracy is only evaluated on the target
words with a non-null alignment. For each target
token, the most attended source word is consid-
ered as alignment (Ghader and Monz, 2017). For
the pointer generator and GPG, we also induce the
posterior alignment by applying the Bayes’ theo-
rem (derivation in appendix A.1). We report the
alignment precision (Och and Ney, 2000) in Ta-
ble 6, i.e., an alignment is considered as valid if it
matches one of the human annotated ground truth.

The results show that GPG improves the align-
ment precision by 0.1 compared with the standard
pointer generator. The posterior alignment is more
accurate than the prior one (also reflected in Fig-
ure 1), enabling better human interpretation.

6 Conclusion

In this work, we propose generalizing the pointer
generator to go beyond exact copy operation. At
each decoding step, the decoder can either gener-
ate from the vocabulary, copy or edit some source
words by estimating a relation embedding. Ex-
periments on abstractive summarization show the
generalized model generates more abstract sum-
maries yet faithful to the source information. The
generalized pointer is able to capture richer latent
alignment relationship beyond exact copies. This
helps improve the alignment accuracy, allowing
better model controllability and interpretation.

We believe the generalized pointer mechanism
could have potential applications in many fields
where tokens are not exactly copied. By integrat-
ing off-the-shelf knowledge bases to clearly model
the transition relation embedding, it should further
improve the interpretability and might be espe-



3770

Article: (...) marseille prosecutor brice robin told cnn that “ so far no videos were used in the crash investigation . ” he
added , “ a person who has such a video needs to immediately give it to the investigators . ” robin ’s comments follow
claims by two magazines , german daily bild and french paris match (...)
Seq2seq: marseille prosecutor brice robin tells cnn that “ so far no videos were used in the crash investigation ”
Point.Gen: robin ’s comments follow claims by two magazines , german daily bild and french (..)

GPG: “ so far no videos were used in the crash investigation , ” prosecutor brice robin says (..)

Article: surviving relatives of a woman who claimed she was raped ## years ago by the british queen ’s representative
in australia are seeking to withdraw a lawsuit against him , after the case drew widespread publicity in australia .
Seq2seq: family of british queen ’s representative in australia seeking to withdraw lawsuit against him .
Point.Gen: surviving relatives of british queen ’s representative seeking to withdraw lawsuit against

him .
GPG: family of woman who claimed she was victim of british queen ’s representative seeks to

withdraw lawsuit .

Article: police were called to love ranch brothel in crystal , nevada , after he was found unresponsive on tuesday . the
american had to be driven to hospital (...) mr odom , 35 , has played basketball for (...) lakers and clippers . he (...) was
suspended from the nba for violating its anti-drug policy (...) was named nba sixth man of the year (...)
Seq2seq: basketball legend odom odom has died at the age of 83 , police have confirmed .
Point.Gen: a former nba sixth man has been arrested on suspicion of anti-drug offences in

the us state of california .
GPG: the american basketball association ( lakers ) star lamar odom has been found unconscious

in the us state of nevada .

Figure 6: Examples of generated summaries. Examples are taken from CNN/DM, Gigaword and XSum (from up
to down). Darker means higher pointing probability.

cially helpful under low-resource settings, which
we leave for future work.

Acknowledgments

We thank anonymous reviewers for valuable com-
ments, thank Marius Mosbach for the proof read-
ing. Xiaoyu Shen is supported by IMPRS-CS fel-
lowship. The work is partially funded by DFG col-
laborative research center SFB 1102

References
Roee Aharoni and Yoav Goldberg. 2017. Morphologi-

cal inflection generation with hard monotonic atten-
tion. In Proceedings of the 55th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 2004–2015.

Carl Allen and Timothy Hospedales. 2019. Analo-
gies explained: Towards understanding word em-
beddings. ICML.

Michael Backes, Pascal Berrang, Mathias Humbert,
Xiaoyu Shen, and Verena Wolf. 2018. Simulating
the large-scale erosion of genomic privacy over time.
IEEE/ACM transactions on computational biology
and bioinformatics, 15(5):1405–1412.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. ICLR.

Antoine Bordes, Nicolas Usunier, Alberto Garcia-
Duran, Jason Weston, and Oksana Yakhnenko.
2013. Translating embeddings for modeling multi-
relational data. In Advances in neural information
processing systems, pages 2787–2795.

Denny Britz, Melody Guan, and Minh-Thang Luong.
2017. Efficient attention using a fixed-size mem-
ory representation. In Proceedings of the 2017 Con-
ference on Empirical Methods in Natural Language
Processing, pages 392–400.

Ziqiang Cao, Furu Wei, Wenjie Li, and Sujian Li. 2018.
Faithful to the original: Fact aware neural abstrac-
tive summarization. In Thirty-Second AAAI Confer-
ence on Artificial Intelligence.

Sumit Chopra, Michael Auli, and Alexander M Rush.
2016. Abstractive sentence summarization with at-
tentive recurrent neural networks. In Proceedings of
the 2016 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, pages 93–98.

Hal Daumé III and Daniel Marcu. 2005. Induction
of word and phrase alignments for automatic doc-
ument summarization. Computational Linguistics,
31(4):505–530.

Yuntian Deng, Anssi Kanervisto, Jeffrey Ling, and
Alexander M Rush. 2017. Image-to-markup gener-
ation with coarse-to-fine attention. In Proceedings
of the 34th International Conference on Machine
Learning-Volume 70, pages 980–989. JMLR. org.



3771

Yuntian Deng, Yoon Kim, Justin Chiu, Demi Guo, and
Alexander Rush. 2018. Latent alignment and varia-
tional attention. In Advances in Neural Information
Processing Systems, pages 9735–9747.

Angela Fan, David Grangier, and Michael Auli. 2018.
Controllable abstractive summarization. In Pro-
ceedings of the 2nd Workshop on Neural Machine
Translation and Generation, pages 45–54. Associa-
tion for Computational Linguistics.

Jonas Gehring, Michael Auli, David Grangier, De-
nis Yarats, and Yann N Dauphin. 2017. Convo-
lutional sequence to sequence learning. In Inter-
national Conference on Machine Learning, pages
1243–1252.

Sebastian Gehrmann, Yuntian Deng, and Alexander
Rush. 2018. Bottom-up abstractive summarization.
In Proceedings of the 2018 Conference on Empiri-
cal Methods in Natural Language Processing, pages
4098–4109.

Hamidreza Ghader and Christof Monz. 2017. What
does attention in neural machine translation pay at-
tention to? In Proceedings of the Eighth Interna-
tional Joint Conference on Natural Language Pro-
cessing (Volume 1: Long Papers), pages 30–39.

Alex Graves. 2012. Sequence transduction with
recurrent neural networks. arXiv preprint
arXiv:1211.3711.

Jiatao Gu, Zhengdong Lu, Hang Li, and Victor OK
Li. 2016. Incorporating copying mechanism in
sequence-to-sequence learning. In Proceedings of
the 54th Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers),
volume 1, pages 1631–1640.

Caglar Gulcehre, Sungjin Ahn, Ramesh Nallapati,
Bowen Zhou, and Yoshua Bengio. 2016. Pointing
the unknown words. In Proceedings of the 54th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), volume 1,
pages 140–149.

Huda Hakami, Kohei Hayashi, and Danushka Bolle-
gala. 2018. Why does pairdiff work?-a mathemati-
cal analysis of bilinear relational compositional op-
erators for analogy detection. In Proceedings of
the 27th International Conference on Computational
Linguistics, pages 2493–2504.

Karl Moritz Hermann, Tomas Kocisky, Edward
Grefenstette, Lasse Espeholt, Will Kay, Mustafa Su-
leyman, and Phil Blunsom. 2015. Teaching ma-
chines to read and comprehend. In Advances in Neu-
ral Information Processing Systems, pages 1693–
1701.

Hakan Inan, Khashayar Khosravi, and Richard Socher.
2017. Tying word vectors and word classifiers: A
loss framework for language modeling. ICLR.

Sarthak Jain and Byron C Wallace. 2019. Attention is
not explanation. NAACL.

Nan Rosemary Ke, Anirudh Goyal ALIAS PARTH
GOYAL, Olexa Bilaniuk, Jonathan Binas,
Michael C Mozer, Chris Pal, and Yoshua Ben-
gio. 2018. Sparse attentive backtracking: Temporal
credit assignment through reminding. In Advances
in Neural Information Processing Systems, pages
7640–7651.

Diederik P Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. ICLR.

Shun Kiyono, Sho Takase, Jun Suzuki, Naoaki
Okazaki, Kentaro Inui, and Masaaki Nagata. 2018.
Unsupervised token-wise alignment to improve in-
terpretation of encoder-decoder models. In Proceed-
ings of the 2018 EMNLP Workshop BlackboxNLP:
Analyzing and Interpreting Neural Networks for
NLP, pages 74–81.

Philipp Koehn and Rebecca Knowles. 2017. Six chal-
lenges for neural machine translation. In Pro-
ceedings of the First Workshop on Neural Machine
Translation, pages 28–39.

Wojciech Kryściński, Romain Paulus, Caiming Xiong,
and Richard Socher. 2018. Improving abstraction
in text summarization. In Proceedings of the 2018
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1808–1817.

Shaohui Kuang, Junhui Li, António Branco, Weihua
Luo, and Deyi Xiong. 2018. Attention focusing for
neural machine translation by bridging source and
target embeddings. In Proceedings of the 56th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1767–
1776.

Piji Li, Wai Lam, Lidong Bing, and Zihao Wang. 2017.
Deep recurrent generative decoder for abstractive
text summarization. In Proceedings of the 2017
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 2091–2100.

Chin-Yew Lin. 2004. Rouge: A package for auto-
matic evaluation of summaries. Text Summarization
Branches Out.

Thang Luong, Hieu Pham, and Christopher D Man-
ning. 2015. Effective approaches to attention-based
neural machine translation. In Proceedings of the
2015 Conference on Empirical Methods in Natural
Language Processing, pages 1412–1421.

Bryan McCann, James Bradbury, Caiming Xiong, and
Richard Socher. 2017. Learned in translation: Con-
textualized word vectors. In Advances in Neural In-
formation Processing Systems, pages 6294–6305.

Stephen Merity, Caiming Xiong, James Bradbury, and
Richard Socher. 2017. Pointer sentinel mixture
models. ICLR.



3772

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in neural information processing
systems, pages 3111–3119.

Courtney Napoles, Matthew Gormley, and Benjamin
Van Durme. 2012. Annotated gigaword. In Pro-
ceedings of the Joint Workshop on Automatic Knowl-
edge Base Construction and Web-scale Knowledge
Extraction, pages 95–100. Association for Compu-
tational Linguistics.

Shashi Narayan, Shay B Cohen, and Mirella Lapata.
2018. Don’t give me the details, just the summary!
topic-aware convolutional neural networks for ex-
treme summarization. In Proceedings of the 2018
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1797–1807.

Toan Nguyen and David Chiang. 2018. Improving lex-
ical choice in neural machine translation. In Pro-
ceedings of the 2018 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, Vol-
ume 1 (Long Papers), pages 334–343.

Franz Josef Och and Hermann Ney. 2000. A com-
parison of alignment models for statistical machine
translation. In COLING 2000 Volume 2: The 18th
International Conference on Computational Lin-
guistics.

Paul Over, Hoa Dang, and Donna Harman. 2007. Duc
in context. Information Processing & Management,
43(6):1506–1520.

Romain Paulus, Caiming Xiong, and Richard Socher.
2018. A deep reinforced model for abstractive sum-
marization. ICLR.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. In Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1532–
1543.

Matthew Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word repre-
sentations. In Proceedings of the 2018 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume 1 (Long Papers), volume 1,
pages 2227–2237.

Ofir Press and Lior Wolf. 2017. Using the output em-
bedding to improve language models. In Proceed-
ings of the 15th Conference of the European Chap-
ter of the Association for Computational Linguistics:
Volume 2, Short Papers, pages 157–163.

Alexander M Rush, Sumit Chopra, and Jason Weston.
2015. A neural attention model for abstractive sen-
tence summarization. In Proceedings of the 2015
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 379–389.

Natalie Schluter. 2017. The limits of automatic sum-
marisation according to rouge. In Proceedings of
the 15th Conference of the European Chapter of the
Association for Computational Linguistics: Volume
2, Short Papers, pages 41–45.

Abigail See, Peter J Liu, and Christopher D Manning.
2017. Get to the point: Summarization with pointer-
generator networks. In Proceedings of the 55th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), volume 1,
pages 1073–1083.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Neural machine translation of rare words with
subword units. In Proceedings of the 54th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), volume 1, pages
1715–1725.

Shiv Shankar, Siddhant Garg, and Sunita Sarawagi.
2018. Surprisingly easy hard-attention for sequence
to sequence learning. In Proceedings of the 2018
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 640–645.

Shiv Shankar and Sunita Sarawagi. 2019. Posterior at-
tention models for sequence to sequence learning.
In International Conference on Learning Represen-
tations.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in neural information process-
ing systems, pages 3104–3112.

Théo Trouillon, Johannes Welbl, Sebastian Riedel, Éric
Gaussier, and Guillaume Bouchard. 2016. Com-
plex embeddings for simple link prediction. In In-
ternational Conference on Machine Learning, pages
2071–2080.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems, pages 5998–6008.

Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly.
2015. Pointer networks. In Advances in Neural In-
formation Processing Systems, pages 2692–2700.

Ekaterina Vylomova, Laura Rimell, Trevor Cohn, and
Timothy Baldwin. 2016. Take and took, gaggle and
goose, book and read: Evaluating the utility of vec-
tor differences for lexical relation learning. In Pro-
ceedings of the 54th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), volume 1, pages 1671–1682.

Sam Wiseman, Stuart Shieber, and Alexander Rush.
2017. Challenges in data-to-document generation.
In Proceedings of the 2017 Conference on Empiri-
cal Methods in Natural Language Processing, pages
2253–2263.



3773

Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V
Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus
Macherey, et al. 2016. Google’s neural ma-
chine translation system: Bridging the gap between
human and machine translation. arXiv preprint
arXiv:1609.08144.

Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho,
Aaron Courville, Ruslan Salakhudinov, Rich Zemel,
and Yoshua Bengio. 2015. Show, attend and tell:
Neural image caption generation with visual atten-
tion. In International conference on machine learn-
ing, pages 2048–2057.

Zichao Yang, Phil Blunsom, Chris Dyer, and Wang
Ling. 2017. Reference-aware language models. In
Proceedings of the 2017 Conference on Empirical
Methods in Natural Language Processing, pages
1850–1859.

Lei Yu, Jan Buys, and Phil Blunsom. 2016. Online seg-
ment to segment neural transduction. In Proceed-
ings of the 2016 Conference on Empirical Methods
in Natural Language Processing, pages 1307–1316.

Fangfang Zhang, Jin-ge Yao, and Rui Yan. 2018. On
the abstractiveness of neural document summariza-
tion. In Proceedings of the 2018 Conference on Em-
pirical Methods in Natural Language Processing,
pages 785–790.

Qingyu Zhou, Nan Yang, Furu Wei, and Ming Zhou.
2017. Selective encoding for abstractive sentence
summarization. In Proceedings of the 55th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), volume 1, pages
1095–1104.


