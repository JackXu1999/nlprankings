



















































Introducing DRAIL -- a Step Towards Declarative Deep Relational Learning


Proceedings of the Workshop on Structured Prediction for NLP, pages 54–62,
Austin, TX, November 5, 2016. c©2016 Association for Computational Linguistics

Introducing DRAIL: a Step Towards Declarative Deep Relational Learning

Xiao Zhang∗, Maria Leonor Pacheco∗, Chang Li and Dan Goldwasser
{zhang923, pachecog, li1873, dgoldwas}@cs.purdue.edu

Abstract

We introduce DRAIL, a new declarative
framework for specifying Deep Relational
Models. Our framework separates structural
considerations, which express domain knowl-
edge, from the learning architecture to sim-
plify the process of building complex struc-
tural models.

We show the DRAIL formulation of two
NLP tasks, Twitter Part-of-Speech tagging and
Entity-Relation extraction. We compare the
performance of different deep learning archi-
tectures for these structural learning tasks.

1 Introduction

Building statistical models capable of dealing with
realistic problems require making predictions over
multiple, often interdependent, variables. In such
settings, correctly capturing the dependencies be-
tween these variables often takes precedence to the
specific algorithm used for estimating the models’
parameters. Capturing these dependencies relies on
compiling expert knowledge about the problem do-
main into the statistical model, and in recent years
several machine learning systems offering intuitive
interfaces for defining the dependencies between
predictions were suggested (Domingos et al., 2006;
McCallum et al., 2009; Rizzolo and Roth, 2010;
Bach et al., 2015; Kordjamshidi et al., 2015).

On the other hand, end-to-end deep learning
methods, which are becoming increasingly popu-
lar, take an almost opposite approach. These meth-
ods map the complex input object to desired outputs

∗* Equal contribution.

directly, without decomposing the decision process
into parts and modeling their dependencies. The re-
cent advances in deep learning allow these meth-
ods to successfully learn such mappings over very
high dimensional latent features space (Duchi et al.,
2011; Srivastava et al., 2014; Bahdanau et al., 2014).

At first glance these two trends seem almost con-
tradictory, as the first highlights the importance of
an easy-to-define, interpretable models, and the sec-
ond focuses on finding complex non-linear mapping
from the raw inputs to outputs that are difficult to in-
terpret, and its definition requires considerable tech-
nical expertise. However, as we argue in this paper,
these two objectives are not at odds, but rather ex-
press the specific considerations required at different
levels of abstraction. We suggest to break the depen-
dency between the two layers, and present a frame-
work for supporting it, by separating the definition
of structural elements and their dependencies from
the specific learning architecture used for learning
them. In this paper we describe the steps we took
towards building a declarative framework for Deep
Relational Learning (DRAIL).

DRAIL is a declarative modeling language for
defining structured prediction problems, that sep-
arates between the modeling layer, which defines
the high level dependencies between variables, and
the learning layer, which defines the learning archi-
tecture that will be used and its parameters. From
a modeling perspective, DRAIL is very similar to
other declarative languages such as Markov Logic
Networks (MLN) (Domingos et al., 2006) and Prob-
abilistic Soft Logic (PSL) (Bach et al., 2015), and
uses first order logic as a template language for
defining factor graph templates, which are instanti-

54



ated from given data.
A DRAIL program is defined over a set of predi-

cates, which can represent either observed values or
output predictions. We can define complex depen-
dencies by connecting these predicates using rules
of the following form:
O(x,c) ∧ O(x,d)⇒ P(x,y)
P(x,a)⇒¬P(x,b)

In this example, a,b,c are constant symbols,
x,y are variables, O(·) is a predicate capturing ob-
served properties of the input, and P(·) is a predi-
cate, representing a predicted property. The first de-
cision rule, captures the mapping between observed
properties of an input object and a prediction, while
the second rule captures the dependency between
two predictions (specifically, it states one output as-
signment for an input object prevents the assignment
of another). Each one of the rule templates is as-
sociated with a score, either learned from data, or
determined by the user, and the overall decision is
made by finding the optimally scored assignments
of output values to variables, by performing MAP
inference. This flexible framework allows DRAIL
to include both soft constraints, which quantify the
dependency between different output decisions, and
hard constraints, which force these dependencies
by manually assigning the rule the highest possible
weight.

Once the structural dependencies between the el-
ements of the model are determined using the deci-
sion rules, we can turn our attention to learning con-
siderations. From that perspective each rule defines
a factor graph template, and using data we learn a
scoring function for it. We learn the parameters of
the scoring function for each rule using a deep learn-
ing architecture, which can be different for each rule,
and normalized into a probability distribution, to al-
low global inference over all competing values. This
flexibility is the key difference between DRAIL and
other declarative learning frameworks: as it makes
no distinction between base classifiers and soft con-
straints, the scoring function for both can be learned
using highly expressive models.

We experimented with two well-known natu-
ral language processing structured prediction tasks,
Twitter Part-of-Speech tagging (Gimpel et al., 2011)
and Entity-Relation extraction (Roth and Yih, 2007).
In section 3 we show how to define these tasks as

DRAIL instances, and associate them with learning
architectures. In section 4 we explain our inference
procedure, converting a DRAIL instance into an In-
teger Linear program. We explain our learning ap-
proach in section 5. In our current experiments we
used both Multi-layer Perceptron networks (MLP)
and Recurrent Neural networks (RNN). We report
our results in section 6.

2 Related Work

The difficulty of building complex machine learn-
ing models over relational data has attracted con-
siderable attention in the machine learning commu-
nity, and several high level languages for specify-
ing the structure of different graphical models have
been suggested. For example, BLOG (Milch et al.,
2005) and CHURCH (Goodman et al., 2012) were
suggested for generative models, and MLN (Domin-
gos et al., 2006), PSL (Bach et al., 2015), FACTO-
RIE (McCallum et al., 2009), and CCM (Rizzolo
and Roth, 2010; Kordjamshidi et al., 2015) were
suggested for conditional models.

In this paper we look into combining such declar-
ative frameworks with deep learning models. Com-
bining deep learning with structured models was
studied by several works, typically in the context of a
specific task or a specific inference procedure. These
include dependency parsing (Chen and Manning,
2014; Weiss et al., 2015), transition systems (Andor
et al., 2016) named entity recognition and sequence
labeling systems (Ma and Hovy, 2016; Lample et
al., 2016), and models for combining deep learning
and graphical models for vision tasks (Zheng et al.,
2015; Chen et al., 2015).

3 DRAIL Modeling Language

The DRAIL modeling language provides a general
way to define relational learning problems that are
highly structured. A relational model is specified in
DRAIL using a set of weighted first-order logic rule
templates that describe predictions and express the
dependencies and constraints of a specific domain.
Each rule is composed of: (1) a template definition
written in first order logic, (2) the neural network
architecture that will be used to learn the parame-
ters of its scoring function, and (3) the set of feature
functions to be extracted. Rules are then compiled
into factor graphs, combining both predictions and

55



observed variables. To further illustrate these defini-
tions, we begin our explanation with two concrete
examples of NLP applications.

Example 1: Part-of-Speech (POS) Tagging This
task aims to map each word in a given sentence to its
corresponding POS category (e.g., noun, verb, ad-
jective, etc.).

Figure 1 describes an example of a simple model
definition for the POS tagging task. Words and their
corresponding POS tags have sequential dependen-
cies that can easily be expressed in a declarative way.
Our model consists of two rules: the first rule (line
1) conditions the POS tag only on the current word
(i.e., similar to an emission feature), and the second
rule (line 5) extends the dependency to both the pre-
vious word in the sentence and its tag assignment
(similar to a transition). In these rules, x is the cur-
rent word, y is the previous word, z is the POS tag
assignment of the previous word, and k is the POS
tag of the current word to be predicted.

1 rule:
2 def: Word(x) ⇒ HasPos(x,k)
3 network: MultiLayer, MultiClass
4 features: ["extract_twitter_glove"]
5 rule:
6 def: Word(x) ∧ HasPrevWord(x,y) ∧

HasPos(y,z) ⇒ HasPos(y,k)
7 network: MultiLayer, MultiClass
8 features: ["extract_twitter_glove", "

extract_tag"]

Figure 1: Modeling POS Tagging using DRAIL

In this example script, both rules are defined as
multi-class prediction problems and are associated
with Multi-layer neural network architectures (lines
3,7 respectively). We represent each word as a vector
using Twitter Glove embedding (Pennington et al.,
2014). We also use a vector representation for the
previous POS tag (lines 4,8 respectively).

Example 2: Entity-Relation Extraction Our sec-
ond example focuses on a simplified version of the
relation extraction task (Roth and Yih, 2007; Kord-
jamshidi et al., 2015), which identifies named enti-
ties and their categories (PER, LOC, ORG) and two
types of relations (LIVEIN, WORKFOR) over these
entities. In Figure 2 we illustrate the model defini-
tion for this task, and write the structural dependen-

cies of the problem using DRAIL. The model con-
sists of three rules: the first rule (line 1) is used for
predicting the entity category of a given phrase, and
the second and third rules (lines 5 and 9, respec-
tively) describe the two possible relations between
pairs of phrases. We force the consistency between
the entity and relation prediction types by encoding
this knowledge as hard constraints (lines 13-16).

1 rule:
2 def: Phrase(x) ⇒ IsEntity(x,y)
3 network: MultiLayer, MultiClass
4 features: ["extract_entity_feats"]
5 rule :
6 def: Phrase(x) ∧ Phrase(y) ∧

InSentence(x,z) ∧ InSentence(y,z) ⇒
LiveIn(x,y)

7 network: MultiLayer, Binary
8 features: ["live_in_feats"]
9 rule :

10 def: Phrase(x) ∧ Phrase(y) ∧
InSentence(x,z) ∧ InSentence(y,z) ⇒
WorkFor(x,y)

11 network: MultiLayer, Binary
12 features: ["work_for_feats"]
13 const: LiveIn(x,y) ⇒ Entity(x,"Per")
14 const: LiveIn(x,y) ⇒ Entity(y,"Loc")
15 const: WorkFor(y,z) ⇒ Entity(y,"Per")
16 const: WorkFor(y,z) ⇒ Entity(z,"Org")

Figure 2: Modeling the Relation Extraction problem using
DRAIL

DRAIL Elements The elementary units of the
model are predicates, which represent relations in
the domains. These can be binary relations be-
tween two variables (e.g., HasPos(x,z) where
a word x has a POS tag z), or unary relations
(e.g., Phrase(x)). The predicates can correspond
to hidden or observed variables. In the latter case,
the data corresponding to each predicate is loaded
from raw files into a relational database that can later
be automatically queried to instantiate groundings
for each rule template. Otherwise, the assignment of
predicted values is determined in an inference pro-
cedure afterwards.

Each rule template defines a learning problem,
which is used to score different assignments to the
head of the rule. Rules have the form A⇒ B, where
A (body) constitutes a conjunction of observations
and predicted values and B (head) is the information
to be predicted. We allow each rule to be defined as
either a multi-class, multi-label or a binary learning
problem.

56



The overhead of DRAIL is considerably light,
considering the size of the tested data sets. We im-
plemented a compiler to translate formatted rules
provided by the user to rule templates speedily.
DRAIL then creates a simple in-memory relational
database instance by loading raw data, based on
the rule templates created. In order to construct the
training, validation and testing data sets, DRAIL
queries the database to create inputs for models cor-
responding to designated rule templates. The over-
head of DRAIL majorly lies on the database queries
process, which can be alleviated by using a more so-
phisticated database that handles queries efficiently
for larger volumes of data. As far as we know, a vari-
ety of matured industrialized database systems carry
this merit. After the data sets are created, the train-
ing procedure will be the same as usual. Hence, we
intend to improve this aspect in later formal releases.

We use several neural network architectures to
learn a probability distribution over the different out-
put value predictions. Our main idea is to have a
model definition that is agnostic of the network ar-
chitecture used. In this way, the type of network
(e.g., MLP, CNN or RNN) as well as other hyper-
parameters (e.g., the number of layers, the number
of hidden units, the learning rate, etc.) can be easily
tuned without changing the dependencies between
output variables.

To be able to learn from observations, each ob-
served grounding must generate features to feed into
the associated neural network. Currently, we provide
a basic feature extractor interface for users to extend.
Features are programmable in the Python language
and there is no limit as to which types of features can
be included for a rule. We also provide a set of out-
of-the-box features that can be directly used, such as
word embedding and one-hot vector representations
for predicate arguments. Since features are added
programmatically, external resources can be easily
incorporated.

Finally, const rules define hard constraints over
the general problem. These constraints allow the
user to inject domain or common-sense knowledge
into the prediction problem. For example, in the re-
lation extraction task, the LIVEIN relation can only
be predicted between an entity phrase of type PER-
SON and an entity phrase of type LOCATION. These
constraints do not require any learning, and they can

be directly translated into inference constraints.

4 Inference

Given a specific instance, we assign values to the
output variables by running an inference procedure,
formulated as an Integer Linear Programming (ILP)
problem over rule groundings.

A rule grounding is an instantiation of a rule tem-
plate. We generate rule grounding by enumerating
all possible values for the rule’s variables given its
domain. For example, the rule template Word(x)
⇒ HasPos(x,y), will be instantiated with each
possible part-of-speech tag for each observed word.

We score the rule groundings by associating each
template with a neural net. We denote the score of
each rule grounding as wi, the weight associated
with rule grounding i. These weights are used as
coefficients of the corresponding ILP variables in
the objective function when performing global in-
ference.

We introduce rule variables ri for each rule
grounding i and head variables hj for each different
head predicate j (and its negation (h̄j)) to indicate
the activation of the variable. The objective function
can then be expressed as

arg max
∀ri

∑
i

wi · ri

.
Note that in the objective function, we do not as-

sign any weights to head variables as their values are
entirely determined by constraints that ensure con-
sistency.

In order to enforce consistency between variable
assignments and dependencies among them, the fol-
lowing five types of constraints are taken into con-
sideration in an ILP formulation.

negation constraints The first type constraints en-
sure exclusive activation of a head predicate
and its negation at the same time. For example,
hHasPos(a,b) + h̄HasPos(a,b) = 1.

implied constraints Each rule template defines the
dependency between body and head. This de-
pendency is reflected between the rule ground-
ings variable and the head variables in the body.
For example, in the rule grounding Word(a)
∧ HasPrevWord(a,b) ∧ HasPos(b,p)

57



⇒ HasPos(a,q), where a, b are words and
p, q are part-of-speech tags, the constraint
rrule ≤ hHasPos(b,p) is needed, as the whole
rule is true only when the body is activated.

rule/head constraints One head predicate can be
associated with multiple rule grounding vari-
ables. Let ri, i ∈ ruleset(j) denote the rule
variables associated with the same head vari-
able hj , where ruleset(j) is the set of rule
groundings that share the same head predicate
j. Activation of any rules in ruleset(j) en-
sures the activation of the head variable, i.e.
hj ≥ ri,∀i ∈ ruleset(j). On the other hand,
the activation of the head variable ensures the
activation of at least one of its corresponding
rule variables, i.e. hj ≤

∑
i ri.

binary/multi-class/multi-label constraints In
many problems, we are facing multi-class or
multi-label decisions. DRAIL guarantees this
by adding suitable constraints. For instance, in
the multi-class case, among all head variables
hj (j ∈ decision(d)) on the same entity,
only one of them is activated while the others
remain inactive, as a decision is made on
which class to choose, i.e.

∑
j hj = 1. Note

that the constraints for binary predicates can be
covered by the negation constraints mentioned
above.

hard constraints from rule definitions Users can
define hard constraints in the rule templates,
which usually infuse prior knowledge and
thus improve the prediction capacity. Rule
groundings of these templates are dealt dif-
ferently as the activation of such a rule de-
pends on the activation of all body predi-
cates. As an example, a hard constraint for
entity-relation extraction problem discussed in
this paper is LiveIn(x,y)⇒ Entity(x,
‘‘Per’’).

We used the Gurobi Optimizer (Gurobi Optimiza-
tion, 2015) to implement the inference module. As
in many real world problem settings, the optimiza-
tion problem based on the ILP formulation is com-
putationally intractable, hence in practice we relax
the inference procedure to linear programming (LP)

problem by adapting the variable type from binary
to continuous, within the range [0, 1].

5 Learning

Each rule template in a DRAIL model file defines
a learning problem, for scoring the mapping of the
variables defined in rule body to its head. We de-
signed the rules to include flexible definitions for the
representation and architecture used for learning this
scoring function.

In the training stage, rule groundings are unfolded
from the data, using rule templates. For each rule
template, the neural networks map the LHS (left
hand side) of a rule to the RHS (right hand side) with
some probability, given all possible groundings for
the right hand side. This mapping is learned using
a deep learning model, which uses a logistic regres-
sion on top and the output probability distribution
will be used as scores over the multi-class classifica-
tion problem in an succeeding inference procedure.

One of the main advantages of DRAIL is that
the neural network architecture is separate from
the structural model. For each deep neural network
model, all the hyper-parameters (e.g., the type of
neural network, the learning rate, etc.) can be config-
ured and optimized. Consequently we can associate
different deep neural network architectures with
different rule templates, as suits the sub-problems
best. This design not only grants more flexibility
when dealing with a structural learning problem, but
also enables users to experiment with more feasible
choices. For example, in the entity relation classifi-
cation problem, we can create a LSTM (long-short-
term-memory) model for entity tagging and then a
Multi-layer Perceptron model for relation classifica-
tion. In our experiments we used two different archi-
tectures, a Multi-layer Perceptron model and a Re-
current neural network, which are briefly described
in the following sections.

5.1 MLP (Multi-layer Perceptron)

MLP is a simple yet widely used feed-forward artifi-
cial neural network model mapping input data onto a
set of appropriate outputs. It contains several layers
of nodes as a directed graph. Nodes in each layer are
fully connected and an activation function is applied
to each node except the input nodes. MLP has been
proved to be a useful modeling tool, capable of ap-

58



proximating any function (Cybenko, 1989), hence it
can be directly applied to data that are linearly insep-
arable. In our experiments we used a simple three-
layer MLP that can be formulated as follows:

hs = sigm(W [x1; x2; ps]),
ys = softmax(hs),

where the ; operator means concatenation of input
vectors, and ps is the feature representation of a hid-
den variable s. Visually, this model can be illustrated
by Figure 3.

x2

Hidden
Layer

x1 ps

y

Figure 3: Multi-layer Perceptron Model for DRAIL.

5.2 RNN (Recurrent Neural Networks)

It has been repeatedly demonstrated that Recurrent
Neural Networks are a good fit for sequence label-
ing tasks (Elman, 1990; Graves, 2013). We there-
fore used it for tackling the POS tagging task. Since
our model (see section 3) for the POS task captures
the dependency between the previous prediction and
the current one, we formulate the RNN to accommo-
date this prediction task by including the unobserved
predicates on the LHS of the rule templates as hid-
den variables. In order to break the dependency of
the hidden predicates and the recurrent information
in the model, we concatenated feature representa-
tions of the hidden variable with the recurrent rep-
resentations that entail the history information and
feed them to the final softmax layer to yield scores.
This can be defined by the following equations:

ht = [sigm(Wxxt); sigm(Whht−1)],
hst = [ht; p

s
t ],

yst = softmax(h
s
t ) (1)

where pst is the feature representation of hidden vari-
able s at step t. A graphical demonstration of this
model is shown in Figure 4.

xt

RNN
Unit

ht−1 ht

ytpst

Figure 4: Recurrent Neural Network Model for DRAIL.

Possible future extension of deep neural network
models include but are not limited to: LSTM (Long
short term memory) model, GRU (Gated Recurrent
Unit) model, Recursive Neural Network and Atten-
tion models. In addition, though our current learning
implementation is training local learning models in
the training stage combined with global inference in
the testing stage, which is analogous to an MEMM
model, in future work we plan to extend DRAIL to
global learning in the training stage, similar to Con-
ditional Random Fields (CRF).

6 Experiments

To demonstrate the generalization ability of our
system to a variety of NLP tasks, we evaluated
DRAIL on two structured prediction problems:
Twitter Part of Speech (POS) tagging task (Gimpel
et al., 2011) and the Entity-mention-Relation extrac-
tion task (Roth and Yih, 2007).

6.1 Part of Speech Tagging for Twitter
To tackle the Twitter POS tagging problem, Gimpel
et al. (2011) used a CRF and defined a comprehen-
sive set of features specific to the Twitter domain.
We modeled the POS tagging problem in DRAIL as
described on section 3 and tested it on the same data.
This data set contains 1, 827 tweets (26, 436 tokens)
in total, divided in three folds: train, validation and
test and it encompasses 25 different tags.

We used different features in our experiments, in-
cluding pre-trained word embeddings1 (Mikolov et

1We refer to Mikolov et al (2013) as W2V, and to Pennington

59



al., 2013; Pennington et al., 2014) to take advantage
of our deep learning system as well as the base fea-
tures defined by (Gimpel et al., 2011): a feature type
for each word, suffixes of size 1 to 3, capitalization
patterns, and features to indicate whether the word
contains digits and hyphens.

We built deep neural network models for each
template separately and trained them locally us-
ing different configurations of the enumerated fea-
ture sets. The same set hyper-parameters were used
across both models: a MLP model with one hidden
layer of 100 hidden units. We used batched stochas-
tic gradient descent (SGD) with batch size 200 to
train the models. Additionally, to prevent over fit-
ting, we applied an early-stop paradigm to deter-
mine the appropriate number of training epochs by
using the validation set. In the prediction stage, In-
teger Linear Programming (ILP) was used to per-
form global inference on the test set and decide the
POS tag for all words in a sentence. In order to speed
up the inference, we relaxed ILP to Linear Program
(LP), approximating the {0, 1} variables as a float
number within the range [0, 1].

Results Experimental results can be observed in
tables 1 and 2. Table 1 shows the advantage of us-
ing deep neural networks over a Maximum Entropy
Markov Model (MEMM) and the CRF results re-
ported by (Gimpel et al., 2011). We obtained an im-
provement of 4.51% and 2.31% respectively, even
by training Neural Networks locally, with zero tun-
ing effort and performing global inference only at
prediction time. In addition, we defined two mod-
els, a local baseline, using only the emission features
(i.e., without inference), and a skyline that used the
gold previous POS tags (i.e., no inference is required
to determine this information). DRAIL’s results af-
ter global inference got quite close to using the gold
dependencies in the same MLP model, missing the
skyline by just 0.37%.

One of the main advantages of deep neural net-
works is the way they can exploit feature embed-
ding. By looking at table 2 we can observe that word
embeddings pre-trained on Twitter data help boost
the performance of this task considerably. In con-
trast, the pre-trained word embeddings from Google
news negatively impact the results, decreasing the
performance drastically. We attribute this result to

et al (2014) as Twitter Embedding

the linguistic style and language characteristics of
tweets, which greatly differ from those of news arti-
cles. To confirm this hypothesis, we did a subsequent
qualitative analysis and discovered that a great num-
ber of twitter tokens are not present in the Google
news corpus.

These results provide evidence that training sim-
ple MLP models locally, using suitable pre-trained
embeddings and applying global inference for pre-
diction, can outperform shallow models with global
training procedures, even without additional man-
ual efforts to tune hype-parameters. Furthermore, we
extended our deep neural networks model to Re-
current Neural networks in DRAIL, and integrated
it with global inference. Due to time limitation,
DRAIL is only tested with simple features, while
still demonstrating comparable performance on this
task.

Model Feature set Accuracy
CRF (Gimpel
et al., 2011)

Base Features 83.38%

MEMM Base Features 81.00%
DRAIL Local
Prediction
(Baseline)

Base Features + Twitter
Embedding

84.20%

DRAIL Gold
Dependencies
(Skyline)

Base Features + Twitter
Embedding

85.88%

DRAIL MLP
with Global
Inference

Base Features + Twitter
Embedding

85.51%

Table 1: Comparison of DRAIL to other models

Model Feature set Accuracy

MLP with
Global
Inference

Google W2V 55.52%
BOW 75.50%
Twitter Embedding 78.35%
Twitter Embedding +
BOW

82.10%

Twitter Embedding +
Base Features

85.51%

Local RNN BOW (randomized vec-
tor)

79.68%

RNN with
Global Infer-
ence

BOW (randomized vec-
tor)

80.12%

Table 2: Accuracy of Twitter POS tagging using DRAIL with
different architectures and feature sets

60



6.2 Entity-Relation Extraction
We used DRAIL to describe a joint model to extract
named entities and relations between them. The data
set used was created by (Roth and Yih, 2004). It
contains 1441 sentences and 37261 phrases. There
are four types of entities: People (1691), Location
(1968), Organization (984), and Other (706) and
five types of relations. Similar to previous work on
this data set, we focused on two specific relations:
LIVEIN (521) indicates a Person lives in a Location,
and WORKFOR (401) indicates a Person works for
an Organization. All other pairs of phrases do not
hold any relation between them, which means the
data set is highly skewed.

We used the model configuration outlined in Fig-
ure 2. For the entity classifier, a set of features were
extracted from phrases with a window of size 4
around the target including itself. These features in-
clude words, word embedding, part-of-speech tags,
suffix, prefix, gazetteers and capitalization patterns.
For the relation classifier, we included the same set
of features mentioned above for both phrases as well
as a small list of indicator words additionally, like
“live”, “native”, “employ”, and its relative position
to the two target phrases. We also used features
from the path between two phrases on the depen-
dency parsing tree. A deep neural network model
was trained locally for each rule. These networks
have one hidden layer and 100 hidden units (300 for
relation classifiers). We used stochastic gradient de-
cent with batch size 100 and AdaGrad (Duchi et al.,
2011) to adapt the learning rate in training. Softmax
probabilities from the neural networks were used as
scores in each ILP instance to find the optimal solu-
tion.

Results The results using 5-fold cross-validation
are shown in Table 3. We report the F1 score of
the positive class, i.e. Fβ=1. To show the model-
ing capacity and expressiveness of DRAIL, we also
tested using 0-order MEMM for local models. For
each configuration, we report the result obtained di-
rectly from locally trained classifiers, and the results
after global inference. As it shows, the deep archi-
tecture has a higher impact on the relation extrac-
tion problem than on entities, demonstrating its ef-
ficacy on difficult decisions. Also, the global infer-
ence procedure helps improve the performance on
relation extraction significantly. Even without incor-

porating numerous handcrafted features, the perfor-
mance of DRAIL is commensurate to or better than
previous work, including (Kordjamshidi et al., 2015)
and (Roth and Yih, 2007). Note that results are not
directly comparable because their data splits specifi-
cations were not available.

Model PER LOC ORG WorkFor LiveIn
MEMM
Local

93.96 88.80 78.82 54.12 43.56

MEMM
Global

93.43 89.02 79.36 54.68 53.09

MLP
Local

92.32 89.78 80.60 54.80 51.19

MLP
Global

92.30 90.05 79.86 62.84 56.86

Table 3: The F1 of different local models with and without in-
ference on Entity-Relation Extraction task using DRAIL, 5-fold
cross validation

7 Discussion and Conclusion

This paper introduces DRAIL, an open-source2

declarative framework for defining structural depen-
dencies between probabilistic concepts trained us-
ing deep learning models. The experimental results
on the Twitter POS tagging problem and the Entity-
mention-Relation extraction task demonstrate the
flexibility of our framework, which can be used for
quick prototyping and evaluating the interplay be-
tween representation complexity and structural com-
plexity. DRAIL takes advantage of both building de-
pendencies between structures and mapping com-
plex inputs to outputs, resulting in a richer and
more flexible hypothesis class, and enriched fea-
ture representations at the same time. These merits
enhance the prediction ability of DRAIL. Our cur-
rent work looks into including efficient and effective
joint training (CRF Neural Networks) into DRAIL,
by taking into account assigning values to all the
variables in a global model and back-propagating
error to the whole structure simultaneously. A po-
tential advantage is that we can pre-train the mod-
els locally, and use global training to boost the per-
formance using the same set of parameters, by only
changing the objective function. Our DRAIL frame-
work carries the capacity to incorporate this training
paradigm into itself without additional effort.

2we intend to release the code and data used in our experi-
ments

61



References

Daniel Andor, Chris Alberti, David Weiss, Aliaksei
Severyn, Alessandro Presta, Kuzman Ganchev, Slav
Petrov, and Michael Collins. 2016. Globally normal-
ized transition-based neural networks.

Stephen H. Bach, Matthias Broecheler, Bert Huang, and
Lise Getoor. 2015. Hinge-loss markov random
fields and probabilistic soft logic. arXiv:1505.04406
[cs.LG].

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
learning to align and translate. arXiv preprint
arXiv:1409.0473.

Danqi Chen and Christopher D Manning. 2014. A
fast and accurate dependency parser using neural net-
works. In Proc. of the Conference on Empirical Meth-
ods for Natural Language Processing (EMNLP).

L.-C. Chen, A. G. Schwing, A. L. Yuille, and R. Urta-
sun. 2015. Learning deep structured models. In Proc.
of the International Conference on Machine Learning
(ICML).

G Cybenko. 1989. Approximation by superpositions of
a sigmoidal function. Mathematics of Control, Signals
and Systems, 2(4):303–314.

Pedro M Domingos, Stanley Kok, Hoifung Poon,
Matthew Richardson, and Parag Singla. 2006. Uni-
fying logical and statistical ai. In Proc. of the National
Conference on Artificial Intelligence (AAAI).

John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning
Research, 12(Jul):2121–2159.

J Elman. 1990. Finding structure in time. Cognitive
Science, 14(2):179–211, jun.

Kevin Gimpel, Nathan Schneider, Brendan O’Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael
Heilman, Dani Yogatama, Jeffrey Flanigan, and
Noah A. Smith. 2011. Part-of-Speech Tagging
for Twitter: Annotation, Features, and Experiments.
pages 42–47.

Noah Goodman, Vikash Mansinghka, Daniel M Roy,
Keith Bonawitz, and Joshua B Tenenbaum. 2012.
Church: a language for generative models.

Alex Graves. 2013. Generating sequences with recurrent
neural networks. CoRR, abs/1308.0850.

Inc. Gurobi Optimization. 2015. Gurobi optimizer refer-
ence manual.

Parisa Kordjamshidi, Dan Roth, and Hao Wu. 2015.
Saul: Towards declarative learning based program-
ming.

Guillaume Lample, Miguel Ballesteros, Sandeep Subra-
manian, Kazuya Kawakami, and Chris Dyer. 2016.

Neural architectures for named entity recognition.
arXiv preprint arXiv:1603.01360.

Xuezhe Ma and Eduard Hovy. 2016. End-to-end se-
quence labeling via bi-directional lstm-cnns-crf. arXiv
preprint arXiv:1603.01354.

Andrew McCallum, Karl Schultz, and Sameer Singh.
2009. FACTORIE: Probabilistic programming via im-
peratively defined factor graphs. In The Conference on
Advances in Neural Information Processing Systems
(NIPS).

Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S
Corrado, and Jeffrey Dean. 2013. Distributed Rep-
resentations of Words and Phrases and their Composi-
tionality. In Advances in Neural Information Process-
ing Systems 26: 27th Annual Conference on Neural In-
formation Processing Systems 2013.

Brian Milch, Bhaskara Marthi, Stuart Russell, David
Sontag, Daniel L Ong, and Andrey Kolobov. 2005.
Blog: probabilistic models with unknown objects. In
Proc. of the International Joint Conference on Artifi-
cial Intelligence (IJCAI).

Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. GloVe: Global Vectors for Word
Representation. In Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 1532–1543.

N. Rizzolo and D. Roth. 2010. Learning based java for
rapid development of nlp systems. In LREC.

Dan Roth and Wen-tau Yih. 2004. A linear program-
ming formulation for global inference in natural lan-
guage tasks. In Hwee Tou Ng and Ellen Riloff, editors,
HLT-NAACL 2004 Workshop: Eighth Conference on
Computational Natural Language Learning (CoNLL-
2004), pages 1–8, Boston, Massachusetts, USA, May
6 - May 7. Association for Computational Linguistics.

D. Roth and W. Yih. 2007. Global inference for entity
and relation identification via a linear programming
formulation.

Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: a simple way to prevent neural networks
from overfitting. Journal of Machine Learning Re-
search, 15(1):1929–1958.

David Weiss, Chris Alberti, Michael Collins, and Slav
Petrov. 2015. Structured training for neural net-
work transition-based parsing. In Proc. of the Annual
Meeting of the Association Computational Linguistics
(ACL).

Shuai Zheng, Sadeep Jayasumana, Bernardino Romera-
Paredes, Vibhav Vineet, Zhizhong Su, Dalong Du,
Chang Huang, and Philip Torr. 2015. Conditional ran-
dom fields as recurrent neural networks. In Proc. of the
International Conference on Computer Vision (ICCV).

62


