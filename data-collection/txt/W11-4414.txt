



















































Stochastic K-TSS Bi-Languages for Machine Translation


Proceedings of the 9th International Workshop on Finite State Methods and Natural Language Processing, pages 98–106,
Blois (France), July 12-15, 2011. c©2011 Association for Computational Linguistics

Stochastic K-TSS bi-languages for Machine Translation

M. Inés Torres
Depto. de Electricidad y Electrónica

Universidad del Paı́s Vasco
Bilbao, Spain

manes.torres@ehu.es

Francisco Casacuberta
Instituto Tecnológico de Informática
Universidad Politécnica de Valencia

Valencia, Spain
fcn@iti.upv.es

Abstract

One of the approaches to statistical machine
translation is based on joint probability dis-
tributions over some source and target lan-
guages. In this work we propose to model
the joint probability distribution by stochas-
tic regular bi-languages. Specifically we in-
troduce the stochastic k-testable in the strict
sense bi-languages to represent the joint prob-
ability distribution of source and target lan-
guages. With this basis we present a refor-
mulation of the GIATI methodology to infer
stochastic regular bi-languages for machine
translation purposes.

1 Introduction

The goal of statistical machine translation (SMT)
is to search for the sentence t̂ that maximizes the
a-posteriori probability P (t|s) of the target sen-
tence t being the translation of a given sentence
s from the source language. The translation mod-
els in SMT are automatically learned from bilingual
samples. In the early nineties machine translation
was tackled as a pure probabilistic process by the
IBM research group (Brown et al., 1993). Within
the SMT framework, stochastic-finite-state trans-
ducers (SFSTs) have also been proposed for ma-
chine translation purposes (Bangalore and Riccardi,
2002) (Shankar et al., 2005) (Casacuberta and Vidal,
2004) (Casacuberta and Vidal, 2007) (Blackwood et
al., 2009). In such a context, SMT can be viewed
as the problem of computing the joint probability
distribution of some source and target languages.
i.e. P (t, s), inferred from a bi-lingual corpus. The

joint probability distributions of pairs of strings may
be modeled by a probability distribution on a set
of strings based on bi-lingual units as proposed
in (Bangalore and Riccardi, 2002) for SFSTs. Al-
ternatively (Casacuberta and Vidal, 2004) (Mariño
et al., 2006) proposed n-grams models of bi-lingual
units. However, only a few techniques to learn
finite-state transducers for machine translation pur-
poses can be found (Bangalore and Riccardi, 2002)
(Oncina et al., 1993) (Knight and Al-Onaizan, 1998)
(Casacuberta and Vidal, 2007). On the other hand,
a method of inference of SFST based on the infer-
ence of stochastic finite-state automata (Casacuberta
and Vidal, 2004) was proposed and then used in
machine translation applications (Casacuberta and
Vidal, 2007) (Pérez et al., 2008) (González and
Casacuberta, 2009). This method was called gram-
matical inference and alignments for transducer in-
ference (GIATI) and is based on some important
properties relating regular translations generated by
finite-state-transducers and regular languages over
some bi-lingual alphabet (Berstel, 1979).

On the other hand, different stochastic regular bi-
languages can be introduced to model P (s, t) dis-
tribution. Turning to stochastic regular languages,
let us note that the class of stochastic k-testable
in the strict sense (k-TSS) languages is a sub-
class of stochastic regular languages that can be in-
ferred from a set of positive training data (Torres
and Varona, 2001) (Vidal et al., 2005a) (Torres and
Casacuberta, 2011) by some stochastic extension of
the inference algorithm in (Garcı́a and Vidal, 1990).
Thus, they belong to the subset of regular languages
that can be used to characterize some pattern recog-

98



nition tasks. In particular, stochastic k-TSS has been
used in many natural language processing tasks such
as phone recognition (Galiano and Segarra, 1993),
speech recognition (Torres and Varona, 2001), lan-
guage identification (Guijarrubia and Torres, 2010),
language modeling (Justo and Torres, 2009) or ma-
chine translation (Pérez et al., 2008).

In this work we propose to model the joint prob-
ability distribution P (t, s) by stochastic regular bi-
languages. A first contribution of our work is the
reformulation of the GIATI methodology to infer
stochastic regular bi-languages for machine transla-
tion purposes. This proposal allows the use of some
stochastic bi-automaton to get the sentence t̂ that
corresponds to the source sentence ŝ. This stochas-
tic bi-automaton need to be inferred from a sample
set of bi-strings. As a consequence, this method-
ology does not required any SFST as original GI-
ATI did. Thus, there is no need to any property re-
lating stochastic regular translations and stochastic
regular languages to support the proposed method.
On the other hand, different stochastic regular bi-
languages can be introduced to model the joint prob-
ability distribution. As a second contribution we
propose in this work the use of stochastic k-TSS bi-
languages to model Pr(s, t). For this purpose we
extend definitions and theorems of stochastic k-TSS
languages (Vidal et al., 2005a) (Torres and Casacu-
berta, 2011) to stochastic k-TSS bi-languages and
then write a corollary to the stochastic extension of
the morphism theorem.

We contribute in Section 2 with some definitions
of bi-strings, stochastic bi-languages and stochas-
tic bi-automata. In Section 3 we propose to model
the joint probability distribution through stochastic
bi-language and then use stochastic bi-automaton
for translation purposes. In Section 4 we deal with
stochastic k-TSS bi-languages and bi-automaton,
introducing some definitions and theorem applica-
tions. Then we present in Section 5 the inference of
stochastic k-TSS bi-automata for machine transla-
tion as a reformulation of the GIATI methodology.
Finally Section 6 deals with some concluding re-
marks and future work.

2 Stochastic regular bi-languages

In this Section we first provide the basic defini-
tions of bi-string, stochastic regular bi-language and
stochastic and deterministic finite state bi-automata
proposed in this work.

Let Σ and ∆ be two finite alphabets and Σ≤m and
∆≤n, the finite sets of sequences of symbols in Σ
and ∆ of length up to m and n respectively. Let
Γ ⊆ (Σ≤m × ∆≤n) be a finite alphabet (extended
alphabet) consisting of pairs of strings, that we call
extended symbols, (s1 . . . si : t1 . . . tj) ∈ Γ such
that s1 . . . si ∈ Σ≤m and t1 . . . tj ∈ ∆≤n with 0 ≤
i ≤ m and 0 ≤ j ≤ n.
Definition 2.1. A bi-language is a set of strings over
an extended alphabet Γ, i.e., a set of strings of the
form b = b1 . . . bk such that bi ∈ Γ for 0 ≤ i ≤ k.
A string over an extended alphabet Γ will be called
bi-string.

Alternatively (Kornai, 2008) defines a bi-string as
composed by two strings and an association relation.
In the same way, bi-languages are defined as sets
of well-formed bi-strings that undergo the usual set-
theoretic operations of intersection, union and com-
plementation. Concatenation of such bi-strings is
also defined in (Kornai, 2008). In this context, reg-
ular bi-languages were previously defined in (Kor-
nai, 1995). In the context of machine translation,
(Mariño et al., 2006) defines a bi-language as com-
posed of bi-lingual units which were referred to as
tuples extracted from alignments of a bilingual cor-
pus. This definition could be consistent with the one
provided in definition 2.1. Also in machine trans-
lation, (Bangalore and Riccardi, 2002) defines a bi-
language corpus as consisting of source-target sym-
bol pair sequences (s1 : t1) . . . (si : ti) . . . (sn : tn)
such that si ∈ Ls ∪ {λ} and its aligned symbol
ti ∈ Lt ∪ {λ} where Ls and Lt are a couple of re-
lated languages. This definition allows for pairs of
symbols by contrast with definition 2.1 where pairs
of finite-length strings are considered. Finally, let us
note that regular tree languages were also been re-
ferred as bilanguages (Pair and Quere, 1968) (Berger
and Pair, 1978).

We are now referring to the work by (Vidal et
al., 2005a). This work is a survey of probabilis-
tic finite-state machines and related definitions and
properties. In this survey, the authors provide a def-

99



inition of probabilistic automata that corresponds to
generative models. Note that in classical (and non
probabilistic) formal theory strings are generated by
grammars. In this paper we are using the formalism
developed in (Vidal et al., 2005a).

Given a finite alphabet Σ, a stochastic language is
defined in (Vidal et al., 2005a) as a probability distri-
bution over Σ∗. Let us extend this definition to con-
sider bi-strings and then get stochastic bi-languages.

Definition 2.2. Given two finite alphabets Σ and ∆,
a stochastic bi-language B is a probability distribu-
tion over Γ∗ where Γ ⊆ (Σ≤m ×∆≤n), m,n ≥ 0.
Let z = z1 . . . z|z| be a bi-string such that zi ∈ Γ
for 1 ≤ i ≤ |z|. If PrB(z) denotes the probabil-
ity of the bi-string z under the distribution B then∑

z∈Γ∗ PrB(z) = 1.

Let now define a deterministic and probabilistic
finite-state bi-automaton (DPFBA) by extending the
standard definition of a deterministic and probabilis-
tic finite-state automaton (DPFA) as follows:

Definition 2.3. A DPFBA is a probabilistic finite-
state bi-automaton BA = (Q,Σ,
∆,Γ, δ, q0, Pf , P ) if Q is a finite set of states, Σ and
∆ are two finite alphabets, Γ is an extended alphabet
such that Γ ⊆ (Σ≤m×∆≤n),m,n ≥ 0, δ ⊆ Q×Γ×
Q is a set of transitions of the form (q, (s̃i : t̃i), q′)
where q, q′ ∈ Q and , (s̃i : t̃i) ∈ Γ, q0 ∈ Q is the
unique initial state, Pf : Q→ [0, 1] is the final-state
probabilistic distribution and P : δ → [0, 1] defines
transition probabilistic distributions (P (q, b, q′) ≡
Pr(q′, b|q) for b ∈ Γ and q, q′ ∈ Q) such that:

Pf (q) +
∑

b∈Γ,q′∈Q
P (q, b, q′) = 1 ∀q ∈ Q (1)

where a transition (q, b, q′) is completely defined by
q and b. Thus, ∀q ∈ Q, ∀b ∈ Γ |{q′ : (q, b, q′)}| ≤ 1

Finally let z ∈ Γ∗ and let θ =
(q0, z1, q1, z2, q2, ..., q|z|−1, z|z|, q|z|) be a path
for z in BA. The probability of generating θ is:

PrBA(θ) =

 |z|∏
j=1

P (qj−1, zj , qj)

 · Pf (q|z|) (2)
BA is a DPFBA and thus unambiguous. Then,
a given bi-string z can only be generated by BA

through a unique valid path θ(z). Thus, the prob-
ability of generating z with BA is PrBA(z) =
PrBA(θ(z)).

3 Statistical translation with bi-automata

Let us consider a source and a target languages from
a source vocabulary Σ and a target vocabulary ∆,
respectively. The goal of machine translation is to
map a sentence in the source language, i.e. a string
of symbols s = s1 . . . s|s|, si ∈ Σ into a sentence
in the target language t = t1 . . . t|t|, ti ∈ ∆. Sta-
tistical machine translation (SMT) is based on the
noisy channel approach (Shannon, 1948) where t is
considered to be a noisy version of s (Brown et al.,
1993). Thus, the translation of a given string s ∈ ∆∗
in the source language is a string t̂ ∈ ∆∗ in the target
language such that:

t̂ = arg max
t∈∆∗

Pr(t|s)

Alternatively, a joint probability distribution can be
used by developing Pr(t|s) in previous Equation as
follows:

t̂ = arg max
t∈∆∗

Pr(s, t)
Pr(s)

= arg max
t∈∆∗

Pr(s, t) (3)

since, Pr(s) does not depend on t. Distribution
Pr(s, t) can be modeled by a stochastic finite state
transducer (Bangalore and Riccardi, 2002) (Casacu-
berta and Vidal, 2004). Alternatively in this paper
we model this distribution by a stochastic regular bi-
language.

To this end, let z be a bi-string over the extended
alphabet Γ ⊆ Σ≤m×∆≤n such as z : z = z1 . . . z|z|,
zi = (s̃i : t̃i) where s̃i = s1 . . . s|s̃i| ∈ Σ≤m
and t̃i = t1 . . . t|t̃i| ∈ ∆≤n. Extended symbols
(s̃i : t̃i) ∈ Γ have been obtained through some
alignment between Σ≤m and ∆≤n. String s ∈ Σ∗
is a sequence of substrings s̃i such as s = s̃1 . . . s̃|z|
that has been obtained through a previously segmen-
tation procedure. In the same way string t ∈ ∆∗ is
a sequence of substrings t̃i such as t = t̃1 . . . t̃|z|.
Then Pr(s, t) can be calculated as follows:

Pr(s, t) =
∑

∀z∈Γ∗:(hΣ(z),h∆(z))=(s,t)
Pr(z) (4)

100



In such a case, Pr(s, t) can be modeled by a
DPFBA BA such as the one defined in Definition
2.3. Thus, the probability Pr(s, t) according to BA
is defined as

PrBA(s, t) =
∑

∀z∈Γ∗:(hΣ(z),h∆(z))=(s,t)
PrBA(z)

=
∑

∀θ∈g(s,t)
PrBA(θ)

where g(s, t) denotes the set of all possible paths in
BA matching (s, t) and PrBA(θ) is calculated ac-
cording to Equation 2.

3.1 The search through a stochastic finite state
bi-automaton

The main goal of SMT according to Equation 3 is
to find the optimal target string t̂ given a source
string ŝ and given a stochastic model of the involved
joint probability. When Pr(s, t) is modeled by a
DPFBA BA we need to be able to get the string
t̂ = t̃1 . . . t̃|z| that corresponds to the source se-
quence s = s̃1 . . . s̃|z|, given PrBA(s, t) through
Equation 5. A bi-automaton BA is ambiguous with
respect to the input sequence s. Thus, all pairs (s, t)
matching the given input sequence s are considered,
i.e the maximization is carried out ∀t ∈ ∆∗ instead
of ∀(s, t) ∈ Γ∗. As a consequence t̂ is obtained as
follows:

t̂ = arg max
t∈∆∗

PrBA(s, t)

= arg max
t∈∆∗

∑
∀θ∈g(s,t)

PrBA(θ)

This search for the optimal t̂ through Equation 5
has proved to be a difficult computational problem
(Casacuberta and de la Higuera, 2000). In practice
Equation 5 can be computed by the so-called max-
imum approximation, which assume that the sum
close the maximum term. In such a case we first
estimate the optimal path θ̂ is obtained as:

θ̂ = arg max
∀θ∈g(s)

PrBA(θ)

where g(s) denotes the set of possible paths in BA
matching s and PrBA(θ) is calculated by Equa-
tion 2. The approximate translation t̂ is then com-
puted as the concatenation of the target substrings

associated to the estimated path θ̂ : (q0, (s̃1 :
t̃1), q1)(q1, (s̃2 : t̃2), q2).....(qm−1, (s̃m : t̃m), qm)
and t̂ = t̃1, t̃2, . . . , t̃m by the recursive algorithm
proposed in (Casacuberta and Vidal, 2004) adapted
now to a bi-automaton.

4 Stochastic k-TSS bi-languages

Different stochastic regular bi-languages can be in-
troduced to model PrBA(s, t) distribution in Equa-
tion 5. In particular we propose in this work the use
stochastic k-TSS DPFBA. In this Section we deal
with stochastic k-TSS bi-languages as a particular
case of stochastic bi-languages defined in Section 2.

To this end, let us now turn to stochastic k-TSS
languages which are a subclass stochastic regular
languages. Stochastic k-TSS languages are defined
in (Vidal et al., 2005a) and (Torres and Casacuberta,
2011) as a four-tuple Zk = (Σ, PIk , PFk , PTk),
where Σ is a finite alphabet; PIk : Σ

<k → [0, 1]
are the initial probabilities, i.e. the probability that
a string a1 . . . aj ∈ Ik ⊆ Σ<k is a starting segment
of a string in the language; PFk : Σ

<k → [0, 1]
are the final probabilities, i.e. the probability that a
string a1 . . . aj ∈ Fk ⊆ Σ<k is a final segment of
a string in the language and PTk : Σ

k → [0, 1] are
the allowed-segments probabilities, i.e. the proba-
bility that a string a1 . . . ak ∈ (Σk − Tk) accord-
ing to the corresponding normalization conditions.
Thus, strings in the stochastic k-TSS language LZk
start with segments in Ik of length up to k − 1, they
end with segments in Fk of length up to k − 1 and
do not include segments in Tk of length k. This defi-
nition can be straightforwardly extended to consider
bi-languages as follows:

Definition 4.1. A stochastic k-TSS bi-language
ZBk = (Γ, PIBk , PFBk , PTBk ) is a stochastic k-
TSS language defined on a extended alphabet Γ ⊆
Σ≤m ×∆≤n.
ZBk defines a probability distribution BZBk on

Γ∗, simplified as Bk from now, such as for any string
of bi-strings z ∈ Γ∗ of size |z| , i.e. z = z1 . . . z|z|
the probability PrBk(z) is calculated according to:


PIk(z1 . . . z|z|) · PFk(z1 . . . z|z|) if |z| < k
PIk(z1 . . . zk−1) ·

∏|z|
i=k PTk(zi−k+1 . . . zi−1, zi)·

PFk(z|z|−(k−2) . . . z|z|) if |z| ≥ k

101



PrBk(z) is the probability of the string z ∈ Γ∗ under
the k-TSS distribution Bk. Thus:∑

z∈Γ∗
PrBk(z) = 1 (5)

Let us now fall back to classical k-TSS to bear
in mind some important theorems. An interesting
subclass of k-TSS is the class of 2-TSS languages,
which are known as local languages. There is an
important generative property which relates local
languages and general regular languages given by
the morphism theorem (Garcı́a et al., 1987), which
establish that any regular language can be gener-
ated by a local language. A stochastic extension of
the morphism theorem was introduced in (Vidal et
al., 2005b). A stochastic regular bi-language is a
particular case of stochastic regular languages for
an extended alphabet Γ ⊆ (Σ≤m × ∆≤n). As a
consequence, we can apply the stochastic extension
of the morphism theorem in (Vidal et al., 2005b)
to stochastic regular bi-languages and then write a
corollary for this theorem as follows:

Corollary 4.1. Let Σ and ∆ be two finite alphabets,
Γ ⊆ (Σ≤m ×∆≤n) be an extended alphabet and B
a stochastic regular bi-language on Γ∗. There exists
then a finite alphabet Γ′, an alphabetic morphism
h : Γ′∗ → Γ∗ and a stochastic local language D2
over Γ′∗ such that B = h(D2); i.e.,

PrB(z) = PrD2(h−1(z))
=

∑
y∈h−1(z) PrD2(y) ∀z ∈ Γ∗

where h−1(z) = {y ∈ Γ′∗|z = h(y)}. Thus,
any stochastic regular bi-language defined over Γ∗

can be generated by a local language over some Γ′∗

where Γ and Γ′ are finite alphabets of extended sym-
bols such that Γ,Γ′ ⊆ Σ≤m ×∆≤n

We need now to deal with stochastic k-TSS bi-
automata as well as with the way to get them from a
training corpus. The inference of k-TSS automata
was first addressed in (Garcı́a and Vidal, 1990).
Given a set of positive sample setR+ of an unknown
language, an efficient algorithm obtains a determin-
istic finite-state automaton that recognizes the small-
est k-TSS language containing the sample setR+. A
preliminary form of a stochastic extension was pre-
sented in (Segarra, 1993) and then fully formalized

in (Torres and Casacuberta, 2011). In that work a
k-TSS DPFA is defined as a class of DPFA able to
generate stochastic k-TSS languages where the un-
ambiguity of the automaton allowed for a maximum
likelihood estimation of each transition probability.
This algorithm, can be easily adapted to infer a k-
TSS DPFBA, BAk, generating a stochastic k-TSS
bi-language by considering an extended alphabet of
bi-strings Γ ⊆ (Σ≤m ×∆≤n). Example 4.1 shows
the way to infer a k-TSS DPFBA BAk that gener-
ates a k-TSS bi-language containing a previously
defined sample R+.

Example 4.1. Let Σ = {a, b} and ∆ = {1, 0} be
two finite alphabets and let Γ ⊆ (Σ≤m × ∆≤n) be
the extended alphabet such as: Γ = {(a : 1), (aa :
11), (b : 0), (bb : 00)}. Let now R+ be a positive
sample set of a stochastic k-TSS bi-language B con-
sisting of strings in Γ∗ such that: R+ = {(a : 1), (b :
0), (aa : 11), (a : 1)(a : 1), (aa, 11)(b : 0), (a : 1)(a :
1)(b : 0), (a : 1)(b : 0)(b : 0), (a : 1)(bb : 00)}
Then for k = 3
I3 = {(a : 1), (b : 0), (aa : 11), (a : 1)(a : 1), (aa :
11)(b : 0), (a : 1)(b : 0), (a : 1)(bb : 00)}
PIk = {0.125, 0.125, 0.125, 0.25, 0.125, 0.125, 0.125}
F3 = {(a : 1), (b : 0), (aa : 11), (a : 1)(a : 1), (aa :
11)(b : 0), (a : 1)(b : 0), (b : 0)(b : 0), (a : 1)(bb : 00)}
Pfk = {1, 1, 1, 0.5, 1, 0.5, 1, 1}
The inferred bi-automaton BA3 is represented as:

a:1
0.2

a:1,a:1
0.5

a:1
 0.4

a:1,b:0
0.5b:0 0.2

a:1,bb:00
1

bb:00
 0.2

b:0
1

aa:11
0.5 aa:11,b:0

1

b:0
 0.5

b:0
 0.5

b:0,b:0
1

b:0
 0.5

λ

a:1
 0.625

b:0
 0.125
aa:11
 0.25

where each state q ∈ Qk is labelled by a bi-string
(s̃1 : t̃1 . . . s̃i : t̃i) ∈ Γi i < k along with the prob-
ability Pf (q) and each edge is labelled by a pair
s̃i : t̃i ∈ Γ such that (q, s̃i : t̃i, q′) ∈ δk along with
the probability Pk(q, s̃i : t̃i, q′).

102



5 Inference of k-TSS bi-automata for
machine translation

In Section 3 we have propose to compute the
joint probability distribution P (s, t) through some
stochastic bi-automaton according to definitions in
Section 2. Then in Section 4 we have shown how to
get an stochastic k-TSS bi-automaton from a posi-
tive sample set of bi-strings. Thus, we can now pro-
pose a technique for the inference of stochastic k-
TSS bi-automata for machine translation purposes
based on GIATI methodology, which takes advan-
tage of theoretical background previously (Casacu-
berta and Vidal, 2004) (Vidal et al., 2005b) (Torres
and Casacuberta, 2011).

Given a finite sample set S+ of strings pairs
(s, t) ∈ Σ∗ ×∆∗ from a bilingual (parallel) corpus
then

• Step 1: Given a pair of strings (s, t) get a bi-
string z ∈ Γ∗ according to some particular
alignment and segmentation procedures. As a
result, the sample set S+ of bilingual sentences
(s, t) ∈ Σ∗ ×∆∗ is transformed into a set R+
of bi-strings z ∈ Γ∗.

S+ ⊆ Σ∗ ×∆∗ → R+ ⊆ Γ∗

• Step 2: From the set of bi-strings R+ ⊆
Γ∗ infer the k-TSS DPFBA BAk generating
a stochastic k-TSS bi-language that includes
R+.

R+ ⊆ Γ∗ → BAk : R+ ⊆ BBAk
5.1 Step 1- Segmentation
The goal of this step is to get a corpus of bi-strings
from a bilingual corpus. Let (s, t) : s ∈ Σ∗, t ∈ ∆∗
be a pair of strings in S+ such that each string
s ∈ Σ∗ and each string t ∈ ∆∗ is a sequence of
substrings s̃i and t̃i. Then a segmentation proce-
dure is required to get a bi-string z ∈ Γ∗ : z =
(s̃1, t̃1) . . . (s̃|z|, t̃|z|) such that string s is a sequence
of substrings s̃i and string t is a sequence of sub-
strings t̃i. The segmentation is monotone if s =
s̃1 . . . s̃|z| and t = t̃1 . . . t̃|z|.

Then a relation between substrings s̃i ∈ Σ∗ and
substrings t̃i ∈ ∆∗ need also be defined. This re-
lation was called alignment in (Kornai, 2008) and

depends on the the application task. In this context
the aim of the alignment is to synchronize sequences
of features from two different finite alphabets (Ko-
rnai, 1995). Correspondences between source and
target strings could be complex, could include long-
distance and/or not consecutive associations, etc,
such that the choice of a suitable alignment is a dif-
ficult problem to be solved. One way to deal with
this problem in the machine translation framework
is the use of statistical alignments models (Brown et
al., 1993) (Och and Ney, 2003).

The choice of an adequate align-
ment/segmentation procedure is also related with
the parsing procedure based on the bi-automaton.
In the translation procedure, the target sentence t̂ is
obtained as the concatenation of target substrings
matching a given source sentence that also consists
of a sequence of source substrings. A monotonic
segmentation guaranties that the procedure to
transform pairs of strings in S+ into bi-strings in Γ∗
is reversible.

Example 5.1. Let Σ = {a, b} and ∆ = {0, 1} be
two finite alphabets. Let now S+ be a bilingual
corpus of translations consisting in pairs of strings
(s, t) such that s ∈ Σ∗ and t ∈ ∆∗ and S+ =
{(a, 1), (b, 0), (aa, 11), (aab, 110), (aab, 110)}.
From this corpus we can obtain, among others, the
following alignments:

a

1

b

0

a

1

a

1

a

1

a

1

a

1

a

1

b

0

a

1

a

1

b

0

a

1

b

0

b

0

a

1

b

0

b

0

From these alignments we get the alphabet of bi-
strings Γ = {(a : 1), (aa : 11), (b : 0), (bb : 00)}.
Thus the positive sample set R+ consisting of
bi-strings in Γ∗ is: R+ = {(a : 1), (b : 0), (aa :
11), (a : 1)(a : 1), (aa, 11)(b : 0), (a : 1)(a : 1)(b :
0), (a : 1)(b : 0)(b : 0), (a : 1)(bb : 00)}

Let us to note that symbols of the general form
(s̃i : t̃i), relate strings in Σm, m ≥ 0 with strings in
∆n, n ≥ 0. Alternatively, some machine translation
models deal with pairs (si : t̃i) where the relation
is established between symbols si ∈ Σ ∪ {λ} and
strings t̃i ∈ ∆n, n ≥ 0. In such a case, the bi-
string is defined as composed by pairs (si : t̃i) ∈
(Σ ∪ {λ} ×∆n), n ≥ 0.

103



5.2 Step 2 - Inferring a k-TSS DPFBA

Next, a stochastic finite-state bi-automaton, such as
the one defined in Section 4, is inferred from the cor-
pus of bi-stings R+. In particular we propose the
inference of a k-TSS DPFBA BAk. To this end, the
inference algorithm for k-TSS DPFA summarized in
(Torres and Casacuberta, 2011) and then extended to
get k-TSS DPFBA in Section 4 need to be applied.
Example 4.1 shows the k-TSS DPFBA inferred from
the positive sample set R+ get in Example 5.1

Notice that in this case a smoothed model is re-
quired since the model has to generate any bi-string
z ∈ Γ∗ with a non-zero probability, even for bi-
strings not in the stochastic bi-language generated
by the inferred bi-automaton. Specific smoothing
schemas has been proposed for stochastic k-TSS au-
tomata for speech recognition purposes in (Torres
and Varona, 2001) and in (Llorens et al., 2002). Un-
der a back-off scheme, these techniques adjust the
maximum likelihood estimation of transition prob-
abilities to recursively obtain probabilities to be as-
signed to unseen combinations of strings from mod-
els with decreasing the value of k, i.e. less accu-
rate (Torres and Varona, 2001) (Llorens et al., 2002).
These procedures are now straightforward extended
to get smoothed k-TSS DPFBA. However let us to
note that this procedure does not assign a non-zero
probability to bi-strings in Γ∗ which does not con-
sists of sequences of extended symbols in Γ. Thus,
it does not guarantee that any target string t ∈ ∆∗
could be obtained (with either high or small proba-
bility) as a liable translation of a given source string.
To this end the smoothing should be applied to get a
non-zero probability for any pair (s, t) ∈ (Σ∗×∆∗).
This problem is similar to the one of smoothing
transducers, which is still an open problem (Llorens
et al., 2002).

The k-TSS DPFBA BAk models the joint prob-
ability distribution P (s, t) for machine translation
purposes. Thus the string t̂ = t̃1 . . . t̃|z| that corre-
sponds to the source sequence s = s̃1 . . . s̃|z|, given
PrBAk(s, t) can be directly obtained parsing with
the bi-automaton using Equation 5 according to the
procedure described in Section 3.1. As a conse-
quence this procedure does not need any final step
aimed to transform back extended symbols into pairs
of strings in Σ∗ × ∆∗ since any SFST is inferred.

Thus, the morphism theorems which are the basis of
the classical GIATI methodology (Casacuberta and
Vidal, 2004) are not now required.

6 Conclusions and future work

Machine translation can be viewed as the problem of
computing the joint probability distribution of some
bi-language inferred from a bilingual corpus. In
such a context, we have proposed to represent trans-
lation models by stochastic regular bi-languages. To
this end we have provided some specific definitions.
Moreover, stochastic bi-automata can directly ob-
tain the target string corresponding to a given source
string.

On the other hand, we have specifically consid-
ered the stochastic k-TSS bi-languages to model
joint probability distributions. The morphism theo-
rem relating stochastic local languages and stochas-
tic regular languages can now be extended to
stochastic k-TSS bi-languages through a corollary.
Moreover, stochastic k-TSS bi-automaton can also
be inferred from a positive sample set through an
extension of the inference algorithm for classical
stochastic k-TSS languages.

With this basis we have reformulated the GIATI
methodology to infer stochastic stochastic k-TSS bi-
languages for machine translation purposes, which
takes advantage of the knowledge about stochas-
tic k-TSS languages and their application to natural
language tasks. Moreover, the finite-state formal-
ism allows easy integration of other automata rep-
resenting target language models or acoustic mod-
els in speech translation tasks. However, the mono-
tonic segmentation does not allow to deal with long-
distance alignments which is a problem when the
distance between the pair of languages is large. On
the other hand smoothing techniques dealing with
any pair of strings need also to be further explored.

Finally let us notice that relationship between
stochastic k-TSS bi-languages and a subclass of
stochastic regular translations, i.e. between stochas-
tic k-TSS bi-automata and a subclass of stochastic
finite state transducers, is going to be explored in
the future.

Acknowledgments.
We would like to acknowledge support for

this work to the Spanish Ministry of Sci-

104



ence and Innovation under the Consolider Inge-
nio 2010 programme (MIPRCV CSD2007-00018),
grant TIN2008-06856-C05-01 and grant TIN2009-
14511; to the the Basque Government under grant
GIC10/158 IT375-10 and to the Generalitat Valen-
ciana under grant Prometeo/2009/014.

References
S. Bangalore and G. Riccardi. 2002. Stochastic finite-

state models for spoken language machine translation.
Machine Translation, 17(3):165–184.

J. Berger and C. Pair. 1978. Inference for regular bi-
languages. Journal of Computer and System Sciences,
16(1):100–122.

J. Berstel. 1979. Transductions and context-free lan-
guages. B.G. Teubner Verlag, Stuttgart.

G. Blackwood, A. de Gispert, J. Brunning, and W. Byrne.
2009. Large-scale statistical machine translation with
weighted finite state transducers. In Proceeding of the
2009 conference on Finite-State Methods and Natural
Language Processing, pages 39–49, Amsterdam, The
Netherlands, The Netherlands. IOS Press.

P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263–311.

F. Casacuberta and C. de la Higuera. 2000. Computa-
tional complexity of problems on probabilistic gram-
mars and transducers. In A.L. Oliveira, editor, Gram-
matical Inference: Algorithms and Applications, vol-
ume 1891 of Lecture Notes in Computer Science,
pages 15–24. Springer-Verlag. 5th International Col-
loquium Grammatical Inference -ICGI2000-. Lisboa.
Portugal. Septiembre.

F. Casacuberta and E. Vidal. 2004. Machine translation
with inferred stochastic finite-state transducers. Com-
putational Linguistics, 30(2):205–225.

F. Casacuberta and E. Vidal. 2007. Learning finite-state
models for machine translation. Machine Learning,
66(1):69–91.

I. Galiano and E. Segarra. 1993. The application of k-
testable languages in the strict sense to phone recogni-
tion in automatic speech recognition. In Proceedings
of the IEE Colloquium of Grammatical Inference. The-
ory, Applications and Alternatives. IEE.

P. Garcı́a and E. Vidal. 1990. Inference of k-testable
languages in the strict sense and application to syntac-
tic pattern recognition. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 12(9):920–925.

P. Garcı́a, E. Vidal, and F. Casacuberta. 1987. Local lan-
guages, the succesor method, and a step towards a gen-

eral methodology for the inference of regular gram-
mars. IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence, 9(6):841–845.

J. González and F. Casacuberta. 2009. GREAT: a
finite-state machine translation toolkit implementing
a Grammatical Inference Approach for Transducer
Inference (GIATI). In Proceedings of the EACL
Workshop on Computational Linguistics Aspects of
Grammatical Inference, pages 24–32, Athens, Greece,
March 30.

V. Guijarrubia and M.I. Torres. 2010. Text and speech
based phonotactic models for spoken language iden-
tification of basque and spanish. Pattern Recognition
Letters, 31(6):523–532.

R. Justo and M.I. Torres. 2009. Phrase classes in two-
level language models for asr. Pattern Analysis and
Applications, 12:427–437.

K. Knight and Y. Al-Onaizan. 1998. Translation with
finite-state devices. In Lecture Notes in Computer Sci-
ence, volume 1529, pages 421–437. Springer-Verlag.

A. Kornai. 1995. Formal Phonology. Outstand-
ing Dissertations in Linguistics. Garland Publishing,
NewYork.

A. Kornai. 2008. Mathematical Linguistics. Advanced
Information and Knowledge Processing. Springer,
Cambridge, MA - USA.

D. Llorens, J.M. Vilar, and F. Casacuberta. 2002. Finite
state language models smoothed using n-grams. Inter-
national Journal of Pattern Recognition and Artificial
Intelligence, 16(3):275–289.

J.B. Mariño, R. E. Banchs ans J.M. Crego, A.de Gispert,
P. Lambert, J.A.R. Fonollosa, and M.R. Costa-juss.
2006. N-gram based machine translation. Computa-
tional Lingustics, 32(4):527–549.

F. Och and H. Ney. 2003. A systematic comparison of
various statistical alignments models. Computational
Linguistics, 29(1):19–51.

J. Oncina, P. Garcı́a, and E. Vidal. 1993. Learning sub-
sequential transducers for pattern recognition interpre-
tation tasks. pami, 15(5):448–458.

C. Pair and A. Quere. 1968. Définition et etude des bilan-
gages réguliers. Information and Control, 13(6):565–
593.

A. Pérez, M.I. Torres, and F. Casacuberta. 2008. Join-
ing linguistic and statistical methods for Spanish-to-
Basque speech translation. Speech Communication,
50:1021–1033.

E. Segarra. 1993. Una aproximación Inductiva a la
Comprensión del Discurso Continuo. Ph.D. thesis,
Universidad Politécnica de Valencia. Advisors: Dr.
P. Garcı́a and Dr. E. Vidal.

K. Shankar, Y. Deng, and W. Byrne. 2005. A weighted
finite state transducer translation template model for

105



statistical machine translation. Natural Language En-
gineering, 12:35–75, December.

C. E. Shannon. 1948. A mathematical theory of commu-
nication. Bell Systems Technical Journal, 27(3):379–
423, July.

M. Inés Torres and F. Casacuberta. 2011. Stochastic
k-tss languages. Technical report, PR&Speech Tech-
nologies Group, Depto. Electricidad y Electrónica,
Universidad del Paı́s Vasco, April.

M. I. Torres and A. Varona. 2001. k-tss language models
in speech recognition systems. Computer, Speech and
Language, 15(2):127–149.

E. Vidal, F. Thollard, C. de la Higuera, F. Casacuberta,
and R. Carrasco. 2005a. Probabilistic finite-state ma-
chines - part I. IEEE Transactions on Pattern Analysis
and Machine Intelligence (PAMI), 27(7):1013–1025.

E. Vidal, F. Thollard, C. de la Higuera, F. Casacuberta,
and R. Carrasco. 2005b. Probabilistic finite-state ma-
chines - part II. IEEE Transactions on Pattern Anal-
ysis and Machine Intelligence (PAMI), 27(7):1025–
1039.

106


