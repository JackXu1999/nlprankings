



















































Learning principled bilingual mappings of word embeddings while preserving monolingual invariance


Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2289–2294,
Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics

Learning principled bilingual mappings of word embeddings while
preserving monolingual invariance

Mikel Artetxe, Gorka Labaka, Eneko Agirre
IXA NLP Group, University of the Basque Country (UPV/EHU)

{mikel.artetxe, gorka.labaka, e.agirre}@ehu.eus

Abstract

Mapping word embeddings of different lan-
guages into a single space has multiple appli-
cations. In order to map from a source space
into a target space, a common approach is to
learn a linear mapping that minimizes the dis-
tances between equivalences listed in a bilin-
gual dictionary. In this paper, we propose
a framework that generalizes previous work,
provides an efficient exact method to learn the
optimal linear transformation and yields the
best bilingual results in translation induction
while preserving monolingual performance in
an analogy task.

1 Introduction

Bilingual word embeddings have attracted a lot of
attention in recent times (Zou et al., 2013; Kočiský
et al., 2014; Chandar A P et al., 2014; Gouws et al.,
2014; Gouws and Søgaard, 2015; Luong et al., 2015;
Wick et al., 2016). A common approach to obtain
them is to train the embeddings in both languages
independently and then learn a mapping that mini-
mizes the distances between equivalences listed in a
bilingual dictionary. The learned transformation can
also be applied to words missing in the dictionary,
which can be used to induce new translations with
a direct application in machine translation (Mikolov
et al., 2013b; Zhao et al., 2015).

The first method to learn bilingual word em-
bedding mappings was proposed by Mikolov et al.
(2013b), who learn the linear transformation that
minimizes the sum of squared Euclidean distances
for the dictionary entries. Subsequent work has pro-
posed alternative optimization objectives to learn

better mappings. Xing et al. (2015) incorporate
length normalization in the training of word embed-
dings and try to maximize the cosine similarity in-
stead, introducing an orthogonality constraint to pre-
serve the length normalization after the projection.
Faruqui and Dyer (2014) use canonical correlation
analysis to project the embeddings in both languages
to a shared vector space.

Beyond linear mappings, Lu et al. (2015) apply
deep canonical correlation analysis to learn a non-
linear transformation for each language. Finally, ad-
ditional techniques have been used to address the
hubness problem in Mikolov et al. (2013b), both
through the neighbor retrieval method (Dinu et al.,
2015) and the training itself (Lazaridou et al., 2015).
We leave the study of non-linear transformation and
other additions for further work.

In this paper, we propose a general framework to
learn bilingual word embeddings. We start with a
basic optimization objective (Mikolov et al., 2013b)
and introduce several meaningful and intuitive con-
straints that are equivalent or closely related to pre-
viously proposed methods (Faruqui and Dyer, 2014;
Xing et al., 2015). Our framework provides a more
general view of bilingual word embedding map-
pings, showing the underlying connection between
the existing methods, revealing some flaws in their
theoretical justification and providing an alterna-
tive theoretical interpretation for them. Our exper-
iments on an existing English-Italian word transla-
tion induction and an English word analogy task
give strong empirical evidence in favor of our the-
oretical reasoning, while showing that one of our
models clearly outperforms previous alternatives.

2289



2 Learning bilingual mappings

Let X and Z denote the word embedding matrices
in two languages for a given bilingual dictionary so
that their ith row Xi∗ and Zi∗ are the word embed-
dings of the ith entry in the dictionary. Our goal is to
find a linear transformation matrix W so that XW
best approximates Z, which we formalize minimiz-
ing the sum of squared Euclidean distances follow-
ing Mikolov et al. (2013b):

arg min
W

∑

i

‖Xi∗W − Zi∗‖2

Alternatively, this is equivalent to minimizing the
(squared) Frobenius norm of the residual matrix:

arg min
W

‖XW − Z‖2F

Consequently, W will be the so called least-
squares solution of the linear matrix equation
XW = Z. This is a well-known problem in lin-
ear algebra and can be solved by taking the Moore-
Penrose pseudoinverse X+ =

(
XTX

)−1
XT as

W = X+Z, which can be computed using SVD.

2.1 Orthogonality for monolingual invariance

Monolingual invariance is needed to preserve the
dot products after mapping, avoiding performance
degradation in monolingual tasks (e.g. analogy).
This can be obtained requiring W to be an orthog-
onal matrix (W TW = I). The exact solution un-
der such orthogonality constraint is given by W =
V UT , where ZTX = UΣV T is the SVD factoriza-
tion of ZTX (cf. Appendix A). Thanks to this, the
optimal transformation can be efficiently computed
in linear time with respect to the vocabulary size.
Note that orthogonality enforces an intuitive prop-
erty, and as such it could be useful to avoid degen-
erated solutions and learn better bilingual mappings,
as we empirically show in Section 3.

2.2 Length normalization for maximum cosine

Normalizing word embeddings in both languages to
be unit vectors guarantees that all training instances
contribute equally to the optimization goal. As long
as W is orthogonal, this is equivalent to maximiz-
ing the sum of cosine similarities for the dictionary

entries, which is commonly used for similarity com-
putations:

arg min
W

∑

i

∥∥∥∥
Xi∗
‖Xi∗‖

W − Zi∗‖Zi∗‖

∥∥∥∥
2

= arg max
W

∑

i

cos (Xi∗W,Zi∗)

This last optimization objective coincides with
Xing et al. (2015), but their work was motivated
by an hypothetical inconsistency in Mikolov et al.
(2013b), where the optimization objective to learn
word embeddings uses dot product, the objective
to learn mappings uses Euclidean distance and the
similarity computations use cosine. However, the
fact is that, as long as W is orthogonal, optimizing
the squared Euclidean distance of length-normalized
embeddings is equivalent to optimizing the cosine,
and therefore, the mapping objective proposed by
Xing et al. (2015) is equivalent to that used by
Mikolov et al. (2013b) with orthogonality constraint
and unit vectors. In fact, our experiments show that
orthogonality is more relevant than length normal-
ization, in contrast to Xing et al. (2015), who intro-
duce orthogonality only to ensure that unit length is
preserved after mapping.

2.3 Mean centering for maximum covariance
Dimension-wise mean centering captures the intu-
ition that two randomly taken words would not be
expected to be semantically similar, ensuring that
the expected product of two random embeddings in
any dimension and, consequently, their cosine sim-
ilarity, is zero. As long as W is orthogonal, this
is equivalent to maximizing the sum of dimension-
wise covariance for the dictionary entries:

arg min
W

‖CmXW − CmZ‖2F

= arg max
W

∑

i

cov (XW∗i, Z∗i)

where Cm denotes the centering matrix
This equivalence reveals that the method pro-

posed by Faruqui and Dyer (2014) is closely re-
lated to our framework. More concretely, Faruqui
and Dyer (2014) use Canonical Correlation Analysis
(CCA) to project the word embeddings in both lan-
guages to a shared vector space. CCA maximizes

2290



the dimension-wise covariance of both projections
(which is equivalent to maximizing the covariance
of a single projection if the transformations are con-
strained to be orthogonal, as in our case) but adds
an implicit restriction to the two mappings, making
different dimensions have the same variance and be
uncorrelated among themselves1:

arg max
A,B

∑

i

cov (XA∗i, ZB∗i)

s.t. ATXTCmXA = BTZTCmZB = I

Therefore, the only fundamental difference be-
tween both methods is that, while our model en-
forces monolingual invariance, Faruqui and Dyer
(2014) do change the monolingual embeddings to
meet this restriction. In this regard, we think that
the restriction they add could have a negative im-
pact on the learning of the bilingual mapping, and
it could also degrade the quality of the monolingual
embeddings. Our experiments (cf. Section 3) show
empirical evidence supporting this idea.

3 Experiments

In this section, we experimentally test the proposed
framework and all its variants in comparison with
related methods. For that purpose, we use the trans-
lation induction task introduced by Mikolov et al.
(2013b), which learns a bilingual mapping on a
small dictionary and measures its accuracy on pre-
dicting the translation of new words. Unfortunately,
the dataset they use is not public. For that reason,
we use the English-Italian dataset on the same task
provided by Dinu et al. (2015)2. The dataset con-
tains monolingual word embeddings trained with the
word2vec toolkit using the CBOW method with neg-
ative sampling (Mikolov et al., 2013a)3. The English
embeddings were trained on a 2.8 billion word cor-
pus (ukWaC + Wikipedia + BNC), while the 1.6 bil-
lion word corpus itWaC was used to train the Italian

1While CCA is typically defined in terms of correlation (thus
its name), correlation is invariant to the scaling of variables, so
it is possible to constrain the canonical variables to have a fixed
variance, as we do, in which case correlation and covariance
become equivalent

2http://clic.cimec.unitn.it/˜georgiana.
dinu/down/

3The context window was set to 5 words, the dimension of
the embeddings to 300, the sub-sampling to 1e-05 and the num-
ber of negative samples to 10

embeddings. The dataset also contains a bilingual
dictionary learned from Europarl, split into a train-
ing set of 5,000 word pairs and a test set of 1,500
word pairs, both of them uniformly distributed in
frequency bins. Accuracy is the evaluation measure.

Apart from the performance of the projected em-
beddings in bilingual terms, we are also interested in
the monolingual quality of the source language em-
beddings after the projection. For that purpose, we
use the word analogy task proposed by Mikolov et
al. (2013a), which measures the accuracy on answer-
ing questions like “what is the word that is similar to
small in the same sense as biggest is similar to big?”
using simple word vector arithmetic. The dataset
they use consists of 8,869 semantic and 10,675 syn-
tactic questions of this type, and is publicly avail-
able4. In order to speed up the experiments, we fol-
low the authors and perform an approximate eval-
uation by reducing the vocabulary size according
to a frequency threshold of 30,000 (Mikolov et al.,
2013a). Since the original embeddings are the same
in all the cases and it is only the transformation that
is applied to them that changes, this affects all the
methods in the exact same way, so the results are
perfectly comparable among themselves. With these
settings, we obtain a coverage of 64.98%.

We implemented the proposed method in Python
using NumPy, and make it available as an open
source project5. The code for Mikolov et al. (2013b)
and Xing et al. (2015) is not publicly available, so
we implemented and tested them as part of the pro-
posed framework, which only differs from the origi-
nal systems in the optimization method (exact solu-
tion instead of gradient descent) and the length nor-
malization approach in the case of Xing et al. (2015)
(postprocessing instead of constrained training). As
for the method by Faruqui and Dyer (2014), we used
their original implementation in Python and MAT-
LAB6, which we extended to cover cases where the
dictionary contains more than one entry for the same
word.

4https://code.google.com/archive/p/
word2vec/

5https://github.com/artetxem/vecmap
6https://github.com/mfaruqui/

crosslingual-cca

2291



EN-IT EN AN.
Original embeddings - 76.66%
Unconstrained mapping 34.93% 73.80%
+ length normalization 33.80% 73.61%
+ mean centering 38.47% 73.71%
Orthogonal mapping 36.73% 76.66%
+ length normalization 36.87% 76.66%
+ mean centering 39.27% 76.59%

Table 1: Our results in bilingual and monolingual tasks.

3.1 Results of our framework

The rows in Table 1 show, respectively, the results
for the original embeddings, the basic mapping pro-
posed by Mikolov et al. (2013b) (cf. Section 2) and
the addition of orthogonality constraint (cf. Section
2.1), with and without length normalization and, in-
crementally, mean centering. In all the cases, length
normalization and mean centering were applied to
all embeddings, even if missing from the dictionary.

The results show that the orthogonality constraint
is key to preserve monolingual performance, and
it also improves bilingual performance by enforc-
ing a relevant property (monolingual invariance) that
the transformation to learn should intuitively have.
The contribution of length normalization alone is
marginal, but when followed by mean centering
we obtain further improvements in bilingual perfor-
mance without hurting monolingual performance.

3.2 Comparison to other work

Table 2 shows the results for our best performing
configuration in comparison to previous work. As
discussed before, (Mikolov et al., 2013b) and (Xing
et al., 2015) were implemented as part of our frame-
work, so they correspond to our uncostrained map-
ping with no preprocessing and orthogonal mapping
with length normalization, respectively.

As it can be seen, the method by Xing et al.
(2015) performs better than that of Mikolov et al.
(2013b) in the translation induction task, which is in
line with what they report in their paper. Moreover,
thanks to the orthogonality constraint their mono-
lingual performance in the word analogy task does
not degrade, whereas the accuracy of Mikolov et al.
(2013b) drops by 2.86% in absolute terms with re-
spect to the original embeddings.

Since Faruqui and Dyer (2014) take advantage of

EN-IT EN AN.
Original embeddings - 76.66%
Mikolov et al. (2013b) 34.93% 73.80%
Xing et al. (2015) 36.87% 76.66%
Faruqui and Dyer (2014) 37.80% 69.64%
Our method 39.27% 76.59%

Table 2: Comparison of our method to other work.

CCA to perform dimensionality reduction, we tested
several values for it and report the best (180 dimen-
sions). This beats the method by Xing et al. (2015)
in the bilingual task, although it comes at the price of
a considerable degradation in monolingual quality.

In any case, it is our proposed method with the
orthogonality constraint and a global preprocessing
with length normalization followed by dimension-
wise mean centering that achieves the best accuracy
in the word translation induction task. Moreover, it
does not suffer from any considerable degradation
in monolingual quality, with an anecdotal drop of
only 0.07% in contrast with 2.86% for Mikolov et
al. (2013b) and 7.02% for Faruqui and Dyer (2014).

When compared to Xing et al. (2015), our results
in Table 1 reinforce our theoretical interpretation
for their method (cf. Section 2.2), as it empirically
shows that its improvement with respect to Mikolov
et al. (2013b) comes solely from the orthogonality
constraint, and not from solving any inconsistency.

It should be noted that the implementation by
Faruqui and Dyer (2014) also length-normalizes the
word embeddings in a preprocessing step. Follow-
ing the discussion in Section 2.3, this means that our
best performing configuration is conceptually very
close to the method by Faruqui and Dyer (2014),
as they both coincide on maximizing the average
dimension-wise covariance and length-normalize
the embeddings in both languages first, the only dif-
ference being that our model enforces monolingual
invariance after the normalization while theirs does
change the monolingual embeddings to make differ-
ent dimensions have the same variance and be un-
correlated among themselves. However, our model
performs considerably better than any configuration
from Faruqui and Dyer (2014) in both the monolin-
gual and the bilingual task, supporting our hypoth-
esis that these two constraints that are implicit in
their method are not only conceptually confusing,

2292



but also have a negative impact.

4 Conclusions

This paper develops a new framework to learn bilin-
gual word embedding mappings, generalizing previ-
ous work and providing an efficient exact method
to learn the optimal transformation. Our experi-
ments show the effectiveness of the proposed model
and give strong empirical evidence in favor of our
reinterpretation of Xing et al. (2015) and Faruqui
and Dyer (2014). It is the proposed method with
the orthogonality constraint and a global preprocess-
ing with length normalization and dimension-wise
mean centering that achieves the best overall results
both in monolingual and bilingual terms, surpassing
those previous methods. In the future, we would like
to study non-linear mappings (Lu et al., 2015) and
the additional techniques in (Lazaridou et al., 2015).

Acknowledgments

This research was partially supported by the Eu-
ropean Commision (QTLeap FP7-ICT-2013-10-
610516), a Google Faculty Award, and the Span-
ish Ministry of Economy and Competitiveness
(TADEEP TIN2015-70214-P). Mikel Artetxe enjoys
a doctoral grant from the Spanish Ministry of Edu-
cation, Culture and Sports.

A Proof of solution under orthogonality

Constraining W to be orthogonal (W TW = I), the
original minimization problem can be reformulated
as follows (cf. Section 2.1):

arg min
W

∑

i

‖Xi∗W − Zi∗‖2

= arg min
W

∑

i

(
‖Xi∗W‖2 + ‖Zi∗‖2 − 2Xi∗WZTi∗

)

= arg max
W

∑

i

Xi∗WZTi∗

= arg max
W

Tr
(
XWZT

)

= arg max
W

Tr
(
ZTXW

)

In the above expression, Tr(·) denotes the trace
operator (the sum of all the elements in the main di-
agonal), and the last equality is given by its cyclic

property. At this point, we can take the SVD of
ZTX as ZTX = UΣV T , so Tr

(
ZTXW

)
=

Tr
(
UΣV TW

)
= Tr

(
ΣV TWU

)
. Since V T ,

W and U are orthogonal matrices, their product
V TWU will also be an orthogonal matrix. In ad-
dition to that, given that Σ is a diagonal matrix,
its trace after an orthogonal transformation will be
maximal when the values in its main diagonal are
preserved after the mapping, that is, when the or-
thogonal transformation matrix is the identity ma-
trix. This will happen when V TWU = I in our
case, so the optimal solution will be W = V UT .

References
Sarath Chandar A P, Stanislas Lauly, Hugo Larochelle,

Mitesh Khapra, Balaraman Ravindran, Vikas C
Raykar, and Amrita Saha. 2014. An autoencoder ap-
proach to learning bilingual word representations. In
Advances in Neural Information Processing Systems
27, pages 1853–1861.

Georgiana Dinu, Angeliki Lazaridou, and Marco Baroni.
2015. Improving zero-shot learning by mitigating
the hubness problem. In Proceedings of the 3rd In-
ternational Conference on Learning Representations
(ICLR2015), workshop track.

Manaal Faruqui and Chris Dyer. 2014. Improving vector
space word representations using multilingual correla-
tion. In Proceedings of the 14th Conference of the Eu-
ropean Chapter of the Association for Computational
Linguistics, pages 462–471.

Stephan Gouws and Anders Søgaard. 2015. Simple task-
specific bilingual word embeddings. In Proceedings
of the 2015 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, pages 1386–1390.

Stephan Gouws, Yoshua Bengio, and Greg Corrado.
2014. Bilbowa: Fast bilingual distributed repre-
sentations without word alignments. arXiv preprint
arXiv:1410.2455.

Tomáš Kočiský, Karl Moritz Hermann, and Phil Blun-
som. 2014. Learning bilingual word representations
by marginalizing alignments. In Proceedings of the
52nd Annual Meeting of the Association for Computa-
tional Linguistics, volume 2, pages 224–229.

Angeliki Lazaridou, Georgiana Dinu, and Marco Baroni.
2015. Hubness and pollution: Delving into cross-
space mapping for zero-shot learning. In Proceed-
ings of the 53rd Annual Meeting of the Association
for Computational Linguistics and the 7th Interna-
tional Joint Conference on Natural Language Process-
ing, volume 1, pages 270–280.

2293



Ang Lu, Weiran Wang, Mohit Bansal, Kevin Gimpel, and
Karen Livescu. 2015. Deep multilingual correlation
for improved word embeddings. In Proceedings of
the 2015 Conference of the North American Chapter
of the Association for Computational Linguistics: Hu-
man Language Technologies, pages 250–256.

Min-Thang Luong, Hieu Pham, and Christopher D. Man-
ning. 2015. Bilingual word representations with
monolingual quality in mind. In NAACL Workshop on
Vector Space Modeling for NLP, pages 151–159.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word representa-
tions in vector space. arXiv preprint arXiv:1301.3781.

Tomas Mikolov, Quoc V Le, and Ilya Sutskever. 2013b.
Exploiting similarities among languages for machine
translation. arXiv preprint arXiv:1309.4168.

Michael Wick, Pallika Kanani, and Adam Pocock. 2016.
Minimally-constrained multilingual embeddings via
artificial code-switching. In Thirtieth AAAI confer-
ence on Artificial Intelligence (AAAI).

Chao Xing, Dong Wang, Chao Liu, and Yiye Lin. 2015.
Normalized word embedding and orthogonal trans-
form for bilingual word translation. In Proceedings
of the 2015 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, pages 1006–1011.

Kai Zhao, Hany Hassan, and Michael Auli. 2015. Learn-
ing translation models from monolingual continuous
representations. In Proceedings of the 2015 Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics: Human Language
Technologies, pages 1527–1536.

Will Y. Zou, Richard Socher, Daniel Cer, and Christo-
pher D. Manning. 2013. Bilingual word embeddings
for phrase-based machine translation. In Proceedings
of the 2013 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1393–1398.

2294


