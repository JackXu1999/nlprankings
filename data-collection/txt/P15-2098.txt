



















































Radical Embedding: Delving Deeper to Chinese Radicals


Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 594–598,

Beijing, China, July 26-31, 2015. c©2015 Association for Computational Linguistics

Radical Embedding: Delving Deeper to Chinese Radicals

Xinlei Shi, Junjie Zhai, Xudong Yang, Zehua Xie, Chao Liu
Sogou Technology Inc., Beijing, China

{shixinlei, zhaijunjie, yangxudong, xiezehua, liuchao}@sogou-inc.com

Abstract

Languages using Chinese characters
are mostly processed at word level. In-
spired by recent success of deep learn-
ing, we delve deeper to character and
radical levels for Chinese language pro-
cessing. We propose a new deep learn-
ing technique, called “radical embed-
ding”, with justifications based on Chi-
nese linguistics, and validate its fea-
sibility and utility through a set of
three experiments: two in-house stan-
dard experiments on short-text catego-
rization (STC) and Chinese word seg-
mentation (CWS), and one in-field ex-
periment on search ranking. We show
that radical embedding achieves com-
parable, and sometimes even better, re-
sults than competing methods.

1 Introduction

Chinese is one of the oldest written languages
in the world, but it does not attract much at-
tention in top NLP research forums, proba-
bly because of its peculiarities and drastic dif-
ferences from English. There are sentences,
words, characters in Chinese, as illustrated in
Figure 1. The top row is a Chinese sentence,
whose English translation is at the bottom. In
between is the pronunciation of the sentence
in Chinese, called PinYin, which is a form of
Romanian phonetic representation of Chinese,
similar to the International Phonetic Alpha-
bet (IPA) for English. Each squared symbol
is a distinct Chinese character, and there are
no separators between characters calls for Chi-
nese Word Segmentation (CWS) techniques to
group adjacent characters into words.

In most current applications (e.g., catego-
rization and recommendation etc.), Chinese is

English:    It is a nice day today.

Pinyin:      jīn tiān/ tiān qì/ zhēn/ hǎo!

Chinese:   !"#"##"$"#%"#&!

a word a character

Figure 1: Illustration of Chinese Language

represented at the word level. Inspired by re-
cent success of delving deep (Szegedy et al.,
2014; Zhang and LeCun, 2015; Collobert et
al., 2011), an interesting question arises then:
can we delve deeper than word level represen-
tation for better Chinese language processing?
If the answer is yes, how deep can it be done
for fun and for profit?

Intuitively, the answer should be positive.
Nevertheless, each Chinese character is seman-
tically meaningful, thanks to its pictographic
root from ancient Chinese as depicted in Fig-
ure 2. We could delve deeper by decomposing
each character into character radicals.

The right part of Figure 2 illustrates the de-
composition. This Chinese character (mean-
ing “morning”) is decomposed into 4 radicals
that consists of 12 strokes in total. In Chi-
nese linguistics, each Chinese character can be
decomposed into no more than four radicals
based on a set of preset rules1. As depicted by
the pictograms in the right part of Figure 2,
the 1st radical (and the 3rd that happens to
be the same) means “grass”, and the 2nd and
the 4th mean the “sun” and the “moon”, re-
spectively. These four radicals altogether con-
vey the meaning that “the moment when sun
arises from the grass while the moon wanes
away”, which is exactly “morning”. On the
other hand, it is hard to decipher the seman-
tics of strokes, and radicals are the minimum
semantic unit for Chinese. Building deep mod-

1http://en.wikipedia.org/wiki/Wubi_method

594



character

pictogram

! " ! #

stroker

Oracle Bone Script 

ca. 1200-1050 BCE

Bronze Script

ca. 800 BCE

Small Seal Script

ca. 220 BCE

Clerical Script

ca. 50 BCE

Regular Script

ca. 200 CE

1
radical

2 3 4

Figure 2: Decomposition of Chinese Character

els from radicals could lead to interesting re-
sults.

In sum, this paper makes the following
three-fold contributions: (1) we propose a
new deep learning technique, called “radical
embedding”, for Chinese language processing
with proper justifications based on Chinese
linguistics; (2) we validate the feasibility and
utility of radical embedding through a set of
three experiments, which include not only two
in-house standard experiments on short-text
categorization (STC) and Chinese word seg-
mentation (CWS), but an in-field experiment
on search ranking as well; (3) this initial suc-
cess of radical embedding could shed some
light on new approaches to better language
processing for Chinese and other languages
alike.

The rest of this paper is organized as fol-
lows. Section 2 presents the radical embed-
ding technique and the accompanying deep
neural network components, which are com-
bined and stacked to solve three application
problems. Section 3 elaborates on the three
applications and reports on the experiment re-
sults. With related work briefly discussed in
Section 4, Section 5 concludes this study. For
clarity, we limit the study to Simplified Chi-
nese in this paper.

2 Deep Networks with Radical
Embeddings

This section presents the radical embedding
technique, and the accompanying deep neu-
ral network components. These components
are combined to solve the three applications
in Section 3.

Word embedding is a popular technique in
NLP (Collobert et al., 2011). It maps words to
vectors of real numbers in a relatively low di-
mensional space. It is shown that the proxim-
ity in this numeric space actually embodies al-
gebraic semantic relationship, such as “Queen

input output

Convolution
f ∈ Rm
k ∈ Rn

y ∈ Rm+n−1
yi =

∑i+n−1
s=i fs · ks−i

0 ≤ i ≤ m− n + 1
Max-pooling x ∈ Rd y = max(x) ∈ R

Lookup
Table

M ∈ Rd×|D|
Ii ∈ R|D|×1

vi = MIi ∈ Rd

Tanh x ∈ Rd
y ∈ Rd

yi =
exi−e−xi
exi+e−xi

0 ≤ i ≤ d− 1
Linear x ∈ Rd y = x ∈ Rd

ReLU x ∈ Rd
y ∈ Rd

yi = 0 if xi ≤ 0
yi = xi if xi > 0

0 ≤ i ≤ d− 1

Softmax x ∈ Rd
y ∈ Rd

yi =
exi∑d

j=1 e
xj

0 ≤ i ≤ d− 1
Concatenate

xi ∈ Rd
0 ≤ i ≤ n− 1

y = (x0, x1, ..., xn−1)
∈ Rd×n

D: radical vocabulary
M : a matrix containing |D| columns, each column
is a d-dimensional vector represent radical in D.
Ii: a one hot vector stands for the ith radical in vocabulary

Table 1: Neural Network Components

− Woman + Man ≈ King” (Mikolov et al.,
2013). As demonstrated in previous work,
this numeric representation of words has led to
big improvements in many NLP tasks such as
machine translation (Sutskever et al., 2014),
question answering (Iyyer et al., 2014) and
document ranking (Shen et al., 2014).

Radical embedding is similar to word em-
bedding except that the embedding is at rad-
ical level. There are two ways of embedding:
CBOW and skip-gram (Mikolov et al., 2013).
We here use CBOW for radical embedding be-
cause the two methods exhibit few differences,
and CBOW is slightly faster in experiments.
Specifically, a sequence of Chinese characters
is decomposed into a sequence of radicals, to
which CBOW is applied. We use the word2vec
package (Mikolov et al., 2013) to train radical
vectors, and then initialize the lookup table
with these radical vectors.

We list the network components in Table 1,
which are combined and stacked in Figure 3
to solve different problems in Section 3. Each
component is a function, the input column of
Table 1 demonstrates input parameters and
their dimensions of these functions, the out-
put column shows the formulas and outputs.

3 Applications and Experiments

In this section, we explain how to stack the
components in Table 1 to solve three prob-
lems: short-text categorization, Chinese word
segmentation and search ranking, respectively.

595



Convolution 1×3

ReLU 256

Lookup Table 30K

Max-Pooling

Short Text

ReLU 256

Linear 128

Softmax 3

LossCal

Input Text

Lookup Table 30K

Concatenate

Tanh 256

ReLU 256

Softmax 2

LossCal

Label 3

Label 2

Query Titlea Titleb

Lookup Table 30K

Convolution 1×3 Convolution 1×3 Convolution 1×3

Linear 100 Linear 100 Linear 100

ReLU 512 ReLU 512 ReLU 512

ReLU 512 ReLU 512 ReLU 512

Linear 256 Linear 256 Linear 256

Max-Pooling Max-Pooling Max-Pooling

LossCal

300

(a) STC (b) CWS (c) Search Ranking

Figure 3: Application Models using Radical Embedding

Accuracy(%)
Competing Methods Deep Neural Networks with Embedding
LR SVM wrd chr rdc wrd+rdc chr+rdc

Finance 93.52 94.06 94.89 95.85 94.75 95.70 95.74
Sports 92.40 92.83 95.10 95.01 92.24 95.87 95.91
Entertainment 91.72 92.24 94.32 94.77 93.21 95.11 94.78

Average 92.55 93.04 94.77 95.21 93.40 95.56 95.46

Table 2: Short Text Categorization Results

3.1 Short-Text Categorization

Figure 3(a) presents the network structure of
the model for short-text categorization, where
the width of each layer is marked out as well.
From the top down, a piece of short text,
e.g., the title of a URL, is fed into the net-
work, which goes through radical decomposi-
tion, table-lookup (i.e., locating the embed-
ding vector corresponding to each radical),
convolution, max pooling, two ReLU layers
and one fully connected layer, all the way to
the final softmax layer, where the loss is cal-
culated against the given label. The stan-
dard back-propagation algorithm is used to
fine tune all the parameters.

The experiment uses the top-3 categories
of the SogouCA and SogouCS news corpus
(Wang et al., 2008). 100,000 samples of each
category are randomly selected for training
and 10,000 for testing. Hyper-parameters
for SVM and LR are selected through cross-
validation. Table 2 presents the accuracy of
different methods, where “wrd”, “chr”, and
“rdc” denote word, character, and radical em-
bedding, respectively. As can be seen, embed-
ding methods outperform competing LR and
SVM algorithms uniformly, and the fusion of
radicals with words and characters improves
both.

3.2 Chinese Word Segmentation

Figure 3(b) presents the CWS network ar-
chitecture. It uses softmax as well because
it essentially classifies whether each charac-
ter should be a segmentation boundary. The
input is firstly decomposed into a radical se-
quence, on which a sliding window of size
3 is applied to extract features, which are
pipelined to downstream levels of the network.

We evaluate the performance using two
standard datasets: PKU and MSR, as pro-
vided by (Emerson, 2005). The PKU dataset
contains 1.1M training words and 104K test
words, and the MSR dataset contains 2.37M
training words and 107K test words. We use
the first 90% sentences for training and the
rest 10% sentences for testing. We compare
radical embedding with the CRF method2,
FNLM (Mansur et al., 2013) and PSA (Zheng
et al., 2013), and present the results in Table
3. Note that no dictionary is used in any of
these algorithms.

We see that the radical embedding (RdE)
method, as the first attempt to segment words
at radical level, actually achieves very compet-
itive results. It outperforms both CRF and
FNLM on both datasets, and is comparable
with PSA.

2http://crfpp.googlecode.com/svn/trunk/doc/
index.html?source=navbar

596



Data Approach Precision Recall F1

PKU
CRF 88.1 86.2 87.1
FNLM 87.1 87.9 87.5
PSA 92.8 92.0 92.4
RdE 92.6 92.1 92.3

MSR
CRF 89.3 87.5 88.4
FNLM 92.3 92.2 92.2
PSA 92.9 93.6 93.3
RdE 93.4 93.3 93.3

Table 3: CWS Result Comparison

3.3 Web Search Ranking

Finally, we report on an in-field experiment
with Web search ranking. Web search lever-
ages many kinds of ranking signals, an impor-
tant one of which is the preference signals ex-
tracted from click-through logs. Given a set of
triplets {query, titlea, titleb} discovered from
click logs, where the URL titlea is preferred
to titleb for the query. The goal of learning is
to produce a matching model between query
and title that maximally agrees with the pref-
erence triplets. This learnt matching model is
combined with other signals, e.g., PageRank,
BM25F, etc. in the general ranking. The deep
network model for this task is depicted in Fig-
ure 3(c), where each triplet goes through seven
layers to compute the loss using Equation (1),
where qi, ai, bi are the output vectors for the
query and two titles right before computing
the loss. The calculated loss is then back prop-
agated to fine tune all the parameters.

m∑
i=1

log

(
1 + exp

(
−c ∗

(
qTi ai

|qi||ai|
− q

T
i bi

|qi||bi|

)))
(1)

The evaluation is carried out on a propri-
etary data set provided by a leading Chi-
nese search engine company. It contains
95,640,311 triplets, which involve 14,919,928
distinct queries and 65,125,732 distinct titles.
95,502,506 triplets are used for training, with
the rest 137,805 triplets as testing. It is worth
noting that the testing triplets are hard cases,
mostly involving long queries and short title
texts.

Figure 4 presents the results, where we vary
the amount of training data to see how the per-
formance varies. The x-axis lists the percent-
age of training dataset used, and 100% means
using the entire training dataset, and the y-
axis is the accuracy of the predicted prefer-
ences. We see that word embedding is over-

dataset percentage (%)
1 5 10 50 100

a
c
c
u

ra
c
y
 (

%
)

54

55

56

57

58

59

60

61
Radical Embedding

Word Embedding

Figure 4: Search Ranking Results

all superior to radical embedding, but it is
interesting to see that word embedding sat-
urates using half of the data, while ranking
with radical embedding catches up using the
entire dataset, getting very close in accuracy
(60.78% vs. 60.47%). Because no more data
is available beyond the 95,640,311 triplets, un-
fortunately we cannot tell if radical embed-
ding would eventually surpass word embed-
ding with more data.

4 Related Work

This paper presents the first piece of work on
embedding radicals for fun and for profit, and
we are mostly inspired by fellow researchers
delving deeper in various domains (Zheng et
al., 2013; Zhang and LeCun, 2015; Collobert
et al., 2011; Kim, 2014; Johnson and Zhang,
2014; dos Santos and Gatti, 2014). For exam-
ple, Huang et al.’s work (Huang et al., 2013) on
DSSM uses letter trigram as the basic repre-
sentation, which somehow resembles radicals.
Zhang and Yann’s recent work (Zhang and Le-
Cun, 2015) represents Chinese at PinYin level,
thus taking Chinese as a western language.
Although working at PinYin level might be
a viable approach, using radicals should be
more reasonable from a linguistic point of
view. Nevertheless, PinYin only represents the
pronunciation, which is arguably further away
from semantics than radicals.

5 Conclusion

This study presents the first piece of evidence
on the feasibility and utility of radical embed-
ding for Chinese language processing. It is in-
spired by recent success of delving deep in var-
ious domains, and roots on the rationale that
radicals, as the minimum semantic unit, could
be appropriate for deep learning. We demon-
strate the utility of radical embedding through

597



two standard in-house and one in-field exper-
iments. While some promising results are ob-
tained, there are still many problems to be ex-
plored further, e.g., how to leverage the lay-
out code in radical decomposition that is cur-
rently neglected to improve performance. An
even more exciting topic could be to train rad-
ical, character and word embedding in a uni-
fied hierarchical model as they are naturally
hierarchical. In sum, we hope this preliminary
work could shed some light on new approaches
to Chinese language processing and other lan-
guages alike.

References

Ronan Collobert, Jason Weston, Léon Bot-
tou, Michael Karlen, Koray Kavukcuoglu, and
Pavel P. Kuksa. 2011. Natural language pro-
cessing (almost) from scratch. Journal of Ma-
chine Learning Research, 12:2493–2537.

Ćıcero Nogueira dos Santos and Maira Gatti. 2014.
Deep convolutional neural networks for senti-
ment analysis of short texts. In 25th Inter-
national Conference on Computational Linguis-
tics, Proceedings of the Conference: Technical
Papers, pages 69–78.

Thomas Emerson. 2005. The second international
chinese word segmentation bakeoff. In Proceed-
ings of the fourth SIGHAN workshop on Chinese
language Processing, volume 133.

Mohit Iyyer, Jordan Boyd-Graber, Leonardo
Claudino, Richard Socher, and Hal Daumé III.
2014. A neural network for factoid question an-
swering over paragraphs. In Proceedings of the
2014 Conference on Empirical Methods in Nat-
ural Language Processing, pages 633–644.

Rie Johnson and Tong Zhang. 2014. Effective use
of word order for text categorization with convo-
lutional neural networks. CoRR, abs/1412.1058.

Yoon Kim. 2014. Convolutional neural networks
for sentence classification. In Proceedings of the
2014 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1746–1751.

Mairgup Mansur, Wenzhe Pei, and Baobao Chang.
2013. Feature-based neural language model and
chinese word segmentation. In Sixth Interna-
tional Joint Conference on Natural Language
Processing, 2013, Nagoya, Japan, October 14-
18, 2013, pages 1271–1277.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Gre-
gory S. Corrado, and Jeffrey Dean. 2013. Dis-
tributed representations of words and phrases

and their compositionality. In Advances in Neu-
ral Information Processing Systems, pages 3111–
3119.

Yelong Shen, Xiaodong He, Jianfeng Gao, Li Deng,
and Gregoire Mesnil. 2014. A latent seman-
tic model with convolutional-pooling structure
for information retrieval. In Proceedings of the
23rd ACM International Conference on Confer-
ence on Information and Knowledge Manage-
ment, pages 101–110.

Ilya Sutskever, Oriol Vinyals, and Quoc VV Le.
2014. Sequence to sequence learning with neural
networks. In Advances in Neural Information
Processing Systems, pages 3104–3112.

Christian Szegedy, Wei Liu, Yangqing Jia, Pierre
Sermanet, Scott Reed, Dragomir Anguelov, Du-
mitru Erhan, Vincent Vanhoucke, and Andrew
Rabinovich. 2014. Going deeper with convolu-
tions. CoRR, abs/1409.4842.

Canhui Wang, Min Zhang, Shaoping Ma, and
Liyun Ru. 2008. Automatic online news is-
sue construction in web environment. In Pro-
ceedings of the 17th International Conference on
World Wide Web, pages 457–466.

Xiang Zhang and Yann LeCun. 2015. Text under-
standing from scratch. CoRR, abs/1502.01710.

Xiaoqing Zheng, Hanyang Chen, and Tianyu Xu.
2013. Deep learning for chinese word segmen-
tation and POS tagging. In Proceedings of the
2013 Conference on Empirical Methods in Nat-
ural Language Processing, pages 647–657.

598


