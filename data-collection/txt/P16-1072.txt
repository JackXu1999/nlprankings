



















































Bidirectional Recurrent Convolutional Neural Network for Relation Classification


Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 756–765,
Berlin, Germany, August 7-12, 2016. c©2016 Association for Computational Linguistics

Bidirectional Recurrent Convolutional Neural Network for Relation
Classification

Rui Cai, Xiaodong Zhang and Houfeng Wang∗
Key Laboratory of Computational Linguistics (Ministry of Education),

School of EECS, Peking University, Beijing, 100871, China
Collaborative Innovation Center for Lanuage Ability, Xuzhou, Jiangsu, 221009, China

{cairui, zxdcs, wanghf}@pku.edu.cn

Abstract

Relation classification is an important se-
mantic processing task in the field of natu-
ral language processing (NLP). In this pa-
per, we present a novel model BRCNN
to classify the relation of two entities in
a sentence. Some state-of-the-art systems
concentrate on modeling the shortest de-
pendency path (SDP) between two entities
leveraging convolutional or recurrent neu-
ral networks. We further explore how to
make full use of the dependency relations
information in the SDP, by combining
convolutional neural networks and two-
channel recurrent neural networks with
long short term memory (LSTM) units.
We propose a bidirectional architecture
to learn relation representations with di-
rectional information along the SDP for-
wards and backwards at the same time,
which benefits classifying the direction of
relations. Experimental results show that
our method outperforms the state-of-the-
art approaches on the SemEval-2010 Task
8 dataset.

1 Introduction

Relation classification aims to classify the seman-
tic relations between two entities in a sentence.
For instance, in the sentence “The [burst]e1 has
been caused by water hammer [pressure]e2”, en-
tities burst and pressure are of relation Cause-
Effect(e2, e1). Relation classification plays a key
role in robust knowledge extraction, and has be-
come a hot research topic in recent years.

Nowadays, deep learning techniques have made
significant improvement in relation classification,

∗Corresponding author

compared with traditional relation classification
approaches focusing on designing effective fea-
tures (Rink and Harabagiu, 2010) or kernels (Ze-
lenko et al., 2003; Bunescu and Mooney, 2005)
Although traditional approaches are able to exploit
the symbolic structures in sentences, they still suf-
fer from the difficulty to generalize over the un-
seen words. Some recent works learn features
automatically based on neural networks (NN),
employing continuous representations of words
(word embeddings). The NN research for relation
classification has centered around two main net-
work architectures: convolutional neural networks
and recursive/recurrent neural networks. Convo-
lutional neural network aims to generalize the lo-
cal and consecutive context of the relation men-
tions, while recurrent neural networks adaptively
accumulate the context information in the whole
sentence via memory units, thereby encoding the
global and possibly unconsecutive patterns for re-
lation classification. Socher et al. (2012) learned
compositional vector representations of sentences
with a recursive neural network. Kazuma et al.
(2013) proposed a simple customizaition of recur-
sive neural networks. Zeng et al. (2014) proposed
a convolutional neural network with position em-
beddings.

Recently, more attentions have been paid to
modeling the shortest dependency path (SDP)
of sentences. Liu et al. (2015) developed a
dependency-based neural network, in which a con-
volutional neural network has been used to capture
features on the shortest path and a recursive neural
network is designed to model subtrees. Xu et al.
(2015b) applied long short term memory (LSTM)
based recurrent neural networks (RNNs) along the
shortest dependency path. However, SDP is a spe-
cial structure in which every two neighbor words
are separated by a dependency relations. Previous
works treated dependency relations in the same

756



Figure 1: The shortest dependency path representation for an example sentence from SemEval-08.

way as words or some syntactic features like part-
of-speech (POS) tags, because of the limitations of
convolutional neural networks and recurrent neu-
ral networks. Our first contribution is that we
propose a recurrent convolutional neural network
(RCNN) to encode the global pattern in SDP uti-
lizing a two-channel LSTM based recurrent neural
network and capture local features of every two
neighbor words linked by a dependency relation
utilizing a convolution layer.

We further observe that the relationship be-
tween two entities are directed. For instance, Fig-
ure 1 shows that the shortest path of the sentence
“The [burst]e1 has been caused by water ham-
mer [pressure]e2 .” corresponds to relation Cause-
Effect(e2, e1). The SDP of the sentence also corre-
sponds to relation Cause-Effect(e2, e1), where e1
refers to the entity at front end of SDP and e2 refers
to the entity at back end of SDP, and the inverse
SDP corresponds to relation Cause-Effect(e1, e2).
Previous work (Xu et al., 2015b) simply trans-
forms a (K+1)-relation task into a (2K + 1) clas-
sification task, where 1 is the Other relation and
K is the number of directed relations. Besides, the
recurrent neural network is a biased model, where
later inputs are more dominant than earlier inputs.
It could reduce the effectiveness when it is used to
capture the semantics of a whole shortest depen-
dency path, because key components could appear
anywhere in a SDP rather than the end.

Our second contribution is that we propose a
bidirectional recurrent convolutional neural net-
works (BRCNN) to learn representations with
bidirectional information along the SDP forwards
and backwards at the same time, which also
strengthen the ability to classifying directions of
relationships between entities. Experimental re-
sults show that the bidirectional mechanism sig-
nificantly improves the performance.

We evaluate our method on the SemEval-2010
relation classification task, and achieve a state-of-
the-art F1-score of 86.3%.

2 The Proposed Method

In this section, we describe our method in detail.
Subsection 2.1 provides an overall picture of our
BCRNN model. Subsection 2.2 presents the ra-
tionale of using SDPs and some characteristics of
SDP. Subsection 2.3 describes the two-channel re-
current neural network, and bidirectional recurrent
convolutional neural network is introduced in Sub-
section 2.4. Finally, we present our training objec-
tive in Subsection 2.5.

2.1 Framework

Our BCRNN model is used to learn representa-
tions with bidirectional information along the SDP
forwards and backwards at the same time. Figure
2 depicts the overall architecture of the BRCNN
model.

Given a sentence and its dependency tree, we
build our neural network on its SDP extracted
from the tree. Along the SDP, two recurrent neural
networks with long short term memory units are
applied to learn hidden representations of words
and dependency relations respectively. A convo-
lution layer is applied to capture local features
from hidden representations of every two neigh-
bor words and the dependency relations between
them. A max pooling layer thereafter gathers in-
formation from local features of the SDP or the
inverse SDP. We have a so f tmax output layer af-
ter pooling layer for classification in the unidirec-
tional model RCNN.

On the basis of RCNN model, we build a bidi-
rectional architecture BRCNN taking the SDP and
the inverse SDP of a sentence as input. Dur-
ing the training stage of a (K+1)-relation task,

757



two fine-grained so f tmax classifiers of RCNNs do
a (2K + 1)-class classification respectively. The
pooling layers of two RCNNs are concatenated
and a coarse-grained so f tmax output layer is fol-
lowed to do a (K + 1)-class classification. The fi-
nal (2K+1)-class distribution is the combination of
two (2K+1)-class distributions provided by fine-
grained classifiers respectively during the testing
stage.

2.2 The Shortest Dependency Path

If e1 and e2 are two entities mentioned in the same
sentence such that they are observed to be in a re-
lationship R, the shortest path between e1 and e2
condenses most illuminating information for the
relationship R(e1, e2). It is because (1) if entities
e1 and e2 are arguments of the same predicate,
the shortest path between them will pass through
the predicate; (2) if e1 and e2 belong to different
predicate-argument structures that share a com-
mon argument, the shortest path will pass through
this argument.

Bunescu and Mooney (2005) first used short-
est dependency paths between two entities to cap-
ture the predicate-argument sequences, which pro-
vided strong evidence for relation classification.
Xu et al. (2015b) captured information from the
sub-paths separated by the common ancestor node
of two entities in the shortest paths. However, the
shortest dependency path between two entities is
usually short (∼4 on average) , and the common
ancestor of some SDPs is e1 or e2, which leads to
imbalance of two sub-paths.

We observe that, in the shortest dependency
path, each two neighbor words wa and wb are
linked by a dependency relation rab. The depen-
dency relations between a governing word and its
children make a difference in meaning. Besides, if
we inverse the shortest dependency path, it corre-
sponds to the same relationship with an opposite
direction. For example , in Figure 1, the short-
est path is composed of some sub-structure like

“burst
nsub jpass−−−−−−−−→ caused”. Following the above

intuition, we design a bidirectional recurrent con-
volutional neural network, which can capture fea-
tures from the local substructures and inversely at
the same time.

2.3 Two-Channel Recurrent Neural Network
with Long Short Term Memory Units

The recurrent neural network is suitable for mod-
eling sequential data, as it keeps hidden state vec-
tor h, which changes with input data at each step
accordingly. We make use of words and depen-
dency relations along the SDP for relations classi-
fication (Figure 2). We call them channels as these
information sources do not interact during recur-
rent propagation. Each word and dependency rela-
tion in a given sentence is mapped to a real-valued
vector by looking up in a embedding table. The
embeddings of words are trained on a large cor-
pus unsupervisedly and are thought to be able to
capture their syntactic and semantic information,
and the embeddings of dependency relations are
initialized randomly.

The hidden state ht, for the t-th input is a func-
tion of its previous state ht−1 and the embedding
xt of current input. Traditional recurrent networks
have a basic interaction, that is, the input is lin-
early transformed by a weight matrix and non-
linearly squashed by an activation function. For-
mally, we have

ht = f (Win · xt + Wrec · ht−1 + bh) (1)

where Win and Wrec are weight matrices for the
input and recurrent connections, respectively. bh
is a bias term for the hidden state vector, and f a
non-linear activation function.

It was difficult to train RNNs to capture long-
term dependencies because the gradients tend to
either vanish or explode. Therefore, some more
sophisticated activation function with gating units
were designed. Long short term memory units are
proposed in Hochreiter and Schmidhuber (1997)
to overcome this problem. The main idea is to in-
troduce an adaptive gating mechanism, which de-
cides the degree to which LSTM units keep the
previous state and memorize the extracted features
of the current data input. Many LSTM variants
have been proposed. We adopt in our method
a variant introduced by Zaremba and Sutskever
(2014). Concretely, the LSTM-based recurrent
neural network comprises four components: an in-
put gate it, a forget gate ft, an output gate ot, and a
memory cell ct.

First, we compute the values for it, the input
gate, and gt the candidate value for the states of

758



 

burst     nsubjpass    caused     prep      by      pobj      pressure 

LSTM LSTM LSTM LSTM 

LSTM LSTM LSTM 

LSTM LSTM LSTM 

LSTM LSTM LSTM LSTM 

Input 

Lookup Table 

Pool 
Two-channel 

LSTM 

Two-channel  

LSTM 

Convolution 

Convolution 

fine-grained 

softmax 

fine-grained 

softmax 

coarse-graind 

softmax 

forwards 

backwards 

Figure 2: The overall architecture of BRCNN. Two-Channel recurrent neural networks with LSTM units
pick up information along the shortest dependency path, and inversely at the same time. Convolution
layers are applied to extract local features from the dependency units.

the memory cells at time t:

it = σ(Wi · xt + Ui · ht−1 + bi) (2)
gt = tanh(Wc · xt + Uc · ht−1 + bc) (3)

Second, we compute the value for ft, the activa-
tions of the memory cells’ forget gates at time t:

ft = σ(W f · xt + U f · ht−1 + b f ) (4)

Given the value of the input gate activations it,
the forget gate activation ft and the candidate state
value gt, we can compute ct the memory cells’ new
state at time t:

ct = it ⊗ gt + ft ⊗ ct−1 (5)

With the new state of the memory cells, we can
compute the value of their output gates and, sub-
sequently, their outputs:

ot = σ(Wo · xt + Uo · ht−1 + bo) (6)
ht = ot ⊗ tanh(ct) (7)

In the above equations, σ denotes a sigmoid
function; ⊗ denotes element-wise multiplication.

2.4 Bidirectional Recurrent Convolutional
Neural Network

We observe that a governing word wa and its chil-
dren wb are linked by a dependency relation rab,
which makes a difference in meaning. For exam-

ple, “kills
nsub j−−−−→ it” is distinct from “kills dob j−−−→

it”. The shortest dependency path is composed of
many substructures like “wa

rab−−→ wb”, which are
hereinafter referred to as “dependency unit”. Hid-
den states of words and dependency relations in
the SDP are obtained, utilizing two-channel recur-
rent neural network. The hidden states of wa, wb
and rab are ha, hb and h′ab, and the hidden state of
the dependency unit dab is [ha ⊕ h′ab ⊕ hb], where⊕ denotes concatenate operation. Local features
Lab for the dependency unit dab can be extracted,
utilizing a convolution layer upon the two-channel
recurrent neural network . Formally, we have

Lab = f (Wcon · [ha ⊕ h′ab ⊕ hb] + bcon) (8)
where Wcon is the weight matrix for the convo-
lution layer and bcon is a bias term for the hid-
den state vector. f is a non-linear activation
function(tanh is used in our model). A pooling
layer thereafter gather global information G from

759



local features of dependency units, which is de-
fined as

G =
D

max
d=1

Ld (9)

where the max function is an element-wise func-
tion, and D is the number of dependency units in
the SDP.

The advantage of two-channel recurrent neural
network is the ability to better capture the con-
textual information, adaptively accumulating the
context information the whole path via memory
units. However, the recurrent neural network is
a biased model, where later inputs are more dom-
inant than earlier inputs. It could reduce the ef-
fectiveness when it is used to capture features for
relation classification, for the entities are located
at both ends of SDP and key components could
appear anywhere in a SDP rather than at the end.
We tackle the problem with Bidirectional Convo-
lutional Recurrent Neural Network.

On the basis of observation, we make a hypoth-
esis that SDP is a symmetrical structure. For ex-
ample, if there is a forward shortest path

−→
S which

corresponds to relation Rx(e1, e2), the backward
shortest path

←−
S can be obtained by inversing

−→
S ,

and
←−
S corresponds to Rx(e2, e1), and both

−→
S and←−

S correspond to relation Rx.
As shown in Figure 2, two RCNNs pick up in-

formation along
−→
S and

←−
S , obtaining global repre-

sentations
−→
G and

←−
G . A representation with bidi-

rectional information is obtained by concatenating−→
G and

←−
G . A coarse-grained so f tmax classifier is

used to predict a (K+1)-class distribution y. For-
mally,

y = so f tmax(Wc · [←−G ⊕ −→G] + bc) (10)
Where Wc is the transformation matrix and bc is
the bias vector. Coarse-grained classifier makes
use of representation with bidirectional informa-
tion ignoring the direction of relations, which
learns the inherent correlation between the same
directed relations with opposite directions, such as
Rx(e1, e2) and Rx(e2, e1).

Two fine-grained so f tmax classifiers are ap-
plied to

−→
G and

←−
G with linear transformation to

give the (2K+1)-class distribution −→y and ←−y re-
spectively. Formally,

−→y = so f tmax(W f · −→G + b f ) (11)
←−y = so f tmax(W f · ←−G + b f ) (12)

where W f is the transformation matrix and b f is

the bias vector. Classifying
−→
S and

←−
S respecitvely

at the same time can strengthen the model ability
to judge the direction of relations.

2.5 Training Objective
The (K + 1)-class so f tmax classifier is used to es-
timate probability that

−→
S and

←−
S are of relation

R . The two (2K + 1)-class so f tmax classifiers
are used to estimate the probability that

−→
S and

←−
S

are of relation
−→
R and

←−
R respectively. For a single

data sample, the training objective is the penalized
cross-entropy of three classifiers, given by

J =
2K+1∑
i=1

−→t i log−→y i +
2K+1∑
i=1

←−t i log←−y i

+

K∑
i=1

ti log yi + λ · ||θ||2
(13)

where t ∈ RK+1, −→t and←−t ∈ R2K+1, indicating the
one-hot represented ground truth. y, −→y and←−y are
the estimated probabilities for each class described
in section 2.4. θ is the set of model parameters to
be learned, and λ is a regularization coefficient.

For decoding (predicting the relation of an un-
seen sample), the bidirectional model provides the
(2K+1)-class distribution −→y and ←−y . The final
(2K+1)-class distribution ytest becomes the com-
bination of −→y and←−y . Formally,

ytest = α · −→y + (1 − α) · z(←−y ) (14)
where α is the fraction of the composition of dis-
tributions, which is set to the value 0.65 according
to the performance on validation dataset. During
the implementation of BRCNN, elements in two
class distributions at the same position are not cor-
responding, e.g. Cause-Effect(e1, e2) in −→y should
correspond to Cause-Effect(e2, e1) in ←−y . We ap-
ply a function z to transform←−y to a corresponding
forward distribution like −→y .
3 Experiments

3.1 Dataset
We evaluated our BRCNN model on the SemEval-
2010 Task 8 dataset, which is an established
benchmark for relation classification (Hendrickx
et al., 2010). The dataset contains 8000 sentences
for training, and 2717 for testing. We split 800
samples out of the training set for validation.

760



Classifier Additional Information F1

SVM
POS, WordNet, Prefixes and other morphological features,

82.2
(Rink and Harabagiu, 2010)

dependency parse, Levin classed, PropBank, FanmeNet,
NomLex-Plus, Google n-gram, paraphrases, TextRunner

RNN Word embeddings 74.8
(Socher et al., 2011) + POS, NER, WordNet 77.6

MVRNN Word embeddings 79.1
(Socher et al., 2012) + POS, NER, WordNet 82.4

CNN Word embeddings 69.7
(Zeng et al., 2014) + word position embeddings, WordNet 82.7

FCM Word embeddings 80.6
(Yu et al., 2014) + dependency parsing, NER 83.0

CR-CNN Word embeddings 82.8
(dos Santos et al., 2015) + word position embeddings 84.1

SDP-LSTM Word embeddings 82.4
(Xu et al., 2015b) + POS + GR + WordNet embeddings 83.7

DepNN Word embeddings, WordNet 83.0
(Liu et al., 2015) Word embeddings, NER 83.6

depLCNN Word embeddings, WordNet, word around nominals 83.7
(Xu et al., 2015a) + negative sampling from NYT dataset 85.6

BRCNN Word embeddings 85.4
(Our Model) + POS, NER, WordNet embeddings 86.3

Table 1: Comparison of relation classification systems.

The dataset has (K+1)=10 distinguished rela-
tions, as follows.

• Cause-Effect
• Component-Whole
• Content-Container
• Entity-Destination
• Entity-Origin
• Message-Topic
• Member-Collection
• Instrument-Agency
• Product-Agency
• Other
The former K=9 relations are directed, whereas

the Other class is undirected, we have (2K+1)=19
different classes for 10 relations. All baseline
systems and our model use the official macro-
averaged F1-score to evaluate model performance.
This official measurement excludes the Other rela-
tion.

3.2 Hyperparameter Settings
In our experiment, word embeddings were 200-
dimensional as used in (Yu et al., 2014), trained on

Gigaword with word2vec (Mikolov et al., 2013).
Embeddings of relation are 50-dimensional and
initialized randomly. The hidden layers in each
channel had the same number of units as their
embeddings (200 or 50). The convolution layer
was 200-dimensional. The above values were cho-
sen according to the performance on the validation
dataset.

As we can see in Figure 1, dependency relation
r “

prep−−−→” in −→S becomes r−1 “ prep←−−−” in ←−S . Exper-
iment results show that, the performance of BR-
CNN is improved if r and r−1 correspond to differ-
ent relations embeddings rather than a same em-
bedding. We notice that dependency relations con-
tain much fewer symbols than the words contained
in the vocabulary, and we initialize the embed-
dings of dependency relations randomly for they
can be adequately tuned during supervised train-
ing.

We add l2 penalty for weights with coefficient
10−5, and dropout of embeddings with rate 0.5.
We applied AdaDelta for optimization (Zeiler,
2012), where gradients are computed with an
adaptive learning rate.

761



3.3 Results

Table 1 compares our BRCNN model with other
state-of-the-art methods. The first entry in the
table presents the highest performance achieved
by traditional feature-based methods. Rink and
Harabagiu. (2010) fed a variety of handcrafted
features to the SVM classifier and achieve an F1-
score of 82.2%.

Recent performance improvements on this
dataset are mostly achieved with the help of neu-
ral networks. Socher et al. (2012) built a recur-
sive neural network on the constituency tree and
achieved a comparable performance with Rink and
Harabagiu. (2010). Further, they extended their
recursive network with matrix-vector interaction
and elevated the F1 to 82.4%. Xu et al. (2015b)
first introduced a type of gated recurrent neural
network (LSTM) into this task and raised the F1-
score to 83.7%.

From the perspective of convolution, Zeng et
al. (2014) constructed a CNN on the word se-
quence; they also integrated word position em-
beddings, which helped a lot on the CNN archi-
tecture. dos Santos et al. (2015) proposed a
similar CNN model, named CR-CNN, by replac-
ing the common so f tmax cost function with a
ranking-based cost function. By diminishing the
impact of the Other class, they have achieved an
F1-score of 84.1%. Along the line of CNNs, Xu
et al. (2015a) designed a simple negative sam-
pling method, which introduced additional sam-
ples from other corpora like the NYT dataset. Do-
ing so greatly improved the performance to a high
F1-score of 85.6%. Liu et al. (2015) proposed a
convolutional neural network with a recursive neu-
ral network designed to model the subtrees, and
achieve an F1-score of 83.6%.

Without the use of neural networks, Yu et al.
(2014) proposed a Feature-based Compositional
Embedding Model (FCM), which combined un-
lexicalized linguistic contexts and word embed-
dings. They achieved an F1-score of 83.0%.

We make use of three types of information
to improve the performance of BRCNN: POS
tags, NER features and WordNet hypernyms.
Our proposed BRCNN model yields an F1-score
of 86.3%, outperforming existing competing ap-
proaches. Without using any human-designed
features, our model still achieve an F1-score of
85.4%, while the best performance of state-of-the-
art methods is 84.1% (dos Santos et al., 2015).

3.4 Analysis
Table 2 compares our RCNN model with CNNs
and RNNs.

Model F1
CNN 81.8
LSTM 76.6
Two-channel LSTM 81.5
RCNN 82.4

Table 2: Comparing RCNN with CNNs and
RNNS.

For a fair comparison, hyperparameters are set
according to the performance on validation dataset
as BRCNN . CNN with embeddings of words, po-
sitions and dependency relations as input achieves
an F1-score of 81.8%. LSTM with word em-
beddings as input only achieves an F1-score of
76.6%, which proves that dependency relations
in SDPs play an important role in relation clas-
sification. Two-channel LSTM concatenates the
pooling layers of words and dependency relations
along the shortest dependency path, achieves an
F1-score of 81.5% which is still lower than CNN.
RCNN captures features from dependency units
by combining the advantages of CNN and RNN,
and achieves an F1-score of 82.4%.

Model Input F1
RCNN

−→
S of all relations 82.4

Bi-RCNN
−→
S and

←−
S of all relations 81.2

Bi-RCNN
−→
S and

←−
S of directed relations ,

84.9−→
S of Other

BRCNN
−→
S and

←−
S of directed relations, 85.4−→

S of Other

Table 3: Comparing different variants of our
model.

Bi-RCNN is a variant of BRCNN, which
doesn’t have the coarse-grained classifier.

−→
S

and
←−
S are shortest dependency paths described

in section 2.4. As shown in Table 3, if we in-
verted the SDP of all relations as input, we ob-
serve a performance degradation of 1.2% com-
pared with RCNN. As mentioned in section 3.1,
the SemEval-2010 task 8 dataset contains an undi-
rected class Other in addition to 9 directed rela-
tions(18 classes). For bidirectional model, it is
natural that the inversed Other relation is also in

762



the Other class itself. However, the class Other is
used to indicate that relation between two nom-
inals dose not belong to any of the 9 directed
classes. Therefore, the class Other is very noisy
since it groups many different types of relations
with different directions.

On the basis of the analysis above, we only in-
verse the SDP of directed relations. A significant
improvement is observed and Bi-RCNN achieves
an F1-score of 84.9%. This proves bidirectional
representations provide more useful information
to classify directed relations. We can see that our
model still benefits from the coarse-grained classi-
fication, which can help our model learn inherent
correlation between directed relations with oppo-
site directions. Compared with Bi-RCNN classi-
fying

−→
S and

←−
S into 19 classes separately, BRCNN

also conducts a 10 classes (9 directed relations
and Other) classification and improves 0.5% in
F1-score. Beyond the relation classification task,
we believe that our bidirectional method is gen-
eral technique, which is not restricted in a specific
dataset and has the potential to benefit other NLP
tasks.

4 Related Work

Relation classification is an important topic in
NLP. Traditional Methods for relation classifica-
tion mainly fall into three classes: feature-based,
kernel-based and neural network-based.

In feature-based approaches, different types
of features are extracted and fed into a classi-
fier. Generally, three types of features are often
used. Lexical features concentrate on the enti-
ties of interest, e.g., POS. Syntactic features in-
clude chunking, parse trees, etc. Semantic fea-
tures are exemplified by the concept hierarchy, en-
tity class. Kambhatla (2004) used a maximum en-
tropy model for feature combination. Rink and
Harabagiu (2010) collected various features, in-
cluding lexical, syntactic as well as semantic fea-
tures.

In kernel based methods, similarity between
two data samples is measured without explicit fea-
ture representation. Bunescu and Mooney (2005)
designed a kernel along the shortest dependency
path between two entities by observing that the
relation strongly relies on SDPs. Wang (2008)
provided a systematic analysis of several kernels
and showed that relation extraction can benefit
from combining convolution kernel and syntactic

features. Plank and Moschitti (2013) combined
structural information and semantic information in
a tree kernel. One potential difficulty of kernel
methods is that all data information is completely
summarized by the kernel function, and thus de-
signing an effective kernel becomes crucial.

Recently, deep neural networks are playing an
important role in this task. Socher et al. (2012) in-
troduced a recursive neural network model that as-
signs a matrix-vector representation to every node
in a parse tree, in order to learn compositional vec-
tor representations for sentences of arbitrary syn-
tactic type and length.

Convolutional neural works are widely used
in relation classification. Zeng et al. (2014)
proposed an approach for relation classification
where sentence-level features are learned through
a CNN, which has word embedding and position
features as its input. In parallel, lexical features
were extracted according to given nouns. dos San-
tos et al. (2015) tackled the relation classification
task using a convolutional neural network and pro-
posed a new pairwise ranking loss function, which
achieved the state-of-the-art result in SemEval-
2010 Task 8.

Yu et al. (2014) proposed a Factor-based Com-
positional Embedding Model (FCM) by deriving
sentence-level and substructure embeddings from
word embeddings, utilizing dependency trees and
named entities. It achieved slightly higher accu-
racy on the same dataset than Zeng et al. (2014),
but only when syntactic information is used.

Nowadays, many works concentrate on extract-
ing features from the SDP based on neural net-
works. Xu et al. (2015a) learned robust rela-
tion representations from SDP through a CNN,
and proposed a straightforward negative sampling
strategy to improve the assignment of subjects and
objects. Liu et al. (2015) proposed a recursive
neural network designed to model the subtrees,
and CNN to capture the most important features
on the shortest dependency path. Xu et al. (2015b)
picked up heterogeneous information along the
left and right sub-path of the SDP respectively,
leveraging recurrent neural networks with long
short term memory units. We propose BRCNN to
model the SDP, which can pick up bidirectional in-
formation with a combination of LSTM and CNN.

763



5 Conclusion

In this paper, we proposed a novel bidirectional
neural network BRCNN, to improve the perfor-
mance of relation classification. The BRCNN
model, consisting of two RCNNs, learns features
along SDP and inversely at the same time. In-
formation of words and dependency relations are
used utilizing a two-channel recurrent neural net-
work with LSTM units. The features of depen-
dency units in SDP are extracted by a convolution
layer.

We demonstrate the effectiveness of our model
by evaluating the model on SemEval-2010 rela-
tion classification task. RCNN achieves a better
performance at learning features along the short-
est dependency path, compared with some com-
mon neural networks. A significant improvement
is observed when BRCNN is used, outperforming
state-of-the-art methods.

6 Acknowledgements

Our work is supported by National Natural Sci-
ence Foundation of China (No.61370117 & No.
61433015) and Major National Social Science
Fund of China (No. 12&ZD227).

References
Razvan C Bunescu and Raymond J Mooney. 2005. A

shortest path dependency kernel for relation extrac-
tion. In Proceedings of the conference on Human
Language, pages 724–731.

Cıcero Nogueira dos Santos, Bing Xiang, and Bowen
Zhou. 2015. Classifying relations by ranking with
convolutional neural networks. In In Proceedings
of the 53rd Annual Meeting of the Association for
Computational Linguistics and the 7th International
Joint Conference, pages 626–634.

Kazuma Hashimoto, Makoto Miwa, Yoshimasa Tsu-
ruoka, and Takashi Chikayama. 2013. Simple cus-
tomization of recursive neural networks for semantic
relation classification. In Proceedings of the 2013
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1372–1376.

Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva,
Preslav Nakov, Diarmuid Ó Séaghdha, Sebastian
Padó, Marco Pennacchiotti, Lorenza Romano, and
Stan Szpakowicz. 2010. Semeval-2010 task
8: Multi-way classification of semantic relations
between pairs of nominals. In Proceedings of
the Workshop on Semantic Evaluations: Recent
Achievements and Future Directions. Association
for Computational Linguistics, pages pages 94–99.

Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long
short-term memory. In Neural computation, pages
1735–1780.

Nanda Kambhatla. 2004. Combining lexical, syntac-
tic, and semantic features with maximum entropy
models for extracting relations. In Proceedings of
the ACL 2004 on Interactive poster and demonstra-
tion sessions, page 22. Association for Computa-
tional Linguistics.

Yang Liu, Furu Wei, Sujian Li, Heng Ji, Ming Zhou,
and Houfeng Wang. 2015. A dependency-based
neural network for relation classification. In Pro-
ceedings of the 53rd Annual Meeting of the Associa-
tion for Computational Joint Conference on Natural
Language Processing and the 7th International Joint
Conference on Natural Language Processing, pages
285–290.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems, pages 3111–3119.

Barbara Plank and Alessandro Moschitti. 2013. Em-
beddings semantic similarity in tree kernels for do-
main adaption of relation extraction. In Proceedings
of the 51st Annual Meeting of the Association for
Computational Linguistics, pages 1498–1507.

Bryan Rink and Sanda Harabagiu. 2010. Utd: Clas-
sifying semantic relations by combining lexical and
semantic resources. In Proceedings of the 5th Inter-
national Workshop on Semantic Evaluation, pages
256–259. Association for Computational Linguis-
tics.

Richard Socher, Jeffrey Pennington, Eric H Huang, An-
drew Y Ng, and Christopher D Manning. 2011.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 151–161.

Richard Socher, Brody Huval, Christopher D Manning,
and Andrew Y Ng. 2012. Semantic compositional-
ity through recursive matrix-vector spaces. In Pro-
ceedings of the 2012 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1201–1211.

Mengqiu Wang. 2008. A re-examination of depen-
dency path kernels for relation extraction. In Pro-
ceedings of the Third International Joint Conference
on Natural Language Processing, pages 841–846.

Kun Xu, Yansong Feng, Songfang Huang, and
Dongyan Zhao. 2015a. Semantic relation classifi-
cation via convolutional neural networks with sim-
ple negative sampling. In Proceedings of the 2015
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 536–540.

764



Yan Xu, Lili Mou, Ge Li, Yunchuan Chen, Hao Peng,
and Zhi Jin. 2015b. Classifying relations via long
short term memory networks along shortest depen-
dency paths. In Proceedings of Conference on Em-
pirical Methods in Natural Language Processing,,
pages 1785–1794.

Mo Yu, Matthew Gormley, and Mark Dredze. 2014.
Factor-based compositional embedding models. In
NIPS Workshop on Learning Semantics, pages 95–
101.

Wojciech Zaremba and Ilya Sutskever. 2014. Learning
to execute. arXiv preprint arXiv:1410.4615.

Mathew D. Zeiler. 2012. An adaptive learning rate
method. In arXiv preprint at arXiv:1212.5701.

Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2003. Kernel methods for relation
extraction. The Journal of Machine Learning Re-
search, 3:1083–1106.

Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou,
Jun Zhao, et al. 2014. Relation classification via
convolutional deep neural network. In Proceedings
of COLING 2014, the 25th International Confer-
ence on Computational Linguistics: Technical Pa-
pers, pages 2335–2344.

765


